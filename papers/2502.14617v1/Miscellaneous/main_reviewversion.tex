%% bare_conf_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE Computer
%% Society conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference]{IEEEtran} %,onecolumn,12pt
%\documentclass{IEEEtran}
% Some/most Computer Society conferences require the compsoc mode option,
% but others may want the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference,compsoc]{../sty/IEEEtran}

% \IEEEoverridecommandlockouts
% % The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Additional fonts
\usepackage[T1]{fontenc} %% https://tex.stackexchange.com/questions/664/why-should-i-use-usepackaget1fontenc
%\usepackage{lmodern} %% https://tex.stackexchange.com/questions/2369/why-do-the-less-than-symbol-and-the-greater-than-symbol-appear-wrong-as
\usepackage[normalem]{ulem} % required for strikeout font
% Default Computer Modern font (no bold implemented)
%\renewcommand{\ttdefault}{cmtt}
% Hence, Using Courier font
\renewcommand{\ttdefault}{pcr}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Inline comments. Pick initials and color of your choice. \ysnote{} refers to Yogesh's note. 
%
\usepackage[usenames,dvipsnames,svgnames,x11names]{xcolor}
\newcommand{\nbc}[3]{
 {\colorbox{#3}{\bfseries\sffamily\tiny\textcolor{white}{#1}}}
 {\textcolor{#3}{\sf\footnotesize$\blacktriangleright$\textit{#2}~$\blacktriangleleft$}}
}

% General comments
% \newcommand{\maybe}[1]{\nbc{MAYBE}{#1}{orange}}

\definecolor{yscolor}{rgb}{0.7,0.3,0.7}
\newcommand{\ysnote}[1]{ {\nbc{YS}{#1}{yscolor}}} % needs a response

% Pick your fave color...
\newcommand{\apnote}[1]{ {\nbc{AP}{#1}{orange}}}
\newcommand{\amnote}[1]{ {\nbc{AM}{#1}{brown}}}
\newcommand{\sjnote}[1]{ {\nbc{SJ}{#1}{blue}}}

\newcommand{\Note}[1]{\textcolor{red}{#1}} % verify if this is correct

\newcommand{\ysnoted}[1]{ {\textcolor{green} { ***TODO Later: #1 }}} % postpone addressing of comment


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Change tracking for article revisions. Added, Deleted, Replaced, or Modified content.
%
\newcommand{\modc}[1]{{\textcolor{blue}{#1}}}
\newcommand{\addc}[1]{{\textcolor{teal}{#1}}}
%% NOTE: \sout will break if the text has citations with an error that there is an extra "}". Place the citation within mbox: \mbox{foo~\cite{foo}}
\newcommand{\delc}[1]{ {\textcolor{gray} {\sout{#1}} }}
%\newcommand{\delc}[1]{} % uncomment this (and comment above line) to ignore showing deletion
\newcommand{\repc}[2]{ {\textcolor{gray} {\sout{#1}} }{\textcolor{teal} {#2}}}
%\newcommand{\repc}[2]{{\textcolor{teal}{#2}}} % uncomment this (and comment above line) to ignore showing deletion
%
%---------------------------------------------------------


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%
%%% UNCOMMENT THE SECTION BELOW TO REMOVE CHANGE TRACKING/COMMENTS
%%%%%%
% \renewcommand{\ysnote}[1]{} % remove comment text
% \renewcommand{\drnote}[1]{} % remove comment text
% \renewcommand{\Note}[1]{#1} % remove red font color, retain text
% \renewcommand{\ysnoted}[1]{} % remove comment text
% \renewcommand{\srnote}[1]{} % remove comment text
% \renewcommand{\rrnote}[1]{} % remove comment text
% \renewcommand{\rrnoted}[1]{} % remove comment text
% \renewcommand{\hgnote}[1]{} % remove comment text
% \renewcommand{\modc}[1]{#1} % remove font color, retain text
% \renewcommand{\addc}[1]{#1} % remove font color, retain text
% \renewcommand{\delc}[1]{} % remove deleted text
% \renewcommand{\repc}[2]{#2} % remove deleted text, remove font color and retain added text

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% *** CITATION PACKAGES ***
%
\usepackage[nocompress]{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later.

%% Balances the bibliography columns in last page
\usepackage{balance}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% When using XML fragments, using pretty-print is helpful.
%
\usepackage{listings}
% \usepackage{color}
% \definecolor{gray}{rgb}{0.4,0.4,0.4}
% \definecolor{darkblue}{rgb}{0.0,0.0,0.6}
%\definecolor{maroon}{rgb}{0.5,0,0}
% \definecolor{cyan}{rgb}{0.0,0.6,0.6}

\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  breaklines=true,
  showstringspaces=false,
  commentstyle=\color{gray}\upshape,
  escapeinside={||},
  mathescape=true
}

\lstdefinelanguage{XML}
{
basicstyle=\ttfamily\footnotesize,
  morestring=[b]",
  moredelim=[s][\bfseries\color{Maroon}]{<}{\ },
  moredelim=[s][\bfseries\color{Maroon}]{</}{>},
  moredelim=[l][\bfseries\color{Maroon}]{/>},
  moredelim=[l][\bfseries\color{Maroon}]{>},
  morecomment=[s]{<?}{?>},
  morecomment=[s]{<!--}{-->},
  commentstyle=\color{gray},
  stringstyle=\color{blue},
  identifierstyle=\color{red}
%  morekeywords={type,id,value,impl}% list your attributes here
}
%
%---------------------------------------------------------


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Better control over verbatim text
\usepackage{moreverb}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% for syntax/grammar
\usepackage[nounderscore]{syntax}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[pdftex]{graphicx}
% declare the path(s) where your graphic files are
\graphicspath{{./figures/}}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
\DeclareGraphicsExtensions{.pdf}
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex

% Definitions for placeholder figures
\newcommand{\dummyfigX}{\fbox{\parbox[h][1.75in][t]{0.95\textwidth}{\emph{Placeholder}}}} % full page width (figure*)
\newcommand{\dummyfigXX}{\fbox{\parbox[h][1.75in][t]{0.95\columnwidth}{\emph{Placeholder}}}} % 1 column width (in 2 column format)
\newcommand{\dummyfigXXX}{\fbox{\parbox[h][1.75in][t]{0.30\textwidth}{\emph{Placeholder}}}} % 1/3 full page width (figure*)
\newcommand{\dummyfigXXXX}{\fbox{\parbox[h][1.75in][t]{0.23\textwidth}{\emph{Placeholder}}}} % 1/4 full page width (figure*)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{gensymb}

\newtheorem{cons}{Constraint}
\newcommand\Overline[1]{\overline{\overline{#1}}}


%
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% *** SUBFIGURE PACKAGES ***
\usepackage{subfig} %[caption=false,font=footnotesize,labelfont=sf,textfont=sf]
%
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** SPECIALIZED LIST PACKAGES ***
%
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[ruled]{algorithm}
\definecolor{light-gray}{gray}{0.75}
\algrenewcommand{\algorithmiccomment}[1]{\hskip3em{{\footnotesize \textcolor{light-gray}{$\blacktriangleright$}}} #1}
%
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table relates packages
\usepackage{multirow} % Multi-row tables
\usepackage{rotating} % sideways table
\usepackage{booktabs} % better lines
\usepackage{colortbl} % cell colors
\usepackage{tablefootnote} % add support for footnote in table

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage[colorlinks,bookmarksopen,bookmarksnumbered,citecolor=red,urlcolor=red,pagebackref=true]{hyperref}
\usepackage[pdftex,colorlinks=true,urlcolor=blue,citecolor=blue]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Avoids contiguous empty spaces
\usepackage{xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% IEEETrans class fix for enumitem. provide for legacy IED commands/lengths when possible
% http://comments.gmane.org/gmane.editors.lyx.general/68611
\let\labelindent\relax
\usepackage{enumitem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% correct bad hyphenation here
\hyphenation{compu-ta-tio-nal}

% define repetitive complex fragments here
\newcommand{\floe}{$\mathcal{F}{\ell}{o}{\varepsilon}$\xspace}
\newcommand{\prc}{\mathcal{P}}
\newcommand{\chn}{\mathcal{C}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% generate lorum ipsum placeholder text
%\usepackage[english]{babel}
\usepackage{blindtext}
\newcommand{\blindtextc}{\color{gray}\blindtext\color{black}}
\newcommand{\Blindtextc}{\color{gray}\Blindtext\color{black}}




\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Serving Models, Fast and Slow:\\
Optimizing Heterogeneous LLM Workloads}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Author Names}\\
\IEEEauthorblockA{Acme Corp., Bangalore, India\\
% Department of Computational and Data Sciences\\
% Indian Institute of Science, Bangalore 560012 INDIA\\
Email: lname1@email.com, lname2@email.com}
% \and
% \IEEEauthorblockN{Dreamer Student}
% \IEEEauthorblockA{Department of Computational and Data Sciences\\
% Indian Institute of Science\\
% Bangalore 560012 INDIA\\
% Email: dreamer@grads.cds.iisc.ac.in}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page (and note that there is less available width in this regard for
% compsoc conferences compared to traditional conferences), use this
% alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}


% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}


% make the title area
\maketitle

%% Uncomment this to enable page numbering. OMit this before submission.
\thispagestyle{plain}
\pagestyle{plain}

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}

% LLM inference jobs often include both latency-sensitive and insensitive workloads, resulting in a wide range of SLAs. The inference stack comprises different LLMs and hardware, making request scheduling and model placement a challenging optimization problem. 
% However, many of these options are currently siloed, resulting in significant under-utilization of cloud resources. Although cloud providers do offer spot instances alongside on-demand instances to optimize resource utilization, these strategies still result in a lot of under-utilization.

% Our proposal is to meet SLA guarantees while leveraging all available resources.We suggest different control knobs at different time scales to improve utilization while meeting SLAs. For example, routing at finer time intervals, scaling in and out with spot instances, and inter-model redeployment at a coarser granularity. In doing so, we can optimize resource utilization and meet the SLA guarantees.

% To better understand the benefits of having different control knobs at different time scales, we conducted an empirical study.
% We characterized the benefits in terms of GPU hours and the utility of serving both latency-sensitive insensitive requests. Furthermore, we propose an ILP-based formulation and incorporate different strategies for optimal resource allocation and request scheduling. 
% By simulating the deployment of four open-source models across three regions and testing different policies at scale, we were able to demonstrate the effectiveness of our proposal in optimizing resource utilization while meeting SLA guarantees. 


LLM inference workloads often span both latency-sensitive and insensitive tasks, creating a diverse range of SLA requirements. Managing these workloads is challenging due to the complexity of the inference stack, which includes multiple LLMs, hardware configurations, and geographic distributions. Current optimization strategies are often siloed, leading to significant under-utilization of cloud resources despite the availability of mechanisms like spot and on-demand instances. To address this, we propose a comprehensive approach that ensures SLA compliance while maximizing resource utilization through adaptive control knobs at varying time scales. Short-term optimizations include efficient request routing, while long-term strategies involve scaling resources and redeploying models to align with traffic patterns. We performed an empirical study to characterize the advantages of this approach in terms of GPU hours saved and utility improvements for handling diverse SLA requirements. \amnote{Need some results for the empirical study also} We also formulated an ILP-based resource allocation strategy and simulated the deployment of four open-source models across three regions to demonstrate the effectiveness of our proposal in optimizing resource usage while meeting SLA commitments. \apnote{ToDo: add some remarks on results.}
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

There is a push towards supporting intelligent features within enterprise products and services, and augment the behavior of clients through the use of LLMs and other ML/AI models. Such features may be unobtrusive and happening proactively in the background, or actively initiated by the users. There are also rapid advances towards exploring the multitude of scenarios to which LLMs can be applied to and newer use-cases are being continuously developed. As their capabilities continue to grow, the use of LLMs for various enterprise and consumer applications will rise exponentially. 

GPU-accelerated servers hosted on the cloud are the vanguard of enabling inferencing over LLMs at massive scales. Cloud providers and large Internet companies are investing heavily on GPU hardware, both for internal consumption by their products and services, and to rent them out as GPU VMs or as AI inferencing services. E.g., AWS UltraClusters offer 20,000 Nvidia H100 GPUs per region for training workloads~\footnote{https://www.theregister.com/2024/01/15/cloud\_providers\_gpu\_analysis/}, while Meta and Microsoft reportedly purchased 150,000 H100 GPUs in 2023~\footnote{https://www.theregister.com/2023/11/27/server\_shipments\_fall\_20\_percent/}.
% Keeping with this growth, 
At the same time, the utilization...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Related Work}
% \label{sec:related}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{System and Application Model}
\label{sec:sys-app-model}


%%==================================================
\subsection{Cloud Regions and GPU VMs}
% regions, VMs, counts
Our system model comprises of a public cloud service provider (CSP) with multiple data centers (regions) having captive GPU server capacity in each. For simplicity, we assume that these regions are in the United States (US), e.g., US-West1, US-East1, etc., and avoid issues of data sovereignty. These regions are connected with high bandwidth network, with a typical network latency between them of $\approx 50ms$. Each region has servers with a mix of GPU generations that can be provisioned as GPU VMs with exclusive access to all underlying server resources, e.g., Azure's ND series of VMs each with 8 Nvidia V100, A100, or H100 cards with infiniband interconnect, or AWS's comparable P5/P4/P3 EC2 instances with NVSwitch connectivity. Each region may have 1000s of such GPU VMs (henceforth called just ``VMs'') that can be provision within the available capacity.



%%==================================================
\subsection{LLM Workloads}
\subsubsection{Deadlines and Latencies of Workloads}
% types of WLs
The CSP has a large demand for LLM inferencing to support several types of \textit{production workloads}.  One, is to support their enterprise products and services that are \textit{interactive and client-facing \textbf{(IW)}}, with \textit{low latency constraints (O(seconds))}, e.g., to summarize a document, generate email responses, chatbots, etc. These are models requiring ``fast'' serving. Another is a set of batch or \textit{non-interactive workloads \textbf{(NIW)}} with more \textit{relaxed deadlines (O(minutes--hours))}, e.g., summarizing a the recording of a video call, nightly summary reports on documents in an Enterprise repository, detailed content generation, etc. Lastly, we have a set of opportunistic workloads \textbf{(OW)}, which are generated by internal developers for testing new features or new LLM models. These do not have any set deadline, are completed based on available capacity and expire if not completed within some relaxed deadline such as 24hrs.
% \ysnote{check if this is a reasonable assumption. 24hrs?}.
These last two workload models, NIW and OW, require ``slow'' (or ``no'') serving.


\subsubsection{LLM Models, Endpoints and Instances}
% custom vs. default models
There are several standard pre-defined \textit{LLM architectures} that are available, e.g., GPT4, GPT3.5 turbo, etc. and used by IW and NIW as well as custom ones for the OW supplied by the developers. Further, each model architecture can either use default weights to give a standard behavior, or have been fine-tuned for a specific application and have custom weights. The combination of model architecture and weights forms a \textit{LLM type}.

Some of the model deployments may also have been configured specifically for high-throughput to handle batch requests and others for low-latency to service interactive requests. For simplicity, we treat them as separate model types where NIW and OW will request the former and IW the latter.

\subsubsection{Workload Requests}
% workload behavior, SLA
For IW, clients for each product/service 
% \ysnote{"scenario" in internal parlance} 
may use one or more pre-defined LLM type, and this inferencing is initiated by the client while using the product. There can be \Note{$100,000$s} of client users and we observe a diurnal pattern in the requests that are received. For simplicity, we assume US-based clients since the cloud regions are are assumed to be in the US and their peak request rate can be as high as $12,000~requests/min$ for a single model type.  
% \ysnote{25k for eastUS, 20k for westUS and 15k for centralUS for gpt35turbo}
\ysnote{need to include some request load plots in the paper after suitable anonymization}
These workloads have a \textit{SLA latency requirement} that ranges from \Note{a second to a minute}, which is influenced by the Time to First Token (TTFT), the Time Between Tokens (TBT), the number of input and output tokens, and any internal network routing overheads.

NIW also uses a set of pre-defined LLMs whose architectures often overlap with the IW ones.
% but which have have specialized weights to form a different model type. 
The request load for NIW, however, is more variable and they are also fewer in number, e.g., \Note{???--???/hour}. \ysnote{need to include some request load plots after suitable anonymization}
Further, given their batch nature, the SLA for these workloads is specified by a \textit{deadline} that can range between 5--30 mins for a normal priority job, summarizing a video conference call, to 1--12 hours for more relaxed ones, such as summarizing a document repository.
% Examples of such NIW workloads include content and document summarizations, content generation, semantic search, etc.
The difference between IW and NIW is primarily in terms of the deadline \ysnote{and also on the request rate?}, and this can be a continuum \ysnote{do we take different approaches to solving them? What is the cutoff point between IW and NIW?}.

The request for OW is unpredictable and we may receive \Note{???--??? per day}. These will typically use a custom LLM architecture and weights, and the request expires after \Note{???~hours} if unserved.

\ysnote{Is this a good place to bring in a utility model for completing these workloads? Penalties for non-completion?}






%%==================================================
\subsection{LLM Instances and Deployment}
% # instances, performance
\ysnote{batch sizes are fixed for a model/HW; Models can run on diff HW but usually fixed number of instances (we will relax this assumption in this paper);}


Each LLM type can have one or more instances created for it. Each instance can require one or more GPU VMs and may be deployed on one of several GPU types, depending on the size of the LLM, e.g., a GPT4 model may require \Note{3 VMs with $8\times$ A100 or H100 GPU each} \apnote{to check if this needs to be anonymizedoA}, while a GPT3.5 turbo model may be deployed on \Note{1 VM with $8\times$ V100 or better GPUs each}. If multiple VMs are required to host a single LLM instance, they will all be of the same VM type.
% \ysnote{confirm that we \textit{do not} deploy multiple models on the same VM.} 
The VM size on which the LLM is instantiated will determine its performance, \Note{defined in terms of tokens/second of throughput achieved at a certain target latency}. E.g., a GPT3.5 turbo 0125 deployed on a VM with $8\times$ H100 GPUs can support a throughput of \Note{35,000 tokens/s} and achieve a \Note{P99 latency of ???~s} for \Note{???~output tokens? Input+Output tokens?} but this throughput drops to \Note{14,000 tokens/s} for a VM with $8\times$ A100 GPUs \cite{splitwise}.

\ysnote{Need data on TPS for a Model type on a GPU type}

\ysnote{need to discuss performance capacity and metrics. Should we assume the latency SLA is fixed for a model instance and as long as input request rate stays within this max throughput, the latency SLA will be met? Or should we also consider diff latency SLA and model thruput vs latency using splitwise/vidhur for diff model types and GPUs?}

\apnote{For a given workload defined by IO and arrival rate, We can include a plot that shows the change in TPS with different model type and underlying hardware?}

Each LLM type also has an \textit{instance set size}, i.e., the number of instances of the model that will be incrementally created or scaled down.  
Each instance set corresponds to an endpoint to which the incoming request load for that LLM type is directed to. There is a simple round-robin redirection that internally happens across these instances. \Note{This is both for load balancing to account for token skews as also for redundancy in case of VM failures.}
% \ysnote{verify}
The VM type for all instances in the set must be identical to ensure similar performance for each LLM instance.
% \ysnote{verify}
All LLM endpoints for the same LLM type have the identical semantic behavior.

% \ysnote{Do we bring in the constraint about infiniband connectvity here? Looking at the documentation for ND VMs, there is a notion of a scale set and VMs within the same scale set have infiniband provisioned. Will it be too much complexity for the paper? \textit{"Each GPU within the VM is provided with its own dedicated, topology-agnostic 200 GB/s NVIDIA Mellanox HDR InfiniBand connection. These connections are automatically configured between VMs occupying the same virtual machine scale set, and support GPUDirect RDMA"~\footnote{https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/gpu-accelerated/nd-family}}. Skip.}

%%==================================================
\subsection{LLM Endpoint Provisioning}

% Difference bwteen arch deployment and weight update. Deployment time for instances
Deploying an LLM instance set to GPU VMs allocated to the set and creating a new endpoint for it has provisioning costs. 
% clean slate
If the GPU VMs allocated to the instance set do not have any prior LLM deployed on them, then the new LLM's model architecture and weights need to be copied and instantiated on the VMs. This can be costly, depending on the size of the LLM and its weights, e.g., taking \Note{???~mins} for a GPT4 model on a VM with $8\times$H100 GPUs. 
% update weights for existing model...from local or remote region
Alternatively, if the VMs already have the same LLM architecture deployed on them from a prior provisioning but is a different LLM type, i.e., with different weights, \Note{this reduces the cost for deployment.} In such cases, we can load the new weights into the existing model with a lower cost than a fresh deployment. 
Here, the loading cost will depend on whether the weights for the model are available in a local storage repository within that region or have to be moved from a remote region. E.g., updating the weights for a GPT4 instance from the local region takes \Note{5~mins} while moving it from, say, US-East to US-West and loading it may take \Note{???~mins}. 
\apnote{is this a good point to discuss the cost matrix associated with redeployement and pulling weights from region a to b, etc.}
\ysnote{Need to check. Is there any reduced deployment cost if (1) an LLM arch is already on the VM but weights need to be updated from the local region, vs. (2) empty VM without any LLM deployed and model has to be deployed with weights from local region? Or is the diff only between whether the weights are local to the region or have to be pulled from a remote region?}

% wasted cycles
During the time that a model is being provisioning on a new GPU VM, or its weights being updated in an existing GPU VM, these instances are not available for use. So the time spent in model provisioning leads to wasted GPU VM cycles. Given that this time can run into hours in some cases, frequent re-provisioning of model instances can be inefficient.

% spot instances
The workloads are executed on LLM instances that are provisioned in a private network space.
However, in case there is over-provisioning of standard LLM models and their endpoints are not in use by these workloads, it is possible to lease these LLM inferencing endpoints as (preemptible) spot instances to external users and reclaim them when there is higher demand. The time to reclaim these endpoints and instance set is usually 5~mins. Typically, the utility benefits of leasing out as spot instances is lower than that gained from executing the internal workloads. During some periods, 25\% of instances in a data center may be donated to spot instances, and this indicates a lost opportunity cost.
% SLA is 5 min for scaling up/down 3 instances. Usually takes <1min.

% managed, trusted environment, no security issues
We assume that we are working in a managed network and trusted security environment, and there are not other security constraints that limit the mapping of models to VMs, or endpoints to workloads or external users.


%%==================================================
\subsection{Inference Request Routing}


% routing to endpoints
All LLM inferencing requests arriving as part of the various internal workloads are directed to a common LLM API service \ysnote{Through a content delivery service (Azure Front Door)? Need more details on how the request hits the LLM API.}. This service redirects the request to one of several LLM endpoints that can service the request. \Note{This is based on the load on the endpoints, ...}

\ysnote{how much of this routing logic do we plan to control/change vs. How much do we plan to reuse it in some deterministic manner? Depending on that, we need to either give more details here or cover it below in the problem definition. If we dont change the routing, will this degenerate to a problem affecting just a single region?}
\ysnote{IW can be routed automatically to local regions (can we get heatmap of source request---sink region to support this?. For NIW and OW, we will have a queue and we will manage the routing.}

\ysnote{can we assume that the slow (NIW and OW) requests are held in a queue and picked up/pulled by an endpoint during low-load periods? Or should they be routed immediately to an endpoint?}

\ysnote{There can be inteference between requests with heavy--heavy token counts and light--light token counts sent to the same instance based on how the batching happens. So can we try and avoid intereference between different request type mixes?}

\ysnote{we may need to consider at what rate the NIW workloads introduce requests into the request stream for an instance as this insatnce may be used by IW workloads as well. If request rate for NIW is high, it will be done fast but inroduce more load and may affect IW. If it is low, lower interference but slower completion.\\
Can the same NIW batch be sent to different model instances?}

\ysnote{capital efficiency as a goal}
\apnote{We need to highlight the unique aspect of this problem? How is this different from other scheduling and binpacking problems? Any aspects of LLM workloads/deployments that make it interesting/challenging.}
\apnote{reference to add: practical scheduling techniques: \cite{kossmann2024gpu} }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Optimization Problem}
\label{sec:optz}


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/arch.pdf}
    \caption{Optimized Routing and Load Balancing Architecture (\apnote{We will replace East and west with Region 1 and 2.}}
    \label{fig:arch}
\end{figure*}



%%==================================================
\subsection{Problem Description}
\label{sec:optz:descr}

% Utility
We solve the problem of request routing
% \ysnote{is it in scope?}} 
and capacity utilization to serve the fast and slow LLM inferencing workloads within the required SLA. 
\apnote{I think it would coming in the later sections. But, do we also need to highlight here the heterogeneity among the workloads, hardware, and models?}
% \apnote{Let's consider that otherwise, as you said the problem gets simplified to a single region and might lose some interesting aspects of latency and OW.}

\noindent \textbf{Definition.}~\textit{
Given a captive set of GPU VMs of specific types in multiple regions, 
\begin{itemize}
    \item we need to ensure that the right number of model instances of different model types are provisioned as endpoints at the regions, and 
    \item we need to route the incoming workload across these endpoints,
\end{itemize}
such that
\begin{itemize}
    \item we maximize the utility of the workload requests completed within their SLA, and 
    \item maximize the capacity utilization of the GPU VMs for the internal workloads.
\end{itemize}}
\apnote{do we need to separate out the objectives on capacity utilization and utility of serving internal workloads?}
We have several parts to solving this high level problem. 
%\apnote{For time being, we can assume that the traffic from IW is predictable and can ignore the organic growth of traffic from different scenarios and also the new scenarios/products that may land.}

One, we need to \textbf{forecast the load} for the different model types over time generated by the different workloads. Here, the forecasting has to be at coarse granularity and time horizon (e.g., at 15~mins for 24~hour) to handle reprovisioning of LLM instances, and at finer granularity and horizon (e.g., at 1~min for 1~hour) for routing requests 
% \ysnote{does routing require forecasts?} \apnote{Routing might not need forecasting as the load will be balanced based on the current status of the LLM instances}.
%\apnote{ToDo: decide the granularity of decision making (min/hrs)?}
\apnote{Do we need to provide the rationale for deciding the granularity? Such as traffic varies in x minutes or the time needed for scaling in/out?}
While the bulk of the workloads that involve product users have a periodicity, NIW and OW do not have such a discernible pattern and we need to account for the same.
We also need to be able to predict the input and output token counts since they influence the latency time for response, besides the input request rate~\cite{melange}.
\Note{Estimating the request load from clients in different geolocations (proximity to the nearest region) will also help with the routing, and reducing network latency.}
% \apnote{we can assume that we know the geolocation of client sending the request.}
% \ysnote{relevant if we're doing routing and network latency between client and endpoint is non-trivial} 
\ysnote{It looks like the NW latency is mostly within 100ms (75 p'centile) based on plots. But knowledge of client source can help route to local region and avoid cross-region routing to reduce NW bandwidth.}

Two, we need to \textbf{provision model endpoints} in different regions to handle this workload within the available VM capacities. Since there is an overhead for (re)provisioning an LLM instance set onto VMs, we need to consider the inefficiency due to this process when the GPU VMs are not serving.
% , and also be able to handle the increased load on the remaining endpoint(s) for any 
Given this, such reprovisioning decisions are viable at a coarse granularity, e.g., considered every 15~mins -- while some reprovisioning such as reclaiming spot instances can be fast, e.g., at every 1~min.
% , to deploy models on new VMs are slow.

% latency SLA under different thruput.
% Three, we need to ensure that the provisioned resources are able to \textbf{meet the SLO request}. As part of reprovisioning resources, we need to be able to estimate the expected latency for serving requests at a certain request rate to ensure we meet the SLA for the product~\cite{splitwise}.

% routing 
Three, we need to \textbf{route the requests} that arrive to the current set of model endpoints across different regions while meeting the SLA. These routing decisions can make use of real-time information on the load and responsiveness of the endpoints in different regions.
As part of reprovisioning resources and routing, we need to use relevant tools~\cite{splitwise} to estimate the expected latency for serving requests at a certain request rate to ensure we meet the SLA for the product.
%
Any spare capacity of model instances that are not being used for workloads can be released to external spot instances.

\ysnote{this is a good place to give an example, in case one has not been used in the introduction}

\ysnote{We need an application/workload utility model to cleanly specify the problem. E.g.,\\
for production sync (IW) requests, a small positive utility if a request is served within SLA, a small negative utility if served below SLA, and large negative utility if timed-out/dropped;\\
for prod async (NIW) requests, a large positive utility if a request is served within deadline and and large negative utility if timed-out;\\
for OW dev workloads, a medium positive utility if a request is served within deadline;\\
and for spot instances, a small positive utility for the duration it is being leased out.}


We can formulate this as an optimization problem, which we defined next. 


%%==================================================
\subsection{Formal Definition}
\label{sec:optz:defn}

\subsubsection{VMs and Regions}
% VMs
We assume there are $r$ data center regions
% , $R_1..R_r$. There 
and there are $g$ GPU VM types.
% , $G_1..G_g$. 
Each region $q$ has sufficient capacity to provision $\kappa_{q,k}$ number of VMs of type $k$.

\subsubsection{Models and Endpoints}
% models and deployment
There are $l$ LLM architectures, $L_1..L_l$, each having a different model architecture. A model architecture $L_i$ can be configured with a particular set of weights $w_j$ to form a model type $\mu_{i,j} \in \mathbb{M}$. \ysnote{in case we dont have any optimization to reload an existing model with new weights, this distinction between model arch and model type is not needed.}
The size of the weights on disk for each model type \ysnote{or model arch?} is $w_{i,j}$~GB.
Each model type will require $v_{i,j,k}$ number of VMs of type $k$ to deploy a single instance.
Each model type also has a minimum instance set size $\overline{n}_{i,j}$, i.e., the minimum number of instances to be created for it to form an endpoint where the incoming load can be directed to, as well as a maximum instance set size, $\widehat{n}_{i,j}$. The current number of instances for a model at a given time is $n_{i,j}$.

%\apnote{To Do: The need for $n_{i,j}$ instances seems not straightforward. It would be good if we can find a rationale for this.}.
%\ysnote{I've reworded this to min and max scale set size.}

There is an internal load balancing among these instances, typically done in a round-robin manner. 
The throughput supported by each model type on each VM type it is deployed on is given by $\overline{\theta}_{i,j,k}$~tokens/s; the cumulative throughput supported by the corresponding endpoint at a given time is $\theta_{i,j,k} = n_{i,j} \cdot \overline{\theta}_{i,j,k}$~tokens/s.

The expected latency for serving a request with $\tau$ (input+output) tokens by an instance of model type $\mu_{i,j}$ running on VM type $k$ is given by a function $\mathcal{L}(i,j,k,\tau)$,
% $ = \frac{\tau}{\overline{\theta}_{i,j,k}}$, 
as long as the total tokens serviced per second at that instance is within the provisioned throughput $\overline{\theta}_{i,j,k}$. 
However, there may be variability in this latency at runtime due to KV Store locality effects, batching, etc.
% } \ysnote{AP to confirm}
% \ysnote{is this correct or simplifies it too much? should we instead just treat this as a blackbox function that we get the estimate from?}
% \apnote{The latency expression above yields the lower bounds. This can be the best we can get and it goes up based on other requests served concurrently and other factors. We can use our latency estimator models from the routing paper to get some approximations.}
If the request rate exceeds the provisioned throughput capacity, the latency degenerates rapidly.
% \ysnote{Is this correct? do we have evidence of this?} \apnote{Yes. I will add relevant citations here.}

% Allocation of GPU VMs to model instances,
Endpoints may be provisioned for internal IW, NIW and OW workloads or as spot instances.
%
% The function $\mathcal{P}_q(i,j,k)$ gives the count of the current number of VM instances of type $k$ in region $q$ that are assigned to a model of type $\mu_{i,j}$. Likewise, $\mathcal{S}_q(i,j,k)$ give the count for the number of VMs assigned to spot model instances.
% Similarly, the number of production and spot model instances of type $\mu_{i,j}$ in region $q$ deployed on VMs of type $k$ is $\overline{\mathcal{P}}_q(i,j,k)$ and $\overline{\mathcal{S}}_q(i,j,k)$, and 
%
\ysnote{confirm that the preference is to scale up to max instance set size for existing endpoint before creating a new endpoint}
Let the number of endpoints of a model type $\mu_{i,j}$ in region $q$ deployed on VMs of type $k$ be $\mathcal{I}_q(i,j,k)$ and $\mathcal{S}_q(i,j,k)$ for internal and spot endpoints.
%
Hence, the number of instances of each model type on a VM type depends on the instance set size, and given by $\overline{\mathcal{I}}_q(i,j,k) = \mathcal{I}_q(i,j,k) \cdot n_{i,j}$ for internal workloads, and similarly for spot instances $\overline{\mathcal{S}}_q(i,j,k)$.
%
The total count of the VM instances of type $k$ in region $q$ that are assigned to a model of type $\mu_{i,j}$ serving internal workloads depends on the number of VMs required by that model, and given by 
$\Overline{\mathcal{I}}_q(i,j,k) = \overline{\mathcal{I}}_q(i,j,k) \cdot v_{i,j,k}$, and similarly for VMs assigned to spot instances, $\Overline{\mathcal{S}}_q(i,j,k)$.

The constraint requires that the number of VMs of each type on which models are deployed in a region $q$ are within the total number of VMs of that type in that region:
\[ 
\sum_{\forall \mu_{i,j} \in \mathbb{M} }\Overline{\mathcal{I}}_q(i,j,k) +
\sum_{\forall \mu_{i,j} \in \mathbb{M} }\Overline{\mathcal{S}}_q(i,j,k)
\leq
\kappa_{q,k} 
~~\mid~~
\forall k \leq g\]

The cumulative throughput available across all production endpoints of a model $\mu_{i,j}$ deployed on different VM types $k$ in a region $q$ is $\Theta_{i,j,q} = \sum_{\forall k \leq g}\theta_{i,j,k} \cdot \mathcal{P}_q(i,j,k)$

The total VM utilization in a region $q$ for production workloads is the VMs allocated to production endpoints across VM types over all VMs of all types, 
$U_P = \frac {\sum_{\forall \mu_{i,j} \in \mathbb{M},~k \leq g }\Overline{\mathcal{P}}_q(i,j,k) }{ \sum_{\forall k \leq g} \kappa_{q,k}}$. 
\ysnote{this is ignoring VM types...weight it by VM gpus?}\ysnote{this has to be maximized, provided their endpoint capacity utilization is high.}

The total VM utilization in a region for spot workloads is $U_S = \frac {\sum_{\forall \mu_{i,j} \in \mathbb{M},~k \leq g }\Overline{\mathcal{S}}_q(i,j,k) }{ \sum_{\forall k \leq g} \kappa_{q,k}}$
\ysnote{this has to be reduced, provided they are assigned to production endpoints with high capacity util.}

The total unused VM utilization in a region is $(1 - U_P - U_S)$.

\ysnote{this has to be zero}

\apnote{Is IW, NIW, and OW part of production endpoint?}

\subsubsection{Reprovisioning}
% Reprovisioning
The time to load a model instance onto a set of $\mu_{i,j}$ onto $v_{i,j,k}$ VMs is $t_{i,j,k}$, assuming the model weights are local to that region and the VM is a fresh instance. \ysnote{does the time depend on just the model arch or the model type, i.e., do the weights play a role?}
The time to update the model type to $\mu_{i,j}$ on a set of $v_{i,j,k}$ VMs already having the same model architecture $L_i$ but different weights is $t'_{i,j,k}$, assuming the weights are in the local region.\ysnote{check if this is relevant or it makes no diff.}


We assume that all model instances in the same instance set can be loaded concurrently. We also assume there is no additional time to release and reassign a VM from one model instance to another.
The Boolean mapping function $\mathcal{A}(i,j,q)$ gives the availability of the model architecture and weights for model type $\mu_{i,j}$ in a region $q$.  If the model is not in the local region $q$, there is a transfer time $\frac{w_{i,j}}{\beta_{p,q}}$ to fetch the model of size $w_{i,j}$~GB from the region $p$ it is present in to region $q$, depending on the known network bandwidth $\beta_{p,q}$~Gbps between the regions.

The expected time to reacquire a spot endpoint for internal use is $\widehat{t}$, without any changes to the model instance it is hosting.
% \ysnote{assuming this is constant, irrespective of model type.}
% \apnote{This is a fair assumption.}

\subsubsection{Workloads}

% \ysnote{YS to resume from here...}
IW and NIW requests are generated by clients in different regions as part of products that they use. A IW product $P_x$ is defined by the model type $\mu_{i,j}$ it uses, the median input+output token count $\tau_x$ for its requests \ysnote{we need a token size distribution instead}, the deadline $\delta_x$ by which the request must be completed, and the utility $\gamma_x$ that is gained from serving the request within the deadline.

For the batch workloads, NIW and OW, we similarly have products $Q_y$ with all the above features, and in addition the number of requests $b_y$ present in the batch, with the deadline specified for completion of the entire batch.

% workload:
% \Note{IW \textit{product} targets a model type, has a token size distribution (input+output) per request, a deadline and a utility for completion within the deadline; }

% \Note{NIW and OW are similar to above, but also have a batch size with the number of requests present in each task.}

The request rate per second for a IW product $P_x$ from clients in a region $q$ during a time window $w=(t_1,t_2)$ is given by $\rho_{q,x,w}$.
The number of requests of type NIW and OW for each product $Q_y$ that arrives within that same window is $\pi_{q,y,w}$
While the request rate for IW products from each region can be forecast, this is not the case for NIW and OW products.

The requests from a client product in a region are sent to a common API hosted on a Content Distribution Network (CDN) like Azure Front Door expected to be close to the client, which then routers this it one of the relevant endpoints in one of the regions. The end to end latency for the request is the sum of network latency to route from the client's region to the endpoint's region, and the latency for executing the request at the endpoint; this should complete within the deadline. The network latency between regions $p$ and $q$ is given by $\eta_{p,q}$, where $\eta_{p,p}=0$. We omit the latency from the client to the CDN since it is proximate and negligible. 
For NIW and OW, we omit the network latency since it is a small fraction of the batch of requests.

The deadline constraint for an IW request can be specified for a request from product $P_x$ in client region $p$ to a model type $\mu_{i,j}$ with total token size $\tau$ that is routed to an endpoint in region $q$ that has the model deployed in VMs of type $k$ as $\mathcal{L}(i,j,k,\tau) + \eta_{p,q} \leq \delta_x$.

% \ysnote{given eqn for constraint}

\subsubsection{Optimization Goal}

\ysnote{if given forecast for IW over time, and provided with NIW and OW requests on-demand}

\ysnote{make routing decision on IW request rate per client/product sent to an endpoint in a region, and}

\ysnote{make routing decision on NIW/OW request routing and release rate sent to endpoint in a region}

\ysnote{make mapping decision in a region}

\ysnote{such that we maximize sum of utility gained across all windows}

\ysnote{such that request capacity constraints for requests sent to an endpoint in a region is not exceeded within a window}



% what is the temporal unit of decision making?


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Long Term Allocation (ILP)}
We use an ILP to find the optimal resource allocation on a longer time frame.

\subsection{ILP Formulation}
The ILP will try to minimize the cost of resources whilst servicing all the low-latency workloads. It will take into account re-routing and having multiple kinds of GPUs.
\subsubsection{Input}
\begin{itemize}
    \item $l$: number of different models we will try to serve
    \item $r$: number of regions
    \item $g$: number of GPU architectures we have
    \item $C: [\texttt{int}]_{l\times r\times g}$: $C_{i, j, k}$ is the number of instances of model $i$ at region $j$ running on GPU architecture $k$
    \item $R: [\texttt{int}]_{l\times r}$: $R_{i, j}$ is the \textbf{TPS requested} for model $i$ at region $j$ 
    \item $\bar{\theta}: [\texttt{float}]_{l\times g}$: $\bar{\theta}_{i, k}$ is the TPS of model $i$ on GPU $k$
    \item $G: [\texttt{float}]_{g}$: $G_{k}$ is the cost of acquiring GPU $k$
    \item $s: [\texttt{float}]_{l\times G}$: $s_{i, k}$ is the cost of acquiring model $i$ on GPU $k$
\end{itemize}
\subsubsection{Decision Variable}
Our decision variable $\delta$ will have dimension $l\times r\times g$ where $\delta_{i,j,k}$ will decide the change in number of instances assigned to model $i$ at region $j$ running on GPU $k$. 
\subsubsection{Constraints}
\begin{itemize}
    \item The first constraint will be to not deallocate more models at a particular configuration than there exist: $$M_{i,j,k} \geq C_{i, j, k}$$
    \item Second, we should ensure that we have capacity to process all the low latency tokens: 
    $$\theta_{i} = \sum^{j}\sum^{k}(C_{i, j, k} + M_{i, j, k}) * \bar{\theta}_{i, k} \text{  }\forall\text{ 
 }i\in\{l\}$$
    $$\theta_{i} \leq \sum^{j}R_{i, j} \text{ 
 }\forall\text{  }i\in \{l\}$$ Because we want to consider re-routing, this condition only checks that for each model, we should have enough capacity over all regions
 \item Next, we want each region $j$ to process atleast $(1-\gamma)R_{i, j}$ and atmost $(1+\gamma)R_{i, j}$ of its tokens for model $i$ in order to limit routing:
 $$\hat{\theta}_{i, j} = \sum^{k}(C_{i, j, k} + M_{i, j, k}) * \bar{\theta}_{i, k}$$
 $$(1-\gamma)R_{i, j} \leq \hat{\theta}_{i, j} \text{  }\forall\text{  }(i, j)\in\{l\}\times \{r\}$$
\end{itemize}
\subsubsection{Objective}
Our objective is to minimize the cost for capacity for answering all queries. Cost is affected by allocating new GPUs as well as the start up time for each model. Similarly, it is reduced when you deallocate any GPUs, but the model does not impact that cost:
$$\texttt{GPU cost} = \sum^{k}((\sum^{i, j}M_{i, j, k}) * G_{k})$$
$$I_{i, j, k} = \max(0, M_{i, j, k})$$
$$\texttt{Model startup cost} = \sum^{k}\sum^{i}(\sum^{j}I_{i, j, k} * s_{i, k})$$
$$\min (\texttt{GPU cost} + \texttt{Model startup cost})$$
\subsection{Load Estimation}
We use ARIMA modelling for estimating the load
\subsection{Buffer Addition using NIW}
We add known non-interactive workloads to the estimated load as buffer
\subsection{Combining Proactive and Reactive Mechanisms}
The proactive mechanism will tell us the final state we should aim for. The reactive mechanism will help us guide there. It will try to converge to the proactive mechanism when there are bursts of requests.


\section{Architecture}
\label{sec:arch}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\label{sec:results}

 
%%==================================================
\subsection{Experiment Setup}
\label{sec:results:setup}
\apnote{we can focus on 2-3 models/model versions depending on their distribution wrt IW, NIW, and OW as well as on A100 and H100.}
\apnote{define simulator setup (Splitwise?) and any add-ons?}
%%==================================================
\subsection{Baselines}
\label{sec:results:base}
% Short term and long term approaches used internally.
\ysnote{Short: gredilly swap hot and cold GPU instances.}
\ysnote{Long: Load balance, max util\% and reduce fragmentation}


%%==================================================
\subsection{Workloads}
\label{sec:results:wl}



\clearpage
%%==================================================
\subsection{Default Setup for Simulation Experiments}
\label{sec:results:default}



%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsubsection{GPUs and LLM Models}
We assume there are three regions, US East, US West and US Central. The \textit{GPU types and counts} available in each region are as follows:\\
- Header: Region, V100, A100, H100, Mi300\\
- US-E: \Note{???, ???, ???, ???}\\
- US-C: ...\\
- US-W: ...\\
\ysnote{we mean only US-E1, US-C1, US-W1}

\ysnote{add simple excel table as figure. Include excel doc in overleaf.}

We consider the following standard LLM model types used by IW and NIW, and for each \textit{model instance} the \textit{number of GPUs required} for each GPU type:\\
Header: LLM Model, V100, A100, H100, Mi300\\
GPT4: \Note{???, ???, ???, ???}\\
GPT4turbo: ...\\
GPT4dv3: ...\\
GPT4ppo: ...\\
GPT35turbo: ...\\
GPT35turbo16k: ...\\
\ysnote{add simple excel table as figure. Include excel doc in overleaf.}
\apnote{To Do: Check if this data needs to be anonymous. }

The \textit{performance (TPS)} for each LLM type \ysnote{(1 instance or 3 instances?)} on each GPU type is as follows:\\
Header: LLM Model, V100, A100, H100, Mi300\\
GPT4: \Note{???, ???, ???, ???}\\
GPT4turbo: ...\\
GPT4dv3: ...\\
GPT4ppo: ...\\
GPT35turbo: ...\\
GPT35turbo16k: ...\\
\ysnote{add simple excel table as figure. Include excel doc in overleaf.}
\apnote{need to anonymize this.}
\ysnote{for the client scenarios, by default lets do a union of all sub-regions in a region. i.e., US-E=US-E1+US-E2+...;}

The \textit{P99 latency} for each LLM type on each GPU type for a request that has \Note{???} input tokens and \Note{???} output tokens \ysnote{pick a median input and output token count across models and regions; get the P99 values from splitwise? Or from logs?} is as follows, within it provisioned TPS:\\
Header: LLM Model, V100, A100, H100, Mi300\\
GPT4: \Note{p99 latency???, ???, ???, ???}\\
GPT4turbo: ...\\
GPT4dv3: ...\\
GPT4ppo: ...\\
GPT35turbo: ...\\
GPT35turbo16k: ...\\
\ysnote{add simple excel table as figure. Include excel doc in overleaf.}

\ysnote{you can bucket token counts as +/1 5\% around the median to get p99.}

Can we include a heatmap of models vs. input+output token count bucket and report the P99 latency on a standard A100 GPU. Repeat this for i/p token count and for o/p token count.

Optionally, get this for each H/W as well.
% make sure to include these values in the common excel sheet}


In addition, we assume the following OW model types along with their GPU count for each type:\\
\ysnote{identify 5-10 opportunistic/eval workloads and the specs for it (GPU counts for each GPU type, TPS for each model on each GPU type). \textbf{TBD}}

\ysnote{add simple excel table as figure. Include excel doc in overleaf.}


%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsubsection{Deployment Characteristics}

% deployment stats & routing stats

% network latency overhead distribution between different clients in a region to data centers in same or different region.
There are network overheads between different regions that affect the latency for client requests from one region being routed to instances in different regions. The latency distributions from source to sink regions are as follows:\\
\ysnote{for each region pair, generate a CDF of network latency}
\apnote{@Shashwat: Use the query shared by Anoop. I have also added the query to the OneNote.}

% Time to do a fresh deployment for each LLM model type on each GPU type.
There is a latency for deploying a new LLM model onto a set of GPU VMs. These times depend on the LLM model size. \ysnote{Do all GPT models take similar time? Or does it depend on the 16k, turbo, etc? Confirm that this is not affected by the GPU type.}
The time taken for a fresh deployment of a LLM model on available GPU instance(s) using local model weights within that VM region is:\\
GPT4: \Note{???}\\
GPT4turbo: ...\\
GPT4dv3: ...\\
GPT4ppo: ...\\
GPT35turbo: ...\\
GPT35turbo16k: ...\\

Similarly, the deployment times taken for the OW LLM models we consider are:\\
\ysnote{provide similar numbers for the 5-10 OW LLM model types. \textbf{TBD}}

% Time to update the weights for an LLM model (same architecture) on each GPU type; weights are local
In case the model architecture is already present in a set of GPU instances and only their weights need to be updated from the local region, then the time taken is as follows.\\
\ysnote{check if this is any diff from above, and if so populate for IW and OW model types. \textbf{TBD}}

% Time to update the weights for an LLM model (same architecture) on each GPU type; weights are remote (time for each region pair)
There is addition time required if the model weights are in a different region and have to be fetched. The sizes of the weights are given below to estimate the fetch time.\\
GPT4: \Note{model weight size???}\\
GPT4turbo: ...\\
GPT4dv3: ...\\
GPT4ppo: ...\\
GPT35turbo: ...\\
GPT35turbo16k: ...\\

\ysnote{also report this for each of the OW models.}

The inter-region bandwidths are given below:\\
Header: LLM Model, US-E, US-C, US-W\\
US-E: \Note{BW???, ??, ??}\\
US-C: \Note{BW???, ??, ??}\\
US-W: \Note{BW???, ??, ??}\\


% Time to scale out and in and existing instance between spot and current
In case existing spot instances are being reclaimed, the time to do so is a median of 1min and a maximum of 5mins. \ysnote{We assume a normal distribution for the same? Should we try and get a distribution from Anoop?}

\Note{we assume that the time for routing the client request to the target region's endpoint is negligible}\ysnote{is this a fair assumption? network latency?}

\ysnote{NOTE: Simulator should capture that the VMs are not available when being reprovisioned}


%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsubsection{Workload Characteristics}

\ysnote{Anjaly, should we disregard minor verions for model types?]} \apnote{yes, let's disregard those.}

We are only going to look at 3 client regions (US-E(all), US-C(all), US-W(all)).  Within each we focus on only 1 week period e.g., 23--29 June, 2024. For these we look at only 6 major model types. For each model type, we only consider top 10 scenarios. This forms the core dataset for our IW workloads. \ysnote{Will revisit for NIW and OW}.

% workload stats: per source region for diff model types we've identified for 1 week. Requests per 5min, token I/O distributions per model type. Only consider top-10 scenarios for each model type. For now, dont bring in scenarios in further workload? granularity.
The timeline distribution of the loads for each of the default 6 LLM model from each region for IW is given for 1 week at 5min granularity, e.g., 23--29 June, 2024. The requests in that 5mins is reported.\\

\ysnote{Timeline plot per client region (US-E(1+2+...), US-C(...), US-W(...), and all LLM models in that plot. Y axis is sum of requests within 5min window. Only consider IW scenarios and top-10 most frequent scenarios for each LLM model type. Update in excel sheet as well}

% what is the rate (interarrival time distribution) at which we bring in NIW and OW? What is the number of requests in each of these tasks? What is the rate at which they are introduced into the mix?
\ysnote{What is the workload request rate for NIW and OW scenarios, per source region? What is the size of the request-count for each? What is the rate at which they are introduced into the mix? Is there a min/max rate? Can this be paused/resumed? \textbf{TBD}}

For each of these LLM models, we provide a distribution of the input and output token counts below:\\
\ysnote{For each LLM model in IW/NIW and OW, give a PDF of the input and the output and the input+output token count: X axis token count Y axis freq. 3 PDFs per LLM. Also include in excel sheet.}


% What is the SLA (P99 latency for IW, batch deadline for NIW) for the different LLM models? What is the size of the batch workload input requests (# of requests at a certain input/output token distribution).
The SLA for these requests are as follows:\\
\ysnote{SLA given for P99 request latency for each model type request. Is this available?}

\ysnote{Distribution of deadlines for the NIW workloads (e.g. 1h median with a distribution?). OW is 24h. \textit{TBD}}

% What is the distribution of model weights in different regions?
The presence of LLM models weights for IW workloads in each region are assumed to as follows:\\
Header: LLM Model, Is in USE?, US-C?, US-W?\\
GPT4: \Note{true/false???, ???, ???, ???}\\
GPT4turbo: ...\\
GPT4dv3: ...\\
GPT4ppo: ...\\
GPT35turbo: ...\\
GPT35turbo16k: ...\\

\ysnote{is thisgoing to be preent only us a subset of regions? Or should we assume these are available in all regions? But if so, the model movement time is not relevant.}

\ysnote{what is the assumption for availability of weights for OW? In only the source region? \textbf{TBD}}


\newpage
%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsubsection{Baseline Behavior, Static}
% default deployment
By default, the GPUs in a region are assigned to models as per the following ratio:\\
% GPT4turbo: 30\\
% GPT4: 18\\
% GPT4dv3: 17\\
\ysnote{get the ratio of number of instances of each LLM model type in each of the 3 regions}

By default, the fraction of GPUs in a region allocated to IW, NIW and OW are as per the following ratio: 75:20:5\\

% what is the baseline behavior for batch workloads introducing their requests into the online workload mix? Is there a default input rate? Is there a min/max rate? Can this be paused/resumed?

% what is the default routing? Same client to same region?


%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\subsubsection{Baseline Behavior, Greedy}

\apnote{comparison of different approaches in terms of utilization, fragmentation, and utility function.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment Design}
\apnote{plots: Number of model instances used (difference in total cost), tail latency, utility of serving requests, overhead of redeployment/unavailability of models (scaling decisions), queue length, Desired TPS of model Vs. actual TPS, memory utilization}
\apnote{Normal scenario}
\apnote{1. Impact of unexpected traffic change or inacuracy in long term traffic predictions: bursty traffic, Tokens per second is greater than historical trend (ARIMA predictions) Vs. less than historical trends (ARIMA predictions).}

\apnote{2. Impact of input and output heavy traffic?}

\apnote{3. Impact of model availability in nearby region Vs. in the local cache}

\apnote{4. Scalability}

\apnote{5. Sanity check for Splitwise}

\apnote{6. Real deployment??}

\sjnote{Once we show evidence using above plots and freeze on the final proposed design: Graceful degradation of latency with A) increasing RPS B) increasing request hardness/input tokens}

\sjnote{Freeze the traces to use:
A) Peak RPS (8, 32, 128), Duration (1, 7) Days, 4 models, 3 regions, prod-dev ratio (2, 4, 8)}

\sjnote{ablation study, from left to right: prod-dev segregated, prod-dev unified with async feed queue, + spot donations, + inter model scaling, + proactive forecasting}


\section{Conclusions}
\label{sec:conclude}


% conference papers do not normally have an appendix
% The Computer Society usually uses the plural form
\section*{Acknowledgments}
\ysnote{Thank all your colleagues who helped with the paper. It is good form.}



\clearpage
% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
% \IEEEtriggeratref{6}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% For non IEEE, you can balance the bibliograhy columns in last page by uncommenting this
% \balance

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{main}





% that's all folks
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
