
\section{Testing Advanced Embedding Models on ATEB}
We test advanced embedding models on ATEB and show their strengths and limitations on our proposed ATEB tasks. 
\subsection{Baseline methods}
Our baseline methods include two advanced embedding models: our Gemma-2B symmmetric dual encoder trained with a prefinetuning stage and Google's gecko embedding model \citep{lee2024geckoversatiletextembeddings}, which has a 1-billion parameter size. Both of these baseline models are highly capable embedding models. Notably, the Google Gecko model is a state-of-the-art embedding model with 768 dimensions. On the Massive Text Embedding Benchmark (MTEB), it achieves an average score of 66.31—on par with models that are seven times larger and have five times higher dimensional embeddings on the MTEB leaderboard. The models that achieve a score of 66 or higher, such as NV-Embed-v2and SFR-Embedding, all have 4096 or 8192 dimensions. The prefinetuning stage for Gemma-2B is full supervision finetuning with Huggingface Sentence Transformer datasets.  \footnote{https://huggingface.co/sentence-transformers}. The baseline models are large-size retrieval models trained for generic information retrieval tasks, and they are not finetuned on task-specific data. We include detailed hyperparameters in the Appendix. 

\subsection{Experimental Results}
\paragraph{Baseline Models have Close-to-Random Performance on New Reranking Tasks}

\begin{table}[t!]
\vspace{-1em}
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{l|c|c|c}
\textbf{Reranking task} & \textbf{Random (\%)} & \textbf{Gemma-2B (\%)}  & \textbf{Gecko (\%)}  \\ 
\toprule
AlpacaFarm & 75 & 75.1 & 75.3 \\ 
\midrule
Genie & 75 & 75.3 & 75.0 \\
\midrule
InstruSum & 75 & 72.8 & 74.1 \\ 
\midrule
Stanford SHP & 75 & 80.47 & 77.1 \\ 
\midrule
BeaverTails Helpful & 75 & 74.51 & 75.9 \\
\midrule
HH RLHF Helpful & 75 & 77.74 & 77.1 \\ 
\midrule
LMSys Chatbot Arena (English) & 75 & 73.18 & 72.9 \\ 
\bottomrule
\end{tabular}
}
\caption{Baseline performance on reranking for evaluating instruction-following.}
\label{table:reranking_comparison}
\vspace{-1em}
\end{table}

Table \ref{table:reranking_comparison} compares the baseline performance of the model against a random chance baseline (75\%) on various reranking tasks designed to evaluate its instruction-following capabilities. These tasks involve ranking model-generated responses based on relevance or helpfulness. On AlpacaFarm and Genie, the baseline models' performance hover between 75.0\% and 75.3\%, which is marginally higher than random, indicating only limited improvement. In contrast, on InstruSum, the baseline models achieve 72.7\% and 74.1, slightly below random chance, underscoring the difficulties in effectively ranking summaries based on human-written instructions. On Stanford SHP, the model performs notably better, achieving 80.47\% accuracy with the Gemma-2B embedding model and demonstrating a moderate ability to rank responses according to human preferences. However, on BeaverTails Helpful, the models' accuracy of 74.51\% and 75.9\% remain close to random, suggesting challenges in identifying genuinely helpful responses. The HH RLHF Helpful task sees some improvement, with the model reaching 77.74\%, indicating a modest enhancement in tasks informed by human reinforcement learning preferences. Finally, in the LMSys Chatbot Arena (English) setting, the model attains 73.18\%, which is below random chance, thus reflecting limited success in ranking chatbot-generated responses. Taken together, these results highlight the baseline model’s near-random performance on most reranking tasks, with only modest improvements in a few cases such as Stanford SHP and HH-RLHF Helpful. 

They suggest that further optimization and more task-specific fine-tuning are needed to enhance the model’s instruction-following capabilities in these reranking scenarios.

\paragraph{Baseline Models Perform Suboptimally on New Retrieval Tasks}

\begin{table}[t!]
% \vspace{-1em}
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{l|c|c|c}
\textbf{Retrieval task} & \textbf{Random (\%)} & \textbf{Gemma-2B (\%)}  & \textbf{Gecko (\%)}  \\ 
\toprule
HellaSwag & 0 & 22.1 & 26.7 \\
\midrule
Winogrande & 0 & 17.3 & 21.2 \\ 
\midrule
PIQA & 0 & 22.2 & 29.8 \\ 
\midrule
AlphaNLI & 0 & 30.3 & 32.1 \\ 
\midrule
ARCChallenge & 0 & 7.62 & 10.9 \\ 
\bottomrule
\end{tabular}
}
\caption{Results of retrieval tasks for evaluating reasoning.}
\label{table:retrieval_comparison}
\vspace{-1em}
\end{table}

Table \ref{table:retrieval_comparison} presents the performance of baseline models compared to random chance in reasoning-based retrieval tasks. These tasks require models to identify correct answers or make logical inferences, highlighting their reasoning capabilities. Key observations include:

On HellaSwag, the baseline embedding models achieve 22.1\% and 26.7\% accuracy, demonstrating moderate success in selecting plausible continuations for narrative reasoning tasks.
With 17.3\% and 21.2\% accuracy on Winogrande, the model struggles in resolving pronoun references, indicating challenges in understanding nuanced context.
Achieving 22.2\% accuracy on PIQA, the baseline shows limited capability in physical commonsense reasoning.
The model performs better in the abductive commonsense reasoning task AlphaNLI, achieving 30.3\% and 32.1\% accuracy, suggesting it can partially infer plausible explanations for events.
On ARCChallenge, with only 7.62\% and 10.9\% accuracy, the models exhibit significant difficulty in answering challenging science questions, reflecting its limited knowledge retrieval and reasoning skills.
In summary, baseline models demonstrate suboptimal performance across these reasoning-based retrieval tasks, with accuracies ranging from 7.62\% to 32.1\%. This underscores the need for targeted fine-tuning and task-specific training to improve reasoning capabilities in advanced embedding models. 


\paragraph{Baseline Models have Close-to-Random Performance on New Classification Tasks}
\begin{table}[t!]
\vspace{-1em}
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{l|c|c|c}
\textbf{Classification task} & \textbf{Random (\%)} & \textbf{Gemma-2B (\%)}  & \textbf{Gecko (\%)}  \\ 
\toprule
ESNLI & 33.3 & 35 & 36.1\\ 
\midrule
DialFact & 33.3 & 33.8 & 33.2 \\ 
\midrule
VitaminC & 33.3 & 37 & 35.4\\ 
\midrule
HH-RLHF Harmlessness & 50 & 50& 50 \\
\midrule
BeaverTails Classify & 50 & 55.9 & 54.7 \\ 
\bottomrule
\end{tabular}
}
\caption{Results of classification tasks for evaluating factuality and safety. }
\label{table:classification_comparison_updated}
\vspace{-1em}
\end{table}

Table~\ref{table:classification_comparison_updated} illustrates the performance of two baseline models, Gemma-2B and Gecko, on five classification tasks—ESNLI, DialFact, VitaminC, HH-RLHF Harmlessness, and BeaverTails Classify—compared to random chance accuracy. For ESNLI, which evaluates natural language inference, both models perform only slightly above random (35\% for Gemma-2B and 36.1\% for Gecko) despite random performance being 33.3\%, indicating limited reasoning capability. Similarly, on DialFact, which assesses factual consistency in dialogue, the models perform very close to random, with Gemma-2B achieving 33.8\% and Gecko 33.2\%. In the VitaminC task, focused on fact verification, both models show modest improvement over random (33.3\%), with Gemma-2B reaching 37\% and Gecko slightly lower at 35.4\%. For the HH-RLHF Harmlessness task, which classifies whether responses are harmless, both models achieve exactly 50\%, matching random performance and indicating no learned capability. Finally, on BeaverTails Classify, a binary classification task where random accuracy is 50\%, the models perform slightly better, with Gemma-2B at 55.9\% and Gecko at 54.7\%, reflecting some potential but still falling short of reliable generalization. These results collectively highlight the close-to-random performance of baseline models on novel classification tasks, underscoring the need for more advanced methods to achieve meaningful improvements in generalization and reasoning.

\paragraph{Baseline Models Perform Reasonably Well on New Pairwise Classification Tasks}
\begin{table}[t!]
\vspace{-1em}
\centering
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{l|c|c|c}
\textbf{Pairwise classification} & \textbf{Random (\%)} & \textbf{Gemma-2B (\%)}  & \textbf{Gecko (\%)}  \\ 
\toprule
Dipper  & 50 & 73.1 \% & 80.1 \\ 
\midrule
\textbf{Bi-text mining} & & \\ 
\midrule
% LITMT & 0 & 5\% \\ \hline
Europarl & 1/n & 86.1\% & 88.2\% \\ 
IWSLT17 & 1/n & 86.4\% & 87.1 \\ 
NC2016 & 1/n & 98\% & 99 \% \\ 
\bottomrule
\end{tabular}
}
\caption{Baseline Accuracy for pairwise classification and bi-text mining tasks}
\label{table:pairwise_classification_random_baseline}
\vspace{-1em}
\end{table}

Table \ref{table:pairwise_classification_random_baseline} compares the baseline accuracy of a model against random predictions across pairwise classification tasks. The results highlight the baseline model's effectiveness in these specific contexts:

On Dipper, the baseline model achieves an accuracy of 73.06\%, significantly outperforming the random baseline of 50\%, showcasing strong performance in pairwise classification tasks.

\paragraph{Baseline Models Perform Very Well on New Bitext-Mininig Tasks}

Bi-text mining Tasks involve identifying semantically equivalent text pairs across multilingual datasets. On each of the three datasets consisting of a few hundred of document-translation pairs, both Gemma-2B model and Gecko model perform very well, excelling particularly in NC2016 with a high accuracy of 98\%, indicating exceptional capability in identifying translations of text correspondences. 

The baseline model performs strongly in bi-text mining tasks, significantly surpassing random baselines, which are based on the inverse of the dataset size (1/n).
For pairwise classification tasks like Dipper, the baseline accuracy of 73.06\% highlights the model's potential for applications requiring pairwise comparisons.
These results emphasize the effectiveness of the baseline model in identifying document-level semantic relationships and alignments, especially in multilingual or structured datasets. 
