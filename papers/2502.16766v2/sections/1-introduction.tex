
% \section{Introduction}
% \begin{itemize}
%     \item Target aware 
%     \item reasoning - complex
% \end{itemize}


Traditional retrieval models are primarily trained and evaluated on traditional Information Retrieval tasks including document retrieval, reranking and sentence similarity \citep{muennighoff-etal-2023-mteb}. However, this approach falls short when applied to more advanced natural language capabilities that require a deeper understanding of text, such as reasoning, factuality, instruction-following and long-form text understanding \citep{su2024brightrealisticchallengingbenchmark, xiao2024rarbreasoningretrievalbenchmark, weller2024followirevaluatingteachinginformation}. These tasks demand an ability to comprehend and process complex information, often involving multiple steps of reasoning, the handling of sensitive content, or the verification of factual statements against reliable sources \citep{vu2024foundationalautoraterstaminglarge, ji2023beavertails, su2024brightrealisticchallengingbenchmark}. 

Recent benchmarks have been proposed to evaluate reasoning-intensive retrieval \citep{su2024brightrealisticchallengingbenchmark, xiao2024rarbreasoningretrievalbenchmark} or instruction following \citep{weller2024followirevaluatingteachinginformation}. However, these benchmarks only consider a single advanced capability. 
In particular, \citet{weller2024followirevaluatingteachinginformation} introduced a new benchmark built on top of traditional retrieval benchmarks to measure instruction-following capability of embedding models, however, they do not capture how well embedding models perform on the most widely adopted instruction following benchmarks such as Stanford Human Preference (SHP) \citep{pmlr-v162-ethayarajh22a} for ranking human preference over model outputs and HH-RLHF \citep{bai-etal-2022-hhrlhf} for ranking helpfulness and safety of model responses. 

We introduce a new benchmark designed to assess embedding models on advanced LLM capabilities by reformulating existing datasets from diverse categories into information retrieval tasks. These include \textbf{safety classification}: \textit{BeaverTails Safety Classification} \citep{ji2023beavertails}, \textit{HH-RLHF Harmlessness Classification} \citep{bai-etal-2022-hhrlhf}; \textbf{factuality classification}:  \textit{ESNLI} \citep{camburu-etal-2018-esnli}, \textit{DialFact} \citep{gupta-etal-2022-dialfact}, \textit{VitaminC} \citep{schuster-etal-2021-vitaminc}, \textbf{instruction following reranking}: \textit{Stanford Human Preference} \citep{pmlr-v162-shp}, \textit{AlpacaFarm} \citep{dubois-etal-2023-alpacafarm}, \textit{LMSys} \citep{chiang-etal-2024-chatbot}, \textit{Genie} \citep{khashabi-etal-2022-genie}, \textit{InstrSum} \citep{ji2023beavertails}, \textit{HH-RLHF Helpful} \citep{bai-etal-2022-hhrlhf};  \textbf{document-level pairwise-classification}: \textit{DIPPER} \citep{dipper}, and \textbf{document-level bitext-mining}: \textit{Europarl} \citep{koehn-2005-europarl}, \textit{IWSLT17} \citep{cettolo-etal-iwslt17-overview}, \textit{NC2016} \citep{maruf-etal-2019-selective}). We also incorporate subsets of \textbf{reasoning retrieval} \citep{xiao2024rarbreasoningretrievalbenchmark},  We evaluate our benchmark with advanced embedding models: a Gemma-2B \citep{gemmateam2024gemmaopenmodelsbased} embedding model trained as a symmetric dual encoder \citep{neelakantan2022}
and Google Gecko embedding model \citep{lee2024geckoversatiletextembeddings}.

% We also adopt the recently proposed advanced IR tasks including FOLLOW-IR \citep{weller2024followirevaluatingteachinginformation}, BRIGHT \citep{su2024brightrealisticchallengingbenchmark} and RAR-B\cite{xiao2024rarbreasoningretrievalbenchmark}.

% (We are evaluating our benchmark with , OpenAI embedding models as well as the voyage-AI embedding model. )

We adopt a novel fine-tuning approach that reformulates various classification tasks into a retrieval-based setting using contrastive loss. In this setup, each task instance is represented as a triplet: an input (query and answer concatenated), a positive target (label text plus explanation and a unique ID), and multiple negative targets (alternative labels with explanations and the same unique ID) \citep{lee2024geckoversatiletextembeddings}. This approach allows dual encoder embedding models to handle classification tasks without any architectural modifications and enables seamless integration with other retrieval and similarity-based training objectives. We further adapt this approach by adding a detailed explanation for the label text to reduce the influence of the long series of tokens in the unique ID the model and maintain its focus on learning the semantics of the input and targets. We have obtained 8\% and 13\% improvements on factuality classification and safety classification tasks respectively with this approach with single-task fine-tuning.  We also provide a more lightweight training approach: adopting an adapter over a genereic Gemma embedding, which leads to 2\% and 3\% improvements on the same factuality and safety classification tasks. 

In summary, the contributions of our paper are threefold. 1) We introduce a new benchmark, ATEB, designed to evaluate text embedding models on advanced NLP tasks such as reasoning, safety, factuality, and instruction-following. Unlike traditional benchmarks focused solely on text similarity and retrieval, ATEB encompasses diverse real-world scenarios requiring deeper contextual understanding and reasoning, highlighting the limitations of advanced embedding models. 2). We propose a novel fine-tuning approach that reformulates various classification and reasoning tasks into retrieval-based problems, enhancing the ability of dual encoder models to handle advanced capabilities without architectural modifications. Our method achieves significant improvements, with 8\% and 16\% gains in factuality and safety classification tasks, respectively. 3). Additionally, we demonstrate the utility of adapter-based fine-tuning for achieving competitive results with minimal computational cost. 