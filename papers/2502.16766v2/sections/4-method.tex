
\section{Method}

\subsection{Model}
We begin by initializing a symmetric dual encoder (DE) using the decoder-only Gemma-2B model \citep{gemmateam2024gemmaopenmodelsbased, palma-gomez-etal-2024-transforming}, which has an embedding size of 2048. Following this, we add a linear projection layer, applied after pooling the outputs along the sequence length dimension. Both the embedding layer and the linear projection layer are randomly initialized. After the model is initialized with Gemma-2B, we train it using a contrastive loss \citep{Hadsell2006DimensionalityRB}. 

\subsection{Training data reformulation with label augmentation}
% \simeng{Add formulas}
% \simeng{Name: label augmentation for classification into retrieval}
While using the dot-product scores along the diagonal as positives and everything else as negatives works well for retrieval and similarity/relatedness matching tasks, it can not be used directly for tasks with targets that are classification labels. Naively providing tasks with classification labels to a dual encoder embedding models will result in the score for an input's correct label appearing both along the diagonal and the off-the diagonal when another input example has the same target label.

Therefore, we adopt a novel method that reformulates various tasks as retrieval tasks during the fine-tuning process, following previous work in fine-tuning with a retrieval setting using contrastive loss \citep{lee2024geckoversatiletextembeddings, meng2024sfrembedding}. \textbf{Input}: query and answer concatenated together. \textbf{Positive target}: [\textit{label text} (e.g., neutral for NLI).] + [\textit{label text explanation}] + [\textit{unique id}]. \textbf{Negative targets}: [each other possible types of \textit{label texts}.] + [\textit{label text explanation}] + [\textit{unique id}]. 

  Including a unique id for for each correct input/target pair alone would allow the model to exploit and rely on the unique identifiers to always pair the correct input with the correct target. However, this can be addressed by including additional incorrect labels for each input as negatives. The negatives are tagged with the same unique id as the input and the correct target label. This allows the unique ids to be used to identify candidate targets for each input, but without revealing which of the targets is correct. The advantage of this approach is that it allows dual encoder based embedding models to be trained on  classification tasks without any modeling changes. In practice, a unique ID often consists of a long sequence of tokens that can inadvertently shift the model’s focus away from learning the semantic relationships within the input and target texts of an example. To address this challenge, we enhance this approach by providing detailed explanations for each label. This additional context helps the model grasp the conceptual meaning behind labels rather than becoming distracted by the long series of unique ID tokens. 

For example, for the label “Entailment,” we augment it by including the following label explanation:

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Label explanation for "entailment"]
\textit{“In the context of Natural Language Inference (NLI), ‘entailment’ refers to a specific type of relationship between two sentences, where the truth of one sentence (the hypothesis) is logically guaranteed by the truth of another sentence (the premise).”}
\end{tcolorbox}
By augmenting labels with such detailed explanations, we guide the model toward a richer, more coherent understanding of the underlying concepts it needs to learn.