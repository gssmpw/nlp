
\subsection{Training details}
\paragraph{Training dataset size}
We use the reformulated training sets of the publicly available datasets in training our factuality and safety models. We adopt the train split in the original tasks. 

\paragraph{Hyper-parameters}

We did not use hard negatives in prefinetuning. We used a batch size of 1024, learning rate of $1e-4$. The number of training steps is $100,000$. The number of warmup steps is st to $20,000$ and the input length is 256, the output length is 1024. We used unmixed batches during training and bidirectional loss.

We finetune both the factuality models and safety models with 20k iterations and a batch size of 1024. Our learning rate is set as $1e-4$ with linear decay. 