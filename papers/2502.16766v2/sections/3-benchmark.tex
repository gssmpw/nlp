\section{ATEB Construction}
\subsection{Design Principles}
The benchmark comprises 21 tasks, encompassing datasets related to instruction-following, factuality, reasoning, document-level translation, and paraphrasing. These tasks simulate real-world scenarios requiring advanced model capabilities. We reformulate these tasks from existing sources based on the following principles. 
\begin{itemize}
    \item \textbf{Factuality as classification}: NLI tasks where the goal is to classify the relationships of the premise and hypothesis into \textit{entailment}, \textit{contradiction}, or \textit{neutral}. 
    \item \textbf{Instruction following as reranking}:  Ranking model-generated responses based on human preference (e.g., Stanford SHP).
    \item \textbf{Safety as classification}: Binary classification tasks or ranking tasks (safe vs. unsafe).
    \item \textbf{Reasoning as retrieval}: Retrieving the gold answers from the gold answer pool of all the examples in the dataset based on the question. 
    \item \textbf{Document-level paraphrasing as pairwise-classification}: Pairing the paraphrase of a document with the document based on paraphrases of all documents in the dataset. 
    \item \textbf{Document-level machine translation (MT) as bitext-mining}: Finding the translation of a document over translation of all documents in the dataset. 
\end{itemize}


We provide detailed illustrations of how each task category is constructed, accompanied by examples. For each task, we utilize the complete test set from the corresponding public datasets.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Factuality %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Factuality as Classification}
We adopt several Natural Language Inference (NLI) classification datasets in our factuality classification collection. This includes ESNLI \citep{camburu-etal-2018-esnli}, VitaminC \citep{schuster-etal-2021-vitaminc} and DialFact \citep{gupta-etal-2022-dialfact}.  An example of the ESNLI dataset is shown in Table~\ref{tab:esnli-reranking-example} where the input consists of a concatenation of one premise and one hypothesis and the target is one of the strings of the three classes including "entailment", "contradictory" and "neutral". 

\begin{table*}[h]
\centering
% \setlength{\tabcolsep}{3pt}
\small
\begin{tabular}{p{15.5cm}}
\toprule
\textbf{Input}: \textit{Premise}: Everyone really likes the newest benefits. \textit{Hypothesis}: The new rights are nice enough. \\ 
\midrule
\textbf{Target}: entailment, contradictory, or neutral. \\ 
\bottomrule
\end{tabular}
\caption{An example of ESNLI.}
\label{tab:esnli-reranking-example}
\end{table*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Instruction-Following %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Instruction-Following as Reranking}


\begin{table*}[htbp]
\centering
% \setlength{\tabcolsep}{3pt}
\small

\begin{tabular}{p{15.5cm}}
\toprule
\textbf{Original SHP} \\ 
\midrule
\textbf{responseA: } "It doesn't sound like they deserve the courtesy of two weeks notice.   Check company policy and state law about whether they have to pay your sick time or other PTO... \\
\midrule
\textbf{responseB: } "...I'd say you are within your rights to kick over the can of kerosene and toss the Zippo..." \\
\midrule
\textbf{preference label:} "responseA" \\ 
\midrule
\textbf{task instruction: } "In this task, you will be provided with a context passage (often containing a question), along with two long-form responses to it (responseA and responseB). The goal is to determine which of the two is a better response for the context..." \\ 
% \midrule
\textbf{input: } "How unprofessional would it be to quit the moment I have a job lined up following my vacation? I hate my coworkers..." \\ 
\bottomrule
\end{tabular}
\caption{Original Stanford Human Preference (SHP) dataset example.}
\label{tab:original-shp-example}
\end{table*}


\begin{table*}[t!]
\centering
% \setlength{\tabcolsep}{3pt}
\small
\begin{tabular}{p{15.5cm}}
\toprule
\textbf{Query:} "In this task, you will be provided with a context passage (often containing a question), along with two long-form responses to it (responseA and responseB). The goal is to determine which of the two is a better response for the context...How unprofessional would it be to quit the moment I have a job lined up following my vacation? I hate my coworkers... \\ 
\midrule
\textbf{Positive}: "It doesn't sound like they deserve the courtesy of two weeks notice.   Check company policy and state law about whether they have to pay your sick time or other PTO... \\
\midrule
\textbf{Negative}: "...I'd say you are within your rights to kick over the can of kerosene and toss the Zippo..." \\ 
\bottomrule
\end{tabular}
\caption{Reformulated example of our SHP-Reranking for evaluating embedding models' reranking capability for model responses given instructions.}
\label{tab:shp-reranking-example}
\end{table*}

We reformulate publicly available instruction-following tasks into reranking tasks where the rank is determined by the human preference. Between two model outputs, the model output preferred by human is ranked higher than the model output less preferred. The query is formulated as the concatenation of the task instruction and input context.  We provide an example of one of the source datasets we adopted, Stanford Human Preference \citep{pmlr-v162-shp}, in Table~\ref{tab:original-shp-example} and the reformulated example based on it in Table~\ref{tab:shp-reranking-example}. We reformulate six more instruction-following tasks into reranking tasks, which include AlpacaFarm \citep{dubois-etal-2023-alpacafarm}, HHRLHF-Helpful \citep{bai-etal-2022-hhrlhf}, BeaverTails-Helpful \citep{ji2023beavertails}, Genie \citep{khashabi-etal-2022-genie}, LMSys ChatBot Arena \citep{chiang-etal-2024-chatbot}, InstruSum \citep{liu-etal-2024-instrusum}. 
% They are named as AlpacaFarm-Reranking, HHRLHF-Helpful-Reranking, BeaverTails-Helpful-Reranking, Genie-Reranking, LMSys-Reranking and InstruSum-Reranking respectively. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% SAFETY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Safety as Classification}
We adopt the safety classification portion of the BeaverTails dataset for LLM safety alignment \citep{ji2023beavertails}, BeaverTails QA-Classification to construct a safety classification task for evaluating embedding models where the goal of the task is to classify the input into \textit{safe} or \textit{unsafe}. An example of the BeaverTails QA-Classification dataset is shown in Table~\ref{tab:beavertails-qa-classification-example}. We also adopt the portion for evaluation harmlessness in the HH-RLHF Harmlessness dataset to construct a safety safety classification task.

%where the less harmful response should be preferred and ranked higher than the less harmful response. 

\begin{table*}[t!]
\centering
% \setlength{\tabcolsep}{3pt}
\small
\begin{tabular}{p{15.5cm}}
\toprule
\textbf{Query:} \textbf{Input}: Is there sites that sell weed? Yes, there are websites that sell marijuana. These sites usually require customers to have a valid prescription and may require a background check to ensure that the individual is of legal age to purchase the drug. However, there are also websites that do not require any forms of identification... \\ 
\midrule
\textbf{Two classes}: safe/unsafe \\ 
\bottomrule
\end{tabular}
\caption{An example of the BeaverTails QA-Classification dataset for evaluating embedding models.}
\label{tab:beavertails-qa-classification-example}
\end{table*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% REASONING %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Reasoning as Retrieval}
We adopt 5 subsets of the RAR-b dataset proposed in \cite{xiao2024rarbreasoningretrievalbenchmark} including HellaSwag NLI dataset \citep{zellers-etal-2019-hellaswag}, Winogrande \citep{winogrande}, PIQA \citep{piqa}, AlphaNLI \citep{alpha-nli} and ARCChallenge \citep{arc}. Table~\ref{tab:reasoning-as-retrieval-example} shows the data format of the reformulated datasets.


\begin{table*}[t!]
\centering
% \setlength{\tabcolsep}{3pt}
\small
\begin{tabular}{p{15.5cm}}
\toprule
\textbf{Input}: a query in the dataset. 
\textbf{Target}: the answer to the query. 
\textbf{Negative targets}: all the other answers in the dataset.  \\ 
\bottomrule
\end{tabular}
\caption{Data format of the reasoning as retrieval datasets for evaluating embedding models.}
\label{tab:reasoning-as-retrieval-example}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% DOCUMENT-LEVEL Paraphrasing %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Document-Level Paraphrasing as Pairwise-Classification}
We reformulate one document-level paraphrasing dataset, DIPPER \citep{dipper} as a pairwise classfication task. These tasks expand over previous sentence-level paraphrasing tasks used for pairwise classification \citep{muennighoff2023mteb} to test the document-level modeling capabilities of most advanced embedding models.   

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BI-TEXTT MINING %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Document-Level MT as Pairwise-Classification}
Following the same design principle of our new pairwise-classification tasks, we reformulate three document-level machine translation datasets as bi-text mining tasks, which include Europarl \citep{koehn-2005-europarl}, IWSLT17 \citep{cettolo-etal-iwslt17-overview} and NC2016 \citep{maruf-etal-2019-selective}. These tasks expand over previous sentence-level machine translation tasks used for bi-text mining \citep{muennighoff2023mteb} to test the document-level modeling capabilities of most advanced embedding models. We adopt the subset of these datasets used in \citet{maruf-etal-2019-selective}. 

