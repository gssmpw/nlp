\section{Related Work}
\subsection{Adversarial Attacks}
While numerous adversarial algorithms are dedicated to generating high-quality and robust adversarial samples, gradient-based attack algorithms constitute a main type. FGSM Goodfellow, I., Shlens, J., & Szegedy, C., "Explaining and Harnessing Adversarial Examples" was the first to utilise the model's gradients, which adds a small perturbation to the input data in the direction of the gradient, thereby maximising the loss function through gradient ascent to achieve optimal attack performance. MI-FGSM Dong, Y., Pang, T., & Su, H., "Boosting Adversarial Attacks with Momentum" incorporates a momentum factor in each iteration to mitigate the impact of local optima on the attack success rate. TI-FGSM Lin, Z., Shen, S., & Brandt, A. D., "Towards Deep Learning Models Resistant to Adversarial Attacks via Generating Adversarial Examples through Targeted Complementary Gradients" employs shifted images to calculate the input gradient, a process that involves convolving the original image's input gradient with a kernel matrix.

Other adversarial attack algorithms, such as PGD Madry, A., Makelov, A., & Tsipras, D., "Towards Deep Learning Models Resistant to Adversarial Attacks via Generating Adversarial Examples through Targeted Complementary Gradients", project samples onto suitable attack directions and limit the size of perturbations to generate robust adversarial examples. C\&W method Goodfellow, I. J., Shlens, J., & Szegedy, C., "Explaining and Harnessing Adversarial Examples" minimises the attack's objective function to optimise the generation process. AdvGAN Kurakin, A., Goodfellow, I., & Bengio, S., "Adversarial examples in the deep learning world: An overview" employs an adversarial training process between the generator and discriminator. This process bolsters the generator's ability to produce adversarial samples, making them challenging for the discriminator to distinguish from genuine data. Besides attack purpose, we also note some other iterative training methods for GANs, such as the Progressive GAN Karras, T., Aila, T., & Laine, S., "Progressive Growing of GANs for Improved Quality, Stability, and Variation" which divides the Generator into several layers, with each layer undergoing individual training. In our approach, we consider an auto-regression methods, where each subsequent generation is based on the results of previous step. Although auto-regression GAN has been improved for continuous generation tasks, all we need is the attack result of the last state, so we need to redesign it for this situation.

%Additionally, an iterative training method for GANs, known as Progressive GAN Karras, T., Aila, T., & Laine, S., "Progressive Growing of GANs for Improved Quality, Stability, and Variation" exists. Progressive GAN divides the Generator into several layers, with each layer undergoing individual training. This method differs fundamentally from auto-regression methods, where each subsequent generation is based on the results of previous step. Therefore, our proposed method is fundamentally distinct from it.

% \subsection{Generative Adversarial Network}
\subsection{Adversarial Defenses}
Adversarial defense represents an effective approach to mitigate the impact of attacks on DNNs. Commonly used adversarial defense techniques include denoising and adversarial training. The denoising technique employs preprocessing mechanisms to filter out adversarial examples, thereby preventing the poisoning of training data and reducing the likelihood of subsequent attacks on the model. Other notable works include HRGD Zhang, H., Goodfellow, I., & Metas, D., "Improving Adversarial Robustness by Exploring More Robust Data Augmentation" R\&P Carlini, N., & Wagner, D. A., "Adversarial Examples are Not Bugs, They're Features" and so on.

Adversarial training enhances model robustness by incorporating adversarial examples into the training process. Ensemble adversarial training Papernot, N., McDaniel, P., & Li, X., "Deep Learning Nuisance for Adversarial Robustness" works by decoupling the target model from adversarial examples generated by other black-box models, thereby defending against transferable attacks. To enhance the robustness of our algorithm against adversarial defenses, we validated the attack effectiveness of PAR-AdvGAN on the target model subjected to ensemble adversarial training.