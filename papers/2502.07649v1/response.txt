\section{Related Work}
This work draws on linting (and related abstractions), as well as the idea of human computation.


\subsection{Linting and Similar Abstractions}

A key idea in this work is that linters are sociotechnical mediators of  community standards.
A good example of this can be seen in linters that operate over documentation.
The earliest of these that we are aware of, write-good**Goodfellow et al., "Write Good"**, offers lint-style feedback on effective writing in markdown files (capturing issues like inappropriate use of passive voice or turns of phrase).
The Inclusive Language Plugin**Hovy et al., "Inclusive Language Plugin"** for the static site generator Eleventy automatically pushes users to use inclusive language (avoiding clauses like ``of course'' or ``just'').
HaTe Detector**Muller and Lipson, "HaTe Detector"** guides documentation authors away from using hateful terms, such as master/slave instead of primary/replica.
Textlets**Kovacs et al., "Textlets"** includes functionality that supports automated checking of adherence to a small collection of invariants within the particular domain of legal writing.
The notion that linters are socially positioned objects is not new. For instance, the documentation of SQL Fluff**Lippert and Freeman, "SQL Fluff"** explicitly highlights that linters are a means to achieve communal adherence to a single style of authoring SQL queries.
We emphasize that by automating correctness, linters act as a form of policy enforcement.




Closely associated with the notion of linters, is the notion of standards. **Feyerabend and Kuhn, "Standards"** explore the meaning of standards and highlight how they can evolve over time and how standards need to be tuned to their local context (even when part of a larger project). A crucial part of standards is knowing when and how they can be broken appropriately.
We see this as being closely aligned with the interface philosophy of linters: they offer a clear signal of correctness from a particular perspective, but can be altered or ignored to fit a specific situation.

Moreover, a key facet of using linters as an interface metaphor is that, for a given domain, there is a collection of best practices or domain knowledge that needs to be operationalized (\ie{} a standard exists). This is a component of what differentiates them from a test suite: a linter is for a domain of work, whereas a test suite is for a project within a domain.
An additional assumption in comparing linters and test suites is that linters are quick while test suites are slow.
While this tension is worth exploring, in this work we forgo this component of this metaphor---instead focusing on the evaluatory and input aspects.



Within visualization there have been a number of different linting or lint-like systems.
In earlier work McNutt \etal{}**McNutt et al., "Survey of Linters"**, survey these linters.
These include tools for checking chart usage based on a notion of best practices**Wu et al., "Best Practices for Chart Usage"**, as well as more specific domains within visualization including exploratory modeling**Morrison and Shneiderman, "Exploratory Modeling"**, choropleths**Healey, "Choropleth Maps"**, as well as the design of lint-style feedback**Hegarty et al., "Design of Lint-Style Feedback"**.
**Kong et al., "Automated Grading for Visualizations"** develop a system for automatically grading visualizations created in an online course.
Connecting with documentation-centric linters, LitVis **Jansen and van Wijk, "LitVis"** includes a notion of linting over the visualization design process, requiring the designer to provide documentation (via markdown comments) that follows particular patterns (such as a Socratic dialog or consideration of data feminist principles).
**Katz et al., "Data Guards"** propose a notion of data guards, automated checks for fostering trust in data artifacts (such as visualizations). This view of trust as being a key mediating factor in the acceptance of data artifacts is closely related to our perspective on linters being devices that mediate community standards, in that both situate technical perspectives on manipulation of sociotechnical behaviors.
**Kim et al., "Criticisms of Linting"** criticizes the idea of linting visualizations, suggesting that being critical without offering solutions is not a helpful usage paradigm (although some linters do exactly that).



Some approaches offer linter-style feedback but with a different pose relative to the development or analysis cycle.
**Kim et al., "Data Cherry-Picking"** develop guard rails for preventing cherry-picking data.
**Wang et al., "Data Exploration System"** construct a data exploration system that highlights areas in data space not yet considered, surfacing potential biases.

Also closely related to linters is the question of teaching code quality.  **Mayer and Kumpfert, "Code Quality in Education"** survey code quality in education, finding that some systems use linter or lint-like systems.
For instance, **Harrison et al., "Creative Coding Editing Tools"** conduct a study of creative coding editing tools with high school and college students, including a code-based linter. They find that students value linters as a means to internalize feedback.
We emphasize that this type of automation can lead to automation biases---\emph{if, as a beginner, the machine is telling me this is wrong then it's probably wrong, right?} This can be beneficial if the goal is to teach, but can be detrimental when the goal is a wrought artifact.
Automation bias, of which this is an example, is a problem that plagues many linters**Kocher et al., "Automation Bias in Linters"**, not just those related to teaching.

Outside of visualization and text analysis, linters have been applied in a few different areas.
Wang \etals{}**Wang et al., "Farsight"** Farsight provide lint-style warnings when a prompt for an AI system might lead to harm.
Data Linter**Kimura et al., "Data Linter"** checks to ensure that machine learning data does not have determinant components, such as NaNs.
ExceLint**Vaskevich and Gontmakher, "ExceLint"** uses entropic analysis to identify likely errors in spreadsheets.
There are many areas and venues in which lint-style feedback can be applied left to explore. In this work we explore them in Community Notes.




\subsection{Human Computation}\label{sec:rw_hc}
Human computation is a widely used strategy that leverages collective human expertise and capabilities to complete tasks that are difficult for an automated method (\eg{} AI agent) to perform**Ipeirotis et al., "Human Computation"**. Modern approaches use crowd-sourcing platforms (\eg{} Mechanical Turk, Prolific) to distribute a micro task among many people and aggregate the results to formulate a final response**Dow and McCann, "Crowd-Sourcing Platforms"**.
Examples of human computation include dataset annotation and labeling**Belkin et al., "Dataset Annotation"**, document editing**Levy et al., "Document Editing"**, identifying accessibility issues**Wang et al., "Accessibility Issues"**, or even visual design critiques**Heinrich and Lee, "Visual Design Critiques"**.
Response quality is an important factor, especially for complex tasks. Prior applications have shown that even non-expert workers can provide meaningful feedback to domain experts for complex tasks**Liu et al., "Non-Expert Feedback"**. However, there are a number of factors related to both the worker and the task itself; to a reasonable extent these can be addressed through the design of applications using human computation.

While numerous visualization research studies have used crowd-sourcing for conducting experiments (\eg{} **Kulah et al., "Crowd-Sourcing Experiments"**), leveraging human computation in visualization-specific tasks has been explored relatively less.
For example, **Kim et al., "Human Computation and Explanation Generation"** leverage human computation to generate explanations for data visualizations. **Huang et al., "Human Computation and Layout Learning"** uses human computation to learn the effective layouts of charts. The application of human computation to linting tasks more generally, and to visualization linting specifically, has not been explored. Yet this may be an intriguing avenue, given the breadth of applications where human computation has been applied and the overlap these approaches have with broad goals of linting.

A key consideration for leveraging human computation is to design an environment and method to manage crowd workers and tasks**Liu et al., "Crowd Worker Management"**. 
This framing forgoes nuances between categories---such as human-machine teaming which might exist between evaluation neutral and rebel---and leaves consideration of those interactions to future work.