\section{Related Work}
\texttt{CoT} prompting has emerged as a transformative technique in the domain of large language models (LLMs). Wei et al.~\cite{cot} demonstrated that eliciting intermediate reasoning steps significantly improves performance on complex tasks such as arithmetic and commonsense reasoning. However, subsequent work has raised important questions about whether these generated reasoning traces reflect a genuine internal problem-solving process or are simply a by-product of pattern recognition over extensive pre-training data~\cite{turpin2023,epflfaithfulness}.

A related line of inquiry investigates the internal mechanisms underlying reasoning in both text-only and multimodal settings. Early research in vision-language understanding established that aligning visual and textual representations is key to coherent reasoning. For instance, models such as LXMERT~\cite{tan-bansal-2019-lxmert}, VilBERT~\cite{lu2019vilbert}, and UNITER~\cite{chen2020uniter} leveraged cross-modal attention mechanisms to learn shared representations, thereby enhancing performance on tasks that require integrating visual cues with textual context. Similarly, contrastive learning approaches, as exemplified by CLIP~\cite{radford2021learning}, have effectively mapped images and text into a common embedding space, underscoring the importance of alignment in developing a unified mental model.

Mental modeling in LLMs—forming internal task or world representations—has been explored in both text-only and multimodal contexts. In text-only settings, models must infer structure solely from textual descriptions, often lacking coherent representations of objects and their relations~\cite{gu2023}. In contrast, multimodal models can leverage visual grounding to enhance internal representations~\cite{gao2024}. Other strategies facilitate reasoning through iterative dialogue between image and language models~\cite{zeng2022socratic} or by translating visual information into natural language~\cite{lu22, mmcot}. Recent advances in multimodal reasoning extend beyond simple image-to-text transcriptions, focusing instead on reasoning across multiple modalities simultaneously~\cite{mvot}. 
These findings align with classic cognitive insights~\cite{larkin1987diagram} suggesting that visual representations reduce cognitive load by making implicit relationships explicit. Collectively, this body of research underscores that multimodal models construct richer and more grounded mental models than their text-only counterparts.

The evolution toward large-scale multimodal models—such as Flamingo~\cite{alayrac2022flamingo} and BLIP-2~\cite{li2023blip}—has further underscored that even when visual inputs are available, the core reasoning engine remains largely text-based. Proprietary systems like GPT-4V~\cite{openai2024gpt4} and PaLM-E~\cite{driess2023palm} reinforce this observation: despite their ability to process visual information, these models often exhibit reasoning processes that are strongly influenced by their language model components. This body of work suggests that whether operating on text alone or in combination with visual modalities, LLMs tend to develop a unified internal representation that drives their reasoning capabilities.

Finally, some recent research has explored how sequential or incremental information delivery influences a model’s reasoning process. Iterative refinement techniques~\cite{nye2021show} and self-consistency methods~\cite{wang2023self} indicate that presenting information step-by-step allows models to update their internal state and progressively build a more accurate understanding of a problem. These findings resonate with cognitive theories of human reasoning, where information is assimilated gradually to form and refine a mental model~\cite{siegler}. 

Our work builds upon these diverse research strands by introducing a mental model approach that explicitly supplies information incrementally. By comparing this approach to traditional full-prompt \texttt{CoT} methods on datasets such as \textsc{MathWorld}, we aim to investigate whether LLMs can truly evolve their internal representations in a human-like, dynamic fashion—or whether their reasoning remains predominantly a product of pattern recognition. In doing so, we not only extend prior studies on \texttt{CoT} and multimodal alignment but also probe the fundamental nature of reasoning in LLMs.