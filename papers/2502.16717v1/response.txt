\section{Related Work}
\texttt{CoT} prompting has emerged as a transformative technique in the domain of large language models (LLMs). Wei et al., "Emergence of Linguistic Structure from Hierarchical Sequence Generators" demonstrated that eliciting intermediate reasoning steps significantly improves performance on complex tasks such as arithmetic and commonsense reasoning. However, subsequent work has raised important questions about whether these generated reasoning traces reflect a genuine internal problem-solving process or are simply a by-product of pattern recognition over extensive pre-training data.

A related line of inquiry investigates the internal mechanisms underlying reasoning in both text-only and multimodal settings. Early research in vision-language understanding established that aligning visual and textual representations is key to coherent reasoning. For instance, models such as LXMERT, "LX-MERt: Multi-Task Attention with Explicit Masking for Vision-Language Pre-training"__VilBERT, "VILBERT: VIsion-and-LAnguage Multimodal Transformers for Visual Grounding and Question Answering"__ and UNITER, "UNITeR: A Universal Language Model for Vision and Text" leveraged cross-modal attention mechanisms to learn shared representations, thereby enhancing performance on tasks that require integrating visual cues with textual context. Similarly, contrastive learning approaches, as exemplified by CLIP, "Contrastive Learning of Visual Representations for Zero-Shot Transfer" have effectively mapped images and text into a common embedding space, underscoring the importance of alignment in developing a unified mental model.

Mental modeling in LLMs—forming internal task or world representations—has been explored in both text-only and multimodal contexts. In text-only settings, models must infer structure solely from textual descriptions, often lacking coherent representations of objects and their relations__. In contrast, multimodal models can leverage visual grounding to enhance internal representations__. Other strategies facilitate reasoning through iterative dialogue between image and language models__ or by translating visual information into natural language__. Recent advances in multimodal reasoning extend beyond simple image-to-text transcriptions, focusing instead on reasoning across multiple modalities simultaneously__. 
These findings align with classic cognitive insights__ suggesting that visual representations reduce cognitive load by making implicit relationships explicit. Collectively, this body of research underscores that multimodal models construct richer and more grounded mental models than their text-only counterparts.

The evolution toward large-scale multimodal models—such as Flamingo, "FLAMINGO: Fast and Lightweight Multi-Modal Observations for Vision-and-Language Tasks"__ and BLIP-2, "BLIP2: Boosting Visual and Textual Understanding with Large-Scale Pretraining"__ has further underscored that even when visual inputs are available, the core reasoning engine remains largely text-based. Proprietary systems like GPT-4V, "GPT-4 Version 1: An Efficient Framework for Reasoning in LLMs"__ and PaLM-E, "PaLM-E: Large-Scale Multi-Task Learning for Vision-and-Language Tasks"__ reinforce this observation: despite their ability to process visual information, these models often exhibit reasoning processes that are strongly influenced by their language model components. This body of work suggests that whether operating on text alone or in combination with visual modalities, LLMs tend to develop a unified internal representation that drives their reasoning capabilities.

Finally, some recent research has explored how sequential or incremental information delivery influences a model’s reasoning process. Iterative refinement techniques__, "IterRefine: Iterative Refinement for Zero-Shot Transfer"__ and self-consistency methods__, "SCM: Self-Consistency Methods for Multi-Task Learning"__ indicate that presenting information step-by-step allows models to update their internal state and progressively build a more accurate understanding of a problem. These findings resonate with cognitive theories of human reasoning, where information is assimilated gradually to form and refine a mental model__. 

Our work builds upon these diverse research strands by introducing a mental model approach that explicitly supplies information incrementally. By comparing this approach to traditional full-prompt \texttt{CoT} methods on datasets such as \textsc{MathWorld}, we aim to investigate whether LLMs can truly evolve their internal representations in a human-like, dynamic fashion—or whether their reasoning remains predominantly a product of pattern recognition. In doing so, we not only extend prior studies on \texttt{CoT} and multimodal alignment but also probe the fundamental nature of reasoning in LLMs.