
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\usepackage{subcaption}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
%\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{makecell}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{caption}
\usepackage{array}
\usepackage{multirow}
\usepackage[most]{tcolorbox}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{makecell}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{caption}
\usepackage{array}
\usepackage{multirow}
\usepackage[most]{tcolorbox}
\usepackage{pdflscape}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{float}

\usetikzlibrary{shapes.geometric, arrows.meta, positioning}
\tikzstyle{criterion} = [rectangle, draw=black, fill=gray!20, rounded corners, text centered, text width=4cm, minimum height=1cm]
\tikzstyle{question} = [rectangle, draw=black, fill=blue!15, rounded corners, text centered, text width=7.5cm, minimum height=1.3cm]
\tikzstyle{example} = [rectangle, draw=black, fill=green!15, rounded corners, text centered, text width=12cm, minimum height=1.6cm]
\tikzstyle{arrow} = [thick,->,>=stealth]
%\usepackage[utf8]{inputenc}
%\usepackage{tikz}
%\begin{document}




\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
\definecolor{darkred}{rgb}{0.5, 0.0, 0.0}
\definecolor{darkorange}{rgb}{0.8, 0.33, 0}

\input{macros}




\title{AIMS.au: A Dataset for the Analysis of \\Modern Slavery Countermeasures in \\Corporate Statements}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{%
\centerline{Adriana Eufrosiana Bora$^{1,2}$ \quad 
Pierre-Luc St-Charles$^{1}$ \quad 
Mirko Bronzi$^1$} \\ 
\centerline{\textbf{Arsène Fansi Tchango}$^1$  \quad 
\textbf{Bruno Rousseau}$^1$ \quad 
\textbf{Kerrie Mengersen}$^2$} \\
\centerline{$^1$Mila - Quebec AI Institute} \\
\centerline{\small\texttt{\{adriana.eufrosina-bora, pl.stcharles, mirko.bronzi\}@mila.quebec}} \\ 
\centerline{\small\texttt{\{arsene.fansi.tchango, bruno.rousseau\}@mila.quebec}} \\
\centerline{$^2$School of Mathematical Sciences, The Queensland University of Technology}\\
\centerline{\small\texttt{adrianaeufrosina.bora@hdr.qut.edu.au}} \\ 
\centerline{\small\texttt{k.mengersen@qut.edu.au}}
}

%\author{%
%  Adriana Eufrosiana Bora \\
%School of Mathematical Sciences\\
% The Queensland University of Technology
%\\
%Brisbane, QLD, 4000, Australia\\
%adrianaeufrosina.bora@hdr.qut.edu.au\\
%Mila - Quebec AI Institute\\
%Montréal, QC H2S 3H1, Canada \\
%\texttt{adriana.eufrosina-bora@mila.quebec}
%  \And
%  Pierre-Luc St-Charles \\
%  Mila - Quebec AI Institute
% \\
% Montréal, QC H2S 3H1, Canada \\
%   \texttt{pierreluc.stcharles@mila.quebec } \\
%   \AND
%  Mirko Bronzi \\
% Mila - Quebec AI Institute
% \\
% Montréal, QC H2S 3H1, Canada \\
%  \texttt{mirko.bronzi@mila.quebec} \\
%  \And
%  Arsène Fansi Tchango \\
%  Mila - Quebec AI Institute
% \\
% Montréal, QC H2S 3H1, Canada\\
%\texttt{arsene.fansi.tchango@mila.quebec } \\
%  \And
%Bruno Rousseau\\
%  Mila - Quebec AI Institute\\
%  \And
%  Kerrie Mengersen
% \\
%  School of Mathematical Sciences\\
% The Queensland University of Technology
%\\
%Brisbane, QLD, 4000, Australia \\
% \texttt{k.mengersen@qut.edu.au} \\
%}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Despite over a decade of legislative efforts to address modern slavery in the supply chains of large corporations, the effectiveness of government oversight remains hampered by the challenge of scrutinizing thousands of statements annually. While Large Language Models (LLMs) can be considered a well established solution for the automatic analysis and summarization of documents, recognizing concrete modern slavery countermeasures taken by companies and differentiating those from vague claims remains a challenging task. To help evaluate and fine-tune LLMs for the assessment of corporate statements, we introduce a dataset composed of 5,731 modern slavery statements taken from the Australian Modern Slavery Register and annotated at the sentence level. This paper details the construction steps for the dataset that include the careful design of annotation specifications, the selection and preprocessing of statements, and the creation of high-quality annotation subsets for effective model evaluations. To demonstrate our dataset's utility, we propose a machine learning methodology for the detection of sentences relevant to mandatory reporting requirements set by the Australian Modern Slavery Act. We then follow this methodology to benchmark modern language models under zero-shot and supervised learning settings. 
\end{abstract}

\section{Introduction}
\label{sec:introduction}
The proliferation of legal mandates requiring corporations to disclose specific information regarding their human rights and environmental actions has necessitated the development of robust platforms and tools to facilitate compliance analysis. In line with other countries, the Australian Modern Slavery Act of 2018 \citep[the AU MSA, or the ``Act'',][]{amsa2018_legislation} requires over 3000 corporations to detail their efforts to combat modern slavery within their operations and supply chains~\citep{amsa2018_guidance2023}. The resulting number of freeform, annually-published statements worldwide exceeds the resources allocated by supervisory bodies to monitor modern slavery compliance. While numerous datasets have been created to support the development of automated approaches for text summarization and understanding such as in the medical and legal domains~\citep{zambrano2023rales, guha2023legalbench}, there exists a gap in large-scale datasets that help detect and extract relevant information explicitly mandated by this type of legislation from corporate statements. We address this gap by introducing a novel dataset tailored to the analysis of modern slavery statements, focusing on the extraction of pertinent information as specified by the Act.

Traditional approaches in machine learning for legal and declarative text understanding have primarily centered on summarization and synthesis~\citep{abdallah2023exploring, niklaus2024flawn, martinez2023survey}. These methodologies aim to condense lengthy documents into concise summaries or to interpret their key points and link them with a given query. The introduction of legislation that mandates corporations to share information without enforcing a document template motivates a shift from summarizing content to precisely identifying and extracting relevant disclosures while avoiding text distractions. These distractions encompass corporate jargon or assertions that, despite appearing positive, do not contain substantial actions or pertinent information. %Many companies fail to deeply engage with their supply chains and stakeholders, resulting in superficial commitments and a significant portion not fulfilling their initial promises or identifying obvious modern slavery risks in their operations\cite{dinshaw2022broken} .Corporations are often criticized for utilizing ambiguous language in their modern slavery statements, to mitigate potential legal liabilities, where applicable, or to avert negative publicity. Consequently, they prioritize adherence to the letter of the law rather than its spirit.


This paper introduces a new, publicly available dataset that can significantly advance machine learning research on modern slavery statements. This dataset is meticulously curated to aid in developing extraction processes that accurately identify and make accessible all relevant information required by the legislation for further analysis. This is made possible by manual annotations aimed at determining whether each sentence contains any mandated information. It provides the largest and most consistent resource specifically designed for retrieving information mandated by legislation. Unlike previous efforts, which were often too inconsistent and relied on broader, self-defined metrics, our dataset includes a substantially larger number of annotated statements aligned strictly with the mandatory criteria of the Australian Modern Slavery Act. Developed with advice from various key stakeholders, including the Australian government team responsible for monitoring the Act, this data set ensures direct legal relevance and robustness for compliance monitoring. What is more,  our benchmark results demonstrate that fine-tuned models trained on our annotations significantly outperform larger language models in zero-shot conditions, underscoring the dataset's value. By releasing this resource and its supporting materials as open source, we aim to foster broader adoption and further research, potentially enabling models to generalize to other legal frameworks with minimal adjustments and reducing the need for future large-scale annotation efforts. 

This paper is organized as follows. First, we provide a short background on the Australian modern slavery legislation (the Act). Next, we detail the construction steps of our dataset, which include the careful design of specifications used by annotators to ensure that relevant information is captured as accurately as possible. We detail the distribution and preprocessing of corporate statements into text that models can ingest, and the distribution of the relevant text extracted by annotators. We also discuss the creation of high-quality annotated statements subsets, which are essential for effective model validation and testing. Next, we describe a machine learning methodology specifically tailored for detecting sentences that are relevant to each mandatory reporting requirement outlined by the Act. This methodology provides an approach to differentiate between substantive disclosures and non-relevant content, for zero-shot and supervised learning settings. We then present benchmarking results that demonstrate the performance of large language models in both zero-shot and supervised settings. Subsequently, we discuss related works and argue that our findings offer insights into the capabilities and limitations of current works in handling this complex task. Finally, we conclude by elaborating on limitations of this paper and by outlining directions for future works.

\section{Background}
\label{sec:background}
\begin{figure}[t]
    \centering
    \hspace*{-0.4cm} % Adjust this value to shift the diagram to the left
    \scalebox{0.55}{ % Adjust the scaling factor to fit the page
    \begin{tikzpicture}[node distance=1cm and 0.5cm]

    % Column Labels
    \node[align=center, text width=4cm] (criterionLabel) at (-3.2, 1.1) {\textbf{AU MSA \\Mandatory Criteria}};
    \node[align=center, text width=9cm] (questionLabel) at (3.5, 1.1) {\textbf{AIMS.au Annotation Questions}};
    \node[align=center, text width=13cm] (exampleLabel) at (12, 1.2) {\textbf{Fictitious Examples of Relevant Disclosures}};

     % Vertical Labels
    \node[rotate=90, align=center, text width=3cm] (basicLabel) at (-6, -2) {\textbf{Basic Criteria}};
    \node[rotate=90, align=center, text width=3cm] (complexLabel) at (-6, -8.5) {\textbf{Advanced Criteria}};

    % Approval and Signature Nodes
    \node[criterion,align=center, fill=gray!5] (approval) at (-3.2, 0) {Approval};
    \node[question, align=left, right=of approval] (approvalQuestion) {Is the statement approved by the reporting entity’s principal governing body?};
    \node[example, align=left, right=of approvalQuestion] (approvalExample) {"This statement was approved by our principal governing body (our board) on March 15th, 2023."};

    \node[criterion,align=center, below=of approval,  fill=gray!5] (signature) {Signature};
    \node[question, align=left, right=of signature] (signatureQuestion) {Is the statement signed by a responsible member of the reporting entity?};
    \node[example,align=left,  right=of signatureQuestion] (signatureExample) {"This statement is signed by Jane Doe in her role as the managing director of Unicorn Pharmaceuticals on 21 November 2020."};

    % Criterion Nodes
    \node[criterion, align=center,below=of signature, fill=gray!5] (criterion1) {Criterion 1:\\Reporting Entity};
    \node[question, align=left, right=of criterion1] (question1) {Does the statement clearly identify the reporting entity?};
    \node[example,align=left,  right=of question1] (example1) {"ABC Corporation Ltd., ABN 123 456 789 is the reporting entity for this statement."};

    \node[criterion, align=center,below=1.1cm of criterion1] (criterion2) {Criterion 2: \\Structure, \\Operations, \\Supply Chains};
    \node[question,align=left, right=of criterion2] (question2) {
        \begin{tabular}{l}
            1. Does the entity describe its \textbf{structure}?\\
            2. Does the entity describe its \textbf{operations}?\\
            3. Does the entity describe its \textbf{supply chains}?
        \end{tabular}
    };
    \node[example,align=left,  right=of question2] (example2) {
        \begin{tabular}{l}
            \textbf{1. Structure:} "ABC Corporation has over 1,000 employees."\\
            \textbf{2. Operations:} "Our operations include manufacturing of lawnmowers parts \\  in Asia, and their   distribution in Australia."\\
            \textbf{3. Supply Chains:} "Our supply chains include raw materials such as timber,\\which is procured via suppliers in Southeast Asia."
        \end{tabular}
    };

    \node[criterion, align=center,below=1.1cm of criterion2] (criterion3) {Criterion 3:\\Modern Slavery Risks};
    \node[question, align=center, right=of criterion3] (question3) {Does the entity describe its modern slavery risks?};
    \node[example,align=left,  right=of question3] (example3) {"Areas in our supply chains with a higher risk of modern slavery include outsourced services such as cleaning, catering, security and facilities management, and use of labor hire contractors."};

    \node[criterion, align=center,below=of criterion3] (criterion4) {Criterion 4:\\Actions Taken};
    \node[question,align=left, right=of criterion4] (question4) {
        \begin{tabular}{l}
           1. Does the entity describe actions to identify,\\ assess,
            and \textbf{mitigate }modern slavery risks?\\
          2. Does it describe \textbf{remediation} actions?
        \end{tabular}
    };
    \node[example,align=left,  right=of question4] (example4) {
        \begin{tabular}{l}
            1. "In this reporting period, we have made progress in implementing our \\ Modern Slavery Policy and have updated our Whistleblowing Policy."\\
            2. "We established a remediation fund for affected workers and provide support \\ services."
        \end{tabular}
    };

    \node[criterion,align=center, below=of criterion4] (criterion5) {Criterion 5:\\Effectiveness};
    \node[question, align=left,right=of criterion5] (question5) {Does the entity describe how it assesses the effectiveness of actions?};
    \node[example,align=left,  right=of question5] (example5) {"We use key performance indicators (KPIs) to measure how effective our actions are, and determined that our 123 employees (100\%) were present at five modern slavery training sessions this year."};

    \node[criterion,align=center, below=of criterion5] (criterion6) {Criterion 6:\\Consultation};
    \node[question, align=left,right=of criterion6] (question6) {Does the entity describe consultation processes with entities it owns or controls?};
    \node[example,align=left,  right=of question6] (example6) {"We engaged and consulted with all companies we own or control in the development of this statement and regarding the policies we plan to enact."};

    % Arrows
    \draw[arrow] (approval.east) -- (approvalQuestion.west);
    \draw[arrow] (approvalQuestion.east) -- (approvalExample.west);

    \draw[arrow] (signature.east) -- (signatureQuestion.west);
    \draw[arrow] (signatureQuestion.east) -- (signatureExample.west);

    \draw[arrow] (criterion1.east) -- (question1.west);
    \draw[arrow] (question1.east) -- (example1.west);

    \draw[arrow] (criterion2.east) -- (question2.west);
    \draw[arrow] (question2.east) -- (example2.west);

    \draw[arrow] (criterion3.east) -- (question3.west);
    \draw[arrow] (question3.east) -- (example3.west);

    \draw[arrow] (criterion4.east) -- (question4.west);
    \draw[arrow] (question4.east) -- (example4.west);

    \draw[arrow] (criterion5.east) -- (question5.west);
    \draw[arrow] (question5.east) -- (example5.west);

    \draw[arrow] (criterion6.east) -- (question6.west);
    \draw[arrow] (question6.east) -- (example6.west);

    \end{tikzpicture}
    }

 \caption{Correspondences between the AU MSA Mandatory Criteria and the questions designed for the annotation of the proposed AIMS.au dataset, with fictitious examples of disclosures that could be found in statements published by reporting entities.}
 \label{fig:modern_slavery_flowchart}
 \vspace{-0.35cm}
\end{figure}
Modern slavery describes situations where coercion, threats, or deception are used to exploit victims and deprive them of their freedom. It encompasses any situation of exploitation that a person cannot refuse or leave due to threats, violence, coercion, deception, or abuse of power~\citep{free2022global}. In 2021, an estimated 50 million people were subject to modern slavery, with 28 million in forced labor. %~\cite{globalslaveryindex2023}.
This issue is believed to affect all industries worldwide, with industries such as agriculture, manufacturing, and construction being at higher risk.
%A critical impediment to eradicating modern slavery is the lack of transparency and accountability in corporate efforts to eliminate it from their supply chains. Without clear reporting requirements and oversight, it is difficult to hold companies responsible for unethical practices and recognize those that adhere to ethical standards. Numerous governments across the globe have passed legislation mandating large corporations to provide detailed statements on how they are addressing the issue of modern slavery in their supply chains and operations~\cite{mccorquodale2022human}. For our work, we focus on the reporting requirements set by the Australian Modern Slavery Act (the ``Act''), passed in 2018~\cite{amsa2018_legislation}. This legislation mandates corporate entities carrying on business in Australia with over AU\$100 million in revenue to provide an annual statement. Along with an official approval and signature, statements must meet seven Mandatory Criteria, which require them to:

A critical impediment to eradicating modern slavery is the lack of transparency and accountability in corporate efforts to eliminate it from their supply chains. Without clear due diligence, reporting requirements and oversight, it is difficult to hold companies responsible for unethical practices and recognize those that adhere to ethical standards. To address this issue, many governments have enacted legislation mandating companies to increase transparency in their supply chains. The movement began with the California Transparency in Supply Chains Act of 2010, which required large retailers and manufacturers doing business in California to disclose their efforts to eradicate slavery and human trafficking from their supply chains. This was followed by the UK's Modern Slavery Act of 2015, the first national law of its kind, mandating companies to publish a slavery and human trafficking statement approved by their governing body and posted on their website. However, these early laws primarily focused on disclosure without specifying mandatory reporting criteria or robust enforcement mechanisms \citep{mccorquodale2022human}.

%It was not until the Australian Modern Slavery Act, passed in 2018, that legislation introduced mandatory reporting criteria, clearly asking companies to report on specific actions taken to tackle modern slavery in their operations and supply chains. This legislation mandates corporate entities carrying on business in Australia with over AU\$100 million in revenue to provide an annual statement. As shown in Figure \ref{fig:modern_slavery_flowchart}, the statements must meet a set of Mandatory Criteria.

The Australian Modern Slavery Act of 2018 is the first legislation to introduce mandatory reporting criteria; see Figure~\ref{fig:modern_slavery_flowchart} for examples. These mandatory reporting requirements apply to companies with revenues exceeding AU\$100 million and compel them to submit an annual statement where they report on specific criteria highlighting actions taken to address modern slavery within their operations and supply chains. Other similar legislation possess compatible mandatory criteria; a comparison is provided in Appendix~\ref{sec:appendix:laws}. %
%
%\begin{enumerate}[label=C\arabic*)]
    %\item identify the reporting entity;
    %\item describe the reporting entity’s structure, operations, and supply chains;
    %\item describe the risks of modern slavery practices in the operations and supply chains of the reporting entity and any entities it owns or controls;
    %\item describe the actions taken by the reporting entity and any entities it owns or controls to assess and address these risks, including due diligence and remediation processes;
    %\item describe how the reporting entity assesses the effectiveness of these actions;
    %\item describe the process of consultation with any entities the reporting entity owns or controls; and
    %\item provide any other relevant information.
%\end{enumerate}
%
Yet, despite such legislation, many companies provide vague and distracting disclosures that hinder effective monitoring and progress. We give examples of such declarations in Appendix~\ref{sec:appendix:text_examples}. The growth in the volume of corporate statements published annually also makes it difficult to hold corporations accountable for misleading statements and broken promises. As a recent report \citep{dinshaw2022broken} highlights, for a set of modern slavery statements published by 92 reporting entities and analyzed by experts: 1) the majority did not meet basic reporting requirements; 2) only a third provided evidence of some form of effective action to tackle modern slavery risks; and 3) over half of all promises made regarding future actions in the past were unfulfilled in later statements. We believe that this type of review is necessary across all modern slavery statements published annually, but modern tools to assist experts in their analysis are required to scale this process. We believe that the AIMS.au dataset could serve as a key milestone in the development of such tools, providing a foundation for further advancements in this area.

%Since the enactment of the Australian Modern Slavery Act, various existing laws, such as the UK Modern Slavery Act ~\cite{uk_modern_slavery_act_2015}, have been strengthened with more robust reporting requirements, and new legislation, such as the Canadian Fighting Against Forced Labour and Child Labour in Supply Chains Act of 2023~\cite{canadian_forced_labour_act}, has been introduced. These laws share overlapping reporting criteria, whether they are recommended or mandated as shown in Appendix~\ref{sec:appendix:laws}. However, the Australian MSA stands out as one of the most comprehensive pieces of legislation in this field. Due to its long-standing enforcement, a substantial number of compliance statements have already been submitted, and the Act has mandated clear reporting criteria from its beginning. Additionally, its supervisory body has shown evident interest in verifying whether companies meet these criteria. For these reasons, we chose to focus on the Australian MSA and developed the AIMS.au dataset as a baseline. This dataset will support transfer and adaptation studies, providing a foundation for the eventual development of tools tailored to other legal contexts, such as those in UK or Canada.

Note that we chose to focus on the Australian Modern Slavery Act (MSA) due to its strong alignment with reporting criteria in other laws, its comprehensiveness, and its established track record of enforcement, which has resulted in a substantial number of compliance statements. Furthermore, its supervisory body actively verifies whether companies meet their obligations. These factors make the Australian MSA an ideal baseline for developing the AIMS.au dataset, which can support transfer and adaptation studies and serve as a foundation for tools tailored to other legal contexts, such as those in the UK or Canada. We expand on this in Appendix~\ref{sec:appendix:laws}.



\section{Dataset Description}
\label{sec:dataset}

Our proposed dataset, AIMS.au, is a combination of modern slavery statements published in PDF format by corporate entities and of sentence-level labels provided by human annotators and domain expert analysts. As shown Figure~\ref{fig:WorkflowAIMS.au}, a total of 5,670 statements were processed by hired annotators with respect to the three basic reporting criteria of the Act to determine whether each statement is approved, signed, and has a clearly-identified reporting entity. The other more advanced reporting criteria (previously shown in Figure~\ref{fig:modern_slavery_flowchart}) involve nuanced interpretations and required higher levels of scrutiny; for these, a subset of 4,657 statements that were found to be of a reasonable length were double annotated by hired annotators. Lastly, two specialized ``gold'' subsets with each 50 unique statements were created by experts to allow for evaluations with higher reliability across all criteria. The first gold subset was annotated by a single expert and validated through team discussions, while the second gold subset underwent a collaborative annotation process involving three experts. In all cases, disagreements were discussed until the experts achieved consensus. Given all these data subsets, we propose that future research utilizes statements annotated by hired workers for model training, statements in the first ``gold'' subset for model validation, and statements in the second gold subset for model testing; this should provide optimal trust in model performance assessments.

%A subset of 4,657 statements was also processed by two more hired annotators with respect to all remaining requirements of the Act (detailed in Section~\ref{sec:background}). Finally, two smaller ``gold'' subsets of 50 statements each have been independently prepared and annotated by more extensively trained members of our team for model validation and testing purposes. 

The final result is over 800,000 labeled sentences across 5,731 unique modern slavery statements covering 7,270 Australian entities between 2019 and 2023. As outlined in the following section and in Appendix~\ref{sec:appendix:annotations}, the annotation process was highly complex and resource-intensive, far from being a low-cost crowdsourced task. This process took over a year and a half to complete and required a large team of highly skilled annotators, working under the close supervision of experts.  Below, we detail the steps involved in the collection and preprocessing of statements, we discuss the choices that were made before and during the annotation process, and we provide summary statistics of our resulting dataset.

%ORIGINAL PLAce: https://app.diagrams.net/#G1gc1bXQ5jjJ1xXSeMkBeKv7tVt6FlAmiG#%7B%22pageId%22%3A%22N7VLN2-xLe97m4c8ZjeL%22%7D
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/AIMS.au_Dataset_Flowchart7.PNG}
    \caption{Overview of the annotation workflow for the AIMS.au dataset.}
    \label{fig:WorkflowAIMS.au}
\end{figure}


\textbf{Statement collection process.} Modern slavery statements to be annotated were first identified based on the already published and available PDF statements hosted on the Australian Modern Slavery Register \citep{amsa2018_register} as of April 2023. We eliminated statements that were fully scanned from our selection to simplify the text extraction process and to minimize errors that would be due to the use of Optical Character Recognition (OCR) tools. The 5,731 statements are associated with a total of more than 7,200 entities and 10,000 trademarks spanning more than 20 industrial sectors. These statements are issued by a diverse range of legal entities, including public and private companies, partnerships, sole proprietorships, trusts, government-owned corporations, and non-profit organizations. On average, each statement comprises 10.4 pages and 141 sentences, resulting in a combined total of nearly 60,000 pages and over 800,000 sentences. Other information on the data distribution is summarized in Figure~\ref{fig:data:statement_stats} and in Appendix~\ref{sec:appendix:dataset_description}.


\textbf{Conversion of text into sentences.} The text was extracted from the PDF statements using PyMuPDF \citep[``fitz'',][]{pymupdf_fitz} as well as ABBYY FineReader PDF (a commercial software). This text was then split into sentences using regular expressions that considered various start and end-of-sentence tokens, including classic punctuation (such as periods, exclamation marks, and question marks) and more unusual tokens (such as bullet points). Special care was taken to avoid issues related to abbreviations with periods to ensure accurate sentence boundaries. Additionally, we removed section numbers and prefixes where possible at the start of sentences using regular expressions. Edge cases such as nested punctuation and enumerations were also handled using regular expressions to improve the accuracy and quality of sentence splitting. Once the sentences were obtained, we retained only those containing at least one two-letter word to eliminate orphaned text resulting from fragmented tables, page numbers, and other non-sentence elements.

\textbf{Development of the annotation specifications.} The Mandatory Criteria listed in Section~\ref{sec:background} highlight two important challenges in the analysis of modern slavery statements with respect to the Act: 1)~there is no explicit definition of what constitutes ``relevant'' information, or a specified amount of relevant information required to meet the Act’s mandates; and 2)~the criteria are fairly high-level, necessitating interpretation and refinement into more precise and actionable items that annotators can verify. To address these challenges, we reviewed guidance material and supplementary examples \citep{amsa2018_guidance2023}, and consulted with the Australian Attorney General's Department to propose a breakdown of these criteria into more granular labeling tasks. Although labeling relevant information at the statement or paragraph level could be simpler than at the sentence level, it would offer limited utility for model training, evaluation, and downstream applications. Additionally, training laypersons to provide consistent and accurate high-level labels would be challenging and prone to significant subjectivity. Consequently, we translated the seven mandatory content criteria into eleven questions designed to be answered by extracting relevant sentences within the context of the entire statement. This approach was detailed in the annotation specifications provided to annotators, complete with training examples. The annotation specifications document is available as supplementary material with this paper. It was developed iteratively by a multidisciplinary team, where refinements alternated with small rounds of annotations to validate the proposed changes. The final version of the document was chosen based on its effectiveness in helping annotators avoid cognitive overload, minimizing inconsistencies in the annotations, and maintaining a reasonable large-scale annotation cost. A comprehensive description of the annotation labels associated with each of the eleven questions can be found in Appendix~\ref{sec:appendix:dataset_description}.
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/page_count_20241121.pdf}
        \caption{Page count per statement.}
        \label{fig:data:statement_stats:page_count}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/sentence_count_distrib_20241121.pdf}
        \caption{Sentence count per statement.}
        \label{fig:data:statement_stats:sentence_count}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/word_count_distrib_20241121.pdf}
        \caption{Word count per sentence.}
        \label{fig:data:statement_stats:word_count}
    \end{subfigure}
    \caption{Overview of the distribution of text across the 5,731 statements in our proposed dataset.}
    \label{fig:data:statement_stats}
\end{figure}
%\vspace{-1.3cm}


\textbf{Annotator selection and training.} Prior to the annotation of our dataset, we conducted preliminary experiments using language models that highlighted the need for a human-driven annotation process. Specifically, language models did not seem able to provide high-quality labels that would directly be adequate for subsequent analyses of modern slavery statements due to hallucinations and due to the impact of vague and distracting text. In fact, even experts can interpret legislative requirements differently and have varying opinions on the relevance of vague language depending on the context. This variability suggests that the most challenging questions should ideally be addressed by multiple annotators. However, assembling a large enough team of already-trained experts to annotate our entire dataset was impractical. Therefore, we engaged a private annotation company to provide workers with a strong understanding of English. We ensured that the company agreed to our contractual clauses on modern slavery, asking for the annotators to be fairly compensated and properly managed by the company; further details are provided in Appendix~\ref{sec:appendix:annotations}. The annotators received training based on our annotation specifications and a set of 20 statements that we manually annotated after thorough internal reviews. This training included Q\&A sessions and direct feedback on annotated examples. After the training phase, we initiated the broader annotation process.


\begin{figure}[t]
  \small
  \centering
  \begin{minipage}[b][1cm][b]{0.4\textwidth}
    \vspace*{\fill}
    \centering
    \captionof{table}{Agreement scores averaged across all double-annotated statements. We report the intersection over union (IAA) and Cohen’s Kappa (CK). The two scores are relatively comparable except for the most imbalanced criterion (C4, “remediation”) whose CK score is more negatively impacted.}
    \label{tab:data:annotation_stats:iaa_scores}
    \begin{tabular}{lcc}
        %\toprule
        \textbf{Question} & \textbf{IAA} & \textbf{CK} \\
        \midrule
        C2 (operations) & 0.66 & 0.76\\
        C2 (structure) & 0.67 & 0.75\\
        C2 (supply chains) & 0.75 & 0.82 \\
        C3 (risk description) & 0.67 & 0.73 \\
        C4 (remediation) & 0.93 & 0.77 \\
        C4 (risk mitigation) & 0.53 & 0.58 \\
        C5 (effectiveness) & 0.69 & 0.68 \\
        C6 (consultation) & 0.94 & 0.86 \\
        \midrule
        Overall & 0.73 & 0.74 \\
        %\bottomrule
      %\hline
    \end{tabular}
    \vspace*{\fill}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.55\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=6cm]{figures/relevant_ratios_20241121.pdf}
    \caption{Distribution of relevant sentences found by annotators over the total number of sentences per statement for our eleven questions.}
    \label{fig:data:annotation_stats:relevant_ratios}
  \end{minipage}
  %\vspace{-3mm}
\end{figure}


\textbf{Quality assurance process.} As shown in Figure \ref{fig:WorkflowAIMS.au}, the annotation process was divided into two phases. Initially, we focused on three simpler questions related to Criterion 1 (C1, ``identifying the reporting entity'') and to the approval and signature of the statement. This phase aimed to refine our interaction with annotators and clarify our quality expectations. Given that the accuracy of sentence-level labels depends on thorough extraction of relevant sentences, we emphasized that no relevant text should be overlooked and that entire statements needed to be read. This first phase lasted several weeks and targeted 5,670 statements, with a single annotator reviewing each statement. Each week, a random sample of 10 annotated statements was inspected to provide corrections and feedback. Upon completing this phase, we conducted a high-level review and found less than 1.2\% of the annotations invalid due to improper formatting, mostly because dates for approval or signature were missed. The second annotation phase focused on the eight questions related to the remaining mandatory criteria. Here, two annotators independently reviewed each statement, and we set consistency targets using Inter-Annotator Agreement (IAA) thresholds. These eight questions are more challenging, so ensuring maximum consistency is critical. The IAA, defined as the intersection over union of relevant sentences found by the two annotators, was used to assess agreement. If the IAA for a statement was below the target threshold, a third annotator revisited and corrected the annotations. The IAA scores obtained for double-annotated statements are presented in Table~\ref{tab:data:annotation_stats:iaa_scores}, alongside Cohen's Kappa (CH) scores; we further discuss the usefulness of these scores in Appendix~\ref{sec:appendix:limitations}. Due to time and budget constraints, this second phase included only statements shorter than 15 pages, which corresponds to 4,657 statements (82\% of the total). We note that longer statements often required over 45 minutes to annotate, and were not necessarily more content-rich. For this phase, less than 1\% of annotations were invalid due to improper formatting, primarily from text not being extracted from figures or tables that were tagged as relevant. Figure~\ref{fig:data:annotation_stats:relevant_ratios} illustrates the distribution of relevant labels across all sentences for our eleven questions. As expected, these plots reveal that the proportion of relevant sentences among all sentences is low, with the highest average ratio reaching only 20\% for the question related to C4 (``risk mitigation'').

\section{Benchmark Experiments}
\label{sec:experiments}
\textbf{Splitting training and evaluation data.} For training and evaluation purposes, we cluster statements based on their associated entities and trademarks. We then assign each statement cluster to either the training set, validation set, or test set. This method ensures that similar statements made by related entities or by the same entity across different years are assigned to the same set, effectively preventing data leakage. For validation and testing, we created ``gold'' sets of statements that were annotated exclusively by extensively trained members of our team based on multiple rounds of review and discussion. Each of these sets contains 50 statements: the validation set was annotated by a single analyst, while the test set was annotated collaboratively by three analysts. These gold sets aim to minimize label noise, which is more prevalent in annotations provided by external annotators. Based on our observations, this noise primarily consists of omissions, such as missed relevant text. We emphasize that omissions are less problematic in the gold set annotations, where we use the union of multi-labeled sentences from multiple annotators; indeed, the likelihood of all annotators omitting exactly the same text is low. The statements in both gold sets were randomly selected based on clustering results while ensuring they were not used elsewhere, such as in the examples for the annotation specifications. We handled the statements and annotations with care (particularly those in the gold sets) to prevent indirect leakage to future generations of language models \citep{balloccu2024leak}.

We detail limitations of our dataset in Section~\ref{sec:conclusion} and in Appendix~\ref{sec:appendix:limitations}. For more specific details on the preparation of our dataset and on its contents, we refer the reader to Appendix~\ref{sec:appendix:dataset_description}.


In this section, we outline our experimental setup and present the results of benchmarking various models for detecting sentences relevant to the mandatory reporting requirements of the Act. We evaluate the performance of these models under both zero-shot and fine-tuning settings to assess their effectiveness in extracting mandated information from statements. We then analyze the results to identify key insights and potential areas for improvement.  % , and strategies for the future development of models that determine whether companies are meeting the requirements of modern slavery legislation  THIS LAST PART IS KEPT FOR THE CONCLUSION

\textbf{Task definition.} Our proposed dataset includes a variety of labels that models could predict; these labels are detailed in Appendix~\ref{sec:appendix:dataset_description}. For conciseness and clarity, we focus on a task that we believe will be of greatest interest to the machine learning community: predicting relevant or irrelevant labels according to our eleven questions. We frame this task as a sentence-level binary classification problem which we evaluate across the eleven questions using the F1 metric. We selected this metric over accuracy because it allows us to identify cases where models simply learn to predict all sentences as irrelevant, since those are over-represented in our dataset (see Figure~\ref{fig:data:annotation_stats:relevant_ratios}). %For experiments based on multiple criteria, we also provide Macro aggregation scores so that all criteria contribute the same to the final score.

For the statements that are double annotated by hired workers, we adopt a ``union'' label combination strategy, where a sentence is considered relevant if any annotator marks it as such. This approach addresses the possibility that individual annotators may have missed relevant text in some statements. We suggest that future works explore more sophisticated methods for leveraging annotator disagreements as a supervision signal. For our current experiments, models are evaluated exclusively using the subsets of ``gold'' annotated statements. Since these gold sets contain high-quality annotations, their smaller size (roughly 7000 sentences each) with respect to the overall dataset size should not significantly impact the reliability of model evaluations. Furthermore, this approach helps us, as well as future researchers, avoid incurring significant API usage costs when using state-of-the-art, closed-source language models for large-scale evaluations. % on a subset of statements annotated by external workers.

\textbf{Evaluated models.} We conduct our experiments using a range of language models that includes four open models --- DistilBERT \citep{sanh2020distilbert}, BERT \citep{devlin2019bert}, \llama{} \citep{touvron2023llama} and \llamathree{} \citep{dubey2024llama3herdmodels} --- and two closed models, namely OpenAI's \gptthree{} and \gptfour{} (see Appendix~\ref{sec:appendix:impl_and_exp_details} for more details). We use the OpenAI and \llamathree{} models to evaluate zero-shot (prompt-based) approaches, and we compare them with DistilBERT, BERT, \llama{} and \llamathree{} models fine-tuned directly on statements annotated by hired workers. Our experiments are structured based on two input data setups: in the first ("\emph{No context}" setup), models only have access to the target sentence being classified; in the second ("\emph{With context"} setup), we provide additional context by including up to 100 words balanced before and after the target sentence (see Appendix~\ref{sec:appendix:prompt_design_and_ex} for an example). These two input setups allow us to assess the impact of contextual information on model performance.

The open models DistilBERT, BERT, \llama{} and \llamathree{} are fine-tuned from self-supervised pre-training checkpoints available on the HuggingFace repository \citep{wolf2019huggingface}. For DistilBERT and BERT, we fine-tune the full model weights, while for \llama{} and \llamathree{}, we use the LoRA approach \citep{hu2021lora} to manage computation costs. All experiments are conducted on a A100L GPU with 80~GB memory using PyTorch. Token sequence lengths are capped at 512 for DistilBERT and BERT, and at 150 for \llama{} and \llamathree{}, due to memory limitations. Models are trained with a batch size of 96 for DistilBERT, 64 for BERT, 32 for \llama{}, and 64 for \llamathree{}, using Adam \citep{kingma2014adam} with a fixed learning rate (0.00003). We select model checkpoints that maximize the Macro F1-score. Links to the model pages and checkpoint names are provided in Appendix~\ref{sec:appendix:impl_and_exp_details}.

\textbf{Prompt design for zero-shot experiments.} Experiments with \gptthree{}, \gptfour{} and \llamathree{} zero-shot are conducted using prompt templates designed specifically and given in Appendix~\ref{sec:appendix:prompt_design_and_ex}. These templates were developed based on insights gained from five iterations of prompt exploration conducted on a small set of documents, while also following best practices on how to formulate intents, how to provide domain definitions, and how to constrain desired outputs \citep{ekin2023prompt}. The definitions provided in the prompt are taken from the Act and its guidance document \citep{amsa2018_legislation, amsa2018_guidance2023}, and are essentially a condensed version of the instructions given to the annotators. We leave the exploration of more sophisticated prompts, or very large prompts that may include multiple examples or even our entire annotation specifications document, for future works.
\vspace{-0.25cm}
\subsection{Results}
\label{main_results}
\vspace{-0.25cm}

Table~\ref{tab:f1_results_zero_shot} presents results in the zero-shot setting. Alongside \gptthree{} and \gptfour{}, we include \llamathree{} for direct comparison within the same model architecture after fine-tuning.
Both \gptthree{} and \gptfour{} outperforms \llamathree{} by a substantial margin. Notably, \llamathree{} exhibits a tendency to predict the criteria for almost all sentences, leading to poor F1 scores due to low Precision. This behavior also explains its relatively better performance on criterion with more positive examples, such as "C4 (risk mitigation)" (see Figure \ref{fig:data:annotation_stats:relevant_ratios}).
In the "With context" experiments, \gptfour{} demonstrates significant performance improvements, whereas \gptthree{} shows a steep decline, defaulting to predicting the criteria for nearly every sentence, similar to the pattern observed with \llamathree{}. We hypothesize that this discrepancy arises because \gptfour{} is better equipped to handle long prompts and inputs compared to \gptthree{}.

%According to the LMSYS leaderboard~\cite{chiang2024chatbot} as of May 2024, \gptfour{} is the top-performing language model across various tasks. While its technical details are undisclosed, we can infer that \gptfour{} is considerably more complex than BERT or \llama{}. Here, \gptfour{} narrows the performance gap with fine-tuned models and outperforms \gptthree{}.
%The fine-tuned models exhibit comparable performance, though there remains room for improvement. Precision and recall are generally balanced, except for \gptthree{}, which appears to classify nearly all sentences as relevant. Comparing results with and without context, we observe performance improvements for all models except \gptthree{}, possibly due to input sequence length exceeding the model's limitations. Future research should explore techniques to prepare or leverage context more effectively.



%OLD FIGURE 3 
%\begin{figure}[t]
%\begin{center}
%\vspace{2mm}
%\includegraphics[width=\textwidth,height=6.5cm]{figures/results__all_labels.pdf}
%\caption{F1 scores obtained by fine-tuned models trained jointly on the eleven relevant content questions, along with their overall (``Macro'') F1 performance. See Appendix~\ref{sec:appendix:add_results} for detailed results.}
%\label{fig:experiments:results:all}
%\end{center}
%\end{figure}

\begin{table}[h]
    \small
    \centering
    \caption{F1 evaluation results for \textbf{zero-shot} approaches conducted
using \gptthree{}, \gptfour{} and \llamathree{}. Results in the "With context" case are unavailable for \llamathree{} due to
time limitations.}
    \label{tab:f1_results_zero_shot}
    \vspace{1mm}
    \begin{tabular}{lcccccc}
        %\toprule
        \textbf{Question} & \multicolumn{3}{c}{\textbf{No context}} & \multicolumn{2}{c}{\textbf{With context}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-6}
                &  \gptthree{}    & GPT4o   & \llamathreeNOSIZE{} & \gptthree{} & GPT4o \\
        \midrule
        Approval &  0.584 & 0.911 & 0.041 & 0.028 & 0.895 \\
        C1 (reporting entity)  & 0.148 & 0.378 & 0.054 & 0.031 & 0.427 \\
        C2 (structure)  & 0.371 & 0.661 & 0.168 & 0.097 & 0.616 \\
        C2 (operations)  & 0.268 & 0.616 & 0.172 & 0.167 & 0.601 \\
        C2 (supply chains)  & 0.317 & 0.543 & 0.211 & 0.174 & 0.556 \\
        C3 (risk description)  & 0.337 & 0.422 & 0.182 & 0.194 & 0.512 \\
        C4 (risk mitigation) & 0.591 & 0.601 & 0.478 & 0.481 & 0.624 \\
        C4 (remediation)  & 0.269 & 0.548 & 0.055 & 0.048 & 0.555 \\
        C5 (effectiveness) & 0.295 & 0.293 & 0.216 & 0.142 & 0.435 \\
        C6 (consultation) & 0.383 & 0.481 & 0.050 & 0.038 & 0.620 \\
        Signature  & 0.684 & 0.480 & 0.091 & 0.030 & 0.763 \\
        \midrule
        Overall (macro)  & 0.386 & 0.439 & 0.156 & 0.130 & 0.600 \\
        %\bottomrule
    \end{tabular}
\end{table}


We present evaluation results for all fine-tuned models jointly trained on the full eleven-question setting in Table~\ref{tab:appendix:add_results:fine_tuned}.
% improvement for Llama3.2 zero-shot to fine-tuned: 0.694 from 0.156 
Results are significantly higher than the zero-shot case; in particular, fine-tuned \llamathree{}, compared to the zero-shot results for the same architecture results in a increase in performances from 0.156 to 0.694 Macro-F1.
% DBERT: (0.702 - 0.670) / 0.670 = 0.047 -- BERT: (0.718 - 0.665) / 0.665 = 0.079 -- LLAMA3: (0.721 - 0.694) / 0.694 = 0.038
Overall, adding context to the input provides better results, with performances increasing for all the three models.
Comparing the models, \bert{} and \dbert{} provides similar results, while \llamathree{} outperforms the other models by some margin; \llama{} instead provides the lowest results, which we speculate is due to having more capacity in the model weights, thus needing more fine-tuning iterations (see Appendix \ref{appendix__result_details__f1_evolution} for more information).

\begin{table}[h]
    \small
    \centering
    \caption{F1 evaluation results for jointly \textbf{fine-tuned} models on all eleven Mandatory Criteria questions. \llama{} results are available only for the "No context" case for computational constraints.}
    \label{tab:appendix:add_results:fine_tuned}
    \vspace{1mm}
    \begin{tabular}{lccccccc}
        %\toprule
        \textbf{Question} & \multicolumn{4}{c}{\textbf{No context}} & \multicolumn{2}{c}{\textbf{With context}} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-8}
                & DistilBERT    & BERT    &   \llamaNOSIZE{} & \llamathreeNOSIZE  & DistilBERT & BERT & \llamathreeNOSIZE\\
        \midrule
        Approval & 0.957 & 0.965 & 0.889 & 0.940 & 0.955 & 0.964 & 0.932\\
        C1 (reporting entity) & 0.639 & 0.605 & 0.579 & 0.643 & 0.698 & 0.728 & 0.715 \\
        C2 (structure) & 0.708 & 0.732 & 0.708 & 0.745 & 0.740 & 0.740 & 0.726\\
        C2 (operations) & 0.741 & 0.718 & 0.672  & 0.753 & 0.769 & 0.758 & 0.773\\
        C2 (supply chains) & 0.723 & 0.675 & 0.719 & 0.729 & 0.755 & 0.772 & 0.787 \\
        C3 (risk description) & 0.653 & 0.660 & 0.650 & 0.686 & 0.705 & 0.741 & 0.752\\
        C4 (risk mitigation) & 0.631 & 0.614 & 0.602 & 0.611 & 0.629 & 0.640 & 0.667\\
        C4 (remediation) & 0.574 & 0.571 & 0.424 & 0.564 & 0.500 & 0.559 & 0.615\\
        C5 (effectiveness) & 0.533 & 0.483 & 0.242 & 0.527 & 0.491 & 0.560 & 0.500\\
        C6 (consultation) & 0.414 & 0.429 & 0.293 & 0.611 & 0.641 & 0.571 & 0.588\\
        Signature & 0.794 & 0.859 & 0.797 & 0.830 & 0.844 & 0.866 & 0.873\\
        \midrule
        Overall (macro) & 0.670 & 0.665 & 0.598 & 0.694 & 0.702 & 0.718 & 0.721\\
        %\bottomrule
  \end{tabular}
\end{table}


% TO DELETE
%Table~\ref{tab:appendix:add_results:allquestions} presents the detailed F1 evaluation results for the jointly fine-tuned models that are also shown in Figure \ref{fig:experiments:results:all} of the paper. For an analysis of the results, refer to Section \ref{main_results}. Table~\ref{tab:f1_results_gpt_distilbert} instead reports F1 results for zero-shot models (\gptthree{} and GPT4o) and for DistilBERT fine-tuned on a single criterion (i.e, not jointly), for all the criteria. Results show that the various criteria have a similar trend to the "C2 (supply chains)" case, already analyzed in Section \ref{main_results}. Results with context generally outperform those  without context, for both DistilBERT and \gptfour{}. \gptthree{}, on the other hand, is not able to handle the extended context scenario, leading to poor results, with the model essentially defaulting to predicting the criteria for every sentence.

One final insight we emphasize is that, based on the presented results and our preliminary prompt engineering experiences, it is challenging to find prompts for zero-shot models that can match the performance of fine-tuned models.
%This contrasts strongly with the \emph{seemingly} straightforward nature of the task: determining whether a single sentence contains a relevant disclosure.
This highlights the necessity for high-quality, curated datasets like \dataset{} to allow for the reliable training and evaluation of language models. Additionally, this underscores the need for further exploration into the importance of context at various scales and the impact of vague and distracting text on large language models.






\section{Related Works}
\label{sec:relworks}


\textbf{AI for analyzing supply chain disclosures under the California Transparency Act.} A few initiatives have considered machine learning to analyze statements in response to modern slavery legislation in the literature. For instance, LegalBench \citep{guha2023legalbench} proposed a benchmark for evaluating legal reasoning capabilities in language models. It consists of 162 tasks crafted by legal experts, and one of these is related to supply chain disclosures under the California Transparency in Supply Chains Act. The analysis of roughly 400 statements with one or two pages each using modern language models reveals only an accuracy of around 75\%. Similar to the high-level decision process used by analysts, the proposed classification approach for this task relies on statement-level decision making for a limited set of questions. The researchers discuss in their report how model performance diminishes in tasks involving longer text or more numerous questions, which suggests that scaling this statement-level decision making strategy to much larger statements is probably not ideal.

\textbf{AI for the analysis of UK modern slavery statements.} Despite numerous studies analyzing a handful of modern slavery statements manually (details in Appendix \ref{sec:appendix:relevant_works}), only a few have investigated the use of machine learning to date. For instance, modern slavery statements from the UK are analyzed without supervision using topic modeling \citep{nersessian2022human, bora2019augmentedintelligence}. While this approach allows the authors to monitor disclosure trends and correlate them across different statements, it is unable to analyze each statement and differentiate vague claims and promises from substantive actions. Consequently, this approach cannot adequately verify compliance with respect to a specific legislation. Based on their analysis, the authors highlight that many companies ``anchor'' their disclosures in broader human rights language and that they emphasize their engagement in social causes in an effort to bolster their company's social reputation. This underlines the challenge of carefully avoiding distractions while assessing whether a statement contains mandated information.

UK modern slavery statements were also analyzed under an initiative of the Walk Free and of The Future Society organizations, resulting in an open-sourced project on GitHub \citep{tfs2022projectaims} and a technical report \citep{weinberg2020ai}. This initiative examined 16,000 statements and utilized approximately 2,400 annotated statements from WikiRate \citep{WikiRate2023} for supervised machine learning experiments. In this work, classifiers were first trained to distinguish statements addressing specific mandatory content. These classifiers were then used to predict whether statements were correctly approved by a governing body based on annotator comments, keyword-based summaries, and n-gram representations. Limitations of this work noted by the authors include the difficulty in scaling to a large number of statements due to the usage of keyword-based and comment-based approaches, and due to the poor quality of the annotated statements. This previous research concluded that a stricter annotation process was necessary for developing new datasets and robust experimental protocols for subsequent studies. 
Moreover, as highlighted by other relevant studies on AI and sustainability reporting discussed in Appendix \ref{sec:appendix:relevant_works}, existing approaches continue to face difficulties in distinguishing concrete actions from vague text addressing relevant topics. Across these studies, many authors have emphasized challenges with training data quality and annotation biases. To the best of our knowledge, our paper now presents the largest annotated dataset globally, designed for machine learning research on modern slavery statements, while also marking the first academic study to scrutinize Australian modern slavery statements at scale, using machine learning techniques. 

\section{Conclusion}
\label{sec:conclusion}

Our work presents a significant contribution to the field of machine learning and natural language processing by introducing a manually annotated dataset of modern slavery statements that is specifically curated to determine whether companies meet the mandatory reporting requirements outlined by the Australian Modern Slavery Act. This dataset is particularly valuable due to the unique and challenging nature of the sentence relevance classification task, characterized by vague and distracting text, as well as by the large amount of context required to understand the most complicated statements.

While this dataset provides a broad collection of annotated statements for future machine learning experiments, several limitations should be acknowledged. First, the reliance on external annotation services, despite extensive training and oversight, may introduce inconsistencies and biases in the labeled data. Annotators' varying interpretations of vague language and subjective judgment in identifying relevant information could affect the overall quality and consistency of the annotations. Another limitation involves figures and tables within statements, which cannot be easily analyzed without OCR or without a vision model. Although we can limit the scope of models to only focus on the extraction of relevant text that is not embedded inside figures or tables, some necessary context might sometimes be missing in order to understand a human annotator's decision. Lastly, we chose not to differentiate past and future information based on reporting periods to simplify the annotation process. In other words, corporations often detail past actions or future plans within their statements, and we consider all such disclosures relevant. This approach may complicate the assessment of whether a reporting entity meets the Act's requirements for a specific period, as it necessitates classifying relevant text according to each reporting period. We discuss potential solutions to these limitations in Appendix~\ref{sec:appendix:limitations}.

We have conducted evaluations on modern language models, establishing performance benchmarks using both zero-shot and fine-tuning approaches. These benchmarks will serve as comparison baselines for future research in this domain. Our findings underscore the necessity of high-quality, curated datasets to reliably train and evaluate language models, especially in tasks that demand nuanced understanding and contextual analysis. Despite the promising results, there is significant room for future improvements, including the exploration of noisy label classification and more sophisticated context-handling techniques. Future research could also investigate the potential of integrating Vision-Language Models \citep[VLMs,][]{bordes2024introduction} to enhance the accuracy of information extraction in complex documents. Lastly, as we highlighted in Appendix \ref{sec:appendix:laws}, this dataset can be considered a key resource for other relevant studies and tools tackling mandatory reporting legislation on business and human rights, such as the UK Modern Slavery Act~\cite{uk_modern_slavery_act_2015} and the Canadian Fighting Against Forced Labour and Child Labour in Supply Chains Act~\cite{canadian_forced_labour_act}. 

\newpage

\bibliography{references}
\bibliographystyle{iclr2025_conference}

\appendix
\section{Other Related Works} 
\label{sec:appendix:relevant_works}
\textbf{Australian modern slavery statement manual reviews.} Some academic groups and non-profit organizations have conducted analyses of Australian modern slavery statements to evaluate the legislation's effectiveness. %For instance, larger corporations listed on the Australian Stock Exchange (ASK) are closely examined by researchers like Christ et al.\cite{christ2019accounting}, the Australian Council of Superannuation Investors\cite{acsimodernslavery2021}, and in annual studies by Pham, Cui, and Ruthbah\cite{pham2023modern}. These researchers have reviewed statements from between 100 to 300 ASK companies.
For instance, in the work of \cite{christ2019accounting,acsimodernslavery2021,pham2023modern}, researchers reviewed statements for 100, 151, and 300 companies listed on the Australian Stock Exchange, respectively. The Human Rights Law Centre, an Australian human rights group, also conducted extensive analyses, examining 102 and 92 statements in two separate studies \citep{sinclair2022paper, dinshaw2022broken}. The Domus 8.7 index, a benchmark initiative facilitated by the Catholic Archdiocese of Sydney, represents one of the more comprehensive analyses of statements conducted so far \citep{acan2022domus}. In this project, seventy interns manually reviewed 1,500 statements for a total investment of over 5,000 hours of work. 
Although these various studies all required significant effort over multiple years, they together cover less than 20\% of all statements published so far on the Australian Modern Slavery Register \citep{amsa2018_register}, and none were scaled up in subsequent years. This underscores the significant challenges in analyzing modern slavery statements, even when only considering a single country and a single legislation. We also highlight that the data generated by analysts for individual statements is usually high-level and abstract (i.e. it consists of statement-wide labels indicating for example whether the issuer complies with the Mandatory Criteria, and justifications), and it is rarely made public or shared for research. %This is likely due to the subjective nature of the evaluations and due to reputational risks.
Lastly, we note that the Australian Attorney-General's Department also performs an annual analysis that includes all statements in order to submit an annual report to Parliament \citep{australiangovernment2022}. Unfortunately, we do not know the depth of this analysis, and the results are not made public directly. They are instead presented at an aggregated statistical level, making it difficult for researchers and organizations to track company-specific actions and promises.

\textbf{AI for the analysis of sustainability reports.} Several relevant studies exist that look at applications of artificial intelligence for compliance and document analysis beyond modern slavery. The Danish Institute for Human Rights (DIHR), for example, developed a text mining method based on a paragraph relevance classifier to analyze company sustainability reports against sustainability and human rights indicators, including modern slavery \citep{dihr2022dataanalysis}. They processed approximately 145,000 UN system recommendations related to Sustainable Development Goal (SDG) targets and analyzed 9,374 reports with a simple text classifier trained to detect paragraphs related to key topics. In their conclusions, DIHR researchers highlight how relevant information may often be found in tables or figures that are challenging to convert into a machine-readable format for analysis. Other researchers also interested in sustainability disclosures studied the application of machine learning on Management Discussion and Analysis (MD\&A) documents \citep{tian2023dataset}. In this case, 29,134 documents collected from the China Research Data Service (CNRDS) platform were analyzed using a Term Frequency, Inverse Document Frequency (tf.idf) weighting scheme to rank them based on their coverage of key sustainability topics. We note that this approach may also be sensitive to distractions, as, once again, it cannot differentiate concrete actions from vague text that covers a relevant topic.

As for advancements in the analysis of climate-related claims in corporate sustainability reports, several works should also be highlighted. \cite{luccioni2020analyzing} developed ClimateQA, a language model that identifies climate-relevant sections in reports through a question-answering approach, processing 2,249 reports and emphasizing input quality. \cite{ni2023chatreport} introduced ChatReport, which leverages language models to automate sustainability report analysis and compute conformity scores with international guidelines. This approach relies heavily on quality information retrieval and expert feedback. \cite{webersinke2022climatebert} proposed ClimateBERT, a model pre-trained on over 2 million climate-related paragraphs specialized for NLP in the climate domain. This led to a series of extensions, such as ClimateBERT-NetZero \citep{schimanski2023climatebert} for detecting net zero and emission reduction targets. \cite{bingler2024cheap} also explored climate disclosures and reputational risks with ClimateBertCTI, stressing the credibility of transition plans. Additionally, ClimateBERT and other language models such as BERT, RoBERTa, and Longformer were benchmarked on LobbyMap documents to estimate corporate climate policy engagement, highlighting the need for model fine-tuning across diverse formats \citep{morio2023nlp}. Across all of these works, many authors have highlighted that their proposed approach faced challenges with training data quality and annotation biases.

\section{Data Availability and Maintenance Strategy}  %%%% DO NOT MOVE ME! I NEED TO STAY HERE!
\label{sec:appendix:data_availability}  %  should always stay "APPENDIX A"

%The dataset is currently being finalized and is not yet available to the public. However, it will be made accessible in time for the camera-ready deadline.

For reviewing purposes, a data sample that is representative of the final dataset is available via \href{https://figshare.com/s/c432d9a4fbd0ab88e84f?file=49484742}{THIS LINK}, and the complete dataset will be made available online upon acceptance with official links added directly to the paper. At that point, download links for the dataset along with evaluation scripts, Python classes for data loading, and baseline experiment configuration files will be available in a dedicated GitHub repository. This repository will also be linked to a Digital Object Identifier (DOI) to ensure easy reference and citation.


We will make the dataset available in two formats: HDF5 \citep{hdf5repo} and Activeloop DeepLake \citep{hambardzumyan2022deep}. The HDF5 format is widely used across various domains and programming languages due to its versatility and efficiency in handling large volumes of data. The Activeloop DeepLake format, on the other hand, offers features specifically tailored for machine learning experimentation, including optimized PyTorch dataloaders, which facilitate seamless integration with machine learning workflows. Both formats are open data formats, promoting accessibility and ease of use. The dataset will be packaged so that it directly contains raw PDF data as well as all metadata from the Australian Modern Slavery Register which may be useful for future studies. The content of the dataset is detailed in Appendix~\ref{sec:appendix:dataset_description} in the data card style of \cite{gehrmann2021gem, suzgun2024harvard}.

The dataset will be hosted on Figshare \citep{figshare}, an online open access repository, ensuring that it is freely available to the research community. By leveraging Figshare's robust infrastructure, we aim to provide a reliable and persistent platform for dataset access. To promote widespread use and proper attribution, the dataset will be licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) license. This license permits unrestricted use, distribution, and reproduction in any medium, provided the original authors and source are credited.

The initial release of the dataset will contain all statements processed by hired annotators as well as our  ``gold'' validation set. We may withhold the release of the ``gold'' test set until 2025 in order to hold a model competition. Details and deadlines will be shared on our project's GitHub page.

\section{Examples of Disclosures}  %%%% DO NOT MOVE ME! I NEED TO STAY HERE!
\label{sec:appendix:text_examples}  %  should always stay "APPENDIX B"

In developing the annotation guidelines, our goal was to assist annotators in identifying concrete supporting evidence in statements. This was necessary as despite legislative mandates for specific disclosures, companies often provide vague, ambiguous, or distracting information that obstructs effective monitoring and progress. %Corporations are frequently criticized for using such language in their modern slavery statements, which leads to a focus on complying with the letter of the law rather than its spirit. Furthermore, even experts may interpret legislative requirements differently and have varying opinions on the relevance of ambiguous language depending on the context, adding to the complexity of the task.
Table~\ref{tab:appendix:examples} provides, for all our questions related to the Mandatory Criteria of the Act, fictitious examples of: 1) relevant information; 2) irrelevant information due to ambiguity (i.e. due to a lack of context); 3) irrelevant information due to vagueness (i.e. unacceptable no matter the context); and 4) distracting information. These examples are inspired by the contents of real statements and highlight the significant challenge of distinguishing between relevant and irrelevant information.

\begin{landscape}
\begin{table}
\centering
\caption{Examples of relevant and irrelevant information for questions related to the Mandatory Criteria of the Act.}
\vspace{1mm}
\label{tab:appendix:examples}
\begin{adjustbox}{scale=0.74}
\begin{tabularx}{26cm}{lXXXXX|}
%\begin{tabular}{lccccc}
    %\toprule
        \textbf{Question} &
        \textcolor{darkgreen}{\textbf{Relevant information}} &
        \textcolor{darkorange}{\textbf{Ambiguous information}} &
        \textcolor{darkred}{\textbf{Vague information}} &
        \textcolor{darkred}{\textbf{Distracting information}} \\
    \midrule
    \small
        \textbf{Approval} &
        "This statement was approved by our principal governing body (our board) on March 15th, 2023." &
        "The ethics board approved the publication of this statement." &
        "Approval was received for this statement." &
        "Our code of conduct was approved by the board." \\
        \textbf{C1 (reporting entity)} &
        "ABC Corporation Ltd., ABN 123 456 789 is the reporting entity for this statement." &
        (Company logo on the first page) &
        "This statement applies to numerous entities across our larger corporate family." &
        "Founded in 1980, X Corp. has a long history as a reporting entity in various jurisdictions." \\
        \textbf{C2 (operations)} &
        "Our operations include the manufacturing of lawnmower parts in Asia and their distribution in Australia." &
        "We are a leader service provider in our sector." &
        "We operate globally." &
        "We produced 10,000 units last year, achieving a 15\% increase in productivity." \\
        \textbf{C2 (structure)} &
        "ABC Corporation has a hierarchical governance structure with over 1000 employees." &
        “This statement covers a number of wholly-owned subsidiaries.” &
        "Our organization has a global structure leadership model." &
        "Here is the organizational chart for 2020 showing the department heads." \\
        \textbf{C2 (supply chains)} &
        "Our supply chain includes raw materials such as timber, which is procured via suppliers in Southeast Asia." &
        "We may procure sensitive goods from higher-risk countries." &
        "We sometimes contract other companies for services." &
        "Our downstream supply chain distributes our products to over 10,000 customers." \\
        \textbf{C3 (risk description)} &
        "Areas in our supply chains with a higher risk of modern slavery include outsourced services such as cleaning, catering, security and facilities management, and use of labor hire contractors." &
        "An assessment concluded that we have a low risk of modern slavery." &
        “Modern slavery has the potential to exist in the technology sector.” &
        “We understand and have mapped our businesses risks with an extensive assessment strategy.” \\
        \textbf{C4 (remediation)} &
        "We established a remediation fund for affected workers and provided support services." &
        “We understand the importance of workers knowing their rights and we will directly address violations when needed." &
        "Remediation actions are a key priority for us." &
        “We deeply believe in the need for concrete remedies when cases are discovered, and the common industry practice is to terminate any contract with faulty suppliers.” \\
        \textbf{C4 (risk mitigation)} &
        "In this reporting period, we have made progress in implementing our Modern Slavery Policy and have updated our Whistleblowing Policy." &
        “We have established a zero-tolerance approach towards modern slavery.” &
        "We have made sure that our suppliers comply with our policies." &
        “We are committed to maintaining the highest level of integrity and honesty throughout all aspects of our business.” \\
        \textbf{C5 (effectiveness)} &
        "We use key performance indicators (KPIs) to measure how effective our actions are, and determined that our 123 employees (100\%) were present at five modern slavery training sessions this year." &
        %"Our related policies were reviewed this year." &
        "We conducted a review of our practices and spent time evaluating actions over the past year." &
        “Our team has spent time reflecting on our activities to enhance our approach.” &
        "As part of our annual review process, we have also gathered and analyzed feedback from customer surveys." \\
        \textbf{C6 (consultation)} &
        "We engaged and consulted with all companies we own or control in the development of this statement and regarding the policies we plan to enact." &
        "Our statement is the result of a comprehensive review process that engaged stakeholders from within our corporate family." &
        "We do not need to consult externally in the preparation of this statement." &
        "Our statement reflects a collaborative effort that draws from various perspectives within our organization." \\
        \textbf{Signature} &
        "This statement is signed by Jane Doe in her role as the managing director of Unicorn Pharmaceuticals on 21 November 2020." &
        "Signed by John Doe, the company secretary of the Trustee." &
        "Signed by Jane Doe (21 November 2020)." &
        "Our company executives have all signed off on our modern slavery policies." \\
%\bottomrule
%\end{tabular}
\end{tabularx}
\end{adjustbox}
\end{table}
\end{landscape}

\section{AIMS.au Data Card}  %%%% DO NOT MOVE ME! I NEED TO STAY HERE!
\label{sec:appendix:dataset_description}  %  should always stay "APPENDIX C"

% inspired by: https://arxiv.org/abs/2207.04043

\subsection{Dataset Description}

\textbf{Dataset summary.} See Section~4 of the paper.

\noindent \textbf{Languages.} The dataset contains English text only. 

\noindent \textbf{Domain.} Long, freeform statements made by corporate entities.

\noindent \textbf{Additional details.} The dataset contains modern slavery statements originally published in PDF format by Australian corporate entities between 2019 and 2023, metadata for those statements, and annotations (labels) provided by hired workers and ourselves. Additional unannotated statements published over the same period and beyond are also packaged in the dataset as supplementary data for unsupervised learning experiments.

\noindent \textbf{Motivation.} We publish this dataset to support the development and evaluation of machine learning models for extracting mandated information from corporate modern slavery statements. Our aim is to facilitate research in this domain and foster future efforts to assess companies' compliance with the Australian Modern Slavery Act and other similar legislation.

\subsection{Meta Information}

\noindent \textbf{Dataset curators.} Withheld for anonymity; will be specified here at the camera-ready deadline. % The dataset was prepared by the authors themselves and... (+ roles?)

\noindent \textbf{Point of contact.} Withheld for anonymity; will be specified here at the camera-ready deadline. % Adriana? also refer to GitHub URL?

\noindent \textbf{Licensing.} The dataset is released under the Creative Commons Attribution 4.0 International (CC BY 4.0) license.

\noindent \textbf{Funding sources.} Withheld for anonymity; will be specified in the paper's acknowledgments at the camera-ready deadline.

\subsection{Dataset Structure}

\noindent \textbf{Data format and structure.} We structure our dataset so that one ``instance'' corresponds to a single statement. Each statement is associated with a unique identifier, a PDF file, and a set of twelve metadata fields, all provided by the Australian Modern Slavery Register. These metadata fields are:
\begin{itemize}
    \item Annual revenue;
    \item Countries where headquartered;
    \item Covered entities;
    \item Industry sectors;
    \item Overseas obligations;
    \item Reporting period end date;
    \item Reporting period start date;
    \item Publication date;
    \item Publication year in the register;
    \item Submission date;
    \item Associated trademarks;
    \item Statement type (normal or joint).
\end{itemize}

The PDFs are freeform, allowing reporting entities the flexibility to choose their format; some use a brochure-style layout, while others incorporate extensive background images or unique design elements. In addition to the provided metadata, we enhance these statements with several annotated fields, filled by our hired annotators or ourselves. These fields capture critical information such as compliance with reporting requirements and supporting content, as detailed in the next few paragraphs.

\noindent \textbf{Data preparation.} See Section~4 (``Conversion of text into sentences'') for information on text extraction. Following this step, we combine the raw PDF data (for researchers that intend on extracting the PDF contents themselves), its metadata, the extracted text (which, for ABBYY FineReader, includes the position of the text inside PDF pages and the OCR confidence levels), and the annotated fields into a single data archive. This archive is based on the Activeloop DeepLake format \citep{hambardzumyan2022deep} by default, and we provide a script to convert the dataset into HDF5 format.

\noindent \textbf{Annotated fields.} As detailed in Section~4 (``Development of the annotation specifications''), we translated the seven Mandatory Criteria of the Act into eleven questions. The questions are detailed in Appendix~\ref{sec:appendix:annotations}, and are tied to a set of fields to be filled by annotators based on their answers. Specifically, the fields shared by all questions are:
\begin{itemize}
    \item Label (yes/no/unclear): specifies whether the reporting entity has provided information that is relevant for the targeted criterion;
    \item Supporting text: contains all sentences found in the main body of the statement that are identified as relevant to justify the selection of the above label, or a justification if the ``unclear'' label was selected;
    \item Supporting visual element: contains several subfields that should be filled with 1) text found in relevant visual elements that also support the above label (if found in a format that allows direct extraction), 2) the page where these elements are found, and 3) the type of elements that were found (figures or tables);
    \item Scanned: a binary flag indicating whether relevant information was found in a ``scanned'' (i.e. embedded) format, for example in an image where the text cannot be copied;
    \item No supporting information: a binary flag indicating whether any information was found to justify the ``no'' label when it is used;
    \item Fully validated: a binary flag indicating whether our team has fully validated the annotations for this question, thus indicating whether the statement is part of a ``gold'' set or not.
\end{itemize}

Questions related to the presence of a signature or an approval have an extra ``date'' field which is filled with a signature or approval date (if available). The question related to the signature also has an extra ``image'' field, which is filled with a binary flag indicating whether the document contains an image of a signature. Lastly, the question related to the approval has an extra ``joint option'' field which is used in the case of joint statements to specify the type of arrangement used between the reporting entities.

Note that some fields (``no supporting information'' and ``scanned'') are currently used solely for data validation and quality assurance purposes. Note also that the yes/no/unclear labels defined above would be used to determine whether companies have meet the Act's requirements, but these are not actually used in our current experiments. This is because these labels do not fully reflect the actual labels assigned by government analysts regarding whether entities have met the requirements of the Act. Hired annotators were instructed to mark ``yes'' for the label as soon as any relevant information was found. In practice, there is no agreed upon threshold for the amount of supporting evidence needed to ensure that a statement meets each Mandatory Criteria. We leave the refinement and evaluation of these labels to future works.

\noindent \textbf{Data split.} See Section~4 (``Splitting training and evaluation data'').

\noindent \textbf{Data statistics.} Our dataset contains:
\begin{itemize}
 \item Text, images, metadata, and raw PDF content for 8,629 modern slavery statements published as of November 2023. These statements were collected from the Australian Modern Slavery Register and processed using open-source and commercial PDF content extractors.
 \item Sentence-level annotations for 5,731 of these statements:
 \begin{itemize}
    \item 5,670 statements published by the start of our annotation process (April 2023) were annotated for three out of our eleven mandatory content questions by hired workers;
    \item 4,657 statements published by April 2023 that are less than 15 pages were also double-annotated for the remaining eight questions by hired workers; and
    \item 100 statements sampled across the entire set were independently annotated for all questions by extensively trained members of our team. Of these, 50 were annotated by a single expert, and the remaining 50 were annotated by a team of three experts.
 \end{itemize}
\end{itemize}

This dataset contains a total of more than 800,000 sentences that are labeled as relevant or irrelevant based on the Mandatory Criteria of the Australian Modern Slavery Act. The compressed size of the entire dataset is roughly 20~GB.

\subsection{Dataset Creation}

\noindent \textbf{Source data.} See Section~4 (``Statement collection process'').

\noindent \textbf{Annotation process.} See Appendix~\ref{sec:appendix:annotations}.

\noindent \textbf{Personal and sensitive information.} The dataset consists exclusively of publicly released statements available on the Australian Modern Slavery Register. As such, it contains no personal or sensitive information. All data included in the dataset are already in the public domain and have been made available for public access by the issuing entities.

\noindent \textbf{Data shift.} Potential data shifts for this dataset should be considered in light of several factors. Firstly, the annotated statements only cover the period from 2019 to 2023, which may not capture evolving practices, changes in corporate reporting standards, or emerging risks (due e.g. to conflicts, natural disasters, or pandemics). Over time, government analysts' interpretation of the Act may also evolve along with their expectations of adequate disclosures, resulting in future statements being evaluated differently. Additionally, it is anticipated that the Australian government will publish improved guidance materials, helping companies better understand their disclosure obligations. As companies become more familiar with these requirements, the quality and consistency of their statements may improve. Finally, while the the requirements set by the Australian Modern Slavery Act closely align with many other existing legislation such as the UK Modern Slavery Act \citep{uk_modern_slavery_act_2015}, the California Transparency in Supply Chains Act \citep{rao2019modern}, or the Canadian Fighting Against Forced Labour and Child Labour in Supply Chains Act \citep{canadian_forced_labour_act}, there are slight differences which could impact the generalizability of models trained on our dataset.

\subsection{Considerations for Using the Data} 

\noindent \textbf{Intended use.} The dataset is intended for researchers and developers to train and evaluate machine learning models that extract relevant information from corporate modern slavery statements. It may also be used for extracting specific details such as signature dates, the type of governing body approving a statement, and the location of relevant infographics or tables.

\noindent \textbf{Social impact of the dataset.} By improving the accuracy and efficiency of identifying mandated disclosures, this dataset can contribute to greater corporate transparency and accountability, helping to combat modern slavery practices. Additionally, the dataset supports the broader goal of fostering responsible business practices and ethical supply chains, potentially leading to better protection of human rights and improved working conditions worldwide.

\noindent \textbf{Known biases.} The dataset has several known biases that should be acknowledged. First, even if there are other legislation that have been enforced for longer, this dataset only includes statements from entities covered by the Australian Modern Slavery Act, limiting its geographic and regulatory scope. Second, while it allows for voluntary reporting, the Act primarily targets large organizations. In consequence, most statements are published by large companies with annual revenues exceeding AU\$100 million. This introduces a bias towards sectors that dominate the Australian economy, such as natural resource extraction. Companies operating in highly regulated industries or those already subject to modern slavery legislation are also likely to provide more comprehensive reports in their first reporting period. In contrast, companies newly required to examine their supply chains and assess modern slavery risks may have less to report initially. Lastly, while the annotation specifications were meticulously designed to minimize subjectivity and adhere closely to the Act and guidance materials, the process still involves human judgment from annotators and analysts, which can introduce variability and bias.

\noindent \textbf{Limitations.} See Section~6 of the paper and Appendix~\ref{sec:appendix:limitations}.

\noindent \textbf{Citation guidelines.} Withheld for anonymity; will be specified at the camera-ready deadline.

\section{Annotation Process}  %%%% DO NOT MOVE ME! I NEED TO STAY HERE!
\label{sec:appendix:annotations}  %  should always stay "APPENDIX D"

\subsection{Annotation Guidelines}
\label{sec:appendix:annotations:guidelines}
\begin{figure}[H]
\centering
\begin{tcolorbox}[title={Text extraction and labeling workflow for C2 (``supply chains'')}]
Does the reporting entity describe its supply chains?
\newline\newline
$\rightarrow$ \textbf{Yes}, the statement describes the supply chains of the reporting entity: 
\begin{itemize}
    \item Copy-paste the text passages from the statement that justify that the reporting entity described its supply chains.
    \item If any relevant information comes in other formats than text, fill in the required information in the ``Visual Element'' fields: note the page where the information is found, and extract any relevant text (if possible).
\end{itemize}
$\rightarrow$ \textbf{No}, the statement does not describe the reporting entity’s supply chains:
\begin{itemize}
    \item Copy-paste the exact text passages from the statement that justifies that the entity does not meet this criterion,  OR
    \item If no information is found about this criterion, set the ``No relevant information found'' flag.
\end{itemize}
$\rightarrow$ \textbf{Unclear}, in any other case:
\begin{itemize}
    \item Select this label if the information found is unclear or there are other concerns. 
    \item If you decide to select this label, you have to provide an explanation that justifies your decision as supporting text.
\end{itemize}
\end{tcolorbox}
\caption{Workflow used for supporting text extraction and labeling for C2 (``supply chains'').}
\label{fig:appendix:annotations:workflow}
\end{figure}

%\textbf{The Annotation Specification Document: } Building the annotated dataset involved careful consideration of how to extract the relevant information for each criterion. Manual annotation was preferred to ensure high-quality and precise data, as previous attempts with automated labeling functions in Project AIMS Phase 1 did not yield satisfactory results. Additionally, efforts to repurpose the WikiRate dataset were unsuccessful\cite{tfs2022projectaims}, further highlighting the need for a manual approach. As such, one important aspect of the project was creating annotation specifications, which will be made available on the camera-ready deadline. The document serves as an extensive guide for annotators labeling the modern slavery statements. 
We provide a copy of our annotation specifications document as supplementary material with this appendix. This document contains guidelines for hired workers to annotate statements according to our eleven questions on the Mandatory Criteria of the Act (listed in Section~2 of the paper). It includes detailed instructions on handling non-contiguous text, intricate formatting, sections with embedded text, headings, and dates. Following the general guidelines, we outline the eleven questions related to the Mandatory Criteria and how to address them. Each of the first six Mandatory Criteria is associated with a question; for example, for C1, we ask which entities covered by the statement are the ``reporting entities''. Exceptions were made for C2 and C4, as these criteria encompass multiple disclosure topics. Specifically, C2 is divided into three questions covering the descriptions of operations, governance structure, and supply chains, while C4 is split into two questions addressing the descriptions of remediation actions and risk mitigation actions. We did not include a direct question for C7 (``any other relevant information'') due to its subjective nature. Instead, we request that any relevant information be extracted in response to the appropriate questions. We note that this criterion was also omitted in the Australian Government's annual analysis report \citep{australiangovernment2022}. Besides, all instructions and questions are accompanied by numerous examples based on real statements. %Furthermore, the document includes a glossary and supporting annexes with examples of organizational diagrams and details a data processing pipeline.
%The length of the final document is much longer than the law document itself. This is not surprising, as there is ambiguity in the Act itself, and our specifications act as guidelines that a layperson can follow to accurately annotate modern slavery statements.

% \begin{tcolorbox}[title= Figure X Annotation Specification Questions Derived from Mandatory Criteria of the Law]
% \textbf{The Australian Modern Slavery Act asks companies to meet seven mandatory criteria, which require them to:}	

% Ensure that the statement is approved by the board. 	
% \begin{itemize}
%     \item Our question: is the statement approved by the entity's principal governing body?
% \end{itemize}
% That the statmeent is signed by a responsible member of the organization. 	
% \begin{itemize}
%     \item Our question: is the statement signed by a responsible member of the reporting entity?
% \end{itemize}
% Mandatory Criterion 1: The statement clearly identifies the Reporting Entity. 	
% \begin{itemize}
%     \item Our question: does the statement clearly identify which entities covered by the statement are the relevant reporting entities?
% \end{itemize}
% Mandatory Criterion 2: describe the reporting entity’s structure, operations, and supply chains. 	
% \begin{itemize}
%     \item Our question: does the reporting entity describe its structure?
%     \item Our question: does the reporting entity describe its operations?
% \end{itemize}
% \begin{itemize}
%     \item Our question: does the reporting entity describe its supply chains?
% \end{itemize}
% Mandatory Criterion 3: describe the risks of modern slavery practices in the operations and supply chains of the reporting entity and any entities it owns or controls. 	
% \begin{itemize}
%     \item Our question: does the reporting entity describe the modern slavery risks it identified?
% \end{itemize}
% Mandatory Criterion 4: describe the actions taken by the reporting entity and any entities it owns or controls to assess and address these risks, including due diligence and remediation processes. 	
% \begin{itemize}
%     \item Our question:does the reporting entity describe the actions applied to identify, assess, and mitigate the modern slavery risks it identified?
%     \item Our question: does the reporting entity describe remediation actions for modern slavery cases?
% \end{itemize}

% Mandatory Criterion 5: describe how the reporting entity assesses the effectiveness of these actions. 

% \begin{itemize}
%     \item Our question: does the reporting entity describe how it assesses the effectiveness of its actions?
% \end{itemize}
% Mandatory Criterion 6: describe the process of consultation with any entities the reporting entity owns or controls; and 	
% \begin{itemize}
%     \item Our question: does the reporting entity describe how it consulted on its statement with any entities it owns or controls?
% \end{itemize}
% Mandatory Criterion 7: provide any other relevant information.	
% \begin{itemize}
%     \item Not included
% \end{itemize}
% \end{tcolorbox}

For each question, the annotators are presented with a labeling workflow; an example is given in Figure~\ref{fig:appendix:annotations:workflow} for C2 (``supply chains''). Recognizing that ambiguous, vague, and distracting sentences can sometimes be challenging to assess, we provide annotators with the option to answer a question with an ``unclear'' label. This helped us understand confusing cases and improve our instructions during early iterations on the guidelines. %In such cases, the project manager of the annotation company would review the unclear classes; if they couldn't resolve them, some unclear cases were discussed with us.[TODO: ADD STATS?]
Ultimately, only a very limited number of ``unclear'' labels were obtained in the final annotated dataset, and these are not considered in our experiments.

In Figure \ref{fig:fictitious example} we present a highly simplified fictitious example of an annotated statement for the proposed tasks and labels, offering readers a clearer high-level overview. However, we strongly encourage readers to consult the full annotation specification document attached to this paper, which contains real examples and highlights the complexity of the task.

\begin{figure}
    \centering
    \includegraphics[width=1.15\linewidth]{figures/tyragain_v2.pdf}
    \caption{Example of a fictitious modern slavery statement with sentence-level annotations. Sentences are highlighted based on their relevance to different criteria, as determined by annotators. Sentences that are not highlighted are considered irrelevant for all criteria. In our actual dataset, the statements are typically much longer and often contain sentences that are relevant to multiple criteria simultaneously.}
    \label{fig:fictitious example}
\end{figure}


\subsection{Contracting and Quality Assurance Details}
\label{sec:appendix:annotations:contract_and_qa}

We contacted and evaluated several companies offering professional annotation services, and shortlisted two of them for a potential contract. A crucial requirement for our project was that the chosen company must agree to clauses on legal, ethical, and best practice obligations (covering procurement practices, subcontracting and sub-funding, modern slavery, and diversity), ensuring fair compensation and treatment for the annotators. Another key element was for the company to ensure that it has a solid quality assurance process in place and a good annotation platform for PDF files. Following the initial assessment, quotation, and agreement on collaboration terms, we chose one of the two withheld companies. Based on the analysis of the selected company's payment structure and operational details, we strongly believe that the participants were fairly compensated. The annotation team consists of management and senior annotators combined with hired annotators that were primarily university students and graduates. These annotators were hired following thorough background checks and interviews. The payment structure for the work allowed us to estimate that the company was paid at least USD\$18 per hour of annotation. Even after deducting the company's costs, it is estimated that the annotators receive a fair wage. We have contacted the company to get a better wage estimate for the camera-ready version of the paper. %This conclusion is further supported by the strong contractual agreement and the smooth collaboration with the company, including numerous meetings with the annotation team leaders.

The annotation specifications were created by a multidisciplinary team, including experts in machine learning, business, human rights, modern slavery, and in the annotation process. Once the initial version of the specifications was finalized, it was tested multiple times by our team until no general patterns of errors were identified. The specifications document was then sent to the professional annotation company which tested it independently and validated it on a small sample of annotations. Afterward, it was sent back to the expert team for validation. If significant patterns of errors were identified, the annotation specification was reviewed and updated, and the entire process was repeated. This occurred with questions related to Approval, Signature, and Criterion 1, where we had to re-annotate approximately 1000 statements.

The internal quality assurance process of the contracted company includes selective recruitment, comprehensive training for annotators, and dedicated project managers. At various stages of the annotation process, random sampling is conducted to verify the reliability and consistency of annotations. Annotators are also given unseen documents from a testing set at different intervals to check if they remain consistent. Additionally, in cases of double-annotated statements, annotators work independently without seeing each other's work. If the Inter-Annotator Agreement (IAA) is below a specified threshold for those statement, a third annotator steps in to correct the answers. Combined with regular communication and feedback on weekly samples, this process ensures a level of confidence in the quality of the annotated dataset.

\subsection{Decisions and Observations}
\label{sec:appendix:annotations:decisions}

During the creation of the annotation specifications, we documented essential decisions and observations that may influence future studies and experiments. Key points that are considered limitations are discussed in Appendix~\ref{sec:appendix:limitations}; here, we discuss other noteworthy points. %These considerations are divided into two main groups: general and those related to each mandatory criterion.

%\textbf{We do not give directives on how to handle past/future information based on reporting periods.} While we tested a Supplementary Button "Time of action", we realised it is hard to validate that all the actions described by the companies took place only within the relevant “reporting period”. We only found very few companies that separate clearly their actions by the reporting period. As such, we decided not to include this in our analysis. % COVERED IN LIMITATIONS

\textbf{Annotators are instructed to never extract section titles or headers.} This means that if the section title itself provides supporting evidence or context, it will still not be extracted. This is sometimes problematic: for example, Criterion 1 (``reporting entity'') evidence is often presented in a section titled ``Reporting Entity''. In those cases, annotators extract sentences from that section containing company names, but that often do not explicitly identify those companies as ``reporting''. This may lead to confusion under the \emph{no-context} experiment setup. Ignoring section titles is however necessary, as they often do not accurately reflect the content of the paragraphs they precede. For example, a section titled ``Supply Chains'' might primarily discuss operations or risks, which could mislead annotators if they rely on the heading rather than thoroughly reading the paragraphs. This also helps avoid the concatenation of section titles with sentences when copy-pasting text from the PDF files, which would be a challenging problem to solve.

\textbf{Statements are expected to be self-contained.} Only text within the statements can be considered: annotators are instructed to NEVER follow URLs or other documents cited in the statements. In consequence,  annotators also cannot always ascertain whether the right ``governing bodies'' are providing approval, whether the right individuals are providing signatures, or whether all owned or controlled entities are included in the statement due to a lack of external context.

\textbf{Statements are expected to be understandable by a layperson.} While we provided a glossary of key terms in the annotation specifications, we do not ask annotators to search for information on specific business or legal terms, on existing legislation or legal frameworks, or on risk assessment tools. We expect the statement issuers to use clear terminology and avoid terminology that may be misleading.

%\textbf{All tasks assigned to the annotators are based on questions under the form of “is this kind of information present}”. We do not want the annotators to make decisions on whether a company has fulfilled a specific requirement of the Modern Slavery Act directly; they should instead extract text from the statement that contains the information expected by the Act, or identify that the statement does not contain such text. This simplifies the annotation process by eliminating potentially subjective decisions.  % COVERED IN LIMITATIONS

%\textbf{Some statements, although not scanned, may not allow text extraction.} Example of a document from which we cannot extract any relevant text: the PDF is not “scanned” per se, but we cannot select normal text. For this we added a Supplementary Button: “Information found in a scanned format”.

\textbf{Statement types indicated in the Modern Slavery Register are not reliable}. This metadata is likely provided by the statement issuer, but may be incorrect. Specifically: ``joint'' statements can sometimes be presented by only one reporting entity, and ``normal'' statements can be issued by a parent entity and cover many of its owned/controlled entities.

%\textbf{It can be unclear whether some mentioned entities are “forced to report”, “voluntarily reporting”, or “irrelevant for the statement”.} For example, in \href{https://modernslaveryregister.gov.au/statements/3841/}{\#3841}, it is unclear which entities are the reporting ones; the statement indicates that one of the named entities is not a reporting entity under the law… should it be treated as a voluntary reporting entity? Should it be excluded from the supporting text?

%\textbf{Visual elements with sparse information are hard to extract properly. } To correct this, we asked the annotators to select the Supplementary Button: “Visual Element Category” and extract, if possible, the information, as well as provide the page number where they found this infographic.

%\textbf{\textbf{It is hard to define how much information must be provided by each company. }Large companies are for example expected to provide much more information on their supply chains and operations than small companies, but this also varies based on the type of business and its activities (e.g. software development companies may have fewer things to describe than construction companies). In our annotation specification, providing a clear definition of “how much is enough” for a yes/no label seemed impossible; therefore, we decided to have annotators label with “yes” for tasks where any non-zero amount of relevant information is found.}
%\textbf{It is not clear how much information is “sufficient” to meet the requirements.} The guidance documentation from the government ([to do add refernce]) does not define a “minimum bar” or exact “requirements” for the information that must be provided by reporting entities in their statements. This means that having yes/no labels for “is the company meeting this criterion” is difficult, as it is an ambiguous and subjective decision to make, and one that annotators probably should.
%\textbf{The annotation company applied an ad hoc rule to correct for misidentified joint statements on the registry.} As indicated earlier in this document, the “normal” or “joint” statement types indicated in the Modern Slavery Register are not reliable. When delivering the annotations for Signature, Approval, C1 on October 10th 2023, the annotation company mentioned:

%\textit{If a normal labeled statement had a document that identified 2 reporting entities clearly using words "reporting entities" there is a note saying "Joint statement".}
%This might be insufficient as there might be joint statements that don't use the words "reporting entities" but which contain text such as “this is the MSA statement for companies A, B and C…”. We decided to accept this limitation imposed by the annotation company, as we cannot clearly know whether the statement is joint or normal.

\textbf{The ``principal governing body'' of an entity is often implicitly defined.} Identifying whether a statement is correctly approved is therefore challenging when dealing with multinational corporations with complex structures, or in the case of trusts. Also, in joint statements, seemingly independent entities can have the same board members, and this rarely mentioned in statements.

\textbf{Only the most relevant mentions of ``reporting entities'' are extracted.} This is specific to the question related to Mandatory Criterion 1: we decided to extract only the most obvious declarations. This is done to avoid having to exhaustively extract each sentence where an entity is named, as this approach does not scale well to large statements.

%\textbf{Mandatory Criterion 2:} This criterion was split into three distinct questions to check separately whether the company provides information about its structure, operations, and supply chains.

\textbf{Arrangements with suppliers do not describe operations.} This is in contradiction with the Australian government's guidance material \citep[see Table~2~of][]{amsa2018_guidance2023}. Specifically, we consider that ``explaining in general terms the type of arrangements the entity has with its suppliers and the way these are structured'' is vague, hard to convey to annotators, and relates more to descriptions of suppliers or supply chains. We found that annotation quality improved following this decision.

\textbf{The ``structure'' of an entity is a vague concept.} A reporting entity may for example describe its management and governance structure (e.g. naming executives or members of its board of directors), while another might focus more on its organizational structure (e.g. naming parent companies, subsidiaries, and affiliates). The latter is usually understood to be more relevant, but the Australian government also considers, for example, Australian Business Number (ABN) and registered office location to be relevant information \citep{amsa2018_guidance2023} while making no mention of the importance of capital structure, reporting structure, or taxation structure descriptions. Classifying information on shareholders is also difficult, as it may sometimes be relevant when few shareholders have significant control over the reporting entity. Lastly, we note that descriptions of ``brick-and-mortar'' locations (e.g. facilities, stores) are often provided as descriptions of structure by companies, but this is instead considered relevant for operations.

%Stating the acquisition of other companies is not regarded as structural information in our annotation specifications. %Although this is not explicitly mentioned in the specification, it is implied through an example (2.4.3.b, REECE). Describing a trend of acquisitions would likely be categorized under operations, as it suggests that it is an activity of the entity.
%Declaring who the shareholder(s) of a company are is not considered structure information. In cases where a company has only one shareholder, this may be problematic, as that shareholder might technically have a controlling interest. Generally, holding shares is instead seen as having a “financial investment”, and therefore it is related to operations.

\textbf{The number of workers is considered structure information.} According to the Australian government's guidance material \citep{amsa2018_guidance2023}, this information may be relevant for both structure and operations. However, for simplicity and clarity, we considered it only relevant for structure in our guidelines to annotators.

%Descriptions of “brick-and-mortar” locations (facilities, stores, …) have been moved to operations. This is aligned with the guidance document, but not necessarily aligned with what our intuition might be for “structure”. This is based on the idea that structure only refers to the legal and organizational structure of the entity.

\textbf{Descriptions of customers are not relevant for supply chains.} In reality, customers can be considered as part of the ``downstream'' supply chain of an entity, but we do not consider this information relevant in our guidelines. The Australian government's guidance material \citep{amsa2018_guidance2023} also mentions that entities are not required to report this information. However, the distribution of products or services to customers is considered a relevant activity (or operation).

%Entities are not required to describe how their customers are using their services or products. Beyond that, the MSA guidance says that \textbf{risks} associated with how customers use the services or products of the entity \textbf{are not included in the scope of the statements for the MSA}, meaning so they are NOT relevant for C3/C4/C5. However, the description of the supply chain of the entity may include a description of clients (i.e. in the “downstream” part of the supply chain) as well as of how they use the services or products of the entity. That would be relevant for C2 (supply chain).

\textbf{Risks and actions may not always apply to owned or controlled entities.} Specifically, Mandatory Criteria 3, 4, and 5 require entities to provide information about risks and actions that apply to ``the reporting entity and any entities it owns or controls.'' However, based on consultations with the Australian Attorney General's Department and annotation experts, we decided that if a description of risks or actions only seem to apply to the reporting entity, this information is still considered relevant. We initially decided to have a separate data field to flag information that would also apply to owned and controlled entities, but we determined during testing that it was rarely used; it was eventually removed from labeling workflows.  

%Criterion four was split into two distinct questions to check separately whether the company provides information about mitigation and remediation actions.

\textbf{Owned or controlled entities might not always be consulted.} Due to ambiguities and the lack of external context, it is difficult to determine whether the list of owned and controlled entities perfectly overlaps with the list of ``consulted'' entities. Although Mandatory Criterion 6 requires reporting entities to consult with all entities they own or control, there are also various reasons why they might not be able to do so. Some of those entities may, for example, be dormant, inactive, or non-trading. Furthermore, only consultation ``on the preparation of the statement'' is considered relevant for this criterion, but reporting entities rarely describe their actual consultation process.
 
%\textbf{Mandatory Criterion 7: }As illustrated in Figure X, criterion 7 was excluded from the analysis. We considered it too broad, and it was impractical to leave the determination of what the company might have deemed relevant information to the annotations. Additionally, providing an exhaustive list of examples was not feasible. Therefore, this criterion would have been too costly and inefficient. Furthermore, we noted that this criterion was also omitted in the AGD's annual report analysis \cite{australiangovernment2022}. Consequently, we chose to exclude it from our analysis as well.

\textbf{Statement signatures are sometimes difficult to interpret.} For example, large statements often contain a ``message from the CEO'' with general comments on the importance of the statement or on the achievements of their company. These message are often signed, but it is unclear if that signature applies to the whole statement, or just to that message. Documents may also occasionally lack the actual image of a signature, or may only include a blank space or a box where a signature is supposed to be. Such cases are still considered valid evidence, as the image of the signature is not necessary, but the intent to sign is acknowledged.

%it can be unclear whether a signature applies to the entire content of a statement or just to the page where there is a “message from the CEO”. %It is ideal when the signature is near the approval text, mentioned directly in the approval text, or at the end of the statement. 

%[repetition]It can be unclear whether a signature is made by a “responsible member” of the reporting entity. For example, in \href{https://modernslaveryregister.gov.au/statements/11945/}{\#11945}, if a “Company Secretary” signs the statement, is that enough? %Would that be a “no” for signature? (Example: TODO) What if there was no box, but the person’s name is typed where a signature would typically be? (Example: \href{https://modernslaveryregister.gov.au/statements/7741/}{\#7741}, \href{https://modernslaveryregister.gov.au/statements/10859/}{\#10859})

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Limitations}  %%%% DO NOT MOVE ME! I NEED TO STAY HERE!
\label{sec:appendix:limitations}  %  should always stay "APPENDIX E"

We concluded the paper by highlighting some of the key limitations of our dataset (Section~6). Among these, the most significant challenge is the subjective and noisy nature of the relevant sentence annotation process. Although our guidelines for annotators were designed to minimize subjectivity and maximize consistency, the Inter-Annotator Agreement (IAA), as shown in Table~1 of the paper, varies significantly across different questions. Based on qualitative analyses of the annotated data, we believe that the IAA is not an ideal measure of annotation quality. Good IAA scores were observed in some statements where a significant amount of relevant information was missed by annotators and where obviously relevant information was correctly extracted. Initially, we set high thresholds for expected IAA scores with the annotators, but we later encouraged lower IAA scores for statements deemed more difficult to annotate. This approach aimed to promote the extraction of more potentially relevant text. Ultimately, we believe that modeling approaches capable of handling noisy labels and leveraging annotator disagreements as an additional supervision signal may lead to more effective solutions for sentence relevance classification.

A somewhat subjective annotation process can also introduce bias in the labeling of disclosures, potentially leading to unfair assessments of whether certain companies (or those operating in specific industrial sectors) meet the requirements of the Act. This bias might result from individual annotators' interpretations of the guidelines or their preconceived notions about particular industries. To mitigate this risk, we consulted with experts in the design of our annotation guidelines, aiming to minimize any disadvantage to specific businesses, and relied on the professionalism of the annotation company and their internal QA process to vouch for their work. Furthermore, for transparency and to allow for external review and improvement, we make both the annotations and the guidelines publicly available.

The extraction of text from PDFs poses other significant challenges. Beyond the difficulty of correctly extracting text from embedded figures and tables, matching any sentence annotated by a human to the automatically extracted text from the PDF is also complex. This difficulty arises due to text fragmentation, OCR errors, non-ASCII character mismatches, and out-of-order parsing. In practice, we found that using ABBYY FineReader, a commercial software with an OCR engine, reduced the match rate for annotated sentences compared to using PyMuPDF (fitz), which lacks an OCR engine, even when employing a Levenshtein sentence matching approach. Revisiting the text extraction and matching methodology, potentially replacing regular expressions with a more advanced method for determining sentence boundaries and matching them, would likely enhance the reliability of evaluations for relevant text classification models.

As for the challenge of differentiating past and future information in our dataset, one potential solution is to introduce temporal labels, where markers indicating whether the information pertains to past actions, ongoing activities, or future plans would be added to annotations. Language models could be employed to automatically infer these markers from the text, reducing the re-annotation burden and providing scalability.

Experiments for single-sentence classification with API-based language models with large context windows can be wasteful due to the high number of model requests required, significantly increasing costs. Future works might want to explore the simultaneous classification of multiple sentences at once, such as paragraph-by-paragraph, to reduce the number of model requests. This approach would however necessitate more substantial prompt engineering and output parsing efforts. Additionally, a hierarchical context processing approach, which involves structuring the input to provide broader context on the statement before drilling down to specific sentence-level details, could be worth investigating for both zero-shot and supervised learning settings.

% Besides, some key metadata provided by the Modern Slavery Register~\cite{amsa2018_register}, including entity names and whether a statement is a joint statement ( e.g., it covers multiple reports entities), can be unreliable or missing. Although this affects only a small proportion of statements, it means annotators cannot rely on this information to improve their understanding of the issuing entity and of the expected governing body. This issue may also affect the quality of our statement clusters.

% Despite the requirement that all AMSR statements must be validated by the AusGov for approval and signature, we have identified some that lack board approval or signatures. The AU government asserts that all statements on the registry are signed and approved, and if not, they are returned to the company for correction. Nevertheless, statements are still published even if the company does not comply with other mandatory criteria. Although the AusGov reviews all statements annually to present summary statistics in the report to parliament, they do not provide that data.

% link to the ideas-and-reminders document: https://docs.google.com/document/d/1QjOsHoSNrSeIWk6Frt_3J6WY561GT0Yjrl9mdJ4dnYs/edit

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Implementation and Experimentation Details}  %%%% DO NOT MOVE ME! I NEED TO STAY HERE!
\label{sec:appendix:impl_and_exp_details}  %  should always stay "APPENDIX F"

Details on the models we selected as baselines for our experiments are presented in Table~\ref{tab:appendix:impl_and_exp_details:model_info}. In addition to the experimentation details presented in Section~5 of the paper (Benchmark Experiments), we report that the models are fine-tuned with a cross-entropy loss using the Adam optimizer and without a learning rate scheduler. Each model is trained for 24 hours on a A100L GPU, with the exception of \llama{}, which is trained for 48 hours to allow the model more time to converge. In the case of \llama{}, a batch size of 32 is simulated using gradient accumulation, where the real batch size is set to 2 and the gradient is accumulated over 16 steps. All the fine-tuning is conducted in 16-bit mixed precision mode. For DistilBERT and BERT, we attach a classification head directly to the CLS token positioned at the beginning of the target sentence for both the \emph{no-context} and \emph{with-context} setups. For \llama{} and \llamathree{}, we use the last token as is typically done with other causal models. In the zero-shot case, we used the default temperature of 0.6 for \llamathree{}; in the GPT model cases, the default temperature means that "the model will use log probability to automatically increase the temperature until certain thresholds are hit" (from OpenAI API reference page).
% see https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/blob/main/generation_config.json
% see https://platform.openai.com/docs/api-reference/audio/createTranscription

For training data preparation, the pre-extracted statement text is split into sentences with various amounts of context at training time. These sentences are then shuffled and assembled into minibatches using a fixed-size sentence buffer (containing up to 8192 sentences). We assign a positive relevance label to any extracted sentence that matches a sentence tagged by an annotator as being relevant, and assign a negative relevance label otherwise. The matching of extracted and tagged sentences is done following text cleanups using regular expressions, and by considering perfect matches, partial matches, and noisy matches based on the Levenshtein distance.

\begin{table}[H]
    \small
  \caption{Baseline model details. For BERT and DistilBERT, full model weights are fine-tuned, and for \llama{} and \llamathree{}, we use the LoRA approach \citep{hu2021lora}, resulting in a smaller number of trainable parameters. The \textsuperscript{*} suffix denotes zero-shot models.}
  \vspace{1mm}
  \label{tab:appendix:impl_and_exp_details:model_info}
  \centering
  \begin{tabular}{llcc}
    %\toprule
        Model  name   & URL & \makecell{Total\\params} & \makecell{Trainable\\params} \\
    \midrule
        DistilBERT &
            \footnotesize \href{https://huggingface.co/distilbert/distilbert-base-uncased}{https://huggingface.co/distilbert/distilbert-base-uncased} &
            66.8M &
            66.8M \\
        BERT &
            \footnotesize \href{https://huggingface.co/google-bert/bert-base-uncased}{https://huggingface.co/google-bert/bert-base-uncased} &
            109M &
            109M \\
        \llama{} &
            \footnotesize \href{https://huggingface.co/NousResearch/Llama-2-7b-hf}{https://huggingface.co/NousResearch/Llama-2-7b-hf}  &
            6.6B &
            4.2M \\
        \llamathree{} &
            \footnotesize \href{https://huggingface.co/meta-llama/Llama-3.2-3B}{https://huggingface.co/meta-llama/Llama-3.2-3B}  &
            3.2 B &
            2.3 M \\
    \midrule
        \gptthree{}\textsuperscript{*} &
            \footnotesize \href{https://platform.openai.com/docs/models/gpt-3-5-turbo}{https://platform.openai.com/docs/models/gpt-3-5-turbo} &
            ? & 
            - \\
        \gptfour{}\textsuperscript{*} &
            \footnotesize \href{https://platform.openai.com/docs/models/gpt-4o}{https://platform.openai.com/docs/models/gpt-4o} &
            ? &
            - \\
        \llamathree{}\textsuperscript{*} &
            \footnotesize \href{https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct}{https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct}  &
            3.2 B &
            - \\
    %\bottomrule
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Prompt Design and Examples}  %%%% DO NOT MOVE ME! I NEED TO STAY HERE!
\label{sec:appendix:prompt_design_and_ex}  %  should always stay "APPENDIX G"

To develop the final version of the prompt, we began with preliminary tests using a small set of five PDFs. These initial documents were excluded from the final analysis to avoid any potential contamination. The prompt development process incorporated a variety of resources, including raw PDFs, extracted text, a complete annotation specification document, a summary cheat sheet, and annotated examples. This iterative approach involved refining the prompts based on manual evaluations conducted by a domain expert in modern slavery reporting, while also accounting for constraints such as token limits and computational costs.
 Version 1 focused on classifying sentences using raw PDFs and relevant text from the annotation specification. Version 2 incorporated both the PDFs and the full annotation specification document. Version 3 experimented with subsets of the annotation specification, cheat sheet, and examples. Version 4 shifted to using extracted text instead of raw PDFs. Finally, Version 5 involved optimizing prompt text using ChatGPT, aiming to generate outputs that included labels and justifications, supported by examples from the annotation specification. Each iteration was refined to achieve a balance between accuracy and efficiency, following best practices on
how to formulate intents, how to provide domain definitions, and how to constrain desired outputs.

We present in Figures~\ref{fig:appendix:prompt_design_and_ex:prompt_template_no_context} and~\ref{fig:appendix:prompt_design_and_ex:prompt_template_with_context} the exact prompt templates we used for the \emph{no-context} and \emph{with-context} setups for zero-shot model experiments. Note that the \texttt{TARGET\_SENTENCE} and \texttt{SENTENCE\_IN\_CONTEXT} placeholders are respectively substituted with the target sentence to classify and the same sentence with surrounding context in actual model prompts. For an example of a target sentence that would be classified along with its context, see Figure~\ref{fig:appendix:prompt_design_and_ex:example_context}.





\clearpage
\begin{figure}
\centering
\begin{tcolorbox}[colback=gray!10!white, colframe=gray!80!black, title={Prompt template (C2, ``supply chains'', \emph{no-context})}]
You are an analyst that inspects modern slavery declarations made by Australian reporting entities. You are specialized in the analysis of statements made with respect to the Australian Modern Slavery Act of 2018, and not of any other legislation.
\newline\newline
You are currently looking for sentences in statements that describe the SUPPLY CHAINS of an entity, where supply chains refer to the sequences of processes involved in the procurement of products and services (including labour) that contribute to the reporting entity's own products and services. The description of a supply chain can be related, for example, to 1) the products that are provided by suppliers; 2) the services provided by suppliers, or 3) the location, category, contractual arrangement, or other attributes that describe the suppliers. Any sentence that contains these kinds of information is considered relevant. Descriptions that apply to indirect suppliers (i.e. suppliers-of-suppliers) are considered relevant. Descriptions of the supply chains of entities owned or controlled by the reporting entity making the statement are also considered relevant. However, descriptions of 'downstream' supply chains, i.e. of how customers and clients of the reporting entity use its products or services, are NOT considered relevant. Finally, sentences that describe how the reporting entity lacks information on some of its supply chain, or how some of its supply chains are still unmapped or unidentified, are also considered relevant.
\newline\newline
Given the above definitions of what constitutes a relevant sentence, you will need to determine if a target sentence is relevant or not. You must avoid labeling sentences with only vague descriptions or corporate talk (and no actual information) as relevant. The answer you provide regarding whether the sentence is relevant or not can only be 'YES' or 'NO', and nothing else.
\newline\newline
The target sentence to classify is the following:
\newline
------------
\newline
\texttt{TARGET\_SENTENCE}
\newline
------------
\newline
Is the target sentence relevant? (YES/NO)
\end{tcolorbox}
\caption{Prompt template used for zero-shot model experiments under the \emph{no-context} setup.}
\label{fig:appendix:prompt_design_and_ex:prompt_template_no_context}
\end{figure}

\begin{figure}
\centering
\begin{tcolorbox}[colback=gray!10!white, colframe=gray!80!black, title={Prompt template (C2, ``supply chains'', \emph{with-context})}]
You are an analyst that inspects modern slavery declarations made by Australian reporting entities. You are specialized in the analysis of statements made with respect to the Australian Modern Slavery Act of 2018, and not of any other legislation.
\newline\newline
You are currently looking for sentences in statements that describe the SUPPLY CHAINS of an entity, where supply chains refer to the sequences of processes involved in the procurement of products and services (including labour) that contribute to the reporting entity's own products and services. The description of a supply chain can be related, for example, to 1) the products that are provided by suppliers; 2) the services provided by suppliers, or 3) the location, category, contractual arrangement, or other attributes that describe the suppliers. Any sentence that contains these kinds of information is considered relevant. Descriptions that apply to indirect suppliers (i.e. suppliers-of-suppliers) are considered relevant. Descriptions of the supply chains of entities owned or controlled by the reporting entity making the statement are also considered relevant. However, descriptions of 'downstream' supply chains, i.e. of how customers and clients of the reporting entity use its products or services, are NOT considered relevant. Finally, sentences that describe how the reporting entity lacks information on some of its supply chain, or how some of its supply chains are still unmapped or unidentified, are also considered relevant.
\newline\newline
Given the above definitions of what constitutes a relevant sentence, you will need to determine if a target sentence is relevant or not inside a larger block of text. The target sentence will first be provided by itself so you can know which sentence we want to classify. It will then be provided again as part of the larger block of text it originally came from (extracted from a PDF file) so you can analyze it with more context. While some of the surrounding sentences may be relevant according to the earlier definitions, we are only interested in classifying the target sentence according to the relevance of its own content. You must avoid labeling sentences with only vague descriptions or corporate talk (and no actual information) as relevant.
\newline\newline
The answer you provide regarding whether the sentence is relevant or not can only be 'YES' or 'NO', and nothing else.
\newline\newline
The target sentence to classify is the following:
\newline
------------
\newline
\texttt{TARGET\_SENTENCE}
\newline
------------
\newline
The same target sentence inside its original block of text:
\newline
------------
\newline
\texttt{SENTENCE\_IN\_CONTEXT}
\newline
------------
\newline
Is the target sentence relevant? (YES/NO)
\end{tcolorbox}
\caption{Prompt template used for zero-shot model experiments under the \emph{with-context} setup.}
\label{fig:appendix:prompt_design_and_ex:prompt_template_with_context}
\end{figure}

\begin{figure}
\centering
\begin{tcolorbox}[colback=gray!10!white, colframe=gray!80!black, width=\textwidth, title=Target sentence example]
The compliance with these communicated expectations is ensured by regular unannounced audits of all suppliers in this region.
\end{tcolorbox}
\vspace{1mm}
\begin{tcolorbox}[colback=gray!10!white, colframe=gray!80!black, width=\textwidth, title=Target sentence example with 100-word context]
\texttt{[...]} we have established clear and stringent expectations for all our suppliers in Southeast Asia regarding labor practices and ethical standards. These expectations are communicated through detailed supplier agreements and comprehensive training programs. Additionally, we collaborate closely with local communities and stakeholders to promote awareness and understanding of ethical labor practices. \textbf{The compliance with these communicated expectations is ensured by regular unannounced audits of all suppliers in this region.} Furthermore, our commitment to transparency and accountability extends beyond audits, as we engage with independent third-party auditors to validate our findings and ensure the integrity of our supply chains. Any detected non-compliance triggers immediate corrective actions and follow-up reviews, demonstrating our dedication to resolving issues swiftly and \texttt{[...]}
\end{tcolorbox}
\caption{Example of a fictitious sentence to be classified as relevant or irrelevant, with and without context. The amount of context here (roughly 100 words) is the same one used in our experiments. For the question related to C5 (assessing the effectiveness of actions), classifying this sentence is difficult when context is not provided, as it is unclear whose and what expectations were communicated, and whose suppliers are audited. With context, it is clear that the sentence contains relevant information mandated by Mandatory Criteria 5 of the Act.}
\label{fig:appendix:prompt_design_and_ex:example_context}
\end{figure}



\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%\begin{figure}[t]
%    \centering
%    \subfigure[BERT and DistilBERT macro F1.]{
%        \includegraphics[width=0.48\textwidth]{figures/bert_distilbert_val_f1_overtime.pdf}
%        \label{fig:appendix:appendix:val_f1_trend_overtime__bert_distilbert}
%    }
%    \subfigure[\llama{} macro F1.]{
%        \includegraphics[width=0.48\textwidth]{figures/llama2_val_f1_overtime.pdf}
%        \label{fig:appendix:appendix:val_f1_trend_overtime__llama2}
%    }
%    \caption{Macro F1 score over the epochs for the fine-tuned models in the all-label case.}
%    \label{fig:appendix:val_f1_trend_overtime}
%\end{figure}





%\begin{landscape}
%\begin{table}
%\centering
%\caption{Examples of relevant and irrelevant information for questions related to the Mandatory Criteria of the Act.}
%\vspace{1mm}
%\label{tab:appendix:examples}
%\begin{adjustbox}{scale=0.74}
%\begin{tabularx}{26cm}{lXXXXX|}
%\begin{tabular}{lccccc}
    %\toprule
        %\textbf{Question} &
        %\textcolor{darkgreen}{\textbf{Relevant information}} &
        %\textcolor{darkorange}{\textbf{Ambiguous information}} &
        %\textcolor{darkred}{\textbf{Vague information}} &
        %\textcolor{darkred}{\textbf{Distracting information}} \\
    %\midrule
    %\small
        %\textbf{Approval} &

\section{Additional Results}  %%%% DO NOT MOVE ME! I NEED TO STAY HERE!
\label{sec:appendix:add_results}  %  should always stay "APPENDIX H"

%\subsection{Results for All Questions}
%\label{appendix__result_details__all_criteria}

\subsection{F1 evolution over the epochs}
\label{appendix__result_details__f1_evolution}

Figure~\ref{fig:appendix:add_results:val_f1_trend_overtime} illustrates the evolution of fine-tuned model performance, measured by validation Macro F1, during training in the \textit{No context} setup. While BERT and DistilBERT achieve strong performance from the first epoch, \llama{}  requires several epochs to reach comparable levels, with \llamathree{} falling in between, needing only a few epochs to perform well. We hypothesize a trend where larger model sizes require more epochs to achieve optimal performance. Furthermore, we observe that \llama{}  could benefit from extended fine-tuning, as its Macro F1 curve has not plateaued even after 48 hours of training.
%We present in Figure~\ref{fig:appendix:add_results:val_f1_trend_overtime} the evolution of fine-tuned model performance in terms of validation Macro F1 during training under the \emph{No context} setup. While BERT and DistilBERT demonstrate strong performance starting from the first epoch, \llama{} requires several epochs to reach comparable performance, and \llamathree{} is somehow in between, requiring few epochs to reach good performances. We suspect a pattern where the bigger the model size, the more epochs is required to reach good performances.
Additionally, we observe that \llama{} may benefit from extended fine-tuning, as the macro F1 curve has not plateaued even after 48 hours of training.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/val_f1_overtime.pdf}
    \caption{Macro F1 score over the epochs for the fine-tuned models in the all-label case.}
    \label{fig:appendix:add_results:val_f1_trend_overtime}
\end{figure}

\section{Comparison of Modern Slavery Reporting Criteria and Metrics}
\label{sec:appendix:laws} 

Since the enactment of the Australian Modern Slavery Act, various existing laws, such as the UK Modern Slavery Act~\citep{uk_modern_slavery_act_2015}, have been strengthened with more robust reporting requirements, and new legislation has been introduced, such as the Canadian Fighting Against Forced Labour and Child Labour in Supply Chains Act of 2023~\citep{canadian_forced_labour_act}. These laws share overlapping reporting criteria, whether recommended or mandated. To demonstrate how our dataset and annotations could be used to build predictive models that generalize to other legal frameworks, Table~\ref{tab:appendix:comparison} compares the questions in our annotation specifications with the reporting obligations set by the Australian MSA, the UK MSA, and the Canadian legislation. This table also includes metrics used by civil society organizations \citep[specifically, those proposed by][]{walkfree2022garment} to assess modern slavery statements.

Table~\ref{tab:appendix:comparison} highlights areas of overlap and divergence based on text color:

\begin{itemize}
    \item Green sections represent requirements where our existing annotations can be used to train algorithms without any or with minimal modifications.
    \item Orange sections indicate areas that may necessitate the use of a subset of our annotations, additional data mining, or potential adjustments and expansions to our current annotation set.
    \item Red sections highlight where there is no overlap; here, our annotations do not apply and would require complete re-annotation to accommodate these aspects.
\end{itemize}

This comparative analysis underscores the adaptability of our annotation framework and identifies specific areas for enhancement to achieve broader applicability across different legislative contexts, with the potential to also support civil society efforts in their assessments.
\begin{landscape}
\begin{table}
\centering
\caption{Comparison of Modern Slavery Reporting Criteria and Metrics}
\vspace{0.1mm}
\fontsize{7.2}{7.2}\selectfont
\label{tab:appendix:comparison}
%\begin{adjustbox}{width=\textwidth,height=\textheight} 
\begin{adjustbox}{scale=0.87}
\renewcommand{\arraystretch}{3} 
    \begin{tabularx}{26cm}{XXXXX}
    %\hline  % horizontal line at the top
         \textbf{AIMS.au Dataset Annotation Specification Questions} & \textbf{Australian Modern Slavery Act Mandatory Reporting Criteria} & \textbf{UK Modern Slavery Act Reporting Suggestions} & \textbf{Canadian Fighting Against Forced Labour and Child Labour in Supply Chains Act Reporting Obligations} & \textbf{The Walk Free's "Beyond Compliance" Study Metrics} \\
    \hline  % horizontal line after header

    \textcolor{darkgreen}{  Question: Is the statement approved by the entity's principal governing body?} & \textcolor{darkgreen}{Ensure that the statement is approved by the board.} & \textcolor{darkgreen}{Approval from the board of directors (or equivalent management body)} & \textcolor{darkgreen}{Approval by the organization’s governing body.} & \textcolor{darkgreen}{MSA Statement Approval} \\
    %\hline

         \textcolor{darkgreen}{Question: Is the statement signed by a responsible member of the reporting entity?} & \textcolor{darkgreen}{The statement is signed by a responsible member of the organization. }&\textcolor{darkgreen}{ Signature from a director (or equivalent) or designated member }&\textcolor{darkgreen}{ Signature of one or more members of the governing body of each entity that approved the report.} & \textcolor{darkgreen}{MSA Statement Signed} \\
    %\hline

        \textcolor{darkgreen}{ Question: Does the statement clearly identify which entities covered by the statement are the relevant reporting entities?}  & \textcolor{darkgreen}{Mandatory Criterion 1: The statement clearly identifies the Reporting Entity.} & \textcolor{darkred}{N/A} & \textcolor{darkred}{N/A } & \textcolor{darkred}{N/A} \\
    %\hline

         \textcolor{darkgreen}{Question: Does the reporting entity describe its structure? \newline Question: Does the reporting entity describe its operations? \newline Question: Does the reporting entity describe its supply chains?} & \textcolor{darkgreen}{Mandatory Criterion 2: Describe the reporting entity’s structure, operations, and supply chains.} & \textcolor{darkgreen}{The organisation’s structure, business and supply chains.} & \textcolor{darkgreen}{Description of the organisation’s structure, activities and supply chains.} & \textcolor{darkgreen}{ MSA Organizational structure and operations \newline MSA Supply Chain Disclosure}\\
    %\hline

         \textcolor{darkgreen}{Question: Does the reporting entity describe its modern slavery risks?} & \textcolor{darkgreen}{Mandatory Criterion 3: Describe the risks of modern slavery practices in the operations and supply chains of the reporting entity and any entities the reporting entity owns or controls.} & \textcolor{darkgreen}{Risk assessment and management.} & \textcolor{darkgreen}{Description of the parts of its business and supply chains that carry a risk of forced labour or child labour being used and the steps it has taken to assess and manage that risk.} & \textcolor{darkgreen}{MSA Identification of Risks} \\

    %\hline

         \textcolor{darkgreen}{Question: Does the reporting entity describe the actions applied to identify, assess, and mitigate the modern slavery risks it identified?} & \textcolor{darkgreen}{ Mandatory Criterion 4: Describe the actions taken by the reporting entity and any entities it owns or controls to assess and address these risks, including due diligence and remediation processes.} & \textcolor{darkorange}{ Description of the organisation’s policies in relation to slavery and human trafficking. \newline Description of the organisation’s due diligence processes in relation to slavery and human trafficking in its business and supply chains. \newline Description of the parts of the organisation’s business and supply chains where there is a risk of slavery and human trafficking taking place, and the steps it has taken to assess and manage that risk. \newline The training and capacity building about slavery and human trafficking available to its staff.} & \textcolor{darkorange}{  Description of the organisation’s policies and due diligence processes in relation to forced labour and child labour. \newline Description of the parts of organisation’s activities and supply chains that carry a risk of forced labour or child labour being used and the steps it has taken to assess and manage that risk. \newline The training provided to employees on forced labour and child labour.} & \textcolor{darkorange}{ MSA Policy \newline MSA Risk assessment \newline MSA Risk management \newline MSA Whistleblowing Mechanism  \newline MSA Training } \\
    %\hline

         \textcolor{darkgreen}{ Question: Does the reporting entity describe remediation actions for modern slavery cases?} & \textcolor{darkgreen}{ Mandatory Criterion 4: Describe the actions taken by the reporting entity and any entities it owns or controls to assess and address these risks, including due diligence and remediation processes.}& \textcolor{darkgreen}{The organisation should paint a detailed picture of all the steps it has taken to address and remedy modern slavery, and the effectiveness of all such steps.} & \textcolor{darkgreen}{Description of any measures taken to remediate any forced labour or child labour.} & \textcolor{darkgreen}{MSA Incidents Remediation } \\
    %\hline

         \textcolor{darkgreen}{Question: Does the reporting entity describe how it assesses the effectiveness of its actions?} & \textcolor{darkgreen}{Mandatory Criterion 5: Describe how the reporting entity assesses the effectiveness of these actions.} & \textcolor{darkgreen}{Description of the organisation’s effectiveness in ensuring that slavery and human trafficking is not taking place in its business or supply chains, measured against such performance indicators as it considers appropriate. \newline The organisation should paint a detailed picture of all the steps it has taken to address and remedy modern slavery, and the effectiveness of all such steps. }& \textcolor{darkgreen}{ Description of how the entity assesses its effectiveness in ensuring that forced labour and child labour are not being used in its business and supply chains.} & \textcolor{darkgreen}{MSA Performance Indicators} \\
    %\hline

         \textcolor{darkgreen}{Question: Does the reporting entity describe how it consulted on its statement with any entities it owns or controls? } & \textcolor{darkgreen}{Mandatory Criterion 6: Describe the process of consultation with any entities the reporting entity owns or controls.} & \textcolor{darkred}{N/A} & \textcolor{darkred}{N/A} & \textcolor{darkred}{N/A} \\
    %\hline

         \textcolor{darkred}{N/A} & \textcolor{darkred}{Mandatory Criterion 7: Provide any other relevant information.} & \textcolor{darkred}{N/A} & \textcolor{darkred}{Any measures taken to remediate the loss of income to the most vulnerable families that results from any measure taken to eliminate the use of forced labour or child labour in its activities and supply chains.} & \textcolor{darkred}{MSA Impact on Company Behaviour \newline MSA Business Performance Indicators\newline MSA Historic Record} \\
    %\hline  % horizontal line at the bottom
    \end{tabularx}
\end{adjustbox}
\end{table}
\end{landscape}



\end{document}

