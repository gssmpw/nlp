\section{Benchmark Experiments}
\label{sec:experiments}

In this section, we outline our experimental setup and present the results of benchmarking various models for detecting sentences relevant to the mandatory reporting requirements of the Act. We evaluate the performance of these models under both zero-shot and fine-tuning settings to assess their effectiveness in extracting mandated information from statements. We then analyze the results to identify key insights and potential areas for improvement.  % , and strategies for the future development of models that determine whether companies are meeting the requirements of modern slavery legislation  THIS LAST PART IS KEPT FOR THE CONCLUSION

\textbf{Task definition.} Our proposed dataset includes a variety of labels that models could predict; these labels are detailed in \appendixDatasetLabels{}. For conciseness and clarity, we focus on a task that we believe will be of greatest interest to the machine learning community: predicting relevant or irrelevant labels according to our eleven questions. We frame this task as a sentence-level binary classification problem which we evaluate across the eleven questions using F1, Precision, and Recall metrics. We selected these metrics over accuracy because they allow us to identify cases where models simply learn to predict all sentences as irrelevant, since those are over-represented in our dataset (see Figure~\ref{fig:data:annotation_stats:relevant_ratios}). %For experiments based on multiple criteria, we also provide Macro aggregation scores so that all criteria contribute the same to the final score.

For the statements that are double annotated by hired workers, we adopt a ``union'' label combination strategy, where a sentence is considered relevant if any annotator marks it as such. This approach addresses the possibility that individual annotators may have missed relevant text in some statements. We suggest that future works explore more sophisticated methods for leveraging annotator disagreements as a supervision signal. For our current experiments, models are evaluated exclusively using the subsets of ``gold'' annotated statements. Since these gold sets contain high-quality annotations, their smaller size (roughly 7000 sentences each) with respect to the overall dataset size should not significantly impact the reliability of model evaluations. Furthermore, this approach helps us, as well as future researchers, avoid incurring significant API usage costs when using state-of-the-art, closed-source language models for large-scale evaluations. % on a subset of statements annotated by external workers.

\textbf{Evaluated models.} We conduct our experiments using a range of language models that includes three open models --- DistilBERT~\cite{sanh2020distilbert}, BERT~\cite{devlin2019bert} and \llama{}~\cite{touvron2023llama} --- and two closed models, namely OpenAI's \gptthree{} and \gptfour{} (see \appendixImplDetails{} for more details). We use the OpenAI and \llama{} models to evaluate zero-shot (prompt-based) approaches, and we compare them with DistilBERT, BERT, and \llama{} models trained directly on statements annotated by hired workers. Our experiments are structured based on two input data setups: in the first (\emph{no-context} setup), models only have access to the target sentence being classified; in the second (\emph{with-context} setup), we provide additional context by including up to 100 words balanced before and after the target sentence (see \appendixPromptDesign{} for an example). This dual input setup allows us to assess the impact of contextual information on model performance. We also evaluate models based on two criteria setups: models either predict labels across all of our Mandatory Criteria questions, or only for the question related to supply chains (which is part of Criteria~2). This latter setup is representative of the dataset in terms of average difficulty, and is used due to the limitations of our current OpenAI API usage tier which prevents us from running large-scale experiments across all questions with \gptthree{} and \gptfour{} in time for submission. Note that we plan on completing these experiments following the submission of this preprint.

The ``open'' models (DistilBERT, BERT, and \llama{}) are fine-tuned from self-supervised pre-training checkpoints available on the HuggingFace repository~\cite{wolf2019huggingface}. For DistilBERT and BERT, we fine-tune the full model weights, while for \llama{}, we use the LoRA approach~\cite{hu2021lora} to manage computation costs. All experiments are conducted on a A100 GPU with 40~GB memory using PyTorch. Token sequence lengths are capped at 512 for DistilBERT and BERT, and at 150 for \llama{}, due to memory limitations. Models are trained with a batch size of 96 for DistilBERT, 64 for BERT, and 32 for \llama{}, using Adam~\cite{kingma2014adam} with a fixed learning rate (0.00003). We select model checkpoints that maximize the Macro F1-score across all criteria or the C2 (``supply chains'') F1-score for the single-question setup. Links to the model pages and checkpoint names are provided in \appendixImplDetails{}.

\textbf{Prompt design for zero-shot experiments.} Experiments with \gptthree{} and \gptfour{} are conducted using a prompt template designed specifically for C2 (``supply chains'') and given in \appendixPromptDesign{}. This template was designed following best practices on how to formulate intents, how to provide domain definitions, and how to constrain desired outputs~\cite{ekin2023prompt}. The definitions provided in the prompt are taken from the Act and its guidance document~\cite{amsa2018_legislation, amsa2018_guidance2023}, and are essentially a condensed version of the instructions given to the annotators. We leave the exploration of more sophisticated prompts, or very large prompts that may include multiple examples or even our entire annotation specifications document, for future works.

\subsection{Results}

\begin{table}[t]
    \centering
    \caption{Single-question evaluation results (for \ctwo{}), with and without context.}
    \vspace{2mm}
    \label{tab:experiments:results:c2supplychains}
    \begin{tabular}{lcccccc}
        %\toprule
        \multirow{2}{*}[-1mm]{\textbf{Model}} & \multicolumn{3}{c}{\textbf{No context}} & \multicolumn{3}{c}{\textbf{With context}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7}
                     & F1    & Precision & Recall & F1    & Precision & Recall \\
        \midrule
        DistilBERT   & 0.727 & 0.731     & 0.724  & 0.763 & 0.806     & 0.724 \\
        BERT         & 0.743 & 0.718     & 0.770  & 0.773 & 0.851     & 0.709 \\
        \llama{}     & 0.737 & 0.808     & 0.678  & -     & -         & -     \\
        \midrule
        \gptthree{}  & 0.317 & 0.195     & 0.837  & 0.174 & 0.096     & 0.996 \\
        \gptfour{}   & 0.543 & 0.464     & 0.656  & 0.556 & 0.412     & 0.852 \\
        %\bottomrule
    \end{tabular}
\end{table}

Table~\ref{tab:experiments:results:c2supplychains} presents the evaluation results for the single-question (C2, ``supply chains'') setting, both with and without context. Note that \llama{} results are unavailable with context due to computational constraints, as the longer input sequence would necessitate substantial token truncation. Across both the \emph{no-context} and \emph{with-context} setups, fine-tuned models consistently outperform zero-shot approaches, with the performance gap between these two model families being substantial.
%According to the LMSYS leaderboard~\cite{chiang2024chatbot} as of May 2024, \gptfour{} is the top-performing language model across various tasks. While its technical details are undisclosed, we can infer that \gptfour{} is considerably more complex than BERT or \llama{}. Here, \gptfour{} narrows the performance gap with fine-tuned models and outperforms \gptthree{}.
The fine-tuned models exhibit comparable performance, though there remains room for improvement. Precision and recall are generally balanced, except for \gptthree{}, which appears to classify nearly all sentences as relevant. Comparing results with and without context, we observe performance improvements for all models except \gptthree{}, possibly due to input sequence length exceeding the model's limitations. Future research should explore techniques to prepare or leverage context more effectively.
\begin{figure}[t]
\begin{center}
\vspace{2mm}
\includegraphics[width=\textwidth]{figures/results__all_labels.pdf}
\caption{F1 scores obtained by fine-tuned models trained jointly on the eleven relevant content questions, along with their overall (``Macro'') F1 performance. See \appendixAddResults{} for detailed results.}
\label{fig:experiments:results:all}
\end{center}
\end{figure}
We present evaluation results for all fine-tuned models on the full eleven-question setting in Figure~\ref{fig:experiments:results:all}, with detailed results in Appendix~\ref{sec:appendix:add_results}. We note again that zero-shot models are not presented here because each question requires a separate prompt, and in consequence the total number of requests to \gptthree{} or \gptfour{} exceeds the limits of our current OpenAI API usage tier. We note that the result trends are similar as in the previous \ctwo{} case, where tested models performed similarly, with \llama{} being slightly behind BERT and DistilBERT. We observe again, in general, that models provided with context outperform the no-context ones. Interestingly, results are visibly worse for \llama{} on the questions related to Criteria~5 (``effectiveness'') and~6 (``consultation''). We attribute this to the use of a suboptimal fine-tuning optimization strategy, chosen due to computational budget constraints, combined with the challenges posed by these two criteria, particularly the inherent difficulty of identifying their relevant text.



One final insight we emphasize is that, based on the presented results and our preliminary prompt engineering experiences, it is challenging to find prompts for zero-shot models that can match the performance of fine-tuned models.
%This contrasts strongly with the \emph{seemingly} straightforward nature of the task: determining whether a single sentence contains a relevant disclosure.
This highlights the necessity for high-quality, curated datasets like \dataset{} to allow for the reliable training and evaluation of language models. Additionally, this underscores the need for further exploration into the importance of context at various scales and the impact of vague and distracting text on large language models.
