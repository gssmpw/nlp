\documentclass{article}

\PassOptionsToPackage{sort,numbers}{natbib}

\usepackage[preprint,anonymous]{neurips_data_2024}
%\usepackage[final]{neurips_data_2024} % ready for submission

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{makecell}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{array}
\usepackage{multirow}
\usepackage[most]{tcolorbox}
\usepackage{pdflscape}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{float}

\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
\definecolor{darkred}{rgb}{0.5, 0.0, 0.0}
\definecolor{darkorange}{rgb}{0.8, 0.33, 0}

\input{macros}

\title{Appendix for the Submitted Preprint:\\
AIMS.au: A Dataset for the Analysis of Modern Slavery Countermeasures in Corporate Statements}

\begin{document}

\maketitle

\appendix

\setcounter{figure}{3} % based on main paper content
\setcounter{table}{2} % based on main paper content

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data Availability and Maintenance Strategy}  %%%% DO NOT MOVE ME! I NEED TO STAY HERE!
\label{sec:appendix:data_availability}  %  should always stay "APPENDIX A"

The dataset is currently being finalized and is not yet available to the public. However, it will be made accessible in time for the camera-ready deadline. At that point, download links for the dataset along with evaluation scripts, Python classes for data loading, and baseline experiment configuration files will be available in a dedicated GitHub repository. This repository will also be linked to a Digital Object Identifier (DOI) to ensure easy reference and citation.

We will make the dataset available in two formats: HDF5~\cite{hdf5repo} and Activeloop DeepLake~\cite{hambardzumyan2022deep}. The HDF5 format is widely used across various domains and programming languages due to its versatility and efficiency in handling large volumes of data. The Activeloop DeepLake format, on the other hand, offers features specifically tailored for machine learning experimentation, including optimized PyTorch dataloaders, which facilitate seamless integration with machine learning workflows. Both formats are open data formats, promoting accessibility and ease of use. The dataset will be packaged so that it directly contains raw PDF data as well as all metadata from the Australian Modern Slavery Register which may be useful for future studies. The content of the dataset is detailed in Appendix~\ref{sec:appendix:dataset_description} in the data card style of \cite{gehrmann2021gem, suzgun2024harvard}.

The dataset will be hosted on Figshare~\cite{figshare}, an online open access repository, ensuring that it is freely available to the research community. By leveraging Figshare's robust infrastructure, we aim to provide a reliable and persistent platform for dataset access. To promote widespread use and proper attribution, the dataset will be licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) license. This license permits unrestricted use, distribution, and reproduction in any medium, provided the original authors and source are credited.

The initial release of the dataset will contain all statements processed by hired annotators as well as our  ``gold'' validation set. We may withhold the release of the ``gold'' test set until 2025 in order to hold a model competition. Details and deadlines will be shared on our project's GitHub page.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Examples of Disclosures}  %%%% DO NOT MOVE ME! I NEED TO STAY HERE!
\label{sec:appendix:text_examples}  %  should always stay "APPENDIX B"

In developing the annotation guidelines, our goal was to assist annotators in identifying concrete supporting evidence in statements. This was necessary as despite legislative mandates for specific disclosures, companies often provide vague, ambiguous, or distracting information that obstructs effective monitoring and progress. %Corporations are frequently criticized for using such language in their modern slavery statements, which leads to a focus on complying with the letter of the law rather than its spirit. Furthermore, even experts may interpret legislative requirements differently and have varying opinions on the relevance of ambiguous language depending on the context, adding to the complexity of the task.
Table~\ref{tab:appendix:examples} provides, for all our questions related to the Mandatory Criteria of the Act, fictitious examples of: 1) relevant information; 2) irrelevant information due to ambiguity (i.e. due to a lack of context); 3) irrelevant information due to vagueness (i.e. unacceptable no matter the context); and 4) distracting information. These examples are inspired by the contents of real statements and highlight the significant challenge of distinguishing between relevant and irrelevant information.

%Our dataset tackles the significant challenge of distinguishing between language that could be considered as supporting evidence or not. Even when the questions are precise and clear (e.g., does the company describe its supply chains?), the table illustrates the range of possible answers from company statements. Therefore, we believe that this data reflects real-world complexities and will aid in developing models to address the limitations of current models in navigating and interpreting nuanced information. This, in turn, pushes models to handle ambiguity and go beyond simple classifications.

%For AI researchers, this dataset offers a range of challenging tasks ideal for evaluating existing language models. It provides a unique opportunity to explore technical difficulties inherent in modern slavery statements, such as noisy labels, vague reporting, and complex legal terminology. The dataset is particularly valuable due to its challenging nature, characterized by vague and distracting text and the substantial context required to understand the most complicated statements.

% TODO: highlight that a vague or ambiguous sentence can become clearer (and thus relevant) given adequate context, but this context is not always close to the target sentence itself

\begin{landscape}
\begin{table}
\centering
\caption{Examples of relevant and irrelevant information for questions related to the Mandatory Criteria of the Act.}
\vspace{1mm}
\label{tab:appendix:examples}
\begin{adjustbox}{scale=0.74}
\begin{tabularx}{26cm}{lXXXXX|}
%\begin{tabular}{lccccc}
    \toprule
        \textbf{Question} &
        \textcolor{darkgreen}{\textbf{Relevant information}} &
        \textcolor{darkorange}{\textbf{Ambiguous information}} &
        \textcolor{darkred}{\textbf{Vague information}} &
        \textcolor{darkred}{\textbf{Distracting information}} \\
    \midrule
    \small
        \textbf{Approval} &
        "This statement was approved by our principal governing body (our board) on March 15th, 2023." &
        "The ethics board approved the publication of this statement." &
        "Approval was received for this statement." &
        "Our code of conduct was approved by the board." \\
        \textbf{C1 (reporting entity)} &
        "ABC Corporation Ltd., ABN 123 456 789 is the reporting entity for this statement." &
        (Company logo on the first page) &
        "This statement applies to numerous entities across our larger corporate family." &
        "Founded in 1980, X Corp. has a long history as a reporting entity in various jurisdictions." \\
        \textbf{C2 (operations)} &
        "Our operations include the manufacturing of lawnmower parts in Asia and their distribution in Australia." &
        "We are a leader service provider in our sector." &
        "We operate globally." &
        "We produced 10,000 units last year, achieving a 15\% increase in productivity." \\
        \textbf{C2 (structure)} &
        "ABC Corporation has a hierarchical governance structure with over 1000 employees." &
        “This statement covers a number of wholly-owned subsidiaries.” &
        "Our organization has a global structure leadership model." &
        "Here is the organizational chart for 2020 showing the department heads." \\
        \textbf{C2 (supply chains)} &
        "Our supply chain includes raw materials such as timber, which is procured via suppliers in Southeast Asia." &
        "We may procure sensitive goods from higher-risk countries." &
        "We sometimes contract other companies for services." &
        "Our downstream supply chain distributes our products to over 10,000 customers." \\
        \textbf{C3 (risk description)} &
        "Areas in our supply chains with a higher risk of modern slavery include outsourced services such as cleaning, catering, security and facilities management, and use of labor hire contractors." &
        "An assessment concluded that we have a low risk of modern slavery." &
        “Modern slavery has the potential to exist in the technology sector.” &
        “We understand and have mapped our businesses risks with an extensive assessment strategy.” \\
        \textbf{C4 (remediation)} &
        "We established a remediation fund for affected workers and provided support services." &
        “We understand the importance of workers knowing their rights and we will directly address violations when needed." &
        "Remediation actions are a key priority for us." &
        “We deeply believe in the need for concrete remedies when cases are discovered, and the common industry practice is to terminate any contract with faulty suppliers.” \\
        \textbf{C4 (risk mitigation)} &
        "In this reporting period, we have made progress in implementing our Modern Slavery Policy and have updated our Whistleblowing Policy." &
        “We have established a zero-tolerance approach towards modern slavery.” &
        "We have made sure that our suppliers comply with our policies." &
        “We are committed to maintaining the highest level of integrity and honesty throughout all aspects of our business.” \\
        \textbf{C5 (effectiveness)} &
        "We use key performance indicators (KPIs) to measure how effective our actions are, and determined that our 123 employees (100\%) were present at five modern slavery training sessions this year." &
        %"Our related policies were reviewed this year." &
        "We conducted a review of our practices and spent time evaluating actions over the past year." &
        “Our team has spent time reflecting on our activities to enhance our approach.” &
        "As part of our annual review process, we have also gathered and analyzed feedback from customer surveys." \\
        \textbf{C6 (consultation)} &
        "We engaged and consulted with all companies we own or control in the development of this statement and regarding the policies we plan to enact." &
        "Our statement is the result of a comprehensive review process that engaged stakeholders from within our corporate family." &
        "We do not need to consult externally in the preparation of this statement." &
        "Our statement reflects a collaborative effort that draws from various perspectives within our organization." \\
        \textbf{Signature} &
        "This statement is signed by Jane Doe in her role as the managing director of Unicorn Pharmaceuticals on 21 November 2020." &
        "Signed by John Doe, the company secretary of the Trustee." &
        "Signed by Jane Doe (21 November 2020)." &
        "Our company executives have all signed off on our modern slavery policies." \\
\bottomrule
%\end{tabular}
\end{tabularx}
\end{adjustbox}
\end{table}
\end{landscape}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{AIMS.au Data Card}  %%%% DO NOT MOVE ME! I NEED TO STAY HERE!
\label{sec:appendix:dataset_description}  %  should always stay "APPENDIX C"

% inspired by: https://arxiv.org/abs/2207.04043

\subsection{Dataset Description}

\textbf{Dataset summary.} See Section~4 of the paper.

\noindent \textbf{Languages.} The dataset contains English text only. 

\noindent \textbf{Domain.} Long, freeform statements made by corporate entities.

\noindent \textbf{Additional details.} The dataset contains modern slavery statements originally published in PDF format by Australian corporate entities between 2019 and 2023, metadata for those statements, and annotations (labels) provided by hired workers and ourselves. Additional unannotated statements published over the same period and beyond are also packaged in the dataset as supplementary data for unsupervised learning experiments.

\noindent \textbf{Motivation.} We publish this dataset to support the development and evaluation of machine learning models for extracting mandated information from corporate modern slavery statements. Our aim is to facilitate research in this domain and foster future efforts to assess companies' compliance with the Australian Modern Slavery Act and other similar legislation.

\subsection{Meta Information}

\noindent \textbf{Dataset curators.} Withheld for anonymity; will be specified here at the camera-ready deadline. % The dataset was prepared by the authors themselves and... (+ roles?)

\noindent \textbf{Point of contact.} Withheld for anonymity; will be specified here at the camera-ready deadline. % Adriana? also refer to GitHub URL?

\noindent \textbf{Licensing.} The dataset is released under the Creative Commons Attribution 4.0 International (CC BY 4.0) license.

\noindent \textbf{Funding sources.} Withheld for anonymity; will be specified in the paper's acknowledgments at the camera-ready deadline.

\subsection{Dataset Structure}

\noindent \textbf{Data format and structure.} We structure our dataset so that one ``instance'' corresponds to a single statement. Each statement is associated with a unique identifier, a PDF file, and a set of twelve metadata fields, all provided by the Australian Modern Slavery Register. These metadata fields are:
\begin{itemize}
    \item Annual revenue;
    \item Countries where headquartered;
    \item Covered entities;
    \item Industry sectors;
    \item Overseas obligations;
    \item Reporting period end date;
    \item Reporting period start date;
    \item Publication date;
    \item Publication year in the register;
    \item Submission date;
    \item Associated trademarks;
    \item Statement type (normal or joint).
\end{itemize}

The PDFs are freeform, allowing reporting entities the flexibility to choose their format; some use a brochure-style layout, while others incorporate extensive background images or unique design elements. In addition to the provided metadata, we enhance these statements with several annotated fields, filled by our hired annotators or ourselves. These fields capture critical information such as compliance with reporting requirements and supporting content, as detailed in the next few paragraphs.

\noindent \textbf{Data preparation.} See Section~4 (``Conversion of text into sentences'') for information on text extraction. Following this step, we combine the raw PDF data (for researchers that intend on extracting the PDF contents themselves), its metadata, the extracted text (which, for ABBYY FineReader, includes the position of the text inside PDF pages and the OCR confidence levels), and the annotated fields into a single data archive. This archive is based on the Activeloop DeepLake format~\cite{hambardzumyan2022deep} by default, and we provide a script to convert the dataset into HDF5 format.

\noindent \textbf{Annotated fields.} As detailed in Section~4 (``Development of the annotation specifications''), we translated the seven Mandatory Criteria of the Act into eleven questions. The questions are detailed in Appendix~\ref{sec:appendix:annotations}, and are tied to a set of fields to be filled by annotators based on their answers. Specifically, the fields shared by all questions are:
\begin{itemize}
    \item Label (yes/no/unclear): specifies whether the reporting entity has provided information that is relevant for the targeted criterion;
    \item Supporting text: contains all sentences found in the main body of the statement that are identified as relevant to justify the selection of the above label, or a justification if the ``unclear'' label was selected;
    \item Supporting visual element: contains several subfields that should be filled with 1) text found in relevant visual elements that also support the above label (if found in a format that allows direct extraction), 2) the page where these elements are found, and 3) the type of elements that were found (figures or tables);
    \item Scanned: a binary flag indicating whether relevant information was found in a ``scanned'' (i.e. embedded) format, for example in an image where the text cannot be copied;
    \item No supporting information: a binary flag indicating whether any information was found to justify the ``no'' label when it is used;
    \item Fully validated: a binary flag indicating whether our team has fully validated the annotations for this question, thus indicating whether the statement is part of a ``gold'' set or not.
\end{itemize}

Questions related to the presence of a signature or an approval have an extra ``date'' field which is filled with a signature or approval date (if available). The question related to the signature also has an extra ``image'' field, which is filled with a binary flag indicating whether the document contains an image of a signature. Lastly, the question related to the approval has an extra ``joint option'' field which is used in the case of joint statements to specify the type of arrangement used between the reporting entities.

Note that some fields (``no supporting information'' and ``scanned'') are currently used solely for data validation and quality assurance purposes. Note also that the yes/no/unclear labels defined above would be used to determine whether companies have meet the Act's requirements, but these are not actually used in our current experiments. This is because these labels do not fully reflect the actual labels assigned by government analysts regarding whether entities have met the requirements of the Act. Hired annotators were instructed to mark ``yes'' for the label as soon as any relevant information was found. In practice, there is no agreed upon threshold for the amount of supporting evidence needed to ensure that a statement meets each Mandatory Criteria. We leave the refinement and evaluation of these labels to future works.

\noindent \textbf{Data split.} See Section~4 (``Splitting training and evaluation data'').

\noindent \textbf{Data statistics.} Our dataset contains:
\begin{itemize}
 \item Text, images, metadata, and raw PDF content for 8,629 modern slavery statements published as of November 2023. These statements were collected from the Australian Modern Slavery Register and processed using open-source and commercial PDF content extractors.
 \item Sentence-level annotations for 5,731 of these statements:
 \begin{itemize}
    \item 5,670 statements published by the start of our annotation process (April 2023) were annotated for three out of our eleven mandatory content questions by hired workers;
    \item 4,657 statements published by April 2023 that are less than 15 pages were also double-annotated for the remaining eight questions by hired workers; and
    \item 100 statements sampled across the entire set were independently annotated for all questions by extensively trained members of our team. Of these, 50 were annotated by a single expert, and the remaining 50 were annotated by a team of three experts.
 \end{itemize}
\end{itemize}

This dataset contains a total of more than 800,000 sentences that are labeled as relevant or irrelevant based on the Mandatory Criteria of the Australian Modern Slavery Act. The compressed size of the entire dataset is roughly 20~GB.

\subsection{Dataset Creation}

\noindent \textbf{Source data.} See Section~4 (``Statement collection process'').

\noindent \textbf{Annotation process.} See Appendix~\ref{sec:appendix:annotations}.

\noindent \textbf{Personal and sensitive information.} The dataset consists exclusively of publicly released statements available on the Australian Modern Slavery Register. As such, it contains no personal or sensitive information. All data included in the dataset are already in the public domain and have been made available for public access by the issuing entities.

\noindent \textbf{Data shift.} Potential data shifts for this dataset should be considered in light of several factors. Firstly, the annotated statements only cover the period from 2019 to 2023, which may not capture evolving practices, changes in corporate reporting standards, or emerging risks (due e.g. to conflicts, natural disasters, or pandemics). Over time, government analysts' interpretation of the Act may also evolve along with their expectations of adequate disclosures, resulting in future statements being evaluated differently. Additionally, it is anticipated that the Australian government will publish improved guidance materials, helping companies better understand their disclosure obligations. As companies become more familiar with these requirements, the quality and consistency of their statements may improve. Finally, while the the requirements set by the Australian Modern Slavery Act closely align with many other existing legislation such as the UK Modern Slavery Act~\cite{uk_modern_slavery_act_2015}, the California Transparency in Supply Chains Act~\cite{rao2019modern}, or the Canadian Fighting Against Forced Labour and Child Labour in Supply Chains Act~\cite{canadian_forced_labour_act}, there are slight differences which could impact the generalizability of models trained on our dataset.

\subsection{Considerations for Using the Data} 

\noindent \textbf{Intended use.} The dataset is intended for researchers and developers to train and evaluate machine learning models that extract relevant information from corporate modern slavery statements. It may also be used for extracting specific details such as signature dates, the type of governing body approving a statement, and the location of relevant infographics or tables.

\noindent \textbf{Social impact of the dataset.} By improving the accuracy and efficiency of identifying mandated disclosures, this dataset can contribute to greater corporate transparency and accountability, helping to combat modern slavery practices. Additionally, the dataset supports the broader goal of fostering responsible business practices and ethical supply chains, potentially leading to better protection of human rights and improved working conditions worldwide.

\noindent \textbf{Known biases.} The dataset has several known biases that should be acknowledged. First, even if there are other legislation that have been enforced for longer, this dataset only includes statements from entities covered by the Australian Modern Slavery Act, limiting its geographic and regulatory scope. Second, while it allows for voluntary reporting, the Act primarily targets large organizations. In consequence, most statements are published by large companies with annual revenues exceeding AU\$100 million. This introduces a bias towards sectors that dominate the Australian economy, such as natural resource extraction. Companies operating in highly regulated industries or those already subject to modern slavery legislation are also likely to provide more comprehensive reports in their first reporting period. In contrast, companies newly required to examine their supply chains and assess modern slavery risks may have less to report initially. Lastly, while the annotation specifications were meticulously designed to minimize subjectivity and adhere closely to the Act and guidance materials, the process still involves human judgment from annotators and analysts, which can introduce variability and bias.

\noindent \textbf{Limitations.} See Section~6 of the paper and Appendix~\ref{sec:appendix:limitations}.

\noindent \textbf{Citation guidelines.} Withheld for anonymity; will be specified at the camera-ready deadline.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Annotation Process}  %%%% DO NOT MOVE ME! I NEED TO STAY HERE!
\label{sec:appendix:annotations}  %  should always stay "APPENDIX D"

\subsection{Annotation Guidelines}
\label{sec:appendix:annotations:guidelines}

%\textbf{The Annotation Specification Document: } Building the annotated dataset involved careful consideration of how to extract the relevant information for each criterion. Manual annotation was preferred to ensure high-quality and precise data, as previous attempts with automated labeling functions in Project AIMS Phase 1 did not yield satisfactory results. Additionally, efforts to repurpose the WikiRate dataset were unsuccessful\cite{tfs2022projectaims}, further highlighting the need for a manual approach. As such, one important aspect of the project was creating annotation specifications, which will be made available on the camera-ready deadline. The document serves as an extensive guide for annotators labeling the modern slavery statements. 
We provide a copy of our annotation specifications document as supplementary material with this appendix. This document contains guidelines for hired workers to annotate statements according to our eleven questions on the Mandatory Criteria of the Act (listed in Section~2 of the paper). It includes detailed instructions on handling non-contiguous text, intricate formatting, sections with embedded text, headings, and dates. Following the general guidelines, we outline the eleven questions related to the Mandatory Criteria and how to address them. Each of the first six Mandatory Criteria is associated with a question; for example, for C1, we ask which entities covered by the statement are the ``reporting entities''. Exceptions were made for C2 and C4, as these criteria encompass multiple disclosure topics. Specifically, C2 is divided into three questions covering the descriptions of operations, governance structure, and supply chains, while C4 is split into two questions addressing the descriptions of remediation actions and risk mitigation actions. We did not include a direct question for C7 (``any other relevant information'') due to its subjective nature. Instead, we request that any relevant information be extracted in response to the appropriate questions. We note that this criterion was also omitted in the Australian Government's annual analysis report~\cite{australiangovernment2022}. Besides, all instructions and questions are accompanied by numerous examples based on real statements. %Furthermore, the document includes a glossary and supporting annexes with examples of organizational diagrams and details a data processing pipeline.
%The length of the final document is much longer than the law document itself. This is not surprising, as there is ambiguity in the Act itself, and our specifications act as guidelines that a layperson can follow to accurately annotate modern slavery statements.

% \begin{tcolorbox}[title= Figure X Annotation Specification Questions Derived from Mandatory Criteria of the Law]
% \textbf{The Australian Modern Slavery Act asks companies to meet seven mandatory criteria, which require them to:}	

% Ensure that the statement is approved by the board. 	
% \begin{itemize}
%     \item Our question: is the statement approved by the entity's principal governing body?
% \end{itemize}
% That the statmeent is signed by a responsible member of the organization. 	
% \begin{itemize}
%     \item Our question: is the statement signed by a responsible member of the reporting entity?
% \end{itemize}
% Mandatory Criterion 1: The statement clearly identifies the Reporting Entity. 	
% \begin{itemize}
%     \item Our question: does the statement clearly identify which entities covered by the statement are the relevant reporting entities?
% \end{itemize}
% Mandatory Criterion 2: describe the reporting entity’s structure, operations, and supply chains. 	
% \begin{itemize}
%     \item Our question: does the reporting entity describe its structure?
%     \item Our question: does the reporting entity describe its operations?
% \end{itemize}
% \begin{itemize}
%     \item Our question: does the reporting entity describe its supply chains?
% \end{itemize}
% Mandatory Criterion 3: describe the risks of modern slavery practices in the operations and supply chains of the reporting entity and any entities it owns or controls. 	
% \begin{itemize}
%     \item Our question: does the reporting entity describe the modern slavery risks it identified?
% \end{itemize}
% Mandatory Criterion 4: describe the actions taken by the reporting entity and any entities it owns or controls to assess and address these risks, including due diligence and remediation processes. 	
% \begin{itemize}
%     \item Our question:does the reporting entity describe the actions applied to identify, assess, and mitigate the modern slavery risks it identified?
%     \item Our question: does the reporting entity describe remediation actions for modern slavery cases?
% \end{itemize}

% Mandatory Criterion 5: describe how the reporting entity assesses the effectiveness of these actions. 

% \begin{itemize}
%     \item Our question: does the reporting entity describe how it assesses the effectiveness of its actions?
% \end{itemize}
% Mandatory Criterion 6: describe the process of consultation with any entities the reporting entity owns or controls; and 	
% \begin{itemize}
%     \item Our question: does the reporting entity describe how it consulted on its statement with any entities it owns or controls?
% \end{itemize}
% Mandatory Criterion 7: provide any other relevant information.	
% \begin{itemize}
%     \item Not included
% \end{itemize}
% \end{tcolorbox}

For each question, the annotators are presented with a labeling workflow; an example is given in Figure~\ref{fig:appendix:annotations:workflow} for C2 (``supply chains''). Recognizing that ambiguous, vague, and distracting sentences can sometimes be challenging to assess, we provide annotators with the option to answer a question with an ``unclear'' label. This helped us understand confusing cases and improve our instructions during early iterations on the guidelines. %In such cases, the project manager of the annotation company would review the unclear classes; if they couldn't resolve them, some unclear cases were discussed with us.[TODO: ADD STATS?]
Ultimately, only a very limited number of ``unclear'' labels were obtained in the final annotated dataset, and these are not considered in our experiments.

\begin{figure}[H]
\centering
\begin{tcolorbox}[title={Text extraction and labeling workflow for C2 (``supply chains'')}]
Does the reporting entity describe its supply chains?
\newline\newline
$\rightarrow$ \textbf{Yes}, the statement describes the supply chains of the reporting entity: 
\begin{itemize}
    \item Copy-paste the text passages from the statement that justify that the reporting entity described its supply chains.
    \item If any relevant information comes in other formats than text, fill in the required information in the ``Visual Element'' fields: note the page where the information is found, and extract any relevant text (if possible).
\end{itemize}
$\rightarrow$ \textbf{No}, the statement does not describe the reporting entity’s supply chains:
\begin{itemize}
    \item Copy-paste the exact text passages from the statement that justifies that the entity does not meet this criterion,  OR
    \item If no information is found about this criterion, set the ``No relevant information found'' flag.
\end{itemize}
$\rightarrow$ \textbf{Unclear}, in any other case:
\begin{itemize}
    \item Select this label if the information found is unclear or there are other concerns. 
    \item If you decide to select this label, you have to provide an explanation that justifies your decision as supporting text.
\end{itemize}
\end{tcolorbox}
\caption{Workflow used for supporting text extraction and labeling for C2 (``supply chains'').}
\label{fig:appendix:annotations:workflow}
\end{figure}

\subsection{Contracting and Quality Assurance Details}
\label{sec:appendix:annotations:contract_and_qa}

We contacted and evaluated several companies offering professional annotation services, and shortlisted two of them for a potential contract. A crucial requirement for our project was that the chosen company must agree to clauses on legal, ethical, and best practice obligations (covering procurement practices, subcontracting and sub-funding, modern slavery, and diversity), ensuring fair compensation and treatment for the annotators. Another key element was for the company to ensure that it has a solid quality assurance process in place and a good annotation platform for PDF files. Following the initial assessment, quotation, and agreement on collaboration terms, we chose one of the two withheld companies. Based on the analysis of the selected company's payment structure and operational details, we strongly believe that the participants were fairly compensated. The annotation team consists of management and senior annotators combined with hired annotators that were primarily university students and graduates. These annotators were hired following thorough background checks and interviews. The payment structure for the work allowed us to estimate that the company was paid at least USD\$18 per hour of annotation. Even after deducting the company's costs, it is estimated that the annotators receive a fair wage. We have contacted the company to get a better wage estimate for the camera-ready version of the paper. %This conclusion is further supported by the strong contractual agreement and the smooth collaboration with the company, including numerous meetings with the annotation team leaders.

The annotation specifications were created by a multidisciplinary team, including experts in machine learning, business, human rights, modern slavery, and in the annotation process. Once the initial version of the specifications was finalized, it was tested multiple times by our team until no general patterns of errors were identified. The specifications document was then sent to the professional annotation company which tested it independently and validated it on a small sample of annotations. Afterward, it was sent back to the expert team for validation. If significant patterns of errors were identified, the annotation specification was reviewed and updated, and the entire process was repeated. This occurred with questions related to Approval, Signature, and Criterion 1, where we had to re-annotate approximately 1000 statements.

The internal quality assurance process of the contracted company includes selective recruitment, comprehensive training for annotators, and dedicated project managers. At various stages of the annotation process, random sampling is conducted to verify the reliability and consistency of annotations. Annotators are also given unseen documents from a testing set at different intervals to check if they remain consistent. Additionally, in cases of double-annotated statements, annotators work independently without seeing each other's work. If the Inter-Annotator Agreement (IAA) is below a specified threshold for those statement, a third annotator steps in to correct the answers. Combined with regular communication and feedback on weekly samples, this process ensures a level of confidence in the quality of the annotated dataset.

\subsection{Decisions and Observations}
\label{sec:appendix:annotations:decisions}

During the creation of the annotation specifications, we documented essential decisions and observations that may influence future studies and experiments. Key points that are considered limitations are discussed in Appendix~\ref{sec:appendix:limitations}; here, we discuss other noteworthy points. %These considerations are divided into two main groups: general and those related to each mandatory criterion.

%\textbf{We do not give directives on how to handle past/future information based on reporting periods.} While we tested a Supplementary Button "Time of action", we realised it is hard to validate that all the actions described by the companies took place only within the relevant “reporting period”. We only found very few companies that separate clearly their actions by the reporting period. As such, we decided not to include this in our analysis. % COVERED IN LIMITATIONS

\textbf{Annotators are instructed to never extract section titles or headers.} This means that if the section title itself provides supporting evidence or context, it will still not be extracted. This is sometimes problematic: for example, Criterion 1 (``reporting entity'') evidence is often presented in a section titled ``Reporting Entity''. In those cases, annotators extract sentences from that section containing company names, but that often do not explicitly identify those companies as ``reporting''. This may lead to confusion under the \emph{no-context} experiment setup. Ignoring section titles is however necessary, as they often do not accurately reflect the content of the paragraphs they precede. For example, a section titled ``Supply Chains'' might primarily discuss operations or risks, which could mislead annotators if they rely on the heading rather than thoroughly reading the paragraphs. This also helps avoid the concatenation of section titles with sentences when copy-pasting text from the PDF files, which would be a challenging problem to solve.

\textbf{Statements are expected to be self-contained.} Only text within the statements can be considered: annotators are instructed to NEVER follow URLs or other documents cited in the statements. In consequence,  annotators also cannot always ascertain whether the right ``governing bodies'' are providing approval, whether the right individuals are providing signatures, or whether all owned or controlled entities are included in the statement due to a lack of external context.

\textbf{Statements are expected to be understandable by a layperson.} While we provided a glossary of key terms in the annotation specifications, we do not ask annotators to search for information on specific business or legal terms, on existing legislation or legal frameworks, or on risk assessment tools. We expect the statement issuers to use clear terminology and avoid terminology that may be misleading.

%\textbf{All tasks assigned to the annotators are based on questions under the form of “is this kind of information present}”. We do not want the annotators to make decisions on whether a company has fulfilled a specific requirement of the Modern Slavery Act directly; they should instead extract text from the statement that contains the information expected by the Act, or identify that the statement does not contain such text. This simplifies the annotation process by eliminating potentially subjective decisions.  % COVERED IN LIMITATIONS

%\textbf{Some statements, although not scanned, may not allow text extraction.} Example of a document from which we cannot extract any relevant text: the PDF is not “scanned” per se, but we cannot select normal text. For this we added a Supplementary Button: “Information found in a scanned format”.

\textbf{Statement types indicated in the Modern Slavery Register are not reliable}. This metadata is likely provided by the statement issuer, but may be incorrect. Specifically: ``joint'' statements can sometimes be presented by only one reporting entity, and ``normal'' statements can be issued by a parent entity and cover many of its owned/controlled entities.

%\textbf{It can be unclear whether some mentioned entities are “forced to report”, “voluntarily reporting”, or “irrelevant for the statement”.} For example, in \href{https://modernslaveryregister.gov.au/statements/3841/}{\#3841}, it is unclear which entities are the reporting ones; the statement indicates that one of the named entities is not a reporting entity under the law… should it be treated as a voluntary reporting entity? Should it be excluded from the supporting text?

%\textbf{Visual elements with sparse information are hard to extract properly. } To correct this, we asked the annotators to select the Supplementary Button: “Visual Element Category” and extract, if possible, the information, as well as provide the page number where they found this infographic.

%\textbf{\textbf{It is hard to define how much information must be provided by each company. }Large companies are for example expected to provide much more information on their supply chains and operations than small companies, but this also varies based on the type of business and its activities (e.g. software development companies may have fewer things to describe than construction companies). In our annotation specification, providing a clear definition of “how much is enough” for a yes/no label seemed impossible; therefore, we decided to have annotators label with “yes” for tasks where any non-zero amount of relevant information is found.}
%\textbf{It is not clear how much information is “sufficient” to meet the requirements.} The guidance documentation from the government ([to do add refernce]) does not define a “minimum bar” or exact “requirements” for the information that must be provided by reporting entities in their statements. This means that having yes/no labels for “is the company meeting this criterion” is difficult, as it is an ambiguous and subjective decision to make, and one that annotators probably should.
%\textbf{The annotation company applied an ad hoc rule to correct for misidentified joint statements on the registry.} As indicated earlier in this document, the “normal” or “joint” statement types indicated in the Modern Slavery Register are not reliable. When delivering the annotations for Signature, Approval, C1 on October 10th 2023, the annotation company mentioned:

%\textit{If a normal labeled statement had a document that identified 2 reporting entities clearly using words "reporting entities" there is a note saying "Joint statement".}
%This might be insufficient as there might be joint statements that don't use the words "reporting entities" but which contain text such as “this is the MSA statement for companies A, B and C…”. We decided to accept this limitation imposed by the annotation company, as we cannot clearly know whether the statement is joint or normal.

\textbf{The ``principal governing body'' of an entity is often implicitly defined.} Identifying whether a statement is correctly approved is therefore challenging when dealing with multinational corporations with complex structures, or in the case of trusts. Also, in joint statements, seemingly independent entities can have the same board members, and this rarely mentioned in statements.

\textbf{Only the most relevant mentions of ``reporting entities'' are extracted.} This is specific to the question related to Mandatory Criterion 1: we decided to extract only the most obvious declarations. This is done to avoid having to exhaustively extract each sentence where an entity is named, as this approach does not scale well to large statements.

%\textbf{Mandatory Criterion 2:} This criterion was split into three distinct questions to check separately whether the company provides information about its structure, operations, and supply chains.

\textbf{Arrangements with suppliers do not describe operations.} This is in contradiction with the Australian government's guidance material (see Table~2~of~\cite{amsa2018_guidance2023}). Specifically, we consider that ``explaining in general terms the type of arrangements the entity has with its suppliers and the way these are structured'' is vague, hard to convey to annotators, and relates more to descriptions of suppliers or supply chains. We found that annotation quality improved following this decision.

\textbf{The ``structure'' of an entity is a vague concept.} A reporting entity may for example describe its management and governance structure (e.g. naming executives or members of its board of directors), while another might focus more on its organizational structure (e.g. naming parent companies, subsidiaries, and affiliates). The latter is usually understood to be more relevant, but the Australian government also considers, for example, Australian Business Number (ABN) and registered office location to be relevant information~\cite{amsa2018_guidance2023} while making no mention of the importance of capital structure, reporting structure, or taxation structure descriptions. Classifying information on shareholders is also difficult, as it may sometimes be relevant when few shareholders have significant control over the reporting entity. Lastly, we note that descriptions of ``brick-and-mortar'' locations (e.g. facilities, stores) are often provided as descriptions of structure by companies, but this is instead considered relevant for operations.

%Stating the acquisition of other companies is not regarded as structural information in our annotation specifications. %Although this is not explicitly mentioned in the specification, it is implied through an example (2.4.3.b, REECE). Describing a trend of acquisitions would likely be categorized under operations, as it suggests that it is an activity of the entity.
%Declaring who the shareholder(s) of a company are is not considered structure information. In cases where a company has only one shareholder, this may be problematic, as that shareholder might technically have a controlling interest. Generally, holding shares is instead seen as having a “financial investment”, and therefore it is related to operations.

\textbf{The number of workers is considered structure information.} According to the Australian government's guidance material~\cite{amsa2018_guidance2023}, this information may be relevant for both structure and operations. However, for simplicity and clarity, we considered it only relevant for structure in our guidelines to annotators.

%Descriptions of “brick-and-mortar” locations (facilities, stores, …) have been moved to operations. This is aligned with the guidance document, but not necessarily aligned with what our intuition might be for “structure”. This is based on the idea that structure only refers to the legal and organizational structure of the entity.

\textbf{Descriptions of customers are not relevant for supply chains.} In reality, customers can be considered as part of the ``downstream'' supply chain of an entity, but we do not consider this information relevant in our guidelines. The Australian government's guidance material~\cite{amsa2018_guidance2023} also mentions that entities are not required to report this information. However, the distribution of products or services to customers is considered a relevant activity (or operation).

%Entities are not required to describe how their customers are using their services or products. Beyond that, the MSA guidance says that \textbf{risks} associated with how customers use the services or products of the entity \textbf{are not included in the scope of the statements for the MSA}, meaning so they are NOT relevant for C3/C4/C5. However, the description of the supply chain of the entity may include a description of clients (i.e. in the “downstream” part of the supply chain) as well as of how they use the services or products of the entity. That would be relevant for C2 (supply chain).

\textbf{Risks and actions may not always apply to owned or controlled entities.} Specifically, Mandatory Criteria 3, 4, and 5 require entities to provide information about risks and actions that apply to ``the reporting entity and any entities it owns or controls.'' However, based on consultations with the Australian Attorney General's Department and annotation experts, we decided that if a description of risks or actions only seem to apply to the reporting entity, this information is still considered relevant. We initially decided to have a separate data field to flag information that would also apply to owned and controlled entities, but we determined during testing that it was rarely used; it was eventually removed from labeling workflows.  

%Criterion four was split into two distinct questions to check separately whether the company provides information about mitigation and remediation actions.

\textbf{Owned or controlled entities might not always be consulted.} Due to ambiguities and the lack of external context, it is difficult to determine whether the list of owned and controlled entities perfectly overlaps with the list of ``consulted'' entities. Although Mandatory Criterion 6 requires reporting entities to consult with all entities they own or control, there are also various reasons why they might not be able to do so. Some of those entities may, for example, be dormant, inactive, or non-trading. Furthermore, only consultation ``on the preparation of the statement'' is considered relevant for this criterion, but reporting entities rarely describe their actual consultation process.
 
%\textbf{Mandatory Criterion 7: }As illustrated in Figure X, criterion 7 was excluded from the analysis. We considered it too broad, and it was impractical to leave the determination of what the company might have deemed relevant information to the annotations. Additionally, providing an exhaustive list of examples was not feasible. Therefore, this criterion would have been too costly and inefficient. Furthermore, we noted that this criterion was also omitted in the AGD's annual report analysis \cite{australiangovernment2022}. Consequently, we chose to exclude it from our analysis as well.

\textbf{Statement signatures are sometimes difficult to interpret.} For example, large statements often contain a ``message from the CEO'' with general comments on the importance of the statement or on the achievements of their company. These message are often signed, but it is unclear if that signature applies to the whole statement, or just to that message. Documents may also occasionally lack the actual image of a signature, or may only include a blank space or a box where a signature is supposed to be. Such cases are still considered valid evidence, as the image of the signature is not necessary, but the intent to sign is acknowledged.

%it can be unclear whether a signature applies to the entire content of a statement or just to the page where there is a “message from the CEO”. %It is ideal when the signature is near the approval text, mentioned directly in the approval text, or at the end of the statement. 

%[repetition]It can be unclear whether a signature is made by a “responsible member” of the reporting entity. For example, in \href{https://modernslaveryregister.gov.au/statements/11945/}{\#11945}, if a “Company Secretary” signs the statement, is that enough? %Would that be a “no” for signature? (Example: TODO) What if there was no box, but the person’s name is typed where a signature would typically be? (Example: \href{https://modernslaveryregister.gov.au/statements/7741/}{\#7741}, \href{https://modernslaveryregister.gov.au/statements/10859/}{\#10859})

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Limitations}  %%%% DO NOT MOVE ME! I NEED TO STAY HERE!
\label{sec:appendix:limitations}  %  should always stay "APPENDIX E"

We concluded the paper by highlighting some of the key limitations of our dataset (Section~6). Among these, the most significant challenge is the subjective and noisy nature of the relevant sentence annotation process. Although our guidelines for annotators were designed to minimize subjectivity and maximize consistency, the Inter-Annotator Agreement (IAA), as shown in Table~1 of the paper, varies significantly across different questions. Based on qualitative analyses of the annotated data, we believe that the IAA is not an ideal measure of annotation quality. Good IAA scores were observed in some statements where a significant amount of relevant information was missed by annotators and where obviously relevant information was correctly extracted. Initially, we set high thresholds for expected IAA scores with the annotators, but we later encouraged lower IAA scores for statements deemed more difficult to annotate. This approach aimed to promote the extraction of more potentially relevant text. Ultimately, we believe that modeling approaches capable of handling noisy labels and leveraging annotator disagreements as an additional supervision signal may lead to more effective solutions for sentence relevance classification.

A somewhat subjective annotation process can also introduce bias in the labeling of disclosures, potentially leading to unfair assessments of whether certain companies (or those operating in specific industrial sectors) meet the requirements of the Act. This bias might result from individual annotators' interpretations of the guidelines or their preconceived notions about particular industries. To mitigate this risk, we consulted with experts in the design of our annotation guidelines, aiming to minimize any disadvantage to specific businesses, and relied on the professionalism of the annotation company and their internal QA process to vouch for their work. Furthermore, for transparency and to allow for external review and improvement, we make both the annotations and the guidelines publicly available.

The extraction of text from PDFs poses other significant challenges. Beyond the difficulty of correctly extracting text from embedded figures and tables, matching any sentence annotated by a human to the automatically extracted text from the PDF is also complex. This difficulty arises due to text fragmentation, OCR errors, non-ASCII character mismatches, and out-of-order parsing. In practice, we found that using ABBYY FineReader, a commercial software with an OCR engine, reduced the match rate for annotated sentences compared to using PyMuPDF (fitz), which lacks an OCR engine, even when employing a Levenshtein sentence matching approach. Revisiting the text extraction and matching methodology, potentially replacing regular expressions with a more advanced method for determining sentence boundaries and matching them, would likely enhance the reliability of evaluations for relevant text classification models.

As for the challenge of differentiating past and future information in our dataset, one potential solution is to introduce temporal labels, where markers indicating whether the information pertains to past actions, ongoing activities, or future plans would be added to annotations. Language models could be employed to automatically infer these markers from the text, reducing the re-annotation burden and providing scalability.

Experiments for single-sentence classification with API-based language models with large context windows can be wasteful due to the high number of model requests required, significantly increasing costs. Future works might want to explore the simultaneous classification of multiple sentences at once, such as paragraph-by-paragraph, to reduce the number of model requests. This approach would however necessitate more substantial prompt engineering and output parsing efforts. Additionally, a hierarchical context processing approach, which involves structuring the input to provide broader context on the statement before drilling down to specific sentence-level details, could be worth investigating for both zero-shot and supervised learning settings.

% Besides, some key metadata provided by the Modern Slavery Register~\cite{amsa2018_register}, including entity names and whether a statement is a joint statement ( e.g., it covers multiple reports entities), can be unreliable or missing. Although this affects only a small proportion of statements, it means annotators cannot rely on this information to improve their understanding of the issuing entity and of the expected governing body. This issue may also affect the quality of our statement clusters.

% Despite the requirement that all AMSR statements must be validated by the AusGov for approval and signature, we have identified some that lack board approval or signatures. The AU government asserts that all statements on the registry are signed and approved, and if not, they are returned to the company for correction. Nevertheless, statements are still published even if the company does not comply with other mandatory criteria. Although the AusGov reviews all statements annually to present summary statistics in the report to parliament, they do not provide that data.

% link to the ideas-and-reminders document: https://docs.google.com/document/d/1QjOsHoSNrSeIWk6Frt_3J6WY561GT0Yjrl9mdJ4dnYs/edit

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Implementation and Experimentation Details}  %%%% DO NOT MOVE ME! I NEED TO STAY HERE!
\label{sec:appendix:impl_and_exp_details}  %  should always stay "APPENDIX F"

Details on the models we selected as baselines for our experiments are presented in Table~\ref{tab:appendix:impl_and_exp_details:model_info}. In addition to the experimentation details presented in Section~5 of the paper (Benchmark Experiments), we report that the models are fine-tuned with a cross-entropy loss using the Adam optimizer and without a learning rate scheduler. Each model is trained for 24 hours on a A100 GPU, with the exception of \llama{} in the setup with all Mandatory Criteria questions, which is trained for 48 hours to allow the model more time to converge. In the case of \llama{}, a batch size of 32 is simulated using gradient accumulation, where the real batch size is set to 2 and the gradient is accumulated over 16 steps. All the fine-tuning is conducted in 16-bit mixed precision mode. For DistilBERT and BERT, we attach a classification head directly to the CLS token positioned at the beginning of the target sentence for both the \emph{no-context} and \emph{with-context} setups. For \llama{}, we use the last token as is typically done with other causal models.

For training data preparation, the pre-extracted statement text is split into sentences with various amounts of context at training time. These sentences are then shuffled and assembled into minibatches using a fixed-size sentence buffer (containing up to 8192 sentences). We assign a positive relevance label to any extracted sentence that matches a sentence tagged by an annotator as being relevant, and assign a negative relevance label otherwise. The matching of extracted and tagged sentences is done following text cleanups using regular expressions, and by considering perfect matches, partial matches, and noisy matches based on the Levenshtein distance.

\begin{table}[H]
  \caption{Baseline model details. For BERT and DistilBERT, full model weights are fine-tuned, and for \llama{}, we use the LoRA approach~\cite{hu2021lora}, resulting in a smaller number of trainable parameters.}
  \vspace{1mm}
  \label{tab:appendix:impl_and_exp_details:model_info}
  \centering
  \begin{tabular}{llcc}
    \toprule
        Model  name   & URL & \makecell{Total\\params} & \makecell{Trainable\\params} \\
    \midrule
        DistilBERT &
            \footnotesize \href{https://huggingface.co/distilbert/distilbert-base-uncased}{https://huggingface.co/distilbert/distilbert-base-uncased} &
            66.8M &
            66.8M \\
        BERT &
            \footnotesize \href{https://huggingface.co/google-bert/bert-base-uncased}{https://huggingface.co/google-bert/bert-base-uncased} &
            109M &
            109M \\
        \llama{} &
            \footnotesize \href{https://huggingface.co/NousResearch/Llama-2-7b-hf}{https://huggingface.co/NousResearch/Llama-2-7b-hf}  &
            6.6B &
            4.2M \\
    \midrule
        \gptthree{} &
            \footnotesize \href{https://platform.openai.com/docs/models/gpt-3-5-turbo}{https://platform.openai.com/docs/models/gpt-3-5-turbo} &
            ? & 
            ? \\
        \gptfour{} &
            \footnotesize \href{https://platform.openai.com/docs/models/gpt-4o}{https://platform.openai.com/docs/models/gpt-4o} &
            ? &
            ? \\
    \bottomrule
  \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Prompt Design and Examples}  %%%% DO NOT MOVE ME! I NEED TO STAY HERE!
\label{sec:appendix:prompt_design_and_ex}  %  should always stay "APPENDIX G"

\begin{figure}
\centering
\begin{tcolorbox}[colback=gray!10!white, colframe=gray!80!black, title={Prompt template (C2, ``supply chains'', \emph{no-context})}]
You are an analyst that inspects modern slavery declarations made by Australian reporting entities. You are specialized in the analysis of statements made with respect to the Australian Modern Slavery Act of 2018, and not of any other legislation.
\newline\newline
You are currently looking for sentences in statements that describe the SUPPLY CHAINS of an entity, where supply chains refer to the sequences of processes involved in the procurement of products and services (including labour) that contribute to the reporting entity's own products and services. The description of a supply chain can be related, for example, to 1) the products that are provided by suppliers; 2) the services provided by suppliers, or 3) the location, category, contractual arrangement, or other attributes that describe the suppliers. Any sentence that contains these kinds of information is considered relevant. Descriptions that apply to indirect suppliers (i.e. suppliers-of-suppliers) are considered relevant. Descriptions of the supply chains of entities owned or controlled by the reporting entity making the statement are also considered relevant. However, descriptions of 'downstream' supply chains, i.e. of how customers and clients of the reporting entity use its products or services, are NOT considered relevant. Finally, sentences that describe how the reporting entity lacks information on some of its supply chain, or how some of its supply chains are still unmapped or unidentified, are also considered relevant.
\newline\newline
Given the above definitions of what constitutes a relevant sentence, you will need to determine if a target sentence is relevant or not. You must avoid labeling sentences with only vague descriptions or corporate talk (and no actual information) as relevant. The answer you provide regarding whether the sentence is relevant or not can only be 'YES' or 'NO', and nothing else.
\newline\newline
The target sentence to classify is the following:
\newline
------------
\newline
\texttt{TARGET\_SENTENCE}
\newline
------------
\newline
Is the target sentence relevant? (YES/NO)
\end{tcolorbox}
\caption{Prompt template used for zero-shot model experiments under the \emph{no-context} setup.}
\label{fig:appendix:prompt_design_and_ex:prompt_template_no_context}
\end{figure}

\begin{figure}
\centering
\begin{tcolorbox}[colback=gray!10!white, colframe=gray!80!black, title={Prompt template (C2, ``supply chains'', \emph{with-context})}]
You are an analyst that inspects modern slavery declarations made by Australian reporting entities. You are specialized in the analysis of statements made with respect to the Australian Modern Slavery Act of 2018, and not of any other legislation.
\newline\newline
You are currently looking for sentences in statements that describe the SUPPLY CHAINS of an entity, where supply chains refer to the sequences of processes involved in the procurement of products and services (including labour) that contribute to the reporting entity's own products and services. The description of a supply chain can be related, for example, to 1) the products that are provided by suppliers; 2) the services provided by suppliers, or 3) the location, category, contractual arrangement, or other attributes that describe the suppliers. Any sentence that contains these kinds of information is considered relevant. Descriptions that apply to indirect suppliers (i.e. suppliers-of-suppliers) are considered relevant. Descriptions of the supply chains of entities owned or controlled by the reporting entity making the statement are also considered relevant. However, descriptions of 'downstream' supply chains, i.e. of how customers and clients of the reporting entity use its products or services, are NOT considered relevant. Finally, sentences that describe how the reporting entity lacks information on some of its supply chain, or how some of its supply chains are still unmapped or unidentified, are also considered relevant.
\newline\newline
Given the above definitions of what constitutes a relevant sentence, you will need to determine if a target sentence is relevant or not inside a larger block of text. The target sentence will first be provided by itself so you can know which sentence we want to classify. It will then be provided again as part of the larger block of text it originally came from (extracted from a PDF file) so you can analyze it with more context. While some of the surrounding sentences may be relevant according to the earlier definitions, we are only interested in classifying the target sentence according to the relevance of its own content. You must avoid labeling sentences with only vague descriptions or corporate talk (and no actual information) as relevant.
\newline\newline
The answer you provide regarding whether the sentence is relevant or not can only be 'YES' or 'NO', and nothing else.
\newline\newline
The target sentence to classify is the following:
\newline
------------
\newline
\texttt{TARGET\_SENTENCE}
\newline
------------
\newline
The same target sentence inside its original block of text:
\newline
------------
\newline
\texttt{SENTENCE\_IN\_CONTEXT}
\newline
------------
\newline
Is the target sentence relevant? (YES/NO)
\end{tcolorbox}
\caption{Prompt template used for zero-shot model experiments under the \emph{with-context} setup.}
\label{fig:appendix:prompt_design_and_ex:prompt_template_with_context}
\end{figure}

\begin{figure}
\centering
\begin{tcolorbox}[colback=gray!10!white, colframe=gray!80!black, width=\textwidth, title=Target sentence example]
The compliance with these communicated expectations is ensured by regular unannounced audits of all suppliers in this region.
\end{tcolorbox}
\vspace{1mm}
\begin{tcolorbox}[colback=gray!10!white, colframe=gray!80!black, width=\textwidth, title=Target sentence example with 100-word context]
\texttt{[...]} we have established clear and stringent expectations for all our suppliers in Southeast Asia regarding labor practices and ethical standards. These expectations are communicated through detailed supplier agreements and comprehensive training programs. Additionally, we collaborate closely with local communities and stakeholders to promote awareness and understanding of ethical labor practices. \textbf{The compliance with these communicated expectations is ensured by regular unannounced audits of all suppliers in this region.} Furthermore, our commitment to transparency and accountability extends beyond audits, as we engage with independent third-party auditors to validate our findings and ensure the integrity of our supply chains. Any detected non-compliance triggers immediate corrective actions and follow-up reviews, demonstrating our dedication to resolving issues swiftly and \texttt{[...]}
\end{tcolorbox}
\caption{Example of a fictitious sentence to be classified as relevant or irrelevant, with and without context. The amount of context here (roughly 100 words) is the same one used in our experiments. For the question related to C5 (assessing the effectiveness of actions), classifying this sentence is difficult when context is not provided, as it is unclear whose and what expectations were communicated, and whose suppliers are audited. With context, it is clear that the sentence contains relevant information mandated by Mandatory Criteria 5 of the Act.}
\label{fig:appendix:prompt_design_and_ex:example_context}
\end{figure}

We present in Figures~\ref{fig:appendix:prompt_design_and_ex:prompt_template_no_context} and~\ref{fig:appendix:prompt_design_and_ex:prompt_template_with_context} the exact prompt templates we used for the \emph{no-context} and \emph{with-context} setups for zero-shot model experiments. Note that the \texttt{TARGET\_SENTENCE} and \texttt{SENTENCE\_IN\_CONTEXT} placeholders are respectively substituted with the target sentence to classify and the same sentence with surrounding context in actual model prompts. For an example of a target sentence that would be classified along with its context, see Figure~\ref{fig:appendix:prompt_design_and_ex:example_context}.

\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Additional Results}  %%%% DO NOT MOVE ME! I NEED TO STAY HERE!
\label{sec:appendix:add_results}  %  should always stay "APPENDIX H"

%\subsection{Results for All Questions}
%\label{appendix__result_details__all_criteria}

We present in Table~\ref{tab:appendix:add_results:allquestions} the detailed F1 evaluation results for the jointly fine-tuned models that are also shown in Figure~3 of the paper.

\begin{table}[h]
    \centering
    \caption{F1 evaluation results for fine-tuned models on all eleven Mandatory Criteria questions.}
    \label{tab:appendix:add_results:allquestions}
    \vspace{1mm}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Question} & \multicolumn{3}{c}{\textbf{No context}} & \multicolumn{2}{c}{\textbf{With context}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-6}
                & DistilBERT    & BERT    &   \llama{}   & DistilBERT & BERT \\
        \midrule
        Approval & 0.957 & 0.965 & 0.889 & 0.955 & 0.964 \\
        C1 (reporting entity) & 0.639 & 0.605 & 0.579 & 0.698 & 0.728 \\
        C2 (structure) & 0.708 & 0.732 & 0.708 & 0.740 & 0.740 \\
        C2 (operations) & 0.741 & 0.718 & 0.672  & 0.769 & 0.758 \\
        C2 (supply chains) & 0.723 & 0.675 & 0.719 & 0.755 & 0.772 \\
        C3 (risk description) & 0.653 & 0.660 & 0.650 & 0.705 & 0.741 \\
        C4 (risk mitigation) & 0.631 & 0.614 & 0.602 & 0.629 & 0.640 \\
        C4 (remediation) & 0.574 & 0.571 & 0.424 & 0.500 & 0.559 \\
        C5 (effectiveness) & 0.533 & 0.483 & 0.242 & 0.491 & 0.560 \\
        C6 (consultation) & 0.414 & 0.429 & 0.293 & 0.641 & 0.571 \\
        Signature & 0.794 & 0.859 & 0.797 & 0.844 & 0.866 \\
        \midrule
        Overall (macro) & 0.670 & 0.665 & 0.598 & 0.702 & 0.718 \\
        \bottomrule
  \end{tabular}
\end{table}

%\subsection{F1 evolution over the epochs}
%\label{appendix__result_details__f1_evolution}

We also present in Figure~\ref{fig:appendix:add_results:val_f1_trend_overtime} the evolution of fine-tuned model performance in terms of validation Macro F1 during training under the \emph{no-context} setup. While BERT and DistilBERT demonstrate strong performance starting from the first epoch, \llama{} requires several epochs to reach comparable performance. We attribute this to \llama{} being fine-tuned using LoRA, which significantly reduces the number of trainable parameters, as shown in Table~\ref{tab:appendix:impl_and_exp_details:model_info}. Additionally, we observe that \llama{} may benefit from extended fine-tuning, as the macro F1 curve has not plateaued even after 48 hours of training.

%\begin{figure}[t]
%    \centering
%    \subfigure[BERT and DistilBERT macro F1.]{
%        \includegraphics[width=0.48\textwidth]{figures/bert_distilbert_val_f1_overtime.pdf}
%        \label{fig:appendix:appendix:val_f1_trend_overtime__bert_distilbert}
%    }
%    \subfigure[\llama{} macro F1.]{
%        \includegraphics[width=0.48\textwidth]{figures/llama2_val_f1_overtime.pdf}
%        \label{fig:appendix:appendix:val_f1_trend_overtime__llama2}
%    }
%    \caption{Macro F1 score over the epochs for the fine-tuned models in the all-label case.}
%    \label{fig:appendix:val_f1_trend_overtime}
%\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/val_f1_overtime.pdf}
    \caption{Macro F1 score over the epochs for the fine-tuned models in the all-label case.}
    \label{fig:appendix:add_results:val_f1_trend_overtime}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{Additional Related Works and Relevant Legislations}  %%%% DO NOT MOVE ME! I NEED TO STAY HERE!
%\label{sec:appendix:add_relworks}  %  should always stay "APPENDIX I"

%todo: more discussion on extra related works or legislations

%Current regulations require that companies disclose their statements on their websites to make them accessible to consumers and other interested parties. Prior to establishing centralized registries in the UK, the Business and Human Rights Resource Center (BHRRC) gathered approximately 16,000 statements made in accordance with the UK's Modern Slavery Act and the California Transparency in Supply Chains Act between 2015 and 2020, which were included in their Modern Slavery Registry\cite{bhrrcmodernslaveryregistry}. Since then, the UK Government introduces a centralized registry\cite{ukgovmodernslaveryregistry} for modern slavery statements in 2021 and has since collected over 46,000 statements. 

%\subsection{Analysis of Similar Statements from Other Jurisdictions}

%Statements published in response to the UK MSA have been the closest analyzed, as the legislation has been in place longer and covers a larger pool of companies, resulting in a larger amount of statements published each year. Here again, the research typically involves detailed analyses of a select number of statements, such as statements from 100 Financial Times Stock Exchange (FTSE) companies assessed by the BHRRC \cite{bhrrc2017ftse100}, or targeting particular sectors, such as the examination of nine statements of three major UK retailers \cite{ras2019quest}, 95 statements of UK government suppliers \cite{pinnington2023transparency} 17 statements that Australian financial entities collectively submitted under UK MSA \cite{dean2020race}, 72\cite{martin2017human} and 118 \cite{rogerson2020organisational}statements from higher education institutions. 

%Austrian modern slavery statements are also collected and analyzed on WikiRate, a platform focused on open data regarding business and human rights, which partners with the Walk Free Foundation on the Beyond Compliance Project. This project entails yearly evaluations of modern slavery statements by volunteers and experts from specific industries in both the Australia and the UK. Their research series encompasses reviews of 128 statements from the hotel industry\cite{walkfree2020hotel}, 79 from the finance sector\cite{walkfree2021finance}, 60 from the renewable energy sector\cite{WF2023}, and 50 from the garment industry\cite{walkfree2022garment}. In addition to these studies, WikiRate consistently engages volunteers to review modern slavery statements. At the time of this report, their platform contains XXXXX assessed modern slavery statements. 

%AI applied to law datasets and benchmarks:
%The field of legal AI research has seen significant advances, as shown through various datasets and projects highlighted at NeurIps aimed at improving document analysis and summarization. Multi-LexSum \cite{shen2022multi}provides a multigranularity summarization data set for civil rights litigation, helping the Civil Rights Litigation Clearinghouse process lengthy documents. The Cambridge Law Corpus (CLC) \cite{ostling2024cambridge} offers over 250,000 UK court cases, with expert annotations facilitating legal outcome extraction. A comprehensive survey of US criminal justice datasets addresses algorithmic fairness and predictive tools in the justice system\cite{zilka2022survey}. Pile of Law presents a 256GB dataset focused on responsible data filtering, enhancing legal text processing \cite{henderson2022pile} . Additionally, the Contract Understanding Atticus Dataset (CUAD) \cite{hendrycks2021cuad} serves as a benchmark for legal contract review, with over 13,000 expert annotations, demonstrating the influence of model design and dataset size on performance. These efforts collectively advance the development of AI tools in legal and sustainability contexts. 

%\subsection{[Don't think is necessary to bring in this angle: ML against modern slavery at large. But if we decide to , here is a good survey] }

%%%%% link witn survey? => While in many application domains of QA systems, there is a problem of ambiguity, i.e., most of the time; the questions are formulated so that a single statement may assume different meanings. When dealing with legal language, this problem also exists. Therefore, the problem of ambiguity is one of the fundamental challenges limiting the development of solutions in this field. Legal experts are aware that ambiguity exists in legal texts [125], sometimes intentionally, so it is necessary to bring to the table solutions that can cope with this problem . A survey on legal question–answering systems

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
