\section{Differential Privacy implies Bounded Max-Information: Sampling without Replacement }\label{app:maxinfo}

Prior work shows that for datasets sampled i.i.d., differentially private algorithms have bounded max-information \cite{DworkFHPRR15,RogersRST16}. In this appendix we prove \Cref{thm:pure-dp-implies-maxinfo} and \Cref{thm:approx-dp-implies-max-info}, which are analogs of those theorems for sampling without replacement.

\subsection{Preliminaries}\label{sec:prelims-appendix}

First we state \Cref{thm:mcdiarmids_without-our-version}, a version of McDiarmid's inequality that applies to the case of sampling without replacement. This result follows from Lemma~2 in \cite{Tolstikhin17}. This result in used in the proof of \Cref{thm:pure-dp-implies-maxinfo}, which says that pure-DP algorithms have bounded max-information (even in the case of sampling without replacement.)

% For this, we use the following result from , which is a version of  For this result, the authors first introduce the notion of \emph{$(n,m)$-symmetric} functions.

% \begin{definition}\label{def:m-n-symmetric}
%     A function $f:\pi\to R$, given on a symmetric group of the set $\{1,\dots,m\}$ is called \emph{$(n,m)$-symmetric with respect to permutations} if it does not change its value under the change of order of the first $n$ and/or last $m-n$ coordinates $\pi$. (For the rest of this work, we will call them simply $(n,m)$-symmetric functions.)
% \end{definition}


% \begin{theorem}[McDiarmid's inequality for sampling without replacement \cite{Tolstikhin17}]\label{thm:mcdiarmids_without}
%     Let $\pi$ be a vector of random permutation chosen uniformly from a symmetric permutation group of the set $\{1,\dots,n\}$. For natural numbers $m < n$, let $f(\pi)$ be an $(m,n)$-symmetric function for which there exists a constant $\Delta > 0 $ such that $\abs{f(\pi)-f(\pi^{i,j})} \leq \Delta$ for all $\pi, i\in\{1,\dots,m\}, j\in\{m+1,\dots,n\}$ where the permutation $\pi^{i,j}$ is obtained from $\pi$ by transposition of its $i^{\text{th}}$ and $j^{\text{th}}$ coordinates. Then for $t \geq 0$
%     \ifnum\usenix=0
%         $$\Pr[f(\pi)-\E[f(\pi)] \geq t] \leq \exp\left(-\frac{2t^2}{m\Delta^2}\cdot\left(\frac{n-1/2}{n-m}\right)\cdot\left(1-\frac{1}{2\max(m,n-m)}\right)\right)$$
%     \else
%     \begin{align*}
%     & \Pr[f(\pi)-\E[f(\pi)] \geq t] \\
%     & \qquad \leq \exp\left(-\frac{2t^2}{m\Delta^2}\cdot\left(\frac{n-1/2}{n-m}\right)\cdot\left(1-\frac{1}{2\max(m,n-m)}\right)\right)
%     \end{align*}
%     \fi 
% \end{theorem}

% We are interested in sampling without replacement, and a version of McDiarmid's inequality for sampling without replacement follows from the above theorem, as given below.

\begin{definition}\label{def:data-order-invariant-function}
    A function $f:\cX^m \to \cY$, is called \emph{order invariant} if for all $X \in \cX^m$, the value of the function $f(X)$ does not depend on the order of the elements of $X$. 
\end{definition}

\begin{theorem}[McDiarmid's for sampling without replacement \cite{Tolstikhin17}]\label{thm:mcdiarmids_without-our-version}
    Let $f:\cX^n \to \cY$ be an an order invariant function with global sensitivity $\Delta > 0$. Let $\cX$ be a data universe of size $n$, let $S$ be a sample of size $m$ chosen without replacement from $\cX$. Then for $t \geq 0$,
    \ifnum\usenix=0
        $$\Pr_S [f(S)-\E[f(S)] \geq t] \leq \exp\left(-\frac{2t^2}{m\Delta^2}\cdot\left(\frac{n-1/2}{n-m}\right)\cdot\left(1-\frac{1}{2\max(m,n-m)}\right)\right)$$
    \else
    \begin{align*}
    & \Pr_S [f(S)-\E[f(S)] \geq t] \\
    & \qquad \leq \exp\left(-\frac{2t^2}{m\Delta^2}\cdot\left(\frac{n-1/2}{n-m}\right)\cdot\left(1-\frac{1}{2\max(m,n-m)}\right)\right)
    \end{align*}
    
    \fi    

    In particular, for $m = n/2$ and $n \geq 3$,
        $$\Pr[f(S)-\E[f(S)] \geq t] \leq \exp\left(-\frac{4t^2}{n\Delta^2}\right)$$
\end{theorem}

Next we state some lemmas that are used in the proof of \Cref{thm:approx-dp-implies-max-info} and \Cref{cor:approx-dp-implies-max-info} which say that approximate-DP algorithms have bounded max-information (even in the case of sampling without replacement.)

\begin{definition}[Point-wise indistinguishibility \cite{KasiviswanathanS14}]
Two random variables $A,B$ are point-wise $(\eps, \delta)$-indistinguishable if with probability at least $1-\delta$ over $ a \sim P(A)$:
$$
e^{-\epsilon} \Prob{}{B = a} \leq \Prob{}{A = a} \leq e^{\epsilon} \Prob{}{B=a}.
$$
\end{definition}

\begin{lemma}[Indistinguishability implies Pointwise Indistinguishability \cite{KasiviswanathanS14}] \label{lem:prelims}
 Let $A, B$ be two random variables. If $A \approx_{\eps, \delta} B$ then $A$ and $B$ are pointwise $\left(2\eps, \frac{2\delta}{1-e^{-\eps}} \right) $-indistinguishable.
\end{lemma}

\begin{lemma}[Conditioning Lemma \cite{KasiviswanathanS14}]\label{lem:conditioning}
Suppose that $(A,B) \approx_{\eps, \delta} (A',B')$.  Then for every $\hat \delta>0$, the following holds:
$$
\Prob{t \sim P(B) }{ A|_{B = t} \approx_{3\epsilon, \hat\delta} A'|_{B'=t} } \geq 1-\frac{2\delta}{\hat\delta} - \frac{2\delta}{1-e^{-\eps}}.
$$
\end{lemma}

\begin{theorem}[Azuma's Inequality]
Let $C_1, \cdots, C_n$ be a sequence of random variables such that for every $i \in [n]$, we have
$$
\Prob{}{|C_i| \leq \alpha} = 1
$$
and for every fixed prefix $\mathbf{C}_1^{i-1} = \mathbf{c}_1^{i-1}$, we have
$$
\Ex{}{C_i|\mathbf{c}_1^{i-1}} \leq \gamma,
$$
then for all $t\geq 0$, we have
$$
\Prob{}{\sum_{i=1}^n C_i > n \gamma + t \sqrt{n} \alpha} \leq e^{-t^2/2}.
$$
\label{thm:azuma}
\end{theorem}

% The bounds on max-information of differentially private algorithms shown in prior work apply only to the case where the datasets consists of i.i.d. samples. This means that the results only apply if the dataset is drawn with replacement. Since most real world datasets are not drawn with replacement, we extend the results to the case where datasets are drawn without replacement. 

\subsection{Pure-DP $\implies$ Bounded Max-Information}

In this appendix we state \Cref{thm:pure-dp-implies-maxinfo}, which is an analog of Theorem~ from \cite{DworkFHPRR15}. The proof of this theorem works exactly as in \cite{DworkFHPRR15}, except replacing the application of McDiarmid's Lemma with a version of McDiarmid's for sampling without replacement (\cref{thm:mcdiarmids_without-our-version}) which we state in \Cref{sec:prelims-appendix}.

\puredpmaxinfo


\subsection{$(\eps,\delta)$-DP $\implies$ Bounded Max-Information}

In this appendix we prove \Cref{thm:approx-dp-implies-max-info}, which is an analog of Theorem~1 from Rogers et al.\cite{RogersRST16}. In fact, the following proof is almost exactly the proof of them from \cite{RogersRST16} with the following differences: (1) we compute all the constants exactly and avoid using asymptotic notation, and (2) we keep the tunable parameters in the final version of the theorem to obtain the most flexible result that we can. Finally, we set the parameters in \Cref{thm:approx-dp-implies-max-info} to get \Cref{cor:approx-dp-implies-max-info}, which is used in the proof of \Cref{thm:approx-dp-implies-coherence-enforcement}.

\approxmaxinfogeneral

\approxmaxinfo

\newcommand{\uglyterm}{342\frac{\hat{\delta}}{\eps} + 112\frac{\hat{\delta}^2}{\eps^2} + 25\frac{\hat{\delta}^2}{\eps} + 240\eps^2}

We will sometimes abbreviate
conditional probabilities of the form $\Prob{}{\bX=\bbx\mid\cA = a}$ as
$\Prob{}{\bX=\bbx\mid a}$ when the random variables are clear from
context.  Further, for any $\bbx \in \cX^n$ and $ a \in \cY$, we define

\begin{equation}
Z_i(a,\bbx_{[i]}) \defeq 
\log\dfrac{\Prob{}{X_i = x_i \mid  a, \bbx_{[i-1]}}}{\Prob{}{X_i = x_i\mid  \bbx_{[i-1]}}}. \label{eqn:Z_i}
\end{equation}

\begin{align*}
Z(a,\bbx) & \defeq 
\log \left( \dfrac
{\Prob{\bbx}{\cA(x)=a, \bX=\bbx}}
{\Prob{}{\cA = a} \cdot \Prob{}{\bX=\bbx}}
\right) \\
 & = \sum\limits_{i=1}^n Z_i(a,\bbx_{[i]}) \numberthis \label{eqn:ln_sum}
\end{align*}

If we can bound $Z(a,\bbx)$ with high probability over $(a,\bbx) \sim p(\cA(\bX),\bX)$, then we can bound the approximate max-information by using the following lemma:

\begin{lemma}[{\cite[Lemma 18]{DworkFHPRR15}}]
\ifnum\usenix=0
$$\Prob{}{\log \left( \dfrac
{\Prob{\bbx}{\cA(x)=a, \bX=\bbx}}
{\Prob{}{\cA = a} \cdot \Prob{}{\bX=\bbx}}
\right) \geq k} \leq \beta \,\,\implies\, I_\infty^\beta(\cA(\bX);\bX) \leq k.$$
\else
\begin{align*}
\Pr&[\log \left( \dfrac
{\Pr[\cA(x)=a, \bX=\bbx]}
{\Pr[\cA = a] \cdot \Pr[\bX=\bbx]}
\right) \geq k] \\
& \leq \beta \,\,\implies\, I_\infty^\beta(\cA(\bX);\bX) \leq k.
\end{align*}
\fi
\label{lem:boundmaxinfo}
\end{lemma}

To bound $Z(a,\bbx)$ with high probability over $(a,\bbx) \sim p(\cA(\bX),\bX)$ we will apply Azuma's inequality (\Cref{thm:azuma}) to the sum of the $Z_i(a,\bbx_{[i]})$'s. For this we must first argue that each $Z_i(a,\bbx_{[i]})$ term is bounded with high probability:
\begin{claim}\label{claim:zibound}
Let $\hat\delta>0$, and $\delta'' \defeq \tfrac{2\hat\delta}{1-e^{-3\eps}}$.
If $\cA$ is $(\eps,\delta)$-differentially private and, $\bX \in \univ^n$ is sampled without replacement from a finite universe $\univ$, then for each $i \in [n]$, and each prefix $\bbx_{[i-1]}\in \cX^{i-1}$ and answer $a$, we have:
    $$
\Prob{x_i \sim X_i |_{\bbx_{[i-1]}}}
{\log\dfrac{\Prob{}{X_i = x_i \mid a, \bbx_{[i-1]}}}{\Prob{}{X_i = x_i\mid \bbx_{[i-1]}}} \leq 6 \eps } \geq 1 - \delta''
$$
\label{claim:Fi}
\end{claim}

\begin{proof}
    Whenever $X_i|_{\bbx_{[i-1]}}$ and $X_i|_{a,\bbx_{[i-1]}}$ are $\left(3 \epsilon, \hat{\delta} \right)$-indistinguishable, \Cref{lem:prelims} tells us that $X_i|_{\bbx_{[i-1]}}$ and $X_i|_{a,\bbx_{[i-1]}}$ are point-wise $\left(6 \epsilon, \delta'' \right)$-indistinguishable. \ie given that $X_i|_{\bbx_{[i-1]}}$ and $X_i|_{a,\bbx_{[i-1]}}$ are $\left(3 \epsilon, \hat{\delta} \right)$-indistinguishable, we have that 
$$
\Prob{x_i \sim X_i |_{\bbx_{[i-1]}}}
{\log\dfrac{\Prob{}{X_i = x_i \mid a, \bbx_{[i-1]}}}{\Prob{}{X_i = x_i\mid \bbx_{[i-1]}}} \leq 6 \eps } \geq 1 - \delta''
$$
\end{proof}

\begin{claim}
Let $\hat\delta>0$, $\delta' \defeq
\tfrac{2\delta}{\hat\delta} + \tfrac{2\delta}{1-e^{-\eps}}$, and $\delta'' \defeq \tfrac{2\hat\delta}{1-e^{-3\eps}}$.
If $\cA$ is $(\eps,\delta)$-differentially private and, $\bX \in \univ^n$ is sampled without replacement from a finite universe $\univ$ , then for each $i \in [n]$, and each prefix $\bbx_{[i-1]}\in \cX^{i-1}$ we have:
    $$
    \Prob{\substack{x_i \sim X_i |_{\bbx_{[i-1]}}\\
    a \sim \cA|_{\bbx_{[i-1]}}}}
    {\log\dfrac{\Prob{}{X_i = x_i \mid  a, \bbx_{[i-1]}}}{\Prob{}{X_i = x_i\mid  \bbx_{[i-1]}}} \leq 6 \eps } \geq 1- \delta' - \delta''
    $$
\label{claim:Gi}
\end{claim}

\begin{proof}
For this proof, we use \Cref{claim:Fi} and then show for each $i \in [n]$, and prefix $\bbx_{[i-1]}\in \cX^{i-1}$,
$$
   \Prob{a \sim p\left( \cA|_{\bbx_{[i-1]}}\right) }{X_i|_{\bbx_{[i-1]}} \approx_{3\eps,\hat{\delta}} X_i|_{a, \bbx_{[i-1]}}} \geq 1-\delta'. 
$$

We use the differential privacy guarantee on $\cA$ to show that $(\cA,X_i)|_{\bbx_{[i-1]}} \approx_{\eps,\delta} \cA|_{\bbx_{[i-1]}} \otimes \pj{X_i|_{\bbx_{[i-1]}}}$. The above equation then follows directly from the conditioning lemma \Cref{lem:conditioning}.

Fix any set $\cO \subseteq \cY \times \cX$ and prefix $\bbx_{[i-1]} \in \cX^{i-1}$. From the differential privacy of $\cA$, and the order-invariance of the algorithm, we get the following (where the first inequality follows from DP.):
\ifnum\usenix=0
\begin{align*}
& \prob{(\cA(\bX),X_i )  \in\cO  \mid \bbx_{[i-1]}}\\
&= \sum_{x_i \sim X_i |_{\bbx_{[i-1]}}}\prob{X_i = x_i \mid \bbx_{[i-1]}} \prob{(\cA(\bX),x_i ) \in \cO  \mid \bbx_{[i-1]}, x_i} \\
&\leq \sum_{x_i \sim X_i |_{\bbx_{[i-1]}}}\prob{X_i = x_i\mid \bbx_{[i-1]}}\left( e^{\eps} \prob{(\cA(\bX),x_i ) \in \cO \mid \bbx_{[i-1]}, t_i} + \delta \right)
\qquad \forall t_i \sim X_i |_{\bbx_{[i-1]}}\\
&= \sum_{x_i,t_i \sim X_i |_{\bbx_{[i-1]}}}\prob{X_i = t_i \mid \bbx_{[i-1]}}\prob{X_i = x_i\mid \bbx_{[i-1]}}\left( e^{\eps} \prob{(\cA(\bX),x_i ) \in \cO \mid \bbx_{[i-1]}, t_i} + \delta \right)
\\
&= \sum_{x_i \sim X_i |_{\bbx_{[i-1]}}}\prob{X_i = x_i\mid \bbx_{[i-1]}}\left( e^{\eps} \prob{(\cA(\bX),x_i ) \in \cO \mid \bbx_{[i-1]}} + \delta \right)\\
&\leq e^{\eps}\left(\sum_{x_i \sim X_i |_{\bbx_{[i-1]}}} \prob{X_i = x_i\mid \bbx_{[i-1]}}\prob{\cA(\bX),X_i ) \in \cO \mid \bbx_{[i-1]}}\right) + \delta\\
&= e^{\eps}\prob{\cA(\bX) \otimes X_i   \in \cO  \mid \bbx_{[i-1]}} + \delta
\end{align*}
\else
\begin{align*}
& \prob{(\cA(\bX),X_i )  \in\cO  \mid \bbx_{[i-1]}}\\
&= \sum_{x_i \sim X_i |_{\bbx_{[i-1]}}}\prob{X_i = x_i \mid \bbx_{[i-1]}} \prob{(\cA(\bX),x_i ) \in \cO  \mid \bbx_{[i-1]}, x_i} \\
&\leq \sum_{x_i \sim X_i |_{\bbx_{[i-1]}}}\prob{X_i = x_i\mid \bbx_{[i-1]}} \\
& \qquad \qquad \cdot \left( e^{\eps} \prob{(\cA(\bX),x_i ) \in \cO \mid \bbx_{[i-1]}, t_i} + \delta \right)
\quad \forall t_i \sim X_i |_{\bbx_{[i-1]}}\\
&= \sum_{x_i,t_i \sim X_i |_{\bbx_{[i-1]}}}\prob{X_i = t_i \mid \bbx_{[i-1]}}\prob{X_i = x_i\mid \bbx_{[i-1]}}\\
& \qquad \qquad \cdot \left( e^{\eps} \prob{(\cA(\bX),x_i ) \in \cO \mid \bbx_{[i-1]}, t_i} + \delta \right)
\\
&= \sum_{x_i \sim X_i |_{\bbx_{[i-1]}}}\prob{X_i = x_i\mid \bbx_{[i-1]}}\\
& \qquad \qquad \left( e^{\eps} \prob{(\cA(\bX),x_i ) \in \cO \mid \bbx_{[i-1]}} + \delta \right)\\
&\leq \delta + e^{\eps}\sum_{x_i \sim X_i |_{\bbx_{[i-1]}}} \prob{X_i = x_i\mid \bbx_{[i-1]}}\prob{\cA(\bX),X_i ) \in \cO \mid \bbx_{[i-1]}} \\
&= e^{\eps}\prob{\cA(\bX) \otimes X_i   \in \cO  \mid \bbx_{[i-1]}} + \delta
\end{align*}
\fi

\footnote{In the step where we apply DP, the parameters will double if the algorithm is not order-invariant.}
\pjnote{Add a small note about the coupling in the step where we apply DP}

Applying a very similar argument, will give us that
$$\prob{\cA(\bX) \otimes X_i   \in \cO  \mid \bbx_{[i-1]}}  \leq e^\eps  \prob{(\cA(\bX),X_i) \in \cO \mid \bbx_{[i-1]}}+ \delta.$$
\end{proof}

Having shown a high probability bound on the terms $Z_i$, our next step is to bound their expectation so that we can continue towards our goal of applying Azuma's inequality. 

We will use the following shorthand notation for conditional expectation:
\begin{align*}& \Ex{}{Z_i(\cA,\bX_{[i]})\mid a,\bbx_{[i-1]}, \abs{Z_i}\leq 6\eps} \\ & \quad 
\stackrel{def}{=} \Ex{}{Z_i(\cA,\bX_{[i]})\mid \cA=a,\bX_{[i-1]}=\bbx_{[i-1]}, \abs{Z_i(\cA,\bX_{[i]})}\leq 6\eps}, 
\end{align*}

\begin{lemma}\label{lem:exp_Z}
Let $\cA$ be $(\eps,\delta)$-differentially private and, $\bX \in \univ^n$ be sampled without replacement from a finite universe $\univ$ . Let $\eps \in (0,1/2]$ and $\hat{\delta} \in \left (0,\eps/15 \right ]$,
\ifnum\usenix=0
\begin{equation*}
{X_i|_{\bbx_{[i-1]}} \approx_{3\eps,\hat{\delta}} X_i|_{a, \bbx_{[i-1]}}}\quad \implies  \qquad \Ex{}{Z_i(\cA,\bX_{[i]})\mid a,\bbx_{[i-1]}, \abs{Z_i}\leq 6\eps} = O(\eps^2 + \hat\delta).
\end{equation*}
\else
\begin{align*}
X_i|_{\bbx_{[i-1]}} & \approx_{3\eps,\hat{\delta}}  X_i|_{a, \bbx_{[i-1]}} \\
& => \Ex{}{Z_i(\cA,\bX_{[i]})\mid a,\bbx_{[i-1]}, \abs{Z_i}\leq 6\eps} = O(\eps^2 + \hat\delta).
\end{align*}
\fi

More precisely, $\Ex{}{Z_i(\cA,\bX_{[i]})\mid a,\bbx_{[i-1]}, \abs{Z_i}\leq 6\eps} \leq \nu (\hat{\delta})$, where $\nu (\hat{\delta})$ is defined in \eqref{eqn: nu}.
\end{lemma}

\begin{proof}
\pj{Let $S \defeq \{x_i\mid a,\bbx_{[i-1]},\abs{Z_i}<6\eps\}$.} Given an outcome and prefix $(a,\bbx_{[i-1]})$ such that ${X_i|_{\bbx_{[i-1]}} \approx_{3\eps,\hat{\delta}} X_i|_{a, \bbx_{[i-1]}}}$, we have the following by definition:
\begin{align*}
    &\Ex{}{Z_i(\cA,\bX_{[i]})\mid a,\bbx_{[i-1]}, \abs{Z_i}\leq 6\eps}\\ 
    &=  \sum_{x_i\in S} \prob{X_i = x_i\mid a,\bbx_{[i-1]}, \abs{Z_i}\leq 6\eps} Z_i(a,x_{[i]})\\
    &=  \sum_{x_i\in S} \prob{X_i = x_i\mid a,\bbx_{[i-1]}, \abs{Z_i}\leq 6\eps} \log\left( \tfrac{\prob{X_i = x_i \mid a,\bbx_{[i-1]}}}{\prob{X_i = x_i \mid \bbx_{[i-1]}}} \right)
\end{align*}
\begin{claim}
\ifnum\usenix=0
    $$ \sum_{x_i\in S} \prob{X_i = x_i\mid \bbx_{[i-1]}} \log\left( \tfrac{\prob{X_i = x_i \mid a,\bbx_{[i-1]}}}{\prob{X_i = x_i\mid\bbx_{[i-1]}}} \right) \leq \log\left(\tfrac{1 - \prob{X_i \notin S\mid a,\bbx_{[i-1]}}}{1- \prob{X_i \notin S\mid \bbx_{[i-1]}}} \right)$$
\else
\begin{multline*}
    \sum_{x_i\in S} \prob{X_i = x_i\mid \bbx_{[i-1]}} \log\left( \tfrac{\prob{X_i = x_i \mid a,\bbx_{[i-1]}}}{\prob{X_i = x_i\mid\bbx_{[i-1]}}} \right) \\
    \leq \log\left(\tfrac{1 - \prob{X_i \notin S\mid a,\bbx_{[i-1]}}}{1- \prob{X_i \notin S\mid \bbx_{[i-1]}}} \right)
\end{multline*}
\fi
\end{claim}
\begin{proof}
\begin{align*}
& \sum_{x_i\in S} \prob{X_i = x_i\mid a,\bbx_{[i-1]}} \log\left( \tfrac{\prob{X_i = x_i \mid a,\bbx_{[i-1]}}}{\prob{X_i = x_i\mid\bbx_{[i-1]}}} \right)\\ 
&=\prob{X_i \in S\mid \bbx_{[i-1]}}  \sum_{x_i\in S}\tfrac{\prob{X_i = x_i\mid \bbx_{[i-1]}}}{\prob{X_i \in S\mid \bbx_{[i-1]}}}  \log\left( \tfrac{\prob{X_i = x_i \mid a,\bbx_{[i-1]}}}{\prob{X_i = x_i\mid\bbx_{[i-1]}}} \right)\\
&\leq  \sum_{x_i\in S}\tfrac{\prob{X_i = x_i\mid \bbx_{[i-1]}}}{\prob{X_i \in S\mid \bbx_{[i-1]}}}  \log\left( \tfrac{\prob{X_i = x_i \mid a,\bbx_{[i-1]}}}{\prob{X_i = x_i\mid\bbx_{[i-1]}}} \right)\\
&\leq \log\left( \sum_{x_i\in S}\tfrac{\prob{X_i = x_i \mid a,\bbx_{[i-1]}}}{\prob{X_i \in S\mid \bbx_{[i-1]}}}\right)\\
&\leq \log\left(\tfrac{\prob{X_i \in S \mid a,\bbx_{[i-1]}}}{\prob{X_i \in S\mid \bbx_{[i-1]}}}\right) = \log\left(\tfrac{1 - \prob{X_i \notin S \mid a,\bbx_{[i-1]}}}{1- \prob{X_i \notin S\mid \bbx_{[i-1]}}}\right)\\
\end{align*}
%
The first inequality follows form the fact that all probabilities are less than one. The second inequality follows from noticing that $ \sum_{x_i\in S}\frac{\prob{X_i = x_i\mid \bbx_{[i-1]}}}{\prob{X_i \in S\mid \bbx_{[i-1]}}} = 1$ and applying Jensen's inequality. 
\end{proof}
%
\pj{Let $\prob{X_i \notin  \{x_i\mid a,\bbx_{[i-1],\abs{Z_i}<6\eps}\} \mid a,\bbx_{[i-1]}} = \prob{X_i \notin S \mid a,\bbx_{[i-1]}} \defeq q$.} Note that, because ${X_i|_{\bbx_{[i-1]}} \approx_{3\eps,\hat{\delta}} X_i|_{a, \bbx_{[i-1]}}}$, we have for $\hat\delta>0$:
$$
\prob{X_i \notin S\mid \bbx_{[i-1]}} \leq e^{3\eps} \prob{X_i \notin S \mid a,\bbx_{[i-1]}} + \hat\delta = e^{3\eps}q + \hat\delta
$$
%
Note that $q \leq \delta''$ by \Cref{claim:Fi}. Now, we can bound the following:
\begin{align*}
\sum_{x_i \in  S} & \prob{X_i = x_i \mid \bbx_{[i-1]}}  \log\left(\tfrac{\prob{X_i = x_i \mid a,\bbx_{[i-1]}}}{\prob{X_i = x_i \mid \bbx_{[i-1]}}} \right)\\
&\leq \log\left(\tfrac{1 - \prob{X_i \notin S \mid a,\bbx_{[i-1]}}}{1- \prob{X_i \notin S\mid \bbx_{[i-1]}}}\right)\\
&\leq \log(1-q) - \log(1- (e^{3\eps}q + \hat{\delta})  ) \\
& \leq \log(e) \cdot(- q+ e^{3\eps}q + \hat{\delta} + 2(e^{3\eps}q + \hat{\delta})^2)\\
&= \log(e) \cdot ( (e^{3\eps}-1)q + \hat{\delta} + 2(e^{3\eps}q + \hat{\delta})^2 )\\
&\defeq \tau(\hat\delta)
\end{align*}
where the second inequality follows by using the inequality $(-x - 2x^2)\log(e) \leq \log(1-x) \leq -x\log(e)$ for $0< x \leq 1/2$, and as $(e^{3\eps}q + \hat{\delta}) \leq 1/2$ for $\eps$ and $\hat\delta$ bounded as in the lemma statement.

We use the results above to to upper bound the expectation we wanted:
\ifnum\usenix=0
{\footnotesize
\allowdisplaybreaks[2]
\begin{align*}
& \Ex{}{Z_i(\cA,\bX_{[i]})\mid a,\bbx_{[i-1]}, \abs{Z_i}\leq 6\eps} \\
& \leq \sum_{x_i \in  S} \prob{X_i = x_i\mid a,\bbx_{[i-1]},\abs{Z_i}\leq 6\eps} \log\left( \tfrac{\prob{X_i = x_i \mid a,\bbx_{[i-1]}}}{\prob{X_i = x_i  \mid \bbx_{[i-1]}}} \right)  \\
& \qquad - \sum_{x_i \in  S} \prob{X_i = x_i  \mid \bbx_{[i-1]}} \log\left(\tfrac{\prob{X_i = x_i \mid a,\bbx_{[i-1]}}}{\prob{X_i = x_i  \mid \bbx_{[i-1]}}} \right) + \tau(\hat\delta) \\
& = \sum_{x_i \in  S} \left(\prob{X_i = x_i\mid a,\bbx_{[i-1]},\abs{Z_i}\leq 6\eps} - \prob{X_i = x_i  \mid \bbx_{[i-1]}}\right) \log\left( \tfrac{\prob{X_i = x_i \mid  a,\bbx_{[i-1]}}}{\prob{X_i = x_i  \mid \bbx_{[i-1]}}} \right)+ \tau(\hat\delta) \\
&\leq_{|Z_i| \leq 6\eps } 6\eps \sum_{x_i \in  S} \abs{\prob{X_i = x_i\mid a,\bbx_{[i-1]},\abs{Z_i}\leq 6\eps} - \prob{X_i = x_i  \mid \bbx_{[i-1]}}}+ \tau(\hat\delta) \\
& \leq_{\text{Def of $S$,  Claim~\ref{claim:zibound}}} 6\eps \sum_{x_i \in  S} \prob{X_i = x_i  \mid \bbx_{[i-1]}} \max\Bigg\{ \dfrac{e^{6\eps} }{\prob{|Z_i| \leq 6\eps \mid a,\bbx_{[i-1]}}} - 1, 1 - \dfrac{e^{-6\eps} }{\prob{|Z_i| \leq 6\eps \mid a,\bbx_{[i-1]}}}  \Bigg\} + \tau(\hat\delta) \\ 
& \leq_{\text{Claim~\ref{claim:zibound}}} 6\eps \left(\tfrac{e^{6\eps}}{1- \tfrac{2\hat\delta}{1-e^{-3\eps}}} - 1\right) + \tau(\hat\delta)\\
&\leq_{\text{Substituting for $\tau(\hat\delta)$}} 6\eps\left(e^{6\epsilon} \left( 1+ \tfrac{4\hat\delta}{1-e^{-3\epsilon} } \right) - 1 \right) + \log(e) \cdot ( (e^{3\eps}-1)q + \hat{\delta} + 2(e^{3\eps}q + \hat{\delta})^2 ) \\
& \leq_{\text{Upper bound for $q$}} 
6\eps\left(e^{6\epsilon} \left( 1+ \tfrac{4\hat\delta}{1-e^{-3\epsilon} } \right) - 1 \right) \\
& \qquad + \log(e) \cdot \left( (e^{3\eps}-1)\frac{2 \hat{\delta}}{1-e^{-3\epsilon}} + \hat{\delta} + 2 \hat{\delta}^2 + 8\frac{\hat{\delta}^2 e^{6 \eps}}{(1-e^{-3\eps})^2} + 8\frac{\hat{\delta}^2 e^{3 \eps}}{1-e^{-3\eps}} \right) \\
& =_{ b = \frac{\hat{\delta}}{1-e^{-3\eps}} } 
6\eps\left(e^{6\epsilon} \left( 1+ 4b \right) - 1 \right) + \log(e) \cdot \left( b \left( 2e^{3\eps} - 2  + 8\frac{\hat{\delta} e^{6 \eps}}{(1-e^{-3\eps})} + 8\hat{\delta} e^{3 \eps}\right) + \hat{\delta} + 2\hat{\delta}^2 \right) \\
& = 
b \left( 24\eps e^{6\eps} + 2e^{3\eps} - 2  + 8\frac{\hat{\delta} e^{6 \eps}}{(1-e^{-3\eps})} + 8\hat{\delta} e^{3 \eps}\right) + \hat{\delta} + 2\hat{\delta}^2 + 6\eps(e^{6\eps} - 1) \\
& = \frac{\hat{\delta}}{1-e^{-3\eps}}  \left( 2e^{3 \eps} (4e^{3\eps}(3\eps + \frac{\hat{\delta}}{1-e^{-3\eps}} )) + 4\hat{\delta} + 1) - 2 \right) + \hat{\delta} + 2\hat{\delta}^2 + 6\eps(e^{6\eps} - 1) \\
& \leq_{{e^{-3\eps}} \leq 1-1.5\eps \text{ for } \eps \in [0,0.5]}
2 \frac{\hat{\delta}}{1.5\eps} e^{3 \eps} \left (4e^{3\eps}(3\eps + \frac{\hat{\delta}}{1.5\eps})) + 4\hat{\delta} + 1\right) + \hat{\delta} \left( \frac{-2}{1.5\eps} + 2\hat{\delta} + 1 \right) + 6\eps(e^{6\eps} - 1) \\
& \leq
8\frac{e^{6\eps} \hat{\delta}}{1.5\eps} \left (3\eps + \frac{\hat{\delta}}{1.5\eps} \right) + 2 \frac{\hat{\delta}}{1.5\eps} e^{3 \eps} \left( 4\hat{\delta} + 1\right) +  \hat{\delta} \left( \frac{-2}{1.5\eps} + 2\hat{\delta} + 1 \right) + 6\eps(e^{6\eps} - 1) \\ 
& \leq_{{e^{3\eps}} \leq 1+7\eps, {e^{6\eps}} \leq 1+40\eps \text{ for } \eps \in [0,0.5]} 8\frac{(1+40\eps) \hat{\delta}}{1.5\eps} \left (3\eps + \frac{\hat{\delta}}{1.5\eps} \right) + 2 \frac{(1+7\eps)\hat{\delta}}{1.5\eps} \left( 4\hat{\delta} + 1\right) \\
& \qquad +   \hat{\delta} \left( \frac{-2}{1.5\eps} + 2\hat{\delta} + 1 \right) + 6\eps(40\eps) \\
& \leq \frac{(8+320\eps) \hat{\delta}}{1.5\eps} \left (3\eps + \frac{\hat{\delta}}{1.5\eps} \right) + \frac{(2+14\eps)}{1.5\eps} \left( 4\hat{\delta}^2 + \hat{\delta}\right) - \frac{2\hat{\delta}}{1.5\eps} + 2\hat{\delta}^2 + \hat{\delta} + 240\eps^2\\
& \leq_{\eps < 0.5}  \frac{168\hat{\delta}}{1.5\eps}(3\eps + \frac{\hat{\delta}}{1.5\eps} ) + \frac{8}{1.5}\frac{\hat{\delta}^2}{\eps} + \frac{56}{1.5}\hat{\delta}^2 + \frac{14}{1.5}\hat{\delta} + 2\hat{\delta}^2 + \hat{\delta} + 240\eps^2 \\
& \leq_{\eps \leq 0.5} 347\hat{\delta} + 75 \left(\frac{\hat{\delta}}{\eps} \right)^2 + 24\frac{\hat{\delta}^2}{\eps}+ 240\eps^2\\
& \defeq \nu(\hat\delta) \numberthis \label{eqn: nu}
\end{align*} 
}
\else

\begin{align*}
& \Ex{}{Z_i(\cA,\bX_{[i]})\mid a,\bbx_{[i-1]}, \abs{Z_i}\leq 6\eps} \\
& \leq \sum_{x_i \in  S} \prob{X_i = x_i\mid a,\bbx_{[i-1]},\abs{Z_i}\leq 6\eps} \log\left( \tfrac{\prob{X_i = x_i \mid a,\bbx_{[i-1]}}}{\prob{X_i = x_i  \mid \bbx_{[i-1]}}} \right)  \\
& \qquad - \sum_{x_i \in  S} \prob{X_i = x_i  \mid \bbx_{[i-1]}} \log\left(\tfrac{\prob{X_i = x_i \mid a,\bbx_{[i-1]}}}{\prob{X_i = x_i  \mid \bbx_{[i-1]}}} \right) + \tau(\hat\delta) \\
& = \sum_{x_i \in  S} \left(\prob{X_i = x_i\mid a,\bbx_{[i-1]},\abs{Z_i}\leq 6\eps} - \prob{X_i = x_i  \mid \bbx_{[i-1]}}\right) \\
& \qquad \cdot \log\left( \tfrac{\prob{X_i = x_i \mid  a,\bbx_{[i-1]}}}{\prob{X_i = x_i  \mid \bbx_{[i-1]}}} \right)+ \tau(\hat\delta) \\
&\leq_{|Z_i| \leq 6\eps } 6\eps \sum_{x_i \in  S} \abs{\prob{X_i = x_i\mid a,\bbx_{[i-1]},\abs{Z_i}\leq 6\eps} \\
& \qquad \qquad \qquad - \prob{X_i = x_i  \mid \bbx_{[i-1]}}}+ \tau(\hat\delta) \\
& \leq_{\text{Definition of $S$, Proof of Claim~\ref{claim:zibound}}} 6\eps \sum_{x_i \in  S} \prob{X_i = x_i  \mid \bbx_{[i-1]}} \\
& \qquad \cdot \max\Bigg\{ \dfrac{e^{6\eps} }{\prob{|Z_i| \leq 6\eps \mid a,\bbx_{[i-1]}}} - 1 \\
& \qquad, 1 - \dfrac{e^{-6\eps} }{\prob{|Z_i| \leq 6\eps \mid a,\bbx_{[i-1]}}}  \Bigg\} + \tau(\hat\delta) \\ 
& \leq_{\text{Claim~\ref{claim:zibound}}} 6\eps \left(\tfrac{e^{6\eps}}{1- \tfrac{2\hat\delta}{1-e^{-3\eps}}} - 1\right) + \tau(\hat\delta)\\
&\leq_{\text{Substituting for $\tau(\hat\delta)$}} 6\eps\left(e^{6\epsilon} \left( 1+ \tfrac{4\hat\delta}{1-e^{-3\epsilon} } \right) - 1 \right) \\
& \qquad + \log(e) \cdot ( (e^{3\eps}-1)q + \hat{\delta} + 2(e^{3\eps}q + \hat{\delta})^2 ) \\
& \leq_{\text{Upper bound for $q$}} 
6\eps\left(e^{6\epsilon} \left( 1+ \tfrac{4\hat\delta}{1-e^{-3\epsilon} } \right) - 1 \right) \\
& \qquad + \log(e) \cdot \Bigg( (e^{3\eps}-1)\frac{2 \hat{\delta}}{1-e^{-3\epsilon}} + \hat{\delta} + 2 \hat{\delta}^2 \\
& \qquad + 8\frac{\hat{\delta}^2 e^{6 \eps}}{(1-e^{-3\eps})^2} + 8\frac{\hat{\delta}^2 e^{3 \eps}}{1-e^{-3\eps}} \Bigg) \\
& =_{ b = \frac{\hat{\delta}}{1-e^{-3\eps}} } 
6\eps\left(e^{6\epsilon} \left( 1+ 4b \right) - 1 \right) + \log(e) \cdot \\
& \qquad \qquad \left( b \left( 2e^{3\eps} - 2  + 8\frac{\hat{\delta} e^{6 \eps}}{(1-e^{-3\eps})} + 8\hat{\delta} e^{3 \eps}\right) + \hat{\delta} + 2\hat{\delta}^2 \right) \\
& = 
b \left( 24\eps e^{6\eps} + 2e^{3\eps} - 2  + 8\frac{\hat{\delta} e^{6 \eps}}{(1-e^{-3\eps})} + 8\hat{\delta} e^{3 \eps}\right) + \hat{\delta} \\
& \qquad +  2\hat{\delta}^2 + 6\eps(e^{6\eps} - 1) \\
& = \frac{\hat{\delta}}{1-e^{-3\eps}}  \left( 2e^{3 \eps} (4e^{3\eps}(3\eps + \frac{\hat{\delta}}{1-e^{-3\eps}} )) + 4\hat{\delta} + 1) - 2 \right) \\
& \qquad + \hat{\delta} + 2\hat{\delta}^2 + 6\eps(e^{6\eps} - 1) \\
& \leq_{{e^{-3\eps}} \leq 1-1.5\eps \text{ for } \eps \in [0,0.5]}
2 \frac{\hat{\delta}}{1.5\eps} e^{3 \eps} \left (4e^{3\eps}(3\eps + \frac{\hat{\delta}}{1.5\eps})) + 4\hat{\delta} + 1\right) \\
& \qquad + \hat{\delta} \left( \frac{-2}{1.5\eps} + 2\hat{\delta} + 1 \right) + 6\eps(e^{6\eps} - 1) 
\end{align*}
\begin{align*}
\leq & 8\frac{e^{6\eps} \hat{\delta}}{1.5\eps} \left (3\eps + \frac{\hat{\delta}}{1.5\eps} \right) + 2 \frac{\hat{\delta}}{1.5\eps} e^{3 \eps} \left( 4\hat{\delta} + 1\right) \\
& \qquad +  \hat{\delta} \left( \frac{-2}{1.5\eps} + 2\hat{\delta} + 1 \right) + 6\eps(e^{6\eps} - 1) \\ 
& \leq_{{e^{3\eps}} \leq 1+7\eps, {e^{6\eps}} \leq 1+40\eps \text{ for } \eps \in [0,0.5]} 8\frac{(1+40\eps) \hat{\delta}}{1.5\eps} \left (3\eps + \frac{\hat{\delta}}{1.5\eps} \right) \\
& \qquad + 2 \frac{(1+7\eps)\hat{\delta}}{1.5\eps} \left( 4\hat{\delta} + 1\right) + \hat{\delta} \left( \frac{-2}{1.5\eps} + 2\hat{\delta} + 1 \right) + 6\eps(40\eps) \\
& \leq \frac{(8+320\eps) \hat{\delta}}{1.5\eps} \left (3\eps + \frac{\hat{\delta}}{1.5\eps} \right) + \frac{(2+14\eps)}{1.5\eps} \left( 4\hat{\delta}^2 + \hat{\delta}\right) \\
& \qquad - \frac{2\hat{\delta}}{1.5\eps} + 2\hat{\delta}^2 + \hat{\delta} + 240\eps^2 \\
& \leq_{\eps < 0.5}  \frac{168\hat{\delta}}{1.5\eps}(3\eps + \frac{\hat{\delta}}{1.5\eps} ) + \frac{8}{1.5}\frac{\hat{\delta}^2}{\eps} + \frac{56}{1.5}\hat{\delta}^2 \\
& + \frac{14}{1.5}\hat{\delta} + 2\hat{\delta}^2 + \hat{\delta} + 240\eps^2 \\
& \leq_{\eps \leq 0.5} 347\hat{\delta} + 75 \left(\frac{\hat{\delta}}{\eps} \right)^2 + 24\frac{\hat{\delta}^2}{\eps}+ 240\eps^2,
%%%%%%%%%%%%
\end{align*}
\fi
\end{proof}

Finally, we need to apply Azuma's inequality (stated in \Cref{thm:azuma}) to a set of variables that are bounded with probability $1$, not just with high probability. Towards this end, we now define (1) the sets $\cG_i(\hat\delta)$ and $\cG_{\leq i}(\hat\delta)$ of ``good'' tuples of outcomes and databases, and (2) a variable $T_i$ that will match $Z_i$ for ``good events'', and will be zero otherwise---and hence, is always bounded:
\begin{align}
\cG_i(\hat\delta) = \left\{(a,\bbx_{[i]}) \,\,\Big|\quad  | Z_i(a,\bbx_{[i]}) | \leq 6 \eps \quad \& \quad X_i\mid_{\bbx_{[i-1]}} \approx_{3\eps,\hat{\delta}} X_i|_{a, \bbx_{[i-1]}}   \right\},
\label{eq:G}\\
\cG_{\leq i}(\hat\delta)   = \left\{(a,\bbx_{[i]}) : (a,x_1) \in \cG_1(\hat\delta), \cdots, (a,\bbx_{[i]}) \in \cG_i(\hat\delta) \right\} \label{eq:Gvect}
\end{align}
\begin{equation}
T_i(a,\bbx_{[i]}) = \begin{cases}
             Z_i(a,\bbx_{[i]})  & \text{if }   
             (a,\bbx_{[i]}) \in \cG_{\leq i}(\hat\delta) \\
             0  & \text{otherwise }
       \end{cases}
\label{eq:Ti}
\end{equation}

Note that the variables $T_i$ indeed satisfy the requirements of Azuma's inequality. The first condition, $\Prob{}{\abs{T_i(\cA,\bX_{[i]})} \leq 6 \eps} = 1$ holds by definition, and the second holds because of Lemma~\ref{lem:exp_Z}.

We are now ready to prove our main theorem.
\begin{proof}[Proof of \Cref{thm:approx-dp-implies-max-info}]
For any constant $\nu$, we have:
\begin{align*}
& \Prob{}{\sum\limits_{i=1}^n Z_i(\cA,\bX_{[i]})  > n\nu + 6t\eps\sqrt{n}} \\%=  
& \leq \Prob{}{\sum\limits_{i=1}^n Z_i(\cA,\bX_{[i]}) > n\nu + 6t\eps\sqrt{n} \cap (\cA,\bX) \in \cG_{\leq n}(\hat\delta)}+ \Prob{}{(\cA,\bX) \notin \cG_{\leq n}(\hat\delta)} \\
& =  \Prob{}{\sum\limits_{i=1}^n T_i(\cA,\bX_{[i]}) > n\nu + 6t\eps\sqrt{n} \cap (\cA,\bX) \in  \cG_{\leq n}(\hat\delta) } + \Prob{}{(\cA,\bX) \notin  \cG_{\leq n}(\hat\delta)}
\end{align*}
We then substitute $\nu$ by $\nu(\hat\delta)$ as defined in \Cref{eqn: nu}, and apply a union bound on $\prob{(\cA,\bX) \notin  \cG_{\leq n}(\hat\delta)}$ using \Cref{claim:Gi} to get
\begin{align*}
 \Prob{}{\sum\limits_{i=1}^n Z_i(\cA,\bX_{[i]})  > n\nu(\hat\delta) + 6t\eps\sqrt{n}} & \leq \Prob{}{\sum\limits_{i=1}^n T_i(\cA,\bX_{[i]}) > n\nu(\hat\delta) + 6t\eps\sqrt{n}  } + n(\delta' + \delta'') \\
&   \leq e^{-t^2/2} + n(\delta' + \delta'')
\end{align*}
where the two inequalities follow from \Cref{claim:Gi} and \Cref{thm:azuma}, respectively.  Therefore,
\begin{align*}
\Prob{ }{Z(\cA(\bX),\bX) > n\nu(\hat\delta) + 6t\eps\sqrt{n}} \leq e^{-t^2/2} + n(\delta' + \delta'')  \stackrel{def}{=} \beta(t,\hat\delta)
\end{align*}

From \Cref{lem:boundmaxinfo}, we have
$I^{\beta(t,\hat\delta)}_\infty(\bX;\cA(\bX)) \leq n\nu(\hat\delta)
+ 6t\eps\sqrt{n}.$  

\end{proof}

\noindent
We now prove the \Cref{cor:approx-dp-implies-max-info}, which we use in \Cref{sec:dp-implies-coherence-enforcement} to prove \Cref{thm:approx-dp-implies-coherence-enforcement}.

\begin{proof}[Proof of~\Cref{cor:approx-dp-implies-max-info}]
    Setting $t = \sqrt{2 \ln(2/\gamma)}$, and $\hat{\delta} = \frac{\sqrt{\eps \delta}}{15}$, in Theorem~\ref{thm:approx-dp-implies-max-info}, we get that $\beta(t,\hat{\delta}) \leq \gamma/2 + 30n\sqrt{\delta/\eps} + n\frac{2\sqrt{\eps \delta}+2\delta}{1.5\eps}$, where we've used that $1-e^{-3\eps} \geq 1.5\eps$ for $\eps \in (0,1/2]$. We note that for $\delta \leq \frac{\eps^2\gamma^2}{(120n)^2}$, we have that $n\frac{2\sqrt{\eps \delta}+2\delta}{1.5\eps} \leq \frac{\gamma}{2}$, ensuring that $\beta(t,\hat{\delta}) \leq \gamma$. Also, the same bound on $\delta$ ensures that $n\left( 347\hat{\delta} + 75 \left(\frac{\hat{\delta}}{\eps} \right)^2 + 24\frac{\hat{\delta}^2}{\eps}+ 240\eps^2\right) \leq   265\eps^2n$. Substituting for $t$ directly in the max-information bound then completes the proof. 
\end{proof}
