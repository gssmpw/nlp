\section{Comparison to Perfect Generalization}

\pj{In this section we include a comparison between 
\emph{demographic coherence} (\Cref{def:coherence})
and the notion of \emph{perfect generalization} introduced by Cummings et al.~\cite{CummingsLNRW16}. (See also the work of Bassily and Freund~\cite{BassilyF16} that independently introduced a generalization thereof.) This notion was originally meant to capture generalization under post-processing but has since been shown to be closely related to other desirable properties  as well (\eg replicability~\cite{BunGHILPSS23}). Our framework shares conceptual similarities with this definition, but the technical details differ in important ways.

The following comparison uses the definition of  \emph{sample perfect generalization} (\Cref{def:sample-perfect-generalization}) from~\cite{BunGHILPSS23} which is roughly equivalent to the original definition from~\cite{CummingsLNRW16}.} Intuitively, a mechanism $M$ running on i.i.d. samples from some distribution (sample) perfectly generalizes if the distribution of its output does not depend ``too much'' on specific realization of its sampled input. That is, its output distributions when run on two i.i.d samples from any distribution are indistinguishable.

\begin{definition}[Sample perfect generalization {\cite[{Def~3.4}]{BunGHILPSS23}} ]\label{def:sample-perfect-generalization}
An algorithm $\cA : \cX^m \to \cY$ is said to be $(\beta, \epsilon, \delta)$-sample perfectly generalizing if, for every distribution $\mathcal{D}$ over $\cX$, with probability at least $1 - \beta$ over the draw of two i.i.d. samples $X_a, X_b \sim \mathcal{D}^m$,
\[
\cA(X_a) \approx_{\epsilon, \delta} \cA(X_b),
\]
where $\approx_{\epsilon, \delta}$ denotes $\epsilon,\delta$ indistinguishability.
\end{definition}

\Cref{def:coherence} has several noticeable syntactic differences when compared to \Cref{def:sample-perfect-generalization}. First, demographic coherence is defined within a specific framework that explicitly lays out the entire data release pipeline, a design choice that intentionally lends itself to concrete intuition (and experimental evaluation) of data release. However, this still leaves open the possibility that the core statistical guarantee of demographic coherence is roughly equivalent to perfect generalization. In other words, it may still be the case that demographic coherence is simply a different way to describe the protections offered by perfect generalization; as we see below, this is not the case.

Second, \pj{while ``closeness'' in the definition of perfect generalization is required for distributions over the entire sets $X_a, X_b$, 
the ``closeness'' in the definition of demographic coherence is required for distributions over the sets $X_a|_C,X_b|_C$ for subpopulations $C \subseteq \univ$ from some collection $\cC$. For the sake of drawing a more direct comparison here, we collapse this difference by comparing sample perfect generalization to demographic coherence with $\cC=\{\univ\}$.}

\pj{A third difference in the definitions is is the choice of sets $X_a,X_b$ that the comparison is made with respect to, \ie a random partition of a fixed dataset in demographic coherence vs. i.i.d. draws from a distribution in the case of perfect generalization. The choice of random partitioning in our framework is made to ensure concreteness and applicability in census-like settings but it is chosen intentionally to maintain both intuitive and quantitative similarities to i.i.d. sampling. Thus, we view this as more of a difference in interpretability and applicability of the definitions, rather than one about their underlying guarantees.}

The main difference between the two definitions, thus, is in how how ``closeness'' is measured---as spelled out in \Cref{fig:coherence-generalization}. 

\begin{figure*}[ht!]
\begin{center}
\fcolorbox{black}{cyan!3}{
\small
\hbox{
\begin{minipage}{0.944\textwidth}
\vspace{0.3em}
An algorithm $\alg:\cX^{n/2}\to \cY$ is:
\begin{enumerate}
    \item \emph{$(\beta, \eps, \delta)$-sample perfectly generalizing if}

    $\forall$ distributions $\cD$ over $\cX$, with probability at least $1-\beta$ over $X_a,X_b \sim \cD^{n/2}$: 
    $$\alg(X_a) \approx_{\eps,\delta} \alg(X_b)$$

    \item \emph{$(\alpha,\beta)$-coherence enforcing if}

    $\forall$ datasets $X \in \cX^n$, learners $\cL:\cY \to (\cX \to [-1,1])$, with probability at least $1-\beta$ over the random split $X_a \cup X_b = X$ and the coins of $\alg$, $\cL$:
    $$\dist_{W}(h_a(X_a), h_a(X_b)) \leq \alpha$$
    where $h_a \leftarrow \cL\circ\cA(X_a)$ is a confidence rated predictor, and $h_a(X_i)$ is the distribution induced by randomly choosing $x\sim X_i$ and computing $h_a(x)$.
\end{enumerate}
\vspace{0.4em}
\end{minipage}
}
}
\end{center}
\caption{Comparing the definition of sample perfect generalization to a simplified definition of demographic coherence.}
\label{fig:coherence-generalization}
\end{figure*}


Perfect generalization asks that w.h.p. over independent samples, $\alg$ produces indistinguishable distributions over reports $R_a \leftarrow \alg(X_a)$ and $R_b \leftarrow \alg(X_b)$. Meanwhile, coherence enforcement asks that w.h.p. $\cL\circ\alg(X_a)$ produces a confidence rated predictor $h_a:\cX \to [-1,1]$ which has ``similar'' predictions on $X_a$ and $X_b$ (a property enforced by $\alg$). \pj{That is, the comparison in perfect generalization is on the behavior of the algorithm $\cA$ itself, while the comparison in demographic coherence is on the likely behavior of a realized hypothesis $h_a$ that is produced only over the report $R_a \leftarrow \alg(X_a)$.} 

Since coherence enforcement limits the set of algorithms against which indistinguishability applies, one might expect that perfectly generalizing algorithms also enforce demographic coherence, and indeed, \cref{thm:max-info-implies-demographic-coherence} proves this to be true. However, the converse need not be true---implying that demographic coherence is a relaxation of perfect generalization. 
% This is because coherence enforcement only requires $\alg$ to enforce all confidence rated predictors $h \leftarrow \cL\circ\alg(X_a)$ to induce `similar' distributions on $X_a$ and $X_b$. 
In particular, the example below shows a set $X$ such that no confidence rated predictor $h:\cX \to [-1,1]$ violates this property. In this case, all data curators are vacuously coherence enforcing.
 
Consider a data curator $\alg:\zo^{n/2}\to\zo^{n/2}$ that (deterministically) publishes its input in the clear. This is clearly not perfectly generalizing as the distribution of the report $X_a$ is a point mass that (for reasonable choices of $X$, with high probability) is distinct from the distribution of the report $X_b$.

Meanwhile, considering a dataset $X\in\zo^n$ and a random split $X_a,X_b \leftarrow X$ of the dataset, there are two possible predictors $h:\zo \to [-1,1]$ that witnesses the highest possible Wasserstein distance when run on $X_a$ vs. $X_b$. That is, without loss of generality $h$ is either $h(0)=-1, h(1) = 1$ or $h(0) = 1, h(1) = -1$. In either case, $h$ cannot be improved even by seeing $X_a$ in the clear. So, any data curation algorithm in this scenario, is coherence enforcing since the data itself doesn't have enough complexity to allow for a violation of demographic coherence. \pj{Note that the absence of information correlated with the bits contained in the dataset $X$ (\eg time, location, computer system) is crucial to this example.}


