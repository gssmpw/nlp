\section{Algorithms that Enforce Wasserstein Demographic Coherence} \label{sec:acheivingdc}

Now, we show that Wasserstein coherence enforcement is instantiable. Firstly, in \Cref{sec:techprelim}, we go over some technical preliminaries necessary to state and prove our results, Then, in \Cref{sec:max-info-implies-demographic-coherence}, we prove \Cref{thm:max-info-implies-demographic-coherence} showing that algorithms with bounded max-information are coherence enforcing. Building on this, in \Cref{sec:dp-implies-coherence-enforcement}, we leverage the connection between differential privacy and max-information to prove Theorem~\ref{thm:pure-dp-implies-coherence-enforcement} which says that pure-DP algorithms enforce demographic coherence, and \Cref{thm:approx-dp-implies-coherence-enforcement} which is says that approx-DP algorithms enforce demographic coherence as well.

\subsection{Technical Preliminaries}\label{sec:techprelim}

For a recap of the notation used, see \Cref{sec:notation}.

\begin{definition}[Differential Privacy \cite{DworkMNS16j}]
Let $n\in\mathbb{N}$. A randomized algorithm $\alg:\mathcal{X}^n \to \cY$ is $(\epsilon, \delta)$-\emph{differentially private} if for all subsets $Y \subseteq \cY$ of the output space, and for all neighboring datasets $X, X' \in \mathcal{X}^n$ (i.e. $\|X - X' \|_0 \leq 1$), we have that
\begin{align*}
    \Pr[\alg(X) \in Y] \leq e^{\eps} \Pr[\alg(X') \in Y] + \delta
\end{align*}
\end{definition}

If $\delta = 0$, we refer to the algorithm as satisfying pure differential privacy (pure-DP), whereas $\delta > 0$ corresponds to approximate differential privacy (approx-DP).

    \begin{definition}[Max-information of random variables \cite{DworkFHPRR15}] Let $X$ and $Y$ be jointly distributed random variables over the domain $(\cX,\cY)$. The \textit{$\beta$-approximate max-information} between $X$ and $Y$, denoted by $I_\infty^\beta(X;Y)$ is
    $$I_\infty^\beta(X;Y) = \ln \left(\underset{\substack{T\subseteq(\cX\times\cY)\\\Pr[(X,Y)\in T]>\beta}}{\sup}\frac{\Pr[(X,Y)\in T]-\beta}{\Pr[X\otimes Y \in T]}\right)$$
    \end{definition}
    \begin{definition}[Max-information of algorithms \cite{DworkFHPRR15}]
    \footnote{This definition, for sampling without replacement, is slightly different than the original one.}  Fix $n \in \mathbb{N}$, $\beta > 0$. Let $\univ$ be a finite data universe of size $m$. Let $S$ be a sample of size $n$ chosen without replacement from $\univ$. Let $\alg: \univ^{n} \to \cY$ be an algorithm.

    Then we define the max-information of the algorithm as follows:
    \begin{align}
        I^{\beta}_{\infty}(\alg, n) = I^{\beta}_{\infty}(S, \alg(S)))
    \end{align}
    \end{definition}

The following definition of order-invariant algorithms appears as a technical assumption in some of our theorem statements.\footnote{Since it is an assumption in the version of McDiarmid's Inequality for sampling without replacement (\Cref{thm:mcdiarmids_without-our-version}) that we use.} This is a minimal assumption because any non-order-invariant algorithm can be made order-invariant by simply pre-processing the dataset with a sorting or shuffling operation.

\begin{definition}[Order-invariant algorithm]
An algorithm $\alg:\univ^m \to \cY$, is \emph{order invariant} if for all $X \in \cX^m$, the distribution of the random variable $\alg(X)$ does not depend on the order of the elements of $X$. 
\end{definition}

The proofs of the following theorems connecting differential privacy and max-information can be found in Appendix~\ref{app:maxinfo}.
\begin{restatable}{theorem}{puredpmaxinfo}\label{thm:pure-dp-implies-maxinfo}
  \emph{(Pure-DP $\implies$ Bounded Max-Information)}  Fix $n \in \mathbb{N}$, $\eps > 0$ and let $\univ$ be a data universe of size at least $n$.
  % \mb{Don't we want a dataset of size $n$? Not a data universe?} 
  Let $\alg:\cX^{n/2} \to \cY$ be an order-invariant $\eps$-DP algorithm. Then for any $\gamma > 0$,
    $$I^{\gamma}_{\infty}(\alg, n/2) \leq \eps^2n/4 + \eps\sqrt{n\ln(2/\gamma)/4}.$$
\end{restatable}

The following theorem is a generalized version of that in \cite{RogersRST16}. The proof follows theirs, with the following key distinctions: (1) it applies to sampling without replacement (2) It carefully tracks constants and 
(3) It maintains flexibility in setting parameters. We anticipate that this version of the result might be independently useful.

\begin{restatable}{theorem}{approxmaxinfogeneral}\label{thm:approx-dp-implies-max-info}
    \emph{(Approx-DP $\implies$ Bounded Max-Information, Generalised)} Let $\mathcal{A}: \cX^n \to \cY$ be an (order-invariant) $(\eps,\delta)$-differentially
    private algorithm for $\eps \in (0,1/2]$, $\delta \in \left (0,\eps \right )$.
    For $\hat{\delta} \in (0,\eps/15]$, $t>0$, and $\beta(t,\hat\delta) = e^{-t^2/2} + n\left(\frac{2\delta}{\hat{\delta}} + \frac{2\hat{\delta} + 2\delta}{1-e^{-3\eps}}\right)$ we have
    \begin{align*}
        I^\beta_{\infty}(\cA,n)
        &\leq n\left( 347\hat{\delta} + 75 \left(\frac{\hat{\delta}}{\eps} \right)^2 + 24\frac{\hat{\delta}^2}{\eps}+ 240\eps^2\right)
        + 6t\eps\sqrt{n}.
    \end{align*}
\end{restatable}

\begin{restatable}{corollary}{approxmaxinfo}\label{cor:approx-dp-implies-max-info}
   \emph{(Approx-DP $\implies$ Bounded Max-Information, Specific)} Fix $n \in \mathbb{N}$, for $\eps \in (0,1/2]$, $\gamma \in (0,1]$, $\delta \in (0,\frac{\eps^2 \gamma^2}{(120n)^2}]$ and let $\univ$ be a data universe of size at least $n$.
   % \mb{Confused about the role of $m$ here. Are we just using the fact that the data universe is at least the size of the dataset? But there's no actual  dependence on the data universe size in the parameters of the statement? Why do we need this assumption?} 
   Let $\mathcal{A}: \cX^{n} \to \cY$ be an (order-invariant) $(\eps,\delta)$-differentially
    private algorithm. We have that
    \begin{align*}
        I^\gamma_{\infty}(\alg,n)
        &\leq 265\eps^2n + 12\eps\sqrt{n\ln(2/\gamma)}.
    \end{align*}
\end{restatable}

\begin{definition}[Hypergeometric distribution]\label{def:hyper}
Fix $0 < a, s \leq b$. Consider a population of $b$ items of which $a$ items have a special property $P$. Consider $s$ items sampled without replacement from $b$. The distribution of the number of items in $s$ with property $P$ is called the hypergeometric distribution parameterized by $b,a,s$ (denoted by $H(b,a,s)$). 
\end{definition}

\begin{theorem} [\cite{HushS05}]\label{thm:hypgeom}
    Let $K$ have a hypergeometric distribution $H(b, a, s)$. Then for every $\gamma \ge 2$,
\ifnum\usenix=1
    \begin{multline*}
        \Pr\left[K > s\frac{a}{b} + \gamma \right] < e^{-2 c(\gamma^2 - 1)}  \text{ and }\\
        \Pr \left[K < s\frac{a}{b} - \gamma \right]  < e^{-2 c(\gamma^2 - 1)}
    \end{multline*}
\else
    \begin{align*}
        \Pr[K > s\frac{a}{b} + \gamma] &< e^{-2 c(\gamma^2 - 1)} \\
        \Pr[K < s\frac{a}{b} - \gamma] &< e^{-2 c(\gamma^2 - 1)}
    \end{align*}
\fi
    where
    \[c= \max\left\{\frac{1}{s+1} + \frac{1}{b-s+1}, \frac{1}{a+1} + \frac{1}{b-a+1}\right\}.\]
\end{theorem}



\subsection{Bounded Max-Information Algorithms are Coherence Enforcing}\label{sec:max-info-implies-demographic-coherence}

\ssnote{Maybe point out one thing that makes this theorem and the ones about DP powerful- apply to all lenses.}
 
\begin{theorem}\label{thm:max-info-implies-demographic-coherence}
    Let $n\in\N$,$\zeta>0$, $\beta \in (0,1)$, $\alpha \in (0,1]$. Let $\wdist(\cdot,\cdot)$ represent the Wasserstein-1 distance metric. Consider a collection $\cC$ of sub-populations $C \subseteq \univ$, a lens $\selection$, and an algorithm ${\alg:\univ^{n/2}\to \cY}$ with bounded max-information 
    $$I^{\beta/2|\cC|}_{\infty}(\alg,n/2) < \zeta.$$ 
    
    Then $\alg$ enforces $(\alpha,\beta)$-Wasserstein-coherence with respect to collection $\cC$, lens $\rho$, and size constraint $\gamma$, where
     \begin{align}
        \gamma = \max\Big\{ & \frac{8.3 \cdot (\zeta + \ln(16|\cC|/\beta))}{\alpha^2}, \frac{36\ln(3/\alpha)}{\alpha^2} \nonumber \\
        &, 16.6 \cdot (\zeta + \ln(16|\cC|/\beta)), \frac{5.3}{\alpha}, 80\Big\}.
    \end{align}
    
\end{theorem}

\noindent
The intuition behind the result in \Cref{thm:max-info-implies-demographic-coherence} is that the output of an algorithm with bounded max-information does not contain too much specific information about the input dataset. This intuition is leveraged to prove \Cref{lemma:max-info-implies-demographic-coherence-int}, which is the main lemma underlying this result. \Cref{thm:max-info-implies-demographic-coherence} follows from this lemma by an appropriate setting of parameters.


\begin{lemma}\label{lemma:max-info-implies-demographic-coherence-int}
    Let $\eta,\zeta>0$, $\alpha \in (0,1)$. Let $\wdist(\cdot,\cdot)$ represent the Wasserstein-1 distance metric. Consider a sub-population $C \subseteq \univ$, a lens $\selection$, and an algorithm ${\alg:\univ^{n/2}\to \cY}$ with bounded max-information 
    $$I^\eta_{\infty}(\alg,n/2) < \zeta.$$ 

    \noindent
    For all algorithms $\cL: \cY \to \{ \univ \to [-1,1] \}$, datasets $X\in\cX^n$, $\mu > 0$, as long as 
    \[|X\cap C|\ge \max\left\{\frac{4.15\cdot\ln(4/\mu)}{\alpha^2}, \frac{16/3}{\alpha}, 8.3\cdot\ln(4/\mu), 40\right\},\]   
    we have
\ifnum\usenix=1
   \begin{align*}
    \underset{X_a \leftarrow X, h \leftarrow \cL \circ \alg(X_a)}{\Pr}&\left[\wdist(h(\restrict{X_a\lvert_C}{\rho}), h(\restrict{X_b\lvert_C}{\rho})) > \alpha\right] \\
    & \,\leq\,\, 2\mu(|X\cap C|+1)\cdot e^\zeta + \eta.
    \end{align*} 
\else
 \begin{align*}
    \underset{X_a \leftarrow X, h \leftarrow \cL \circ \alg(X_a)}{\Pr}&\left[\wdist(h(\restrict{X_a\lvert_C}{\rho}), h(\restrict{X_b\lvert_C}{\rho})) > \alpha\right]
    & \,\leq\,\, 2\mu(|X\cap C|+1)\cdot e^\zeta + \eta.
    \end{align*} 
\fi
    Here $X_a$ and $X_b$ denote a random split of the dataset $X$ as in the $\coherenceExp_{\cL\circ\alg,X,\cC^*,\selection}(\alpha)$ experiment in \Cref{fig:coherenceExp}.
\end{lemma}

\vspace{-0.5em}
\medskip\noindent\textbf{Proof sketch of \Cref{lemma:max-info-implies-demographic-coherence-int}:} 
Considering a particular subpopulation $C\subseteq \univ$, 
We need to show for all algorithms $\cL:\cY\to\{\univ \to [-1,1]\}$, all datasets $X\in\cX^n$, $\mu > 0$ that with high probability over a choice of split $X_a, X_b \setrandomly X$, and predictor $h\leftarrow \cL\circ\alg(X_a)$ as in the $\coherenceExp_{\cL\circ\alg,X,\cC^*,\selection}(\alpha)$ experiment in \Cref{fig:coherenceExp}, the following holds:
$$\wdist(h(\restrict{X_a\lvert_C}{\rho}), h(\restrict{X_b\lvert_C}{\rho})) < \alpha.$$
Note that instead of the split $X_a,X_b$ which predictor $h$ depends on, if we consider an independent split $S,\overline{S}\setrandomly X$, then we could hope to use a concentration inequality to get the bound we desire. (The proof of \Cref{lem:unrelated-predictor} below redefines the sampling process in a way that allows us to use a concentration bound of Hush and Scovel (\Cref{thm:hypgeom}) for the hypergeometric distribution to prove such a result.)

With the goal of analysing an independent split, we combine the fact that bounded max-information is preserved under post-processing with the intuition that the output of an algorithm with bounded max-information does not contain too much specific information about the input dataset to decouple the predictive hypothesis $h$ from the dataset $X_a$ in the following way (in \Cref{lem:max-info-decouples-Xa-from-ha}):
\ifnum\usenix=0
\begin{align*}
\wdist(h(\restrict{X_a\lvert_C}{\rho}), h(\restrict{X_b\lvert_C}{\rho})) \approx \wdist(h(\restrict{S\lvert_C}{\rho}), g(\restrict{\overline{S}\lvert_C}{\rho}))
\end{align*}
\else
\begin{multline*}
\wdist(h(\restrict{X_a\lvert_C}{\rho}), h(\restrict{X_b\lvert_C}{\rho})) \\
\approx \wdist(h(\restrict{S\lvert_C}{\rho}), h(\restrict{\overline{S}\lvert_C}{\rho})).
\end{multline*}

\fi

\begin{proof}[Proof of \Cref{lemma:max-info-implies-demographic-coherence-int}]
Fix any arbitrary lens $\rho$. The proof proceeds in two claims. First, in \Cref{lem:max-info-decouples-Xa-from-ha}, we use the definition of max-information to replace $X_a,X_b$ with an independently chosen half-sample $S$ and its complement $\overline{S}=X\setminus S$.

\begin{claim}\label{lem:max-info-decouples-Xa-from-ha}
    Consider $\eta,\alpha,\zeta>0$ and a fixed dataset $X \in \univ^n$. Consider a data-curation algorithm ${\alg:\univ^{n/2}\to\cY}$ with bounded max-information, $I^{\eta}_{\infty,P}(\alg,n/2) \leq \zeta$, and an algorithm $\cL: \cY \to \{ \univ \to [-1,1] \}$ that uses the data report to create a predictor. Independently choose two random half samples $X_a, S\leftarrow X$, and let sets $X_b = X\setminus X_a, \overline{S} = X\setminus S$. Finally let $h \leftarrow \cL(X_a)$. Then, we have that
    \begin{align*}
    &\underset{X_a, h}{\Pr}[\wdist(h(\restrict{X_a\lvert_C}{\rho}), h(\restrict{X_b\lvert_C}{\rho})) > \alpha]\\
    \leq 
    &\underset{S,X_a,h}{\Pr}[\wdist(h(\restrict{S\lvert_C}{\rho}), h(\restrict{\overline{S}\lvert_C}{\rho})) > \alpha]\cdot e^{\zeta} + \eta
    \end{align*}
\end{claim}

\noindent Then, in \Cref{lem:unrelated-predictor}, we bound $\Pr[\wdist\left(g(\restrict{S\lvert_C}{\rho}),h\left(\restrict{\overline{S}\lvert_C}{\rho}\right)\right) > \alpha]$ for any confidence rated predictor $g:\univ \to [-1,1]$ that is produced independently of $S$.
% 
\begin{claim}\label{lem:unrelated-predictor}
Let $\alpha \in (0, 1)$, let $S$ be a sample of size $n/2$ drawn uniformly without replacement from $X$, let $\overline{S} = X\setminus S$,
and let $g:\univ \to [-1,1]$ be any confidence rated predictor.
\medskip\noindent
For any $\mu > 0$, when $|X\cap C|\ge \max\{\frac{4.15}{\alpha^2}\ln(4/\mu), \frac{5.3}{\alpha}, 8.3\ln(4/\mu), 40\}$, we have that
$$\Pr\left[\wdist\left(g(\restrict{S\lvert_C}{\rho}),g(\restrict{\overline{S}\lvert_C}{\rho})\right) > \alpha\right] \leq 2(1 + |X \cap C|)\mu.$$ 
\end{claim}

\noindent Putting these claims together, we get that
\ifnum\usenix=0
\begin{equation*}
    \underset{X_a, h}{\Pr}\left[\wdist(h(\restrict{X_a\lvert_C}{\rho}), h(\restrict{X_b\lvert_C}{\rho})) > \alpha\right] \,\leq\,\, 2\mu(|X \cap C| +1)\cdot e^\zeta + \eta.
\end{equation*}
\else
\begin{multline*}
    \underset{X_a, h}{\Pr}\left[\wdist(h(\restrict{X_a\lvert_C}{\rho}), h(\restrict{X_b\lvert_C}{\rho})) > \alpha\right] \\
    \,\leq\,\, 2\mu(|X \cap C| +1)\cdot e^\zeta + \eta.
\end{multline*}
\fi

\ignore{
\begin{proof}[Proof of \cref{lem:applying-pure-dp-maxinfo-to-private-report}]
    Let $\alg^*:\cX^{n/2} \to \cY$ be an $\eps$-DP algorithm for $\eps \in (0,1]$, and let $\beta > 0$.
    Let $X \in \cX^n$, let $X_a$, $S$ be independent draws (sampled uniformly without replacement) of size $n/2$ from $X$. Note that, by the definition of max-information,  $$I_\infty^\beta(X_a;\alg(X_a)) = \dist_\infty^\beta\Big( \big(\,X_a,\alg(X_a)\,\big) || \big(\,X_a,\alg(S)\,\big)\Big)$$
    Since $\alg$ is differentially private, we know that there is bounded max-information between any random sample $X_a$ and the output of $\alg$ on the sample. In particular, applying \Cref{thm:pure-dp-implies-maxinfo} gives us the following
    $$\dist_\infty^\beta\Big( \big(\,X_a,\alg(X_a)\,\big) || \big(\,X_a,\alg(S)\,\big)\Big) \leq \eps^2n/4 + \eps\sqrt{n\ln(2/\beta)/4}$$
    % 
    Since $(X_a,\alg(S))$ is distributed exactly the same as $(S,\alg(X_a))$ then we have the following:
    $$\dist_\infty^\beta\Big( \big(\,X_a,\alg(X_a)\,\big) || \big(\,S,\alg(X_a)\,\big)\Big) \leq \eps^2n/4 + \eps\sqrt{n\ln(2/\beta)/4}$$
    % 
    By the definition of max-information this means
    $$\log\left(\underset{\{T\,|\,\, \Pr[(X_a,\alg^*(X_a)\in T] > \beta\}}{\sup}\frac{\Pr[(X_a,\alg^*(X_a)) \in T]-\beta}{\Pr[(S,\alg^*(X_a))) \in T]}\right) \leq \eps^2n/4 + \eps\sqrt{n\ln(2/\beta)/4}$$
    % 
    Which means that for all $\{T\,|\,\, \Pr[(X_a,\alg^*(X_a)\in T] > \beta\}$,
    $$\log\left(\frac{\Pr[(X_a,\alg^*(X_a)) \in T]-\beta}{\Pr[(S,\alg^*(X_a))) \in T]}\right) \leq \eps^2n/4 + \eps\sqrt{n\ln(2/\beta)/4}$$
    Let $C$ be any fixed subpopulation. Given $C$, we can post-process a pair $(X_a,\alg(X_a))$ to compute $(h(\restrict{X_a\lvert_C}{\selection}))$ and $(h(\restrict{X_b\lvert_C}{\selection}))$. This is because $h \leftarrow \cL(\alg(X_a))$ and $X_b = X\setminus X_a$. Similarly, we can post-process $(S,\alg(X_a))$ to compute $(h(S\lvert_C))$ and $(h(\overline{S}\lvert_C))$.

Let $T = \{ (S,h) \in (X^n,\cY)\,\mid\,\, \wdist(g(S\lvert_C), g(\overline{S}\lvert_C)) > \alpha^* \}$. Then if $\Pr[(X_a,\alg^*(X_a)\in T] > \beta$,
$$\log\left(\frac{\Pr[(X_a,\alg^*(X_a)) \in T]-\beta}{\Pr[(S,\alg^*(X_a))) \in T]}\right) \leq O(\eps^2n + n\sqrt{\delta/\eps})$$
Which means that if $\Pr[\wdist(h(\restrict{X_a\lvert_C}{\selection}), h(\restrict{X_b\lvert_C}{\selection})) > \alpha^* ] > \beta$,
\[
    \log\left(\frac{\Pr[\wdist(h(\restrict{X_a\lvert_C}{\selection}), h(\restrict{X_b\lvert_C}{\selection})) > \alpha^*] - \beta}{\Pr[\wdist(h(S\lvert_C), h(\overline{S}\lvert_C)) > \alpha^*]}\right) \leq \eps^2n/4 + \eps\sqrt{n\ln(2/\beta)/4}
\]
% 
We can rearrange the above equation to get that 
\begin{align*}
    &\Pr[\wdist(h(\restrict{X_a\lvert_C}{\selection}), h(\restrict{X_b\lvert_C}{\selection})) > \alpha^*]\\
    \leq 
    &\Pr[\wdist(h(S\lvert_C), h(\overline{S}\lvert_C)) > \alpha^*]\cdot e^{\eps^2n/4 + \eps\sqrt{n\ln(2/\beta)/4}} + \beta
\end{align*}
\end{proof}
}

Now we proceed to prove Claims~\ref{lem:max-info-decouples-Xa-from-ha}~and~\ref{lem:unrelated-predictor}.

\begin{proof}[Proof of \cref{lem:max-info-decouples-Xa-from-ha}]
    First, note that since the algorithm $\cL$ postprocesses the report output by the data curator, by the fact that max-information is preserved under postprocessing, it inherits its max-information. Let $\alg^*$ be the combined algorithm $\cL \circ \alg$. Then by the definition of max-information, and since $(X_a,\alg^*(S))$ is distributed exactly the same as $(S,\alg^*(X_a))$,
    \ifnum\usenix=0
    $$I_\infty^\eta(X_a;\alg^*(X_a)) = \dist_\infty^\eta\Big( \big(\,X_a,\alg^*(X_a)\,\big) || \big(\,X_a,\alg^*(S)\,\big)\Big) = \dist_\infty^\eta\Big( \big(\,X_a,\alg^*(X_a)\,\big) || \big(\,S,\alg^*(X_a)\,\big)\Big).$$
    \fi
    we have that for all $T$ such that $\Pr[(X_a,\alg^*(X_a)\in T] > \eta$,
    $$\log\left(\frac{\Pr[(X_a,\alg^*(X_a)) \in T]-\eta}{\Pr[(S,\alg^*(X_a))) \in T]}\right)\leq \zeta.$$
   Given $C$, we can post-process a pair $(X_a,\alg^*(X_a))$ to compute $(h(\restrict{X_a\lvert_C}{\selection}))$ and $(h(\restrict{X_b\lvert_C}{\selection}))$. This is because $h \leftarrow \alg^*(X_a)$ and $X_b = X\setminus X_a$. Applying the same post-processing to $(S,\alg^*(X_a))$ yields $(h(\restrict{S\lvert_C}{\selection}))$ and $(h(\restrict{\overline{S}\lvert_C}{\selection}))$.
    
    \medskip\noindent
    Let $T = \{ (S,h) \,\mid\,\, \wdist(h(\restrict{S\lvert_C}{\selection}), h(\restrict{\overline{S}\lvert_C}{\selection})) > \alpha \}$.%\mb{Use either $h$ or $g$ consistently} 
    Then if $\Pr[(X_a,\alg^*(X_a))\in T] > \eta$,%\mb{$\alg^*$ not defined here}
    $$\log\left(\frac{\Pr[(X_a,\alg^*(X_a)) \in T]-\eta}{\Pr[(S,\alg^*(X_a))) \in T]}\right) \leq \zeta.$$
   This means that if $\Pr[\wdist(h(\restrict{X_a\lvert_C}{\selection}), h(\restrict{X_b\lvert_C}{\selection})) > \alpha] > \eta$,
    \[
    \log\left(\frac{\Pr[\wdist(h(\restrict{X_a\lvert_C}{\selection}), h(\restrict{X_b\lvert_C}{\selection})) > \alpha] - \eta}{\Pr[\wdist(h(\restrict{S\lvert_C}{\selection}), h(\restrict{\overline{S}\lvert_C}{\selection})) > \alpha]}\right) \leq \zeta.
    \]
% 
We can rearrange the above equation to get that 
\begin{align*}
    &\Pr[\wdist(h(\restrict{X_a\lvert_C}{\selection}), h(\restrict{X_b\lvert_C}{\selection})) > \alpha]\\
    \leq 
    &\Pr[\wdist(h(\restrict{S\lvert_C}{\selection}), h(\restrict{\overline{S}\lvert_C}{\selection})) > \alpha]\cdot e^{\zeta} + \eta,
\end{align*}
as required.
\end{proof}

\begin{proof}[Proof of \cref{lem:unrelated-predictor}]
    
\noindent 
Let $\alpha, \mu \in (0, 1)$ (the statement holds trivially for $\mu > 1$), and suppose $|X\cap C|\ge \max\{\frac{4.15}{\alpha^2}\ln(4/\mu), \frac{5.3}{\alpha}, 8.3\ln(4/\mu), 40\}$. By the definition of the distance metric we have the following:
\ifnum\usenix=0
\begin{equation}\label{eq:wasserstein-integral}    \wdist\left(g(\restrict{S\lvert_C}{\selection}),g(\restrict{\overline{S}\lvert_C}{\selection})\right) = \int_{-1}^{1} \abs{\cdf_{g(\restrict{S\lvert_C}{\selection})}(g) - \cdf_{g(\restrict{\overline{S}\lvert_C}{\selection})}(g)} \,dg.
\end{equation}
\else
\begin{align}\label{eq:wasserstein-integral}
& \wdist\left(g(\restrict{S\lvert_C}{\selection}),g(\restrict{\overline{S}\lvert_C}{\selection})\right) \nonumber \\
   & \qquad = \int_{-1}^{1} \abs{\cdf_{g(\restrict{S\lvert_C}{\selection})}(\ell) - \cdf_{g(\restrict{\overline{S}\lvert_C}{\selection})}(\ell)} \,d \ell
\end{align}


\fi

To bound this value, we first prove the following for a fixed $y \in [-1,1]$.
\begin{equation}\label{eq:cdf-diffs-single-y}
    \Pr\left[|\cdf_{g(\restrict{S\lvert_C}{\selection})}(y) - \cdf_{g(\restrict{\overline{S}\lvert_C}{\selection})}(y)| \ge \alpha\right]\le \mu.
\end{equation}
Then, we observe that there are at most $|X \cap C|+1$ effectively different values of $y$ we need to consider with respect to any fixed $g$ and $C$. (For every realization of $S, \overline{S}$, $\cdf_{g(\pi_{\selection}(S|_C))}$ can only change for values of $y$ on which $\cdf_{g(\pi_{\selection}(X|_C))}$ changes. These values correspond to the partitioning of $[-1,1]$ into intervals induced by applying $g \circ \pi_{\selection}(\cdot)$ to the elements in $X \cap C$.) By union bounding over these $|X \cap C| + 1$ effectively different values of $y$, 
\Cref{eq:cdf-diffs-single-y} gives us the following.
\ifnum\usenix=0
\begin{equation}\label{eq:supremum-of-cdf-diffs}
\Pr\left[\sup_{y\in[-1,1]}\abs{\cdf_{g(\restrict{S\lvert_C}{\selection})}(y) - \cdf_{g(\restrict{\overline{S}\lvert_C}{\selection})}(y)} \ge \alpha\right]\le (1+|X\cap C|)\mu.
\end{equation}
\else
\begin{align}\label{eq:supremum-of-cdf-diffs}
\Pr&\left[\sup_{y\in[-1,1]}\abs{\cdf_{g(\restrict{S\lvert_C}{\selection})}(y) - \cdf_{g(\restrict{\overline{S}\lvert_C}{\selection})}(y)} \ge \alpha\right] \nonumber \\
& \qquad \le (1+|X\cap C|)\mu.
\end{align}
\fi

Finally, substituting the bound from \Cref{eq:supremum-of-cdf-diffs} in \Cref{eq:wasserstein-integral} proves the lemma \ssnote{I think $\alpha$ needs to be multiplied by $2$}. 

\medskip\noindent
To show \Cref{eq:cdf-diffs-single-y}, we redefine the sampling process in a way that will allow us to use \Cref{thm:hypgeom}, a concentration result for the hypergeometric distribution (See Definition~\ref{def:hyper} for a definition of this distribution): Consider an urn consisting of $n$ balls. Among those $n$ balls, $m$ are marked with a red stripe, representing membership in $C\cap X$. Among the $m$ red-striped balls, $t$ are further marked with a blue stripe, representing $x\in C\cap X$ such that $g(\restrict{x}{\selection}) \leq y$ for the value of $y$ being considered. Consider the experiment where we sample $n/2$ balls without replacement, and define the joint pair of random variables $(V, W)$ where $V$ counts the number of red-striped balls in the sample, (i.e., the number of sampled points that are in $X\cap C$) and $W$ counts the number of (red and) blue-striped balls in the sample, (i.e., the number of sampled points $x$ that are in $X\cap C$ and satisfy $g(\restrict{x}{\selection}) \leq y$. The random variables $W$ and $V$ follow hypergeometric distributions as follows:
\ifnum\usenix=1
		$$V \sim H(n, m, n/2) \text{ and }
		(W | V = v) \sim H(m, t, v).$$
\else
    \begin{align*}
		V &\sim H(n, m, n/2) \\
		(W | V = v) &\sim H(m, t, v).
	\end{align*}
\fi
Observe that the absolute value of the CDF difference we are trying to bound is equal to $\left|\frac{W}{V} - \frac{t - W}{m - V}\right|$ by definition.

Let $E_1$ be the event that the number of red-striped balls in the sample is close to its expected value (i.e., $|V - m/2| < m/4$). Then applying Theorem~\ref{thm:hypgeom} and using $m > 40$ and $m > 8.3\ln(4/\mu)$ we have that
\begin{align*}
    \Pr[\overline{E_1}] &< 2\exp\left(\frac{-2}{m+1} \cdot \Big( (m/4)^2 - 1\Big)\right)\\
    &\leq 2\exp\left(\frac{-2}{1.025m} \cdot \Big( 0.99(m/4)^2 \Big)\right)\\
    &= 2\exp\left(\frac{1.98}{16.4}m\right) < \mu/2.
\end{align*}
	
Now let us condition on a realization $V = v$. Given this, let $E_2$ be the event that the number of blue-striped balls in the sample is close to its expected value (i.e., $\left|W -\frac{tv}{m} \right| \leq \zeta$). Then applying Theorem~\ref{thm:hypgeom} for $\zeta \geq 2$ and $c$ as in the theorem:
    \[\Pr[ \overline{E_2} \bigmid V = v] < 2\exp\left(-2c\cdot \left( \zeta^2 - 1\right)\right).\]
Observe that 
\ifnum\usenix=0
    $$c = \max\left\{\frac{1}{v+1} + \frac{1}{m-v+1}, \frac{1}{t+1} + \frac{1}{m-t+1}\right\} \geq \frac{1}{t+1} + \frac{1}{m-t+1} \geq \frac{2}{\frac{m}{2}+1}.$$
\else
  \begin{align*}
  c & = \max\left\{\frac{1}{v+1} + \frac{1}{m-v+1}, \frac{1}{t+1} + \frac{1}{m-t+1}\right\} \\
  & \geq \frac{1}{t+1} + \frac{1}{m-t+1} \geq \frac{2}{\frac{m}{2}+1}.
  \end{align*}
\fi
Therefore,
\begin{equation}\label{eq:zeta}
    \Pr[\overline{E_2} \bigmid V = v] < 2\exp\left(\frac{-4}{\frac{m}{2}+1}\cdot \left( \zeta^2 - 1\right)\right).
\end{equation}

Assume that both events $E_1$ and $E_2$ hold. Then in this case we will argue that:
\begin{equation}\label{eq:alpha}
    \left|\frac{W}{V} - \frac{t - W}{m - V}\right| < \frac{m\zeta}{m^2/4 - \gamma^2} = \alpha.
\end{equation}
We will then substitute the derived value of $\zeta$ into Equation~\ref{eq:zeta} to show that $\Pr[ \overline{E_2} \bigmid V = v] < \mu/2$.

To this end, observe that if the number of blue-striped balls in the sample is within $\zeta$ of the expected value, $\frac{tV}{m}$, then the number of blue-striped balls in the sample is also within $\zeta$ of its own expected value, $\frac{t(m-V)}{m}$. This is because the balls can only be in one of these two sets. Therefore, when both events $E_1$ and $E_2$ hold, we have that:
\[\quad \Big|(t-W)-\frac{t(m-V)}{m} \Big| < \zeta.\]
Therefore,
\ifnum\usenix=0
    \begin{align*}
    \frac{W}{V}\, -\, &\frac{t-W}{m - V}\\
    &< \big(\E[W] + \zeta\big)\cdot\frac{1}{V} - \big(\E[t-W] - \zeta\big)\frac{1}{m - V}
    &&\left(\text{because }\abs{W-\frac{tV}{m}} < \zeta\right)\\
    &= \left(\frac{tV}{m} + \zeta\right)\cdot\frac{1}{V} - \left(\frac{t(m-V)}{m} - \zeta\right)\cdot\frac{1}{m - V}\\
    &=\frac{m\zeta}{V(m-V)}\\
    &< \frac{m\zeta}{m^2/4 - \gamma^2}
    &&\left(\text{because }|V - m/2| < \gamma\right).
    \end{align*}
\else
    \begin{align*}
    \frac{W}{V}\, -\, &\frac{t-W}{m - V} < \big(\E[W] + \zeta\big)\cdot\frac{1}{V} - \big(\E[t-W] - \zeta\big)\frac{1}{m - V}
    %&&\left(\text{because }\abs{W-\frac{tV}{m}} < \zeta\right) 
    \\
    &= \left(\frac{tV}{m} + \zeta\right)\cdot\frac{1}{V} - \left(\frac{t(m-V)}{m} - \zeta\right)\cdot\frac{1}{m - V}\\
    &=\frac{m\zeta}{V(m-V)} < \frac{m\zeta}{m^2/4 - \gamma^2}.
    % &&\left(\text{because }|V - m/2| < \gamma\right)
    \end{align*}
    where the first inequality is because $\abs{W-\frac{tV}{m}} < \zeta$, and the final inequality is because $|V - m/2| < \gamma$.
\fi
 Similarly, 
    $$\frac{t-W}{m - V} - \frac{W}{V} < \frac{m\zeta}{m^2/4 - \gamma^2}.$$
    This gives us Equation~\ref{eq:alpha}. We can now set $\alpha = \frac{m\zeta}{m^2/4 - \gamma^2}$ to get that $\zeta = \frac{3m\alpha}{16}$. 
    Now, substituting this $\zeta$ value back into Equation~\ref{eq:zeta} and using $m > 40$, $m>16/3\alpha$, and $m > \frac{4.15}{\alpha^2}\ln(4/\mu)$ we have the following:
    
    \begin{align*}
        \Pr& [ \overline{E_2} \bigmid V = v] < \exp\left(\frac{-4}{\frac{m}{2}+1}\cdot \left( \zeta^2 - 1\right)\right)\\
        &= 2\exp\left(\frac{-4}{\frac{m}{2}+1}\cdot \left( \frac{9m^2\alpha^2}{16^2} - 1\right)\right)\\
        &\leq 2\exp\left(\frac{-8}{1.05m}\cdot \left( \frac{0.9\cdot9m^2\alpha^2}{16^2}\right)\right)\\
        &= 2\exp\left(\frac{8.1\alpha^2m}{33.6}\right) < \mu/2.
    \end{align*}

    
    Finally, we get the following:
    \[\Pr\left[\left|\frac{W}{V} - \frac{t - W}{m - V}\right| > \alpha\right]\leq \Pr[\overline{E_1}\lor\overline{E_2}]\leq \mu.\] 
\end{proof}

\noindent
With Claims~\ref{lem:max-info-decouples-Xa-from-ha}~and~\ref{lem:unrelated-predictor} now proved, this concludes the proof of \Cref{thm:max-info-implies-demographic-coherence}.
\end{proof}

\smallskip\noindent\textbf{Proof of \Cref{thm:max-info-implies-demographic-coherence}.}
In the remaining part of this section, we use Lemma~\ref{lemma:max-info-implies-demographic-coherence-int} to prove that any data curation algorithm $\alg$ with bounded max-information also enforces wasserstein-coherence.

\begin{proof}[Proof Of ~\Cref{thm:max-info-implies-demographic-coherence}]

Fix $\eta > 0$. Fix any subpopulation $C \in \cC$. Consider any dataset $X$. Then for any algorithm $\cL:\cY\to\{\univ\to [-1,1]\}$, dataset $X \in \univ^n$, and $\mu > 0$, such that \begin{align}\label{eq:condgrpsize}
        |X\cap C|\ge \max\left\{\frac{4.15\cdot\ln(4/\mu)}{\alpha^2}, \frac{5.3}{\alpha},8.3\cdot\ln(4/\mu), 40\right\},
    \end{align}
we have the following (by \Cref{lemma:max-info-implies-demographic-coherence-int})
 \ifnum\usenix=0   \begin{equation}\label{eq:probdemcoh}
        \Pr_{X_a,X_b \leftarrow X, h\leftarrow\cL\circ\alg(X_a)}\left[ \wdist\Big(h(\restrict{X_a|_C}{\selection}),\, h(\restrict{X_b|_C}{\selection})\Big) > \alpha \right] \leq  2(|X\cap C|+1)\mu\cdot e^{\zeta} +\eta
    \end{equation} 
\else
\begin{align}\label{eq:probdemcoh}
        \Pr&_{X_a,X_b \leftarrow X, h\leftarrow\cL\circ\alg(X_a)}\left[ d_{\mathcal{W}_1}\Big(h(\restrict{X_a|_C}{\selection}),\, h(\restrict{X_b|_C}{\selection})\Big) > \alpha \right] \nonumber \\
        & \qquad \leq  2(|X\cap C|+1)\mu\cdot e^{\zeta} +\eta
    \end{align} 
\fi
where the choice of split $X_a,X_b \setrandomly X$ and the computed predictor $h\leftarrow \cL(X_a)$ are as in the $\coherenceExp_{\cL\circ\alg,X,\cC^*,\selection}(\alpha)$ experiment in \Cref{fig:coherenceExp}. 
    
We need to show, instead, that for
\ifnum\usenix=0
\begin{align*}
\gamma = \max\Big\{ & \frac{8.3 \cdot (\zeta + \ln(16|\cC|/\beta))}{\alpha^2}, \frac{36\ln((3/\alpha)}{\alpha^2} \nonumber &, 16.6 \cdot (\zeta + \ln(16|\cC|/\beta)), \frac{5.3}{\alpha}, 80\Big\},
\end{align*}
\else
\begin{align*}
\gamma & = \max\Big\{ \frac{8.3 \cdot (\zeta + \ln(16|\cC|/\beta))}{\alpha^2}, \frac{36\ln((3/\alpha)} {\alpha^2} \nonumber \\ 
&, 16.6 \cdot (\zeta + \ln(16|\cC|/\beta)), \frac{5.3}{\alpha}, 80\Big\},
\end{align*}
\fi
and all algorithms $\cL:\cY\to(\univ\to[-1,1])$, all datasets $X \in \univ^n$, the probability the following holds for all $C \in \cC$ (such that $|X|_C| \geq \gamma$,) is low:
 \[\wdist\left( h\big(\restrict{X_a|_C}{\selection}\big),\, h\big(\restrict{X_b|_C}{\selection}\big) \right) > \alpha,\]
where the split $X_a,X_b \setrandomly X$ and predictor $h\leftarrow \cL(X_a)$ are as in \Cref{fig:coherenceExp}.

\bigskip\noindent
To that end, we start by considering the fixed subpopulation $C$ and setting $\mu = \eta/\left(2(|X\cap C|+1)\cdot e^{\zeta} \right)$ (ensuring that the RHS of \Cref{eq:probdemcoh} is 2 $\eta$ ). 

    The main content in the proof will be arguing that the following lower bound on the size of $|X \cap C|$    implies the condition in \Cref{eq:condgrpsize}. We can then union bound over all sub-populations in $\cC$ to get the theorem.
    \begin{align}
        |X \cap C| \geq \max\Big\{ & \frac{8.3 \cdot (\zeta + \ln(16/\eta))}{\alpha^2}, \frac{36\ln(3/\alpha)}{\alpha^2} \nonumber \\
        &, 16.6 \cdot (\zeta + \ln(16/\eta)), \frac{5.3}{\alpha}, 80\Big\}.
    \end{align}
 

    
    Substituting the value of $\mu$ back in \Cref{eq:condgrpsize}, the first term in the max corresponds to the condition
    \[
        |X\cap C|\ge \frac{4.15\cdot\ln(8 (|X\cap C|+1) e^{\zeta}/\eta)}{\alpha^2}
    \]
    which can also be written as:
    \[
        |X\cap C|\ge \frac{4.15\cdot\ln((|X\cap C|+1) 
 + g}{\alpha^2}
    \]
   where $g = 4.15 \cdot (\zeta + \ln(8/\eta))$. 

    Assume $ \frac{|X\cap C|}{2} \ge \frac{4.15\cdot\ln((|X\cap C|+1)}{\alpha^2}.$ Then, as long as $|X\cap C| \geq \frac{2g}{\alpha^2}$, the condition is satisfied. Now, using the fact that $|X \cap C| \geq 80$, we have that $\ln((|X\cap C|+1) \leq 1.01\ln((|X\cap C|+1)$, which implies that it suffices for
     $ |X\cap C| \ge \frac{9\cdot\ln((|X\cap C|)}{\alpha^2}.$ Consider the inequality  $ \frac{|X\cap C|}{\ln((|X\cap C|)} \ge  c$. Note that the left hand side is an increasing function of $|X \cap C|$. Let $|X\cap C| \geq 2c \ln c$. Then, we get that  $ \frac{|X\cap C|}{\ln((|X\cap C|)} \ge \frac{2c \ln c}{\ln(2c \ln c)}$, and some arithmetic shows that for $c \geq 9$ (which is true whenever $\alpha \leq 1$), the right hand side is indeed larger than $c$. Hence, it is additionally sufficient that  $ |X\cap C| \ge \frac{36\ln((3/\alpha)}{\alpha^2}.$ \ssnote{For after submission: check if the previous lemma proof goes through for $\alpha > 1$, and uncomment the line after if so.}

     Similarly, to be larger than the third term in the max in~\Cref{eq:condgrpsize}, we need 
    \[
        |X\cap C|\ge 8.3 \cdot\ln(8 (|X\cap C|+1) e^{\zeta}/\eta)
    \]
    which can also be written as
    \[
        |X\cap C|\ge 8.3\cdot\ln((|X\cap C|+1) 
 + g
    \]
   where $g = 8.3 \cdot (\zeta + \ln(8/\eta))$. 

     Assume $ \frac{|X\cap C|}{2} \ge 8.3 \cdot\ln((|X\cap C|+1).$ Then, as long as $|X\cap C| \geq 2g$, the condition is satisfied. Now, using the fact that $|X \cap C| \geq 80$, we have that $\ln((|X\cap C|+1) \leq 1.01\ln((|X\cap C|+1)$, which implies that it suffices for
     $ |X\cap C| \ge 18\cdot\ln((|X\cap C|)$,which is true as long as $|X \cap C| \geq 80$, hence this case is taken care of. 

     Hence, for a single subpopulation $C$ we have argued that it is sufficient that
    \begin{align}
        |X \cap C| \geq \max\Big\{ & \frac{8.3 \cdot (\zeta + \ln(8/\eta))}{\alpha^2}, \frac{36\ln(3/\alpha)}{\alpha^2} \nonumber \\
        &, 16.6 \cdot (\zeta + \ln(8/\eta)), \frac{5.3}{\alpha}, 80\Big\}.
    \end{align}

    Setting $\eta = \beta/2|\cC|$, and applying a union bound on \Cref{eq:probdemcoh} (over all subpopulations in the class) then gives the theorem.
\end{proof}



\subsection{Differentially Private Algorithms Enforce Demographic Coherence}\label{sec:dp-implies-coherence-enforcement}

In this section, we argue that our definition of demographic coherence can be achieved via differentially private algorithms. We do this by adapting known connections between differential privacy and max-information, and~\Cref{thm:max-info-implies-demographic-coherence} connecting max-information and demographic coherence. 

The proofs for pure differential privacy and approximate differential privacy follow a similar flavor. Firstly, we adapt known connections between (pure and approximate) differential privacy and max-information to the setting of sampling without replacement. 
%These proofs are essentially identical to those in the i.i.d. case; but for our application we carefully track the constants used, and ensure that nothing breaks with the different sampling. 
Then, we use~\Cref{thm:max-info-implies-demographic-coherence}
(connecting bounded max-information to demographic coherence) to argue that differential privacy implies demographic coherence. \ifnum\usenix=1 For the formal proofs, see~\Cref{sec:DPdemcoh}. \fi 

\begin{restatable}{theorem}{pureDPcoh}
    [Pure-DP Enforces Wasserstein Coherence]\label{thm:pure-dp-implies-coherence-enforcement}
    Fix any $\eps, \beta \in (0,1]$, $\alpha \in (0,1], n \in \mathbb{N}$.
     Let $\cC$ be a collection of subpopulations $C \in \univ^*$. Consider an order-invariant $\eps$-DP algorithm ${\alg:\univ^{n/2}\to \cY}$. Fix any lens $\rho$. Then, $\alg$ enforces $(\alpha,\beta)$-Wasserstein-coherence with respect to  collection $\cC$, lens $\rho$, and size constraint $\gamma$, where
     \ifnum\usenix=0
     \begin{align}
        \gamma= \max\Big\{ & \frac{8.3 \cdot (\eps^2n/4 + \eps\sqrt{n\ln(4|\cC|/\beta)}/2 + \ln(16|\cC|/\beta))}{\alpha^2}, \frac{36\ln((3/\alpha)}{\alpha^2} \nonumber \\
        &, 16.6 \cdot (\eps^2n/4 + \eps\sqrt{n\ln(4|\cC|/\beta)}/2 + \ln(16|\cC|/\beta)), \frac{5.3}{\alpha}, 80\Big\}.
    \end{align}
    \else
   \begin{align}
        \gamma= \max\Big\{ & \frac{8.3 \cdot (\eps^2n/4 + \eps\sqrt{n\ln(4|\cC|/\beta)}/2 + \ln(16|\cC|/\beta))}{\alpha^2}, \nonumber \\ & \frac{36\ln(3/\alpha)}{\alpha^2}, 16.6 \cdot (\eps^2n/4 + \eps\sqrt{n\ln(4|\cC|/\beta)}/2 \nonumber \\
        & \qquad + \ln(16|\cC|/\beta)), \frac{5.3}{\alpha}, 80\Big\}.
    \end{align}
    \fi
    
    
    
\end{restatable}
\ifnum\usenix=0
\begin{proof}

    Fix $\beta > 0$. By \Cref{thm:pure-dp-implies-maxinfo} connecting differential privacy and max-information, we have that we have that,
    $$I^{\beta/2|\cC|}_{\infty}(\alg,n/2) \leq \eps^2n/4 + \eps\sqrt{n\ln(4|\cC|/\beta)/4}.$$

    Applying~\Cref{thm:max-info-implies-demographic-coherence} and substituting the bound on max-information then completes the proof.
\end{proof}
\fi

\begin{restatable}{theorem}{approxDPcoh}
[Approx-DP Enforces Wasserstein Coherence]\label{thm:approx-dp-implies-coherence-enforcement}
    Fix any $\beta \in (0,1]$, $\alpha \in (0,1]$, $n \in N$. Let $\eps \in (0,\frac{1}{2}]$, and $\delta \in (0, \frac{\eps^2 \beta^2}{(120n)^2 |\cC|^2}]$ 
    Let $\cC$ be a collection of subpopulations $C \in \univ^*$. Consider an order-invariant \footnote{Order-invariance can be relaxed by multiplying $\eps$ by $2$ in the $\gamma$ value, and dividing by $2$ in the range of $\delta$.} $(\eps,\delta)$-DP algorithm ${\alg:\univ^{n/2}\to \cY}$. Fix any lens $\rho$. Then, $\alg$ enforces $(\alpha,\beta)$-Wasserstein-coherence with respect to  collection $\cC$, lens $\rho$, and size constraint $\gamma$, where
    \ifnum\usenix=0
     \begin{align}
        \gamma = \max\Big\{ & \frac{8.3 \cdot (265\eps^2n + 12\eps\sqrt{n\ln(4|\cC|/\beta)} + \ln(32|\cC|/\beta))}{\alpha^2}, \frac{36\ln((3/\alpha)}{\alpha^2} \nonumber \\
        &, 16.6 \cdot (265\eps^2n + 12\eps\sqrt{n\ln(4|\cC|/\beta)} + \ln(32|\cC|/\beta)), \frac{16/3}{\alpha}, 80\Big\}.
    \end{align}
    \else
\begin{align}
        \gamma = \max\Big\{ & \frac{8.3 \cdot (133\eps^2n + 9\eps\sqrt{n\ln(4|\cC|/\beta)} + \ln(32|\cC|/\beta))}{\alpha^2}, \nonumber \\
        & \qquad \frac{36\ln((3/\alpha)}{\alpha^2}, 16.6 \cdot (133\eps^2n + 9\eps\sqrt{n\ln(4|\cC|/\beta)} \nonumber \\ 
        & \qquad + \ln(32|\cC|/\beta)), \frac{16/3}{\alpha}, 80\Big\}.
\end{align}




 \fi
    
\end{restatable}
\ifnum\usenix=0
\begin{proof}
  Fix $\beta > 0$ and $\gamma = \frac{\beta}{2|\cC|}$. By \Cref{cor:approx-dp-implies-max-info} connecting differential privacy and max-information, we have that we have that, as long as $\delta \in (0, \frac{\eps^2 \beta^2}{(120n)^2 |\cC|^2}]$ 
  \begin{align*}
  I^{\beta/2|\cC|}_{\infty}(\alg,n/2) \leq \frac{265}{2} \eps^2n + 12\eps\sqrt{\frac{n}{2}\ln(4|\cC|/\beta)}. 
  \end{align*}

    Applying~\Cref{thm:max-info-implies-demographic-coherence} and substituting the bound on max-information then completes the proof.
    
\end{proof}
\fi