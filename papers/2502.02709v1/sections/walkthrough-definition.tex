\section{A Walk Through Our Definition}\label{sec:walkthrough}

In order to clearly motivate and explain the choices embedded within our definition, we incrementally build up our approach in this section; for the formal definition see Section~\ref{sec:formaldef}. 

\medskip\noindent
\textbf{Notation and Conventions.}
Assume that the data curator has collected a sample $X$ from the overall population of interest. We make no requirements on the relative sizes of $X$ and the population such that our framework can be used broadly---even in Census-like circumstances, in which the goal is to sample the entire population. Our ultimate goal is to reason about a data curator $\alg$ who uses $X$ to generate a privacy-preserving release $R$.\footnote{In our formal experiment, we actually suppress the formal object of the report.  Specifically, we reason directly about the composition of some data processing algorithm $\alg$ with an arbitrary algorithm $\cL$, rather than making the report an explicit object that is then passed to $\cL$.}

\subsection{Predictive Harms}\label{sec:incoherent-predictions}

Within our framework, we characterize the adversary as a party interested in making predictions about individuals, \eg if people have some particular stigmatized feature or are going to buy a product if they are targeted with an advertisement. We formalize this conceptual approach by considering an arbitrary algorithm $\cL$ used by the adversary to design the predictor $h$.\footnote{A more complex interpretation could also cover an approach to prediction that takes into account a social decision-making process.  While this interpretation is beyond the technical scope of our work, it may be interesting to consider in future work.} We choose to measure privacy risk in terms of predictive harms for the following reasons:
\begin{enumerate}[--,leftmargin=*]
    \item \emph{Predictors are commonplace:}  The predictions made by machine learning models increasingly have direct impacts on people's daily lives. Diagnostic models are being tested as potential aides for medical experts~\cite{AhsanLS22}, and increasingly complex and opaque models are used to ``match'' job candidates with prospective employees~\cite{indeed,ideal,aihr} in order to increase the odds that an individual ends up with a lucrative job. Even complex infrastructures, like those used in digital advertising, can be seen as predictive models that are attempting to classify individuals into target audiences. The fact that predictive algorithms are increasingly commonplace---and the decisions they make concretely impact our daily lives---makes them a very \emph{believable} source of harm.
    
    \item \emph{Harmful predictors need not be maliciously produced:} By considering the impact of predictors, we free ourselves from needing to see the adversary as \emph{intentionally} trying to cause harm and instead can refocus on the (perhaps accidental) harms that a data release has the potential to cause. 
    % Harmful inferences can be propagated by even well meaning data-dependent systems. 
    

\end{enumerate}

A conceptual concern about considering predictive harms is whether we are explicitly ruling out particular, important types of adversarial behavior that are attempting to extract information (\eg reconstruction attacks). However, we note that by discussing predictors we are only limiting the input/output behavior of the adversary's product, and not how the predictor is produced. For example, our framework could capture an adversary that runs a known reconstruction algorithm (\eg \cite{DickDKLRVZ22}) and then makes predictions about individuals based on the produced table.  In this way, our approach highlights the ways in which existing approaches could be \emph{used} when applied in decision-making contexts. Looking ahead, in theoretically analyzing our framework we will \emph{universally quantify} over algorithms to preserve generality, which means that reconstruction-based approaches---or other known malicious approaches to data extraction---are naturally captured.

Still, in choosing to concretize the type of our adversary, we do risk failing to consider a \textit{different} type of attacker with inconsistent goals. Definitions created with this philosophy can be extremely helpful at establishing \emph{necessary} conditions for ensuring privacy, but do not claim to be \textit{sufficient}. On the other hand, our approach helps highlight a specific way in which data is likely to be weaponized in the real world. 

\newcommand{\examplenameone}{Asahi\xspace} %Made using Ahttps://www.behindthename.com/random/random.php?gender=u&number=2&sets=1&surname=&norare=yes&all=yes
\newcommand{\examplenametwo}{Blair\xspace} %Made using Ahttps://www.behindthename.com/random/random.php?gender=u&number=2&sets=1&surname=&norare=yes&all=yes
    
\medskip\noindent
\textbf{Incoherent predictions.} 
Within this work we focus on capturing predictive harms that occur specifically by virtue of a data subject appearing in a dataset. To give a concrete example, consider the now classic case of Narayanan and Shmatikov's re-identification of Netflix users within an anonymized data release using public IMDB data \cite{SP:NarShm08}.  In this setting, we might consider an adversary interested in learning a predictor which predicts queerness (\eg imagine the adversary is operating in a regime in which queerness is criminalized or highly stigmatized). Now, imagine two similar\footnote{The notion of similarity is obviously a loaded one, as the ways in which two individuals are similar or different depend on the types of predictions being made about them. We eventually handle this by quantifying over many notions of similarity. For the sake of this motivation, it is enough to assume that the similarity of these individuals is meaningful with respect to the characteristic being predicted about them.} individuals \examplenameone and \examplenametwo; each intentionally avoids being perceived as queer, and in particular does not provide ratings on movies with queer themes on their public IMDB profiles. Assume that based on a random sample, one of them (\eg \examplenameone) has their movie ratings released by Netflix and the other (\eg \examplenametwo) does not. A predictor that is likely to guess that \examplenameone is queer when they are present in the dataset but would not have guessed they are queer otherwise indicates that the predictor was able to extract some information about \examplenameone from the data release.  Given the assumption that we claim that \examplenameone and \examplenametwo are similar, this would also be true of a predictor that guessed that \examplenameone is queer while \examplenametwo is not. 

Importantly, this is true even if it's not clear exactly what form leakage takes or if the  prediction as to their queerness is inaccurate. We call predictors that act in this way ``demographically incoherent.'' There are two important (if unintuitive) subtleties that immediately emerge from this description of incoherent predictions:
\begin{enumerate}[(1),leftmargin=*]
    \item \emph{Harmful predictors need not be accurate:} Incoherent predictions focus on the behavior of the predictor \emph{independent of accuracy}. Within the example above, it is not important if \examplenameone is actually queer, it is enough that the predictor guesses that \examplenameone is queer because of their presence in the data.
    This is because we envision our predictor being used to make some real-world decision, \eg limiting the opportunities available to \examplenameone due to their perceived queerness. As such, the prediction's accuracy is a secondary concern.
    \item \emph{Measuring confidence is critical:}  When considering the ways in which data releases can be translated into real-world harms, it's important to recognize that enabling an adversary to make high confidence predictions about private attributes is a problem. 
    Importantly, this means that we should not require that the adversary can predict private attributes with 100\% certainty in order for it to be considered harmful. 
    Indeed, there is no particular cut-off threshold for certainty at which point it is natural to consider a harm occurring for all contexts.  In turning to predictors as our adversarial strategy, we naturally arrive in a context within which notions of confidence have been extensively explored. 
    Specifically, our approach considers confidence-rated predictors $h$, 
    allowing us to directly reckon with predictive uncertainty.
\end{enumerate}

We note that there are other pathological predictors which do not indicate privacy-loss, and are therefore not considered demographically incoherent. For example, a predictor may make guesses that are entirely random or guess that everyone in the population has some feature, \ie make predictions that do not depend on the characteristics of individuals. The challenge then is to detect demographically incoherent predictors, whose behavior indicates privacy leakage, without depending on accuracy and without accidentally measuring variance in behavior that is not dataset dependent.


\subsection{An Experiment to Detect Demographically Incoherent Predictors} \label{sec:detecting-incoheret-algorithms}

With intuition about our class of ``bad'' predictors in hand, we now turn our attention to designing an experiment for detecting algorithms that produce them. In this discussion, we will defer to the concrete ways in which we will measure the demographic incoherence, and first focus on the experiment itselfâ€”that is, first we will decide what values we should measure, and then proceed to deciding how to do that measurement.
 
Because a symptom of incoherent predictors is differing performance on in-sample and out-of-sample individuals, it is clear that a \emph{comparison} is required.  However, it is not immediately obvious  what the ``right'' comparison should actually be.  In fact, some natural approaches fall short of our goals. As such, we walk through two seemingly natural, but flawed, experiments before discussing our final choice. Recall that the data curator has a dataset $X$ and will be releasing a privacy-preserving report $R$.

\begin{enumerate}[(1),leftmargin=*]

    \item \emph{Comparing before and after a data release:}  A very natural approach  would be to compare the performance of a predictor created \emph{before} a data release with one created \emph{after}. \ie comparing the performance (on individuals in $X$) of $h_0$ produced by an algorithm $\cL$ with access to the adversary's pre-existing, auxiliary information $\mathsf{Aux}$ to a predictor $h_1$ produced by $\cL$ with access to both $\mathsf{Aux}$ and the report $R$.\footnote{As this approach sketch is mainly to motivate our final approach below, we gloss over some formalities in this description.  For example, how do we know that $\cL$ does not act differently when provided one input ($\mathsf{Aux}$) and two inputs ($\mathsf{Aux}$ and $R$)?} 
    Such a comparison, intuitively, should isolate exactly the predictive changes associated with releasing $R$.

    Where this approach fails is that it does not recognize that there \emph{should} be a difference between the predictors $h_0$ and $h_1$ over the inputs in $X$.  After all, if there was no difference between $h_0$ and $h_1,$ there would be no value whatsoever in releasing $R$! As such, this comparison is necessarily conflating potential ``bad'' types of predictions that releasing $R$ enables with the ``good'' types of predictions that motivated the release of $R$ in the first place.

    \item \emph{Comparing to the base population:} The next most natural approach would be to compare the behavior of a single predictor $h$ on individuals in $X$ with that on individuals in the rest of the population. For example, by comparing its behavior on another similarly sampled dataset $Y$. This improves on our previous approach because we might expect that a ``good'' learning algorithm uses the dataset to learn about the population at large instead of revealing specifics about individuals.

    While this approach gets to the core of our interests, it has an important flaw. Technically speaking, we can not assert that a real world sampling procedure has access to the base population distribution.
    \ie one cannot assume that two real world datasets are i.i.d samples from the same distribution. Also, we could conceivably be in a situation where the \emph{entire} population of interest is contained in $X$, leaving no one in $\bar{X}$ against which we could compare.  Therefore, keeping in mind the ergonomics of our definition in a concrete deployment scenario, 
    this approach also falls short of our goals.   
\end{enumerate}

\noindent
\emph{Our approach.} We build off the second approach above by taking control of the randomness used to separate the two comparison populations.  Specifically, we split the dataset $X$ into two uniformly selected halves, $X_a$ and $X_b$. We then use the data curation algorithm to generate the report $R$ using only $X_a$, holding $X_b$ in reserve as our ``test'' data set. We then test the behavior of a predictor $h$, designed based on the report $R$. Specifically, we compare the predictions of $h$ on individuals in $X_a$ and $X_b$. This approach ``fixes'' our second failed attempt by moving the assumptions about the randomness used in sampling $X$---something over which we have no control---into the randomness we use to split $X$ into $X_a$ and $X_b$---something over which we do have control. We say that a data release is \emph{demographically incoherent} with respect to $X$ if its predictions on members of  $X_a$ are noticeably different than the predictions it makes on members of $X_b$ (who necessarily have similar demographic distributions, given the uniform split.) 

\subsection{Measuring the Incoherence of a Predictor}

Finally, we discuss how to compare the behavior of $h$ on $X_a$ and $X_b$ without relying on accuracy.
Formally, we consider real-valued confidence-rated predictors $h: \univ \to [-1,1]$ which predict something about individuals. To capture the fact that these predictions are confidence rated, $h$ outputs values in $[0,1]$ when it predicts the attribute is likely to be true, and values in $[-1,0]$ when it predicts the attribute likely to be false, with a higher absolute value representing higher confidence.  

For any such predictor, we will consider $h(X_a)$ and $h(X_b)$ as representing the uniform distribution over the predictions of $h$ on $X_a$ and $X_b$ respectively. Comparing these distributions allows us to reason about the general behavior of the predictor $h$ on $X_a$ and $X_b$ without considering accuracy of predictions on individuals.
In order to get a more granular understanding of the behavior of $h$ we further formalize the intuition of making  comparisons over ``similar" individuals in $X_a$ and $X_b$ as explained below.

\medskip
\noindent
\textbf{Measuring a difference with respect to ``similar" individuals.}     
In our motivating discussion of incoherent predictions, our representative individuals \examplenameone and \examplenametwo were assumed to be ``similar'' to one another. 
To formalise this intuition, 
we ask that a predictor is demographically coherent not only on the population as a whole, but also on recognizable subgroups from the population, \eg men, women, college freshmen, middle-school teachers etc\ldots\footnote{We borrow this conceptual approach from \cite{Hebert-JohnsonK18}.} For each of these subgroups of the population, the things that bind them together make them similar, in some particular sense. 
By operationalizing our earlier intuition in this way, we ask that the demographic coherence property holds not only over some particular notion of similarity, but rather over many notions of similarity at the same time. It also has the following technical and social benefits: (1) From a technical perspective, considering only the full population might hide incoherent decisions within sub-populations that effectively ``cancel out.'' That is, there might be a right-shift in one group that masks a left-shift in a different group, each shift effectively ``disappearing'' in the collective distribution over all individuals.  (2) From a social perspective, there may be particularly important groups within the population for whom we want to ensure coherent predictors for normative reasons. For example, if $X$ is a Census-like dataset, we may want to ensure that there are not sub-geographies on which incoherent predictors are allowed. Similarly, we may want to ensure that there aren't legally protected categories (\eg race, sex, religion, etc\ldots) on whom incoherent predictions are allowed. 

\pj{\medskip\noindent\textbf{The \emph{lens} of a predictor.} Consider the adversary using the Netflix dataset to learn a predictor for queerness. We assume that at the time of making predictions, the adversary sees a public user profile (their IMDB ratings) which contains only some of the user information that was contained in the dataset. To formalize this intuition we introduce the \emph{lens} $\selection$ of the predictor, which indicates the attributes contained in the dataset which the predictor can ``see.'' We then compare the behavior of $h$ on members of $X_a$ and $X_b$ as seen through the lens $\rho$.\footnote{The predictor may also have side information about individuals not contained in the dataset, which can be formally included in the description of the adversarial algorithm $\cL$.}}

\medskip\noindent
\textbf{Choosing a metric.} In this work, we recommend instantiating the demographic coherence experiment with the distance metric of \emph{Wasserstein distance} (\Cref{def:wasserstein-distance}),
also known as earth-mover's distance, when measuring demographic coherence (or lack thereof).  Intuitively, this metric measures the minimum amount of work that it requires to deform one probability distribution into another.  For example, if one visualizes a probability distribution as a mount of dirt, \emph{Wasserstein distance} measures the effort required to move enough dirt to make one mount look like the other (thus, earth-mover's distance). Unlike Total Variation distance, which only measures the distance between the distribution curves, Wasserstein distance is greater with a higher shift in confidence; the importance of measuring confidence is one of the insights we highlight in \Cref{sec:incoherent-predictions}. Another advantage of the Wasserstein distance is that it has been widely studied and used in theoretical and empirical statistics, and so there is a rich mathematical toolkit that one can borrow from when reasoning about it. 

We recognize that there may be other measurement metrics that could be applied to the demographic coherence experiment that might highlight risk in different ways, and encourage this as important follow-up work.
