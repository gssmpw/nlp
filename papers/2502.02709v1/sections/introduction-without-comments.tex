% The file with all our old edits and comments is in old-files

\section{Introduction}
The collection of data and dissemination of aggregated statistics is a key function of government and civil society, driving critical data-driven decision making processes, \eg democratic apportionment, collective resource allocation, and documenting ongoing social ills.  Indeed, data has become an indispensable modern tool for producing knowledge. 
However, the collection of personal data---particularly mass scale collection---introduces the potential for the inappropriate disclosure of information that individuals might prefer to remain private. 
Thus, data curators must carefully apply privacy protection mechanisms to their data, ideally without compromising the utility of the eventual data release. 
The study of privacy preserving data releases started with \emph{attacks} that compellingly demonstrated that data releases which had not taken steps to ensure privacy could be weaponized for harm, \eg Latanya Sweeney’s infamous re-identification of the Massachusetts Governor Bill Weld’s medical records~\cite{sweeney}. Since then, there has been a robust literature describing increasingly sophisticated attacks which continue to motivate efforts towards privacy preserving data release \cite{transyouthintexas,censusreconstruction,censusreconstruction2,USENIX:Cohen22}. While these attacks have proved convincing enough to shift data protection practices in many fields, attack demonstrations do not provide a clear path towards designing data protection mechanisms themselves, \pj{even in the form of ``prevent all attacks like this one''; the attack demonstrations do not take on the task of distilling a set of agreed upon properties that make the attack convincing.} In the aftermath of these attacks, the research community has developed a set of formal approaches that aim to provide robust privacy guarantees. While early attempts, like k-anonymity, proved inadequate, differential privacy \cite{DworkMNS16j} has recently emerged as an accepted standard, seeing deployment in both industry \cite{CCS:ErlPihKor14,apple2017,ding2017collecting,tezapsidis2017uber} and government \cite{abowd2018us}. 
These formal approaches are often seen as \emph{sufficient} for ensuring data subject's privacy, 
in that they are ``one-size-fits-all,'' \ie data curators can apply best practice protections without needing to consider the intricacies of each deployment.

% providing robust properties that continue to hold under composition and post-processing. 
% Additinally, even if we focus just on leakage of individual information, there can be significant barriers to applying differential privacy.
% In practice, there can be significant barriers to applying DP, from the implementation to actually explaining its guarantees to data users and data subjects. Both of which stem from an inability to intuitively connect the guarantees of differential privacy with the kinds of privacy harms one might want to prevent.
In practice, however, there can be significant barriers to applying differential privacy, which stem from the need to strike a delicate balance between the benefits of privacy preservation and its cost to utility~\cite{amin2024practicalconsiderationsdifferentialprivacy}. Moreover, the generality of sufficient conditions means that they naturally lend themselves to being very abstract, which can make it far too easy to lose sight of the concrete privacy harms they are intended to prevent~\cite{CummingsHSS24}. 

The deficiencies inherent in each of the existing approaches compels us to explore an intermediary design philosophy:  \emph{necessary} conditions.  Within this approach, we can formally define (possibly many) properties that any private data release should guarantee without needing to provide a single, unifying, sufficient condition.  These necessary conditions can be seen as giving formal procedures for recognizing when an attacker has inflicted harm. Specifying necessary conditions promises to be an approach that simultaneously embraces the formality of sufficient conditions, while being just as concrete and convincing as attacks. Thinking in terms of necessary conditions has always been implicit in the practice of differential privacy (albeit, usually informally), where \pj{selecting the ``best'' privacy parameters $\eps,\delta,$ for a deployment requires a trade-off with other metrics, such as accuracy. This makes it necessary to understand how small the parameters \emph{must be} for the prevention of concrete privacy harms. Therefore, necessary conditions can provide a concrete methodology for justifying parameter choices by identifying parameter regimes that could enable specific harms.} 

\vspace{0.5em}
%\medskip
\noindent
\textbf{A new necessary condition: Demographic Coherence.} In this work, we design a novel necessary privacy notion rooted in three key insights: 
(1) privacy harms are increasingly going to come in the form of inferences at the hands of predictive algorithms.\footnote{In this work we intentionally us the term ``algorithm'' broadly to capture, \eg informal decision-making process made by humans that might not be explicitly codified as algorithms in the traditional sense.} That is, we should be interested in the predictions that these algorithms make about people---and the decisions organizations may make based on these predictions---even when predictive algorithms are not intentionally designed with causing harm in mind; (2) we should consider the confidence with which an algorithm can make predictions, because simply increasing the \emph{confidence} that an individual or a group has a certain attribute may be enough to result in harm;
and (3) The harms associated with breaches of privacy are not experienced uniformly among members of a population. This means that, if not defined carefully, an aggregate measure across an entire population could easily `hide' effects on vulnerable subgroups by averaging them away.

\pj{Our} resulting notion, which we call \emph{demographic coherence}, is intentionally designed to be \emph{ergonomic}\footnote{We use ergonomic in this context to mean ease of use by many different stakeholders.  We intentionally move away from the term ``usable,'' as this typically focuses only on end-users and we are interested in ease of use from a more diverse set of communities.} in many different contexts. For example, we provide sufficient formalism to enable rigorous analysis and provable realization, all while keeping the specific harms against which demographic coherence protects compellingly  salient.  Additionally, we provide a vision as to how demographic coherence can support the type of intuition building required to set real-world parameters. 

\subsection{Our Contributions}

In this work we make the following contributions:

\ifnum\usenix=1
\else
\begin{itemize}[--,leftmargin=*]
\fi

\ifnum\usenix=1
\smallskip
\noindent
\else
\item[--] 
\fi \textbf{Demographic Coherence.} In this work we introduce \emph{demographic coherence}, an analytical framework for reasoning about the privacy provided by data release algorithms. Demographic coherence has the following qualities:
\begin{itemize}
\item[--] \emph{Captures predictive harms.} Demographic coherence builds on conceptual tools from generalization ~\cite{vapnik1971, DworkFHPRR15, CummingsLNRW16, ImpLPS22, BunGHILPSS23} and multicalibration~\cite{HebertJohnsonKRR18, KimGZ19} to 
(1) evaluate the risk of predictive harms distributionally without relying on measuring accuracy with respect to an unknown (and possibly unknowable) ground truth and (2) evaluate the risk of predictive harms local to the different subgroups within a population. Evaluating risks distributionally allows the framework to remain applicable even when ground truth is unavailable, and evaluating risks for different subgroups allows the framework to identify effects specific to vulnerable subgroups.

\item[--] \emph{Lends itself to experimental auditing.} Demographic coherence has a natural translation to an experimental setup for comparing the effects of various algorithms for privacy preserving data release. 
In addition, demographic coherence is measured by computing a distance metric over two distributions, which facilitates quantification of the concrete risk. In this work we study an instantiation of demographic coherence measured using Wasserstein distance.

\item[--] \emph{Lends itself to analytical arguments.} Finally, the formalism we build supports rigorous analytical arguments about algorithms. For example, 
we show that all algorithms with bounded max information are also coherence enforcing.
\end{itemize}

\ifnum\usenix=1
\smallskip
\noindent
\else
\item[--] 
\fi
\textbf{Demographic coherence enforcement is \pj{achievable}.} 
% \mb{I prefer ``achievable'' to ``instantiable'' both here and in the first sentence below}
We prove that demographic coherence enforcement is \pj{achievable},
% instantiable, 
showing parameter conversions under which any pure differentially private (pure-DP) algorithm and any approximate differentially private (approx-DP) algorithm enforce demographic coherence. For an overview of these theorems, see \Cref{sec:overview-of-technical-results}.

\ifnum\usenix=1
\else
\end{itemize}
\fi
    