\subsection{Related Work}

The study of privacy-preserving data release broadly falls into two categories: demonstration of potential harms via concrete attacks, and the development of formal methodologies that provide robust guarantees. These two approaches provide complementary insights. Formal approaches provide a concrete path to implementing privacy-protections, and the motivation for their use is derived from attacks. In particular differential privacy provably protects against membership inference (\eg \cite{Homer08, DworkSSUV15, ShokriSSS17,YeomGFJ18}), reconstruction (\eg \cite{DinurN03, CohenN20a, C:HaiNukYog22, carlini2021private}), and reidentification (\eg \cite{sweeney, NarayananS08}), as shown by Dwork et. al.\cite{DworkSSU17}. In practice, however, there are fundamental challenges in using attacks to guide the many choices one must make when implementing privacy protections. These challenges arise from (1) identifying successful attacks, (2) identifying realistic attacks, and  (3) determining the privacy protections \emph{necessary} to prevent the attacks being considered.

\medskip\noindent
\textbf{Evaluating the success of an attack.} In using attacks to motivate formal methodologies, one must start by demonstrating the extent of potential vulnerabilities. For example, membership inference is an attack that relates directly to the definition of differential privacy---however, the potential to infer membership in a dataset isn't a convincing vulnerability in the case of large data collection efforts like the US decennial Census. Therefore, differential privacy frequently derives its motivations from re-identification and reconstruction attacks. Still, the success of these attacks is difficult to evaluate.\footnote{\eg reconstruction of features like gender can be carried out simply from knowing population statistics rather than breaking anonymity \cite{ruggles2022role}.}
In recent work, Dick et al. implemented a reconstruction attack \cite{pnas_reconstruction} along with robust evaluations of its success. Their work has since been cited by the US Census Bureau’s chief scientist as evidence that “database reconstruction does compromise confidentiality”~\cite{KellerA24}. The key insight in their evaluation comparing the results of the reconstruction to a baseline in which reconstruction is conducted with complete access to the distribution underlying the data. 
While the intuition behind this work---that an attack is much more concerning if it reveals more than what could be learned from a detailed knowledge of the distributional properties---applies to many attack paradigms, 
the baseline considered in their work is specific to reconstruction attacks.  Reconstruction attacks are not always possible to carry out, and, furthermore, conducting a reconstruction attack assumes malicious intent in a way that may or may not be convincing to all stakeholders. We introduce the demographic coherence framework which extends this intuition to the evaluation of a more general class of attacks. 

\pj{Another place where the efficacy of specific attacks is measured via comparison to baselines is the literature on auditing differentially private algorithms (\eg \cite{JayaramanE19, JagielskiUO20, NasrH0BTJCT23, JagielskyNS23}). Here, attacks are carried out on existing systems, and the efficacy of the attack is used to measure the maximum level of ``effective privacy'' that the system confers.}

    
\medskip\noindent
\textbf{Identifying realistic attacks.} Research into conducting privacy attacks makes a variety of assumptions about the setting in which those attacks could be conducted, including  the goal of the attacker, the power of the attacker, the type of system attacked, etc. 
These assumptions can radically change the extent to which an attack should be considered a realistic threat against real-world data releases; attacks that require unrealistic assumptions may not be concrete threats. 
The works of Rigaki \& Garcia~\cite{RigakiG24}, Salem et al.~\cite{SalemCEKPSTB23}, and Cummings et al.~\cite{CummingsHSS24} classify existing attack strategies by adversarial resources and goals in order to provide a structure for evaluating privacy risks.
In addition to this, Cohen~\cite{Cohen22} and Giomi et al.~\cite{giomi2022unified} take a different approach, appealing to the law to determine the goals of a realistic attacker.  Specifically, they contextualize the attacks they consider by tying them to existing privacy law. Still, individual attacks, even if successful and realistic, don't provide a clear path forward in terms of designing protections.

\medskip\noindent
\textbf{Identifying necessary conditions.} Some prior work has started to identify \emph{necessary} conditions for achieving privacy.
Cohen \& Nissim \cite{CohenN20b} introduce a necessary condition, called ``predicate singling out,'' inspired by the GDPR notion of singling out.
Balle et. al.~\cite{BalleCH22} introduce an alternative necessary condition called ``reconstruction robustness,'' which is closely related to reconstruction attacks. Cummings et. al.~\cite{CummingsHSS24} build on the notion of reconstruction robustness, extending it to a weaker adversarial setting. 
Our framework extends this general approach but applies to a much broader class of attacks---namely, any attacks from which a confidence rated predictor could be distilled.

Recent work by Cohen et al.~\cite{CohenKMMNST24} also recognizes the need to bridge the gap between formal privacy guarantees and practical attacks. Building on definitions in prior work \cite{BalleCH22, CohenN20b, CummingsHSS24} they introduce ``narcissus resiliency,'' a framework for establishing precise conditions under which an algorithm prevents various classes of existing attacks, including reconstruction attacks, singling out attacks, and membership inference attacks.
Our definition defines invulnerability against a different type of privacy loss, providing complementary insights in the form of necessary conditions that can be considered alongside their definitions.  Specifically, we believe that it is important to consider demographic coherence alongside their notion of narcissus singling out; the latter captures an important property that the former does not. \pj{(An algorithm that chooses a small subset of the data to publish in the clear does not meet the definition of \emph{singling out security} even though it may be \emph{demographic coherence enforcing} if the subset is small enough.)} 
Another key difference between our works is that the narcissus framework does not naturally lend itself to concrete experimental evaluation, whereas demographic coherence is intentionally designed with this use case in mind.
    
\medskip\noindent
Finally, \pj{most of the} works discussed above measure the success of an attack via its accuracy (\ie is the information extracted about the data subject \emph{true}?).  We observe that harm is not necessarily predicated on accuracy, and we design demographic coherence to be intentionally independent of accuracy.  One impact of this choice is that demographic coherence is a more natural fit for settings in which ground truth is difficult or impossible to measure.



