\section{Experimental Results}\label{sec:experiment}

To evaluate the effectiveness of our proposed model, 
we choose the property Flory-Huggins $\chi$-parameter which characterizes
the interaction between two molecules, solute $\bbC_1$ and solvent $\bbC_2$ 
to conduct numerical experiments.
Since it is well-known that the $\chi$-parameter value varies according to the temperature $T$, 
even for the same pair of solute-solvent pair\cite{Aoki:2023aa, Nistane:2022aa},
we consider the environment $\tau$ for the property $\chi$-parameter as a scalar representing the temperature $T$,
i.e., $\tau = (T)$, and thus one instance of $\chi$-parameter will be a triplet $(\bbC_1, \bbC_2, T)$.
Other environmental factors, such as pressure, are excluded due to their unavailability
in the data sets we collected.

We implemented the extended framework described in Section~\ref{sec:formulation} with a given range of $\chi$-parameter value and conducted experiments to evaluate the effectiveness.
The experiments were conducted on a PC with a Processor: Core i7-10700 (2.9GHz; 4.8 GHz at the maximum) and Memory: 16GB RAM DDR4.

\subsection{Experimental Results on Phase~1}\label{sec:exp_phase1}

We began by collecting two experimental $\chi$-parameter data sets that are
prepared by Aoki~et~al.~\cite{Aoki:2023aa} (\chiaoki) and 
Nistane~et~al.~\cite{Nistane:2022aa} (\chinistane), respectively. 
These two data sets were chosen for their experimental nature,
the relatively large number of available instances,
and the inclusion of temperature information for all solute-solvent pairs.
While the ``Flory-Huggins $\chi$ Database" from~\cite{pppdb} is also a widely-used data set in $\chi$-parameter studies, 
we opted to exclude it from our experiment 
due to the lack of temperature information for a significant number of solute-solvent pairs.

In addition to these two data sets, we also constructed a simulation-based $\chi$-parameter data set, \chijsol,
using the simulation software \jocta~\cite{JOCTA}.
In this data set, the $\chi$-parameter values between two molecules are calculated using the following formula:

\[
\chi = \chi_s + \frac{V}{R T} (\delta_A - \delta_B)^2,
\]
where $V$ represents the molar volume of the polymer-polymer/polymer-solvent system, 
$R = 8.314~\si{\joule / (\mole.\kelvin)}$ is the ideal gas constant, $T$ is the temperature in Kelvin ($\si{\kelvin}$), 
$\chi_s$ accounts for contributions besides enthalpy, and $\delta_A$ and $\delta_B$ are the Hildebrand solubility parameter of the two molecules.
For generating this data set,
we fixed the molar volume at $100~\si{\centi \metre^{3}/\mole}$,
and $\chi_s$ is set to be $0.34$ in the case of polymer-solvent pair and $0.0$ in the case of polymer-polymer pair.
Hildebrand solubility parameters for polymers were
calculated using either the Fedors method~\cite{Fedors:1974aa} or
the Krevelen method~\cite{Van-Krevelen:2009aa},
both are the so-called group-contribution methods implemented in \jocta.
%For each pair of molecules, 
%$\chi$-parameter values were computed at four distinct temperatures 
%$T \in \{ 273 \si{\kelvin}, 298\si{\kelvin}, 323\si{\kelvin}, 348\si{\kelvin} \}$.

The two data sets \chiaoki\ and \chinistane\ consist of only polymer-solvent pairs, i.e., the solute $\bbC_1$ is always a polymer, 
and the solvent $\bbC_2$ is always a small molecule.
In contrast, the data set \chijsol\
was generated by first collecting 88 polymers and calculating the $\chi$-parameter values for all possible polymer-polymer pairs
at four distinct temperatures 
$T \in \{ 273 \si{\kelvin}, 298\si{\kelvin}, 323\si{\kelvin}, 348\si{\kelvin} \}$
using \jocta. In this process, the Fedors method was used to calculate the Hildebrand solubility parameters.
When generating this data set, we treated one polymer-polymer pair symmetrically, i.e., 
given a fixed temperature, two instances were created such that for one instance, one polymer was used as the solute and the other was the solvent,
and for the other instance, the roles of solute and solvent were reversed.
It is worth noting that the two available group-contribution methods (Fedors and Krevelen)
can produce different solubility parameter values.
Figure~\ref{fig:figure_chiFKdiff} presents a histogram of the differences between $\chi$-parameter values calculated
using these two methods for polymer-polymer pairs in the \chijsol\ data set.
Notably, the 95\% percentile of the differences is 3.298,
highlighting the variability in the results from the two approaches,
and also the difficulty of the task to accurately calculate $\chi$-parameter values.

\begin{figure}[t!]
\begin{center} 
 \includegraphics[width=.89\columnwidth]{figures/chiF_chiK_diff.eps}
\end{center}
\caption{A histogram of the differences between two methods (Fedors and Krevelen) available in \jocta\
for the data set \chijsol.
}
\label{fig:figure_chiFKdiff}  
\end{figure} 


 
For a data set $D_\chi$, we denote the set of all different solutes (resp., solvents) appearing in it by $D_{\chi, 1}$ (resp., $D_{\chi,2}$).
In the preprocessing procedure, we first convert all the polymers to their monomer representations that are defined in Section~\ref{sec:polymer}, and then remove some polymers such that a self-loop appears in the monomer form as in~\cite{Ido:2024aa} for each data set $D_\chi \in \{ $\chiaoki,  \chinistane, \chijsol$ \} $. 

We then calculated the proposed solute-solvent feature vectors $f_\chi(\bbC_1, \bbC_2, T)$ as defined in Section~\ref{sec:formulation_phase1}.
For the feature function $f_i$ used for each chemical graph $\bbC_i$, 
we choose the one consisting of only graph-theoretic descriptors defined 
in~\cite{Ido:2024aa} (resp., \cite{Zhu:2022ad}) in the case that $\bbC_i$ is a polymer (resp., a small molecule),
for $i=1, 2$, respectively.
See Appendix~\ref{sec:descriptor} for a complete list of descriptors used in the experiments.
For the environment $\tau = (T)$,
we tried three different kinds of feature functions as follows:
\begin{itemize}
%\item[-] $f_{\tau, 0}(T) = \emptyset$ (i.e., don't use the information of $\tau$); {\color{red} Any other notations?}
\item[-] $f_{\tau, 1}(T) = (T) $;
\item[-] $f_{\tau,-1}(T) =  (\frac{1}{T}) $;
\item[-] $f_{\tau, 1, -1}(T) =  (T,  \frac{1}{T} )$.
\end{itemize}
These three simple feature functions for temperature are selected based on intuitive considerations. 
More complex transformations, such as $T^2$ or $\frac{1}{T^2}$, could introduce extreme value distributions
after standardization, making it less effective and harder to contribute to a prediction function.

Together with the one that neglects the environment $\tau$,
we obtained four kinds of feature vectors for a $\chi$-parameter instance  $(\bbC_1, \bbC_2, T)$ as follows:
\begin{itemize}
\item[-] $f_\chi (\bbC_1, \bbC_2, T) \triangleq [  f_1(\bbC_1) \ f_2(\bbC_2) ] $;
\item[-] $f_{\chi, 1}(\bbC_1, \bbC_2, T) \triangleq [ f_1(\bbC_1) \ f_2(\bbC_2) \ T ]$;
\item[-] $f_{\chi,-1}(\bbC_1, \bbC_2, T) \triangleq [ f_1(\bbC_1) \ f_2(\bbC_2) \ \frac{1}{T} ]$;
\item[-] $f_{\chi,1, -1}(\bbC_1, \bbC_2, T) \triangleq [ f_2(\bbC_1) \ f_2(\bbC_2) \ T \ \frac{1}{T} ]$.
\end{itemize}


%\begin{table}[t!]\caption{Results of Stages~1 and~2.}
%\begin{center}\scalebox{0.99}{
%\begin{tabular}{@{} c c c c c c c c c c c @{}} \hline
%$D_\chi$ & $|D_\chi|$ & $\underline{n_1}, \overline{n_1}$ & $\underline{n_2}, \overline{n_2}$ & $\underline{a}, \overline{a}$ & $|\Gamma_1|$ &  $|\mathcal{F}_1|$ & $K_1$ & $|\Gamma_2|$ &  $|\mathcal{F}_2|$ & $K_2$ \\ \hline
%\chiaoki\cite{Aoki:2023aa} & 1190 & 2, 23 & 1, 16  & -2.24, 4.40  & 13 & 31 & 88 & 11 & 89 & 136 \\
%\chinistane\cite{Nistane:2022aa} & 1581 & 2, 42 & 1, 16 & -0.55, 4.11 &  17 & 34 & 106 & 11 & 89 & 139 \\ 
%\chijsol & 30624 & 2, 50 & 2, 50 & 0.0, 10.797 & 19 & 39 & 119 & 19 & 39 & 119 \\ \hline
%\end{tabular}
%}
%\end{center}\label{table:phase1a}
%\end{table}

\begin{table}[t!]\caption{Information on the data sets.}
\begin{center}\scalebox{0.99}{
\begin{tabular}{@{} c c c c c c c c c @{}} \hline
$D_\chi$ & $|D_\chi|$ & $\underline{n_1}, \overline{n_1}$ & $\underline{n_2}, \overline{n_2}$ & $\underline{a}, \overline{a}$ &  $|\mathcal{F}_1|$ & $K_1$ & $|\mathcal{F}_2|$ & $K_2$ \\ \hline
\chiaoki\cite{Aoki:2023aa} & 1190 & 2, 23 & 1, 16  & $-$2.24, 4.40  &  31 & 88 &  89 & 136 \\
\chinistane\cite{Nistane:2022aa} & 1581 & 2, 42 & 1, 16 & $-$0.55, 4.11 &   34 & 106 &  89 & 139 \\ 
\chijsol & 30624 & 2, 50 & 2, 50 & 0.0, 10.797 & 39 & 119 & 39 & 119 \\ \hline
\end{tabular}
}
\end{center}\label{table:phase1a}
\end{table}

We follow~\cite{Ido:2024aa} to first standardize the range of each descriptor and 
the range $\{ t \in \bbR \mid \underline{a} \leq t \leq \overline{a} \}$ of the property value $a(\bbC_1, \bbC_2, T)$.
Table~\ref{table:phase1a} summarizes 
some statistical information on the
data sets we prepared for $\chi$-parameter in Stage~1,
where the following notations are used:
\begin{itemize}
\item[-] $D_\chi$: the name of the data set;
\item[-] $|D_\chi|$: the total number of instances in $D_\chi$;
\item[-] $\underline{n_1}, \overline{n_1}$: the minimum and maximum values for the number of non-hydrogen atoms $n(\bbC_1)$ in solutes $\bbC_1$ within $D_{\chi, 1}$;
\item[-] $\underline{n_2}, \overline{n_2}$: the minimum and maximum values for the number of non-hydrogen atoms $n(\bbC_2)$ in solvents $\bbC_2$ within $D_{\chi, 2}$;
\item[-] $\underline{a}, \overline{a}$: the minimum and maximum values of $a(\bbC_1, \bbC_2, T)$ over the triplets $(\bbC_1, \bbC_2, T)$ in $D_\chi$;
%\item[-] $|\Gamma_1|$: the number of different edge-configurations for interior-edges over the compounds in $D_{\chi, 1}$;
\item[-] $|\mathcal{F}_1|$: the number of distinct non-isomorphic chemical rooted trees among all 2-fringe-trees in $D_{\chi, 1}$;
\item[-] $K_1$: the number of descriptors included in a feature vector $f(\bbC_1)$ for solutes $\bbC_1$;
%\item[-] $|\Gamma_2|$: the number of different edge-configurations for interior-edges over the compounds in $D_{\chi, 2}$;
\item[-] $|\mathcal{F}_2|$: the number of distinct non-isomorphic chemical rooted trees among all 2-fringe-trees in $D_{\chi, 2}$; and
\item[-] $K_2$: the number of descriptors included in a feature vector $f(\bbC_2)$ for solvents $\bbC_2$.
\end{itemize}


We used three different machine learning methods to 
construct a prediction function $\eta$ from the feature vector $f_\chi(\bbC_1, \bbC_2, T)$ 
to the observed value $a(\bbC_1, \bbC_2, T)$, 
namely ANN, %artificial neural network (ANN), 
multiple linear regression with reduced quadratic descriptors (R-MLR)~\cite{Zhu:2025aa}, 
and random forest (RF). 
%We refer~\cite{Azam:2021ab} for ANN, \cite{Zhu:2022ac} for R-MLR, and~\cite{Tanaka:2021aa} \qqq for RF 
%for a detailed description of these machine learning methods, respectively. 



For a given data set $D$,
let $x^i \triangleq f(\bbC_1^i, \bbC_2^i, T^i)$ and $a^i \triangleq a(\bbC_1^i, \bbC_2^i, T^i)$ for short,
where $(\bbC_1^i, \bbC_2^i, T^i) \in D$ represents an indexed triplet of instance.
To evaluate the learning performance of a prediction function $\eta$ on the data set $D$,
we define an error function
\[
\mathrm{Err}(\eta; D) \triangleq \sum_{(\bbC_1^i, \bbC_2^i, T^i) \in D} (a^i - \eta(x^i))^2,
\]
and the \emph{coefficient of determination} $\Rt(\eta, D)$ to be
\[
\Rt(\eta, D) \triangleq 1 - \frac{\mathrm{Err}(\eta; D)}{\sum_{(\bbC_1^i, \bbC_2^i, T^i) \in D} (a^i - \tilde{a})^2},
\]
 where $\tilde{a} \triangleq \frac{1}{|D|} \sum_{(\bbC_1^i, \bbC_2^i, T^i) \in D} a^i$  the average property value in the data set $D$.


\begin{table}[t!]\caption{Results of Stage~3 for the data set \chiaoki.}
\begin{center}\scalebox{0.99}{
\begin{tabular}{@{} c c c c @{}} \hline
 $f_\chi$ & ML & Train $\Rt$ & Test $\Rt$  \\ \hline
 & ANN & 0.922 & 0.725 \\
 $f_\chi(\bbC_1, \bbC_2)$ & R-MLR & 0.892 & 0.841 \\
& RF & 0.917 & 0.753 \\ \hline
& ANN & 0.922  & 0.725 \\
 $f_{\chi,1}(\bbC_1, \bbC_2, T)$ & R-MLR & 0.919 & *{\bf 0.891} \\
 & RF & 0.971 & 0.822 \\ \hline
 & ANN & 0.990 & {\bf 0.801} \\
 $f_{\chi,-1}(\bbC_1, \bbC_2, T)$ & R-MLR & 0.919 & 0.863 \\
 & RF & 0.973 & {\bf 0.822} \\ \hline
 & ANN & 0.990 & 0.767 \\
 $f_{\chi,1,-1}(\bbC_1, \bbC_2, T)$ & R-MLR & 0.921 & 0.879  \\
 & RF & 0.971  & 0.819 \\ \hline
\end{tabular}
}
\end{center}\label{table:phase1b_aoki}
\end{table}

\begin{table}[t!]\caption{Results of Stage~3 for the data set \chinistane.}
\begin{center}\scalebox{0.99}{
\begin{tabular}{@{} c c c c @{}} \hline
 $f_\chi$ & ML & Train $\Rt$ & Test $\Rt$  \\ \hline
 & ANN & 0.863 & 0.671 \\
$f_\chi(\bbC_1, \bbC_2)$ & R-MLR & 0.835 & 0.776 \\
 & RF & 0.882 & 0.731 \\ \hline
 & ANN & 0.971  & {\bf 0.684} \\
 $f_{\chi,1}(\bbC_1, \bbC_2, T)$ & R-MLR & 0.840 & {\bf 0.793} \\
 & RF & 0.968 & 0.793 \\ \hline
 & ANN & 0.950 & 0.655 \\
 $f_{\chi,-1}(\bbC_1, \bbC_2, T)$ & R-MLR & 0.824 & 0.768 \\
 & RF & 0.968 & 0.792 \\ \hline
 & ANN & 0.871 & 0.647 \\
$f_{\chi,1,-1}(\bbC_1, \bbC_2, T)$ & R-MLR & 0.860 & *{\bf 0.801}  \\
 & RF & 0.967  & 0.792 \\ \hline
\end{tabular}
}
\end{center}\label{table:phase1b_nistane}
\end{table}

\begin{table}[t!]\caption{Results of Stage~3 for the data set \chijsol.}
\begin{center}\scalebox{0.99}{
\begin{tabular}{@{} c c c c @{}} \hline
 $f_\chi$ & ML & Train $\Rt$ & Test $\Rt$  \\ \hline
 & ANN & 0.980 & 0.975  \\
$f_\chi(\bbC_1, \bbC_2)$ & R-MLR & 0.745 & 0.741  \\
 & RF & 0.987  &  0.977 \\ \hline
 & ANN & 0.993  &  0.990 \\
 $f_{\chi,1}(\bbC_1, \bbC_2, T)$ & R-MLR & 0.756 & 0.750  \\
 & RF & 0.998 & *{\bf 0.991} \\ \hline
 & ANN & 0.993  & {\bf 0.991}  \\
 $f_{\chi,-1}(\bbC_1, \bbC_2, T)$ & R-MLR & 0.787  & {\bf 0.782}  \\
 & RF & 0.998  & 0.991 \\ \hline
 & ANN & 0.993  & 0.991  \\
$f_{\chi,1,-1}(\bbC_1, \bbC_2, T)$ & R-MLR & 0.762 & 0.758  \\
 & RF & 0.998  & 0.991 \\ \hline
\end{tabular}
}
\end{center}\label{table:phase1b_jsol}
\end{table}


In Stage~3, we first conducted a preliminary experiment to decide the hyperparameter used for 
each machine learning method of ANN, R-MLR, and RF, and then conducted the experiments to 
evaluate the learning performance of each prediction function for each data set based on cross-validation. 
Please refer~\cite{Azam:2021ab, Zhu:2025aa} for the details about how we conducted these preliminary experiments.
For a given data set $D$, a \emph{cross-validation} procedure consists of five trials of constructing a prediction function as follows. 
The data set $D$ is first randomly divided into five subsets $D^{(k)}$, $k \in [1,5]$. For each trial $k \in [1,5]$, a prediction function $\eta^{(k)}$ is constructed using the hyperparameter determined in the preliminary experiment, with the subset $D \backslash D^{(k)}$ serving as the training data set, respectively.
We did ten times cross-validations for each data set and each kind of feature vector proposed in Section~\ref{sec:formulation_phase1}. 
The learning results are summarized in Tables~\ref{table:phase1b_aoki} to \ref{table:phase1b_jsol} 
%Tables~\ref{table:phase1b_aoki} to \ref{table:phase1b_jsol} show the computational results 
for the data sets \chiaoki, \chinistane\ and \chijsol, respectively, where the following notations are used:
\begin{itemize}
%\item[-] $D_\chi$: name of the data set;
\item[-] $f_\chi$: the feature vector for the triplet $(\bbC_1, \bbC_2, T)$ proposed in Section~\ref{sec:formulation_phase1};
\item[-] ML: the machine learning method used to construct a prediction function;
\item[-] Train/Test $\Rt$: the median of train/test $\Rt$ scores over all 50 trials in ten cross-validations; and
\item[-] the bold score indicates the highest $\Rt$ score achieved across the four different feature vectors for each learning method,
while the score marked with an asterisk denotes the overall highest $\Rt$ score among the three machine learning methods for each data set.
%\item[-] the bold score marked with an asterisk indicted the best performance among the three machine learning methods and four different kinds of feature vector for each data set.
\end{itemize}

%The top test $\Rt$ score is made bold in the tables for each data set.

We can observe from Tables~\ref{table:phase1b_aoki} to~\ref{table:phase1b_jsol} that, 
in general, the feature function containing the information of the temperature 
enjoys a better learning performance than the simple one $f_\chi(\bbC_1, \bbC_2)$,
which matches our intuition.
%However, despite the domain knowledge that 
%$\chi$-parameter value can be theoretically expressed 
%as a linear equation as
%\[
%\chi = A + \frac{B}{T},
%\]
%we found that using $T$ instead of $\frac{1}{T}$ can indeed improve the learning performance, 
%which is a surprise.

As a comparison, Aoki et al.~\cite{Aoki:2023aa} attained an average $\Rt$ score of 0.834 on one-time cross-validation
 by using neural networks on the data set \chiaoki, 
 and Nistane et al.~\cite{Nistane:2022aa} attained a $\Rt$ score of 0.83 on a 10\% test set
  by utilizing Gaussian process regression (GPR) on the data set \chinistane. Although the specific way to evaluate the learning performance used in these works is different from ours, we demonstrate that our proposed feature vector can attain a competitively high learning performance.

\subsection{Experimental Results on Phase~2}\label{sec:exp_phase2}

To execute Phase~2 to infer chemical graphs with desired $\chi$-parameter values, 
we used two instances $I_a$ and $I_b$ prepared by Ito~et~al.~\cite{Ido:2024aa}. 
Figures~\ref{fig:seed_graph_a} and~\ref{fig:seed_graph_b} 
illustrate the seed graphs $G_{\mathrm{C}}$ and the sets $\mathcal{F}$ of chemical rooted trees for $I_a$ and $I_b$, respectively. 
See Appendix~\ref{sec:test_instances} for a detailed description of the instances.
%Here, the instance $I_b$ is introduced to represent a set of polymers that includes the four examples of polymers in Figure~\ref{fig:four_polymers}.

%\begin{figure}[t!]
%\begin{center} 
% \includegraphics[width=.89\columnwidth]{figures/test_seed_graph_polymer.eps}
%\end{center}
%\caption{(i) A seed graph $G_{\mathrm{C}}$ for $I_a$; (ii) A set $\mathcal{F}$ of chemical rooted trees.
%}
%\label{fig:seed_graph_a}  
%\end{figure} 

\begin{figure}[t!]
\begin{center} 
 \includegraphics[width=.89\columnwidth]{figures/test_set_fringe-trees_polymers.eps}
\end{center}
\caption{(i) A seed graph $G_{\mathrm{C}}$ for $I_b$; (ii) A set $\mathcal{F}$ of chemical rooted trees.
The figure is adapted from~\cite{Ido:2024aa}.
}
\label{fig:seed_graph_b}  
\end{figure}

%\begin{figure}[t!]
%\begin{center} 
% \includegraphics[width=.89\columnwidth]{figures/four_polymers.eps}
%\end{center}
%\caption{Illustrations of  four polymers: 
%(i)  1$\underline{~}$1-(2-methylPropane)Bis(4-phenyl)carbonate;
%(ii) 2$\underline{~}$2-pentaneBis(4-phenyl)carbonate;
%(iii)  1$\underline{~}$1-dichloroethyleneBis(4-phenyl)carbonate;
%(iv) thioBis(4-phenyl)carbonate.
%Hydrogens are omitted and connecting-edges are depicted with thick lines. }
%\label{fig:four_polymers}  
%\end{figure} 

We executed Stage~4 for the two data sets $D \in \{ $\chiaoki, \chinistane $ \} $, 
with three different machine learning methods ANN, R-MLR, RF, respectively. 
Here the feature vector is selected as the one with the best median test $\Rt$ score attained 
for each learning method in Tables~\ref{table:phase1b_aoki} and \ref{table:phase1b_nistane}, and
the prediction functions used are the ones that attained the median test $\Rt$.
For the detailed MILP formulations that are formulated to simulate the prediction functions, 
we refer~\cite{Akutsu:2019aa} for ANN, and~\cite{Zhu:2025aa} for R-MLR, respectively.
The one for RF will appear in our future work,
while the formulation for decision tree is presented in~\cite{Tanaka:2021aa}.

%{\color{red} 
For the inference of chemical graphs with desired $\chi$-parameter values,
we focus on the situation that the temperature $T$ and the solvent $\bbC_2$ are fixed, i.e., inferring the solute $\bbC_1$.
%Meanwhile, in the practical situation, the solvent $\bbC_2$ and temperature $T$ are usually considered to be constant.
%People want to design a solute $\bbC_1$ such that it enjoys some desired $\chi$-parameter value when interacting with
%the fixed solvent $\bbC_2$ under the fixed temperature.
%Taking this into consideration, in this paper,
%we mainly focus on the task
%to infer a solute $\bbC_1$  such that the solute-solvent pair consisting of the solute and a fixed solvent $\bbC_2$ has a desired 
%$\chi$-parameter value under a fixed temperature $T$.
%}
%
%As described in Section~\ref{sec:formulation_phase2}, the solvent $\bbC_2$ and the temperature $T$ were fixed in our experiment. 
We selected carbon tetrachloride (resp., methyl acetate) as the solvent $\bbC_2^{\mathrm{F}}$ and both 373.5~\si{\kelvin} 
as the temperature during the experiment for the data set \chiaoki \ (resp., \chinistane). 
$\mathrm{CPLEX}$ 12.10 was used to solve the MILPs in Stage~4. 



\begin{table}[t!]\caption{Results of Stages~4 and~5 of \chiaoki\ and \chinistane.}
\begin{center}\scalebox{0.69}{
\begin{tabular}{@{} c | c c c c c | c c c c c c | c c c @{}} \hline
No. & $D_\chi$ & $f_\chi$ & ML & inst. & $\underline{y}^*, \overline{y}^*$ & \#v & \#c & I-time & $n$ & $n^{\mathrm{int}}$ & $\eta$
& D-time & $\bbC$-LB & \#$\bbC$  \\ \hline
(a) & \chiaoki & $f_{\chi, -1}(\bbC_1, \bbC_2, T)$ & ANN & $I_a$ & 0.60, 0.70 & 10511 & 14017 & 518.289 & 48 & 27 & 0.624 & 0.114 & 8 & 8 \\
(b) &              & $                                        $ &           & $I_b$ & 1.00, 1.10 & 7062 & 8870 & 53.274 & 25 & 18 & 1.013 & 0.033 & 2 & 2 \\ \hline
(c) & \chiaoki & $f_{\chi, 1}(\bbC_1, \bbC_2, T)$ & R-MLR & $I_a$ & $-$0.10, 0.00 & 13177 & 17506 & 12.657 & 50 & 30 & -0.013 & 0.150 & 20 & 20 \\
(d) &              & $                                        $ &                  & $I_b$ & 1.50, 1.60 & 9728 & 12358 & 3.880 & 28 & 18 & 1.518 & 0.037 & 3 & 3 \\ \hline
(e) & \chiaoki & $f_{\chi, -1}(\bbC_1, \bbC_2, T)$ & RF & $I_a$ & 0.40, 0.50 & 62366 & 697356 & 33.213 & 50 & 28 & 0.476 & 0.115 & 8 & 8  \\
(f) &              & $                                        $ &       & $I_b$ & 0.60, 0.70 & 58917 & 692208 & 17.050 & 29 & 18 & 0.655 & 0.046 & 6 & 6  \\ \hline
(g) & \chinistane & $f_{\chi, 1}(\bbC_1, \bbC_2, T)$ & ANN & $I_a$ & 0.25, 0.35 & 12185 & 15198 & 37.684 & 50 & 30 & 0.303 & 0.194 & 32 & 32 \\
(h) &              & $                                        $ &              & $I_b$ & 0.40, 0.50 & 8061 & 9930 & 5.274 & 27 & 18 & 0.466 & 0.105 & 20 & 20 \\ \hline
(i) & \chinistane & $f_{\chi, 1, -1}(\bbC_1, \bbC_2, T)$ & R-MLR & $I_a$ & 0.60, 0.70 & 15035 & 18664 & 9.851 & 49 & 27 & 0.678 & 0.107 & 4 & 4 \\
(j) &              & $                                        $ &                  & $I_b$ & 3.20, 3.30 & 10913 & 13397 & 7.469 & 31 & 22 & 3.241 & 0.196 & 2706 & 100 \\ \hline
(k) & \chinistane & $f_{\chi, 1}(\bbC_1, \bbC_2, T)$ & RF & $I_a$ & 0.80, 0.90 & 80250 & 1001951 & 40.093 & 50 & 30 & 0.867 & 0.131 & 12 & 12 \\
(l) &              & $                                        $ &           & $I_b$ & 0.70, 0.80 & 76128 & 996684 & 25.653 & 28 & 18 & 0.742 & 0.035 & 2 & 2 \\ \hline
\end{tabular}
}
\end{center}\label{table:phase2}
\end{table}

The computational results of the experiment in Stage 4 are shown in
Table~\ref{table:phase2},
where the following notations are used:
\begin{itemize}
\item[-] No.: numbering of the MILP instance;
\item[-] $D_\chi$: name of the data set;
\item[-] $f_\chi$: the feature vector for the triplet $(\bbC_1, \bbC_2, T)$ proposed in Section~\ref{sec:formulation_phase1};
\item[-] ML: the machine learning method used to construct a prediction function;
\item[-] inst.: instance $I_a$ or $I_b$;
\item[-] $\underline{y}^*, \overline{y}^*$: range $[\underline{y}^*, \overline{y}^*]$ on the value $a(\bbC_1, \bbC_2, T)$ of a polymer $\bbC_1$ to be inferred;
\item[-] \#v (resp., \#c): the number of variables (resp., constraints) in the MILP; %in Stage 4;
\item[-] I-time: the time (in seconds) to solve the MILP; %in Stage 4;
\item[-] $n$: the number $n(\bbC_1^\dagger)$ of non-hydrogen atoms of the inferred polymer in the monomer representation $\bbC_1^\dagger$; %inferred in Stage 4;
\item[-] $n^{\mathrm{int}}$: the number $n^{\mathrm{int}}(\bbC_1^\dagger)$ of interior-vertices of the inferred polymer in the monomer representation $\bbC_1^\dagger$; and % inferred in Stage 4; and
\item[-] $\eta$: the predicted $\chi$-parameter value $\eta(f_\chi(\bbC_1^\dagger, \bbC_2^{\mathrm{F}}, T))$ of the inferred polymer $\bbC_1^\dagger$. %inferred in Stage 4.
\end{itemize}

\begin{figure}[t!]
\begin{center} 
 \includegraphics[width=.99\columnwidth]{figures/milp_results_all.eps}
\end{center}
\caption{Illustration of the solute polymers inferred in Stage~4. The link-edges are shown with thick lines, and the two connecting-vertices are marked with asterisks. (a)-(l) correspond to the first column No. in Table~\ref{table:phase2}, respectively.
}
\label{fig:milp_results}  
\end{figure}

From Table~\ref{table:phase2}, we observe that all 12 instances can be solved within 520 seconds, with 28 to 50 non-hydrogen atoms
in their monomer presentation,
and the predicted $\chi$-parameter values $\eta(f_\chi(\bbC_1^\dagger, \bbC_2^{\mathrm{F}}, T))$ 
all fall inside the range $[\underline{y}^*, \overline{y}^*]$ that we specified.
The inferred molecules are illustrated in Figure~\ref{fig:milp_results}.

Finally, we executed Stage~5 to generate more target chemical graphs $\bbC^*$, i.e., the chemical isomers of $\bbC^*$.
Utilizing the algorithm proposed by Zhu~et~al.~\cite{Zhu:2022ad},
we generated the chemical isomers for each target chemical graph $\bbC^\dagger$ inferred in Stage~4.
If the total number of chemical isomers exceeded 100, we limited the generation to the first 100 isomers.
Additionally, the algorithm can also provide an evaluation of a lower bound 
on the total number of possible chemical isomers of $\C^\dagger$.
%without generating all of them.
The computational results are summarized in 
%We show the computational results of the experiment in Stage~5 in 
Table~\ref{table:phase2}, where the following notations are used:
\begin{itemize}
\item[-] D-time: the execution time (in seconds) of the algorithm in Stage 5;
\item[-] $\bbC$-LB: a lower bound on the number of all chemical isomers $\bbC^*_1$ of $\bbC_1^\dagger$; and
\item[-] \#$\bbC$: the number of generated chemical isomers $\bbC^*_1$ of $\bbC_1^\dagger$. %generated.
\end{itemize}


It can be observed that the algorithm can find chemical isomers efficiently, 
in less than 0.2 seconds for all cases.

\subsection{Comparison with the Simulation Software \jocta}
In this subsection, we present experimental results of inferring chemical graphs based on the data set \chijsol\
to demonstrate that the inferred polymer-polymer pairs are of good quality, by comparing the $\chi$-parameter
values inferred by prediction functions to those computed using the simulation software \jocta~\cite{JOCTA}.

For this purpose, we prepared two instances $I_{\mathrm{c1}}$ and $I_{\mathrm{c2}}$ which are similar to the polymers in our collected data sets.
The two seed graphs for $I_{\mathrm{c1}}$ and $I_{\mathrm{c2}}$ are illustrated in Figure~\ref{fig:figure_c1c2}(a) and (b), respectively, with the detailed descriptions provided in Appendix C.
Instance $I_{\mathrm{c1}}$ is designed to represent a relatively simple polymer structure, whereas
instance $I_{\mathrm{c2}}$ is introduced for the situation that a benzene ring is included.
Although RF achieved the best learning performance in Table~\ref{table:phase1b_jsol},
we selected ANN as the learning method for constructing the prediction function.
This choice was motivated by the strong learning performance obtained by ANN and the significantly reduced
number of variables and constraints in the MILP compared to RF,
which can be observed in Table~\ref{table:phase2}.
For each instance, we selected four polymers 
(polyethylene (PE), polyethylene glycol (PEG), polystyrene sulfonates (PSS), and polyphthalamide (PPA)) 
as the fixed solvent $\bbC_2^{\mathrm{F}}$.
Additionally, we defined nine ranges of target $\chi$-parameter values ($[0.20, 0.70]$, $[0.40, 0.90]$, 
$[0.70, 1.20]$, $[0.90, 1.40]$, $[1.20, 1.70]$, $[1.40, 1.90]$, $[1.70, 2.20]$, $[1.90, 2.40]$, and $[2.20, 2.70]$)
and four temperatures ($273 \si{\kelvin}, 298\si{\kelvin}, 323\si{\kelvin},$ and $348\si{\kelvin}$)
to conduct the experiments, resulting in a total of 288 MILPs.

\begin{figure}[t!]
\begin{center} 
 \includegraphics[width=.89\columnwidth]{figures/figure_3_c1c2.eps}
\end{center}
\caption{Seed graphs for instances (a) $I_{\mathrm{c1}}$ and (b) $I_{\mathrm{c2}}$, respectively.
}
\label{fig:figure_c1c2}  
\end{figure} 

For these experiments, ANN was used
as the machine learning method to construct the prediction function
and $\mathrm{CPLEX}$ 12.10 was used to solve the MILPs,
with a time limit of 3600 seconds for each MILP. 
Once a polymer was inferred from MILP,
we utilized \jocta\ to compute the $\chi$-parameter value between the polymer as the solute and the corresponding solvent
 using the same way for generating the data set \chijsol.

Out of the 288 MILPs,
251 (87.2\%) were successfully solved within the time limit. 
%(Detailed results can be found in Supplement File \qqq)
We then computed the differences between the $\chi$-parameter values obtained from the MILPs
and those calculated using \jocta\ via the Fedors method.
Figure~\ref{fig:figure_chiMFdiff} presents a histogram of these differences,
showing that 95\% of the differences are less than 2.399.
Notably, the 95\% percentile for the differences between the MILP results and the Fedors method
is smaller than that for the differences between the Fedors method and Krevelen method, suggesting that
the differences are acceptable given that both methods are considered valid for predicting $\chi$-parameter values.
Since accurate calculation of $\chi$-parameter values is inherently challenging,
as evidenced in Figure~\ref{fig:figure_chiFKdiff},
where discrepancies between different methods are apparent,
we content ourselves that the polymers inferred by our MILP approach
are of high quality,
based on the relatively close agreement between the values obtained from MILP
and those calculated by \jocta.

\begin{figure}[t!]
\begin{center} 
 \includegraphics[width=.89\columnwidth]{figures/chiF_MILP_diff.eps}
\end{center}
\caption{A histogram of the differences between the Fedors method in \jocta\ and the value obtained from MILP.
}
\label{fig:figure_chiMFdiff}  
\end{figure} 

We illustrate some solutions whose $\chi$-parameter value obtained from MILP is close to the one calculated 
by \jocta\ using the Fedors method in Figure~\ref{fig:milp_results_JOCTA}. 
The detailed results for them are summarized in Table~\ref{table:milp_JOCTA}, where we denote the following:
\begin{itemize}
\item[-] No.: numbering of the MILP instance;
\item[-] $f_\chi$: the feature vector for the triplet $(\bbC_1, \bbC_2, T)$ proposed in Section~\ref{sec:formulation_phase1};
\item[-] ML: the machine learning method used to construct a prediction function;
\item[-] inst.: instance $I_{\mathrm{c1}}$ or $I_{\mathrm{c2}}$;
\item[-] $\underline{y}^*, \overline{y}^*$: range $[\underline{y}^*, \overline{y}^*]$ on the value $a(\bbC_1, \bbC_2, T)$ of a polymer $\bbC_1$ to be inferred;
\item[-] $\bbC_2^{\mathrm{F}}$: the selected solvent; %in Stage 4;
\item[-] $T$: the selected temperature; %in Stage 4;
\item[-] I-time: the time (in seconds) to solve the MILP; %in Stage 4;
\item[-] $\chi_M$: the predicted $\chi$-parameter value $\eta(f_\chi(\bbC_1^\dagger, \bbC_2^{\mathrm{F}}, T))$ of the inferred polymer $\bbC_1^\dagger$; %inferred in Stage 4;
\item[-] $\chi_F$: the $\chi$-parameter value calculated between $\bbC_1^\dagger$ and $\bbC_2^{\mathrm{F}}$
under the temperature $T$ using the Fedors method implemented in \jocta; and
\item[-] $|\chi_M - \chi_F|$: the absolute difference between $\chi_M$ and $\chi_F$.
\end{itemize}

%The polymer in Figure~\ref{fig:milp_results_JOCTA}(a) is solved for instance $I_{c1}$, under the condition that the range of target $\chi$-parameter value $[1.20, 1.70]$, PEG as the fixed solvent $\bbC_2^{\mathrm{F}}$, and temperature of 273 $\si{\kelvin}$. The $\chi$-parameter value obtained from MILP is 1.242205, whereas the one calculated by \jocta\ is 1.242973, resulting in a difference of merely 0.00768.
%The polymer in Figure~\ref{fig:milp_results_JOCTA}(b) is solved for instance $I_{c2}$, under the condition that the range of target $\chi$-parameter value $[1.90, 2.40]$, PE as the fixed solvent $\bbC_2^{\mathrm{F}}$, and temperature of 298 $\si{\kelvin}$. The $\chi$-parameter value obtained from MILP is 2.230320, whereas the one calculated by \jocta\ is 2.256827, resulting in a difference of 0.026507.

\begin{figure}[t!]
\centering
\begin{minipage}{.45\textwidth}
\begin{center}
 \includegraphics[width=.45\columnwidth]{figures/milp_results_JOCTA_a_gray.eps}
\end{center}
\subcaption{}
\end{minipage}
%\hspace{0.1\textwidth}
\begin{minipage}{.45\textwidth}
\begin{center}
 \includegraphics[width=.45\columnwidth]{figures/milp_results_JOCTA_b_gray.eps}
\end{center}
\subcaption{}
\end{minipage}
\begin{minipage}{.45\textwidth}
\begin{center}
 \includegraphics[width=.45\columnwidth]{figures/milp_results_JOCTA_c_gray.eps}
\end{center}
\subcaption{}
\end{minipage}
%\hspace{0.1\textwidth}
\begin{minipage}{.45\textwidth}
\begin{center}
 \includegraphics[width=.45\columnwidth]{figures/milp_results_JOCTA_d_gray.eps}
\end{center}
\subcaption{}
\end{minipage}
\caption{Illustrations of the solute polymers inferred in Stage~4 using the data set \chijsol.
The link-edges are shown with thick lines, and the two connecting-vertices are marked with asterisks.
}
\label{fig:milp_results_JOCTA}  
\end{figure}


\begin{table}[t!]\caption{Selected results of Stage~4 for the data set \chijsol}
\begin{center}\scalebox{0.85}{
\begin{tabular}{|c|cccccc|c|cc|c|}
\hline
No. & $f_\chi$ & ML & inst. & $\underline{y}^*, \overline{y}^*$ &  $\bbC_2^{\mathrm{F}}$ & $T$ & I-time & $\chi_M$ & $\chi_F$ & $|\chi_M - \chi_F|$ \\ \hline
(a) & $f_{\chi, -1}(\bbC_1, \bbC_2, T)$ & ANN & $I_{c_1}$ & $0.40, 0.90$ & PPA & 298 $\si{\kelvin}$  & 14.723 & 0.790285 & 0.813052 & 0.022766 \\ \hline 
(b) & $f_{\chi, -1}(\bbC_1, \bbC_2, T)$ & ANN & $I_{c_1}$ & $0.90, 1.40$ & PSS & 273 $\si{\kelvin}$  & 13.305 & 1.149399 & 1.147394 & 0.002005 \\ \hline 
(c) & $f_{\chi, -1}(\bbC_1, \bbC_2, T)$ & ANN & $I_{c_2}$ & $0.20, 0.70$ & PE   & 348 $\si{\kelvin}$ & 34.422  & 0.644275 & 0.723993  & 0.079719 \\ \hline
(d) & $f_{\chi, -1}(\bbC_1, \bbC_2, T)$ & ANN & $I_{c_2}$ & $0.70, 1.20$ & PEG   & 298 $\si{\kelvin}$ & 65.631  & 0.972358 & 0.963446  & 0.008912 \\ \hline
\end{tabular}
\label{table:milp_JOCTA}
}
\end{center}
\end{table}



%Moreover, since \jocta\ currently does not support direct
As far as we know, there is no method that conducts
 inference of molecules or molecule pairs
with specific $\chi$-parameter values,
therefore we believe that our proposed method 
represents a significant advancement in this area and related fields.



