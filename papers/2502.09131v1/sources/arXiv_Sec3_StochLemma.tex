\section{Forward Uncertainty Propagation Without Disturbance Data} \label{sec:MainResults}
As the realizations of process disturbances are in general difficult to obtain, we aim for a variant of Lemma~\ref{lem:StochFundam} that does not explicitly rely on past disturbance data. To this end, we will exploit the superposition principle and causality requirements of stochastic systems.

\subsection{VARX model}
To avoid the use of state measurements, we switch to the Vector AutoRegressive with eXogenous input (VARX) model structure
\[
	Y_{k} = \hat{A}\tra{Y}{k-\tini}{k-1} + \hat{B}\tra{U}{k-\tini}{k-1}
    + \hat{E}\tra{W}{k-\tini}{k-1}.
\]
\citet{sadamoto23equivalence} shows the equivalence between the above VARX model and the state-space model~\eqref{eq:Dyn} with an observability assumption. Moreover, \citet{sadamoto23equivalence} explicitly derives the relation between matrices $A$, $B$, $E$ of \eqref{eq:Dyn} and $\hat{A}$, $\hat{B}$, $\hat{E}$ of \eqref{eq:VARX} as
\begin{align*}
	\hat{A} &= CA^{\tini}\mcl{O}^{\dagger},\\
	\hat{B} &= C\left(\mcl{C}(B)-A^{\tini}\mcl{O}^{\dagger}\mcl{M}(B) \right),\\
	\hat{E} &= C\left(\mcl{C}(E)-A^{\tini}\mcl{O}^{\dagger}\mcl{M}(E) \right),
\end{align*}
where $\cdot^\dagger$ denotes the Moore-Penrose inverse. The controllability matrix $\mcl{C}$ and the matrix $\mcl{M}$ are defined as $\mcl{C}(D) \coloneqq \begin{bmatrix} A^{\tini-1}D & \cdots & AD & D\end{bmatrix}$ and $\mcl{M}(D) \coloneqq \begin{bmatrix} 0 \\ CD & \ddots \\ \vdots & \ddots & &\ddots \\ CA^{\tini-2}D & \cdots & &CD & 0 \end{bmatrix}$ for $D\in\{B,E\}$.
Note that the mapping between matrices $A$, $B$, $E$ and $\hat{A}$, $\hat{B}$, $\hat{E}$ is not unique, e.g. \citet{phan96relationship} provide another possibility. The equivalence between the state-space model and the VARX model suggests that Lemma~\ref{lem:StochFundam} and Corollary~\ref{coro:StochFundamPCE} also hold for the VARX model \citep{pan24data}. Henceforth, we consider a simplified case of the above VARX model for which $\hat{E} = \begin{bmatrix}
	0_{n_y\times (\tini-1)n_y} & I_{n_y}
\end{bmatrix}$ holds.
\begin{assum}[VARX model] \label{ass:VARX}
	The state-space model~\eqref{eq:Dyn} is equivalent to
	\begin{subequations} \label{eq:VARX}
		\begin{align}
			&Y_k = \hat{A}\tra{Y}{k-\tini}{k-1} + \hat{B}\tra{U}{k-\tini}{k-1} + W_{k-1},\\
			&\tra{(U,Y)}{1-\tini}{0} = \tra{(\tilde{U},\tilde{Y})}{1-\tini}{0},
		\end{align}
	\end{subequations}
	where $\tra{(\tilde{U},\tilde{Y})}{1-\tini}{0}$ is an initial input-output trajectory of length $\tini$.
\end{assum}
\begin{rem} \label{rem:VARX}
	Besides $\tra{(U,Y)}{k-\tini}{k-1}$, the current output $Y_k$ only directly depends on the last disturbance $W_{k-1}$ in~\eqref{eq:VARX}. This assumption is in general difficult to verify as the state-space model and the VARX model are both unknown.
    However, for the systems where only the input and output measurements are available, one can use the i.i.d. disturbances $W_{k-1}$, $k\in\mbb{N}$ in \eqref{eq:VARX} to represent all the unknown disturbances affecting the system. Then, from the measured data one can obtain the estimation of the distribution of $W$ \citep{pan23stochastic, turan22data}, e.g. the least-square estimator
    \begin{equation} \label{eq:Estimator}
    	\begin{split}
	    	\Hankel_1(\trad{\hat{w}}{0}{T-1}) = \Hankel_1(\trad{y}{1}{T})\Bigg(I-&\\
    		\begin{bmatrix} \Hankel_1(\trad{y}{0}{T-1}) \\ \Hankel_1(\trad{u}{0}{T-1}) \end{bmatrix}^\dagger
    		&\begin{bmatrix} \Hankel_1(\trad{x}{0}{T-1}) \\ \Hankel_1(\trad{u}{0}{T-1}) \end{bmatrix}\Bigg).
    	\end{split}
    \end{equation}
\end{rem}

\begin{exmp}
	We illustrate the equivalence between the state-space model~\eqref{eq:Dyn} and the VARX model~\eqref{eq:VARX} via a simple example. Consider the 2-dimensional system
	\begin{align*}
		X_{k+1} &= \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} X_k+ \begin{bmatrix} 0 \\ 1\end{bmatrix} U_k+ \begin{bmatrix} 1\\1\end{bmatrix}W_k\\
		Y_k & = \begin{bmatrix} 1 & 0\end{bmatrix}X_k.
	\end{align*}
	with $\tini=2$. Based on the approach given above, we obtain the corresponding VARX model
	\begin{multline*}
		Y_k = \begin{bmatrix} 3 & 2 \end{bmatrix} \tra{Y}{k-2}{k-1} + \begin{bmatrix} 1 & 0 \end{bmatrix} \tra{U}{k-2}{k-1} \\+ \begin{bmatrix} 0 & 1 \end{bmatrix} \tra{W}{k-2}{k-1},
	\end{multline*}
	which satisfies Assumption~\ref{ass:VARX}.
\end{exmp}

Similar to~\eqref{eq:DynPCE}, we obtain the corresponding dynamics of realizations
\begin{subequations} \label{eq:VARXReal}
	\begin{align}
		&y_k = \hat{A}\tra{y}{k-\tini}{k-1} + \hat{B}\tra{u}{k-\tini}{k-1} + w_{k-1},\\
		&\tra{(u,y)}{1-\tini}{0} = \tra{(\tilde{u},\tilde{y})}{1-\tini}{0},
	\end{align}
\end{subequations}
and the dynamics of PCE coefficients for~\eqref{eq:VARX} for all $j\in\I_{[0,L-1]}$, $k\in\I_{[1,N]}$
\begin{subequations} \label{eq:VARXPCE}
	\begin{align}
		&\pce{y}_k^j = \hat{A}\tra{\pce{y}^j}{k-\tini}{k-1} + \hat{B}\tra{\pce{u}^j}{k-\tini}{k-1} + \pce{w}^j_{k-1},\\
		&\tra{(\pce{u},\pce{y})^j}{1-\tini}{0} = \tra{(\tilde{\pce{u}},\tilde{\pce{y}})^j}{1-\tini}{0}.
	\end{align}
\end{subequations}
Especially, given a deterministic initial trajectory $\tra{(\tilde{u},\tilde{y})}{1-\tini}{0}$, we get $\mean[\tra{(\tilde{u},\tilde{y})}{1-\tini}{0}]=\tra{(\tilde{u},\tilde{y})}{1-\tini}{0}$ and thus its PCE representation reads
\begin{align*}
	\tra{(\tilde{\pce{u}},\tilde{\pce{y}})^j}{1-\tini}{0} = \begin{cases}
		\tra{(\tilde{u},\tilde{y})}{1-\tini}{0},\quad &\text{for } j=0\\
		0,&\text{otherwise}
	\end{cases}.
\end{align*}
We also note that the dynamics of realizations and of PCE coefficients, i.e.~\eqref{eq:VARXReal} and~\eqref{eq:VARXPCE}, share the same system matrices. Therefore, the Hankel matrices consisting of realizations and PCE coefficients span the same column space provided persistency of excitation holds. This way, one can use the Hankel matrices in realizations to predict the future trajectories of PCE coefficients, see the column-space equivalence given by Lemma~3 by~\citet{pan23stochastic} for details.

\subsection{System decomposition}
For the sake of simplified notation, we consider deterministic initial input-output condition $\tra{(U,Y)}{1-\tini}{0}=\tra{(\tilde{u},\tilde{y})}{1-\tini}{0}$. We discuss the generalization to an uncertain initial condition in Remark~\ref{rem:UncertainIni}.
	
According to the superposition principle, one can decouple the stochastic LTI system~\eqref{eq:VARX} into a nominal deterministic subsystem, which corresponds to the dynamics of the expected value, and other stochastic error systems, each of which is related to one disturbance $W_i$, $i\in\N$. That is, given the decomposition
\begin{subequations} \label{eq:Superposition}
	\begin{equation}
		Z_k = \mean[Z_k] + \sum_{i=0}^{k-1} Z_k^{w_i},\quad Z\in\{U,Y\},
	\end{equation}
the dynamics of the nominal deterministic system are
	\begin{equation} \label{eq:SubNom}
		\begin{split}
			&\mean[Y_k] = \hat{A}\mean[\tra{Y}{k-\tini}{k-1}] + \hat{B}\mean[\tra{U}{k-\tini}{k-1}] +\mean[W],\\
			&\mean[\tra{(U,Y)}{1-\tini}{0}] = \tra{(\tilde{u},\tilde{y})}{1-\tini}{0}.
		\end{split}
	\end{equation}
The stochastic error system for each $W_i$, $i\in\N$ reads
\begin{equation} \label{eq:SubError}
	\begin{split}
		&Y^{w_i}_k = \hat{A}\tra{Y^{w_i}}{k-\tini}{k-1} + \hat{B}\tra{U^{w_i}}{k-\tini}{k-1},\\
		&\tra{(U,Y)^{w_i}}{1-\tini}{i} = 0,\quad Y^{w_i}_{i+1} = W_i-\mean[W]
	\end{split}
\end{equation}
\end{subequations}
with $\mean[Y_k^{w_i}]=\mean[U_k^{w_i}]=0$, $k\in\N$. Note that due to the inherent causality requirement, i.e., the input $U_k$ and output $Y_k$ may only depend on past disturbances $W_s$, $s<k$, we have $\tra{(U,Y)^{w_i}}{1-\tini}{i} = 0$, $i\in\N$. Thus,
\[
	Y^{w_i}_{i+1} = \hat{A}\cdot 0+ \hat{B}\cdot 0 + W_i-\mean[W]=W_i-\mean[W].
\]
Splitting the LTI system~\eqref{eq:VARX} into decoupled subsystems~\eqref{eq:Superposition}, the additive disturbance~$W_i$, $i\in\N$ is converted into the initial output condition $Y_{i+1}^{w_i}=W_i-\mean[W]$. That is, the subsystems~\eqref{eq:Superposition} are disturbance free except for $\mean[W]$ in~\eqref{eq:SubNom}. Therefore, Lemma~\ref{lem:StochFundam} and Corollary~\ref{coro:StochFundamPCE} can be applied to predict the future trajectories of~\eqref{eq:Superposition} without the need for Hankel matrices involving disturbance data.

\subsection{PCE basis structure} \label{sec:Structure}
To obtain a PCE reformulation of subsystems~\eqref{eq:Superposition} for further analysis, one needs to construct a joint PCE basis applicable to all inputs and outputs.

Let  $\Psi^{w}_k\coloneqq \{ \psi^n(\xi_k)\}_{n=0}^{L_w-1}$, $L_w\in\N^\infty$ be the finite-dimensional basis for $W_k$, $k\in\I_{[0,N-1]}$. Then the i.i.d. disturbances $W_{k}$, $k\in \I_{[0,N-1]}$ admit exact PCEs 
\begin{equation} \label{eq:WkPCE}
	W_k = \sum_{n=0}^{L_w-1} \pce{w}^n \psi^n(\xi_k).
\end{equation}
As the disturbances are identically distributed, the PCEs of $W_k$, $k\in\I_{[0,N-1]}$ have the same algebraic structure of the basis functions $\psi^n$ and coefficients $w^n$, $n\in\I_{[0,L_w-1]}$. The independence of $W_k$, $k\in\I_{[0,N-1]}$ is modelled by the use of different stochastic germs $\xi_k$.
We construct the joint disturbance basis $\Phi\coloneqq \cup_{k=0}^{N-1} \Psi^{w}_k$ as
\begin{equation}\label{eq:BasisElement}
	\begin{split} 
		\Phi = &\Big\{ 1, \underbrace{\psi^1(\xi_0),...,\psi^{L_w-1}(\xi_0)}_{\Phi^{w_0}\setminus \{\psi^0(\xi_0)\}},
		\\ &\qquad...,\underbrace{\psi^1(\xi_{N-1}),...,\psi^{L_w-1}(\xi_{N-1})}_{\Phi^{w_{N-1}}\setminus \{\psi^0(\xi_{N-1})\}}\Big\},
	\end{split}
\end{equation}
which contains a total of $L=1+N(L_w-1)$ terms. We also enumerate the joint basis $\Phi$ from 0 to $L-1$ according to the sequence given above.

Recall that the PCEs of i.i.d. $W_k$, $k\in\I_{[0,N-1]}$ in the joint basis~$\Phi$ be $W_k=\sum_{j=0}^{L-1}\pce{w}_k^j\phi^j$. From the structure of~\eqref{eq:BasisElement} we observe that the basis functions related to the disturbance $W_k$, $k\in\I_{[0,N-1]}$ are $\phi^j$, $j\in \{0\}\cup\ik{k}$ with
\[
	\ik{k}\coloneqq \I_{[1+k(L_w-1),(k+1)(L_w-1)]}.
\] 
Conversely, given the PCE dimension $j$, using
\begin{equation*} \label{eq:k}
	k^\prime(j) \coloneqq \begin{cases}
		0,~&\text{for } j=0,\\
		k \text{ such that } j\in\ik{k}, \quad &\text{for } j\in\I_{[1,L-1]}
	\end{cases},
\end{equation*}
we see that the PCE coefficients $(\pce{u},\pce{x},\pce{y})^j$, $j\in\I_{[1,L-1]}$  relate to the disturbance $W_{k^\prime(j)}$. Note that $(\pce{u},\pce{x},\pce{y})^0$ is the expected value and thus it is linked to system~\eqref{eq:SubNom}.

With the index set $\ik{k}$,  $k\in\I_{[0,N-1]}$, we can identify the PCE coefficients corresponding to the disturbance $W_k$ in the joint basis $\Phi$
\begin{equation*}
	\pce{w}_k^j = \begin{cases}
		\pce{w}^0=\mean[W],~&\text{for } j=0\\
		\pce{w}^{j-k(L_w-1)},~&\text{for } j\in\ik{k}\\
		0,&\text{otherwise}
	\end{cases},
\end{equation*}
where $\pce{w}^{n}$, $n\in\I_{[0,L_w-1]}$ are defined in~\eqref{eq:WkPCE}. Moreover, in~Proposition~1 of \citet{pan23stochastic}, we have shown that all $Z_k$, $Z\in\{U,X,Y\}$ of~\eqref{eq:VARX} admit exact PCEs in this joint basis $\Phi$ for all $k \in \I_{[0,N]}$ over the entire horizon $N$. Therein the basis $\Phi$ entails the same basis directions but is indexed differently.

\subsection{Causality in PCE}
Given the joint basis $\Phi$, the PCEs of $\mean[Z]$ and $Z^{w_i}$, $Z\in\{(U,Y)\}$ with non-zero coefficients read
\begin{equation} \label{eq:RVDecompositionPCE}
	\mean[Z_k] = \pce{z}_k^0\phi^0,\quad Z_k^{w_i} = \sum_{j\in\ik{i}}\pce{z}_k^j\phi^j.
\end{equation}
Expressing the causality conditions $\tra{(U,Y)^{w_i}}{1-\tini}{i} = 0$ and $Y^{w_i}_{i+1} = W_i-\mean[W]$ in the PCE framework, we have
\begin{equation} \label{eq:Causality}
	\tra{(\pce{u},\pce{y})^j}{1-\tini}{k^{\prime}(j)}=0,~ \pce{y}_{k^{\prime}(j)+1}^j = \pce{w}^{I(j)}
\end{equation}
for all $j\in\I_{[1,L-1]}$, where $I(j)\coloneqq j-k^{\prime}(j)(L_w-1)$ \citep{ou25polynomial}. This way, the PCE coefficients of disturbances in~\eqref{eq:DynPCE} become the initial value $\pce{y}_{k^{\prime}(j)+1}^j = \pce{w}^{I(j)}$. It is straightforward to rewrite the dynamics of the PCE coefficients for $j=0$,
\begin{subequations} \label{eq:SuperpositionPCE}
	\begin{equation}\label{eq:VARXSimExp}
		\begin{split}
			&\pce{y}_k^0 = \hat{A}\tra{\pce{y}^0}{k-\tini}{k-1} + \hat{B}\tra{\pce{u}^0}{k-\tini}{k-1}+\mean[W],\\
			&\tra{(\pce{u},\pce{y})^0}{1-\tini}{0} = \tra{(\tilde{u},\tilde{y})}{1-\tini}{0},
		\end{split}
	\end{equation}
which corresponds to the nominal deterministic system~\eqref{eq:SubNom}. For $j\in\ik{i}$, $i\in\I_{[0,N-1]}$, we obtain
	\begin{equation} \label{eq:VARXSimDist}
			\pce{y}_k^j = \hat{A}\tra{\pce{y}^j}{k-\tini}{k-1} {+}\hat{B}\tra{\pce{u}^j}{k-\tini}{k-1}~\text{with}~\eqref{eq:Causality},
	\end{equation}
\end{subequations}
which corresponds to the stochastic error system~\eqref{eq:SubError} for $W_i$.
Thus, one can apply the fundamental lemma, cf. Theorem~1 by \citet{willems05note}, to predict the future trajectory~$\tra{(\pce{u},\pce{y})}{1}{N}$ for all $j\in\I_{[0,L-1]}$.

\subsection{Propagation without past disturbance data}
We move on showing how one can forward propagate the PCE coefficients of LTI system~\eqref{eq:VARXPCE}, in a data-driven fashion without past disturbance data. The crucial observation is that the reformulated dynamics~\eqref{eq:SuperpositionPCE} are deterministic systems---disturbed by the constant $\mean[W]$ in case of \eqref{eq:VARXSimExp} and undisturbed in case of \eqref{eq:VARXSimDist}.
Hence, we require a recording of an input-output trajectory $\traud{(u,y)}{0}{T-1}$ of the undisturbed system
\begin{subequations} \label{eq:VARXFree}
	\begin{align}
		&y_k^{\ud} = \hat{A}\traud{y}{k-\tini}{k-1} + \hat{B}\traud{u}{k-\tini}{k-1}, \label{eq:VARXDyn}\\
		&\traud{(u,y)}{1-\tini}{0} = \traud{(\tilde{u},\tilde{y})}{1-\tini}{0} \label{eq:VARXFreeIni}
	\end{align}
\end{subequations}
in the offline phase, where the superscript $\cdot^{\ud}$ denotes the undisturbed system and data. It is worth mentioning that the input ${u}^{\ud}$ of~\eqref{eq:VARXFree} is indeed identical to that of the disturbed system~\eqref{eq:VARXReal}. The superscript $\cdot^{\ud}$ of the input is used to maintain consistent notation with the undisturbed output $y^{\ud}$.

To obtain the data of~\eqref{eq:VARXFree}, we record the data of system~\eqref{eq:VARX} when it is temporarily undisturbed, i.e. $w=0$. However, the disturbance realizations in the past still affect the current and the future outputs. Consider disturbance $w_{-\tini}$ that acts on system~\eqref{eq:VARXFree} at time step $k=-\tini$. Then we decompose the output as $y^{\ud} = \bar{y} + y^w$ with
\begin{align*} 
		&\bar{y}_k = \hat{A}\tra{\bar{y}}{k-\tini}{k-1} + \hat{B}\traud{u}{k-\tini}{k-1},\\
		&\tra{\bar{y}}{1-\tini}{0} = \tra{(\tilde{y}-y^w)}{1-\tini}{0},\\
		&y^w_k = \hat{A}\tra{y^w}{k-\tini}{k-1} + \hat{B}\cdot 0,\\
		& y^w_{1-\tini} = w_{-\tini},~y^w_{i} = 0, \forall i \leq -\tini,
\end{align*}
where $\bar{y}$ denotes the output unaffected by $w_{-\tini}$, and $y^w$ denotes the affect of $w_{-\tini}$ in the output. Constructing the Hankel matrices from $(u^{\ud},\bar{y})_{[1,T]}$ and $\traud{(u,y)}{1}{T}$, we have
\[
	\mathrm{colsp}\left( \begin{bmatrix} \Hankel_{\tini}(\traud{u}{1}{T})\\ \Hankel_{\tini}(\tra{\bar{y}}{1}{T}) \end{bmatrix}\right)\subset \mathrm{colsp}\left( \begin{bmatrix} \Hankel_{\tini}(\traud{u}{1}{T})\\ \Hankel_{\tini}(\traud{y}{1}{T}) \end{bmatrix}\right)
\]
since $y^{\ud}=\bar{y}$ for the case $y^w=0$.
In other words, the disturbances before the time step $k=1-\tini$, e.g. $w_{-\tini}$, enrich the column space of Hankel matrices. Therefore, given an initial condition $\tra{(\tilde{u},\tilde{y})}{1-\tini}{0}$ of disturbed system~\eqref{eq:VARX}, there exists $g\in\R^{T-\tini+1}$ such that
\[
	\begin{bmatrix}
		\Hankel_{\tini}(\traud{u}{1}{T-\tini})\\
		\Hankel_{\tini}(\traud{y}{1}{T-\tini})
	\end{bmatrix} g = 
	\begin{bmatrix}
		\tra{\tilde{u}}{1-\tini}{0}\\
		\tra{\tilde{y}}{1-\tini}{0} 
	\end{bmatrix},
\]
while such such a vector $g$ does not exist if the data $\tra{(u^{\ud},\bar{y})}{1}{T}$ are used to construct the Hankel matrices.
\begin{rem}[Obtaining undisturbed system data]
	There are different approaches to obtain the data of system~\eqref{eq:VARXFree}: 
		\begin{enumerate}[label=(\roman*)]
			\item Measure the input-output data of system~\eqref{eq:VARX} when there is temporarily no disturbance, e.g., a power system can be considered undisturbed when there are only very limited users at midnight.
			\item\label{itm:measurement} Deploy additional high-fidelity sensors to measure the realizations of disturbances of system~\eqref{eq:VARX} in the offline phase, e.g. environment temperature for building climate control and user demands in power systems. Then, based on the collected data, one can compute the undisturbed trajectory of~\eqref{eq:VARXFree}, cf.~Section~\ref{sec:Estimation}. This assumption on the availability of additional data, e.g. disturbance or state measurements, appears frequently in recent literature \citep{wolff24robust,disaro24equivalence}.
			\item Collect the input-output data of the disturbed system~\eqref{eq:VARX} and estimate the corresponding disturbance realizations, see Remark~\ref{rem:VARX}. Then, one can compute the undisturbed trajectory of~\eqref{eq:VARXFree} as in Approach~\ref{itm:measurement}.
		\end{enumerate}
\end{rem}
Using the data of~\eqref{eq:VARXFree} to construct Hankel matrices, one may wonder whether the predicted trajectory $\tra{(u,y)}{1}{N}$ is guaranteed to satisfy the  dynamics~\eqref{eq:VARXDyn} since the system is only undisturbed during the data collection. Intuitively, the answer is positive as disturbances only appear before the data collection phase and thus the dynamics~\eqref{eq:VARXDyn} still hold for $\traud{(u,y)}{1}{T}$.

\begin{assum}[Persistently exciting data] \label{ass:PE}
	We assume that the collected data $\traud{(u,y)}{1}{T}$ satisfy
	\[
		\rank\left( \begin{bmatrix*}[l]
			\Hankel_{\tini+N}(\traud{u}{1}{T}) \\ \Hankel_{\tini}(\traud{y}{1}{T-N+1})
		\end{bmatrix*}\right) = (\tini+N)n_u + \tini n_y,
	\]
	i.e., the collected data $\traud{(u,y)}{1}{T}$ are persistently exciting.
\end{assum}
The next result shows how to capture the undisturbed realization dynamics~\eqref{eq:VARXDyn}.
\begin{lem} \label{lem:Prediction}
	 Let Assumption~\ref{ass:Sys}, \ref{ass:VARX}, and \ref{ass:PE} hold. Then a sequence $\tra{(u,y)}{1-\tini}{N}$ satisfy the dynamics~\eqref{eq:DynReal} if and only if there exists $g\in\R^{T-N-\tini+1}$ such that
\begin{equation} \label{eq:FundaLemma}
	\left[\begin{array}{ll} \Hankel_{\tini+N}(\traud{u}{1}{T})\\ \Hankel_{\tini+N}(\traud{y}{1}{T}) \end{array}\right] g = 
	\left[\begin{array}{ll} \tra{u}{1-\tini}{N} \\ \tra{y}{1-\tini}{N} \end{array}\right].
\end{equation}
\end{lem}
\begin{proof}
	The proof follows along the same lines as the extension of the fundamental lemma towards affine systems, cf. Theorem~1 by \citet{berberich22linear}. First we show the sufficiency of~\eqref{eq:FundaLemma}. It follows from the VARX model~\eqref{eq:VARX} that, for $i\in\N$,
	\begin{equation}\label{eq:VARXN}
		\traud{y}{\tini+i}{\tini+N+i-1} = \hat{A}_{\ini} \traud{y}{i}{i+\tini-1} + \hat{B}_N\traud{u}{i}{i+\tini+N-2}
	\end{equation}
	holds for the offline collected data, where the structure of matrices $\hat{A}_{\ini}$ and $\hat{B}_N$ is omitted for the brevity. Then, it follows
	\begin{align*}
		&\tra{y}{1}{N} \overset{\eqref{eq:FundaLemma}}{=} \sum_{i=1}^{T-N-\tini+1} \traud{y}{\tini+i}{\tini+N+i-1}g^i\\
		\overset{\eqref{eq:VARXN}}{=} &\sum_{i=1}^{T-N-\tini+1}  ( \hat{A}_{\ini} \traud{y}{i}{i+\tini-1} + \hat{B}_N\traud{u}{i}{i+\tini+N-2})g^i\\
		\overset{\eqref{eq:FundaLemma}}{=} &\hat{A}_{\ini}\tra{y}{1-\tini}{0} + \hat{B}_N\tra{u}{1-\tini}{N-1},
	\end{align*}
	where $g^i$ denotes the $i$-th element of $g$. Therefore,  $\tra{(u,y)}{1-\tini}{N}$ satisfies the VARX model~\eqref{eq:VARX}.
	
	Then we prove the necessity of~\eqref{eq:FundaLemma}. Assumption~\ref{ass:PE} implies that for any $\tra{u}{1-\tini}{N}$ and $\tra{y}{1-\tini}{0}$, there exists $g\in\R^{T-N-\tini+1}$ such that
	\begin{equation}\label{eq:PredYN}
		\begin{bmatrix*}[l]
			\Hankel_{\tini+N}(\traud{u}{1}{T}) \\ \Hankel_{\tini}(\traud{y}{1}{T-N+1})
		\end{bmatrix*}g = \begin{bmatrix*}[l]
			\tra{u}{1-\tini}{N} \\ \tra{y}{1-\tini}{0}
		\end{bmatrix*}.
	\end{equation}
	The VARX model~\eqref{eq:VARX} implies that
	\begin{align*}
		&\tra{y}{1}{N} = \hat{A}_{\ini}\tra{y}{1-\tini}{0} + \hat{B}_N\tra{u}{1-\tini}{N-1}\\
		=& \sum_{i=1}^{T-N-\tini+1} \left(\hat{A}_{\ini} \traud{y}{i}{i+\tini-1} + \hat{B}_N \traud{u}{i}{i+\tini+N-2}\right)g^i\\
		\overset{\eqref{eq:VARXN}}{=}& \sum_{i=1}^{T-N-\tini+1} \traud{y}{\tini+i}{\tini+N+i-1}g^i=\Hankel_{N}(\traud{y}{\tini+1}{T})g.
	\end{align*}
	Hence the assertion follows.
\end{proof}
As Assumption~\ref{ass:PE} holds, there always exists $g$ such that~\eqref{eq:PredYN} holds for an arbitrary initial trajectory $\tra{(u,y)}{1-\tini}{0}$. That is, one can use Lemma~\ref{lem:Prediction} to compute the response $\tra{(u,y)}{1}{N}$ of dynamics~\eqref{eq:VARXDyn} even for a disturbed initial trajectory.
We arrive at the following main results, which are provided in Lemma~\ref{lem:j0}, \ref{lem:jOther}, and \ref{lem:PropRV} below.

\begin{lem}[Propagation in expectation ($j=0$)] \label{lem:j0}
	Consider system~\eqref{eq:Dyn} and a trajectory $\traud{(u,y)}{1}{T}$ of~\eqref{eq:VARXFree}. Let Assumptions~\ref{ass:Sys}, \ref{ass:VARX}, and \ref{ass:PE} hold. Then given a measured initial condition $\tra{(\tilde{u},\tilde{y})}{1-\tini}{0}$,
	$\tra{(\pce{u},\pce{y})^0}{1}{N}$ is a trajectory of~\eqref{eq:DynPCE} for $j=0$ if and only if there exists $\pce{g}^0\in\R^{T-N-\tini+1}$ such that
	\begin{subequations} \label{eq:UYj0}
	\begin{equation} \label{eq:TrajUYr}
		\left[\begin{array}{ll} \Hankel_{\tini}(\traud{u}{1}{T-\tini})\\ \Hankel_{\tini}(\traud{y}{1}{T-\tini}) \\ \midrule \Hankel_{N}(\traud{u}{\tini+1}{T}) \\ \Hankel_{N}(\traud{y}{\tini+1}{T})  \end{array}\right] \pce{g}^0 = 
		\left[\begin{array}{ll} \tra{\tilde{u}}{1-\tini}{0} \\ \tra{\tilde{y}}{1-\tini}{0} \\ \midrule \tra{\pce{u}^0}{1}{N} \\ \tra{y^u}{1}{N} \end{array}\right]
	\end{equation}
	holds, where $ y_k^u = \pce{y}_k^0 - \sum_{i=1}^{k}y_i^{w}$, $y_1^w=\mean[W]$ and the system output $\tra{y^w}{2}{N}$ is given by
	\begin{multline} \label{eq:yw}
		\tra{y^w}{2}{N} = \Hankel_{N-1}(\traud{y}{\tini+1}{T})\cdot\\
		\begin{bmatrix*}[l] \Hankel_{\tini+N-1}(\traud{u}{1}{T}) \\ \Hankel_{\tini-1}(\traud{y}{1}{T-N})\\ \Hankel_{1}(\traud{y}{\tini}{T-N+1}) \end{bmatrix*}^\dagger \begin{bmatrix*}[l]  0_{(\tini+N-1)\dimu\times1} \\ 0_{(\tini-1)\dimy\times1} \\ \mean[W] \end{bmatrix*}.
	\end{multline}
	\end{subequations}
\end{lem}
\begin{proof}
    The core idea of the proof is to decompose $\pce{y}_k^0$ into two parts $\pce{y}_k^0=y_k^u + \sum_{i=1}^{k}y_i^{w}$, where $y^u$ related to the input $\pce{u}^0$ and $y^w$ related to the disturbances.
	
	For $j=0$ it holds that $\pce{w}_k^0=\mean[W]$, $k\in\I_{[0,N-1]}$. Then we split system~\eqref{eq:VARXPCE} into a disturbance free system
	\begin{subequations}
		\begin{align}
			&y_k^u = \hat{A}\tra{y^u}{k-\tini}{k-1} + \hat{B}\tra{\pce{u}^0}{k-\tini}{k-1}, \label{eq:DynR}\\
			&\tra{(y^u,\pce{u}^0)}{1-\tini}{0} = \tra{(\tilde{y},\tilde{u})}{1-\tini}{0},
		\end{align}
	and an error system
		\begin{align}
			&y_k^{\tilde{w}} = \hat{A}\tra{y^{\tilde{w}}}{k-\tini}{k-1} + \mean[W],  \label{eq:DynE}\\
			&\tra{y^{\tilde{w}}}{1-\tini}{0} = 0.
		\end{align}
	\end{subequations}
	Moreover, \eqref{eq:DynE}  can be rewritten as $y^{\tilde{w}}_k = \sum_{i=1}^{k}y^w_i$, $k\in\I_{[1,N]}$ with the autonomous system
	\begin{subequations}
			\begin{align}
					&y^w_k = \hat{A}\tra{y^w}{k-\tini}{k-1}+\hat{B}\cdot0,\label{eq:DynTE}\\
					&\tra{y^w}{1-\tini}{0} = 0,\quad y_1^w =\mean[W].
				\end{align}
		\end{subequations}
	We notice that $y^w_1 \neq \hat{A}\tra{y^w}{1-\tini}{0}=0$. Therefore, we choose $\tra{y^w}{2-\tini}{1}=[0_{1\times(\tini-1)\dimy},\mean[W]^\top]^\top$ to be the initial condition, which is indeed not a trajectory of~\eqref{eq:DynTE}. However, applying Lemma~\ref{lem:Prediction} we can compute $\tra{y^w}{1}{N}$. That is, $\tra{y^w}{1}{N}$ satisfies \eqref{eq:DynTE} if and only if there exist a $g^w\in\R^{T-N-\tini+2}$ such that  
	\[
	 \left[\begin{array}{ll} \Hankel_{\tini+N-1}(\traud{u}{1}{T}) \\ \midrule \Hankel_{\tini-1}(\traud{y}{1}{T-N})\\ \Hankel_{1}(\traud{y}{\tini}{T-N+1}) \\ \Hankel_{N-1}(\traud{y}{\tini+1}{T}) \end{array}\right] g^w =
	 \left[\begin{array}{ll}  0_{(\tini+N-1)\dimu\times1} \\ \midrule 0_{(\tini-1)\dimy\times1} \\ \mean[W] \\ \tra{y^w}{2}{N} \end{array}\right].
	\]
	From the above equation we can explicitly compute the trajectory $\tra{y^w}{2}{N}$ as~\eqref{eq:yw}.
	Again, applying Lemma~\ref{lem:Prediction} to dynamics~\eqref{eq:DynR}, we conclude \eqref{eq:TrajUYr}. From $\pce{y}_k^0=y_k^u+y_k^{\tilde{w}}=y_k^u+\sum_{i=1}^{k}y_i^w$, we have the predicted trajectory $\tra{(\pce{u},\pce{y})^0}{1}{N}$.
\end{proof}
Notice that since the distribution of $W$ and hence $\mean[W]$ are known, one can compute the forward propagation of $\mean[W]$, i.e. the trajectory $\tra{y^w}{1}{N}$, in advance.

\begin{table*}[t!]
	\tiny
	\caption{Comparison of different fundamental lemmas for LTI systems.}
	\label{tab:ComparisonLemmas}
	\centering
	\begin{adjustbox}{width=1\linewidth,center}
		\begin{tabular}{llcc}
			\toprule
			Lemma  & \hspace{30pt} System  & Data & Non-zero entries in $\Hankel$\\
			\midrule
			\citet{willems05note}  & $\arraycolsep=1.4pt \begin{array}{rcl} x_{k+1} &= & Ax_k+Bu_k,\\ y_k &=& Cx_k+Du_k\end{array}$ & $(u,y)^{\da}$ & $(\tini+N)(n_u+n_y)n_g$\\
			&&&\\
			\citet{berberich22linear} & $\arraycolsep=1.4pt\begin{array}{rcl} x_{k+1} &= & Ax_k+Bu_k+e,\\ y_k &=& Cx_k+Du_k+r\end{array}$ & $(u,y)^{\da}$ & $(\tini+N)(n_u+n_y)n_g$ \\
			&&&\\
			\citet{kerz23data}     & $\arraycolsep=1.4pt\begin{array}{rcl} X_{k+1}&=&AX_k+BU_k+EW_k \\ \hat{X}_k &=&X_k+\mu_k \end{array} $ & $(u,x,w)^{\da}$ & $N(n_u+n_y+n_w)n_g$\\
			&&&\\
			\begin{tabular}{@{}l@{}} \citet{pan23stochastic} \\ (Lemma~\ref{lem:StochFundam})\\ \end{tabular} &  $\arraycolsep=1.4pt\begin{array}{rcl} X_{k+1} &= & AX_k+BU_k+EW_k,\\ Y_k &=& CX_k+DU_k\end{array}$ & $(u,y,w)^{\da}$ & $(\tini+N)(n_u+n_y+n_w)n_gL$ \\
			&&&\\
			Lemma~\ref{lem:PropRV} &
			$\arraycolsep=1.4pt\hspace{6pt}\begin{array}{rcl} Y_k & = & \hat{A}\tra{Y}{k-\tini}{k-1} \\ & &+\hat{B}\tra{U}{k-\tini}{k-1} + W_{k-1}\end{array}$ 
			& $(u,y)^{\ud}$ & $\approx(\tini+\frac{N}{2})(n_u+n_y)n_gL$ \\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
\end{table*}

\begin{lem}[Propagation for $j\in\I_{[1,L-1]}$]\label{lem:jOther}
	Let the conditions of Lemma~\ref{lem:j0} hold.
	Then, for all $j \in \I_{[1,L-1]}$, $\tra{(\pce{u},\pce{y})^j}{1}{N}$ is a trajectory of \eqref{eq:DynPCE} if and only if there exist $\pce{g}^j \in \R^{T-N-\tini+1}$ such that
	\begin{subequations}\label{eq:UYjRest}
		\begin{gather}
			\tra{(\pce{u},\pce{y})^j}{1}{k^{\prime}(j)}=0,\quad \pce{y}_{k^\prime(j)+1}^j = \pce{w}^{I(j)}, \label{eq:UY0}\\
			\left[\begin{array}{ll} \Hankel_{\tini-1}(\traud{u}{1}{T-N-1})\\ \Hankel_{\tini-1}(\traud{y}{1}{T-N-1}) \\ \midrule \Hankel_{\bar{N}}(\traud{u}{\tini}{T-k^{\prime}-1}) \\ \Hankel_{\bar{N}}(\traud{y}{\tini}{T-k^{\prime}-1}) \end{array}\right] \pce{g}^j 
			=\left[\begin{array}{ll} 0_{(\tini-1)\dimu\times1} \\ 0_{(\tini-1)\dimy\times1} \\ \midrule\tra{\pce{u}^j}{k^{\prime}(j)+1}{N} \\ \tra{\pce{y}^j}{k^{\prime}(j)+1}{N} \end{array}\right], \label{eq:PCEUYj}
		\end{gather}
	\end{subequations}
	where $\bar{N}=N-k^{\prime}(j)$.
\end{lem}
\begin{proof}
	The causality condition in the PCE framework, i.e. \eqref{eq:Causality}, implies \eqref{eq:UY0}.
	Similar to Lemma~\ref{lem:j0}, we notice that
	\[
		\pce{y}_{k^{\prime}(j)+1}^j\neq \hat{A}\tra{\pce{y}^j}{k^{\prime}(j)-\tini+1}{k^{\prime}} +  \hat{B}\tra{\pce{u}^j}{k^{\prime}(j)-\tini+1}{k^{\prime}(j)}
	\]
	with $\tra{(\pce{u},\pce{y})^j}{k^{\prime}(j)-\tini+1}{k^{\prime}(j)}=0$. Hence, the initial condition is given as $\tra{(\pce{u},\pce{y})^j}{k^{\prime}(j)-\tini+2}{k^{\prime}(j)}=0$ and $\pce{y}_{k^{\prime}(j)+1}^j=\pce{w}^{I(j)}$, while $\pce{u}_{k^{\prime}(j)+1}^j$ remains an input variable. Then applying Lemma~\ref{lem:Prediction}, \eqref{eq:PCEUYj} immediately follows.
\end{proof}
As a by-product, the prediction horizon of \eqref{eq:DynPCE} for $j\in\I_{[1,L-1]}$ is shortened from $N$ to $N-k^{\prime}(j)$. Consequently, we have smaller Hankel matrices and less PCE coefficients as decision variables when we apply Lemma~\ref{lem:jOther} to stochastic OCPs. Therefore, Lemma~\ref{lem:jOther} also accelerates the computation in numerical implementations, see the numerical example in Section~\ref{sec:Simulation}.
Summarizing Lemma~\ref{lem:j0}-\ref{lem:jOther}, we conclude the following lemma in random variables.

\begin{lem}[Propagation in random variables]\label{lem:PropRV}
	Let Assumptions~\ref{ass:Sys}, \ref{ass:VARX}, and \ref{ass:PE} hold.
	Then $\tra{(U,Y)}{1}{N}$ is a trajectory of \eqref{eq:Dyn} for a measured initial trajectory $\tra{(\tilde{u},\tilde{y})}{1-\tini}{0}$ if and only if there exist $g\in\R^{T-N-\tini+1}$ and $G^{w_i}\in\splx{T-N-\tini+1}$, $i\in\I_{[0,N-1]}$ such that $Z_k = \mean[Z] + \sum_{i=0}^{k-1} Z_k^{w_i}$, $Z\in\{U,Y\}$, where
	\begin{enumerate}[label=(\roman*)]
		\item\label{Cond1} $\mean[\tra{(U,Y)}{1}{N}]=\tra{(\pce{u},\pce{y})^0}{1}{N}$ and $g=\pce{g}^0$ in Lemma~\ref{lem:j0},
		\item\label{Cond2}  $\tra{(U,Y)^{w_i}}{1-\tini}{i} = 0$, $Y^{w_i}_{i+1} = W_i-\mean[W]$, and
		\[
		\left[\begin{array}{ll} \Hankel_{\tini-1}(\traud{u}{1}{T-N-1})\\ \Hankel_{\tini-1}(\traud{y}{1}{T-N-1}) \\ \midrule \Hankel_{N-i}(\traud{u}{\tini}{T-i-1}) \\ \Hankel_{N-i}(\traud{y}{\tini}{T-i-1}) \end{array}\right] G^{w_i} 
		=\left[\begin{array}{ll} 0_{(\tini-1)\dimu\times1} \\ 0_{(\tini-1)\dimy\times1} \\ \midrule \tra{U^{w_i}}{i+1}{N} \\ \tra{Y^{w_i}}{i+1}{N} \end{array}\right].
		\]
	\end{enumerate}
\end{lem}
\begin{proof}
	First we prove that the conditions~\ref{Cond1}-\ref{Cond2} are necessary. Consider any trajectory~$\tra{(U,Y)}{1}{N}$ of system~\eqref{eq:Dyn} and its decomposition~\eqref{eq:Superposition}. Replacing all the inputs and outputs with their PCEs as~\eqref{eq:RVDecompositionPCE}, we obtain the PCE reformulated dynamics~\eqref{eq:SuperpositionPCE}, which are equivalent to the data-driven representations~\eqref{eq:UYj0} and \eqref{eq:UYjRest} as Lemmas~\ref{lem:j0}-\ref{lem:jOther} have shown. Since $\phi^0=1$, the condition~\ref{Cond1} directly follows from \eqref{eq:UYj0}. Then for the PCE representation~\eqref{eq:UYjRest}, $j\in\ik{i}$, we multiply them with the corresponding PCE basis functions $\phi^j$, $j\in\ik{i}$ and sum the results over. Let $G^{w_i}\coloneqq \sum_{j\in\ik{i}}\pce{g}^j\phi^j$, we get the condition~\ref{Cond2}.
	
	Next we show the sufficiency of the conditions~\ref{Cond1}-\ref{Cond2}. The stochastic fundamental lemma, i.e. Lemma~\ref{lem:StochFundam}, indicates that the trajectories~$\tra{(\mean[U],\mean[Y])}{1}{N}$ and $\tra{(U,Y)^{w_i}}{1}{N}$ are trajectories of the decomposed systems~\eqref{eq:SubNom} and \eqref{eq:SubError}, respectively, when the conditions \ref{Cond1}-\ref{Cond2} hold. Thus, $(U,Y)_k = (\mean[U],\mean[Y])_k + \sum_{i=0}^{k-1} (U,Y)_k^{w_i}$, $k\in\I_{[1,N]}$ satisfy the dynamics~\eqref{eq:VARX} as well as~\eqref{eq:Dyn}.
\end{proof}
\begin{rem}[Extension to $\tra{(\tilde{U},\tilde{Y})}{1-\tini}{0}$] \label{rem:UncertainIni}
	Consider system~\eqref{eq:Dyn} with an uncertain initial input-output trajectory $\tra{(U,Y)}{1-\tini}{0}=\tra{(\tilde{U},\tilde{Y})}{1-\tini}{0}$. One can split $Z_k = \mean[Z] +Z^{\ini} + \sum_{i=0}^{k-1} Z_k^{w_i}$, $Z\in\{U,Y\}$ with
	\begin{align*}
		&Y^{\ini}_k = \hat{A}\tra{Y^{\ini}}{k-\tini}{k-1} + \hat{B}\tra{U^{\ini}}{k-\tini}{k-1},\\
		&\tra{(U,Y)^{ini}}{1-\tini}{i} = \tra{(\tilde{U}-\mean[\tilde{U}],\tilde{Y}-\mean[\tilde{Y}])}{1-\tini}{0}.
	\end{align*}
	The forward propagation for the PCE coefficients of $Z^{\ini}$, $Z\in\{U,Y\}$ is a simplified case of the propagation of expectation in Lemma~\ref{lem:j0} with $\mean[E]=0$, i.e., the predicted trajectory of the PCE coefficients of an uncertain initial condition satisfies~\eqref{eq:TrajUYr}.
	Then, besides the conditions of Lemma~\ref{lem:PropRV}, $\tra{(U,Y)}{1}{N}$ is a trajectory of \eqref{eq:Dyn} if and only if there exists $G^{\ini}\in\splx{T-N-\tini+1}$ such that 
	\[
			\left[\begin{array}{ll} \Hankel_{\tini}(\traud{u}{1}{T-N-1})\\ \Hankel_{\tini}(\traud{y}{1}{T-N-1}) \\ \midrule \Hankel_{N}(\traud{u}{\tini}{T-i-1}) \\ \Hankel_{N}(\traud{y}{\tini}{T-i-1}) \end{array}\right] G^{w_i} 
		=\left[\begin{array}{ll} \tra{(\tilde{U}-\mean[\tilde{U}])}{1-\tini}{0}\\ \tra{(\tilde{Y}-\mean[\tilde{Y}])}{1-\tini}{0} \\ \midrule \tra{U^{\ini}}{1}{N} \\ \tra{Y^{\ini}}{1}{N} \end{array}\right].
	\]
	A similar result in PCE coefficients also immediately follows and is omitted for brevity.
\end{rem}

In Table~\ref{tab:ComparisonLemmas}, we compare the different fundamental lemmas, including the stochastic fundamental lemma by \citet{pan23stochastic} (Lemma~\ref{lem:StochFundam}) and Lemma~\ref{lem:PropRV}; \citet{berberich22linear} consider deterministic affine systems, while \citet{kerz23data} propose a pre-stabilized deterministic fundamental lemma for stochastic LTI systems subject to process disturbances and measurement noise. We also compare the required data and the number of non-zero entries of the Hankel matrices in the different fundamental lemmas. Here $n_g$ denotes the dimension of vector $g$ or $G$ and $L$ denotes the PCE dimension, which is proportional to the prediction horizon $N$ and is $L=1+N(L_w-1)$ for measured initial condition. Let $n_w=n_y$ and $\tini\ll N$, then we have the approximation
\[
	\frac{\text{non-zero entries in~}\Hankel~\text{of Lemma~\ref{lem:PropRV}}}{\text{non-zero entries in~}\Hankel~\text{of Lemma~\ref{lem:StochFundam}}}\approx\frac{n_u+n_y}{2(n_u+2n_y)}.
\]
Thus, the non-zeros in the Hankel matrix of Lemma~\ref{lem:PropRV} can easily be fewer than 50\% of those in Lemma~\ref{lem:StochFundam} for the same $n_g$ and $N$.

\subsection{Estimation of Disturbance-Free Data} \label{sec:Estimation}
In this section, we first propose a procedure to find a stabilizing feedback gain for the stochastic LTI system~\eqref{eq:VARX}. Then we show how one can estimate an undisturbed trajectory of~\eqref{eq:VARXFree} from the recorded realization trajectory $\trad{(u,y,w)}{1-\tini}{T}$ of~\eqref{eq:VARXReal}.

Given an unstable stochastic LTI system with a sequence of randomly sampled input in the offline data collection phase, the system response grows exponentially over time. Consequently, the constructed Hankel matrix of output data may exhibit ill-conditioning, which causes numerical issues. To prevent the system response from diverging, we design a stabilizing feedback controller
\[
u_k^{\da}=Kz_k^{\da} +v_k \text{ with }z_k^{\ud} = \begin{bmatrix} \traud{u}{k-\tini}{k-1}\\ \traud{y}{k-\tini}{k-1} \end{bmatrix},
\]
where $v_k$ is an additional small random noise to guarantee the persistency of excitation of the inputs. Based on recorded input-output data, one can compute $K$ via solving an optimization problem, see \citet{doerfler23on} for state feedback and \citet{pan24data} for output feedback. When disturbance measurements are unavailable, an estimator for the disturbance realizations is required to obtain $K$, e.g. the least-square estimator~\eqref{eq:Estimator}.
Note that the accuracy of the estimated disturbances remains an open question. Moreover, the recorded input-output trajectory and estimated disturbance realizations, i.e., $\trad{(u,y,\hat{w})}{0}{T-1})$, satisfy dynamics~\eqref{eq:VARX} but with different system matrices $\hat{A}$ and $\hat{B}$, cf. Corollary~3 by \citet{pan24data}. Therefore, Lemma~\ref{lem:StochFundam} and \ref{coro:StochFundamPCE} remain valid with the estimated disturbances.

Due to the estimation error of disturbance realizations, the computed feedback gain $K$ is not guaranteed to stabilize the system~\eqref{eq:DynReal}. Here we propose the following experimental procedure to resolve this issue:
\begin{itemize}
	\item[i)] Sample an input-state trajectory of system~\eqref{eq:DynReal} for a short length, i.e. $\trad{(u,y)}{0}{T}$ with small $T$, and estimate the disturbance $\trad{\hat{w}}{0}{T-1}$
	\item[ii)] Solve the optimization problem proposed in \citet{pan24data} with the sampled data and compute the corresponding $K$.
	\item[iii)] Let the feedback be $u_k = Kz_k + v_k$, where $v_k$ is uniformly sampled from a small interval, e.g. $[10^{-3},10^{-3}]$, and implement the input.
	\item[iv)] Sample the output data of system~\eqref{eq:DynReal} and check whether it stays in a neighbor of origin. If not, go to step i) and repeat the procedure with current input policy; otherwise the procedure terminates.
\end{itemize}
With a stabilizing feedback $K$, we can sample $\trad{(u,y)}{0}{T}$ of~\eqref{eq:DynReal} and construct the Hankel matrices with acceptable condition numbers. 

Consider system~\eqref{eq:VARX} and a corresponding input-output realization trajectory$\trad{(u,y)}{1-\tini}{T}$. We estimate the disturbance realizations $\trad{\hat{w}}{1-\tini}{T-1}$ for the least-square estimator.
Let $\trad{u}{0}{T-1}$ be persistently exciting of order $\hat{T}+\dimx$. Then an estimation of an undisturbed trajectory $\traud{(u,y)}{1}{\hat{T}}$ of system~\eqref{eq:VARXFree} can be computed from
\begin{equation}\label{eq:PredictorT}
\begin{bmatrix*}[l] \Hankel_{\hat{T}+\tini}(\trad{u}{1-\tini}{T}) \\ \Hankel_{\hat{T}+\tini}(\trad{y}{1-\tini}{T}) \\ \Hankel_{\hat{T}}(\trad{\hat{w}}{0}{T-1}) \end{bmatrix*} g = \begin{bmatrix*}[l] \traud{u}{1-\tini}{\hat{T}} \\ \traud{y}{1-\tini}{\hat{T}} \\ 0_{\hat{T}n_y\times1} \end{bmatrix*},
\end{equation}
where the initial trajectory $\traud{(u,y)}{1-\tini}{0}$ is an arbitrary piece of the recorded data $\trad{(u,y)}{1-\tini}{T}$ and is thus known. It is straightforward to see that $\hat{T}\ll T$. Thus, we may need to repeat the procedure \eqref{eq:PredictorT} until an undisturbed trajectory of sufficient length to construct Hankel matrices is obtained.
Moreover, similar to Lemma~3 by \citet{kerz23data}, we can modify~\eqref{eq:PredictorT} to compute a trajectory $\traud{(v,z)}{1}{\hat{T}}$ close to the origin
\begin{equation} \label{eq:PredictorModify}
\begin{bmatrix*}[l] \Hankel_{\hat{T}}(\trad{(u-Kz)}{1}{T})  \\ \Hankel_{\hat{T}}(\trad{z}{1}{T}) \\ \Hankel_{\hat{T}}(\trad{\hat{w}}{0}{T-1})  \end{bmatrix*} g = \begin{bmatrix*}[l]
	\traud{v}{1}{\hat{T}} \\ \traud{z}{1}{\hat{T}} \\ 0_{\hat{T}n_y\times 1} \end{bmatrix*},
\end{equation}
where the input-output trajectory is included in $\traud{z}{1}{\hat{T}}$. This way, we construct the Hankel matrices from the computed data of~\eqref{eq:VARXFree} with acceptable condition numbers.

