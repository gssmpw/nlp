
\appendix


\section{Word and Fixation Level Features}
\label{app:word_fix_level_features}

This section describes the features that constitute both \( e^{word} \) and \( e^{fix} \) in human and generated trials. The full lists of eye movement features at both word and fixation levels appear in \Cref{app:tab:combined-eye-movement-features}. Additionally, the linguistic word properties that, together with word-level eye movement features, constitute \( e^{word} \) are listed in \Cref{app:tab:linguistic-features}.

Here we provide definitions of the six standard eye movement measures presented in \Cref{sec:feature_based_methods}:

\paragraph{Total Fixation Duration (TFD)} The total duration of fixations on a word.

\paragraph{Gaze Duration (GD)} The total duration of all fixations on a word from the first time entering it to the first time exiting it. See \Cref{app:synthesis_details} for details on GD implementation in \mbox{E-Z} Reader.

\paragraph{First Fixation Duration (FFD)} The duration of the first fixation on a word.

\paragraph{Regression Rate} The proportion of saccades per word that move regressively (backwards).

\paragraph{Skip Rate} Unlike the other features defined in this section, this is a trial-level feature, defined as the proportion of skipped words in a trial (i.e., the fraction of words where \texttt{total\_skip = 1}; see \Cref{app:tab:combined-eye-movement-features}).

\begin{table*}[ht]
% \onecolumn
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}ll@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Feature Name}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Description}}}                                                  \\
\multicolumn{1}{c}{}                                       & \multicolumn{1}{c}{}                                                                                       \\ \midrule
\textbf{Word-Level Eye Movement Features}                  &                                                                                                            \\ \midrule
IA\_DWELL\_TIME                                            & (\textbf{TFD}) The sum of the duration across all fixations that fell in the current interest area                        \\
IA\_DWELL\_TIME\_\%                                        & Percentage of trial time spent on the current interest area (IA\_DWELL\_TIME / PARAGRAPH\_RT).        \\
IA\_FIXATION\_COUNT                                        & Total number of fixations falling in the interest area.                                                    \\
IA\_REGRESSION\_IN\_COUNT                                  & (\textbf{Regression Rate}) Number of times interest area was entered from a higher IA\_ID (from the right in English).                \\
IA\_REGRESSION\_OUT\_FULL\_COUNT                           & Number of times interest area was exited to a lower IA\_ID (to the left in English).                       \\
IA\_FIRST\_FIX\_PROGRESSIVE                                & Checks whether the first fixation in the interest area is a first-pass fixation.                           \\
IA\_FIRST\_FIXATION\_DURATION                              & (\textbf{FFD}) Duration of the first fixation event that was within the current interest area                             \\
IA\_FIRST\_RUN\_DWELL\_TIME & (\textbf{GD}) Dwell time of the first run (i.e., the sum of the duration of all fixations in the first run of fixations within the current interest area). \\
IA\_TOP                                                    & Y coordinate of the top of the interest area.                                                              \\
IA\_LEFT                                                   & X coordinate of the left-most part of the interest area.                                                   \\
normalized\_Word\_ID                                       & Position in the paragraph of the word interest area, normalized from zero to one.                          \\
IA\_REGRESSION\_OUT\_COUNT  & Number of times interest area was exited to a lower IA\_ID (to the left in English) before a higher IA\_ID was fixated in the trial.         \\
PARAGRAPH\_RT                                              & Reading time of the entire paragraph.                                                                      \\
total\_skip                                                & Binary indicator whether the word was fixated on.                                                          \\ \midrule
\textbf{Fixation-level Eye Movement Features}              &                                                                                                            \\ \midrule
CURRENT\_FIX\_INDEX                                        & The position of the current fixation in the trial.                                                         \\
CURRENT\_FIX\_DURATION                                     & Duration of the current fixation.                                                                          \\
CURRENT\_FIX\_X                                            & X coordinate of the current fixation.                                                                      \\
CURRENT\_FIX\_Y                                            & Y coordinate of the current fixation.                                                                      \\
CURRENT\_FIX\_INTEREST\_AREA\_INDEX                        & The word index (IA\_ID) on which the current fixation occurred.                                            \\
NEXT\_FIX\_INTEREST\_AREA\_INDEX                           & The word index (IA\_ID) on which the next fixation occurred.                                               \\ \bottomrule
\end{tabular}%
}
\caption{Word-level and fixation-level eye movement features, defined and extracted by SR Data Viewer.}
\label{app:tab:combined-eye-movement-features}
\end{table*}


\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}ll@{}}
\toprule
\multicolumn{1}{c}{\textbf{Feature Name}} & \multicolumn{1}{c}{\textbf{Description}} \\
\midrule
Surprisal &  \makecell[l]{\cite{hale2001probabilistic,levy2008expectation}, formulated as $-\log_2(p(word|context))$ for each \textit{word} given the preceding textual content of the \\ paragraph as \textit{context}, probabilities extracted from the GPT-2-small language model \cite{radford2019language,wolf-etal-2020-transformers}.} \\
Wordfreq\_Frequency & Frequency of the word based on the Wordfreq package \cite{robyn_speer_2022_7199437}, formulated as $-\log_2(p(word))$. \\
Length & Length of the word in characters. \\
start\_of\_line & Binary indicator of whether the word appeared at the beginning of a line. \\
end\_of\_line & Binary indicator of whether the word appeared at the end of a line. \\
Is\_Content\_Word & \makecell[l]{Binary indicator of whether the word is a content word. \\ A content word is defined as a word that has a part-of-speech tag of either PROPN, NOUN, VERB, ADV, or ADJ.} \\
n\_Lefts & The number of leftward immediate children of the word in the syntactic dependency parse.\\
n\_Rights & The number of rightward immediate children of the word in the syntactic dependency parse.
\\
Distance2Head & The number of words to the syntactic head of the word.\\
\bottomrule
\end{tabular}%
}
\caption{Linguistic word properties and their descriptions. POS tags and parse trees were obtained using SpaCy \cite{Honnibal2020}.}
\label{app:tab:linguistic-features}
\end{table*}

\section{Trial Level Feature Sets}
\label{app:trial_level_features}

For all tasks, we computed all features for each trial independently of other trials.

In all feature-based models, we address the strong co-linearity observed among several features by applying Principal Component Analysis (PCA). A PCA model is fit on the training set, and the training, validation, and test features are transformed using the trained PCA. The number of PCA components is determined as the minimum required to maintain a specified fraction of explained variance. This fraction is optimized during hyperparameter tuning (see \Cref{app:hyperparams} for the search space we use).

In addition to the six standard eye movement features listed in \Cref{sec:feature_based_methods}, we also computed two additional measures for each trial:
\begin{itemize}
    \item \textbf{\texttt{num\_of\_words\_with\_TFD\_GD\_diff}:} This is the proportion of fixated words for which TFD $>$ GD, indicating refixations on a word after the first pass.
    
    \item \textbf{\texttt{mean\_without\_first\_run\_dwell\_time}:} For words fixated more than once (including first pass only), this feature represents the average extra fixation duration per additional fixation (i.e., TFD minus GD, divided by the number of additional fixations). If no word is fixated more than once, the value is set to 0.
\end{itemize}

% In addition, following 

\subsection{Word Property Coefficients}
\label{app:response_to_ling}
The formula for the linear model is:
\begin{equation*}
\begin{aligned}
Measure \sim 1 + Surp + Freq + Length + \\
Freq:Length + normalized\_word\_index
\end{aligned}
\end{equation*}

For each trial, we fit a linear model using the $OLS$ function from the \texttt{Statsmodels} library \cite{seabold2010statsmodels}. Before fitting the model, we normalize all measures. In order to maintain the assumptions of the linear model, we exclude zero values for the measures TFD, FFD, GD (their original distribution is normally-shaped with a point mass at zero due to the high number of skips).
Surprisal \cite{hale2001probabilistic,levy2008expectation} is defined as $-\log_2(p(word|context))$ for each \textit{word} given the preceding textual content of the 
 textual item as \textit{context}, probabilities extracted from the GPT-2-small language model \cite{radford2019language,wolf-etal-2020-transformers}. Frequency is based on the Wordfreq package \cite{robyn_speer_2022_7199437}, formulated as $\log_2(p(word))$. Length is defined by the number of characters, ignoring punctuation.
 We also include $normalized\_word\_index$ following the results presented in \cite{shubi2023}, which show general decrease in reading times for later words within each paragraph in OneStop.



\subsection{Saccade Network Measures}
\label{app:network_measures}


As described in \Cref{sec:feature_based_methods}, we define the directed graph $G=\{V,T\}$ such that $V$ is the set of words in $W$, and for all $u,v\in V$:
 $$T=\{\left(u,v\right): \text{there is a saccade from }u \text{ to }v\}$$ 

A visualization example of two such networks appears in \Cref{fig:sacc-net-viz}.
 
 The following measures are computed for each saccade network instance:
\begin{enumerate}
    \item \textbf{Average Degree}:
    \[
    \text{Avg Degree} = \frac{\sum_{v \in V} \deg(v)}{|V|}
    \]
    where \( \deg(v) \) is the degree of vertex \( v \), and \( |V| \) is the number of vertices in \( G \).

    \item \textbf{Density}:
    \[
    \text{Density} = \frac{2|T|}{|V|(|V| - 1)}
    \]
    for an undirected graph \( G \), where \( |T| \) is the number of edges in \( G \).

    \item \textbf{Average Clustering Coefficient}:
    \[
    \text{Avg CC} = \frac{1}{|V|} \sum_{v \in V} C(v)
    \]
    where \( C(v) \) is the clustering coefficient of vertex \( v \), defined as the fraction of pairs of neighbors of \( v \) that are connected.

    \item \textbf{Average Betweenness Centrality}:
    \[
    \text{Avg Betweenness} = \frac{1}{|V|} \sum_{v \in V} b(v)
    \]
    where \( b(v) \) is the betweenness centrality of vertex \( v \), defined as the fraction of shortest paths in \( G \) that pass through \( v \).

    \item \textbf{Average Closeness Centrality}:
    \[
    \text{Avg Closeness} = \frac{1}{|V|} \sum_{v \in V} c(v)
    \]
    where \( c(v) \) is the closeness centrality of vertex \( v \), defined as the reciprocal of the average shortest path length from \( v \) to all other vertices in \( G \).

    \item \textbf{Transitivity}:
    \[
    \text{Transitivity} = \frac{3 \times \text{number of triangles}}{\text{number of connected triplets}}
    \]
    where a triangle is a set of three mutually connected vertices, and a triplet is a set of three vertices where at least two are connected.

    \item \textbf{Number of Bridges}:
    \begin{equation*}
\begin{aligned}
    \text{num\_bridges} = |\{e \in T \mid G \setminus \{e\} \\ \text{ has more connected components than } G\}|
\end{aligned}
\end{equation*}
    where a bridge is an edge whose removal increases the number of connected components in \( G \).
\end{enumerate}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/arcfig.pdf}
    \caption{Visualization of two Saccade Networks as defined in \Cref{sec:feature_based_methods}.The top network represents the first reading, while the bottom network corresponds to the repeated reading of the same paragraph by the same participant. Different colors indicate different sentences within the paragraph.}
    \label{fig:sacc-net-viz}
\end{figure*}



\section{Scanpath Generation}
\label{app:synthesis_details}
For eye movement data generation we use \mbox{E-Z} Reader 10.4 \cite{veldre2023understanding} with the default parameters estimated from \cite{schilling1998comparing}:
\begin{align*}
    &A = 25.0, \quad \alpha_1 = 124.0, \quad \alpha_2 = 11.1,\\
    &\alpha_3 = 76.0, \quad \delta = 1.68, \quad \epsilon_1 = 0.1,\\
    &\epsilon_2 = 0.5, \quad \epsilon_3 = 1.0, \quad \eta_1 = 0.5, \quad \xi = 0.5\\
    &\eta_2 = 0.1, \quad I = 50.0, \quad \lambda = 0.25,\\
    &M_1 = 150.0, \quad M_2 = 25.0, \quad \omega_1 = 6.0,\\
    &\omega_2 = 3.0, \quad p_F = 0.01, \quad \psi = 7.0,\\
    &S = 25.0, \quad \sigma_{\gamma} = 20.0, \quad V = 60.0
\end{align*}
Parameter definitions appear in \cite{reichle_using_2013}. We set the  \texttt{includeRegressionTrials} parameter to \texttt{True} to allow inter-word regressions Out of the 1000 generated scanpaths per text, we choose the prototype scanpath to be the one which minimizes the mean Scasim \cite{von2011scanpath} distance to all other scanpaths. We use Scasim with the following formula 
\begin{align*}
    &\texttt{CURRENT\_FIX\_DURATION} \sim
    \texttt{CURRENT\_FIX\_X}\\
    & + \texttt{CURRENT\_FIX\_Y}
\end{align*}
with the parameters \texttt{center\_x=1280}, \texttt{center\_y=720}, \texttt{distance=77}, \texttt{unit\_size=1/60} and \texttt{normalize=False} because all scanpaths correspond to the same text and model parameters.

\subsection{Modifications in Word and Fixation Level Features}
For each text, we compute the same set of word- and fixation-level features used in human trials, as detailed in \Cref{app:word_fix_level_features}. Here, we highlight differences in feature definitions between human trials and those generated by the \mbox{E-Z} Reader model.

In OneStop, each trial corresponds to a single reading of a textual item by one subject. As a result, the word-level measures for a trial are extracted from a single scanpath. In contrast, the E-Z Reader model derives its word-level measures by aggregating scanpaths from 1000 statistical subjects. This aggregation introduces several differences compared to human-derived features:

\begin{itemize}
    \item \textbf{\texttt{IA\_SKIP}:} \\
    In human trials (see \Cref{app:tab:combined-eye-movement-features}), \texttt{IA\_SKIP} is binary, indicating whether a word was skipped. In E-Z Reader trials, however, this variable takes on a continuous value between 0 and 1, representing the proportion of statistical subjects who skipped the word.
    
    \item \textbf{Reading Time Measures for Skipped Words:} \\
    For human data, skipped words (i.e., those with \texttt{total\_skip = 1}) are assigned a value of zero for all reading-time features (e.g., TF, GD, FFD). In the E-Z Reader model, reading-time measures are summed over all statistical subjects and then normalized by the number of subjects who fixated the word. Consequently, these measures reflect only the fixation cases. To address this discrepancy, we excluded non-fixated words from human trials in the analysis presented in \Cref{tab:gen-quality} and from the trial-level feature extraction used in the feature-based models.
    
    \item \textbf{\texttt{PARAGRAPH\_RT} and \texttt{IA\_DWELL\_TIME\_\%}:} \\
    For E-Z Reader trials, these measures were computed using an "expected \texttt{IA\_DWELL\_TIME}," which is calculated as
    \[
    \texttt{IA\_DWELL\_TIME} \times \left(1 - \texttt{total\_skip}\right).
    \]
    This adjustment ensures that the measures account for the proportion of fixated versus skipped words.
\end{itemize}


\paragraph{Gaze Duration (GD)} We retain the original implementation of GD, which differs from the version provided in SR Data Viewer. In \mbox{E-Z} Reader, GD is computed at the word level as: $$GD(w)=\frac{\sum_{S_{FP}}{\text{First run dwell time}}}{|S|}$$
Where $S$ represents the subset of statistical subjects who fixated on word $w$, and $S_{FP}$ represents the subset of statistical subjects who fixated on word $w$ \textbf{during first pass}.
This formulation can result in GD being smaller than FFD in some cases.

\paragraph{Fixation Location on Screen} For human trials, the features \texttt{CURRENT\_FIX\_X} and \texttt{CURRENT\_FIX\_Y} specify the coordinates of each fixation on the screen. As \mbox{E-Z} Reader is inherently incapable of providing such features, we approximate them by using the center of each word 







\subsection{Synthetic Data Analysis}
For the comparison between human and \mbox{E-Z} Reader generated trials, presented in \Cref{tab:gen-quality}, we use the same set of trial pairs as in model training (both first reading and repeated reading). For each human trial, we first extract all measures listed in \Cref{tab:gen-quality}, yielding a single value per measure type and human trial (in total: 4 measures $\times$ (1944 $\times$ 2) trials).
To obtain the values presented in \Cref{tab:gen-quality}, for each combination of eye movement measure and comparison type (either First Reading versus \mbox{E-Z} Reader or Repeated Reading versus \mbox{E-Z} Reader) we fit a linear mixed model formulated as $measure\_diff \sim 1 + (1|text)$, and extract the mean and standard error of the intercept.

For comparing the absolute differences between first, repeated reading and \mbox{E-Z} Reader, we apply the following procedure:

\begin{enumerate}[leftmargin=*, label=\textbf{\arabic*.}]
    \item \textbf{Modeling:} For each measure (Fixation Count, Mean TFD, Regression Rate, and Skip Rate), we fit a mixed-effects model:
    \[
    \resizebox{0.45\textwidth}{!}{$
    \texttt{measure\_diff} \sim \texttt{comparison\_type} + \left(1\mid \texttt{text\_id}\right)
    $}
    \]
    where \texttt{comparison\_type} is a binary indicator being 1 for first reading minus \mbox{E-Z} Reader differences and 0 for repeated reading minus \mbox{E-Z} Reader differences.

    \item \textbf{Test Statistic:} We define
    \[
    d=|\beta_{FR-EZ}|-|\beta_{FR-EZ}+\beta_{RR-EZ}|,
    \]
    where $\beta_{FR-EZ}$ is the intercept (first reading) and $\beta_{FR-EZ}+\beta_{RR-EZ}$ is the mean for \mbox{E-Z} Reader vs. repeated reading differences.
    
    \item \textbf{Bootstrapping:} We perform 1000 text-level bootstrap iterations (sampling with replacement) to derive the sampling distribution of $d$ and compute one-sided $p$-values for all measures other than Mean TFD (for which we compute two-sided).
\end{enumerate}
In addition, in \Cref{app:fig:gen-quality}, we provide histograms that illustrate the distribution of trial-level differences for each measure.

\label{app:sec:synth-data-analysis}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/comparison_histograms.pdf}
    \caption{This visualization displays histograms representing the trial-level differences between human trials and the corresponding E-Z Reader synthesized trials. The arrangement of histograms mirrors the transposed rows and columns of \Cref{tab:gen-quality}. In each histogram, the top left corner shows the mean and standard deviation of the values.}
    \label{app:fig:gen-quality}
\end{figure*}

\section{Cross-Validation and Participant-to-Article Assignment}
\label{app:CV}

For our cross-validation procedure, we split the data separately for each combination of article batch (1–3) and reading regime (information-seeking, ordinary reading). Each such combination consists of 10 articles and 60 participants, resulting in 60 “article-participant” pairs including both first and repeated reading version of each aritlce-participant pair. 

\paragraph{Data Splitting Rationale.}
Our data splitting is derived from an assignment of 6 participants to each article (for a total of 60 participants). This assignment must satisfy the following:
\begin{enumerate}
    \item All 6 participants assigned to article~\(i\) have reread it (repeated reading).
    \item Of these 6 participants, exactly half (3) performed \emph{consecutive} rereading and the other half performed \emph{nonconsecutive} rereading.
    \item All 6 participants assigned to article~\(i\) have read different \emph{other} articles in the repeated reading. 
\end{enumerate}
Section~\ref{app:CV_alg_section} below details the algorithmic procedure used to achieve this participant-to-article assignment.

\paragraph{Illustration of the Splits.}
Figure~\ref{fig:CV} illustrates one of the cross-validation folds for a single combination of article batch and reading regime (60 participants out of the 180). Participants are grouped by columns (from \(6\cdot(i-1)\) to \(6 \cdot i\)) according to the article to which they are assigned. Each table entry corresponds to the first and repeated readings of article~\(i\) by participant~\(j\). Since each participant reads exactly \emph{two} articles in repeated reading, each column has exactly two non-empty entries (one for consecutive and one for nonconsecutive repeated reading).

To create the data splits for the \(i\)-th fold, we select article \((10-i)\) and its corresponding participants as the unseen item and participants for the validation split, and \((10+1-i)\) as the unseen item and participants for the test split. Figure~\ref{fig:CV} demonstrates this process for fold~1.

\paragraph{Balanced Evaluation Regimes.}
Thanks to the properties of our participant-to-article assignment, we ensure:
\begin{enumerate}
    \item \emph{Balanced evaluation regimes for validation and test sets.} For each set, we balance “new item,” “new participant,” and “new item \& participant” splits. Specifically, in the test set, each regime has 6 “article readings.” In the validation set, the new item and new participant splits have 5 “article readings” each, while the new item \& participant split has 6.
    \item \emph{Balanced rereading conditions.} In the test split, all evaluation regimes are further balanced with respect to consecutive and nonconsecutive repeated reading (3 of each per regime).
\end{enumerate}

\subsection{Participant-to-Article Assignment Algorithm}
\label{app:CV_alg_section}

To assign participants to articles under the constraints described above, we used \texttt{cvxpy} \cite{diamond2016cvxpy} to solve the optimization problem detailed in Algorithm~\ref{app:CV_alg}. This procedure is applied independently for each combination of article batch (3 batches) and reading regime (information-seeking, ordinary reading). For each combination, we extract matrices \(P\) and \(Q\) as defined in Algorithm~\ref{app:CV_alg}, then solve to obtain a feasible assignment of participants to articles.

The optimization problem includes several key constraints to ensure the assignment is valid and meets the desired balance:
\begin{itemize}
    \item \textbf{Constraint 1:} Each participant is assigned to exactly one article.
    \item \textbf{Constraints 2 and 3:} Guarantee that:
    \begin{enumerate}
        \item Exactly 6 participants are assigned to each article.
        \item All 6 assigned participants had reread that article.
        \item Among them, half had read it in a consecutive rereading and half in a nonconsecutive rereading.
    \end{enumerate}
    These constraints explain the “diagonal” structure in Figure~\ref{fig:CV} and the distribution of symbols '{\scriptsize \protect\divorced}' and '{\scriptsize \protect\unmarried}' within each diagonal cell.
    \item \textbf{Constraint 4:} Among participants assigned to article~\(i\), at most one participant has reread any other article. This maximizes the coverage of articles in the unseen participant regime and ensures exactly 5 different participants in the unseen participant regime for the validation split.
\end{itemize}

\begin{algorithm*}[ht!]
\caption{Participant-to-articles assignment algorithm}
\label{app:CV_alg}
\begin{algorithmic}
    \State \textbf{Input:} Parameters and variables:
    \begin{itemize}
        \item \(m\): number of items (articles).
        \item \(n\): number of participants.
        \item \(P \in \{0, 1\}^{m\times n}\): \(P_{i,j} = 1\) if participant \(j\) read article \(i\) in a \emph{consecutive} repeated reading, 0 otherwise.
        \item \(Q \in \{0, 1\}^{m\times n}\): \(Q_{i,j} = 1\) if participant \(j\) read article \(i\) in a \emph{nonconsecutive} repeated reading, 0 otherwise.
    \end{itemize}
    \State \textbf{Variable:} \( B \in \{0, 1\}^{n\times m} \)

    \State \textbf{Objective:} Minimize a constant (null optimization problem):
    \[
        \text{Minimize: } 0
    \]
    
    \State \textbf{Constraints:} 
    \begin{align*}
        & \text{Constraint 1: } \forall j \in [n]: \sum_{i\in [m]}{B_{j,i}} = 1 \\
        & \text{Constraint 2: } \forall i \in [m]: \text{row}_i(P) \cdot \text{col}_i(B) = 3 \\
        & \text{Constraint 3: } \forall i \in [m]: \text{row}_i(Q) \cdot \text{col}_i(B) = 3 \\
        & \text{Constraint 4: } (P+Q) \cdot B - 6\cdot I \leq 1
    \end{align*}
    
    \State \textbf{Output: } 
    Solve this constrained null optimization problem using \texttt{cvxpy} to obtain a feasible solution \(B\) which satisfies all constraints. The assignment for each participant \(j \in [n]\) is then
    \[
        \text{assigned\_article}(j) = \arg\max(\text{row}_j(B)).
    \]
\end{algorithmic}
\end{algorithm*}

This assignment directly underlies the cross-validation splits described earlier in this appendix (see Figure~\ref{fig:CV}).


\section{Hyperparameter Tuning}
\label{app:hyperparams}

We apply standardization for each feature in both word and fixation level representations. Mean and standard deviation are computed on the train set and applied to the validation and test sets, separately for each split. Feature normalization is performed using Scikit-learn \cite{pedregosa_scikit-learn_2011}.

For all the neural models, we use the AdamW optimizer \cite{loshchilov_decoupled_2018} with a batch size of $16$, a linear warmup ratio of $0.1$, and a weight decay of 0.1, following best practice recommendations from \citet{liu_roberta_2019} and \citet{mosbach_stability_2021}. The search space for learning rates is $\{0.00001, 0.00003, 0.0001\}$ and for dropout $\{0.1, 0.3, 0.5\}$. For RoBERTEye models, we allow the backbone RoBERTa weights to either be frozen or trainable. 
For all XGBoost based models, we searched over learning rate $\in \{0.3, 0.01, 0.001\}$, number of estimators $\in \{10, 100, 1000\}$, maximal tree depth $\in \{4, 6, 10\}$ and a regularization parameter $\alpha\in \{0, 0.1, 1, 10\}$. The XGBoost search space is a subset of the default search space used in \cite{h2o_Python_module} and includes the default hyperparameters implemented in \cite{chen2016xgboost}. In addition, in all feature based models we optimize the lower bound of the fraction of explained variance after a PCA transformation which constrains the number of components taken $\in \{0.8, 0.9, 1\}$.

\section{Hardware and Software}
\label{app:hardware-software}

All neural networks are trained using the Pytorch Lighting library \cite{Falcon_PyTorch_Lightning_2019,paszke2019pytorch} and evaluated using torch-metrics \cite{TorchMetrics_-_Measuring_2022} on a NVIDIA A100-40GB and A40-48GB GPUs.  We adapt Huggingface's RoBERTa implementation \cite{wolf-etal-2020-transformers}.
The baselines described in \Cref{sec:baselines} are reimplemented in this framework as well. A single training epoch took approximately 3 minutes. We train for a maximum of 30 epochs, stopping after 5 epochs without improvement on the validation set.

The number of model parameters for RoBERTEye is either between 2-3M parameters (depending on RoBERTEye-Words or RoBERTEye Fixations) when the backbone RoBERTa is frozen, otherwise 355M.

The code base for this project was developed with the assistance of GitHub Copilot, an AI-powered coding assistant. All generated code was carefully reviewed.

We utilized the \texttt{lme4} package in \texttt{R} \cite{Bates2015-ts} and the \texttt{MixedModels} package in \texttt{Julia} \cite{alday_2025_14838413} for fitting linear mixed-models.

\section{Data}
\label{sec:app-data}

We use OneStop Eye Movements \cite{onestop2025preprint} an eyetracking dataset in which native speakers of English read articles in English from the Guardian. The eyetracking data was collected with an Eyelink 1000 Plus eyetracker (SR Research) at a sampling rate of 1000Hz. The dataset includes 360 participants reading articles from the OneStopQA reading comprehension dataset \citep{berzak_starc_2020}, which includes 30 articles with 4-7 paragraphs (162 paragraphs in total). Each article is available in two text difficulty levels, an original Advanced version and a simplified Elementary version and each paragraph is accompanied by 3 multiple choice reading comprehension questions that can be answered based on any of the text difficulty versions. 

The 30 articles are divided into 3 batches. Each participant reads a single 10-article batch with a random ordering of the articles. Participants read each paragraph in one of its two difficulty versions on a separate screen, and then answer one of the three multiple choice reading comprehension questions on a new screen, without the ability to return to the paragraph. The participants are split between two reading regimes. Half of the participants are assigned to an ordinary reading regime, where the question is presented only after the paragraph. The other half is in an information-seeking regime where participants are also presented with the question (but not the answers) prior to reading the paragraph. In this work, we exclude all participants in the information seeking condition from both the analysis and the training procedure (180 participants are left).

After reading a 10-article batch, participants read two articles for a second time. In repeated reading, the paragraphs are identical to the first reading, while the questions are different. The article in position 11 is a \emph{consecutive} second presentation of the article in position 10. The article in position 12 is a \emph{nonconsecutive} second presentation of an article in one of the positions 1-9. Thus, half of the repeated reading data captures immediate consecutive rereading of the same article, and the other half is rereading with intervening reading material, ranging from 2 to 10 articles. Overall, there are 720 second presentations of articles, 360 in consecutive rereading in position 11 and 360 in a nonconsecutive rereading in position 12. The first reading of position 12 articles occurs 72 times in position 1 and 36 times in each of the positions 2-9. The 720 repeated article readings correspond to 3,888 paragraph trials with a total of 211,081 word tokens over which eye movements were collected, split equally between positions 11 and 12 and the two reading regimes.

\section{Results}
\label{app:results}
In this section, we present additional results for the two task variations. Validation accuracy is reported in \Cref{app:tab:val_accuracy}, F1 scores for both validation and test partitions are shown in \Cref{app:tab:f1_scores}, and Recall and Precision for both validation and test partitions are provided in \Cref{app:tab:prec_recall}. For all result tables, 95\% confidence intervals are standard normal bootstrap confidence interval \cite{davison1997bootstrap} with $B=1000$. In addition, when comparing between models, we also experimented with adding a random effect for the fold number, but the low variance between folds prevented the model from converging.

\renewcommand{\arraystretch}{1.5}
\begin{table*}[ht]
\centering
\small
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllccccc}
\toprule
\makecell{\textbf{Eval}\\\textbf{Type}} & 
\makecell{\textbf{Task}\\\textbf{Variant}} &
\makecell{\textbf{Model}} &
\makecell{\textbf{Eye Movements}\\\textbf{Input}} &
\makecell{\textbf{New Item}\\\textbf{Seen Participant}} &
\makecell{\textbf{New Participant}\\\textbf{Seen Item}} &
\makecell{\textbf{New Item \&}\\\textbf{Participant}} &
\makecell{\textbf{All}} \\
\midrule
\multirow{12}{*}{\makecell{Validation}} & 
\multirow{4}{*}{\makecell{Paired\\Trials}} & 
Reading Speed & $E_S^{W,r}, E_S^{W,r'}$ & $87.7_{\pm2.3}$ & $88.8_{\pm2.2}$ & $87.1_{\pm2.1}$ & $87.9_{\pm1.3}$ \\[1ex]
 & & XGBoost & $E_S^{W,r}, E_S^{W,r'}$ & $92.4_{\pm1.8}$ & $92.5_{\pm1.8}$ & $90.7_{\pm1.9}$ & $91.8_{\pm1.0}$ \\[1ex]
 & & RoBERTEye Fixations & $E_S^{W,r}, E_S^{W,r'}$ & $87.4_{\pm2.2}$ & $89.8_{\pm2.0}$ & $87.9_{\pm2.0}$ & $88.3_{\pm1.2}$ \\[1ex]
 & & RoBERTEye Words & $E_S^{W,r}, E_S^{W,r'}$ & $92.4_{\pm1.8}$ & $92.4_{\pm1.7}$ & $90.8_{\pm1.8}$ & $91.8_{\pm1.0}$ \\ 
\cmidrule{2-8}
 & \multirow{8}{*}{\makecell{Single\\Trial}} & 
 \multirow{2}{*}{Reading Speed} & $E_S^{W,r}$ & $67.7_{\pm2.2}$ & $66.9_{\pm2.2}$ & $66.0_{\pm2.1}$ & $66.8_{\pm1.3}$ \\[1ex]
 & &  & $E_{EZ}^{W,1}, E_S^{W,r}$ & $66.7_{\pm2.2}$ & $66.8_{\pm2.2}$ & $66.5_{\pm2.2}$ & $66.7_{\pm1.3}$ \\ 
\cmidrule{3-8} 
 & & \multirow{2}{*}{XGBoost} & $E_S^{W,r}$ & $69.5_{\pm2.0}$ & $70.7_{\pm2.0}$ & $68.7_{\pm2.0}$ & $69.6_{\pm1.2}$ \\[1ex]
 & &  & $E_{EZ}^{W,1}, E_S^{W,r}$ & $70.1_{\pm2.1}$ & $71.2_{\pm1.9}$ & $69.3_{\pm2.0}$ & $70.2_{\pm1.2}$ \\ 
\cmidrule{3-8} 
 & & \multirow{2}{*}{RoBERTEye Fixations} & $E_S^{W,r}$ & $72.7_{\pm2.2}$ & $72.2_{\pm2.1}$ & $71.2_{\pm2.0}$ & $72.0_{\pm1.3}$ \\[1ex]
 & &  & $E_{EZ}^{W,1}, E_S^{W,r}$ & $72.8_{\pm2.1}$ & $73.0_{\pm2.1}$ & $71.3_{\pm2.0}$ & $72.3_{\pm1.2}$ \\ 
\cmidrule{3-8} 
 & & \multirow{2}{*}{RoBERTEye Words} & $E_S^{W,r}$ & $72.9_{\pm2.1}$ & $73.8_{\pm2.0}$ & $72.2_{\pm1.9}$ & $72.9_{\pm1.2}$ \\[1ex]
 & &  & $E_{EZ}^{W,1}, E_S^{W,r}$ & $72.5_{\pm2.2}$ & $72.7_{\pm2.1}$ & $70.8_{\pm2.0}$ & $72.0_{\pm1.2}$ \\ 
\bottomrule
\end{tabular}
}
\caption{Validation accuracy results for the two variants of the first vs. second reading prediction task with 95\% confidence intervals, aggregated across 10 cross-validation splits, and presented for both test and validation partitions.$E_{EZ}^{W,1}$ denotes synthesized eye movements generated using \mbox{E-Z} Reader \cite{reichle2003ez}.}
\label{app:tab:val_accuracy}
\end{table*}


\renewcommand{\arraystretch}{1.5}
\begin{table*}[ht]
\centering
\small
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllc cccc}
\toprule
\multicolumn{1}{l}{\begin{tabular}[l]{@{}l@{}}Eval\\Type\end{tabular}} & 
\multicolumn{1}{l}{\begin{tabular}[l]{@{}l@{}}Task\\Variant\end{tabular}} &
\multicolumn{1}{l}{Model} &
\multicolumn{1}{c}{Eye Movements Input} &
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}New Item\\ Seen Participant\end{tabular}} &
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}New Participant\\ Seen Item\end{tabular}} &
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}New Item \&\\ Participant\end{tabular}} &
\multicolumn{1}{c}{All} \\ \midrule
\multirow{12}{*}{Validation} & \multirow{4}{*}{Paired Trials} & 
Reading Speed & $E_S^{W,r}, E_S^{W,r'}$ & $87.9_{\pm2.4}$ & $88.5_{\pm2.3}$ & $87.3_{\pm2.2}$ & $87.9_{\pm1.3}$ \\ 
 &  & XGBoost & $E_S^{W,r}, E_S^{W,r'}$ & $92.5_{\pm1.9}$ & $92.4_{\pm1.8}$ & $90.7_{\pm2.0}$ & $91.8_{\pm1.1}$ \\ 
 &  & RoBERTEye Fixations & $E_S^{W,r}, E_S^{W,r'}$ & $87.9_{\pm2.3}$ & $89.8_{\pm2.1}$ & $88.1_{\pm2.2}$ & $88.6_{\pm1.3}$ \\ 
 &  & RoBERTEye Words & $E_S^{W,r}, E_S^{W,r'}$ & $92.5_{\pm1.8}$ & $92.4_{\pm1.8}$ & $90.9_{\pm1.8}$ & $91.9_{\pm1.1}$ \\ \cmidrule{2-8}
 & \multirow{8}{*}{Single Trial} & \multirow{2}{*}{Reading Speed} & $E_S^{W,r}$ & $65.1_{\pm2.7}$ & $64.7_{\pm2.7}$ & $65.3_{\pm2.5}$ & $65.1_{\pm1.5}$ \\ 
 &  &  & $E_{EZ}^{W,1}, E_S^{W,r}$ & $65.9_{\pm2.7}$ & $64.7_{\pm2.6}$ & $64.6_{\pm2.5}$ & $65.0_{\pm1.5}$ \\ \cmidrule{3-8} 
 &  & \multirow{2}{*}{XGBoost} & $E_S^{W,r}$ & $70.5_{\pm2.5}$ & $70.2_{\pm2.5}$ & $69.0_{\pm2.3}$ & $69.9_{\pm1.4}$ \\ 
 &  &  & $E_{EZ}^{W,1}, E_S^{W,r}$ & $70.4_{\pm2.5}$ & $68.3_{\pm2.6}$ & $68.8_{\pm2.5}$ & $69.1_{\pm1.5}$ \\ \cmidrule{3-8} 
 &  & \multirow{2}{*}{RoBERTEye Fixations} & $E_S^{W,r}$ & $71.3_{\pm2.6}$ & $71.1_{\pm2.6}$ & $70.3_{\pm2.4}$ & $70.9_{\pm1.4}$ \\ 
 &  &  & $E_{EZ}^{W,1}, E_S^{W,r}$ & $72.5_{\pm2.4}$ & $71.2_{\pm2.4}$ & $71.0_{\pm2.3}$ & $71.5_{\pm1.4}$ \\ \cmidrule{3-8} 
 &  & \multirow{2}{*}{RoBERTEye Words} & $E_S^{W,r}$ & $71.2_{\pm2.6}$ & $70.8_{\pm2.5}$ & $69.5_{\pm2.4}$ & $71.1_{\pm1.5}$ \\ 
 &  &  & $E_{EZ}^{W,1}, E_S^{W,r}$ & $71.1_{\pm2.5}$ & $71.7_{\pm2.5}$ & $70.5_{\pm2.3}$ & $71.1_{\pm1.5}$ \\ \midrule

\multirow{12}{*}{Test} & \multirow{4}{*}{Paired Trials} & 
Reading Speed & $E_S^{W,r}, E_S^{W,r'}$ & $87.8_{\pm2.2}$ & $87.9_{\pm2.2}$ & $87.2_{\pm2.1}$ & $87.6_{\pm1.3}$ \\ 
 &  & XGBoost & $E_S^{W,r}, E_S^{W,r'}$ & $91.5_{\pm1.8}$ & $92.1_{\pm1.8}$ & $90.6_{\pm1.9}$ & $91.4_{\pm1.1}$ \\ 
 &  & RoBERTEye Fixations & $E_S^{W,r}, E_S^{W,r'}$ & $85.6_{\pm2.4}$ & $86.2_{\pm2.3}$ & $85.5_{\pm2.3}$ & $85.7_{\pm1.3}$ \\ 
 &  & RoBERTEye Words & $E_S^{W,r}, E_S^{W,r'}$ & $88.6_{\pm2.1}$ & $89.4_{\pm2.0}$ & $88.7_{\pm2.1}$ & $88.9_{\pm1.2}$ \\ \cmidrule{2-8}
 & \multirow{8}{*}{Single Trial} & \multirow{2}{*}{Reading Speed} & $E_S^{W,r}$ & $64.8_{\pm2.4}$ & $65.3_{\pm2.6}$ & $65.2_{\pm2.5}$ & $65.1_{\pm1.4}$ \\ 
 &  &  & $E_{EZ}^{W,1}, E_S^{W,r}$ & $65.4_{\pm2.6}$ & $65.6_{\pm2.5}$ & $65.2_{\pm2.6}$ & $65.4_{\pm1.5}$ \\ \cmidrule{3-8} 
 &  & \multirow{2}{*}{XGBoost} & $E_S^{W,r}$ & $68.6_{\pm2.4}$ & $69.8_{\pm2.4}$ & $67.8_{\pm2.5}$ & $68.7_{\pm1.4}$ \\ 
 &  &  & $E_{EZ}^{W,1}, E_S^{W,r}$ & $68.7_{\pm2.4}$ & $70.3_{\pm2.3}$ & $68.6_{\pm2.4}$ & $69.2_{\pm1.4}$ \\ \cmidrule{3-8} 
 &  & \multirow{2}{*}{RoBERTEye Fixations} & $E_S^{W,r}$ & $68.2_{\pm2.4}$ & $68.2_{\pm2.5}$ & $68.5_{\pm2.4}$ & $68.3_{\pm1.4}$ \\ 
 &  &  & $E_{EZ}^{W,1}, E_S^{W,r}$ & $69.0_{\pm2.3}$ & $68.2_{\pm2.4}$ & $68.3_{\pm2.4}$ & $68.5_{\pm1.4}$ \\ \cmidrule{3-8} 
 &  & \multirow{2}{*}{RoBERTEye Words} & $E_S^{W,r}$ & $68.1_{\pm2.4}$ & $69.1_{\pm2.4}$ & $67.3_{\pm2.5}$ & $68.1_{\pm1.4}$ \\ 
 &  &  & $E_{EZ}^{W,1}, E_S^{W,r}$ & $68.0_{\pm2.5}$ & $67.4_{\pm2.5}$ & $67.7_{\pm2.5}$ & $67.7_{\pm1.5}$ \\ 
\bottomrule
\end{tabular}%
}
\caption{F1 results for the two variants of the first vs. second reading prediction task with 95\% confidence intervals, aggregated across 10 cross-validation splits, and presented for both test and validation partitions. $E_{EZ}^{W,1}$ denotes synthesized eye movements generated using \mbox{E-Z} Reader \cite{reichle2003ez}.}
\label{app:tab:f1_scores}
\end{table*}


\renewcommand{\arraystretch}{1.5}
\begin{table*}[ht]
\centering
\small
\resizebox{\textwidth}{!}{%
  \begin{tabular}{lllc*{4}{cc}}
    \toprule
    Eval Type & Task Variant & Model & Eye Movements Input & \multicolumn{2}{c}{New Item Seen Participant} & \multicolumn{2}{c}{New Participant Seen Item} & \multicolumn{2}{c}{New Item \& Participant} & \multicolumn{2}{c}{All} \\
    \cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-10}\cmidrule(lr){11-12}
              &              &       &                      & Prec. & Recall & Prec. & Recall & Prec. & Recall & Prec. & Recall \\
    \midrule
    % --- Validation: Paired Trials ---
    \multirow{12}{*}{Validation} 
      & \multirow{4}{*}{Paired Trials} 
          & Reading Speed 
              & $E_S^{W,r},E_S^{W,r'}$ 
              & $87.1_{\pm3.2}$ & $88.7_{\pm3.1}$ 
              & $89.0_{\pm3.1}$ & $88.1_{\pm3.2}$ 
              & $87.5_{\pm2.9}$ & $87.2_{\pm2.9}$ 
              & $87.8_{\pm1.8}$ & $88.0_{\pm1.7}$ \\[1ex]
      & 
          & XGBoost 
              & $E_S^{W,r},E_S^{W,r'}$ 
              & $92.0_{\pm2.7}$ & $93.1_{\pm2.4}$ 
              & $91.2_{\pm2.7}$ & $93.7_{\pm2.3}$ 
              & $92.1_{\pm2.5}$ & $89.5_{\pm2.8}$ 
              & $91.8_{\pm1.5}$ & $91.9_{\pm1.5}$ \\[1ex]
      & 
          & RoBERTEye Fixations 
              & $E_S^{W,r},E_S^{W,r'}$ 
              & $85.5_{\pm3.3}$ & $90.5_{\pm2.8}$ 
              & $87.8_{\pm3.1}$ & $92.0_{\pm2.6}$ 
              & $88.0_{\pm2.8}$ & $88.3_{\pm2.9}$ 
              & $87.1_{\pm1.8}$ & $90.1_{\pm1.6}$ \\[1ex]
      & 
          & RoBERTEye Words 
              & $E_S^{W,r},E_S^{W,r'}$ 
              & $91.8_{\pm2.6}$ & $93.3_{\pm2.4}$ 
              & $91.0_{\pm2.7}$ & $93.7_{\pm2.3}$ 
              & $91.5_{\pm2.4}$ & $90.2_{\pm2.5}$ 
              & $91.5_{\pm1.6}$ & $92.3_{\pm1.5}$ \\
    \cmidrule(lr){2-12}
    % --- Validation: Single Trial ---
      & \multirow{8}{*}{Single Trial} 
          & \multirow{2}{*}{Reading Speed}  
              & $E_S^{W,r}$ 
              & $68.5_{\pm3.3}$ & $62.0_{\pm3.2}$ 
              & $69.2_{\pm3.3}$ & $60.8_{\pm3.2}$ 
              & $67.7_{\pm3.1}$ & $63.0_{\pm3.0}$ 
              & $68.4_{\pm1.9}$ & $62.0_{\pm1.7}$ \\[1ex]
      & 
          & 
              & $E_{EZ}^{W,1},E_S^{W,r}$ 
              & $69.9_{\pm3.2}$ & $65.9_{\pm2.7}$ 
              & $69.2_{\pm3.3}$ & $64.7_{\pm2.6}$ 
              & $67.4_{\pm3.0}$ & $64.6_{\pm2.5}$ 
              & $68.7_{\pm1.9}$ & $65.0_{\pm1.5}$ \\ \cmidrule{3-12}
      & 
          & \multirow{2}{*}{XGBoost}  
              & $E_S^{W,r}$ 
              & $72.1_{\pm3.1}$ & $70.5_{\pm2.5}$ 
              & $73.2_{\pm3.0}$ & $70.2_{\pm2.5}$ 
              & $70.8_{\pm2.8}$ & $69.0_{\pm2.3}$ 
              & $72.0_{\pm1.8}$ & $69.9_{\pm1.4}$ \\[1ex]
      & 
          & 
              & $E_{EZ}^{W,1},E_S^{W,r}$ 
              & $72.3_{\pm3.1}$ & $64.6_{\pm3.2}$ 
              & $72.5_{\pm3.3}$ & $67.4_{\pm3.1}$ 
              & $70.2_{\pm3.0}$ & $66.9_{\pm1.8}$ 
              & $71.5_{\pm1.8}$ & $71.1_{\pm2.2}$ \\ \cmidrule{3-12}
      & 
          & \multirow{2}{*}{RoBERTEye Fixations}   
              & $E_S^{W,r}$ 
              & $75.3_{\pm3.2}$ & $71.3_{\pm2.6}$ 
              & $76.3_{\pm3.1}$ & $71.1_{\pm2.6}$ 
              & $72.8_{\pm2.9}$ & $70.3_{\pm2.4}$ 
              & $74.7_{\pm1.7}$ & $70.9_{\pm1.4}$ \\[1ex]
      & 
          & 
              & $E_{EZ}^{W,1},E_S^{W,r}$ 
              & $73.2_{\pm3.1}$ & $72.5_{\pm2.4}$ 
              & $73.7_{\pm3.1}$ & $71.2_{\pm2.4}$ 
              & $71.6_{\pm2.9}$ & $71.0_{\pm2.3}$ 
              & $72.7_{\pm1.8}$ & $71.5_{\pm1.4}$ \\ \cmidrule{3-12}
      & 
          & \multirow{2}{*}{RoBERTEye Words} 
              & $E_S^{W,r}$ 
              & $76.3_{\pm3.1}$ & $71.2_{\pm2.6}$ 
              & $77.7_{\pm2.9}$ & $70.8_{\pm2.5}$ 
              & $75.2_{\pm2.8}$ & $69.5_{\pm2.4}$ 
              & $76.3_{\pm1.7}$ & $71.1_{\pm1.5}$ \\[1ex]
      & 
          &  
              & $E_{EZ}^{W,1},E_S^{W,r}$ 
              & $74.5_{\pm3.2}$ & $71.1_{\pm2.5}$ 
              & $75.9_{\pm3.2}$ & $71.7_{\pm2.5}$ 
              & $72.8_{\pm3.0}$ & $70.5_{\pm2.3}$ 
              & $74.3_{\pm1.7}$ & $71.1_{\pm1.5}$ \\
    \midrule
    % --- Test: Paired Trials ---
    \multirow{12}{*}{Test} 
      & \multirow{4}{*}{Paired Trials} 
          & Reading Speed 
              & $E_S^{W,r},E_S^{W,r'}$ 
              & $88.4_{\pm2.9}$ & $87.2_{\pm3.0}$ 
              & $88.3_{\pm2.9}$ & $87.6_{\pm3.0}$ 
              & $88.4_{\pm2.8}$ & $86.1_{\pm3.0}$ 
              & $88.3_{\pm1.7}$ & $86.9_{\pm1.8}$ \\[1ex]
      & 
          & XGBoost 
              & $E_S^{W,r},E_S^{W,r'}$ 
              & $90.5_{\pm2.6}$ & $93.0_{\pm2.3}$ 
              & $91.4_{\pm2.5}$ & $89.3_{\pm2.7}$ 
              & $91.9_{\pm2.4}$ & $91.6_{\pm1.5}$ 
              & $91.2_{\pm1.5}$ & $91.5_{\pm1.7}$ \\[1ex]
      & 
          & RoBERTEye Fixations 
              & $E_S^{W,r},E_S^{W,r'}$ 
              & $82.8_{\pm3.3}$ & $88.6_{\pm2.9}$ 
              & $83.7_{\pm3.1}$ & $88.8_{\pm2.9}$ 
              & $82.4_{\pm3.3}$ & $88.7_{\pm2.8}$ 
              & $82.9_{\pm1.9}$ & $88.7_{\pm1.6}$ \\[1ex]
      & 
          & RoBERTEye Words 
              & $E_S^{W,r},E_S^{W,r'}$ 
              & $87.6_{\pm3.0}$ & $89.6_{\pm2.7}$ 
              & $88.2_{\pm2.8}$ & $90.7_{\pm2.6}$ 
              & $89.6_{\pm2.6}$ & $87.9_{\pm2.9}$ 
              & $88.4_{\pm1.6}$ & $89.4_{\pm1.6}$ \\
    \cmidrule(lr){2-12}
    % --- Test: Single Trial ---
      & \multirow{8}{*}{Single Trial} 
          & \multirow{2}{*}{Reading Speed} 
              & $E_S^{W,r}$ 
              & $69.2_{\pm2.9}$ & $60.9_{\pm3.1}$ 
              & $69.1_{\pm3.0}$ & $61.9_{\pm3.1}$ 
              & $68.4_{\pm3.1}$ & $62.4_{\pm3.2}$ 
              & $68.9_{\pm1.8}$ & $61.7_{\pm1.7}$ \\[1ex]
      & 
          & 
              & $E_{EZ}^{W,1},E_S^{W,r}$ 
              & $69.4_{\pm3.2}$ & $61.8_{\pm3.1}$ 
              & $69.4_{\pm3.1}$ & $62.2_{\pm2.9}$ 
              & $68.0_{\pm3.1}$ & $62.6_{\pm3.1}$ 
              & $68.9_{\pm1.8}$ & $62.2_{\pm1.8}$ \\ \cmidrule{3-12}
      & 
          & \multirow{2}{*}{XGBoost} 
              & $E_S^{W,r}$ 
              & $70.7_{\pm2.9}$ & $66.7_{\pm3.0}$ 
              & $72.2_{\pm2.9}$ & $67.6_{\pm3.0}$ 
              & $69.7_{\pm2.9}$ & $66.0_{\pm3.1}$ 
              & $70.8_{\pm1.7}$ & $66.7_{\pm1.2}$ \\[1ex]
      & 
          & 
              & $E_{EZ}^{W,1},E_S^{W,r}$ 
              & $72.2_{\pm3.0}$ & $65.5_{\pm2.9}$ 
              & $72.7_{\pm2.9}$ & $68.0_{\pm2.8}$ 
              & $70.1_{\pm2.9}$ & $67.2_{\pm3.0}$ 
              & $71.6_{\pm1.7}$ & $70.1_{\pm2.1}$ \\ \cmidrule{3-12}
      & 
          & \multirow{2}{*}{RoBERTEye Fixations}
              & $E_S^{W,r}$ 
              & $72.1_{\pm2.9}$ & $64.8_{\pm3.0}$ 
              & $72.9_{\pm3.0}$ & $64.1_{\pm3.0}$ 
              & $71.5_{\pm3.0}$ & $65.8_{\pm3.0}$ 
              & $72.1_{\pm1.7}$ & $64.8_{\pm1.7}$ \\[1ex]
      & 
          & 
              & $E_{EZ}^{W,1},E_S^{W,r}$ 
              & $70.5_{\pm3.0}$ & $67.7_{\pm2.8}$ 
              & $70.7_{\pm2.9}$ & $65.9_{\pm3.0}$ 
              & $68.6_{\pm3.0}$ & $68.0_{\pm2.9}$ 
              & $69.8_{\pm1.7}$ & $67.2_{\pm1.7}$ \\ \cmidrule{3-12}
      & 
          & \multirow{2}{*}{RoBERTEye Words} 
              & $E_S^{W,r}$ 
              & $71.9_{\pm3.0}$ & $64.6_{\pm3.0}$ 
              & $74.0_{\pm3.0}$ & $64.8_{\pm2.9}$ 
              & $71.2_{\pm3.1}$ & $63.8_{\pm2.9}$ 
              & $72.3_{\pm1.7}$ & $64.3_{\pm1.8}$ \\[1ex]
      & 
          & 
              & $E_{EZ}^{W,1},E_S^{W,r}$ 
              & $73.8_{\pm3.1}$ & $63.0_{\pm3.0}$ 
              & $73.0_{\pm3.0}$ & $62.5_{\pm3.0}$ 
              & $72.4_{\pm3.0}$ & $63.5_{\pm3.0}$ 
              & $73.0_{\pm1.8}$ & $63.1_{\pm1.8}$ \\
    \bottomrule
  \end{tabular}%
}
\caption{Precision and Recall (repeated reading being positive and first reading being negative) results for the two variants of the first vs. second reading prediction task with 95\% confidence intervals, aggregated across 10 cross-validation splits, and presented for both test and validation partitions. $E_{EZ}^{W,1}$ denotes synthesized eye movements generated using \mbox{E-Z} Reader \cite{reichle2003ez}.}
\label{app:tab:prec_recall}
\end{table*}



