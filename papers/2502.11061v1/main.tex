% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% Shubi Additions
% For Results Table
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{bbm}
\usepackage{makecell}
\usepackage{xcolor}
\graphicspath{{figures/}}
% for abstract filler
\usepackage{lipsum}
\newcommand{\tbd}[1]{\textcolor{cyan}{#1}}
\newcommand{\tbdref}{\colorbox{orange}{[ref]}}
\newcommand{\tbdfig}[1]{\colorbox{magenta}{[fig #1]}}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{float}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{algorithm, algpseudocode}
\usepackage{newunicodechar}
\usepackage{footnotehyper, tablefootnote}
\usepackage{svg}
\usepackage{amsfonts}
\usepackage{scalerel}
\usepackage[separate-uncertainty,group-minimum-digits=4]{siunitx}

% \newcommand{\divorced}{\scalerel*{\includegraphics{figures/Divorced.pdf}}{}
\def\divorced{\scalerel*{\includegraphics{figures/Divorced.pdf}}{X}}
\def\unmarried{\scalerel*{\includegraphics{figures/Unmarried.pdf}}{X}}


\usepackage{booktabs}
\usepackage{cleveref}

\usepackage{marvosym}

\usepackage{enumitem} %added by Yevgeni


\title{Déjà Vu? Decoding Repeated Reading from Eye Movements}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\ 
% Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
% Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
% \And  ... \And
% Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
% \AND
% Author 2 \\ Address line \\ ... \\ Address line \And
% Author 3 \\ Address line \\ ... \\ Address line}

\author{Yoav Meiri$^1$, Omer Shubi$^1$, Cfir Avraham Hadar$^1$\\ {\bf Ariel Kreisberg Nitzav$^1$ Yevgeni Berzak$^{1,2}$} \\
 $^1$Faculty of Data and Decision Sciences, \\
 Technion - Israel Institute of Technology, Haifa, Israel \\
 $^2$Department of Brain and Cognitive Sciences, \\
 Massachusetts Institute of Technology, Cambridge, USA \\
 \texttt{\{meiri.yoav,shubi,kfir-hadar,ariel.kr\}@campus.technion.ac.il} \\
\texttt{berzak@technion.ac.il} \\
}

\begin{document}
\maketitle
\begin{abstract}
Be it your favorite novel, a newswire article, a cooking recipe or an academic paper -- in many daily situations we read the same text more than once. In this work, we ask whether it is possible to automatically determine whether the reader has previously encountered a text based on their eye movement patterns. We introduce two variants of this task and address them with considerable success using both feature-based and neural models. We further introduce a general strategy for enhancing these models with machine generated simulations of eye movements from a cognitive model. Finally, we present an analysis of model performance which on the one hand yields insights on the information used by the models, and on the other hand leverages predictive modeling as an analytic tool for better characterization of the role of memory in repeated reading. Our work advances the understanding of the extent and manner in which eye movements in reading capture memory effects from prior text exposure, and paves the way for future applications that involve predictive modeling of repeated reading.\footnote{Code is available anonymously \href{https://anonymous.4open.science/r/deja-vu}{here}. 
%The OneStop dataset used in this study is available \href{https://lacclab.github.io/OneStop-Eye-Movements}{here}.
}
\end{abstract}

 
\section{Introduction}

Reading is a widely practiced skill that occupies many hours of our daily lives. During these hours, there are various ways in which we interact with texts. % -- we read for learning, in order to obtain specific information of interest, and some of us even read for pleasure. While
While reading is often thought of as an interaction with new linguistic material, in many daily scenarios we read texts more than once. This can happen because we might want to understand or recall the text better, re-examine specific parts of interest, or simply because we enjoyed reading the text for the first time. 

%\tbd{TODO (Yevgeni): add more on the scientific value - enables better understanding of (1) the extent to which eye movements capture interaction with the text. (2) reflect memory/memorization and prior information integration (3) ?}

The importance of studying repeated reading for understanding human language processing has long been recognized in psychology and psycholinguistics. In these areas of study, it was shown that when reading a text for a second time, the way our eyes move over the text and the extent to which the eye movements depend on the linguistic characteristics of the text tend to differ compared to the first reading. In essence, eye movements in repeated reading reflect reading facilitation: for example, readers tend to read faster and skip more words compared to the first reading. Although the precise differences can depend on the experimental setup, and some are debated, the presence of facilitation effects comes as no surprise, as when encountering a text for the second time readers already have knowledge of its content, and can more easily foresee what comes next at any given time.

Despite the advances in the study of eye movements in repeated reading, prior work has been limited to \emph{descriptive} analyses of overall effects, averaged across texts and participants. Consequently, it is currently unknown how much information can be extracted regarding the type of interaction of a specific reader with a specific text. Addressing this question is important both for improving the scientific understanding of the extent and manner in which eye movements reflect the reader's memory of the text, and for building the foundations for practical applications in areas such as e-learning and educational settings more broadly, where it can be beneficial to infer whether the reader has already encountered the text. 

In this work, we tackle this challenge using a \emph{predictive} modeling approach for determining the interaction of a single reader with a specific text from their eye movements. We pose the following question: is it possible to decode whether the reader is reading a text for the first or the second time from their eye movements over the text? Addressing this question is made possible by OneStop Eye Movements \citep{onestop2025preprint}, the first publicly available dataset that contains eye movement recordings of both first and repeated reading.

We operationalize this question via two prediction tasks. In the first task, given two eye movement samples from the same participant over the same text, the goal is to determine which is first reading and which is repeated reading. In the second, more general, and more challenging variant, the task is to determine whether a single eye movement sample is a first or repeated reading. We address these tasks with a feature-based approach with features that build on the psycholinguistic literature on repeated reading, and with multimodal neural models that combine eye movements with text. We further introduce a strategy for improving predictive accuracy by augmenting models with synthetic eye movement trajectories from a cognitive model of eye movements in reading. 

The contributions of this work are the following:
\begin{itemize}[leftmargin=*]
 \item \textbf{Tasks}: We introduce a new prediction task for the interaction of the reader with the text from their eye movements - automatically determine whether the reader encountered the text previously. We address this task in two variants of decreasing difficulty: (i) a single eye movement sample; (ii) a pair of first and second reading samples from the same participant.
 \item \textbf{Modeling}: We experiment with two types of predictive approaches: (i) feature-based models; (ii) neural multimodal language models. We further introduce a strategy for integrating into the prediction pipeline synthetic data for first reading.
 \item \textbf{Analyses}: We present analyses of model performance as a function of article location in the experiment and the amount of intervening material between readings. These analyses provide insights on the information used by models and on the role of memory in repeated reading effects.
\end{itemize}


\section{Related Work}
\label{sec:related-work}

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/trial_examples.pdf}
    \caption{Examples of eye movements over a single passage; top: first reading, bottom: repeated reading. Circles represent fixations, and lines represent saccades.}
    \label{fig:trial-example}
\end{figure}

When we read, the eye movement trajectory, or \emph{scanpath} over the text is divided into \emph{fixations}, prolonged periods of time during which the gaze location is relatively fixed, and \emph{saccades}, fast transitions between fixations \cite{rayner1998eye,schotter2025beginner}. Prior work in psycholinguistics has consistently demonstrated that this trajectory differs in repeated reading compared to first reading, with large \emph{facilitation} effects marked by shorter text reading times, fewer fixations, shorter fixation durations, longer saccades and fewer regressions (backward saccades) \citep{hyona1990repeated,raney1995freq,schnitzer2006,meiri2024deja}. Several studies have also examined the interaction between repeated reading and the effect of linguistic word properties such as word length, frequency and surprisal on reading times, mostly finding less sensitivity to word properties in repeated reading \citep{raney1995freq,foster2013repeated,zawoyski2015repeated,meiri2024deja}. In line with these studies, \citet{hyona1990repeated} further demonstrated a reduction in the sensitivity of eye movements to the introduction of new topics in rereading. All the above studies examined individual features aggregated across participants and texts, and it is currently unknown whether first and repeated reading can be effectively distinguished using predictive modeling at the level of an individual participant and text.

%Moreover, \citet{meiri2024deja}  found that the presence of intervening material between readings can have a subtle modulating effect on facilitation as observed in differences in global measures such as reading times and skip rate. Facilitation slightly attenuates in ordinary reading for comprehension when intervening material is present, but remains robust in information-seeking reading. The former case hints at the possibility of better memory retention in consecutive compared to non-consecutive reading. Notably, they found that the amount of intervening material, measured by the number of articles between readings, has no effect on these global eye movement measures.
%Building on these findings, our work defines a set of decoding tasks and designs features that capture both standard eye movement metrics and the sensitivity of reading times to linguistic properties of the text.

In machine learning and NLP, a nascent line of work focuses on decoding the properties of the reader and their interaction with the text, from eye movements in reading. These include, among others, decoding of linguistic knowledge \citep{berzak_predicting_2017,berzak_assessing_2018,skerath2023native}, reading comprehension \cite{ahn_towards_2020,reich_inferring_2022,meziere2023using,Shubi2024Finegrained}, subjective text difficulty \cite{reich_inferring_2022} and the reader's goals \citep{hollenstein2023zuco,shubi2024decoding}. The current study falls broadly within this area, but introduces and addresses a new task of decoding repeated reading. %We build on feature sets and models introduced in prior studies on other decoding tasks and on the psycholinguistic literature, and further introduce new features and models for the current task. 
Following \citet{sood2020improving}, our work leverages the output of \mbox{E-Z} Reader, a computational cognitive model for automatic generation of reading scanpath trajectories \cite{reichle1998toward,reichle2003ez,reichle2009using,veldre2023understanding}.


\section{Problem Formulation}
\label{sec:problem}

We ask whether it is possible to accurately distinguish between first and second readings, %as well as consecutive versus non-consecutive repeated readings. 
%We address these questions 
from an eye movement recording of a single participant over a single textual item.
We assume a setup in which a participant \(S\) reads a textual item \(W\), optionally reads \(k\) other items \(\{W'\}^k\), and then reads \(W\) again. The parameter \(k\) can range from 0 for consecutive repeated reading to any $k>0$ for non-consecutive repeated reading. 
Hence, each reading \(r \in \{1,2\}\) (first or repeated) of \(W\) produces a distinct eye movement recording \(E_S^{W,r}\). We define a \emph{decoding task} where the goal is to distinguish between eye movements of a single participant over a single text in first reading \((r=1)\) and repeated reading \((r=2)\). The task has two variants as described below.

\paragraph{Single Trial Task}
The input is an eye movement recording \(E_S^{W,r}\) for text $W$.
The output \(\hat{r} \in \{1,2\}\) corresponds to whether the eye movements \(E\) are from a first or a repeated reading of \(W\). Formally,
\[
(W,\; E_S^{W,r}) \;\longrightarrow\; \hat{r}
\]

\paragraph{Paired Trials Task}
Here the input consists of two eye movement recordings \(E_S^{W,r}\) and \(E_S^{W,r'}\) of the same participant \(S\) in an unknown presentation order and the same text \(W\). % and optionally the text $W$. 
The output is \(\quad  \hat{\left(r,r'\right)} \in \{\left(1,2\right),\left(2,1\right)\}\), i.e., which recording corresponds to the first reading of $W$, and which to the second. Formally,
\[
(W,\; E_S^{W,r},\; E_S^{W,r'}) \;\longrightarrow\; \hat{\left(r,r'\right)} 
\]


\section{Data}
\label{sec:data}




We use OneStop Eye Movements \cite{onestop2025preprint} an eyetracking dataset collected with an Eyelink 1000 Plus eyetracker, where native (L1) speakers of English read Guardian newswire articles. The textual materials are taken from the OneStopQA dataset \citep{berzak_starc_2020}. OneStop Eye Movements includes 180 participants who read for comprehension, each reading a 10-article batch in a randomized order, where each article contains between 4 and 7 paragraphs. Participants read each paragraph on a single page without the ability to return to previous paragraphs. %The participants are split between two reading regimes. Half of the participants are assigned to an ordinary reading regime, where the question is presented only after the paragraph. The other half is in an information-seeking regime where participants are also presented with the question (but not the answers) prior to reading the paragraph.

After reading a 10-article batch, participants read two articles for a second time. In repeated reading, the paragraphs are identical to the first reading, while the questions are different. The article in position 11 is a consecutive second presentation of the article in position 10. The article in position 12 is a non-consecutive second presentation of an article in one of the positions 1-9. Thus, half of the repeated reading data captures immediate consecutive rereading of the same article, and the other half is rereading with intervening reading material, ranging from 2 to 10 articles. \Cref{fig:exp-schema} presents the experimental design schematically.

Overall, there are 360 second presentations of articles, 180 in consecutive rereading in position 11 and 180 in a non-consecutive rereading in position 12. The first reading of position 12 articles occurs 36 times in position 1 and 18 times in each of the positions 2-9. The 360 repeated article readings correspond to 1,944 paragraph trials with a total of 105,540-word tokens over which eye movements were collected, split equally between positions 11 and 12. \Cref{fig:trial-example} shows example trials for first reading and repeated reading. \Cref{sec:app-data} presents further details on the data. %An identical amount of data is used from the first reading of the same articles. 

\begin{figure}
    \centering
    \includegraphics[width=0.65\linewidth]{figures/Experiment_Structure_RR.pdf}
    \caption{First and repeated reading for one participant. After reading a 10-article batch in a random order of articles, there is a consecutive repeated reading of the last article in position 10, and then a non-consecutive repeated reading of one of the articles in positions 1-9.}
    \label{fig:exp-schema}
\end{figure}

\section{Modeling}

We experiment with both feature-based and neural language modeling approaches for representing eye movements and their interaction with the text. %: globally at the text level, at the level of single words, and at the level of single fixations. The models also integrate text either by using linguistic word properties or by using word embeddings from a language model. 
%For the paired trials task, we propose a modeling approach that captures the differences between the two readings. 
For the single trial variant, we present methods that directly predict first versus repeated reading. We also propose a general approach for leveraging synthetic scanpaths as an additional first reading reference, which enables using the same input representations as in the paired trials task.

%\subsection{Eye Movements Representation}
%The eye movement recording \( E_S^{W,r} \) for participant \( S \) during reading \( r \) of text \( W \) can be represented at two aggregation levels:

%\paragraph{Fixation-Level Representation} consists of \( N_{fix} \) fixation vectors, where each \( e_S^{\text{fix}_i} \in \mathbb{R}^{d_{\text{fix}}} \) encodes fixation level features.

%\paragraph{Word-Level Representation} consists of \( N_{words} \) word vectors, where each \(e_S^{\text{word}_j} \in \mathbb{R}^{d_{\text{word}}} \) aggregates all fixation-level features on a word level.

%The full initial representation of eye movement is denoted by:
%\[ E_S^{W,r} =\left( \left(e_S^{\text{word}_j} \right)_{j=1}^{N_{words}} , \left( e_S^{\text{fix}_i} \right)_{i=1}^{N_{fix}}\right) \] 


%Note that \( E_S^{W,r} \) can be modeled at the fixation level, word level, or both, depending on the model.

\subsection{Single Trial Modeling}

\subsubsection*{Feature-Based Model}

For feature-based modeling, we use XGBoost tree-boosting models \cite{chen2016xgboost} with the following global eye movement features, resulting in an input feature vector $e_S^{\text{global}} \in \mathbb{R}^{d_{\text{global}}}$ where $d_{global}=35$. The features are motivated by the psycholinguistic literature in general, and work on differences between first and repeated reading in particular. See \Cref{app:trial_level_features} for further details on the trial-level features and training procedure.

\begin{itemize}[leftmargin=*]
 
 \item \textbf{Standard Eye Movement Measures} 8 standard eye movement measures from the psycholinguistic literature, including per word averages of Total Fixation Duration, First Fixation Duration, Gaze Duration, number of Fixations, skip rate and regression rate. See \Cref{app:word_fix_level_features} for additional features and definitions. These features were previously used for prediction tasks from eye movements \citep[e.g.][]{meziere2023using} and were shown to differ between first and repeated reading (see \Cref{sec:related-work}). 
      
 \item \textbf{Word Property Coefficients} 20 features that measure the responsiveness of reading measures to linguistic word properties: frequency, surprisal and length. Building on \citet{berzak_assessing_2018}, the features are coefficients from linear models that predict the participant's speed-normalized eye movement measures from these three word properties. This feature-set is motivated by prior work that has demonstrated that the responsiveness of eye movements to linguistic word properties varies across reading scenarios and readers \citep[e.g.][]{reichle_eye_2010,berzak2023traces,shubi2023}. In repeated reading, this responsiveness is weaker compared to first reading %, even when normalizing for the participant's reading speed 
 \cite{meiri2024deja}. See \Cref{app:response_to_ling} for further details on the models.
 
 \item \textbf{Saccade Network Measures}\label{sec:feature_based_methods} 
 Following \citet{zhu2015exploratory}, we define a directed graph that encodes the scanpath of eye movements over the paragraph $G=\{V,T\}$ such that $V$ is the set of words in the paragraph, and for all $u,v\in V$:
 $$T=\{\left(u,v\right): \text{there is a transition from }u \text{ to }v\}$$ We extract 7 features which capture connectivity, centrality and clustering measures of this graph.
 Additional details and definitions of the measures, along with network visualization examples are provided in \Cref{app:network_measures}.

\end{itemize}

\subsubsection*{Neural Models}

We use two variants of the \textbf{RoBERTEye} multimodal language model  \citep{Shubi2024Finegrained}, which is a state-of-the-art approach in predictive modeling using eye movements in reading. This model was previously applied to the prediction of reading comprehension \citep{Shubi2024Finegrained} and reading goals \citep{shubi2024decoding} from eye movements, outperforming in most cases prior models from the literature.
RoBERTEye extends the RoBERTa model \citep{liu_roberta_2019} by incorporating eye movement information. It does so by projecting an input eye movement feature vector for each word/fixation into the embedding space of the language model and then concatenating these projections with the word embedding sequence. %(left to the paragraph initial word embeddings) 

The model has two variants, with \textbf{word-level} and \textbf{fixation-level} eye movement representations. In \textbf{RoBERTEye-Words} 
the eye movements input consists of \( \left(e_S^{\text{word}_j} \right)_{j=1}^{N_{\text{words}}} \) where each \(e_S^{\text{word}_j} \in \mathbb{R}^{d_{\text{word}}} \) is an eye movement feature vector for the word $j$, with $d_{word}=13$ features. 

In \textbf{RoBERTEye-Fixations}, both fixation-level and word-level features are used. Each fixation $i$ on word $j$ has a fixation vector \( e_S^{\text{fix}_{i,j}} \in \mathbb{R}^{d_{\text{fix}}} \) with $d_{fix}=6$ features of the fixation. 
This vector is concatenated with the word-level feature vector \( e_{S}^{\text{word}_j} \):
\[
\left( e_S^{\text{fix}_{i,j}} \oplus e_S^{\text{word}_{j}} \right)_{i=1}^{N_{\text{fixations}}}
\]
where \( \oplus \) denotes the concatenation operation along the feature dimension. To help the model distinguish between eye movement and textual information, two special token vectors are added, one to all the text embeddings, and the other to all the projected eye movement embeddings. 
Complete lists for both fixation-level and word-level features are provided in \Cref{app:word_fix_level_features}.


\subsection{Single Trial Modeling with Synthetic Scanpath References}

%onsider a relaxed variant of the single-trial prediction task, where models have prior exposure to eye movement dynamics over text. This exposure can be achieved through pretraining on an external but related task, such as next fixation prediction or word-level reading time estimation. In this scenario, detecting the effect of repeated reading reduces to identifying deviations from the model’s expectations of a "normal-state" first reading of the text. This two-hop structure may enhance performance, as current models implicitly need to learn both reading dynamics and their modulation by repetition within a single model. A complementary approach to informing models about expected reading behavior is to provide an explicit reference of "normal-state" eye movements for each text.

We introduce a new method, where in addition to the human eye movement data, the model input further includes a \emph{synthetic scanpath} reference \(E_M^{W,1}\) of eye movements  \(E\) generated for each text $W$ from an external model \(M\) for scanpath generation. %The generated scanpath can then serve as a \emph{reference} for a first or a second reading. 
As all existing computational models for scanpath generation assume a first reading, in this work we focus on the generation of first reading reference scanpaths. In essence, this reference provides an external source of information on how a typical first reading should looke like. This addition of the reference yields a task whose input structure resembles the paired trials task, only that one of the eye movement inputs is now machine generated:
\[
(W,\; E_M^{W,1} \; E_S^{W,r}) \;\longrightarrow\; \hat{r}.
\]
%We denote the synthetic eye movements input as:
%\[ E_M^{W,1} =\left( \left( e_M^{\text{word}_j} \right)_{j=1}^{N_{words}} , \left(e_M^{\text{fix}_i} \right)_{i=1}^{N_{fix}}\right) \] 
%And add this input to the single trial task variant:
%\[
%(W,\; E_M^{W,1} \; E_S^{W,r}) \;\longrightarrow\; \hat{r}.
%\]
%The aggregation of eye movement representations of the two eye movement inputs (human and synthetic) is dependent on the aggregation level (word / fixation) and model type.
We then obtain the following three types of representations of eye movements:
\paragraph{Feature-based representations} The representation is a concatenation of the synthetic features, and their difference from the human features. Formally,
\[
 e_S^{\text{global}} \oplus (e_M^{\text{global}} -e_S^{\text{global}})
\]
\paragraph{Word-level representations} 
For each word, we concatenate its machine generated features with their difference from the human features. Formally,
\[
 \left( e_S^{\text{word}_j} \oplus (e_M^{\text{word}_j} -e_S^{\text{word}_j})\right)_{j=1}^{N_{words}}
\]

\paragraph{Fixation-level representations} Unlike the global and word-level representations, fixation-level features are not aligned. We therefore construct the input as follows
\[
\left( e_S^{\text{fix}_{i,j}} \oplus e_S^{\text{word}_{j}} \right)_{i=1}^{N_{\text{fixations}}} \mathbin\Vert \left( e_M^{\text{fix}_{i,j}} \oplus e_M^{\text{word}_{j}} \right)_{i=1}^{\tilde{N}_{\text{fixations}}}
\]
where \(\mathbin\Vert\) denotes concatenation along the sequence dimension, and \(\tilde{N}_{\text{fixations}}\) is the length of the scanpath generated by \(M\).
To help RoBERTEye distinguish between human and synthetic scanpath features, in addition to the word and human eye movement special tokens, we introduce a third token that marks machine generated scanpaths.

\paragraph{\mbox{E-Z} Reader Scanpaths} The synthetic scanpaths are generated using \mbox{E-Z} Reader \cite{reichle1998toward,reichle2003ez,reichle2009using,veldre2023understanding}, a prominent computational cognitive model for eye movements generation. The full details of the generation process and the adaptations made to the original model are discussed in \Cref{app:synthesis_details}. As we expect the effectiveness of the augmentation approach to depend on the quality of the generated scanpaths, we perform an evaluation of \mbox{E-Z} Reader outputs in the context of our task. %Some of the factors that can hinder performance is that \mbox{E-Z} Reader was developed for single sentences while we use full passages, and that the model parameters are based on other corpora. 
To this end, we compare \mbox{E-Z} Reader outputs with human eye movements in both first and repeated reading. Our analysis examines the overall similarity of the scanpaths, which we expect to be greater in first reading, as well as the direction of the deviations.
A necessary condition for \mbox{E-Z} Reader outputs to be effective as approximations of first reading behavior, is that on average, they should be more similar to human first reading than to human repeated reading.  

\Cref{tab:gen-quality} suggests that this indeed tends to be the case. It presents four measures for which robust differences between first and repeated reading were previously observed \citep{meiri2024deja}. Using mixed-effects models with text-level bootstrapping (see \Cref{app:sec:synth-data-analysis}), we compared the absolute differences of  \mbox{E-Z} Reader from first and repeated reading.
For Fixation Count and Skip Rate, \mbox{E-Z} Reader is significantly closer to first reading ($p<0.001$), whereas Regression Rate is significantly closer to repeated reading ($p<0.001$).
Although mean Total Fixation Duration (TF) does not show a significant difference ($p\approx0.53$), the direction of the difference remains useful for classification. Overall, these findings support the viability of \mbox{E-Z} Reader as an approximation of human first reading scanpath trajectories.

%Note that the simulated eye movements are not perfect replications of real scanpaths. Possible reasons include that \mbox{E-Z} is designed for single sentence generation. Here we have multiple sentences and multiple lines. Further, future work can fit onestop-specific parameters or even reader-sepcific parameters instead of average reader with default parameters.
% Define a new column type that aligns on the \pm symbol.
\begin{table}[ht]
\small
\resizebox{\columnwidth}{!}{%
    \setlength{\tabcolsep}{2pt}%
    \centering
    \begin{tabular}{
        l
        c
        c
    }
        \toprule
        \makecell[l]{\textbf{Measure}} & \makecell{\textbf{\textcolor[HTML]{347BB9}{First Reading}}} & \makecell{\textbf{\textcolor[HTML]{AE0135}{Repeated Reading}}} \\
        \midrule
        \makecell[l]{Fixation Count} & $0.03_{\pm0.01}$ & $-0.31_{\pm0.01}$ \\
        \addlinespace
        \makecell[l]{Mean TF (ms)} & $27_{\pm2.3}$  & $-29_{\pm1.7}$ \\
        \addlinespace
        \makecell[l]{Regression Rate} & $0.2_{\pm0.003}$ & $0.1_{\pm0.002}$ \\
        \addlinespace
        \makecell[l]{Skip Rate} & $0.2_{\pm0.003}$ & $0.3_{\pm0.003}$ \\
        \bottomrule
    \end{tabular}%
}
\caption{Trial-level mean differences between human and E-Z Reader-generated measures for four standard eye movement metrics, with 95\% confidence intervals.}

\label{tab:gen-quality}
\end{table}


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/eval_reread_stratified.pdf}
    \caption{Visualization of a 10-article, 60-participant data split, divided into train, validation, and three test regimes.  
    % for a single combination of article batch and reading regime 
    Each non-empty cell represents a participant-article pair, comprising the first and repeated readings of an article by the same participant. '{\scriptsize \protect\divorced}' denotes consecutive repeated reading and '{\scriptsize \protect\unmarried}' denotes non-consecutive repeated reading (i.e. with intervening articles between the first and second readings).}
    \label{fig:CV}
\end{figure*}

\mbox{E-Z} Reader is a probabilistic model that samples scanpaths for a given text. We generate 1000 synthetic scanpaths for each paragraph, and then augment the human eye movement data with reference features derived from these first reading simulated scanpaths. To obtain global and word-level representations, we first average measures across all the generated scanpaths. %This contrasts with the human data, where word-level measures are derived from a single scanpath. 
The averaging aims to enhance the robustness of the representations by reducing noise inherent to a single scanpath. 
%Both word- and fixation-level feature-sets are the same as those used for human eye movement data, as detailed in \Cref{app:word_fix_level_features}.
For the fixation-level representation, averaging scanpaths is not applicable, and we therefore follow  \citet{meziere2024scanpath} in selecting a \emph{prototype scanpath} that minimizes the mean scanpath distance to all other scanpaths, using the Scasim scanpath similarity metric \cite{von2011scanpath}. % This ensures that the chosen scanpath is representative of the distribution of generated scanpaths. 
\subsection{Paired Trials Modeling}

In the paired trials task, we use the same feature representation as in the single trial task with machine generated scanpaths described above, only that now both eye movement samples are from a human participant, and the third special token marks the second human input. Further, differently from the single-augmented setting, the order of the two inputs is randomized, and the output of the model is the probability of the repeated reading trial being second in order.

\subsection{Baselines}
\label{sec:baselines}
\begin{itemize}[leftmargin=*]
    \item \textbf{Majority Class}: the most frequent class in the training set. As our data is balanced, this baseline is equivalent to a random choice.
    \item \textbf{Reading Speed}: the number of words read per second. Note that when the text is available, this measure can be calculated from the total reading time of the trial, and therefore does not require eye tracking. Prior work has consistently shown that reading is faster in repeated reading compared to first reading (see \Cref{sec:related-work}). We therefore expect this to be a strong baseline, which crucially enables determining the added value of eye tracking information for our decoding tasks.
\end{itemize}

\section{Experimental Setup}



\input{main_res_table.tex}

\subsubsection*{Evaluation Regimes}

We use 10-fold cross validation with three evaluation regimes:
\begin{itemize}[leftmargin=*]
 \item \textbf{New Participant} eye movement data is available for the given paragraph, but no prior data was collected for the participant.
 \item \textbf{New Item} prior eve movement data is available for the participant, but not for the paragraph.
 \item \textbf{New Participant and Item} no prior data is available for the participant nor for the paragraph.
 \item \textbf{All} the union of the above three regimes.
\end{itemize}

\subsubsection*{Data Splits}
To allow complete matching of participants across the first and repeated reading of each article, out of the 10 articles read by each participant during first reading, we use only the 2 articles that were read twice. % The remaining data in the training set is used for ?. 
Further, we leverage the counterbalancing properties of OneStop to obtain data splits that fulfill the following properties: 1)  the three test regimes are balanced in number of participants 2)  the three validation regimes are balanced to the extent possible in number of participants 3) there is an equal number of consecutive and non-consecutive repeated readings in each portion of the split. 

We define a constrained combinatorial problem that has an algorithmic solution that satisfies these constraints. We provide further details on the solution in \Cref{app:CV}. All resulting splits satisfy that the training set has 264 participant-article pairs, the validation set has 48 pairs, and the test set has 54 pairs, where each test regime has exactly 18 pairs, all balanced with respect to consecutive and non-consecutive repeated reading. In \Cref{fig:CV} we present an example of one split.


\subsubsection*{Model Training and Selection}
We perform hyperparameter optimization and model selection separately for each split. We assume that at test time, the evaluation regime of the trial is \emph{unknown}. Model selection is therefore based on the entire validation set of the split. 
All neural network-based models were trained using the PyTorch Lighting \hbox{\citep{Falcon_PyTorch_Lightning_2019}} library on L40S-48GB GPUs. Further details on the training procedure, including the full hyperparameter search space for all models are provided in \Cref{app:hardware-software} and \Cref{app:hyperparams}.

% ----------------------------------------------------------------------------


\section{Results}
\label{sec:results}
Below, we summarize our main experimental findings for both the paired and single-trial variants of the task. \Cref{tab:main_res_table} presents the quantitative results.

\paragraph{Single Trial} %In the more challenging single trial task, both Accuracy and Recall are notably lower for the baseline and across all the models.
%This difference is expected since the single-trial variant removes the advantage of an explicit same-participant, same-paragraph pairing. 
In this task all models outperform the reading speed baseline, demonstrating the added value of eye movement information.
The highest All Accuracy of $70.2$ is achieved with the XGBoost model augmented with \mbox{E-Z} Reader Scanpaths. However, it does not outperform the other models statistically, and different models come first on different evaluations, again, with no statistically significant differences from the other models. Within each model, performance is relatively stable across the three evaluation regimes. %This is encouraging as in other prediction tasks from eye movements, generalization to new items is often especially challenging.
%We hypothesize that this stems from individual differences in the magnitude of the facilitation effect caused by repeated reading. We address this hypothesis in the error analysis in \Cref{sec:error-analysis}.

\paragraph{The Effects of Synthetic References} 

With one exception, the best performing model in each evaluation regime includes a synthetic scanpath augmentation. However, the gains over the non-augmented model counterparts are not statistically significant and not consistent within each model. %Moreover, the gap between Accuracy and Recall remains when synthetic scanpaths are included, indicating that they do not substantially mitigate the underlying issue of high false negatives shared across models.

\paragraph{Paired Trials} In the paired setup, the model’s output is an ordering of two trials. To make the evaluation of these predictions comparable to the single-trial task, we “unaggregate” the model’s predictions so that predicting the correct order counts as two correct single-trial classifications (and vice versa for an incorrect prediction).
%Partial correctness— is thus disallowed.
The reading speed baseline achieves a high All Accuracy of $87.7$. While the neural models exhibit baseline-level results, XGBoost substantially outperforms the baseline and the neural models in all the evaluation regimes, reaching an overall Accuracy of $91.4$. Overall, the feature-based method tends to yield stronger results than the neural methods.
%We note that Recall and Accuracy remain similar across the different evaluation regimes (\textit{New Participant}, \textit{New Item}, and \textit{New Participant and Item}), indicating robustness to participant- or text-specific variation. 
%These high scores highlight the benefit of having explicit pairings of two eye movement recordings for the same participant and paragraph.

In \Cref{app:results} we present complementary evaluation measures such as Precision, Recall and F1 for both the validation and test partitions.

% \begin{table}[ht]
% \small
%     \centering
%     \begin{tabular}{llcccc}
%         \toprule
%              \textbf{Measure} & \makecell{\textbf{First}\\ \textbf{Reading}} & \makecell{\textbf{Generated} \\\textbf{FR}} & \makecell{\textbf{Repeated}\\\textbf{Reading}} \\
%         \midrule
%         \makecell{Scanpath Length\\ Diff}    
%         & $0.0_{\pm 55.7}$  & $3.0_{\pm 39.3}$  & $-85.7_{\pm 53.5}$  \\
%         \addlinespace
%         \makecell{Mean TF\\ Diff (sec)}    
%         & $-0.0_{\pm 12.7}$  & $-2.9_{\pm 8.9}$  & $-17.5_{\pm 11.0}$  \\
%         \addlinespace
%         \makecell{Regression \\Rate Diff}    
%         & $0.0_{\pm 0.2}$  & $0.2_{\pm 0.1}$  & $-0.1_{\pm 0.1}$  \\
%                 \addlinespace
%         \makecell{Skip \\Rate Diff}    
%         & $0.0_{\pm 0.2}$  & $0.2_{\pm 0.1}$  & $-0.1_{\pm 0.1}$  \\
%         \bottomrule
%     \end{tabular}
    
% \end{table}

\section{Fine-Grained Analysis of Model Performance}
\label{sec:error-analysis}

The controlled experimental design of OneStop enables going beyond aggregated performance measures and understanding model behavior as a function of trial characteristics. Prior work with OneStop observed that individual eye movement measures in first and repeated reading vary systematically across different item and participant characteristics \citep{meiri2024deja}. Here, we analyze this variability from the perspective of the classification performance of models that integrate multiple eye movement features. This allows for a fine-grained characterization of model behavior with respect to different data characteristics, and further leverages the models as an analytic tool for inspecting the data itself. %In this analysis, we explore various factors that modulate both the model's probability assignments and its performance (i.e., the probability of correct classification).

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/post_analysis_article_idx.pdf}
    \caption{Analysis of the \mbox{E-Z} Reader augmented XGBoost model's behavior as a function of item position. Depicted are probability assignment (top) and classification accuracy (bottom) with 95\% confidence intervals. (a) First and repeated reading (RR) as a function of article position in the experiment. (b) Repeated reading as a function of the article position in the first reading. See \Cref{fig:exp-schema} for the experiment structure.}
    \label{fig:post-analysis-article-idx}
\end{figure}

Specifically, we focus on the more challenging single trial task, and analyze the assigned probabilities and prediction accuracy of the best performing model, XGBoost augmented with \mbox{E-Z} Reader scanpaths. 
% Starting with \emph{item} characteristics, 
\Cref{fig:post-analysis-article-idx} presents the mean probability assigned to trials for being a repeated reading trial (top) and the mean probability of classifying trials correctly (bottom) as a function of the position of the article in the experiment, see \Cref{fig:exp-schema}.


 %Means are comprised of probabilities from all folds, and from all evaluation regimes. 
%Confidence intervals are computed using a linear mixed model with random effects w.r.t participant and textual items as random groups. 

In \textbf{first reading} trials (blue) in \Cref{fig:post-analysis-article-idx} (a), as the experiment progresses, the model exhibits decreasing confidence in classifying trials as first reading ($p<10^{-8}$) and goes down in prediction accuracy ($p\approx0.001$).
This outcome mirrors a decrease in reading times during first reading as the experiment progresses observed in \citet{meiri2024deja}. When adding reading speed as a predictor in models that predict the probability assignments for repeated reading and accuracy\footnote{In R notation: $outcome \sim fixed\_terms + (fixed\_terms | subject) + (fixed\_terms | paragraph)$ where $outcome \in \{P(RR=1), P(Correct)\}$, and $fixed\_terms=position+reading\_speed$}, we find that for both the effect of position is no longer significant ($p\approx0.7$ for probabilities and $p\approx0.5$ for accuracy). This suggests that the model heavily relies on reading speed or correlates thereof. Consequently, as reading speed increases during the experimental session and comes closer to the reading speed in repeated reading, the model becomes worse at correctly classifying first reading items. 

%the effect of article position on the model's confidence remains significant ($p<10^{-6}$) %, as well as the interaction effect ($p<10^{-6}$). Yet, the effect on the model's accuracy becomes non significant  ($p\approx0.27$), as well as the interaction term ($p\approx0.45$). %Note that the co-linearity between article location and reading speed is moderate ($R=0.172$), thus the assumptions of the linear model are maintained after adding reading speed.

In \textbf{repeated reading} trials in \Cref{fig:post-analysis-article-idx} (a), the model assigns higher probabilities to consecutive repeated readings compared to non-consecutive ones ($p\approx0.02$). Accuracy is also numerically lower compared to consecutive repeated readings, but the difference is not significant  ($p\approx0.16)$. These outcomes are again in line with the analysis of \citet{meiri2024deja} who found lower reading times and less skipping in non-consecutive versus consecutive reading. Taken together with these results, our analysis strengthens \citet{meiri2024deja} interpretation that reading facilitation is greater in consecutive reading potentially due to better memory retention of the first reading. 

In \Cref{fig:post-analysis-article-idx} (b) we examine repeated reading item probabilities and accuracy as a function of the position of the first reading. Reflecting again reading speed and other single measure analyses in \citet{meiri2024deja}, we find no evidence for a better model's classification accuracy with fewer articles between the two readings ($p\approx0.16$). However, we do find an effect in model probabilities for repeated reading which increases with article position ($p\approx0.03$), hinting at favorable effect of the recency of the first reading. This suggests that model analysis can unveil finer grained patterns in the data than with traditional univariate analyses.

%is more confident when the first reading of the article occurred earlier in the experiment ($p\approx0.03$), but this trend is not reflected in classification accuracy . 


\section{Conclusion and Discussion}
\label{sec:discussion}

Our study presents the first attempt to decode the number of prior interactions of a reader with a text from their eye movements. We demonstrate that it is feasible to perform this task, with various degrees of success depending on the difficulty of the task variant. In addition, we propose and experiment with a general method for leveraging synthetic ordinary reading data to improve predictive modeling of non-ordinary reading. Overall, the results indicate that there is highly informative signal for the presence or absence of a prior text interaction in eye movements at the level of a single paragraph and reader. This signal tends to be better captured by feature-based models than by neural language models. This outcome provides further evidence for the informativeness of eye movements regarding the reader's cognitive state during online language processing. It also opens the door for practical, user facing applications in areas such as education and online content delivery, where inferences about the current interaction of the reader with the text can be used for customizing and personalizing upcoming textual content to best support user goals.

\section{Ethical Considerations}

The eyetracking data used in our experiments was collected under an institutional IRB protocol \cite{onestop2025preprint}. All the participants provided written consent prior to taking part in the eyetracking experiment and received monetary compensation for their participation. The dataset is anonymized. Analyses and modeling of eye movements in repeated reading are among the main use cases for which the data was collected.

Decoding of repeated reading can be valuable in applications for monitoring reading acquisition, learning progress and effectiveness of retaining learned material. It can further facilitate special assistance to individuals and populations that struggle with reading comprehension, which can potentially be diagnosed via repeated reading. However, such technologies also pose a potential for inaccurate predictions and biases that may put various individuals and populations at a disadvantage. These include L2 learners, participants with cognitive impairments, participants with eye conditions and others. Additional data collection, modeling and analysis work for these groups is required before considering the deployment of such technology.

Finally, it is important to consider the issues of privacy and consent in the scope of eyetracking technologies. It was previously shown that eye movements contain information that can be used for user identification \cite[e.g.][]{bednarik2005eye,jager2020deep}. We do not perform user identification in this study, and point out the importance of not storing information that could enable participant identification in future studies on repeated reading and other reading regimes. We further stress that future systems that perform prediction of repeated reading are to be used only with explicit consent from potential users to have their eye movements collected and analyzed for this purpose. 


\section{Limitations}
\label{sec:limitations}

Our work has a number of limitations that are related to the experimental design and the eyetracking data. Consecutive repeated reading occurs at the level of a full article, such that there are minimally 3 intervening paragraphs between two readings of the same paragraph. This setup does not address immediate repeated reading that involves working memory. The maximal amount of intervening material between two readings of the same article is 10 articles, leaving out larger time intervals between the readings. The experiment is also restricted to two readings of any given text, while in daily life the same text can be read more than twice. The underlying texts are all newswire articles, and while they include a wide range of topics, other textual domains are not covered. We intend to collect data and investigate both shorter and longer repetition intervals, multiple repeated readings and additional textual domains in future work.

An additional limitation is the experimental procedure, where reading occurs in-lab, and the presence of a reading comprehension question after each paragraph. Both aspects can negatively affect the ecological validity of the data and lead to reading behavior that is not fully representative of everyday life. Relatedly, while we use the term ordinary reading to refer to reading for general text comprehension, we acknowledge that this term, and similar terms such as ``normal reading'', are not without faults \cite{huettig2022myth}. In information seeking, while question answering is a general framework for formulating information seeking tasks, the experiment captures only a limited range of tasks that are restricted to a single paragraph and constrained by the annotation structure of STARC. Furthermore, readers who forgot the task cannot return to it during paragraph reading. Future work with different tasks, amounts of text, and experimental setups is needed to address these limitations.

Additional limitations concern the participants, the experiment language and the equipment used. Although OneStop \citep{onestop2025preprint} is the first public dataset that enables studying repeated reading, it is restricted to adult L1 speakers, with no cognitive impairments, and mostly with no eye conditions. This pool of participants does not cover multiple populations, including L2 speakers, children, elderly, participants with cognitive and physical impairments and others. %Future data collection and analysis work is required to test the generalization capabilities and potential biases of the models in other populations. 
Moreover, the eyetracking data and modeling work is restricted to English. %To the best of our knowledge, no eyetracking data with repeated reading is currently available for languages other than English. 
These factors limit the scope and the generality of the results. Both data collection and model development work is required to include additional languages and populations. Finally, our approach has currently only been tested using a state-of-the-art eyetracker (Eyelink 1000 Plus) at a sampling rate of 1000Hz. This eye tracker allows extracting gaze position and duration at a very high temporal resolution and character-level precision. %While studies such as  \citet{ishimaru_towards_2017} and \citet{chen_characteristics_2023} have demonstrated predictive modeling capabilities from gaze using lower spatial and temporal resolution eye tracking systems, 
Such equipment is generally not available for end users, limiting the application potential of the current work. The feasibility of using lower spatial and temporal resolution eye tracking systems, as well as standard front-facing cameras on devices such as laptops, tablets and phones should be examined in future work.

\bibliography{custom}

\clearpage

\input{appendix.tex}

\end{document}

