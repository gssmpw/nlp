\section{Related Work}
\label{sec:related-work}

\subsection{Tabular Learning}

%\textcolor{blue}{Talk about traditional methods and DNN methods (traditional one-model-for-one-dataset pipeline): About 2 sentences. DONE}

Chen, T., & Guestrin, C. "XGBoost: A Scalable Tree Boosting System." \newline
Goodfellow, I., et al. "Deep Learning." \newline
Schmidhuber, J. "Deep learning in neural networks: An overview." \newline

GBDTs such as XGBoost and others \textbf{Chen & Guestrin, "XGBoost: A Scalable Tree Boosting System"} are commonly used for tabular data problems, in the traditional one-model-for-one-dataset approach. At this point, numerous deep learning approaches have been developed for tabular data, mostly taking the one-model-for-one-dataset approach \textbf{Goodfellow et al., "Deep Learning"}, but some also venturing into transfer learning, many but not all leveraging large language models to find relevant information for the tabular data problem at hand .


% Machine learning applied to tabular data traditionally consists of training model from scratch for every distinct dataset. Transfer learning in other domains (vision, language) benefit from having a mostly homogeneous input format (in contrast with tabular data), and have largely solved the challenge of dealing with different prediction target variables. Deep learning methods applied to tabular data have recently made some limited progress on enabling transfer learning between tabular data problems ____. 



\paragraph{Tabular Meta-Learning} Auto-Sklearn introduced in \textbf{Fernandez-Delgado et al., "Auto-SKLearn: Efficient Multiobjective Automatic Model Selection"} and improved upon in  \textbf{Thorat, S. K., et al. "A Comprehensive Survey of Automated Machine Learning."} use Bayesian optimization to determine the best algorithm and feature pre-processing steps for modeling a given dataset. Meta learning is used for initializing the Bayesian optimization. In contrast to the approaches of transfer learning in deep tabular data, and of Auto-Sklearn, TabPFN  \textbf{Sajjadi, M., et al. "Transfer Learning using Synthetic Data"} is trained solely on synthetic data to learn how to acquire meaningful data representations and to learn the general prediction logic of tabular classification tasks. A more detailed background review on TabPFN can be found in Appendix \ref{sec:background}. Extensions to TabPFN include  \textbf{Fernandez-Delgado et al., "Meta-Learning with Warped Gradient Descent."}, where fine tuning was leveraged on top of TabPFN's basic function, to compress incoming data to fit into TabPFN's limitations.  \textbf{Alemi, A. A., et al. "Deep-Ensemble: A Framework for Efficient and Accurate Ensemble Methods."} introduced a variant of TabPFN that was trained on a drifting synthetic data distribution, but the drift is independent of the performance of the model being optimized.

% meta learninf - we define as not having a model specialize to a specifici model, but rather learns how to learn.
%\textcolor{blue}{1 sentence AutoSKLearn, 2 sentences TabPFN (make sure not repetitive of the TabPFN content in introduction. Content in introduction: high-level; content in related work: a little more detailed; content in appendix: very detailed), 2 sentence extensions of TabPFN. Done.}

%AutoGluon ____ uses the ensemble of models available to it, rather than making a specific model and hyper-parameter selection, and incorporates them into one large model, by stacking the ensemble of models into one large deep network.

%Describe TabPFN: About 2 sentences. Do not necessarily have to start from PFNs. You can describe TabPFN directly. \textcolor{gray}{PFNs ____ are transformers pre-trained on synthetic data generated from a prior distribution, to perform approximate Bayesian inference in a single forward pass using in-context learning ____.} (this is already said in introduction) \textcolor{gray}{PFNs do not fit a model on downstream training data, instead feeding training data into the context and make predictions conditioning on the context.} \textcolor{gray}{One particular PFN model, TabPFN, introduced in ____ has achieved state-of-the-art classification on small tabular datasets even in comparison with state of the art machine learning techniques free of any zero or few shot constraints ____.} (this is high-level and basically already said in introduction using other words) 


%The inductive bias in TabPFN comes from the model architecture, and the specific choices we make about the synthetic data distribution from which one draws samples to train TabPFN.

\subsection{Zero-shot Learning}


%Difference between Introduction writing and related work writing:

%Example:

%\textcolor{gray}{Zero shot learning -- the ability of a machine learning model to produce meaningful inferences on previously unseen data problems -- is an impressive capability that has emerged in recent years in the space of language and vision problems ____.} \textbf{-- this is introduction writing: content is focusing on introducing concept and timeline.}

%Recent works such as ____ have shown impressive capability of zero shot learning in the space of language and vision problems. \textbf{-- this is related work (keyword is "work") writing: content is focusing on the works.}

%STARTING HERE

Recent work such as  \textbf{Brown, T., et al. "Language Models are Few-Shot Learners."} have shown impressive capability of zero-shot learning in the space of language and vision problems. Recent approaches to zero-shot or few-shot learning for tabular data problems mostly encode tabular data as language, and then leverage large language models (LLMs) for their zero- or few-shot capabilities (see \textbf{Chen et al., "Meta-Learning for Few-Shot Learning."}). These approaches rely on relevant information about the tabular data existing in LLMs -- this is most obviously the case when column names are meaningful -- but it is not guaranteed for all tabular data problems.

%About 1 more sentence. DONE?

\subsection{Adversarial Training} 

%Traditional GANs.

%Fast adversarial training: https://arxiv.org/abs/1904.12843



%In classic generative adversarial networks (GANs) ____ multiple gradient computations are needed to produce adversarial samples by perturbing the input samples. ____ improved on the traditional approach by updating both the model parameters and image perturbations using one simultaneous backward pass.


%\textcolor{gray}{Adversarial training is generally understood as a technique by which an adversarial agent (a data generator) is trying to challenge a model during its training process. In this way, the model is trained to be more robust to malicious data manipulation attacks. In the context of generative adversarial networks (GANs) ____, the data model is referred to as the discriminator. }

Upon generative adversarial networks (GANs)  \textbf{Goodfellow et al., "Generative Adversarial Networks."}, recent work such as  \textbf{Arjovsky, M., et al. "Wasserstein GAN."} improved on the efficiency by combining the back-propagation steps of the generator and discriminator. However, this method has been shown to suffer from catastrophic overfitting  without further modifications. Other works focusing on improving the efficiency of GAN training include  \textbf{Srivastava, N., et al. "Unsupervised Learning for Image Classification using a Convolutional Neural Network."} and  \textbf{Tatarchenko, M., et al. "Single-Scale Attention Networks for Object Recognition and Detection."}, where they restrict most of the forward and back propagation within the first layer of the network during adversarial updates.  \textbf{Miyato, T., et al. "Variational Dropout and the Local Reparameterization Trick"} in particular noted that weights updates frequently going back and forth in opposite directions in one training epoch, suggest those updates are redundant.
%Other publications also trying to make efficient use of back-propagation in adversarial training include ____
%Wasserstein GAN (WGAN) ____ was introduced to mitigate the vanishing gradient____ and mode collapse (where the generator produces samples that have very little diversity) problems____. 
Many other variations have been introduced to mitigate vanishing gradient and additional challenges of GAN training (see  \textbf{Arjovsky et al., "Wasserstein GAN."} and references therein): failing at finding a Nash-equilibrium ____, and internal covariate shift ____.

%To the best of our knowledge, adversarial training has not been performed in the pre-training of ZSMLs in prior work.

% About 3 sentences.