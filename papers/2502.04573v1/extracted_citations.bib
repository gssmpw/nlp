@inproceedings{10.1145/3637528.3671893,
author = {Chen, Jintai and Yan, Jiahuan and Chen, Qiyuan and Chen, Danny Z. and Wu, Jian and Sun, Jimeng},
title = {Can a Deep Learning Model be a Sure Bet for Tabular Prediction?},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671893},
doi = {10.1145/3637528.3671893},
abstract = {Data organized in tabular format is ubiquitous in real-world applications, and users often craft tables with biased feature definitions and flexibly set prediction targets of their interests. Thus, a rapid development of a robust, effective, dataset-versatile, user-friendly tabular prediction approach is highly desired. While Gradient Boosting Decision Trees (GBDTs) and existing deep neural networks (DNNs) have been extensively utilized by professional users, they present several challenges for casual users, particularly: (i) the dilemma of model selection due to their different dataset preferences, and (ii) the need for heavy hyperparameter searching, failing which their performances are deemed inadequate. In this paper, we delve into this question: Can we develop a deep learning model that serves as a sure bet solution for a wide range of tabular prediction tasks, while also being user-friendly for casual users? We delve into three key drawbacks of deep tabular models, encompassing: (P1) lack of rotational variance property, (P2) large data demand, and (P3) over-smooth solution. We propose ExcelFormer, addressing these challenges through a semi-permeable attention module that effectively constrains the influence of less informative features to break the DNNs' rotational invariance property (for P1), data augmentation approaches tailored for tabular data (for P2), and attentive feedforward network to boost the model fitting capability (for P3). These designs collectively make ExcelFormer a sure bet solution for diverse tabular datasets. Extensive and stratified experiments conducted on real-world datasets demonstrate that our model outperforms previous approaches across diverse tabular data prediction tasks, and this framework can be friendly to casual users, offering ease of use without the heavy hyperparameter tuning. The codes are available at https://github.com/whatashot/excelformer.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {288â€“296},
numpages = {9},
keywords = {mixup, tabular data prediction},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{GoodfellowAdversarial,
title	= {Explaining and Harnessing Adversarial Examples},author	= {Ian Goodfellow and Jonathon Shlens and Christian Szegedy},year	= {2015},URL	= {http://arxiv.org/abs/1412.6572},booktitle	= {International Conference on Learning Representations}}

@inproceedings{NEURIPS2019_812b4ba2,
 author = {Zhang, Dinghuai and Zhang, Tianyuan and Lu, Yiping and Zhu, Zhanxing and Dong, Bin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf},
 volume = {32},
 year = {2019}
}

@InProceedings{Xian_2017_CVPR,
author = {Xian, Yongqin and Schiele, Bernt and Akata, Zeynep},
title = {Zero-Shot Learning - the Good, the Bad and the Ugly},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@article{agtabular,
  title={AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data},
  author={Erickson, Nick and Mueller, Jonas and Shirkov, Alexander and Zhang, Hang and Larroy, Pedro and Li, Mu and Smola, Alexander},
  journal={arXiv preprint arXiv:2003.06505},
  year={2020}
}

@article{andriushchenko2020understanding,
  title={Understanding and improving fast adversarial training},
  author={Andriushchenko, Maksym and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={16048--16059},
  year={2020}
}

@inproceedings{arik2021tabnet,
  title={Tabnet: Attentive interpretable tabular learning},
  author={Arik, Sercan {\"O} and Pfister, Tomas},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  pages={6679--6687},
  year={2021}
}

@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International conference on machine learning},
  pages={214--223},
  year={2017},
  organization={PMLR}
}

@inproceedings{arora2017generalization,
  title={Generalization and equilibrium in generative adversarial nets (gans)},
  author={Arora, Sanjeev and Ge, Rong and Liang, Yingyu and Ma, Tengyu and Zhang, Yi},
  booktitle={International conference on machine learning},
  pages={224--232},
  year={2017},
  organization={PMLR}
}

@article{borisov2022deep,
  title={Deep neural networks and tabular data: A survey},
  author={Borisov, Vadim and Leemann, Tobias and Se{\ss}ler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
  journal={IEEE transactions on neural networks and learning systems},
  year={2022},
  publisher={IEEE}
}

@inproceedings{chang2008importance,
  title={Importance of Semantic Representation: Dataless Classification.},
  author={Chang, Ming-Wei and Ratinov, Lev-Arie and Roth, Dan and Srikumar, Vivek},
  booktitle={Aaai},
  volume={2},
  pages={830--835},
  year={2008}
}

@inproceedings{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  pages={785--794},
  year={2016}
}

@inproceedings{feurer-neurips15a,
    title     = {Efficient and Robust Automated Machine Learning},
    author    = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
    booktitle = {Advances in Neural Information Processing Systems 28 (2015)},
    pages     = {2962--2970},
    year      = {2015}
}

@article{feurer2022auto,
  title={Auto-sklearn 2.0: Hands-free automl via meta-learning},
  author={Feurer, Matthias and Eggensperger, Katharina and Falkner, Stefan and Lindauer, Marius and Hutter, Frank},
  journal={Journal of Machine Learning Research},
  volume={23},
  pages={1--61},
  year={2022}
}

@misc{gardner2024largescaletransferlearning,
      title={Large Scale Transfer Learning for Tabular Data via Language Modeling}, 
      author={Josh Gardner and Juan C. Perdomo and Ludwig Schmidt},
      year={2024},
      eprint={2406.12031},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.12031}, 
}

@article{goodfellow2016nips,
  title={Nips 2016 tutorial: Generative adversarial networks},
  author={Goodfellow, Ian},
  journal={arXiv preprint arXiv:1701.00160},
  year={2016}
}

@article{gorishniy2021revisiting,
  title={Revisiting deep learning models for tabular data},
  author={Gorishniy, Yury and Rubachev, Ivan and Khrulkov, Valentin and Babenko, Artem},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18932--18943},
  year={2021}
}

@inproceedings{gorishniy2022embeddings,
    title={On Embeddings for Numerical Features in Tabular Deep Learning},
    author={Yury Gorishniy and Ivan Rubachev and Artem Babenko},
    booktitle={{NeurIPS}},
    year={2022},
}

@article{grinsztajn2022tree,
  title={Why do tree-based models still outperform deep learning on typical tabular data?},
  author={Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={507--520},
  year={2022}
}

@article{hollmann2022tabpfn,
  title={Tabpfn: A transformer that solves small tabular classification problems in a second},
  author={Hollmann, Noah and M{\"u}ller, Samuel and Eggensperger, Katharina and Hutter, Frank},
  journal={arXiv preprint arXiv:2207.01848},
  year={2022}
}

@misc{huang2020tabtransformertabulardatamodeling,
      title={TabTransformer: Tabular Data Modeling Using Contextual Embeddings}, 
      author={Xin Huang and Ashish Khetan and Milan Cvitkovic and Zohar Karnin},
      year={2020},
      eprint={2012.06678},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2012.06678}, 
}

@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}

@article{jabbar2021survey,
  title={A survey on generative adversarial networks: Variants, applications, and training},
  author={Jabbar, Abdul and Li, Xi and Omar, Bourahla},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={8},
  pages={1--49},
  year={2021},
  publisher={ACM New York, NY}
}

@article{kadra2021well,
  title={Well-tuned simple nets excel on tabular datasets},
  author={Kadra, Arlind and Lindauer, Marius and Hutter, Frank and Grabocka, Josif},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={23928--23941},
  year={2021}
}

@article{ke2017lightgbm,
  title={Lightgbm: A highly efficient gradient boosting decision tree},
  author={Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{kim2021understanding,
  title={Understanding catastrophic overfitting in single-step adversarial training},
  author={Kim, Hoki and Lee, Woojin and Lee, Jaewook},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={8119--8127},
  year={2021}
}

@inproceedings{larochelle2008zero,
  title={Zero-data learning of new tasks.},
  author={Larochelle, Hugo and Erhan, Dumitru and Bengio, Yoshua},
  booktitle={AAAI},
  volume={1},
  pages={3},
  year={2008}
}

@article{levin2022transfer,
  title={Transfer learning with deep tabular models},
  author={Levin, Roman and Cherepanova, Valeriia and Schwarzschild, Avi and Bansal, Arpit and Bruss, C Bayan and Goldstein, Tom and Wilson, Andrew Gordon and Goldblum, Micah},
  journal={arXiv preprint arXiv:2206.15306},
  year={2022}
}

@article{luo2018neural,
  title={Neural architecture optimization},
  author={Luo, Renqian and Tian, Fei and Qin, Tao and Chen, Enhong and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{mann2020language,
  title={Language models are few-shot learners},
  author={Mann, Ben and Ryder, N and Subbiah, M and Kaplan, J and Dhariwal, P and Neelakantan, A and Shyam, P and Sastry, G and Askell, A and Agarwal, S and others},
  journal={arXiv preprint arXiv:2005.14165},
  volume={1},
  year={2020}
}

@article{mcelfresh2024neural,
  title={When do neural nets outperform boosted trees on tabular data?},
  author={McElfresh, Duncan and Khandagale, Sujay and Valverde, Jonathan and Prasad C, Vishak and Ramakrishnan, Ganesh and Goldblum, Micah and White, Colin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{muller2021transformers,
  title={Transformers can do bayesian inference},
  author={M{\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
  journal={arXiv preprint arXiv:2112.10510},
  year={2021}
}

@InProceedings{nam2023stunt,
    title={{STUNT}: Few-shot Tabular Learning with Self-generated Tasks from Unlabeled Tables},
    author={Jaehyun Nam and Jihoon Tack and Kyungmin Lee and Hankook Lee and Jinwoo Shin},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=_xlsjehDvlY}
}

@article{palatucci2009zero,
  title={Zero-shot learning with semantic output codes},
  author={Palatucci, Mark and Pomerleau, Dean and Hinton, Geoffrey E and Mitchell, Tom M},
  journal={Advances in neural information processing systems},
  volume={22},
  year={2009}
}

@InProceedings{pmlr-v206-hegselmann23a,
  title = 	 {TabLLM: Few-shot Classification of Tabular Data with Large Language Models},
  author =       {Hegselmann, Stefan and Buendia, Alejandro and Lang, Hunter and Agrawal, Monica and Jiang, Xiaoyi and Sontag, David},
  booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {5549--5581},
  year = 	 {2023},
  editor = 	 {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
  volume = 	 {206},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--27 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v206/hegselmann23a/hegselmann23a.pdf},
  url = 	 {https://proceedings.mlr.press/v206/hegselmann23a.html}
}

@article{popov2019neural,
  title={Neural oblivious decision ensembles for deep learning on tabular data},
  author={Popov, Sergei and Morozov, Stanislav and Babenko, Artem},
  journal={arXiv preprint arXiv:1909.06312},
  year={2019}
}

@article{prokhorenkova2018catboost,
  title={CatBoost: unbiased boosting with categorical features},
  author={Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{ratliff2016characterization,
  title={On the characterization of local Nash equilibria in continuous games},
  author={Ratliff, Lillian J and Burden, Samuel A and Sastry, S Shankar},
  journal={IEEE transactions on automatic control},
  volume={61},
  number={8},
  pages={2301--2307},
  year={2016},
  publisher={IEEE}
}

@article{rubachev2022revisiting,
  title={Revisiting pretraining objectives for tabular deep learning},
  author={Rubachev, Ivan and Alekberov, Artem and Gorishniy, Yury and Babenko, Artem},
  journal={arXiv preprint arXiv:2207.03208},
  year={2022}
}

@article{shafahi2019adversarial,
  title={Adversarial training for free!},
  author={Shafahi, Ali and Najibi, Mahyar and Ghiasi, Mohammad Amin and Xu, Zheng and Dickerson, John and Studer, Christoph and Davis, Larry S and Taylor, Gavin and Goldstein, Tom},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{somepalli2021saint,
  title={Saint: Improved neural networks for tabular data via row attention and contrastive pre-training},
  author={Somepalli, Gowthami and Goldblum, Micah and Schwarzschild, Avi and Bruss, C Bayan and Goldstein, Tom},
  journal={arXiv preprint arXiv:2106.01342},
  year={2021}
}

@article{xian2018zero,
  title={Zero-shot learningâ€”a comprehensive evaluation of the good, the bad and the ugly},
  author={Xian, Yongqin and Lampert, Christoph H and Schiele, Bernt and Akata, Zeynep},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={9},
  pages={2251--2265},
  year={2018},
  publisher={IEEE}
}

@article{yuan2021meta,
  title={Meta adversarial perturbations},
  author={Yuan, Chia-Hung and Chen, Pin-Yu and Yu, Chia-Mu},
  journal={arXiv preprint arXiv:2111.10291},
  year={2021}
}

@inproceedings{zhang2021free,
  title={Free Adversarial Training with Layerwise Heuristic Learning},
  author={Zhang, Haitao and Shi, Yucheng and Dong, Benyu and Han, Yahong and Li, Yuanzhang and Kuang, Xiaohui},
  booktitle={International Conference on Image and Graphics},
  pages={120--131},
  year={2021},
  organization={Springer}
}

@article{zhu2023xtab,
  title={Xtab: Cross-table pretraining for tabular transformers},
  author={Zhu, Bingzhao and Shi, Xingjian and Erickson, Nick and Li, Mu and Karypis, George and Shoaran, Mahsa},
  journal={arXiv preprint arXiv:2305.06090},
  year={2023}
}

