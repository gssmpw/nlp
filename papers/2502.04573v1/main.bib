@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}


%--------------------------------------------------




@article{hollmann2022tabpfn,
  title={Tabpfn: A transformer that solves small tabular classification problems in a second},
  author={Hollmann, Noah and M{\"u}ller, Samuel and Eggensperger, Katharina and Hutter, Frank},
  journal={arXiv preprint arXiv:2207.01848},
  year={2022}
}

@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}

@article{maddison2016concrete,
  title={The concrete distribution: A continuous relaxation of discrete random variables},
  author={Maddison, Chris J and Mnih, Andriy and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1611.00712},
  year={2016}
}

@inproceedings{verma2020meta,
  title={Meta-learning for generalized zero-shot learning},
  author={Verma, Vinay Kumar and Brahma, Dhanajit and Rai, Piyush},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  pages={6062--6069},
  year={2020}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@article{DBLP:journals/corr/abs-2010-11929,
  author       = {Alexey Dosovitskiy and
                  Lucas Beyer and
                  Alexander Kolesnikov and
                  Dirk Weissenborn and
                  Xiaohua Zhai and
                  Thomas Unterthiner and
                  Mostafa Dehghani and
                  Matthias Minderer and
                  Georg Heigold and
                  Sylvain Gelly and
                  Jakob Uszkoreit and
                  Neil Houlsby},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition
                  at Scale},
  journal      = {CoRR},
  volume       = {abs/2010.11929},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.11929},
  eprinttype    = {arXiv},
  eprint       = {2010.11929},
  timestamp    = {Fri, 20 Nov 2020 14:04:05 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-11929.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{hartung2011statistical,
  title={Statistical meta-analysis with applications},
  author={Hartung, Joachim and Knapp, Guido and Sinha, Bimal K},
  year={2011},
  publisher={John Wiley \& Sons}
}

# PFN
@article{muller2021transformers,
  title={Transformers can do bayesian inference},
  author={M{\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
  journal={arXiv preprint arXiv:2112.10510},
  year={2021}
}

% zero shot learning references:
@article{xian2018zero,
  title={Zero-shot learningâ€”a comprehensive evaluation of the good, the bad and the ugly},
  author={Xian, Yongqin and Lampert, Christoph H and Schiele, Bernt and Akata, Zeynep},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={9},
  pages={2251--2265},
  year={2018},
  publisher={IEEE}
}

@InProceedings{Xian_2017_CVPR,
author = {Xian, Yongqin and Schiele, Bernt and Akata, Zeynep},
title = {Zero-Shot Learning - the Good, the Bad and the Ugly},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@inproceedings{chang2008importance,
  title={Importance of Semantic Representation: Dataless Classification.},
  author={Chang, Ming-Wei and Ratinov, Lev-Arie and Roth, Dan and Srikumar, Vivek},
  booktitle={Aaai},
  volume={2},
  pages={830--835},
  year={2008}
}

@inproceedings{larochelle2008zero,
  title={Zero-data learning of new tasks.},
  author={Larochelle, Hugo and Erhan, Dumitru and Bengio, Yoshua},
  booktitle={AAAI},
  volume={1},
  pages={3},
  year={2008}
}

@article{palatucci2009zero,
  title={Zero-shot learning with semantic output codes},
  author={Palatucci, Mark and Pomerleau, Dean and Hinton, Geoffrey E and Mitchell, Tom M},
  journal={Advances in neural information processing systems},
  volume={22},
  year={2009}
}

% few or zero shot learning applied to tabular data:

@InProceedings{pmlr-v206-hegselmann23a,
  title = 	 {TabLLM: Few-shot Classification of Tabular Data with Large Language Models},
  author =       {Hegselmann, Stefan and Buendia, Alejandro and Lang, Hunter and Agrawal, Monica and Jiang, Xiaoyi and Sontag, David},
  booktitle = 	 {Proceedings of The 26th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {5549--5581},
  year = 	 {2023},
  editor = 	 {Ruiz, Francisco and Dy, Jennifer and van de Meent, Jan-Willem},
  volume = 	 {206},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--27 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v206/hegselmann23a/hegselmann23a.pdf},
  url = 	 {https://proceedings.mlr.press/v206/hegselmann23a.html}
}

@InProceedings{nam2023stunt,
    title={{STUNT}: Few-shot Tabular Learning with Self-generated Tasks from Unlabeled Tables},
    author={Jaehyun Nam and Jihoon Tack and Kyungmin Lee and Hankook Lee and Jinwoo Shin},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=_xlsjehDvlY}
}

@misc{gardner2024largescaletransferlearning,
      title={Large Scale Transfer Learning for Tabular Data via Language Modeling}, 
      author={Josh Gardner and Juan C. Perdomo and Ludwig Schmidt},
      year={2024},
      eprint={2406.12031},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.12031}, 
}

% tabular data challenges articles:
@article{chui2018notes,
  title={Notes from the AI frontier: Insights from hundreds of use cases},
  author={Chui, Michael and Manyika, James and Miremadi, Mehdi and Henke, Nicolaus and Chung, Rita and Nel, Pieter and Malhotra, Sankalp},
  journal={McKinsey Global Institute},
  volume={2},
  pages={267},
  year={2018}
}

@article{borisov2022deep,
  title={Deep neural networks and tabular data: A survey},
  author={Borisov, Vadim and Leemann, Tobias and Se{\ss}ler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
  journal={IEEE transactions on neural networks and learning systems},
  year={2022},
  publisher={IEEE}
}

@article{shwartz2022tabular,
  title={Tabular data: Deep learning is not all you need},
  author={Shwartz-Ziv, Ravid and Armon, Amitai},
  journal={Information Fusion},
  volume={81},
  pages={84--90},
  year={2022},
  publisher={Elsevier}
}

@article{DBLP:journals/corr/abs-2407-00956,
  publtype={informal},
  author={Han-Jia Ye and Si-Yang Liu and Hao-Run Cai and Qi-Le Zhou and De-Chuan Zhan},
  title={A Closer Look at Deep Learning on Tabular Data},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2407.00956},
  url={https://doi.org/10.48550/arXiv.2407.00956}
}

% LLM in context learning articles:
@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{xie2021explanation,
  title={An explanation of in-context learning as implicit bayesian inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2111.02080},
  year={2021}
}

% adversarial learning:
@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

% meta learning references:

@article{lemke2015metalearning,
  title={Metalearning: a survey of trends and technologies},
  author={Lemke, Christiane and Budka, Marcin and Gabrys, Bogdan},
  journal={Artificial intelligence review},
  volume={44},
  pages={117--130},
  year={2015},
  publisher={Springer}
}

@article{vanschoren2018meta,
  title={Meta-learning: A survey},
  author={Vanschoren, Joaquin},
  journal={arXiv preprint arXiv:1810.03548},
  year={2018}
}

@article{feurer2022auto,
  title={Auto-sklearn 2.0: Hands-free automl via meta-learning},
  author={Feurer, Matthias and Eggensperger, Katharina and Falkner, Stefan and Lindauer, Marius and Hutter, Frank},
  journal={Journal of Machine Learning Research},
  volume={23},
  pages={1--61},
  year={2022}
}

@article{hospedales2021meta,
  title={Meta-learning in neural networks: A survey},
  author={Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={44},
  number={9},
  pages={5149--5169},
  year={2021},
  publisher={IEEE}
}

@article{JMLR:v22:21-0657,
  author  = {Luisa Zintgraf and Sebastian Schulze and Cong Lu and Leo Feng and Maximilian Igl and Kyriacos Shiarlis and Yarin Gal and Katja Hofmann and Shimon Whiteson},
  title   = {VariBAD: Variational Bayes-Adaptive Deep RL via Meta-Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {289},
  pages   = {1--39},
  url     = {http://jmlr.org/papers/v22/21-0657.html}
}

@article{nichol2018reptile,
  title={Reptile: a scalable metalearning algorithm},
  author={Nichol, Alex and Schulman, John},
  journal={arXiv preprint arXiv:1803.02999},
  volume={2},
  number={3},
  pages={4},
  year={2018}
}

% source for def of meta learning:
@book{hutter2019automated,
  title={Automated machine learning: methods, systems, challenges},
  author={Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  year={2019},
  publisher={Springer Nature}
}

@inproceedings{feurer-neurips15a,
    title     = {Efficient and Robust Automated Machine Learning},
    author    = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
    booktitle = {Advances in Neural Information Processing Systems 28 (2015)},
    pages     = {2962--2970},
    year      = {2015}
}

@ARTICLE{8951014,
  author={Khan, Irfan and Zhang, Xianchao and Rehman, Mobashar and Ali, Rahman},
  journal={IEEE Access}, 
  title={A Literature Survey and Empirical Study of Meta-Learning for Classifier Selection}, 
  year={2020},
  volume={8},
  number={},
  pages={10262-10281},
  keywords={Machine learning algorithms;Task analysis;Classification algorithms;Machine learning;Heuristic algorithms;Clustering algorithms;Space exploration;Meta-learning;algorithm selection;classification;machine learning},
  doi={10.1109/ACCESS.2020.2964726}
}

@article{huisman2021survey,
  title={A survey of deep meta-learning},
  author={Huisman, Mike and Van Rijn, Jan N and Plaat, Aske},
  journal={Artificial Intelligence Review},
  volume={54},
  number={6},
  pages={4483--4541},
  year={2021},
  publisher={Springer}
}

@article{gevaert2021meta,
  title={Meta-learning reduces the amount of data needed to build AI models in oncology},
  author={Gevaert, Olivier},
  journal={British Journal of Cancer},
  volume={125},
  number={3},
  pages={309--310},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{frans2021population,
  title={Population-based evolution optimizes a meta-learning objective},
  author={Frans, Kevin and Witkowski, Olaf},
  journal={arXiv preprint arXiv:2103.06435},
  year={2021}
}

% GBDT
@article{friedman2001greedy,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}

% other deep tabular references:
@inproceedings{arik2021tabnet,
  title={Tabnet: Attentive interpretable tabular learning},
  author={Arik, Sercan {\"O} and Pfister, Tomas},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  pages={6679--6687},
  year={2021}
}

@article{popov2019neural,
  title={Neural oblivious decision ensembles for deep learning on tabular data},
  author={Popov, Sergei and Morozov, Stanislav and Babenko, Artem},
  journal={arXiv preprint arXiv:1909.06312},
  year={2019}
}

@article{kadra2021well,
  title={Well-tuned simple nets excel on tabular datasets},
  author={Kadra, Arlind and Lindauer, Marius and Hutter, Frank and Grabocka, Josif},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={23928--23941},
  year={2021}
}

@article{levin2022transfer,
  title={Transfer learning with deep tabular models},
  author={Levin, Roman and Cherepanova, Valeriia and Schwarzschild, Avi and Bansal, Arpit and Bruss, C Bayan and Goldstein, Tom and Wilson, Andrew Gordon and Goldblum, Micah},
  journal={arXiv preprint arXiv:2206.15306},
  year={2022}
}

@article{rubachev2022revisiting,
  title={Revisiting pretraining objectives for tabular deep learning},
  author={Rubachev, Ivan and Alekberov, Artem and Gorishniy, Yury and Babenko, Artem},
  journal={arXiv preprint arXiv:2207.03208},
  year={2022}
}

@article{gorishniy2021revisiting,
  title={Revisiting deep learning models for tabular data},
  author={Gorishniy, Yury and Rubachev, Ivan and Khrulkov, Valentin and Babenko, Artem},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18932--18943},
  year={2021}
}

@article{grinsztajn2022tree,
  title={Why do tree-based models still outperform deep learning on typical tabular data?},
  author={Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={507--520},
  year={2022}
}

@article{somepalli2021saint,
  title={Saint: Improved neural networks for tabular data via row attention and contrastive pre-training},
  author={Somepalli, Gowthami and Goldblum, Micah and Schwarzschild, Avi and Bruss, C Bayan and Goldstein, Tom},
  journal={arXiv preprint arXiv:2106.01342},
  year={2021}
}

% LLMS doing few shot stuff
@article{mann2020language,
  title={Language models are few-shot learners},
  author={Mann, Ben and Ryder, N and Subbiah, M and Kaplan, J and Dhariwal, P and Neelakantan, A and Shyam, P and Sastry, G and Askell, A and Agarwal, S and others},
  journal={arXiv preprint arXiv:2005.14165},
  volume={1},
  year={2020}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{perez2021true,
  title={True few-shot learning with language models},
  author={Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={11054--11070},
  year={2021}
}

@article{tsimpoukelli2021multimodal,
  title={Multimodal few-shot learning with frozen language models},
  author={Tsimpoukelli, Maria and Menick, Jacob L and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={200--212},
  year={2021}
}

@article{cahyawijaya2024llms,
  title={LLMs Are Few-Shot In-Context Low-Resource Language Learners},
  author={Cahyawijaya, Samuel and Lovenia, Holy and Fung, Pascale},
  journal={arXiv preprint arXiv:2403.16512},
  year={2024}
}

@inproceedings{ahmed2022few,
  title={Few-shot training LLMs for project-specific code-summarization},
  author={Ahmed, Toufique and Devanbu, Premkumar},
  booktitle={Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
  pages={1--5},
  year={2022}
}

@article{bischl2021openml,
  title={Openml benchmarking suites},
  author={Bischl, Bernd and Casalicchio, Giuseppe and Feurer, Matthias and Gijsbers, Pieter and Hutter, Frank and Lang, Michel and Mantovani, Rafael G and van Rijn, Jan N and Vanschoren, Joaquin},
  journal={Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
  year={2021}
}

@inproceedings{fischer2023openml,
  title={OpenML-CTR23--a curated tabular regression benchmarking suite},
  author={Fischer, Sebastian Felix and Feurer, Matthias and Bischl, Bernd},
  booktitle={AutoML Conference 2023 (Workshop)},
  year={2023}
}

@article{prokhorenkova2018catboost,
  title={CatBoost: unbiased boosting with categorical features},
  author={Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  pages={785--794},
  year={2016}
}

@article{ke2017lightgbm,
  title={Lightgbm: A highly efficient gradient boosting decision tree},
  author={Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{mcelfresh2024neural,
  title={When do neural nets outperform boosted trees on tabular data?},
  author={McElfresh, Duncan and Khandagale, Sujay and Valverde, Jonathan and Prasad C, Vishak and Ramakrishnan, Ganesh and Goldblum, Micah and White, Colin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{cover1967nearest,
  title={Nearest neighbor pattern classification},
  author={Cover, Thomas and Hart, Peter},
  journal={IEEE transactions on information theory},
  volume={13},
  number={1},
  pages={21--27},
  year={1967},
  publisher={IEEE}
}

@inproceedings{
levin2023transfer,
title={Transfer Learning with Deep Tabular Models},
author={Roman Levin and Valeriia Cherepanova and Avi Schwarzschild and Arpit Bansal and C. Bayan Bruss and Tom Goldstein and Andrew Gordon Wilson and Micah Goldblum},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=b0RuGUYo8pA}
}

@article{agtabular,
  title={AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data},
  author={Erickson, Nick and Mueller, Jonas and Shirkov, Alexander and Zhang, Hang and Larroy, Pedro and Li, Mu and Smola, Alexander},
  journal={arXiv preprint arXiv:2003.06505},
  year={2020}
}

@inproceedings{
perciballi2024adapting,
title={Adapting Tab{PFN} for Zero-Inflated Metagenomic Data},
author={Giulia Perciballi and Federica Granese and Ahmad Fall and Farida ZEHRAOUI and Edi Prifti and Jean-Daniel Zucker},
booktitle={NeurIPS 2024 Third Table Representation Learning Workshop},
year={2024},
url={https://openreview.net/forum?id=3I0bVvUj25}
}

@inproceedings{
koshil2024towards,
title={Towards Localization via Data Embedding for Tab{PFN}},
author={Mykhailo Koshil and Thomas Nagler and Matthias Feurer and Katharina Eggensperger},
booktitle={NeurIPS 2024 Third Table Representation Learning Workshop},
year={2024},
url={https://openreview.net/forum?id=LFyQyV5HxQ}
}

@inproceedings{
hoo2024the,
title={The Tabular Foundation Model Tab{PFN} Outperforms Specialized Time Series Forecasting Models Based on Simple Features},
author={Shi Bin Hoo and Samuel M{\"u}ller and David Salinas and Frank Hutter},
booktitle={NeurIPS Workshop on Time Series in the Age of Large Models},
year={2024},
url={https://openreview.net/forum?id=ho8Yx5YfiM}
}

@inproceedings{
helli2024driftresilient,
title={Drift-Resilient Tab{PFN}: In-Context Learning Distribution Shifts on Tabular Data},
author={Kai Helli and David Schnurr and Noah Hollmann and Samuel M{\"u}ller and Frank Hutter},
booktitle={AutoML Conference 2024 (Workshop Track)},
year={2024},
url={https://openreview.net/forum?id=VbmqcoHpGT}
}

@article{luo2018neural,
  title={Neural architecture optimization},
  author={Luo, Renqian and Tian, Fei and Qin, Tao and Chen, Enhong and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{
feuer2024tunetables,
title={TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks},
author={Benjamin Feuer and Robin Tibor Schirrmeister and Valeriia Cherepanova and Chinmay Hegde and Frank Hutter and Micah Goldblum and Niv Cohen and Colin White},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=FOfU3qhcIG}
}

@inproceedings{nagler2023statistical,
  title={Statistical foundations of prior-data fitted networks},
  author={Nagler, Thomas},
  booktitle={International Conference on Machine Learning},
  pages={25660--25676},
  year={2023},
  organization={PMLR}
}

@article{tikhonov1963solution,
  title={Solution of incorrectly formulated problems and the regularization method.},
  author={Tikhonov, Andrei N},
  journal={Sov Dok},
  volume={4},
  pages={1035--1038},
  year={1963}
}

@article{tibshirani1996regression,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={58},
  number={1},
  pages={267--288},
  year={1996},
  publisher={Oxford University Press}
}

@article{cortes1995support,
  title={Support-Vector Networks},
  author={Cortes, Corinna},
  journal={Machine Learning},
  year={1995}
}

@inproceedings{ho1995random,
  title={Random decision forests},
  author={Ho, Tin Kam},
  booktitle={Proceedings of 3rd international conference on document analysis and recognition},
  volume={1},
  pages={278--282},
  year={1995},
  organization={IEEE}
}

% more tabular deep learning citations 

@misc{huang2020tabtransformertabulardatamodeling,
      title={TabTransformer: Tabular Data Modeling Using Contextual Embeddings}, 
      author={Xin Huang and Ashish Khetan and Milan Cvitkovic and Zohar Karnin},
      year={2020},
      eprint={2012.06678},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2012.06678}, 
}

@misc{
kotelnikov2023tabddpm,
title={Tab{DDPM}: Modelling Tabular Data with Diffusion Models},
author={Akim Kotelnikov and Dmitry Baranchuk and Ivan Rubachev and Artem Babenko},
year={2023},
url={https://openreview.net/forum?id=EJka_dVXEcr}
}

@inproceedings{
gorishniy2024tabr,
title={TabR: Tabular Deep Learning Meets Nearest Neighbors},
author={Yury Gorishniy and Ivan Rubachev and Nikolay Kartashev and Daniil Shlenskii and Akim Kotelnikov and Artem Babenko},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=rhgIgTSSxW}
}

@inproceedings{gorishniy2022embeddings,
    title={On Embeddings for Numerical Features in Tabular Deep Learning},
    author={Yury Gorishniy and Ivan Rubachev and Artem Babenko},
    booktitle={{NeurIPS}},
    year={2022},
}

@inproceedings{10.1145/3637528.3671893,
author = {Chen, Jintai and Yan, Jiahuan and Chen, Qiyuan and Chen, Danny Z. and Wu, Jian and Sun, Jimeng},
title = {Can a Deep Learning Model be a Sure Bet for Tabular Prediction?},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671893},
doi = {10.1145/3637528.3671893},
abstract = {Data organized in tabular format is ubiquitous in real-world applications, and users often craft tables with biased feature definitions and flexibly set prediction targets of their interests. Thus, a rapid development of a robust, effective, dataset-versatile, user-friendly tabular prediction approach is highly desired. While Gradient Boosting Decision Trees (GBDTs) and existing deep neural networks (DNNs) have been extensively utilized by professional users, they present several challenges for casual users, particularly: (i) the dilemma of model selection due to their different dataset preferences, and (ii) the need for heavy hyperparameter searching, failing which their performances are deemed inadequate. In this paper, we delve into this question: Can we develop a deep learning model that serves as a sure bet solution for a wide range of tabular prediction tasks, while also being user-friendly for casual users? We delve into three key drawbacks of deep tabular models, encompassing: (P1) lack of rotational variance property, (P2) large data demand, and (P3) over-smooth solution. We propose ExcelFormer, addressing these challenges through a semi-permeable attention module that effectively constrains the influence of less informative features to break the DNNs' rotational invariance property (for P1), data augmentation approaches tailored for tabular data (for P2), and attentive feedforward network to boost the model fitting capability (for P3). These designs collectively make ExcelFormer a sure bet solution for diverse tabular datasets. Extensive and stratified experiments conducted on real-world datasets demonstrate that our model outperforms previous approaches across diverse tabular data prediction tasks, and this framework can be friendly to casual users, offering ease of use without the heavy hyperparameter tuning. The codes are available at https://github.com/whatashot/excelformer.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {288â€“296},
numpages = {9},
keywords = {mixup, tabular data prediction},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{
kadra2021welltuned,
title={Well-tuned Simple Nets Excel on Tabular Datasets},
author={Arlind Kadra and Marius Lindauer and Frank Hutter and Josif Grabocka},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=d3k38LTDCyO}
}

% approaches leveraging LLMs - so not one model per one dataset, but not pure meta leanring:

@inproceedings{
yan2024making,
title={Making Pre-trained Language Models Great on Tabular Prediction},
author={Jiahuan Yan and Bo Zheng and Hongxia Xu and Yiheng Zhu and Danny Chen and Jimeng Sun and Jian Wu and Jintai Chen},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=anzIzGZuLi}
}

@inproceedings{
borisov2023language,
title={Language Models are Realistic Tabular Data Generators},
author={Vadim Borisov and Kathrin Sessler and Tobias Leemann and Martin Pawelczyk and Gjergji Kasneci},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=cEygmQNOeI}
}

@inproceedings{
ye2024towards,
title={Towards Cross-Table Masked Pretraining for Web Data Mining},
author={Chao Ye and Guoshan Lu and Haobo Wang and Liyao Li and Sai Wu and Gang Chen and Junbo Zhao},
booktitle={The Web Conference 2024},
year={2024},
url={https://openreview.net/forum?id=9jj7cMOXQo}
}

@inproceedings{
spinaci2024portal,
title={{PORTAL}: Scalable Tabular Foundation Models via Content-Specific Tokenization},
author={Marco Spinaci and Marek Polewczyk and Johannes Hoffart and Markus C. Kohler and Sam Thelin and Tassilo Klein},
booktitle={NeurIPS 2024 Third Table Representation Learning Workshop},
year={2024},
url={https://openreview.net/forum?id=TSZQvknbLO}
}

@inproceedings{
kim2024carte,
title={{CARTE}: Pretraining and Transfer for Tabular Learning},
author={Myung Jun Kim and Leo Grinsztajn and Gael Varoquaux},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=9kArQnKLDp}
}

@article{zhu2023xtab,
  title={Xtab: Cross-table pretraining for tabular transformers},
  author={Zhu, Bingzhao and Shi, Xingjian and Erickson, Nick and Li, Mu and Karypis, George and Shoaran, Mahsa},
  journal={arXiv preprint arXiv:2305.06090},
  year={2023}
}

% this one too: pmlr-v206-hegselmann23a


% GAN references

@article{shafahi2019adversarial,
  title={Adversarial training for free!},
  author={Shafahi, Ali and Najibi, Mahyar and Ghiasi, Mohammad Amin and Xu, Zheng and Dickerson, John and Studer, Christoph and Davis, Larry S and Taylor, Gavin and Goldstein, Tom},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{GoodfellowAdversarial,
title	= {Explaining and Harnessing Adversarial Examples},author	= {Ian Goodfellow and Jonathon Shlens and Christian Szegedy},year	= {2015},URL	= {http://arxiv.org/abs/1412.6572},booktitle	= {International Conference on Learning Representations}}

@inproceedings{
madry2018towards,
title={Towards Deep Learning Models Resistant to Adversarial Attacks},
author={Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJzIBfZAb},
}

@inproceedings{
kurakin2017adversarial,
title={Adversarial Machine Learning at Scale},
author={Alexey Kurakin and Ian J. Goodfellow and Samy Bengio},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=BJm4T4Kgx}
}

@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}

% GAN problems:

% Wasserstein GAN:
@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International conference on machine learning},
  pages={214--223},
  year={2017},
  organization={PMLR}
}

% NAsh eq:

@article{ratliff2016characterization,
  title={On the characterization of local Nash equilibria in continuous games},
  author={Ratliff, Lillian J and Burden, Samuel A and Sastry, S Shankar},
  journal={IEEE transactions on automatic control},
  volume={61},
  number={8},
  pages={2301--2307},
  year={2016},
  publisher={IEEE}
}

% vanishing gradient and mode collapse:
@article{goodfellow2016nips,
  title={Nips 2016 tutorial: Generative adversarial networks},
  author={Goodfellow, Ian},
  journal={arXiv preprint arXiv:1701.00160},
  year={2016}
}

% just mode collapse:
@inproceedings{arora2017generalization,
  title={Generalization and equilibrium in generative adversarial nets (gans)},
  author={Arora, Sanjeev and Ge, Rong and Liang, Yingyu and Ma, Tengyu and Zhang, Yi},
  booktitle={International conference on machine learning},
  pages={224--232},
  year={2017},
  organization={PMLR}
}

% GAN problem survey paper:
@article{jabbar2021survey,
  title={A survey on generative adversarial networks: Variants, applications, and training},
  author={Jabbar, Abdul and Li, Xi and Omar, Bourahla},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={8},
  pages={1--49},
  year={2021},
  publisher={ACM New York, NY}
}

% batch norm paper - solve internal covariate shift?
@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}

% Other articles trying to make efficient use of backpropagation in adversarial training

@article{yuan2021meta,
  title={Meta adversarial perturbations},
  author={Yuan, Chia-Hung and Chen, Pin-Yu and Yu, Chia-Mu},
  journal={arXiv preprint arXiv:2111.10291},
  year={2021}
}

% adversarial for free problems:
@article{andriushchenko2020understanding,
  title={Understanding and improving fast adversarial training},
  author={Andriushchenko, Maksym and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={16048--16059},
  year={2020}
}

@inproceedings{kim2021understanding,
  title={Understanding catastrophic overfitting in single-step adversarial training},
  author={Kim, Hoki and Lee, Woojin and Lee, Jaewook},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={8119--8127},
  year={2021}
}

% other methods to make adversarial training more efficient:
@inproceedings{
Wong2020Fast,
title={Fast is better than free: Revisiting adversarial training},
author={Eric Wong and Leslie Rice and J. Zico Kolter},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJx040EFvH}
}

@inproceedings{NEURIPS2019_812b4ba2,
 author = {Zhang, Dinghuai and Zhang, Tianyuan and Lu, Yiping and Zhu, Zhanxing and Dong, Bin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf},
 volume = {32},
 year = {2019}
}


@inproceedings{zhang2021free,
  title={Free Adversarial Training with Layerwise Heuristic Learning},
  author={Zhang, Haitao and Shi, Yucheng and Dong, Benyu and Han, Yahong and Li, Yuanzhang and Kuang, Xiaohui},
  booktitle={International Conference on Image and Graphics},
  pages={120--131},
  year={2021},
  organization={Springer}
}
