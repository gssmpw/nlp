\section{Related Work}
\label{sec:related-work}

\subsection{Tabular Learning}

%\textcolor{blue}{Talk about traditional methods and DNN methods (traditional one-model-for-one-dataset pipeline): About 2 sentences. DONE}

GBDTs such as XGBoost and others \citep{chen2016xgboost, prokhorenkova2018catboost, ke2017lightgbm} are commonly used for tabular data problems, in the traditional one-model-for-one-dataset approach. At this point, numerous deep learning approaches have been developed for tabular data, mostly taking the one-model-for-one-dataset approach \citep{borisov2022deep, somepalli2021saint, grinsztajn2022tree, gorishniy2021revisiting, rubachev2022revisiting, kadra2021well, arik2021tabnet, popov2019neural, arik2021tabnet, kotelnikov2023tabddpm, gorishniy2024tabr, gorishniy2022embeddings, 10.1145/3637528.3671893, kadra2021welltuned, huang2020tabtransformertabulardatamodeling}, but some also venturing into transfer learning, many but not all leveraging large language models to find relevant information for the tabular data problem at hand \citep{levin2022transfer, yan2024making, borisov2023language, ye2024towards, spinaci2024portal, pmlr-v206-hegselmann23a, kim2024carte, zhu2023xtab}.






% Machine learning applied to tabular data traditionally consists of training model from scratch for every distinct dataset. Transfer learning in other domains (vision, language) benefit from having a mostly homogeneous input format (in contrast with tabular data), and have largely solved the challenge of dealing with different prediction target variables. Deep learning methods applied to tabular data have recently made some limited progress on enabling transfer learning between tabular data problems \citet{levin2023transfer}. 



\paragraph{Tabular Meta-Learning} Auto-Sklearn introduced in \citet{feurer-neurips15a} and improved upon in \citet{feurer2022auto} use Bayesian optimization to determine the best algorithm and feature pre-processing steps for modeling a given dataset. Meta learning is used for initializing the Bayesian optimization. In contrast to the approaches of transfer learning in deep tabular data, and of Auto-Sklearn, TabPFN \citep{muller2021transformers} is trained solely on synthetic data to learn how to acquire meaningful data representations and to learn the general prediction logic of tabular classification tasks. A more detailed background review on TabPFN can be found in Appendix \ref{sec:background}. Extensions to TabPFN include \citet{feuer2024tunetables}, where fine tuning was leveraged on top of TabPFN's basic function, to compress incoming data to fit into TabPFN's limitations. \citet{helli2024driftresilient} introduced a variant of TabPFN that was trained on a drifting synthetic data distribution, but the drift is independent of the performance of the model being optimized.

% meta learninf - we define as not having a model specialize to a specifici model, but rather learns how to learn.
%\textcolor{blue}{1 sentence AutoSKLearn, 2 sentences TabPFN (make sure not repetitive of the TabPFN content in introduction. Content in introduction: high-level; content in related work: a little more detailed; content in appendix: very detailed), 2 sentence extensions of TabPFN. Done.}

%AutoGluon \citet{agtabular} uses the ensemble of models available to it, rather than making a specific model and hyper-parameter selection, and incorporates them into one large model, by stacking the ensemble of models into one large deep network.

%Describe TabPFN: About 2 sentences. Do not necessarily have to start from PFNs. You can describe TabPFN directly. \textcolor{gray}{PFNs \citep{muller2021transformers} are transformers pre-trained on synthetic data generated from a prior distribution, to perform approximate Bayesian inference in a single forward pass using in-context learning \citep{luo2018neural, mann2020language}.} (this is already said in introduction) \textcolor{gray}{PFNs do not fit a model on downstream training data, instead feeding training data into the context and make predictions conditioning on the context.} \textcolor{gray}{One particular PFN model, TabPFN, introduced in \citet{hollmann2022tabpfn}, has achieved state-of-the-art classification on small tabular datasets  even in comparison with state of the art machine learning techniques free of any zero or few shot constraints \citep{hollmann2022tabpfn, mcelfresh2024neural}.} (this is high-level and basically already said in introduction using other words) 


%The inductive bias in TabPFN comes from the model architecture, and the specific choices we make about the synthetic data distribution from which one draws samples to train TabPFN.

\subsection{Zero-shot Learning}


%Difference between Introduction writing and related work writing:

%Example:

%\textcolor{gray}{Zero shot learning -- the ability of a machine learning model to produce meaningful inferences on previously unseen data problems -- is an impressive capability that has emerged in recent years in the space of language and vision problems \citep{xian2018zero, Xian_2017_CVPR, chang2008importance, larochelle2008zero, palatucci2009zero}.} \textbf{-- this is introduction writing: content is focusing on introducing concept and timeline.}

%Recent works such as \citet{xian2018zero, Xian_2017_CVPR, chang2008importance, larochelle2008zero, palatucci2009zero} have shown impressive capability of zero shot learning in the space of language and vision problems. \textbf{-- this is related work (keyword is "work") writing: content is focusing on the works.}

%STARTING HERE

Recent work such as \citet{xian2018zero, Xian_2017_CVPR, chang2008importance, larochelle2008zero, palatucci2009zero} have shown impressive capability of zero-shot learning in the space of language and vision problems. Recent approaches to zero-shot or few-shot learning for tabular data problems mostly encode tabular data as language, and then leverage large language models (LLMs) for their zero- or few-shot capabilities (see \citet{pmlr-v206-hegselmann23a, nam2023stunt, gardner2024largescaletransferlearning}). These approaches rely on relevant information about the tabular data existing in LLMs -- this is most obviously the case when column names are meaningful -- but it is not guaranteed for all tabular data problems.

%About 1 more sentence. DONE?

\subsection{Adversarial Training} 

%Traditional GANs.

%Fast adversarial training: https://arxiv.org/abs/1904.12843



%In classic generative adversarial networks (GANs) \citet{GoodfellowAdversarial, madry2018towards, kurakin2017adversarial} multiple gradient computations are needed to produce adversarial samples by perturbing the input samples. \citet{shafahi2019adversarial} improved on the traditional approach by updating both the model parameters and image perturbations using one simultaneous backward pass.




%\textcolor{gray}{Adversarial training is generally understood as a technique by which an adversarial agent (a data generator) is trying to challenge a model during its training process. In this way, the model is trained to be more robust to malicious data manipulation attacks. In the context of generative adversarial networks (GANs) \citet{GoodfellowAdversarial, madry2018towards, kurakin2017adversarial}, the data model is referred to as the discriminator. }

Upon generative adversarial networks (GANs) \citep{GoodfellowAdversarial, madry2018towards, kurakin2017adversarial}, recent work such as \citet{shafahi2019adversarial} improved on the efficiency by combining the back-propagation steps of the generator and discriminator. However, this method has been shown to suffer from catastrophic overfitting \citep{andriushchenko2020understanding, kim2021understanding} without further modifications. Other works focusing on improving the efficiency of GAN training include \citet{Wong2020Fast} and \citet{NEURIPS2019_812b4ba2} where they restrict most of the forward and back propagation within the first layer of the network during adversarial updates. \citet{zhang2021free} in particular noted that weights updates frequently going back and forth in opposite directions in one training epoch, suggest those updates are redundant.
%Other publications also trying to make efficient use of back-propagation in adversarial training include \citet{yuan2021meta}
%Wasserstein GAN (WGAN) \citep{arjovsky2017wasserstein} was introduced to mitigate the vanishing gradient\citep{goodfellow2016nips} and mode collapse (where the generator produces samples that have very little diversity) problems\cite{arora2017generalization}. 
Many other variations have been introduced to mitigate vanishing gradient and additional challenges of GAN training (see \citet{jabbar2021survey} and references therein): failing at finding a Nash-equilibrium \citep{ratliff2016characterization}, and internal
covariate shift \citep{ioffe2015batch}.

%To the best of our knowledge, adversarial training has not been performed in the pre-training of ZSMLs in prior work.

% About 3 sentences.