@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{perez2022discovering,
  title={Discovering language model behaviors with model-written evaluations},
  author={Perez, Ethan and Ringer, Sam and Luko{\v{s}}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and others},
  journal={arXiv preprint arXiv:2212.09251},
  year={2022}
}

@inproceedings{rimsky-etal-2024-steering,
    title = "Steering Llama 2 via Contrastive Activation Addition",
    author = "Rimsky, Nina  and
      Gabrieli, Nick  and
      Schulz, Julian  and
      Tong, Meg  and
      Hubinger, Evan  and
      Turner, Alexander",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.828",
    doi = "10.18653/v1/2024.acl-long.828",
    pages = "15504--15522",
    abstract = "We introduce Contrastive Activation Addition (CAA), a method for steering language models by modifying their activations during forward passes. CAA computes {``}steering vectors{''} by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user{'}s prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA{'}s effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA{'}s mechanisms by employing various activation space interpretation methods. CAA accurately steers model outputs and sheds light on how high-level concepts are represented in Large Language Models (LLMs).",
}

@article{zou2023representation,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@article{li2024inference,
  title={Inference-time intervention: Eliciting truthful answers from a language model},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{wang2022spectral,
  title={Spectral evolution and invariance in linear-width neural networks},
  author={Wang, Zhichao and Engel, Andrew and Sarwate, Anand and Dumitriu, Ioana and Chiang, Tony},
  journal={arXiv preprint arXiv:2211.06506},
  year={2022}
}

@article{ba2022high,
  title={High-dimensional asymptotics of feature learning: How one gradient step improves the representation},
  author={Ba, Jimmy and Erdogdu, Murat A and Suzuki, Taiji and Wang, Zhichao and Wu, Denny and Yang, Greg},
  journal={arXiv preprint arXiv:2205.01445},
  year={2022}
}

@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}

@inproceedings{khanov2024alignment,
  title={ARGS: Alignment as Reward-Guided Search},
  author={Khanov, Maxim and Burapacheep, Jirayu and Li, Yixuan},
  booktitle={Proceedings of the International Conference on Learning Representations},
  year={2024}
}

@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}
}

@article{fan2020spectra,
  title={Spectra of the conjugate kernel and neural tangent kernel for linear-width neural networks},
  author={Fan, Zhou and Wang, Zhichao},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={7710--7721},
  year={2020}
}

@article{velikanov2021explicit,
  title={Explicit loss asymptotics in the gradient descent training of neural networks},
  author={Velikanov, Maksim and Yarotsky, Dmitry},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2570--2582},
  year={2021}
}

@article{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{burns2023weak,
  title={Weak-to-strong generalization: Eliciting strong capabilities with weak supervision},
  author={Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others},
  journal={arXiv preprint arXiv:2312.09390},
  year={2023}
}

@article{scherlis2022polysemanticity,
  title={Polysemanticity and capacity in neural networks},
  author={Scherlis, Adam and Sachan, Kshitij and Jermyn, Adam S and Benton, Joe and Shlegeris, Buck},
  journal={arXiv preprint arXiv:2210.01892},
  year={2022}
}

@article{jiang2024origins,
  title={On the Origins of Linear Representations in Large Language Models},
  author={Jiang, Yibo and Rajendran, Goutham and Ravikumar, Pradeep and Aragam, Bryon and Veitch, Victor},
  journal={arXiv preprint arXiv:2403.03867},
  year={2024}
}

@article{park2023linear,
  title={The linear representation hypothesis and the geometry of large language models},
  author={Park, Kiho and Choe, Yo Joong and Veitch, Victor},
  journal={arXiv preprint arXiv:2311.03658},
  year={2023}
}

@article{hoorfar2008inequalities,
  title={Inequalities on the Lambert W function and hyperpower function},
  author={Hoorfar, Abdolhossein and Hassani, Mehdi},
  journal={J. Inequal. Pure and Appl. Math},
  volume={9},
  number={2},
  pages={5--9},
  year={2008}
}

@article{bricken2023towards,
  title={Towards monosemanticity: Decomposing language models with dictionary learning},
  author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and others},
  journal={Transformer Circuits Thread},
  pages={2},
  year={2023}
}

@article{elhage2022toy,
  title={Toy models of superposition},
  author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others},
  journal={arXiv preprint arXiv:2209.10652},
  year={2022}
}

@article{gurnee2023finding,
  title={Finding neurons in a haystack: Case studies with sparse probing},
  author={Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
  journal={arXiv preprint arXiv:2305.01610},
  year={2023}
}

@article{scherlis2022polysemanticity,
  title={Polysemanticity and capacity in neural networks},
  author={Scherlis, Adam and Sachan, Kshitij and Jermyn, Adam S and Benton, Joe and Shlegeris, Buck},
  journal={arXiv preprint arXiv:2210.01892},
  year={2022}
}

@article{papyan2020prevalence,
  title={Prevalence of neural collapse during the terminal phase of deep learning training},
  author={Papyan, Vardan and Han, XY and Donoho, David L},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={40},
  pages={24652--24663},
  year={2020},
  publisher={National Acad Sciences}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@book{abramowitz1968handbook,
  title={Handbook of mathematical functions with formulas, graphs, and mathematical tables},
  author={Abramowitz, Milton and Stegun, Irene A},
  volume={55},
  year={1968},
  publisher={US Government printing office}
}

@incollection{sambale2023some,
  title={Some notes on concentration for $\alpha$-subexponential random variables},
  author={Sambale, Holger},
  booktitle={High Dimensional Probability IX: The Ethereal Volume},
  pages={167--192},
  year={2023},
  publisher={Springer}
}

@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{bradley1952rank,
  title={Rank analysis of incomplete block designs: I. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{mcinnes2018umap,
  title={Umap: Uniform manifold approximation and projection for dimension reduction},
  author={McInnes, Leland and Healy, John and Melville, James},
  journal={arXiv preprint arXiv:1802.03426},
  year={2018}
}

@inproceedings{loshchilov2018decoupled,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@misc{willison2023prompt,
    title = {Prompt injection},
    author = {Simon Willison},
    howpublished = {\url{https://simonwillison.net/series/prompt-injection/}},
    year={2023}
}

@misc{albert2023jailbreak,
    title = {Jailbreak Chat},
    author = {Alex Albert},
    howpublished = {\url{https://www.jailbreakchat.com/}},
    year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@misc{anthropic2023claude,
    title = {Introducing claude},
    author = {Anthropic},
    howpublished = {https://www.anthropic.com/index/introducing-claude},
    year={2023}
}

@article{ziegler2019finetuning,
      title={Fine-Tuning Language Models from Human Preferences},
      author={Daniel M. Ziegler and Nisan Stiennon and Jeffrey Wu and Tom B. Brown and Alec Radford and Dario Amodei and Paul Christiano and Geoffrey Irving},
      year={2019},
      journal={arXiv preprint arXiv:1909.08593},
}


@article{openai2023gpt4,
      title={GPT-4 Technical Report},
      author={OpenAI},
      year={2023},
      journal={arXiv preprint arXiv:2303.08774},
}

@article{wei2022emergent,
  title={Emergent Abilities of Large Language Models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={Transactions on Machine Learning Research},
  year={2022}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{lee2021pebble,
    title={PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training},
    author={Kimin Lee and Laura Smith and Pieter Abbeel},
    year={2021},
    booktitle={International Conference on Machine Learning},
}

@article{nakano2022webgpt,
      title={WebGPT: Browser-assisted question-answering with human feedback},
      author={Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},
      year={2022},
      journal={arXiv preprint arXiv:2112.09332},
}

@article{snell2023offline,
      title={Offline RL for Natural Language Generation with Implicit Language Q Learning},
      author={Charlie Snell and Ilya Kostrikov and Yi Su and Mengjiao Yang and Sergey Levine},
      year={2023},
      journal={arXiv preprint arXiv:2206.11871},
}

@article{liu2023chain,
      title={Chain of Hindsight Aligns Language Models with Feedback},
      author={Hao Liu and Carmelo Sferrazza and Pieter Abbeel},
      year={2023},
      journal={arXiv preprint arXiv:2302.02676},
}

@article{yuan2023rrhf,
      title={RRHF: Rank Responses to Align Language Models with Human Feedback without tears},
      author={Zheng Yuan and Hongyi Yuan and Chuanqi Tan and Wei Wang and Songfang Huang and Fei Huang},
      year={2023},
      journal={arXiv preprint arXiv:2304.05302},
}

@article{glaese2022improving,
      title={Improving alignment of dialogue agents via targeted human judgements},
      author={Amelia Glaese and Nat McAleese and Maja Trębacz and John Aslanides and Vlad Firoiu and Timo Ewalds and Maribeth Rauh and Laura Weidinger and Martin Chadwick and Phoebe Thacker and Lucy Campbell-Gillingham and Jonathan Uesato and Po-Sen Huang and Ramona Comanescu and Fan Yang and Abigail See and Sumanth Dathathri and Rory Greig and Charlie Chen and Doug Fritz and Jaume Sanchez Elias and Richard Green and Soňa Mokrá and Nicholas Fernando and Boxi Wu and Rachel Foley and Susannah Young and Iason Gabriel and William Isaac and John Mellor and Demis Hassabis and Koray Kavukcuoglu and Lisa Anne Hendricks and Geoffrey Irving},
      year={2022},
journal={arXiv preprint arXiv:2209.14375},
}



@article{casper2023open,
  title={Open problems and fundamental limitations of reinforcement learning from human feedback},
  author={Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, J{\'e}r{\'e}my and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and others},
  journal={arXiv preprint arXiv:2307.15217},
  year={2023}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{lee2023rlaif,
  title={Rlaif: Scaling reinforcement learning from human feedback with ai feedback},
  author={Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie and Mesnard, Thomas and Bishop, Colton and Carbune, Victor and Rastogi, Abhinav},
  journal={arXiv preprint arXiv:2309.00267},
  year={2023}
}

@article{wolf2023fundamental,
  title={Fundamental limitations of alignment in large language models},
  author={Wolf, Yotam and Wies, Noam and Levine, Yoav and Shashua, Amnon},
  journal={arXiv preprint arXiv:2304.11082},
  year={2023}
}

@article{azar2023general,
  title={A general theoretical paradigm to understand learning from human preferences},
  author={Azar, Mohammad Gheshlaghi and Rowland, Mark and Piot, Bilal and Guo, Daniel and Calandriello, Daniele and Valko, Michal and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:2310.12036},
  year={2023}
}

@article{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{papyan2020prevalence,
  title={Prevalence of neural collapse during the terminal phase of deep learning training},
  author={Papyan, Vardan and Han, XY and Donoho, David L},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={40},
  pages={24652--24663},
  year={2020},
  publisher={National Acad Sciences}
}

@article{deng2022model,
  title={A model of double descent for high-dimensional binary linear classification},
  author={Deng, Zeyu and Kammoun, Abla and Thrampoulidis, Christos},
  journal={Information and Inference: A Journal of the IMA},
  volume={11},
  number={2},
  pages={435--495},
  year={2022},
  publisher={Oxford University Press}
}

@inproceedings{liang2018understanding,
  title={Understanding the loss surface of neural networks for binary classification},
  author={Liang, Shiyu and Sun, Ruoyu and Li, Yixuan and Srikant, Rayadurgam},
  booktitle={International Conference on Machine Learning},
  pages={2835--2843},
  year={2018},
  organization={PMLR}
}

@article{kim2021fast,
  title={Fast convergence rates of deep neural networks for classification},
  author={Kim, Yongdai and Ohn, Ilsang and Kim, Dongha},
  journal={Neural Networks},
  volume={138},
  pages={179--197},
  year={2021},
  publisher={Elsevier}
}

@article{song2023preference,
  title={Preference ranking optimization for human alignment},
  author={Song, Feifan and Yu, Bowen and Li, Minghao and Yu, Haiyang and Huang, Fei and Li, Yongbin and Wang, Houfeng},
  journal={arXiv preprint arXiv:2306.17492},
  year={2023}
}

@article{dong2023raft,
  title={Raft: Reward ranked finetuning for generative foundation model alignment},
  author={Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong},
  journal={arXiv preprint arXiv:2304.06767},
  year={2023}
}

@article{ji2023ai,
  title={Ai alignment: A comprehensive survey},
  author={Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others},
  journal={arXiv preprint arXiv:2310.19852},
  year={2023}
}

@article{shi2022theoretical,
  title={A theoretical analysis on feature learning in neural networks: Emergence from inputs and advantage over fixed features},
  author={Shi, Zhenmei and Wei, Junyi and Liang, Yingyu},
  journal={arXiv preprint arXiv:2206.01717},
  year={2022}
}

@article{hendrycks2021unsolved,
  title={Unsolved problems in ml safety},
  author={Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2109.13916},
  year={2021}
}

@article{park2023ai,
  title={AI deception: A survey of examples, risks, and potential solutions},
  author={Park, Peter S and Goldstein, Simon and O'Gara, Aidan and Chen, Michael and Hendrycks, Dan},
  journal={arXiv preprint arXiv:2308.14752},
  year={2023}
}

@article{carroll2023characterizing,
  title={Characterizing Manipulation from AI Systems},
  author={Carroll, Micah and Chan, Alan and Ashton, Henry and Krueger, David},
  journal={arXiv preprint arXiv:2303.09387},
  year={2023}
}

@article{sharma2023towards,
  title={Towards understanding sycophancy in language models},
  author={Sharma, Mrinank and Tong, Meg and Korbak, Tomasz and Duvenaud, David and Askell, Amanda and Bowman, Samuel R and Cheng, Newton and Durmus, Esin and Hatfield-Dodds, Zac and Johnston, Scott R and others},
  journal={arXiv preprint arXiv:2310.13548},
  year={2023}
}

@article{leike2018scalable,
  title={Scalable agent alignment via reward modeling: a research direction},
  author={Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
  journal={arXiv preprint arXiv:1811.07871},
  year={2018}
}

@article{bang2023multitask,
  title={A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity},
  author={Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and others},
  journal={arXiv preprint arXiv:2302.04023},
  year={2023}
}

@article{hubinger2019risks,
  title={Risks from learned optimization in advanced machine learning systems},
  author={Hubinger, Evan and van Merwijk, Chris and Mikulik, Vladimir and Skalse, Joar and Garrabrant, Scott},
  journal={arXiv preprint arXiv:1906.01820},
  year={2019}
}

@article{berglund2023taken,
  title={Taken out of context: On measuring situational awareness in LLMs},
  author={Berglund, Lukas and Stickland, Asa Cooper and Balesni, Mikita and Kaufmann, Max and Tong, Meg and Korbak, Tomasz and Kokotajlo, Daniel and Evans, Owain},
  journal={arXiv preprint arXiv:2309.00667},
  year={2023}
}

@article{ngo2022alignment,
  title={The alignment problem from a deep learning perspective},
  author={Ngo, Richard and Chan, Lawrence and Mindermann, S{\"o}ren},
  journal={arXiv preprint arXiv:2209.00626},
  year={2022}
}

@article{shevlane2023model,
  title={Model evaluation for extreme risks},
  author={Shevlane, Toby and Farquhar, Sebastian and Garfinkel, Ben and Phuong, Mary and Whittlestone, Jess and Leung, Jade and Kokotajlo, Daniel and Marchal, Nahema and Anderljung, Markus and Kolt, Noam and others},
  journal={arXiv preprint arXiv:2305.15324},
  year={2023}
}

@article{shah2022goal,
  title={Goal misgeneralization: Why correct specifications aren't enough for correct goals},
  author={Shah, Rohin and Varma, Vikrant and Kumar, Ramana and Phuong, Mary and Krakovna, Victoria and Uesato, Jonathan and Kenton, Zac},
  journal={arXiv preprint arXiv:2210.01790},
  year={2022}
}

@inproceedings{pan2022effects,
  title={The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models},
  author={Pan, Alexander and Bhatia, Kush and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{munos2023nash,
  title={Nash Learning from Human Feedback},
  author={Munos, R{\'e}mi and Valko, Michal and Calandriello, Daniele and Azar, Mohammad Gheshlaghi and Rowland, Mark and Guo, Zhaohan Daniel and Tang, Yunhao and Geist, Matthieu and Mesnard, Thomas and Michi, Andrea and others},
  journal={arXiv preprint arXiv:2312.00886},
  year={2023}
}

@article{hejna2023contrastive,
  title={Contrastive Prefence Learning: Learning from Human Feedback without RL},
  author={Hejna, Joey and Rafailov, Rafael and Sikchi, Harshit and Finn, Chelsea and Niekum, Scott and Knox, W Bradley and Sadigh, Dorsa},
  journal={arXiv preprint arXiv:2310.13639},
  year={2023}
}

@article{dai2023safe,
  title={Safe rlhf: Safe reinforcement learning from human feedback},
  author={Dai, Josef and Pan, Xuehai and Sun, Ruiyang and Ji, Jiaming and Xu, Xinbo and Liu, Mickel and Wang, Yizhou and Yang, Yaodong},
  journal={arXiv preprint arXiv:2310.12773},
  year={2023}
}

@article{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{goldt2019dynamics,
  title={Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup},
  author={Goldt, Sebastian and Advani, Madhu and Saxe, Andrew M and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{wang2023rlhf,
  title={Is RLHF More Difficult than Standard RL? A Theoretical Perspective},
  author={Wang, Yuanhao and Liu, Qinghua and Jin, Chi},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}

@article{xu2023dynamics,
  title={Dynamics in deep classifiers trained with the square loss: Normalization, low rank, neural collapse, and generalization bounds},
  author={Xu, Mengjia and Rangamani, Akshay and Liao, Qianli and Galanti, Tomer and Poggio, Tomaso},
  journal={Research},
  volume={6},
  pages={0024},
  year={2023},
  publisher={AAAS}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{allen2019learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{cao2020generalization,
  title={Generalization error bounds of gradient descent for learning over-parameterized deep relu networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={3349--3356},
  year={2020}
}

@article{subramanian2022generalization,
  title={Generalization for multiclass classification with overparameterized linear models},
  author={Subramanian, Vignesh and Arya, Rahul and Sahai, Anant},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23479--23494},
  year={2022}
}

@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}

@inproceedings{arora2018stronger,
  title={Stronger generalization bounds for deep nets via a compression approach},
  author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  booktitle={International Conference on Machine Learning},
  pages={254--263},
  year={2018},
  organization={PMLR}
}

@article{lotfi2022pac,
  title={Pac-bayes compression bounds so tight that they can explain generalization},
  author={Lotfi, Sanae and Finzi, Marc and Kapoor, Sanyam and Potapczynski, Andres and Goldblum, Micah and Wilson, Andrew G},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31459--31473},
  year={2022}
}

@article{lotfi2023non,
  title={Non-vacuous generalization bounds for large language models},
  author={Lotfi, Sanae and Finzi, Marc and Kuang, Yilun and Rudner, Tim GJ and Goldblum, Micah and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2312.17173},
  year={2023}
}

@article{shen2023loose,
  title={Loose lips sink ships: Mitigating length bias in reinforcement learning from human feedback},
  author={Shen, Wei and Zheng, Rui and Zhan, Wenyu and Zhao, Jun and Dou, Shihan and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2310.05199},
  year={2023}
}

@article{park2024disentangling,
  title={Disentangling length from quality in direct preference optimization},
  author={Park, Ryan and Rafailov, Rafael and Ermon, Stefano and Finn, Chelsea},
  journal={arXiv preprint arXiv:2403.19159},
  year={2024}
}

@article{pal2024smaug,
  title={Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive},
  author={Pal, Arka and Karkhanis, Deep and Dooley, Samuel and Roberts, Manley and Naidu, Siddartha and White, Colin},
  journal={arXiv preprint arXiv:2402.13228},
  year={2024}
}

@article{rafailov2024r,
  title={From $ r $ to $ Q* $: Your Language Model is Secretly a Q-Function},
  author={Rafailov, Rafael and Hejna, Joey and Park, Ryan and Finn, Chelsea},
  journal={arXiv preprint arXiv:2404.12358},
  year={2024}
}

@article{izmailov2022feature,
  title={On feature learning in the presence of spurious correlations},
  author={Izmailov, Pavel and Kirichenko, Polina and Gruver, Nate and Wilson, Andrew G},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38516--38532},
  year={2022}
}

@article{fort2020deep,
  title={Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel},
  author={Fort, Stanislav and Dziugaite, Gintare Karolina and Paul, Mansheej and Kharaghani, Sepideh and Roy, Daniel M and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5850--5861},
  year={2020}
}

@inproceedings{yang2021tensor,
  title={Tensor programs iv: Feature learning in infinite-width neural networks},
  author={Yang, Greg and Hu, Edward J},
  booktitle={International Conference on Machine Learning},
  pages={11727--11737},
  year={2021},
  organization={PMLR}
}

@inproceedings{liu2020deep,
  title={Deep representation learning on long-tailed data: A learnable embedding augmentation perspective},
  author={Liu, Jialun and Sun, Yifan and Han, Chuchu and Dou, Zhaopeng and Li, Wenhui},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2970--2979},
  year={2020}
}

@article{mousavi2022neural,
  title={Neural networks efficiently learn low-dimensional representations with sgd},
  author={Mousavi-Hosseini, Alireza and Park, Sejun and Girotti, Manuela and Mitliagkas, Ioannis and Erdogdu, Murat A},
  journal={arXiv preprint arXiv:2209.14863},
  year={2022}
}

@article{aghajanyan2020intrinsic,
  title={Intrinsic dimensionality explains the effectiveness of language model fine-tuning},
  author={Aghajanyan, Armen and Zettlemoyer, Luke and Gupta, Sonal},
  journal={arXiv preprint arXiv:2012.13255},
  year={2020}
}

@article{kumar2022fine,
  title={Fine-tuning can distort pretrained features and underperform out-of-distribution},
  author={Kumar, Ananya and Raghunathan, Aditi and Jones, Robbie and Ma, Tengyu and Liang, Percy},
  journal={arXiv preprint arXiv:2202.10054},
  year={2022}
}

@article{tian2023scan,
  title={Scan and snap: Understanding training dynamics and token composition in 1-layer transformer},
  author={Tian, Yuandong and Wang, Yiping and Chen, Beidi and Du, Simon S},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={71911--71947},
  year={2023}
}

@inproceedings{attias2019improved,
  title={Improved generalization bounds for robust learning},
  author={Attias, Idan and Kontorovich, Aryeh and Mansour, Yishay},
  booktitle={Algorithmic Learning Theory},
  pages={162--183},
  year={2019},
  organization={PMLR}
}

@article{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  journal={arXiv preprint arXiv:1703.11008},
  year={2017}
}

@article{valle2020generalization,
  title={Generalization bounds for deep learning},
  author={Valle-P{\'e}rez, Guillermo and Louis, Ard A},
  journal={arXiv preprint arXiv:2012.04115},
  year={2020}
}

@article{lei2019data,
  title={Data-dependent generalization bounds for multi-class classification},
  author={Lei, Yunwen and Dogan, {\"U}r{\"u}n and Zhou, Ding-Xuan and Kloft, Marius},
  journal={IEEE Transactions on Information Theory},
  volume={65},
  number={5},
  pages={2995--3021},
  year={2019},
  publisher={IEEE}
}

  @inproceedings{im2024understanding,
      title={Understanding the Learning Dynamics of Alignment with Human Feedback}, 
      author={Shawn Im and Yixuan Li},
      booktitle = {International Conference on Machine Learning},
      year = {2024}
}

@article{liu2023statistical,
  title={Statistical rejection sampling improves preference optimization},
  author={Liu, Tianqi and Zhao, Yao and Joshi, Rishabh and Khalman, Misha and Saleh, Mohammad and Liu, Peter J and Liu, Jialu},
  journal={International Conference on Learning Representations},
  year={2024}
}


@article{ethayarajh2024kto,
  title={Kto: Model alignment as prospect theoretic optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306},
  year={2024}
}

@article{xiong2023gibbs,
  title={Gibbs sampling from human feedback: A provable kl-constrained framework for rlhf},
  author={Xiong, Wei and Dong, Hanze and Ye, Chenlu and Zhong, Han and Jiang, Nan and Zhang, Tong},
  journal={arXiv preprint arXiv:2312.11456},
  year={2023}
}

@misc{vonwerra2022trl,
  author = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang},
  title = {TRL: Transformer Reinforcement Learning},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/trl}}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@article{tang2024generalized,
  title={Generalized Preference Optimization: A Unified Approach to Offline Alignment},
  author={Tang, Yunhao and Guo, Zhaohan Daniel and Zheng, Zeyu and Calandriello, Daniele and Munos, R{\'e}mi and Rowland, Mark and Richemond, Pierre Harvey and Valko, Michal and Pires, Bernardo {\'A}vila and Piot, Bilal},
  journal={arXiv preprint arXiv:2402.05749},
  year={2024}
}

@book{wainwright2019high,
  title={High-dimensional statistics: A non-asymptotic viewpoint},
  author={Wainwright, Martin J},
  volume={48},
  year={2019},
  publisher={Cambridge university press}
}

@article{meng2024simpo,
  title={Simpo: Simple preference optimization with a reference-free reward},
  author={Meng, Yu and Xia, Mengzhou and Chen, Danqi},
  journal={arXiv preprint arXiv:2405.14734},
  year={2024}
}


@InProceedings{pmlr-v235-ethayarajh24a,
  title = 	 {Model Alignment as Prospect Theoretic Optimization},
  author =       {Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {12634--12651},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/ethayarajh24a/ethayarajh24a.pdf},

}


@InProceedings{pmlr-v235-zeng24c,
  title = 	 {Token-level Direct Preference Optimization},
  author =       {Zeng, Yongcheng and Liu, Guoqing and Ma, Weiyu and Yang, Ning and Zhang, Haifeng and Wang, Jun},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {58348--58365},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/zeng24c/zeng24c.pdf},
 
}


@InProceedings{pmlr-v235-calandriello24a,
  title = 	 {Human Alignment of Large Language Models through Online Preference Optimisation},
  author =       {Calandriello, Daniele and Guo, Zhaohan Daniel and Munos, Remi and Rowland, Mark and Tang, Yunhao and Avila Pires, Bernardo and Richemond, Pierre Harvey and Le Lan, Charline and Valko, Michal and Liu, Tianqi and Joshi, Rishabh and Zheng, Zeyu and Piot, Bilal},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {5409--5435},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/calandriello24a/calandriello24a.pdf},

}


@InProceedings{pmlr-v235-muldrew24a,
  title = 	 {Active Preference Learning for Large Language Models},
  author =       {Muldrew, William and Hayes, Peter and Zhang, Mingtian and Barber, David},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {36577--36590},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/muldrew24a/muldrew24a.pdf},
 
}


@InProceedings{pmlr-v235-ray-chowdhury24a,
  title = 	 {Provably Robust {DPO}: Aligning Language Models with Noisy Feedback},
  author =       {Ray Chowdhury, Sayak and Kini, Anush and Natarajan, Nagarajan},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {42258--42274},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/ray-chowdhury24a/ray-chowdhury24a.pdf},

}


@InProceedings{pmlr-v235-tajwar24a,
  title = 	 {Preference Fine-Tuning of {LLM}s Should Leverage Suboptimal, On-Policy Data},
  author =       {Tajwar, Fahim and Singh, Anikait and Sharma, Archit and Rafailov, Rafael and Schneider, Jeff and Xie, Tengyang and Ermon, Stefano and Finn, Chelsea and Kumar, Aviral},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {47441--47474},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/tajwar24a/tajwar24a.pdf},
 
}


@InProceedings{pmlr-v235-nika24a,
  title = 	 {Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences},
  author =       {Nika, Andi and Mandal, Debmalya and Kamalaruban, Parameswaran and Tzannetos, Georgios and Radanovic, Goran and Singla, Adish},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {38145--38186},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/nika24a/nika24a.pdf},

}


@InProceedings{pmlr-v235-xu24h,
  title = 	 {Is {DPO} Superior to {PPO} for {LLM} Alignment? {A} Comprehensive Study},
  author =       {Xu, Shusheng and Fu, Wei and Gao, Jiaxuan and Ye, Wenjie and Liu, Weilin and Mei, Zhiyu and Wang, Guangju and Yu, Chao and Wu, Yi},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {54983--54998},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/xu24h/xu24h.pdf},
 
}


@InProceedings{pmlr-v235-liu24r,
  title = 	 {Decoding-time Realignment of Language Models},
  author =       {Liu, Tianlin and Guo, Shangmin and Bianco, Leonardo and Calandriello, Daniele and Berthet, Quentin and Llinares-L\'{o}pez, Felipe and Hoffmann, Jessica and Dixon, Lucas and Valko, Michal and Blondel, Mathieu},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {31015--31031},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/liu24r/liu24r.pdf},
 
}

@inproceedings{xiong2024iterative,
  title={Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint},
  author={Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{gaolinear,
  title={Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback},
  author={Gao, Songyang and Ge, Qiming and Shen, Wei and Dou, Shihan and Ye, Junjie and Wang, Xiao and Zheng, Rui and Zou, Yicheng and Chen, Zhi and Yan, Hang and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{yangrewards,
  title={Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment},
  author={Yang, Rui and Pan, Xiaoman and Luo, Feng and Qiu, Shuang and Zhong, Han and Yu, Dong and Chen, Jianshu},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{chakrabortymaxmin,
  title={MaxMin-RLHF: Alignment with Diverse Human Preferences},
  author={Chakraborty, Souradip and Qiu, Jiahao and Yuan, Hui and Koppel, Alec and Manocha, Dinesh and Huang, Furong and Bedi, Amrit and Wang, Mengdi},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={International conference on machine learning},
  pages={1225--1234},
  year={2016},
  organization={PMLR}
}

@inproceedings{liu2017algorithmic,
  title={Algorithmic stability and hypothesis complexity},
  author={Liu, Tongliang and Lugosi, G{\'a}bor and Neu, Gergely and Tao, Dacheng},
  booktitle={International Conference on Machine Learning},
  pages={2159--2167},
  year={2017},
  organization={PMLR}
}

@inproceedings{chen2022human,
  title={Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation},
  author={Chen, Xiaoyu and Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Wang, Liwei},
  booktitle={International Conference on Machine Learning},
  pages={3773--3793},
  year={2022},
  organization={PMLR}
}

@inproceedings{zhu2023principled,
  title={Principled reinforcement learning with human feedback from pairwise or k-wise comparisons},
  author={Zhu, Banghua and Jordan, Michael and Jiao, Jiantao},
  booktitle={International Conference on Machine Learning},
  pages={43037--43067},
  year={2023},
  organization={PMLR}
}

@article{du2024haloscope,
  title={Haloscope: Harnessing unlabeled llm generations for hallucination detection},
  author={Du, Xuefeng and Xiao, Chaowei and Li, Yixuan},
  journal={arXiv preprint arXiv:2409.17504},
  year={2024}
}

@article{yi2024jailbreak,
  title={Jailbreak attacks and defenses against large language models: A survey},
  author={Yi, Sibo and Liu, Yule and Sun, Zhen and Cong, Tianshuo and He, Xinlei and Song, Jiaxing and Xu, Ke and Li, Qi},
  journal={arXiv preprint arXiv:2407.04295},
  year={2024}
}

@article{jain2023baseline,
  title={Baseline defenses for adversarial attacks against aligned language models},
  author={Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2309.00614},
  year={2023}
}

@article{malinin2020uncertainty,
  title={Uncertainty estimation in autoregressive structured prediction},
  author={Malinin, Andrey and Gales, Mark},
  journal={arXiv preprint arXiv:2002.07650},
  year={2020}
}

@article{kuhn2023semantic,
  title={Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation},
  author={Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian},
  journal={arXiv preprint arXiv:2302.09664},
  year={2023}
}

@article{duan2023shifting,
  title={Shifting attention to relevance: Towards the uncertainty estimation of large language models},
  author={Duan, Jinhao and Cheng, Hao and Wang, Shiqi and Wang, Chenan and Zavalny, Alex and Xu, Renjing and Kailkhura, Bhavya and Xu, Kaidi},
  journal={arXiv preprint arXiv:2307.01379},
  year={2023}
}

@article{su2024unsupervised,
  title={Unsupervised real-time hallucination detection based on the internal states of large language models},
  author={Su, Weihang and Wang, Changyue and Ai, Qingyao and Hu, Yiran and Wu, Zhijing and Zhou, Yujia and Liu, Yiqun},
  journal={arXiv preprint arXiv:2403.06448},
  year={2024}
}

@article{yin2024characterizing,
  title={Characterizing truthfulness in large language model generations with local intrinsic dimension},
  author={Yin, Fan and Srinivasa, Jayanth and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2402.18048},
  year={2024}
}

@article{chen2024inside,
  title={INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection},
  author={Chen, Chao and Liu, Kai and Chen, Ze and Gu, Yi and Wu, Yue and Tao, Mingyuan and Fu, Zhihang and Ye, Jieping},
  journal={arXiv preprint arXiv:2402.03744},
  year={2024}
}

@article{liu2023context,
  title={In-context vectors: Making in context learning more effective and controllable through latent space steering},
  author={Liu, Sheng and Ye, Haotian and Xing, Lei and Zou, James},
  journal={arXiv preprint arXiv:2311.06668},
  year={2023}
}

@article{lee2024programming,
  title={Programming refusal with conditional activation steering},
  author={Lee, Bruce W and Padhi, Inkit and Ramamurthy, Karthikeyan Natesan and Miehling, Erik and Dognin, Pierre and Nagireddy, Manish and Dhurandhar, Amit},
  journal={arXiv preprint arXiv:2409.05907},
  year={2024}
}

@article{cao2024personalized,
  title={Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization},
  author={Cao, Yuanpu and Zhang, Tianrong and Cao, Bochuan and Yin, Ziyi and Lin, Lu and Ma, Fenglong and Chen, Jinghui},
  journal={arXiv preprint arXiv:2406.00045},
  year={2024}
}

@article{stickland2024steering,
  title={Steering without side effects: Improving post-deployment control of language models},
  author={Stickland, Asa Cooper and Lyzhov, Alexander and Pfau, Jacob and Mahdi, Salsabila and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2406.15518},
  year={2024}
}

@article{wang2024adaptive,
  title={Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement Method for Diverse Hallucinations Categories},
  author={Wang, Tianlong and Jiao, Xianfeng and He, Yifan and Chen, Zhongzhi and Zhu, Yinghao and Chu, Xu and Gao, Junyi and Wang, Yasha and Ma, Liantao},
  journal={arXiv preprint arXiv:2406.00034},
  year={2024}
}

@article{cao2024nothing,
  title={Nothing in excess: Mitigating the exaggerated safety for llms via safety-conscious activation steering},
  author={Cao, Zouying and Yang, Yifei and Zhao, Hai},
  journal={arXiv preprint arXiv:2408.11491},
  year={2024}
}

@article{tan2024analyzing,
  title={Analyzing the generalization and reliability of steering vectors},
  author={Tan, Daniel and Chanin, David and Lynch, Aengus and Kanoulas, Dimitrios and Paige, Brooks and Garriga-Alonso, Adria and Kirk, Robert},
  journal={arXiv preprint arXiv:2407.12404},
  year={2024}
}

@article{subramani2022extracting,
  title={Extracting latent steering vectors from pretrained language models},
  author={Subramani, Nishant and Suresh, Nivedita and Peters, Matthew E},
  journal={arXiv preprint arXiv:2205.05124},
  year={2022}
}

@article{turner2023activation,
  title={Activation addition: Steering language models without optimization},
  author={Turner, Alexander Matt and Thiergart, Lisa and Leech, Gavin and Udell, David and Vazquez, Juan J and Mini, Ulisse and MacDiarmid, Monte},
  journal={arXiv e-prints},
  pages={arXiv--2308},
  year={2023}
}

@article{marks2023geometry,
  title={The geometry of truth: Emergent linear structure in large language model representations of true/false datasets},
  author={Marks, Samuel and Tegmark, Max},
  journal={arXiv preprint arXiv:2310.06824},
  year={2023}
}

@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{singhrepresentation,
  title={Representation Surgery: Theory and Practice of Affine Steering},
  author={Singh, Shashwat and Ravfogel, Shauli and Herzig, Jonathan and Aharoni, Roee and Cotterell, Ryan and Kumaraguru, Ponnurangam},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{chu2024causal,
  title={A causal explainable guardrails for large language models},
  author={Chu, Zhixuan and Wang, Yan and Li, Longfei and Wang, Zhibo and Qin, Zhan and Ren, Kui},
  booktitle={Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
  pages={1136--1150},
  year={2024}
}

