% \section{Design of \ours}
\section{Knowledge-oriented QA System Designs in \ours}
\ours focuses on the scenario where application users expect to obtain accurate answers based on knowledge from multiple homogeneous or heterogeneous sources.
We use the following hypothetical use case to demonstrate the challenges of building knowledge-oriented QA applications in practice.
The real use cases are presented in the following Section~\ref{sec:usecases}.

\mysubpar{\underline{Hypothetical use case.}}
If a developer of a GitHub repository wants to build  an LLM-based QA plugin based on his repository, he may need to consider the following potential questions:
\begin{enumerate}
    \item Technical questions that LLMs can answer if enough locally available knowledge is provided, such as the code snippet, API documents or tutorials available in this GitHub repository;
    \item Questions related to this GitHub but requiring knowledge beyond the locally-host one, such as whether there are any existing applications built based on this GitHub repository but implemented by a third party, whether there is any third-party solution for some issuing when using this repository, or whether is any third-party comments about this repository.
\end{enumerate}

Inspired by this hypothetical use case, we identify the following problems in building a knowledge-intensive QA system for practical use cases. 
Our system design focuses mainly on solving these problems.


\mypar{(1) Technical questions may not always be formulated clearly.}
There exists a gap between many research-oriented Retrieval-Augmented Generation (RAG) benchmarks and practical scenarios. 
When application users encounter technical issues, they often struggle to articulate their questions accurately, particularly in terms of using the correct terminology, for example, the meaning of an input parameter of a function in the context of the specific GitHub repository in the hypothetical use case above. 
Application users may require additional "warm-up" conversations to clarify and refine their inquiries in these cases. 
The missed information is expected to be filled in by understanding the conversation and knowledge contexts for better retrieval and QA performance.


\mypar{(2) Different questions may prefer different knowledge sources.}
Consider the first type of question in the hypothetical use case above. 
A pipeline built with processed GitHub repository data stored in a local vector database can be an economical and flexible solution that can solve most of the questions.
For the second type of question, collecting information through some online search APIs can provide LLMs with more comprehensive information to generate reliable and up-to-date answers.
In addition, different knowledge sources may also require different query preprocessing techniques and information post-processing operations.
For example, when using online search engines, it may be more important to propose a set of correct keywords based on the complete sentences in natural language, but complete but more organized sentences may be preferred when using embedding similarity-based search in vector databases.


\mypar{(3) Irrelevant information need to be filtered and adopted information need to be cited explicitly.}
The retrieval mechanism cannot always guarantee that all the retrieved content is helpful for answering the question.
The useless information need to be filtered out before being fed into the LLMs as the LLMs have limited effective context windows to process information.
On the other hand, existing LLMs can still generate hallucinated messages or cannot generate perfect answers for some complicated questions in conversations (e.g., code generation with very specific requirements).
As many commercial closed-source solutions (e.g., Perplexity\cite{perplexity}), providing information sources (e.g., links to GitHub files or websites) has been proven as a common way to accommodate such limitations, as the application users can further refer to those provided links for verification or proceeding their difficult task with ground-truth knowledge support.

\mypar{(4) Application users desire similar response latency as simple RAG solutions.}
Another critical metric that influences user experience is response latency.
Application users generally expect LLM applications to deliver responses with consistent speed, regardless of the complexity of the backend information processing. 
However, a significant portion of RAG application developers use only the available LLM APIs and cannot control how model inference optimization works.
This expectation introduces a significant challenge in scenarios where multi-stage information processing is required. 
Despite the inherent complexity of such systems, maintaining efficiency remains a crucial metric that must be considered.

\subsection{Modules and System Structure Overview}
\label{subsec:agents}
\mypar{Agentive Modularization.}
In our design, three modules serve as cores in \ours:  conversation context management, information retrieval, and final answer generation.
As in Figure~\ref{fig:overall}, we design a specialized agent class for each module: context manager, retrieval agent, and summarizer.

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/copilot_overall.png}
    \caption{\ours system with agentive modularization and configurable pipeline.}
    \label{fig:overall}
\end{figure}

\begin{itemize}
    \item
    \textit{Context manager}: 
    In everyday communication, conversations often rely heavily on contextual information. 
    The same is true in knowledge-intensive conversational QA tasks.
    In general, the context manager is designed to enrich the query by extracting missing information from the conversation context, ensuring its completeness.
    For instance, when \ours is used to provide QA services for the AgentScope~\cite{gao2024agentscope} GitHub repository, a user might query whether there is any multi-agent games application in the repository. 
    An initial response can be ``Yes. For example, there is a simulation for the game Werewolf that...''
    After that, the user may ask a follow-up question, ``Where can I find the code for it?'' In this case, the context manager must rely on context to determine that ``it'' refers to the Werewolf game in the AgentScope repository and enrich the user question for the following operations.

    \item 
    \textit{Retrieval agents}: 
    Each retrieval agent has its own information source(s) (e.g., similarity search with a local vector database or used by an online search engine). 
    One of the primary tasks of these agents is to further revise the query based on the agent's specific knowledge context and retrieve relative information.
    For example, when the retrieval agent has access to an online search engine, it rewrites the query into a set of keywords to align the search mechanism. 
    In contrast, when a retrieval agent relies primarily on a vector database with dense and sparse retrieval, a sentence enriched with conversation context and task background can improve the matching process during retrieval.

    \item 
    \textit{Summarizer}:
    The summarizer is designed to finalize the answer to a query by integrating information from the context manager and retrieval agents. 
    The goal of the summarizer is to generate content that is both faithful to the information provided by the retrieval agents (i.e., faithful) and appropriate within the context of the conversation (i.e., helpful).
\end{itemize}

\mypar{Configurable end-to-end pipeline.}
While three types of built-in agents are provided and their system prompt can be adjusted according to the application, there is still large room for configuration for different kinds of knowledge-intensive QA applications.
The following points are exposed for configuration by developers.

\begin{itemize}
    \item \textit{Whether to enable context manager.}
    While the context manager can enrich the user query and provide necessary context information for the final answer generation, analyzing and processing the context will take extra time for LLM.
    In some cases, where extremely low latency is required, the context manager can be disabled to further reduce latency.
    Instead, one can directly provide the entire conversation history for the summary and ask it to generate the final answer directly.
    Of course, if the context manager is disabled, the quality of the final answer may be hard to control if the conversation is complicated or the LLM is weaker.

    \item \textit{Which knowledge context query rewrite strategies are applied.}
    As discussed above, query rewrite can be a critical point for a knowledge-intensive application, helping to retrieve more accurate content from knowledge sources.
    \ours provides interfaces in the configuration with multiple build-in rewrite strategies (and rewrite prompts of some strategies) for the developers.
    More details will be discussed in Section~\ref{subsec:query-rewrite}.
    
    \item \textit{Which knowledge sources are used.}
    We provide built-in vector database and online search engine APIs as the two built-in knowledge sources for the application.
    For locally hosted knowledge, \ours inherits LlamaIndex~\cite{llamaindex} functions and is integrated into our system so that developers can switch different functions by just changing their knowledge configuration files.
    Details are discussed in Section~\ref{subsec:routing+retrieval}.
    
    \item \textit{Whether routing is required (with human intervention).}
    We also prove some flexibility for the routing mechanism.
    Besides the built-in mechanism routing, we allow developers to cast their human preferences for the routing by providing additional manually written mix-in text or scaling the importance of some weights.   
    More details are deferred to Section~\ref{subsec:routing+retrieval}.

\end{itemize}

With these key agents and configurable pipeline design working together, it becomes \ours.


% ================
\subsection{Context-based Query Enhancing}
\label{subsec:query-rewrite}

Knowledge-intensive QA applications usually have two kinds of ``contexts'' as essential factors to answer a question: the context of conversation and the context of knowledge sources.
Correspondingly, two types of agents in our design can rewrite the user query to enrich its semantic meaning with different information sources: context manager and retrieval agent.
In general, the context manager, designed to help the retrieval agent and summarize digesting conversation in advance, ensures that the query contains necessary information from the conversation context;
on the other hand, a retrieval agent is supposed to rewrite the query better to fit the retrieval mechanism in its knowledge source context.
While the rewrite mechanisms of context manager are fixed, the ones for retrieval agents are designed to be more flexible.
The following are details about context-based query enhancement.

\begin{figure}
\centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/conversation_rewrite.png}
        \caption{Conversation context rewrite mechanisms}
        \label{fig:conversation_rewrite}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/retrieval_rewrite.png}
        \caption{Knowledge context query rewrite mechanisms}
        \label{fig:retrieval_rewrite}
    \end{subfigure}
\caption{Query rewrite mechanisms in \ours.}
\label{fig:rewrite}
\end{figure}

\paragraph{Context manager: conversation-contextual query rewrite for retrieval agents.}
As discussed in Section~\ref{subsec:agents}, understanding the user's real intention behind a query heavily depends on the context of the conversation.
Without some keywords in the conversation context, knowledge retrieval can be pointless.
Thus, the goal of the query rewrite mechanism at this stage is to enrich the query with precise conversation context information.
1) The first point to consider is that some key information is lost in the query but can be obtained from the conversation history (e.g., the example mentioned in Section~\ref{subsec:agents}).
The context manager checks whether the query itself is ambiguous, including containing pronouns referring to terms appearing in the previous conversation.
2) The second consideration is that some users may rephrase their previous questions when they find that the answer provided by \ours is not satisfactory.
Therefore, the context manager also needs to revise the query with reflection.
3) Besides, this step is expected not to consume too much time as one of the intermediate steps in the pipeline.
Therefore, these two goals are integrated into the LLM prompt together with the conversation history and the LLM is expected to properly rewrite the question with necessary details filled and emphasized.
Considering the time constraint, we also recommend using a lightweight LLM to handle this task.



\paragraph{Context manager: conversation-contextual query rewrite for summarizer.}
The final step in generating the answer is another step that is heavily based on the context information.
In this step, the quality of the generated answer depends directly on the degree to which the LLM understands the relevant context.
Our observations indicate that while many LLMs perform well for requests with simple requirements, but they may struggle to handle more complex requests involving multiple sub-tasks (e.g., understanding the conversation and answering the current question with given context in one generation).
Besides, when the context is long (for example, a long answer is provided to the previous query), including the entire conversation history in the final step can unnecessarily consume the LLM's effective context length, reducing efficiency and potentially affecting output quality.
Nevertheless, the final answer generation requires more detailed information to generate the final answer, which may not always aligned with the rewrite goal for retrieval agents.
To address these challenges, a second function of the context manager is to perform a detailed and reliable analysis of the conversation context, and only the digested conversation context will be passed to the summarizer for final answer generation. 
This ensures that only the most relevant information is retained, improving the overall quality and efficiency of the final response generation.

More precisely, we prompt the context manager to generate answers with two fields: \texttt{analysis} and \texttt{indices\_of\_related\_messages}.
Generating \texttt{analysis} for the relation between the history and the current query can be considered with a similar effect to the Chain of Thoughts~\cite{wei2022cot}, which enables LLMs to generate answers more rationally.
After the \texttt{analysis}, LLMs should have higher confidence about which previous historical conversation piece is indeed related to the current one.
To provide more precise information for summarization, we also prompt the LLM to generate \texttt{indices\_of\_related\_messages}, with which we can directly extract the related messages by the indices.

\paragraph{Retrieval agent: knowledge-contextual query rewrite.}
In \ours, different retrieval agents have different knowledge sources with potentially heterogeneous data types.
In order to maximize the accuracy of information retrieval, different query rewrite prompts or even query rewrite algorithms can be employed at the agent level.

The built-in rewriting strategies are listed as follows and illustrated in Figure~\ref{fig:retrieval_rewrite}.
\begin{itemize}
    \item \texttt{Prompt rewrite (PR):} Rewrite the query following the instructions of the prompt.
    This mechanism allows developers to customize the prompt to LLM inject necessary information to the query, typically prior knowledge or understanding of the knowledge.
    
    \item \texttt{Retrieval rewrite (RR):} Rewrite the query with some knowledge content retrieved with the original query.
    An intuition of this method is that LLMs can help revise more accurate queries when providing some knowledge context.
    
    \item \texttt{Keyword rewrite (KR):} Extract only keywords from the original query for search engines.
    When using online search engine APIs, keywords are usually the best way to perform a search, as the redundant information in the original query may diverge the search to unrelated content.

    
    \item \texttt{HyDE rewrite~\cite{gao2023hyde} (HR):} Generate a paragraph with LLM's internal knowledge to answer the query, then take the paragraph as the new query.
    It is shown that in some cases, the embeddings of the LLM generated response without the interference of external knowledge can be more similar than the one of the raw query to the desired embeddings of the desired knowledge, so that it can improve the retrieval performance.

    
    \item \texttt{Translation rewrite (TR):} 
    Rewrite the query in the same language as the one specified in the configuration of the knowledge source.
    For the cases where the language in the knowledge source is different from the one of the query, it is believed that maybe mapping it first to the targeted language can provide a more accurate retrieval result.
\end{itemize}

All the above rewrite strategies are available and only require changing configuration files. 
However, developers can provide their own rewrite strategies and easily integrate them into their applications.

% ================
\subsection{Efficient Multi-source Information Retrieval}
\label{subsec:routing+retrieval}

% ================
\subsubsection{Knowledge retrieval mechanisms}
The retrieval agents play key roles in \ours because they have access to knowledge sources.
Their responsibility is to retrieve and provide relative knowledge to user queries.
The following built-in support for the different knowledge sources can be easily assigned to retrieval agents via configuration.
\begin{itemize}
    \item \textit{Local knowledge stored in vector databases (VDB).}
    Using local vector databases is the most popular and efficient method to construct local knowledge bases, especially after LLMs demonstrate their in-context learning capability.
    The embeddings of the knowledge in VDB only need to be computed once and used to construct the indexing.
    At inference time, it only requires the embedding model to encode a query into a vector for a similarity match.
    The storage of local knowledge and retrieval from local VDB has a few advantages.
    The first advantage is that it can utilize local knowledge, which is available locally, with little data privacy or data sovereignty concerns.
    A second advantage is that VDBs usually support a mixture of dense (embedding similarity) and sparse (BM25) retrieval, which can provide superior performance when semantic similarity is the only metric for retrieval.
    A third advantage is that even if the embedding relies on API, generating embedding for a query is usually much more affordable (even if it can be done locally) than calling the API of search engines.
    In \ours, users can choose to use LlamaIndex~\cite{llamaindex} built-in in-memory VDB or  Elasticsearch~\cite{elasticsearch2018elasticsearch}.

    \item \textit{Online search engines.}
    There are many advantages of using online search engines as an information source. 
    One is that it can guarantee the information is up-to-date.
    A second advantage is that the search engine algorithms rank the returned results, which consider many different factors (e.g., timeliness, popularity and authority with Internet-scale information).
    Therefore, even if it is more expensive than the local VDB method, online search engines are still one of the popular knowledge sources in many applications.
    The built-in support in \ours for search engine is built on Bing search~\cite{bing}, but developers can easily switch to Google search and others.
    

    \item \textit{Domain specific HTTP API.}
    Some websites provide in-site search APIs, which can return some domain-specific knowledge presented on the website but are not open to web crawlers.
    To leverage such APIs, we provide some flexible request-response parsing functions that facilitate the calling step and response parsing steps.
\end{itemize}


\paragraph{Discussion: raw text information or LLM digested information?}
Determining whether the retrieval agent should process the retrieved information before feeding it into the summarizer is tricky.
The answer should be given case by case, depending on many different factors.
For example: 1) Can the LLM used by the summarizer support processing long context information and still provide reasonably good reasoning capability?
2) Do we expect that the final answer of \ours can be generated efficiently (e.g., seeing the first token within ten seconds)?
If the answers to both questions are ``yes'', then letting the retrieval agents return the raw text and letting the summarizer process the raw information directly may be more appropriate because it can avoid additional latency because of LLM processing.
Besides, such a more straightforward approach can reduce the chance of introducing LLM hallucinations.
However, suppose only context-length-limited LLMs are available, or the available LLMs cannot provide acceptable performance in long-context reasoning; it may be a better solution to let each retrieval agent process the raw retrieved information, extract the key information first, and only pass the valuable information to the final answer generation step.

% =========== 
\subsubsection{Embedding clustering routing}
We expect each retrieval agent to take charge of one or a few knowledge sources.
The key guidance is that one can group similar knowledge sources to the same retrieval agent so that the knowledge context query rewrite can benefit the retrieval of all similar knowledge sources.
On the other hand, knowledge sources with very different topics or contents are supposed to be assigned to different retrieval agents.

However, a challenge is correctly selecting the best-fit retrieval agent(s) with the most relevant knowledge source.
Most existing multi-agent routing mechanisms rely on 1) manually created descriptions for the (functionalities of) agents and 2) using LLMs as decision-makers to decide which agents should be activated to provide knowledge.
However, such a combination is not suitable for routing between knowledge retrieval agents.
As a knowledge source may contain a large volume of knowledge, it can easily exceed the context length of any LLMs; nevertheless, it is challenging for human beings to summarize all the knowledge from a source comprehensively.
Meanwhile, using LLMs to select the knowledge source may not be a good idea in practice because 1) its output can be non-negligible randomness, which can make it hard to ensure consistency; 2) LLMs inference can be slow and restricted by the context length (i.e., not suitable for high-efficiency applications or context with long conversation).

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{figures/routing.png} % Path to your placeholder image
\caption{A simple visualization of the routing mechanism. Because the query embedding is closer to centroids in Agent A's knowledge domain, Agent A is roused to conduct knowledge retrieval.}
\label{fig:routing}
\end{figure}

\paragraph{Backbone routing mechanism.}
We adopt an algorithm similar to \citep{feijie}.
Figure~\ref{fig:routing} gives a simple visualization of the core idea.
The key idea is to utilize the embeddings of the knowledge in each agent.
In addition to being used in retrieval, the embeddings of the knowledge (e.g., text chunks) are also perfect representations of that knowledge.
Therefore, the embeddings of chunks of knowledge are first clustered in each agent, and the centroids of the cluster are considered to be \emph{synopses} of the knowledge.
When routing, an embedding of the query needs to be used for similarity search with the centroids of all retrieval agents. 
Only the agents with top-$K$ similar centroids to the query embedding are activated for exact knowledge retrieval.

\paragraph{Manual mix-in.}
Different from \citep{feijie}, we also need to consider retrieval agents using online search APIs as the knowledge source, which has limited or even no local knowledge to guide the selection of knowledge.
In addition, the method of in \citep{feijie} is completely data-driven.
While complete data-driven is a desired feature, it also means difficult to impose human preferences when it comes to real practice.

To resolve such inconveniences, we allow developers to either 1) provide the initial description or 2) provide a set of typical knowledge chunks (e.g., QA pairs or chunks of plain text) as mix-in to enrich the routing.
Such manual descriptions are also encoded into embeddings. 
When routing, the similarity considered becomes a weighted average of the similarity score of the local knowledge pieces to the query and one of the manual mix-ins to the query.
When developers have higher confidence in their manual description and want it to dominate the process of the selection, a higher weight can be assigned to the similarity score of the manual description; if the developer wants to save effort and have plenty of coherent knowledge in each agent's knowledge base, a higher weight can be assigned to the similarity score of the local knowledge pieces.
If the knowledge source is not local, the manual description can serve as the only centroid of the knowledge.


\paragraph{Score scaling.}
In practice, the similarity scores when comparing the same query with different types of documents can vary in various ranges.
For example, when matching a natural language query with Python code knowledge, it usually has lower scores than matching with natural language documents, even if the key relative information related to the query is indeed in the Python code.
To handle such cases, we employ a simple but effective strategy that allows users to scale some similarity score related to specific types of knowledge, either up or down.



\subsection{Summarization}

\paragraph{Reranking for filtering.}
A common strategy to avoid the false-negative cases (i.e., high-relevance knowledge is not retrieved) is to retrieve slightly overwhelming information that is larger than the context length of the LLMs.
In \ours, since multiple retrieval agents can provide different knowledge sources, the summarizer can receive overwhelming information that cannot fit into the effective context window of LLMs.
However, although embedding similarity matching mechanisms can efficiently retrieve a lot of relevant information, they are not metrics to rank or filter information due to the following important issues.
1) Some retrieval agents may even never compute the similarity score, such as those using the online search engine as the knowledge source.
2) High similarity scores can contain false positive signals as they are computed in compressed vector space.
3) The similarity scores from different resources may not be comparable because the retrieval agents may rewrite the query for some purpose, so the similarity scores used in retrieval are actually based on different queries.
4) The nature of knowledge can affect the outcome, such as the data format, chunk size, etc.

Reciprocal rank fusion (RRF)~\cite{cormack2009reciprocal} can be a model-free statistic that may help in reranking and filtering. 
However, it is unlikely to have duplicated knowledge pieces retrieved from different knowledge sources.
Therefore, RRF may not be that useful for reranking in multi-source knowledge retrieval.
Instead, we use the reranking model to sort the raw retrieved fragments.
Although a reranking model can introduce additional computation cost, it is a more reliable and general method for our tasks.


\paragraph{Citation generation.}
Citations and references can provide additional confidence for the generated content as users can verify the answer by looking at the references provided.
However, generating citations is a challenging task for LLMs \cite{gao2023enabling}.
In \ours, our goal is to generate citations efficiently without training a specific model or introducing a significant regression in latency or reasoning performance for the final answer.

We tested several approaches. 
Our initial is \emph{one-step}, with which the LLMs are prompted to finish the following tasks in a single answer: 1) analyze which chunks can help answer the query question; 2) generate the answer to the query question; 3) select the indices of the related chunks.
Then, the final answer is to assemble the generated answer and extract the reference with the indices.
However, such an approach has several limitations.
First, the LLM must have strong reasoning and context-processing ability.
Second, the LLM must generate structure output (e.g. in JSON format) from which different information can be extracted, which means that it is impossible to use the stream mode of the LLMs to provide a low-latency experience for users and introduce additional task failure risk as LLMs cannot always correctly format its output.
Third, as the retrieved knowledge is usually in chunks, they can be from different or the same information source (e.g., the same paper).
Therefore, it requires additional effort to handle or distinguish duplications.

\textit{Solution: A look-back approach.}
To bypass the above problems, we design a look-back strategy for citation generation.
The entire generation utilizes the stream mode provided by many LLM APIs or local inference frameworks and consists of two stages.
The first stage is to prompt the LLM to generate an answer to the query question based on the retrieved knowledge.
The answer is presented directly to the users.
The second stage is to provide the generated answer together with the retrieved knowledge to the LLM and let it output the reference of the knowledge used to generate answers.
Such a strategy is robust with weaker requirements on the reasoning capability of LLMs.
Besides, as the content is generated in stream mode, the user's experience can be significantly improved as the first token they observed at the same time it is generated, and only a small pause will be observed after the answer is generated and waiting for the citation generation.


\subsection{Optimizing Pipeline for Efficient Execution}
Many LLM-based applications seek low response latency, which poses a challenge for multi-stage applications like \ours.
More specifically, since multiple sub-tasks exist, LLM will inevitably be used to generate multiple times and introduce some intermediate result waiting time. 
Therefore, efficiently arranging the execution requires parallelizing as much LLM usage as possible.
In order to optimize efficiency, the execution of \ours can be executed in parallel with requests for asynchronous model APIs as follows.
\begin{itemize}
    \item \textbf{Parallelization 1: Query ingest.}
    At this stage, two functions are executed in parallel.
    One is the \emph{context manager conversation context query rewrite for retrieval agents}, which fills in missing key information for the current query question based on the whole conversation history so that the following retrieval stage can use more accurate information.
    The other is \emph{query routing}, deciding which retrieval agents are the correct ones to execute.

    \item \textbf{Parallelization 2: Knowledge retrieval and context analysis.}
    At this stage, each retrieval agent obtains the query enriched with the necessary context information of the conversation.
    At this stage, each agent can rewrite a knowledge-context query to match the knowledge context and retrieve knowledge.
    Meanwhile, the context manager analyzes the context of the previous conversation and generates distilled information to ensure the final answer fits the conversation.
    All of the above agent operations are performed in parallel.
    At the end of this stage, the retrieval information and the conversation context analysis results are shared with the summarizer for the final response generation.
\end{itemize}
