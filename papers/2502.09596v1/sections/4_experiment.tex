\section{Use Cases}
\label{sec:usecases}

\begin{table}
\centering
\begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{3.5cm}|p{3.5cm}|}
\hline
\textbf{Use case} & With context manager & Routing / Preference adjust & Offline knowledge source(s) & Online knowledge source(s) \\ \hline
AgentScope QA   & Yes    & ON / No    &AgentScope tutorial, code, examples, API docs and FQA set & -      \\ \hline
ModelScope QA     & Yes      & ON / Yes   & ModelScope tutorials, and 5 GitHub repos  &ModelScope offical article, models, datasets,   \\ \hline
Olympic Bot    & No      & OFF/-     & - & Olympic events      \\ \hline
\end{tabular}
\caption{Use Case Configurations}
\label{tab:use_cases}
\end{table}

\mypar{Sytem implementation.}
\ours is implemented based on a multi-agent framework, AgentScope~\cite{gao2024agentscope}.
At the agent level, the agents inherits from the built-in agents in AgentScope but with extra features specialized for \ours, such as the methods supporting the efficient routing mechanism and some external knowledge management and retrieval functions.
At the pipeline level, the agents receive input and pass process information via AgentScope's message objects; some of the parallel execution stages in the pipeline are tailored specifically in \ours for easy management and code management. 

In the following, we demonstrate how we configure \ours to build different QA applications for three different tasks.



\subsection{Small scale application: AgentScope QA}\par

As a proof-of-concept example, we first present an example with offline knowledge sources with different data formats and topics as a starting point to demonstrate how \ours works.

\begin{wrapfigure}{r}{0.25\textwidth}
  \vspace{-0.95cm}
  \begin{center}
    \includegraphics[width=0.24\textwidth]{figures/as_qa.jpg}
  \end{center}
  \vspace{-0.2cm}
  \caption{Use case in Q\&A group of AgentScope}
  \label{fig:as_use_case}
  \vspace{-1.5cm}
\end{wrapfigure}


\mypar{Goals.}
In this use case, we adapt \ours to help answer questions about AgentScope's GitHub repository.
The expectation for this application is to serve as a chatbot in a Q\&A group for developers building their multi-agent application with AgentScope framework, providing an accurate response in time to the raised questions.
It is observed that the most common questions fall in the following categories:
\begin{itemize}
    \item \textit{Preliminary project questions.}
    As the Q\&A group is open for everyone, some potential users of AgentScope are also presented in the group. 
    Their questions are usually about the feature of AgentScope, its advantages compared with other similar open-source frameworks, and the feasibility of using AgentScope for their task.
    Once some of them decide to initiate their project with AgentScope, they may be looking for whether there are already demonstration examples for tasks similar to theirs provided in the GitHub repository for reference.

    \item \textit{Coding or debugging related questions.}
    Another majority of questions in the Q\&A group appear to be developers using AgentScope but encounter issues in their development process.
    For example, they may seek clarifications of two different agent classes or help to solve the execution errors in their AgentScope-based applications.
\end{itemize}

\mypar{Application Configuration.}
To satisfy the above goals and provide reliable answers to the questions in Q\&A group, we configure our \ours in terms of knowledge and execution as the following.

\begin{itemize}
    \item \textit{Knowledge sources configuration.}
    The knowledge bases equipped to the agents in this use case span the content related to the core functions of library: \underline{the tutorial} (Markdown files), \underline{code} (mainly Python code), \underline{API documents} (processed into text files).
    The \underline{examples} in the AgentScope library, which serve as good demonstration examples for beginners, are also included as knowledge of how to use the AgentScope framework with code and description in Markdown format.
    Besides, some of the frequently asked questions beyond the scope of the repository (e.g., comparison with other frameworks) are gathered manually, and are summarized and filled with appropriate answers to form a \underline{FQA set} as an additional knowledge source to the repository. 
    All of this knowledge is processed (i.e., chunking and generating embeddings) and stored in a vector database for query by a knowledge configuration file.
    Each type of knowledge is assigned to a retrieval agent.

    With the routing mechanism, the preliminary project questions are usually answered with the knowledge shared by the agents charging tutorial, examples, and FQA set, while answering the coding or debugging questions will depend on information from tutorial, code, and examples.

    \item \textit{Pipeline.}
    This application is configured with all three types of agents activated: context manager, retrieval agent, and summarizer.
    The context manager is used to help understand the real intention of the user in a conversation context. 
    The retrieval agents are configured with the \texttt{Prompt rewrite} module with prompts design for each agent according to their equipped knowledge.
\end{itemize}

Figure~\ref{fig:as_use_case} is a screenshot of a real QA with \ours AgentScope QA application in the DingTalk Q\&A group.


\subsection{Larger scale: ModelScope QA}
\mypar{Goal.}
While serving as a Q\&A chatbot similar to the AgentScope use case, the spectrum of questions to be handled is significantly larger than the ones in Agentscope.
Modelscope community is a platform of open-sourced machine learning models, datasets, training/finetuning libraries and applications built with LLMs and other models.
It is expected a chatbot to provide an accurate initial answers potentially based on all these kinds of the knowledges from different sources.

\mypar{Application Configuration.}
To helpfully serve the users in the community, we highlight the specialties in configure as the following.
\begin{itemize}
    \item \textit{Knowledge sources configuration.}
    The knowledge sources used in these applications can be categorized into two types, online and offline.
    
    The online knowledge sources include \underline{model} and \underline{dataset} information, \underline{official articles} about the latest open-source community technical progress.
    These knowledge sources are achieved by Bing search API with different constraints, i.e., restricted to domain of available models/datasets/articles on the website.
    It is configured to use a commercial search engine instead of in-site search because the results of Bing can also be used to provide related knowledge that ranked by more sophisticated mechanisms beyond text similarity, such as recommending the most popular Text-to-Image models.

    The offline knowledge sources include the \underline{tutorial} documents and \underline{eight different GitHub repositories} affiliated with ModelScope.
    The tutorial covers knowledge about how to use models, datasets and computation resources available on \url{modelscope.cn}, and the repositories contain code files and repository-level instructions.
    Compared with the online knowledge sources, these knowledge sources can be hosted and retrieved locally because the retrieval standard is more 

    \item \textit{Pipeline configuration.}
    Generally speaking, the pipeline configuration is similar to the AgentScope use case, involving all three kinds of agents.
    But key difference is in the routine mechanism configuration. 
    We provide some manual description mix-in for the routing mechanism to bias some selections manually.
    For example, both the tutorial and model knowledge source contains the information of some models (or models with similar names); however, the information from model knowledge source is more up-to-date than the one from the tutorial. 
    Therefore, we add some manual mix-in to bias the "model recommendation" type question to use model knowledge source rather than tutorial.
    Because some knowledge sources are considered more reliable and official (i.e., official articles and tutorial), we set a scaling factor greater than 1 to make the information pieces from those sources have a higher chance of being selected.
    
\end{itemize}

Figure~\ref{fig:ms_use_csae} shows three QA pairs from the ModelScope QA using different knowledge sources.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/ms_online.png}
        \caption{Online sources usage}
        \label{fig:ms_online}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/ms_offline.png}
        \caption{Offline sources usage}
        \label{fig:ms_offline}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/ms_mix.png}
        \caption{Mixture usage}
        \label{fig:ms_mix}
    \end{subfigure}
    \caption{Three demonstration QA pairs using different knowledge resources in \href{https://www.modelscope.cn/studios/AI-ModelScope/modelscope_copilot_beta}{Modelscope QA}.}
    \label{fig:ms_use_csae}
\end{figure}


% \subsection{Character Chatbot}

\subsection{Turbo Scale: Olympic Bot on Weibo}

\mypar{Goal.}
During the Paris 2024 Olympic Games, \ours serves as the core of the back-end algorithm to generate auto-comments for the posts related to the Olympics\footnote{\url{https://weibo.com/u/7929611818}}.
The posts and comments of this Weibo bot are supposed to focus only on Olympic Games, including the news and historic Olympic events.
However, there are many other bots performing on Weibo, but there is a restriction that no more than two bots can reply to the same Weibo post.
Therefore, there is a race condition and the response generation efficiency becomes a key point in this scenario.
To encounter such challenge, our configuration of \ours needs to make the following changes.

\mypar{Application Configuration.}
In order to fulfill the requirements, \ours is configured as the following.
\begin{itemize}
    \item \textit{Knowledge sources configuration.}
    We configure a retrieval agent to use the specialized \underline{search API} for Olympic related events.
    The APIs perform searches similar to public search engines using keywords to match related information.

    \item \textit{Pipeline configuration.}
    To extremely reduce response latency, the pipeline configuration becomes simple. 
    The context manager is deactivated, and the only retrieval agent will perform a keyword query rewrite only.
    The full conversation history and the retrieved knowledge will be directly fed to the summarizer to generate the final answer.
\end{itemize}

With such knowledge and pipeline configuration, the end-to-end latency is reduced to less than 10 seconds per post or comment, while the responses are still very informative and popular.