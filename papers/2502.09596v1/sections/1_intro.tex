\section{Introduction}
Large language models (LLMs) have had a profound impact on various aspects of people's lives, particularly as the foundational technology behind conversational applications such as chatbots. 
These models have become indispensable as virtual assistants, offering powerful capabilities for various tasks, including addressing common-sense queries, generating summaries for academic papers~\cite{lin-etal-2024-arxiv}, and solving programming challenges and tasks~\cite{jimenezswe}.  
Despite their impressive functionality, LLMs are of some limitations. 
Issues such as hallucinations and the inability to provide the most up-to-date information or private knowledge hinder their reliability in directly serving for knowledge-intensive applications. 
These shortcomings can be mitigated by integrating LLMs with external information in the input context~\cite{min2022rethinking, wei2022emergent}.
One notable approach is retrieval-augmented generation (RAG) techniques~\cite{izacard2020leveraging, borgeaud2022improving}, which enhances LLMs by equipping them with retrieval capabilities, allows LLMs to address questions that exceed the scope of their pre-trained internal knowledge. 
RAG has proven highly effective in improving performance on question-answering (QA) tasks emphasizing faithfulness to truths, showcasing its potential to bridge the gap between static pre-trained knowledge and dynamic, context-specific information.

While many real-world applications have adopted RAG techniques~\cite{perplexity, kimi}, open-source frameworks have also emerged to facilitate the adaptation of RAG to a wide range of tasks~\cite{llamaindex, langchain} for the public to hold RAG application services themselves with local data. 
While these open-source RAG frameworks provide convenient starting points for building RAG-based applications, there remain significant opportunities for improvement, especially in more practical and complicated scenarios, e.g., efficient multi-source knowledge retrieval, which provides primary motivations for this paper.

\mypar{Challenge 1.} 
While developing a basic chatbot using LLM APIs is relatively straightforward, the complexity increases significantly when the conversation requires intensive external knowledge.
A user's question may contain only partial information, while the rest is in the conversation history, such as containing pronouns referring to an object in the previous question.
Using such a question with unclear pronouns as a query to retrieve knowledge will significantly limit knowledge retrieval accuracy, and answering such unclear questions may increase the risk of hallucination.
Another common case is that when the application is built in a specific knowledge context (e.g., QA based on a GitHub repository), the user may tend to omit some meaningful keywords in their questions, without which the LLMs may misuse its internal knowledge to generate improper answers.
Therefore, such applications require mechanisms to clarify and detail the user questions into queries that contain sufficient information for retrieval knowledge and final answer generation. 

\mypar{Challenge 2.} 
Unlike commercial products~\cite{perplexity, kimi}, building a complete powerful data collection and preprocessing pipeline may be too heavy for tasks relying on private knowledge.
Therefore, a lightweight and configurable retrieval mechanism may be a more desirable solution for developers to utilize their local knowledge from different sources with heterogeneous topics and formats.
Besides, when it comes to running time, accurately answering questions with multi-source knowledge also relies on correctly using a subset of knowledge.
However, there is no existing solution for automatically and efficiently handling the knowledge selection while providing flexibility for taking human intervention.

\mypar{Challenge 3.}
Moreover, as a QA application, the final answer generation is arguably the most critical step.
Retrieved content from multiple knowledge sources may easily consume LLM's effective context window.
To maximize the answer performance, correctly identifying the useful retrieved content and organizing them for question answering are the keys.
But it may be hard to tell how one retrieved knowledge piece from a source is more helpful than the other.
Moreover, for the user experience, providing references is believed to be an effective operation to increase the trust level of the generated answer.
However, it is unclear how to achieve it efficiently with minimum costs.

\mypar{Challenge 4.}
Finally, in addition to correctly answering questions, users may expect the system's response time (i.e., latency) to be as short as possible.
Considering the system's needs for necessary query processing, handling multi-source knowledge retrieval, and answer generation, optimization of execution efficiency requires careful design.


In this paper, we present the design and implementation of an open-source solution, \ours, based on AgentScope~\cite{gao2024agentscope} framework. 
The solution is developed with the following key properties to address the challenges:
\begin{enumerate}
    \item 
    \emph{Context management and query rewrite mechanisms to enhance retrieval accuracy and conversation coherency.}
    \ours provides a built-in \emph{conversation context} query enrichment mechanism that can fill the semantic gaps in the user's current question based on a long conversation history.
    A set of configurable \emph{knowledge context} query rewrite mechanisms is provided for the developers to further adjust the query to fully unleash the power of the retrieval mechanisms and match the most desirable knowledge.       
    
    \item 
    \emph{Efficient routing and retrieval with heterogeneous knowledge sources.}
    \ours supports various knowledge sources with different data topics.
    \ours is also equipped with an efficient routing mechanism that can ensure when a query comes, only the most appropriate knowledge sources will be used to minimize the overall cost while ensuring the answer quality.
    The routing mechanism is also configurable in the sense that developers can provide extra manually written mix-in or scale the score to bias the selection according to their preference and actual use cases.

    \item 
    \emph{Simple yet effective answer and reference generation.}
    We provide a filtering mechanism that is compatible with content from different knowledge sources so that the LLMs can effectively use their window size to generate the final answer.
    Recognizing that providing citations and references is critical for ensuring trust and transparency in knowledge-intensive applications, \ours also implements a straightforward yet reliable strategy for generating citations without requiring model training or introducing additional latency for the central answer generation.

    \item 
    \emph{Configurable and low-latency multi-agent pipeline.}
    \ours employs a multi-agent pipeline architecture. 
    While the pipeline is configurable with three different types of agents, we provide an optimized configuration with parallelization to maximize efficiency and resource utilization.
\end{enumerate}

In summary, this paper introduces \ours, an open-source framework designed to address the challenges of integrating RAG techniques into real-world applications. 
By enabling versatile query enhancement mechanisms with conversation and knowledge context, handling heterogeneous knowledge sources, and ensuring reliable citation generation, \ours seeks to improve the utility and trustworthiness of RAG-based systems. 
Through the practical use cases of \ours, we demonstrate its robustness and adaptability to building effective QA chatbots and other RAG-driven applications, advancing the state of the art in this rapidly evolving domain.