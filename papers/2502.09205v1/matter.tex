%\renewcommand{\textbf}{\uppercase}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% %%% LaTeX Template for AAMAS-2023 (based on sample-sigconf.tex)
% %%% Prepared by the AAMAS-2023 Program Chairs based on the version from AAMAS-2022.
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% %%% Start your document with the \documentclass command.
% %%% Use the first variant below for the final paper.
% %%% Use the second variant below for submission.
%
% \documentclass[letterpaper]{article} % DO NOT CHANGE THIS
% \usepackage[submission]{aaai23}  % DO NOT CHANGE THIS
%
% \usepackage{times}
% \usepackage{helvet,courier}
% % \usepackage{courier,txfonts}
% \usepackage[italic]{mathastext}
%
% \usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
% \usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
% \frenchspacing  % DO NOT CHANGE THIS
% \setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
% \setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
% %
% \usepackage{algorithm}
% \usepackage{algorithmic,latexsym,amsmath}
%
% %
% % These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
% \usepackage{newfloat}
% \usepackage{listings}
% \DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
% \lstset{%
% 	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
% 	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
% 	aboveskip=0pt,belowskip=0pt,%
% 	showstringspaces=false,tabsize=2,breaklines=true}
% \floatstyle{ruled}
% \newfloat{listing}{tb}{lst}{}
% \floatname{listing}{Listing}
% %
% % Keep the \pdfinfo as shown here. There's no need
% % for you to add the /Title and /Author tags.
% \pdfinfo{
% /TemplateVersion (2023.1)
% }
%
% % DISALLOWED PACKAGES
% % \usepackage{authblk} -- This package is specifically forbidden
% % \usepackage{balance} -- This package is specifically forbidden
% % \usepackage{color (if used in text)
% % \usepackage{CJK} -- This package is specifically forbidden
% % \usepackage{float} -- This package is specifically forbidden
% % \usepackage{flushend} -- This package is specifically forbidden
% % \usepackage{fontenc} -- This package is specifically forbidden
% % \usepackage{fullpage} -- This package is specifically forbidden
% % \usepackage{geometry} -- This package is specifically forbidden
% % \usepackage{grffile} -- This package is specifically forbidden
% % \usepackage{hyperref} -- This package is specifically forbidden
% % \usepackage{navigator} -- This package is specifically forbidden
% % (or any other package that embeds links such as navigator or hyperref)
% % \indentfirst} -- This package is specifically forbidden
% % \layout} -- This package is specifically forbidden
% % \multicol} -- This package is specifically forbidden
% % \nameref} -- This package is specifically forbidden
% % \usepackage{savetrees} -- This package is specifically forbidden
% % \usepackage{setspace} -- This package is specifically forbidden
% % \usepackage{stfloats} -- This package is specifically forbidden
% % \usepackage{tabu} -- This package is specifically forbidden
% % \usepackage{titlesec} -- This package is specifically forbidden
% % \usepackage{tocbibind} -- This package is specifically forbidden
% % \usepackage{ulem} -- This package is specifically forbidden
% % \usepackage{wrapfig} -- This package is specifically forbidden
% % DISALLOWED COMMANDS
% % \nocopyright -- Your paper will not be published if you use this command
% % \addtolength -- This command may not be used
% % \balance -- This command may not be used
% % \baselinestretch -- Your paper will not be published if you use this command
% % \clearpage -- No page breaks of any kind may be used for the final version of your paper
% % \columnsep -- This command may not be used
% % \newpage -- No page breaks of any kind may be used for the final version of your paper
% % \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% % \pagestyle -- This command may not be used
% % \tiny -- This is not an acceptable font size.
% % \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% % \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference
%
% \setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.
%
% % The file aaai23.sty is the style file for AAAI Press
% % proceedings, working notes, and technical reports.
% %
%
% % Title
%
% % Your title must be in mixed case, not sentence case.
% % That means all verbs (including short verbs like be, is, using,and go),
% % nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% % articles, conjunctions, and prepositions are lower case unless they
% % directly follow a colon or long dash
% \title{Counterfactual Explanations as Plans: \\ A Situation Calculus Formalisation}
% \author{
%    ~
% }
% \affiliations{
%    ~
% }
%
% %Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \iffalse
% \title{My Publication Title --- Single Author}
% \author {
%     Author Name
% }
% \affiliations{
%     Affiliation\\
%     Affiliation Line 2\\
%     name@example.com
% }
% \fi
%
% \iffalse
% %Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
% \title{My Publication Title --- Multiple Authors}
% \author {
%     % Authors
%     First Author Name,\textsuperscript{\rm 1}
%     Second Author Name, \textsuperscript{\rm 2}
%     Third Author Name \textsuperscript{\rm 1}
% }
% \affiliations {
%     % Affiliations
%     \textsuperscript{\rm 1} Affiliation 1\\
%     \textsuperscript{\rm 2} Affiliation 2\\
%     firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
% }
% \fi
%
%
% % REMOVE THIS: bibentry
% % This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
% \usepackage{bibentry}
% % END REMOVE bibentry
% \input defn
% \begin{document}
%
% \maketitle
%
% % \begin{abstract}
% % AAAI creates proceedings, working notes, and technical reports directly from electronic source furnished by the authors. To ensure that all papers in the publication have a uniform appearance, authors must adhere to the following instructions.
% % \end{abstract}
%
% \begin{abstract} There has been considerable recent interest in explainability in AI, especially with black-box machine learning models.  As correctly observed by the planning community, when the application at hand is not a single-shot decision or prediction, but a sequence of actions that depend on observations, a richer notion of explanations are desirable.
%
% In this paper, we look to provide a formal account of ``counterfactual explanations," based in terms of action sequences. We then show that this naturally leads to an account of model reconciliation, which might take the form of the user correcting the agent's model, or suggesting actions to the agent's plan. For this, we will need to articulate what is true versus what is known, and we appeal to a modal fragment of the situation calculus to formalise these intuitions. We consider various settings: the agent knowing partial truths, weakened truths and having false beliefs, and show that our definitions easily generalize to these different settings.


% Fairness in machine learning is of considerable interest in the recent years owing to the propensity of algorithms trained on historical data to amplify and perpetuate historical biases. In this paper, we argue for a formal reconstruction of fairness definitions, not so much to replace existing definitions but to ground their application in an epistemic setting and allow for rich environmental modelling. Consequently we look into three notions: fairness through unawareness, demographic parity and counterfactual fairness, and formalise these in the epistemic situation calculus.

% \end{abstract}

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

% \keywords{Logic, Fairness}
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% %%% Include any author-defined commands here.
%
% \newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}
%
% \newcommand{\cal}{\mathcal}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \begin{document}
%
% 	\pagestyle{fancy}
% 	\fancyhead{}

	%%% The next command prints the information defined in the preamble.

	\maketitle

	

\section{Introduction}

There has been considerable recent interest in explainability in AI, especially with black-box machine learning models, given applications in  credit-risk analysis,  insurance pricing and self-driving cars.  Much of this focus is on single-shot decision or prediction, but as correctly observed by the automated planning community \cite{fox2017explainable,cashmore2019towards,borgo2018towards}, in applications involving sequence of actions and observations and conditional plans that depend on observations, a richer notion of explanations are desirable. Fox et al.~\cite{fox2017explainable}, for instance, argue that understanding why a certain action was chosen (and not some other), why one sequence is more optimal than another, etc, are all desired constructs in explanations. 

Despite all this attention, there is yet to emerge a theory on how exactly to frame explanations in a general way. One candidate is the notable line of work on model reconciliation \cite{sreedharan2018handling}. The idea is that the agent might have incomplete or even false information about the world, and the user can advise the agent by correcting the agent’s model, or suggesting actions to the agent’s plan. The latter move is in the spirit of human-aware AI \cite{kambhampati2020challenges}. But such an idea is inherently \emph{epistemic}, and this brings the explainable AI literature closer to  \emph{epistemic planning} \cite{baral_et_al:DR:2017:8285}. Many  recent threads of work have tried to explicate this connection. 

In \cite{shvo2022resolving}, for example, the idea of discrepancy and resolving such discrepancy is studied. Using the epistemic logic fragment from \cite{muise-aaai-15}, where so-called proper epistemic knowledge bases are chosen for initial plan states that allow only for modal literals, discrepancy emerges when (in a dynamic logic-like language) \( \theory \models [\delta] \know\sub i \phi \) but \( \theory \not \models [\delta] \know\sub j \phi \). Given a goal \( \phi, \) an action sequence \( \delta, \) background knowledge \( \theory \) for agents \( i \) and \( j \) which include dynamic axioms, it turns out that \( i \) believes that \( \phi \) is true but \( j \) does not. So the resolution is to find some course of action \( \delta' \) that ensures that either \( \theory \models [\delta'] (\know \sub i \phi \land \know \sub j \phi) \) or that \( \theory \models [\delta'] (\know \sub i \neg \phi \land \know\sub j \neg \phi) \). That is, they both jointly believe after \( \delta' \) that \( \phi \) is made true, or that \( \phi \) is made false. And as pointed out in \cite{shvo2022resolving}, articulating the difference between beliefs and ground truth is needed for clarity. 

Likewise, the works on \emph{contrastive explanations} \cite{krarup2019model} as well as the credulous/skeptical semantics for model reconciliation from  \cite{vasileiou2022logic} are related. In the former, the ``why'' question is tackled by offering to add or remove actions from the current plan. In the latter, a notion of credulous and skeptical entailment is suggested as a way to keep updated mental states consistent in service of explanations.
Credulous entailment is when a single belief state suffices to check the validity and achievement of a plan, and skeptical entailment  when every belief state is involved.  The problem of ``explanation generation'', then,  between the user's theory \( \Sigma \) and the agent's theory \( \Sigma' \) is triggered when \( \Sigma\models [\delta] \phi \) for some goal \( \phi \) after sequence \( \delta \) but \( \Sigma' \not\models [\delta] \phi. \) This necessitates the updating of \( \Sigma' \) to \( \Sigma'' \)  such that \( \Sigma'' \models [\delta] \phi. \) Note that, even though  skeptical/credulous entailment involves belief states, the formalism itself is not epistemic. 
Perhaps it does not need to be in some limited cases, but ultimately a distinction between truth in the real-world and beliefs provides clarity on how the agent's model needs to be adjusted when plans do not work, either because its missing actions or entertaining incomplete/false truths. 

The reader may also surmise that there is clearly some relationship between these proposals, but it is not spelt out. In fact, no clear logical justification is given as why the definitions are reasonable in the first place.
As we mentioned, a theory on how exactly to frame explanations in such epistemic settings is yet to emerge. 





%  Recently, a case along these lines have been made in \cite{shvo2022resolving}, for example.
%
% Our view is that a single theory for explanations that encompasses all the desiderata from \cite{fox2017explainable} is not likely, much like how there is no consensus on what type of explanations suffice for stakeholders generally for ML models   \cite{bhatt2020explainable,omeiza2021explanations,bhatt2020machine}.

What we seek to do in this paper is to develop and formalize ``counterfactual explanations'' over plans in the presence of both physical and sensing actions. Counterfactual explanations in machine learning are widely popular \cite{wachter2017counterfactual}, because they provide an intuitive account of ``what if'' and ``what could have been'', which helps us realize alternative worlds where a desired outcome might be achieved.\footnote{These notions will vary slightly in the dynamic version. We will focus on identifying the course of actions that can toggle the outcome (i.e., ``which plan"), and the information that is necessary to enable a  property (i.e., ``what should be known").}  Our account  can be seen as attempting to establish a logical relationship between contrastive explanations and knowledge based updates for model reconciliation and/or discrepancy. In fact, this relationship is a very simple one: it can be seen as a counterfactual! The ``why,'' ``what if'', and other such ``wh''-questions can be interpreted as counterfactuals \cite{pearl2009causality}. In the static setting, we are simply interested in an alterative world where some properties are different. In the dynamic setting, we might also be interested in an alternative plan that achieves a different outcome, very much in the spirit of contrastive plans. To our knowledge, there is no general account in the literature on counterfactual explanations as plans.

This brings us to the choice of the representation language. As already discussed above, we need to identify, in the first instance, a knowledge representation language for articulating things like what is true, what is known, whether something is not known, and whether something is falsely believed. This will give us an opportunity to formalize explanations in an epistemically adequate representation language. Although proposals such as \cite{muise-aaai-15,shvo2022resolving} might be perfectly reasonable, and might even be preferred for an automated planning account, but they do come with various syntactic stipulations that might affect the properties of the logic (e.g., disjunctive entailments). We believe the characterization is more easily stated and readability  improved by considering a general knowledge representation language, but nothing in the formalisation necessitates one choice of language over another. In fact, the underlying implementation can involve any of the recent proposals from the epistemic planning literature, e.g.,  \cite{muise-aaai-15,son2001formalizing}, of which we think \cite{shvo2022resolving} is particularly fitting. Our notion of counterfactual plans could be easily adapted from the algorithm of that work. 



% is two-fold. On the one hand, given the emerging literature on epistemic planning, theory of mind and model reconciliation, we argue that it does make sense to
%
%  Arguably, the closest in spirit is the work on \emph{contrastive explanations} \cite{krarup2019model}, and recent work on the logical foundations of explainable planning \cite{vasileiou2022logic}. In the former, the ``why'' question is tackled by offering to add or remove actions from the current plan. In the latter, a notion of credulous and skeptical entailment is suggested as a way to keep updated mental states consistent in service of explanations. The problem of ``explanation generation'' between the user's theory \( \Sigma \) and the agent's theory \( \Sigma' \) is triggered when \( \Sigma\models \phi \) for some goal \( \phi \) but \( \Sigma' \not\models\phi. \) This necessitates the updating of \( \Sigma' \) to \( \Sigma'' \)  such that \( \Sigma'' \models \phi. \)
%
% In both works, no clear semantic justification is given as why the definitions are reasonable. And because they are not framed in an epistemic logic, one does not immediately see that this is essentially an epistemic problem formulation, where articulating the difference between beliefs and ground truth is needed for clarity \cite{shvo2022resolving}.

 % This is why we explore the topic in a logic, but as is usual \cite{classen2007towards}, the downside is that many operational parts of practical planning like look at plan optimality, resources and time \cite{krarup2019model} will be ignored for the scope of this paper. We also note that we will not seek to reproduce the concepts from works such as \cite{krarup2019model,sreedharan2018handling} exactly but rather study the simplest well-defined formulation, and use examples to illustrate them.\\



\textbf{Dimensions to formalisation} 
When attempting to formalise counterfactual (CF) explanations, as we shall show, there are multiple dimensions under which a definition can be explored. In the simplest case, given a plan \( \delta \) that achieves \( \phi \), a CF explanation might be a plan \( \delta' \) that negates \( \phi \). An interpretation could be as follows: if a plan ends up denying the loan to individual \( b, \) find  a plan that approves the loan for \( b \), but ensure that it is ``close'' to the original plan. This roughly captures counterfactuals in ML: given a data point \( x \) that has label \( y \), find  a data point \( x' \) that is minimally distant from \( x \) and has label  \( \neg y. \) We can further concretize this by explicating what closeness means, and whether additional  constraints can be provided so that a diverse range of \( x' \) are found \cite{mothilal2020explaining}. 

But when viewed through the lens of an interaction between an automated agent and a human user, in the sense of human-guided planning \cite{kambhampati2020challenges,sreedharan2018handling}, the framework becomes  richer. We will consider the case where the user assists in achieving goals. So this leads to a broader notion of counterfactuals: consider an alternate history or additional knowledge such that the goal now becomes true. This will require us to articulate the difference between what is true in the real world (the user's knowledge) versus what is believed (by the agent). Interestingly, this does not require a modelling language with multiple agents because the user's knowledge can serve  as proxy for truth in the real world.\footnote{This leads to a single-agent version formulation of, for example, the discrepancy condition from \cite{shvo2022resolving} in that it emerges whenever the user/root agent/real world is one where \( \theory \models [\delta] \phi \) but as far as the agent is concerned: \( \theory \not \models [\delta] \know \phi \). An account with multi-agent modal operators is possible in a straightforward way, using e.g., \cite{shapiro2002cognitive,DBLP:conf/kr/KellyP08} and \cite{Belle:2014aa} in particular, which is a many-agent extension to the language used in this paper.}
We consider various settings: the agent knowing partial truths, weakened truths and having false beliefs, and show that our definitions easily generalize to these different settings. The discrepancy model from \cite{shvo2022resolving}, the skeptical/credulous entailment model from \cite{vasileiou2022logic}, among others, can be seen as variations of this more general recipe in epistemic logic. We focus on the mathematical aspects here, but as mentioned, implementations from works such as \cite{shvo2022resolving} can be adapted for generating CF explanations as plans.

 % appreciate that the underlying implementation can involve any of the recent proposals from the planning literature, e.g.,  \cite{kominis2015beliefs,shvo2022resolving,muise-aaai-15,petrick2004extending,reifsteck2019epistemic,son2001formalizing}.

This then brings us to the choice of the formal language for our exposition. We choose the situation calculus \cite{reiter2001knowledge}, which has been well-explored for formalizing the semantics of planning problems \cite{pednault1989adl,reiter2001knowledge,Fritz:2008uq,DBLP:conf/aaai/Levesque96}. But because we are interested in nested beliefs, we explore a (newer) modal variant, the logic \( \es \) \cite{LakemeyerLevesque2004}, which provides a simpler semantics for planners \cite{classen2007towards}, as well closely mirrors the semantics of proposals such as  dynamic logic. Perhaps what makes it most interesting that under some conditions reasoning about actions and knowledge can reduce to non-modal (first-order or propositional) reasoning \cite{LakemeyerLevesque2004}, a feature we feel has not been considered extensively in the planning community. Our ideas 
do not hinge on this language, and so any planning language that helps us reason about truth, knowledge, actions and sensing should suffice. 


% conclude and related work, add some more points to intro? 

% what we get 
% what language - exercise in FO language 
% what is true vs what is known
% multiple agents is not necessary 
% conceptual - do nto commit, many candidates 


% In some of these applications, the prevalence of machine learning techniques has raised concerns about the potential for learned algorithms to become biased against certain groups. This issue is of particular concern in cases when algorithms are used to make decisions that could have far-reaching consequences for individuals (for example in recidivism prediction) \cite{Recidivism_study, ProPublica}. Attributes which the algorithm should be ``fair'' with respect to are typically referred to as \textit{protected} attributes. The values to these are often hidden from the view of the decision maker (whether automated or human).
% There are multiple different potential fields that might qualify as protected attributes in a given situation, including ethnicity, sex, age, nationality and marital status  \cite{Learning_Fair_Reps}. Ideally, such attributes should not affect any prediction made by  ``fair''  algorithms. However, even in cases where it is clear which attributes should be protected, there are multiple (and often mutually exclusive) definitions of what it means for an algorithm to be unbiased with respect to these attributes, and there is disagreement on what is most appropriate  within the academic community \cite{Fairness_through_awareness, Counterfactual_Fairness, Disparate_Impact_&_Historical}.
%
% Even if algorithms are not provided with information about the protected attribute directly, there is still the potential for algorithms to discriminate on the basis of \textit{proxy} variables, which may contain information about the protected attribute \cite{Causal_Reasoning}. For example, the name of an individual may not be regarded as a protected attribute, but it will likely contain information about an individual's ethnicity or sex. Unfair algorithmic biases can also be introduced if an algorithm is trained on historical data, particularly in cases where the training labels were allocated on the basis of human discretion \cite{Disparate_Impact_&_Historical, tick_cross_paper}. In such a scenario, the algorithm can inherit the historical biases of those responsible for allocating the labels, by actively discriminating against attributes prevalent in a particular group \cite{Disparate_Impact_&_Historical, Learning_Fair_Reps}. Similarly, an algorithm may be unfair if (at test time) any input attribute was decided by a human (e.g. risk-score of reoffence based on expert's opinion). In such a scenario, the outcome of the prediction will inevitably be influenced by the biases of the individual allocating the score if not adjusted accordingly.
%
%
%
% In recent years, the topic of fairness has become an increasingly important  issue within the field of machine learning, both in academic circles \cite{Counterfactual_Fairness, Causal_Reasoning, Equality_of_opportunity}, and more recently in the public and political domains \cite{ProPublica, White_House_Report}. However, even amid pressing concerns that algorithms currently in use may exhibit racial biases, there remains a lack of agreement about how to effectively implement fairness, given the complex socio-technical situations that such applications are deployed in and the  background knowledge and context needed to assess the impact of outcomes (e.g., denying a loan to someone in need, accidentally harming a pedestrian or passenger).
%
% To address such issues broadly, an interesting argument has been loosely put forward by the symbolic community: by assuming a rich enough understanding of the application domain, we can encode machine ethics in a formal language. Of course, with recent advances in statistical relational learning, neuro-symbolic AI and inductive logic programming \cite{raedt2016statistical,muggleton2012ilp}, it is possible to integrate low-level pattern recognition based on  sensory data  with high-level formal specifications. For example, the \emph{Hera} project \cite{lindner2017hera} allows for the implementation of several kinds of (rule-based) moral theory to be captured. \emph{Geneth} \cite{anderson2014geneth} uses inductive logic programming to create generalised moral principles from the judgements of ethicists about particular ethical dilemmas, with the system's performance being evaluated using an \emph{ethical Turing test}. On the formalisation side, study of  consequentialist and deontological concepts has long been a favored topic in the knowledge representation community \cite{conway2013deontological,sep-ethics-deontological,czelakowski1997action,hooker2018toward}, that can be further coupled against notions of beliefs, desires and intentions \cite{broersen2001boid,georgeff1998belief}. Closer to the thrust of this paper, \cite{pagnucco2021epistemic} formalize consequentialist and deontological ethical principles in terms of ``desirable'' states in the epistemic situation calculus, and \cite{classen2020dyadic} formalize obligations using complex programs.
%
% Our thesis, in essence, is this: complementing the vibrant work in the ML community, it is worthwhile to study ethical notions in formal languages. This serves three broad objectives: \begin{enumerate}
% 	\item We can identify what the system needs to know versus what is simply true \cite{DBLP:journals/tocl/Reiter01,DBLP:journals/jair/HalpernM14} and better articulate how this knowledge should impact the agent's choices. It is worth remarking that epistemic logic has served as the foundation for investigating the impact of knowledge on plans and protocols \cite{DBLP:conf/aaai/Levesque96,DBLP:journals/sLogica/LesperanceLLS00,DBLP:conf/tark/HalpernPR09}.
% 	\item We implicitly understand that we can further condition actions against background knowledge (such as ontologies and databases), as well as  notions such as intentions and obligations \cite{Sardina:2010aa}.
% 	\item We can position the system's actions not simply as a single-shot decision or prediction, as is usual in the ML literature, but as a sequence of complex events that depend on observations and can involve loops and recursion: that is, in the form of programs \cite{Levesque97-Golog}.
% \end{enumerate}
%
% It would beyond the scope of a single paper to illustrate the interplay between the three objectives, and in this sense, our paper is more of a ``research agenda'' that is advocated for, and less about a single technical result, or a demonstration of a single application. In particular, what we seek to do is a formal reconstruction of some fairness definitions, not so much to replace existing definitions but to ground their application in an epistemic setting. Consequently we look into three notions: fairness through unawareness, demographic parity and counterfactual fairness, and formalise these in the epistemic situation calculus \cite{citeulike:528170}.
%
% Finally, let us remark that although there has been considerable work on fomralizing moral rules, there is no work (as far as we are aware) on the formalization of fairness and bias. In that regard,  we will not seek to simply retrofit existing ML notions in a logical language (which might be a mildly interesting but likely tedious exercise in a probabilistic logic \cite{halpern2017reasoning,Bacchus1999171}); rather we aim to identify the principles and emphasize the relationship between knowledge, actions and outcomes when discussing fairness and bias. In fact, we will rely on the standard epistemic situation calculus with knowledge (and not degrees of belief) and exact (i.e., noise-free) sensing. Thus, our formalizations should be seen as being loosely inspired by existing notions, rather than an exact reconstruction. What we hope our paper provides is an early insight on the promise of unifying KR languages and ML notions, leaving lots of scope for future work, including considering a probabilistic logic, axiomatizing a real-world application against commonsensical knowledge, and so on.
%
%
%
%
% 	\section{Existing Notion} % (fold)
% 	\label{sec:existing_notion}
%
% 	% section existing_notion (end)
%
%
% %
% % \section{Existing notions} % (fold)
% % \label{sec:context_and_existing_notions}
%
%
% One recent episode serves to highlight the the impact of machine learning models, and necessitates the appropriate application of ethical constraints and de-biasing to prediction modeels. Pro-Publica, a US-based entity specialising in not-for-profit journalism, published an article suggesting that an algorithm widely used to predict the probability of re-offense in criminals was biased against black offenders \cite{ProPublica}. The article raised concerns about the fairness and efficacy of the Correctional Offender Management Profiling for Alternative Sanctions (or COMPAS) algorithm, which is widely used in the US justice system \cite{ProPublica}. Their article received criticism from both members of the academic community \cite{ProPublica_Criticism_Academics} and Northpointe, the company who created the algorithm \cite{Northpointe_refute}. Their primary complaint concerned the metric used by ProPublica to measure discrimination; the original concerns about racial bias were based mainly on the discrepancy in false positive and false negative rates between black and white offenders \cite{ProPublica_Criticism_Academics, ProPublica}. This analysis was critiqued in part because the authors of \cite{ProPublica} failed to appreciate that the outcome of the algorithm was not a prediction of future behaviour per se, but actually a risk allocation \cite{ProPublica_Criticism_Academics}. ProPublica defined a false-positive as any individual considered ``high-risk'' who did not re-offend \cite{ProPublica}, whereas in reality the risk categories were simply an indication of reoffence probability. Moreover, Northpointe and others illustrated that the algorithm satisfied a calibration criterion \cite{Northpointe_refute, Recidivism_study}, which they regarded as a more appropriate measure of fairness in this particular instance. This calibration requirement states that if an algorithmic process maps a series of inputs $X$ to a score $s$, such that:
% \begin{equation}
%     f:X\rightarrow{}s,
% \end{equation}
% then the probability of re-offence (denoted $r=1$) of individuals at every value of $s$ is independent of the value of the protected attribute $a_p$ (which in this case is race) \cite{Recidivism_study}, i.e.
% \begin{equation}
%     P(r=1|a_p,s)=P(r=1|s) \hspace{2mm} \forall \hspace{1mm} s,a_p.
% \end{equation}
%
%  Northpointe claim that their algorithm is fair, since individuals allocated the same risk level are equally likely to re-offend regardless of ethnicity \cite{Northpointe_refute}.
%
% This episode illustrates the situation where there is the potential for injustices to arise as a consequence of bias on the basis of training over sensitive factors and variables. Ideally, such factors should be outside the purview of the algorithm’s decision making process. Moreover, it is apparent that many criteria used to determine fairness are mutually incompatible \cite{friedler2016possibility}, and that caution should be used when selecting the criterion for a specific situation. This can lead to significant discrepancies in the interpretation of the same model and its outcomes.
%
% As discussed, we will not seek to simply retrofit existing ML notions in a logical language; rather we aim to identify the principles and emphasize the provenance of unfair actions in complex events. Nonetheless, it is useful to emphasize a few popular definitions that might guide our intuition.
%
% \subsection{Fairness Through Unawareness}
% Fairness through unawareness (FTU) is the simplest definition of fairness; as its name suggests, under this definition an algorithm is “fair” if it is unaware of the protected attribute $a_p$ of a particular individual when making a prediction \cite{Counterfactual_Fairness}.
% \begin{definition}{Fairness Through Unawareness}\newline
%     For some set of attributes $X$ any mapping
%     \begin{equation}
%         f:X\xrightarrow{}\hat{y}
%     \end{equation}
%     where $a_p \not \in X$ satisfies fairness through unawareness \cite{Counterfactual_Fairness}. (Assume \( y \) denotes the true label.)
% \end{definition}
%
% This prevents the algorithm learning direct bias on the basis of the protected attribute, but does not prevent indirect bias, which the algorithm can learn by exploiting the relationship between other training variables and the protected attribute \cite{Discrimination_Aware_DM, Equality_of_opportunity}. Moreover, if any of the training attributes are allocated by humans there is the potential for bias to be introduced.
%
% \subsection{Statistical Measures of Fairness}
% Rather than defining fairness in terms of the scope of the training data, much of the existing literature instead assesses whether an algorithm is fair on the basis of a number of statistical criteria that depend on the predictions made by the algorithm \cite{Equality_of_opportunity, Counterfactual_Fairness, Learning_Fair_Reps}. One widely used and simple criterion is demographic parity (DP). In the case that both the predicted outcome and protected attribute $a_p$ are both binary variables, a classifier is said to satisfy predictive parity \cite{Equality_of_opportunity} if: \[P(\hat y=1|a_p=1) = P(\hat y=1|a_p=0).\] By this definition, a classifier is considered fair if it is equally likely to make a positive prediction  regardless of the value of the protected attribute $a_p$.
%
%
% \subsection{Fairness and the Individual}
%
% Another problem with statistical measures is that, provided that the criterion is satisfied, an algorithm will be judged to be fair regardless of the impact on individuals. For example, consider a classifier which predicts debt default risk for individuals. Even if a `fair' algorithm incorrectly classifies certain low-risk individuals as high risk, it can still be deemed `fair' according to the statistical definitions, despite the fact that the incorrectly classified individuals may regard themselves as having been treated unfairly. Therefore, these statistical metrics may be regarded as incomplete, even if they are useful in certain circumstances \cite{Fairness_through_awareness}.
%
%
%
% In view of these limitations, various works have introduced fairness metrics which aim to ensure that individuals are treated fairly, rather than simply considering the statistical impact on the population as a whole \cite{Fairness_through_awareness, Counterfactual_Fairness}. Counterfactual fairness (CF), for example,  was proposed as a fairness criterion in  \cite{Counterfactual_Fairness}. The fundamental principle behind this definition of fairness is that the outcome of the algorithm's prediction should not be altered if different individuals within the sample training set were allocated different values for their protected attributes \cite{Counterfactual_Fairness}. This criterion is written  in the following form:
% \begin{equation}
%     P(\hat{y}_{A_p \leftarrow a_p}|A=a, X=x) = P(\hat{y}_{A_p \leftarrow a_p'}|A=a, X=x) \hspace{1mm}\forall y,a'.
% \end{equation}
%
%
%
% The notation $\hat{y} \leftarrow _{A_p \leftarrow a_p}$ is understood as ``the value of $\hat{y}$ if $A_p$ had taken the value $a_p$'' \cite{Counterfactual_Fairness}.
%
% In the following sections, we will introduce a logical language for knowledge, actions and sensing, and attempt to deconstruct such notions in the language.
%
% % section context_and_existing_notions (end)
%

	
%\clearpage 
\section{A Logic for Knowledge and Action} % (fold)
	\label{sec:reconstructing_the_epistemic_situation_calculus}

% {\bf TBD: We use the objective fragment for CFs, but the epistemic version for model reconciliation and user-guided explanations.}
%
% {\bf TBD: similar to linear dynamic logic, and LTL etc., and so we prefer this to classical sit calc}


We now introduce the logic \( \es \) \cite{LakemeyerLevesque2004}.\footnote{Our choice of language may seem unusual, but it is worth noting that this language is a modal syntactic variant of the classical epistemic situation that is better geared for reasoning about knowledge \cite{Lakemeyer:2011:SCU:1897346.1897552}. But more importantly, it can be shown that reasoning about actions and knowledge reduces to first-order reasoning via the so-called regression and representation theorems \cite{LakemeyerLevesque2004}. (For space reasons, we do not discuss such matters further here.) There are, of course, many works explicating the links between the situation calculus and logic programming; see \cite{reiter2001knowledge} for starters.} It is an epistemic logic, but we only need the objective fragment for formalizing counterfactual explanations as plans. When we consider the more elaborate notion of reconciliation-type explanations where both a user and an agent is necessary, we will use the full language. 



	The non-modal fragment of \( \es \)  consists of standard first-order logic with $=$. That is, connectives \( \set{\land, \forall, \neg} \),  syntactic 
	  abbreviations \( \set{\exists,\equiv, \supset} \) defined from those connectives, and a supply of variables variables \( \set{x,y,\ldots, u, v, \ldots} \). Different to the standard syntax, however, is the inclusion of (countably many) \emph{standard names} (or simply, names) for both objects and actions \(\R,  \)  which will allow a simple, substitutional interpretation for \( \forall \) and \( \exists. \) These can be thought of as special extra constants that satisfy the unique name assumption and an infinitary version of domain closure.
  
	Like in the situation calculus, to model immutable properties, we assume rigid predicates and functions, such as {\it $IsPlant(x)$} and {\it $father(x)$} respectively. To model changing properties, \( \es \) includes fluent predicates and functions of every arity, such as {\it $Broken(x)$}  and {\it $height(x)$}. Note that there is no longer a situation term as an argument in these symbols to distinguish the fluents from the rigids. For example, \( \es \) also includes a distinguished  fluent predicates \( \poss \) and \( SF \) to model the executability of actions and capture sensing outcomes respectively, but they are now a unary predicates. Terms and formulas are constructed as usual. The set of ground atoms \( \P \) are  obtained by applying all object names in \( \R \) to the predicates in the language. 

	There are four modal operators in \( \es \): \( [a], \Box, \know \) and \( \oknow. \) For any formula \( \alpha, \) we read \( [a]\alpha, \Box\alpha \) and \( \know\alpha \) as ``$\alpha$ holds after $a$", ``$\alpha$ holds after any sequence of actions" and ``\( \alpha \) is known,'' respectively. Moreover, \( \oknow\alpha \) is to be read as ``\( \alpha \) is only-known.'' Given a sequence \( \delta = a\sub 1 \cdots a\sub k, \) we write \( 
[\delta]\alpha \) to mean \( [a\sub 1] \cdots [a\sub k]\alpha. \) We write \( a\cdot \delta \cdot a' \) to mean \( [a]\cdot [a\sub 1] \cdots [a\sub k] \cdot [a']. \)
	
	In classical  situation calculus parlance, we would use \( [a]\alpha \) to capture successor situations as properties that are true after an action in terms of  the current state of affairs. 
Together with the \( \Box \) modality, which  allows to capture quantification over situations and histories, basic action theories can be defined. Like in the classical approach, one is interested in the entailments of the basic action theory. \\


% We use the following syntactic sugar: we write \( \wh\alpha \) (read as ``knowing whether \( \alpha \)") to mean \( \know\alpha \lor \know\neg\alpha \). So the agent  knows \( \alpha \) or knows that \( \alpha \) is false. Given a sequence \( \delta = a\sub 1 \cdots a\sub k, \) we write \(
% [\delta]\alpha \) to mean \( [a\sub 1] \cdots [a\sub k]\alpha. \)

	% To model dynamical domains, \( \es \) will include fluent predicates of every arity, such as {\it Broken(x)} and {\it NextTo(x,y)}, but note that there is no longer a situation term as an argument in these symbols.
	%    Moreover, following the classical situation calculus, \( \es \) includes a distinguished (unary) fluent predicate \( \poss \) to model the executability of actions. Likewise, \( \es \) will include function symbols of every arity. In particular, rigid nullary symbols, such as {\it obj5} and {\it joe42}, might capture the objects of the domain and rigid \( k \)-ary symbols, such as {\it drop(obj5)} and {\it move(obj5,loc1,loc2)}, might capture actions.
	%
	% We construct terms as usual too: every variable is a term, and by applying terms \( t\sub 1,\ldots, t\sub k \) in a \( k \)-ary function symbol \( f \), we obtain the term \( f(t\sub 1, \ldots, t\sub k) \). In the interest of simplicity, we do not distinguish between
	%
	% From a quantificational viewpoint, we let \( \R \) denote the set of all ground rigid terms. Basically, \( \R \) is considered to be isomorphic to the domain of discourse, which will allow a simple, substitutional interpretation for \( \forall \) and \( \exists. \)
	% The set of ground atoms \( \P \) are  obtained by applying all ground terms \( \R \) to the predicates in the language.

	\textbf{Semantics} % (fold)
	\label{sub:semantics}
	Recall that in the simplest setup of the possible-worlds semantics, worlds mapped propositions to \( \set{0,1} \), capturing the (current) state of affairs. \( \es \) is based on the very same idea, but extended to dynamical systems. So, suppose a world maps \( \P \) and \( \Z \) to \( \set{0,1} \).\footnote{We need to extend the mapping to additionally interpret fluent functions and rigid symbols, omitted here for simplicity.} Here,  \( \Z \) is the set of all finite sequences of action names, including the empty sequence \( \lan\ran. \) Let \( \W \) be the set of all worlds, and \( e\subseteq \W \) be the \emph{epistemic state}. By a \emph{model}, we mean a triple \( (e,w,z) \) 
	 where \( z\in \Z. \)


	Intuitively, each world can be thought of a situation calculus tree, denoting the properties true initially but also after every sequence of actions. \( \W \) is then the set of all such trees. 
	Given a triple \( (e,w,z) \), \( w \) denotes the real world, and \( z \) the actions executed so far. Interestingly, 
	\( e \) captures the accessibility relation between worlds, but by modeling the relation as a set, we are enabling positive and negative introspection using a simple technical device. 

	% In particular, letting the real world \( w\in e \) means that \( \know \alpha \supset \alpha \) is valid, as we shall see.



	% TODOs BAT fOR ES?



	To account for how knowledge changes after (noise-free)  sensing, one defines \( w' \sim\sub z w \), which is to be read as saying  ``\( w' \) and \( w \) agree on the sensing for \( z \)'', as follows: 



	\begin{itemize}
		\item if \( z=\lan\ran, \) \( w' \sim\sub {z} w \) for every \( w' \); and
		 \item \( w'\sim \sub {z\cdot a} w \) iff \( w'\sim\sub {z} w, \) and \( w'[SF(a),z] = w[SF(a),z]. \)
	\end{itemize}
	
	%\( w'[\poss(a),z]=1 \)


 
	This is saying that initially, we would consider all worlds compatible, but after actions, we would need the world \( w' \) to agree on sensing outcomes. The reader might notice that this is clearly a reworking of the successor state axiom for the knowledge fluent in \cite{citeulike:528170}. 


	With this, we get a simply account for truth. We define the satisfaction of formulas wrt the triple \( (e,w,z) \), and the semantics is defined inductively:
	 \begin{itemize}
		\item \( e,w,z\models p \) iff \( p \) is an atom and \( w[p,z] =1 \);
		\item \( e,w,z \models \alpha\land\beta \) iff \( e,w,z\models \alpha \) and \( e,w,z\models \beta; \) 

		\item \( e,w,z\models \neg\alpha \) iff \( e,w,z\not\models \alpha; \) 

		\item \( e,w,z\models \forall x\alpha \) iff \( e,w,z\models \alpha^x_n \) for all \( n \in \R; \)

		\item \( e,w,z\models [a]\alpha \) iff \( e,w,z\cdot a\models \alpha; \)
			\item \( e,w,z\models \Box \alpha \) iff \( e,w,z\cdot z' \models \alpha \) for all \( z' \in \Z \); 
		
		\item \( e,w, z\models\know\alpha \) iff for all \( w' \sim \sub z w, \) if \( w'\in e, \) \( e,w',z\models\alpha \);  
		
		 \item \( e,w, z\models\oknow\alpha \) iff for all \( w' \sim \sub z w, \)  \( w'\in e, \) iff \( e,w',z\models\alpha \). 
	
	\end{itemize} 
  
	To define entailment for a logical theory, we write \( \D \models \alpha \) (read as ``$\D$ entails $\alpha$'') to mean for every \( M= (e,w, \lan\ran) \), if \( M\models \alpha' \) for all \( \alpha' \in \D,  \) then \( M\models \alpha. \) We write \( \models \alpha \) (read as ``$\alpha$ is valid'') to mean \( \set{} \models \alpha. \) \\


  

 
 % subsection properties (end)
	\textbf{Properties} % (fold)
	\label{sub:properties} 
	Let us first begin by observing that given a model \( (e,w,z), \)  we do not require \( w\in e.  \) It is easy to show that if we stipulated the inclusion of the real world in the epistemic state, \( \know\alpha\supset \alpha \) would be true. That is, suppose \( \know \alpha. \) By the definition above, \( w \) is surely compatible with itself after any \( z \), and so \( \alpha \) must hold at \( w. \) Analogously, properties regarding knowledge can be proven with comparatively simpler arguments in a modal framework, in relation to the classical epistemic situation calculus. Valid properties include: (a) \(  \Box(\know(\alpha)\land \know(\alpha\supset \beta)\supset \know(\beta)) \);  (b) \(  \Box(\know(\alpha) \supset \know(\know(\alpha))); \) (c) $\Box(\neg\know(\alpha) \supset \know(\neg \know(\alpha)))$; (d) $\Box(\forall x.~\know(\alpha) \supset \know(\forall x.~\alpha))$; and
	(e) $\Box(\exists x.~\know(\alpha) \supset \know(\exists x.~\alpha)).$


	Note that such properties hold over all possible action sequences, which explains the presence of the \( \Box \) operator on the outside. The first is about the closure of modus ponens within the epistemic modality. The second and third are on positive and negative introspection. The last two reason about quantification outside the epistemic modality, and what that means in terms of the agent's knowledge. For example, item 5 says that if there is some individual \( n \) such that the agent knows \( Teacher(n) \), it follows that the agent believes \( \exists x Teacher(x) \) to be true. This may seem obvious, but note that the property is really saying that the existence of an individual in some possible world implies that such an individual exists in all accessible worlds. It is because there is a fixed domain of discourse that these properties come out true; they are referred to a the  Barcan formula. 
	
	It is worth nothing that in single-agent epistemic planning \cite{baral_et_al:DR:2017:8285}, it is most common to have epistemic goals of the sort \( \know\phi \), \( \neg \know \phi \) and \( \know\neg\know\phi \), where \( \phi \) is non-modal. The idea is that we might be interested in interleaving physical and sensing actions such that (respectively) \( \phi \) becomes known, or as an observer (e.g., user) we make note that the agent does not know \( \phi \), or that the agent knows that it does not know \( \phi \), in which case it might choose to do actions so that it gets to know \( \phi \). Multiple nestings of modalities are allowed but usually not necessary in the single-agent case. When multiple agents are involved, however, \cite{DBLP:conf/kr/KellyP08,muise-aaai-15,Belle:2014aa}, it becomes necessary to interleave epistemic operators, often arbitrarily, in service of notions such as common knowledge \cite{DBLP:journals/jacm/HalpernM90}. 
	
	 As seen above, the logic \( \es \) allows for a simple definition of the notion of only-knowing in the presence of actions \cite{77758},  which allows one to capture both the beliefs as well as the non-beliefs of the agent. Using the modal operator \( \oknow \) for only-knowing, it can be shown that \( \oknow\alpha\models \know\beta \) if \(\alpha\models\beta \) but \( \oknow\alpha\models \neg \know\beta \) if \( \alpha\not\models\beta \) for any non-modal \( \set{\alpha,\beta}. \) That is, only-knowing a knowledge base also means knowing everything entailed by that knowledge base. Conversely, it also means not believing everything that is not entailed by the knowledge base. In that sense, \( \know \) can be seen as an ``at least'' epistemic operator, and \( \oknow \) captures both at least and ``at most" knowing. This can be powerful to ensure, for example, that the agent provably does not know protected attributes. 

	We will now consider the axiomatization of a basic action theory in \( \es \). But before explaining how successor state axioms are written, one might wonder whether a successor state axiom for \( \know \) is needed, as one would for \( \knows \) in the epistemic situation calculus. It turns out because the compatibility of the worlds already accounted for the executability of actions and sensing outcomes in accessible worlds, such an axiom is actually a property of the logic:  \begin{align*}\label{eq:kssa}
		\models \Box[a]\know(\alpha) \equiv 
			(SF(a) \land \know(SF(a) \supset [a]\alpha)) ~\lor    (\neg SF(a) \land \know(\neg SF(a) \supset [a]\alpha)).
	\end{align*} 
	% \[
%
% 	\]

({Free variables are implicitly quantified from the outside.}) 
What will be known after an action is 
based on what is true in the real world and the incorporation of this information with the agent's knowledge. \\

% understood in terms of what was known previously together with the sensing outcome.
	
	% The example below illustrates how \( SF \) works.
	
	% TBD: only knowing, prob, golog, temporal operators

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% subsection finite_fragment (end)

\textbf{Basic Action Theories} 
To illustrate the language towards the axiomatization of the domain, we consider the analogue of the basic action theory in the situation calculus \cite{reiter2001knowledge}. It consists of: \begin{itemize}
	\item axioms that describe what is true in the initial states, as well as what is known initially; 
	\item  precondition  axioms that describe the conditions under which actions are executable using a distinguished predicate \( Poss \);  
	\item  successor state axioms that describe the conditions under which changes happen to fluents on executing actions,  incorporating Reiter's monotonic solution to the frame problem; and 
	\item sensing axioms that inform the agent about the world using a distinguished predicate \( SF. \)
\end{itemize}

Note that foundational axioms as usually considered in Reiter's variant of the situation calculus \cite{reiter2001knowledge} are not needed as the tree-like nature of the situations is baked into the semantics. 


We will lump the successor state, precondition and sensing axioms as \( \theorydyn \). The sentences that are true initially will be referred to by \( \theoryinit \). When we are not interested in epistemic goals, and do not concern ourselves with sensing actions, we can restrict our attention to entailments of \( \init \land \dyn. \) Note that because \( \init \) might include disjunctions (and possibly quantifiers), there might be multiple worlds where \( \init \) is true. For example, in a propositional language with only two propositions \( \set{p,q} \), \( \init = (p\land \neg q) \)  means there is only a single world where \( \init \) is true initially, but \( \init = (p \lor q) \)  means that there are three worlds where \( \init \) is true initially. In other words, \( \init \) might correspond to a single or multiple initial states in classical planning parlance. 

 % \cite{series/synthesis/2013Geffner}.
 %

If we are wanting to model knowledge, the agent cannot be expected to know everything that is true, and so let \( \theoryinits \) be what is believed initially. It may seem natural to let \( \theoryinits \subseteq \theoryinit \), but that it not necessary. The agent might  be uncertain about what is true (e.g., \( \theoryinit \) might have \( p \) but \( \theoryinits  \) has \( p\lor q \) instead).\footnote{If the agent believes facts that are conflicted by observations about the real world, beliefs may need to be revised \cite{Delgrande:2012fk}, a matter we ignore for now. Our theory of knowledge is based on \emph{knowledge expansion} where sensing ensures that the agent is more certain about the world \cite{citeulike:528170,reiter2001knowledge}. In the case of reconciliation-based explanations, however, we will need to entertain a simple type of revision based on the deletion of facts from \( \theoryinits, \) as we shall shortly see. A general treatment of deleting in first-order languages might be based on \emph{forgetting}  \cite{lin1994forget}.} However, for simplicity, we will require that agents at least believe the dynamics works as would the real world. Therefore, we consider entailments wrt the following \emph{background theory}: \begin{equation}\label{eq:example}
	\theory = \theoryinit \land \theorydyn \land \oknow(\theoryinits \land \theorydyn).
\end{equation} 

There are conveniences afforded by a basic action theory of this form. Firstly, as far as a non-epistemic account of planning is concerned (that is, one where the knowing modality is not present in the goal), we would be  checking the entailment of non-modal goal formulas, and therefore, it is immediate that only \( \theoryinit \land \theorydyn \) from \( \theory \) is involved. Everything in the context of an epistemic operator in \( \theory \) can be ignored. This is precisely what we will explore in the first set of results on counterfactual plans. But when we need to refer to formulas involving epistemic modalities, we will only need to refer to the non-objective parts of \( \theory. \) (When the agent performs sensing actions, however, they will provide values from \( \theoryinit. \)) Thus, we can simply concern ourselves with entailments of \( \theory \) henceforth. \\



% As can be expected in any propositional planning formalism, there is a unique world when restricted to such a fragment.

% \begin{proposition} Suppose \( \phi \in \esprop \) is any non-modal formula. Suppose for every atom \( p\in\esprop, \) either \( \phi\models p \) or \( \phi\models\neg p. \) There is precisely one \( \esprop \) world \( w \) such that \( w, \noact \models \phi.\)
% \end{proposition}
%
% \begin{proof} Consider \( \langle p\sub 1= b\sub 1, \ldots, p\sub k = b\sub k \rangle \) where \( b\sub i\in \set{0,1}, \) \( p\sub i \) is an atom of \( \esprop \), and \( \phi \models (p\sub i = b\sub i) \). By definition, \( \esprop \) worlds would only map atoms \( \set{p\sub 1, \ldots, p\sub k} \),
%
% \end{proof}




\textbf{Example} % (fold)
\label{sub:example}
	Let us consider a simple blocks world example, involving picking up and dropping objects, but also quenching (rapidly cooling to very low temperatures) objects so that they become fragile, adapted from \cite{DBLP:journals/sLogica/LesperanceLLS00,LakemeyerLevesque2004}. As usual, picking up is only when possible when the robot is not already holding anything, and dropping is only possible when it is already holding the object. Also, broken objects in the robot's hand can be repaired. So, \begin{align*}
		 \Box Poss(a)  \equiv~ & (a=pickup(x) \land \forall z.\neg Holding(z)) \lor  (a=drop(x)\land Holding(x)) \lor \\ & (a=quench(x)\land Holding(x)) \lor  (a=repair(x) \land Holding(x) \land Broken(x)). 
	\end{align*} 
Let us also permit a sensing axiom that allows one to look up if an object is made of glass: \[
	\Box SF(a) \equiv (a=isGlass(x) \land Glass(x)) \lor a \neq isGlass(x).
\]
% For simplicity, we assume binary genders, but it is a simple matter of using a predicate such as \( Gender(x,y) \) instead to allow individuals \( x \) to take on gender \( y \) from an arbitrary set.

To now consider successor state axioms, let us suppose holding an object is possible by picking it up. A fragile object gets broken on dropping it, and not repairing it. Quenching makes an object fragile, regardless of whether it was previously fragile or not. An object being a glass is a rigid property. These are formalized as the axioms below, where the left hand side of the equivalence captures the idea that for every sequence of actions, the effect of doing \( a \) on a predicate is given by the right hand side of the equivalence.   These capture Reiter's monotonic solution to the frame problem using successor state axioms, but now in \( \es. \)
\begin{align*}
	 \Box [a]& Holding(x) \equiv   a=pickup(x) \lor     (Holding(x) \land a \neq drop(x)).  \\
	\Box [a] & Broken(x) \equiv  (a=drop(x) \land Fragile(x)) \lor    (Broken(x) \land a \neq repair(x)).		\\ 
	\Box [a] & Fragile(x) \equiv   Fragile(x) \lor a =quench(x). \\
		\Box [a]   & Glass(x) \equiv Glass(x).
\end{align*}


Let us suppose the initial theory only-believed by the agent is the following, where nothing is held, there is a non-broken object \( c \) made of glass, 
and as one would assume, glass objects are fragile: \begin{align*}
	\inits = & \{ Glass(c), \neg \exists x Holding(x), \neg Broken(c),  \forall x (Glass(x) \supset Fragile(x)) \}. 
\end{align*}
In the real world, let us additionally suppose there is another glass object \( d \) but also a non-fragile object \( h \):\footnote{We use the set notation and the formula notation (that is, using conjunctions of formulas) for  theories as per convenience.} \(
	\init = \inits \cup \set{(Glass(d)),  (\neg Glass(h)), (\neg Fragile(h))}.
\) 

That is, whatever the agent believes happens to be true in the real world, but the agent does not know about \( d \) being made of glass and \( h \) not being fragile. \( \init \) in itself does not commit to how many objects there are in the universe, and so it should be clear to the agent that there are (possibly infinitely) many  objects outside of \( c \) for which it is not known whether they are fragile or made of glass, for example. 

Here a few examples of entailments of \( \theory \): (a) \(  (\neg \know Glass(d) \land \neg \know \neg Glass(d)) \); (b) \(   \know \neg \know Glass(d) \); (c) \(   [isGlass(h)] \know  Glass(d) \); and (d) \(  [isGlass(h)] \know \know \know Glass(d). \)


That is, the agent’s initial beliefs imply that the agent does not know whether \( d \) is made of glass. Moreover, by introspection, the agent knows that it does know if 	\( d \) is made of glass. But after sensing $h$ for glass, it knows that it knows that it knows (and so on arbitrarily) that $d$ is made of glass. 

% In our example, let us suppose: \[
% 	\theoryinit = \{Male(n\sub i), \neg  Male(n'\sub i), Eligible(n\sub i), \neg Eligible(n'\sub i)\mid i\in N\}
% \] whereas, what is believed by the agent initially is: \[
% 	\theoryinits = \{Eligible(n\sub i), \neg Eligible(n'\sub i) \mid i \in N\}
% \]
% So there are two groups of individuals, \( n \sub i\) and \( n'\sub i, \) the first male and the second female, the first considered eligible and the second not considered eligible. All that the agent knows is the eligibility of the individuals.  Note that \( N \) here is any set, possibly an infinite one. For ease of readability, however, we let \( N = \{1\} \) in our examples below, and
% we write \( n\sub 1  \) as \( n \) and \( n'\sub 1 \) as \( n'. \)\footnote{Note that although the language has infinitely many constants, a finite domain can be enforced using domain relativization. For example, let: \[
% \forall x	(Individual(x) \equiv x = john \lor \ldots  \lor x = jane).
% \]
% This declares finitely many individuals.
% Then instead of saying \( \exists x.~Eligible(x) \), which in general means that any one of the infinitely many constants is eligible, we would write: \[
% 	\exists x (Individual(x) \land Eligible),
% \]
% which declares that only one from \( \set{john, \ldots, jane} \) is eligible.
% }

% The agent does not know about the gender of the individuals.

% TBD: loan approval program, epistemic fairness

% It is worth quickly remarking that many features of the language are omitted here for simplicity. For example, \( \es \) can be extended with second-order variables \cite{DBLP:conf/kr/ClassenL08}, which allows one to consider the equivalent of GOLOG programs \cite{Levesque97-Golog}. For example, we could imagine a company policy of the following sort that automatically approves all individuals making a loan request provided they have a high salary:
% \begin{equation*} \begin{split}
% 	{\bf loop}: & ~~{\bf if } \neg Empty(queue) \\
% 	&~~~~ {\bf then } ~(\pi p) selectRequest(p); \\
% 	&\qquad {\bf if }~salaryHigh(p)~{\bf then }~approve(p)~{\bf else }~deny(p) \\
% 	&~~{\bf else }~wait
% \end{split}
% \end{equation*}
% Reasoning about programs and how they implement fairness is an interesting and rich direction that we leave for the future.\footnote{Our thrust is considerably different to \cite{albarghouthi2016fairness} where programs are written to define distributions over which fairness is enforced. As shortly discussed, in our setting, a program enables an executable sequence of actions, and we are to determine if, say, knowledge about protected attributes is gained through the sequence, or whether a course of action could be advised that leads to equitable situations.} Likewise, notions of probabilistic actions \cite{Bacchus1999171}, epistemic achievability \cite{DBLP:journals/sLogica/LesperanceLLS00}, and causality \cite{batusov2018situation} in addition to studying program properties \cite{Classen2018} are interesting dimensions to explore in the fairness context.

% \subsection{Forgetting} % (fold)
% \label{sub:forgetting}
%
% In some of the definitions of fairness, we will need to force the setting where information about protected attributes is forgotten. While standard ML approaches propose to do this via column deletion (e.g., remove all entries for the gender attribute), a richer notion is arguably needed for a first-order knowledge base. We appeal to the notion of forgetting \cite{lin1994forget}.
%
% Lin and Reiter defined the notion of forgetting, which is adapted to \( \es \) below. They show that  while forgetting ground atoms is first-order definable, forgetting relations needs second-order logic. We only focus on the case of atoms, but it would interesting to study how fairness notions are affected when protected attributes  are completely absent from a theory.
%
% Suppose \( S \) denotes a finite set of ground atoms. We write $\M(S)$ to mean the set of all truth assignments to $S$. Slightly abusing notation, given a ground atom \( p \), we write $w' \sim\sub p w$  to mean that $w'$ and $w$ agree on everything initially, except maybe \( p. \) That is, for every atom \( q\neq p, \) \( w[q,\langle\rangle] = w'[q,\langle\rangle] \). Next, for every action sequence \( z\neq \langle\rangle \) and every atom \( q', \) \( w[q',z] = w'[q',z]. \)
%
% \begin{definition}  Given a formula \( \phi \) not mentioning modalities, we say \( \phi' \) is the result of forgetting atom \( p \), denoted $\forget(\phi, p)$, if for any world $w$, \( w\models \phi' \) iff there is a \( w' \) such that \( w' \models \phi \)   and \( w\sim \sub p w' \).  Inductively, given a set of atoms \( \set{p\sub 1, \ldots, p\sub k} \),
% 	define $\forget(\phi, \set{p\sub 1, \ldots, p\sub k})$ as $\forget(\forget(\phi, p\sub 1), \ldots, p\sub k)$.
%
% \end{definition}
%
% It is not hard to show that forgetting amounts to setting an atom to true everywhere or setting it false everywhere. In other words:
%
%  \begin{proposition}
% 	\( \forget(\phi,S) \equiv \bigvee\sub {M\in \M(s)} \phi[M], \)
% 	where \( \phi[M] \) is equivalent to \( \phi \land \bigwedge \sub i (p\sub i = b\sub i) \) understood to mean that the proposition \( p\sub i \) is accorded the truth value \( b\sub i\in \set{0,1} \) by \( M. \)
% \end{proposition}
%
% % \begin{definition}  Suppose $M\in \M(S)$.
% % 	Let $\phi[M]$ denote replacing every occurrence of \( P(\vec n) \) in \( \phi \) with: \[
% % 		(\vec n = \vec m)
% % 	\]
% %
% % \end{definition}
%
% % TBD: definition of forgetting from IJCAI-11, description from Liu, forgetting for theta(x)
%
% Abusing notation, we extend the notion of forgetting of an atom \( p \) for basic action theories and the background theory as follows in applying it solely to what is true/known initially: \begin{itemize}
% 	\item \( \forget(\theoryinit \land \theorydyn,p) = \forget(\theoryinit,p) \); and
% 	\item \( \forget(\theory,p) = \forget(\theoryinit,p) \land \theorydyn \land  \oknow(\forget(\theoryinits,p)  \land \theorydyn).  \)
% \end{itemize}
%
% One of the benefits of lumping the knowledge of the agent as an objective formula in the context of the only-knowing operator is the relatively simple definition of forgetting.
%
% \begin{proposition} Suppose \( \phi \) is non-modal. Suppose  \( p \) is an atom. For every objective \( \psi \) such that \( \forget(\phi,p) \models \psi \) it is also the case that \( \oknow(\forget(\phi,p)) \models \know\psi. \)
%
% \end{proposition}
%
% Because \( \oknow\phi\models\know\psi \) for every \( \set{\phi,\psi} \) provided \( \phi\models \psi \), the above statement holds immediately. In so much as we are concerned with a non-modal initial theory and the effects of forgetting, our definition of \( \forget(\theory,p) \) above (notational abuse notwithstanding) suffices.
% In contrast, forgetting with arbitrary epistemic logical formulas is far more involved \cite{DBLP:journals/ai/ZhangZ09}.
%
%
% % \begin{align*}
% % 	{\bf loop}: ~~{\bf if } \neg Empty(queue) \\
% % 	\quad {\bf then } (\pi p) selectRequest(p);
% % \end{align*}
%
% % subsection forgetting (end)
%
%
% % subsection basic_action_theories (end)


\section{Reasoning \& Planning} % (fold)
\label{sub:reasoning}


Given a background theory \( \theory \), an action sequencen \( \delta = a\sub 1 \cdots a\sub k, \) and a (non-modal) goal formula \( \phi \), the classical problem of \emph{projection} \cite{reiter2001knowledge} is to identify if the sequence enables \( 
\phi \). That is, whether
\(
	\theory \models [\delta]\phi.
\) 
We also want to ensure that the action sequence is executable (aka \emph{valid} and/or \emph{legal}). So let us \( Exec(\noact) = true \), and \( Exec(a\cdot \delta) = Poss(a) \land [a]Exec(\delta) \). Then, we check: \(
	\theory \models Exec(\delta) \land  [\delta]\phi.
\) 
In the epistemic setting \cite{citeulike:528170}, we are interested in checking if \( \phi \) is known after executing \( \delta \), which might include sensing actions too. That is, whether\footnote{It might also be of interest to know whether \( \phi \) is true \cite{fan2015contingency}, that is, checking that \(
	\theory \models [\delta] (\know \phi \lor \know \neg \phi).
\) 
Here, the second disjunct is asserting that the agents knows \( \phi \) to be false.} \(
	\theory \models [\delta]\know \phi. 
\)
When adding action executability, we would have: \(
	\theory \models [\delta]\know\phi \land \know Exec(\delta).
\) 
So the agent also knows that the sequence is executable: which means \( \delta \) is executable in every world in the agent's epistemic state.
Note that because we do not require the real world to be necessarily included  in the epistemic state, it is not necessarily that \( \delta \) is actually executable in the real world. If we needed to additionally enforce that, we would need: \(
	\theory \models Exec(\delta) \land [\delta]\know\phi \land \know Exec(\delta).
\)


The task of planning, then, is to identify a sequence \( \delta \) such that \( \phi \) is made true (in the non-epistemic setting) or that \( \phi \) is known to be true, and that the appropriate executability condition holds. 

%For works on synthesizing plans, especially in the epistemic context, see \cite{levesque2005planning,bolander}

% For the sake of readability, we will drop any mention of the executability condition in the rest of the presentation, including in the definitions, but it is understood that the executability condition is needed everywhere for \emph{valid} plans \cite{vasileiou2022logic}.

Note that, we do not require plans to simply be a sequence of (physical) actions. For one thing, they may involve sensing actions, based on which the agent obtains information about the world. For another, we might be interested plans involving recursion \cite{siddthesis}, conditional statements and tests \cite{DBLP:conf/aaai/Levesque96,Levesque97-Golog,Fritz:2008uq}. Such plan structures do not change the nature of the reasoning problem, however: no matter the plan structure, we will be evaluating if the sequence of actions executed by the agent enables some goal, that is, whether the structure instantiates a sequence \( \delta \) such that \( \theory \models [\delta]\phi \) and \( \theory\models [\delta] \know \phi \) for world-state and epistemic planning respectively. Likewise, regression is not limited to only action sequences and can work with conditional and recursive plans.\footnote{In the expressive programming formalism of {\sc GOLOG} \cite{Levesque97-Golog}, for example, we provide the semantics for program execution such that there is a history (an action sequence) that terminates the program in addition to satisfying the goal \cite{Fritz:2008uq}. Likewise, with loopy plans, we provide the semantics for plan execution such that there is a history that reaches the final state of the plan structure in addition to  goal satisfaction \cite{DBLP:conf/aaai/Levesque96}.}

Finally, it can be shown that reasoning about actions and knowledge can be reduced to non-modal reasoning.  We omit the details but refer readers to \cite{LakemeyerLevesque2004}. (We will included an extended report with some examples.) With a finite domain assumption, this can be further reduced to propositional reasoning. 

% % subsection reasoning (end)
%
% \subsection{Propositional Reasoning} % (fold)
% \label{sub:propositional_reasoning}
%
% It can be shown that reasoning about actions and knowledge can be reduced to non-modal reasoning.
%
% To establish this, we need to resort to the notion of \emph{regression} \cite{reiter2001knowledge} and the so-called representation theorem for only-knowing \cite{levesque2001logic}.
%
% \begin{theorem} \cite{LakemeyerLevesque2004} Suppose \( \theory,  \) is as above, \( \delta \) is any action sequence, and \( \phi \) is any formula that
% 	not mentioning \( \set{\oknow,[a],\Box} \). (That is, it can mention \( \know. \)) Then, there is a non-modal formula \( \E(\R(\delta,\phi),\theoryinits) \) where: \[
% 		\theory \models [\delta]\phi \text{ iff } \theoryinit \models \E(\R(\delta,\phi),\theoryinits).
% 	\]
% \end{theorem}
%
% Essentially \( \R \) is the regression operator, and \( \E \) uses whatever is only-known to reduce epistemic reasoning to non-modal reasoning. By enforcing a finite domain of constants, we can further reduce the problem to propositional reasoning. We give the details and an example in the Appendix.

% subsection propositional_reasoning (end)

\section{Counterfactual Explanations} % (fold)
\label{sec:counterfactual_e}

We will firstly attempt to characterize counterfactual (CF) explanations as plans, at an objective level. This could be viewed, therefore,  as an instance of planning with incomplete information,  but it can also be ultimately linked to the epistemic setting, as we shall see below. Simply put,  a CF explanation is an alternative course of action that negates the goal.  (Conversely, if some sequence does not enable the goal, we search for an explanation that does.\footnote{In relation to machine learning \cite{wachter2017counterfactual}, the idea is to produce an action sequence that changes the outcome. For example, if \( \phi \) represents an applicant getting rejected for a job application, then we find a plan to ensure that they are accepted. Conversely, if \( \phi \) states that moving an object to a different room causes it to break, we find a plan to ensure that the object is not broken during the move. In another scenario, if \( \phi \) states that high-risk individuals have their loan approved, we might be interested in additional assumptions that ensure that such individuals are mostly denied loans unless further constraints hold. 

Thus, it is not the polarity of the formula that is relevant here, and our use of the term “goal” is perhaps slightly misleading. Essentially, our definitions below formalize the identification of conditions and sequences that toggle the outcome.})



% {\bf TBD: In this section, we assume non-modal goal $\phi$}

\begin{definition} Suppose \( \theory \models Exec(\delta) \land [\delta]\phi. \) 
	 A CF explanation for \( \phi \) after \( \delta \) is an action sequence \( \delta' \) such that \( \theory \models Exec(\delta') \land  [\delta'] \neg \phi \) and \( dist(\delta',\delta) \) is minimal.
	
\end{definition}

One natural candidate for the distance metric is the cost of actions (and hence the cost of plans) \cite{vasileiou2022logic}. Let us explore some  measures below that does not necessitate associating explicit numbers with actions for simplicity. Of course, the appropriate measure might very well depend on the application domain. 

\begin{definition} Given two sequences \( \delta, \delta' \), define length-based minimality as minimizing 
	
	\( \abs{(length(\delta') - length(\delta))} \), which is the absolute value of the difference in lengths. Length is defined inductively: \( length(\noact) = 0 \), and $length(\delta \cdot a) = length(\delta) + 1$. 
\end{definition}



\begin{example} Suppose \( \delta = pickup(c)\cdot drop(c) \cdot repair(c) \) and the goal is \(  \neg Broken(c) \). The shortest CF explanation for \( Broken(c) \) is \( \delta' = pickup(c)\cdot drop(c) \). That is: (a)
	\( \theory \models [\delta] \neg Broken(c) \); and  (b) \( \theory \models [\delta'] Broken (c). \) 
\end{example}

Let us consider another measure based on  all the properties of the world that are affected. 
For any \( \delta \), define \( fluents(\delta) \) as the set of all  fluents mentioned in the successor state and precondition axioms of actions in \( \delta. \)

\begin{definition} Given \( \delta, \delta' \) as above, define  fluent-based minimality as minimizing
	
	 \( \abs{ size(fluents(\delta)) - size(fluents(\delta'))} \). 	
\end{definition}


\begin{example} Suppose \( \delta = pickup(h)\cdot drop(h) \) for goal \( \neg Broken(h) \). The fluent set for \( \delta \) is 
	
	\( \set{Holding(x), Broken(x)} \). Because \( h \) is not fragile, we would need to quench it. Consider that the fluent set for \( \delta' = pickup(h)\cdot  quench(h)\cdot drop(h) \) is \( \set{Holding(x), Fragile(x), Broken(x)} \), and so it is minimally larger: that is, there is no other \( \delta' \) with the same fluent set as \( \delta \) which achieves \( Broken(h) \). As desired, \( \theory \models [\delta'] Broken(h) \).
	
\end{example}

As it turns out, only optimizing for the affected set is not quite right because many irrelevant ground actions could be included.

% {\bf We can jointly minimize the two above.}
%
% {\bf tbd: loan example from notes}

\begin{definition} Given \( \delta, \delta' \) as above, define plan-and-effect minimality as jointly minimizing both length-based and fluent-based minimality. 
	
\end{definition}

\begin{example} It should be clear that only optimizing for fluent-based minimality is problematic. Consider once more, \( \delta = pickup(h)\cdot drop(h) \) for goal \( \neg Broken(h) \). Let \( \delta'' = pickup(d)\cdot   drop(d) \cdot \delta' \),  where \( \delta' = pickup(h)\cdot  quench(h)\cdot drop(h). \) The fluent set of \( \delta'' \) does not differ from that of \( \delta \) much more than that of \( \delta' \) does. However, \( \delta'' \) has some irrelevant actions for achieving \( Broken(h) \). Thus, \( \delta' \) achieves plan-and-effect minimality.
	
\end{example}

An important additional ingredient with counterfactual explanations is \emph{diversity} \cite{mothilal2020explaining}, where we might seek multiple CF explanations but constrained according some feature. For example, we could be looking for students whose scored low in mathematics (the constraint) while still graduating (the latter being the goal), looking for tall students (the constraint) who still do not play basketball well (the goal), and so on. Properties such as people being tall can be modelled as rigid predicates, but our definition does not limit itself to rigids. 


% Note that \( \alpha \) usually involves rigid properties.

\begin{definition} Given \( \theory, \delta, \phi \) as above, \( k\in \mathbb N, \) and any non-modal formula \( \alpha \) as a diversity constraint, a diverse CF explanation is a sequence \( \delta' \) such that \( \theory \models Exec(\delta') \land [\delta'](\alpha \land \neg \phi) \)	and \( dist(\delta',\delta) \leq k. \) 
\end{definition}

\begin{example} Suppose \( \delta = pickup(h)\cdot drop(h) \) for goal \( \phi = \exists x \neg Broken (x) \). Suppose we are interested in a broken object, but with the diversity constraint of it being made of glass. In other words, \( \alpha = \exists x Glass(x) \), and so we are to find a sequence \( \delta' \)  such that \( \exists x (Glass(x)\land Broken(x)) \) is made true. It is easy to see that \( \delta' = pickup(c)\cdot drop(c) \) is such an explanation. 	
\end{example}


This then leads to multiple explanations that are close enough. 

\begin{definition} Let \( k \) be any positive integer denoting the closeness upper bound. Given \( \theory, \delta, \phi \) as above, and any non-modal formula \( \alpha \) as a diversity constraint, diverse CF explanations is a set of sequences \( \set{\delta\sub 1, \ldots, \delta\sub n} \) such that \( \theory \models Exec(\delta\sub i) \land  [\delta\sub i](\alpha \land \neg \phi) \) for every \( i \)	and \( dist(\delta\sub i,\delta) \leq k \). 
	
\end{definition}



\section{Reconciliation-based Explanations} % (fold)
\label{sec:reconciliation_based_explanations}

The simplest case of an agent providing a counterfactual explanation is that we formulate plans in the context of knowledge, and so goals can involve nested beliefs. 

\begin{definition} Suppose 
	\( \theory \models [\delta] \know \phi \land \know Exec(\delta) \), where \( \phi \) might mention \( \know \) but no other modality. Then a counterfactual explanation is \( \delta' \)  such that \( dist(\delta',\delta) \) is minimal and \( \theory \models [\delta'] \know \neg \phi \land \know Exec(\delta'). \) 
	
\end{definition}

Recall that we are not seeking \( [\delta] \neg \know \phi \), because this is  the case of an agent being ignorant. We instead seek  \( \delta' \) after which the agent knows that \( \phi \) is false. 

Note that in the above definition we were not stipulating executability in the real world, because it suffices for the account to be purely epistemic. We can enforce this additionally, of course, but it will come up naturally for the definitions below because the user needs to make sure she is only suggesting legal actions. 


\begin{example} Following our examples above, consider \( \delta = pickup(c)\cdot drop(c)  \) and clearly \( \theory \) entails \( [\delta]\know Broken(c)\). The explanation \( \delta' = \delta\cdot repair(c) \) achieves \( \know\neg Broken(c). \)
	
\end{example}


What we will consider below is the case where the user assists in achieving goals. So this leads to a broader notion of counterfactuals: consider an alternate history or additional knowledge such that the goal becomes true.


\subsection{Agents Only-Knowing Partial Truths} % (fold)
\label{sub:ignorant_agents}

For this case of ignorant agents, we  assume \( \theoryinits \subseteq \theoryinit \).\footnote{As mentioned before, we do not require \( \Box(\know\alpha\supset\alpha) \) to be valid, but if this was stipulated in the logic (by insisting that the real world \( w\in e \)), then it should always be that \( \init\models\inits \). (If not, then \( \know\inits\supset \inits \) would be falsified in the real world.)}  Suppose a plan \( \delta \) fails in achieving \( \know\phi. \) A CF explanation here amounts to considering a possible world and a sequence such that the agent knows \( \phi \). So either there are missing actions, or missing knowledge, or both.
 
 % There are numerous variants possible here.

\begin{definition} (Missing actions.) Suppose \( \theory \models Exec(\delta) \land \know Exec(\delta) \) but  \( \theory \not \models [\delta] \know \phi \), that is, \( \theory \models [\delta] \neg \know \phi. \) Suppose there is a sequence \( \delta' \) such that \( dist(\delta',\delta) \) is minimal,  \( \theory \models [\delta'] \phi \land Exec(\delta'), \) and \( \theory \models [\delta'] \know \phi \land \know Exec(\delta'). \) Then a CF explanation is \( \delta' \).
\end{definition} 

Note that we do not insist \( \theory \models [\delta] \phi \), because as the definition title suggests, there might actions missing. 
One might also wonder why we insist on the agent also needing to know \( \phi \) after \( \delta' \): is it not redundant? The answer is no. Firstly, notice that even if there is \( \delta' \) such that \( \theory\models[\delta' ] \phi \), it is not necessary that \( \delta' \) is minimally away from \( \delta. \) For example, as far as entailment of objective formulas is concerned, the presence of sensing actions in \( \delta' \) is irrelevant: sensing only affects the knowledge of the agent and does not affect the real world. But the agent may very well need sensing actions to learn more about the world. Therefore, we insist that we need to find a \( \delta' \) that is minimally different to \( \delta, \)  enables \( \phi \) in the real world but also enables the knowing of \( \phi \). In other words, if both \( \delta' \) and \( \delta'' \) enable \( \phi \) and they differ only in that \( \delta' \) includes sensing actions whereas \( \delta'' \) does not, then we want \( \delta' \) to be the explanation. 

\begin{example} Consider \( \delta = pickup(c) \) for the goal \( \neg \know Broken(c) \). The explanation is \( \delta' = \delta \cdot drop(c) \), and indeed, \( \theory \models [\delta'] Broken(c) \) but also \( \theory \models [\delta'] \know Broken(c) \). 
	
\end{example}

\begin{example} Suppose \( \delta = pickup(d)\cdot drop(d) \) for the goal \( \neg \know Broken(d) \). But in fact, \( \theory \models [\delta] Broken(d) \), and so the agent does not know \( d \) is broken owing to the fact that it does not know that \( d \) is made of glass. So consider \( \delta' = pickup(d)\cdot isGlass(d) \cdot drop(d) \). This does not affect what is true in the world, but does lead the agent to know that \( Glass(d) \). Therefore, \( \delta' \) is the explanation since \( \theory \models [\delta'] Broken(d) \), and \( \theory\models [\delta'] \know Broken(d) \). 
	
\end{example}

\begin{definition} (Missing knowledge.) Suppose \( \theory \models Exec(\delta)  \). 
	 Suppose \( \theory \not\models [\delta] \know \phi \land \know Exec(\delta) \) but \( \theory \models [\delta]\phi. \) 
	% Moreover, there is no \( \delta' \) that counts as a CF explanation in the above sense.
	Then suppose there is some \( \alpha \in \theoryinit - \theoryinits \) such that \( \init \land \dyn \land \oknow(\inits \land \alpha \land \dyn) \models [\delta]\know\phi \land \know Exec(\delta). \) Then the smallest such \( \alpha \) is the explanation. 
	
\end{definition}


Note that we do not assume in the definition that  \( \theory \models  \know Exec(\delta) \), because such an \( \alpha \) could be necessary knowledge to reason about the executability of actions.

% \footnote{This definition is not quite complete from an epistemic viewpoint because it could



\begin{example} Given \( \delta = pickup(d)\cdot drop(d) \), we know that \( [\delta] \neg \know Broken(d) \). 
	% \( \theory\models \neg \know Exec(\delta) \)
	%
	% [\delta] \neg \know Broken(d) \).
	But for \( \alpha = Glass(d) \), and owing to the fact that every object made of glass is declared to be fragile in \( \inits \), 
	we see that \( \init\land \dyn \land \oknow(\inits\land\alpha\land \dyn) \) entails \( [\delta] \know Broken(d) \). So \( \alpha \) is the explanation. 
	
\end{example}

\begin{definition}\label{defn missing knowledge and action} (Missing knowledge and action.) Suppose \( \theory \models Exec(\delta) \) but \( \theory \not\models [\delta] \phi, \)  or 
	\( \theory \not\models [\delta]\know\phi \). Suppose there is a minimally distant \( \delta' \) and some \( \alpha \) such that \( \init \land \dyn \land \oknow(\inits \land \alpha \land \dyn)  \) entails: \(  [\delta'](\phi \land \know\phi) \land Exec(\delta') \land \know Exec(\delta'). \) Then the smallest such \( \alpha \) together with \( \delta' \) is the explanation.
	
	% Suppose \( \theory \not\models [\delta]\know\phi \). Suppose  \( \theory\not\models[\delta]\phi \) and there is a minimally distance sequence \( \delta' \) such that
	
\end{definition}

Note that, firstly, if \( \Box (\know\alpha\supset \alpha) \) was true in the logic, \( \theory \not\models [\delta] \phi \) also means \( \theory \not\models [\delta] \know \phi \), because the real world \( w\in e \). By assumption, if \( \phi \) is not made true after \( \delta, \) then the agent cannot come to know \( \phi \) after \( \delta. \) Since we do not require knowledge being true, the definition has to make stipulations about both \( [\delta]\phi \) and \( [\delta]\know \phi. \) Moreover, it is possible that \( \theory\models [\delta] \phi \) but \( \theory\not\models [\delta]\know \phi \)  because there are  sensing actions that could enable the agent to learn sufficient information for knowing  \( \phi \)  after \( \delta \), or because there is some information that cannot be accessed by sensing that needs to be added to \( \inits \) for the agent to infer \( \know\phi \) after \( \delta, \) (or both).\footnote{This suggests a ``criteria'' for triggering the addition of knowledge. Define two sequences \( \delta \) and \( \delta' \) to be close iff \( \delta' \) only differs from \( \delta \) in having sensing actions. Condition knowlege addition only when there is no \( \delta' \) that is close in this sense, and \( \theory \models [\delta]\phi \land [\delta] \neg \know \phi \land [\delta'] \know \phi \) along with \( \theory \models Exec(\delta) \land Exec(\delta') \land \know Exec(\delta) \land \know Exec(\delta') \). So,  if there is a legal sequence that only augments \( \delta \) with sensing  but enables knowing the goal, then we conclude that no new knowledge needs to be added. If there are multiple such augmented sequences \( \delta'  \) and \( \delta'' \), we would choose the shortest such sequence. This would avoid applying sensing actions arbitrarily or using sensors that do not inform the agent about anything relevant for \( \phi. \)
}

\begin{example}\label{ex:missing knowledge and action} Let us assume quenching comes with an additional condition that it only works with metals: \(
	\Box Poss(quench(x)) \equiv Holding(x) \land Metal(x).
\) Let \( \inits \) be as before, and let the initial state of the world be given by \(
	\init\su * = \inits \cup \set{\neg Fragile(h), \neg Glass(h), Metal(h)}.
\) 
Consider \( \delta = pickup(h)\cdot drop(h) \), and it is easy to see that \(  \init\su *\land \dyn \land \oknow(\inits \land \dyn) \) entails \( [\delta] \neg \know Broken(h) \). Consider \( \delta' = pickup(h)\cdot quench(h)\cdot drop(h) \) which adds the quenching action. In itself, the sequence is not known to be executable because the agent does not know that \( h \) is metallic. So  \( \alpha = Metal(h) \) together with \( \delta' \) is the explanation because $\init\su *\land \dyn \land \oknow(\inits \land \alpha \land \dyn) $ entails 
 \(  [\delta'] (\phi\land \know\phi) \) where \( \phi = Broken(h) \), as well as the legality of the sequence and knowledge of its legality. 
	
\end{example}

% subsection ignorant_agents (end)

\subsection{Agents Only-Knowing Weakened Truths} % (fold)
\label{sub:agents_only_knowing_weakened_truths}

Here we assume \( \inits \not\subseteq \init \) but for every \( \alpha\in\inits \), \( \init \models \alpha. \) In other words, 	\( \init \models \inits. \) 
That is, we might have an atom \( p\in\init, \) but \( \inits \) instead has \( (p\lor q). \) We then can define an account involving missing knowledge and actions, and so the same definition from \ref{defn missing knowledge and action} applies.

\begin{example} Let us consider  Example \ref{ex:missing knowledge and action} except that the initial theory of the agent is \( \init'' = \inits \land (Metal(h) \lor Metal(d)) \). That is, it includes all the formulas from \( \inits \) but also information that \( h \) is metallic or (falsely) that \( d \) is metallic. However, \( \init\su * \models (Metal(h) \lor Metal(d)) \), and so \( \alpha \) and \( \delta' \) from Example \ref{ex:missing knowledge and action} counts as the explanation. 
	
\end{example}

% subsection agents_only_knowing_weakened_truths (end)

% section reconciliation_based_explanations (end)

% section counterfactual_fairness (end)

\subsection{Agents with False Beliefs} % (fold)
\label{sub:agents_with_false_beliefs}

% {\bf TBD: forgetting more general}
False beliefs are only satisfiable when \( \Box (\know \alpha \supset \alpha) \) is not valid, as it happens to be in our case. 
For simplicity, we deal with the case of missing knowledge, and this can be easily coupled with missing actions in the manner discussed above. (Our example will deal with both.)

\begin{definition}\label{defn false beliefs}
	Suppose \( \inits \not\subseteq \init, \) and moreover \( \init\not\models \inits. \) Suppose \( \theory \not \models ([\delta]\know\phi \land \know Exec(\delta)) \) but \( \theory \models  Exec(\delta) \land [\delta]\phi. \) Suppose there is \( \alpha\in\init \) and \( \beta\in\inits \) such that \( \init\cup \dyn\cup \oknow( (\inits - \set{\beta}) \cup \set{\alpha} \cup \dyn ) \models [\delta]\know\phi \land \know Exec(\delta). \) Then the smallest such \( \alpha \) and \( \beta \)  are the explanations. 
\end{definition}

\begin{example} Consider Example \ref{ex:missing knowledge and action} and let \( \init'' = \init' \land \neg Metal(h) \). So the agent only-knows everything from  \( \init' \) as well as a false fact about \( h. \) Let \( \init\su *, \delta,  \)  and \( \delta' \) be  as in Example \ref{ex:missing knowledge and action}. Now note that given \( \alpha = Metal(h) \), \( \beta = \neg Metal(h) \) and \( \delta' \), we have \( \init\su * \land \dyn\land \oknow(\init'\land \alpha\land \dyn) \) entails \(
	 [\delta'](\phi \land \know\phi) \land Exec(\delta') \land \know Exec(\delta'), 
\)
for \( \phi = Broken(h) \). As it turns out \( \init' \) is obtained by removing \( \beta \) from \( \init'' \) by construction, and 
% Here, of course, \( \init' = \init'' - \set{\beta} \), and
so \( \alpha,\beta  \) and \( \delta' \) constitutes as the explanation. 
	
\end{example}

% \begin{proposition} Given \( \theory \) as above, let \[ \beta = \inits - formulas(\inits,fluents([\delta]\phi)),  \] where the latter is the set of formulas from \( \inits \) which mentions any of the fluents from \( fluents([\delta]\phi). \) Likewise, let \[ \alpha = formulas(\init,fluents([\delta]\phi)), \] which is the set of formulas from \( \init \) that mentions any of the fluents from \( fluents([\delta]\phi). \)
% Then \( \alpha \) and \( \beta \) are  candidates for an explanation in the above sense, but not necessarily the smallest.
% \end{proposition}



% subsection agents_with_false_beliefs (end)

% \section{Formalizing Fairness} % (fold)
% \label{sec:fairness}
%
%
% Let us now consider the the three notions discussed earlier: fairness through unawareness (FTU), demographic parity (DP) and counterfactual fairness (CF), and formalise these in the epistemic situation calculus. After noting a popular criticism about CF, we introduce a notion of \emph{equity} based on the thrust of DP.
%
% At the outset, it is worth noting a few  salient points about our formalization: \begin{enumerate}
% 	\item Because we are not modeling a prediction problem,  our definitions below should be seen as being loosely inspired by existing notions rather that faithful reconstructions. In particular, we will look at ``fair outcomes'' after a sequence of actions.
% 	Indeed, debates about problems with the mathematical notions of fairness in single-shot predictions problems are widespread \cite{Fairness_through_awareness, Counterfactual_Fairness, Disparate_Impact_&_Historical}, leading to recent work on looking at the long-term effects of fairness \cite{creager2020causal}. However, we are  ignoring probabilities in the formalization in current work only to better study the principles behind the above notions -- we suspect with a probabilistic epistemic dynamic language  \cite{Bacchus1999171}, the definitions might resemble mainstream notions almost exactly and yet organically use them over actions and programs, which is attractive.
% 	\item The first-order nature of the language, such as quantification, will allow us to easily differentiate fairness for an individual versus groups. In the mainstream literature, this has to be argued informally, and the intuition grasped  meta-linguistically.
% 	\item Because we model the real-world in addition the agent's knowledge, we will be able to articulate what needs to be true vs just believed by the agent. In particular, our notion of equity will refer to the real-world.
% 	\item De-re vs de-dicto knowledge will mean having versus not having information about protected attributes respectively. Sensing actions can be set up to enable de-re knowledge if need be, but it is easy to see in what follows that de-dicto is preferable.
% 	\item Action sequences can make predicates true, and this will help us think about equity in terms of balancing opportunities across instances of protected attributes (e.g., making some property true so that we achieve gender balance).
% \end{enumerate}
%
%
% Let us begin with FTU: recall that it requires that the agent does not know the protected attributes of the individuals. To simplify the discussion, let us assume we are concerned with one such attribute \( \theta(x) \), say, \( Male(x) \), in our examples for concreteness. We might be interested in achieving \( hasLoan(x) \) or \( highSalary(x) \), for example, either for all \( x \) or some individual.
% 	% quantification and groups
% 	% not recreating definitions exactly but being inspired by it, to see what sticks
% 	%
% 	% as you can lots of options for trying different things in the definitions
%
% % Consider \( \phi \doteq \exists x, y.~hasLoan(x) \land \neg hasLoan(y) \), and let \( \theta(x) \doteq Male(x). \)
%
% \begin{definition} A sequence \( \delta = a\sub 1 \cdots a\sub k \) implements FTU for \( \phi \) wrt protected attribute \( \theta(x) \) iff \begin{itemize}
% 	\item \( \theory \models [\delta] \know\phi \); and
%
% 	\item for every $\delta' \leq \delta$: \( \theory \models [\delta']  \neg \exists x (\know \theta(x)). \)
%
% 	% \item for every $\delta' \leq \delta$: \[ \theory \models [\delta']  \forall x.~\neg (\wh \theta(x)). \]
%
% 	% \item for every $\delta' \leq \delta$: \[ \theory \models [\delta']  \neg \exists x (\know \theta(x)). \]
%
% 	% sequence \( \delta' = a\sub 1 \cdots a\sub m  \), where \( \delta = \delta' \cdot a\sub {m+1} \cdots a\sub k \): \[ \theory \models [\delta']  \neg \exists x \know \theta(x)). \]
% \end{itemize}
% \end{definition}
% The attractiveness of a first-order formalism is that in these and other definitions below where we quantify over all individuals, it is immediate to limit the applicability of the conditions wrt specific individuals. Suppose \( n \) is such an individual. Then:
%
% \begin{definition} A sequence \( \delta = a\sub 1 \cdots a\sub k \) implements FTU for \( \phi \) wrt attribute \( \theta(x) \) for individual \( n \) iff \begin{itemize}
% 	\item \( \theory \models [\delta] \know\phi \); and
%
% 	\item for every $\delta' \leq \delta$: \( \theory \models [\delta']  \neg \know \theta(n). \)
%
% 	% \item for every $\delta' \leq \delta$: \[ \theory \models [\delta']  \neg \exists x (\know \theta(x)). \]
%
% 	% sequence \( \delta' = a\sub 1 \cdots a\sub m  \), where \( \delta = \delta' \cdot a\sub {m+1} \cdots a\sub k \): \[ \theory \models [\delta']  \neg \exists x \know \theta(x)). \]
% \end{itemize}
%
% \begin{example} Consider \( \theory \) from \eqref{eq:example}, \( Male(x) \) as the protected attribute, and suppose \( \delta = approve(n)\cdot approve(n') \). It is clear that \( \delta \) implements FTU for both the universal \( \phi = \forall x hasLoan(x) \) as well as an individual \( \phi =  hasLoan(n). \) Throughout the history, the agent does not know the gender of the individual.
% \end{example}
%
% % By means of p
% % Equivalently,
%
%
% 	%  \[
% % 	\theory \models [\delta] \know\phi \land (\forall \delta'.~ \delta' \leq \delta \supset ~[\delta']  \neg \exists x \know \theta(x)).
% % \]
% % %\[
% % 	\theory \models [\delta] \know\phi \land (\forall \delta' \leq \delta.~[\delta']  \neg  x \know \theta).
% % \]
%
% \end{definition}
%
% Before turning to other notions, let us quickly reflect on proxy variables. Recall that in the ML literature, these are variables that indirectly provide informations about protected attributes. We might formalize this using entailment:
%
% \begin{definition} Given a protected attribute \( \theta(x) \) and background theory \( \theory \), let the proxy set \( Proxy(\theta(x)) \) be the set of predicates \( \set{\eta\sub 1(x), \ldots \eta\sub k(x)} \) such that: \[
% 	\theory \models \forall x(\eta\sub i (x)\supset \theta(x)),
% \]
% for \( i\in\set{1,\ldots,k} \).
% \end{definition}
% That is, given the axioms in the background theory, \( \eta\sub i(x) \) tells us about \( \theta(x) \).
%
% \begin{example} Suppose the agent knows the following sentence: \[
% 	\forall x (EtonForBoys(x) \supset Male(x)).
% \]
% Let us assume \( EtonForBoys(x) \)	 is a rigid, like $Male(x)$. Let us also assume that \( \know(EtonForBoys(n)) \).
% It is clear that having information about this predicate for \( n \) would mean the agent can infer  that  \( n \) is male.
% \end{example}
%
% The advantage of looking at entailment in our definitions is that we do not need to isolate the proxy set at all, because whatever information we might have the proxy set and its instances, all we really need to check is that \( \theory \not\models \exists x\know\theta(x) \).\footnote{With this discussion, we do not mean to insist that analyzing ``relevant'' predicates for \( \theta(x) \) is a pointless endeavor. Rather we only want to point out that regardless of the information  available to the agent, as long as we check that  it is actually ignorant about the gender, other relevant predicates may not matter. Of course, a biased agent can enable actions that favors individuals based on such proxy predicates instead, but in that case, such proxy predicates would also need to be included in the protected attribute list.}
%
%
% % In fact, formally, we can construction a notion, say FTU-proxy, that imposes the ignorance of both \( \theta(x) \) as well as \( Proxy(\theta(x)) \).
%
% Let us now turn to DP. In the probabilistic context, DP is a reference to the proportion of individuals in the domain: say, the proportion of males promoted is the same as the proportion of females  promoted. In logical terms, although FTU permitted its definition to apply to both groups and individuals, DP, by definition, is  necessarily a quantified constraint. In contrast, CF   will stipulate conditions solely on individuals.
%
% % the remaining properties are also stipulated for individuals.
%
% \begin{definition} A sequence \( \delta = a\sub 1 \cdots a\sub k \) implements DP for \( \phi(x) \) wrt attribute \( \theta(x) \)  iff: \[
% 	\theory \models [\delta] \know((\forall x \theta (x) \supset \phi(x)) \land (\forall x \neg \theta(x)\supset \phi(x)) ).
% \]
%
% % 	\begin{itemize}
% % 	\item \( \forget(\theory,\theta(n)) \models [\delta]  \wh\theta(n) \); and
% % 	\item \( \forget(\theory,\theta(n)) \land \theta(n) \models [\delta] \know \phi \) iff \( \forget(\theory,\theta(n)) \land \neg \theta(n) \models [\delta] \know \phi \).
% % \end{itemize}
%
% 	% \item \( \forget(\theory,\theta(x)) \models [\delta] \exists x \know\theta(x) \); and
% 	% \item \( \forget(\theory,\theta(x)) \land \theta(m) \models [\delta] \know \phi \) iff \( \forget(\theory,\theta(x)) \land \theta(f) \models [\delta] \know \phi \).
%
% \end{definition}
%
% To reiterate, in probabilistic terms, the proportion of men who are promoted equals the proportion of women who are promoted. In the categorial setting, the agent knows that all men are promoted as well as that all women are promoted.
%
% \begin{example} Consider  \( \delta = approve(n)\cdot approve(n') \). It implements DP for \(  hasLoan(x) \) wrt attribute \( isMale(x). \)
% \end{example}
%
% Note that even though the agent does not know the gender of the individuals, in every possible world, regardless of the gender assigned to an individual \( n \) in that world, \( n \) has the loan. In other words, all men and all women hold the loan. This is de-dicto knowledge of the genders, and it is sufficient  to capture the thrust of DP.
%
% We might be tempted to propose a stronger requirement, stipulating de-re knowledge:
%
% \begin{definition} A sequence \( \delta = a\sub 1 \cdots a\sub k \) implements strong DP for \( \phi(x) \) wrt attribute \( \theta(x) \)  iff: \begin{itemize}
% 	\item \( \theory \models [\delta] \know((\forall x \theta (x) \supset \phi(x)) \land (\forall x \neg \theta(x)\supset \phi(x)) ). \)
% 	\item \( \theory \models [\delta] \forall x (\know\theta(x) \lor \know\neg\theta(x)) \)
% \end{itemize}
%
% 	%  \[
% %
% % \]
% %
% % 	\begin{itemize}
% % 	\item \( \forget(\theory,\theta(n)) \models [\delta]  \wh\theta(n) \); and
% % 	\item \( \forget(\theory,\theta(n)) \land \theta(n) \models [\delta] \know \phi \) iff \( \forget(\theory,\theta(n)) \land \neg \theta(n) \models [\delta] \know \phi \).
% % \end{itemize}
%
% 	% \item \( \forget(\theory,\theta(x)) \models [\delta] \exists x \know\theta(x) \); and
% 	% \item \( \forget(\theory,\theta(x)) \land \theta(m) \models [\delta] \know \phi \) iff \( \forget(\theory,\theta(x)) \land \theta(f) \models [\delta] \know \phi \).
%
% \end{definition}
%
% That is, the agent knows whether \( x \) is a male or not, for every \( x. \)
%
% \begin{example} Consider  \( \delta = isMale(n)\cdot isMale(n')\cdot  approve(n)\cdot approve(n') \). It implements strong DP for \( hasLoan(x) \) wrt attribute \( isMale(x). \) Of course, by definition, $\delta$ also implements DP for $hasLoan(x).$
% \end{example}
%
% In general, since we do not wish the agent to know the values of protected attributes, vanilla DP is more attractive. Formally, we may impose a FTU-style constraint of not knowing on any fairness definition.
% For example,
% \begin{definition} A sequence \( \delta = a\sub 1 \cdots a\sub k \) implements FTU-DP for \( \phi(x) \) wrt attribute \( \theta(x) \)  iff: \begin{itemize}
% 	\item \( \theory \models [\delta] \know((\forall x \theta (x) \supset \phi(x)) \land (\forall x \neg \theta(x)\supset \phi(x)) ) \); and
% 	 \item for every $\delta' \leq \delta$: \( \theory \models [\delta']  \neg \exists x \know \theta(x). \)
% \end{itemize}
%
% \end{definition}
%
% Again, it is worth remarking that mixing and matching constraints is straightforward in a logic, and the semantical apparatus provides us with the tools to study the resulting properties.
%
% \begin{example} The example for de-dicto DP is applicable here too.
% 	Consider  \( \delta = approve(n)\cdot approve(n') \). It implements FTU-DP for \(  hasLoan(x) \) wrt attribute \( isMale(x). \) That is, \begin{itemize}
% 		\item \( \theory \not\models \exists x \know \theta(x), \)
% 		\item \( \theory \not\models [approve(n)] \exists x \know \theta(x), \) and
% 		\item \( \theory \not\models [approve(n)\cdot approve(n')]  \exists x \know \theta(x). \)
% 	\end{itemize}    Reversing the actions, not surprisingly, \( \delta' = approve(n')\cdot approve(n) \) does not affect the matter: \( \delta' \) also implements FTU-DP. Had the sequence including sensing actions, a reversal could matter.
%
% 	% nor is it the case that \( \theory\models [\delta] \know \theta(n). \)
% \end{example}
%
% 	%  \[
% %
% % \]
% %
% % 	\begin{itemize}
% % 	\item \( \forget(\theory,\theta(n)) \models [\delta]  \wh\theta(n) \); and
% % 	\item \( \forget(\theory,\theta(n)) \land \theta(n) \models [\delta] \know \phi \) iff \( \forget(\theory,\theta(n)) \land \neg \theta(n) \models [\delta] \know \phi \).
% % \end{itemize}
%
% 	% \item \( \forget(\theory,\theta(x)) \models [\delta] \exists x \know\theta(x) \); and
% 	% \item \( \forget(\theory,\theta(x)) \land \theta(m) \models [\delta] \know \phi \) iff \( \forget(\theory,\theta(x)) \land \theta(f) \models [\delta] \know \phi \).
%
%
%
% One can also consider situations where some knowledge of protected attributes is useful to ensure there is parity but to also account for special circumstances.  In this, the protected attribute itself could be ``hidden'' in a more general class, which is easy enough to do in a relational language.
%
% \begin{example} Suppose we introduce a new predicate for underrepresented groups. We might have, for example: \[
% 	\forall x(\neg Male(x) \lor \ldots \lor RaceMinority(x) \supset Underrepresented(x))
% \]
% This could be coupled with a sensing axiom of the sort:\[
% 	\Box SF(checkU(x)) \equiv Underrepresented(x).
% \]
% Add the predicate definition and the sensing axioms to the initial theories and dynamic axioms in \( \theory \) respectivley.
% Consider \( \delta = checkU(n)\cdot checkU(n') \cdot approve(n) \cdot approve(n'). \) Then \( \delta \) implements strong DP for \( hasLoan(x) \) wrt attribute \( Underrepresented(x). \) That is, both represented and underrepresented groups have loans.
%
% \end{example}
%
% {One problem with DP  is that (unless the instance rate of $y=1$ happens to be the same in both the $a_p=0$ group and $a_p=1$ group), the classifier cannot achieve 100\% classification accuracy and satisfy the fairness criterion simultaneously \citep{Equality_of_opportunity}. Also, there are scenarios where this definition is completely inappropriate because the instance rate of $y=1$ differs so starkly between different demographic groups. For example, consider the case of recidivism prediction. On average, men are far more likely to be arrested than women, and women comprise less than 10\% of the incarcerated population in the United Kingdom \citep{UK_criminal_justice}. A hypothetical system that insisted on predictive parity between sexes for prediction of future criminality would unfairly penalise women, and an alternative criterion would be more applicable in this case \citep{Equality_of_opportunity}. Finally, there are also concerns that statistical parity measures fail to account for fair treatment of individuals \citep{Fairness_through_awareness}. Nonetheless it is widely used as a target and is often regarded as the most appropriate statistical definition when an algorithm is trained on historical data \citep{tick_cross_paper,Learning_Fair_Reps}.}
%
% A modification of demographic parity is ``equality of opportunity'' (EO). By this definition, a classifier is considered fair if, among those individuals who meet the positive criterion, the instance rate of correct prediction is identical, regardless of the value of the protected attribute \citep{Equality_of_opportunity}. This condition can be expressed as  \citep{Equality_of_opportunity}:
% \begin{equation*}
%     P(y=1|a_p=a,\hat{y}=1)=P(y=1|a_p=a',\hat{y}=1) \hspace{2mm} \forall \hspace{1mm} a,a'
% \end{equation*}
% In  \citep{Equality_of_opportunity}, it is  pointed out that a classifier can simultaneously satisfy equality of opportunity and achieve perfect prediction whereby $\hat{y}=y$ (prediction=true label) in all cases.
%
% In the logical setting, this can be seen as a matter of only looking at individuals that satisfy a criterion, such as being eligible for promotion or not being too old to run for office.
%
% \begin{definition} A sequence \( \delta = a\sub 1 \cdots a\sub k \) implements EO for \( \phi(x) \) wrt attribute \( \theta(x) \)  and criterion \( \eta(x) \) iff: \[
% 	\theory \models [\delta] \know((\forall x (\eta(x)\land \theta (x)) \supset \phi(x)) \land (\forall x \neg (\eta(x)\land \theta(x))\supset \phi(x)) ).
% \]
%
% % 	\begin{itemize}
% % 	\item \( \forget(\theory,\theta(n)) \models [\delta]  \wh\theta(n) \); and
% % 	\item \( \forget(\theory,\theta(n)) \land \theta(n) \models [\delta] \know \phi \) iff \( \forget(\theory,\theta(n)) \land \neg \theta(n) \models [\delta] \know \phi \).
% % \end{itemize}
%
% 	% \item \( \forget(\theory,\theta(x)) \models [\delta] \exists x \know\theta(x) \); and
% 	% \item \( \forget(\theory,\theta(x)) \land \theta(m) \models [\delta] \know \phi \) iff \( \forget(\theory,\theta(x)) \land \theta(f) \models [\delta] \know \phi \).
%
% \end{definition}
%
% \begin{example} Consider \( \delta = promote(n)\cdot promote(n') \), let \( \phi(x) = highSalary(x) \) and the criterion \( \eta(x) = Eligible(x). \) Although the promote action for \( n' \) does not lead her to obtain a high salary, because we condition the definition only for eligible individuals, \( \delta \) does indeed implement EO. Note again that the agent does not know the gender for \( n' \), but in every possible world, regardless of the gender \( n' \) is assigned, \( n' \) is known to be ineligible. In contrast, \( n \) is eligible and \( \delta \) leads to \( n \) having a high salary. That is, every eligible male now has high salary, and every eligible female also has high salary. (It just so happens there are no eligible females, but we will come to that.)
% 	\end{example}
%
%
% In general, the equality of opportunity criterion might well be better applied in instances where there is a known underlying discrepancy in positive outcomes between two different groups, and this discrepancy is regarded as permissible. However, as we might observe in our background theory, there is systematic bias in that no women is considered eligible.
%
%
%
% Let us now turn to CF. The existing definition forces us to consider a ``counterfactual world" where the protected attribute values are reversed, and ensure that the action sequence still achieves the goal.
%
%
%
% \begin{definition} A sequence \( \delta = a\sub 1 \cdots a\sub k \) implements CF for \( \phi \) wrt attribute \( \theta(x) \) for individual \( n \) iff: \begin{itemize}
% 	\item \( \theory \models (\theta(n)  = b) \) for \( b\in \set{0,1} \) and \( \theory \models [\delta] \know \phi \); and
% 	\item \( \forget(\theory,\theta(n)) \land (\theta(n) \neq b) \models [\delta] \know \phi \).
% \end{itemize}
%
%
% \end{definition}
%
% \begin{example} Let us consider the case of loan approvals. Consider the individual \( n \) and the action \( \delta = approve(n) \). Let \( \phi = hasLoan(n) \), and the protected attribute \( Male(x) \). Clearly \( \theory \models Male(n) \), and indeed \( \theory \models [\delta] hasLoan(n) \). If we consider \( \theory' \) where the gender for \( n \) is swapped, it is still the case that \( \theory'  \models [\delta] hasLoan(n) \). Thus \( \delta \) implements CF for \( hasLoan(n) \) wrt \( Male(n) \).
% \end{example}
%
% The definition of CF  is well-intentioned, but does not quite capture properties that might enable equity. Indeed, there is a gender imbalance in the theory, in the sense that only the  male employee is eligible for promotions and the female employee can never become eligible. Yet CF does not quite capture this. Let us revisit the example with getting high salaries:
%
% \begin{example} Consider \( \delta = promote(n) \) for property \( highSalary(n) \) wrt attribute \( Male(n) \). It is clear that \( \delta \) implements CF because the gender is irrelevant given that \( n \) is eligible. However, given \( \delta' = promote(n') \), we see that \( \delta' \) does not implement CF for \( highSalary(n') \) wrt \( Male(n') \). Because \( n' \) is not eligible, \( highSalary(n') \) does not become true after the promotion.
% \end{example}
%
% Among the many growing criticisms about formal definitions of fairness is that notions such as CF fail to capture systemic injustices and imbalances. We do not suggest that formal languages would address such criticisms, but they provide an opportunity to study desirable  augmentations to the initial knowledge or action theory.
%
% Rather than propose a new definition, let us take inspiration from DP, which seems fairly reasonable except that it is the context of what the agent knows. Keeping in mind a desirable ``positive'' property such as \( Eligible(x) \), let us consider DP but at the world-level:
%
% % example with promotion - works by gender, but doesn't seem right. what if you asked about eligibility and promotion, seems CF fair
%
% % equity based on DP. what if we looked for strong or weak equity wrt eligible, doesn't have. so make eligible, and thenn force CF
%
% % example with eliginble and then promotion
%
%
%
% % \begin{definition} A sequence \( \delta = a\sub 1 \cdots a\sub k \) implements epistemic CF for \( \phi \) wrt attribute \( \theta(x) \) for individual \( n \) iff: \begin{itemize}
% % 	\item \( \theory \models \know(\theta(n)  = b) \) for \( b\in \set{0,1} \) and \( \theory \models [\delta] \know \phi \) iff
% % 	\item \( \forget(\theory,\theta(n)) \land (\theta(n) \neq b) \models \know(\theta(n) \neq b) \land [\delta] \know \phi \).
% % \end{itemize}
% %
% %
% % \end{definition}
%
% % Consider \( \eta(x) = Hirable(x) \) as a positive property.
%
% \begin{definition} Given a theory \( \theory \), protected attribute \( \theta(x) \),  positive property \( \eta(x) \),
% 	where \( x \) is the individual,
% 	 define \emph{strong equity}: \[
% 		\theory \models \forall x (\theta(x) \supset \eta(x)) \land \forall x(\neg\theta(x) \supset \eta(x)).
% 	\]
% % %
% % %
% % % 	 follows: \begin{itemize}
% % % 	\item We say a theory \( \theory \) is \emph{inequitable} for protected attribute \( \theta(x,y) \), where \( x \) is the individual and \( y \) is the value, wrt property \( \eta(x) \) iff \[
% % % 	\theory \models \exists y\forall x~(\theta(x,y) \supset \neg \eta(x)).
% % % \]
% % % \item We say \( \theory \) is \emph{inequitable} for attribute value \( a \) iff \[
% % % 	\theory \models \forall x~(\theta(x,a) \supset \neg \eta(x)).
% % % \]
% % % \item We say \( \theory \) is \emph{equitable} iff \[
% % % 	\theory \models \forall x~(\theta(x,a) \supset \neg \eta(x))
% % % \] for every \( a\in \set{m,f}. \)
% %
% % \end{itemize}
%
% % If the above condition holds, we say \( \theory \) is strongly equitible
%
%
% 	% We say a theory is \emph{equitable} wrt \( \theta(x,y) \), where \( x \) is the individual and \( y \) is the attribute value iff
%
% \end{definition}
%
% In general, it may not be feasible to ensure that properties hold for all instances of both genders. For example, there may be only a handful of C-level executives, and we may wish that there are executives of both genders.
%
% % Consider \( \eta(x) = highSalary(x) \) as a positive property.
%
% \begin{definition} Given a theory \( \theory \), protected attribute \( \theta(x) \),  positive property \( \eta(x) \),
% 	where \( x \) is the individual,
% 	 define \emph{weak equity}: \[
% 		\theory \models \exists  x (\theta(x) \land \eta(x)) \land \exists  x(\neg\theta(x) \land \eta(x)).
% 	\]
% It is implicitly assumed that the set of positive and negative instances for \( \theta(x) \)	is non-empty: that is, assume the  integrity constraint: \[
% 	\theory \models \exists x,y (\theta(x)\land \neg \theta(y)).
% \]
% % %
% % %
% % % 	 follows: \begin{itemize}
% % % 	\item We say a theory \( \theory \) is \emph{inequitable} for protected attribute \( \theta(x,y) \), where \( x \) is the individual and \( y \) is the value, wrt property \( \eta(x) \) iff \[
% % % 	\theory \models \exists y\forall x~(\theta(x,y) \supset \neg \eta(x)).
% % % \]
% % % \item We say \( \theory \) is \emph{inequitable} for attribute value \( a \) iff \[
% % % 	\theory \models \forall x~(\theta(x,a) \supset \neg \eta(x)).
% % % \]
% % % \item We say \( \theory \) is \emph{equitable} iff \[
% % % 	\theory \models \forall x~(\theta(x,a) \supset \neg \eta(x))
% % % \] for every \( a\in \set{m,f}. \)
% %
% % \end{itemize}
%
%
%
%
% 	% We say a theory is \emph{equitable} wrt \( \theta(x,y) \), where \( x \) is the individual and \( y \) is the attribute value iff
%
% \end{definition}
%
% We assume weak equity and focus on FTU below. The definitions could be extended to strong equity or other fairness notions  depending on the modelling requirements.
%
%  % We assume weak equity  below.
%
% % \begin{definition} We define \emph{inequitable} and \emph{equitable} as follows: \begin{itemize}
% % 	\item We say a theory \( \theory \) is \emph{inequitable} for protected attribute \( \theta(x,y) \), where \( x \) is the individual and \( y \) is the value, wrt property \( \eta(x) \) iff \[
% % 	\theory \models \exists y\forall x~(\theta(x,y) \supset \neg \eta(x)).
% % \]
% % \item We say \( \theory \) is \emph{inequitable} for attribute value \( a \) iff \[
% % 	\theory \models \forall x~(\theta(x,a) \supset \neg \eta(x)).
% % \]
% % \item We say \( \theory \) is \emph{equitable} iff \[
% % 	\theory \models \forall x~(\theta(x,a) \supset \neg \eta(x))
% % \] for every \( a\in \set{m,f}. \)
% %
% % \end{itemize}
% %
% %
% %
% %
% % 	% We say a theory is \emph{equitable} wrt \( \theta(x,y) \), where \( x \) is the individual and \( y \) is the attribute value iff
% %
% % \end{definition}
%
% \begin{definition} A sequence \( \delta = a\sub 1 \cdots a\sub k \) implements equitable FTU for \( \phi \) wrt protected attribute \( \theta(x) \) and property \( \eta(x) \)  iff \begin{itemize}
% 	\item Either weak equity holds in \( \theory \)  and \( \delta \) implements FTU; or
% 	\item \( \delta \) implements equitable FTU for \( \phi \) wrt \( \theta(x) \) and  \( \eta(x) \) for the updated  theory \( \forget(\theory,S) \), where \( S = \set{\eta(n\sub i) \mid i \in N}. \)
%
% 	% \( \delta\su * \) implements CF, where \( \delta\su * = \forget(\delta,\eta(x)). \)
% \end{itemize}
%
% \end{definition}
%
% Note that we are assuming that \( N \) is finite here because we have only defined forgetting wrt finitely many atoms. Otherwise, we would need a second-order definition.
%
% \begin{example} Let us consider \( \delta = promote(n)\cdot promote(n') \) for goal \( \phi = \forall x (highSalary(x)) \) wrt protected attribute \( Male(x) \) and property \( Eligible(x) \). It is clear that weak equity does not hold for \( \theory \) because there is a female who is not eligible. In this case, we consider \( \theory'= \forget(\theory, S) \) where \( S =\set{Eligible(n), Eligible(n')}. \) However, with that, \( \theory' \) also does not mention that \( n \) is eligible, so the promotion actions does not lead to anybody having high salaries. So \( \delta \) does not enable knowledge of the goal \( \phi. \)
%
% 	% the first condition in the definition of implementing CF fails, so \( \delta \) does not implement equitible CF.
%
% \end{example}
%
% \begin{example} Let us consider \( \theory' \) that is like \( \theory \) except that \( Eligible(x) \) is not rigid, and can be affected using the action \( make(x) \): \[
% 	\Box [a] Eligible(x) \equiv Eligible(x) \lor (a=make(x)).
% \]
% That is, either an individual is eligible already or the manager makes them. Of course,  \( \delta = promote(n)\cdot promote(n') \) from above still does not implement equitable FTU, because we have not considered any actions yet to make individuals eligible. However, consider \( \delta' = make(n)\cdot  make(n')\cdot promote(n)\cdot promote(n')  \). Because \( \theory \) does not satisfy weak equity, we turn to the second condition of the definition. On forgetting, no one is eligible in the updated theory, but the first two actions in \( \delta' \) makes both \( n \) and \( n' \) eligible, after which, they are both promoted. So \( \delta' \) enables knowledge of \( \forall x (highSalary(x)) \). Thus, the actions have made clear that eligibility is the first step in achieving gender balance, after which promotions guarantee that there are  individuals of both genders with high salaries.
% \end{example}
%
% % eligibility means no one is promoted
% % eligibility enable, and gender balance achieved
%
%
% % Many options here: eliminate preconditions, or forget tests in programs.
% %
% % \begin{definition} We say \( \theory \) satisfies \emph{epistemic equity} iff \[
% % 	\theory \models \exists  x,y~\know((\theta(x) \land \eta(x)) \land (\neg\theta(x)\land \eta(x)))
% % \] % for every \( a\in \set{m,f}. \) TBD: what if you don't know gender
% %
% % 	\end{definition}
% %
% %
% %
% % % \begin{definition} We say \( \theory \) is \emph{epistemically equitable} iff \[
% % % 	\theory \models \forall x~\know(\theta(x,a) \supset \neg \eta(x))
% % % \] for every \( a\in \set{m,f}. \) TBD: what if you don't know gender
% % % 	\end{definition}
% %
% %
% % \begin{definition} A sequence \( \delta = a\sub 1 \cdots a\sub k \) implements epistemically equitable CF for \( \phi \) wrt protected attribute \( \theta(x,y) \) and property \( \eta(x) \)  iff \begin{itemize}
% % 	\item Either \( \theory \) satisfies  epistemic equity and \( \delta \) implements CF; or
% % 	\item \( \delta \) implements epistemically equitable  CF for \( \phi \) wrt \( \theta(x) \) and property \( \eta(x) \) for the updated background theory \( \forget(\theory,\eta(x)). \)
% % \end{itemize}
% %
% % \end{definition}
% %
% % 	maybe too strong, why should the agent know of it - we need to introduce actions that do this.
% %
% % \begin{definition} We define \( \theory\su + \) the augmentation of \( \theory \) wrt protected attribute \( \theta(x) \) and positive property \( \eta(x) \) as the union of \( \theory \) and: \[
% % 	\set{\exists x,y (\theta(x)\land \eta(x)) \land (\neg \theta(y)\land \eta(y)) }.
% % \]
% % \end{definition}
% %
% % \begin{proposition} Suppose \( \theory\models \theory\su +. \) Then \( \theory  \) satisfies weak equity.
% % 	\end{proposition}
% %
% %
% %
% % TBD: morally permissable, morally acting agent, good states can also be captured

\subsection{Possibility vs Knowledge} % (fold)
\label{sub:possibility_vs_knowledge}

In many applications, we may not require that the agent knows \( \phi \), only that it considers \( \phi \) possible. In the explanation generation framework of  \cite{vasileiou2022logic}, for example, there is a notion of credulous entailment where a single belief state suffices to checking the validity and achievement of a plan. (In contrast, skeptical entailment is when every belief state is involved.) The analogous notion in an epistemic language is to introduce a companion modal operator \( \Bel \) with the following semantics: \begin{itemize}
	\item \( e,w,z\models \Bel\alpha \) iff there is some \( w' \sim\sub z w, w'\in e \) such that \( e,w',z\models \alpha. \)
\end{itemize}
As long as there is at least one world where \( \alpha \) is true, \( \Bel\alpha \) is evaluated to true at \( (e,w,z) \). This is a companion  modal operator to \( \know \) for \emph{possibility.}  We might introduce an analogue to Definition \ref{defn missing knowledge and action} in using \( \Bel \) instead of \( \know \) as the modality in the goal. To see how this works, let us revisit Example \ref{ex:missing knowledge and action}. 

\begin{example} Consider the modified precondition axiom for \( quench(x) \), and let \( \inits \), \( \init\su * \), \( \delta \) and \( \delta' \) be as in Example \ref{ex:missing knowledge and action}. By only-knowing \( \inits \), the agent considers some worlds where \( h \) is metallic, and others where it is not. Thus, we now see that we do not really need to suggest \( \alpha = Metal(h) \) to the agent if the weaker notion of a CF explanation is considered. Indeed, \( \init\su * \land \dyn \land \oknow(\inits\land \dyn) \) entails \( [\delta'](\phi\land \Bel\phi) \) for \( \phi = Broken(h) \). 
	
	
\end{example}

In other words, we have augmented actions but we did not need to augment knowledge in the above example.  Had the agent believed false things, then we might have needed to augment both, and so can appeal to Definition \ref{defn false beliefs} but using \( \Bel \) instead of \( \know \) to allow for credulous-type reasoning. 

% subsection possibility_vs_knowledge (end)

\section{Other related efforts} % (fold)
\label{sec:other_related_efforts}

In addition to the works discussed in previous sections, the following efforts are  related.


There is some syntactic (and perhaps intuitive) connection to explanation-based diagnosis. For example, in \cite{sohrabi2010diagnosis}, the idea is to encode the behavior of the system to be diagnosed as a situation calculus action theory, encode observations as situation calculus formulae, and conjecture a sequence of actions to explain what went wrong with the system. (However, they often need to model  faulty or abnormal components when defining the notion of a diagnosis.)
In our setting, in contrast, we identify actions and/or knowledge that determine how an outcome can be changed. Nonetheless, we believe that a further formal study to relate such accounts would be useful, and could nicely complement empirical works such as \cite{dai2022counterfactual}. See also Ginsberg \cite{ginsberg1986counterfactuals}.


We previously discussed the reduction of projection and reasoning about knowledge to non-modal reasoning \cite{LakemeyerLevesque2004}, but we did not elaborate on generating plans. For more information on synthesizing plans, programs, and epistemic plans, see \cite{classen2007towards,DBLP:journals/logcom/DitmarschHL11,reifsteck2019epistemic,baral_et_al:DR:2017:8285} and their references.

An alternative approach to computing properties and plans is through answer set programming (ASP) \cite{baral2005logic,gelfond1993representing}, which also supports  reasoning about knowledge   \cite{fandinno2022thirty}. In general, our formalization does not preclude consideration of other logical languages. For example, in the simplest setting in this paper, a counterfactual explanation is the synthesis of a course of action that negates the goal or knowledge about the goal. In fact, \cite{bogatarkan2020explanation} consider counterfactual explanations for multi-agent systems, that is also motivated in terms of offering an  alternative course of action. Although they do not explore a range of definitions with references to knowledge as we do, exploring whether our definitions can be implemented in such approaches is worthwhile.

Such a course for formalisation may help better relate our efforts to declarative approaches to counterfactual explanations. For example,  \cite{bertossi2021declarative} explores the use of ASP for generating counterfactual explanations, but in a classical machine-learning sense, determined by how much certain features affect the overall prediction (understood as a causal link). 


% section other_related_efforts (end)

\section{Conclusions} % (fold)
\label{sec:conclusions}

We developed an account of counterfactual explanations and reconciliation-based counterfactual explanations in this paper. This allows for a simple and clear specification  in the presence of missing  actions, partial knowledge, weakened beliefs and false beliefs. Existing accounts of discrepancy in plans,  among others, can be seen as variations of this more general specification.  For the future, it would be interesting to incorporate other notions  in our formalization, such as operational aspects of plans, costs, optimality and conciseness  \cite{fox2017explainable,sreedharan2018handling,vasileiou2022logic}, towards a unified mathematical specification of explainable planning.


% We deliberately chose a general language for exploring the topic, so that we could carefully delineate between what is true, what is known, and how the objective notion of an explanation could be constrated against an agent that is getting informed about the world through sensing actions. Although we were still able to establish that a propositional reduction is possible as far as entailment is concerned, we did not quite build a proposal on existing planning languages. We felt the general language provides clarity in the exposition, and one might be able  piggyback on existing proposals that link the situation calculus, linear dynamic logic and planning \cite{classen2007towards,Fritz:2008uq,de2013linear}, and \cite{shvo2022resolving} in particular whose plan generation algorithm could be adapted for our purposes. For the future, it would be interesting to incorporate other notions from explainable AI planning  in our formalization, as well as operational aspects of plans, such as costs, optimality and conciseness  \cite{fox2017explainable,sreedharan2018handling,vasileiou2022logic}.

% We believe the language might also allow for a fuller study of other popular proposals in explainable AI planning \cite{fox2017explainable}, such as model reconciliation \cite{sreedharan2018handling} and theory of mind reasoning \cite{shvo2022resolving} where the interaction between truth and knowledge is paramount. Handling optimality and conciseness  \cite{fox2017explainable,sreedharan2018handling} are clearly relevant too, and formalizing these at the logical level is advantageous from a theoretical standpoint \cite{vasileiou2022logic}.


% One problem with this definition is that (unless the instance rate of $y=1$ happens to be the same in both the $a_p=0$ group and $a_p=1$ group), the classifier cannot achieve 100\% classification accuracy and satisfy the fairness criterion simultaneously \cite{Equality_of_opportunity}. Also, there are scenarios where this definition is completely inappropriate because the instance rate of $y=1$ differs so starkly between different demographic groups. For example, consider the case of recidivism prediction. On average, men are far more likely to be arrested than women, and women comprise less than 10\% of the incarcerated population in the United Kingdom \cite{UK_criminal_justice}. A hypothetical system that insisted on predictive parity between sexes for prediction of future criminality would unfairly penalise women, and an alternative criterion would be more applicable in this case \cite{Equality_of_opportunity}. Finally, there are also concerns that statistical parity measures fail to account for fair treatment of individuals \cite{Fairness_through_awareness}, a topic we will explore more when considering the shortcomings of fairness through intra-processing. Nonetheless it is widely used as a target and is often regarded as the most appropriate statistical definition when an algorithm is trained on historical data \cite{tick_cross_paper,Learning_Fair_Reps}.
%
% A modification of demographic parity is ``equality of opportunity''. By this definition, a classifier is considered fair if, among those individuals who meet the positive criterion, the instance rate of correct prediction is identical, regardless of the value of the protected attribute \cite{Equality_of_opportunity}. Mathematically, this condition can be expressed as follows \cite{Equality_of_opportunity}:
% \begin{equation}
%     P(y=1|a_p=a,\hat{y}=1)=P(y=1|a_p=a',\hat{y}=1) \hspace{2mm} \forall \hspace{1mm} a,a'
% \end{equation}
% The authors of \cite{Equality_of_opportunity} point out that a classifier can simultaneously satisfy equality of opportunity and achieve perfect prediction whereby $\hat{y}=y$ in all cases. The equality of opportunity criterion might well be better applied in instances where there is a known underlying discrepancy in positive outcomes between two different groups, and this discrepancy is regarded as permissible.

% In this paper, we looked into notions of fairness from the machine learning literature, and inspired by these, we attempted a formalization in an epistemic logic. Although we limited ourselves to categorical knowledge and noise-free observations, we enrich the literature by considering actions.  Consequently we looked into three notions: fairness through unawareness, demographic parity and counterfactual fairness, but then expanded these notions to also tackle equality of opportunity as well as equity. We were also able to mix and match constraints, showing the advantage of a logical approach, where one can formally study the properties of (combinations of) definitions. Using a  simple basic action theory we were nonetheless able to explore these notions using action sequences.
%
% As mentioned earlier, this is only a first step and as argued in works such as \cite{pagnucco2021epistemic,dehghani2008integrated,halpern2018towards} there is much promise in looking at ethical AI using rich  logics. In fact, we did not aim to  necessarily faithfully reconstruct existing ML notions in this paper but rather  study underlying principles. This is primarily because we are not focusing on single-shot prediction problems but how actions, plans and programs might implement fairness and de-biasing. The fact that fairness was defined in terms of actions making knowledge of the goal true, exactly as one would in planning \cite{DBLP:conf/aaai/Levesque96}, is no accident.
%
% State-of-the-art analysis in fairness is now primarily based on false positives and false negatives  \cite{verma2018fairness}. So we think as the next step, a probabilistic language such as \cite{Bacchus1999171} could bring our notions closer to mainstream definitions, but now in the presence of actions. In the long term, the goal is to logically capture bias in the presence of actions as well as repeated harms caused by systemic biases \cite{creager2020causal}. This  would enable the use of formal tools (model theory, proof strategies and verification algorithms) to study the long-term impact of bias while ensuring fair outcomes throughout the operational life of autonomous agents.

% TBD: need probabilistic notions after all, equality of opportunity

% section conclusions (end)

% section fairness (end)

% {\small \bibliographystyle{abbrv}
% \bibliography{main,references,bibliography}
% }
% \clearpage
% \appendix
%
% \section{Propositional Fragments} % (fold)
% \label{sub:finite_fragment}
%
% As is usual in first-order logic \cite{smullyan1995first}, the language allows predicates of every arity, and as established a countably infinite set of domains. But given any background theory \( \theory \), it can only mention finitely many predicates, of course. Moreover, we might further restrict ourselves to finitely many constants \( D \), such that there are only finitely many atoms and finitely many action terms wrt \( (\theory,D) \). {We can also enforce a finite domain using  \emph{relativization}. For example, let: \[
% \forall x	(Object(x) \equiv x = c \lor \ldots  \lor x = d).
% \]
% This declares finitely many objects.
% Then instead of saying \( \exists x.~Fragile(x) \), which in general means that any one of the infinitely many constants is fragile, we would write: \[
% 	\exists x (Object(x) \land Fragile),
% \]
% which declares that only those from \( \set{c, \ldots, d} \) are fragile.
% }
%
% Henceforth, we write \( \esprop \) to mean such a propositional fragment, regardless of how it might have been instantiated. It can be shown that projection reduces to propositional reasoning in \( \esprop. \)
%
% To establish this, we need to resort to the notion of \emph{regression} \cite{reiter2001knowledge} and the so-called representation theorem for only-knowing \cite{levesque2001logic}.
%
% \begin{theorem} \cite{LakemeyerLevesque2004} Suppose \( \theory,  \) is as above, \( \delta \) is any action sequence, and \( \phi \) is any formula that
% 	not mentioning \( \set{\oknow,[a],\Box} \). (That is, it can mention \( \know. \)) Then, there is a non-modal formula \( \E(\R(\delta,\phi),\theoryinits) \) where: \[
% 		\theory \models [\delta]\phi \text{ iff } \theoryinit \models \E(\R(\delta,\phi),\theoryinits).
% 	\]
% \end{theorem}
% Essentially \( \R(\delta,\phi) \) is the regression of \( \phi \) wrt the action sequence \( \delta \), and it will be a formula about the initial situation. For example, suppose \( \phi \) was of the form \( \alpha \land \know \beta \). The regression of \( \phi \) wrt \( \delta \) might yield a formula of the form \( \phi' = \alpha' \land \know \beta'. \) Checking if \( \phi \) is true after \( \delta \) wrt \( \theory \), then, would be equivalent to checking whether \( \theoryinit \land \oknow\theoryinits \models \phi' \). The representation theorem uses whatever is only-known, which is \( \theoryinits \) in this case, and establishes that checking if \( \know\beta' \) is equivalent to checking whether \( \beta'' =  \E(\know\beta',\theoryinits) \) is valid. In other words, we are to check if \( \theoryinit \models \alpha' \land \beta''.  \)
%
% %Prop
% \begin{corollary} Given \( \theory, \delta \) and \( \phi \) as before, the projection problem in \( \esprop \) reduces to propositional reasoning.
%
% \end{corollary}
%
% \begin{example} Suppose we are interested in the following properties: \begin{itemize}
% 	\item \( \theory \models  (\neg \know Glass(d) \land \neg \know \neg Glass(d)) \);
% 	\item \( \theory \models \know \neg \know Glass(d) \);
% 	\item \( \theory \models [isGlass(h)] \know  Glass(d) \); and
% 	\item \( \theory \models [isGlass(h)] \know \know \know Glass(d). \)
% \end{itemize}
% That is, the agent's initial beliefs imply that the agent does not know whether \( d \) is made of glass. Moreover, by introspection, the agent knows that it does know if \( d \) is made of glass. But after sensing \( h \) for glass, it knows that it knows that it knows (and so on arbitrarily) that \( d \) is made of glass.
%
% For the first item, we are really checking the knowing whether expression against \( \oknow(\inits \land \dyn) \). However, because no actions are involved, regression establishes that we only need to check the expression against \( \oknow\inits. \) The representation theorem attempts to eliminate nested belief operators, so that \( \oknow\inits \models \neg \know Glass(d) \) amounts to checking whether \( \inits \not\models Glass(d).  \) Since \( \inits \) mentions nothing about \( d \), it follows that \( Glass(d) \) is not entailed by \( \inits \), so the property follows. Note that neither is \( \neg Glass(d) \) entailed by \( \inits \), and so we can reduce \( \oknow\inits \models \neg \know\neg Glass(d) \) to non-modal reasoning as well.
%
% For the second item, again by regression, we only \( \oknow\inits \) to check \( \know\neg\know Glass(d) \). The representation theorem establishes that provided \( \oknow\inits \models \know \psi \), the desired entailment holds. Here \( \psi \) is a test that returns \( \true \) if \( \inits \not\models Glass(d) \), and \( \false \) otherwise. From the first item, we know that \( \psi \) would return \( \true. \) So we are to check if \( \oknow\inits \models \know \true. \) Once again, by means of the representation theorem, this boils down to checking whether \( \inits \models \true, \) which is trivially the case, and therefore item 2 holds.
%
% For the third item, for the regression of \( [a] \know \phi \), we appeal to the equivalent of the successor state axiom for \( \know \), and return its RHS. In other words, we need to check whether \( \theory \) entails:
% 	 \begin{align*}
% 		 (SF(a) \land \know(SF(a) \supset [a] Glass(h))) \lor \\  (\neg SF(a) \land [a]\know(\neg SF(a) \supset  Glass(h)))
% 	\end{align*}
% where \( a = isGlass(h) \), and as expressed by the sensing axioms, \( SF(a) \equiv  Glass(h) \). Because \( \theory \models Glass(h) \), the second disjunct is false. In other words, we are to check if: \[
% 	\theory \models Glass(h) \land \know(Glass(h) \supset [a] Glass(h))
% \]
% Regression is applied repeatedly until all actions are eliminated from the entailment.  Because the sensing action does not affect the predicate \( Glass(x) \), the regression of  \( [a] Glass(h) \)  is simply \( Glass(h) \). We are left to check whether: \[
% 	\init \land \oknow\inits \models Glass(h)\land \know(Glass(h) \supset Glass(h)).
% \]
% That is, we use the truth of the real world (\( \init \)) as well as whatever is only-known to check if \( Glass(h) \land \know \true \) holds. We know that \( \init\models Glass(h) \) and from item 2, we know \( \oknow\inits \models \know\true, \) so item 3 holds.
%
% 	Item 4 is an extension of item 3 where we repeatedly regress the nested belief operators against \( a \), until we are left with checking if \( \init \land \oknow\inits \) entails \( Glass(h) \land \know \true \), which we ensured does hold in item 3.
%
%
%
%
% \end{example}
%
% 	Note that in all of the above, by establishing finitely many propositions and finitely many ground action symbols, non-modal reasoning = propositional reasoning.
%
% Finally, a note about complexity of reasoning: when applying the representation theorem for properties such as \( \oknow\inits \models\know\psi \), where both \( \inits \) and \( \psi \) are propositional, we need to appeal to propositional validity for asserting \( \inits' \models \psi. \) Thus, the reduction and reasoning is coNP-complete.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
