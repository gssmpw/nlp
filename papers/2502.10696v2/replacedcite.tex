\section{Background and Related Work}
\label{sec_background}

\subsection{Unit Test Generation}
\revise{
In the literature, to reduce the manual effort involved in writing unit tests, researchers have proposed numerous approaches for automatically generating test cases, including heuristic-based____, random-based____, and symbolic execution____
Among them, EvoSuite____ and Randoop____ are widely regarded as foundational works in the field of automated test generation, providing critical guidance for the conception and development of subsequent approaches.
However, such traditional test generation tools often rely on heuristics or randomness to generate assertion statements without considering the code semantics of focal methods, and thus are limited in their ability to generate useful and meaningful assertions____.
For example, Almasi~\etal____ conduct an investigation of EvoSuite and Randoop in an industrial software system, and find that ``the assertions are meaningful and useful unlike the generated ones''.
As a complement to traditional test generation tools, automated assertion generation attempts to synthesize program assertions based on the functions under test.
This topic has garnered significant attention in recent years____, and is the focus of this paper.
}

\label{sec:background_AG}



\subsection{DL-based Assertion Generation}
\label{sec:background_dl_ag}

With the success of DL, researchers have increasingly been utilizing advanced DL techniques to automate a variety of software engineering tasks____, such as program repair____ and vulnerability detection____.
In the era of unit testing, Watson~\etal____ introduce \atla{}, the first DL-based AG approach to directly predict an accurate assertion from its focal-test by sequence-to-sequence learning.
They first extract a mass of TAPs from open-source projects in the wild and then abstract source code to train a recurrent neural network (RNN) model.
As illustrated in~\figref{fig:tap}, each TAP in \atla{} consists of two components: a focal-test (\ie a focal method denoting the method under test and a test prefix denoting a test method without any oracle) and its assertion.

\begin{figure}[t]
	\graphicspath{{figs/}}
	\centering
	\includegraphics[width=0.99\linewidth]{tap.pdf}
	\caption{Example of a Test-Assertion Pair (TAP) in \atla{}}
	\label{fig:tap}
\end{figure}

The community has also seen some preliminary explorations of PLMs in supporting assertion generation____.
For example, Mastropaolo~\etal____ investigate the performance of the T5 model in supporting four tasks by transfer learning, including bug-fixing, mutant injection, assertion generation, and code summarization.
These studies typically pre-train a language model from scratch with source code or English texts, and fine-tune them to benefit multiple downstream tasks.
On the contrary, we aim to propose a specific AG approach \toolname{} empowered with off-the-shelf PLMs.
Recently, Nashid____ propose CEDAR, a prompt-based few-shot learning approach for both program repair and assertion generation.
CEDAR queries a large language model (LLM) Codex____ to generate an assertion by constructing a prompt that includes natural language instructions, several examples of task demonstrations, and an output query.
Our work essentially differs from CEDAR regarding (1) the retriever (offline retrieval \textit{v.s.} online optimization); (2) the generator (a black-box billion-level LLM \textit{v.s.} an open-source million-level PLM); and (3) the learning paradigm (few-shot learning with prompt engineering \textit{v.s.} retrieval-augmented fine-tuning). 

\subsection{Retrieval-based Assertion Generation}

Information Retrieval (IR) has been regarded as an effective means to boost the application of DL techniques in software engineering tasks____.
Inspired by the integration of IR and DL, Yu~\etal____ propose a family of retrieval-based approaches for assertion generation:
(1) \irar{} retrieves a TAP with the highest Jaccard similarity on the code token level given a focal-test, and returns its corresponding assertion as output;
(2) $RA_{adapt}$ further replaces incorrect tokens in retrieved assertions from \irar{} with two adaption strategies, \ie a heuristic-based approach \rah{}, and a neural network-based approach \rann{};
and (3) \inte{} builds an inference model to calculate the ``compatibility'' of assertions produced by the above three approaches, and utilizes \atla{} to predict an assertion from scratch if the compatibility is below a pre-defined threshold.
To address limitations of \inte{}, Sun~\etal____ propose \edit{}, a retrieve-and-edit AG approach based on an IR retriever and an LSTM-based sequence-to-sequence model.
\revise{Similar to aforementioned studies____, \toolname{} also follows a \textit{retrieval-and-generation} pipeline; however, the distinction between \toolname{} and prior work mainly lies in three key aspects: the retriever, the generator, and the training paradigm.
First, prior work utilizes a sparse retriever (\eg Jaccard similarity____) based on lexical matching, while \toolname{} builds a dense retriever to search for relevant TAPs with more meaningful code semantics.
Second, prior work trains an assertion generator with a basic encoder-decoder model (\eg RNNs____) from a limited number of labeled data, 
while \toolname{} is built upon off-the-shelf CodeT5, which is optimized from a large codebase to obtain meaningful vector representations for unit tests.
Third, prior work restricts the retriever only to provide similar assertions without benefiting from training, \toolname{} trains the dense retriever to learn how to better guide the generation process with a unified joint
training strategy.}

\delete{Unlike prior work____ relying on a token-based retriever, \toolname{} utilizes a dense retriever to consider code semantics, which is a parametric model and can be further optimized with the generator.}



\subsection{Pre-Trained Language Model}
\label{sec_background_LLM}

PLMs have demonstrated their potential capabilities to revolutionize a mass of software engineering tasks____.
Existing PLMs are fundamentally built with the Transformer architecture____ and are categorized into three main types.
(1) \textbf{Encoder-only PLMs}____ train the encoder to convert an input into a fixed-size context vector with masked language modeling.
(2) \textbf{Decoder-only PLMs}____ train the decoder to predict the next word in a sequence given the previous word with unidirectional language modeling.
(3) \textbf{Encoder-Decoder PLMs}____ train both the encoder and decoder to encode the input sequence and generate an output sequence with denoising objectives.
Overall, encoder-only PLMs are trained to produce bidirectional representations, thus suitable for code understanding, such as vulnerability detection____, while decoder-only LLMs are typically used for auto-regressive generation, such as code completion____.
Encoder-decoder LLMs combine the respective advantages of both the encoder and decoder to understand inputs and generate relevant outputs and suitable sequence-to-sequence generation, such as program repair____.

In this work, we select encoder-decoder PLMs as the foundation model of \toolname{}, because \toolname{} generates assertions in a sequence-to-sequence learning manner.
Following previous PLM-based studies____, we implement \toolname{} with CodeT5, a generic code-aware language model that is pre-trained on a large code corpus, and achieve state-of-the-art performance in both code understanding and generation tasks.
In the literature____, CodeT5 is the most popular and widely-adopted language model that is fine-tuned to support sequence-to-sequence code generation tasks.


\subsection{\revise{Information Retrieval for SE}}
\label{sec:background_IR}

\revise{
Information Retrieval (IR) involves searching for relevant data within large datasets, typically in response to a specific query. 
IR techniques have been widely applied in various code-related tasks, such as fault localization____ and test case prioritization____.
These techniques aim to identify the most relevant objects within a database by leveraging different similarity measures, such as Jaccard similarity, which quantifies the overlap between elements in two sets.
In recent years, the advent of PLMs has spurred the development of retrieval-augmented paradigms for generation tasks. 
This paradigm has found applications in areas such as program repair____ and code summarization____. 
By incorporating external knowledge through retrieval, this paradigm enhances the quality of generated outputs, supplementing the internal knowledge representation of PLMs____.
Although the retrieval-augmented generation pipeline has been explored in prior work____, we are the first to investigate its effectiveness for assertion generation by using external knowledge sources to fine-tune the PLM-based retriever and generator jointly.
}