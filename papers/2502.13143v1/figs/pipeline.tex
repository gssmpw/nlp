\begin{figure*}[t!]
  \begin{center}
  \includegraphics[width=\linewidth]{figs/src/pipeline.pdf}
  \caption{\textbf{Overview of \sofar~system}. \sofar~takes RGB-D images as inputs, where the depth images can be obtained from depth senor or metric 3D prediction~\cite{Metric3D23}. Given the language instruction, \sofar~prompts the VLM to obtain task-oriented object phases and semantic orientation descriptions. Then, \sofar~leverage foundation models Florence-2~\cite{florence2} and SAM~\cite{SAM23} to segment depth point clouds and our PointSO (\cref{sec:PointSO}) to obtain semantic orientations. Summarizing 3D object-centric information, an orientation-aware scene graph is constructed and encoded into languages (\cref{sec:sofar_graph}). The VLM takes the RGB image and the scene graph and outputs the queried spatial understanding VQA or translation for manipulation.
  } \label{fig:pipeline}
  \end{center}
  \vspace{-5pt}
\end{figure*}
