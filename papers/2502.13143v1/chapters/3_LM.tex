\vspace{5pt}
\section{\sofar: Semantic Orientation Bridges
Spatial Reasoning and Object Manipulation}\label{sec:sofar_graph}
Our proposed PointSO model now paves the off-the-shelf for object-centric spatial orientation understanding.
However, it is still unclear how to leverage such object-centric spatial understanding for scene-level spatial reasoning both in the digital world (\eg, orientation-aware visual question answering, VQA) and in the physical world (\eg, robot manipulations).
To enable such applications, we build an integrated reasoning system where a powerful VLM acts as an agent and reasons about the scene while communicating with off-the-shelf models including PointSO and SAM~\cite{SAM23}. 
\cref{fig:pipeline} illustrates an overview of our proposed framework, aiming at \textbf{S}emantic \textbf{O}rientation \textbf{F}or \textbf{A}utonomous \textbf{R}obotic manipulation (\textbf{\sofar}). \sofar~consumes an RGB-D image and a language query as input and first leverages off-the-shelf models including SAM and PointSO to convert the image into an orientation-aware 3D scene graph. Then \sofar~leverages a VLM agent to produce planning outcomes based upon the scene graph and the input language query, which can be later used for robot manipulation. We will introduce the construction of the orientation-aware 3D scene graph in Section~\ref{sec:scene_graph} and how to perform spatial-aware task reasoning and plan for robot manipulation in Section~\ref{sec:manip_pipline}.


\input{figs/real}
\subsection{Orientation-Aware Scene Graph from RGB-D}
\label{sec:scene_graph}
To convert the input RGB-D image into an orientation-aware 3D scene graph, we first segment the RGB image to obtain object-level 3D point clouds using SAM~\cite{SAM23} and then construct a scene graph with object-attribute nodes.

\input{tabs/open6dor}
\subsubsection{Task-Oriented Object Segmentation}
Given a language query $\mathcal{Q}$, we first prompt a VLM model $\mathcal{F}_{\text{VLM}}$ to abstract the task-oriented \textit{object phrase set}.
Thus, a set $\mathcal{P} = \{p_i|i=1,2,\dots,M\}$ with $M$ object phrases in language will be generated from $\mathcal{Q}$.
With set $\mathcal{P}$, we use language-conditioned object segmentation with SAM to obtain an object set $\mathcal{X} = \{X_i|i=1,2,\dots, M\}$, where $X_i$ is the 3D point cloud of the $i$-th object.
Besides, we assign individual IDs to objects which are used for Set-of-Mark (SoM) prompting~\cite{SoM23} on VLM's image input.
Next, we prompt the VLM to generate every object's corresponding task-oriented language description set $L_i$. We predict the semantic orientation using pretrained PointSO for each description in the description set $L_i$, forming the semantic orientation set $S_i$ for the $i$-th object.


\subsubsection{Orientation-Aware 3D Scene Graph}
From segmented object set $\mathcal{X}$, we construct an orientation-aware scene graph $\mathcal{G}=(\mathbf{V},\mathbf{E})$ with $M$ object nodes.
Each node $\mathbf{o}_i \in \mathbf{V}$ consists of following \textit{semantic} and \textit{spatial} attributes:
\ding{182} object name $p_i$ with an individual ID $i$ that distinguishes instances;
\ding{183} object position as 3D centroid coordinate $\mathbf{c}_i =(x,y,z)\in \mathbb{R}^3$ from segmented depth;
\ding{184} object's 3D bounding box size $\mathbf{b}_i=(h,w,l)\in \mathbb{R}^3$;
\ding{185} semantic orientation set $S_i$ and the corresponding task-oriented language-description set $L_i$.
Each edge $\mathbf{e}_{ij}\in\mathbf{E}$ stores the relative translation and bounding box size ratio between the connected two objects $\mathbf{o}_i$ and $\mathbf{o}_j$.

\subsection{Spatial-Aware Task Reasoning}\label{sec:manip_pipline}
We convert the orientation-aware scene graph $\mathcal{G}$ into descriptive language and input this, along with the RGB image $I$ and query $\mathcal{Q}$, into a VLM. With spatial knowledge accurately encoded in the scene graph, the VLM can leverage its robust image and language understanding capabilities to produce high-quality spatial reasoning results.

\subsubsection{Chain-of-Thought Spatial Reasoning}
Since most of the robot manipulation problems involving rigid bodies can be abstract as changing the position and orientation of objects, we especially focus on instructing the VLM to output a goal transformation as the plan for robot manipulation. To realize this, we use chain-of-thought (CoT) reasoning~\cite{CoT22} that regularizes the VLM to derive a transformation given a language-described manipulation goal in 3 steps: 
i) scene analysis of the language query $\mathcal{Q}$ and object nodes $\mathbf{V}$;
ii) manipulation analysis to provide an analytical calculation process of the target object's desired position and orientation;
iii) output task-desired object position $\Tilde{\mathbf{c}}_i$ and semantic orientation set $\Tilde{S}_i$ for each object. Afterward, given each object's initial position $\mathbf{c}_i$ and semantic orientation set $S_i$, the 6-DoF transformation matrix $\mathbf{P}_i$ can be derived.
Specifically, the translation transformation $\mathbf{t}_i = \Tilde{\mathbf{c}}_i-\mathbf{c}_i$, and we solve the rotation transformation $\mathbf{R}_i$ from $S_i$ and  $\Tilde{S}_i$ using Kabsch-Umeyama algorithm~\cite{Kabsch76,Kabsch78,Umeyama1991least}.

\subsubsection{Low-Level Motion Execution}
\label{execution}
Similar to CoPa~\cite{CoPa24}, our model includes task-oriented grasping and task-aware motion planning.
We first segment the manipulated objects or parts using Florence-2~\cite{florence2} and SAM~\cite{SAM23} to obtain the object point cloud, and then we use GSNet~\cite{GraspNet1B20} to generate grasp pose candidates.
We select the most effective grasp pose by balancing the grasp score and the angle between the robot’s approaching direction and the world frame’s z-axis.
Conditioned on the text instruction, \ours~predicts the target object’s translation and rotation matrix, which define the transformation from the grasp pose to the placement pose. An open-source motion planning module \cite{ompl} is then used to generate a collision-free trajectory. In addition, we set the initial joint position as the midpoint to achieve smooth motion while reducing collisions between the manipulated object and the environment.
