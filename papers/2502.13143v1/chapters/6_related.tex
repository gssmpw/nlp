\section{Related Works}
\subsection{Vision-Language Models for Spatial Understanding}
Vision-Language Models(VLMs) are rapidly being developed in research community, driven by the storming lead in extending GPT-style~\cite{GPT1_18,GPT2_19,GPT3_20} Large Language Models (LLMs) like LLaMA~\cite{LLaMA23,LLaMA2_23} to VLMs~\cite{Flamingo22,LLaVA23,LLaVA1.523,DreamLLM23,Emu23,LLaMA-Adapter23,ChatSpot23,MolmoAndPixMo24,Cambrian24,PaLME23,LVM23}.
SpatialVLM~\cite{SpatialVLM24} pioneers this direction by constructing VQA data in spatial understanding from RGB-D, which is used for training an RGB-only VLM.
Following SpatialVLM, SpatialRGPT~\cite{SpatialRGPT24} extends RGB-based spatial understanding to RGB-D by constructing spatial understanding data using 3D scene graphs.
SpatialBot~\cite{SpatialBot24} explores RGB-D spatial reasoning through hierarchical depth-based reasoning.
Some other works propose visual prompting for improving GPT-4V's spatial understanding~\cite{3DAxiesPrompts23,SoM23,SpatialPIN24}.
Meanwhile, another line of works explores VLMs using 3D representations such as point clouds for 3D scene~\cite{3DLLM23,SceneLLM24} and object-centric~\cite{ShapeLLM24,pointllm23,GPT4Point24} understanding.
Despite the remarkable progress, these works are limited in 3-DoF understanding which is not actionable.
In contrast, we explore spatial understanding in 6-DoFs from RGB-D via VLMs.
Unlike vanilla 3D scene graphs used by SpatialRGPT for data construction, we propose orientation-aware 3D scene graphs realized by our proposed PointSO.
In addition, we formulate spatial understanding as graph learning, where the scene graph nodes are directly input during inference.

\subsection{Language-Grounded Robot Manipulation}
Language-grounded robot Manipulation adopts the human language as a general instruction interface.
Existing works can be categorized into two groups:
\textbf{i)} \textit{End-to-end} models like RT-series~\cite{RT123,RT223,RTH24} built upon unified cross-modal Transformers with tokenized actions~\cite{PERACT22,InstructRL22,ALOHA23,Peract224,QueST24,RoboFlamingo24,RoboCat24}, large vision-language-action (VLA) models built from VLMs~\cite{OpenVLA24}, or 3D representations~\cite{PolarNet23,3DVLA24,RoboPoint24}.
Training on robot data such as Open X-Embodiment~\cite{OpenXEmbodiment24} and DROID~\cite{DROID24}, a remarkable process has been made.
However, the data \textit{scale} is still limited compared to in-the-wild data for training VLMs.
\textbf{ii)} \textit{Decoupled} high-level reasoning and low-level actions in large VLMs and small off-the-shelf policy models, primitives~\cite{SayCan22,InnerMonologue22,CodeAsPolicy23,VoxPoser23,GroundedDecoding23,CoPa24,MOKA24,UniSim24,ManipAnywhere24,PIVOT24,RoboEXP24,robomatrix24}, or articulated priors~\cite{a3vlm24,ManipLLM24}.
Our \sofar~lies in this group, where an open-world generalization property emerges from VLMs and our proposed PointSO empowered by orientation-aware spatial understanding.

