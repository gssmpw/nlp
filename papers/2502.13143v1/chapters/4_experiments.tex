\input{tabs/simpler_env}
\input{tabs/simpler_widowx}
\section{Experiments}
% \subsection{Model Configuration}
% We consider two \sofar~variants in our experiments.
% \begin{itemize}
%     \item \textbf{\sofar} is a \textit{zero-shot} system using pretrained VLMs as agents. GPT-4o is used as the default VLM, and \sofar~refers to \sofar-Zeroshot without additional specifications.
%     \item \textbf{\sofar-LLaVA} is a \textit{fine-tune} system but uses LLaVA~\cite{LLaVA1.523} instead of GPT-4o as the base VLM.
% \end{itemize}

\subsection{Benchmarks}
We propose two benchmarks to demonstrate the effectiveness of our \sofar~in spatial reasoning and robotic manipulation.
\subsubsection{Open6DOR V2}
For simulation experiments, we choose the Open6DOR\cite{Open6DOR24} Benchmark to comprehensively evaluate our spatial understanding abilities. 
Beyond the perception tasks that it originally proposes, we further construct an execution track to enable comparison with close-loop policies. 
We name the new combined benchmark Open6DOR V2.
\begin{itemize} 
    \item \textbf{Perception tasks}. In line with Open6DOR's definition, the model takes an RGB-D scene image along with a language instruction as input and directly outputs the translation and orientation of the target object. 
    \item \textbf{Execution tasks}. We replicate Open6DOR scenes and ground them into a robosuite simulation environment for execution, excluding single-object scenes to better assess the understanding of spatial relationships. 
    The model takes the RGB-D image with a language instruction as input and completes the entire execution process. We build it based on robosuite \cite{robosuite2020} and adopt the format established by LIBERO \cite{LIBERO23}. Evaluation is conducted according to the final position and orientation of the target object.
\end{itemize}

\subsubsection{6-DoF SpatialBench}
To further evaluate spatial understanding that requires 6-DoF awareness,
we propose a VQA benchmark for spatial understanding evaluation, named 6-DoF SpatialBench.
Previous benchmarks~\cite{SpatialRGPT24,SpatialBot24,embspatial24,space3d24} for assessing VLMs primarily focus on understanding spatial positions, with little attention given to orientation. 
Additionally, most assessments emphasize relative and imprecise spatial relationships (\eg, ``to the left,'' ``nearest'') while lacking quantitative metrics.
In contrast, 6-DoF SpatialBench focuses on both positional and orientational understanding, encompassing 223 manual annotated samples, divided into the position track and orientation track depending on the question.
Each task includes an RGB image of a scene and multiple-choice questions with four options. Specifically, the tasks cover numerical queries (\eg, counting), positional relationships (\eg, left / right), and orientation (the facing direction of an object). The model needs to analyze the image and select the correct answer from four options. All questions and ground-truth answers are carefully designed through \textbf{human annotation}.
% , who are well-trained experts for annotation.

\input{figs/real_experment}
\subsection{6-DoF object rearrangement evaluation in Simulation}
We conduct experiments on the proposed Open6DOR V2 benchmark, with results presented in ~\cref{tab:open6dor}. In perception tasks, we compare our results against the same baselines used in the original Open6DOR~\cite{Open6DOR24} experiments. \ours~outperforms all baselines, demonstrating effective spatial understanding and zero-shot generalizability.
In the execution tasks, we record the initial and final poses of the objects to evaluate execution success. We use the original pretrained Octo~\cite{Octo24} and the LIBERO-finetuned OpenVLA~\cite{OpenVLA24} as baselines, conducting all experiments in the same LIBERO environment where OpenVLA was fine-tuned to minimize the domain gap. 
Despite this, both Octo and OpenVLA show lower success rates, indicating their poor generalizability. In contrast, \ours~achieves about 40\% success, even with a vanilla execution implementation. It is worth noting that some of the objects are inherently difficult to grasp, which significantly hampers execution success. We call for more robust execution policies and manipulation strategies, such as prehensile grasping and adaptive techniques, to demonstrate better performance on Open6DOR V2.

\input{tabs/spatial_vqa}

\subsection{Zero-shot Object Manipulation Evaluation in Simulation}
SIMPLER~\cite{simplerenv24} is a suite of open-source simulated evaluation environments designed for real-world robot manipulation setups. SIMPLER provides a standardized platform for benchmarking manipulation tasks, emphasizing reproducibility and alignment with real-world scenarios.
We conduct quantitative evaluations of \ours's zero-shot execution performance on Google Robot tasks \& Widow-X tasks and compare it to baselines including Octo~\cite{Octo24}, OpenVLA~\cite{OpenVLA24} and more concurrent works~\cite{robovlm25,spatialvla25}.
The robot follows the planned trajectory generated by the planning module, as described in Sec. \ref{execution}, to execute the task.
Furthermore, leveraging the error detection and re-planning capabilities of VLMs~\cite{GPT4o24,gemini23}, we can make multiple attempts following a single-step execution failure to approximate achieve a closed-loop effect. For fairness, we limit the maximum number of attempts to three. Detailed visualizations and analyses are provided in the~\cref{app:close_loop}.
As shown in \cref{tab:simpler_env,tab:widowx}, despite the training data for Octo and OpenVLA including Google Robot tasks, \ours~demonstrates superior zero-shot performance compared to most baselines.

\input{tabs/semantic_orientation}
\input{tabs/semantic_orientation_c}

\subsection{Zero-shot Real-world Manipulation}
\subsubsection{Hardware Setup}
We set up a real-world tabletop environment utilizing a Franka Panda robotic arm equipped with a parallel gripper. For perception, we integrate a single RGB-D camera (Intel RealSense D415) mounted on the end-effector. The details of the environmental visualization and additional real-world robot setup are provided in the~\cref{app:robot_setup}.

\input{figs/navigation}

\subsubsection{Tasks and Evaluations}
To comprehensively evaluate the generalization of \sofar, we design \textbf{60 diverse real-world experimental tasks involving over 100 diverse objects}. Following Open6DOR~\cite{Open6DOR24}. These tasks are categorized into three tracks—position, orientation, and comprehensive \& 6-DoF, each with simple and hard levels. The position-simple track focuses on basic spatial relationships (front/back/left/right), while the position-hard track involves more complex spatial concepts (between/center/customized). The orientation-simple track targets object part orientations, whereas the orientation-hard track requires precise angle judgments for upright or flipped objects. Comprehensive tasks demand intricate instruction understanding and long-horizon operations, and 6-DoF tasks require simultaneous position and orientation control. Each task is executed three times to ensure statistical reliability. Detailed visualization and analysis are provided in the~\cref{app:detail_realworld}.

\subsubsection{Results}
The quantitative results are shown in \cref{fig:real}, \sofar~outperform the baseline across all tracks, particularly in the challenging orientation and comprehensive \& 6-DoF tasks. Meanwhile, \sofar~also utilizes the minimal planning time overhead.
In addition, \sofar~is not limited to a single embodiment. In qualitative experiments, we also test different embodiments, such as a dexterous hand and a suction cup, as shown in \cref{fig:real_demo}. Additional robot setups and generalization experiments are included in the~\cref{app:robot_setup}.


\subsection{Visual Question Answer on 6-DoF SpatialBench}
We evaluate \ours~on our proposed 6-DoF SpatialBench, as shown in \cref{tab:spatial_vqa}. We choose several VLMs and comparable methods as baselines.
\ours~achieves superior performance across both position-track and orientation-track, outperforming baselines by over 18\%.


\subsection{Semantic Orientation Prediction}
Using free-text descriptions to extract semantic orientations from object point clouds is challenging. In Objaverse~\cite{objaverse23}, we manually annotate 128 diverse objects and form an OrienText300K test split to evaluate the directional accuracy of PointSO. We trained different model variants on OrienText300K, and the results in \cref{tab:semantic_direction}, report varying accuracies between 45° $ \sim $ 5°.
In the real world, acquiring complete point clouds is difficult even impractical. To assess the robustness, we introduce random rotations, single-sided point clouds, and Gaussian noise. The accuracy at 45°, as shown in \cref{tab:semantic_direction_c}, demonstrates the model's performance under these conditions.

\subsection{Additional Applications}

\subsubsection{Cross Embodiedment Generalization}
Our approach determines grasp poses by generating masks and plans the target pose and transformation using our PointSO and large language model. It does not rely on trajectory data specific to any robotic arm, making \sofar~embodiment-agnostic. \cref{fig:real_demo} illustrates the diverse embodiments employed in our real-world experiments. Leveraging the GSNet~\cite{GSNet21} algorithm based on LeapHand~\cite{leaphand23}, we perform 6-DoF object manipulation experiments on dexterous hands. We conduct three position-related and three rotation-related experiments. Leveraging the PointSO and large language models, \ours~is capable of performing complex 6-DoF manipulation tasks, such as ``\textit{Upright the fallen wine glass and arrange it neatly in a row with the other wine glasses.}''

\subsubsection{Long Horizon Planning \& Close-Loop Execution}
Similar to ReKep~\cite{ReKep24}, \ours~leverages VLMs~\cite{GPT4o24,gemini23} to perform long-horizon decomposition of complex tasks and employs dual-system VLMs~\cite{GPT4o24,gemini23} to determine the success of execution between tasks and subtasks, enabling closed-loop execution. When a discrepancy between the results and expectations is detected, \ours~re-percepts and re-executes the current subtask. Detailed visualization and analysis can be found in~\cref{app:close_loop,app:long_horizon}.

\subsubsection{Orientation-Aware Robotic Navigation}
\textit{Semantic orientation} can not only be applied to manipulation tasks but also to robotic navigation tasks. As shown in \cref{fig:navigation}, we conduct experiments using a robotic dog for orientation-aware robotic navigation tasks.
 Unlike traditional navigation approaches, the dog is required to face a specific direction during navigation.
 This orientation-aware constraint enhances the navigation process by ensuring precise alignment with the desired orientation, thereby improving task performance in scenarios where directionality is critical.
