\section{Introduction}\label{sec:intro}

Open-world spatial intelligence is crucial for embodied AI, as a robot must understand not only ``what'' an object is but also its precise ``where'' for effective interaction.
To this end, vision-language-models (VLMs)~\cite{Flamingo22,BLIP22,LLaVA23,DreamLLM23} with spatial understanding~\cite{SpatialVLM24,SpatialBot24,SpatialRGPT24} that comprehend spatial concepts~\cite{PhysicalSceneUnderstanding24,ClassifyingEventsSceneGraph07} and relationships~\cite{GlanceRealWorldScene07,VisualGenome17,3DSemanticSceneGraph20} have been built. These models incorporate spatial knowledge into their architecture design or training data, enabling them to perform tasks such as distinguishing left from right~\cite{SpatialVLM24,SpatialRGPT24}, counting objects~\cite{SpatialBot24,embspatial24}, and even planning for position-only manipulations
~\cite{RoboPoint24,SpatialBot24}.
Despite the remarkable achievements, we ask: What is the missing cornerstone of such spatial understanding? 
Given the original intent of ``seeing is for doing''~\cite{WorldISee23}, \textit{how can we push spatial understanding further}?


We observe that current VLMs struggle with understanding object \textbf{orientation}, making them insufficient for generic robot manipulation planning.
Consider some everyday scenarios: inserting a pen into a pen holder, righting a tilted wine glass, or plugging a cord into a power strip. 
Previous approaches~\cite{SpatialVLM24,SpatialRGPT24,SpatialBot24} primarily focused on understanding ``where is the pen'' or ``where is the wine glass'' while ignoring their orientations, making them insufficient for accomplishing these seemingly simple object manipulation tasks.

More importantly, different orientations of an object hold varying semantic significance. The capability of connecting specific orientations to their semantic meanings is essential for language-guided robot manipulations. For example, inserting a pen into a pen holder requires aligning the pen tip with the direction of the pen holder's opening; righting a wine glass necessitates aligning the glass's top with the z-axis in the world coordinate frame; and plugging into a power strip involves understanding the ``insertion'' direction, which is perpendicular to the power strip's surface.
However, translating a specific language description into a desired object orientation is challenging for existing VLMs.

To move forward, we introduce \textbf{language-grounded orientation that bridges spatial reasoning and object manipulation}, characterized by the following:
\begin{itemize}
\item \textbf{From Position Awareness to Orientation Awareness.}
While prior works~\cite{SpatialVLM24,SpatialRGPT24,SpatialBot24} emphasize position relationship, orientation understanding is equally critical for defining the full six degrees of freedom (6-DoF) of object or end-effector poses~\cite{OV6DOFPose24,FoundationPose24,PoseCNN18,ManipLLM24,ManipulabilityME85,ManipulateAnything24}. Orientation awareness involves understanding object orientations and their relationships in the open world, enabling robots to complete tasks requiring precise alignment and rearrangement together with position awareness.

\item \textbf{From Orientation to Semantic Orientation.}
Traditional orientation, defined relative to a base frame or template model~\cite{PoseCNN18,OnePose22,MegaPose22,FoundationPose24,OV6DOFPose24}, is insufficient for open-world manipulation guided by language instructions~\cite{LanguageInstructRobots11,SayCan22,OWManipVLM23}. We introduce semantic orientation, linking orientational vectors of an object to open-vocabulary prompts (\eg, the ``handle'' direction of a knife or ``plug-in'' direction of a USB). This bridges geometric reasoning with functional semantics, enabling robots to interpret task-specific orientation changes.
\end{itemize}

Achieving such spatial awareness requires addressing two key challenges: acquiring semantic orientation knowledge in the open world and integrating it with VLMs. To tackle the first, we propose PointSO, a generalizable cross-modal 3D Transformer~\cite{AttentionIsAllYouNeed,ACT23,ReCon23,ShapeLLM24}, which serves as a robust and versatile framework for open-world spatial orientation understanding. To train PointSO effectively, we construct OrienText300K, a large-scale orientation-text paired dataset curated from internet sources. This dataset, devoid of expensive robot data, is automatically labeled by prompting GPT-4o~\cite{GPT4o24} with extensive and diverse language-grounded semantic orientation queries. These queries encompass \textit{intra-object} spatial understanding and \textit{inter-object} interaction-related semantics, such as manipulation orientations. OrienText300K comprises over 350K 3D models of diverse everyday objects. Powered by OrienText300K, PointSO can reliably infer semantic orientation for an arbitrary object without being restricted to a known category or instance.

We further develop an integrated reasoning system, \sofar, to coordinate our proposed PointSO and existing position foundation models for achieving more comprehensive spatial understanding, where Florence-2~\cite{florence2} and SAM~\cite{SAM23} handle the object positions, while our PointSO focuses on understanding and outputting orientations complimentary. Specifically, we parse an input RGB-D observation as an orientation-aware 3D scene graph using SAM-segmented object point clouds and our PointSO. The RGB-D observation, together with the scene graph, is then input to the VLM, which outputs a chain-of-thought~\cite{CoT22} spatial reasoning for both position and orientation commands. These commands can then serve as visual planning outcomes to support robotic manipulation tasks.

To assess our system, we introduce Open6DOR V2, a large-scale robot manipulation benchmark designed for 6-DoF object rearrangement in simulation. This benchmark demands robust positional and orientational reasoning in open-world settings and supports both open-loop and closed-loop robotic control. Our experiments demonstrate that our system considerably outperforms state-of-the-art vision-language models and popular vision-language-action (VLA) models—even those trained with extensive and costly robot trajectories. These performance gains are also observed in real-world experiments. Additionally, we establish a new spatial visual-question-answering benchmark, that confirms the system’s exceptional open-world spatial reasoning capabilities.

In summary, our contributions are fourfold. First, we introduce PointSO, an orientation base model that infers the semantic directions of novel objects in an open-world context. Second, we curate OrienText300K, a large-scale 3D model dataset annotated with semantic directions to support the training of orientation models. Third, we develop an integrated system that enhances powerful VLMs with advanced spatial understanding, facilitating robot manipulations that require both positional and orientational spatial knowledge. Fourth, we establish 6-DoF SpatialBench \& Open6DOR V2, an orientation-aware spatial visual-question-answering benchmark and a comprehensive robot manipulation benchmark designed to evaluate both open-loop and closed-loop open-world 6D rearrangement strategies. Extensive experiments demonstrate the superior performance of our method together with a series of perception and robot manipulation benchmarks.