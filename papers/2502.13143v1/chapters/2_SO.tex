\vspace{8pt}
\section{Semantic Orientation: Connecting Language and Object Orientation}
\subsection{Definition of Semantic Orientation}
Traditionally, the orientation of an object is defined within a reference frame, using quaternions or Euler angles to represent relative rotations. Intuitively, object orientations commonly correspond to some specific semantics in most interactive behaviors. This aligns with the fact that humans typically understand an object’s orientation in a more semantic, reference-free way. For instance, when plugging a plug into a charger, we accomplish the action of ``plugging in'' by matching the metal prongs’ direction with the outward direction of the charger’s socket.
Drawing on this observation, we define an object's \textit{Semantic Orientation} as follows.
Given an object $X$ and a description $\ell$, the corresponding semantic orientation $\mathbf{s}_{\ell}^X\in S(2)$ is an object-centric
direction represented as a unit vector semantically matching the description $\ell$.
\input{figs/data_construction}
\begin{equation}
    \mathbf{s}_{\ell}^X = \mathcal{F}(X, \ell). 
\end{equation}
$\ell$ is an open-vocabulary language description that should have a clear semantic correspondence to a general orientation (\eg, front, top), an object part (\eg, handle, cap), or a specific manipulation goal (\eg, pour out, plug-in).

For an object $X$, it may have multiple semantic orientations corresponding to different functions or attributes via changing the language description $\ell$, forming a semantic orientation set $ S_X = \{ \mathbf{s}_{\ell_1}^X, \mathbf{s}_{\ell_2}^X, \dots, \mathbf{s}_{\ell_n}^X \}$. Based on this set, the rotation of $X$ can be characterized by transforming its semantic orientations.

\subsection{Robotic Manipulation via Semantic Orientation}
Semantic orientations are powerful representations that help characterize various orientation-related knowledge. What relates most to robot manipulation is the knowledge of object reorientation. Given an initial observation of an object $X$ and a task command $c$ 
that specifies the desired reorientation, semantic orientations can be used to determine the necessary object rotation.
First, we identify task-related semantic orientation descriptions $\{l^{c}\}$ from the task command $c$ whose desired directions are clearly outlined in the command. For instance, a command such as ``turn a bottle over and put it on the ground'' necessitates identifying the ``up direction of the bottle'' as a semantic orientation, with the desired direction being $(0,0,-1)$ in a world coordinate system where the z-axis is perpendicular to the ground. Then, by extracting the semantic orientations from the initial observation $X$ and calculating the rotation needed to align these with the desired directions, we can effectively determine how the object should be reoriented.

% Going beyond object reorientation, it is worth noting that semantic orientations can also easily connect to traditional instance-level and category-level orientations, or even support defining cross-category orientations.
% To be specific, by orthogonalizing the semantic orientation set, we can form reference frames that define instance-level orientations for specific objects. For objects within the same category, using a consistent set of language descriptions ensures that semantic orientation sets are aligned within the category, resulting in category-level consistent references. These references allow for the derivation of category-level orientations. Furthermore, even for objects from different categories, applying the same set of language descriptions enables cross-category consistent semantic orientation sets. This facilitates the derivation of cross-category consistent references for cross-category orientation. Therefore, we build our orientation understanding on semantic orientations, with a focus on learning to estimate these directions for open-world objects based on open-ended descriptions.
Beyond object reorientation, semantic orientations can be linked to traditional instance-level and category-level orientations, and even facilitate cross-category orientation. Specifically, by orthogonalizing a set of semantic orientations, we can establish reference frames that define instance-level orientations for individual objects. For objects within the same category, using a consistent set of linguistic descriptions aligns their semantic orientation sets, resulting in category-level consistent references from which category-level orientations can be derived. Moreover, applying the same linguistic descriptors across different categories creates cross-category consistent semantic orientation sets, enabling the derivation of cross-category reference frames. Thus, our approach builds orientation understanding on semantic orientations, with an emphasis on learning to estimate these directions for open-world objects based on open-ended descriptions.


\input{figs/data_validation}
\subsection{OrienText300K: Orientation-Text Paired Data at Scale}
Our goal is to learn an \textit{orientation model} on a large-scale 3D model dataset so that it can identify semantic orientations in open-world scenarios.
To achieve it, we first introduce OrienText300K, a newly curated 3D model dataset with diverse and extensive semantic orientation labels for training our language-conditional orientation model.

\subsubsection{Scalability Analysis}
Before discussing the data curation process, it is important to note that semantic orientations are typically defined within the six \textit{standard orthogonal views} of an object from a canonical setup. For example, the orientation for placing a pen into a pencil holder is opposite to the holder’s upright direction. Object canonicalization often involves specifying the up and front directions of an object. Fortunately, most 3D models available in web datasets are already canonicalized, except for potential axis flipping. Based on this observation, we propose to scale up the semantic orientation annotations by leveraging readily available web 3D datasets with automatic labeling using GPT. Specifically, we can use the six orthogonal directions as candidates and generate descriptions from rendered multi-view images. This associates various language descriptions with salient directions on objects, providing the supervision needed for learning an orientation model.
% Before introducing the data curation process, it is noteworthy that semantic orientations are typically defined within the 6 \textit{standard orthogonal views} of an object from a canonical setup.
% For instance, the orientation of putting a pen into a pencil holder is the opposite of the holder's up orientation. Such object canonicalization usually requires specifying the up and front directions of an object but luckily most of the 3D models in the web datasets are already canonicalized up to an axis flipping.
% Based on this observation, we propose to scale up by leveraging off-the-shelf web 3D datasets for automatic labeling using GPT.
% Specifically, we can use 6 orthogonal orientations as candidates and generate languages from rendered multi-view images. During training, we can simply argument samples and labels by random translation and rotation to generate semantic orientation from arbitrary positions.

\subsubsection{Data Source}\label{sec:objaverse-so}
To scale up, we build the OrienText300K dataset from Objaverse~\cite{objaverse23}, which originally contains $\sim$800K Internet 3D models across substantial categories.
However, such Internet data contains a lot of noisy annotations or low-quality samples that cannot be used. Based on Blender, we carefully set up the light and rendered more than 8M high-fidelity rendering images.

\input{figs/PointSO}
\subsubsection{Data Filtering}
To clean the data that can better be used for generating semantic orientation annotations, we first clean the data by using a dedicated filtering strategy that filters data to preserve samples that satisfy the following 6 requirements.
\ding{182} Standard orthogonal view only. Samples in random views will be filtered.
\ding{183} Clean objects without the ground for auxiliary visualization.
\ding{184} Reasonable objects that have sufficient spatial reasoning potentials.
\ding{185} High-quality objects. Low-quality objects such as blurry and wrong samples are filtered.
\ding{186} Distinguishable objects. Abstract objects such as meaningless solids are filtered.
\ding{187} Non-scene objects. Samples that describe a 3D scene are filtered for object-centric understanding purposes.

However, it is non-trivial to conduct filtering on such big data using manual labor.
Inspired by recent works showing large VLMs are human-aligned 2D or 3D image-based judgers~\cite{LLMAsJudge23,GPT4V-3DEvaluator24,DreamBenchPlus24}, we employ GPT-4o~\cite{GPT4o24} by prompting requirements above.
To be specific, the multi-view images of 3D objects are concatenated together with our designed prompts into GPT-4o, and GPT-4o will decide whether one sample should be filtered.
The filtered dataset yields 350K+ clean samples, significantly reducing data noise.

\input{figs/pipeline}
\subsubsection{Data Annotation}
Like data filtering, we annotate OrienText300K with semantic orientations using GPT-4o.
GPT-4o will take rendered object views as input and generate the language description and one best-grounded direction from the 6 standard orthogonal direction candidates.
We render the six \textit{standard} orthogonal view images in Blender, which are concatenated as input of GPT-4o.
Besides, four 45-degree \textit{oblique} orthogonal views are rendered and concatenated as an additional input, ensuring more robust annotation.


\subsubsection{Quality Validation}
To validate the accuracy of our prompted GPT agents as the data filter and annotator, we construct a validation set containing 208 samples with manually labeled filtering criteria and semantic orientation labels, respectively.
From \cref{fig:data_val}, we observe that GPT-4o achieves an average accuracy of 88.3\% and 97.1\% accuracy on filtering and annotating, respectively.
This provides a quality guarantee of our OrienText300K.

\subsection{PointSO: A Cross-Modal 3D Transformer for Semantic Orientation Prediction}\label{sec:PointSO}
We introduce PointSO, a plain Transformer-based architecture~\cite{AttentionIsAllYouNeed} with cross-modal 3D-language fusion as our orientation model. 
As illustrated in \cref{fig:PointSO}, PointSO takes the object's 3D point clouds and a language description as inputs, and predicts the corresponding semantic orientation.

\subsubsection{3D and Language Embeddings}
Given an object's point cloud $X = \{\mathbf{x}_i \in\mathbb{R}^3|i=1,2,\dots, N\}$ with $N$ 3D points defined in (x, y, z) Cartesian space, and an arbitrary language description $\ell$, we first embed both into discrete token embeddings.
For the 3D point clouds, we follow~\cite{ACT23,PointBERT,ReCon23} to first sample $N_s$ seed points using farthest point sampling (FPS) and then group inputs with KNN for point feature embedding with a local geometric extraction network such as lightweight PointNet~\cite{PointNet,PointNet++}. 
An MLP head is used which maps a special \texttt{[CLS]} token~\cite{ViT} to a predicted direction.
As for the language inputs, we adopt OpenAI's CLIP~\cite{CLIP} and use the global token as cross-modal fusion inputs.
Like Vision Transformer~\cite{ViT}, we explore three model configurations, \ie, small, base, and large versions.
Each configuration is of a different number of Transformer layers, dimensions, and CLIP models, detailed in the~\cref{app:pointso_details}.

\subsubsection{Cross-Modal Fusion}
We adopt a dense layer-wise feature fusion strategy by injecting the global text features into the Transformer layers of the 3D Transformer, where a cross-modal fusion is conducted.
Generally, this fusion operation can be implemented in various fashions such as cross-attention, adapter, or concatenating features along spatial/channel dimensions.
Empirically, however, we find that the sum of the text token to every point token is the most simple but effective solution (see~\cref{app:fusion}).
This may be due to the relatively short length of the languages, where a per-token sum enhances attention to languages. 

\subsubsection{Optimization}
Let $\mathcal{F}_{\text{SO}}$ represent the PointSO model parameterized by $\theta_{\text{SO}}$ (the CLIP is kept frozen and thus its parameters are not included).
Given every object point cloud $X_i \in \mathcal{D}_{\text{OrienText300K}}$ in the OrienText300K dataset, where each object is labeld with a language set $L_i=\{\ell_j^i, j=1,2,\dots,Q\}$ and the corresponding ground truth semantic orientation set, $S_i=\{\mathbf{s}^i_j, j=1,2,\dots,Q\}$.
The optimization is to minimize the negative cosine similarity $\mathcal{L}_{\text{cos}}(\mathbf{v},\mathbf{k})=\mathbf{1}-\frac{\mathbf{v}\cdot\mathbf{k}}{\|\mathbf{v}\|\cdot\|\mathbf{k}\|}$ between predicted and the ground truth semantic orientations:
\begin{equation}
    \min_{\theta_{\text{SO}}} \sum_{X_i \in \mathcal{D}_{\text{OrienText300K}}} \sum_{\ell_j^i\in L_i} 
    \mathcal{L}_{\text{cos}}
    \Big(
    \mathcal{F}_{\text{SO}}(X_i, \ell_j^i), \mathbf{s}^i_j
    \Big).
\end{equation}