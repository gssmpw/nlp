\newpage
\appendix
\appendices

\section{Robot Setups}\label{app:robot_setup}

\subsection{Simulation Robot Setups}
To ensure fairness, we utilize the same Franka Panda arm for evaluations in both the LIBERO~\cite{LIBERO23} and our Open6DOR V2 benchmarks. For SIMPLER~\cite{simplerenv24}, we use the Google Robot exclusively to conduct the baseline experiments, adhering to all configurations outlined in SIMPLER, as presented in Table~\ref{tab:simpler_env}. 

\subsection{Real World Robot Setups}
As for manipulation tasks, in \cref{fig:robots}, we perform 6-DoF rearrangement tasks using the Franka Panda equipped with a gripper and the UR robot arm with a LeapHand, while articulated object manipulation is conducted using the Flexiv arm equipped with a suction tool. All the robot arms mount a Realsense D415 camera to its end for image capturing.
\input{figs/robots}

In \cref{fig:franka_setup}, we present the workspace and robotic arm for real-world 6-DoF rearrangement. Unlike Rekep~\cite{ReKep24}, CoPa~\cite{CoPa24} et al., we utilize only a single RealSense D415 camera. This setup significantly reduces the additional overhead associated with environmental setup and multi-camera calibration, and it is more readily reproducible.
\input{figs/franka_setup}

As for navigation tasks, we provide a visualization of our robotic dog in~\cref{fig:dog_setup}. Following Uni-Navid~\cite{uninavid24}, our robotic dog is Unitree GO2 and we mount a RealSense D455 camera on the head of the robotic dog. Here, we only use the RGB frames with a resolution of $640\times480$ in the setting of  $90^\circ$ HFOV. We also mount a portable Wi-Fi at the back of the robot dog, which is used to communicate with the remote server (send captured images and receive commands). Unitree GO2 is integrated with a LiDAR-L1, which is only used for local motion planning. 
\input{figs/dog_setup}


\section{Additional Experiments}\label{app:add_exp}

\subsection{Articulated Objects Manipulation Evaluation}
We further integrate \ours~with articulated object manipulation, as illustrated in \cref{tab:manip}, and evaluate its practicality in robotic manipulation tasks using the PartNet-Mobility Dataset within the SAPIEN~\cite{SAPIEN20} simulator. Our experimental setup follows ManipLLM~\cite{ManipLLM24}, employing the same evaluation metrics. Specifically, we directly utilize the segmentation centers provided by SAM as contact points, leverage PointSO to generate contact directions, and use VLM to determine subsequent motion directions. The results demonstrate significant improvements over the baseline. Notably, our model achieves this performance without dividing the data into training and testing sets, operating instead in a fully zero-shot across most tasks. This underscores the robustness and generalization of our approach.
\input{tabs/manip}


\subsection{Spatial VQA on EmbSpatial-Bench~\cite{embspatial24} \& SpatialBot-Bench~\cite{SpatialBot24}}
To further demonstrate \sofar's spatial reasoning capabilities, we conducted Spatial VQA tests within the EmbSpatial-Bench~\cite{embspatial24} and SpatialBot-Bench~\cite{SpatialBot24}. As shown in \cref{tab:embspatial,tab:spatialbot}, \sofar~significantly outperformed all baselines, achieving more than a 20\% performance improvement in EmbSpatial-Bench.
\input{tabs/embspatial}
\input{tabs/spatialbot}


\subsection{Close-Loop Execution Experiment}\label{app:close_loop}
We demonstrate the closed-loop replan capabilities of \sofar~within Simpler-Env~\cite{simplerenv24} in \cref{fig:close_loop}. The instruction for both tasks is ``pick the coke can'' In \cref{fig:close_loop} (a), the model initially misidentified the coke can as a Fanta can. After correction by the VLM, the model re-identified and located the correct object. In \cref{fig:close_loop} (b), the model accidentally knocks over the Coke can during motion due to erroneous motion planning. Subsequently, the model re-plans and successfully achieves the grasp.

\subsection{Long Horizon Object Manipulation Experiment}\label{app:long_horizon}
\cref{fig:long_horizon} illustrates the execution performance of our model on long-horizon tasks. Through the VLM~\cite{GPT4o24,gemini23}, complex instructions such as ``making breakfast'' and ``cleaning up the desktop'' can be decomposed into sub-tasks. In the second example, we deliberately chose complex and uncommon objects as assets, such as ``Aladdin's lamp'' and ``puppets'', but \sofar~is able to successfully complete all tasks.
\input{figs/close_loop}

\subsection{In the Wild Evaluation of Semantic Orientation}
We provide a qualitative demonstration of the accuracy of PointSO under in-the-wild conditions, as shown in \cref{fig:in_the_wild}, where the predicted Semantic Orientation is marked in the images. We obtained single-sided point clouds by segmenting objects using Florence-2~\cite{florence2} and SAM~\cite{SAM23} and fed them into PointSO. It can be observed that our model achieves good performance across different views, objects, and instructions, which proves the effectiveness and generalization of PointSO.
\input{figs/long_horizon}
\input{figs/in_the_wild}
\input{figs/cross_view}

\subsection{Cross-View Generalization}
\sofar~gets point clouds in the world coordinate system using an RGB-D camera to obtain grasping poses, and it is not limited to a fixed camera perspective. In addition, PointSO generates partial point clouds from different perspectives through random camera views to serve as data augmentation for training data, which also generalizes to camera perspectives in the real world. \cref{fig:cross_view} illustrates \sofar's generalization capability for 6-DoF object manipulation across different camera poses. It can be observed that whether it's a front view, side view, or ego view, \sofar~can successfully execute the ``upright the bottle'' instruction.

\subsection{Failure Case Distribution Analysis}
Based on the failure cases from real-world experiments, we conducted a quantitative analysis of the failure case distribution for \sofar, with the results shown in \cref{fig:failure_case}. It can be observed that 31\% of the failures originated from grasping issues, including objects being too small, inability to generate reasonable grasping poses, and instability after grasping leading to sliding or dropping. Next, 23\% were due to incorrect or inaccurate Semantic Orientation prediction. For tasks such as upright or upside - down, highly precise angle estimation (<5°) is required for smooth execution. Object analysis and detection accounted for approximately 20\% of the errors. The instability of open-vocabulary detection modules like Florence2~\cite{florence2} and Grounding DINO~\cite{groundingdino23} often led to incorrect detection of out-of-distribution objects or object parts. In addition, since our Motion Planning did not take into account the working space range of the robotic arm and potential collisions of the manipulated object, occasional deadlocks and collisions occurred during motion. Finally, there were issues with the Task Planning of the VLM~\cite{GPT4o24,gemini23}. For some complex Orientations, the VLM occasionally failed to infer the required angles and directions to complete the task. Employing a more powerful, thought-enabled VLM~\cite{gpt_o1} might alleviate such errors.

\input{figs/failure_case}

\subsection{Ablation Study}\label{app:ablation}
\subsubsection{Scaling Law}
The scaling capability of models and data is one of the most critical attributes today and a core feature of foundation models~\cite{FoundationModel21}. We investigate the performance of PointSO across different data scales, as illustrated in \cref{tab:scaling_law}. 
We obtain the subset for OrienText300K from Objaverse-LVIS, which consists of approximately 46,000 3D objects with category annotations. The selection was based on the seven criteria mentioned in the main text. Objects meeting all seven criteria formed the strict subset, comprising around 15k objects. When including objects without textures and those of lower quality, the total increases to approximately 26k objects.
It can be seen that the increase in data volume is the most significant factor driving the performance improvement of PointSO. It can be anticipated that with further data expansion, such as Objaverse-XL~\cite{ObjaverseXL23}, PointSO will achieve better performance.
\input{tabs/scaling_law}

\subsubsection{Cross-Modal Fusion Choices}\label{app:fusion}
We further conduct an ablation study on the multi-modal fusion methods in PointSO, testing commonly used feature fusion techniques such as cross-attention, multiplication, addition, and concatenation, as shown in \cref{tab:fusion}. The results indicate that simple addition achieves the best performance. This may be attributed to the fact that instructions in the semantic domain are typically composed of short phrases or sentences, and the text CLS token already encodes sufficiently high-level semantic information.
\input{tabs/fusion}

\input{tabs/detection_ab}
\subsubsection{Open Vocabulary Object Detection Module}
\sofar~utilize a detection foundation model to localize the interacted objects or parts, then generate masks with SAM~\cite{SAM23}. Although not the SOTA performance on the COCO benchmark, Florence-2~\cite{florence2} exhibits remarkable generalization in in-the-wild detection tasks, even in simulator scenarios. \cref{tab:detection_ab} illustrates the performance of various detection modules in Open6DOR~\cite{Open6DOR24} Perception, where Florence-2 achieves the best results and outperforms Grounding DINO~\cite{groundingdino23} and YOLO-World~\cite{yoloworld24}.

\vspace{3pt}
\section{Additional Implementation Details}\label{app:implementation_details}

\subsection{Detail Real World Experiment Results}\label{app:detail_realworld}
To fully demonstrate the generalization of \sofar~rather than cherry-picking, we carefully design 60 different real-world experimental tasks, covering more than 100 different and diverse objects. Similar to the Open6DOR~\cite{Open6DOR24} benchmark in the simulator, we divide these 60 tasks into three parts: position-track, orientation-track, and the most challenging comprehensive \& 6-DoF-track. Each track is further divided into simple and hard levels. The position-simple track includes tasks related to front \& back \& left \& right spatial relationships, while the position-hard track includes tasks related to between, center, and customized. The orientation-simple track includes tasks related to the orientation of object parts, and the orientation-hard track includes tasks related to whether the object is upright or flipped (with very strict requirements for angles in both upright and flipped cases). Comprehensive tasks involve complex instruction understanding and long-horizon tasks; 6-DoF tasks simultaneously include requirements for both object position and orientation instructions. In \cref{tab:detailed_realworld}, we present the complete task instructions, as well as the performance metrics of \sofar~and the baseline. Due to the large number of tasks, we performed each task three times. It can be seen that \sofar~achieves the best performance in all tracks, especially in the orientation-track and comprehensive \& 6-DoF-track. We also show all the objects used in the real-world experiments in \cref{fig:real_obj}, covering a wide range of commonly and uncommonly used objects in daily life.

\input{tabs/detailed_real_world}
\input{figs/real_obj}

\input{tabs/PointSO_configurations}
\input{tabs/param}\subsection{PointSO Model Details}\label{app:pointso_details}
For PointSO, we utilize FPS + KNN to perform patchify and employ a small PointNet~\cite{PointNet} as the patch encoder. Subsequently, a standard Transformer encoder is adopted as the backbone, followed by a single linear layer to map the output to a three-dimensional vector space. All parameter configurations follow prior work on point cloud representation learning~\cite{ACT23,ReCon23,ShapeLLM24}. Detailed hyperparameter and model configurations are provided in \cref{tab:hyper_params,tab:PointSO_configs}.

\subsection{SoFar-LLaVA Model Details}\label{app:model_details}
\input{figs/sofarllava}
In addition to leveraging the extensive knowledge and strong generalization capabilities of closed-source/open-source pretrained VLMs~\cite{ChatGPT22,gemini23,qwenvl23} for zero-shot or in-context learning, \sofar~can also enhance the planning performance of open-source models through visual instruction tuning for rapid fine-tuning. The pipeline of the model is illustrated in \cref{fig:sofar_llava}. A JSON-formatted 6-DoF scene graph, processed through a text tokenizer, along with the image refined by SoM~\cite{SoM23}, is fed into an LLM (\eg, LLaMA~\cite{LLaMA23,LLaMA2_23}) for supervised fine-tuning~\cite{LLaVA23}.
In the Open6DOR~\cite{Open6DOR24} task, we supplement the training dataset with additional samples retrieved and manually annotated from Objaverse~\cite{objaverse23}, ensuring alignment with the object categories in the original benchmark. This dataset includes approximately 3,000 6-DoF object manipulation instructions. Using this data, we construct dialogue-style training data based on ChatGPT and train the \sofar-LLaVA model. The training hyperparameters are detailed in \cref{tab:hyper_params}. Similarly, we finetune PointSO on this training dataset and achieve superior performance on the Open6DOR task.

\subsection{ChatGPT API Costs}
The knowledge of OrienText300K is derived from the annotations of 3D modelers on Sketchfab, combined with ChatGPT's filtering and comprehension capabilities. To generate semantic direction annotations, we filter the 800K dataset of Objaverse~\cite{objaverse23} and apply ChatGPT to approximately 350K of the filtered data to generate semantic text-view index pairs. The OpenAI official API was used for these calls, with the GPT-4o version set to 2024-08-06 and the output format configured as JSON. The total cost for debugging and execution amounted to approximately \$10K.


\section{Additional Benchmark Statistic Analysis}
\subsection{6-DoF SpatialBench Analysis}
We conduct a statistical analysis of the manually constructed 6-DoF SpatialBench, with category comparisons and word cloud visualizations shown in \cref{fig:spatialvqa_statistic}. We collect diverse image data from the internet, encompassing scenes such as indoor, outdoor, and natural landscapes. The questions may involve one or multiple objects, with varying levels of uncertainty in image resolution. Most importantly, we are the first to propose a VQA benchmark for orientation understanding, focusing on both quantitative and qualitative evaluation of orientation.


\subsection{Open6DOR V2 Analysis}
Open6DOR V2 builds upon Open6DOR V1 by removing some incorrectly labeled data and integrating assets and metrics into Libero, enabling closed-loop policy evaluation. The detailed number of tasks is presented in \cref{tab:open6dorv2_statistic}, comprising over 4,500 tasks in total. Notably, we remove level 2 of the position track in Open6DOR V1~\cite{Open6DOR24} because it requires manual inspection, which is not conducive to open-source use and replication by the community. Besides, due to the randomness of object drops in the scene, approximately 8\% of the samples already satisfy the evaluation metrics in their initial state.

\vspace{3pt}
\section{Additional Related Works}\label{app:related_work}
\subsection{3D Representation Learning}
Research on 3D Representation Learning encompasses various methods, including point-based~\cite{PointNet,PointNet++}, voxel-based~\cite{voxelnet15}, and multiview-based approaches~\cite{MVCNN3D15,MVTN}. 
Point-based methods~\cite{PointNext,PointTrans21} have gained prominence in object classification~\cite{ModelNet15,ScanObjectNN19} due to their sparsity yet geometry-informative representation. On the other hand, voxel-based methods~\cite{voxelrcnn21,SyncSpecCNN17,VPP23} offer dense representation and translation invariance, leading to a remarkable performance in object detection~\cite{ScanNet17} and segmentation~\cite{ShapeNetPart16, S3DIS16}.
The evolution of attention mechanisms~\cite{AttentionIsAllYouNeed,ReKo23} has also contributed to the development of effective representations for downstream tasks, as exemplified by the emergence of 3D Transformers~\cite{PointTrans21,groupfree21, voxeltransformer21}. Notably, 3D self-supervised representation learning has garnered significant attention in recent studies. PointContrast~\cite{PointContrast20} utilizes contrastive learning across different views to acquire discriminative 3D scene representations. Innovations such as Point-BERT~\cite{PointBERT} and Point-MAE~\cite{PointMAE} introduce masked modeling~\cite{MAE,BERT} pretraining into the 3D domain. 
ACT~\cite{ACT23} pioneers cross-modal geometry understanding through 2D or language foundation models such as CLIP~\cite{CLIP} or BERT~\cite{BERT}. 
Following ACT, {\scshape ReCon}~\cite{ReCon23} further proposes a learning paradigm that unifies generative and contrastive learning. PPT~\cite{ppt24} highlights the significance of positional encoding in 3D representation learning
Additionally, leveraging foundation vision-language models like CLIP~\cite{ACT23,CLIP} has spurred the exploration of a new direction in open-world 3D representation learning. This line of work seeks to extend the applicability and adaptability of 3D representations in diverse and open-world/vocabulary scenarios~\cite{OpenScene23,CLIPFO3D23,PLA23,Lowis3D23,OVIR3D23,PointGCC23}.

\section{Additional Discussions}
\subsection{Relation to Affordance \& 6-DoF Pose Estimation}
Conceptually, this semantic orientation is a counterpart of \textit{affordance}~\citep{Affordance77,AffordanceHRI16,MoveWithAffordanceMaps20,HandsAsAffordancesProbes22} but beyond,
as SO and affordance all present potential actions and interactions with objects.
However, SO also contains the spatial understanding of intra-object part-level attributes more than affordance learning.
Compared to vanilla 6-DoF pose estimation, our proposed SO combined with the 3-DoF translation understanding has the same DoF completeness.
The difference is, our proposed SO is grounded by languages, making it useful for open-world manipulation requiring complicated spatial reasoning~\cite{RobotsThatUseLanguage20,SayCan22,Open6DOR24}. 
In addition, our Semantic Orientation can be auto-labeled from Internet 3D data that achieves higher scalability, introduced in the next section.
\input{figs/spatialvqa_statistic}


\subsection{Comparison to Concurrent Works}
\input{figs/simpler_visual}
\subsubsection{Comparison with ReKep~\cite{ReKep24}}
Recently, ReKep has succeeded in executing complex robotic tasks, such as long-horizon manipulation, based on the relationships and constraints between spatial key points. 
Its structural design offers many insights that \sofar~can draw upon, yet it also presents several issues: 
(1) Overly customized prompt engineering. ReKep requires manually designed complex system prompts for each task during inference. 
While this approach may be described as ``no training'', it cannot be considered a true zero-shot transfer. In contrast, \sofar~achieves genuine zero-shot transfer by eliminating the need for any human involvement during inference; (2) Using constraints based solely on key points fails to capture the full 6-DoF pose integrity of objects. For example, in the ``pouring water'' task, merely bringing the spout of the kettle close to the cup may lead to incorrect solutions, such as the kettle overturning; (3) ReKep requires all key points to be present in the first frame, and each step of the process—from mask extraction to feature dimensionality reduction, clustering, and filtering—introduces additional hyperparameters.

\subsubsection{Comparison with Orient Anything~\cite{orient_anything24}}
Recently, Orient Anything also highlighted the importance of orientation in spatial perception and adopted a training data construction approach similar to Our PointSO. Our primary distinction lies in semantic orientation, which is language-conditioned orientation. In contrast, Orient Anything is limited to learning basic directions such as ``front'' and ``top''. By aligning with textual information, semantic orientation better enhances spatial perception, understanding, and robotic manipulation.

\subsection{Future Works}
Future work includes further expanding the OrienText300K with larger datasets like Objaverse-XL~\cite{ObjaverseXL23}, enhancing the performance of semantic orientation through self-supervised learning and pretraining methods~\cite{MAE,CLIP,ACT23,ReCon23}, and demonstrating its effectiveness in a broader range of robotic scenarios, such as navigation~\cite{GOAT24}, mobile manipulation~\cite{homerobot23}, lifelong learning~\cite{LIBERO23}, spatio-temporal reasoning~\cite{ReKep24,LeaFLF23,CrossVideoSC24,thinking24}, humanoid~\cite{OmniH2O24,SmoothHumanoidLCP24,Exbody24,humanup25}, and human-robot interaction~\cite{HOI4D22,InteractiveHO23}.

\input{tabs/open6dorv2_statistic}

\section{Additional Visualizations}\label{app:visualization}

\subsection{Robotic Manipulation}
As shown in \cref{fig:simpler_visual}, we present a visualization of executing a task named ``move near''.
According to the input image and task instruction - ``\textit{move blue plastic bottle near pepsi can}'', \ours~can predict the center coordinate of the target object (bottle) and relative target (pepsi can), and it would infer the place coordinate and produce a series of grasp pose.

\subsection{6-DoF SpatialBench}
To further evaluate 6-DoF spatial understanding, we construct a 6-DoF SpatialBench.
We present examples of question-answer pairs from the 6-DoF SpatialBench, with quantitative and qualitative questions shown in \cref{fig:spatialbench_show1,fig:spatialbench_show2}, respectively. 
The benchmark we constructed is both challenging and practical, potentially involving calculations based on the laws of motion, such as ``\textit{Assuming a moving speed of 0.5 m/s, how many seconds would it take to walk from here to the white flower?}'' Moreover, it covers a wide range of spatially relevant scenarios across both indoor and outdoor environments.


\subsection{System Prompts}
Prompt engineering significantly enhances ChatGPT's capabilities. The model's understanding and reasoning abilities can be greatly improved by leveraging techniques such as Chain-of-Thought~\cite{CoT22} and In-Context Learning~\cite{GPT3_20}. \cref{fig:filter_prompt,fig:instruction_prompt} illustrate the system prompt we used in constructing OrienText300K.
\cref{fig:open6dor_prompt}, \cref{fig:manip_prompt}, and \cref{fig:vqa_prompt} illustrate the system prompt we used when evaluating \sofar on Open6DOR (simulation), object manipulation (both simulation and real worlds), and VQA, respectively.
Note that different from previous methods~\cite{VoxPoser23,ReKep24}, \sofar does not require complicated in-context examples.


\input{figs/spatialbench_show}
\input{figs/system_prompts}
