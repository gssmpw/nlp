In this section, we analyze the cumulative regret of our algorithm and show a $\tilde{O}(\sqrt{T})$ regret guarantee with high probability. We leave all of the proof details to \Cref{app:sec:proof_detail}, and here we only display key lemmas and proof sketches.

We firstly propose our main theorem.
\begin{theorem}[Regret]
    \label{thm:regret}
    Let $\tau=\frac1{\sqrt{T}}$ in \Cref{algo:C20CB}. For any adversarial $\{\gamma_t\}_{t=1}^T$ input sequence, C20CB suffers at most $\tilde{O}\left(\sqrt{T}\cdot\log\frac{T}{\delta}\right)$ regret, with probability $\Pr\geq 1-\delta$.
\end{theorem}

\begin{proof}
    \label{proof:thm_regret_sketch}
    In order to prove \Cref{thm:regret}, we have to show the following three components:
    \begin{enumerate}
        \item The reward function $r_t(p)$ is unimodal. Also, $r_t(p)$ is smooth at $p_t^*$, and is strongly concave on a neighborhood of $p_t^*$.
        \item The estimation error of $a$ and $b$ are bounded by $O(\frac1{T^{1/4}})$ at the end of Stage 1.
        \item The price whose derivative estimate has the closest-to-zero confidence bound is asymptotically close to $p_t^*$.
    \end{enumerate}


In the following, we present each corresponding lemma regarding to the roadmap listed above.
\begin{lemma}[revenue function $r_t(p)$]
    \label{lemma:r_t_p}
    For the expected revenue function $r_t(p)$ defined in \cref{equ:revenue_function}, the following properties hold:
    \begin{enumerate}
        \item We have
        \begin{equation}
            \label{equ:r_p_and_derivatives}
            \begin{aligned}
                r_t(p) & = p(\gamma_t-c+G(c)-G(\gamma_t-a+bp))\\
                r'_t(p) & = \gamma_t-c+G(c)-G(\gamma_t-a+bp)-bp\cdot F(\gamma_t-a+bp).
            \end{aligned}
        \end{equation}
        \item There exists a constant $L_r>0$ such that $r'(p)$ is $L_r$-\emph{Lipschitz}.
        \item $r'(p)$ is monotonically \emph{non-increasing}.
        \item $r_t(p)$ is \emph{unimodal}: There exists a unique $p_t^*\in[0, \frac{a}{b}]$ such that $r'_t(p_t^*)=0$, and $r_t(p)$ monotonically increase in $[0, p_t^*]$ and decrease in $[p_t^*, \frac{a}{b}]$. Notice that $\frac{a}{b} > p_{\max}$ according to \Cref{assumption:inequality}.
        \item $r_t(p)$ is \emph{smooth} at $p_t^*$: There exists a constant $C_s >0$ such that $r_t(p_t^*)-r_t(p)\leq C_s(p_t^*-p)^2, \forall p\in[0, p_{\max}]$.
        \item $r_t(p)$ is \emph{locally strongly concave}: There exist $\epsilon_t>0$ and $C_{\epsilon}>0$ such that $\forall p_1, p_2\in[p_t^*-\epsilon_t, p_t^*+\epsilon_t]$ we have $|r'_t(p_1)-r'_t(p_2)|\geq C_{\epsilon}\cdot|p_1-p_2|$.
        \item There exists a constant $C_v>0$ such that for any $t\in[T]$ and $p\in(p_t^*-\epsilon_t, p_t^*+\epsilon_t)$, we have $|r_t(p_t^*)-r_t(p)|\leq C_v\cdot(r'_t(p))^2$.
    \end{enumerate}
\end{lemma}
Please kindly check \Cref{app:subsec:proof_lemma_r_t_p} as a detailed proof of \Cref{lemma:r_t_p}. The properties of $r_t(p)$ and $r'_t(p)$ enable us upper bound the cost of estimation error and decision bias. In the following, we propose a lemma that serves as a milestone of estimation error upper bounds.








\begin{lemma}[Estimation error of $a$ and $b$]
    \label{lemma:a_b_estimation_error}
    With probability $\Pr\geq 1-2\eta\delta$, we have
    \begin{equation}
        \label{equ:error_b_hat_and_a_hat}
        \begin{aligned}
            |\hat b - b| \leq& C_b\cdot\frac1{\sqrt{\tau}},\\
            |\hat a - a| \leq& C_a\cdot\frac1{\sqrt\tau},
        \end{aligned}
    \end{equation}
    where $C_a:=p_{\max}(C_b + b_{\max}\cdot\sqrt{\frac12\log\frac{2}{\eta\delta}})$ and $C_b:=\frac{8b_{\max}^2}{\gamma_{\min}-\gamma_0}\cdot\sqrt{\frac12\log\frac{2}{\eta\delta}}$.
\end{lemma}

The key observation to prove this lemma lies in the expectation of each $e_{i,t}$ that indicates whether the demand exceeds certain level under uniformly distributed prices. With the help of a method-of-moment estimate as introduced in \Cref{subsec:pure_exploration}, we get rid of the influence of noise distribution and achieves an unbiased estimator of $\frac1b$ (and therefore $\hat b$). With $\hat b$ serving as a plug-in estimator, we later get $\hat a$. By applying Hoeffding's Inequalities, we obtain those error bounds. We defer the detailed proof of \Cref{lemma:a_b_estimation_error} to \Cref{app:subsec:proof_lemma_a_b_estimation_error}. With the help of \Cref{lemma:a_b_estimation_error}, we may upper bound the estimation error of $r'_t(p)$ at discrete prices. The error bound is displayed as the following lemma.

\begin{lemma}[Estimation error of $r'_t(p_{k, t})$]
    \label{lemma:r_derivative_estimation_error}
    There exists constants $C_N>0, C_{\tau}>0$ such that for any $t\in[T], k\in\{-M, -M+1, \ldots, M-1, M\}$, its holds with probability $\Pr\geq 1-6\eta\delta$
    \begin{equation}
        \label{eq:derivative_estimation_error}
        |r'_t(p_{k,t})-\hat r_{k,t}|\leq C_N\cdot\frac1{\sqrt{N_k(t)}} + C_\tau\cdot\frac1{\sqrt{\tau}} = \Delta_k(t).
    \end{equation}
    Here $N_k(t)$ and $\Delta_k(t)$ denotes the value of $N_k\text{ and }\Delta_k$ at the beginning of time period $t$.
\end{lemma}

We kindly refer the readers to \Cref{app:subsec:proof_lemma_r_derivative_estimation_error} as a rigorous proof of \Cref{lemma:r_derivative_estimation_error}. Given this lemma, the derivatives of each discrete price $p_{k,t}$ is truthfully reflected by their corresponding error bound. Therefore, we intuitively see that the closest-to-zero confidence bound represents the closest-to-$p_t^*$ discrete price. We formulate this intuition as the following lemma:

\begin{lemma}[Closest-to-zero confidence to performance]
    \label{lemma:c20_to_performance}
    Denote $\Delta_k(t)$ as the value of $\Delta_k$ at the beginning of period $t$. There exists two constants $N_0>0, N_1>0$ such that for any $t=1,2,\ldots, T$ in Stage 2, either of the following events occurs with high probability.
    \begin{enumerate}
        \item When $\exists k\in\{-M, -M+1, \ldots, M-1, M\}$ such that the Number $k$ confidence bound satisfies $\hat{r}_{k,t}-\Delta_k(t)\leq 0 \leq \hat{r}_{k,t}+\Delta_k(t)]$, and also $N_k(t)>N_0$, then we have $p_{k, t}\in[p_t^*-\epsilon_t, p_t^*+\epsilon_t]$. Furthermore, there exists constant $C_{in}$ such that $r_t(p_t^*)-r_t(p_{k,t})\leq C_{in}(\frac1{N_k(t)} + \frac1{\tau})$.
        \item When there exists no confidence bound that contains $0$, i.e. either $\hat{r}_{k,t}-\Delta_k(t)>0$ or $\hat{r}_{k,t}+\Delta_k(t)<0, \forall k\in\{-M, -M+1, \ldots, M-1, M\}$ (happens at least for one $k$), and also $N_k(t)>N_1$, then we have $$\inf_{k}\min\{|\hat{r}_{k,t}-\Delta_k(t)|, |\hat{r}_{k,t}+\Delta_k(t)|\}\leq \frac{L_r(C_a + C_b\cdot p_{\max})}{2b_{\min}}\cdot\frac1{\sqrt{\tau}},$$ and also $p_{k, t}\in[p_t^*-\epsilon_t, p_t^*+\epsilon_t]$ Furthermore, there exists constant $C_{out}$ such that $r_t(p_t^*)-r_t(p_{k,t})\leq C_{out}(\frac1{N_k(t)} + \frac1{\tau})$. 
    \end{enumerate}
\end{lemma}

The intuition to prove \Cref{lemma:c20_to_performance} is twofold:
\begin{enumerate}
    \item When an error bar contains $0$, the true derivative of the corresponding price is close to $0$ within the distance of its error bound. By applying \Cref{lemma:r_t_p} Property (7), we may upper bound the performance loss with the square of its derivatives, which is further upper bounded by the square of error bound.
    \item When no error bar contains $0$, there exists an adjacent pair of prices whose error bars are separated by $y=0$. On the one hand, their derivatives difference is upper bounded due to the Lipschitzness of $r'_t(p)$. On the other hand, the same derivatives difference is lower bounded by the closest-to-zero confidence bound. Therefore, the gap between $y=0$ and the closest-to-zero confidence bound should be very small, and we still have a comparably small $|r'_t(p_t)|$ if $p_t$ possesses that confidence bound. As a consequence, we have similar upper bound on the performance loss comparing with Case (1), up to constant coefficients.
\end{enumerate}
The detailed proof of \Cref{lemma:c20_to_performance} is presented in \Cref{app:subsec:proof_lemma_c20_to_performance}. Finally, we have a lemma that upper bounds the regret of proposing $p_t=\frac{\hat a}{2\hat b}$ under specific circumstances.
\begin{lemma}[Proposing $\frac{\hat a}{2\hat b}$]
    \label{lemma:corner_case}
    When $\gamma_t>\frac{\hat a + C_a\cdot\frac1{\sqrt{\tau}}}2+c$ and when $\hat r_{k,t}-\Delta_k(t)>0, \forall k=-M, -M+1,\ldots, M-1, M$, we have $p_t^*=\frac{a}{2b}$ and there exists a constant $C_{non}$ such that
    \begin{equation}
        \label{eq:lemma_corner_case}
        \begin{aligned}
            r_t(\frac{a}{2b})-r_t(\frac{\hat a}{\hat 2b}) \leq C_{non}\frac1{\tau}.
        \end{aligned}
    \end{equation}
\end{lemma}



The intuition of \Cref{lemma:corner_case} is that $\frac{a}{2b}$ is the optimal price without censoring, and we only need to show that either the optimal price or $\frac{a}{2b}$ is not censored (which are equivalent as the revenue function is unimodal). We defer its proof to \Cref{app:subsec:proof_lemma_corner_case}. This lemma serves as the last puzzle of the proof. With all lemmas above, we may upper bound the cumulative regret as follows:

\begin{equation}
    \label{eq:total_regret_sketch}
    \begin{aligned}
        Regret&=\sum_{t=1}^T r_t(p_t^*) - r_t(p)\\
        &\leq \tau\cdot a_{\max} p_{\max} + (2M+1)(1+N_0+N_1)\cdot a_{\max} p_{\max}\\
        &\quad + \sum_{t=1}^T (\max\{C_{in}, C_{out}\})(\frac1{N_k(t)} + \frac1{\tau}) +\sum_{t=1}^T C_{non}\cdot\frac1{\tau}\\
        (\text{ let }\tau=\sqrt{T})\rightarrow\quad&=\tilde{O}(\sqrt{T} + T^{\frac14} + \sum_{t=1}^T\frac1{N_{k}(t)} + \frac{T}{\tau})\\
        &=\tilde{O}(\sqrt{T} + \sum_{k=1}^{2M+1}\sum_{i_k=1}^{N_k(T)}\frac1{i_k})\\
        &=\tilde{O}(\sqrt{T} + T^{\frac14}\log T)\\
        &=\tilde{O}(\sqrt{T}).
    \end{aligned}
\end{equation}
By applying a union bound, we know that \Cref{eq:total_regret_sketch} holds with probability $$\Pr\geq 1-2\eta\delta - 6\eta\delta\cdot T(2M+1) \geq 1- 20\frac{c}{2(C_a+C_b\cdot p_{\max})}T^{5/4}\cdot \eta\delta.$$
Here the first part comes from \Cref{lemma:a_b_estimation_error}, and the second part comes from \Cref{lemma:r_derivative_estimation_error} for any $t\in[T]$ and $k\in\{-M, \ldots, M\}$. Let $\eta:=\frac{C_a + C_b\cdot p_{\max}}{10c\cdot T^{5/4}}$, and we show that \Cref{thm:regret} holds.
\end{proof}
\begin{remark}
    This $\tilde{O}(\sqrt{T})$ regret upper bound is (near) optimal up to $\log{T}$ factors, as it matches the $\Omega(\sqrt{T})$ information-theoretic lower bound proposed by \citet{broder2012dynamic} for a \emph{no-censoring} problem setting with linear noisy demand.
\end{remark}

From the analysis above, we notice that the threshold of learning the optimal prices in our problem setting is still the estimation of parameters. The assumptions we made in \Cref{sec:preliminaries} scales the efficiency of learning $\hat b$ and $\hat a$, which we will discuss in \Cref{sec:discussion}.