In this section, we present our core algorithm,\textbf{C20CB}, which stands for a \emph{Closest-To-Zero Confidence Bound} strategy that proposes asymptotically optimal prices over differentiated inventory levels and censoring effects. The structure of C20CB is displayed as \Cref{algo:C20CB}.


\begin{algorithm}[htbp]
    \caption{C20CB:Closest-To-Zero Confidence Bound}
    \label{algo:C20CB}
    \begin{algorithmic}[1]
        \STATE {\bfseries Input}: {Quantities $\tau$ and $a_{\max}, b_{\min}, b_{\max}, p_{\max}, c$, coefficients $\gamma_0, C_a, C_b, C_F, C_G, C_N, C_{\tau}$.}
        \STATE \textbf{STAGE 1: Pure Exploration}
        \FOR{ $t=1,2,\ldots, \tau$}
            \STATE Sample and propose a price $p_t\sim U[0, p_{\max}]$ uniformly at random.
            \STATE Observe demand $D_t$ and indicators $e_{i,t}:=\ind[D_t\geq\frac{i\cdot \gamma_t + (4-i)\gamma_0}4], i=1,2,3$.
        \ENDFOR
        \STATE Estimate
        \begin{equation}
                \label{eq:a_hat_and_b_hat}
                \begin{aligned}
                    \hat b = & \frac1{4p_{\max}\cdot\frac1{\tau}\sum_{t=1}^\tau\frac{e_{1,t}-e_{2,t}}{\gamma_t - \gamma_0}}\\
                    \hat a = & \frac1{\tau}\sum_{t=1}^{\tau}( \hat b p_{\max}e_{3,t} + \frac{3\gamma_t+\gamma_0}4).
                \end{aligned}
            \end{equation}
        \STATE \textbf{STAGE 2: Optimistic Acting}
        \STATE Denote $\Delta := (C_a + C_b\cdot p_{\max})\cdot\frac1{\sqrt{\tau}}$.
        \STATE Define $M:=\lfloor\frac{c}{2\Delta}\rfloor$ and $w_k:=2k\Delta, k=-M, -M+1, \ldots, -1,0,1,\ldots,M$.
        \FOR{$t=1,2,\ldots, 2M+1$}
            \STATE Let $k_t:=-M-1+t$ and propose $p_t=\frac{w_{k_t}-(\gamma_t-\hat a)}{\hat b}$.
            \STATE Observe $D_t$ and $\ind_t:=\ind[D_t<\gamma_t]$.
            \STATE Initialize $F_{k_t}\leftarrow \ind_t, N_{k_t}\leftarrow 1, G_{k_t}\leftarrow D_t-\gamma_t + c, \Delta_{k_t}\leftarrow C_F\cdot b_{\max}p_{\max} + C_G + C_b\cdot\frac1{\sqrt{\tau}}$.
        \ENDFOR
        \FOR {$t=1,2,\ldots, T-\tau-(2M+1)$}
            \IF{$\gamma_t\geq\frac{\hat a + C_a\cdot\frac1{\sqrt{\tau}}}2 + c$}
                \STATE Propose $p_t = \frac{\hat a}{2\hat b}$ and continue to $t+1$ (without recording feedback).
            \ENDIF
            % \FOR {$k=-M, -M+1,\ldots, M-1, M$}
            %     \STATE Denote $p_{k, t}:=\frac{w_k-(\gamma_t-\hat a)}{\hat b}$ and $\hat r_{k,t}:=\gamma_0 - c + G_k - \hat b \cdot p_{k,t}\cdot F_k$
            %     % \begin{equation}
            %     %     \label{eq:hat_r_k_t_in_algo}
            %     %     \hat r_{k,t}:=\gamma_0 - c + G_k - \hat b \cdot p_{k,t}\cdot F_k.
            %     % \end{equation}
            % \ENDFOR
            \STATE Initialize $k_t \leftarrow M, \rho_t\leftarrow +\infty$.
            \FOR{$k=M, M-1, \ldots, -M+1, -M$}
                \STATE Denote $p_{k, t}:=\frac{w_k-(\gamma_t-\hat a)}{\hat b}$ and $\hat r_{k,t}:=\gamma_0 - c + G_k - \hat b \cdot p_{k,t}\cdot F_k$.
                \IF {$\hat r_{k, t} - \Delta_{k}\leq 0 \leq \hat r_{k, t} + \Delta_{k}$}
                    \STATE Update $k_t\leftarrow k, \rho_t\leftarrow 0$, and Break.
                \ENDIF
                \STATE Let $\rho_{k, t}:=\min\{|\hat r_{k, t} - \Delta_{k}|, |\hat r_{k, t} + \Delta_{k}|\}$.
                \IF {$\rho_{k, t} < \rho_t$}
                    \STATE Update $\rho_t\leftarrow\rho_{k, t}$ and $k_t\leftarrow k$.
                \ENDIF
            \ENDFOR
            \IF{$\hat r_{k, t}-\Delta_k>0, \forall k=-M, -M+1, \ldots, M-1, M$}
                \STATE Propose $p_t=\frac{\hat a}{2 \hat b}.$
            \ELSE
                \STATE Propose $p_t = p_{k_t, t}$.
            \ENDIF
            % \STATE Propose $p_{k_t, t}$ as $p_t$.
            \STATE Observe $D_t$ and $\ind_t=\ind[D_t < \gamma_t]$.
            \STATE Update
            \begin{equation}
                \label{eq:updates_DFGN}
                F_{k_t}\leftarrow\frac{N_{k_t} F_{k_t} + \ind_t}{N_{k_t}+1},\ G_{k_t}\leftarrow\frac{N_{k_t} G_{k_t} + D_t - \gamma_t + c}{N_{k_t}+1},\ \Delta_{k_t}\leftarrow \frac{C_N}{\sqrt{N_{k_t}+1}} + \frac{C_{\tau}}{\sqrt{\tau}}
            \end{equation}
            \STATE Update $N_{k_t}\leftarrow N_{k_t} + 1$.
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\begin{comment}
\begin{algorithm}[t]
	\SetAlgoNoLine
	\KwIn{Node $\alpha$'s ID ($ID_{\alpha}$), and node $\alpha$'s
		neighbors' IDs within two communication hops.}
	\KwOut{The frequency number ($FreNum_{\alpha}$) node $\alpha$ gets assigned.}
	$index$ = 0; $FreNum_{\alpha}$ = -1\;
	\Repeat{$FreNum_{\alpha} > -1$}{
		$Rnd_{\alpha}$ = Random($ID_{\alpha}$, $index$)\;
		$Found$ = $TRUE$\;
		\For{each node $\beta$ in $\alpha$'s two communication hops
		}{
			$Rnd_{\beta}$ = Random($ID_{\beta}$, $index$)\;
			\If{($Rnd_{\alpha} < Rnd_{\beta}$) \text{or} ($Rnd_{\alpha}$ ==
				$Rnd_{\beta}$ \text{and} $ID_{\alpha} < ID_{\beta}$)\;
			}{
				$Found$ = $FALSE$; break\;
			}
		}
		\eIf{$Found$}{
			$FreNum_{\alpha}$ = $index$\;
		}{
			$index$ ++\;
		}
	}
	\caption{Frequency Number Computation}
	\label{alg:one}
\end{algorithm}
\end{comment}
\subsection{Algorithm Design Overview}
\label{subsec:algorithm_design}

Our algorithm has two stages:
\begin{enumerate}
    \item { \textbf{Stage 1: Exploration}}: During the first $\tau=\sqrt{T}$ rounds, the seller (we) proposes uniformly random prices in the range of $[0, p_{\max}]$. By the end of Stage 1, we obtain $\hat{a}$ and $\hat{b}$ as plug-in estimators of $a$ and $b$ in the following stage.
    \item { \textbf{Stage 2: Optimistic Decision}}: We estimate the derivatives of the revenue function at discretized prices $\{p_{k,t}\}$'s. For each $p_{k,t}$, we not only estimate $r'_t(p_{k,t}$ but also maintain an error bar of that estimate. At each time $t$, we propose the price whose corresponding error bar covers $0$ or closest to $0$ if no covering exists.
\end{enumerate}

\Cref{algo:C20CB} exhibits several advantageous properties. It is suitable for processing streaming data as the constructions of $\hat a, \hat b, \hat r'_t(\cdot)$ are updated \emph{incrementally} with each new observation (including $e_{i,t}, D_t, \ind_t$) without the need of revisiting any historical data. Additionally, it consumes $\tilde{O}(T^{\frac54})$ time complexity and $O(T^{\frac14})$ extra space, which are plausible for large $T$. A potential risk of computation might arise on the calculation of $\hat b$, where $\sum_{t=1}^{\tau}e_{1,t}-e_{2,t}$ can be 0 with a small but nonzero probability. Although this event does not undermine the high-probability regret guarantee, it might still be harmful to the computational system for numerical experiments. To mitigate this incident in practice, we may either extend Stage 1 until one non-zero $e_{1,t}-e_{2,t}=1$ is observed, or restart Stage 1 at $t=\tau$.

In the following sections, we will introduce each stage and technical component in details.

\subsection{Pure-Exploration to Estimate Parameters from Biased Observations}
\label{subsec:pure_exploration}
We incorporate a uniform-exploration phase for estimating $a$ and $b$ in our algorithm, bypassing the obstacle brought by demand censoring. This approach is supported by the following insight: When $Y$ is a uniformly distributed random variable within a closed interval $[L,R]$, and $X$ is another random variable, independent to $Y$ and also distributed within $[L,R]$, we have:
\begin{equation}\label{equ:total_expectation}
    \E[\ind[Y\geq X]]=\Pr[Y\geq X] = \E[\Pr[Y\geq X| X]] = \E[\frac{X-L}{R-L}] = \frac{\E[X]-L}{R-L}.
\end{equation}
Here the second step uses the Law of Total Expectation. \Cref{equ:total_expectation} indicates that we can derive an unbiased estimator of $\E[X]$ through $\ind[Y\geq X]$ even in the absence of any direct observation of $X$. Looking back to our algorithm, when $p_t\sim U[0, p_{\max}]$, we have
\begin{equation}
\label{equ:e_i_expectation_demo}
\begin{aligned}
    \E[e_{i,t}] & = \E[\ind[a-bp_t+N_t\geq \gamma_i]] = \E[\E[N_t\geq \gamma_i-a+bp_t|N_t]]=\E[\frac{N_t-\gamma_i+a}{bp_{\max}}]=\frac{a-\gamma_i}{bp_{\max}}.
\end{aligned}
\end{equation}
Here $\gamma_i:=\frac{i\gamma_t + (4-i)\gamma_0}4$. The last equality comes from $\E[N_t]=0$. By deploying different $\gamma_i$ at $i=1,2,3$, we can estimate $a$ and $b$ through the observations of $e_{i,t}$, effectively circumventing the censoring effect. A similar technique has been utilized by \citet{fan2021policy} to construct an unbiased estimator of the \emph{valuations} instead of the demands as we concern. However, their application of uniform exploration might be sub-optimal as they adopt an \emph{exploration-then-exploitation} design in each epoch. On the contrary, our algorithm uses this uniform exploration merely as a \emph{trigger} of further learning. Our tight regret bound indicates that uniform exploration can still contribute to an optimal algorithm for a broad range of online learning instances.

\subsection{Optimistic Strategy to Balance Derivatives Estimates v.s. Revenue Loss}
\label{subsec:optimistic_strategy}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{illustration_c20cb_new.pdf}
    \caption{The price which C20CB proposes based on confidence bounds of $\hat r_{k,t}$: (a) If there exist prices whose error bar contain $0$, then we propose the largest price among them. (b) If no error bar contains $0$ but there does exist at least one below $0$, we propose the price whose corresponding error bar is closest to $0$. (c) If all error bars are above $0$, then we propose $\frac{\hat a}{2\hat b}$ for pure exploitation.}
    \label{fig:illustration_c20cb}
\end{figure}
We proceed to Stage 2 with $\hat a$ and $\hat b$ established, which indicates we have an estimate on the expected potential linear demand $d_t(p)=a-bp$. However, we are still unaware of the noise distribution, which is crucial for the current optimal price as the inventory level $\gamma_t$ partially censors the noise.

In order to balance the learning of noise distribution versus the loss of proposing sub-optimal prices, we adopt an \emph{optimistic} which involves the following components:
\begin{enumerate}[label=(\roman*)]
    \item We discretize the $[-c,c]$ domain of noise CDF $F(\cdot)$ and its intergration $G(\cdot)$ into small intervals of length $2\Delta$, with $\Delta=O(\frac1{T^{1/4}})$. At the center of each interval $k$ (which is $2k\Delta$) we maintain independent estimates of $F$ and $G$, including their expectations and high-confidence error bars.
    \item At each time $t$, we construct a set of discrete prices $\{p_{k,t}\}_{k=-M}^{M}$ such that the quantity $\gamma_t-\hat a+\hat bp_{k,t}$ matches the center of the interval $k$ established above. According to \Cref{equ:r_p_calculate}, we construct an estimate of each $r'_t(p_{k,t})$ with plug-in estimators $\hat a, \hat b$ and discrete estimators of $F$ and $G$ for the specific Interval $k$. Again, the estimate includes its expectation and error bar.
    \item Since the optimal price $p_t^*$ satisfies $r'_t(p_t^*)=0$, we identify the discrete price $p_{k,t}$ where the derivative estimate is "possibly $0$" or "closest to $0$". We illustrate this process in \Cref{fig:illustration_c20cb}, which includes the following three cases:
    \begin{enumerate}[label=(\alph*)]
        \item If there exists some $p_{k,t}$ such that the corresponding error bar (of its derivative estimate) contains $0$, we propose the largest price satisfying this condition. % We illustrate this process in \Cref{fig:illustration_c20cb} Part (a).
        \item If there does not exist $p_{k,t}$ whose corresponding error bar contains $0$ (but there does exist error bar below $0$), we propose the price whose \emph{error bound} is closest to $0$. % We illustrate this process in \Cref{fig:illustration_c20cb} Part (b).
        \item If all error bars are above $0$, indicating that the reward function is monotonically increasing over the ``censoring area'', we propose $p_t=\frac{\hat a}{2\hat b}$ to exploit the non-censoring optimal price $\frac{a}{2b}$. In this case, we do not need to record any observations nor to update any parameter/estimate. % We illustrate this process in \Cref{fig:illustration_c20cb} Part (c).
        
    \end{enumerate}
    \item After proposing the price $p_{k_t, t}$ and observing feedback $D_t$ and $\ind_t$, we update the estimates of $F(\cdot)$ and $G(\cdot)$ for Interval $k_t$ in which $\gamma_t-\hat a+\hat bp_{k_t,t}$ exists.
\end{enumerate}

In a nutshell, we maintain estimates and error bars of $F(\cdot)$ and $G(\cdot)$ at discrete points $2k\Delta$, and map each $2k\Delta$ to a corresponding price $p_{k,t}$ once an inventory $\gamma_t$ occurs. Then we propose the price whose derivative estimate $\hat r_{k,t}\pm\Delta_k$ is \emph{closest-to-zero} among all $k$. Finally, we update the estimates with observations. 

Here we provide an intuition of the optimality: On the one hand, the width of interval can tolerate the error of mapping from $2k\Delta$ to $p_{k,t}$, and the Lipschitzness of $F$ and $G$ ensures that our estimate within each small interval is roughly correct. On the other hand, we can show that the closest-to-zero derivative estimate implies a \emph{closest-to}-$p_t^*$ price according to some locally strong convexity in the neighborhood of $p_t^*$. As we have smoothness on the regret function, we suffer a quadratic loss at $(T\cdot\frac1{T^{1/4}})^2=O(\sqrt{T})$ cumulatively, which balances the loss of Stage 1 that costs $O(\tau)=O(\sqrt{T})$ as well. For a rigorous regret analysis, we kindly refer the readers to \Cref{sec:regret_analysis}.

\subsection{Technical Novelty.}
To the best of our knowledge, we are the first to introduce \emph{optimism} on the derivatives and achieve optimal regret in an \emph{adversarial} online learning problem. In contrast, existing works either develop optimistic algorithms on the reward (or loss) function as the original UCB strategy \citep{lai1985asymptotically}, or instead use unbiased stochastic gradients and conduct first-order methods for online optimization \citep{hazan2019introduction}.