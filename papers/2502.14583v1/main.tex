%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}
\usepackage[preprint]{icml2025}


% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow} 
\usepackage{makecell}
% \usepackage{caption}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following use package line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
% \usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}

% my packages
\usepackage[inline, shortlabels]{enumitem} % for inline list
\usepackage{svg} 
% \usepackage{xcolor}
\definecolor{xkcdLightGreen}{HTML}{15B01A}
\input{math_commands}



% commands for math symbols
\newcommand{\br}{\cN_{[]}}
\newcommand{\cov}{\cN}
\newcommand{\multi}{\mathrm{multi}}
\newcommand{\single}{\mathrm{single}}
\newcommand{\sfp}{\mathsf{p}}
\newcommand{\sfq}{\mathsf{q}}
\newcommand{\rtv}{\cR_{\overline{\mathrm{TV}}}}
\newcommand{\de}{d_{\mathrm{e}}}
\newcommand{\relu}{\mathrm{ReLU}}
\newcommand{\pmlp}{{\omega}}
\newcommand{\pnn}{{\theta}}
\newcommand{\meang}{\boldsymbol{\mu}}
\newcommand{\vparm}{\boldsymbol{\rho}}


% commands for coauthors
% \newcommand{\rz}[1]{{\color{magenta}{[\textbf{rz: }#1]}}}
% \newcommand{\todo}[1]{{\color{purple}{[\textbf{TODO: }#1]}}}
% \newcommand{\guoqiang}[1]{{\color{blue}{[\textbf{gw: }#1]}}}
% \newcommand{\zcy}[1]{{\color{blue}{[\textbf{zcy: }#1]}}}
% \newcommand{\cx}[1]{{\color{red}{[\textbf{cx: }#1]}}}
% \newcommand{\cxw}[1]{{\color{orange}{#1}}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{A Theory for Conditional Generative Modeling on Multiple Data Sources}

\begin{document}

\twocolumn[
\icmltitle{A Theory for Conditional Generative Modeling on Multiple Data Sources}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}
\icmlsetsymbol{corre}{$\dagger$}

\begin{icmlauthorlist}
\icmlauthor{Rongzhen Wang}{ruc}
\icmlauthor{Yan Zhang}{sdu}
\icmlauthor{Chenyu Zheng}{ruc}
\icmlauthor{Chongxuan Li}{ruc,corre}
\icmlauthor{Guoqiang Wu}{sdu,corre}
\end{icmlauthorlist}

\icmlaffiliation{ruc}{Gaoling School of AI, Renmin University of China; Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China}
\icmlaffiliation{sdu}{School of Software, Shandong University, Shandong, China}

\icmlcorrespondingauthor{Chongxuan Li}{chongxuanli@ruc.edu.cn}
\icmlcorrespondingauthor{Guoqiang Wu}{guoqiangwu@sdu.edu.cn}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{multiple data sources, generative model, distribution estimation, maximum likelihood estimation}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\newpage

\begin{abstract}

The success of large generative models has driven a paradigm shift, leveraging massive multi-source data to enhance model capabilities. However, the interaction among these sources remains theoretically underexplored. This paper takes the first step toward a rigorous analysis of multi-source training in conditional generative modeling, where each condition represents a distinct data source. 
Specifically, we establish a general distribution estimation error bound in average total variation distance for conditional maximum likelihood estimation based on the bracketing number.
Our result shows that when source distributions share certain similarities and the model is expressive enough, multi-source training guarantees a sharper bound than single-source training.
We further instantiate the general theory on conditional Gaussian estimation and deep generative models including autoregressive and flexible energy-based models, by characterizing their bracketing numbers. 
The results highlight that the number of sources and similarity among source distributions improve the advantage of multi-source training. 
Simulations and real-world experiments validate our theory. 
Code is available at: \url{https://github.com/ML-GSAI/Multi-Source-GM}.

\end{abstract}

\section{Introduction}

Large generative models have achieved remarkable success in generating realistic and complex outputs across natural language~\cite{brown_2020_nips_gpt3,llamma_2023} and computer vision~\cite{rombach_2022_cvpr_ldm,openai_2024_sora}. A key factor behind their strong performance is the diverse and rich training data. For instance, large language models are trained on \textit{heterogeneous} datasets comprising web content, books, and code~\cite{brown_2020_nips_gpt3,modelbest_2024_minicpm}, while image generation models benefit from vast datasets spanning various categories and aesthetic qualities~\cite{peebles_2023_iccv_dit,chen_2024_iclr_pixart-alpha,esser_2024_icml_recflow}. Empirical evidence suggests that, under certain conditions, training on \textit{multiple data sources} can enhance performance across all sources~\cite{google_2019_multilingualbert,chen_2024_iclr_pixart-alpha,allen-zhuL_2024_icml_physicsofllm3-1}. Consequently, data mixture strategies have become an essential research topic~\cite{Nguyen_2022_nips_multidataset-clip,chidambaram_2022_iclr_mixup,modelbest_2024_minicpm}. 

However, the theoretical underpinnings of this multi-source training paradigm remain poorly understood. This raises a fundamental question: \emph{is it more effective to train separate models on individual data sources, or to train a single model using data from multiple sources?}  In this paper, we take the first step toward a rigorous analysis of multi-source training, focusing on its impact on conditional generative models, where each condition represents a distinct data source. 

Our first contribution is establishing a general upper bound on distribution estimation error for conditional generative modeling via maximum likelihood estimation (MLE) in \cref{sec:general_guarantee_for_multi}. Specifically, we measure the error using average total variation (TV) distance between the true and estimated conditional distributions across all sources, which scales as $\tilde{\cO}(\sqrt{{\log \cN_{\cP_{X\vert Y}}}/{n}})$, where $n$ is the training set size and $\cN_{\cP_{X\vert Y}}$ is the bracketing number of the conditional distribution space $\cP_{X\vert Y}$. Further, when source distributions exhibit parametric similarity, multi-source training effectively reduces the complexity of the distribution space, leading to a provably sharper bound than single-source training.

Technically, our analysis extends classical MLE estimation error bounds from empirical process theory~\cite{wong_1995_aos_likelihood-ratios,geer_2000_book_empiricalprocess,ge_2024_iclr_unsupervised} to the conditional setting by adapting the complexity of the distribution space and measuring the estimation error in terms of average TV distance. Further discussions are provided in \cref{sec:related_works}.

As the second contribution, we instantiate our general theory in three specific cases: (1) parametric estimation of conditional Gaussian distributions, a simple example clearly illustrating how source distribution properties influence the benefits of multi-source training, (2) autoregressive models (ARMs), the foundation of large language models~\cite{brown_2020_nips_gpt3,llamma_2023}, and (3) energy-based models (EBMs), a general class of generative models for continuous data~\cite{lecun_2006_tutorial_ebm,du_2019_nips_ebm,song_2019_nips_gradient}. 
For each model, we derive explicit estimation error bounds for both multi-source and single-source training by measuring the bracketing number of corresponding conditional distribution space. 
Our theoretical results show that, across all cases, the ratio of multi-source to single-source estimation error bounds has a formulation $\sqrt{1 - ({K-1}/{K}) \beta_{\mathrm{sim}}}$, where $K$ is the number of sources and $\beta_{\mathrm{sim}} \in [0,1]$ quantifies source distribution similarity, with model-specific definitions detailed in \cref{sec:instantiations}. 
This ratio decreases with both $K$ and $\beta_{\mathrm{sim}}$, indicating that the number of sources and their similarity improve the benefits of multi-source training. 

A core technical contribution is establishing novel bracketing number bounds for ARMs and EBMs. 
This is challenging since on the one hand, the bracketing number provides a refined measure of function spaces, requiring carefully designed one-sided bounds over the entire domain.
On the other hand, the conditional distribution space of deep generative models is inherently complex, involving both neural network architectures and specific probabilistic characteristics for different generative modeling methods. Please refer to Appendixes~\ref{app:proof_arm} and \ref{app:proof_ebm} for detailed proofs and discussions.

Finally, we validate our theoretical findings through simulations and real-world experiments in \cref{sec:experiments}. 
In simulations, we perform conditional Gaussian estimation, where the MLE solutions can be analytically derived, enabling a rigorous assessment of the tightness of our bounds. 
The close match between the empirical and theoretical error orders supports the validity of our results.
Beyond simulations, we train class-condition diffusion models~\cite{karras2024analyzing} on ILSVRC2012~\cite{ILSVRC15}
% , treating each category as a distinct data source, 
where its semantic hierarchy~\cite{imagenet_statistics} provides a natural way to define inter-source distribution similarity.
Empirical results confirm that multi-source training outperforms single-source training by achieving lower FID scores, consistent with our theoretical guarantee in \cref{sec:general_guarantee_for_multi}, and this advantage depends on both the number of sources and their similarity, supporting our insights in \cref{sec:instantiations}.
% Our code is available in the supplementary.
 

\section{Problem formulation}
\label{sec:formulation_for_conditional_generative_modeling}

\paragraph{Elementary notations.} 
Scalars, vectors, and matrices are denoted by lowercase letters (e.g., $a$), lowercase boldface letters (e.g., $\va$), and uppercase boldface letters (e.g., $\vA$).
We use $\va[m]$ to denote the $m$-th entry of vector $\va$, and $\vA[m,:]$, $\vA[:,n]$, and $\vA[m,n]$ to denote the $m$-th row, the $n$-th column, and the entry at the $m$-th row and the $n$-th column of $\vA$.  
$(\va,\vb)$ denotes the concatenation of $\va$ and $\vb$ as a column vector. 
We denote $[n] \!\coloneqq\! \{1, \dots, n\}$ for any $n \in \N$ and $a \vee b$ as $\max\{a, b\}$.
For any measurable scalar function $ f(\vx) $ on domain $ \sX $ and real number $ 1 \leq \sfp \leq \infty $, its $ L^\sfp(\sX) $-norm is defined as  
$\norm{f(\vx)}_{L^\sfp(\sX)} \!\coloneqq\! \paren{\int_{\sX} \abs{f(\vx)}^{\sfp} \, d\vx}^{\frac{1}{\sfp}}.$  
When $ \sfp \!=\! \infty $, $ \norm{f(\vx)}_{L^\infty(\sX)} \!=\! \sup_{\vx \in \sX} \abs{f(\vx)} $. 
$\I(\cdot)$ denotes the indicator function.
Notation $a_n \!=\! \tilde{\cO}(b_n)$ indicates $a_n$ is asymptotically bounded above by $b_n$ up to logarithmic factors.  


\subsection{Data from multiple sources}

Let $X$ denote the random variable for data (e.g., a natural image) in a data space $\sX$, and $Y$ denote the random variable for the source label in a label space $\sY$. 
Suppose there are $K$ data sources (e.g., $K$ categories of images), each corresponding to an unknown conditional distribution $p^*_{X\vert k}$ for $k\in[K]$.
We assume that $p^*_{X\vert k}$ is parameterized by a source-specific feature $\phi_k^*$ in a parameter space $\Phi$ and a shared feature $\psi^*$ in a parameter space $\Psi$, such that $p^*_{X\vert k}(x \vert k) = p_{\phi_k^*, \psi^*}(x \vert k)$. 
The conditional distribution of $X$ given $Y=y$ is consequently expressed as
\begin{align*}
    p^*_{X\vert Y}(\vx\vert y)
    % \!=\!p_{\{\phi_k^*\}_{k=1}^K,\psi^*}(\vx\vert y)\!=\!\!
    =\prod_{k=1}^K\!\paren*{p_{\phi_k^*, \psi^*}(\vx\vert k)}^{\I(y=k)}.
\end{align*}
This compact representation provides convenience for subsequent discussions. 

We further assume the distribution of $Y$ is known since the proportion of data from different sources is often manually designed in practice~\cite{imagenet_cvpr09,krizhevsky2009learning-cifar,brown_2020_nips_gpt3,chen_2024_iclr_pixart-alpha}. The joint distribution of $X$ and $Y$ is then given by $p^*_{X,Y}(\vx,y)=p^*_{X \vert Y}(\vx\vert y)p^*_Y(y)$. 

\subsection{Conditional generative modeling}

Consider a dataset $S = \{(\vx_i, y_i)\}_{i=1}^n$ consisting of $n$ independent and identically distributed (i.i.d.) data-label pairs sampled from the joint distribution $p^*_{X,Y}$.
In the learning phase, a conditional generative model uses maximum likelihood estimation (MLE) to estimate $p^*_{X \vert Y}$ based on the dataset $S$, where the conditional likelihood is defined as 
\begin{align}
\label{eq:likelihood}
\cL_{S}(p_{X\vert Y})\coloneqq\prod_{i=1}^n p_{X\vert Y}(\vx_i\vert y_i).
\end{align}

\paragraph{Multi-source training.}
Under multi-source training, the conditional distribution space is given by $\cP_{X\vert Y}^{\multi}\coloneqq$
\begin{align*}
    \textstyle{\curl*{p_{X\vert Y}^{\multi}(\vx\vert y)\!=\!\prod_{k=1}^K\paren*{p_{\phi_k, \psi}(\vx\vert k)}^{\I(y=k)}\!:\! \phi_k\!\in\! \Phi, \!\psi\!\in\!\Psi},}
\end{align*}
and the corresponding estimator of $p^*_{X\vert Y}$ is 
\begin{align}
\label{eq:multi_mle_solution}
    \hat{p}_{X\vert Y}^{\multi}=\argmax_{p_{X\vert Y}^{\multi} \in \cP_{X\vert Y}^{\multi}}\cL_{S}(p_{X\vert Y}^{\multi}).
\end{align}

% We adopt the realizable assumption following~\cite{ge_2024_iclr_unsupervised}, i.e., the true parameters satisfy $\phi_k^* \in \Phi$ and $\psi^* \in \Psi$. 
% This assumption simplifies the analysis by eliminating approximation error of the parameter space.
% Our general theory in \cref{thm:TV_upper_of_conditional_MLE} is based on this assumption.
Here, we adopt the realizable assumption that true parameters satisfy $\phi_k^* \in \Phi$ and $\psi^* \in \Psi$ following~\cite{ge_2024_iclr_unsupervised}, which allows the estimation error analysis to focus on the generalization property of the distribution space.

\paragraph{Single-source training.}
Under single-source training, we train $K$ conditional generative models for each source using data exclusively from the corresponding source.
For any particular source $k$, denoting $S_k\coloneqq\{(\vx_i, y_i)\in S\vert y_i=k\}=\{\vx_j^k,k\}_{j=1}^{n_k}$, the corresponding generative model estimate $p^*_{X\vert k}$ by maximizing the conditional likelihood on $S_k$ as
$$\hat{p}_{X\vert k}^{\single} = \argmax_{p_{X\vert k}^\single\in\cP_{X\vert k}^\single}\cL_{S_k}(p_{X\vert k}^\single),$$
where $\cL_{S_k}(p_{X\vert k})\coloneqq\prod_{j=1}^{n_k} p_{X\vert k}(\vx_j^k \vert k)$ and $\cP_{X\vert k}^\single\coloneqq\curl*{p_{\phi_k, \psi_k}(\vx\vert k):\phi_k\in\Phi,\psi_k\in\Psi}$.

Separately maximizing these $K$ objectives is equivalent to finding the maximizer of $L_S$ in conditional distribution space $\cP_{X\vert Y}^{\single}\coloneqq$
\begin{align*}
    \textstyle{\curl*{p_{X\vert Y}^{\single}(\vx\vert y)\!=\!\!\prod_{k=1}^K\paren*{p_{\phi_k, \psi_k}(\vx\vert k)}^{\I(y=k)}\!:\! \phi_k\!\in\! \Phi, \psi_k\!\!\in\!\Psi\!}.}
\end{align*}
Therefore, the estimator of $p^*_{X\vert Y}$ under single-source training is 
\begin{align}
\label{eq:single_mle_solution}
    \hat{p}_{X\vert Y}^{\single}=\argmax_{p_{X\vert Y}^{\single} \in \cP_{X\vert Y}^{\single}}\cL_{S}(p_{X\vert Y}^{\single}).
\end{align}
 
The introduced multi-source setting abstracts practical scenarios where different sources share certain underlying data structures (via $\psi$) while retaining unique characteristics (via $\phi_k$). 
At the same time, the single-source setting provides a controlled comparison to rigorously assess whether incorporating other sources improves the model's learning. 


\paragraph{Evaluation for conditional distribution estimation.}

% We measure the accuracy of distribution estimation by the \emph{average TV error} between the estimated and true conditional distributions, which is formally defined as:
We measure the accuracy of conditional distribution estimation by average TV distance between the estimated and true conditional distributions, referred to as \emph{average TV error}:
\begin{align}
\label{eq:expected_TV_distance}
\rtv(\hat{p}_{X\vert Y})\coloneqq\E_Y\bracket*{\mathrm{TV}(\hat{p}_{X\vert Y}, p^*_{X\vert Y})},
\end{align}
where 
$\mathrm{TV}(\hat{p}_{X\vert Y}, p^*_{X\vert Y})\!=\!\frac{1}{2}\!\int_{\sX}\!\vert\hat{p}_{X\vert Y}(\vx\vert y)\!-\!p^*_{X\vert Y}(\vx\vert y)\vert d\vx$ is the total variation distance between $\hat{p}_{X\vert Y}$ and $p^*_{X\vert Y}$. 

In the following sections, we investigate the effectiveness of multi-source training by measuring and comparing $\rtv(\hat{p}^{\multi}_{X\vert Y})$ and $\rtv(\hat{p}^{\single}_{X\vert Y})$. 


\section{Provable advantage of multi-source training}
\label{sec:general_guarantee_for_multi}

In this section, we establish a general upper bound on the average TV error for conditional MLE and provide a statistical guarantee for the benefits of multi-source training.  
Our analysis extends the classical MLE guarantees~\cite{geer_2000_book_empiricalprocess,ge_2024_iclr_unsupervised}, which leverage the bracketing number and the uniform law of large numbers.  

\subsection{Complexity of the conditional distribution space}
We begin by introducing an extended notion of the bracketing number as follows.

\begin{definition}[$\epsilon$-upper bracketing number for conditional distribution space]
\label{def:upper_bracketing_number}
    Let $\epsilon$ be a real number that $\epsilon>0$ and $\sfp$ be an integer that $1\leq \sfp\leq\infty$.
    An $\epsilon$-upper bracket of a conditional distribution space $\cP_{X\vert Y}$ with respect to $L^\sfp(\sX)$ is a finite function set $\cB$ such that for any $p_{X\vert Y}\in \cP_{X\vert Y}$, there exists some $p^\prime\in \cB$ such that given any $y \in \sY$, it holds 
    \begin{align*}
        &\forall \vx\in \sX: p^\prime(\vx,y)\geq p_{X\vert Y}(\vx\vert y), \text{ and }\\
        &\norm{p^\prime(\cdot,y)-p_{X\vert Y}(\cdot\vert y)}_{L^\sfp(\sX)}\leq \epsilon.
    \end{align*}
    The $\epsilon$-upper bracketing number $\br\paren*{\epsilon;\cP_{X\vert Y},L^\sfp(\sX)}$ is the cardinality of the smallest $\epsilon$-upper bracket. 
\end{definition}

This notion quantifies the minimal set of functions needed to upper bound every conditional distribution within a small margin, reducing error analysis from an infinite to a finite function class. 
Unlike traditional bracketing numbers for unconditional distributions $p_X$ using two-sided brackets~\cite{wellner_2002_empirical}, this extension employs one-sided upper brackets~\cite{ge_2024_iclr_unsupervised} and requires uniform coverage across $y$ for all conditional distributions tailored for our setting.

\subsection{Guarantee for conditional MLE}

We now present a general error bound which applies to both training strategies.

\begin{theorem}[Average TV error bound for conditional MLE, proof in \cref{app:proof_thm:TV_upper_of_conditional_MLE}]
\label{thm:TV_upper_of_conditional_MLE}
Given a dataset $S$ of size $n$ that i.i.d. sampled from $p^*_{X, Y}$, let $\hat{p}_{X\vert Y}$ be the maximizer of $L_S(p_{X\vert Y})$ defined in \cref{eq:likelihood} in conditional distribution space $\cP_{X\vert Y}$. Suppose the real conditional distribution $p^*_{X\vert Y}$ is contained in $\cP_{X\vert Y}$. Then, for any $0<\delta\leq1/2$, it holds with probability at least $1-\delta$ that 
\begin{align*}
    \rtv(\hat{p}_{X\vert Y}\!)\!\leq\!3\sqrt{\!\frac{1}{n}\paren*{\log\br\paren*{\!\frac{1}{n};\cP_{X\vert Y},L^1(\sX)\!\!}\!\!+\!\log\frac{1}{\delta}\!}}.
\end{align*}
\end{theorem}

As formulated in \cref{eq:multi_mle_solution} and \cref{eq:single_mle_solution}, multi-source and single-source training apply conditional MLE on $S$ within different conditional distribution spaces. 
% The following proposition shows that the guaranteed advantage of multi-source training stems from its reducing the complexity of the distribution space through proper source similarity.
The following proposition shows that multi-source training reduces the bracketing number of its distribution space through source similarity.

\begin{proposition}[Multi-source training reducing complexity, proof in \cref{app:prop:multi_has_bracket_smaller_then_single}.]
\label{prop:multi_has_bracket_smaller_then_single}
    Let $\cP_{X\vert Y}^{\multi}$ and $\cP_{X\vert Y}^{\single}$ be as defined in \cref{sec:formulation_for_conditional_generative_modeling}. Then, for any $\epsilon >0$ and $1\leq \sfp\leq\infty$, we have
    \begin{align*}
        \br\paren*{\epsilon;\cP_{X\vert Y}^{\multi},L^{\sfp}(\sX)}\leq \br\paren*{\epsilon;\cP_{X\vert Y}^{\single},L^{\sfp}(\sX)}.
    \end{align*}
\end{proposition}

Combining Theorem~\ref{thm:TV_upper_of_conditional_MLE} and Proposition~\ref{prop:multi_has_bracket_smaller_then_single}, we conclude that when source distributions have parametric similarity and the model satisfies the realizable assumption, multi-source training can enjoy a sharper estimation guarantee than single-source training. 
Simulations and real-world experiments in \cref{sec:experiments} support our result.


\section{Instantiations}
\label{sec:instantiations}

We now apply our general analysis to conditional Gaussian estimation and two deep generative models to obtain concrete error bounds. 

\subsection{Parametric estimation on Gaussian distributions}
\label{sec:instantiate_conditional_gaussian}

As employed in extensive work~\cite{montanari2022universality,wang2022binary,he2022information,zheng2023toward,dandi2024universality}, Gaussian models provide a simple yet insightful case for illustrating the benefits of multi-source training and enable analytically tractable simulations under our theoretical assumptions. 

\paragraph{Parametric distribution family.} 
Suppose each of the $K$ conditional distributions is a $d$-dimensional standard Gaussian distribution, i.e., 
\begin{align*}
    \forall k\in [K], \quad X\vert k \sim \cN(\meang_k^*, \vI_d)=(2\pi)^{-\frac{d}{2}}e^{-\frac{1}{2}\norm{\vx-\meang_k^*}_2^2},
\end{align*}
with a mean vector $\meang_k^*$ and an identity covariance matrix $\vI_d\in \R^{d\times d}$. 
We assume each mean vector has two parts: the first $d_1$ entries $\meang_k^*[1\!:\!d_1]$ represent the source-specific feature which is potentially different for each source, and the remaining entries $\meang_k^*[d_1\!+\!1\!:\!d]$ represent the shared feature which is identical across all sources.
Corresponding to the general formulation in \cref{sec:formulation_for_conditional_generative_modeling}, we denote
\begin{align*}
\phi_k \!\coloneqq\! \meang_k^*[1:d_1], 
\psi \!\coloneqq\! \meang_1^*[d_1\!+\!1\!:\!d]\!=\!\cdots\!=\!\meang_K^*[d_1\!+\!1\!:\!d],
\end{align*}
and the conditional distribution is parameterized as
\begin{align}
\label{eq:conditional_density_gaussian}
    p_{\phi_k,\psi}(\vx\vert k)=(2\pi)^{-\frac{d}{2}}e^{-\frac{1}{2}\norm{\vx-(\phi_k,\psi)}_2^2}.
\end{align}

\paragraph{Statistical guarantee of the average TV error.} 
In this formulation, the conditional MLE in $\cP_{X\vert Y}^{\multi}$ under multi-source training leads to the following result.

\begin{theorem}[Average TV error bound for conditional Gaussian estimation under multi-source training, proof in \cref{app:proof_thm:tv_upper_gaussian}]
\label{thm:tv_upper_gaussian}
Let $\hat{p}_{X\vert Y}^{\multi}$ be the likelihood maximizer defined in \cref{eq:multi_mle_solution} given $\cP_{X\vert Y}^{\multi}$ with conditional distributions as in \cref{eq:conditional_density_gaussian}. 
Suppose $\Phi = [-B,B]^{d_1}$, $\Psi = [-B,B]^{d-d_1}$ with constant $B>0$, and $\phi_k^*\in \Phi$, $\psi^*\in \Psi$. Then, for any $0<\delta\leq 1/2$, it holds with probability at least $1-\delta$ that 
\begin{align*}
    \rtv(\hat{p}_{X\vert Y}^{\multi})=\tilde{\cO}\paren*{\sqrt{\frac{(K-1)d_1+d}{n}}}.
\end{align*}
\end{theorem}
In contrast, single-source training results in an error of $\rtv(\hat{p}_{X\vert Y}^{\single})=\tilde{\cO}\paren*{\sqrt{{Kd}/{n}}}$, with a formal result provided 
in Theorem~\ref{thm:tv_upper_gaussian_single}.  
The advantage of multi-source learning can be quantified by the ratio of error bounds:  
${\sqrt{\frac{(K-1)d_1+d}{Kd}}}=\sqrt{1 - \frac{K-1}{K}\frac{d-d_1}{d}}.$ 
Denoting $\beta_{\mathrm{sim}}\coloneqq\frac{d-d_1}{d}$, this general form of ratio $\sqrt{1 - \frac{K-1}{K}\beta_{\mathrm{sim}}}$ applies across  \cref{sec:instantiation_arm} and \cref{sec:instantiation_ebm}, decreasing with both the number of sources $K$ and source similarity $\beta_{\mathrm{sim}}$. 

Specifically, as $K$ increases from $1$ to $\infty$, the ratio decreases from $1$ to $\sqrt{1 - \beta_{\mathrm{sim}}}$, and as $\beta_{\mathrm{sim}}$ increases from $0$ (completely dissimilar distributions) to $1$ (completely identical distributions), it decreases from $1$ to $\sqrt{1/K}$, reflecting a transition from no asymptotic gain to a constant improvement. 
This highlights that the number of sources and distribution similarity enhance the benefits of multi-source training. Empirical results in~\cref{sec:real-world_experiment} confirm this trend.

% In the best case, the error improves by a factor of $\sqrt{1/K}$, while in the worst case, it remains no worse than single-source training. 


\subsection{Conditional ARMs on discrete distributions}
\label{sec:instantiation_arm}


For deep generative models, our formulations are based on multilayer perceptrons (MLPs), a fundamental network component, with potential extensions to Transformers and convolution networks with existing literature~\cite{lin2019generalization,ledent2021norm, shen2021non,hu2024statistical,trauger2024sequence, jiao2024convergence}. We formally define MLPs mainly following notations in ~\citet{oko_2023_icml_diffusionminimax}

\begin{definition}[Class of MLPs]
\label{def:class_of_nn}
A class of MLPs $\cF(L,W,S,B)$ with depth $L$, width $W$, sparsity $S$, norm $B$, and element-wise ReLU activation that $\relu(x)=0\vee x$ is defined as 
$\cF(L,W,S,B)\!\coloneqq\!\{\vf(\vx)\!=\!(\vA^{(L)}\relu(\cdot)\!+\!\vb^{(L)})\circ\cdots\circ(\vA^{(1)}\vx\!+\!\vb^{(1)})\!:\! \{(\vA^{(l)},\vb^{(l)})\}_{l=1}^L\!\in\!\cW(L,W,S,B)\}$, where parameter space $\cW(L,W,S,B)$ is defined by $\cW(L,W,S,B)\coloneqq \{\{(\vA^{(l)},\vb^{(l)})\}_{l=1}^L: \vA^{(l)}\in\R^{W_l\times W_{l-1}}, \vb^{(l)}\in\R^{W_l}, \max_l W_l\leq W, \sum_{l=1}^L(\norm{\vA^{(l)}}_0+\norm{\vb^{(l)}}_0)\leq S, \max_l\norm{\vA^{(l)}}_{\infty}\vee \norm{\vb^{(l)}}_{\infty}\leq B\}$. 
\end{definition}

We now present the formulation for ARMs, which can be viewed as an extension of~\citet{uria_2016_jmlr_nade}.

\paragraph{Probabilistic modeling with autoregression.}
Consider a common data scenario for the natural language where $X$ represents a $D$-length text in $[M]^D$. Each dimension of $X$ is an integer token following an $M$-categorical distribution with $M$ being the vocabulary size. 
Adopting the autoregressive approach of probabilistic modeling, conditional distribution $p_{X \vert Y}(\vx\vert y)$ is factorized using the chain rule as: 
\begin{align*}
    p(\vx\vert y)&=p\paren*{x_1\vert y}\cdots p\paren*{x_D\vert \vx_{<D},y}\\
    &=p\paren*{x_1;\vparm(y)}\cdots p\paren*{x_D;\vparm(\vx_{<D},y)}.
\end{align*}
We omit the subscripts for notation simplicity.
Here, for any $d\in[D]$, $\vparm(x_{<d},y)$ is the distribution parameter for $X_d$ given $X_{<d}, Y\!=\!\vx_{<d}, y$ that 
$$p\paren*{x_d=m\vert \vx_{<d},y}=\vparm(\vx_{<d},y)[m],$$ 
satisfying $\vparm(\vx_{<d},y)\in\R_{+}^M$ and $\sum_{m=1}^M\vparm(\vx_{<d},y)[m]=1$. 

\paragraph{Distribution estimation via neural network.}
Aligning with common practices, we suppose the distribution parameter vector is estimated with $\vparm_{\pnn}(x_{<d},y)$ using a shared neural network parameterized by $\pnn$ across all dimensions. The network comprises an embedding layer, an encoding layer, an MLP block, and a softmax output layer. 
% Below, we specify this architecture in detail and relate it to the general formulation introduced in \cref{sec:formulation_for_conditional_generative_modeling}.

Specifically, we first look up $\vx$ and $y$ in two embedding matrices $\vV_X\in[0,1]^{M\times \de}$ and $\vV_Y\in[0,1]^{K\times \de}$, then stack the embeddings to get
$$\vE_{\vV_Y,\vV_X}(\vx,y)=\begin{bmatrix} \vV_Y[y,:] \\ \vV_X[x_1,:]\\ \vdots\\\vV_X[x_{D-1},:]  \end{bmatrix}\in[0,1]^{D\times\de},$$
where the last dimension of $x$ is excluded since it is not used when estimating the distribution.

Subsequently, we encode each embedding by a linear transformation with parameters $\vA_0\in \R^{D\times \de}$, $\vb_0\in\R^D$ and normalize the output with an element-wise sigmoid function $\sigma(x)=\frac{1}{1+e^{-x}}$ as
\begin{align*}
    &\vv_{\vA_0,\vb_0}(\vE_{\vV_Y,\vV_X}(\vx,y)) \\
    &=\begin{bmatrix} \sigma\paren*{\vA_0[1,:]\vV_Y[y,:]^\top \!+\!\vb_0[1]} \\\vdots \\ \sigma\paren*{\vA_0[D,:]\vV_X[x_{D-1},:]^\top\!+\!\vb_0[D]} \end{bmatrix}
    \in [0,1]^D.
\end{align*}

To ensure no components related to $\vx_{\geq d}$ is seen when estimating the conditional probability for $x_d$, we mask $\vv_{\vA_0,\vb_0}$ using a $(D-d)$-dimensional zero vector $\bm{0}_{D-d}$ as
$$\vv_{\vA_0,\vb_0}^{\backslash \bm{0}_{D-d}}\coloneqq \begin{bmatrix} \vv_{\vA_0,\vb_0}[1:d]^\top \ \bm{0}_{D-d}^\top\end{bmatrix}^\top.$$ 
Then we calculate the distribution parameter vector by an MLP $\vf_{\pmlp}\!\in\!\cF(L,W,S,B)$ with $W_0 \!=\! D$ and $W_L\!=\!M$, followed by a softmax layer as
\begin{align*}
    \vparm_{\pnn}(\vx_{<d},\!y)\!=\!\mathrm{softmax}\paren*{\!\vf_{\pmlp}\paren*{\!\vv_{\vA_0,\vb_0}^{\backslash \bm{0}_{D-d}}\!\paren*{\vE_{\vV_Y,\vV_X}\!(\vx,y)\!}\!}\!\!},
\end{align*}
% where $\pnn\coloneqq \{\vV_Y,\vV_X,\vA_0,\vb_0,\pmlp\}$ denotes the parameterization of $\vparm$.
This leads to conditional distribution as
\begin{align}
\label{eq:conditional_density_ar}
    p_{\pnn}\paren*{\vx\vert y}=p\paren*{x_1; \vparm_{\pnn}(y)}\cdots p\paren*{x_D; \vparm_{\pnn}(\vx_{<D},y)}.
\end{align}

When training such an ARM, each row of $\vV_Y$ is only optimized on data with the corresponding condition, while parameters in $\vV_X,\vA_0,\vb_0$, and $\pmlp$ are optimized on data with all conditions. 
That means $\vV_Y[k,:]$ serves as the source-specific parameter, while other parameters are shared across all sources. 
Corresponding to the general formulation in \cref{sec:formulation_for_conditional_generative_modeling}, we denote
\begin{align*}
    \phi_k\coloneqq \vV_Y[k,:], \ \text{and }
    \psi\coloneqq \{\vV_X,\vA_0,\vb_0,\pmlp\}.
\end{align*}


\paragraph{Statistical guarantee of the average TV error.}
In this formulation, the conditional MLE in $\cP_{X\vert Y}^{\multi}$ under multi-source training leads to the following result.

\begin{theorem}[Average TV error bound for ARMs under multi-source training, proof in \cref{app:proof_of_ar}]
\label{thm:tv_upper_ar}
Let $\hat{p}_{X\vert Y}^{\multi}$ be the likelihood maximizer defined in \cref{eq:multi_mle_solution} given $\cP_{X\vert Y}^{\multi}$ with conditional distributions as in \cref{eq:conditional_density_ar}. 
Suppose $\Phi \!=\! [0,1]^{\de}$, $\Psi \!=\! [0,1]^{M\times \de}\!\times\! [-B,B]^{D\times\de}\!\times\![-B,B]^{D}\!\times\! \cW(L,W,S,B)$ with constants $L,W,S,B>0$, and $\phi_k^*\in \Phi$, $\psi^*\in \Psi$. Then, for any $0<\delta\leq 1/2$, it holds with probability at least $1-\delta$ that 
\begin{align*}
\rtv(\hat{p}_{X\vert Y}^{\multi})=\tilde{\cO}\paren*{\sqrt{\frac{L\paren*{S+D+(D+M+K)\de}}{n}}}.
\end{align*}

\end{theorem}


In contrast, single-source training results in an error of $\rtv(\hat{p}_{X\vert Y}^{\single})=\tilde{\cO}\paren*{\sqrt{{KL(S\!+\!D\!+\!(D\!+\!M\!+\!1)\de)}/{n}}}$ with a formal result provided in Theorem~\ref{thm:tvbound_arm_single}.  
The advantage of multi-source learning is quantified by the ratio of error bounds:  
$\sqrt{\frac{L(S+D+(D+M+K)\de)}{KL(S+D+(D+M+1)\de)}} = \sqrt{1 \!-\! \frac{K-1}{K} \beta_{\mathrm{sim}} }$, 
% Here, $S\!+\!D\!+\!(D\!+\!M)\de$ approximates the number of shared parameters in $\psi$, while $\de$ measures the number of source-specific parameters in $\phi_k$. 
where the term  
$\beta_{\mathrm{sim}} \coloneqq\frac{S+D+(D+M)\de}{S+D+(D+M+K)\de+\de} \in [0,1]$  
quantifies source distribution similarity based on the proportion of shared parameters. This ratio follows the same pattern discussed in \cref{sec:instantiate_conditional_gaussian} where the number of sources $K$ and the distribution similarity $\beta_{\mathrm{sim}}$ are two key factors improving the advantage of multi-source training.



\subsection{Conditional EBMs on continuous distributions}
\label{sec:instantiation_ebm}

In this section, we study distribution estimation for conditional EBMs, a flexible probabilistic modeling approach on continuous data. Our formulation follows~\citet{du_2019_nips_ebm} with simplified neural network architecture. 


\paragraph{Probabilistic modeling with energy function.}
Consider a common scenario with natural image $X$ flattened and normalized in $[0,1]^D$. 
The conditional distribution $p_{X \vert Y}(\vx\vert y)$ is factorized with an energy function $u(\vx\vert y)$ as: 
\begin{align*}
    p(\vx\vert y)=\frac{e^{-u(\vx\vert y)}}{\int_{\sX}e^{-u(\vs\vert y)}d\vs}.
\end{align*}

\paragraph{Distribution estimation via neural network.}
We suppose the energy function is estimated with $u_{\pnn}(\vx\vert y)$ using a neural network parameterized by $\pnn$, which comprises a condition embedding layer and an energy-estimating MLP. 

Specifically, we first look up $y$ in a condition embedding matrix $\vV\in[0,1]^{K\times \de}$ and concat the embedding with $\vx$ 
$$\ve_{\vV}(\vx,y)=\begin{bmatrix}  \vx \\ \vV[y,:] \end{bmatrix}\in[0,1]^{D+\de}.$$
% \zcy{$\vV[y,:]^\top$?}
Then we use an MLP $f_{\pmlp}\in\cF(L,W,S,B)$ with $W_0=D+\de$ and $ W_L=1$ to estimate the energy as
\begin{align*}
    u_{\pnn}(\vx\vert y)=f_{\pmlp}\paren*{\ve_{\vV}(\vx,y)},
\end{align*}
where $\pnn\coloneqq\{\vV,\pmlp\}$. 
This leads to a conditional distribution as 
\begin{align}
\label{eq:conditional_density_ebm}
    p_{\pnn}(\vx\vert y)=\frac{e^{-u_{\pnn}(\vx\vert y)}}{\int_{\sX}e^{-u_{\pnn}(\vs\vert y)}d\vs}.
\end{align}

When training such an EBM, each row of $\vV$ is only optimized on data with the corresponding condition, while $\pmlp$ is optimized on data with all conditions. 
That means $\vV[k,:]$ serves as the source-specific parameter and $\pmlp$ is shared across all sources. 
Corresponding to the general formulation in \cref{sec:formulation_for_conditional_generative_modeling}, we denote
\begin{align*}
    \phi_k\coloneqq \vV[k,:], \ \text{and }
    \psi\coloneqq \pmlp.
\end{align*}


\paragraph{Statistical guarantee of the average TV error.}
In this formulation, the conditional MLE in $\cP_{X\vert Y}^{\multi}$ under multi-source training leads to the following result.

\begin{theorem}[Average TV error bound for EBMs under multi-source training, Proof in \cref{app:proof_of_ebm}]
\label{thm:tv_upper_ebm}
Let $\hat{p}_{X\vert Y}^{\multi}$ be the likelihood maximizer defined in \cref{eq:multi_mle_solution} given $\cP_{X\vert Y}^{\multi}$ with conditional distributions in \cref{eq:conditional_density_ebm}. 
Suppose $\Phi\!=\![0,1]^{\de}$ and $\Psi \!=\! \cW(L,W,S,B)$ with constants $L,W,S,B>0$ and assume $\phi_k^*\in \Phi$, $\psi^*\in \Psi$. Then, for any $0\!<\!\delta\!\leq\! 1/2$, it holds with probability at least $1\!-\!\delta$ that
\begin{align*}
\rtv(\hat{p}_{X\vert Y}^{\multi})=\tilde{\cO}\paren*{\sqrt{\frac{L\paren*{S+K\de}}{n}}}.
\end{align*}
\end{theorem}

In contrast, single-source training results in an error of $\rtv(\hat{p}_{X\vert Y}^{\single})=\tilde{\cO}\paren*{\sqrt{{LK\paren*{S+\de}}/{n}}}$ with a formal proof provided in Theorem~\ref{thm:tvbound_ebm_single}.  
The advantage of multi-source learning is quantified by the ratio of error bounds:  
$\sqrt{\frac{L\paren*{S+K\de}}{LK\paren*{S+\de}}} = \sqrt{1 - \frac{K-1}{K} \beta_{\mathrm{sim}}}$,  
% Here, $S$ measures the number of shared parameters in $\psi$, while $\de$ measures the number of source-specific parameters in $\phi_k$. 
where  
$\beta_{\mathrm{sim}} \coloneqq \frac{S}{S+\de} \in [0,1]$  
quantifies source distribution similarity based on the proportion of shared parameters.  
Similar to the former two cases, the number of sources $K$ and the distribution similarity $\beta_{\mathrm{sim}}$ improve the advantage of multi-source training.

\begin{figure*}[ht]
    \centering
    \begin{minipage}[t]{0.33\linewidth} 
        % \includesvg[height=0.15\textheight]{figs/gaussian_K.svg}
        \includegraphics[height=0.15\textheight]{figs/gaussian_K.png}
        
        \label{fig:gaussian_K}
        % \centerline{Image}
    \end{minipage}
    \hspace{-5pt}
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[height=0.15\textheight]{figs/gaussian_n.png}
        \label{fig:gaussian_n}
        % \centerline{Image}
    \end{minipage}
    \hspace{-5pt}
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[height=0.15\textheight]{figs/gaussian_sim.png}
        \label{fig:gaussian_sim}
        % \centerline{Image}
    \end{minipage}
    \caption{Simulation results for conditional Gaussian estimation.  
    Empirical values (solid lines) correspond to the left vertical axis, while theoretical values (dashed lines) correspond to the right. \textcolor{orange}{Single-source} results are shown in orange, and \textcolor{xkcdLightGreen}{multi-source} results in green. 
    The matched orders of empirical errors and theoretical upper bounds support the validity of results in \cref{sec:instantiate_conditional_gaussian}.}
    \label{fig:gaussian_simulation_results}
\end{figure*}

\section{Experiments}
\label{sec:experiments}

In this section, simulations and real-world experiments are conducted to verify our theoretical results in Section~\ref{sec:general_guarantee_for_multi} and~\ref{sec:instantiations}.

\subsection{Simulations on conditional Gaussian estimation}
\label{sec:simulation}

In this part, we aim to examine the tightness of the derived upper bound that  $\rtv(\hat{p}_{X\vert Y}^\multi)=\tilde{\cO}\paren{\sqrt{\frac{(K-1)d_1+d}{n}}}$ in Theorem~\ref{thm:tv_upper_gaussian} and $\rtv(\hat{p}_{X\vert Y}^\single)=\tilde{\cO}\paren{\sqrt{\frac{Kd}{n}}}$ in Theorem~\ref{thm:tv_upper_gaussian_single}.

The number of sources $K$, sample size $n$, and the similarity factor $\beta_{\mathrm{sim}}\in [0,1]$ are key parameters. 
In all of our simulations, we fix data dimension $d=10$ and $p_Y^*(k) = {1}/{K}$ all $k \in [K]$. 
The dissimilar
dimension $d_1 = d - \lfloor\beta_{\mathrm{sim}}d\rfloor$.
We set the source-specific feature as $\phi_k = k\vone \in \R^{d_1}$ and the shared feature as $\psi = \vzero \in \R^{d-d_1}$. 
Under the setting of \cref{sec:instantiate_conditional_gaussian}, conditional MLE has analytical solutions: under multi-source training, we have
% \begin{equation}
% \label{eqn:mle_gaussian_multi}
$$\textstyle{\hat{\phi}_k ={\sum_{y_i=k} \vx_i[1:d_1]}/{n_k}, \ \hat{\psi} = {\sum_{i=1}^n \vx_i[d_1+1:d]}/{n}}, $$
% \end{equation}
and under single-source training, we have
% \begin{equation}
% \label{eqn:mle_gaussian_single}
$$\textstyle{\hat{\phi}_k \!=\! {\sum_{y_i=k} \vx_i[1\!:\!d_1]}/{n_k}, \ \hat{\psi}_k \!=\! {\sum_{y_i=k} \vx_i[d_1+1\!:\!d]}/{n_k}}.$$
% \end{equation}

For evaluation, we randomly sample $n^{\mathrm{test}}=500$ data points according to the true joint distribution $p^*_{X,Y}$. 
Empirically, we approximate the true TV distance by using the Monte Carlo method based on the test set, which can be written formally as
\begin{align*}
\rtv(\hat{p}_{X\vert Y}\!) \!\approx\! \frac{1}{2n^{\mathrm{test}}} \!\sum_{i=1}^{n^{\mathrm{test}}} \abs*{ \frac{\hat{p}_{X\vert Y}(\vx_i \vert y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)} \!-\! 1} \!=\! \rtv^{\mathrm{em}}(\hat{p}_{X\vert Y}\!).
\end{align*}

To eliminate the randomness, we average over 5 random runs for each simulation and report the mean results.

\paragraph{Order of the average TV error about $K$.} We range the number of sources $K$ in $[1,3,5,10,15]$ with fixed sample size $n=500$ and similarity factor $\beta_{\mathrm{sim}}=0.5$.  
We display the empirical average TV error for each $K$ in Figure~\ref{fig:gaussian_simulation_results}(a), with $\rtv^{\mathrm{em}}(\hat{p}_{X\vert Y}^\multi)$ colored in green and $\rtv^{\mathrm{em}}(\hat{p}_{X\vert Y}^\single)$ colored in orange. 
Ignoring the influence of constants, it shows a good alignment between empirical errors (in solid lines) and theoretical upper bounds (in dashed lines), both scaling as $\tilde{\cO}(\sqrt{K})$. 

\paragraph{Order of the average TV error about $n$.}
We range sample size $n$ in $[100,300,500,1000,5000]$ with fixed number of sources $K=5$ and similarity factor $\beta_{\mathrm{sim}}=0.5$.
We display the empirical error for each $n$ in Figure~\ref{fig:gaussian_simulation_results}(b), with $\rtv^{\mathrm{em}}(\hat{p}_{X\vert Y}^\multi\!)$ colored in green and $\rtv^{\mathrm{em}}(\hat{p}_{X\vert Y}^\single\!)$ colored in orange. 
Ignoring the influence of constants, it shows that the orders of empirical error about $n$ match well with the theoretical upper bounds which scale as $\tilde{\cO}(1/\sqrt{n})$. 

\paragraph{Order of the average TV error about $\beta_{\mathrm{sim}}$.}
We range similarity factor $\beta_{\mathrm{sim}}$ in $[0,0.3,0.5,0.7,1]$ with fixed sample size $n=500$ and number of data sources $K=5$.
We display the empirical average TV error for each $\beta_{\mathrm{sim}}$ in Figure~\ref{fig:gaussian_simulation_results}(c) to observe how similarity factor $\beta_{\mathrm{sim}}$ impacts the advantage of multi-source training. 
Concretely, as predicted by the theoretical bounds, the changing of $\beta_{\mathrm{sim}}$ will not influence the performance of single-source training but will decrease the error of multi-source training in the order of $\tilde{\cO}(\sqrt{d_1}) = \tilde{\cO}(\sqrt{1-\beta_{\mathrm{sim}}})$.
The results show that the theoretical bounds predict the empirical performance well. 

To sum up, our simulations verify the validity of our theoretical bounds in \cref{sec:instantiate_conditional_gaussian}.
Moreover, in all experiments, $\rtv^{\mathrm{em}}(\hat{p}_{X\vert Y}^\multi)$ is consistently smaller than $\rtv^{\mathrm{em}}(\hat{p}_{X\vert Y}^\single)$, supporting our results in \cref{sec:general_guarantee_for_multi}


\subsection{Real-world experiments on diffusion models}
\label{sec:real-world_experiment}
In this section, we conduct experiments on diffusion models to validate our theoretical findings in real-world scenarios from two aspects: 
\begin{enumerate*}[(1)]
    \item We empirically compare multi-source and single-source training on conditional diffusion models and evaluate their performance to validate the guaranteed advantage of multi-source training against single-source training proved in \cref{sec:general_guarantee_for_multi}.
    \item We investigate the trend of this advantage about key factors---the number of sources and distribution similarity---as discussed in \cref{sec:instantiations}. 
\end{enumerate*}


\paragraph{Experimental settings.}
We train class-conditional diffusion models following EDM2~\cite{karras2024analyzing} at
256$\times$256 resolution on the selected classes from the ILSVRC2012 training set~\cite{ILSVRC15}, which is a subset of ImageNet~\cite{imagenet_cvpr09} containing 1.28M natural images from 1000 classes, each annotated with an integer class label from 1 to 1000. 
In our experiments, we treat each class as a distinct data source.
To control similarity among data sources, we manually design two levels of distribution similarity based on the semantic hierarchy of ImageNet~\cite{imagenet_statistics,imagenet_hierarchy} as shown in \cref{fig:hierachy} in Appendix~\ref{app:supplementary_material_for_real-world_experements} along with other experimental details.

For each controlled experiment comparing multi-source and single-source training, we fix $K$ target classes within one similarity level $\mathrm{Sim}$ and train the models on a dataset $S$ consisting of $N$ examples per class.
Under multi-source training, we train a single conditional diffusion model for all $K$ classes jointly. Under single-source training, we train $K$ separate conditional diffusion models, one for each class. Please refer to \cref{sec:formulation_for_conditional_generative_modeling} for the formal formulation of these two strategies.
We set each factor with two possible values: the number of classes $K$ in 3 or 10, distribution similarity  $\mathrm{Sim}$ in 1 or 2, and the sample size per class $N$ in 500 or 1000.
This results in a total of 8 sets of experiments comparing multi-source and single-source training.

We evaluate model performance using the average FrÃ©chet Inception Distance~\cite{heusel2017gans} (FID, a widely used metric for image generation quality) across all conditions to assess the overall conditional generation performance. Results are displayed in Table~\ref{tab:real-world_fid}. 
Specifically, for multi-source training, we compute the FID for each class and take the average over all $K$ classes. 
For single-source training, we compute the FID for each of the $K$ separately trained models on their respective classes and calculate the average. 
Relative advantage of multi-source training is measured by $\frac{\text{Avg. FID (Single)}-\text{Avg. FID (Multi)}}{\text{Avg. FID (Single)}}$ as displayed in Figure~\ref{fig:figure2}.



\paragraph{Experimental results}

In the following, we interpret the results sequentially from the view of our theoretical findings.
% in \cref{sec:general_guarantee_for_multi} and \cref{sec:instantiations}.

From Table~\ref{tab:real-world_fid}, we observe that under different amounts of classes $K$, similarity level $\mathrm{Sim}$, and per-class sample size $N$, multi-source training generally achieves lower average FID than that of single-source training, which is consistent with our theoretical guarantees derived in \cref{sec:general_guarantee_for_multi}, 


From Figure~\ref{fig:figure2}, we observe that for any fixed similarity level $\mathrm{Sim}$ and per-class sample size $N$, the relative advantage of multi-sources training with a larger $K$ (the green bars) is larger than that with a smaller $K$ (the nearby orange bars). 
Additionally, for any fixed $K$ and $N$, the relative advantage of multi-sources training with a larger distribution similarity is larger than that with a smaller distribution similarity (as shown through the dashed lines). 
These results support our theoretical insights in \cref{sec:instantiations} that the number of sources and similarity among source distributions improves the advantage of multi-source training.

\begin{table}[t]
% \vskip -0.1in
\begin{center}
        \centering
        \caption{Average FID for single-source and multi-source training. 
        Under different amounts of classes $K$, similarity level $\mathrm{Sim}$, and per-class sample size $N$, multi-source training generally achieves lower average FID than that of single-source training, which is consistent with our theoretical guarantees derived in \cref{sec:general_guarantee_for_multi}.
        }
        \vskip 0.15in
        \label{tab:real-world_fid}
        \begin{tabular}{ccccc}
            \toprule
             $N$&$\mathrm{Sim}$ & $K$ & \makecell{Avg. FID $\downarrow$ \\ (Single)} & \makecell{Avg. FID $\downarrow$\\ (Multi)}\\
            \midrule
            \multirow{4}{*}{500} & \multirow{2}{*}{1} & 3  &  30.15& \textbf{29.82}\\
                                 &                    & 10 & 30.16& \textbf{29.36}\\
                                 & \multirow{2}{*}{2} & 3  & 32.87& \textbf{31.16}\\
                                 &                    & 10 & 29.96& \textbf{28.83}\\
            \midrule
            \multirow{4}{*}{1000} & \multirow{2}{*}{1} & 3  & 27.76 & \textbf{26.86} \\
                                  &                    & 10 & 27.46  & \textbf{25.01} \\

                                  & \multirow{2}{*}{2} & 3  & 30.20 & \textbf{28.31} \\
                                  &                    & 10 & 29.60 & \textbf{26.70} \\
            \bottomrule
        \end{tabular}
\end{center}
\vskip -0.2in
\end{table}


\section{Other related works}
\label{sec:related_works}

\paragraph{Distribution estimation guarantee for MLE.}
Classical approaches investigate distribution estimation for MLE in Hellinger distance based on the bracketing number and the uniform law of large numbers from empirical process theory~\cite{wong_1995_aos_likelihood-ratios,geer_2000_book_empiricalprocess}, 
which yields high-probability bounds of similar order as \cref{thm:TV_upper_of_conditional_MLE}.
\citet{ge_2024_iclr_unsupervised} extend the analysis to derive TV error bound under the realizable assumption.
% where TV distance is a widely used metric for measuring the distribution estimation for generative models~\cite{oko_2023_icml_diffusionminimax,chen_2023_icml_diffusion,chen_2023_iclr_samplingeasy}. 
We further adapt their techniques to conditional generative modeling by introducing the upper bracketing number to quantify the complexity of conditional distribution space in Definition~\ref{def:upper_bracketing_number} and modify the proofs to handle conditional MLE in \cref{app:proof_thm:TV_upper_of_conditional_MLE}.


\paragraph{Theory on multi-task learning.}
Multi-task learning is a well-studied topic in supervised learning~\cite{caruana_1997_ml_multitask,baxter_2000_jair_inductivebias}.
It typically benefits from similarities across tasks, sharing some commonality with multi-source training. 
However, theoretical analyses in supervised learning often assume a bounded objective~\cite{ben-david_2008_ml_notiontaskrelatedness,maurer_2016_jmlr_multitask,tripuraneni_2020_nips_transfer}, whereas our MLE analysis imposes no such restriction.


\paragraph{Advanced theory on generative models.} 
Among generative models based on (approximate) MLE~\cite{lecun_2006_tutorial_ebm,uria_2016_jmlr_nade,Ho_2020_nips_ddpm}, diffusion models have been extensively studied theoretically on its score approximation, sampling behavior, and distribution estimation~\cite{oko_2023_icml_diffusionminimax,chen_2023_icml_diffusion,chen_2023_iclr_samplingeasy,fu_2024_arxiv_conditional-diffusion}. 
% A recent work~\cite{fu_2024_arxiv_conditional-diffusion} establishes an expectation bound on the average TV error for conditional diffusion models from a nonparametric statistical perspective. 
This paper focuses on distribution estimation for general conditional generative modeling. 
Incorporating existing literature could be a promising direction for future work.


\begin{figure}[t]
\vskip 0.1in
\begin{center}
    \centering
    \hspace{-18pt}
    \begin{minipage}{0.5\linewidth} 
        \centering
        \includegraphics[height=0.18\textheight]{figs/diffusion_n500.pdf}
    \end{minipage}
    \hspace{-10pt}
    \begin{minipage}{0.5\linewidth} 
        \centering
        \includegraphics[height=0.185\textheight]{figs/diffusion_n1000.pdf}
    \end{minipage}
    \caption{Relative advantage of multi-source training.
    For any fixed similarity level $\mathrm{Sim}$ and per-class sample size $N$, a \textcolor{xkcdLightGreen}{larger $K$} yields a greater FID improvement than a \textcolor{orange}{smaller $K$}.  
    For any fixed $K$ and $N$, higher distribution similarity leads to greater FID improvement (illustrated by dashed lines).  
    These results support the theoretical findings in \cref{sec:instantiations}.
    }
    \label{fig:figure2}
\end{center}
\vskip -0.2in
\end{figure}


\section{Conclusion and discussion}

This paper provides the first attempt to rigorously analyze the conditional generative modeling on multiple data sources from a distribution estimation perspective.
In particular, we establish a general estimation error bound in average TV distance under the realizable assumption based on the bracketing number of the conditional distribution space.
When source distributions share parametric similarities, multi-source training has a provable advantage against single-source training by reducing the bracketing number.
We further instantiate the general theory on three specific models to obtain concrete error bounds. 
To achieve this, novel bracketing number bounds for ARMs and EBMs are established. 
The results show that the number of data sources and the similarity between source distributions enhance the benefits of multi-source training. 
Simulations and real-world experiments support our theoretical findings. 


Our theoretical setting differs from practice in some aspects, e.g., language models have no explicit conditions, and image generation models are commonly conditioned on descriptive text involving multiple conditions.
% factors rather than a single source. 
However, our abstraction provides a simplified framework that preserves the core properties of multi-source training and isolates how individual source distributions are learned.
Moreover, recent studies suggest adding source labels, such as domain names, at the start of training text for language models can enhance performance~\cite{allen-zhuL_2024_icml_physicsofllm3-3,gao_2025_arxiv_metadata}, which may become a standard practice in the future. 


% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of  Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. 
 

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Proofs for \cref{sec:general_guarantee_for_multi}}
\label{app:proofs_for_general_guarantee_for_multi}

\subsection{Proof of \cref{thm:TV_upper_of_conditional_MLE}}
\label{app:proof_thm:TV_upper_of_conditional_MLE}

\begin{proof}[Proof of \cref{thm:TV_upper_of_conditional_MLE}]

This theorem applies to both discrete and continuous random variables, while we use integration notation in the proof for generality. 
In the following, we first present an elementary inequality (in \cref{eq:upper_bound_of_one_minus_sqrt_of_product}) which serves as a toolkit for the subsequent derivations. Then we decompose the TV distance and derive its complexity-based upper bound (in \cref{eq:upper_bound_of_expected_TV_error_with_general_epsilon}) using the former inequality. Finally, after specifying certain constants in this upper bound, a clearer order w.r.t. $n$ is revealed (in \cref{eq:upper_bound_of_expected_TV_error}). 


\paragraph{Intermediate result induced by union bound.}
Let $\epsilon$ be a real number that $\epsilon>0$ and $\sfp$ be an integer that $1\leq \sfp\leq \infty$. Let $\cB$ be an $\epsilon$-upper bracket of $\cP_{X\vert Y}$ w.r.t. $L^1(\sX)$ such that $\abs{\cB}=\br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}$. 

According to the minimum cardinality requirement, we obtain a proposition of $\cB$ that: for any $p^\prime\in\cB$, $p^\prime(\vx,y)\geq 0$ on $\sX \times \sY$. 
Let's first consider $\prod_{i=1}^{n}\sqrt{\frac{p^\prime(\vx_i, y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)}}$ as a random variable on $S$, where we suppose $p^*_{X, Y}(\vx_i, y_i) > 0$ since $(\vx_i, y_i)$ are sampled from $p^*_{X, Y}$ and thus $p^*_{X|Y}(\vx_i | y_i)\neq 0$.
By applying the \emph{Markov inequality}, we have: given any $0<\delta^\prime<1$,  
\begin{align}
\label{eq:markov_for_fix_p_prime}
\PR_{S}\paren*{\prod_{i=1}^{n}\sqrt{\frac{p^\prime(\vx_i, y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)}}\geq\frac{1}{\delta^\prime}\E_{S}\bracket*{\prod_{i=1}^{n}\sqrt{\frac{p^\prime(\vx_i, y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)}}}} \leq \delta^\prime.
\end{align}
Applying the \emph{union bound} on all $p^\prime\in\cB$, we further have:
\begin{align}
& \PR_{S}\paren*{\forall p^\prime\in\cB, \prod_{i=1}^{n}\sqrt{\frac{p^\prime(\vx_i, y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)}}<\frac{1}{\delta^\prime}\E_{S}\bracket*{\prod_{i=1}^{n}\sqrt{\frac{p^\prime(\vx_i, y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)}}}} \notag \\
= & 1-\PR_{S}\paren*{\exists p^\prime\in\cB, \prod_{i=1}^{n}\sqrt{\frac{p^\prime(\vx_i, y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)}}\geq\frac{1}{\delta^\prime}\E_{S}\bracket*{\prod_{i=1}^{n}\sqrt{\frac{p^\prime(\vx_i, y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)}}}} \notag \\
= & 1-\PR_{S}\paren*{\bigcup_{p^\prime\in\cB}\curl*{ \prod_{i=1}^{n}\sqrt{\frac{p^\prime(\vx_i, y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)}}\geq\frac{1}{\delta^\prime}\E_{S}\bracket*{\prod_{i=1}^{n}\sqrt{\frac{p^\prime(\vx_i, y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)}}}}}\notag \\
\geq & 1-\sum_{p^\prime\in\cB}\PR_{S}\paren*{ \prod_{i=1}^{n}\sqrt{\frac{p^\prime(\vx_i, y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)}}\geq\frac{1}{\delta^\prime}\E_{S}\bracket*{\prod_{i=1}^{n}\sqrt{\frac{p^\prime(\vx_i, y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)}}}} \tag{by \emph{union bound}} \\
\geq & 1- \br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}\delta^\prime. \tag{by \cref{eq:markov_for_fix_p_prime}}
\end{align}
By denoting that $\delta \coloneqq \br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}\delta^\prime$, we have: it holds with probability at least $1-\delta$ that for all $p^\prime\in\cB$,
\begin{align*}
\prod_{i=1}^{n}\sqrt{\frac{p^\prime(\vx_i, y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)}}<\frac{\br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}}{\delta} \E_{S}\bracket*{\prod_{i=1}^{n}\sqrt{\frac{p^\prime(\vx_i, y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)}}}.
\end{align*}

Taking logarithms at both sides, we have 
\begin{align}
\frac{1}{2}\sum_{i=1}^{n} \log\frac{p^\prime(\vx_i, y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)}
& \leq \log \E_{S}\bracket*{\prod_{i=1}^{n}\sqrt{\frac{p^\prime(\vx_i, y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)}}} + \log\frac{\br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}}{\delta} \notag \\
& = \log \prod_{i=1}^{n}\E_{(\vx_i,y_i)\sim p^*_{X,Y}}\bracket*{\sqrt{\frac{p^\prime(\vx_i, y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)}}} + \log\frac{\br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}}{\delta} \tag{$\{x_i\}_{i=1}^n$ are i.i.d. sampled from $p^*_X$} \\
& = n \log \E_{X,Y}\bracket*{\sqrt{\frac{p^\prime(\vx, y)}{p^*_{X\vert Y}(\vx \vert y)}}} + \log\frac{\br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}}{\delta} \notag \\
& = n \log \E_Y\bracket*{\E_{X\vert Y}\bracket*{\sqrt{\frac{p^\prime(\vx, y)}{p^*_{X\vert Y}(\vx \vert y)}}}} + \log\frac{\br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}}{\delta} \notag \\
& = n \log \E_Y\bracket*{\int_{\sX}p^*_{X\vert Y}(\vx \vert y)\sqrt{\frac{p^\prime(\vx, y)}{p^*_{X\vert Y}(\vx \vert y)}}d\vx} + \log\frac{\br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}}{\delta} \notag\\ 
& = n \log \E_Y\bracket*{\int_{\sX}\sqrt{p^\prime(\vx, y)p^*_{X\vert Y}(\vx \vert y)}d\vx} + \log\frac{\br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}}{\delta}. \notag 
\end{align}
As $\log x \leq x - 1$ for all $x > 0$, the inequality can be further transformed into
\begin{align}
\label{eq:intermediate_by_union_bound}
    \frac{1}{2}\sum_{i=1}^{n} \log\frac{p^\prime(\vx_i, y_i)}{p^*_{X\vert Y}(\vx_i \vert y_i)} \leq n \paren*{\E_Y\bracket*{\int_{\sX}\sqrt{p^\prime(\vx, y)p^*_{X\vert Y}(\vx \vert y)}d\vx}-1} + \log\frac{\br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}}{\delta}.
\end{align}

\paragraph{Elementary inequality for MLE estimators.}
Since the real conditional distribution $p^*_{X\vert Y}$ is in $\cP_{X\vert Y}$, for the likelihood maximizers $\hat{p}_{X\vert Y}\in\cP_{X\vert Y}$, we have $L_S(\hat{p}_{X\vert Y})=\prod_{i=1}^n \hat{p}_{X\vert Y}(\vx_i\vert y_i)\geq L_S(p^*_{X\vert Y})=\prod_{i=1}^n p^*_{X\vert Y}(\vx_i\vert y_i)$, and thus $\frac{1}{2}\sum_{i=1}^n \log \frac{\hat{p}_{X\vert Y}(\vx_i\vert y_i)}{p^*_{X\vert Y}(\vx_i\vert y_i)}=\frac{1}{2}\log \frac{\prod_{i=1}^n\hat{p}_{X\vert Y}(\vx_i\vert y_i)}{\prod_{i=1}^np^*_{X\vert Y}(\vx_i\vert y_i)}\geq \frac{1}{2}\log 1 = 0$.
According to the definition of upper bracketing number, there exists some $\hat{p}^\prime\in\cB$ such that given any $y \in \sY$, it holds that: (i) $\forall x\in \sX, \hat{p}^\prime(\vx,y)\geq \hat{p}_{X\vert Y}(\vx\vert y)$, and (ii) $\norm{\hat{p}^\prime(\cdot,y)-\hat{p}_{X\vert Y}(\cdot\vert y)}_{L^1(\sX)}=\int_{\sX}\abs{\hat{p}^\prime(\vx,y)-\hat{p}_{X\vert Y}(\vx\vert y)}d\vx \leq \epsilon.$ 
Applying (i), we have: 
\begin{align*}
\frac{1}{2}\sum_{i=1}^n \log \frac{\hat{p}^\prime(\vx_i,y_i)}{p^*_{X\vert Y}(\vx_i\vert y_i)} \geq \frac{1}{2}\sum_{i=1}^n \log \frac{\hat{p}_{X\vert Y}(\vx_i\vert y_i)}{p^*_{X\vert Y}(\vx_i\vert y_i)}\geq 0.
\end{align*} 
Combining this with \cref{eq:intermediate_by_union_bound} and rearranging the terms, we have: it holds with at least probability $1-\delta$ that
\begin{align}
\label{eq:upper_bound_of_one_minus_sqrt_of_product}
1-\E_Y\bracket*{\int_{\sX}\sqrt{p^\prime(\vx, y)p^*_{X\vert Y}(\vx \vert y)}d\vx}\leq\frac{1}{n}\log\frac{\br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}}{\delta}.
\end{align}
This serves as an elementary toolkit for deriving the subsequent upper bounds.

\paragraph{Decomposing the square of the TV distance.}
Recalling that $\mathrm{TV}(\hat{p}_{X\vert Y}, p^*_{X\vert Y}) = \frac{1}{2}\int_{\sX}\abs{\hat{p}_{X\vert Y}(\vx \vert y)- p^*_{X\vert Y}(\vx \vert y)}d\vx$, we will decompose its square and then bound each term sequentially.
First, we use the above $\hat{p}^\prime(\vx, y)$ as an intermediate term to decompose the square of $2\mathrm{TV}(\hat{p}_{X\vert Y}, p^*_{X\vert Y})$ into parts that can be effectively upper bounded:
\begin{align*}
&\paren*{2\mathrm{TV}(\hat{p}_{X\vert Y}, p^*_{X\vert Y})}^2=\paren*{\int_{\sX}\abs{\hat{p}_{X\vert Y}(\vx \vert y)- p^*_{X\vert Y}(\vx \vert y)}d\vx}^2 \\
= & \underbrace{\paren*{\int_{\sX}\abs{\hat{p}_{X\vert Y}(\vx \vert y)- p^*_{X\vert Y}(\vx \vert y)}d\vx}^2 - \paren*{\int_{\sX}\abs{\hat{p}^\prime(\vx, y)- p^*_{X\vert Y}(\vx \vert y)}d\vx}^2}_{\text{(I)}}+ \underbrace{\paren*{\int_{\sX}\abs{\hat{p}^\prime(\vx, y)- p^*_{X\vert Y}(\vx \vert y)}d\vx}^2}_{\text{(II)}}.
\end{align*}
For (I), we have 
\begin{align*}
& \paren*{\int_{\sX}\abs{\hat{p}_{X\vert Y}(\vx \vert y)- p^*_{X\vert Y}(\vx \vert y)}d\vx}^2 - \paren*{\int_{\sX}\abs{\hat{p}^\prime(\vx, y)- p^*_{X\vert Y}(\vx \vert y)}d\vx}^2\\
= & \paren*{\int_{\sX}\abs{\hat{p}_{X\vert Y}(\vx \vert y)- p^*_{X\vert Y}(\vx \vert y)} + \abs{\hat{p}^\prime(\vx, y)- p^*_{X\vert Y}(\vx \vert y)}d\vx} \paren*{\int_{\sX}\abs{\hat{p}_{X\vert Y}(\vx \vert y)- p^*_{X\vert Y}(\vx \vert y)} - \abs{\hat{p}^\prime(\vx, y)- p^*_{X\vert Y}(\vx \vert y)}d\vx} \\
\leq & \paren*{\int_{\sX}\abs{\hat{p}_{X\vert Y}(\vx \vert y)}+\abs{p^*_{X\vert Y}(\vx \vert y)} + \abs{\hat{p}^\prime(\vx, y)-\hat{p}_{X\vert Y}(\vx \vert y)}+\abs{\hat{p}_{X\vert Y}(\vx \vert y)}+\abs{p^*_{X\vert Y}(\vx \vert y)}d\vx} \paren*{\int_{\sX}\abs{\hat{p}_{X\vert Y}(\vx \vert y)- \hat{p}^\prime(\vx, y)}d\vx}\\
\leq & (\epsilon + 4)\epsilon. 
\end{align*}

The first inequality holds for the \emph{triangle inequality} $\abs{a+b}\leq\abs{a}+\abs{b}$ and the \emph{reverse triangle inequality} $\abs{\abs{a}-\abs{b}}\leq\abs{a-b}$. 
The second inequality holds for the normalization property of conditional distributions ($\int_{\sX}\abs{\hat{p}_{X\vert Y}(\vx \vert y)}d\vx$ and $\int_{\sX}\abs{p^*_{X\vert Y}(\vx \vert y)}d\vx$ equal $1$) and the property of the $\epsilon$-upper bracket ($\int_{\sX}\abs{\hat{p}^\prime(\vx,y)-\hat{p}_{X\vert Y}(\vx\vert y)}d\vx \leq \epsilon$). 

For (II), we have 
\begin{align}
& \paren*{\int_{\sX}\abs{\hat{p}^\prime(\vx, y)- p^*_{X\vert Y}(\vx \vert y)}d\vx}^2 \notag \\
\leq & \paren*{\int_{\sX}\paren*{\sqrt{\hat{p}^\prime(\vx, y)}+\sqrt{p^*_{X\vert Y}(\vx \vert y)}}^2dx} \paren*{\int_{\sX}\paren*{\sqrt{\hat{p}^\prime(\vx, y)}-\sqrt{p^*_{X\vert Y}(\vx \vert y)}}^2dx} \tag{by \emph{CauchyâSchwarz inequality}} \\
\leq & \paren*{\int_{\sX}2\paren*{\hat{p}^\prime(\vx, y)+p^*_{X\vert Y}(\vx \vert y)}d\vx} \paren*{\int_{\sX}{\hat{p}^\prime(\vx, y)+p^*_{X\vert Y}(\vx \vert y)-2\sqrt{\hat{p}^\prime(\vx, y)p^*_{X\vert Y}(\vx \vert y)}}d\vx} \tag{by $(a+b)^2\leq 2(a^2+b^2)$} \\
=& 2\paren*{\int_{\sX}{\hat{p}^\prime(\vx, y)-\hat{p}_{X\vert Y}(\vx \vert y)+\hat{p}_{X\vert Y}(\vx \vert y)+p^*_{X\vert Y}(\vx \vert y)}d\vx} \notag\\
&\paren*{\int_{\sX}{\hat{p}^\prime(\vx, y)-\hat{p}_{X\vert Y}(\vx \vert y)+\hat{p}_{X\vert Y}(\vx \vert y)+p^*_{X\vert Y}(\vx \vert y)-2\sqrt{\hat{p}^\prime(\vx, y)p^*_{X\vert Y}(\vx \vert y)}}d\vx} \notag \\
\leq & 2(\epsilon + 2) \paren*{\epsilon + 2 - 2 \int_{\sX}\sqrt{\hat{p}^\prime(\vx, y)p^*_{X\vert Y}(\vx \vert y)}d\vx}. \tag{by $\int_{\sX}\abs{\hat{p}_{X\vert Y}(\vx \vert y)}d\vx=\int_{\sX}\abs{p^*_{X\vert Y}(\vx \vert y)}d\vx=1$ and $\int_{\sX}\abs{\hat{p}^\prime(\vx, y)-\hat{p}_{X\vert Y}(\vx \vert y)}d\vx \leq \epsilon$} 
\end{align}

Putting together (I) and (II), we get: 
\begin{align}
\label{eq:upper_bound_of_TV}
\mathrm{TV}(\hat{p}_{X\vert Y}, p^*_{X\vert Y})
&=\frac{1}{2}\sqrt{\paren*{\int_{\sX}\abs{\hat{p}_{X\vert Y}(\vx \vert y)- p^*_{X\vert Y}(\vx \vert y)}d\vx}^2}\notag\\
&\leq \frac{1}{2}\sqrt{(\epsilon + 4)\epsilon+2(\epsilon + 2) \paren*{\epsilon + 2 - 2 \int_{\sX}\sqrt{\hat{p}^\prime(\vx, y)p^*_{X\vert Y}(\vx \vert y)}d\vx}}.
\end{align}

\paragraph{Bounding the average TV error.}
Based on the above results, we upper bound the average TV error (defined in \cref{eq:expected_TV_distance}) of $\hat{p}_{X\vert Y}$ as follows:
\begin{align*}
\rtv(\hat{p}_{X\vert Y})
= & \E_Y\bracket*{\mathrm{TV}(\hat{p}_{X\vert Y}, p^*_{X\vert Y})}\\
\leq & \frac{1}{2}\E_Y\bracket*{\sqrt{(\epsilon + 4)\epsilon+2(\epsilon + 2) \paren*{\epsilon + 2 - 2 \int_{\sX}\sqrt{\hat{p}^\prime(\vx, y)p^*_{X\vert Y}(\vx \vert y)}d\vx}}} \tag{by \cref{eq:upper_bound_of_TV}} \\
\leq & \frac{1}{2}\sqrt{\E_Y\bracket*{(\epsilon + 4)\epsilon+2(\epsilon + 2) \paren*{\epsilon + 2 - 2 \int_{\sX}\sqrt{\hat{p}^\prime(\vx, y)p^*_{X\vert Y}(\vx \vert y)}d\vx}}} \tag{by concavity of $f(x)=\sqrt{x}$ and \emph{Jensen's inequality}} \\
= & \frac{1}{2}\sqrt{(\epsilon + 4)\epsilon +  2(\epsilon + 2)\paren*{\epsilon + 2\paren*{1 -  \E_Y\bracket*{\int_{\sX}\sqrt{\hat{p}^\prime(\vx, y)p^*_{X\vert Y}(\vx \vert y)}d\vx}}}} \tag{by the linearity of expectation}.
\end{align*}

Recalling the elementary inequality we derived formerly in \cref{eq:upper_bound_of_one_minus_sqrt_of_product}, we have: it holds with at least probability $1-\delta$ that
\begin{align}
\label{eq:upper_bound_of_expected_TV_error_with_general_epsilon}
\rtv(\hat{p}_{X\vert Y})\leq \frac{1}{2}\sqrt{(\epsilon + 4)\epsilon +  2(\epsilon + 2)\paren*{\epsilon + \frac{2}{n}\log\frac{\br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}}{\delta}}}.
\end{align}

Recalling that $0\leq \delta\leq \frac{1}{2}$ and for non-empty $\cP_{X\vert Y}$, $\br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}\geq 1$, we have $\br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}/\delta\geq 2\geq e^{\frac{1}{2}}$.
Taking $\epsilon = 1/n$ in \cref{eq:upper_bound_of_expected_TV_error_with_general_epsilon}, it then holds with probability at least $1-\delta$ that
\begin{align}
\rtv(\hat{p}_{X\vert Y})
& \leq \frac{1}{2}\sqrt{(\frac{1}{n} + 4)\frac{1}{n} + 2(\frac{1}{n} + 2) \paren*{\frac{1}{n} + \frac{2}{n} \log \frac{\br\paren*{\frac{1}{n};\cP_{X\vert Y},L^1(\sX)}}{\delta}}}\notag\\
&\leq \frac{1}{2}\sqrt{\frac{5}{n} + 6 \paren*{\frac{1}{n} + \frac{2}{n} \log \frac{\br\paren*{\frac{1}{n};\cP_{X\vert Y},L^1(\sX)}}{\delta}}} \tag{by $\frac{1}{n} \leq 1$}\\
&\leq \frac{1}{2}\sqrt{\frac{10}{n} \log \frac{\br\paren*{\frac{1}{n};\cP_{X\vert Y},L^1(\sX)}}{\delta} + 6 \paren*{\frac{4}{n} \log \frac{\br\paren*{\frac{1}{n};\cP_{X\vert Y},L^1(\sX)}}{\delta}}}\tag{by $\frac{1}{n}\leq \frac{2}{n} \log \frac{\br\paren*{\frac{1}{n};\cP_{X\vert Y},L^1(\sX)}}{\delta}$}\\
&=\frac{1}{2}\sqrt{\frac{34}{n} \log \frac{\br\paren*{\frac{1}{n};\cP_{X\vert Y},L^1(\sX)}}{\delta}}\leq 3\sqrt{\frac{1}{n}{ \log \frac{\br\paren*{\frac{1}{n};\cP_{X\vert Y},L^1(\sX)}}{\delta}}}\notag\\
&=3\sqrt{\frac{1}{n}\paren*{ \log \br\paren*{\frac{1}{n};\cP_{X\vert Y},L^1(\sX)}+\log\frac{1}{\delta}}}. \label{eq:upper_bound_of_expected_TV_error}
\end{align}
Until now, we have completed the proof of this theorem.

\end{proof}

\subsection{Proof of Proposition~\ref{prop:multi_has_bracket_smaller_then_single}}
\label{app:prop:multi_has_bracket_smaller_then_single}
\begin{proof}[Proof of Proposition~\ref{prop:multi_has_bracket_smaller_then_single}]
    As defined in \cref{sec:formulation_for_conditional_generative_modeling}, it holds that $\cP_{X\vert Y}^{\multi}\subset\cP_{X\vert Y}^{\single}$. 
    Then, for any $p_{X\vert Y}^{\multi}\in \cP_{X\vert Y}^{\multi}$, there exists some $p_{X\vert Y}^{\single}\in\cP_{X\vert Y}^{\single}$ such that $p_{X\vert Y}^{\single}=p_{X\vert Y}^{\multi}$. 
    Given any $\epsilon>0$ and $1\leq \sfp \leq \infty$, let $\cB^{\single}$ be a $\epsilon$-upper bracket w.r.t. $L^\sfp(\sX)$ for $\cP_{X\vert Y}^{\single}$ such that $\abs{\cB^{\single}}=\br\paren*{\epsilon;\cP_{X\vert Y}^{\single},L^\sfp(\sX)}$. 
    According to the definition of $\epsilon$-upper bracket (as in Definition~\ref{def:upper_bracketing_number}), there exists some $p^\prime\in \cB^{\single}$ such that given any $y \in \sY$, it holds that: $\forall \vx\in \sX, p^\prime(\vx,y)\geq p_{X\vert Y}^{\single}(\vx\vert y)=p_{X\vert Y}^{\multi}(\vx\vert y)$, and $\norm{p^\prime(\cdot,y)-p_{X\vert Y}^{\multi}(\cdot\vert y)}_{L^\sfp(\sX)}=\norm{p^\prime(\cdot,y)-p_{X\vert Y}^{\single}(\cdot\vert y)}_{L^\sfp(\sX)}\leq \epsilon.$ 
    Therefore, $\cB^{\single}$ is also a $\epsilon$-upper bracket w.r.t. $L^\sfp(\sX)$ for $\cP_{X\vert Y}^{\multi}$, and thus $\br\paren*{\epsilon;\cP_{X\vert Y}^{\multi},L^\sfp(\sX)}\leq \abs{\cB^{\single}}=\br\paren*{\epsilon;\cP_{X\vert Y}^{\single},L^\sfp(\sX)}$.
\end{proof}



\section{Proofs for \cref{sec:instantiate_conditional_gaussian}}
\label{app:proof_of_gaussian}

\subsection{Bracketing number of conditional Gaussian distribution space}

According to  Theorem~\ref{thm:TV_upper_of_conditional_MLE}, to derive the upper bound of average TV error, we need to measure the upper bracketing number for the conditional Gaussian distribution space. This result mainly follows the bracketing number analysis of Gaussian distribution space in Lemma C.5 in~\cite{ge_2024_iclr_unsupervised}, and slightly modifies it to conditional Gaussian distribution space.
 
\begin{theorem}[Bracketing number upper bound for conditional Gaussian distribution space under multi-source training]
\label{thm:bracket_gaussian}
Let $B$ be a constant that $0< B< \infty$, suppose that $\Phi = [-B,B]^{d_1}$, $\Psi = [-B,B]^{d-d_1}$, and conditional distributions in $\cP_{X\vert Y}^{\multi}$ are formulated as in \cref{eq:conditional_density_gaussian}. Then, given any $0<\epsilon\leq 1$, the $\epsilon$-upper bracketing number of $\cP_{X\vert Y}^{\multi}$ w.r.t. $L^1(\sX)$ satisfies
\begin{align*}
    \br\paren*{\epsilon;\cP_{X\vert Y}^{\multi},L^1(\sX)}\leq \paren*{{\frac{2(1+d)B}{\epsilon}}+1}^{(K-1)d_1+d}.
\end{align*}
\end{theorem}

\begin{proof}
    According to the assumptions, the conditional distribution space expressed by the parametric estimation model is 
    \begin{align*}
        \cP_{X\vert Y}^{\multi}\!\coloneqq\!\!\curl*{\!p_{X\vert Y}^{\multi}(\vx \vert y)\!=\!\!\prod_{k=1}^K\!\paren*{p_{\phi_k, \psi}(\vx\vert k)}^{\I(y=k)}\!=\!\!\prod_{k=1}^K\!\paren*{(2\pi)^{-\frac{d}{2}}e^{-\frac{1}{2}\norm{\vx-(\phi_k, \psi)}_2^2}}^{\!\I(y=k)}\!\!:\!\phi_k\!\in\! [-B,B]^{d_1}, \!\psi\!\in\! [-B,B]^{d-d_1}\!}.
    \end{align*} 
    For any $p_{X\vert Y}^{\multi}(\vx \vert y)\!=\!\prod_{k=1}^K\paren*{(2\pi)^{-\frac{d}{2}}e^{-\frac{1}{2}\norm{\vx-(\phi_k, \psi)}_2^2}}^{\I(y=k)}\in\cP_{X\vert Y}^{\multi}$, let's first divide the mean vector $(\phi_k, \psi)$ into $\eta$-width grids with a small constant $\eta>0$ (the value of $\eta$ will be specified later): 
    If $(\phi_k)_i \in [j\eta, (j+1)\eta)$ for some $j\in\Z$, let $(\bar{\phi}_k)_i=j\eta$ and $\bar{\phi}_k\coloneqq\paren*{(\bar{\phi}_k)_1, \dots, (\bar{\phi}_k)_{d_1}}$. Similarly, if $(\psi)_i \in [j\eta, (j+1)\eta)$ for some $j\in\Z$, let $(\bar{\psi})_i=j\eta$ and $\bar{\psi}\coloneqq\paren*{(\bar{\psi})_1, \dots, (\bar{\psi})_{d-d_1}}$. In this case, we have $\norm{(\phi_k, \psi)-(\bar{\phi}_k, \bar{\psi})}_2^2\leq d\eta^2$. 
    
    Let
    \begin{align*}
        p^\prime(\vx, y)=\prod_{k=1}^K\paren*{(2\pi)^{-\frac{d}{2}}e^{-\frac{c_1}{2}\norm{\vx-(\bar{\phi}_k, \bar{\psi})}_2^2+c_2}}^{\I(y=k)}.
    \end{align*}
    According to the definition of the bracketing, we want to prove that $p^\prime(\vx, y) \geq  p_{X\vert Y}^{\multi}(\vx \vert y)$. By completing the square w.r.t. $\vx$, we have 
    \begin{align*}
        &-\frac{c_1}{2}\norm{\vx-(\bar{\phi}_k, \bar{\psi})}_2^2+c_2 - \paren*{-\frac{1}{2}\norm{\vx-(\phi_k, \psi)}_2^2}\\
        =&\frac{1}{2}\paren*{(1-c_1)\norm*{\vx+\frac{c_1(\bar{\phi}_k, \bar{\psi})-(\phi_k, \psi)}{1-c_1}}_2^2-\frac{c_1}{1-c_1}\norm*{(\bar{\phi}_k, \bar{\psi})-(\phi_k, \psi)}_2^2+2c_2}.
    \end{align*}
    Further taking $c_1=1-\eta$ and $c_2=d(1-\eta)\eta/2$, we have 
    \begin{align*}
        &(1-c_1)\norm*{\vx+\frac{c_1(\bar{\phi}_k, \bar{\psi})-(\phi_k, \psi)}{1-c_1}}_2^2-\frac{c_1}{1-c_1}\norm*{(\bar{\phi}_k, \bar{\psi})-(\phi_k, \psi)}_2^2+2c_2\\
        =&\eta\norm*{\vx+\frac{c_1(\bar{\phi}_k, \bar{\psi})-(\phi_k, \psi)}{1-c_1}}_2^2-\frac{1-\eta}{\eta}\norm*{(\bar{\phi}_k, \bar{\psi})-(\phi_k, \psi)}_2^2+2c_2 \tag{$c_1=1-\eta$}\\
        \geq & -\frac{1-\eta}{\eta}\norm*{(\bar{\phi}_k, \bar{\psi})-(\phi_k, \psi)}_2^2+2c_2 \tag{$\eta>0$}\\
        \geq & -\frac{1-\eta}{\eta}d\eta^2+2c_2 \tag{$\norm{(\phi_k, \psi)-(\bar{\phi}_k, \bar{\psi})}_2^2\leq d\eta^2$}\\
        = & -d (1-\eta)\eta + d(1-\eta)\eta = 0.
    \end{align*}
    Therefore, it holds that for all $y\in \sY$, 
    \begin{align}
    \label{eq:gaussian_bracket_condition_1}
        \forall \vx\in \sX: p^\prime(\vx, y)\geq p_{X\vert Y}^{\multi}(\vx \vert y).
    \end{align}  
    Moreover, given any $0<\epsilon \leq 1$, we take $\eta=\frac{\epsilon}{1+d}$, and thus $c_1=1-\frac{\epsilon}{1+d}$ and $c_2=\frac{1}{2}(1-\frac{\epsilon}{1+d})\frac{\epsilon}{\frac{1}{d}+1}$. Since $d\in\N$, we have $\eta\leq\frac{1}{2}$ and $c_2\leq\frac{1}{2}$. 
    Then, $\norm{p^\prime(\cdot,y)-p_{X\vert Y}^{\multi}(\cdot\vert y)}_{L^1(\sX)}$ can be bounded as
    \begin{align}
        &\norm{p^\prime(\cdot,y)-p_{X\vert Y}^{\multi}(\cdot\vert y)}_{L^1(\sX)}=\int_{\sX}\abs{p^\prime(\vx, y)-p_{X\vert Y}^{\multi}(\vx \vert y)}d\vx\notag\\
        =&\int_{\sX}p^\prime(\vx, y)d\vx-\int_{\sX}p_{X\vert Y}^{\multi}(\vx \vert y)d\vx
        =\frac{1}{\sqrt{c_1}}e^{c_2}-1\tag{$\int_{\sX} e^{-\frac{1}{2}\norm{\vx}_2^2}d\vx=(2\pi)^{\frac{d}{2}}$}\\
        \leq&\frac{1}{\sqrt{c_1}}(1+2c_2)-1\tag{$e^x\leq 1+2x$ for $x\in[0,\frac{1}{2}]$}\\
        =&\frac{1}{\sqrt{1-\eta}}(1+d(1-\eta)\eta)-1 \tag{$c_1=1-\eta$ and $c_2=d(1-\eta)\eta/2$}\\
        \leq&(1+\eta)(1+d(1-\eta)\eta)-1 \tag{$\frac{1}{\sqrt{1-x}}\leq 1+x$ for $x\in[0,\frac{1}{2}]$}\\
        =&\eta\paren*{1+d(1-\eta^2)}
        \leq\eta\paren*{1+d}
        =\epsilon \label{eq:gaussian_bracket_condition_2}
    \end{align}

    Combining \cref{eq:gaussian_bracket_condition_1} and \cref{eq:gaussian_bracket_condition_2}, we know that for any $p_{X\vert Y}^{\multi}(\vx \vert y)\in\cP_{X\vert Y}^{\multi}$ and $0<\epsilon\leq 1$, there exists some $p^\prime(\vx, y)\in \cB$ such that given any $y \in \sY$, it holds that $\forall \vx\in \sX: p^\prime(\vx, y)\geq p_{X\vert Y}(\vx \vert y),$ and $\norm{p^\prime(\cdot,y)-p_{X\vert Y}(\cdot\vert y)}_{L^\sfp(\sX)}\leq \epsilon$, where 
    \begin{align*}
        \cB\coloneqq\curl*{p^\prime(\vx, y)=\prod_{k=1}^K\paren*{(2\pi)^{-\frac{d}{2}}e^{-\frac{c_1}{2}\norm{\vx-(\bar{\phi}_k, \bar{\psi})}_2^2+c_2}}^{\I(y=k)}:(\bar{\phi}_k)_i, (\bar{\psi})_i\in [-B,B]\cap \eta\Z}
    \end{align*}
    
    Recalling the definition of the upper bracketing number in Definition~\ref{def:upper_bracketing_number}, we know that $\cB$ is an $\epsilon$-upper bracket of $\cP_{X\vert Y}^{\multi}$ w.r.t. $L^1(\sX)$. 
    Therefore, 
    \begin{align*}
        &\br\paren*{\epsilon;\cP_{X\vert Y}^{\multi},L^1(\sX)}\\
        \leq & \abs*{\cB}
        =\abs*{\curl*{\{\bar{\phi}_k\}_{k=1}^K, \bar{\psi}:(\bar{\phi}_k)_i, (\bar{\psi})_i\in [-B,B]\cap \eta\Z}}\\
        \leq & \paren*{\frac{2B}{\eta} +1}^{Kd_1+d-d_1}\\
        =&\paren*{\frac{2(1+d)B}{\epsilon} +1}^{(K-1)d_1+d},
    \end{align*}
    which completes the proof.
\end{proof}

\subsection{Proof of \cref{thm:tv_upper_gaussian}}
\label{app:proof_thm:tv_upper_gaussian}

\begin{proof}[Proof of \cref{thm:tv_upper_gaussian}]
    As $\phi_k^*\in\Phi, \psi^*\in\Psi$, and $\hat{p}_{X\vert Y}^{\multi}$ is the maximizer of likelihood $L_S(p_{X\vert Y})$ in $\cP_{X\vert Y}^{\multi}$, according to \cref{thm:TV_upper_of_conditional_MLE}, we know that 
    \begin{align*}
        \rtv(\hat{p}_{X\vert Y}^{\multi})\leq3\sqrt{\frac{1}{n}\paren*{\log\br\paren*{\frac{1}{n};\cP_{X\vert Y}^{\multi},L^1(\sX)}+\log\frac{1}{\delta}}}.
    \end{align*}
    According to \cref{thm:bracket_gaussian}, it holds that 
    \begin{align*}
        \br\paren*{\frac{1}{n};\cP_{X\vert Y}^{\multi},L^1(\sX)}\leq \paren*{{2(1+d)Bn}+1}^{(K-1)d_1+d}.
    \end{align*}
    Therefore, we obtain the result that 
    \begin{align*}
        \rtv(\hat{p}_{X\vert Y}^{\multi})\leq3\sqrt{\frac{1}{n}\paren*{\paren*{(K-1)d_1+d}\log \paren*{{2(1+d)Bn}+1}+\log\frac{1}{\delta}}}.
    \end{align*}
    Omitting constants about $n,K,d_1,d,B$, and the logarithm term we have $\rtv(\hat{p}_{X\vert Y}^{\multi})=\tilde{\cO}\paren*{\sqrt{\frac{(K-1)d_1+d}{n}}}$.
\end{proof}

\subsection{Average TV error bound under single-source training}

\begin{theorem}[Average TV error bound for conditional Gaussian distribution space under single-source training]
\label{thm:tv_upper_gaussian_single}
Let $\hat{p}_{X\vert Y}^{\single}$ be the likelihood maximizer defined in \cref{eq:single_mle_solution} given $\cP_{X\vert Y}^{\single}$ with conditional distributions as in \cref{eq:conditional_density_gaussian}. 
Suppose $\Phi = [-B,B]^{d_1}$, $\Psi = [-B,B]^{d-d_1}$ with constant $B>0$, and $\phi_k^*\in \Phi$, $\psi^*\in \Psi$. Then, for any $0<\delta\leq 1/2$, it holds with probability at least $1-\delta$ that 
\begin{align*}
    \rtv(\hat{p}_{X\vert Y}^{\single})=\tilde{\cO}\paren*{\sqrt{\frac{Kd}{n}}}.
\end{align*}
\end{theorem}

\begin{proof}
The proof is very similar to that in the multi-source case.
According to the assumptions, the conditional distribution space expressed by the parametric estimation model is 
    \begin{align*}
        \cP_{X\vert Y}^{\single}\!\coloneqq\!\!\curl*{\!p_{X\vert Y}^{\single}(\vx \vert y)\!=\!\!\prod_{k=1}^K\!\paren*{p_{\phi_k, \psi_k}(\vx\vert k)}^{\I(y=k)}\!=\!\!\prod_{k=1}^K\!\paren*{(2\pi)^{-\frac{d}{2}}e^{-\frac{1}{2}\norm{\vx-(\phi_k, \psi_k)}_2^2}}^{\!\I(y=k)}\!\!:\!\phi_k\!\in\! [-B,B]^{d_1}, \!\psi_k\!\in\! [-B,B]^{d-d_1}\!}.
    \end{align*} 
    For any $p_{X\vert Y}^{\single}(\vx \vert y)\!=\!\prod_{k=1}^K\paren*{(2\pi)^{-\frac{d}{2}}e^{-\frac{1}{2}\norm{\vx-(\phi_k, \psi_k)}_2^2}}^{\I(y=k)}\in\cP_{X\vert Y}^{\single}$, let's first divide the mean vector $(\phi_k, \psi_k)$ into $\eta$-width grids with a small constant $\eta>0$ (the value of $\eta$ will be specified later): 
    If $(\phi_k)_i \in [j\eta, (j+1)\eta)$ for some $j\in\Z$, let $(\bar{\phi}_k)_i=j\eta$ and $\bar{\phi}_k\coloneqq\paren*{(\bar{\phi}_k)_1, \dots, (\bar{\phi}_k)_{d_1}}$. Similarly, if $(\psi_k)_i \in [j\eta, (j+1)\eta)$ for some $j\in\Z$, let $(\bar{\psi}_k)_i=j\eta$ and $\bar{\psi}_k \coloneqq \paren*{(\bar{\psi}_k)_1, \dots, (\bar{\psi}_k)_{d-d_1}}$. In this case, we have $\norm{(\phi_k, \psi_k)-(\bar{\phi}_k, \bar{\psi}_k)}_2^2\leq d\eta^2$. 
    
    Let
    \begin{align*}
        p^\prime(\vx, y)=\prod_{k=1}^K\paren*{(2\pi)^{-\frac{d}{2}}e^{-\frac{c_1}{2}\norm{\vx-(\bar{\phi}_k, \bar{\psi}_k)}_2^2+c_2}}^{\I(y=k)}.
    \end{align*}
    
    We need $p^\prime(\vx, y) \ge  p_{X\vert Y}^{\single}(\vx \vert y)$ by the definition of the bracketing. By completing the square w.r.t. $\vx$, we have 
    \begin{align*}
        &-\frac{c_1}{2}\norm{\vx-(\bar{\phi}_k, \bar{\psi}_k)}_2^2+c_2 - \paren*{-\frac{1}{2}\norm{\vx-(\phi_k, \psi_k)}_2^2}\\
        =&\frac{1}{2}\paren*{(1-c_1)\norm*{\vx+\frac{c_1(\bar{\phi}_k, \bar{\psi}_k)-(\phi_k, \psi_k)}{1-c_1}}_2^2-\frac{c_1}{1-c_1}\norm*{(\bar{\phi}_k, \bar{\psi}_k)-(\phi_k, \psi_k)}_2^2+2c_2}.
    \end{align*}
    Further taking $c_1=1-\eta$ and $c_2=d(1-\eta)\eta/2$, we have 
    \begin{align*}
        &(1-c_1)\norm*{\vx+\frac{c_1(\bar{\phi}_k, \bar{\psi}_k)-(\phi_k, \psi_k)}{1-c_1}}_2^2-\frac{c_1}{1-c_1}\norm*{(\bar{\phi}_k, \bar{\psi}_k)-(\phi_k, \psi_k)}_2^2+2c_2\\
        =&\eta\norm*{\vx+\frac{c_1(\bar{\phi}_k, \bar{\psi}_k)-(\phi_k, \psi_k)}{1-c_1}}_2^2-\frac{1-\eta}{\eta}\norm*{(\bar{\phi}_k, \bar{\psi}_k)-(\phi_k, \psi_k)}_2^2+2c_2 \tag{$c_1=1-\eta$}\\
        \geq & -\frac{1-\eta}{\eta}\norm*{(\bar{\phi}_k, \bar{\psi}_k)-(\phi_k, \psi_k)}_2^2+2c_2 \tag{$\eta>0$}\\
        \geq & -\frac{1-\eta}{\eta}d\eta^2+2c_2 \tag{$\norm{(\phi_k, \psi)-(\bar{\phi}_k, \bar{\psi}_k)}_2^2\leq d\eta^2$}\\
        = & -d (1-\eta)\eta + d(1-\eta)\eta = 0.
    \end{align*}
    Therefore, it holds that for all $y\in \sY$, 
    \begin{align}
    \label{eq:gaussian_bracket_condition_1_single}
        \forall \vx\in \sX: p^\prime(\vx, y)\geq p_{X\vert Y}^{\single}(\vx \vert y).
    \end{align}  
    Moreover, given any $0<\epsilon \leq 1$, we take $\eta=\frac{\epsilon}{1+d}$, and thus $c_1=1-\frac{\epsilon}{1+d}$ and $c_2=\frac{1}{2}(1-\frac{\epsilon}{1+d})\frac{\epsilon}{\frac{1}{d}+1}$. Since $d\in\N$, we have $\eta\leq\frac{1}{2}$ and $c_2\leq\frac{1}{2}$. 
    Then, $\norm{p^\prime(\cdot,y)-p_{X\vert Y}^{\single}(\cdot\vert y)}_{L^1(\sX)}$ can be bounded as
    \begin{align}
        &\norm{p^\prime(\cdot,y)-p_{X\vert Y}^{\single}(\cdot\vert y)}_{L^1(\sX)}=\int_{\sX}\abs{p^\prime(\vx, y)-p_{X\vert Y}^{\single}(\vx \vert y)}d\vx\notag\\
        =&\int_{\sX}p^\prime(\vx, y)d\vx-\int_{\sX}p_{X\vert Y}^{\single}(\vx \vert y)d\vx
        =\frac{1}{\sqrt{c_1}}e^{c_2}-1\tag{$\int_{\sX} e^{-\frac{1}{2}\norm{\vx}_2^2}d\vx=(2\pi)^{\frac{d}{2}}$}\\
        \leq&\frac{1}{\sqrt{c_1}}(1+2c_2)-1\tag{$e^x\leq 1+2x$ for $x\in[0,\frac{1}{2}]$}\\
        =&\frac{1}{\sqrt{1-\eta}}(1+d(1-\eta)\eta)-1 \tag{$c_1=1-\eta$ and $c_2=d(1-\eta)\eta/2$}\\
        \leq&(1+\eta)(1+d(1-\eta)\eta)-1 \tag{$\frac{1}{\sqrt{1-x}}\leq 1+x$ for $x\in[0,\frac{1}{2}]$}\\
        =&\eta\paren*{1+d(1-\eta^2)}
        \leq\eta\paren*{1+d}
        =\epsilon \label{eq:gaussian_bracket_condition_2_single}
    \end{align}

    Combining \cref{eq:gaussian_bracket_condition_1_single} and \cref{eq:gaussian_bracket_condition_2_single}, we know that for any $p_{X\vert Y}^{\single}(\vx \vert y)\in\cP_{X\vert Y}^{\single}$ and $0<\epsilon\leq 1$, there exists some $p^\prime(\vx, y)\in \cB$ such that given any $y \in \sY$, it holds that $\forall \vx\in \sX: p^\prime(\vx, y)\geq p_{X\vert Y}(\vx \vert y),$ and $\norm{p^\prime(\cdot,y)-p_{X\vert Y}(\cdot\vert y)}_{L^\sfp(\sX)}\leq \epsilon$, where 
    \begin{align*}
        \cB\coloneqq\curl*{p^\prime(\vx, y)=\prod_{k=1}^K\paren*{(2\pi)^{-\frac{d}{2}}e^{-\frac{c_1}{2}\norm{\vx-(\bar{\phi}_k, \bar{\psi}_k)}_2^2+c_2}}^{\I(y=k)}:(\bar{\phi}_k)_i, (\bar{\psi}_k)_i\in [-B,B]\cap \eta\Z}
    \end{align*}
    
    Recalling the definition of the upper bracketing number in Definition~\ref{def:upper_bracketing_number}, we know that $\cB$ is an $\epsilon$-upper bracket of $\cP_{X\vert Y}^{\single}$ w.r.t. $L^1(\sX)$. 
    Therefore, 
    \begin{align*}
        &\br\paren*{\epsilon;\cP_{X\vert Y}^{\single},L^1(\sX)}\\
        \leq & \abs*{\cB}
        =\abs*{\curl*{\{\bar{\phi}_k\}_{k=1}^K, \{\bar{\psi}_k\}_{k=1}^K:(\bar{\phi}_k)_i, (\bar{\psi}_k)_i\in [-B,B]\cap \eta\Z}}\\
        \leq & \paren*{\frac{2B}{\eta} +1}^{Kd_1+K(d-d_1)}\\
        =&\paren*{\frac{2(1+d)B}{\epsilon} +1}^{Kd}.
    \end{align*}

Besides, according to \cref{thm:TV_upper_of_conditional_MLE}, we know that 
    \begin{align*}
        \rtv(\hat{p}_{X\vert Y}^{\single}) &\leq3\sqrt{\frac{1}{n}\paren*{\log\br\paren*{\frac{1}{n};\cP_{X\vert Y}^{\single},L^1(\sX)}+\log\frac{1}{\delta}}} \\
        &\leq 3\sqrt{\frac{1}{n}\paren*{Kd\log \paren*{{2(1+d)Bn}+1}+\log\frac{1}{\delta}}}.
    \end{align*}
    Omitting constants about $n,K,d_1,d,B$, and the logarithm term we have $\rtv(\hat{p}_{X\vert Y}^{\multi})=\tilde{\cO}\paren*{\sqrt{\frac{Kd}{n}}}$.
\end{proof}

\section{Proofs for \cref{sec:instantiation_arm}}
\label{app:proof_arm}

\subsection{Preliminaries for evaluating the bracketing number of neural networks}
\label{app:Preliminaries for evaluating the bracketing number of neural networks}

Our results build on the intrinsic connection between the bracketing number of conditional distribution spaces and the covering number of neural network models.
There is a rich body of work on the complexity of ReLU fully connected neural networks, also known as multilayer perceptrons (MLPs) as defined in Definition~\ref{def:class_of_nn} from various perspectives, including Rademacher complexity~\cite{bartlett_2017_nips_spectrally}, VC-dimension~\cite{bartlett_2019_jmlr_vcdim}, and covering numbers~\cite{suzuki_2019_iclr_relunetwork,shen_2024_icml_networkequivalence,ou_2024_arxiv_covering}. 
Specifically, prior results indicate that the logarithm of the covering number of an MLP scales as $\tilde{\cO}(LS)$, where $L$ is the depth and $S$ is the sparsity constraint. Furthermore, \citet{ou_2024_arxiv_covering} establish a lower bound, showing that for $B\geq 1$ and $W, L\geq 60$, the covering number scales as $\tilde{\Theta}(LS)$.
Their proofs share a common idea.
To enhance clarity, we include a detailed derivation below following these prior works.


\begin{definition}[$\epsilon$-covering number]
\label{def:covering_number}
    Let $\epsilon$ be a real number that $\epsilon>0$ and $\sfp, \sfq$ be integers that $1\leq \sfp, \sfq\leq\infty$.
    An $\epsilon$-cover of a function space $\cF$ with respect to $\norm*{\cdot}_{}$ is a finite function set $\cC\subset\cF$ such that for any $\vf\in \cF$, there exists some $\vf^\prime\in \cC$ such that 
    $\norm*{\norm{\vf(\vx)-\vf^\prime(\vx)}_{\sfq}}_{L^{\sfp}(\sX)}\leq \epsilon.$ 
    In particular, when $\sfp=\sfq=\infty$, it requires $\norm*{\norm{\vf(\vx)-\vf^\prime(\vx)}_{\infty}}_{L^{\infty}(\sX)}=\sup_{\vx\in\sX}\norm{\vf(\vx)-\vf^\prime(\vx)}_{\infty}\leq \epsilon.$
    The $\epsilon$-covering number $\cov\paren*{\epsilon;\cF,\norm*{\cdot}_{\sfq,L^{\sfp}(\sX)}}$ is the cardinality of the smallest $\epsilon$-cover with respect to $\norm*{\cdot}_{\sfq,L^{\sfp}(\sX)}$. 

\end{definition}

\begin{lemma}[Lipschitz property of ReLU and sigmoid, Lemma A.1 in \cite{bartlett_2017_nips_spectrally}]
\label{lem:lip_relu}
   Element-wise ReLU $\relu(\cdot):\R^d\to\R^d$ and sigmoid $\sigma(x)=\frac{1}{1+e^{-x}}$ are $1$-Lipschitz according to $\norm{\cdot}_{\sfp}$ for any $\sfp \geq 1$.
\end{lemma}


\begin{lemma}[Covering number of a composite function class]
\label{lem:covering_number_of_composite_function_class}
Suppose we have two classes of functions, $\cG$ consisting of functions mapping from $\sX_1$ to $\sX_2$ and $\cF$ consisting of functions mapping from $\sX_2$ to $\sX_3$. We denote by $\cF\circ\cG$ all possible composition of functions in $\cF$ and $\cG$ that $\cF\circ\cG=\{\vf\circ \vg: \vf\in\cF,\vg\in\cG\}$.
Assume that any $\vf\in\cF$ is $\kappa_{\cF}$-Lipschitz w.r.t. $\norm{\cdot}_{\infty}$, i.e., for all $\vx_2, \vx_2^\prime\in\sX_2$, $\norm{\vf(\vx_2)-\vf(\vx_2^\prime)}_{\infty}\leq\kappa_{\cF}\norm{\vx_2-\vx_2^\prime}_{\infty}$. 
Then, given constants $\epsilon_{\cF}, \epsilon_{\cG}>0$ we have
\begin{align*}
    \cov\paren*{\epsilon_{\cF}+\kappa_{\cF}\epsilon_{\cG};\cF\circ\cG,\norm*{\cdot}_{\infty,L^{\infty}(\sX_1)}} \leq \cov\paren*{\epsilon_{\cF};\cF,\norm*{\cdot}_{\infty,L^{\infty}(\sX_2)}}\cov\paren*{\epsilon_{\cG};\cG,\norm*{\cdot}_{\infty,L^{\infty}(\sX_1)}}
\end{align*}
\begin{proof}
    Let $\cC_{\cF}$ be an $\epsilon_{\cF}$-cover of $\cF$ w.r.t. $\norm*{\cdot}_{\infty,L^{\infty}(\sX_2)}$ such that $\abs{\cC_{\cF}}=\cov\paren*{\epsilon_{\cF};\cF,\norm*{\cdot}_{\infty,L^{\infty}(\sX_2)}}$, and $\cC_{\cG}$ be an $\epsilon_{\cG}$-cover of $\cG$ w.r.t. $\norm*{\cdot}_{\infty,L^{\infty}(\sX_1)}$ such that $\abs{\cC_{\cG}}=\cov\paren*{\epsilon_{\cG};\cG,\norm*{\cdot}_{\infty,L^{\infty}(\sX_1)}}$.
    For any $\vf\circ \vg\in\cF\circ\cG$, there exists $\vf^\prime\in\cC_{\cF}$ and $\vg^\prime\in\cC_{\cG}$ such that 
    \begin{align*}
        \forall \vx_2\in\sX_2, \norm{\vf(\vx_2)-\vf^\prime(\vx_2)}_{\infty}\leq\epsilon_{\cF}, \quad \text{and} \quad  \forall \vx_1\in\sX_1, \norm{\vg(\vx_1)-\vg^\prime(\vx_1)}_{\infty}\leq\epsilon_{\cG}.
    \end{align*}
    Then, for any $\vx_1\in\sX_1$, we have 
    \begin{align*}
        \norm{\vf\circ \vg(\vx_1)-\vf^\prime \circ \vg^\prime(\vx_1)}_{\infty}
        &\leq \norm{\vf\circ \vg(\vx_1)-\vf^\prime\circ \vg(\vx_1)}_{\infty}+\norm{\vf^\prime\circ \vg(\vx_1)-\vf^\prime \circ \vg^\prime(\vx_1)}_{\infty}\\
        &\leq \epsilon_{\cF}+\kappa_{\cF} \norm{\vg(\vx_1)- \vg^\prime(\vx_1)}_{\infty}\\
        &\leq \epsilon_{\cF}+\kappa_{\cF}\epsilon_{\cG}.
    \end{align*}
    Therefore, we have $\cC_{\cF}\circ\cC_{\cG}$ is an $\epsilon_{\cF}+\kappa_{\cF}\epsilon_{\cG}$-cover of $\cF\circ\cG$, and thus
    \begin{align*}
        \cov\paren*{\epsilon_{\cF}+\kappa_{\cF}\epsilon_{\cG};\cF\circ\cG,\norm*{\cdot}_{\infty,L^{\infty}(\sX_1)}}
        \leq \abs{\cC_{\cF}\circ\cC_{\cG}}
        \leq \abs{\cC_{\cF}}\abs{\cC_{\cG}}
        =\cov\paren*{\epsilon_{\cF};\cF,\norm*{\cdot}_{\infty,L^{\infty}(\sX_2)}}\cov\paren*{\epsilon_{\cG};\cG,\norm*{\cdot}_{\infty,L^{\infty}(\sX_1)}}.
    \end{align*}
\end{proof}
    
\end{lemma}

\begin{lemma}[Covering number of an MLP class]
\label{lem:covering_number_of_nn}
Given any constant $\delta>0$, the covering number with respect to $\norm*{\cdot}_{\infty,L^{\infty}(\sX)}$ with $\sX\subset[0,1]^{W_0}$ of an MLP class $\cF(L, W, S, B)$ defined in Definition~\ref{def:class_of_nn} can be bounded by
\begin{align*}
    \cov\paren*{L(B\vee 1)^{L-1}(W+1)^L\delta;\cF(L,W,S,B),\norm*{\cdot}_{\infty,L^{\infty}(\sX)}}\leq \paren*{\frac{2B}{\delta}+1}^S.
\end{align*}
% \begin{align*}
%     \cov\paren*{\epsilon;\cF(L,W,S,B),\norm*{\cdot}_{\infty,L^{\infty}([0,1]^D)}}\leq \paren*{\frac{2L(B\vee 1)^L(W+1)^L}{\epsilon}}^S.
% \end{align*}
\end{lemma}
\begin{proof}
    Fix any $\vx\in[0,1]^{W_0}$. Given any network $\vf\in\cF(L,W,S,B)$ expressed as 
    \begin{align*}
        \vf(\vx)=(\vA^{(L)}\relu(\cdot)+\vb^{(L)})\circ\dots\circ(\vA^{(1)}\vx+\vb^{(1)}),
    \end{align*}
    let $\vf_l(\vx)\coloneqq (\vA^{(l)}\relu(\cdot)+\vb^{(l)})\circ\dots\circ(\vA^{(1)}\vx+\vb^{(1)})$ for $l=2,\dots,L$ and $\vf_1(\vx)=\vA^{(1)}\vx+\vb^{(1)}$.

    \paragraph{Sup-norm of the output at each layer.} 
    We first prove the statement that $\norm{\vf_l(\vx)}_{\infty}\leq(B\vee 1)^l(W+1)^l$ for all $l\in[L]$ by induction. 
    When $l=1$, 
    \begin{align*}
        \norm{\vf_1(\vx)}_{\infty}
        &=\norm{\vA^{(1)}\vx+\vb^{(1)}}_{\infty}\leq \max_i\norm{\vA^{(1)}[i,:]}_1\norm{\vx}_{\infty}+\norm{\vb^{(1)}}_{\infty}\\
        &\leq WB+B \tag{$W_0\leq W, \norm{\vA^{(1)}}_{\infty}\leq B, \norm{\vb^{(1)}}_{\infty}\leq B, \vx\in\sX\subset[0,1]^{W_0}$}\\
        &=B(W+1) \leq(B\vee 1)^1(W+1)^1,
    \end{align*}
    which implies the statement is true for $l=1$.
    Assume that for some $l=i\geq 1$, $\norm{\vf_i(\vx)}_{\infty}\leq(B\vee 1)^i(W+1)^i$, then we have
    \begin{align*}
        \norm{\vf_{i+1}(\vx)}_{\infty}
        &=\norm{\vA^{(i+1)}\relu\paren*{\vf_i(\vx)}+\vb^{(i+1)}}_{\infty}\leq \max_i\norm{\vA^{(i+1)}[i,:]}_1\norm{\relu\paren*{\vf_i(\vx)}}_{\infty}+\norm{\vb^{(i+1)}}_{\infty}\\
        &\leq WB\norm{\relu\paren*{\vf_i(\vx)}}_{\infty}+B \tag{$W_{i}\leq W, \norm{\vA^{(i+1)}}_{\infty}\leq B, \norm{\vb^{(i+1)}}_{\infty}\leq B$}\\
        &\leq WB\norm{\vf_i(\vx)}_{\infty}+B 
        \tag{$\relu(\cdot)$ is $1$-Lipschitz continuous for Lemma~\ref{lem:lip_relu} and $\relu(\bm{0})=\bm{0}$}\\
        % \tag{$\norm{\relu\paren*{f_i(x)}}_{\infty}=\max_j\abs{\max\{f_i(x)[j],0\}}\leq \max_j\abs{f_i(x)[j]}=\norm{f_i(x)}_{\infty}$}\\
        &\leq WB(B\vee 1)^i(W+1)^i+B  \\
        &\leq\left\{
             \begin{array}{lr}
             WB^{i+1}(W+1)^i+B^{i+1}(W+1)^i, & B\geq 1,\\
             W(W+1)^i+(W+1)^i, & B<1,  
             \end{array}\right.\\
        &=(B\vee 1)^{i+1}(W+1)^{i+1},
    \end{align*}
    which implies the statement is true for $l=i+1$, completing the induction steps.
    Therefore, it holds that for all $l\in[L]$, 
    \begin{align}
    \label{eq:bounding_sup_form}
        \norm{\vf_l(\vx)}_{\infty}\leq(B\vee 1)^l(W+1)^l.
    \end{align}
    
    \paragraph{Parameter-Lipschitzness at each layer.} 
    For any two different neural networks $\vf,\vf^\prime\in\cF(L,W,S,B)$ expressed by 
    \begin{align*}
        \vf(x)=(\vA^{(L)}\relu(\cdot)+\vb^{(L)})\circ\dots\circ(\vA^{(1)}\vx+\vb^{(1)}), 
        \vf^\prime(\vx)= ({\vA^{(L)}}^\prime\relu(\cdot)+{\vb^{(L)}}^\prime)\circ\dots\circ({\vA^{(1)}}^\prime \vx+{\vb^{(1)}}^\prime),
    \end{align*}
    with parameter distance that $\max_{l}\norm{\vA^{(l)}-{\vA^{(l)}}^\prime}_{\infty}\vee \norm{\vb^{(l)}-{\vb^{(l)}}^\prime}_{\infty}\leq \delta$, 
    we prove the statement that $\norm{\vf_l(\vx)-\vf_l^\prime(\vx)}_{\infty}\leq l(B\vee 1)^{l-1}(W+1)^l\delta$ for all $l\in[L]$ by induction.
    When $l=1$, 
    \begin{align*}
        \norm{\vf_1(\vx)-\vf_1^\prime(\vx)}_{\infty}
        &=\norm{\vA^{(1)}\vx+\vb^{(1)}-{\vA^{(1)}}^\prime \vx-{\vb^{(1)}}^\prime}_{\infty}\\
        &\leq \norm{(\vA^{(1)}-{\vA^{(1)}}^\prime)\vx}_{\infty}+\norm{\vb^{(1)}-{\vb^{(1)}}^\prime}_{\infty}\\
        &\leq W\delta+\delta \tag{$W_0\leq W, \norm{\vA^{(1)}-{\vA^{(1)}}^\prime}_{\infty}\leq \delta, \norm{\vb^{(1)}-{\vb^{(1)}}^\prime}_{\infty}\leq \delta, \vx\in[0,1]^{W_0}$}\\
        &=(W+1)\delta \\
        &\leq (B\vee 1)^{0}(W+1)^1 \delta,
    \end{align*}
    which implies the statement is true for $l=1$.   
    Assume that for some $l=i\geq 1$, $\norm{\vf_i(\vx)-\vf_i^\prime(\vx)}_{\infty}\leq i(B\vee 1)^{i-1}(W+1)^i\delta$, then we have
    \begin{align*}
        \norm{\vf_{i+1}(\vx)-\vf_{i+1}^\prime(\vx)}_{\infty}
        &=\norm{\vA^{(i+1)}\relu\paren*{\vf_i(\vx)}+\vb^{(i+1)}-{\vA^{(i+1)}}^\prime \relu\paren*{\vf_i^\prime(x)}-{\vb^{(i+1)}}^\prime}_{\infty}\\
        &\leq \norm{\paren*{\vA^{(i+1)}-{\vA^{(i+1)}}^\prime }\relu\paren*{\vf_i(\vx)}}_{\infty} +\norm{{\vA^{(i+1)}}^\prime\paren*{\relu\paren*{\vf_i(\vx)}-\relu\paren*{\vf_i^\prime(\vx)}}}_{\infty}\\
        &\quad +\norm{\vb^{(i+1)}-{\vb^{(i+1)}}^\prime}_{\infty}\\
        &\leq W\delta\norm{\relu\paren*{\vf_i(\vx)}}_{\infty}+WB\norm*{\relu\paren*{\vf_i(\vx)}-\relu\paren*{\vf_i^\prime(\vx)}}_{\infty}+\delta \tag{$W_{i}\leq W, \norm{\vA^{(i+1)}-{\vA^{(i+1)}}^\prime}_{\infty}\leq \delta, \norm{{\vA^{(i+1)}}^\prime}_{\infty}\leq B, \norm{\vb^{(i+1)}-{\vb^{(i+1)}}^\prime}_{\infty}\leq \delta$}\\
        &\leq W\delta\norm{\vf_i(\vx)}_{\infty}+WB\norm*{\vf_i(\vx)-\vf_i^\prime(\vx)}_{\infty}+\delta \tag{$\relu(\cdot)$ is $1$-Lipschitz continuous for Lemma~\ref{lem:lip_relu} and $\relu(\bm{0})=\bm{0}$}\\
        &\leq W\delta(B\vee 1)^i(W+1)^i+WB\norm*{\vf_i(\vx)-\vf_i^\prime(\vx)}_{\infty}+\delta \tag{\cref{eq:bounding_sup_form}}\\
        &\leq W\delta(B\vee 1)^i(W+1)^i+WBi(B\vee 1)^{i-1}(W+1)^i\delta+\delta \\
        &\leq\paren*{W(B\vee 1)^i(W+1)^i+iW(B\vee 1)^{i}(W+1)^i+1}\delta\\
        &=\paren*{(i+1)W(B\vee 1)^i(W+1)^i+1}\delta\\
        &\leq\left\{
             \begin{array}{lr}
             \paren*{W(i+1)B^i(W+1)^i+(i+1)B^i(W+1)^i}\delta, & B\geq 1, \\
             \paren*{W(i+1)(W+1)^i+(i+1)(W+1)^i}\delta, & B<1,  
             \end{array}\right.\\
        &= (i+1)(B\vee 1)^i(W+1)^{i+1}\delta
    \end{align*}
    which implies the statement is true for $l=i+1$, completing the induction steps.
    Therefore, it holds that for all $l\in[L]$, 
    \begin{align}
    \label{eq:parameter_Lip_nn}
        \norm{\vf_l(\vx)-\vf_l^\prime(\vx)}_{\infty}\leq l(B\vee 1)^{l-1}(W+1)^l\delta.
    \end{align}
    
    \paragraph{Discretization of entry space.} Let $\cS_\mathrm{entry}(\{(\vA^{(l)}, \vb^{(l)})\}_{l=1}^L)\coloneqq\bigcup_{l=1}^L\paren*{\cS_\mathrm{entry}(\vA^{(l)})\bigcup\cS_\mathrm{entry}(\vb^{(l)})}$ with $\cS_\mathrm{entry}(\vA)$ denotes the value space of all entries in $\vA$ and $\cS_\mathrm{entry}(\vb)$ denotes the value space of all entries in $b$. 
    % For $\cF(L, W, S, B)$, as defined in \cref{def:class_of_nn}, we have $\cS_\mathrm{entry}(\{(A^{(l)},b^{(l)})\}_{l=1}^L)=[-B,B]$. 
    Now we discretize the value spaces of $\cF(L, W, S, B)$ into $\delta$-width grids and get a finite class of neural network as 
    $\cF_{\delta\Z}(L, W, S, B) \coloneqq \{\vf\in\cF(L, W, S, B):\cS_\mathrm{entry}(\{(\vA^{(l)}, \vb^{(l)})\}_{l=1}^L))=[-B,B]\cap \delta\Z\}$, where $\delta\Z=\{k\delta \vert k\in\Z\}$
    Then, for any $\vf\in\cF(L, W, S, B)$ expressed as $\vf(x)=(\vA^{(L)}\relu(\cdot)+\vb^{(L)})\circ\dots\circ(\vA^{(1)}\vx+\vb^{(1)})$, there exists $\vf^\prime\in\cF_{\delta\Z}(L, W, S, B)$ expressed as $\vf^\prime(\vx)=({\vA^{(L)}}^\prime\relu(\cdot)+{\vb^{(L)}}^\prime)\circ\dots\circ({\vA^{(1)}}^\prime \vx+{\vb^{(1)}}^\prime)$ such that $\max_{l}\norm{\vA^{(l)}-{\vA^{(l)}}^\prime}_{\infty}\vee \norm{\vb^{(l)}-{\vb^{(l)}}^\prime}_{\infty}\leq \delta$.
    According to \cref{eq:parameter_Lip_nn}, we have for any $\vx\in[0,1]^{W_0}$, $\norm{\vf(\vx)-\vf^\prime(\vx)}_{\infty}\leq L(B\vee 1)^{L-1}(W+1)^L\delta.$
    Therefore, $\cF_{\delta\Z}(L, W, S, B)$ is an $L(B\vee 1)^{L-1}(W+1)^L\delta$-cover of $\cF(L, W, S, B)$ with respect to $\norm*{\cdot}_{\infty,L^{\infty}(\sX)}$ and thus we have
    \begin{align*}
        &\cov\paren*{L(B\vee 1)^{L-1}(W+1)^L\delta;\cF(L,W,S,B),\norm*{\cdot}_{\infty,L^{\infty}(\sX)}}\\
        \leq &\abs{\cF_{\delta\Z}(L, W, S, B)}=\abs*{\curl*{\{(\vA^{(l)},\vb^{(l)})\}_{l=1}^L: \cS_\mathrm{entry}(\vA^{(l)})=\cS_\mathrm{entry}(\vb^{(l)})=[-B,B]\cap \delta\Z}}\leq \paren*{\frac{2B}{\delta}+1}^S,
    \end{align*}
    which completes the proof.
\end{proof}

Here, we further establish the Lipschitz property of MLPs, which is useful in the following proofs for deriving the covering number for MLPs with an embedding layer applied to input data.

\begin{lemma}[Lipschitz property of MLPs about the input]
\label{lem:lip_of_nn_about_input}
    For any $\vf\in\cF(L, W, S, B)$ defined in Definition~\ref{def:class_of_nn}, $\vf$ is $B^LW^L$-Lipschitz continuous w.r.t. $\norm{\cdot}_{\infty}$ about $\vx$ on $\sX$, i.e., for any $\vx,\vx^\prime\in\sX\subset\R^{W_0}$, it holds that
    \begin{align*}
        \norm{\vf(\vx)-\vf(\vx^\prime)}_{\infty}\leq B^LW^L\norm{\vx-\vx^\prime}_{\infty}.
    \end{align*}
\end{lemma}
\begin{proof}
    Fix any $\vx,\vx^\prime\in\sX$. Given any $\vf\in\cF(L,W,S,B)$ expressed as $\vf(\vx)=(\vA^{(L)}\relu(\cdot)+\vb^{(L)})\circ\dots\circ(\vA^{(1)}\vx+\vb^{(1)}),$
    let $\vf_l(x)\coloneqq (\vA^{(l)}\relu(\cdot)+\vb^{(l)})\circ\dots\circ(\vA^{(1)}\vx+\vb^{(1)})$ for $l=2,\dots,L$ and $\vf_1(x)=\vA^{(1)}\vx+\vb^{(1)}$.
    We prove the statement that 
    $\norm{\vf_l(\vx)-\vf_l(\vx^\prime)}_{\infty}\leq B^lW^l\norm{\vx-\vx^\prime}_{\infty}$ for all $l\in[L]$ by induction. 
    When $l=1$, 
    \begin{align*}
        \norm{\vf_1(\vx)-\vf_1(\vx^\prime)}_{\infty}
        &=\norm{\vA^{(1)}\vx+\vb^{(1)}-\vA^{(1)}\vx^\prime-\vb^{(1)}}_{\infty}=\norm{\vA^{(1)}\paren*{\vx-\vx^\prime}}_{\infty}\\
        &\leq BW\norm{\vx-\vx^\prime}_{\infty}\tag{$W_0\leq W, \norm{\vA^{(1)}}_{\infty}\leq B$}\\
        &=B^1W^1\norm{\vx-\vx^\prime}_{\infty},
    \end{align*}
    which implies the statement is true for $l=1$.
    Assume that for some $l=i\geq 1$, $\norm{\vf_i(\vx)-\vf_i(\vx^\prime)}_{\infty}\leq B^iW^i\norm{\vx-\vx^\prime}_{\infty}$, then we have
    \begin{align*}
        \norm{\vf_{i+1}(\vx)-\vf_{i+1}(\vx^\prime)}_{\infty}
        &=\norm{\vA^{(i+1)}\relu\paren*{\vf_i(\vx)}+\vb^{(i+1)}-\vA^{(i+1)}\relu\paren*{\vf_i(\vx^\prime)}-\vb^{(i+1)}}_{\infty}\\
        &=\norm{\vA^{(i+1)}\paren*{\relu\paren*{\vf_i(\vx)}-\relu\paren*{\vf_i(\vx^\prime)}}}_{\infty}\\
        &\leq WB\norm{\relu\paren*{\vf_i(\vx)}-\relu\paren*{\vf_i(\vx^\prime)}}_{\infty}\tag{$W_{i}\leq W, \norm{\vA^{(i+1)}}_{\infty}\leq B$}\\
        &\leq  WB\norm{\vf_i(\vx)-\vf_i(\vx^\prime)}_{\infty}\tag{$\relu(\cdot)$ is $1$-Lipschitz continuous for Lemma~\ref{lem:lip_relu}}\\
        &\leq WBB^iW^i\norm{\vx-\vx^\prime}_{\infty}\\
        &= B^{i+1}W^{i+1}\norm{\vx-\vx^\prime}_{\infty},
    \end{align*}
    which implies the statement is true for $l=i+1$, completing the induction steps.
    Therefore, it holds that for all $l\in[L]$, 
    \begin{align*}
        \norm{\vf_l(\vx)-\vf_l(\vx^\prime)}_{\infty}\leq B^lW^l\norm{\vx-\vx^\prime}_{\infty}.
    \end{align*}
    Thus, when $l=L$, we have $\norm{\vf(\vx)-\vf(\vx^\prime)}_{\infty}=\norm{\vf_L(\vx)-\vf_L(\vx^\prime)}_{\infty}\leq B^LW^L\norm{\vx-\vx^\prime}_{\infty}$. 
\end{proof}


\subsection{Covering number of the logit space of ARMs}


We first characterize the output function space of the neural network without softmax operation, i.e., the unnormalized distribution parameter space, commonly referred to as logits in ARMS. This result serves as the foundation for deriving the bracketing number of the conditional probability mass function for each dimension.  
The derivation carefully analyzes the covering number of outputs for the entire network, which consists of the embedding layer, encoding layer, and an MLP. This analysis makes use of the previously established Lemma~\ref{lem:covering_number_of_nn} and Lemma~\ref{lem:lip_of_nn_about_input}.

\begin{lemma}[Covering number of the unnormalized distribution parameter vectors]
\label{lem:arm_covering_of_distribution_parameter_vector}
    Let $\vH_{\pnn}(\vx,y)\coloneqq\begin{bmatrix} \vh_{\pnn,1}(\vx,y)\ \cdots \ \vh_{\pnn,D}(\vx,y) \end{bmatrix}:\sX\times\sY\to\R^{M\times D}$ with $\vh_{\pnn,d}(\vx,y)=\vf_{\pmlp}\paren*{\vv_{\vA_0,\vb_0}^{\backslash \bm{0}_{D-d}}\!\paren*{\vE_{\vV_Y,\vV_X}(\vx,y)}}:\sX\times\sY\to\R^M$ as defined in \cref{sec:instantiation_arm}. 
    Let $\cH_{\pnn}=\{\vH_{\pnn}(\vx,y):\pmlp\in\cW(L,W,S,B), \vA_0\in[-B,B]^{D\times\de}, \vb_0\in[-B,B]^{D}, \vV_X\in[0,1]^{M\times\de}, \vV_Y\in[0,1]^{K\times\de}\}$ with constants $L,W,S,B>0$. 
    Then, given any $\epsilon>0$, we have
    \begin{align*}
        \cov\paren*{\epsilon;\cH_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}} \leq \paren*{\frac{3(L+3)(B\vee 1)^{L+2}(W+1)^L}{\epsilon}}^{S+D+(D+M+K)\de}
    \end{align*}
\end{lemma}
\begin{proof}
    For any $d\in[D]$, $\vh_{\pnn,d}$ can be written as $\vf_{\pmlp}\circ{\vv_{\vA_0,\vb_0}^{\backslash \bm{0}_{D-d}}\circ{\vE_{\vV_Y,\vV_X}}}$.
    Let the embedding space
    \begin{align*}
        \cG_{\alpha}\coloneqq\{&\vG_{\pnn}(\vx,y)=\begin{bmatrix} \vv_{\vA_0,\vb_0}^{\backslash \bm{0}_{D}}\circ{\vE_{\vV_Y,\vV_X}}(\vx,y)\ ,\cdots ,\ \vv_{\vA_0,\vb_0}^{\backslash \bm{0}_{0}}\circ{\vE_{\vV_Y,\vV_X}}(\vx,y) \end{bmatrix}: \alpha = \{\vA_0,\vb_0,\vV_X,\vV_Y\}, \\
        &\vA_0\in[-B,B]^{D\times\de}, \vb_0\in[-B,B]^{D}, \vV_X\in[0,1]^{M\times\de}, \vV_Y\in[0,1]^{K\times\de}\}.
    \end{align*}
    where 
    \begin{align*}
        \vv_{\vA_0,\vb_0}\circ\vE_{\vV_Y,\vV_X}(\vx,y) =\begin{bmatrix} \sigma\paren*{\vA_0[1,:]\vV_Y[y,:]^\top \!+\!\vb_0[1]} \\\vdots \\ \sigma\paren*{\vA_0[D,:]\vV_X[x_{D-1},:]^\top\!+\!\vb_0[D]} \end{bmatrix}=\sigma\paren*{\mathrm{diag}\paren*{\vA_0 \vE_{\vV_Y,\vV_X}(\vx,y)^\top}+\vb_0}\in[0,1]^D.
    \end{align*}
    
    Given any $\delta>0$, we first evaluate the $\delta$-covering number of $\cG_{\alpha}(\vx,y)$ w.r.t. $\norm*{\cdot}_{\infty,L^{\infty}(\sX_1)}$.

    \paragraph{Covering number of the embedding layer.}
    Let $\cS_\mathrm{entry}(\vA)$ denote the union of value spaces of all entries in $\vA$ and $\cS_\mathrm{entry}(\va)$ denote the union of value spaces of all entries in $\va$.
    We first discretize the value spaces of $\cG_{\alpha}$ into $\delta$-width grids to get a finite embedding function class:
    \begin{align*}
        \cG_{\alpha,\delta\Z} \coloneqq \{\vG_{\pnn} \in \cG_{\alpha}:\cS_\mathrm{entry}(\vA_0)=\cS_\mathrm{entry}(\vb_0)=[-B,B]\cap \delta\Z,  \cS_\mathrm{entry}(\vV_Y)=\cS_\mathrm{entry}(\vV_X)=[0,1]\cap \delta\Z\}.        
    \end{align*}
    % \zcy{I know what you mean, but it is not clear for readers.} 
    Denote by $\norm{\alpha-\alpha^\prime}_{\infty}\coloneqq \sup\curl*{\norm{\vA_0-\vA_0^\prime}_{\infty}, \norm{\vb_0-\vb_0^\prime}_{\infty}, \norm{\vV_Y-\vV_Y^\prime}_{\infty}, \norm{\vV_X-\vV_X^\prime}_{\infty}}.$
    For any $\vG_{\alpha} \in \cG_{\alpha}$ with $\alpha\coloneqq\{\vA_0,\vb_0,\vV_X,\vV_Y\}$, there exists $\vG_{\alpha^\prime}\in\cG_{\alpha,\delta\Z}$ with $\alpha^\prime\coloneqq\{\vA_0^\prime,\vb_0^\prime,\vV_X^\prime,\vV_Y^\prime\}$ such that $\norm{\alpha-\alpha^\prime}_{\infty}\leq \delta$.
    Then we have for any $\vx,y\in\sX\times\sY$, 
    \begin{align*}
        &\norm{\vG_{\alpha}(\vx,y)-\vG_{\alpha^\prime}(\vx,y)}_{\infty}\notag\\
        &=\norm*{\begin{bmatrix} \vv_{\vA_0,\vb_0}^{\backslash \bm{0}_{D}}\circ{\vE_{\vV_Y,\vV_X}}(\vx,y)-\vv_{\vA_0^\prime,\vb_0^\prime}^{\backslash \bm{0}_{D}}\circ{\vE_{\vV_Y^\prime,\vV_X^\prime}}(\vx,y),\ \cdots, \ \vv_{\vA_0,\vb_0}^{\backslash \bm{0}_{0}}\circ{\vE_{\vV_Y,\vV_X}}(\vx,y)-\vv_{\vA_0^\prime,\vb_0^\prime}^{\backslash \bm{0}_{0}}\circ{\vE_{\vV_Y^\prime,\vV_X^\prime}}(\vx,y) \end{bmatrix}}_{\infty}\\
        &\leq \norm*{\vv_{\vA_0,\vb_0}\circ{\vE_{\vV_Y,\vV_X}}(\vx,y)-\vv_{\vA_0^\prime,\vb_0^\prime}\circ{\vE_{\vV_Y^\prime,\vV_X^\prime}}(\vx,y)}_{\infty}\\
        &=\norm*{\sigma\paren*{\mathrm{diag}\paren*{\vA_0 \vE_{\vV_Y,\vV_X}(\vx,y)^\top}+\vb_0}-\sigma\paren*{\mathrm{diag}\paren*{\vA_0^\prime \vE_{\vV_Y\prime,\vV_X\prime}(\vx,y)^\top}-\vb_0^\prime}}_{\infty}\notag\\
        &\leq \norm*{{\mathrm{diag}\paren*{\vA_0 \vE_{\vV_Y,\vV_X}(\vx,y)^\top}+\vb_0}-{\mathrm{diag}\paren*{\vA_0^\prime \vE_{\vV_Y\prime,\vV_X\prime}(\vx,y)^\top}-\vb_0^\prime}}_{\infty}\tag{$\sigma(\cdot)$ is $1$-Lipschitz continuous for Lemma~\ref{lem:lip_relu}}\\
        &\leq \sup_{d\in[D]}\abs*{\vA_0[d,:]\vE_{\vV_Y,\vV_X}(\vx,y)[d,:]^\top-\vA_0^\prime[d,:]\vE_{\vV_Y^\prime,\vV_X^\prime}(\vx,y)[d,:]^\top}+\norm*{\vb_0-\vb_0^\prime}_{\infty}\notag\\
        &\leq \sup_{d\in[D]}\abs*{(\vA_0[d,:]-\vA_0^\prime[d,:])\vE_{\vV_Y,\vV_X}(\vx,y)[d,:]^\top}+\abs*{\vA_0^\prime[d,:](\vE_{\vV_Y,\vV_X}(\vx,y)[d,:]^\top-\vE_{\vV_Y^\prime,\vV_X^\prime}(\vx,y)[d,:]^\top)}+\delta\notag\\
        &\leq \sup_{d\in[D]}\de \delta\norm*{\vE_{\vV_Y,\vV_X}(\vx,y)[d,:]}_{\infty}+\de B\norm*{\vE_{\vV_Y,\vV_X}(\vx,y)[d,:]-\vE_{\vV_Y^\prime,\vV_X^\prime}(\vx,y)[d,:]}_{\infty}+\delta\tag{$\vA_0\in[-B,B]^{D\times \de}$,$\norm{\vA_0-\vA_0^\prime}_{\infty}\leq \delta$}\\
        &=\left\{
             \begin{array}{lr}
             \de \delta\norm*{\vV_Y[y,:]}_{\infty}+\de B\norm*{\vV_Y[y,:]-\vV_Y^\prime[y,:]}_{\infty}+\delta, & d= 1, \\
             \sup_{d\in[D]}\de \delta\norm*{\vV_X[x_{d-1},:]}_{\infty}+\de B\norm*{\vV_X[x_{d-1},:]-\vV_X^\prime[x_{d-1},:]}_{\infty}+\delta, & d =2,\dots,D,  
             \end{array}\right.\notag\\
        &\leq \de \delta+\de B\delta+\delta\tag{$\norm{\vV_Y}_{\infty}\leq 1$, $\norm{\vV_X}_{\infty}\leq 1$, $\norm{\vV_Y-\vV_Y^\prime}_{\infty}\leq \delta$, $\norm{\vV_X-\vV_X^\prime}_{\infty}\leq \delta$}\\
        &=(1+\de+\de B)\delta. 
    \end{align*}
    Therefore, $\cG_{\alpha,\delta\Z}$ is an $(1+\de+\de B)\delta$-cover of $\cG_{\alpha}$ w.r.t. $\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}$ and thus we have
    \begin{align}
        &\cov\paren*{(1+\de+\de B)\delta;\cG_{\alpha},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}}\leq \abs{\cG_{\alpha,\delta\Z}}\notag\\
        =&\abs*{\{\alpha=(\vA_0,\vb_0,\vV_Y,\vV_X):\cS_\mathrm{entry}(\vA_0)=\cS_\mathrm{entry}(\vb_0)=[-B,B]\cap \delta\Z,  \cS_\mathrm{entry}(\vV_Y)=\cS_\mathrm{entry}(\vV_X)=[0,1]\cap \delta\Z\}}\notag\\
        \leq& \paren*{\frac{2B}{\delta}+1}^{D\de+D}\paren*{\frac{1}{\delta}+1}^{M\de+K\de}. \label{eq:ebm_cover_encoded_embedding}
    \end{align}

    \paragraph{Composition of an MLP.}
    Let $\cC_{\cF}$ be an $\epsilon_{\cF}=L(B\vee 1)^{L-1}(W+1)^L\delta$-cover of $\cF\{L,W,S,B\}$ w.r.t. $\norm*{\cdot}_{\infty,L^{\infty}([0,1]^D)}$ such that $\abs{\cC_{\cF}}=\cov\paren*{\epsilon_{\cF};\cF(L,W,B,S),\norm*{\cdot}_{\infty,L^{\infty}([0,1]^D)}}$ and $\cC_{\cG}$ be an $\epsilon_{\cG}=(1+\de+\de B)\delta$-cover of $\cG_{\alpha}$ w.r.t. $\norm*{\cdot}_{\infty,L^{\infty}([0,1]^D)}$ such that $\abs{\cC_{\cG}}=\cov\paren*{\epsilon_{\cG};\cG_{\alpha},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}}$.    
    For any $\vH_{\pnn}\coloneqq\begin{bmatrix}\vf_{\pmlp}\circ\vG_{\alpha}[:,1]\ \cdots \ \vf_{\pmlp}\circ\vG_{\alpha}[:,D] \end{bmatrix}\in\cH_{\pnn}$, there exists $\vf_{\pmlp}^\prime\in\cC_{\cF}$ and $\vG_{\alpha}^\prime\in\cC_{\cG}$ such that 
    \begin{align*}
        \forall \vv\in[0,1]^D, \norm{\vf_{\pmlp}(\vv)-\vf_{\pmlp}^\prime(\vv)}_{\infty}\leq \epsilon_{\cF}, \ \text{ and }\ 
        \forall \vx,y\in\sX\times\sY, \norm{\vG_{\alpha}(\vx,y)-\vG_{\alpha}^\prime(\vx,y)}_{\infty}\leq \epsilon_{\cG}.
    \end{align*}
    Let $\vH_{\pnn}^\prime\coloneqq\begin{bmatrix}\vf_{\pmlp}^\prime\circ\vG_{\alpha}^\prime[:,1]\ \cdots \ \vf_{\pmlp}^\prime\circ\vG_{\alpha}^\prime[:,D] \end{bmatrix}.$
    We have for all $\vx,y\in\sX\times\sY$,
    \begin{align*}
        \norm{\vH_{\pnn}-\vH_{\pnn}^\prime}_{\infty}
        &=\norm*{\begin{bmatrix}\vf_{\pmlp}\circ\vG_{\alpha}[:,1]-\vf_{\pmlp}^\prime\circ\vG_{\alpha}^\prime[:,1],\ \cdots, \ \vf_{\pmlp}\circ\vG_{\alpha}[:,D]-\vf_{\pmlp}^\prime\circ\vG_{\alpha}^\prime[:,D] \end{bmatrix}}_{\infty}\\
        &=\sup_{d}\norm*{\vf_{\pmlp}\circ\vG_{\alpha}[:,d]-\vf_{\pmlp}^\prime\circ\vG_{\alpha}^\prime[:,d]}_{\infty}\\
        &\leq\sup_{d}\curl*{\norm*{\vf_{\pmlp}\circ\vG_{\alpha}[:,d]-\vf_{\pmlp}^\prime\circ\vG_{\alpha}[:,d]}+\norm*{\vf_{\pmlp}^\prime\circ\vG_{\alpha}[:,d]-\vf_{\pmlp}^\prime\circ\vG_{\alpha}^\prime[:,d]}_{\infty}}\\
        &\leq \sup_{d}\curl*{\epsilon_{\cF}+B^LW^L\epsilon_{\cG}} \tag{$\vf_{\pmlp}$ is $B^LW^L$-Lipschitz continuous as in Lemma~\ref{lem:lip_of_nn_about_input}}\\
        &= \epsilon_{\cF}+B^LW^L\epsilon_{\cG}.
    \end{align*}
    Therefore, $\cC_{\cH}\coloneqq\{\vH_{\pnn}^\prime\coloneqq\begin{bmatrix}\vf_{\pmlp}^\prime\circ\vG_{\alpha}^\prime[:,1]\ \cdots \ \vf_{\pmlp}^\prime\circ\vG_{\alpha}^\prime[:,D] \end{bmatrix}: \vf_{\pmlp}^\prime\in\cC_{\cF}, \vG_{\alpha}^\prime\in\cC_{\cG}\}$ is an $\epsilon_{\cF}+B^LW^L\epsilon_{\cG}$-cover of $\cH_{\pnn}$ w.r.t. $\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}$, and thus
    \begin{align*}
        &\cov\paren*{ \paren*{L(B\vee 1)^{L-1}(W+1)^L+B^LW^L(1+\de+\de B)}\delta;\cH_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}}\\
        \leq&\abs{\cC_{\cH}}\leq\abs{\cC_{\cF}}\abs{\cC_{\cG}}\\
        =&\cov\paren*{L(B\vee 1)^{L-1}(W+1)^L\delta;\cF(L,W,B,S),\norm*{\cdot}_{\infty,L^{\infty}([0,1]^D)}}\cov\paren*{(1+\de+\de B)\delta;\cG_{\alpha},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}}\\
        \leq & \paren*{\frac{2B}{\delta}+1}^S\paren*{\frac{2B}{\delta}+1}^{D\de+D}\paren*{\frac{1}{\delta}+1}^{M\de+K\de}\tag{Lemma~\ref{lem:covering_number_of_nn} and \cref{eq:ebm_cover_encoded_embedding}}\\
        \leq&\paren*{\frac{(2B\vee 1)}{\delta}+1}^{S+D\de+D+M\de+K\de}\\
        \leq&\paren*{\frac{3(B\vee 1)}{\delta}}^{S+D+(D+M+K)\de}.\tag{$\frac{(B\vee 1)}{\delta}\geq 1$}
    \end{align*}
    Taking $\epsilon=\paren*{L(B\vee 1)^{L-1}(W+1)^L+B^LW^L(1+\de+\de B)}\delta$, we have
    \begin{align*}
        \cov\paren*{\epsilon;\cH_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}} 
        &\leq \paren*{\frac{3(B\vee 1)\paren*{L(B\vee 1)^{L-1}(W+1)^L+B^LW^L(1+\de+\de B)}}{\epsilon}}^{S+D+(D+M+K)\de}\\
        &\leq \paren*{\frac{3(B\vee 1)\paren*{L(B\vee 1)^{L-1}(W+1)^L\de (B\vee 1)+3B^LW^L\de (B\vee 1))}}{\epsilon}}^{S+D+(D+M+K)\de} \tag{$\de,(B\vee 1)\geq 1$}\\
        &\leq \paren*{\frac{3(B\vee 1)\paren*{L(B\vee 1)^{L+1}(W+1)^L\de+3(B\vee 1)^{L+1}(W+1)^L\de)}}{\epsilon}}^{S+D+(D+M+K)\de} \\
        &=\paren*{\frac{3(L+3)(B\vee 1)^{L+2}(W+1)^L}{\epsilon}}^{S+D+(D+M+K)\de}.
    \end{align*}
\end{proof}

\subsection{Bracketing number of conditional probability space on each dimension}

\begin{lemma}[Bracketing number of conditional probability space on each dimension]
\label{lem:arm_bracket_density_class_via_parameter_vector}
    Let $\vP_{\pnn}(\vx,y)\coloneqq\begin{bmatrix} \vparm_{\pnn}(y)\ \cdots \ \vparm_{\pnn}(\vx_{<D},y) \end{bmatrix}=\mathrm{softmax}\paren*{\vH_{\pnn}(\vx,y)}:\sX\times\sY\to[0,1]^{M\times D}$ where $\mathrm{softmax}(\vH)$ denotes element-wise softmax operation that $\mathrm{softmax}(\vH)[m:d]=\frac{e^{\vH[m,d]}}{\sum_{i=1}^Me^{\vH[i,d]}}$.
    Given a class of autoregressive conditional distributions that $\cP_{X\vert Y}=\curl*{p_{\pnn}\paren*{\vx\vert y}=p\paren*{x_1; \vparm_{\pnn}(y)}\cdots p\paren*{x_D; \vparm_{\pnn}(\vx_{<D},y)}:\vH_{\pnn}\in\cH_{\pnn}}$ with $p_{\pnn}\paren*{\vx\vert y}$ as defined in \cref{sec:instantiation_arm}, then for any $0<\epsilon\leq 1$, it holds that 
    \begin{align*}
        \br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}
        \leq \cov\paren*{\frac{\epsilon}{8ed};\cH_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}}. 
    \end{align*}
\end{lemma}
\begin{proof}
    Let $\cC_{\cH}$ be an $\epsilon_{\cH}$-cover of $\cH_{\pnn}$ that $\abs{\cC_{\cH}}=\cov\paren*{\epsilon_{\cH};\cH_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}}$.
    According to the definition in \cref{sec:instantiation_arm}, 
    \begin{align*}
        &p_{\pnn}\paren*{\vx\vert y}=p\paren*{x_1; \vparm_{\pnn}(y)}\cdots p\paren*{x_D; \vparm_{\pnn}(\vx_{<D},y)}\\
        =&p\paren*{x_1; \vP_{\pnn}[:,1]}\cdots p\paren*{x_D;\vP_{\pnn}(\vx,y)[:,D]}=\prod_{d=1}^D p\paren*{x_d; \vP_{\pnn}(\vx,y)[:,d]}=\prod_{d=1}^D \vP_{\pnn}(\vx,y)[x_d,d]\\
        =&\prod_{d=1}^D \prod_{m=1}^M\paren*{\mathrm{softmax}(\vH_{\pnn}(\vx,y))[x_d,d]}^{\I(x_d=m)}=\prod_{d=1}^D \prod_{m=1}^M\paren*{\frac{e^{\vH_{\pnn}(\vx,y)[m,d]}}{\sum_{i=1}^Me^{\vH_{\pnn}(\vx,y)[i,d]}}}^{\I(x_d=m)}.
    \end{align*}
    Then, for any $p_{\pnn}\in\cP_{X\vert Y}$, there exists $\vH_{\pnn}^\prime\in\cC_{\cH}$ such that for all $\vx,y\in\sX\times\sY$, $\norm{\vH_{\pnn}(\vx,y)-\vH_{\pnn}^\prime(\vx,y)}\leq \epsilon_{\cH}$ which equals to $\forall m\in[M],d\in[D]$, $\vH_{\pnn}^\prime(\vx,y)[m,d]-\epsilon_{\cH} \leq\vH_{\pnn}(\vx,y)[m,d]\leq \vH_{\pnn}^\prime(\vx,y)[m,d]+\epsilon_{\cH}$.
    Let $p_{\pnn}^\prime\paren*{\vx\vert y}=\prod_{d=1}^D \prod_{m=1}^M\paren*{\frac{e^{\vH_{\pnn}^\prime(\vx,y)[m,d]+\epsilon_{\cH}}}{\sum_{i=1}^Me^{\vH_{\pnn}^\prime(\vx,y)[i,d]-\epsilon_{\cH}}}}^{\I(x_d=m)}$ and denote $\vP_{\pnn}^\prime(\vx,y)[x_d,d]\coloneqq \prod_{m=1}^M\paren*{\frac{e^{\vH_{\pnn}^\prime(\vx,y)[m,d]+\epsilon_{\cH}}}{\sum_{i=1}^Me^{\vH_{\pnn}^\prime(\vx,y)[i,d]-\epsilon_{\cH}}}}^{\I(x_d=m)}$.
    We immediately have: for all $\vx,y\in\sX\times\sY$, $p_{\pnn}^\prime\paren*{\vx\vert y}\geq p_{\pnn}\paren*{\vx\vert y},$ 
    since $\forall m\in[M], d\in[D]$, $e^{\vH_{\pnn}^\prime(\vx,y)[m,d]+\epsilon_{\cH}}\geq e^{\vH_{\pnn}(\vx,y)[m,d]}$ and $e^{\vH_{\pnn}^\prime(\vx,y)[i,d]-\epsilon_{\cH}}\leq e^{\vH_{\pnn}(\vx,y)[i,d]}$.
    Moreover, we have for all $\vx,y\in\sX\times\sY$, 
    \begin{align}
        &\vP_{\pnn}^\prime(\vx,y)[x_d,d]-\vP_{\pnn}(\vx,y)[x_d,d]\notag\\
        =&\prod_{m=1}^M\paren*{\frac{e^{\vH_{\pnn}^\prime(\vx,y)[m,d]+\epsilon_{\cH}}}{\sum_{i=1}^Me^{\vH_{\pnn}^\prime(\vx,y)[i,d]-\epsilon_{\cH}}}}^{\I(x_d=m)}-\prod_{m=1}^M\paren*{\frac{e^{\vH_{\pnn}(\vx,y)[m,d]}}{\sum_{i=1}^Me^{\vH_{\pnn}(\vx,y)[i,d]}}}^{\I(x_d=m)}\notag\\
        =&\prod_{m=1}^M\paren*{\frac{e^{\vH_{\pnn}^\prime(\vx,y)[m,d]+2\epsilon_{\cH}}}{\sum_{i=1}^Me^{\vH_{\pnn}^\prime(\vx,y)[i,d]}}-\frac{e^{\vH_{\pnn}(\vx,y)[m,d]}}{\sum_{i=1}^Me^{\vH_{\pnn}(\vx,y)[i,d]}}}^{\I(x_d=m)}\notag\\
        =&\prod_{m=1}^M\paren*{\frac{\paren*{e^{\vH_{\pnn}^\prime(\vx,y)[m,d]+2\epsilon_{\cH}}
        -e^{\vH_{\pnn}(\vx,y)[m,d]}}\sum_{i=1}^Me^{\vH_{\pnn}(\vx,y)[i,d]}
        +e^{\vH_{\pnn}(\vx,y)[m,d]}{\sum_{i=1}^M\paren*{e^{\vH_{\pnn}(\vx,y)[i,d]}
        -e^{\vH_{\pnn}^\prime(\vx,y)[i,d]}}}}{\sum_{i=1}^Me^{\vH_{\pnn}^\prime(\vx,y)[i,d]}\sum_{i=1}^Me^{\vH_{\pnn}(\vx,y)[i,d]}}}^{\I(x_d=m)}\notag\\
        \leq& \prod_{m=1}^M\paren*{\frac{e^{\vH_{\pnn}^\prime(\vx,y)[m,d]+2\epsilon_{\cH}}3\epsilon_{\cH}\sum_{i=1}^Me^{\vH_{\pnn}(\vx,y)[i,d]}
        +e^{\vH_{\pnn}(\vx,y)[m,d]}{\sum_{i=1}^M{e^{\vH_{\pnn}^\prime(\vx,y)[i,d]+\epsilon_{\cH}}\epsilon_{\cH}}}}{\sum_{i=1}^Me^{\vH_{\pnn}^\prime(\vx,y)[i,d]}\sum_{i=1}^Me^{\vH_{\pnn}(\vx,y)[i,d]}}}^{\I(x_d=m)}\tag{$\abs{e^a-e^b}\leq e^{a\vee b}\abs{a-b}$ and $\norm{\vH_{\pnn}(\vx,y)-\vH_{\pnn}^\prime(\vx,y)}\leq \epsilon_{\cH}$}\\
        \leq& \prod_{m=1}^M\paren*{\frac{e^{\vH_{\pnn}(\vx,y)[m,d]+3\epsilon_{\cH}}3\epsilon_{\cH}\sum_{i=1}^Me^{\vH_{\pnn}^\prime(\vx,y)[i,d]+\epsilon_{\cH}}
        +e^{\vH_{\pnn}(\vx,y)[m,d]}{\sum_{i=1}^M{e^{\vH_{\pnn}^\prime(\vx,y)[i,d]+\epsilon_{\cH}}\epsilon_{\cH}}}}{\sum_{i=1}^Me^{\vH_{\pnn}^\prime(\vx,y)[i,d]}\sum_{i=1}^Me^{\vH_{\pnn}(\vx,y)[i,d]}}}^{\I(x_d=m)}\tag{$\norm{\vH_{\pnn}(\vx,y)-\vH_{\pnn}^\prime(\vx,y)}\leq \epsilon_{\cH}$}\\
        =&\prod_{m=1}^M\paren*{\frac{e^{\vH_{\pnn}(\vx,y)[m,d]+3\epsilon_{\cH}}3\epsilon_{\cH}e^{\epsilon_{\cH}}
        +e^{\vH_{\pnn}(\vx,y)[m,d]}e^{\epsilon_{\cH}}\epsilon_{\cH}}{\sum_{i=1}^Me^{\vH_{\pnn}(\vx,y)[i,d]}}}^{\I(x_d=m)}\tag{$\norm{\vH_{\pnn}(\vx,y)-\vH_{\pnn}^\prime(\vx,y)}\leq \epsilon_{\cH}$}\\
        =& \paren*{3\epsilon_{\cH}e^{4\epsilon_{\cH}}+e^{\epsilon_{\cH}}\epsilon_{\cH}}\prod_{m=1}^M\paren*{\frac{e^{\vH_{\pnn}(\vx,y)[m,d]}}{\sum_{i=1}^Me^{\vH_{\pnn}(\vx,y)[i,d]}}}^{\I(x_d=m)}\notag\\
        =&\paren*{3\epsilon_{\cH}e^{4\epsilon_{\cH}}+e^{\epsilon_{\cH}}\epsilon_{\cH}}\vP_{\pnn}(\vx,y)[x_d,d]\leq 4\epsilon_{\cH}e^{4\epsilon_{\cH}}\vP_{\pnn}(\vx,y)[x_d,d]. \label{eq:arm_paremeter_vector_distance}
    \end{align}
    
    Given any $y\in\sY$, denoting $a_d\coloneqq\sum_{\vx_{\leq d}\in[M]^{d}}\abs*{p_{\pnn}^\prime(\vx_{\leq d}\vert y)-p_{\pnn}(\vx_{\leq d}\vert y)}$ for $d=1, \dots, D$, we have the following recursive formula for $\{a_d\}_{d\in[D]}$:
    \begin{align*}
        &a_d=\sum_{\vx_{\leq d}\in[M]^{d}}\abs*{p_{\pnn}^\prime(\vx_{\leq d}\vert y)-p_{\pnn}(\vx_{\leq d}\vert y)}=\sum_{\vx_{\leq d}\in[M]^{d}}\abs*{\prod_{j=1}^d \vP_{\pnn}^\prime(\vx,y)[x_j,j]-\prod_{j=1}^d \vP_{\pnn}(\vx,y)[x_j,j]}\\
        =&\sum_{\vx_{\leq d}\in[M]^{d}}\abs*{\paren*{\prod_{j=1}^{d-1} \vP_{\pnn}^\prime(\vx,y)[x_j,j]-\prod_{j=1}^{d-1}\vP_{\pnn}(\vx,y)[x_j,j]} \vP_{\pnn}^\prime(\vx,y)[x_d,d]}\\
        &+\sum_{\vx_{\leq d}\in[M]^{d}}\abs*{\prod_{j=1}^{d-1}\vP_{\pnn}(\vx,y)[x_j,j] \paren*{\vP_{\pnn}^\prime(\vx,y)[x_d,d]-\vP_{\pnn}(\vx,y)[x_d,d]}}\\
        \leq&\sum_{\vx_{\leq d-1}\in[M]^{d-1}}\abs*{p_{\pnn}^\prime(\vx_{\leq d-1}\vert y)-p_{\pnn}(\vx_{\leq d-1}\vert y)}\paren*{\sum_{x_d\in[M]}\vP_{\pnn}^\prime(\vx,y)[x_d,d]}+4\epsilon_{\cH}e^{4\epsilon_{\cH}}\sum_{\vx_{\leq d}\in[M]^{d}}\prod_{j=1}^{d}\vP_{\pnn}(\vx,y)[x_j,j]\tag{\cref{eq:arm_paremeter_vector_distance}}\\
        =&a_{d-1}{\sum_{x_d\in[M]}\vP_{\pnn}^\prime(\vx,y)[x_d,d]}+4\epsilon_{\cH}e^{4\epsilon_{\cH}}\sum_{\vx_{\leq d}\in[M]^{d}}p_{\pnn}(\vx_{\leq d-1}\vert y)\\
        =&e^{2\epsilon_{\cH}}a_{d-1}+4\epsilon_{\cH}e^{4\epsilon_{\cH}}\tag{$\sum_{x_d\in[M]}\vP_{\pnn}^\prime(\vx,y)[x_d,d]=e^{2\epsilon_{\cH}}$ and $\sum_{\vx_{\leq d}\in[M]^d}p_{\pnn}\paren*{\vx_{\leq d}\vert y}=1$}
    \end{align*}
    According to this recursive relation, and 
    \begin{align*}
        a_1
        &=\sum_{x_1\in[M]}\abs*{p_{\pnn}^\prime(x_1\vert y)-p_{\pnn}(x_1\vert y)}=\sum_{x_1\in[M]}\abs*{ \vP_{\pnn}^\prime(\vx,y)[x_1,1]-\vP_{\pnn}(\vx,y)[x_1,1]}\\
        &\leq \sum_{x_1\in[M]}4\epsilon_{\cH}e^{4\epsilon_{\cH}}\vP_{\pnn}(\vx,y)[x_d,d]=4\epsilon_{\cH}e^{4\epsilon_{\cH}}\tag{\cref{eq:arm_paremeter_vector_distance}},
    \end{align*}
    we have $a_d\leq 4\epsilon_{\cH}e^{4\epsilon_{\cH}}\frac{e^{2d\epsilon_{\cH}}-1}{e^{2\epsilon_{\cH}}-1}$. Therefore, 
    \begin{align*}
        \sum_{\vx\in[M]^{D}}\abs*{p_{\pnn}^\prime(\vx\vert y)-p_{\pnn}(\vx\vert y)}&=\sum_{\vx_{\leq D}\in[M]^{D}}\abs*{p_{\pnn}^\prime(\vx_{\leq D}\vert y)-p_{\pnn}(\vx_{\leq D}\vert y)}\\
        &=a_d\leq 4\epsilon_{\cH}e^{4\epsilon_{\cH}}\frac{e^{2d\epsilon_{\cH}}-1}{e^{2\epsilon_{\cH}}-1}
        \leq 4\epsilon_{\cH}e^{4\epsilon_{\cH}}\frac{e^{2d\epsilon_{\cH}}-1}{2\epsilon_{\cH}}=2e^{4\epsilon_{\cH}}\paren*{e^{2d\epsilon_{\cH}}-1}.
    \end{align*}
    Suppose that $\epsilon_{\cH}\in(0,\frac{1}{4d})$, we have $e^{4\epsilon_{\cH}}\leq e^{\frac{1}{d}}\leq e$ and $e^{2d\epsilon_{\cH}}-1\leq 4d\epsilon_{\cH}$ as $e^x\leq 1+2x$ for $x\in[0,1]$, and thus $2e^{4\epsilon_{\cH}}\paren*{e^{2d\epsilon_{\cH}}-1}\leq 8ed\epsilon$.
    Therefore, given any $y\in\sY$, the $L^1(\sX)$ distance between $p_{\pnn}^\prime(\cdot\vert y)$ and $p_{\pnn}(\cdot\vert y)$ can be bounded as
    \begin{align*}
        \norm{p_{\pnn}^\prime(\cdot\vert y)-p_{\pnn}(\cdot\vert y)}_{L^1(\sX)}
        =\sum_{\vx\in[M]^{D}}\abs*{p_{\pnn}^\prime(\vx\vert y)-p_{\pnn}(\vx\vert y)}\leq 2e^{4\epsilon_{\cH}}\paren*{e^{2d\epsilon_{\cH}}-1}= 8ed\epsilon_{\cH}.
    \end{align*}    
    Therefore, $\cB_{\cP}\coloneqq \curl*{p_{\pnn}^\prime\paren*{\vx\vert y}=\prod_{d=1}^D \prod_{m=1}^M\paren*{\frac{e^{\vH_{\pnn}^\prime(\vx,y)[m,d]+\epsilon_{\cH}}}{\sum_{i=1}^Me^{\vH_{\pnn}^\prime(\vx,y)[i,d]-\epsilon_{\cH}}}}^{\I(x_d=m)}:\vH_{\pnn}^\prime\in\cC_{\cH}}$ is an $8ed\epsilon_{\cH}$-upper bracket w.r.t. $L^1(\sX)$ of $\cP_{X\vert Y}$, and we have
    \begin{align*}
        \br\paren*{8ed\epsilon_{\cH};\cP_{X\vert Y},L^1(\sX)}
        \leq \abs{\cB_{\cP}} \leq \abs{\cC_{\cH}}
        &=\cov\paren*{\epsilon_{\cH};\cH_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}}.
    \end{align*}
    Letting $8ed\epsilon_{\cH}=\epsilon\in(0,1]$, we have $\epsilon_{\cH}\leq\frac{1}{8ed}<\frac{1}{4d}$, and thus
    \begin{align*}
        \br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}
        \leq \cov\paren*{\frac{\epsilon}{8ed};\cH_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}}.
    \end{align*}
    
\end{proof}

\subsection{Proof of \cref{thm:tv_upper_ar}}
\label{app:proof_of_ar}

Based on the relation between the bracketing number of conditional distribution space $\cP_{X\vert Y}$ and the covering number of output logit space of the neural network $\cH_{\pnn}$ derived in previous lemmas, we obtain the final result.

\begin{proof}[Proof of \cref{thm:tv_upper_ar}]

    With conditional distributions as defined in \cref{eq:conditional_density_ar}, we have $$\cP_{X\vert Y}^\multi=\curl*{p_{\pnn}\paren*{\vx\vert y}=p\paren*{x_1; \vparm_{\pnn}(y)}\cdots p\paren*{x_D; \vparm_{\pnn}(\vx_{<D},y)}:\vH_{\pnn}\in\cH_{\pnn}}$$
    where $\vH_{\pnn}(\vx,y)=\begin{bmatrix} \vh_{\pnn,1}(\vx,y)\ \cdots \ \vh_{\pnn,D}(\vx,y) \end{bmatrix}:\sX\times\sY\to\R^{M\times D}$ with $\vh_{\pnn,d}(\vx,y)=\vf_{\pmlp}\paren*{\vv_{\vA_0,\vb_0}^{\backslash \bm{0}_{D-d}}\!\paren*{\vE_{\vV_Y,\vV_X}(\vx,y)}}:\sX\times\sY\to\R^M$ as defined in \cref{sec:instantiation_arm}. 
    
    According to Lemma~\ref{lem:arm_bracket_density_class_via_parameter_vector} and Lemma~\ref{lem:arm_covering_of_distribution_parameter_vector}, 
    \begin{align*}
        \br\paren*{\frac{1}{n};\cP_{X\vert Y},L^1(\sX)}
        \leq \cov\paren*{\frac{1}{8edn};\cH_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}}\leq\paren*{24edn(L+3)(B\vee 1)^{L+2}(W+1)^L}^{S+D+(D+M+K)\de}.
    \end{align*}
    According to \cref{thm:TV_upper_of_conditional_MLE}, we arrive at the conclusion that 
    \begin{align*}
        \rtv(\hat{p}_{X\vert Y}^{\multi})
        &\leq 3\sqrt{\frac{1}{n}\paren*{\log\br\paren*{\frac{1}{n};\cP_{X\vert Y}^{\multi},L^1(\sX)}+\log\frac{1}{\delta}}}\\
        &\leq 3\sqrt{\frac{1}{n}\paren*{\paren*{S+D+(D+M+K)\de} \log\paren*{24edn(L+3)(B\vee 1)^{L+2}(W+1)^L}+\log\frac{1}{\delta}}}
    \end{align*}
    Omitting constants about $n,K,\de, L,W,S,B$, and the logarithm term we have $\rtv(\hat{p}_{X\vert Y}^{\multi})=\tilde{\cO}\paren*{\sqrt{\frac{L\paren*{S+D+(D+M+K)\de}}{n}}}$.    
\end{proof}

\subsection{Average TV error bound under single-source training}

\begin{theorem}[Average TV error bound for ARMs under single-source training]
\label{thm:tvbound_arm_single}
    Let $\hat{p}_{X\vert Y}^{\single}$ be the likelihood maximizer defined in \cref{eq:single_mle_solution} given $\cP_{X\vert Y}^{\single}$ with conditional distributions as in \cref{eq:conditional_density_ar}. Suppose that $\Phi=[0,1]^{\de}$ and $\Psi = \cW(L,W,S,B)$ and assume $\phi_k^*\in \Phi$, $\psi^*\in \Psi$. Then, for any $0<\delta\leq 1/2$, it holds with probability at least $1-\delta$ that
    \begin{align*}
    \rtv(\hat{p}_{X\vert Y}^{\single})=\tilde{\cO}\paren*{\sqrt{\frac{KL\paren*{S+D+(D+M+1)\de}}{n}}}.
    \end{align*}
\end{theorem}

\begin{proof}
As formulated in \cref{sec:formulation_for_conditional_generative_modeling} and with conditional distributions as in \cref{eq:conditional_density_ar}, we have 
        \begin{align*}
        \cP_{X\vert Y}^{\single}
        =\curl*{\prod_{k=1}^K\paren*{p_{\pnn_k}(\vx\vert y)}^{\I(y=k)}: 
        p_{\pnn_k}(\vx\vert y)=p\paren*{x_1; \vparm_{\pnn_k}(y)}\cdots p\paren*{x_D; \vparm_{\pnn_k}(\vx_{<D},y)}:\vH_{\pnn_k}\in\cH_{\pnn_k} },
    \end{align*}
where $\vH_{\pnn_k}(\vx,y)=\begin{bmatrix} \vh_{\pnn_k,1}(\vx,y)\ \cdots \ \vh_{\pnn_k,D}(\vx,y) \end{bmatrix}:\sX\times\sY\to\R^{M\times D}$ with $\vh_{\pnn_k,d}(\vx,y)=\vf_{\pmlp_k}\paren*{\vv_{\vA_{0k},\vb_{0k}}^{\backslash \bm{0}_{D-d}}\!\paren*{\vE_{\vV_Y[k,:],\vV_{Xk}}(\vx,y)}}:\sX\times\sY\to\R^M$ as defined in \cref{sec:instantiation_arm}. 

    
    where 
    \begin{align*}
        \cU_{\pnn_k}=\curl*{u_{\pnn_k}(\vx\vert y)=f_{\pmlp_k}\circ \ve_{\vV[k,:]}(\vx,y): \pmlp_k\in\cW(L,W,S,B), \vV[k,:]\in[0,1]^{\de}}.
    \end{align*}
    For all $k\in[K]$, let $\cB_{\cP_k}$ be an $\frac{1}{n}$-upper bracket of $\cP_{X\vert Y,k}=\curl*{p_{\pnn_k}(\vx\vert y)=p\paren*{x_1; \vparm_{\pnn_k}(y)}\cdots p\paren*{x_D; \vparm_{\pnn_k}(\vx_{<D},y)}:\vH_{\pnn_k}\in\cH_{\pnn_k} }$ w.r.t. $L^1(\sX)$ such that $\abs{\cB_{\cP_k}}=\br\paren*{\frac{1}{n};\cP_{X\vert Y,k},L^1(\sX)}$. 
    According to Lemma~\ref{lem:arm_covering_of_distribution_parameter_vector} and Lemma~\ref{lem:arm_bracket_density_class_via_parameter_vector}, we know that 
    $$\abs{\cB_{\cP_k}}\leq \cov\paren*{\frac{1}{8edn};\cH_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}} \leq \paren*{24edn(L+3)(B\vee 1)^{L+2}(W+1)^L}^{S+D+(D+M+1)\de}.$$

    For any $p(\vx\vert y)=\prod_{k=1}^K\paren*{p_{\pnn_k}(\vx\vert y)}^{\I(y=k)}\in \cP_{X\vert Y}^{\single}$, there exists $p_{\pnn_1}^\prime\in\cB_{\cP_1}, \dots, p_{\pnn_K}^\prime\in\cB_{\cP_K}$ such that for all $k\in[K]$, we have: Given any $y\in\sY$, it holds that $\forall \vx\in\sX, p_{\pnn_k}^\prime(\vx\vert y)\geq p_{\pnn_k}(\vx\vert y)$, and $\norm{p_{\pnn_k}^\prime(\cdot\vert y)-p_{\pnn_k}(\cdot\vert y)}_{L^1(\sX)}\leq \frac{1}{n}.$ 

    Let $p^\prime(\vx\vert y)=\prod_{k=1}^K\paren*{p_{\pnn_k}^\prime(\vx\vert y)}^{\I(y=k)}$, 
    then we have that given any $y\in\sY$, 
    \begin{align*}
        \forall \vx\in \sX, p^\prime(\vx\vert y)=\prod_{k=1}^K\paren*{p_{\pnn_k}^\prime(\vx\vert y)}^{\I(y=k)}\geq\prod_{k=1}^K\paren*{p_{\pnn_k}(\vx\vert y)}^{\I(y=k)}= p(\vx\vert y),
    \end{align*}
    and 
    \begin{align*}
        \norm{p^\prime(\cdot\vert y)-p(\cdot\vert y)}_{L^1(\sX)}\leq\sup_{k\in[K]}\norm{p_{\pnn_k}^\prime(\cdot\vert y)-p_{\pnn_k}(\cdot\vert y)}_{L^1(\sX)}\leq \frac{1}{n}.
    \end{align*}
    Therefore, $\cB_{\cP}\coloneqq\curl*{p^\prime(\vx\vert y)= \prod_{k=1}^K\paren*{p_{\pnn_k}^\prime(\vx\vert y)}^{\I(y=k)}: p_{\pnn_k}^\prime\in\cB_{\cP_k}}$ is an $\frac{1}{n}$-upper bracket of $\cP_{X\vert Y}^{\single}$ w.r.t. $L^1(\sX)$. 
    Thus we have 
    \begin{align*}
        \br\paren*{\frac{1}{n};\cP_{X\vert Y}^{\single},L^1(\sX)}
        &\leq \abs{\cB_{\cP}}=\abs*{\bigcup_{k\in[K]}\cB_{\cP_k}}\leq \prod_{k\in[K]}\abs*{\cB_{\cP_k}}\\
        &=\prod_{k\in[K]} \paren*{24edn(L+3)(B\vee 1)^{L+2}(W+1)^L}^{S+D+(D+M+1)\de} \\
        &=\paren*{24edn(L+3)(B\vee 1)^{L+2}(W+1)^L}^{K(S+D+(D+M+1)\de)}.
    \end{align*}
    According to \cref{thm:TV_upper_of_conditional_MLE}, we arrive at the conclusion that 
    \begin{align*}
        \rtv(\hat{p}_{X\vert Y}^{\single})
        &\leq 3\sqrt{\frac{1}{n}\paren*{\log\br\paren*{\frac{1}{n};\cP_{X\vert Y}^{\single},L^1(\sX)}+\log\frac{1}{\delta}}}\\
        &\leq 3\sqrt{\frac{1}{n}\paren*{ K(S+D+(D+M+1)\de) \log\paren*{24edn(L+3)(B\vee 1)^{L+2}(W+1)^L}+\log\frac{1}{\delta}}}.
    \end{align*}
    Omitting constants about $n,K,\de, L,W,S,B$, and the logarithm term we have $\rtv(\hat{p}_{X\vert Y}^{\single})=\tilde{\cO}\paren*{\sqrt{\frac{KL\paren*{S+D+(D+M+1)\de}}{n}}}$.
    
\end{proof}

\section{Proofs for \cref{sec:instantiation_ebm}}
\label{app:proof_ebm}

\subsection{Covering number of the energy function class}

\begin{lemma}[Covering number of the energy function class]
\label{lem:ebm_covering_of_energy}
    Given $\cU_{\pnn}=\{u_{\pnn}(\vx\vert y)=f_{\pmlp}\circ \ve_{\vV}(\vx,y)\}$ with $u_{\pnn}(\vx\vert y):\sX\times\sY\to\R$ as defined in \cref{sec:instantiation_ebm}. Suppose $\vV[k,:]\in[0,1]^{\de}$, $\pmlp\in\cW(L,W,S,B)$ with constants $L,W,S,B>0$, and $\sX=[0,1]^D, \sY=[K]$. 
    Then, given any $\epsilon>0$, we have
    \begin{align*}
        \cov\paren*{\epsilon;\cU_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}} 
        \leq \paren*{\frac{3(L+1)(B\vee 1)^{L+1}(W+1)^L}{\epsilon}}^{S+K\de}.
    \end{align*}
\end{lemma}
\begin{proof}
    As defined, $\cU_{\pnn}$ can be written as
    \begin{align*}
        \cU_{\pnn}=\curl*{u_{\pnn}(\vx\vert y)=f_{\pmlp}\circ \ve_{\vV}(\vx,y): f_{\pmlp}\in\cF(L,W,B,S), \ve_{\vV}\in \cE_{\vV}}=\cF(L,W,B,S)\circ\cE_{\vV},
    \end{align*}
    where $\cE_{\vV}=\{\ve_{\vV}(\vx,y)=\begin{bmatrix}  \vx \\ \vV[y,:] \end{bmatrix}: \vV\in[0,1]^{K\times\de}\}$.
    Denote by $\sX_1\coloneqq\sX\times\sY=[0,1]^D\times[K]$ and $\sX_2\coloneqq[0,1]^{\de+D}$.
    % According to Lemma~\ref{lem:covering_number_of_nn} and Lemma~\ref{lem:lip_of_nn_about_input} we know the covering number upper bound of $\cF(L,W, B,S)$ and the Lipschitzness of $f_{\pmlp}$.
    Given any $\delta>0$, we first evaluate the $\delta$-covering number of $\cE_{\vV}$ w.r.t. $\norm*{\cdot}_{\infty,L^{\infty}(\sX_1)}$.

    \paragraph{Covering number of the embedding layer.}
    Let $\cS_\mathrm{entry}(\vV)$ denote the value space of all entries in $\vV$.
    We first discretize the value spaces of $\vV$ into $\delta$-width grids to get a finite embedding function class:
    $$\cE_{\vV, \delta\Z} \coloneqq \{\ve_{\vV}\in \cE_{\vV}:\cS_\mathrm{entry}(\vV))=[0,1]\cap \delta\Z\}.$$ 
    For any $\ve_{\vV}\in \cE_{\vV}$, there exists $\ve_{{\vV}^\prime}\in\cE_{\vV, \delta\Z}$ such that $\norm{\vV-{\vV}^\prime}_{\infty}\leq \delta$.
    Then we have for any $\vx,y\in\sX_1$, 
    \begin{align}
    \label{eq:ebm_covering_of_embedding}
        \norm{\ve_{\vV}(\vx,y)-\ve_{{\vV}^\prime}(\vx,y)}_{\infty}
        &=\norm*{\begin{bmatrix} \vx \\ \vV[y,:]  \end{bmatrix}-\begin{bmatrix} \vx \\ {\vV}^\prime[y,:]  \end{bmatrix}}_{\infty}
        =\norm*{\begin{bmatrix} \bm{0} \\ \vV[y,:]-{\vV}^\prime[y,:]\end{bmatrix}}_{\infty}
        \leq \norm*{\vV-{\vV}^\prime}_{\infty}
        \leq \delta. 
    \end{align}
    Therefore, $\cE_{\vV, \delta\Z}$ is an $\delta$-cover of $\cE_{\vV}$ w.r.t. $\norm*{\cdot}_{\infty,L^{\infty}(\sX_1)}$ and thus we have
    \begin{align*}
        \cov\paren*{\delta;\cE_{\vV},\norm*{\cdot}_{\infty,L^{\infty}(\sX_1)}}
        \leq \abs{\cE_{\vV, \delta\Z}}
        =\abs*{\vV:\cS_\mathrm{entry}(\vV))=[0,1]\cap \delta\Z}
        \leq \paren*{\frac{1}{\delta}+1}^{K\de}.
    \end{align*}

    \paragraph{Composite energy function.}
    According to Lemma~\ref{lem:covering_number_of_composite_function_class}, given any $\epsilon_{\cF}, \epsilon_{\cE}>0$, the covering number of $\cU_{\pnn}$ can be bounded by
    \begin{align*}
        \cov\paren*{\epsilon_{\cF}+\kappa_{\cF}\epsilon_{\cE};\cU_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX_1)}} 
        \leq \cov\paren*{\epsilon_{\cF};\cF(L,W,B,S),\norm*{\cdot}_{\infty,L^{\infty}(\sX_2)}}\cov\paren*{\epsilon_{\cE};\cE_{\vV},\norm*{\cdot}_{\infty,L^{\infty}(\sX_1)}}.
    \end{align*}
    According to Lemma~\ref{lem:lip_of_nn_about_input} , $\kappa_{\cF}=B^LW^L$. Further taking $\epsilon_{\cF}=L(B\vee 1)^{L-1}(W+1)^L\delta$ and $\epsilon_{\cE}=\delta$, we have
    \begin{align*}
        \epsilon_{\cF}+\kappa_{\cF}\epsilon_{\cE}= L(B\vee 1)^{L-1}(W+1)^L\delta+B^LW^L\delta=\paren*{L(B\vee 1)^{L-1}(W+1)^L+B^LW^L}\delta.
    \end{align*}
    According to Lemma~\ref{lem:covering_number_of_nn} and \cref{eq:ebm_covering_of_embedding}, we have
    \begin{align*}
        \cov\paren*{\epsilon_{\cF};\cF(L,W,B,S),\norm*{\cdot}_{\infty,L^{\infty}(\sX_2)}}\leq\paren*{\frac{2B}{\delta}+1}^S, \text{ and }
        \cov\paren*{\epsilon_{\cE};\cE_{\vV},\norm*{\cdot}_{\infty,L^{\infty}(\sX_1)}} \leq \paren*{\frac{1}{\delta}+1}^{K\de}.
    \end{align*}
    Therefore, 
    \begin{align*}
        \cov\paren*{\paren*{L(B\vee 1)^{L-1}(W+1)^L+B^LW^L}\delta;\cU_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX_1)}} 
        &\leq \paren*{\frac{2B}{\delta}+1}^S\paren*{\frac{1}{\delta}+1}^{K\de}\\
        &\leq\paren*{\frac{(2B\vee 1)}{\delta}+1}^{S+K\de}\\
        &\leq\paren*{\frac{2(B\vee 1)}{\delta}+1}^{S+K\de}\\
        &\leq \paren*{\frac{3(B\vee 1)}{\delta}}^{S+K\de}.\tag{$\frac{(B\vee 1)}{\delta}\geq 1$}
    \end{align*}
    Taking $\epsilon=\paren*{L(B\vee 1)^{L-1}(W+1)^L+B^LW^L}\delta$, we have
    \begin{align*}
        \cov\paren*{\epsilon;\cU_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX_1)}} 
        &\leq \paren*{\frac{3(B\vee 1)\paren*{L(B\vee 1)^{L-1}(W+1)^L+B^LW^L}}{\epsilon}}^{S+K\de}\\
        &\leq \paren*{\frac{3(B\vee 1)\paren*{L(B\vee 1)^{L}(W+1)^L+(B\vee 1)^L(W+1)^L}}{\epsilon}}^{S+K\de}\\
        &=\paren*{\frac{3(L+1)(B\vee 1)^{L+1}(W+1)^L}{\epsilon}}^{S+K\de},
    \end{align*}
    which completes the proof.
\end{proof}

\subsection{Bracketing number of the conditional distribution via the energy function}


\begin{lemma}[Bracketing number of the conditional distribution via the energy function]
\label{lem:ebm_bracket_via_energy}
    Given a class of energy-based conditional distributions that $\cP_{X\vert Y}=\curl*{p_{\pnn}(\vx\vert y)=\frac{e^{-u_{\pnn}(\vx\vert y)}}{\int_{\sX}e^{-u_{\pnn}(\vx\vert y)}d\vx}: u_{\pnn}\in\cU_{\pnn}},$ for any $0<\epsilon\leq 1$, it holds that 
    \begin{align*}
        \br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}
        \leq \cov\paren*{\frac{\epsilon}{4e};\cU_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}}. 
    \end{align*}
\end{lemma}
\begin{proof}
    Let $\cC_{\cU}$ be an $\epsilon_{\cU}$-cover of $\cU_{\pnn}$ w.r.t. $\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}$ such that $\abs{\cC_{\cU}}=\cov\paren*{\epsilon_{\cU};\cU_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}}$. 
    For any $p_{\pnn}(\vx\vert y)=\frac{e^{-u_{\pnn}(\vx\vert y)}}{\int_{\sX}e^{-u_{\pnn}(\vx\vert y)}d\vx}\in\cP_{X\vert Y}$, there exists $u_{\pnn}^\prime\in\cC_{\cU}$ such that for all $\vx\in\sX$ and $y\in\sY$, $\norm{u_{\pnn}(\vx\vert y)-u_{\pnn}^\prime(\vx\vert y)}_{\infty}=\abs{u_{\pnn}(\vx\vert y)-u_{\pnn}^\prime(\vx\vert y)}\leq \epsilon_{\cU}$, which equals $u_{\pnn}^\prime(\vx\vert y)-\epsilon_{\cU}\leq u_{\pnn}(\vx\vert y)\leq u_{\pnn}^\prime(\vx\vert y)+\epsilon_{\cU}.$
    
    Let $p_{\pnn}^\prime(\vx\vert y)=\frac{e^{-u_{\pnn}^\prime(\vx\vert y)+2\epsilon_{\cU}}}{\int_{\sX}e^{-u_{\pnn}^\prime(\vx\vert y)}d\vx}.$ 
    Then we immediately obtain that: given any $y\in\sY$,
    \begin{align}
    \label{eq:ebm_bracket_condition1_geq}
        \forall x\in \sX,
        p_{\pnn}^\prime(\vx\vert y)=\frac{e^{-u_{\pnn}^\prime(\vx\vert y)+\epsilon_{\cU}}}{\int_{\sX}e^{-u_{\pnn}^\prime(\vx\vert y)-\epsilon_{\cU}}d\vx}\geq \frac{e^{-u_{\pnn}(\vx\vert y)}}{\int_{\sX}e^{-u_{\pnn}(\vx\vert y)}d\vx}=p_{\pnn}(\vx\vert y),
    \end{align}
    since for all $\vx\in\sX, y\in\sY$, $e^{-u_{\pnn}^\prime(\vx\vert y)+\epsilon_{\cU}}\geq e^{-u_{\pnn}(\vx\vert y)}$ and $\int_{\sX}e^{-u_{\pnn}^\prime(\vx\vert y)-\epsilon_{\cU}}d\vx\leq\int_{\sX}e^{-u_{\pnn}(\vx\vert y)}d\vx$.
    
    Moreover, we can bound the $L^1(\sX)$ distance between $p_{\pnn}^\prime(\cdot\vert y)$ and $p_{\pnn}(\cdot\vert y)$ as
    \begin{align}
        &\norm{p_{\pnn}^\prime(\cdot\vert y)-p_{\pnn}(\cdot\vert y)}_{L^1(\sX)}\notag\\
        =&\int_{\sX}\abs{p_{\pnn}^\prime(\vx\vert y)-p_{\pnn}(\vx\vert y)}d\vx\notag\\
        =&\int_{\sX}\abs*{\frac{e^{-u_{\pnn}^\prime(\vx\vert y)+2\epsilon_{\cU}}}{\int_{\sX}e^{-u_{\pnn}^\prime(\vs\vert y)}d\vs}-\frac{e^{-u_{\pnn}(\vx\vert y)}}{\int_{\sX}e^{-u_{\pnn}(\vs\vert y)}d\vs}}d\vx\notag\\
        =&\int_{\sX}\abs*{\frac{e^{-u_{\pnn}^\prime(\vx\vert y)+2\epsilon_{\cU}}\int_{\sX}e^{-u_{\pnn}(\vs\vert y)}d\vs-e^{-u_{\pnn}(\vx\vert y)}\int_{\sX}e^{-u_{\pnn}^\prime(\vs\vert y)}d\vs}{\int_{\sX}e^{-u_{\pnn}^\prime(\vs\vert y)}d\vs \int_{\sX}e^{-u_{\pnn}(\vs\vert y)}d\vs}}d\vx\notag\\
        \leq&\int_{\sX}\abs*{\frac{\paren*{e^{-u_{\pnn}^\prime(\vx\vert y)+2\epsilon_{\cU}}-e^{-u_{\pnn}(\vx\vert y)}}\int_{\sX}e^{-u_{\pnn}(\vs\vert y)}d\vs+e^{-u_{\pnn}(\vx\vert y)}\paren*{\int_{\sX}e^{-u_{\pnn}(\vs\vert y)}d\vs -e^{-u_{\pnn}^\prime(\vs\vert y)}d\vs}}{\int_{\sX}e^{-u_{\pnn}^\prime(\vs\vert y)}d\vs \int_{\sX}e^{-u_{\pnn}(\vs\vert y)}d\vs}}d\vx\notag\\
        \leq&\int_{\sX}\frac{\abs*{e^{-u_{\pnn}^\prime(\vx\vert y)+2\epsilon_{\cU}}-e^{-u_{\pnn}(\vx\vert y)}}\int_{\sX}e^{-u_{\pnn}(\vs\vert y)}d\vs+e^{-u_{\pnn}(\vx\vert y)}\int_{\sX}\abs*{e^{-u_{\pnn}(\vs\vert y)}-e^{-u_{\pnn}^\prime(\vs\vert y)}}d\vs}{\int_{\sX}e^{-u_{\pnn}^\prime(\vs\vert y)}d\vs \int_{\sX}e^{-u_{\pnn}(\vs\vert y)}d\vs}d\vx\notag\\
        \leq&\int_{\sX}\frac{e^{-u_{\pnn}^\prime(\vx\vert y)+2\epsilon_{\cU}}\abs*{u_{\pnn}(\vx\vert y)-u_{\pnn}^\prime(\vx\vert y)+2\epsilon_{\cU}}\int_{\sX}e^{-u_{\pnn}(\vs\vert y)}d\vs+e^{-u_{\pnn}(\vx\vert y)}\int_{\sX}e^{\paren*{-u_{\pnn}(\vs\vert y)}\vee \paren*{-u_{\pnn}^\prime(\vs\vert y)}}\abs*{u_{\pnn}(\vs\vert y)-u_{\pnn}^\prime(\vs\vert y)}d\vs}{\int_{\sX}e^{-u_{\pnn}^\prime(\vs\vert y)}d\vs \int_{\sX}e^{-u_{\pnn}(\vs\vert y)}d\vs}d\vx \tag{$\abs{e^a-e^b}=\abs{\int_{b}^ae^{x}dx}\leq\abs{\int_{b}^ae^{a\vee b}dx}= e^{a\vee b}\abs{a-b}$}\\
        \leq&\int_{\sX}\frac{e^{-u_{\pnn}^\prime(\vx\vert y)+2\epsilon_{\cU}}3\epsilon_{\cU}\int_{\sX}e^{-u_{\pnn}(\vs\vert y)}d\vs+e^{-u_{\pnn}(\vx\vert y)}\int_{\sX}e^{-u_{\pnn}^\prime(\vs\vert y)+\epsilon_{\cU}}\epsilon_{\cU} d\vs}{\int_{\sX}e^{-u_{\pnn}^\prime(\vs\vert y)}d\vs \int_{\sX}e^{-u_{\pnn}(\vs\vert y)}d\vs}d\vx \tag{$\forall x\in\sX, y\in\sY, \abs*{u_{\pnn}(\vx\vert y)-u_{\pnn}^\prime(\vx\vert y)}\leq \epsilon_{\cU}$}\notag\\
        \leq & \int_{\sX}\frac{3\epsilon_{\cU} e^{-u_{\pnn}(\vx\vert y)+3\epsilon_{\cU}}\int_{\sX}e^{-u_{\pnn}^\prime(\vs\vert y)+\epsilon_{\cU}}d\vs+\epsilon_{\cU} e^{-u_{\pnn}(\vx\vert y)}\int_{\sX}e^{-u_{\pnn}^\prime(\vs\vert y)+\epsilon_{\cU}} d\vs}{\int_{\sX}e^{-u_{\pnn}^\prime(\vs\vert y)}d\vs \int_{\sX}e^{-u_{\pnn}(\vs\vert y)}d\vs}d\vx \tag{$\forall x\in\sX, y\in\sY, \abs*{u_{\pnn}(\vx\vert y)-u_{\pnn}^\prime(\vx\vert y)}\leq \epsilon_{\cU}$}\\
        = & \int_{\sX}\frac{3\epsilon_{\cU} e^{4\epsilon_{\cU}} e^{-u_{\pnn}(\vx\vert y)}\int_{\sX}e^{-u_{\pnn}^\prime(\vs\vert y)}d\vs+\epsilon_{\cU} e^{\epsilon_{\cU}} e^{-u_{\pnn}(\vx\vert y)}\int_{\sX}e^{-u_{\pnn}^\prime(\vs\vert y)} d\vs}{\int_{\sX}e^{-u_{\pnn}^\prime(\vs\vert y)}d\vs \int_{\sX}e^{-u_{\pnn}(\vs\vert y)}d\vs}d\vx\notag\\
        = & \int_{\sX}\frac{\paren*{3\epsilon_{\cU} e^{4\epsilon_{\cU}}+\epsilon_{\cU} e^{\epsilon_{\cU}}} e^{-u_{\pnn}(\vx\vert y)}}{\int_{\sX}e^{-u_{\pnn}(\vs\vert y)}d\vs}d\vx
        = 3\epsilon_{\cU} e^{4\epsilon_{\cU}}+\epsilon_{\cU} e^{\epsilon_{\cU}} 
        \leq 4\epsilon_{\cU} e^{(4\epsilon_{\cU})\vee 1}. \label{eq:ebm_bracket_condition2_norm}
    \end{align}
    Therefore, $\cB_{\cP}\coloneqq\curl*{p_{\pnn}^\prime(\vx\vert y)=\frac{e^{-u_{\pnn}^\prime(\vx\vert y)+\epsilon_{\cU}}}{\int_{\sX}e^{-u_{\pnn}^\prime(\vx\vert y)-\epsilon_{\cU}}d\vx}: -u_{\pnn}^\prime(\vx\vert y)\in\cC_{\cU}}$ is an $4\epsilon_{\cU} e^{4\epsilon_{\cU}}$-upper bracket of $\cP_{X\vert Y}$ w.r.t. $L^1(\sX)$. 
    Thus we have 
    \begin{align*}
        \br\paren*{4\epsilon_{\cU} e^{(4\epsilon_{\cU})\vee 1};\cP_{X\vert Y},L^1(\sX)}
        \leq \abs{\cB_{\cP}}=\abs{\cC_{\cU}}=\cov\paren*{\epsilon_{\cU};\cU_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}}. 
    \end{align*}
    Let $4\epsilon_{\cU} e^{(4\epsilon_{\cU})\vee 1}=\epsilon$, we have $4\epsilon_{\cU}=\frac{\epsilon}{e^{(4\epsilon_{\cU})\vee 1}}\leq 1$ and thus $4\epsilon_{\cU} e^{(4\epsilon_{\cU})\vee 1}=4e\epsilon_{\cU}$, so that we get $\epsilon_{\cU}=\frac{\epsilon}{4e}$.
    Therefore, we have for any $0<\epsilon\leq 1$,
    \begin{align*}
        \br\paren*{\epsilon;\cP_{X\vert Y},L^1(\sX)}
        \leq \cov\paren*{\frac{\epsilon}{4e};\cU_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}}
    \end{align*}    
\end{proof}

\subsection{Proof of \cref{thm:tv_upper_ebm}}
\label{app:proof_of_ebm}

Based on the relation between the bracketing number of conditional distribution space $\cP_{X\vert Y}$ and the covering number of energy function space $\cU_{\pnn}$ derived in previous lemmas, we obtain the final result.

\begin{proof}[Proof of \cref{thm:tv_upper_ebm}]
    With conditional distributions as defined in \cref{eq:conditional_density_ebm}, we have 
    \begin{align*}
        \cP_{X\vert Y}^{\multi}
        =\curl*{p_{\pnn}(\vx\vert y)=\frac{e^{-u_{\pnn}(\vx\vert y)}}{\int_{\sX}e^{-u_{\pnn}(\vx\vert y)}d\vx}: u_{\pnn}\in\cU_{\pnn}},
    \end{align*}
    where 
    \begin{align*}
        \cU_{\pnn}=\curl*{u_{\pnn}(\vx\vert y)=f_{\pmlp}\circ \ve_{\vV}(\vx,y): \pmlp\in\cW(L,W,S,B), \vV[k,:]\in[0,1]^{\de}}.
    \end{align*}
    Let $\epsilon_{\cU}$ be an constant that $\epsilon_{\cU}>0$, according to Lemma~\ref{lem:ebm_covering_of_energy}, we have $\cov\paren*{\epsilon_{\cU};\cU_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}} 
    \leq \paren*{\frac{3(L+1)(B\vee 1)^{L+1}(W+1)^L}{\epsilon_{\cU}}}^{S+K\de}.$
    Then with Lemma~\ref{lem:ebm_bracket_via_energy}, we further obtain that
    \begin{align*}
        \br\paren*{\frac{1}{n};\cP_{X\vert Y}^{\multi},L^1(\sX)}
        \leq \cov\paren*{\frac{1}{4en};\cU_{\pnn},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}}\leq\paren*{12e(L+1)(B\vee 1)^{L+1}(W+1)^L}^{S+K\de}. 
    \end{align*}

    According to \cref{thm:TV_upper_of_conditional_MLE}, we arrive at the conclusion that 
    \begin{align*}
        \rtv(\hat{p}_{X\vert Y}^{\multi})
        &\leq 3\sqrt{\frac{1}{n}\paren*{\log\br\paren*{\frac{1}{n};\cP_{X\vert Y}^{\multi},L^1(\sX)}+\log\frac{1}{\delta}}}\\
        &\leq 3\sqrt{\frac{1}{n}\paren*{\paren*{S+K\de}\log\paren*{12en(L+1)(B\vee 1)^{L+1}(W+1)^L}+\log\frac{1}{\delta}}}\\
        &= 3\sqrt{\frac{1}{n}\paren*{L\paren*{S+K\de}\log\paren*{12en(L+1)^{\frac{1}{L}}(B\vee 1)^{1+\frac{1}{L}}(W+1)}+\log\frac{1}{\delta}}}
    \end{align*}
    Omitting constants about $n,K,\de, L,W,S,B$, and the logarithm term we have $\rtv(\hat{p}_{X\vert Y}^{\multi})=\tilde{\cO}\paren*{\sqrt{\frac{L\paren*{S+K\de}}{n}}}$.
\end{proof}

\subsection{Average TV error bound under single-source training}
\label{thm:tvbound_ebm_single}

\begin{theorem}[average TV error bound for EBMs under single-source training]
    Let $\hat{p}_{X\vert Y}^{\single}$ be the likelihood maximizer defined in \cref{eq:single_mle_solution} given $\cP_{X\vert Y}^{\single}$ with conditional distributions as in \cref{eq:conditional_density_ebm}, suppose that $\Phi=[0,1]^{\de}$ and $\Psi = \cW(L,W,S,B)$ and assume $\phi_k^*\in \Phi$, $\psi^*\in \Psi$. Then, for any $0<\delta\leq 1/2$, it holds with probability at least $1-\delta$ that
    \begin{align*}
    \rtv(\hat{p}_{X\vert Y}^{\single})=\tilde{\cO}\paren*{\sqrt{\frac{LK\paren*{S+\de}}{n}}}.
    \end{align*}
\end{theorem}

\begin{proof}
As formulated in \cref{sec:formulation_for_conditional_generative_modeling} and with conditional distributions as in \cref{eq:conditional_density_ebm}, we have 
        \begin{align*}
        \cP_{X\vert Y}^{\single}
        =\curl*{\prod_{k=1}^K\paren*{p_{\pnn_k}(\vx\vert y)}^{\I(y=k)}: 
        p_{\pnn_k}(\vx\vert y)=\frac{e^{-u_{\pnn_k}(\vx\vert y)}}{\int_{\sX}e^{-u_{\pnn_k}(\vx\vert y)}d\vx}: u_{\pnn_k}\in\cU_{\pnn_k}},
    \end{align*}
    where 
    \begin{align*}
        \cU_{\pnn_k}=\curl*{u_{\pnn_k}(\vx\vert y)=f_{\pmlp_k}\circ \ve_{\vV[k,:]}(\vx,y): \pmlp_k\in\cW(L,W,S,B), \vV[k,:]\in[0,1]^{\de}}.
    \end{align*}
    For all $k\in[K]$, let $\cB_{\cP_k}$ be an $\frac{1}{n}$-upper bracket of $\cP_{X\vert Y,k}=\curl*{p_{\pnn_k}(\vx\vert y)=\frac{e^{-u_{\pnn_k}(\vx\vert y)}}{\int_{\sX}e^{-u_{\pnn_k}(\vx\vert y)}d\vx}: u_{\pnn_k}\in\cU_{\pnn_k}}$ w.r.t. $L^1(\sX)$ such that $\abs{\cB_{\cP_k}}=\br\paren*{\frac{1}{n};\cP_{X\vert Y,k},L^1(\sX)}$. 
    According to Lemma~\ref{lem:ebm_covering_of_energy} and Lemma~\ref{lem:ebm_bracket_via_energy}, we know that 
    $$\abs{\cB_{\cP_k}}\leq \cov\paren*{\frac{1}{4en};\cU_{\pnn_k},\norm*{\cdot}_{\infty,L^{\infty}(\sX\times\sY)}}\leq\paren*{12en(L+1)(B\vee 1)^{L+1}(W+1)^L}^{S+\de}.$$

    For any $p(\vx\vert y)=\prod_{k=1}^K\paren*{p_{\pnn_k}(\vx\vert y)}^{\I(y=k)}\in \cP_{X\vert Y}^{\single}$, there exists $p_{\pnn_1}^\prime\in\cB_{\cP_1}, \dots, p_{\pnn_K}^\prime\in\cB_{\cP_K}$ such that for all $k\in[K]$, we have: Given any $y\in\sY$, it holds that $\forall \vx\in\sX, p_{\pnn_k}^\prime(\vx\vert y)\geq p_{\pnn_k}(\vx\vert y)$, and $\norm{p_{\pnn_k}^\prime(\cdot\vert y)-p_{\pnn_k}(\cdot\vert y)}_{L^1(\sX)}\leq \frac{1}{n}.$ 

    Let $p^\prime(\vx\vert y)=\prod_{k=1}^K\paren*{p_{\pnn_k}^\prime(\vx\vert y)}^{\I(y=k)}$, 
    then we have that given any $y\in\sY$, 
    \begin{align*}
        \forall \vx\in \sX, p^\prime(\vx\vert y)=\prod_{k=1}^K\paren*{p_{\pnn_k}^\prime(\vx\vert y)}^{\I(y=k)}\geq\prod_{k=1}^K\paren*{p_{\pnn_k}(\vx\vert y)}^{\I(y=k)}= p(\vx\vert y),
    \end{align*}
    and 
    \begin{align*}
        \norm{p^\prime(\cdot\vert y)-p(\cdot\vert y)}_{L^1(\sX)}\leq\sup_{k\in[K]}\norm{p_{\pnn_k}^\prime(\cdot\vert y)-p_{\pnn_k}(\cdot\vert y)}_{L^1(\sX)}\leq \frac{1}{n}.
    \end{align*}
    Therefore, $\cB_{\cP}\coloneqq\curl*{p^\prime(\vx\vert y)= \prod_{k=1}^K\paren*{p_{\pnn_k}^\prime(\vx\vert y)}^{\I(y=k)}: p_{\pnn_k}^\prime\in\cB_{\cP_k}}$ is an $\frac{1}{n}$-upper bracket of $\cP_{X\vert Y}^{\single}$ w.r.t. $L^1(\sX)$. 
    Thus we have 
    \begin{align*}
        \br\paren*{\frac{1}{n};\cP_{X\vert Y}^{\single},L^1(\sX)}
        &\leq \abs{\cB_{\cP}}=\abs*{\bigcup_{k\in[K]}\cB_{\cP_k}}\leq \prod_{k\in[K]}\abs*{\cB_{\cP_k}}\\
        &=\prod_{k\in[K]}\paren*{12en(L+1)(B\vee 1)^{L+1}(W+1)^L}^{S+\de}\\
        &=\paren*{12en(L+1)(B\vee 1)^{L+1}(W+1)^L}^{K(S+\de)}.
    \end{align*}
    According to \cref{thm:TV_upper_of_conditional_MLE}, we arrive at the conclusion that 
    \begin{align*}
        \rtv(\hat{p}_{X\vert Y}^{\single})
        &\leq 3\sqrt{\frac{1}{n}\paren*{\log\br\paren*{\frac{1}{n};\cP_{X\vert Y}^{\single},L^1(\sX)}+\log\frac{1}{\delta}}}\\
        &\leq 3\sqrt{\frac{1}{n}\paren*{LK\paren*{S+\de}\log\paren*{(12enL+1)^{\frac{1}{L}}(B\vee 1)^{1+\frac{1}{L}}(W+1)}+\log\frac{1}{\delta}}}
    \end{align*}
    Omitting constants about $n,K,\de, L,W,S,B$, and the logarithm term we have $\rtv(\hat{p}_{X\vert Y}^{\single})=\tilde{\cO}\paren*{\sqrt{\frac{LK\paren*{S+\de}}{n}}}$.
    
\end{proof}



\section{Supplementary for experiments}
\label{app:supplementary_material_for_real-world_experements}

% The following can be moved to Appendix
Following EDM2, we use the Latent Diffusion Model (LDM)~\citep{rombach_2022_cvpr_ldm} to down-sample each image $x \in \mathbb{R}^{3 \times 256 \times 256}$ to a corresponding latent $z \in \mathbb{R}^{4 \times 32 \times 32}$ for training a diffusion models.

% device
All experiments are trained and sampled on 8 $\times$ NVIDIA A800 80GB, 8 $\times$ NVIDIA GeForce RTX 4090, and 8 $\times$ NVIDIA GeForce RTX 3090 on the Linux Ubuntu-22.04 platform.

% parameters
For a fair comparison, we set different hyper-parameters for experiments with different numbers of sources as shown in Table \ref{tab:hyper_params}, but these parameters are the same with different similarity.

\begin{table}[t]
\vskip 0.2in
\begin{center}
    \centering
    \caption{Hyparameters of our experiments. `1c' denotes training from single-source, and others denote training from multi-source which contains 3,5, and 10 classes.}
    \label{tab:hyper_params}
    \centering
    \begin{tabular}{lccc}
    \toprule
        Setup& \makecell{Iterations  (kimg)} & Learning rate & \makecell{Decay  (kimg)}\\
    \midrule
        1c& 184549& 0.005&2500\\
        3c& 268435& 0.006&4000\\
        % 5c& 402653& 0.006&4000\\
        10c& 1610612& 0.012&6000\\
    \bottomrule
    \end{tabular}
\end{center}
\vskip -0.2in
\end{table}

\begin{figure}[t]
\vskip 0.2in
\begin{center}
    \centering
    \includegraphics[height=0.2\textheight]{figs/hierachy.pdf}
    \vspace{-8pt}
    \caption{Similarity level.}
    \label{fig:hierachy}
\end{center}
\vskip -0.2in
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
