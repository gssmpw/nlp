%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{placeins} %lan

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}
% \usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{enumitem}
\usepackage{pifont}
\usepackage{float}

\newcommand{\ours}{IntepVLM}
\newcommand{\ourss}{IntepVLM~}

\newcommand{\ieno}{\textit{i}.\textit{e}.}
\newcommand{\egno}{\textit{e}.\textit{g}.} %there is no space
\newcommand{\etcno}{\textit{etc}} %there is no "."
\newcommand{\etal}{\textit{et al.}}
\newcommand{\mywidth}{0.9}
\newcommand{\myratio}{0.85}%0.85}

\definecolor{ForestGreen}{RGB}{0, 179, 45}
\newcommand{\tcb}{\textcolor{black}}%blue
\newcommand{\tcr}{\textcolor{red}}
\newcommand{\tco}{\textcolor{orange}}
\newcommand{\tcg}{\textcolor{ForestGreen}}
\newcommand{\tcbk}{\textcolor{black}}
\newcommand{\tcp}{\textcolor{purple}} %related to new VLM

\renewcommand{\thefootnote}{} %lan added
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}
\icmltitlerunning{Deciphering Functions of Neurons in Vision-Language Models}

%This paper is the result of an open source research project starting from March, 2024.

\begin{document}

\twocolumn[
\icmltitle{Deciphering Functions of Neurons in Vision-Language Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jiaqi Xu}{equal,ustc}%
\icmlauthor{Cuiling Lan}{msra}
\icmlauthor{Xuejin Chen}{ustc}
\icmlauthor{Yan Lu}{msra}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{ustc}{University of Science and Technology of China}
\icmlaffiliation{msra}{Microsoft Research Asia}

%\icmlcorrespondingauthor{}{}


%\icmlsetsymbol{equal}{*}

% \begin{icmlauthorlist}
% \icmlauthor{Firstname1 Lastname1}{equal,yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
% %\icmlauthor{}{sch}
% %\icmlauthor{}{sch}
% \end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{}{}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% \printAffiliationsAndNotice{\icmlIntern}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

% this must go after the closing bracket ] following \twocolumn[ ...


% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


% %%%icml 2024 style%%%
% \icmlsetsymbol{equal}{*}


% % \author{
% % Jiaqi Xu$^1$\thanks{This work was done when Jiaqi was an intern at MSRA.}\quad  
% % Cuiling Lan$^2$\quad 
% % Wenxuan Xie$^2$\quad
% % Xuejin Chen$^1$\quad 
% % Yan Lu$^2$\\
% % $^1$University of Science and Technology of China \quad
% % $^2$Microsoft Research Asia \\
% % \tt\small{xujiaqi@mail.ustc.edu.cn, xjchen99@ustc.edu.cn}, \\
% % \tt\small{\{culan,wenxie,yanlu\}@microsoft.com} \\
% % }

% \begin{icmlauthorlist}
% \icmlauthor{Jiaqi Xu}{equal,ustc}
% \icmlauthor{Cuiling Lan}{msra}
% % \icmlauthor{Wenxuan Xie}{msra}
% \icmlauthor{Xuejin Chen}{ustc}
% \icmlauthor{Yan Lu}{msra} 
% \end{icmlauthorlist}

% \icmlaffiliation{ustc}{University of Science and Technology of China}
% \icmlaffiliation{msra}{Microsoft Research Asia}

% % \icmlcorrespondingauthor{Cuiling Lan}{culan@microsoft.com}
% %\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% % You may provide any keywords that you
% % find helpful for describing your paper; these are used to populate
% % the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

% \vskip 0.3in
% ]

% % this must go after the closing bracket ] following \twocolumn[ ...

% % This command actually creates the footnote in the first column
% % listing the affiliations and the copyright notice.
% % The command takes one argument, which is text to display at the start of the footnote.
% % The \icmlEqualContribution command is standard text for equal contribution.
% % Remove it (just {}) if you do not need this facility.

% % \printAffiliationsAndNotice{\icmlIntern}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
% %%%%%



\begin{abstract}
The burgeoning growth of open-sourced vision-language models (VLMs) has catalyzed a plethora of applications across diverse domains. Ensuring the transparency and interpretability of these models is critical for fostering trustworthy and responsible AI systems. In this study, our objective is to delve into the internals of VLMs to interpret the functions of individual neurons. We observe the activations of neurons with respects to the input visual tokens and text tokens, and reveal some interesting findings. Particularly, we found that there are neurons responsible for only visual or text information, or both, respectively, which we refer to them as visual neurons, text neurons, and multi-modal neurons, respectively. We build a framework that automates the explanation of neurons with the assistant of GPT-4o. Meanwhile, for visual neurons, we propose an activation simulator to assess the reliability of the explanations for visual neurons. System statistical analyses on top of one representative VLM of LLaVA, uncover the behaviors/characteristics of different categories of neurons.
\end{abstract}

\section{Introduction}
\label{sec:intro}

The advent of vision-language models (VLMs) has ushered in a new era in the field of artificial intelligence, where the synergy between visual perception and language understanding has been leveraged to solve complex tasks, such as image captioning, visual question answering, and multi-modal information retrieval. They have found widespread application across various domains, including, but not limited to, embodied AI, agents, and human-machine interaction. 

The proliferation of open-source VLMs, such as LLaVA \cite{liu2023llava}, BLIP-2~\cite{li2023blip}, QwenVL \cite{bai2023qwen}, PaliGemma \cite{beyer2024paligemma}, InternVL 2.5~\cite{chen2024expanding}, and others, has significantly democratized access to cutting-edge AI technologies, enabling researchers and practitioners to build upon these foundations to create innovative applications that span educational, medical, and entertainment spheres. However, the complexity and opacity of these models often pose a significant challenge to their broader adoption, particularly in domains where trustworthiness, transparency, and accountability are paramount. The black box nature of VLMs, has raised concerns regarding the interpretability of these models. Bills \etal~\cite{bills2023language} investigated the automatic explanation of neurons in Language Model with pure texts as input. Understanding the internal workings of VLMs, including how the neurons function and how they treat vision and text differently, is crucial for ensuring that these systems operate in a manner that is both understandable and predictable to human users. Stan \etal \cite{stan2024lvlm} provided an interactive LVLM-Interpret tool to visualize how the generated outputs related to the input image through raw attention, relevancy maps, and causal interpretation. However, there is rare work for interpreting individual neurons in VLMs. 


\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{figures/Response.pdf} \\
\vspace{-2mm}
\caption{To understand the function of a neuron, we jointly observe the activations of this neuron on visual tokens and text tokens. For visual tokens, a higher transparent degree indicates a higher activation. For text tokens, the darker of the green color indicates a higher activation.}
\vspace{-1mm}
\label{fig:Response}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/Framework-modulated.pdf} \\ %Framework-two.pdf 
\vspace{-2.5mm}
\caption{Illustrate of our main workflow to investigate the functions of neurons in a VLM (\egno, LLaVA). (a) A visual-language model takes an image and the text prompt as input, and output the predicted texts. For each neuron, we record its activations with respect to each visual token and text token for a sample (see Figure \ref{fig:Response}), which will be analyzed to uncover the function of the neuron. (b) We automate the explanation of neurons with an explainer by taking the top activated samples as input. We evaluate the reliability of the explanations with the assistance of a visual simulator and a text simulator. }
%(c) Based on the VLM of LLaVA, we reveal that there are neurons responsible only for vision, neurons responsible only for text, and neurons responsible for both vision and text. We will study the characteristics of them to have deeper understanding of the internals of LLaVA.
\vspace{-1mm}
\label{fig:framework}
\end{figure*}

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.9\linewidth]{figures/Response.pdf} \\
% \vspace{-1mm}
% \caption{To understand the function of a neuron, we jointly observe the activations of this neuron on visual tokens and text tokens. For visual tokens, a higher transparent degree indicates a higher activation. For text tokens, the darker of the green color indicates a higher activation.}
% \vspace{-1mm}
% \label{fig:Response}
% \end{figure}

% \begin{figure*}[t]
% \centering
% \includegraphics[width=\linewidth]{figures/Framework-latest.pdf} \\
% \vspace{-1mm}
% \caption{Illustrate of our main workflow to investigate the functions of neurons in a VLM (\egno, LLaVA). (a) A visual-language model takes an image and the text prompt as input, and output the predicted texts. For each neuron, we record its activations with respect to each visual token and text token for a sample (see Figure \ref{fig:Response}), which will be analyzed to uncover the function of the neuron. (b) We automate the explanation of neurons with an explainer by taking the top activated samples as input. We evaluate the reliability of the explanations with the assistance of a vision simulator and a text simulator. (c) Based on the VLM of LLaVA, we reveal that there are neurons responsible only for vision, neurons responsible only for text, and neurons responsible for both vision and text. We will study the characteristics of them to have deeper understanding of the internals of LLaVA.}
% \vspace{-1mm}
% \label{fig:framework}
% \end{figure*}



In this paper, our study aims to shed light on the internal mechanisms of VLMs by exploring the functions of individual neurons within a VLM. Specifically, we seek to understand whether neurons within VLMs exhibit different responses to visual and text tokens, and whether there exist specialized subsets of neurons that are tailored for processing visual-specific or text-specific information. How do such neurons spread over the entire network? How can we automatically interpret the functions of neurons and access the reliability of the explanation? Are there neurons that possess functions insightful to human?   


To achieve our objectives of unveiling the mystery of VLMs, we systematically observed the responses of neurons with respect to visual tokens and text tokens, where Figure \ref{fig:Response} shows an example of the visualization of neuron responses for visual and text tokens, respectively. Through the abundant observations over various response patterns, we have some interesting findings and reveal the existence of visual-specific neurons, text-specific neurons, and multi-modal neurons. These discoveries outline a broad overview of neuron functions. As illustrated in Figure \ref{fig:framework}~(b), in order to automate the interpretation of neuron functions, we introduce a framework that leverages GPT-4o as an explainer to automatically generate interpretations of neuron functions. For visual neurons, we introduce a visual activation simulator to assess the reliability of the explanations to vision-related neurons. For texts, we use a text activation simulator to predict the activations on text tokens. We conducted statistical analysis on top of one representative VLM of LLaVA 1.5 \cite{liu2023llava}, uncovering the behaviors/characteristics of different categories of neurons. 
% \tcp
{Similar observations were found in another typical VLM, InternVL 2.5 \cite{chen2024expanding} (see the Appendix).}       

%Our work represents the first attempt to automate the explanation of neuron functions in VLMs by concurrently analyzing their responses to both vision and text tokens. By applying our framework over a representative open-sourced VLM, we aim to enhance the interpretability of the model, thereby contributing to the development of more transparent, trustworthy, and responsible AI system. 

% To automate the finding of patterns and neuron functions, we introduce a framework that leverages the capabilities of a Large Language Model (LLM), such as ChatGPT, to serve as an explainer that can automatically generate interpretations of the neuron functions. 
% Our work represents the first attempt to automate the explanation of neuron functions in VLMs by concurrently analyzing their responses to both vision and text tokens. By applying our framework over a representative open-sourced VLM, we aim to enhance the interpretability of the model, thereby contributing to the development of more transparent, trustworthy, and responsible AI system. 

In summary, we have four contributions:
\begin{itemize} [noitemsep,nolistsep,leftmargin=*]
\vspace{-1mm}
\item We delve into the internals of VLMs to interpret the functions of neurons. Based on the activation patterns on visual and text tokens, we reveal that there are neurons responsible for only visual or text information, or both.    
%We present a systematic analysis of the functions of individual neurons within a vision-language model through abundant observations of the neuron responses on visual tokens and text tokens. The analysis reveals some interesting findings, such as \tcr{... xxx}.

\item We observed the activations of abundant neurons and outline a broad overview of neuron functions.

\item We introduce a framework that enables the automatic explanation of the functions of VLM neurons, and an activation simulator to simulate the activations of visual neurons to facilitate the assessment of the reliability of explanations.  

\item We have conducted extensive analyses of the characteristics of different categories of neurons to enable a comprehensive understanding.
%We introduce a novel framework that utilizes a Large Language Model (LLM), such as ChatGPT, as an explainer to automatically generate interpretations of neuron functions in VLMs. This approach overcomes significant technical challenges, particularly in processing and interpreting visual tokens, by using elaborately designed prompts.

%\item We demonstrate the applicability of our framework to explaining neurons in VLM, showcasing its potential to enhance the model interpretability. 
\vspace{-1mm}
\end{itemize}

Our work contributes to the goal of fostering the development of transparent, trustworthy, and responsible AI systems by providing a methodological foundation for understanding and explaining the functions of neurons in VLMs.  

\begin{figure*}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.99\linewidth]{figures/neuron_functions.pdf}
   \vspace{-6mm}
   \caption{Visualization of neuron function categories. The caption above each sub-figure specifies the neuron category (visual, text, or multi-modal neuron), neuron id, and the max activation values for visual and text tokens, normalized on a scale from 0 to 10. Positioned centrally to the left within each sub-figure is the original image, whereas the image to the central right showcases the visualization generated post-activation by the authentic activation values of the corresponding image patches. Beneath each sub-figure, the text tokens alongside their respective activations are displayed. The darker the green, the greater the activation value.} 
   \label{fig:neuron_functions}
\end{figure*}

\section{Related Work}
\label{sec:related_work}

%In this section, we explore the landscape of existing research related to our study, focusing on vision-language models (VLMs), interpretability in artificial intelligence, and the use of large language models (LLMs) for explanation generation. Our work intersects with these areas, contributing to the ongoing dialogue on making complex AI models more transparent and understandable.

\noindent\textbf{Vision-Language Models (VLMs)} VLMs are designed to process and understand both visual and textual data simultaneously.  Recent advancements, such as those presented by LLaVA \cite{liu2023llava}, BLIP-2 \cite{li2023blip}, QwenVL \cite{bai2023qwen}, and PaliGemma \cite{beyer2024paligemma}, demonstrate the rapid evolution and expanding capabilities of these models. A typical work LLaVA aligns the visual tokens (that are encoded by CLIP encoder) with the text space as the input to Large Language Model (LLM) to enable visual understanding capability \cite{liu2023llava,liu2023improvedllava}. These systems have been instrumental in pushing the boundaries of what is possible in tasks like image captioning, visual question answering, and multi-modal information retrieval. Our work builds upon these foundations, focusing on the interpretability aspect of VLMs, an area that is crucial for their broader adoption and trustworthiness.
% VLMs have become a cornerstone in the field of AI, enabling a myriad of applications that require the integration of visual and textual information. Recent advancements, such as those presented by LLaVA \cite{liu2023llava}, BLIP-2 \cite{li2023blip}, QwenVL \cite{bai2023qwen}, and Gemma \cite{gemma2024}, demonstrate the rapid evolution and expanding capabilities of these models. These systems have been instrumental in pushing the boundaries of what is possible in tasks like image captioning, visual question answering, and multi-modal information retrieval. Our work builds upon these foundations, focusing on the interpretability aspect of VLMs, an area that is crucial for their broader adoption and trustworthiness.



\noindent\textbf{Interpretability in AI} The interpretability of AI systems has garnered significant attention, driven by the need for transparency, accountability, and trust in AI applications \cite{xu2019explainable, naveed2023comprehensive, bills2023language, rai2024practical,singh2024rethinking, bereska2024mechanistic}. The black box nature of many sophisticated AI models, including VLMs, poses challenges to understanding their decision-making processes. Bills \etal \cite{bills2023language} explore automatic explanation mechanisms for language models but have not yet studied how to explain VLMs. The domain of VLMs presents unique challenges due to the multi-modal nature of the data they process. Stan \etal~\cite{stan2024lvlm} design an interface to visualize how generated outputs are related to the input image through raw attention, relevancy maps, and causal interpretation. Guertler \etal \cite{guertler2024tellme} propose a method to generate and evaluate explanations for neurons in vision models. To the best of our knowledge, there is rare work on investigating the functions of neurons in VLMs. We aim to delve into the internals of VLMs to unveiling the mystery of VLM neurons.



\section{Method}

For VLMs, it is still a mystery of the functions of neurons and whether they tackle visual tokens and text tokens differently. 
%We aim to unveiling the mystery of neurons in VLMs. 
Following \cite{bills2023language}, we conduct analyses based on the neuron responses with respect to tokens. Differently, we jointly consider visual tokens and text tokens to win insights of VLMs.

%Figure \ref{} illustrate the flowchat of our framework.   
Figure \ref{fig:framework} illustrates the main workflow to investigate the functions of neurons in a VLM (\egno, LLaVA). A visual-language model takes an image and the text prompt as input and output the predicted texts. For each neuron, we record its activations with respect to each visual token and text token for a sample (see an example in Figure \ref{fig:Response}), which will be analyzed to uncover the function of the neuron. 
Neuron explanations are automated by providing their top-activated samples (and activations) as input to the explainer. 
%We automate the explanation of a neuron by taking its top activated samples (and activations) as input to the explainer. 
We evaluate the reliability of the explanations with the assistance of a visual simulator and a text simulator. 
% \tcp
{Without loss of generality, we conduct analysis on the representative VLM of LLaVA 1.5 (7B). Similar trends were found in the VLM of InternVL 2.5 (8B) (see Appendix).} Particularly, we reveal that there are neurons responsible only for vision, neurons responsible only for text, and neurons responsible for both vision and text. We study the characteristics of them to gain a deeper understanding of the internal workings.




\subsection{Existence of specific neurons}
%As the field of artificial intelligence advances, multi-modal learning has emerged as a vibrant area of research. It seeks to enrich data comprehension and accuracy by weaving together information from disparate modalities, such as text, images, and sounds. 
There are few studies on the internal working of VLMs capable of processing information from both visual tokens and text tokens. 
This raises a compelling question: do these models contain neurons that are uniquely responsive to specific modalities.
%A compelling question arises: do models house neurons that are uniquely responsive to specific modalities?  
%Through meticulous investigation into the internal neurons, we have uncovered an intriguing phenomenon: not all neurons are uniformly engaged during the model's reasoning processes. Specifically, certain neurons are predominantly activated for processing text tokens, whereas others exhibit heightened activity when processing visual tokens. 
% As the field of artificial intelligence advances, multi-modal learning has emerged as a vibrant area of research. It seeks to enrich data comprehension and accuracy by weaving together information from disparate modalities, such as text, images, and sounds. Through meticulous investigation into the intricate internal mechanisms of sophisticated models, we have uncovered an intriguing phenomenon: not all neurons are uniformly engaged during the model's reasoning processes. Specifically, certain neurons are predominantly activated for language understanding when the model processes textual tasks, whereas others exhibit heightened activity during visual tasks or when handling other visual stimuli. This observation raises a compelling question: do models house neurons that are uniquely responsive to specific modalities?  

To demystify this, particularly, we studied a vast array of neurons within a VLM, scrutinizing their reactions across both visual and text modalities. Our findings reveal some fascinating things. \textbf{1)} Some neurons demonstrate pronounced responsiveness to visual tokens while exhibiting diminished activation for text tokens, as illustrated in Figure \ref{fig:neuron_functions}~(a). \textbf{2)} Some neurons demonstrate high responsiveness to text tokens while exhibiting diminished activation for visual tokens, as showcased in Figure \ref{fig:neuron_functions}~(b). We name these as `visual neurons' and `text neurons', respectively. \textbf{3)} Moreover, we also found neurons that simultaneously have high activations for both visual and text tokens, as depicted in Figure \ref{fig:neuron_functions}~(c)-(f), which we refer to as `multi-modal neurons'. For example, in (d), the visual tokens and text related to `doughnuts' have high activations. Intriguingly, we observed that the responses to visual and text tokens usually deliver consistent concept in most cases. 


\subsection{Locating specific neurons}
\label{subsec:neuron_types}
Upon verifying the presence of neurons with specialized functions and multi-modal neurons, how to automatically identify them (pinpoint their exact locations)  from the abundant  neurons? 




We define four categories of neurons as: visual neurons, text neurons, multi-modal neurons, and unknown neurons, respectively.
To classify a given neuron, we analyze its top $N$ samples with the highest activation values (e.g., $N=50$).
By analyzing the responses of this neuron on the $N$ samples, we estimate the probability of this neuron belonging to visual neuron, text neuron, multi-modal neuron, or unknown neuron as $(p_{v}, p_{t}, p_{m}, p_{u})$, where $p_{v} + p_{t} + p_{m} + p_{u} = 1$. Specifically, we calculate the proportion of samples in which the neuron's activations meet the criteria for visual, text, or multi-modal responses.  
For a sample, if the number of highly activated visual tokens (where the activation value is greater than $T_v=2$) is larger than $n_v$ ($n_v=4$), we think this neuron activates visual tokens of this sample and name this sample  visual-activated sample. Similarly, if the number of highly activated text tokens (where the activation value is greater than $T_t=3$) is larger than $n_t$ ($n_v=2$), we think this neuron activates text tokens and name this sample text-activated sample. 
As illustrated in Table~\ref{tab:sample_category}, we define a sample as a visual sample whenever this sample is visual-activated but not text-activated. Similarly, we define a sample as atext sample whenever this sample is text-activated but not visual-activated. A sample is defined as a multi-modal sample whenever it is both text-activated and visual-activated. A sample is defined as an unknown sample whenever it is neither text-activated nor visual-activated. Based on the distribution of four types of samples, we obtain the probability distribution of this neuron $(p_{v}, p_{t}, p_{m}, p_{u})$ of belonging to the four types of neurons. In general, the higher the probability $p_v$, the neuron is more prone to be a visual neuron. We can estimate the type of a neuron based on the probability distribution. More analysis can be found in the experimental section.   


\begin{table}[t]%[b]  
\centering  
\resizebox{0.42\textwidth}{!}{ 
\begin{tabular}{c c c}  
\hline  
Sample category   & Visual-activated & Text-activated     \\ \hline  
Visual sample & \ding{51} & \ding{55}       \\  
Text sample & \ding{55} & \ding{51}       \\  
Multi-modal sample & \ding{51} & \ding{51} \\  
Unknown sample & \ding{55} & \ding{55}       \\ \hline  
\end{tabular}  
}
\caption{Definition of four types of samples respectively, based on whether this sample is visual-activated and text-activated by a given neuron.}  
\label{tab:sample_category}  
\vspace{-4.5mm}
\end{table} 

\subsection{Explanation and simulation}
Upon identifying a neuron as specialized, we delve further into the neuron's function in representing certain concepts. 
For a visual neuron, if it in general exhibits high responses to visual tokens featuring a ``person" and low or no response to other tokens, the neuron may be responsible for the concept of ``person".  Similarly, a text neuron may exhibit selectivity and focus on particular words or positions within sentences to be responsible to certain concepts. 
% For a visual neuron associated with the concept of ``person", it is expected to exhibit a high response to image segments featuring a ``person" and low or no response to other segments. Similarly, a text neuron may exhibit selectivity, focusing on particular words or positioning within sentences to be responsible to certain concept. 

Instead of interpreting the function of a neuron by a human, our aim is to develop a framework that enables the automatic explanation and assessment of the explanations. Bills \etal \cite{bills2023language} employed GPT-4 to develop an explanation-simulation framework. However, their analysis was confined to text tokens, incapable of the handling both visual and textual domains. Directly extending it by using visually-capable LLMs, such as GPT-4o, is infeasible for simulation. Although GPT-4o has the capability to understand images, it lacks the capability to pinpoint specific objects within an image and results in the difficulty in simulation of visual activation. Consequently, designing a pipeline tailored for visual token simulation is crucial for enabling simulation of activations to evaluate the quality of explanations.
%advancing our understanding and capabilities in this area.
%In essence, GPT-4o can recognize contents within an image but falls short when tasked with locating objects, such as identifying the position of a chair in a room image.

As illustrated in Figure \ref{fig:framework}~(b), our framework comprises two steps: explanation and simulation. 

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{figures/img_maskximage_simulate_pair.pdf} \\
\vspace{-1mm}
\caption{An example for illustrating (a) the original image, (a) activation-modulated image, and (c) simulated activation-modulated image of a visual neuron. Note that the explanation of this neuron is ``concept related to lamps". We can see that the image patches where the desk lamp is located are mainly activated by this visual neuron in (b), and the image patches where the desk lamp is located are activated by the simulator in (c).}
% \caption{\tcr{An example of the image, activation-modulated image pair and simulated activation of an visual neuron. The explanation of this neuron is "concept related to lamps". Only the image patches where the desk lamp is located are activated by the visual neuron in (b) and the image patches where the desk lamp is located are activated by the simulator in (c).}}
\vspace{-4.5mm}
\label{fig:img_maskximage_simulate_pair}
\end{figure}


\noindent\textbf{Explainer} In the explanation phase, for a given neuron, we select $k$ activation-modulated image-text pairs alongside their corresponding activation values as inputs to the explainer. Here, we use GPT-4o as the explainer. The explainer outputs the explanation of the function of this neuron. For the text part, the pairs of (text token, activation) are input to GPT-4o to facilitate the reasoning. For the visual part, because GPT-4o is a black-box where the visual tokenization is not the same as the VLM to be interpreted, it is infeasible to input the visual token and activation as what text part does. We attempted to use VQ-GAN quantized token index and activation as the input to GPT-4o but failed. The reason is that this strategy requires abundant context examples to fully catch the meaning of the indexes of VQ-GAN, where only a few context examples are far from enough to learn the reasoning capability. Abundant context examples would increase the computation burden and exceed the contextual length limitation. We propose to input the activation modulated image to facilitate the explainer to understand the activations of visual regions. Figure~\ref{fig:img_maskximage_simulate_pair} (a) and (b) shows an example of the image and the activation-modulated image, respectively. Note that the more transparent the image patch, the more activated this visual token is. 
Please see more details about the prompt in our Appendix.   

\noindent\textbf{Simulator} In the simulation phase, the visual simulator receives both the explanation provided by the explainer and the images as input to generate/simulate the activation value for each visual token. 
% We leverage the vision-language model CLIP \cite{radford2021learning} to build our visual simulator. Particularly, the explanation is encoded by the CLIP text encoder and taken as the text query to calculate the affinity to the visual tokens encoded by CLIP visual encoder. In general, for a visual token, the more correlated to the text query, the high affinity it has. The affinity values are taken as the simulated activations. Figure~\ref{simu-activation} shows an example of the simulated activation, given the explanation and an input image. We can see that the simulated activations present high positive correlation with the real activations.
We employ Grounded SAM 2 \cite{ren2024grounded} as our visual simulator, which processes text prompt and the image as input, and outputs object mask with respect to the text prompts. 
The mask is then mapped to activation taking the assigning regions as 1 and the unmasked regions as 0. 

Similarly, the text simulator takes the neuron explanation and text tokens as input and outputs the activation value for each text token. We use GPT-4o as the text simulator. Figure~\ref{fig:img_maskximage_simulate_pair}~(c) shows an example of the simulated activations of a visual neuron, given an input image and the explanation of the neuron (for more visualization, refer to Figure~\ref{fig:vis_vision}-\ref{fig:vis_multi}).
More details about the prompt for the simulator are described in our Appendix.


\noindent\textbf{Scoring} Subsequently, we compute the Pearson correlation coefficient \cite{bills2023language, pearson1895notes} between token activations and ground truth (GT) token activations, which produces a simulation score that quantifies the quality of the generated explanation.


\section{Experiments}
\label{sec:experiment}

% \tcp
{We conducted experiments based on LLaVA 1.5 (7B) \cite{Liu_2024_CVPR}. More results based on VLM of InternVL 2.5 (8B) \cite{chen2024expanding} can be found in Appendix \ref{sec:InternVL}.}

\subsection{Neuron activation dataset}

Here, we delve into the details of obtaining the neuron activation dataset derived from the representative VLM, LLaVA 1.5 (7B). 
%Our objective is to elucidate the quality and interpretability of the neurons within this model. 
We curated a subset of data from LLaVA 1.5 training dataset. This includes 23k image and text pairs of detailed descriptions, with the images sourced from the COCO2017 \cite{lin2014microsoft} dataset. 
We conduct the anlaysis on the 23k image-text pairs.
%This careful curation process ensures a robust foundation for our subsequent analyses.
%We curated a subset of data from LLaVA 1.5 \cite{Liu_2024_CVPR} training dataset, prioritizing both the quality of the data and the clarity of neuron interpretation. This involved the selection of 23k image and text pairs of detailed descriptions, with the images being sourced from the COCO2017 \cite{lin2014microsoft} dataset. This careful curation process ensures a robust foundation for our subsequent analyses and discussions.

Similar to \cite{tang2024language, pan2023finding}, our study focuses on the interpretability of neurons in the Feedforward Neural (FFN) layers. To assess neuron activations, we gauge the output from the activation function of the FFN layers as the neuron activations. During the data collection process, for each sample, an image and its text prompt are fed into LLaVA 1.5 \cite{Liu_2024_CVPR} for inference and we record the responses to tokens.
%, during which we document each neuron's responses to next output token. 
For each neuron, we meticulously select the top $N=50$ samples eliciting the maximum response values from the 23k samples for analysis. Given that LLaVA 1.5 7B comprises 32 blocks with 11,008 neurons in each FFN layer of a block, there are a total of 352,256 neurons for analysis. 


\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figures/pdf_pv_pt_pm_pu.pdf} \\
\vspace{-3mm}
\caption{Histogram for the probability $p_v$, $p_t$, $p_m$, and $p_u$ of belonging to visual neuron, text neuron, multi-modal neuron, and unknown neuron types, respectively.}
\vspace{-2mm}
\label{fig:pdf_pv_pt_pm_pu}
\end{figure*}

\begin{figure}[t]
\centering
\includegraphics[width=0.7\linewidth]{figures/dist_4cls_neurons.pdf} \\
\vspace{-1mm}
\caption{Distribution of the four types of neurons (visual-prone neurons, text-prone neurons, multimodal-prone neurons, unknown neurons) when we treat the determination of the neuron type as a classification problem.}
\vspace{-4mm}
\label{fig:dist_4cls_neurons}
\end{figure}

\subsection{Findings of neuron functions}
We artificially observed responses of abundant neurons. Based on the observations, we found that there exist visual neurons, text neurons, and multi-modal neurons, which are mainly responsible only for visual tokens, text tokens, and both, respectively. 

We find that the neurons in VLM are not homogeneous. Figure \ref{fig:neuron_functions} shows some typical examples. For visual tokens, a higher transparent degree indicates a higher activation. For text tokens, the darker of the green color indicates a higher activation.
Specifically, Figure \ref{fig:neuron_functions}~(a) shows that the prominent activations of the neuron predominantly occurs in text tokens, with low activations for visual tokens. Such neuron is responsible for text and we refer to it as text neuron. Conversely, as demonstrated in Figure \ref{fig:neuron_functions}~(b), the activation pattern of another neuron category shows high responses to visual tokens while ignorable responses to text tokens. These neurons are referred to as visual neurons. Notably, as presented in Figure \ref{fig:neuron_functions}~(c)-(f), a third category of neurons exhibits significant activations on both visual and text tokens, underscoring their role in processing multi-modal content. We refer to them as multi-modal neurons. 
% Through extensive observations, we have categorized neuron functionalities. As depicted in Figure \ref{fig:neuron_functions}, our analysis reveals the existence of visual neurons, text neurons, and multi-modal neurons, which concurrently process both visual and textual information. Specifically, Figure \ref{fig:neuron_functions}(a) illustrates that the activation of such neurons predominantly occurs in visual tokens, with markedly low activation for text tokens, indicating minimal engagement. These neurons are henceforth referred to as visual neurons. Conversely, as demonstrated in Figure \ref{fig:neuron_functions}(b), the activation pattern of another neuron category diametrically opposes that of visual neurons, showing a heightened response to text tokens while minimally engaging with visual tokens. These neurons are designated as text neurons. Notably, as presented in Figure \ref{fig:neuron_functions}(c)-(f), a third category of neurons exhibits significant activation in response to both visual and text tokens, underscoring their role in processing multi-modal content. Accordingly, we term these neurons as multi-modal neurons.

For multi-modal neurons, our findings highlight an intriguing alignment in the response semantics to both visual and text tokens. This alignment encompasses correlations with actual objects as well as abstract states. Figures \ref{fig:neuron_functions}~(c)(d) elucidate the alignment related to actual objects, demonstrating that both image and text consistently are related to the concepts of ties and donuts, respectively. Figure \ref{fig:neuron_functions}~(e) exemplifies the alignment of abstract states, where text tokens with high activation values such as walking, running, interacting, and standing are aligned with the highly-activated image regions of the zebras. Similarly, Figure \ref{fig:neuron_functions}~(f) showcases text tokens with pronounced activations relating to drinking, standing (referring to birds), and walking (relating to humans), alongside activated image patches depicting birds and humans. These observations show that the multi-modal neurons usually deliver aligned concepts over text and visual tokens.
%capability to synchronize the visual representations of objects with their respective actions and states.
% In the context of multi-modal neurons, our findings highlight an intriguing alignment in the response semantics to both vision and text tokens. This alignment encompasses correlations with actual objects as well as abstract states. Figures \ref{fig:neuron_functions}~(c)(d) elucidate the alignment concerning actual objects, demonstrating that both image and textual descriptions consistently reference the concepts of ties and donuts. Figure \ref{fig:neuron_functions}~(e) further exemplifies the alignment of abstract states. text tokens with high activation values such as walking, running, interacting, and standing are juxtaposed with an image of a zebra. Similarly, Figure \ref{fig:neuron_functions}~(f) showcases text tokens with pronounced activation in the context of drinking, standing (referring to birds), and walking (pertaining to humans), alongside activated image patches depicting birds and humans. This observation underlines the neurons' capability to synchronize the representation of objects with their respective actions and states.

Besides the three types of neurons, there are neurons that have low responses on both text tokens and visual tokens. We refer them to unknown neurons. Such neurons may be redundancy neurons that are not well optimized. 
The multifaceted functionality of neurons contributes to VLM's strong visual understanding and reasoning capability.  %visual-language model, facilitating a deeper understanding of the internals of a VLM.


\subsection{Distributions of different types of neurons}

How can we automatically identify the type of neuron? As discussed in Subsection \ref{subsec:neuron_types}, based on the highly activated samples, we can estimate the probability of belonging to the four different neuron types by $(p_v, p_t, p_m, p_u)$. 

We show the histogram for the probability of $p_v$, $p_t$, $p_m$, and $p_u$ over all the neurons, respectively in Figure~\ref{fig:pdf_pv_pt_pm_pu}(a)-(d). 
% We show the accumulated histogram in Figure~\ref{fig:cdf}(a)-(d) in our Appendix. 
Figure~\ref{fig:pdf_pv_pt_pm_pu}~(a) shows that there is a small portion of neurons having very high probability of being visual neurons (where $p_v$ approaches 1). There are about 5.4\% neurons having probability higher than 80\% to be visual neurons. From Figure~\ref{fig:pdf_pv_pt_pm_pu}~(b)-(d), there are about 14.2\%, 5.5\%, and 5.7\% neurons having a probability higher than 80\% to be text neurons, multi-modal neurons, and unknown neurons, respectively. 

When we consider the identification of neuron type as a classification problem by recognizing the type of high probability as the neuron type, we obtain the class distribution for the four types of neurons (we refer to them as visual-prone neurons, text-prone neurons, multimodal-prone neurons, unknown neurons) from all neurons (352,226) as shown in Figure~\ref{fig:dist_4cls_neurons}. In fact, as shown in Figure~\ref{fig:pdf_pv_pt_pm_pu}~(a), besides a small portion of neurons that can be clearly identified of their types, there are abundant neurons that are less confident to be clearly identified as specific neurons. For example, a neuron may have the probability of $p_v=0.4$, $p_t=0.1$, $p_m=0.3$, and $p_u=0.2$ of being the four types. Therefore, more precisely, we refer to it as a text-prone neuron. From Figure~\ref{fig:dist_4cls_neurons}, we can see that text-prone neurons have the highest frequency, while the other three types of neurons exhibit similar proportions among all the neurons.  
%It class this neuron as visual neuron since $p_v$ is the largest probability. Figure~\ref{fig:dist_4cls_neurons} shows the class distribution for the four types of neurons.    

\begin{figure*}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figures/distribution_special_neurons.pdf}
   \vspace{-6mm}
      \caption{The distribution of special neurons over different layers. (a)-(d) show four categories of neurons, namely visual, text, multi-modal and unknown neurons. The horizontal axis denotes the model layer, from 0 to 31. The vertical axis denotes the number of neurons in the corresponding layer.} 
      \vspace{-2mm}
   % \caption{The distribution of special neurons. Sub-figure (a)-(d) show four categories of neurons, namely vision, text, multi-modal and unknown neurons. The horizontal axis is the model layers, from 0 to 31. The vertical axis is the number of neurons in the corresponding layer.} 
   \label{fig:distribution_special_neurons}
\end{figure*}

\begin{figure*}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=\myratio\linewidth]{figures/visualization_vision_neuron.pdf}
   \vspace{-2mm}
   \caption{Visualization of real activations and simulation results of a visual neuron.} 
   \label{fig:vis_vision}
\end{figure*}

\subsection{Neuron distributions across layers}

In this section, we explore the distribution of four distinct neuron types across the entirety of the model. We selected the top 3\% (\ieno, about 10,000 neurons in total, selected based on the probability) neurons for each neuron category to conduct the analysis. The distributions of the neurons across the blocks/layers are shown in Figure \ref{fig:distribution_special_neurons}.
% In this section, we explore the distribution of four distinct neuron types across the entirety of the model. As outlined in the methodology section, we selected the top 3\% (10,000 neurons) for each specialized neuron category in our analysis, with the distribution across the blocks/layers illustrated in Figure \ref{fig:distribution_special_neurons}.



% \begin{figure*}[t]
%   \centering
%   %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    \includegraphics[width=\myratio\linewidth]{figures/visualization_text_neuron.pdf}
%    \caption{Visualization of real activations and simulation results of a text neuron.} 
%    \label{fig:vis_text}
% \end{figure*}

\begin{figure*}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=\myratio\linewidth]{figures/visualization_vision_text_neuron.pdf}
   \caption{Visualization of real activations and simulation results of a multi-modal neuron.} 
   \vspace{-1mm}
   \label{fig:vis_multi}
\end{figure*}

We can see that for visual neurons and text neurons, they present high frequency in the early and middle layers while low frequency in the high layers.
In contrast, the multi-modal neurons have higher frequency in the high layers. That may because in the early and middle layers, more visual neurons and text neurons are learned to focus on processing the individual modality. In the high layers, more multi-modal neurons are learned to jointly deal with visual and text information, enabling efficient reasoning of visual and text information.  

%The unknown neurons have low activations for both visual tokens and text tokens. Maybe these are redundancy neurons and are not well optimized yet. 
%We are still unclear about their functions. 


\subsection{Illustration of simulation results}
We visualize the simulation results of three types of neurons with different functions. For each type of neurons, we show simulation results for two samples of the same neuron.
%In this section, we visualize the simulation results of three neurons with different functions. For each type of neuron, we show simulation results for two samples of the same neuron.

For the visual neurons, the results are shown in Figure \ref{fig:vis_vision}. When observing the real activations (groundtruth) of both the visual and text tokens, there are predominant activations over visual tokens but negligible activations for text tokens. Further scrutiny of the activations over visual tokens reveals that the visual semantics elicited by the visual simulator are in harmony with those real activations. This substantiates the efficacy and logical soundness of our simulator.
% For the visual neurons, the results are depicted in Figure \ref{fig:vis_vision}. Upon examination of both the visual and text tokens corresponding to the ground truth, it is evident that there is a predominant activation in response to image tokens, with negligible to no activation for text tokens. This observation aligns with the characteristics of a visual neuron, thereby indirectly validating the precision and efficacy of our methodology in identifying visual neurons. Further scrutiny of the visual tokens, as predicted, reveals that the visual semantics elicited by the visual simulator are in harmony with those anticipated, with an absence of activation in text tokens akin to the ground truth observations. Taken together, these findings substantiate the efficacy and logical soundness of our simulator.



Figure \ref{fig:vis_multi} shows an example of multi-modal neuron. We can see the neuron exhibited significant responses to both visual and text tokens related to airplanes and airports, which is consistent with the explanation. This coherence between visual and textual responses underscores the neurons' capacity to capture information across modalities. Such findings highlight the interesting functionality of multi-modal neurons in bridging the gap between disparate modalities.
%, reinforcing the intricacies of neural representation in processing and integrating multi-modal information.

% Finally, our investigation extended to multi-modal neurons. From figure \ref{fig:vis_multi}, we can see these neurons exhibited significant responses to both visual and text tokens. Remarkably, the content of the visual and textual responses was nearly identical, with the neurons specifically elucidating concepts related to airplanes or airports. This coherence between visual and textual activations underscores the neurons' capacity to synthesize information across modalities, effectively encoding complex, theme-specific knowledge. Such findings highlight the sophisticated functionality of multi-modal neurons in bridging the gap between disparate forms of input, reinforcing the intricacies of neural representation in processing and integrating multi-modal information.

For text neurons, Figure \ref{fig:vis_text} in the Appendix shows a typical example of text neuron. We observed a consistently high activation for the first word in each sentence, which aligns with the explanation generated by the explainer. In contrast, activations over visual tokens were very small and very sparse since this neuron is a text neuron where the explanation is only correlated with text.
\footnote{This paper is the result of an open source research project starting from March, 2024.}
% In analysis of text neurons, we observed a consistently high activation probability for the first word in each sentence, corroborating our theoretical expectations, as shown in Figure \ref{fig:vis_text}. In contrast, activation among visual tokens was sparse. This phenomenon can be attributed to the fact that the initiation of a sentence or paragraph embodies a text-specific characteristic that inherently lacks a direct visual representation.



\subsection{Explanation quality}


We evaluate the quality of generated explanations for the neurons based on top activated samples, where we randomly selected 800 neurons of high confidence from each type of neurons. As shown in Table~\ref{table: top_vs_random}, the generated explanations obtain score of 0.160, 0.240, and 0.235 for visual neurons, text neurons, and multi-modal neurons, respectively. %three types of 

In addition, we explore the impact of two different sampling strategies for generating explanations of the neurons: the first method uniformly selects five samples~(top 1, 5, 9, 13, 17) from the top 20 samples to elucidate the function of the neuron, while the second randomly chooses five samples from the whole 23k image-text pairs. We conduct simulation of the activations on the uniformly selecting five samples  (top 3, 7, 11, 15, 19) from the top 20 samples for each neuron.  The more accurate of the generated explanations, the simulated activations will more closely approach the real activations and result in higher scores. The results are shown in Table \ref{table: top_vs_random}. We can see that the explanations generated from the top samples are more accurate than those of the random samples, indicating the efficiency of the sampling strategy.
% \subsection{Explanation quality using top samples v.s. random samples}
% % Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{table}[t]%[htbp]
%   \centering
%   \caption{Impacts of different sampling strategies on three neuron categories.}
%   \resizebox{1.0\linewidth}{!}{
%     \begin{tabular}{ccc}
%      \hline
%     Neuron category & Top sampling & Random sampling \\
%     \hline
%     Visual neurons & \textbf{0.161} & 0.139 \\
%     %\hline
%     Text neurons & \textbf{0.258} & 0.167 \\
%     %\hline
%     Multi-modal neurons & \textbf{0.233} & 0.165 \\
%     \hline
%     \end{tabular}%
%     }
%   \label{table: top_vs_random}%
%   \vspace{-2mm}
% \end{table}%

% In this section, we explore the impact of different sampling strategies for generating explanations. We employ two distinct approaches for sampling to generate explanations of the neurons: the first method uniformly selects five samples~(top 1, 5, 9, 13, 17) from the top 20 samples to elucidate the function of the neuron, while the second randomly chooses five samples from the whole 23k image-text pairs. We conduct simulation of the activations on the uniformly selecting five samples  (top 3, 7, 11, 15, 19) from the top 20 samples for each neuron.  The more accurate of the generated explanations, the simulated activations will more closely approach the real activations and result in higher scores. The results are shown in Table \ref{table: top_vs_random}. We can see that the explanations generated from the top samples are more accurate than those of the random samples, indicating the efficiency of the sampling strategy.

% Table generated by Excel2LaTeX from sheet 'Sheet1'

\begin{table}[t]%[htbp]
  \vspace{-2mm}
  \centering
  \caption{Explaination quality and impacts of different sampling strategies on three neuron categories.}
  \resizebox{1\linewidth}{!}{
    \begin{tabular}{ccc}
     \hline
    Neuron category & Top sampling & Random sampling \\
    \hline
    Visual neurons & \textbf{0.160} & 0.140 \\
    %\hline
    Text neurons & \textbf{0.240} & 0.149 \\
    %\hline
    Multi-modal neurons & \textbf{0.235} & 0.175 \\
    \hline
    \end{tabular}%
    }
  \label{table: top_vs_random}%
  \vspace{-5mm}
\end{table}%




\subsection{Impact of pruning unknown neurons}

We investigate the impact of pruning unknown neurons on LVM by evaluating its performance on visual question answering (VQA) task. The results show that dropping unknown neurons brings smaller performance drop than randomly dropping neurons (see Appendix \ref{sec:pruning_supp}).
%Please see the Appendix \ref{sec:pruning_supp} for more analysis.

% to achieve model light-weighting while maintaining the performance of large language models (LLMs) in visual question answering (VQA). Following \cite{maaz2023video}, we use average accuracy/score given by GPT-4o as metrics.

% \tcr{We sequentially pruned unknown neurons with activation probabilities $p_u \geq th$ and an equivalent number of randomly selected neurons from the language model, where $th \in \{1.0, 0.9, 0.8, 0.7, 0.6\}$. Subsequently, we assessed the VQA performance of the pruned models. As shown in Figure \ref{fig:pruning_acc_score}, the model performance gradually declined with increasing numbers of pruned unknown neurons, while pruning the same number of random neurons led to a more pronounced performance drop. This demonstrates the accuracy and effectiveness of our method for identifying unknown neurons. }




% \section{Downstream task: pruning}
% \tcr{In this section, we investigate the impact of pruning these unknown neurons to achieve model light-weighting while maintaining the performance of large language models (LLMs) in visual question answering (VQA). Following \cite{maaz2023video}, We use average accuracy/score given by GPT-4o as metrics.}

% \tcr{We sequentially pruned unknown neurons with activation probabilities $p_u \geq th$ and an equivalent number of randomly selected neurons from the language model, where $th \in \{1.0, 0.9, 0.8, 0.7, 0.6\}$. Subsequently, we assessed the VQA performance of the pruned models. As shown in Figure \ref{fig:pruning_acc_score}, the model performance gradually declined with increasing numbers of pruned unknown neurons, while pruning the same number of random neurons led to a more pronounced performance drop. This demonstrates the accuracy and effectiveness of our method for identifying unknown neurons. }

% \begin{figure}[t]
%   \centering
%    \includegraphics[width=1.0\linewidth]{figures/pruning_acc_score.pdf}
%    \vspace{-5mm}
%       \caption{\tcr{Comparison of pruning unknown neurons and random neurons on the visual question answering task.}} 
%    \label{fig:pruning_acc_score}
% \end{figure}

\section{Conclusion}
Our study provides new insights into the internal workings of vision-language models (VLMs) by systematically analyzing the functions of individual neurons. Our findings reveal the presence of specialized neurons for vision, text, and multi-modal processing, which we refer them as visual neurons, text neurons, and multi-modal neurons. We developed a framework utilizing GPT-4o for automated neuron explanation and introduced an activation simulator to evaluate the reliability of explanations for visual neurons. Through comprehensive analysis on a representative VLM of LLaVA, we shed light on distinct neuron behaviors and characteristics, contributing to the transparency and interpretability in VLMs.


\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{More implementation details}

In this section, we further provide more implementation details of our explanation and simulation framework. The pipeline can be divided into three steps: explanation, simulation, and scoring. 

%We take the neurons of the first fully connected layer in each block of the 32 blocks as our studied neurons (with total number of 352,256).


\subsection{Explanation}

We use GPT-4o as the explainer. The explainer outputs the explanation of the function of this neuron.
The prompt is input to GPT-4o to drive the automatic explanation as shown in Figure \ref{fig:supp_prompt_exp}. The prompt consists of a system prompt (marked in black), a small number (\ieno, $M$) of examples as context (marked in green). 
%In each example, we provide the (token, activation) pairs and the manually drafted explanation of the function of the neuron. 
Similar to [4], the neural activations are normalized to discrete values between 0 and 10, where negative values are mapped to 0 and the maximum activation value of the neuron is mapped to 10, as the activations.

For a visual neuron, activation-modulated images of its top activated samples are input to the explainer to generate explanation. For a text neuron, the text and activation pairs are taken as input to the explainer. For a multi-modal neuron, activation-modulated images, and (text, activation) pairs are input to the explainer to generate explanation. 


\begin{figure*}[th]
  \centering
  % \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.90\linewidth]{figs_supp/supp_prompt_explanation.pdf}
   \caption{The prompt we used for the explainer. The system prompt is marked in black. We provide $M$=8 examples (marked in green) as the contextual examples to enable the explainer to learn the task. Given a neuron X to be explained, we input the top responded $N$=5 samples with each sample represented by the activation-modulated image, text tokens and text token activations (marked in red). The explainer outputs the explanation to the neuron.}
   \label{fig:supp_prompt_exp}
\end{figure*}




\subsection{Simulation}
Given a neuron and its explanation, simulator aims to predict the activations of tokens. Note that the activation values are discrete values from 0 to 10. 
The localization capability of GPT-4o and other VLMs is still poor. We cannot harness them to simulator the responses of visual tokens. Our attempt using GPT-4o failed since it cannot sense well the spatial visual patches/tokens and their positions. Therefore, we use different strategy to simulate the activations for visual and text tokens. 

%We use Grounded SAM 2 as the visual simulator, and GPT-4o as the textual simulator. %\cite{ren2024grounded} 

%\noindent\textbf{Visual simulator} For a given neuron, Grounded SAM 2 takes the explanation and the image as input and produces a binary mask for each referred concept/object in the explanation. Then we take the union of these binary masks to get the final mask. The position with the mask value of 1 indicates that the area is related to the explanation. To build correspondence to the 256 visual tokens, we partition the final mask into $16 \times 16$ grids. We calculate the mean of the mask values within each grid as the token activation. We normalize the activations to 0-10 by multiplying the mask by 10. Figure \ref{fig:supp_vision_neuron} and Figure \ref{fig:supp_multi_neuron} show some visualizations of the activations of the visual tokens. Even though the simulated activations are cleaner than the real activations, the consistency on the trends and focused areas makes this method a feasible solution to make simulation on visual tokens. More advanced methods can be used to replace the current one and we leave that as future work.

\noindent\textbf{Text simulator} We use GPT-4o as the text simulator. As illustrated in Figure \ref{fig:supp_prompt_sim}, with system prompt (marked in black) and $M$ examples (marked in green) as context, the simulator generates activation value for each text token for the test sample. 

For a text neuron, we do not include activation-modulated images as input to the text simulator but only input the text sample to be simulated (and the context examples). 

For a multi-modal neuron, the context examples include both the activation-modulated images and text activation pairs. Only the text sample is input to the text simulator.

\noindent\textbf{Visual simulator} As described in the main manuscript, we use Grounded SAM 2 \cite{ren2024grounded} as our visual simulator. We score the visual part based on the simulated activations and groudtruth activations. 

\begin{figure*}[th]
  \centering
  % \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.90\linewidth]{figs_supp/supp_prompt_simulation.pdf}
  \caption{The prompt we used for the text simulator. The system prompt is marked in black. We provide $M$=8 examples (marked in green) as the contextual examples to enable the simulator to learn the task. Given a testing sample, the simulator turns each `unknown' character of the text tokens to an activation value (an integer from 0-10) based on the explanation and prompt. Note that we use the text simulator of multi-modal neurons as example here. For the text simulator of text neurons, the activation-modulated images for both the examples and testing sample will not be taken as input. }
   \label{fig:supp_prompt_sim}
\end{figure*}

\subsection{Scoring}
In this section, we introduce the metric we use to compute the simulation scores and how to score for visual, text and multi-modal neurons, respectively.

\noindent\textbf{Metric} Following Bill \etal [4], we calculate the Pearson correlation coefficient between the simulated activations and the real activations to measure the degree of their correlation. The Pearson correlation coefficient between $X$ and $Y$ is calculated as follows:
\begin{equation}  
r_{xy} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2}\sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^2}},
\end{equation}
where $x_i$ and $y_i$ denote the observed values of $X$ and $Y$, respectively, $\bar{x}$ and $\bar{y}$ denote the means of $X$ and $Y$, respectively, and $n$ denotes the number of observations.

The values of the Pearson correlation coefficient range from -1 to 1. A value of 1 signifies a perfect correlation, whereas -1 denotes a perfect negative correlation. A value of 0 indicates the absence of any linear correlation.

\noindent\textbf{Scoring for specific neurons} For visual neurons, since their true activation values have no response on text tokens, it is meaningless to calculate the Pearson correlation coefficient between the true activations and the simulated activations on text tokens. Therefore, we only calculate the Pearson correlation coefficient (score) for visual tokens. Similarly, for text neurons, we only calculate the Pearson correlation coefficient between the true activation values and the simulated activation values on text tokens. For multi-modal neurons, we calculate the Pearson correlation coefficients on visual and text tokens separately and average them as the final score.

\FloatBarrier


\begin{table}[th]%[htbp]
  %\vspace{-1mm}
  \centering
  \caption{Comparison of dropping/pruning unknown neurons and randomly dropping neurons on the visual question answering task. w/o $p_u \geq \tau$ represents pruning unknown neurons that having probability of being unknown neurons larger than $\tau$. Performance is measured by accuracy/score. Full model represents the VQA performance without pruning. ``Random neurons" denotes randomly dropping neurons of the same ratio.}
  \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{cccccc}
     \hline
    {} & Full model & w/o $p_u \geq 0.9$ & w/o $p_u \geq 0.8$ & w/o $p_u \geq 0.7$ & w/o $p_u \geq 0.6$ \\
    \hline
    Unknown neurons (Acc./Score) & 65.6/3.46  & 64.1/3.40 & 61.3/3.37 & 60.3/3.32 & 58.9/3.26 \\
    % \hline
    Random neurons (Acc./Score) & 65.6/3.46 & 62.9/3.37 & 57.7/3.24 & 55.9/3.18 & 54.3/3.14 \\
    % \hline
    Pruning ratio~(\%) & 0.0 & 3.2 & 6.4 & 10.2 & 14.6 \\
    \hline
    \end{tabular}%
    }
  \label{table: pruning}%
  \vspace{-1mm}
\end{table}%

%\FloatBarrier
\begin{figure}[th]
  \centering
   \includegraphics[width=0.9\linewidth]{figs_supp/supp_pruning_acc_score.pdf}
   \vspace{-5mm}
      \caption{Comparison of dropping unknown neurons and randomly dropping neurons on the visual question answering task.}
   \label{fig:supp_pruning_acc_score}
\end{figure}

\section{More analysis on top of LLaVA}

\subsection{Impact of pruning unknown neurons}
\label{sec:pruning_supp}


Unknown neurons exhibit small activations in response to both visual and text tokens. 
We investigate the impact of pruning unknown neurons on LVM by evaluating its performance on visual question answering (VQA) task. We randomly sampled 1,000 instances from the COCO dataset for evaluation.

Following \cite{maaz2023video}, we use average accuracy/score given by GPT-4 to measure the quality of generated answers. The accuracy is calculated based on the binary correctness given by the evaluator. A score ranging from 0 to 5 is assigned to quantify the similarity between the predicted answers and the ground truths, with higher scores indicating closer alignment.

We sequentially pruned unknown neurons by selecting those having high probability of being unknown neurons (\ieno, $p_u \geq \tau$), where we set $\tau$ as 0.9, 0.8, 0.7, and 0.6, respectively.  As shown in Table~\ref{table: pruning}, 3.2\% to 14.5\% neurons were dropped based on those threshoulds.  In addition, we randomly drop neurons of the same ratios for comparison as marked by ``Random neurons". Figure~\ref{fig:supp_pruning_acc_score} also shows the performance curves for the two schemes.
We can see that dropping/pruning unknown neurons leads to smaller performance drop than randomly dropping neurons, indicating the unknown neurons with high confidence (high $p_u$ value) have less influence on the model performance.










\subsection{More visualization of simulation results}

Figure \ref{fig:vis_text} shows a typical example of text neuron. We observed a consistently high activation for the first word in each sentence, which aligns with the explanation generated by the explainer. 
In Figure~\ref{fig:supp_vision_neuron}-\ref{fig:supp_multi_neuron}, we present more visualization of visual, text and multi-modal neurons, respectively. We can see that the visual neurons exhibit large activations to some visual tokens, with negligible activations to text tokens. Text neurons have large activations on some text tokens while negligible activations on visual tokens. Multi-modal neurons, on the other hand, demonstrated significant responses to both visual and text tokens, with the represented concepts using being aligned across both modalities.  

   
Moreover, we observe that the predicted activations are consistent with the groundtruth activations, indicating the reliableness of the generated explanations from the explainer. The explanations are accurate and consistent with human's understanding.



\begin{figure*}[t]
  \centering
  \vspace{-2mm}
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=\myratio\linewidth]{figures/visualization_text_neuron.pdf}
   \caption{Visualization of real activations and simulation results of a text neuron.} 
   \label{fig:vis_text}
   \vspace{-6mm}
\end{figure*}


% \begin{figure}[t]
%   \centering
%   % \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=0.9\linewidth]{figs_supp/supp_vis_vision_neuron.pdf}
%    \caption{Visualization of real activations and simulation results of three visual neurons.}
%    \label{fig:supp_vision_neuron}
% \end{figure}

% \begin{figure}[t]
%   \centering
%   % \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=0.9\linewidth]{figs_supp/supp_vis_text_neuron.pdf}
%    \caption{Visualization of real activations and simulation results of three text neurons.}
%    \label{fig:supp_text_neuron}
% \end{figure}

% \begin{figure}[t]
%   \centering
%   % \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=0.9\linewidth]{figs_supp/supp_vis_multi_neuron.pdf}
%    \caption{Visualization of real activations and simulation results of three multi-modal neurons.}
%    \label{fig:supp_multi_neuron}
% \end{figure}

\begin{figure}[t]
  \centering
  % \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.9\linewidth]{figs_supp/supp_vis_vision_neuron.pdf}
   \caption{Visualization of real activations and simulation results of three visual neurons.}
   \label{fig:supp_vision_neuron}
\end{figure}

\begin{figure}[t]
  \centering
  % \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.9\linewidth]{figs_supp/supp_vis_text_neuron.pdf}
   \caption{Visualization of real activations and simulation results of three text neurons.}
   \label{fig:supp_text_neuron}
\end{figure}

\begin{figure}[t]
  \centering
  % \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.9\linewidth]{figs_supp/supp_vis_multi_neuron.pdf}
   \caption{Visualization of real activations and simulation results of three multi-modal neurons.}
   \label{fig:supp_multi_neuron}
\end{figure}


\subsection{Ablation on the explainer: using raw images v.s. activation-modulated images}

For the explainer, we provide contextual examples to enable its explanation capability. To facilitate the awareness of visual token activations for the explainer, we use activation-modulated images as input instead of the original image. We compare the performance of the two manners. As shown in Figure \ref{fig:supp_activation_modulated}~(a), with all the five raw images depict indoor scenes, the explanation generated by the explainer is ``things related to indoor settings or rooms".  In contrast, by using activation-modulated images, the explainer gives a more reasonable explanation ``things related to lamps". Similarly, in Figure \ref{fig:supp_activation_modulated}~(b), being aware that the neuron is mainly activated related to the concept of ``people", the explainer provides more reasonable explanation ``people holding or interacting with objects".
%rather than food and dining.

% \section{Explanation: raw images v.s. activation-modulated images}
% \tcr{In this section, we study the benefits of using activation-modulated images over raw images in the explanation step. We use raw images and activation-modulated images as input to the explainer (GPT4o) and then compare the explanations given by the explainer, as shown in Figure \ref{fig:supp_activation_modulated}. In Figure \ref{fig:supp_activation_modulated}(a), all five raw images depict indoor scenes, so the explanation given by the explainer is things related to indoor settings or rooms. Actually, by observing the activation-modulated images, it can be found that the neuron is only activated on the concept of lamps. Therefore, the explainer gives a more reasonable explanation of things related to lamps. Similarly, in Figure \ref{fig:supp_activation_modulated}(b), the neuron is mainly activated on the concept of "people" rather than food and dining.}


\begin{figure*}[th]
  \centering
  % \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.9\linewidth]{figs_supp/supp_activation_modulated.pdf}
   \caption{Visualization of the impact of using original images versus using activation-modulated images as the explainer input during the explanation step. Here we use visual neurons as examples. (a) The generated explanation using original images is ``things related to indoor settings or rooms". In contrast, The generated explanation using the activation-modulated images is ``things related to lamps", which is more adhere to the visual activations.}
   \label{fig:supp_activation_modulated}
\end{figure*}



\subsection{Neuron monosemanticity and polysemanticity}

%\tcr{In this section, we will analyze the monosemanticity and polysemanticity of visual neurons, text neurons, and multi-modal neurons in the visual large language model.}


We found that different neurons exhibit different  characteristics. Some neurons are prone to be monosemanticity while some are prone to be polysemanticity. Monosemantic neurons' responses are prone to be highly specific, with each neuron reacting to a particular concepts or piece of information. Polysemous neurons display a more complex behavior pattern, capable of responding to multitude of different concepts simultaneously. 
%This means a polysemous neuron can integrate multiple signals or features in its activity, thereby playing an important role in high-level feature fusion and complex pattern recognition. The existence of these two types of neurons not only reveals the diverse mechanisms within neural networks but also offers different perspectives for exploring and optimizing network design and functionality.}

We analyze by observing the top 50 samples of the maximum activation value of the neuron. Figure \ref{fig:supp_mono_poly_semantic_vision}~(a) shows the activations of a monosemantic neuron. For different input images, this neuron has a higher activation value only on the image patches related to lamp. Figure \ref{fig:supp_mono_poly_semantic_vision}~(b) shows the activations of a polysemantic neuron, where some images have high responds to people (the first two rows) while some images have high responds to the bracket (the last row). 

For text neurons, some neurons are monosemantic. As shown in Figure \ref{fig:supp_mono_poly_semantic_text}~(a), this neuron focus on ``the beginning of sentences or paragraphs". In Figure \ref{fig:supp_mono_poly_semantic_text}~(b), the first two examples show that the neuron has high responses mainly on the text tokens ``There are" and ``The table", while in the last example the neuron is activated on the words ``laptop", ``monitor" and ``mouse". This indicates that the neuron is polysemantic. 

For multi-modal neurons, Figure \ref{fig:supp_mono_poly_semantic_both}~(a) shows a monosemantic multi-modal neuron which is activated related to the concepts ``airplane" and ``airport" on both visual tokens and text tokens. Figure \ref{fig:supp_mono_poly_semantic_both}~(b) shows a polysemantic multi-modal neuron that responds highly to both donuts and carrots. 

Sometimes, monosemanticity and polysemanticity cannot be clearly distinguished, since a couple of concepts could be summarized by a higher level concept. For exmpale, ``airplane" and ``airport" can be considered as two different concepts or considered as a concept related to ``aviation". 


\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.7\linewidth]{figs_supp/supp_mono_poly_semantic_vision.pdf}
  \caption{Examples for observing the monosemanticity or polysemanticity of visual neurons. (a) A monosemantic visual neuron, where its top 50 samples with the largest activation responses mainly are activated over the region related to lamps. Note we only show three images here to save space. (b) A polysemantic visual neuron that responds highly to both the concepts of people (the first two rows) and bracket (the last rows).}
   % \caption{\tcr{Visualization of monosemanticity and polysemy of visual neurons. For visual neurons, we only focus on the vision input and ignore the activation on the text. (a) A monosemantic visual neuron, whose top 50 samples with the largest activation responses, activates only in the concept of lamps. (b) A polysemantic visual neuron responds to both the concepts of people(the first two rows) and brackets(the last rows).}}
   \label{fig:supp_mono_poly_semantic_vision}
\end{figure*}

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.9\linewidth]{figs_supp/supp_mono_poly_semantic_text.pdf}
     \caption{Examples for observing the monosemanticity or polysemanticity of text neurons. (a) A monosemantic text neuron, which focuses on ``the words at the beginning of a sentence or paragraph". (b) A polysemantic text neuron, where that the first two examples show that the neuron has high responses mainly on the text tokens ``There are" and ``The table", while in the last example the neuron is activated on the words ``laptop", ``monitor" and ``mouse". }
   % \caption{\tcr{Visualization of monosemanticity and polysemy of text neurons. For text neurons, we only focus on the text input and ignore the activation on the images. (a) The monosemantic neuron mainly activates words at the beginning of a sentence or paragraph. (b)The first two rows show that the neuron has a high response mainly on the text tokens "There are" and "The table", while in the last row the neuron is activated on the words "laptop", "monitor" and "mouse".}}
   
   \label{fig:supp_mono_poly_semantic_text}
\end{figure*}

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{0.5in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.9\linewidth]{figs_supp/supp_mono_poly_semantic_both.pdf}
     \caption{
   Examples for observing the monosemanticity or polysemanticity of multi-modal neurons. (a) A monosemantic multi-modal neuron, which is activated related to the concepts ``airplane" and ``airport" on both visual tokens and text tokens. (b) A polysemantic multi-modal neuron, which responds hihgly to both donuts and carrots.}
   \label{fig:supp_mono_poly_semantic_both}
\end{figure*}


\clearpage
\section{Experiments on another typical VLM of InternVL 2.5}
\label{sec:InternVL}

In the main manuscript, we conducted experiments on top of LLaVA 1.5 (7B) \cite{Liu_2024_CVPR}. In this section, we show the analysis and results on another typical VLM of InternVL 2.5 (8B) \cite{chen2024expanding}.


\subsection{Neuron activation dataset}

Similar to the experiments on LLaVA 1.5, we collected a dataset of neuron activations from the FFN layers on InternVL 2.5. For each neuron, we meticulously select the top $N=50$ samples eliciting the maximum response values from the 23k samples for analysis. Given that InternVL 2.5 8B comprises 32 blocks with 14,336 neurons in each FFN layer of a block, there are a total of 458,752 neurons for analysis.



\subsection{Distributions of different types of neurons}


We estimate the probability of belonging to the four different neuron types by $(p_v, p_t, p_m, p_u)$. 
We show the histograms for the probability of $p_v$, $p_t$, $p_m$, and $p_u$ over all the neurons, respectively in Figure~\ref{fig:supp_internvl25_pdf_pv_pt_pm_pu}~(a)-(d).  

In Figure~\ref{fig:supp_internvl25_pdf_pv_pt_pm_pu}~(a), we can see that there are a small portion of neurons having very high probability of being visual neurons (where $p_v$ approaches 1). There are about 8.6\% neurons having probability higher than 80\% to be visual neurons. From Figure~\ref{fig:supp_internvl25_pdf_pv_pt_pm_pu}~(b)-(d), there are about 5.6\%, 3.5\%, and 12.8\% neurons having a probability higher than 80\% to be text neurons, multi-modal neurons, and unknown neurons, respectively. 

When we consider the identification of neuron type as a classification problem by recognizing the type of high probability as the neuron type, we obtain the class distribution for the four types of neurons (we refer to them as visual-prone neurons, text-prone neurons, multimodal-prone neurons, unknown neurons) from all neurons (458,752) as shown in Figure~\ref{fig:supp_internvl25_dist_4cls_neurons}. In comparison to LLaVA 1.5, the InternVL 2.5 exhibits a notable increment in the quantity of visual-prone neurons and a significant decrease in the proportion of text-prone neurons. This phenomenon may be attributed to the enhanced emphasis placed by InternVL 2.5 on bolstering its visual data processing capabilities \cite{chen2024expanding}, particularly with respect to the handling of multi-image and video inputs. During the full model instruction tuning phase (stage 2), the entire model is optimized. In fact, as shown in Figure~\ref{fig:supp_internvl25_pdf_pv_pt_pm_pu}~(a), besides a small portion of neurons that can be clearly identified of their types, there are abundant neurons that are less confident to be clearly identified as specific neurons.

%When we consider the identification of neuron type as a classification problem (by recognizing the type of high probability as the neuron type), we obtain the class distribution for the four types of neurons from all the neurons (458,752) as shown in Figure~\ref{fig:supp_internvl25_dist_4cls_neurons}. In comparison to LLaVA 1.5, the InternVL 2.5 exhibits a notable increment in the quantity of visual neurons and a significant decrease in the proportion of text neurons. This phenomenon may be attributed to the enhanced emphasis placed by InternVL 2.5 on bolstering its visual data processing capabilities \cite{chen2024expanding}, particularly with respect to the handling of multi-image and video inputs. During the full model instruction tuning phase (stage 2), the entire model is optimized. In fact, as shown in Figure~\ref{fig:supp_internvl25_pdf_pv_pt_pm_pu}~(a), besides a small portion of neurons that can be clearly identified of their types, there are abundant neurons that are less confident to be clearly identified as specific neurons. 
%Thus the number of visual neurons increases.




\subsection{Neuron distributions across layers}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs_supp/supp_internvl25_pdf_pv_pt_pm_pu.pdf} \\
\vspace{-1mm}
\caption{Histogram of InternVL 2.5 for the probability $p_v$, $p_t$, $p_m$, and $p_u$ of belonging to visual neuron, text neuron, multi-modal neuron, and unknown neuron types, respectively.}
\vspace{-2mm}
\label{fig:supp_internvl25_pdf_pv_pt_pm_pu}
\end{figure*}

\begin{figure}[t]
\centering
\includegraphics[width=0.4\linewidth]{figs_supp/supp_internvl25_dist_4cls_neurons.pdf} \\
\vspace{-1mm}
\caption{Distribution of the four types of neurons  (visual-prone neurons, text-prone neurons, multimodal-prone neurons, unknown neurons) of InternVL 2.5. We treat the determination of the neuron type as a classification problem.}
\vspace{-1mm}
\label{fig:supp_internvl25_dist_4cls_neurons}
\end{figure}

In this section, we explore the distribution of four distinct neuron types across the layers of the model. We selected the top 10,000 neurons (selected based on the probability) for each neuron category to conduct the analysis. The distributions of the neurons across the layers are shown in Figure \ref{fig:supp_internvl25_distribution_special_neurons}.

For visual neurons and text neurons, they present high frequency in the early and middle layers while low frequency in the high layers.
In contrast, the multi-modal neurons have higher frequency in the high layers. That may because in the early and middle layers, more visual neurons and text neurons are learned to focus on processing the individual modality. In the high layers, more multi-modal neurons are learned to jointly deal with visual and text information, enabling efficient reasoning of visual and text information. Interestingly, this distribution trend is similar to that of LLaVA 1.5.
%, suggesting the hidden mode of multi-modal information processing in the visual language models.

\begin{figure*}[t]
  \centering
  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{figs_supp/supp_internvl25_distribution_special_neurons.pdf}
   \vspace{-5mm}
      \caption{The distribution of special neurons over different layers of InternVL 2.5. (a)-(d) show four categories of neurons, namely visual, text, multi-modal and unknown neurons. The horizontal axis denotes the model layer, from 0 to 31. The vertical axis denotes the number of neurons in the corresponding layer.} 
\label{fig:supp_internvl25_distribution_special_neurons}
\end{figure*}

\subsection{Explanation quality}

We evaluate the quality of generated explanations for the three types of neurons based on top activated samples, where we randomly selected 800 neurons of high confidence from each type of neurons. As shown in Table~\ref{table: top_vs_random_internvl25}, the generated explanations obtain score of 0.148, 0.194, and 0.185 for visual neurons, text neurons, and multi-modal neurons, respectively. 

In addition, we explore the impact of two different sampling strategies for generating explanations of the neurons: the first method uniformly selects five samples~(top 1, 5, 9, 13, 17) from the top 20 samples to elucidate the function of the neuron, while the second randomly chooses five samples from the whole 23k image-text pairs. We conduct simulation of the activations on the uniformly selecting five samples  (top 3, 7, 11, 15, 19) from the top 20 samples for each neuron.  The more accurate of the generated explanations, the simulated activations will more closely approach the real activations and result in higher scores. The results are shown in Table \ref{table: top_vs_random_internvl25}. We can see that the explanations generated from the top samples are more accurate than those of the random samples, indicating the efficiency of the sampling strategy.

\begin{table}[h]%[htbp]
  \vspace{-2mm}
  \centering
  \caption{Explaination quality and impacts of different sampling strategies on three neuron categories for InternVL 2.5.}
  \resizebox{0.43\linewidth}{!}{
    \begin{tabular}{ccc}
     \hline
    Neuron category & Top sampling & Random sampling \\
    \hline
    Visual neurons & \textbf{0.148} & 0.132 \\
    %\hline
    Text neurons & \textbf{0.194} & 0.104 \\
    %\hline
    Multi-modal neurons & \textbf{0.185} & 0.154 \\
    \hline
    \end{tabular}%
    }
  \label{table: top_vs_random_internvl25}%
\end{table}%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
