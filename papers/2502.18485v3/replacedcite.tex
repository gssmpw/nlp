\section{Related Work}
\label{sec:related_work}

%In this section, we explore the landscape of existing research related to our study, focusing on vision-language models (VLMs), interpretability in artificial intelligence, and the use of large language models (LLMs) for explanation generation. Our work intersects with these areas, contributing to the ongoing dialogue on making complex AI models more transparent and understandable.

\noindent\textbf{Vision-Language Models (VLMs)} VLMs are designed to process and understand both visual and textual data simultaneously.  Recent advancements, such as those presented by LLaVA ____, BLIP-2 ____, QwenVL ____, and PaliGemma ____, demonstrate the rapid evolution and expanding capabilities of these models. A typical work LLaVA aligns the visual tokens (that are encoded by CLIP encoder) with the text space as the input to Large Language Model (LLM) to enable visual understanding capability ____. These systems have been instrumental in pushing the boundaries of what is possible in tasks like image captioning, visual question answering, and multi-modal information retrieval. Our work builds upon these foundations, focusing on the interpretability aspect of VLMs, an area that is crucial for their broader adoption and trustworthiness.
% VLMs have become a cornerstone in the field of AI, enabling a myriad of applications that require the integration of visual and textual information. Recent advancements, such as those presented by LLaVA ____, BLIP-2 ____, QwenVL ____, and Gemma ____, demonstrate the rapid evolution and expanding capabilities of these models. These systems have been instrumental in pushing the boundaries of what is possible in tasks like image captioning, visual question answering, and multi-modal information retrieval. Our work builds upon these foundations, focusing on the interpretability aspect of VLMs, an area that is crucial for their broader adoption and trustworthiness.



\noindent\textbf{Interpretability in AI} The interpretability of AI systems has garnered significant attention, driven by the need for transparency, accountability, and trust in AI applications ____. The black box nature of many sophisticated AI models, including VLMs, poses challenges to understanding their decision-making processes. Bills \etal ____ explore automatic explanation mechanisms for language models but have not yet studied how to explain VLMs. The domain of VLMs presents unique challenges due to the multi-modal nature of the data they process. Stan \etal____ design an interface to visualize how generated outputs are related to the input image through raw attention, relevancy maps, and causal interpretation. Guertler \etal ____ propose a method to generate and evaluate explanations for neurons in vision models. To the best of our knowledge, there is rare work on investigating the functions of neurons in VLMs. We aim to delve into the internals of VLMs to unveiling the mystery of VLM neurons.