
\section{Details of Experiments}\label{sec:experiments_detailed}

\textbf{Setup.}
All experiments were run on single core of an AMD EPYC 7742 CPU, a single experiment with 1000 independent agents and 100000 iterations takes roughly 1 hour.
For all experiments, we use parameters $\tau, \epsilon$ as implied by Corollaries~\ref{corollary:expert},\ref{corollary:bandit}.
Projections to the probability simplex were implemented using the algorithm by \cite{duchi2008efficient}.

\subsection{Problem Generation Details}

We provide further details on how we generate/simulate the SMFG problems.

\textbf{Linear payoffs.}
We generate a payoff map
\begin{align*}
    \vecF_{lin}(\vecmu) := (\matS + \matX) \vecmu + \vecb
\end{align*}
for some $\matS \in \mathbb{S}_{++}^{K\times K}$ and $\matX$ anti-symmetric matrix, which makes $ \vecF_{lin}$ monotone.
We randomly sample $\matS$ from a Wishart distribution (which has support contained in positive definite matrices), generate $\matX$ by computing $\frac{\matU - \matU^\top}{2}$ for a random matrix $\matU$ with entries sampled uniformly from $[0,1]$ and $\vecb$ having entries uniformly sampled from $[0,1]$.

\textbf{Payoffs with KL potential.}
Next, we construct the following payoff operator $\vecF_{KL}$ for some reference distribution $\vecmu_{\text{ref}} \in \Delta_\setA$:
\begin{align*}
    \Phi_{KL}(\vecmu) &:= \operatorname{D}_{KL}(\gamma\vecmu + (1-\gamma)\vecmu_{\text{ref}}||\vecmu_{\text{ref}}) \\
    \vecF_{KL}(\vecmu) &:= \nabla \Phi_{KL} (\vecmu) \\
        \vecF_{KL}(\vecmu, a) &= \gamma \log\left(\frac{\gamma \vecmu(a) + (1-\gamma) \vecmu_{\text{ref}}(a)}{\vecmu_{\text{ref}}(a)}\right) + \gamma
\end{align*}
Note that as $\Phi_{KL}$ is convex, $\vecF_{KL}$ is monotone.
In our experiments, we use $\gamma = 0.1$, and we generate $\vecmu_{\text{ref}}$ by sampling $K$ uniform random variables in $[0,1]$ and normalizing.

\textbf{Beach bar process.}
Following the example given in \cite{perrin2020fictitious}, we use the action set $\setA = \{1, \ldots, K \}$ for potential locations at the beach and assume a bar is located at $x_{bar} := \lfloor\frac{K}{2} \rfloor$.
Taking into the proximity to the bar and the occupancy measure over actions (i.e., the crowdedness of locations at the beach), the payoff map is given by:
\begin{align*}
    \vecF_{bb}(\vecmu, a) = 1-\frac{|a - x_{bar}|}{K} - \alpha \log(1 + \vecmu(a)).
\end{align*}
The above payoff map is monotone.
We use $\alpha = 1$ for our experiments.

\subsection{Learning curves - Full Feedback}

We provide the learning curves under full feedback for various choices of the number of agents $N \in \{ 20, 100, 1000 \}$.
The errors in terms of maximum exploitability and distance to MF-NE are presented in Figure~\ref{figure:expert_exp_curves} and Figure~\ref{figure:expert_l2_curves} respectively.

\begin{figure}[h]
\centering
\begin{tabular}{cc}
  \includegraphics[width=0.35\linewidth]{figures/expert_feedback_exploitability_curve_linear.png} &   \includegraphics[width=0.35\linewidth]{figures/expert_feedback_exploitability_curve_exp.png} \\
(a) & (b) \\
\includegraphics[width=0.35\linewidth]{figures/expert_feedback_exploitability_curve_kl.png} &   \includegraphics[width=0.35\linewidth]{figures/expert_feedback_exploitability_curve_bb.png} \\
(c) & (d)
\end{tabular}
\caption{
The (smoothed) maximum exploitability $\max_{i\in\setN} \phi^i(\{\vecpi^j\}_{j=1}^N)$ among $N$ agents throughout learning with full feedback for three different $N$, on the problems (a) linear payoffs, (b) exponentially decreasing payoffs, (c) payoffs with KL potential and (d) the beach bar payoffs.
}
\label{figure:expert_exp_curves}
\end{figure}

As expected, the games with larger number of players $N$ converge to better approximate NE in the sense that the final maximum exploitability is smaller at convergence.
Furthermore, in most cases the exploitability converges slightly slower with more agents, also supporting the theoretical finding that there is a dependence on $N$.
As before, the exploitability curves have oscillations at later stages of the training, even though they remain upper bounded as foreseen the theoretical results.
This does not contradict our results as long as for larger $N$, the upper bound on the oscillations us smaller.
The confidence intervals plotted in figures support a high-probability upper bound on the maximum exploitability as one would expect.


\begin{figure}[h]
\centering
\begin{tabular}{cc}
  \includegraphics[width=0.35\linewidth]{figures/expert_feedback_l2_curve_linear.png} &   \includegraphics[width=0.35\linewidth]{figures/expert_feedback_l2_curve_exp.png} \\
(a) & (b) \\
\includegraphics[width=0.35\linewidth]{figures/rebuttal_expert_feedback_l2_curve_kl.png} &   \includegraphics[width=0.35\linewidth]{figures/expert_feedback_l2_curve_bb.png} \\
(c) & (d)
\end{tabular}
\caption{
The mean $\ell_2$ distance to MF-NE given by $\frac{1}{N}\sum_{i\in\setN} \| \vecpi^i - \vecpi^*\|_2$ with $N$ agents throughout learning with full feedback for three different $N$, on the problems (a) linear payoffs, (b) exponentially decreasing payoffs, (c) payoffs with KL potential and (d) the beach bar payoffs.
}
\label{figure:expert_l2_curves}
\end{figure}



\subsection{Learning curves - Bandit Feedback}

We provide the learning curves under bandit feedback for various choices of the number of agents $N \in \{ 50, 100, 1000 \}$.
The errors in terms of maximum exploitability and distance to MF-NE are presented in Figure~\ref{figure:bandit_exp_curves} and Figure~\ref{figure:bandit_l2_curves} respectively.

\begin{figure}[h]
\centering
\begin{tabular}{cc}
  \includegraphics[width=0.35\linewidth]{figures/bandit_feedback_exploitability_curve_linear.png} &   \includegraphics[width=0.35\linewidth]{figures/bandit_feedback_exploitability_curve_exp.png} \\
(a) & (b) \\
\includegraphics[width=0.35\linewidth]{figures/bandit_feedback_exploitability_curve_kl.png} &   \includegraphics[width=0.35\linewidth]{figures/bandit_feedback_exploitability_curve_bb.png} \\
(c) & (d)
\end{tabular}
\caption{
The (smoothed) maximum exploitability $\max_{i\in\setN} \phi^i(\{\vecpi^j\}_{j=1}^N)$ among $N$ agents throughout learning with bandit feedback for three different $N$, on the problems (a) linear payoffs, (b) exponentially decreasing payoffs, (c) payoffs with KL potential and (d) the beach bar payoffs.
}
\label{figure:bandit_exp_curves}
\end{figure}

As in the case of full feedback, the curves converge to smaller values as $N$ increases.
Furthermore, one straightforward observation is that the variance at early stages of learning is much higher than in the full feedback case.
This can be due to the added variance of the importance sampling estimator constructed through exploration epochs.
As exploration occurs in shorter duration at early epochs, the variance between agent policies will be high as well, explaining the initial increase in exploitability in certain toy experiments in Figure~\ref{figure:bandit_exp_curves}.

\begin{figure}[h]
\centering
\begin{tabular}{cc}
  \includegraphics[width=0.35\linewidth]{figures/bandit_feedback_l2_curve_linear.png} &   \includegraphics[width=0.35\linewidth]{figures/bandit_feedback_l2_curve_exp.png} \\
(a) & (b) \\
\includegraphics[width=0.35\linewidth]{figures/bandit_feedback_l2_curve_kl.png} &   \includegraphics[width=0.35\linewidth]{figures/bandit_feedback_l2_curve_bb.png} \\
(c) & (d)
\end{tabular}
\caption{
The mean $\ell_2$ distance to MF-NE given by $\frac{1}{N}\sum_{i\in\setN} \| \vecpi^i - \vecpi^*\|_2$ with $N$ agents throughout learning with bandit feedback for three different $N$, on the problems (a) linear payoffs, (b) exponentially decreasing payoffs, (c) payoffs with KL potential and (d) the beach bar payoffs.
}
\label{figure:bandit_l2_curves}
\end{figure}

Furthermore, comparing the observations for bandit feedback (Figure~\ref{figure:bandit_exp_curves}) and full feedback cases (Figure~\ref{figure:expert_exp_curves}), we empirically confirm that learning take more iterations in the bandit case.
This is likely due to the fact that exploration occurs probabilistically, inducing additional variance in the policy updates that increases with $N$ and incorporates an additional logarithmic term in the theoretical bounds.
However, the number of exploration epochs in the bandit case is comparable to the number of time steps in the full feedback case.

Finally, we also emphasize the fact that in earlier stages of training with bandit feedback, the cases with $N=1000$ had much higher exploitability and $\ell_2$ distance to MF-NE at earlier time steps.
This is due to the fact that policy updates occur with larger intervals in between when $N$ is large, as can be seen in Algorithm~\ref{alg:bandit}.
This can be explicitly observed in Figure~\ref{figure:bandit_l2_curves}, as the policies of agents are constant in between policy updates.
However, at later stages as the time-dependent term in the bound on epxloitability in Corollary~\ref{corollary:bandit} disappears, we observe that the experiments for larger $N$ converge to a better policy (i.e., one with lower exploitability) than the cases with smaller $N$ as the theory suggests.

Finally, comparing Figures~\ref{figure:bandit_exp_curves},\ref{figure:bandit_l2_curves}, we see that in certain experiments for some $N$ despite having a lower exploitability we observe a greater $\ell_2$ distance to MF-NE.
This likely due to fact that the non-vanishing bias term in exploitability and $\ell_2$ distance have differing dependence on problem parameters such as $L, \lambda, K$.
Therefore, for instance for the KL potential payoffs, we observe a greater mean $\ell_2$ distance to MF-NE but a smaller exploitability for $N=1000$.




\subsection{Comparison with Multiplicative Weight Updates}\label{appendix:comparison_omd}

The SMFG problem of interactive learning without communications and bandit feedback has not been studied in the literature previously, and there is no alternative algorithm with theoretical guarantees to the best of our knowledge in this setting.
However, in Figure~\ref{figure:fig_mwu} we provide comparison with a heuristic extension of the OMD algorithm proposed by \citet{perolat2022scaling} which was analyzed in the case of an infinite agent game.
In general, independent learning with $N$ agents introduces unique challenges that cause the algorithm to cycle or diverge.

\begin{figure}[h]
\begin{tabular}{cc}
 \includegraphics[width=0.45\linewidth]{figures/linear_comparison_rebuttal_mwu.png} &   \includegraphics[width=0.45\linewidth]{figures/bb_comparison_rebuttal_mwu.png} \\
 \includegraphics[width=0.45\linewidth]{figures/kl_comparison_rebuttal_mwu.png} &   \includegraphics[width=0.45\linewidth]{figures/exp_comparison_rebuttal_mwu.png}
\end{tabular}
\caption{
Comparison of maximum exploitability between our TRPA-Bandit and multiplicative weight updates (equivalently OMD of \citet{perolat2022scaling}) in benchmarks \textsc{linear}, \textsc{bb}, \textsc{kl}, \textsc{exp} (left to right). $N=1000$ agents, averaged over 10 experiments.
}
\label{figure:fig_mwu}
\end{figure}

\subsection{Experiments on Traffic Congestion}\label{appendix:exp:traffic}

\textbf{UTD19 and closed-loop sensors.}
The UTD19 dataset contains measurements by closed-loop sensors which report the fraction of the time a particular section of the route remains occupied (i.e., a car is located in between sensors placed on the sides).
The data consists of measurements every 5 minutes, from various sensors across 41 European cities.
The dataset contains 2 weeks of data collected by sensors placed around Zurich.


\textbf{Payoff models.}
We fit kernelized ridge regression models to model the flow as a function of occupancy using the UTD19 dataset.
We use an RBF kernel and a regularization of $\alpha=1.0$ for all models.
The dependence of traffic speed on route occupancy in two sample paths in the dataset can be seen in Figure~\ref{figure:utd_models_occupancy_speed}-(a, b).
The measurements motivate the monotonicity assumption as both the original data and the fitted regression model, increased occupancy leads to increased travel time, at least, when ignoring interactions.
We compute a proxy for the travel velocity using the flow and occupancy measurements on each route, and a scaling factor $c_{\text{dist}}$ due to varying lengths of each route, leading to the estimated travel time
\begin{align*}
    T_{travel} &= c_{\text{dist}} \frac{\text{flow}}{\text{occupancy}}.
\end{align*}
We use $-T_{travel}$ as the reward for each agent.


\begin{figure}[h]
\centering
\begin{tabular}{ccc} 
\includegraphics[width=0.31\linewidth]{figures/sensor_speed_K324D11.png} & \includegraphics[width=0.31\linewidth]{figures/sensor_speed_K418D12.png} & \includegraphics[width=0.3\linewidth]{figures/server1.png} \\
(a) & (b) & (c)
\end{tabular}
\caption{
(a, b) Data and fitted models of occupancy vs. speed on two different urban roads (sensors K324D11 and K418D12) from the UTD19 dataset.
The red line indicates the fitted model predictions of the kernelized regression model.
(c) Measured ping times of a selected Tor entry node as a function of parallel requests (i.e., number of agents accessing the node). }
\label{figure:utd_models_occupancy_speed}
\end{figure}




\subsection{Experiments on Network Access}\label{appendix:exp:tor}

For the Tor network access experiment, we randomly chose 5 entry guard servers (the complete list available publicly \cite{torentryguards}) in various geographic locations, among the servers that have the longest recorded uptime.
To simulate access to each server, we ping each node 5 consecutive times and average the delays to compute the cost.
As expected, due to varying bandwidths/computational power, each server has different sensitivities to load in terms of delay.
We plot measured ping times vs the number of agents simultaneously accessing a particular node in Figure~\ref{figure:utd_models_occupancy_speed}-(c).
The ping times were measured to be stochastic but clearly sensitive to occupancy, motivating a monotone payoff operator.

We use parameters $\tau = 0.01, \varepsilon = 0.3$ for the experiments in this section.
The arbitrary choice is due to the fact that in the presence of external agents in the game that do not use TRPA-Bandit (in this case, thousands of other users accessing the Tor network), the theoretically optimal parameters implied by Corollary~\ref{corollary:bandit} can not be used.
While more realistic simulations that are also closer to the theory could be run by keeping $N$ larger and simulating a Tor access rather than simple pings, we refrain from this in order to minimize the footprint of our experiments on the Tor network.



