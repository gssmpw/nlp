\section{Formalizing Learning Algorithms}\label{section:alg_formalization}

In this section, we formalize the concept of an independent learning algorithm in the full feedback and bandit feedback setting.
In general, we formalize the notion of an algorithm as a map $A_t^i: \mathcal{H}_{t}^i \rightarrow \Delta_\setA$ that maps the set of past observations of agent $i$ at time $t$ to action selection probabilities.
The definition of the set $\mathcal{H}_{t}^i$ varies between the feedback models.

\begin{definition}[Learning algorithm with full feedback]
\label{definition:alg_expert}
An independent learning algorithm with full information $\mathbf{A} = \{ A_t^i\}_{i,t}$ is a sequence of mappings for each player with
\begin{align*}
    &A_t^i : \Delta_\setA^{t-1} \times \setA^{t-1} \times [0,1]^{(t-1) \times K} \rightarrow \Delta_\setA, \text{ for all $t > 0$}, \\
    &A_0^i \in \Delta_\setA,
\end{align*}
that maps past $t-1$ observations from previous rounds to a mixed strategy on actions $\setA$ at time $t > 0$ for each agent $i$.
\end{definition}

Naturally, we are interested in algorithms that yield a good approximation of the NE in expectation.
More explicitly, we will be interested in designing algorithms that converge to policy profiles with low expected explotability.

\begin{definition}[Rational learning algorithm with full feedback]
Let $\mathbf{A}$ be an algorithm with full feedback as defined in Definition~\ref{definition:alg_expert}.
We call $\mathbf{A}$ $\delta$-rational if it holds that for all $i$, the induced mixed strategies $\vecpi_t^i$ under $\vecpi_0^i = A_0^i, \vecpi_t^i = A_t^i(\vecpi^i_0, \ldots, \vecpi_{t-1}^i, a_0^i, \ldots, a_{t-1}^i, \vecr_{0}^i, \ldots, \vecr_{t-1}^i)$ satisfy
\begin{align*}
    \lim_{t\rightarrow \infty} \Exop[ \setE^i_{\text{exp}}(\{\vecpi_t^j \}_{j=1}^N)] \leq \delta, \text{ for all } i \in \setN.
\end{align*}
\end{definition}

Note that while not specified in the definition above, we will also be interested in the rate of convergence of the exploitability term for a rational algorithm.
Since we are solving the SMFG at the finite-agent regime, we will be interested in $\delta$-rational algorithms that have $\delta \rightarrow 0$ as $N\rightarrow\infty$, that is, the non-vanishing bias should scale inversely with the number of agents.

Finally, we also formalize the concepts of a learning algorithm and $\delta$-rationality in the bandit setting.

\begin{definition}[Algorithm with bandit feedback]
\label{definition:alg_bandit}
An algorithm with bandit feedback $\mathbf{A} = \{ A_t^i\}_{i,t}$ is a sequence of mappings for each player with
\begin{align*}
    &A_t^i : \Delta_\setA^{t-1} \times \setA^{t-1} \times [0,1]^{(t-1) } \rightarrow \Delta_\setA, \text{ for all $t > 0$}, \\
    &A_0^i \in \Delta_\setA,
\end{align*}
that maps past $t-1$ observations from previous rounds at all times $t > 0$ (only including the payoffs of the \emph{played} actions) to a probability distribution on actions $\setA$.
\end{definition}

\begin{definition}[Rational algorithm with bandit feedback]
Let $\mathbf{A}$ be an algorithm with bandit feedback as defined in Definition~\ref{definition:alg_bandit}.
We call $\mathbf{A}$ $\delta$-rational if it holds that for all $i$, the induced (random) mixed strategies $\vecpi_t^i$ under $\vecpi_0^i = A_0^i, \vecpi_t^i = A_t^i(\vecpi^i_0, \ldots, \vecpi_{t-1}^i, a_0^i, \ldots, a_{t-1}^i, r_{0}^i, \ldots, r_{t-1}^i)$ satisfy
\begin{align*}
    \lim_{t\rightarrow \infty} \Exop[ \setE^i_{\text{exp}}(\{\vecpi_t^j \}_{j=1}^N)] \leq \delta, \text{ for all } i \in \setN.
\end{align*}
\end{definition}
