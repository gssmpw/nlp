
\section{A Detailed Comparison to the Setting in \cite{gummadi2013mean}}\label{sec:detailed_comparison}

Since specific keywords seem to correspond to the works on mean-field approximations with bandits, we provide a greater discussion of our setting and the results by \citet{gummadi2013mean}.
In general, our settings and models are very different, hence almost none of the results between our work and Gummadi et al. are transferable to the other.
Our problem formulation, analysis, and results are fundamentally different from their setting due to the following points.

\textbf{Stationary equilibrium vs Nash equilibrium.}
The most critical difference between the two works is the solution concepts.
Our setting is competitive, as a natural extension, the solution concept is that of a Nash equilibrium where each agent has no incentive to change their policy.
On the other hand, the setting of Gummadi et al. need not be competitive or collaborative and this distinction is not significant for their framework, their goal is to characterize convergence of the population to a stationary distribution.
Their main results show that if a particular policy map $\sigma: \mathbb{Z}_{\geq 0}^{2 n} \rightarrow \Delta_\setA$ is prescribed on agents, the population distribution will converge to a steady state.
The equilibrium concept of \cite{gummadi2013mean} is not \emph{Nash}, rather stationarity.

\textbf{Optimality considerations.}
As a consequence of analyzing stationarity, the results by \citep{gummadi2013mean} do not analyze or aim to characterize optimality.
In their analysis, a fixed map $\sigma: \mathbb{Z}_{\geq 0}^{2 n} \rightarrow \Delta_\setA$ is assumed to be the policy/strategy of a continuum of (i.e., infinitely many) agents, which maps observed loss/win counts (from Bernoulli distributed arm rewards) to arm probabilities.
The stationary distribution in general obtained from $\sigma$ in \cite{gummadi2013mean} does not have optimality properties, for instance, a fixed agent will can have arbitrary large exploitability.
The main goal of \cite{gummadi2013mean} is to prove the convergence of the population distribution to a steady state behaviour.

\textbf{Algorithms.}
As a consequence of the previous points, Gummadi et al. abstract away any algorithmic considerations to the fixed map $\sigma$ and the particular algorithms employed by agents do not directly have significance in terms of their theoretical conclusions.
Since we analyze optimality in our setting, we require a specific algorithm to be employed (TRPA and Algorithm~\ref{alg:bandit}).

\textbf{Independent learning.}
In our setting, the notion of learning and independent learning become significant since we are aiming to obtain an approximate NE.
Hence, our theoretical results bound the expected exploitability (Theorems~\ref{theorem:expert_short}, \ref{theorem:bandit_short}) in terms of number of samples.
In the work of \cite{gummadi2013mean}, the main aim is convergence to a steady state rather than learning.

\textbf{Population regeneration.}
Finally, to be able to obtain a contractive mapping yielding a population stationary distribution/steady state, \cite{gummadi2013mean} assume that the population regenerates at a constant rate $\beta$, implying agents are constantly being replaced by oblivious agents that have not observed the game dynamics.
This smooths the dynamics by introducing a forgetting mechanism to game participants.
Our results on the other hand are closer to the traditional bandits/independent learning setting.
For instance, this would correspond to non-vanishing exploitability scaling with $\mathcal{O}(\beta)$ in our system as agents constantly ``forget'' what they learned.

\textbf{Other model differences.}
In our setting, we assume general noisy rewards while in \cite{gummadi2013mean}, the rewards are Bernoulli random variables with success probability dependent on the population.
