\section{Table for Notation}
\begin{longtable}{ p{.23\textwidth}  p{.77\textwidth} } 
\underline{General notation:} & \\
$\Delta_\setX$ & probability simplex on discrete set $\setX$ \\ 
$[M]$ & $:= \{1, \ldots, M\}$, for any $M\in\mathbb{N}_{>0}$ \\
$\Delta_{\setX, N}$ & $:= \{ \vecu = \{u_i\}_i \in\Delta_\setX | N u_i \in \mathbb{N}_{\geq 0} \}$, for discrete set $\setX$, $N\in\mathbb{N}_{>0}$ \\
$\mathbb{M}^{D_1,D_2}$ & $D_1 \times D_2$ matrices \\
$\mathbb{S}_{++}^D$ & positive definite $D \times D$ matrices \\
$\Pi_K$ & projection onto convex, compact set $K\subset \mathbb{R}^H$\\
$\vece_a$ & $\in \mathbb{R}^\setA$, standard unit vector with coordinate $a$ set to 1 \\
 &  \\ 
\underline{For SMFGs:} & \\
$\vecF$ & payoff function, $\vecF: \Delta_\setA \rightarrow [0,1]^K$  \\
$\lambda$ & strong monotonicity modulus of $\vecF$ \\  
$L$ & Lipschitz modulus of $\vecF$ \\
$\tau$ & Tikhonov regularization parameter \\ 
$\vecpi^*$ & unique solution of $\tau$ regularized \eqref{eq:mfg_rvi_statement} \\
$K$ & number of actions \\ 
$N$ & number of players \\  
$\setN$ & $:= \{1, \ldots, N \}$, the set of players \\ 
$\setA$ & the set of actions, $|\setA| = K$. \\ 
$\sigma^2$ & upper bound of the standard deviation of received payoff \\
$\widehat{\vecmu}$ & $:= \frac{1}{N} \sum_{i=1}^N \vece_{a^i}$, when actions $\{a^i\}_{i=1}^N$ clear in context \\
$V^i(\vecpi^1, \ldots, \vecpi^N)$ & expected payoff of player $i$ under strategy profile $(\vecpi^1, \ldots, \vecpi^N)$ \\
$\setE^i_{\text{exp}}(\{\vecpi^j\}_{j=1}^N)$ & $:= \max_{\vecpi'\in \Delta_\setA} V^i(\vecpi', \vecpi^{-i}) - V^i(\vecpi^1, \ldots, \vecpi^N)$, exploitability \\
$\widehat{\vecmu}(\{a^i\}_{i=1}^N)$ & $:= \frac{1}{N} \sum_{i=1}^N \vece_{a^i}$, the action distribution induced by particular $\{a^i\}_{i=1}^N \in \setA^N$ \\
 &  \\ 
\underline{For full feedback:} & \\
$t$ & round of play \\
$a^i_t$ & $\in\setA$ action take by player $i$ at round $t$ \\
$\widehat{\vecmu}_t $ & $:= \frac{1}{N} \sum_{i=1}^N \vece_{a_t^i}$, empirical distribution over actions $\setA$ on round $t$ \\ 
$\vecr^i_t$ & $:= \vecF(\widehat{\vecmu}_t) + \vecn_t^i$, payoff vector observed by player $i$ \\ 
$\eta_t$ & learning rate \\
$ \bar{\vecmu}_t $ & $ := \frac{1}{N} \sum_{i=1}^N \vecpi_t^i,$  mean policy at round $t$\\ 
$e_t^i$ & $:= \|\vecpi^i_t - \bar{\vecmu}_t \|_2^2 $, deviation of policy of player $i$ \\
$u_t^i $ & $:= \Exop\left[\| \vecpi_t^i - \vecpi^* \|_2^2\right]$ \\
 &  \\ 
\underline{For bandit feedback:} & \\
$h$ & epoch of play \\
$t$ & round of play in epoch \\
$a^i_{h,t}$ & $\in\setA$ action take by player $i$ at round $t$ of epoch $h$ \\
$T_h$ & number of rounds in epoch $h$ \\
$\varepsilon$ & exploration probability \\
$\widehat{\vecmu}_{h,t} $ & $:= \frac{1}{N} \sum_{i=1}^N \vece_{a_{h, t}^i}$, empirical distribution over actions $\setA$ on round $t$ of epoch $h$ \\ 
$\vecr^i_{h,t}$ & $:= \vecF(\widehat{\vecmu}_{h,t}) + \vecn_{h,t}^i$, (unobserved) payoff vector by player $i$ \\ 
$r^i_{h,t}$ & $:= \vecr^i_{h,t} (a^i_{h,t})$, payoff observed by player $i$ at round $t$ of epoch $h$ \\
$X_{h,t}^i$ & $\sim \operatorname{Ber}(\varepsilon)$ indicator variable, $1$ if $i$ explores at round $t$ of epoch $h$ \\
$\widehat{\vecr}^i_h $ & $:= K r^i_{h,t}\vece_{a^i_{h,t}}$ if $X_{h,t}^i=1$, importance sampling estimate of player $i$ \\
$\eta_h$ & learning rate \\
$ \bar{\vecmu}_h $ & $ := \frac{1}{N} \sum_{i=1}^N \vecpi_h^i,$  mean policy at epoch $h$\\ 
$e_h^i$ & $:= \|\vecpi^i_h - \bar{\vecmu}_h \|_2^2 $, deviation of policy of player $i$ \\
$u_h^i $ & $:= \Exop\left[\| \vecpi_h^i - \vecpi^* \|_2^2\right]$ \\
\end{longtable}
