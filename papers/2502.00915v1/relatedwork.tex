\section{Related Work and Comparisons}
This work is situated at the intersection of multiple areas of research, and we discuss each.

\textbf{Mean-field games.}
Mean-field games (MFG), originally proposed independently by \citet{lasry2007mean} and \citet{huang2006large}, have been an active research area in multi-agent RL literature.
MFG is a useful theoretical tool for analyzing a specific class of MARL problems consisting of a large number of players with symmetric (but competitive) interests by formulating the infinite-agent limit.
The MFG formalism has been used successfully to model large-scale problems such as
auctions \citep{pinasco2023learning},
electricity markets \citep{gomes2021mean},
epidemics \citep{aurell2022finite, huang2022game}, 
and carbon markets \citep{carmona2022mean}.


In this work, we analyze a learning problem in a static (or stateless) finite-action MFG.
Closer to our setting, discrete-time dynamic MFGs have been analyzed under various assumptions, reward models, and solution concepts, such as Lasry-Lions games \citep{perrin2020fictitious,perolat2022scaling}, stationary MFG \citep{anahtarci2022q,xie2021learning,zaman2023oracle, yardim2023policy}, linear quadratic MFG \citep{guo2022entropy, fu2019actor}, and MFGs on graphs \citep{yang2017learning,gu2021mean}.
However, finite-sample learning guarantees for mean-field games only exist under specific assumptions.
For Lasry-Lions games, convergence guarantees exist with an infinite agent oracle \citep{perrin2020fictitious, zhang2024learning}, while IL guarantees with finite agents in this setting do not exist to the best of our knowledge.
Recently, theoretical learning guarantees for centralized learning on monotone MFGs with graphon structure \citep{zhang2024learning} and potential MFGs \citep{geist2021concave} have been shown.
IL in stationary MFGs has been studied either under a subjective equilibrium solution concept \citep{yongacoglu2022independent} or with strong regularization bias and  poor sample complexity \citep{yardim2023policy}.

\textbf{Variational inequalities (VI).}
VIs and relevant algorithms have been an active area of research in classical and recent optimization literature; see e.g., \citet{nemirovski2004prox, facchinei2003finite, nesterov2007dual,lin2022perseus, kotsalis2022simple}, just to list a few.
VIs have been extensively analyzed in the case of (strongly) monotone operators.
Classical algorithms in monotone VIs include the projected gradient method \citep{facchinei2003finite}, the extragradient method \citep{korpelevich1976extragradient}, and proximal point algorithms \citep{rockafellar1976monotone}.
Several works provide extensions to more general operator classes such as generalized monotone operators \citep{kotsalis2022simple} or pseudo-monotone operators \citep{karamardian1976complementarity}.
A recent survey for methods for solving VIs can be found in \citet{beznosikov2023smooth}.
In this work, we use VIs to formalize the infinite-player limit and use algorithmic tools from VI literature to analyze the more challenging IL setting.
Our algorithms are adaptations of the projected gradient method to the decentralized learning under partial feedback.
However, in the SMFG setting, it is not possible to have oracle access to the VI operator (or even unbiased samples of it), posing an algorithmic and theoretical design challenge.
Strictly speaking, the VI given by \eqref{eq:mfg_vi_statement} exists only as an abstract approximation in our work and not as an oracle model.

\textbf{Decentralized VI algorithms.}
Another setting that is related but orthogonal to our problem is the distributed/decentralized VI problem studied for example by \citet{srivastava2011distributed, mukherjee2020decentralized, kovalev2022optimal}.
In this setting, the VI operator is assumed to decompose into $M$ components $\vecF := \sum_{m=1}^M \vecF_m$ each of which can be evaluated locally by $M$ workers.
Workers are permitted to occasionally transmit information along a communication graph: the communication complexity of algorithms as well as the computational complexity is of interest.
Our setting fundamentally differs from the decentralized VI problem. 
Firstly, the VI operator in our case exists only implicitly as an approximation of the finite-player game dynamics: strictly speaking, the problem becomes a VI only at the infinite-player limit.
Furthermore, the VI operator $\vecF$ in our setting does not admit components that can be independently evaluated by any player due to the game interactions: our stochastic oracle is much more restrictive.
Finally, the independent players cannot communicate at all throughout the repeated plays, introducing the algorithmic challenge of fully decentralized learning.

\textbf{Multi-player MAB (MMAB).}
Our model is related to the MMAB problem.
The standard reward model for MMAB is collisions \citep{anandkumar2011distributed}, where agents receive a reward of 0 if more than one pulls the same arm.
Results have focused on the so-called no-communications setting, proving regret guarantees for the cooperative setting without a coordinator \citep{avner2014concurrent,rosenski2016multi, bubeck2021cooperative}.
Our SMFG model can capture much more general reward models than binary collisions.
Moreover, in our competitive setting, our primary metric is exploitability or proximity to NE rather than regret as typically employed in the collaborative setting of MMAB (similar to the NE for MMAB result established by \citet{lugosi2022multiplayer}).

\textbf{Other work.}
A setting that has a similar set of keywords is mean-field bandits \citep{gummadi2013mean,wang2021mean}, as mentioned in the survey by \citet{lauriere2022learning}.
However, these models do not analyze a \emph{competitive (Nash)} equilibrium, but rather a population steady-state reached by an infinite population under prescribed unknown agent behavior.
In these works, optimality or exploitability is not a concern, hence a direct comparison with our work is not possible (differences detailed in appendix, Section~\ref{sec:detailed_comparison}).
On the other hand, our model is equivalent to decentralized learning on static MFGs \citep{lauriere2022learning} with finitely many agents and partial feedback.
The problem of IL is fundamental in algorithmic game theory and has been separately investigated in zero-sum Markov games \citep{daskalakis2020independent, sayin2021decentralized, ozdaglar2021independent} and potential games \citep{ding2022independent, heliou2017learning,alatur2024independent}.
Another related literature is population games and evolutionary dynamics \citep{sandholm2015population, quijano2017role}, where competitive populations are analyzed with differential equations.
While solution concepts overlap, we are interested in IL with repeated play under partial feedback, not the continuous-time dynamic system.