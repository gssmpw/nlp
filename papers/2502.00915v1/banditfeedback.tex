
\section{Convergence in the Bandit Feedback Case}\label{sec:bandit_feedback_results}

We now move on to the more challenging and realistic bandit feedback case, where agents can only observe the payoffs of the actions they have chosen.
Once again, we analyze the IL setting (or in bandits terminology, the ``no communications'' setting) where agents can not interact or coordinate with each other.
One of the main challenges of bandit feedback with IL in our setting is that it is difficult for each agent to identify itself (i.e., assign itself a unique number between $1,\ldots,N$) so that exploration of action payoffs can be performed in turns.
For instance, in MMAB algorithms, this is typically achieved using variants of the so-called musical chairs algorithm \citep{lugosi2022multiplayer}, which is not available in our formulation.
Instead, we adopt a \emph{probabilistic} exploration scheme where each agent probabilistically decides it is its turn to explore payoffs while the rest of the agents induce the required empirical population distribution on which $\vecF$ should be evaluated.

Our algorithm, which we call TRPA-Bandit, is straightforward and relies on exploration occurring over epochs, where policies are updated once in between epochs using the estimate of action payoffs constructed during the exploration phase.
We use the subscript $h$ to index epochs, which consist of $T_h$ repeated plays indexed by $(h,t)$ for $t=1,\ldots,T_h$.
While we formally presented TRPA-Bandit (Algorithm~\ref{alg:bandit}), the procedure informally is as follows for each agent, fixing an exploration parameter $\varepsilon \in (0,1)$ and an agent $i\in\setN$:
\begin{enumerate}
    \item At each epoch $h$, for $T_h > 0$ time steps, repeat the following:
    \begin{enumerate}
        \item With probability $\varepsilon$, sample uniformly an action $a^i_{h,t}$, observe the payoff $r^i_{h,t}$, and keep the importance sampling estimate $\widehat{\vecr}^i_h \leftarrow K r_{h,t}^i \vece_{a^i_{h,t}}$.
        \item Otherwise (with probability $1-\varepsilon$), sample action according to current policy $\vecpi^i_h$.
    \end{enumerate}
    \item Update the policy using TRPA, $\vecpi^i_{h+1} = \Pi_{\Delta_\setA} ( (1 - \tau \eta_h) \vecpi^i_h + \eta_h \widehat{\vecr}^i_h )$.
    If the agent did not explore this epoch, use $\widehat{\vecr}^i_h = \veczero$.
\end{enumerate}
Intuitively, the probabilistic sampling scheme allows some agents to build a low-variance estimate of $\vecF$, while others simply sample actions with their current policy in order to induce the empirical population distribution at which $\vecF$ should be evaluated.

\begin{algorithm}
    \caption{TRPA-Bandit: IL with bandit feedback algorithm for each agent $i\in\setN$.}\label{alg:bandit}
    \begin{algorithmic}
    \Require Number of actions $K$, regularization $\tau > 0$, exploration probability $\varepsilon > 0$, number of epochs $H$, epoch lengths $\{T_h\}_h$, learning rates $\{\eta_h\}_h$
    \State $\vecpi^i_0 \leftarrow \frac{1}{K} \vecone$
    \For{$h = 0, \ldots, H-1$}
    \State $\widehat{\vecr}^i_h \leftarrow \veczero$ %
    \For{$t = 1, \ldots, T_h$} \Comment{Exploration for $T_h$ rounds before policy update,}
    \State Sample Bernoulli r.v. $X_{h,t}^i \sim \operatorname{Ber}(\varepsilon)$.
    \If{$X_{h,t}^i=1$}
        \State \text{Play action $a^i_{h,t} \sim \operatorname{Unif}(\setA)$ uniformly at random}.
        \Comment{Explore with prob. $\varepsilon$,}
        \State \text{Observe payoff $r^i_{h,t}$}, set  $\widehat{\vecr}^i_h \leftarrow K r^i_{h,t}\vece_{a^i_{h,t}}$.
    \ElsIf{$X_{h,t}^i=0$}
        \State \text{Play action with current policy $a^i_{h,t}\sim \vecpi^i_h$}.
        \Comment{Else, play the current policy.}
    \EndIf
    \EndFor
    \State $\vecpi^i_{h+1} = \Pi_{\Delta_\setA} ( (1 - \tau \eta_h) \vecpi^i_h + \eta_h \widehat{\vecr}^i_h )$
    \Comment{After each epoch, update policy.}
    \EndFor
    \State Return $\vecpi^i_H$
    \end{algorithmic}
    \end{algorithm}



Similar to the full feedback setting, we introduce useful notation used throughout this chapter.
We define the sigma algebra $\mathcal{F}_{h} := \mathcal{F}(\{ \vecpi_{h'}^i \}_{h'=0, \ldots, h}^{i=1, \ldots, N})$.
Adapting the notation from Section~\ref{sec:game_initial_formulation} to the case with multiple epochs, we use
\begin{align*}
\widehat{\vecmu}_{h, t} := \frac{1}{N} \sum_{i=1}^N \vece_{a_{h, t}^i}, \qquad
    \vecr^i_{h, t} := \vecF(\widehat{\vecmu}_{h, t}) + \vecn_{h, t}^i,
\end{align*}
where the updated time indices $h, t$ simply refer to the $t$-th round of play in epoch $h$, and $a_{h, t}^i$ is the action played by player $i$ at epoch $h$, round $t$.
Under the dynamics of Algorithm~\ref{alg:bandit}, we define the following random variables to assist our proofs:
\begin{align*}
    \bar{\vecmu}_h &:= \frac{1}{N} \sum_{i=1}^N \vecpi_h^i, 
    \qquad
    e_t^i := \|\vecpi^i_h - \bar{\vecmu}_h \|_2^2, 
    \qquad
    u_h^i := \Exop\left[\| \vecpi_h^i - \vecpi^* \|_2^2\right],
\end{align*}
which correspond to the mean policy at epoch $h$, the policy deviation of agent $i$ from the mean at epoch $h$ and the $\ell_2$ distance from the MF-NE.
Note that since policies are updated only in between epochs, the above quantities are indexed by epochs $h$ rather than rounds $h, t$.

Our analysis follows the ideas in the case of expert feedback, the main difference being randomization due to the exploration probabilities and the errors being analyzed per epoch rather than per round.
Similar to the full feedback setting, we will proceed in several steps expressed as intermediate lemmas:
(1) we bound the added bias and variance due to the importance sampling strategy,
(2) we obtain a non-linear recursion for the expectation of the terms $e_t^i$ and possible sampling bias, 
(3) we bound the expected differences of each agent's action probabilities $e_t^i$, showing the deviation of the policies of the agents goes to zero in expectation, 
(4) we solve the recursion to obtain the convergence rate.

The next result, Lemma~\ref{lemma:exploration_bias_trpa_bandit}, provides an answer to the first step.
We show that despite the probabilistic exploration step, the estimates $\widehat{\vecr}_h^i$ in Algorithm~\ref{alg:bandit} have low bias and variance.

\begin{lemma}[Exploration bias]\label{lemma:exploration_bias_trpa_bandit}
Under the dynamics of TRPA-Bandit, it holds almost surely for each epoch $h \geq 0$ that
\begin{align*}
    \| \Exop[\widehat{\vecr}_h^i | 
 \mathcal{F}_h ] - \vecF(\varepsilon \frac{1}{K}\vecone + (1-\varepsilon) \bar{\vecmu}_h) \|_2 \leq K^{\sfrac{3}{2}} \sqrt{1 + \sigma^2} \exp\left\{ -\varepsilon T_{h}\right\} + \frac{2L}{N} + \frac{2L}{\sqrt{N}}.
\end{align*}
\end{lemma}

The full proof has been postponed to Appendix~\ref{sec:proof_lemma_bandit_exploration_bias}.
In summary, the proof strategy is to decompose and analyze the bias due to the possibility of no exploration round happening (the term $K^{\sfrac{3}{2}} \sqrt{1 + \sigma^2} \exp\left\{ -\varepsilon T_{h}\right\}$),  the impact of the exploring agent on payoffs (the term $\frac{2L}{N}$), and bias due to the finitely many agents, similar to Theorem~\ref{theorem:mfg_ne} (the term $\frac{2L}{\sqrt{N}}$).
The additional bias due to probabilistic exploration originates from the possibility that no exploration occurs in a given epoch: the probability of this event can be bounded by $\exp\left\{ -\varepsilon T_{h}\right\}$.

Lemma~\ref{lemma:exploration_bias_trpa_bandit} shows that even if the players do not have full feedback, they can obtain low-bias, low-variance estimates of $\vecF(\varepsilon \frac{1}{K}\vecone + (1-\varepsilon) \bar{\vecmu}_h) \approx \vecF(\bar{\vecmu}_h)$ when $\varepsilon$ is small.
It guarantees that even if the agents do not explore each epoch, in expectation the probabilistic exploration scheme yields a low bias if the epoch lengths $T_h$ are logarithmically large: hence, full feedback can be simulated by paying a logarithmic cost.
Therefore, in our epoched exploration scheme, the bias in ``querying'' the payoff operator $\vecF$ due to an exploring population can be controlled by tuning $\varepsilon$ and $T_h$.

We next state the error recursion in Lemma~\ref{lemma:bandit_main_recurrence}, which uses the result of Lemma~\ref{lemma:exploration_bias_trpa_bandit} to construct the main recurrence for the bandit feedback case.

\begin{lemma}[Main recurrence for TRPA-Bandit]\label{lemma:bandit_main_recurrence}
Under TRPA-Bandit dynamics, it holds for any $i \in \{1, \ldots, N \}$ and each epoch $h\geq 0$ that
\begin{align*}
    \Exop\left[\| \vecpi_{h+1}^i - \vecpi^* \|_2^2\right] \leq & 4 \eta_h^2 K^3(1 + \sigma^2) + 8\eta_h^2(L+\tau)^2 + 8 K^{\sfrac{3}{2}}\eta_h \sqrt{1+\sigma^2}  \exp\{-\varepsilon T_h\} \\
        &+128\eta_h\lambda^{-1} L^2 N^{-1} + 16\eta_h\lambda^{-1}L^2\varepsilon^2 + 2\eta_h\lambda^{-1} L^2 \Exop\left[e_h^i\right] \\
        &+\left(1 - 2 \eta_h(\sfrac{\lambda}{2} + \tau)\right) \Exop\left[\| \vecpi_h^i - \vecpi^* \|_2^2\right],
\end{align*}
for strongly monotone $\lambda >0$ payoffs, and
\begin{align*}
    \Exop\left[\| \vecpi_{h+1}^i - \vecpi^* \|_2^2\right] \leq &  4\eta_h^2 K^3(\sigma^2 + 1) + 8 \eta_h^2 (L+\tau)^2 + 8 K^{\sfrac{3}{2}} \eta_h \sqrt{1+\sigma^2} \exp\{-\varepsilon T_h\} \\
    &+64\tau^{-1} \eta_h \delta^{-1}L^2 N^{-1}+8\tau^{-1} \eta_h \delta^{-1}L^2 \varepsilon^{2} + \tau^{-1} \eta_h\delta^{-1}L^2 \Exop\left[e_h^i\right] \\  
        &+ \left(1 - 2 \tau \eta_h (1-\delta)\right) \Exop\left[\| \vecpi_h^i - \vecpi^* \|_2^2\right],
\end{align*}
for monotone payoffs for arbitrary $\delta \in (0,1)$.
\end{lemma}
Once again, the full proof has been postponed to Appendix~\ref{sec:proof_lemma_bandit_recurrence}.
The proof of Lemma~\ref{lemma:bandit_main_recurrence} follows a similar path as in the recurrence in the full feedback case (Lemma~\ref{lemma:full_error_recurrence}), with the exception that $\vecr_{h,t}^i$ has been replaced by the importance sampling estimator $\widehat{\vecr}^i_h$.
In the decomposition due to Inequality~\eqref{ineq:decomp_abc_full_recur}, the analysis of term (a) remains the same, whereas the terms (b), (c) must be further analyzed using Lemma~\ref{lemma:exploration_bias_trpa_bandit} to account for deviations between $\vecr_{h,t}^i$ and $\widehat{\vecr}^i_h$, as well as the $\varepsilon$ fraction of the population now exploring each round.

The recurrence in Lemma~\ref{lemma:bandit_main_recurrence} is similar in form to the full feedback case (Lemma~\ref{lemma:full_error_recurrence}), apart from the term $8 K^{\sfrac{3}{2}}\eta_h \sqrt{1+\sigma^2}  \exp\{-\varepsilon T_h\}$ due to the exploration scheme.
However, keeping the exploration epoch lengths $T_h$ logarithmically large can make this term small.
Furthermore, once again the recursion produces a dependence on expected mean policy deviations, $\Exop[e_h^i]$.
Hence, the expected policy deviation $\Exop[e_h^i]$ at epoch $h$ must be bounded once again at a rate $\mathcal{O}(\sfrac{1}{h})$ in order to obtain a non-vacuous upper bound on exploitability.
As in the full feedback case, we employ regularization to ensure $\Exop[e_h^i]$ is small.
The next lemma presents our upper bound.

\begin{lemma}[Policy deviation under TRPA-Bandit]\label{lemma:bandit_pol_deviation}
Under TRPA-Bandit dynamics, with learning rates $\eta_h:=\frac{\tau^{-1}}{h+2}$, arbitrary exploration rate $\varepsilon > 0$ and epoch lengths $T_h := \lceil \varepsilon^{-1} \log(h+2) \rceil$
it holds for any $i,j \in \{1, \ldots, N \}, i\neq j$ and each epoch $h\geq 0$ that
\begin{align*}
    \Exop[e_h^i] \leq \frac{24\tau^{-2} K^3 (\sigma^2 + 2) + 48\tau^{-2} + 24}{h+1} + \frac{16\tau^{-2} L ^ 2}{N^2}.
\end{align*}
\end{lemma}
The proof of Lemma~\ref{lemma:bandit_pol_deviation} follows similar ideas to Lemma~\ref{lemma:policy_variations_bound_trpa_full}, while accounting for (a) the increased variance due to importance sampling, and (b) potential further deviation between agent policies due to the $\mathcal{O}(\sfrac{1}{N})$ impact of exploration on $\widehat{\vecmu}_{h,t}$.
In particular, an additional source of policy deviation occurs when an agent does not explore in a given epoch, in which case the payoff estimator is uninformative ($\widehat{\vecr}_h^i = \veczero$) causing additional policy deviation.
The full proof has been postponed to Appendix~\ref{sec:proof_lemma_bandit_pol_dev}.

In the case of TRPA-Bandit, due to the additional variance of probabilistic exploration, the policy deviations between agents might be larger: compare the upper bounds of Lemma~\ref{lemma:bandit_pol_deviation} and Lemma~\ref{lemma:policy_variations_bound_trpa_full}.
In particular, the upper bound of Lemma~\ref{lemma:bandit_pol_deviation} contains a non-vanishing term unlike Lemma~\ref{lemma:policy_variations_bound_trpa_full}.
Nevertheless, they can still be controlled of order $\mathcal{O}(\sfrac{1}{(h+1)} + \sfrac{1}{N^2})$, where the additional $\mathcal{O}(\sfrac{1}{N^2})$ term compared to TRPA-Full vanishes very quickly when $N$ is large.

With these intermediate lemmas established, we state and prove the main convergence result for TRPA-Bandit  Theorem~\ref{theorem:bandit_short}, the main result of this work.
We provide asymptotic rates for brevity, although the proof of the theorem provides explicit bounds.

\begin{theorem}[Convergence, bandit feedback]\label{theorem:bandit_short}
Assume $\vecF$ is Lipschitz, monotone.
Assume $N$ agents run TRPA-Bandit (Algorithm~\ref{alg:bandit}) for $T$ time steps with regularization $\tau>0$ and exploration parameter $\varepsilon > 0$, and agents return policies $\{\vecpi^i\}_i$ after executing Algorithm~\ref{alg:bandit}.
Then, for any agent $i \in \setN$ that $\Exop\left[ \setE^i_{\text{exp}}( \{\vecpi^j\}_{j=1}^N ) \right] \leq \widetilde{\mathcal{O}} (\frac{\tau^{-2}\varepsilon^{-\sfrac{1}{2}}}{\sqrt{T}} + \tau^{-1}\varepsilon + \tau + \frac{\tau^{-1}}{\sqrt{N}} + \frac{\tau^{-\sfrac{3}{2}} }{N} )$.
If $\vecF$ is $\lambda$-strongly monotone, then $\Exop\left[ \setE^i_{\text{exp}}( \{\vecpi^j\}_{j=1}^N ) \right] \leq \widetilde{\mathcal{O}} (\frac{\tau^{-\sfrac{3}{2}} \lambda^{-\sfrac{1}{2}} \varepsilon^{-\sfrac{1}{2}}}{\sqrt{T}} + \tau^{-\sfrac{1}{2}} \lambda^{-\sfrac{1}{2}} \varepsilon + \tau + \frac{\tau^{-\sfrac{1}{2}} \lambda^{-\sfrac{1}{2}}}{\sqrt{N}} + \frac{\tau^{-1} \lambda^{-\sfrac{1}{2}} }{N} )$.
\end{theorem}
 The proof follows from a straightforward combination of Lemmas~\ref{lemma:exploration_bias_trpa_bandit}, \ref{lemma:bandit_main_recurrence}, and \ref{lemma:bandit_pol_deviation} as in the full feedback case.
The exact bounds and proof are in the appendix, Section~\ref{sec:bandit_theorem_full}.


Once again, the values of $\tau$ and exploration probability $\varepsilon$ can be chosen to incorporate tolerable exploitability.
In the case where the number of participants $N$ in the game is known, the following corollary indicates the asymptotically optimal choices for the hyperparameters.

\begin{corollary}[Optimal $\varepsilon, \tau$, bandit feedback]\label{corollary:bandit}
Assume the conditions of Theorem~\ref{theorem:bandit_short} for $N$ agents running TRPA-Bandit.
For monotone $\vecF$, choosing $\tau = \sfrac{1}{\sqrt[4]{N}}$ and $\varepsilon = \sfrac{1}{\sqrt{N}}$ yields
$\Exop\left[\setE^i_{\text{exp}}(\{\vecpi^j\}_{j=1}^N) \right] \leq \widetilde{\mathcal{O}}(\frac{N^{\sfrac{3}{4}}}{\sqrt{T}} + \frac{1}{\sqrt[4]{N}})$ for any $i$.
For strongly monotone $\vecF$, choosing $\tau = \sfrac{1}{\sqrt[3]{N}}$ and $\varepsilon = \sfrac{1}{\sqrt{N}}$ yields $\Exop\left[\setE^i_{\text{exp}}(\{\vecpi^j\}_{j=1}^N) \right] \leq \widetilde{\mathcal{O}}(\frac{N^{\sfrac{3}{4}} \lambda^{-\sfrac{1}{2}}}{\sqrt{T}} + \frac{\lambda^{-\sfrac{1}{2}}}{\sqrt[3]{N}})$.
\end{corollary}

The dependence of $N$ of the sample complexity in the bandit case is worse compared to the full feedback setting as expected: intuitively the agents must take turns to estimate the payoffs of each action in bandit feedback.
Furthermore, while our problem framework is different and a direct comparison is difficult in terms of bounds, we point out that classical MMAB results such as \citep{lugosi2022multiplayer} have a linear dependence on $N$, while in our case the dependence on $N$ scales with $N^{\sfrac{3}{4}}$.
We emphasize that the time-dependence is sublinear in terms of $N$, up to the non-vanishing finite population bias.
As in the full feedback case, the non-vanishing finite population bias in the bandit feedback case scales with $\mathcal{O}(\sfrac{1}{\sqrt[4]{N}})$ or $\mathcal{O}(\sfrac{1}{\sqrt[3]{N}})$, rather than $\mathcal{O}(\sfrac{1}{\sqrt{N}})$ which would match Theorem~\ref{theorem:mfg_ne}.
Note that the dependence of the bias on $N$ varies in various mean-field game results \citep{saldi2019approximate}, but asymptotically is known to converge to zero as $N\rightarrow\infty$, as our explicit finite-agent bound also demonstrates.

Finally, we note that as expected the algorithm for bandit feedback has a worse dependency on the number of actions.
This is as expected due to the fact that (i) the importance sampling estimator increases variance on payoff estimators by a factor of $K$, and (2) in other words, a factor of $\mathcal{O}(K)$ is introduced in order to explore all actions.
