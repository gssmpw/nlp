\section{Convergence in the Full Feedback Case}\label{sec:expert_feedback_results}

We first present an IL algorithm for the full feedback setting, as a first step towards analyzing the more interesting bandit feedback setting.
In this setting, while there is no centralized controller, independent noisy reports of all action payoffs are available to each agent after each round.

\textbf{Is it possible to simply solve MF-RVI in our IL setting?}
Before we present our results, we note the following:
Past works in MFG have already proved approximation results of $N$-agent games by MFG albeit in different settings \citep{saldi2019approximate, yardim2024mean}, but these results do not consider when \emph{learning itself} is carried out with $N$ agents. 
If $N$ agents can not communicate, it is theoretically challenging to approximate the MF-RVI and to tackle bandit feedback.
Most importantly, the IL algorithms formalized in Section~\ref{section:alg_formalization} can not query an operator oracle or maintain a common iterate throughout repeated plays.
Therefore, the approximation properties of \eqref{eq:mfg_vi_statement} do not immediately imply the MF-NE can be learned using VI algorithms.
In this section and the next, we prove the more challenging result of convergence with IL, first under full feedback and later under partial (bandit) feedback.

Our analysis builds up on Tikhonov regularized projected ascent (TRPA).
The TRPA operator is defined as
\begin{align}
    \Gamma^{\eta, \tau}(\vecpi) := \Pi_{\Delta_\setA} ( \vecpi + \eta (\vecF - \tau \matI)(\vecpi) ) = \Pi_{\Delta_\setA} ( (1-\eta\tau) \vecpi + \eta \vecF(\vecpi) ), \tag{TRPA}
\end{align}
for a learning rate $\eta > 0$ and regularization $\tau > 0$.
Intuitively, $\Gamma^{\eta, \tau}$ uses $\vecF$ evaluated at $\vecpi$ to modify action probabilities in the direction of the greatest payoff, incorporating an $\ell_2$ regularizer of $\tau$.
Furthermore, the unique MF-NE $\vecpi^*$ of \eqref{eq:mfg_rvi_statement} is also a fixed point of $\Gamma^{\eta, \tau}$.
The analysis of TRPA is standard and known to converge for monotone $\vecF$ \citep{facchinei2003finite, nemirovski2004prox}, when (stochastic) oracle access to $\vecF$ is assumed.
Naturally, the main complication in applying the method above will be the fact that in the IL setting, agents can not evaluate the operator $\vecF$ arbitrarily, but rather can only observe (a noisy) estimate of $\vecF$ as a function of the empirical population distribution and not of their policy $\vecpi$.
In the full feedback setting, we analyze the following dynamics:
\begin{align}
     \vecpi_0^i := \operatorname{Unif} (\setA) = \frac{1}{K}\vecone_K, \hspace{1em} \vecpi^i_{t+1} =\Pi_{\Delta_\setA} ( (1 - \tau \eta_t) \vecpi_t^i + \eta_t \vecr_t^i ), \tag{TRPA-Full}
\end{align}
for a time varying learning rate $\eta_t$, for each agent $i \in \setN$.
The extraneous $\ell_2$-regularization incorporated in each agent running TRPA-Full is critical for the analysis and convergence in IL, as it allows explicit synchronization of policies of agents without communication.
We state the TRPA-Full algorithm in Algorithm~\ref{alg:full} for reference.

We state the following standard result regarding the TRPA operator without proof, as it will be used later.

\begin{lemma}[cf. Theorem 12.1.2 of \cite{facchinei2003finite}]\label{lemma:contraction_pg}
Assume $\vecF$ is $\lambda\geq 0$-monotone and $L$-Lipschitz.
Then $\Gamma^{\eta,\tau}$ is Lipschitz with constant $\sqrt{1 - 2 (\lambda + \tau) \eta + \eta^2 (L+\tau)^2}$ with respect to the $\ell_2$-norm.
\end{lemma}

\begin{algorithm}
    \caption{TRPA-Full: IL with full feedback algorithm for each agent $i \in \setN$.}\label{alg:full}
    \begin{algorithmic}
    \Require Number of actions $K$, regularization $\tau > 0$, learning rate $\{\eta_t\}_{t=0}^T$, rounds $T > 0$.
    \State $\vecpi^i_0 \leftarrow \frac{1}{K} \vecone$
    \For{$t = 0, \ldots, T-1$}
    \State \text{Play action with current policy $a^i_{t}\sim \vecpi^i_t$}.
    \State \text{Observe payoff $\vecr^i_{t}$}
    \State $\vecpi^i_{t+1} = \Pi_{\Delta_\setA} ( (1 - \tau \eta_t) \vecpi^i_t + \eta_t \vecr^i_t )$
    \EndFor
    \State Return $\vecpi^i_T$
    \end{algorithmic}
\end{algorithm}

For abuse of notation, let $\vecpi^* \in \Delta_\setA$ be the unique solution of \eqref{eq:mfg_rvi_statement} for the regularization $\tau > 0$.
Also define the sigma algebra $\mathcal{F}_{t} := \mathcal{F}(\{ \vecpi_{t'}^i \}_{t'=0, \ldots, t}^{i=1, \ldots, N})$.
We maintain the definitions of the core random variables of the SMFG dynamics introduced in Section~\ref{sec:game_initial_formulation},
\begin{align*}
    \widehat{\vecmu}_t := \frac{1}{N} \sum_{i=1}^N \vece_{a_t^i}, \quad \vecr^i_t := \vecF(\widehat{\vecmu}_t) + \vecn_t^i.
\end{align*}
Under TRPA-Full dynamics, we also define the following random variables that assist our analysis.
\begin{align*}
    \bar{\vecmu}_t &:= \frac{1}{N} \sum_{i=1}^N \vecpi_t^i, \quad e_t^i := \|\vecpi^i_t - \bar{\vecmu}_t \|_2^2, \quad
    u_t^i := \Exop\left[\| \vecpi_t^i - \vecpi^* \|_2^2\right].
\end{align*}
We call $\bar{\vecmu}_t$ the mean policy, $e_t^i$ the mean policy deviation, and $u_t^i$ the expected $\ell_2$-deviation from the regularized MF-NE.
Our goal is to bound the sequence or error terms $u_t^i$; however, the process is complicated by the fact that in general the policy deviations of agents $e_t^i$ are nonzero.
Our strategy is as follows: 
(1) derive a  recursion for $u_t^i$ incorporating the terms $e_t^i$, 
(2) bound the terms $e_t^i$, showing the deviation of the policies of the agents goes to zero in expectation, and
(3) solve the recursion to obtain the convergence rate.

The following lemma captures the first step and provides a recurrence for the evolution of $u_t^i$ under TRPA-Full.

\begin{lemma}[Error recurrence under full feedback]\label{lemma:full_error_recurrence}
    Under TRPA-Full with learning rates $\eta_t$, it holds for $L$-Lipschitz and $\lambda$-strongly monotone $\vecF$ that
    \begin{align*}
    \Exop\left[\| \vecpi_{t+1}^i - \vecpi^* \|_2^2\right] \leq &3\eta_t^2 K(1 + \sigma^2) + 2\eta_t^2(L+\tau)^2 + \frac{4\eta_t L^2 \lambda^{-1}}{ N } \\
        & + 2\eta_t L^2 \lambda^{-1} \Exop\left[e_t^i\right] + \left(1 - 2 \eta_t(\sfrac{\lambda}{2} + \tau)\right) \Exop\left[\| \vecpi_t^i - \vecpi^* \|_2^2\right],
\end{align*}
and for $L$-Lipschitz and monotone $\vecF$ that
\begin{align*}
    \Exop\left[\| \vecpi_{t+1}^i - \vecpi^* \|_2^2\right] \leq &3\eta_t^2 K(1 + \sigma^2) + 2\eta_t^2(L+\tau)^2 + \frac{4\tau^{-1} \eta_t L^2 \delta^{-1}}{ N} \\
        & + \tau^{-1}\eta_t L^2\delta^{-1} \Exop\left[e_t^i\right] + \left(1 - 2\tau \eta_t (1-\delta)\right) \Exop\left[\| \vecpi_t^i - \vecpi^* \|_2^2\right],
\end{align*}
for arbitrary $\delta \in (0,1)$.
\end{lemma}
\begin{proof}
We analyze for any $i\in[N]$ the error term $\| \vecpi_t^i - \vecpi^*\|_2^2$.
Denote $\alpha_t := (1 - \tau \eta_t)$.
For the regularized solution $\vecpi^*$, we have the fixed point result
\begin{align*}
    \Pi_{\Delta_{\setA}} ((1 - \tau \eta_t) \vecpi^* + \eta_t \vecF(\vecpi^*)) = \Pi_{\Delta_{\setA}} (\vecpi^* + \eta_t (\vecF - \tau \matI)(\vecpi^*)) = \vecpi^*.
\end{align*}
The proof strategy is to decompose the $\ell_2$ distance of player policies to $\vecpi^*$ into 3 components using this property.
We can bound the quantity $\| \vecpi_{t+1}^i - \vecpi^*\|_2^2$ by using the non-expansiveness of $\Pi_{\Delta_{\setA}}$:
\begin{align}
    \| \vecpi_{t+1}^i - \vecpi^*\|_2^2 = &\| \Pi_{\Delta_{\setA}}(\alpha_t \vecpi^i_t + \eta_t \vecr_t^i) - \Pi_{\Delta_{\setA}} (\alpha_t \vecpi^* + \eta_t \vecF(\vecpi^*)) \|_2^2 \notag \\
        \leq &\| \alpha_t \vecpi_t^i + \eta_t \vecF(\vecpi_t^i) - \alpha_t \vecpi^* - \eta_t \vecF(\vecpi^*) + \eta_t (\vecr_t^i - \vecF(\vecpi_t^i) )\|_2^2 \notag \\
        = & \eta_t^2\| \vecr_t^i - \vecF(\vecpi_t^i) \|_2^2 + 2\eta_t (\alpha_t (\vecpi_t^i - \vecpi^*) + \eta_t (\vecF(\vecpi_t^i) - \vecF(\vecpi^*)) )^\top (\vecr_t^i - \vecF(\vecpi_t^i)) \notag \\
         & + \|\alpha_t (\vecpi_t^i - \vecpi^*) + \eta_t (\vecF(\vecpi_t^i) - \vecF(\vecpi^*))\|_2^2 \notag \\
         \leq & \underbrace{\eta_t^2\| \vecr_t^i - \vecF(\vecpi_t^i) \|_2^2 + 2\eta_t^2 (\vecF(\vecpi_t^i) - \vecF(\vecpi^*))^\top (\vecr_t^i - \vecF(\vecpi_t^i))}_{(a)} \notag \\
            &+ \underbrace{2\eta_t\alpha_t (\vecpi_t^i - \vecpi^*)^\top (\vecr_t^i - \vecF(\vecpi_t^i))}_{(b)} + \underbrace{\|\alpha_t (\vecpi_t^i - \vecpi^*) + \eta_t (\vecF(\vecpi_t^i) - \vecF(\vecpi^*))\|_2^2 }_{(c)}. \label{ineq:decomp_abc_full_recur}
\end{align}

We analyze the three marked terms separately.
For term $(a)$, using the independence assumption of the noise vectors and Young's inequality, in expectation we obtain
\begin{align*}
    \Exop[(a)] \leq &\eta_t^2 \Exop[\| \vecr_t^i - \vecF(\vecpi_t^i) \|_2^2] + \eta_t^2 \Exop[\|\vecF(\vecpi_t^i) - \vecF(\vecpi^*)\|_2^2] + \eta_t^2 \Exop[\| \vecr_t^i - \vecF(\vecpi_t^i) \|_2^2] \\
    \leq &2\eta_t^2 \Exop[\| \vecr_t^i - \vecF(\vecpi_t^i) \|_2^2] + \eta_t^2 \Exop[\|\vecF(\vecpi_t^i) - \vecF(\vecpi^*)\|_2^2] \\
    \leq & 2\eta_t^2 \Exop[\| \vecr_t^i - \vecF(\widehat{\vecmu}_t)\|_2^2 + \| \vecF(\widehat{\vecmu}_t) - \vecF(\vecpi^i_t)\|_2^2 ] + \eta_t^2 K \\
    \leq & 2\eta_t^2 \sigma^2 K + 3\eta_t^2 K \leq 3\eta_t^2 K(\sigma^2 + 1)
\end{align*}
For the term $(c)$, we obtain
\begin{align*}
    (c) = &\|\alpha_t (\vecpi_t^i - \vecpi^*) + \eta_t (\vecF(\vecpi_t^i) - \vecF(\vecpi^*))\|_2^2 \\
        = & \|(\vecpi_t^i - \vecpi^*) + \eta_t (\vecF(\vecpi_t^i) - \tau \vecpi_t^i - \vecF(\vecpi^*) + \tau \vecpi^* )\|_2^2 \\
        \leq & \left(1 - 2 (\lambda + \tau) \eta_t + (L + \tau)^2 \eta_t^2\right) \| \vecpi_t^i - \vecpi^* \|_2^2 \\
        \leq & \left(1 - 2 (\lambda + \tau) \eta_t \right) \| \vecpi_t^i - \vecpi^* \|_2^2 + 2(L + \tau)^2 \eta_t^2
\end{align*}
where the last inequality holds from the Lipschitz continuity result of Lemma~\ref{lemma:contraction_pg}.

For the term $(b)$, first taking the strongly monotone problem $\lambda > 0$ , we have that
\begin{align*}
(b) = & 2\eta_t\alpha_t (\vecpi_t^i - \vecpi^*)^\top (\vecr_t^i - \vecF(\vecpi_t^i)) \\
 = & 2\eta_t\alpha_t (\vecpi_t^i - \vecpi^*)^\top (\vecr_t^i - \vecF(\widehat{\vecmu}_t)) + 2\eta_t\alpha_t (\vecpi_t^i - \vecpi^*)^\top (\vecF(\widehat{\vecmu}_t) - \vecF(\bar{\vecmu}_t)) \\
    & + 2\eta_t\alpha_t (\vecpi_t^i - \vecpi^*)^\top (\vecF(\bar{\vecmu}_t) - \vecF(\vecpi_t^i)) \\
\leq &2\eta_t\alpha_t \left( \frac{\lambda}{4} \|\vecpi_t^i - \vecpi^* \|_2^2 + \frac{1}{\lambda} \|\vecF(\widehat{\vecmu}_t) - \vecF(\bar{\vecmu}_t)\|_2^2\right) + 2\eta_t\alpha_t \left(\frac{\lambda}{4} \|\vecpi_t^i - \vecpi^* \|_2^2 + \frac{1}{\lambda} \|\vecF(\bar{\vecmu}_t) - \vecF(\vecpi_t^i)\|_2^2 \right) \\
    &+2\eta_t\alpha_t (\vecpi_t^i - \vecpi^*)^\top (\vecr_t^i - \vecF(\widehat{\vecmu}_t)) \\
\leq & \eta_t \lambda \|\vecpi_t^i - \vecpi^* \|_2^2 + 2\eta_t\lambda^{-1}\|\vecF(\widehat{\vecmu}_t) - \vecF(\bar{\vecmu}_t)\|_2^2 + 2\eta_t\lambda^{-1} \|\vecF(\bar{\vecmu}_t) - \vecF(\vecpi_t^i)\|_2^2 \\
    &+2\eta_t\alpha_t (\vecpi_t^i - \vecpi^*)^\top (\vecr_t^i - \vecF(\widehat{\vecmu}_t)),
\end{align*}
which follows from applications of Young's inequality.
For the last three terms we observe:
\begin{align*}
    \Exop\left[2\eta_t\alpha_t (\vecpi_t^i - \vecpi^*)^\top (\vecr_t^i - \vecF(\widehat{\vecmu}_t)) | \mathcal{F}_t\right] = &0, \\
    \Exop[\|\vecF(\widehat{\vecmu}_t) - \vecF(\bar{\vecmu}_t)\|_2^2 | \mathcal{F}_{t}] \leq & L^2 \Exop\left[ \|\widehat{\vecmu}_t - \bar{\vecmu}_t\|_2^2 | \mathcal{F}_{t}\right] \\
    \leq & L^2\Exop\left[\frac{1}{N^2}\|\sum_{i}\vecpi_t^i - \sum_{i} \vece_{a_t^i}\|^2_2 | \mathcal{F}_{t}\right] \\
    = & \frac{L^2}{N^2}\sum_{i}\Exop[\|\vecpi_t^i - \vece_{a_t^i}\|^2_2 | \mathcal{F}_{t}] \leq \frac{2L^2}{N}, \\
    \|\vecF(\bar{\vecmu}_t) - \vecF(\vecpi_t^i)\|_2^2 \leq & L^2 \|\bar{\vecmu}_t - \vecpi_t^i\|_2^2 = L^2 e_t^i.
\end{align*}
The second inequality above follows from the fact that $\widehat{\vecmu}_t$ is the sum of $N$ independent random variables and has expectation $\bar{\vecmu}_t$.
Hence, putting in the bounds for $(a), (b), (c)$ and taking expectations, we obtain the inequality
\begin{align*}
    \Exop\left[\| \vecpi_{t+1}^i - \vecpi^*\|_2^2 \right] \leq & 3 \eta_t^2 K(1 + \sigma^2) + \frac{4\eta_t L^2}{\lambda N} + \frac{2\eta_t L^2}{\lambda} \Exop\left[e_t^i\right] \\
        &+\left(1 - 2 (\sfrac{\lambda}{2} + \tau) \eta_t \right) \Exop\left[\| \vecpi_t^i - \vecpi^* \|_2^2\right] + 2(L + \tau)^2 \eta_t^2.
\end{align*}

Turning back to the monotone case, if $\lambda=0$, vary the upper bound on $(b)$ as follows.
Take any arbitrary $\delta \in (0,1)$.
Then, once again applying Young's inequality, we obtain
\begin{align*}
(b) = & 2\eta_t\alpha_t (\vecpi_t^i - \vecpi^*)^\top (\vecr_t^i - \vecF(\vecpi_t^i)) \\
 = & 2\eta_t\alpha_t (\vecpi_t^i - \vecpi^*)^\top (\vecr_t^i - \vecF(\widehat{\vecmu}_t)) + 2\eta_t\alpha_t (\vecpi_t^i - \vecpi^*)^\top (\vecF(\widehat{\vecmu}_t) - \vecF(\bar{\vecmu}_t)) \\
    & + 2\eta_t\alpha_t (\vecpi_t^i - \vecpi^*)^\top (\vecF(\bar{\vecmu}_t) - \vecF(\vecpi_t^i)) \\
\leq &2\eta_t\alpha_t \left( \frac{\tau\delta}{2} \|\vecpi_t^i - \vecpi^* \|_2^2 + \frac{1}{2\tau\delta} \|\vecF(\widehat{\vecmu}_t) - \vecF(\bar{\vecmu}_t)\|_2^2\right) + 2\eta_t\alpha_t \left(\frac{\tau\delta}{2} \|\vecpi_t^i - \vecpi^* \|_2^2 + \frac{1}{2\tau\delta} \|\vecF(\bar{\vecmu}_t) - \vecF(\vecpi_t^i)\|_2^2 \right)  \\
    &+2\eta_t\alpha_t (\vecpi_t^i - \vecpi^*)^\top (\vecr_t^i - \vecF(\widehat{\vecmu}_t)) \\
\leq & 2 \eta_t \tau\delta \|\vecpi_t^i - \vecpi^* \|_2^2 + \frac{\eta_t}{\tau\delta}\|\vecF(\widehat{\vecmu}_t) - \vecF(\bar{\vecmu}_t)\|_2^2 + \frac{\eta_t}{\tau\delta} \|\vecF(\bar{\vecmu}_t) - \vecF(\vecpi_t^i)\|_2^2 \\
    &+2\eta_t\alpha_t (\vecpi_t^i - \vecpi^*)^\top (\vecr_t^i - \vecF(\widehat{\vecmu}_t)).
\end{align*}
Applying the same bounds for the terms $(a), (c)$ as before yields  the lemma.
\end{proof}

This above lemma has two key features: a dependence on expected mean policy deviation $\Exop\left[e_t^i\right]$, and a term that scales as $\mathcal{O}(\sfrac{1}{N})$.
While the $\mathcal{O}(\sfrac{1}{N})$ term can be anticipated (and asymptotically ignored when $N$ is large) due to the finite-agent mean-field bias (as shown previously in Section~\ref{sec:theoretical_tool}), the term $\Exop\left[e_t^i\right]$ must be controlled separately in the independent learning setting, where policies cannot be synchronized through explicit communication between agents.
The term $\Exop\left[e_t^i\right]$ reflects the core difference of the SMFG model from typical VI stochastic oracles.
Unlike typical VI oracle models, in SMFG the operator $\vecF$ cannot be evaluated at the current iterate $\vecpi^i_t$ of a player $i$  but only approximately at the mean $\bar{\vecmu}_t$.
This is due to decentralized learning: players can only evaluate the current payoffs at the ``mean-iterate'' given by $\vecF(\widehat{\vecmu}_t) \approx \vecF(\bar{\vecmu}_t)$ (up to some stochastic noise) that is almost surely different than their iterates $\{\vecpi^i_t\}_i$ apart from the case with degenerate/zero noise.
Furthermore, Lemma~\ref{lemma:full_error_recurrence} suggests that the algorithmic scheme must guarantee that $\Exop\left[e_t^i\right]$ decays with the rate at least $\mathcal{O}(\sfrac{1}{t})$ to obtain a non-vacuous bound on exploitability.
Taking inspiration from algorithmic stability literature \citep{ahn2022reproducibility, zhang2024optimal}, we utilize a regularization scheme to ensure the iterates of players do not diverge.
The following lemma shows that by introducing explicit regularization $\tau>0$, the expected mean policy deviation can be controlled throughout training.



\begin{lemma}[Policy variations bound]\label{lemma:policy_variations_bound_trpa_full}
    Under TRPA-Full with learning rates $\eta_t :=\frac{\tau^{-1}}{t+2}$, we have $\Exop\left[e_t^i\right] \leq \frac{14 \tau^{-2} K\sigma^2 + 14}{t+2}$.
\end{lemma}
\begin{proof}
Note that for any $i,j\in\setN$ such that $i\neq j$, using the non-expansiveness of the projection operator it holds that
\begin{align*}
    \| \vecpi^i_{t+1} - \vecpi^j_{t+1} \|_2^2 = &  \| \Pi_{\Delta_\setA}((1 - \tau \eta_t) \vecpi^i_t + \eta_t \vecr_t^i) - \Pi_{\Delta_\setA}((1 - \tau \eta_t) \vecpi^j_t + \eta_t \vecr_t^j) \|_2^2 \\
    \leq & \| (1 - \tau \eta_t) \vecpi^i_t + \eta_t \vecr_t^i - (1 - \tau \eta_t) \vecpi^j_t - \eta_t \vecr_t^j \|_2^2 \\
    \leq & \| (1 - \tau \eta_t) (\vecpi^i_t - \vecpi^j_t) + \eta_t (\vecr_t^i - \vecr_t^j) \|_2^2 \\
    = & (1 - \tau \eta_t)^2 \| \vecpi^i_t - \vecpi^j_t \|_2^2 + \eta_t^2 \|  \vecr_t^i - \vecr_t^j \|_2^2 + 2 (1 - \tau \eta_t) \eta_t (\vecpi^i_t - \vecpi^j_t) ^ \top ( \vecr_t^i - \vecr_t^j )
\end{align*}
Taking the conditional expectation on both sides, we obtain
\begin{align*}
    \Exop \left[ \| \vecpi^i_{t+1} - \vecpi^j_{t+1} \|_2^2 | \mathcal{F}_t \right] \leq & (1 - \tau \eta_t)^2 \| \vecpi^i_t - \vecpi^j_t \|_2^2 + \Exop \left[ \eta_t^2 \|  \vecr_t^i - \vecr_t^j \|_2^2 | \mathcal{F}_t \right] \\
        &+ 2 (1 - \tau \eta_t) \eta_t (\vecpi^i_t - \vecpi^j_t) ^ \top \Exop\left[\vecr_t^i - \vecr_t^j | \mathcal{F}_t \right] \\
    = & (1 - \tau \eta_t)^2 \|  \vecpi^i_t - \vecpi^j_t \|_2^2 + \eta_t^2 \Exop \left[\|\vecn^i_t - \vecn^j_t\|_2^2 | \mathcal{F}_t \right] \\
    = & (1 - \tau \eta_t)^2 \|  \vecpi^i_t - \vecpi^j_t \|_2^2 + 2\eta_t^2 K \sigma^2
\end{align*}
almost surely, since we have $\vecr_t^i := \vecF(\widehat{\vecmu}_t) + \vecn^i_t$.
Then, taking the expectation on both sides, 
\begin{align*}
    \Exop \left[ \| \vecpi^i_{t+1} - \vecpi^j_{t+1} \|_2^2 \right] \leq &(1 - \tau \eta_t)^2 \Exop\left[\|  \vecpi^i_t - \vecpi^j_t \|_2^2\right] + 2\eta_t^2 K\sigma^2 \\
    \leq & \left(1 - \frac{1}{t+2}\right)^2 \Exop\left[\|  \vecpi^i_t - \vecpi^j_t \|_2^2\right] + \left(\frac{\tau^{-1}}{t+2}\right)^2 2K\sigma^2 \\
    \leq & \left(1 - \frac{2}{t+2}\right) \Exop\left[\|  \vecpi^i_t - \vecpi^j_t \|_2^2\right] + \frac{1}{(t+2)^2} \Exop\left[\|  \vecpi^i_t - \vecpi^j_t \|_2^2\right] + \frac{2\tau^{-2}K\sigma^2}{(t+2)^2} \\
    \leq & \left(1 - \frac{2}{t+2}\right) \Exop\left[\|  \vecpi^i_t - \vecpi^j_t \|_2^2\right] + \frac{2\tau^{-2}K\sigma^2 + 2}{(t+2)^2}
\end{align*}
To bound the recurrence, we can use the recurrence lemma (Lemma~\ref{lemma:general_recurrence}, noting $\gamma=2, a = 2, u_0 = 0, c_0 = 0, c_1 = 2\tau^{-2}K\sigma^2 + 2$ in its statement):
\begin{align*}
    \Exop \left[ \| \vecpi^i_{t+1} - \vecpi^j_{t+1} \|_2^2 \right] \leq & 5\frac{2\tau^{-2}K\sigma^2 + 2}{(t+2)^2} + 3\frac{2\tau^{-2}K\sigma^2 + 2}{t+2} + \frac{2\tau^{-2}K\sigma^2 + 2}{(t+2)^2} \leq \frac{14 \tau^{-2} K\sigma^2 + 14}{t+2}.
\end{align*}
Then, the expected values of $e_t^i$ can be bounded using:
\begin{align*}
     e_t^i = &\|\vecpi^i_t - \bar{\vecmu}_t \|_2^2 
     =  \left\|\vecpi^i_t - \frac{1}{N} \sum_{j=1}^N \vecpi^j_t  \right\|_2^2 
     \leq \frac{1}{N} \sum_{j=1}^N \| \vecpi^i_t - \vecpi^j_t \|_2^2
\end{align*}
by an application of Jensen's inequality.
Then we have $\Exop\left[e_t^i\right] \leq \frac{14 \tau^{-2} K\sigma^2 + 14}{t+2}$.
\end{proof}

With an explicit bound in expectation on the mean policy deviation $e_t^i$, we can now proceed to the main recurrence for the expected error terms $u_t^i$ in order to prove our main convergence result.
We state our main convergence result for TRPA-Full dynamics in Theorem~\ref{theorem:expert_short} by solving these two recurrences for the monotone and strongly monotone cases.

\begin{theorem}[Convergence, full feedback]\label{theorem:expert_short}
Assume $\vecF$ is Lipschitz, monotone.
Assume $N$ agents run the TRPA-Full update rule for $T$ time steps with learning rates $\eta_t := \frac{\tau^{-1}}{t+2}$ and arbitrary regularization $\tau>0$.
Then it holds for any $i\in[N]$ that $\Exop\left[ \setE^i_{\text{exp}}( \{\vecpi^j_{T}\}_{j=1}^N ) \right] \leq \mathcal{O} (\frac{\tau^{-2}}{\sqrt{T}}+ \frac{\tau^{-1}}{\sqrt{N}} + \tau)$.
Furthermore, if $\vecF$ is $\lambda$-strongly monotone, then $\Exop\left[ \setE^i_{\text{exp}}( \{\vecpi^j_{T}\}_{j=1}^N ) \right] \leq \mathcal{O} (\frac{\tau^{-\sfrac{3}{2}} \lambda^{-\sfrac{1}{2}}}{\sqrt{T}} + \frac{\tau^{-\sfrac{1}{2}} \lambda^{-\sfrac{1}{2}}}{\sqrt{N}} + \tau)$.
\end{theorem}
\begin{proof}
Note that the exploitability in the main statement of the theorem can be related to $u_t^i$ as follows using Lemma~\ref{lemma:phi_lipschitz}:
\begin{align*}
    \Exop[\setE^i_{\text{exp}}(\{\vecpi^j_{t}\}_{j=1}^N)] \leq &\setE^i_{\text{exp}}(\{\vecpi^*\}_{j=1}^N) + \sqrt{K} \Exop[\| \vecpi_t^i - \vecpi^* \|_2] + \frac{4L\sqrt{2K}}{N} \sum_{j\neq i} \Exop[\| \vecpi_t^j - \vecpi^* \|_2] \\
    \leq & \setE^i_{\text{exp}}(\{\vecpi^*\}_{j=1}^N) + \sqrt{K} \sqrt{u_t^i} +  \frac{4L\sqrt{2K}}{N} \sum_{j\neq i} \sqrt{u_t^j} \\
    \leq & \setE^i_{\text{exp}}(\{\vecpi^*\}_{j=1}^N) + \frac{\max\{ \sqrt{K}, 4L\sqrt{2K} \}}{N} \sum_{j} \sqrt{u_t^j}
\end{align*}
Hence the bounds on $u_t^j$ will yield the result of the theorem by linearity of expectation, along with an invocation of Theorem~\ref{theorem:mfgrvi_and_explotability}.

Finally, we solve the recurrences for $\lambda = 0$ and $\lambda > 0$ using Lemma~\ref{lemma:full_error_recurrence}.
For the case $\lambda > 0$, if $\eta_t=\frac{\tau^{-1}}{t+2}$, Lemma~\ref{lemma:full_error_recurrence} provides the bound 
\begin{align*}
    u_{t+1}^i \leq &\frac{ 3\tau^{-2} K(1 + \sigma^2) + 2\tau^{-2}(L+\tau)^2}{(t+2)^2} + \frac{4\tau^{-1} L^2 \lambda^{-1}}{ N (t+2)} + \frac{2\tau^{-1} L^2 \lambda^{-1}}{t+2} \Exop\left[e_t^i\right] \\
        &+ \left(1 - \frac{2 \tau^{-1}(\sfrac{\lambda}{2} + \tau)}{t+2}\right) u_{t}^i. 
\end{align*}
By placing $\Exop\left[ e^i_t\right] \leq \frac{14 \tau^{-2} K\sigma^2 + 14}{t+2}$ due to Lemma~\ref{lemma:policy_variations_bound_trpa_full}, we obtain
\begin{align*}
    u_{t+1}^i \leq &\frac{ 3\tau^{-2}K(1 + \sigma^2) + 2\tau^{-2}(L+\tau)^2 +  28\tau^{-3} K \sigma^2 \lambda^{-1} L^2 + 28 \tau^{-1} L^2 \lambda^{-1}}{(t+2)^2} \\
        &+ \frac{4\tau^{-1} L^2 \lambda^{-1}}{ N (t+2)} + \left(1 - \frac{2}{t+2}\right) u_{t}^i.
\end{align*}
Invoking a generic recurrence lemma (Lemma~\ref{lemma:general_recurrence} in Appendix~\ref{app:basic_inequalities}) leads to the main statement of the theorem.

For the monotone case $\lambda = 0$, we have the recursion:
\begin{align*}
    u_{t+1}^i \leq &\frac{ 3\tau^{-2}K(1 + \sigma^2) + 2\tau^{-2}(L+\tau)^2}{(t+2)^2} + \frac{4\tau^{-2} L^2 \delta^{-1}}{ N (t+2)} + \frac{ \tau^{-2} L^2\delta^{-1}}{ t+2 } \Exop\left[e_t^i\right] \\
        & + \left(1 - \frac{2 (1-\delta)}{t+2}\right) u_{t}^i.
\end{align*}
and once again placing the upper bound on expected policy deviation due to Lemma~\ref{lemma:policy_variations_bound_trpa_full},
\begin{align*}
    u_{t+1}^i \leq &\frac{ 3\tau^{-2}K(1 + \sigma^2) + 2\tau^{-2}(L+\tau)^2 + 28 K \tau^{-4} L^2\delta^{-1}\sigma^2 +28 \tau^{-2} L^2\delta^{-1}}{(t+2)^2} \\
        &+ \frac{2\tau^{-2} L^2 \delta^{-1}}{ N (t+2)} + \left(1 - \frac{2 (1-\delta)}{t+2}\right) u_{t}^i.
\end{align*}
Another invocation of Lemma~\ref{lemma:general_recurrence} concludes the proof, choosing $\delta=\sfrac{1}{4}$.
\end{proof}

This convergence result is stated in terms of exploitability of the unregularized game, leading to an additional $\mathcal{O}(\tau)$ term.
However, in many cases, the Nash equilibrium of the regularized game itself is of interest, in which case the upper bounds should read
$\mathcal{O} (\frac{\tau^{-2}}{\sqrt{T}}+ \frac{\tau^{-1}}{\sqrt{N}})$
and
$\mathcal{O} (\frac{\tau^{-\sfrac{3}{2}} \lambda^{-\sfrac{1}{2}}}{\sqrt{T}} + \frac{\tau^{-\sfrac{1}{2}} \lambda^{-\sfrac{1}{2}}}{\sqrt{N}})$
for the monotone and strongly monotone cases respectively.

In the choice of learning rate $\eta_t$ above, no intrinsic problem parameter is assumed to be known.
Furthermore, due to (1) the regularization $\tau$ and (2) a finite population, a non-vanishing exploitability of $\mathcal{O}(\tau + \sfrac{\tau^{-1}}{\sqrt{N}})$ will be induced in terms of the NE in the monotone case.
While Theorem~\ref{theorem:mfgrvi_and_explotability} readily suggested a bias of order $\mathcal{O}(\sfrac{1}{\sqrt{N}})$ is fundamental, when learning is conducted with finitely many agents Theorem~\ref{theorem:expert_short} shows this is amplified to $\mathcal{O}(\sfrac{\tau^{-1}}{\sqrt{N}})$.
Since for finite population SMFG, there will always be a non-vanishing exploitability in terms of NE due to the mean-field approximation, in practice $\tau$ could be chosen to incorporate an acceptable bias level.
Alternatively, if the exact value of the number of players $N$ is known by each agent, one could choose $\tau$ optimally, to obtain the following corollary.

\begin{corollary}[Optimal $\tau$, full feedback]\label{corollary:expert}
Assume the conditions of Theorem~\ref{theorem:expert_short}.
For monotone $\vecF$, choosing regularization parameter $\tau = \sfrac{1}{\sqrt[4]{N}}$ yields
$\Exop\left[\setE^i_{\text{exp}}(\{\vecpi^j_T\}_{j=1}^N) \right] \leq \mathcal{O}(\frac{\sqrt{N}}{\sqrt{T}} + \frac{1}{\sqrt[4]{N}})$ for any $i$.
For $\lambda$-strongly monotone $\vecF$, choosing $\tau = \sfrac{1}{\sqrt[3]{N}}$ yields $\Exop\left[\setE^i_{\text{exp}}(\{\vecpi^j_T\}_{j=1}^N) \right] \leq \mathcal{O}(\frac{ \lambda^{-\sfrac{1}{2}} \sqrt{N}}{\sqrt{T}} + \frac{\lambda^{-\sfrac{1}{2}}}{\sqrt[3]{N}})$.
\end{corollary}

Even though TRPA-Full solves the regularized (hence strongly monotone) problem, compared to the $\mathcal{O}(\sfrac{1}{T})$ rate in classical strongly monotone VI \citep{kotsalis2022simple} or strongly convex optimization \citep{rakhlin2011making},
our worse $\mathcal{O}(\sfrac{1}{\sqrt{T}})$ time dependence is due to independent learning.
Intuitively, additional time is required to ensure the policies of independent learners are sufficiently close when ``collectively'' evaluating $\vecF$.
The additional dependence of the time-vanishing term on $\sqrt{N}$ is also a result of this fact.
Furthermore, when learning itself is performed by $N$ agents, we note that the bias as a function of $N$ decreases with $\mathcal{O}(\sfrac{1}{\sqrt[4]{N}})$ (or $\mathcal{O}(\sfrac{1}{\sqrt[3]{N}})$ for strongly monotone problems), and not with $\mathcal{O}(\sfrac{1}{\sqrt{N}})$ as Theorem~\ref{theorem:mfg_ne} might suggest.
We leave the question of whether this gap can be improved and whether knowledge of $N$ is required in Corollary~\ref{corollary:expert}, as future work.

