
\section{Introduction}

Multi-agent reinforcement learning (MARL) has been an active area of research, with a vast range of successful applications in games such as Chess, Shogi \citep{silver2017mastering}, Go \citep{silver2018general}, Stratego \citep{perolat2022mastering}, as well as real-world applications for instance in robotics \citep{matignon2007hysteretic}.
However, the applications of MARL to games of a much larger scale involving thousands or millions of agents remain a theoretical and practical challenge \citep{wang2020breaking}.

Despite this limitation, competitive games with many players are ubiquitous and typically high-stakes.
In many real-world games, extremely large-scale competitive multi-agency is the rule rather than the exception: for instance, when commuting every morning between cities using infrastructure shared with millions of other commuters,
when periodically accessing an internet resource by querying servers used simultaneously by many users,
or when sending information over common communication channels.
Crucially, in such settings, learning can only feasibly occur in a \emph{decentralized manner} due to the massive scale of the problem. 
Each agent can only utilize their local observations (often in the form of partial/bandit feedback) to maximize their selfish utilities.
Another common feature in such games is \emph{statelessness}.
For instance, the capacity of an intercity highway or an internet server is only a function of its (and other highways'/servers') immediate load and not a function of time\footnote{Unless the timescale is years/decades, in which long-term degradation effects will become significant.}.
In our setting, we are interested in \emph{independent learning (IL) using repeated play} in such games, that is, algorithms where each agent learns from their own (noisy) interactions without observing others or a centralized coordinator.
IL, despite being theoretically challenging, is natural for such games as centralized control can be an unrealistic assumption for large populations.







\subsection{Game Formulation}\label{sec:game_initial_formulation}
We formalize the static mean-field game (SMFG), the main mathematical object of interest in this work.
The SMFG problem with repeated plays consists of the following.
\begin{itemize}
 \setlength\itemsep{0em}
    \item Finite set of players $\setN = \{ 1, \ldots, N\}$ with $|\setN| = N \in \mathbb{N}_{ > 0}$,
    \item Set of finitely many actions $\setA$, with $|\setA| = K \in \mathbb{N}_{ > 0}$, 
    \item A payoff function $\vecF: \Delta_\setA \rightarrow [0,1]^K$, which maps the occupancy measure of the population over actions to a corresponding payoff in $[0,1]$ for each action. 
\end{itemize}
Here $\Delta_\setA$ denotes the probability simplex over a finite set $\setA$. For $\vecmu\in\Delta_\setA$, $a\in\setA$, we denote the entry of $\vecF(\vecmu)\in\mathbb{R}^{\setA}$  corresponding to action $a$ as  $\vecF(\vecmu, a) = \vecF(\vecmu)(a) \in\mathbb{R}$.

SMFG is played across multiple rounds where players are allowed to alter their strategies in between observations, where at each round $t\in \mathbb{N}_{\geq 0}$,
\begin{enumerate} 
 \setlength\itemsep{0em}
    \item Each player $i \in \setN$ picks an action $a_t^i \in \setA$,
    \item The empirical occupancy measure over actions is set to be $\widehat{\vecmu}_t := \frac{1}{N} \sum_{i=1}^N \vece_{a_t^i}$ 
    \item Depending on the feedback model, each agent $i \in \setN$ observes noisy rewards:
    
    \textbf{Full feedback: } The (noisy) payoffs for \emph{each} action $\vecr^i_t := \vecF(\widehat{\vecmu}_t) + \vecn_t^i \in \mathbb{R}^{K}$ or,
    
    \textbf{Bandit feedback: } The (noisy) payoff for their chosen action $r^i_t := \vecr^i_t(a_t^i) \in \mathbb{R}$.
    \item Each agent receives the payoff $r^i_t$.
\end{enumerate}
Here $\vece_a \in \mathbb{R}^\setA$ is the standard unit vector with coordinate $a$ set to 1, and we assume for each action $a\in\setA$ the noise  $\vecn_t^i(a)$ are independent for all $i \in \setN, t \geq 0$ and have zero mean and variance upper bounded by $\sigma^2$.
Intuitively, SMFG models games where the payoff obtained from choosing an action depends on how the population is statistically distributed over actions, without distinguishing between particular players.
Hence, each agent observes a noisy version of the payoff of their action (or the payoff of all actions with full feedback) under the current empirical occupancy over actions.

We motivate the relevance of the SMFG framework with three examples from real-world applications.
\begin{enumerate}
    \item \textbf{Network resource management.}
Assume there are $K$ resources (e.g., servers or computational nodes) available on a computer network shared by a large number of (say, $N \gg K$) users.
These resources might have varying capabilities, hence the delay in accessing each will be a non-linear function of the overall load.
At each time step, each user tries to access a resource and experiences a delay that increases with the number of users trying to access the same node.
The Tor network application is a particularly good example of SMFGs: the network has 2-3 million users and functions by each user choosing an entry node to use in a decentralized fashion \citep{tormainpage}.

    \item \textbf{Repeated commuting with large populations.}
Every morning, $N$ commuters from city A to city B choose among $K$ routes to drive to their target (typically $N \gg K$), observing only how long it takes them to commute.
The distribution of choices among the population affects how much time each person spends traveling.
The commute times in modern road infrastructure can be a very complex function of overall load \citep{hoogendoorn2013traffic} due to non-trivial feedback loops and adaptive systems such as load-dependent traffic lights.
On the other hand, city traffic is a prime example of finite resources (with potentially complex payoff functions) shared by thousands or millions of inhabitants, especially during heavy commute hours \citep{ambuhl2017empirical}. 
    The empirical study of macroscopic flow diagrams \citep{geroliminis2008existence, ambuhl2017empirical} suggests that at least in aggregate, city traffic flow is explained by its momentary occupancy.
    This empirical observation motivates modeling periodic commutes as a stateless game.
\item \textbf{Multi-player multi-armed bandits with soft collisions.}
Multiplayer MAB have already been studied in the special case where collisions (i.e., multiple players choosing the same arm) result in zero returns.
In many real-world applications, arms used by multiple players yield diminished (but non-zero) utilities when occupied by multiple players.
For instance, in many radio communications applications (Bluetooth, Wi-Fi), common frequencies are automatically used via \emph{time-sharing}, yielding delayed but successful communication when collisions occur.

    
\end{enumerate}

\subsection{Learning Objectives}
We consider competitive games, that is, each agent aims to maximize their personal expected payoff without regard to social welfare.
We allow agents to play randomized actions (mixed strategies), where each agent $i$ randomly chooses their actions at time $t$ with respect to their mixed strategy $\vecpi_t^i \in \Delta_\setA$.
The primary solution concept for such a game is the Nash equilibrium (NE).

\begin{definition}[Expected payoff, exploitability, Nash equilibrium]\label{def:main_def_nplayer}
For an $N$-tuple of mixed strategies $(\vecpi^1, \ldots, \vecpi^N) \in \Delta_\setA^N$, we define the expected payoff $V^i$ of an agent $i \in \setN$ as
\begin{align*}
    V^i(\vecpi^1, \ldots, \vecpi^N) := \Exop \left[ \vecF\left( \frac{1}{N} \sum_{j=1}^N \vece_{a^j}\right)(a^i) \middle|
a^j \sim \vecpi^j, \forall j = 1, \ldots, N
\right].
\end{align*}
We define the \emph{exploitability} of agent $i$ for the tuple as $\setE^i_{\text{exp}}(\{\vecpi^j\}_{j=1}^N) := \max_{\vecpi'\in \Delta_\setA} V^i(\vecpi', \vecpi^{-i}) - V^i(\vecpi^1, \ldots, \vecpi^N)$. An $N$-tuple $(\vecpi^1, \ldots, \vecpi^N) \in \Delta_\setA^N$ is called a NE if for all $i$, $\setE^i_{\text{exp}}(\{\vecpi^j\}_{j=1}^N)=0$, namely, 
 $V^i(\vecpi^1, \ldots, \vecpi^N) = \max_{\vecpi\in \Delta_\setA} V^i(\vecpi, \vecpi^{-i}),$
and  it is called a $\delta$-NE ($\delta>0$) if 
for all $i$, $\setE^i_{\text{exp}}(\{\vecpi^j\}_{j=1}^N)\leq\delta$, namely, $V^i(\vecpi^1, \ldots, \vecpi^N) \geq \max_{\vecpi\in \Delta_\setA} V^i(\vecpi, \vecpi^{-i}) - \delta$.

\end{definition}

Intuitively, under a mixed strategy profile $(\vecpi^1, \ldots, \vecpi^N)$ that is a Nash equilibrium, no agent has the incentive to deviate from their mixed strategy as in expectation each agent plays optimally with respect to the rest.
A Nash equilibrium of the SMFG can be shown to always exist in this setting without any further assumptions \citep{nash1950non}, as the SMFG is also an $N$-player normal form game when $\setA$ is finite.

\textbf{Goal.}
With the SMFG problem definition and the solution concept introduced, we state our objective: in both feedback models, we would like to find \emph{sample efficient} algorithms\footnote{We rigorously formalize the notion of an algorithm in the different feedback models in the appendix (Section~\ref{section:alg_formalization}).} that learn an \emph{approximate NE} (in the sense of low exploitability) \emph{independently} from repeated plays by $N$ agents when $N$ is \emph{large}.

We will solve the SMFG in the regime when $N$ is very large by leveraging its connection to the corresponding problem in the infinite-player limit, i.e., $N\rightarrow\infty$. 
For this purpose, we introduce the following MF-NE concept.
\begin{definition}[MF-NE]
A mean-field Nash equilibrium (MF-NE) $\vecpi^* \in \Delta_\setA$ associated with payoff operator $\vecF$ is a probability distribution over actions such that $\sum_a \vecpi^*(a) \vecF(\vecpi^*, a) = \max_{\vecpi \in \Delta_\setA} \sum_a \vecpi(a) \vecF(\vecpi^*, a)$.
If it holds that $\sum_a \vecpi^*(a) \vecF(\vecpi^*, a) \geq \max_{\vecpi \in \Delta_\setA} \sum_a \vecpi(a) \vecF(\vecpi^*, a) - \delta$ for some $\delta > 0$, we call $\vecpi^*$ a $\delta$-MF-NE.
\end{definition}
Intuitively, the mean-field limit simplifies the SMFG problem by assuming each agent follows the same policy and replacing the notion of $N$ independent agents with a single agent playing against a continuum of infinitely many identical agents.
Notably, finding an MF-NE is equivalent to solving the following variational inequality (VI) problem corresponding to the operator $\vecF$ and domain $\Delta_\setA$:
\begin{align}\label{eq:mfg_vi_statement}
    \text{Find } \vecpi^* \in \Delta_\setA \text{ s.t. } \vecF(\vecpi^*)^\top (\vecpi^* - \vecpi) \geq 0, \forall \vecpi\in \Delta_\setA. \tag{MF-VI}
\end{align}
The $\delta$-MF-NE is related to the strong gap function of \eqref{eq:mfg_vi_statement}, as $\vecpi^*$ is a $\delta$-MF-NE if and only if $\min _{\vecpi \in \Delta_\setA}\vecF(\vecpi^*)^\top (\vecpi^* - \vecpi) \geq -\delta$.
While the MF-VI is not a concrete game and MF-NE is not strictly speaking a Nash equilibrium of any game, they will be crucial tools in constructing independent learning (IL) algorithms for SMFG.




\subsection{Summary of Contributions}
Based on the VI perspective, we develop algorithms that efficiently converge to approximate NE solutions for the SMFG problem, providing finite-sample, finite-agent, and IL guarantees under different feedback models.
Our key technical contributions are as follows:
\begin{enumerate}
    \item \textbf{VI formulation and approximation.} We formulate the infinite-agent mean-field game limit as a VI.
    We make connections between MF-NE, VIs with regularization, and NE of the SMFG, providing explicit bounds on the bias introduced due to approximating the $N$-agent game with an infinite player game.
    In particular, we show that a solution of the \eqref{eq:mfg_vi_statement} is a $\mathcal{O}(\sfrac{1}{\sqrt{N}})$-NE of the $N$-player game, thereby justifying the usefulness of the approximation \eqref{eq:mfg_vi_statement} when $N$ is large.
    \item \textbf{Independent learning with $N$ agents and full feedback.} Leveraging techniques from optimization and VIs, we analyze independent learning in the $N$-agent setting and prove finite sample bounds with regularized learning. Our work highlights how VI and operator theory can provide insights into IL with full or partial feedback.
    In each feedback model, we propose a $\tau$-Tikhonov regularization scheme that stabilizes the divergence between agent policies without communication, which introduces a $\mathcal{O}(\tau)$ bias.
   When agents have full (though noisy) feedback of reward payoffs, we show that after $T$ rounds of play, our algorithm produces policies with expected exploitability upper bounded by $\mathcal{O} (\frac{\tau^{-2}}{\sqrt{T}}+ \frac{\tau^{-1}}{\sqrt{N}} + \tau)$ with monotone payoffs. 
    For $\lambda$-strongly monotone payoffs, we improve this to $\mathcal{O} (\frac{\tau^{-\sfrac{3}{2}} \lambda^{-\sfrac{1}{2}}}{\sqrt{T}} + \frac{\tau^{-\sfrac{1}{2}} \lambda^{-\sfrac{1}{2}}}{\sqrt{N}} + \tau)$.
    \item \textbf{Learning guarantees with bandit feedback.} We further analyze the learning algorithm under a bandit feedback model and prove finite sample guarantees.
    In this case, we propose a probabilistic exploration scheme that enables agents to obtain low-variance estimates of the payoff operator $\vecF$ without centralized coordination.
    In this scenario, we show that after $T$ rounds of play in expectation the exploitability in the \emph{unregularized} SMFG is bounded by
    $\widetilde{\mathcal{O}}(\frac{N^{\sfrac{3}{4}}}{\sqrt{T}} + \frac{1}{\sqrt[4]{N}})$ for monotone payoffs and by
    $\widetilde{\mathcal{O}}(\frac{N^{\sfrac{3}{4}} \lambda^{-\sfrac{1}{2}}}{\sqrt{T}} + \frac{\lambda^{-\sfrac{1}{2}}}{\sqrt[3]{N}})$ for $\lambda$-strongly monotone payoffs.
    \item \textbf{Empirical validation.} We verify our theoretical results through synthetic and real-world experiments such as the beach bar problem, linear payoffs, city traffic management, and network access within the Tor network.
    These experiments demonstrate the effectiveness of the SMFG formulation and our IL algorithms for addressing games with large populations. 
\end{enumerate}





