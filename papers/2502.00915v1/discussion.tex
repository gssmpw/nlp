

\section{Discussion and Conclusion}

In this work, we proposed a finite-agent, independent learning algorithm for learning SMFGs under bandit feedback, and established finite sample guarantees. Our results hinge on the variational inequality perspective of the mean-field game limit,  demonstrating that efficient independent learning is achievable. 

As the first study on decentralized learning on SMFGs, our results are based on the standard (strong) monotonicity assumption on VIs.
However, research on VIs has identified several more general yet still tractable conditions that might be more applicable to real-world SMFGs, such as  the weak Minty condition \citep{diakonikolas2021efficient}, the variationally stable condition \citep{mertikopoulos2019learning}, pseudo-monotone operators \citep{karamardian1976complementarity}, and generalized monotonicity \citep{kotsalis2022simple}.
More abstractly, an intriguing open question is whether the existence of efficient algorithms for solving the (possibly non-monotone) MF-VI always implies the existence of decentralized algorithms for learning SMFG.
At present, several challenges remain when generalizing our results beyond monotonicity: firstly, our current method requires the expected policy deviation to converge to zero, restricting the algorithmic tools that can be deployed.
Moreover, our proof strategy relies on the convergence of iterates $\vecpi_t^i$ to the unique regularized solution $\vecpi^*$ due to the contractivity of the regularized ascent operator $\Gamma^{\eta,\tau}$. Yet for more general VIs, obtaining such a contractive operator in the $\ell_2$-norm might be difficult.


From a game-theoretic and learning perspective, another relevant question is whether our VI approach to decentralized learning can be extended to Markovian MFGs. 
The current theory does not immediately suggest our analysis could be extended to Markovian games.
Specifically, it is not clear if the Markovian Nash equilibrium would still be characterized compactly as a VI,  or when the corresponding operator would be monotone.
The question of IL in stationary MFGs has been tackled by \cite{yardim2023policy} but with poor sample complexity; future work could address this problem using more general VIs.
Finally, while our results focus on expected exploitability, they could potentially be generalized to guarantee convergence to low exploitability policies with high probability.
