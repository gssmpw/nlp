



\section{Assumptions on Payoffs and Examples}\label{sec:assumptions_examples}


In general, finding a NE in normal form games is known to 
be challenging and possibly computationally intractable~\citep{papadimitriou1994complexity, daskalakis2009complexity}. This difficulty persists even in the mean-field regime \citep{yardim2024mean}.
Therefore, we will analyze SMFG under certain structural assumptions on $\vecF$ that are closely aligned with many real-world applications.

\subsection{Assumption on Lipschitz continuity}
Our first assumption is the Lipschitz continuity of the payoff.
In practice, action payoffs should not change drastically with small variations in population behavior: Assumption~\ref{ass:lipschitz} formalizes this intuition.

\begin{assumption}[Lipschitz continuous payoffs]\label{ass:lipschitz}
The payoff map $\vecF: \Delta_\setA \rightarrow [0,1]^K$ is called Lipschitz continuous with parameter $L > 0$ if for any $\vecmu, \vecmu' \in \Delta_\setA$, $\| \vecF(\vecmu) - \vecF(\vecmu')\|_2 \leq L \| \vecmu - \vecmu' \|_2$.
\end{assumption}

As a direct consequence of the Lipschitz continuous payoffs, we have the following technical lemmas, which will be used in the subsequent analysis. 
\begin{lemma}\label{lemma:technical_bound_1N}
For any policy profile $(\vecpi^1, \ldots, \vecpi^N) \in \Delta_\setA^N$, it holds that
 \begin{align*}
| V^i(\vecpi^1, \ldots, \vecpi^N) - \vecpi^{i, \top} \Exop \left[ \vecF(\widehat{\vecmu}) \middle| \substack{
a^j \sim \vecpi^j, \forall j \\
\widehat{\vecmu} := \frac{1}{N} \sum_{j=1}^N \vece_{a^j}} \right] | 
\leq 
\frac{L\sqrt{2}}{N}.
 \end{align*}
\end{lemma}

In addition, we can also obtain the Lipschitz continuity of $V^i$ and $\setE^i_{\text{exp}}$, as stated below. 

\begin{lemma}[$V^i, \setE^i_{\text{exp}}$ are Lipschitz]\label{lemma:phi_lipschitz}
For any $i\in \setN$, the value functions $V^i:\Delta_\setA^N \rightarrow \mathbb{R}$ and exploitability functions $\setE^i_{\text{exp}}: \Delta_\setA^N \rightarrow \mathbb{R}$ are Lipschitz continuous, that is, for any $(\vecpi^1, \ldots, \vecpi^N) \in \Delta_\setA^N, \vecpi,\bar{\vecpi} \in \Delta_\setA$,
\begin{align*}
| V^i(\vecpi, \vecpi^{-j}) - V^i(\bar{\vecpi}, \vecpi^{-j}) | \leq &L_{j,i} \| \vecpi - \bar{\vecpi}\|_2, \\
    | \setE^i_{\text{exp}}(\vecpi, \vecpi^{-j}) - \setE^i_{\text{exp}}(\bar{\vecpi}, \vecpi^{-j}) | \leq &\bar{L}_{j,i} \| \vecpi - \bar{\vecpi}\|_2,
\end{align*}
with the Lipschitz moduli given by
\begin{align*}
    L_{i,j} = \begin{cases}
\sqrt{K}, \text{if } i = j, \\
\frac{2L\sqrt{2K}}{N}, \text{if } i\neq j,
\end{cases}
\qquad 
\bar{L}_{i,j} = \begin{cases}
\sqrt{K}, \text{if } i = j, \\
\frac{4L\sqrt{2K}}{N}, \text{if } i\neq j.
\end{cases}
\end{align*}
\end{lemma}
We defer the proofs to Sections~\ref{app:technical_lemma} and \ref{sec:extended_proof_lema_lipschitz_phi} in the appendix.  



\subsection{Assumption on Monotonicity}
The next fundamental assumption, monotonicity, is standard in variational inequality literature \citep{facchinei2003finite} and is similar in form to Lasry-Lions conditions in mean-field games literature (initially studied by \cite{lasry2007mean}, later analyzed with reinforcement learning in MFGs by \cite{perrin2020fictitious, perolat2022scaling} and many others).
\begin{assumption}[Monotone/Strongly monotone payoff]
\label{ass:monotone}
The vector map $\vecF: \Delta_\setA \rightarrow [0,1]^K$ is called monotone if for some $\lambda \geq 0$, for all $\forall \vecmu_1, \vecmu_2 \in \Delta_\setA$, it holds that
\begin{align*}
     \left(\vecF(\vecmu_1) - \vecF(\vecmu_2)\right)^\top (\vecmu_1 - \vecmu_2) &= \sum_{a\in\setA} (\vecmu_1(a) - \vecmu_2(a) ) ( \vecF(\vecmu_1, a) - \vecF(\vecmu_2, a) ) \\
     &\leq - \lambda \| \vecmu_1 - \vecmu_2 \|_2^2.
\end{align*}
Furthermore, if the above holds for some $\lambda > 0$, $\vecF$ is called $\lambda$-strongly monotone.
\end{assumption}
Monotone payoffs, as in the case of Lasry-Lions conditions, can be intuitively thought of as modeling problems where the payoff of an action on average decreases as the occupancy increases.
In comparison to \cite{lasry2007mean, perrin2020fictitious}, since the game is stateless in our case, the monotonicity condition degenerates to ``decreasing payoffs with increasing \emph{action occupancy}''.

While Assumption~\ref{ass:monotone} is abstract, one large class of concrete monotone payoff functions are given by games where actions have payoffs non-increasing with their occupancy and there is no interaction between actions, demonstrated below by Example~\ref{ex:decreasing}.


 

\begin{example}[Non-increasing payoffs]\label{ex:decreasing}
Let $F_a:[0,1] \rightarrow [0,1]$ for $a\in\setA$ be Lipschitz continuous and non-increasing functions, define $\vecF(\vecmu) := \sum_a F_a(\vecmu(a)) \vece_a$.
Then $\vecF$ is monotone and Lipschitz.
If $F_a$ is also \emph{strictly} decreasing and there exists $\lambda > 0$ such that $|F_a(x) - F_a(x')| \geq \lambda |x-x'|$ for all $x,x'\in[0,1], a \in \setA$, then $\vecF$ is $\lambda$-strongly monotone.
\end{example}
Applications where such an $F_a$ exists are common and intuitive: non-increasing $F_a$ models congestion effects on common resources.
Even theoretically, the existence of a non-increasing $F_a$ in wireless communications can be motivated by well-known results on the capacity of multiple access channels in information theory \citep{el1980multiple, skwirzynski1981new}.
Likewise, the conceptual models of traffic flow (e.g., see the macroscopic flow diagram \citep{loder2019understanding}) imply traffic \emph{speed} is monotonically decreasing with occupancy in aggregate.
As a special case of Example~\ref{ex:decreasing}, the classical multi-player multi-armed bandits setting with collisions yields a monotone Lipschitz continuous payoff.
We state this explicitly in the following example. 

\begin{example}[Multi-player MAB with Collisions]
For the $N$-player game, for each action $a\in\setA$, take the functions $F_{col}^a: [0,1] \rightarrow [0,1]$ such that
\begin{align*}
    F_{col}^a(x) = \begin{cases}
        \alpha^a , \text{if } x \leq \frac{1}{N}, \\
        \alpha^a N (\frac{2}{N} - x), \text{if } \frac{1}{N} \leq x \leq \frac{2}{N}, \\
        0, \text{if } x \geq \frac{2}{N}.
    \end{cases}
\end{align*}
where $\alpha^a \in [0,1]$ is the expected payoff of the action $a$ when no collision occurs.
Take the payoff map $\vecF(\vecmu) := \sum_{a\in\setA} F_{col}^a(\mu_a) \vece_a$.
The payoff map $\vecF$ is Lipschitz continuous and monotone and corresponds to the classical multi-player MAB with collisions. 

Due to this fact, one interpretation of our SMFG setting is that we allow soft collisions for action payoffs, as actions may yield non-zero but diminished payoffs when chosen by multiple players.
In fact, a model with hard collisions would not be feasible (or realistic) in applications where $N \gg K$ as any policy supported by all actions would lead to collisions with probability approaching $1$.
On the other hand, a general non-increasing function $F_{col}^a$ as in Example~\ref{ex:decreasing} can model more realistic use cases where collisions happen almost surely due to a large number of players/users.
\end{example}









Before concluding the discussion of monotonicity of payoffs, we also compare our payoff assumptions with two other common game-theoretical settings -- potential games\footnote{Not to be confused with $\vecF$ that admits a potential $\Phi$ such that $\vecF = \nabla \Phi$, also called a gradient field.} \citep{monderer1996potential} and their subset,  congestion games \citep{rosenthal1973class}, for which  algorithms with IL guarantees already exist \citep{leonardos2021global}. 
We show that SMFGs with monotone payoffs are not special cases of either of these settings. Specifically, we provide a counter-example of SMFG that does not admit a game potential.

\begin{remark}[SMFG is not a potential game]
The SMFG is a \emph{potential game} if there exists a map $P: \setA^N \rightarrow \mathbb{R}$ such that for all $i\in\setN, \veca = (a_1, \ldots, a_N) \in \setA^N, a\in\setA$, it holds that
\begin{align}\label{eq:potgame}
    V^{i}(a^i, \veca^{-i}) - V^{i}(a, \veca^{-i}) = P(a^i, \veca^{-i}) - P(a, \veca^{-i}),
\end{align}
where $V^{i}(\veca)$ denotes the expected reward of player $i$ when each player $j \in \setN$ (deterministically) plays action $a^j$.
Note that $V^{i}(\veca) = \vecF(\widehat{\vecmu}(\veca))(a^i)$ where $\widehat{\vecmu}(\veca) \in \Delta_\setA$ is the empirical distribution of actions over actions induced by action profile $\veca$.

We provide an example of an SMFG where no such $P$ exists.
Let $\matS \in \mathbb{S}_{++}^{D}$ be a symmetric positive definite matrix, $\vecb\in\mathbb{R}^D$ be an arbitrary vector, and $\matX \in \mathbb{M}^{D, D}$ be a anti-symmetric matrix such that $\matX = -\matX^\top$.
Take the payoff operator
\begin{align*}
    \vecF(\vecmu) = (-\matS + \matX) \vecmu + \vecb,
\end{align*}
which can be trivially shown to be monotone.
In particular, take $N=3, K=3, \setA = \{ a_1, a_2, a_3\}$, and define the reward $\vecF$ as
\begin{align*}
    \vecF \begin{pmatrix}
\mu_1 \\
\mu_2 \\
\mu_3
\end{pmatrix} = \begin{pmatrix}
-\mu_1 - \mu_2 \\
\mu_1 \\
-\mu_3
\end{pmatrix},
\end{align*}
which is monotone.
Assume that a potential $P$ exists and satisfies Equation~(\ref{eq:potgame}).
Then, it would follow that:
\begin{align*}
    V^1(a_2, a_1, a_1) - V^1(a_3, a_1, a_1) &= P(a_2, a_1, a_1) - P(a_3, a_1, a_1), \\
    V^2(a_3, a_1, a_1) - V^2(a_3, a_2, a_1) &= P(a_3, a_1, a_1) - P(a_3, a_2, a_1), \\
    V^1(a_3, a_2, a_1) - V^1(a_2, a_2, a_1) &= P(a_3, a_2, a_1) - P(a_2, a_2, a_1), \\
    V^2(a_2, a_2, a_1) - V^2(a_2, a_1, a_1) &= P(a_2, a_2, a_1) - P(a_2, a_1, a_1) .
\end{align*}
Hence, adding the inequalities above, we have that
\begin{align*}
    0 = & V^1(a_2, a_1, a_1) - V^1(a_3, a_1, a_1)
        + V^2(a_3, a_1, a_1) - V^2(a_3, a_2, a_1) \\
        & + V^1(a_3, a_2, a_1) - V^1(a_2, a_2, a_1)  
        + V^2(a_2, a_2, a_1) - V^2(a_2, a_1, a_1) \\
     = & \vecF((\sfrac{2}{3}, \sfrac{1}{3}, 0), a_2) - \vecF((\sfrac{2}{3}, 0, \sfrac{1}{3}), a_3) +
     \vecF((\sfrac{2}{3}, 0, \sfrac{1}{3}), a_1) - \vecF((\sfrac{1}{3}, \sfrac{1}{3}, \sfrac{1}{3}), a_2) \\
     & + \vecF((\sfrac{1}{3}, \sfrac{1}{3}, \sfrac{1}{3}), a_3) - \vecF((\sfrac{1}{3}, \sfrac{2}{3}, 0), a_2)
     + \vecF((\sfrac{1}{3},\sfrac{2}{3},0), a_2) - \vecF((\sfrac{2}{3},\sfrac{1}{3},0), a_1) \\
     = &  \sfrac{2}{3} -  (-\sfrac{1}{3}) 
     + (-\sfrac{2}{3}) - \sfrac{1}{3} 
     + (-\sfrac{1}{3}) - \sfrac{1}{3}
     + \sfrac{1}{3} - (-1) 
     \neq  0,
\end{align*}
leading to a contradiction.
As a result,  no such potential $P$ could exist, and this (monotone) SMFG cannot be a potential game.
\end{remark}

\subsection{Assumption on Stochastic Noise}
Finally, our results require the standard assumption of independent noise with bounded variance. 
We formalize this below in Assumption~\ref{ass:noise}. 
\begin{assumption}[Independent, bounded variance noise]
\label{ass:noise}
We assume that the payoff noise is zero mean, has entrywise variance upper bounded by $\sigma^2$, and is independent.
That is, we assume the following hold:
\begin{enumerate}
    \item For all $t \geq 0, i \in \setN$ and $a\in \setA$, it holds that $\Exop[ \vecn_t^i(a) ] = 0$ and $\Exop[ \vecn_t^i(a)^2 ] \leq \sigma^2$, 
    \item $\left\{\vecn_t^i(a)\right\}_{i\in  \setN, t \geq 0, a \in \setA}$ are independent random variables,
    \item For any $t \geq 0, i\in \setN$, the random vector $\vecn_t^i$ is independent from $\{a_{t'}^j\}_{t' \leq t, j \in \setN}$ (i.e., actions taken up to step $t$ by all players).
\end{enumerate}
\end{assumption}

The assumption of independent noise with bounded variance is common in bandits literature \citep{anandkumar2011distributed, avner2014concurrent, bubeck2021cooperative}.
We emphasize that the noise vectors $\vecn_t^i$ have entry-wise bounded variance by $\sigma^2$ in Assumption~\ref{ass:noise}.
In optimization or variational inequality settings, the assumption of independent sources of noise in stochastic oracles is also standard, although typically a bound on $\Exop[\| \vecn_t^i \|_2^2]$ is assumed \citep{  juditsky2011solving}.
