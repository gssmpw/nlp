% \subsection{Note: Sub-Section Names in Results}

% \begin{itemize}
%     \item Training a foundation model for fetal ultrasound (sort of an overview of FetalCLIP and FetalCLIP pre-trained data)
%     \item Classifying standard views from unseen data
%     \item Estimating fetal biometry using FetalCLIP
%     \item FetalCLIP can serve as a strong pretrained model for FUS tasks --> Maybe we should merge with "Probing for downstream tasks"
%     \item Probing for downstream tasks --> In most cases we have to adapt or build something on top of a model to address a problem. Here we address related and unrelated downstream tasks (with respect to the pretraining data)
%     \item Probing for standard view classification
%     \item Probing for CHD detection from videos
%     \item Probing for fetal head segmentation
%     \item Probing for fetal abdomen structure segmentation
%     \item Probing for 4CH structures segmentation
    
% \end{itemize}

\subsection{Developing a visual-language foundation model for fetal US}

\begin{itemize}
    % \item \st{summary dataset: size, sources (Corniche and textbook), n patients, pregnancy weeks.}
    % \item \st{Corniche $->$ generated texts (prompts)} that include view, gestational age, and pixel spacing.
    % \item \st{textbook $->$ extract figure-caption pairs.}
    % \item \st{FetalCLIP(?) consists of image encoder and text encoder.}
    % \item Mention image encoder, text encoder, and tokenizer architecture / model.
    \item Mention how we benchmark our foundation model (compared models, downstream tasks and datasets, also mention linear/decoder probing, and few-show (patients) learning).
\end{itemize}


% \begin{itemize}
    % \item Bilang FetalCLIP outperforms both specialist dan other visual-language foundation models pada semua kelas
    % \item Bilang rata-rata F1 score untuk semua kelas itu ABC, terus compare dengan SonoNet, BiomedCLIP, dan CLIP (seperti 2x, 3x, 10x)
    % \item Natural images CLIP ga punya semantic understanding untuk classify different fetal ultrasound anatomical views
    % \item BiomedCLIP struggles untuk classify abdomen dan gabisa distinguish brain subplanes
    % \item SonoNet struggles untuk clasify cervix dan thalamic karena memang ngga ditrain untuk distinguish cervix dan thalamic
    % \item GradCAM (paragraph baru)
    % \item Bilang kalau GradCAM FetalCLIP bisa focus pada area/landmark , seperti femur, cervix, cerebellum, cavum septi blablabla, thalamic blablabla
% \end{itemize}

% We further evaluated EchoCLIP’s performance on quantitative tasks, including evaluation of left ventricular ejection fraction (LVEF) and PAP. EchoCLIP predicts LVEF on the held-out internal test dataset with a mean absolute error (MAE) of 8.4\% and an MAE of 7.1\% on an external test set of videos from the EchoNet-Dynamic dataset from Stanford Healthcare (Fig. 1). At key clinical LVEF thresholds, EchoCLIP achieves an AUC of 0.89–0.90 for an LVEF threshold of 50\%, 0.93–0.94 for an LVEF threshold of 40\% and 0.95–0.97 for an LVEF threshold of 30\% (Supplementary Table 3 and Supplementary Fig. 1). Furthermore, EchoCLIP predicts estimated PAP with an MAE of 10.8 mmHg on the internal test dataset and an MAE of 10.8 on the external test dataset (Fig. 2).

% \begin{itemize}
    % \item Bilang gestational age information was incorporated during FetalCLIP pre-training --> enable network to learn/extract biometry information(?).
    % \item Bilang kita estimate gestational age based on fetal brain view. We leverage HC18, which comes from a different source than the pretraining. Bilang HC18 ini menyediakan pixel size dan head circumference
    % \item Bilang gimana caranya kita prediksi gestational age in summary
    % \item Kita pakai guideline WHO untuk menentukan apakah prediksi gestational age FetalCLIP valid atau tidak, dengan cara cek apakah ground truth HC lies di 95\% interval di prediksi GA
    % \item Compare the CLIP dan BiomedCLIP (biasa, bilang outperform atau sejenisnya) --> CLIP dan BiomedCLIP gapunya capability (cari word lain) untuk gestational age --> Kalau FetalCLIP, punya reasonable blablabla, understand fetal biometry embedded from gestational age
    % \item Analisis banyak salahnya di mana --> Di bagian yang kecil GA nya itu distribusinya sedikit saat training --> Perlu ga ya(?)
    % \item Dr Yaqub --> Bilang kalau dibandingin sama classical method (dari segmentation), segmentation is much better. Tapi kita bukan pengen achieve the maximum performance, tapi cuman mau cek zero shot --> Terus nanti bilang FetalCLIP can understand fetal biometry to some extend, while CLIP and BiomedCLIP do not have that capability. --> Kayaknya ini lebih cocok di discussion(?)
% \end{itemize}

% In addition, the GA distribution in the FetalCLIP pretraining data ({\color{red}Fig. A}) influenced this zero-shot capability; for true HC that lies in between Q1 and Q2 of GA, FetalCLIP achieved 89\% validity rate. Adding more pretraining data accross diverse GA ranges can further improve the performance.



% Most AI solutions relied on pretrained models to act as feature extractor. This is very important especially when the dataset is small or the model is very complex, such as ... . Training a foundation model using large-scale data enables the model to learn generalizable features for unseen data ... Based on this motivation, we assessed the FetalCLIP's image encoder ability to act as a feature extractor.

% Based on this motivation, we evaluated the FetalCLIP's image encoder to assess its capability as a robust feature extractor for downstream tasks.

% \begin{itemize}
    % \item Bilang banyak AI solutions bergantung pada pretrained model
    % \item Khususnya untuk dataset yang sedikit dan model yang complex seperti SAM
    % \item Training a foundation model using large-scale data enables the model to learn generalizable features for unseen data ... --> Sama different tasks
    % \item Based on this motivation --> we assessed or evaluated FetalCLIP as feature extractor, dan juga compare, untuk fetal ultrasound
    % \item We tested the foundation model image extractor pada tasks: (1) standard view classifier, (2) CHD detection from US videos, (3) segmentation
    % \item Bilang kita pakai foundation model ini untuk extract feature aja dan freeze
    % \item Bilang kita juga test model dengan few-patients settings
% \end{itemize}

% We further assessed the FetalCLIP image encoder capability to extract generalizable features for fetal ultrasound downstream tasks. In this setting, the image encoder was completely frozen and a light-weight network was trained to harness the extracted features for an ultimate task—e.g. a linear layer to perform a classification-, heavily relying on the image encoder to provide good image representations. We tested the image encoders on the following tasks: (1) standard view fetal classifier, (2) congenital heart defect (CHD) from fetal ultrasound video, and (3) segmentation of fetal structures. We did the benchmark for this evaluation on two settings; full data and few-patient settings. Our experiments demonstrate that FetalCLIP consistently outperformed CLIP and BiomedCLIP accross all of the benchmarks, suggesting the superiority of FetalCLIP image encoder compared to other visual foundation models for fetal ultrasound tasks.

% These findings underscore the potential of FetalCLIP as a strong feature extractor capable of generating highly generalizable image representations, paving the way for its application in diverse fetal ultrasound analysis pipelines.


% The evaluations were carried out under two scenarios: full-data and few-patient settings.

% \begin{itemize}
    % \item Bilang kita melakukan experiment untuk check/evaluate FetalCLIP image representation untuk classify 6 fetal views dan 3 brain subviews
    % \item Kita melakukannya dengan apply a linear layer on top of the frozen FetalCLIP image encoder --> Kayaknya mesti mention juga kalau image encoder ini transform image jadi 768 dimensional embedding (Mention di Methods)
    % \item Komentar kalau outperformed existing visual foundation models di semua class
    % \item Bilang kalau kita juga ngadain experiment di mana kita hanya punya data dari sedikit patient
    % \item Bilang kalau FetalCLIP outperforms
%     \item Mention kalau dengan 32 patients saja, FetalCLIP on-par or even better than CLIP dan BiomedCLIP trained using the full data, showcasing the xxx of fetalCLIP image representations.
% \end{itemize}

% "In addition, we evaluated their performance in a data-efficient setting, using training data from only a few patients. Even in this constrained scenario, FetalCLIP consistently outperformed both the natural and medical vision foundation models ({\color{red}Fig. A}). Remarkably, when trained on data from just 32 patients, FetalCLIP achieved accuracy comparable to, or even surpassing, BiomedCLIP trained on the full dataset of 1,000 patients ({\color{red}Fig. A})."

% To address this, we evaluated the performance of FetalCLIP in generating robust features capable of distinguishing six anatomical views and three brain subplanes using the Planes DB dataset. To test its adaptability, we attached a single linear layer to the frozen FetalCLIP image encoder, leveraging its pre-trained features to assess classification accuracy and generalizability. This approach enabled us to systematically investigate FetalCLIP's potential for cross-domain robustness in real-world fetal imaging scenarios.


% \begin{itemize}
    % \item Bilang kita adapt FetalCLIP untuk deteksi CHD
    % \item Kita melakukannya dengan extract dan combine features dari ultrasound frames dan attach a linear layer) --> Kayaknya mesti mention juga kalau image encoder ini transform image jadi 768 dimensional embedding 
    % \item Komentar kalau outperformed existing visual foundation models di semua class dengan median AUROC sekian
    % \item Mention kalau FetalCLIP juga suitable atau adaptable untuk fetal ultrasound video analysis
% \end{itemize}
% In some clinical practices, clinicians are often required to analyze ultrasound videos to assess fetal condition. Developing AI to solve such tasks is typically harder as ultrasound videos are typically more scarce than images. Thus, leveraging a pretrained model is essential improve the model generalizability. Motivated by this, we adapt FetalCLIP to analyze fetal ultrasound videos showing 4 chamber view, aiming to tell if the fetal heart is normal or abnormal. We processed each frame to extract features using a vision foundation model (i.e. FetalCLIP, BiomedCLIP, or CLIP). We then combined the frame features (see Method) and apply a linear layer to ... .




% We compared the performance of using the FetalCLIP image encoder with that of the CLIP and BiomedCLIP image encoders.



% \begin{itemize}
    % \item Bilang kita test FetalCLIP pada pixel-level segmentation to check if FetalCLIP has fine-grained semantic (coarse-grained ini copy dari CONCH) of understanding or analyzing fetal ultrasound images
    % \item Bilang kita apply a lightweight decoder (with only 1k parameters) to utilize FetalCLIP intermediate image features to segment fetal structures.
    % \item Bilang kita ada 3 segmentation tasks, fetal structures in brain view, abdomen view, and 4 chamber view.
    % \item Bilang kita report average dice score of each structure in every view karena kita juga peduli pada semua structures.
    % \item Bilang FetalCLIP outperform other models in the full patient data training setting and few patients
% \end{itemize}

% Accurate pixel-level classification is critical for precise growing fetal biometry calculations \cite{hc18}. We investigated the foundation models' ability to provide fine-grained intermediate image features essential for segmentation. We apply a lightweight decoder with few parameters (~1.3k for ViT-B models and ~1.6k for ViT-L models) to utilize the intermediate image features to/for ... . We conducted this segmentation experiment on three different fetal views: 1) brain view to segment the fetal head, 2) abdomen view to segment the fetal abdomen, stomach, and spine, and 3) 4ch views to estimate nine structures. We reported the average dice score (DSC) for each view and weighted each structure equaly. FetalCLIP achieved DSC of X\%, X\%, and X\% for brain, abdomen, and 4-chamber view respectively surpassing BiomedCLIP by X\%, X\%, and X\%. Similar trend was also observed for the data-efficient settings (Fig. A) where FetalCLIP consistently outperformed other foundation models. These results demonstrated that FetalCLIP can serve as a strong feature extractor for fetal ultrasound tasks requiring fine-grained details.

% \subsection{Discussion}


% The concept of foundation models has transformed image analysis domain into an exciting paradigm. This enables building a complex AI system by transforming inputs such as image and text into useful representations. Unlike in the natural domain, developing foundation models for the medical domain is more challenging as the modality is very heterogeneous. Although recent studies have revealed that a foundation model developed specifically for a medical modality often performed better than general medical foundation models, both general and specific medical foundation models did not much take into account the fetal ultrasound. The slow advancement in fetal ultrasound can be attributed to the scarcity of available data in this domain that is both sufficient in quantity and contextual information.

% To fill in the aforementioned gap, in this study, we present the first visual-language foundation model for fetal ultrasound by leveraging a large data  routine prenatal scans.

% To fill in the aforementioned gap, in this study,
% In this study, our findings suggest that despite the absence of large image-clinical description pairs, a strong visual-language foundation model can be developed by leveraging routine prenatal scans with minimum contextual information and image-caption pairs from a textbook. Unlike other foundation models that give low priority to fetal ultrasound, FetalCLIP is highly adaptable to various fetal ultrasound tasks. Despite not being trained to solve a specific task, FetalCLIP demonstrated an excellent zero-shot view classification across different fetal structures and a remarkable ability to estimate gestational age. Furthermore, our downstream probing experiments on diverse fetal ultrasound tasks, encompassing image classification, video classification, and image segmentation, highlighted the significant gains of FetalCLIP over other foundation models. These findings align with recent studies that specific-modality visual-language models perform better than the generalist ones on that specific modality.


% FetalCLIP represents the first visual-language foundation model for fetal ultrasound. The release of FetalCLIP model to the public can enable a faster advancement of fetal ultrasound analysis by building solutions on top of this foundation model. Additionally, this study encourage further exploration for foundation model for fetal ultrasound by expanding the pretraining data to cover wider gestational age, include Doppler images, and extending FetalCLIP to video encoder.

% This work poses several significant limitations which are mainly attributed to the FetalCLIP pretraining data. While our zero-shot results demonstrate substantial performance gains of FetalCLIP over other visual-language foundation models, we anticipate that the FetalCLIP zero-shot capability is not optimal for detecting abnormalities in fetal ultrasound scans as the largest portion of the pretraining data was collected from routine pregnancy scans. To add, the pretraining data did not include doppler images and scans were mostly captured at 2nd trimester. Even with our pretraining data constraints, FetalCLIP demonstrates superior performance and adabtability accross various tasks compared to existing foundation models which were typically trained with millions of data. This leaves an immense potential to enhance FetalCLIP representations by incorporating more diverse fetal ultrasound images. Additionally, due to computational constraint and to have fair benchmark with most visual-language foundation models, we limited our experiments to facilitate $224\times224$ image size, where higher resolution is typicall required to clearly visualized fine-grained details such as  valves in fetal hearts and structures in brains.


% Limitations  ... . \begin{itemize}
%     \item \st{a large portion our dataset contains limited information (from routine pregnancy scans) --> just have gestational age, pixel spacing, and visible fetal structures --> limiting zero-shot performance untuk beberapa specific tasks seperti view classification and gestational age estimation}
%     \item \st{gestational age mayoritas ada di tengah2 sehingga gestational age estimation kurang reliable untuk first semester and third semester --> adding this kind of dataset can eliminate this issue}
%     \item \st{the data does not include doppler images}
%     \item \st{Bilang dengan data yang limited ini aja performance atau representationnya masih lebih bagus dibanding general medical foundation models yang ditrain dengan millions of data, maka nambahin data atau making data more rich bisa jadi potensi untuk push boundary/improve significantly}
%     \item \st{image size 224 x 224}
%     \item image-based only --> video-based can work better to extract spatio-temporal features
% \end{itemize}


% \subsection{Zero-shot view classification}
% \begin{itemize}
%     \item \st{Mention tentang prompt from typical CLIP vs. prompt yang juga mempertimbangkan prompt yang dipakai saat training}
%     \item \st{Prompt ensembling vs tanpa ensembling}
%     \item \st{Bilang test SonoNet SN-16 32 dan 64 --> 32 yang paling bagus --> Supaya reviewer ga bilang "ini kayaknya dari model sizenya aja, kalau SonoNet punya ViT-L 14 juga bisa jadi lebih bagus juga"}
%     \item \st{Mention juga tentang dataset yang dipake, motivation, image size kita berapa, data preprocessing, dll}
% \end{itemize}

% We extended the zero-shot protocol of CLIP to test the visual-language foundation models ability to classify standard fetal ultrasound views.

% To perform zero-shot standard view classification using vision-language foundation models, we first defined a set of views as target classes. Then, as VLMs are sensitive to text prompts for zero-shot prediction, we provided five text prompts for each target class, enabling us to perform prompts ensembling for improved robustness to text prompts. We transformed each text prompt into a 768-dimension text embedding by using VLM tokenizer and text encoder and ensemble five prompts for each class by taking the mean embedding ({\color{red}Supplementary Table X}). Similarly, an input image was mapped into a 768-dimension feature embedding using VLM image encoder. We then computed cosine-similarity between image embedding and each text class embedding, and ultimately selected the class with highest score as the final prediction.

% This zero-shot performance was evaluated by using the Planes DB \cite{planes_db} dataset. The data contains six classes, i.e. abdomen, femur, brain, thorax, cervix, and another class called 'other' to represent other diverse views. The data also consists of three fine-grained brain views, which are cerebellum, thalamic, and ventricular. For every image, we squared the image by padding symmetrically with zero and resized the image to the uniform size of 224$\times$224. We first did an evaluation on distinguishing five standard views (the 'other' class was excluded). To facilitate a fair comparison with a specialist model (SonoNet), nine standard views were selected as the target classes ({\color{red}Supplementary Table X}). As SonoNet does not have a distinguished class for Cervix, we consider "Other" class in SonoNet as cervix. Secondly, we did an evaluation to classify three brain subviews. As SonoNet does not have a distinguished class for thalamic, we consider "Other" class in SonoNet as thalamic. We provided SonoNet performance accross different model sizes in ({\color{red}Supplementary Table X}), where the mid-size SonoNet model achieved the highest F1-score among SonoNet model size variants.

% We transformed each text prompt into a 768-dimension text embeddings by first tokenizing the text prompt using BPE \cite{bpe}, adding the start and stop tokens at the begining and the end respectively, padding the tokens if shorter than 117, and passing the tokens to FetalCLIP text encoder.

% \subsection{FetalCLIP architecture and pretraining}

% \begin{itemize}
%     \item \st{Mention model yang digunakan}
%     \item \st{Mention augmentations}
%     \item \st{Mention how to do it}
%     \item \st{Mention learning rate, epoch, and other training hyperparameters}
%     \item \st{Bilang kalau pada akhir training, kita keep model dengan zero-shot performance tertinggi pada viwe classification}
%     \item \st{Dont forget to mention initialization} %\footnote{The pretained model is available at \url{https://huggingface.co/ryanyip7777/pmc_vit_l_14}}
% \end{itemize}
% We built the FetalCLIP model by adapting the architecture and training procedure used in CLIP \cite{clip}. FetalCLIP image encoder is based on ViT-L \cite{vit} with an image input size of 224$\times$224, patch size of 14$\times$14, and 24 transformer layers. We used a text transformer \cite{radford2019language} with 12 transformer layers as FetalCLIP text encoder. Both FetalCLIP image and text encoders project their inputs accordingly to a shared space of 768 dimensions. FetalCLIP was pretrained by maximizing the similarity between embeddings of fetal ultrasound image-caption pairs, and minimizing the similarity of unpaired images and captions. This pretraining process enables FetalCLIP to extract semantically valuable feature embeddings from fetal ultrasound images and captions.

% We applied data augmentation techniques during FetalCLIP pretraining, including random rotation ($\theta_{rotation} \in [-7 \degree , 7\degree]$), translation ($\theta_{translation} \in [-0.05, 0.05]$), and color jittering ($\theta_{brightness},\theta_{contrast},\theta_{saturation} \in [0.85, 1.15]$). We pretrained FetalCLIP for 20 epochs with a learning rate of 5e-6, warmup phase of 2,000 steps, cosine scheduler, and 0.1 weight decay. We implemented mixed-precision training using 4x RTX A6000, enabling us to allocate a batch size of 140 for each GPU. Model checkpoint was saved after every epoch, and tested on zero-shot standard view classifications, and the highest would be kept. We tried several model initializations ({\color{red}Supplementary Table 1}), with the final FetalCLIP was initialized from , which is the CLIP model fine-tuned with the PMC-OA dataset and ROCO.

% \subsection{Zero-shot gestational age estimation}
% \begin{itemize}
%     \item \st{awal. Describe dataset}
%     \item \st{bilang --> model dievaluasi dengan nilai HC sebagai ground truth}
%     \item \st{terus bilang kita pakai konversi statistik dari WHO yang konfersi dari GA ke HC, dengam rumus AbCd}
%     \item \st{jelasin parameternya bergantung pada quartile}
%     \item \st{bilang karena equation WHO ini cuman hold dari 14 weeke sampai 40 weeks, kita hanya exclude nilai HC di antara 1 mm sampai 10 mm, which are 50\% HC percentile untuk GA 14 weeks dan 40 weeks respectively, resulting in X test brain images}
%     \item \st{terus bilang kar3na ini based on statistics, kita evaluasi apakah prediksi GA dari model itu ada di 95\% populasi untuk nilai HC ground truth}
%     \item \st{prompt embedding}
% \end{itemize}

% We leveraged the HC18~\cite{hc18} dataset to test VLM's ability to estimate gestational age. This dataset contains fetal brain view images accompanied by head circumference (HC) and pixel spacing. As the dataset didn't come with the true gestational age, the evaluation was based on the true HC. The GA can be converted to HC by using the quantile regression~\cite{fetalcalculator} below
% \begin{equation}
%     \label{eq:ga_to_hc}
%     HC = b_0 + b_1 t + b_2 t^2 + b_3 t^3 + b_4 t^4
% \end{equation}
% where $t$ denotes GA in days, and $b_0,b_1,b_2,b_3,b_4\in \mathbb{R}$ are coefficients where their values depend on which quantile we are looking to. As Equation \ref{eq:ga_to_hc} only holds for GA from 14 weeks to 40 weeks, we only included images having HC in between 100 mm and 342 mm, which are the 50th quantiles of HC from GA 14 weeks and 40 weeks, respectively, resulting in 814 test brain images. To determine if the predicted GA makes sense, we developed a proxy task by defining a valid prediction if the true HC lies within the 2.5th to 97.5th percentile (95\% of the population) associated to the prediction.

% Inspired by \cite{echoclip}, we estimated GA by first extracting image features and subsequently constructing text prompts that describe the brain view, pixel spacing, and GA ranging from 14 weeks 0 days to 40 weeks. Similar to the zero-shot classifier, we provided five text prompts for every GA for ensembling. We then computed the cosine similarity between an image and text prompts for every GA, and performed prompts ensembling by taking the mean of cosine similarities across a GA. As a postprocessing step, we ultimately selected the gestational age prediction as the median of the gestational ages in the top 15 text prompts with the highest cosine similarity to the image features. The text prompts and ablation study of other alternative post-processing steps are provided in Supplementary Data 1 and Supplementary Data 2, respectively.


% \subsection{Probing FetalCLIP for downstream tasks}
% \subsubsection{Experimental setup}

% \begin{itemize}
%     \item \st{Bilang kita split dataset into training dan testing}
%     \item \st{Training dipake buat train model dan validasi dan tune hyperparameter, dan testing bener2 buat test doang, ga dipake selama model development}
%     \item \st{Bilang untuk full data patients, kita pake 5 fold CV, train using 4 folds and save yang bisa ngehasilin valid loss paling kecil, terus test ini model dengan test set}
%     \item \st{Bilang 5 set 5-fold, dan 5 different seeds --> Supaya dapet result yang robust dari pemilihan training set dan seeds}
%     \item \st{Untuk few-pasient setting, bilang pakai N data buat training dan validasi. Terus kita ada 5 support set untuk setiap experiment --> Motivasinya adalah karena kalau di few-shot setting, pemilihan support set penting juga, jadi kita ini tujuannya supaya dapat yang lebih reliable lah intinya}
%     \item \st{Untuk statistical test, kita report mean dan P-value yang di-compute dengan cara methode Wilcoxon Signed-Rank --> to support our claim}
% \end{itemize}
% The probing experiments were conducted for downstream fetal ultrasound tasks to assess the quality of the image embeddings of foundation models. In this study, the probing involved freezing image encoders and attaching a trainable prediction head, which was a linear layer for classification tasks or a trainable light-weight decoder for segmentation tasks. We compared FetalCLIP with both natural and medical vision language foundation models for various fetal ultrasound benchmarks ($p$-values were computed using Wilcoxon signed-rank test). For each evaluation, we splitted the corresponding dataset by patients into training and testing sets (80\%-20\%), preventing test data leakage due to patient attributes. For classification tasks, the split was stratified split. The prediction head was tuned and validated using the training set and tested using the testing set, leaving the testing set untouched during model development. For the full data training experiment, using the training set, we performed stratified 5-fold cross-validation, where we trained the prediction head and saved the model with lowest validation loss, and then evaluated this model on test set. For each fold, the experiments were run for five times using different seeds which resulted in total 25 runs, in order to get reliable result statistics. For the few-patient training, we randomly selected $N$ patients for prediction head training and $N$ patients for validation, and validated using the testing set. These $N$ training and $N$ validaton patients are defined as a support set. For each experiment, we constructed five support sets, and we did five runs for five different seeds, resulting in total 25 results.

% We conducted probing experiments on downstream fetal ultrasound tasks to evaluate the quality of image embeddings from foundation models. In these experiments, the image encoder was frozen, and a trainable prediction head—a linear layer for classification tasks or a lightweight decoder for segmentation tasks—was attached. We compared FetalCLIP with both natural and medical vision-language foundation models across various fetal ultrasound benchmarks, with statistical significance assessed using the Wilcoxon signed-rank test (p-values). For each evaluation, datasets were split by patients into training (80\%) and testing (20\%) sets to prevent data leakage due to patient attributes. Stratified splitting was employed for classification tasks. The prediction head was tuned and validated using the training set, while the testing set remained untouched during model development.

% For full-data training experiments, we performed stratified 5-fold cross-validation on the training set. The prediction head was trained, and the model with the lowest validation loss was saved and subsequently evaluated on the test set. Each fold was run five times using different random seeds, resulting in 25 total runs to ensure robust statistical evaluation. For few-patient training, N patients were randomly selected for training the prediction head, and an additional N patients were used for validation, collectively referred to as a support set. Five distinct support sets were constructed, and each was evaluated across five random seeds, resulting in 25 total outcomes per experiment.

% \subsubsetion{View classification}

% \begin{itemize}
%     \item \st{Describe problem}
%     \item \st{Jelasin dataset \& data preprocessing}
%     \item \st{Bilang cuman attach 1 layer untuk 6 classes dan 3 classes}
%     \item \st{Hyperparameter}
%     \item \st{Bilang model terbaik saat training itu yang punya val loss paling kecil}
% \end{itemize}

% The probing for view classification experiments was conducted using the Planes DB~\cite{planes_db} dataset, with the average F1 score computed as the performance metric. The dataset comprises six categories of fetal ultrasound views (abdomen, brain, cervix, femur, thorax, and other) and three brain sub-views (cerebellum, thalamic, and ventricular). A single linear layer was appended to the VLM image encoder, configured with six output classes for general view classification and three output classes for fine-grained brain view classification. The dataset was split into 9999 training samples and 9999 testing samples. Cross-entropy loss was used as the optimization objective, and training was performed for 50 epochs. The AdamW optimizer was employed with a learning rate of 3e-4, a weight decay of 1e-2, and a batch size of 64.

% \subsubsection{CHD detection}

% \begin{itemize}
%     \item \st{Describe the problem}
%     \item \st{Jelasin dataset \& data preprocessing --> normal vs abnormal, size, temporal dimension range}
%     \item \st{Leverage FetalCLIP to build a CHD detection, dengan 2 output classes (normal vs. abnormal)}
%     \item \st{Jelasin caranya gimana --> Extract features from each frame, and then concatenate --> Kasih tau ambil berapa frames, terus sampling stride nya brp --> Bilang juga kita coba average and concatenate, tapi average ga berhasil (train loss stucks at 0.5)}
%     \item \st{Hyperparameter, terus bilang model terbaik saat training itu punya val loss paling kecil}
% \end{itemize}
% We studied how well image embeddings of foundation models transferred to fetal ultrasound video analysis using an internal data. We collected 418 four-chamber videos, consisting of 161 normal heart scans and 257 abnormal, with a temporal length ranging from 16 to 128 frames. To simplify the task, instead of passing the entire sequences to a classifier, we extracted a clip with 16 consequent frames from a video by sampling with approximately uniform spacing if a video contains 64 frames, and sampling with temporal stride of 4 if the video is longer than 64 frames. This clip sampling approach ensures that the clip contains at least 50\% of video length, retaining adequate information to allow diagnosis. Image features of each frame were extracted and further combined to construct clip features. We tried two trivial feature combination methods and found out that concatenating the frames features resulted in the best performance ({\color{red}Supplementary Table X}). We trained a single linear layer to classify the heart as normal or abnormal. We trained the linear layer using cross-entropy loss over 5 epochs with AdamW optimizer, 1e-4 $lr$, 0.2 $wd$, and a batch size of 32.

% \begin{itemize}
    % \item {\color{red}Tambahin tentang. Bilang augmentation-nya offline, terus tiap sample/image diaugment beberapa kali terus di-save. Terus mention juga apa aja augmentation.}
% \end{itemize}
% including random rotation ($\theta_{rotation} \in [-7^\circ, 7^\circ]$), translation ($\theta_{translation} \in [-0.05, 0.05]$), and color jittering 
% AUGMENTATION = A.Compose([
%     A.ColorJitter(0.2, 0.2, 0.2, 0.2, p=0.5),
%     A.CLAHE(p=0.5),
%     A.ShiftScaleRotate(
%         shift_limit=0.2,
%         scale_limit=0.0,
%         rotate_limit=20,
%         interpolation=cv2.INTER_LINEAR,
%         border_mode=cv2.BORDER_CONSTANT, value=0, p=1.
%     ),
% ])












% \subsection{Zero-shot classification of standard fetal views from different clinical sources}


% We performed the evaluation using the Planes DB \cite{planes_db} dataset, employing the models to classify five anatomical planes—abdomen, brain, cervix, femur, and thorax—as well as three subplanes within the brain—cerebellum, thalamic, and ventricular.


% Our experiment ({\color{red}Fig. A}) demonstrated that FetalCLIP outperforms the compared models by a notable margin, with an average F1 score of {\color{red}X}. This performance is {\color{red}X} higher than the specialist model SonoNet, {\color{red}X} times higher than BiomedCLIP, and {\color{red}X} times higher than CLIP.


% Our confusion matrix analysis further showed that SonoNet struggled in differentiating the cervix view from other fetal planes and UniMed-CLIP could not distinguish between spine and other fetal planes.

% In addition, our qualitative analysis showed that FetalCLIP can effectively highlight key fetal landmarks when identifying anatomical views ({\color{red}Extended Data Fig. \ref{ext_fig_cam}a-b}). This showcased FetalCLIP's ability to localize specific structures within fetal ultrasound images.



% \subsection{FetalCLIP interpretability}

% To analyze the FetalCLIP reliability from the clinical perspective, we conducted interpretation studies to understand how FetalCLIP derives its prediction and to investigate whether it aligns with clinical practice. We provided class activation mapping via ScoreCAM \cite{scorecam} in {\color{red}Fig. \ref{fig:interpretation_studies}a} to visualize the importance of regions, areas, or pixels in an image to FetalCLIP decision. It shows that FetalCLIP can effectively highlight key fetal landmarks when identifying anatomical views, such as stomach in the abdomen view, femur bone, heart circumference, cerebellar hemispheres, and cavum septum pellucidi. This showcased FetalCLIP's ability to localize specific structures within fetal ultrasound images. In addition, as visualized in {\color{red}Fig. \ref{fig:interpretation_studies}b}, FetalCLIP highlighted regions surrounding the head circumference and some brain structures, such as the choroid plexus, to estimate GA.


% \section{Discussion}

% In this study, our findings suggest that despite the absence of large image-clinical description pairs, a strong visual-language foundation model can be developed by leveraging routine pregnancy scans with limited contextual information, supplemented by image-caption pairs from a textbook. Unlike other foundation models that place limited emphasis on fetal ultrasound, FetalCLIP is highly adaptable across various fetal ultrasound tasks. Despite not being explicitly trained for specific tasks, FetalCLIP achieved excellent zero-shot view classification of different fetal structures and exhibited remarkable accuracy in estimating gestational age. Moreover, our downstream probing experiments across diverse fetal ultrasound tasks—including image classification, video classification, and image segmentation—highlighted substantial performance gains of FetalCLIP over to other foundation models. This establishes our visual-language foundation model as the most preferred pretrained model for developing AI models to solve challenging problems in fetal ultrasound analysis, especially in data-efficient settings. These findings align with recent studies reporting the superiority of modality-specific visual-language foundation models over their general medical counterparts when applied to the targeted modality \cite{PLIP,CONCH,FLAIR,eyeclip,unimedclip}.

% --> Bilang fetalclip estimage GA dengan memperhatikan struktur2 yang ada pada gambar. Estimasi ini, ditambah dengan hasil biometry pengukuran, bisa digunakan untuk mengecek kondisi perkembangan bayi, apakah normal atau tidak (matching antara pengukuran biometry dan gestational age). Akan tetapi, FetalCLIP ga terlalu bagus untuk estimasi GA diujung-ujung extreme (terlalu muda atau terlalu tua fetal nya), yang kami hipotesis disebabkan oleh pretraining data yang mayoritas second trimester.

% This study also demonstrates that FetalCLIP can serve as a robust feature extractor. Our downstream probing experiments across diverse fetal ultrasound tasks—including image classification, video classification, and image segmentation—highlighted substantial performance gains of FetalCLIP over to other foundation models. ... . This establishes our visual-language foundation model as the most preferred pretrained model for developing AI models to solve challenging problems in fetal ultrasound analysis, especially in data-efficient settings. These findings align with recent studies reporting the superiority of modality-specific visual-language foundation models over their general medical counterparts when applied to the targeted modality \cite{PLIP,CONCH,FLAIR,eyeclip,unimedclip}.

% —including image classification, video classification, and image segmentation-



% \subsection{FetalCLIP architecture and pretraining}
% We explored various model initializations ({\color{red}Supplementary Table 1}). Our experiments showed that fine-tuning the original CLIP in the general medical domain provided a strong initialization for our FetalCLIP model. This underscores the importance of aligning a foundation model for natural domain to the medical domain as a crucial step for better initialization, resulting in improved performance on more specific modalities.
% We explored various model initializations ({\color{red}Supplementary Table 1}), with the final FetalCLIP model initialized from 'X,' which was the CLIP fine-tuned on the PMC-OA dataset and ROCO.



% \subsubsection{View classification}
% The dataset was divided into a train-test split, resulting in {\color{red}9999}-{\color{red}9999} samples and {\color{red}9999}-{\color{red}9999} samples for the first and second view classification tasks, respectively.
% Cross-entropy loss was applied as the optimization objective over 50 epochs. The AdamW optimizer was employed with a learning rate of 3e-4, a weight decay of 1e-2, and a batch size of 64.


% \subsubsection{CHD Detection}
% We tested two trivial feature combination methods and found that concatenating the frame features yielded the best performance ({\color{red}Supplementary Table X}).
% {\color{red}To classify the heart as normal or abnormal, we attached a single linear layer trained using cross-entropy loss over 5 epochs with the AdamW optimizer, configured with a learning rate of 1e-4, weight decay of 0.2, and a batch size of 32}.

% \subsubsection{Segmentation}

% \begin{itemize}
%     \item \st{Describe the light-weight decoder}
%     \item \st{evaluation metric (mention the averaging process also)}
%     \item \st{Describe kita pake multilabel}
%     \item \st{General hyperparameter}
%     \item \st{Describe dataset and specialized hyperparameter}
% \end{itemize}

% A lightweight decoder was designed to harness intermediate image representations of foundation models for segmenting fetal structures. As in {\color{red}Extended Data Fig. X}, we adapted the UNETR \cite{unetr} decoder for 2D and to reduce the computation and number of parameters in the decoder, we replaced the deconvolution layer to depthwise deconvolution and convolution blocks into depthwise separable convolution \cite{mobilenetv1} blocks with a kernel size of 3. We reported the dice score coefficient (DSC) to assess the quality of the segmentation results.  For views with more than one segmented structure, we considered the segmentation as multilabels where one pixel can belong to multiple structures, e.g. a pixel can be four-chamber, heart, and thorax at the same time. This multilabel segmentation approach involved providing $N_s$ output channels fo $N_s$ structures. For this case also, we reported the average DSC across all structures to treat each structure equally important. In addition, the lightweight decoder was trained to minimize the dice loss, and the AdamW optimizer was used with a learning rate of 3e-4 and a weight decay of 0.01.

% A lightweight decoder was developed to harness intermediate image representations from foundation models to segment fetal structures. As shown in {\color{red}Extended Data Fig. X}, we adapted the UNETR \cite{unetr} decoder for 2D applications. To reduce computational complexity and the number of parameters, the deconvolution layers were replaced with depthwise deconvolution layers, and convolutional blocks were replaced with depthwise separable convolution \cite{mobilenetv1} blocks with a kernel size of 3. We reported the dice score coefficient (DSC) to assess the quality of the segmentation results. For views containing multiple segmented structures, we employed a multilabel segmentation approach, wherein a single pixel could belong to multiple structures (e.g., a pixel might simultaneously represent the four-chamber view, heart, and thorax). This approach required $N_s$ output channels for $N_s$ structures, with the average DSC across all structures computed to give equal importance for each structure. The lightweight decoder was trained to minimize the Dice loss using the AdamW optimizer, with a learning rate of 3e-4 and a weight decay of 0.01.


        % \textbf{FetalCLIP interpretation studies.}
        % \textbf{a,} ScoreCAM of FetalCLIP when identifying fetal ultrasound views. FetalCLIP effectively emphasized important structures when determining anatomical views.
        % \textbf{b,} ScoreCAM of FetalCLIP when estimating gestational age. FetalCLIP highlighted brain structures and surrounding head circumference regions when estimating gestational age.
        % \textbf{c-d,} Visualization of FetalCLIP image embeddings via UMAP on the PlanesDB dataset ignoring the Other class.
        % \textbf{f,} FetalCLIP image embeddings on the PlanesDB dataset including the Other class. Images containing similar information were mapped into close proximity.




        % \textbf{Data-efficient transfer learning for downstream tasks.} The image encoder was frozen during this experiments. We used data from $N$ patients for training and another $N$ patients for validation, which were defined as a support set. The model was tested on a separate testing set. To get a reliable statistic, we did experiments using five support sets across 5 different seeds, resulting in 25 results.
        % \textbf{a-b}, Classification of six fetal planes and three brain sub-planes, respectively.
        % \textbf{c-e} Segmentation performance in the head, abdomen, and 4-chamber views, respectively.



    % \caption{
    %     \textbf{Prompts used to generate captions for the routine clinical scans data and image-caption pairs from a textbook.}
    %     \textbf{a,} Prompts for generating caption templates for the 12 standard views, four heart sub-views, and brain sub-planes.
    %     \textbf{b,} Prompts for generating caption templates for the other diverse labels that include images with multiple labels or views.
    %     \textbf{c,} Prompts for preprocessing caption to generate sub-captions for dataset from image-caption pairs from a textbook.
    % }