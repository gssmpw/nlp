\section{Introduction or Main}
The widespread adoption of ultrasound, driven by its accessibility, safety, and cost-effectiveness, has revolutionized prenatal care by enabling real-time anatomical evaluation of fetal development and the early detection of congenital abnormalities {\color{red}[ref]}. However, despite its transformative role, fetal ultrasound image interpretation remains inherently subjective and heavily operator-dependent, often challenged by subtle visual cues, complex fetal anatomy, and significant inter-observer variability. These challenges can lead to inconsistencies in clinical assessments, particularly in resource-limited settings where access to highly trained sonographers is restricted {\color{red}[ref]}. Studies have shown significant inter-observer variability in fetal ultrasound measurements, with differences of approximately ±4.9\% for head circumference (HC), ±8.8\% for abdominal circumference (AC), and ±11.1\% for femur length (FL) {\color{red}[pubmed]}. These variabilities highlight the subjectivity and potential inconsistencies in fetal ultrasound assessments, emphasizing the need for AI-powered tools to enhance diagnostic objectivity and accuracy. These limitations underscore an urgent need for robust AI-powered tools that enhance diagnostic objectivity, accuracy, and accessibility in fetal ultrasound imaging.

Recent advances in artificial intelligence (AI), particularly in foundation models, have demonstrated remarkable capabilities in improving medical imaging analysis. These models, pretrained on large-scale datasets, enable powerful feature extraction and knowledge transfer to downstream tasks, enhancing diagnostic precision, optimizing clinical workflows, and broadening access to expert-level interpretation {\color{red}[ref]}. However, existing foundation models, such as CLIP {\color{red}[ref]}, BiomedCLIP {\color{red}[ref]}, and UniMed-CLIP {\color{red}[ref]}, exhibit fundamental limitations when applied to fetal ultrasound. CLIP, primarily trained on natural images, lacks the domain-specific anatomical knowledge necessary for medical imaging. BiomedCLIP, while tailored for the biomedical domain, is primarily optimized for text-based biomedical knowledge retrieval rather than complex image-text reasoning required for fetal ultrasound interpretation, limiting its ability to effectively capture fine-grained anatomical details. UniMed-CLIP, though leveraging a large-scale open-source medical multimodal dataset, demonstrates variable performance across different imaging modalities and remains largely unexplored in the context of fetal ultrasound. These models struggle to generalize to the unique challenges posed by fetal imaging, failing to capture subtle morphological variations that are crucial for detecting congenital anomalies. Moreover, most AI solutions in fetal ultrasound rely on limited datasets and do not achieve the level of generalizability required for robust clinical deployment, particularly in detecting rare fetal conditions. Encoding the entirety of fetal anatomy while preserving diagnostically critical features remains a significant challenge for existing AI methods. To overcome these barriers, a dedicated fetal ultrasound-specific foundation model is required to harness the full potential of AI for advancing prenatal care.

{\color{red}(\textbf{Reference to data table or figure} also \textbf{Reference to embeddings})} Here, we introduce Fetal-CLIP, a novel visual-language foundation model explicitly engineered for fetal ultrasound analysis, trained on the most comprehensive dataset of its kind. Fetal-CLIP is pretrained at an unprecedented scale, leveraging 207,943 fetal ultrasound images with corresponding GPT-4o-generated captions and 2,092 expert-annotated image-caption pairs from a fetal ultrasound textbook, covering a broad spectrum of fetal anatomical structures and developmental stages {\color{red}(\textbf{spanning 15+ fetal anatomical structures and gestational ages from 12–40 weeks})} to ensure diversity and robustness. It incorporates an innovative multimodal contrastive learning strategy that integrates visual and textual representations of fetal ultrasound data, allowing the model to effectively align anatomical structures with diagnostic descriptions and enhance interpretability. This advanced pretraining paradigm empowers Fetal-CLIP to learn rich, generalizable representations of fetal ultrasound scans and effectively transfer this knowledge to a diverse range of downstream tasks, including zero-shot classification of standard fetal views, congenital heart disease (CHD) detection from ultrasound videos, segmentation of fetal anatomical structures, and feature extraction for downstream fetal ultrasound tasks, ensuring adaptability across clinical applications. Unlike existing models, Fetal-CLIP’s dual-modality learning approach allows it to discern subtle, clinically actionable patterns in fetal ultrasound images, surpassing the capabilities of vision-only models and yielding substantial improvements in diagnostic accuracy and clinical interpretability. 

By addressing the critical gaps in fetal ultrasound analysis, Fetal-CLIP represents a significant step toward more reliable, accessible, and AI-driven prenatal diagnostics. Fetal-CLIP undergoes rigorous evaluation across multiple downstream tasks to assess its generalizability and clinical applicability. It achieves **[exact accuracy]\% accuracy in zero-shot classification of standard fetal views**, outperforming existing models trained with supervised learning. In the critical task of **congenital heart disease (CHD) detection from ultrasound videos**, Fetal-CLIP demonstrates **[ accuracy metric]\% improvement over previous models**, showcasing its ability to detect subtle morphological variations crucial for early diagnosis. Furthermore, for **fetal anatomical segmentation**, it attains a **[ segmentation metric]\% Dice score**, highlighting its proficiency in delineating fetal structures with high precision. These findings firmly establish Fetal-CLIP as a pivotal advancement, bridging the gap between human-level expertise and AI-driven prenatal diagnostics and setting a new benchmark for the field of fetal ultrasound analysis.

