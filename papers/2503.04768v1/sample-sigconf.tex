%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%%
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
% \documentclass[sigconf]{acmart}
\documentclass[sigconf]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}
\usepackage{caption}
% \usepackage{minted}
% % \usepackage{xcolor} % 可选：用于高亮

% \usepackage{listings}
\usepackage{subcaption}
% 自定义 lstlisting 的样式

% \lstset{
%     basicstyle=\ttfamily\footnotesize,    % 设置字体
%     backgroundcolor=\color{gray!10},      % 设置背景颜色
%     frame=single,                         % 添加边框
%     keywordstyle=\color{blue}\bfseries,   % 关键字颜色和加粗
%     commentstyle=\color{green}\itshape,   % 注释颜色和斜体
%     breaklines=true,                      % 自动换行
%     breakindent=0pt,                      % 设置换行后没有缩进
%     xleftmargin=0pt,                      % 设置左边距为 0
%     captionpos=b                          % 标题位置
% }
%\usepackage[utf8x]{inputenc}
\newcommand{\sign}{\text{sign}}
\newtheorem{myDef}{Definition} 
\newtheorem{myAss}{Assumption} 
%Assumption
\newtheorem{myTheo}{Theorem}
\newtheorem{myexam}{Example}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\usepackage[ruled,linesnumbered]{algorithm2e}
% \usepackage{subfigure}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{graphicx}
% \setlist[itemize]{leftmargin=*}
\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}

\newcommand{\etal}{\emph{et~al.}\xspace}
\newcommand{\eg}{\emph{e.g.},\xspace}
\newcommand{\ie}{\emph{i.e.},\xspace}
\newcommand{\wrt}{\emph{w.r.t.}\xspace}
\newcommand{\aka}{\emph{a.k.a.}\xspace}
\newcommand{\etc}{\emph{etc.}\xspace}
\newcommand{\cf}{cf.\/~}
\newcommand\figref[1]{Figure~\ref{#1}}
\newcommand\algoref[1]{Algorithm~\ref{#1}}
\newcommand\tabref[1]{Table~\ref{#1}}
\newcommand\secref[1]{Section~\ref{#1}}
\newcommand\equref[1]{Eq.~(\ref{#1})}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}

\newtheorem{pro}{Problem}
\newtheorem{defi}{Definition}
\newcommand{\e}[1]{\ensuremath{\times 10^{#1}}}
\newcommand{\eat}[1]{}
\ifodd 1
\newcommand{\rev}[1]{{\color{purple}{#1}}} %revise of the text
\newcommand{\hao}[1]{{\color{blue}{#1}}}
\newcommand{\song}[1]{{\color{brown}{#1}}}
\newcommand{\response}[1]{{\color{black}{#1}}}
\newcommand{\TODO}[1]{{\color{red}TODO:{#1}}}
\newcommand\beftext[1]{{\color[rgb]{0.5,0.5,0.5}{BEFORE:#1}}}
\newcommand{\cai}[1]{{\color{cyan}{#1}}}
\else
\newcommand{\rev}[1]{#1}
\newcommand{\hao}[1]{#1}
\newcommand{\TODO}[1]{}
\newcommand{\beftext}[1]{#1}
\fi

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
%\title{DiMA: A Ride-Hailing Assistant with Tool-Augmented Large Language Models}
\title{DiMA: An LLM-Powered Ride-Hailing Assistant at DiDi}
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
% \author{Yansong Ning\textsuperscript{1 \dag}, Shuowei Cai\textsuperscript{1 \dag}, Wei Li\textsuperscript{2}, Jun Fang\textsuperscript{2}, Naiqiang Tan\textsuperscript{2}, Hua Chai\textsuperscript{2}, Hao Liu\textsuperscript{1,3 \ddag}}
% \affiliation{%
%   \institution{\textsuperscript{1}The Hong Kong University of Science and Technology (Guangzhou), \textsuperscript{2}Didichuxing Co. Ltd \\ \textsuperscript{3}The Hong Kong University of Science and Technology}
% \country{}
% }
% \email{{yning092, scaiak}@connect.hkust-gz.edu.cn}
% \email{{peterliwei, fangjun, tannaiqiang, chaihua}@didiglobal.com, liuh@ust.hk}
% \thanks{ \textsuperscript{\dag} Equal contribution. Work done during internship at Didichuxing Co. Ltd.}
% \thanks{ \textsuperscript{\ddag} Corresponding author.}


\author{Yansong Ning\textsuperscript{\dag}}
\affiliation{%
\institution{The Hong Kong University of Science and Technology (Guangzhou)}
\country{}
}
\email{yning092@connect.hkust-gz.edu.cn}


\author{Shuowei Cai\textsuperscript{\dag}}
\affiliation{%
\institution{The Hong Kong University of Science and Technology (Guangzhou)}
\country{}
}
\email{scaiak@connect.hkust-gz.edu.cn}

\author{Wei Li, Jun Fang}
\affiliation{%
\institution{Didichuxing Co. Ltd}
\country{}
}
\email{peterliwei@didiglobal.com}
\email{fangjun@didiglobal.com}


\author{Naiqiang Tan, Hua Chai}
\affiliation{
\institution{Didichuxing Co. Ltd}
\country{}
}
\email{tannaiqiang@didiglobal.com}
\email{chaihua@didiglobal.com}

\author{Hao Liu\textsuperscript{\ddag}}
% \authornote{Corresponding author.}
\affiliation{
\institution{The Hong Kong University of Science and Technology (Guangzhou)}
\institution{The Hong Kong University of Science and Technology}
\country{}
}
\email{liuh@ust.hk}
\thanks{ \textsuperscript{\dag} Equal contribution. Work done during internship at Didichuxing Co. Ltd.}
\thanks{ \textsuperscript{\ddag} Corresponding author.}

\begin{abstract}
On-demand ride-hailing services like DiDi, Uber, and Lyft have transformed urban transportation, offering unmatched convenience and flexibility.
In this paper, we introduce \textbf{DiMA}, an LLM-powered ride-hailing assistant deployed in DiDi Chuxing. 
Its goal is to provide seamless ride-hailing services and beyond through a natural and efficient conversational interface under dynamic and complex spatiotemporal urban contexts.
To achieve this, we propose a spatiotemporal-aware order planning module that leverages external tools for precise spatiotemporal reasoning and progressive order planning. Additionally, we develop a cost-effective dialogue system that integrates multi-type dialog repliers with cost-aware LLM configurations to handle diverse conversation goals and trade-off response quality and latency. Furthermore, we introduce a continual fine-tuning scheme that utilizes real-world interactions and simulated dialogues to align the assistant's behavior with human prefered decision-making processes.
Since its deployment in the DiDi application, DiMA has demonstrated exceptional performance, achieving $93\%$ accuracy in order planning and $92\%$ in response generation during real-world interactions. Offline experiments further validate DiMA’s capabilities, showing improvements of up to $70.23\%$ in order planning and $321.27\%$ in response generation compared to three state-of-the-art agent frameworks, while reducing latency by $0.72\times$ to $5.47\times$. 
These results establish DiMA as an effective, efficient, and intelligent mobile assistant for ride-hailing services.
\begin{figure}
    \centering
    \includegraphics[width=1 \linewidth]{Figure/Figure1_v3.png}
    \caption{
    The conversational interface in DiDi Chuxing App.
    Given a user query issued on 2024-08-28, (a) DiMA proactively guides the user to complete necessary spatiotemporal trip information, and 
    (b) automatically schedules a future ride-hailing order from Guangdong Museum East Gate to Terminal T2 of Guangzhou Baiyun International Airport.
    }
    \label{fig:product}
\end{figure}
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010179.10010181</concept_id>
       <concept_desc>Computing methodologies~Discourse, dialogue and pragmatics</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010179.10010182</concept_id>
       <concept_desc>Computing methodologies~Natural language generation</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Natural language generation}
% \ccsdesc[500]{Computing methodologies~Discourse, dialogue and pragmatics}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{ride-hailing, mobile assistant, large language model, spatiotemporal understanding.}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


\section{Introduction}
On-demand ride-hailing services like DiDi, Uber, and Lyft have significantly transformed urban transportation systems and human mobility patterns \cite{xu2018large}. 
These platforms have introduced a paradigm shift in how people access transportation, offering unparalleled convenience, efficiency, and flexibility \cite{feng2021we, liu2022machine}. 
As one of the largest ride-hailing platforms, the DiDi mobile application processes over ten billion user requests daily, encompassing tasks such as destination selection, travel time estimation, ride-hailing, re-routing, and feedback. 
These interactions span all stages of the ride-hailing process including pre-ride, in-ride, and post-ride \cite{s2020capturing}. Despite their transformative impact, existing mobile applications still rely heavily on manual inputs like typing, clicking, and scrolling, making the process time-intensive and often inefficient \cite{hsu2023usability}.


\begin{figure}
 
    \centering
    \begin{subfigure}[b]{\linewidth}
       
        \centering
        \includegraphics[width=\linewidth]{Figure/performance_spatiotemporal_benchmarking_v2.png}
        \caption{Existing LLMs fail to meet the production accuracy requirements on travel intention understanding.}
        \label{fig:intro_a}
    \end{subfigure}

    \begin{subfigure}[b]{\linewidth}
       
        \centering
        \includegraphics[width=\linewidth]{Figure/performance_latency_dialog_generation_benchmarking_v2.png}
        \caption{Quality and latency trade-off of size-varying LLMs for response generation. Under three conversation goals, larger LLMs tend to generate better responses over a longer inference time.}
        \label{fig:intro_b}
    \end{subfigure}
    \caption{Performance of existing LLMs as a ride-hailing assistant. The results are obtained based on 500 human annotated real-world ride-hailing requests.}
    \label{fig:intro}
    
\end{figure}

Intelligent virtual assistants such as Siri, Alexa, and Google assistant have emerged as tools to streamline user interaction by providing conversational interfaces \cite{apple_siri_guidelines}.
These intelligent assistants offer voice-activated functionality for setting reminders, searching the web, or making a phone call. 
However, their capabilities fall short in managing the unique demands of ride-hailing services, from handling open-world queries to following multi-step complex instructions that interact with the real-world urban environment.
For instance, the assistant is required to understand real-time traffic and weather conditions to accurately dispatch the order and dynamically adjust it~(\eg changing destination) during the ride. 
Recent advances in Large Language Models (LLMs), such as ChatGPT \cite{gpt35} and Qwen \cite{bai2023qwen}, have introduced new possibilities for creating domain-specific intelligent assistants. 
LLMs demonstrate exceptional capabilities in understanding and processing complex natural language queries and have been adopted to construct autonomous web operation agents. 
For example, Mind2Web \cite{deng2024mind2web} and AutoWebGLM \cite{lai2024autowebglm} serve as human-like web navigation assistants, while LLMPA \cite{guan2023intelligent} automates multi-step travel planning tasks in Alipay. Inspired by these attempts, we propose constructing an LLM-powered assistant to enable natural, conversational ride-hailing for the DiDi application.


However, building an LLM-based ride-hailing assistant is a non-trivial task due to the following three challenges.
\emph{(1) Spatiotemporal Travel Intention Understanding.} The ride-hailing process relies heavily on understanding the spatiotemporal trip information such as destination and departure time. 
LLMs often struggle with spatial reasoning \cite{xie2024travelplanner} and temporal calculations \cite{su2024timo}. 
For instance, when issuing a query shown in Figure \ref{fig:product}, general LLMs may fail to derive the exact date of next Friday and neglect the user's current location as the start location.
As reported in Figure \ref{fig:intro}(a), our quantitative studies validate that even state-of-the-art LLMs like GPT-4o and Qwen2.5-72B achieve less than 80\% accuracy in spatiotemporal travel intention understanding, therefore limiting their applicability for handling real-world ride-hailing requests. 
\emph{(2) Proactive Order Planning.} Ride-hailing involves multiple factors (\eg waiting time, willing car type, destination) for matching passengers and drivers, which may require multi-turn conversations. 
Moreover, during a conversation, users may cancel confirmed orders, request orders with infeasible conditions (\eg unrealistically low costs), or engage in ride-hailing policy inquiry. 
How to proactively and naturally guide users through the order planning process also poses a significant challenge.
\emph{(3) Cost-Effective Response Generation.} The assistant should generate responses adapted to varying conversation goals and order statuses. 
For instance, guiding users to place an order often requires formulaic language responses, handling infeasible request needs clarification, whereas answering ride-hailing policy queries may involve retrieving DiDi’s knowledge base.
As shown in Figure \ref{fig:intro}(b), in general, larger LLMs tend to be more versatile to handle complex scenarios but result in higher response latency. Generating accurate responses within strict service latency is another critical challenge.




To address the aforementioned challenges, we propose \textbf{DiMA}, an LLM-based intelligent assistant for ride-hailing at DiDi.
% As shown in Figure \ref{fig:pipeline}, as its core, 
As its core, we first propose a \emph{Spatiotemporal-aware Tool-augmented Order Planning} module to enhance LLMs’ ability to interpret spatiotemporal queries by leveraging external tools for precise location mapping and temporal reasoning. By injecting domain knowledge and utilizing tools like time calculators and geographic databases, DiMA accurately extracts travel intentions (\eg start and end locations and preferred departure times), facilitating progressive order planning.
Moreover, we construct a \emph{Cost-effective Dialog System} to handle diverse conversation goals in a cost-effective way. By proactively guiding users through multi-type dialog repliers, the dialog system ensures seamless order planning and addresses diverse scenarios, from complex order planning to ride-hailing policy answering. The system further adopts a cost-aware LLM configuration, which adaptively allocates smaller models for simpler queries and larger models for complex scenarios, ensuring high-quality responses while meeting strict production latency requirements.
Finally, we propose a \emph{Continual Fine-tuning} scheme to align the assistant with human decision processes. By continuously collecting real-world user interactions and augmenting user decision trajectories from a constructed role-playing simulation environment, we periodically fine-tune backbone LLMs to overcome cold start problem and align the assistant behaviors with shifting user preferences, thereby enabling proactive and natural order planning guidance.



Since May 2024, DiMA has been deployed in the DiDi application, as depicted in Figure \ref{fig:product}. 
Two months’ online experiments in production demonstrate that DiMA achieves 93\% accuracy in order planning and 92\% in response generation. 
We further conduct offline experiments on synthetic datasets in Beijing and Shanghai. The results validate that DiMA significantly outperforms three state-of-the-art agent frameworks on order planning and response generation tasks, with improvements up to 70.23\% and 321.27\%, respectively.
Moreover, the results show that DiMA maintains comparable performance to models like Qwen2.5-72B, Mistral-123B, and Llama3.1-405B, while reducing latency by 0.72x to 5.47x.




Our contributions are summarized as follows:
\begin{itemize}
    \item We propose the first LLM-based assistant deployed in DiDi mobile application, offering a new interaction alternative for ride-hailing.
    \item We develop a spatiotemporal-aware tool-augmented order planning module and a cost-effective dialogue system to enhance accuracy and efficiency.
    \item We introduce a continual fine-tuning scheme to ensure sustained proactive order planning guidance aligned with human decision processes.
    \item Extensive experiments validate DiMA’s effectiveness and demonstrate its advantages against state-of-the-art LLM agent frameworks in both online and offline settings.
\end{itemize}



\section{Preliminaries}
In this section, we first present the formal problem definition, and then briefly introduce the evolution of DiDi’s ride-hailing service.

\subsection{Definition and Problem Statement}
We begin with the definition of trip order and ride-hailing conversation, then define the problem we aim to address.

\begin{myDef}
\textbf{Trip order.}
A trip order is defined as a 9-tuple $o = \left (l_{s}, l_{v}, l_{e}, t_{d}, t_{a}, dist, dur, c, p\right )$, where $l_{s}$, $l_{v}$ and $l_{e}$ denote the start location, via location and end location of the trip, $t_{d}$ and $t_{a}$ is the trip departure time and estimated arrival time. In addition, $dist$ and $dur$ denote the distance and estimated duration of the trip, $c$ is the car type, and $p$ is the expected price of this trip.
\end{myDef}

Note that the start location $l_{s}$, via location $l_{v}$, and end location $l_{e}$ usually corresponds to Point-of-Interests (POIs) associated with geographical coordinates. 
Figure \ref{fig:product}(b) illustrates a trip order example.

\begin{myDef}
\textbf{Ride-hailing conversation.}
Let $q_i$ and $r_i$ denote the user query and assistant response at round $i$,
a conversation $d$ is defined as a sequence of queries and responses $\{q_{1}, r_{1}, q_{2}, r_{2}, ...\}$.
In the context of ride-hailing, a query $q_i$ can be a trip order request, a policy inquiry, or even a chit-chat. 
$r_i$ can be responses proactively guiding the user to provide specific instructions, \eg providing preferences, confirming the order, and re-routing, or also be responses answering questions, providing travel suggestions, etc.
% Per conversation goals, $r_i$ can also be used to answer questions, provide travel suggestions, etc.

\end{myDef}

Note in a conversation, a trip order $o$ can be completed by multi-turn user queries. 
Figure \ref{fig:product}(a) show a conversation example.

%Note that under the conversational interface, a trip request can be one or multiple user queries.
%$q_i$ can also be a policy inquiry or even a chit-chat.
%With the above notations and definitions, we now define the problem we aim to address.
%We use $q$ to denote the user query.

\eat{
\begin{pro}
Given a user query $q_{i}$ at round $i$, we aim to create the trip order $o_{i}$ and provide corresponding response $r_{i}$ based on history dialog $\mathcal{H} = 
\left \{  q_{1}, r_{1}, q_{2}, r_{2}, ..., q_{i-1}, r_{i-1} \right \}$, which is a set of historical user queries and assistant responses from round $1$ to round $i-1$.
\end{pro}
}
%This work focuses on constructing a mobile assistant for autonomous ride-hailing via a conversational interface.

\begin{pro}
We aim to construct an end-to-end mobile assistant for seamless ride-hailing services through a conversational interface, i.e. ride-hailing conversation.
%that continuously generates mult-turn responses based on user queries. 
As its core, the assistant automatically guides the users to provide the required spatiotemporal trip information, reasoning about user travel intentions under real-world urban contexts, and creating the trip order $o$ to satisfy the trip requirements.
 
%provide autonomous ride-hailing services that satisfying user travel intentions in dynamic and complex spatiotemporal urban contexts.
%(1) automatically plan a feasible order that matches the user with available drivers to complete the trip, and (2) generate proactive and natural responses 
\end{pro}



\subsection{The Ride-Hailing Service at DiDi}


The ride-hailing service at DiDi continuously matches passenger requests with available drivers \cite{xu2018large}. The service can be structured into three key stages: pre-ride, in-ride, and post-ride \cite{s2020capturing}.

In the pre-ride stage, users interact with the application by setting their origin and destination, scheduling the departure time, and selecting their preferred car type. The app then provides essential details, such as estimated arrival times \cite{liu2022machine, liu2023uncertainty} and expected prices. Once the user confirms the order, the process transitions into the in-ride stage, where users can make route changes, update prices, and access additional ride details. Finally, in the post-ride stage, users complete payment and are encouraged to provide feedback, which helps improve service quality.

Despite the diverse goals of the ride-hailing service, interactions primarily rely on manual inputs such as typing, clicking, and scrolling, which are time-consuming and inefficient. To enhance the user experience, DiDi has tried integrating Task-oriented Dialogue (ToD) systems \cite{qin2023end}, which simplify the ride-hailing process through natural language interaction. However, traditional ToD methods \cite{hosseini2020simple, he2022galaxy} are typically designed for fixed tasks, such as reservations or question answering, which often fall short of understanding the spatiotemporal travel context and conduct open-world reasoning. These limitations make ToD an impractical solution for real-world ride-hailing scenarios where user intents are often dynamic. For instance, users may wish to change destinations in-ride, inquire about estimated arrival times, or engage in casual chit-chat.

Based on the above observations and attempts, next we introduce our proposed LLM-based ride-hailing assistant framework, designed to handle dynamic user interactions and adapt to the complexities of real-world ride-hailing scenarios.


\begin{figure*}
   
    \centering
    \includegraphics[width=1 \linewidth]{Figure/Figure3_v3.png}
    \caption{An overview of DiMA.
    }
    \label{fig:method}

\end{figure*}

\section{Framework Overview}

Figure \ref{fig:method} shows the framework of DiMA, which is comprised of three major components, \emph{Spatiotemporal-aware Tool-augmented Order Planning}, \emph{Cost-effective Dialog System} and \emph{Continual Fine-tuning}.
Given an online user query, \emph{Spatiotemporal-aware Tool-augmented Order Planning} first conducts spatial mapping and temporal reasoning to extract travel intention (\ie start, via and end location, departure and arrival time), and then executes progressive order planning to obtain order information.
Moreover, the \emph{Cost-effective Dialog System} constructs multi-type dialog replier to handle diverse conversation goals.
Based a cost-aware LLM configuration, the system adaptively allocate smaller models for simpler queries and larger models for complex queries, improving the response quality and reduces the latency.
%performance could be improved, and online inference latency could be reduced.
Besides, the \emph{Continual Fine-tuning} schema periodically fine-tunes models using real-world ride-hailing data from the mobile application together with instructions acquired from the constructed role-playing simulator to align the assistant behavior with the user decision-making process.


\section{Spatiotemporal-Aware Tool-Augmented Order Planning}


One key objective of DiMA is to achieve accurate order planning.
Over time, DiDi engineers have developed a comprehensive suite of spatial and temporal tools that encapsulate extensive knowledge and operational insights, which can be leveraged to enhance the assistant’s spatiotemporal reasoning and multi-step planning capabilities. 
In this section, we introduce the \emph{Spatiotemporal-aware Tool-augmented Order Planning} module, including spatial mapping, temporal reasoning, and progressive order planning.
% We begin with spatial mapping.

\subsection{Spatial Mapping}


In ride-hailing scenario, user queries usually contain crucial spatial travel information, which is essential for accurate order planning. As reported in Figure \ref{fig:intro}(a), general LLMs lack specialized capabilities to effectively extract and interpret spatial travel intention. To address this limitation, we enable DiMA to learn from and utilize spatial tools through structured function calls~\cite{li2024large}.

Specifically, we establish a structured pipeline encompassing four key stages: 
(1) Spatial tool collection and annotation. 
We first collect tool APIs from DiDi’s code base that related to spatial information processing and analysis, such as \emph{POI\_search()} and \emph{Get\_current\_location()}. 
% We first collect tool APIs from DiDi’s code base that related to spatial information processing and analysis, such as \emph{POI\_search()}, \emph{get\_coordinates()}, and \emph{get\_pickup\_location()}. 
These APIs are carefully annotated by engineers, including metadata such as name, description, required parameters, optional parameters, and expected outputs. 
A pool of geographic database tool is maintained for spatial mapping.
(2) Spatial decomposition. 
% Based on historical ride-hailing interactions and operational needs, we define a set of structured spatial tasks, such as extracting start and end location from the input, completing missing locations, and mapping locations to POIs. 
% The entire process is structured as a sequence of tool interactions.
Based on historical ride-hailing interactions and operational needs, we define a set of structured spatial tasks, such as extracting start and end location from the user query, completing missing locations. 
The entire process is structured as a sequence of tool interactions.
(3) Tool selection. Given a user query, DiMA first classifies the required process and dynamically selects required tool from the pool based on spatial annotations.
(4) Tool calling. Once the appropriate tools are selected, DiMA generates structured function calls \cite{li2024large} by extracting and completing key parameters. 


For example, given the user query shown in Figure \ref{fig:product}, where neither the start nor end location is explicitly specified.
DiMA defaults to treating the mentioned location as the destination, and automatically generate structured function call \emph{POI\_search("Terminal T2 of Guangzhou Baiyuan international airport")} to determine the destination and using \emph{Get\_current\_location()} to infer the starting location. 
% After that, the system employs corresponding geographic database retrieval tools to map the extracted spatial intention to a real-world POI.
After calling corresponding geographic database tools, the extracted spatial intention is mapped to a real-world POI, \eg [\emph{"Terminal T2 of Guangzhou Baiyuan international airport", 23.3672, 113.2998, 2506217808}].
Finally, we derive the mapped start/via/end locations, which will be passed for order planning.



\subsection{Temporal Reasoning}



Besides spatial mapping, ride-hailing interactions also require accurate temporal reasoning, \eg understanding of departure and arrival times. As illustrated in Figure \ref{fig:intro}(a), LLMs struggle with accurate date and time interpretation, achieving less than 80\% and 60\% accuracy on departure and arrival time understanding, respectively.

Similar to spatial mapping, we construct a four stage pipeline: (1) Temporal tool collection and annotation, (2) Temporal decomposition. (3) Tool selection, and (4) Tool calling.
The whole process is similar with spatial mapping.
Take the user query in Figure \ref{fig:product} again for example.
Since the departure time is not explicitly specified, we follow product design to threat mentioned time as departure time.
Therefore, DiMA automatically generate structured function call \emph{Get\_departure\_time()} with key parameters \{\emph{"date": "1 week later, Fri", "time": "18:00:00"}\} to obtain user's departure time.
Then, based on the issued date \emph{"2024-08-28"}, DiMA calls annotated external time computation tool to transform this information into the exact datetime [\emph{"2024-09-06 18:00:00"}].


\subsection{Progressive Order Planning}
\label{Progressive Order Planning}


With the extracted spatiotemporal intentions, DiMA executes progressive order planning through tool-assisted decision-making.
% Intuitively, we can straightforwardly use the extracted information to create ride-hailing order.
However, ride-hailing interactions often require multi-turn conversations, where each round corresponds to refining or supplementing spatiotemporal intentions. 
Additionally, ride-hailing requests encompass diverse conversation goals, including order creation, modification, confirmation, cancellation, ride-hailing policy inquiries, or casual chit-chat. 
Recognizing and responding dynamically to multi-turn user interactions is critical for ensuring a seamless and responsive ride-hailing experience.

To achieve this, we formulate the order planning process as structured functions (\eg \emph{Order\_create()} and \emph{Order\_cancel()}), which takes multiple spatial and temporal input parameters. 
The assistant learns to invoke these functions by proactively seeking missing input values—such as start/via/end locations and departure/arrival times—through multi-turn dialog. 
For instance, in the illustrative example, executing tools like \emph{POI\_search()} results in structured travel intentions, including extracted start and end locations. 
This information is subsequently processed via \emph{Route\_planning()} to retrieve estimated trip distance, duration, vehicle type options, and pricing details. 
Once all parameters are determined, the assistant invokes \emph{Order\_create()} to obtain the order information.

Furthermore, we construct illustrative spatiotemporal descriptions for various conversation goals and integrate them into the LLM prompt. 
For example, during policy inquiries, users typically do not specify structured spatial or temporal information, distinguishing these interactions from explicit ride-hailing requests. 
By leveraging illustrative description guidance, the assistant enhances its instruction-following ability to execute complex order planning, such as confirming, modification, or canceling an order.

In this way, the assistant ensures reliable spatiotemporal interpretation and order planning, making it well-suited for real-world ride-hailing scenarios. Please refer to Appendix \ref{appendix task planner} and \ref{appendix: apis and tools} for more detailed prompt template and tool information.




\section{Cost-Effective Dialog System}


Then, we present the \emph{Cost-effective Dialog System}, including multi-type replier construction and cost-aware LLM configuration, designed to optimize assistant responses to fulfill the production requirements. 
These components enable the dialog system to generate high-quality responses with reduced latency while adapting to various conversation goals, such as order creation, modification, policy inquiries, and casual conversations.

\subsection{Multi-Type Replier Construction}

In online serving, the response generation policy plays a crucial role in determining how the assistant interacts with users~\cite{young2013pomdp}.
%As illustrated in Figure \ref{fig:intro}(b), conversation goals vary based on user intent and order information. To address this, 
Following DiDi's product design, we decompose dialog policy into three distinct types and construct corresponding LLM repliers.
\textbf{(1)~Specialized replier:} This replier is designed for cases where the order can be properly created with alignment of the user’s intention. 
It follows a structured, formulaic~\cite{wray2012we} response style to proactively guide the users' action through ride-hailing, such as prompting them to confirm the created order. This replier is predominantly used for order planning tasks.
\textbf{(2)~Error-handling replier:} 
Not all user requests can be satisfied.
Handling infeasible or conflicted user requests and accidental system errors are essential for maintaining a smooth user experience \cite{bohus2005error}. 
This replier intervenes when an order request cannot be fulfilled or when incorrect information is detected. 
Despite rarely invoked in practice, it generates clarification messages or apologetic responses to help users refine their requests and prevent frustration.
\textbf{(3)~Knowledge-enhanced replier:} In cases where users inquire about company policies or engage in casual conversations (\eg ``How many car types can I book now?'' and ``What is the traffic restriction policy today?''), this replier retrieves relevant information from DiDi’s dynamically maintained knowledge base~\cite{gao2023retrieval}. Retrieved question-answer pairs are appended to the LLM prompt, ensuring the response is factually accurate and contextually relevant.

In practical implementation, we provide the description of each dialog policy in the prompt to guide DiMA select the most suitable replier for response generation.
To enhance response accuracy, for each dialog replier, we provide task descriptions along with demonstration examples in the prompt. 
Detailed prompt structures can be found in Appendix \ref{appendix response router} to Appendix \ref{appendix retrieval-augmented response generation pipeline} .




\subsection{Cost-Aware LLM Configuration}
Given the selected dialog replier, using an appropriate language model for response generation requires balancing computational efficiency and performance. A naive approach would always be to employ a high-capacity LLM (\eg Qwen2-72B) to ensure ride-hailing response quality. However, this approach is impractical due to system budget and latency considerations~\cite{sarikaya2016overview, padmanabha2018mitigating}. Instead, we propose a cost-aware LLM configuration strategy.
%, formulating the problem as an optimization task.

%(\eg order creation, modification, policy inquiries, and casual conversations)
Our key insight is that for each utterance, some dialog goals~(\eg policy answering) are simple and can be handled by smaller, generalist LLMs, whereas others~(\eg error-handling) require customized reasoning capabilities of larger models. Along this line, we first construct an LLM pool containing both high-latency but strong LLMs and low-latency but weaker ones. 
Following~\cite{bansal2024smaller, chen2024self}, we carefully measure the cost-effectiveness of each language model variant for each replier and choose the best configuration (\ie, model family and size) for response generation. More implementation details will be discussed in Section~\ref{deployment}. Through these optimized replier customizations, we adaptively allocate LLM resources based on the given dialog policy. This approach ensures high system throughput while preserving response quality. 





\section{Continual Fine-Tuning}
\label{Continual Fine-tuning}


This section details \emph{Continual Fine-tuning}, aiming to align assistant behaviors with user preferences, enabling proactive and natural ride-hailing guidance and beyond.
By collecting real-world user dialogs and synthesizing dialogs based on multi-turn user interaction trajectories through role-playing simulation, we continuously fine-tune the model to improve the assistant effectiveness.



\subsection{Data Collection and Augmentation}
\label{role-playing environment}
We collect real-world ride-hailing dialogs from online user requests, which are periodically stored in a MySQL database. Each record corresponds to a multi-turn conversation between a user and the ride-hailing assistant, \eg \emph{<user: Hi, book a ride to Tsinghua for me; assistant: Sure; user: ...; assistant: ...>}.

Since real-world ride-hailing interactions were initially scarce and lacked sufficient scenario coverage, we further augment the dataset using a role-playing simulator. 
% Specifically, we first sample history trip orders $\mathbf{O}=\{o_1, o_2, \dots \}$ and conversations ${d}=\{q_1, r_1, q_2, q_2, \dots \}$ from DiDi's application database. 
Specifically, we first sample historical trip orders $\mathbf{O}=\{o_1, o_2, \dots \}$ and ride-hailing conversations $\mathbf{D}=\{d_1, d_2, \dots \}$ from DiDi's application database. 
Each record is associated with a user profile (\eg age, sex, occupation). To protect user privacy and enhance dataset diversity, user profiles are synthetically generated based on a predefined schema rather than real-world user information.
Using the above information, we employ GPT-4o to simulate both passenger and ride-hailing assistant roles, generating structured multi-turn dialogs that mimic real-world ride-hailing interactions. For example, a generated conversation might include: \emph{<user: Hi, book a ride to Tsinghua for me; assistant: Sure; user: ...; assistant: ...>}.
To ensure the quality of simulated dialogs, we employ a GPT-based evaluation mechanism \cite{zheng2023judging} to filter out unfaithful or low-quality samples before constructing the final instruction tuning dataset. 
The detailed prompt templates used for dialog role-playing simulation and instruction dataset statistics are provided in Appendix \ref{simulation}.

\subsection{Continuous Model Tuning}
Since real-world ride-hailing dialogs are continuously collected from the online application and unexpected failure cases may emerge over time, DiMA employs an automated fine-tuning pipeline to iteratively optimize the assistant. 
Let $\mathcal{D}_{1}$, $\mathcal{D}_{2}$, ..., $\mathcal{D}_{t}$ represent the mixed instruction sets constructed above, where the subscript indicates different time steps, and let $\mathcal{M}$ denote the backbone language model. The probability distribution of response $y$ given instruction $x$ is represented as $\mathcal{P}_{\mathcal{M}}\left ( y \mid x \right ) $. 
The continual tuning process is formulated as minimizing the time-evolving loss:

\begin{equation}
\label{fine-tune}
\begin{aligned}
\mathcal{L}_{t} = \mathbb{E}_{(x, y) \sim \mathcal{D}{t}}\left[\log \mathcal{P}_{\mathcal{M}}(y \mid x)\right],
\end{aligned}
\end{equation}
where $\mathcal{L}_{t}$ represents the loss function at a given time step $t$. We utilize the Low-Rank Adaptation (LoRA)~\cite{hu2021lora} to efficiently fine-tune DiMA with minimal computational overhead. This process allows DiMA to evolve dynamically, ensuring it remains aligned with the latest user interactions and ride-hailing trends.


\section{System Deployment}
\label{deployment}
Since May 2024, DiMA has launched on the DiDi mobile application to provide intelligent ride-hailing assistant service.
In this section, we present the deployment experience of DiMA, including the model selection and adaptation mechanism, asynchronous model update, and online serving strategy.



\textbf{Model selection and adaptation.}
The order planning and response generation module requires size-varying LLMs to achieve cost-effective ride-hailing service.
Specifically, we fine-tune Qwen2-72B and Qwen2-7B for spatiotemporal-aware tool-augmented order planning and multi-type dialog replier selection.
For response generation, we fine-tune Qwen1.5-32B for the specialized replier.
Besides, we observe fine-tuning leads to performance degradation for both error-handling generation and knowledge retrieval. Therefore, we choose vanilla Qwen-2-72B and Qwen-1.5-32B for error-handling and knowledge-enhanced repliers.
The above configuration guarantees the effectiveness and efficiency of DiMA cost-effectively.



\textbf{Asynchronous model update.}
As described in Section~\ref{Continual Fine-tuning}, the LLMs are continuously updated to enhance the ride-hailing assistant performance.
Benefiting from the decoupled LLM configuration for each module, we devise an asynchronous model update mechanism. 
Specifically, we employ a professional annotation team to review and label ride-hailing data collected from the online environment.
Based on the identified failure cases, we periodically refine prompts and fine-tune each model.
%and enhancing response generation through role-playing dialog simulation, as described in Section \ref{Continual Fine-tuning}. 
Once product managers validate the updated model, the latest version will be pushed into the online environment.

\textbf{Online serving.}
To optimize the online inference latency, we apply Activation-aware Weight Quantization (AWQ) techniques \cite{lin2024awq} to compress and optimize LLMs for deployment.
Additionally, we integrate the Medusa acceleration framework~\cite{cai2024medusa} to efficiently deploy LLMs of different sizes: a 72B-size model on 4 NVIDIA H800s, a 32B-size model on 2 NVIDIA H800s, and a 7B-size model on a single NVIDIA H800. This deployment strategy ensures optimal resource utilization while maintaining high responsiveness.
By incorporating these techniques, we successfully reduce DiMA’s average response latency within 4 seconds, ensuring both efficiency and scalability in real-world ride-hailing applications.




\section{Experiments}
We conduct extensive experiments to evaluate the proposed method in both of online and offline environment.
We aim to answer the following research questions:
\textbf{RQ1:} How does DiMA perform in the online environment?
\textbf{RQ2:} How does DiMA perform compared with existing agent frameworks?
\textbf{RQ3:} How do different components affect the assistant's performance?
\textbf{RQ4:} How about the cost-effectiveness of DiMA?


\begin{table}[]
\caption{The statistics of online dataset, and two offline datasets in the role-playing dialog simulation environment.}
\label{dataset}
\begin{tabular}{c|ccc}
\midrule
\multirow{2}{*}{Data type}  & \multirow{2}{*}{Online} & \multicolumn{2}{c}{Offline} \\
                            &                         & Beijing       & Shanghai      \\ \midrule
Time span                   & \multicolumn{3}{c}{2024/05/06 - 2024/08/24}             \\
\# of training user queries & \multicolumn{3}{c}{2,086}                               \\
\# of testing users                 & 3,624 &    272 &  168                         \\
\# of testing user queries  & 10,153                  & 1,139         & 687           \\ 
\# of average dialog rounds  & 2.82                  & 4.19        & 4.08           \\ \midrule
\end{tabular}
\end{table}


\begin{table*}[]
\caption{Online experimental results on DiDi Chuxing ride-hailing platform.}
\label{online results}
\scalebox{0.85}{
\begin{tabular}{cc|cccc|cccc}
\midrule
\multicolumn{1}{c|}{\multirow{2}{*}{User query types}} & \multirow{2}{*}{Percentage} & \multicolumn{4}{c|}{Human Evaluation($\%$)} & \multicolumn{4}{c}{GPT Evaluation($\%$)} \\
\multicolumn{1}{c|}{}                                 &                             & ROA       & RRA       & SOA      & SRA      & ROA      & RRA      & SOA      & SRA     \\ \midrule
\multicolumn{1}{c|}{Order / modify / confirm / cancel order} & 89.26\%                     & 93.48     & 91.34     & 88.15    & 85.37    & 86.63    & 94.09    & 72.09    & 86.84   \\
\multicolumn{1}{c|}{Policy inquiries}             & 2.07\%                      & 90.02     & 97.31     & 88.89    & 94.32    & 83.76    & 98.18    & 78.36    & 95.52   \\
\multicolumn{1}{c|}{Casual chit-chat}                        & 8.67\%                      & 98.62     & 98.61     & 96.36    & 96.35    & 98.72    & 97.19    & 98.26    & 96.58   \\ \midrule
\multicolumn{2}{c|}{Overall}                                                        & 93.83     & 92.17     & 87.11    & 85.02    & 86.94    & 94.66    & 70.32    & 86.68   \\ \midrule
\end{tabular}
}
\end{table*}

\begin{table*}[]
\caption{Overall performance of order planning and response generation task on simulation environment on Beijing and Shanghai dataset. 
The best baselines performance is \underline{underlined}.}
\label{simulation results}
\scalebox{0.85}{
\begin{tabular}{cc|cccccccc|cccccccc}
\midrule
\multirow{3}{*}{Backbones} & \multirow{3}{*}{Methods} & \multicolumn{8}{c|}{Beijing}                                          & \multicolumn{8}{c}{Shanghai}                                         \\
                           &                         & \multicolumn{4}{c|}{Human Evaluation ($\%$)}                 & \multicolumn{4}{c|}{GPT Evaluation ($\%$)} & \multicolumn{4}{c|}{Human Evaluation ($\%$)}                 & \multicolumn{4}{c}{GPT Evaluation ($\%$)} \\
                           &                         & ROA & RRA & SOA & \multicolumn{1}{c|}{SRA} & ROA  & RRA & SOA  & SRA & ROA & RRA & SOA & \multicolumn{1}{c|}{SRA} & ROA  &  RRA & SOA & SRA \\ \midrule
\multirow{3}{*}{Qwen-2-72B} & CoT                    & 56.49    &76.57     &17.65     & \multicolumn{1}{c|}{45.59}    &  61.24    &  50.02    &  14.26    & 6.62   & 50.75    &  68.66   & 14.29    & \multicolumn{1}{c|}{42.86}    &  59.12     & 48.14    &  13.81   &  8.33  \\
                           & ReAct                  &  51.93   &  77.25   & 14.71    & \multicolumn{1}{c|}{45.59}    & 63.26    &  51.44  &  16.47    & 6.99   &  48.28   & 71.72    & 23.81    & \multicolumn{1}{c|}{51.06}    &  59.93    &  48.19   &  13.64   & 7.74  \\
                           & Reflexion              & 49.59    & 70.97    & 10.29    & \multicolumn{1}{c|}{42.65}    & 66.07     & 50.99    &    18.39  & 11.03    &  51.72   &   78.62  & 11.90    & \multicolumn{1}{c|}{42.86}    &  61.29    & 54.87    & 22.81    & 12.52                             
                  \\ \midrule
\multirow{3}{*}{GPT-4o} & CoT                    & 76.19    & 85.71    & 26.71    & \multicolumn{1}{c|}{70.59}    & 66.21     &  54.69    &  23.89    &  9.93   &  74.67   &  80.67   &  19.05   & \multicolumn{1}{c|}{63.81}    &  61.95    &  52.85    &  14.88   &  9.52   \\
                           & ReAct                  & 73.78    &  \underline{86.67}   &  17.65   & \multicolumn{1}{c|}{72.09}    &  66.91    &  56.43    &  17.27    &  13.97   &  75.27   &  \underline{82.95}   &  25.93   & \multicolumn{1}{c|}{62.96}    & 62.69     &  52.79    & 11.82    &  10.04   \\
                           & Reflexion              & \underline{76.52}    & 83.74  & \underline{27.94}  & \multicolumn{1}{c|}{\underline{75.12}}    &  \underline{67.05}    &  \underline{64.32}     &   \underline{34.56}     &  \underline{22.43}   & \underline{77.59}    & 79.31    &  \underline{26.28}   & \multicolumn{1}{c|}{\underline{75.07}}    &  \underline{63.01}    & \underline{63.73}     & \underline{27.97}    &   \underline{23.21}       
                           \\ \midrule
\multicolumn{2}{c|}{\textbf{DiMA}}   & \textbf{97.96}    & \textbf{96.94}   & \textbf{94.11}    & \multicolumn{1}{c|}{\textbf{91.18}}    &  \textbf{84.05}    & \textbf{97.89}     &  \textbf{58.83}    & \textbf{94.49}    & \textbf{98.82}   & \textbf{99.19}    &  \textbf{96.24}   & \multicolumn{1}{c|}{\textbf{97.62}}    &    \textbf{84.34}  & \textbf{97.36}     &  \textbf{63.69}   &  \textbf{93.45}  
\\ \midrule

\end{tabular}

}
\end{table*}



\subsection{Experimental Setup}
\subsubsection{Data Description}


We collect data from both the online environment and the role-playing dialog simulator constructed in section \ref{role-playing environment}. All datasets are collected from DiDi’s ride-hailing platform, covering the period from May 06, 2024, to August 24, 2024.
For both online and simulation-based evaluations, we utilize data from May 06, 2024, to May 24, 2024, as the training set for constructing the ride-hailing assistant. We select data from May 25, 2024, to July 25, 2024, as the testing set for the online evaluation. 
For offline evaluation, we use data from July 26, 2024, to August 09, 2024, to synthesize the testing set for Shanghai, and data from August 10, 2024, to August 24, 2024, to synthesize the testing set for Beijing.
The statistics of the three datasets are reported in Table~\ref{dataset}.



\subsubsection{Metrics}
We use four metrics for evaluation.
For the order planning task, we use round-level order accuracy (ROA) and session-level order accuracy (SOA) for evaluation. 
In particular, the ROA is defined as the ratio of dialog round with accurate order information, which satisfy user's request at current round.
The SOA measures the ratio of dialog sessions in which the order information is accurate for every dialog round throughout the entire session.
Similarly, we construct round-level response accuracy (RRA) and session-level response accuracy (SRA) for the response generation task.


\subsubsection{Baselines}
We compare our proposed framework with the following prevailing agent frameworks, including 
CoT \cite{wei2022chain} which introduces reasoning chain to enhance LLM's reasoning capacity, ReAct \cite{yao2023react} which synergizes reasoning and acting of LLM to solve given task, and Reflexion \cite{shinn2024reflexion} which is a multi-agent framework utilizing a self-reflection mechanism to iteratively enhance LLM's decision-making ability.
We select Qwen-2-72B and GPT-4o as the backbone of the above agent frameworks, aiming to demonstrate the effectiveness of DiMA against current state-of-the-art agent frameworks for ride-hailing. 



\subsubsection{Evaluation Protocol}
In this work, we use both human and GPT evaluation.

\textbf{Human Evaluation.}
We employ an annotation team including 7 experts, each of them has a bachelor's degree or above in an IT-related field and possess work experience as an algorithm engineer at AI companies.
Before engaging in annotation work, they received more than one week of ride-hailing annotation training.
This training helps them how to learn to evaluate the correctness of trip orders and assistant responses according to serious product requirements.
We provide each annotator with an average hourly wage of 7.25 dollars.
Each record will be annotated by an annotator, and then another quality inspector will double-check whether the labeling is correct.

\textbf{GPT Evaluation.}
We also propose an LLM-based evaluation method, which is widely adopted in many recent studies \cite{zheng2023judging}.
In this work, we use GPT-4o as the evaluator.
For the trip order, we prompt GPT-4o to judge if the current order information is TRUE or FALSE.
For the assistant response, we prompt GPT-4o to provide a score (from 0 to 5), and the response with a score of more than 4 will be considered as TRUE.
The GPT evaluation method has also been used for annotation team to make a pre-labeling.

To reduce labor cost, we use GPT-4o to evaluate the model performance on the entire dataset and randomly sample 25\% data for human evaluation.
% Experimental results demonstrate that the GPT evaluation method is consistent with the human evaluation.
We provide more details in Appendix \ref{Evaluation of DiMA}.



\subsection{Online Test (RQ1)}
Table \ref{online results} presents the overall result of the proposed method across four evaluation metrics under six types of user conversation goals.
Under the human evaluation, it can be observed that DiMA achieves (93.83\%, 92.17\%, 87.11\%, 85.02\%) accuracy on round-level order creation, round-level response generation, session-level order creation, session-level response generation, respectively.
The GPT evaluation results (86.94\%, 94.66\%, 70.32\%, 86.68\%) also show the high accuracy of the constructed ride-hailing assistant, demonstrating its effectiveness.
In addition, we also find that order planning query accounts for the highest proportion (89.26\%), while DiMA achieves the best performance on casual chit-chat.
The performance variations across six types of user queries also indicates that distinct conversational goals may vary in complexity.

\subsection{Offline Test (RQ2)}
We also conduct experiments on two synthetic dataset to evaluate the effectiveness of DiMA compared with state-of-the-art agent frameworks.
The performance result is reported in Table \ref{simulation results}.
As can be seen from GPT evaluation results, DiMA achieves (25.35\%, 52.19\%, 70.23\%, 321.27\%) improvement compared with the state-of-the-art baseline using ROA, RRA, SOA, and SRA in Beijing.
The improvements in Shanghai are (31.88\%, 52.77\%, 128.71\%, 302.63\%), respectively.
In addition, DiMA has achieved significant improvement through human evaluation.
Specifically, the improvement on four metric are (28.02\%, 11.85\%, 236.83\%, 213.79\%) in Beijing, and (27.36\%, 19.58\%, 266.21\%, 30.04\%) in Shanghai, respectively.
More importantly, it is worth mentioning that the state-of-the-art multi-agent framework Reflexion performs poorly on order planning and response generation tasks, despite utilizing the powerful GPT-4o LLM backbone.
Furthermore, Reflexion significantly outperforms single-agent frameworks (\ie CoT and ReAct) in response generation tasks under GPT-evaluation, while the improvement in order planning task is relatively modest.
This may suggest that the complexity of these two tasks is different from each other, and the self-reflection mechanism may only be able to improve the response generation task.
Finally, we observe that all models achieves better performance on round-level metric (ROA and RRA) but worse on session-level metric (SOA and SRA), indicating the complexity of user queries may vary between different rounds.


\begin{table}[]
\caption{Ablation study on Shanghai dataset.}
\label{ablation study results}
\scalebox{0.8}{
\begin{tabular}{c|cccc|cccc}
\midrule
\multirow{2}{*}{Model Variants} & \multicolumn{4}{c|}{Human Evaluation (\%)} & \multicolumn{4}{c}{GPT Evaluation($\%$)} \\
                                & ROA   & RRA & SOA  & SRA  & ROA  & RRA & SOA & SRA \\ \midrule
DiMA w/o-TT                     &    88.29   &  91.03    & 90.48     & 95.85     & 76.71     & 88.37    & 60.69    & 90.23    \\
DiMA w/o-CG                     &  66.32     &  67.03    & 23.28     & 40.16     &   56.24   &   78.07   &  20.24   & 53.42    \\
DiMA w/o-MR                      & -      &  92.28    & -  &   95.24  &  -    &  92.72    &   -   & 80.35      \\
DiMA w/o-CLC                     &  -     &  87.26    & - &      79.05 &   -   &   76.54   & -     & 70.13    \\ 
\midrule
\textbf{DiMA}                            &  98.82     &  99.19    &   96.24   &  97.62    &     84.34 &   97.36   &  63.69   & 93.45    \\ \midrule
\end{tabular}
}
\end{table}
\subsection{Ablation Study (RQ3)}
To validate the effectiveness of each module in DiMA, we conduct an ablation study on the Shanghai dataset using four metrics.
Specifically, we compare the following variants.
(1) DiMA w/o-TT removes the \textbf{T}ime \textbf{T}ool utilization in temporal reasoning process.
(2) DiMA w/o-CG removes designing different structured function for each \textbf{C}onversation \textbf{G}oal in progressive order planning module.
(3) DiMA w/o-MR removes the \textbf{M}ulti-type \textbf{R}eplier design.
(4) DiMA w/o-CLC removes the \textbf{C}ost-aware \textbf{L}LM \textbf{C}onfiguration, and the response generation task is randomly allocated.
As DiMA w/o-MR and DiMA w/o-CLC variants only affect response generation, we did not include their performance on the order planning.

As reported in Table \ref{ablation study results}, we obtain the following observations.
First, the time tool utilization contributes to the performance of the order planning task.
Removing it will decrease DiMA's accuracy on order planning tasks.
Second, after removing tailored function call for each conversation goal, the order accuracy decreases significantly, indicating the importance of progressive order planning module.
In addition, improper order planning will also lead to serious performance degradation on subsequent response generation tasks.
Third, we observe a performance improvement by defining three types of dialog repliers, which validates diverse response scenarios in ride-hailing assistant and the necessity of multi-type repliers.
Moreover, the cost-aware LLM configuration is very important, as we can observe significant performance degradation after removing explicit model and size allocation.

Finally, we investigate the performance variation when employing continual fine-tuning.
As can be seen in Figure \ref{fig:performance-vs-time}, DiMA achieves consistent improvement across four metric from May 9th to May 24th, demonstrating the effectiveness of continual fine-tuning technique.
The above results verify the effectiveness of important modules in DiMA.
We provide data statistics over time in Appendix \ref{simulation}, and an case study in Appendix \ref{case study} for more illustrative analysis.



\begin{figure}
    \centering
    \includegraphics[width=1 \linewidth]{Figure/performance_over_time.png}
    \caption{Comparison of performance over time. We fine-tune DiMA using collected data up to the latest date.}
    \label{fig:performance-vs-time}
    \vspace{-0.5cm}
\end{figure}

\subsection{Cost-Effectiveness Analysis (RQ4)}
We further discuss the cost-effectiveness of DiMA.
Specifically, we replace the backbone of every module in DiMA with size-varying LLMs to analyze potential variants in accuracy and latency.
As shown in Figure \ref{fig:performance-vs-latency}, we select, Llama3.1-405B, Mistral-123B \cite{Jiang2023Mistral7}, Qwen2.5-72B, Yi-1.5-34B \cite{young2024yi}, Qwen2.5-32/14/7B \cite{bai2023qwen}, Mistral-7B and QwQ-32B \cite{qwq-32b-preview} to replace the module backbones used in DiMA.
It can be observed that DiMA achieves comparable performance compared with replacing backbone with much larger LLM, while the inference latency was maintained at 3.89 seconds. 
While Qwen2.5-72B, Mistral-123B, and Llama3.1-405B could outperform DiMA by a small margin, they spend an additional amount of latency, respectively 0.72 times (3.89s->6.72s), 3.17 times (3.89s->16.21s) and 5.47 times more (3.89s->25.17s).
This renders them suboptimal for deployment in an online serving environment.
Additionally, DiMA's latency is higher than smaller models like Yi1.5-34B, Qwen2.5-14B, and Mistral-7B, but their performance across four metrics is significantly lower, reducing their competitiveness. 
Overall, the results demonstrate the advantages of DiMA's cost-aware LLM configuration mechanism in deployment.


\begin{figure}
   
    \centering
    \includegraphics[width=1 \linewidth]{Figure/performance_vs_latency.png}
    \caption{Comparison of performance and latency between DiMA and replacing the backbone of each of DiMA's modules as size-varying LLMs.
    Result is obtained on shanghai dataset.
    }
    \label{fig:performance-vs-latency}
     \vspace{-0.5cm}
\end{figure}
\section{Related Work}
\textbf{Task-oriented dialog systems.}
Task-oriented dialog (ToD) systems facilitate users in achieving specific objectives through natural language interactions, such as making reservations or inquiries \cite{qin2023end}. Numerous methodologies \cite{hosseini2020simple, he2022galaxy, yang2021ubar, hudevcek2023llms, chung2023instructtods} have been proposed to enhance ToD performance. However, existing ToD frameworks lack specialization for open-world ride-hailing scenarios. The dynamic nature of ride-hailing interactions—spanning order creation, modification, and cancellation—necessitates a tailored solution that current ToD approaches fail to provide.



\textbf{LLM-based assistants.}
The development of domain-specific intelligent assistants powered by Large Language Models (LLMs) \cite{dong2023towards} has gained significant traction in both academia and industry. LLMs enable automated planning and execution of complex industrial tasks across various domains. For instance, Mind2Web \cite{deng2024mind2web}, WebGPT \cite{gur2023real}, and AutoWebGLM \cite{lai2024autowebglm} have been introduced as LLM-powered web navigation assistants. LLMPA \cite{guan2023intelligent} serves as a virtual assistant for travel planning within the Alipay App, while Flowris \cite{sun2023flowris} and GRILLBot \cite{fischer2024grillbot} function as intelligent assistants for data management and multimodal conversation, respectively. Despite these advancements, no LLM-based assistant has been designed specifically to automate ride-hailing services, highlighting a gap in existing research.


\section{Conclusion}

In this paper, we introduced DiMA, an LLM-powered ride-hailing assistant designed to enhance user experience and operational efficiency in real-world ride-hailing scenarios. DiMA integrates multiple key components, including a spatiotemporal-aware tool-augmented order planning module, which ensures accurate ride-hailing order creation by leveraging external spatial and temporal tools. Additionally, we developed a cost-effective dialog system, which dynamically allocates response tasks to cost-aware LLM repliers, optimizing the trade-off between response quality and system latency. A continual fine-tuning scheme has been applied to align the assistant's behavior with the human decision-making process.
The online and offline evaluation demonstrated the effectiveness of DiMA's performance in handling diverse ride-hailing user requests, achieving high accuracy and efficiency. 
The assistant has been deployed on DiDi's mobile application since May 2024. 

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
\clearpage
\appendix

\section{Appendix}
\label{Appendix:NULL_APPENDIX}


\begin{table*}[]
\centering
\setlength{\abovecaptionskip}{-0.05cm}
\caption{The augmented instruction set for every module in DiMA.}
\label{instruction set statistics}
\scalebox{0.85}{
\begin{tabular}{c|cc|ccccc}
\midrule
\multirow{3}{*}{Time span} & \multirow{3}{*}{Users} & \multirow{3}{*}{User queries} & \multicolumn{5}{c}{Mixed instruction set}                                                                                                                                                                                                                                                         \\
                           &                        &                               &\begin{tabular}[c]{@{}c@{}}Order \\ planning\end{tabular} & \begin{tabular}[c]{@{}c@{}}Dialog replier \\ selection\end{tabular} & \begin{tabular}[c]{@{}c@{}}Specialized \\ replier\end{tabular} & \begin{tabular}[c]{@{}c@{}}Error-handling \\ replier\end{tabular} & \begin{tabular}[c]{@{}c@{}}Knowledge-enhanced \\ replier\end{tabular} \\ \midrule
2024/05/06 - 2024/05/09    & 160                    & 508                           & 837            & 849                                                                 & 572                                                            & 102                                                               & 115                                                                   \\
2024/05/09 - 2024/05/12    & 142                    & 438                           & 762            & 811                                                                 & 513                                                            & 76                                                                & 109                                                                   \\
2024/05/12 - 2024/05/15    & 162                    & 461                           & 771            & 906                                                                 & 522                                                            & 94                                                                & 93                                                                    \\
2024/05/15 - 2024/05/18    & 50                     & 158                           & 303            & 312                                                                 & 208                                                            & 42                                                                & 39                                                                    \\
2024/05/18 - 2024/05/21    & 67                     & 210                           & 412            & 422                                                                 & 273                                                            & 62                                                                & 45                                                                    \\
2024/05/21 - 2024/05/24    & 101                    & 311                           & 564            & 627                                                                 & 368                                                            & 82                                                                & 74                                                                    \\ \midrule
Overall                    & 682                    & 2,086                          & 3,649           & 3,927                                                                & 2,456                                                           & 458                                                               & 475                                                                   \\ \midrule
\end{tabular}
}
\end{table*}
\subsection{Simulation Environment Construction}
\label{simulation}
We provide more details here on how to use collected online data for simulation testing, including dialog content generation, GPT-based user role-playing, and the instruction set statistics information.

\subsubsection{Dialog Content Generation}
Given a collected online data, we prompt GPT-4o to generate a dialog content based on a randomly generated start location and end location, and randomly generated user profile.
Specifically, the online data is a multi-dialog content between user and ride-hailing assistant, \ie <user: Hi, book a order to Tsinghua for me, assistant: Sure, user: ..., assistant: ...>.
To save the personal privacy, any semantic location information (\eg phone number) related to the user are removed.
The randomly generated start/end location contains a address name, latitude and longitude, \eg \emph{<Dongli Garden-Side Gate, 31.1862, 121.5165>}.
The randomly generated user profile contains user's occupation and age, \eg \emph{<Lawyer, 38 years old>}.

With the above information, we prompt GPT-4o to follow the real-world multi-dialog content style to generate a simulated dialog content, and then output a ride-hailing intent and the dialog timestamp.
We provide the generated example in the Figure \ref{Dialog Content}.
\begin{figure}
\setlength{\abovecaptionskip}{-0.05cm}
    \centering
\includegraphics[width=1 \linewidth]{Figure/appendix_1.png}
    \caption{An illustrative example of generated dialog content in Shanghai dataset.}
     \label{Dialog Content}
     \vspace{-0.5cm}
\end{figure}
% \begin{figure}
% \setlength{\abovecaptionskip}{-0.05cm}
%     \centering
% \begin{lstlisting}
% Dialog Timestamp: 2024-7-28 19:00:00

% Ride-hailing Intent: The user wants to travel to the intersection of Lou Shan Guan Road and Mao Tai Road.

% Simulated Dialog Content:
% <User>: I want to go to the intersection of Lou Shan Guan Road and Mao Tai Road.
% <Assistant>: Okay, XiaoDi has set "the intersection of Lou Shan Guan Road and Mao Tai Road" as your drop-off point. The driver is expected to pick you up between 19:58 and 20:02 today, and it will take approximately 25 minutes to reach your destination.
% <User>: Will there be traffic during this time period?
% <Assistant>: Sorry, XiaoDi is unable to provide real-time traffic flow information.
% \end{lstlisting}
%     \caption{An illustrative example of generated dialog content in Shanghai dataset.}
%      \label{Dialog Content}
%      \vspace{-0.5cm}
% \end{figure}



\subsubsection{GPT-based User Role-playing}
With the generated dialog content using collected online data, we can further prompt GPT-4o to play the role of a real-world user following the dialog content. Specifically, we first provide a task profile in the prompt to guide GPT-4o in understanding the task, and then we provide the dialog content and prompt it to generate the first sentence to begin a dialog. 
In such a multi-dialog scenario, the response of the assistant will also be iteratively fed into the prompt to help GPT-4o understand if the ride-hailing intent has been completed.
We provide detailed prompt in Figure \ref{Dialog Player} to facilitate the understanding of GPT-based user role-playing.

\subsubsection{Instruction Dataset Statistics}
As can be seen in Table \ref{instruction set statistics}, we report the statistic information of mixed instruction set used for continual fine-tuning.
Overall, origin the training set (mixed instruction set from May 06 2024 to May 24 2024) contains 2,086 user queries.
After data augmentation with role-playing simulator, the instruction set is expanded, \eg the instruction set quantities of order planning and dialog replier selection is 3,649 and 3,927 respectively. 
For three types of replier, we observe that DiMA allocates specialized replier for response generation in most cases.
This resulted in a smaller instruction set for the other two repliers, \ie the instruction set quantities of error-handling replier and knowledge-enhanced replier is 458 and 475 respectively. 
The performance degradation caused by fine-tuning may also stem from this reason. 
Therefore, as discussed in Section \ref{deployment}, we did not fine-tune LLMs for error-handling replier and knowledge-enhanced replier  during actual deployment.
Moreover, we also display the instruction tuning dataset statistics (used in Figure \ref{fig:performance-vs-time}) over the time to facilitate better understanding of continual fine-tuning mechanism.


% \begin{figure}
% \setlength{\abovecaptionskip}{-0.05cm}
%     \centering
% \begin{lstlisting}
% You are a ride-hailing user (user-bot) interacting with an AI ride-hailing assistant (bot-user).

% === Role Guidance ===

% Remember that you are playing the role of a user utilizing a ride-hailing assistance service (user-bot). You need to refer to the [intent] and lead the entire conversation.

% === Special Responses ===

% If the [bot-user] response includes a [search list], select an appropriate option or address, \eg go to the first one, option (1), go to XX, where XX represents the address name from the search list.
% If the [bot-user] response includes [order information], reply with '<confirm ride>' if the order information is correct. If there is an error in the [order information], provide feedback about the error.
% If you intend to cancel an order, reply with '<cancel order>', but only when there is an existing order, and do not mention cancellation consecutively.
% If the bot-user repeatedly fails to understand, respond with '<cannot continue conversation>' and provide a reason for not being able to continue.
% Once all intents are achieved, reply with '<end conversation> confirm completion of all intents'.

% === Points to Note ===
% If the current order information indicates an extremely long distance, regardless of whether the current intent has been completed, directly respond with '<cannot continue conversation> order information abnormal'. Reiterating: if the current order information indicates an extremely long distance, regardless of whether the current intent has been completed, directly respond with '<cannot continue conversation> order information abnormal'.

% === Speaking Style ===

% Do not include too much information in one sentence; follow the style of the script but keep it concise. Express yourself very colloquially, like a real person. Good performance will earn you a $100 tip.

% === Dialog Content ===
% ...


% Let's start!!

% \end{lstlisting}
    
%     \caption{The prompt template of GPT-based user role-playing.}
%     \label{Dialog Player}
%     \vspace{-0.5cm}
% \end{figure}

\begin{figure}
\setlength{\abovecaptionskip}{-0.05cm}
    \centering
\includegraphics[width=1 \linewidth]{Figure/appendix_2.png}
    \caption{The prompt template of GPT-based user role-playing.}
     \label{Dialog Content}
     \vspace{-0.5cm}
\end{figure}


\subsection{Prompt Template of DiMA}
\label{response generation}
This section presents the prompt template of every module in DiMA to facilitate a better understanding.

\subsubsection{Spatiotemporal-aware Tool-augmented Order Planning}
\label{appendix task planner}
As mentioned in the methodology, this module conduct spatial mapping, temporal reasoning and progressive order planning.
The entire process is achieved through function call.
In this section, we provide the detailed prompt template in Figure \ref{Task planner-prompt}.
As can be seen in the demonstration in the prompt , when the user just input a address \emph{Hongyuan}, DiMA will automatically generate a set of function call, which will be sequentially executed to achieve order planning.

% \begin{figure*}
%     \centering
% \begin{lstlisting}
% # Background knowledge
% You are Didi Chuxing's intelligent travel assistant. Your task is to fill in the new travel needs of the user in the current round of dialogue into the function call parameters based on historical user inputs, function call parameters, function call results, and system responses.

% # Function Lib
% <FUNCTIONS>
% <FUNCTION>{"function_name": "Get_user_intent", "description": "Fill in 'Create_Order', 'Confirm_Order', 'Consult_Order', 'Cancel_Order', 'Consult_Company_Rule', 'Chat' according to the actual intent of the user in the parameter.", "parameters": {"name": "intent", "type": "string"}, "result": {"intent": ""}}</FUNCTION>
% <FUNCTION>{"function_name": "Get_start_location", "description": "The starting point of the user's ride. Fill in one of 'GetUserLocation', 'SearchLocation($pickupPoint)', 'SelectLocation($index,$locationName)', 'DeleteLocation($index,$locationName)', 'GetHomeLocation', 'GetWorkLocation', 'GetSchoolLocation' based on the actual needs of the user.", "parameters": {"name": "pickupPoint", "type": "string"}, "result": {"searchStatus": "Multiple" or "Single" or "SearchFailed", "candidatePickupPoints": [{"name": "", "distance": "", "index": ""}]}}</FUNCTION>
% <FUNCTION>{"function_name": "Get_via_location", "description": "A place the user needs to pass through during their ride. The format is same with Get_start_location.", "parameters": {"name": "viaPoint", "type": "string"}, "result": {"searchStatus": "Multiple" or "Single" or "SearchFailed", "candidateViaPoints": [{"name": "", "distance": "", "index": ""}]}}</FUNCTION> 
% <FUNCTION>{"function_name": "Get_end_location", "description": "A place the user needs to pass through during their ride. The format is same with Get_start_location.", "parameters": {"name": "viaPoint", "type": "string"}, "result": {"searchStatus": "Multiple" or "Single" or "SearchFailed", "candidateViaPoints": [{"name": "", "distance": "", "index": ""}]}}</FUNCTION> 
% <FUNCTION>{"function_name": "Get_departure_time", "description": "The departure time of the user's ride.", "parameters": [{"name": "currentTime", "type": "%Y-%m-%d %H:%M:%S"}, {"name": "departureDate", "type": "%Y-%m-%d" or "N weeks after week X" or "Fuzzy" or "Invalid", "restriction": "Use 'N weeks after week X' to fill the parameter only when the user explicitly mentions 'weekdays' and 'weekends'."}, {"name": "departureTime", "type": " %H:%M:%S" or "Fuzzy" or "Invalid"}], "result": {"departureTime": "%Y-%m-%d  %H:%M:%S" or "TimeFuzzy" or "TimePassed" or "TimeInvalid"}}</FUNCTION>
% <FUNCTION>{"function_name": "Get_arrival_time", "description": "The arrival time of the user's ride.", "parameters": [{"name": "currentTime", "type": "%Y-%m-%d %H:%M:%S"}, {"name": "arrivalDate", "type": "%Y-%m-%d" or "N weeks after week X" or "Fuzzy" or "Invalid", "restriction": "Use 'N weeks after week X' to fill the parameter only when the user explicitly mentions 'weekdays' and 'weekends'."}, {"name": "arrivalTime", "type": " %H:%M:%S" or "Fuzzy" or "Invalid"}], "result": {"arrivalTime": "%Y-%m-%d %H:%M:%S" or "TimeFuzzy" or "TimePassed" or "TimeInvalid"}}</FUNCTION>
% <FUNCTION>{"function_name": "Create_order", "description": "Call this function to plan the route and generate an order based on the pick-up and drop-off points when both are clearly defined. Fill in the parameters 'pickupPointIndex', 'viaPointIndex', and 'dropOffPointIndex' if applicable. When the user has specific requirements for the type of ride, price preferences for the ride, route preferences, etc., filter out orders that meet the user's needs from the historical function call results and fill in the 'orderNumberList' with the order numbers, where each string in the list comes from the number assigned to candidate orders in the historical function call results.", "parameters": [{"name": "pickupPointIndex", "type": "string"}, {"name": "viaPointIndex", "type": "string"}, {"name": "dropOffPointIndex", "type": "string"}, {"name": "orderNumberList", "type": "list of strings"}], "callRestriction": "This function can be called when the user has clearly selected a drop-off point, or when the search status for the pick-up and drop-off points in the historical conversation function call results is 'Single'.", "result": {"orderStatus": "Multiple" or "Single", "candidateOrders": [{"orderNumber": "", "orderDuration": "", "orderDistance": "", "tripTag": "", "orderCarType": "", "orderQueueNumber": "", "orderPrice": ""}]}}</FUNCTION> 
% </FUNCTIONS>

% # Demonstration 1
% Round 1: Current Time: 2023-06-17 14:14:10
% User Input: Hongyuan.
% Function Call: {"function_call_parameters": [{"function_name": "Get_user_intent", "parameters": {"intent": "Create_Order"}}, {"function_name": "Get_start_location", "parameters": {"pickupPoint": "GetUserLocation"}}, {"function_name": "Get_end_location", "parameters": {"dropOffPoint": "SearchLocation(Hongyuan)"}}]}
% # Dmonstration 2
% ...

% # Function Call Restrictions
% Following this, I will provide you with the current time of the user's historical dialog, user input, function calls, and system responses. Please convert the user's current round of dialogue into function call parameters without filling in function call results and system responses. Please note the following function call restrictions:
% (1) If the user only describes the departure time, then transform the user's travel time demand into a function call for departure time...
% (2) If the user only describes the pick-up point, for example, 'depart from Peking University', then transform the user's pick-up point demand into a function call for the pick-up point...
% (3) If the user only describes the date in their time demand and not the exact time, such as 'leave for Tsinghua tomorrow morning', fill in 'Fuzzy' in the time parameter and use '%Y-%m-%d' to fill in the date parameter....
% (4) Special cases: When the user says something like 'don't need to arrive at a certain time' or 'cancel the trip at 15:00 PM'...
% (5) For the parameter 'N weeks after week X' in time-related functions, use 'N weeks after week X' to fill the parameter only when the user explicitly mentions 'which weekday' or 'which day of the week'...

% # Output Format
% Following the example in demonstrations to return the function call parameters in the JSON format without any explanation.

% \end{lstlisting}
    
%     \caption{Prompt template of spatiotemporal-aware tool-augmented order planning.}
%     \label{Task planner-prompt}
% \end{figure*}


\begin{figure*}
    \centering
\includegraphics[width=1 \linewidth]{Figure/appendix_3.png}
    \caption{Prompt template of spatiotemporal-aware tool-augmented order planning.}
    \label{Task planner-prompt}
\end{figure*}

% \begin{figure*}
%     \centering
% \begin{lstlisting}
% # Task profile
% You are response router in DiDi's smart ride-hailing assistant, you handle the flow of operations. The DiDi smart ride-hailing assistant generates two pieces of information based on the <historical conversation>, <current user query>, and <current system time>: <Executing results> and <order status>. The Assistant then responds to the user based on these pieces of information.

% The content and explanation of each item are as follows:
% <history dialog>: This refers to the results of historical conversations between the user and the Assistant. You need to understand the current state of the dialogue from the history dialog, which may also contain content that needs attention in the response.
% <current user query>: This is the new query made by the user based on the history dialog. You need to respond to the user's current query.
% <Executing results>: This is the result after M1 analyzes the user's current query and calls the DiDi service functions. The Executing results may contain the results of multiple function calls. The specific functions that may appear include: </FUNCTIONs><FUNCTION>{"function_name": "Get_user_intent", "description": "Fill in 'Create_Order', 'Confirm_Order', 'Consult_Order', 'Cancel_Order', 'Consult_Company_Rule', 'Chat' according to the actual intent of the user in the parameter.", "result": {"intent": ""}}<FUNCTION>{"function_name": "Get_start_location", "description": "The starting point of the user's ride.", "result": {"searchStatus": "Multiple" or "Single" or "SearchFailed", "candidatePickupPoints": [{"name": "", "distance": "", "index": ""}]}}</FUNCTION><FUNCTION>{"function_name": "Get_via_location", "description": "A place the user needs to pass through during their ride.", "result": {"searchStatus": "Multiple" or "Single" or "SearchFailed", "candidateViaPoints": [{"name": "", "distance": "", "index": ""}]}}</FUNCTION><FUNCTION>{"function_name": "Get_end_location", "description": "A place the user needs to pass through during their ride.", "result": {"searchStatus": "Multiple" or "Single" or "SearchFailed", "candidateViaPoints": [{"name": "", "distance": "", "index": ""}]}}</FUNCTION><FUNCTION>{"function_name": "Get_departure_time", "description": "The departure time of the user's ride.",  "result": {"departureTime": "%Y-%m-%d  %H:%M:%S" or "TimeFuzzy" or "TimePassed" or "TimeInvalid"}}</FUNCTION><FUNCTION>{"function_name": "Get_arrival_time", "description": "The arrival time of the user's ride.", "result": {"arrivalTime": "%Y-%m-%d %H:%M:%S" or "TimeFuzzy" or "TimePassed" or "TimeInvalid"}}</FUNCTION></FUNCTIONs>
% <current time>: Contains the time of the user's current inquiry. This information is important for scheduled orders.
% <order status>: There are usually three situations with the order status. 1) [Search List] contains optional locations for pick-up and drop-off points, indicating that there are multiple pick-up/drop-off points in the system for the user to choose from. 2) [Order Information] provides complete ride-hailing order information, indicating that the user's order is ready. If it is a real-time order, the user can confirm the ride, and DiDi will arrange a car for them. If it is a scheduled order, DiDi will directly schedule a car for them. 3) The order status is empty, indicating that the user's order information is incomplete (pick-up/drop-off points not fully determined). Or the user has canceled the order. Note that in the order information, 'optional models' indicate all available car models and related trip information and prices. 'Current selected model' in the order information is the currently selected car model for the order.

% # Description of Response Generation Pipeline
% Based on the historical conversation, the user's current inquiry, and the system information, you need to determine the appropriate response scenario for the Assistant's response and provide the corresponding <response_generation_pipeline> and reason to allocate the model to different Assistants for responses. Currently available <response_generation_pipeline>s are as follows:
% 1: The user's current inquiry is simple and clear, without objections to the order content, and the function call information and order status match the user's current inquiry content.
% 2: The user's intention is to inquire about order details, DiDi service rules, and casual chat, or the current inquiry is ambiguous and not directly related to ride-hailing.
% 3: The user's current inquiry is clear, but there is a serious inconsistency between the function call information and the order status, or there are serious anomalies in the order content (such as an estimated travel time of 1000+ minutes, consistent pick-up/drop-off points), or the expected order does not exist (order information loss). Or the user expresses disagreement with the current order content and DiDi services.

% # Demonstration Example 1
% Input:
% <history dialog>: <user: Go to Tsinghua, assistant: We have schedule your trip from "Southwest Gate of Hongyuan New Era" to "Tsinghua University". Please click on the "Confirm " button on the card to place this order>
% <current user query>: Prefer within 20 dolloars
% <executing results>: {'function_name': ' Get_user_intent', 'result': {'intent': 'Create_Order'}}
% <current time>: 2024-08-22 14:30:00 Thursday
% <order status>: [Order Information][start locaton: Southwest Gate of Hongyuan New Era; via location:
% N/A; End location: Tsinghua University; departure time: Now; arrival time: 2024-08-22 14:52:30; estimated distance: 6.0 km; estimated duration: 22 minutes; car type: <<Discounted Express, 18.4$>, <Express, 22.4$>, <Premier, 26.3$>, <Luxe, 39.8$>>; currently selected car type: <<Discounted Express, 18.4$>, <Express, 22.4$>>]
% Output: 
% {"Dialog Policy": "Error-handling", "reason": "The user inquired about selecting a vehicle type within 20$, but the currently selected vehicle types in the order content include Express Car, which is priced above 20$"}
% # Demonstration Example 2
% ...

% # Format
% Now begin your response in the format {"Dialog Policy": <dialog_policy>, "reason": "<classification reason>"}.
% The classification reason should be within 50 words, preferably no more than 100 words.
% Do not provide explanations. The information you receive is as follows:

% \end{lstlisting}
    
%     \caption{Prompt template of dialog replier selection mechanism.}
%     \label{Route_prompt}
% \end{figure*}


\begin{figure*}
    \centering
\includegraphics[width=1 \linewidth]{Figure/appendix_4.png}
    \caption{Prompt template of dialog replier selection mechanism.}
    \label{Route_prompt}
\end{figure*}

\subsubsection{Multi-type Dialog Replier Selection}
\label{appendix response router}
After order planning, the dialog policy will be generated to allocate different dialog replier for response generation.
Specifically, we fully utilize all current information for allocating, including current user query, history dialog, current time, and intermediate executed system results.
We provide prompt template in Figure \ref{Route_prompt}.
As can be seen, we first provide a well-illustrative description of this task, and then we explain the input respectively, including history dialog, current user query, executing results, current time and order status.
Then, we provide description of each dialog replier scenario to help it understand how to perform response replier allocation.
Before controlling output format, we add demonstration example in the prompt.


% \begin{figure}
%     \centering
% \begin{lstlisting}
% # Task profile
% You are the response generator assistant in DiDi ride-hailing app, and your name is Xiao Di. You were created by Didi engineers. Your main function is to respond to users and guide them through trip planning. You need to respond based on the five pieces of information provided by the current system: <history dialog>, <current user query>, <Executing results>, <order status>, and <current time>. The following paragraph provides the content and explanation for each piece of information.

% The content and explanation of each item are as follows:
% <history dialog>: ...
% <current user query>: ...
% <Executing results>: ...
% <current time>: ...
% <order status>: ...

% # Response Style Control
% ***Scenario 1: If the <order status> starts with [Order Status][Real-time Order], it means the user real-time order is ready. You need to inform the user to click the "confirm" button.
% ## Demonstration Respon Style: Your trip from Hongyuan Xinshi Era Southwest Gate to Yongtai Dongli has been planned. After clicking the "Confirm" button on the card, XiaoDi will hail a car for you.

% ***Scenario 2: If the <order status> starts with [Order Information][Scheduled Order], it means the user's scheduled order is ready. You need to inform the user about relevant information such as the departure time.
% ## Demonstration Respon Style: You will depart on July 12th at 22:49. XiaoDi will start hailing a car for you up to 15 minutes before the departure time. The driver is expected to pick you up around 22:49, and the estimated travel time to your destination is 11 minutes.

% ***Scenario 3: If the <order status> starts with [Search List], it means there are multiple pick-up and drop-off points to choose from, and the user should not be allowed to click the "confirm ride" button.
% ## Demonstration Respon Style: Multiple drop-off points related to "Lingxiu New Silicon Valley Area D" have been found. You can let XiaoDi know which one to choose, or select one directly on the card~

% ***Scenario 4: If the <order status> is N/A, the user needs to complete the order information, and the ride should not be dispatched.
% ## Demonstration Respon Style: Your pick-up point has been confirmed as Yongtai Dongli (Northwest Gate 5). May I ask where your destination is?

% Let's start now!

% \end{lstlisting}
%     \caption{Prompt template of specialized dialog replier.}
%     \label{domain_replier_prompt}
% \end{figure}


\begin{figure}
    \centering
\includegraphics[width=1 \linewidth]{Figure/appendix_5.png}
   \caption{Prompt template of specialized dialog replier.}
    \label{domain_replier_prompt}
\end{figure}

\subsubsection{Specialized Dialog Relpier}
\label{appendix specialized response generation pipeline}
In specialized response generation pipeline, we will follow a formulaic response style to generate the assistant's response. 
The prompt template is provided in Figure \ref{domain_replier_prompt}. 
As can be seen, we provide four illustrative response styles to deal with different response scenarios. 
To conserve space, we omitted the full prompt and detailed explanation of each component (\ie <historical conversation>, <current user query>, and <current time>) since they are identical to those shown in Figure \ref{Route_prompt}.
The explanations of these items are also shortened for the error-handling and knowledge-enhanced dialog replier.

\subsubsection{Error-handling Dialog Replier}
\label{appendix error-handling response generation pipeline}
As discussed in the methodology, the error-handling pipeline aims to politely reject the user's unmet request or further ask the user for more detailed ride-hailing intent. 
Specifically, we add a few instructions and provide corresponding demonstrations (\ie how to incorporate the system error for a better response) based on the prompt of the specialized dialog replier shown in Figure \ref{domain_replier_prompt}. 
The added prompt content is shown in Figure \ref{error-handling response generation pipeline}. 
As shown in the prompt, the core is to add the explanation of the contradictions identified and to provide examples of contradictions to help the LLMs know how to reply in these situations.

% \begin{figure}
%     \centering
% \begin{lstlisting}
% <prompt of specialized response generation>

% # Task re-profle for error-handling response generation pipeline 
% To assist the response generation, router conducted an initial evaluation of the samples received by the assistant. Please generate your response based on the following example, combining the reasoning from Router.

% # Explanation of the contradict
% Please note that, according to DiDi's Router judgment, there may be significant anomalies in the order content (such as an estimated trip duration of 1000+ minutes, pick-up and drop-off points being the same), or an order that should exist does not (order information loss). Alternatively, the user may have expressed disagreement with the current order details or DiDi's service.
% In these situations, you should respond clarify any erroneous information for the user. If the user has any objections, you can reassure them and guide them to articulate the specific issues they have.

% # Contradict Example
% (1) When the function call fails to provide content, you can clarify the situation to the user and guide them to supplement the relevant details to complete the order.
% ## Demonstration Example
% ...

% (2) When there are significant anomalies in the user's order, such as trips that are too short (less than 0.2 kilometers) or too long (more than 500 kilometers), you should inquire based on these abnormalities.
% ## Demonstration Example
% ...

% Let's start now!

% \end{lstlisting}
%     \caption{The extended part of the prompt for error-handling dialog replier.}
%     \label{error-handling response generation pipeline}
% \end{figure}


\begin{figure}
    \centering
\includegraphics[width=1 \linewidth]{Figure/appendix_6.png}
   \caption{The extended part of the prompt for error-handling dialog replier.}
    \label{error-handling response generation pipeline}
\end{figure}

\subsubsection{Knowledge-enhanced Dialog Replier}
\label{appendix retrieval-augmented response generation pipeline}
We add retrieved QA pairs into the prompt and explicitly prompt LLMs to follow these similar QA pairs to generate the response. 
Specifically, we achieve prompt extension (\ie adding a task re-profile and retrieved QA pairs) based on the prompt of the specialized dialog replier shown in Figure \ref{domain_replier_prompt}. 
The added prompt template is shown in Figure \ref{RAG Replier}. 
As can be seen, we provide current user query, retrieved assistant response, and corresponding similarity score in the prompt, beginning with an initial task re-profile.

% \begin{figure}
%     \centering
% \begin{lstlisting}
% # Task reprofile
% Please follow the similar cases retrieved by the Didi system to generate a proper response. These cases are responses (<Customer Service Reply>) made by Didi's customer service representatives to past user inquiries in accordance with Didi's service rules. These replies may assist you in responding to users, and you can appropriately incorporate the content to address user queries.

% # Retrieved cases
% ## case 1
% <Current user query>: ...
% <Retrieved assistant response>: ...
% <Similarity score>: ...

% ## case 2
% ...

% Note: 
% When the user asks Xiaodi to introduce content unrelated to ride-hailing, Xiaodi can directly refuse to answer and steer the conversation back to ride-hailing.
% If the retrieved cases are not similar or have low similarity, do not refer to them.

% Let's start now!

% \end{lstlisting}
%     \caption{The extended part of the prompt for knowledge-enhanced dialog replier.}
%     \label{RAG Replier}
% \end{figure}



\begin{figure}
    \centering
\includegraphics[width=1 \linewidth]{Figure/appendix_7.png}
   \caption{The extended part of the prompt for knowledge-enhanced dialog replier.}
    \label{RAG Replier}
\end{figure}




\subsection{APIs and Tools Lib of DiMA}
\label{appendix: apis and tools}
This section presents all annotated APIs and tools used in DiMA, including \emph{Get\_current\_location}, \emph{POI\_search}, \emph{POI\_select}, \emph{Route\_planning} and a Time tool.
The following table briefly illustrates the input and output of these APIs and tools.


\begin{algorithm}
% \caption{API and Tool Lib}
\begin{verbatim}
def Get_current_location(session_id):
    POI_list = requests.post(session_id)
    # [{displayname, lat, lng, id}, ...]
    return POI_list

def POI_search(session_id, POI_name):
    POI_list = requests.post(session_id, POI_name)
    # [{displayname, lat, lng, id}, ...]
    return POI_list

def POI_select(POI_list, Selected_POI_name):
    # {displayname, lat, lng, id}
    return Selected_POI
    
def Route_planning_API(session_id, SrcLat, SrcLng,
ViaLat, ViaLng, DstLat, DstLng, SrcId, ViaId, DstId):
    order_list = requests.post(session_id, ...)
    # [{dist, duration, fee, car_type}, ...]
    return Order_list

def Time_tool(date, time, current_time):
    """
    date: "%Y-%m-%d" or "N week later, Mon" or "absent"
    time: "%H:%M:%S" or "absent"
    current_time: "%Y-%m-%d %H:%M:%S"
    """
    return date_time # "%Y-%m-%d %H:%M:%S"
\end{verbatim}
\end{algorithm}

Specifically, the \emph{$session , id$} is the tag of each dialog and is also the token in the DiDi ride-hailing platform to access diverse APIs service in DiDi. 
When executing the function generated, the extracted spatial information (\ie the POI name of the user's start location, via location, or end location) and temporal information (\ie the date and time in the user query) will be used to invoke these APIs and tools. 
After invocation, as discussed in Section \ref{Progressive Order Planning}, the corresponding ride-hailing order will be created.

\subsection{Evaluation of DiMA}
\label{Evaluation of DiMA}
In this section, we will provide more details about the GPT evaluation and Human evaluation in this section, including the prompt used for GPT evaluation and how to instruct human annotator to fill the evaluation form.

\subsubsection{Prompt Template of GPT Evaluation}
We evaluate the performance of ride-hailing assistant from two perspective, \ie order planning and assistant response.

% \begin{figure}
%     \centering
% \begin{lstlisting}
% # Task profile
% You are an evaluation expert for Didi's Intelligent Travel Assistant. Given a user query, the assistant needs to identify the user's intent and then create an order. Your task is to assess whether the assistant correctly identified the user's intent and whether the current order information meets the user's needs.

% # Explanation of user intent
% ...

% # Explanation of order information
% ...

% # Tips
% Next, I will provide you with the user's history dialog, the information of the current pickup point, waypoints, and drop-off point, as well as order information. Please evaluate the order information. Fill in 'TRUE' or 'FALSE' in the evaluation results, and provide a reason for the evaluation in no more than 50 characters in the evaluation analysis.

% # Output format
% Return the evaluation results in the following JSON format: {"order_information_evaluation_result": "", "order_information_evaluation_analysis": ""}.

% \end{lstlisting}
%     \caption{The prompt template of GPT evaluation for order planning.}
%     \label{GPT evaluation-order}
% \end{figure}



\begin{figure}
    \centering
\includegraphics[width=1 \linewidth]{Figure/appendix_8.png}
   \caption{The prompt template of GPT evaluation for order planning.}
    \label{GPT evaluation-order}
\end{figure}

As shown in Figure \ref{GPT evaluation-order}, we explicitly prompt the LLM to return its evaluation results and corresponding reasons. Specifically, we first clearly explain each of conversation goals, \eg confirming the order or wanting to cancel the order. Then we explain what information should be contained in the order information, \eg start location and the price of the car type. After that, we prompt GPT-4o to evaluate if the current order information satisfies the user's intent and to return its evaluation.
Overall, in the GPT evaluation, the following spatiotemporal information and order information should be evaluated: (1) start location; (2) via location; (3) end location; (4) departure time; (5) arrival time; (6) price and car type in the order information; (7) user intent. Only if all the above information is correct will the LLM view the order creation as TRUE.

For the response generation, as shown in Figure \ref{GPT evaluation-response}, we first provide several evaluation examples in the prompt (both positive and negative samples) to cover three response scenarios.
Then we provide evaluation rules, which is mainly formulated by the product manager for DiDi ride-hailing assistant, to guide LLM evaluate if current response can satisfy user's request and align with pre-defined product rule.
Specifically, the LLM will return a score (from 0 to 5), the detailed scoring criteria is as follow:
\begin{itemize}
    \item Scored 1 out of 5: If the response is completely irrelevant to the user's conversation, give 1 point.
    \item Scored 2 out of 5: If the response contains incorrect information, is inconsistent with the basic information provided by <executed results> and <order information>, promises features not supported by <executed results> or <order information>, or fails to address abnormal content in either of the two, give 2 points.
    \item Scored 3 out of 5: If the response is relevant and provides some information related to the user's inquiry, but there are issues with phrasing and a significant amount of redundant content, give 3 points.
    \item Scored 4 out of 5: If the response directly and comprehensively addresses the user's question but has some room for improvement in phrasing, give 4 points.
    \item Scored 5 out of 5: If the response perfectly addresses the user's question with appropriate phrasing, is concise and polite, give 5 points.
\end{itemize}

% \begin{figure}
%     \centering
% \begin{lstlisting}
% # Task profile 
% You are the evaluator responsible for assessing the responses generated by the assistant in Didi's intelligent ride-hailing assistant. You should utilize history dialog, current user query, Executing results, current time and order status to assess the quality of these responses.

% # Explanation
% <history dialog>: ...
% <current user query>: ...
% <executing results>: ...
% <current time>: ...
% <order status>: ...

% # Demonstration examples
% ...

% # Tips
% When evaluating, you need to pay attention to the following:
% 1. In terms of communication style, the assistant's responses must be user-centric, regardless of whether the user's needs are met...
% 2. In terms of facts, the assistant's responses must be based on <function call information> and <order status>...
% ...

% # Output format
% Let's start now, and reply in the format: {"score": "", "reason": ""}.

% \end{lstlisting}
%     \caption{The prompt template of GPT evaluation for response generation.}
%     \label{GPT evaluation-response}
% \end{figure}



\begin{figure}
    \centering
\includegraphics[width=1 \linewidth]{Figure/appendix_9.png}
   \caption{The prompt template of GPT evaluation for response generation.}
    \label{GPT evaluation-response}
\end{figure}
\subsubsection{Detailed Process of Human Evaluation}
The human evaluation process consists of two sequential steps, \ie annotation and quality validation.
As shown in Table \ref{evaluation form}, in the first step, we ask human annotator to fill the evaluation forum.
Specifically, given the user query, created order and generated response, the annotator will follow serious annotation rules created by product manager at DiDi to evaluate the results.
In the second step, another inspector will be required to double-check if these evaluation is true.
Once such a evaluation process is finished, we will calculate the overall evaluation metric, \ie the round-level order accuracy (ROA) and session-level order accuracy (SOA), round-level response accuracy (RRA) and session-level response accuracy (SRA).

\begin{table*}[]
\caption{An illustrative evaluation forum.}
\label{evaluation form}
\scalebox{0.85}{
\begin{tabular}{cccccc|ccc}
\midrule
\multicolumn{6}{c|}{Sample}                                & \multicolumn{2}{c}{Evaluation} & \multirow{2}{*}{Validation}\\
Time & Session\_id & Round\_id & User query & Order & Response & Order        & Response  &        \\ \midrule
   2024-07-01 13:23:25 &   XXXX         &  0         &  To Peiking University now    & <Order Info>      &  \begin{tabular}[c]{@{}c@{}}Sure. Click the 'Confirm'\\ button to place this order.\end{tabular}      &   TRUE           & TRUE & TRUE                \\
   ...  &  ...           &   ...        &  ...    &  ...     & ...         &   ...           &    ... &    ...            \\ \midrule
\end{tabular}}
\end{table*}


\begin{table*}[]
\caption{Study of cross-city transferability.}
\label{cross-city transferability}
\scalebox{0.85}{
\begin{tabular}{c|cccc|cccc}
\midrule
\multirow{2}{*}{Setting} & \multicolumn{4}{c|}{Human Evaluation (\%)} & \multicolumn{4}{c}{GPT Evaluation($\%$)} \\
                                & ROA   & RRA & SOA  & SRA  & ROA  & RRA & SOA & SRA \\ \midrule

\textbf{Beijing $\to$ Shanghai}                            &  98.03     &  98.92    &   95.96   &  96.81    &     83.95 &   96.13   &  63.20   & 93.13    \\
\textbf{Shanghai $\to$ Beijing}                            &  97.73     &  96.36    &   93.26   &  90.03    &     83.36 &   97.21   &  59.09   & 94.32   
\\ \midrule
\end{tabular}
}
\end{table*}
\subsection{Study of Transferability}
At the initial stage, DiMA was deployed to support ride-hailing services within Beijing. 
To ensure its scalability to other cities, we conducted further research on cross-city transferability. 
As shown in Table \ref{cross-city transferability}, we adopted two experimental setups: (1) training on Beijing data and testing on Shanghai data, and (2) training on Shanghai data and testing on Beijing data. 
The experimental results demonstrate that the LLM-based ride-hailing assistant is not influenced by city-specific data, exhibiting strong scalability.
Currently, DiMA is no longer restricted by city boundaries and can provide ride-hailing assistant services in any city across the country.


\subsection{Case Study of DiMA}
\label{case study}
We present a case study of DiMA to illustrate how DiMA creates a ride-hailing order and provides a response given a user query. 
Figure \ref{fig:casestudy} shows the entire dialog session. 

As can be seen, in the first dialog round, the user said "Hello, Xiaodi. I want to go to Huateng Garden South-Gate". 
Then we concatenate this user query and the current time into a prompt, which is fed into the spatiotemporal-aware tool-augmented order planning module. 
Based on the generated function, DiMA will automatically invoke APIs and tools to execute these functions and obtain the executed results and corresponding information. 
In this ride-hailing scenario, there are multiple candidate drop-off locations. 
Therefore, DiMA allocate this task to a specialized dialog replier. 
As can be seen, the replier successfully guides the user to select his preferred drop-off location.

\begin{figure*}
    \centering
    \includegraphics[width=1 \linewidth]{Figure/Case_Study.png}
    \caption{Illustrative case study. 
    We display the concatenated prompt and the output of the spatiotemporal-aware tool-augmented order planning with the blue block.
    Yellow color block is used to represent the order information and the executed results. 
    For the dialog replier selection, we use the green block to represent the output.
    Finally, we display response content of three dialog repliers in a orange block.
    }
    \label{fig:casestudy}
\end{figure*}
In dialog round 2, we first concatenate history user query, function call, response into the prompt and then DiMA generates function call for this round.
After executing generated function, we obtain the ride-hailing order information and the executed results.
% Then, with the allocated dialog replier, DiMA guides user to click "Confirm" button in the card.
However, current user request (want to book a order less than 40 dollars) cannot be satisfied because all available order are priced greater than 40 dollars.
Therefore, DiMA allocates this response task for error-handling dialog replier, who first guides the user to confirm this suggested order, then politely rejects the user's infeasible prices requirement and provides corresponding explanation. 
After such guidance, the user follows the suggestion to click the "Confirm" button.

In the last dialog round, the user wants to ask about information on nearby restaurants and even hopes the assistant can book a restaurant. 
This scenario involves ride-hailing policies, thus DiMA allocates this response task to knowledge-enhanced dialog replier. 
According to the retrieved policy information, this is not within the scope of the assistant's duties, thus the replier tactfully rejects the user's request and make an apology.
\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
