\section{Related work}
\textbf{Classifier guards.} Markov et al.~\cite{markov2023holisticapproachundesiredcontent} proposed an active learning strategy that identifies relevant samples for labeling, balances between uncertainty and diversity, and leverages redundancy to capture rare events. Rebedea et al.~\cite{rebedea2023nemoguardrailstoolkitcontrollable} developed guardrails control LLM output, preventing harmful topics, following dialogue paths, and maintaining language styles. Chi et al.~\cite{chi2024llamaguard3vision} introduced multimodal LLM-based safeguard that classifies content as safe or unsafe based on user-provided guidelines, conversation context, and output formats. Kim et al.~\cite{kim2024testinglimitsjailbreakingdefenses} highlight the necessity of developing better definitions for unsafe outputs. Wang et al.~\cite{wang2024jailbreakdefensenarrowdomain} point to challenges in jailbreak defense even in narrow contexts, suggests future directions involving classifier calibration and human feedback integration, suggesting future directions involving classifier calibration and human feedback integration. Sharma et al.~\cite{sharma2025constitutionalclassifiersdefendinguniversal} proposed "Constitutional Classifiers", which use natural-language rules to train classifier safeguards defining what constitutes permitted and restricted content.

\textbf{Jailbreaking} Lapid et al.~\cite{lapid2024opensesameuniversalblack} developed a black-box Genetic Algorithm (GA) to manipulate LLMs. Huang et al.~\cite{huang2023catastrophicjailbreakopensourcellms} show that manipulation of generation strategies, such as removing system prompts and altering decoding parameters, can easily disrupt model alignment. Samvelyan et al.~\cite{samvelyan2024rainbowteamingopenendedgeneration} introduced "Rainbow Teaming" methodology that sefines features like risk category and attack style to diversify prompts and rank them based on their effectiveness. Doumbouya et al.~\cite{doumbouya2024h4rm3ldynamicbenchmarkcomposable} developed an open-source automated red-teaming platform for generating and analyzing jailbreak attacks. Andriushchenko et al.~\cite{andriushchenko2024jailbreakingleadingsafetyalignedllms} applied prompt templates, random suffix search, self transfer from easier tasks, transfer and prefilling attacks and showed that adaptive attacks are necessary to accurately assess LLM robustness. Hughes et al.~\cite{hughes2024bestofnjailbreaking} proposed "Best-of-N (BoN) Jailbreaking," a black-box algorithm and demonstrated its effectiveness across text, vision, and audio language models, achieving high Attack Success Rates (ASR).