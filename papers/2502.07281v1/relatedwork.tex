\section{Related work}
\paragraph{Disentangled representation learning}
% \label{section:disentangled representation learning}

Our goal of block disentanglement is closely related to that of disentangled representation learning, which assumes that a relatively small number of independent factors are sufficient to explain the important patterns of variation in $\rvx$. Disentangled representation learning is typically cast as learning a latent variable $\rvz \in \R^{D_\rvz}$, where $\rvz$ is disentangled if its individual components $z_1, \dotsc, z_{D_\rvz}$ are independent and semantically meaningful~\citep{higgins2017beta,esmaeili2019structured,kim2018disentangling,chen2018isolating}. This informal definition of disentanglement is generally agreed upon, and it is not trivial to define this concept quantitatively~\citep{eastwood2018framework,higgins2018towards}. This is related to independent component analysis~\citep{comon1994independent,jutten1991blind,hyvarinen2000independent}, which makes the additional assumption that the encoding is noiseless.

With block disentanglement, instead of assuming there are $D_\rvz$ independent scalar factors, we assume there are two independent vector-valued factors $\rvz_c \in \R^{D_{\rvz_c}}$ and $\rvz_s \in \R^{D_{\rvz_s}}$. Recent works study identifiability for block disentanglement~\citep{von2021self,lachapelle2022partial,kong2022partial,lachapelle2024additive,lopez2024toward}. While we believe this is an important research direction, we focus on developing a simple algorithm that achieves strong empirical results on difficult real-world problems.

\paragraph{Invariant representation learning}
% \label{section:invariant representation learning}

The challenge of domain generalization has gained significant attention as ML systems often fail to generalize out-of-distribution. \citet{peters2016causal} introduced a framework for causal inference using invariant prediction, helping maintain predictive accuracy under interventions or environmental changes. Building on this foundation, \citet{arjovsky2019invariant} proposed IRM, a learning paradigm for learning an embedding of the data representation such that the optimal classifier on top of that representation remains invariant across different environments. These works, as well as many extensions~\citep{lu2021invariant}, have been benchmarked on datasets created by the research community, such as those in the DomainBed~\citep{gulrajani2020search} and WILDS~\citep{koh2021wilds} suites. \citet{gulrajani2020search} revealed that with rigorous model selection, ERM often achieves state-of-the-art performance, challenging the perceived benefits of more complex domain generalization methods.