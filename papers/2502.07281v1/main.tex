%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\input{math_commands.tex}

\usepackage{multirow}
\usepackage{siunitx}
\sisetup{group-minimum-digits=4,group-separator={,}}
\usepackage{subcaption}

\DeclareMathOperator{\enc}{Enc}
\DeclareMathOperator{\proj}{Proj}
\DeclareMathOperator{\dec}{Dec}

\DeclarePairedDelimiter\abs\lvert\rvert
\DeclarePairedDelimiterX{\KLutil}[2]{(}{)}{#1\;\delimsize\|\;#2}
\renewcommand{\KL}{D_{KL}\KLutil}

\newcommand{\supcon}{\mathcal{L}^{\text{sup}}}
\newcommand{\inv}{\mathcal{L}^{\text{inv}}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Supervised Contrastive Block Disentanglement}

\begin{document}

\twocolumn[
\icmltitle{Supervised Contrastive Block Disentanglement}

\begin{icmlauthorlist}
\icmlauthor{Taro Makino}{cds,pd,gred}
\icmlauthor{Ji Won Park}{pd}
\icmlauthor{Natasa Tagasovska}{pd}
\icmlauthor{Takamasa Kudo}{gred}
\icmlauthor{Paula Coelho}{gred}
\icmlauthor{Jan-Christian Huetter}{braid}
\icmlauthor{Heming Yao}{braid}
\icmlauthor{Burkhard Hoeckendorf}{braid}
\icmlauthor{Ana Carolina Leote}{gred}
\icmlauthor{Stephen Ra}{pd}
\icmlauthor{David Richmond}{braid}
\icmlauthor{Kyunghyun Cho}{cds,pd}
\icmlauthor{Aviv Regev}{gred}
\icmlauthor{Romain Lopez}{gred,dg}
\end{icmlauthorlist}

\icmlaffiliation{cds}{Center for Data Science, New York University}
\icmlaffiliation{pd}{Prescient Design, Genentech}
\icmlaffiliation{gred}{Research and Early Development (gRED), Genentech}
\icmlaffiliation{braid}{Biology Research | AI Development (BRAID), Genentech}
\icmlaffiliation{dg}{Department of Genetics, Stanford University}

\icmlcorrespondingauthor{Taro Makino}{taro@nyu.edu}

\icmlkeywords{}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

\begin{abstract}
Real-world datasets often combine data collected under different experimental conditions. This yields larger datasets, but also introduces spurious correlations that make it difficult to model the phenomena of interest. We address this by learning two embeddings to independently represent the phenomena of interest and the spurious correlations. The embedding representing the phenomena of interest is correlated with the target variable $y$, and is invariant to the environment variable $e$. In contrast, the embedding representing the spurious correlations is correlated with $e$. The invariance to $e$ is difficult to achieve on real-world datasets. Our primary contribution is an algorithm called Supervised Contrastive Block Disentanglement (SCBD) that effectively enforces this invariance. It is based purely on Supervised Contrastive Learning, and applies to real-world data better than existing approaches. We empirically validate SCBD on two challenging problems. The first problem is domain generalization, where we achieve strong performance on a synthetic dataset, as well as on Camelyon17-WILDS. We introduce a single hyperparameter $\alpha$ to control the degree of invariance to $e$. When we increase $\alpha$ to strengthen the degree of invariance, out-of-distribution performance improves at the expense of in-distribution performance. The second problem is batch correction, in which we apply SCBD to preserve biological signal and remove inter-well batch effects when modeling single-cell perturbations from 26 million Optical Pooled Screening images.
\end{abstract}

\section{Introduction}
\label{section:introduction}

Real-world machine learning (ML) datasets often combine data collected under different experimental conditions, such as medical images or stained histopathology sections collected at different hospitals~\citep{bandi2019detection,mckinney2020international}. This practice yields larger datasets, but the different experimental conditions alter the images' appearance, and induce spurious correlations that make it difficult to model the phenomena of interest. While human perception is relatively robust~\citep{makino2022differences}, ML models tend to rely on hospital-specific spurious correlations, and fail to generalize out-of-distribution to unseen hospitals~\citep{koh2021wilds}.

Similar spurious correlations are a long-standing problem in experimental biology~\citep{chandrasekaran2024three}, where they are called batch effects~\citep{leek2010tackling}. They can arise between experiments conducted in different labs, within the same lab, and even within a single large parallelized experiment. Removing batch effects by batch correction is an active research direction~\citep{arevalo2024evaluating}.

In some cases, we can manually remove the spurious correlations by using our prior knowledge. For example, color-based data augmentation can remove the staining variation in histopathology images~\citep{nguyen2023contrimix}. Similarly, in experimental biology, there are post-processing methods that remove specific known batch effects~\citep{carpenter2006cellprofiler}. However, such approaches have two significant limitations. First, they require manual post-hoc quality checks to ensure the post-processing did not remove desirable information. Second, some spurious correlations may be unknown, and therefore remain uncorrected. This motivates the development of automated approaches that maximize the removal of the spurious correlations, while minimizing the impact on the phenomena of interest.

To address these issues, we propose to learn two embeddings, one encoding the phenomena of interest, and the other encoding the spurious correlations. We break symmetry between the embeddings using the target variable $y$ and the environment variable $e$. Let $\rvx \in \R^{D_\rvx}$ be the observation, such as a histopathology image, and let $y \in \mathbb{Z}_{\geq 0}$ represent the phenomenon of interest, such as the presence of disease. Additionally, let $e \in \mathbb{Z}_{\geq 0}$ represent the experimental conditions, such as the hospital that processed the image. From these observed variables, we learn two embeddings $\rvz_c \in \R^{D_{\rvz_c}}$ and $\rvz_s \in \R^{D_{\rvz_s}}$, where $\rvz_c$ represents the variation of $\rvx$ induced by $y$, and $\rvz_s$ represents the variation of $\rvx$ induced by $e$. As we discuss in Section~\ref{section:SCBD}, we can also let $\rvz_s$ represent the variation of $\rvx$ induced by both $y$ and $e$. Our goal is to \emph{block disentangle} $\rvz_c$ and $\rvz_s$ so that they independently represent distinct information.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.4\textwidth]{fig/block_disentanglement.pdf}
    \caption{Spurious correlations emerge when collecting medical images from different hospitals, or conducting single-cell perturbation screens across multiple wells. $\rvz_s$ models these spurious correlations, while $\rvz_c$ models the environment-invariant correlations.}
\end{figure}

The promise of estimating $\rvz_c$ such that it captures the variation of $\rvx$ due to $y$ while remaining invariant to $e$ is significant for many downstream applications. However, existing methods for this task require additional regularization and hyperparameter tuning to ensure that $\rvz_c$ remains invariant to $e$. Optimizing such hyperparameters in the presence of distribution shifts has proven to be challenging in practice~\citep{gulrajani2020search}. While a few existing approaches have shown success in simplified settings~\citep{peters2016causal,ganin2016domain,louizos2016variational,lopez2018information,arjovsky2019invariant,lu2021invariant,kong2022partial}, most methods tested on real-world data have not outperformed simple baselines~\citep{gulrajani2020search}. Consequently, the problem of learning block-disentangled representations remains largely unsolved.

Our primary contribution is an algorithm called Supervised Contrastive Block Disentanglement (SCBD). We claim that SCBD achieves the desired invariance to $e$ with minimal and interpretable hyperparameter tuning. Unlike prior work on block disentanglement that use variational or adversarial objectives, our algorithm is based purely on Supervised Contrastive Learning (SCL)~\citep{khosla2020supervised}. Following the authors' notation, we learn two encoder networks $\enc_c(\cdot)$ and $\enc_s(\cdot)$ that map $\rvx$ to the intermediate representations given by
\begin{align*}
    \rvr_c \coloneqq \enc_c(\rvx) \in \R^{D_{\rvr_c}}, \quad \rvr_s \coloneqq \enc_s(\rvx) \in \R^{D_{\rvr_s}}.
\end{align*}
We additionally learn two projection networks $\proj_c(\cdot)$ and $\proj_s(\cdot)$ that map the intermediate representations to the lower-dimensional embeddings given by
\begin{align*}
    \rvz_c \coloneqq \proj_c(\rvr_c) \in \R^{D_{\rvz_c}}, \quad \rvz_s \coloneqq \proj_s(\rvr_s) \in \R^{D_{\rvz_s}},
\end{align*}
which are normalized to the unit hypersphere. In \citet{khosla2020supervised}, it was shown that some prediction tasks benefit from using the intermediate representations, rather than the embeddings. Finally, we learn a decoder $\dec(\rvz_c, \rvz_s)$ that reconstructs $\rvx$ from $\rvz_c$ and $\rvz_s$. The optimization objective consists of four terms, and is given by
\begin{align}
    \label{eq:SCBD}
    \min \supcon_{\rvz_c, y} + \supcon_{\rvz_s, e} + \alpha \inv_{\rvz_c, e} -\log p(\rvx \mid \dec(\rvz_c, \rvz_s)).
\end{align}
The first term directly applies SCL to cluster $\rvz_c$ with respect to $y$. Similarly, the second term directly applies SCL to cluster $\rvz_s$ with respect to $e$. The third term is our novel invariance loss, which is also based on SCL, and ensures that $\rvz_c$ is well-mixed with respect to $e$. In other words, our invariance loss purges $\rvz_c$ of the influence of $e$. The fourth term is an optional reconstruction loss. We describe these terms in detail in Section~\ref{section:SCBD}. SCBD incorporates a single hyperparameter $\alpha \in \R_{\geq 0}$ to adjust the degree to which $\rvz_c$ is invariant to $e$. When we increase $\alpha$, we observe a monotonic improvement on several downstream evaluation metrics that benefit from block disentanglement.

We empirically validate SCBD on three datasets that span two challenging real-world problems. The first problem is domain generalization~\citep{blanchard2011generalizing,muandet2013domain}, where $\rvz_c$ represents features whose correlation with $y$ is invariant to $e$. We use SCBD to generalize out-of-distribution on the synthetic Colored MNIST (CMNIST) dataset, as well as on the real-world histopathology dataset Camelyon17-WILDS ~\citep{koh2021wilds}. We demonstrate that SCBD enables precise control over the trade-off between in-distribution and out-of-distribution generalization performance through adjustment of the hyperparameter $\alpha$. Additionally, we show that on both datasets, SCBD achieves better out-of-distribution performance relative to the conventional baselines in the literature.

The second problem is batch correction, where we apply SCBD to a dataset of images of over 26 million individual cells ~\citep{funk2022phenotypic}. The cells are treated with \num{5050} genetic perturbations which are labeled as $y$, and collected across 34 wells which are labeled as $e$. We use SCBD to represent the effect of the perturbation with $\rvz_c$, and the variation across wells with $\rvz_s$. We show that relative to strong baselines including CellProfiler~\citep{carpenter2006cellprofiler}, SCBD provides estimates of $\rvz_c$ that preserve more biological signal while being less sensitive to batch effects.

\section{Supervised Contrastive Block Disentanglement}
\label{section:SCBD}

We now define the individual terms in the SCBD optimization objective in Equation~\ref{eq:SCBD}. Our starting point is a probabilistic interpretation of SCL that helps derive our novel invariance loss. Following the notation from \citet{khosla2020supervised}, let $I$ be the set of indices of examples within a minibatch. For each anchor point $i \in I$, we denote the set of the remaining examples as $\gA(i) = I \setminus \{i\}$. In SCL, anchor points are compared to other examples via their dot product. We define $\abs{\gA(i)}$ independent Bernoulli random variables $M_{i, c}^j$ for $j$ in $\{1, \dotsc, \abs{\gA(i)}\}$ to represent whether $\rvz_c^i$ is matched with $\rvz_c^j$. The matching probability is defined as
\begin{align*}
    P(M_{i, c}^j = 1) = \frac{\exp(\rvz_c^i \cdot \rvz_c^j / \tau)}{\sum_{a \in \gA(i)} \exp(\rvz_c^i \cdot \rvz_c^a / \tau)}.
\end{align*}
The softmax normalization over $\gA(i)$ ensures that the matching probabilities are computed relative to all other examples in $\gA(i)$, even though each individual matching event $M_{i,c}^j$ is binary. A similar definition holds for the random variable $M_{i, s}^j$, which is defined with respect to $\rvz_s$.

The first term in Equation~\ref{eq:SCBD} is a direct application of SCL, and is given by
\begin{align*}
    \supcon_{\rvz_c, y} = -\sum_{i \in I} \frac{1}{\abs{\gP_y(i)}} \sum_{p \in \gP_y(i)} \log P(M_{i, c}^p = 1),
\end{align*}
where $\gP_y(i) = \{j \in \gA(i) : y^i = y^j\}$ are the positive pairs for the anchor point $i$ with respect to $y$. This represents the negative log joint probability of observing the positive pairs, normalized by the number of positive pairs, and summed across all anchor points. Minimizing this loss clusters $\rvz_c$ with respect to $y$.

The second term in Equation~\ref{eq:SCBD} is also a direct application of SCL, and is given by
\begin{align*}
    \supcon_{\rvz_s, e} = -\sum_{i \in I} \frac{1}{\abs{\gP_e(i)}} \sum_{p \in \gP_e(i)} \log P(M_{i, s}^p = 1),
\end{align*}
where $\gP_e(i) = \{j \in \gA(i) : e^i = e^j\}$ are the positive pairs for the anchor point $i$ with respect to $e$. Minimizing this loss clusters $\rvz_s$ with respect to $e$. As we later discuss in our experiments (Section~\ref{section:domain generalization}), it can be useful to let $\rvz_s$ represent the variation of $\rvx$ with respect to the pair $(y, e)$, rather than just $e$. We can do this by replacing $\supcon_{\rvz_s, e}$ with
\begin{align*}
    \supcon_{\rvz_s, (y, e)} = -\sum_{i \in I} \frac{1}{\abs{\gP_{(y, e)}(i)}} \sum_{p \in \gP_{(y, e)}(i)} \log P(M_{i, s}^p = 1),
\end{align*}
where $\gP_{(y, e)}(i) = \{j \in \gA(i) : y^i = y^j, e^i = e^j\}$ are the positive pairs for the anchor point $i$ with respect to the pairs of labels $(y, e)$.

The third term in Equation~\ref{eq:SCBD} is our novel invariance loss. We define $\gN_e(i) = \gA(i) \setminus \gP_e(i)$ as the negative pairs with respect to the label $e$. Since $\{\gP_e(i), \gN_e(i)\}$ is a partition of $\gA(i)$, we consider the binary classification task of whether $\rvz_c^i$ is more likely to be matched with its positive or negative pairs with respect to $e$. One way to perform this classification is to predict that $i$ is more likely to be matched with $\gP_e(i)$ if
\begin{align*}
    \sum_{p \in \gP_e(i)} \log P(M_{i, c}^p = 1) > \sum_{n \in \gN_e(i)} \log P(M_{i, c}^n = 1).
\end{align*}
Since our goal is to make $\rvz_c$ invariant to $e$, we optimize $\rvz_c$ to make this classifier fail. We do this by minimizing
\begin{align*}
    \inv_{\rvz_c, e} = \abs*{\sum_{p \in \gP_e(i)} \log P(M_{i, c}^p = 1) - \sum_{n \in \gN_e(i)} \log P(M_{i, c}^n = 1)},
\end{align*}
which makes it equally probable that $\rvz_c^i$ is matched with its positive and negative pairs with respect to $e$. In other words, it disperses $\rvz_c^i$ with respect to $e$. This is analogous to adversarial approaches that train a discriminator to predict $e$, where the objective is to fool the discriminator~\citep{ganin2016domain,edwards2016censoring}. However, it can be difficult to apply these adversarial methods due to the complexity of minimax optimization. SCBD circumvents the need to train a discriminator, since the dot products between pairs of $\rvz_c$ can be used to predict $e$.

The fourth term in Equation~\ref{eq:SCBD} reconstructs $\rvx$ from $\rvz_c$ and $\rvz_s$. It is optional, and its inclusion makes SCBD semantically similar to competing approaches that are based on Variational Autoencoders (VAEs)~\citep{kingma2014auto,rezende2014stochastic}. As we show in Section~\ref{section:qualitative results}, the ability to reconstruct $\rvx$ can enable qualitative interpretation of $\rvz_c$ and $\rvz_s$. Importantly, we only use the reconstruction loss to optimize the decoder parameters, while holding $\rvz_c$ and $\rvz_s$ fixed. Therefore, the learning of $\rvz_c$ and $\rvz_s$ is done purely through SCL. It is possible to train the encoders and decoder jointly, which would likely improve the reconstruction quality. However, this adds the further complexity of balancing the relative contributions of the supervised contrastive and reconstruction losses by incorporating an additional hyperparameter. We leave this to future work, and focus on achieving strong performance on downstream tasks.

\section{Variational approaches fall short of SCBD}
\label{section:iVAE}

As a basis of comparison for SCBD, we develop a block-disentanglement algorithm based on Identifiable Variational Autoencoders (iVAEs)~\citep{khemakhem2020variational}. While VAEs are not common in the domain generalization literature, they are popular for modeling single-cell omics data~\citep{wang2023multi,tu2024supervised,mao2024learning}. Several VAE extensions address the problem of invariance to auxiliary variables, including the Variational Fair Autoencoder~\citep{louizos2016variational} and the HSIC-constrained VAE~\citep{lopez2018information}. These methods learn a single block of latent variables, and apply additional regularization to achieve invariance to an auxiliary variable. While successful on low-dimensional data, these approaches have had limited success with high-dimensional data. \citet{wang2023multi} applied contrastive learning to train a VAE with two blocks of latent variables, where one block does not condition on any auxiliary variables, and the other does. Our approach differs from theirs because we do not use contrastive learning, and both of our latent blocks condition on auxiliary variables.

We specify an iVAE with the same blocks of latent variables as SCBD. The generative model is defined as
\begin{align*}
    p_\theta(\rvx, \rvz_c, \rvz_s \mid y, e) = p_\theta(\rvx \mid \rvz_c, \rvz_s) p_\theta(\rvz_c \mid y) p_\theta(\rvz_s \mid e),
\end{align*}
while the inference model is defined as
\begin{align*}
    q_\phi(\rvz_c, \rvz_s \mid \rvx, e) = q_\phi(\rvz_c \mid \rvx) q_\phi(\rvz_s \mid \rvx, e).
\end{align*}
We fit this model by maximizing the evidence lower bound (ELBO)~\citep{jordan1999introduction}, given by
\begin{align*}
    \min_{\theta, \phi} \ &\E_{q_\phi(\rvz_c \mid \rvx) q_\phi(\rvz_s \mid \rvx, e)}[-\log p_\theta(\rvx \mid \rvz_c, \rvz_s)]\\
    + \ &\KL{q_\phi(\rvz_c \mid \rvx)}{p_\theta(\rvz_c \mid y)} \\
    + \ &\KL{q_\phi(\rvz_s \mid \rvx, e)}{p_\theta(\rvz_s \mid e)}.
\end{align*}

Empirically, conditioning the posterior of $\rvz_s$ on both $\rvx$ and $e$, rather than just $\rvx$, significantly impacts downstream performance. We hypothesize that conditioning the posterior of $\rvz_s$ on $e$ makes it easier to encode the variation with respect to $e$ in $\rvz_s$, which reduces the incentive to encode it in $\rvz_c$. We use a mixture of experts approach to condition on $e$, where a neural network takes in $\rvx$ and outputs separate posterior parameters for each value of $e$.

We find that iVAE performs better than some other baselines, but worse than SCBD. VAE-based block disentanglement methods inherently struggle to balance reconstruction and KL divergence minimization, leading to several failure modes. First, posterior collapse happens when the KL term is trivially minimized to zero by making the latent variables uninformative~\citep{bowman2015generating,razavi2019preventing,fu2019cyclical,dai2020usual,wang2021posterior}. Second, prior collapse occurs when learned parameters for $p_\theta(\rvz_c \mid y)$ collapse to the uninformative prior $p_\theta(\rvz_c)$. Third, numerical instability necessitates heuristics such as gradient clipping or skipping~\citep{child2020very}. These issues significantly limit the ability to train VAEs with large-capacity neural networks, likely explaining why open-source VAE implementations rarely use generic image encoders like those in torchvision~\citep{marcel2010torchvision}.

\section{Experiments}
\label{section:experiments}

We empirically validate SCBD on three datasets that span two difficult real-world problems. We discuss domain generalization in Section~\ref{section:domain generalization}, and batch correction in Section~\ref{section:batch correction}. In domain generalization, we use one synthetic and one realistic dataset, while in batch correction we use one large-scale realistic dataset with over 26 million images. The two problems are similar in that in both cases, we want $\rvz_c$ to represent the correlation between $\rvx$ and $y$ that is invariant to $e$, and $\rvz_s$ to encode the remaining spurious correlations that depend on $e$. The key difference between the two problems lies in the evaluation. In domain generalization, we evaluate the ability to predict $y$ given $\rvz_c$ on an out-of-distribution test set. In contrast, in batch correction the evaluation is in-distribution, and measures the degree to which $\rvz_c$ preserves the information in $y$, while discarding the information in $e$.

\subsection{Domain generalization}
\label{section:domain generalization}

\subsubsection{Problem description}

Domain generalization is an out-of-distribution generalization problem, where the data come from different environments. Environments represent different conditions under which data are generated, such as the hospital that collected the samples. We assume data are sampled from a family of distributions $p_{\text{all}} = \{p_e(\rvx_e, y_e) : e \in \gE_{\text{all}}\}$ indexed by the environment $e \in \gE_{\text{all}} \subseteq \mathbb{N}$. The training data are sampled from $p_{\text{tr}} = \{p_e(\rvx_e, y_e) : e \in \gE_{\text{tr}}\}$, where $\gE_{\text{tr}} \subset \gE_{\text{all}}$ is the set of training environments. The test data are sampled from $p_{\text{te}} = \{p_e(\rvx_e, y_e) : e \in \gE_{\text{te}}\}$, where $\gE_{\text{te}} \subset \gE_{\text{all}}$ is the set of test environments. Because $\gE_{\text{tr}}$ and $\gE_{\text{te}}$ are disjoint, there is a distribution shift between $p_{\text{tr}}$ and $p_{\text{te}}$. The goal is to predict $y$ from $\rvx$ in a way that is invariant to $e$, so that we can generalize from $p_{\text{tr}}$ to $p_{\text{te}}$.

\subsubsection{In- and out-of-distribution performance must be negatively correlated}

We begin by precisely characterizing the conditions under which SCBD should be effective at domain generalization. This is important, as it motivates our choice of datasets for our experiments. The conditions are intuitive and empirically testable. SCBD is helpful when the training data contain spurious, environment-specific features that create a trade-off. The more a model relies on these features, the better it performs on the training environments, and the worse it performs on unseen test environments. SCBD prevents the model from relying on such features, by enforcing the condition that the features cannot be predictive of the training environments.

We therefore want to evaluate SCBD on datasets that exhibit this trade-off. Fortunately, there is an empirical test for this, which is to train Empirical Risk Minimization (ERM)~\citep{vapnik1995nature} across a large region of the hyperparameter search space, and check whether there are regions where in-distribution performance is strong, and is negatively correlated with out-of-distribution performance. \citet{teney2024id} carried out such a study, and found the trade-off to be particularly prominent on the Camelyon17-WILDS~\citep{koh2021wilds} dataset. We therefore include this dataset in our experiments.

This trade-off between in- and out-of-distribution performance is the exception rather than the rule for domain generalization datasets. That is, despite the datasets being constructed to have qualitatively different training and test environments, it is often the case that in- and out-of-distribution performance are positively correlated. \citet{wenzel2022assaying} reached this conclusion by carrying out a large-scale empirical study involving 172 datasets, including those in the DomainBed~\citep{gulrajani2020search} and WILDS~\citep{koh2021wilds} suites. It is difficult to outperform ERM when the correlation is positive, which may explain why \citet{gulrajani2020search} found it to be state-of-the-art across the DomainBed suite.

\subsubsection{Datasets}

In addition to Camelyon17-WILDS, we experiment with one synthetic dataset. This dataset is called Colored MNIST (CMNIST), and extends the version from \citet{arjovsky2019invariant}. The target label $y \in \{0, \dotsc, 9\}$ represents the digit. There are two training environments and a test environment. In the training environments $e \in \{0, 1\}$, there is an environment-dependent correlation between the color and $y$ (Figure~\ref{fig:cmnist}). For $e = 0$ the color changes from dark to light red as the digit increases. In contrast, for $e = 1$ the color changes from light to dark green as the digit increases. All digits are white in the test environment. This presents a severe distribution shift, since color is perfectly predictive of $y$ in the training environments, but is unpredictive in the test environment. Details regarding the data generating process are in Appendix~\ref{appendix:CMNIST}. We train ERM across a range of learning rates and maximum training steps on this dataset, and observe that in- and out-of-distribution performance are negatively correlated (Appendix Figure~\ref{fig:cmnist,val_test_scatter}). This satisfies the assumptions of SCBD, and therefore we expect $\rvz_c$ to encode the digit, and $\rvz_s$ to encode the environment-specific colors.

Camelyon17-WILDS~\citep{koh2021wilds} is a patch-based variant of the original Camelyon17 dataset~\citep{bandi2019detection} of histopathology images of breast tissue, and represents a binary classification task of predicting the presence of a tumor. The data were collected in five hospitals, and have significant inter-hospital batch effects. It has been reported that for similar datasets, the most significant batch effects are from differences in how the slides are stained~\citep{tellez2019quantifying}. As mentioned previously, \citet{teney2024id} showed that this dataset exhibits a trade-off between in- and out-of-distribution performance, and therefore satisfies the assumptions of SCBD. We also verify this in Appendix Figure~\ref{fig:camelyon17,val_test_scatter}. On this dataset, we want $\rvz_c$ to represent the biomarkers of disease that are invariant across hospitals, and $\rvz_s$ to represent the hospital-specific spurious correlations.

\begin{figure*}[htb]
    \centering
    \hspace*{\fill}
    \begin{subfigure}[t]{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/cmnist.pdf}
        \caption{Dataset}
        \label{fig:cmnist}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=0.65\textwidth]{fig/cmnist,gen,scbd.pdf}
        \caption{Counterfactual generation}
        \label{fig:cmnist,gen,scbd}
    \end{subfigure}
    \hspace*{\fill}
    \vspace{-0.2cm}
    \caption{Colored MNIST. (a) There is an environment-dependent correlation between color and digit on the training set, which does not persist on the test set where all digits are white. (b) We can generate images counterfactually using SCBD. When we swap $\rvz_c$ across examples, it changes the digit without affecting the color. In contrast, when we swap $\rvz_s$ across examples, it changes the color without affecting the digit. By composing digit and color independently, we generate images outside of the support of the training distribution, such as a light red one (bottom middle) and a bright green five (bottom right).}
    \vspace{-0.2cm}
\end{figure*}

\subsubsection{Baselines}

We compare SCBD to a diverse range of algorithms that are considered to be standard baselines in the domain generalization literature. This includes ERM, CORrelation ALignment (CORAL)~\citep{sun2016deep}, Domain-Adversarial Neural Networks (DANN)~\citep{ganin2016domain}, Invariant Risk Minimization (IRM)~\citep{arjovsky2019invariant}, Fish~\citep{shi2021gradient}, and Group Distributionally Robust Optimization (Group DRO)~\citep{sagawa2020distributionally}. We additionally include our iVAE from Section~\ref{section:iVAE} for completeness.

\subsubsection{Qualitative results}
\label{section:qualitative results}

Our image generation results in Figure~\ref{fig:cmnist,gen,scbd} qualitatively demonstrate that SCBD achieves block disentanglement. This is possible on CMNIST because we know that the ground-truth phenomenon of interest is the digit, and the spurious correlation is the color. These results show that when we swap $\rvz_c$ between examples, it changes the digit without affecting the color. In contrast, when we swap $\rvz_s$ between examples, it changes the color without affecting the digit. Note that the quality of the reconstructed images is relatively poor because, as mentioned in Section~\ref{section:SCBD}, the decoder is not trained jointly with the encoders. We leave it to future work to train the decoder jointly and improve the image reconstruction capability of SCBD. We provide similar visualization results with the iVAE in Appendix Figure~\ref{fig:cmnist,gen,ivae}.

\subsubsection{Quantitative results}

We present two kinds of quantitative results. In Figure~\ref{fig:domain generalization}, we show that by increasing $\alpha$, SCBD removes spurious correlations that are specific to the training environments. This results in learning features that are invariant to the environment, and yields a clear trade-off between in- and out-of-distribution performance on both datasets.

\begin{figure*}[htb]
    \centering
    \hspace*{\fill}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/cmnist,resnet18.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/camelyon17,densenet121.pdf}
    \end{subfigure}
    \hspace*{\fill}
    \vspace{-0.2cm}
    \caption{Increasing $\alpha$ strengthens the degree that $\rvz_c$ is invariant to $e$, and monotonically improves test accuracy at the expense of validation accuracy.}
    \vspace{-0.2cm}
    \label{fig:domain generalization}
\end{figure*}

\begin{table}[htb]
    \centering
    \caption{Test accuracy (\%) for domain generalization for ten random seeds. SCBD with $\alpha = 192$ significantly outperforms all baselines on both CMNIST and Camelyon17-WILDS.}
    \begin{tabular}{lcc}
    \toprule
    Algorithm & CMNIST & Camelyon17-WILDS\\
    \midrule
    SCBD ($\alpha = 0$) & $25.5 \pm 3.0$ & $61.9 \pm 3.8$\\
    SCBD ($\alpha = 192$) & $\bm{82.9 \pm 12.1}$ & $\bm{72.7 \pm 3.0}$\\ \midrule
    ERM & $37.8 \pm 2.6$ & $65.8 \pm 4.9$ \\ 
    CORAL & $37.6 \pm 3.6$ & $59.5 \pm 7.7$\\
    DANN & $39.0 \pm 4.5$ & $55.2 \pm 6.7$ \\
    IRM & $37.0 \pm 4.2$ & $66.3 \pm 2.1$\\
    Fish & $48.2 \pm 3.5$ & $49.1 \pm 0.9$\\
    Group DRO & $35.0 \pm 2.9$ & $68.4 \pm 7.3$\\ \midrule
    iVAE & $52.1 \pm 37.6$ & $52.0 \pm 2.0$\\
    \bottomrule
    \end{tabular}
    \label{table:domain_generalization}
\end{table}

In Table~\ref{table:domain_generalization}, we show the test accuracy on both datasets for SCBD and the baseline algorithms. We report the average and standard deviation for ten random seeds. For the baseline algorithms, we optimize the hyperparameters with respect to the performance on the in-distribution validation set. The hyperparameter search space for each algorithm is provided in Appendix Table~\ref{table:hyperparameter search space}. Most of the baseline results for Camelyon17-WILDS are taken from the authors' leaderboard, with the exception of Fish~\citep{shi2021gradient}, which we evaluate ourselves. Our results for Fish are weaker than those reported on the leaderboard, because we additionally included the pretraining duration in the hyperparameter search space. The leaderboard results used the value of this hyperparameter that achieved the best test accuracy, as described in the appendix of \citet{shi2021gradient}.

For SCBD, we apply the same model selection procedure to optimize the learning rate and weight decay. We do not optimize $\alpha$ during model selection, since this would result in choosing $\alpha = 0$. We report the test accuracy for $\alpha = 0$ and $\alpha = 192$ as evidence that the invariance loss in SCBD is effective at removing spurious correlations and improving out-of-distribution performance. With $\alpha = 192$, SCBD significantly outperforms all baseline algorithms across both datasets.  Tuning $\alpha$ corresponds to model selection with respect to an unknown test distribution, which is a difficult open problem~\citep{gulrajani2020search}, and is a limitation shared by other works~\citep{makino2022generative,wortsman2022robust}. We demonstrate the robustness of our approach to the choice of hyperparameters by providing the results of ablation studies in Appendix~\ref{appendix:CMNIST} and \ref{appendix:Camelyon17-WILDS}, where we vary $D_{\rvz_c}$ and $D_{\rvz_s}$, the batch size, and the degree of weight decay.

We additionally experiment with PACS~\citep{li2017deeper} and VLCS~\citep{fang2013unbiased} from DomainBed~\citep{gulrajani2020search}, and include the results in Appendix Sections~\ref{appendix:PACS} and \ref{appendix:VLCS}. We find that these datasets exhibit a positive correlation between in- and out-of-distribution performance, which is consistent with \citet{wenzel2022assaying}. Since this violates the assumptions of SCBD, we are unable to trade off in- and out-of-distribution performance.

\subsection{Batch correction with a real-world Optical Pooled Screen dataset}
\label{section:batch correction}

\subsubsection{Problem description and dataset}

Having demonstrated the efficacy of SCBD on domain generalization, we proceed to batch correction. Here, we experiment with a realistic single-cell perturbation dataset that is significantly large in scale. We use the Optical Pooled Screen (OPS)~\citep{feldman2019optical} dataset from \citet{funk2022phenotypic} comprised of 26 million images of single cells, each perturbed with one of \num{5050} genetic perturbations targeting an expressed gene, including one non-targeting control. Such data are collected in order to understand the effect of each perturbation on cellular morphology. The $100 \times 100$ pixel images have four channels that measure staining information for key cellular features: DNA damage, F-actin, DNA content, and microtubules. Each channel therefore measures a unique aspect of a cell's phenotype, which taken together shed light on how each perturbed gene affects the cell. An important problem in the field is to build a cartography of perturbation effects by grouping perturbed genes by their phenotypic similarity~\citep{celik2024building}. This map is then interpreted to characterize the function of unknown genes, recapitulate protein complexes, and highlight interacting pathways~\citep{rood2024toward}. 

OPS generates large quantities of data in a cost-effective manner by processing several batches of experiments in parallel. This dataset was collected at a single lab using 34 wells. There can be significant unintended variation across wells, based on minor differences in experimental conditions. For example, if the wells are stained sequentially, the difference in elapsed time can result in different image brightness across wells. Our goal with SCBD is to capture this unintended variation across wells in $\rvz_s$, so that $\rvz_c$ is an unconfounded representation of the impact of genetic perturbations on cell morphology.

For each image of a single cell $\rvx$, $y$ labels the genetic perturbation, and $e$ labels which of the 34 wells the cell was in. By optimizing the SCBD objective in Equation~\ref{eq:SCBD}, we ensure that the variation in the images due to the perturbation is represented by $\rvz_c$, and the variation due to the well $e$ is represented by $\rvz_s$. We can then use $\rvz_c$ for downstream analysis. We use ResNet-18 encoders for this task, and use $\rvz_c$ with $D_{\rvz_c} = D_{\rvz_s} = 64$, whereas we use $\rvr_c$ for domain generalization. This is because all of our baselines for this task use 64 dimensional embeddings, so the lower-dimensional $\rvz_c$ helps ensure a fair comparison. We find that $\alpha = 1$ is sufficient for enforcing environment-invariance for batch correction.

We evaluate two tasks to understand the degree to which we remove the influence of $e$, while preserving the information in $y$. We describe the tasks at a high level here, and provide details in Appendix~\ref{appendix:batch correction}.

The first task is CORUM prediction, which is one measure of the biological information content in the embeddings. This task relies on the CORUM database~\citep{ruepp2010corum} as the ground truth of whether two genes are functionally related based on their membership in the same protein complex (a definition previously used in the context of this biological screen). We take the biological embeddings corresponding to those genetic perturbations $y$, interpret their dot product as the prediction that they are similar, and use these predictions to compute the area under the precision-recall curve. We want the performance on this task to be strong.

The second task is to use the perturbation embeddings to predict $e$, which measures the sensitivity to inter-well batch effects. We fit a linear classifier on top of each of the embeddings, and compute the F1 score. In contrast to the first task, we want the performance on this task to be weak.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.7\textwidth]{fig/funk22,resnet18.pdf}
    \vspace{-0.2cm}
    \caption{Comparison of SCBD to CellProfiler and VAE-based baselines on real-world batch correction. Left: Performance on predicting protein complex membership (biological content). Higher is better. Right: Performance of predicting the well label $e$. Lower is better. SCBD is unambiguously better than all baselines, as it preserves more biological signal, while being less sensitive to the inter-well batch effects.}
    \vspace{-0.2cm}
    \label{fig:funk22,resnet18}
\end{figure*}

\subsubsection{Baselines}

CellProfiler~\citep{carpenter2006cellprofiler} is the most important baseline that we compare SCBD against. It is an open-source software that takes in an image of a cell, and outputs several thousand manually-engineered morphological features that describe the cell's phenotype. It is a very strong baseline in which substantial human-expert effort has been invested, and its representations are post-processed to batch correct the variation across plates and wells. Following conventional practice, we use the top-64 principal components of the full set of CellProfiler features.

The remaining baselines are all based on VAEs, which are popular for modeling single-cell omics data. We experiment with iVAE from Section~\ref{section:iVAE}, as well as the Multi-Contrastive VAE (mcVAE)~\citep{wang2023multi}, which uses two blocks of latent variables in order to represent the perturbation effect and the natural cell-to-cell variation. Although it was previously shown that mcVAE is effective for modeling genetic perturbations, it has a significant weakness in that it does not effectively correct for batch effects. Finally, for our simplest baseline we use a vanilla VAE~\citep{kingma2014auto}, which has a single block of latent variables, and ignores $y$ and $e$. For all VAE-based models, we use 64 dimensional latent variables in each block. The perturbation embedding is $\rvz_c$ for SCBD and iVAE. For mcVAE it is the block of salient variables, and for CellProfiler and the vanilla VAE, there is only a single block of latent variables.

\subsubsection{Quantitative results}

We show our results on both tasks in Figure~\ref{fig:funk22,resnet18}. SCBD performs unambiguously better than all baselines, as it retains more biological information, while being less sensitive to inter-well batch effects. Thus, $\rvz_c$ estimated with SCBD can be used by biologists for downstream analysis, and they can be confident that any conclusions reached are not due to the inter-well variation. This is a significant achievement, as CellProfiler is considered a very strong baseline for this problem. Although mcVAE performs better than the vanilla VAE on CORUM due to its ability to incorporate the perturbation labels $y$, they are both highly susceptible to the inter-well batch effects. This highlights the fact that explicit regularization is required in order to purge the effect of $e$ from the embeddings, and that this does not occur naturally.

\section{Related work}

\paragraph{Disentangled representation learning}
% \label{section:disentangled representation learning}

Our goal of block disentanglement is closely related to that of disentangled representation learning, which assumes that a relatively small number of independent factors are sufficient to explain the important patterns of variation in $\rvx$. Disentangled representation learning is typically cast as learning a latent variable $\rvz \in \R^{D_\rvz}$, where $\rvz$ is disentangled if its individual components $z_1, \dotsc, z_{D_\rvz}$ are independent and semantically meaningful~\citep{higgins2017beta,esmaeili2019structured,kim2018disentangling,chen2018isolating}. This informal definition of disentanglement is generally agreed upon, and it is not trivial to define this concept quantitatively~\citep{eastwood2018framework,higgins2018towards}. This is related to independent component analysis~\citep{comon1994independent,jutten1991blind,hyvarinen2000independent}, which makes the additional assumption that the encoding is noiseless.

With block disentanglement, instead of assuming there are $D_\rvz$ independent scalar factors, we assume there are two independent vector-valued factors $\rvz_c \in \R^{D_{\rvz_c}}$ and $\rvz_s \in \R^{D_{\rvz_s}}$. Recent works study identifiability for block disentanglement~\citep{von2021self,lachapelle2022partial,kong2022partial,lachapelle2024additive,lopez2024toward}. While we believe this is an important research direction, we focus on developing a simple algorithm that achieves strong empirical results on difficult real-world problems.

\paragraph{Invariant representation learning}
% \label{section:invariant representation learning}

The challenge of domain generalization has gained significant attention as ML systems often fail to generalize out-of-distribution. \citet{peters2016causal} introduced a framework for causal inference using invariant prediction, helping maintain predictive accuracy under interventions or environmental changes. Building on this foundation, \citet{arjovsky2019invariant} proposed IRM, a learning paradigm for learning an embedding of the data representation such that the optimal classifier on top of that representation remains invariant across different environments. These works, as well as many extensions~\citep{lu2021invariant}, have been benchmarked on datasets created by the research community, such as those in the DomainBed~\citep{gulrajani2020search} and WILDS~\citep{koh2021wilds} suites. \citet{gulrajani2020search} revealed that with rigorous model selection, ERM often achieves state-of-the-art performance, challenging the perceived benefits of more complex domain generalization methods.

\section{Conclusion}

We presented Supervised Contrastive Block Disentanglement (SCBD), an algorithm for block disentanglement that is based purely on SCL. We use SCBD to estimate $\rvz_c$ such that it represents the correlation between $\rvx$ and $y$ that is invariant to $e$. This invariance, which is considered difficult to achieve in practice, allows us to solve two difficult real-world problems. The first problem is domain generalization, where we achieve strong out-of-distribution generalization on a synthetic dataset called Colored MNIST, as well as a real-world histopathology dataset called Camelyon17-WILDS. The second problem is batch correction, where we use SCBD to learn representations of single-cell perturbations from over 26 million images that preserves biological signal while removing inter-well batch effects.

We believe a promising direction for future work is to investigate how to jointly train the decoder to combine the capabilities of SCL and generative modeling. The generative capability of SCBD is a relatively small aspect of this work, since we only use it to qualitatively interpret the embeddings on our CMNIST experiments. This gave us confidence that our algorithm block disentangles the phenomena of interest and the spurious correlations in a toy setting. With improved generative modeling, SCBD has the potential to be used for impactful counterfactual image generation on real-world data, such as generating images of the same cell under different perturbations. Also, in this work we assumed access to the variable $e$, which labels the source of unwanted variation. We leave it to future work to learn this variable from data.

\section*{Acknowledgements and Disclosure of Funding}

We thank Henri Dwyer, Joseph Kleinhenz, Philipp Hanslovsky, and Shenglong Wang for data- and compute-related help with this project, and the Genentech and NYU high-performance computing teams for helping us access the compute resources that were used in this work.

This work was done while Taro Makino was an intern at Genentech. Ji Won Park, Natasa Tagasovska, Takamasa Kudo, Paula Coelho, Jan-Christian Huetter, Heming Yao, Burkhard Hoeckendorf, Ana Carolina Leote, Stephen Ra, David Richmond, Kyunghyun Cho, Aviv Regev, and Romain Lopez are employees of Genentech. Ji Won Park, Natasa Tagasovska, Jan-Christian Huetter, Heming Yao, Burkhard Hoeckendorf, Stephen Ra, David Richmond, Kyunghyun Cho, and Aviv Regev have equity in Roche. Aviv Regev is a co-founder and equity holder of Celsius Therapeutics and an equity holder in Immunitas. She was an SAB member of ThermoFisher Scientific, Syros Pharmaceuticals, Neogene Therapeutics, and Asimov until July 31, 2020, and has been an employee of Genentech since August 1, 2020. Kyunghyun Cho is supported by the Samsung Advanced Institute of Technology (under the project Next Generation Deep Learning: From Pattern Recognition to AI).

\section*{Impact statement}

This work addresses a pressing need in ML. As ML systems become more prevalent in real-world applications, it is becoming increasingly important that we can understand and control their behavior. This is especially true in life-critical applications such as medical diagnosis. However, it has been repeatedly demonstrated that machines do not diagnose the same way that human doctors do. Instead, they rely on high frequency patterns that doctors ignore~\citep{makino2022differences}, or spurious features such as surgical skin markings that doctors know are unrelated to underlying pathology~\citep{winkler2019association}. The key difference between doctors and machines is that the former are trained to think causally, while the latter has traditionally focused on statistical association. A very clear example of this was shown in \citet{oakden2020hidden}, who showed that 
machines used the presence of chest drains in order to detect pneumothorax in chest x-rays. This corresponds to detecting a disease based on the evidence of it having been treated. Due to these shortcomings, there is a concerted effort to steer ML towards learning robust features, and this work is one example.

\bibliography{main}
\bibliographystyle{icml2025}

\newpage
\appendix
\onecolumn

\section*{Appendix}

\section{Experiments}

\subsection{Experimental setup}
\label{appendix:experimental setup}

All of our experiments were done using a single NVIDIA A100 GPU on our institutions' high-performance computing clusters.

\subsubsection{Supervised Contrastive Block Disentanglement}

Our experimental setup for SCBD is remarkably similar across all of our experiments, which highlights the generality of our approach. We set the temperature parameter in the supervised contrastive losses to $\tau = 0.1$. We adopt both of these practices from \citet{khosla2020supervised}. For optimization, we use AdamW~\citep{loshchilov2019decoupled} with \num{1e-4} learning rate and 0.01 weight decay. We set the batch size to \num{2048}. We chose these values because they resulted in stable training and validation curves across our experiments, and did not tune them extensively.

For domain generalization, we resize the images to $32 \times 32$ pixels. We set $D_{\rvz_c} = D_{\rvz_s} = 128$, which we adopt from \citet{khosla2020supervised}. We train for a maximum of \num{25000} steps, and select the weights that minimize the validation loss. We obtain error bars by repeating each experiment with ten random seeds.

For batch correction, we resize the images to $64 \times 64$ pixels. We set $D_{\rvz_c} = D_{\rvz_s} = 64$ in order to ensure a fair comparison with the top-64 PCA features of CellProfiler. To sample a minibatch, we first sample 256 distinct values of $y$ from the class distribution of the training set, and then sample the same number of examples per value of $y$. This was necessary in order to ensure a large number of positive pairs with respect to $y$ in our supervised contrastive losses, given that there are \num{5050} classes. We trained for a maximum of \num{150000} steps, and evaluated on the test set using the weights that minimize the validation loss. We obtain error bars by repeating each experiment with three random seeds.

We use standard architectures such as ResNet-18~\citep{he2016deep} and DenseNet-121~\citep{huang2017densely} for the encoders $\enc_c(\rvx)$ and $\enc_s(\rvx)$. The projection networks $\proj_c(\rvr_c)$ and $\proj_s(\rvr_s)$ are two-layer Multilayered Perceptrons~\citep{rumelhart1986learning} with hidden sizes of $D_{\rvr_c}$ and $D_{\rvr_s}$, and GELU activations~\citep{hendrycks2016gaussian}. Our decoder $\dec(\rvz_c, \rvz_s)$ architecture is shown in Appendix Table~\ref{table:SCBD decoder architecture}, with GELU activations~\citep{hendrycks2016gaussian} between layers. We use an additive decoder~\citep{lachapelle2024additive}, and found this to be necessary to achieve sensible visualization results on CMNIST. That is, we define
\begin{align*}
    \log p(\rvx \mid \rvz_c, \rvz_c) = \dec_c(\rvz_c) + \dec_s(\rvz_s),
\end{align*}
where both $\dec_c$ and $\dec_s$ have the same architecture.

\begin{table}[H]
    \centering
    \caption{SCBD decoder architecture}
    \begin{tabular}{l}
    \toprule
    \texttt{Linear(64, 256 * (2 ** 2))}\\
    \texttt{ConvTranspose2d(256, 256, 3, stride=2, padding=1, output\_padding=1)}\\
    \texttt{Conv2d(256, 256, 3, padding=1)}\\
    \texttt{ConvTranspose2d(256, 256, 3, stride=2, padding=1, output\_padding=1)}\\
    \texttt{Conv2d(256, 256, 3, padding=1)}\\
    \texttt{ConvTranspose2d(256, 256, 3, stride=2, padding=1, output\_padding=1)}\\
    \texttt{Conv2d(256, 256, 3, padding=1)}\\
    \texttt{ConvTranspose2d(256, 256, 3, stride=2, padding=1, output\_padding=1)}\\
    \texttt{Conv2d(256, 128, 3, padding=1)}\\
    \texttt{Conv2d(128, img\_ch, 1)}\\
    \bottomrule
    \end{tabular}
    \label{table:SCBD decoder architecture}
\end{table}

\subsubsection{Variational Autoencoders}
\label{appendix:VAE}

For our experiments with VAE-based approaches, we use the same experimental setup used in \citet{wang2023multi}, including the architecture and hyperparameters. We resize the images to $64 \times 64$ pixels and use a batch size of 1024. The encoder and decoder architectures are in Appendix Tables~\ref{table:VAE encoder architecture} and \ref{table:VAE decoder architecture}, with GELU activations~\citep{hendrycks2016gaussian} between layers. Since the CMNIST images are $32 \times 32$, we modify the architectures to reduce the up- and down-sampling. For optimization, we use the AdamW~\citep{loshchilov2019decoupled} optimizer with \num{1e-4} learning rate and 0.01 weight decay. We additionally skip gradients with a norm above \num{1e12}, and clip gradients with a norm above \num{1e6}, as done in \citet{child2020very}. We train for a maximum of \num{50000} steps for domain generalization, and three epochs for batch correction, and select the weights with minimum validation loss. We report the validation and test performance across ten random seeds for domain generalization, and three random seeds for batch correction.

\begin{table}[H]
    \centering
    \caption{VAE encoder architecture}
    \begin{tabular}{l}
    \toprule
    \texttt{Conv2d(img\_c, 32, 3, stride=2, padding=1)}\\
    \texttt{Conv2d(32, 32, 3, padding=1)}\\
    \texttt{Conv2d(32, 64, 3, stride=2, padding=1)}\\
    \texttt{Conv2d(64, 64, 3, padding=1)}\\
    \texttt{Conv2d(64, 64, 3, stride=2, padding=1)}\\
    \texttt{Linear(64 * (8 ** 2), 2 * 64)}\\
    \bottomrule
    \end{tabular}
    \label{table:VAE encoder architecture}
\end{table}

\begin{table}[H]
    \centering
    \caption{VAE decoder architecture}
    \begin{tabular}{l}
    \toprule
    \texttt{Linear(2 * 64, 64 * (8 ** 2))}\\
    \texttt{ConvTranspose2d(64, 64, 3, stride=2, padding=1, out\_padding=1)}\\
    \texttt{Conv2d(64, 64, 3, padding=1)}\\
    \texttt{ConvTranspose2d(64, 32, 3, stride=2, padding=1, out\_padding=1)}\\
    \texttt{Conv2d(32, 32, 3, padding=1)}\\
    \texttt{ConvTranspose2d(32, img\_c, 3, stride=2, padding=1, out\_padding=1)}\\
    \bottomrule
    \end{tabular}
    \label{table:VAE decoder architecture}
\end{table}

\subsubsection{Other baselines}

\begin{table}[H]
    \centering
    \caption{Hyperparameter search space}
    \begin{tabular}{lll}
    \toprule
    Condition & Hyperparameter & Search space\\
    \midrule
    \multirow{4}{*}{CMNIST} & Learning rate & \{0.0001, 0.001, 0.01\}\\
    & Weight decay & \{0, 0.001, 0.01\}\\
    & Batch size & \{32\}\\
    & Maximum epochs & \{1, 20, 100\}\\
    \midrule
    \multirow{4}{*}{Camelyon17-WILDS} & Learning rate & \{0.0001, 0.001, 0.01\}\\
    & Weight decay & \{0, 0.001, 0.01\}\\
    & Batch size & \{32\}\\
    & Maximum epochs & \{5\}\\
    \midrule
    CORAL & Penalty weight & \{0.1, 1, 10\}\\
    \midrule
    DANN & Penalty weight & \{0.1, 1, 10\}\\
    \midrule
    IRM & Penalty weight & \{1, 10, 100, 1000\}\\
    \midrule
    \multirow{2}{*}{Fish} & Pretrain steps & \{1000, 10000\}\\
    & Meta learning rate & \{0.001, 0.01, 0.1\}\\
    \midrule
    Group DRO & Step size & \{0.01\}\\
    \bottomrule
    \end{tabular}
    \label{table:hyperparameter search space}
\end{table}

\subsection{Domain generalization}

\subsubsection{Colored MNIST}
\label{appendix:CMNIST}

\paragraph{Data generating process}

The images are $32 \times 32$ pixels, and are RGB. There are two training environments and a test environment. In the first training environment, which we label $e = 0$, we set the foreground pixels in the red channel to the value one, and those in the green and blue channels to the value $y / \abs{\gY}$, where $\abs{\gY} = 10$ is the number of digits. For images with the digit zero, $y = 0$, so the digit is colored completely red. Then, as the digit increases from zero to nine, the digits are colored red, but with a decreasing intensity. In the second training environment, which we label $e = 1$, we set the foreground pixels in the green channel to the value one, and those in the red and blue channels to the value $(\abs{\gY} - 1 - y) / \abs{\gY}$. This has the effect of the digits being colored green, where the intensity increases with as the digit increases from zero to nine. In the test environment, the foreground pixels are set to one in all channels, which makes all of the digits white.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{fig/cmnist,val_test_scatter.pdf}
    \caption{In- and out-of-distribution performance are negatively correlated on CMNIST, which satisfies the assumptions made by SCBD.}
    \label{fig:cmnist,val_test_scatter}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{fig/cmnist,gen,ivae.pdf}
    \caption{Counterfactual generation with iVAE. When we swap $\rvz_c$, it changes the digit but not the color, and when we swap $\rvz_s$, it changes the color but not the digit. iVAE generates better-looking images than SCBD, since the decoder is trained jointly with the encoder for iVAE.}
    \label{fig:cmnist,gen,ivae}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{fig/cmnist,resnet18,single_block.pdf}
    \caption{CMNIST results for an ablation in which we omit $\rvz_s$, and learn a single block of latent variables $\rvz_c$ that are correlated with $y$ and invariant to $e$. These results are similar to the model that learns $\rvz_s$. We use ResNet-18 encoders here, as we did in the main text.}
    \label{fig:cmnist,resnet18,single_block}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/cmnist,z_size.pdf}
    \caption{CMNIST embedding size ($D_{\rvz_c}$ and $D_{\rvz_s}$) ablation study for SCBD with ResNet-18 encoders and $\alpha = 192$. The results are relatively consistent across different embedding sizes.}
    \label{fig:cmnist,z_size}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/cmnist,batch_size.pdf}
    \caption{CMNIST batch size ablation study for SCBD with ResNet-18 encoders and $\alpha = 192$. The results are generally better for larger batch sizes, which was also observed by the authors of SCL~\citep{khosla2020supervised}.}
    \label{fig:cmnist,batch_size}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/cmnist,weight_decay.pdf}
    \caption{CMNIST weight decay ablation study for SCBD with ResNet-18 encoders and $\alpha = 192$. The results are relatively consistent across different degrees of weight decay.}
    \label{fig:cmnist,weight_decay}
\end{figure}

\subsubsection{Camelyon17-WILDS}
\label{appendix:Camelyon17-WILDS}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{fig/camelyon17,val_test_scatter.pdf}
    \caption{In- and out-of-distribution performance are negatively correlated on Camelyon17-WILDS. This is consistent with \citet{teney2024id}, and therefore this dataset satisfies the assumptions made by SCBD.}.
    \label{fig:camelyon17,val_test_scatter}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{fig/camelyon17,resnet18.pdf}
    \caption{Camelyon17-WILDS results for SCBD using ResNet-18 encoders. The conclusions are the same as with the DenseNet-121 encoders.}
    \label{fig:camelyon17,resnet18}
\end{figure}

\begin{figure}[H]
    \centering
    \hspace*{\fill}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/camelyon17,resnet18,single_block.pdf}
        \caption{ResNet-18}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/camelyon17,densenet121,single_block.pdf}
        \caption{DenseNet-121}
    \end{subfigure}
    \hspace*{\fill}
    \caption{Camelyon17-WILDS results for an ablation in which we  omit $\rvz_s$, and learn a single block of latent variables $\rvz_c$ that are correlated with $y$ and invariant to $e$. We observe a clean trade-off between validation and test accuracy with respect to $\alpha$, but the test accuracy error bars are larger than those of the model that includes $\rvz_s$.}
    \label{fig:camelyon17,single_block}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/camelyon17,z_size.pdf}
    \caption{Camelyon17-WILDS embedding size ($D_{\rvz_c}$ and $D_{\rvz_s}$) ablation study for SCBD with DenseNet-121 encoders and $\alpha = 192$. The results are relatively consistent across different embedding sizes.}
    \label{fig:camelyon17,z_size}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/camelyon17,batch_size.pdf}
    \caption{Camelyon17-WILDS batch size ablation study for SCBD with DenseNet-121 encoders and $\alpha = 192$. The results are generally better for larger batch sizes, which was also observed by the authors of SCL~\citep{khosla2020supervised}.}
    \label{fig:camelyon17,batch_size}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/camelyon17,weight_decay.pdf}
    \caption{Camelyon17-WILDS weight decay ablation study for SCBD with DenseNet-121 encoders and $\alpha = 192$. The results are relatively consistent across different degrees of weight decay.}
    \label{fig:camelyon17,weight_decay}
\end{figure}

\subsubsection{PACS}
\label{appendix:PACS}

\begin{figure}[H]
    \centering
    \hspace*{\fill}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/pacs,art_painting,val_test_scatter.pdf}
        \caption{In- and out-of-distribution performance are positively correlated, which violates the assumptions made by SCBD.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/pacs,art_painting.pdf}
        \caption{Due to the violation of assumptions made by SCBD, increasing $\alpha$ does not lead to a trade-off between in- and out-of-distribution performance.}
    \end{subfigure}
    \hspace*{\fill}
    \caption{PACS with art painting as the test domain.}
    \label{fig:PACS,art_painting}
\end{figure}

\begin{figure}[H]
    \centering
    \hspace*{\fill}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/pacs,cartoon,val_test_scatter.pdf}
        \caption{In- and out-of-distribution performance are positively correlated, which violates the assumptions made by SCBD.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/pacs,cartoon.pdf}
        \caption{Due to the violation of assumptions made by SCBD, increasing $\alpha$ does not lead to a trade-off between in- and out-of-distribution performance.}
    \end{subfigure}
    \hspace*{\fill}
    \caption{PACS with cartoon as the test domain.}
    \label{fig:PACS,cartoon}
\end{figure}

\begin{figure}[H]
    \centering
    \hspace*{\fill}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/pacs,photo,val_test_scatter.pdf}
        \caption{In- and out-of-distribution performance are positively correlated, which violates the assumptions made by SCBD.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/pacs,photo.pdf}
        \caption{Due to the violation of assumptions made by SCBD, increasing $\alpha$ does not lead to a trade-off between in- and out-of-distribution performance.}
    \end{subfigure}
    \hspace*{\fill}
    \caption{PACS with photo as the test domain.}
    \label{fig:PACS,photo}
\end{figure}

\begin{figure}[H]
    \centering
    \hspace*{\fill}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/pacs,sketch,val_test_scatter.pdf}
        \caption{In- and out-of-distribution performance are positively correlated, which violates the assumptions made by SCBD.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/pacs,sketch.pdf}
        \caption{Due to the violation of assumptions made by SCBD, increasing $\alpha$ does not lead to a trade-off between in- and out-of-distribution performance.}
    \end{subfigure}
    \hspace*{\fill}
    \caption{PACS with sketch as the test domain.}
    \label{fig:PACS,sketch}
\end{figure}

\begin{table}[H]
    \centering
    \caption{Test accuracy (\%) for PACS with three random seeds.}
    \begin{tabular}{lccccc}
    \toprule
    Algorithm & Art painting & Cartoon & Photo & Sketch & Average\\
    \midrule
    SCBD ($\alpha = 0$) & $86.6 \pm 2.1$ & $81.3 \pm 1.9$ & $97.0 \pm 0.8$ & $70.6 \pm 4.9$ & $83.9$\\
    ERM & $88.1 \pm 0.1$ & $77.9 \pm 1.3$ & $97.8 \pm 0.0$ & $79.1 \pm 0.9$ & $85.7$\\
    CORAL & $87.7 \pm 0.6$ & $79.2 \pm 1.1$ & $97.6 \pm 0.0$ & $79.4 \pm 0.7$ & $86.0$\\
    DANN & $85.9 \pm 0.5$ & $79.9 \pm 1.4$ & $97.6 \pm 0.2$ & $75.2 \pm 2.8$ & $84.6$\\
    IRM & $85.0 \pm 1.6$ & $77.6 \pm 0.9$ & $96.7 \pm 0.3$ & $78.5 \pm 2.6$ & $84.4$\\
    Group DRO & $86.4 \pm 0.3$ & $79.9 \pm 0.8$ & $98.0 \pm 0.3$ & $72.1 \pm 0.7$ & $84.1$\\
    \bottomrule
    \end{tabular}
    \label{table:PACS}
\end{table}

\subsubsection{VLCS}
\label{appendix:VLCS}

\begin{figure}[H]
    \centering
    \hspace*{\fill}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/vlcs,Caltech101,val_test_scatter.pdf}
        \caption{In- and out-of-distribution performance are positively correlated, which violates the assumptions made by SCBD.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/vlcs,Caltech101.pdf}
        \caption{Due to the violation of assumptions made by SCBD, increasing $\alpha$ does not lead to a trade-off between in- and out-of-distribution performance.}
    \end{subfigure}
    \hspace*{\fill}
    \caption{VLCS with Caltech101 as the test domain.}
    \label{fig:VLCS,Caltech101}
\end{figure}

\begin{figure}[H]
    \centering
    \hspace*{\fill}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/vlcs,LabelMe,val_test_scatter.pdf}
        \caption{In- and out-of-distribution performance are positively correlated, which violates the assumptions made by SCBD.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/vlcs,LabelMe.pdf}
        \caption{Due to the violation of assumptions made by SCBD, increasing $\alpha$ does not lead to a trade-off between in- and out-of-distribution performance.}
    \end{subfigure}
    \hspace*{\fill}
    \caption{VLCS with LabelMe as the test domain.}
    \label{fig:VLCS,LabelMe}
\end{figure}

\begin{figure}[H]
    \centering
    \hspace*{\fill}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/vlcs,SUN09,val_test_scatter.pdf}
        \caption{In- and out-of-distribution performance are positively correlated, which violates the assumptions made by SCBD.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/vlcs,SUN09.pdf}
        \caption{Due to the violation of assumptions made by SCBD, increasing $\alpha$ does not lead to a trade-off between in- and out-of-distribution performance.}
    \end{subfigure}
    \hspace*{\fill}
    \caption{VLCS with SUN09 as the test domain.}
    \label{fig:VLCS,SUN09}
\end{figure}

\begin{figure}[H]
    \centering
    \hspace*{\fill}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/vlcs,VOC2007,val_test_scatter.pdf}
        \caption{In- and out-of-distribution performance are positively correlated, which violates the assumptions made by SCBD.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/vlcs,VOC2007.pdf}
        \caption{Due to the violation of assumptions made by SCBD, increasing $\alpha$ does not lead to a trade-off between in- and out-of-distribution performance.}
    \end{subfigure}
    \hspace*{\fill}
    \caption{VLCS with VOC2007 as the test domain.}
    \label{fig:VLCS,VOC2007}
\end{figure}

\begin{table}[H]
    \centering
    \caption{Test accuracy (\%) for VLCS with three random seeds.}
    \begin{tabular}{lccccc}
    \toprule
    Algorithm & Caltech101 & LabelMe & SUN09 & VOC2007 & Average\\
    \midrule
    SCBD ($\alpha = 0$) & $94.7 \pm 1.8$ & $67.1 \pm 1.3$ & $74.7 \pm 3.0$ & $71.4 \pm 1.3$ & $77.0$\\
    ERM & $97.6 \pm 1.0$ & $63.3 \pm 0.9$ & $72.2 \pm 0.5$ & $76.4 \pm 1.5$ & $77.4$\\
    CORAL & $98.8 \pm 0.1$ & $64.6 \pm 0.8$ & $71.7 \pm 1.4$ & $75.8 \pm 0.4$ & $77.7$\\
    DANN & $98.5 \pm 0.2$ & $64.9 \pm 1.1$ & $73.1 \pm 0.7$ & $78.3 \pm 0.3$ & $78.7$\\
    IRM & $97.6 \pm 0.3$ & $65.0 \pm 0.9$ & $72.2 \pm 0.5$ & $76.4 \pm 1.5$ & $78.1$\\
    Group DRO & $97.7 \pm 0.4$ & $62.5 \pm 1.1$ & $70.1 \pm 0.7$ & $78.4 \pm 0.9$ & $77.2$\\
    \bottomrule
    \end{tabular}
    \label{table:VLCS}
\end{table}

\subsection{Batch correction}
\label{appendix:batch correction}

\paragraph{CORUM prediction}

Our CORUM prediction task mirrors that of \citep{wang2023multi}, with some modifications to ensure a fair comparison with CellProfiler. We begin by computing $\rvz_c$ for every single-cell image in the dataset, including the training, validation, and test sets. Then, we discard all embeddings for which we do not have a corresponding CellProfiler embedding. The median number of cells per gene is \num{6000}, and we want to average them to obtain a single embedding per gene. We have four sgRNA sequences for each perturbed gene, and 250 sgRNA sequences for the non-targeting control. We first average the $\rvz_c$'s across cells for each sgRNA sequence, and then average the resulting sgRNA embeddings that correspond to the same gene. For each gene embedding, we subtract the non-targeting control embedding, then standardize such that each of the 64 components has mean zero and unit variance.

Then, we incorporate the CORUM database, which defines the pairs of genes that belong to the same protein complex. We discard all gene embeddings that are not in this database. We compute the cosine similarity between each pair of gene embeddings, and interpret it as the prediction that they belong to the same family. The prediction target is one if they belong to the same family according to the CORUM database, and zero otherwise. We turn the cosine similarities into binary predictions by across various prediction thresholds by using the $i$'th percentile as the upper threshold and the $100 - i$'th percentile as the lower threshold for each integer $i \in \{80, \dotsc, 100\}$. Finally, we use the binary predictions and prediction targets to obtain a precision and recall at each value of $i$, and plot the precision and recall curve.

\paragraph{Batch prediction}

We begin by computing the perturbation embedding for every example in the dataset, including the training, validation, and test sets. We then discard all examples for which we do not have a corresponding CellProfiler embedding. Using a randomly sampled $60\%$ of the data as the training set, and the remaining data as the test set, we apply logistic regression to predict $e$ given the embeddings, and report the F1 score on the test set.

\section{Code}

Here is our implementation of the supervised contrastive and invariance losses.
\begin{verbatim}
def supcon_loss(z, u, temperature):
    batch_size = len(z)
    u_col = u.unsqueeze(1)
    u_row = u.unsqueeze(0)
    mask_pos = (u_col == u_row).float()
    offdiag_mask = 1. - torch.eye(batch_size)
    mask_pos = mask_pos * offdiag_mask
    logits = torch.matmul(z, z.T) / temperature
    p = mask_pos / mask_pos.sum(dim=1, keepdim=True).clamp(min=1.)
    q = F.log_softmax(logits, dim=1)
    return F.cross_entropy(q, p)

def invariance_loss(zc, e, temperature):
    batch_size = len(zc)
    e_col = e.unsqueeze(1)
    e_row = e.unsqueeze(0)
    mask_pos = (e_col == e_row).float()
    mask_neg = 1. - mask_pos
    offdiag_mask = 1. - torch.eye(batch_size)
    mask_pos = mask_pos * offdiag_mask
    logits = torch.matmul(zc, zc.T) / temperature
    q = F.log_softmax(logits, dim=1)
    log_prob_pos = (q * mask_pos).mean(dim=1)
    log_prob_neg = (q * mask_neg).mean(dim=1)
    return (log_prob_pos - log_prob_neg).abs().mean()
\end{verbatim}

\end{document}