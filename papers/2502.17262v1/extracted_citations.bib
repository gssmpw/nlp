@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{alabdulmohsin2022scaling,
  title={Revisiting Neural Scaling Laws in Language and Vision},
  author={Alabdulmohsin, Ibrahim M and Neyshabur, Behnam and Zhai, Xiaohua},
  journal=NIPS,
  volume={35},
  pages={22300--22312},
  year={2022}
}

@article{chen2024scaling,
  title={Scaling Laws for Predicting Downstream Performance in LLMs},
  author={Chen, Yangyi and Huang, Binxuan and Gao, Yifan and Wang, Zhengyang and Yang, Jingfeng and Ji, Heng},
  journal={arXiv preprint arXiv:2410.08527},
  year={2024}
}

@article{du2024understanding,
  title={Understanding emergent abilities of language models from the loss perspective},
  author={Du, Zhengxiao and Zeng, Aohan and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2403.15796},
  year={2024}
}

@article{gadre2024language,
  title={Language models scale reliably with over-training and on downstream tasks},
  author={Gadre, Samir Yitzhak and Smyrnis, Georgios and Shankar, Vaishaal and Gururangan, Suchin and Wortsman, Mitchell and Shao, Rulin and Mercat, Jean and Fang, Alex and Li, Jeffrey and Keh, Sedrick and others},
  journal={arXiv preprint arXiv:2403.08540},
  year={2024}
}

@article{henighan2020scaling,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Maxwell and Levskaya, Anselm and McCandlish, Sam and Stuhlmuller, Andreas and Gray, Scott and Amodei, Dario},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}

@article{hernandez2021scaling,
  title={Scaling laws for transfer},
  author={Hernandez, Danny and Kaplan, Jared and Henighan, Tom and McCandlish, Sam},
  journal={arXiv preprint arXiv:2102.01293},
  year={2021}
}

@inproceedings{hu2023predicting,
  title={Predicting Emergent Abilities with Infinite Resolution Evaluation},
  author={Hu, Shengding and Liu, Xin and Han, Xu and Zhang, Xinrong and He, Chaoqun and Zhao, Weilin and Lin, Yankai and Ding, Ning and Ou, Zebin and Zeng, Guoyang and others},
  booktitle=ICLR,
  year={2024}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{lingle2024hyperparameters,
  title={A Large-Scale Exploration of $\mu$-Transfer},
  author={Lingle, Lucas},
  journal={arXiv preprint arXiv:2404.05728},
  year={2024}
}

@article{ma2024scaling,
  title={Do Neural Scaling Laws Exist on Graph Self-Supervised Learning?},
  author={Ma, Qian and Mao, Haitao and Liu, Jingzhe and Zhang, Zhehua and Feng, Chunlin and Song, Yu and Shao, Yihan and Ma, Yao},
  journal={arXiv preprint arXiv:2408.11243},
  year={2024}
}

@article{owen2024predictable,
  title={How predictable is language model benchmark performance?},
  author={Owen, David},
  journal={arXiv preprint arXiv:2401.04757},
  year={2024}
}

@inproceedings{schaeffer2024emergent,
  author       = {Rylan Schaeffer and
                  Brando Miranda and
                  Sanmi Koyejo},
  title        = {Are Emergent Abilities of Large Language Models a Mirage?},
  booktitle    = NIPS,
  year         = {2023}
}

@article{shao2024scaling,
  title={Scaling Retrieval-Based Language Models with a Trillion-Token Datastore},
  author={Shao, Rulin and He, Jacqueline and Asai, Akari and Shi, Weijia and Dettmers, Tim and Min, Sewon and Zettlemoyer, Luke and Koh, Pang Wei},
  journal={arXiv preprint arXiv:2407.12854},
  year={2024}
}

@article{tao2024scaling,
  title={Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies},
  author={Tao, Chaofan and Liu, Qian and Dou, Longxu and Muennighoff, Niklas and Wan, Zhongwei and Luo, Ping and Lin, Min and Wong, Ngai},
  journal={arXiv preprint arXiv:2407.13623},
  year={2024}
}

@inproceedings{tay2021scale,
  author       = {Yi Tay and
                  Mostafa Dehghani and
                  Jinfeng Rao and
                  William Fedus and
                  Samira Abnar and
                  Hyung Won Chung and
                  Sharan Narang and
                  Dani Yogatama and
                  Ashish Vaswani and
                  Donald Metzler},
  title        = {Scale Efficiently: Insights from Pretraining and Finetuning Transformers},
  booktitle    = ICLR,
  year         = {2022}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{xiao2024densing,
  title={Densing Law of LLMs},
  author={Xiao, Chaojun and Cai, Jie and Zhao, Weilin and Zeng, Guoyang and Han, Xu and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2412.04315},
  year={2024}
}

@article{yang2022hyperparameters,
  title={Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer},
  author={Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2203.03466},
  year={2022}
}

@inproceedings{zhai2022scaling,
  author       = {Xiaohua Zhai and
                  Alexander Kolesnikov and
                  Neil Houlsby and
                  Lucas Beyer},
  title        = {Scaling Vision Transformers},
  booktitle    = CVPR,
  pages        = {1204--1213},
  year         = {2022}
}

