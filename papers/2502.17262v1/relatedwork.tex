\section{Related Work}
\label{sec:relatedwork}

\subsection{Loss Scaling Laws}
Loss scaling laws provide a systematic framework for understanding the relationship between computational resources, data, model size, and the final performance of LLMs. Early work by \citet{kaplan2020scaling} demonstrates that the pre-training loss of LLMs follows a power-law relationship with the compute (the number of floating-point operations) used in training. Subsequent studies extend these findings to other domains, such as computer vision~\citep{zhai2022scaling}, graph learning~\citep{ma2024scaling} and vision-language models~\citep{alabdulmohsin2022scaling, henighan2020scaling}. Recent research has also explored scaling laws in specific contexts, such as fine-tuning \citep{hernandez2021scaling, tay2021scale}, vocabulary size optimization \citep{tao2024scaling}, retrieval-augmented models \citep{shao2024scaling}, and hyperparameter tuning \citep{lingle2024hyperparameters, yang2022hyperparameters}. These studies highlight the broad applicability of scaling laws and their potential to guide the efficient allocation of computational resources.


\subsection{Downstream Task Performance Scaling}

Predicting downstream task performance remains a critical challenge due to emergent abilities in LLMs that manifest only after exceeding task-specific thresholds \citep{wei2022emergent,schaeffer2024emergent}. 
Recent works, such as using loss as a proxy \citep{chen2024scaling} or increasing metric resolution \citep{hu2023predicting}, have demonstrated potential but encounter challenges in aligning surrogate metrics with original task objectives. Here, we briefly review the two main types of methods for predicting downstream performance:

\noindent\textbf{1. Loss-intermediate prediction.}
These methods predict the final training loss (or in-domain validation loss) of LLMs with loss scaling laws first, and then predict downstream performance through loss-performance relationships~\citep{chen2024scaling, gadre2024language, du2024understanding}. While these methods leverage established scaling laws for loss predictions, they encounter a fundamental limitation: the inconsistent mapping between loss and performance metrics. In addition, \citet{xiao2024densing} employ the evaluation set answer loss as an intermediate variable for estimation. Although answer loss correlates with the final performance metrics, its predictability remains low as predicting answer loss shares the challenges with predicting performance, including emergence phenomenon and high variance in task difficulty.


\noindent\textbf{2. End-to-end performance-compute prediction.}
These methods~\cite{hu2023predicting, owen2024predictable, achiam2023gpt} directly model the relationship between performance and compute (or the number of model parameters). Additionally, \citet{achiam2023gpt} estimate and fit this relationship using a subset of the evaluation set. \citet{hu2023predicting} address the challenge of non-emergent capabilities in smaller models by employing multiple non-greedy decoding evaluations, thereby enabling accurate extrapolation of performance predictions for models with up to 2.4B parameters.



% , which predicts $-\log(\mathrm{Answer}|\mathrm{Instruction})$ for tasks in an evaluation set


% 也有研究采用测试集answer loss作为中间变量进行预估，即$-\log(\mathrm{Answer}|\mathrm{Instruction})$~\citep{xiao2024densing}。然而虽然answer loss与最终指标具有关联性，但是answer loss本身的可预估性较低，也同样收到评估集涌现现象与难度方差大等因素影响。





%