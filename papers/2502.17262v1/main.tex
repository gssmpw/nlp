\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
%\usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}
\usepackage{colortbl}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables


\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{amsmath} 
\usepackage{amssymb}
% \usepackage{arydshln}
\usepackage[capitalize]{cleveref}
\usepackage{multirow} 
\usepackage{subcaption}  
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumitem} 
\usepackage{xcolor}
\usepackage{wrapfig}
\usepackage{framed}
\usepackage{fancyhdr}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}

\usepackage{soul}
\usepackage{xcolor}

% \usepackage{graphicx}    
% \usepackage{titling} 



% 定义浅绿背景
\definecolor{lightgreen}{RGB}{144, 238, 144}
\newcommand{\greenhl}[1]{\sethlcolor{lightgreen}\hl{#1}}

% 定义浅红背景
\definecolor{lightred}{RGB}{255, 182, 193}
\newcommand{\redhl}[1]{\sethlcolor{lightred}\hl{#1}}

% \usepackage[ruled,linesnumbered]{algorithm2e}

\usepackage{xspace}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
% \def\eg{\textit{e.g.}}
% \def\Eg{\textit{E.g.}}
% \def\ie{\textit{i.e.}}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\textit{e.g}\onedot} \def\Eg{\textit{E.g}\onedot}
\def\ie{\textit{i.e}\onedot} \def\Ie{\textit{I.e}\onedot}
\def\cf{\textit{cf}\onedot} \def\Cf{\textit{Cf}\onedot}
\def\etc{\textit{etc}\onedot} \def\vs{\textit{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\iid{i.i.d\onedot} \def\wolog{w.l.o.g\onedot}
\def\etal{\textit{et al}\onedot}
\def\bbE{\mathbb{E}}
\makeatother

\newcommand{\xiao}[1]{{\color{red}{\bf\sf [lx: #1]}}}

% \title{Clustering by Difficulty: Precisely Predicting the Scaling Performance of LLMs on Downstream Tasks}
% \title{Accurate Performance scaling of Language Models with Predictable Clusters}
%\title{Unveiling the Predictability and Scalability of LLMs on Downstream Tasks: A Clustering-Based Perspective}
%\title{Reveal Downstream Tasks Predictability and Scalability: a Clustering-based Method}
\title{Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective}
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Chengyin Xu\thanks{Equal contribution.}, \ \ Kaiyuan Chen\footnotemark[1], \ \ Xiao Li, \ \ Ke Shen, \ \ Chenggang Li \\
  Seed-LLM, ByteDance \\
  {\small \texttt{\{xuchengyin.98, chenkaiyuan.99, lixiao.20, shenke, lichenggang\}@bytedance.com}}
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

% \fancypagestyle{firstpage}{
%      \fancyhf{}
%      \fancyhead[L]{\includegraphics[width=100pt]{figures/seed-logo.png}}
%      \renewcommand{\headrulewidth}{0.5pt}
%  }

\begin{document}
\maketitle

%不需要目录就注释掉 注意目录不要和第一页放在一块 要有\newpage
%\newpage
%\tableofcontents
%\newpage

\begin{abstract}

The rapid advancements in computing dramatically increase the scale and cost of training Large Language Models (LLMs). Accurately predicting downstream task performance prior to model training is crucial for efficient resource allocation, yet remains challenging due to two primary constraints: 
(1) the ``emergence phenomenon'', wherein downstream performance metrics become meaningful only after extensive training, which limits the ability to use smaller models for prediction;
(2) Uneven task difficulty distributions and the absence of consistent scaling laws, resulting in substantial metric variability.
Existing performance prediction methods suffer from limited accuracy and reliability, thereby impeding the assessment of potential LLM capabilities.
To address these challenges, we propose a \textit{Clustering-On-Difficulty} (COD) downstream performance prediction framework. 
%
COD first constructs a predictable support subset by clustering tasks based on difficulty features, strategically excluding non-emergent and non-scalable clusters. The scores on the selected subset serve as effective intermediate predictors of downstream performance on the full evaluation set. 
% We further map the predicted subset score to the final prediction for the full evaluation set.
%
With theoretical support, we derive a mapping function that transforms performance metrics from the predictable subset to the full evaluation set, thereby ensuring accurate extrapolation of LLM downstream performance.
% we further derive the fitting curve for extrapolation on the subset each difficulty cluster.
 % establish the scaling law for downstream tasks for performance-compute extrapolation and
%
The proposed method has been applied to predict performance scaling for a 70B LLM, providing actionable insights for training resource allocation and assisting in monitoring the training process.
%
% Empirical evaluations demonstrate that COD achieves robust predictions, with an absolute mean deviation of 1.36\% across eight important LLM evaluation sets.
Notably, COD achieves remarkable predictive accuracy on the 70B LLM by leveraging an ensemble of small models, demonstrating an absolute mean deviation of 1.36\% across eight important LLM evaluation benchmarks.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Large Language Models (LLMs) have emerged as transformative technologies in natural language understanding, generation, and reasoning~\cite{achiam2023gpt, guo2025deepseek, bubeck2023sparks}. Their impressive success heavily relies on scaling model parameters and pre-training data, with training loss empirically following a power-law relationship with compute~\cite{hoffmann2022training, kaplan2020scaling}. However, this reduction in training loss primarily reflects an in-domain compression effect and does not necessarily indicate improved out-of-domain generalization or downstream performance--the factor of primary concern in practice. Specifically, performance scaling of downstream tasks aims to predict the accuracy of the target LLM on downstream tasks using metrics from smaller models. Our objective is to develop a prediction method that works reliably across a diverse range of downstream tasks, minimizing the worst-case prediction error.

Despite extensive efforts, a reliable \textit{scaling law for downstream tasks} remains elusive. One line of work attempts to extrapolate large-model performance by modeling the performance-loss relationship~\cite{chen2024scaling,gadre2024language,du2024understanding,xiao2024densing,owen2024predictable}, but this often fails to capture the emergent behaviors of LLMs and the mismatch between in-domain loss and downstream metrics~\cite{zhang2021understanding}. Another line of research focuses on direct extrapolation of performance-compute relationship~\cite{achiam2023gpt,hu2023predicting}, yet the uneven difficulty distribution across different evaluation samples undermines its accuracy. We observe that different evaluation samples actually follow distinct performance scaling patterns, and thus applying a single extrapolation formula to the entire evaluation set is suboptimal. We give the detailed analysis in \cref{sec:pilot}.

To address these challenges, we propose a new performance scaling law, derived from the existing loss scaling law \cite{kaplan2020scaling}, specifically applicable to evaluation subsets that exhibit consistent performance scaling patterns. Building on the performance scaling law, we develop a \textit{Clustering-On-Difficulty} (COD) multi-stage framework for predicting downstream performance. Specifically, we first cluster tasks by their difficulty features, and then filter out clusters that lack valid extrapolation patterns. Next, we fit the performance-compute relationships in the remaining clusters under our performance scaling law, extrapolate the performance of large models within these clusters, and finally map the aggregated predictions to the complete task set.

We validate our COD approach on eight evaluation sets, including popular MATH~\cite{hendrycks2021measuring}, BBH~\cite{suzgun2023challenging}, and MMLU pro~\cite{wang2024mmlu}. COD achieves an average prediction error of 1.36\% on a 70B-parameter LLM. Our results demonstrate that this difficulty-aware framework substantially outperforms existing methods, establishing a promising paradigm for accurate downstream performance scaling of LLMs.

% In conclusion, we list following contributions.
Our contributions can be summarized as follows:
\begin{itemize}
\item We propose the COD framework to address high variance and emergent phenomena in the LLM performance by effectively modeling the difficulty distribution within the evaluation sets.
\item We introduce a performance scaling law for cluster-wise performance prediction, with theoretical support and experimental validation.
\item Extensive experiments conducted across eight diverse evaluation sets demonstrate that COD achieves a state-of-the-art average prediction error of 1.36\% on a 70B-parameter LLM.
\end{itemize}



% \label{sec:intro}
% Large Language Models (LLMs) have emerged as transformative technologies across multiple domains of artificial intelligence, demonstrating unprecedented capabilities in natural language understanding, generation, and complex reasoning tasks \cite{achiam2023gpt, guo2025deepseek}, generally recognized as one of the most promising ways towards Artificial General Intelligence (AGI) \cite{bubeck2023sparks}. The impressive success of these LLMs relies heavily on the scaling of model parameters and the pre-training data. It has been widely observed that the training loss of LLMs follows a power-law relationship with the compute, also known as the (loss) scaling law for LLMs~\cite{hoffmann2022training, kaplan2020scaling}. Scaling law provides guidance for the scientific scaling of LLMs, enabling developers to estimate the loss before training. However, the reduced training loss primarily reflects the in-domain performance of LLMs on the training corpus, which does not necessarily indicate improved downstream performance -- the metric of primary concern in the development of LLMs.
% % The rapid advancement in parallel computing capabilities has dramatically increased the scale and cost of training Large Language Models (LLMs). 
% % Training Large Language Models
% %
% % Scaling Law指出我们可以通过训练一次小模型来预测大模型的Final loss，从而预判规模增长带来的模型能力提升。
% %
% % 然而，Final loss更多表明模型对训练数据in-domain的拟合表现，与我们希望模型在下游任务中的能力表现具有Gap
% %
% % The loss scaling law indicates that we can predict the final loss of a large model by training a smaller model once, thereby forecasting the performance improvement brought by scaling up the model size~\cite{hoffmann2022training, kaplan2020scaling}.
% %
% % However, the final loss mainly reflects the model's in-domain fitting performance on the training corpus, which has a gap with the performance we expect the model to achieve on downstream tasks.


% Unveiling the scaling law for downstream performance has become increasingly critical for informed resource allocation~\cite{achiam2023gpt}, which is more challenging due to the complex scaling behaviors and high variability in metrics.
% %
% First, unlike training loss, downstream task performance exhibits an ``emergence phenomenon'' where capabilities only manifest after substantial computational investment. This characteristic severely limits the utility of small-scale models for evaluation purposes. 
% %
% Second, the uneven distribution of task difficulty across evaluation samples, combined with inconsistent scaling patterns in certain tasks, leads to significant performance volatility during training.

% % 已有研究采用了两类方法：1. End-to-end直接外推小模型到大模型Performance-Compute曲线，该方法为能考虑评估任务内不同题目难度分布的差异，同时预估结果准确性在不同评估任务之间不一致，可信度较低；2. 先预估大模型的Final Loss，然后利用不同规模模型的Performance-Loss曲线完成预估，该方法假设了Loss与下游任务表现之间的一致性，但实际上，Loss更侧重Indomain的压缩效果，下游任务表现更侧重Out-of-domain的泛化能力。并且如学习率、优化器参数、模型结构等诸多因素会对Loss产生干扰，导致相同Loss下，模型能力有可能具有明显差异。
% Existing researches mainly adopt two categories:  
% %
% 1. End-to-end extrapolation of the Performance-Compute curve from small models to large models~\cite{achiam2023gpt,hu2023predicting}. These methods, while able to measure growth patterns of the overall performance of the evaluation sets, show inconsistent prediction accuracy across different evaluation tasks, resulting in low reliability.  
% %
% 2. First predicting the final loss of large models and then using the Performance-Loss curve across different model scales to make predictions~\cite{chen2024scaling,gadre2024language,du2024understanding,xiao2024densing,owen2024predictable}. These methods assume a consistency between training loss and downstream task performance, which does not generally hold. Loss primarily reflects the in-domain compression effect, while downstream task performance focuses more on out-of-domain generalization ability. Furthermore, factors such as learning rate, optimizer parameters, and model architecture may interfere with the loss, leading to significant differences in model performance even with the same loss~\cite{zhang2021understanding}.

% % Downstream Task Performance外推公式的选择尚未形成共识，已有工作结合Task Answer Loss与Passrate之间的指数关系，以及Loss幂率Scaling Law给出Performance scaling公式。我们推导得到只有在Task Passrate接近的情况下，这一规律才能近似成立。而实际评估集的题目难度方差是极大的。这引导我们采用难度聚类的方式分组完成预估任务。
% There is no consensus on the choice of extrapolation formulae for Downstream Task Performance. Previous studies have proposed Performance scaling formulae by combining the exponential relationship between Task Answer Loss and Pass Rate with the power Loss Scaling Law~\cite{hu2023predicting}. Our derivation reveals that this relationship holds approximately only when tasks within an evaluation set have similar difficulty features. However, practical evaluation sets exhibit substantial variance in task difficulty. This observation motivates our adoption of difficulty-based clustering to reduce inner-group passrate variance.

% To address these challenges, we propose \textbf{C}lustering-\textbf{O}n-\textbf{D}ifficulty multi-stage downstream performance prediction framework comprising four key steps:

% \begin{enumerate}
%     \item \textbf{Clustering:} Geven an evaluation task set, We acquire the difficulty feature of each task with its passrates on a series of language models with different sizes and training tokens. We cluster tasks on difficulty features to acquire task subsets with similar performance scaling properties.
    
%     \item \textbf{Fitting:} For each task cluster, we fit its performance-compute curve based on evaluations of a group of small models with the same training tokens (D) / model parameters (N) ratio as the target large model. We remove both outliers and tasks where small models fail to demonstrate above-random performance.
    
%     \item \textbf{Extrapolation:} For clusters exhibiting consistent scaling patterns, we extrapolate large model performance using the fitted prediction curves. The aggregate predictions for extrapolatable samples are obtained through cluster-size-weighted averaging.
    
%     \item \textbf{Mapping:} We handle outlier samples and non-scaling clusters by establishing interpolation-based mapping relationships between extrapolatable subset metrics and full evaluation set performance. This approach enables us to predict performance for the complete evaluation task set.
% \end{enumerate}

% Empirical validation demonstrates the robust accuracy of our approach. Across multiple downstream freeform benchmarks, including MATH~\cite{hendrycks2021measuring}, BBH~\cite{suzgun2023challenging}, MMLU pro~\cite{wang2024mmlu}, etc. 
% %
% We also perform comprehensive ablation studies on usage performance scaling functions and the selection of clustering algorithms. 
% % To the best of our knowledge, we are first work to provide robust performance scaling prediction on large language models.

% The contributions of this work can be summarized as follows:
% \begin{itemize}
%     % 我们提出了一种Novel的基于聚类的大语言模型下游任务预估的方法，建模了评估集的难度分布以及能够有效应对具有能力涌现特点的评估集；
%     % 我们给出了具有可解释性的Perfermance Scaling Law，并提供了理论支撑。
%     % 我们在把所提方法应用在70B大模型的能力预估上，在8个语言模型的多维度核心评估取得了1.36的平均预测误差，显著优于已有方法。
%     \item We propose a novel clustering-based method for predicting the downstream task performance of large language models. This method models the difficulty distribution of evaluation sets and effectively handles tasks requiring emergent capabilities.

%     \item We present a Performance scaling Law characterized by interpretable hyperparameters, grounded in theoretical foundations, and validated through empirical demonstrations of predictive accuracy.
    
%     \item We achieve 1.36\% average prediction error on the target 70B-parameter large language model across popular multidimensional evaluations of 8 evaluation sets, significantly outperforming existing methods.
% \end{itemize}


\section{Related Work}
\label{sec:relatedwork}

\subsection{Loss Scaling Laws}
Loss scaling laws provide a systematic framework for understanding the relationship between computational resources, data, model size, and the final performance of LLMs. Early work by \citet{kaplan2020scaling} demonstrates that the pre-training loss of LLMs follows a power-law relationship with the compute (the number of floating-point operations) used in training. Subsequent studies extend these findings to other domains, such as computer vision~\citep{zhai2022scaling}, graph learning~\citep{ma2024scaling} and vision-language models~\citep{alabdulmohsin2022scaling, henighan2020scaling}. Recent research has also explored scaling laws in specific contexts, such as fine-tuning \citep{hernandez2021scaling, tay2021scale}, vocabulary size optimization \citep{tao2024scaling}, retrieval-augmented models \citep{shao2024scaling}, and hyperparameter tuning \citep{lingle2024hyperparameters, yang2022hyperparameters}. These studies highlight the broad applicability of scaling laws and their potential to guide the efficient allocation of computational resources.


\subsection{Downstream Task Performance Scaling}

Predicting downstream task performance remains a critical challenge due to emergent abilities in LLMs that manifest only after exceeding task-specific thresholds \citep{wei2022emergent,schaeffer2024emergent}. 
Recent works, such as using loss as a proxy \citep{chen2024scaling} or increasing metric resolution \citep{hu2023predicting}, have demonstrated potential but encounter challenges in aligning surrogate metrics with original task objectives. Here, we briefly review the two main types of methods for predicting downstream performance:

\noindent\textbf{1. Loss-intermediate prediction.}
These methods predict the final training loss (or in-domain validation loss) of LLMs with loss scaling laws first, and then predict downstream performance through loss-performance relationships~\citep{chen2024scaling, gadre2024language, du2024understanding}. While these methods leverage established scaling laws for loss predictions, they encounter a fundamental limitation: the inconsistent mapping between loss and performance metrics. In addition, \citet{xiao2024densing} employ the evaluation set answer loss as an intermediate variable for estimation. Although answer loss correlates with the final performance metrics, its predictability remains low as predicting answer loss shares the challenges with predicting performance, including emergence phenomenon and high variance in task difficulty.


\noindent\textbf{2. End-to-end performance-compute prediction.}
These methods~\cite{hu2023predicting, owen2024predictable, achiam2023gpt} directly model the relationship between performance and compute (or the number of model parameters). Additionally, \citet{achiam2023gpt} estimate and fit this relationship using a subset of the evaluation set. \citet{hu2023predicting} address the challenge of non-emergent capabilities in smaller models by employing multiple non-greedy decoding evaluations, thereby enabling accurate extrapolation of performance predictions for models with up to 2.4B parameters.



% , which predicts $-\log(\mathrm{Answer}|\mathrm{Instruction})$ for tasks in an evaluation set


% 也有研究采用测试集answer loss作为中间变量进行预估，即$-\log(\mathrm{Answer}|\mathrm{Instruction})$~\citep{xiao2024densing}。然而虽然answer loss与最终指标具有关联性，但是answer loss本身的可预估性较低，也同样收到评估集涌现现象与难度方差大等因素影响。





% \section{Related Work}
% \label{sec:relatedwork}

% \subsection{Loss Scaling Laws}
% Loss scaling laws provide a systematic framework for understanding the relationship between computational resources, data, model size, and the final performance of LLMs. Early work by \citet{kaplan2020scaling} demonstrates that the pre-training loss of LLMs follows a power-law relationship with the compute (the number of floating-point operations) used in training. Subsequent studies extend these findings to other domains, such as computer vision~\citep{zhai2022scaling}, graph learning~\citep{ma2024scaling} and vision-language models~\citep{alabdulmohsin2022scaling, henighan2020scaling}. Recent research has also explored scaling laws in specific contexts, such as fine-tuning \citep{hernandez2021scaling, tay2021scale}, vocabulary size optimization \citep{tao2024scaling}, retrieval-augmented models \citep{shao2024scaling}, and hyperparameter tuning \citep{lingle2024hyperparameters, yang2022hyperparameters}. These studies highlight the broad applicability of scaling laws and their potential to guide the efficient allocation of computational resources.


% \subsection{Downstream Task Performance scaling}

% Despite progress, estimating downstream task performance remains a critical challenge due to emergent abilities in LLMs that only appear after exceeding task-specific thresholds \citep{wei2022emergent}. 
% Previous approaches, such as using answer loss as a proxy \citep{schaeffer2024emergent} or increasing metric resolution \citep{hu2023predicting}, have shown promise but face difficulties in aligning surrogate metrics with original task goals. 

% Current literature presents two mainstream approaches for predicting large language models' downstream task performance:

% \noindent\textbf{Loss-intermediate Prediction Method}
% These approaches first predict the large model's final loss using loss scaling laws, and then estimate performance based on loss-performance relationships~\citep{chen2024scaling, gadre2024language, du2024understanding, xiao2024densing}. While these methods benefit from accurate loss predictions using established scaling laws, they face a fundamental issue: the inconsistency in loss-to-performance mapping.

% The popular assumption that models with identical loss values achieve similar downstream task performance proves problematic. Our observations indicate that even with identical training data, the loss-performance relationship is influenced by multiple factors, including learning rate, batch size, model size, weight decay, dropout rate, model depth, model architecture, etc.

% % \begin{figure*}[htbp]
% %     \parbox{0.3\textwidth}{
% %         \caption{\raggedright Performance-Loss relationship across difference model size (left) and learning rate schedule (right). At the same loss values, smaller models or models with lower learning rates generally achieve higher accuracy than larger ones.}
% %     }
% %     \hfill
% %     \parbox{0.7\textwidth}{
% %         \centering
% %         \includegraphics[width=\linewidth]{figures/related_works/ff_gsm8k_cot_v1_avg_score_loss_perf.pdf}
% %         \label{fig:perf-loss}
% %     }
% % \end{figure*}



% For example, compared to the same level of training loss, we observe that smaller models achieve higher performance on CEval~\cite{nguyen2024ceval} and the model with a constant learning rate schedule performs better than the cosine scheduler on GSM8k~\cite{cobbe2021training}, as shown in \cref{fig:perf-loss}. This phenomenon arises because downstream tasks are Out-Of-Domain (OOD) relative to training data, while training loss is an In-Domain (ID) metric. The disparity between fitting and generalization capacity leads to inconsistent performance between OOD and ID scenarios. 
% Larger models, which have stronger fitting capacity, can reach a lower loss state more efficiently. In contrast, smaller models require a larger volume of data to attain the same training loss, thus performing better generalization.

% % 不同颜色的曲线表示不同参数量模型训练过程中CEval评估集Accuracy和Training Loss的关系。不同Size模型Acc-Loss关系显著不同，并且相同Loss下，小模型指标高于大模型，图中Loss=2.25是，973M模型Acc高于1.9B及以上大小的模型。
% % \begin{figure}[htbp]
% %     \centering
% %     \includegraphics[width=0.3\textwidth]{figures/related_works/bbh_loss_perf.pdf}
% %     \hspace{0.03\textwidth}  % 调整两图之间的间距
% %     \includegraphics[width=0.3\textwidth]{figures/related_works/math_loss_perf.pdf}
% %     \hspace{0.03\textwidth}  % 调整两图之间的间距
% %     \includegraphics[width=0.3\textwidth]{figures/related_works/MMLU_loss_perf.pdf}
% %     \caption{Performance-Loss relationship across difference model size.}
% % \label{fig:tsne_vis}
% % \end{figure}



% % \begin{enumerate}
% %     \item Limited model size ranges with dense sampling, leading to clustered data points;
% %     \item Inadequate smoothing of loss and performance values, resulting in high volatility on performance;
% %     % \item Coincidental alignment of late-stage small model performance with early/mid-stage larger model metrics due to learning rate decay in cosine scheduling.
% % \end{enumerate}
% %
% The learning rate schedule methods also influence downstream performance under the same training loss. Compared to traditional cosine scheduling, Warmup-Stable-Decay (WSD) scheduling shows significant improvements in downstream task performance~\cite{hu2024minicpm}. The loss-performance curves closely follow learning rate patterns, demonstrating high sensitivity to learning rate values, as shown in~\cref{fig:perf-loss}(right).

% \noindent\textbf{End-to-end Performance-Computation Prediction}
% Existing approaches~\cite{hu2023predicting, owen2024predictable} directly fit the relationship between performance and computation (or the number of model parameters). Additionally, there are methods~\cite{achiam2023gpt} to calculate and fit this relationship on a subset of the evaluation set.
% While end-to-end methods avoid loss-performance curve inconsistencies, they face several challenges:

% \begin{enumerate}
%     \item Difficulty in obtaining accurate downstream task performance from small models before capability emergence;
%     \item High performance volatility due to large variance of task difficulty distribution;
%     \item Lack of theoretical foundations for Performance-to-Compute relationships, relying primarily on empirical approximations.
% \end{enumerate}

% \citet{hu2023predicting} addresses the challenge of non-emergent capabilities in small models by averaging metrics from multiple non-greedy decoding evaluations, accurately extrapolating to models of 2.4B scale. This represents an advancement for end-to-end extrapolation methods from small to large models.
% %
% However, due to computational constraints, this work does not provide reliable predictions for models beyond 2.4B, leaving uncertainty about its accuracy in predicting larger model performance.
% % Additionally, given the small parameter gap between the largest fitted model (1.5B) and the target model (2.4B), one can reasonably argue that other function families could achieve similar prediction accuracy. In contrast, our method utilizes models ranging from 122M to 12B parameters for clustering and fitting, successfully predicting the final metrics of 70B models, demonstrating both accuracy and robustness.
% %
% %
% % Even if their method accurately extrapolates large model performance, it struggles to predict the desired greedy decoding metrics. Our approach addresses this by first clustering small models using non-greedy decoding metrics, then fitting and extrapolating using greedy decoding metrics from the clustered results, accurately predicting the target large model's greedy decoding performance, leading to more reliable predictions.
% Another limitation of their approaches is the discrepancy between non-greedy decoding evaluation and the greedy decoding typically used in large models. 
% %
% Passrate~\cite{achiam2023gpt} and Passuntil~\cite{hu2023predicting} metrics are only available for non-greedy decoding.
% %
% For certain public evaluation sets (like AGIEval~\cite{zhong2023agieval}), the metrics with different decode patterns differ significantly. 

% Furthermore, while \citet{achiam2023gpt} attempts to directly predict GPT-4's performance using small model metrics and achieves impressive accuracy, their selected downstream tasks are a subset of HumanEval~\cite{chen2021humaneval} chosen based on posterior prediction performance. This approach lacks generality and scalability, making it difficult to achieve good prediction accuracy on complete public evaluation set metrics.



% Considering the limitations in existing methods, we propose a novel \textbf{C}lustering-\textbf{O}n-\textbf{D}ifficulty (\textbf{COD}) multi-stage performance scaling prediction method, and provide both theoretical supports and experimental verifications. To the best of our knowledge, this is the first work that provides accurate capability estimation for 70B-scale large language models on mainstream evaluation tasks.

\section{Pilot Study}
\label{sec:pilot}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/related_works/ff_gsm8k_cot_v1_avg_score_loss_perf.pdf}
    \caption{Performance-loss relationship across difference model size (left) and learning rate schedule (middle). At equivalent loss values, smaller models or those with lower learning rates generally achieve higher accuracy than larger models or those with higher learning rates. Performance-compute relationship for different clusters of BBH samples(right). Different clusters demonstrate diverse scaling patterns.}
    \label{fig:perf-loss}
\end{figure}
In this section, we present the pilot experiments to illustrate the shortcomings of existing approaches. 

\noindent\textbf{Training loss may mismatch downstream tasks performance.}
Predicting downstream performance based on training loss relies on the assumption that LLMs achieve identical downstream performance at the same loss value--an assumption that often does not hold. In practice, training loss primarily serves as an indicator of in-domain fitting, whereas downstream tasks typically represent out-of-domain evaluations. Moreover, training configurations, such as model size and learning rate, can significantly affect not only the final loss but also the model’s generalization capabilities.


\cref{fig:perf-loss}(left) illustrates the performance–loss relationships for LLMs of different sizes on the CEval benchmark~\cite{nguyen2024ceval}. At the same training loss level, smaller models can outperform larger ones in terms of test accuracy. Because smaller models initially exhibit weaker in-domain fitting capacity, they typically require more training steps to reach the same loss value, which can lead to better in-domain generalization once they do. \cref{fig:perf-loss}(middle) compares the performance of LLMs trained under different learning rate schedules on the GSM8k dataset~\cite{cobbe2021training}. At the same loss level, the performance under the cosine schedule is always worse than that under the constant schedule, indicating that a lower learning rate may prioritize memorization over generalization, thereby diminishing downstream performance.

\noindent\textbf{Diverse scaling patterns within the evaluation set.}
Scaling patterns capture the performance–compute relationship for a single task sample. However, different task samples exhibit unique computational thresholds, learning slopes, and upper bounds, making it challenging to find a single fitting function (or set of fitting functions) that generalizes well across diverse task samples. \cref{fig:perf-loss}(right) illustrates the performance-compute relationships on three random clusters of the BBH benchmark~\cite{suzgun2023challenging}, with each cluster containing samples with similar difficulty. Even within a single evaluation set, these scaling curves can vary significantly, indicating that a one-size-fits-all performance-compute curve is insufficient for capturing the full spectrum of a downstream benchmark.
 

Taken together, these observations highlight the importance of modeling the heterogeneous scaling properties within an evaluation set and identifying a robust intermediate metric to serve as a reliable indicator of the downstream performance of LLMs.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/method/MainPipeline.pdf}
    \caption{The pipeline of Cluster-On-Difficulty downstream task performance scaling, including 4 stages: \textbf{a}. Represent task difficulty feature with task-wise passrate vector. Cluster on difficulty feature and filter outliers. \textbf{b}. Fit cluster-wise performance-compute curve. Classify clusters into extrapolatable clusters, non-extrapolatable clusters, and non-emergent cluster. \textbf{c}. Predict accuracy on extrapolatable clusters. \textbf{d}. Map subset accuracy prediction to full evaluation set performance.}
    \label{fig:main-pipeline}
\end{figure}


\section{Method}
\label{sec:method}

% Before training a large model, we hope to answer a crucial question: Given the number of model parameters and the total amount of training tokens, what capabilities can a large model achieve in various dimensions? 
%
% More specifically, what is the accuracy of a large model on downstream tasks? 
%


In this section, we introduce the COD method in four parts, illustrated in \cref{fig:main-pipeline}: 1) We show the advantages of COD and present an improved mean-shift clustering algorithm (\cref{sec:method-clustering}); 2) We derive a performance scaling law corresponding to task difficulty variance, which enhances the benefit of extrapolating the performance-compute relationship for task clusters with similar difficulty features (\cref{sec:fitting}). We fit cluster-wise performance-compute curves on small models and filter extrapolatable clusters; 3) We extrapolate the performance on extrapolatable clusters and predict the accuracy of the target large model on the predictable subset(\cref{sec:extrapolation}); 4) We show how to map accuracy on the predictable subset to full evaluations (\cref{sec:mapping}). 


% In response to this question, we propose a multi-stage Downstream Task Prediction method with high prediction accuracy, and show the illustration in \cref{fig:main-pipeline}, 

% \begin{itemize}%[label=\textbf{\alph*.}] % 设置为粗体小写英文字母
% \item We start with a series of small models, estimate difficulty of a task through passrates on these models. 
% \item Then we cluster the tasks on difficulty feature, and cluster-wise extrapolate small model metric based on the clustering results. 
% \item We set rules to distinguish the extrapolatable clusters and predict the large model metric on the extrapolatable clusters. 
% \item Finally, we use the mapping relationship from metric of small model extrapolatable clusters to total indicators to estimate the total indicators of large models. 
% \end{itemize}

% This section will introduce in detail the proposed multi-stage Downstream Task Prediction method.

\subsection{Clustering on Difficulty}
\label{sec:method-clustering}
% 每个评估集的任务即使具有相同的主题，但难度特征具有显著差异，进而导致不同子任务Performance scaling不一致，无法有一个固定的拟合公式完成预测。
% 我们希望把具有相似Performance scaling性质的任务聚合在一起进行预测，降低集合内部题目难度特征的差异，同时保证每个类簇内依然有足够多的样本以维持评估结果的稳定性。

Despite sharing common themes, tasks within evaluation sets demonstrate substantial difficulty differences. These differences result in diverse performance scaling patterns across tasks, making it challenging to apply a universal fitting function for predictions. Instead, we propose clustering tasks with comparable performance scaling behaviors to enable more accurate predictions. This approach minimizes the heterogeneity of difficulty features within clusters while ensuring that each cluster contains a sufficient number of samples for robust evaluation.

% Considering small models do not exhibit emergent behavior on most tasks, 
%

We adopt the \textit{passrate} metric to quantify the capabilities of small-scale models~\cite{hu2023predicting}. For each model, we conduct multiple evaluation runs (\eg, 100 trials) and calculate the mean accuracy as the expected probability of correct responses. % This approach enables a more reliable assessment of both model capabilities and question difficulty levels.
% To encourage diverse outputs, we set the sampling parameters to Temperature = 1 and Top P = 0.7. 
%
For each task, we characterize its difficulty through the passrates of models of increasing size. These passrates are arranged in ascending order of model scale, forming feature vectors that ideally exhibit monotonic growth within the [0, 1] range, as model capability typically increases with size.
%
However, we observe that some tasks deviate from the expected scaling pattern, showing non-monotonic difficulty features. This phenomenon may be attributed to metric instability or fluctuations in model performance during training.





% These unscaling tasks are the main reason for the fluctuation of evaluation indicators. How to model and estimate these samples has become the main challenge of Downstream Task Prediction. After obtaining the difficulty feature vector of each sample, we next perform clustering on the difficulty distribution.

\noindent\textbf{Improved clustering methods.}
% 在聚类方法的选择上，我们希望具有如下特性：1. 得到尽可能小的类内方差，更小的类内方差意味着单个类粗的外推性质更加相似；2. 无需预先指定聚类个数，不同评估集合理的聚类个数互不一致，并且难以预先决定。
% 在主流的聚类算法中，KMeans算法需要预先指定类簇个数，虽然有自动选取最优类簇个数的方法，例如，但这些方法需要引入更多的超参数；
% DBScan会在聚类时不断加入新的样本点，可能会导致最终类内方差较大，不满足当前任务中的聚类需求。因此最终采用了另一种无监督聚类算法MeanShift，通过传入聚类半径的参数以约束类内方差。
We hope to adopt clustering algorithms with the following features: 
1. Minimizing intra-class variance to ensure similar extrapolation properties within each cluster,
2. Automatic determination of cluster numbers, as the optimal number varies across evaluation sets and is difficult to pre-specify. 

Among classical clustering algorithms, the K-Means algorithm~\cite{macqueen1967some} needs to specify the number of clusters in advance. 
Although there are methods for automatically selecting the optimal number of clusters, e.g. Elbow method~\cite{thorndike1953belongs} and Silhouette~\cite{rousseeuw1987silhouettes}, these methods need to introduce additional hyperparameters. 
DBSCAN~\cite{ester1996density} is a non-parametric density-based clustering algorithm that marks points in low-density regions as outliers while cluster points in the connected high-density regions. In practice, DBSCAN may lead to a larger final intra-class variance and does not meet the clustering requirements of the current task. 
MeanShift~\cite{fukunaga1975estimation} algorithm adopts an extra clustering radius parameter to constrain the intra-class variance, which better fits our demands.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.3\textwidth]{figures/method/DBSCAN_BBH_v2.jpeg}
    \hspace{0.03\textwidth}  % 调整两图之间的间距
    \includegraphics[width=0.3\textwidth]{figures/method/MeanShift_BBH_v2.jpeg}
    \hspace{0.03\textwidth}  % 调整两图之间的间距
    \includegraphics[width=0.3\textwidth]{figures/method/ImprovedMeanShift_BBH_v2.jpeg}
    \caption{t-SNE visualization of different clustering methods. Each point represents an evaluation sample. DBSCAN(left): Continuous diffusion of clustering leads to too many samples within a class and large inner-group variance. MeanShift(Middle): Keep the high-density region as a unique group. Improved-MeanShift(Right): Constrain inner-group variance with the radius parameter.}
\label{fig:tsne_vis}
\end{figure}

% However, the MeanShift algorithm still may spread clustering to adjacent sample points, resulting in some samples being farther from the cluster center and having a larger variance. 
%
To further reduce intro-class variance, we propose an improved MeanShift algorithm to constrain the cluster diameter.
%
At the same time, we maintain a minimum number of tasks in each cluster to reduce metric fluctuations.
%
We provide the t-SNE visualization of evaluation tasks on BBH~\cite{suzgun2023challenging}. Each point represents an evaluation sample and its color denotes the cluster type. \cref{fig:tsne_vis}. The improved MeanShift prevails, as it effectively splits dense areas into reasonable clusters. 
%
We explain the details of clustering algorithms in \cref{sec:improved}, and smoothing techniques in \cref{sec:smoothing}.



\subsection{Fitting}
\label{sec:fitting}
% 得到聚类结果后，我们在每个类簇上计算评估指标，并完成独立的外推曲线拟合。
% 我们给出Scaling Law for Downstream Task Performance以及对应的理论分析, 并可以从中推导出难度特征相似的类簇上的Performance scaling预测公式。

Following cluster analysis, we compute evaluation metrics of small models within each cluster and conduct separate extrapolation curve fitting procedures. Small models are trained with the same ratio of training tokens to Compute Per Token (CPT). We propose a scaling law for downstream task performance, supported by theoretical analysis, which allows us to derive prediction formulas for performance scaling within clusters of tasks that share similar difficulty features. The fitting process initially excludes outlier samples, focusing only on the clustered sample set. For each cluster identified in the previous step, we compute accuracy metrics across small models, yielding an expected accuracy array for each cluster. By fitting these accuracy values against the computational costs of small models, we derive the expected accuracy-to-compute curve for each cluster.

We derive the fitting formula for the downstream task scaling law based on the following three assumptions:
\begin{enumerate}
    \item The relationship between the answer loss and the compute follows a power law, which generalizes the power law in loss prediction into (Question, Answer) format data. 
    \item For task samples with a finite set of answers, the model gives a random guess choice if it cannot accurately solve it.
    \item The task passrate is defined as the product of the predicted probabilities for each token, implying that each task sample has a unique answer, and the model outputs the answer only without any intermediate reasoning progress.
\end{enumerate}
Note that these assumptions may not perfectly hold in practice, we provide additional discussions on Assumption 3 in \cref{sec:discussion}.
Under the above assumptions we can derive the scaling law for downstream task performance.
\begin{proposition}[Scaling Law for Downstream Task Performance]
\label{main:proof:task_scaling_law}
Given a language model trained with computational budget $C$, and a set of downstream tasks $P$, under the following assumptions:
The expected accuracy on tasks $P$ can be modeled as:
$$\mathbb{E}_p[\mathrm{Acc}(C)]=g+(1-g)\left(e^{-aC^{-b}-c} + \frac{\sigma^2}{\mu}\right)+o(\mu)$$
where:
\begin{itemize}
    \item $g$ represents the random guess performance floor;
    \item $1-g$ represents the maximum achievable performance improvement;
    \item $a,b,c$ are positive constants;
    \item $\mu = \frac{1}{\#P}\sum_{(q,\mathrm{ans})\in P} \mathrm{loss}_{\mathrm{ans}}$;
    \item $\sigma^2 = \frac{1}{\#P}\sum (\mathrm{loss}_{\mathrm{ans}\_t}-\mu)^2$.
\end{itemize}
\end{proposition}

We outline the key proof intuition here, with detailed proofs provided in \cref{sec:prop_proof}. Like existing approaches~\cite{hu2023predicting}, we aim to establish the relationship between  $\mathrm{loss}_{\mathrm{ans}}$ and model $passrate$, leveraging loss power-law scaling to derive a scaling formula for downstream task passrate metrics.

% 我们简述主要的证明思想，详细的证明过程在附录。首先和~\cite{hu}一样，我们希望建立Task Answer的Loss与模型Passrate之间的关系，并利用Loss的幂率Scaling Law来给出Task Passrate指标的Scaling公式。
% 假设三约束了题目答案的唯一性，并忽略模型与县进行Thinking对答案的影响。此时模型做对题目的概率等于模型输出每个答案token概率的乘机。从而得出对于单个题目而言，其答案的Loss与Passrate应当是负log关系。

% 但已有工作没有考虑的是，在计算整个评估集的Passrate指标时，会对每个题目的exp(-Loss)求平均，如果像使用Loss Scaling Law，需要先对Loss去平均再计算指数。从数学角度，Passrate Scaling Law计算的是exp(-Loss)的算数均值，而Loss Scaling Law取指数之后计算的是exp(-Loss)的集合均值。

% 我们可以证明，算数均值与集合均值的插值可以被$\sigma^2/\mu^2$所估计，其中\sigma^2和\mu^2分别是题目passrate的方差和均值。也就是说只有再难度方差较小的评估集上，从Loss Scaling Law对到得到的Downstream Tasks Performance scaling Law才是有效的。但在我们提出的Clustering based的方法中限制了类簇内题目难度特征的方差，从而可以更好的适配Performance scaling Law。

% 最后，Passrate表示模型真实会做出Task的通过率，在不会做的情况下，依然有random guess水平的期望得分g，我们最后利用全期望公式得到Acc的Scaling Law公式。
\noindent\textbf{Proof intuition}
\begin{framed}
The assumption 3 ensures unique task answers and neglecting the impact of model thinking before answers, the probability of correct task completion equals the product of token probabilities in the model output. This implies a negative logarithmic relationship between $\mathrm{loss}_{\mathrm{ans}}$ and $passrate$ for individual tasks.

Previous works overlook that computing the $passrate$ metric for an evaluation set requires averaging $\exp(-\mathrm{loss_{\mathrm{ans}}})$ across tasks, whereas applying the loss scaling law necessitates averaging loss before exponentiation. Mathematically, the performance scaling law computes the arithmetic mean of $\exp(-\mathrm{loss_{\mathrm{ans}}})$, while the loss scaling law after exponentiation yields the geometric mean.

We show that the difference between arithmetic and geometric means can be estimated by $\sigma^2/2\mu$, where $\sigma^2$ and $\mu$ denote the variance and mean of task passrates, respectively. Consequently, the downstream tasks performance scaling law derived from the loss scaling law is valid only for evaluation sets with limited difficulty variance. Our proposed clustering-based COD method constrains the variance of difficulty features within clusters, enabling better alignment with the performance scaling law.

Finally, we constrain the model output space to a finite answer set, random guessing yields an expected score $g$ for unsuccessful attempts. 
\end{framed}

\cref{main:proof:task_scaling_law} demonstrates that a metric of an evaluation set with similar difficulty features can be effectively modeled using the following formula:
\begin{equation}
y(C)=g+(1-g)*e^{-aC^{-b}-c},
\label{eq:fitting}
\end{equation}
where $a$ and $b$ jointly influence how accuracy varies with $C$, $c$ controls the upper bound of the fitting curve, and $g$ represents the expected random guess metric for the model on this cluster. 
Parameters $a$, $b$, $c$, and $g$ are to be fitted. 
% To achieve a better fit in the experiment, we set the parameter ranges a priori as $a > 1$, $b > 0.1$, and $0 \leq c < 1$.


\subsection{Extrapolation}
\label{sec:extrapolation}
We aim to identify clusters exhibiting robust scaling patterns for reliable performance extrapolation since some clusters have saturated or non-emergent performance on small models and are not expected to give reasonable predictions.
%
We will show that performance prediction on these scalable clusters contributes to the prediction on the full evaluation set. 
%
We give the following definition to check whether a cluster is extrapolatable.

\textbf{Definition:} A cluster demonstrates scaling patterns if: (1) its expected accuracy increases monotonically with model size, and (2) the probability of correct responses converges to at least $P$ as computational resources approach infinity, where $P \leq 1$ is a predefined threshold accounting for practical limitations such as ambiguous questions and finite training coverage.

We filter clusters lacking scaling patterns by the following two rules:
\begin{enumerate}
\item Negligible accuracy growth with increased computational resources, manifested as minimal $a$ or $b$ values in \cref{eq:fitting};
\item Poor extrapolation reliability, indicated by excessive $c$ values in \cref{eq:fitting}.
\end{enumerate}
In practice, we set the parameter ranges a priori as $a > 1$, $b > 0.1$, and $0 < c < 1$.

The predictable subset comprises all samples from clusters that exhibit scaling patterns.
%
%排除了未涌现
%
The final performance prediction for a target model in the predictable subset is computed as the weighted average of the individual cluster predictions, with weights proportional to the cluster sizes.

\subsection{Mapping from Predictable Subset to Target Evaluation Set}
\label{sec:mapping}
We extend our predictions from the predictable subset to the complete evaluation set through a principled mapping approach. Our method rests on the observation that extrapolatable and non-extrapolatable samples share question types but differ primarily in difficulty features, suggesting a preserved partial order of metrics across these subsets. We formalize this relationship through a mapping function $f:f(T')\rightarrow T$ from predictable subset metrics $T'$ to total evaluation set metrics $T$. This function exhibits key properties:
1. continuity and smoothness over $[0,1]$, 2. monotonic increase, and 3. passage through points $(0,0)$ and $(1,1)$. Empirical validation reveals that a quartic function optimally captures this relationship:
\begin{equation}
    f(x)=\alpha_1x^4+\alpha_2x^3+\alpha_3x^2+(1-\alpha_1-\alpha_2-\alpha_3)x
    \label{eq:mapping}
\end{equation}
To ensure reliable extrapolation, we calibrate the mapping curve using evaluation results of existing models as anchors. Our results show that the subset-to-full mapping generally maintains robustness across model architectures and training data, enabling the use of external models (e.g., Qwen2-72B~\cite{yang2024qwen2}) as anchors for most tasks. We conduct corresponding experiments in \cref{sec:anchor}.
%
For data-sensitive tasks, models with similar training distributions provide more reliable anchors, indicating that data consistency takes precedence over architectural variation ensuring mapping accuracy. This calibration strategy enables accurate metric predictions for the complete evaluation set while maintaining computational efficiency.

Finally, combining \cref{eq:fitting} and \cref{eq:mapping}, we get our final metric prediction $p = f\circ y(C_{0})$, where $C_0$ is the estimated computation of training the target LLM.
\section{Experiments}
\label{sec:expr}

\subsection{Experimental Setups}
\label{sec:exp_setups}

In our experimental setup, we train several smaller versions of the target model architecture for prediction. These models vary in size but share similar training procedures, with the training data scaled proportionally to their sizes.

\noindent\textbf{Downstream evaluation sets.}
We adopt the following widely-used benchmarks as our target downstream tasks:
For evaluation, we adopt the following widely-used benchmarks, shown in \cref{tab:evaluation set_info}. Evaluation sets cover popular downstream tasks of the language model, including math, logic, coding, reading comprehension, professional knowledge, etc.

% \begin{itemize}
% \item GSM8K \cite{cobbe2021training} for elementary math word problems, evaluated under 8-shot setting with chain-of-thought prompting;
    
% \item MATH \cite{hendrycks2021measuring} for mathematical reasoning tasks, evaluated under 4-shot setting with chain-of-thought prompting;
    
% \item BBH \cite{suzgun2023challenging} for challenging logic reasoning tasks, evaluated under 3-shot setting with chain-of-thought prompting;
    
% \item TriviaQA \cite{joshi2017triviaqa} for question answering tasks, evaluated under 5-shot setting;
    
% \item MBPP \cite{austin2021program} for coding tasks, evaluated under 3-shot setting with chain-of-thought prompting;
    
% \item AGIEval \cite{zhong2023agieval} for comprehensive cognitive abilities evaluation, evaluated under 5-shot setting;
    
% \item DROP \cite{dua2019drop} for reading comprehension and numerical reasoning, evaluated under 3-shot setting with chain-of-thought prompting;
    
% \item MMLU-pro \cite{wang2024mmlu} for professional knowledge evaluation, evaluated under 5-shot setting.
% \end{itemize}

\begin{table}[!t]
\centering
\small
\caption{Information of evaluation datasets used in the study.}
% \resizebox{\textwidth}{!}{
\setlength{\tabcolsep}{1.5pt}
\begin{tabular}{c|ccccccccc}
\toprule
Dataset & GSM8K & MATH & BBH & TriviaQA & MBPP & AGIEval & DROP & MMLU-pro \\
& \cite{cobbe2021training} & \cite{hendrycks2021measuring} & \cite{suzgun2023challenging} & \cite{joshi2017triviaqa} & \cite{austin2021program} & \cite{zhong2023agieval} & \cite{dua2019drop} & \cite{wang2024mmlu} \\
\midrule
Domain & Math & Math & Reasoning & Knowledge & Coding & Comprehensive & Reading & Comprehensive \\
\#Questions & 1,319 & 5,000 & 6,511 & 17,944 & 500 & 8,063 & 9,536 & 12,032 \\
\#Shots in Prompt & 8 & 4 & 3 & 5 & 3 & 5 & 3 & 5 \\
\bottomrule
\end{tabular}
% }
\label{tab:evaluation set_info}
\end{table}

All models are evaluated in a few-shot in-context learning manner, where they need to generate final answer labels based on given demonstrations and test inputs. We aligned our evaluation setups with LLaMa3~\cite{dubey2024llama}.

\noindent\textbf{Model training.}
To establish performance predictions for large language models, we conduct systematic experiments with a suite of smaller-scale models across different parameter counts and training data volumes, while controlling for other training configurations such as learning rate, batch size, and additional hyperparameters. All models are trained on a constant learning rate scheduler and data with the same distribution.
We list model configurations in \cref{tab:model_configs_dense}.
% \noindent\textbf{Model Specifications}

\begin{table}[!t]
\centering
\small
\caption{Model architecture specifications across different sizes.}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{c|ccccccccc}
\toprule
 & 122M & 238M & 411M & 652M & 973M & 1.9B & 7B & 12B & 70B (Target) \\
\midrule
Param. (M) & 122 & 238 & 411 & 652 & 973 & 1,901 & 6,980 & 12,022 & 68,452 \\
Compute Per Token (B) & 1.535 & 2.684 & 4.275 & 6.378 & 9.060 & 16.436 & 54.761 & 91.609 & 475.131 \\
Tokens (B) & 26 & 45 & 72 & 108 & 153 & 277 & 923 & 1,544 & 8,012 \\
Layers & 8 & 10 & 12 & 14 & 16 & 20 & 32 & 43 & 80 \\
Model Dimension & 1,024 & 1,280 & 1,536 & 1,792 & 2,048 & 2,560 & 4,096 & 4,608 & 8,192 \\
FFN Dimension & 3,584 & 4,480 & 5,376 & 6,272 & 7,168 & 8,960 & 14,336 & 16,128 & 28,672 \\
Heads & 8 & 10 & 12 & 14 & 16 & 20 & 32 & 36 & 64 \\
KV Heads & 8 & 10 & 12 & 14 & 16 & 20 & 8 & 12 & 8 \\
\bottomrule
\end{tabular}
\label{tab:model_configs_dense}
\end{table}



\subsection{Prediction Experiments}
\noindent\textbf{Baselines.}
% 我们比较本文提出的难度聚类的LLM下游任务能力预估方法与已有方法。我们利用在上述章节介绍的多个公开评估集作为检验任务，并利用在相同数据分布与模型结构下不同配置的小模型能力预估大模型能力。
% 我们选用三种下游任务预估方式作为参考。
% 1. 测试集整理指标预测。采用小模型测试集指标直接通过外推公式预测大模型指标。
% 2. Passuntil。利用小模型Passrate指标预估大模型Passrate指标，再基于Passrate指标映射回Accurate指标。
% 3. Loss-two-stage。先预估大模型的Loss，基于小模型评估集指标与Loss之间的关系预估在大模型Loss下的下游任务指标。
% 同时我们设置两个实验组：
% COD one stage。 采用基于难度特征聚类，每类进行预测后汇总预测结果。取消划分可预估子集、在可预估子集上外推并映射回完整评估集指标的流程
% COD two stage。 完整的所提出的方法。
We evaluate our proposed COD performance scaling for LLMs against existing approaches. The evaluation is conducted on multiple public benchmarks mentioned above, where we utilize a series of smaller models with identical data distribution and architecture but different configurations to estimate the downstream tasks performance of the target 70B large language model.

We compare against three existing prediction methods:
\begin{enumerate}
    \item \textbf{End-to-end performance-compute prediction}: Extrapolate larger model metrics directly from smaller model evaluation set metrics using performance scaling laws.
    
    \item \textbf{Passrate-compute prediction}: Estimate large model passrates from smaller model passrates~\cite{achiam2023gpt,hu2023predicting}. We repeat and evaluate 100 trials for each evaluation set to enhance the performance reliability on smaller models. For a fair comparison, we report the absolution prediction error on the passrate metric instead of greedy decoding accuracy.
    
    \item \textbf{Loss-intermediate performance prediction}: First predict the final training loss of large language model, then estimate downstream task metrics based on the relationship between smaller model evaluation metrics and their corresponding losses~\cite{chen2024scaling}.
\end{enumerate}

% 我们设计了两个实验组，分辨验证聚类与完整pipeline对预估带来的收益。
We design two experimental groups to validate the benefits of clustering and the complete pipeline, respectively:
\begin{enumerate}
    \item \textbf{COD w/o. mapping}: Performing difficulty-based clustering using K-Means, extrapolating within each cluster independently, and then aggregating metrics across clusters without requiring subset-to-full mappings.
    \item  \textbf{COD complete}: Complete multi-stage proposed approach consisting of clustering, predictable cluster filtering, subset extrapolation, and subset-to-full mapping.
\end{enumerate}

The comparative results across different benchmarks and estimation approaches are presented in \cref{tab:baseline_comparison}.
%
We evaluate prediction accuracy with the absolute error between predicted and actual performance. 
%
We report the prediction error on each single evaluation set and list the mean and the max prediction error.


\begin{table}[!t]
\small
\setlength{\tabcolsep}{1.8pt}
\centering
\caption{Absolute prediction error on several evaluation sets. A prediction error less than 2 pp is considered an accurate estimate (marked in green), while an error greater than 5 pp is regarded as an invalid estimate (marked in red).}
% \vspace{0.1cm}
% \resizebox{\textwidth}{!}{
\begin{tabular}{c|cc|cccccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Overall Metrics} & \multicolumn{8}{c}{Individual Task Sets} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-11}
& Mean$\downarrow$ & Max$\downarrow$ & GSM8k & MATH & BBH & TriviaQA & MBPP & AGIEval & DROP & MMLU-pro \\
\midrule
End-to-end & 3.10 & 6.00 & 4.00 & 3.86 & \greenhl{0.64} & \greenhl{0.68} & \greenhl{1.75} & \redhl{6.00} & 4.11 & 3.72 \\
Passrate & 5.02 & 8.80 & \redhl{6.71} & \redhl{8.80} & 3.51 & 4.00 & \redhl{7.34} & \redhl{6.78} & \greenhl{0.26} & 2.74 \\
Loss-intermediate & 5.29 & 9.39 & \redhl{9.39} & \redhl{6.95} & 2.33 & \redhl{5.81} & \redhl{5.52} & \greenhl{1.41} & \redhl{5.37} & \redhl{5.55} \\
\midrule
COD (w/o mapping) & \ul{2.24} & \ul{5.26} & 4.70 & \greenhl{0.50} & 2.91 & \greenhl{1.98} & \greenhl{0.89} & \redhl{5.26} & \greenhl{1.08} & \greenhl{0.57} \\
COD (Complete) & \textbf{1.63} & \textbf{2.38} & 2.23 & \greenhl{1.28} & \greenhl{1.77} & \greenhl{1.64} & 2.19 & 2.38 & \greenhl{0.23} & \greenhl{1.35} \\
\bottomrule
\end{tabular}
% }
\label{tab:baseline_comparison}
\end{table}

\noindent\textbf{Results.} Predictions with an absolute error of less than 2 percentage points (pp) are considered accurate estimations, and when the predicted values fall within the training metric fluctuation range, they are marked in green; predictions with an absolute error greater than 5 indicate invalid estimations, which reduce the overall reliability of the prediction method and are marked in red.
% 结果表明，我们的方法在平均预测误差与最大预测误差上显著优于已有方法，平均预测误差被控制在2pp以内，对大模型训练具有指导价值。
% 虽然已有方法在部分评估集上具有良好表现，但总存在少数评估集上预估误差较大，这使得这些方法的预估结果的可信度降低。
These results show our approach significantly outperforms existing methods in both mean and maximum prediction errors, maintaining mean prediction error within 2 pp, thus offering practical guidance for large model training.
%
While existing methods demonstrate good performance on certain evaluation sets, they consistently exhibit substantial estimation errors on a minority of sets, undermining the credibility of their predictions.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/experiments/perf_compute/BBH.pdf}
        % \caption*{BBH}
    \end{minipage}
    \begin{minipage}{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/experiments/perf_compute/MATH.pdf}
        % \caption*{MATH}
    \end{minipage}
    \begin{minipage}{0.30\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/experiments/perf_compute/MMLU_pro.pdf}
        % \caption*{MMLU pro}
    \end{minipage}
    \caption{Performance-compute relationship across difference prediction method}
    \label{fig:prediction_experiment_vis}
\end{figure}

% 我们进一步可视化Performance-compute关系来展示不同预测方法的差异。在BBH评估集上，三个方法给出了类似的估计，但是End-to-end与Loss-intermediate无法对小模型评估点进行有效拟合，而Cluster方法则呈现出更复杂且拟合效果更好的多阶段走势。而在MATH与MMLU pro评估集上，预估难点是判断大模型指标会随算力加速上涨还是遇到瓶颈，这里Loss-intermediate方法预估模型上限能力过低，而End-to-end方法同样出现了3pp以上的预测误差。
% Clustering方法的良好表现，来自于其更加充分分析了评估集的难度分布与Scaling规律，发现大部分题目上涨空间加大的数学评估集，可以给出增长的预估；而对于题目得分比较饱和的评估集也可以预测出其随算力提升Scaling性质放缓的特点。
Through visualization of the performance-compute relationship, we illustrate the distinctive characteristics of different prediction methods, shown in~\cref{fig:prediction_experiment_vis}. On the BBH evaluation set, while all three methods yield comparable estimates, end-to-end and loss-intermediate methods demonstrate inadequate fitting for small model evaluation points. In contrast, the cluster method reveals a more sophisticated and well-fitted multi-phase trajectory. For MATH and MMLU pro evaluation sets, the critical challenge involves determining whether large model metrics will experience accelerated growth with increased computing power or encounter performance plateaus. The loss-intermediate method exhibits an underestimation of the model capability ceiling, while the end-to-end method shows prediction errors exceeding 3 pp.

The clustering method's effectiveness can be attributed to its comprehensive analysis of evaluation set difficulty distributions and scaling laws. It successfully predicts growth patterns in mathematical evaluation sets where most problems demonstrate expanded improvement potential, while accurately capturing the diminishing scaling properties in evaluation sets with score saturation as computational resources increase.

\subsection{Comparison of Clustering Methods}

% 我们验证聚类方法对预估方法的影响。我们希望控制类簇内样本到聚类中心的平均距离，让类簇内的难度特征更加接近。同时让最小类簇题目数量不少于10个，这是考虑到过小的类簇会带来指标数值的不稳定性。
% 我们把所提出的Improved-MeanShift算法对比了DBScan, MeanShift以及K-Means等聚类方法。其中，由于普通K-Means不具备筛选离群点、以及直接控制类内距离的能力。我们对其进行如下调整：
% 1. 搜索类簇个数，使得最小类簇的样本数量接近且不少于10个；
% 2. 利用2倍平局类内距离以类簇中心绘制圆球，把不被任意圆球覆盖的样本视作离群点。
% 我们利用类内平均距离（IAD）与离群率（OR）作为直接衡量指标，在离群率接近的情况下，IAD越小表示聚类效果越好，如表~\cref{tab:clustering_basic}所示。
% 同时，我们利用聚类后的外推预测误差与最终预估指标的误差来衡量不同聚类方法对预估流程的收益，如表~\cref{tab:clustering_prederror}所示。

We evaluate the impact of clustering methods on the estimation approach. Our goal is to control the average distance between samples and cluster centers within clusters, making difficulty features more similar within clusters. We also ensure that the minimum number of questions in any cluster is not less than 10, considering that too small clusters may lead to instability in metric values.
We compared our proposed Improved-MeanShift algorithm with clustering methods including DBScan, MeanShift, and K-Means. Since standard K-Means lacks the ability to filter outliers and directly control intra-cluster distances, we made the following adjustments: (1) Search for the number of clusters such that the minimum cluster size is close to but not less than 10 samples; (2) Draw spheres around cluster centers with a given threshold radius, and treat samples not covered by any sphere as outliers. If a cluster drops to less than 10 samples, we treat its samples as outliers.



We use Intra-cluster Average Distance (IAD) and Outlier Rate (OR) as direct evaluation metrics. With similar OR, a smaller IAD indicates better clustering performance, as shown in \cref{tab:clustering_basic}.
Additionally, we measure the benefits of different clustering methods on the prediction process by comparing the Extrapolation Errors(EE) of the predictable subset and Final prediction Errors (FE) after clustering, as shown in \cref{tab:clustering_prederror}.

\begin{table}[!t]
\centering
\small
\caption{Clustering performance on popular benchmarks.}
%\vspace{0.1cm}
%\resizebox{\textwidth}{!}{
\begin{tabular}{c|cccccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{MMLU-pro} & \multicolumn{2}{c}{GSM8k} & \multicolumn{2}{c}{MATH} & \multicolumn{2}{c}{BBH} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
& IAD$\downarrow$ & OR(\%) & IAD$\downarrow$ & OR(\%) & IAD$\downarrow$ & OR(\%) & IAD$\downarrow$ & OR(\%) \\
\midrule
K-Means & 0.3236 & - & 0.2238 & - & 0.2238 & - & 0.6284 & - \\
DBScan & 0.4242 & 0.56 & 0.5131 & 0.53 & 0.4775 & 0.68 & 0.7113 & 18.92 \\
MeanShift & 0.2859 & 0.39 & 0.2852 & 0.61 & 0.2110 & 1.44 & 0.2679 & 20.72 \\
Improved-KMeans & \textbf{0.1609} & 2.85 & \textbf{0.1321} & 2.73 & \textbf{0.0902} & 2.22 & \textbf{0.1953} & 37.23 \\
Improved-MeanShift & \ul{0.2225} & 4.40 & \ul{0.1854} & 4.93 & \ul{0.1463} & 2.66 & \ul{0.2143} & 33.58 \\
\bottomrule
\end{tabular}
%}
\label{tab:clustering_basic}
\end{table}


\begin{table}[!t]
\centering
\small
\caption{Prediction errors across clustering algorithms.}
%\vspace{0.1cm}
%\resizebox{\textwidth}{!}{
\setlength{\tabcolsep}{4.5pt}
\begin{tabular}{c|cccc|cccccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Mean} & \multicolumn{2}{c|}{Max} & \multicolumn{2}{c}{MMLU-pro} & \multicolumn{2}{c}{GSM8k} & \multicolumn{2}{c}{MATH} & \multicolumn{2}{c}{BBH} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
& EE$\downarrow$ & FE$\downarrow$ & EE$\downarrow$ & FE$\downarrow$ & EE$\downarrow$ & FE$\downarrow$ & EE$\downarrow$ & FE$\downarrow$ & EE$\downarrow$ & FE$\downarrow$ & EE$\downarrow$ & FE$\downarrow$ \\
\midrule
K-Means & 3.62 & 3.76 & 8.16 & 8.99 & 3.69 & 3.69 & 0.01 & 0.00 & 2.62 & 2.34 & 8.16 & 8.99 \\
DBScan & 3.93 & 4.08 & 4.38 & 4.36 & 3.72 & 3.69 & 4.08 & 4.12 & 4.38 & 4.16 & 3.53 & 4.36 \\
MeanShift & 2.12 & 1.68 & 3.15 & 3.08 & 3.15 & 3.08 & 0.67 & 0.74 & 2.55 & 2.26 & 2.12 & 0.65 \\
Improved-KMeans & 1.33 & 1.84 & 3.92 & 4.08 & 0.56 & 0.61 & 3.92 & 4.08 & 0.81 & 0.51 & 0.02 & 2.17 \\
Improved-MeanShift & \textbf{1.23} & \textbf{1.66} & \textbf{2.20} & \textbf{2.23} & 1.27 & 1.35 & 2.20 & 2.23 & 1.14 & 1.28 & 0.29 & 1.77 \\
\bottomrule
\end{tabular}
%}
\label{tab:clustering_prederror}
\end{table}
% 表格~\cref{tab:clustering_basic}表示Improved-KMeans与Improved-MeanShift具有较好的聚类效果，这是源于两方法在聚类过程中引入了对类内距离的约束。
% 从表格~\cref{tab:clustering_prederror}可以看出，具有更好聚类效果的两个方法对应的可预估子集外推误差与最终指标预测误差都相对更小。
% 虽然Improve-KMeans的聚类效果最优，但其在GSM8k下游预估表现明显差于其他评估集。我们认为，这是因为K-Means算法需要预先指定更加明确的聚类个数，但评估集类簇个数是很难预先知道的。这里我们采用搜索方式得到的类簇个数虽然在部分评估集上有效，但具有不稳定性，并最终导致少数评估集预测误差过大。
% 而相对来说，我们采用的Improved-MeanShift算法其原生不需要预先指定类簇个数，而是根据我们对类内距离的约束自动判定的。这让聚类效果更佳稳定，具有最小的评估集之间最大预估预估误差。
% 我们把更多评估集上的聚类实验结果放在附录~\cref{appdix:cluster}，其结论与已有评估集相同。
\cref{tab:clustering_basic} shows that Improved-KMeans and Improved-MeanShift achieve better clustering performance, which is attributed to their incorporation of intra-cluster distance constraints during the clustering process. From \cref{tab:clustering_prederror}, we can observe that the two methods with better clustering performance correspond to smaller extrapolation errors of estimable subsets and final metric prediction errors.

Although Improved-KMeans achieves optimal clustering performance, its downstream estimation performance on GSM8k is notably inferior compared to other evaluation sets. We believe this is because the K-Means algorithm requires pre-specifying a more explicit number of clusters, but the number of clusters in evaluation sets is difficult to know in advance. While the number of clusters obtained through our search method is effective for some evaluation sets, it lacks stability and ultimately leads to excessive prediction errors in a few evaluation sets.
%
In contrast, our adopted Improved-MeanShift algorithm inherently does not require pre-specifying the number of clusters; instead, it automatically determines this based on our intra-cluster distance constraints. This results in a more stable clustering performance and yields the smallest maximum estimation error across evaluation sets.
%
We present additional clustering experimental results on more evaluation sets in \cref{appdix:cluster}, where the conclusions are consistent with existing evaluation sets.

\subsection{Extrapolation Formula}
To evaluate the effectiveness of different fitting formulas, we conducted an ablation study comparing various formulations of the accuracy-compute relationship. Our baseline formula incorporates random guess probability, exponential decay, and a constant offset term:
\begin{align}
    f(C)=g+(1-g)*e^{-aC^{-b}-c}
\end{align}
To understand the contribution of each component, we perform ablation experiments by removing or modifying different terms. 1) Without random guess component: $f_1 (C)=e^{-aC^{-b}-c}$; 2) Without constant term $c$: $f_2(C)=g+(1-g)*e^{-aC^{-b}}$; 3) Direct power law relationship~\cite{hu2023predicting}: $f_3(C) = e^{-aC^{-b}}$.

The comparative results of these formulations are shown in \cref{tab:ablation_results}. 
% 这里展示了BBH, Math与MMLU-pro三个评估集的结果，并列举了可预估子集外推预估误差（EE）、可预估子集占比（TR）以及最终指标预估误差（FE）
Results from three evaluation sets, BBH, Math, and MMLU-pro, are presented here, showing the Extrapolation Error of extrapolatable clusters (EE), the Task Ratio of predictable subset (TR), and the Final prediction Error (FE). These results show that the proposed formula $f$ consistently achieves the smallest extrapolation error and final prediction error, while the ratio of estimable subsets remains similar across different clustering methods.

\begin{table}[!t]
\centering
\small
\caption{Ablation study results across different benchmarks.}
\begin{tabular}{c|ccccccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{BBH} & \multicolumn{3}{c}{MATH} & \multicolumn{3}{c}{MMLU-pro} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
& EE$\downarrow$ & TR(\%) & FE$\downarrow$ & EE$\downarrow$ & TR(\%) & FE$\downarrow$ & EE$\downarrow$ & TR(\%) & FE$\downarrow$ \\
\midrule
Direct Power Law & 8.90 & 49.06 & 8.88 & 3.81 & 81.46 & 3.35 & 4.30 & 95.15 & 4.27 \\
w/o Random Guess & 10.27 & 45.75 & 11.20 & 4.04 & 81.46 & 3.55 & 4.40 & 95.05 & 4.37 \\
w/o Constant c & 2.14 & 57.26 & 4.01 & 1.40 & 81.46 & 1.56 & 3.85 & 95.60 & 3.88 \\
Ours & \textbf{0.29} & 52.46 & \textbf{1.77} & \textbf{1.14} & 81.24 & \textbf{1.28} & \textbf{1.27} & 94.38 & \textbf{1.35} \\
\bottomrule
\end{tabular}
\label{tab:ablation_results}
\end{table}

% 结果表明所提公式$f$取得了一致最小的外推误差与最终预估误差，可预估子集占比在不同聚类方法上是接近的。
% 在对照组中，$f_1$在有限答案集任务上表现不好，这些评估集的小模型指标非零，无法有效被$f_1$拟合；
% $f_2$则去除了决定预估曲线最大值的$c$，这些评估集均默认足量计算与参数量的情况下，评估集性能会达到满分。但受限于训练数据分布的有限性、以及测试集题目存在模棱两可的答案，这样的假设是不合理的。导致预估不准确。
% 而直接的幂率拟合没有建模出指标范围是0至1的区间，以及在Random Guess能力与能力饱和附近指标提升通常比其他区域更困难的特征。


In the control group, $f_1$ performs poorly on tasks with finite answer sets, where small models achieve non-zero scores that $f_1$ cannot effectively fit. $f_2$ removes $c$, which determines the maximum value of the prediction curve, assuming that the evaluation set performance would reach perfect scores given sufficient computation and parameters. However, this assumption is unreasonable due to the limited training data distribution and ambiguous answers in evaluation set questions, leading to inaccurate predictions. Direct power-law fitting $f_3$ fails to model both the metric range constraint of 0 to 1 and the characteristic that metric improvement is typically more difficult near Random Guess capability and capability saturation compared to other regions.

We also observe that TR has little influence on prediction error, which also indicate the robustness of the proposed method. Some evaluation set with low TR due to the non-emergent subset, and our results show that this subset can largely be predicted with metrics of the extrapolatable clusters. We visualize the difficulty distribution of predictable subset and the full evaluation set in \cref{sec:difficulty_distribution}.
\subsection{Anchor Point in Interpolation Mapping}
\label{sec:anchor}
% 在可预估子集指标映射完整评估集指标阶段，我们发现不同训练数据与模型结构的模型具有相似的Mapping关系。这让我们可以利用已训练好的模型指标修正Mapping映射关系，进而提升我们对最终指标预估的准确性。
% 我们分别采用Qwen2-72B~\cite{qwen2}以及一个相同训练数据的MoE模型$M$作为Mapping阶段的Anchor point。我们首先仅用小模型指标与(0,0)，（1，1）得到插值曲线，并验证Anchor point与已有插值曲线的匹配性。
% 结果表明虽然这些模型具有不同的计算量、模型结构与训练数据，他们共享相同的Mapping关系。
% 该结果也说明可预估子集指标与全集指标高度相关，相比于Loss-intermediate预估，可预估子集指标在保持可预估性的同时一定程度摆脱了其他模型参数的干扰。

% 根据上述观察，我们把已经训练好模型的Mapping关系纳入插值，进而提升在对大模型指标预估的准确度与置信度。
During the mapping phase from estimable subset metrics to full evaluation set metrics, we discovered that models with different training data and architectures exhibit similar mapping relationships. This allows us to leverage metrics from pre-trained models to refine the mapping relationship, thereby improving the accuracy of our final metric estimation.

We use both Qwen2-72B~\cite{yang2024qwen2} and an in-house model $M$ with a Mixture-of-Experts(MoE)~\cite{lepikhin2020gshardscalinggiantmodels} structure trained on the same data distribution as anchor points in the mapping phase. 
%
We first obtain the interpolation curve using only small model metrics with points (0,0) and (1,1), then verify the compatibility of anchor points with the existing interpolation curve. 
%
When the score of the full set is 0 (1), the score of the subset must also be 0 (1).
%
Results show that despite differences in computational requirements, model architectures, and training data, these models share similar mapping relationships.

This finding also indicates that estimable subset metrics are highly correlated with full-set metrics. Compared to loss-intermediate estimation, estimable subset metrics maintain predictability while reducing interference from other model parameters. Based on these observations, we incorporated the mapping relationships from pre-trained models into the interpolation process, thereby improving both the accuracy and confidence of our large model metric estimations.

We establish three experimental configurations of our method:

\begin{itemize}
    \item \textbf{COD w/o. anchor}: The complete estimation process is employed except for not using anchor point interpolation in the Mapping phase.

    \item \textbf{COD w. out-of-distribution (OOD) anchor}: The complete proposed methodology incorporates both difficulty-based clustering and predictable subset identification. Using the 72B Qwen2 pretraining model as the anchor model~\cite{yang2024qwen2}.
    
    \item \textbf{COD w. in-domain(ID) anchor}: Using an in-house MoE model with consistent training distribution but different model architecture as the anchor point.
\end{itemize}

% 我们把结果展示在~\cref{tab:ablation_study}中，可以看出添加OOD模型与ID模型作为Anchor都可以帮助提升预估准确性。该结果说明可预估子集指标与全集指标之间具有较为稳定的关联，不同训练数据与模型结构的模型在子集与全集上的指标关系是接近的。这使得我们在预测新数据与结构的大模型指标时也可以利用上已有模型的评估结果来提升预估准确性。
% 另外，我们基于聚类得到评估集划分属于评估集的内在属性，它与模型结构和训练数据无关，因此有聚类得到的可预估子集也可以延用到新模型的指标预估上。
% 该特性在Loss-intermediate方法上是不可行的，因为模型Loss受训练算法影响过大。
We present the results in~\cref{tab:ablation_study}, demonstrating that incorporating both out-of-distribution models and in-distribution models as anchors consistently enhance prediction accuracy. These findings suggest a relatively stable correlation between the metrics of predictable subsets and the full dataset, indicating that the relationship between subset and full-set metrics remains consistent across models trained on different data and with varying architectures. This property enables us to leverage evaluation results from existing models to improve the accuracy of metric predictions for large models with new data and structures. 
Besides, the division of the evaluation set obtained through clustering is an intrinsic property of the evaluation set itself, independent of the model architecture and training data. Therefore, the predictable subset derived from clustering can also be extended to estimate the metrics of new models. 
Additionally, we conduct the ablation study on the interpolation method in \cref{sec:interpolation_ablation}, and results indicate that quartic functions are suitable in our setting.
% Notably, such an approach is not feasible with the Loss-intermediate method, as model loss is excessively influenced by the training algorithm.

\begin{table}[!t]
\centering
\small
\caption{Influence of anchor point usage in the mapping stage.}
\setlength{\tabcolsep}{3.5pt}  % 减小列间距以适应页面宽度
%\resizebox{\textwidth}{!}{
\begin{tabular}{c|cc|c*{7}{c}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c|}{Overall} & \multicolumn{8}{c}{Individual Task Sets} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-11}
& Mean & Max & GSM8k & MATH & BBH & TriviaQA & MBPP & AGIEval & DROP & MMLU-pro \\
\midrule
w/o. anchor & 3.96 & 8.80 & \textbf{2.17} & 5.46 & 5.08 & 1.68 & 8.80 & {2.38} & 4.44 & 1.68 \\
w. ood anchor & \textbf{1.59} & {2.56} & {2.22} & \textbf{0.94} & {1.84} & \textbf{1.04} & {2.56} & \textbf{1.86} & {1.18} & \textbf{1.04} \\
w. id anchor & {1.63} & 2.38 & {2.23} & {1.28} & \textbf{1.77} & {1.64} & \textbf{2.19} & \textbf{2.38} & \textbf{0.23} & {1.35} \\
\bottomrule
\end{tabular}
%}
\label{tab:ablation_study}
\end{table}

\section{Conclusion and Discussion}
\label{sec:discussion}
In this work, we introduce a novel downstream performance scaling framework including (1) a difficulty-based clustering approach that effectively models the underlying distribution of each evaluation set; (2) a theoretically grounded scaling law for downstream task performance that provides a fitting formula for performance-compute extrapolation; and (3) a systematic methodology for identifying and leveraging predictable subset that provides a robust intermediate metric for accurate full-set performance predictions. 

Our framework, while effective for dense transformers, has not been fully explored for cost-efficient MoE models and does not account for the annealing phase in training, where high-quality data can rapidly enhance performance. The COD method requires sufficient test cases and is not suited for multiple-choice tasks, where performance metrics may diverge from true passrates. Additionally, the framework's theoretical foundation is insufficient for chain-of-thought reasoning, necessitating future adaptations to address these challenges. We provide detailed discussion in \cref{sec:limiations}.

Looking forward, our approach can be further expanded across model architectures, training methods, and evaluation set types, while extending this framework to address chain-of-thought reasoning patterns offer promising avenues for future research.
%Looking forward, 我们的方法在模型架构、训练方法以及评估集的类型上还可以有更全面的拓展，同时extending this framework to handle chain-of-thought reasoning patterns and exploring its connections with model capability density analysis represent promising directions for future research.

% \noindent\textbf{Connection with densing law}
% % Densing Law表明随着LLM技术演进，模型的有效参数量占实际参数的比率在快速提升，这个比率被定义为模型的能力密度。该方法比较不同系列模型达到相同性能时，参数量的差异。为了对比能力不同的模型，该方法需要反向利用Task Performance预估方法：根据目标性能反推其等效参数量。利用本文所提方法可以更准确比较不同系列模型之间能力密度。

% The Densing Law indicates that as LLM technology evolves, the ratio of effective parameters to actual parameters in models is rapidly increasing, and this ratio is defined as the model's capability density~\cite{xiao2024densing}. This method compares the parameter differences between different model series when they achieve the same performance level. To compare models with different capabilities, the method needs to inversely utilize task performance estimation: deriving equivalent parameter counts based on target performance. Using the method proposed in this work allows for a more accurate comparison of capability density between different model series.

% \section{Conclusion}
% \label{sec:conclusion}
% \noindent\textbf{Conclusion}



%\section*{Acknowledgment}
% \clearpage

% {\small
\bibliographystyle{plainnat}
\bibliography{main}
% }

% \clearpage

\newpage

\appendix
\setcounter{table}{0} 
\setcounter{figure}{0}
\setcounter{proposition}{0}



\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\theproposition}{A.\arabic{proposition}}

\section{Improvements of Clustering Algorithm}
\label{sec:clustering_opt}
% 具体来说，我们迭代式地使用MeanShift算法，并预设一个聚类半径R，以及一个类簇的最小样本数量K。在每次迭代中，对于完成聚类的样本，我们检验每个样本到类簇中心的距离是否大于R，并把大于R的样本重新标记为没有聚类。同时，对于样本数量小于K的聚类，我们也把这些类的样本标记为没有聚类。在每次迭代结束，我们把MeanShift的离群点与我们标记出没有聚类的样本重新纳入下一轮地聚类中，直到所有样本类别不再发生变化。我们把我们把伪代码展示在\cref{improved_shift}。
\subsection{Improved MeanShift Algorithm}
\label{sec:improved}
We iteratively apply the MeanShift algorithm with a predefined cluster radius $R$ and a minimum cluster size $K$. 
In each iteration, for the clustered samples, we examine whether the distance between each sample and its cluster center exceeds R, and relabel those samples that exceed this threshold as unclustered. 
For clusters containing fewer than K samples, we mark all samples in these clusters as unclustered. 
At the end of each iteration, we incorporate both the outliers from MeanShift and our marked unclustered samples into the next round of clustering, continuing this process until no further changes occur in sample labels. We present the pseudocode in \cref{algo:improved_meanshift}.

\begin{algorithm}
\caption{Iterative MeanShift Clustering Algorithm}
\label{algo:improved_meanshift}
\begin{algorithmic}[1]
\State Initialize all labels in the evaluation set to $-1$
\Repeat
    \State Perform MeanShift clustering with radius $R$ on all samples labeled $-1$
    \State Assign new labels to clustered samples
    \For{each newly labeled sample $i$}
        \State Calculate distance $dist_i$ to its cluster center
        \If{$dist_i > R$}
            \State Reset label to $-1$
        \EndIf
    \EndFor
    \For{each cluster}
        \If{number of samples in cluster $< K$}
            \State Reset all samples in this cluster to $-1$
        \EndIf
    \EndFor
    \State Renumber all non-$\{-1\}$ newly labeled samples to avoid overlap with old labels
\Until{no label changes}
\end{algorithmic}
\end{algorithm}

\paragraph{Filtering Zero-performance Samples}
In the evaluation set, there may exist a few extremely difficult problems that require sufficient model parameters to emerge. All small models may fail to solve these problems even after 100 evaluation attempts, resulting in difficulty feature vectors of all zeros. We refer to these as zero-performance samples. Their presence leads to two issues:

\begin{enumerate}
    \item Zero performance on small models does not necessarily indicate zero accuracy on large models. For these samples, we cannot estimate when emergence will occur or predict large model metrics.
    \item During clustering, they may be confused with other low-performing but non-zero samples. Including them in the same cluster would lower the expected accuracy of that cluster, leading to inaccurate fitting and extrapolation later.
\end{enumerate}

Therefore, we pre-filter these zero-performance samples before clustering, treating them as outliers that do not participate in the clustering process. This approach eliminates the need to consider their metrics under large models during subsequent extrapolation and prevents disruption to the clustering of normal difficult samples.

\subsection{Smoothing Techniques}
\label{sec:smoothing}
\noindent\textbf{Horizontal smoothing: adjacent checkpoint smoothing.}
Metric fluctuations of individual samples in downstream tasks are not solely due to limited sampling. Another potential factor is noise from uneven data distribution in recent training batches. Therefore, in addition to performing 100 evaluations to mitigate sampling variance, we evaluated 100 times on each of the adjacent checkpoints before and after the selected model. We then averaged these accuracy expectation values across three checkpoints, further reducing sampling variance while offsetting noise from uneven training data distribution. This approach also reduces the number of zero-performance samples, further improving clustering and prediction effectiveness.

\noindent\textbf{Vertical smoothing.}
Each sample's features represent the expected correct response rate across models of increasing size, forming a partially ordered sequence. However, the Euclidean distance used for measurement does not consider this sequential information. For example, if a cluster center has a feature sequence of $[0, 0, 0, 0.5]$, sample A with $[0, 0, 0.2, 0.5]$ and sample B with $[0.2, 0, 0, 0.5]$, sample A clearly fits the cluster better than sample B, yet their Euclidean distances are identical. 

Note that this smoothing method may not be effective for all downstream tasks. Our current observations suggest that for freeform tasks with limited solution spaces (such as multiple choice, ordering, or judgment questions in freeform format), once models learn to answer within the solution space, their random guess metrics on the evaluation set will be non-zero, more significantly affected by recent training batch data and few-shot cases in prompts. In such cases, vertical smoothing is more likely to bring positive benefits.

In our experiment, we only adopt horizontal smoothing, and leave vertical smoothing as an optional selection.

% \subsection{Setting Appropriate Clustering Radius}
% An appropriate clustering radius of the MeanShift algorithm can significantly improve clustering performance:
% \begin{itemize}
%     \item Small radius $\rightarrow$ fewer samples per cluster $\rightarrow$ high cluster metric variance $\rightarrow$ inaccurate predictions
%     \item Large radius $\rightarrow$ high intra-cluster variance $\rightarrow$ high extrapolation metric variance $\rightarrow$ inaccurate predictions
% \end{itemize}

% We performed grid search optimization of the clustering radius on each evaluation set and cross-validated across different evaluation sets. We ultimately selected the radius parameter that achieved the best overall clustering performance across all evaluation sets.



\renewcommand{\thefigure}{B\arabic{figure}}
\renewcommand{\thetable}{B\arabic{table}}
\renewcommand{\theproposition}{B.\arabic{proposition}}

\section{Proof of Proposition}
\label{sec:prop_proof}
We use \cref{proof:am_gm} to derive scaling law for downstream task performance (\cref{proof:task_scaling_law}).
\begin{proposition}[Arithmetic-geometric mean difference]
\label{proof:am_gm}
For any sequence of positive real numbers $\{x_i\}_{i=1}^n$, let:
\begin{itemize}
    \item $\mu_a = \frac{1}{n}\sum_{i=1}^n x_i$ be the arithmetic mean;
    \item $\mu_g = \prod_{i=1}^n x_i^{1/n}$ be the geometric mean; 
    \item $\sigma^2 = \frac{1}{n}\sum_{i=1}^n (x_i-\mu)^2$ be the variance.
\end{itemize}
Then the difference between the arithmetic mean and geometric mean can be estimated as:
\begin{align}
    \Delta = \mu_a-\mu_g = \frac{1}{n}\sum_{i=1}^n x_i-\left(\prod_{i=1}^n x_i\right)^{\frac{1}{n}} =\frac{\sigma^2}{2\mu_a}+o(\mu_a)
\end{align}
\end{proposition}

\begin{proof}
Taking the logarithm of the geometric mean $\mu_g$:
\begin{align}
    \log (\mu_g)=\frac{1}{n} \sum_{i=1}^n \log x_i
\end{align}

Using Taylor expansion of $\log x$ around $\mu$:
\begin{align}
    \log x = \log \mu + \frac{x-\mu}{\mu}-\frac{(x-\mu)^2}{2\mu^2}+o\left((x-\mu)^2\right)
\end{align}

We can simplify:
\begin{align*}
\log (GM) &= \frac{1}{n} \sum_{i=1}^n \log x_i \\
&= \log \mu + \frac{1}{n}\sum_{i=1}^n\left(\frac{(x_i-\mu_a)}{\mu_a}-\frac{(x_i-\mu_a)^2}{2\mu_a^2}\right)+o(\mu_a) \\
&= \log\mu +\frac{1}{\mu} \underbrace{\left(\frac{1}{n}\sum_{i=1}^nx_i -\mu_a\right)}_{\text{equal to 0}} + \frac{1}{2\mu_a^2}\underbrace{\left(\frac{1}{n}\sum_{i=1}^n(x_i-\mu_a)^2\right)}_{\sigma^2}+o(\mu_a)\\
&= \log \mu -\frac{\sigma^2}{2\mu^2} + o(\mu_a)
\end{align*}

Therefore:
\begin{align}
    \mu_a-\mu_g = \mu_a \left(1-\exp{\left(-\frac{\sigma^2} {2\mu^2}\right)}\right) + o(\mu_a)
\end{align}

When $\frac{\sigma^2}{2\mu^2}$ is small, this can be approximated as:
\begin{align}
    \Delta \approx \frac{\sigma^2}{2\mu_a}
\end{align}
\end{proof}

\begin{proposition}[Scaling law for downstream task performance]
\label{proof:task_scaling_law}
Given a language model trained with computational budget $C$, and a set of downstream tasks $P$, under the following assumptions:
\begin{enumerate}
    \item The relationship between the answer loss and compute follows a power law, $\frac{1}{n}\sum_{(q,\mathrm{ans})}\mathrm{loss}_{\mathrm{ans}}(C)\sim aC^{-b} +c$;
    \item For tasks with a finite answer set, the model gives a random guess choice if it cannot truly solve it;
    \item Task passrate equals the product of the predicted probability of each token $p_{\mathrm{ans}}=\prod_{t\in \mathrm{ans}} p(t)$, which means that each task has unique answer and model output answer only without thinking progress.
\end{enumerate}
The expected accuracy on tasks $P$ can be modeled as:
\begin{align}
    \mathbb{E}_p[\mathrm{Acc}(C)]=g+(1-g)\left(e^{-aC^{-b}-c} + \frac{\sigma^2}{2\mu}\right)+o(\mu)
\end{align}
where:
\begin{itemize}
    \item $g$ represents the random guess performance floor;
    \item $a,b,c$ are positive constants;
    \item $\mu = \frac{1}{\#P}\sum_{(q,\mathrm{ans})\in P} loss_{\mathrm{ans}}$;
    \item $\sigma^2 = \frac{1}{\#P}\sum(\mathrm{loss}_{\mathrm{ans}\_t}-\mu)^2$.
\end{itemize}
\end{proposition}

\begin{proof}
We first use assumption 3 to establish the relationship between model passrate and loss on a task.
\begin{align}
\label{eq:loss_passrate}
    -\log (p_{\mathrm{ans}})= -\log\left(\prod_{t\in \mathrm{ans}}p(t)\right)=-\sum_{t\in \mathrm{ans}}\log(p_{t})=\mathrm{loss}_{\mathrm{ans}}
\end{align}
Then take the exponential of both sides, and then take the expectation with respect to different tasks in the evaluation set $p=(q,\mathrm{ans})\in P$. We note that both $p_{\mathrm{ans}}$ and $\mathrm{loss}_{\mathrm{ans}}$ are functions of $C$.
\begin{align}
\mathbb{E}_p[p_{\mathrm{ans}}(C)]&=\mathbb{E}_p[\exp(-\mathrm{loss}_{\mathrm{ans}}(C))] \\
&=\frac{1}{n}\sum_{(q, \mathrm{ans})\in P} \exp(-\mathrm{loss}_{\mathrm{ans}}(C)).
\end{align}
We can adopt \cref{proof:am_gm} to switch from arithmetic mean to geometric mean of $\mathrm{loss}$, and apply the power law assumption 1. 
\begin{align}
\label{eq:loss_function}
    \frac{1}{n}\sum_{(q, \mathrm{ans}_t)\in P} \exp(-\mathrm{loss}_{\mathrm{ans}}(C)) &= \exp{\underbrace{\left(-\frac{1}{n}\sum_{(q, \mathrm{ans}_t)\in P}\mathrm{loss}_{\mathrm{ans}}(C)\right)}_{\text{use loss scaling law}}} + \frac{\sigma^2}{2\mu} + o(\mu) \\
    &= \exp{(-aC^{-b}-c)} + \frac{\sigma^2}{2\mu} + o(\mu) 
\end{align}
where $n=\#P$, and $\mu$, $\sigma^2$ follow definitions in the proposition.

Finally, we use assumption 2 to align the answer passrate and the accuracy metric. We can adopt the law of total expectation:
\begin{align}
\label{eq:total_expectation}
    \mathbb{E}_p[\mathrm{Acc}(C)] &= \mathbb{P}_p(\mathrm{correct})\mathbb{E}[\mathrm{Acc}|\mathrm{correct}] + \mathbb{P}_p(\mathrm{incorrect})\mathbb{E}[\mathrm{Acc|incorrect}]
\end{align}
Note that $\mathbb{P}_p(\mathrm{correct})=\mathbb{E}_p[p_{\mathrm{ans}}(C)]$, $\mathbb{E}[\mathrm{Acc}|\mathrm{correct}]=1$ and $\mathbb{P}_p(\mathrm{incorrect}) = 1- \mathbb{P}_p(\mathrm{correct})$. We also define $g$ as the random guess accuracy performance, thus we have $\mathbb{E}[\mathrm{Acc|incorrect}]=g$. Take these results into \cref{eq:total_expectation}, and we have:
\begin{align}
  \mathbb{E}_p[\mathrm{Acc}(C)]  &=\mathbb{E}_p[p_{\mathrm{ans}}(C)] + (1-\mathbb{E}_p[p_{\mathrm{ans}}(C)])*g\\
  &=g + (1-g) \mathbb{E}_p[p_{\mathrm{ans}}(C)] \\
    &=g+(1-g)\left(e^{-aC^{-b}-c} + \frac{\sigma^2}{2\mu}\right)+o(\mu)
\end{align}

\end{proof}

\cref{proof:task_scaling_law} demonstrates that a metric of an evaluation set with similar difficulty features can be effectively modeled using the following formula:
\begin{align}
f(C) = g+(1-g)\exp{(-aC^{-b}-c)}
\end{align}



\renewcommand{\thefigure}{C\arabic{figure}}
\renewcommand{\thetable}{C\arabic{table}}
\renewcommand{\theproposition}{C.\arabic{proposition}}

\section{Additional Ablation Studies}
\label{sec:additional_ablation}
\subsection{Comparison of Clustering Methods on Extra Evaluation Sets.}
\label{appdix:cluster}
% 我们补充在更多评估集上的聚类评估结果，这些结果结论与正文中保持一致。
We provide additional clustering evaluation results across more evaluation sets in~\cref{tab:clustering_advanced1} and~\cref{tab:clustering_advanced2}, which maintain consistency with the conclusions presented in the main text.
\begin{table}[!t]
\centering
\small
\caption{Clustering performance on advanced task benchmarks (IAD: Intra-cluster Average Distance, OR: Outlier Rate)}
%\vspace{0.1cm}
%\resizebox{\textwidth}{!}{
\begin{tabular}{c|c*{7}{c}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{TriviaQA} & \multicolumn{2}{c}{AGIEval} & \multicolumn{2}{c}{DROP} & \multicolumn{2}{c}{MBPP} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
& IAD$\downarrow$ & OR(\%) & IAD$\downarrow$ & OR(\%) & IAD$\downarrow$ & OR(\%) & IAD$\downarrow$ & OR(\%) \\
\midrule
K-Means & 0.4388 & - & 0.4572 & - & 0.5554 & - & 0.3383 & - \\
DBScan & 0.7039 & 6.38 & 0.5591 & 3.67 & 0.6651 & 11.08 & 0.5060 & 12.80 \\
MeanShift & 0.2521 & 6.77 & 0.2886 & 2.99 & 0.2507 & 11.81 & 0.2167 & 15.60 \\
Improved-KMeans & \textbf{0.1239} & 11.97 & \textbf{0.1536} & 7.60 & \textbf{0.1428} & 21.42 & \textbf{0.1667} & 19.40 \\
Improved-MeanShift & \ul{0.1871} & 11.54 & \ul{0.2100} & 11.50 & \ul{0.1974} & 19.88 & \ul{0.1745} & 21.60 \\
\bottomrule
\end{tabular}
%}
\label{tab:clustering_advanced1}
\end{table}

\begin{table}[!t]
\centering
\caption{Prediction errors on advanced task benchmarks.}
\small
%\vspace{0.1cm}
%\resizebox{\textwidth}{!}{
\setlength{\tabcolsep}{4.5pt}
\begin{tabular}{c|cccc|{c}*{8}{c}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Mean} & \multicolumn{2}{c|}{Max} & \multicolumn{2}{c}{TriviaQA} & \multicolumn{2}{c}{AGIEval} & \multicolumn{2}{c}{DROP} & \multicolumn{2}{c}{MBPP} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
& EE & FE & EE & FE & EE & FE & EE & FE & EE & FE & EE & FE \\
\midrule
K-Means & 2.44 & 3.36 & 2.97 & 8.99 & 2.97 & 2.46 & 2.61 & 2.68 & 1.66 & 1.64 & 2.53 & 2.67 \\
DBScan & 3.04 & 2.79 & 6.43 & 4.36 & 1.11 & 0.81 & 6.43 & 6.27 & 3.03 & 2.66 & 1.57 & 1.41 \\
MeanShift & 3.21 & 2.34 & 4.18 & 4.90 & 3.64 & 4.90 & 2.63 & 3.23 & 4.18 & 4.00 & 2.40 & 1.22 \\
Improved-KMeans & 3.38 & 3.80 & 5.96 & 5.56 & 1.18 & 1.12 & 5.96 & 5.56 & 3.99 & 5.24 & 2.39 & 3.25 \\
Improved-MeanShift & \textbf{1.13} & \textbf{1.61} & \textbf{1.58} & \textbf{2.38} & 1.58 & 1.64 & 1.11 & 2.38 & 0.26 & 0.23 & 1.56 & 2.19 \\
\bottomrule
\end{tabular}
%}
\label{tab:clustering_advanced2}
\end{table}
\subsection{Interpolation Method}
\label{sec:interpolation_ablation}
To evaluate different interpolation methods for prediction accuracy, we compared various mathematical approaches. Our baseline method uses quartic polynomial interpolation, which we compare against several alternative approaches, including Cubic spline interpolation, Cubic polynomial interpolation, and Quintic polynomial interpolation

The comparative results across different benchmarks are shown in \cref{tab:interpolation_results}. We report the prediction error between the real performance of a large model and the mapping result.

\begin{table}[!t]
\caption{Comparison of different interpolation methods across benchmarks.}
\centering
\small
\begin{tabular}{c|ccc}
\toprule
Prediction Error & BBH & Math & MMLU-pro \\
\midrule
Cubic Spline & 0.68 & 1.31 & 1.37 \\
Cubic Polynomial & 3.38 & 1.12 & 1.35 \\
Quintic Polynomial & 0.18 & 1.42 & 1.36 \\
Quartic Polynomial & 1.77 & 1.28 & 1.35 \\
\bottomrule
\end{tabular}
\label{tab:interpolation_results}
\end{table}
% 此外，我们在\cref{fig:mappling_results_bbh}绘制里不同内插公式Mapping过程的图像，横轴表示可预估子集指标，纵轴表示全集指标。其中红色点是待拟合的数值，绿色点表示预测值，紫色点表示Anchor Point，蓝色点表示真实表现。
% 预测效果在不同插值方法之间具有一定鲁棒性，我们希望在保持低预测误差的情况下采用尽可能简单的内插函数。 综合以上结果，我们看到Cubic Polynomial由于在BBH上欠拟合出现了较大的预测误差，而Cubic Spline具有一定的过拟合现象。Quartic Polynomial与Quintic Polynomial表现不错，因此我们选取了拟合参数更少的Quartic Polynomial方法。
Furthermore, in \cref{fig:mappling_results_bbh}, we plot the mapping process using different interpolation formulas, where the x-axis represents the predictable subset indices and the y-axis represents the full set indices. The red points are the numerical values to be fitted, green points represent predicted values, purple points represent anchor points and blue points show the actual performance.

The prediction performance shows certain robustness across different interpolation methods. We aim to use the simplest possible interpolation function while maintaining low prediction errors. Based on the above results, we observe that the Cubic Polynomial shows larger prediction errors due to underfitting on BBH, while the Cubic Spline exhibits some overfitting. Both Quartic Polynomial and Quintic Polynomial perform well, therefore we chose the Quartic Polynomial method as it requires fewer fitting parameters.
\begin{figure}[!t]
    \centering
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/experiments/mapping/cubic_spline.pdf}
        % \caption*{Cubic Polynomial}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/experiments/mapping/cubic_polynomial.pdf}
        % \caption*{Quartic Polynomial}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/experiments/mapping/quintic_polynomial.pdf}
        % \caption*{Quintic Polynomial}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/experiments/mapping/quartic_polynomial.pdf}
        % \caption*{Cubic Spline}
    \end{minipage}
    \caption{Performance mapping with different interpolation methods on BBH evaluation set. The cubic spline is overfitted, and the cubic polynomial method is underfitted. Quartic polynomials and quintic polynomials are comparable while quartic polynomial has fewer parameters.}
    \label{fig:mappling_results_bbh}
\end{figure}


\renewcommand{\thefigure}{D\arabic{figure}}
\renewcommand{\thetable}{D\arabic{table}}
\renewcommand{\theproposition}{D.\arabic{proposition}}

\section{Difficulty Distribution of Predictable Subset}
\label{sec:difficulty_distribution}
% 我们观察在不同难度层级上的可预估子集任务占比，我们把不同评估集的可预估子集与全集的难度分布绘制在了~\cref{fig:diff_dist}中。我们采用12B模型的得分做为难度分级的依据。可以看出，在MMLU-pro, GSM8k评估集上，可预估子集占比较大。说明该评估集大部分题目Scaling性质良好。而在Math评估集上，很多得分接近0的困难题目不在可预估子集中，这部分题目的预估则需要Mapping阶段进行修正。同时BBH在各个难度分布上可预估子集的占比接近，这是因为BBH有部分题目随Compute提升的提升收益较小，表现出震荡走势。

% 我们可以通过可预估子集的占比来评估测试集质量。对于可预估子集占比更大的评估集，我们在小模型上进行的实验结论会更加可靠。
% 同时在我们构建评估集时，对不可预估类簇进行筛查或补充。保证每一类难度特征题目的最小数量，以此降低指标波动性。
We analyze the proportion of predictable subset tasks across different difficulty levels. The difficulty distributions of predictable subset versus complete sets for different evaluation benchmarks are illustrated in \cref{fig:diff_dist}. We use the scores from the 12B model as the basis for difficulty classification. The results show that MMLU-pro and GSM8k evaluation sets have larger proportions of predictable subset, indicating that most questions in these datasets exhibit good performance scaling properties. In contrast, many difficult questions with near-zero scores in the Math evaluation set fall outside the predictable subset, requiring adjustment during the mapping phase. Meanwhile, BBH shows consistent proportions of predictable subset across difficulty levels, as some of its questions demonstrate oscillatory patterns with limited improvement despite increased computing.

The proportion of predictable subset can serve as a metric for assessing evaluation set quality. Evaluation sets with larger predictable subset yield more reliable experimental conclusions from smaller models. When constructing evaluation sets, we recommend screening or supplementing unpredictable clusters and ensuring a minimum number of questions for each difficulty feature to reduce metric volatility.

\begin{figure}[htbp]
    \centering
    % First row
    \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/appendix/difficulty_distributions/bbh.pdf}
        % \caption*{Title 1}
    \end{minipage}
    \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/appendix/difficulty_distributions/math.pdf}
        % \caption*{Title 2}
    \end{minipage}
    \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/appendix/difficulty_distributions/mmlu_pro.pdf}
        % \caption*{Title 3}
    \end{minipage}
    \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/appendix/difficulty_distributions/gsm8k.pdf}
        % \caption*{Title 4}
    \end{minipage}
    
    \vspace{0.5cm} % Add space between rows
    
    % Second row
    \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/appendix/difficulty_distributions/drop.pdf}
        % \caption*{Title 5}
    \end{minipage}
    \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/appendix/difficulty_distributions/mbpp.pdf}
        % \caption*{Title 6}
    \end{minipage}
    \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/appendix/difficulty_distributions/triviaqa.pdf}
        % \caption*{Title 7}
    \end{minipage}
    \begin{minipage}{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/appendix/difficulty_distributions/agieval.pdf}
        % \caption*{Title 8}
    \end{minipage}
    
    \caption{Difficulty distribution comparison on a 12B model between predictable subset and full evaluation set.}
    \label{fig:diff_dist}
\end{figure}


\renewcommand{\thefigure}{E\arabic{figure}}
\renewcommand{\thetable}{E\arabic{table}}
\renewcommand{\theproposition}{E.\arabic{proposition}}

\section{Limitations}
\label{sec:limiations}

\noindent\textbf{Influence of model structure and training configurations.} Mixture-of-Experts (MoE) models excel in training and inference cost, and are widely used in production. In this work, we reveal the performance scaling on dense transformers, while prediction on MoE models is still underexplored.
% 不过我们认为所提方法不会收到模型结构影响很多，如果我们把完整Pipeline应用在MoE模型上，我们同样预期能获得相似效果。
However, we believe that the proposed method is not significantly affected by the model architecture. If we apply the complete pipeline to MoE models, we expect to achieve similar results.
% 我们只研究了恒定学习率的预训练能力预估，没有覆盖到退火阶段对指标的影响。该阶段通常采用更高质量的数据，可以快速提升模型能力，也因此Performance Prediction具有更大挑战。
Besides, we only study the pre-training performance prediction with a constant learning rate and do not cover the impact of the annealing training. In this phase, higher-quality data is usually adopted, which can rapidly improve the model's capabilities. As a result, performance prediction faces greater challenges.

\noindent\textbf{Category of evaluation sets.}
% 我们提出的Clustering-on-Difficulty Method对测试集题目数量有一点要求，过少的题目导致类簇指标不稳定从而无法有效预估。不过从测试集设计的角度看，一个具有良好预估性质的评估集可以更有效的通过小模型实验结果推广到大模型，对于模型迭代具有更好的指导意义。
%
%此外，针对Multi-choice类型题目，仅需要保证预测出正确选项概率大于其余选项，此时指标与模型真实的Passrate存在差异。考虑到更多的评估集采用Chain-of-Thoughts模式，我们没有覆盖近输出选项的选择题任务。
The proposed Clustering-on-Difficulty method requires a sufficient number of test cases, as too few samples can lead to unstable cluster metrics and ineffective estimation. However, from an evaluation set design perspective, an evaluation set with good predictive properties enables more effective generalization from small-scale to large-scale models, thus providing better guidance for model iteration.

Furthermore, for multiple-choice tasks, the model only needs to assign a higher probability to the correct option compared to others, creating a discrepancy between this metric and the model's true passrate. Given that more evaluation sets are adopting the Chain-of-Thoughts prompts, we have not included multiple-choice tasks that only require option selection.

\noindent\textbf{Chain-of-thought performance prediction}.
% 我们提出的Scaling Law for Downstream Tasks~\cref{proof:task_scaling_law}假设了测试集评价模型直接给出答案的能力。但实际上越来越多的评测会允许模型进行思考后给出答案。Inference time scaling更进一步表明在数学、推理、代码等任务上，通过训练模型通过更长推理计算来完成任务可以大幅提升下游任务效果。
% 在推理过程或答案不唯一的情况下，模型在一个Task的Answer Loss与其Passrate不一定具有$p_{ans}=\exp(-loss_{ans})$的关系。虽然我们的方案在该情况下保持其预估效果，但理论解释是不充分的。因此，我们把根据Chain-of-thought特性改进预估方法与扩充理论依据作为未来工作。
\cref{proof:task_scaling_law} assumes that evaluation sets directly assess models' ability to provide answers. However, increasingly more evaluations allow models to think before providing answers. Recent works on inference time scaling~\cite{snell2024scaling, bansal2024smaller} further demonstrate that for tasks involving mathematics, reasoning, and coding, training models to complete tasks through longer inference computation can significantly improve downstream task performance. In cases where the reasoning process or answers are not unique, the relationship between a model's answer loss and passrate on a task may not necessarily follow the exponential relationship between the answer loss and the sample passrate. Although our approach maintains its prediction effectiveness in such situations, the theoretical explanation for these cases is insufficient. Therefore, we consider improving prediction methods based on chain-of-thought characteristics and expanding theoretical foundations as future work.

\end{document}