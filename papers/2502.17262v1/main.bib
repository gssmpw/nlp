@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI)})
@String(IJCV = {Int. J. Comput. Vis. (IJCV)})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)})
@String(ICCV= {Int. Conf. Comput. Vis. (ICCV)})
@String(ECCV= {Eur. Conf. Comput. Vis. (ECCV)})
@String(NIPS= {Adv. Neural Inform. Process. Syst. (NeurIPS)})
@String(ICPR = {Int. Conf. Pattern Recog. (ICPR)})
@String(BMVC= {Brit. Mach. Vis. Conf. (BMVC)})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process. (TIP)})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph. (TVCG)})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia (ACM MM)})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Rep. (ICLR)})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(ACL = {Annual Meeting of the Association for Computational Linguistics})



@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})
@String(ICML = {Int. Conf. Mach. Learn. (ICML)})
@String(PR    = {Pattern Recognition})
@String(NML	  = {Nature Mach. Intell.})


@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}


@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}




@article{llmscalinglaw,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

### Introduction

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  year={2021},
  volume={64},
  number={3},
  pages={107--115}
}


@article{chen2021humaneval,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

### Related Work

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}


@inproceedings{zhai2022scaling,
  author       = {Xiaohua Zhai and
                  Alexander Kolesnikov and
                  Neil Houlsby and
                  Lucas Beyer},
  title        = {Scaling Vision Transformers},
  booktitle    = CVPR,
  pages        = {1204--1213},
  year         = {2022}
}

@article{henighan2020scaling,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Maxwell and Levskaya, Anselm and McCandlish, Sam and Stuhlmuller, Andreas and Gray, Scott and Amodei, Dario},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}

@article{alabdulmohsin2022scaling,
  title={Revisiting Neural Scaling Laws in Language and Vision},
  author={Alabdulmohsin, Ibrahim M and Neyshabur, Behnam and Zhai, Xiaohua},
  journal=NIPS,
  volume={35},
  pages={22300--22312},
  year={2022}
}


@article{ma2024scaling,
  title={Do Neural Scaling Laws Exist on Graph Self-Supervised Learning?},
  author={Ma, Qian and Mao, Haitao and Liu, Jingzhe and Zhang, Zhehua and Feng, Chunlin and Song, Yu and Shao, Yihan and Ma, Yao},
  journal={arXiv preprint arXiv:2408.11243},
  year={2024}
}

@inproceedings{tay2021scale,
  author       = {Yi Tay and
                  Mostafa Dehghani and
                  Jinfeng Rao and
                  William Fedus and
                  Samira Abnar and
                  Hyung Won Chung and
                  Sharan Narang and
                  Dani Yogatama and
                  Ashish Vaswani and
                  Donald Metzler},
  title        = {Scale Efficiently: Insights from Pretraining and Finetuning Transformers},
  booktitle    = ICLR,
  year         = {2022}
}



@article{hernandez2021scaling,
  title={Scaling laws for transfer},
  author={Hernandez, Danny and Kaplan, Jared and Henighan, Tom and McCandlish, Sam},
  journal={arXiv preprint arXiv:2102.01293},
  year={2021}
}

@article{tao2024scaling,
  title={Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies},
  author={Tao, Chaofan and Liu, Qian and Dou, Longxu and Muennighoff, Niklas and Wan, Zhongwei and Luo, Ping and Lin, Min and Wong, Ngai},
  journal={arXiv preprint arXiv:2407.13623},
  year={2024}
}


@article{shao2024scaling,
  title={Scaling Retrieval-Based Language Models with a Trillion-Token Datastore},
  author={Shao, Rulin and He, Jacqueline and Asai, Akari and Shi, Weijia and Dettmers, Tim and Min, Sewon and Zettlemoyer, Luke and Koh, Pang Wei},
  journal={arXiv preprint arXiv:2407.12854},
  year={2024}
}


@article{yang2022hyperparameters,
  title={Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer},
  author={Yang, Greg and Hu, Edward J and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2203.03466},
  year={2022}
}

@article{lingle2024hyperparameters,
  title={A Large-Scale Exploration of $\mu$-Transfer},
  author={Lingle, Lucas},
  journal={arXiv preprint arXiv:2404.05728},
  year={2024}
}



@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}



@inproceedings{schaeffer2024emergent,
  author       = {Rylan Schaeffer and
                  Brando Miranda and
                  Sanmi Koyejo},
  title        = {Are Emergent Abilities of Large Language Models a Mirage?},
  booktitle    = NIPS,
  year         = {2023}
}


@article{chen2024scaling,
  title={Scaling Laws for Predicting Downstream Performance in LLMs},
  author={Chen, Yangyi and Huang, Binxuan and Gao, Yifan and Wang, Zhengyang and Yang, Jingfeng and Ji, Heng},
  journal={arXiv preprint arXiv:2410.08527},
  year={2024}
}

@article{gadre2024language,
  title={Language models scale reliably with over-training and on downstream tasks},
  author={Gadre, Samir Yitzhak and Smyrnis, Georgios and Shankar, Vaishaal and Gururangan, Suchin and Wortsman, Mitchell and Shao, Rulin and Mercat, Jean and Fang, Alex and Li, Jeffrey and Keh, Sedrick and others},
  journal={arXiv preprint arXiv:2403.08540},
  year={2024}
}

@article{du2024understanding,
  title={Understanding emergent abilities of language models from the loss perspective},
  author={Du, Zhengxiao and Zeng, Aohan and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2403.15796},
  year={2024}
}

@article{xiao2024densing,
  title={Densing Law of LLMs},
  author={Xiao, Chaojun and Cai, Jie and Zhao, Weilin and Zeng, Guoyang and Han, Xu and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2412.04315},
  year={2024}
}

@inproceedings{hu2023predicting,
  title={Predicting Emergent Abilities with Infinite Resolution Evaluation},
  author={Hu, Shengding and Liu, Xin and Han, Xu and Zhang, Xinrong and He, Chaoqun and Zhao, Weilin and Lin, Yankai and Ding, Ning and Ou, Zebin and Zeng, Guoyang and others},
  booktitle=ICLR,
  year={2024}
}

@article{owen2024predictable,
  title={How predictable is language model benchmark performance?},
  author={Owen, David},
  journal={arXiv preprint arXiv:2401.04757},
  year={2024}
}


@article{hu2024minicpm,
  title={Minicpm: Unveiling the potential of small language models with scalable training strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={arXiv preprint arXiv:2404.06395},
  year={2024}
}



% Method 

@inproceedings{macqueen1967some,
  title={Some methods for classification and analysis of multivariate observations},
  author={MacQueen, James and others},
  booktitle={Proceedings of the fifth Berkeley symposium on mathematical statistics and probability},
  volume={1},
  number={14},
  pages={281--297},
  year={1967}
}

@article{thorndike1953belongs,
  title={Who belongs in the family?},
  author={Thorndike, Robert L},
  journal={Psychometrika},
  volume={18},
  number={4},
  pages={267--276},
  year={1953},
  publisher={Springer}
}

@article{rousseeuw1987silhouettes,
  title={Silhouettes: a graphical aid to the interpretation and validation of cluster analysis},
  author={Rousseeuw, Peter J},
  journal={Journal of computational and applied mathematics},
  volume={20},
  pages={53--65},
  year={1987},
  publisher={Elsevier}
}

@inproceedings{ester1996density,
  title={A density-based algorithm for discovering clusters in large spatial databases with noise},
  author={Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei and others},
  booktitle={{KDD}},
  volume={96},
  number={34},
  pages={226--231},
  year={1996}
}

@article{fukunaga1975estimation,
  title={The estimation of the gradient of a density function, with applications in pattern recognition},
  author={Fukunaga, Keinosuke and Hostetler, Larry},
  journal={IEEE Transactions on information theory},
  volume={21},
  number={1},
  pages={32--40},
  year={1975},
  publisher={IEEE}
}
### Experiment 

@article{yang2024qwen2,
  title={Qwen2.5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}


@inproceedings{hendrycks2020measuring,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  year=2021,
  booktitle=ICLR
}


@inproceedings{suzgun2023challenging,
  title={Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc and Chi, Ed and Zhou, Denny and others},
  booktitle={Findings of the Association for Computational Linguistics},
  pages={13003--13051},
  year={2023}
}


@inproceedings{hendrycks2021measuring,
  author       = {Dan Hendrycks and
                  Collin Burns and
                  Saurav Kadavath and
                  Akul Arora and
                  Steven Basart and
                  Eric Tang and
                  Dawn Song and
                  Jacob Steinhardt},
  title        = {Measuring Mathematical Problem Solving With the {MATH} Dataset},
  booktitle    = NIPS,
  year         = {2021}
}


@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}




@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{joshi2017triviaqa,
  title={TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  booktitle=ACL,
  pages={1601--1611},
  year={2017}
}


@inproceedings{zhong2023agieval,
  title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models},
  author={Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
  booktitle={Findings of the Association for Computational Linguistics},
  pages={2299--2314},
  year={2024}
}


@inproceedings{dua2019drop,
  author       = {Dheeru Dua and
                  Yizhong Wang and
                  Pradeep Dasigi and
                  Gabriel Stanovsky and
                  Sameer Singh and
                  Matt Gardner},
  title        = {{DROP:} {A} Reading Comprehension Benchmark Requiring Discrete Reasoning
                  Over Paragraphs},
  booktitle    = {{NAACL-HLT}},
  pages        = {2368--2378},
  year         = {2019}
}


@article{wang2024mmlu,
  title={Mmlu-pro: A more robust and challenging multi-task language understanding benchmark},
  author={Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and others},
  journal={arXiv preprint arXiv:2406.01574},
  year={2024}
}


@inproceedings{nguyen2024ceval,
  title={Ceval: A benchmark for evaluating counterfactual text generation},
  author={Seifert, Christin and Schl{\"o}tterer, J{\"o}rg and others},
  booktitle={International Natural Language Generation Conference},
  pages={55--69},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
### Conclusion and Discussion
@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@article{bansal2024smaller,
  title={Smaller, weaker, yet better: Training llm reasoners via compute-optimal sampling},
  author={Bansal, Hritik and Hosseini, Arian and Agarwal, Rishabh and Tran, Vinh Q and Kazemi, Mehran},
  journal={arXiv preprint arXiv:2408.16737},
  year={2024}
}



@article{lepikhin2020gshardscalinggiantmodels,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}