\clearpage

\twocolumn[
\begin{center}
    \fontsize{16}{25}\selectfont \textbf{Adapting Human Mesh Recovery with Vision-Language Feedback} \par
    \textnormal{Supplementary Material}
\end{center}
]

In this supplementary material, we provide additional details on data processing, model architecture, and more qualitative results.

\section{Additional Data Details}

\subsection{Prompt Generation} Prompt engineering plays a crucial role in enhancing both performance and efficiency. Given the complexity of accurately describing body poses, carefully crafted prompts are used to extract detailed human pose descriptions. Specifically, GPT-4~\cite{GPT4} is first employed to automatically generate ten prompt sentences for each body part, as shown in Figure~\ref{fig:sup_gpt}. These prompts are then manually reviewed to ensure their accuracy and minimize ambiguity.

\begin{figure}[h]
    \centering
    \includegraphics[width=.35\textwidth]{figs/sup_gpt.pdf}
    \caption{\textbf{Prompt Generation.} We use GPT-4~\cite{GPT4} to automatically generate 10 prompts, which are then verified and used with ChatPose~\cite{ChatPose} to describe each part of the human pose.}
    \label{fig:sup_gpt}
\end{figure}

\vspace{-4mm}

\subsection{Part Description Generation.} 
Using the generated prompts, we feed the RGB image, human bounding box, and prompt sentences for each body part into ChatPose~\cite{ChatPose}, an open-source large model designed for extracting pose descriptions, as shown in Figure~\ref{fig:sup_des_process}. Since the character in video datasets exhibits minimal movement over short periods, we extract text descriptions from the 30th frame of every 60-frame sequence. Since CLIP~\cite{CLIP} accepts only short sentences, we use ChatPose to reorganize the key information and limit the final description to 77 words or fewer.

\begin{figure}[h]
    \centering
    \includegraphics[width=.40\textwidth]{figs/sup_des_process.pdf}
    \caption{\textbf{Schematic of description generation and reorganization.} We use the generated prompts to create descriptive texts for each image. The key information from these texts is then extracted, and the final description is truncated to 77 words or fewer.}
    \label{fig:sup_des_process}
\end{figure}

\begin{figure}[]
    \centering
    \includegraphics[width=.36\textwidth]{figs/sup_denoiser_network.pdf}
    \caption{\textbf{Architecture of our conditional diffusion model.} We adopt the Transformer architecture and replace the standard normalization layer with an adaptive normalization layer. This layer combines the noisy SMPL parameters, positional embeddings, and observations through adaptive normalization.}
    \label{fig:sup_denoiser_network}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/sup_vis.pdf}
    \caption{\textbf{More qualitative results.} From left to right are our method, HMR2, and ProHMR, including both front and side views. Our method has good alignment with better 3D accuracy.}
    \label{fig:enter-label}
\end{figure}
