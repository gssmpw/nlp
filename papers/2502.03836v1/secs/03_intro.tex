\section{Introduction}
\hbz{
\IEEEPARstart{M}{ONOCULAR} human mesh recovery aims to reconstruct 3D human meshes from a single image, which can be applied to various downstream tasks, such as 3D pose estimation~\cite{splposeestimation, splposeestimation1}, person re-identification~\cite{splperson-re1, splperson-re2, splperson-re3}, and crowd analysis~\cite{splcrowd}. This task is typically addressed using either regression-based~\cite{HMR, HMR2.0} or optimization-based~\cite{SMPLify, Refit} methods. Recent regression models~(\figref{fig:head_fig}(a)) leverage extensive human data to learn pose priors, enabling the prediction of accurate joint positions and body meshes. However, they often face challenges in aligning 3D models with 2D images due to the absence of explicit 2D-3D correspondences. In contrast, optimization-based methods(\figref{fig:head_fig}(b)) provide better model-to-image alignment but are sensitive to local minima and depth ambiguity, resulting in suboptimal joint accuracy. Additionally, off-the-shelf detectors\cite{fang2022alphapose} may introduce noises, which can degrade 3D reconstruction performance.

\begin{figure}[t] % 't' 选项确保图片在页面顶部
    \centering
    \includegraphics[width=.5\textwidth]{figs/head.pdf}
    \caption{\hbz{(a) Regression-based methods struggle with model-image alignment for challenging poses. (b) Optimization-based methods are prone to overfitting noisy 2D inputs and suffer from severe depth ambiguity. (c) Our method leverages prior knowledge from large vision-language models to improve both 2D and 3D performance.}}
    \label{fig:head_fig}
\vspace{-8mm}
\end{figure}

Several works~\cite{kolotouros2019learning, stathopoulos2024score} have attempted to integrate regression and optimization methods into a unified framework. These approaches first train a regression model to generate initial parameters and then refine the results using additional observations, such as 2D keypoints~\cite{kolotouros2019learning} and physical laws~\cite{huang2022neural, CloseInteraction}. However, 2D keypoints are often unreliable in complex environments (e.g., occlusions). Physics-based optimization also suffers from a knowledge gap between simulation and the real world, which may result in suboptimal simulated outcomes under the given constraints. Therefore, existing approaches have yet to fully address the trade-off between image observations and model-based assumptions.

Recently, human motion generation works~\cite{Guo_2022_CVPR, petrovich23tmr, jiang2024motiongpt} reveal that texts can provide rich 3D pose information. Therefore, our key idea is to leverage textual descriptions from large vision-language models (VLMs)~\cite{ChatPose} to compensate for insufficient 2D image observations. Benefiting from the 3D reasoning capabilities of VLMs (e.g., a person sitting with one leg crossed over the other), text-image inputs can enhance 3D perception and 2D-3D consistency for human pose estimation, thereby reducing the trade-off between image observations and model-based assumptions.
}

\hbz{To this end, we propose a framework that combines regression and optimization approaches, leveraging both image observations and vision-language models (VLMs) to facilitate human mesh recovery. The initial pose is first predicted using a Vision Transformer (ViT)~\cite{ViT}, which may be inaccurate due to depth ambiguity. To refine the pose, part-aware interactive descriptions are further extracted from the image using a Vision-Language Model (VLM)~\cite{ChatPose} with carefully designed prompts. Since text cannot directly provide detailed pose information, we define the alignment between pose and text in the latent space as a guiding signal. Consequently, we train a shared space based on VQ-VAE to bridge the gap between these two modalities. In the reverse diffusion process, we evaluate the reconstructed pose using re-projection error and text-pose similarity, and then use the derived gradients as conditions in each timestep. With the text-image conditions, the initial pose is iteratively updated and will ultimately converge to the real pose. In summary, our key contributions are: (1) We propose a framework that integrates multi-modal feedback to achieve both accurate 3D pose estimation and precise model-image alignment. (2) We demonstrate that fine-grained textual interactive descriptions can enhance human mesh recovery. (3) We introduce a novel conditioning mechanism that combines vision and language observations to guide the diffusion process.}