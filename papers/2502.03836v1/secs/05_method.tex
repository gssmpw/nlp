\vspace{-4mm}
\section{Method}

\hbz{
In this work, we aim to reconstruct the human mesh from monocular images by optimizing body parameters to achieve accurate alignment with vision-language observations.

\vspace{-5mm}

\subsection{Preliminaries}

We use SMPL model~\cite{SMPL} with 6D representation~\cite{6D_Rotation} to represent 3D humans, and thus the parameters for a single person consists of pose $ \theta \in \mathbb{R}^{144} $, shape $ \beta \in \mathbb{R}^{10}$, and translation $\pi \in \mathbb{R}^{3}$. 

}
\vspace{-3mm}

\hbz{
\subsection{Initial Prediction}
Many diffusion-based methods~\cite{stable_diffusion} in the image generation domain rely on sampling from Gaussian noise and require numerous iterative steps during training. This results in a significant demand for large datasets and substantial computational resources, making direct image generation with diffusion models computationally expensive. To mitigate this, we follow \cite{CloseInteraction} to obtain an initial pose estimate through a regression-based approach, which serves as the starting point for the subsequent optimization process, ultimately ensuring more accurate human mesh recovery. To extract image features $I$, we use ViT~\cite{ViT} as the backbone, and integrate bounding-box information to regress the SMPL parameters, which is similar to CLIFF~\cite{li2022cliff}, where the translation $\pi$ is derived from the estimated camera parameters and transformed into the global coordinate system. The regressor network is trained on normal datasets by the following loss function:
\begin{equation}\label{5}
\mathcal{L}_{regressor} = \lambda_{smpl}\mathcal{L}_{smpl} + \lambda_{joint}\mathcal{L}_{joint} + \lambda_{reproj}\mathcal{L}_{reproj},
\end{equation}
% which is the weighted sum of the supervisions from the SMPL parameters, joint location and reprojection. The losses of SMPL and joint are calculated like:
where $\mathcal{L}_{smpl} = ||[\beta, \theta]-[\hat{\beta},  \hat{\theta}]||_2^2$, $\mathcal{L}_{joint} = ||J_{3D} - \hat{J}_{3D}||_2^2$
and the reprojection loss is given by: $\mathcal{L}_{reproj} = ||\Pi{(J_{3D})} - \hat{J}_{2D}||_2^2$,
where $\Pi(\cdot)$ projects the 3D joints to 2D image with camera parameters, and $\hat{J_{2D}}$ is the ground-truth 2D keypoints. $\lambda_{smpl}$, $\lambda_{joint}$, and $\lambda_{reproj}$ control the relative importance of each term.

\vspace{-2mm}
\hbz{
\subsection{Description Extraction and Modal Alignment}

Texts contain rich 3D information for describing human body poses, such as joint positions, part orientations and intra-body interactions, which provide essential cues to improve 3D pose estimation.
}

\vspace{-3mm}
\hbz{
\subsubsection{Description Extraction} We describe the human in the image with overall and part-based (e.g., head, arms, torso, and legs) textual descriptions, which offer a more holistic understanding of pose perception. Initially, a large language model (LLM) is used to automatically generate prompt templates for various body parts, such as \texttt{Describe the {part} interaction of the person. How are the {parts} positioned?}. Following this, the images and chosen prompts are fed into ChatPose, which generates the corresponding pose descriptions. Additional details are provided in Sup. Mat. A.
}

\begin{figure}[t] % 't' 选项确保图片在页面顶部
    \centering
    \includegraphics[width=.5\textwidth]{figs/alignment.pdf}
    \caption{\textbf{Pose-Text Alignment.} \hbz{We first train a discrete pose codebook via VQ-VAE. To bridge the gap between text and 3D pose modalities, we then train a text encoder to align the texts to body poses in latent space with contrastive learning.} 
    % We first train the pose encoder, decoder, and codebook using VQ-VAE, followed by training the text encoder. After encoding the paired text and pose into latent variables, we compute the contrastive loss and use the text features to reconstruct the pose, thereby aligning the text and pose representations. 
    }
    \label{fig:alignment_fig}
    \vspace{-4mm}
\end{figure}

\begin{figure*}[t] % 使用 figure* 环境
\centering
\includegraphics[width=\textwidth]{figs/pipeline.pdf}
\caption{\textbf{Overview of our method.} Given an image, a large vision-language model is first used to extract detailed interactive descriptions for the body parts. An initial prediction is then made, followed by the construction of a diffusion-based framework that refines this prediction using multi-modal feedback. At each time step, the gradients of 2D keypoints are computed, along with the similarity loss between text embeddings and the pose, while image features from the backbone are concatenated to form the condition $c$, which is then fed into the diffusion model to estimate the noise. The distribution is updated based on this guidance, ultimately yielding accurate body pose estimations.}
\label{fig:pipeline}
\vspace{-4mm}
\end{figure*}

\subsubsection{Pose-Text Alignment} \hbz{CLIP~\cite{CLIP} learns word embeddings through large-scale image-text contrastive learning, aligning representations with natural language distributions. However, these embeddings lack explicit structural information, such as joint positions and pose angles, which are essential for pose tasks. Thus, additional alignment between pose and text embeddings is necessary for pose optimization. 

For pose representation, we use VQ-VAE~\cite{van2017neural}, which quantizes the latent space into discrete encoding vectors, capturing structural features like joint positions and angles more effectively than traditional VAEs, which suffer from gradient vanishing and blurry generation. VQ-VAE's discrete representation is better suited for pose tasks as it directly encodes discrete features critical for pose. We train the VQ-VAE by optimizing the following objective:
\begin{equation}
\mathcal{L}_{vq} = \alpha \| \mathcal{E}_p(\theta) - \text{sg}[ \hat{Z} ] \|^2 + \| \mathcal{D}_p(\hat{Z}) - \theta \|^2,
\end{equation}
where $\mathcal{E}p$ and $\mathcal{D}p$ denote the pose encoder and decoder, respectively. The tokens in the codebook are represented by $\hat{Z}$. $\text{sg}[\cdot]$ and $\alpha$ refer to the stop-gradient operator and a hyperparameter. We begin by embedding the text into the CLIP space, represented as $f{c}$, and then use $\mathcal{E}t$ to map it into the pose feature space. To align the pose and text features, we apply a contrastive loss, and further refine the alignment through the reconstruction loss of the text features via the pose decoder. The following objective is used:}
\xcy{
\begin{equation}
\mathcal{L}_\text{align} = \underbrace{- \frac{1}{N} \sum_{i=1}^{N} \log \frac{\exp(z_i^{\text{pose}} \cdot z_i^{\text{text}} / \tau)}{\sum_{j=1}^{N} \exp(z_i^{\text{pose}} \cdot z_j^{\text{text}} / \tau)}
}_{\mathcal{L}_{\text{contra}}} + \mathcal{L}_{\text{rec}},
\end{equation}
\noindent where \( \mathcal{L}_{\text{rec}} = \| \mathcal{D}_p(\mathcal{E}_t(f_{c})) - \theta \|^2 \) and \( \mathcal{L}_{\text{contra}} \) represent the reconstruction and contrastive losses, respectively. \( z^{\text{pose}} \) and \( z^{\text{text}} \) represent the latent variables obtained from the encoder for the pose and text. $\tau$ is the temperature parameter, used to scale the similarity.}

\subsection{Vision-Language Feedback Adaptation}

Since the initial prediction involves minor misalignments and 3D pose errors, we formulate the optimization process as a distributional optimization, where the initial value serves as the mean of the initial distribution. We assume a probability distribution around the initial prediction, representing the potential optimization space. This allows us to fine-tune and optimize the model based on this distribution.

\subsubsection{Diffusion process} We assume that the optimized result $x$ follows a Gaussian distribution with the initial prediction \( \hat{x}^{\text{init}} \) as the mean and \( \sigma \) as the standard deviation. Assume the true distribution is $p(x)$. By training a diffusion model based on the contrast of the log gradients, \ie, $s_{\text{model}}(x; \phi) = \nabla_x \log q_\phi(x)$, the initial distribution can undergo gradient descent towards the true data distribution by the following loss function:
\begin{equation}
\mathcal{L}(\phi) = \mathbb{E}_{x \sim p(x)} \left[ \| s_{\text{model}}(x; \phi) - s_{\text{data}}(x) \|^2 \right],
\end{equation}
where \( s_{\text{model}}(x; \phi) \) and \( s_{\text{data}}(x) \) are the gradient of the model's distribution and true data with respect to \( x \). \( \mathbb{E}_{x \sim p(x)} \) denotes the expectation with respect to the data distribution \( p(x) \). During the inference phase, we can compute the gradient of the loss function with respect to the condition \( c \), and then adjust the generated sample as the following formula:

\begin{equation}
\Delta x_t = \Delta t \cdot \nabla_x \log q(x_t \mid c),
\end{equation}
where $\Delta t$ is a scaling factor, and  $\Delta x_t$ is the change in the sample $x_t$ at the current step.

\subsubsection{Vision-Language Guided Denoising} In the diffusion network, we refine the initial distribution using multi-modal observations. We treat information from different modalities as conditions 
$c$. This design allows prior information from various modalities to help for sampling out results that satisfy the condition.

\textbf{2D Keypoints.} 2D keypoint observations serve as valuable constraints due to the rich semantic information they provide. To detect keypoints, we employ an additional keypoint detector~\cite{fang2022alphapose}, followed by the computation of the gradient of the 3D joints with respect to the detected 2D keypoints:
\begin{equation}\label{10}
\mathcal{G}_{keyp} = \frac{\partial||\Pi(J_{3D})- p_{2D}||_2^2}{\partial J_{3D}},
\end{equation}
where $p_{2D}$ represents the detected 2D keypoints, and $J_{3D}$ is the set of 3D joints, computed as a linear combination of vertices: $J=WM$.

\textbf{Text.} Since human pose and depth are strongly coupled, relying solely on 2D information often leads to poor performance due to depth ambiguity and information loss after projection; thus, we consider text as additional information to implicitly constrain the body pose in 3D space. Specifically, we compute the similarity loss between the pose and text features in the latent space:
\begin{equation}
\mathcal{L}_{cos}  =  \left(
\frac{\mathcal{E}_p(\theta)\cdot\mathcal{E}_t(f_{c})}{\|\mathcal{E}_p(\theta)\|\|\mathcal{E}_t(f_{c})\|} \right)^2,
\end{equation}
The gradient of the pose parameters with respect to the similarity loss, $\mathcal{G}_{text} = \frac{\partial \mathcal{L}_{cos}}{\partial \hat{\theta}}$, can implicitly provide guidance. Finally, the condition $c = \text{concat}(I,\mathcal{G}_{\text{keyp}}, \mathcal{G}_{\text{text}})$ serves as vision-language feedback to optimize the sampling process.}

\vspace{-2mm}