@inproceedings{DALeToR,
author = {Yan, Le and Qin, Zhen and Pasumarthi, Rama Kumar and Wang, Xuanhui and Bendersky, Michael},
title = {Diversification-Aware Learning to Rank using Distributed Representation},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449831},
doi = {10.1145/3442381.3449831},
abstract = {Existing work on search result diversification typically falls into the “next document” paradigm, that is, selecting the next document based on the ones already chosen. A sequential process of selecting documents one-by-one is naturally modeled in learning-based approaches. However, such a process makes the learning difficult because there are an exponential number of ranking lists to consider. Sampling is usually used to reduce the computational complexity but this makes the learning less effective. In this paper, we propose a soft version of the “next document” paradigm in which we associate each document with an approximate rank, and thus the subtopics covered prior to a document can also be estimated. We show that we can derive differentiable diversification-aware losses, which are smooth approximation of diversity metrics like α-NDCG, based on these estimates. We further propose to optimize the losses in the learning-to-rank setting using neural distributed representations of queries and documents. Experiments are conducted on the public benchmark TREC datasets. By comparing with an extensive list of baseline methods, we show that our Diversification-Aware LEarning-TO-Rank (DALETOR) approaches outperform them by a large margin, while being much simpler during learning and inference.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {127–136},
numpages = {10},
keywords = {diversification-aware loss, learning-to-rank, search result diversification},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{MMRSearchRelevanceDivMeasures,
  title={Learning maximal marginal relevance model via directly optimizing diversity evaluation measures},
  author={Xia, Long and Xu, Jun and Lan, Yanyan and Guo, Jiafeng and Cheng, Xueqi},
  booktitle={Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval},
  pages={113--122},
  year={2015}
}

@inproceedings{R-LTR,
author = {Zhu, Yadong and Lan, Yanyan and Guo, Jiafeng and Cheng, Xueqi and Niu, Shuzi},
title = {Learning for search result diversification},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2609634},
doi = {10.1145/2600428.2609634},
abstract = {Search result diversification has gained attention as a way to tackle the ambiguous or multi-faceted information needs of users. Most existing methods on this problem utilize a heuristic predefined ranking function, where limited features can be incorporated and extensive tuning is required for different settings. In this paper, we address search result diversification as a learning problem, and introduce a novel relational learning-to-rank approach to formulate the task. However, the definitions of ranking function and loss function for the diversification problem are challenging. In our work, we firstly show that diverse ranking is in general a sequential selection process from both empirical and theoretical aspects. On this basis, we define ranking function as the combination of relevance score and diversity score between the current document and those previously selected, and loss function as the likelihood loss of ground truth based on Plackett-Luce model, which can naturally model the sequential generation of a diverse ranking list. Stochastic gradient descent is then employed to conduct the unconstrained optimization, and the prediction of a diverse ranking list is provided by a sequential selection process based on the learned ranking function. The experimental results on the public TREC datasets demonstrate the effectiveness and robustness of our approach.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research \& Development in Information Retrieval},
pages = {293–302},
numpages = {10},
keywords = {diversity, plackett-luce model, relational learning-to-rank, sequential selection},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}

@article{ResDivSearchReco,
author = {Wu, Haolun and Zhang, Yansen and Ma, Chen and Lyu, Fuyuan and He, Bowei and Mitra, Bhaskar and Liu, Xue},
title = {Result Diversification in Search and Recommendation: A Survey},
year = {2024},
issue_date = {Oct. 2024},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {36},
number = {10},
issn = {1041-4347},
url = {https://doi.org/10.1109/TKDE.2024.3382262},
doi = {10.1109/TKDE.2024.3382262},
abstract = {Diversifying return results is an important research topic in retrieval systems in order to satisfy both the various interests of customers and the equal market exposure of providers. There has been growing attention on diversity-aware research during recent years, accompanied by a proliferation of literature on methods to promote diversity in search and recommendation. However, diversity-aware studies in retrieval systems lack a systematic organization and are rather fragmented. In this survey, we are the first to propose a unified taxonomy for classifying the metrics and approaches of diversification in both search and recommendation, which are two of the most extensively researched fields of retrieval systems. We begin the survey with a brief discussion of why diversity is important in retrieval systems, followed by a summary of the various diversity concerns in search and recommendation, highlighting their relationship and differences. For the survey's main body, we present a unified taxonomy of diversification metrics and approaches in retrieval systems, from both the search and recommendation perspectives. In the later part of the survey, we discuss the open research questions of diversity-aware research in search and recommendation in an effort to inspire future innovations and encourage the implementation of diversity in real-world systems.},
journal = {IEEE Trans. on Knowl. and Data Eng.},
month = apr,
pages = {5354–5373},
numpages = {20}
}

@inproceedings{divRankPolicyNets,
author = {Feng, Yue and Xu, Jun and Lan, Yanyan and Guo, Jiafeng and Zeng, Wei and Cheng, Xueqi},
title = {From Greedy Selection to Exploratory Decision-Making: Diverse Ranking with Policy-Value Networks},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3209979},
doi = {10.1145/3209978.3209979},
abstract = {The goal of search result diversification is to select a subset of documents from the candidate set to satisfy as many different subtopics as possible. In general, it is a problem of subset selection and selecting an optimal subset of documents is NP-hard. Existing methods usually formalize the problem as ranking the documents with greedy sequential document selection. At each of the ranking position the document that can provide the largest amount of additional information is selected. It is obvious that the greedy selections inevitably produce suboptimal rankings. In this paper we propose to partially alleviate the problem with a Monte Carlo tree search (MCTS) enhanced Markov decision process (MDP), referred to as M$^2$Div. In M$^2$Div, the construction of diverse ranking is formalized as an MDP process where each action corresponds to selecting a document for one ranking position. Given an MDP state which consists of the query, selected documents, and candidates, a recurrent neural network is utilized to produce the policy function for guiding the document selection and the value function for predicting the whole ranking quality. The produced raw policy and value are then strengthened with MCTS through exploring the possible rankings at the subsequent positions, achieving a better search policy for decision-making. Experimental results based on the TREC benchmarks showed that M$^2$Div can significantly outperform the state-of-the-art baselines based on greedy sequential document selection, indicating the effectiveness of the exploratory decision-making mechanism in M$^2$Div.},
booktitle = {The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
pages = {125–134},
numpages = {10},
keywords = {monte carlo tree search, markov decision process, diverse ranking},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{liu2020dvgan,
  title={DVGAN: A minimax game for search result diversification combining explicit and implicit features},
  author={Liu, Jiongnan and Dou, Zhicheng and Wang, Xiaojie and Lu, Shuqi and Wen, Ji-Rong},
  booktitle={Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={479--488},
  year={2020}
}

@inproceedings{mdpSearchDiversify,
author = {Xia, Long and Xu, Jun and Lan, Yanyan and Guo, Jiafeng and Zeng, Wei and Cheng, Xueqi},
title = {Adapting Markov Decision Process for Search Result Diversification},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080775},
doi = {10.1145/3077136.3080775},
abstract = {In this paper we address the issue of learning diverse ranking models for search result diversification. Typical methods treat the problem of constructing a diverse ranking as a process of sequential document selection. At each ranking position, the document that can provide the largest amount of additional information to the users is selected, because the search users usually browse the documents in a top-down manner. Thus, to select an optimal document for a position, it is critical for a diverse ranking model to capture the utility of information the user have perceived from the preceding documents. Existing methods usually calculate the ranking scores (e.g., the marginal relevance) directly based on the query and the selected documents, with heuristic rules or handcrafted features. The utility the user perceived at each of the ranks, however, is not explicitly modeled. In this paper, we present a novel diverse ranking model on the basis of continuous state Markov decision process (MDP) in which the user perceived utility is modeled as a part of the MDP state. Our model, referred to as MDP-DIV, sequentially takes the actions of selecting one document according to current state, and then updates the state for the chosen of the next action. The transition of the states are modeled in a recurrent manner and the model parameters are learned with policy gradient. Experimental results based on the TREC benchmarks showed that MDP-DIV can significantly outperform the state-of-the-art baselines.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {535–544},
numpages = {10},
keywords = {learning to rank, markov decision process, search result diversification},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

