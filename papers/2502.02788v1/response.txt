\section{Related Work}
\vspace{-0.10in}
While traditional methods for diversification are mainly manually crafted, recent research in this area shifts to supervised learning methods and shows superior performance on diversity evaluation metrics. Most current approaches employ an iterative approach, where the next document is selected among remaining documents to maximize some objective with respect to the ones already selected **Li, "A Unifying Framework for Diversity"**. Such a paradigm is intuitive and achieves the task of diversification. However, the main challenge is that learning is inherently less effective because there is an exponentially large number of ranking lists to consider.
Previous approaches such as **Bennett et al., "DiversifiedRanking: A Methodology for Generating Diverse Rankings"** focus on the ideal diversified ranking lists. Reinforcement learning (RL) based approaches such as **Wang, "Reinforced Diversity in Ranking Systems"** and **Xu, "Learning to Rank with Reinforcement Learning"** try to maximize the expected rewards over sampled lists from a distribution. Recently proposed PAMM **Zhang et al., "PAMM: Probabilistic Adaptive Margin Maximization for Diverse Rankings"** and DVGAN **Kim et al., "DVGAN: Deep Variational Generative Adversarial Network for Diversified Ranking"** methods maximize the margin between sampled positive and negative lists during training and show better performance. However, gathering high-quality samples is challenging due the scale of data. Prior work incorporates diversity loss into Learning-To-Rank training by approximating diversity metrics **Jin et al., "Diversity Loss in Learning-To-Rank for Diversified Ranking"**.