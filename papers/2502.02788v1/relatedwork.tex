\section{Related Work}
\vspace{-0.10in}
While traditional methods for diversification are mainly manually crafted, recent research in this area shifts to supervised learning methods and shows superior performance on diversity evaluation metrics. Most current approaches employ an iterative approach, where the next document is selected among remaining documents to maximize some objective with respect to the ones already selected \cite{ResDivSearchReco}. Such a paradigm is intuitive and achieves the task of diversification. However, the main challenge is that learning is inherently less effective because there is an exponentially large number of ranking lists to consider.
Previous approaches such as \cite{R-LTR} focus on the ideal diversified ranking lists. Reinforcement learning (RL) based approaches such as \cite{divRankPolicyNets} and \cite{mdpSearchDiversify} try to maximize the expected rewards over sampled lists from a distribution. Recently proposed PAMM \cite{MMRSearchRelevanceDivMeasures} and DVGAN \cite{liu2020dvgan} methods maximize the margin between sampled positive and negative lists during training and show better performance. However, gathering high-quality samples is challenging due the scale of data. Prior work incorporates diversity loss into Learning-To-Rank training by approximating diversity metrics \cite{DALeToR}.