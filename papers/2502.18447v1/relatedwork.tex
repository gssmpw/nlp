\section{Related Work}
\label{sec:relatedwork}

To the best of our knowledge, our work is the first study of reward inference from general behavior. 
Here, we review prior work extending behavior imitation \citep{pomerleau1988alvinn} and reward inference \citep{ng2000algorithms} to use training data, account for various kinds of suboptimality, and learn from behavior classes other than demonstrations.







\textbf{Meta-IRL} Prior work has studied the use of multi-task demonstration datasets to improve the efficiency of IRL inference, under the names of multi-task IRL \citep{babes2011apprenticeship, dimitrakakis2012bayesian, choi2012nonparametric, gleave2018multitask}, meta-IRL \citep{xu2019learning, yu2019meta, ghasemipour2019smile, wang2021meta}, and lifelong IRL \citep{mendez2018lifelong}. Similarly to SRI, such works allow fast, data-efficient reward inference, but do not directly enable reward inference from suboptimal demonstrations.






\textbf{Behavior model misspecification} \citet{armstrong2018occam} showed that it is generally impossible to simultaneously infer a demonstrator's reward function and their behavior model; thus, reward inference methods must either assume the behavior model (as most methods do) or learn it from data (as \citet{shah2019feasibility} and SRI do). Later, \citet{Skalse2022MisspecificationII} showed theoretically that assuming behavior models is dangerous, as it almost always produces incorrect reward functions when the model is incorrect. For example, assuming Boltzmann rationality will only provide an asymptotically correct optimal policy set for ground-truth behavior models that take optimal actions most frequently, and no others.
Other works quantified the error induced by incorrect behavior models: \citet{shah2019feasibility} and \citet{chan2021human} showed that a variety of misspecifications can induce sometimes dramatically incorrect reward functions, while \citet{hong2023sensitivity} showed that in continuous-action MDPs, even arbitrarily small errors in the behavior model can result in almost arbitrarily large errors in the inferred reward parameters.



\textbf{Learning from suboptimal demonstrations} Reward inference algorithms have been developed to account for a wide variety of specific suboptimalities in human behavior, including hyperbolic discounting, myopia, false beliefs and bounded cognition \citep{evans2015learning, evans2016learning, zhi2020online}, autocorrelated action noise \citep{zheng2014robust}, mistaken transition models \citep{reddy2018where}, and risk-sensitive behavior \citep{singh2018risk, ratliff2020inverse}. Notably, humans have little trouble inferring and accounting for each other's suboptimality \citep{evans2016learning, zhi2020online}.

\citet{shiarlis2016inverse} studied the setting where demonstrations are arbitrarily suboptimal, but they are labeled as failures (to be avoided) or successes; similarly, \citet{brown2020safe} used preferences over suboptimal demonstrations. \citet{brown2020better} and \citet{chen2021learning} augment that paper by using noise injection to automatically rank synthetic demonstrations, learning a reward model that disprefers noisy trajectories.



\textbf{Learning from general behavior} \citet{hadfieldmenell2016cooperative} and \citet{malik2018efficient}
studied how to provide and learn from demonstrations which are selected according to their information content for the other agent, rather than how much reward they accumulate. \citet{shah2019feasibility} developed the research direction which is closest to SRI. They present two algorithms for human behavior models and rewards, both based on value iteration networks (VINs) \citep{tamar2016value}, and differentiable planners in general. While the second algorithm they present uses a heuristic to infer rewards from near-optimal agents, the first algorithm uses a similar setting to SRI: it trains a value iteration network on a dataset of demonstration policies and corresponding reward functions to predict a policy given a reward function. Then, at inference time, given a policy, the reward function is recovered through gradient descent. Unlike SRI, however, this algorithm expects the demonstrator's full policy at inference time, and has not been extended to continuous domains (which are beyond the default capabilities of VINs).