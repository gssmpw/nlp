\documentclass[twocolumn]{article} %

\usepackage{times}
\usepackage{abstract}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{makecell}

\usepackage[protrusion=true,expansion=true]{microtype}

\usepackage[top=2cm, bottom=2cm, left = 1.9cm, right = 1.9cm,columnsep=20pt]{geometry}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{amsmath}

\usepackage{amsthm}
\usepackage{amssymb}                %
\usepackage{mathtools}              %
\usepackage{mathrsfs}               %
\mathtoolsset{showonlyrefs}         %

\usepackage{lipsum}                 %

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{defn}{Definition}[]
\newtheorem{thm}{Theorem}[]
\newtheorem{claim}{Claim}[]
\newtheorem{prop}{Property}[]
\newtheorem{ass}{Assumption}[]
\newtheorem{cor}{Corollary}[]
\newcommand{\thistheoremname}{}

\theoremstyle{plain}
\newtheorem*{genericthm*}{\thistheoremname}
\newenvironment{namedthm*}[1]
  {\renewcommand{\thistheoremname}{#1}%
   \begin{genericthm*}}
  {\end{genericthm*}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\opt}{opt}
\DeclareMathOperator*{\supp}{supp}
\newcommand\norm[1]{\left\lVert#1\right\rVert}

\renewcommand{\textfraction}{0.2}     %
\renewcommand{\floatpagefraction}{0.8} %

\providecommand*{\defnautorefname}{\textcolor{black}{Definition}}
\providecommand*{\thmautorefname}{\textcolor{black}{Theorem}}
\providecommand*{\claimautorefname}{\textcolor{black}{Claim}}
\providecommand*{\lemmaautorefname}{\textcolor{black}{Lemma}}
\providecommand*{\propautorefname}{\textcolor{black}{Property}}
\providecommand*{\assautorefname}{\textcolor{black}{Assumption}}
\providecommand*{\corautorefname}{\textcolor{black}{Corollary}}

\usepackage{courier} %

\usepackage{graphicx} %
\usepackage{subcaption}
\usepackage[space]{grffile} %

\usepackage[numbers]{natbib}

\usepackage{hyperref}

\usepackage{xcolor}
\definecolor{dark-blue}{rgb}{0,0,0.7}
\definecolor{dark-green}{rgb}{0,0.6,0}
\definecolor{dark-red}{rgb}{0.8,0,0}
\definecolor{dark-yellow}{rgb}{0.8, 0.8, 0.0}
\definecolor{ppt-red}{HTML}{C00000}
\definecolor{ppt-blue}{HTML}{0F9ED5}
\hypersetup{
    colorlinks, linkcolor={dark-blue},
    citecolor={dark-blue}, urlcolor={dark-blue}
}


\usepackage{pifont}
\newcommand{\cmark}{\textcolor{dark-green}{\ding{51}}} %
\newcommand{\xmark}{\textcolor{dark-red}{\ding{53}}}

\RequirePackage{fancyhdr}
\pagestyle{fancy} %
\fancyhf{} %
\fancyhead[C]{Supervised Reward Inference}
\fancyfoot[C]{\thepage}

\newcommand{\toptitlebar}{
  \hrule height 4pt
  \vskip 0.25in
  \vskip -\parskip%
}
\newcommand{\bottomtitlebar}{
  \vskip 0.25in
  \vskip -\parskip
  \hrule height 1pt
  \vskip 0.09in%
}

\newcommand{\customtitle}[1]{%
  \vbox{%
    \toptitlebar
    \centering
    {\LARGE\bf #1\par}
    \bottomtitlebar
  }
}

\usepackage{authblk}



\usepackage{tabularx}

\renewcommand{\abstractnamefont}{\normalfont\large\bfseries}
\renewcommand{\abstracttextfont}{\normalfont\normalsize}


\title{\customtitle{Supervised Reward Inference}}

\author[1]{Will Schwarzer\thanks{\relax\protect{Correspondence to: \texttt{wschwarzer@umass.edu}}}}
\author[2]{Jordan Schneider\thanks{Work done while at the University of Texas.}}
\author[1]{Philip S. Thomas}
\author[1]{Scott Niekum}
\affil[1]{University of Massachusetts}
\affil[2]{Anthropic}
\date{}

\begin{document}

\maketitle


\begin{abstract}
    \noindent Existing approaches to reward inference from behavior typically assume that humans provide demonstrations according to specific models of behavior. However, humans often indicate their goals through a wide range of behaviors, from actions that are suboptimal due to poor planning or execution to behaviors which are intended to communicate goals rather than achieve them. We propose that supervised learning offers a unified framework to infer reward functions from any class of behavior, and show that such an approach is asymptotically Bayes-optimal under mild assumptions. Experiments on simulated robotic manipulation tasks show that our method can efficiently infer rewards from a wide variety of arbitrarily suboptimal demonstrations.
\end{abstract}

\section{Introduction}

In order for artificial agents to achieve human goals, humans must first communicate their goals to the agents. While the traditional method of goal communication in reinforcement learning (RL) is explicit reward specification, specifying correct rewards can be challenging \citep{hadfieldmenell2017inverse, ratner2018reward, booth23perils, knox2024how}. 
This has highlighted the need for alternative modalities for reward specification, such as human demonstrations, the modality studied in inverse reinforcement learning (IRL) \citep{ng2000algorithms}.

IRL generally assumes that demonstrations are generated according to a specific model of human behavior, which ranges from noisy optimality \citep{ng2000algorithms, Ramachandran2007bayesian, zheng2014robust, chan2021human, laidlaw2022boltzmann, barnett2023active} to bounded reasoning \citep{evans2015learning, evans2016learning, zhi2020online} and beyond. 
Yet while such models produce solvable learning problems, they are still far from accurate descriptions of the entirety of human behavior. First, real-world human behavior demonstrates all of these suboptimalities at once, and many more that have yet to be accounted for \citep{kahneman2011thinking, shah2019feasibility}; second, people frequently use entirely non-optimal behavior such as gestures in order to communicate their goals.

In this paper, we investigate one approach to learning rewards from the full range of human behavior: framing a human's actions as an \textit{indication} of their goals, rather than an attempted \textit{optimization} of them. Similar to previous work \citep{enayati2018skill, reddy2018where, shah2019feasibility}, we assume access to a dataset of behaviors (e.g., demonstrations or gestures) and their associated ground-truth rewards.
However, rather than explicitly learning a behavior model that maps these rewards onto the behaviors, we use supervised learning to directly learn a mapping of behaviors onto rewards. This approach, which we call Supervised Reward Inference (SRI), is fast, data efficient, and asymptotically Bayes-optimal (see Sections \ref{sec:theory} and \ref{sec:experiments}).

\begin{table*}
\caption{Comparison of several reward inference methods. Inverse planning refers to Algorithm 1 by \citet{shah2019feasibility}. The final three capabilities refer to the ability to infer at least Bayes-optimal rewards from demonstrations generated according to the specified type of behavior; for the latter two types, this assumes access to a dataset of demonstrations drawn from the same distribution (including the same behavior model) as the inference demonstrations. For CIRL \citep{hadfieldmenell2016cooperative}, we assume a non-interactive CIRL game with one demonstration and one deployment phase. Extensions of Algorithm 1 by \citet{shah2019feasibility} may be able to infer correct rewards from arbitrarily suboptimal behaviors in the settings we consider, and possibly from a small number of observed trajectories, but the current algorithm cannot (see Section \ref{sec:relatedwork}).
  }
  \centering
  \begin{tabularx}{\textwidth}{>{\hsize=2\hsize}X | >{\centering\arraybackslash\hsize=0.7\hsize}X>{\centering\arraybackslash\hsize=0.7\hsize}X>{\centering\arraybackslash\hsize=0.7\hsize}X>{\centering\arraybackslash\hsize=0.7\hsize}X | >{\centering\arraybackslash\hsize=0.7\hsize}X}
    \toprule
    Capability & IRL & Meta IRL & CIRL & Inverse Planning & \textbf{SRI} \\
    \midrule
    No reward dataset required & \cmark & \cmark & \cmark & \xmark & \xmark \\
    Few-shot inference & \xmark & \cmark & \xmark & \xmark & \cmark \\
    Simulator-free reward inference & \xmark  & \cmark & \xmark & \cmark & \cmark \\
    LfO possible & \cmark & \cmark & \cmark & \xmark & \cmark \\
    Known suboptimal behavior class & \cmark & \cmark & \cmark & \cmark & \cmark \\
    Limited suboptimal behavior class & \xmark & \xmark & \xmark & \cmark & \cmark \\
    Arbitrary behavior class & \xmark & \xmark & \xmark & \xmark & \cmark \\
    \bottomrule
  \end{tabularx}
  
  \label{tab:ml_transposed_comparison_transposed}
\end{table*}

\section{Related Work}
\label{sec:relatedwork}

To the best of our knowledge, our work is the first study of reward inference from general behavior. 
Here, we review prior work extending behavior imitation \citep{pomerleau1988alvinn} and reward inference \citep{ng2000algorithms} to use training data, account for various kinds of suboptimality, and learn from behavior classes other than demonstrations.







\textbf{Meta-IRL} Prior work has studied the use of multi-task demonstration datasets to improve the efficiency of IRL inference, under the names of multi-task IRL \citep{babes2011apprenticeship, dimitrakakis2012bayesian, choi2012nonparametric, gleave2018multitask}, meta-IRL \citep{xu2019learning, yu2019meta, ghasemipour2019smile, wang2021meta}, and lifelong IRL \citep{mendez2018lifelong}. Similarly to SRI, such works allow fast, data-efficient reward inference, but do not directly enable reward inference from suboptimal demonstrations.






\textbf{Behavior model misspecification} \citet{armstrong2018occam} showed that it is generally impossible to simultaneously infer a demonstrator's reward function and their behavior model; thus, reward inference methods must either assume the behavior model (as most methods do) or learn it from data (as \citet{shah2019feasibility} and SRI do). Later, \citet{Skalse2022MisspecificationII} showed theoretically that assuming behavior models is dangerous, as it almost always produces incorrect reward functions when the model is incorrect. For example, assuming Boltzmann rationality will only provide an asymptotically correct optimal policy set for ground-truth behavior models that take optimal actions most frequently, and no others.
Other works quantified the error induced by incorrect behavior models: \citet{shah2019feasibility} and \citet{chan2021human} showed that a variety of misspecifications can induce sometimes dramatically incorrect reward functions, while \citet{hong2023sensitivity} showed that in continuous-action MDPs, even arbitrarily small errors in the behavior model can result in almost arbitrarily large errors in the inferred reward parameters.



\textbf{Learning from suboptimal demonstrations} Reward inference algorithms have been developed to account for a wide variety of specific suboptimalities in human behavior, including hyperbolic discounting, myopia, false beliefs and bounded cognition \citep{evans2015learning, evans2016learning, zhi2020online}, autocorrelated action noise \citep{zheng2014robust}, mistaken transition models \citep{reddy2018where}, and risk-sensitive behavior \citep{singh2018risk, ratliff2020inverse}. Notably, humans have little trouble inferring and accounting for each other's suboptimality \citep{evans2016learning, zhi2020online}.

\citet{shiarlis2016inverse} studied the setting where demonstrations are arbitrarily suboptimal, but they are labeled as failures (to be avoided) or successes; similarly, \citet{brown2020safe} used preferences over suboptimal demonstrations. \citet{brown2020better} and \citet{chen2021learning} augment that paper by using noise injection to automatically rank synthetic demonstrations, learning a reward model that disprefers noisy trajectories.



\textbf{Learning from general behavior} \citet{hadfieldmenell2016cooperative} and \citet{malik2018efficient}
studied how to provide and learn from demonstrations which are selected according to their information content for the other agent, rather than how much reward they accumulate. \citet{shah2019feasibility} developed the research direction which is closest to SRI. They present two algorithms for human behavior models and rewards, both based on value iteration networks (VINs) \citep{tamar2016value}, and differentiable planners in general. While the second algorithm they present uses a heuristic to infer rewards from near-optimal agents, the first algorithm uses a similar setting to SRI: it trains a value iteration network on a dataset of demonstration policies and corresponding reward functions to predict a policy given a reward function. Then, at inference time, given a policy, the reward function is recovered through gradient descent. Unlike SRI, however, this algorithm expects the demonstrator's full policy at inference time, and has not been extended to continuous domains (which are beyond the default capabilities of VINs). 




\section{Background}

While SRI does not assume that human behavior is generated by any particular learning or control algorithm, we still formalize the notions of ``behavior" and ``goals" using notation from reinforcement learning (RL) \citep{SuttonBarto} and inverse reinforcement learning (IRL) \citep{ng2000algorithms}, which we review here.


Control problems studied in RL are formalized mathematically as a Markov decision process (MDP). An MDP $\mathcal{M} = (\mathcal{S}, \mathcal{A}, p, r, d_0, \gamma)$ consists of possibly infinite sets of states, $\mathcal{S}$, and actions, $\mathcal{A}$; a transition function $p: \mathcal{S}\times \mathcal{A} \times \mathcal{S}  \rightarrow [0, 1]$; a reward function $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$; an initial state distribution $d_0: \mathcal{S} \rightarrow [0, 1]$; and a discount factor $\gamma \in [0, 1]$.


A policy $\pi: \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$ is a function describing the agent's probability of selecting an action in any given state; let $\Pi$ be the set of all policies. Policies interact with an MDP to produce stochastic processes known as episodes: $(S_0, A_0, R_0, S_1, A_1, R_1, \dots)$ such that $S_0 \sim d_0$, $A_t \sim \pi(S_t, \cdot)$, $R_t = r(S_t, A_t)$, and $S_{t+1} \sim p(S_t, A_t, \cdot)$. 
MDPs can also be partially observable, meaning they also have a set of observations $\mathcal{O}$ and an emission function $\Omega: \mathcal{S} \times \mathcal{O} \rightarrow [0, 1]$. In this case, episodes include observations generated by the emission function, $O_i \sim \Omega(S_i, \cdot)$, and the policy $\pi: \mathcal{O} \times \mathcal{A} \rightarrow [0, 1]$ instead maps observations to actions: $A_i \sim \pi(O_i, \cdot)$.

In the notation used in this paper, we assume for simplicity that the reward function can be described as a function of state alone: $\left(R_t \perp\!\!\!\perp A_t | S_t\right)$. Thus, we will write $r(S_t)$ for brevity. Such state-based rewards are common in goal-based robotic manipulation tasks, for example. However, our methods apply equally well to the case where actions influence the reward.

The objective of an RL agent in an MDP is to accumulate as much reward as possible, subject to exponential time discounting. Formally, the discounted return starting at time $t$ is the sum of rewards at and after $t$, discounted exponentially by $\gamma$: $G_t = \sum_{i=0}^\infty \gamma^{i}R_{t+i}$. The expected discounted episodic return in an MDP is the expected value of $G_0$ for a given policy: $J(\pi)=\E[G_0 ; \pi]$, where semicolon $\pi$ indicates that $A_t \sim \pi(S_t, \cdot)$. RL in a given MDP is thus the optimization problem $\argmax_\pi J(\pi)$. Let $\pi^*$ be such an optimal policy, and let its expected return be $J^*$.


\textbf{Reward Inference.}
In reward inference problems such as SRI, IRL, or CIRL, the reward function is unknown to the agent, but typically train-time access to the underlying reward-free MDP, $\mathcal{M} \setminus \{r\}$, is still assumed.\footnote{In this paper, we use ``reward inference'' to denote reward inference from behavior, such as IRL.} In place of the reward, some number of trajectories in the environment are provided, consisting of sequences of either observations, states, or states and actions.\footnote{This setup describes a two-phase CIRL game; CIRL allows for multiple learning-deployment interactions between agent and demonstrator, but such an interactive problem setup is beyond the scope of this paper.}
In settings where the human behaves roughly optimally for the task they intend the imitator to complete, these trajectories are called \textit{demonstrations}, but for generality we call them ``behavior trajectories''. In this paper, we will assume the most difficult setting, where trajectories are sequences of observations: $\tau = (o_0, o_1, \dots, o_{L_B}) \in \mathcal{T}$, where $\mathcal{T} := (\mathcal{O})^{L_B}$.
Thus, to indicate a single task, the agent is provided with $\left\{\tau_n\right\}_{n=1}^N \in \mathcal{T}^N$.

The agent's goal is to use these trajectories to infer the reward function. The reward function itself is sometimes the final output, but our focus is on optimizing the inferred reward function and evaluating the resulting policy against the hidden ground truth reward function. 

\subsection{Learning Behavior Models from Known Rewards}
\label{sec:learningfromknownrewards}
Reward inference models traditionally infer a completely unknown reward function $r$ by assuming that the trajectories $\{\tau_n\}$ are generated according to a specific, known mapping $b: \mathcal{R} \rightarrow \Pi$, where $\mathcal{R}$ is the space of reward functions \citep{Ramachandran2007bayesian, Skalse2022MisspecificationII}. 
However, some recent work has partially inverted this setting, instead assuming that the \textit{behavior model} is partially or completely unknown, but can be inferred from samples of human behavior collected for partially or completely known \textit{reward functions}.
For example, \citet{reddy2018where}, \citet{enayati2018skill}, \citet{carrenomedrano2019incrementalestimationofusersexpertiselevel}, and \citet{ghosal2023effectmodelinghumanrationalitylevel} use demonstrations by humans in tasks with known rewards to infer parameters of their behavior models, such as their Boltzmann-rationality temperature parameter or their internal beliefs about the transition model, $p$; similarly, \citet{milliken2017modelinguserexpertiseforchoosinglevelsofsharedautonomy} estimate a human's expertise in a driving task using the knowledge that hitting obstacles is undesirable. Finally, in their first algorithm, \citet{shah2019feasibility} use a dataset of known reward functions and known policies to learn any behavior model that can be produced by a value iteration network in a tabular MDP.










\section{Supervised Reward Inference}
In our work, we study a simpler approach for performing reward inference using a dataset of human behavior for known rewards. Rather than training a parameterized behavior model from data, we simply train the inverse model to directly map trajectories to reward functions. 

This direct reward function inference approach, if it performed $N$-shot inference (i.e., used $N$ trajectories as input) with trajectories of length $L_B$, would produce a model from trajectories to reward functions \(f_\theta: \mathcal{T}^N \rightarrow \mathcal{R}\), 
allowing reward inference on a single state $s$ through
\(f_\theta(\{\tau_n\}_{n=1}^N)(s).\) 
Such a model would work well in those cases studied previously where the exact reward function is known \citep{shah2019feasibility}, or where a parameterized form $r_\psi$ of the reward function is known, in which case $f_\theta$ could use $\psi$ as its target. However, this is not a general solution, as true human reward functions in complex environments are unlikely to have known parameterizations (see Section \ref{sec:datasetconstruction}).

Instead, we teach $f_\theta$
to predict samples of the reward given a state as input: $r(s) \approx f_\theta(\{\tau_n\}_{n=1}^N, s)$. A further enhancement offers an immense efficiency gain: the behavior trajectories (and thus task) 
need not be reprocessed at every timestep, and can instead 
be preprocessed into a task encoding. We call the resulting task encoder $f_{\theta_f}$ (the \textcolor{ppt-blue}{blue path} in Figure \ref{fig:arch}), and the state-encoder and final reward model $g_{\theta_g}$ (the \textcolor{ppt-red}{red path} in Figure \ref{fig:arch}).\footnote{Note that this structure also allows us to train multi-task policies by conditioning on the task embedding output from $f_{\theta_f}$ \citep{yu2019meta}, which we explore in experiments with reach tasks.} This final structure allows us to formally define SRI.















\begin{definition}[Supervised Reward Inference]

Given: \textbf{a}) a random set of behavior trajectories $\{T_n\}_{n=1}^N \in \mathcal{T}^N$ and a random reward function $R$ jointly following a distribution $\mathcal{D}_T$; \textbf{b}) a random state $S$ following some distribution $\mathcal{D}_S$; \textbf{c})
some parameterized function families $f_{\theta_f}: \mathcal{T}^N \rightarrow \Psi$ and $g_{\theta_g}: \mathcal{S} \times \Psi \rightarrow \mathbb{R}$; and \textbf{d}) some regression loss function $\mathcal{L}: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$, \textit{supervised reward inference} (SRI) is the following minimization problem:
\begin{equation} 
    \argmin_{\theta_f, \theta_g}\mathbb{E}\bigg[\mathcal{L}\bigg (g_{\theta_g}\big(S, f_{\theta_f}(\{T_n\}_{n=1}^N) \big), R(S)\bigg) \bigg ]. \label{sri}
\end{equation}
\end{definition}

See Algorithm \ref{aLg:sri} for an example gradient-descent-based implementation. Note that the state samples that are labeled with rewards do \textbf{not} need to be taken from the behavior trajectories. Indeed, it is often best for states and behaviors to be separate: the state samples should be representative of the state space optimized during RL, but the behaviors need not be.

\subsection{Example Architecture}
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/SRI_Arch_Draft_3.pdf}
    \caption{Example SRI model architecture. Behavior trajectories are processed independently into trajectory representations by a sequence model such as a transformer, and then these representations are combined into an overall task representation $\psi$ by a set model such as a set transformer \citep{lee2019set} (\textcolor{ppt-blue}{blue path}). This computation is done only \textcolor{ppt-blue}{once per task}. Independently, the current state is processed into a representation $\phi$ by a standard multi-layer perceptron or convolutional neural network (\textcolor{ppt-red}{red path}). This process is done \textcolor{ppt-red}{once per timestep}, but is very fast: it consists of one forward pass through two small MLPs. Finally, the task and observation representations are combined by another multi-layer perceptron into a scalar reward.}
    \label{fig:arch}
\end{figure}
Figure \ref{fig:arch} demonstrates the abstract structure of the architecture that we used for SRI in our experiments; see Appendix \ref{app:arch} for exact details. All $g_{\theta_g}$ networks used (red path) were MLPs, while the trajectory encoder was a transformer \citep{vaswani2017attention}, and the task encoder was a set transformer \citep{lee2019set}.

\begin{algorithm}[t]
\caption{SRI Training with Gradient Descent}
\footnotesize
\label{aLg:sri}
\begin{algorithmic}[1] %
   \State {\bfseries Input:} Number of training tasks $K$, number of trajectories per task $N_T$, number of state-reward samples per task $N_s$, batch size $M$, number of inference trajectories per task $N_I$, learning rate $\alpha$, and training dataset
   \[\mathcal{D} = \left\{\left(\left\{\tau_{k, n}\right\}_{n=1}^{N_T}, \{(s_{k, n}, r_{k, n})\}_{n=1}^{N_s}\right)\right\}_{k=1}^K\] 
   \State Initialize $f_{\theta_f}: \mathcal{T}^{N_I} \rightarrow \mathbb{R}^d$
   \State Initialize $g_{\theta_g}: \mathcal{S} \times \mathbb{R}^d \rightarrow \mathbb{R}$
   \Repeat
      \State Sample batch of behavior and state-reward samples, 
      \[\left\{\left(\left\{\tau_{k, n}\right\}_{n=1}^{N_T}, \{(s_{k, n}, r_{k, n})\}_{n=1}^{N_s}\right)\right\}_{k=1}^M \sim \mathcal{D}\]
      \State Randomly sample $N_I$ trajectories from each set of $N_T$

      \State $(\psi_{k, n})_{k=1,n=1}^{M, N_s} = \left(f_{\theta_f}\left(\left\{\tau_{k, n}\right\}_{n=1}^{N_I}\right)\right)_{k=1}^{M}$
      \State $(\hat{r}_{k, n})_{k=1,n=1}^{M, N_s} = \left(g_{\theta_g}\left(s_{k, n}, \psi_{k, n}\right)\right)_{k=1, n=1}^{M, N_s}$
      
      \State $\mathcal{L} (\theta_f, \theta_g) \leftarrow \frac{1}{MN_s}\sum_{k=1}^M \sum_{n=1}^{N_s} \left(\hat{r}_{k, n} - r_{k, n}\right)^2$
      \State $\theta_f \leftarrow \theta_f - \alpha \nabla_{\theta_f} \mathcal{L} (\theta_f, \theta_g)$
      \State $\theta_g \leftarrow \theta_g - \alpha \nabla_{\theta_g} \mathcal{L} (\theta_f, \theta_g)$
   \Until{convergence}
   \State {\bfseries Output:} Learned demonstration encoder $f_{\theta_f}$ and reward model $g_{\theta_g}$
\end{algorithmic}
\end{algorithm}

\subsection{Dataset Construction}
\label{sec:datasetconstruction}
As discussed in Section \ref{sec:learningfromknownrewards}, prior work suggests one straightforward way of constructing a behavior-reward dataset for SRI: collect human behavior attempting to optimize known reward functions. Because the reward functions are known, states can be arbitrarily sampled (following whatever $\mathcal{D}_S$ is desired) and labeled computationally. Such an approach offers a limited number of behavior samples but an arbitrary number of state-reward pairs, which our experiments suggest is sufficient for strong performance (see Tables \ref{tab:reachdata} and \ref{tab:ppdata} in Section \ref{sec:experiments}). Our experiments follow this approach, using simulations of suboptimal human behavior.

However, assuming computational access to a ground-truth reward function is not reasonable for most complex environments and tasks, such as those where human goals are implicit rather than explicit or where tradeoffs must be made. In these cases, it may be more feasible to observe human behavior ``in the wild'' and use human labeling of goals, rewards or preferences, or other computational reward estimation methods, such as reward inference from their speech or facial expressions \citep{cui2020empathic}. In such a scenario, state-reward samples could be taken from the observed behavior itself, though care will have to be taken to ensure their representativeness of the RL state space.





\section{Bayes Optimality of SRI}
\label{sec:theory}



Are there any classes of behavior which are too suboptimal or arbitrary even for SRI? For which classes of reward inference problems will SRI produce an optimal reward model? In this section, we state the theoretical answer to these questions: as you give it more data, SRI approaches Bayes optimality for \textit{any} reward inference problem as long as the problem and SRI's model family satisfy certain compactness and `niceness' assumptions.
Concretely, in Appendix \ref{app:theory}, using the Bayesian inverse reinforcement learning framework \citep{Ramachandran2007bayesian}, we formally state and prove the following theorem.

\begin{namedthm*}{Main Theorem, Paraphrased}
    [Asymptotic Optimality of SRI Algorithms]
\label{thm:policy_convergence}
Consider an SRI problem in an MDP with jointly distributed reward $R$ and trajectory samples $\{T_n\}_{n=1}^N$. Suppose that the dataset, SRI algorithm, and MDP satisfy the following assumptions.

\textit{Assumptions:} \textbf{1)} The MDP has compact state and action spaces $\mathcal{S}$ and $\mathcal{A}$ and bounded returns; \textbf{2)} The MDP has a random (unknown) continuous reward function $R$;
\textbf{3)} SRI's model family is $\{f_\theta\}_{\theta \in \Theta}$ for compact $\Theta$;
\textbf{4)} $\{f_\theta\}$ is equicontinuous;
\textbf{5)} SRI minimizes mean-squared error, and $\{f_\theta\}$ contains the minimizer of mean-squared error, $\mathbb{E}[R | \{T_n\} = \{\tau_n\}]$;
\textbf{6)} We sample trajectories and rewards from the true distribution, and we sample states from a distribution with full support over the state space.


\textbf{Claim:} As the size of the dataset increases, any SRI algorithm in this setting is asymptotically Bayes-optimal in two senses: first, its inferred reward functions almost surely converge uniformly over the state space to the expectation of the posterior distribution of \(R\) given $\{T_n\}$; second, consequently, the returns of its optimal policies converge almost surely to the maximum expected return under this posterior distribution.

\end{namedthm*}

\begin{proof}[Proof sketch]

The proof follows three steps: \textbf{1)} show that SRI's inferred reward function almost surely converges uniformly over the state space to the expectation of the posterior reward; \textbf{2)} show that maximizing return under the expectation of the posterior reward is equivalent to maximizing expected return under the posterior; \textbf{3)} show that SRI's uniform convergence in reward accuracy causes its optimal policies to also converge to optimal performance. (Note that all convergence discussed here is almost sure.)

\textbf{1)} To show uniform convergence of SRI's inferred reward functions over the state space, we first show pointwise convergence. First, boundedness of the reward function and the function family lets us conclude that the loss function is Glivenko-Cantelli, and so converges uniformly over parameters. Thus, any convergent subsequence of our sequence of parameters converges to an optimal $\theta$, and so our sequence of parameters must also converge to an optimal $\theta$.

Next, to conclude uniform convergence of SRI's reward functions over the state space, we use the assumption of equicontinuity of $\{f_\theta\}$ to apply the Arzel\`a-Ascoli theorem, which establishes that any convergent sequence of parameters has a subsequence that is uniformly convergent over the state space. The desired result then follows by iterated application of Arzel\`a-Ascoli on any non-uniformly-convergent subsequence in our sequence of parameters. We therefore conclude that SRI's inferred reward functions converge uniformly over the state space to the expectation of the posterior reward function.

\textbf{2)} To show that SRI's limit 
is the correct maximization target for imitation policies, we generalize a tabular result by \citet{Ramachandran2007bayesian}. Concretely, we use Fubini's Theorem and the Bounded Convergence Theorem to prove that maximizing expected return under the reward posterior is equivalent to maximizing return under the expectation of the reward posterior.

\textbf{3)} Finally, because the MDP's returns are bounded, and because we can uniformly bound SRI's deviation from the expected reward posterior across all states, we can also bound the deviation of SRI's predicted \textit{returns} across all policies. As the quantity of data increases, this return deviation decreases. Therefore, SRI's optimal policies approach optimality with respect to the expectation of the reward posterior, and hence expected optimality with respect to the reward posterior.
    
\end{proof}
























    
    
    

    
    







\section{Experiments}
\label{sec:experiments}

In the previous section, we showed that, theoretically, an ideal SRI algorithm can ideally solve any reward inference task as well as it is possible to solve it. In this section, we demonstrate this ability in practice, using several concrete examples of tasks that are not possible to solve with previous methods: inferring a pick-place reward function from a gesture, inferring correct goals from demonstrations that systematically show the \textit{wrong} goal, and more.


However, in addition to providing evidence that SRI can infer reward functions from radically suboptimal demonstrations, as expected, we also designed experiments to explore the following concrete questions about SRI's behavior: \textbf{1)} Can SRI learn models that infer sharp complex, discontinuous reward functions precisely? \textbf{2)} Can SRI learn models that infer reward functions from behaviors like gestures that each indicate the goal only indirectly or partially? 
\textbf{3)} How does the performance of SRI models relative to classical imitation learning models change as the suboptimality of the demonstrations increases? \textbf{4)} How does the performance of SRI models vary with respect to amount of training data and number of test-time demonstrations? 

\subsection{Experiment Design}
\label{sec:tasks}

For additional experimental details, see Appendix \ref{app:experimentaldetails}.

\textbf{Tasks.} Ground-truth tasks are Meta-World pick-place and reach tasks \citep{yu2019metaworld} with randomly distributed goals.
Because we do not use the ground-truth reward function to evaluate policies,
we added an additional discontinuous reward of 5.0 upon success in order to test SRI's ability to infer sharp reward functions.

\textbf{Demonstrations.}
Experiments used five classes of behavior trajectories, all generated by oracular policies with reach goals. Gestures (\textbf{\textsc{gesture}}): robot hand starts at a random position above the table and reaches towards the goal for 50 timesteps. Used for pick-place tasks. Noisy actions (\textbf{\textsc{noisy}$_\varepsilon$}):the hand starts in the default location and reaches directly towards the goal for 150 timesteps, but with a probability $\varepsilon$ each timestep of instead reaching towards a random location. Noisy actions with random starts (\textbf{\textsc{noisy gesture}$_\varepsilon$}): uses random actions like \textsc{noisy}$_\varepsilon$, but otherwise identical to \textsc{gesture} (including a horizon of 50). Goal offset (\textbf{\textsc{psychic}$_\alpha$}): the hand reaches deterministically towards the \textit{wrong} position, offset towards the origin by some amount ($\alpha=1.0$ is no offset, while $-1.0$ is mirroring through the origin).
Finally, mirrored and circled (\textsc{\textbf{hard}}): the hand starts at the origin, then draws a circle around a deterministically incorrect position.


Except where otherwise specified, we provided 100 demonstrations per task for \textsc{gesture}, \textsc{noisy}, and \textsc{noisy gesture}, 1 demonstration for the deterministic \textsc{psychic}, and 10 demonstrations for \textsc{hard}.

\textbf{State-reward Sampling.}
For all tasks, SRI received a state-reward dataset (see Section \ref{sec:datasetconstruction})
with states produced by the robot hand attempting to reach to random locations on and around the table while randomly opening and closing its gripper. For pick-place tasks, SRI also received examples of states from an oracular pick-place agent bringing the object to random positions. Of course, SRI could not infer goals from such states, as they were randomly shuffled and uncorrelated with the goal of the task is was attempting to infer.

\textbf{Data quantity.} Except where otherwise noted, SRI received 1,280 tasks, each with 100 demonstrations and 10,000 state-reward pairs (note, however, that such a large amount of data was unnecessary; see Tables \ref{tab:reachdata} and \ref{tab:ppdata}). For pick-place tasks, 80\% of states were from random reaching, while 20\% were from random pick-placing.

\begin{table}[tp]
\caption{Performance of SRI and baselines on pick-place tasks given demonstrations from the \textsc{gesture} class (see Section \ref{sec:tasks}). Performance is measured by the object's average proximity to the goal under each method's learned policy, clipped per-trial to a minimum of 0 (see Section \ref{sec:metrics}), with standard error variance over 30 trials. 
    Results show that SRI can infer a complex reward function from completely non-optimal gestures as demonstrations.}
    \centering
    \begin{tabular}{lc}
        \toprule
        \textit{Method} & \textit{$\phantom{-}$Ave. Goal Proximity} \\
        \midrule
        SRI & 
        $\phantom{-}0.822 \pm 0.051$ \\
        GAIL & $-0.001 \pm 0.000$ \\
        AIRL & $-0.001 \pm 0.001$ \\
        BC & $-0.001 \pm 0.000$ \\
        GT RL & 
        $\phantom{-}
        0.903 \pm 0.011$ \\
        \bottomrule
    \end{tabular}
    
    \label{tab:pickplace}
\end{table}




\textbf{RL.}
For learning policies with SRI's learned reward functions, we used Truncated Quantile Critics \citep{kuznetsov2020controllingoverestimationbiaswithtruncatedmixtureofcontinuousdistributionalquantilecritics} for reach tasks and Proximal Policy Optimization \citep{schulman2017proximalpolicyoptimizationalgorithms} for pick-place tasks, each as implemented in Stable-Baselines3 \citep{stable-baselines3}. 


For reach tasks, which are easier, we trained multi-task policies by conditioning the policy on the task representation $\psi$; for pick-place tasks, we trained task-specific policies (though note that all tasks within each trial were inferred by a single SRI model).

\textbf{Metrics and statistics.}
\label{sec:metrics}
The evaluated metric is average normalized goal proximity: 1 minus the distance to the goal of the hand (for reach tasks) or object (for pick-place tasks), scaled such that the initial distance is 1, and averaged across all timesteps and trials. To avoid distraction by large negative proximity values in plots, we clip average proximity of each trial in plots to a minimum of 0 (but do not clip in tables).

All methods and all settings in all experiments were run for 30 trials, and statistical uncertainty was quantified using standard error without Bonferroni correction.


\textbf{Baselines.}
We used four baselines for comparison. First, three imitation learning baselines, for all of which we used the Imitation library in Python \citep{gleave2022imitation}: behavioral cloning (BC) \citep{pomerleau1988alvinn}, generative adversarial imitation learning (GAIL) \citep{ho2016generative} and adversarial inverse reinforcement learning (AIRL) \citep{fu2018learning}. Finally, we also used a ground-truth RL baseline (GT RL), consisting of policies learned through RL with the ground-truth reward.


\subsection{Results}
\label{sec:results}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/noise_plot_ci.pdf}
    \caption{Performance of SRI and baselines when given noisy demonstrations. Error bars indicate standard error over 30 trials. Tasks are Meta-World reach tasks with demonstrations from the \textsc{noisy gesture}$_\varepsilon$ class for various values of $\varepsilon$ (see Section \ref{sec:tasks}). Performance is measured by the robot hand's average proximity to the goal under each method's learned policy, clipped per-trial to a minimum of 0 (see Section \ref{sec:metrics}), with 30-trial standard error bars. Note that $\varepsilon=1.0$ is effectively impossible, as demonstrations are pure noise, and is only included for completeness. Results show that SRI approaches ground-truth RL performance in the presence of perfect demonstrations, and suffers less from noisily suboptimal demonstrations than other methods.}
    \label{fig:noisy_reach}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/adjustment_plot_ci.pdf}
    \caption{Performance of SRI and baselines when given demonstrations that deterministically reach to the wrong location. Tasks are Meta-World reach tasks with demonstrations from the \textsc{psychic}$_\alpha$ class (see Section \ref{sec:tasks}) for various values of $\alpha$. Performance is measured by the robot hand's average proximity to the goal under each method's learned policy, clipped per-trial to a minimum of 0 (see Section \ref{sec:metrics}), with 30-trial standard error bars. Results show that the performance of optimality-assuming algorithms decreases to zero with suboptimality of the demonstrations, while SRI's learned policies remain nearly optimal regardless of demonstration optimality.}
    \label{fig:lazy_reach}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/n_experiment_plot_ci.pdf}
    \caption{Performance of SRI and baselines when given varying numbers of noisy demonstrations. Tasks are Meta-World reach tasks, with demonstrations from the \textsc{noisy}$_{0.87}$ class (see Section \ref{sec:tasks}). Performance is measured by the robot hand's average proximity to the goal under each method's learned policy, clipped per-trial to a minimum of 0 (see Section \ref{sec:metrics}), with 30-trial standard error bars. Results show that SRI performs better than optimality-assuming methods regardless of demonstration quantity.}
    \label{fig:reach_dem_efficiency}
\end{figure}

\begin{table}[htp]
\caption{Performance of SRI trained on varying numbers of tasks and observations per task, with baseline results as follows: GAIL: $-1.808 \pm 0.368$; AIRL: $-1.582 \pm 0.302$; BC: $-0.930 \pm 0.083$; Ground Truth RL: $0.957 \pm 0.005$. Variance is measured with 30-trial standard error. Tasks are Meta-World reach tasks with demonstrations from the \textsc{hard} class (see Section \ref{sec:tasks}). Performance is measured by the robot hand's average proximity to the goal under each method's learned policy, clipped per-trial to a minimum of 0 (see Section \ref{sec:metrics}). Results show that despite the difficulty of reward inference from \textsc{hard} demonstrations, and despite SRI's complex deep architecture, SRI needs surprisingly little data to learn reasonable reward functions for this task, making do with as little as 8,000 labeled observations.
    }
  \centering
  \begin{tabular}{lccc}
    \toprule
    \multirow{3}{*}{\makecell[b]{\textit{Num.}\\  \textit{Tasks}}}  
      & \multicolumn{3}{c}{\textit{Number of Labeled Observations per Task}} \\ 
    \cmidrule(lr){2-4}
      & $100$ & $1000$ & $10000$ \\
    \midrule
      $1280$ & \(0.860 \pm 0.006\) & \(0.916 \pm 0.004\) & \(0.930 \pm 0.003\) \\
      $320$  & \(0.780 \pm 0.007\) & \(0.866 \pm 0.006\) & \(0.893 \pm 0.006\) \\
      $80$   & \(0.495 \pm 0.017\) & \(0.792 \pm 0.008\) & \(0.812 \pm 0.010\) \\
    \bottomrule
\end{tabular}
  
  \label{tab:reachdata}
\end{table}

\begin{table}[htp]
\caption{Performance of SRI trained on varying numbers of tasks and observations per task, with baseline results as follows: GAIL: $-0.004 \pm 0.002$; AIRL: $-0.001 \pm 0.000$; BC: $-0.002 \pm 0.001$; Ground Truth RL: $0.903 \pm 0.011$. Variance is measured with 30-trial standard error. Tasks are Meta-World reach tasks with demonstrations from the \textsc{hard} class (see Section \ref{sec:tasks}). Performance is measured by the robot hand's average proximity to the goal under each method's learned policy, clipped per-trial to a minimum of 0 (see Section \ref{sec:metrics}). Results show that pick-place is a far more difficult task than reach, unsurprisingly (see Table \ref{tab:reachdata}), but SRI can still learn well with 320,000 labeled observations. Note that the counterintuitively strong result of 320 tasks and 1000 observations is due to model capacity: with higher-capacity models, we find that SRI's performance in this experiment increases monotonically with data quantity.
    }
  \centering
  \begin{tabular}{lccc}
    \toprule
    \multirow{3}{*}{\makecell[b]{\textit{Num.}\\  \textit{Tasks}}}  
      & \multicolumn{3}{c}{\textit{Number of Labeled Observations per Task}} \\ 
    \cmidrule(lr){2-4}
      & $100$ & $1000$ & $10000$ \\
    \midrule
      $1280$ & \(0.614 \pm 0.076\) & \(0.715 \pm 0.069\) & \(0.784 \pm 0.052\) \\
      $320$  & \(0.221 \pm 0.067\) & \(0.768 \pm 0.052\) & \(0.708 \pm 0.059\) \\
      $80$   & \(0.018 \pm 0.021\) & \(0.269 \pm 0.065\) & \(0.502 \pm 0.085\) \\
    \bottomrule
\end{tabular}
  \label{tab:ppdata}
\end{table}

Throughout our experiments, we discovered that SRI was capable of learning 
reward functions with a high degree of accuracy (see Table \ref{tab:pickplace}), in a wide variety of situations where optimality-assuming methods completely fail (Figure \ref{fig:lazy_reach}; Tables \ref{tab:pickplace}, \ref{tab:reachdata}, \ref{tab:ppdata}). It performed well even when the behavior-reward mapping was profoundly noisy (Figure \ref{fig:noisy_reach}), and even when it only received a single noisy demonstration (Figure \ref{fig:reach_dem_efficiency}); unsurprisingly, though, it performed even better when the behavior-reward mapping was arbitrarily suboptimal but still invertible (Figure \ref{fig:lazy_reach}). Finally, while challenging reward functions still require a substantial number of labeled observations to be learned accurately (Table \ref{tab:ppdata}), potentially necessitating the use of self-supervised learning methods, simpler tasks may be solvable with quantities of data small enough to collect manually (Table \ref{tab:reachdata}).

Nevertheless, our results did not come easily: we also discovered that data quality is crucial for SRI's performance, and that failing to sample states from all regions that a policy might explore
can lead SRI to infer a hackable reward function with incorrect local optima. Pick-place tasks were particularly difficult, as the reward model needed to provide accurate pre-grasping shaping rewards in addition to a large, discontinuous success reward; however, limited model capacity often forced the model to reduce error in success prediction, to the detriment of the shaped rewards necessary to find and grasp the object.

In general, reward inference from arbitrary behavior is a fundamentally difficult problem, as the policy and reward function cannot be regularized to be similar to the demonstration. However, one could regularize the policy or reward function to prefer states that are similar to those seen in the state-reward dataset. Future work exploring this direction or iterative data labeling \citep{christiano2017deep} may further reduce the data requirements of SRI.










\section{Conclusion}
In this paper, we introduced supervised reward inference (SRI), which to our knowledge is the first algorithm to be able to infer accurate reward functions from arbitrary behavior. We showed that SRI is asymptotically Bayesian-optimal, as long as the model is strong enough: as the quantity of training data available to it approaches infinity, its learned policies approach the highest ground-truth return possible given limited behavior at inference time. Finally, we showed that SRI can infer difficult reward functions correctly from behavior with complex suboptimalities, and can infer simpler reward functions from suboptimal behavior with only a few thousand labeled states.

Several important questions remain to be answered before SRI can be easily applied to existing real-world problems. First, many real-world tasks will likely be visual, thus requiring an image encoder as part of the SRI model, a change which we believe will be straightforward but did not study here. Second, while SRI's data efficiency on reach tasks might allow manual dataset construction, real-world tasks are likely to be far more complex than even the simulated pick-place task studied here. In such cases, SRI's labeled data efficiency may be enhanced through self- and weakly-supervised learning methods. For example, DINOv2 \citep{oquab2024dino} or other visual foundation models could help it learn rewards more easily, while behavior foundation models \citep{pirotta2024fast} might help it more easily infer optimal policies.


We believe that SRI's compatibility with self-supervised foundation models is thus one advantage offering it great promise in the effort to align artificial agents.
Just as natural language processing tasks have been made easier through leveraging massive datasets \citep{brown2020language}, so too we hope that artificial agents can achieve humans' real-world goals more easily through the power of data.

\section{Acknowledgements}
We are grateful to Bruno Castro da Silva for his exceptionally generous help in pushing this project to completion. We are also grateful to Russell Coleman for his early work on this project.

This work has taken place in part in the Safe, Correct, and Aligned Learning and Robotics Lab (SCALAR) at The University of Massachusetts Amherst. SCALAR research is supported in part by the NSF (IIS-2323384), the Center for AI Safety (CAIS), and the Long-Term Future Fund. Separately, this work was also supported by the NSF under
Grant No. CCF-2018372. Any opinions, findings, and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily reflect the views of the National
Science Foundation.

\bibliography{mybib}
\bibliographystyle{unsrtnat}

\appendix
\section{Proof of the Bayes Optimality of SRI}
\label{app:theory}

In this section, we use the Bayesian inverse reinforcement learning framework \citep{Ramachandran2007bayesian} to show that any ``ideal'' SRI algorithm is asymptotically optimal for both reward inference and imitation learning (see Theorem \ref{thm:policy_convergence} for details). In particular, our analysis relies on only three main assumptions for the SRI algorithm and the problem: niceness and compactness of the function class and MDP, appropriate model capacity, and data coverage.

Our proof proceeds as follows: first, in Section \ref{sec:prelim}, we lay out our notation and assumptions; second, in Section \ref{sec:rbar-optim}, we derive the closed form of the Bayes-optimal reward function for imitation given limited behavior trajectories; third, in Section \ref{sec:sup-convergence}, we show that SRI converges uniformly in the limit of infinite data to this Bayes-optimal reward function; finally, in Section \ref{sec:policy-convergence}, we use these results to prove that the optimal policies for SRI also converge uniformly to Bayes optimality, i.e., they asymptotically provide the maximum possible expected return given irreducible uncertainty about the ground truth reward function.

\subsection{Preliminaries}
\label{sec:prelim}
\textbf{Notation:} Each of the sets we consider generally has at most one $\sigma$-algebra associated with it. Thus, as standard in probability theory, we will often use each set interchangeably with its measurable space.

Let $\mathcal{R}$ be a measurable space of reward functions in $\mathcal{M}$, equipped with any $\sigma$-algebra, and let $\mathcal{P}(\mathcal{R})$ be the set of all probability measures with respect to $\mathcal{R}$'s $\sigma$-algebra. (Define the operator $\mathcal{P}$ similarly for any measurable space.) Let $P \in \mathcal{P}(\mathcal{R})$ be the ground-truth marginal distribution of reward functions; in particular, these reward functions are potentially co-dependent with behavior trajectories of maximum length $L_B$. 
For any number of trajectories $N$ define the task-generating distribution 
\[\mathcal{D}_R \in \mathcal{P}\left(\mathcal{R} \times \mathcal{T}^N\right).\]

Rather than seeing the actual reward function, an SRI algorithm sees a set of $M$ state-reward pairs. In particular, let $P_S \in \mathcal{P}(\mathcal{S})$ be a distribution over states. The data-generating distribution is defined by the following process: first, for dataset size (number of tasks) $K$, take $K$ i.i.d. samples 
\[\left\{\left(R_k, \{\tau_{k, n}\}_{n=1}^N\right)\right\}_{k=1}^K\] 
from $\mathcal{D}_R$, then sample $M$ states from $P_S$ for each task: \[\left\{s_{k, m}\right\}_{k=1, m=1}^{K, M}.\] 
Finally, label each state $s_{k, m}$ with its reward for SRI to predict, $R_k(s_{k, m})$. The resulting dataset has $K$ tasks, and each task has $N$ trajectories and $M$ state-reward pairs: 
\[\left\{\left(\left\{\left(s_{k, m}, R_k(s_{k,m})\right)\right\}_{m=1}^M, \left\{\tau_{k,n}\right\}_{n=1}^N\right)\right\}_{k=1}^K.\] 

We aim to learn a parameterized reward function $R_\theta: \mathcal{S} \times \mathcal{T}^N \rightarrow \mathbb{R}$ on this dataset, for $\theta$ in some space $\Theta$ (see Assumption \ref{ass:well-behaved}).

\paragraph{Notation} For brevity, we henceforth omit index specifications when they are clear from context; e.g., $\{\tau_n\} \vcentcolon = \{\tau_n\}_{n=1}^N$, and similar for sequences. Furthermore, in conditional statements we omit the conditioned random variable when it is clear from context: $\mathbb{E}[Y | x] \vcentcolon = \mathbb{E}[Y | X=x]$.

We first lay out all assumptions necessary for our results.

\begin{ass}[Well-Behaved Spaces and Functions]
\label{ass:well-behaved}
\leavevmode
    \textbf{(a)} The state space \( \mathcal{S} \), observation space \( \mathcal{O}\), and action space \( \mathcal{A} \) are compact measurable metric spaces equipped with $\sigma$-algebras \( \mathcal{F}_\mathcal{S} \), \(\mathcal{F}_\mathcal{O}\) and \( \mathcal{F}_\mathcal{A} \), respectively, and all relevant probability measures (e.g., $\pi(s)$ for any $s \in \mathcal{S}$) are defined with respect to $\mathcal{F}_\mathcal{S}$, \(\mathcal{F}_\mathcal{O}\), and $\mathcal{F}_\mathcal{A}$. (It follows that $\mathcal{T}^N$ is also a compact metric space, using some reasonable product metric.)
    \textbf{(b)} The transition probability function $p: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{P}(\mathcal{A})$ is a Markov kernel (hence measurable), all policies $\pi: \mathcal{S} \rightarrow \mathcal{P}(\mathcal{A})$ is a Markov kernel, and the reward function $R: \mathcal{S} \rightarrow \mathbb{R}$ is measurable. 
    \textbf{(c)} The space $\Theta$ of reward model parameters is compact.
    \textbf{(d)} $R_\theta(s, \{\tau_n\})$ is measurable and continuous with respect to $\theta$, $s$, and $\{\tau_n\}$, and $\left\{R_\theta\right\}_{\theta \in \Theta}$ is equicontinuous in $s$ and $\{\tau_n\}$. 
    \textbf{(e)} All measure spaces are $\sigma$-finite.
\end{ass}

\begin{remark}
\label{rem:boundedtheta}
    Compactness of $\Theta$ and $\mathcal{S}$ and equicontinuity (Assumption 1(a, c, d)) imply that $\{R_\theta\}_{\theta \in \Theta}$ is uniformly bounded.
\end{remark}

\begin{ass}[Bounded Returns]
\label{ass:bounded}
    All reward functions $R \in \mathcal{R}$ are bounded, and either $\gamma < 1$ or $\exists L \in \mathbb{Z}_{\geq 0}$ such that for all $t > L$, $R_t = 0$. 
    Therefore, all returns are bounded, and $J_\theta$ is bounded for all $\theta$.
\end{ass}

\begin{ass}[MSE]
\label{ass:mse}
    In this section, SRI is defined using squared error over the dataset. In particular, we define the single-sample loss for a parameter \(\theta\) as
\[
\ell_\theta(R,\{\tau_n\},s) = (R_\theta(s,\{\tau_n\}) - R(s))^2.
\]
\end{ass}

\begin{remark}
\label{rem:nouniquetheta}
    We do not assume the uniqueness of minimizing parameters, but our definitions naturally imply unique minimizing functions.
\end{remark}

\begin{ass}[Model Capacity]
\label{ass:modelcapacity}
    The hypothesis class $\{R_\theta\}$ contains $\bar{R}_| \coloneqq \mathbb{E}\left[ R \mid \{\tau_n\} \right]$, which Lemma \ref{lem:linearity} shows to be the optimal reward function for maximizing expected imitation return.
\end{ass}

\begin{ass}[Data Coverage]
\label{ass:datacoverage}
    The data distribution over states has full support on $\mathcal{S}$: $\supp(P_S) = \mathcal{S}.$ (If the MDP contains unreachable states for any reason, the support need not include those states.)
\end{ass}

\subsection{Optimality of $\bar{R}$ as a reward function for imitation}
\label{sec:rbar-optim}

We first adapt a tabular result by \citet{Ramachandran2007bayesian} to show that, when optimizing expected return under an uncertain reward function, it is always optimal to optimize return under the expected value of that reward function.

\begin{lemma}
\label{lem:linearity}
 Let \( M_R = (\mathcal{S}, \mathcal{A}, p, R, d_0, \gamma) \) be an MDP with random reward function $R$ following any distribution \( P_R \in \mathcal{P}(\mathcal{R})\). Define the expected reward function \( \bar{R}: \mathcal{S} \rightarrow \mathbb{R} \) to be \( \bar{R}(s) = \mathbb{E}[R(s)] \). Then, the policy \( \pi^* \) that maximizes the expected cumulative reward \( \mathbb{E}_R[J_R(\pi)] \), where
\[
J_R(\pi) = \mathbb{E}\left[ \sum_{t=0}^\infty \gamma^t R(S_t) \,; \pi \right],
\]
is an optimal policy for the MDP \( M_{\bar{R}} = (\mathcal{S}, \mathcal{A}, p, \bar{R}, d_0, \gamma) \) with reward function \( \bar{R} \). Similarly, any optimal policy for $M_{\bar{R}}$ is also optimal under $J_R$.
\end{lemma}

\begin{proof}
We aim to show that
\begin{equation}\label{eq:main-equality}
\mathbb{E}\left[ J_R(\pi) \right] = \mathbb{E}\left[ \sum_{t=0}^\infty \gamma^t \bar{R}(S_t) \,; \pi \right],
\end{equation}
where the left expectation is taken over both the randomness in \( R \) and the stochastic transitions in the MDP under policy \( \pi \), while the right expectation is taken over the latter alone.

The proof establishes the following equalities:
\begin{align}
\mathbb{E}_R\left[ J_R(\pi) \right] &= \mathbb{E}_R \left[ \mathbb{E}\left[ \sum_{t=0}^\infty \gamma^t R(S_t) \,; \pi \right] \right] \label{eq1}  \\
&= \mathbb{E}\left[ \mathbb{E}_R\left[ \sum_{t=0}^\infty \gamma^t R(S_t) \right] \,; \pi \right] \label{eq2} \\
&= \mathbb{E}\left[ \sum_{t=0}^\infty \gamma^t \mathbb{E}_R\left[ R(S_t) \right] \,; \pi \right]. \label{eq3}
\end{align}
Step $\eqref{eq2} \rightarrow \eqref{eq3}$ follows immediately from Assumption \ref{ass:bounded} and the Bounded Convergence Theorem. Therefore, all that remains is to justify the application of Fubini's Theorem in step $\eqref{eq1} \rightarrow \eqref{eq2}$.

Let \( (\Omega, \mathcal{F}, \mu) \) be the product measure space of reward functions and trajectories, where \( \Omega = \mathcal{R} \times \mathcal{S}^\infty \), \( \mathcal{F} = \mathcal{R} \otimes \mathcal{S}^\infty \) (using the usual cylinder $\sigma$-algebra), and \( \mu = P(R) \times \mathbb{P}_\pi \). Here, \( \mathbb{P}_\pi \) is the probability measure over trajectories induced by \( \pi \) in this MDP.

Define the function \( f(R, \{ S_t \}) = \sum_{t=0}^\infty \gamma^t R(S_t) \).

To apply Fubini's Theorem, we need to verify that
    \( f \) is measurable with respect to \( \mathcal{F} \) 
    and that 
    \( \int_\Omega |f| \, d\mu < \infty \).

\textbf{Measurability:} First, note that the projection of $\Omega$ onto each individual $S_t$ is measurable by definition of the cylinder $\sigma$-algebra; thus, because $R$ is measurable by assumption, each summand of $f$ is individually measurable. The function \(f\) is therefore the limit of finite sums of measurable functions, and hence is measurable.

\textbf{Integrability:} For some non-negative $c \in \mathbb{R}$, whose exact value depends on $\gamma$ and $L$ (if applicable) we have
\begin{align*}
\int_\Omega |f| \, d\mu &= \int_{\mathcal{R}} \int_{S^\infty} \left| \sum_{t=0}^\infty \gamma^t R(S_t) \right| \, d\mathbb{P}_\pi(\{ S_t \}) \, dP(R) \\
&= \mathbb{E}_{R, S_t}\left[ \left| \sum_{t=0}^\infty \gamma^t R(S_t) \right| \right] \\
&\leq \mathbb{E}_{R, S_t}\left[ c \right],
\end{align*}

by Assumption \ref{ass:bounded}.

Thus, we have established \eqref{eq:main-equality}. To conclude, since the expected cumulative reward under the distribution \( P(R) \) equals the expected cumulative reward in the MDP with reward function \( \bar{R} \), maximizing \( \mathbb{E}[J_R(\pi)] \) over policies \( \pi \) is equivalent to maximizing \( J_{\bar{R}}(\pi) \) in the MDP \( M' = (S, A, p, \gamma, \bar{R}) \).

Therefore, the policy \( \pi^* \) that maximizes \( \mathbb{E}[J_R(\pi)] \) is the optimal policy for the MDP with reward function \( \bar{R} \).
\end{proof}



Equipped with the result of Lemma \ref{lem:linearity}, we now show that, under our assumptions, any SRI algorithm asymptotically produces optimal policies for $\bar{R}$, and thus optimal policies for the posterior distribution over rewards, $P_{R|\{\tau_n\}}$. 

\subsection{Convergence of $(R_{\theta_K})$}
\label{sec:sup-convergence}

The first step is to demonstrate the convergence of the learned SRI model in the limit of infinite data. First, we define our risk functions in a manner that allows us to invoke a Glivenko-Cantelli argument \citep{Sen2022gentle}. As discussed in the preliminaries, consider a single sample \((R,\{\tau_n\}, s)\) drawn according to \(\mathcal{D}_R \times P_S\), where \((R,\{\tau_n\}) \sim \mathcal{D}_R\) and \(s \sim P_S\). 
The population risk is then
\[
L(\theta) = \mathbb{E}_{(R,\{\tau_n\}) \sim \mathcal{D}_R, s \sim P_S}[\ell_\theta(R,\{\tau_n\},s)].
\]

\begin{remark}
\label{remark:leastsquaresmin}
    As always for least squares problems, for any \[\theta^* \in \argmin_{\theta \in \Theta} L(\theta),\] we know $R_{\theta^*} = \bar{R}_|$.
\end{remark}

Remark \ref{remark:leastsquaresmin} and Lemma \ref{lem:linearity} show that the optimal reward function for MSE SRI indeed produces Bayesian-optimal policies. We must now show that this happens in the limit of infinite data, as well. Specifically, we show: 1) that SRI algorithms indeed converge to some optimal $\theta^*$; 2) that this convergence is uniform in the state space; 3) that uniform convergence implies convergence of the optimal policies. Steps 1 and 2 follow from standard learning theory patterns in Lemmas \ref{lem:pointwise} and \ref{lem:uniform}. Step 3 is completed in Theorem \ref{thm:policy_convergence}.

Given a dataset of \(K\) tasks and \(M\) state samples per task,
\[
\{(R_k,\{\tau_{n,k}\})\}_{k=1}^K \sim \mathcal{D}_R^K, \quad \{s_{m,k}\}_{m=1}^M \sim P_S^M,
\]
we form the empirical risk:
\[
\hat{L}_K(\theta) = \frac{1}{K M} \sum_{k=1}^K \sum_{m=1}^M \ell_\theta(R_k,\{\tau_{n,k}\},s_{m,k}).
\]

\begin{remark}
\label{rem:boundedrisk}
    Under Assumptions \ref{ass:well-behaved}, \ref{ass:bounded}, and \ref{ass:mse}, $l_\theta$, $L(\theta)$ and $\hat{L}_K(\theta)$ are all bounded, continuous and measurable. (See Remark \ref{rem:boundedtheta}.)
\end{remark}

\begin{remark}
\label{rem:onlyk}
    Because increasing $K$ provides additional data of all types, we can ignore $M$ in the empirical risk ($M$ need not approach $\infty$). In particular, note that $L(\theta)$ remains the same even if $l_\theta$ is a sample risk over states $s_m$ (i.e., a loss for all state-reward samples in a single task) instead of a loss for one individual sample.
\end{remark}

We can now show both pointwise and uniform convergence of $R_{\theta_K}$.

\begin{lemma}[Pointwise Convergence of $R_{\theta_K}$]
\label{lem:pointwise}
Suppose \(\theta_K \in \arg\min_{\theta \in \Theta}\hat{L}_K(\theta)\) for each \(K\). Under Assumptions \ref{ass:well-behaved}$-$\ref{ass:datacoverage}, even without a unique minimizer \(\theta^*\), we have for all \((s,\{\tau_n\})\):
\[
R_{\theta_K}(s, \{\tau_n\}) \xrightarrow{\text{a.s.}} \bar{R}_|(s, \{\tau_n\}).
\]
\end{lemma}

\begin{proof}
As usual for this type of result, our proof follows three steps: \textbf{1)} we show that $l_\theta$ is Glivenko-Cantelli; \textbf{2)} we conclude that $\hat{L}_K$ almost surely converges uniformly to $L$; \textbf{3)} we use the existence of a convergent subsequence $\theta_{K_j}$ to conclude the desired result.

\paragraph{Steps 1 and 2} By Remark \ref{rem:boundedrisk}, we can directly conclude that $l_\theta$ is Glivenko-Cantelli \citep[see, for example, Remark 3.1 and Theorem 3.2,][]{Sen2022gentle}. Thus, by definition, $\hat{L}_K$ almost surely uniformly converges to $L$:
\[
\sup_{\theta \in \Theta}|\hat{L}_K(\theta) - L(\theta)| \xrightarrow{\text{a.s.}} 0.
\]

\paragraph{Step 3} Because $\Theta$ is compact, and because all minimizers of $L(\theta)$ produce the same function (see Remark \ref{remark:leastsquaresmin}), it is nearly sufficient to show that any convergent subsequence of $\theta_K$ converges to a minimizer of $L(\theta)$.

Since $\Theta$ is compact, let $(\theta_{K_j})$ be a convergent subsequence of $\theta_{K}$, and let $\theta^*$ be its limit. Fix $\varepsilon > 0$, and set $K_0$ such that for all $K' \geq K_0$, $\sup_{\theta \in \Theta}|\hat{L}_K(\theta) - L(\theta)| < \frac{\varepsilon}{2}$. Set $j'$ such that 1) $K_{j'} \geq K_0$, and 2) $|L(\theta_{K_{j'}}) - L(\theta^*)| < \frac{\varepsilon}{2}$ (recall that $L$ is continuous). Then
\begin{align*}
    |L_{K_{j'}}(\theta_{K_{j'}}) - L(\theta^*)| \leq& |L_{K_{j'}}(\theta_{K_{j'}}) - L(\theta_{K_{j'}})| \\ &+ |L(\theta_{K_{j'}}) - L(\theta^*)| \\
    <& \frac{\varepsilon}{2} + \frac{\varepsilon}{2} \\
    =& \varepsilon,
\end{align*}
so $L_{K_{j'}}(\theta_{K_{j'}}) \xrightarrow{\text{a.s.}} L(\theta^*)$. But by definition, for any $\theta \in \Theta$,
\begin{align*}
    & L_{K_{j'}}(\theta_{K_{j'}}) \leq L_{K_{j'}}(\theta)\\
    &\Rightarrow \lim_{j\rightarrow \infty} L_{K_{j'}}(\theta_{K_{j'}}) \leq \lim_{j\rightarrow \infty} L_{K_{j'}}(\theta) \\
    &\Rightarrow L(\theta^*) \leq L(\theta).
\end{align*}
Hence, $\theta^* \in \argmin_{\theta' \in \Theta} L(\theta')$. Letting $\Theta \supseteq \Theta^* = \argmin_{\theta \in \Theta} L(\theta)$, we therefore conclude by compactness of $\Theta$ that $d(\theta_K, \Theta^*)\footnote{Standard point-set distance: $d(x, A) = \inf_{a \in A} |x - a|$.} \xrightarrow{\text{a.s.}} 0$. The desired result follows by continuity of $R_\theta$.
\end{proof}

\begin{lemma}[Uniform Convergence of $(R_{\theta_K})$]
\label{lem:uniform}

Define $\theta_K$ as in Lemma \ref{lem:pointwise}. Given the equicontinuity of $\{R_\theta\}$, we also have almost sure uniform convergence of $(R_{\theta_K})$ across $(s, \{\tau_n\})$:
\[
\norm{R_{\theta_K} - \bar{R}_|}_\infty \xrightarrow{\text{a.s.}} 0.
\]

\end{lemma}


\begin{proof}


The desired result follows smoothly from pointwise convergence and the Arzel\`a-Ascoli Theorem.

If $(R_{\theta_K})$ did not converge uniformly to $\bar{R}(s) = \mathbb{E}[R(s) | \{\tau_n\}]$, then there would exist an $\varepsilon > 0$, a subsequence $(R_{\theta_{K_i}})$, and a sequence $\big((s_i,\{\tau_n\}_i)\big)$ such that
\[
|R_{\theta_{K_i}}(s_i, \{\tau_n\}_i) - \bar{R}_|(s_i, \{\tau_n\}_i)| \geq \varepsilon \text{ for all } i.
\]

Of course, this subsequence also satisfies the conditions of Arzel\`a-Ascoli -- most notably, equicontinuity -- so we can extract a further uniformly convergent subsequence $(R_{\theta_{K_{i_j}}})$. By Lemma \ref{lem:pointwise}, we know that pointwise, $(R_{\theta_{K_{i_j}}})$ almost surely converges to $\bar{R}_|$ (using the same event for almost sure convergence as all other almost sure convergences here).

But this contradicts the assumption that $|R_{\theta_{K_i}}(s_i, \{\tau_n\}_i) - \bar{R}_|(s_i, \{\tau_n\}_i)| \geq \varepsilon$. Thus, $(R_{\theta_K})$ must converge uniformly.

\end{proof}

\subsection{Convergence of SRI's optimal policies to Bayesian optimality}
\label{sec:policy-convergence}

Finally, we conclude that any SRI algorithm satisfying our assumptions produces asymptotically Bayes-optimal policies.

\begin{namedthm*}{Main Theorem}
[Asymptotic Optimality of SRI Algorithms]
\label{thm:policy_convergence}
Let $\mathcal{M}_R = (\mathcal{S}, \mathcal{A}, p, R, d_0, \gamma)$ be an MDP with random reward \(R\), which is drawn together with $\{\tau_n\}$ from a distribution $\mathcal{D_\mathcal{R}} \in \mathcal{P}(\mathcal{R} \times \mathcal{T}^N)$.

\textbf{Claim:} Any SRI algorithm satisfying Assumptions \ref{ass:well-behaved}$-$\ref{ass:datacoverage} is asymptotically optimal in the sense that the policies it produces approach those that maximize the expected return under the posterior distribution of \(R\) given $\{\tau_n\}$.

\end{namedthm*}

\paragraph{Uniform Convergence of Inferred Rewards}
   By Lemma \ref{lem:uniform}, we have that as \(K \to \infty\),
   \[
   \|R_{\theta_K} - \bar{R}_|\|_\infty \xrightarrow{\text{a.s.}} 0.
   \]
   Here \(\bar{R}_|\) is the Bayesian-optimal reward function, as shown in Lemma \ref{lem:linearity}. In particular, \(\bar{R}_|\) is the reward function that, if known, would yield the policy maximizing the expected return given \(\{\tau_n\}\).

   Thus, for any \(\delta > 0\), there exists \(K_\delta\) such that for all \(K > K_\delta\),
   \[
   \|R_{\theta_K} - \bar{R}_|\|_\infty < \delta.
   \]

\paragraph{Policy Convergence} 
   Let \(\pi_{\theta_K}^*\) be any optimal policy for the MDP \((\mathcal{S}, \mathcal{A}, p, R_{\theta_K}, d_0, \gamma)\). We wish to show that \(\{\pi_{\theta_K}^*\}\) approaches optimality for \(\bar{R}_|\) as \(K \to \infty\).

   Define the value functions under a reward function \(R'\) and policy \(\pi\) as:
   \[
   V_{R'}^\pi(s) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R'(S_t) \mid S_0=s,\pi\right].
   \]
   Let \(V_{R_{\theta_K}}^*(s) = V_{R_{\theta_K}}^{\pi_{\theta_K}^*}(s)\) be the optimal value under \(R_{\theta_K}\), and let \(V_{\bar{R}_|}^*(s)\) be the optimal value under \(\bar{R}_|\).

   We know:
   \[
   V_{R_{\theta_K}}^{\pi_{\theta_K}^*}(s) \geq V_{R_{\theta_K}}^\pi(s), \quad \forall \pi.
   \]

   We want to relate \(V_{\bar{R}_|}^{\pi_{\theta_K}^*}\) to \(V_{\bar{R}_|}^*\). Consider the difference:
   \[
   V_{\bar{R}_|}^*(s) - V_{\bar{R}_|}^{\pi_{\theta_K}^*}(s).
   \]
   Introduce the intermediate value functions under \(R_{\theta_K}\):
   \begin{align*}
   V_{\bar{R}_|}^*(s) - V_{\bar{R}_|}^{\pi_{\theta_K}^*}(s) &= [V_{\bar{R}_|}^*(s) - V_{R_{\theta_K}}^*(s)] \\ &+ [V_{R_{\theta_K}}^*(s) - V_{\bar{R}_|}^{\pi_{\theta_K}^*}(s)].
   \end{align*}

   We will bound each piece.

   Bounding \(|V_{\bar{R}_|}^*(s) - V_{R_{\theta_K}}^*(s)|\):     
     Because \(\|R_{\theta_K} - \bar{R}_|\|_\infty < \delta\), we have for any policy \(\pi\):
     \begin{align*}
         |V_{R_{\theta_K}}^\pi(s) - V_{\bar{R}_|}^\pi(s)| &\leq \mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^t |R_{\theta_K}(S_t)-\bar{R}_|(S_t)|\right] \\ &\leq \frac{\delta}{1-\gamma}.
     \end{align*}
     
     Thus:
     \[
     \|V_{R_{\theta_K}}^\pi - V_{\bar{R}_|}^\pi\|_\infty \leq \frac{\delta}{1-\gamma}, \quad \text{for any } \pi.
     \]
     In particular, this applies to the optimal policies under either reward:
     \[
     |V_{\bar{R}_|}^*(s) - V_{R_{\theta_K}}^*(s)| \leq \frac{\delta}{1-\gamma}.
     \]

   Bounding \(|V_{R_{\theta_K}}^*(s) - V_{\bar{R}_|}^{\pi_{\theta_K}^*}(s)|\):    
     By optimality of \(\pi_{\theta_K}^*\) under \(R_{\theta_K}\),
     \[
     V_{R_{\theta_K}}^*(s) = V_{R_{\theta_K}}^{\pi_{\theta_K}^*}(s).
     \]
     Thus:
     \[
     V_{R_{\theta_K}}^*(s) - V_{\bar{R}_|}^{\pi_{\theta_K}^*}(s) = [V_{R_{\theta_K}}^{\pi_{\theta_K}^*}(s) - V_{\bar{R}_|}^{\pi_{\theta_K}^*}(s)].
     \]
     This difference is bounded by the same \(\frac{\delta}{1-\gamma}\) argument as above:
     \[
     |V_{R_{\theta_K}}^{\pi_{\theta_K}^*}(s) - V_{\bar{R}_|}^{\pi_{\theta_K}^*}(s)| \leq \frac{\delta}{1-\gamma}.
     \]

   Combining these results, we have:
   \[
   V_{\bar{R}_|}^*(s) - V_{\bar{R}_|}^{\pi_{\theta_K}^*}(s) \leq \frac{\delta}{1-\gamma} + \frac{\delta}{1-\gamma} = \frac{2\delta}{1-\gamma}.
   \]

   Since \(\delta>0\) was arbitrary and can be made as small as desired by taking \(K\) sufficiently large, for any \(\varepsilon > 0\), choose \(\delta = \frac{\varepsilon(1-\gamma)}{2}\). Then for all \(K > K_\delta\),
   \[
   V_{\bar{R}_|}^*(s) - V_{\bar{R}_|}^{\pi_{\theta_K}^*}(s) \leq \varepsilon.
   \]
   Therefore:
   \[
   V_{\bar{R}_|}^{\pi_{\theta_K}^*}(s) \geq V_{\bar{R}_|}^*(s) - \varepsilon, \quad \forall s \in \mathcal{S},
   \] and in particular
   \[
   J_{\bar{R}_|}(\pi^*_{\theta_K}) \geq J^*_{\bar{R}_|} - \varepsilon.
   \]

\paragraph{Conclusion}
   We have shown that as \(K \to \infty\), the SRI algorithm's inferred reward functions \(R_{\theta_K}\) converge uniformly to \(\bar{R}_|\), and that the corresponding optimal policies \(\pi_{\theta_K}^*\) approach optimality under \(\bar{R}_|\). Since \(\bar{R}_|\) the Bayesian-optimal reward given \(\{\tau_n\}\) (Lemma \ref{lem:linearity}), the policies derived from the SRI algorithm asymptotically maximize the expected return under the posterior over \(R\).

\section{Architecture Details}
\label{app:arch}

We encoded trajectories using a 3-head transformer with two 256-dimensional transformer layers and a final 2-layer MLP with hidden dimension 50, outputting a trajectory representation of length 100; processed trajectory encodings into a task encoding with a default set transformer \citep{lee2019set} with hidden dimension 128, outputting a task representation of dimension 256; encoded states using a 2-layer MLPs with hidden dimension 256 and encoding dimension 100; and processed task and state representations into a final reward with a 2-layer MLP of hidden dimension 256. All activations were leaky ReLU.



\section{Experimental Details}
\label{app:experimentaldetails}

\subsection{Tasks}

Goals were selected randomly with $x \in [-0.3, 0.3]$, $y \in [0.4, 0.7]$, and $z \in [0.05, 0.3]$. For pick-place tasks, the object is randomly initialized per-episode for $x \in [-0.1, 0.1]$, $y \in [0.6, 0.7]$, and goals are chosen sufficiently far away to avoid initial success due to random object placement. All tasks used a horizon of 500. The default shaped reward functions were used both for SRI reward samples and ground-truth RL, except for the extra 5.0 success reward.

Note that the Meta-World reward is not computable from the default state representation; therefore, we provided SRI's state encoder with augmented states during both training and RL. Specifically, we provided SRI with: the location and starting location of the left and right pads of the hand; the location of the ``TCP (Tool Center Point) Center''; and the initial positions of the hand and the object. All of these values are computable from the history, but our goal in these experiments was not to test SRI's performance in partially observable environments. Note that the learned policy did not see these augmented state dimensions. 

Unfortunately, we could not augment states for the baseline algorithms to include initial position information, as doing so provided the adversarial discriminators with a ``hack'' to distinguish demonstrations from generated behavior, but we provided them with the TCP center and pad locations.

To simplify both supervised learning and reinforcement learning, we shifted and scaled all rewards to have the range $[-3, 3]$ (after adding the extra success reward of 5.0).

\subsection{Demonstration Details}

\textsc{Psychic}$_\alpha$ details: Given goal position $g$ and x-y origin $o_{x,y}$ = (0, 0.55), the hand reaches deterministically from its starting position towards target $t_{x, y} = o_{x, y} + \alpha(g_{x,y} - o_{x, y}).$

\textsc{Hard} details: the hand starts in a random position, then over 250 timesteps draws a 0.1-radius circle around the location of the goal mirrored through the x-y-z origin $o$ = (0, 0.55, 0.175). Note that the hand cannot always reach the far point of this circle, so the mirrored goal cannot be determined through averaging alone.

For \textsc{gesture} tasks, the hand starts in a random position satisfying $x \in [-0.4, 0.4], y\in[0.4,0.8],z\in[0.1,0.4]$.

\subsection{SRI Training Details}
SRI was trained for 2,000 epochs on all tasks with a batch size of 16 and learning rate of 0.0003, using the Adam optimizer \citep{kingma2015adam}.

As mentioned in Section \ref{sec:results}, we found hacking of SRI's rewards to be a particular problem in pick-place tasks, where a lack of model capacity forced SRI to trade off modeling the success reward on one hand and the grasping and pre-grasping shaped rewards on the other. SRI generally chose to optimize the success reward, as doing so was optimal for minimizing MSE, but this caused pick-place policies trained with the resulting reward to often fail to grasp the object at all. In order to encourage SRI to focus on modeling the non-success rewards, we linearly increased the proportion of pick-place state samples in the state-reward dataset from 5\% to 20\% over the course of training the reward model. However, the total proportion of reach and pick-place state samples per task was still 0.8 and 0.2, respectively; when necessary, we therefore sampled reach states with replacement to achieve the required total number of states per task.

\subsection{Reinforcement Learning}
The reason we used PPO for pick-place tasks and TQC for reach tasks is empirical: we found that PPO learned slowly in reach tasks and that TQC often failed to learn in pick-place tasks. All RL and baseline policies were trained for 5,000,000 environment interactions for reach tasks and 10,000,000 for pick-place tasks.

All networks of all RL policies were two-layer, hidden dimension 512 MLPs. For baselines, the actor had this structure, but the critic had the same structure as SRI's state encoder: a two-layer MLP with hidden dimension 256.

All algorithms, RL and baseline, learned for 10,000,000 total environment interactions for pick-place tasks and 5,000,000 interactions for reach tasks. Baseline hyperparameters were set to equal RL hyperparameters where appropriate (e.g., number of interactions and network size), and otherwise were taken from their respective papers as much as possible. Behavioral cloning was trained for 50 epochs.

RL algorithms used batch sizes of 128, learning rates of 0.0001, and $\gamma=0.9$. TQC used two critics, a rollout buffer size of 10,000,000, one update per transition, and a soft update coefficient of 0.005; PPO collected 2,048$*$16 transitions between updates and trained for 10 epochs per transition batch.

Note that the strangely poor behavior of the baselines in some conditions, such as AIRL in single-demonstration tasks and GAIL in noisy tasks, may in part be due to Meta-World tasks violating their assumptions. For example, GAIL assumes infinite-horizon tasks \citep{ho2016generative}. Nevertheless, as dominant imitation learning algorithms, we still believe them to be important baselines for comparison.
















\end{document}
