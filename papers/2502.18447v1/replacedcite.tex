\section{Related Work}
\label{sec:relatedwork}

To the best of our knowledge, our work is the first study of reward inference from general behavior. 
Here, we review prior work extending behavior imitation ____ and reward inference ____ to use training data, account for various kinds of suboptimality, and learn from behavior classes other than demonstrations.







\textbf{Meta-IRL} Prior work has studied the use of multi-task demonstration datasets to improve the efficiency of IRL inference, under the names of multi-task IRL ____, meta-IRL ____, and lifelong IRL ____. Similarly to SRI, such works allow fast, data-efficient reward inference, but do not directly enable reward inference from suboptimal demonstrations.






\textbf{Behavior model misspecification} ____ showed that it is generally impossible to simultaneously infer a demonstrator's reward function and their behavior model; thus, reward inference methods must either assume the behavior model (as most methods do) or learn it from data (as ____ and SRI do). Later, ____ showed theoretically that assuming behavior models is dangerous, as it almost always produces incorrect reward functions when the model is incorrect. For example, assuming Boltzmann rationality will only provide an asymptotically correct optimal policy set for ground-truth behavior models that take optimal actions most frequently, and no others.
Other works quantified the error induced by incorrect behavior models: ____ and ____ showed that a variety of misspecifications can induce sometimes dramatically incorrect reward functions, while ____ showed that in continuous-action MDPs, even arbitrarily small errors in the behavior model can result in almost arbitrarily large errors in the inferred reward parameters.



\textbf{Learning from suboptimal demonstrations} Reward inference algorithms have been developed to account for a wide variety of specific suboptimalities in human behavior, including hyperbolic discounting, myopia, false beliefs and bounded cognition ____, autocorrelated action noise ____, mistaken transition models ____, and risk-sensitive behavior ____. Notably, humans have little trouble inferring and accounting for each other's suboptimality ____.

____ studied the setting where demonstrations are arbitrarily suboptimal, but they are labeled as failures (to be avoided) or successes; similarly, ____ used preferences over suboptimal demonstrations. ____ and ____ augment that paper by using noise injection to automatically rank synthetic demonstrations, learning a reward model that disprefers noisy trajectories.



\textbf{Learning from general behavior} ____ and ____
studied how to provide and learn from demonstrations which are selected according to their information content for the other agent, rather than how much reward they accumulate. ____ developed the research direction which is closest to SRI. They present two algorithms for human behavior models and rewards, both based on value iteration networks (VINs) ____, and differentiable planners in general. While the second algorithm they present uses a heuristic to infer rewards from near-optimal agents, the first algorithm uses a similar setting to SRI: it trains a value iteration network on a dataset of demonstration policies and corresponding reward functions to predict a policy given a reward function. Then, at inference time, given a policy, the reward function is recovered through gradient descent. Unlike SRI, however, this algorithm expects the demonstrator's full policy at inference time, and has not been extended to continuous domains (which are beyond the default capabilities of VINs).