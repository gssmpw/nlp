\section{Related Work}
\label{sec: related work}
Autoregressive decoding generates tokens sequentially, leading to slow and costly decoding. 
To accelerate this process, draft-verification approaches ____ have gained popularity recently as they enhance speed without compromising performance, which fall into generation-based and retrieval-based categories based on their draft generation techniques (more information in Appendix \ref{appendix: related work}).

\noindent\textbf{Generation-based approaches.}
Draft tokens can be generated either by a smaller model or by the target model itself. 
Speculative decoding ____ employs a smaller model for drafting and uses the target LLM for efficient parallel verification. Ouroboros ____ generates draft phrases to enhance parallelism and extend drafts.
Alternatively, the target LLM itself can be utilized to efficiently draft ____, which reduces system complexity and selection difficulties.
Medusa ____ introduces multiple heads to predict multiple draft tokens in parallel.
Self-speculative decoding ____ employs the target model with selectively certain intermediate layers skipped as the draft model.


\noindent\textbf{Retrieval-based approaches.}
The retrieval-based draft generation approach replaces the model generation with a search in a retrieval datastore to obtain candidate sequences. These approaches avoid extra training and can reduce computational overhead.
LLMA ____ is an inference-with-reference decoding mechanism by exploiting the overlap between the output and the reference of an LLM.
REST ____ replaces the parametric draft model with a non-parametric retrieval datastore.