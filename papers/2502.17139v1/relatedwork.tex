\section{Related Work}
\label{sec: related work}
Autoregressive decoding generates tokens sequentially, leading to slow and costly decoding. 
To accelerate this process, draft-verification approaches \cite{chen2023accelerating,miao2024specinfer,he2024rest} have gained popularity recently as they enhance speed without compromising performance, which fall into generation-based and retrieval-based categories based on their draft generation techniques (more information in Appendix \ref{appendix: related work}).

\noindent\textbf{Generation-based approaches.}
Draft tokens can be generated either by a smaller model or by the target model itself. 
Speculative decoding \cite{chen2023accelerating, leviathan2023fast} employs a smaller model for drafting and uses the target LLM for efficient parallel verification. Ouroboros \cite{zhao2024ouroboros} generates draft phrases to enhance parallelism and extend drafts.
Alternatively, the target LLM itself can be utilized to efficiently draft \cite{stern2018blockwisedecoding, li2024eagle, fu2024lookahead}, which reduces system complexity and selection difficulties.
Medusa \cite{cai2024medusa} introduces multiple heads to predict multiple draft tokens in parallel.
Self-speculative decoding \cite{zhang2024draft} employs the target model with selectively certain intermediate layers skipped as the draft model.


\noindent\textbf{Retrieval-based approaches.}
The retrieval-based draft generation approach replaces the model generation with a search in a retrieval datastore to obtain candidate sequences. These approaches avoid extra training and can reduce computational overhead.
LLMA \cite{yang2023llma} is an inference-with-reference decoding mechanism by exploiting the overlap between the output and the reference of an LLM.
REST \cite{he2024rest} replaces the parametric draft model with a non-parametric retrieval datastore.