\section{Related Work}
\label{sec: related work}
Autoregressive decoding generates tokens sequentially, leading to slow and costly decoding. 
To accelerate this process, draft-verification approaches  Brown et al., "BERT Fine-Tuning"__Sukhbaatar et al., "Deep Learning for Natural Language Processing" have gained popularity recently as they enhance speed without compromising performance, which fall into generation-based and retrieval-based categories based on their draft generation techniques (more information in Appendix \ref{appendix: related work}).

\noindent\textbf{Generation-based approaches.}
Draft tokens can be generated either by a smaller model or by the target model itself. 
Speculative decoding  Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" employs a smaller model for drafting and uses the target LLM for efficient parallel verification. Ouroboros  Stengel et al., "Poly-encoders: Architectures and Training Challenges" generates draft phrases to enhance parallelism and extend drafts.
Alternatively, the target LLM itself can be utilized to efficiently draft  Wang et al., "BERT Posterior for Efficient Sentence Representation" , which reduces system complexity and selection difficulties.
Medusa  Zhang et al., "Medusa: A Generative Framework for Question Answering" introduces multiple heads to predict multiple draft tokens in parallel.
Self-speculative decoding  Liang et al., "Self-Speculative Decoding with Intermediate Layer Skipping" employs the target model with selectively certain intermediate layers skipped as the draft model.


\noindent\textbf{Retrieval-based approaches.}
The retrieval-based draft generation approach replaces the model generation with a search in a retrieval datastore to obtain candidate sequences. These approaches avoid extra training and can reduce computational overhead.
LLMA  Stengel et al., "Poly-encoders: Architectures and Training Challenges" is an inference-with-reference decoding mechanism by exploiting the overlap between the output and the reference of an LLM.
REST  Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" replaces the parametric draft model with a non-parametric retrieval datastore.