% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{xcolor,colortbl}
\definecolor{lightgrey}{rgb}{0.93,0.93,0.93}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% \usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{bbding}
\usepackage[normalem]{ulem}
\usepackage{enumitem}

\newcommand{\ourmodel}{\textsc{CodeSwift}}

\newcommand{\fang}[1]{{\color{red}[#1]}}
\newcommand{\xlian}[1]{{\color{olive}[#1]}}
\newcommand{\lj}[1]{{\color{blue}[#1]}}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\ourmodel{}: Accelerating LLM Inference for Efficient Code Generation}

%\xlian{Efficiency Through Simplicity:Accelerating LLM Inference in Code Generation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Qianhui Zhao$^{1}$, Li Zhang$^{1}$, Fang Liu$^{1*}$, Xiaoli Lian$^{1*}$, Qiaoyuanhe Meng$^{1}$, \\ {\bf Ziqian Jiao}$^{1}$,  {\bf Zetong Zhou}$^{1}$, {\bf Borui Zhang}$^{1}$,{\bf Runlin Guo}$^{1}$, {\bf Jia Li}$^{2}$ \\ 
$^1$School of Computer Science and Engineering, \\ State Key Laboratory of Complex \& Critical Software Environment,\\ Beihang University\\
$^2$Key Lab of High Confidence Software Technology (Peking University)\\
\texttt{\{zhaoqianhui, fangliu, lianxiaoli\}@buaa.edu.cn}
}

% \author{Qianhui Zhao \\

%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

% \author{
%  \textbf{Qianhui Zhao\textsuperscript{1}},
%  \textbf{Li Zhang\textsuperscript{1}},
%  \textbf{Fang Liu\textsuperscript{1*}},
%  \textbf{Xiaoli Lian\textsuperscript{1*}},
%  \textbf{Qiaoyuanhe Meng\textsuperscript{1}},
%  \\
%  \textbf{Ziqian Jiao\textsuperscript{1}},
%  \textbf{Zetong Zhou\textsuperscript{1}},
%  \textbf{Borui Zhang \textsuperscript{1}},
%  \textbf{Runlin Guo\textsuperscript{1}},
%  \textbf{Jia Li\textsuperscript{2}}

% \\
% \\
%  \textsuperscript{1}School of Computer Science and Engineering, \\ State Key Laboratory of Complex \& Critical Software Environment,\\ Beihang University\\
%  \textsuperscript{2}Key Lab of High Confidence Software Technology (Peking University)\\
%   \textsuperscript{*}Corresponding authors

% \\
% \{zhaoqianhui, fangliu, lianxiaoli\}@buaa.edu.cn
%  % \small{
%  %   \textbf{Correspondence:} \href{}{\{zhaoqianhui, fangliu, lianxiaoli\}@buaa.edu.cn}
%  % }
% }

\begin{document}
\maketitle
\begin{abstract}
Code generation is a latency-sensitive task that demands high timeliness, but the autoregressive decoding mechanism of Large Language Models (LLMs) leads to poor inference efficiency. 
Existing LLM inference acceleration methods mainly focus on standalone functions using only built-in components. Moreover, they treat code like natural language sequences, ignoring its unique syntax and semantic characteristics. As a result, the effectiveness of these approaches in code generation tasks remains limited and fails to align with real-world programming scenarios.
To alleviate this issue, we propose \ourmodel{}, a simple yet highly efficient inference acceleration approach specifically designed for code generation, without comprising the quality of the output. 
\ourmodel{} constructs a multi-source datastore, providing access to both general and project-specific knowledge, facilitating the retrieval of high-quality draft sequences.
Moreover, \ourmodel{} reduces retrieval cost by controlling retrieval timing, and enhances efficiency through parallel retrieval and 
a context- and LLM preference-aware cache.
Experimental results show that \ourmodel{} can reach up to $2.53 \times$ and $2.54\times$ speedup compared to autoregressive decoding in repository-level and standalone code generation tasks, respectively, outperforming state-of-the-art inference acceleration approaches by up to $88\%$.
Our code and data are available at \url{https://anonymous.4open.science/r/CodeSwift}.



\end{abstract}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Corresponding authors}
\renewcommand{\thefootnote}

\section{Introduction}

Large Language Models (LLMs) such as GPT-4o \cite{achiam2023gpt4} and DeepSeek-Coder \cite{guo2024deepseek} have demonstrated impressive performance in coding tasks, revolutionizing the landscape of software development \cite{GitHub-Copilot, li2023starcoder}. 
These models excel in code completion and generation but face a challenge: the significant inference time. LLMs use the autoregressive decoding mechanism, where each new token is generated conditioned on the previously generated tokens and the given context. 
However, developers typically hold high expectations regarding the responsiveness of code recommendations \cite{liu2024non}. If LLMs fail to deliver precise and efficient feedback, it may directly affect development efficiency and user experience.


To accelerate the inference process of LLMs, speculative decoding \cite{chen2023accelerating, leviathan2023fast} is regarded as
one of the effective solutions, which employs a draft-verification framework to minimize the number of forward steps. 
Specifically, it utilizes a small language model as a draft model to rapidly generate candidate output tokens, which are then verified for acceptability by the target LLM through a single forward step while keeping the output consistent with that decoded autoregressively by the target LLM itself.
Based on the draft-verification paradigm, many inference acceleration approaches have emerged \cite{chen2023cascade, zhang2024draft, zhao2024ouroboros, li2024eagle, miao2024specinfer}, most of which rely on an additional draft model, either selected from the same model family or trained for specific use cases. 
However, identifying a suitable draft model remains challenging, as it requires striking a delicate balance between maintaining a small model size and ensuring high output quality. Additionally, the draft model must align with the vocabulary of the target LLM,
further complicating the selection process.
More recently, researchers have explored replacing the parametric draft model with a non-parametric retrieval system \cite{he2024rest,yang2023llma}, which can easily be ported to any LLM without introducing additional training costs and have be applied to code generation task.


\begin{figure}[t]
    \centering
    \setlength{\abovecaptionskip}{0.1cm}
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{figures/repo_example/humaneval.png}
        \caption{A standalone function}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{figures/repo_example/repoeval.png}
        \caption{A repository-level function}
    \end{subfigure}
    \caption{Examples of standalone and repository-level functions. Intra-file and cross-file dependencies are highlighted in green and yellow, respectively.}
    \label{fig: repo-level example}
    \vspace{-0.4cm}
\end{figure}

While some of the above approaches have demonstrated promising performance in code generation task \cite{he2024rest,zhao2024ouroboros}, they primarily focus on standalone code functions that solely rely on built-in components. 
However, in real-world software development, it is crucial for developers to be aware of other files within the repository during programming \cite{zhang2023repocoder}, which gives rise to repository-level code generation (more details in Appendix \ref{appendix: repo-level code generation}).
As shown in Figure \ref{fig: repo-level example}, complex dependencies that span multiple levels can exist in repository-level functions. \textit{Experimental results show that existing inference acceleration approaches typically perform worse on repository-level code generation under the same settings than standalone ones.}
For example, Self-speculative decoding \cite{zhang2024draft} can achieve over $1.5 \times$ acceleration compared to autoregressive decoding in standalone code generation (Figure \ref{fig: humaneval results}), but falls short when applied to repository-level tasks, offering virtually no speedup in comparison to autoregressive inference (Table \ref{tab: main results}).
Moreover, existing approaches treat source code as sequences similar to natural language, without accounting for code's unique syntactic and semantic characteristics. 
\textit{As a result, the effects of existing LLM inference acceleration approaches on code generation tasks may be limited and fail to align with real-world scenarios.}


To alleviate this issue, in this paper, we primarily focus on improving the inference speed of LLMs on code generation task, covering both repository-level and standalone code, without comprising the quality of the output.
We propose \ourmodel{}, a \textbf{simple yet highly efficient} approach to accelerate the inference of LLMs through an efficient and effective retrieval strategy.
Concretely, we first construct a multi-source datastore, providing access to both general and project-specific knowledge and enhancing the quality of draft sequences. 
Then, \ourmodel{} reduces unnecessary retrieval overhead by controlling the retrieval timing. Besides, \ourmodel{} improves retrieval efficiency through parallel retrieval and the maintenance of a context- and LLM preference-aware cache. Finally, tree attention is employed to avoid redundant computation caused by verifying multiple draft sequences. 
Experimental results show that the decoding speed of \ourmodel{} surpasses existing inference acceleration approaches substantially on both repository-level and standalone code generation tasks. For repository-level code generation, \ourmodel{} achieves up to $2.30 \times$ and $2.53 \times$ speedup on DevEval \cite{li2024deveval} and RepoEval \cite{zhang2023repocoder}, respectively.
\ourmodel{} can also achieve up to $2.54 \times$ acceleration on standalone code generation dataset, HumanEval \cite{chen2021codex}.
It is worth noting that incorporating project-specific knowledge enables the generation of high-quality drafts, reducing the verification time and, consequently, the inference time of our model for repository-level code generation. However, this knowledge can be omitted in standalone code generation where such context is unnecessary.

Our contributions can be summarized as follows:
\begin{itemize}[nosep]
    \item We identify limitations of current LLM inference acceleration approaches within the context of real-world code generation and provide insights for potential improvements.
    \item We propose \ourmodel{}, a simple yet efficient approach to accelerate LLM inference for code generation by leveraging effective retrieval and verification mechanisms.
    \item We conduct a comprehensive evaluation of \ourmodel{} and results show that it achieves state-of-the-art results in both repository-level and standalone code generation tasks.
\end{itemize}



\section{Related Work} \label{sec: related work}
Autoregressive decoding generates tokens sequentially, leading to slow and costly decoding. 
To accelerate this process, draft-verification approaches \cite{chen2023accelerating,miao2024specinfer,he2024rest} have gained popularity recently as they enhance speed without compromising performance, which fall into generation-based and retrieval-based categories based on their draft generation techniques (more information in Appendix \ref{appendix: related work}).

\noindent\textbf{Generation-based approaches.}
Draft tokens can be generated either by a smaller model or by the target model itself. 
Speculative decoding \cite{chen2023accelerating, leviathan2023fast} employs a smaller model for drafting and uses the target LLM for efficient parallel verification. Ouroboros \cite{zhao2024ouroboros} generates draft phrases to enhance parallelism and extend drafts.
Alternatively, the target LLM itself can be utilized to efficiently draft \cite{stern2018blockwisedecoding, li2024eagle, fu2024lookahead}, which reduces system complexity and selection difficulties.
Medusa \cite{cai2024medusa} introduces multiple heads to predict multiple draft tokens in parallel.
Self-speculative decoding \cite{zhang2024draft} employs the target model with selectively certain intermediate layers skipped as the draft model.


\noindent\textbf{Retrieval-based approaches.}
The retrieval-based draft generation approach replaces the model generation with a search in a retrieval datastore to obtain candidate sequences. These approaches avoid extra training and can reduce computational overhead.
LLMA \cite{yang2023llma} is an inference-with-reference decoding mechanism by exploiting the overlap between the output and the reference of an LLM.
REST \cite{he2024rest} replaces the parametric draft model with a non-parametric retrieval datastore.





\section{Preliminaries}

\subsection{Retrieval-based Speculative Decoding}

Building upon the draft-verification framework introduced by speculative decoding \cite{chen2023accelerating, leviathan2023fast}, retrieval-based decoding acceleration approaches leverage a retrieval mechanism to generate draft tokens \cite{he2024rest,yang2023llma}, which can eliminate the challenge of selecting an appropriate draft model and avoid additional training costs.
A notable example is Retrieval-Based Speculative Decoding (REST) \cite{he2024rest}, which has proven to be effective in standalone function generation task \cite{chen2021codex}. Below is an explanation of how it works.
% It draws from the reservoir of existing knowledge, retrieving and employing relevant tokens based on the current context.
Pre-built from a code corpus, the datastore of $D = \{(c_i, t_i)\}$ serves as the source for the draft token sequence, where $c_i$ represents a context and $t_i$ represents the corresponding continuation of $c_i$.
As an alternative to the draft model, the objective of retrieval is to identify the most likely continuations of the current context from the datastore $D$ using a suffix match \cite{manber1993suffix}.
Specifically, given a context $s = (x_1,...,x_t)$, it aims to find contexts in $D$ that match the longest suffix of $s$.
Starting from a pre-defined match length upper limit $n_{max}$ (measured in the number of tokens), for each suffix length $n$, it extracts the suffix of $s$ with $n$ tokens, denoted as $q$, and obtains all contexts $c_i$ that match $q$ as a suffix. If at least one context in $D$ matches $q$, the corresponding context continuation pairs are returned as the retrieval result $S$; otherwise, the match length $n$ is decreased by one to attempt matching a shorter suffix. 
Subsequently, the top $k$ high-frequency prefixes in $S$ are selected as the draft sequences for later verification.
Inspired by REST, \ourmodel{} also incorporates a similar suffix-match-based retrieval algorithm, leveraging its advantages in time and memory efficiency.


\begin{figure}[t]
 \setlength{\abovecaptionskip}{0.1cm}
    \includegraphics[width=0.9\linewidth]{figures/preliminaries/motivation_example.png}
  \caption{Localness of source code.}
  \label{fig: motivation example}
  \vspace{-0.3cm}
\end{figure}




\begin{figure}[t]
 \setlength{\abovecaptionskip}{0.1cm}
     \begin{subfigure}[b]{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/preliminaries/Retrieved_Rate_heatmap.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/preliminaries/Whitespace_Rate_heatmap.png}
        \caption{}
    \end{subfigure}
  \caption {Heatmaps of (a) retrieval performance and (b) whitespace distribution with token positions in REST. The maximum token index is selected based on the average token number per line (12).
  }
  \vspace{-0.5cm}
  \label{fig: retrieval analysis}
\end{figure}

\subsection{Motivating Examples} \label{sec: observations}
To identify the limitations of current inference acceleration methods in code generation, we present motivating examples that highlight the localness of source code and the retrieval performance in retrieval-based approaches.

\noindent\textbf{Localness of source code.} Human-written programs are typically localized \cite{tu2014localness}, with
program entities (token sequences) defined or used in the preceding snippets frequently being reused in the subsequent code snippets within the same code file.
As shown in Figure \ref{fig: motivation example}, \textit{user\_id\_file\_path} is a user-defined variable within the current code segment, which does not exist in the datastore 
but appears multiple times in subsequent code snippets. Additionally, the blue-highlighted statements demonstrate the repetition of token sequences. \textit{By effectively leveraging these frequently occurring token sequences within the code file, such as storing them in a cache for subsequent retrieval, the acceptance length for draft validation can be increased, thereby enhancing the inference speed.}





\begin{figure*}[t]
    \centering
    \setlength{\abovecaptionskip}{0.1cm}
  \includegraphics[width=0.9\linewidth]{figures/architecture.pdf}
  \caption{Architecture of \ourmodel{}. The left part illustrates an overview, and the right part offers a detailed depiction of timing selection and retrieval operation.} 
  \label{fig: architecture}
  \vspace{-0.5cm}
\end{figure*}

\noindent\textbf{Retrieval is not always essential.}
Current work performs retrieval operation at \textit{every} position, which may bring unnecessary cost.
To investigate the relationship between retrieval performance and token position in code generation, we randomly selected 200 samples from DevEval \cite{li2024deveval}, a repository-level code generation benchmark, and employed DeepSeek-Coder-6.7B \cite{guo2024deepseek} for evaluation. 
For each token, we recorded whether it was: (a) retrieved from the datastore rather than generated by the model, and (b) a whitespace character (e.g., spaces or newline characters). 
Results are presented as heatmaps in Figure \ref{fig: retrieval analysis}. 
As seen from Figure \ref{fig: retrieval analysis}(a), \textit{retrieval failures are frequent, with a particularly notable pattern: the second token in each line has the lowest probability of being successfully retrieved}.
A comparison with the whitespace rate heatmap suggests that this phenomenon may stem from the fact that the second token is typically the first non-whitespace character at the beginning of a line.
The first non-whitespace token in each line dictates the direction of the line, making it more variable and consequently more challenging to retrieve.
Thus, \textit{skipping retrieval or reducing the retrieval probability at such positions may improve performance}. 


\section{Method}

The architecture of \ourmodel{} is shown in Figure \ref{fig: architecture}. In this section, we first describe the construction of datastore and cache, and then provide a detailed explanation of retrieval and verification process. 


\subsection{Multi-source Datastore Construction}
The quality of the retrieval datastore, which serves as the source of draft sequences, critically determines the acceleration potential. 
A larger datastore may enhance the probability of result acceptance, but it also correspondingly increases retrieval time, making the trade-off between the two critically important.
To achieve optimal performance with a compact datastore and facilitate effective retrieval, \ourmodel{} incorporates a smaller repository-related datastore $D_r$ and a larger common code datastore $D_c$ to construct a comprehensive retrieval datastore $D$. This design supports parallel retrieval, providing access to both general and project-specific knowledge. To enable fast retrieval with minimal overhead, we organize the datastore into context-continuation pairs, facilitating a rapid exact-match method for context search.


\noindent\textbf{Repository-related datastore $D_r$.} During software development, developers often reference cross-file elements such as classes and methods, making intra-repository files highly relevant to the generated code. Additionally, repository-specific factors, including domain variations and coding conventions, lead to distinct patterns of idiomatic expressions. For instance, web development repositories frequently involve HTTP request-response handling, while data science repositories focus on data processing and modeling tasks. To this end, we collect the code files from current repository (with the portions to be generated excluded) and form repository-related datastore $D_r$.

\noindent\textbf{Common datastore $D_c$.} To ensure that common programming operations are also retrievable, a subset of data from commonly used pre-trained code datasets \cite{kocetkov2022stack} is used to form $D_{c}$, which serves as another component of datastore $D$. 

\noindent{\textbf{Datastore organization.} For efficient retrieval, the datastore is organized as contexts and the corresponding continuations following \citet{he2024rest}.
Specifically, for each code file utilized in constructing the datastore, the content preceding every position will constitute a context, whereas the content subsequent to that position is the corresponding continuation.
The datastore $D$ of \ourmodel{} can be summarized as:
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\begin{align}
     D &= ( D_{r}, D_{c} ) \\
    (D_{r}, D_{c}) &= ( \{(c_i, t_i)\}_{i=1}^{|D_{r}|}, \{(c_j, t_j)\}_{j=1}^{|D_{c}|} )
\end{align}
where $c_i$ ($c_j$) represents the context, $t_i$ ($t_j$) represents the corresponding continuation of $c_i$ ($c_j$), $|D_{r}|$ ($|D_{c}|$) is the number of samples in $D_r$ ($D_c$).
For standalone code generation, $D_r$ can be omitted.

\subsection{Context- and LLM Preference-aware Caching}
To reduce retrieval costs and improve the alignment of retrieved results with LLM preferences—thereby increasing both the accepted sequence length and inference speed—we design a context- and LLM preference-aware caching strategy to cache the verified retrieved sequences and LLM generated sequences. 
Specifically, based on the observations in Section \ref{sec: observations}, program entities (token sequences) defined or used in preceding snippets are often reused in the subsequent code snippets.
Consequently, if the draft sequence $r=(y_1,...,y_j)$, retrieved by the context $s=(x_1,...,x_t)$, is verified by the LLM, we concatenate them as $(x_1,...,x_t,y_1,...,y_j)$ and add it into \textsc{cache}.
Moreover, since the datastore $D$ is static, the draft sequences retrieved for the identical context $s$ remain consistent.
However, different LLMs exhibit distinct generation preferences, leading to varied decoding outputs after draft verification. Additionally, earlier decoding outputs must maintain contextual relevance and coherence with subsequent outputs. Therefore, we also incorporate the verified decoding output sequence into \textsc{cache} for future use.


To maintain the \textsc{cache}, we assess whether the two aforementioned update conditions are satisfied after each forward step of the LLM. If the number of sequences inside the \textsc{cache} exceeds the pre-defined threshold $l$, it is accessible and will remain active throughout the entire inference process.



\setlength{\textfloatsep}{6pt}
\begin{algorithm}[t]
\small
\caption{Retrieval Algorithm}
\label{retrieval algorithm}
\KwIn{current context $s$, datastore $D$, retrieval cache \textsc{cache}, minimum activation size $l$, missing table $M$, skip token $\textit{token}_\text{skip}$, retrieval probability $p$}
\KwOut{Retrieved sequences $R$}

\If{$\textsc{cache}.size \geq l$}{ 
    // retrieval from cache\\
    $R \gets \text{search(\textsc{cache})}$
}
\If{$\textsc{cache}.size < l$ \textbf{or} $R=\emptyset$}{
    // retrieval timing selection \\
    \If{$s \in M$}{
        pass
    }
    \ElseIf{$s$ ends with $\textit{token}_\text{skip}$}{
        \If{random number < $p$}{
            // parallel retrieval from datastore\\
            $R_{r}, R_{c} \gets \text{par\_search}(D_{r}, D_{c})$ \\
            $R \gets (R_{r}, R_{c})$
        }
    }
}
\If{$R = \emptyset$}{
    // update missing table \\
    $M \gets M \cup \{s\}$
}
\Else{
    update \textsc{cache}
}
\Return $R$\;
\end{algorithm}


\subsection{Dynamic and Efficient Retrieval Strategy}


Algorithm \ref{retrieval algorithm} illustrates the complete retrieval process of \ourmodel{}.
Before each forward step, given current context $s$, \ourmodel{} initially verifies the availability of \textsc{cache}. If the \textsc{cache} is accessible, that is, the number of sequences inside exceeds $l$, retrieval is prioritized from \textsc{cache}. 
If \textsc{cache} is unavailable or fails to yield valid (non-empty) results, \ourmodel{} utilizes a dynamic and efficient retrieval strategy to minimize unnecessary retrieval cost. 
Specifically, \ourmodel{} optimizes retrieval timing by addressing two key considerations as follows.

\noindent{\textbf{Skip token.}
As mentioned in Section \ref{sec: observations}, the intrinsic characteristics of code lead to a low retrieval success rate at the first non-whitespace character of each line. 
Since obvious patterns are not found in other positions, and the introduction of intricate judgment processes may incur additional computational overhead, we set the first non-whitespace character of each line as the skip token.
We strategically reduce the retrieval probability of skip token through a control parameter $p$, which refers to the retrieval probability at these positions.


\noindent{\textbf{Missing table.}
When utilizing the current context $s$ to retrieve its continuations from datastore $D$, it may fail to yield any valid results in some cases. 
To prevent time wastage resulting from invalid retrieval, we maintain a missing table $M=\{s_{m_i}\}$ that stores suffixes $s_{m_i}$ for which no valid results can be retrieved from the datastore $D$.
Thus, when $s_{m_i}$ is encountered again during the subsequent inference, \ourmodel{} will bypass the retrieval and directly utilize the LLM to generate the next token. 

If \ourmodel{} decides to proceed with retrieval according to the above strategy, parallel retrieval is conducted from repository-related datastore $D_{r}$ and common datastore $D_{c}$ to further boost the retrieval efficiency, and the results refer to $R_{r}$ and $R_{c}$, separately. Specifically, if $R_{r}$ and $R_{c}$ are both empty, $s$ will be denoted as $s_m$ and added into the missing table $M$. Otherwise, relevant sequences are employed to update the \textsc{cache}.



\subsection{Draft Construction and Verification with Weighted Prefix Optimization} 
The retrieval results $R =(R_{r}, R_{c})$ contain potential continuations of the current context $s$, often sharing the same prefix. To reduce the cost brought by verification each $r_i \in R$ one by one, we construct the draft sequences using a Trie, where the unique path from a node to the root node corresponds to a prefix of the retrieval results, aiming to reduce the repeated verification of shared prefixes in $R$. 
We use following equation to assign a weight for each node:
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt} 
\begin{align}
    N_{weight} = \alpha \cdot t_{r} + \beta \cdot t_{c}
\end{align}
where $t_{r}$ and $t_{c}$ represents the times that the node occurs in $R_{r}$ and $R_{c}$ respectively, and  $\alpha$ and $\beta$ refers to the corresponding coefficient.
By controlling the values of $\alpha$ and $\beta$, the preference of draft sequences can be adjusted to accommodate different scenarios.
We select top-$k$ sequences from the Trie, ordered by their weights from highest to lowest, as the draft sequences.
Subsequently, the draft sequences are verified by LLM using tree attention \cite{spector2023treeattention, miao2024specinfer}.
As our objective is to accelerate the inference without compromising model performance, all correct tokens from the beginning will be accepted, while the draft tokens following the first error will be rejected. 








\section{Experiments}

\subsection{Experimental Setup}


\noindent\textbf{Datasets.}
We conduct experiments on both repository-level and standalone code generation benchmarks. For repository-level code generation, we choose two widely-used benchmarks, DevEval \cite{li2024deveval} and RepoEval \cite{zhang2023repocoder}.
DevEval comprises 1,825 testing samples from 115 repositories, covering 10 popular domains. It aligns with real-world repositories in code distributions and dependency distributions.
RepoEval is constructed using the high-quality repositories sourced from GitHub.
We use the function-level subset for evaluation, which contains 455 testing samples.
For standalone code generation, we conduct experiments on HumanEval \cite{chen2021codex}, a widely-used standalone code generation dataset including 164 human-written programming problems. 

\noindent\textbf{Backbone Models.}
We use the 1.3B and 6.7B configurations of Deepseek-Coder-base \cite{guo2024deepseek}, as well as 7B and 13B configurations of CodeLlama-Python \cite{roziere2023codellama} for evaluation, which are popular and  well-performing LLMs in code generation.



\begin{table*}[t]
    \centering
    \setlength{\abovecaptionskip}{0.1cm}
    \caption{Decoding speed and speedup ratio on repository-level code generation datasets.}
    \resizebox{\linewidth}{!}{ 
    \begin{tabular}{l|l|cc|cc|cc|cc}
        \toprule
        \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Approach}} & \multicolumn{2}{c|}{\textbf{Deepseek-Coder-1.3B}} & \multicolumn{2}{c|}{\textbf{Deepseek-Coder-6.7B}} & \multicolumn{2}{c|}{\textbf{CodeLlama-7B}} & \multicolumn{2}{c}{\textbf{CodeLlama-13B}} \\
         & &  \textbf{ms/token} & \textbf{Speedup} &  \textbf{ms/token} & \textbf{Speedup} &  \textbf{ms/token} & \textbf{Speedup} &  \textbf{ms/token} & \textbf{Speedup}\\
        \midrule
        \multirow{5}{*}{DevEval} & Autoregressive & 20.00 & 1.00$\times$ & 26.15 & 1.00$\times$ & 26.29 & 1.00$\times$ & 46.35 & 1.00$\times$\\
         % & Speculative & - & - & &  \\
         & Self-speculative & 18.72 & 1.07$\times$ & 22.55 & 1.16$\times$ & 25.10 & 1.05$\times$ & 42.74 & 1.08$\times$ \\
         & Ouroboros & - & - & 15.69 & 1.67$\times$ & 29.14 & 0.90$\times$ & \underline{39.73} & \underline{1.17$\times$} \\
         & REST & \underline{12.10} & \underline{1.65$\times$} & \underline{15.28} & \underline{1.71$\times$} & \underline{15.57} & \underline{1.69$\times$} & 43.38 & 1.07$\times$ \\
         % & \ourmodel{} & 8.70 & 2.30$\times$ &  &  &  &  \\
         & \ourmodel{} & \textbf{8.71} & \textbf{2.30$\times$} & \textbf{11.69} & \textbf{2.24$\times$} & \textbf{12.17} & \textbf{2.16$\times$} & \textbf{21.56} & \textbf{2.15$\times$}\\
        \midrule
        \multirow{5}{*}{RepoEval} & Autoregressive & 19.91 & 1.00$\times$ & 25.75 & 1.00$\times$ & 26.21 & 1.00$\times$ & 47.86 & 1.00$\times$\\
         % & Speculative & - & -  & & \\
         & Self-speculative & 19.63 & 1.02$\times$ & 22.48 & 1.16$\times$ & 24.36 & 1.08$\times$ & 42.09 & 1.14$\times$ \\
         & Ouroboros & - & - & \underline{14.56} & \underline{1.77$\times$} & 33.12 & 0.79$\times$ & \underline{35.60} & \underline{1.34$\times$} \\
         & REST & \underline{12.09} & \underline{1.65$\times$} & 15.46 & 1.67$\times$ & \underline{15.43} & \underline{1.70$\times$} & 44.59 & 1.04$\times$ \\
         & \ourmodel{} & \textbf{7.88} & \textbf{2.53$\times$} & \textbf{10.83} & \textbf{2.38$\times$} & \textbf{10.80} & \textbf{2.43$\times$} & \textbf{19.02} & \textbf{2.52$\times$}\\
         % & \ourmodel{} & 8.27 & 2.41x & 11.07 & 2.33x & 11.09 & 2.36x\\
        \bottomrule
    \end{tabular}
    }
    \label{tab: main results}
    \vspace{-0.3cm}
\end{table*}

\begin{figure*}[t]
 \centering
  \setlength{\abovecaptionskip}{0.1cm}
  \includegraphics[width=\linewidth]{figures/humaneval_results.pdf}
  \caption{Decoding speed and speedup ratio on HumanEval.}
  \label{fig: humaneval results}
  \vspace{-0.5cm}
\end{figure*}


\noindent\textbf{Baselines.}
We compare \ourmodel{} with vanilla autoregressive decoding and several state-of-the-art inference acceleration approaches that follow the draft-verification framework and have demonstrated effectiveness in code generation, including 
Self-speculative decoding \cite{zhang2024draft}, Ouroboros \cite{zhao2024ouroboros}, and REST \cite{he2024rest}.
Self-speculative decoding requires several hours to identify skipped layers in the target LLM for draft model construction.
Ouroboros demands manual selection of a suitable draft model for the target LLM.
REST is draft model-free but suffers from misalignment between retrieval sequences and the LLM output.



\noindent\textbf{Evaluation Metrics.}
We report the \textit{decoding speed} (ms/token) and the \textit{speedup ratio} compared with vanilla autoregressive decoding.
We also compare the \textit{average acceptance length}, 
defined as the average number of tokens accepted per forward step by the target LLM, which reflects the upper bound of achievable acceleration.
Since \ourmodel{} and baselines do not compromise model performance, the correctness of the generated code is not evaluated.




\noindent\textbf{Implementation Details.} 
To provide essential contextual information, we prepend preceding code snippets from the same file as context for DevEval and RepoEval.
All results are obtained with a maximum input length of 2k and a maximum generation length of 512 under greedy decoding.
We focus on greedy decoding results as baseline approaches perform optimally with greedy decoding and comparably to other sampling strategies.
$D_{c}$ is constructed from a subset of Python pre-training code in The Stack \cite{kocetkov2022stack}, taking approximately 9 minutes and yielding a 0.9GB datastore.
$D_{r}$ ranges from 60KB and 289MB across repositories, taking an average of 10 seconds.
Hyper-parameters include $l=50$, $p=0.5$, $\alpha=\beta=1$, with LLM output truncated every 20 tokens and added to the \textsc{cache}. 
Following \citet{he2024rest}, for retrieval, the starting context suffix length $n_{max}=16$, and a maximum of 64 draft tokens of the top-$k$ sequences are selected in the Trie.
Baseline implementation details are in Appendix \ref{appendix: baseline details}.
Experiments for Deepseep-Coder and CodeLlama-7B use a single NVIDIA 4090 GPU and 28 CPU cores, and CodeLlama-13B experiments use a single NVIDIA A6000 GPU and 12 CPU cores.




\subsection{Main Results}
\subsubsection{Repository-level Code Generation}
The comparison results between \ourmodel{} and baselines are shown in Table \ref{tab: main results}.
\ourmodel{} achieves up to $2.30 \times$ and $2.53 \times$ speedup on DevEval and RepoEval, respectively, 
outperforming state-of-the-art approaches by up to $88\%$.
\ourmodel{} consistently maintains a stable speedup of more than $2 \times$ across a variety of backbone models and datasets, and repositories spanning various topics (Appendix \ref{appendix: code topic}), demonstrating its robustness.

Compared to the substantial speedups gained by \ourmodel{}, baseline approaches achieve limited accelerations.
As a retrieval-based approach, the datastore utilized by REST is approximately 8 times the size of the one employed by \ourmodel{}.
REST exhibits the optimal speedup of around $1.7\times$ in most cases, but it performs poorly in experiments of CodeLlama-13B. 
This may be attributed to the fact that the significant CPU resource demands posed by both the 13B model inference and the retrieval of data from a large datastore in REST, leading to decreased performance.
Besides, Ouroboros demonstrates comparable performance to REST on Deepseek-Coder-6.7B, yet its generation speed is even slower than autoregressive decoding on CodeLlama-7B, indicating that its efficacy is subject to considerable fluctuations influenced by factors such as model selection.
Self-speculative decoding consistently maintains a stable yet modest acceleration.
On the contrast, \textbf{\ourmodel{} does not require a draft model or additional training, yet it can maintain a stable speedup ratio even under resource-constrained conditions.}


\subsubsection{Standalone Code Generation}
For \ourmodel{}, we remove $D_r$ from the datastore and retain $D_c$, which is the same as the one used in the previous experiments. 
The results are shown in Figure \ref{fig: humaneval results}. \textbf{Even without the benefit of the multi-source datastore, \ourmodel{} still outperforms the baselines}, further demonstrating the effectiveness of the retrieval strategy and caching modules.
Additionally, we observe that the baselines consistently perform better on HumanEval compared to repository-level datasets.
This may be affected by the difficulty difference between standalone and repository-level code generation tasks.
For instance, Deepseek-Coder-1.3B achieves pass@1 scores of 34.8 on HumanEval and 18.2 on DevEval.
Thus, for approaches such as Ouroboros and Self-speculative which require a draft model, the performance in repository-level code generation may be negatively affected by the poor performance of the draft model.
For REST, HumanEval involves no project-specific knowledge, and the common datastore may adequately satisfy retrieval requirements.
The performance differences of existing approaches on the two types of code generation tasks also highlight that \textit{evaluations based solely on standalone datasets may fail to reflect performance in real-world application scenarios.}







\subsection{Ablation Study}


To analyze the effectiveness of each component within \ourmodel{}, we conduct an ablation study with the results presented in Table \ref{tab: ablation}. 
Each component is found to contribute to a speedup gain.
The multi-source datastore provides richer and more interrelated retrieval content, not only enhancing the average acceptance length but also minimizing the external retrieval cost through parallel search.
The retrieval strategy accelerates the inference by reducing unnecessary retrieval operations (4.02\% of the total count of retrieval), with negligible impact on the average acceptance length. 
The \textsc{cache} is the most effective component, which provides an additional increase in average acceptance length of over 30\% compared to the baseline.
Statistical analysis shows that, although the \textsc{cache} contains only 174 sequences at most for DevEval, 33.13\% of all retrieval operations can successfully obtain valid results directly from the \textsc{cache}. 
The average retrieval time from the cache is 0.2ms, which is approximately 15\% of the retrieval time from the datastore.
A case study is shown in Appendix \ref{appendix: case study}.


\begin{table}[h]
\setlength{\abovecaptionskip}
{0.1cm}
    \caption{Ablation study results of \ourmodel{} on DevEval using Deepseek-Coder-6.7B. Each component is incrementally added. The baseline results are obtained using REST with $D_c$ as the datastore. \textit{AccLen} refers to average acceptance length.}
    \centering
    \resizebox{\linewidth}{!}{ 
    \begin{tabular}{lccc}
        \toprule
         & \textbf{AccLen} & \textbf{ms/token} & \textbf{Speedup} \\
        \midrule
        Baseline & 1.89 & 15.86 & 1.65$\times$\\
        + multi-source datastore & 2.28 & 14.82 & 1.76$\times$\\
        + retrieval strategy & 2.28 & 14.19 & 1.84$\times$\\
        + \textsc{cache} & 2.85 & 11.69 & 2.24$\times$\\
        \bottomrule
    \end{tabular}
    }
    \label{tab: ablation}
\end{table}



\subsection{Analysis of Acceptance Length}

We compare the acceptance length between \ourmodel{} and REST (the best performing baseline), which represents the upper bound of achievable acceleration.
The results is shown in Figure \ref{fig: acc comparison and heatmap}(a) (more results in Appendix \ref{appendix: acceptance length}).
\textbf{\ourmodel{} exhibits a longer acceptance length across all datasets, with an increase exceeding 50\% compared to REST on RepoEval.}
Although the size of REST's datastore is approximately 8 times that of \ourmodel{}, \ourmodel{} achieves a higher acceleration upper bound.
As REST's performance improves with the increasing size of the datastore when resources are sufficient \cite{he2024rest}, we do not claim that \ourmodel{} can outperform REST under all circumstances. 
Nonetheless, \ourmodel{} provides a more lightweight and efficient inference acceleration approach.

\setlength{\textfloatsep}{6pt}
\setlength{\intextsep}{6pt}
\begin{figure}[h]
 \setlength{\abovecaptionskip}{0.1cm}
    \begin{subfigure}[b]{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/acceptance_length.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/Retrieved_Rate_heatmap_our.png}
        \caption{}
    \end{subfigure}
  \caption {(a) Acceptance length of \ourmodel{} and REST; (b) Retrieval performance of \ourmodel{}. }
  \label{fig: acc comparison and heatmap}
  \vspace{-0.3cm}
\end{figure}

\subsection{Heatmap of Retrieval Performance}
To explicitly illustrate \ourmodel{}'s effectiveness, we depict its retrieval performance heatmap in Figure \ref{fig: acc comparison and heatmap}(b), with all settings aligned with Figure \ref{fig: retrieval analysis}(a).
A clear observation is that the overall color intensity of Figure \ref{fig: acc comparison and heatmap}(b) is markedly darker compared to Figure \ref{fig: retrieval analysis}(a), indicating a significant increase in the probability of \ourmodel{} retrieving valid results. This improvement underscores the enhanced retrieval efficacy of \ourmodel{}.






\section{Conclusion}
In this paper, we propose \ourmodel{}, a simple and efficient LLM inference acceleration approach for code generation without compromising generation quality.
\ourmodel{} leverages a multi-source datastore and a context- and LLM preference- aware cache to improve the acceptance length of the retrieved draft while minimizing redundant retrieval operations through a dynamic and efficient retrieval strategy.
Experimental results demonstrate that \ourmodel{} outperforms state-of-the-art inference approaches in decoding speed for both standalone and repository-level code generation tasks.
Requiring no draft model or additional training, \ourmodel{} provides a lightweight and practical solution for LLM inference acceleration in code generation.


\section*{Limitations}

Although \ourmodel{} offers advantages in accelerating LLM inference for code generation, it also has limitations that need to be taken into account.
Firstly, we only present the experimental results on code generation benchmarks written in Python. Nevertheless, \ourmodel{} is designed to be universally applicable and can be seamlessly extended to other programming languages.
Additionally, in the process of integrating repository code into the datastore, \ourmodel{} directly utilizes the entire code files. However, the development of an effective method for extracting high-frequency expressions from repositories could potentially enhance performance.


\section*{Ethical Considerations}
We emphasize that the entirety of our research is based on open-source datasets, models, and tools.
Our method has no potential risk since it is training-free and has no impact on the generation results.

\bibliography{references}

\clearpage
\appendix

\begin{table*}[t]
\setlength{\abovecaptionskip}{0.1cm}
\caption{The skipped layers utilized in draft models for Self-speculative decoding.}
\centering
\resizebox{\linewidth}{!}{ 
    \begin{tabular}{lll}
        \toprule
         & \textbf{Index of Skipped Attention Layers} & \textbf{Index of Skipped MLP Layers} \\
         \midrule
        \textbf{Deepseek-Coder-1.3B} & {[}3, 6, 8, 9, 10, 13, 14, 15, 16, 18, 21, 22{]} & {[}4, 6, 9, 10, 20{]} \\
        \textbf{Deepseek-Coder-6.7B} & {[}2, 5, 7, 8, 11, 12, 16, 18, 19, 20, 22, 23, 24, 25, 26, 28{]} & {[}2, 5, 6, 12, 15, 25, 26, 27, 28{]} \\
        \textbf{CodeLlama-7B} & {[}4, 5, 7, 10, 11, 12, 13, 14, 18, 20, 21, 22, 27, 29, 31{]} & {[}8, 11, 13, 22, 23, 25, 27, 28, 31{]} \\
        \textbf{CodeLlama-13B} & {[}5, 6, 9, 10, 11, 14, 15, 16, 21, 23, 24, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37{]} & {[}10, 11, 12, 14, 15, 25, 26, 27, 30, 32, 33, 34{]} \\
        \bottomrule
    \end{tabular}
}
\label{tab: skip layers}
\vspace{-0.3cm}
\end{table*}



\section{Repository-level Code Generation} \label{appendix: repo-level code generation}
Code generation refers to the generation of code snippets that meet requirements based on natural language requirements.
Most previous researches, such as the widely used datasets HumanEval\cite{chen2021codex} and MBPP \cite{austin2021mbpp}, focus on standalone scenarios, which means the generated functions may invoke or access only built-in functions and standard libraries.

Researches on standalone code generation often diverges from the complexities of real-world programming tasks. In practical development settings, developers typically work within project environments, where the code to be generated is frequently intertwined with external contexts, such as API calls. This code often relies on the methods and properties defined in other files. These non-standalone functions constitute more than 70\% of the functions in popular open-source projects, and evaluating models’ effectiveness on standalone functions cannot reflect these models’ effectiveness on pragmatic code generation scenarios (i.e., code generation for real settings of open source or proprietary code) \cite{yu2024codereval}.
Consequently, there has been growing interest in repository-level code generation \cite{liu2024repobench, wu2024repoformer, liang2024repofuse}, which refers to leveraging repository-level context during code generation tasks, rather than restricting the context to the file under development. Code files within a repository are often interdependent, featuring cross-module API calls, shared global snippets, and other forms of linkage. 
Researchers have introduced benchmark datasets such as RepoEval \cite{zhang2023repocoder}, CoderEval \cite{yu2024codereval}, CrossCodeEval \cite{ding2024crosscodeeval} and DevEval \cite{li2024deveval}. These datasets provide structured means for assessing the quality and relevance of generated code in realistic scenarios.

\section{LLM inference acceleration approaches} \label{appendix: related work}

Autoregressive decoding generates tokens in a step-by-step manner and results in a slow and costly decoding process.
In order to accelerate decoding, non-autoregressive decoding approaches \cite{ghazvininejad2019maskpredict,liu2024non} that can generate multiple tokens in parallel have been proposed. While improving decoding speed, these approaches typically affect the model performance. Therefore, draft-verification decoding acceleration approaches \cite{chen2023accelerating,miao2024specinfer,he2024rest} have been widely adopted recently, which do not comprise the model performance. These approaches can be further categorized into generation-based and retrieval-based, depending on the technique used for draft generation.

\subsection{Generation-based Approaches}
The draft token can be generated either by the target LLM itself or by a small model. Using the target LLM itself to directly generate the token may get a higher acceptance rate, while using a small model is more likely to have a faster generation speed.
\paragraph{Using a small model.}
Speculative decoding \cite{chen2023accelerating, leviathan2023fast} is one of the effective acceleration approaches that minimize the target LLM forward steps by using an smaller model for drafting and then employing the target LLM to verify the draft in a low-cost parallel manner. 
Ouroboros \cite{zhao2024ouroboros} generates draft phrases to parallelize the drafting process and lengthen drafts.
Specinfer \cite{miao2024specinfer} uses many draft models obtained from distillation, quantization, and pruning to conduct speculations together.

\paragraph{Using the target LLM itself.}
Identifying an appropriate draft model continues to pose significant challenges, as it must align with the vocabulary of the target LLM and achieve a delicate balance between keeping quick decoding speed and ensuring output quality.
Thus, researchers have investigated utilizing the target LLM itself to generate efficient draft sequences.
Blockwise Decoding \cite{stern2018blockwisedecoding} installs multiple heads on the transformer decoder, enabling parallel generation of multiple tokens per step.
Medusa \cite{cai2024medusa} introduces multiple heads to predict multiple draft tokens in parallel.
Lookahead decoding \cite{fu2024lookahead} uses a n-gram pool to cache the historical n-grams generated so far. 
Eagle \cite{li2024eagle} conducts the drafting process at the more structured feature level.
Self-speculative decoding \cite{zhang2024draft}) employs the target LLM with selectively certain intermediate layers skipped as the draft model.


\subsection{Retrieval-based Approaches}
The retrieval-based draft generation approach replaces the model generation with a search in a retrieval datastore to obtain candidate sequences. These approaches can avoid extra training and reduce computational overhead.
LLMA \cite{yang2023llma} is an inference-with-reference decoding mechanism by exploiting the overlap between the output and the reference of an LLM. It provides generic speedup through speculative retrieval and batched verification.
REST \cite{he2024rest} replaces the parametric draft model with a non-parametric retrieval datastore.
As many subsequences during generation likely appear in the datastore, it can frequently generate multiple correct tokens per step.




\section{Implementation Details of Baselines} \label{appendix: baseline details}

\paragraph{Self-speculative decoding.}
For the selection of skipped layers, we adopt the results provided by the authors \cite{zhang2024draft} for CodeLlama-13B. As for DeepSeek-Coder and CodeLlama-7B, for which the authors did not provide skipped layer configurations, we utilize Bayesian optimization on 4 samples from The Stack \cite{kocetkov2022stack} to determine the layers to skip during the drafting stage. The results can be seen in Table \ref{tab: skip layers}.
Other settings remain consistent with the original paper.

\paragraph{Ouroboros.}
This approach requires a draft model for the target LLM, and our selection is illustrated in Table \ref{tab: draft model}. 
We prioritize the selection of a smaller model from the same series as the target LLM to serve as the draft model.
For CodeLlama-7B, which is the smallest model in its series, we opt for TinyLlama-1.1B as the draft model due to its shared architecture and tokenizer compatibility.
For the configuration  of hyper-parameters, we used $\gamma = 11$ for DeepSeek-Coder and $\gamma = 4$ for CodeLlama, following the recommendations provided in the original paper.

\begin{table}[h]
\caption{Draft model selection for Ouroboros.}
\resizebox{\columnwidth}{!}{
    \begin{tabular}{ll}
        \toprule
        \textbf{Target Model} & \textbf{Draft Model} \\
        \midrule
        Deepseek-Coder-base-6.7B & Deepseek-Coder-base-1.3B \\
        CodeLlama-Python-7B & TinyLlama-1.1B-v1\_math\_code \\
        CodeLlama-Python-13B & CodeLlama-Python-7B \\ \bottomrule
    \end{tabular}
}
\label{tab: draft model}
\vspace{-0.3cm}
\end{table}

\paragraph{REST.}
To construct the datastore, we select the first 10 files out of the 145 files in The Stack dataset \cite{kocetkov2022stack}, resulting in a datastore of approximately 8.7 GB in size. The results of REST demonstrate that its performance on the HumanEval dataset improves as the size of the datastore increases \cite{he2024rest}. However, due to hardware limitations, we have chosen the largest feasible datastore that could be operated under the given constraints.
The values of the other hyper-parameters are consistent with those in the original paper.
Specifically, when performing exact match in the datastore, the starting context suffix length, $n_{max}$, is set to 16. The maximum number of selected draft tokens in the constructed Trie is set to 64.

\section{Performance on Different Code Topics} \label{appendix: code topic}
As DevEval includes code repositories spanning 10 distinct topics, we present the results of \ourmodel{} using Deepseek-Coder-6.7B for code generation separately for each topic. As shown in Figure \ref{fig: topic comparison}, \ourmodel{} demonstrates consistent and substantial acceleration in code generation across all topics, highlighting its robustness and effectiveness in diverse contexts.

\begin{figure}[h]
  \includegraphics[width=\linewidth]{figures/topic.png}
  \caption {Performance of \ourmodel{} on different code topics.}
  \label{fig: topic comparison}
  \vspace{-0.3cm}
\end{figure}


 
\section{Comparison of Acceptance Length} \label{appendix: acceptance length}

We compare the acceptance length between \ourmodel{} and REST on both DeepSeek-Coder and CodeLlama. The results are shown in Table \ref{tab: accept length aomparison}.
\ourmodel{} exhibits a longer acceptance length across all datasets and backbone models.

\begin{table}[h]
    \caption{Acceptance length comparison between \ourmodel{} and REST. \textit{DC} and \textit{CL} are abbreviations for Deepseek-Coder and CodeLlama, respectively.}
    \resizebox{\linewidth}{!}{ 
    \begin{tabular}{lcccccc}
        \toprule
         & \multicolumn{2}{c}{\textbf{DevEval}} & \multicolumn{2}{c}{\textbf{RepoEval}} & \multicolumn{2}{c}{\textbf{HumanEval}} \\
         & \textbf{REST} & \textbf{\ourmodel{}} & \textbf{REST} & \textbf{\ourmodel{}} & \textbf{REST} & \textbf{\ourmodel{}} \\ 
         \midrule
        \textbf{DC-1.3B} & 2.04 & 2.97 & 2.04 & 3.21 & 2.38 & 2.87 \\
        \textbf{DC-6.7B} & 2.06 & 2.85 & 2.08 & 3.05 & 2.38 & 2.92 \\
        \textbf{CL-7B} & 2.05 & 2.77 & 2.07 & 3.06 & 2.27 & 2.79 \\
        \textbf{CL-13B} & 2.06 & 2.75 & 2.06 & 2.99 & 2.25 & 2.63 \\ 
        \bottomrule
    \end{tabular}
    }
    \label{tab: accept length aomparison}
\end{table}

\begin{figure*}[h]
\centering
  \includegraphics[width=0.9\linewidth]{figures/case_study.pdf}
  \caption {Case study of \ourmodel{}'s retrieval performance.}
  \label{fig: case}
\end{figure*}

\section{Case Study} \label{appendix: case study}
To demonstrate the effectiveness of \ourmodel{}, we conduct a case study. As shown in Figure \ref{fig: case}, we use different background colors to highlight the sources of the accepted draft tokens.
Additionally, the tokens enclosed in red boxes are those that can be retrieved by \ourmodel{} but not by the baseline (REST with $D_c$ as the datastore).
When generating the earlier parts of the sequence, the \textsc{cache} remains unavailable due to an insufficient accumulation of sequences. Nonetheless, lots of repository-related tokens can be additionally retrieved by \ourmodel{} benefiting from the multi-source datastore.
When the \textsc{cache} is available, a larger number of consecutive tokens becomes retrievable, thereby enhancing the inference speed through the extension of acceptable sequence lengths and the reduction of retrieval overhead.






\end{document}