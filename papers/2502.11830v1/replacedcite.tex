\section{Related Work}
\label{sec:related}
Text classification, the task of classifying a given text into a pre-defined list of categories, is a well-studied problem. From bag-of-words features to the current state-of-the-art LLMs, numerous approaches have been explored in the past. Access to large amounts of human labeled data has traditionally played a significant role in improving text classifiers, and NLP research in the past two decades addressed this issue by looking at different solutions to learn from little or no labeled data.

\paragraph{Zero-shot Pre-LLM Approaches: }Some of the earlier classification approaches relied on using only label names to build "data less" text classifier ____ and embedding the texts and labels in a shared space ____. ____ proposed to formulate zero-shot text classification as a textual entailment problem, although ____ point to the limitations of this approach in terms of variability across datasets and reliance on spurious lexical patterns. Another practical approach for zero-shot classification is cross-lingual transfer i.e., train a classification model in one or more languages, and use it as a zero-shot classifier on the target language ____. Except ____, who studied sentiment and hate speech classification tasks, all the research has  focused primarily on English datasets. 

\paragraph{Few-shot fine-tuning: } Approaches that can learn from a small amount of ($<20$ samples per category) labeled examples have also been explored in the recent years ____. SetFit ____ introduced an approach based on supervised contrastive learning, transforming a language model into a topic encoder using only a few examples per label, and demonstrated effectiveness with datasets where the number of categories are low (under 5). FastFit ____ proposed an approach that scales to many classes (50--150) effectively, and showed its usefulness with English datasets. Out of these only SetFit evaluated with a few non-English datasets. %We consider FastFit, and compare its performance with the recent zero-shot LLMs and other approaches in this paper. 

\paragraph{Zero-shot Classification with LLMs: }With the arrival of Large Language Models, some recent approaches explored proprietary models like GPT3.5 and GPT4 for zero-shot or few-shot in-context learning for text classification across several datasets ____. Extending this line of work, open LLMs were studied in the context of intent classification ____ and computational social science ____. However, comparing such zero-shot approaches with few-shot and full-data  based fine-tuning, ____ show that smaller, fine-tuned classifiers outperform zero-shot approaches. Whether supervised fine-tuning of LLMs offers any benefit is an unexplored question. Surprisingly, except ____, all these experiments have been focused only on English datasets so far. We expand this strand of work to 7 other languages, and provide more detailed comparisons across different LLMs. 

\paragraph{Synthetic Data: } One approach to address the labeled data problem is to augment existing data by creating new data by applying text transformations such as replacing synonyms, paraphrasing, back translation etc. ____ presents a detailed survey of such data augmentation techniques for text classification. An extension of this idea is to directly synthesize the labeled data using generative language models ____. In the recent past, Large Language Model based synthetic data generation is increasingly observed across different NLP tasks ____. GPT4 has been used for English ____ and code-mixed ____ synthetic data generation for text classification with mixed results. We extend this line of work by covering more languages and exploring multiple LLMs as sources for synthetic data instead of relying on one, and extending to handle datasets with a larger label set. 

Overall, we address several gaps in existing research by comparing zero-shot classification, few-shot fine-tuning, synthetic data based classification, and classification with full data together, and also study how the comparison works out once we go beyond English. In this process, we also present a comparison between different open and closed recent LLMs.