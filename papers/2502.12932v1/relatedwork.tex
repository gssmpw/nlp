\section{Related Works}
% [FJ]: This needs to be elaborated further to become three paragraphs. Explain what tasks, methods that these citations do. How is the results -- complete
\paragraph{LLM-Generated Data Creation} One possible solution to tackle data scarcity in NLP is applying data augmentation \citep{feng-etal-2021-survey, ding-etal-2020-daga, ahmed-buys-2024-neural, liu2024best, yong-etal-2024-lexc, guo2024generativeaisyntheticdata, liu-etal-2022-wanli}. These methods leverage the generative capabilities of LLMs to produce high-quality synthetic data that can sometimes complement or even substitute manual dataset creation.

% Synthetic data has become an increasingly valuable solution to address issues like data scarcity, privacy concerns, and the high costs of manual dataset creation. In this direction, \citet{liu2024bestpracticeslessonslearned} discuss best practices for ensuring that synthetic data remains diverse, factually accurate, and high in fidelity, ensuring that it can effectively support the development of more inclusive and reliable language models. However, despite its potential, synthetic data comes with limitations. \citet{guo2024generativeaisyntheticdata} highlight key concerns, particularly regarding correctness and diversity. While LLMs can generate vast amounts of data, ensuring factual accuracy remains a challenge, as models may introduce hallucinations or inconsistencies. Additionally, maintaining diversity is essential to prevent biases and improve generalization, requiring careful curation and filtering to ensure that datasets remain balanced and representative.

% \citet{liu2024bestpracticeslessonslearned} highlight the importance of maintaining diversity, factual accuracy, and fidelity in synthetic data to ensure it is useful for real-world applications. While synthetic data offers a scalable alternative to manual dataset creation, \citet{guo2024generativeaisyntheticdata} point out key limitations, particularly regarding correctness and diversity. LLMs can generate vast amounts of data, but they may introduce hallucinations or biases, making careful curation and filtering essential to maintain quality and representativeness.

% In another research, \citet{kaddour2024syntheticdatagenerationlowresource} explore how fine-tuning a LLM can be used to generate high-quality synthetic data, improving the performance of smaller student models. Their approach, tested on four text classification and two text generation tasks, shows that even with limited initial training data, synthetic examples—whether through annotating unlabeled data or generating new input-output pairs—can significantly boost model performance. 

% LexC-Gen, proposed by \citet{yong2024lexcgen}, leverages LLMs for generating classification task data in extremely low-resource Indonesian languages. By employing a lexicon-conditioned approach, it mitigates data-lexicon mismatches through controlled text generation and word-to-word translation using bilingual lexicons, leading to competitive performance with expert-translated datasets in sentiment analysis and topic classification.

% WANLI, introduced by \citet{liu-etal-2022-wanli}, uses a hybrid approach where synthetic data is first overgenerated by GPT-3 and then filtered or revised by human experts to ensure high quality. This process results in a more diverse and reliable dataset, improving model generalization across different domains. Inspired by this approach, we adapt and modify it to better fit the needs of our research.

% Several studies have proposed different techniques to improve the effectiveness of synthetic data. \citet{kaddour2024syntheticdatagenerationlowresource} show that fine-tuning an LLM can generate high-quality data for downstream models, significantly improving their performance across text classification and text generation tasks. In low-resource settings, \citet{yong2024lexcgen} introduce LexC-Gen, a lexicon-conditioned method for generating classification data in underrepresented Indonesian languages, helping mitigate mismatches between synthetic and real data. Another hybrid approach is seen in WANLI \citep{liu-etal-2022-wanli}, where synthetic data is overgenerated by GPT-3 and then filtered or revised by human annotators, resulting in a more diverse and reliable dataset. %Inspired by these methods, we adapt and refine LLM-generated synthetic data techniques to better fit the needs of our research.

% Inspired by these methods, we refine LLM-generated synthetic data techniques by utilizing more LLMs—three open-weight and three closed-weight—ensuring greater variability. Unlike WANLI, which relies on a single LLM, our approach combines LLM-generated, machine-translated, and human-written samples. We further guide LLMs to generate content based on predefined topics while embedding cultural nuances relevant to Javanese and Sundanese, creating more contextually rich and diverse narratives.

%\citet{kaddour2024syntheticdatagenerationlowresource} demonstrate that fine-tuning LLMs can significantly improve downstream model performance in text classification and generation. 
%\citet{yong2024lexcgen} introduce LexC-Gen, a lexicon-conditioned approach for generating classification data in underrepresented Indonesian languages,

Prior studies have explored techniques for enhancing synthetic data generation, primarily for classification tasks. WANLI \citep{liu-etal-2022-wanli} used GPT-3 \citep{brown2020language} to generate synthetic natural language inference data in English \cite{bowman-etal-2015-large}, followed by human refinement. \citet{yong-etal-2024-lexc} leveraged LLMs to generate synthetic English data for sentiment analysis and topic classification, then applied a bilingual lexicon for word-to-word translation into low-resource languages. 
% RIFKI >> in contemporernous works, one low-ersource untuk QA tapi tidak benchmarking.
In contemporaneous work, \citet{putri-etal-2024-llm} utilized GPT-4 \cite{openai2024gpt4technicalreport} for question-answering tasks in one low-resource language. Unlike these works, we focus specifically on reasoning tasks in two low-resource languages and compare multiple strategies, including LLM-assisted generation using both open-weight and closed-weight models, machine translation, and human-written data by native speakers. To ensure topic coverage, we provide predefined topics and example seeds for both LLM-assisted generation and human writing.


%Inspired on these methods, we refine LLM-generated data creation by leveraging six LLMs—three open-weight and three closed-weight—to ensure greater variability.

\paragraph{English Commonsense Reasoning} 
% [FJ]: This need to be added beyond StoryClozeTest. At least two paragraphs. Try to discuss what are the data, what are the task. You can elaborate datasets listed in Paragraph-1 (Introduction) -- complete
Story comprehension has long been a significant challenge in NLP due to the complex interplay of causal and temporal relationships within narratives. A landmark contribution to the evaluation of these aspects is the \texttt{Story Cloze} test, introduced by \citet{mostafazadeh-etal-2016-corpus, mostafazadeh-etal-2017-lsdsem}. Another set of benchmarks for assessing commonsense reasoning includes \texttt{WinoGrande} \cite{sakaguchi2021winogrande}, COPA \citep{gordon-etal-2012-semeval}, and \texttt{HellaSwag} \citep{zellers-etal-2019-hellaswag}. 

WinoGrande \citep{sakaguchi2021winogrande} is designed to measure the resolution of pronouns in ambiguous contexts. The Choice of Plausible Alternatives (\texttt{COPA}) \citep{gordon-etal-2012-semeval} evaluates causal reasoning by presenting two possible continuations of a given premise, requiring models to infer the most plausible outcome. \texttt{HellaSwag} \citep{zellers-etal-2019-hellaswag} builds on the \texttt{SWAG} \cite{zellers-etal-2018-swag} dataset by increasing difficulty through adversarial filtering, which ensures that the correct continuation of a given context is not easily identified by simple statistical cues, but rather requires deep reasoning. These benchmarks collectively contribute to a more comprehensive evaluation of commonsense understanding in NLP models.


\paragraph{Commonsense Reasoning in Languages Beyond English}
% [FJ]: First paragraph: Start first by talking about the multilingual benchmark, You can include XCOPA, XStoryCloze. Tell that they are based on translation, and its drawback (e.g., less cultural relevant?).  D they have sundanese and javanese in multilingual benchmarks? -- complete

Evaluating commonsense reasoning across multiple languages has gained attention through multilingual benchmarks such as \texttt{XCOPA} \citep{ponti-etal-2020-xcopa} and \texttt{XStoryCloze} \citep{lin-etal-2022-shot}. These benchmarks extend existing commonsense reasoning datasets to a wider set of languages by translating English corpora into various target languages. However, such translation-based approaches often fail to capture cultural nuances and may introduce artifacts that affect naturalness and contextual relevance. Additionally, neither \texttt{XCOPA} nor \texttt{XStoryCloze} include benchmarks for Sundanese or Javanese, leaving a gap in evaluating commonsense reasoning in these widely spoken Indonesian local languages.

% [FJ]: Second par: And then talk about Indonesian datasets such as StoryCloze, Copal-ID,  IndoCulture. Acknowledge that they are high-quality because human-written, but Indonesian is the medium-resource language; Say that there is also IndoMMLU, with MCQ student style. 
%Say that we focus here more on low-resource languages, and we use IndoCloze as seed to guide the LLM, We also follow dataset creation by manual writing. Tell the difference between ours and previous dataset! We are story comprehension, not MCQ, not pair of sentences. We explore various dataset creation methods: LLM, MT, and Human-written.

% USE THIS CURRENT PARAGRAPHS BELOW AS PART OF THE CONTENT
% Recognizing the need for cross-linguistic research, \citet{koto-etal-2022-cloze} extended the Story Cloze framework to Indonesia, developing IndoCloze. The scarcity of annotated datasets for low-resource languages has driven researchers to explore synthetic data generation techniques. 


Several high-quality Indonesian commonsense reasoning datasets have been developed, primarily through human annotation, including \texttt{IndoCloze} \cite{koto-etal-2022-cloze} for narrative understanding, \texttt{COPAL-ID} \cite{wibowo-etal-2024-copal} integrating Indonesian cultural knowledge (i.e., Jakarta region), and \texttt{IndoCulture} \cite{koto-etal-2024-indoculture} for cultural reasoning. While these resources support commonsense reasoning in Indonesian, a medium-resource language, extensive datasets for local languages remain scarce. In contrast, our work focuses on low-resource languages, specifically Javanese and Sundanese, two of Indonesia’s most widely spoken local languages. We build on \texttt{IndoCloze} as a reference for synthetic data generation and complement it with manually written stories. Unlike prior datasets that rely on MCQs or sentence-pair tasks, we emphasize full story comprehension. To ensure diversity, we adopt multiple data creation approaches, including LLM-generated, machine-translated, and human-written narratives, enabling a broader and more culturally rich evaluation setting.

%WILL BE REUSED AGAIN
%Several high-quality Indonesian commonsense reasoning datasets have been developed, primarily through human annotation. \citet{koto-etal-2022-cloze} introduced \texttt{IndoCloze}, an extension of the \texttt{Story Cloze} framework for Indonesian, focusing on narrative understanding. \citet{wibowo2024copalidindonesianlanguagereasoning} developed \texttt{COPAL-ID}, a dataset that integrates Indonesian cultural and commonsense knowledge, covering both standard and Jakartan Indonesian. Additionally, \texttt{IndoCulture} \citep{koto-etal-2024-indoculture} was constructed from scratch to capture cultural reasoning in Indonesian. While these resources contribute to commonsense reasoning in Indonesian (i.e., the national language), the language remains medium-resource, and there is still a lack of extensive datasets for its local languages.

%WILL BE REUSED AGAIN
%In contrast, our work shifts the focus to low-resource languages, specifically Javanese and Sundanese, two of the most widely spoken local languages in Indonesia. We take \texttt{IndoCloze} as a foundational reference to guide synthetic data generation and complement it with manually written stories. Unlike previous datasets that rely on MCQs or sentence-pair tasks, we emphasize full story comprehension. To diversify our dataset, we explore multiple data creation methods, including LLM-generated, machine-translated, and human-written narratives, ensuring a broader and more culturally rich evaluation setting.

% \begin{table*}
% \centering
% \begin{tabularx}{\linewidth}{lXXX}
% \hline
% & \textbf{English} \citep{mostafazadeh-etal-2016-story} & \textbf{Javanese} & \textbf{Sundanese} \\
% \hline
% Premise & Karen was assigned a roommate during her first year of college. Her roommate asked her to go to a nearby city for a concert. Karen agreed happily. The show was absolutely exhilarating. & \textit{Ing desa, sadurunge mantenan, biasane kulawarga manten nganakake kenduri. Warga lan sedulur-sedulur kumpul kanggo ndedonga bebarengan. Panganan khas desa disajikake kanggo wong-wong sing teka. Sakwise kenduri, keluarga ngaturaken ater-ater paring warga sing teka.} & \textit{Kabayan resep cicing di walungan bari niup suling. Barudak sok milu ulin di saung bari ngadengekeun sora suling. Kadang Kabayan oge sok ngajarkeun barudak supaya bisa maenkeun suling. Kabayan resep mun melong barudak anu kataji kana kasenian sunda.} \\
% Correct Ending & Karen became good friends with her roommate. & \textit{Sakwise kenduri, kabeh kulawarga lan warga desa rumangsa seneng amarga bisa bebarengan ndedonga kanggo manten.} & \textit{Kabayan hayang kasenian sunda dimumule ku budak ngora} \\
% Incorrect Ending & Karen hated her roommate. & \textit{Sakwise kenduri, manten lan kulawargane lunga adoh lan ora bali maneh.} & \textit{Kabayan teu resep mun aya nu bisa maen suling lian ti manéhna.} \\
% \hline
% \end{tabularx}
% \caption{Story Cloze Sample Data in English, Javanese, Sundanese}
% \label{tab:sample_data}
% \end{table*}


% \begin{table*}
% \centering
% \begin{tabularx}{\linewidth}{lXX}
% \hline
% & \textbf{Javanese} & \textbf{Sundanese} \\
% \hline
% Original & \textbf{Premise:} \textit{Ing desa, sadurunge mantenan, biasane kulawarga manten nganakake kenduri. Warga lan sedulur-sedulur kumpul kanggo ndedonga bebarengan. Panganan khas desa disajikake kanggo wong-wong sing teka. Sakwise kenduri, keluarga ngaturaken ater-ater paring warga sing teka.} 
% \textbf{Correct:} \textit{Sakwise kenduri, kabeh kulawarga lan warga desa rumangsa seneng amarga bisa bebarengan ndedonga kanggo manten.} 
% \textbf{Incorrect:} \textit{Sakwise kenduri, manten lan kulawargane lunga adoh lan ora bali maneh.}
% & \textbf{Premise:} \textit{Kabayan resep cicing di walungan bari niup suling. Barudak sok milu ulin di saung bari ngadengekeun sora suling. Kadang Kabayan oge sok ngajarkeun barudak supaya bisa maenkeun suling. Kabayan resep mun melong barudak anu kataji kana kasenian sunda.}  
% \textbf{Correct:} \textit{Kabayan hayang kasenian sunda dimumule ku budak ngora.}
% \textbf{Incorrect:} \textit{Kabayan teu resep mun aya nu bisa maen suling lian ti manéhna.} \\
% Translated & \textbf{Premise:} In the village, before the wedding, the bride and groom's family usually hold a feast. The villagers and relatives gather to pray together. Typical village food is served to those who come. After the feast, the family greets the villagers. \textbf{Correct:} After the feast, the whole family and villagers feel happy because they can pray together for the wedding. \textbf{Incorrect:} After the feast, the bride and groom and their families go far away and never return & \textbf{Premise:} Kabayan likes to live by the river while playing the flute. The children often join in playing in the hut while listening to the sound of the flute. Sometimes Kabayan also teaches children to play the flute. Kabayan likes to see children who are interested in Sundanese art. \textbf{Correct:} Kabayan wants Sundanese art to be nurtured by young children. \textbf{Incorrect:} Kabayan doesn't like it when anyone else can play the flute other than him.
%  \\
% \hline
% \end{tabularx}
% \caption{Story Cloze Sample Data in English, Javanese, Sundanese}
% \label{tab:sample_data}
% \end{table*}