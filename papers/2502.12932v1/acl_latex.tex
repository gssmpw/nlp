\pdfoutput=1
\documentclass[11pt]{article}
\usepackage[final]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{amsmath} 
\usepackage{booktabs} 
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{diagbox}
\usepackage{array} 
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{microtype}
\usepackage{float}
% \usepackage[table]{xcolor}
\usepackage{colortbl} % For table colors
\definecolor{promptgreen}{HTML}{13765a}
\definecolor{promptred}{HTML}{c04d5a}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}



\pagestyle{plain}
% Remove the "review" option to generate the final version.
% \usepackage[review]{ACL2023}

\title{Synthetic Data Generation for Culturally Nuanced Commonsense Reasoning in Low-Resource Languages}

%\title{JavaCloze \& SundaCloze: Utilizing Synthetic Story Cloze Data for Commonsense Story Reasoning in Indonesian Local Languages}

\author{
    Salsabila Zahirah Pranida\thanks{Equal contribution} \quad Rifo Ahmad Genadi\footnotemark[1] \quad Fajri Koto \\
    Department of Natural Language Processing, MBZUAI \\
    \texttt{\{salsabila.pranida,rifo.genadi,fajri.koto\}@mbzuai.ac.ae}\\
}

\begin{document}
\maketitle
\begin{abstract}

Quantifying reasoning capability in low-resource languages remains a challenge in NLP due to data scarcity and limited access to annotators. While LLM-assisted dataset construction has proven useful for medium- and high-resource languages, its effectiveness in low-resource languages, particularly for commonsense reasoning, is still unclear. In this paper, we compare three dataset creation strategies: (1) LLM-assisted dataset generation, (2) machine translation, and (3) human-written data by native speakers, to build a culturally nuanced story comprehension dataset. We focus on Javanese and Sundanese, two major local languages in Indonesia, and evaluate the effectiveness of open-weight and closed-weight LLMs in assisting dataset creation through extensive manual validation. To assess the utility of synthetic data, we fine-tune language models on classification and generation tasks using this data and evaluate performance on a human-written test set. Our findings indicate that LLM-assisted data creation outperforms machine translation.

%Story comprehension, involving complex causal and temporal relationships, presents a significant challenge in NLP. However, most of the research has focused on English, leaving questions about how these insights transfer to low-resource languages like Javanese and Sundanese. Datasets for such languages are also scarce due to the cost and difficulty of manual data collection and annotation. In this paper, we build on the Story Cloze Test framework from \citet{mostafazadeh-etal-2016-corpus} and \citet{koto-etal-2022-cloze}, applying \citet{liu-etal-2022-wanli}'s synthetic dataset creation approach with the assistance of LLMs. Our interim results show that some LLM-generated augmented datasets have sufficiently good quality and can boost machine learning models' performance on the Story Cloze Evaluation Dataset, while some do not; this shows the capability limitations of current LLMs and the difficulty of the task on low-resource and underrepresented language.
\end{abstract}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.85\linewidth, scale=0.5]{images/local_cloze.png} 
    \caption{Proposed pipeline, including the train set overgeneration process}
    \label{local_cloze_pipeline}
\end{figure*}


\section{Introduction}

Reasoning—the ability to draw conclusions, make inferences, and understand relationships between concepts based on available information—is a key capability assessed in recent developments of large language models \citep{dubey2024llama, openai2024gpt4technicalreport, openai2024gpt4ocard, almazrouei2023falconseriesopenlanguage}. It has been widely studied in English through various benchmarks, including \texttt{StoryCloze} \cite{mostafazadeh-etal-2016-corpus, mostafazadeh-etal-2017-lsdsem}, \texttt{WinoGrande} \cite{sakaguchi2021winogrande}, and \texttt{HellaSwag} \cite{zellers-etal-2019-hellaswag}, which evaluate different aspects of reasoning such as story comprehension, pronoun resolution, and natural language inference. Beyond English, research on reasoning has largely relied on machine translation \cite{ponti-etal-2020-xcopa,lin-etal-2022-shot}, often overlooking the cultural context of the target languages. While some datasets have been developed from scratch for medium-resource languages like Indonesian \cite{koto-etal-2024-indoculture}, reasoning in low-resource languages remains significantly underexplored. The primary challenges include the high cost of annotation, difficulties in recruiting native speakers, and the limited availability of culturally relevant text.

In this paper, we ask: \textit{to what extent can large language models (LLMs) assist in creating reasoning datasets for low-resource languages? How do they compare to machine translation and building datasets from scratch?} While LLM-assisted dataset construction has been widely explored, most efforts have focused on English \cite{liu-etal-2022-wanli,liu-etal-2024-multilingual} and traditional NLP tasks such as named entity recognition \citep{mohit2014named} and question answering \cite{ghazaryan-etal-2025-syndarin,putri-etal-2024-llm}. However, their potential for generating reasoning datasets in low-resource languages remains under-studied, particularly for \textit{culturally nuanced story comprehension} tasks.

% Our focus and why 
Unlike sentence-pair classification tasks for assessing reasoning (e.g., the \texttt{COPA} dataset), we take a story comprehension approach to evaluate LLMs' ability to generate coherent narratives in Javanese and Sundanese over longer texts (five sentences). Specifically, we follow the \texttt{StoryCloze} framework \cite{mostafazadeh-etal-2016-corpus, mostafazadeh-etal-2017-lsdsem} to assess commonsense reasoning in daily life scenarios, where a four-sentence premise is provided along with two possible options for the concluding sentence (see Figure~\ref{tab:sample_data}).

We focus on Javanese and Sundanese because, despite having over 80 million and 32 million speakers, respectively \cite{aji-etal-2022-one}, these languages remain underrepresented in language technology development. Beyond their linguistic diversity, both languages are deeply intertwined with distinct cultural traditions that influence their usage. Sundanese, for instance, conveys social nuanced through varying degrees of politeness and phonetic distinctions, while Javanese employs a structuted system of speech levels to reflect hierarchical relationships in communication \citep{wolff1982communicative}. 


Our contribution can be summarized as follows:
\begin{itemize}
    \item We benchmark three open-weight and three closed-weight LLMs for StoryCloze-style synthetic data generation in Javanese and Sundanese through extensive human evaluation.
    \item We introduce the first commonsense reasoning datasets for Javanese and Sundanese. For training data, we use two strategies: (1) LLM-generated data with few-shot prompting and (2) machine translation. For the test set, we employ two approaches: (1) machine translation with post-editing and (2) manually written samples by native speakers based on predefined topics, with extensive validation to ensure quality. In total, we release 1.22K high-quality LLM-generated samples, 1K human-reviewed machine-translated texts, and 1.12K human-written samples\footnote{Our data are available at \url{https://huggingface.co/datasets/rifoag/javanese_sundanese_story_cloze}}.
    \item We conducted extensive experiments in both classification and generation settings to assess the utility of LLM-generated data and machine translation.
\end{itemize}

%For example, Sundanese has multiple politeness levels, expressed through terms such as \textit{Lemes} (polite), \textit{Loma} (friendly), and \textit{Kasar} (rude). It also features three distinct ``e'' sounds: e, é, and eu. Similarly, Javanese employs different speech levels, including \textit{Ngoko} (informal), \textit{Madya} (intermediate), and \textit{Krama} (formal and polite), each used in specific social contexts \citep{wolff1982communicative}. 

%Both languages also incorporate unique cultural references, including traditions related to breakfast, weddings, and childbirth. For instance, \textit{sawéran} is a customary practice in Sundanese and Javanese wedding receptions, where money and gifts are tossed to guests as a symbol of blessings and prosperity.

%commonsense reasoning is crucial for natural language understanding (NLU). Yet, machines often struggle to make accurate conclusions due to the need for implicit information and world knowledge, such as social conventions and common logic. Story Cloze Test can be used to evaluates commonsense reasoning through short English stories. \citet{koto-etal-2022-cloze} extended this by developing the first Indonesian version. The same test could also be done in Indonesian regional languages; however, one of the main challenges is the lack of robust corpora, while fully manual corpus creation is costly. A possible way to address them is by using large language models (LLMs) to automatically generate the dataset. Ideally, the generated data should be linguistically correct and faithful to the related cultural aspects. However, it is not clear to what extent current LLMs can generate a plausible story that includes cultural nuances to these languages.

%Motivated by previous work in IndoCloze, we explore commonsense reasoning in Javanese and Sundanese, two of the most spoken local languages in Indonesia \citep{aji-etal-2022-one, brown2005encyclopedia, silitonga2018teaching}. Both local languages share some similarities with Indonesian, but it does have unique characteristics that differ from Bahasa Indonesia; for example, Sundanese has various ways of expressing politeness, with terms such as \textit{Lemes} (polite), \textit{Loma} (friendly), and \textit{Kasar} (rude). It features three different "e" letters pronounced as e, é, and eu. Meanwhile, Javanese also has different levels of speech, such as \textit{Ngoko} (informal), \textit{Madya} (intermediate), and \textit{Krama} (formal and polite), each used in specific social contexts \citep{wolff1982communicative}.  Both languages also have unique cultural references, including traditions related to breakfast, weddings, and childbirth, as well as sociocultural terms such as \textit{sawéran}; a tradition in Sundanese and Javanese wedding receptions.


%We are curious to know how good LLM-generated data compared to human-written data in commonsense story generation. This work aims to investigate how well current LLMs can generate plausible stories that are both linguistically accurate to Indonesian local languages (Javanese and Sundanese), while also culturally relevant. We also constructed a human-generated evaluation dataset to ensure a reliable story cloze evaluation on those languages. Our contributions are summarized as follows:
%\begin{enumerate}
%    \item We assess the quality of synthetic training data for Javanese and Sundanese Story Cloze task generated by three open-weight LLMs and closed-weight LLMs.
%    \item We construct a high-quality Javanese and Sundanese Story Cloze Evaluation Dataset, developed by respective native speaker of the language with rigorous quality control. The dataset contains $\approx$1,000 human generated story cloze dataset per language.
%    \item We performed a prelim    inary analysis of Javanese and Sundanese story cloze as a classification and generation task in cross-lingual settings.
%\end{enumerate}


\begin{table*}[ht]
\centering
\resizebox{0.9\textwidth}{!}{
\begin{tabularx}{1.4\linewidth}{XX}
\toprule
\textbf{Javanese} & \textbf{Sundanese} \\
\midrule
\multicolumn{2}{l}{\cellcolor{blue!7} Original} \\
\textbf{Premise:} \textit{Ing Yogyakarta, gudeg dadi panganan khas. Gudeg dimasak suwene jam-jaman nganggo nangka enom lan santan. Biasane disajikake karo krecek, endog, lan ayam suwir. Wong Yogyakarta biasane mangan gudeg ing esuk utawa nalika ana acara khusus.} 
\textbf{Correct:} \textit{Gudeg diwarisake turun-temurun lan dadi bagian saka budaya kuliner Yogyakarta.} 
\textbf{Incorrect:} \textit{Gudeg biasane digoreng nganggo lenga panas lan disajikake pedhes.}
& \textbf{Premise:} \textit{Mang Tarya indit ka sawah keur ngarambét. Geus loba jukut nu mucunghul ngaganggu paré. ngarambét ngabaladah biasana dilaksanakeun tilu puluh poé saprak tandur. Mun ngarambét mindo dilaksanakeun lima puluh poe saprak tandur.}  
\textbf{Correct:} \textit{Ngarambét dilaksanakeun pikeun ngaberesihan jukut di petakan sawah.}
\textbf{Incorrect:} \textit{Ngarambét dilaksanakeun saméméh panén.} \\

\midrule
\multicolumn{2}{l}{\cellcolor{blue!7} English translation} \\
\textbf{Premise:} In Yogyakarta, \textit{gudeg} is a traditional dish. It is cooked for hours using young jackfruit and coconut milk. It is usually served with \textit{krecek}, eggs, and shredded chicken. People in Yogyakarta typically eat \textit{gudeg} in the morning or during special occasions. \textbf{Correct:} Gudeg is passed down through generations and has become a part of Yogyakarta's culinary heritage. \textbf{Incorrect:}  Gudeg is usually deep-fried in hot oil and served spicy. & \textbf{Premise:} Mang Tarya went to the rice field to do \textit{ngarambét}. There are already lots of wild grass that may harm the rice crops. \textit{Ngarambét} ngabaladah is done thirty days after tandur. Meanwhile, \textit{Ngarambét} mindo is done fifty days after tandur. \textbf{Correct:} \textit{Ngarambét} is done to clean up wild grass in the rice fields. \textbf{Incorrect:} \textit{Ngarambét} is done before harvest.
 \\
\bottomrule
\end{tabularx}
}
\caption{Examples of \texttt{story cloze} in Javanese and Sundanese, with English translations for illustrative purposes.}
\label{tab:sample_data}
\end{table*}

\section{Related Works}

% [FJ]: This needs to be elaborated further to become three paragraphs. Explain what tasks, methods that these citations do. How is the results -- complete
\paragraph{LLM-Generated Data Creation} One possible solution to tackle data scarcity in NLP is applying data augmentation \citep{feng-etal-2021-survey, ding-etal-2020-daga, ahmed-buys-2024-neural, liu2024best, yong-etal-2024-lexc, guo2024generativeaisyntheticdata, liu-etal-2022-wanli}. These methods leverage the generative capabilities of LLMs to produce high-quality synthetic data that can sometimes complement or even substitute manual dataset creation.

% Synthetic data has become an increasingly valuable solution to address issues like data scarcity, privacy concerns, and the high costs of manual dataset creation. In this direction, \citet{liu2024bestpracticeslessonslearned} discuss best practices for ensuring that synthetic data remains diverse, factually accurate, and high in fidelity, ensuring that it can effectively support the development of more inclusive and reliable language models. However, despite its potential, synthetic data comes with limitations. \citet{guo2024generativeaisyntheticdata} highlight key concerns, particularly regarding correctness and diversity. While LLMs can generate vast amounts of data, ensuring factual accuracy remains a challenge, as models may introduce hallucinations or inconsistencies. Additionally, maintaining diversity is essential to prevent biases and improve generalization, requiring careful curation and filtering to ensure that datasets remain balanced and representative.

% \citet{liu2024bestpracticeslessonslearned} highlight the importance of maintaining diversity, factual accuracy, and fidelity in synthetic data to ensure it is useful for real-world applications. While synthetic data offers a scalable alternative to manual dataset creation, \citet{guo2024generativeaisyntheticdata} point out key limitations, particularly regarding correctness and diversity. LLMs can generate vast amounts of data, but they may introduce hallucinations or biases, making careful curation and filtering essential to maintain quality and representativeness.

% In another research, \citet{kaddour2024syntheticdatagenerationlowresource} explore how fine-tuning a LLM can be used to generate high-quality synthetic data, improving the performance of smaller student models. Their approach, tested on four text classification and two text generation tasks, shows that even with limited initial training data, synthetic examples—whether through annotating unlabeled data or generating new input-output pairs—can significantly boost model performance. 

% LexC-Gen, proposed by \citet{yong2024lexcgen}, leverages LLMs for generating classification task data in extremely low-resource Indonesian languages. By employing a lexicon-conditioned approach, it mitigates data-lexicon mismatches through controlled text generation and word-to-word translation using bilingual lexicons, leading to competitive performance with expert-translated datasets in sentiment analysis and topic classification.

% WANLI, introduced by \citet{liu-etal-2022-wanli}, uses a hybrid approach where synthetic data is first overgenerated by GPT-3 and then filtered or revised by human experts to ensure high quality. This process results in a more diverse and reliable dataset, improving model generalization across different domains. Inspired by this approach, we adapt and modify it to better fit the needs of our research.

% Several studies have proposed different techniques to improve the effectiveness of synthetic data. \citet{kaddour2024syntheticdatagenerationlowresource} show that fine-tuning an LLM can generate high-quality data for downstream models, significantly improving their performance across text classification and text generation tasks. In low-resource settings, \citet{yong2024lexcgen} introduce LexC-Gen, a lexicon-conditioned method for generating classification data in underrepresented Indonesian languages, helping mitigate mismatches between synthetic and real data. Another hybrid approach is seen in WANLI \citep{liu-etal-2022-wanli}, where synthetic data is overgenerated by GPT-3 and then filtered or revised by human annotators, resulting in a more diverse and reliable dataset. %Inspired by these methods, we adapt and refine LLM-generated synthetic data techniques to better fit the needs of our research.

% Inspired by these methods, we refine LLM-generated synthetic data techniques by utilizing more LLMs—three open-weight and three closed-weight—ensuring greater variability. Unlike WANLI, which relies on a single LLM, our approach combines LLM-generated, machine-translated, and human-written samples. We further guide LLMs to generate content based on predefined topics while embedding cultural nuances relevant to Javanese and Sundanese, creating more contextually rich and diverse narratives.

%\citet{kaddour2024syntheticdatagenerationlowresource} demonstrate that fine-tuning LLMs can significantly improve downstream model performance in text classification and generation. 
%\citet{yong2024lexcgen} introduce LexC-Gen, a lexicon-conditioned approach for generating classification data in underrepresented Indonesian languages,

Prior studies have explored techniques for enhancing synthetic data generation, primarily for classification tasks. WANLI \citep{liu-etal-2022-wanli} used GPT-3 \citep{brown2020language} to generate synthetic natural language inference data in English \cite{bowman-etal-2015-large}, followed by human refinement. \citet{yong-etal-2024-lexc} leveraged LLMs to generate synthetic English data for sentiment analysis and topic classification, then applied a bilingual lexicon for word-to-word translation into low-resource languages. 
% RIFKI >> in contemporernous works, one low-ersource untuk QA tapi tidak benchmarking.
In contemporaneous work, \citet{putri-etal-2024-llm} utilized GPT-4 \cite{openai2024gpt4technicalreport} for question-answering tasks in one low-resource language. Unlike these works, we focus specifically on reasoning tasks in two low-resource languages and compare multiple strategies, including LLM-assisted generation using both open-weight and closed-weight models, machine translation, and human-written data by native speakers. To ensure topic coverage, we provide predefined topics and example seeds for both LLM-assisted generation and human writing.


%Inspired on these methods, we refine LLM-generated data creation by leveraging six LLMs—three open-weight and three closed-weight—to ensure greater variability.

\paragraph{English Commonsense Reasoning} 
% [FJ]: This need to be added beyond StoryClozeTest. At least two paragraphs. Try to discuss what are the data, what are the task. You can elaborate datasets listed in Paragraph-1 (Introduction) -- complete
Story comprehension has long been a significant challenge in NLP due to the complex interplay of causal and temporal relationships within narratives. A landmark contribution to the evaluation of these aspects is the \texttt{Story Cloze} test, introduced by \citet{mostafazadeh-etal-2016-corpus, mostafazadeh-etal-2017-lsdsem}. Another set of benchmarks for assessing commonsense reasoning includes \texttt{WinoGrande} \cite{sakaguchi2021winogrande}, COPA \citep{gordon-etal-2012-semeval}, and \texttt{HellaSwag} \citep{zellers-etal-2019-hellaswag}. 

WinoGrande \citep{sakaguchi2021winogrande} is designed to measure the resolution of pronouns in ambiguous contexts. The Choice of Plausible Alternatives (\texttt{COPA}) \citep{gordon-etal-2012-semeval} evaluates causal reasoning by presenting two possible continuations of a given premise, requiring models to infer the most plausible outcome. \texttt{HellaSwag} \citep{zellers-etal-2019-hellaswag} builds on the \texttt{SWAG} \cite{zellers-etal-2018-swag} dataset by increasing difficulty through adversarial filtering, which ensures that the correct continuation of a given context is not easily identified by simple statistical cues, but rather requires deep reasoning. These benchmarks collectively contribute to a more comprehensive evaluation of commonsense understanding in NLP models.


\paragraph{Commonsense Reasoning in Languages Beyond English}
% [FJ]: First paragraph: Start first by talking about the multilingual benchmark, You can include XCOPA, XStoryCloze. Tell that they are based on translation, and its drawback (e.g., less cultural relevant?).  D they have sundanese and javanese in multilingual benchmarks? -- complete

Evaluating commonsense reasoning across multiple languages has gained attention through multilingual benchmarks such as \texttt{XCOPA} \citep{ponti-etal-2020-xcopa} and \texttt{XStoryCloze} \citep{lin-etal-2022-shot}. These benchmarks extend existing commonsense reasoning datasets to a wider set of languages by translating English corpora into various target languages. However, such translation-based approaches often fail to capture cultural nuances and may introduce artifacts that affect naturalness and contextual relevance. Additionally, neither \texttt{XCOPA} nor \texttt{XStoryCloze} include benchmarks for Sundanese or Javanese, leaving a gap in evaluating commonsense reasoning in these widely spoken Indonesian local languages.

% [FJ]: Second par: And then talk about Indonesian datasets such as StoryCloze, Copal-ID,  IndoCulture. Acknowledge that they are high-quality because human-written, but Indonesian is the medium-resource language; Say that there is also IndoMMLU, with MCQ student style. 
%Say that we focus here more on low-resource languages, and we use IndoCloze as seed to guide the LLM, We also follow dataset creation by manual writing. Tell the difference between ours and previous dataset! We are story comprehension, not MCQ, not pair of sentences. We explore various dataset creation methods: LLM, MT, and Human-written.

% USE THIS CURRENT PARAGRAPHS BELOW AS PART OF THE CONTENT
% Recognizing the need for cross-linguistic research, \citet{koto-etal-2022-cloze} extended the Story Cloze framework to Indonesia, developing IndoCloze. The scarcity of annotated datasets for low-resource languages has driven researchers to explore synthetic data generation techniques. 


Several high-quality Indonesian commonsense reasoning datasets have been developed, primarily through human annotation, including \texttt{IndoCloze} \cite{koto-etal-2022-cloze} for narrative understanding, \texttt{COPAL-ID} \cite{wibowo-etal-2024-copal} integrating Indonesian cultural knowledge (i.e., Jakarta region), and \texttt{IndoCulture} \cite{koto-etal-2024-indoculture} for cultural reasoning. While these resources support commonsense reasoning in Indonesian, a medium-resource language, extensive datasets for local languages remain scarce. In contrast, our work focuses on low-resource languages, specifically Javanese and Sundanese, two of Indonesia’s most widely spoken local languages. We build on \texttt{IndoCloze} as a reference for synthetic data generation and complement it with manually written stories. Unlike prior datasets that rely on MCQs or sentence-pair tasks, we emphasize full story comprehension. To ensure diversity, we adopt multiple data creation approaches, including LLM-generated, machine-translated, and human-written narratives, enabling a broader and more culturally rich evaluation setting.

%WILL BE REUSED AGAIN
%Several high-quality Indonesian commonsense reasoning datasets have been developed, primarily through human annotation. \citet{koto-etal-2022-cloze} introduced \texttt{IndoCloze}, an extension of the \texttt{Story Cloze} framework for Indonesian, focusing on narrative understanding. \citet{wibowo2024copalidindonesianlanguagereasoning} developed \texttt{COPAL-ID}, a dataset that integrates Indonesian cultural and commonsense knowledge, covering both standard and Jakartan Indonesian. Additionally, \texttt{IndoCulture} \citep{koto-etal-2024-indoculture} was constructed from scratch to capture cultural reasoning in Indonesian. While these resources contribute to commonsense reasoning in Indonesian (i.e., the national language), the language remains medium-resource, and there is still a lack of extensive datasets for its local languages.

%WILL BE REUSED AGAIN
%In contrast, our work shifts the focus to low-resource languages, specifically Javanese and Sundanese, two of the most widely spoken local languages in Indonesia. We take \texttt{IndoCloze} as a foundational reference to guide synthetic data generation and complement it with manually written stories. Unlike previous datasets that rely on MCQs or sentence-pair tasks, we emphasize full story comprehension. To diversify our dataset, we explore multiple data creation methods, including LLM-generated, machine-translated, and human-written narratives, ensuring a broader and more culturally rich evaluation setting.

% \begin{table*}
% \centering
% \begin{tabularx}{\linewidth}{lXXX}
% \hline
% & \textbf{English} \citep{mostafazadeh-etal-2016-story} & \textbf{Javanese} & \textbf{Sundanese} \\
% \hline
% Premise & Karen was assigned a roommate during her first year of college. Her roommate asked her to go to a nearby city for a concert. Karen agreed happily. The show was absolutely exhilarating. & \textit{Ing desa, sadurunge mantenan, biasane kulawarga manten nganakake kenduri. Warga lan sedulur-sedulur kumpul kanggo ndedonga bebarengan. Panganan khas desa disajikake kanggo wong-wong sing teka. Sakwise kenduri, keluarga ngaturaken ater-ater paring warga sing teka.} & \textit{Kabayan resep cicing di walungan bari niup suling. Barudak sok milu ulin di saung bari ngadengekeun sora suling. Kadang Kabayan oge sok ngajarkeun barudak supaya bisa maenkeun suling. Kabayan resep mun melong barudak anu kataji kana kasenian sunda.} \\
% Correct Ending & Karen became good friends with her roommate. & \textit{Sakwise kenduri, kabeh kulawarga lan warga desa rumangsa seneng amarga bisa bebarengan ndedonga kanggo manten.} & \textit{Kabayan hayang kasenian sunda dimumule ku budak ngora} \\
% Incorrect Ending & Karen hated her roommate. & \textit{Sakwise kenduri, manten lan kulawargane lunga adoh lan ora bali maneh.} & \textit{Kabayan teu resep mun aya nu bisa maen suling lian ti manéhna.} \\
% \hline
% \end{tabularx}
% \caption{Story Cloze Sample Data in English, Javanese, Sundanese}
% \label{tab:sample_data}
% \end{table*}


% \begin{table*}
% \centering
% \begin{tabularx}{\linewidth}{lXX}
% \hline
% & \textbf{Javanese} & \textbf{Sundanese} \\
% \hline
% Original & \textbf{Premise:} \textit{Ing desa, sadurunge mantenan, biasane kulawarga manten nganakake kenduri. Warga lan sedulur-sedulur kumpul kanggo ndedonga bebarengan. Panganan khas desa disajikake kanggo wong-wong sing teka. Sakwise kenduri, keluarga ngaturaken ater-ater paring warga sing teka.} 
% \textbf{Correct:} \textit{Sakwise kenduri, kabeh kulawarga lan warga desa rumangsa seneng amarga bisa bebarengan ndedonga kanggo manten.} 
% \textbf{Incorrect:} \textit{Sakwise kenduri, manten lan kulawargane lunga adoh lan ora bali maneh.}
% & \textbf{Premise:} \textit{Kabayan resep cicing di walungan bari niup suling. Barudak sok milu ulin di saung bari ngadengekeun sora suling. Kadang Kabayan oge sok ngajarkeun barudak supaya bisa maenkeun suling. Kabayan resep mun melong barudak anu kataji kana kasenian sunda.}  
% \textbf{Correct:} \textit{Kabayan hayang kasenian sunda dimumule ku budak ngora.}
% \textbf{Incorrect:} \textit{Kabayan teu resep mun aya nu bisa maen suling lian ti manéhna.} \\
% Translated & \textbf{Premise:} In the village, before the wedding, the bride and groom's family usually hold a feast. The villagers and relatives gather to pray together. Typical village food is served to those who come. After the feast, the family greets the villagers. \textbf{Correct:} After the feast, the whole family and villagers feel happy because they can pray together for the wedding. \textbf{Incorrect:} After the feast, the bride and groom and their families go far away and never return & \textbf{Premise:} Kabayan likes to live by the river while playing the flute. The children often join in playing in the hut while listening to the sound of the flute. Sometimes Kabayan also teaches children to play the flute. Kabayan likes to see children who are interested in Sundanese art. \textbf{Correct:} Kabayan wants Sundanese art to be nurtured by young children. \textbf{Incorrect:} Kabayan doesn't like it when anyone else can play the flute other than him.
%  \\
% \hline
% \end{tabularx}
% \caption{Story Cloze Sample Data in English, Javanese, Sundanese}
% \label{tab:sample_data}
% \end{table*}




\section{Dataset Construction}
\label{sect:dataset_construction}
% TODO: add data distribution 

As shown in Figure~\ref{local_cloze_pipeline}, our dataset construction is describued in two streams: training and testing—using \texttt{IndoCloze} \cite{koto-etal-2022-cloze} as an initial source. \texttt{IndoCloze} is the only story comprehension dataset for commonsense reasoning evaluation in Indonesian and incorporates Indonesian cultural elements. Each sample consists of a four-sentence premise followed by two candidate endings: one correct and one incorrect, representing the fifth sentence.

We use Indonesian dataset in our data construction as it is the national language and shares cultural elements with Javanese and Sundanese, making it a relevant source of seed data. In addition to leveraging \texttt{IndoCloze}, we also create a dataset from scratch by asking native speakers to write and validate samples to ensure cultural and linguistic authenticity. Similar to \citet{mostafazadeh-etal-2016-corpus} and \citet{koto-etal-2022-cloze}, our dataset focuses on everyday events within Javanese and Sundanese cultural contexts, incorporating relevant locations, names, foods, and cultural practices.
%The process of constructing the dataset is detailed in §\ref{subsect:llm_generated_data_construction} for training set and §\ref{subsect:eval_set_creation} for the evaluation set.

%For training, we use the training set of \texttt{IndoCloze} \cite{koto-etal-2022-cloze} as the initial source. \texttt{IndoCloze} is the only story comprehension dataset for commonsense reasoning evaluation in Indonesian and incorporates Indonesian cultural elements. To generate the training data, we leverage LLMs and machine translation. For testing, we use human-written samples alongside machine-translated data from the test split of \texttt{IndoCloze}, which undergo manual review, correction, and localization to align with the respective cultural contexts. The following sections provide details on each of these processes.

\subsection{Training Data}  

Our training set is constructed using two strategies: (1) LLM-assisted data generation and (2) machine translation.

\paragraph{LLM-assisted Data Generation}
To assess the capability of different LLMs in generating meaningful training data, we create synthetic datasets using three open-weight models: \texttt{Mixtral-8x7B-Instruct} \citep{jiang2024mixtral}, \texttt{Gemma2-27B-it} \citep{team2024gemma}, and \texttt{Llama3.1-70B} \citep{dubey2024llama}—and three closed-weight models: \texttt{GPT-4o} \citep{openai2024gpt4ocard, openai2024gpt4technicalreport}, \texttt{Cohere Command-R-Plus} \citep{cohere2024commandrplus}, and \texttt{Claude-3-Opus} \citep{anthropic2024claude3}. 

For each language and LLM, we provide seed examples and predefined topics. To create seed samples, we manually translate 50 samples from \texttt{IndoCloze}'s training set into Javanese and Sundanese, ensuring cultural localization. For example, we adapt references to food, such as \textit{Gudeg} (a traditional breakfast from Yogyakarta), and traditions like \textit{Sawéran} (a ritual in Sundanese wedding ceremonies).

To ensure that the generated examples align with the intended cultural context, we use cultural topics derived from \texttt{IndoCulture} \citep{koto-etal-2024-indoculture}, covering areas such as food, weddings, family relationships, pregnancy and children, death, religious holidays, agriculture, fisheries and trade, art, traditional games, daily activities, and socio-religious aspects of life. The full prompt used in this study is provided in Figure~\ref{fig:prompt_template}. 

Each LLM independently generate 2,000 samples (1,000 per language), resulting in six distinct datasets, each treated as a separate training set. For each LLM call, we randomly select five seed samples and prompted the model to generate new examples. The temperature is set to 0.7 to introduce variability in the generated outputs. The generation process is repeated until each LLM produced at least 1,000 valid examples. On average, each model generated approximately 166 samples per topic (see Appendix~\ref{sec:topic_distribution} for topic distribution).

Since not all LLM-generated samples are high quality, we train an XLM-R binary classifier to distinguish between good and bad samples. To create training data, we ask workers to rate the quality of 600 LLM-generated outputs (details provided in Section~\ref{sec:quality_analysis}). These rated samples are then used to train the classifier.\footnote{The classifier achieves 68.33\% accuracy on the dev set (20\% of the train set), highlighting the complexity of the task. The experimental setup for the data filtering classifier is described in Appendix~\ref{sec:training_configurations}.} Once trained, we apply the classifier to all LLM-generated data. After filtering, the dataset is reduced from 12,000 generated samples to 592 Sundanese and 628 Javanese high-quality samples. About half of the data generated by Claude and GPT-4o are retained, while a large portion of samples from other LLMs, especially Mixtral, are filtered out. More details on the filtered data proportions can be found in Appendix~\ref{sec:filtered_data_proportion}.


%Ideally, we want to filter out low-quality LLM-generated training data, but it cannot be easily filtered using simple heuristics. To achieve this, we train a binary classifier that classifies whether the generated data are good quality or not. We ask the workers to rate the quality data generated by each LLM (the detailed process is specified in  Section~\ref{sec:quality_analysis}). We then used the rated samples as training data for the binary classifier. The experimental settings for the data filtering classifier are specified in the appendix~\ref{sec:training_configurations}. We also apply simple heuristics to remove some obvious cases of broken generated examples such as duplicated entries, excessively short samples, or repeated premises of stories.

%We used 600 rated samples of LLM-generated data to train the model, 20\% of which are used as a dev dataset. The classifier got 68.33\% accuracy in the dev set, indicates the complexity of the task. After we trained the model, we then applied it to all LLM-generated data, which is the concatenation of all the data generated by each LLM, denoted \( LLM\textsubscript{full} \). Before filtering, \( LLM\textsubscript{full} \) consist of 6,000 generated samples for each language, and then it reduced to 592 Sundanese samples and 628 Javanese samples. Around half of the data generated by Claude and GPT-4o are retained, while many generated samples from other LLMs, especially Mixtral, are filtered (See Appendix~\ref{sec:filtered_data_proportion} for more details). 


\paragraph{Machine Translation} To compare LLM-assisted dataset construction with machine translation, we translate 1,000 instances from the original \texttt{IndoCulture} training set into 1,000 Javanese and 1,000 Sundanese examples. Both translations are generated using Google Translate\footnote{\url{https://translate.google.com/}, accessed in September 2024.}, chosen for its strong translation performance in both languages. Since this dataset is intended for training, no human validation was conducted. From this point forward, we refer to it as \textbf{MT\textsubscript{train}}.




%As a baseline, we uses the original ID train set \cite{koto2022cloze}. We also uses \( \mathcal{S}_{\text{MT}} \) train set as a comparison, which includes 1,000 Javanese and 1,000 rows of Sundanese raw translations of the original IndoCloze train dataset. Both translations were generated using Google Translate\footnote{\url{https://translate.google.com/}, accessed in September 2024.}


%\textbf{Data Generation with LLM} For each LLM call, we choose five randomly selected samples from the seed pool and then ask it to generate several new samples. The LLM temperature is set to 0.7 to add some varieties in the generated samples. We repeat the generation process for each LLM until it produces at least 1,000 non-broken examples. We also make sure that each LLM generated train set is balanced and each contained exactly 1,000 samples. As indicated in the Appendix~\ref{sec:topic_distribution}, the number of samples generated per topic averages around 166 for each dataset.

%Formally, let \( LLM_{i} \) denote the synthetic dataset generated by the \( i \)-th LLM, where \( i \in \{ \text{GPT4-o}, \text{Cohere}, \text{Claude}, \text{Mixtral}, \text{Gemma2}, \newline \text{Llama3.1} \} \). Each dataset \( LLM_{i} \) consists of 2,000 samples. Finally, we apply a data filtering pipeline and got 1,220 curated samples out of total 12K generated samples.
%\[
%LLM_{i} = \{ x_1, \dots, x_i \dots, x_{2000}\}
%\]
%\[
%LLM_{\text{i}} \in 
%\left\{
%\begin{aligned}
%    &LLM_{\text{GPT4o}}, LLM_{\text{Cohere}}, LLM_{\text{Claude}}, \\
%    &LLM_{\text{Mixtral}}, LLM_{\text{Gemma2}}, LLM_{\text{Llama3.1}}
%\end{aligned}
%\right\}.
%\]


\subsection{Test Set}
%\subsection{LLM-Generated Data}
\label{subsect:llm_generated_data_construction}

We carefully construct the test set using two strategies: (1) machine translation with human verification, resulting in 500 Javanese and 500 Sundanese instances, and (2) manual writing by native speakers based on pre-defined topics, producing roughly 529 Javanese instances and 595 Sundanese instanecs. Each strategy undergoes rigorous quality control to ensure accuracy and reliability. In total, we create a high-quality test set of 2,124 instances across both languages.

To ensure the authenticity and quality of the dataset, we recruited 4 expert workers (2 per language) who are not only native Indonesian speakers but also fluent in Javanese and Sundanese. Each expert worker has a deep understanding of their respective language, culture, and customs. They have at least 10 years of experience speaking Javanese or Sundanese and possess strong linguistic and cultural expertise. The recruited workers, aged between 21 and 35 years, hold bachelor's degrees and were carefully selected for their proficiency in both language and cultural knowledge.

%\textbf{Evaluation Set} We constructed a verified evaluation dataset consisting of 1K manually verified samples in Javanese and Sundanese, randomly sampled and translated from the original Indonesian test split (MT\textsubscript{test}) of the IndoCloze Dataset \citep{koto-etal-2022-cloze}. Furthermore, we collected human-written evaluation data (HW), approximately consist of 1,200 Javanese and Sundanese samples, bringing the total evaluation data set to approximately 2.2K samples across both languages. 


%We have two sets of evaluation data: (1) Translated IndoCloze: where we use machine translation to translate Indonesian Story Cloze data \citep{koto2022cloze} and then it is verified through a two-stage review by two native speakers of each respective language; and (2) Human-written: Where each worker asked to write story cloze data in respective local language based on given topics.

\paragraph{Machine translation with human verification} As shown in Figure~\ref{local_cloze_pipeline}, we translate 500 randomly selected samples from the \texttt{IndoCloze} test set into Javanese and Sundanese using Google Translate.\footnote{Google Translate was accessed in September 2024. Similar to the construction of training set, we choose Google Translate for its strong translation performance in both Javanese and Sundanese.} To ensure quality, we employ two native speakers for each language and implement a two-stage quality control process. In Stage 1, the first worker manually corrects translation errors and localizes content by replacing entities (e.g., names, buildings, food) with culturally relevant alternatives. In Stage 2, a second worker validates the revised text and directly corrects any remaining errors from the first stage. This verification step ensures the accuracy and naturalness of the machine-translated test set. From this point forward, we refer to this data as \textbf{MT\textsubscript{test}}.


\paragraph{Human-written Dataset} Each expert worker in Sundanese and Javanese is tasked with writing 600 short stories following the \texttt{IndoCloze} format: a four-sentence premise, a correct fifth sentence, and an incorrect fifth sentence. Stories are written based on 12 predefined topics, adhering to the same topic taxonomy used for training (see Figure~\ref{fig:topic_dist_human} and Section~\ref{subsect:llm_generated_data_construction}). Each topic requires 50 stories, resulting in a total of 600 stories per language.\footnote{See Appendix~\ref{sec:story_guideline} for further details on the writing guidelines.}

To ensure quality, each expert worker reviewed their peer’s written stories using the same pairing system as in the \texttt{IndoCloze} portion. The reviewing worker was presented with a premise and two randomized alternate endings from another worker’s story and was asked to identify the correct one. Instances incorrectly identified by the second worker were discarded, as they likely contained incorrect endings or exhibited ambiguity. After quality control, 529 Javanese and 595 Sundanese instances remained from the original 600 per language. From this point forward, we refer to this human-written dataset as \textbf{HW}.


%each worker reviews a dataset previously checked by another worker from the same province. This verification step ensures the accuracy and naturalness of the machine-translated test set.


%or the test set, denoted as \textbf{MT\textsubscript{test}}, we randomly select 500 rows from the 1,135 original samples in the IndoCloze dataset \citep{koto2022cloze} and translate them into Javanese and Sundanese using machine translation. To ensure quality, we implement a two-stage quality control process, where each worker reviews a dataset previously checked by another worker from the same province. This verification step ensures the accuracy and naturalness of the machine-translated test set.


%To investigate whether LLMs are capable of generating plausible Story Cloze data in Javanese and Sundanese, we build a synthetic training dataset by utilizing LLMs with minimum human intervention. An illustration of the proposed pipeline is shown in Figure \ref{local_cloze_pipeline}; we ask each LLM to produce 1,000 clean and unbroken datasets using a simple prompt.

%\textbf{Seed Example} We manually translate about 50 samples from the Indonesian Story Cloze train dataset into Javanese and Sundanese to be used as seed. We try to incorporate cultural elements from the two local languages, for example, \textit{Gudeg}, a traditional breakfast from Yogyakarta, and \textit{Sawéran}, a tradition in Sundanese wedding ceremonies.

%\textbf{Prompt} We build a prompt with in-context samples from seed data\footnote{For more details, see \nameref{sec:prompt_template}.}. To ensure the generated examples align with the intended cultural context, we select one topic per request from the taxonomy of IndoCulture \citep{koto-etal-2024-indoculture}. The topic distribution across LLMs is approximately 166 samples per topic for balanced representation (see Figure \ref{fig:topic_distribution}). The available topics include: \textit{Food, Wedding, Family Relationships, Pregnancy and Kids, Death, Religious Holidays, Agriculture, Fisheries and Trade, Art, Traditional Games, Daily Activities, and Socio-religious Aspects of Life}. 

%For each LLM call, we choose five randomly selected samples from the seed pool and then ask it to generate several new samples. The LLM temperature is set to 0.7 to add some varieties in the generated samples. We repeat the generation process for each LLM until it produces at least 1,000 non-broken examples. We also make sure that each LLM generated train set is balanced and each contained exactly 1,000 samples. As indicated in the Appendix~\ref{sec:topic_distribution}, the number of samples generated per topic averages around 166 for each dataset.


%\subsection{Evaluation Dataset Construction}
%\label{subsect:eval_set_creation}
%\paragraph{Machine Translation (MT)} As shown in Figure \ref{local_cloze_pipeline}, we apply machine translation for both training and testing. For the training set, denoted as \textbf{MT\textsubscript{train}}, we perform a direct translation from Indonesian into Javanese and Sundanese without further human verification, resulting in raw machine-translated data. For the test set, denoted as \textbf{MT\textsubscript{test}}, we randomly select 500 rows from the 1,135 original samples in the IndoCloze dataset \citep{koto2022cloze} and translate them into Javanese and Sundanese using machine translation. To ensure quality, we implement a two-stage quality control process, where each worker reviews a dataset previously checked by another worker from the same province. This verification step ensures the accuracy and naturalness of the machine-translated test set.


\section{Data Analysis}
\subsection{Overall Statistics}
For LLM-assisted dataset creation, GPT-4o and Claude model demonstrated the highest efficiency, generating nearly 1,000 clean samples with minimal discarded output, while Mixtral was the least efficient, requiring significantly more samples to reach the same threshold (see Appendix~\ref{sec:overgeneration_distribution}). The LLM-generated data does not tend to have a uniform topic distribution due to variations in broken samples.

In total, the LLM-generated datasets contain 72K sentences and approximately 557K words. The word distribution across sentence positions remains consistent across the six LLMs, with word counts per position relatively uniform and a median sentence length ranging from 5 to 10 words (see Appendix~\ref{sec:sentence_length_distribution}).
%FJ: What about Human-written and MT? Are they similar to LLM-generated? Please add

The MT and Human-written datasets exhibit a similar word distribution pattern to the LLM-generated datasets. MT contains around 6K sentences with 44,5K words, while Human-written data has 6,7K sentences with 58,2K words. Despite the slight difference in word count, both datasets maintain a consistent distribution, with a median sentence length ranging from 4 to 11 words.

\begin{table*}[ht!]
\small
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c}{\cellcolor{blue!7}\textbf{Sundanese}} & \multicolumn{4}{c}{\cellcolor{red!7}\textbf{Javanese}} \\
 & \cellcolor{blue!7}\textbf{Coherence} & \cellcolor{blue!7}\textbf{Fluency} & \cellcolor{blue!7}\textbf{Correctness} & \cellcolor{blue!7}\textbf{Cultural Rel.} & \cellcolor{red!7}\textbf{Coherence} & \cellcolor{red!7}\textbf{Fluency} & \cellcolor{red!7}\textbf{Correctness} & \cellcolor{red!7}\textbf{Cultural Rel.} \\ \midrule
GPT-4o & 4.7 & 4.2 & 80 & \textbf{96} & 4.9 & 4.5 & 96 & \textbf{68} \\
Claude & 4.7 & 4.4 & 86 & 92 & 4.9 & 4.3 & 92 & 74 \\
Cohere & 3.4 & 3.0 & 28 & 46 & 4.5 & 4.1 & 70 & 52 \\
Llama3.1 & 3.7 & 3.4 & 56 & 70 & 4.5 & 4.3 & 54 & 44 \\
Gemma2 & 3.9 & 3.1 & 42 & 78 & 4.8 & 3.6 & 76 & 54 \\
Mixtral & 2.0 & 1.5 & 0 & 4 & 2.2 & 2.1 & 0 & 6 \\ \hline
MT\textsubscript{test} & \textbf{5.0} & \textbf{5.0} & \textbf{100} & 14 & \textbf{5.0} & \textbf{5.0} & \textbf{100} & 18 \\
HW & \textbf{5.0} & \textbf{5.0} & \textbf{100} & \textbf{96} & \textbf{5.0} & \textbf{5.0} & \textbf{100} & 66 \\
\bottomrule
\end{tabular}%
}
\caption{Quality analysis of various models on Sundanese and Javanese. Higher scores indicate better performance in each category. ``HW'' denotes human-written dataset}
\label{tab:quality_analysis}
\end{table*}

\begin{table*}[ht!]
\small
\centering
\resizebox{0.85\linewidth}{!}{%
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c}{\cellcolor{blue!7}\textbf{Sundanese}} & \multicolumn{4}{c}{\cellcolor{red!7}\textbf{Javanese}} \\
 & \cellcolor{blue!7}\textbf{\#data} & \cellcolor{blue!7}\textbf{\#vocab} & \cellcolor{blue!7}\textbf{LW (\%) $\downarrow$} & \cellcolor{blue!7}{\textbf{MATTR} $\uparrow$} & \cellcolor{red!7}\textbf{\#data} & \cellcolor{red!7}\textbf{\#vocab} & \cellcolor{red!7}\textbf{LW (\%) $\downarrow$} & \cellcolor{red!7}{\textbf{MATTR} $\uparrow$} \\ \hline
GPT-4o & 1000 & 3444 & 0 & \textbf{0.82} & 1000 & 3073 & 0 & \textbf{0.80} \\
Claude & 1000 & 3898 & 0 & 0.80 & 1000 & 3104 & 0 & 0.79 \\
Cohere & 1000 & 2654 & 3 & 0.65 & 1000 & 2254 & 3 & 0.68 \\
Llama3.1 & 1000 & 3126 & 0 & 0.69 & 1000 & 2836 & 2 & 0.69 \\
Gemma2 & 1000 & 3758 & 5 & 0.72 & 1000 & 3334 & 4 & 0.71 \\
Mixtral & 1000 & 4584 & \textbf{16} & 0.71 & 1000 & 4215 & \textbf{13} & 0.67 \\ 
\midrule
MT\textsubscript{test} & 500 & 3877 & 0 & 0.82 & 500 & 3620 & 3 & 0.81 \\
HW & 594 & 4693 & 0 & \textbf{0.84} & 529 & 3497 & 0 & \textbf{0.80} \\ 
\bottomrule
\end{tabular}%
}
\caption{Lexical diversity analysis of different models on Sundanese and Javanese.}
\label{tab:lexical_diversity}
\end{table*}

\subsection{Quality Analysis based on Human Evaluation}
\label{sec:quality_analysis}
We evaluate the quality of LLM-generated, machine-translated, and human-written datasets through human assessment based on four key criteria: coherence, fluency, correctness, and cultural relevance. Specifically, we randomly select 50 samples (5–10\% of the total dataset) for both Sundanese and Javanese and engage two native speakers for evaluation. Coherence and fluency are rated on a Likert scale from 0 to 5, while correctness and cultural relevance are assessed using binary annotation and reported as percentages. More details on the annotation guideline can be found in Appendix~\ref{sec:worker_guideline}.
Table~\ref{tab:quality_analysis} summarizes the results of human evaluation, with scores for coherence and fluency being averaged between the annotators, while we count the percentage of data perceived as correct or culturally relevant by both annotators to aggregate scores on correctness and cultural relevance. The Pearson correlation between annotators for coherence and fluency ranges from 0.6 to 0.7, while Kappa scores for correctness and cultural relevance range from 0.4 to 0.6.

We observe that across LLMs, GPT-4o and Claude model consistently demonstated strong performance across all metrics, especially in correctness and cultural relevance.  Notably, in terms of cultural relevance, both models achieved cultural relevance scores comparable to human written text, making them the best-performing LLMs in this aspect. This also indicates that GPT-4o and Claude model are capable of generating culturally appropriate content at a level similar to human writing. However the major issue is in correctness, where GPT-4o has 20\% error in Sundanese. Machine translation with post-edit might produce a good coherence, fluency, and correctness. However, the data has lower cultural relevance score. This highlight that traditional approach with machine translation is not the best practice in low-resource


% Moreover, in terms of name generation, LLMs frequently produced common Javanese names as \textit{Ayu}, \textit{Dwi}, \textit{Bayu}, \textit{Eko}, and \textit{Sari}, while Mixtral and Gemma showed greater diversity rather than other LLM. For Sundanese, LLMs primarly generated names like \textit{Lia}, \textit{Budi}, \textit{Dewi}, \textit{Rina}, whereas Llama and Gemma introduced more varied name choices. All these names are widely used in Indonesia. However, variations in honorific terms were minimal across all models, with only \textit{Pak} and \textit{Bu} (along with variants like \textit{Ibu} and \textit{Bapak}) appearing consistently.

%To ensure the consistency in evaluation, we provided the workers with a scoring guide, outlining the criteria for coherence, fluency, correctness, and cultural relevance. Further details on this guideline can be found in Appendix~\ref{sec:worker_guideline}. Furthermore, we computed Cohen Kappa scores for each criterion to measure worker agreement, which ranged between 0.2 and 0.7 (see the Appendix~\ref{sec:agreement_score}).

%\subsection{LLM-Generated Data}
%\label{subsect:llm_generated_data_analysis}

%\textbf{General Statistics} The LLM-assisted dataset creation resulted in imbalanced datasets across different LLMs due to variations in broken samples and uneven topic distributions. GPT-4o and Claude model demonstrated the highest efficiency, generating nearly 1,000 clean samples with minimal discarded output, while Mixtral was the least efficient, requiring significantly more samples to reach the same threshold (see Appendix~\ref{sec:overgeneration_distribution}). 








\subsection{Lexical Diversity}
We analyze the lexical diversity of LLM-generated data for Sundanese and Javanese using key metrics such as vocabulary size, moving-average type-token ratio (MATTR) \cite{covington2010cutting}, and the proportion of loanwords. To measure loanword presence, we manually review the top 100 most frequent words for each model.

As shown in Table~\ref{tab:lexical_diversity}, GPT-4o achieves a MATTR score of 0.82 for Sundanese and 0.80 for Javanese, making it highly comparable to human-written data. Claude also demonstrates strong lexical diversity, maintaining a rich vocabulary while avoiding borrowed words. Machine translation (MT\textsubscript{test}) has a competitive MATTR score but contains 3\% loanwords in Javanese.

Among the LLM-generated datasets, Mixtral has the largest vocabulary size but also introduced the highest proportion of loanwords, with 16\% in Sundanese and 13\% in Javanese, suggesting a significant reliance on non-native terms. In contrast, GPT-4o and Claude generate text entirely in Sundanese and Javanese without incorporating foreign words, highlighting their ability to produce better datasets. 

In name generation, LLMs frequently produce common Javanese names as \textit{Ayu}, \textit{Dwi}, \textit{Bayu}, \textit{Eko}, and \textit{Sari}, while Mixtral and Gemma2 showed greater diversity rather than other LLM. For Sundanese, LLMs primarily generate names like \textit{Lia}, \textit{Budi}, \textit{Dewi}, \textit{Rina}, whereas Llama3.1 and Gemma2 introduce more varied name choices. All these names are widely used in Indonesia. However, variations in honorific terms are minimal across all models, with only \textit{Pak} and \textit{Bu} (along with variants like \textit{Ibu} and \textit{Bapak}) appearing consistently.

% Furthermore, some LLMs introduce repetitive patterns. In \( \mathcal{S}_{\text{Gemma2}} \) , \textit{Pak Budi} appears 526 times, making up 0.92\% of the dataset, while \( \mathcal{S}_{\text{Llama3.1}} \)  frequently generates \textit{idul fitri} (84 times).\footnote{See Appendix \ref{sec:top5_ngrams} for a detailed breakdown of the most frequent bigrams and trigrams across LLM-generated datasets.}

\section{Experiments and Analysis}

\subsection{Classification}
\paragraph{Setup} We adopt the classification accuracy metric, as proposed in both \citet{mostafazadeh-etal-2016-corpus} and \citet{koto-etal-2022-cloze}, defined as the ratio of correctly classified cases to the total number of test cases, $\frac{correct}{testcases}$. We fine-tune all models using the training set in Javanese and Sundanese combined, and average the results over three runs to ensure robustness. In this study, we train several models and evaluate their adaptability in the two languages. We employ several models for our experiments:  BiLSTM, and XLM-R \citep{abs-1911-02116} (Details in Appendix~\ref{sec:training_configurations}).


\begin{table}[htbp]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccc}
\hline
\textbf{Training} & \textbf{\#Data} & \textbf{BiLSTM} & \textbf{XLM-R} \\ \hline
ID & 1K & 47.7 ± 1.1 & 62.8 ± 0.4\\
MT\textsubscript{Train} & 2K & 58.2 ± 0.6 & 69.4 ± 0.5\\
\midrule
{GPT-4o} & 2K & 62.8 ± 0.6 & \textbf{72.7 ± 0.5}\\
{Claude} & 2K & 64.9 ± 0.6 & 72.0 ± 0.8\\
{Cohere} & 2K & 57.1 ± 0.1 & 68.6 ± 0.8\\
{Llama3.1} & 2K & 60.1 ± 0.6 & 72.0 ± 0.4\\
{Mixtral} & 2K & 60.2 ± 0.5& 54.9 ± 0.5\\
{Gemma2} & 2K& 55.3 ± 0.9& 56.6 ± 0.7\\
\midrule
All LLMs  & 12K & \textbf{65.1 ± 0.1}& 72.5 ± 0.4\\
All LLMs + Filtering & 1.2K  & 62.7 ± 0.3& 70.4 ± 0.2\\
All LLMs + Filtering & 0.6K & 61.6 ± 0.7 & 69.4 ± 0.1\\ \bottomrule
\end{tabular}
}
\caption{Accuracy across different training sets. ``All LLMs + Filtering'' has two variants: (1) a curated set of 1,220 samples selected through the automatic filtering pipeline and (2) a subset of 674 samples containing only filtered data generated by GPT-4o and Llama3.1. }
\label{tab:overall_classification_heatmap}
\end{table}

%The baseline method using n-gram overlap achieves 58,5\% in MT\textsubscript{test} test set and 64,05\% in HW test set. Meanwhile, fasttext-similarity method achieves 62\% in MT\textsubscript{test} and 62,7\% in HW. 

\paragraph{Overall Performance} Table~\ref{tab:overall_classification_heatmap} shows that training on LLM-generated datasets mostly improves performance over the baseline ID dataset, highlighting the decent quality of synthetic data for both languages. Among the LLM-generated training sets, Claude and GPT-4o perform particularly well, 3--4 points higher than machine translation (MT\textsubscript{Train}). We also observe that data generated by Mixtral and Gemma2 yields worse accuracy than MT\textsubscript{Train}. The full results of the classification experiments are shown in Appendix~\ref{sec:classification_test_performance_on_jvsu}
%We can also see from the table that training with data generated by some LLMs such as Mixtral and Gemma2 gives a worse result than using LLM\textsubscript{MT}. Another interesting take is that data generated by Llama3.1 give a decent result even though it is not rated as good as Claude and GPT-4o by human evaluators.


\paragraph{Train with Filtered Data} In As shown in Table~\ref{tab:overall_classification_heatmap}, when using the full LLM-generated dataset (12K samples), XLM-R achieves an accuracy of 72.5, which is competitive with GPT-4o. After applying filtering, the training set is reduced to 1,220 and 674 samples (61\% and 33.7\% of the total training data generated by GPT-4o). Despite this substantial reduction in data size, accuracy drops by only 1–3\%, highlighting the efficiency of training with a smaller yet more representative subset.

%We experiment using two filtered subsets (LLM\textsubscript{Filtered} and LLM\textsubscript{Filtered - Top 2}) containing only 1,220 and 674 samples, respectively. Despite the significant reduction in data—using only 61\% and 33.7\% of the total training set in LLM\textsubscript{GPT-4o}—the model maintains competitive performance. The accuracy drops by only 1-3\% compared to training using LLM\textsubscript{GPT-4o}, demonstrating the efficiency of training with a smaller yet representative subset of data.

\begin{figure}[ht] %t
    \centering
    \resizebox{0.9\linewidth}{!}{%
        \includegraphics[width=\textwidth]{images/barchart_different_test_set.png} % Adjusted to fit one column
    }
    \caption{Accuracy comparison on two different test sets: MT$_\textsubscript{test}$ and human-written data (HW).}
    \label{fig:barchart_different_test_set}
\end{figure}

%\textbf{Train with ID Data} We tried to use Indonesian story-cloze train data in addition to LLM-generated local language train data. Table~\ref{fig:barchart_different_test_set} shows that adding an ID train set generally increases the model performance in both languages and both test sets.

\paragraph{Accuracies on Machine-translated and Human-written Test Set} Figure~\ref{fig:barchart_different_test_set} shows that when XLM-R is trained on MT\textsubscript{Train}, it achieves similar accuracy (70\%) on both MT\textsubscript{Test} and HW. Interestingly, compared to LLM-generated training data, MT\textsubscript{Train} performs comparably or even better on MT\textsubscript{Test} but worse on HW. This is likely because MT\textsubscript{Train} and MT\textsubscript{Test} originate from \citet{koto2022cloze}, resulting in a similar topical distribution. In contrast, the human-written test set and the LLM-generated training data were created using the same predefined topics.

\begin{figure}[ht] %t
    \centering
    \includegraphics[width=0.5\textwidth]{images/radarchart_topical_accuracies.png} 
    \caption{Classification accuracy across topics.}
    \label{fig:radarchart_topical_accuracies}
\end{figure}

\paragraph{Performance across Diferrent Topics} Figure~\ref{fig:radarchart_topical_accuracies} shows the performance of XLM-R in different topics. The chart shows that the data generated by GPT-4o and Claude consistently achieve high accuracy in most categories, but they also still underperform in some. The two models exhibit strong accuracy in categories such as \textit{Art}, \textit{Socio-religious aspects of life}, \textit{Agriculture} and \textit{Wedding}, and are generally weaker in categories like \textit{Fisheries and Trade}, \textit{Death}, \textit{Daily Activities} and \textit{Pregnancy and Kids}. It can also be seen that the two models give almost identical performance in Sundanese whereas GPT-4o seems slightly better in Javanese.

%The figure also shows that both models exhibit similar strengths and weaknesses, with slight variations in certain aspects. GPT-4o demonstrates superior performance in categories such as \textit{Fisheries and Trade} and \textit{Pregnancy and Kids}, while Claude (red line) slightly outperforms in areas like \textit{Religious Holiday} and \textit{Daily Activities} in Sundanese. 

\subsection{Generation}
\paragraph{Setup} We use a four-sentence premise as input and train Indonesian LLM, \texttt{Sahabat.AI} \citep{goto2024sahabatai}, to generate the fifth sentence in both Javanese and Sundanese. We use QLoRA \citep{dettmers2023qlora} to perform supervised fine-tuning. The hyperparameters for training can be seen on Appendix \ref{sec:training_configurations}. Automatic evaluation methods are used to evaluate the model performance: ROUGE-L \citep{lin-2004-rouge}, METEOR \citep{banerjee-lavie-2005-meteor}, and BERTScore \citep{zhang2019bertscore}.

\paragraph{Overall Performance} Table~\ref{tab:generation_performance} presents the automated evaluation metrics for different training sets across Sundanese and Javanese. Training on LLM-generated datasets generally improves performance over the baseline ID dataset. However, MT achieves the highest scores in Sundanese for ROUGE-L and METEOR and outperforms all LLM-generated datasets across all metrics in Javanese. Among LLM-generated datasets, Llama3.1 performs best, particularly in Javanese, where it achieves the highest ROUGE-L and METEOR scores. % In contrast, Claude shows more balanced performance across both languages but does not surpass MT in any metric.

\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|ccc|ccc}
\hline
\multirow{2}{*}{\textbf{Train Set}} & \multicolumn{3}{c|}{\textbf{Sundanese}} & \multicolumn{3}{c}{\textbf{Javanese}} \\
\cline{2-7}
& \textbf{R-L} & \textbf{M} & \textbf{BS} & \textbf{R-L} & \textbf{M} & \textbf{BS} \\ \hline
ID                         & 17.8  & 12.0  & 70.7  & 19.9  & 13.7  & 72.1  \\
{MT}         & 19.1  & 13.4  & 70.4  & 21.8  & 16.7  & 72.5  \\
%{GPT-4o}       & 17.0  & 11.8  & 70.7  & 20.2  & 15.4 & 72.0  \\
{Claude}     & 18.1  & 12.7  & 71.6  & 20.4  & 15.5  & 72.2  \\
{Llama3.1}      & 18.6  & 12.7  & 70.5  & 23.2  & 18.0  & 72.6  \\\hline
\end{tabular}%
}
\caption{Generation performance across different training sets for Sundanese and Javanese test sets. Metrics: ROUGE-L (R-L), METEOR (M), and BERTScore (BS).}
\label{tab:generation_performance}
\end{table}

\begin{figure}[ht] %t
    \centering
    \includegraphics[width=0.5\textwidth]{images/barchart_human_evaluation_generation.png} % Adjusted to fit one column
    \caption{Human evaluation on generation task. LLM means model trained with LLM-generated data}
    \label{fig:barchart_human_evaluation_generation}
\end{figure}

\paragraph{Human Evaluation} We conducted a human evaluation to compare models trained on machine-translated data with those trained on LLM-generated data, selecting Claude as the representative LLM model. Workers were presented with two randomized responses and asked to determine whether the first response was better, the second was better, or if both were equally good (draw). The results in Figure~\ref{fig:barchart_human_evaluation_generation} indicate a slight preference for the model trained on LLM-generated data over MT\textsubscript{Train} in both languages.



% \subsection{Challenges in Indonesian Local Languages}
% % error analysis
% We observe that models frequently misclassify sentence endings due to two key factors: limitations in vocabulary representation and an overreliance on lexical similarity. Many Javanese and Sundanese words are rarely used, even among native speakers, making them difficult for models to interpret accurately. For example, in Sundanese, \textit{jajan tambarakan} means snacking carelessly (without considering hygiene), but when translated literally, it is broken down as \textit{snacking} (\textit{jajan}) + \textit{car crash} (\textit{tambarakan}), leading to misinterpretations by Google Translate and NLP models, which incorrectly translate it as \textit{cake}. Additionally, models tend to rely heavily on lexical overlap between the premise and the candidate endings, rather than properly understanding the sentence structure or meaning. As a result, they often favor endings that share more words with the premise, failing to correctly process negation and contextual cues.

\section{Conclusion}
We explored the potential and limitations of LLM-generated data for commonsense reasoning and story generation in Javanese and Sundanese, introducing the first cloze dataset for these languages with high-quality test sets. Our preliminary analysis in classification and generation settings shows that GPT-4o and Claude-3 Opus demonstrate strong capabilities in generating plausible short stories but face challenges in fluency and cultural accuracy. Despite these limitations, our findings suggest that LLM-assisted data generation is a practical and effective approach for constructing datasets in low-resource languages.

% \textbf{Future Work} One possible direction is improving the automatic filtering method to exclude low-quality generated examples. This could significantly improve the quality of the data set and allow systematic comparisons of filtered versus unfiltered datasets on model performance. Another direction is the exploration of advanced prompting strategies to improve the relevance and quality of synthetic examples generated by LLMs.

\section{Limitations}
\textbf{Cultural Nuance, Language, and Dialect Coverage} This study acknowledges inherent limitations in the scope of its cultural nuance, language, and dialect coverage. While we carefully curated our data and covered 12 distinct cultural topics, our data sources and predefined topics may not fully capture the comprehensive range of cultural expressions within these multicultural languages, encompassing all dialects and regional variations. Furthermore, while we performed manual human evaluation to assess the quality of our generated data to some extent, the evaluation was limited in scale and depth. Future research should explore a broader array of cultural themes, incorporate diverse data sources, improve human evaluation methods, and deepen the dialectal representation to better reflect the diversity of the targeted languages. We also suggest future studies extends the research to other local languages in Indonesia.

\textbf{Prompt and Hyperparameter Variation} Due to resource constraints and the primary focus on data generation strategies, this research explored a limited range of LLM prompts and hyperparameter configurations. While prompt engineering and model training were performed with due diligence, it is important to acknowledge that alternative prompts or hyperparameter settings could potentially yield different results. Future research should investigate a wider spectrum of prompts and hyperparameters to identify configurations that maximize the performance of LLMs in generating culturally nuanced common sense reasoning in Javanese and Sundanese.

\section{Ethical Considerations}
All human-written datasets have been manually
validated to ensure that harmful or offensive questions are not present in the dataset. We paid our expert workers fairly, based on the monthly minimum wage in Indonesia\footnote{The average monthly minimum wage in Indonesia is approximately 3,000,000 IDR. The workload to complete all the tasks equates to roughly 8 days of full-time work. Each worker was paid 1,250,000 IDR accordingly}. All workers were informed that their stories submitted would be used and distributed for research. Furthermore, no sensitive or personal information about the workers would be disclosed.

% Entries for the entire Anthology, followed by custom entries
% \bibliographystyle{acl_natbib}
\bibliography{anthology,custom}
% \bibliographystyle{custom}
% \addbibresource{anthology.bib}
% \addbibresource{custom.bib}
% \printbibliography


\appendix
\label{section:appendix}
\section{Training Configurations}
\label{sec:training_configurations}

\subsection{Classification}
\begin{itemize}
    \item \textbf{n-gram overlap}: For this approach, we select the candidate with the highest overlap of n gram, measured by ROUGE-1 \citep{lin-2004-rouge}, calculated between the premise and the end. This metric ensures optimal alignment between the candidate and the reference text.
    \item \textbf{FastText-similarity}: We used a similarity-based classification method, selecting the candidate with the highest cosine similarity between the premise and the ending, using the 300-dimensional FastText word embeddings trained in respective local languages \citep{mikolov2018advances}.\footnote{\url{https://fasttext.cc/docs/en/crawl-vectors.html}}
    \item \textbf{Hierarchical BiLSTM}: We used a two-level 200-dimensional BiLSTM architecture similar to \citet{koto-etal-2022-cloze}, with 300-dimensional FastText representations of the respective language as input.
    \item \textbf{Pre-trained Language Models (PLM)}: We fine-tuned XLM-R \citep{abs-1911-02116}, a pre-trained multilingual language model.
\end{itemize}

For the LSTM model, we limited each sentence to a maximum of 30 tokens and trained it for up to 100 epochs, using early stopping with patience of 20 epochs. The model was trained with a batch size of 20, using the Adam optimizer and a learning rate of 0.1.

We set the maximum token length for the pre-trained language model to 450 for the premise and 50 for the ending sentence. The model was trained over 20 epochs with early stopping (patience set to 5), using a batch size of 40, Adam optimizer, an initial learning rate of 5e-6 for XLM-R, and a warm-up phase comprising 10\% of the total training steps.

\subsection{Generation}
We do supervised fine-tuning on Gemma2-9B-CPT-Sahabat-AI-v1-base \citep{goto2024sahabatai} base model using QLoRA. The model was trained with 4 bit quantization, lora rank 64, lora alpha 128, using a batch size of 8, gradient accumulation of 8, learning rate 2e-4, with 1 epoch. We use Unsloth.ai to perform the training \citep{unsloth}.

\subsection{Data Filtering Classifier}
We train XLM-R model with the maximum token length of 1024 for the premise and 128 for the ending sentence. The model was trained over 26 epochs, using a batch size of 16, Adam optimizer, an initial learning rate of 1e-5 for XLM-R, and a warm-up phase comprising 5\% of the total training steps.

\section{Prompt Template}
Figure~\ref{fig:prompt_template} shows the prompt template used for in-context learning, guiding the LLM to generate new Javanese and Sundanese.


\section{Filtered LLM-Generated Train Data Proportion}
\label{sec:filtered_data_proportion}
Initially, training data was generated using six different LLMs, with each model contributing approximately 16.67\% of the total 12,000 samples (around 2,000 samples per model). However, after filtering the bad examples, the final dataset composition shifted. The data consist of 1,220 samples, the distribution is as follows: Claude (37.7\%), GPT-4o (29.0\%), LLama (8.6\%), Cohere (4.3\%), Mixtral (2.7\%), and Gemma-2 (1.7\%). 

\section{Topic Distribution}
\label{sec:topic_distribution}

Figure \ref{fig:topic_distribution} illustrates the balanced distribution of topics in various datasets generated by different LLM (Claude, Cohere, GPT-4o, Mixtral, Llama3.1 and Gemma2). Each topic contains approximately 166 samples, with minor variations, ensuring an even distribution across all categories. Meanwhile, Figure~\ref{fig:topic_dist_human} visualizes the human-written dataset, where topics are similarly well distributed across both local languages.

\begin{figure}[ht] %ht
    \centering
    \includegraphics[width=0.5\textwidth]{images/everytopic.png} % Adjusted to fit one column
    \caption{Topic Distribution Across Datasets}
    \label{fig:topic_distribution}
\end{figure}

\begin{figure}[ht] %ht
    \centering
    \includegraphics[width=\linewidth]{images/topic_dist_human.png}
    \caption{Topic Distribution in Human-Written Data}
    \label{fig:topic_dist_human}
\end{figure}


\section{Sentence Length Distribution}
\label{sec:sentence_length_distribution}
Figure \ref{fig:sentence_length_distribution} illustrates the distribution of sentence lengths across six datasets generated by various LLMs. Each dataset exhibits a similar distribution, with most sentence lengths ranging from 5 to 10 words. While Figure \ref{fig:sentence_length_distribution_HWMT} shows the distribution of sentence lengths from MT and Human-written, and both dataset exhibit a similar pattern of distribution as LLM-generated data, with most sentence lengths are ranging from 4 to 11.

\begin{figure}[ht] %ht
    \centering
    \includegraphics[width=\linewidth]{images/HW-MT-sentence-distribution.png}
    \caption{Sentence Length Distribution Across Datasets: The figure shows the distribution of sentence lengths across MT and Human-written. Each bar represents the frequency of sentence lengths for different sentence types in each dataset.}
    \label{fig:sentence_length_distribution_HWMT}
\end{figure}

\begin{figure}[ht] %ht
    \centering
    \includegraphics[width=0.5\textwidth]{images/number_of_words_stacked.png} 
    \caption{Sentence Length Distribution Across Datasets: The figure shows the distribution of sentence lengths across various datasets generated by different LLMs. Each bar represents the frequency of sentence lengths for different sentence types in each dataset.}
    \label{fig:sentence_length_distribution}
\end{figure}

\section{Overgeneration Distribution}
\label{sec:overgeneration_distribution}
Figure \ref{fig:overgeneration_comparison} shown, the number of generated samples needed to produce 1,000 clean training data for Javanese and Sundanese varies between models, where lower numbers indicate higher efficiency due to fewer unusable examples such as duplicates or repetitions of premises.

\begin{figure}[ht] %ht
    \centering
    \includegraphics[width=1\linewidth]{images/chart.png} 
    \caption{Samples required to generate 1,000 clean training data: lower values indicate higher efficiency.}
    \label{fig:overgeneration_comparison}
\end{figure}

\section{Top-5 Bigrams Trigrams}
\label{sec:top5_ngrams}
Table~\ref{tab:top_bigrams_trigrams} presents the top-5 bigrams and trigrams found in the dataset, highlighting frequent patterns such as "Pak Budi" and "warga desa" (villagers), reflecting common themes in LLM-generated Javanese and Sundanese texts.


\begin{table}[ht!]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{lc}
\hline
\textbf{Bigram (unique: 103,022)} & \textbf{Freq (\%)} \\ \hline
pak budi (Mr. Budi) & 0.2488 \\
seneng banget (really happy) & 0.2136 \\
warga desa (villagers) & 0.1660 \\
lan kanca-kancane (and their friends) & 0.1406 \\
dina acara (event day) & 0.1077 \\ \hline
\textbf{Trigram (unique: 167,659)} & \textbf{Freq (\%)} \\ \hline
seneng banget karo (very happy with) & 0.670 \\
warga desa padha (villagers will) & 0.440 \\
dina budaya sunda (in Sundanese culture) & 0.417 \\
nu aya di (the one in) & 0.335 \\
kabeh warga desa (every villager) & 0.326 \\ \hline
\end{tabular}%
}
\caption{Top bigram and trigram occurrences in the dataset.}
\label{tab:top_bigrams_trigrams}
\end{table}

\section{Agreement Score}
\label{sec:agreement_score}
We calculated the fluency and coherency agreement scores using Pearson's correlation and computed the correctness and cultural relevance scores using Cohen's kappa. To evaluate the quality of LLM-generated examples, the workers followed a predefined scoring guideline (Appendix~\ref{sec:worker_guideline}). The agreement scores for Javanese and Sundanese are summarized in Table~\ref{tab:agreement_scores}.

\begin{table}[ht]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Metric} & \textbf{Javanese} & \textbf{Sundanese} \\ \hline
\textbf{Fluency}           & 0.74 & 0.76 \\ 
\textbf{Coherency}         & 0.78 & 0.71 \\ 
\textbf{Correctness}       & 0.73 & 0.50 \\ 
\textbf{Cultural Relevance} & 0.23 & 0.49 \\ \hline
\end{tabular}
\caption{Agreement Scores for Javanese and Sundanese}
\label{tab:agreement_scores}
\end{table}

\section{Classification Test Performance on Javanese and Sundanese}
\label{sec:classification_test_performance_on_jvsu}

Tables~\ref{tab:test_performance_su} and \ref{tab:test_performance_jv} present test performance on Sundanese and Javanese across different training datasets. XLM-R and BiLSTM outperform the n-gram and FastText baselines, with LLM-generated datasets achieving comparable or better results than machine translation. Notably, All \(LLMs + Filtered\) training sets yield the best performance, especially on human-written test sets, highlighting the benefit of curated synthetic data.

\section{Workers Guidelines}
\label{sec:worker_guideline}

\textbf{Rubric for Story Evaluation}

\begin{itemize}
    \item \textbf{Fluency:} Each sentence should be grammatically correct and fluent.
    \begin{itemize}
        \item \textbf{Score Range:} 0 to 5
        \item \textbf{5:} All sentences are grammatically correct and very fluent.
        \item \textbf{0:} Sentences are grammatically incorrect and lack fluency.
    \end{itemize}
    
    \item \textbf{Coherency:} The story should be coherent, with all sentences logically connected.
    \begin{itemize}
        \item \textbf{Score Range:} 0 to 5
        \item \textbf{5:} Story is highly coherent, with clear and logical flow between sentences.
        \item \textbf{0:} Sentences are disconnected and lack a logical sequence.
    \end{itemize}
    
    \item \textbf{Correctness:} The correct story closure should be valid, while the incorrect closure should clearly be wrong.
    \begin{itemize}
        \item \textbf{Binary Score:}
        \item \textbf{1:} The correct ending is indeed correct, and the incorrect ending is clearly wrong.
        \item \textbf{0:} Either the correct ending is not valid, or the incorrect ending is not clearly wrong.
    \end{itemize}
    
    \item \textbf{Cultural Relevance:} The story should reflect appropriate cultural norms, values, or symbols relevant to the language.
    \begin{itemize}
        \item \textbf{Binary Score:}
        \item \textbf{1:} The story contains relevant cultural norms/values/symbols for the corresponding language.
        \item \textbf{0:} The story lacks cultural relevance or includes irrelevant cultural aspects.
    \end{itemize}
\end{itemize}

\textbf{Scoring Guidelines:}
\begin{itemize}
    \item \textbf{Coherency:} Ranges from 0 to 5, where 5 means each sentence is strongly connected and flows well with the previous and next sentence.
    \item \textbf{Fluency:} Ranges from 0 to 5, where 5 indicates all sentences are grammatically sound and highly fluent.
    \item \textbf{Correctness:} A binary score of 0 or 1 to ensure that the correct ending is truly valid and the incorrect ending is clearly wrong.
    \item \textbf{Cultural Relevance:} A binary score of 0 or 1 to ensure the whole story contains appropriate and relevant cultural symbols and norms for the language being used.
\end{itemize}

\section{Topic Taxonomy and Story-Writing Guidelines}
\label{sec:story_guideline}

\textbf{Topic Taxonomy}

As part of the Javanese and Sundanese Cloze Project, a total of 300 stories must be created. These stories are evenly distributed across 12 predefined topic categories, with 25 stories per topic. Each story must reflect traditional Javanese and Sundanese values and customs, with attention to detail and coherence in the narrative. The topic categories and their subcategories are as follows:

\begin{itemize}
    \item \textbf{Food:} Breakfast, Lunch, Dinner, Snacks, Food souvenirs, Traditional foods and beverages, Eating habits, Cutlery, Cooking ware, Fruits.
    \item \textbf{Wedding:} Traditions before marriage, Traditions during marriage, Traditions after marriage, Men's wedding clothes, Women's wedding clothes, Invited guests, Wedding location, Foods at a wedding, Gifts brought to a wedding.
    \item \textbf{Family Relationship:} Relationships within the immediate family, Relationships in the extended family, Relations with society or neighbors, Clan or descendant system.
    \item \textbf{Pregnancy and Kids:} Traditions during pregnancy, Traditions after birth, Caring for a newborn baby, Caring for toddlers, Caring for children, Caring for teenagers, Parent-child interactions as adults.
    \item \textbf{Death:} When death occurs, The process of caring for a corpse, Traditions after burial, Mourners' clothes, Inheritance matters.
    \item \textbf{Religious Holiday:} Traditions before religious holidays, Traditions during religious holidays, Traditions after religious holidays.
    \item \textbf{Agriculture:} What to plant, Traditions during planting, Harvest.
    \item \textbf{Fisheries and Trade:} Traditions of taking care of livestock or fish, Buying and selling traditions.
    \item \textbf{Art:} Musical instruments, Folk songs, Traditional dances, Use of art at specific events, Poetry or similar literature.
    \item \textbf{Traditional Games:} Types of games, Locations where games are played.
    \item \textbf{Daily Activities:} Morning activities, Afternoon activities, Evening activities, Leisure activities, Household tasks, Transportation.
    \item \textbf{Socio-religious Aspects of Life:} Regular religious activities, Mystical elements, Traditional ceremonies, Lifestyle, Self-care, Traditional medicine, Traditional sayings.
\end{itemize}

\textbf{Story-Writing Guidelines}

Each story must consist of 4 sentences and two endings: one correct and one incorrect. The following example demonstrates how stories should be written (Table \ref{tab:story_example}):

Ensure that all stories adhere to the topics and categories outlined in the taxonomy and reflect traditional values and cultural relevance.

\label{sec:prompt_template}
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{images/prompt.png}
    \caption{Prompt template instructing LLM to generate a new example for Javanese and Sundanese, given a set of in-context examples.}
    \label{fig:prompt_template}
\end{figure*}



\begin{table*}[ht!]
\hspace{-1cm} 
\begin{tabular}{l|cccc|cccc} \hline
\multirow{2}{*}{Train Set} & \multicolumn{4}{c|}{Machine Translated - Test} & \multicolumn{4}{c}{Human-written} \\ \cline{2-9} 
 & \multicolumn{1}{l}{n-gram} & \multicolumn{1}{l}{FastText} & \multicolumn{1}{l}{BiLSTM} & \multicolumn{1}{l|}{XLM-R} & \multicolumn{1}{l}{n-gram} & \multicolumn{1}{l}{FastText} & \multicolumn{1}{l}{BiLSTM} & \multicolumn{1}{l}{XLM-R} \\ \hline
ID & \multirow{10}{*}{54.6} & \multirow{10}{*}{63.0} & 51.4 ± 1.1 & \multicolumn{1}{r|}{58.4 ± 0.6} & \multirow{10}{*}{55.7} & \multirow{10}{*}{62.5} & \multicolumn{1}{l}{49.3 ± 0.2} & \multicolumn{1}{l}{68.0 ± 0.6} \\ 
MT &  &  & 59.3 ± 0.2 & 67.2 ± 0.2 &  &  & \multicolumn{1}{c}{57.5 ± 0.1} & \multicolumn{1}{c}{74.3 ± 0.1} \\ 
GPT-4o &  &  & 63.5 ± 0.1 & 67.2 ± 0.1 &  &  & 62.8 ± 0.1 & 76.6 ± 0.1 \\
Claude &  &  & 63.5 ± 0.1 & 68.0 ± 0.1 &  &  & 63.7 ± 0.1 & \textbf{76.9 ± 0.1} \\
Cohere &  &  & 57.0 ± 0.1 & 62.6 ± 0.1&  &  & 54.3 ± 0.1 & 70.5 ± 0.2 \\
Llama3.1 &  &  & 60.4 ± 0.2 & 67.2 ± 0.1 &  &  & 58.1 ± 0.1 & 73.7 ± 0.1 \\
Mixtral &  &  & 57.4 ± 0.1&54.4 ± 0.1&  &  & 59.2 ± 0.2 & 54.7 ± 0.1 \\
Gemma2 &  &  & 59.4 ± 0.1 & 55.6 ± 0.1 &  &  & 61.2 ± 0.1 & 54.3 ± 0.1 \\ 
All LLMs &  &  & 63.4 ± 0.1 & \textbf{68.3 ± 0.1} &  &  & 63.1 ± 0.2 & 75.3 ± 0.1 \\
All LLMS + Filtering &  &  & 62.0 ± 0.1 & 67.2 ± 0.1
 &  &  & 61.7 ± 0.1 & 71.5 ± 0.1 \\ \hline
\end{tabular}
\caption{Test Performance on Sundanese across Different Models and Training Data}
\label{tab:test_performance_su}
\end{table*}

\begin{table*}[ht!]
\hspace{-1cm} 
\begin{tabular}{l|cccc|cccc} \hline
\multirow{2}{*}{Train Set} & \multicolumn{4}{c|}{Machine Translated - Test} & \multicolumn{4}{c}{Human-written} \\ \cline{2-9} 
 & \multicolumn{1}{l}{n-gram} & \multicolumn{1}{l}{FastText} & \multicolumn{1}{l}{BiLSTM} & \multicolumn{1}{l|}{XLM-R} & \multicolumn{1}{l}{n-gram} & \multicolumn{1}{l}{FastText} & \multicolumn{1}{l}{BiLSTM} & \multicolumn{1}{l}{XLM-R} \\ \hline
ID & \multirow{10}{*}{63.5} & \multirow{10}{*}{61.0} & 49.7 ± 0.1 & \multicolumn{1}{r|}{63.2 ± 0.7} & \multirow{10}{*}{72.4} & \multirow{10}{*}{62.9} & \multicolumn{1}{l}{40.6 ± 0.1} & \multicolumn{1}{l}{61.8 ± 0.1} \\ 
MT &  &  & 60.4 ± 0.1 & \textbf{70.5 ± 0.3} &  &  & 55.6 ± 0.2 & 65.7 ± 0.2 \\ 
GPT-4o &  &  & 56.7 ± 0.3 & 63.6 ± 0.1 &  &  & 71.0 ± 0.2 & \textbf{83.7 ± 0.1} \\
Claude &  &  & 58.9 ± 0.1 & 63.8 ± 0.1 &  &  & 73.4 ± 0.1 & 79.5 ± 0.1 \\
Cohere&  &  & 52.8 ± 0.2 & 64.2 ± 0.1 &  &  & 64.2 ± 0.1 & 77.2 ± 0.1 \\
Llama3.1 &  &  & 58.5 ± 0.2 & 65.3 ± 0.1 &  &  & 63.5 ± 0.1 & 81.9 ± 0.1 \\
Mixtral &  &  & 58.1 ± 0.1 & 56.2 ± 0.1 &  &  & 66.2 ± 0.1 & 54.5 ± 0.1 \\
Gemma2 &  &  & 52.4 ± 0.2 & 56.6 ± 0.1 &  &  & 48.3 ± 0.2 & 59.9 ± 0.1 \\
All LLMs &  &  & 60.4 ± 0.3 & 66.0 ± 0.1 &  &  & 73.4 ± 0.1 & 80.6 ± 0.1 \\
All LLMs + Filtering & & & 53.9 ± 0.1 & 63.6 ± 0.1 &  & & 73.0 ± 0.1 & 79.5 ± 0.1 \\ \hline
\end{tabular}
\caption{Test Performance on Javanese across Different Models and Training Data}
\label{tab:test_performance_jv}
\end{table*}

\begin{table*}
\centering
% \begin{tabularx}{\linewidth}{p{1.2cm}p{2cm}XX}
\begin{tabularx}{1\linewidth}{p{1.2cm}p{2cm}XX}
\hline
\textbf{Topic} & \textbf{Category} & \textbf{Javanese} & \textbf{Translated} \\
\toprule
\multicolumn{4}{l}{\cellcolor{blue!7} Javanese} \\
Food & Traditional foods and beverages & \textbf{Premise:} \textit{Laras seneng banget karo wedang uwuh, minuman tradisional saka Yogyakarta. Saben sore, dheweke mesthi ngombe wedang uwuh kanggo njaga kesehatan lan semangat. Wedang uwuh digawe saka campuran rempah-rempah alami sing wangi. Laras percaya yen wedang uwuh bisa ngusir rasa kesel lan lemes.} 
\textbf{Correct:} \textit{Saben sore, dheweke seneng bareng-bareng karo kanca-kanca ngombe wedang uwuh.} 
\textbf{Incorrect:} \textit{Dheweke ngindhari ngombe wedang uwuh saben sore. }
 & \textbf{Premise:} Laras is really happy with \textit{wedang uwuh}, a traditional drink from Yogyakarta. Every evening, she must always drinks \textit{wedang uwuh} to maintain her health and vitality. \textit{Wedang uwuh} is made from a mixture of fragrant natural spices. Laras believes that \textit{wedang uwuh} can drive away fatigue and weakness.
\textbf{Correct:} Every evening, she enjoys drinking \textit{wedang uwuh} with her friends.
\textbf{Incorrect:} She avoids drinking \textit{wedang uwuh} every evening. \\ 
\midrule
\multicolumn{4}{l}{\cellcolor{blue!7} Sundanese} \\
Wedding & Traditions when getting married & \textbf{Premise:} \textit{Kulawargi bapak Syakur nembe pisan hajat nikahan. Anjeunna parantos nikahkeun putrana anu bungsu, Anton. Acara sawéranna heboh pisan, aya rupi-rupi doorprize. Abi emut aya hiji ibu-ibu nu kéngéng rice cooker.}  
\textbf{Correct:} \textit{Aya ibu-ibu nu kéngéng doorprize tina sawéran hajatan pak Syakur.}
\textbf{Incorrect:} \textit{Pak Syakur masihan rice cooker kumargi ibu-ibu eta orang } 
 & \textbf{Premise:} My family recently had a wedding. He has married his youngest son, Anton. The event was very exciting, there were various door prizes. I remember there was a mother who received a rice cooker.  
\textbf{Correct:} There were mothers who received door prizes from Mr. Syakur's wedding.
\textbf{Incorrect:} Mr. Syakur gave the rice cooker because the mothers were improtant people. \\
\bottomrule
\end{tabularx}
\caption{Example of a story with correct and incorrect endings.}
\label{tab:story_example}
\end{table*}

\end{document}
