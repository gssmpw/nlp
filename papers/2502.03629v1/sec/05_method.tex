\section{An editing model trained with \RealEdit}
To demonstrate the value of \RealEdit, we develop an image editing model using training examples in \RealEdit.
Specifically, we utilize InstructPix2Pix~\cite{brooks2023instructpix2pix} as the model backbone on which we finetune using our data.
We leave exploration on using different base models as future work.

\input{tables/our_benchmark_all_empty_column}

\paragraph{Aligning with pretraining data.}
Since we finetune InstructPix2Pix rather than training the model from scratch, we align our finetuning dataset with the data distribution used in InstructPix2Pix's pretraining data to avoid substantial distributional shifts that may deteriorate model's performance.
In particular, InstructPix2Pix~\cite{brooks2023instructpix2pix} applies CLIP-based~\cite{radford2021learning} filtering to ensure the quality of image pairs. In addition, as it employs Prompt-to-Prompt~\cite{hertz2022prompt} in generating its training data, the input-output image pairs are with high structural similarity.
To align our training set, we thus follow the same CLIP-based filtering and additionally use SSIM~\cite{wang2004image} to include structurally similar images, recognizing that human edits collected in \RealEdit often alter structure with techniques like drag-and-drop adjustments and symmetrical flipping.
Tasks incompatible with InstructPix2Pix’s capabilities, such as resizing images, changing file types, or highly ambiguous prompts (particularly those involving humor) are thus excluded.
In total, we trained on 39K examples.
Aligning our training data with the InstructPix2Pix distribution allows for more competitive performance on metrics, and accounts for limitations in the InstructPix2Pix's architecture and pretraining.
For training our model, we closely follow the configuration of MagicBrush~\cite{zhang2024magicbrush}. Specifically, we train our model for 51 epochs, utilizing cosine learning rate decay and incorporating a learning rate warm-up phase (details in Appendix).
\paragraph{Decoding at inference.}
We observe that Stable Diffusion~\cite{rombach2022high} struggles with accurately reconstructing human faces and fine-grained image details.As shown in Section~\ref{sec:dataset_analysis}, real-world requests are human-centric with detailed edit needs. To address this, we incorporate OpenAI's Consistency Decoder~\cite{openai_consistencydecoder} at inference time, significantly enhancing generation quality for faces, patterns, and text without altering the diffusion process.

% For training, we adhered closely to InstructPix2Pix’s~\cite{brooks2023instructpix2pix} methodology, with our primary modification being the addition of cosine learning rate decay. Full configuration details will be shared for reproducibility.

% For data selection, we applied the following criteria:
% - SSIM above 0.6
% - CLIP score above 0.7, following IP2P's approach
% - CLIP score below 0.99 to eliminate duplicates
% - Exclusion of creative prompts to reduce ambiguity
% - Removal of formatting tasks due to execution challenges

