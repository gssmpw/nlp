\section{Related work}
\noindent\textbf{Image editing datasets.}
While extensive datasets exist for captioning and identifying edited images within fixed domains~\cite{desai2021redcaps, park2018double}, there is a notable lack of large-scale, human-edited image datasets.
% Creating these datasets is challenging due to the labor-intensive nature of manual editing. 
% For model training, high-quality image editing datasets are essential.
Currently, larger-scale image editing datasets mostly rely on synthetic data~\cite{brooks2023instructpix2pix, zhang2024magicbrush, sheynin2024emu, zhang2024hive, zhao2024ultraedit}, while the ones with human edited images are limited in size~\cite{shi2020benchmark, tan2019expressing}.
While synthetic datasets may include human inputs, such as generating instructions or ranking edits \cite{zhang2024magicbrush, zhang2024hive, brooks2023instructpix2pix}, these datasets do not contain \textit{edits} that are completed by humans.
Most importantly, existing datasets are curated in ways that do not necessarily characterize real-world editing distribution well. We compare \RealEdit to existing datasets in Table~\ref{tab:dataset_comparison}.
% Table \ref{tab:dataset_comparison} compares existing image editing datasets, illustrating that there is no large, fully human-edited dataset with ground-truth edits, a gap that restricts the domain of current editing models to arbitrary cases and inferior quality.

% As a result, edited images may contain varying quality model performance suffers.
% Most importantly, while notable datasets like GIER \cite{shi2020benchmark} and the Image Editing Request Dataset \cite{tan2019expressing} contain around 6,000 and 4,000 samples respectively, they are insufficient for training robust models that capture the wide range of real-world editing use cases.


\paragraph{Text-guided image editing.}
There is a rich literature in models focusing on specific image editing tasks, such as inpainting~\cite{yu2018generative}, denoising~\cite{goyal2020image}, and style transfer~\cite{gatys2016image}.
Recent advancements emphasize generalized models that better align with human use cases, leading to innovative methods such as generating programs to modify images~\cite{gupta2023visual}, as well as end-to-end diffusion-based or GAN-based editing models~\cite{karras2019style, patashnik2021styleclip, avrahami2022blended, meng2021sdedit, wang2023imagen, xie2023smartbrush}. Diffusion models like Stable Diffusion~\cite{rombach2022high} excel at generating images from text prompts,
serving as versatile models for image generation~\cite{zhan2023multimodal}.
Several models~\cite{brooks2023instructpix2pix, mokady2023null, kawar2023imagic} utilize diffusion-based techniques for editing, though generating images from captions alone may compromise fidelity. To mitigate this, some models~\cite{brooks2023instructpix2pix, mokady2023null, zhang2024magicbrush, krojer2024learning} leverage Prompt-to-Prompt technique~\cite{hertz2022prompt}, employing cross-attention maps to preserve most of the original image. Others achieve consistency by fine-tuning diffusion models to reconstruct images using optimized text embeddings, blending these with target text embeddings~\cite{kawar2023imagic}.
However, limitations persist, such as stuggles with face generation~\cite{borji2022generated} and cross-attention requiring minimal, often single-token caption variation.
% Our work builds upon a diffusion-based model, InstructPix2Pix~\cite{brooks2023instructpix2pix}, with the adoption of
% Consistency Decoder~\cite{openai_consistencydecoder} to more effectively tackle real-world editing requests that often involve human faces.\\

% \textbf{Other Editing Models.}
% Generative adversarial networks (GANs) like StyleGAN \cite{karras2019style} have also been adapted for editing, using CLIP similarity \cite{radford2021learning} to allow natural language inputs \cite{patashnik2021styleclip}. GAN-based methods, however, struggle with stability and generalization beyond the training data domain. Additionally, models often perform better when a mask specifies the region for edits \cite{avrahami2022blended, meng2021sdedit, wang2023imagen, xie2023smartbrush}, though mask dependency may be arduous for human users and cannot support global image modifications.

% \subsection{Fake Image Detection}
% The detection of fake images has become an escalating arms race, where advances in generation techniques are rapidly met with new detection methods, only to be later circumvented by more sophisticated manipulation approaches. \cite{lee2024tugofwardeepfakegenerationdetection} With the emergence of deepfakes and AI-generated content, particularly from diffusion models like Stable Diffusion and DALL-E, the volume and quality of synthetic images have increased dramatically. Recent works have proposed universal fake detection approaches to address this challenge. For example, \cite{ojha2024universalfakeimagedetectors, cozzolino2024raisingbaraigeneratedimage} found that conventional classifiers struggle to generalize across newer generative models. They propose instead to model latent image representations in the CLIP feature space and then applying nearest neighbors or linear probing, achieving significant improvements in detecting fakes from previously unseen models with minimal training data.

% While significant progress has been made in detecting AI-generated content, the detection of human-edited images presents a distinct set of challenges. Unlike AI-generated images, which often leave behind artifacts that are easily learnable, human edits can be subtle, diverse, and highly contextual, making them particularly difficult to detect with existing approaches. Our work addresses this gap by investigating whether in-the-wild human-edited images leave behind artifacts that a fine-tuned UFD on the REALEDIT dataset can detect effectively and delves into how REALEDIT can be used to further edited image detection research.

\paragraph{Evaluating image editing models.}
% \textbf{VQA-based Evaluation.}
Originating from early text summarization in NLP~\citep{Narayan2018RankingSF}, QA-based evaluation methods automatically transform prompts into questions and use them to validate generated content~\citep{Durmus2020FEQAAQ, Deutsch2020TowardsQA, Eyal2019QuestionAA}. 
In text-to-image generation, VQA-based evaluation methods transfer text into atomic questions and conduct VQA to verify generated images, providing enhanced fine-grained and interpretable benchmark results~\citep{cho2023davidsonian, lin2024evaluating, chen2024mllm}.
Notably, TIFA~\citep{hu2023tifa} pioneered the use of VQA for automatic evaluation, while subsequent works enhanced model-human correlation~\citep{yarom2024you, lu2024llmscore}, incorporated additional modules and MLLM-as-a-Judge~\citep{ghosh2024geneval, cho2024visual, ye2024justice, chen2024mllm, ku2023viescore}.
To evaluate image editing models, we follow and extend existing work~\cite{sheynin2024emu} in casting the evaluation into image generation evaluation wherein we measure the faithfulness of the edited images to their target output captions, using the aforementioned VQA-based frameworks.

% \textbf{VIEScore}



