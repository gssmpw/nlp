\section{Experiments} \label{sec:results}
% We evaluate our model trained on \RealEdit across various evaluation benchmarks, where  model demonstrates strong performance on both real user data and synthetic datasets.
\noindent\textbf{Setup.} We benchmark our model against six open-source baselines: InstructPix2Pix~\cite{brooks2023instructpix2pix}, MagicBrush~\cite{zhang2024magicbrush}, AURORA~\cite{krojer2024learning}, SDEdit~\cite{meng2021sdedit}, HIVE~\cite{zhang2024hive}, and Null-text Inversion~\cite{mokady2023null}.
We leverage the input and output captions generated in Section~\ref{sec:realedit} for models that require them.

% LLaVA-Next and GPT-4o as mentioned  

To evaluate the models, we adopt a comprehensive suite of metrics.
First, we utilize VQA-based automated metrics to measure task completion, as these metrics have been shown to closely reflect human judgments. In particular, we use VIEScore~\cite{ku2023viescore} with a GPT-4o backbone as our default metric, as it evaluates semantic consistency (VIE\_SC), perceptual quality (VIE\_PQ), and overall alignment with human-requested edits (VIE\_O) each on a scale of 0 to 10. 
Similarly, we use VQAscore~\cite{lin2024evaluating} (with different base models: 
 LLaVa and FLAN-T5) and TIFA~\cite{hu2023tifa} to evaluate the fine-grained faithfulness of the output image to the edit instruction.
We also include standard metrics such as L1- and L2 pixel distance, DINO~\cite{zhang2022dino}, CLIP-I and CLIP-T, following prior work~\cite{zhang2024magicbrush, sheynin2024emu}.
Most importantly, we leverage real users to make pairwise comparisons between edits and compute Elo ranking of the models~\cite{jiang2024genai}. We further qualitatively study the response Reddit users have on edits produced by our model on recent posts.
% \noindent\textbf{Training configuration.}



\subsection{Automated evaluations on \ours test set}
In Table~\ref{tab:our_benchmark_quantitative}, we show that existing models struggle to capture the semantic nuances of human requests, while our model achieves notable improvements, particularly in VIE\_SC scores. Our model also significantly outperforms other baselines on finer-grained metrics like VQAScore and TIFA. Although our model achieves state-of-the-art (SOTA) results on standard metrics, these metrics are limited as they fail to fully capture task completion. Notably, using the input image as the output yields the highest scores on four out of five metrics, with the fifth, CLIP-T, exhibiting saturation effects. This underscores the importance of more nuanced automated metrics, such as VQA-based approaches, to better evaluate task completion.

% These additional metrics indicate that our model’s performance is robust and capable of generalizing beyond its proprietary data, demonstrating it is not simply overfitting.




\subsection{Human evaluation on \ours test set}

Methods like VIEScore\cite{ku2023viescore} align more closely with human judgment, but rely on vision-language models, which often miss subtle differences and produce inconsistent results. 
%Thus, human judgment remains the most dependable evaluation method.

\input{tables/ELO.tex}

% \noindent\textbf{Elo score with human preference.}
To counteract this, we conducted a qualitative evaluation using Elo scores, following the methodology from GenAI Arena~\cite{jiang2024genai} and LMSYS~\cite{zheng2023judging}. This evaluation, conducted via Amazon Mechanical Turk, involved pairwise comparisons against the baselines on 200 diverse images from our test set. Results in Table~\ref{tab:elo_rankings} demonstrate that our model outperforms baselines on human judgement.


% \ranjay{I elevated the reddit experiments to be their own subsection.}
\subsection{Deploying our model on Reddit}
One limitation of standard Elo evaluations is that they are conducted by individuals with no personal connection to the image. To ecologically validate the utility of our model with photo owners, we deploy our model back on Reddit. We provide editing services for new user requests, posting edited images in the comments per subreddit guidelines.

On multiple occasions, we received positive feedback.
For example, the model successfully removed red-eye from a photo. The original poster (OP) responded with: ``Thank you so much! Solved." and closed the request. On another occasion, we edited a picture of a car, and the OP remarked, ``It looks pretty good, man." On an edit of removing a person from the background, OP commented ``Wow this looks great! I love the way you smoothed out the lighting on me as well'' indicating that our model not only is successful semantically but produces aesthetic images.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figs/reddit_figure_2.pdf}
    \caption{\textbf{Real requests completed on Reddit.} We deployed our model on r/PhotoshopRequest to complete in-the-wild requests. We received positive feedback from users on the examples above.}
    \label{fig:reddit_figure}
    \vspace{-2mm}
\end{figure}

% The best measure of model performance is whether the request issuer is satisfied with the result. Different people often have different tastes for subjective editing tasks, and the image owner has the greatest stake and most relevant perspective on the edit’s success. To assess this, we turn to the original inspiration for this work: the Reddit community. We collect recent requests from r/estoration (and possibly r/PhotoshopRequests) and qualitatively analyze responses from other commenters as well as the original poster (OP). We find that [INSERT RESULTS HERE].

\subsection{Evaluations on existing test sets}

We also conduct evaluations on external test sets including the test sets in GenAI Arena~\cite{jiang2024genai}, Emu Edit~\cite{sheynin2024emu}, and MagicBrush~\cite{zhang2024magicbrush}.
On GenAI Arena, we report Elo ranking in Table~\ref{tab:elo_rankings} computed with real human preferences.
Our model ranks second among the evaluated models. While these results were insightful, we found the examples in GenAI Arena to be less representative of real-world tasks. 
%For instance, the first test case shows a picture of a zebra and has the instruction of: ``Give the zebra a single front leg''.
We include full automated evaluation results on Emu Edit and MagicBrush in Appendix, where our model performs competitively with the individual strongest models on respective test sets across varying metrics.
% We use the CLIP-based metrics proposed in their papers, with detailed results available in Appendix.
% The Emu Edit metrics are particularly limited, as they primarily measure similarity to the input image rather than ground truth, which does not accurately reflect task completion.

% We evaluate our model on the Emu Edit~\cite{sheynin2024emu} and MagicBrush~\cite{zhang2024magicbrush} test sets.
% We use the CLIP-based metrics proposed in their papers, with detailed results available in Appendix. The Emu Edit metrics are particularly limited, as they primarily measure similarity to the input image rather than ground truth, which does not accurately reflect task completion.
%\input{tables/mb_emu_metrics}



\subsection{Improving edited image detection}

\input{tables/truemedia_new}
\begin{figure}[!t]
\label{paris}
\centering\includegraphics[width=0.38\textwidth]{figs/edit_detection.PNG}
\caption{The baseline misclassifies both images as real, whereas our model correctly spots the fake (right) that spawned the 2005 Paris Hilton ``Stop Being Poor'' meme.}
\end{figure}

%Here, we discuss edited image detection as another use case of \RealEdit data.
We partnered with \truemedia, a platform where users can upload media to assess authenticity. Their primary fake image detection model is a fine-tuned version of Universal Fake Detect (UFD) ~\cite{ojha2024universalfakeimagedetectors}, which effectively detects model-generated deepfakes. 
%While effective at detecting model-generated deepfake images, UFD fails to reliably detect \textit{human-edited} images. 
We leverage the human-edited images in \RealEdit to enhance the model’s ability to detect such edits, which has significant real-world impact. 

UFD is trained on a recipe of 62K images from academic datasets~\cite{wang2023diffusiondblargescalepromptgallery, karras2019style, karras2020analyzingimprovingimagequality, karras2018progressivegrowinggansimproved, lin2015microsoftcococommonobjects, sdfd} and some proprietary data, none of which includes \textit{human edits}. We trained a model from scratch using the UFD training pipeline with added \RealEdit training data. We evaluated on the \RealEdit test set and on a random subset of 100 reals plus 100 in-the-wild edited images from \truemedia.
% less than 20\% of which involved AI-manipulation.
We show in Table~\ref{tab:ufd-combined} that fine-tuning on \RealEdit improves F1 by 45.5 and 14 points on \RealEdit and \truemedia's test sets respectively.
% This is likely because \truemedia’s model had limited exposure to manually edited images.
\RealEdit also serves as a challenging human-edited image detection benchmark for models that are more specialized for this task compared to deepfake detection~\cite{triaridis2024mmfusioncombiningimageforensic, zhang2023editguardversatileimagewatermarking, 6625374, Zhang_2024}.


% Fine-tuning UFD on \RealEdit improves F1 by 14\%, but there remains significant room for improvement. This highlights the difficulty of detecting in-the-wild edits, which often lack the traceable artifacts found in fully AI-generated images.




% Grad-CAM result would go here
% To understand how our model learned, we computed the saliency map of the above example using Grad-CAM \cite{Selvaraju_2019} and find that 

% \begin{figure}[H]
% \label{saliency}
% \centering\includegraphics[width=0.35\textwidth]{figs/edit_detection.PNG}
%     \caption{Saliency map of the Paris Hilton image computed with Grad-CAM}
% \end{figure}




\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figs/page_of_examples_leftalign.pdf}
    \caption{\textbf{Examples of the \RealEdit model} on \RealEdit test set images compared to other editing models. Our edits are often more semantically correct as well as more visually appealing.}
    \label{fig:examples}
\end{figure*}