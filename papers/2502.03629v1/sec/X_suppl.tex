% \documentclass[10pt,twocolumn,letterpaper]{article}
% \usepackage{}

% \begin{document}

\appendix

\renewcommand{\contentsname}{Table of contents}

\onecolumn
\noindent\rule{\textwidth}{1pt}
\begin{center}
\vspace{3pt}
{\LARGE\textbf{\textsc{RealEdit}: Reddit Edits As a Large-scale\\ Empirical Dataset for Image Transformations}}\\
\vspace{1em}
{\Large\textbf{Supplementary Material}}
\vspace{-3pt}
\end{center}
\noindent\rule{\textwidth}{1pt}
% \centering \LARGE\textbf{\textsc{RealEdit}: Reddit Edits As a Large-scale\\ Empirical Dataset for Image Transformations}\par\vspace{0.5em}\Large{Supplementary Material}\par\vspace{1em}

% \input{tables/test}
% \section*{Table of contents}
\tableofcontents

\clearpage

\twocolumn
\section{Data taxonomy}
\subsection{Full taxonomy}
We include the taxonomies of \ours (Figure \ref{fig:taxonomy_ours}), Emu Edit (Figure \ref{fig:taxonomy_emu}), and MagicBrush (Figure \ref{fig:taxonomy_mb}) test sets, as well as the unabridged comparison between all three (Figure \ref{fig:dist_us_both_full}). The prompt used to taxonomize these requests is included in Figure \ref{fig:taxonomy_prompt}. We notice \ours has a more diverse set of tasks as well as a more even distribution with greater focus in tasks like ``remove'' and ``enhance''. Emu Edit~\cite{sheynin2024emu} has a fairly even task distribution, though a smaller set of common tasks. MagicBrush~\cite{zhang2024magicbrush} has a very skewed distribution, with a high focus on ``add'' tasks which are not likely to be requested by human users, as humans generally include all desired elements when taking a photograph. 
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/taxonomy_ours_fixed_font.png}
    \caption{\textbf{Taxonomy of \ours image edit requests.} There is a wide variety of task types and edit subjects, with subtle tasks like ``remove'' and ``enhance'' being the most requested.}
    \label{fig:taxonomy_ours}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figs/taxonomy_emu_fixed_font.png}
    \caption{\textbf{Taxonomy of Emu Edit image edit requests.} There is a smaller range of task types than \ours, but the distribution is fairly even.}
    \label{fig:taxonomy_emu}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figs/taxonomy_mb_fixed_font.png}
    \caption{\textbf{Taxonomy of MagicBrush image edit requests.} There is a limited selection and extremely uneven distribution of task types, with ``add'' accounting for almost half of all requests.}
    \label{fig:taxonomy_mb}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/us_vs_both_horizontal.png}
    \caption{\textbf{Differences in the distribution} of our test set compared to MagicBrush and Emu Edit test sets. MagicBrush and Emu Edit tend to be similar in distribution to each other, but starkly different from \RealEdit.}
    \label{fig:dist_us_both_full}
    \vspace{-4mm}
\end{figure}
\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/taxonomy_prompt_fixed_font.pdf}
    \caption{\textbf{Prompt used for taxonomizing edit requests.} We passed this along with input images to GPT-4o.}
    \label{fig:taxonomy_prompt}
\end{figure}
\subsection{Performance across edit operations}
We show the VIEScore comparisons of \ours, AURORA~\cite{krojer2024learning}, InstructPix2Pix~\cite{brooks2023instructpix2pix} and MagicBrush~\cite{zhang2024magicbrush} in Table \ref{tab:vie_by_taxonomy}. We notice that in all of the editing tasks, the \ours model has the highest overall VIEScore. However, in ``add'' tasks, which comprise a much smaller percentage of our dataset compared to InstructPix2Pix and MagicBrush, we have a lower perceived quality, indicating that having more ``add'' data might improve the aesthetics. The task with the highest score for \ours is ``remove'', with a VIE\_O score of 4.35. The ``remove'' task comprises the largest portion of our dataset, which may explain this result. The hardest task is ``formatting'', the only operation for which we do not have the highest semantic completion score. This is due to the fact that this task is impossible for current models to fulfill properly, as changing file formats, resizing, etc. are not supported by current model architecture. 
\input{tables/vie_by_taxonomy}

% \clearpage

\section{Data processing}
\paragraph{Test set image captioning}

We caption all input and ground truth images in the test set to enable evaluations with models that require captions. The process involves two main stages. First, for input image captioning, we pass the processed instruction along with the input image to LLaVA-Next\cite{liu2024llavanext}. This generates a caption for the input image that integrates the instruction, emphasizing key aspects of the image relevant to the editing task.

For output image captioning, we pass the input caption and edit instruction to GPT-4o, which combines these elements to generate a caption for the ground truth (edited) image, reflecting both the original content and the changes made according to the instruction. Refer to Figure \ref{fig:test_set_figure} for examples of captions.



% Here are teh prompts.

% Here are some examples:

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figs/test_set_figure.pdf}
    \caption{Examples of test set data with captions for input image and ground truth image.}
    \label{fig:test_set_figure}
\end{figure*}

\section{Discussion}
\subsection{Limitations and future work}
\ours is collected from Reddit posts from 2012-2021. As such, we have less data and a danger of it getting outdated. We plan to regularly update our dataset to ensure that the edits reflect as current culture as much as possible. This will also help in edited image detection, by facilitating the detection of edits where newer AI tools were used, as the line between human editing and model editing is increasingly blurred. \\
We also filter our dataset in order to more closely match the training distribution, removing some natural diversity of human edit requests. In future work, we hope to explore different architectures capable of handling real world edit requests and editing styles. \\
The pretraining of the \ours model uses CLIP embeddings, which while very useful for semantic changes to an image, a large portion of the \ours dataset involves edits that do not involve semantic changes. Additionally, in edited image detection, some of the edits may not change the embeddings much. We urge future work to explore alternatives to such embeddings that may capture purely aesthetic changes.
\subsection{Social impacts}
The social impact of our dataset stems from both the effect on model training as well as the ability of our test set to be used to accurately and justly benchmark other models. The training data will inform how well the \ours model performs certain types of edits. The test set on the other hand determines the factors we incentivize in other models. \\
Accessible image editing models that are capable of handling real world tasks are extremely useful in democratizing the documentation of people's lives. For example, some requests in \ours involve restoring old photographs, many of which were paid. The \ours model can help more users to document meaningful family histories, even if they cannot afford to pay for edits. We have demonstrated the efficacy of our model on making real world edits by uploading our model's generations to Reddit. Additionally, our exploration of the contribution of \ours in deepfake image detection has shown that \ours increases the ability of \truemedia's ability to detect fake images, which is extremely useful in a world where images are routinely edited to cause scandals or spread misinformation. \\
There is a known issue in image generation models of generating images or making edits based on demographic biases such as smoothing wrinkles, lightening skin, and male bias in certain professions, which may offend users. Additionally, our dataset mirrors the demographic profile of Reddit users, who are predominantly Western, younger, male, and left-leaning, potentially influencing the types of images and editing requests included. We hope to study the effect of this extensively in \ours in future work.\\
There is also an issue of inappropriate edits, which we have mitigated to our knowledge in \ours through filtering of NSFW content using opennsfw~\cite{bhky_opennsfw2}, along with manual filtering in our test set. \\

\subsection{Ethics}
Some other editing datasets \cite{zhang2024magicbrush} do not use human faces in order to evade biases as well as privacy concerns. However, in \ours, we determine that since over half of edit requests contain images focused on people, we must train on human data in order to be successful in completing real world editing tasks. To mitigate privacy concerns, we use the URL in place of the actual input image so that if the original poster (OP) deletes their post, it will be removed from our dataset. We also include a form for users to request their data to be removed. In the case of mitigating biases, we hope in future work to study the effects of using Reddit data on task completion for a wide array of demographic groups, as well as techniques or supplementary data sources to boost performance on underrepresented groups. This is a known problem in the field, and we are compelled by user preferences to include human data. Given this, although we appreciate the importance of mitigating demographic biases, this is outside the scope of a single paper.

% \clearpage

\section{Modeling ablations}

\subsection{Implementation details}

We fine-tune the checkpoint of InstructPix2Pix~\cite{brooks2023instructpix2pix} using the \ours training set for 51 epochs on a single 80GB NVIDIA A100 GPU. The total batch size is 128, and the learning rate starts at $2 \times 10^{-4}$
. We resize images to 256 × 256, disable symmetrical flipping to maintain structural integrity, and apply a cosine learning rate decay to $10^{-6}$ over 15,000 steps with 400 warmup steps. The training process takes 24 hours.


\subsection{Consistency decoder}
\input{tables/consistency_decoder_metrics}

We integrate OpenAI’s Consistency Decoder~\cite{openai_consistencydecoder}, which is designed to enhance the quality of specific features during inference. This has a minimal impact on overall model performance metrics but proves highly effective for improving the handling of faces, textures, and intricate patterns. 

As the decoder operates independently of the underlying model, we evaluate its effectiveness with InstructPix2Pix\cite{brooks2023instructpix2pix} and MagicBrush\cite{zhang2024magicbrush} on a sample of 500 tasks. The results indicate that while the decoder minimally affects standard metrics, such as VIEScore\cite{ku2023viescore} and CLIP-T (Table \ref{tab:consistency_decoder_combined}), it often enhances the aesthetic quality in areas requiring fine detail, such as facial reconstruction and complex textures (Figure \ref{fig:decoder_figure}).

These findings demonstrate the decoder’s potential as a lightweight, inference-only addition to improve the output quality of existing image-editing models without altering their core architectures or diffusion processes.

% \clearpage

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/consistency_decoder_figure.pdf}
    \caption{Consistency decoder allows for more aesthetic generation of faces.}
    \label{fig:decoder_figure}
\end{figure*}

% \clearpage

\subsection{Data filtering}

We observe that human-generated edits often introduce substantial diversity, such as rearranging objects or people, which significantly impacts Structural Similarity Index Measure (SSIM) scores. These variations create a distributional mismatch with InstructPix2Pix’s pretraining data (Figures \ref{fig:ssim-ip2p}, \ref{fig:ssim-mb}, \ref{fig:ssim-us}), where edits are generally more constrained. To better understand this difference, we analyze SSIM distributions, highlighting the gap between human edits and the structured outputs of synthetic datasets.

To make our dataset more compatible with InstructPix2Pix, we currently apply SSIM-based filtering to exclude edits that deviate too far from the pretraining distribution. Following this, we use the same CLIP-based filtering methodology employed by InstructPix2Pix to further refine the data. We verify that this filtering leads to a more capable model using the VIE-scores (Table \ref{tab:filtered-data}) and CLIP-based metrics (Figure \ref{fig:filtered_data}). Our approach relies on thresholding to identify and remove outliers, but we recognize that soft sampling techniques could offer a more flexible and nuanced alternative. Exploring such methods remains a promising direction for future work.

\input{tables/filtered_data}


\subsection{Processing instructions}
Reddit users often provide vague, unclear instructions with unnecessary details, hindering the editing process. To address this, we refined these instructions for greater clarity and relevance. To evaluate the impact of this preprocessing, we trained two models under the same conditions: one with the original instructions and the other with the processed versions. Results in Table \ref{tab:process_insructions} and Figure \ref{fig:processed_instruction} show that these have a significant effect on model performance. 

We ran this experiment early in the development processes with a suboptimal training strategy and a smaller subset of the data, leading to much lower scores compared to our final model.

\input{tables/processed_instructions}

% \begin{flushleft}
% \newpage
% \end{flushleft}

\begin{figure}[b]
    \centering
    \includegraphics[width=\columnwidth]{figs/ssim-ip2p.png}
    \caption{SSIM distribution of InstructPix2Pix training data.}
    \label{fig:ssim-ip2p}
\end{figure}

\begin{figure}[b]
    \centering
    \includegraphics[width=\columnwidth]{figs/ssim-mb.png}
    \caption{SSIM distribution of MagicBrush training data.}
    \label{fig:ssim-mb}
\end{figure}

\begin{figure}[b]
    \centering
    \includegraphics[width=\columnwidth]{figs/ssim-us.png}
    \caption{SSIM distribution of \ours training data.}
    \label{fig:ssim-us}
\end{figure}

% \clearpage

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figs/filtered_data.pdf}
    \caption{Filtering the data massively improved CLIP-based metrics.}
    \label{fig:filtered_data}
\end{figure*}

% \clearpage



\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/instruction_rewriting_prompt.pdf}
    \caption{GPT-4o prompt for instruction rewriting.}
    \label{fig:instruction_rewriting}
\end{figure}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figs/processed_instruction.pdf}
    \caption{Processing instructions consistently yields better results on CLIP-based results.}
    \label{fig:processed_instruction}
\end{figure*}

\section{Inference time results}
\subsection{Hyperparameters}

We conducted several inference-time experiments: varying the number of diffusion steps, the image and text guidance scales, and further rewriting instructions with GPT-4o to add more details. 

See equation (6) in ~\cite{ho2022classifier} for the definition of classifer-free guidance scale. The conventional wisdom is that higher image guidance scale make the generated image look more similar to the original image, while higher text guidance scale improve instruction adherence. Additionally, higher number of inference steps are believed to improve the quality of the generated image at the expense of computational time. Our statistical experiments do not capture these relationships, and even demonstrate the opposite relationship in case of image guidance scale.

\paragraph{Number of inference steps}
We observe that 20 inference steps strike a good balance between the computational time and the image quality. Specifically, we find that the average CLIP similarity between the generated image and the most upvoted Reddit edit is approximately the same for any setting of inference steps above 20. See Figure \ref{fig:num_inference_steps} for the statistical plot and figure \ref{fig:inference_steps_example} for an example.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/hyperparameters/Star_wars_number_of_inference_steps.pdf}
    \caption{Increasing the number of diffusion steps above 20 usually does not improve the quality.}
    \label{fig:inference_steps_example}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/hyperparameters/num_inference_steps.png}
    \caption{The number of inference steps does not improve the generated image quality, as measured by the CLIP similarity between the generated image and the most upvoted Reddit edit.}
    \label{fig:num_inference_steps}
\end{figure}

\paragraph{Text guidance scale}
We observe \textbf{no correlation} ($\rho=.005$) between the text guidance scale in range $[1,14]$ and instruction adherence, as measured by CLIP similarity between the generated image and the caption describing the desired output. See Figure \ref{fig:text_guidance_scale}. While there is no correlation in aggregate, some individual edits may still change significantly with different text guidance scales, see Figure \ref{fig:guidance_scales_example} for such an example.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/hyperparameters/Frozen_lake_guidance_scales.pdf}
    \caption{An example where guidance scales behave as expected.}
    \label{fig:guidance_scales_example}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/hyperparameters/text_guidance_scale_val_0_14.png}
    \caption{Text guidance scale has no effect on instruction adherence, as measured by the CLIP similarity between the generated image and the caption of the expected output, as in figure \ref{fig:test_set_figure}.}
    \label{fig:text_guidance_scale}
\end{figure}

\paragraph{Image guidance scale}
The generated image quality decreases sharply if the image guidance scale is above 3. Inside the $[1,3]$ range, the image scale makes little difference in aggregate. Counter-intuitively, we observe a \textbf{negative} correlation ($\rho=-.106$) between image guidance scale and CLIP similarity between the input and generated images. In other words, higher image guidance values result in \textbf{less similar} images on average, which contradicts conventional assumptions about guidance scales and warrants further investigation. See Figure \ref{fig:image_guidance_scale}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/hyperparameters/image_guidance_scale_val_set_0_3.png}
    \caption{Increased image guidance scale results in \textbf{less} similar images, as measured by CLIP similarity between the input and generated images.}
    \label{fig:image_guidance_scale}
\end{figure}

\subsection{Instruction rewriting} 
As the diffusion model lacks reasoning capabilities, it often fails when asked to interpret abstract or creative instructions. To improve outcomes on these examples, we employ a large language model (LLM) to rewrite instructions in a more specific manner, similar to Dalle-3 ~\cite{betker2023dalle3}. Since only creative edit tasks benefit from this technique, we do not make this part of our main pipeline. We gave the input image and the original instruction to GPT-4o with the prompt ``\textit{You are given an image editing instruction. If the instruction is already concrete and specific, do not rewrite it at all. If the instruction is vague or does not make sense for the image, then rewrite it. Make the new instruction specific and detailed, e.g. do not use words 'enhance', 'adjust', 'any'.}"

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figs/instruction_rewriting/RealEdit_instruction_rewrite_examples_v1.pdf}
    \caption{Detailed instructions can improve edit quality on certain classes of tasks.}
    \label{fig:instruction_rewrite}
\end{figure*}



\subsection{Quantitative evaluation on external test sets}

\input{tables/mb_emu_metrics}

Despite being out of distribution, the \ours model performs comparably to other models on the synthetic datasets Emu Edit~\cite{sheynin2024emu} and MagicBrush~\cite{zhang2024magicbrush}. On several metrics (VQA\_CLIP and TIFA on MagicBrush and VQA\_llava, VQA\_Flan-t5 and TIFA on Emu Edit), the \ours model is within 1 standard deviation of the highest scoring model, indicating that it is fairly generalizable to new tasks. 

\subsection{Elo scores}

To evaluate Elo scores, we leverage Amazon Mechanical Turk (MTurk) for conducting pairwise comparisons. We selected 200 diverse examples from our dataset to ensure coverage of various editing tasks and performed comparisons across all seven models in our benchmark. This process resulted in a total of 4,200 pairwise evaluations, providing a robust dataset for assessing human preferences. We present a table of pairwise winrates (Figure \ref{fig:elo_heatmap})  

In addition to evaluating our dataset, we extended our analysis to the Imagen Hub Museum\cite{ku2024imagenhub} tasks, building on the results from the GenAI Arena\cite{jiang2024genai}. Using their generations, available on HuggingFace, we incorporated results from our model to facilitate direct comparisons. For these evaluations, we conducted a new round of pairwise comparisons where we matched one model from their benchmark against our model for the same tasks. This allowed us to directly assess how our model performs relative to state-of-the-art models on external datasets.

The evaluations on MTurk followed a structured protocol to ensure reliability and consistency. Workers were asked to compare image outputs based on task completion, realism, and alignment with instructions. The use of MTurk enabled us to gather diverse human feedback efficiently and at scale. The full results are presented in Table \ref{tab:elo_genai}, highlighting the comparative performance across different models.

\input{tables/elo_genai}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/elo_heatmap_200.png}
    \caption{\textbf{Heatmap of pairwise winrates on our test set.} We excluded draws for this heatmap. }
    \label{fig:elo_heatmap}
\end{figure*}

% \clearpage

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/mturk_interface.pdf}
    \caption{\textbf{Interface for Elo evaluation on MTurk}. To complete Elo evaluations, we hired workers on Amazon Mechanical Turk to compare the quality of different editing models.}
    \label{fig:mturk_interface}
\end{figure*}

% \clearpage

\section{Reddit experiment}

To evaluate the generalization capability of our model, we deployed it on Reddit. Specifically, we targeted two subreddits: r/PhotoshopRequest and r/estoration, which focus on image editing and restoration tasks. Adhering to the community guidelines of these subreddits, we collected posts requesting image edits and processed them using our model.

For each processed request, we submitted a comment containing the generated output image along with a brief message asking for feedback from the original poster (OP). With this experiment, we gathered qualitative evaluations from humans, and provide insight into the model's performance in real world scenarios. See Figures \ref{fig:reddit_dog}
and \ref{fig:reddit_man}.  
\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/reddit_dog.pdf}
    \caption{\textbf{Our model successfully completes new requests on Reddit.} Deployed on the original subreddits, it handled in-the-wild requests effectively as seen by OP's response.}
    \label{fig:reddit_dog}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figs/reddit_figure_man.pdf}
    \caption{\textbf{Our model successfully completes new requests on Reddit.} Deployed on the original subreddits, it handled in-the-wild requests effectively as seen by OP's response.}
    \label{fig:reddit_man}
\end{figure}

\clearpage
\section{Edited image detection}

% \subsection{Implementation details}

% \noindent\textbf{Model architecture}

% Universal Fake Detect (UFD) processes images into latent representations using CLIP and performs binary classification through linear probing. The original model by ~\citet{ojha2024universalfakeimagedetectors} employs a single linear layer, whereas \truemedia enhances the architecture by adding a second linear layer with a ReLU activation in between. \truemedia continues to utilize the CLIP:ViT-L/14 encoder with a resolution of 224x224. The input dimension of the first linear layer is 768, and the second linear layer has a dimension of 384. The classifier predicts an image as fake (positive class) when the output logit exceeds a threshold of 0.5.

% Universal Fake Detect (UFD) works by processing images into latent representations via CLIP and then applying linear probing for binary classification. The original model trained by ~\citet{ojha2024universalfakeimagedetectors} uses one linear layer while \truemedia adds a second linear layer with a ReLU in between;\truemedia also continues using the 224 CLIP:ViT-L/14 encoder. The input dimension for the first linear layer is 768 and the second layer has dimension 384. The model makes a fake (positive class) prediction when the single output logit exceeds the threshold of 0.5. \\

% This is the only architectural difference. 

\paragraph{Data processing and training}
\label{UFD_training}
The baseline classifier undergoes a multi-stage training process: initially on academic datasets and subsequently fine-tuned on \truemedia's proprietary data. In total, the baseline model is trained on 65K images with a near equal 50/50 split between real and generated images. To assess the value of \ours data for fake image detection, we train a second version of UFD by combining the original data with \ours data. Specifically, we include only photographs, excluding non-photographic images such as digital artworks, screenshots, cartoons, and infographics, filtered using GPT-4o. This single-stage training incorporates an additional 37K original and 37K edited images, resulting in a total of 139K images.

In the first stage of training, the \truemedia model took over 24 hours to train on an A10G GPU with 20GB of RAM and the remaining three stages took 4 hours. Our optimized model took 1.5 hours to train on a L40S GPU with 40GB of RAM.\\

% To train and evaluate UFD, we used GPT4o to filter out all non-photographic images from the \RealEdit train set. These includ digital artworks, screenshots, cartoons, and infographics. 

% Our competing model was trained in a single stage, combining the entire filtered \RealEdit train set with the data described in Tables \ref{tab:fake-sources} and \ref{tab:real-sources}, for a total of 68K images. 

% To train and evaluate UFD, we filter out all non-photographic original images from the \RealEdit train and test sets using GPT4o. These include digitally created artworks, screenshots, cartoons, and infographics. as \truemedia's UFD was trained to classify these types of images as "edited". 37K and 7K real images remained in the \RealEdit train and test sets, respectively. In total, our fine-tuned version of UFD saw 136K images. \\


% . This filtering was necessary because \truemedia's UFD classifies these types of images as "edited," due to their presence in the user-uploaded fake data, where such images—like fake infographics and memes—are common

% To train and evaluate UFD, we filter out all non-photographic original images from the \RealEdit train and test sets using GPT4o. These include digitally created artworks, screenshots, cartoons, and infographics, as \truemedia's UFD learned to predict these types of images as fake a sizable portion of user-uploaded fakes, which were then trained on (see \ref{UFD_training}). 37K and 7K real images remained in the \RealEdit train and test sets, respectively. In total, our fine-tuned version of UFD saw almost 100K real and fake images. \\




% The baseline UFD from \truemedia is trained on 62K images (50/50 ratio) with the real and fake breakdowns described in \ref{tab:fake-sources} and \ref{tab:real-sources}. 

% Note that the distribution of images uploaded to \truemedia has a skewed distribution of 25/75 real/fake. The baseline was trained in multiple stages: first in 6/2024 with the academic datasets, then fine-tuned on \truemedia's proprietary data from 2/20/2024-6/15/2024, then with data from 6/16/2024-7/15/2024, and lastly with 7/15/2024-8/15/2024 uploads. Our competing model was trained in a single stage combining the entire filtered \RealEdit train set with the the data described in \ref{tab:real-sources} and \ref{tab:fake-sources} for a total of 68K images.

% The distribution of images uploaded to \truemedia is skewed, with a 25/75 real/fake ratio. 


% The baseline UFD from \truemedia is trained on 62K images (50/50 ratio) with the real and fake breakdowns described in \ref{tab:fake-sources} and \ref{tab:real-sources}. Note that the distribution of images uploaded to \truemedia has a skewed distribution of 25/75 real/fake. The baseline was trained in multiple stages: first in 6/2024 with the academic datasets, then fine-tuned on \truemedia's proprietary data from 2/20/2024-6/15/2024, then with data from 6/16/2024-7/15/2024, and lastly with 7/15/2024-8/15/2024 uploads. Our competing model was trained in a single stage combining the entire filtered \RealEdit train set with the the data described in \ref{tab:real-sources} and \ref{tab:fake-sources} for a total of 68K images.

\begin{table}[h]
    \centering
    \caption{Breakdown of fake image sources in the training recipe of the \truemedia model used as our baseline.}
    \label{tab:fake-sources}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{l|c}
        \toprule
        \textbf{Source} & \textbf{Count} \\ \midrule
        DiffusionDB ~\cite{wang2023diffusiondblargescalepromptgallery} & 16K \\ 
        StyleGAN2-FFHQ ~\cite{karras2020analyzingimprovingimagequality} & 8K \\ 
        Stable-Diffusion-Face ~\cite{sdfd} (512 resolution) & 2.4K \\ 
        Stable-Diffusion-Face (768 resolution) & 2.4K \\ 
        Stable-Diffusion-Face (1024 resolution) & 2.4K \\ 
        Fakes uploaded to \truemedia & 2K \\ \bottomrule
        % (2/20–8/15)
    \end{tabular}%
    }
\end{table}

\begin{table}[h]
    \centering
    \caption{Breakdown of real image sources in the training recipe of the baseline model.}
    \label{tab:real-sources}
    \resizebox{\linewidth}{!}{%
        \begin{tabular}{l|c}
        \toprule
        \textbf{Source} & \textbf{Count} \\ \midrule
        CelebA-HQ (Reals) ~\cite{karras2018progressivegrowinggansimproved} & 23K \\ 
        Random sample of COCO-Train-2017 ~\cite{lin2015microsoftcococommonobjects} & 5K \\ 
        Flickr-Faces-HQ Dataset (FFHQ) ~\cite{karras2019style} & 3K \\ 
        Reals uploaded to \truemedia  & 0.7K\\ \bottomrule
        % (2/20–8/15)
        \end{tabular}%
        }
\end{table}

% The hyperparameters for the baseline model are: batch size of 512, learning rate of 0.0001, 10 epochs, Adam optimizer with momentum ($\beta$) of 0.9, and a loss weight of 100. Data augmentation for the baseline included horizontal flipping and Gaussian noise with a $(0.0, 3.0)$ signal at a probability of 0.5. When fine-tuning on \RealEdit data, a batch size of 2048 was used, with data augmentations disabled, while the other hyperparameters remained unchanged.

% The baseline model uses the following hyperparameters: a batch size of 512, a learning rate of 0.0001, 10 epochs, the Adam optimizer with a momentum ($\beta$) of 0.9, and a loss weight of 100. Data augmentation includes horizontal flipping and Gaussian noise with a $(0.0, 3.0)$ signal, applied with a probability of 0.5. For fine-tuning on \RealEdit data, the batch size is increased to 2048, and data augmentations are disabled, while all other hyperparameters remain unchanged.

% The hyper-parameters for the baseline model are: batch size of 512, learning rate of 0.0001, 10 epochs, Adam optimizer with momentum ($\beta$) of 0.9, and a loss weight of 100.

% For the baseline model, data augmentation included horizontal flipping and Gaussian noise with a $(0.0, 3.0)$ signal at a probability of 0.5. When fine-tuning the baseline on \RealEdit data, a batch size of 2048 was used, with data augmentations disabled. The remaining hyperparameters were kept unchanged.

% \noindent\textbf{Hardware}



% \noindent\textbf{\truemedia's In-the-wild test set}

\paragraph{\truemedia's in-the-wild test set}

\truemedia's in-the-wild test set includes images uploaded between 8/16/2024 and 11/10/2024. We randomly sample 100  real images and then sample 100 fake images selected from those tagged as "likely photoshopped" by professional sleuths in \truemedia's media database, ensuring the evaluation focuses on human-edited images rather than exclusively AI-generated edits. Tool usage data was available for some images, revealing that approximately 80\% of the fake images were human edits created with Photoshop, while the remaining 20\% involved human edits combined with AI tools such as Dream Studio AI, Insightface AI, and Remaker AI.

\paragraph{Qualitative example}

To understand how the classifier operates, we use Gradient-weighted Class Activation Mapping (Grad-CAM) \cite{Selvaraju_2019} to analyze an example. In Figure~\ref{fig:bear_camera_crew}, we show an edited image where a bear was added to the background using Photoshop. The original image did not include the bear. The baseline model incorrectly classified this photo as unedited, whereas the classifier trained with \ours data correctly identified it as edited. Grad-CAM highlights the areas of the image most influential to the classifier’s decision, as seen in the figure, where the focus is on the region around the bear. The specific implementation we adapted is from \citet{jacobgilpytorchcam}.

% We used \citet{jacobgilpytorchcam}'s implementation

% \truemedia's in-the-wild test set consists of 100 randomly sampled real images from images uploaded to 8/16/2024-11/10/2024. The 100 fake images were randomly sampled from the set of images that professional sleuthers tagged as ``likely photoshopped" in \truemedia's media database. While it's unknown what specific tools were used for every image, some users who uploaded images provided this information: from this, 80\% of the fake images were human edits from Photoshop where as the other 20\% are human edits combined with the use of AI editing tools such as Dream Studio AI, Insightface AI, and Remaker AI. The dataset encapsulates 15\% political images with faces and 85\% non-political images with and without faces. An example of a viral political edit which is contained in this set is the doctored Trump assassination photo where the body guards facial expressions were changed to smiles. \\



% \truemedia's in-the-wild test set consists of 100 randomly sampled real images uploaded between 8/16/2024 and 11/10/2024. The 100 fake images we used were randomly selected from those tagged as ``likely photoshopped" by professional sleuths in \truemedia's media database in order to ensure that we are measuring accuracy on human-edited images rather than solely AI-edited images. Some users provided data on the tools used. Of the fake images, 80\% were human edits made with Photoshop, while the remaining 20\% involved human edits combined with AI tools such as Dream Studio AI, Insightface AI, and Remaker AI. The dataset includes 15\% political images with faces and 85\% non-political images, both with and without faces. One notable example is the doctored Trump assassination photo, where the bodyguards' facial expressions were altered to smiles.

% We evaluated the two models side by side on two test sets: the test set from \ours and the in-the-wild test set collected by \truemedia. \truemedia's test set includes 100 original and 100 generated/edited images created using tools like Photoshop, Dream Studio AI, InsightFace AI, and Remaker AI. Our data proved valuable, as it improved F1 scores on both test sets. While there is a tradeoff between precision and recall, the overall performance gain justifies the approach. Detailed evaluation results are presented in Table~{REFERENCE}.  

% Additionally, we analyze an edited image missed by the baseline models but correctly detected by our models using Gradient-weighted Class Activation Mapping (Grad-CAM). See the example in {REFERENCE} for further details.

% The 100 fake images we used were randomly selected from those tagged as ``likely photoshopped" by professional sleuths in \truemedia's media database in order to ensure that we are measuring accuracy on human-edited images rather than solely AI-edited images. Some users provided data on the tools used. Of the fake images, 80\% were human edits made with Photoshop, while the remaining 20\% involved human edits combined with AI tools such as Dream Studio AI, Insightface AI, and Remaker AI. The dataset includes 15\% political images with faces and 85\% non-political images, both with and without faces. One notable example is the doctored Trump assassination photo, where the bodyguards' facial expressions were altered to smiles.


% \subsection{Discussion}

% \noindent\textbf{Complete metrics}

% \input{tables/truemedia_metrics_sup}

% \noindent\textbf{Interpretability with Grad-CAM}

% We used Grad-CAM (Gradient-weighted Class Activation Mapping) \cite{Selvaraju_2019} to visualize which regions most influenced our model's prediction for an image that \truemedia's baseline didn't detect as fake. See Figures \ref{fig:bear_camera_crew} and \ref{fig:bear_camera_crew_gradcam} \\

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/bear.pdf}
    \caption{Top: An edited image that inserted a bear to make it seem the camera crew was being chased. Bottom: Grad-CAM heat-map visualization highlighting the regions of attention.}
    \label{fig:bear_camera_crew}
\end{figure}


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\columnwidth]{figs/bear_camera_crew.jpg}
%     \caption{An edited image that inserted a bear to make it seem the camera crew was being chased.}
%     \label{fig:bear_camera_crew}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\columnwidth]{figs/GradCamBear.jpg}
%     \caption{Grad-CAM heat-map visualization where the region surrounding the bear has high intensity demonstrating our fine-tuned model learned to pick up some artifacts left by edits.}
%     \label{fig:bear_camera_crew_gradcam}
% \end{figure}

% \noindent\textbf{Future work}

% Detecting edited images, particularly those created in-the-wild, presents unique challenges distinct from traditional deepfake detection. However, as image editing increasingly integrates AI tools such as inpainting, the boundary between human-edited images and deepfakes is blurring. This evolution calls for new approaches that unify these detection tasks.

% One direction is to develop models specifically tailored to detect both deepfakes and human-edited images, leveraging training paradigms that incorporate not only ground-truth labels for edits but also masks indicating the regions of modification. This dual supervision could enhance interpretability, allowing models to localize edits and better distinguish subtle manipulations.

% % Another avenue involves improving dataset quality. Current datasets, like \RealEdit, focus on pre-2021 techniques and may not fully represent modern workflows incorporating AI-assisted tools, such as Adobe’s generative fill, which emerged in 2023. Expanding datasets to include these technologies would better reflect real-world editing practices.

% Finally, the similarity of image embeddings for edited and original images underscores the need for feature representations that are more sensitive to localized changes. Integrating interpretable model outputs, such as saliency maps or edit-specific attention mechanisms, could help identify subtle, context-dependent edits while maintaining robust performance on deepfake detection.

% By addressing these gaps, future research can bridge the divide between detecting fully AI-generated content and more nuanced human-AI collaborative edits, paving the way for models that excel in both domains.

\onecolumn
\section{Additional results}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth, height=0.8\textheight, keepaspectratio]{figs/page_of_examples_2_no_leashcat.pdf}
    \caption{\textbf{Additional examples of \ours generations on \ours test set} compared to all other baseline models. We notice that the \ours model consistently outperforms other models in task completion as well as aesthetic quality.}
    \label{fig:generation_examples_2}
\end{figure}

\begin{figure*}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{figs/3rd_page_of_examples.pdf}
    \caption{\textbf{Additional examples of \ours generations on \ours test set} compared to select high performing baseline models. We notice that the \ours model consistently outperforms other models in task completion as well as aesthetic quality.}
    \label{fig:generation_examples_3}
\end{figure*}

\twocolumn
% \end{document}