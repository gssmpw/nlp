\section{\RealEdit dataset analysis}
\label{sec:dataset_analysis}
% We looked at requests submitted on reddit
% We decided to analyze what kind of requests they are
% What did we use?
% gpt 4o to taxonomize promtps
% Here is what we noticed people care about.
% We see that models are unable to perform these requests. 
% some tasks are impossible right now. Like make a vector. Resizing is imporssible for many models too. 

\RealEdit provides insight into practical applications of image editing by analyzing real-world requests. We observe notable differences between \RealEdit and existing datasets including InstructPix2Pix~\cite{brooks2023instructpix2pix}, MagicBrush~\cite{zhang2024magicbrush}, Emu Edit~\cite{sheynin2024emu}, HIVE~\cite{zhang2024hive}, Ultra Edit~\cite{zhao2024ultraedit}, AURORA~\cite{krojer2024learning}, Image Editing Request~\cite{tan2019expressing} and GIER~\cite{shi2020benchmark}. 
While we focus primarily on differences with MagicBrush~\cite{zhang2024magicbrush} and Emu Edit~\cite{sheynin2024emu} in the following discussions, these observations broadly apply across datasets used to train image editing models. Figure~\ref{fig:dist_us_both} details the main differences. %We summarize the most important distinctions below.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\linewidth]{figs/taxonomy_ours_fixed_font.png}
%     \caption{Taxonomy of image edit requests in our dataset. There is a wode variety of task types and edit subjects, with subtle tasks like ``remove'' and ``enhance'' being the most requested. \ranjay{I would be ok with moving this to the supplementary if we need more space.}}
%     \label{fig:taxonomy_ours}
% \end{figure}

\paragraph{Qualitative analysis and taxonomy.} %We qualitatively determine the primary subject in a sample of 500 input images from \RealEdit. 
We create a taxonomy of image editing tasks people have requested. This involves (1) categorizing our edit requests into \textit{operations}, (2) subcategorizing requests by \textit{subject} of the edit, and (3) prompting GPT-4o~\cite{openai2023gpt4} to categorize the request based on the input image and edit instruction. To determine the operations, we modify the MagicBrush~\cite{zhang2024magicbrush} set of operations for clarity. We base our possible subjects on the significant categories from the sample of 500 images. We tune our GPT-4o prompt using samples of 100 data points to ensure accuracy, then validate on a separate sample to avoid overfitting. We find that both the categories and their distribution differ greatly from prior work. Since our categorizations are fairly similar to MagicBrush~\cite{zhang2024magicbrush} and Emu Edit~\cite{sheynin2024emu} test sets, we run our taxonomy on these test sets and highlight key distributional differences in Figure~\ref{fig:dist_us_both}. Full taxononomies and comparisons are listed in Appendix.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/us_vs_both_abridged.png}
    \caption{\textbf{Key differences in the distribution} of our test set compared to MagicBrush and Emu Edit test sets. MagicBrush and Emu Edit tend to be similar in distribution to each other, but starkly different from \RealEdit.}
    \label{fig:dist_us_both}
    \vspace{-4mm}
\end{figure}

\paragraph{Differences in edit operations.}
Synthetic datasets contain a greater use of ``add'' requests (36\% less than MagicBrush than). In contrast, real-life photos typically contain the intended objects within the frame, with many of \RealEdit's semantically focused tasks involving the \textit{removal} of unintended elements, such as strangers in the background, shadows on faces, or cars on the street.
% \RealEdit contains a larger proportion of ``remove'' requests (diff. of 23\% MagicBrush and 16\% Emu Edit).
Additionally, there are numerous cases where input images are semantically aligned with the owner's intent, but errors in photography such as bad lighting, motion blur, or graininess.
Following this, \RealEdit contains more ``enhance'' requests (14\% greater than MagicBrush and Emu Edit) compared to existing datasets. These findings indicate that real users often prioritize \textit{subtler requests}, whereas synthetic datasets are dominated by larger semantic changes, such as ``add."

\paragraph{Differences in image content.}  Analysis on 500 samples reveals that around 55\% of the input images feature \textit{people} as the main subject. Consequently, the subjects of the requested edits are more likely to be people (13\% more than Emu Edit), and less likely to be man-made objects (20\% less than MagicBrush).
Animals and media (characters, movie/book posters, memes, etc.) are the next most common categories, comprising about 10\% of the test set each. Common media requests include restoring old photographs, participating in fandoms, making memes, or other forms of online entertainment.
The fixation on media is not paralleled in other datasets (15\% more than Emu Edit). 
These findings reveal a clear difference: Reddit users tend to prioritize \textit{personal significance} by including people and \textit{entertainment} by incorporating media, and synthetic datasets often fail to reflect these preferences accurately.



Given the substantial distributional differences of \RealEdit compared to existing datasets, we demonstrate in Section~\ref{sec:results} that current models struggle to perform well on real-world requests.

% \noindent\textbf{Additional considerations.} In \RealEdit, approximately 36\% of images are 1080p, indicating that humans are more concerned with editing higher resolution images.  Instructions exceed 77 tokens in approximately 3.4\% of cases. Editing models should cater to such preferences in resolution and instruction length in order to better serve human users.

% Given the substantial distributional differences of \RealEdit compared to existing datasets, we demonstrate in Section \ref{sec:results} that current models struggle to perform well on editing tasks within the \RealEdit dataset.

% Considering the significant differences in distribution of \RealEdit compared to existing models, we will prove in Section \ref{sec:results} that existing models cannot perform well on editing tasks in the \RealEdit dataset.

