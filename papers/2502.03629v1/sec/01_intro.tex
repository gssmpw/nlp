

\section{Introduction}

The need to edit photos is more important than ever—people everywhere seek to perfect, enhance, or restore their images, from casual snapshots to treasured memories. 
If more effective and aligned editing models were readily available, many would use them for a variety of purposes: to remove an unwanted photobomber, adjust lighting in their selfies, restore their grandparents' wedding photos, or even add creative effects.
% Text-guided image editing has gained remarkable popularity by offering users an intuitive way to specify these transformations: through natural language prompts~\cite{brooks2023instructpix2pix, zhang2024magicbrush}.
This demand is vividly demonstrated in online communities like Reddit’s \textit{r/PhotoshopRequest}\footnote{\url{https://www.reddit.com/r/PhotoshopRequest}} and \textit{r/estoration}\footnote{\url{https://www.reddit.com/r/estoration}}, with over 1.5 million combined members. Many users pay money for quality edits, highlighting the demand for advanced, user-friendly editing tools. 


Despite the impressive capabilities in image generation and modification led by recent advancement of diffusion models~\cite{Ramesh2022DALLE2, rombach2022high, brooks2023instructpix2pix, zhang2024magicbrush}, seemingly straightforward real-world editing tasks, like ones from the Reddit's \textit{r/PhotoshopRequest}, continue to pose significant challenges to existing models.
For instance, while existing models are effective at artistic transformations or generating stylized content~\cite{ zhang2024magicbrush, mokady2023null, krojer2024learning, meng2021sdedit, sheynin2024emu}, they fall short at some of the most common real-world requests such as restoring a damaged image (see Figure~\ref{fig:soldier}). 
This discrepancy highlights a critical misalignment between the capabilities of current editing models and the actual needs of users.

One major challenge for models to effectively tackle real-world image editing is the diversity and open-ended nature of the tasks involved. However, most existing models are trained with synthetic or arbitrarily created datasets that do not characterize human-centered objectives well, as is shown in Table \ref{tab:dataset_comparison}.
For example, in Ultra-Edit~\cite{zhao2024ultraedit}, ``adding a rainbow'' to an image constitutes a significant portion of the data set. As a result, models trained on these datasets struggle to address the practical needs of real-world users.



In this work, we introduce \ours, a large-scale text-guided image editing dataset meticulously compiled from Reddit. \ours, by design, more faithfully reflects the distribution of image editing needs.
Specifically, we source image editing requests from two of the largest relevant subreddit communities, \textit{r/PhotoshopRequest} and \textit{r/estoration}, into a dataset consisting of over $57$K editing examples, wherein each example comprises of an input image, an instruction, and one or multiple edits performed by humans. Overall, there are a total of $151$K input and edited images in this collection.
By carefully preprocessing and filtering out ambiguous and noisy examples with meticulous manual verification, we transform part of the collected examples in \ours into an evaluation set that consists of more than $9.3$K real-world image editing requests to test models' capability.
Notably, \ours evaluation set shows that real-world requests differ drastically from existing evaluation datasets~\cite{zhang2024magicbrush, sheynin2024emu}, on which existing models struggle.

To build an effective image editing model for real-world tasks, we finetune a new text-guided image editing model, on \ours's training examples. To produce useful edits that preserve the identities of the people in photos, we upgrade InstructPix2Pix~\cite{brooks2023instructpix2pix} by replacing its Stable Diffusion~\cite{rombach2022high} decoder with OpenAI's Consistency decoder~\cite{openai_consistencydecoder}, which was pretrained on more human-centric data.


\begin{figure*}[!h]
    \centering
    \includegraphics[width=\textwidth]{figs/figure_2_soldier.pdf}
    \caption{Baselines struggle on simple, practical tasks, such as restoring a damaged photograph. Our model is successful.}
    \label{fig:soldier}
\end{figure*}



Our model demonstrates significantly better performance than existing state-of-the-art models on \ours's test set with a human preference (N=4,196) Elo score of $1184$, beating the next best model by $165$ points. We also outperform existing models using automated metrics: our model achieves $4.61$ VIEScore versus the next best score of $2.4$ (amongst other metrics).
Moreover, our model still remains competitive with MagicBrush and EmuEdit~\cite{zhang2024magicbrush, sheynin2024emu} on their test set. We further validate our model by completing new Reddit requests and receiving positive feedback.

Finally, we partner with \textless{}REDACTED\textgreater{}, a non-profit aimed at AI-generated content detection. By adding human-made edits from our dataset, we improve their model's F1-score by $14$ points. Our ecologically valid experimentation highlights the dataset's value outside of editing tasks.
