\begin{table*}[t!]
    \centering
    \caption{\textbf{Quantitative evaluation} on \ours test set. On all metrics other than pixel distance, the \ours model scores the highest.}
    \label{tab:our_benchmark_quantitative}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{lcccccc p{0.01cm} ccccc}
    \toprule
    \textbf{Model} & \textbf{VIES\_O $\uparrow$} & \textbf{VIE\_PQ $\uparrow$} & \textbf{VIE\_SC $\uparrow$} & \textbf{VQA\_llava $\uparrow$} & \textbf{VQA\_Flan-t5 $\uparrow$} & \textbf{TIFA $\uparrow$} & & \textbf{L1 $\downarrow$} & \textbf{L2 $\downarrow$} & \textbf{CLIP-I $\uparrow$} & \textbf{DINO-I $\uparrow$} & \textbf{CLIP-T $\uparrow$} \\
    \cmidrule(lr){1-7} \cmidrule(lr){9-13}
    AURORA~\cite{krojer2024learning}            & 2.20 & 3.43 & 2.40 & 0.606 & 0.711 & 0.724 & & 0.154 & 0.069 & 0.793 & 0.733 & 0.246 \\
    HIVE~\cite{zhang2024hive}                   & 1.73 & 3.40 & 1.86 & 0.596 & 0.678 & 0.685 & & 0.246 & 0.142 & 0.743 & 0.646 & 0.250 \\
    InstructPix2Pix~\cite{brooks2023instructpix2pix} & 1.64 & 3.12 & 1.76 & 0.594 & 0.650 & 0.698 & & 0.181 & 0.073 & 0.752 & 0.638 & 0.244 \\
    MagicBrush~\cite{zhang2024magicbrush}      & 1.87 & 3.88 & 1.89 & 0.620 & 0.726 & 0.741 & & \textbf{0.138} & \textbf{0.064} & 0.830 & 0.782 & 0.251 \\
    Null-text Inv.~\cite{mokady2023null}       & 1.89 & 3.27 & 2.14 & 0.637 & 0.751 & 0.731 & & 0.152 & 0.067 & 0.743 & 0.669 & \textbf{0.261} \\
    SDEdit~\cite{meng2021sdedit}              & 0.59 & 1.47 & 0.75 & 0.588 & 0.653 & 0.703 & & 0.156 & 0.068 & 0.678 & 0.613 & 0.230 \\
    \textbf{RealEdit (Ours)}                   & \textbf{3.68} & \textbf{4.01} & \textbf{4.61} & \textbf{0.660} & \textbf{0.795} & \textbf{0.751} & & 0.143 & 0.066 & \textbf{0.840} & \textbf{0.792} & \textbf{0.261} \\ 
    \bottomrule
    \end{tabular}%
    }
\end{table*}
