% \setlength{\tabcolsep}{1pt}
\begin{table*}[t!]
\centering
\caption{Comparison of Methods on Emu Edit and MagicBrush Test Sets. We use their metrics.} 
\label{tab:mb_emu_benchmarks}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccc|ccccc}
\toprule
 & \multicolumn{5}{c|}{Emu Edit Test Set} & \multicolumn{5}{c}{MagicBrush Test Set}\\
\midrule
Method & $\text{CLIP}_{dir}\!\uparrow$ &  $\text{CLIP}_{im}\!\uparrow$ & $\text{CLIP}_{out}\!\uparrow$ &  $\text{L1}\!\downarrow$  & DINO$\uparrow$ & $\text{L1}\!\downarrow$ &  $\text{L2}\!\downarrow$ & CLIP-I $\uparrow$ & DINO-I $\uparrow$ & CLIP-T $\uparrow$ \\
\midrule
InstructPix2Pix~\cite{brooks2023instructpix2pix} & 0.070 & 0.834 & 0.218 & 0.118 & 0.759
                & 0.118 & 0.040 & 0.844 & 0.731 & 0.266 \\
MagicBrush~\cite{zhang2024magicbrush}      & 0.053 & \textbf{0.913} & 0.220 & \textbf{0.047} & \textbf{0.923}
                & \textbf{0.072} & 0.026 & \textbf{0.917} & \textbf{0.872} & 0.268\\
AURORA~\cite{krojer2024learning}      & 0.071 & 0.888 & 0.225 & 0.057 & 0.896
                & 0.089 & 0.034 & 0.894 & 0.832 & \textbf{0.272} \\
SDEdit~\cite{meng2021sdedit}      & 0.018 & 0.831 & 0.211 & 0.054 & 0.857
                & 0.124 & 0.038 & 0.722 & 0.549 & 0.246 \\
HIVE~\cite{zhang2024hive}        & 0.053 & 0.882 & 0.218 & 0.077 & 0.850
                & 0.094 & 0.030 & 0.886 & 0.816 & 0.268 \\
Null-text Inv.~\cite{mokady2023null} & 0.078 & 0.834 & 0.231 & 0.055 & 0.815
                & 0.084 & \textbf{0.023} & 0.859  & 0.787 & 0.271 \\
Emu-Edit~\cite{sheynin2024emu}        & \textbf{0.109} & 0.861 & \textbf{0.232} & 0.084 & 0.840 
                & N/A & N/A & N/A & N/A & N/A \\
RealEdit        & 0.080 & 0.877 & 0.229 & 0.075 & 0.868 
                & 0.102 & 0.039 & 0.870 & 0.780 & \textbf{0.272} \\ 
\bottomrule
\end{tabular}%
}
\vspace{-0.5cm}
\end{table*}
