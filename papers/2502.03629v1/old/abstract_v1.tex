We introduce \RealEdit, a large-scale image editing dataset with real-world editing requests and human-edits sourced from Reddit. Unlike existing datasets that mostly contain synthetic editing examples and are often limited in scale, \RealEdit presents a total of () examples that capture realistic user editing needs. With \RealEdit, we first evaluate a range of state-of-the-art editing models, showing that they fail to cater to real human requests despite their promising performances on synthetic tasks.
We then demonstrate that by finetuning on \RealEdit training examples, our resultant model outperforms existing models by up to 165 Elo points with human judgment, and a gain of 2.2/10 points on automated metrics. Additionally, when employed to complete most recent Reddit requests, our model receives multiple positive real user feedback.
Finally, we examine the use of our data to generalize a SOTA deepfake detector to also detect in-the-wild human-made edits and find that we are able to improve F1 by 14\%.