\section{Introduction}
Text-guided image editing, which allows users to specify desired transformations to an input image using natural language descriptions~\cite{brooks2023instructpix2pix, zhang2024magicbrush}, has become increasingly popular due to its intuitive and accessible approach.
This demand is clearly illustrated in online communities such as Reddit’s \textit{r/PhotoshopRequest}\footnote{\url{https://www.reddit.com/r/PhotoshopRequest}} and \textit{r/estoration}\footnote{\url{https://www.reddit.com/r/estoration}}, serving a combined 1.5 million members, spanning from removing an unwanted object from the background, restoring a damaged photo, to enhancing the quality of a cherished personal image.

Despite the impressive capabilities in image generation and modification led by recent advancement of diffusion-based models~\cite{Ramesh2022DALLE2, rombach2022high, brooks2023instructpix2pix, zhang2024magicbrush}, seemingly straightforward real-world editing tasks, like ones from the Reddit's \textit{r/PhotoshopRequest}, continue to pose significant challenges to existing models.
For instance, while existing models are effective at artistic transformations or generating stylized content~\cite{brooks2023instructpix2pix, zhang2024magicbrush, mokady2023null, krojer2024learning, meng2021sdedit, zhang2024hive, sheynin2024emu}, they fall short at some of the most common real-world requests such as restoring a damaged image (see Figure~\ref{fig:soldier}). This discrepancy highlights a critical misalignment between the capabilities of current editing models and the actual needs of users.

One major challenge for models to effectively tackle real-world image editing is the diversity and open-ended nature of the tasks involved. However, most existing models are trained with synthetic or somewhat arbitrarily created datasets that do not characterize real-world human-centered objectives well, as is shown in Table \ref{tab:our_benchmark_quantitative}.
For instance, in Ultra-Edit~\cite{zhao2024ultraedit}, ``adding rainbow'' to an image constitutes a significant portion of the dataset.
Consequently, models trained with these datasets fail to capture the types of edits real-world users genuinely need, limiting the models' ability to perform them.

In this work, we introduce \ours, a large-scale text-guided image editing dataset meticulously compiled from Reddit that more faithfully reflects the realistic distribution of image editing needs.
Specifically, we source image editing requests from two of the largest subreddit communities related to image editing, \textit{r/PhotoshopRequest} and \textit{r/estoration}, into a dataset consisting of over 48,000 examples.
By carefully preprocessing and filtering out ambiguous and noisy examples with manual verifications, we transform part of the collected examples in \ours into an evaluation set that consists of more than 9000 real-world image editing requests to test models' capability.
Notably, \ours evaluation set shows that real-world requests differ drastically to existing evaluation datasets~\cite{zhang2024magicbrush, sheynin2024emu}, on which existing models struggle.

Towards building an effective image editing model for real-world tasks, we finetune a text-guided image editing model, InstructPix2Pix~\cite{brooks2023instructpix2pix}, on \ours's training examples. Using real human preferences to compute Elo scores~\cite{}, we show that the resultant model trained on \ours demonstrates significantly better performances than existing state-of-the-art models on \ours's test set with an Elo score of 1184, as well as competitive performances on existing evaluation sets~\cite{zhang2024magicbrush, sheynin2024emu}. More intriguingly, when employing our model to generate edits for unseen, recent requests on Reddit, the edits receive positive feedback from real Reddit users, having successfully completed 3 requests with positive feedback, demonstrating exciting new possibilities unlocked by the proposed \ours dataset.

% we present a series of contributions to better align editing models with real user needs. Our work includes: (1) an analysis of disconnects between model functionality and user needs, (2) a taxonomy of edit requests, (3) a large, high-quality training dataset, (4) a corresponding test set, (5) a state of the art model trained on this data, (6) a benchmark in human-centered image editing, (7) adoption of semantic-centered metrics (such as VIEscore\cite{ku2023viescore}, VQA, TIFA and ELO), and (8) an additional application of our data for fake image detection.

% By addressing these needs through improved data and benchmarking, we aim to steer model development toward practical, user-centered solutions in image editing.


\begin{figure*}[!h]
    \centering
    \includegraphics[width=\textwidth]{figs/Figure_2.pdf}
    \caption{Baselines struggle on simple tasks, such as restoring this image of a soldier.}
    \label{fig:soldier}
\end{figure*}


% State of the world: How are people doing stuff today? In your case, how are people developing editing models?

% We have cool diffusion models. They generate fascinateing stuff. However, simple requests like removing stranger in the background are still done manually. There is even money involved, as requests are paid.


% [some interesting sentence to hook the reader] Current image editing datasets are comprised of machine-generated images, with varying degrees of human input. Some datasets do not use any human input \cite{sheynin2024emu}, whereas some have humans writing some of the instructions \cite{brooks2023instructpix2pix, zhang2024magicbrush, zhao2024ultraedit}. Some datasets even had humans select outputs from the model generations, or select some samples to remove \cite{zhang2024hive, krojer2024learning}. There are a couple datasets that use image pairs where the edits were performed by a human \cite{shi2020benchmark, tan2019expressing}, however, these are very small (4k-6k samples) and insufficient to train a high-quality model. \textbf{There does not exist a large-scale image editing dataset where skilled humans have performed the edits.}

% Problem with the state: What problem is unsolved given the state of the world? In your case, you want to say how online real world human edit requests are different and can’t be simply solved using existing methods. You want to highlight the need support data during the editing process.

 % Why don't we have the technologyf liek this it yet? because the tech isnt centered on people!  These model that have fascinating demos fail when applied to human cases. They can make classic paintings look funny but cant help restore a damaged photo.

% The underlying problem with this is that image editing models are intended for real human use. Using partially or completely contrived datasets does not account for 1) the styles of edit requests and distribution  of editing tasks in the world, 2) how a human would interpret the image and edit instructions, and 3) the precision and expertise of humans skilled in photo editing. Considering the ubiquity of image editing in the 21\textsuperscript{st} century human experience, models should be able to understand and reflect these human needs more closely. Current benchmarks for image editing do not include ground-truth human edits, which also makes it impossible to determine the quality of an output image. Additionally, since models cannot generate human-quality images and often leave ``artifacts" [citation needed], using such data sets a lower ceiling for the quality of potential model outputs, leading to models that can never achieve close to human performance. 


% Insight: What insight do you have to solve this problem? In your case, it’s using RAG editing. How RAG gets you a support set to aid in the editing.

% We looked at an online communities that revolve around this and noticed that people care for this that researchers dont develop. People care about people pictures of people. We are able to analyze and quantify the discrepancy between what models are capable of and what people need. We collected the data that represents human tasks. We trained a model on this task. We 


% it's not anymore :( we gotta plug this dataset and that we have a GOATED model
% We introduce \textsc{RealEdit}, a dataset of 60k samples comprised of real-world data sourced from two prominent Reddit communities (subreddits) \href{https://www.reddit.com/r/PhotoshopRequest/}{r/PhotoshopRequest} and \href{https://www.reddit.com/r/estoration/}{r/estoration}. This addresses the above concerns, as we can see how people want their images edited in the real world, as well as having edits by hobbyist photo editors ranked by votes to showcase how a human would edit given the request, and ensure high quality ground truth images. We also finetune InstructPix2Pix \cite{brooks2023instructpix2pix} on our dataset and create a state of the art image-editing model which (hopefully) beats competitor state of the art models on several automatic metrics as well as a subjective human study.   

% Challenges: What are the technical challenges to operationalize your insight? In your case, you want to talk about two things: (1) the technical challenges associated with curating a useful benchmark, and (2) the technical challenges associated with developing a RAG model
% \ps{ Challenges: What are the technical challenges to operationalize your insight? NOt sure how to word this}


% Collecting data from a source as noisy as Reddit does not come without its challenges. Firstly, though the images may be high quality edits, Reddit users (Redditors) tend to have a sense of humor when executing requests (often following the letter but not intent of the request), which may lead to noisy results. Additionally, requests may be ambiguous, \textit{(e.g. ``Improve this image.", ``Do something funny with this image.")}, in which case there may be a diverse set of valid interpretations and it does not make sense to have a singular ground truth image. We prune our dataset for such noisy samples. Reddit users often describe their requests in long form, which may includes information not specific to the request, and often times the request can be succinctly stated \textit{(e.g. ``This photo was taken of my Mother and me at my Grandmother’s wake. I would love to get this framed for my Mom’s birthday next month. I love the photo, but the person who took it put filters all over it. I was wondering if someone could \textbf{make it look more natural}.'')}. We have implemented a pipeline for processing wordy captions to be more focused and direct. Training a model on this data is also difficult, as the diversity of human requests, as well as the tendency for humans to slightly shift or resize images, leads to data that is hard to learn by a model designed to work in fixed sizes with minimal pixel-wise differences. We filter out samples with a low CLIP-similarity to ensure that our model will be able to learn the edits. Furthermore, current metrics for image editing tasks such as L1 and L2 pixel distance, CLIP similarity \cite{radford2021learning} and DINO \cite{zhang2022dino} cannot accurately evaluate whether an edit was performed successfully, and assess the aesthetic quality of the image, so qualitative evaluation is a necessity. 

% Evaluation: How will your design your experiments and metrics? In your case, you want to mention the metrics you introduce and justify them.


% We curate a test set of 9k samples with ground truth output images, and manually verify it for appropriateness by removing NSFW or offensive samples, and removing samples where the edit performed does not match the edit instruction. We evaluate the performance of our model against other models on our test set on standard metrics (L1 and L2 pixel distance, CLIP-I, CLIP-T, \cite{radford2021learning} DINO \cite{zhang2022dino}). We taxonomize our dataset to identify specific tasks where our model outperforms/underperforms against competitors. We also evaluate our model and competitors' models on two other test sets to ensure our results generalize to other tasks. We perform a qualitative study where we ask participants to select/rank/score (????) images and use this to rank models.  Finally, we use our dataset to finetune a deepfake detection model [citation needed] to better classify edited images as deepfakes, showing that having human edits makes a large impact on the perceived realism of the image.


% We highlight six key contributions.

% We highlight key contributions of our work:
% (1) Analyzing the difference,  (2) providing a taxonomy of what important for people, (3) collecting a large high quality training set, (4) providing a test set, (5) a SOTA model trained on the data, (6) the benchmark to beat, (7) metrics to use and avoid (VIE scores? or ELO) and an (8) additional case for our data in fake image detection