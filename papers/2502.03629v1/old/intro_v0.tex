\section{Introduction}
The rapid advancement of diffusion-based models has led to impressive capabilities in image generation and modification~\cite{Ramesh2022DALLE2, rombach2022high, brooks2023instructpix2pix, zhang2024magicbrush}.
Among various applications of these models, text-guided image editing has been a core functionality, allowing users to specify desired transformations to an input image using natural language descriptions~\cite{brooks2023instructpix2pix, zhang2024magicbrush}.
Despite recent achievements, fundamental editing tasks that are central to real-world applications---such as removing an unwanted object from the background, restoring a damaged photo, or enhancing the quality of a cherished personal image---continue to pose significant challenges.


% Despite these achievements, fundamental editing tasks that real-world users are interested in, such as removing an unwanted object from the background, restoring a damaged photo, or enhancing the quality of a cherished personal image, still often present challenges for existing models and typically performed manually.
% Specifically, as shown in online communities like Reddit’s \href{https://www.reddit.com/r/PhotoshopRequest}{r/PhotoshopRequest}\footnote{\url{https://www.reddit.com/r/PhotoshopRequest}} and \href{https://www.reddit.com/r/estoration}{r/estoration}\footnote{\url{https://www.reddit.com/r/estoration}} forums, everyday internet users highly value these tasks.

% where custom image edits are frequently completed for money.

% These models can produce intricate, high-quality images, transform styles, and even synthesize entirely new scenes.

Existing models often fall short in addressing these tasks effectively (see Figure~\ref{}), leaving many of these refinements to be performed manually~\cite{}.
For instance, online communities such as Reddit’s \textit{r/PhotoshopRequest}\footnote{\url{https://www.reddit.com/r/PhotoshopRequest}} and \textit{r/estoration}\footnote{\url{https://www.reddit.com/r/estoration}} forums receives a substantial number (XXX) of user requests for such edits daily, illustrating the ongoing demand for an effective image editing solution.
One biggest challenge in real-world text-guided image editing is the diversity and open-ended nature of the tasks involved.
While existing models are effective at artistic transformations or generating stylized content, they often fail when applied to realistic, human-centered editing tasks (Figure \ref{fig:soldier}).

This discrepancy highlights a critical misalignment between the capabilities of current editing models and the actual needs of users. Existing models, while effective at artistic transformations or generating stylized content, often fail when applied to realistic, human-centered editing tasks (Figure \ref{fig:soldier}).

We believe this misalignment largely stems from the data, rather than models themselves, as modern frameworks like DALL-E and Midjourney demonstrate the impressive capabilities of current methodology. Existing synthetic datasets are often created around arbitrary, artistic tasks rather than realistic, human-centered objectives. For instance, in Ultra-Edit~\cite{zhao2024ultraedit}, the most trained task is simply to ``add rainbow." Consequently, these datasets fail to capture the types of edits users genuinely need limiting the models' ability to perform them.


In this paper, we present a series of contributions to better align editing models with real user needs. Our work includes: (1) an analysis of disconnects between model functionality and user needs, (2) a taxonomy of edit requests, (3) a large, high-quality training dataset, (4) a corresponding test set, (5) a state of the art model trained on this data, (6) a benchmark in human-centered image editing, (7) adoption of semantic-centered metrics (such as VIEscore\cite{ku2023viescore}, VQA, TIFA and ELO), and (8) an additional application of our data for fake image detection.

% By addressing these needs through improved data and benchmarking, we aim to steer model development toward practical, user-centered solutions in image editing.


\begin{figure*}[!h]
    \centering
    \includegraphics[width=\textwidth]{figs/soldier.png}
    \caption{Baselines struggle on simple tasks}
    \label{fig:soldier}
\end{figure*}


% State of the world: How are people doing stuff today? In your case, how are people developing editing models?

% We have cool diffusion models. They generate fascinateing stuff. However, simple requests like removing stranger in the background are still done manually. There is even money involved, as requests are paid.


% [some interesting sentence to hook the reader] Current image editing datasets are comprised of machine-generated images, with varying degrees of human input. Some datasets do not use any human input \cite{sheynin2024emu}, whereas some have humans writing some of the instructions \cite{brooks2023instructpix2pix, zhang2024magicbrush, zhao2024ultraedit}. Some datasets even had humans select outputs from the model generations, or select some samples to remove \cite{zhang2024hive, krojer2024learning}. There are a couple datasets that use image pairs where the edits were performed by a human \cite{shi2020benchmark, tan2019expressing}, however, these are very small (4k-6k samples) and insufficient to train a high-quality model. \textbf{There does not exist a large-scale image editing dataset where skilled humans have performed the edits.}

% Problem with the state: What problem is unsolved given the state of the world? In your case, you want to say how online real world human edit requests are different and can’t be simply solved using existing methods. You want to highlight the need support data during the editing process.

 % Why don't we have the technologyf liek this it yet? because the tech isnt centered on people!  These model that have fascinating demos fail when applied to human cases. They can make classic paintings look funny but cant help restore a damaged photo.

% The underlying problem with this is that image editing models are intended for real human use. Using partially or completely contrived datasets does not account for 1) the styles of edit requests and distribution  of editing tasks in the world, 2) how a human would interpret the image and edit instructions, and 3) the precision and expertise of humans skilled in photo editing. Considering the ubiquity of image editing in the 21\textsuperscript{st} century human experience, models should be able to understand and reflect these human needs more closely. Current benchmarks for image editing do not include ground-truth human edits, which also makes it impossible to determine the quality of an output image. Additionally, since models cannot generate human-quality images and often leave ``artifacts" [citation needed], using such data sets a lower ceiling for the quality of potential model outputs, leading to models that can never achieve close to human performance. 


% Insight: What insight do you have to solve this problem? In your case, it’s using RAG editing. How RAG gets you a support set to aid in the editing.

% We looked at an online communities that revolve around this and noticed that people care for this that researchers dont develop. People care about people pictures of people. We are able to analyze and quantify the discrepancy between what models are capable of and what people need. We collected the data that represents human tasks. We trained a model on this task. We 


% it's not anymore :( we gotta plug this dataset and that we have a GOATED model
% We introduce \textsc{RealEdit}, a dataset of 60k samples comprised of real-world data sourced from two prominent Reddit communities (subreddits) \href{https://www.reddit.com/r/PhotoshopRequest/}{r/PhotoshopRequest} and \href{https://www.reddit.com/r/estoration/}{r/estoration}. This addresses the above concerns, as we can see how people want their images edited in the real world, as well as having edits by hobbyist photo editors ranked by votes to showcase how a human would edit given the request, and ensure high quality ground truth images. We also finetune InstructPix2Pix \cite{brooks2023instructpix2pix} on our dataset and create a state of the art image-editing model which (hopefully) beats competitor state of the art models on several automatic metrics as well as a subjective human study.   

% Challenges: What are the technical challenges to operationalize your insight? In your case, you want to talk about two things: (1) the technical challenges associated with curating a useful benchmark, and (2) the technical challenges associated with developing a RAG model
% \ps{ Challenges: What are the technical challenges to operationalize your insight? NOt sure how to word this}


% Collecting data from a source as noisy as Reddit does not come without its challenges. Firstly, though the images may be high quality edits, Reddit users (Redditors) tend to have a sense of humor when executing requests (often following the letter but not intent of the request), which may lead to noisy results. Additionally, requests may be ambiguous, \textit{(e.g. ``Improve this image.", ``Do something funny with this image.")}, in which case there may be a diverse set of valid interpretations and it does not make sense to have a singular ground truth image. We prune our dataset for such noisy samples. Reddit users often describe their requests in long form, which may includes information not specific to the request, and often times the request can be succinctly stated \textit{(e.g. ``This photo was taken of my Mother and me at my Grandmother’s wake. I would love to get this framed for my Mom’s birthday next month. I love the photo, but the person who took it put filters all over it. I was wondering if someone could \textbf{make it look more natural}.'')}. We have implemented a pipeline for processing wordy captions to be more focused and direct. Training a model on this data is also difficult, as the diversity of human requests, as well as the tendency for humans to slightly shift or resize images, leads to data that is hard to learn by a model designed to work in fixed sizes with minimal pixel-wise differences. We filter out samples with a low CLIP-similarity to ensure that our model will be able to learn the edits. Furthermore, current metrics for image editing tasks such as L1 and L2 pixel distance, CLIP similarity \cite{radford2021learning} and DINO \cite{zhang2022dino} cannot accurately evaluate whether an edit was performed successfully, and assess the aesthetic quality of the image, so qualitative evaluation is a necessity. 

% Evaluation: How will your design your experiments and metrics? In your case, you want to mention the metrics you introduce and justify them.


% We curate a test set of 9k samples with ground truth output images, and manually verify it for appropriateness by removing NSFW or offensive samples, and removing samples where the edit performed does not match the edit instruction. We evaluate the performance of our model against other models on our test set on standard metrics (L1 and L2 pixel distance, CLIP-I, CLIP-T, \cite{radford2021learning} DINO \cite{zhang2022dino}). We taxonomize our dataset to identify specific tasks where our model outperforms/underperforms against competitors. We also evaluate our model and competitors' models on two other test sets to ensure our results generalize to other tasks. We perform a qualitative study where we ask participants to select/rank/score (????) images and use this to rank models.  Finally, we use our dataset to finetune a deepfake detection model [citation needed] to better classify edited images as deepfakes, showing that having human edits makes a large impact on the perceived realism of the image.


% We highlight six key contributions.

% We highlight key contributions of our work:
% (1) Analyzing the difference,  (2) providing a taxonomy of what important for people, (3) collecting a large high quality training set, (4) providing a test set, (5) a SOTA model trained on the data, (6) the benchmark to beat, (7) metrics to use and avoid (VIE scores? or ELO) and an (8) additional case for our data in fake image detection