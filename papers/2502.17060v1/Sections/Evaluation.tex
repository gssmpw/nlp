\section{Evaluation}

% Our proposed framework was compared with Apollo \cite{b7Apollo1, b7Apollo2}, which demonstrates that it can model analytic operators using data content. Two loss functions were utilized, the root-mean-square deviation error (RMSE), and the mean absolute error (MAE). The selection of these two loss functions is because they fulfil the disadvantages of each other, while RMSE is sensitive to outlier MAE is not and the MAE cannot take into account the direction of the error while the RMSE can achieve it. Speedup was computed to determine how quickly our framework can model the operator $\Phi$. We utilised the \textit{Speedup} and \textit{Amortized Speedup}, which assesses the require time to approximate each operator in comparison to exhaustively executing them on all datasets (more is better). Particularly, the speedup is equalled $\frac{T{^{(i)}_{op}}}{T{^{(i)}_{SimOp} + T_{vec} + T_{sim} + T_{pred}}}$, where $T{^{(i)}_{op}}$ is the execution time for operator $i$, across all the datasets, $T{^{(i)}_{SimOp}}$ is the time needed to model the operator with the datasets selected from the similarity search, $T_{vec}$  is the time needed to compute the vector embedding for each dataset, $T_{sim}$, is the time needed to perform similarity search, and $T_{pred}$ is the time needed to predict on the dataset $D_o$. In addition to the dataset vectorisation, which is done once for each data lake, we calculate amortised speedup. Furthermore, an experimental evaluation of our proposed model for dataset vectorization NumTabData2Vec has been performed to show that our approach can transform a dataset to a vector embedding representation space $z$. For the evaluation experiments, three different NumTabData2Vec were built to project the dataset representation with vector sizes of $100$, $200$, and $300$. Each model has eight transformer layers and is trained parallel using four NVIDIA A10s GPUs, and trained for fifty epochs.
We compared our framework with Apollo \cite{b7Apollo1, b7Apollo2}, which models analytic operators using data content. Two loss functions to measure prediction accuracy are employed: root-mean-square error (RMSE) and mean absolute error (MAE). RMSE is sensitive to outliers, while MAE is not; conversely, RMSE accounts for error direction, which MAE cannot. Speedup metrics are also used to evaluate how efficiently our framework models operator $\Phi$. Specifically, \textit{Speedup} and \textit{Amortized Speedup} measure the time required to approximate each operator versus exhaustively executing them on all datasets. Speedup is defined as $\frac{T{^{(i)}_{op}}}{T{^{(i)}_{SimOp} + T_{vec} + T_{sim} + T_{pred}}}$, where $T{^{(i)}_{op}}$ is the time to execute operator $i$ on all datasets, $T{^{(i)}_{SimOp}}$ is the time to model the operator with datasets from similarity search, $T_{vec}$ is the vector embedding computation time, $T_{sim}$ is the similarity search time, and $T_{pred}$ is the prediction time for $D_o$. Amortized speedup includes dataset vectorization, performed once per data lake for multiple operators (in our case two operators).
We also evaluate our dataset vectorization model, NumTabData2Vec, which projects datasets into vector embedding space $z$. Three versions were built with vector sizes of $100$, $200$, and $300$, each featuring eight transformer layers. The models were trained for 50 epochs on four NVIDIA A10 GPUs in parallel.

\subsection{Evaluation Setup}
Our framework is deployed over an AWS EC2 virtual machine server running with 48 VCPUs of AMD EPYC 7R32 processors at 2.40GHz, and four A10s GPUs with 24GB of memory each, $192GB$ of RAM memory, and $2TB$ of storage, running over Ubuntu 24.4 LTS. Our code is written in Python (v.3.9.1) and PyTorch modules (v.2.4.0). Apollo was deployed in a virtual machine with 8 VCPUs Intel Xeon E5-2630 @ 2.30GHz, $64GB$ of RAM memory, and $250GB$ of storage, running Ubuntu 24.4 LTS like in their experimental evaluation. 

\subsection{Datasets}
\begin{table}[!ht]
    \centering
    \setlength\doublerulesep{0.5pt}
    \caption{Dataset properties for experimental evaluation}
    \label{tab:table-evaluation-datasets}
    \begin{tabular}{||c|c|c|c||}
        \hline
         \makecell{Dataset Name}& \makecell{\# Files} & \makecell{\# Tuples} & \makecell{\# Columns}\\ \hline\hline
         Household Power & & & \\
         Consumption \cite{b21HPCdataset} & $401$ & $2051$ & 7\\
         \hline
         Adult \cite{b22AdultDataset} & $100$ & $228$ & 14\\
         \hline
         Stocks \cite{b23StockMarketDataset} & $508$ & $1959 - 13$ & 7 \\
         \hline
         Weather \cite{b23WeatherDataset} & $49$ & $516$ & 7 \\ \hline
    \end{tabular}

\end{table}

We evaluated our framework using four diverse datasets to represent real-world scenarios. Table \ref{tab:table-evaluation-datasets} summarizes these datasets and their attributes. The vectorization module, NumTabData2Vec, was trained on data separate from the experimental evaluation data, split $60\%$ for training and $40\%$ for testing.
The Household Power Consumption (HPC) dataset \cite{b21HPCdataset} contains electric power usage measurements from a household in Sceaux, France. It includes $401$ datasets, each with $2051$ tuples and seven features recorded at one-minute intervals. The Adult dataset \cite{b22AdultDataset}, commonly used for binary classification, predicts whether an individual earns more or less than $50K$ annually. It comprises $100$ datasets, each with $228$ individuals and various socio-economic features.
The Stock Market dataset \cite{b23StockMarketDataset} includes daily NASDAQ stock prices obtained from Yahoo Finance, with $508$ datasets. Each dataset contains $13$ to $1959$ tuples, each describing seven feature attributes. The Weather dataset \cite{b23WeatherDataset} provides hourly weather measurements from $36$ U.S. cities between $2012$ and $2017$, split into $49$ datasets, each with $516$ tuples and seven features.


\begin{figure}[!t]
     \centering
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/Sim_Search/HPC/HPC_LR_RMSE_Loss_fig.pdf}
         \caption{Linear Regression RMSE error loss}
         \label{fig:HPC-LR-RMSE}
     \end{subfigure}
     \hfill 
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/Sim_Search/HPC/HPC_LR_MAE_Loss_fig.pdf}
         \caption{Linear Regression MAE error loss}
         \label{fig:HPC-LR-MAE}
     \end{subfigure}
        
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/Sim_Search/HPC/HPC_MLP_RMSE_Loss_fig.pdf}
         \caption{MLP for Regression RMSE error loss}
         \label{fig:HPC-MLP-RMSE}
     \end{subfigure}
     \hfill 
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/Sim_Search/HPC/HPC_MLP_MAE_Loss_fig.pdf}
         \caption{MLP for Regression MAE error loss}
         \label{fig:HPC-MLP-MAE}
     \end{subfigure}
        \caption{Household power consumption dataset prediction error loss}
        \label{fig:HPC-EVAL-RES}
\end{figure}

Our framework was evaluated by registering the accuracy of predicting the output of various ML operators over multiple datasets in $D$ without actually executing the operator on them. To evaluate our scheme and its parameters, we use all four datasets, ranging the size of the produced vectors as well as the similarity functions used.
We project all datasets into $k$-dimensional spaces with varying vector dimensions ($100$, $200$, and $300$). For each dataset in Table \ref{tab:table-evaluation-datasets}, we model different operators: For the regression datasets (Household Power Consumption and Stock Market), we model Linear Regression (LR) and Multi-Layer Perceptron (MLP) operators; for the classification datasets (Weather and Adult), we model the Support Vector Machine (SVM) and MLP classifier operators. Each experiment has been executed $10$ times and we report the average of the error loss, as well as the speedup. 

\begin{figure}[!t]
     \centering
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/Sim_Search/Stocks/Stocks_LR_RMSE_Loss_fig.pdf}
         \caption{Linear Regression RMSE error loss}
         \label{fig:Stock-LR-RMSE}
     \end{subfigure}
     \hfill 
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/Sim_Search/Stocks/Stocks_LR_MAE_Loss_fig.pdf}
         \caption{Linear Regression MAE error loss}
         \label{fig:Stock-LR-MAE}
     \end{subfigure}
        
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/Sim_Search/Stocks/Stocks_MLP_RMSE_Loss_fig.pdf}
         \caption{MLP for Regression RMSE error loss}
         \label{fig:Stock-MLP-RMSE}
     \end{subfigure}
     \hfill 
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/Sim_Search/Stocks/Stocks_MLP_MAE_Loss_fig.pdf}
         \caption{MLP for Regression MAE error loss}
         \label{fig:Stock-MLP-MAE}
     \end{subfigure}
        \caption{Stock market dataset prediction error loss}
        \label{fig:Stock-EVAL-RES}
\end{figure}
\subsection{Evaluation Results}



Figures \ref{fig:HPC-EVAL-RES}, \ref{fig:Stock-EVAL-RES}, \ref{fig:Weather-EVAL-RES}, and \ref{fig:Adult-EVAL-RES} present the evaluation results for each method, comparing the performance of different similarity search techniques across various vector embedding representation spaces. The red (with hatches), brown, and blue bars correspond to vector embeddings of size 100, 200, and 300 respectively. In each sub-figure, the y-axis represents the error loss value, while the x-axis displays the similarity search method applied over the vector embeddings. Figures \ref{fig:HPC-EVAL-RES} and \ref{fig:Stock-EVAL-RES} show the results for the Stock market and Household power consumption datasets, where the bottom sub-figure demonstrates the MLP regression model, and the top sub-figure presents the LR model. Figures \ref{fig:Weather-EVAL-RES} and \ref{fig:Adult-EVAL-RES} depict the evaluation results for the Weather and Adult datasets. In these Figures, the top sub-figure shows the SVM with SGD results, while the bottom sub-figure shows the MLP classifier. The left sub-figures in all Figures use the RMSE loss function, whereas the right sub-figures use the MAE loss function. 



Figure \ref{fig:HPC-EVAL-RES}, we show, for the HPC dataset, shows as increase the vector dimension size there is slightly lower prediction error for all the operator modelling. While for different similarity methods did not result in any significant differences in the prediction error loss for all the operator modelling. This suggests that, regardless the similarity selection method, our framework effectively selects the most optimal subset of data to improve model predictions on the unseen input dataset $D_o$. Additionally, we observe higher error loss with a vector size of 100, which can be attributed to the reduced representation capacity of lower-dimensional vectors. This limitation results in fewer ``right" datasets being selected.

For the stock market dataset, Figure \ref{fig:Stock-EVAL-RES} depicts that a vector embedding representation of size $300$ models more accurate operators, with cosine similarity performing best in the similarity search and modelling the most optimal operator. However, due to the inherent volatility in Stock market data from different days, all models in the stock market dataset experiments exhibit high loss values. 

In the weather dataset, the SVM operator results from sub-figures \ref{fig:Weather-SVM-RMSE} and \ref{fig:Weather-SVM-MAE} show that using $300$ vectors in the representation space consistently led to more accurate operator models across all similarity methods. Specifically, cosine similarity in combination with the $300$-dimensional vector embedding reduced the error rate in operator predictions, demonstrating that projecting datasets into this representation space and applying cosine similarity improves the prediction accuracy on the modelled operator. For the MLP classifier from sub-figures \ref{fig:Weather-MLP-RMSE} and \ref{fig:Weather-MLP-MAE}, the results illustrate that using vector embeddings of size $200$ and K-Means clustering produced the most accurate MLP classifier operators.

% Overall, we observe that the error loss was minimized 
% (** what do you mean, minimized? In general, here you should comment on the effect of similarity function, the effect of vector size and the effect of different operators to the accuracy of prediction. E.g., in Household dataset shows little effect in all bars, but in Stock, the cosine seems better and larger size of vectors leads to better performance etc. **)
% in most cases, indicating that our framework effectively selects the most relevant datasets from the data lake $D$, thereby improving data quality and reducing $\Phi$ prediction errors on the target dataset $D_o$. This demonstrates that the datasets are accurately transformed into the vector embedding representation space, allowing for the selection of datasets most similar to $D_o$. 

%Adult


%Weather
\begin{figure}[t!]
     \centering
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/Sim_Search/Weather/Weather_SVM_RMSE_Loss_fig.pdf}
         \caption{SVM with SGD RMSE error loss}
         \label{fig:Weather-SVM-RMSE}
     \end{subfigure}
     \hfill 
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/Sim_Search/Weather/Weather_SVM_MAE_Loss_fig.pdf}
         \caption{SVM with SGD MAE error loss}
         \label{fig:Weather-SVM-MAE}
     \end{subfigure}
        
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/Sim_Search/Weather/Weather_MLP_RMSE_Loss_fig.pdf}
         \caption{MLP RMSE error loss}
         \label{fig:Weather-MLP-RMSE}
     \end{subfigure}
     \hfill 
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/Sim_Search/Weather/Weather_MLP_MAE_Loss_fig.pdf}
         \caption{MLP MAE error loss}
         \label{fig:Weather-MLP-MAE}
     \end{subfigure}
        \caption{Weather dataset prediction error loss}
        \label{fig:Weather-EVAL-RES}
\end{figure}

On the other hand, the Adult dataset shows the lowest error rates, with error loss values consistently below $0.5$ across all vector embedding dimensions and similarity search methods (see Figure \ref{fig:Adult-EVAL-RES}). The Adult dataset, besides exhibiting a high number of rows, also has a higher number of columns, which demonstrates that our framework performs consistently well even with larger datasets.
Additionally, we observe that the lowest prediction error across all datasets occurs when using higher-dimensional vector embeddings. With a trade-off between accuracy and execution time as the difference to generate all data lake available datasets vector embedding representation between $100$ and $300$ size dimension in the vector representation space to be less than $60$ seconds. This confirms that a higher number of vector dimensions leads to more accurate predictions, consistent with findings in previous research \cite{b8Word2Vec}.


\begin{figure}[!t]
     \centering
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/Sim_Search/Adult/Adult_MLP_RMSE_Loss_fig.pdf}
         \caption{SVM with SGD RMSE error loss}
         \label{fig:Adult-LR-RMSE}
     \end{subfigure}
     \hfill 
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/Sim_Search/Adult/Adult_MLP_RMSE_Loss_fig.pdf}
         \caption{SVM with SGD MAE error loss}
         \label{fig:Adult-LR-MAE}
     \end{subfigure}
     
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/Sim_Search/Adult/Adult_MLP_RMSE_Loss_fig.pdf}
         \caption{MLP RMSE error loss}
         \label{fig:Adult-MLP-RMSE}
     \end{subfigure}
     \hfill 
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/Sim_Search/Adult/Adult_MLP_MAE_Loss_fig.pdf}
         \caption{MLP MAE error loss}
         \label{fig:Adult-MLP-MAE}
     \end{subfigure}
        \caption{Adult dataset prediction error loss}
        \label{fig:Adult-EVAL-RES}
\end{figure}





We conducted an experimental evaluation using the Sampling Ratio (SR) approach, similar to Apollo \cite{b7Apollo1}, but employed neural networks built from the vector embeddings of each dataset. The SR approach involves a unified random selection of $l\%$ datasets from the vector representation space, using this subset to construct a neural network for predicting operator outputs. We tested SR values of $0.1$, $0.2$, and $0.4$, as well as vector embedding dimensions of $100$, $200$, and $300$, across all datasets. 
Figure \ref{fig:SR-EVAL-RES} presents the sampling ratio results for the Adult dataset using MLP (sub-figure \ref{fig:Adult-SR-RMSE}) and for the Weather dataset using LR (sub-figure \ref{fig:Weather-SR-SVM-MAE}). In each sub-figure the y-axis represents the RMSE prediction error loss while the x-axis denotes the vector dimension



\begin{figure}[htpb!]
     \centering
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/SR/Adult/Adult_MLP_SR_RMSE_Loss_fig.pdf}
         \caption{Adult Dataset MLP Operator RMSE error loss}
         \label{fig:Adult-SR-RMSE}
     \end{subfigure}
     \hfill 
     \begin{subfigure}[b]{0.24\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figures/Results/SR/HPC/HPC_LR_SR_RMSE_Loss_fig.pdf}
         \caption{HPC dataset LR Operator RMSE error loss}
         \label{fig:Weather-SR-SVM-MAE}
     \end{subfigure}
     \caption{Sampling Ratio prediction results}
        \label{fig:SR-EVAL-RES}
\end{figure}

Both experiments demonstrate that as the vector embedding dimension increases, coupled with a larger sampling ratio (SR) value, there is a slight decrease in the prediction error loss. This improvement occurs because higher-dimensional vector embeddings provide a more accurate representation of the datasets in k-dimensions, with better dataset selection leading to enhanced prediction accuracy. Comparing the SR approach to our similarity search method for the HPC dataset, the SR approach was approximately $15\%$ less accurate in operator prediction across all vector embedding dimensions. A similar trend was observed in the Weather dataset. However, the Stock dataset exhibited a much larger discrepancy, with the SR approach performing about $70\%$ worse in prediction accuracy across all vector embedding dimensions. Likewise, in the Adult dataset, the SR approach delivered the poorest performance, with nearly $90\%$ lower prediction accuracy compared to the similarity search methods.

\begin{table*}[htbp]
    \centering
        \caption{Evaluation results of our framework exported analytic operator with lowest prediction error in comparison with Apollo}
    \label{tab:table-eval-res}
    % \scalebox{0.8}{
    \setlength\doublerulesep{0.5pt}
    % \begin{adjustbox}{width=\linewidth,center}
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
         \makecell{Dataset\\Name} & Method & Operator & RMSE &  MAE & Speedup  & Amortized Speedup \\
         \hline\hline
         \multirow{7}{*}{\makecell{Household\\Power\\Consumption}}& \makecell{$300$V Cosine} & LR & $\mathbf{6.61}$ & $\mathbf{5.42}$ & $0.0017$ & $\mathbf{1.99}$ \\ \cline{2-7}
                  & \makecell{$300$V SR-$0.2$} & LR & $7.77$ & $6.66$ &  $0.0018$  & $1.42$\\ \cline{2-7} 
        & \makecell{Apollo-SR $0.1$} & LR & $2968.01$ &  $2352.55$ & $\mathbf{0.015}$ & $0.024$ \\ \cline{2-7}
         & \makecell{Apollo-SR $0.2$} & LR & $2811.49$ &  $2229.50$ & $0.015$ & $0.024$ \\ \cline{2-7}\cline{2-7}
         & \makecell{$300$V K-Means} & MLP Regr. & $\mathbf{6.70}$ & $\mathbf{3.38}$ &  $0.9249$  & $\mathbf{1.99}$\\ \cline{2-7}
         & \makecell{Apollo-SR $0.1$} & MLP Regr. & $3322.05$ &  $2606.99$ & $2.38$ & $1.74$ \\ \cline{2-7}
         & \makecell{Apollo-SR $0.2$} & MLP Regr. & $3850.01$ &  $2609.36$ & $\mathbf{2.38}$ & $1.74$\\ \cline{1-7} \cline{1-7} 
         % Stock
         % \multirow{5}{*}{\makecell{Stock}}& \multirow{1}{*}{ \makecell{$100$V Euclidean}} & LR & $229388.93$ & $193066.03$ \\ \cline{2-5}
        \multirow{7}{*}{\makecell{Stock}} &  \makecell{$300$V Cosine} & LR & $306382.28$ & $125335.65$ & $0.00085$ & $\mathbf{1.91}$\\ \cline{2-7}
        & \makecell{$300$V SR-$0.4$} & LR & $21861625.91$ & $5674215.265$ &  $0.00087$  & $0.33$\\ \cline{2-7}
        & \makecell{Apollo-SR $0.1$} & LR & $\mathbf{153665.92}$ &  $\mathbf{118236.48}$ & $\mathbf{0.00093}$ & $0.00096$\\ \cline{2-7}
         & \makecell{Apollo-SR $0.2$} & LR & $166844.95$ &  $133306.68$ & $0.00093$ & $0.00096$\\ \cline{2-7}\cline{2-7}
         &  \makecell{$300$V Cosine} & MLP Regr. & $\mathbf{140236.47}$ & $\mathbf{123571.12}$ & $0.63$ & $\mathbf{1.91}$\\ \cline{2-7}
         & \makecell{Apollo-SR $0.1$} & MLP Regr. &  $175150.82$ &  $145123.09$ & $\mathbf{0.93}$ & $0.96$\\ \cline{2-7}
         & \makecell{Apollo-SR $0.2$} & MLP Regr. & $174390.81$ &  $146338.73$ & $0.93$ & $0.96$\\ \cline{1-7} \cline{1-7}
         % Weather
         \multirow{7}{*}{\makecell{Weather}}& \multirow{1}{*}{ \makecell{$300$V Cosine}} & \makecell{SVM SGD}& $\mathbf{14.13}$ & $\mathbf{7.63}$ & $1.06$ & $\mathbf{22.8}$ \\ \cline{2-7}
               & \makecell{Apollo-SR $0.1$} & SVM & $69.51$ &  $25.52$ & $\mathbf{2.10}$ &  $1.16$\\ \cline{2-7}
                        & \makecell{Apollo-SR $0.2$} & SVM & $68.70$ &  $22.81$ & $2.10$ & $1.16$\\ \cline{2-7} \cline{2-7}
       &  \multirow{1}{*}{ \makecell{$200$V Cosine}}& MLP & $\mathbf{14.29}$ & $\mathbf{4.03}$ & $1.03$  & $\mathbf{22.8}$\\ \cline{2-7}
        &  \multirow{1}{*}{ \makecell{$200$V SR-$0.4$}}& MLP & $15.95$ & $13.31$ & $1.02$  & $1.77$\\ \cline{2-7}
         & \makecell{Apollo-SR $0.1$} & MLP & $69.62$ &  $23.10$ & $\mathbf{1.34}$ & $1.14$ \\ \cline{2-7}
         & \makecell{Apollo-SR $0.2$} & MLP & $673.56$ &  $\mathbf{84.70}$ & $1.32$ & $1.14$\\ \cline{1-7} \cline{1-7}
         
         % Adult
         \multirow{7}{*}{\makecell{Adult}}& \multirow{1}{*}{ \makecell{$300$V Cosine}} & \makecell{SVM SGD}& $\mathbf{0.36}$ & $\mathbf{0.2}$ & $0.37$   & $\mathbf{2.78}$\\ \cline{2-7}
                  & \makecell{Apollo-SR $0.1$} & SVM & $68.32$ &  $22.95$ & $\mathbf{0.75}$ & $0.85$ \\ \cline{2-7}
                 & \makecell{Apollo-SR $0.2$} & SVM & $68.88$ &  $22.88$ & $0.74$ & $0.85$\\ \cline{2-7} \cline{2-7}

         &  \multirow{1}{*}{ \makecell{$300$V K-Means}}& MLP & $\mathbf{0.36}$ & $\mathbf{0.19}$ & $0.30$ & $2.78$ \\ \cline{2-7}
        & \makecell{$300$V SR-$0.2$} & MLP & $6.01$ & $6.00$ &  $0.54$  & $\mathbf{3.54}$\\ \cline{2-7}
         & \makecell{Apollo-SR $0.1$} & MLP & $71.11$ &  $26.51$ & $\mathbf{1.07}$ & $1.31$\\ \cline{2-7}
         & \makecell{Apollo-SR $0.2$} & MLP & $70.16$ &  $25.74$ & $1.05$ & $1.31$\\ \cline{1-7}
         
    \end{tabular}
    % }
\end{table*}

% Table \ref{tab:table-eval-res} illustrates the model operators for each dataset and each loss function, amortized speedup and speedup from our framework in comparison with the same model operators from the Apollo \cite{b7Apollo1, b7Apollo2} framework with SR of $0.1$ and $0.2$. The values $100$V, $200$V, and $300$V in the method column correspond to the dimensions of the vector embedding used for each dataset. The lowest prediction error for each modelled operator in each dataset is highlighted in the method that is used in the similarity search step from our pipeline. Apollo outperforms our framework only on the stock dataset for SR equal with $0.1$ in the LR analytic operator for both RMSE and MAE loss function which performs $50\%$ and $6\%$ better on each loss function equivalent. While our framework for the MLP for Regression outperforms the Apollo modelled operator for $20\%$ and $84\%$ for RMSE and MAE loss functions. However, this difference in the Stock dataset for LR operator modelling is not significant. In the remaining datasets, our framework illustrates that it can outperform Apollo for different values of SR. This makes us confirm that our similarity search using similarity functions selects the most similar datasets $D_r$ from data lake directory $D$, increasing data quality and minimising $\Phi$ prediction errors on the dataset $D_o$. For the Adult dataset, our model operators also perform better, which indicates our method's advantage with an increased number of dataset features (columns). In term of speedup we can see that Apollo outperformed our framework of all modelled operators. In terms of speedup we can see that Apollo outperformed our framework of all modelled operators. This is due to the vectorisation method of our framework which consists of big complexity time. Furthermore, in amortized speedup in most of the amortized speedup in which the vectorization is not counted because it is executed only one time and can be reused our framework surpasses Apollo framework in most of the operators with a big difference with our framework to be between $10\%$ and $60\%$ faster than Apollo. Additionally, most datasets demonstrate better amortized speedup when using the SR approach within our framework. This is because the prediction process relies solely on the vector representation, rather than leveraging all dataset tuples as done in the similarity search method for operator modelling. However, in terms of prediction accuracy, the SR approach does not perform as well as the similarity search method, which achieves superior results.

Table \ref{tab:table-eval-res} compares model operators, loss functions, and speedup metrics for our framework and Apollo at SR values of $0.1$ and $0.2$. Methods $100$V, $200$V, and $300$V denote vector embedding dimensions. The lowest prediction errors align with our pipeline's similarity search method.
Apollo outperforms our framework on the Stock dataset for the LR analytic operator at SR equals with $0.1$ (with $50\%$ and $6\%$ improvements for RMSE and MAE, respectively). However, our framework excels with the MLP regression operator, improving RMSE and MAE by $20\%$ and $17\%$, respectively. The LR operator's performance gap on the Stock dataset is minor.
For other datasets, our framework consistently surpasses Apollo across different SR values. This demonstrates the effectiveness of our similarity search approach, which enhances data quality and reduces $\Phi$ prediction errors by identifying relevant datasets $D_r$ from the data lake directory $D$. The Adult dataset also highlights our framework's advantage with increasing feature dimensions.
Although Apollo achieves better raw speedup due to the higher complexity of our framework's vectorization step, our framework outperforms it in amortized speedup. By excluding the reusable vectorization process, it achieves speed gains of $10\%$ to $60\%$ for most operators.
The SR approach, leveraging vector embedding representations, enhances operator prediction compared to Apollo and achieves greater amortized speedup. However, the similarity search method outperforms both Apollo and the SR approach in prediction accuracy and amortized speedup, establishing its clear superiority across most datasets and operator scenarios.

\subsection{NumTabData2Vec Evaluation Results}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\textwidth]{Figures/Results/Representation/V200_representation.pdf}
    \caption{Vector representation for each dataset from NumTabData2Vec}
    \label{fig:eval-data-repr}
\end{figure}


\begin{table}[!htp]
    \centering
    \caption{Similarity between vectors of different datasets scenarios}
    \label{tab:vec-rep-sim}
    \setlength\doublerulesep{0.5pt}
    \begin{tabular}{||c|c||}
    \hline
    Model Name & Similarity \\
    \hline\hline
     \makecell{NumTabData2Vec\\$100$ Vector size} & $0.54$\\
     \hline
      \makecell{NumTabData2Vec\\$200$ Vector size}   & $0.18$\\
      \hline
       \makecell{NumTabData2Vec\\$300$ Vector size}  & $0.16$\\ \hline
    \end{tabular}
\end{table}

% Our proposed model, \textit{NumTabData2Vec}, for dataset vectorization is compared between all the available dataset scenarios to determine whether it can effectively distinguish between them based on qualitative differences. The comparison involves selecting $n$ random datasets for each detaset scenario and projecting them into their respective vector embedding representations. Then for each dataset scenario, it gains the average vector embedding representation by the average vector embedding representation of the $n$ random datasets. The vector embedding representation for each dataset scenario depicted in Figure \ref{fig:eval-data-repr} in from the $k$-dimensional space (size of $200$) transformed to the 3d space using the PCA. Figure \ref{fig:eval-data-repr} demonstrates that each dataset occupies a distinct dimension, with non-overlapping or clustering closely together. This indicates that \textit{NumTabData2Vec} can identify the datasets from various situations and does not have a close representation like previous methods achieved it with the same accuracy but on different data types (such as word, and graphs) \cite{b8Word2Vec, b9Graph2Vec} and not in an entire dataset. Table \ref{tab:vec-rep-sim}, further illustrates the average cosine similarity between the vector embeddings of all datasets, demonstrating how dissimilar are the datasets in their vector representation. As the size dimension of the vector embedding representation increases, the model's ability to distinguish across datasets improves as their average similarity decreases. Furthermore, this indicates that larger vector dimension sizes are unneeded since between $100$ and $300$ is sufficient.

Our proposed model, \textit{NumTabData2Vec}, was evaluated to determine its ability to distinguish dataset scenarios based on qualitative differences. For each scenario, $n$ random datasets were selected, and their vector embeddings averaged to represent the scenario. These embeddings, initially in a 200-dimensional space, were projected into 3D using PCA and are shown in Figure \ref{fig:eval-data-repr}. The figure illustrates that each dataset scenario occupies a distinct space, with minimal overlap or clustering. This demonstrates that \textit{NumTabData2Vec} effectively distinguishes datasets, outperforming prior methods like Word2Vec and Graph2Vec \cite{b8Word2Vec, b9Graph2Vec}, which achieved similar accuracy but on different data types (e.g., words, graphs) rather than entire datasets. Table \ref{tab:vec-rep-sim} further highlights the average cosine similarity between dataset embeddings, showing greater dissimilarity as vector dimensions increase. However, results suggest that dimensions between $100$ and $300$ are sufficient for accurate distinction, avoiding the need for larger vector sizes.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\textwidth]{Figures/Results/Representation/plot_representation_200Vectors.pdf}
    \caption{Synthetic data vector embedding representation}
    \label{fig:eval-sd-data-repr}
\end{figure}

To evaluate \textit{NumTabData2Vec}'s ability to distinguish datasets with varying row and column counts, we generated synthetic numerical tabular datasets of different dimensions and vectorized them. Figure \ref{fig:eval-sd-data-repr} shows datasets with columns ranging from three to thirty and rows from ten to one thousand, projected from a $200$-dimensional space to 2D using PCA. Each bullet caption c and r corresponds to the columns and rows of the dataset, respectively. Datasets with the same number of columns cluster closely in the representation space, and a similar pattern is observed for datasets with the same number of rows. These results indicate that our method effectively distinguishes datasets based on size during vectorization.

\begin{table}[!htp]
    \centering
    \caption{NumTabData2Vec execution time for different dataset dimensions and different vector sizes }

    \begin{adjustbox}{width=\columnwidth,center}
    \label{tab:vec-exec-time}
    \setlength\doublerulesep{0.5pt}
    \begin{tabular}{||c|c|c|c|c||}
    \hline
     \makecell{\# of columns} & \makecell{\# of rows} & \makecell{$50$ Vectors\\Execution time} & \makecell{$100$ Vectors\\Execution time} & \makecell{$200$ Vectors\\Execution time} \\
    \hline\hline
     $3$ & $100$ & $0.0004$ sec & $0.00042$ sec & $0.00051$ sec\\ \hline
     $3$ & $500$ & $0.0004$ sec & $0.00041$ sec & $0.00049$ sec\\ \hline
     $3$ & $1000$ & $0.0004$ sec & $0.00041$ sec & $0.00049$ sec\\ \hline
     $3$ & $1500$ & $0.0004$ sec & $0.00041$ sec & $0.00055$ sec\\ \hline
     $3$ & $1800$ & $0.0004$ sec & $0.00041$ sec & $0.00055$ sec\\ \hline
     \hline
     $10$ & $100$ & $0.0004$ sec & $0.0004$ sec & $0.00057$ sec\\ \hline
     $10$ & $500$ & $0.00039$ sec & $0.0004$ sec & $0.00051$ sec\\ \hline
     $10$ & $1000$ & $0.00041$ sec & $0.00042$ sec & $0.00052$ sec\\ \hline
     $10$ & $1500$ & $0.00041$ sec & $0.00042$ sec & $0.00055$ sec\\ \hline
     $10$ & $1800$ & $0.00041$ sec & $0.00042$ sec & $0.00052$ sec\\ \hline
     \hline
     $20$ & $100$ & $0.0004$ sec & $0.00042$ sec & $0.0005$ sec\\ \hline
     $20$ & $500$ & $0.0004$ sec & $0.00042$ sec & $0.0005$ sec\\ \hline
     $20$ & $1000$ & $0.00042$ sec & $0.00043$ sec & $0.00052$ sec\\ \hline
     $20$ & $1500$ & $0.00043$ sec & $0.00044$ sec & $0.00054$ sec\\ \hline
     $20$ & $1800$ & $0.00044$ sec & $0.00044$ sec & $0.00054$ sec\\ \hline    
     \hline\hline
    \end{tabular}
    \end{adjustbox}
\end{table}

To evaluate how dataset dimensions affect the execution time of \textit{NumTabData2Vec}, we created synthetic datasets with varying numbers of rows ($100$, $500$, $1000$, $1500$, and $1800$) and columns ($3$, $10$, and $20$). These datasets were vectorized into different dimensions, and the execution times were recorded. Table \ref{tab:vec-exec-time} shows that increasing the k-dimension requires approximately $20\%$ more time to generate the vector embeddings. This is expected, as a higher k-dimension involves more hyperparameters, which naturally increases computation time.

Interestingly, varying the number of columns did not significantly impact execution time. However, increasing the number of rows resulted in approximately $5\%$ additional execution time. This is because larger datasets require the extraction of more features, which has a modest impact on the model's execution time.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\textwidth]{Figures/Results/Representation/plot_representation_noise_data_200Vectors.pdf}
    \caption{HPC Dataset vector embedding representation with addition of Noise}
    \label{fig:eval-nd-data-repr}
\end{figure}

To evaluate \textit{NumTabData2Vec}'s ability to distinguish datasets based on different properties like distribution and order, we introduced Gaussian noise to random $l\%$ of data tuples in an HPC dataset. Figure \ref{fig:eval-nd-data-repr} visualises the original and noise-modified datasets, projected from a 200-dimensional space to 2D using PCA. Each bullet caption g denotes the percentage of Gaussian noise added in the dataset. As noise increases, the representation space shifts further from the original dataset, indicating that \textit{NumTabData2Vec} effectively captures distribution differences. Additionally, since the HPC dataset has an inherent order, the model's sensitivity to noise demonstrates its ability to distinguish datasets based on ordering as well.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\textwidth]{Figures/Results/Representation/plotrepresentationnoisedata1col200Vectors.pdf}
    \caption{HPC Dataset vector embedding representation with addition of Noise in the first column}
    \label{fig:eval-nd-data-repr-1col}
\end{figure}

To evaluate how fine-grained as distinction can be, we introduced noise into a single column and repeated the previous experiment, with the difference being that noise was added exclusively to the first column. Figure \ref{fig:eval-nd-data-repr-1col} visualizes the dataset's 2D vector space. The amount of Gaussian noise added to the dataset's first column is indicated by g in the bullet caption. The results show that as more noise is introduced to the column, the vector representation moves further away from the original dataset. In contrast to the previous experiment shown in Figure \ref{fig:eval-nd-data-repr}, the noisy dataset's representation stays closest to the original when only a single column is modified. Also in this experiment the dataset points in the 2-dimension are more grouped between them instead the previous experiment. 

%closely grouped compared to the previous experiment.