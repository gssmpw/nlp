% Na balw mathimatika mesa
\section{Methodology}

\begin{figure*}[!htbp]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=\textwidth, keepaspectratio]{Figures/Methodology/VectorEmbeddingFramework.pdf}
    \caption{Pipeline framework architecture}
    \label{fig:fig-method1}
\end{figure*}

In this section, we describe our proposed framework for modelling analytic operators over a large number of available input datasets. We also describe our approach for vectorizing tabular datasets, \emph{NumTabData2Vec}.

\subsection{Framework Architecture}

Consider a data lake store that contains a (possibly large) number $n$ of datasets $D = \left( d_1,\ d_2,\ d_3,\ \ldots,\ d_n\right)$. Also, let us consider an analytics operator (e.g., a ML algorithm) $\Phi$ and an ``unseen" dataset $D_o$. We assume that each $D_i, 1 \leq i \leq n $ as well as $D_o$ consist of records with the same number of columns with numerical values only. Each dataset can, naturally, have a different number of records. Operator $\Phi$ consumes a single such dataset as input to produce a single numerical output: $\Phi: D_i \rightarrow \mathbb{R}$. 
Our goal is to predict $\Phi(D_o), \forall D_o$ with a minimal cost and prediction error. Our premise is that the output of an analytical operator on any unseen $D_o$ can be accurately modelled and runtime efficiency improved by selecting a small subset of $D$. This can be achieved by identifying the most similar datasets to $D$, denoted: $D_o$: $D_r\subseteq D_o$.
% Our premise is that the output of an analytical operator on any unseen $D_o$ can be accurately modelled if we consider only a small subset of $D$ which corresponds to the most similar datasets to $D_o$: $D_r\subseteq D_o$. 
% Additionally, we assume that randomly selecting a small subset of datasets, $D_s\subseteq D_o$ can improve runtime efficiency for model an operator $\Phi$. However, this approach is likely to yield lower accuracy compared to the previous assumption.
 
% Datasets in $D_r$ have the most similar features in terms of order, data attributes, and distribution to $D_o$. While, datasets in $D_s$ trying to identify $D_o$ data features attributes by covering most of datasets $D$ in whole representation. In previous work \cite{b7Apollo1, b7Apollo2}, individual similarity functions that targeted order, size and distribution were necessary in order to assess similarity over a \emph{single} such quality. In this work, we propose the use of the embedding vectorization $D_z$ in order to find the most similar datasets $D_r$. 

Datasets in $D_r$ closely match $D_o$ in their properties (e.g., order, distribution, and size to name a few). Previous work \cite{b7Apollo1, b7Apollo2} had to use separate similarity functions for each such property. In contrast, we leverage the embedding vectorization ($D_z$) to efficiently identify the most similar datasets using all dataset properties.

% Given a data lake $D = \left( d_1,\ d_2,\ d_3,\ \ldots,\ d_n\right)$ containing $n$ datasets, and a Dataset $D_o$ we want to find a subset of datasets, $D_r = \left( d_{r_1},\ d_{r_2},\ d_{r_3},\ \ldots,\ d_{r_k}\right) $from $D$ which are the most relevant dataset according to $D_o$. Giving the datasets $D_r$ an analytic operator $\Phi$ will be created where the prediction on the dataset $D_o$ will be enhanced. To achieve that the most relevant dataset's $D_r$ from the data lake $D$ must valid the following condition, 
% \begin{equation}
%     D_r\subseteq D_o \label{eq_1_method}
% \end{equation}
% where the dataset's $D_o$ is a subset to $D_r$ with common characteristics. Relevant datasets $D_r$, are those with the closest characteristics between them in different aspects such as order, related data features and distribution.

Figure \ref{fig:fig-method1} depicts the pipeline of our proposed framework. Datasets in $D$ are transformed into $k$-dimensional vectors via our \textit{NumTabData2Vec} embedding scheme. Dataset vectorization needs no recomputation as the vectors are saved and can be reused for the rest of the framework steps. Each time a new dataset $D_o$ needs to be inferred relative to an analytics operator $\Phi$, its vector embedding is equally created. The datasets used for the creation of the model are selected via similarity search. Using different similarity functions, a small subset of datasets similar to $D_o$ are identified and used in order to create a model that predicts $\Phi(D_o)$. With this approach, our framework is utilizing ``right quality" data in its inference mechanism, with irrelevant and extraneous datasets being excluded from the modelling process.

% Via similarity search (using different similarity functions), a small subset of datasets similar to $D_o$ are identified and they are used in order to create a model that predicts $\Phi(D_o)$. With this approach, our framework is utilizing data of the ``right quality" in its inference mechanism, with irrelevant and extraneous datasets being excluded from the modelling of the analytic operator $\Phi$. 
% The SR approach selects a small percentage ($l\%$) of datasets based on their vector embeddings, using a unified random selection strategy to ensure broad representation coverage without duplicates. While this method resembles \cite{b7Apollo1}, it introduces a key difference: the Neural Network (NN) predicting operator values is trained on vector embeddings generated by \textit{NumTabData2Vec}. 

% Additionally, when $D_r$ datasets are selected through similarity search, the operator $\Phi$ is trained on dataset features, enabling it to produce multi-value outputs.

% Furthermore, when the SR approach is selected from the vector embedding representation are selected $l\%$, using a unified random dataset selection where selects datasets that cover most of the representation. Also, utilising the SR method we output an operator modelling similar to \cite{b7Apollo1}. However, with the difference that the Neural Network (NN) which is trained to output the operator value takes as an input the vector embedding representation from \textit{NumTabData2Vec} as input. Furthermore, when the operator is modelled using the $D_r$ datasets from similarity search, the operator $\Phi$ is trained using the datasets features and the model output can be a multi-value. 

% Via similarity search (using different similarity functions), a small subset of datasets similar to $D_o$ are identified and they are used in order to create a model that predicts $\Phi(D_o)$.  With this approach, our framework is utilizing data of the ``right quality" in its inference mechanism, with irrelevant and extraneous datasets being excluded from the modelling of the analytic operator $\Phi$.

The datasets are embedded by our NumTabData2Vec method. It takes an entire dataset as input and transforms it into a $k$-dimensional vector embedding representation space named $z$. The Vector embedding $z$, is a lower dimension representation of the dataset with the entire characteristics of the dataset being represented. Each dataset from $D$ is projected into such a $k$-dimensional vector embedding representation. The datasets are consequently represented by $D_z$ which contains all vector representations $\left( z_1,\ z_2,\ z_3,\ \ldots,\ z_n\right)$. Dataset $D_o$ is also vectorized into $z_o$. Using the embedding representation $z_o$ and applying different similarity functions over the vector representations $D_z$, we may choose the most similar subset of $D$. 
%Otherwise, from the vectors embedding representation $D_z$ are selected randomly using a SR, keeping knowledge from whole dataset representation. We then apply the desired operator $\Phi$ over each of the identified datasets, collecting a small set of outputs. 
The final step of the pipeline involves the operator modelling with any relevant method (e.g., Linear Regression, SVM, Multi-Layer Perceptron, etc.). This model can be used in order to infer the value of $\Phi(D_o)$.


% We then apply the desired operator $\Phi$ over each of the identified datasets, collecting a small set of outputs. The final step of the pipeline involves the operator modelling with any relevant method (e.g., Linear Regression, SVM, Multi-Layer Perceptron, etc.) which is depending based on the dataset selection method. Model output value to $D_o$ 

% We then apply the desired operator $\Phi$ over each of the identified datasets, collecting a small set of outputs. The final step of the pipeline involves the modelling of the operator with any relevant method (e.g., Linear Regression, SVM, Multi-Layer Perceptron, etc.). This model can be used in order to infer the value of $\Phi(D_o)$.

% Our framework have a diverse of analytic operators and ML models where different configurations could be applied. MLP classifier can be model using different optimisation method this of stochastic gradient descent and adam optimiser methods. While logistic regression and svm classifiers can be trained with the traditional way or using the SGD optimiser and improve their prediction.



% Giving a data lake $D$ containing $n$ datasets $\left( d_1,\ d_2,\ d_3,\ \ldots,\ d_n\right)$, and a dataset $D_o$, that includes organisational requirements for the generation of an analytic operator $\phi$, must find among the data lake the most relevant dataset to $D_o$. To be valid the following conditions must be satisfied, 
% \begin{equation}
%     D_o\subseteq D_r \label{eq_1_method}
% \end{equation}
% where the relevant datasets $D_r$ from the data lake D, are a subset of $D_o$ and can produce an analytic operator $\phi$, where prediction accuracy will be higher. Analytic operator $\phi$ accuracy will be improved as the highest quality of data based on the organisation requirements will be selected and on top of that, we are expecting that the performance of the model will be improved.

% Figure \ref{fig:fig-method1}, depicts the proposed pipeline framework architecture, in which equation \ref{eq_1_method} is used and selects the most relevant dataset for the creation of an analytic operator $\Phi$ based on the organisation requirements from the dataset $D_o$. The datasets are processed by a new proposed method called NumTabData2Vec, which takes a whole tabular dataset as an input and transforms it from a high dimensional data into a lower vector dimensional representation $z$ of $k$-dimension, which includes the representation of each dataset. Dataset vectorisation will assist in dealing with the curse of dimensionality problem that have existed in recent years in big data analytics. Furthermore, this newly proposed method can be applied in any dataset scenario that has been passed through the training of the model without the need for any online learning technique in every time that it is used. Providing a new dataset containing the requirements of the organisation is transformed into a vector and, using a variety of techniques (such similarity, clustering, etc.), it choose the most relevant datasets $D_r$ based on the equation \ref{eq_1_method}. The following condition is applied using the relevant datasets,
% \begin{equation}
%     Algorithm\left( D_r \right) \rightarrow \Phi \label{eq_2_method}
% \end{equation}
% where using an algorithm (e.g., SVM, LR, MLP, etc.), produce an analytic operator $\phi$, which includes all the organisation requirements to achieve higher prediction accuracy.


% based the organisation requirements and the analytic operator that they want to create we must select the most relevant dataset to create the analytic operator $O$. Given a new dataset $D_o$, where contains organisation requirements for the creation of the analytic performance to improve the prediction must selects the most relevant dataset among the data lake D where,



% Giving a data lake $D$ with $n$ datasets $\left( d_1,\ d_2,\ d_3,\ \ldots,\ d_n\right)$, based the organisation requirements and the analytic operator that they want to create we must find and select the most relevant dataset to create the analytic operator. Figure \ref{fig:fig-method1}, depicts the proposed pipeline framework architecture, for the selection of the most relevant datasets and creates an analytic operator. Datasets are transformed from a higher dimensional data to a lower dimensional data dimension vector representation with dimension $k$, using a new proposed method named NumTabData2Vec. Dataset vectorisation, will assist to deal with the curse of dimensionality problem where exists in the recent years in big data analytics. Giving a new dataset which contains the organisation requirements, transformed into vector and with various methods (like similarity, clustering, etc.) selects the most relevant dataset and produce an operator $F$.

\subsection{NumTabData2Vec}
\begin{figure}[!ht]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.5\textwidth]{Figures/Methodology/NumTabData2Vec.pdf}
    \caption{NumTabData2Vec deep learning model architecture}
    \label{fig:fig-method2}
\end{figure}

This method transforms a dataset $D_i$ to a lower dimension representation $z$ by using only the numerical values of the dataset's records and excluding metadata, column names, and filenames. We present a deep learning model architecture based on the variational autoencoder (VAE) \cite{b1VAE} concept. For each dataset, data is projected from a high dimensional space to a lower vector embedding representation space $z$, with $1\times k$ dimension. The proposed method is thus defined as: 
\begin{equation}
    NumTabData2Vec\left( D_i \right) \rightarrow z, \label{eq_3_method}
\end{equation}
where the model takes an $m\times n$ dimensional numerical dataset and projects it to a lower $k$-dimension space $z$ ($k > 1$). We wish for our method 
to be generally applicable and able to use any scenario dataset that learns to project a vector embedding representation during the training phase. Also, we wish this method to be able to learn this vector embedding representation using a diverse amount of data during the training phase, and execute thereafter without the need of further training, fine-tuning, or online-training methods. Finally, we need our scheme to be able to quickly and precisely extract the vector for every dataset input as well as deal with varying dataset dimensions without requiring any modifications. 

The deep learning model architecture is depicted in Figure \ref{fig:fig-method2}, where a dataset $D_i$ with dimensions $m \times n$ passes through the encoder and is projected into a vector embedding representation $z$. The dataset has to be recreated since the decoder uses the same layers as the encoder but in the opposite direction. The learning of the vector representation $z$ is using the probabilistic encoder $q_\phi\left(z|x\right)$ distribution and the probabilistic decoder $p_\theta\left(x|z\right)$ distribution utilising the Kullback–Leibler (KL) divergence \cite{b2KLDivergence}. To achieve that, the following condition:
\begin{equation}
    minD_{KL}\left( q_\phi\left(z|x\right) \Vert p_\theta\left(x|z\right) \right)\label{eq_4_method}
\end{equation} 
of Kullback–Leibler (KL) divergence must be minimised. % ** I put the minimised there 
To learn the new vector representation $z$, the dataset must be reconstructed back from $z$ to their input format using the decoder, to verify if the vector representation is on a lower dimension in terms of the input dataset. This is the reconstruction loss. The loss function can be described as:
\begin{equation}
     \resizebox{.9\hsize}{!}{ $\mathcal{L}_{\theta, \phi}\left(x\right) = \mathbb{E}_{q_\phi\left(z|x\right)}\left(\log{\left(p_\theta\left(x|z\right)\right)}\right) - D_{KL}\left( q_\phi\left(z|x\right) \Vert p_\theta\left(x|z\right) \right) \label{eq_5_method} $}% aDD LOSS FUNCTION
\end{equation}
This loss function is called evidences lower bound (ELBO). While the KL divergence is minimised to learn the vector embedding representation $z$, the ELBO is maximized so the condition, 
\begin{equation}
      argmax\mathcal{L}_{\theta, \phi}\left(x\right) \label{eq_6_method}
\end{equation}
must be satisfied. The Decoder $p_\theta\left(x|z\right)$ is only used during the training phase to teach the encoder how to project the vector embedding representation $z$.

The decoder takes as an input the dataset $D_i$, first extracts the feature embeddings from the input layer and passes them into the Transformer layers. For the transformer layer we are using the pre-LN Transformer layer \cite{b3PreNormTL} instead of the traditional post-LN transformer layer where the normalisation layer is employed inside the residual connection and before the prediction of the feed-forward layer. Our model design takes into account and uses the Pre-LN Transformer layer as it demonstrates improved learning outcomes with shorter training times \cite{b3PreNormTL}. % Added reference
Following that, the transformed embedding space is projected onto a probabilistic distribution vector space $z$ using the mean ($\mu$) and standard deviation ($\sigma$). Then the transformed embedding space using $\mu$ and $\sigma$ are projected into a probabilistic distribution vector space $z$. 
The vector embedding space $z$ is a lower-dimensional representation of the dataset $D_i$ that preserves all the essential information needed to describe the dataset within the space $z$. % rephrase BY Andreas
The higher the $k$-dimension of the $z$ vector embedding representation, the higher and more accurate is this representation, as more features are extracted from the dataset \cite{b12Dataset2Vec}.% ** reference for this here** They mention : higher dimensional word vectors will improve the accuracy


% Transform a dataset $D_i$, from a high representation to a lower vector dimensional representation $z$ by using only the features of the entire dataset and excluding metadata, column names, and filenames. Furthermore, we designed a model architecture based on the variational autoencoder (VAE) \cite{b1VAE} model architecture concept. For each dataset, we project the data from a high-dimensional space, $z$, dimension $1\times k$, to a lower vector dimensional space, $z$, using only the tabular data features. The following equation represents out proposed method,
% \begin{equation}
%     NumTabData2Vec\left( D_i \right) \rightarrow z \label{eq_3_method}
% \end{equation}
% where the model takes a tabular dataset $m\times n$ dimension without embedding separate different parts of the dataset and projects it to a lower vector dimension space $z$. In addition, we wanted our model to be able to use any scenario dataset, that learns his representation through the training only require a little amount of data during training, and be able to function with any dataset thereafter without the need for further training, fine-tuning, or online training methods. Moreover, we needed our model to be able to quickly and precisely extract the vector for every dataset circumstance. Furthermore, the model would be able to deal with varying dataset dimensions without requiring any modifications. 


% The model architecture is depicted in Figure \ref{fig:fig-method2}, where a dataset $D_i$ $m \times n$ dimension pass through the encoder and projected into a vector embedding dimension $z$. The dataset has to be recreated since the decoder uses the same layers as the encoder but in the opposite direction. To learn this new vector dimension $z$, we encode the data to the vector space $z$, using the encoder $q_\phi\left(z|x\right)$ distribution and the decoder $p_\theta\left(x|z\right)$ distribution must minimise the Kullback–Leibler (KL) divergence \cite{b2KLDivergence} and the condition 
% \begin{equation}
%     minD_{KL}\left( q_\phi\left(z|x\right) \Vert p_\theta\left(x|z\right) \right)\label{eq_4_method}
% \end{equation} 
% must be fulfilled. To learn the new vector dimension $z$, must reconstructed back from the vector dimension $z$ to the input dataset to verify if the vector dimension is on a lower dimension in terms of the input dataset and this is named as the reconstruction loss. The loss function can be described as 
% \begin{equation}
%      \resizebox{.9\hsize}{!}{ $\mathcal{L}_{\theta, \phi}\left(x\right) = \mathbb{E}_{q_\phi\left(z|x\right)}\left(\log{\left(p_\theta\left(x|z\right)\right)}\right) - D_{KL}\left( q_\phi\left(z|x\right) \Vert p_\theta\left(x|z\right) \right) \label{eq_5_method} $}% aDD LOSS FUNCTION
% \end{equation}
% the loss function is named as evidences lower bound (ELBO). While the KL divergence is minimising to learn the vector embedding representation $z$, the ELBO is maximizing so the condition, 
% \begin{equation}
%       argmax\mathcal{L}_{\theta, \phi}\left(x\right) \label{eq_6_method}
% \end{equation}
% must be satisfied. The Decoder $p_\theta\left(x|z\right)$, is only used during the training phase to learn the encoder how to project the vector embedding representation $z$.


% Decoder taking as input the dataset $D_i$ first extracts the features embeddings from the input layer and passes them into the Transformer layers. For the transformer layer we are using the Pre-LN Transformer layer \cite{b3PreNormTL} instead of the traditional post-LN transformer layer where the normalisation layer is employed inside the residual connection and before the prediction of the feed-forward layer. Our model design takes into account and uses the Pre-LN Transformer layer as it demonstrates improved learning outcomes with shorter training times. Following that, the transformed embedding space is projected onto a probabilistic distribution vector space $z$ using the mean ($\mu$) and standard deviation ($\sigma$). Then the transformed embedding space using the mean ($\mu$) and the standard deviation ($\sigma$) are projected into a probabilistic distribution vector space $z$. Vector embedding space $z$ is a lower dimension representation of the dataset $D_i$ where is all the information to describe the dataset.  As higher is the  $k$-dimension of the $z$  vector embedding representation, then higher and more accurate is this representation as more features are extracted from the dataset.

\subsection{Dataset Selection}
\begin{algorithm}
\caption{K-Means Clustering Algorithm for dataset selection}
\begin{algorithmic}[1]
\label{algo_1_K_Means}
\Require Data Lake Vectors $\mathbf{V} = \{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n\}$, $D_o$ vector $v_o$, range of clusters $S = (2, \dots, p)$

\State \textbf{Initialize} the number of cluster $s$ using Silhouette score: $s = Silhouette Score(V, S)$

\State \textbf{Initialize} the $s$ cluster centroids $\mathbf{C} = \{\mathbf{c}_1, \mathbf{c}_2, \dots, \mathbf{c}_s\}$ randomly from the vectors $\mathbf{V}$.
\Repeat
    \State \textbf{Assignment Step:}
    \For{each vector $\mathbf{v}_i \in \mathbf{V}$}
        \State Assign $\mathbf{v}_i$ to the nearest centroid based on Euclidean distance:
        \[
        \text{Assign } \mathbf{v}_i \text{ to cluster } j = \arg\min_{j} \|\mathbf{v}_i - \mathbf{c}_j\|^2
        \]
    \EndFor
    \State \textbf{Update Step:}
    \For{each centroid $\mathbf{c}_j$}
        \State Update $\mathbf{c}_j$ as the mean of all vectors assigned to cluster $j$:
        \[
        \mathbf{c}_j = \frac{1}{|\{ \mathbf{v}_i \in C_j \}|} \sum_{\mathbf{v}_i \in C_j} \mathbf{v}_i
        \]
    \EndFor
\Until{Centroids $\mathbf{C}$ do not change significantly}
\State Save the cluster model \textbf{K-Means}
\State Find in which cluster the vector $v_o$ belongs,\par
\hskip\algorithmicindent $c = \textbf{K-Means}(v_o)$
\State Find which datasets $D_r$ are belongs to cluster $c$
\State \textbf{Return} Datasets $D_r$
\end{algorithmic}
\end{algorithm}


The selection of the most similar datasets has been implemented using three different approaches. Different similarity functions are easily plugged into our pipeline. The first method uses the cosine similarity to determine how similar two datasets are in their vector embedding space $z$. Cosine similarity calculates the angle between the two vectors and a measure of similarity that is independent of the vector magnitudes. The higher its value, the most similar are the two datasets. The following equation describes the cosine similarity between vectors $A$ and $B$: 
\begin{equation}
    \cos\left(\theta\right) = \dfrac {A \cdot B} {\left\| A\right\| _{2}\left\| B\right\| _{2}} = \dfrac{\sum_{i=1}^{n} A_iB_i}{\sqrt{\sum_{i=1}^{n} A^2_i} \sqrt{\sum_{i=1}^{n} B^2_i}} \label{eq_7_cos_sim}
\end{equation}
 The alternative method involves calculating the Euclidean distance between the two vectors in order to determine how similar the two datasets are. Euclidean distance is particularly useful for capturing the geometric closeness between vectors and finding the most relevant datasets based on the organisation's requirements. The Euclidean distance between vectors $\mathbf{u}, \mathbf{v}$ is defined as:
\begin{equation}
d(\mathbf{u}, \mathbf{v}) = \sqrt{\sum_{i=1}^{n} (u_i - v_i)^2}\label{eq_8_eucl_dist}
\end{equation}
The smaller the distance value the most similar are the datasets. Dataset selection using cosine similarity or euclidean distance selects a percentage of $\lambda, \lambda > 0$ of the closest datasets to dataset $D_o$. The third approach involves utilising the K-Means \cite{b17KMeansa, b17KMeansb} clustering technique to choose the most relevant datasets. The datasets from $D$ are divided into $s$ ($s > 1$) separate clusters, where datasets with similar features are grouped together in the same cluster based on similarity equations.
The silhouette scores \cite{b18Silhouettes} are used to determine the number of clusters $s$ for each possible value of $S$. This is done by the following equation:
\begin{equation}
    SilhouetteScore(v_i) = \frac{b(v_i) - a(v_i)}{\max(a(v_i), b(v_i))} \label{eq_9_shilouete_score}
\end{equation}
where for each vector $v_i$ finds the mean intra-cluster distance ($a(v_i)$) which is the distance with the other vectors in the same cluster, and the mean nearest-cluster distance ($b(v_i)$) is the minimum average distance with the other vectors in the different cluster. Silhouette Score ranges from $-1$ to $1$ and the higher value defines the best $s$ number for clusters. 

Algorithm \ref{algo_1_K_Means}, shown how k-means clustering is created and how the most relevant datasets $D_r$ are selected based on the target dataset $D_o$. Using the most optimal $s$ value with the highest Silhouette score, K-means clustering is performed on the vector representations $V$ of each dataset, grouping datasets with same features into the same cluster. Next, the algorithm uses the vector $v_o$ of dataset $D_o$ to find the closest cluster centroid. All datasets in that cluster are defined as the relevant datasets $D_r$. These datasets are then used to model the analytics operator at hand.

% Using the SR dataset selection method, our framework selects $\lambda\%$ of datasets from the vector embedding representation. These datasets are randomly and uniformly sampled from the vector embedding representation, ensuring that the operator receives representative knowledge from the entire dataset distribution. This approach allows the operator to be modelled once, eliminating the need for retraining.

% The K-means clustering is created using the best k value as the algorithm \ref{algo_1_K_Means} is shown. Then the vector $v_o$ from the dataset which describing the organisation requirements is applied to the cluster and finds in which cluster it belongs and selects all the relevant datasets from the same cluster to create the analytic operator.




