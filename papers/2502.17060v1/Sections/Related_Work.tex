\section{Related Work}
%Categorisation in related work
Prior efforts largely concentrate on boosting/scaling an algorithm's performance utilizing only the size of data input (record number) without examining its quality. As such, we also discuss works that try to identify the most suitable data features for the optimisation of analytic task operators. Vectorising data to lower embedding representation is a modern method that helps in identifying significant features across data types and datasets. As vector embeddings extract important features from data, we discuss studies that used the feature representation of data tuples to improve ML model prediction.

\subsection{Data Quality}
% Big data applications aim at increasing data quality by focussing on several issues. Dagger \cite{b4DaggerRW} is an end-to-end framework that focuses on enhancing data quality by addressing data errors through code-handling data (e.g., scalar variable, arrays). Dagger allows the design of data pipelines and the interaction with them in order to detect errors at any point in the pipeline. Data error identification is achieved with the use of a an SQL-like language. ReClean \cite{b4ReClean} demonstrates an automated data cleaning method for tabular datasets that uses reinforcement learning. Data cleaning is performed as a sequential decision process in which the manual configuration is purged and the most optimal data repair tools or tool combinations are selected from an agent for the data cleaning and data quality improvement. IterClean \cite{b5IterCleanRW} is an iterative data cleaning framework that utilises a large language model (LLM). At the start of the procedure, the LLM labels a few tuples to better comprehend the unique distribution of data errors. The system accomplishes data cleaning by employing an error detector, an error verification, and an error repairer. In \cite{b6DataShapley}, the authors establish a framework to measure the data quality of a dataset tuple using the Shapley value from game theory. They develop two approximations, the Truncated Monte Carlo Shapley and Gradient Shapley, to estimate the metric of how valuable is data tuple in a dataset for the learning algorithm. Apollo \cite{b7Apollo1, b7Apollo2}, is a content-based method that utilises the similarity between the input datasets the predict the outcome of an analytic operator. The method consists of three step, where a similarity matrix between the datasets is created, and each dataset is projected to a lower dimension space and an operator is modelled using a small random subset of the datasets. In contrast with the above, our method improves data quality by selecting the most relevant datasets for the prediction of an analytic operator. The main goal of our approach is to model analytic operators using the highest quality of data to enhance the model prediction performance.

Big data applications aim to improve data quality by addressing various challenges. Dagger \cite{b4DaggerRW} enhances data quality by detecting errors in data pipelines using an SQL-like language and allowing code-handling for variables and arrays. ReClean \cite{b4ReClean} automates tabular data cleaning via reinforcement learning, optimizing tool selection for error repair. IterClean \cite{b5IterCleanRW} employs a large language model (LLM) to iteratively clean data by labelling initial tuples and using error detection, verification, and repair. In \cite{b6DataShapley}, data tuple quality is measured using Shapley values from game theory, with Truncated Monte Carlo Shapley and Gradient Shapley methods estimating a tuple's value to a learning algorithm.
Apollo \cite{b7Apollo1, b7Apollo2} is a content-based method that predicts the outcome of an analytic operator by leveraging dataset similarity. It involves three steps: creating a similarity matrix, projecting datasets to a lower-dimensional space, and modelling the operator using a small random subset of datasets. Unlike Apollo, our approach selects the most relevant, high-quality datasets to model analytic operators, aiming to improve prediction performance. Moreover, modelling and similarity occur over the vector embeddings which are able to incorporate all dataset properties. In contrast, similarity functions used in \cite{b7Apollo1} can only target a single property.

% \begin{itemize}
%   \item Apollo:
%   \item IterClean Done
%   \item Dagger. Done
%   \item Data Shapley Done
%   \item  ReClean Done
% \end{itemize}

\subsection{Dataset Selection Inference}
% Right data selection leads to greater performance in analytics models since the data quality improves. The work in \cite{b19SOALA} developed the SOALA to select the most suitable data features utilising online pairwise comparisons of the data to maintain a ML model over time. Group-Soala is an extension that created where additional data features groups came. Group-SAOLA is capable of online group maintenance and obtaining the highest quality new group set. In \cite{b20tfdata}, the authors propose tf.data, an API framework which allows pipelines for machine learning tasks with various capacities to be created, with main the main goal to enhancing data quality. This approach is able to choose the most related dataset for a machine learning model creation, as well as the best data features. Our pipeline framework uses datasets vector embedding representation to choose the most appropriate datasets for modelling an analytic operator or an ML model, leading to improved prediction accuracy.

SOALA \cite{b19SOALA} selects optimal data features through online pairwise comparisons to maintain ML models over time. Its extension, Group-SOALA, introduces group maintenance to identify high-quality feature sets. In \cite{b20tfdata}, the tf.data API framework enables the creation of ML pipelines focused on selecting relevant datasets and features to improve data quality. Similarly, our framework uses dataset vector embeddings to select the most suitable datasets for modelling analytic operators or ML models, enhancing prediction accuracy.

\subsection{Data Vectorization and Embeddings}

% Data vectorizations goal is to project different types of data from a higher dimension space to a lower dimension. Word2Vec \cite{b8Word2Vec} vectorizes words to a lower continuous vector space proposing two Neural Network architectures the Continuous Bag of Words and Skip-Gram, in which the word context around the vector embedding representation is used to generate it. An unsupervised learning deep neural network architecture called Graph2Vec \cite{b9Graph2Vec} converts a graph into a vector embedding representation.  Their approach mostly consists of dividing the graph into sub-graphs using a skip-graph model to transform it into a vector embedding representation. An aggregation of each sub-graph's vector embedding produces the graph's final vector embedding representation. ImageDataset2Vec \cite{b11Image2Vec}, extracts meta-features from datasets using a pre-trained model and projects them to a vector embedding representation of the whole image dataset. Using the vector representation, the most suitable algorithm for image classification is selected. A dataset to vector representation was proposed in Dataset2Vec \cite{b12Dataset2Vec}. Meta-features from datasets are extracted using the Dataset2Vec model and using the DeepSet model architecture these features are regress to different set of predictor/target pairs. Additionally, a dataset similarity measure was proposed to predict if two vector embeddings are from the same dataset scenario or from a different one. Table2Vec \cite{b13Table2Vec}, generates a vector embedding representation of a whole table taking into consideration not only the data features of the table, but various table elements (such as metadata, caption, column headings, etc.) and entity embeddings. %We can shrink the size of the following sentence
% Four approximations for table vector embedding representation were suggested by them. Table2VecE randomly selects a sequence of entities as input and extracts all entities that appear within the table cells to create the vector representation of the table; Table2VecW considers all table elements; Table2VecH considers only the names of the column headers; and Table2VecE* takes the core column and extracts the vector embedding representation. Inspired by these works, we created a model architecture that takes a tabular dataset as an input and projects it to a vector representation using only the data features of them. 
The goal of data vectorization is to project high-dimensional data into a lower-dimensional vector space. Word2Vec \cite{b8Word2Vec} generates word embeddings using two neural network architectures, Continuous Bag of Words and Skip-Gram, leveraging word context. Graph2Vec \cite{b9Graph2Vec} creates graph embeddings by dividing graphs into sub-graphs with a skip-graph model and aggregating their embeddings. ImageDataset2Vec \cite{b11Image2Vec} extracts meta-features from image datasets to generate embeddings, helping to select the most suitable classification algorithm. Dataset2Vec \cite{b12Dataset2Vec} uses meta-features and the DeepSet model to project datasets into embeddings and measure dataset similarity. Table2Vec \cite{b13Table2Vec} generates table embeddings by incorporating data features, metadata, and structural elements like captions and column headings. Inspired by these methods, we designed a model that generates vector representations of tabular datasets over their record data values, not their metadata.

%\subsection{Vector Embedding for Prediction} 
% Since vector embeddings can extract valuable information from data tuples, they have been used for classification problems. TransTab \cite{b14TransTab} is a flexible tabular learning system that transforms each data tuple into a vector embedding representation and applies a stack of transformer layers for feature encoding to predict a class. A gated transformer with  input consisting of a mixture of table cells and column descriptions, as well as supervised and self-supervised pre-training, to enhance the model's performance. In \cite{b15FTransformer}, proposed two  two model architectures for the classification task a Res-Net approximation and the FT-Transformer. The FT-Transformer utilises a tabular dataset transform to embedding vectorization of the categorical and numerical features. The embeddings then pass through a stack of Transformer layers to predict the class for every tabular dataset data tuple. Tab-Transformer \cite{b16TabTransformer}, tries to address limitations of Multi-Layer Perceptions. Getting as an input a tabular dataset, their categorical features are embedding to a vector representation and pass through a stack of transformer layers, while the continuous features are passed from a normalisation layer. Then the pre-processing extracted features are concatenated and passed through a Multi-Layer Perception model, to predict the class for each data tuple. While previous works tried to enhance the prediction using the embedding representation of each data tuple, our framework implementation uses the vector embedding representation of each dataset to find the most relevant dataset for a scenario to create and improve analytic operator performance.

Vector embeddings, which capture valuable information from data tuples, are widely used in classification tasks. TransTab \cite{b14TransTab} transforms data tuples into vector embeddings, applying transformer layers to encode features and predict classes, enhanced by supervised and self-supervised pretraining. FT-Transformer \cite{b15FTransformer} and Res-Net architectures similarly use embeddings of categorical and numerical features, processed through transformer layers for class prediction. Tab-Transformer \cite{b16TabTransformer} embeds categorical features into vectors, normalizes continuous features, and combines them in a Multi-Layer Perceptron (MLP) to predict classes. Unlike these methods, which focus on tuple-level embeddings, our framework uses dataset-level vector embeddings to identify the most relevant datasets, improving analytic operator performance.



 % \begin{itemize}
 %     \item TransTab
 %     \item Tab Transformer
 %     \item FT Transformer
 % \end{itemize}
