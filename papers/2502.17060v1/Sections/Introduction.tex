\section{Introduction}

Big data technologies daily face the rapid evolution in volume as well as variety and velocity of processed data \cite{b1Intdata_quality_for_data_scienceBenjaminHazen}. Such big data characteristics routinely force analytics pipelines to underperform, requiring continuous maintenance and optimization. One major reason for this is bad quality of data\footnote{\href{https://tinyurl.com/de62sf48}{https://tinyurl.com/de62sf48}}. Poor data quality leads to low data utilisation efficiency and even brings forth serious decision-making errors\cite{b2DataQualityBigDataChai}.

Data quality can be improved when focusing on the actual content of the data. Data-centric Artificial Intelligence (AI) \cite{b3IntDataCentricAI} emphasises on the quality, context, and structure of the data to improve its quality, as well as the analytical or machine learning (ML) algorithmic performance. Understating the data context properties such as data features, origins, relevance, and potential biases plays a critical role in modelling more accurate and reliable models. Data-centric AI prioritises the process of refining and enriching datasets to make them more suitable for real-world applications. In a similar way, a lot of scientists make the case that improving data quality by focusing on the content is a crucial factor in achieving better results \cite{b3IntDataCentricAI}.

A plethora of available data sources and datasets in the organisations' data centres and lakes pose a significant challenge: Deciding which datasets should be selected and passed into analytic workflows in order to produce more accurate results/predictions. While modern analytics workflows have evolved to incorporate diverse operators, the process of selecting the most suitable datasets using data-centric AI techniques to optimise a possibly complex computation is still under research \cite{b4IntDataCentricAI2}. When dataset selection is left to human experts, it can result in low prediction performance and can be time-consuming. Equally costly can be the process of exhaustively executing over all (or many) available dataset inputs, in order to identify high-quality inputs according to business standards. 
%With data-centric AI techniques , organisations can automate the selection of high-quality datasets, enabling the efficient modelling of operators and improving overall performance. This not only enhances prediction accuracy but also saves organisations both time and resources by reducing the complexity and duration of the dataset selection process.

In previous work \cite{b7Apollo1}, predicting the output of an analytics operator assuming a plethora of available input datasets was tackled via the creation of an all-pair similarity matrix, which, relative to the similarity function used, reflected the distance between datasets over a single characteristic (e.g., data distribution). 
%Add vectorisation

To improve the big data analysis and utilize modern AI methods and systems, data or vector embeddings have been suggested. Data embedding vectorization \cite{b8Word2Vec, b9Graph2Vec} aims at projecting data from a high-dimensional representation space into a more compact, lower dimensional space. By extracting meaningful patterns from the data features using deep learning, data is projected to a lower representation space. Using this space, one can find the most similar data points among a dataset.   

%Add out project
For the purpose of improving the accuracy of a modelled analytic operator (i.e., predict the outcome of a ML algorithm without actually executing it due to its cost), we propose a modelling framework that employs vector embeddings for dataset selection from a data lake store that contains a large number of available datasets. In order to predict an operator's output for an ``unseen" query dataset, our method considers only datasets that are \emph{qualitatively similar} to the query, utilising a similarity search over the vector embeddings. The selection of similar datasets reduces the prediction error, as well as the cost to model the operator, under the assumption that realistic analytical operators perform similarly under similar inputs. Similarity search is thus executed on the dataset vector embedding representations in a low-dimensional space. Each vector embedding is generated using our proposed deep learning method, \textit{NumTabData2Vec}, which takes an entire dataset as input and transforms it into a vector embedding representation space. While previous methods split the dataset into chunks and produced a vector for each chunk or used the dataset metadata information, we use the entire dataset utilising all data record values. Our framework thus models an operator using similar datasets to the queried one. An interesting feature of our methodology is the ability of the \textit{NumTabData2Vec} to efficiently distinguish with detail between different datasets and record properties in the lower vector representation space. Furthermore, the same vector representation is used for the modelling of multiple analytics operators, offering enhanced flexibility and time-efficiency.
Compared to similar previous work \cite{b7Apollo1, b7Apollo2}, our work uses state-of-the-art data representation (vector embeddings) which are able to capture multiple data properties that can be used in order to assess similarity, namely record order, dataset size, data distribution, etc. 

The main contributions of our work can be summarised as follows:
\begin{itemize}
\item We introduce a framework for operator modelling in order to predict its outcome on an unseen tabular input dataset from a plethora of available ones. Our method uses dataset vector embedding representations to improve the prediction performance via selecting the most relevant datasets to base its prediction upon.
\item We develop a deep learning model architecture that transforms an entire tabular dataset of numerical values to a vector embedding representation.
\item We provide an experimental evaluation of our proposed methodology using multiple real-world scenarios and compare it directly to system Apollo \cite{b7Apollo1, b7Apollo2}.
\end{itemize}
Our evaluation illustrates that our proposed methodology can model operators with low prediction error by adaptively selecting similar quality datasets for each model. Moreover, it is able to do so with considerable amortized speed-ups. The evaluation of \textit{NumTabData2Vec} indicates that it accurately projects any available dataset into a vector embedding representation and accurately models different dataset properties within the representation space. %**with speed and quality? why is this method better than existing ones? **





