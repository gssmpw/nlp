@misc{boreiko2024realisticthreatmodellarge,
      title={A Realistic Threat Model for Large Language Model Jailbreaks}, 
      author={Valentyn Boreiko and Alexander Panfilov and Vaclav Voracek and Matthias Hein and Jonas Geiping},
      year={2024},
      eprint={2410.16222},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.16222}, 
}

@inproceedings{castello-etal-2024-examining,
    title = "Examining Cognitive Biases in {C}hat{GPT} 3.5 and {C}hat{GPT} 4 through Human Evaluation and Linguistic Comparison",
    author = "Castello, Marta  and
      Pantana, Giada  and
      Torre, Ilaria",
    editor = "Knowles, Rebecca  and
      Eriguchi, Akiko  and
      Goel, Shivali",
    booktitle = "Proceedings of the 16th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)",
    month = sep,
    year = "2024",
    address = "Chicago, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2024.amta-research.21",
    pages = "250--260",
    abstract = "This paper aims to investigate the presence of cognitive biases, more specifically of Availability heuristics, Representativeness heuristics and Framing, in OpenAI{'}s ChatGPT 3.5 and ChatGPT 4, as well as the linguistic dependency of their occurrences in the Large Language Models{'} (LLMs) outputs. The innovative aspect of this research is conveyed by rephrasing three tasks proposed in Kahneman and Tversky{'}s works and determining whether the LLMs{'} answers to the tasks are correct or incorrect and human-like or non-human-like. The latter classification is made possible by interviewing a total of 56 native speakers of Italian, English and Spanish, thus introducing a new linguistic comparison of results and forming a {``}human standard{'}. Our study indicates that GPTs 3.5 and 4 are very frequently subject to the cognitive biases under discussion and their answers are mostly non-human-like. There is minimal but significant discrepancy in the performance of GPT 3.5 and 4, slightly favouring ChatGPT 4 in avoiding biased responses, specifically for Availability heuristics. We also reveal that, while the results for ChatGPT 4 are not significantly language dependent, meaning that the performances in avoiding biases are not affected by the prompting language, their difference with ChatGPT 3.5 is statistically significant.",
}

@misc{chen2024aicognitivelybiasedexploratory,
      title={AI Can Be Cognitively Biased: An Exploratory Study on Threshold Priming in LLM-Based Batch Relevance Assessment}, 
      author={Nuo Chen and Jiqun Liu and Xiaoyu Dong and Qijiong Liu and Tetsuya Sakai and Xiao-Ming Wu},
      year={2024},
      eprint={2409.16022},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.16022}, 
}

@inproceedings{dong-etal-2024-survey,
    title = "A Survey on In-context Learning",
    author = "Dong, Qingxiu  and
      Li, Lei  and
      Dai, Damai  and
      Zheng, Ce  and
      Ma, Jingyuan  and
      Li, Rui  and
      Xia, Heming  and
      Xu, Jingjing  and
      Wu, Zhiyong  and
      Chang, Baobao  and
      Sun, Xu  and
      Li, Lei  and
      Sui, Zhifang",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.64",
    doi = "10.18653/v1/2024.emnlp-main.64",
    pages = "1107--1128",
    abstract = "With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.",
}

@inproceedings{echterhoff-etal-2024-cognitive,
    title = "Cognitive Bias in Decision-Making with {LLM}s",
    author = "Echterhoff, Jessica Maria  and
      Liu, Yao  and
      Alessa, Abeer  and
      McAuley, Julian  and
      He, Zexue",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.739",
    doi = "10.18653/v1/2024.findings-emnlp.739",
    pages = "12640--12653",
    abstract = "Large language models (LLMs) offer significant potential as tools to support an expanding range of decision-making tasks. Given their training on human (created) data, LLMs have been shown to inherit societal biases against protected groups, as well as be subject to bias functionally resembling cognitive bias. Human-like bias can impede fair and explainable decisions made with LLM assistance. Our work introduces BiasBuster, a framework designed to uncover, evaluate, and mitigate cognitive bias in LLMs, particularly in high-stakes decision-making tasks. Inspired by prior research in psychology and cognitive science, we develop a dataset containing 13,465 prompts to evaluate LLM decisions on different cognitive biases (e.g., prompt-induced, sequential, inherent). We test various bias mitigation strategies, while proposing a novel method utilizing LLMs to debias their own human-like cognitive bias within prompts. Our analysis provides a comprehensive picture of the presence and effects of cognitive bias across commercial and open-source models. We demonstrate that our selfhelp debiasing effectively mitigates model answers that display patterns akin to human cognitive bias without having to manually craft examples for each bias.",
}

@inproceedings{human-cognitive,
 author = {Jones, Erik and Steinhardt, Jacob},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {11785--11799},
 publisher = {Curran Associates, Inc.},
 title = {Capturing Failures of Large Language Models via Human Cognitive Biases},
 volume = {35},
 year = {2022}
}
 %url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/4d13b2d99519c5415661dad44ab7edcd-Paper-Conference.pdf}

@inproceedings{indirect-prompt-inject,
author = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
title = {Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection},
year = {2023},
isbn = {9798400702600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605764.3623985},
doi = {10.1145/3605764.3623985},
abstract = {Large Language Models (LLMs) are increasingly being integrated into applications, with versatile functionalities that can be easily modulated via natural language prompts. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We show that LLM-Integrated Applications blur the line between data and instructions and reveal several new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (i.e., without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved at inference time. We derive a comprehensive taxonomy from a computer security perspective to broadly investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We then demonstrate the practical viability of our attacks against both real-world systems, such as Bing Chat and code-completion engines, and GPT-4 synthetic applications. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing reliance on LLMs, effective mitigations of these emerging threats are lacking. By raising awareness of these vulnerabilities, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users from potential attacks.},
booktitle = {Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security},
pages = {79â€“90},
numpages = {12},
keywords = {indirect prompt injection, large language models},
location = {Copenhagen, Denmark},
series = {AISec '23}
}

@misc{jin2024guardroleplayinggeneratenaturallanguage,
      title={GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models}, 
      author={Haibo Jin and Ruoxi Chen and Andy Zhou and Yang Zhang and Haohan Wang},
      year={2024},
      eprint={2402.03299},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.03299}, 
}

@inproceedings{koo-etal-2024-benchmarking,
    title = "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
    author = "Koo, Ryan  and
      Lee, Minhwa  and
      Raheja, Vipul  and
      Park, Jong Inn  and
      Kim, Zae Myung  and
      Kang, Dongyeop",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.29",
    doi = "10.18653/v1/2024.findings-acl.29",
    pages = "517--545",
    abstract = "Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 16 LLMs encompassing four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLer), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (40{\%} of comparisons made by all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 44{\%}, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences.",
}

@misc{kumar2024manipulatinglargelanguagemodels,
      title={Manipulating Large Language Models to Increase Product Visibility}, 
      author={Aounon Kumar and Himabindu Lakkaraju},
      year={2024},
      eprint={2404.07981},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2404.07981}, 
}

@misc{li2023evaluatinginstructionfollowingrobustnesslarge,
      title={Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection}, 
      author={Zekun Li and Baolin Peng and Pengcheng He and Xifeng Yan},
      year={2023},
      eprint={2308.10819},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.10819}, 
}

@misc{liu2024autodangeneratingstealthyjailbreak,
      title={AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models}, 
      author={Xiaogeng Liu and Nan Xu and Muhao Chen and Chaowei Xiao},
      year={2024},
      eprint={2310.04451},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.04451}, 
}

@misc{liu2024automaticuniversalpromptinjection,
      title={Automatic and Universal Prompt Injection Attacks against Large Language Models}, 
      author={Xiaogeng Liu and Zhiyuan Yu and Yizhe Zhang and Ning Zhang and Chaowei Xiao},
      year={2024},
      eprint={2403.04957},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2403.04957}, 
}

@misc{liu2024decoydilemmaonlinemedical,
      title={The Decoy Dilemma in Online Medical Information Evaluation: A Comparative Study of Credibility Assessments by LLM and Human Judges}, 
      author={Jiqun Liu and Jiangen He},
      year={2024},
      eprint={2411.15396},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2411.15396}, 
}

@misc{lou2024anchoringbiaslargelanguage,
      title={Anchoring Bias in Large Language Models: An Experimental Study}, 
      author={Jiaxu Lou and Yifan Sun},
      year={2024},
      eprint={2412.06593},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.06593}, 
}

@inproceedings{lu-etal-2022-fantastically,
    title = "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
    author = "Lu, Yao  and
      Bartolo, Max  and
      Moore, Alastair  and
      Riedel, Sebastian  and
      Stenetorp, Pontus",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.556",
    doi = "10.18653/v1/2022.acl-long.556",
    pages = "8086--8098",
    abstract = "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are {``}fantastic{''} and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13{\%} relative improvement for GPT-family models across eleven different established text classification tasks.",
}

@misc{lyu2024cognitivebiaseslargelanguage,
      title={Cognitive Biases in Large Language Models for News Recommendation}, 
      author={Yougang Lyu and Xiaoyu Zhang and Zhaochun Ren and Maarten de Rijke},
      year={2024},
      eprint={2410.02897},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2410.02897}, 
}

@misc{macmillanscott2024irrationalitycognitivebiaseslarge,
      title={(Ir)rationality and Cognitive Biases in Large Language Models}, 
      author={Olivia Macmillan-Scott and Mirco Musolesi},
      year={2024},
      eprint={2402.09193},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.09193}, 
}

@misc{malberg2024comprehensiveevaluationcognitivebiases,
      title={A Comprehensive Evaluation of Cognitive Biases in LLMs}, 
      author={Simon Malberg and Roman Poletukhin and Carolin M. Schuster and Georg Groh},
      year={2024},
      eprint={2410.15413},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.15413}, 
}

@misc{mckenzie2024inversescalingbiggerisnt,
      title={Inverse Scaling: When Bigger Isn't Better}, 
      author={Ian R. McKenzie and Alexander Lyzhov and Michael Pieler and Alicia Parrish and Aaron Mueller and Ameya Prabhu and Euan McLean and Aaron Kirtland and Alexis Ross and Alisa Liu and Andrew Gritsevskiy and Daniel Wurgaft and Derik Kauffman and Gabriel Recchia and Jiacheng Liu and Joe Cavanagh and Max Weiss and Sicong Huang and The Floating Droid and Tom Tseng and Tomasz Korbak and Xudong Shen and Yuhui Zhang and Zhengping Zhou and Najoung Kim and Samuel R. Bowman and Ethan Perez},
      year={2024},
      eprint={2306.09479},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.09479}, 
}

@misc{nestaas2024adversarialsearchengineoptimization,
      title={Adversarial Search Engine Optimization for Large Language Models}, 
      author={Fredrik Nestaas and Edoardo Debenedetti and Florian TramÃ¨r},
      year={2024},
      eprint={2406.18382},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2406.18382}, 
}

@misc{shayegani2023surveyvulnerabilitieslargelanguage,
      title={Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks}, 
      author={Erfan Shayegani and Md Abdullah Al Mamun and Yu Fu and Pedram Zaree and Yue Dong and Nael Abu-Ghazaleh},
      year={2023},
      eprint={2310.10844},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.10844}, 
}

@misc{sumita2024cognitivebiaseslargelanguage,
      title={Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments}, 
      author={Yasuaki Sumita and Koh Takeuchi and Hisashi Kashima},
      year={2024},
      eprint={2412.00323},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.00323}, 
}

@misc{wang2023largelanguagemodelsreally,
      title={Are Large Language Models Really Robust to Word-Level Perturbations?}, 
      author={Haoyu Wang and Guozheng Ma and Cong Yu and Ning Gui and Linrui Zhang and Zhiqi Huang and Suwei Ma and Yongzhe Chang and Sen Zhang and Li Shen and Xueqian Wang and Peilin Zhao and Dacheng Tao},
      year={2023},
      eprint={2309.11166},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.11166}, 
}

@misc{wang2023robustnesschatgptadversarialoutofdistribution,
      title={On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective}, 
      author={Jindong Wang and Xixu Hu and Wenxin Hou and Hao Chen and Runkai Zheng and Yidong Wang and Linyi Yang and Haojun Huang and Wei Ye and Xiubo Geng and Binxin Jiao and Yue Zhang and Xing Xie},
      year={2023},
      eprint={2302.12095},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2302.12095}, 
}

@misc{wei2023jailbrokendoesllmsafety,
      title={Jailbroken: How Does LLM Safety Training Fail?}, 
      author={Alexander Wei and Nika Haghtalab and Jacob Steinhardt},
      year={2023},
      eprint={2307.02483},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.02483}, 
}

@article{ye2024justiceprejudicequantifyingbiases,
      title={Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge},
      author={Jiayi Ye and Yanbo Wang and Yue Huang and Dongping Chen and Qihui Zhang and Nuno Moniz and Tian Gao and Werner Geyer and Chao Huang and Pin-Yu Chen and Nitesh V Chawla and Xiangliang Zhang},
      journal={arXiv preprint arXiv:2410.02736},
      year={2024}
    }

@misc{zhao2024weaktostrongjailbreakinglargelanguage,
      title={Weak-to-Strong Jailbreaking on Large Language Models}, 
      author={Xuandong Zhao and Xianjun Yang and Tianyu Pang and Chao Du and Lei Li and Yu-Xiang Wang and William Yang Wang},
      year={2024},
      eprint={2401.17256},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.17256}, 
}

