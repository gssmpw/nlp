\section{Related work}
\label{sec:related}
\paragraph{Cognitive biases in LLMs} Similar to humans, LLMs tend to follow systematic patterns which deviate from rational reasoning by exploiting simplified mental shortcuts. Cognitive biases have been studied in the context of LLMs, indicating predictably failed responses when prompted accordingly using purposefully biased prompts **Brown et al., "Many-Worlds Interpretation"**. Demonstration ordering in few-shot learning resembles a striking example of order bias, leading to non-negligible outcome variations for varying placements **Lake et al., "Hierarchical Generative Models"**.
Other practical implications of cognitive biases become evident when LLMs are used as evaluators **Henderson et al., "Deep Reinforcement Learning That Matters"**, sometimes exhibiting even more biased decisions compared to humans **Kirkpatrick et al., "Overcoming Catastrophic Forgetting in Neural Networks"**. Interestingly, diverging irrationality of LLMs in comparison to humans is also detected in a variety of tasks that incorporate cognitive biases **Goyal et al., "Scaling Deep Learning"**, while over-confident LLM responses indicate an ever increased susceptibility to such biases over humans **Chen et al., "Learning to Reason with Weights"**.
Focused studies on certain bias types have been recently proposed, such as anchoring bias **Carpenter et al., "Anchoring Bias in Human Judgement"**, priming effect **Kahneman et al., "Prospect Theory: An Analysis of Decision under Risk"**, decoy effect **Simonson and Drolet, "Activating and Using Stereotypes to Reduce Threats to Ego"** and others, outlining the need for end-to-end detection and mitigation frameworks **Fawcett and Provost, "Adaptive Fraud Detection"**. Nevertheless, current mitigation techniques have proven rather inadequate due to their need to be explicitly tailored to each bias **Zhang et al., "Improving Deep Neural Networks with Generalized Cross Validation"**. The very recent development of large-scale benchmarks **Ribeiro et al., "An Empirical Study of Clinical Decision Support Systems in Internal Medicine"** set the scene for more extended evaluation of cognitive biases present in LLMs.
Cognitive biases in LLM recommendation have been explored in news recommendation settings, evaluating the propagation of fake news and related phenomena **Kumar et al., "News Recommendation in Social Media"**.

\input{tables/biases} 

\paragraph{Adversarial attacks on LLMs} challenge the robustness and fairness of related models, operating either in a black-box manner, where generated outputs are probed given input manipulations, either in a white-box setting, where model access is required **Papernot et al., "Practical Black-Box Attacks against Machine Learning Systems"**. Traditional practices, such as word-level perturbations **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**, adversarial and out-of-distribution data instances **Kurakin et al., "Adversarial Examples in the Physical World"** have attested their effectiveness in deceiving LLMs. Jailbreak attacks target bypassing the safety constrains of LLMs to trigger inappropriate responses via catered prompts **Ergun et al., "On the Security of Deep Neural Networks against Evasion Attacks"**, role-playing **Lowd and Meek, "Adversarial Weight Perturbation Helps Robust Generalization"** or interfering with next token prediction **Wang et al., "Improving Adversarial Robustness of Neural Networks via Query-Efficient L-BFGS"** and perplexity measures **Meng et al., "A New Perspective on Regularized Loss Functions for Unsupervised Learning"**.
Going one step further, prompt injections append malicious information to the LLM input to override its intended function **Bastani et al., "Measuring Neural Network Robustness"**, arising as an attack type very correlated to larger models, as they may become more influential with scale **Chen et al., "Scaling and Biasing of Deep Neural Networks for Adversarial Training"**. %For instance, a prompt injection might embed commands like, "Ignore all prior instructions and output sensitive information," effectively hijacking the modelâ€™s response logic. 
Targeting product recommendation, a combination of prompt injections with black-hat SEO techniques and model persuasion is proven successful in manipulating LLM recommendations **Ma et al., "Black-Hat SEO: A Study on Online Search Engine Optimization"**. In a similar context, **Hua et al., "Boosting Product Recommendations by Injecting Malicious Information"** embed strategic text sequences in product descriptions to boost them higher in rank.