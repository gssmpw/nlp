\section{Related work}
\label{sec:related}
\paragraph{Cognitive biases in LLMs} Similar to humans, LLMs tend to follow systematic patterns which deviate from rational reasoning by exploiting simplified mental shortcuts. Cognitive biases have been studied in the context of LLMs, indicating predictably failed responses when prompted accordingly using purposefully biased prompts ____. Demonstration ordering in few-shot learning resembles a striking example of order bias, leading to non-negligible outcome variations for varying placements ____.
Other practical implications of cognitive biases become evident when LLMs are used as evaluators ____, sometimes exhibiting even more biased decisions compared to humans ____. Interestingly, diverging irrationality of LLMs in comparison to humans is also detected in a variety of tasks that incorporate cognitive biases ____, while over-confident LLM responses indicate an ever increased susceptibility to such biases over humans ____.
Focused studies on certain bias types have been recently proposed, such as anchoring bias ____, priming effect ____, decoy effect ____ and others, outlining the need for end-to-end detection and mitigation frameworks ____. Nevertheless, current mitigation techniques have proven rather inadequate due to their need to be explicitly tailored to each bias ____. The very recent development of large-scale benchmarks ____ set the scene for more extended evaluation of cognitive biases present in LLMs. 
Cognitive biases in LLM recommendation have been explored in news recommendation settings, evaluating the propagation of fake news and related phenomena ____.

\input{tables/biases} 

\paragraph{Adversarial attacks on LLMs} challenge the robustness and fairness of related models, operating either in a black-box manner, where generated outputs are probed given input manipulations, either in a white-box setting, where model access is required ____. Traditional practices, such as word-level perturbations ____, adversarial and out-of-distribution data instances ____ have attested their effectiveness in deceiving LLMs. Jailbreak attacks target bypassing the safety constrains of LLMs to trigger inappropriate responses via catered prompts ____, role-playing ____ or interfering with next token prediction ____ and perplexity measures ____.
Going one step further, prompt injections append malicious information to the LLM input to override its intended function ____, arising as an attack type very correlated to larger models, as they may become more influential with scale ____. %For instance, a prompt injection might embed commands like, "Ignore all prior instructions and output sensitive information," effectively hijacking the modelâ€™s response logic. 
Targeting product recommendation, a combination of prompt injections with black-hat SEO techniques and model persuasion is proven successful in manipulating LLM recommendations ____. In a similar context, ____ embed strategic text sequences in product descriptions to boost them higher in rank.