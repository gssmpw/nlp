% LLMs cognitive
@inproceedings{human-cognitive,
 author = {Jones, Erik and Steinhardt, Jacob},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {11785--11799},
 publisher = {Curran Associates, Inc.},
 title = {Capturing Failures of Large Language Models via Human Cognitive Biases},
 volume = {35},
 year = {2022}
}
 %url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/4d13b2d99519c5415661dad44ab7edcd-Paper-Conference.pdf},


@inproceedings{lu-etal-2022-fantastically,
    title = "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
    author = "Lu, Yao  and
      Bartolo, Max  and
      Moore, Alastair  and
      Riedel, Sebastian  and
      Stenetorp, Pontus",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.556",
    doi = "10.18653/v1/2022.acl-long.556",
    pages = "8086--8098",
    abstract = "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are {``}fantastic{''} and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13{\%} relative improvement for GPT-family models across eleven different established text classification tasks.",
}

@inbook{Shaki_2023,
   title={Cognitive Effects in Large Language Models},
   ISBN={9781643684376},
   ISSN={1879-8314},
   url={http://dx.doi.org/10.3233/FAIA230505},
   DOI={10.3233/faia230505},
   booktitle={ECAI 2023},
   publisher={IOS Press},
   author={Shaki, Jonathan and Kraus, Sarit and Wooldridge, Michael},
   year={2023},
   month=sep }

@misc{wu2023stylesubstanceevaluationbiases,
      title={Style Over Substance: Evaluation Biases for Large Language Models}, 
      author={Minghao Wu and Alham Fikri Aji},
      year={2023},
      eprint={2307.03025},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.03025}, 
}

@inproceedings{dong-etal-2024-survey,
    title = "A Survey on In-context Learning",
    author = "Dong, Qingxiu  and
      Li, Lei  and
      Dai, Damai  and
      Zheng, Ce  and
      Ma, Jingyuan  and
      Li, Rui  and
      Xia, Heming  and
      Xu, Jingjing  and
      Wu, Zhiyong  and
      Chang, Baobao  and
      Sun, Xu  and
      Li, Lei  and
      Sui, Zhifang",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.64",
    doi = "10.18653/v1/2024.emnlp-main.64",
    pages = "1107--1128",
    abstract = "With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.",
}

@article{ye2024justiceprejudicequantifyingbiases,
      title={Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge},
      author={Jiayi Ye and Yanbo Wang and Yue Huang and Dongping Chen and Qihui Zhang and Nuno Moniz and Tian Gao and Werner Geyer and Chao Huang and Pin-Yu Chen and Nitesh V Chawla and Xiangliang Zhang},
      journal={arXiv preprint arXiv:2410.02736},
      year={2024}
    }

@misc{sumita2024cognitivebiaseslargelanguage,
      title={Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments}, 
      author={Yasuaki Sumita and Koh Takeuchi and Hisashi Kashima},
      year={2024},
      eprint={2412.00323},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.00323}, 
}

@inproceedings{koo-etal-2024-benchmarking,
    title = "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
    author = "Koo, Ryan  and
      Lee, Minhwa  and
      Raheja, Vipul  and
      Park, Jong Inn  and
      Kim, Zae Myung  and
      Kang, Dongyeop",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.29",
    doi = "10.18653/v1/2024.findings-acl.29",
    pages = "517--545",
    abstract = "Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 16 LLMs encompassing four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLer), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (40{\%} of comparisons made by all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 44{\%}, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences.",
}

@misc{lou2024anchoringbiaslargelanguage,
      title={Anchoring Bias in Large Language Models: An Experimental Study}, 
      author={Jiaxu Lou and Yifan Sun},
      year={2024},
      eprint={2412.06593},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.06593}, 
}

@misc{chen2024aicognitivelybiasedexploratory,
      title={AI Can Be Cognitively Biased: An Exploratory Study on Threshold Priming in LLM-Based Batch Relevance Assessment}, 
      author={Nuo Chen and Jiqun Liu and Xiaoyu Dong and Qijiong Liu and Tetsuya Sakai and Xiao-Ming Wu},
      year={2024},
      eprint={2409.16022},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.16022}, 
}

@misc{liu2024decoydilemmaonlinemedical,
      title={The Decoy Dilemma in Online Medical Information Evaluation: A Comparative Study of Credibility Assessments by LLM and Human Judges}, 
      author={Jiqun Liu and Jiangen He},
      year={2024},
      eprint={2411.15396},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2411.15396}, 
}

@inproceedings{echterhoff-etal-2024-cognitive,
    title = "Cognitive Bias in Decision-Making with {LLM}s",
    author = "Echterhoff, Jessica Maria  and
      Liu, Yao  and
      Alessa, Abeer  and
      McAuley, Julian  and
      He, Zexue",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.739",
    doi = "10.18653/v1/2024.findings-emnlp.739",
    pages = "12640--12653",
    abstract = "Large language models (LLMs) offer significant potential as tools to support an expanding range of decision-making tasks. Given their training on human (created) data, LLMs have been shown to inherit societal biases against protected groups, as well as be subject to bias functionally resembling cognitive bias. Human-like bias can impede fair and explainable decisions made with LLM assistance. Our work introduces BiasBuster, a framework designed to uncover, evaluate, and mitigate cognitive bias in LLMs, particularly in high-stakes decision-making tasks. Inspired by prior research in psychology and cognitive science, we develop a dataset containing 13,465 prompts to evaluate LLM decisions on different cognitive biases (e.g., prompt-induced, sequential, inherent). We test various bias mitigation strategies, while proposing a novel method utilizing LLMs to debias their own human-like cognitive bias within prompts. Our analysis provides a comprehensive picture of the presence and effects of cognitive bias across commercial and open-source models. We demonstrate that our selfhelp debiasing effectively mitigates model answers that display patterns akin to human cognitive bias without having to manually craft examples for each bias.",
}

@misc{macmillanscott2024irrationalitycognitivebiaseslarge,
      title={(Ir)rationality and Cognitive Biases in Large Language Models}, 
      author={Olivia Macmillan-Scott and Mirco Musolesi},
      year={2024},
      eprint={2402.09193},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.09193}, 
}

@misc{niu2024largelanguagemodelscognitive,
      title={Large Language Models and Cognitive Science: A Comprehensive Review of Similarities, Differences, and Challenges}, 
      author={Qian Niu and Junyu Liu and Ziqian Bi and Pohsun Feng and Benji Peng and Keyu Chen and Ming Li and Lawrence KQ Yan and Yichao Zhang and Caitlyn Heqi Yin and Cheng Fei and Tianyang Wang and Yunze Wang and Silin Chen and Ming Liu},
      year={2024},
      eprint={2409.02387},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2409.02387}, 
}

@inproceedings{castello-etal-2024-examining,
    title = "Examining Cognitive Biases in {C}hat{GPT} 3.5 and {C}hat{GPT} 4 through Human Evaluation and Linguistic Comparison",
    author = "Castello, Marta  and
      Pantana, Giada  and
      Torre, Ilaria",
    editor = "Knowles, Rebecca  and
      Eriguchi, Akiko  and
      Goel, Shivali",
    booktitle = "Proceedings of the 16th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)",
    month = sep,
    year = "2024",
    address = "Chicago, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2024.amta-research.21",
    pages = "250--260",
    abstract = "This paper aims to investigate the presence of cognitive biases, more specifically of Availability heuristics, Representativeness heuristics and Framing, in OpenAI{'}s ChatGPT 3.5 and ChatGPT 4, as well as the linguistic dependency of their occurrences in the Large Language Models{'} (LLMs) outputs. The innovative aspect of this research is conveyed by rephrasing three tasks proposed in Kahneman and Tversky{'}s works and determining whether the LLMs{'} answers to the tasks are correct or incorrect and human-like or non-human-like. The latter classification is made possible by interviewing a total of 56 native speakers of Italian, English and Spanish, thus introducing a new linguistic comparison of results and forming a {``}human standard{'}. Our study indicates that GPTs 3.5 and 4 are very frequently subject to the cognitive biases under discussion and their answers are mostly non-human-like. There is minimal but significant discrepancy in the performance of GPT 3.5 and 4, slightly favouring ChatGPT 4 in avoiding biased responses, specifically for Availability heuristics. We also reveal that, while the results for ChatGPT 4 are not significantly language dependent, meaning that the performances in avoiding biases are not affected by the prompting language, their difference with ChatGPT 3.5 is statistically significant.",
}

@misc{hagendorff2024machinepsychology,
      title={Machine Psychology}, 
      author={Thilo Hagendorff and Ishita Dasgupta and Marcel Binz and Stephanie C. Y. Chan and Andrew Lampinen and Jane X. Wang and Zeynep Akata and Eric Schulz},
      year={2024},
      eprint={2303.13988},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.13988}, 
}

@misc{lyu2024cognitivebiaseslargelanguage,
      title={Cognitive Biases in Large Language Models for News Recommendation}, 
      author={Yougang Lyu and Xiaoyu Zhang and Zhaochun Ren and Maarten de Rijke},
      year={2024},
      eprint={2410.02897},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2410.02897}, 
}

@misc{opedal2024languagemodelsexhibitcognitive,
      title={Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?}, 
      author={Andreas Opedal and Alessandro Stolfo and Haruki Shirakami and Ying Jiao and Ryan Cotterell and Bernhard Schölkopf and Abulhair Saparov and Mrinmaya Sachan},
      year={2024},
      eprint={2401.18070},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.18070}, 
}

@misc{malberg2024comprehensiveevaluationcognitivebiases,
      title={A Comprehensive Evaluation of Cognitive Biases in LLMs}, 
      author={Simon Malberg and Roman Poletukhin and Carolin M. Schuster and Georg Groh},
      year={2024},
      eprint={2410.15413},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.15413}, 
}

%% SEO
@article{seo-survey,
author = {Kumar, R.Anil and Shaik, Zaiduddin and Furqan, Mohammed},
year = {2019},
month = {01},
pages = {5-8},
title = {A Survey on Search Engine Optimization Techniques},
volume = {9},
journal = {International Journal of P2P Network Trends and Technology},
doi = {10.14445/22492615/IJPTT-V9I1P402}
}

@incollection{black-white-hat-seo,
title = {Chapter 1 - Search Engine Optimization—Black and White Hat Approaches},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {78},
pages = {1-39},
year = {2010},
booktitle = {Advances in Computers: Improving the Web},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(10)78001-3},
url = {https://www.sciencedirect.com/science/article/pii/S0065245810780013},
author = {Ross A. Malaga},
abstract = {Today the first stop for many people looking for information or to make a purchase online is one of the major search engines. So appearing toward the top of the search results has become increasingly important. Search engine optimization (SEO) is a process that manipulates Web site characteristics and incoming links to improve a site's ranking in the search engines for particular search terms. This chapter provides a detailed discussion of the SEO process. SEO methods that stay within the guidelines laid out by the major search engines are generally termed “white hat,” while those that violate the guidelines are called “black hat.” Black hat sites may be penalized or banned by the search engines. However, many of the tools and techniques used by “black hat” optimizers may also be helpful in “white hat” SEO campaigns. Black hat SEO approaches are examined and compared with white hat methods.}
}

@article{black-hat-seo,
author = {Gaharwar, R.D. and Shah, D.},
year = {2018},
month = {11},
pages = {21-32},
title = {Blackhat Search Engine Optimization Techniques (SEO) and Counter Measures},
journal = {International Journal of Scientific Research in Science and Technology},
doi = {10.32628/IJSRST1840117}
}

%% LLM robustness
@inproceedings{indirect-prompt-inject,
author = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
title = {Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection},
year = {2023},
isbn = {9798400702600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605764.3623985},
doi = {10.1145/3605764.3623985},
abstract = {Large Language Models (LLMs) are increasingly being integrated into applications, with versatile functionalities that can be easily modulated via natural language prompts. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We show that LLM-Integrated Applications blur the line between data and instructions and reveal several new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (i.e., without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved at inference time. We derive a comprehensive taxonomy from a computer security perspective to broadly investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We then demonstrate the practical viability of our attacks against both real-world systems, such as Bing Chat and code-completion engines, and GPT-4 synthetic applications. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing reliance on LLMs, effective mitigations of these emerging threats are lacking. By raising awareness of these vulnerabilities, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users from potential attacks.},
booktitle = {Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security},
pages = {79–90},
numpages = {12},
keywords = {indirect prompt injection, large language models},
location = {Copenhagen, Denmark},
series = {AISec '23}
}
@misc{shayegani2023surveyvulnerabilitieslargelanguage,
      title={Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks}, 
      author={Erfan Shayegani and Md Abdullah Al Mamun and Yu Fu and Pedram Zaree and Yue Dong and Nael Abu-Ghazaleh},
      year={2023},
      eprint={2310.10844},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.10844}, 
}

@misc{wang2023robustnesschatgptadversarialoutofdistribution,
      title={On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective}, 
      author={Jindong Wang and Xixu Hu and Wenxin Hou and Hao Chen and Runkai Zheng and Yidong Wang and Linyi Yang and Haojun Huang and Wei Ye and Xiubo Geng and Binxin Jiao and Yue Zhang and Xing Xie},
      year={2023},
      eprint={2302.12095},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2302.12095}, 
}

@misc{li2023evaluatinginstructionfollowingrobustnesslarge,
      title={Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection}, 
      author={Zekun Li and Baolin Peng and Pengcheng He and Xifeng Yan},
      year={2023},
      eprint={2308.10819},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.10819}, 
}

@misc{wei2023jailbrokendoesllmsafety,
      title={Jailbroken: How Does LLM Safety Training Fail?}, 
      author={Alexander Wei and Nika Haghtalab and Jacob Steinhardt},
      year={2023},
      eprint={2307.02483},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.02483}, 
}

@misc{liu2024autodangeneratingstealthyjailbreak,
      title={AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models}, 
      author={Xiaogeng Liu and Nan Xu and Muhao Chen and Chaowei Xiao},
      year={2024},
      eprint={2310.04451},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.04451}, 
}

@misc{zhao2024weaktostrongjailbreakinglargelanguage,
      title={Weak-to-Strong Jailbreaking on Large Language Models}, 
      author={Xuandong Zhao and Xianjun Yang and Tianyu Pang and Chao Du and Lei Li and Yu-Xiang Wang and William Yang Wang},
      year={2024},
      eprint={2401.17256},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.17256}, 
}

@misc{boreiko2024realisticthreatmodellarge,
      title={A Realistic Threat Model for Large Language Model Jailbreaks}, 
      author={Valentyn Boreiko and Alexander Panfilov and Vaclav Voracek and Matthias Hein and Jonas Geiping},
      year={2024},
      eprint={2410.16222},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.16222}, 
}

@misc{jin2024guardroleplayinggeneratenaturallanguage,
      title={GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models}, 
      author={Haibo Jin and Ruoxi Chen and Andy Zhou and Yang Zhang and Haohan Wang},
      year={2024},
      eprint={2402.03299},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.03299}, 
}

@misc{zhu2024promptrobustevaluatingrobustnesslarge,
      title={PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts}, 
      author={Kaijie Zhu and Jindong Wang and Jiaheng Zhou and Zichen Wang and Hao Chen and Yidong Wang and Linyi Yang and Wei Ye and Yue Zhang and Neil Zhenqiang Gong and Xing Xie},
      year={2024},
      eprint={2306.04528},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.04528}, 
}

@misc{wang2023largelanguagemodelsreally,
      title={Are Large Language Models Really Robust to Word-Level Perturbations?}, 
      author={Haoyu Wang and Guozheng Ma and Cong Yu and Ning Gui and Linrui Zhang and Zhiqi Huang and Suwei Ma and Yongzhe Chang and Sen Zhang and Li Shen and Xueqian Wang and Peilin Zhao and Dacheng Tao},
      year={2023},
      eprint={2309.11166},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.11166}, 
}

%% Attacks
@misc{chaudhari2024phantomgeneraltriggerattacks,
      title={Phantom: General Trigger Attacks on Retrieval Augmented Language Generation}, 
      author={Harsh Chaudhari and Giorgio Severi and John Abascal and Matthew Jagielski and Christopher A. Choquette-Choo and Milad Nasr and Cristina Nita-Rotaru and Alina Oprea},
      year={2024},
      eprint={2405.20485},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2405.20485}, 
}

@misc{xue2024badragidentifyingvulnerabilitiesretrieval,
      title={BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models}, 
      author={Jiaqi Xue and Mengxin Zheng and Yebowen Hu and Fei Liu and Xun Chen and Qian Lou},
      year={2024},
      eprint={2406.00083},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2406.00083}, 
}

@misc{wei2024hiddenplainsightexploring,
      title={Hidden in Plain Sight: Exploring Chat History Tampering in Interactive Language Models}, 
      author={Cheng'an Wei and Yue Zhao and Yujia Gong and Kai Chen and Lu Xiang and Shenchen Zhu},
      year={2024},
      eprint={2405.20234},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2405.20234}, 
}

@misc{wan2024evidencelanguagemodelsconvincing,
      title={What Evidence Do Language Models Find Convincing?}, 
      author={Alexander Wan and Eric Wallace and Dan Klein},
      year={2024},
      eprint={2402.11782},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.11782}, 
}

@inproceedings{prompt-inject,
author = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
title = {Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection},
year = {2023},
isbn = {9798400702600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605764.3623985},
doi = {10.1145/3605764.3623985},
abstract = {Large Language Models (LLMs) are increasingly being integrated into applications, with versatile functionalities that can be easily modulated via natural language prompts. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We show that LLM-Integrated Applications blur the line between data and instructions and reveal several new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (i.e., without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved at inference time. We derive a comprehensive taxonomy from a computer security perspective to broadly investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We then demonstrate the practical viability of our attacks against both real-world systems, such as Bing Chat and code-completion engines, and GPT-4 synthetic applications. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing reliance on LLMs, effective mitigations of these emerging threats are lacking. By raising awareness of these vulnerabilities, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users from potential attacks.},
booktitle = {Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security},
pages = {79–90},
numpages = {12},
keywords = {indirect prompt injection, large language models},
location = {Copenhagen, Denmark},
series = {AISec '23}
}

%% LLMs recommenders
@misc{li2023gpt4recgenerativeframeworkpersonalized,
      title={GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation}, 
      author={Jinming Li and Wentao Zhang and Tian Wang and Guanglei Xiong and Alan Lu and Gerard Medioni},
      year={2023},
      eprint={2304.03879},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2304.03879}, 
}

@misc{gao2023chatrecinteractiveexplainablellmsaugmented,
      title={Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System}, 
      author={Yunfan Gao and Tao Sheng and Youlin Xiang and Yun Xiong and Haofen Wang and Jiawei Zhang},
      year={2023},
      eprint={2303.14524},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2303.14524}, 
}

@inproceedings{llm-recom,
author = {Xi, Yunjia and Liu, Weiwen and Lin, Jianghao and Cai, Xiaoling and Zhu, Hong and Zhu, Jieming and Chen, Bo and Tang, Ruiming and Zhang, Weinan and Yu, Yong},
title = {Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640457.3688104},
doi = {10.1145/3640457.3688104},
abstract = {Recommender system plays a vital role in various online services. However, its insulated nature of training and deploying separately within a specific closed domain limits its access to open-world knowledge. Recently, the emergence of large language models (LLMs) has shown promise in bridging this gap by encoding extensive world knowledge and demonstrating reasoning capabilities. Nevertheless, previous attempts to directly use LLMs as recommenders cannot meet the inference latency demand of industrial recommender systems. In this work, we propose an Open-World Knowledge Augmented Recommendation Framework with Large Language Models, dubbed KAR, to acquire two types of external knowledge from LLMs — the reasoning knowledge on user preferences and the factual knowledge on items. We introduce factorization prompting to elicit accurate reasoning on user preferences. The generated reasoning and factual knowledge are effectively transformed and condensed into augmented vectors by a hybrid-expert adaptor in order to be compatible with the recommendation task. The obtained vectors can then be directly used to enhance the performance of any recommendation model. We also ensure efficient inference by preprocessing and prestoring the knowledge from the LLM. Extensive experiments show that KAR significantly outperforms the state-of-the-art baselines and is compatible with a wide range of recommendation algorithms. We deploy KAR to Huawei’s news and music recommendation platforms and gain a 7\% and 1.7\% improvement in the online A/B test, respectively.},
booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
pages = {12–22},
numpages = {11},
keywords = {Knowledge Augmentation, Large Language Model, Recommender System},
location = {Bari, Italy},
series = {RecSys '24}
}

@misc{yang2023palrpersonalizationawarellms,
      title={PALR: Personalization Aware LLMs for Recommendation}, 
      author={Fan Yang and Zheng Chen and Ziyan Jiang and Eunah Cho and Xiaojiang Huang and Yanbin Lu},
      year={2023},
      eprint={2305.07622},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2305.07622}, 
}

@misc{lin2024recommendersystemsbenefitlarge,
      title={How Can Recommender Systems Benefit from Large Language Models: A Survey}, 
      author={Jianghao Lin and Xinyi Dai and Yunjia Xi and Weiwen Liu and Bo Chen and Hao Zhang and Yong Liu and Chuhan Wu and Xiangyang Li and Chenxu Zhu and Huifeng Guo and Yong Yu and Ruiming Tang and Weinan Zhang},
      year={2024},
      eprint={2306.05817},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2306.05817}, 
}

@misc{lyu2024llmrecpersonalizedrecommendationprompting,
      title={LLM-Rec: Personalized Recommendation via Prompting Large Language Models}, 
      author={Hanjia Lyu and Song Jiang and Hanqing Zeng and Yinglong Xia and Qifan Wang and Si Zhang and Ren Chen and Christopher Leung and Jiajie Tang and Jiebo Luo},
      year={2024},
      eprint={2307.15780},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.15780}, 
}

@misc{deldjoo2024reviewmodernrecommendersystems,
      title={A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)}, 
      author={Yashar Deldjoo and Zhankui He and Julian McAuley and Anton Korikov and Scott Sanner and Arnau Ramisa and René Vidal and Maheswaran Sathiamoorthy and Atoosa Kasirzadeh and Silvia Milano},
      year={2024},
      eprint={2404.00579},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2404.00579}, 
}

@misc{li2024surveygenerativesearchrecommendation,
      title={A Survey of Generative Search and Recommendation in the Era of Large Language Models}, 
      author={Yongqi Li and Xinyu Lin and Wenjie Wang and Fuli Feng and Liang Pang and Wenjie Li and Liqiang Nie and Xiangnan He and Tat-Seng Chua},
      year={2024},
      eprint={2404.16924},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2404.16924}, 
}

@misc{xiong2024searchengineservicesmeet,
      title={When Search Engine Services meet Large Language Models: Visions and Challenges}, 
      author={Haoyi Xiong and Jiang Bian and Yuchen Li and Xuhong Li and Mengnan Du and Shuaiqiang Wang and Dawei Yin and Sumi Helal},
      year={2024},
      eprint={2407.00128},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2407.00128}, 
}

@InProceedings{seo-llms,
author="Chodak, Grzegorz
and B{\l}a{\.{z}}yczek, Klaudia",
editor="Garg, Deepak
and Rodrigues, Joel J. P. C.
and Gupta, Suneet Kumar
and Cheng, Xiaochun
and Sarao, Pushpender
and Patel, Govind Singh",
title="Large Language Models for Search Engine Optimization in E-commerce",
booktitle="Advanced Computing",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="333--344",
abstract="The paper discusses how Large Language Models (LLMs) can be used in search engine optimization activities dedicated to e-commerce. In the first part the most important Search Engine Optimization (SEO) issues are discussed, such as technical SEO aspects, keyword selection, and content optimization. Then the study presents an in-depth look at OpenAI's advancements, including ChatGPT and DALL-E. The latter sections describe the capabilities of Large Language Models into the realm of SEO, particularly in e-commerce. Firstly, a set of prompts for LLMs that can be used to create content and HTML code for online shops is proposed. Then advantages, and drawbacks of incorporating LLMs in SEO for e-commerce are presented. The research concludes by synthesizing the potential of merging AI with SEO practices, offering insights for future applications.",
isbn="978-3-031-56700-1"
}


@misc{nestaas2024adversarialsearchengineoptimization,
      title={Adversarial Search Engine Optimization for Large Language Models}, 
      author={Fredrik Nestaas and Edoardo Debenedetti and Florian Tramèr},
      year={2024},
      eprint={2406.18382},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2406.18382}, 
}

@misc{kumar2024manipulatinglargelanguagemodels,
      title={Manipulating Large Language Models to Increase Product Visibility}, 
      author={Aounon Kumar and Himabindu Lakkaraju},
      year={2024},
      eprint={2404.07981},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2404.07981}, 
}

@misc{liu2024automaticuniversalpromptinjection,
      title={Automatic and Universal Prompt Injection Attacks against Large Language Models}, 
      author={Xiaogeng Liu and Zhiyuan Yu and Yizhe Zhang and Ning Zhang and Chaowei Xiao},
      year={2024},
      eprint={2403.04957},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2403.04957}, 
}

@misc{mckenzie2024inversescalingbiggerisnt,
      title={Inverse Scaling: When Bigger Isn't Better}, 
      author={Ian R. McKenzie and Alexander Lyzhov and Michael Pieler and Alicia Parrish and Aaron Mueller and Ameya Prabhu and Euan McLean and Aaron Kirtland and Alexis Ross and Alisa Liu and Andrew Gritsevskiy and Daniel Wurgaft and Derik Kauffman and Gabriel Recchia and Jiacheng Liu and Joe Cavanagh and Max Weiss and Sicong Huang and The Floating Droid and Tom Tseng and Tomasz Korbak and Xudong Shen and Yuhui Zhang and Zhengping Zhou and Najoung Kim and Samuel R. Bowman and Ethan Perez},
      year={2024},
      eprint={2306.09479},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.09479}, 
}

@misc{llama3,
    title={The Llama 3 Herd of Models},
    author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vítor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma},
    year={2024},
    eprint={2407.21783},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@article{hou2024bridging,
  title={Bridging Language and Items for Retrieval and Recommendation},
  author={Hou, Yupeng and Li, Jiacheng and He, Zhankui and Yan, An and Chen, Xiusi and McAuley, Julian},
  journal={arXiv preprint arXiv:2403.03952},
  year={2024}
}