\onecolumn
\section{Experiment Settings}
\subsection{Baseline Models}
\label{appendix:baseline}
\subsubsection{Model-based Diffusion}
\label{appendix:mbd}

We attempt to adopt the model-based diffusion method proposed in ~\cite{pan2024model} as a baseline for this work. Since practical application scenarios may not strictly satisfy the conditions specified in the original paper, we adapt the method with specific modifications. Concretely, when calculating the probability score for each sample, we compute it as follows:  
\begin{equation}
p_i =\mathrm{P}(y_i|x) := f(y_i;x) + \lambda_h \|h(y_i;x)\|_2 + \lambda_g \|\mathrm{ReLU}(g(y_i;x))\|_2
\end{equation}
Here, \( y_i \) represents the \( i \)-th sample within the complete collection of samples in one diffuse step. The subsequent algorithmic steps remain consistent with Algorithm 1 in ~\cite{pan2024model}. For all experiments, the number of samples is set to 100, and \(\lambda_h=\lambda_g=10\).  The specific process of the model-based diffusion with completion is outlined in Algorithm~\ref{algo:model-based-diffusion}, where $\mathrm{Completion}$ denotes the task-specific completion procedure.
\input{algo_mbd}
\subsubsection{Diffusion}
We investigate the direct application of diffusion models for solving optimization problems. Specifically, a solver is employed to compute an optimal solution \( y \) for each conditional variable \( x_0 \), thereby constructing a dataset \( \mathcal{D} \). The neural network is trained by minimizing the following loss function:  
\begin{equation}
\mathbb{E}_{x_0, y\sim \mathcal{D}, t, \epsilon}\left[\left\|\epsilon_\theta(y_t, t, x_0) - \epsilon\right\|_2^2 \right],
\end{equation}
where \( y_t = \sqrt{\bar{\alpha}_t}y + \sqrt{1- \bar{\alpha}_t}\epsilon \). Similar methodologies have been adopted in prior works, such as ~\cite{li2024diffusolve, graebner2024learning}.

\subsection{Experiment}
In this section, we provide detailed descriptions of some tasks referenced in the main text. These include concave quadratic optimization problems, relatively complex nonconvex optimization problems with practical significance, and others.  

\subsubsection{Concave Quadratic Programming}  
\label{appendix:exper-cqp}

The concave quadratic optimization problem (CQP) discussed in the text is defined as follows: 

\begin{align}  
\begin{aligned}
\min_{y\in \mathbb{R}^n}\quad & \frac{1}{2}y^TQy + p^Ty\\  
\mathrm{s.t.}   \quad & Ay = x\\  
                \quad & Gy \leq h
\end{aligned}
\end{align}  
 
Here, \( x \) is treated as a conditional parameter of the optimization problem, sampled uniformly from \([-1,1]\) across all instances. \( Q \), \( p \), \( A \), and \( h \) remain fixed. \( Q \) is a diagonal matrix whose diagonal elements are independently and identically sampled from \([-1,0]\). The vector \( p \) is generated using the same method as \( Q \). Elements of matrices \( A \) and \( G \) are sampled from a standard normal distribution. To ensure the feasibility of \( Gy \leq h \), \( h \) is constructed as:  
\begin{equation}  
h_i = \sum_{j}|(GA^+)_{ij}|  
\end{equation}
where \( A^+ \) denotes the pseudoinverse of \( A \). The optimal solutions are generated through \(\text{IPOPT}\)~\cite{wachter2006implementation}. \(10000\) examples have been generated for this task.

\subsubsection{Quadratic Programming with Sine Regularization}  
\label{appendix:exper-ncp}

The Quadratic Programming with Sine Regularization (QSPR) in the text is formulated as:  

\begin{align}  
\begin{aligned}
\min_{y\in \mathbb{R}^n}\quad & \frac{1}{2}y^TQy + \alpha\cdot p^T\sin(y)\\  
\mathrm{s.t.}   \quad & Ay = x\\  
                      & Gy \leq h  
\end{aligned}
\end{align}  

Here, \( x \) similarly serves as a conditional parameter, and the generation methods for \( x \), \( Q \), \( p \), \( A \), \( G \), and \( h \) align with those in the CQR.  In our experiment, \(\alpha\) was setted as \(10\). The optimal solutions are generated through \(\text{IPOPT}\). \(10000\) examples have been generated for this task.

\subsubsection{ACOPF}  
\label{appendix:exper-opf}

The AC Optimal Power Flow (ACOPF)~\cite{cain2012history, shi2017global,shi2018global} is a core problem in power systems, aiming to minimize generation costs by adjusting active/reactive power outputs of generators, voltage magnitudes, and phase angles while satisfying constraints such as power balance, line flow limits, and voltage limits. Although the generation costs are merely simple quadratic functions, the intricate constraints render the ACOPF a highly non-convex problem. This results in traditional solution algorithms for ACOPF encountering issues such as global optimality and excessive computation times etc. Recent studies have proposed relaxation approaches~\cite{8392367} and machine learning-based approaches~\cite{9303008,10012237,jiang2024advancements, zhao2024synergizingmachinelearningacopf} to address these issues.

More specifically, an ACOPF problem involves \( N \) nodes, including load buses \( \mathcal{L} \), a reference bus \( \mathcal{R} \), and generator buses \( \mathcal{G} \). Variables include active power \( p_g \), reactive power \( q_g \), active demand \( p_d \), reactive demand \( q_d \), voltage magnitude \( |v| \), and voltage phase angle \( \theta \). Load buses (representing non-generating nodes) satisfy \( (p_g)_{\mathcal{L}} = (q_g)_{\mathcal{L}} = 0 \). The reference bus provides a phase angle reference, with \( \theta_{\mathcal{R}} = \theta_{\text{ref}} \). Network parameters are described by the admittance matrix \( Y \). The ACOPF is formalized as follows, where \( v = |v|e^{i\theta} \), and \( A \), \( b \) are fixed parameters related to generation costs:  

\begin{align}  
\begin{aligned}
\min_{p_g, q_g, v, \theta} \quad & p_g^TAp_g + b^Tp_g\\  
\text{s.t.}  
\quad & \underline{p}_g \leq p_g \leq \overline{p}_g\\  
\quad & \underline{q}_g \leq q_g \leq \overline{q}_g\\  
\quad & \underline{|v|} \leq |v| \leq \overline{|v|}\\  
\quad & \theta_{\mathcal{R}} = \theta_{\text{ref}}\\  
\quad & (p_g)_{\mathcal{L}} = (q_g)_\mathcal{L} = 0\\  
\quad & (p_g - p_d) + i(q_g - q_d) = \text{diag}(v)Yv^*  
\end{aligned}
\end{align}  

where \(A,b\) represent as the cost coefficient, underline represent the lower bound and overline represent the upper bound. In this formulation, nodal demands \( p_d \) and \( q_d \) act as conditional parameters. Our experiments test the 57-bus system (Case57) and 118-bus system (Case118), with optimal solutions obtained via MATPOWER~\cite{5491276}.  

\subsubsection{Retargeting Problem}
\label{appendix:exper-retargetting}

The motion retargeting task can be formulated as an optimization problem, where the objective is to minimize the discrepancy between the motion of the SMPL human model and the H1 robot model. This task involves not only the alignment of joint positions but also the consideration of differences in kinematic structure, body proportions, joint alignment, and end-effector positioning.
Due to the significant differences between the kinematic structure of the SMPL model and the kinematic tree of the H1 humanoid robot, \cite{he2024learning} proposed a two-step method for preliminary motion retargeting. In the first step, given that the body shape parameters $\beta$ of the SMPL human model can represent a variety of body proportions, we optimize to find a body shape $\beta'$ that best matches the humanoid robot’s structure, thereby minimizing the joint position discrepancies between the models. This ensures that the joint positions of the SMPL model and H1 robot align as closely as possible, laying the foundation for subsequent retargeting.

Once the optimal $\beta'$ is determined, the second step involves mapping the joint positions and postures of the H1 robot to their corresponding positions in the SMPL model using forward kinematics. This process takes into account the kinematic constraints of the robot, ensuring the validity of joint positions.
Finally, to further refine the joint alignment, we minimize the differences in the positions of 11 key joints, adjusting the joint configuration between the SMPL model and the H1 robot. It is important to note that the retargeting process goes beyond adjusting joint positions—it also involves the alignment of end-effectors (such as ankles, elbows, and wrists). Special attention is given to the precise alignment of these key points to ensure that the human motion is smoothly transferred to the humanoid robot.
Given a sequence of motions expressed in SMPL parameters, which takes as input the joint positions $\boldsymbol{P}_{SMPL}$, root rotation $\boldsymbol{R}_{root}$, and transform offset $\boldsymbol{O}_{offset}$ from the SMPL model and computes the global joint positions $\boldsymbol{P}_{H1}$ of the H1 robot model using forward kinematics. The loss function is defined as the difference between the computed H1 joint positions and the corresponding SMPL joint positions. The optimization problem is defined as:

\begin{align}  
\begin{aligned}
\min_{\boldsymbol{P}_{H1}}\quad& \|\text{FK}(\boldsymbol{P}_{H1}, \boldsymbol{R}_{root}, \boldsymbol{O}_{offset}) - \boldsymbol{P}_{SMPL}\|_2^2 + \lambda \|\boldsymbol{P}_{H1}\|_2^2,\\ \text{s.t.}\quad &\boldsymbol{P}_{lower}\le\boldsymbol{P}_{H1}\le\boldsymbol{P}_{upper}
\end{aligned}
\end{align}  

The L2 norm penalty ensures smoother values and prevents $\boldsymbol{P}_{H1}$ from becoming excessively large during optimization. Large control inputs could be impractical and could even damage the robot hardware.\(1836\) examples have been generated for this task via IPOPT.


\section{Hyperparameter}

All of our experiments are implemented on a GPU of NVIDIA GeForce RTX 4090 with 24GB and a CPU of Intel Xeon w5-3435X. The implementation of NN and DC3 is based on \url{https://github.com/locuslab/DC3}. which is the official code library. Due to the absence of open-resource code, MBD is reproduced by us according to pseudo code reported in \cite{pan2024model}. Table \ref{tab:hyper} presents the hyper-parameters used in our experiments. 

\input{table/hyper}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%