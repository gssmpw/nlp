\section{SubTrack-Grad: Tracking the Gradient Subspace}
\label{sec:method}
Since gradients typically evolve within a small subspace, compressing this space can significantly reduce the memory footprint of the optimizer. As demonstrated in \citet{zhao2024galorememoryefficientllmtraining} whenever the gradient takes the general form 
\begin{equation}
\small
    G = \sum_i A_i + \sum_i B_iWC_i
    \label{eq:gradient-form}
\end{equation}
where \(i\) denotes the batch index, and \(B_i\) and \(C_i\) are positive semi-definite (PSD) matrices, this gradient can be projected onto a small subspace that remains nearly stable while ensuring that the optimization process continues to converge, as discussed in Section \ref{sec:theory} and Appendix \ref{appendix:C}.
However, the subspace of the gradient is not always stable, making it crucial to track its changes. GaLore \citep{zhao2024galorememoryefficientllmtraining} addresses this by periodically performing SVD on gradient matrices, while keeping the update frequency low to align with the assumption of a stable subspace, which is used in its convergence guarantee. This approach poses several challenges: {\bf 1)} not all gradients converge to a stable subspace within a few iterations, {\bf 2)} SVD is computationally expensive, and increasing the frequency of updates to capture changes significantly raises both run-time and environmental costs, and {\bf 3)} increasing the update frequency contradicts the assumption of a stable subspace, as SVD is sensitive to noise and does not account for the previously computed subspace or estimation error to regulate the extent of change applied.

We propose SubTrack-Grad, {\bf a memory- and time-efficient method that allows full and simultaneous parameter tuning.} SubTrack-Grad utilizes the estimation error and the previously computed subspace to adjust the core subspace. It effectively controls the amount change, allowing more frequent updates without compromising the stability assumption; while, as illustrated in Figure \ref{fig:time_consistency}, increasing the update frequency will not drastically increase the runtime. The subspace is initialized using SVD as follows:
\begin{equation}
\small
\begin{split}
    \label{eq:svd_p_q}
    G_0 = U S V^{\top} \approx \sum_{i=1}^{r} s_{i} u_{i} v_{i}^{\top},  \\
    P_0 = [u_1, u_2, ..., u_r], 
    \quad
    Q_0 = [v_1, v_2, ..., v_r].
\end{split}
\end{equation}
Here, \(G_0\) is an \(m \times n\) gradient matrix at step \(0\), \(U\), \(S\), and \(V\) are its SVD components, and \(r\) is the specified rank. In each optimization step, the gradients are projected onto the subspace of the left singular vectors if \(m \leq n\), or onto the right singular vectors otherwise, thereby optimizing memory usage \citep{zhao2024galorememoryefficientllmtraining}. After the optimizer processes the low-rank gradient, it gets projected back for a normal weight update. From now on, we assume \(m \leq n\) without loss of generality, implying that \(S_0 = P_0\), an \(m \times r\) orthonormal matrix whose columns span the underlying subspace.

At each step \(t\), the matrix \(S_t\) represents the subspace at that step, and projects the gradient matrix \(G_t\) onto that subspace by \(\widetilde{G}_t = S_t^\top G_t\); where \(\widetilde{G}_t\), the projection of the gradient onto a rank-\(r\) subspace, will be an \(r \times n\) matrix. The optimizer then performs within this low-rank space, which substantially reduces the number of state parameters and thus the memory footprint. The optimizer outputs \(\widetilde{G}_t^O\), which is then projected back to the original space by \(\widehat{G}_t = S_t \rho_t \widetilde{G}_t^O\), where \( \rho_t \) represents the entry-wise regularizer used in the optimizer, to be passed to the network.
\input{algorithms/SubTrackGrad}
As previously discussed, the gradient does not always evolve within a stable subspace; hence, \(S_t\), the orthonormal matrix spanning the core subspace, must be appropriately updated. SubTrack-Grad updates this subspace by moving along Grassmannian geodesics, and leverages the previously computed subspace and the current estimation error to minimize abrupt changes and noise effects, as illustrated in Figure \ref{fig:contour}.

We frame the problem of identifying the subspace as selecting the appropriate element from the Grassmannian, the set of all \(d\)-dimensional subspaces within an \(n\)-dimensional vector space \citep{Bendokat_2024}. The subspace is updated after \(k\) steps, the subspace update interval, and our objective is to minimize the Euclidean distance between the subspace and the current observed gradient in an update step. The corresponding cost function for this aim is defined as:
\begin{equation}\label{eq:loss-function}
\small
	F(S_t) = \min_{A} \|S_t A - G_t\|^2_F,
\end{equation}
where \(A\) is the solution to the least squares problem. The derivative of \eqref{eq:loss-function} with respect to \(S_t\) is given in \eqref{eq:partial-derivatives}, and the residual \(R = {G_t} - S_t A\) lies in the orthogonal complement of \(S_t\). To update the subspace in the appropriate direction, we calculate the tangent vector \( \nabla F \) on the Grassmannian, as shown in \eqref{eq:tangent-vector} based on \citet{edelman1998geometryalgorithmsorthogonalityconstraints}, where the second equality holds because \(R\) is orthogonal to \(S_t S_t^\top\).
\begin{equation}
\small
\frac{\partial F}{\partial S_t} = 2(S_t A - G_t) A^{\top} = -2R A^{\top}
\label{eq:partial-derivatives}
\end{equation}
\begin{equation}
\small
\nabla F = (I - S_tS_t^\top)\frac{\partial F}{\partial S_t} = \frac{\partial F}{\partial S_t} = -2R A^{\top} \approx \widehat{U}_F\widehat{\Sigma}_F\widehat{V}^\top_F
\label{eq:tangent-vector}
\end{equation}
The tangent vector \( \nabla F\) provides the direction for adjusting the subspace by accounting for the error lying in the orthogonal complement. However, to minimize changes to the subspace, SubTrack-Grad first computes a rank-\(1\) approximation of \( \nabla F \), determined by its largest singular value and the corresponding singular vector obtained from its SVD, and represented as \( \widehat{U}_F \widehat{\Sigma}_F \widehat{V}^\top_F \) in the final equality of \eqref{eq:tangent-vector}. This approximation is then used to update the subspace.

As shown by \citet{edelman1998geometryalgorithmsorthogonalityconstraints, Bendokat_2024}, from which we have included the necessary definitions and theorem in section \ref{sec:theory}, we can move along a Grassmannian geodesic guided by the computed tangent vector, taking a step of size \(\eta\), as presented in \eqref{eq:update-role}.
\begin{equation}\label{eq:update-role}
\small
S_{t+1}(\eta) = (S_t\widehat{V}_F \quad \widehat{U}_F) \begin{pmatrix} \cos{\widehat{\Sigma}_F \eta} \\ \sin{\widehat{\Sigma}_F \eta} \end{pmatrix} \widehat{V}^\top_F + S_t(I - \widehat{V}_F\widehat{V}^\top_F)
\end{equation}
This update rule preserves the orthonormality of \(S_{t+1}\), ensuring it remains on the Grassmannian. The last term in \eqref{eq:tangent-vector}, is required when using thin-SVD; it projects the previous subspace onto the orthogonal complement of \(\widehat{V}_F\), ensuring that the portion of \(S_t\) which has not been updated in this step is still included. By leveraging the geometry of the Grassmannian, SubTrack-Grad effectively tracks the underlying subspace of the gradient space, and the pseudo-code for this method is provided in Algorithm \ref{alg:SubTrack}.
