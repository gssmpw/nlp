\section{Experiments and Results}
\input{tabels/glue_perf}
\input{tabels/superglue_perf}
\input{images/mem_time}
To ensure a fair comparison between SubTrack-Grad and baselines, we conducted fine-tuning and pre-training experiments across various models and datasets; to measure performance, wall-time, and memory consumption, while all shared hyperparameters remain consistent.
\vspace{2pt}

\noindent\textbf{Fine-Tuning Experiments.} RoBERTa-Base and RoBERTa-Large were fine-tuned on GLUE and SuperGLUE tasks respectively; with the results presented in Table \ref{tab:glue} and Table \ref{tab:superglue}.   
Figures \ref{fig2:b} and \ref{fig2:c} compare wall-time during fine-tuning, with detailed reports provided in Appendix \ref{appendix:mem-time}, as demonstrated, SubTrack-Grad enjoys the same memory reduction as GaLore. For comparing wall-times, models were fine-tuned up to a number of iterations to exactly include five subspace updates, while the evaluation steps were excluded to provide an accurate comparison. 
SubTrack-Grad demonstrates significant efficiency improvements compared to GaLore by {\bf reducing wall-time by up to 20.57\% on GLUE tasks (15\% average reduction)} and {\bf up to 65\% on SuperGLUE tasks (22\% average reduction).} Despite limiting updates to rank-\(1\) modifications of the previous subspace, SubTrack-Grad achieves superior or comparable performance compared to GaLore. In addition, BAdam is a computationally efficient method due to optimizing one block at a time; but its inability to update all the parameters simultaneously has caused performance loss, and the loss gets greater as the model size increases. Details of fine-tuning hyperparameters are provided in Appendix \ref{appendix:A}.
\vspace{2pt}

\noindent\textbf{Pre-Training Experiments.} 
Different Llama-based architectures were pre-trained on C4 datasets for 10K steps, and the results are reported in Table \ref{tab:lama}.
For wall-time comparison, each architecture was trained for 2K iterations, with the subspace update interval set to 200, ensuring exactly 10 subspace updates. 
Figure \ref{fig2:a} illustrates wall-times, with detailed reports provided in Appendix \ref{appendix:mem-time} that indicates same memory consumption between GaLore and SubTrack-Grad. 
Notably, GaLore incurred a substantial 157\% increase in wall-time compared to full-rank training for the model of size 3B, whereas SubTrack-Grad exhibited only a 31\% increase—{\bf representing a 49\% reduction in wall-time compared to GaLore.} On average, SubTrack-Grad required {\bf 27\% less training time} across these models. Additionally, BAdam demonstrate significant runtime reduction, but its poor performance in pre-training experiments due to its partial optimization, makes it an unsuitable choice for training from scratch. Additional architectural details for the Llama-based models are provided in Appendix \ref{appendix:B}.
\input{tabels/lama_perf}
\vspace{2pt}

\noindent\textbf{Time and Space Complexity.}
Table \ref{tab:time-mem-analysis} provides memory requirements of the optimizer states and the time complexity of the subspace update step considering an \(m \times n\) gradient matrix with \(m \le n\). As discussed earlier, GaLore performs SVD on the gradient matrix to estimate the underlying subspace, whereas SubTrack-Grad uses subspace tracking. Comparing the time complexities of these methods, highlights why SubTrack-Grad is significantly more efficient than GaLore. A breakdown of the subspace update time complexity for SubTrack is shown in Appendix \ref{appendix:time-complexity}. 
As shown in Figure \ref{fig2:a}, for pre-training a Llama model of size 3B, GaLore incurs over 157\% wall-time overhead, compared to 31\% for SubTrack-Grad. This demonstrates a 49\% wall-time reduction compared to GaLore. Additionally, the memory required for storing optimizer states in SubTrack-Grad, is equivalent to GaLore’s.
\input{tabels/time_mem_analysis}
\vspace{2pt}

\noindent\textbf{Reducing Optimization Jumps.} We argue that GaLore is susceptible to noise, as it relies on the SVD of a single gradient matrix at each subspace update step. Figure \ref{fig:contour} compares SubTrack-Grad and GaLore on the Ackley function to illustrate potential jumps and deviations. These jumps prevent GaLore from reaching the global minimum within 100 steps when the scale factor is set to 1. While the scale factor to 3 enables GaLore to reach that minimum, it also results in larger jumps. Figure \ref{fig:pert-wt} in Appendix \ref{appendix:perf-wt} illustrates that the overall convergence during both pre-training and fine-tuning is similar to other methods when using SubTrack-Grad.
\input{images/contour}
\vspace{2pt}

\noindent\textbf{Runtime Consistency.} 
Figure \ref{fig:time_consistency} compares the performance and wall-times of GaLore and SubTrack-Grad across subspace update intervals ranging from 50 to 500 while fine-tuning RoBERTa-Base on the COLA task and pre-training a Llama architecture with 60M parameters on the C4 dataset. The subspace update interval denotes the number of iterations between two updates; thus, increasing this interval reduces the update frequency. GaLore's runtime significantly increases with more frequent subspace updates, whereas SubTrack-Grad maintains minimal overhead. In Table \ref{tab:time-perf-consistency} different performance achieved on COLA dataset for different subspace update intervals are compared, illustrating that increasing the update frequency significantly raises GaLore's runtime, while the performance of the two methods remains very similar.
\input{images/time_consistency}


