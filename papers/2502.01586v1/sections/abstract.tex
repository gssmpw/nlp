\begin{abstract}
Training Large Language Models (LLMs) demand significant time and computational resources due to their large model sizes and optimizer states. To overcome these challenges, recent methods, such as BAdam, employ partial weight updates to enhance time and memory efficiency, though sometimes at the cost of performance. Others, like GaLore, focus on maintaining performance while optimizing memory usage through full parameter training, but may incur higher time complexity. By leveraging the low-rank structure of the gradient and the Grassmannian geometry, we propose SubTrack-Grad, a subspace tracking-based optimization method that efficiently tracks the evolving gradient subspace by incorporating estimation errors and previously identified subspaces. SubTrack-Grad delivers better or on-par results compared to GaLore, while significantly outperforming BAdam, which, despite being time-efficient, compromises performance. SubTrack-Grad {\bf reduces wall-time by up to 20.57\% on GLUE tasks (15\% average reduction)} and {\bf up to 65\% on SuperGLUE tasks (22\% average reduction) compared to GaLore.} Notably, for a 3B parameter model, GaLore incurred a substantial 157\% increase in wall-time compared to full-rank training, whereas SubTrack-Grad exhibited a 31\% increaseâ€”{\bf representing a 49\% reduction in wall-time}, while enjoying the same memory reductions as GaLore.   
\end{abstract}
