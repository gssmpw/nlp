\input{theories/convergence_proof}
\input{theories/update_rule_proof}

\section{Time Complexity Analysis}
\label{appendix:time-complexity}

Table \ref{tab:time-subtrack-update-step} presents the time complexity breakdown for the subspace update step in the SubTrack-Grad algorithm assuming a $m \times n$ gradient matrix and rank $r$ projection matrix, where $r \ll m \leq n$. As outlined in Algorithm \ref{alg:SubTrack}, the subspace update step begins by solving the least squares problem (\ref{eq:loss-function}) to estimate the optimal update for $S_t$, the $m \times r$ orthonormal matrix. This operation has a time complexity of $O(mr^2)$. Computing the residual and the partial derivative with respect to $S_t$ requires $O(mrn)$ and $O(mnr)$ time respectively. This is because the solution to the least squares problem, $A$, has shape $ r \times n$ which is multiplied by $S_t$ in the residual $R=G_t - S_tA$, resulting in time complexity $O(mrn)$. The following operation for the partial derivative is $-2RA^T$, where the matrix multiplication has $O(mnr)$ complexity. The tangent vector computation (\ref{eq:tangent-vector}) which involves an identity transformation and matrix multiplication has time complexity of $O(m^2r)$.  The rank-1 approximation step uses largest singular value from the SVD of the $ m \times r$ tangent vector, and has time complexity of $O(mr^2)$. Finally, the update rule as shown in  (\ref{eq:update_rule}) which has a time complexity of $O(mr^2)$. The overall complexity of the algorithm is dominated by the matrix multiplication calculations of time complexity $O(mnr)$. However, unlike GaLore, since we avoid computing SVD operation on the $m \times n$ gradient matrix, which has complexity of $O(nm^2)$, the overall update step in SubTrack-Grad is still more efficient with respect to time complexity. 

\input{tabels/time_subtrack_update_step}

\section{Memory and Time Comparison}
\label{appendix:mem-time}
Table \ref{tab:lama-time} presents the wall-times measured to compare the computational efficiency of SubTrack-Grad and GaLore. Additionally, Table \ref{tab:lama-mem} shows the peak memory consumption for each architecture during training, using the hyperparameters detailed in \ref{tab:pt_hyperparameters}, on an NVIDIA A100 GPU with 40GB of memory. These values were used to create the bar plots shown in \ref{fig2:a}.
For wall-time comparisons, each architecture was trained for 2,000 steps, ensuring exactly 10 subspace updates while excluding evaluation steps. The number of subspace updates was doubled compared to the fine-tuning experiments because larger models and more complex tasks naturally require more frequent updates. This adjustment allowed for more accurate measurements considering this demand. 
\input{tabels/lama_time}
\input{tabels/lama_mem}

Table \ref{tab:glue-time} and Table \ref{tab:glue-mem} present the wall-time and peak memory consumption, respectively, for fine-tuning RoBERTa-Base on GLUE tasks. These values were used to generate the bar plots shown in \ref{fig2:c}. For wall-time comparisons, the same settings were used, but evaluation steps were excluded, and fine-tuning was limited to number of iterations to ensure exactly 5 subspace update steps during the process.
\input{tabels/glue_time}
\input{tabels/glue_mem}

Table \ref{tab:superglue-time} and Table \ref{tab:superglue-mem} present the wall-time and peak memory consumption for each method when fine-tuning RoBERTa-Large on SuperGLUE tasks. These values were used to generate the bar plots shown in \ref{fig2:b}. For wall-time comparisons, as with GLUE tasks, evaluation steps were omitted to ensure accurate measurements of fine-tuning time, and the number of iterations was adjusted to include exactly 5 subspace updates during fine-tuning.
\input{tabels/superglue_time}
\input{tabels/superglue_mem}

\section{Fine-Tuning RoBERTa}
\label{appendix:A}
RoBERTa-Base was fine-tuned on GLUE tasks using the hyperparameters detailed in Table \ref{tab:ft_hyperparameters}, matching those reported in the GaLore \citep{zhao2024galorememoryefficientllmtraining} for rank-\(4\) and rank-\(8\) subspaces, with a subspace update interval set at 500 iterations.
\input{tabels/glue_hyperparameters}

We also fine-tuned RoBERTa-Large on SuperGLUE tasks using the hyperparameters from \citet{luo2024badammemoryefficientparameter}, as detailed in Table \ref{tab:sg_hyperparameters}, with the exception that we fine-tuned each task for 30 epochs.
\input{tabels/superglue_hyperparameters}

\section{Pre-Training LLama-Based Architectures}
\label{appendix:B}
To manage resources, we pre-trained all five Llama-based architectures for 10,000 iterations using hyperparameters reported in Table \ref{tab:pt_hyperparameters}. While larger models typically require more iterations, this setup was sufficient for comparing methods rather than creating perfectly optimized models. 
\input{tabels/lama_hyperparameters}

\section{Time and Performance Consistency}
\label{appendix:time-consistency}
Table \ref{tab:time-perf-consistency} demonstrates that increasing the update frequency significantly increases GaLore's runtime, while the performance of both methods remains comparable. This underscores SubTrack-Grad's efficiency in reducing runtime without sacrificing performance. Notably, this advantage becomes even more pronounced for tasks that demand more frequent updates.
\input{tabels/time_perf_consistency}

\section{Convergence vs. Wall-Time}
\label{appendix:perf-wt}
Figure \ref{fig:pert-wt} depicts the changes in the training loss function relative to wall-time. The results demonstrate that SubTrack-Grad's wall-time reduction has minimal impact on the learning process, showcasing a convergence pattern comparable to other methods without introducing significant computational overhead.
\input{images/trainloss}

