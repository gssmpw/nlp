\section{Discussion and Conclusion}
We proposed a time- and memory-efficient method that projects gradients into a lower-dimensional subspace, and preserves the previously computed subspace to incorporate gradient components from the orthogonal complement to perform rank-\(1\) updates. This approach reduces the frequency of abrupt transitions between iterations and leverages the available information effectively.

In some cases, the extent of changes in the subspace may require updates of ranks greater than \(1\); the pre-training of Llama-based architectures is a clear example of this need. During our experiments, we observed that applying updates as per \eqref{eq:update-role}, using the full-rank SVD of the tangent vector from \eqref{eq:tangent-vector}, can hinder convergence if the singular values of the tangent vector become very small. Furthermore, simultaneously updating vector spaces associated with different singular values caused convergence issues in some cases. Therefore, we restricted updates to rank-\(1\) in this paper, as this approach still enabled SubTrack-Grad to achieve performance comparable to or better than GaLore. In future work, we plan to explore increasing the rank of updates without compromising convergence. Additionally, dynamically selecting the step size could eliminate the need for manual tuning as a hyperparameter which is also left for future works. 

Surprisingly, SubTrack-Grad outperforms full-rank fine-tuning on certain tasks, suggesting that low-rank optimization may also serve as a regularizer. Exploring this property further is also an exciting direction for future work.