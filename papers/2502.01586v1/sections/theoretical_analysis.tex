\section{Theoretical Analysis}
\label{sec:theory}
In this section, we analyze the convergence of SubTrack-Grad using theoretical analysis.  To begin, the weights update rule of SubTrack-Grad is as follows:
\begin{equation}
\small
    W_t = W_0 + \sum_{t'=0}^{t'=t-1} \widehat{G}_{t'}
    \label{eq:update_rule}
\end{equation}
As previously mentioned, we use left projection if \(m \leq n\), where \(m\) and \(n\) are the dimensions of the gradient matrix, and vice versa. Thus, \(\widehat{G}_{t'}\) can be computed as shown in \eqref{eq:project_back}.
\begin{equation}
\small
    \widehat{G}_{t'} = 
    \begin{cases}
        S_{t'} \rho_{t'} (S_{t'}^\top G_{t'}), & \text{if \(m \leq n\)} \\
        \rho_{t'} (G_{t'} S_{t'})S_{t'}^\top, & \text{otherwise}
    \end{cases}
    \label{eq:project_back}
\end{equation}
Here, \(S_{t'}\) is the projection matrix that projects the gradient onto the subspace, and \(\rho_{t'}\) is representing the entry-wise regularizer used in the optimizer. If we use the full projection, then \(\widehat{G}_{t'}\) will be computed as shown in \eqref{eq:full_project_back}; where \(S_{t'}^l\) and \(S_{t'}^r\) are the rank-\(r\) left and right projection matrices.
\begin{equation}
\small
    \widehat{G}_{t'} = S_{t'}^l \rho_{t'} ({S_{t'}^l}^\top G_{t'} S_{t'}^r) {S_{t'}^r}^\top
    \label{eq:full_project_back}
\end{equation}

\input{theories/convergence}

The proof of Theorem \ref{th:convergence} is provided in Appendix \ref{appendix:C}, based on \citet{zhao2024galorememoryefficientllmtraining}. Note that while both GaLore and SubTrack-Grad assume that the subspace remains unchanged for the proof of convergence, GaLore must limit the number of updates to ensure convergence, as each update can potentially change the entire subspace. In contrast, SubTrack-Grad leverages rank-\(1\) updates to the subspace, preventing drastic changes with each update. While a deeper analysis of slowly changing subspaces and their impact on convergence remains an open problem, in practice, this allows SubTrack-Grad to perform more frequent updates.

Here we investigate the Grassmannian update rule presented in \eqref{eq:update-role}, which is
a direct application of Grassmann geometry \citep{edelman1998geometryalgorithmsorthogonalityconstraints, Bendokat_2024}.

\input{theories/update_rule}

The proof of Theorem \ref{theorem:gr_exp} can be found in Appendix \ref{appendix:grassmann-proof}. Leveraging this theorem and our notation in section \ref{sec:method}, one can easily verify that the subspace update rule is as follows:
\begin{equation*}
\small
    S_{t+1}(\eta) = (S_t\widehat{V}_F \quad \widehat{U}_F) \begin{pmatrix} \cos{\widehat{\Sigma}_F \eta} \\ \sin{\widehat{\Sigma}_F \eta} \end{pmatrix} \widehat{V}^\top_F + S_t(I - \widehat{V}_F\widehat{V}^\top_F)
\end{equation*}
This update rule generally converges to a stable subspace if the step size \(\eta\) decreases over time \citep{balzano2011onlineidentificationtrackingsubspaces}. However, a decreasing step size can impair the ability to accurately track and adapt to subspace changes. Consequently, SubTrack-Grad uses a constant step size during training and fine-tuning to effectively adjust subspaces. This approach does not hinder convergence, as proved in Theorem \ref{th:convergence}, which guarantees convergence as long as changes are controlled to maintain the stable subspace assumption. 
