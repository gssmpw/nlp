\section{Related Works}
Several works aim to improve the efficiency of training and fine-tuning LLMs, addressing a growing demand as their popularity rapidly increases. LoRA \citep{hu2021lora}, a widely recognized method for reducing the number of trainable parameters, that projects model weights into a lower-dimensional space, resulting in two trainable low-rank matrices. This approach significantly reduces memory requirements for fine-tuning LLMs.
\citet{dettmers2024qlora} build on LoRA by employing quantization techniques and paged optimizers to further reduce memory usage. Additionally, \citet{yaras2024compressible} introduce Deep LoRA, which uses deep matrix factorization for low-rank optimization, addressing overfitting issues and reducing the need for precise tuning of the rank parameter. Several other works have also extended LoRA to enhance the efficiency of training and fine-tuning LLMs \citep{lialin2023relorahighranktraininglowrank, renduchintala-etal-2024-tied, xia2024chainloraefficientfinetuning, pan2024lisalayerwiseimportancesampling}.
\citet{miles2024veloramemoryefficienttraining} propose compressing intermediate activation vectors and reconstructing them during backpropagation to enhance memory efficiency. Additionally, \citet{hao2024floralowrankadapterssecretly} demonstrate that full-parameter fine-tuning is feasible by random projections on the gradient matrix, showing that LoRA essentially performs a down-projection of the gradient. BAdam \citep{luo2024badammemoryefficientparameter} leverages the block coordinate descent (BCD) framework to reduce memory consumption while maintaining capabilities comparable to Adam.

Several approaches aim to reduce memory consumption in optimizers, as optimizers like Adam \citep{kingma2017adammethodstochasticoptimization} account for a significant portion of memory usage due to their storage of element-wise states to improve the optimization process \citep{li2023memoryefficientoptimizers4bit, anil2019memoryefficientadaptiveoptimization, lv-etal-2024-full, dettmers20228bitoptimizersblockwisequantization, rajabi2024memoryefficient, rajabi2024accelerating}. MicroAdam \citep{modoranu2024microadamaccurateadaptiveoptimization} tackles this issue by compressing the gradient space for optimization and utilizing the resulting compression error through feedback loops to improve the optimization process. Adam-mini \citep{zhang2024adamminiusefewerlearning} partitions model parameters into blocks, assigning a single learning rate to each block. This design achieves a significant reduction in memory usage while maintaining performance.
\citet{gurari2018gradientdescenthappenstiny} suggests that a substantial portion of gradients lies within a small, largely consistent subspace, a finding also reported by other studies, including \citet{schneider2024identifyingpolicygradientsubspaces, yaras2023invariant}. GaLore \citep{zhao2024galorememoryefficientllmtraining} leverages this property to reduce the memory requirements of optimizers by projecting gradients into a lower-dimensional subspace and then projecting them back for full parameter tuning. This approach has been integrated with other methods to further reduce memory usage during training and fine-tuning of LLMs \citep{li2024owloreoutlierweighedlayerwisesampled, rajabi2024enhancing}. However, not all layers' gradients evolve within a stable low-rank subspace. \citet{jaiswal2024galorewelorelowrankweights} identifies layers with constantly changing gradients where low-rank projection may be inefficient for tuning. By analyzing the distribution of singular values across different layers, they select those that evolve within a small subspace for fine-tuning while freezing the remaining layers. Gradient Structured Sparsification (Grass) \citep{muhamed2024grasscomputeefficientlowmemory} further reduces memory usage by applying sparse projection matrices to the gradient, transforming the gradient matrix into a sparse space. This approach leverages sparse representations to significantly decrease the memory footprint. In \citet{ramesh2024blockllmmemoryefficientadaptationllms} the authors propose an approach that dynamically selects and updates a small subset of parameters, for a faster and more memory-efficient training without altering the model's structure.

A common approach in working with high-dimensional data is to project the data into a lower-dimensional space, and many studies focus on tracking these subspaces as they evolve. \citet{balzano2011onlineidentificationtrackingsubspaces} introduce an incremental method for updating subspaces on the Grassmannian when the data is partially observed. \citet{zhang2016globalconvergencegrassmanniangradient} and \citet{kasai2017fastonlinelowranktensor} propose methods to handle noise effect in tracking these subspaces. Furthermore, \citet{blocker2023dynamicsubspaceestimationgrassmannian} present a method for evolving geodesic-based data in the Grassmannian for updating the subspace effectively. Also, \citet{rajabi2024optimizing} have applied Grassmannian-based subspace tracking on gradient matrices to increase the efficiency of training. 