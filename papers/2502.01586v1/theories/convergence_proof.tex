\section{Convergence of SubTrack-Grad}
\label{appendix:C}

\convergence*

{\bf proof.} To demonstrate that SubTrack-Grad converges to the global minimum during training, we begin by deriving the recursive form of the gradients.

Let \(\otimes\) denote the Kronecker product. Then, \( vec(AXB) = (B^\top \otimes A) vec(X)\). 

By applying \(vec\) to the gradient form given in the theorem, we obtain:
\begin{equation}
\label{eq:11}
    g_t = vec(G_t) = vec(\sum_i A_i + \sum_i B_iWC_i) = a_t - D_t w_t 
\end{equation}
where \(g_t := vec(G_t)\), \(w_t := vec(W_t)\), \(a_t := \frac1N\sum_i vec(A_{i,t})\), and \(D_t = \frac1N\sum_i C_{i,t} \otimes B_{i,t}\).

As defined in the theorem, let \(P_t = {S_t^l}^\top G_t S_t^r\). Its vectorized form can be expressed using the Kronecker product as follows:

\begin{equation}
\begin{gathered}
    p_t = vec(P_t) = vec({S_t^l}^\top G_t S_t^r) = ({S_t^r}^\top \otimes {S_t^l}^\top)vec(G_t) \\ \quad
    = {(S_t^r \otimes S_t^l)}^\top vec(G_t) = {(S_t^r \otimes S_t^l)}^\top g_t 
    \label{eq:p_t}
\end{gathered}
\end{equation}
Now recalling \(\widehat{G}_t\) from \eqref{eq:full_project_back}, it can be written as:
\begin{equation*}
    \widehat{G}_{t} = S_{t}^l {S_{t}^l}^\top G_{t} S_{t}^r {S_{t}^r}^\top
\end{equation*}
Thus, its vectorized form will be:
\begin{equation}
\begin{gathered}
    vec(\widehat{G}_t) = \widehat{g}_t = vec(S_{t}^l {S_{t}^l}^\top G_{t} S_{t}^r {S_{t}^r}^\top) = vec(S_{t}^l P_t {S_{t}^r}^\top) \\ \quad
    = (S_{t}^r \otimes S_{t}^l)vec(P_t) = (S_{t}^r \otimes S_{t}^l)p_t
    \label{eq:g_hat_t}
\end{gathered}
\end{equation}
This is where the constant subspace assumption becomes necessary. To derive the recursive form of \(g_t\), we assume that the projection matrices remain fixed throughout training, i.e., \(S_t^r = S^r\) and \(S_t^l = S^l\). Consequently, we can restate equations \eqref{eq:p_t} and \eqref{eq:g_hat_t} as follows:
\begin{eqnarray}
p_t = {(S^r \otimes S^l)}^\top g_t \label{eq:14} \\
\widehat{g}_t = (S^r \otimes S^l) p_t \label{eq:15}
\end{eqnarray}
Then we can write the recursive form of \(g_t\):
\begin{equation}
\begin{gathered}
\label{eq:16}
    g_t = a_t - D_t w_t = (a_t - a_{t-1}) + (D_{t-1} - D_t) w_t + a_{t-1} - D_{t-1}w_t \\ \quad 
    = e_t + a_{t-1} - D_{t-1}(w_{t-1} + \mu \widehat{g}_{t-1}) = e_t + g_{t-1} - \mu D_{t-1} \widehat{g}_{t-1}  
\end{gathered}
\end{equation}
where \(e_t := (a_t - a_{t-1}) + (D_{t-1} - D_t) w_t\). 

Note that in deriving \eqref{eq:16}, we utilized the general form of the weight update rule, \( w_{t+1} = w_t - \mu g_t \), which can be rewritten as \( w_t = w_{t+1} + \mu g_t \). By applying this rule along with \eqref{eq:11}, we arrive at the second equality in \eqref{eq:16} as follows:
\begin{equation*}
\begin{gathered}
    g_t = a_t - D_t w_t = a_t - D_t w_t - g_{t-1} + g_{t-1}  \\ \quad
    = a_t - D_t w_t - a_{t-1} + D_{t-1}w_{t-1} + a_{t-1} - D_{t-1}w_{t-1} \\ \quad
    = a_t - D_t w_t - a_{t-1} + D_{t-1}(w_t + \mu g_{t-1}) + a_{t-1} - D_{t-1}(w_t + \mu g_{t-1}) \\ \quad
    = a_t - D_t w_t - a_{t-1} + D_{t-1}w_t + \mu D_{t-1} g_{t-1} + a_{t-1} - D_{t-1}w_t - \mu D_{t-1} g_{t-1} \\ \quad
    = a_t - a_{t-1} + (D_{t-1} - D_t)w_t + a_{t-1} - D_{t-1}
\end{gathered}
\end{equation*}

To obtain \(p_t\) from this recursive formulation, we can left-multiply by \({(S^r \otimes S^l)}^\top\), as shown in \eqref{eq:15}:
\begin{eqnarray}
\begin{split}
    p_t = {(S^r \otimes S^l)}^\top e_t + {(S^r \otimes S^l)}^\top g_{t-1} - 
    \mu {(S^r \otimes S^l)}^\top D_{t-1} \widehat{g}_{t-1}
\end{split}
\end{eqnarray}
Now, based on \eqref{eq:14} and \eqref{eq:15}, \(p_t\) can be written as:
\begin{eqnarray}
    p_t = {(S^r \otimes S^l)}^\top e_t + p_{t-1} - \mu {(S^r \otimes S^l)}^\top D_{t-1} {(S^r \otimes S^l)}p_{t-1} \label{eq:recurstive_pt}
\end{eqnarray}
Let define:
\begin{equation}
\begin{gathered}
 \widehat{D}_t := {(S^r \otimes S^l)}^\top D_t (S^r \otimes S^l) = \frac1N \sum_i {(S^r \otimes S^l)}^\top (C_{i,t} \otimes B_{i,t}) (S^r \otimes S^l) \\ \quad
 =\frac1N \sum_i ({S^r}^\top C_{i,t}S^r) \otimes ({S^l}^\top B_{i,t} S^l) 
 \end{gathered}
\end{equation}
Then we can expand \eqref{eq:recurstive_pt} and show that:
\begin{equation}
    p_t = (I - \mu \hat D_{t-1})p_{t-1} + (S^r \otimes S^l)^\top e_t
    \label{eq:20}
\end{equation}
Note that \(S^l\) and \(S^r\) are orthonormal matrices. This is ensured because the subspace is initialized using the SVD of \(G_0\), and the Grassmannian update rule provided in \eqref{eq:update-role} preserves the orthonormality of the subspace matrices throughout training. Since \(S^l\) and \(S^r\) are orthonormal, we have \({S^l}^\top S^l = I\) and \({S^r}^\top S^r = I\). Consequently, we can bound the norm of the second term in \eqref{eq:20} as follows:
\begin{equation}
\|(S^r \otimes S^l)^\top e_t\|_2 = \|vec({S^l}^\top E_t S^r)\|_2 = \|{S^l}^\top E_t S^r\|_F \le \|E_t\|_F
\end{equation}
Here \(E_t\) is the matrix form of \(e_t\), and as declared before, \(e_t := (a_t - a_{t-1}) + (D_{t-1} - D_t) w_t\), thus:
\begin{equation}
E_t := \frac1N\sum_i (A_{i,t} - A_{i,t-1}) + \frac1N\sum_i (B_{i,t-1} W_t C_{i,t-1} - B_{i,t} W_t C_{i,t}) \label{eq:22}
\end{equation}
Next, we need to find an upper bound for the norm of each term in \eqref{eq:22} to establish an upper bound for \(\|E_t\|_F\). Based on the assumptions of the theorem, \(A_i\), \(B_i\), and \(C_i\) exhibit L-Lipschitz continuity with constants \(L_A\), \(L_B\), and \(L_C\), respectively. Additionally, \(\|W_t\|_F\) is bounded by a scalar \(M\). We have:
\begin{eqnarray}
    \|A_t - A_{t-1}\|_F &\le& L_A \|W_t - W_{t-1}\|_F = \mu L_A \|\tilde G_{t-1}\|_F \le \mu L_A \|P_{t-1}\|_F 
\end{eqnarray}
In the first equality, we apply \eqref{eq:update_rule}, while the last equality holds due to \eqref{eq:15} and the orthonormality of the projection matrices. The subsequent two inequalities can be derived similarly using these equations.
\begin{equation}
\begin{gathered}
    \|(B_t - B_{t-1})W_t C_{t-1}\|_F \le L_B \|W_t - W_{t-1}\|_F \|W_t\|_F \|C_{t-1}\|_F 
    \\ \quad
    = \mu L_B L_C M^2 \|P_{t-1}\|_F  
\end{gathered}
\end{equation}
\\
\begin{equation}
\begin{gathered}
    \|B_t W_t (C_{t-1} - C_t)\|_F \le L_C  \|B_t\|_F \|W_t\|_F\|W_{t-1} - W_t\|_F 
    \\ \quad
    = \mu L_B L_C M^2 \|P_{t-1}\|_F
\end{gathered}
\end{equation}
We can now derive the bound for \(\|E_t\|_F\) as follows:
\begin{equation}
\begin{gathered}
    \|E_t\|_F \le \mu L_A \|\tilde G_{t-1}\|_F \le \mu L_A \|P_{t-1}\|_F + \mu L_B L_C M^2 \|P_{t-1}\|_F + \mu L_B L_C M^2 \|P_{t-1}\|_F \\ \quad 
    = \mu(L_A + 2L_BL_CM^2)\|P_{t-1}\|_F
\end{gathered}
\end{equation}
To calculate the norm bound for the first term in \eqref{eq:20}, we first need to establish the bounds for \(\widehat{D}_t\). This involves estimating the minimum eigenvalue of \(\widehat{D}_t\). 

If we define \(\gamma_{min, i,t} = \lambda_{min}({S^l}^\top B_{i,t} S^l)\lambda_{min}({S^r}^\top C_{i,t}S^r)\), then it follows that \(\lambda_{min}(({S^l}^\top B_{i,t} S^l)\otimes({S^r}^\top C_{i,t}S^r)) = \gamma_{min, i,t}\). Consequently, \(\widehat{D}_t\) will satisfy the following inequality for every unit vector \(\vv v\):

\begin{equation}
    \vv v^\top \widehat{D}_t \vv v = \frac1N \sum_i \vv v^\top \left[({S^l}^\top B_{i,t} S^l)\otimes({S^r}^\top C_{i,t}S^r)\right]\vv v \ge \frac1N \sum_i \gamma_{min, i,t} 
\end{equation}
this actually provides a lower bound for eigenvalues of \(\widehat{D}_t\), thus:
\begin{eqnarray}
    \lambda_{\max}(I - \mu \widehat{D}_{t-1}) \le 1 - \frac{\mu}{N} \sum_i \gamma_{min, i,t-1}
\end{eqnarray}
considering the definition of \(\kappa_t\) in the theorem, we can now easily show that:
\[
    \|P_t\|_F \leq [1-\mu(\kappa_{t-1} - L_A - 2L_B L_C M^2)]\|P_{t-1}\|_F.
\]
and completing the proof.

While SubTrack-Grad utilizes right/left projections to reduce memory consumption, the proof is presented using both projection matrices to ensure generality. Here, we demonstrate how the proof proceeds under the assumption \( m \leq n \) (without loss of generality), which allows the use of the left projection matrix.

Using the left projection matrix, the current formulation of \( P_t \), defined as \( P_t = {S_t^l}^\top G_t S_t^r \), simplifies to \( P_t = {S_t^l}^\top G_t \). Similarly, \( \widehat{G}_t = S_t^l {S_t^l}^\top G_t S_t^r {S_t^r}^\top \) reduces to \( \widehat{G}_t = S_t^l {S_t^l}^\top G_t \). From this point, the proof continues by substituting \( S_t^r \) with the identity matrix, allowing the derivation of the vectorized forms of \( g_t \), \( \widehat{g}_t \), \( p_t \), and related terms.

The remainder of the proof remains largely unaffected. It can be readily verified that the recursive formulation of \( g_t \) is unchanged. Although the definition of \( P_t \) is modified, it continues to satisfy the bounds required for convergence, ensuring that \( P_t \) converges to 0 when the left projection matrix is used.