\begin{definition}[\bf{L-continuity}]\label{def:cont}
 A function \(f(X)\) has Lipschitz-continuity (L-continuity) if for any \(X_1\) and \(X_2\), \( \|f(X_2) - f(X_1)\|_F \leq L\|X_2 - X_1\|_F \)
 \end{definition}

\begin{restatable}[\bf Convergence of SubTrack-Grad]{theorem}{convergence}
\label{th:convergence}
Suppose gradient has the following form (also \eqref{eq:gradient-form}) with functions \(A_i\), \(B_i\), and \(C_i\) being L-continuous as per \textbf{Def.}~\ref{def:cont} with constants \(L_A\), \(L_B\), and \(L_C\) w.r.t. weight matrix \(W_t\); and \(\|W_t\|_F \leq M\); where \(W_t\) denotes the weight matrix at step \(t\), and \(M\) is a scalar value,
\begin{equation*}
\small
    G = \sum_i A_i + \sum_i B_iWC_i.
\end{equation*}
Now, define \(\widehat{B}_{i,t} = (S_{i, t}^l)^\top B_i(W_t) S_{i, t}^l\) and 
\(\widehat{C}_{i,t} = (S_{i, t}^r)^\top C_i(W_t) S_{i, t}^r\), where \(S_{i, t}^l\) and \(S_{i, t}^r\) are the rank-\(r\) left and right projection matrices; \(B_i(W_t)\) and \(C_i(W_t)\) denote the dependence of \(B_i\) and \(C_i\) on the weight matrices \(W_t\). Further letting \(P_t = {S_t^l}^\top G_t S_t^r\), and \(\kappa_t = \frac{1}{N} \sum_i \lambda_{min}(\widehat{B}_{i, t}) \lambda_{min}(\widehat{C}_{i, t})\), where $\lambda_{min}(\cdot)$ denotes the minimum eigenvalue over each batch, and \( N \) representing the number of samples in a batch. Assuming that the projection matrices remain constant during the training. Then  for learning-rate \(\mu\) and \(min(\kappa_t) > (L_A + 2L_B L_C M^2)\), the  SubTrack-Grad, with \(\rho_t \equiv 1\) (the element-wise regularizer of the optimizer) satisfies:
\[
    \|P_t\|_F \leq [1-\mu(\kappa_{t-1} - L_A - 2L_B L_C M^2)]\|P_{t-1}\|_F.
\]
That is, \(P_t \rightarrow 0\) and SubTrack-Grad converges.
% \end{theorem}
\end{restatable}