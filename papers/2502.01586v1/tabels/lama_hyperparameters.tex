\begin{table*}[h]
    \caption{Hyperparameters of pre-training Llama-based architectures.}
    \centering
    \label{tab:pt_hyperparameters}
    \begin{tabular}{l|c|ccccc}
    \midrule
    \midrule
    & & 60M   & 130M & 350M & 1B & 3B   \\
    \midrule
    \midrule
    Architectural & Hidden        &512  &768  &1024 &2048 &4096 \\
    Parameters &Intermediate  &1376 &2048 &2736 &5461 &11008 \\
    &Heads         &8    &12   &16 &24 &32 \\
    &Layers        &8    &12   &24 &32 &32 \\
    \midrule
    Shared Parameters & Learning Rate &1e-3  &1e-3  &1e-3 &1e-5 &1e-5   \\
    & Batch Size    &128  &128  &64 &8 &8   \\
    & Iterations & \multicolumn{5}{c}{10K} \\
    & Gradient Accumulation & \multicolumn{5}{c}{2} \\
    & Gradient Clipping & \multicolumn{5}{c}{1.0} \\
    & Warmup Steps & \multicolumn{5}{c}{1000} \\
    & scale & \multicolumn{5}{c}{0.25} \\
    & dtype & \multicolumn{5}{c}{bfloat16} \\
    \midrule
    SubTrack \& &Rank          &128  &256  &256 &512 &512   \\
    GaLore Parameters & Subspace Update Interval & \multicolumn{5}{c}{200} \\
    &SubTrack-Grad Step-Size & \multicolumn{5}{c}{10} \\
    \midrule
    BAdam Parameters & Block Swithch Interval & \multicolumn{5}{c}{100} \\
    & Switch Mode & \multicolumn{5}{c}{Random} \\
    \bottomrule
    \end{tabular}
\end{table*}
