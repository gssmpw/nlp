\begin{table*}[h]
\caption{\small Wall-time comparison for pre-training Llama-based architectures with varying model sizes on the C4 dataset. The experiments consist of 2000 iterations, corresponding to exactly 10 subspace update steps. The last two columns present the average percentage change in runtime relative to Full-Rank and GaLore, respectively.}
\label{tab:lama-time}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|ccccc|c|c|c}
\toprule
 & \bf 60M & \bf 130M & \bf 350M & \bf 1B & \bf 3B  & \bf {\small Avg}& \bf {\small w.r.t FR}& \bf {\small w.r.t GaLore}  \\
  & r=128 & r=256 & r=256 & r=512 & r=512  &  &  & \\
\midrule
\midrule
{\bf Full-Rank}   
                    &524.0      &1035.1      &1396.4      
                    &974.9      &1055.9      &997.3 &- & -\\
\midrule
\midrule
{\bf BAdam}
                    &511.3      &779.2      &961.6      
                    &798.6      &1004.1     &811.0  &-18.68\%  &- \\
\midrule
{\bf GaLore}
                    &547.8      &1094.2      &1589.0      
                    &1729.5     &2715.5      &1535.2 & +53.94\% & - \\
\midrule
{\bf SubTrack-Grad}
                    &534.9      &1061.0      &1465.9       
                    &1191.1     &1385.7      &1127.7 & +13.08\% & -26.54\% \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table*}
