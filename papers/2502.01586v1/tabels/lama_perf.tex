\begin{table}[!t]
\caption{\small Evaluation loss comparison for pre-training different Llama-based architectures on C4 dataset after 10K iterations.}
\label{tab:lama}
\begin{center}
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|ccccc|c}
\toprule
 & \bf 60M & \bf 130M & \bf 350M & \bf 1B & \bf 3B  & \bf Avg \\
  & r=128 & r=256 & r=256 & r=512 & r=512  & - \\
\midrule
\midrule
{\bf Full-Rank}   
                    &3.41      &3.25      &3.40       
                    &4.61      &4.52      &3.84 \\
\midrule
\midrule
{\bf GaLore}
                    &4.02      &3.61      &3.62       
                    &6.53      &6.57      &4.87 \\
\midrule
{\bf BAdam}   
                    &7.86      &7.08      &7.62       
                    &7.28      &7.12      &7.39 \\
\midrule
{\bf SubTrack-Grad}
                    &4.13      &3.77      &3.82       
                    &6.53      &6.59      &4.97 \\
\bottomrule
\end{tabular}
}
\end{center}
\end{table}
