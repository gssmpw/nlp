\begin{figure*}[h]
    \centering
    \begin{subfigure}{0.329\textwidth}
    \includegraphics[width=\textwidth]{images/time_lama.pdf}
    \caption{\small \textcolor{blue}{}}
    \label{fig2:a}
    \end{subfigure}
    \hfill%
    \begin{subfigure}{0.329\textwidth}
    \includegraphics[width=\textwidth]{images/time_superglue.pdf}
    \caption{\small \textcolor{blue}{}}
    \label{fig2:b}
    \end{subfigure}
    \hfill%
    \begin{subfigure}{0.329\textwidth}
    \includegraphics[width=\textwidth]{images/time_glue.pdf}
    \caption{\small \textcolor{blue}{}}
    \label{fig2:c}
    \end{subfigure}
    \caption{\small Comparing wall-time on different architecture and datasets. (a) shows the memory and wall-time measured on pretraining different Llama-architectures sizing from 60M to 3B. (b) demonstrates wall-time measured on fine-tuning RoBERTa-Large on SuperGLUE tasks. (c) presents wall-time measured on fine-tuning RoBERTa-Base on GLUE tasks. As demonstrated, GaLore can lead to 157\% wall-time increase in training Llama with 3B parameters, while SubTrack-Grad significantly reduces this overhead.}
\vspace{-3mm}
\end{figure*}
