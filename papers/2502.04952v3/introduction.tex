
\section{Introduction}
\label{sec:intro}

Path-sensitive value-flow analysis~\cite{cherem2007practical,livshits2003tracking,shi2018pinpoint,fan2019smoke, shi2020conquering,shi2021path, sui2014detecting, sui2016sparse, sui2016svf, wang2023anchor, wang2022complexity} is highly effective in detecting a broad spectrum of software bugs,
such as memory leaks in resource usage, null pointer dereference in memory safety, and the propagation of tainted data in security properties,
by tracking the flow of values along data dependence relations.
Essentially, detecting these bugs boils down to collecting feasible source-sink paths over a program dependence graph~\cite{cherem2007practical, shi2018pinpoint}.
For instance, detecting the null pointer dereference (NPD), considering the null value as the source and the pointer dereference statement as the sink. 
The process involves a two-step process: collecting paths that link a null value and a pointer dereference statement, and then verifying the satisfiability of the path conditions for those paths.

% Motivation
To scale the analysis to large-scale software systems with millions of lines of code, existing approaches~\cite{shi2018pinpoint, shi2020conquering, shi2021path, fan2019smoke, babic2008calysto, xie2005scalable,wu2024libalchemy} employ a bottom-up strategy to gather feasible source-sink paths. 
Specifically, when analyzing a function, these approaches compute the intra-procedural value-flow paths and the corresponding path conditions as function summaries.
The value-flow paths and conditions are referred to as summary paths and summary conditions, respectively.
To ensure that only feasible summaries are collected, a constraint solver is invoked to verify the summary condition once the summary path is collected, despite being a computationally costly process.
To avoid redundant path searching and analysis of callee functions, 
existing approaches clone the summaries of callees and reuse them continuously to supplement the more extended summaries collected within caller functions. 
The summary cloning and summary condition verification process continues until the highest function in the call graph (known as the root) is reached. 
At this point, the algorithm can directly identify source-sink paths by examining summary paths originating from sources and terminating at sinks. 
Since each summary path carries its corresponding path conditions, we can use the terms ``summary'' and ``summary path'' interchangeably without losing generality.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/motivation_example.pdf}
    \caption{Bottom-up analysis for the code shown in (a).
    The (b) shows the corresponding program dependence graph (PDG). 
    %In the PDG, solid black arrows ($\rightarrow$) are data dependence edges, while dashed curved arrows ($\dashrightarrow$) depict control dependence edges. 
    %The guards for data dependence are labeled.
    (c) shows the partial function summaries collected during the bottom-up analysis.
    Redundant summaries are highlighted in red.}
    \label{fig:motivation_ex}
    % \vspace{-0.4cm}
\end{figure*}


We use the buggy program shown in Fig.~\ref{fig:motivation_ex}(a) to illustrate the existing bottom-up compositional value flow analysis.
We use the symbol $\pi$ to represent a value-flow path, while $\phi$ and $\varphi$ represent path conditions. Moreover, the variable $v_{i}$ denotes that variable $v$ is either used or defined at Line $i$.
Specifically, one of the function summaries for \textit{foo}, represented as $\pi_{4}$ in Fig.~\ref{fig:motivation_ex}(c), summarizes the propagation path of variable $a_{2}$. 
The variable $a_{2}$ receives the return value from the \textit{qux} function in Line 2 and is subsequently passed to the \textit{bar} function in Line 5.
On one hand, the summary path $\pi_{4}$ is generated by combining the summaries $\pi_{1}$ and $\pi_{3}$, which are collected during the analysis of the \textit{qux} and \textit{bar} functions, respectively, before analyzing the \textit{foo} function.
On the other hand, the summary condition $\phi_{\pi_{4}}$ is obtained by instantiating edges and the corresponding guards along the summary path $\pi_{4}$~\cite{shi2018pinpoint}. 
The guard $\varphi_2$ of the edge $p_{11} \rightarrow printf(*p_{13})$ is the constraint of $p_{11} \neq NULL_{12}$, which is instantiated when collecing the summary $\pi_{3}$ by traversing from vertex $\varphi_2$ on the program dependence graph, shown in Fig.~\ref{fig:motivation_ex}(b).
Once the summary condition $\phi_{\pi_{4}}$ is instantiated, the summary is verified by the constraint solver \textit{Z3}~\cite{de2008z3} before it is stored.
The summary conditions $\phi_{\pi_{1}}$, $\phi_{\pi_{2}}$, and $\phi_{\pi_{3}}$ are also verified when collected.
If a summary condition is unsatisfiable(\textit{unsat}), it is discarded to avoid an unfeasible summary being maintained.
% (We provide such a case in a longer version of this paper~\cite{toappear}.)
(We provide such a case in Appendix Section~\ref{app:unsat-summary}.)
Moreover, the summary $\pi_1$ of the \textit{qux} function is cloned twice (cloned one denoted as $\pi_1^{\prime}$) with different calling contexts (Line 2 and Line 3) to account for the different propagation paths between the summaries $\pi_4$ and $\pi_6$ (which summarizes the propagation path of variable $b_{3}$). 
This cloning mechanism eliminates the necessity of searching for and analyzing the \textit{qux} function again, leading to improved efficiency.

\textbf{The Explosive Summary Problem.}
However, the bottom-up approach still faces challenges in terms of analysis time and memory consumption. 
According to a report~\cite{shi2018pinpoint}, a single analysis for a project with millions of lines of code can take several hours and require hundreds of gigabytes (GB) of memory. 
The main reason for this is the exponential growth in the size of summaries due to cloning for different calling contexts.
Even worse, exploded summaries lead to frequent calls to the constraint solver, as each summary collection necessitates a call to the solver.
For example, in the previous example, the summary $\pi_1$ is cloned and stored as two copies ($\pi_1$ and $\pi^{\prime}_{1}$) within the function \textit{foo}.
This leads to the collection of three new summaries $\pi_4$, $\pi_5$, and $\pi_6$, which eventually result in three calls to the constraint solver.
When analyzing higher-level callers, the need for additional clones can cause significant performance issues.


Existing techniques to improve the performance have focused on achieving efficient summary path collection~\cite{shi2020pipelining, tang2023Scaling} and the verification of summary conditions \cite{shi2021path}.
Specifically, Shi~\cite{shi2020pipelining} and Tang~\cite{tang2023Scaling} proposed a parallel algorithm to accelerate summary path collection, reducing the analysis time. 
Shi's recent work~\cite{shi2021path} introduced a unified representation of summary paths and summary conditions on the program dependence graph, enabling the direct verification of summary conditions on the PDG and eliminating the need for additional computation and storage of summary conditions.

\textbf{Our Approach.}
To tackle this problem, we propose the first approach that identifies and eliminates ``useless'' summaries while also reducing the size of collected summaries from callee functions.
Our key observation is that certain summaries in the callee functions do not contribute to any source-sink path (even when their summary condition is satisfiable(\textit{sat})) and, thus, can be safely ignored without compromising the analysis's precision.
For example, in Fig.~\ref{fig:motivation_ex}(c), the summary $\pi_6$ obtained from the function \textit{foo} does not lead to any bugs since there are no dereference operations on the inlined $NULL_{19}^{'}$ from the callee function $qux$. 
As a result, it is unnecessary to compute the summary $\pi_6$ and solve its path condition $\phi_{\pi_6}$.
Additionally, we can further reduce unnecessary computations by avoiding the cloning of $\pi_1$, not collecting $\pi_2$ (and solving the summary condition) to eliminate the redundant $\pi_6$.
Our experiments (as shown in Table~\ref{table:benchmark} under the ``\#Redun'' column) indicate that approximately 20\% of redundant summaries are computed, solved, and maintained throughout the analysis process on average.

The benefit of our approach is twofold. 
First, our approach efficiently reduces time and memory usage by eliminating unnecessary computations of summaries, which can become exponentially large as the analysis progresses.
Second, the high-cost invocation of a constraint solver to verify ``useless'' summaries is subsequently avoided.
These two unique advantages make our approach more practical for efficiently analyzing large-scale software systems.
Our approach is orthogonal and can be used in conjunction with other approaches, 
such as enhancing the summary representation 
by employing advanced data structures, i.e., graph structures, for representing and resolving summary conditions~\cite{shi2021path}, and effectively collecting summaries
in a parallel~\cite{tang2023Scaling} or pipelined manner~\cite{shi2020pipelining}. 

% For example, analyzing the functions \textit{bar}, \textit{baz}, and \textit{qux} simultaneously~\cite{tang2023Scaling} or in a pipelined manner~\cite{shi2020pipelining}.

% For example, even though the functions \textit{bar}, \textit{baz}, and \textit{qux} are analyzed simultaneously using the approaches in~\cite{shi2020pipelining, tang2023Scaling}, or a graph structure is used to represent and solve summary conditions as in~\cite{shi2021path},
% these "useless" summaries are still computed and verified, and redundant computation cannot be avoided.



% In our evaluation, even after adopting these optimizations in our baseline implementation, about 20\% of redundant summaries still remain. 
% In contrast, our approach can eliminate 80\% of these redundant summaries, significantly enhancing the performance of the baseline.

\textbf{Challenges and Solutions.} 
The challenge lies in identifying redundant summaries precisely and efficiently without hurting the precision of the analysis.
Specifically, determining the contribution of a summary often relies on information from upper-layer functions that have not been analyzed yet. 
% Note that delaying the determination until all callers and upper callers have been analyzed is ineffective. 
% This is because, at that time, the redundant summaries would have already been computed, rendering the identification process meaningless.


Our key insight is that a useful summary should \emph{be a component of} paths or path conditions associated with a source-sink path of interest.
Specifically, the summary should be reachable from at least one pair of sources and sinks or derived from the path conditions of the source-sink paths.
\sloppy
In Fig.~\ref{fig:motivation_ex}, the usability of summary $\pi_2$ is decided by evaluating its reachability with the source-sink pairs $(NULL_{19}, printf(*a_9))$ and $(NULL_{19}, printf(*p_{13}))$. 
That is, we assess the reachability of the source $NULL_{19}$ and the sink $printf(*a_9)$ (or $printf(*p_{13})$) using the head $f_{15}$ and tail $f_{16}$ of $\pi_2$, which are parameter and return of function $baz$. 
They are represented as $(NULL_{19}, f_{15})$, $(f_{16}, printf(*a_9))$, and $(f_{16}, printf(*p_{13}))$.
Without considering how $\pi_2$ will be used in caller \textit{foo}, we can still decide that $\pi_2$ is not reachable from the two mentioned source-sink pairs. 
Consequently, $\pi_2$ is deemed redundant in the long run and can be promptly discarded.
Using this insight, we have devised a principled, sound, and efficient contribution identification algorithm powered by a novel concept, namely \textit{contribution abstraction}, to identify the contributing summaries.
We give more details in Section~\ref{sec:overview}.

% Our key insight is that any summary should be reachable from at least one pair of sources and sinks if it is used to concatenate with a source-sink path.
% Instead of considering whether a summary is used for uncollected summaries in the upper layer, we could determine the contribution of a summary by pre-computing its reachability between source and sink pairs.
% Our key insight is that redundant summary identification could be achieved without instantiating any summary, and meanwhile, the contribution of a summary that completes the source-sink path has a higher priority to be identified.

% Our idea comes from three key observations.
% First, any summary should be reachable from at least one pair of sources and sinks if it is used to connect a source-sink path.
% Instead of considering whether a summary is used for uncollected summaries in the upper layer, we could determine the usefulness of a summary by pre-computing its reachability between source and sink pairs.

% Second, for any summary to fulfill the path condition of other summaries, it is essential that the summary be reachable from the guards annotated on those summaries. For instance, in the case of summaries $\pi_1$ and $\pi_2$ implicitly contributing to $\pi_7$, both summaries can be reached from the guard $\varphi_1$ annotated on the path edge of $\pi_7$ in the program dependence graph.
% Third, when assessing a summary's contribution to fulfilling the path condition of other summaries, it is crucial to ensure these summaries eventually contribute to the connection of a source-sink path.
% As discussed, even though $\pi_1$ and $\pi_2$ take part in the establishment of the summary condition of $\pi_7$, they are still non-contributing summaries because $\pi_6$ is not used to completing a source-sink path.
% Second, when a summary is involved in the path condition of another summary that is used to connect a source-sink path, its contribution is assured.
% Based on the observations, our key insight is that the contribution of a summary that completes the source-sink path has a higher priority to be identified, and meanwhile, their identification could be achieved without instantiating any summary.

% Based on the observations, our key insight is that redundant summary identification could be achieved without instantiating any summary, and meanwhile, the contribution of a summary that completes the source-sink path has a higher priority to be identified.





% For example, in Fig.~\ref{fig:motivation_ex}(a),
% when analyzing the potential null pointer dereference(NPD) errors if the $malloc$ function fails to allocate memory and returns $null$ value to pointer $p$ in function $bar$,
% the summary, path segment $y \rightarrow x$, within the $qux$ function can be safely ignored, as it does not contribute to any propagation path of $p$ leading to a dereference site. 
% Instead, the analyzer should focus on the summaries that cross the $bar$, $baz$, and $foo$ functions and lead to potential NPD errors: $null \rightarrow p \rightarrow ptr \rightarrow z \rightarrow deref(z)$ (crossing $bar$, $baz$, and $foo$) and $null \rightarrow p \rightarrow ptr \rightarrow printf \rightarrow deref(ptr)$ (crossing $bar$ and $foo$).
% However, Fusion computes all path segments within the analyzed function and verifies their feasibility, regardless of whether they could contribute to the paths leading to errors. 
% Moreover, these non-contributing path segments are then instantiated into the caller functions, allowing the analyzer to stitch together path segments from other functions to form more complete paths.
% However, the resulting paths still do not cause any errors. 
% For instance, a path segment such as $y \rightarrow x$ within the $qux$ function could be computed and verified during the analysis of $qux$, 
% even though it cannot cause an NPD error concatenating with other path segments, such as the path $null \rightarrow p \rightarrow ptr \rightarrow y \rightarrow x$ in the example.
% This conservative approach results in redundant computations, which can slow the analysis process and lead to inefficient use of computational resources.
% As the analysis progresses, the analyzer instantiates these path segments further into upper-level callers leading to an explosive growth of the path segments.
% Significant resources are wasted on maintaining and concatenating these redundant path segments after checking all the possible path segments that could be stitched together among all functions. They were eventually concluding that there was no error following the paths. However, this conclusion comes at a high computational cost, particularly in large and complex codebases.


\textbf{Results.}
We have implemented the contribution identification (CI) algorithm based on the state-of-the-art value flow analysis, Fusion~\cite{shi2021path}, and evaluated it on 17 real-world programs.
The evaluation results show our CI algorithm can significantly reduce the time and memory overhead of the Fusion by 45\% and 27\%, respectively. 
Furthermore, the CI algorithm can efficiently identify almost 80\% of redundant summaries while only incurring a minimal additional overhead.
In the largest project, the \textit{mysqld} case, CI helps Fusion save 8107 seconds (2.25 hours) with only 17.31 seconds of overhead, resulting in a ratio of time savings to paid overhead (performance gain) of 468.48 $\times$. Overall, CI achieves a substantial average performance gain of 632.1 $\times$.


To sum up, this paper makes three main contributions:
\begin{itemize}
    \item We identify and address the redundant summary deficiency in the prior value flow analysis.
    \item We design the contribution identification algorithm to identify redundant summaries efficiently and effectively.
    \item On average, the contribution identification algorithm can substantially enhance the performance of value flow analysis, reducing time consumption by 45\% and minimizing memory utilization by 27\%.
\end{itemize}
