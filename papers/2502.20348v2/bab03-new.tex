\begin{table*}[t]
\caption{NASA dataset.}
\label{tab:dataset_nasa}
\begin{tabular}{lllllllll}
\toprule
Job & Submit Time & Wait Time & Run Time & Proc Alloc & User ID & Group ID & Executable Number & .... \\ 
\midrule
1   & 0           & -1        & 1451     & 128        & 1       & 1        & -1                & ...  \\ 
2   & 1460        & -1        & 3726     & 128        & 1       & 1        & -1                & ...  \\ 
3   & 5198        & -1        & 1067     & 128        & 1       & 1        & -1                & ...  \\ 
... & ...         & ...       & ...      & ...        & ...     &          &                   & ...  \\ 
\bottomrule
\end{tabular}
\end{table*}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Weekly-Job.png}
    \caption{Daily job submission pattern in the real dataset.}
    \label{fig:weeklyGraph}
\end{figure}
\begin{figure}[t]
        \centering
         \includegraphics[width=\linewidth]{Daily-job.png}
         \caption{Hourly job submission pattern in the real dataset.}
         \label{fig:dailyGraph}
\end{figure}

\section{Improving the DRL agent with CL to solve PSMP}\label{sec:ProposedMethod}

In this research, we propose to adopt \ac{cl} into the \ac{drl}-based method for \ac{psmp}. To adopt \ac{cl}, a real workload trace of an \ac{hpc} job history is analyzed, and additional datasets are generated. After that, the curriculum sequence is organized based on the generated datasets. The process then continues with training the \ac{drl} model.



\subsection{Dataset description/workload trace}\label{subsec:Dataset}

This study uses a real workload trace from NASA's \ac{hpc} system, accessible via the Parallel Workloads Archive \citep{Feitelson}. The log data contains jobs submitted over three months to the \ac{hpc} iPSC/860 system at NASA Ames Research Center, which has 128 nodes. In this system, each submitted job must request a power-of-two number of nodes---a common requirement in HPC workloads \cite{pollinger2023leveraging}.
%Job log data from the \ac{hpc} iPSC/860 system is provided in \ac{swf} \citep{SWF}. 

This dataset has six available features: Submit Time, Run Time, Number of Allocated Nodes, User ID, Group ID, and Executable Number, as shown in Table \ref{tab:dataset_nasa}. Columns with a value of -1 lack data and cannot be used. Submit Time indicates the time a job is submitted relative to the first submitted job, measured in seconds. Run Time specifies how long the job runs, also measured in seconds. Number of Allocated Processors is the number of nodes allocated to the job. User ID and Group ID represent the IDs of the user and their group, respectively. Additionally, the dataset contains information about StartTime and EndTime, which indicate the date and time when the first job entered the queue.


\begin{figure}[t]
        \centering
         \includegraphics[width=\linewidth]{Job-Distribution.png}
         \caption{Job size distribution in the real dataset.}
         \label{fig:jobGraph}
\end{figure}
\begin{figure}[t]
        \centering
         \includegraphics[width=\linewidth]{Walltime-Distribution.png}
         \caption{Walltime distribution in the real dataset.}
         \label{fig:walltimeGraph}
\end{figure}

% \subsection{Dataset Preprocessing}
In this research, not all features are required. The features used are Submit Time (Submission Time), Run Time (Walltime), and Number of Allocated Processors (Requested Node). The User ID, Group ID, and Executable Number features are excluded as they are considered less influential on the job runtime. 
% The dataset also needs to be converted into JSON format for use in the simulator. 
The dataset is divided into an 80:20 ratio for training and testing. The dataset used for training is also used to create additional datasets. 
%The result of dataset preprocessing in JSON format can be seen in Listing \ref{json-dataset}. 
This dataset, referred to as the real dataset, contains 18,067 jobs. Moreover, the statistical patterns of the real dataset---discussed in the following paragraphs---will be used to construct the sampled and synthetic datasets. The arrangements of these three datasets will then be used for the training curricula.



Figure~\ref{fig:weeklyGraph} shows the weekly job submission pattern, where the x-axis represents days, and the y-axis represents the average daily submissions. Figure~\ref{fig:dailyGraph} shows the percentage of jobs submitted by hour, with the x-axis representing hours and the y-axis representing the percentage of total daily submissions. Figure~\ref{fig:jobGraph} displays the job size distribution, where the x-axis represents the number of requested nodes, and the y-axis represents the number of submissions. Meanwhile, Figure~\ref{fig:walltimeGraph} presents the walltime distribution divided into seven groups, with the x-axis representing walltime groups and the y-axis representing the number of submissions.

\subsection{Creating the datasets for CL}

This research employs predefined \ac{cl}, where the difficulty of the dataset is manually determined before training begins. Difficulty indicates how challenging it is for the \ac{drl} agent to predict the arrival patterns of jobs.
%\santodo{The dataset used is divided into two levels of difficulty: easy and hard. This method requires additional datasets beyond the real dataset to train the model}.
In this research, three types of datasets are used: real, sampled, and synthetic datasets.

\subsubsection{Real dataset}

The real dataset consists of raw data that has undergone preprocessing, as explained in Sec.~\ref{subsec:Dataset}. Compared to the sampled dataset, this dataset is categorized as hard difficulty because it contains data originating from real-world scenarios. The real dataset is the source for creating the sampled and synthetic datasets.

\subsubsection{Sampled dataset}
\label{sec:sampleddataset}

The sampled dataset is created by randomly selecting a subset of jobs from the real 
dataset and modifying their submission times to make them easier to predict while 
retaining the characteristics of the real dataset. The submission times are adjusted so 
that the inter-arrival times of this dataset follow an exponential distribution 
commonly used for modeling queues. 
%The PDF and CDF of this distribution are shown in (\ref{eq:pdf}) and (\ref{eq:cdf}). 
The exponential distribution is memoryless and describes the inter-arrival times in homogeneous Poisson processes; thus, making the job arrival stream more predictable than the real and synthetic datasets. Hence, this dataset can be considered as an easy-difficulty dataset.
%(difficulty easy). 
To preserve the original dataset's characteristics, the mean inter-arrival time of the real dataset ($\mu$) is calculated by:
\begin{equation}
\label{interarrival}
    \mu = \frac{1}{18066} \sum_{i=1}^{18066} (t_{i+1} - t_i).
\end{equation}
Here, 18066 represents the total number of records in the dataset minus one, and $t_i$ is the submission time of the $i$-th job. Random samples are selected from the entire real dataset as in \cite{Fan}. Then, $\mu$
%is substituted into (\ref{eq:pdf}),
is used to calculate the rate parameter of the \ac{pdf},
and the submission times of the selected jobs are modified using Algorithm \ref{generatesample}. The function `generateRandomExponential' takes $\mu$ as input and generates random numbers whose \ac{pdf} follow an exponential distribution.

\begin{algorithm}[]
\DontPrintSemicolon
    \caption{Generate Sampled Dataset}
    \label{generatesample}

        \SetKwFunction{FMain}{generateSubmissionTime}
        \SetKwProg{Fn}{Function}{:}{}
        \Fn{\FMain{$F$} \textbf{return} int}{
            interArrivalTime $\gets$ generateRandomExponential(average\_inter\_arrival\_time) \;
            currentTime $\gets$ currentTime + interArrivalTime \;
            \Return currentTime
        }
        currentTime $\gets$ 0 \;
        sampled\_jobs $\gets$ sample from real\_dataset \;
        \For{$i$ \textbf{from} $1$ \textbf{to} length(sampled\_jobs-1)}{
            sampled\_jobs[i]['submission\_time'] $\gets$
            generateSubmissionTime(currentTime)
        }
    
\end{algorithm}

\subsubsection{Synthetic dataset}\label{sec:syntheticdataset}

% \sanedited{\st{The synthetic dataset is an artificial dataset designed to resemble the real dataset, making it categorized as difficulty hard. In addition to increasing data volume, creating a synthetic dataset also aims to introduce certain variations and complexities that may not be adequately represented in the real dataset.} 
The synthetic dataset is an artificial dataset designed to resemble the real dataset. As a result, it is classified as a hard-difficulty dataset.
% \santodo{In addition to increasing data volume, the creation of this synthetic dataset also aims to introduce certain variations and complexities that may not be adequately represented in the real dataset.}
A critical aspect of creating this synthetic dataset is ensuring that it mimics the patterns of the real dataset. This allows the \ac{drl} agent to understand realistic data and improve its adaptability to new data variations. To replicate the real dataset's patterns, the following four types of information are extracted:
\begin{enumerate}
    \item Weekly job submission pattern: the average number of jobs submitted on each day of the week (Monday–Sunday).
    \item Daily job submission pattern: the percentage of jobs submitted within each hour of the day. For instance, jobs submitted between 13:00–14:00 are categorized as jobs for hour 13.
    \item Job size distribution: the distribution of node requests for jobs.
    \item Walltime distribution: the distribution of job walltimes.
\end{enumerate}


Based on the extracted patterns, the StartTime information from the dataset is used to convert submission times into date, day, and time formats. Jobs are generated to match the average daily count from the weekly job submission pattern, and the hourly job counts are derived from the daily job submission pattern. Once each job receives a submission hour, random minutes and seconds are assigned. After all jobs for a day are created, they are sorted by time, and the process continues for the next day. Once all jobs are completed, submission times are converted back to seconds. Walltime and requested nodes are generated based on the walltime and job size distributions in the real dataset. Similar to \cite{Fan}, the synthetic dataset is created in larger quantities than the real dataset to provide more variety for the model to learn from. Therefore, in this study, the synthetic dataset spans one year.



\subsection{Curriculum design}\label{curriculum_design}
The curriculum is created by arranging the sequence of datasets to be trained into the model. Three types of datasets are used: real, sampled, and synthetic datasets, enabling the creation of six different curricula. By combining these three datasets in various sequences, the curriculum can be designed to allow the model to learn progressively, from easy difficulty to hard difficulty or vice versa.

\begin{figure}[t]
\includegraphics[width=\linewidth]{kurikulum.png}
\centering
\caption{Possible curriculum arrangements.}
\label{fig:curriculum}
\end{figure}

Figure \ref{fig:curriculum} shows three different approaches to curriculum design: easy-to-hard, hard-to-easy, and hard-easy-hard. Each approach has two variations of dataset arrangements. The easy-to-hard approach starts with training using the sampled dataset, followed by the real or synthetic dataset. Conversely, the hard-to-easy approach begins with the real or synthetic dataset and ends with the sampled dataset. In the hard-easy-hard approach, the sampled dataset is flanked by the real and synthetic datasets.
In total, there are six possible curriculum arrangements. In this research,
the impact of all possible arrangements on the model performance is studied.
Therefore, six different \ac{drl} models will be trained, each using one out of six possible curriculum arrangements. 

