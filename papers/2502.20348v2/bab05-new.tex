\section{Results and discussion}\label{sec:Results}
% masukkan thomas 5.2 ke sini (ini bab hasil)

% \subsection{Dataset Characteristics}
\subsection{Dataset validation}

% \todo{[Editing (and wanting to remove) paragraph. replace this paragraph with the next one] 
% This section analyzes the characteristics of the dataset, including the distribution of job requested nodes, job walltime, daily job submissions, and weekly job submissions, as described in Subsection \ref{sec:syntheticdataset}. Additionally, the sample dataset is analyzed to assess its conformity with an exponential distribution. This analysis aims to determine whether the sample and synthetic datasets meet the desired criteria.
% }


% \sanedited{\st{This section aims to prove that our dataset generation method is sound and would improve the training process with our proposed} \ac{cl} \st{strategies. This section aims to show how similar our generated dataset is to the actual dataset:} 
In this section, 
we validate and show that our sampled and synthetic datasets follow the desired statistical characteristics. This analysis includes the distribution of job requested nodes, job walltime, daily job submissions, and weekly job submissions, as described in Sec.~\ref{sec:syntheticdataset}. Additionally, the sampled dataset is analyzed to assess its conformity with an exponential distribution.


% \edited{
% In this section, we validate and show that our sample and synthetic datasets follow the desired statistical characteristics. This analysis includes the distribution of job requested nodes, job walltime, daily job submissions, and weekly job submissions, as described in Subsection~\ref{sec:syntheticdataset}. Additionally, the sample dataset is analyzed to assess its conformity with an exponential distribution. Overall, this section aims to prove that our dataset generation method is sound and would improve the training process with our proposed \ac{cl} strategies.
% }

\subsubsection{Sampled dataset}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{interarrival-cdf.pdf}
\caption{
CDF of the inter-arrival time of the sampled dataset compared to the CDF of a true exponential distribution.}
%CDF of the inter-arrival time of the sample dataset
\label{fig:sampleCDF}
\end{figure}

% \begin{table}[t]
% \caption{Number of jobs by requested nodes in the real dataset} \label{tab:realJobSize}
% \begin{tabular}{lr}
% \toprule
% Number of requested nodes & Number of jobs \\
% \midrule
% 1 & 3949 \\
% 2 & 1461 \\
% 4 & 2085 \\
% 8 & 1337 \\
% 16 & 1462 \\
% 32 & 2926 \\
% 64 & 888 \\
% 128 & 344 \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[t]
% \caption{Number of jobs by walltime in the real dataset} \label{tab:walltimeReal}
% \begin{tabular}{lr}
% \toprule
% Walltime (seconds) & Number of jobs \\
% \midrule
% 1-60 & 5991 \\
% 61-600 & 6094 \\
% 601-1800 & 1007 \\
% 1801-3600 & 574 \\
% 3601-9600 & 501 \\
% 9601-21600 & 239 \\
% 21601-43200 & 40 \\
% 43201-86400 & 6 \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[t]
% \caption{Number of jobs by requested nodes in the synthetic dataset} \label{tab:jobSynth}
% \begin{tabular}{lr}
% \toprule
% Number of requested nodes & Number of jobs \\
% \midrule
% 1 & 19546 \\
% 2 & 6898 \\
% 4 & 10318 \\
% 8 & 6994 \\
% 16 & 6879 \\
% 32 & 14278 \\
% 64 & 4600 \\
% 128 & 1547 \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[t]
% \caption{Number of jobs by walltime in the synthetic dataset} \label{tab:walltimeSynth}
% \begin{tabular}{lr}
% \toprule
% Walltime (seconds) & Number of jobs \\
% \midrule
% 1-60 & 29384 \\
% 61-600 & 30039 \\
% 601-1800 & 5077 \\
% 1801-3600 & 2752 \\
% 3601-9600 & 2353 \\
% 9601-21600 & 1214 \\
% 21601-43200 & 211 \\
% 43201-86400 & 30 \\
% \bottomrule
% \end{tabular}
% \end{table}

The sampled dataset is a dataset where the walltime and requested node information are sampled from a real dataset. Then, the job arrival stream is generated using an exponential distribution. As a result, the walltime and requested node information will indirectly be similar to the real dataset. 
%The arrival stream distribution, depicted by the CDF of the inter-arrival time, is shown in Figure \ref{fig:sampleCDF}. 
In Figure~\ref{fig:sampleCDF}, the \ac{cdf} of the inter-arrival time is compared to the \ac{cdf} of the true exponential distribution. The figure shows that the arrival stream of the sampled dataset distribution is extremely close to the true exponential distribution, with a \ac{rmse} of $0.0025$ and a relative \ac{rmse} of $0.0049$ on the sample instances.
% \st{This exponential distribution makes the job arrival stream more predictable than the real dataset and the synthetic dataset.}
% Since the exponential distribution is memoryless and describes the interarrival times in homogeneous poisson processes, it makes the job arrival stream more predictable than the real dataset and synthetic dataset.
This indicates that the sampled dataset follows an exponential distribution; thus, it is more predictable than the real and synthetic datasets---as explained in Sec.~\ref{sec:sampleddataset}.


\subsubsection{Comparison of characteristics of real and synthetic datasets}
\label{hasilReal}

The real dataset serves as a reference for creating additional datasets. 
% Table \ref{tab:realJobSize} shows the details of the number of jobs for each requested node, and Table \ref{tab:walltimeReal} displays the number of jobs by walltime in the real dataset. Meanwhile, the synthetic dataset is designed to resemble the real dataset as a new variation for training. To achieve this, the synthetic dataset is created to follow the characteristics of the real dataset. The details of the number of jobs against requested nodes and walltime are shown in Tables \ref{tab:jobSynth} and \ref{tab:walltimeSynth}, respectively.
The distribution of requested nodes in both datasets is shown in Figure~\ref{fig:jobSizeDistribution}. It can be seen that the distribution of requested nodes in the synthetic dataset closely resembles that of the real dataset. Similarly, the distribution of walltime is compared in Figure \ref{fig:walltimeDistribution}, and the distribution of walltime in the synthetic dataset accurately mimics the distribution in the real dataset.



\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{Perbandingan-job-size-Real-vs-Synthetic.pdf}
\caption{Comparison of the requested node distribution.}
\label{fig:jobSizeDistribution}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{Perbandingan-walltime-Real-vs-Synthetic.pdf}
\caption{Comparison of walltime distribution.}
\label{fig:walltimeDistribution}
\end{figure}


% \subsection{Analysis of strategies on the dataset}


\subsection{Comparison between curricula}

% \begin{table}[t]
% \caption{Total energy consumption across curricula.} 
% \label{tab:totalEnergy}
% \begin{tabular}{lr} 
% \toprule
% \textbf{Curriculum}    & \textbf{Total Energy Consumption (Joules)}  \\ 
% \midrule
% Sample-Real-Synthetic & 76,570,309,456 \\ 
% Sample-Synthetic-Real & 77,085,781,495 \\ 
% Real-Synthetic-Sample & 77,425,704,567 \\ 
% Synthetic-Real-Sample & 78,052,475,668 \\ 
% Synthetic-Sample-Real & 78,050,001,903 \\ 
% Real-Sample-Synthetic & 78,067,199,395 \\ 
% Without Curriculum    & 77,333,721,351 \\
% \bottomrule
% \end{tabular}
% \end{table}

\begin{figure*}
    \centering
    
    \begin{subfigure}{0.96\textwidth}
        \centering
        \fbox{\includegraphics[width=\textwidth]{curriculum-legend.pdf}}
        % \caption{Total energy consumption}
        % \label{fig:morl:drlmoa}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{total-energy.pdf}
        \caption{Total energy consumption.}
        \label{fig:total-energy}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{total-waste-energy.pdf}
        \caption{Total wasted energy.}
        \label{fig:waste-energy}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{average-waiting-time.pdf}
        \caption{Average job waiting time.}
        \label{fig:avg-waittime}
    \end{subfigure}
    \caption{Evaluation metrics results.}
    \label{fig:multiple_figures}
\end{figure*}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{Hasil New/Total Konsumsi Energi Vs Kurikulum New.pdf}
%     \caption{Comparison of total energy consumption across curricula.}
%     \label{fig:totalEnergyCur}
% \end{figure}

%Table \ref{tab:totalEnergy} and 
% Figure~\ref{fig:totalEnergyCur} presents the total energy consumption 
% and the comparison graph between curricula and the no-CL Agent configuration. Total energy consumption includes the energy used for computation. Therefore, to conduct a more detailed analysis, the wasted energy graph needs to be examined.
The total energy consumption of the six models trained with different curricula
is shown in Figure~\ref{fig:multiple_figures} (a). The result of the
model trained without \ac{cl} is also included as a reference. As shown by the figure, 
the two models trained by easy-to-hard curricula have lower total energy consumption
than those trained without \ac{cl}, where
the model trained by the sampled-real-synthetic curriculum has the lowest total
energy consumption. On the other hand, the
models trained with hard-to-easy and hard-easy-hard curricula perform
similarly, and even worse, compared to the model trained without \ac{cl}.
To further compare the models' energy-saving performance, we examine
the total wasted energy as depicted in Figure~\ref{fig:multiple_figures} (b).
As shown by the figure, we can see that the difference in total energy consumption
between the models is mainly attributed to the difference in total wasted energy
because the difference in total wasted energy resembles the difference in total energy
consumption. Therefore, we can conclude that the agent trained with sampled-real-synthetic curriculum is the best in terms of energy-saving performance.


% %As explained in Subsection \ref{drlhpc}, 
% % Wasted energy refers to the energy used by nodes during idle, switching on, and switching off states. 
% % Table \ref{tab:totalWastedCur} and 
% Figure \ref{fig:totalWastedCur} shows the comparison of total wasted energy across curricula. The Hard-Easy-Hard curriculum (bar with yellow shades) has the worst performance compared to the other curricula. The Hard to Easy curriculum (bar with red shades) ranks second, with the Real-Synthetic-Sample sequence performing better than the Synthetic-Real-Sample sequence. The no-CL Agent training outperforms the four previous curriculum arrangements. Lastly, the Easy to Hard curriculum (bar with green shades) performs better than the others, with the Sample-Real-Synthetic sequence significantly outperforming the Sample-Synthetic-Real sequence.

% \begin{table}[t]
% \caption{Total energy wastage across curricula.} 
% \label{tab:totalWastedCur}
% \begin{tabular}{lr} 
% \toprule
% \textbf{Curriculum}    & \textbf{Total Energy Wastage (Joules)}  \\ 
% \midrule
% Sample-Real-Synthetic & 21,214,388,566 \\ 
% Sample-Synthetic-Real & 21,757,725,352 \\ 
% Real-Synthetic-Sample & 22,123,444,728 \\ 
% Synthetic-Real-Sample & 22,795,569,448 \\ 
% Synthetic-Sample-Real & 22,784,644,296 \\ 
% Real-Sample-Synthetic & 22,810,205,884 \\ 
% Without Curriculum    & 22,037,255,928 \\
% \bottomrule
% \end{tabular}
% \end{table}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{Hasil New/Total PemborosanEnergi Vs Kurikulum New.pdf}
%     \caption{Comparison of total wasted energy across curricula.}
%     \label{fig:totalWastedCur}
% \end{figure}

In terms of \ac{qos}, i.e., the average job waiting time, the models'
performance is depicted in Figure~\ref{fig:multiple_figures} (c).
The figure demonstrates the consistency of the easy-to-hard curriculum in outperforming other curricula, reducing waiting time by 43.84\% to 44.75\% compared to the no-CL agent configuration. The sampled-real-synthetic sequence consistently outperforms the sampled-synthetic-real sequence by a small margin, making the model trained with the sampled-real-synthetic curriculum the Best Agent in this study.

% \begin{table}[t]
% \caption{Average job waiting time across curricula.} 
% \label{tab:averageWaitingCur}
% \begin{tabular}{lr}
%     \toprule
%     \textbf{Curriculum}    & \textbf{Average Job Waiting Time (seconds)} \\
%     \midrule
%     Sample-Real-Synthetic & 2,735.98 \\
%     Sample-Synthetic-Real & 2,780.60 \\
%     Real-Synthetic-Sample & 3,852.42 \\
%     Synthetic-Real-Sample & 6,488.44 \\
%     Synthetic-Sample-Real & 4,530.35 \\
%     Real-Sample-Synthetic & 6,488.15 \\
%     \bottomrule
% \end{tabular}
% \end{table}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{Rerata Job Waiting Time Vs Kurikulum.pdf}
%     \caption{Comparison of average job waiting time across curricula.}
%     \label{fig:averageWaitingCur}
% \end{figure}

\subsection{Comparison of the Best Agent with the no-CL agent and other timeout policies}
\label{sec:comparisonagent}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Total-Pemborosan-Energi-Vs-Konfigurasi-New.pdf}
    \caption{Comparison of total wasted energy for the Best Agent, the no-CL agent, and various timeout policy methods.}
    \label{fig:totalWastedConf}
\end{figure}



This section compares the Best Agent with the no-CL agent and the timeout policy methods. Figure~\ref{fig:totalWastedConf} shows how much total energy is wasted under each approach. From the figure, we can see that the optimal timeout policy configuration is 15 minutes. Our Best Agent achieves a 3.73\% reduction in wasted energy compared to the no-CL agent and a 4.66\% reduction compared to the optimal timeout policy. Although the difference may seem modest in the figure, the Best Agent still saves approximately 800 million Joules more than the no-CL agent—an amount that can translate into substantial cost savings.

%This section compares the Best Agent with the timeout policy methods and the no-CL Agent. Figure~\ref{fig:totalWastedConf} shows the comparison of total wasted energy. From this figure, it is clear that the optimal configuration for the timeout policy method is 15 minutes. Our Best Agent achieved a 3.73\% reduction in wasted energy compared to the no-CL Agent and a 4.66\% reduction compared to the optimal timeout policy configuration. Although the difference appears small in the figure, the Best Agent saved approximately 800 million Joules compared to the no-CL Agent. This is a substantial amount of energy, potentially resulting in significant cost savings.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Rerata-Job-Waiting-Time-Vs-Konfigurasi.pdf}
    \caption{Comparison of average job waiting time for the Best Agent, the no-CL agent, and various timeout policy methods. The orange line indicates the average job waiting time of the Best Agent.}
    \label{fig:averageWaitingConf}
\end{figure}

The Best Agent's advantages extend beyond reducing wasted energy. As shown in Figure~\ref{fig:averageWaitingConf}, the Best Agent achieves a 13.02\% shorter job waiting time compared to the optimal 15-minute timeout policy
%and the no-CL a significantly greater 44.74\% reduction compared to the no-CL Agent.

%The Best Agent’s advantages extend beyond reducing wasted energy. In the average job waiting time metric shown in Figure~\ref{fig:averageWaitingConf}, the Best Agent outperforms the optimal timeout policy (15-minute) configuration by 13.02\%, reaching a level of performance the no-CL Agent could not match.

%The superiority of the Best Agent is not limited to wasted energy. In the average job waiting time metric shown in Figure~\ref{fig:averageWaitingConf}, the Best Agent outperforms the optimal timeout policy configuration by 9.24\%, a level of performance that was not achieved by the no-CL Agent. 
%Detailed values for each timeout policy configuration can be found in Appendix \ref{lampiranTimeout}.

To further evaluate the performance of the Best Agent, we analyze the job-filling rate $\eta$ \cite{shoji2017lessons} defined by:
\begin{equation}
\label{jobfillingrate}
    \eta = \frac{t_{compute}}{t_{idle} + t_{compute}},
\end{equation}
where $t_{compute}$ is the total amount of time all nodes spend performing computations, and $t_{idle}$ is the total amount of time all nodes are powered on but not executing any job. This metric reflects how effectively the system is utilized when nodes are active: if a node is powered on but remains idle, the job-filling rate decreases. Figure \ref{fig:JobFillingRate} shows the job-filling rate for each strategy.

In the baseline setting, where nodes remain powered on at all times, the job-filling rate is approximately 48.17\%. This indicates that without turning off idle nodes, the system operates inefficiently. As shown in Figure \ref{fig:JobFillingRate}, our Best Agent and the no-CL agent achieve a notably higher job-filling rate compared to the other policies, surpassing the baseline by 32.70\% and marginally outperforming the optimal 15-minute timeout policy by about 2\%. These results emphasize the effectiveness of RL in minimizing idle time relative to the required computing time, thereby improving overall system utilization. However, as explained in Sec.~\ref{sec:comparisonagent}, our Best Agent still performs better than the no-CL agent and the 15-minute timeout policy in reducing wasted energy and job waiting time.
%From this figure, it can be seen that our Best Agent, together with the no-CL Agent, achieves a high job-filling rate compared to the other policies, greatly outperforming the baseline by 32.70\% and slightly better than the 10-minutes timeout policy by approximately 1\%, indicating the outstanding capability of RL in minimizing idle time relative to the required compute time. However, as explained in Sec.~\ref{sec:comparisonagent}, our Best Agent is still much better than the no-CL Agent and the 10-minutes timeout policy in terms of wasted energy and job waiting time.%Outperforming both in by X\% and Y\%, respectively, terms of wasted energy, and in terms

%is the total amount of time all nodes spend being active but not executing jobs. The job-filling rate describes.... Fig.~\ref{fig:JobFillingRate} shows the job-filling rate of each of the strategies. It can be seen that our agent achieves the best job-filling rate out of all strategies. This shows that our agent achieves the lowest idle time proportional to the required computing time.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{all_results_Job-filling-rate.pdf}
\caption{The job-filling rate of each strategy on the dataset. The red line indicates the job-filling rate of the Best Agent.}
\label{fig:JobFillingRate}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{all_results_Number-of-switching-off.pdf}
\caption{The number of node shutdowns using each strategy. The red line indicates the number of nodes shutdown by the Best Agent.}
\label{fig:SwitchOffCount}
\end{figure}


% \subsubsection{Switch-off Count}

Finally, we evaluate how effectively each strategy manages node power transitions by analyzing the number of node shutdowns. Figure~\ref{fig:SwitchOffCount} illustrates the number of times nodes are powered down under each strategy. In this figure, the number of node shutdowns performed by the Best Agent is between those of the 10- and 15-minute timeout policies, suggesting that, on average, it typically shuts down nodes after an idle interval of 10 to 15 minutes. This aligns with the observation in Figure~\ref{fig:totalWastedConf}, where the 15-minute timeout is shown to be the most energy-efficient among the other timeout policies, while energy consumption starts to increase again with the 10-minute policy. Although the Best Agent still performs shutdowns at such relatively short intervals, its ability to dynamically adjust timing allows it to not only improve energy efficiency but also to help maintain acceptable job waiting times, ensuring a better overall QoS.
%%achieve a better balance between energy savings and job waiting time compared to the static timeout policies. As a result, our agent


%Finally, we assess how effectively each strategy controls node power transitions by analyzing the frequency of node shutdowns. Figure~\ref{fig:SwitchOffCount} presents the number of times nodes are powered down under each strategy.
%Compared to the no-CL Agent and the 5- or 10-minute timeout policies, our agent switches nodes off less frequently, demonstrating its ability to determine the optimal timing for powering nodes on or off.
%The number of shutdowns performed by the Best Agent falls between the 10-minute and 15-minute timeout policies, indicating that it tends to shut down nodes with an average node idle interval between 10 and 15 minutes. This aligns with the observation in Figure~\ref{fig:totalWastedConf}, where 15-minute proofed to be the most energy-efficient policy among other timeout policies, and energy consumption starts increasing again with the 10-minute policy. Although the Best Agent still performs shutdowns at intervals averaging below 15 minutes, its ability to dynamically adjust timing allows it to balance energy savings and job scheduling efficiency more effectively than static timeout policies. Thus, our agent not only enhances energy savings but also helps maintain acceptable job waiting times, thereby preserving QoS.

%Finally, we examine how each strategy manages node power transitions by evaluating the number of node shutdowns. Figure~\ref{fig:SwitchOffCount} presents the number of times nodes are shutdown under each strategy. Compared to the no-CL Agent and the 5- or 10-minute timeout policies, our agent switches nodes off less frequently, demonstrating its ability to determine the optimal timing for powering nodes on or off. This contrasts with the timeout-based approaches, which may shut nodes down prematurely or unnecessarily. Thus, our agent not only enhances energy savings but also helps maintain acceptable job waiting times, thereby preserving QoS.

%Fig.~\ref{fig:SwitchOffCount} presents the number of times nodes are powered down by each strategy on the dataset. Compared to the no-CL Agent and the 5- or 10-minute timeout policies, our agent switches nodes off less frequently, demonstrating that it carefully identifies the optimal timing for shutting down and powering up nodes. This contrasts with a straightforward timeout approach, which may shut nodes down prematurely or unnecessarily. Thus, our agent is not only able to improve energy saving but also helps maintain acceptable job waiting times, thereby preserving QoS.




\subsection{Comparison of impact on job scheduling}

In this section, we compare
the impact of the Best Agent, no-CL agent, and 15-minute timeout policy on job scheduling-related metrics:  
% we evaluate the impact of the methods on job scheduling
% in the \ac{hpc} system. 
% The related evaluation metrics used are as follows:
\begin{enumerate} 
%\item Average waiting time 
\item Maximum waiting time: The longest time a job waits in the queue. 
\item Average response time: The average time between the job being submitted and completed. 
\item Average slowdown: The average ratio of job response time to the actual time the job is executed. 
\item System utilization: Equivalent to job-filling rate. 
\end{enumerate}
% Based on the previous discussion, t
% This section compares 
% Figure~\ref{fig:schedulingPerform} 
% compares five metrics previously described in Sec.~\ref{sec:evaluation}
Figure~\ref{fig:schedulingPerform}  shows inverse of the metrics, except for system utilization, and then normalizing them. The larger the area of the graph, the better the overall performance.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Radar-chart.pdf}
    \caption{Comparison of impact on scheduling performance.}
    \label{fig:schedulingPerform}
\end{figure}

% In this aspect, The Best Agent also performs better than the other two. 
%Interestingly, we can see that the no-CL Agent has slightly better system utilization compared to the 15-minute timeout policy. However, it is outperformed in all other metrics. On the other hand, the Best Agent performs better compared to the other two methods, especially in terms of job waiting time and response time. Therefore, these results demonstrate the ability of the Best Agent to efficiently switch nodes on or off at the optimal time, optimizing the energy-saving potential while maintaining the \ac{qos} and the job scheduling performance of the \ac{hpc} system.

% Interestingly, t
The no-CL agent achieves slightly better system utilization than the 15-minute timeout policy, which corresponds to the job-filling rate rankings in Figure \ref{fig:JobFillingRate}. However, it lags behind in all other performance metrics. On the other hand, the Best Agent outperforms both methods, on all metrics.
% particularly in terms of job waiting time and response time. 
% Thus, these findings highlight the Best Agent’s ability to optimally switch nodes on or off, maximizing energy-saving potential while preserving the \ac{qos} and overall job scheduling efficiency of the \ac{hpc} system.
emphasizing the overall superiority of the Best Agent. 

%\sanedited{This whole subsection onwards}

%\subsubsection{Job-filling rate}



%Fig.~\ref{fig:SwitchOffCount} shows the total number of times nodes are switched-off using each strategy on the dataset. We can see that our agent switches nodes off less than the no-CL Agent, as well as the 5 and 10-minute timeout policies. This indicates that our agent attempts to minimize energy consumption while still trying to maintain a reasonable job waiting time, hence the QoS.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{All.pdf}
    \caption{Comparison of average job waiting time and total wasted energy for the Best Agent, the no-CL agent, and two best timeout policies on many variations of switch-on/off duration, sleep/active power consumption, and number of nodes.}
    \label{fig:var}
\end{figure*}

\subsection{Sensitivity study}
%\sanedited{This whole subsection}

We conduct a sensitivity analysis to evaluate how well our model, trained under a specific parameter environment, generalizes to different system configurations. We systematically vary switch-on and switch-off times, power consumption rates, and the number of nodes, respectively. When altering a specific parameter, the others remain fixed at their default values, as described in Sec.~\ref{sec:AgentTrainingAndTesting}. For switch-on/off times, we test three configurations: {30/45, 20/30, 10/15} minutes. To examine the agent's adaptability, we reduce the default 30/45 setting to 20/30 and further down to 10/15, which closely aligns with real-world values observed in Supercomputer AOBA~\cite{ohmura2022toward}. For power consumption, we evaluate two settings: {90/190, 9/100} watts. The second configuration reduces the gap between active and sleep power to test how the agent performs in systems where the power-saving potential is more limited ~\cite{rossi2015impact,georgakoudis2019evaluating}. For the number of nodes, we increase the system size across three configurations: {128, 160, 256}. This expansion in system size enlarges the action space, making decision-making more challenging for the agent and providing insight into its scalability. The results are summarized in Figure \ref{fig:var}. Within each bar group, we compare our Best Agent, the no-CL agent, and the two best-performing timeout policies in terms of wasted energy and average job waiting time for the corresponding parameter settings. We select the two best-performing timeout policies for comparison because their performance fluctuates across different parameter settings.
%We choose two-best performing policies because there are some fluctuations on the performance of the timeout policy-based methods.

Figure \ref{fig:var}~(a) and (b) show that the no-CL agent exhibits highly fluctuating performance across different switch-on and switch-off durations. Its occasional promising results appear to be more a matter of luck rather than an indication of true generalization across various scenarios. In contrast, our Best Agent consistently achieves the lowest average job waiting time while maintaining stable performance. In terms of total wasted energy, it performs comparably to the 5-minute timeout policy at switch-on/off times of 10/15 but surpasses all other methods at higher values. This highlights the advantage of CL in enhancing both consistency and generalization across different system configurations.

%We conduct a sensitivity analysis to evaluate how well our model, trained under a specific parameter environment, generalizes to different system configurations. In Figures~\ref{fig:var-s},~\ref{fig:var-w}, and~\ref{fig:var-n}, we systematically vary switch-on and switch-off times, power consumption rates, and the number of nodes, respectively. When altering a specific parameter, the others remain fixed at their default values, as described in Section~\ref{sec:AgentTrainingAndTesting}. The chosen parameter values reflect real-world systems: switch-on/switch-off times \{10/15, 20/30, 30/45\}~\cite{ohmura2022toward}, active/sleep power consumption \{9/100, 90/190\} watts~\cite{rossi2015impact,georgakoudis2019evaluating}, and node counts \{128, 144, 160, 196, 256\}. Within each bar group, we compare the two best-performing policies based on wasted energy for the corresponding parameter settings.

%Figure~\ref{fig:var}~(a) shows that in terms of total wasted energy, our agent performs comparably to the 5-minute timeout policy at switch-on/off times = 10/15 but surpasses all other methods at higher values. Additionally, it achieves the lowest average job waiting time across different switch-on and switch-off durations.
%Figure~\ref{fig:var-s} illustrates that our agent achieves the lowest average waiting time across various switch-on and switch-off durations. In terms of total wasted energy, it performs comparably to the 5-minute timeout policy at 10/15 but surpasses all other policies at higher values.

Figure \ref{fig:var} (c) and (d) highlights once again the inconsistency of the no-CL agent. It performs well in the 9/190 setting, which matches its training environment, but becomes overly aggressive in energy saving at 9/100 power setting. This is evident from its significant reduction in wasted energy, which comes at the cost of an excessive increase in waiting time. These findings indicate that the no-CL agent lacks adaptability and requires retraining to perform well under different conditions—a limitation not observed in our Best Agent. In contrast, our agent significantly outperforms the 10-minute timeout policy in average waiting time at the 9/100 power setting and still maintains an advantage over the 15-minute timeout policy at 9/190. Moreover, it achieves slightly lower total wasted energy than the timeout-based policies. These results demonstrate that our agent remains effective across different switch-on/off times and power consumption rates without requiring retraining.

%Fig.~\ref{fig:var}~(d) demonstrates that our agent significantly outperforms the 10-minute timeout policy in average waiting time at the 9/100 watt power consumption setting and still maintains an advantage over the 15-minute timeout policy at 9/190. Moreover, Fig.~\ref{fig:var}~(c) shows that it achieves slightly lower total wasted energy than the timeout-based policies. These results indicate that our agent remains effective across different switch-on/off times and power consumption rates without requiring retraining.

Figure \ref{fig:var} (e) and (f) reveal that our agent consistently outperforms all other methods in terms of job waiting time while remaining competitive in energy efficiency. Although it is not the most energy-efficient approach, it still achieves reasonable energy savings without compromising the job waiting time. This is an arguably remarkable result, considering that the proposed method was trained for only ten epochs. With additional training or fine-tuning, it is likely that the agent could achieve even better performance, further optimizing both energy efficiency and job scheduling in different node configurations

%Fig.~\ref{fig:var}~(e) reveals that our agent still maintain competitive energy efficiency and job waiting time when the system comprises 128, 160, or 196 nodes.
%However, for configuration with 144 or 256 nodes, it does not rank among the top two in terms of minimizing wasted energy.
%This suggests that additional training or fine-tuning is necessary to enhance performance for specific node counts.


% We provide a sensitivity study in regards to the simulated system's parameters to see how far can our model which was trained at a specific parameter environment can generalize to other systems. In Figures.~\ref{fig:var-s},~\ref{fig:var-w}, and~\ref{fig:var-n}, we vary the switch-on and switch-off times, power consumptions, and the number of nodes, respectively. When varying some variables, the unvarying variables are set as their default value, as described in \ref{sec:AgentTrainingAndTesting}. The varying values are set to correspond to real-world systems: \{10/15, 20/30, 30/45\} for switch-on/switch-off times \cite{ohmura2022toward}, \{9/100, 90/190\} for active/sleep power consumptions \cite{rossi2015impact,georgakoudis2019evaluating}, and \{128, 144, 160, 196, 256\} for the number of nodes \cite{cao2024novel,andujar2023energy}. In each bar group, we compare the two best policies based on wasted energy for the corresponding parameter values. 

% Fig.~\ref{fig:var-s} shows that our agent performs the best on average waiting time at various values of switch-on and switch-off time. It also performs well in terms of total wasted energy, being only slightly worse than the 5-minute timeout policy at 10/15 switch-on and switch-off time, while being the best at the other values of the parameters. 

% Fig.~\ref{fig:var-w} shows that our agent performs significantly better than the 10-minute timeout policy in terms of average waiting time at the 9/100 watts sleep and active power consumption rate values, and still performs better than the 15-minute timeout policy at 9/190. Meanwhile, our agent achieves slightly lower total wasted energy than the timeout policies. Overall, the agent can somewhat be used in various different scenarios of switch-on and switch-off times, as well as power consumption rate without the need of re-training.  

% Fig.~\ref{fig:var-n} shows that our agent achieves competitive values of wasted energy only on systems where the number of nodes are 128, 160, or 196. For cases where the number of nodes are 144 and 256, our agent did not achieve top-2 in terms of least wasted energy. The results show that further training or re-training for specific number of nodes is required to improve the agent's performance. 



% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{varying_soff_son_Average waiting time (second).pdf}
%     \includegraphics[width=\linewidth]{varying_soff_son_Total wasted energy (Joule).pdf}
%     \caption{Comparison of average job waiting time and total wasted energy for the two best policies on varying switch-on and switch-off durations.}
%     \label{fig:var-s}
% \end{figure}


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{varying_wattage_Average waiting time (second).pdf}
%     \includegraphics[width=\linewidth]{varying_wattage_Total wasted energy (Joule).pdf}
%     \caption{Comparison of average job waiting time and total wasted energy for the two best policies on varying sleep and active power consumption rates.}
%     \label{fig:var-w}
% \end{figure}


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{varying_num_nodes_256_Average waiting time (second).pdf}
%     \includegraphics[width=\linewidth]{varying_num_nodes_256_Total wasted energy (Joule).pdf}
%     \caption{Comparison of average job waiting time and total wasted energy for the two best policies on varying number of nodes.}
%     \label{fig:var-n}
% \end{figure}


