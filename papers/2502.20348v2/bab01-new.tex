\section{Introduction}

High energy usage is a major issue in operating \ac{hpc} systems \citep{Dayarathna,Amrizal}.
%The primary cause of this energy consumption is that nodes in an HPC system configuration remain in standby mode, ready to immediately accept jobs from users.
To maintain good \ac{qos}, the nodes of an \ac{hpc} system usually remain on standby mode so that the system is ready to immediately execute incoming jobs \citep{Bridi}. Even in standby mode, these nodes consume substantial energy \citep{Barroso}. As a result, the total energy consumption of the system remains high. Moreover, \ac{hpc} systems often consist of hundreds or even thousands of such nodes, further compounding the overall energy consumption.
%exacerbated by the fact that HPC systems often consist of hundreds or thousands of nodes.

%Nodes can be turned off if they have been in standby mode for a certain period of time, referred to as a timeout policy in this study. However, turning nodes off at inappropriate times can delay job execution. (need extra explanation). A challenge with this approach is determining the appropriate timeout duration due to the dynamic arrival of jobs.
%Timeout policy can save energy with a slight compromise on job queue waiting time, provided the timeout duration is chosen carefully.

Nodes can be turned off if they have been in standby mode for a certain period of time \citep{Hikita,Pinheiro,Chen}. This study refers to this method as a \emph{timeout policy}. However, turning off nodes at inappropriate times can delay job execution. For example, if some nodes are turned off and a new job arrives shortly afterward, enough active nodes may not be available to execute the job immediately. In such cases, the turned-off nodes must be powered back on, which introduces additional delays due to the time required for the nodes to become operational again. To make matters worse, waking up or turning off nodes in large-scale \ac{hpc} systems takes significantly longer than in regular computers; in some cases, it can take tens of minutes \cite{Ohmura}. This prolonged transition time is mainly due to the complexity of \ac{hpc} environments, where turning on/off the nodes involves intricate power sequencing, GPU initialization, parallel file system recovery, and network reconfiguration, all of which contribute to substantial delays \citep{agung2017memory, ahmad2017design, amrizal2012improving, moran2024exploring, ottaviano2024controlpulp}. As job arrivals are dynamic and unpredictable, setting the timeout policy to a fixed time might not be the best solution, and, hence, accurately estimating the appropriate time for turning off/on nodes is crucial for efficient power management in \ac{hpc} systems.
%to efficiently manage the power state transitions is crucial to the efficiency of the FCFS + Backfilling scheduling method. This makes the decision on the appropriate timeout duration particularly challenging, as job arrivals are dynamic and unpredictable.

%Nodes can be turned off if they have been in standby mode for a certain period \citep{Hikita,Pinheiro,Chen}, referred to as a timeout policy in this study. However, turning nodes off at inappropriate times can delay job execution. For example, if some nodes are turned off and a new job arrives shortly afterward, there may not be enough active nodes available to execute the job immediately. In such cases, the turned-off nodes must be turned back on, introducing additional delays due to the time required for them to become fully operational. Moreover, turning nodes off/on in large-scale HPC systems is far more time-consuming than in regular computers, often taking 30–60 minutes \citep{Ohmura}. Given the dynamic and unpredictable nature of job arrivals, determining the optimal timing for turning nodes off/on remains a significant challenge for efficient power management of HPC systems.

%One strategy to improve energy efficiency in HPC systems involves combining FCFS (First-Come, First-Served) scheduling with Backfilling \cite{Fan}. In FCFS scheduling, jobs are allocated based on their order in the queue, while Backfilling allows jobs later in the queue to bypass earlier jobs, provided this does not delay the allocation of the first job. Combining FCFS and Backfilling not only reduces the average waiting time for jobs in the queue but also optimizes energy usage by incorporating power state transitions in nodes.

%Each node has multiple power states, such as active and sleep. Significant energy savings can be achieved by transitioning idle nodes (nodes that are active but not processing jobs) to a sleep state after being idle for a specified timeout duration \citep{Hikita,Pinheiro,Chen}. However, transitioning a node to sleep mode at the wrong time can compromise scheduling efficiency. For instance, when a job that could have been backfilled arrives shortly after a node enters sleep mode, it cannot be executed immediately due to insufficient active nodes. Moreover, waking up or turning off nodes in large-scale HPC systems takes significantly longer than in regular computers, often requiring 30–60 minutes \cite{Ohmura}. Therefore, accurately estimating the appropriate timing for power state transitions is crucial to the efficiency of the FCFS + Backfilling scheduling method.

\Ac{rl}, a subfield of \ac{ml}, is particularly well suited for prediction in dynamic environments \cite{Sutton}. The combination of \ac{rl} with \ac{dnn}, known as \ac{drl}, has further improved its effectiveness \citep{lavet}, increasing its popularity for predicting incoming jobs and has been integrated with the job scheduler of \ac{hpc} systems in some literature. Kumar et al. \cite{Kumar} designed a scheduler leveraging \ac{drl} to effectively reduce the average job waiting time. Liang et al. \cite{Liang} introduced a \ac{drl}-based model that not only minimizes the average job waiting time but also promotes fairness between large and small jobs, preventing starvation. Similarly, Fan et al. \cite{Fan} developed a two-level hierarchical neural network-based \ac{drl} agent for handling \ac{fcfs} scheduling with backfilling that can adapt to changes in workload without human intervention. Despite their success, none of these studies has explicitly considered the energy efficiency of \ac{hpc} systems. In the domain of energy efficiency, Khasyah et al. \cite{Fitra} applied \ac{drl} to decide when to turn off standby/idle nodes. Their model, trained with the \ac{a2c} algorithm \cite{Sutton}, demonstrated energy savings in \ac{hpc} systems, outperforming most of the fixed-time timeout policies except the extremely short one, i.e., turning off a node every five minutes of idle time, which is highly impractical in real-case scenarios. However, the \ac{a2c} agent in their study significantly increased job waiting time, thus reducing the system's \ac{qos}. Hence, there is a need for improved methods to enhance the model's performance.

%Machine Learning (ML) can be used to predict the optimal timings for turning nodes off/on or off in HPC systems. This can be achieved by predicting job arrival dynamics. A particularly promising subfield of ML for this purpose is Reinforcement Learning (RL) \citep{Sutton}. RL is an ML technique that enables an agent to learn optimal policies through interactions with its environment. In dynamic environments like HPC systems, RL can model job arrivals and power state transitions, using a framework such as the Markov Decision Process (MDP). The use of Deep Neural Networks (DNNs) alongside RL, known as Deep Reinforcement Learning (DRL), has further enhanced the effectiveness of these approaches \citep{lavet}.

%\todo{selain itu, drl juga bisa dipakai di scheduling, sitasi2.} \edited{<Mengikuti paper Fitra> Kumar et al. \cite{Kumar} designed a scheduler leveraging DRL to effectively reduce the average job waiting time in the system. Similarly, Liang et al. \cite{Liang} introduced a DRL-based model that not only minimizes the average job waiting time but also promotes fairness between large and small jobs, preventing starvation.} 
%In the domain of energy efficiency, \cite{Fitra} applied DRL to turn off standby nodes at optimal times. Their model, trained with the Advantage Actor-Critic (A2C) algorithm, demonstrated energy savings in HPC systems while maintaining acceptable job queue waiting times. However, the A2C agent in their study significantly increased queue waiting times, thereby reducing the system’s Quality of Service. Hence, there is a need for improved methods to enhance the model’s performance.

In this study, we extend the work of Khasyah et al. and propose an integration of \ac{cl} \citep{Bengio} to the \ac{drl} agent to further improve its performance, balancing energy savings and the system's \ac{qos}. \ac{cl} is one of the training methods to improve the performance of DRL agents/models. It mimics human learning by training models with problems that gradually increase in complexity. For instance, a model is first trained with a low-difficulty, then with a medium-difficulty, and finally with high-difficulty datasets. The sequence of training datasets, known as the curriculum, can accelerate convergence and improve model performance \citep{Weinshall}. This approach has also been applied successfully in several general scheduling problems \citep{mao2019learning, muller2024reinforcement}. However, its impact on \ac{hpc} energy management systems remains unexplored. Consequently, a knowledge gap exists in understanding how CL can improve the performance of a \ac{drl} agent deployed to save energy in HPC systems. Our main goals are twofold:
%enhancement to the energy efficiency of deep learning agents for managing node power states by integrating Curriculum Learning (CL). Our goals are:  
(1) to improve energy efficiency beyond the existing approach when deploying a \ac{drl} agent to an \ac{hpc} system and (2) to reduce job waiting time.  
In this paper, we incorporate CL with different training strategies across datasets of varying difficulty levels to determine the optimal training sequence. Our contributions are:  
\begin{enumerate}
    \item Developing a systematic approach to generate datasets for training the energy-saving \ac{drl} agent.
    \item Investigating the best curriculum for training the agent based on the generated datasets.
    \item Demonstrating the impact of \ac{cl} on energy reduction and improved \ac{qos} in HPC systems through extensive simulations.
\end{enumerate}

%\todo{tuliskan paragraf penutup yg isinya the rest of the paper is organized as follows.}

%\todo{revamp this paragraph based on the reorganization of the later sections.}
The rest of this paper is organized as follows. Sec.~\ref{sec:Background} discusses a more detailed background and related work of the paper. In Sec.~\ref{sec:ProposedMethod}, we explain the workflow of our proposed methodology, from the methods to generate the datasets for \ac{cl} up to training and testing the \ac{drl} agent.
%training and testing processes, as well as the performance metrics of interest.
Sec.~\ref{sec:ExperimentalSettings} explains the experimental settings of this work, which results are interpreted in Sec.~\ref{sec:Results}. Lastly, the conclusions and future work are stated in Sec.~\ref{sec:Conclusions}.


%\todo{Remove this paragraph? Move it to Section 2?}
%One effective method to improve model performance is CL. CL mimics human learning by training models with problems that gradually increase in complexity \citep{Bengio}. For instance, a model is first trained with a low-difficulty dataset, then with medium-difficulty, and finally with a high-difficulty dataset. The sequence of training datasets, known as the curriculum, can accelerate convergence and improve model performance \citep{Weinshall}. This approach has also been applied successfully in job scheduling problems \citep{waubert}.

%\todo{Remove this paragraph? Move it to Section 2?}
%While DRL has proven effective for various HPC scheduling problems \cite{Mao,Zhang}, applying CL in the context of energy efficiency for HPC systems remains unexplored. This study aims to address this gap, demonstrating that CL can significantly enhance DRL models for power management in HPC systems.


%\subsection{Batasan Masalah}
%\textcolor{red}{maybe dipakai (commented out)}
% Adapun beberapa batasan masalah pada penelitian ini antara lain:
% \begin{enumerate}
%     \item \textit{Node} pada sistem HPC memiliki empat \textit{power state} yaitu aktif, \textit{switching on}, \textit{switching off}, dan \textit{sleep}.
%     \item Penggunaan energi \textit{node} aktif ketika sedang mengeksekusi \textit{job} dan tidak mengeksekusi \textit{job} adalah sama.
%     \item Waktu eksekusi \textit{job} sama dengan waktu yang diminta oleh \textit{user}.
% \end{enumerate}