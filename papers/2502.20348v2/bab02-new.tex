\section{Background}\label{sec:Background}

\subsection{Reinforcement learning}
\ac{rl} is a category within the domain of \ac{ml} that aims to solve problems involving sequential decision-making. When decisions made in earlier stages influence decisions in subsequent stages within an environment, \ac{rl} can be employed to achieve optimal decisions in a continuous manner~\citep{Sutton}.

\begin{figure}[t]
\includegraphics[width=\linewidth]{DiagramRL.png}
\centering
\caption{Reinforcement learning~\citep{Sutton}.}
\label{diagramRL}
\end{figure}

In \ac{rl}, an agent learns how to act in an environment to maximize the expected return. The agent's interaction with the environment in \ac{rl} is illustrated in \mbox{Figure~\ref{diagramRL}}. This interaction of an agent and the environment is formulated as a \ac{mdp}. An \ac{mdp} can be described by a tuple $\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, R, \gamma \rangle$, where:
\begin{enumerate}
    \item $\mathcal{S}$ is the set of all possible states.
    \item $\mathcal{A}$ is the set of all possible actions.
    \item $\mathcal{P}$ is the transition probability of the environment's state.
    \item $R$ is the reward function.
    \item $\gamma$ is the reward discount factor.
\end{enumerate}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{psmp.pdf}
    \caption{A visualization of the decision-making process in \ac{psmp}.}
    \label{fig:psmp}
\end{figure*}

At timestep $t$, the agent observes the current state of the environment $s_t \in \mathcal{S}$, then decides to take action $a_t \in \mathcal{A}$. The agent takes an action based on the policy $a_t \sim \pi(\cdot\mid s_t, \ldots, s_1)$. Afterward, the environment progresses, returning the state of the next timestep, $s_{t+1}$, and the reward for the agent $r_{t+1}=R(a_t, s_t)$. The primary objective of \ac{mdp} is to find the policy that maximizes the expected return of the environment, $\mathbb{E}_{\pi}[G_t]$, where $G_t$ is given by:
\begin{equation}
    \label{eq:NCF}
        G_t = \sum_{i=0}^{T-t} \gamma^{i}r_{t+i},
\end{equation}
where $T$ is the last timestep and $\gamma \in [0,1]$. The best policy is then denoted as $\pi_* = \underset{\pi}{\arg\max} \; \mathbb{E}_{\pi}[G_0]$.
% The agent observes the 
% environment and its state, then makes decisions by performing an action. After the 
% action is performed, the agent receives a reward, which serves as a factor for 
% determining the next action. The agent's learning process can experience delays, for 
% example, when the reward is only received after several actions, requiring the agent to 
% explore various actions to achieve optimal learning.




% The reward plays a crucial role as it is directly related to the agent's objectives. Each time the agent performs an action, the environment provides a reward $r_t$. The agent's primary objective is to maximize the discounted cumulative reward from timestep $t$ to $T$, which is represented as follows:
% \begin{equation}
%     \label{eq:NCF}
%         R_t = \sum_{i=t}^T \gamma^{i-t}r_i,
% \end{equation}
% where the discount factor $\gamma$ ranges between 0 and 1. A larger $\gamma$ makes future rewards more valuable than past rewards.

\ac{drl} is a field that combines deep learning and \ac{rl} \citep{Mnih2015}. Deep learning involves the use of \ac{dnn} to handle complex problems. In \ac{drl}, \ac{dnn}s can be used for representing parts of the agent's policy or embedding the environment state. Various \ac{drl} algorithms have been recently developed and widely used~\cite{Sutton,Mnih,Mnih2015,Schulman17}. One of those is \ac{a2c}, which is a \ac{drl} algorithm that combines two types of models: the actor and the critic. The actor is the part of the agent that makes rules, while the critic provides feedback to improve those rules. \ac{a2c} uses a more accurate estimation of the advantage function to provide feedback to the actor \citep{Mnih}. Specifically, it employs a bootstrapped estimate of the action value rather than relying on the full return, balancing computational efficiency and learning stability. The training steps for \ac{a2c} are as follows:
\begin{enumerate}
    \item Initialize the actor and critic networks with random weights.
    \item Start an epoch with the initial environment and state.
    \item Take and execute an action.
    \item The critic network predicts the advantage value of the performed action, which is the difference between the received reward and the expected reward value of the current state.
    \item Update the weights of the critic network using backpropagation on the critic loss function.
    \item Update the weights of the actor using backpropagation on the actor objective function, which is the average log-likelihood of the actor policy distribution and advantage for each timestep $t$.
    \item Repeat steps 2--6 until convergence or for a specified number of epochs.
\end{enumerate}


% The two main components of \ac{a2c} are the Actor and the Critic.
% The parameters of the Actor and the critic networks are denoted
% as $\Vec{theta}$ and $\Vec{theta}_v$, respectively.
% The Actor's parameter updates are defined as:
% \begin{equation}
% d\Vec{\theta} = \nabla_{\Vec{\theta}} \sum^{N_b}_{k=1} \log \pi (\Vec{a}_k \mid s_k ; \Vec{\theta})A(s_k , \Vec{a}_k; \Vec{\theta}, \Vec{\theta}_v).
%     \label{eq:actor}
% \end{equation}
% The Critic's parameter updates are defined as:
% \begin{equation}
% d\Vec{\theta} = \nabla_{\Vec{\theta}_v} \frac{1}{N_b} \sum^{N_b}_{k=1}(A(s_k;\Vec{a}_k;\Vec{\theta},\Vec{\theta}_v - V(s_k;\Vec{\theta}_v))^2,
%     \label{eq:critic}
% \end{equation}
% where $N_b$ is the number of experiences (the pair of state, action and reward)
% collected to update the parameters.


% \edited{mentioned DRL in the previous paragraph}
% \todo{paragraf ini terlalu mendadak dan kurang smooth transisinya, mungkin bisa dimulai dengan sedikit pendahuluan: ada banyak algortima pada RL dan salah satunya yg cukup populer yaitu A2C.} 
% \edited{There are numerous \ac{drl} algorithms that have been developed recently. One such algorithm, namely \ac{a2c},}
% is a \ac{drl} algorithm that combines elements of two types of models: the actor and the critic. The actor is the part of the agent that makes rules, while the critic provides feedback to improve those rules. \ac{a2c} introduces several innovations, such as the use of parallelism to collect data and better estimation of advantage to provide feedback to the actor \citep{Mnih}. The training steps for \ac{a2c} are as follows:
% \begin{enumerate}
%     \item Initialize the actor and critic networks with random weights.
%     \item Start an epoch with the initial environment and state.
%     \item Take and execute an action.
%     \item The critic network predicts the advantage value of the performed action, which is the difference between the received reward and the expected reward value of the current state.
%     \item Update the weights of the critic network using backpropagation on the critic loss function.
%     \item Update the weights of the actor using backpropagation on the actor objective function, which is the average log-likelihood of the actor policy distribution and advantage for each timestep $t$.
%     \item Repeat steps 2–6 until convergence or for a specified number of epochs.
% \end{enumerate}


\subsection{Power state management problem and its DRL-based solution}
% \todo{add PSMP problem formulation briefly from Fitra's Paper}
In this section, we briefly present the formulation of a problem called \ac{psmp}, first introduced by Khasyah et al. \cite{Fitra}, along with a \ac{drl}-based method used by the authors to solve it. In \ac{psmp}, a set of computing nodes $M$ of an \ac{hpc} system handles a set of incoming jobs $N$. The incoming jobs first enter the queue before being allocated to the nodes in an \ac{fcfs} manner with backfilling \cite{jiang2009pb}. The job $i \in N$ requests $\mu_i$ nodes to be run, $\mu_i \leq M$, has $e_i$ runtime, and is submitted at time $t^{sub}_i$. A job can be allocated to several nodes, depending on the number of nodes it requests, but a node can only run one job at a time. The starting time of job $i$, $t^{start}_i$, represents the time the job is allocated to the nodes. A running job cannot be canceled, paused, or reallocated. Thus, preemptive job allocation and switching off nodes with a running job are not allowed.
% As an effort to minimize energy waste, idle nodes can be switched off to sleep.

% Each node has four possible power states: active, sleep, switching on, and switching 
% off. The power consumption for each state is represented as 
% $p_a, p_s, p_{son},$ and 
% $p_{soff}$, respectively. Furthermore, the time required for switching on and off is 
% $T_{son}$ and  $T_{soff}$, respectively. A node in the switching off state must first enter the sleep state before being able to be switched back on again, and vice versa.

Each node has four possible power states: active, sleep, switching on, and switching off. The power consumption for each state is represented as $p_1$ (active), $p_2$ (sleep), $p_3$ (switching on), and $p_4$ (switching off). Furthermore, the time required for switching on and off is $T_{son}$ and  $T_{soff}$, respectively. A node in the switching-off state must first enter the sleep state before being able to be switched back on again, and vice versa. At time $t$, the power state of node $m\in M$ is determined by the binary flags $(u_{tm1},u_{tm2},u_{tm3},u_{tm4}) \in \{0,1\}^4$. The value of $u_{tm1}$ corresponds to the active state, $u_{tm2}$ the sleep state, $u_{tm3}$ the switching on state, and $u_{tm4}$ the switching off state. The value of $u_{tmn}=1$, $1 \leq n \leq 4$, when node $m$ is in the corresponding state at time $t$, otherwise $u_{tmn}=0$.

The nodes allocation for the job $i$ is denoted by $l_{im}$, where $l_{im}=1$ if job $i$ is allocated to node $m$, and $l_{im}=0$ otherwise. For example, if job $i$ requests two nodes and is allocated to nodes $m=1$ and $m=3$, then $l_{i1}=l_{i3}=1$ and $l_{i\hat{m}}=0, \forall \hat{m}\in M\setminus\{1,3\}$. The computing state of the node $m$ is denoted as $c_{tm}$, where $c_{tm}=1$ if node $m$ is running a job at time $t$, and $c_{tm}=0$ otherwise.

The primary objective of this problem is to determine when and which nodes are switched on and off to minimize energy usage. A simple visualization of such decision-making processes is depicted in Figure~\ref{fig:psmp}. The total energy consumption (in Joules) is formulated as:
\begin{equation}
Z = \sum_{m \in M} \sum_{t=1}^T \sum_{n=1}^4 p_n u_{tmn},
\label{eq:totalPower}
\end{equation}
where $T$ is the time when the whole operation is complete, i.e., when the last job is completed, and each node is either in active or sleep state. Another metric for energy usage is \emph{wasted energy}, defined as the energy consumed when a node is not performing computations. The total wasted energy (in Joules) is formulated as:
\begin{equation}
Z_1 = \sum_{m \in M} \sum_{t=1}^T u_{tm1}(1-c_{tm})p_1 + u_{tm3}p_3 + u_{tm4}p_4.
\label{eq:totalWaste}
\end{equation}
As previously mentioned, we also have to consider the impact of the decision to switch off the nodes on the \ac{qos}, specifically the waiting time of the jobs in the queue. The average waiting time of all jobs, which can be computed when the whole operation is complete, is given by:
\begin{equation}
    Z_2=\sum_{i \in N} \frac{t^{start}_i - t^{sub}_i}{|N|}.
\label{eq:total-waiting-time}
\end{equation}

The wasted energy and waiting time are minimized without violating the constraints:
\begin{align}
    &u_{tm1} \geq l_{im}, \forall i \in N, \forall m \in M, t^{start}_i \leq t \leq t^{start}_i + e_i, \label{eq:constraint:active-computing}\\
    &\delta_{ij} (l_{im} + l_{jm}) \leq 1, \forall m \in M, \forall i, j \in N, i \neq j, \label{eq:constraint:no-double-alloc} \\
    &\sum_{n=1}^4 u_{tmn} = 1, \forall m \in M, 1 \leq t \leq T, \label{eq:constraint:single-state}\\
    &\sum{m \in M} l_{im} = \mu_{i}, \forall i \in N, \label{eq:constraint:num-req-nodes}\\
    &u_{tmn}, l_{im}, c_{tm}, \delta_{ij} \in \{0,1\}, \forall i,j \in N, \forall m \in M, 1 \leq t \leq T, 1 \leq n \leq 4. \label{eq:constraint:range}
\end{align}
Constraint \eqref{eq:constraint:active-computing} ensures that a node must be in active state when computing. Constraint \eqref{eq:constraint:no-double-alloc} ensures that a node cannot run more than one job, where $\delta_{ij}$ is a helper variable, and $\delta_{ij}=1$ if $[t^{start}_i,t^{start}_i+e_i]$ overlaps with $[t^{start}_j,t^{start}_j+e_j]$. Constraint \eqref{eq:constraint:single-state} ensures that every node can only be in one state at a time. Lastly, Constraint \eqref{eq:constraint:num-req-nodes} ensures the number of nodes allocated to job $i$ is exactly as it requests.


% The primary objective of this problem is to determine when and which nodes
% are switched on and off to minimize energy usage. The total energy consumption (in Joules) is formulated as:
% \begin{equation}
% \begin{split}
% F(U) = \sum_{m=1}^M \sum_{t=1}^T &(\llbracket u_{tm} = 0] \rrbracket p_a + \llbracket u_{tm} = 1\rrbracket p_s \\ 
% &\quad + \llbracket u_{tm} = 2\rrbracket p_{son} + \llbracket u_{tm} = 3\rrbracket p_{sof}),
% \end{split}
% \label{eq:totalPower}
% \end{equation}
% where $T$ is the time when the whole operation in PSMP is complete---when the last job is completed and all nodes are either in active or sleep state---and   
% $U = (\Vec{u_1},..., \Vec{u_T})$. The operation $\llbracket .\rrbracket $ denotes the Iverson bracket that equals 1 if the condition is satisfied, and 0 otherwise.

% Another metric for energy usage is \emph{wasted energy}, defined as the energy consumed 
% when a node is not performing computations. The total wasted energy (in Joules) is formulated as:
% \begin{equation}
% \begin{split}
% Z_1 = \sum_{m=1}^M \sum_{t=1}^T &(\llbracket u_{tm} = 0 \wedge c_{tm} = 0\rrbracket p_a \\ 
% &\quad + \llbracket u_{tm} = 2\rrbracket p_{son} + \llbracket u_{tm} = 3\rrbracket p_{sof}),
% \end{split}
% \label{eq:totalWaste}
% \end{equation}
% where $C = (\Vec{c_1},..., \Vec{c_T})$ and $c_{tm}$ represents the active state of the 
% $m$-th node. If a node is active but not performing computations (idle), $c_{tm} = 0$. 
% Conversely, if the node is doing computations, $c_{tm} = 1$.


% with $q_{ti} = 1$ if the $i$-th job is in the queue at time $t$ and $q_{ti} = 0$ otherwise.

% The energy waste and job waiting time are minimized without violating the constraints given as:
% \begin{align}
  
% \end{align}

% \begin{enumerate}
%     \item A node must always be active when a job is allocated to it
%     \item A node can only run one job at a time
% \end{enumerate}

We briefly describe the \ac{drl}-based approach to solve \ac{psmp} by first describing the \ac{mdp} formulation of \ac{psmp}. The agent is designed to perform actions to switch nodes on or off at each fixed time interval $\Delta t$, which we refer
as the \textit{action step} and denote by $k$. At the action step $k$, the agent observes the environment state $s_k \in \mathcal{S}$, and performs action $a_k \in \mathcal{A}$. Afterward, the environment progresses and returns the reward $r_{k+1}$ and the next state $s_{k+1}$. Actions are determined by a policy $\pi(\Vec{a}_k \mid s_k)$ that maximizes the expected return $\mathbb{E}_\pi[G_k]$, where $G_k$ is defined as in Eq.~\eqref{eq:NCF}.

The state $s_k$ is defined as the system features $X_k = (\Vec{x}_{k1},\ldots,\Vec{x}_{kM})$, where $\Vec{x}_{km}$ is the feature of the node $m$ before the action $\Vec{a}_k$ is executed. The actions are denoted as $\Vec{a}_k = (a_{k1},\ldots,a_{kM}) \in \mathcal{A}=\{0,1\}^M$. The value $a_{km}=0$ indicates an action of switching node $m$ off at decision step $k$, while $a_{km}=1$ indicates switching it on. The policy used for the decision-making is parametrized by a \ac{dnn} whose parameters are $\Vec{\theta}$, represented as $\pi(\Vec{a}_k \mid s_k; \Vec{\theta})$. Rewards $r_k$ are obtained based on the agent's actions $\Vec{a}_k$.

The reward function combines two components: wasted energy and job waiting time, denoted as $R_1$ and $R_2$, respectively. The $R_1$ reward is expressed as:
\begin{equation}
    R_1(k,k+1) =
    \sum_{m\in M} \sum^{t=\Delta t \times (k+1)}_{t=\Delta t \times k}   u_{tm1}(1-c_{tm})p_1 + u_{tm3}p_3 + u_{tm4}p_4,
\end{equation}
where $R_1(k,k+1)$ computes the wasted energy between the decision steps $k$ and $k+1$. The $R_2$ reward is expressed as:
\begin{equation}
    R_2(k,k+1) = \sum_{i \in N} \sum^{t=\Delta t \times (k+1)}_{t=\Delta t \times k} q_{ti},
\end{equation}
where $R_2(k,k+1)$ the total waiting time of the jobs in the queue between decision steps $k$ and $k+1$, $q_{ti}=1$ if job $i$ is still in the queue at time $t$, and $q_{ti}=0$, otherwise. Combining these components balances energy savings and job waiting time:
\begin{equation}
r_k = -\alpha \frac{R_1 (k,k+1)}{Mp_1\Delta t}- \beta \frac{R_2 (k,k+1)}{J(k,k+1)},
\label{eq:rewardGabung}
\end{equation}
where $0\leq \alpha, \beta \leq 1$, $\alpha+\beta=1$, and they determine the weights of the two reward components. $J(k, k+1)$ represents the possible maximum total job waiting time in the queue during actions $k$ and $k+1$, given by:
\begin{equation}
    J(k,k+1) = \sum_{i\in N} \min\left(1, \sum^{t=\Delta t \times (k+1)}_{t=\Delta t \times k} q_{ti} \right) \Delta t.
\end{equation}

% The two main components of \ac{a2c} are the Actor and the Critic.
% The parameters of the Actor and the critic networks are denoted
% as $\Vec{theta}$ and $\Vec{theta}_v$, respectively.
% The Actor's parameter updates are defined as:
% \begin{equation}
% d\Vec{\theta} = \nabla_{\Vec{\theta}} \sum^{N_b}_{k=1} \log \pi (\Vec{a}_k \mid s_k ; \Vec{\theta})A(s_k , \Vec{a}_k; \Vec{\theta}, \Vec{\theta}_v).
%     \label{eq:actor}
% \end{equation}
% The Critic's parameter updates are defined as:
% \begin{equation}
% d\Vec{\theta} = \nabla_{\Vec{\theta}_v} \frac{1}{N_b} \sum^{N_b}_{k=1}(A(s_k;\Vec{a}_k;\Vec{\theta},\Vec{\theta}_v - V(s_k;\Vec{\theta}_v))^2,
%     \label{eq:critic}
% \end{equation}
% where $N_b$ is the number of experiences (the pair of state, action and reward)
% collected to update the parameters.

\subsection{Curriculum learning}

\hyphenation{Basic-Shapes}
% The potential of CL in the field of ML has been explored in research by \cite{Bengio}. 
The concept of \ac{cl} and its application to improve training in \ac{ml} was first formalized by Bengio et al.~\cite{Bengio}. At its core, \ac{cl} is a strategy to train different concepts to a model at different times. The training comprises a sequence of different tasks, commonly starting with easy tasks first, followed by tasks with gradually increasing difficulty. To adopt \ac{cl}, one needs to determine how to design these tasks and their sequence in the training. In addition, one also needs to consider when to switch from one task to the next. The experimental study by Bengio et al.~\cite{Bengio} for shape recognition and language modeling has shown that models trained with \ac{cl} perform better than models trained without \ac{cl}.

% Early study of CL in ML is conducted by Bengio et al.~\cite{Bengio}.
% This study designed various curricula for different types of problems, such as shape 
% recognition and language modeling. In the shape recognition experiment, the training 
% data was divided into two sets: \textit{BasicShapes} and \textit{GeomShapes}. 
% \textit{BasicShapes} included data on squares, circles, and equilateral triangles, 
% while \textit{GeomShapes} contained data on quadrilaterals, ellipses, and triangles. 
%and models trained only with \textit{BasicShapes}
% Models without CL were shown to perform worse than models trained using CL with the order of training \textit{BasicShapes} followed by \textit{GeomShapes}. 

Due to its promising results, \ac{cl} has also been adopted to various \ac{drl} studies, e.g., in job-shop scheduling problem (JSSP)~\cite{waubert,muller2024reinforcement}, routing problems~\cite{Ma2021}, autonomous vehicles~\cite{Dinneweth2022}, scheduling in data processing clusters~\cite{mao2019learning}, and job scheduling in \ac{hpc}~\cite{Fan}. Interested readers are referred to a survey of \ac{cl} for \ac{drl}~\cite{Narvekar2020}.

One study of \ac{cl} in \ac{drl} that is closely related to our study is DRAS~\cite{Fan},  a \ac{drl} agent for job scheduling in \ac{hpc}. Interestingly, although the authors did not state explicitly for adopting \ac{cl}, it is apparent that they adopt \ac{cl} in training their agent.
Their training strategy allows the agent to learn gradually, from simple average cases to unseen rare cases. This technique has experimentally performed best compared to other possible training strategies.

%The agent in DRAS is trained in a three-phase training process. In the first phase, the training dataset comprises randomly sampled jobs from real job traces, followed by a training dataset comprising real job traces in the second phase. Lastly, in the third phase, the agent is trained with synthetic jobs generated to mimic the job patterns of the target HPC system.
%ordering of the training phases
%is not only intuitive but also experimentally shown to perform best compared to other possible orders.



In this study, we propose to design a similar training strategy to improve the performance of the trained agent in solving \ac{psmp}. We propose three different types of datasets to train the agent based on a real workload trace of the target \ac{hpc} system. We conduct an experimental study to find the ordering of the training phases that result in the best-performing agent. 


% In the language modeling experiment, the model began training with 5,000 words. The number of words was then gradually increased by 5,000 during training, up to a total of 20,000 words. The CL model was compared to a non-CL model trained directly with 20,000 words. The average log rank during testing for the CL model was 2.78, while for the non-CL model, it was 2.83. These results indicate that the CL model significantly outperformed the non-CL model in prediction accuracy. The study demonstrated that CL can improve performance during testing and hypothesized that CL can accelerate convergence. However, this research focused solely on supervised learning and language modeling, without delving into DRL.

% \hyphenation{make-span}

% The study by \cite{waubert} discussed the use of CL in combination with DRL for the Job Shop Scheduling Problem (JSSP). JSSP is a scheduling problem aimed at completing jobs using a specific number of processing machines. Each job has a predefined sequence of processing machines it must follow. Additionally, each job has a different processing time (\textit{makespan}). The objective of this study was to find an optimal schedule to minimize the total \textit{makespan}. 

% In the study, the difficulty of each job was determined and categorized as either \textit{easy} or \textit{hard}. A job with a \textit{makespan} below the average was classified as \textit{easy}, while a job with a \textit{makespan} above the average was classified as \textit{hard}. The curriculum in this problem was designed by creating two types of datasets—\textit{normal} and \textit{reverse}—for each difficulty level. This resulted in four datasets: e\_r (\textit{easy reverse}), e\_n (\textit{easy normal}), h\_r (\textit{hard reverse}), and h\_n (\textit{hard normal}). 

% The \textit{normal} dataset contained more training instances with \textit{makespan} values closer to the minimum for the corresponding difficulty type and fewer instances closer to the maximum. Conversely, the \textit{reverse} dataset had the opposite distribution. Each curriculum combined two types of datasets into a sequence, resulting in 16 curricula. Each curriculum significantly influenced the model's performance. The CL-enhanced model outperformed random scheduling by 3.2\% in terms of average job \textit{makespan}. However, JSSP has characteristics distinct from scheduling in HPC systems, where job submission time is not considered, and thus there is no dynamic job arrival in this problem.
% \todo{this feels like an incomplete paragraph.}