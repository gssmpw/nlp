\section{Experimental setup}\label{sec:ExperimentalSettings}

% 4.1 metode pembanding
% 4.2 workload trace (ambil dari paper fitra penjelasannya, parafrase)
% 4.3 validasi visual hasil generate dataset (thomas 5.1)
% 4.4 Evulation metric (thomas 4.8)

\subsection{Agent training and testing}
\label{sec:AgentTrainingAndTesting}

%We set up the agents to operate within an HPC system comprising 128 nodes. These nodes execute jobs based on the \ac{fcfs} and backfilling scheduling policies. The \ac{fcfs} policy ensures that jobs are executed in the order they arrive, and backfilling is employed to enhance resource utilization. Specifically, jobs further down the queue may start execution earlier if they do not delay the start of any preceding jobs. This mechanism effectively fills gaps in the schedule, minimizing idle time and improving throughput.

We set up the agents to operate within a simulated environment of an HPC system. These HPC nodes execute jobs based on the \ac{fcfs} and backfilling scheduling policies. The \ac{fcfs} policy ensures that jobs are executed in the order they arrive, and backfilling is employed to enhance resource utilization. In backfilling, jobs further down the queue may start execution earlier if they do not delay the start of any preceding jobs. This policy effectively fills idle nodes with jobs, minimizing idle time and improving throughput. We implemented these scheduling policies to ensure that the baseline setup of the simulated environment achieves good resource utilization and, hence, the source of the wasted energy is not caused by the scheduler's poor node utilization.

Each node in the environment has distinct power consumption levels depending on its current state. The specific parameter values the agent is trained on is set based on the values used by Khasyah et al.~\cite{Fitra}:
\begin{itemize}
    \item Number of nodes: 128. 
    \item Active and switch-on power consumption: 190 Watts.
    \item Sleep and switch-off power consumption: 9 Watts.
    \item Switch-on time: 45 minutes.
    \item Switch-off time: 30 minutes.
\end{itemize}

The agent uses a set of system features for decision-making, as outlined in Table~\ref{table:features}. Features 1 to 5 represent the overall condition of the \ac{hpc} system and are uniform across all nodes. Conversely, features 6 to 11 are node-specific. These features are updated at every action step to ensure the agent has access to the latest state of the system. The hyperparameter configuration used during training is specified in Table~\ref{tab:hyperparameter}.

All simulations are conducted using the Batsim framework simulator~\cite{dutot2017batsim}, which facilitates the modeling of job scheduling and power management in \ac{hpc} systems. For Python-based interaction, the Batsim-py \ac{api} is utilized~\cite{casagrandebatsim}. The experiments, including model training and job scheduling simulations, are executed on hardware equipped with a 32GB RAM Intel Core i9-9820X CPU @ 3.30GHz (20 CPUs).

% The agent is trained using the \ac{a2c} (Advantage Actor-Critic) algorithm, following the methodology outlined in prior research~\cite{Fitra}. The hyperparameter configuration used during training is presented in Table~\ref{tab:hyperparameter}. The dataset used for training and testing is split in an 80:20 ratio.

% Similar to the previous research in ~\cite{Fitra}, the agent used in this study is trained with \ac{a2c}. The agent is trained with the hyperparameter configuration shown in Table~\ref{tab:hyperparameter}. The system features used in this study are shown in Table~\ref{table:features}. Features 1 to 5 are the same for all nodes, as it describes the overall condition of the \ac{hpc} system. On the other hand, features 6 to 11 are specific for each node. The features are updated at every decision step.

% Based on the study by Khasyah et al.~\cite{Fitra}, the HPC system's configuration values are set as follows: (1) Active and switch-on energy consumption = 190 Watts, (2) Sleep and switch-off energy consumption = 9 Watts, (3) Switch-on time = 45 minutes, and (4) Switch-off time = 30 minutes. The \ac{fcfs}+Backfilling scheduling policies utilized in HPC systems are designed to balance efficiency and fairness. Under these policies, jobs at the front of the queue are prioritized and executed according to the \ac{fcfs} principle. However, subsequent jobs in the queue are permitted to start execution earlier if, and only if, their execution does not delay the start of any preceding jobs. This approach allows for better resource utilization by filling gaps in the schedule with smaller or shorter jobs, thereby reducing system idle time and improving overall throughput.

% Testing is conducted using the Batsim framework simulator \cite{casagrandebatsim}. This simulator allows the simulation of job scheduling and power management on HPC systems with various configurations. To facilitate the use of Batsim in Python programming, this research utilizes the Batsim-py \ac{api}. Testing is conducted using the \ac{fcfs} and Backfilling scheduling methods. These scheduling methods schedule jobs based on their arrival time and use Backfilling to schedule jobs on idle nodes without disrupting previously scheduled tasks. Models' training and the job scheduling with power management simulations was run on a 32GB RAM Intel Core i9-9820X CPU @ 3.30GHz (20 CPUs) hardware.


% \begin{table}[t]
% \centering
% \caption{The features to represent the state of the system}
% \label{table:features}
% \begin{tabular}{|l|p{6.5cm}|}
% \hline
% \multicolumn{1}{|c|}{No.} & \multicolumn{1}{c|}{Features} \\ \hline
% \multicolumn{1}{|c|}{1} & Number of jobs in the queue \\ \hline
% \multicolumn{1}{|c|}{2} & Current arrival rate \\ \hline
% \multicolumn{1}{|c|}{3} & Average waiting time of the jobs in the queue \\ \hline
% \multicolumn{1}{|c|}{4} & Total wasted energy \\ \hline
% \multicolumn{1}{|c|}{5} & Average requested runtime of the jobs in the queue \\ \hline
% \multicolumn{1}{|c|}{6} & The power state of node $m$ \\ \hline
% \multicolumn{1}{|c|}{7} & A flag for idling for node $m$ ($u_{tm1}c_{tm}$) \\ \hline
% \multicolumn{1}{|c|}{8} & The current idle time of node $m$\\ \hline
% \multicolumn{1}{|c|}{9} & The release time of node $m$\\ \hline
% \multicolumn{1}{|c|}{10} & The wasted energy of node $m$\\ \hline
% \multicolumn{1}{|c|}{11} & The total time node $m$ used for switching on and off\\ \hline
% \end{tabular}
% \end{table}

\begin{table}[t]
\centering
\caption{The features to represent the state of the system.}
\label{table:features}
\begin{tabular}{cl}
\toprule
\textbf{No.} & \textbf{Features} \\ 
\midrule
1 & Number of jobs in the queue \\ 
2 & Current arrival rate \\ 
3 & Average waiting time of the jobs in the queue \\ 
4 & Total wasted energy \\ 
5 & Average requested runtime of the jobs in the queue \\ 
6 & The power state of node $m$ \\ 
7 & A flag for idling for node $m$ ($u_{tm1}c_{tm}$) \\ 
8 & The current idle time of node $m$\\ 
9 & The release time of node $m$\\ 
10 & The wasted energy of node $m$\\ 
11& The total time node $m$ used for switching on and off\\ 
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\caption{Agent's hyperparameter values.}\label{tab:hyperparameter}
\begin{tabular}{lr}
    \toprule
    \textbf{Hyperparameter} & \textbf{Value} \\
    \midrule
    % Architecture & A2C \\
    Number of head in multi-head attention & 8 \\
    Number of layers in multi-head attention & 3 \\
    Embedding size & 128 \\
    Learning rate & $10^{-5}$ \\
    Discounted return $\gamma$ & 0.99 \\
    Gradient clipping norm & 2 \\
    $N_b$ & 64 \\
    $\Delta t$ & 1800 \\
    $\alpha$ & 0.5 \\
    $\beta$ & 0.5 \\
    Training epochs & 10 \\
    \bottomrule
\end{tabular}
\end{table}


\subsection{Comparison methods and evaluation metrics}
\label{sec:evaluation}

% \todo{
% \begin{itemize}
%     \item 1 brief paragraph of Non-curr Learning of Fitra with specified (hyper)params
%     \item 1 brief paragraph of timeout policy (or several timeout policy with different timeouts?)
%     \item List every order of Curriculum learning methods
% \end{itemize}
% }

% \edited{
% Before the emergence of DRL, PSMP systems were typically managed using static timeout policies, where nodes were deactivated after remaining idle for a fixed period \cite{kveton2007adaptive}. While simple to implement, these static policies often rely on predefined, inflexible timeout values that may not be optimal for dynamic system conditions. DRL offers a more adaptive approach by learning to adjust timeout values based on historical job submission data, enabling more efficient resource utilization. To further enhance the performance of DRL models, CL can be employed during the training process, guiding the model through progressively challenging learning stages.
% }

% In this work, we evaluate the performance of these three approaches on PSMP: (1) static timeout policies, (2) DRL without CL, and (3) DRL with our proposed CL strategy. 


% In this research, evaluations will be conducted on the six curricula, the model without curriculum learning (CL), and the timeout policy method. First, a comparison of the results from the six curricula will be made to identify the best-performing curriculum, referred to as the Best Agent. The Best Agent will then be compared with the model without CL and the timeout policy method in terms of energy savings. The comparison metrics used are:

% \begin{enumerate}
% \item Average waiting time: The average time jobs wait in the queue.
% \item Total energy consumption: The overall energy usage of the HPC system, as formulated in (\ref{eq:totalPower}).
% \item Total energy wastage: The total energy consumption when nodes are idle, as formulated in (\ref{eq:totalWaste}).
% \end{enumerate}

% In this research, the timeout policy will be tested under 12 configurations: 5, 10, 15, 20, 25, 30, 35, 40, 50, 55, and 60 minutes.

% Regarding the impact on job scheduling in the HPC system, the Best Agent, the model without CL, and the timeout policy method with the best configuration will be re-evaluated. The comparison metrics used are:

% \begin{enumerate}
% \item Average waiting time
% \item Maximum waiting time: The longest time a job waits in the queue.
% \item Average response time: The average time between job submission and job completion.
% \item Average slowdown: The average ratio of job response time to the actual runtime of the job.
% \item System utilization: The ratio of the time nodes spend computing to the total time nodes are idle and computing.
% \end{enumerate}

%\edited{
% Within these configurations, 
In this work, we evaluate the performance of these three approaches on \ac{psmp}: (1) fixed timeout policies, (2) \ac{drl} without \ac{cl} (referred to as the no-CL agent) \cite{Fitra}, and (3) \ac{drl} with our proposed \ac{cl} strategy over six curricula. 
% this study evaluates six curricula, a model without CL, and the timeout policy method. 
First, the results of training the agent with the six curricula are compared to identify the best outcome, named the \emph{Best Agent}. This Best Agent is compared to the timeout policy methods and the model without \ac{cl}. The timeout policy will be tested using 12 configurations, namely 5, 10, 15, 20, 25, 30, 35, 40, 50, 55, and 60 minutes. The performance of these methods are compared based on
the evaluation metrics given as:
\begin{enumerate}
% \item Total energy consumption as formulated in Eq.~\eqref{eq:totalPower}.
\item Total wasted energy as formulated in Eq.~\eqref{eq:totalWaste}.
\item Average waiting time as formulated in Eq.~\eqref{eq:total-waiting-time}.
\item Job-filling rate.
\item Number of node shutdowns.
\item The impact on job scheduling performance.
\end{enumerate}
%}
In addition, we evaluate the generalization capability of our Best Agent by testing it in different HPC environments.
%\sanedited{In addition, we perform a sensitivity study by applying our best agent on different HPC environments.}




% \subsection{Workload Trace}
% \todo{Ambil penjelasan dari Fitra dan parafrase (GAIA?)}


