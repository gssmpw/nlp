\section{Conclusion}\label{sec:Conclusions}

% Based on the research and simulations conducted, the following conclusions can be drawn:
% \begin{enumerate}
% \item The Easy-to-Hard curriculum outperforms the Hard-to-Easy and Hard-Easy-Hard curricula. This indicates that a DRL agent for HPC system power management performs better when learning starts from simpler problems and progresses to more complex ones.
% \item The Best Agent surpasses the model without a curriculum and the most optimal timeout policy method in terms of energy-saving metrics. The Best Agent saves 3.73\% more energy compared to the model without a curriculum and 4.66\% more than the most optimal timeout policy method.
% \item The Best Agent outperforms the most optimal timeout policy method in terms of average job waiting time, a metric where the model without a curriculum falls short. The Best Agent reduces average waiting time in the queue by 9.24\% compared to the most optimal timeout policy method.
% \item The Best Agent demonstrates superior overall performance in its impact on job scheduling efficiency. These results highlight the Best Agent's improved ability to determine the optimal timing for turning nodes on and off.
% \end{enumerate}

% Further considerations that could improve our proposed CL strategies include:
% \begin{enumerate}
%     \item Enhance training with CL focused on difficulty adjustments per epoch.
%     \item Develop additional datasets that better consider the correlations between requested nodes, walltime, and user ID.
%     \item Evaluate CL using datasets with a more varied number of jobs.
% \end{enumerate}

% \santodo{aaaaaaaaaaaaaaaaaaaaaaa}

% Based on the research and simulations conducted, several key conclusions can be drawn. Firstly, the easy-to-hard curriculum significantly outperforms the hard-to-easy and hard-easy-hard curricula. This finding suggests that a \ac{drl} agent for \ac{hpc} system power management achieves better performance when it begins by tackling simpler problems before progressing to more complex ones. This structured learning approach enhances the agent's capacity to manage the system more effectively. 

% The Best Agent surpasses both the model without a curriculum and the most optimal timeout policy method in terms of energy-saving metrics. Specifically, the Best Agent achieves 3.73\% more energy savings compared to the model without a curriculum and 4.66\% more than the most optimal timeout policy method. These results emphasize the importance of \ac{cl} in optimizing energy consumption for \ac{hpc} systems. Furthermore, the Best Agent also outperforms the most optimal timeout policy method in terms of average job waiting time, a metric where the model without a curriculum underperforms. The Best Agent reduces the average job waiting time in the queue by 9.24\% compared to the most optimal timeout policy method. This improvement highlights the agent's ability to reduce system congestion, a critical factor for efficient \ac{hpc} scheduling. Finally, the Best Agent demonstrates superior overall performance in its impact on job scheduling efficiency. The results highlight the agent's enhanced capacity to identify the optimal timing for turning \ac{hpc} nodes on and off, further validating the effectiveness of the proposed \ac{cl} strategy.

% Several considerations could further improve the proposed \ac{cl} strategies. One possible improvement is to enhance training by incorporating \ac{cl} that adjusts difficulty at each epoch. This adaptive approach could better align the learning process with the agent's evolving capabilities. Another avenue for improvement is the development of additional datasets that more effectively capture correlations between requested nodes, walltime, and user ID. Such datasets would provide the agent with richer training scenarios. Finally, evaluating \ac{cl} with datasets containing more diverse numbers of jobs could further generalize the approach, ensuring robustness across varying workloads and job patterns. 
% These considerations present opportunities for further refinement of the \ac{cl} strategy for \ac{hpc} system power management.


% \santodo{aaaaaaaaaaaaaaaaaaaaaaa}


% In conclusion, this study confirms that incorporating curriculum learning (CL) dramatically improves the performance of a DRL-based HPC power management agent. Firstly, an easy-to-hard curriculum proves to be the most effective training strategy: the agent trained with gradually increasing difficulty (specifically, a sample-real-synthetic progression) significantly outperforms agents trained with reverse or mixed difficulty progressions. In our experiments, the easy-to-hard trained models achieved the lowest total energy consumption, whereas the hard-to-easy and hard-easy-hard approaches performed on par with or even worse than the no-CL baseline. This outcome underscores that presenting learning tasks in increasing complexity allows the agent to build competence on simpler scenarios before tackling more complex ones.

% As a result, the agent following the easy-to-hard curriculum emerged as the Best Agent, exhibiting superior efficiency and quality-of-service compared to all baselines. In terms of energy use, the Best Agent reduced total wasted energy by 3.73\% relative to the no-CL agent and by 4.66\% compared to the optimal static timeout policy (15-minute idle threshold). This translates to roughly $8\times10^8$~Joules saved over the No-CL approach. Importantly, these energy gains did not come at the cost of performance. The CL-trained agent actually shortened average job waiting time by 13.02\% compared to the best timeout policy, whereas the agent without CL experienced significantly longer waits (approximately 44\% higher than the CL agent's). By reducing queue delays, the Best Agent decreases system congestion and improves overall quality of service, which is a critical factor in HPC scheduling. Moreover, in a holistic evaluation of scheduling effectiveness, the Best Agent outperformed both the no-CL agent and the static policy across all metrics (including average job slowdown and system utilization). In particular, its system utilization was substantially higher (roughly 32.7\% above an always-on baseline and even slightly above the 15-minute timeout baseline), indicating that the agent keeps HPC nodes busy whenever they are powered on. In essence, the CL strategy enabled our agent to learn near-optimal on/off timing for idle nodes, yielding a balanced improvement in energy efficiency and job throughput.

% Furthermore, the curriculum-trained agent demonstrated robust generalization across varied system conditions. In sensitivity tests with different node switch-on/off durations, power consumption profiles, and cluster sizes, the Best Agent consistently maintained its energy-saving and wait-reduction advantages without retraining. In contrast, the agent trained without a curriculum showed highly variable behavior under these altered conditions. For example, the no-CL agent became overly aggressive in saving energy in certain scenarios, which dramatically increased waiting times. The stable performance of the CL-trained agent under diverse configurations highlights the adaptability conferred by the curriculum approach. In practice, this means our agent remains effective even as HPC system parameters change, an important property for real-world deployments.

% Building on these promising results, several improvements can be explored. One potential direction is to adopt an adaptive curriculum that adjusts difficulty at each training epoch, better aligning with the agent's evolving skill level. Another is to enrich the training data with additional datasets capturing a wider range of job characteristics and correlations (e.g., between requested nodes, walltimes, and user IDs), providing the agent with more diverse scenarios. Finally, evaluating the CL strategy on larger-scale and more varied workloads would ensure the agent's strategies remain robust across different conditions. These avenues offer opportunities to further enhance the agent's effectiveness in optimizing energy usage and job scheduling performance in HPC systems.

% \santodo{aaaaaaaaaaaaaaaaaaaaaaa}


Several key conclusions can be drawn based on the research and simulations conducted. Firstly, the easy-to-hard curriculum significantly outperforms the hard-to-easy and hard-easy-hard curricula, with the sampled-real-synthetic training sequence (easy-to-hard) yielding the best overall performance. This finding suggests that a DRL agent for HPC system power management achieves better performance when it begins by tackling simpler problems before progressing to more complex ones. This structured learning approach enhances the agent's capacity to manage the system more effectively.

The Best Agent surpasses both the model without a curriculum and the optimal 15-minute timeout policy in terms of energy-saving metrics. Specifically, the Best Agent achieves 3.73\% more energy savings than the model without a curriculum and 4.66\% more than the optimal 15-minute timeout policy. These results emphasize the importance of CL in optimizing energy consumption for HPC systems. Furthermore, the Best Agent also outperforms the optimal timeout policy in terms of average job waiting time, a metric where the model without a curriculum underperforms. The Best Agent reduces the average job waiting time in the queue by 13.02\% compared to the optimal timeout policy. This improvement highlights the agent's ability to reduce system congestion, a critical factor for efficient HPC scheduling. The Best Agent also demonstrates superior overall performance in its impact on job scheduling efficiency, with improvements observed across all evaluated metrics (for example, attaining higher system utilization and lower average job slowdown than both the no-CL agent and the timeout-based policy). These results highlight the agent's enhanced capacity to identify the optimal timing for turning HPC nodes on and off, further validating the effectiveness of the proposed CL strategy. Finally, the CL-trained agent's advantages persist across different system configurations. In our sensitivity tests, the Best Agent maintained its energy-saving and wait-reduction benefits under varying node switching durations, power consumption rates, and numbers of nodes without requiring retraining. In contrast, the performance of the no-CL agent fluctuated under these changes. This consistency demonstrates the curriculum approach's strong generalization capability in adapting to diverse HPC scenarios.

Several considerations could further improve the proposed CL strategies. One possible improvement is to enhance training by incorporating CL that adjusts difficulty at each epoch. This adaptive approach could better align the learning process with the agent's evolving capabilities. Another avenue for improvement is the development of additional datasets that more effectively capture correlations between requested nodes, walltime, and user IDs. Such datasets would provide the agent with richer training scenarios. Finally, evaluating CL with datasets containing more diverse numbers of jobs could further generalize the approach, ensuring robustness across varying workloads and job patterns. These considerations present opportunities for further refinement of the CL strategy for HPC system power management.