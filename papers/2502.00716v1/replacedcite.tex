\section{Related works}
\textbf{Imbalance Graph Classification:}
Due to the non-Euclidean characteristics of graph data, the imbalance problem in this type of data necessitates additional attention and specialized focus. Following the categorization of imbalance classification methodologies, we similarly categorize imbalance graph classification models into two main groups: data-level and algorithm-level methods.

\textit{Data-level methodologies} try to rebalance the learning environment by manipulating training data in feature or label spaces. Fundamental approaches such as over-sampling and under-sampling, prevalent in traditional class-imbalanced learning, require adaptation to suit the complexities inherent in graph data, characterized by intricate node dependencies and interconnections. Data-level methods could subcategorized into data interpolation, adversarial generation, and pseudo-labeling ____. Data interpolation techniques, exemplified by SMOTE ____ and its graph-oriented variant, GraphSMOTE ____, generate synthetic minority nodes by interpolating between existing node representations. Methods like GATSMOTE ____ and GNN-CL ____ enhance this process through attention mechanisms, ensuring quality edge predictions between synthetic and real nodes. Moreover, Mixup strategies, as seen in GraphMixup ____ and GraphENS ____, introduce sophisticated blending techniques to combat neighbor memorization issues and prevent out-of-domain sample generation.

\textit{Algorithm-level methodologies} focus on refining learning algorithms to accommodate class imbalance effectively. Model refinement strategies adapt graph representation learning architectures to improve performance by incorporating class-specific adjustments and modules tailored to handle imbalance. For instance, approaches like ACS-GNN ____ and EGCN ____ modify aggregation operations within graph neural networks to prioritize minority class information, ultimately enhancing classification accuracy. Additionally, loss function engineering tries to design customized loss functions to prioritize minority class errors or encourage wider class separations, a challenging task given the connectivity properties of nodes in graphs. Recent innovations like ReNode ____ and TAM ____ integrate graph topology information into loss function designs, showcasing advancements in addressing class imbalance within the graph context. ____ proposed a KL regularization for imbalance node classification.

\textbf{Generalization Error and Node Classification:} There are two different scenarios in node classification, including inductive and transductive scenarios. Inductive learning node classification involves (semi-)supervised learning on graph data samples where the test data are unseen during training. In Inductive node classification For node classification tasks, ____ discussed the generalization error under node classification for GNNs via algorithm stability analysis in an inductive learning setting based on the SGD optimization algorithm. The work by ____ extended the results in ____ and found that increasing the depth of Graph Convolutional Neural Network (GCN) enlarges its generalization error for the node classification task. In Transductive Node Classification, based on transductive Rademacher complexity, a high-probability upper bound on the generalization error of the transductive node classification task was proposed by ____. The transductive modeling of node classification was studied in ____ and ____. ____ presented an upper bound on the generalization error of GCNs for node classification via transductive uniform stability, building on the work of ____. In contrast, our research focuses on the task of imbalance transductive node classification and we provide an upper bound on population risk.

\textbf{Uncertainty in Self-training: }
The integration of uncertainty estimation in GNNs has been extensively explored, particularly in self-training frameworks ____. Early contributions, such as ____, introduced a simple strategy that selects the top K most confident nodes based on softmax probabilities for pseudo-labeling, expanding the labeled set iteratively. This approach was later refined by ____, which employed a multi-stage self-training method, iteratively updating node labels to enhance GNN performance in sparse label settings. Extending these ideas, ____ proposed a self-enhanced GNN framework that utilizes an ensemble of models to ensure consistency in pseudo-label predictions, thereby improving robustness against label noise. A common technique for uncertainty estimation in these methods is the use of entropy, as demonstrated in ____, which measures prediction confidence to guide node selection. Building on this, ____ introduced an entropy-aware self-training framework, incorporating an entropy-aggregation layer to account for graph structural information in uncertainty estimation. While traditional approaches in entropy-based uncertainty estimation, prioritize high-confidence nodes for pseudo-labeling, this practice might unintentionally reinforce distributional biases, as highlighted by ____, where over-reliance on highly certain nodes risks a distribution shift between the original labeled set and the expanded pseudo-labeled set. Furthermore, ____ observed that nodes with very high confidence often contribute redundant information, limiting the diversity of the labeled dataset and potentially biased learning process. To address this concerning issue, in this work, we suggest to balance node selection across a broader confidence range to improve the performance of our model.
% \vspace{-1em}