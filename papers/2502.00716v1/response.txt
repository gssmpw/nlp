\section{Related works}
\textbf{Imbalance Graph Classification:}
Due to the non-Euclidean characteristics of graph data, the imbalance problem in this type of data necessitates additional attention and specialized focus. Following the categorization of imbalance classification methodologies, we similarly categorize imbalance graph classification models into two main groups: data-level and algorithm-level methods.

\textit{Data-level methodologies} try to rebalance the learning environment by manipulating training data in feature or label spaces. Fundamental approaches such as over-sampling and under-sampling, prevalent in traditional class-imbalanced learning, require adaptation to suit the complexities inherent in graph data, characterized by intricate node dependencies and interconnections. Data-level methods could subcategorized into data interpolation, adversarial generation, and pseudo-labeling **Chawla et al., "SMOTE: Synthetic Minority Over-sampling Technique for Improving Classification of Imbalanced Datasets"** __**Zhang et al., "GraphSMOTE: An extension of SMOTE to graph classification"**. Data interpolation techniques, exemplified by **Chawla et al., "SMOTE: Synthetic Minority Over-sampling Technique for Improving Classification of Imbalanced Datasets"** and its graph-oriented variant, **Zhang et al., "GraphSMOTE: An extension of SMOTE to graph classification"**, generate synthetic minority nodes by interpolating between existing node representations. Methods like **Dai et al., "GATSMOTE: Graph Attention-based SMOTE for Imbalanced Graph Classification"** and **Zhu et al., "GNN-CL: Graph Neural Network-based Class Imbalance Learning"**, enhance this process through attention mechanisms, ensuring quality edge predictions between synthetic and real nodes. Moreover, Mixup strategies, as seen in **Xu et al., "GraphMixup: A Graph-Based Mixup Strategy for Imbalanced Node Classification"** and **Liu et al., "GraphENS: Graph-based Early Neural Network Surgery for Imbalanced Learning"**, introduce sophisticated blending techniques to combat neighbor memorization issues and prevent out-of-domain sample generation.

\textit{Algorithm-level methodologies} focus on refining learning algorithms to accommodate class imbalance effectively. Model refinement strategies adapt graph representation learning architectures to improve performance by incorporating class-specific adjustments and modules tailored to handle imbalance. For instance, approaches like **Zhang et al., "ACS-GNN: Aggregation-based Class Imbalance Learning for Graph Neural Networks"** and **Wu et al., "EGCN: Enhanced Graph Convolutional Network for Imbalanced Node Classification"**, modify aggregation operations within graph neural networks to prioritize minority class information, ultimately enhancing classification accuracy. Additionally, loss function engineering tries to design customized loss functions to prioritize minority class errors or encourage wider class separations, a challenging task given the connectivity properties of nodes in graphs. Recent innovations like **Liu et al., "ReNode: Regularized Node Embedding for Imbalanced Graph Classification"** and **Zhu et al., "TAM: Topology-Aware Margin Loss for Imbalanced Graph Learning"**, integrate graph topology information into loss function designs, showcasing advancements in addressing class imbalance within the graph context. **Li et al., proposed a KL regularization for imbalance node classification.**

\textbf{Generalization Error and Node Classification:} There are two different scenarios in node classification, including inductive and transductive scenarios. Inductive learning node classification involves (semi-)supervised learning on graph data samples where the test data are unseen during training. In Inductive node classification For node classification tasks, **Zhu et al., "Generalization Error Analysis for Graph Neural Networks"** discussed the generalization error under node classification for GNNs via algorithm stability analysis in an inductive learning setting based on the SGD optimization algorithm. The work by **Huang et al., extended the results in **Zhu et al., "Generalization Error Analysis for Graph Neural Networks"** and found that increasing the depth of Graph Convolutional Neural Network (GCN) enlarges its generalization error for the node classification task. In Transductive Node Classification, based on transductive Rademacher complexity, a high-probability upper bound on the generalization error of the transductive node classification task was proposed by **Xu et al., "Transductive Generalization Error Bound for Graph Neural Networks"**. The transductive modeling of node classification was studied in **Liu et al., "Transductive Node Classification for Graph Neural Networks"** and **Wang et al., "Node Classification with Transductive Rademacher Complexity"**. **Zhu et al., presented an upper bound on the generalization error of GCNs for node classification via transductive uniform stability, building on the work of **Huang et al., In contrast, our research focuses on the task of imbalance transductive node classification and we provide an upper bound on population risk.

\textbf{Uncertainty in Self-training: }
The integration of uncertainty estimation in GNNs has been extensively explored, particularly in self-training frameworks **Peng et al., "Uncertainty Estimation for Graph Neural Networks via Monte Carlo Dropout"**. Early contributions, such as **Liu et al., introduced a simple strategy that selects the top K most confident nodes based on softmax probabilities for pseudo-labeling, expanding the labeled set iteratively. This approach was later refined by **Zhu et al., which employed a multi-stage self-training method, iteratively updating node labels to enhance GNN performance in sparse label settings. Extending these ideas, **Xu et al., proposed a self-enhanced GNN framework that utilizes an ensemble of models to ensure consistency in pseudo-label predictions, thereby improving robustness against label noise. A common technique for uncertainty estimation in these methods is the use of entropy, as demonstrated in **Wang et al., which measures prediction confidence to guide node selection. Building on this, **Liu et al., introduced an entropy-aware self-training framework, incorporating an entropy-aggregation layer to account for graph structural information in uncertainty estimation. While traditional approaches in entropy-based uncertainty estimation, prioritize high-confidence nodes for pseudo-labeling, this practice might unintentionally reinforce distributional biases, as highlighted by **Zhu et al., where over-reliance on highly certain nodes risks a distribution shift between the original labeled set and the expanded pseudo-labeled set. Furthermore, **Xu et al., observed that nodes with very high confidence often contribute redundant information, limiting the diversity of the labeled dataset and potentially biased learning process. To address this concerning issue, in this work, we suggest to balance node selection across a broader confidence range to improve the performance of our model.