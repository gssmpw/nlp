\section{Models and Benchmarks}
\label{sec:appendix:benchmarks}

We collect model candidates for all provenance pairs from the Hugging Face (HF) platform~\cite{huggingface}.
Since there is no inherent ground truth to determine whether two models constitute a provenance pair, we employ multiple heuristic approaches. These include analyzing metadata available through the HF API and comparing the weights of downloaded models.
We consider the most reliable ground truth to be cases where model uploaders explicitly specify their model as a fine-tuned version of another model, indicated by the presence of \texttt{"base\_model:finetune:<basemodel\_name>"} keyword in the model description on HF. 
When this explicit indication is not present, we resort to less reliable methods: we attempt to infer parent-child relationships through model naming patterns and by analyzing model descriptions on HF. Additionally, we identify potential provenance pairs by measuring the similarity between model weights, assuming that highly similar weights suggest a parent-child relationship. 
From these models we build two benchmarks.


The first benchmark, called \bencho, consists of LLM pairs for model provenance constructed from popular pre-trained models and their fine-tuned derivatives. To build this benchmark, we manually selected 10 widely-used pre-trained models (refer to Tbl.~\ref{tab:eval:bencho}) with between 1 billion and 4 billion parameters (the upper bound was determined by our GPU memory constraints). Among these, we purposefully included four pairs of architecturally similar models from Meta, Microsoft, Google, and AliBaba to evaluate our tester's ability to distinguish between closely related base models and to have some control models. For each pre-trained model, we then randomly sampled 10 fine-tuned derivatives using the Hugging Face API (i.e. use highly reliable ground truth verification), prioritizing diversity in model creators. 
This sampling strategy resulted in 100 derived models, that constitute \bencho. 

\begin{table}[h]
\setlength{\tabcolsep}{5pt}
%
\begin{center}
\caption{All 10 pre-trained LLMs from \bencho.}
\label{tab:eval:bencho}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|r}
\hline
Hugging Face Model & \# params \\
\hline
\texttt{meta-llama/Llama-3.2-1B-Instruct}  & 1,235,814,400\\
\texttt{meta-llama/Llama-3.2-3B-Instruct} & 3,212,749,824\\
\texttt{microsoft/Phi-3-mini-4k-instruct} & 3,821,079,552\\
\texttt{microsoft/phi-2} & 2,779,683,840\\
\texttt{google/gemma-2b} & 2,506,172,416\\
\texttt{google/gemma-2-2b} & 2,614,341,888\\
\texttt{Qwen/Qwen2-1.5B} & 1,543,714,304\\
\texttt{Qwen/Qwen2.5-1.5B-Instruct} & 1,543,714,304\\
\texttt{deepseek-ai/deepseek-coder-1.3b-base} & 1,346,471,936\\
\texttt{TinyLlama/TinyLlama-1.1B-Chat-v1.0} & 1,100,048,384\\
\end{tabular}
}
\end{center}
\end{table}


\begin{table}[h]
\setlength{\tabcolsep}{5pt}
\begin{center}
\caption{Top 10 pre-trained LLMs from \bencht.}
\label{tab:eval:benchtwo}
\begin{tabular}{l|r}
\hline
Hugging Face Model & \# params \\
\hline
\texttt{openai-community/gpt2} & 124,439,808 \\
\texttt{EleutherAI/pythia-70m} & 70,426,624 \\
\texttt{microsoft/DialoGPT-medium} & 345,000,000 \\
\texttt{facebook/opt-125m} & 125,239,296 \\
\texttt{distilbert/distilgpt2} & 81,912,576 \\
\texttt{openai-community/gpt2-large} & 774,030,080 \\
\texttt{openai-community/gpt2-medium} & 354,823,168 \\
\texttt{Qwen/Qwen2-0.5B} & 494,032,768 \\
\texttt{JackFram/llama-68m} & 68,030,208 \\
\texttt{EleutherAI/gpt-neo-125m} & 125,198,592 \\
\ldots & \ldots \\
\end{tabular}
\end{center}
\end{table}

The second benchmark, denoted as \bencht, was constructed through a more automated and comprehensive approach. We began by downloading the 1,000 most popular models from Hugging Face with less than 1B parameters, ranked by download count. We then filtered out non-English models\footnote{Due to lack of control models for them.} and those exhibiting low entropy or high self-perplexity, which are indicators of poor training quality or insufficient learning\footnote{We avoid testing low quality models.}. This filtering process resulted in 608 viable models. To establish ground truth provenance relationships among these models, 
besides the model owners provided \texttt{fine-tune} keyword approach, we also used the other less reliable methods. Through this analysis, we identified 57 pre-trained models and established 383 ground-truth model provenance pairs. The remaining 148 models are considered to be independent, having no clear derivation relationship with any other models in analyzed set. Part of models from \bencht is given in Table~\ref{tab:eval:benchtwo}.

