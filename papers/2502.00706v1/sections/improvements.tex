
%
%
%


\begin{comment}
\subsection{Extension to Many Candidate Parents}
The basic provenance problem described thus far assumes we have one candidate parent $f$ that we want to test our child model $g$ against. 
We now extend it to the general case when there is a set of parents:
\begin{definition}[Model Provenance Problem with  Unspecified Parent]
Given only query access to models, determine whether a model $g$ is derived from some model from the set ${f_1,\ldots,f_s}$ of candidate parent models.
\end{definition}

\prateek{Explain how this Algo works as follows. It finds the most similar model to the given model $g$ among all the control models $C$ and candidate parents $F$. If that model is a control model, the Algorithm terminates with False. Otherwise, the Algorithm goes on to test whether the FWER
of this model is overall below $\alpha$, the desired significance level. The test for the latter is the same as in Algorithm 1, except now all the alternate hypotheses (including control and candidates) are in the family tested against. When the Algorithm return True, it has the guarantee that the most similar model is one of the candidate models and that the total significance level across all hypotheses meets the threshold $\alpha$.}

While running the basic tester $s$ times (once for each provenance pair $(f_i,g)$) would solve the unspecified parent problem, this approach besides requiring more effort, also would require additional correction for multiple testing to maintain the same level of confidence. The probability of false positives would grow with the number of candidate parents $s$ unless appropriate adjustments (such as Bonferroni correction) are made to the significance level. 
%
We thus consider improved tester given in Algorithm~\ref{alg:unknown_parent_tester}. Our  tester avoids this issue by conducting a single set of hypothesis tests after identifying the most similar candidate.

\input{algorithms/alg_unknown_tester}

\end{comment}

\subsection{Reducing Query Complexity}
\label{sec:query}

Most of LLMs available currently allow cheap (even free) API access, thus the monetary query cost of running our testers is insignificant. 
When this is not the case, for example, either when the cost of queries is high (e.g. one query to OpenAI model O1 can cost more than \$1~\cite{openai_pricing}), or the models have some rate restrictions, one can consider enhancements to our testers from Algorithms~\ref{alg:basic_tester},~\ref{alg:unknown_parent_tester}. Furthermore, there are use cases when query complexity can be reduced without any side effects, thus it makes sense from optimization perspective. 
Note that our proposed enhancements for query reduction are not meant to preserve the theoretical guarantees of classical hypothesis testing that our previous testers inherit, but they can be useful in setups where query costs are prohibitive. 

We can divide the queries used in the tester (see Algorithms~\ref{alg:basic_tester},~\ref{alg:unknown_parent_tester}) into two distinctive types: \emph{online queries} made to the tested child model $g$, and \emph{offline queries} made to the parent model $f$ (or models $f_1,\ldots,f_s$) and to the control models $c_1,\ldots,c_m$. We make this distinction for two reasons. First, often offline queries are much cheaper, as the potential parent models (and the control models  as we will see in the Section~\ref{sec:eval}) are well established, and available from multiple sources, thus they are usually cheaper or free. Second, in some use cases, we can reuse the offline queries to perform many provenance tests of different $g_i$. Thus further we analyze separately these two scenarios.

\vspace{10pt}
\noindent
\textbf{Reducing Online Complexity.}
Since our tester is fundamentally based on statistical hypothesis testing, any reduction in query complexity must be compensated by increasing the statistical power of individual queries. Rather than querying model $g$ with $T$ random prompts, we can strategically select a smaller set of $T'<T$ prompts that yield comparable statistical power for detecting model provenance\footnote{It means in  Algorithms~\ref{alg:basic_tester},~\ref{alg:unknown_parent_tester}, instead of random sampling $x_1,\ldots,x_T \stackrel{\text{iid}}{\sim} \Omega$, the goal is to find set $x_1,\ldots,x_{T'}$  from $x_1,\ldots,x_T$ and $F,C$.}. We achieve this through an informed sampling approach: instead of uniform sampling from $\Omega$, we employ rejection sampling with an entropy-based selection criterion. Specifically, to generate each prompt in $T'$, we sample $k$ candidate prompts from $\Omega$ and select the one that maximizes the entropy of output tokens across all parent and control models. The selection criterion is dynamically weighted to favor prompts that have stronger discriminative power between similar models. While this approach introduces dependencies between the sampled  prompts (so the theoretical guarantees of classical hypothesis testing used in Algorithm~\ref{alg:basic_tester} and~\ref{alg:unknown_parent_tester} do not carry over), our empirical results in Section~\ref{sec:eval:online} demonstrate its practical effectiveness.
Full details about the approach are given in Appendix~\ref{sec:appendix:advanced_sampling}.

\vspace{10pt}
\noindent
\textbf{Reducing Offline Complexity.}
In non-adversarial settings where multiple provenance tests are performed against the same parent model $f$, we can trivially reduce the offline complexity by reusing the same set of offline queries across all tests. 
A concrete example of such scenarios arises when issues are discovered in a pre-trained LLM, such as problematic training data or generation of harmful content. A recent example is the lawsuit against the pre-trained model LLama for using copyrighted data in its training set~\cite{Reuters2024Meta}.  Since various teams and organizations may have fine-tuned their applications using this model, but precise provenance information is not readily available, there is a need to identify which models are derived from this problematic base model. In this case, the same set of offline queries to the base model and control models can be reused across all provenance tests.

We further consider the case of reducing offline complexity in settings where offline queries cannot be reused. The current version of our provenance tester samples $T$ prompts for each parent/control model, then runs the hypothesis test to discover the most similar candidate to the tested model $g$ and shows it has significantly higher similarity.
The key observation for reducing offline query complexity is that we may not need an equal number of queries to all parent/control models to identify the most similar one. If a particular parent model shows consistently higher similarity to $g$ compared to other models, we might be able to confirm it as the top candidate with fewer queries to the clearly dissimilar models. The challenge lies in determining when we have sufficient statistical evidence to conclude that one model is significantly more similar than the others, while maintaining our desired confidence levels.

This observation naturally leads us to formulate the problem as a Best Arm Identification (BAI)~\cite{audibert2010best} problem in the Multi-Armed Bandit (MAB) setting. In this formulation, each parent or control model represents an ``arm'' of the bandit, and querying a model with a prompt corresponds to ``pulling'' that arm. The ``reward'' for each pull is the binary outcome indicating whether the model's output matches that of the tested model $g$. The goal is to identify the arm (model) with the highest expected reward (similarity to $g$) while minimizing the total number of pulls (queries). So, we can leverage well-studied MAB algorithms that adaptively allocate queries, focusing more on promising candidates while quickly eliminating clearly dissimilar ones. The implementation of the tester based on BAI is detailed in Appendix~\ref{sec:appendix:offline_bai}. Theoretical guarantees from the MAB literature could be applied to bound the number of queries needed to identify the correct parent model with high probability, but this is beyond our goals.
