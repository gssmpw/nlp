\subsection{Correctness of Assumptions}
\label{sec:eval:assumptions}

As discussed in Section~\ref{sec:analysis}, our approach relies on two key assumptions. While the high accuracy demonstrated in the previous section indirectly validates these assumptions, we provide here a detailed experimental analysis of both.

Our first assumption posits that derived models maintain significant similarity to their parent models. To evaluate this, we analyzed the similarity rankings across all provenance tests using $3,000$ prompts. For each derived model, we examined where its true parent ranked among all models in terms of similarity ratio $\mu$. The results strongly support this: in \bencho, the true parent had the highest similarity ratio in $93\%$ of cases, while in \bencht this occurred in $89\%$ of cases. When considering whether parents ranked in the top $50$th percentile by similarity, these percentages increased to $98\%$ and $96\%$ respectively. Thus we can conclude that our experiments indicate that derived models do indeed maintain strong similarity patterns with their parent models. Inadvertently, we have shown as well that with $3,000$ prompts the tester almost approaches the statistical limit (only the model with highest similarity ratio can be identified as a parent), as the recalls are very close to the percentages of highest similarity ($89\%$ recall vs. $93\%$ highest parent similarity, and $82\%$ recall vs. $89\%$ similarity, for the two benchmarks, respectively).

\custombox{2}{\newtext{The assumption that derived models show significant similarity to their parent models is valid for most provenance pairs.}}

%

Our second assumption concerns whether control models can effectively establish a baseline for similarity between unrelated models. We stress that in our experiments we have chosen the control models to be simply the set of all pre-trained models in an unbiased way, without any special selection or optimization for particular parent models they are tested against. 
We empirically observe that such unbiased selection of control model establishes a good baseline similarity as evident from the accuracy results presented thus far.

We further examine how the size and quality of the set of control models might affect tester accuracy. We conducted experiments varying the size of the control set while keeping other parameters constant ($3,000$ prompts per test). We randomly sampled different-sized subsets from our full control sets ($10$ models for \bencho and $57$ for \bencht) and ran $100$ complete benchmark tests for each size, and averaged the outcomes. The results, shown in Figure~\ref{fig:eval:assumption_control}, reveal that both size and quality of the control set significantly impact tester performance. 
%
For \bencho, even with just $4$ control models, the tester achieved $55\%$ precision. This relatively good performance with few controls can be attributed to \bencho consisting entirely of general-purpose, well-trained LLMs - thus any subset of these models provides a reasonable baseline for measuring similarity between unrelated models. However, for \bencht, the randomly sampled $4$-model control set yielded less than $10\%$ precision. This poor performance stems from \bencht containing a much more diverse set of models, including domain-specific models (e.g., for medical or coding tasks) and smaller models with varying capabilities. With such diversity, a small random subset of control models is unlikely to establish good baselines for all test cases - for instance, when testing a coding-focused model, we need coding-related models in the control set to establish proper baselines\footnote{Note that in practice, unlike our random sampling experiments, one can deliberately select control models matching the domain and capabilities of the suspected parent model, thus reducing significantly the impact of size of control sets, and leaving quality of the control set as the main factor on efficiency of the tester.}. Performance improves steadily as control set size increases in both benchmarks, since larger control sets are more likely to include appropriate baseline models for each test case. 

\begin{figure}[t]
    \centering
    \subfigure
    {
        \includegraphics[width=8cm]{plots/assumption_control_large.png}
        %
    }
    \subfigure
    {
        \includegraphics[width=8cm]{plots/assumption_control_small.png}
        %
    }
    \caption{Precision and recall of \bencht (top) and \bencho (bottom) with respect to smaller control set size. }
    \label{fig:eval:assumption_control}
\end{figure}



\custombox{3}{\newtext{The tester's performance significantly degrades when the control set is too small or poorly selected.
}}

