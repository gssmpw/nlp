\section{Overview}
\label{sec:problem}

Model provenance testing has many motivating applications, but we present one representative scenario for concreteness.

\subsection{Motivating Scenario}
\label{sec:motivation}

%
Pretraining large language models (LLMs) involves significant investment, requiring substantial computational resources costing millions of dollars in infrastructure and thousands of GPU hours. When Company A releases a pretrained LLM (called \llmp), it employs specific licensing terms crucial for protecting this investment, maintaining competitive advantage, and controlling the model's usage~\cite{chatgpt-api,anil2023palm}. These licenses typically include restrictions on commercial usage, model modification, or redistribution, and may incorporate provisions for monitoring downstream usage and revenue sharing.

Startup B might download \llmp, perform only fine-tuning, but claim to have pretrained their model (\llmf) from scratch, thereby circumventing licensing requirements and misrepresenting company A's work. In such cases, we want to be able to \emph{determine if \llmf is derived through fine-tuning \llmp} and resolve the \emph{model provenance problem}. Given the significant economic incentive to fine-tune existing models rather than pretrain from scratch, verifying model provenance becomes essential for enforcing licensing terms and protecting intellectual property rights.
%
Aside from fine-tuning, there are a few 
other inexpensive techniques (mixture-of-experts, prompt engineering) that Startup B could employ to derive \llmf from \llmp while still claiming independent development. 
%
All these approaches require orders of magnitude less computational resources than pretraining, making them particularly attractive for circumventing licensing restrictions. Thus, the model provenance problem extends beyond detecting fine-tuning to identifying any form of model derivation that might violate licensing terms.
We have witnessed such a case recently when the pretrained model Llama 3.1 was reportedly used for military operations, violating the license agreement~\cite{china2024llm}. 

Startup B can release \llmf either by making its weights publicly available (e.g., on Hugging Face) or by offering it as an online service through an API (e.g., ChatGPT or PaLM-2).
The API-like interface is more general than if weights were also available to a tester trying to determine provenance.
In this setup, the tester must rely solely on the model's responses to input queries. This distinction significantly impacts the techniques available for detecting whether \llmf is derived from \llmp.

We consider a testing framework with \emph{minimal assumptions} only, where the tester can query \llmf on arbitrary prompts (through the API) and get responses. 
The tester has no access to the training datasets, hyperparameters used by either company, or any information about potential modifications performed by Startup B. 
%
This mirrors real-world conditions where companies do not always disclose their training procedures, data sources, or modification techniques, making the provenance testing problem both practical and challenging.


\subsection{Our Approach}
\label{sec:overview-approach}

\begin{figure}[!t]  
    \centering
    \includegraphics[width=0.5\textwidth]{figures/tester.png}
    \caption{Our model provenance tester that decides if model $g$ is derived from model $f$.
    %
    }
    \label{fig:tester}
\end{figure}


Our approach to testing model provenance is based on a key observation: fine-tuning and other model derivation techniques typically result in only limited changes to the original model, as they primarily adapt the model for new tasks. After fine-tuning, the derived model \llmf  may remain similar to its parent model  \llmp, as the process focuses on refining specific capabilities rather than creating fundamental changes to the model distribution.

In our setting, the tester can only interact with models through their API interfaces, submitting prompts and analyzing the generated outputs. We thus compute similarity of models by analyzing the behavioral patterns between \llmp and \llmf through their responses.
%
For efficiency and simplicity, we focus on comparing next-token predictions on random prompts rather than analyzing entire sequences. While extending the analysis to longer sequences ($n$-grams) might provide additional signals, 
our empirical evidence suggests that
the next-token distribution already captures significant information about a model's decision-making patterns and internal representations.

Our method tests a number of random diverse prompts, looking for evidence that \llmf's output distributions consistently align more closely with \llmp than would be expected from a model that was not derived from \llmp. However, there is no fixed threshold for what constitutes ``close alignment''. We cannot simply say that matching on 10\% or 20\% of outputs indicates provenance, as the degree of similarity can vary significantly depending on the intensity of fine-tuning, how different the downstream task is and other modifications performed. Even models from which \llmf was not derived may occasionally produce matching outputs, and we need to understand this rate of coincidental alignment. We thus need more rigorous and universal approach, that is based not only on similarity between models, but on \emph{finding a level of similarity that is unusually higher than coincidence}.

To establish a meaningful baseline, i.e., to characterize the expected level of coincidental agreement between unrelated models, we take several additional control models and analyze how closely their outputs align with \llmf. The selection of control models is crucial for establishing reliable baselines for coincidental agreement.
These control models are selected to represent a diverse range of output distributions, thus allowing to better estimate the average levels of similarity that might occur between unrelated models. 
%
If the similarity between \llmp and \llmf is substantially higher than the similarity observed with the control models, it provides compelling evidence that \llmf was indeed derived from \llmp, as their behavioral patterns exhibit a level of consistency that cannot be easily attributed to random chance. Conversely, if the similarity falls within the typical range observed among the control models, it may indicate that the observed alignment is not sufficient to conclusively determine provenance.

The actual test for excess similarity between \llmp and \llmf is performed using multiple hypothesis testing. Each hypothesis is formulated to show that similarity of \llmf to \llmp is greater than the similarity to any control models. 
When all these hypothesis tests yield statistically significant results, we obtain strong evidence that the similarity between these models is systematically higher than what would be expected by chance.
%
This statistical framework offers several key advantages. First, it handles varying degrees of fine-tuning adaptively, without requiring assumptions about the expected similarity between \llmf and \llmp. The test naturally identifies any significant bias in matching rates compared to the best non-parent baseline model. Second, rather than relying on arbitrary thresholds, it provides a principled way to quantify confidence in provenance determination through $p$-values, i.e., observed bias in matching rates, and confidence intervals. Third, the approach is computationally efficient, requiring only a modest number of test prompts to achieve statistical significance.


\subsection{Advantages and Effectiveness}

Our approach stands out in its reliance on minimal assumptions, particularly the requirement of only black-box query access to the models in question. By operating without access to proprietary model weights, training data, or hyperparameters, we mirror real-world conditions where such internal details are typically inaccessible. This makes our method highly practical and broadly applicable, as it can be employed in scenarios where models are available exclusively through API endpoints with restricted internal information.

%
A key strength of our method is its statistical foundation. 
By employing multiple hypothesis testing, we objectively assess whether the observed similarities between \llmf and \llmp are significant beyond what could be attributed to random chance. This framework allows us to compute precise $p$-values and confidence intervals, providing quantifiable evidence to support our conclusions about model similarities. Importantly, our approach avoids reliance on arbitrary thresholds and naturally adapts to varying degrees of similarity.
%

From a practical standpoint, our method is both efficient and scalable. Focusing on next-token predictions over a diverse set of random prompts minimizes the computational overhead and the number of queries required. This efficiency is particularly valuable when dealing with large models and the inherent limitations of API access, such as query costs and rate limits. The simplicity of our approach allows for straightforward implementation without the need for specialized hardware or extensive computational resources.

Regarding effectiveness, our method has demonstrated high accuracy in detecting model provenance. As we will detail in Section~\ref{sec:eval}, our approach consistently identifies provenance pairs with over $90$\% precision and $80$\% recall when tested on large set of pairs. These results highlight the method's capability to reliably determine provenance, even when the derived model has been significantly fine-tuned or altered. This effectiveness underscores its practical utility in real-world applications where accurate provenance detection is critical.

%

\subsection{Challenges}
\label{sec:limitations}

While our approach offers a robust and practical solution to the model provenance problem, it is important to acknowledge its limitations and the challenges inherent in this domain. Understanding these factors is crucial for interpreting the results of our method and guiding future enhancements.

First, a primary challenge is dealing with cases where the derived model \llmf has undergone extensive modifications that significantly alter its behavior relative to the parent model \llmp. Such modifications might include heavy fine-tuning on large and diverse datasets, architectural changes, or techniques designed to intentionally mask the model's origins. In these scenarios, the residual similarities between \llmf and \llmp may diminish to the point where our statistical tests lose sensitivity, potentially leading to false negatives.

Second, our method depends on the selection of control models used for baseline comparisons. The effectiveness of our statistical tests hinges on having appropriate control models that are sufficiently diverse and representative of unrelated models. If the control models are inadvertently dissimilar to \llmp, this could affect the baseline similarity levels and impact the test's reliability. Careful consideration is therefore required in choosing control models to ensure they provide a meaningful contrast in the hypothesis testing framework.

Another challenge lies in the inherent variability of language models and the possibility of coincidental similarities. Language models trained on similar datasets might exhibit overlapping behaviors, even if one is not derived from the other. This could potentially lead to false positives, where our method incorrectly identifies a provenance relationship due to shared patterns that are common in the domain rather than indicative of direct derivation. Addressing this requires careful interpretation of the results and may need an additional analysis to rule out such coincidental similarities.

In Section~\ref{sec:eval} we will experimentally determine the impact of some of these assumptions on the tester's effectiveness.




\begin{comment}
We further extend our framework to consider query efficiency as an explicit goal. In practice, either just \llmf, or both \llmf and \llmp along with other baseline models, may only be accessible through rate-limited or paid APIs, making it desirable to minimize the number of required queries. \prateek{This is actually part of your problem statement, and shouldn't be in the middle of the baseline solution. Also, can you be more specific---what's the real cost of making a few thousand extra queries. Is this an artificial or theoretical concern, or one that matters in practice? Show data!}
This opens up interesting questions about how to achieve reliable provenance testing with fewer prompts and developing early stopping criteria when rejecting the null hypothesis. Some prompts are more effective at discriminating between different models than others. For example, "To be or not to" will likely yield similar completions across models and thus should be avoided. We can go a step further and start choosing prompts adaptively to discriminate only between the best baseline model and \llmp. Since the best baseline model must be discovered during the testing process, this naturally leads to formulating the problem as a multi-armed bandit framework.
\end{comment}


\begin{comment}
We propose to quantify the degree of modification between two models by measuring similarities between them - through analysis of their output words. 
The intuition is that a genuinely pretrained model will exhibit substantial differences from existing models, while a derived model will maintain measurable similarities to its parent model, even after modification. \prateek{Ok, this is cool. But do you have any quick data for me to believe your intuition. E.g. how far are 2 models that are not derived from the same LLM?}
This approach is particularly effective because the computational constraints that make fine-tuning attractive also inherently limit how extensively the original model can be altered. 
\prateek{I am not sure I follow this last sentence. Are you saying that there is a limit to how much the model deviates, based on rounds of fine-tuning? I am not aware of any such prior evidence, so would be great if you show some data. (Small changes in model parameters may shift the distribution a lot)}

%
In our setting, the tester can only interact with models through their API interfaces, submitting prompts and analyzing the generated outputs. We thus compare behavioral patterns between \llmp and \llmf through their responses.
%
For efficiency and simplicity, we focus on comparing next-token predictions rather than analyzing entire sequences. While extending the analysis to longer sequences might provide additional signals, the next-token distribution already captures significant information about a model's decision-making patterns and internal representations.
Among these baseline models, we identify the one that shows the highest similarity to \llmf. This represents the strongest naturally occurring similarity we might expect by chance. We then use statistical hypothesis testing based on the ratios of matching tokens, where the null hypothesis is that \llmf's similarity to \llmp is no greater than its similarity to this best-matching baseline model. Rejecting this null hypothesis provides strong evidence that \llmf was derived from \llmp rather than being independently developed.

\prateek{At this point, you're sounding very ad-hoc, eg. the above para. I suggest you to not give away your solution in such haste. Instead, just setup the crisis to be resolved.}

\end{comment}


\begin{comment}
We note that common watermarking techniques could solve this provenance testing problem when \llmf does not diverge significantly from \llmp during fine-tuning. These techniques rely on a few carefully chosen prompts where the watermarked model outputs sequences of tokens that would be extremely unlikely to occur by chance. However, for arbitrary models, we may not know which prompts would trigger such distinctive behaviors, or such discriminative prompts may not even exist.
\end{comment}

