\input{figures/fig_precision_recall}

\subsection{Accuracy of Model Provenance Tester}
\label{sec:eval:basic}

We evaluate the accuracy of the provenance tester by examining its performance on both \bencho and \bencht under different numbers of prompts. 
Figure~\ref{fig:eval:accuracy} shows
the precision and recall results from these experiments. The tester demonstrates similar performance patterns on both benchmarks, with slightly better results on \bencho.

The precision is notably high (approximately $0.95$) when the tester uses up to $1,000$ prompts.
Interestingly, however, the precision reduces as the number of prompts (test samples) increases. This is in direct contrast to common hypothesis testing, where larger sample size leads to smaller standard errors, thus higher precision. 
We get different results because our model provenance tester relies on detecting similarities of models. When using a smaller number of prompts, it can detect only the stronger similarities which are usually due to model provenance. However, as we increase the prompts, it starts detecting similar models that not necessarily have provenance relation. This leads to misclassification and reduced precision.

The recall behavior shows an opposite trend - it improves with a larger number of prompts, eventually reaching $80\%-90\%$ depending on the benchmark. This follows expected behavior: more prompts increase the statistical power of our hypothesis tests, enabling detection of small but significant differences in similarities. This increased sensitivity leads to higher recall rates, as the tester can detect more subtle provenance relationships that might be missed with fewer prompts.

We also examine the impact the randomness of  prompt sampling on the tester's accuracy. We conduct experiments on both benchmarks using five different randomly sampled sets of $1,000$ prompts\footnote{We chose smaller number of prompts due to larger computation effort required to complete five full runs of both benchmarks. The running time is completely dominated by producing outputs from the models, which in theory is parallelizable, but in our case it was not due to limited GPU resources.}, with the same set of prompts used in all testers, and record the precision and recall for each run -- see Table~\ref{tab:eval:random_sampling} of Appendix~\ref{sec:appendix:tables}. The results show that these values vary by $1-4\%$ between runs, indicating consistent performance across different prompt samples.


\custombox{1}{\newtext{
%
Our model provenance tester demonstrates high accuracy across different benchmarks, achieving precision of $90\%-95\%$ and recall of $80\%-90\%$ with $3,000$ prompts per model. 
Simply increasing the number of prompts does not guarantee uniformly better results, reflecting a fundamental trade-off: gains in recall might be accompanied by losses in precision.
}}


The evaluations above are in the default framework, which assumes no candidate parent is given in each provenance test. 
We run similar experiments when the candidate parent is given  in Appendix~\ref{sec:appendix:known_parent}. This is an easier problem (to make a wrong prediction, one needs not only to have conclusive hypothesis test that output a wrong parent, but also it should match the candidate parent), and the results confirm this: the recall of the tester in this framework is similar to the recall on the default framework, whereas the precision is very close to $100\%$. 


