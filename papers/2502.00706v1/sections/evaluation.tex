\section{Evaluation}
\label{sec:eval}


We evaluate our proposed provenance testing approach experimentally. Our evaluation aims to assess both the effectiveness of the approach and examine the validity of its core two assumptions. Specifically, we seek to answer the following research questions:\begin{enumerate}[label=(RQ\arabic*)]
\item How accurate is our provenance tester in practice and how does the number of prompts affect its performance?
\item To what extent do derived models maintain similarity to their parents?
\item How does the size and selection of control models impact the tester?\item How effective are the query reduction approaches?
\end{enumerate}

%
%
\paragraph{Experimental Setup.} 
We run our model provenance testers on a Linux machine with 64-bit Ubuntu 22.04.3 LTS, 128GB RAM and
2x 24 CPU AMD EPYC 7443P @1.50GHz and 4x NVIDIA A40 GPUs with 48GB RAM. All experiments are implemented using PyTorch framework~\cite{paszke2019pytorch} and the Hugging Face Transformers library~\cite{transformers}.

%
%
\paragraph{Models and Provenance Pairs.}  
We collect model candidates for all provenance pairs from the Hugging Face (HF) platform\cite{huggingface}.
To avoid selection bias, we used download counts as our selection criterion, taking the most popular models subject only to hardware constraints on model size. 
To increase variety of candidates, we create two distinct benchmarks \bencho and \bencht, that differ in aspects such as model sizes, choice of pre-trained models, and ground-truth verification procedure. 
The full procedure of collection of models and constructions of benchmarks is described in Appendix~\ref{sec:appendix:benchmarks} and their brief comparison is given in Table~\ref{tab:eval:bench-a-b}.
%
\begin{table}[t]
\setlength{\tabcolsep}{5pt}
%
\begin{center}
\caption{Comparison of \bencho to \bencht on different features.}
\label{tab:eval:bench-a-b}
\begin{tabular}{l||l|l}
\hline
Feature &  \bencho & \bencht\\ \hline 
pre-trained models & 10 & 57 \\
derived models & 100 & 383 \\ 
total models   & 100 & 531 \\
model parameters  &  1B-4B  & $<$ 1B \\
compilation method & manual (partially) & automatic \\
ground-truth verification & higher & lower 
\end{tabular}
\end{center}
\end{table}

%
%
\paragraph{Framework.}

Our default evaluation framework assumes extended model provenance testing (refer to Algorithm~\ref{alg:unknown_parent_tester}) as it allows to test for all parents at once.
For sanity check, we also include an alternative framework, that considers the case where one parent is suspected (based on Algorithm~\ref{alg:basic_tester}), and this is discussed at the end of Section~\ref{sec:eval:basic} and evaluated in Appendix~\ref{sec:appendix:known_parent}. 
We use the common value for significance parameter $\alpha=0.05$. Sampling of prompts is described in Appendix~\ref{sec:appendix:sampling_prompts}.

\paragraph{Selection of control set.}
%
In all of our provenance tests, we use the complete set of available pre-trained models from the benchmark as control models - 10 models for \bencho and 57 for \bencht. This selection was done to demonstrates that effective control sets can be constructed without careful manual curation or domain-specific analysis.
%
Specifically, we make no effort to align control models with particular parent models' domains or capabilities. We neither analyze the outputs of parent models $f$ nor attempt to match control models to specific use cases. Instead, we simply include all pre-trained models that rank among the most popular on the Hugging Face platform. This sampling approach, while simple, helps avoid introducing obvious selection bias while ensuring broad coverage of model types and capabilities.
%
This straightforward selection strategy allows us to evaluate whether provenance testing can be effective even without carefully chosen control sets. 
%

\teo{How does the precision and recall change with the significance level?}
\ivica{did not test this}

\input{sections/eval_1_no_prompts}
\input{sections/eval_assuptions}


\input{sections/eval_2_adv_sampling}
\input{sections/eval_3_offline}
%

