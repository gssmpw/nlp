\section{Related Work}


\vspace{10pt}
\noindent
\textbf{Model Ownership and Copyright Detection.}
The need for methods for determining model ownership and detecting illegitimate use of model is now well recognized; the main difference is in our formulation to tackle the concern. To determine model ownership, one type of techniques apply changes to the training dataset and model training in order to insert ``watermarks''~\cite{jia2021entangled,shao2024explanation,yan2023rethinking},  backdoors~\cite{peng2023you,adi2018turning} or fingerprints~\cite{xu2024instructional,cao2021ipguard,peng2022fingerprinting,lukasdeep}.
Specialized model ownership schemes have also been proposed for other models such as graph neural networks~\cite{zhou2024revisiting, waheed2024grove}.
All of these techniques, however, are orthogonal to the problem of provenance testing which we formulate in this paper. They require changes to the training of the parent model which may degrade the performance of the model. Moreover, most existing approaches do not have provable guarantees that model ownership can verified with a given confidence, so the verification is often empirically determined. 
Their focus is not on designing tests for determining provenance under model customizations. In particular, this is often a challenge for watermarking as they have limited robustness to typical model customizations, with some recent work acknowledging the challenge~\cite{krauss2024clearstamp}.
The closest work to ours is the recent work motivated by similar copyright licensing concerns~\cite{chen2022copy}. The main difference to our approach is that we propose a tester with minimal assumptions of black-box access, while the prior work requires much more extra knowledge such as training and testing dataset and model parameters. Additionally, we evaluate and consider real-world customizations that are available on public repositories.
%
%


\vspace{10pt}
\noindent
\textbf{Hypothesis Tests in Machine Learning Security.}
Statistical hypothesis testing has been recently introduced for issues in machine learning security such as formalizing membership inference attacks via likelihood ratio tests~\cite{carlini2022membership,salem2023sok}, auditing differentially private algorithms~\cite{nasr2023tight,jagielski2020auditing}, property inference~\cite{mahloujifar2022property}, attribute inference~\cite{zhang2021leakage,fredrikson2015model}, or to proposing statistical verification methods for neural networks~\cite{baluta2021scalable}.
In this work, we also phrase the problem of provenance for fine-tuned LLMs as multiple statistical hypothesis testing, but our considered problem formulation is considerably different from those considered in prior works.
%
There are several noteworthy differences in our formulation, compared to all of these other problem domains. In order to upper bound the hypothesis test's false positive rate, we need to have a good estimate of the null hypothesis. For example, in some settings such as the membership inference, estimating the null hypothesis may be computationally expensive since it requires training different models under training datasets with or without given data points. Instead, in our formulation, we have multiple control models and we determine provenance depending on the reference similarity with them.
%
As an example of another difference to privacy related inference tests, the randomness in our tests is only over the inputs given to the model, not training data points. As the choice of the input samples is independent from the training dataset of the model, our tests have soundness with respect to statistical significance. 


%
%
%
%
%
%
%
%
%
%
%
%

\vspace{10pt}
\noindent
\textbf{Customization Techniques.}
Not many works study how fine-tuning or other customization techniques change the output distribution or the features of the pretrained model.
Both training from scatch and fine-tuning optimize a similar training objective and they differ only in their initialization (random vs. pretrained weights).
Because of the non-convex nature of the optimization, it is non-trivial to analytically analyze how their training dynamics converge to different minima. Some works theoretically analyze the effect of initialization (known as implicit regularization) for two-layer networks (or restricted setups), and not for pretraining~\cite{neyshabur2015norm,saxe2014exact,soudry2018implicit}. 
Prior works denote that fine-tuning affects the robustness to out-of-distribution, pointing that some pretrained features get distorted in this process~\cite{kumarfine,radford2021learning,shuttleworth2024lora}.