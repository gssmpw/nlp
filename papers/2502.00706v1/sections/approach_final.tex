\section{Main Approach}
\label{sec:approach}

In this section we provide a more formal introduction of the problem and present the main model provenance tester.

\subsection{Problem Setup}
A large language model (LLM) takes as input a sequence of tokens $x_{<i}=(x_1,\ldots,x_{i-1})$ from a fixed set of tokens $\vocabset$ and outputs the next token $x_i \in \vocabset$, i.e., $f_\theta(x_{<i}) = x_i$. Formally, LLM is denoted with $f_\theta$, defined by its architecture $f$ and its model parameters $\theta$.  Herein, it is sufficient for us to consider the LLM as a mapping $f_\theta:\vocabset^I\rightarrow \vocabset$, where $I$ is an arbitrary sequence length. 
Conditioned on an input sequence $x_{<i}$, the $f_\theta$ induces a probability distribution on the whole vocabulary set $\vocabset$ so each token is assigned a probability score. 
In the sequel, we assume $f_\theta$ chooses the token with the highest probability, so-called temperature $t=0$ case. 
Note, when $f_\theta$ uses higher temperatures $t>0$, the most likely token can still be empirically determined by repeatedly sampling from the induced distribution and selecting the most frequently occurring token.

%
%

%

%
%
%

A model $f_\theta$ can be customized through various techniques to create a new model. The most common approach is fine-tuning, where the original parameters $\theta$ are updated using gradient descent on a new dataset to obtain $\theta'$. Other customization techniques such as mixture-of-experts can involve creating a model $g_{\theta'}$ with different architecture that reuses parts of the architecture of the initial model $f_\theta$. Thus, in the sequel, when addressing customizations and models, we omit parameter notations, and denote the initial (parent) model as $f$, and the derived (child) model as $g$, and say that $(f,g)$ constitutes a model provenance pair. Importantly, these customizations typically require orders of magnitude less computational resources than training from scratch.
%
Under our setup, we have only query access to the models - we can provide input sequences $x_{<i}=(x_1,\ldots,x_{i-1})$ and observe their corresponding output tokens $x_i$. This reflects real-world scenarios where models are accessed through APIs, while keeping internal model details private. 
The model provenance problem consists thus in detecting if $g$ was derived from $f$, given only query access to models. 
\ivica{removed formal 'Defintion', and just added the sentence above.}
\begin{comment}
We are now ready to state the main problem.
\begin{definition}[Model Provenance Problem]
Given only query access to models,  decide if a pair of models $(f,g)$ constitutes a model provenance pair. 
\end{definition}
\ivica{removed "f,g" from "query access to models $f,g$", because we need to query control models as well, not only f and g.}

\prateek{The definition is circular. It may be better to phrase the technical statement as that of testing similarity of the two distributions directly now. Define your notion of similarity first, and then say why you think its reasonable thereafter.}
\ivica{model provenance has nothing to do with similarity, we used similarity to detect, but definition cannot contain similarity.}

\teo{At first I didn't see a problem but I think the issue is that we have "Definition" as if we are formally defining something but it is not really doing that because we don't say what something of the type property P(f,g). If P(f,g) is True, model provenance tester returns True. }

\ivica{Ok, lets remove the formal definitions from the paper, we will only use informal.}
\end{comment}

\paragraph{Threat Model.} Our proposed methods are primarily evaluated for a {\em non-adaptive adversary}. The model customizer is not aware of the strategy of the model provenance tester.
%
They are not aware of the knowledge the tester has, i.e., the control models, the set of randomly sampled inputs, or the parameters of the statistical test.
%
%
On the other hand, one may consider  {\em an adaptive adversary}, i.e.,  customizers that may employ techniques specifically designed to evade provenance detection, especially based on the knowledge of the provenance tester. For instance, they might introduce noise or deliberate alterations to the output distributions to reduce detectable similarities with the parent model. Such adaptive obfuscation pose a significant challenge, as they can diminish the effectiveness of our statistical tests. Developing methods to counteract such adaptive adversarial strategies is beyond the scope of this work.


%
%
%
%
%

%
%


\subsection{Model Provenance Tester}

%
%
%
%
%

We formulate the model provenance question as the problem of checking whether two models are similar. This approach is based on the observation that models derived from one another tend to exhibit higher similarity compared to unrelated models. We now detail how the similarity check is implemented.


Since the tester has only query access to the models, the only data it can collect is from providing inputs (called \emph{prompts}) and analyzing the corresponding output tokens. Furthermore, due to our minimal assumptions and lack of information about the training datasets, the tester queries the models on randomly chosen short sequences designed to avoid predictable (\emph{low-entropy}) continuations. 
For example, the tester avoids prompts like ``To be or not to'' since the next token is highly likely to be ``be'' a common continuation that many models would predict.\footnote{Tokens with low entropy are those for which the next token can be easily predicted by any model, making them unsuitable for distinguishing between models.}
We further denote such prompt space with $\Omega$ and sample from it uniformly at random.


The tester independently samples from $\Omega$ a set of $T$ such prompts, queries each model on the same set of prompts, and then compares their output tokens pairwise. For each prompt, we compare only the first output token generated by each model; however, in principle, $n$-grams (sequences of $n$ consecutive tokens) could also be considered. The \emph{similarity} between two models is then calculated as the proportion $\mu$ of prompts on which the models produce the same output token:
$$\mu=\frac{1}{T}\sum_{j=1}^T \mathds{1}(f(x_{<i}^j)=g(x_{<i}^j))$$



%

However, the ratio $\mu$ by itself may not be sufficient to determine whether $f$ and $g$ are truly similar. We need to understand if the similarity $\mu$ between $g$ and $f$ is higher than what we would expect if $g$ were unrelated to $f$. To make this assessment, we need to establish what level of similarity we should expect between $g$ and models that are unrelated to it but share key characteristics with $f$.
%
We do this by introducing a set of \emph{control models} $C=\{c_1, \ldots, c_m\}$ chosen specifically for $f$. These control models are selected to match $f$'s domain, purpose or capabilities as closely as possible, while being clearly not ancestors of $g$. 
%
They are selected from publicly available models that share similar capabilities with $f$ but have been independently developed using different training datasets, architectures, or approaches. Such suitable control models are readily available in existing model repositories with sufficient variability in their development approaches. 
For instance, if $f$ is a French-language model for generating Python code, appropriate control models would include other French language models, Python code generation models, or ideally other French Python code generation models.
%
On the other hand, if $f$ is a more general LLM such as \texttt{Llama-3.2-1B}, then the control models would include other general LLMs, such as \texttt{Llama-3.2-3B}, \texttt{GPT2-large} and \texttt{Qwen2-1.5B}.
We do not have knowledge and influence beyond the domain or capabilities (e.g., ``general'' vs. ``'French'').
%
The number $m$ of control models depends on availability; generally, more control models from $f$'s domain lead to better baseline estimation. We observe empirically that there is enough diversity in existing pre-trained models such that constructing the control set does not require significant computational effort (e.g., additional training) or specialized machine learning expertise. Moreover, if the domain of the parent model $f$ is unknown, one can deduce its domain by manually analyzing the outputs of $f$ on some sample prompts. If such manual analysis is not feasible, we can just consider a larger and more diverse set of LLMs as the control set.
%
For each control model $c_i$, we compute its similarity ratio $\mu_i$ with $g$ using the same process as before. These $\mu_i$ values form our baseline - they tell us how much $g$ typically agrees with models that operate in the same space as $f$ but are definitely not its ancestors. If the similarity $\mu$ between $g$ and $f$ significantly exceeds these baseline similarities $\mu_i$, this suggests a provenance relationship. 


The final step of the tester is to verify that the similarity ratio $\mu$ between $f$ and $g$ exceeds all similarity ratios $\mu_i$. However, we want to ensure this is not merely due to random chance, but rather reflects a true difference. To establish such theoretical guarantees, we employ multiple hypothesis testing.
More precisely, for each control model $c_i$, we formulate the following hypothesis test $H^i$:
\begin{align*}
H_0^i: & \quad \mu \leq \mu_i, \\
H_1^i: & \quad \mu > \mu_i,
\end{align*}
where $H_0^i$ is the null hypothesis that the similarity between $f$ and $g$ is less than or equal to the similarity between $c_i$ and $g$, and $H_1^i$ is the alternative hypothesis that the similarity between $f$ and $g$ is greater.  
%
To test each of the hypothesis $H^i$, we employ a z-test, which is a standard statistical test well-suited for comparing proportions like our similarity ratios when working with large samples. The z-test helps us determine if the observed difference between the two proportions ($\mu$ and $\mu_i$) is large enough to be statistically significant, or if it could have occurred by chance.
%
The z-test produces a $p$-value, which represents the probability of observing such a difference in proportions if there were truly no difference between the models (i.e., if the null hypothesis were true). A small $p$-value (typically less than a predetermined significance level $\alpha$) suggests that the observed difference is unlikely to have occurred by chance, indicating that the similarity $\mu$ between $f$ and $g$ is indeed significantly larger than the similarity $\mu_i$ between $g$ and the control model $c_i$.
We want all hypothesis tests to yield small $p$-values, indicating that the similarity $\mu$ is significantly higher than every baseline similarity $\mu_i$. 

The significance level $\alpha$ represents our tolerance for incorrectly concluding that a difference is significant when it actually occurred by chance (a false positive). This threshold is set to a commonly used default value of $0.05$, meaning we accept a $5\%$ risk of claiming a significant difference where none truly exists. In our context, this would mean wrongly concluding that $\mu$ is significantly higher than some $\mu_i$ when the observed difference is merely due to chance.


%
When conducting multiple hypothesis tests simultaneously, we want to maintain this same overall risk level of $\alpha$, regardless of how many tests we perform. However, running multiple tests increases our chances of obtaining at least one false positive across all tests (known as the family-wise error rate FWER). To control this cumulative risk, we employ the Holm-Bonferroni method~\cite{holm1979simple}, which adjusts the significance thresholds $\alpha_k$ for individual tests $H^k$ to ensure the overall false positive rate remains at or below our desired level of $\alpha$.
The Holm-Bonferroni procedure works as follows.
We first sort the individual $p$-values in ascending order: $p_{(1)} \leq p_{(2)} \leq \dots \leq p_{(m)}$. We then compare each p-value $p_{(k)}$ to its adjusted significance level $\alpha_k = \alpha / (m - k + 1)$. Starting with the smallest $p$-value, we sequentially test each hypothesis. 
If a $p$-value $p_{(k)}$ is less than or equal to its corresponding $\alpha_k$, we reject the null hypothesis $H_0^{(k)}$ and proceed to the next one. This process continues until we encounter a $p$-value that exceeds its adjusted significance level, at which point we fail to reject the remaining null hypotheses. To conclude that $\mu$ is significantly greater than all $\mu_i$, we need to reject all null hypotheses $H_0^i$. By carefully controlling the FWER and ensuring that all null hypotheses are rejected through this method, we provide strong statistical evidence that the similarity between $f$ and $g$ is higher than that between $g$ and any control model $c_i$. This supports the assertion that $(f, g)$ has significantly higher similarity and thus constitutes a model provenance pair.

\input{algorithms/alg_basic_tester}


A pseudo-code of the tester is given in Algorithm~\ref{alg:basic_tester}. In summary, when the procedure returns True, it ensures that the total family-wise error rate (FWER) is controlled at the significance level of $\alpha=0.05$  or lower. This means we can confidently state that the similarity between $f$ and $g$ is significantly higher than between $g$ and any control model, supporting the existence of a provenance relationship. Conversely, when the procedure returns False, it indicates that we could not establish this higher similarity with the desired level of statistical significance. This may occur either because there is genuinely no significant similarity indicative of provenance or because the test lacked sufficient power under the given parameters (e.g., sample size or number of prompts) to detect it.


\input{algorithms/alg_unknown_tester}


\paragraph{Extended Model Provenance Test.} The above provenance problem described thus far assumes we have one candidate parent $f$ that we want to test our child model $g$ against. 
We now extend it to the general case when there is a set of parents, which we refer to as the extended model provenance problem (with unspecified parent). In this problem, given only query access to models, the goal is to determine whether a model $g$ is derived from some model from the set ${f_1,\ldots,f_s}$ of candidate parent models.

While running the basic tester $s$ times (once for each provenance pair $(f_i,g)$) would solve the extended parent problem, this approach besides requiring more effort, also would require additional correction for multiple testing to maintain the same level of confidence. The probability of false positives would grow with the number of candidate parents $s$ unless appropriate adjustments (such as Holm-Bonferroni correction) are made to the significance level. 
%
We thus consider improved tester given in Algorithm~\ref{alg:unknown_parent_tester}. Our  tester avoids this issue by conducting a single set of hypothesis tests after identifying the most similar candidate.
%
It works as follows. First it finds the most similar model to the given model $g$ among all the control models $C$ and candidate parents $F$. If that model is a control model, the algorithm terminates with False. Otherwise, it goes on to test whether the FWER of this model is overall below $\alpha$, the desired significance level. The test for the latter is the same as in Algorithm~\ref{alg:basic_tester}, except now all the alternate hypotheses (including control and candidates) are in the family tested against. When the algorithm return True, it has the guarantee that the most similar model is one of the candidate models and that the total significance level across all hypotheses meets the threshold $\alpha$.



\subsection{Understanding Sources of Error}
\label{sec:analysis}

Our provenance tester can make two types of errors: false positives, where it incorrectly identifies a provenance relationship between independently developed models, and false negatives, where it fails to detect an actual derivation relationship between models. 

These errors arise in part from our statistical hypothesis tests. When performing such tests, we can control false positives by setting a stricter significance level $\alpha$ (requiring stronger evidence before claiming excess similarity), while we can reduce false negatives by increasing the sample size $T$ (more samples provide better power to detect true similarities).

Besides errors introduced by statistical tests, our approach of testing provenance through similarity detection may introduce additional errors. Since we claim provenance only when we detect higher-than-expected similarity between models, we need to examine what this approach implies for our error analysis.
%
This approach relies on two key assumptions that can impact error rates:
\paragraph{Assumption 1: Derivation implies similarity.}
We assume that when model $g$ is derived from $f$, they will exhibit above-average similarity in their outputs. This assumption leads to two potential types of errors:
\begin{itemize}
\item
False negatives occur when a derived model shows insufficient similarity to its parent. This can happen when a model customizer applies extensive modifications that significantly alter the model's behavior. While resource constraints typically prevent such extreme modifications (as they would approach the cost of training from scratch), some legitimate derivation relationships may still go undetected.
\item
False positives arise when independently developed models exhibit high similarity. This typically happens when models are trained on similar datasets or designed for similar specialized tasks - for instance, two independent medical diagnosis models may produce very similar outputs due to their shared domain constraints.
\end{itemize}
\paragraph{Assumption 2: Control models establish a valid baseline.} 
We assume our control models provide a reliable baseline for the similarity we should expect between unrelated models. Poor selection of control models can lead to two types of errors:
\begin{itemize}
\item
False positives occur when our control models are too dissimilar from the domain of $f$. For example, using general language models as controls for specialized code generation models sets an artificially low baseline, making normal domain-specific similarities appear significant.
\item
False negatives happen when control models are themselves derived from $f$ or trained on very similar data. This establishes an artificially high baseline that masks genuine derivation relationships.
\end{itemize}

The overall error rates of our tester depend on the combination of errors from both our statistical hypothesis testing and the two core assumptions. While we can provide theoretical guarantees for controlling error rates in hypothesis testing through parameters $\alpha$ and $T$, we cannot derive analytical bounds for errors arising from the assumptions about derivation implying similarity or the validity of the control model baseline. These assumption-based error rates can only be evaluated empirically. However, our extensive experiments in Section \ref{sec:eval} demonstrate that these assumptions hold well in practice across a wide range of models, suggesting that our approach of provenance testing to similarity detection is sound for real-world applications with non-adaptive adversaries.
