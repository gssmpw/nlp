\section{Introduction}
\label{sec:introduction}

%
Platforms such as Amazon SageMaker and Hugging Face have enabled wide scale distribution of ML models, most notably large language models (LLMs)~\cite{roziere2023code,wu2024pmc-huggingface,Intelligence2024}. 
%
Certain models, called {\em foundational models}, require extensive computational resources and datasets to train. For instance, it was reported in~\cite{meta-cluster} that Meta used two $24,000$ GPU clusters to train Llama 3. But foundational models are relatively cheap to {\em fine-tune} or customize for downstream applications~\cite{kenton2019bert,wu2021newsbert}. We are thus seeing a proliferation of fine-tuned models.

The increase of publicly available foundation models and datasets, however, has also triggered concerns over unauthorized use of intellectual property (IP) and concerns of compromised datasets~\cite{carlini2024poisoning} or models~\cite{hubinger2024sleeper}.
%
These issues are present not just in open-source ecosystems, but also for proprietary models that are hidden behind APIs~\cite{anil2023palm,chatgpt-api}. For instance, concerns about model stealing attacks wherein one can extract the model parameters even for production-level models are on the rise~\cite{carlini2024stealing,tramer2016stealing,orekondy2019knockoff}.
%
Similarly, there is a growing concern that proprietary models may contain backdoors or vulnerabilities, making them susceptible to jailbreaking~\cite{hubinger2024sleeper,anil2024many,zou2023universal}. 
Despite best efforts to create a safe environment for the development of (public and commercial) foundation models, there have already been instances of reported misuse~\cite{china2024llm,payload-malicious,childabuse-diffusion}.

This landscape highlights the growing need for {\em model provenance testing}. The problem of model provenance testing is as follows: 
Determine whether a target model has been derived from a foundational model by lighter customization such as fine-tuning.
%
This problem has applications in tracking reuse of models not just in open marketplaces but also across product teams in large organizations. Current policy regulations such as GDPR~\cite{GDPR2016a} and Artificial Intelligence (AI) Safety Act~\cite{council2024regulation} require security and privacy compliance for all AI-enabled apps~\cite{falco2021governing}. When a security or privacy audit finds a problem with a foundational model, it becomes important to identify which other models in use by the organization may be derived from the problematic one and take appropriate remedial actions (e.g. revoke, retrain, or fortify) to mitigate the risk of further non-compliant use. Model provenance tracking is often useful after the fine-tuned model has been deployed, and when authentic ground truth is unavailable or unreliable.




%
One challenging aspect of designing a model provenance tester is achieving high accuracy. There is a cost associated with a provenance verdict. For instance, as a result of provenance tracking, a company may initiate legal action or investigation. For use cases within the same organization, developers might have to revoke the use of an existing model and even retrain from a clean parent model. False positives, i.e., the deployed LLM is wrongly flagged as a derivation of a problematic LLM, thus entail a downstream cost. At the same time, false negatives, i.e., not being able to flag that the LLM is customized from a problematic parent, also increase the risk of non-compliance. Therefore, we want a principled way to decide provenance and to make accuracy trade-offs.

%
%
Another challenge is that a practical provenance tool needs to have {\em minimal assumptions} to be readily usable in many post-deployment settings. We focus on techniques that do not change typical training and data pipelines, and can be integrated for current state-of-the-art LLMs. The tester is expected to only have {\em black-box query} access to the models and has no additional information, such as the training dataset, test set, or the algorithm used for training. We are not aware of any prior work addressing the question of model provenance testing systematically and in such practical setups.


In this paper, we design the first practical model provenance tester for 
LLMs that requires only query access.
Our proposed techniques stem from a key empirical observation: The output distribution of fine-tuned models is often close to that of their parent model. This distance between a model and its true parent is typically smaller than that between the model and another unrelated models, making it possible to reliably trace back a derived model to the original parent. %
In order to keep assumptions to a minimum, we propose to employ the principled framework of statistical hypothesis testing. 
Specifically, we use black-box sampling and estimation to determine whether the distribution captured by the given model is close to that of the candidate parent. Such estimation can provide formal statistical significance measures, which can be used to check for the {\em null hypothesis}, i.e., the customized LLM is not 
close to the given parent model.
%

We conduct an extensive empirical evaluation across two comprehensive benchmarks comprising over $600$ models from Hugging Face, ranging from $30$M to $4$B model parameters. Our tester achieves $90-95$\% precision and $80-90$\% recall in detecting model provenance relationships, even with a limited number of queries. Therefore, while being simple, we find that our proposed method achieves excellent accuracy.  

%
%
%

%


\vspace{10pt}
\noindent
\textbf{Contributions.} We present the problem of model provenance testing. Our work initiates the study of such testers in the context of customized LLMs, and keeps assumptions minimal, i.e., having only black-box query access to models. We show that in the existing landscape of open-source LLM models, it is practical to fairly accurately determine provenance.

%