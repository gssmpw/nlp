\section{Additional Tables}
\label{sec:appendix:tables}


\begin{table}[h]
\setlength{\tabcolsep}{5pt}
%
\begin{center}
\caption{Precision and recall of the provenance tester on \bencho and \bencht with five different sets of 1,000 prompts.}
\label{tab:eval:random_sampling}
%
\resizebox{0.7\columnwidth}{!}{%
\begin{tabular}{c|c|c|c|c}
\hline
run & \multicolumn{2}{c|}{\bencho} & \multicolumn{2}{c}{\bencht}   \\
\hline 
& precision & recall & precision & recall \\
\hline
1 & 1.00 & 0.83 &   0.93 & 0.67 \\
2 & 0.99 & 0.83 &   0.94 & 0.68 \\
3 & 0.98 & 0.86 &   0.95 & 0.67 \\
4 & 1.00 & 0.83 &   0.95 & 0.67 \\
5 & 1.00 & 0.83 &   0.94 & 0.66 \\
 \end{tabular}
 }
\end{center}
\end{table}


\begin{table}[h]
\setlength{\tabcolsep}{2pt}
%
\begin{center}
\caption{Most similar pre-trained models from \bencht sorted for $k=1$ (no advanced prompt sampling), and their corresponding values for $k=4,16, 64$.}
\label{tab:eval:similarity_pre_train}
\footnotesize
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|l|l|l|l|l}
\hline
Model 1 & Model 2 & $k=1$  & $k=4$ & $k=16$ & $k=64$ \\
\hline
 \texttt{gpt2-large                    } & \texttt{gpt2-medium                   } & 0.64 & 0.36 & 0.22 & 0.16 \\ 
 \texttt{gpt2-large                    } & \texttt{megatron-gpt2-345m            } & 0.64 & 0.38 & 0.25 & 0.15 \\ 
 \texttt{pythia-410m-deduped           } & \texttt{pythia-410m                   } & 0.62 & 0.37 & 0.24 & 0.15 \\ 
 \texttt{gpt2-medium                   } & \texttt{megatron-gpt2-345m            } & 0.62 & 0.34 & 0.22 & 0.15 \\ 
 \texttt{Qwen1.5-0.5B                  } & \texttt{Sailor-0.5B                   } & 0.61 & 0.35 & 0.20 & 0.17 \\ 
\ldots & \ldots & \ldots  & \ldots  & \ldots  & \ldots \\
 \hline
 \multicolumn{2}{r|}{average}           & 0.33 & 0.13 & 0.08 & 0.06\\
 \end{tabular}
}
\end{center}
\end{table}


