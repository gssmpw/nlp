% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@misc{zeng2024scalingsearchlearningroadmap,
      title={Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective}, 
      author={Zhiyuan Zeng and Qinyuan Cheng and Zhangyue Yin and Bo Wang and Shimin Li and Yunhua Zhou and Qipeng Guo and Xuanjing Huang and Xipeng Qiu},
      year={2024},
      eprint={2412.14135},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.14135}, 
}

@article{zhang2024supervised,
  title={Supervised chain of thought},
  author={Zhang, Xiang and Ding, Dujian},
  journal={arXiv preprint arXiv:2410.14198},
  year={2024}
}
@article{wang2023large,
  title={Large language models are not fair evaluators},
  author={Wang, Peiyi and Li, Lei and Chen, Liang and Cai, Zefan and Zhu, Dawei and Lin, Binghuai and Cao, Yunbo and Liu, Qi and Liu, Tianyu and Sui, Zhifang},
  journal={arXiv preprint arXiv:2305.17926},
  year={2023}
}

@article{liu2024aligning,
  title={Aligning with human judgement: The role of pairwise preference in large language model evaluators},
  author={Liu, Yinhong and Zhou, Han and Guo, Zhijiang and Shareghi, Ehsan and Vuli{\'c}, Ivan and Korhonen, Anna and Collier, Nigel},
  journal={arXiv preprint arXiv:2403.16950},
  year={2024}
}
@misc{zhang2024supervisedchainthought,
      title={Supervised Chain of Thought}, 
      author={Xiang Zhang and Dujian Ding},
      year={2024},
      eprint={2410.14198},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.14198}, 
}

@misc{wei2023chainofthoughtpromptingelicitsreasoning,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903}, 
}

@inproceedings{
madaan2023selfrefine,
title={Self-Refine: Iterative Refinement with Self-Feedback},
author={Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=S37hOerQLB}
}

@misc{wang2024mixtureofagentsenhanceslargelanguage,
      title={Mixture-of-Agents Enhances Large Language Model Capabilities}, 
      author={Junlin Wang and Jue Wang and Ben Athiwaratkun and Ce Zhang and James Zou},
      year={2024},
      eprint={2406.04692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.04692}, 
}
@misc{zhang2024restmctsllmselftrainingprocess,
      title={ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search}, 
      author={Dan Zhang and Sining Zhoubian and Ziniu Hu and Yisong Yue and Yuxiao Dong and Jie Tang},
      year={2024},
      eprint={2406.03816},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.03816}, 
}
@misc{zhang2024llamaberrypairwiseoptimizationo1like,
      title={LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning}, 
      author={Di Zhang and Jianbo Wu and Jingdi Lei and Tong Che and Jiatong Li and Tong Xie and Xiaoshui Huang and Shufei Zhang and Marco Pavone and Yuqiang Li and Wanli Ouyang and Dongzhan Zhou},
      year={2024},
      eprint={2410.02884},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.02884}, 
}
@misc{tian2024selfimprovementllmsimaginationsearching,
      title={Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing}, 
      author={Ye Tian and Baolin Peng and Linfeng Song and Lifeng Jin and Dian Yu and Haitao Mi and Dong Yu},
      year={2024},
      eprint={2404.12253},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.12253}, 
}

@article{wang2024q,
  title={Q*: Improving multi-step reasoning for llms with deliberative planning},
  author={Wang, Chaojie and Deng, Yanchen and Lyu, Zhiyi and Zeng, Liang and He, Jujie and Yan, Shuicheng and An, Bo},
  journal={arXiv preprint arXiv:2406.14283},
  year={2024}
}

@inproceedings{
zhuang2024toolchain,
title={ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search},
author={Yuchen Zhuang and Xiang Chen and Tong Yu and Saayan Mitra and Victor Bursztyn and Ryan A. Rossi and Somdeb Sarkhel and Chao Zhang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=B6pQxqUcT8}
}

@inproceedings{
chen2024are,
title={Are More {LLM} Calls All You Need? Towards the Scaling Properties of Compound {AI} Systems},
author={Lingjiao Chen and Jared Quincy Davis and Boris Hanin and Peter Bailis and Ion Stoica and Matei Zaharia and James Zou},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=m5106RRLgx}
}

@misc{gao2022scalinglawsrewardmodel,
      title={Scaling Laws for Reward Model Overoptimization}, 
      author={Leo Gao and John Schulman and Jacob Hilton},
      year={2022},
      eprint={2210.10760},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.10760}, 
}


@inproceedings{
wu2024scaling,
title={Scaling Inference Computation: Compute-Optimal Inference for Problem-Solving with Language Models},
author={Yangzhen Wu and Zhiqing Sun and Shanda Li and Sean Welleck and Yiming Yang},
booktitle={The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24},
year={2024},
url={https://openreview.net/forum?id=j7DZWSc8qu}
}

 @article{Graves_2012,  
     title={Sequence Transduction with Recurrent Neural Networks}, 
     journal={arXiv: Neural and Evolutionary Computing,arXiv: Neural and Evolutionary Computing}, 
     author={Graves, Alex}, 
     year={2012}, 
     month={Nov}, 
     language={en-US} 
 }

@article{Jurafsky_Martin_1999,   title={Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},  journal={Prentice Hall eBooks,Prentice Hall eBooks},  author={Jurafsky, Daniel and Martin, JamesH.},  year={1999},  month={Dec},  language={en-US}  }

@article{ACKLEY1985147,
title = {A learning algorithm for boltzmann machines},
journal = {Cognitive Science},
volume = {9},
number = {1},
pages = {147-169},
year = {1985},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(85)80012-4},
url = {https://www.sciencedirect.com/science/article/pii/S0364021385800124},
author = {David H. Ackley and Geoffrey E. Hinton and Terrence J. Sejnowski}}

@inproceedings{DBLP:conf/iclr/HoltzmanBDFC20,
  author       = {Ari Holtzman and
                  Jan Buys and
                  Li Du and
                  Maxwell Forbes and
                  Yejin Choi},
  title        = {The Curious Case of Neural Text Degeneration},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=rygGQyrFvH},
  timestamp    = {Sat, 29 Apr 2023 10:09:26 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/HoltzmanBDFC20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{li-etal-2023-making,
    title = "Making Language Models Better Reasoners with Step-Aware Verifier",
    author = "Li, Yifei  and
      Lin, Zeqi  and
      Zhang, Shizhuo  and
      Fu, Qiang  and
      Chen, Bei  and
      Lou, Jian-Guang  and
      Chen, Weizhu",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.291/",
    doi = "10.18653/v1/2023.acl-long.291",
    pages = "5315--5333",
    abstract = "Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9{\%} to 58.1{\%} in problem-solving rate. In this paper, we present DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DiVeRSe has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DiVeRSe on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4{\%} to 83.2{\%})."
}

@inproceedings{
wang2023selfconsistency,
title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc V Le and Ed H. Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=1PL1NIMMrw}
}

@misc{learningtoreason2024,
  title        = {Learning to reason with LLMs},
  author       = {OpenAI},
  year         = 2024,
  note         = {\url{https://openai.com/index/learning-to-reason-with-llms/} [Accessed: 01/08/2025]}
}

@misc{zeng2024scalingsearchlearningroadmap,
      title={Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective}, 
      author={Zhiyuan Zeng and Qinyuan Cheng and Zhangyue Yin and Bo Wang and Shimin Li and Yunhua Zhou and Qipeng Guo and Xuanjing Huang and Xipeng Qiu},
      year={2024},
      eprint={2412.14135},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.14135}, 
}

@misc{brown2024largelanguagemonkeysscaling,
      title={Large Language Monkeys: Scaling Inference Compute with Repeated Sampling}, 
      author={Bradley Brown and Jordan Juravsky and Ryan Ehrlich and Ronald Clark and Quoc V. Le and Christopher Ré and Azalia Mirhoseini},
      year={2024},
      eprint={2407.21787},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.21787}, 
}

@misc{stroebl2024inferencescalingflawslimits,
      title={Inference Scaling fLaws: The Limits of LLM Resampling with Imperfect Verifiers}, 
      author={Benedikt Stroebl and Sayash Kapoor and Arvind Narayanan},
      year={2024},
      eprint={2411.17501},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.17501}, 
}

@article{xiang2025towards,
  title={Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Though},
  author={Xiang, Violet and Snell, Charlie and Gandhi, Kanishk and Albalak, Alon and Singh, Anikait and Blagden, Chase and Phung, Duy and Rafailov, Rafael and Lile, Nathan and Mahan, Dakota and others},
  journal={arXiv preprint arXiv:2501.04682},
  year={2025}
}

@misc{snell2024scalingllmtesttimecompute,
      title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters}, 
      author={Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
      year={2024},
      eprint={2408.03314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.03314}, 
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@misc{zelikman2024quietstarlanguagemodelsteach,
      title={Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking}, 
      author={Eric Zelikman and Georges Harik and Yijia Shao and Varuna Jayasiri and Nick Haber and Noah D. Goodman},
      year={2024},
      eprint={2403.09629},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.09629}, 
}

@misc{yao2023reactsynergizingreasoningacting,
      title={ReAct: Synergizing Reasoning and Acting in Language Models}, 
      author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
      year={2023},
      eprint={2210.03629},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.03629}, 
}

@misc{agarwal2024manyshotincontextlearning,
      title={Many-Shot In-Context Learning}, 
      author={Rishabh Agarwal and Avi Singh and Lei M. Zhang and Bernd Bohnet and Luis Rosias and Stephanie Chan and Biao Zhang and Ankesh Anand and Zaheer Abbas and Azade Nova and John D. Co-Reyes and Eric Chu and Feryal Behbahani and Aleksandra Faust and Hugo Larochelle},
      year={2024},
      eprint={2404.11018},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.11018}, 
}

@misc{zhao2025samplescrutinizescaleeffective,
      title={Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification}, 
      author={Eric Zhao and Pranjal Awasthi and Sreenivas Gollapudi},
      year={2025},
      eprint={2502.01839},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.01839}, 
}

@misc{muennighoff2025s1simpletesttimescaling,
      title={s1: Simple test-time scaling}, 
      author={Niklas Muennighoff and Zitong Yang and Weijia Shi and Xiang Lisa Li and Li Fei-Fei and Hannaneh Hajishirzi and Luke Zettlemoyer and Percy Liang and Emmanuel Candès and Tatsunori Hashimoto},
      year={2025},
      eprint={2501.19393},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.19393}, 
}

@misc{liu2024reifereevaluatinginstructionfollowingevaluation,
      title={ReIFE: Re-evaluating Instruction-Following Evaluation}, 
      author={Yixin Liu and Kejian Shi and Alexander R. Fabbri and Yilun Zhao and Peifeng Wang and Chien-Sheng Wu and Shafiq Joty and Arman Cohan},
      year={2024},
      eprint={2410.07069},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.07069}, 
}

@inproceedings{liu-etal-2024-benchmarking,
    title = "Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization",
    author = "Liu, Yixin  and
      Fabbri, Alexander  and
      Chen, Jiawen  and
      Zhao, Yilun  and
      Han, Simeng  and
      Joty, Shafiq  and
      Liu, Pengfei  and
      Radev, Dragomir  and
      Wu, Chien-Sheng  and
      Cohan, Arman",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.280/",
    doi = "10.18653/v1/2024.findings-naacl.280",
    pages = "4481--4501",
    abstract = "While large language models (LLMs) can already achieve strong performance on standard generic summarization benchmarks, their performance on more complex summarization task settings is less studied. Therefore, we benchmark LLMs on instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for desired summary characteristics. To this end, we curate an evaluation-only dataset for this task setting and conduct human evaluations of five LLM-based systems to assess their instruction-following capabilities in controllable summarization. We then benchmark LLM-based automatic evaluation for this task with 4 different evaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) no LLM-based evaluation methods can achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance gaps in summary generation and evaluation capabilities. We make our collected benchmark InstruSum publicly available to facilitate future research in this direction."
}

@inproceedings{xiao-etal-2024-personalized,
    title = "Personalized Abstractive Summarization by Tri-agent Generation Pipeline",
    author = "Xiao, Wen  and
      Xie, Yujia  and
      Carenini, Giuseppe  and
      He, Pengcheng",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2024",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-eacl.39/",
    pages = "570--581",
    abstract = "Tailoring outputs from large language models, like ChatGPT, to implicit user preferences remains a challenge despite their impressive generative capabilities. In this paper, we propose a tri-agent generation pipeline comprising a generator, an instructor, and an editor to enhance output personalization. The generator produces an initial output, the instructor automatically generates editing instructions based on user preferences, and the editor refines the output to align with those preferences. The inference-only large language model (ChatGPT) serves as both the generator and editor, with a smaller model acting as the instructor to guide output generation. We train the instructor using editor-steered reinforcement learning, leveraging feedback from a large-scale editor model to optimize instruction generation. Experimental results on two abstractive summarization datasets demonstrate the effectiveness of our approach in generating outputs that better meet user expectations."
}
@inproceedings{jin2024contranovo,
  title={Contranovo: A contrastive learning approach to enhance de novo peptide sequencing},
  author={Jin, Zhi and Xu, Sheng and Zhang, Xiang and Ling, Tianze and Dong, Nanqing and Ouyang, Wanli and Gao, Zhiqiang and Chang, Cheng and Sun, Siqi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  pages={144--152},
  year={2024}
}
@article{zhang2024counting,
  title={Counting Ability of Large Language Models and Impact of Tokenization},
  author={Zhang, Xiang and Cao, Juntai and You, Chenyu},
  journal={arXiv preprint arXiv:2410.19730},
  year={2024}
}
@article{yin2023ttida,
  title={TTIDA: Controllable generative data augmentation via text-to-text and text-to-image models},
  author={Yin, Yuwei and Kaddour, Jean and Zhang, Xiang and Nie, Yixin and Liu, Zhenguang and Kong, Lingpeng and Liu, Qi},
  journal={arXiv preprint arXiv:2304.08821},
  year={2023}
}
@inproceedings{zhang2023bridging,
  title={Bridging the Gap Between BabelNet and HowNet: Unsupervised Sense Alignment and Sememe Prediction},
  author={Zhang, Xiang and Shi, Ning and Hauer, Bradley and Kondrak, Grzegorz},
  booktitle={Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={2789--2798},
  year={2023}
}
@article{zhang2024autoregressive+,
  title={Autoregressive+ Chain of Thought= Recurrent: Recurrence's Role in Language Models' Computability and a Revisit of Recurrent Transformer},
  author={Zhang, Xiang and Abdul-Mageed, Muhammad and Lakshmanan, Laks VS},
  journal={arXiv preprint arXiv:2409.09239},
  year={2024}
}
@article{liu2022character,
  title={A character-level length-control algorithm for non-autoregressive sentence summarization},
  author={Liu, Puyuan and Zhang, Xiang and Mou, Lili},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={29101--29112},
  year={2022}
}

@article{zhang2024cross,
  title={Cross-Modal Consistency in Multimodal Large Language Models},
  author={Zhang, Xiang and Li, Senyu and Shi, Ning and Hauer, Bradley and Wu, Zijun and Kondrak, Grzegorz and Abdul-Mageed, Muhammad and Lakshmanan, Laks VS},
  journal={arXiv preprint arXiv:2411.09273},
  year={2024}
}

@inproceedings{zhang-etal-2023-dont,
    title = "Don`t Trust {C}hat{GPT} when your Question is not in {E}nglish: A Study of Multilingual Abilities and Types of {LLM}s",
    author = "Zhang, Xiang  and
      Li, Senyu  and
      Hauer, Bradley  and
      Shi, Ning  and
      Kondrak, Grzegorz",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.491/",
    doi = "10.18653/v1/2023.emnlp-main.491",
    pages = "7915--7927",
    abstract = "Large language models (LLMs) have demonstrated exceptional natural language understanding abilities, and have excelled in a variety of natural language processing (NLP) tasks. Despite the fact that most LLMs are trained predominantly on English, multiple studies have demonstrated their capabilities in a variety of languages. However, fundamental questions persist regarding how LLMs acquire their multilingual abilities and how performance varies across different languages. These inquiries are crucial for the study of LLMs since users and researchers often come from diverse language backgrounds, potentially influencing how they use LLMs and interpret their output. In this work, we propose a systematic way of qualitatively and quantitatively evaluating the multilingual capabilities of LLMs. We investigate the phenomenon of cross-language generalization in LLMs, wherein limited multilingual training data leads to advanced multilingual capabilities. To accomplish this, we employ a novel prompt back-translation method. The results demonstrate that LLMs, such as GPT, can effectively transfer learned knowledge across different languages, yielding relatively consistent results in translation-equivariant tasks, in which the correct output does not depend on the language of the input. However, LLMs struggle to provide accurate results in translation-variant tasks, which lack this property, requiring careful user judgment to evaluate the answers."
}

@article{zhang-etal-2024-benchmarking,
    title = "Benchmarking Large Language Models for News Summarization",
    author = "Zhang, Tianyi  and
      Ladhak, Faisal  and
      Durmus, Esin  and
      Liang, Percy  and
      McKeown, Kathleen  and
      Hashimoto, Tatsunori B.",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "12",
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.tacl-1.3/",
    doi = "10.1162/tacl_a_00632",
    pages = "39--57",
    abstract = "Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, not model size, is the key to the LLM`s zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human written summaries."
}

@misc{song2025learningsummarizellmgeneratedfeedback,
      title={Learning to Summarize from LLM-generated Feedback}, 
      author={Hwanjun Song and Taewon Yun and Yuho Lee and Jihwan Oh and Gihun Lee and Jason Cai and Hang Su},
      year={2025},
      eprint={2410.13116},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.13116}, 
}

@misc{pu2023summarizationalmostdead,
      title={Summarization is (Almost) Dead}, 
      author={Xiao Pu and Mingqi Gao and Xiaojun Wan},
      year={2023},
      eprint={2309.09558},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.09558}, 
}

@article{vanveen2024clinical,
  title={Adapted Large Language Models Can Outperform Medical Experts in Clinical Text Summarization},
  author={Van Veen, Dave and Van Uden, Cara and Blankemeier, Louis and Delbrouck, Jean-Benoit and Aali, Asad and Bluethgen, Christian and Pareek, Anuj and Polacin, Malgorzata and Collins, William and Ahuja, Neera and Langlotz, Curtis P. and Hom, Jason and Gatidis, Sergios and Pauly, John and Chaudhari, Akshay S.},
  journal={Nature Medicine},
  year={2024},
  doi={10.1038/s41591-024-02855-5},
  url={https://doi.org/10.1038/s41591-024-02855-5},
  published={27 February 2024}
}

@misc{belem2024singlemultillmshallucinate,
      title={From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization}, 
      author={Catarina G. Belem and Pouya Pezeskhpour and Hayate Iso and Seiji Maekawa and Nikita Bhutani and Estevam Hruschka},
      year={2024},
      eprint={2410.13961},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.13961}, 
}

@inproceedings{liu-etal-2023-towards-interpretable,
    title = "Towards Interpretable and Efficient Automatic Reference-Based Summarization Evaluation",
    author = "Liu, Yixin  and
      Fabbri, Alexander  and
      Zhao, Yilun  and
      Liu, Pengfei  and
      Joty, Shafiq  and
      Wu, Chien-Sheng  and
      Xiong, Caiming  and
      Radev, Dragomir",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.1018/",
    doi = "10.18653/v1/2023.emnlp-main.1018",
    pages = "16360--16368",
    abstract = "Interpretability and efficiency are two important considerations for the adoption of neural automatic metrics. In this work, we develop strong-performing automatic metrics for reference-based summarization evaluation, based on a two-stage evaluation pipeline that first extracts basic information units from one text sequence and then checks the extracted units in another sequence. The metrics we developed include two-stage metrics that can provide high interpretability at both the fine-grained unit level and summary level, and one-stage metrics that achieve a balance between efficiency and interpretability. We make the developed tools publicly available at https://github.com/Yale-LILY/AutoACU."
}

@misc{shi2024judgingjudgessystematicstudy,
      title={Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge}, 
      author={Lin Shi and Chiyu Ma and Wenhua Liang and Weicheng Ma and Soroush Vosoughi},
      year={2024},
      eprint={2406.07791},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.07791}, 
}

@inproceedings{wang-etal-2024-large-language-models-fair,
    title = "Large Language Models are not Fair Evaluators",
    author = "Wang, Peiyi  and
      Li, Lei  and
      Chen, Liang  and
      Cai, Zefan  and
      Zhu, Dawei  and
      Lin, Binghuai  and
      Cao, Yunbo  and
      Kong, Lingpeng  and
      Liu, Qi  and
      Liu, Tianyu  and
      Sui, Zhifang",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.511/",
    doi = "10.18653/v1/2024.acl-long.511",
    pages = "9440--9450",
    abstract = "In this paper, we uncover a positional bias in the evaluation paradigm of adopting large language models (LLMs), e.g., GPT-4, as a referee to score and compare the quality of responses generated by candidate models. We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context. This manipulation allows us to skew the evaluation result, making one model appear considerably superior to the other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator. We propose a simple yet effective calibration framework to address our discovered positional bias.To evaluate the effectiveness of our framework, we manually annotate the {\textquotedblleft}win/tie/lose{\textquotedblright} outcomes of responses from ChatGPT and Vicuna-13B in the Vicuna Benchmark`s question prompt. Extensive experiments demonstrate that our approach successfully alleviates evaluation bias, resulting in closer alignment with human judgments."
}

@inproceedings{li-etal-2024-split,
    title = "Split and Merge: Aligning Position Biases in {LLM}-based Evaluators",
    author = "Li, Zongjie  and
      Wang, Chaozheng  and
      Ma, Pingchuan  and
      Wu, Daoyuan  and
      Wang, Shuai  and
      Gao, Cuiyun  and
      Liu, Yang",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.621/",
    doi = "10.18653/v1/2024.emnlp-main.621",
    pages = "11084--11108",
    abstract = "Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, taking into account both length and semantics, and merges them back into a single prompt for evaluation by LLMs. Extensive experiments with six LLMs on 11,520 answer pairs demonstrate that PORTIA markedly enhances the consistency rates for all models and forms of comparison tested, achieving an average relative improvement of 47.46{\%}. It also enables PORTIA-enhanced GPT-3.5 to achieve agreement rates with humans comparable to GPT-4 and elevates GPT-4`s consistency rate up to 98{\%}. Subsequent human evaluations indicate that the PORTIA-enhanced GPT-3.5 model can even surpass standalone GPT-4 in terms of alignment with human evaluators, highlighting PORTIA`s ability to correct position bias, improve LLM consistency, and boost performance while keeping cost efficiency."
}

@misc{li2023makinglargelanguagemodels,
      title={Making Large Language Models Better Reasoners with Step-Aware Verifier}, 
      author={Yifei Li and Zeqi Lin and Shizhuo Zhang and Qiang Fu and Bei Chen and Jian-Guang Lou and Weizhu Chen},
      year={2023},
      eprint={2206.02336},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.02336}, 
}

@misc{arora2022askanythingsimplestrategy,
      title={Ask Me Anything: A simple strategy for prompting language models}, 
      author={Simran Arora and Avanika Narayan and Mayee F. Chen and Laurel Orr and Neel Guha and Kush Bhatia and Ines Chami and Frederic Sala and Christopher Ré},
      year={2022},
      eprint={2210.02441},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.02441}, 
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013/",
    pages = "74--81"
}

@inproceedings{mihalcea-tarau-2004-textrank,
    title = "{T}ext{R}ank: Bringing Order into Text",
    author = "Mihalcea, Rada  and
      Tarau, Paul",
    editor = "Lin, Dekang  and
      Wu, Dekai",
    booktitle = "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-3252/",
    pages = "404--411"
}

@article{Erkan2004LexRankGL,
  title={LexRank: Graph-based Lexical Centrality as Salience in Text Summarization},
  author={G{\"u}nes Erkan and Dragomir R. Radev},
  journal={ArXiv},
  year={2004},
  volume={abs/1109.2128},
  url={https://api.semanticscholar.org/CorpusID:506350}
}

@misc{zhang2020pegasuspretrainingextractedgapsentences,
      title={PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization}, 
      author={Jingqing Zhang and Yao Zhao and Mohammad Saleh and Peter J. Liu},
      year={2020},
      eprint={1912.08777},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1912.08777}, 
}

@inproceedings{liu-lapata-2019-hierarchical,
    title = "Hierarchical Transformers for Multi-Document Summarization",
    author = "Liu, Yang  and
      Lapata, Mirella",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1500/",
    doi = "10.18653/v1/P19-1500",
    pages = "5070--5081",
    abstract = "In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence. Our model learns latent dependencies among textual units, but can also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines."
}

@inproceedings{Giorgi2022OpenDM,
  title={Open Domain Multi-document Summarization: A Comprehensive Study of Model Brittleness under Retrieval},
  author={John Giorgi and Luca Soldaini and Bo Wang and Gary Bader and Kyle Lo and Lucy Lu Wang and Arman Cohan},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:258865156}
}

@inproceedings{bhandari-etal-2020-metrics,
    title = "Metrics also Disagree in the Low Scoring Range: Revisiting Summarization Evaluation Metrics",
    author = "Bhandari, Manik  and
      Gour, Pranav Narayan  and
      Ashfaq, Atabak  and
      Liu, Pengfei",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.501/",
    doi = "10.18653/v1/2020.coling-main.501",
    pages = "5702--5711",
    abstract = "In text summarization, evaluating the efficacy of automatic metrics without human judgments has become recently popular. One exemplar work (Peyrard, 2019) concludes that automatic metrics strongly disagree when ranking high-scoring summaries. In this paper, we revisit their experiments and find that their observations stem from the fact that metrics disagree in ranking summaries from any narrow scoring range. We hypothesize that this may be because summaries are similar to each other in a narrow scoring range and are thus, difficult to rank. Apart from the width of the scoring range of summaries, we analyze three other properties that impact inter-metric agreement - Ease of Summarization, Abstractiveness, and Coverage."
}

@article{Yuan2021BARTScoreEG,
  title={BARTScore: Evaluating Generated Text as Text Generation},
  author={Weizhe Yuan and Graham Neubig and Pengfei Liu},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.11520},
  url={https://api.semanticscholar.org/CorpusID:235593404}
}

@misc{zhang2020bertscoreevaluatingtextgeneration,
      title={BERTScore: Evaluating Text Generation with BERT}, 
      author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
      year={2020},
      eprint={1904.09675},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09675}, 
}

@inproceedings{fabbri-etal-2019-multi,
    title = "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model",
    author = "Fabbri, Alexander  and
      Li, Irene  and
      She, Tianwei  and
      Li, Suyi  and
      Radev, Dragomir",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1102/",
    doi = "10.18653/v1/P19-1102",
    pages = "1074--1084",
    abstract = "Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting."
}

@inproceedings{amar-etal-2023-openasp,
    title = "{O}pen{A}sp: A Benchmark for Multi-document Open Aspect-based Summarization",
    author = "Amar, Shmuel  and
      Schiff, Liat  and
      Ernst, Ori  and
      Shefer, Asi  and
      Shapira, Ori  and
      Dagan, Ido",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.121/",
    doi = "10.18653/v1/2023.emnlp-main.121",
    pages = "1967--1991",
    abstract = "The performance of automatic summarization models has improved dramatically in recent years. Yet, there is still a gap in meeting specific information needs of users in real-world scenarios, particularly when a targeted summary is sought, such as in the useful aspect-based summarization setting targeted in this paper. Previous datasets and studies for this setting have predominantly concentrated on a limited set of pre-defined aspects, focused solely on single document inputs, or relied on synthetic data. To advance research on more realistic scenarios, we introduce OpenAsp, a benchmark for multi-document open aspect-based summarization. This benchmark is created using a novel and cost-effective annotation protocol, by which an open aspect dataset is derived from existing generic multi-document summarization datasets. We analyze the properties of OpenAsp showcasing its high-quality content. Further, we show that the realistic open-aspect setting realized in OpenAsp poses a challenge for current state-of-the-art summarization models, as well as for large language models."
}

@misc{lior2024seamstochasticbenchmarkmultidocument,
      title={SEAM: A Stochastic Benchmark for Multi-Document Tasks}, 
      author={Gili Lior and Avi Caciularu and Arie Cattan and Shahar Levy and Ori Shapira and Gabriel Stanovsky},
      year={2024},
      eprint={2406.16086},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.16086}, 
}

@inproceedings{liu-etal-2023-revisiting,
    title = "Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation",
    author = "Liu, Yixin  and
      Fabbri, Alex  and
      Liu, Pengfei  and
      Zhao, Yilun  and
      Nan, Linyong  and
      Han, Ruilin  and
      Han, Simeng  and
      Joty, Shafiq  and
      Wu, Chien-Sheng  and
      Xiong, Caiming  and
      Radev, Dragomir",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.228/",
    doi = "10.18653/v1/2023.acl-long.228",
    pages = "4140--4170",
    abstract = "Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which is based on fine-grained semantic units and allows for a high inter-annotator agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of 22,000 summary-level annotations over 28 top-performing systems on three datasets. (3) We conduct a comparative study of four human evaluation protocols, underscoring potential confounding factors in evaluation setups. (4) We evaluate 50 automatic metrics and their variants using the collected human annotations across evaluation protocols and demonstrate how our benchmark leads to more statistically stable and significant results. The metrics we benchmarked include recent methods based on large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators' prior, input-agnostic preferences, calling for more robust, targeted evaluation methods."
}

@misc{allenzhu2024physicslanguagemodels31,
      title={Physics of Language Models: Part 3.1, Knowledge Storage and Extraction}, 
      author={Zeyuan Allen-Zhu and Yuanzhi Li},
      year={2024},
      eprint={2309.14316},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.14316}, 
}

@inproceedings{mehdad-etal-2014-abstractive,
    title = "Abstractive Summarization of Spoken and Written Conversations Based on Phrasal Queries",
    author = "Mehdad, Yashar  and
      Carenini, Giuseppe  and
      Ng, Raymond T.",
    editor = "Toutanova, Kristina  and
      Wu, Hua",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P14-1115/",
    doi = "10.3115/v1/P14-1115",
    pages = "1220--1230"
}

@inproceedings{gerani-etal-2014-abstractive,
    title = "Abstractive Summarization of Product Reviews Using Discourse Structure",
    author = "Gerani, Shima  and
      Mehdad, Yashar  and
      Carenini, Giuseppe  and
      Ng, Raymond T.  and
      Nejat, Bita",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1168/",
    doi = "10.3115/v1/D14-1168",
    pages = "1602--1613"
}

@inproceedings{
dubois2024lengthcontrolled,
title={Length-Controlled AlpacaEval: A Simple Debiasing of Automatic Evaluators},
author={Yann Dubois and Percy Liang and Tatsunori Hashimoto},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=CybBmzWBX0}
}

@misc{hu2024explaininglengthbiasllmbased,
      title={Explaining Length Bias in LLM-Based Preference Evaluations}, 
      author={Zhengyu Hu and Linxin Song and Jieyu Zhang and Zheyuan Xiao and Tianfu Wang and Zhengyu Chen and Nicholas Jing Yuan and Jianxun Lian and Kaize Ding and Hui Xiong},
      year={2024},
      eprint={2407.01085},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.01085}, 
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022},
url={https://proceedings.neurips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@misc{li2022humanguidedexploitationinterpretable,
      title={Human Guided Exploitation of Interpretable Attention Patterns in Summarization and Topic Segmentation}, 
      author={Raymond Li and Wen Xiao and Linzi Xing and Lanjun Wang and Gabriel Murray and Giuseppe Carenini},
      year={2022},
      eprint={2112.05364},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2112.05364}, 
}