\section{Related Work}
\subsection{Test-time scaling}
Test-time scaling strategies can be broadly classified into three categories: repeated sampling, deliberative approaches, and self-refinement. Repeated sampling leverages techniques like temperature sampling \citep{ACKLEY1985147}, top-$k$, and top-$p$ sampling \citep{DBLP:conf/iclr/HoltzmanBDFC20} to generate diverse outputs, which are then enhanced through aggregation strategies such as majority voting \citep{wang2023selfconsistency}, weighted majority voting \citep{li-etal-2023-making}, or best-of-$n$ selection \citep{cobbe2021training}. 

Recent work \citep{brown2024largelanguagemonkeysscaling,wu2024scaling,stroebl2024inferencescalingflawslimits,zhao2025samplescrutinizescaleeffective} demonstrates that repeated sampling can significantly expand LLM capabilities across various domains. \textbf{Deliberative approaches} incorporate structured reasoning through methods like chain-of-thought prompting \citep{wei2023chainofthoughtpromptingelicitsreasoning} and tree search. These approaches range from informed search methods \citep{zhuang2024toolchain,wang2024q} to Monte Carlo Tree Search (MCTS) variants \citep{tian2024selfimprovementllmsimaginationsearching,zhang2024llamaberrypairwiseoptimizationo1like,zhang2024restmctsllmselftrainingprocess}. A key characteristic of tree search methods is to use process reward models (PRMs) to guide the search trajectory during generation \citep{yao2023reactsynergizingreasoningacting,zelikman2024quietstarlanguagemodelsteach}. 
\textbf{Self-refinement}  \citep{madaan2023selfrefine} enables models to iteratively improve their responses through self-critique and editing. Additionally, all categories of test-time scaling methods can be enhanced through model ensembling \citep{wang2024mixtureofagentsenhanceslargelanguage,jin2024contranovo,chen2024are} to combine the strengths of multiple models to achieve better performance.

Tree search methods often struggle with the high-dimensional search space created by multiple source documents, making it computationally intensive to explore meaningful trajectories. Self-refinement approaches, which rely on iterative improvements, may lead to information loss as they tend to focus on refining a single perspective rather than maintaining diverse viewpoints from multiple documents. Therefore, we adopt the repeated sampling approach to scale MDS at test time, using diverse prompts to generate multiple perspectives that are then consolidated through specialized aggregation methods.

\subsection{Multi Document Summarization}

Multi-document summarization (MDS) has evolved significantly from traditional methods \citep{Erkan2004LexRankGL,mehdad-etal-2014-abstractive,gerani-etal-2014-abstractive} to modern  approaches powered by neural networks, which introduced encoder-decoder architectures for better summary generation \citep{liu-lapata-2019-hierarchical, zhang2020pegasuspretrainingextractedgapsentences, Giorgi2022OpenDM,li2022humanguidedexploitationinterpretable}.
The advent of LLMs has boosted MDS capabilities even further, with models demonstrating impressive zero- and few-shot performance \citep{zhang-etal-2024-benchmarking}. Recent work has shifted the focus from architectural modifications to improve LLMs' summarization abilities to exploring various prompting strategies \citep{xiao-etal-2024-personalized, liu-etal-2024-benchmarking}. Despite these advances, MDS continues to face challenges including maintaining cross-document consistency, ensuring factual accuracy, and addressing content incompleteness where key information may be omitted \citep{belem2024singlemultillmshallucinate}. In this paper, we propose to tackle these challenges through a scaling approach that leverages prompt ensemble techniques to generate more comprehensive and accurate summaries. 

Traditional evaluation metrics for summarization, such as ROUGE \citep{lin-2004-rouge}, only rely on lexical overlap with reference summaries. These metrics often fail to capture semantic similarity and summary quality adequately \citep{bhandari-etal-2020-metrics}. This limitation has led to the development of learned metrics that better align with human judgments \citep{Yuan2021BARTScoreEG,zhang2020bertscoreevaluatingtextgeneration}.
The emergence of LLMs has enabled even more sophisticated evaluation approaches. Recent work has explored using LLMs as evaluation agents \citep{liu-etal-2024-benchmarking, liu2024reifereevaluatinginstructionfollowingevaluation}, demonstrating their ability to assess multiple quality dimensions including coherence, faithfulness, and informativeness. However, these approaches face challenges such as positional bias and inconsistency across different model sizes \citep{wang-etal-2024-large-language-models-fair, shi2024judgingjudgessystematicstudy}. In this paper, we address these limitations by proposing two novel metrics that remain consistent regardless of position or choice of evaluation model.