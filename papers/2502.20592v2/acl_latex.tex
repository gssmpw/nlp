% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{adjustbox}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{amsmath}

\usepackage{tcolorbox}
\usepackage{booktabs}
\tcbuselibrary{skins}    % For professional table rules (\toprule, \midrule, \bottomrule)
\usepackage{multirow}    % For cells spanning multiple rows
\usepackage{tabularx}
\usepackage{colortbl}
\usepackage{longtable}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Multi$^2$: Multi-Agent Test-Time Scalable Framework for \\ Multi-Document Processing}


\author{
Juntai Cao$^{1*}$,  Xiang Zhang$^{1}$\thanks{Equal contribution.},  Raymond Li$^{1}$, Chuyuan Li$^{1}$, Shafiq Joty$^{2}$, Giuseppe Carenini$^{1}$ \\ 
$^{1}$ University of British Columbia \\
$^{2}$ Salesforce Research \\ 
\texttt{\{jtcao7, raymondl,
chuyuan.li, carenini\}@cs.ubc.ca} \\ \texttt{xzhang23@ualberta.ca}, \ \ \texttt{sjoty@salesforce.com}}

\begin{document}
\maketitle
\begin{abstract}

Recent advances in test-time scaling have shown promising results in improving Large Language Models (LLMs) performance through strategic computation allocation during inference. While this approach has demonstrated strong performance improvements in logical and mathematical reasoning tasks, its application to natural language generation (NLG), especially summarization, has yet to be explored.
Multi-Document Summarization (MDS) is a challenging task that focuses on extracting and synthesizing useful information from multiple lengthy documents.
Unlike reasoning tasks, MDS requires a more nuanced approach to prompt design and ensemble, as there is no ``best'' prompt to satisfy diverse summarization requirements.
To address this, we propose a novel framework that leverages inference-time scaling for this task.
Precisely, we take prompt ensemble approach by leveraging various prompt to first generate candidate summaries and then ensemble them with an aggregator to produce a refined summary.
We also introduce two new evaluation metrics: Consistency-Aware Preference (CAP) score and LLM Atom-Content-Unit (ACU) score, to enhance LLM's contextual understanding while mitigating its positional bias. 
Extensive experiments demonstrate the effectiveness of our approach in improving summary quality while identifying and analyzing the scaling boundaries in summarization tasks.\footnote{Our code, model outputs, and evaluation module will be made publicly available upon acceptance.}
% Our work provides insights into the scaling characteristics of summarization and establishes new benchmarks for evaluating summary quality.
\end{abstract}



\section{Introduction}

Test-time scaling (or inference-time scaling) has emerged as a promising approach for enhancing LLM's performance beyond traditional architectural or data improvements \cite{learningtoreason2024}. While earlier work focused on relationships between models' capabilities, size, and training resources, recent research demonstrates that strategic compute allocation during inference can yield substantial performance gains. For instance, studies show that increased inference computation produces better results than equivalent investments in pretraining 
\citep{snell2024scalingllmtesttimecompute,agarwal2024manyshotincontextlearning, muennighoff2025s1simpletesttimescaling}.
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.86\linewidth]{score_viz.png}
    % \vspace{-10pt}
    \caption{
    % Visualization of 
    Visualization of our proposed Consistency-aware Preference (CAP) Score for text generation task. Applying LLMs' strong language understanding ability, CAP assign higher score to summary which 
    \textit{consistently} gets ranked higher by the LLM.}
    \label{fig:cap_viz}
\end{figure}
% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{score_viz.png}
%     \caption{
%     % Visualization of 
%     Distribution of our proposed Consistency-aware Preference (CAP) Score.}
%     \label{fig:cap_viz}
% \end{figure}

Research on test-time scaling has largely centered on logical and math reasoning tasks, leaving traditional natural language generation (NLG) tasks relatively unexplored. 
This gap is particularly notable in text summarization, a domain where LLMs have already demonstrated significant advances, generating summaries competitive with human performance \citep{xiao-etal-2024-personalized,zhang-etal-2024-benchmarking,pu2023summarizationalmostdead}.
Beyond generation, LLMs have proven effective as judges when guided by well-designed evaluation protocols \citep{liu-etal-2024-benchmarking,liu2024reifereevaluatinginstructionfollowingevaluation}. 
Recent expansions in context window sizes have created new opportunities to study scaling effects on length-constrained tasks like summarization \citep{liu2022character}. However, LLMs still struggle with key challenges including hallucination, incomplete coverage, language inconsistency, and verbosity \citep{zhang2024cross,liu-etal-2024-benchmarking,zhang2023bridging,belem2024singlemultillmshallucinate,zhang-etal-2023-dont}.



% To thoroughly examine LLMs' summarization capabilities and their scaling properties, we focus on multi-document summarization (MDS), one of the most challenging subdomains of summarization. 
% MDS requires synthesizing and linking information across lengthy documents, making it particularly time- and labor-intensive \citep{vanveen2024clinical}. 
In this paper, we aim to examine LLMs' summarization capabilities and their scaling properties by focusing on the multi-document summarization (MDS) task. 
MDS requires synthesizing and linking information across lengthy documents, handling information redundancy, maintaining factual consistency, and generating coherent and concise summaries while preserving key details. 
In addition, MDS demands effective reasoning to determine relevance and priority among diverse pieces of information.
These characteristics make MDS particularly time- and labor-intensive \citep{vanveen2024clinical}. 
To tackle these challenges, we propose a multi-agent approach that leverages prompt ensemble techniques to scale summarization at test time. While traditional prompt ensemble methods exist—such as (a) applying different sampling strategies to a single prompt \citep{li2023makinglargelanguagemodels}, or (b) varying few-shot examples within prompts \citep{arora2022askanythingsimplestrategy}—their direct application to summarization presents notable limitations. The first approach merely explores variations in the output space, while the second heavily relies on example-based learning, which is better suited for reasoning tasks \citep{zhang2024supervisedchainthought}.
Furthermore, summarization differs fundamentally from reasoning tasks~\cite{zhang2024counting}, where specific prompts like ``\textit{Let's think step by step}'' \citep{kojima2022large} can effectively guide models through predetermined reasoning patterns~\cite{zhang2024autoregressive+}. In contrast, no single "optimal" prompt exists for generating summaries that satisfy diverse requirements. Given these distinctions, summarization demands a more sophisticated approach to prompt ensemble techniques. We provide a comprehensive theoretical analysis of our approach in Appendix~\ref{app:theory}.

Therefore,
we propose Multi$^2$ framework (Fig. \ref{fig:framework}) to 
address this challenge by generating multiple summaries through diverse prompts that maintain consistent requirements. 
We then employ an aggregation strategy to construct a comprehensive final summary that leverages the strengths of each summary candidate.
While increased inference-time computation generally improves performance, recent studies have also identified an \textit{inverse scaling} phenomenon, where excessive computation can paradoxically degrade performance \citep{gao2022scalinglawsrewardmodel, stroebl2024inferencescalingflawslimits}. 
We also investigate this phenomenon by systematically varying the number of samples and examining its boundaries. 

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{framework-2.png}
    \caption{
    Overview of Multi$^2$ summarization inference-time scaling framework. Documents are first summarized by independent LLM agents, each guided by a different prompt from a curated prompt bank and constrained by user requirements. 
    The resulting %diverse 
    summaries are then processed by an aggregator (Voter, Context-Preserving Summarizer, or Context-Independent Summarizer)
    % also guided by the same requirements, 
    to generate the final consolidated summary.
    % that preserves key information from all perspectives.
    }
    \label{fig:framework}
    % \vspace{-10pt}
\end{figure*}

Another challenge in MDS is the reliability of automatic evaluation metrics. Traditional metrics like ROUGE \citep{lin2004rouge} have proven insufficient for capturing summary quality, while more recent LLM-based metrics—such as Auto-ACU \citep{liu-etal-2023-towards-interpretable}, LLMCompare \citep{liu-etal-2024-benchmarking}, and LLMRank \citep{liu2024reifereevaluatinginstructionfollowingevaluation}—show limitations, including constraints in contextual understanding for smaller models  and persistent positional biases \citep{wang-etal-2024-large-language-models-fair}.  
We specifically highlight \textbf{positional bias}, where LLMs tend to favor summaries appearing in a particular position (first or second in a pairwise comparison), leading to inconsistencies in evaluation, particularly during test-time scaling.  
To improve evaluation consistency, we propose two novel metrics: Consistency-Aware Preference (CAP) score and LLM Atom-Content-Unit (LLM-ACU) score. As shown in our experiments, these metrics leverage LLMs' contextual understanding while incorporating mechanisms to mitigate positional bias, ensuring more reliable and robust summary assessment.

In summary, (1) We present the first comprehensive investigation of test-time scaling laws in text summarization, extending the analysis beyond traditionally explored reasoning tasks; (2) We introduce a new framework Multi$^2$ that enhances summarization performance through ensemble at test time; (3) We propose two novel LLM-based evaluation protocols for text summarization and provide reference ACU datasets for quantitative assessment of summary quality, changing the way evaluation is done in summarization tasks compared to traditional ROUGE-score-like metrics. 


\section{Related Work}
\subsection{Test-time scaling}
Test-time scaling strategies can be broadly classified into three categories: repeated sampling, deliberative approaches, and self-refinement. Repeated sampling leverages techniques like temperature sampling \citep{ACKLEY1985147}, top-$k$, and top-$p$ sampling \citep{DBLP:conf/iclr/HoltzmanBDFC20} to generate diverse outputs, which are then enhanced through aggregation strategies such as majority voting \citep{wang2023selfconsistency}, weighted majority voting \citep{li-etal-2023-making}, or best-of-$n$ selection \citep{cobbe2021training}. 

Recent work \citep{brown2024largelanguagemonkeysscaling,wu2024scaling,stroebl2024inferencescalingflawslimits,zhao2025samplescrutinizescaleeffective} demonstrates that repeated sampling can significantly expand LLM capabilities across various domains. \textbf{Deliberative approaches} incorporate structured reasoning through methods like chain-of-thought prompting \citep{wei2023chainofthoughtpromptingelicitsreasoning} and tree search. These approaches range from informed search methods \citep{zhuang2024toolchain,wang2024q} to Monte Carlo Tree Search (MCTS) variants \citep{tian2024selfimprovementllmsimaginationsearching,zhang2024llamaberrypairwiseoptimizationo1like,zhang2024restmctsllmselftrainingprocess}. A key characteristic of tree search methods is to use process reward models (PRMs) to guide the search trajectory during generation \citep{yao2023reactsynergizingreasoningacting,zelikman2024quietstarlanguagemodelsteach}. 
\textbf{Self-refinement}  \citep{madaan2023selfrefine} enables models to iteratively improve their responses through self-critique and editing. Additionally, all categories of test-time scaling methods can be enhanced through model ensembling \citep{wang2024mixtureofagentsenhanceslargelanguage,jin2024contranovo,chen2024are} to combine the strengths of multiple models to achieve better performance.

Tree search methods often struggle with the high-dimensional search space created by multiple source documents, making it computationally intensive to explore meaningful trajectories. Self-refinement approaches, which rely on iterative improvements, may lead to information loss as they tend to focus on refining a single perspective rather than maintaining diverse viewpoints from multiple documents. Therefore, we adopt the repeated sampling approach to scale MDS at test time, using diverse prompts to generate multiple perspectives that are then consolidated through specialized aggregation methods.

\subsection{Multi Document Summarization}

Multi-document summarization (MDS) has evolved significantly from traditional methods \citep{Erkan2004LexRankGL,mehdad-etal-2014-abstractive,gerani-etal-2014-abstractive} to modern  approaches powered by neural networks, which introduced encoder-decoder architectures for better summary generation \citep{liu-lapata-2019-hierarchical, zhang2020pegasuspretrainingextractedgapsentences, Giorgi2022OpenDM,li2022humanguidedexploitationinterpretable}.
The advent of LLMs has boosted MDS capabilities even further, with models demonstrating impressive zero- and few-shot performance \citep{zhang-etal-2024-benchmarking}. Recent work has shifted the focus from architectural modifications to improve LLMs' summarization abilities to exploring various prompting strategies \citep{xiao-etal-2024-personalized, liu-etal-2024-benchmarking}. Despite these advances, MDS continues to face challenges including maintaining cross-document consistency, ensuring factual accuracy, and addressing content incompleteness where key information may be omitted \citep{belem2024singlemultillmshallucinate}. In this paper, we propose to tackle these challenges through a scaling approach that leverages prompt ensemble techniques to generate more comprehensive and accurate summaries. 

Traditional evaluation metrics for summarization, such as ROUGE \citep{lin-2004-rouge}, only rely on lexical overlap with reference summaries. These metrics often fail to capture semantic similarity and summary quality adequately \citep{bhandari-etal-2020-metrics}. This limitation has led to the development of learned metrics that better align with human judgments \citep{Yuan2021BARTScoreEG,zhang2020bertscoreevaluatingtextgeneration}.
The emergence of LLMs has enabled even more sophisticated evaluation approaches. Recent work has explored using LLMs as evaluation agents \citep{liu-etal-2024-benchmarking, liu2024reifereevaluatinginstructionfollowingevaluation}, demonstrating their ability to assess multiple quality dimensions including coherence, faithfulness, and informativeness. However, these approaches face challenges such as positional bias and inconsistency across different model sizes \citep{wang-etal-2024-large-language-models-fair, shi2024judgingjudgessystematicstudy}. In this paper, we address these limitations by proposing two novel metrics that remain consistent regardless of position or choice of evaluation model.

\section{Multi$^2$ Framework}
\subsection{Multi-agent Text Generation}
Our Multi$^2$ test-time scaling framework for MDS is illustrated in Figure~\ref{fig:framework}. The framework operates in two main stages: candidate generation and summary aggregation. In the first stage, input documents are processed by multiple independent LLM agents using randomly selected prompts from a curated prompt bank, simulating real-world summarization scenarios. The generated candidate summaries, along with the original requirements, are then passed to the aggregator module.
The aggregator module implements three distinct approaches: vote, context-preserving summarizer (CPS), and context-independent summarizer (CIS). 

The \textbf{vote} agent evaluates all candidate summaries against the original input documents and provides a detailed explanation before selecting the best summary. To mitigate positional bias (Section~\ref{sec:posbias}), we explicitly require the agent to complete its reasoning before indicating its final selection, ensuring the choice is constrained by the documented rationale. 
Instead of selecting the best candidate summary, CPS and CIS aggregate the candidate summaries into a final summary. 
The \textbf{CPS} agent generates a refined summary by consulting both the original documents and the candidate summaries, aiming for completeness and conciseness. 
In contrast, the \textbf{CIS} agent focuses solely on the candidate summaries without access to the original documents, producing a consolidated summary through reference-based synthesis. We attached our prompts for aggregation agents in Appendix~\ref{app:ensembleprompt}.

The details of ensemble framework and an theoretical ground for such ensemble on prompt space level and its complexity analysis can be referred to in Appendix~\ref{app:theory}.
% \section{Metrics}
\subsection{LLM-based Metrics for text Generation}

\subsubsection{Positional Bias and Motivation} 
\label{sec:metricmotiv}
Recent approaches to automatic evaluation have increasingly leveraged LLMs, either through comparative (pairwise) assessment or direct scoring mechanisms. However, both approaches face  challenges. Comparative methods struggle with positional bias, an inherent limitation of LLM judges. While previous research \cite{liu2024reifereevaluatinginstructionfollowingevaluation} suggested that advanced models (like \texttt{gpt-4o}) might mitigate this issue, our experiments in Section~\ref{sec:posbias} demonstrate that LLM evaluations remain extremely susceptible to position-dependent variations, especially on contextual tasks like MDS. 
Direct scoring approaches face different challenges: defining clear scoring guidelines could be difficult, and ensuring consistent application of grading rubrics across different generations remains challenging. Moreover, the complexity of nuanced scoring—a task challenging even for human evaluators who struggle more with five-point Likert scales than binary preferences—makes it particularly difficult for LLMs to provide reliable quantitative assessments. The pairwise comparative setup offers utility to practitioners (e.g., evaluation for A/B testing) while eliciting evaluations better aligned with humans judgment from automatic evaluators~\citep{wang2023large,liu2024aligning}.

To address these limitations and enable reliable large-scale evaluation of generated summaries, we propose two novel metrics %\shafiq{in the pairwise setup}: 
Consistency-aware Preference (CAP) score and LLM-ACU score. These metrics are specifically designed to mitigate positional bias, while providing repeatable quantitative measurements for systematic comparison of summary quality.


\subsubsection{Consistency-Aware Preference Score}
We develop the Consistency-Aware Preference (CAP) score as an enhancement to the LLMCompare \citep{liu-etal-2024-benchmarking} method for quantitatively evaluating preference rates of summaries compared to a baseline. 
% While LLMCompare uses an LLM agent to compare two summaries against source documents, %and requirements, 
% rating one as superior (1 or 2) or equivalent (tie), 
% our experiments (Section~\ref{sec:posbias}) reveal its susceptibility to positional bias. 
LLMCompare employs an LLM judge to evaluate two summaries against the source documents, determining which is superior (1 or 2) or if they are equivalent (tie). 
To address the positional bias (detailed in Section~\ref{sec:metricmotiv}), we implement a metric with two-phase comparison process. 
First, we use an LLM as judge to obtain preferences with summaries (target and baseline) in their original positions. 
Then, we swap the positions of the two summaries and obtain a second set of preferences, relabeling them based on their new positions to eliminate labeling bias. 
From
this two-step
comparison, we compute the \textit{win rates} ($w_1$, $w_2$) of the target summarization method against the baseline in 
% each direction,
each step,
and the \textit{consistency rate} ($C$) of predictions across both orderings (Figure~\ref{fig:cap1}).

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{cap.png}
    \caption{Two-step calculation of CAP using LLM. In this example, LLM prefers the first summary in both step 1 and step 2, resulting in inconsistent evaluation.}
    \label{fig:cap1}
\end{figure}

% Notably, 
Importantly,
when evaluating consistency, if either comparison (i.e., before or after the swapping) results in a tie, we consider it consistent with any outcome in the other comparison to avoid over-penalizing borderline cases.
The final CAP score is computed as follows:
\begin{equation}
\text{CAP} = W_{\text{pref}}\frac{1}{1+\exp^{-k(C-0.5)}},
\end{equation}
where $W_{\text{pref}}$ refers to \textit{preference rate} calculated from \textit{win rates} ($w_1$ and $w_2$); $C$ refers to \textit{consistency score}; 
$k$ controls sensitivity to consistency variations (default to 10 according to the our experiments on a validation set).
In practice, the preference weight $W_{\text{pref}}$ can be determined using either max-pooling or averaging:
\begin{align}
    W_{\text{pref}}^{\text{max}} &= \max(w_1,w_2) \\
    W_{\text{pref}}^{\text{avg}} &= \frac{(w_1,w_2)}{2}
\end{align} 
Figure~\ref{fig:cap_viz} illustrates %how CAP scores distribute 
the distribution of CAP score across different preference weights $W$ and consistency values $C$. 
The CAP score is designed to reflect both the \textit{preference rate} and \textit{consistency score}, ensuring a reliable evaluation. A high CAP score is achieved only when both factors are high, meaning the model consistently prefers the same summary. If the model's predictions are stable, the CAP score varies proportionally with the \textit{preference rate}. However, when predictions are inconsistent, the CAP score remains low regardless of the preference outcome, as the metric intentionally penalizes unreliable decisions.





\subsubsection{LLM-ACU Score}
Inspired by the Atomic Content Unit (ACU) score \citep{liu-etal-2023-revisiting,liu-etal-2023-towards-interpretable}, we propose an LLM-based ACU metric to quantitatively measure the completeness of summaries. The process consists of two phases. First, using few-shot prompting, we guide an LLM to extract ACUs from reference summaries. These ACUs are designed to capture essential factual units that are independently interpretable without
references. In the evaluation phase, we present the extracted ACUs alongside the model generated summary and ask an LLM to determine which of the ACUs are entailed in the generated summary. The final score $f$ for a set of summaries $S$ and their corresponding ACU sets $\mathcal{A}$ is computed as the average unnormalized ACU score:
\begin{equation}
f(S,\mathcal{A}) = \frac{1}{|S|}\sum_{s\in S}\frac{e_s}{|\mathcal{A}_s|},
\end{equation}
where $e_s$ represents the number of ACUs in the system output that are entailed by the gold standard ACUs $\mathcal{A}_s$ determined by the LLM.
We use \texttt{gpt-4o} for both ACU extraction\footnote{Our extracted ACUs for MultiNews and OpenASP datasets will be released.} and entailment verification. 


Recent work suggests that fine-tuning primarily enables format adaptation rather than information acquisition in language models \citep{allenzhu2024physicslanguagemodels31}. Therefore, we do not finetune models for extracting ACUs and checking entailment, but instead leverage the advanced language understanding capabilities of LLMs directly for both steps. 


\begin{table*}[t]
   \centering
   \setlength{\tabcolsep}{6pt}  % Adjust column spacing
   \begin{tabular}{c cc cccc cccc cc}
   \toprule
   & \multicolumn{2}{c}{\textbf{Baseline}} & \multicolumn{4}{c}{\textbf{gpt-4o}} & \multicolumn{4}{c}{\textbf{gpt-4o-mini}} & \multicolumn{2}{c}{} \\
   \cmidrule(lr){2-3} \cmidrule(lr){4-7} \cmidrule(lr){8-11} \cmidrule(lr){12-13}
   \multirow{2}{*}{\textbf{\# Samples}} & 
   \multirow{2}{*}{\textbf{High}} & \multirow{2}{*}{\textbf{Avg}} & 
   \multicolumn{2}{c}{\textbf{CIS}} & \multicolumn{2}{c}{\textbf{CPS}} & 
   \multicolumn{2}{c}{\textbf{CIS}} & \multicolumn{2}{c}{\textbf{CPS}} & 
   \multicolumn{2}{c}{\textbf{Vote}} \\
   \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
   & & & \textbf{High} & \textbf{Avg} & \textbf{High} & \textbf{Avg} & 
   \textbf{Max} & \textbf{Avg} & \textbf{Max} & \textbf{Avg} & \textbf{Max} & \textbf{Avg} \\
   \midrule
   2 & \multirow{5}{*}{0.25} & \multirow{5}{*}{0.15} & 0.69 & 0.51 & 0.82 & 0.62 & 0.67 & 0.49 & 0.80 & 0.61 & 0.37 & 0.23 \\
   3 & & & 0.73 & 0.55 & 0.79 & 0.62 & 0.72 & 0.53 & 0.72 & 0.54 & 0.27 & 0.16 \\
   4 & & & 0.68 & 0.50 & 0.82 & 0.64 & 0.73 & 0.55 & 0.80 & \textbf{0.60} & 0.27 & 0.16 \\
   5 & & & 0.71 & 0.52 & \textbf{0.85} & \textbf{0.69} & \textbf{0.81} & \textbf{0.62} & \textbf{0.78} & \textbf{0.60} & 0.28 & 0.17 \\
   6 & & & \textbf{0.79} & \textbf{0.60} & 0.81 & 0.63 & 0.77 & 0.57 & 0.77 & \textbf{0.60} & \textbf{0.37} & \textbf{0.23} \\
   \bottomrule
   \end{tabular}
   \caption{
    CAP scores on Multinews dataset using \texttt{gpt-4o} and \texttt{gpt-4o-mini} models with context-independent summarizer (CIS) and context-preserving summarizer (CPS). The aggregator using Vote is model-invariant.
   We report CAP with max-pooled (``High'') and average (``Avg'') preference scores ($W_\text{pref}$).
   Baseline shows both max-pooled and average CAP across all samples. 
   Best scores per column are shown in \textbf{bold}.
   }
   \label{tab:cap_multinews}
\end{table*}



\section{Experiment Setup}

\paragraph{Datasets.} We evaluate our framework on two datasets: MultiNews \citep{fabbri-etal-2019-multi} for general-purpose summarization and OpenASP \citep{amar-etal-2023-openasp} for aspect-based summarization. These datasets represent distinct summarization challenges, with MultiNews focusing on general-purpose news article consolidation and OpenASP targeting aspect-specific information extraction and synthesis. For a balanced comparison, we conduct our experiments on the test sets of both datasets. For MultiNews, we select the first 600 entries from its test set to match the size of OpenASP's test set.

\paragraph{Models.}  To investigate scaling properties and leverage extended context windows, we evaluate our framework using two state-of-the-art models of different scales: \texttt{gpt-4o} and \texttt{gpt-4o-mini}. These models enable us to analyze how performance scales with model size while maintaining consistent architectural characteristics.

\paragraph{Prompt Bank.} We adapt the prompt collection from \citet{lior2024seamstochasticbenchmarkmultidocument} to explore the prompt space. While some prompts in their work were originally designed for extractive summarization, we modified them for abstractive summary generation while preserving their core instructional elements. The prompts are shown in Appendix~\ref{app:summprompt}.

% \paragraph{Design.} 
\paragraph{Implementation Details.}
We establish our baseline using summaries generated by \texttt{gpt-4o} with a single prompt randomly selected from our prompt bank using a fixed random seed. We then conduct experiments by scaling this baseline approach across models of varying sizes and applying different aggregation methods to combine the generated summaries. For experimental consistency, we run each configuration twice with the default temperature setting (0.8) and report the averaged results.
In our experiments, we focused on two primary control variables: (1) the inference model size, and (2) the scaling factor, determined by the number of ensembled samples.



\begin{table*}[t]
   \centering
   \setlength{\tabcolsep}{6pt}  % Adjust column spacing
   \begin{tabular}{c cc cccc cccc cc}
   \toprule
   & \multicolumn{2}{c}{\textbf{Baseline}} & \multicolumn{4}{c}{\textbf{gpt-4o}} & \multicolumn{4}{c}{\textbf{gpt-4o-mini}} & \multicolumn{2}{c}{} \\
   \cmidrule(lr){2-3} \cmidrule(lr){4-7} \cmidrule(lr){8-11} \cmidrule(lr){12-13}
   \multirow{2}{*}{\textbf{\# Samples}} & 
   \multirow{2}{*}{\textbf{High}} & \multirow{2}{*}{\textbf{Avg}} & 
   \multicolumn{2}{c}{\textbf{CIS}} & \multicolumn{2}{c}{\textbf{CPS}} & 
   \multicolumn{2}{c}{\textbf{CIS}} & \multicolumn{2}{c}{\textbf{CPS}} & 
   \multicolumn{2}{c}{\textbf{Vote}} \\
   \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13}
   & & & \textbf{Max} & \textbf{Avg} & \textbf{Max} & \textbf{Avg} & 
   \textbf{Max} & \textbf{Avg} & \textbf{Max} & \textbf{Avg} & \textbf{Max} & \textbf{Avg} \\
   \midrule
   2 & \multirow{5}{*}{0.51} & \multirow{5}{*}{0.36} & 0.63 & 0.50 & 0.70 & 0.55 & 0.73 & 0.57 & 0.79 & 0.63 & 0.61 & 0.45 \\
   3 & & & 0.72 & 0.57 & 0.76 & 0.59 & 0.75 & 0.60 & 0.83 & 0.69 & 0.64 & 0.48 \\
   4 & & & 0.72 & 0.55 & 0.74 & 0.59 & 0.77 & 0.62 & 0.83 & 0.71 & \textbf{0.66} & \textbf{0.51} \\
   5 & & & \textbf{0.74} & 0.59 & 0.76 & \textbf{0.61} & \textbf{0.82} & \textbf{0.67} & \textbf{0.86} & \textbf{0.72} & 0.64 & 0.48 \\
   6 & & & \textbf{0.74} & \textbf{0.60} & \textbf{0.77} & 0.60 & 0.81 & 0.66 & 0.85 & \textbf{0.72} & 0.56 & 0.42 \\
   \bottomrule
   \end{tabular}
   \caption{
   % Comparison of OpenASP CAP scores under same settings as Table~\ref{tab:cap_multinews}.
   CAP scores on OpenASP dataset under the same settings as in Table~\ref{tab:cap_multinews}.
   }
   \label{tab:cap_openasp}
\end{table*}









\section{Results}
\subsection{Main Results}
Our experimental results are presented across four tables. The CAP scores are shown in Table~\ref{tab:cap_multinews} for MultiNews and Table~\ref{tab:cap_openasp} for OpenASP, respectively. 
The completeness scores, i.e., LLM-ACU scores, are shown in Table~\ref{tab:acu_multinews} for MultiNews and Table~\ref{tab:acu_openasp} for OpenASP, respectively. 
We also provide an analysis of how summary length correlates with the number of captured ACUs in Appendix~\ref{app:lenimpact}. 
In essence, we demonstrate that our prompt ensemble approach successfully captures diverse information from source documents by generating longer but informationally dense summaries. 





\paragraph{Effectiveness of Test-Time Scaling.}
Our experiments demonstrate significant improvements through inference-time scaling across both preference (Table~\ref{tab:cap_multinews} and \ref{tab:cap_openasp}) and completeness metrics (Table~\ref{tab:acu_multinews} and \ref{tab:acu_openasp}).



On MultiNews, starting from a low preference baseline, all scaling methods show substantial gains in overall quality. 
For completeness specifically, CPS aggregator achieves the strongest performance in information coverage, with \texttt{gpt-4o-mini} showing substantial gains from a baseline of 47.13 to 54.64 with 6 samples.
Similarly for OpenASP, despite beginning from a stronger preference baseline, scaling with prompt ensemble still provides notable improvements in overall quality. The completeness metrics show comparable trends, with CPS improving \texttt{gpt-4o-mini}'s coverage from 42.35 to 47.82 using 5 samples. These results consistently demonstrate that scaling at test time can effectively enhance both summarization quality and information coverage across different datasets.
Analysis of the LLM-ACU scores reveals several patterns in information preservation during scaling. First, CPS consistently outperforms both CIS and voting approaches across all experimental conditions, suggesting that access to source documents during ensemble is crucial for maintaining comprehensive coverage. Second, the completeness improvements are more pronounced on MultiNews (up to 7.51 points) compared to OpenASP (up to 5.47 points), suggesting that general-purpose summarization may benefit more from diverse prompt sampling in terms of information capture.


\begin{table}[b]
   \centering
   \adjustbox{width=0.5\textwidth}{
      \setlength{\tabcolsep}{7pt}  
      \begin{tabular}{c c cc cc c}
         \toprule
         \multicolumn{2}{c}{\textbf{LLM-ACU (OpenASP)}} & \multicolumn{2}{c}{\textbf{gpt-4o}} & \multicolumn{2}{c}{\textbf{gpt-4o-mini}} & \\
         \cmidrule(lr){3-4} \cmidrule(lr){5-6}
         \textbf{\# Samples} & \textbf{Baseline} & \textbf{CIS} & \textbf{CPS} & \textbf{CIS} & \textbf{CPS} & \textbf{Vote} \\
         \midrule
         \centering 2 & \multirow{5}{*}{\centering 42.35} & 43.05 & 44.16 & 44.36 & 46.07 & 43.86 \\
         \centering 3 & & 44.00 & 45.00 & 45.04 & 47.35 & 44.03 \\
         \centering 4 & & 43.64 & 45.51 & 45.05 & 47.55 & 44.47 \\
         \centering 5 & & 44.07 & \textbf{46.47} & 46.13 & \textbf{47.82} & 44.47 \\
         \centering 6 & & \textbf{44.66} & 46.30 & \textbf{46.35} & 47.46 & 45.00 \\
         \bottomrule
      \end{tabular}
   }
   \caption{Comparison of LLM-ACU scores on OpenASP dataset under same settings as Table~\ref{tab:acu_multinews}.}
   \label{tab:acu_openasp}
\end{table}

\paragraph{Scaling Boundaries and Limitations.}
Our analysis reveals clear boundaries in the effectiveness of inference-time scaling across both preference and completeness metrics. For preferences, both datasets~\cite{yin2023ttida} exhibit saturation points around 5-6 samples, beyond which the benefits diminish or reverse. This inverse scaling phenomenon is particularly evident in MultiNews, where CPS performance peaks at 5 samples (0.85/0.69) before declining to 0.81/0.63 at 6 samples. Similarly for completeness, \texttt{gpt-4o}'s scores with CPS plateau around 5-6 samples (52.70 and 52.40 respectively), with \texttt{gpt-4o-mini} showing similar saturation patterns.
The scaling limitations manifest differently across ensemble methods. In terms of completeness, voting shows minimal improvement across all sample sizes (maximum gain of ~1.2 points), suggesting that simple selection-based ensemble may be insufficient for maintaining comprehensive information coverage.
The impact of document context during ensemble emerges as a crucial factor. While CIS performs better than voting, it consistently achieves lower completeness scores than CPS, indicating that losing document context during ensemble creates a ceiling on information preservation. These observations suggest that excessive ensemble sizes may introduce noise rather than improvements, and that the choice of ensemble method significantly affects both quality and coverage outcomes. This highlights the importance of identifying optimal scaling thresholds and maintaining document context throughout the ensemble process.

\begin{table}[t]
   \centering
   \adjustbox{max width=0.5\textwidth}{%
      \setlength{\tabcolsep}{3pt}  % Adjust column spacing
      \begin{tabular}{c c cc cc c}
         \toprule
         \multicolumn{2}{c}{\textbf{LLM-ACU (MultiNews)}} & \multicolumn{2}{c}{\textbf{gpt-4o}} & \multicolumn{2}{c}{\textbf{gpt-4o-mini}} & \\
         \cmidrule(lr){3-4} \cmidrule(lr){5-6}
         \textbf{\# Samples} & \textbf{Baseline} & \textbf{CIS} & \textbf{CPS} & \textbf{CIS} & \textbf{CPS} & \textbf{Vote} \\
         \midrule
         \centering 2 & \multirow{5}{*}{\centering 47.13} & 48.75 & 51.00 & 49.14 & 52.35 & 47.44 \\
         \centering 3 & & 49.25 & 51.11 & 50.03 & 52.88 & 48.31 \\
         \centering 4 & & 49.69 & 51.96 & 51.02 & 54.17 & 48.29 \\
         \centering 5 & & \textbf{50.86} & \textbf{52.70} & 50.95 & 53.90 & 47.65 \\
         \centering 6 & & 50.35 & 52.40 & \textbf{51.70} & \textbf{54.64} & 48.34 \\
         \bottomrule
      \end{tabular}
   }
   \caption{Comparison of LLM-ACU scores on MultiNews dataset using different ensemble methods. The vote scores are model-invariant and apply to both models. Baseline indicates single sample performance without prompt ensemble. Best score for each model and aggregation agent is shown in \textbf{bold}.}
   \label{tab:acu_multinews}
\end{table}

\paragraph{Scaling Effect across Model Sizes.}
Our experiments with \texttt{gpt-4o} and \texttt{gpt-4o-mini} reveal interesting patterns in how model size interacts with scaling benefits across two key dimensions: completeness and preference scores.
In terms of completeness, \texttt{gpt-4o-mini} often achieves larger relative improvements compared to \texttt{gpt-4o} when scaled through prompt ensemble. For instance, on MultiNews, \texttt{gpt-4o-mini}'s CPS performance improves by 7.51 points from baseline to peak, while \texttt{gpt-4o} shows a more modest improvement of 5.27 points. This suggests that prompt ensemble can partially compensate for model size limitations in terms of information capture. However, the absolute performance gap between the two models persists despite scaling. \texttt{gpt-4o} maintains higher baseline performance and generally achieves better peak scores, particularly with fewer ensembled samples. This indicates that while scaling can enhance the capabilities of smaller models, it cannot completely bridge the fundamental differences in model capacities. 
Regarding preference scores, the relationship between model size and performance is more nuanced. While \texttt{gpt-4o} generally outperforms \texttt{gpt-4o-mini} on MultiNews when using CPS, the smaller model achieves competitive results with CIS. More surprisingly, on OpenASP, \texttt{gpt-4o-mini} consistently outperforms its larger version across both CIS and CPS aggregators. This suggests that the benefits of model scale are not uniform across different summarization tasks, and that smaller models, when combined with appropriate scaling strategies, may sometimes be more effective.
These findings challenge the assumption that larger models necessarily benefit more from inference-time scaling and emphasize the importance of considering both model size and ensemble size in optimization strategies. Moreover, the persistence of completeness gaps across datasets suggests that model size remains a crucial factor in determining the upper bounds of summarization quality, even when enhanced through inference-time scaling techniques, though this relationship does not extend uniformly to preference metrics.


\begin{table}[ht]
\small
\centering
% \begin{tabular}{@{}llcc@{}}
\begin{tabular}{llcc}
\toprule
\textbf{Model} & \textbf{Dataset} & \textbf{Sum1 Win} & \textbf{Sum2 Win} \\ \midrule
GPT    & MultiNews & 456 & 92 \\
Claude & MultiNews & 262 & 336 \\
GPT    & OpenASP   & 355 & 177 \\
Claude & OpenASP   & 186 & 401 \\
\bottomrule
\end{tabular}
\caption{Model Preference Analysis - Number of wins when comparing summaries in order \{Sum1, Sum2\}.}
\label{tab:modelpref1}
\end{table}

\begin{table}[ht]
\small
\centering
% \begin{tabular}{@{}llcc@{}}
\begin{tabular}{llcc}
\toprule
\textbf{Model} & \textbf{Dataset} & \textbf{Sum2 Win} & \textbf{Sum1 Win} \\ \midrule
GPT    & MultiNews & 468 & 86 \\
Claude & MultiNews & 285 & 308 \\
GPT    & OpenASP   & 384 & 174 \\
Claude & OpenASP   & 188 & 396 \\
\bottomrule
\end{tabular}
\caption{Model Preference Analysis - Number of wins when comparing summaries in order \{Sum2, Sum1\}.}
\label{tab:modelpref2}
\end{table}

\begin{table}[ht]
\small
\centering
\scalebox{0.95}{
% \begin{tabular}{@{}lccc@{}}
\begin{tabular}{llcc}
\toprule
\textbf{Model/Dataset} & \textbf{Disc.(\%)} & \textbf{Pref Pos} & \textbf{Inc. Ratio} \\ \midrule
GPT/MultiNews    & 56.00\% & 1 & 333:3 \\
Claude/MultiNews & 16.67\% & 2 & 27:73 \\
GPT/OpenASP      & 30.03\% & 1 & 174:5 \\
Claude/OpenASP   & 34.72\% & 2 & 6:217 \\
\bottomrule
\end{tabular}
}
\caption{Model Consistency Analysis - Comparing discrepancy rates, positional bias, and inconsistency ratios between \texttt{gpt-4o} and \texttt{claude-3.5-sonnet}.}
\label{tab:modelconsist}
\end{table}
\subsection{Positional Bias and Choice of Evaluation Models.}
\label{sec:posbias}
In this section, we analyze the positional bias and consistency of two mainstream LLMs (\texttt{gpt-4o} and \texttt{claude-3.5-sonnet}). 

Tables~\ref{tab:modelpref1} and~\ref{tab:modelpref2} demonstrate a clear positional bias in both models' evaluations, though in opposing directions. \texttt{gpt-4o} shows a strong preference for summaries presented in the first position, with notably higher win ratios across both datasets. Conversely, \texttt{claude-3.5-sonnet} exhibits a preference for summaries in the second position, though this bias is relatively less pronounced in the MultiNews dataset.
This positional bias is further confirmed in Table~\ref{tab:modelconsist}, where the inconsistency ratios tell a similar story. The discrepancy percentages indicate that \texttt{claude-3.5-sonnet} generally achieves better consistency on MultiNews, though both models show comparable discrepancy rates on OpenASP.
While claude demonstrates marginally better consistency metrics overall, we opted to use \texttt{gpt-4o} in our final implementation due to practical considerations regarding speed and computational budget constraints. Since our evaluation framework incorporates both consistency and preference metrics, the choice between these models does not significantly impact the validity of our methodology or results.

These findings suggest that positional bias is still an inherent challenge in current language models when performing comparative evaluations, regardless of the specific model architecture or training approach. This observation underscores the importance of implementing appropriate debiasing strategies in evaluation frameworks.




\section{Conclusion}
In this work, we introduced Multi$^2$ framework for test-time scaling in MDS through prompt ensemble, demonstrating that strategic computation allocation during inference can effectively improve summary quality. Our experiments showed that increasing summary length through prompt ensemble leads to better information coverage while maintaining efficiency, and our novel evaluation metrics, CAP score and LLM-ACU score, provide more reliable assessments of summary quality. Through systematic analysis, we also identified the scaling boundaries in summarization tasks, providing insights into the limitations and optimal configurations of our approach.

Our findings also suggest two promising future research directions: (1) incorporating test time search algorithms to dynamically guide the prompt ensemble process and optimize summary generation, and (2) extending our evaluation metrics beyond summarization to assess model performance in reasoning tasks. These directions underscore the broader potential of optimizing LLMs' test time behavior across diverse applications, particularly in scenarios requiring both factual accuracy and logical consistency.

\section*{Limitations}
Our multi-agent text summarization system faces some limitations, primarily stemming from our reliance on large language models (LLMs) for evaluation. This approach incurs additional computational costs and requires computational resources, limiting the frequency and depth of evaluations. Additionally, the black-box nature of LLMs complicates the understanding of evaluation outcomes, making it challenging to pinpoint errors. 
\bibliography{custom}

\appendix
\section{Ensemble of Prompt for Text Generation}
\label{app:theory}
\subsection{Prompt Ensemble Formulation}
In our approach, we leverage a prompt ensemble mechanism to improve the quality of generated text. Let \( x \) denote the input text and \(\mathcal{P} = \{ p_1, p_2, \ldots, p_N \}\) be a collection of prompts designed to elicit different aspects of information from the underlying language model. For each prompt \( p_i \in \mathcal{P} \), the model produces an output \( y_i \) according to a generation function \( f \):
\[
y_i = f(x, p_i).
\]
The intuition behind this methodology is that different prompts \( p_i \) induce the model to focus on distinct features or details in the input \( x \), thereby generating complementary outputs.

To combine these outputs, we define an aggregation function \( g: \mathcal{Y}^N \to \mathcal{Y} \) that fuses the individual outputs \(\{ y_1, y_2, \ldots, y_N \}\) into a final output \( y \):
\[
y = g(y_1, y_2, \ldots, y_N).
\]
A common choice for \( g \) in our experiments is a weighted average or majority voting scheme, although the exact form of \( g \) may vary depending on the application. The overall system can therefore be formalized as:
\[
y = g\big(f(x, p_1), f(x, p_2), \ldots, f(x, p_N)\big).
\]

This formulation ensures that the final generated text \( y \) benefits from the diverse perspectives provided by the prompt ensemble. Empirical results indicate that the ensemble method consistently outperforms individual prompt-based generations, as it effectively mitigates the shortcomings of any single prompt by incorporating a broader range of contextual insights from the input \( x \).

\subsection{Prompt Space Theory}

In this section, we formalize the notion of the \emph{prompt space} and analyze its complexity in the context of Chain-of-Thought (CoT) reasoning. The prompt space, denoted as \(\mathcal{P}\), represents the set of all possible step templates that a language model (LM) may generate or be guided to generate during the reasoning process. Each template \(p \in \mathcal{P}\) is a discrete instruction that dictates how information is to be extracted from the latent representation \(h \in {R}^d\) and subsequently discretized into a sequence of tokens \(o = (o_1, o_2, \dots, o_k)\). In effect, the prompt space forms the interface between the continuous latent space and the discrete textual output~\cite{zhang2024autoregressive+}.

The latent vector \(h\) is assumed to encode \(m\) bits of information relevant to the task at hand. When the model follows a given prompt template \(p\), it extracts up to \(s\) bits of information per reasoning step. Thus, each template can be viewed as a function
\[
p: h \rightarrow o, \quad o \in \{0,1\}^s,
\]
where the mapping is constrained by the model’s capacity to “read out” a subset of the information encoded in \(h\). The total number of unique ways to extract \(s\) bits from \(m\) bits is given combinatorially by
\[
C(m,s) = \binom{m}{s} = \frac{m!}{s!(m-s)!}.
\]
This expression characterizes the \emph{prompt space complexity}, as it represents the number of potential step templates available to the model at each CoT step.

In practice, the prompt space is not uniformly sampled; instead, the LM employs learned heuristics to navigate this enormous space. That is, while the theoretical upper bound \(C(m,s)\) may be astronomically high, the effective search space is significantly reduced through task-specific training and, in many cases, human supervision. In an unsupervised setting, the model’s intrinsic biases might lead it to select suboptimal templates, thereby increasing the difficulty of navigating the subsequent \emph{answer space} \( \mathcal{S} \) – the space of all possible reasoning paths and final outputs.

More formally, let \(f\) denote the underlying computation that updates the hidden state:
\[
h_{t+1} = f(h_t, p),
\]
For brevity, we summarize the CoT process as follows: for \(t = 1, \dots, T\),
\[
o_t = p_t(h_{t-1}), \quad h_t = f(h_{t-1}, p_t).
\]
This compact notation encapsulates the iterative extraction of output tokens \(o_t\) and the recurrent update of the hidden state \(h_t\) via the chosen prompt \(p_t\).

Here, the selection of each \(p_t \in \mathcal{P}\) not only determines the immediate output \(o_t\) but also has a cascading effect on the evolution of the hidden state \(h_t\) and, consequently, the trajectory within the answer space \(\mathcal{S}\).

This intricate relationship between the prompt space and the answer space can be seen as a two-tier search problem: first, the model must identify a suitable template \(p\) from the high-dimensional prompt space \(\mathcal{P}\), and then it must effectively navigate the answer space \(\mathcal{S}\) defined by the recurrence \(h_t \rightarrow h_{t+1}\). Empirical evidence shows that even small deviations in the chosen template \(p\) can lead to exponentially larger errors in the final answer, underscoring the sensitivity of the overall reasoning process to prompt selection.

In summary, the prompt space theory emphasizes that the effectiveness of CoT reasoning hinges on the model’s ability to manage the combinatorial complexity inherent in extracting relevant information from its latent space. Supervised methods, which incorporate task-specific guidance, can significantly reduce the search complexity from the theoretical bound \(C(m,s)\) by constraining the model to a subset of high-quality prompts. This not only simplifies the navigation of the answer space but also enhances the overall reliability of the reasoning process. The insights derived here build upon recent analyses in the literature \cite{zhang2024supervised}.


\section{Human Evaluation}
To evaluate the effectiveness of our proposed scaling approach, we conducted a human evaluation study involving three graduate students. The evaluators were presented with 10 samples, each containing summaries generated by our three ensemble methods (Voting, CIS, and CPS) using the optimal ensemble size of 5, as determined by our automatic evaluation. Each sample also included the corresponding baseline summary for comparison.
The evaluation was structured as a preference-based comparison between each ensemble method and the baseline. For each sample, evaluators were asked to indicate their preference between the ensemble-generated summary and the baseline summary, resulting in 30 comparisons per method (10 samples $\times$ 3 aggregation types).
The human evaluation results strongly support the effectiveness of our proposed methods, particularly the CPS approach. The voting-based ensemble was preferred over the baseline in 60\% of cases (18/30 comparisons). The CIS method demonstrated stronger performance, being preferred in 76.7\% of comparisons (23/30). Most notably, the CPS method achieved unanimous preference, being chosen over the baseline in all comparisons (29/30).
These results demonstrate a clear hierarchy among the ensemble methods, with CPS showing superior performance in human evaluation. The strong preference for CPS (96.7\%) aligns with our automatic evaluation findings, confirming that the method produces summaries that are not only technically sound but also qualitatively superior from a human perspective. The significant improvement over both the baseline and other ensemble methods suggests that CPS effectively captures and maintains important aspects of text summarization that human readers value.



\section{Impact of Generation Length}
\label{app:lenimpact}
Previous work \citep{hu2024explaininglengthbiasllmbased,dubois2024lengthcontrolled} reveals LLM evaluation mechanisms tend to favor long summaries. This raises an important question: do longer summaries actually contain more useful information? To investigate this, we study the relationship between generation length and summary quality using the general-purpose MDS dataset MultiNews.

The results in Table~\ref{tab:gen_cost} demonstrate how different configurations of our framework affect summary length and the associated computational costs.
 While the summary length increases substantially from baseline to our most comprehensive setting (from 129.4 to 201.17 words), the computational cost grows more slowly, suggesting efficient information packaging. The CPS aggregator consistently produces longer summaries than CIS, particularly with \texttt{gpt-4o-mini}, indicating its effectiveness in capturing diverse information from source documents without introducing excessive computational overhead.

 \begin{table}[htbp]
\centering
\begin{tabular}{l|cc}
\toprule
\textbf{Experiment} & \textbf{\# Words} & \textbf{Word/ACU} \\
\midrule
Baseline                  & 129.4 & 17.03 \\
\midrule
\texttt{gpt-4o}/CIS         & 147.61 & 18.42 \\
\texttt{gpt-4o}/CPS         & 163.15 & 19.51 \\
\texttt{gpt-4o-mini}/CIS    & 172.45 & 20.74 \\
\texttt{gpt-4o-mini}/CPS    & 201.17 & 22.63 \\
\bottomrule
\end{tabular}
\caption{Summary length and word cost per ACU across different model configurations on MultiNews dataset. Length shows the average number of words in generated summaries, while Cost measures the average number of words needed to capture each ACU.}
\label{tab:gen_cost}
\end{table}

\section{Prompts}
\subsection{Summarization Prompts}
\label{app:summprompt}
In Tables~\ref{tab:promptbankmultinews1} and~\ref{tab:promptbankmultinews2}, we present the prompt bank used for the MultiNews dataset. Similarly, Tables~\ref{tab:promptbankopenasp1} and~\ref{tab:promptbankopenasp2} contain the prompt bank for the OpenASP dataset. These prompts were adapted and modified from the work of \citet{lior2024seamstochasticbenchmarkmultidocument}. We utilized the same few-shot examples as provided in their benchmark.

\subsection{Ensemble Prompts}
\label{app:ensembleprompt}
We present our summary ensemble prompts for general purpose MDS (for datasets like MultiNews) in Table~\ref{tab:ensemblegeneral}, and for aspect- (or query-) based MDS (for datasets like OpenASP) in Table~\ref{tab:ensembleaspect}.



\newpage% First table (Prompts 1-10)
\definecolor{tableheader}{RGB}{52, 73, 94}
\definecolor{tablegray}{RGB}{245, 246, 250}

\begin{table*}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabularx}{\textwidth}{|>{\columncolor{tablegray}}c|>{\columncolor{white}}X|}
        \hline
        \rowcolor{tableheader}
        \textbf{\textcolor{white}{No.}} & \textbf{\textcolor{white}{Prompt}} \\
        \hline
        1 & In this task, you are presented with multiple news articles about related topics. Your job is to generate a summary that integrates information from the provided articles. Your summary should be short and concise, that includes content only from the provided articles, avoiding any external data sources. \\
        \hline
        2 & Please provide a brief summary by synthesizing only the key points from the articles provided. Focus on the main arguments and conclusions without incorporating any information from outside these texts. Keep your summary concise and directly related to the content of the documents. \\
        \hline
        3 & Generate a concise summary using only the information from the provided articles. Your summary should distill the most essential information, capturing the core insights without adding any external content. Aim for brevity and clarity in your summarization. \\
        \hline
        4 & Please sift through the provided articles and distill their essence into a sharp, concise summary. Focus solely on the facts and key points within these texts, avoiding any embellishment or reference to external information. Your summary should read like a bullet-point list of the most critical insights. \\
        \hline
        5 & You are presented with multiple news articles about related topics. Summarize the contents in a way that captures the key information in a narrative form, but strictly using the details mentioned in the provided documents. Keep it engaging yet brief. \\
        \hline
        6 & Imagine you're preparing a brief for a decision-maker who has limited time. Summarize the provided documents by extracting only the most essential information. Present this in a clear, straightforward manner, focusing on the key facts and figures. \\
        \hline
        7 & Using only the details from the articles I've given you, craft a summary that distills the most important information. Avoid any interpretations or external data, and keep your summary short and direct. Emphasize the main arguments, data points, and conclusions. \\
        \hline
        8 & Operate as an information synthesizer: Draw the essence from multiple articles, focusing solely on the information contained within them. Your summary should be a tight, focused digest of the articles, free from any influence of external data. \\
        \hline
        9 & Scan through the provided articles and compile a summary that highlights only the most significant facts and figures, ensuring the exclusion of all external references. Aim for clarity and brevity. \\
        \hline
        10 & Operate as an academic summarizer: Imagine you are creating a summary for an academic review. Extract and emphasize the most pertinent information, ensuring your summary remains true to the original texts and free of external content. \\
        \hline
    \end{tabularx}
    \caption{Summarization Prompt Bank for MultiNews Dataset (Part 1)}
    \label{tab:promptbankmultinews1}
\end{table*}

% Second table (Prompts 11-20)
\begin{table*}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabularx}{\textwidth}{|>{\columncolor{tablegray}}c|>{\columncolor{white}}X|}
        \hline
        \rowcolor{tableheader}
        \textbf{\textcolor{white}{No.}} & \textbf{\textcolor{white}{Prompt}} \\
        \hline
        11 & Condense the provided information into a compact summary that emphasizes the main points and crucial data from the documents. Exclude any external information to maintain the integrity of the sources. \\
        \hline
        12 & From the provided articles, pull out the core messages and data points. Shape these into a brief, clear summary that directly reflects the content of the documents without any external additions. \\
        \hline
        13 & Compile a concise summary from the news articles given, focusing only on the information contained within. Your summary should integrate the main points without adding any outside information. \\
        \hline
        14 & Create a succinct summary by focusing exclusively on the details provided in the articles. Avoid using any external sources and ensure the summary remains clear and to the point. \\
        \hline
        15 & Produce a brief summary that distills the essential facts from the provided articles. Keep your summary strictly to the content presented in the documents, avoiding external influences. \\
        \hline
        16 & Develop a concise summary using only the information from the articles provided. Emphasize the main points and conclusions while avoiding the inclusion of any external data. \\
        \hline
        17 & Prepare a short, integrated summary by synthesizing key points from the given news articles. Ensure that no external content is included and that the summary is clear and direct. \\
        \hline
        18 & Your task is to distill the primary information from the provided articles into a concise summary. Make sure to exclude any external sources and focus strictly on the given texts. \\
        \hline
        19 & Summarize the provided articles by extracting only the key information and conclusions. Your summary should be brief and must not incorporate any external data. \\
        \hline
        20 & Generate a clear and brief summary using just the information from the provided articles. Focus on distilling the essential points and data without referencing external content. \\
        \hline
    \end{tabularx}
    \caption{Summarization Prompt Bank for MultiNews Dataset (Part 2)}
    \label{tab:promptbankmultinews2}
\end{table*}



\begin{table*}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabularx}{\textwidth}{|>{\columncolor{tablegray}}c|>{\columncolor{white}}X|}
        \hline
        \rowcolor{tableheader}
        \textbf{\textcolor{white}{No.}} & \textbf{\textcolor{white}{Prompt}} \\
        \hline
        1 & In this task you are required to generate an aspect-based summary of a set of documents related the same topic. Please write a short, concise aspect-based summary, only summarize content from the above documents, avoiding any external data sources. \\
        \hline
        2 & Your goal is to create a short, concise aspect-based summary of the given documents. Summarize the key points accurately, using only the information from these documents and excluding any external sources. \\
        \hline
        3 & Produce a brief, aspect-based summary of the collection of documents on the same topic. Ensure your summary is concise and derived only from the provided documents, avoiding any external data sources. \\
        \hline
        4 & Your task is to generate a detailed yet concise aspect-based summary from a collection of documents that focus on the same topic. Begin by thoroughly examining each document to understand the main aspects and themes. Then, synthesize this information into a coherent summary that highlights the significant points. \\
        \hline
        5 & Given a set of documents related to a specific topic, generate a short, concise aspect-based summary. Ensure that the summary is based solely on the content of the documents provided. \\
        \hline
        6 & You will receive several documents on the same topic. Your task is to write a brief aspect-based summary, using only the information from the provided documents and excluding any external sources. \\
        \hline
        7 & You are tasked with generating an aspect-based summary of several documents. Summarize the content briefly and accurately, using only the information from the documents give. \\
        \hline
        8 & In this task, you are required to create an aspect-based summary of a set of documents all related to the same topic. Carefully read through each document and identify the key aspects discussed. Summarize these aspects in a concise manner, ensuring that your summary captures the essential points. \\
        \hline
        9 & You are tasked with producing an aspect-based summary for a series of documents related to the same topic. Start by analyzing each document to identify the critical aspects covered. Your goal is to condense this information into a clear and concise summary. \\
        \hline
        10 & Generate a concise aspect-based summary of the given documents. Focus on summarizing the content based solely on the information from these documents, avoiding any external sources. \\
        \hline
    \end{tabularx}
    \caption{Summarization Prompt Bank for OpenASP Dataset (Part 1)}
    \label{tab:promptbankopenasp1}
\end{table*}

\begin{table*}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabularx}{\textwidth}{|>{\columncolor{tablegray}}c|>{\columncolor{white}}X|}
        \hline
        \rowcolor{tableheader}
        \textbf{\textcolor{white}{No.}} & \textbf{\textcolor{white}{Prompt}} \\
        \hline
        11 & Create a concise aspect-based summary for the provided set of documents. Focus on the main aspects and themes discussed in these documents, ensuring that your summary is based entirely on the content of the provided documents. \\
        \hline
        12 & Produce a short and precise aspect-based summary of the given documents. Identify the key aspects discussed in these documents and synthesize a concise summary based solely on the provided content. \\
        \hline
        13 & You will receive a collection of documents focused on the same topic. Your task is to create an aspect-based summary that highlights the key aspects discussed in these documents. Ensure your summary is brief and does not include any external information. \\
        \hline
        14 & You are provided with multiple documents related to a single topic. Your task is to generate an aspect-based summary that captures the main aspects discussed in these documents. Ensure your summary is concise and solely based on the provided texts. \\
        \hline
        15 & You are tasked with generating an aspect-based summary of several documents on the same topic. Carefully review each document, identify the main aspects, and write a brief summary that captures these aspects using only the provided documents. \\
        \hline
        16 & Your role is to create an educational summary for students using a collection of documents on the same topic. Focus on the main aspects that would help students understand the core concepts discussed in the documents. \\
        \hline
        17 & Imagine you are preparing a briefing for a busy executive who needs to understand the key aspects of several documents quickly. Summarize the most important points from these documents in a concise manner. \\
        \hline
        18 & As an advanced AI tasked with summarizing documents, your goal is to generate an aspect-based summary. Think of yourself as a summarization expert, extracting the most critical aspects from the documents provided. \\
        \hline
        19 & Imagine you are a journalist tasked with writing a summary article based on a series of documents related to a single topic. Identify the key aspects discussed in these documents and compose a brief, coherent summary. \\
        \hline
        20 & Your task is to act as a knowledge distiller, creating a concise aspect-based summary from a series of documents on the same topic. Focus on identifying and summarizing the critical aspects discussed in these documents. \\
        \hline
        21 & You are an AI assistant tasked with providing a summary for a set of documents related to a specific topic. Focus on the key aspects and themes discussed in these documents. Create a summary that captures these aspects in a concise manner, ensuring that your summary is based solely on the provided documents and excludes any external information.\\
        \hline
    \end{tabularx}
    \caption{Summarization Prompt Bank for OpenASP Dataset (Part 2)}
    \label{tab:promptbankopenasp2}
\end{table*}
\newpage

\definecolor{headerblue}{RGB}{47, 72, 88}
\definecolor{lightgray}{RGB}{249, 250, 251}
\definecolor{darktext}{RGB}{44, 62, 80}

\begin{table*}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.4}
    \begin{tabularx}{\textwidth}{|>{\columncolor{lightgray}\raggedright\arraybackslash}p{0.18\textwidth}|>{\columncolor{white}}X|}
        \hline
        \rowcolor{headerblue}
        \textbf{\textcolor{white}{Ensemble Type}} & \textbf{\textcolor{white}{Content}} \\
        \hline
        
        \textbf{Vote} & 
        Provide your explanation, then select the best summary of the given documents based on clarity, accuracy, conciseness, and completeness.
        
        \vspace{1mm}
        Documents: \{doc\}
        \vspace{2mm}
        
        Summary 1: \{sum1\}
        \vspace{2mm}
        
        Summary 2: \{sum2\}
        \vspace{2mm}
        
        ...
        \vspace{2mm}
        
        Explanation: ``Your explanation here''
        \vspace{2mm}
        
        Decision: [1-5] \\
        \hline
        
        \textbf{CIS} & 
        Take all provided summaries into account and generate a better, cohesive summary. Combine and refine the content from the summaries to ensure clarity, accuracy, conciseness, and completeness. Provide the final summary directly.
        
        \vspace{1mm}
        Summary 1: \{sum1\}
        \vspace{2mm}
        
        Summary 2: \{sum2\}
        \vspace{2mm}
        
        ...
        \vspace{2mm}
        
        Final revised summary: \\
        \hline
        
        \textbf{CPS} & 
        Take all provided summaries into account and generate a better, cohesive summary of the given documents. Combine and refine the content from the summaries to ensure clarity, accuracy, conciseness, and completeness. Provide the final summary directly.

        \vspace{1mm}
        Documents: \{doc\}
        \vspace{2mm}
        
        Summary 1: \{sum1\}
        \vspace{2mm}
        
        Summary 2: \{sum2\}
        \vspace{2mm}
        
        ...
        \vspace{2mm}
        
        Final revised summary: \\
        \hline
    \end{tabularx}
    \caption{Ensemble Prompts for General MDS}
    \label{tab:ensemblegeneral}
\end{table*}

\begin{table*}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.4}
    \begin{tabularx}{\textwidth}{|>{\columncolor{lightgray}\raggedright\arraybackslash}p{0.18\textwidth}|>{\columncolor{white}}X|}
        \hline
        \rowcolor{headerblue}
        \textbf{\textcolor{white}{Ensemble Type}} & \textbf{\textcolor{white}{Content}} \\
        \hline
        
        \textbf{Vote} & 
        Provide your explanation, then select the best summary of the given documents based on clarity, accuracy, conciseness, and completeness, focusing on the specified aspect.
        \vspace{2mm}
        
        Example Response:
        \vspace{1mm}
        
        Explanation: ``Your explanation here''
        \vspace{1mm}
        
        Decision: 1 (or 2 or 3 or 4 or 5)
        \vspace{2mm}
        
        Aspect: \{query\}
        \vspace{2mm}
        
        Documents: \{doc\}
        \vspace{2mm}
        
        Summary 1: \{sum1\}
        \vspace{2mm}
        
        Summary 2: \{sum2\}
        \vspace{2mm}
        
        ...
        \vspace{2mm}
        
        Response: \\
        \hline
        
        \textbf{CIS} & 
        Take all provided summaries into account and generate a better, cohesive summary, focusing on the specified aspect. Combine and refine the content from the summaries to ensure clarity, accuracy, conciseness, and completeness. Provide the final summary directly.
        \vspace{2mm}
        
        Aspect: \{query\}
        \vspace{2mm}
        
        Summary 1: \{sum1\}
        \vspace{2mm}
        
        Summary 2: \{sum2\}
        \vspace{2mm}
        
        ...
        \vspace{2mm}
        
        Final revised summary: \\
        \hline
        
        \textbf{CPS} & 
        Take all provided summaries into account and generate a better, cohesive summary of the given documents, focusing on the specified aspect. Combine and refine the content from the summaries to ensure clarity, accuracy, conciseness, and completeness. Provide the final summary directly.
        \vspace{2mm}
        
        Aspect: \{query\}
        \vspace{2mm}
        
        Documents: \{doc\}
        \vspace{2mm}
        
        Summary 1: \{sum1\}
        \vspace{2mm}
        
        Summary 2: \{sum2\}
        \vspace{2mm}
        
        ...
        \vspace{2mm}
        
        Final revised summary: \\
        \hline
    \end{tabularx}
    \caption{Ensemble Prompts for Aspect-based MDS}
    \label{tab:ensembleaspect}
\end{table*}


\end{document}
