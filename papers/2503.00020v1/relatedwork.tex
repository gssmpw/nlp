\section{Related Work}
Datasets are critical to the performance and safety of generative models. Several studies have been done on the datasets used for training and finetuning LLMs. \cite{liu2024datasets} categorized large amount of datasets into pre-training corpora, instruction fine-tuning datasets, preference datasets, evaluation datasets and traditional NLP datasets and studied their challenges. More recently, a comprehensive study of preference datasets used in fine-tuning has been done in \cite{xiao2024preference} and a study of 16 pre-train datasets and 16 fine-tune datasets from qualitative perspective has been done in \cite{du2024survey}. There have also been systematic studies of LLM benchmarks itself, for instance \cite{li2024multimodal} surveys MLLM benchmarks, reviewing 211 benchmarks that assess MLLMs across understanding, reasoning, generation, and application and a detailed analysis of dataset constructions across diverse modalities has been provided and a critical assessment of 23 state-of-the-art LLM benchmarks using a unified evaluation framework through the lenses of people, process, and technology has been done in \cite{mcintosh2024inadequacies}.

For T2I applications, the research while not as extensive as on LLMs, is surely catching up. For safety, several studies have focused on analyzing the datasets used in T2I models and examining their biases, coverage, and diversity. Societal bias has shown to exist in datasets \cite{meister2023gender}\cite{wang2021revise}\cite{yang2020fairer} and a study of widely used datasets for text-to-image synthesis, including Oxford-102, CUB-200-2011, and COCO has been done in \cite{tan2023texttoimage},  calling inadequate diversity of real-world scenes. Similarly, a study of the LAION-400M dataset contains problematic content across misogyny, pornography, and malignant stereotypes \cite{birhane2021multimodal}. 

Most existing research is on analyzing certain datasets for specific harm types as shown above. There is a new body of work that is being done to address the gaps in the datasets. DataPerf introduced the Adversarial Nibbler challenge \cite{quaye2024nibbler}, emphasizing the importance of gathering prompts to identify harmful or biased outputs in T2I models. Other studies, such as LatentGuardCoPro \cite{liu2024latentguard}, have developed datasets specifically targeting unsafe input prompts, focusing on areas such as violence and hate speech.

By doing a systematic review of existing open datasets for T2I safety, we hope to encourage more such research to build comprehensive safety datasets covering broad classes of harm, that are composed of diverse set of topics and are that are in several languages. One such systematic study of safety datasets has been done for evaluating and improving safety of LLMs \cite{roettger2024safetyprompts}. Our research aims to do the same and fill the gap for T2I model safety.