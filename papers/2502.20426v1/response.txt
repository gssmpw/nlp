\section{Related works}
\label{sec:related}

The study of the persuasion capabilities of LLMs, and more broadly generative AI models, is an emerging field of research with a rapidly growing literature base that already includes surveys**Stoyanovich et al., "A Survey on Persuasive Language Models"**.
We can roughly divide this diverse body of research into two categories: (1) evaluation of persuasion abilities of LLMs and (2) investigation into the factors that make LLMs persuasion effective.

Into the first category will fall the works in which persuasion is evaluated solely in terms of final success**Rieser et al., "Persuasion via Language Models"**.
For example, Wongkamjan \textit{et al.} investigated how AI models use deception to manipulate people in online environments**Wongkamjan et al., "The Dark Side of AI: A Study on Deception and Manipulation by AI Agents"**.
In contrast to our approach, persuasion was recognized by a change of the agent's initial intended action to the one suggested by some other player during negotiations, but there was no attempt to further characterize particular persuasion techniques used.
Other works in this category differ in how they define and measure the success of persuasion but are similar in that they leave for others the challenge of relating tokens returned by a language model to results from human psychology.

More in line with our work are studies in the second category, which try to investigate the role of different factors in successful LLM persuasion, including the use of persuasion strategies. For example, Breum~\textit{et al.} analyzed persuasion tactics used in online settings**Breum et al., "Persuasion in Online Environments: A Study on Tactics and Strategies"**, based their analysis on linguistic dimensions of social pragmatics (e.g., status, similarity, identity) **Jin et al., "Linguistic Dimensions of Social Pragmatics in Persuasive Language Models"** , whereas others  manually curated lists of factors.
Our work also considers factors of persuasion success in a particular social scenario (in our case, Among Us), and we base these factors on a wide selection of persuasion techniques compiled from various sources (Section~\ref{sec:methods:persuasion}).

Because of their universality and ability to understand language, LLMs are often used as game agents**Brown et al., "Language Models as Game Agents"**, for example in Avalon**Avalon Development Team, "Avalon: A Multiplayer Online Strategy Game"**, Diplomacy**Diplomacy Development Team, "Diplomacy: A Real-Time Strategy Game"**, Minecraft**Minecraft Development Team, "Minecraft: A Sandbox Video Game"**, and even Red Dead Redemption 2**Rockstar Games, "Red Dead Redemption 2: A Western Action-Adventure Game"**. In these studies, the emphasis is placed on efficient and human-competitive gameplay.
In the context of this work, the most relevant is the recent application of LLM agents in Among Us**Among Us Development Team, "Among Us: A Multiplayer Online Strategy Game"**, where the focus was on evaluating the reasoning capability of a single LLM (GPT-3.5 Turbo), configured into several distinct personas based on the high-level strategies for crewmates and impostors. The importance of deception and manipulation to effectively play the game is recognized, and in one of the experiments, conversations were tagged by a different LLM with five non-mutually exclusive categories, of which the most relevant for us are `deception', `leadership \& influence', and `suspicion {\&} defense'. Beyond these high-level categories, however, no attempt was made to investigate the particular deception and defense strategies used. In contrast, our work aims at a fine-grained analysis of persuasion techniques employed by a broad selection of state-of-the-art LLMs. To the best of our knowledge, our work is the first comprehensive attempt at examining persuasion techniques employed by LLM game agents at this level of detail.