\documentclass{article} 

\PassOptionsToPackage{round}{natbib}

\usepackage{dsfont}
\usepackage{xargs}

\usepackage{authblk,breakcites}
\usepackage[round]{natbib}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{sidecap}
\usepackage{caption}
\usepackage{subfigure}
% \usepackage{subcaption}
\usepackage{booktabs} 
\usepackage{multirow}
\usepackage[svgnames,dvipsnames]{xcolor}
\usepackage{float} 
\usepackage{wrapfig}
\usepackage{geometry}



\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=Emerald
    }
    


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{enumerate}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{mathrsfs}

\usepackage{algorithm}
\usepackage[noend]{algorithmic}

\usepackage{ulem}

\input{ICML_notation.tex}

\title{Discrete Markov Probabilistic Models}

\author[${}^1$]{Le-Tuyet-Nhi Pham}
\author[${}^2$]{Dario Shariatian}
\author[${}^1$]{Antonio Ocello}
\author[${}^3$]{Giovanni Conforti}
\author[${}^1$]{Alain Durmus}



\affil[${}^1$]{{\small Centre de Math\'ematiques Appliqu\'ees (CMAP), \'Ecole Polytechnique, France}}
\affil[${}^2$]{{\small Sierra lab, Inria, Paris, France}}
\affil[${}^3$]{{\small Department of Mathematics, University of Padova, Italy}}

\date{}

\graphicspath{{fig/}}


\begin{document}

\maketitle

\begin{abstract}
    This paper introduces the Discrete Markov Probabilistic Model (DMPM), a novel algorithm for discrete data generation. The algorithm operates in the space of bits $\{0,1\}^d$, where the noising process is a continuous-time Markov chain that can be sampled exactly via a Poissonian clock that flips labels uniformly at random. The time-reversal process, like the forward noise process, is a jump process, with its intensity governed by a discrete analogue of the classical score function. Crucially, this intensity is proven to be the conditional expectation of a function of the forward process, strengthening its theoretical alignment with score-based generative models while ensuring robustness and efficiency.
    We further establish convergence bounds for the algorithm under minimal assumptions and demonstrate its effectiveness through experiments on low-dimensional Bernoulli-distributed datasets and high-dimensional binary MNIST data. The results highlight its strong performance in generating discrete structures. 
    This work bridges theoretical foundations and practical applications, advancing the development of effective and theoretically grounded discrete generative modeling.
\end{abstract}

     
\section*{Introduction}
%\alain{use the new template https://icml.cc/Conferences/2025/CallForPapers}

% Score-based Generative Models (SGMs) have become a key reference for generating complex data, such as images \citep[see, $e.g.$,][]{rombach2022high,ramesh2022hierarchical,saharia2022photorealistic}, audio files \citep{chen2020wavegrad,kong2020diffwave}, and videos \citep{ho2022video,villegas2022phenaki,bar2024lumiere}. In continuous time, their power lies in several key elements: a simple forward distribution for the noise-adding phase, often using the Ornstein--Uhlenbeck process, which allows (1) exact simulation; (2) exponential convergence towards the stationary distribution; and (3) efficient estimation of the score function through an $\mrl^2$-norm regression problem. These features provide a robust theoretical framework and facilitate learning for data in continuous spaces.

% However, adapting these models to discrete spaces proves to be much more challenging. Several approaches have been proposed to extend these methods to discrete spaces \citep{austin2021structured,hoogeboom2021argmax,shi2024simplified}, but they struggle to compete with other generative models available in the literature. This difficulty stems mainly from the complexity of the proposed formulations. Most of these approaches impose approximations on the transition kernel of the forward process, which undermines the mixing properties essential for good modeling. Moreover, the estimation of the score function for the backward process often relies on ratios of probabilities of successive processes, a procedure that is numerically unstable compared to the standard $\mrl^2$-norm minimization.
% % One of powerful methods of generative techniques, score-based generative models (SBGMs), use gradient-based approaches to discover the underlying data distribution. SBGMs have drawn much attention in the past decades, but they were mostly addressed in the context of continuous data, such as in diffusion models. After \citet{conforti2024klconvergenceguaranteesscore} showed the comprehensive understanding of SBGMs in continuous domains, recent studies have looked at ways to extend score-based approaches to discrete state spaces, opening a door for generative modeling in situations where data takes on discrete values. 

% {\red Generative models for the discrete state space have significant applications such as graph generation \citep{vignac2022digress}, text-to-sound generation \citep{yang2023diffsound}, protein design \citep{gruver2024protein}, financial tabular data generation \citep{sattarov2023findiff}, or text modeling \citep{gong2022diffuseq}. The discrete structure can be handled by embedding in the Euclidean space \cite{dieleman2022continuous,chen2022analog} or the simplex \cite{richemond2022categorical} while keeping using the forward continuous diffusion models; or by designing the flow processes over discrete state space \citep{campbell2022continuous}. Following the discrete flow approach of \citep{campbell2024generative}, \citet{gat2024discrete} provided a computational point of view of discrete flow matching. However, it is still not universally used due to the complexity and high cost of training objects. 
% Another recent approach is generative modeling with arbitrary Markov processes using generator matching, which can be estimated efficiently using a Neural Network, introduced by \citet{holderrieth2024generator}. This model can be used for various state spaces, including continuous and discrete settings. However, it lacks the theoretical guarantee of the sampling and includes complex formulations.
% One remarkable model to step towards the complex formulations is the discrete diffusion process namely "masked" diffusion \cite{austin2021structured}. Recently, \citet{shi2024simplified} established several properties for this model and built a simpler expression of the training objective. However,  the model lacks the theoretical guarantee and is costly-trained. This paper is a step towards closing this gap.}
% % , which can be rewritten as a signal-to-noise ratio and hide some useful features. However, the theoretical guarantee is still missing and this model is costly to train in practice due to the ratio. This paper is a step towards closing this gap.
% % Specifically, \citet{holderrieth2024generator} constructed an appropriate Markov process that transforms a simple distribution into the desired data distribution via the generator, which can be estimated efficiently using a Neural Network. This model can be used for various state spaces, including continuous and discrete settings. Despite its general application, the generator matching model is still not widely used as it requires a lot of regularity assumptions, the objective is complex, and there is a lack of convergence guarantee.
% % In contrast to their successful applications in many fields, they remain not widely used due to the complex formulations and training objectives.
% \textbf{Contributions.} In this work, we focus on Discrete Markov Probabilistic Models (DMPMs) for the hypercube in $\R^d$. Using some effective properties of the time-reversal Markov dynamic explored by \citet{conforti2022time}, we successfully not only show the theoretical guarantee for our model but also simplify the training objective extraordinarily. Our contributions are as follows:
% \begin{itemize}
% \itemsep=0em
%   \item As we have an explicit interpretation of the nosing process, we provide a comprehensive understanding of the backward evolution. %under both the transition matrix and optimal control point of view.
%     \item We establish a conditional expectation expression of the score function, which is the drift between the noising and denoising process. This enables using a Neural Network to train a simple objective, as the score function is then an $\mrl^2$-projection instead of a ratio as before.
%     \item We develop a robust algorithm to generate the samples that closely mimic the clean data, and show its convergence.
%     \item We test our algorithm on the Binary MNIST and achieve an outstanding performance compared with the previous proposals.
% \end{itemize}


Score-based Generative Models (SGMs) have become a key reference for generating complex data, such as images \citep[see, \textit{e.g.},][]{rombach2022high,ramesh2022hierarchical,saharia2022photorealistic}, audio \citep{chen2020wavegrad,kong2020diffwave}, and video \citep{ho2022video,villegas2022phenaki,bar2024lumiere}. In continuous time, this approach benefits from a strong theoretical framework, and a scalable, stable learning objective. 



% By contrast, \emph{discrete} generative modeling continues to pose significant challenges. Several methods extend SGMs to discrete spaces by introducing tailored discrete noising processes. Examples include masked diffusion \citep{austin2021structured}, multinomial diffusion and Argmax Flows \citep{hoogeboom2021argmax}, or further refinements thereof \citep{shi2024simplified}. Although these approaches have yielded promising results, they often require non-trivial approximations of the forward transition probabilities. 
% % and often resort to computationally unstable ratio-based estimators of backward transitions. 
% As a result, they lack favorable mixing properties and the simple $\mrl^2$-type training objectives that characterize their continuous counterparts. Furthermore, the theoretical understanding of convergence rates and sampling guarantees in these discrete diffusion models remains relatively scarce.

% The approach pioneered in \citet{campbell2022continuous} sidesteps these issues by introducing continuous-time Markov chains (CTMCs), a flexible framework that also allows using additional correction steps, to increase sample quality at the expense of training cost, as implemented in a flow matching approach \citep{gat2024discrete}. However, training and sampling suffer high costs in higher dimensional settings, and the convergence guarantees are limited. \citet{holderrieth2024generator} proposed a more general generator-matching framework that adapts to arbitrary Markov processes, but this method, being very generic, introduces intricate training objective involving ratio-based estimators, and requires very strong theoretical assumptions for convergence, thus suffering from the same pitfalls.

% Another line of work establishes stochastic integral expressions for discrete diffusion \citep{ren2024discrete}, obtaining the first KL divergence error bounds under strong assumptions. However, they obtain complicated and sub-optimal bounds as compared to ours, in particular with respect to time horizon, and ultimately lack empirical validation.

% For further details on existing approaches, see \cref{app:related_works}.

By contrast, \emph{discrete} generative modeling continues to pose significant challenges. Multiple diffusion-based methods have recently been proposed for discrete spaces \citep{austin2021structured,hoogeboom2021argmax,shi2024simplified,campbell2022continuous,holderrieth2024generator,ren2024discrete}, or spaces of mixed type \citep{bertazzi2024piecewisedeterministicgenerativemodels}, but there is still no consensus on which approach is theoretically sound or most practically efficient. Various formulations rely on complex forward kernels or computationally unstable ratio-based estimators for backward transitions, leading to limited convergence guarantees and high computational costs in high dimensions. Furthermore, recent analyses of discrete diffusions have introduced valuable theoretical tools \citep{campbell2022continuous,holderrieth2024generator,ren2024discrete}, yet most methods remain either overly generic or require strong assumptions, making them difficult to scale or to deploy with simple, stable training objectives.


\noindent
\textbf{Contributions.} 
In this paper, we introduce \emph{Discrete Markov Probabilistic Models} (DMPMs), a new class of generative models for discrete data that bridges these gaps.  
Our framework specializes the forward noising process to a continuous-time Markov chain on the hypercube $\{0,1\}^d$. 
% where each bit is flipped according to a Poisson clock.
Leveraging theoretical insights on time-reversal Markov dynamics of this process, 
this choice preserves the key strengths and structure of continuous SGMs, addressing the issues raised in prior work.
% ---exact simulation of the forward process and simple mixing behavior---while yielding a backward \emph{jump process} whose drift is governed by a \emph{discrete analogue of the classical score function}.  
% Notably, we show that this “discrete score” can be expressed as a conditional expectation, paving the way for a simple $\mrl^2$-regression objective that avoids the costly ratio-based estimators used by prior works.
Our main results are summarized as follows:
\begin{itemize}
\itemsep=0em
\item 
\textbf{Forward-Backward Construction.}\ We provide a principled derivation of the noising (forward) and denoising (backward) processes.

\item 
\textbf{Score function and stable estimation.} Our analysis reveals how the time-reversal inherit a score function with an explicit conditional expectation form. By casting learning as an $\mrl^2$ projection, we eliminate the need for numerically unstable ratios of transition probabilities. This leads to a robust training procedure.

\item 
\textbf{Theoretical Guarantees.} We prove that DMPMs converge to the underlying data distribution under minimal assumptions, providing non-asymptotic error bounds that underscore the method’s reliability.

\item 
\textbf{Empirical Performance.} We demonstrate that our approach attains competitive or superior performance on discrete datasets, including binarized MNIST, frequently with fewer function evaluations compared to existing discrete diffusion frameworks \citep[\eg, 2.89 vs 7.34 FID compared to Discrete Flow Matching,][with 2.5x fewer network calls]{gat2024discrete}.
\end{itemize}

% In summary, DMPMs offer a conceptually clean and computationally efficient alternative to existing diffusion-based models for discrete data, combining rigorous theoretical grounding with empirical effectiveness. We believe these features will make DMPMs broadly applicable in diverse tasks ranging from text to tabular and graph-structured data.









%The goal of classic generative models is to generate the samples that closely mimic the data has been observed. SBGMs aim at learning the explicit distribution of data through the time-reversal process starting from a prior simple distribution, which can be expressed as a perturbation of the forward diffusion process, with the drift includes the score function. In other words, SBGMs concentrate on learning the score function, which is the log-probability density's gradient. The idea behind score-based models is that noisy samples may be gradually refined towards the target distribution provided this score function can be reliably estimated.




% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% Section~\ref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% $\mathit{38}^{th}$ International Conference on Machine Learning},
% Online, PMLR 139, 2021.
% Copyright 2021 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2021\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2021\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a
% single line centered above a horizontal rule which is $1$~point thick.
% The running head should be centered, bold and in $9$~point type. The
% rule should be $10$~points above the main text. For those using the
% \textbf{\LaTeX} style file, the original title is automatically set as running
% head using the \texttt{fancyhdr} package which is included in the ICML
% 2021 style file package. In case that the original title exceeds the
% size restrictions, a shorter form can be supplied by using

% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.









\textbf{Notation.} Given a measurable space $(\mse, \mce)$, we denote by $\Pc(\mse)$ the set of probability measures on $\mse$. Given two probability measures $\mu, \nu \in \Pc(\mse)$, the Kullback-Leibler divergence (also called relative entropy) of $\mu$ with respect to $\nu$ is defined as $\KL(\mu|\nu):=\int \log(\rmd\mu/\rmd\nu)d\mu$ if $\mu$ is absolutely continuous with respect to $\nu$, and $\KL(\mu|\nu)=+\infty$ otherwise. The total variation distance of $\mu$ and $\nu$ is defined as $\tvnorm{\mu-\nu}=\int |\rmd\mu/\rmd R-\rmd\nu/\rmd R|\rmd R$ for any $R \in \Pc(\mse)$ such that $\mu$ and $\nu$ are absolutely continuous with respect to $R$. Consider a random variable $X$, we denote by $\mathrm{Law}(X)$ the law of $X$. 



\section{Forward and backward process of DMPMs}\label{matrix_POV}
%\alain{after some thoughts, keep index component as superscrit, but wirte $s^{\theta,\ell}$ and not $s^{\theta_{\ell}}$}

% As already emphasized in the introduction, the model that we consider shares many similarities with Markovian generative models. From a high-level perspective, these models are based on Markov processes $(\foX_t)_{t \in  \ccint{0,\Tf}}$ starting from $\foX_0\sim \mustar$ and defined on a fixed interval with final horizon $\Tf>0$. The $(\foX_t)_{t \in  \ccint{0,\Tf}}$ will be referred to as the forward or noising process. These Markov processes  define though their marginal distributions a path that  smoothly transport the data distribution $\mustar$ to a known base distribution by slowly adding noise. In common diffusion-based generative models, the forward process is a Ornstein--Uhlenbeck process and  the base distribution is Gaussian.  Once the noising process has been fixed, the corresponding time-reversal $(\baX_t)_{t\in\ccint{0,\Tf}}$ is defined as $\baX_{t} = \foX_{\Tf-t}$ which by design, naturally transports this time through their marginals distribution, the base distribution to $\mustar$. The process $(\baX_t)_{t\in\ccint{0,\Tf}}$ is also coined the backward process. However, in most cases simulation of  $(\baX_t)_{t\in\ccint{0,\Tf}}$ is in general intractable and approximate inference can only be performed. This approximation leverages the fact that for most noising processes that has been proposed in the literature, the time-reversal process $(\baX_t)_{t\in\ccint{0,\Tf}}$ is still a Markov process with characteristics that can be learned relying only on the simulation of the noising process.  Taking the example of common diffusion-based generative models again, the time-reversal is also solution a stochastic differential equation where the drift is intractable but can be learned efficiently using score matching estimation using that this function can be expressed on the form of a conditional expectation with respect to the forward process. 

% Our methodology follows the generic rationale just described above. We first present the Markov noising process that we consider and its time-reversal. In particular, the noising process is a relatively simple Continuous Time Markov chain (CTMC) and we show that that this time-reversal is also a CTMC. Up to our knowledge, in contrast to existing Markovian generative models that have been proposed, we show that the intractable of the characteristics (jump rate and kernel) of this CTMC can be expressed on the form of a conditional expectation with respect to the forward process, and therefore can be efficiently learnt solving the corresponding regression problem.
% As an appetizer, we first focus on the simplest case $\msx = \{0,1\}$. 


We introduce a generative modeling framework that adapts classical diffusion-based methods to discrete state spaces. Let $(\foX_t)_{t \in [0,T]}$ be a forward Markov process, starting from $\foX_0 \sim \mustar$, the data distribution of interest, and evolving over the fixed time horizon $\Tf>0$. This forward process smoothly transports $\mustar$ into a simple base distribution, at time $\Tf$. In the continuous setting, a well-known example is the Ornstein--Uhlenbeck process, which converges to a Gaussian base measure. 

Given the forward dynamics, we define the corresponding backward process $(\baX_t)_{t \in [0,\Tf]}$ by $\baX_t \;\coloneqq\; \foX_{T - t}$. By construction, $(\baX_t)$ evolves in reverse from the base distribution (at $t=0$) back to the original data distribution $\mustar$ (at $t=\Tf$). Although $(\baX_t)$ is also Markovian in many classical settings, its transition rates or drift terms are often {intractable}, preventing direct simulation. Instead, most generative models leverage the fact that the backward dynamics can be characterized and learned from forward-simulated paths. In the continuous setting, this learning is achieved via score-matching methods.

Following this principle, we define a simple forward continuous time Markov chain (CTMC) on $\{0,1\}^d$ where bits flip according to a Poisson clock, and show its time-reversal remains a tractable CTMC. We derive closed-form expressions for the backward transition rates, which involve conditional expectations over the forward process. This enables, for the first time, an efficient training procedure based on regression in the discrete setting.

%\alain{provide general introduction to generative models based on time reversal}

\subsection{Simplest case $\msx=\l\{0,1\r\}$}\label{simplestcase}

%\begin{enumerate}
%\item Simulation of the forward
%\item Introduction of the characteristics of the backward (jump rate and Markov kernel)
%\item Simulation of the backward
%\end{enumerate}
%\alain{start of the forward with $\mustar$}

% We describe here the noising process that we consider in this paper, its simulation and proceed similarly with its time-reversal in the case $d=1$, \ie, the state space includes only two points $0$ and $1$.
To introduce the key ideas, we first consider the simplest case $\msx = \{0,1\}$.
The forward process $( \overrightarrow{X}_t)_{t\in \ccint{0,\Tf}}$ starting from  $\overrightarrow{X}_0 \sim \mustar$, is defined as follows. Consider the fixed jump times $(T_i)_{i\in\{1,\ldots,N\}}|N\simiid \unif(\ccint{0,\Tf})$ of a Poisson process over $\ccint{0,\Tf}$ where $N \sim \poissondist(\lambda \Tf)$ is the number of jump, and $\lambda >0$ is a prescribed jump rate. 
Without loss of generality, we assume that $0=T_0 \leq T_1< \ldots < T_N$. We define recursively $(\overrightarrow{X}_t)_{t \in \ccint{0,\Tf}}$ over $\ocint{T_{i},T_{i+1}}$. Suppose that $\overrightarrow{X}_{T_i}$ has been defined we set $\overrightarrow{X}_t = \overrightarrow{X}_{T_i}$ for any $t \in \ooint{T_{i},T_{i+1}}$ and %{\color{red} $\overrightarrow{X}_{T_i} = \overrightarrow{X}_{T_{i-1}} - \bfe_{\ell_i} \pmod{2}$,}
 $\ovr{X}_{T_{i+1}} = 1-\ovr{X}_{T_i}$. It is well known that $(\overrightarrow{X}_t)_{t \in \ccint{0,\Tf}}$ is a Markov jump process \citep[Section 6]{mcbook} with generator $\overrightarrow{q}$ defined for any $x,y \in \msx$ as
\begin{align}\label{eq:def_generator_d=1}
     \overrightarrow{q}(x,y):=\begin{cases}
        \hspace{0.2cm}\lambda \eqsp, \quad &\text{if } y \neq x \eqsp,\\
        -\lambda\eqsp, \quad &\text{otherwise\eqsp.} 
    \end{cases}
\end{align}
The transition probability matrix $\PP(\foX_t = y | \foX_0 = x) =\overrightarrow{p}^1_{t}(x, y)$, for $x,y \in \msx, 0\leq t\leq  \Tf$, is known to be
\begin{equation}
\label{eq:def_density_one_dim_transition}
  \overrightarrow{p}_{t}^1(x,y) = \begin{cases}
        \frac{1}{2}+\frac{1}{2}\rme^{-2\lambda t}\eqsp, \quad &\text{if } x=y \eqsp,\\
        \frac{1}{2}-\frac{1}{2}\rme^{-2\lambda t}\eqsp, \quad &\text{otherwise\eqsp.} 
    \end{cases} 
\end{equation}
The proof of this result is given in the supplementary material \ref{proof_eq:transition_d=1}.
%\alain{give ref sec}.
%\ao{add result in the appendix}
To recover the data distribution, we analyze the time-reversed process, which is denoted by $(\overleftarrow{X}_t)_{t\in\ccint{0,\Tf}}$, and defined as $\overleftarrow{X}_t=\overrightarrow{X}_{\Tf-t}$ for any $t\in \ccint{0,\Tf}$.
 \citet[Theorem 2.8]{conforti2022time} shows that $(\overleftarrow{X}_t)_{t\in\ccint{0,\Tf}}$ is also a \emph{non-homogeneous} CTMC, \ie, it is associated with a family of generator matrices $(\overleftarrow{q}_t)_{t \in\ccint{0,\Tf}}$ which satisfies the time-reversal formula: for any $0 \leq t \leq \Tf$ and  $x,y \in \msx$,
\begin{equation}
\label{eq:time_rev_formula}
    \mu_{\Tf-t}(x)\overleftarrow{q}_t(x,y)=\mu_{\Tf-t}(y)\overrightarrow{q}(y,x) \eqsp.
\end{equation}
Since $\overrightarrow{q}$ is symmetric (see \eqref{eq:def_generator_d=1}) and $\mu_{\Tf-t}(x) > 0$ for all $x \in \msx$, $t\in [0,\Tf)$, we deduce that the backward generator $\ovl{q}_t$ for $0\leq t < \Tf$ is given for any $x,y \in\msx$ by
\begin{equation}\label{eq:generator_backward_d=1}
    \overleftarrow{q}_t(x,y) =\overrightarrow{q}(x,y)\dfrac{\mu_{\Tf-t}(y)}{\mu_{\Tf-t}(x)}\eqsp,
\end{equation}
% and $\overleftarrow{q}_t(x,y)=\overrightarrow{q}(x,y)\parenthese{1-\dfrac{\mu_{\Tf-t}(x)-\mu_{\Tf-t}(y)}{\mu_{\Tf-t}(x)}}$
with $\mu$ the forward marginal distribution satisfies $\mu_0=\mustar$. 
%This means that the backward process can be expressed as a perturbation of the forward dynamic, and we just need to compute the backward generator $\overleftarrow{q}_t(x,y)$ in cases $y=1-x$.
%This means we just need to estimate the backward generator $\overleftarrow{q}_t(x,y)$ in cases $y=1-x$.
Having access to $(\overrightarrow{q}_t)_{t\in\ccint{0,\Tf}}$ and $\mu_t$ allows to sample from $\overleftarrow{X}_t$ for any $t\in\ccint{0,\Tf}$ as follows. Define  $s_t: \msx \to \R$  for any $x\in \msx$ by
\begin{equation}
\label{eq:def_s_d_1}
    s_{t}(x):=\dfrac{\mu_{\Tf-t}(x)-\mu_{\Tf-t}(1-x)}{\mu_{\Tf-t}(x)} \eqsp.
\end{equation}
$s_{t}$ acts as a discrete derivative in $\msx$ of $\log \mu_t$, and thus serves as a discrete analogue of the score function in continuous models.
With this notation, $\overleftarrow{q}_t(x,y)$ \eqref{eq:generator_backward_d=1} can be expressed, for any $x,y \in \msx$, for $0\leq t < \Tf$, as:
\begin{equation*}%\label{eq:generator_backward_d=1_1}
  \overleftarrow{q}_t(x,y) = -\lambda \1_{\{ y = x\}}+ \lambda(1- s_t(x))  \1_{\{ y = 1-x\}} \eqsp.
\end{equation*}

%This expression allows us to conclude that the score function is an $\mrl^2$-projection, meaning that Neural Network can be used to train the following simple objective to discover the approximate score
%{\small
%\begin{align*}
%   \min_{\theta\in \Theta} \E \l[ \l| s_{t}^{\theta}(\overrightarrow{X}_{\Tf-t})-\dfrac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}+\dfrac{4(\overrightarrow{X}_{\Tf-t}-\overrightarrow{X}_0)^2}{1-\alpha_{\Tf-t}^2} \r|^2 \r]\eqsp.
%\end{align*}
%}

Then, starting from a sample $\overleftarrow{X}_0 \sim \mu_{\Tf}$, and a sequence of \iid~random variables distributed according to the exponential distribution with parameter $1$, $\{E_i\,: \, i \in \nset\,\}$, we can define the jump times $(T_i)_{i\in \nset}$ %$(T_i)_{i\in\{1,\ldots,N\}}|N\simiid \unif(\ccint{0,\Tf})$ 
of the backward process and its transition by induction setting $T_0=0$. Given $(T_i,\baX_{T_i})$, we define the next jump time as $T_{i+1} = T_i + \Delta T_{i+1}$, where $\Delta T_{i+1} = \inf \{t \geq 0\, :\, \int_{0}^t \blambda_{T_i+r}(\baX_{T_i})\rmd r \geq E_i \}$, 
% $\Delta T_{i+1} = \inf \{t \geq 0\, :\, \int_{0}^t \blambda_{T_i+t}(\baX_{T_i}) \geq E_i \}$, 
where $\blambda_{t}(x) = \lambda (1- s_t(x))$. Then, set $\overleftarrow{X}_t = \overleftarrow{X}_{T_i}$ for $t\in\ooint{T_i,T_{i+1}\wedge \Tf}$, and finally if $T_{i+1} < \Tf$, set $\overleftarrow{X}_{T_{i+1}} = 1-\overleftarrow{X}_{T_i}$.
% \np{reference: also the Owen's book?}
% \np{in this simple case, kernel $k=1$?}


Eventhough $\overleftarrow{q}_t$ is intractable, it can be approximated with the function $s_t$. 
% Indeed, this function plays a role similar to the score function in the usual diffusion model, in particular, 
In particular, we show that $s_t$ can be expressed as a conditional expectation over the forward process. For $x\in\msx$ and $t \in [0,\Tf)$,
{\small
  \begin{equation}
    \label{eq:function_score_1d}
  \textstyle        s_{t}(x)=\E \l[\dfrac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}-\dfrac{4\alpha_
  {\Tf-t}(\overrightarrow{X}_{\Tf-t}-\overrightarrow{X}_0)^2}{1-\alpha_{\Tf-t}^2} \middle| \overrightarrow{X}_{\Tf-t}=x\r]\eqsp,
    \end{equation}
}

with 
{\small 
\begin{equation}
\label{eq:def_alpha}
    \alpha_t := \rme^{-2\lambda t} \eqsp.
\end{equation}
}

Indeed, with $\overleftarrow{d}_t : (z, x) \mapsto \P[\foX_0=z | \foX_{\Tf-t}=x]$:
{\small
\begin{align*}
s_{t}(x) &= \dfrac{\mu_{\Tf-t}(x)-\mu_{\Tf-t}(1-x)}{\mu_{\Tf-t}(x)}\\
    &=\sum_{z\in \msx}(1 -\dfrac{p^1_{\Tf-t}(z,1-x)}{p^1_{\Tf-t}(z,x)})\dfrac{\mu_0(z)\overrightarrow{p}^1_{\Tf-t}(z,x)}{\mu_{\Tf-t}(x)}\\
    & = \sum_{z\in \msx}\dfrac{2\alpha_{\Tf-t}}{1-\alpha_{\Tf-t}^2}(1-\alpha_{\Tf-t}-2(x-z)^2) \overleftarrow{d}_t(z, x) \\ 
    =\E &\l[\dfrac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}-\dfrac{4\alpha_{\Tf-t}(\overrightarrow{X}_{\Tf-t}-\overrightarrow{X}_0)^2}{1-\alpha_{\Tf-t}^2} \middle| \overrightarrow{X}_{\Tf-t}=x\r]\eqsp.
\end{align*}}

Therefore, the function $s$ is an $\mrl^2$-projection and its approximation boils down to a regression problem. %\alain{Take one page to describe the one dimensional case}
\subsection{General state space $\msx=\l\{0,1\r\}^d$}

\subsubsection{Forward noising process}
We generalize the previous results for the hypercube in $\R^d$, i.e., the state space is $\msx=\l\{ 0,1\r\}^d$ with $d\in \N^*$.
We consider the forward homogeneous Markov process $(\overrightarrow{X}_t)_{t \in \ccint{0,\Tf}}$ starting from  $\overrightarrow{X}_0 \sim \mustar$, defined as follows.

We consider the jump times $(T_i)_{i\in\{1,\ldots,N\}}|N\simiid \unif(\ccint{0,\Tf})$ of a Poisson process over $\ccint{0,\Tf}$ where $N \sim \poissondist(\lambda \Tf)$ is the number of jump. Without loss of generality, we suppose that $T_0 = 0 \leq T_1< \ldots < T_N$. We define recursively $(\overrightarrow{X}_t)_{t \in \ccint{0,\Tf}}$ over $\ocint{T_{i},T_{i+1}}$ as follows. Suppose $\overrightarrow{X}_{T_i}$ has been defined. We set $\overrightarrow{X}_t = \overrightarrow{X}_{T_i}$ for $t\in\ooint{T_i,T_{i+1}}$, and finally, 
%if $T_{i+1} < \Tf$, 
set $\ovr{X}_{T_{i+1}}^{\ell_i} = 1-\ovr{X}_{T_i}^{\ell_i}$, where $\ell_i \sim \unif(\{1,\ldots,d\})$, with $\ell_i$ independent from the past, and $\ovr{X}_{T_{i+1}}^{j} = \ovr{X}_{T_i}^{j}$ for $j \neq \ell_i$. The process $(\overrightarrow{X}_t)_{t \in \ccint{0,\Tf}}$ is a Markov jump process again, associated with the generator $\overrightarrow{q}$ defined for any function $g : \msx \to \rset$ as
%  \alain{change $Q$ to $k$}
{\small 
\begin{equation}
  \label{eq:def_generator}
  \overrightarrow{q} g (x) = \lambda   \{ kg(x) - g(x) \}\eqsp,
\end{equation}
}

where $\lambda >0$ is a prescribed jump rate and $k$ is the Markov kernel defines as: for any $x,y \in \msx$,
% \begin{align*}
%     k(x,y) :=\begin{cases}
%         1/d \eqsp, \quad &\text{ if } \|x-y\|^2=1\eqsp,\\
%         \hspace{0.2cm}0  \eqsp, \quad &\text{otherwise}\eqsp,
%     \end{cases} 
% \end{align*}
{\small
\begin{align*}
    k(x,y) := \mathds{1}_{\|x-y\|^2=1}\cdot 1/d\eqsp.
\end{align*}
}
% This means that at any time $t$, the process $\overrightarrow{X}_t$ can either stay or jump into a new state that differs from the old state on only one component.

Similarly to the one-dimensional case, we can establish an explicit expression for the transition probability matrix $\overrightarrow{p}_{t}$ for $0\leq t\leq \Tf$ as
{\small
\begin{align}\label{eq:transition_d}
    \overrightarrow{p}_{t}(x,y)=\prod_{i=1}^d \overrightarrow{p}_{t}^1(x^i,y^i) \eqsp,
\end{align}
}
% \alain{drop all right left parenthesis in text}

where $\ovr{p}_t^1$ is defined in \eqref{eq:def_density_one_dim_transition} and $x=(x^i)_{i=1}^d \in \msx$ and $y=(y^i)_{i=1}^d \in \msx$. The detailed computation is given in the supplementary material \ref{proof_eq:transition_d}
%\alain{provide link to the section}. 
The factorization of the transition probability in \eqref{eq:transition_d} is of great practical interest, as this tells us that the dynamic of the forward process simply consists in the single-bit forward dynamic applied independently to each component, as described in \cref{simplestcase}. As a consequence, the forward marginal distribution $\mu_t$ of $\foX_t$ admits the formula
{\small
\begin{align}\label{marginaldist}
    \mu_t(x)=\sum_{z\in \msx} \mu_0(z)\prod_{i=1}^d \overrightarrow{p}_{t}(z,x) \eqsp.
\end{align}
}
% Expression of the transition probability in terms of the product of each component is interesting to interpret. This 
% tells us that given that the initial point $\overrightarrow{X}_0$ the Markov process considered on the product space $\l\{0,1\r\}^d$ is indeed constructed as $d$ independent copies $(\overrightarrow{X}^i_t)_{t\in\ccint{0,\Tf}}$ of the dynamics mentioned in section \ref{simplestcase} starting $\foX_0^i$.

% With the previous interpretation at hand, 

% As a consequence, the forward marginal distribution $\mu_t$ of $\foX_t$ can be expressed as 
% $\mu_t(x)=\sum_{z\in \msx} \mu_0(z)\prod_{i=1}^d \overrightarrow{p}_{t}(z,x)$.



\subsubsection{Backward process}\label{section_backward}

Denote by $(\overleftarrow{X}_t)_{t\in\ccint{0,\Tf}}$, the time-reversal process associated with $(\overrightarrow{X}_t)_{t \in\ccint{0,\Tf}}$, and defined as $\overleftarrow{X}_t = \overrightarrow{X}_{\Tf-t}$ for any $t\in\ccint{0,\Tf}$. As in the case $d=1$,
\citet[Theorem 2.8]{conforti2022time} shows that $(\overleftarrow{X}_t)_{t\in\ccint{0,\Tf}}$ is also a \emph{non-homogeneous} CTMC, with backward generator matrix $(\overleftarrow{q}_t)_{t \in\ccint{0,\Tf}}$ that  satisfies \eqref{eq:time_rev_formula} and therefore \eqref{eq:generator_backward_d=1},  proceeding as before. 
As in the case $d=1$, we show that $(\overleftarrow{q}_t)_{t \in\ccint{0,\Tf}}$ depends only on a discrete score function, which we now introduce.
 
First note that  \eqref{eq:generator_backward_d=1} and \eqref{eq:def_generator} yield $\overleftarrow{q}_t(x,y)= 0$, for $x,y\in\msx$ satisfying $\|x-y\|^2\neq 1$ and $x\neq y$.
Then, for $0\leq t < \Tf$, define $s_t : \msx \to \rset^d$ for any $x \in\msx$, $s_t(x)=\{s_t^{\ell}(x)\}_{\ell=1}^d$ as the vector in $\R^d$, with components $\ell \in\{1,\ldots,d\}$,
{\small
\begin{align}\label{score1}
   s_t^{\ell}(x):=\dfrac{\mu_{\Tf-t}(x)-\mu_{\Tf-t}(\varphi^{(\ell)}(x))}{\mu_{\Tf-t}(x)} \eqsp,
\end{align}
}

where $\varphi^{(\ell)}: \msx \to \msx$ is defined  as  $\varphi^{(\ell)}(x)=y$,
% \alain{change $\varphi_{\ell}$ in $\varphi^{(\ell)}$}
with $y$ obtained by flipping the $\ell$-th bit of $x$, \ie, $y^\ell=1-x^\ell$, and $y^i=x^i$ for $i\neq \ell$.
Note that the function $s$ thus defined is an extension to the case $d\geq 1$ of the function $s$ defined for $d=1$ in \eqref{eq:def_s_d_1}. As a result, $s_t$ is a conditional expectation over the forward process, where each of its components admits an expression similar to the 1d case \eqref{eq:function_score_1d}.
% For fixed $t$, $s_t$ can be interpreted as a discrete gradient in $\msx$ for $\log \mu_t$ and therefore plays the natural counterpart of score functions in diffusion models. 
\begin{proposition}\label{prop:1}
The score function can be expressed as a conditional expectation:
{\small
\begin{align}\label{eq:function_score_d}
     s_{t}^\ell(x)=\E \l[  f^{\ell}_t(\overrightarrow{X}_0^\ell,  \overrightarrow{X}_{\Tf-t})| \overrightarrow{X}_{\Tf-t}=x\r]\eqsp,
\end{align}
}

where  $t \in [0,\Tf)$, $x\in \msx$, $\ell = 1,\ldots,d$, $s_t^{\ell}$ is the $\ell$-th component of the score function $s_t$, and
{\small 
\begin{equation}\label{def:f}
f^{\ell}_t(\overrightarrow{X}_0^\ell,  \overrightarrow{X}_{\Tf-t})   =  \dfrac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}-\dfrac{4\alpha_{\Tf-t}(\overrightarrow{X}_{\Tf-t}^\ell-\overrightarrow{X}_0^\ell)^2}{1-\alpha_{\Tf-t}^2}\eqsp.
\end{equation}
}
\end{proposition}

The proof of this result is given in \cref{proof_prop:1}.

Furthermore, for $0\leq t < \Tf$, $x, y \in \msx$, we can write the backward generator $\overleftarrow{q}_t(x,y)$, as given in  \eqref{eq:time_rev_formula}, as:
{\small
\begin{equation*}
  \overleftarrow{q}_t(x,y) =-\lambda \1_{\{ y = x\}}+ \sum_{\ell=1}^d     \lambda(1- s_t^{\ell}(x))  \1_{\{ y = \varphi^{(\ell)}(x)\}} \eqsp.
\end{equation*}
}

This defines the non-homogeneous jump rate $\balambda_t$ and jump kernel $\bak_t$ of the backward process:
{\small
\begin{equation}
\label{eq:def_char_back}
\begin{aligned}
    \balambda_t(x)& = \lambda \sum_{\ell=1}^{d}(1- s_{t}^{\ell}(x))\\
    \bak_t(x,y) &=  \mathds{1}_{y=\varphi^{(\ell)}(x)} \cdot \lambda(1- s_{t}^{\ell}(x)) / \balambda_{t}(x)
    \end{aligned}
\end{equation}
}

for $x,y \in\msx$ and $t\in\ccint{0,\Tf}$.
Thus, having access to $(s_t)_{t\in\ccint{0,\Tf}}$ and $\mu_{\Tf}$, for any $t\in\ccint{0,\Tf}$, allows to sample from $\overleftarrow{X}_t$, as follows.
% \alain{fill, define $\baq$, $\balambda$ and $\bak$}
Starting from a sample $\overleftarrow{X}_0$ from $\mu_{\Tf}$ and a sequence of \iid~random variables distributed according to the exponential distribution with parameter $1$, $\{E_i \,: \, i \in \nset\ \}$, we can define the jump times $(T_i)_{i \in \nset}$
%$(T_i)_{i\in\{1,\ldots,N\}}|N\simiid \unif(\ccint{0,\Tf})$ 
of the backward process and its transition by induction setting $T_0=0$. Given $(T_i,\baX_{T_i})$, we define the next jump time as $T_{i+1} = T_i + \Delta T_{i+1}$, where $\Delta T_{i+1} = \inf \{t \geq 0\, :\, \int_0^t \balambda_{T_i+r}(\baX_{T_i})\rmd r \geq E_i \}$.
% $\Delta T_{i+1} = \inf \{t \geq 0\, :\, \int_0^t \blambda_{T_i+t}(\baX_{T_i}) \geq E_i \}$
Then, set $\overleftarrow{X}_t = \overleftarrow{X}_{T_i}$ for $t\in\ooint{T_i,T_{i+1}\wedge \Tf}$, and finally if $T_{i+1} < \Tf$, $\overleftarrow{X}_{T_{i+1}} = \varphi^{(\ell_i)}(\overleftarrow{X}_{T_i})$ for $\ell_i\in\{1,\ldots,d\}$ which is distributed according to $ \categorial(\{ \bak_{T_{i+1}}(\baX_{T_i},\varphi^{(\ell)}(\baX_{T_i}))\} _{\ell=1}^d)$.
% $\bak_{T_i}(x,\cdot) = \categorial(\{  \lambda(1- s_{T_{i+1}}^{\ell}(\baX_{T_i})) / \balambda_{T_{i+1}}(\baX_{T_i}) \}_{\ell=1}^d)$. 
Another equivalent procedure to generate the backward is provided in the supplement \ref{appendix:simulation_backward}.
%\alain{give ref to section}. 



\subsection{Approximating the backward characteristics}
\label{sec:backward_characteristics}

%\alain{adapt section 2 from paper with Marta to explain the generative model}
% In previous Sections, we have focused on the description
% of the forward noising process that we propose and its corresponding
% time-reversal. 

Similarly to common diffusion-based generative
models, we aim to sample from the time-reversal process.
% associated to the forward dynamics started from the data distribution.
However, exact simulations are not possible and face similar challenges:
a) we do not have access to i.i.d. samples from $\mu_{\Tf}$, b) the score function of the forward process defined in \eqref{score1} is intractable.

  %\alain{$\Tf$ for the time horizon, $\foX$ for the forward process, $\baX$ for the backward}
  % uwhile simulation of the forward process
%   starting from the
%   simulation of the We first interpret the ideas
%   behind the construction of DMPMs. Recall that
%   $\mustar \in \Pc(\msx )$ is the data distribution. The starting
%   point of DMPMs is to consider a homogeneous Markov process
%   $(\overrightarrow{X}_t)$ on $\ccint{0,\Tf}$, for a fixed time horizon
%   $T>0$ under a given generator. The second step of DMPMs is to
%   initialize this process at the data distribution $\mustar$, $i.e.$
%   setting $\overrightarrow{X}_0 \sim \mustar$, then consider its
%   corresponding marginal time densities $\mu_t$.

% \citet{conforti2022time} showed that $(\overrightarrow{X}_t)_{t\in \ccint{0,\Tf}}$ admits a time-reversal dynamic $(\overleftarrow{X}_t)_{t\in \ccint{0,\Tf}}$, which is also a Markov process, and satisfies that $\overrightarrow{X}_t=\overleftarrow{X}_{\Tf-t}$ for any $t\in \ccint{0,\Tf}$. The associated backward generator is given in \eqref{eq:generator_backward}. Accordingly, the last step of DMPMs involves following the backward evolution \eqref{eq:generator_backward} with $\overleftarrow{X}_0$ initialized at $\mu_T$, succeeding in a sample from the data distribution $\mustar$.
%In practice, three simulation challenges arise:

% \begin{enumerate}[leftmargin=*, label=\alph*)]
% \itemsep=0em
% % [wide, labelwidth=!, labelindent=0pt, label=\alph*)]
% \item We do not have access to i.i.d. samples from $\mu_{\Tf}$ \label{item:issue_1}
% \item The score function of the forward process defined in \eqref{score1} is intractable. \label{item:issue_2}
% \end{enumerate}

\textbf{Initialize the backward from the uniform distribution}.\ 
we show that $(\foX_t)_{t \in\ccint{0,\Tf}}$ converges geometrically to $\gamma^d$, the uniform distribution over $\msx$
(see \ref{invariant_measure} in  the supplementary document).
%\alain{if used before, introduce it also before}
%\alain{denote $s_t^{\ell}$ by $s_t^{\ell}$} 
This should be put in parallel with diffusion-based models, where the stochastic process at hand, e.g., Ornstein--Uhlenbeck, converges geometrically fast to some Gaussian distribution.

\textbf{Training procedure to fit the score function}.\
To address b), we exploit the conditional expecation structure of the score function, as given in \cref{prop:1}.
% \begin{proposition}\label{prop:1}
% The score function admits the following conditional expectation expression:
% {\small
% \begin{align}\label{eq:function_score_d}
%      s_{t}^\ell(x)=\E \l[  f^{\ell}_t(\overrightarrow{X}_0^\ell,  \overrightarrow{X}_{\Tf-t})| \overrightarrow{X}_{\Tf-t}=x\r]\eqsp,
% \end{align}
% }

% where  $t \in [0,\Tf)$, $x\in \msx$, $\ell = 1,\ldots,d$, $s_t^{\ell}$ is the $\ell$-th component of the score function $s_t$, and
% {\small 
% \begin{equation}\label{def:f}
% f^{\ell}_t(\overrightarrow{X}_0^\ell,  \overrightarrow{X}_{\Tf-t})   =  \dfrac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}-\dfrac{4(\overrightarrow{X}_{\Tf-t}^\ell-\overrightarrow{X}_0^\ell)^2}{1-\alpha_{\Tf-t}^2}\eqsp.
% \end{equation}
% }
% \end{proposition}
% The proof of this result is given in the supplementary material \ref{proof_prop:1}.
We approximate $(s_t){t \in\ccint{0,\Tf}}$ using a parameterized family ${(t,x) \mapsto s_t^{\theta}(x) }{\theta \in \Theta}$, where the parameter $\theta$ is fitted minimizing an adapted score-matching objective $\lossscore$, defined as the function
  \begin{equation}
    \label{eq:obje_modified_sm}
    {\small
     \theta \mapsto \int_0^{\Tf} \E \l[\| s^\theta_{\Tf-t}(\overrightarrow{X}_t) -f_{\Tf-t}(\overrightarrow{X}_0,\overrightarrow{X}_t) \|^2 \r]\rmd t\eqsp.
}
\end{equation}

Another option to fit $\theta$ is to use
the fact that for any $x \in \msx$, $t\in\ccint{0,\Tf},  \ell \in \iint{1}{d}$, 
$1-s_t^{\ell}(x)$ is non-negative. Thus, we introduce the following entropy-based term:
% the add to the loss $\loss_{\mrl^2}$ 
{\small
\begin{equation*}
     \theta \mapsto  \int_0^{\Tf} \expe{\sum_{\ell=1}^d(1-s_{\Tf-t}^{\theta,\ell})h\parenthese{\frac{1-s_{\Tf-t}^{\ell}}{1-s_{\Tf-t}^{\theta,\ell}}}(\foX_t)} \rmd t \eqsp,
  \end{equation*}}

where $h(a) = a \log(a) -(a-1)$. Minimizing this function is equivalent to minimizing:
  {\small
  \begin{align}
  \label{eq:1}
    \lossKL : &\theta \mapsto \int_0^{\Tf} \E\Bigg[
    \sum_{\ell=1}^d\Bigg(-s_{\Tf-t}^{\theta,\ell}(\foX_t)
    % \nonumber\\
    % &
    +(f_{\Tf-t}^{\ell}(\foX_t)-1)\log (1-s_{\Tf-t}^{\theta, \ell}(\foX_t) )\Bigg) \Bigg]\rmd t \eqsp.
\end{align}}

We further derive a discrete-denoiser structure.

\begin{proposition} 
\label{prop:discrete_denoiser}
    The score function admits the following discrete denoiser expression:
        {\small
    \begin{align}
         s_{t}^\ell(x)=\dfrac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}-\dfrac{4\alpha_{\Tf-t} d_{t}^\ell(x)}{1-\alpha_{\Tf-t}^2}\eqsp,
    \end{align}
    }
where $d_{t}^\ell(x) = \mathbb{P} (\fX_0^\ell \neq x^\ell \big| \fX_{\Tf - t} = x)$ serves as an optimal classifier, referred to as a \emph{discrete denoiser}.
\end{proposition}
The proof is given in \cref{app:discrete_denoiser}. We leverage this structure by reparameterizing our score model:
\begin{equation}
\label{eq:discrete_score_reparameterization}
     \small s_{t}^{\theta, \ell}(x)= \frac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}-\frac{4\alpha_{\Tf-t} d_t^{\theta, \ell}(x)}{1-\alpha_{\Tf-t}^2}
     \eqsp.
\end{equation}

As a result, we modify our objective $\lossscore$ to $\lossscoredenoiser$ to fit the conditional expectation $d_{t}(x)$ rather than $f_t$, see \eqref{eq:l2_loss_denoiser} in \cref{app:objective_functions}.
To fit $d_{t}^{\theta}(x)$ to $d_{t}(x)$, we introduce an additional cross-entropy loss $\lossCE$:
{ \small 
\begin{align*}
\lossCE : \theta & \mapsto \int_{0}^{\Tf} \mE \bigg[ \sum_{l=1}^d 
    \mathds{1}_{\fX_0^{\ell} \neq \fX_{\Tf - t}^{\ell}} \log d_t^{\theta, \ell}(\fX_{\Tf - t}^{\ell})
    % \\ 
    % &
     + (1 - \mathds{1}_{\fX_0^{\ell} \neq \fX_{\Tf - t}^{\ell}}) \log(1 - d_t^{\theta, \ell}(\fX_{\Tf - t}^{\ell}))
    \bigg]
    \rmd t \eqsp.
\end{align*}
}

Based on the previous discussions, we consider a linear combination of the losses $\lossscoredenoiser$, $\lossKL$, $\lossCE$, respectively weighted by factors $\varpi_1, \varpi_2, \varpi_3$, which results in the loss $\losslinear$: 
% (see \eqref{eq:linear_loss}).
\begin{equation} \label{eq:linear_loss}
\losslinear = \varpi_1 \lossscoredenoiser + \varpi_2 \lossKL + \varpi_3 \lossCE\eqsp.
\end{equation}
The expected value of $d_t^{\ell}$ is given by
\begin{equation}
w_t = \mE \l[ d_t^{\ell}(\fX_{\Tf - t}) \r] = (1 - \alpha_{\Tf - t})/2\eqsp,
\end{equation}
as detailed in \cref{app:objective_functions}. Thus, we scale losses $\lossscore, \lossCE$ by $1 / w_t$, ensuring a constant average magnitude across timesteps; see \eqref{eq:gamma_losses} and \Cref{fig:ce-l2-comparison} in \cref{app:objective_functions}. This leads to the updated loss $\losslinearw$ (see \eqref{eq:linear_loss_gamma}). 
Detailed derivations are provided in \cref{app:objective_functions}.
The final training procedure is outlined in \cref{alg:training}.

% we suggest to consider as loss
% $\theta \mapsto \loss_{\mrl^2}(\theta) +\zeta \loss_{\rme}(\theta)$, with $\zeta$ is the parameter that balances between these two losses.

% Full derivations are given in \Cref{app:objective_functions}. 


% in the supplement \ref{pseudo_code}
% \alain{add in the supplement for all pseudo-code}.
%\alain{fill}.
      
\subsection{Generative process}

Suppose we have access to a score approximation
%{\color{blue}$(s^{\theta^\star}_{t_k})_{k\in\{0,\ldots,M\}}$} with $t_0=0$ and $t_M=\Tf$.
$(s^{\theta^\star}_{t})_{t\in\ccint{0,\Tf}}$. 
The generative model can then be sampled analogously to the backward process, replacing 
replacing $\mu_{\Tf}$ with $\gamma^d$ and $(s_t){t\in\ccint{0,\Tf}}$ with $(s^{\theta^\star}{t})_{t\in\ccint{0,\Tf}}$, leading to
the non-homogeneous jump rate and kernel approximating \eqref{eq:def_char_back}: for $x,y \in\msx$ and $t\in\ccint{0,\Tf}$,
\begin{equation}
\label{eq:def_char_back_approx}
\begin{aligned}
    \lambda_t^{\theta^\star}(x)& = \lambda \sum_{\ell=1}^{d}(1- s_{t}^{\theta^\star,\ell}(x))\\
    k^{\theta^\star}_t(x,y) &=  \mathds{1}_{y=\varphi^{(\ell)}(x)} \cdot  \lambda(1- s_t^{\theta^\star,\ell}(x)) / \lambda_{t}^{\theta^\star}(x)
    % \begin{cases}
    %     \lambda(1- s_t^{\theta^\star,\ell}(x)) / \lambda_{t}^{\theta^\star}(x) & \text{ if } y=\varphi^{(\ell)}(x) \eqsp,\\
    %     0 & \text{ otherwise} \eqsp,
    % \end{cases} 
    \end{aligned}
\end{equation}
where we denote by $s_t^{\theta^\star,\ell}$ the $\ell$-th component of $s_t^{\theta^\star}$.
For completeness, \cref{alg:backward_approximation_continuous} in \cref{pseudo_code} provides the pseudo-code for an ideal, continuous-time approximation of the backward process.
%   Note that to sample the backward dynamic on a discrete-time scheme, we use the same sequence of step sizes as for the forward sampling.
   %\alain{fill and put into supplement}.

   %true approximationthen consists in sampling that 
%   Once we obtain
%   approximations for $\mu_T$ and the score function, the backward
%   continuous evolution can be simulated using diverse discretization
%   schemes. A typical choice is the stochastic Euler Exponential
%   Integrator (EI) \citep{durmus2015quantitative}. For a choice of
%   sequence of step sizes $\l\{h_k\r\}_{k=0}^N$, $N \ge 1$ associated
%   with the time discretization $t_k=\sum_{i=0}^k h_i$ such that
%   $t_N=T$, it defines a process
%   $(\overleftarrow{X}_t^{\theta^\star} )$ approximating
%   $(\overleftarrow{X}_t)$ for $t\in \ccint{0,\Tf}$ recursively on
%   intervals $[t_k,t_{k+1}]$ following the backward generative
%   \eqref{eq:generator_backward} involving the approximate score
%   function $s^{\theta^\star}_{t_k}(\overleftarrow{X}_{t_k} )$.

%Note that in our context, we have an explicit formula for the score function, therefore we can write it as a conditional expectation as in the following Proposition. 

%\begin{proposition}\label{prop:1}
%    The score function's components admit the following conditional expectation 
%    {\small
%    \begin{align*}
%         s_{t}^\ell(x)=\E \l[\dfrac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}-\dfrac{4(\overrightarrow{X}_{\Tf-t}^\ell-\overrightarrow{X}_0^\ell)^2}{1-\alpha_{\Tf-t}^2} | \overrightarrow{X}_{\Tf-t}=x\r]\eqsp,
%    \end{align*}
%    }
%    for $t \in \ccint{0,\Tf}$, $x\in \msx$ and $\ell = 1,\ldots,d$, with $\alpha_t:=e^{-2t}$.
%\end{proposition}
%The detailed proof of \Cref{prop:1} is given in the supplementary material \np{link to the proof}.

%This results follows that the score function is an $\mrl^2$-projection, meaning that the approximate score $s^{\theta^\star}_t=(s_t^{\theta^\star_\ell})_{\ell=1}^d$ can be obtained by minimizing the following loss through Neural Network 
%{\small
%\begin{align}\label{approscore}
  % \min_{\theta_\ell\in \Theta} \mathbb E \l[ \l| s_{t}^{\theta_\ell}(\overrightarrow{X}_{\Tf-t})-\dfrac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}+\dfrac{4(\overrightarrow{X}_{\Tf-t}^\ell-\overrightarrow{X}_0^\ell)^2}{1-\alpha_{\Tf-t}^2} \r|^2 \r]\eqsp,
%\end{align}
%}


%\section{Characteristics of the time-reversal process}

% \alain{justify that we can not really integrate the jump rate and therefore, we need to descretize the simualtion}

In practice, exact integration of the jump rate is infeasible, requiring time discretization in the backward simulation. We approximate the jump rate and kernel using piecewise constant functions $\hlambda, \hk$ \eqref{eq:def_char_back_approx}, such that, for $x,y \in \msx$ and $t \in [t_k,t_{k+1})$,
% \dario{why not just write $\hlambda_t^{\theta^\star}(x) = \lambda_{t_k}^{\theta^\star}(x)$ etc. ?}
\begin{equation}
\label{eq:def_char_back_approx_discrete}
\begin{aligned}
    \hlambda_t^{\theta^\star}(x)& = \lambda_{t_k}^{\theta^\star}(x)\\ % \lambda \sum_{\ell=1}^{d}(1- s_{t_k}^{\theta^\star,\ell}(x)) \\
    \hk_t^{\theta^\star}(x,y) &= k^{\theta^\star}_{t_k}(x,y)\eqsp, \\ %\begin{cases}
    %     \lambda(1- s_{t_k}^{\theta^\star,\ell}(x)) / \hlambda_{t_k}^{\theta^\star}(x) & \text{ if } y=\varphi^{(\ell)}(x) \eqsp,\\
    %     0 & \text{ otherwise} \eqsp,
    % \end{cases} 
\end{aligned}
\end{equation}
where  the discrete time scheme $\{t_k\}_{k=0}^K$ are associated with step-sizes $\{h_k\}_{k=1}^K$, $t_{k} = \sum_{i=1}^k h_i$ with $t_0= 0$ and $t_K= \Tf$.
Based on this jump rate and jump kernel, we can define a new CTMC which can be sampled in practice, following the same procedure as the ideal backward process and starting from an observation from $\gamma^d$. We denote the resulting process $(\baX^{\star}_t)_{t\in\ccint{0,\Tf}}$. Under some assumptions, the approximations of the backward process on both continuous and discrete time schemes 
% $(\baX^\star_{t_k})_{k\in \{0,\ldots,M\}}$ where $t_0=0$ and $t_K=\Tf$ 
have the final law converging toward the desired data distribution. Details are provided in the next section.

The resulting DMPM sampler used to approximate the backward dynamic is provided in \cref{alg:dmpm_sampler},  \cref{app:generative_process_sampling}. Time discretization strategies, referred to as time-schedules, are listed in \cref{tab:time_schedules}.

Using the index distribution $k_t^{\theta^*}$, we propose a methodological modification to the previous algorithm by flipping $M_{t_k}$ many bits instead of one at each timestep $t_k$. The sequence $\{M_{t_k}\}_{k=1}^K$ is referred to as a flip-schedule, with two natural choices listed in \cref{tab:M_schedules}. The resulting procedure is given in \cref{alg:dmpm_sampler_flip_schedule}, \cref{app:generative_process_sampling}.

Finally, leveraging the discrete denoiser structure from \cref{prop:discrete_denoiser}, we introduce a denoise-renoise sampler. This method alternates one-step denoising from $0 = t_0$ to $\Tf$, followed by re-noising back to $t_{1}$ etc., resembling multistep sampling in consistency models \citep{song2023consistencymodels}. The resulting procedure is given in \cref{alg:dmpm_dennoise_cycling}, \cref{app:generative_process_sampling}.

We refer the reader to \cref{app:generative_process_sampling} for more details.

% \cref{alg:backward_approximation_discrete}.

% \begin{algorithm}[t]
% \caption{Backward sampling of DMPM with piecewise-constant score (with inner loop $M$)}
% \label{alg:dmpm_sampler}
% \begin{algorithmic}[1]

% \REQUIRE 
%   Final time $\Tf>0$; rate $\lambda>0$; 
%   number of intervals $K$ with partition $0=t_0 < t_1<\cdots <t_K=\Tf$;
%   an approximate score function $s^{\theta^\star}$;
% %   discrete denoiser model $d^{\theta}$;
%   integer $M \ge 1$;
% \vspace{1ex}
% \STATE $\baX^\star_0 \sim \mathrm{Unif}(0,1)^{\otimes d}$ \COMMENT{initialize in $\{0,1\}^d$}
% \STATE $E \sim \mathcal{E}(1)$  %\COMMENT{exponential(1) random variable}
% \STATE $\Lambda \gets 0$   %\COMMENT{Rate integral}

% \FOR{$k = 0$ to $K-1$} 
%  \STATE $s^{\theta^\star}_{t_k} \gets s^{\theta^\star}_t(\baX_{t_k}^\star)$
% %   \STATE $s^{\theta}_{t_k} \gets \dfrac{2\,\alpha_{\Tf - t_k}}{\,1+\alpha_{\Tf - t_k}\,} 
% %          \;-\; \dfrac{4\,d_t^{\theta}\bigl(\iXtheta_{t_k}\bigr)}{\,1-\alpha_{\Tf - t_k}^2\,}$
%           %\COMMENT{Compute score from discrete denoiser model}
%   \STATE $\hlambda_{t_k} \gets \lambda \sum_{\ell=1}^d \bigl(1 - s^{\theta^\star, \ell}_{t_k}\bigr)$  %\COMMENT{Compute rate}
%   \STATE $\Delta t_{k} \gets t_{k+1}-t_k$ 
%   \STATE $\Lambda \gets \Lambda + \hlambda_{t_k}\,\Delta t_{k}$  %\COMMENT{Piecewise constant approximation of the rate integral}

%   \IF{ $\Lambda > E$ }
%     % \STATE  $\displaystyle [\,\ell_1^\star,\dots,\ell_M^\star\,]
%     %   \;\sim\; \mathrm{Multinomial} \ \!\Bigl(\Bigl\{\dfrac{\lambda \,\bigl(1 - s^{\theta^\star,\ell}_{t_k}\bigr)}{\hlambda_{t_k}}\Bigr\}_{\ell=1}^d,\ M\Bigr)$  %\COMMENT{Components to flip}
%     \FOR{$i=1$ to $M$}
    
%         \STATE $\ell_i \sim \categorial\l(\l\{\dfrac{\lambda \bigl(1 - s^{\theta^\star,\ell}_{t_k}\bigr)}{\hlambda_{t_k}}\r\}_{\ell=1}^d\r)$ 
%       \STATE $\iX_{t_k}^{\star, \ell_i} \gets 1 - \iX_{t_k}^{\star, \ell_i}$  %\COMMENT{bit flip}
      
%     \ENDFOR
%     \STATE $\Lambda \gets 0$  %\COMMENT{Reset integral}
%     \STATE $E \sim \mathcal{E}(1)$
%   \ENDIF
% \vspace{1ex}
% \STATE $\iX^\star_{t_{k+1}} \gets \iX^\star_{t_{k}}$ \COMMENT {Update state}

% \ENDFOR

% \textbf{Output:} $\iXtheta_{\Tf}$ \COMMENT{Final state}

% \end{algorithmic}
% \end{algorithm}


% . Provide the pseudo code here.
% \alain{fill and define $\hlambda$ and $\hk$ using $\hat{\cdot}$}

%The backward sampling on discrete time scheme $\{t_k\}_{k=0}^K$ associated with step-sizes $\{h_k\}_{k=1}^K$, $t_{k} = \sum_{i=1}^K h_i$ such that $t_0= 0$ and $t_K= \Tf$ is as follows. Starting from a sample $\overleftarrow{X}_0$ from $\gamma^d$ and a sequence of \iid~random variables distributed according to the exponential distribution with parameter $1$, $\{E_i \,: \, i \in \nset\ \}$, we can define the jump times $(T_i)_{i \in \nset}$
%$(T_i)_{i\in\{1,\ldots,N\}}|N\simiid \unif(\ccint{0,\Tf})$ 
%of the backward process and its transition by induction setting $T_0=0$. Given $(T_i,\baX_{T_i}^\star)$, we define the next jump time as {\color{red}$T_{i+1}  = \inf \{t_k > T_i, k =0,\ldots,K \, :\, \sum_{j=m}^{k} (t_{j+1}-t_j)\blambda_{t_j}(\baX^\star_{T_i}) \geq E_i \}$} with $m \in \l\{0,\ldots,K\r\}$ such that $T_i=t_m$
% $\Delta T_{i+1} = \inf \{t \geq 0\, :\, \int_0^t \blambda_{T_i+t}(\baX_{T_i}) \geq E_i \}$
%, where $\blambda_{t}(x) = \lambda \sum_{\ell=1}^{d}(1- s_{t}^{\theta^\star,\ell}(x))$. Then, with convention $\inf \emptyset = \infty$, set $\overleftarrow{X}^\star_{t_k} = \overleftarrow{X}^\star_{T_i}$ for $t_k\in\ooint{T_i,T_{i+1}\wedge \Tf}$, and finally if $T_{i+1} < \Tf$, $\overleftarrow{X}_{T_{i+1}}^{\star} = \varphi_{\ell_i}(\overleftarrow{X}_{T_i}^\star)$ for $\ell_i\in\{1,\ldots,d\}$ which is distributed according to $\categorial(\{  \lambda(1- s_{T_{i+1}}^{\theta^\star,\ell}(\baX^\star_{T_i})) / \blambda_{T_{i+1}}(\baX^\star_{T_i}) \}_{\ell=1}^d)$. For completeness, the pseudo-code associated with approximating the backward dynamic on the discrete time scheme is provided in \cref{alg:backward_approximation_discrete} in the supplement \ref{pseudo_code}.

% $\blambda_t$ and $k_t$ is defined as 
% $$\blambda_{t}(x) = \lambda \sum_{\ell=1}^{d}(1- s_{t}^{\ell}(x))$$

% $$k_t(x,\cdot) = \categorial(\{  \lambda(1- s_{t}^{\ell}(x)) / \blambda_{t}(x) \}_{\ell=1}^d)$$


% $\tlambda_t$ and $\tk_t$ is defined as 
% $$\tlambda_{t}(x) = \blambda_{t_k}(x) \eqsp,$$
% for $t \in \ccint{t_k,t_{k+1}}$ where $\{t_k\}_{k=0}^K$ are timestep associated with $\{h_k\}_{k=1}^K$, $t_{k} = \sum_{i=1}^K h_i$ such that $t_0= 0$ and $t_K= \Tf$.

% $$\tk_t(x,\cdot) = \categorial(\{  \lambda(1- s_{t_{k}}^{\ell}(x)) / \blambda_{t_k}(x) \}_{\ell=1}^d)$$

\section{Convergence of DMPMs algorithm}

This section provides quantitative error estimates between the generated final distribution $\mathrm{Law}(\baX^\star_{\Tf})$ and our data distribution $\mustar$ via the Kullback-Leibler divergence $\KL$. To this end, we consider the following assumptions on the parameterized score and the original data distribution:

%\alain{meditate about this loss and the one that we will be using in practice}
% \np{discuss: if discretize time: then the time scheme for forward and backward are the same?}
\begin{assumption}  \label{ass:approx_score}
There exists $\epsilon>0$ such that
    {\small
    \begin{align}\label{assumptiononscore}
       \max_{0\leq k\leq M} \E\l[\sum_{\ell= 1}^d (1-s_{\Tf-t_k}^{\theta^\star,\ell} )h\l(\dfrac{1-s^\ell_{\Tf-t_{k}}}{1-s_{\Tf-t_k}^{\theta^\star,\ell}}\r)(\foX_{{t_k}}) \r] \leq \epsilon \eqsp,
    \end{align}
    }
    with $h(a):=a\log (a)-(a-1)$ for $a>0$.
    \end{assumption}
    
    Note that \Cref{ass:approx_score} is induced by the entropic term $\loss_{\rme}$ defined in \eqref{eq:1} of the loss function we consider in practice. This condition naturally appears as we bound the $\KL$ divergence of the path probability measures corresponding to the approximate score $s^{\theta^\star}$ and the ideal one $s$ respectively. Indeed, we prove a Girsanov type theorem which provide an explicit expression of the density between these two measures in \cref{girsanovtheorem} in the supplement \ref{sec:girsanov}. 
    % \alain{fill}. 
    While standard Girsanov theorem for diffusion implies an $\mrl^2$-type approximation error condition for generative models (see e.g., \citet{conforti2024klconvergenceguaranteesscore, lee2023convergence, chen2022sampling})
%     % \alain{cite also convergence of score-based generative modeling for general data
% distribution, sampling is as easy as learning
% the score: theory for diffusion models with minimal data assumptions.})
, our result naturally involve the entropic-type condition \eqref{assumptiononscore} due to the discrete structure of our noising process.  
  
  
%   \alain{comment assumption and link with existing assumption for diffusion models}
  
  \begin{assumption}\label{ass:finite_fisher}
 The data distribution does not admit any zero-value, \ie, $\mustar(x) \in (0,1) $ for any $x\in \msx$.
 
 %has finite Fisher-like information
  %  \alain{instead the notation $\varphi^{(\ell)}(x)$ introduce the map $\varphi^{(\ell)}(x)$ which switch the $\ell$th* bit of $x$.}
  

  %  }
  
   \end{assumption}
   
\cref{ass:finite_fisher} follows that the data distribution has the finite Fisher-like information
{\small
 \begin{align}\label{eq:finite_fisher}
        \beta_{\gamma^d}(\mustar):=\E\l[\sum_{\ell=1}^d h\l(\rme^{g(\overrightarrow{X}_0)-g(\varphi^{(\ell)}(\overrightarrow{X}_{0}))}\r)\r]<+\infty \eqsp, 
   \end{align}
}
% \alain{comment assumption and link with existing assumption for diffusion models}
with $g:=-\log(\rmd \mustar/\rmd \gamma^d)$.

Note that \cref{ass:finite_fisher} is put in parallel with the finite relative Fisher information condition provided by \citet{conforti2024klconvergenceguaranteesscore}. However, \cref{ass:finite_fisher} is much simpler as the state space considered is finite, and the function $h$ is only infinite if $\mustar$ has not full support. 


We are now ready to state the error's bound of the generated data using DMPMs given in \cref{alg:dmpm_sampler}.
\begin{theorem}\label{theo:5}
    Under \Cref{ass:approx_score} and \Cref{ass:finite_fisher}, the following bound holds
    % \alain{use KL for H and $\mathrm{Law}$ for $\loss$}
    {\small
    \begin{align}\label{eq:bound_convergence}
      \KL(\mustar| \mathrm{Law}(\overleftarrow{X}_{\Tf}^\star ) ) \leq \rme^{-\Tf}\KL(\mustar|\gamma^d)+\tau \beta_{\gamma^d}(\mustar)+\epsilon\Tf \eqsp,
    \end{align}}
    with $\tau:=\max\{h_k, k= 1,\ldots,K\}$.
    % =\max_{0\leq k\leq M-1}(t_{k+1}-t_k)$.
\end{theorem}
\Cref{theo:5} is one of our distinguishing results, which guarantees the convergence of DMPMs algorithm, and makes it stronger than other algorithms built before for discrete target distribution. 

The term $\epsilon\Tf$ in \eqref{eq:bound_convergence} appears because the score function $s_t$ is replaced in the discretization by its approximation $s_t^{\theta^\star}$ satisfying \cref{ass:approx_score}. The term $\rme^{-\Tf}\KL(\mustar|\gamma^d)$ represents the initialization error, as our backward dynamic starts at $\gamma^d$ instead of $\mu_{\Tf}$. Finally, the term $\tau\beta_{\gamma^d}(\mustar)$ means that the data distribution $\mustar$ cannot be peculiar, in the sense that $\mustar$ does not admit any zero-value.  
The detailed proof of \cref{theo:5} is given in the supplementary material \ref{proof_theo:5}.
We deduce the following complexity result for DMPMs to achieve an $\varespilon >0$ discretization error.

\begin{corollary}
Consider fixed step-size $h_k = h$ for any $h>0$.
Assume \Cref{ass:approx_score} and \Cref{ass:finite_fisher} hold.  If for $\varespilon >0$, we set the step size and the number of iterations as
\begin{align*}
    h &\leq  \frac{\varespilon}{2\beta_{\gamma^d}(\mu^*)}\\
     \Kf &\geq \frac{\log(2\KL(\mustar|\gamma^d)/\varespilon)}{h}\eqsp,
\end{align*}
    then setting the horizon $\Tf = h \Kf$, it holds  $ \KL(\mustar| \mathrm{Law}(\overleftarrow{X}_{\Tf}^\star ) ) \leq  \varespilon+\epsilon\Tf$.
\end{corollary}


In our next result, we get rid of \Cref{ass:finite_fisher} using an early stopping strategy. 
% \begin{theorem}\label{theo:early_stopping}
%     Under \Cref{ass:approx_score}, for any $\eta >0$ the following bound holds
%     % \alain{use KL for H and $\mathrm{Law}$ for $\mathcal{L}$}
    
%     \begin{align}\label{eq:bound_convergence_early_stopping}
%       \KL(\mu_{\eta}| \mathrm{Law}(\overleftarrow{X}_{\Tf -\eta }^\star ) ) &\leq \rme^{-(\Tf-\eta)}\KL(\mustar|\gamma^d) \nonumber\\
%       &+\tau \beta_{\gamma^d}(\mu_{\eta})+\epsilon(\Tf-\eta) \eqsp,
%     \end{align}
%     with $\tau:=\max\{h_k, k= 1,\ldots,K\}$.
% \end{theorem}
% The proof of \cref{theo:early_stopping} is given in the supplement \ref{proof_theo:early_stopping}.

\begin{theorem}\label{theo:early_stopping}
    Under \Cref{ass:approx_score}, for any $\eta \in (0,\Tf)$, consider the discrete time scheme $\{\bt_m\}_{m=0}^M$ associated with step-sizes $\{\bh_m\}_{m=1}^M$, $\bt_m=\sum_{i=1}^m \bh_i$ such that $\bt_0=0$ and $\bt_M=\Tf-\eta$. Then, the following bound holds
    % \alain{use KL for H and $\mathrm{Law}$ for $\mathcal{L}$}
    
    \begin{align}\label{eq:bound_convergence_early_stopping}
      \KL(\mu_{\eta}| \mathrm{Law}(\overleftarrow{X}_{\Tf -\eta }^\star ) ) &\leq \rme^{-\Tf}\KL(\mustar|\gamma^d) 
    %   \nonumber\\
    %   &
      +\btau \beta_{\gamma^d}(\mu_{\eta})+\epsilon(\Tf-\eta) \eqsp,
    \end{align}
    with $\btau:=\max\{\bh_m, m= 1,\ldots,M\}$.
\end{theorem}
The proof of \cref{theo:early_stopping} is a consequence of \cref{theo:5}. Note that \eqref{marginaldist} yields that $\mu_\eta$ is always positive for any $\eta \in (0,\Tf)$, thus the Fisher-like information $\beta_{\gamma^d}(\mu_\eta)$ is always finite and \cref{ass:finite_fisher} holds. 



To obtain then a complexity bound for DMPMs on its discretization error without \Cref{ass:finite_fisher}, we bound in our next result, the total variation distance between $\mustar$ and $\mu_{\eta}$ for $\eta >0$. 

\begin{proposition}
    \label{prop:bound_tv_mustar_and_noise}
    For any $\eta \in (0,\Tf)$, we have
    \begin{equation}
        \tvnorm{\mu_{\eta}-\mustar} \leq  2-2\l(\frac{1}{2}+\frac{1}{2}\rme^{-2\lambda \eta}\r)^d  \leq  2-2(1-\lambda \eta)^d \eqsp.
        % \tvnorm{\mu_{\eta}-\mustar} \leq  1- \rme^{-\lambda \eta} \leq  \lambda \eta \eqsp.
    \end{equation}
\end{proposition}
The proof of \cref{prop:bound_tv_mustar_and_noise} is provided in the supplement \ref{proof_prop:bound_tv_mustar_and_noise}.
Combining \Cref{theo:early_stopping} and \Cref{prop:bound_tv_mustar_and_noise}, we deduce

\begin{corollary}\label{coro:early_stopping}
Consider fixed step-size $\bh_k = \bh$ for any $\bh>0$. Assume   \Cref{ass:approx_score} holds.  If for $\varespilon >0$ sufficiently small, we set the step size and the number of iterations as,
\begin{equation}\label{cond:early_stopping}
    \begin{aligned}
    % \Kf \geq \\
    \eta &\leq \frac{1-(1-\varespilon/2)^{1/d}}{\lambda}\\
    \bh &\leq  \frac{\varepsilon^2(\lambda\eta)^d}{2^{d+3} d (1+2\lambda\eta)^d} \\
    \bK_f &\geq \frac{\log(2\KL(\mustar|\gamma^d)/\varespilon^2)-\eta}{h}   \eqsp,
    \end{aligned}
\end{equation}
 then setting the horizon $\Tf - \eta = \bh \bK_f$, it holds 
    % {\small
    $$ \tvnorm{\mustar- \mathrm{Law}(\overleftarrow{X}_{\Tf-\eta}^\star ) } \leq 2\varespilon+ \sqrt{2\epsilon\Tf} \eqsp.$$
\end{corollary}
The detailed proof of \cref{coro:early_stopping} is given in \cref{proof_coro:early_stopping}.



%The proof of \Cref{theo:5} requires understanding the time-reversal evolution as an optimal control problem via the martingale problem. This point of view gives us a better perception of the backward dynamic and is a powerful tool for showing convergence.

%We now provide a framework of the canonical process point of view. Let $\Omega = \D(\ccint{0,\Tf};\msx)$ be the space of all càdlàg (right continuous and left limited) paths from $\ccint{0,\Tf}$ to $\msx$. With abuse of notation, we can express our forward Markov process $(\overrightarrow{X}_t)_{t\in \ccint{0,\Tf}}$ as the canonical process as follows
%\begin{align*}
%    \overrightarrow{X}_t(\omega) = \omega_t, \quad \text{ for }t\in \ccint{0,\Tf},(\omega_s)_{s\in \ccint{0,\Tf}}\in \Omega.
%\end{align*}
%Denote by $\Delta \overrightarrow{X}_t = \overrightarrow{X}_t - \overrightarrow{X}_{t-}$ the size of jump at time $t$. Consider one-hot vectors $\rme_i$ for $i=1,\ldots,d$, and denote by $\Qc_d$ the set of all effective jumps, $i.e.$
%$$\Qc_d=\l\{(-1)^p\rme_i \quad\text{for } i=1,\ldots,d \text{ and } p=0,1 \r\}.$$
%Denote $|q|:=(|q^j|)^d_{j=1}$ for $ q\in \Qc_d$.
%Endow $\Omega$ with the $\sigma$-field $\sigma(X_t;t\in\ccint{0,\Tf})$ generated by the canonical projections and consider the associated canonical filtration. 
% We denote the set 
%$$\Ac:=\l\{(x,q)\in \msx \times \Qc_d \text{ s.t. } x+q \in \msx \r\},$$œ
 %(|q|^Tx,|q|^Tq) \in \l\{ (0,1),(1,-1) \r\}\r\}~,$$ 
% and the rate of jump $\varepsilon_t$ by
%\begin{align*}
%    \varepsilon_t(\foX_{t-},\foX_{t-}+q):&=\begin{cases}
%        1, \quad &\text{if } (\foX_{t-},q )\in \Ac,\\
%        0, \quad &\hspace{1cm}\text{otherwise. }
%    \end{cases}
%\end{align*}



\section{Existing works on diffusion-based generative models for discrete data}

We provide details of existing approaches in discrete generative models. For further details see \cref{app:related_works}.

A variety of diffusion-based techniques have been adapted for discrete data, often by embedding categorical variables into continuous spaces \citep{dieleman2022continuous,chen2022analog,richemond2022categorical}. Although this preserves many strengths of continuous diffusion, it can require heavy training regimes and lacks certain theoretical guarantees. Other methods, such as Argmax Flows and Multinomial Diffusion \citep{hoogeboom2021argmax}, use categorical noise models or argmax transformations to handle discrete tokens, but can impose considerable computational overhead.

Recently, continuous-time Markov chains have become central to modeling discrete diffusion. \citet{campbell2022continuous} pioneered a CTMC-based approach, on top of which \citet{gat2024discrete} adapted flow matching to the discrete setting, 
with correction steps for further performance gains at the expense of sampling cost, while \citet{holderrieth2024generator} proposed a more general generator-matching framework that adapts to arbitrary Markov processes at the cost of potential complexity. Masked diffusion models \citep{austin2021structured,shi2024simplified} and stochastic integral formulations \citep{ren2024discrete} further exemplify modern attempts at balancing tractability, performance, and theoretical underpinnings. However, many of these methods still lack rigorous error bounds or scale poorly in high dimensions.

Our proposed method takes a step toward bridging these gaps, yielding provable error guarantees and reduced computational burdens, expressing the score function as a conditional expectation and, .e.g., avoiding the costly signal-to-noise ratio training used in \citet{shi2024simplified}.


% We provide details of the recent research on discrete generative models.

% \textbf{Embedding Discrete Structure in the Continuous Space.}
% Several methods map categorical data into continuous domains, preserving much of the flexibility of classical diffusion. This includes representing discrete tokens in either Euclidean space \citep{dieleman2022continuous,chen2022analog} or the probability simplex \citep{richemond2022categorical}. While these approaches can handle arbitrary infilling and flexible sampling, they often incur high training costs and lack solid convergence guarantees.

% \textbf{Argmax Flows and Multinomial Diffusion.}
% \citet{hoogeboom2021argmax} adapt continuous diffusion frameworks to discrete tokens either through Argmax Flows, which rely on a probabilistic inverse of the argmax function, or Multinomial Diffusion, which models noise addition via categorical distributions. Both can address diverse discrete data but come with increased computational overhead and no strong theoretical bounds.

% \textbf{Designing Flow Processes over the Discrete State Space.}
% Recent work leverages continuous-time Markov chains for discrete diffusion \citep{campbell2022continuous}, providing a time-reversal perspective but often requiring complicated correction steps \citep{gat2024discrete} or density-ratio estimations. Similarly, \citet{holderrieth2024generator} introduce \emph{generator matching} for general Markov processes but risk high complexity in large state spaces.

% \textbf{Masked Diffusion Models.}
% Structured or masked diffusion has seen renewed interest \citep{austin2021structured,shi2024simplified}, reducing objectives to signal-to-noise ratios in discrete settings. However, many of these techniques still lack solid theoretical assurances and remain expensive to train.

% \textbf{Discrete Diffusion via Stochastic Integrals.}
% Another line of work interprets discrete diffusion with Poisson random measures \citep{ren2024discrete}, establishing the first KL divergence error bounds under strong assumptions.


% In contrast, our method directly defines and exploits a discrete forward process that admits a tractable score function and yields strong theoretical convergence guarantees. Full details on these related approaches are provided in Appendix~\ref{app:related_works}.

\section{Experiments}
% We evaluate DMPM's performance across various design choices on two discrete datasets. For comprehensive implementation details, we refer readers to the  on \emph{sawtooth} dataset (\Cref{app:experiment_small}) and binarized MNIST (\Cref{app:experiment_image}).

The full experimental details are available in \cref{app:experiment}. We evaluate our Discrete Markov Path Model (DMPM) on two datasets. The first is a low-dimensional synthetic \emph{sawtooth} dataset, with dimension $4 \leq d \leq 16$. The second is binarized MNIST, with $d = 32 \times 32$. 
We explore various design choices, and compare DMPM against MD4 (masked diffusion) \citep{shi2024simplified} and DFM (discrete flow matching) \citep{gat2024discrete}, two state-of-the-art discrete generative approaches.



% We explore the behaviour of our models on two datasets defined over $\{0, 1\}^d$. The first is a small dimensional dataset we call \emph{sawtooth}, with $4 \leq d \leq 16$. See \Cref{app:experiment_small} for details about the data distribution, neural network and training procedure. The second is \emph{binarized MNIST}, where $d = 32 \times 32$. See  \Cref{app:experiment_image} for similar details in this higher dimensional dataset.

% \begin{itemize}[leftmargin=1em]
% \itemsep0em 
%     \item \emph{Loss configuration} (\Cref{sec:backward_characteristics}): We use $\losslinear$, a linear combination of the $\lossscore$, $\lossKL$, and $\lossCE$ losses, optionally scaled by a factor $\gamma$ to balance magnitudes across timesteps, yielding $\losslinearw$.
%     \item \emph{Time horizon and time-schedule}: We vary $\Tf$ (the end of the forward process) and consider different spacings for the time grid: uniform, quadratic, cosine (see Table~\ref{tab:time_schedules})
%     \item \emph{DMPM sampler and flip-schedule}: At each reverse step, we can flip a variable number of bits $M_{t_k}$, instead of just one, guided by the learned score model (see Table~\ref{tab:M_schedules} and \cref{alg:dmpm_sampler_flip_schedule}).
%     \item \emph{Denoise-renoise sampler}: We exploit our discrete-denoiser structure (see \eqref{eq:discrete_score_reparameterization}) to alternate denoising from $t_N$ to $0$ and then re-noising back up to $t_{N-1}$, and so on. See \cref{alg:dmpm_dennoise_cycling}.
% \end{itemize}

% \emph{Loss configuration} (\Cref{sec:backward_characteristics}): We use $\losslinear$ \eqref{eq:linear_loss}, a linear combination of the $\lossscore$, $\lossKL$, and $\lossCE$ losses, optionally scaled by a factor $w$ to balance magnitudes across timesteps, yielding the loss $\losslinearw$  \eqref{eq:linear_loss_gamma}.

% \emph{Time horizon and time-schedule}: We vary $\Tf$ (the end of the forward process) and consider different spacings for the time grid: uniform, quadratic, cosine (see Table~\ref{tab:time_schedules})

% \emph{DMPM sampler and flip-schedule}: At each reverse step, we can flip $M_{t_k}$ many bits instead of one, based on the index distribution derived from the score model (see Table~\ref{tab:M_schedules} and \cref{alg:dmpm_sampler_flip_schedule}). The sequence $\{M_{t_k}\}_{k=1}^K$ is called the flip-schedule.

% \emph{Denoise-renoise sampler}: We exploit our discrete-denoiser structure (see \eqref{eq:discrete_score_reparameterization}) to alternate denoising from $0$ to $\Tf$ and then re-noising back up to $t_{1}$, and so on. See \cref{alg:dmpm_dennoise_cycling}.










% \paragraph{Configuration for the loss function} As mentioned in \Cref{sec:backward_characteristics}, we work with a loss $\loss_{\varpi_1, \varpi_2, \varpi_3}$ consisting in a linear combination of the losses $\lossscore, \lossKL, \lossCE$, with the discrete denoiser reparameterization as given in \eqref{eq:discrete_score_reparameterization}. Furthermore, we consider its variation $\loss_{\varpi_1, \varpi_2, \varpi_3}^{w}$ where the loss terms are scaled to be of similar magnitude across timesteps.

% \paragraph{Time horizon and time-schedule}
% For the sampling process, we study various time horizons $\Tf$ and time-schedules used for time-discretization: we introduce a uniform, quadratic and a cosine spacing, as given in Table~\ref{tab:time_schedules}.

% \paragraph{DMPM sampler and flip-schedule}
% For our default DMPM sampler, we introduce a slight variation enabling vast computational speedups for similar performance. Leveraging the specific structure of our backward process, we flip $M_{t_k}$ bits at each reverse step $t_k$, instead of just one, according to the distribution on indices derived from the score model. We call the sequence $(M_t)$ the \emph{flip-schedule}, and we explore a constant and a linear schedule, as defined in Table~\ref{tab:M_schedules}. We give the associated procedure in \cref{alg:dmpm_sampler_flip_schedule}. 

% \paragraph{Denoise-renoise sampler}
% We explore an alternative sampler, which relies on the discrete-denoiser structure of our process. Given a time-schedule $0 = t_0 < t_1 < \cdots < t_N = \Tf$, the algorithm consists in denoising a noise sample from $t_N$ to $0$ in a single step, renoising it to $t_{N-1}$ with the transition kernel of the forward process, denoising it to $0$ again, renoising it to $t_{N-2}$ etc.
% We give the associated procedure in \cref{alg:dmpm_dennoise_cycling}. 

% \paragraph{Comparison with state of the art}
% Finally, we compare DMPM against two state of the art generative methods : MD4 (Masked Diffusion for Discrete Data) as developped in \citet{shi2024simplified}, and DFM (Discrete Flow Matching), as developped in \citet{gat2024discrete}. We are able to match their performance on MNIST with 5x less network calls. 

 

\subsection{Experiments on Small-Dimensional Bernoulli Data}

We study a discrete data distribution $p$ such that each component of $X = (X_i)_{i=1}^d \sim p$ is independently distributed as Bernoulli$(p_i)$. The map $i \mapsto p_i$ forms a sawtooth pattern (see \Cref{fig:sawtooth_pattern}). We evaluate performance using a custom Sliced Wasserstein Distance ($\text{SWD}$) between the learned and true distributions (see \Cref{app:experiment_metric}). Indeed, the state space size $2^d$ can get too big for traditional histogram-based metrics like $\KL$ divergence or Hellinger distance. 

\paragraph{Time horizon and time-schedule}

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\columnwidth]{mnist_experiment_results_img/schedule_Tf_comparison-final-arxiv.png}
\caption{Comparison of time-schedules (\textit{cosine}, \textit{linear}, \textit{quadratic}) and time horizon ($\Tf=3$ vs.\ $\Tf=10$). 
% DMPM obtains its best performance with the cosine schedule and $\Tf=3$.
}
\label{fig:tf-schedule-comparison}
\end{figure}

We vary the time horizon $\Tf$ and consider different time-schedules: uniform, quadratic, cosine (see Table~\ref{tab:time_schedules}) and investigate impact on performance. In \Cref{fig:tf-schedule-comparison}, we show results for a model trained with the simplest $\lossscore$ loss with $d=16$, and evaluated with various reverse steps.
The cosine schedule with $\Tf=3$ achieves optimal SWD values, outperforming linear/quadratic schedules and longer horizons, and uses less reverse steps. This suggests $\Tf = 3$ sufficiently approaches the uniform distribution during forward diffusion, avoiding excessive uniform-state transitions. In light of these observations, we fix this choice in subsequent experiments.

% Moreover, a \textbf{shorter time horizon} ($\Tf=3$) outperforms a longer horizon ($\Tf=10$) in all practical reverse-step ranges. Intuitively, $\Tf=3$ already allows the forward diffusion to approach the uniform distribution sufficiently, and longer time horizons spend too much time in uniform-like states.

% Interestingly, we observe a \emph{U-shaped} behavior in performance with respect to the number of reverse steps: when the number of steps becomes too large, our method exhibits \emph{mode collapse}. {\red{We provide further evidence and discussion of this phenomenon in the Appendix, where we show histograms of the generated samples.}}

% \dario{remove mode collapse mention}


% \paragraph{Choosing the loss function}
% \dario{Drop this, we will discuss this in MNIST}

% Next, we compare various loss configurations of DMPM. We consider ten variants based on various points on the 3-simplex to assign weights $\varpi_1, \varpi_2, \varpi_3$, and whether or not we divide by $w_t$. Figures~\ref{fig:loss-barplot} and \ref{fig:loss-vs-steps} illustrate the results for $d=16$, a cosine schedule, and $\Tf=3$.

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.95\columnwidth]{experiment_results_img/loss_barplot_dim16_stepsbest_schedulecosine_Tf3.png}
% \caption{Impact of different loss configurations on $\text{SWD}$. Including KL loss ($\zeta>0$) substantially degrades performance, unless we use the $w$ trick.}
% \label{fig:loss-barplot}
% \end{figure}

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.95\columnwidth]{experiment_results_img/loss_vs_steps_dim16_schedulecosine_Tf3.png}
% \caption{Loss configurations vs.\ number of reverse steps, with $d=16$, cosine schedule, $\Tf=3$. When $w=\text{True}$, the method becomes slightly more robust to the amount of reverse steps.}
% \label{fig:loss-vs-steps}
% \end{figure}

% From Figure~\ref{fig:loss-barplot}, we see that adding the  \textbf{KL loss} ($\zeta \neq 0$) tends to harm performance across the board. \textbf{Enabling $w$} helps especially in configurations with nonzero $\zeta$, but its advantage remains modest in the pure cross-entropy (CE) or L2 settings.

% Figure~\ref{fig:loss-vs-steps} shows that, when $w$ is enabled, the performance is more stable for varying reverse steps. Overall, simple \textbf{CE or L2 losses} work best in terms of both absolute results, and divding by  $w$ helps with robustness across different numbers of reverse steps.

\paragraph{Comparison with state of the art methods}

We compare DMPM (cosine schedule, $\Tf=3$, loss $\lossscore$) against MD4 and DFM. 
\Cref{fig:method-vs-data-dim} reports $\text{SWD}$ with varying data dimension $d$. We find that DMPM outperforms both baselines, with significantly fewer reverse steps required for optimal performance (typically $30$ vs $100$).

% We compare DMPM against two other generative models, \textit{MD4} and \textit{DFM}. For DMPM we use the cosine time-schedule and $\Tf=3$, as based on the previous study, and we use the model trained with the default $\ell_2$ loss $\loss_{1, 0, 0}$. We set the data dimension to $d = 16$, but we have observed the same results for $4 \leq d \leq 64$.
% Figure~\ref{fig:method-vs-data-dim} displays $\text{SWD}$ for each method as the data dimension varies. We observe that DMPM outperforms the other two methods for every tested data dimension. Moreover, DMPM achieves superior $\text{SWD}$ performance with much fewer reverse steps. 
% % In our example with the data dimension being equal to $16$, 
% DMPM achieves better performance with $30$ reverse steps than MD4 and DFM at $100$ steps, in terms of $\text{SWD}$. Together, these results confirm that DMPM is both sample-efficient and accurate for small-dimensional discrete data.

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.95\columnwidth]{experiment_results_img/method_vs_data_dim_stepsbest.png}%
% \caption{Comparison of DMPM, md4, and dfm across different data dimensions. Our DMPM consistently achieves the lowest $\text{SWD}$ values.}
% \label{fig:method-vs-data-dim}
% \end{figure}

\begin{figure}[t]
\centering
\scalebox{0.85}{
\begin{tabular}{lcccc}
\toprule
$d$ & 4 & 8 & 12 & 16 \\ %& optimal \#steps\\
\midrule
\textbf{DFM} & 6.102 & 8.864 & 5.019  & 8.302 \\ % & 100 \\
\textbf{MD4} & 9.376 & 7.670 & 4.045 & 8.037 \\ % & 100 \\
\textbf{DMPM} & \textbf{3.174} & \textbf{3.308} & \textbf{2.342} & \textbf{2.515} \\ % & 30 \\
\bottomrule
\end{tabular}
}
\caption{$\text{SWD} \downarrow$, in 1e-3, for DMPM, MD4, and DFM across data dimension $d$. Selected the best result with \#steps $2\leq K \leq 200$ for each method.
% for their respective optimal number of reverse steps $2\leq N \leq 200$
}
\label{fig:method-vs-data-dim}
\end{figure}


% \begin{figure}[t]
% \centering
% \includegraphics[width=0.8\columnwidth]{experiment_results_img/method_vs_steps_data_dim16.png}%
% \caption{Comparison of DMPM, MD4, and DFM with respect to reverse steps, $d=16$. DMPM attains optimal performance with much fewer reverse steps.}
% \label{fig:method-vs-steps}
% \end{figure}

% \dario{modify plot so that mode collapse is less visible}

% \begin{figure}[t]
% \centering
% \begin{tabular}{lcccccccccc}
% \toprule
% method & 2 & 4 & 8 & 12 & 16 & 20 & 30 & 50 & 100 & 200 \\
% \midrule
% dfm & 0.365769 & 0.181084 & 0.086625 & 0.065551 & 0.047764 & 0.037257 & 0.026923 & 0.018821 & 0.014380 & 0.008302 \\
% dmpm & 0.048270 & 0.038552 & 0.024122 & 0.016364 & 0.008839 & 0.005034 & 0.002515 & 0.005577 & 0.008420 & 0.010795 \\
% md4 & 0.353976 & 0.193211 & 0.102337 & 0.070179 & 0.055927 & 0.042022 & 0.031060 & 0.018857 & 0.014195 & 0.008037 \\
% \bottomrule
% \end{tabular}
% \caption{Comparison of DMPM, MD4, and DFM with respect to reverse steps ($d=16$). Our model attains its best performance with much fewer reverse steps.}
% \label{fig:method-vs-steps}
% \end{figure}









% \begin{table}[H]
% \centering
% \caption{Hellinger distance with $d=8$, for different $(\Tf, \lambda)$ and loss-term configurations $(\mu,\zeta,\eta)$. Each row corresponds to a distinct pair $(\Tf,\lambda)$, and columns denote different loss-weight vectors. All values are shown with 3--4 digits. Lower is better.}
% \label{tab:tf-lambda-results}
% \scalebox{0.7}{
% \begin{tabular}{llrrrr}
% \toprule
% \textbf{Row} & $(\Tf,\lambda)$ & $(0.0,\,0.0,\,1.0)$ & $(0.33,\,0.33,\,0.33)$ & $(0.5,\,0.0,\,0.5)$ & $(1.0,\,0.0,\,0.0)$ \\
% \midrule
% 0 & $(3.0,\,1.0)$  & 0.391 & 0.392 & 0.388 & 0.392 \\
% 1 & $(3.0,\,5.0)$  & 0.497 & 0.507 & 0.481 & 0.487 \\
% 2 & $(10.0,\,1.0)$ & 0.450 & 0.459 & 0.437 & 0.447 \\
% 3 & $(10.0,\,5.0)$ & 0.580 & 0.620 & 0.578 & 0.575 \\
% \bottomrule
% \end{tabular}
% }
% \end{table}


% \begin{table}[H]
% \centering
% \caption{Hellinger distance as a function of the data dimension $d$, for several $(\mu,\zeta,\eta)$. All other hyperparameters held fixed at $\Tf=3$, $\lambda=1$.}
% \label{tab:dimension-results}
% \scalebox{0.65}{
% \begin{tabular}{lrrrrr}
% \toprule
% \textbf{Row} & $d$ & $(0.0,\,0.0,\,1.0)$ & $(0.33,\,0.33,\,0.33)$ & $(0.5,\,0.0,\,0.5)$ & $(1.0,\,0.0,\,0.0)$ \\
% \midrule
% 0 & 4  & 0.0657 & 0.0433 & 0.0383 & 0.0341 \\
% 1 & 8  & 0.0863 & 0.0847 & 0.1008 & 0.0745 \\
% 2 & 12 & 0.1870 & 0.1653 & 0.1788 & 0.1852 \\
% 3 & 16 & 0.3908 & 0.3917 & 0.3880 & 0.3923 \\
% \bottomrule
% \end{tabular}
% }
% \end{table}

% From Table~\ref{tab:tf-lambda-results}, focusing on $d=8$, we observe that smaller time horizons ($\Tf=3$) outperform larger horizons ($\Tf=10$) consistently, and similarly $\lambda=1$ yields better results than $\lambda=5$. Table~\ref{tab:dimension-results} indicates that, as $d$ grows, the model becomes less precise, but all loss configurations $(\mu,\zeta,\eta)$ remain close in performance for these smaller tasks. In essence, the differences among these loss-weight vectors are minimal when $d$ is not too large.





\subsection{Experiments on Higher-Dimensional Binary MNIST}

\paragraph{DMPM sampler and flip-schedule}

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.95\columnwidth]{mnist_experiment_results_img/lines_mu_zeta_eta_gamma_vs_ema_0.99_fid_by_reverse_steps.png}
% \caption{Default algorithm with various loss configurations, showing a U-shaped curve as the number of reverse steps increases. Excessively few steps yield noisy images, whereas too many cause mode collapse.}
% \label{fig:exp-fig1}
% \end{figure}

% \dario{modify discussion to remove mode collpase discussion, focus on sample efficiency with the M-schedule. Don't mention $B$ so reviewer are not tempted to ask us to vary it.}

% We first study how the default algorithm performs as we vary the number of reverse steps, focusing on potential mode collapse.
% Figure~\ref{fig:exp-fig1} plots the performance (FID) for different loss configurations against the number of reverse steps.
% In each configuration, we observe a characteristic U-shaped curve.
% To explain this phenomena, 

% As a reminder, at each reverse step $t_k$, we can flip $M_{t_k}$ bits, instead of just one, based on the distribution on indices derived from the learned score model. 
We investigate our DMPM sampler with a constant and linear flip-schedule $\{M_{t_k}\}_{k=1}^K$. In practice, we observe optimal performance when we set $\sum_{k=1}^K M_{t_k} = 1000$, thus encouraging as many total bit flips as the data dimension $d$. We dimension each flip-schedule accordingly.
% In Figure~\ref{fig:exp-fig2}, we find an explanation for the U-shaped curve previously observed. Ee use a \emph{constant} schedule, setting $M_t = M$ for all $t$ and varying the number of reverse steps $N$ such that $N \times M \approx B$. For a fixed $B$, the performance remains relatively steady across $N$. Nonetheless, if $B$ is too small ($B=500 < d$), the sampler is unable to flip enough bits, producing noisy samples; if $B$ is too large ($B=2000 > d$), excessive flipping collapses the process toward a single mode, for instance repeatedly generating the digit '1'. We refer the reader to {\red{Appendix}} for visual illustration of this phenomena.
Figure~\ref{fig:exp-fig3} illustrate the performance of our two schedule strategies. The performance is stable across the exact choice of reverse steps $K$, as long as total bit flips $\sum_{k=1}^K M_{t_k} = 1000$ stays constant. This facilitates large speedups by reducing reverse steps, thus network calls, for similar performance. Using a model trained with the loss $\loss_{1/3, 1/3, 1/3}^{w}$, and using the linear flip-schedule, we achieve an optimal FID of $4.77$ with only $25$ network calls. The linear flip-schedule consistently outperforms the constant one, as fewer bits flipped in the early phases help the model converge to more sensible samples, working similarly to the cosine masking schedule introduced in MD4 \citep{shi2024simplified}.

% In order to improve the performance of the default sampler, we are led to adopt a \emph{linear} schedule, in which $M_t$ scales proportionally to $\pi\,B\, t/(2\,\Tf)$, thus reducing bit flips during the early phases of generation. As illustrated in Figure~\ref{fig:exp-fig3}, this carefully increasing flip-schedule yields superior performance and robustness.

% To mitigate mode collapse while retaining sufficient sample quality, we introduce a mechanism that leverages the reverse process structure. At each reverse step  $t_k$ , our method flips bits according to a multinomial distribution derived from the learned score model. Let $M_{t_k}$ be the time-dependent number of bits to flip. In practice, we choose an overall budget $B$ of bit flips, distributing it over the reverse steps in the flip-schedule. Figures~\ref{fig:exp-fig2} and \ref{fig:exp-fig3} illustrate two scheduling strategies. In Figure~\ref{fig:exp-fig2}, we vary $N$ while ensuring that $N\times M \approx B$, with a simple constant schedule ($M_t = M$). We observe that for a fixed $B$, the performance remains fairly steady across different $N$, where $B=500 < d$ generates noisy data since not enough bits can be flipped, and $B=2000 > d$ leads to mode collapse as too many bit flips during generation collapses the process towards a single data mode, as illustrated in {\red{Appendix}}


% \begin{figure}[t]
% \centering
% \scalebox{0.8}{
% \begin{tabular}{lccccc}
% \toprule
% $N$ & 10 & 25 & 50 & 100 \\
% \midrule
% $B=500$ & 131.48 & 108.05 & 102.13 & 98.57 \\
% $B=1000$  & 20.54 & 16.77 & 12.37 & 12.25 \\
% $B=2000$  & 68.72 & 68.35 & 55.43 & 43.07 \\
% \bottomrule
% \end{tabular}
% }
% \caption{Constant flip-schedule, ensuring a constant number of bit flips $B = N \times M_B$ for each choice of $N$ reverse steps, i.e., network calls. The FID remains stable for fixed $B$.}
% \label{fig:exp-fig2}
% \end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{mnist_experiment_results_img/lines_M_schedule_loss_vs_ema_0.99_fid_by_reverse_steps.png}
\caption{FID$\downarrow$ on MNIST, linear vs.\ constant flip-schedules scaled for $1000$ total bit flips, with various loss configurations.}
\label{fig:exp-fig3}
\end{figure}

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.95\columnwidth]{mnist_experiment_results_img/lines_total_steps_vs_ema_0.99_fid_by_reverse_steps.png}
% \caption{Constant-M scheduling. Each curve corresponds to a different total number of reverse steps, i.e., network calls, but we ensure $\sum_t M_{t} = B$. The FID remains stable across $N$.}
% \label{fig:exp-fig2}
% \end{figure}


\paragraph{Configuration for the loss function}

Through extensive experimentation, the balanced loss $\loss_{1,\,1,\,1}^{w}$ yields better performance. The scale factor $w$ notably helps to balance magnitudes at each timestep, and improves the synergy of the $\ell_2$, cross-entropy, and KL components. We refer the reader to \cref{fig:gamma_no_gamma} for an appropriate illustration of these comments. It should be noted that the simplest losses $\lossscore, \lossscore^w$ already yield excellent, close to optimal results. 


% Again, referring to \cref{fig:exp-fig3}, we can see that the balanced loss $\loss_{1/3,\,1/3,\,1/3}^{w}$ achieves the strongest overall FID values, across the different choices of reverse steps. This is an observation consistent with all the experiments we have done and did not include in this paper. Importantly, scaling the loss with the $w$ factor was crucial in improving the synergies between the different losses, and we refer the reader to {\red{Appendix}} for an appropriate illustration of this phenomena. 

% \dario{Already here we can discuss and and select the best loss configuration. Choose the balanced 1/3 loss, and redirect to the barplot in the appendix, which will compare losses with and without gamma; or put the corresponding table here. Redirect to a graph in appendix showing that these takeaways are also applicable in the case of the denoise-renoise sampler.}

% \paragraph{Denoise-renoise sampler}

% We next compare our two distinct sampling procedures: the default DMPM sampler and the denoise-renoise sampler, with 

% Figure~\ref{fig:exp-fig4} contrasts these methods under a target of 800 total diffusion steps, and sorts them by different numbers of reverse steps on the x-axis. 

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.8\columnwidth]{mnist_experiment_results_img/lines_loss_sampling_vs_ema_0.99_fid_by_reverse_steps.png}
% \caption{Comparison of sampling algorithms (\textit{default} vs.\ \textit{denoise\_renoise}) and various losses, all with $w$-division. The \textit{denoise\_renoise} sampler ultimately outperforms in higher reverse-step regimes, especially with the balanced $\loss_{1/3, 1/3, 1/3}^{w}$ loss.}
% \label{fig:exp-fig4}
% \end{figure}


% \begin{figure}[t]
% \centering
% \includegraphics[width=0.8\columnwidth]{mnist_experiment_results_img/bar_mu_zeta_eta_gamma_vs_ema_0.99_fid_pairedTrue.png}
% \caption{Comparison loss configurations for the denoise renoise sampler.}
% \label{fig:exp-fig4bis}
% \end{figure}

\paragraph{Denoise-renoise sampler and comparison with state-of-the-art.}

We further exploit the discrete-denoiser structure 
% by alternating single-step denoising from $t_N$ to $0$ and single-step re-noising from $0$ back to $t_{N-1}$. We repeat this until we reach $t_0$. 
with our denoise-renoise sampler, as given in \cref{alg:dmpm_dennoise_cycling}, \cref{app:objective_functions}. This approach tends to leverage the model’s learned transitions effectively, leading to further improvements in sample quality. We compare our DMPM model, trained with the balanced $\loss_{1, 1, 1}^w$ loss, with the denoise-renoise sampler and the default sampler with a linear flip schedule, to MD4 (masked diffusion) and DFM (discrete flow matching). 


For each method, we vary the total number of reverse steps K. We report both the Fréchet Inception Distance (FID) and an $\text{F}_1^{\mathrm{dc}}$ score, which is a harmonic mean of coverage and density metrics \citep{naeem2020reliablefidelitydiversitymetrics}. These two metrics complement each other, with FID measuring the global realism of generated samples and $\text{F}_1^{\mathrm{dc}}$ capturing how well the generated data distribution covers the real data (coverage) while maintaining sample fidelity (density). In other words, $\text{F}_1^{\mathrm{dc}}$ provides a more localized, distributional perspective; see \cref{app:experiment_metric} for further details.

As shown in \cref{tab:mnist_fid_f1dc}, the proposed DMPM approaches (rows 3 and 4) consistently outperform the baselines (DFM and MD4) across a range of step counts. At $K=200$ reverse steps, DMPM (denoise-renoise) achieves the lowest FID of $2.89$ (compared to $4.48$ for MD4 and $16.26$ for DFM), alongside an $\text{F}_1^{\mathrm{dc}}$ of $1.00$. Even at a lower number of steps (e.g., $K=50$), DMPM (denoise-renoise) attains an FID of $8.62$, while preserving a strong $\text{F}_1^{\mathrm{dc}}$ of $0.87$. Meanwhile, DMPM (flip-schedule) shows a similarly favorable trade-off, achieving FID below $10$ for $K=25$ and $\text{F}_1^{\mathrm{dc}}$ above $0.90$, which are remarkable results for this few network calls. 


To visually validate generation quality, we refer the reader to \cref{fig:dmpm_default_grid} for an image grid generated using the DMPM sampler, with $25$ reverse steps, and to \cref{fig:dmpm_denoise_renoise_grid} for an image grid generated using the denoise-renoise sampler, with $200$ reverse steps.




% For each method, we vary the total number of reverse steps $K$. We report the Fr\'echet Inception Distance (FID), and an $\text{F}_1^{\mathrm{dc}}$ score, which is a harmonic mean of coverage and density metrics \citep{naeem2020reliablefidelitydiversitymetrics}. They assess the overlap of sample distributions using local geometric structures, and provide an effective alternate measure of performance, see \cref{app:experiment_metric} for further discussions.

% As we observe in \cref{tab:mnist_fid_f1dc}, our DMPM approaches obtain better scores while requiring fewer network inferences. Notably, the denoise-renoise variant achieves the overall best FID performance and maintains high scores with 5$\times$ fewer steps compared to the baselines. To visually validate generation quality, we refer the reader to \cref{fig:dmpm_default_grid} for an image grid generated using the default DMPM sampler, with $25$ reverse steps, and to \cref{fig:dmpm_denoise_renoise_grid} for an image grid generated using the denoise-renoise sampler, with $200$ reverse steps.

% \begin{table}[t]
% \centering
% \scalebox{0.75}{
% \begin{tabular}{lrrrrrr}
% \toprule
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{6}{c}{\textbf{Reverse Steps}} \\
% \cmidrule(lr){2-7}
%  & \textbf{10} & \textbf{25} & \textbf{50} & \textbf{100} & \textbf{200} & \textbf{500}\\
% \midrule
% \textbf{DFM} & 227.55 & 156.26 & 88.93 & 39.62 & 16.26 & 7.34  \\
%              & \scriptsize\textit{0.00} & \scriptsize\textit{0.00} & \scriptsize\textit{0.01} & \scriptsize\textit{0.14} & \scriptsize\textit{0.41} & \scriptsize\textit{0.68}\\
% \textbf{MD4} & 97.97 & 33.50 & 14.06 & 6.83 & 4.48 & \textit{3.43} \\
%              & \scriptsize\textit{0.04} & \scriptsize\textit{0.29} & \scriptsize\textit{0.57} & \scriptsize\textit{0.76} & \scriptsize\textit{0.83} & \scriptsize\textit{0.86} \\
% \textbf{DMPM} & {16.30} & {9.98} & 11.07 & 9.07 & 7.80 & 10.84  \\
%  \scriptsize \textit{flip-schedule}           &  \scriptsize\textit{0.64} & \scriptsize\textit{0.92} & \scriptsize\textit{0.93} & \scriptsize\textit{0.93} & \scriptsize\textit{0.93} & \scriptsize\textit{0.70} \\
% \textbf{DMPM} & 78.20 & 20.94 & {8.62} & \underline{3.98} & \textbf{2.89} & 4.36 \\
% \scriptsize \textit{denoise-renoise}    & \scriptsize\textit{0.13} & \scriptsize\textit{0.67} & \scriptsize\textit{0.87} & \scriptsize\textit{0.96} & \scriptsize\textit{1.00} & \scriptsize\textit{1.00} \\
% \bottomrule
% \end{tabular}
% }
% \caption{FID$\downarrow$ (top) and \textit{F$_1^{\mathrm{dc}}$}$\uparrow$ (\emph{bottom}) on MNIST, for various numbers of total reverse steps. We identify the best result in \textbf{bold}, the 2nd best in \textit{italics}, and \underline{underline} the 3rd best.}
% \label{tab:mnist_fid_f1dc}
% \end{table}


\begin{table}[t]
\centering
\scalebox{0.75}{
\begin{tabular}{llrrrrrr}
\toprule
\textbf{Method} & & \textbf{10} & \textbf{25} & \textbf{50} & \textbf{100} & \textbf{200} & \textbf{500} \\
\midrule
\multirow{2}{*}{\textbf{DFM}}
  & FID & 227.55 & 156.26 & 88.93 & 39.62 & 16.26 & 7.34 \\
  & F$_1^{\mathrm{dc}}$ & 0.00 & 0.00 & 0.01 & 0.14 & 0.41 & 0.68 \\
\midrule
\multirow{2}{*}{\textbf{MD4}}
  & FID & 97.97 & 33.50 & 14.06 & 6.83 & 4.48 & \textit{3.43} \\
  & F$_1^{\mathrm{dc}}$ & 0.04 & 0.29 & 0.57 & 0.76 & 0.83 & 0.86 \\
\midrule
\multirow{2}{*}{\textbf{DMPM}$_{\textit{flip}}$}
  & FID & 16.30 & 9.98 & 11.07 & 9.07 & 7.80 & 10.84 \\
  & F$_1^{\mathrm{dc}}$ & 0.64 & 0.92 & 0.93 & 0.93 & 0.93 & 0.70 \\
\midrule
\multirow{2}{*}{\textbf{DMPM}$_{\textit{denoise}}$}
  & FID & 78.20 & 20.94 & 8.62 & \underline{3.98} & \textbf{2.89} & 4.36 \\
  & F$_1^{\mathrm{dc}}$ & 0.13 & 0.67 & 0.87 & \underline{0.96} & \textit{1.00} & \textbf{1.00} \\
\bottomrule
\end{tabular}
}
\caption{FID$\downarrow$ (first row of each method) and F$_1^{\mathrm{dc}}$ $\uparrow$ (second row) on MNIST for various total reverse steps. We highlight the best result in \textbf{bold}, the 2\textsuperscript{nd} best in \textit{italics}, and \underline{underline} the 3\textsuperscript{rd} best.}
\label{tab:mnist_fid_f1dc}
\end{table}



% \begin{table}[t]
% \centering
% \scalebox{0.75}{
% \begin{tabular}{lrrrrrrr}
% \toprule
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{7}{c}{\textbf{Reverse Steps}} \\
% \cmidrule(lr){2-8}
%  & \textbf{10} & \textbf{25} & \textbf{50} & \textbf{100} & \textbf{200} & \textbf{500} & \textbf{1000}\\
% \midrule
% \textbf{DFM} & 227.55 & 156.26 & 88.93 & 39.62 & 16.26 & 7.34 & 3.87 \\
%              & \scriptsize\textit{0.00} & \scriptsize\textit{0.00} & \scriptsize\textit{0.01} & \scriptsize\textit{0.14} & \scriptsize\textit{0.41} & \scriptsize\textit{0.68} & \scriptsize\textit{0.81}\\
% \textbf{MD4} & 97.97 & 33.50 & 14.06 & 6.83 & 4.48 & \textit{3.43} & \textbf{3.11} \\
%              & \scriptsize\textit{0.04} & \scriptsize\textit{0.29} & \scriptsize\textit{0.57} & \scriptsize\textit{0.76} & \scriptsize\textit{0.83} & \scriptsize\textit{0.86} & \scriptsize\textit{0.89}\\
% \textbf{DMPM} & {16.30} & {9.98} & 11.07 & 9.07 & 7.80 & 10.84 & 12.21 \\
%  \scriptsize \textit{flip-schedule}           &  \scriptsize\textit{0.64} & \scriptsize\textit{0.92} & \scriptsize\textit{0.93} & \scriptsize\textit{0.93} & \scriptsize\textit{0.93} & \scriptsize\textit{0.70} & \scriptsize\textit{0.75}\\
% \textbf{DMPM} & 78.20 & 20.94 & {8.62} & \underline{3.98} & \textbf{2.89} & 4.36 & 6.26 \\
% \scriptsize \textit{denoise-renoise}    & \scriptsize\textit{0.13} & \scriptsize\textit{0.67} & \scriptsize\textit{0.87} & \scriptsize\textit{0.96} & \scriptsize\textit{1.00} & \scriptsize\textit{1.00} & \scriptsize\textit{0.99}\\
% \bottomrule
% \end{tabular}
% }
% \caption{FID$\downarrow$ (top) and \textit{F$_1^{\mathrm{dc}}$}$\uparrow$ (\emph{bottom}) on MNIST, for various numbers of total reverse steps. We identify the show the best result in \textbf{bold}, 2nd best in \emph{italics}, and \underline{underline} the 3rd best.}
% \label{tab:mnist_fid_f1dc}
% \end{table}














% We observe that DMPM, particularly with the denoise-renoise sampler, not only reaches lower FID overall, but also does so with markedly fewer (5x) reverse steps. This reduced sample complexity points to the efficiency advantages of our design, enabling higher-quality generations with significantly fewer network calls.

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.95\columnwidth]{mnist_experiment_results_img/lines_method_vs_ema_0.99_fid_by_reverse_steps.png}
% \caption{FID$\downarrow$Comparison of our DMPM variants against MD4 and DFM. DMPM outperforms the baselines across most reverse-step ranges and obtains better results at even lower network inference counts.}
% \label{fig:exp-fig5}
% \end{figure}

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.95\columnwidth]{mnist_experiment_results_img/lines_method_vs_ema_0.99_f_1_dc_by_reverse_steps.png}
% \caption{$\text{F}_1^{\text{dc}}\uparrow$ Comparison of our DMPM variants against MD4 and DFM. DMPM outperforms the baselines across most reverse-step ranges and obtains better results at even lower network inference counts.}
% \label{fig:exp-fig6}
% \end{figure}

% \dario{Rather use a big latex table for multiple metrics: FID, f1 dc, and coverage (to measure mode collapse). Takeaway: DMPM-default best at very small number of reverse steps, DMPM-denoise-renoise best overall, with optimal performance achieved with 5x less network inference}

% \begin{table}[H]
% \centering
% \caption{FID scores (lower is better) comparing ``divide by $w_t$'' versus no division. We examine four loss-weight configurations.}
% \label{tab:divide-by-gamma}
% \scalebox{0.75}{
% \begin{tabular}{lrrrrr}
% \toprule
% divide by $w_t$ & $(0.0,\,0.0,\,1.0)$ & $(0.33,\,0.33,\,0.33)$ & $(0.5,\,0.0,\,0.5)$ & $(1.0,\,0.0,\,0.0)$ \\
% \midrule
% False & 35.09 & 334.05 & 34.60 & 33.53 \\
% True  & 36.37 & 54.54  & 32.32 & 35.76 \\
% \bottomrule
% \end{tabular}
% }
% \end{table}

% \begin{table}[H]
% \centering
% \caption{FID scores for varying $\Tf\in\{3,10\}$, with the same four loss configurations. Larger $\Tf$ degrades performance noticeably.}
% \label{tab:tf-binarymnist}
% \scalebox{0.75}{
% \begin{tabular}{lrrrrr}
% \toprule
% $\Tf$ & $(0.0,\,0.0,\,1.0)$ & $(0.33,\,0.33,\,0.33)$ & $(0.5,\,0.0,\,0.5)$ & $(1.0,\,0.0,\,0.0)$ \\
% \midrule
% 3.0  & 35.09 & 334.05 & 34.60 & 33.53 \\
% 10.0 & 145.23 & 353.42 & 137.12 & 138.45 \\
% \bottomrule
% \end{tabular}
% }
% \end{table}

% Table~\ref{tab:tf-binarymnist} shows that a larger time horizon ($\Tf=10$) worsens FID, consistent with the small-dimensional results. Table~\ref{tab:divide-by-gamma} compares dividing the loss by $w_t$ or not: we see a substantial improvement only for the mixed cross-entropy + L2 case $(\mu=0.5,\,\eta=0.5)$, yielding the best overall FID score of $\mathbf{32.32}$. Introducing the KL term ($\zeta\neq 0$) leads to substantial performance drops (e.g.\ $334.05$ or $54.54$), so we do not pursue $\zeta > 0$ in future high-dimensional experiments.

\subsection{Conclusions}

Our experiments demonstrate that DMPM matches or outperforms state-of-the-art discrete generative models, on low and high dimensional data. 
% On the synthetic sawtooth dataset, DMPM achieves smaller approximation error with fewer reverse steps. 
On binarized MNIST, DMPM obtains lower FID and $F_1^{\text{dc}}$ than baseline methods, like Discrete Flow Matching, with at least 5$\times$ fewer network calls. 
We trace this success to the discrete diffusion structure, and subsequent sensible design choices. We hope for fast progress and adoption of our DMPM models, thanks to their similarity with existing continuous diffusion approaches.
% both theoretically and in terms of training and sampling methodologies.


\section*{Acknowledgements}
The work of A. Ocello was funded by the European Union (ERC-2022-SYG-OCEAN-101071601). Views and opinions expressed are however those of the author only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.
% , including our loss configuration, the cosine time-scheduling, or our denoise-renoise sampler. 

% Together, these results suggest that discrete diffusion approaches, when carefully structured, can efficiently scale to high-dimensional spaces while retaining strong generative performance.


% Our experiments on both small and higher-dimensional Bernoulli data (sawtooth distributions and binary MNIST) show consistent benefits to using a smaller time horizon $\Tf$ and smaller jump rate $\lambda$. In the higher-dimensional regime, introducing the KL loss component significantly damages performance. Dividing by a time-varying $w_t$ factor only helps if cross-entropy and L2 terms are equally weighted in the loss. Overall, we find that a moderate combination of cross-entropy and $\mrl^2$ yields the best fidelity metrics, especially when $\Tf$ remains small.




% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

% \clearpage
% \newpage
% \section*{Impact Statement}

% This paper presents work whose goal is to advance the field of
% Machine Learning. There are many potential societal consequences
% of our work, none which we feel must be specifically highlighted here.

\bibliography{ICML2025.bib}
\bibliographystyle{plainnat}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
%\section{You \emph{can} have an appendix here.}

\section{Existing works on diffusion-based generative models for discrete data}
\label{app:related_works}
This section provides details of the recent researches on discrete generative models.

\textbf{Embedding discrete structure in the continuous space. }
To keep the benefits of continuous representations, \citet{dieleman2022continuous} and \citet{chen2022analog} mapped discrete structures into Euclidean space, while \citet{richemond2022categorical} placed them into the simplex, all while continuing to use forward continuous diffusion models. In particular, \citet{dieleman2022continuous} proposed a continuous diffusion model for categorical data, which has some advantages over autoregressive models, such as the ability to perform arbitrary infilling and a more flexible sampling process. However, this method comes with an expensive training cost  and lacks of strong theoretical guarantees.

\textbf{Argmax flows and Multinomial Diffusion. }\citet{hoogeboom2021argmax} introduced two new generative models, Argmax Flows and Multinomial Diffusion, to handle categorical data like text and image segmentation. Argmax Flows connect discrete data with continuous models by using an argmax function combined with a probabilistic inverse, making categorical distributions easy-learning. Multinomial Diffusion process uses a categorical distribution to add noise to discrete data and then trains a model to reverse the process. However, both Argmax Flows and Multinomial Diffusion have some limitations: computational costs increase due to additional steps
% (such as learning probabilistic inverses or training a diffusion process)
, and the theoretical guarantee is missing.


\textbf{Designing the flow processes over the discrete state space. }
\citet{campbell2022continuous}  introduced the first complete continuous-time framework for denoising diffusion models applied to discrete data. They used CTMCs to model the forward noising process and its time-reversal dynamics. While the core idea is similar to ours, their approach is more complex because their method consider generic CTMC and is not specialized to the noising process that we consider. As a result, their method essentially boils down learning density ratios which can be computationally demanding and fail to offer efficient approximation in high dimensions.
 They also added a correction step to bring the sample distribution closer to the desired one, which increased the practical training cost \citet{gat2024discrete}.
 By focusing on the random-walk CTMC on $\msx$, we were able to provide a discrete counterpart to the score function that is learn in continuous diffusion models and also to establish strong convergence guarantees for our method.  


\textbf{Generator Matching. }
Another recent approach to handle discrete data is generative modeling with arbitrary Markov processes using generator matching, introduced by \citet{holderrieth2024generator}. In this approach, the authors design an appropriate Markov process that transforms a simple distribution into the desired data one using a generator, which can be efficiently trained with a neural network. This method is quite flexible and can be applied to different state spaces, especially in discrete settings. However, this method being very generic suffer from the same drawback as \citet{campbell2022continuous}.

\textbf{Masked diffusion models. }
One important step toward more advanced models is the "masked" diffusion process, a discrete diffusion approach first introduced by \citet{austin2021structured}. Recently, \citet{shi2024simplified} looked into this model further, simplifying its training objective by expressing it as a signal-to-noise ratio, which helps highlight some useful features. However, despite these improvements, the model still lacks theoretical guarantees and remains expensive to train in practice.

\textbf{Discrete Diffusion Models via a Stochastic Integral Framework. }
\citet{ren2024discrete} introduced a new way to analyze discrete diffusion models via L\'evy-type stochastic integrals and expanded Poisson random measures. Specifically, they established the stochastic integral expressions of the noising and denoising processes for the categorical data. They provided a unified error analysis framework and showed the first error bound for their algorithms in $\KL$ divergence. However, their results rely on strong assumptions in contrast to our results. Besides, our bounds are simpler and better, in particular with respect to the time horizon.  % has some drawbacks such as strong assumptions required, high computational complexity, complicated formulations, and especially a lack of empirical validation.  


Our paper takes a step toward bridging these gaps. By clearly describing the forward Markov process, we can express the score function as a conditional expectation, which helps us avoid the costly signal-to-noise ratio training used in \citet{shi2024simplified}. This way, we not only offer a simpler and more affordable training approach, but also provide solid theoretical guarantees for our models in practice.




\section{Interpretation of DMPMs}
\subsection{The simple case $\msx=\l\{0,1\r\}$}
\begin{proof}[Detailed calculation of the transition probability in \eqref{eq:def_density_one_dim_transition}]\label{proof_eq:transition_d=1}
Based on the Kolmogorov equation, the transition matrix $\ovr{p}^1_t$ for $0\leq t \leq \Tf$ admits the following formula
\begin{equation*}
    \ovr{p}^1_t=\rme^{t\ovr{q}} \eqsp,
\end{equation*}
where $\ovr{q}$ is define in \eqref{eq:def_generator_d=1}. Clearly, the generator $\ovr{q}$ admits two eigenvalues $0$ and $-2\lambda$ associated with the eigenvectors $\begin{pmatrix}1 & 1 \end{pmatrix}^{\rmT}$ and $\begin{pmatrix} 1 & -1 \end{pmatrix}^{\rmT}$ respectively. Then we can diagonalize $\ovr{q}$ as
\begin{equation*}
    \ovr{q}=\begin{pmatrix} 
    1 & 1 \\
    1 & -1
    \end{pmatrix}
    \begin{pmatrix} 
    0 & 0 \\
    0 & -2\lambda
    \end{pmatrix}
    \begin{pmatrix} 
    1 & 1 \\
    1 & -1
    \end{pmatrix}^{-1} \eqsp,
\end{equation*}
and the transition matrix $\ovr{p}^1_t$ follows
\begin{equation*}
    \ovr{p}^1_t=\rme^{t\ovr{q}}=\begin{pmatrix} 
    1 & 1 \\
    1 & -1
    \end{pmatrix}
    \begin{pmatrix} 
    1 & 0 \\
    0 & \rme^{-2\lambda}
    \end{pmatrix}
    \begin{pmatrix} 
    1 & 1 \\
    1 & -1
    \end{pmatrix}^{-1} = 
    \dfrac{1}{2}\begin{pmatrix} 
    1+\rme^{-2\lambda} & 1-\rme^{-2\lambda} \\
    1-\rme^{-2\lambda} & 1+\rme^{-2\lambda}
    \end{pmatrix} \eqsp.
\end{equation*}
\end{proof}




\subsection{General state space $\msx=\l\{0,1\r\}^d$}
\subsubsection{Forward transition probability}\label{proof_eq:transition_d}
\begin{proof}[\textbf{Proof of \eqref{eq:transition_d}}]
We start with a note that the generator matrix $\overrightarrow{q}$ can be expressed as a sum of matrices $\overrightarrow{q}^\ell$ as follows
\begin{align*}
    \overrightarrow{q} =\sum_{\ell=1}^d \overrightarrow{q}^\ell, \quad \text{with } \overrightarrow{q}^\ell(x,y)=\begin{cases}
        \hspace{0.2cm}\lambda \eqsp, \quad &\text{if } x^i=y^i \text{ for } i \neq \ell \text{ and } x^\ell \neq y^\ell \eqsp,\\
        -\lambda \eqsp, \quad &\text{if } x=y \eqsp,\\
        \hspace{0.2cm}0 \eqsp, \quad &\text{otherwise} \eqsp.
    \end{cases}
\end{align*}
Notice that $\overrightarrow{q}^\ell$ also admits the following formula with respect concerning the tensor product
\begin{align}\label{qlt}
    \overrightarrow{q}^\ell=\underbrace{\I \otimes \I \otimes... \otimes \I \otimes  \overrightarrow{A}\otimes \I\otimes... \otimes \I}_\text{$d$ times}\eqsp,
\end{align}
with $\I$ the $2\times 2$ identity matrix and $\overrightarrow{A}=\begin{pmatrix}
    -\lambda & \lambda\\
    \lambda & -\lambda
\end{pmatrix}$, which is the $\ell^{th}$ matrix in the previous product. Indeed, by the definition of tensor product, for any $x=(x^i)_{i=1}^d, y=(y^i)_{i=1}^d \in \msx$, we observe that
\begin{align*}
    (\I \otimes \I \otimes... \otimes \I \otimes \underbrace{\overrightarrow{A}}_{\ell^{th}}\otimes \I\otimes... \otimes \I) (x,y)&=\I(x^1,y^1)\I(x^2,y^2)...\overrightarrow{A} (x^\ell,y^\ell)...\I(x^d,y^d)\\
    &=\begin{cases}
         ~~1 \eqsp, \quad &\text{if } x^i=y^i \text{ for } i \neq \ell \text{ and } x^\ell \neq y^\ell \eqsp,\\
        -1 \eqsp, \quad &\text{if } x=y \eqsp,\\
        ~~0 \eqsp, \quad &\text{otherwise} \eqsp.
    \end{cases}
\end{align*}
which is exactly the expression of $\overrightarrow{q}^\ell(x,y)$. We now use the Kolmogorov equation combined with the expression of $\overrightarrow{q}^\ell_t$ in \eqref{qlt}, and apply the formula $\rme^{\I \otimes A+B\otimes \I}=\rme^{A}\otimes \rme^B$ for any matrix $A,B$ \citep[Appendix]{gavrilyuk2011exponentially} to get
\begin{align*}
    \overrightarrow{p}_{t}=\rme^{t\overrightarrow{q}}=\rme^{\sum_{\ell=1}^d t\overrightarrow{q}^\ell}=\underbrace{\rme^{t\overrightarrow{A}}\otimes...\otimes \rme^{t\overrightarrow{A}}}_\text{$d$ times}\eqsp.
\end{align*}
We are thus left with the computation of $\rme^{t\overrightarrow{A}}$. It is clear that the eigenvalues of $\overrightarrow{A}$ are $0$ and $-2\lambda $, with the corresponding eigenvectors $\begin{pmatrix}
    1 & 1
\end{pmatrix}^T$ and $\begin{pmatrix}
    1 & -1
\end{pmatrix}^T$  respectively. Consequently, we can compute $\rme^{t\ovr{A}}$ as: for any $a,b \in \l\{0,1\r\}$,
\begin{align*}
    \overrightarrow{p}_{t}^1(a,b):=\rme^{t\overrightarrow{A}}(a,b)=\begin{cases}
        \frac{1}{2}+\frac{1}{2}\rme^{-2t}\eqsp, \quad &\text{if } a=b\eqsp,\\
        \frac{1}{2}-\frac{1}{2}\rme^{-2t}\eqsp, \quad &\text{if } a\neq b\eqsp,
    \end{cases}
\end{align*}
and the formula of transition probability $\ovr{p}_t$ for $0\leq t \le\Tf$ follows: for any $x=(x^i)_{i=1}^d$ and $ y=(y^i)_{i=1}^d$ in $\msx$,
\begin{align*}
    \overrightarrow{p}_{t}(x,y)=\prod_{i=1}^d \overrightarrow{p}^1_{t}(x^i,y^i), \quad \text{with } \overrightarrow{p}_{t}^1(x^i,y^i)=\begin{cases}
        \frac{1}{2}+\frac{1}{2}\rme^{-2\lambda t}\eqsp, \quad &\text{if } x^i=y^i\eqsp,\\
        \frac{1}{2}-\frac{1}{2}\rme^{-2\lambda t}\eqsp, \quad &\text{otherwise\eqsp.}
        \end{cases}
\end{align*}
\end{proof}
 
\subsubsection{Conditional expectation expression of the score function}\label{proof_prop:1}
\begin{proof}[\textbf{Proof of \Cref{prop:1}}]
Fix $x \in \msx$ and $\ell =1,\ldots, d$. By the formula of the marginal distribution, we see that
\begin{align}\label{eq:7}
    \mu_{\Tf-t}(x)-\mu_{\Tf-t}(\varphi^{(\ell)}(x))&=\sum_{z\in \msx}\mu_0(z)( \overrightarrow{p}_{\Tf-t}(z,x)-\overrightarrow{p}_{\Tf-t}(z,\varphi^{(\ell)}(x)))\eqsp.
\end{align}
The formula of transition probabilities $\overrightarrow{p}_{\Tf-t}(z,\varphi^{(\ell)}(x))$ combined with the definition of $\varphi^{(\ell)}(x)$ lead to
\begin{align*}
    \overrightarrow{p}_{\Tf-t}(z,\varphi^{(\ell)}(x))&=\prod_{i=1}^d \ovr{p}^1_{\Tf-t}(z^i,\varphi^i_\ell(x))\\
    &=\ovr{p}^1_{\Tf-t}(z^\ell,\varphi^\ell_\ell(x))\prod_{\substack{i=1\\i\neq \ell}}^d \ovr{p}^1_{\Tf-t}(z^i,x^i)\\
    &=\dfrac{\ovr{p}^1_{\Tf-t}(z^\ell,\varphi^\ell_\ell(x))}{\ovr{p}^1_{\Tf-t}(z^\ell,x^\ell)}\overrightarrow{p}_{\Tf-t}(z,x)\eqsp.
\end{align*}
Substituting this into \eqref{eq:7} implies
\begin{align*}
    \mu_{\Tf-t}(x)-\mu_{\Tf-t}(\varphi^{(\ell)}(x))&=\sum_{z\in \msx}\mu_0(z)\overrightarrow{p}^1_{\Tf-t}(z,x)(1 -\dfrac{\ovr{p}^1_{\Tf-t}(z^\ell,\varphi^{(\ell),\ell}(x))}{\ovr{p}^1_{\Tf-t}(z^\ell,x^\ell)})\\
    &=\sum_{z\in \msx}\l[\dfrac{2\rme^{-2\lambda(\Tf-t)}}{1+\rme^{-2\lambda(\Tf-t)}}-\dfrac{4\rme^{-2\lambda(\Tf-t)}(x^\ell-z^\ell)^2}{1-\rme^{-4\lambda(\Tf-t)}}\r]\P \l[\foX_0=z, \foX_{\Tf-t}=x \r]\eqsp,
\end{align*}
where the last equality comes from the formula of $\ovr{p}^1_{\Tf-t}$ and the fact that if $z^\ell=\varphi^{(\ell),\ell}(x)$ then $z^\ell \neq x^\ell$. Therefore, the score function in components are
\begin{align*}
    s_{t}^\ell(x) &= \dfrac{\mu_{\Tf-t}(x)-\mu_{\Tf-t}(\varphi^{(\ell)}(x))}{\mu_{\Tf-t}(x)}\\
    &= \sum_{z\in \msx}\l[\dfrac{2\rme^{-2\lambda(\Tf-t)}}{1+\rme^{-2\lambda(\Tf-t)}}-\dfrac{4\rme^{-2\lambda(\Tf-t)}(x^\ell-z^\ell)^2}{1-\rme^{-4\lambda(\Tf-t)}}\r]\P\l[\foX_0=z | \foX_{\Tf-t}=x \r]\\
    &=\E \l[\dfrac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}-\dfrac{4\alpha_{\Tf-t}(\foX_{\Tf-t}^\ell-\foX_0^\ell)^2}{1-\alpha^2_{\Tf-t}} | \foX_{\Tf-t}=x\r] \eqsp,
\end{align*}
where $\alpha_t=\rme^{-2\lambda t}$, and we finish the proof of \cref{prop:1}.
\end{proof}

\subsubsection{Invariant measure of the forward process}\label{invariant_measure}
As we have a comprehensive understanding of the forward process, we observe that its invariant measure is the uniform distribution over $\msx$, denoted by $\gamma^d$. Indeed, for any $x\in \msx$ and $t \in \ccint{0,\Tf}$,
\begin{equation*}
    (\gamma^d \ovr{p}_t)(x)=\sum_{z\in \msx} \gamma^d(z)\ovr{p}_t(z,x)=\dfrac{1}{2^d}\sum_{z\in \msx}\ovr{p}_t(z,x)=\dfrac{1}{2^d}=\gamma^d(x) \eqsp.
\end{equation*}
Furthermore, by formula of $\ovr{p}$ given in \eqref{eq:transition_d}, we have $\ovr{p}_t(x,y) \xrightarrow[]{t\to \infty} \frac{1}{2^d}$ for any $x,y \in \msx$. Consequently, the following holds for any $x\in \msx$,
\begin{equation*}
    \mu_t(x)=\sum_{z\in \msx} \mu_0(z)\ovr{p}_t(z,x) \xrightarrow[]{t\to \infty} \dfrac{1}{2^d}\sum_{z\in \msx} \mu_0(z)=\dfrac{1}{2^d}=\gamma^d(x) \eqsp,
\end{equation*}
meaning that the forward dynamic $(\foX_t)_{t\in \ccint{0,\Tf}}$ converges geometrically fast to $\gamma^d$.




\section{Implementation of DMPMs}
\subsection{Alternative ideal backward simulation}\label{appendix:simulation_backward}

Besides the simulation of the backward process provided in \cref{section_backward}, we can also use the following procedure to produce the time-reversal dynamic. 


The second procedure to sample $(\baX_t)_{t \in\ccint{0,\Tf}}$ is to
consider a sample $\overleftarrow{X}_0$ from $\mu_{\Tf}$ and a sequence of
\iid~random variables distributed according to the exponential
distribution with parameter $1$,
$\{E_i^{\ell} \,: \, i \in \nset\, , \, \ell \in\iint{1}{d}\}$, we can define the jump times $(T_i)_{i\in \nset}$
%$(T_i)_{i\in\{1,\ldots,N\}}|N\simiid \unif(\ccint{0,\Tf})$ 
of the backward process and its transition by induction setting $T_0=0$. Given $(T_i,\baX_{T_i})$, we define the next jump time as $T_{i+1}^j = T_i + \Delta T_{i+1}^j$, where $\Delta T_{i+1}^j = \inf \{t \geq 0\, :\, \int_{0}^t \lambda(1-s^{j}(\baX_{T_i})) \rmd r \geq E_i^{j} \}$.
Then, set $T_{i+1} = T_{i+1}^{\ell_i}$, where $\ell_i = \argmin_{j\in\iint{1}{d}} T_{i+1}^{j}$, and $\overleftarrow{X}_t = \overleftarrow{X}_{T_i}$ for $t\in\ooint{T_i,T_{i+1}\wedge \Tf}$, and finally if $T_{i+1} < \Tf$, $\overleftarrow{X}_{T_{i+1}}^{\ell_i} = 1-\overleftarrow{X}_{T_i}^{\ell_i}$ for $\ell_i\in\{1,\ldots,d\}$.

\subsection{Perfect backward approximation}\label{pseudo_code}
 We provide here the pseudo-code of backward approximation sampling in continuous time scheme:

\begin{algorithm}
    \caption{DMPMs Algorithm (Continuous time scheme)}\label{alg:backward_approximation_continuous}
    \textbf{Input:} a time horizon $\Tf \gg 1$ large enough, a prescribed jump rate $\lambda$, an approximate score function $s^{\theta^\star}$
\begin{algorithmic}
\STATE \textbf{Backward process:}
\STATE Set $T_0=0$ and initialize $\overleftarrow{X}_0 \sim \gamma^d$ 
\STATE $i \gets 0$ 
\WHILE{$T_i\leq \Tf$}
    \STATE Draw $E_i\sim \text{Exp}(1)$
    \STATE Solve $\Delta T_{i+1} = \inf \{t \geq 0\, :\, \int_0^t \lambda^{\theta^\star}_{T_i+r}(\baX_{T_i})\rmd r \geq E_i \} $, with $\lambda^{\theta^\star}_t(x)=\lambda\sum_{\ell=1}^d(1-s^{\theta^\star,\ell}_{t}(x))$
    \STATE Set $T_{i+1}=T_i+\Delta T_{i+1}$
    % \FOR {$j=1,\ldots, M$}
    \IF{$T_i < t <\min(T_{i+1},\Tf)$}
        \STATE Set $\baX_{t}=\baX_{T_i}$
    \ENDIF
    % \ENDFOR
    \IF{$T_{i+1}<\Tf$}
        \STATE Draw $\ell_i\in\{1,\ldots,d\} \sim \categorial(\{  \lambda(1- s_{T_{i+1}}^{\theta^\star,\ell}(\baX_{T_i})) / \lambda^{\theta^\star}_{T_{i+1}}(\baX_{T_i}) \}_{\ell=1}^d)$
        \STATE Set $\baX_{T_{i+1}}=\varphi^{(\ell_i)}(\baX_{T_i})$
    \ENDIF
     \STATE $i \gets i+1$
\ENDWHILE

%\STATE Sample the backward process $(\overleftarrow{X}^\star_{t_k} )_{k=1,\ldots,N}$ , using the parametrized score $s^{\theta^\star}$
\end{algorithmic}
\textbf{Output:} $\overleftarrow{X}_{\Tf}$
\end{algorithm}


\subsection{Discrete denoiser and score reparameterization}
\label{app:discrete_denoiser}

\paragraph{Discrete-denoiser structure}

Recall from \cref{prop:1} that each score component admit the following conditional expectation:
{
    \begin{align}
         s_{t}^\ell(x)=\E \l[  f^{\ell}_t(\overrightarrow{X}_0^\ell,  \overrightarrow{X}_{\Tf-t})| \overrightarrow{X}_{\Tf-t}=x\r]\eqsp,
    \end{align}
    }
where 
\begin{equation}
 f^{\ell}_t(\overrightarrow{X}_0^\ell,  \overrightarrow{X}_{\Tf-t})   =  \dfrac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}-\dfrac{4\alpha_{\Tf-t}(\overrightarrow{X}_{\Tf-t}^\ell-\overrightarrow{X}_0^\ell)^2}{1-\alpha_{\Tf-t}^2}
\end{equation}
 for $t \in [0,\Tf)$, $x\in \msx$ and $\ell = 1,\ldots,d$.

Remark that 
\begin{equation}
    \E \l[ f^{\ell}_t(\overrightarrow{X}_0^\ell,  \overrightarrow{X}_{\Tf-t}) | \overrightarrow{X}_{\Tf-t}=x \r] = 
    \dfrac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}-\dfrac{4\alpha_{\Tf-t}\mE \l[(\overrightarrow{X}_{\Tf-t}^\ell-\overrightarrow{X}_0^\ell)^2 | \overrightarrow{X}_{\Tf-t}=x \r]}
{1-\alpha_{\Tf-t}^2}\eqsp.
\end{equation}
Thus we introduce the function $d^\ell_t$ defined as
\begin{equation}
    d^\ell_t : x \mapsto \mE \l[(\overrightarrow{X}_{\Tf-t}^\ell-\overrightarrow{X}_0^\ell)^2 | \overrightarrow{X}_{\Tf-t}=x \r],
\end{equation}
which can be further rewritten as
\begin{align*}
    d_{t}^\ell(x) &= \mE \l[\l(\fX_{\Tf - t}^\ell - \fX_0^\ell\r)^2 \bigg| \fX_{\Tf - t} = x\r] \\
    &= \mE \l[\mathds{1}_{\fX_{\Tf - t}^\ell \neq \fX_0^\ell} \bigg| \fX_{\Tf - t} = x\r] \\ 
    &= \mathbb{P} \l(\fX_0^\ell \neq x^\ell \bigg| \fX_{\Tf - t} = x\r) \eqsp. 
    % \\ &= \mathbb{P} \l(\fX_0^l \neq x^l \r)\eqsp.
\end{align*}
In some sense, this is the discrete version of the continuous denoiser $\mE[\fX_0 | \fX_t]$ approximated by classical diffusion models \citep{song2021scorebasedgenerativemodelingstochastic}, as obtained from the score by Tweedie's formula. Thus we call $d_{t}^\ell(x)$ the discrete denoiser.

\paragraph{Score reparameterization}
Based on the previous derivations, each score component $s_{t}^\ell(x)$ can be written as a function of $d^\ell_t$:
\begin{equation}
     s_{t}^\ell(x) = \dfrac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}-\dfrac{4\alpha_{\Tf-t} d^\ell_t(x)}
{1-\alpha_{\Tf-t}^2}\eqsp,
\end{equation}
So we can reparameterize our score models $s_{t}^{\theta}$ as 
\begin{equation}
    \label{eq:score_reparameterization}
     s_{t}^{\theta, \ell}(x)= \dfrac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}-\dfrac{4\alpha_{\Tf-t} d_t^{\theta, \ell}(x)}{1-\alpha_{\Tf-t}^2}\eqsp,
\end{equation}
where $d_t^{\theta, \ell}(x)$ aims to approximate $d_t^{\ell}(x)$.

\subsection{Objective functions derived from the discrete denoiser structure}

Inspired by the previous derivations, we modify our existing $\lossscore$ loss function to replace by a denoising loss equivalent. We introduce a cross-entropy loss, and finally propose a scaling of the loss functions, based on the average output magnitude of the discrete denoiser, thus helping with the learning, and improving synergies between loss elements.

\label{app:objective_functions}
\paragraph{Score-matching objective $\loss_{\mrl^2}$}
We rewrite the objective function $\loss_{\mrl^2}$ to fit the \emph{discrete denoiser}, considered as a conditional expectation:
 \begin{equation}
 \label{eq:l2_loss_denoiser}
    \lossscoredenoiser : 
     \theta \mapsto \int_0^{\Tf} \loss_{t, \mrl^2}(\theta) \rmd t\eqsp, \quad  \loss_{t, \mrl^2}(\theta) = \mE \l[\| d^\theta_{\Tf-t}(\overrightarrow{X}_t) - (\overrightarrow{X}_0 - \overrightarrow{X}_t) \odot (\overrightarrow{X}_0 - \overrightarrow{X}_t) \|^2 \r]\eqsp,
\end{equation}
where $\odot$ is the element-wise product.

\paragraph{Cross-entropy objective $\lossCE$}
Instead of the $\loss_{\mrl^2}$ loss suggested by the conditional expectation structure, we can consider a cross-entropy loss to fit our model to the correct distribution: classical derivations from the conditional log-likelihood $ \sum_{\ell=1}^d \E \l[\log p_t^{\theta, \ell}(\fX_{\Tf - t}^{\ell} | \fX_{0}^{\ell})\r]$, where 
\begin{equation}
    p_t^{\theta, \ell}(x_{\Tf - t} | x_0) =
\begin{cases}
    d_t^{\theta, \ell}(x_{\Tf - t}) & \text{if} \ x_{\Tf - t} \neq x_{0} \\
    1 - d_t^{\theta, \ell}(x_{\Tf - t}) & \text{else}\\
\end{cases}\eqsp,
\end{equation}
lead to the following cross entropy loss:

\begin{align}
\label{eq:CE_loss}
   \lossCE(\theta) = &- \int_{0}^{\Tf} \loss_{t, \text{CE}}(\theta) \rmd t \eqsp,
\end{align}
where
\begin{equation*}
\loss_{t, \text{CE}}(\theta) = \mE \l[\sum_{l=1}^d 
    Y_t^{\ell} \log d_t^{\theta, \ell}(\fX_{\Tf - t}^{\ell})+ (1 - Y_t^{\ell}) \log\l(1 - d_t^{\theta, \ell}(\fX_{\Tf - t}^{\ell})\r)\r]\eqsp, \quad
    Y_t^{\ell} = \begin{cases}
        1 \quad \text{if} \quad \fX_0^{\ell} \neq \fX_{\Tf - t}^{\ell} \eqsp, \\ 
        0 \quad \text{else} \eqsp.
    \end{cases}
\end{equation*}


\paragraph{Further improvements} To address vanishing gradient problems, we inspect the average magnitude of the loss across the dataset, at each timestep. Indeed, the average value of $d_t^{\ell}$ is
\begin{align}
\label{eq:gamma}
    w_{\Tf - t} &= \mE \l[ d_t^{\ell}(\fX_{\Tf - t}) \r] = \mE\l[ \mE \l[ d_t^{\ell}(\fX_{\Tf - t}) \Big| \fX_0 \r] \r] \\ 
    &= \mE\l[ \mathbb{P} \l(\fX_0^\ell \neq \fX_{\Tf - t}^\ell  \Big| \fX_0)\r) \r] \\ 
    &= \frac{1}{2}\l(1 - \alpha_{\Tf - t}\r)\eqsp,
\end{align}
as given by the formulas for the transition kernels of the forward process. We can see that the value of $w_t$ is close to zero for small values of $t$, which stalls the learning process. Empirically, we find that dividing the integrand of either loss terms $\loss_{\mrl^2}$ or $\lossCE$ by $w_t$ yields improvements. As a result, we modify the losses to counterbalance their diminishing magnitude across timesteps: $\loss_{t, \rml^2}^{w} = \frac{\loss_{t, \rml^2}^{\text{denoiser}}}{w_t}$,  $\loss_{t, \text{CE}}^{w} = \frac{ \loss_{t, \text{CE}}}{w_t} $,
% \begin{equation*}
%     \loss_{t, \rml^2}^{w} = \frac{\loss_{t, \rml^2}^{\text{denoiser}}}{w_t}\eqsp, \quad \loss_{t, \text{CE}}^{w} = \frac{ \loss_{t, \text{CE}}}{w_t}\eqsp,
%     %  \quad \text{and set} \quad\lossKL^{\text{Final}} = \lossKL^{\text{DN}} 
% \end{equation*}
and define the associated losses
\begin{equation}
\label{eq:gamma_losses}
    \lossscorew(\theta) = \int_0^{\Tf} \loss_{t, \rml^2}^{w}(\theta) \rmd t \eqsp, \quad \lossCEw(\theta) = - \int_0^{\Tf} \loss_{t, \text{CE}}^{w}(\theta) \rmd t \eqsp.
\end{equation}

\paragraph{Comparing $\lossscore, \lossCE, \lossscorew, \lossCEw$}

In \Cref{fig:ce-l2-comparison}, we plot the average loss per timestep, for a trained model on MNIST (following the specifications given in \Cref{app:experiment_image})). It shows that, on average, the $\loss_{\rml}$ loss effectively becomes a scaled variant of the cross-entropy objective, which is reflected in similar performance results. This corroborates our derivation that L2 acts as an effective lower bound to the log-likelihood. This also supports its relevancy with respect to the underlying structure of this generative model. It must be noted that both losses still benefit from positive synergies when used together.

Importantly, dividing by $w_t = (1 - \alpha_t) / 2$ particularly helps at smaller timesteps, and keeps the loss values at the same magnitude across timesteps, enhancing training dynamics. This is illustrated in \cref{fig:gamma_no_gamma}, where scaling the losses with the $w$ scale factor consistently yields improvements. 


% In \Cref{fig:ce-l2-comparison}, we 

\begin{figure}[H]
\centering
\begin{minipage}{0.99\textwidth}
    \begin{minipage}{0.49\textwidth}
        \centering
        % \vspace{0pt}
        \includegraphics[width=0.9\textwidth]{experiment_results_img/loss_with_and_no_gamma.png}
        \caption{Comparison of $\lossscore, \lossCE, \lossscorew, \lossCEw$ average losses over timesteps. The two losses become scaled version of one another only when averaged over data, but otherwise benefit from positive synergies when mixed together.}
        \label{fig:ce-l2-comparison}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        % \vspace{0pt}
        \includegraphics[width=0.9\textwidth]{mnist_experiment_results_img/bar_mu_zeta_eta_gamma_vs_ema_0.99_fid_pairedTrue.png}
        \caption{FID$\downarrow$, on MNIST, for models trained with $\losslinear$ and $\losslinearw$ losses, evaluated using $200$ reverse steps with the denoise-renoise sampler. Scaling with $w$ yields consistent improvements, with the best loss configuration $\loss_{1/3, 1/3, 1/3}^w$ involving all the methodological improvements we discussed.}
        \label{fig:gamma_no_gamma}
    \end{minipage}
\end{minipage}
\end{figure}
% \begin{figure*}[ht]
% \centering
% \includegraphics[width=0.7\textwidth]{experiment_results_img/gamma_effect.png}
% \caption{Impact of enabling $w$ on the training loss at each time step. $w=\text{True}$ yields smoother training dynamics, especially early in the diffusion process.}
% \label{fig:gamma-curve}
% \end{figure*}

\paragraph{Final objective functions} We choose a linear combination of the previous loss objectives, weighted by positive coefficients $\varpi_1, \varpi_2, \varpi_3$:
\begin{equation} %\label{eq:linear_loss}
\losslinear = \varpi_1 \lossscoredenoiser + \varpi_2 \lossKL + \varpi_3 \lossCE\eqsp, 
\end{equation}
and, if we choose their version weighted by $1 / w_t$:
\begin{equation}
\label{eq:linear_loss_gamma}
\losslinearw = \varpi_1 \lossscorew + \varpi_2 \lossKL + \varpi_3 \lossCEw\eqsp.
\end{equation}

% \item \textbf{Adaptively weighted loss:} we can also consider an adaptively weighted loss, as introduced in Vector Quantized Generative Adversarial Network, and also used, e.g., in Consistency Trajectory Models. In that case, we set $\mu = 1$, and 
% \begin{equation}\label{eq:balance_parameters}
%     \zeta = \frac{\| \nabla_{\theta_L} \loss_{\mrl^2}^{w} \|}{\| \nabla_{\theta_L} \lossKL \|}, \quad 
%     \eta = \frac{\| \nabla_{\theta_L} \loss_{\mrl^2}^{w} \|}{\| \nabla_{\theta_L} \lossCEw \|}\eqsp,
% \end{equation}
% where $\theta_L$ is the last layer of the neural network at the current training step.


\begin{algorithm}[ht]
\caption{Training Algorithm for DMPM (Reparameterized Score)}\label{alg:training}
\begin{algorithmic}[1]
\REQUIRE

Dataset $\mathcal{D}$ of samples $X \in \{0,1\}^d$;  

Time horizon $\Tf > 0$ and rate $\lambda > 0$;  

Parameterized discrete denoiser model $\{ d_{t}^{\theta,\ell}(x) : \theta \in \Theta \}_{t,\ell,x}$;

Derived score function $s_{t}^{\theta} := \frac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}-\frac{4\alpha_{\Tf-t} d_t^{\theta}}{1-\alpha_{\Tf-t}^2}$ (score reparameterization \eqref{eq:discrete_score_reparameterization});

Define $\alpha_t$ as in \eqref{eq:def_alpha}, $f_t$ as in \eqref{def:f};

Loss coefficients $\varpi_1, \varpi_2, \varpi_3 \geq 0$;
% forming the combined loss $\ell_{\text{lin}}(\theta) = \mu\,\lossscoredenoiser + \zeta\,\lossKL + \eta\,\lossCE$, optionally scaled by $1/w_t$ to keep gradient magnitudes uniform over time.

\WHILE{optimization has not converged}
    \STATE Sample a batch $\{X_{i}\}_{i=1}^B$ from $\mathcal{D}$.
    \STATE Draw $t_1,\ldots,t_B \simiid \unif(\ccint{0,\Tf})$
    \STATE \textbf{Forward sampling: fast simulation via $p_{t|0} = (p_{t|0}^1)^{\otimes d}$}
    \FOR{$i = 1$ to $B$}
        \STATE $\fX_{i, 0} \gets X_i$
        \STATE $p_{\Tf - t_i} \gets (1 - \alpha_{\Tf - t_i}) / 2$
        \STATE Compute $\fX_{i, \Tf - t_i}$ by flipping each bit of $\fX_{i, 0}$ independently with probability $p_{\Tf - t_i}$
        \IF{Scaling losses with average $d_t$ magnitude} % in order to stabilize loss magnitude
            \STATE $w_i \gets (1 - \alpha_{\Tf - t_i}) / 2$
        \ELSE
            \STATE $w_i \gets 1$
        \ENDIF
    \ENDFOR
    \vspace{1em}
    \STATE  $ \lossscore(\theta) \gets \frac{1}{B}\sum_{i=1}^B \frac{1}{w_i} \| d_{t}^{\theta}(\fX_{i, \Tf - t_i}) - (\fX_{i, \Tf - t_i} - \fX_{i, 0})\odot (\fX_{i, t} - \fX_{i, 0}) \|^2$
    \STATE  $ \lossCE(\theta) \gets \frac{1}{Bd} \sum_{i=1}^B \frac{1}{w_i} \sum_{l=1}^d \l(
    \mathds{1}_{\fX_0^{\ell} \neq \fX_{\Tf - t_i}^{\ell}} \log d_{t_i}^{\theta, \ell}(\fX_{\Tf - t_i}^{\ell})
    + (1 - \mathds{1}_{\fX_0^{\ell} \neq \fX_{\Tf - t_i}^{\ell}}) \log(1 - d_{t_i}^{\theta, \ell}(\fX_{\Tf - t_i}^{\ell})
    \r)$
    \STATE  $ \lossKL(\theta) \gets \frac{1}{B} \sum_{i=1}^B\sum_{\ell=1}^d\Bigg(-s_{\Tf-t_i}^{\theta,\ell}(\foX_{t_i})
    +(f_{\Tf-t_i}^{\ell}(\foX_{t_i})-1)\log (1-s_{\Tf-t_i}^{\theta, \ell}(\foX_{t_i}) )\Bigg)$
    \STATE $\losslinear(\theta) \gets \varpi_1\,\lossscoredenoiser + \varpi_2\,\lossKL + \varpi_3\,\lossCE$
    \STATE Perform a gradient step on $\losslinear(\theta)$ w.r.t.\ $\theta$.
\ENDWHILE
\STATE \textbf{Return} the final parameter $\theta^\star$.
\end{algorithmic}
\end{algorithm}


% \begin{algorithm}[ht]
%     \caption{Training algorithm for DMPM}
%     % \textbf{Sample the forward process:}
    
% \begin{algorithmic}[1]
% \REQUIRE
%      An underlying data sample $X \in \l\{0,1\r\}^d$;
     
%     a time horizon $\Tf$;
    
%     %  a sequence of step sizes $\l\{h_k\r\}_{k=1}^M$, $N \ge 1$ associated with the time discretization $t_k=\sum_{i=1}^k h_i$ such that $t_M=\Tf$ together with a convention $t_0=0$,
%     a prescribed jump rate $\lambda$;
    
%     a parameterized vector field family $\l\{s^{\theta,\ell}: \theta \in \Theta ,~ \ell =1,\ldots,d \r\}$;
    
%     a balance parameter $\zeta$;
    
%     a number of samples $M$
    
    
%     \vspace{1ex}
% \textbf{Sample the forward process:}
% \STATE Draw the number of jump $N \sim \poissondist(\lambda \Tf)$
% \STATE Draw the jump times $(T_i)_{i\in\{1,\ldots,N\}}|N\simiid \unif(\ccint{0,\Tf})$ with $T_1 < \ldots < T_N$
% \STATE Set $T_0=0$ and initialize $\overrightarrow{X}_0=X$ 
% %\STATE $i \gets 0$ and $j \gets 0$
% \FOR{$i =0$ to $N-1$}
%     \IF{$T_i < t <T_{i+1}$}
%         \STATE Set $\foX_{t}=\foX_{T_i}$
%     \ENDIF
%         %\STATE $j \gets j+1$
%     %\IF{$T_{i+1}<\Tf$}
%     \STATE Draw $\ell_i \sim \unif(\{1,\ldots,d\})$
%     \STATE Set $\foX_{T_{i+1}}=\varphi_{\ell_i}(\foX_{T_i})$
%         %\STATE $i \gets i+1$
%     %\ENDIF
% \ENDFOR
% %\STATE Sample the forward Markov process $(\overrightarrow{X}_{t_k})_{k=1,\ldots,N}$ using the given generator matrix $\overrightarrow{Q}$ %\eqref{transitionmatrix}
% % \end{algorithmic}

% \textbf{Learn the approximate score $s^{\theta^\star}$:}

% % \begin{algorithmic}
% \STATE Draw $u_1,\ldots,u_M \simiid \unif(\ccint{0,\Tf})$
% \STATE Take gradient descent step on
% {\small
%     \begin{align*}
%         &\nabla_\theta \Bigg\{\dfrac{1}{M}\sum_{k=1}^M \sum_{\ell=1}^d\Bigg[\l| s^{\theta,\ell}_{\Tf-u_k}(\overrightarrow{X}_{u_k}) -f^\ell_{\Tf-u_k}(\overrightarrow{X}_{u_k}) \r|^2\\
%         &+\zeta\Big((-1+f_{\Tf-u_k}^{\ell}(\foX_{u_k}))\log (1-s_{\Tf-u_k}^{\theta, \ell}(\foX_{u_k}) )\\
%         &\hspace{4 cm}-s_{\Tf-u_k}^{\theta,\ell}(\foX_{u_k}) \Big) \Bigg]\Bigg\}
%         \eqsp,
%     \end{align*}
% }
%     to update $\theta^\star$,
%     where $f_t^{\ell}$ is defined in \eqref{def:f}
%     % $\alpha_t=\rme^{-2\lambda t}$ and
%     % $$f_{\Tf-u_k}^{\ell}(\foX_{u_k})=\dfrac{2\alpha_{u_k}}{1+\alpha_{u_k}}-\dfrac{4(\overrightarrow{X}_{u_k}^\ell-\overrightarrow{X}_0^\ell)^2}{1-\alpha_{u_k}^2} \quad \text{ and } $$
% \end{algorithmic}
% \textbf{Output:} $\theta^\star$
% \end{algorithm}


\subsection{Generative process and sampling procedures}
\label{app:generative_process_sampling}

Once we obtain our neural network $d_t^{\theta}$ approximating $d_t$, we use it to produce fresh samples that closely mimic the observed data. To do so, we first introduce a DMPM sampler based on the true reverse process. We then propose a slight modification, leveraging the distribution on indices available at each step, by flipping multiple bits instead of just one, using a flip-schedule. Finally, we derive a denoise-renoise sampler, solely based on the discrete denoiser structure of the problem, as inspired by similar lines of work in conitnuous diffusion. 

\paragraph{DMPM sampler} A first sampling procedure is given in \cref{alg:dmpm_sampler}. It is designed to be as close as possible to the true backward process, while enabling efficient parallelization when implemented. It consists in a piecewise-approximation of the functions of interest, parameterized by the choice of a time discretization grid $0 = t_0 < t_1 < \cdots < t_K = \Tf$, which we call a \textbf{time-schedule}. 

In Table~\ref{tab:time_schedules}, we give the different time-schedules we experiment with. We draw inspiration from numerous lines of work on continuous and discrete diffusion \citep{shi2024simplified, karras2022elucidatingdesignspacediffusionbased}, in which these are common choices.

% Now that we have trained our neural network $d_t^{\theta}$ to approximate $d_t$ via the objective function \eqref{eq:final_loss} with , we can use it to generate samples. 

\textbf{DMPM sampler with flip-schedule} In \cref{alg:dmpm_sampler_flip_schedule}, we further take advantage of the specific structure of our backward process, by leveraging the distribution over indices given by the learned score model at each timestep $t$. Instead of flipping a single bit per timestep $t_k$, we flip a total of $M_{t_k}$ bits sampled without replacements from the given distribution. We call the sequence $\{M_{t}\}_{0 \leq t \leq \Tf}$ the flip-schedule. When a time-schedule $\{t_k\}_{k=1}^K$ has been chosen, we also call the corresponding discrete sequence $\{M_{t_k}\}_{k=1}^K$ a {flip-schedule}.

In Table~\ref{tab:M_schedules}, we give the two flip-schedules we explore in this paper. The choice for the linear schedule is inspired from the philosophy of the masking schedule introduced in the context of masked diffusion by \citet{shi2024simplified}.

\begin{table}[H]
\centering
\begin{minipage}{0.49\textwidth}
    \centering 
    \scalebox{0.85}{ % Adjusts table size
        \begin{tabular}{lc}
            \toprule
            \textbf{Time-schedule} & \textbf{Value of $t_k$} \\
            \midrule
            Linear & $\Tf \frac{k}{K}$ \\
            Quadratic & $\Tf \left(\frac{k}{K}\right)^2$ \\
            Cosine & $\Tf \cos\left(\frac{(1 - k / K) \pi}{2} \right)$ \\
            \bottomrule
        \end{tabular}
    }
    \caption{Different time schedules $(t_k)_{k=1}^K$ used in our experiments. $\Tf$ denotes the final time, and $K$ is the number of reverse steps.}
    \label{tab:time_schedules}
\end{minipage}
\hfill
\begin{minipage}{0.49\textwidth}
    \centering
    \scalebox{0.85}{    
        \begin{tabular}{lc}
            \toprule
            \textbf{Flip-schedule} & \textbf{Value of $M_t$} \\
            \midrule
            Constant & $M$ \\
            Linear & $M \frac{t}{\Tf}$ \\
            \bottomrule
        \end{tabular}
    }
    \caption{Different flip schedules $(M_t)_{0 \leq t \leq \Tf}$ used in our experiments. In both schedules, $M$ is a constant to be fixed and controls the total number of bits flipped during generation.}
    \label{tab:M_schedules}
\end{minipage}
\end{table}


\paragraph{Denoise-renoise sampler} In \Cref{alg:dmpm_dennoise_cycling}, we introduce the following denoise/renoise cycle, interpreting the model output $d^\theta_t$ as the probability that each bit should be flipped at timestep $t$ to reach timestep $0$. After doing a full denoise pass (from time $\Tf\to 0$), we noise the sample with the transition kernel of the forward process (from time $0 \to \Tf - \Delta$). Then we can do another denoise pass from $(\Tf - \Delta)\to 0$, etc.

% In \Cref{alg:dmpm_dennoise_deterministic}, we derive a deterministic procedure, based on a threshold schedule in order to bit flips or not. The threshold schedule can be constant (like $0.9$), or gradually adjusted across timesteps.

% \begin{algorithm}[H]
% \caption{Deterministic Threshold-Based Flips}
% \label{alg:dmpm_dennoise_deterministic}
% \begin{algorithmic}[1]

% \REQUIRE 
%   Final time $\Tf>0$; 
%   number of intervals $K$ with partition $0=t_0 < t_1<\cdots <t_K=\Tf$;
%   discrete denoiser $d^\theta$;
%   threshold schedule $\{\tau_{t_k}\}_{k=0}^K$ with each $\tau_{t_k} \in [0,1]$.

% \vspace{1ex}
% \STATE $X_{t_0}^\theta \sim \mathrm{Unif}(0,1)^{\otimes d}$

% \FOR{$k=0$ \textbf{to} $K-1$}
%   \STATE $d_{t_k} \;\gets\; d_{t_k}^\theta\bigl(\iXtheta_{t_k} \bigr)$
%   \FOR{$l = 1$ to $d$}
%     \IF{$d_{t_k}^l > \tau_{t_k}$}
%       \STATE $\iX_{t_k}^{\theta, l} \;\gets\; 1 - \iX_{t_k}^{\theta, l}$
%     \ENDIF
%   \ENDFOR
%   \STATE $\iXtheta_{t_{k+1}} \iXtheta_{t_{k}}$
% \ENDFOR

% \textbf{Output:}  $\iXtheta_{\Tf}$  %\COMMENT{final deterministic sample}

% \end{algorithmic}
% \end{algorithm}



% \begin{algorithm}[H]
% \caption{Backward sampling of DMPM with piecewise-constant score}
% % \label{alg:dmpm_dennoise}
% \begin{algorithmic}[1]
% \STATE $\iXtheta_0 \sim \mathrm{Unif}(0,1)^{\otimes d}$ % \COMMENT{initialize in $\{0,1\}^d$}
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}[ht]
%     \caption{DMPMs Algorithm in discrete time scheme}\label{alg:dmpm_sampler}
%     % \textbf{Input:} 
% \begin{algorithmic}

% \REQUIRE a time horizon $\Tf$, a partition $\{t_k\}_{k=0}^K$ associated with step-sizes $\{h_k\}_{k=1}^K$, $t_{k} = \sum_{i=1}^K h_i$ such that $t_0= 0$ and $t_K= \Tf$, a prescribed jump rate $\lambda$, and an approximate score function $s^{\theta^\star}$
% \STATE \textbf{Backward process:}
% \STATE Set $T_0=0$ and initialize $\overleftarrow{X}^\star_0 \sim \gamma^d$ 
% % \STATE Draw $(E_i)_{i\in\nset}\simiid \text{Exp}(1)$
% \STATE $i \gets 0$ 
% \WHILE{$T_i\leq \Tf$}
%     \STATE Draw $E_i\sim \text{Exp}(1)$
%     \STATE Solve $T_{i+1}  = \inf \{t_k > T_i, k =0,\ldots,K \, :\, \sum_{j=m}^{k} (t_{j+1}-t_j)\hlambda^{\theta^\star}_{t_j} (\baX^\star_{T_i}) \geq E_i \}$ with convention $\inf\emptyset =\infty$, where $m \in \l\{0,\ldots,K\r\}$ such that $T_i=t_m$
%     % , and $\blambda_t(x)=\lambda\sum_{\ell=1}^d(1-s^{\theta^\star,\ell}_{t}(x))$
%     % \inf \{t \geq 0\, :\,  \lambda\sum_{\ell=1}^d(1-s^{\theta^\star,\ell}_{T_i}(\baX^\star_{T_i})) \geq E_i \}$
%     \FOR {$k=1,\ldots, K$}
%         \IF{$T_i < t_k <\min(T_{i+1},\Tf)$}
%             \STATE Set $\baX^\star_{t_k}=\baX^\star_{T_i}$
%         \ENDIF
%     \ENDFOR
%     \IF{$T_{i+1}<\Tf$}
%         \STATE Draw $\ell_i \sim \categorial(\{  \hk^{\theta^\star}_{T_{i+1}}(\baX^\star_{T_i},\varphi^{(\ell)}(\baX^\star_{T_i}))
%         \}_{\ell=1}^d)$
%         \STATE Set $\baX^\star_{T_{i+1}}=\varphi^{(\ell_i)}(\baX^\star_{T_i})$
%     \ENDIF
%      \STATE $i \gets i+1$
% \ENDWHILE

% %\STATE Sample the backward process $(\overleftarrow{X}^\star_{t_k} )_{k=1,\ldots,N}$ , using the parametrized score $s^{\theta^\star}$
% \end{algorithmic}
% \textbf{Output:} $\overleftarrow{X}_{\Tf}^\star$
% \end{algorithm}



\begin{algorithm}[H]
\caption{Backward sampling of DMPM with piecewise-constant score}
\label{alg:dmpm_sampler}
\begin{algorithmic}[1]

\REQUIRE 

  Time horizon $\Tf>0$ and rate $\lambda>0$; 
  
  $K>0$ number of reverse steps and time-schedule $0=t_0 < t_1<\cdots <t_K=\Tf$;
  
  Flip-schedule, i.e., sequence of positive integers $\{M_{t_k}\}_{k=1}^{K}$;
  
  Discrete denoiser model $d^{\theta}$;
  
  Derived score function $s_{t}^{\theta} := \frac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}-\frac{4\alpha_{\Tf-t} d_t^{\theta}}{1-\alpha_{\Tf-t}^2}$ (score reparameterization \eqref{eq:discrete_score_reparameterization});

  Define $\alpha_t$ as in \eqref{eq:def_alpha};
  
\vspace{1ex}
\STATE $\iXtheta_0 \sim \mathrm{Unif}(0,1)^{\otimes d}$ % \COMMENT{initialize in $\{0,1\}^d$}
\STATE $E \sim \mathcal{E}(1)$  %\COMMENT{exponential(1) random variable}
\STATE $\Lambda \gets 0$   %\COMMENT{Rate integral}

\FOR{$k = 0$ to $K-1$} 
  \STATE $\overline{\lambda}_{t_k} \gets \lambda \sum_{l=1}^d \bigl(1 - s^{\theta, \ell}_{t_k}\bigr)$  %\COMMENT{Compute rate}
  \STATE $\Delta t_{k} \gets t_{k+1}-t_k$ 
  \STATE $\Lambda \gets \Lambda + \overline{\lambda}_{t_k}\,\Delta t_{k}$  %\COMMENT{Piecewise constant approximation of the rate integral}

  \IF{ $\Lambda > E$ }
    \STATE $\ell^\star 
       \;\sim\; \mathrm{Cate]} \ \!\Bigl(\Bigl\{\dfrac{\lambda \,\bigl(1 - s^{\theta,l}_{t_k}\bigr)}{\overline{\lambda}_{t_k}}\Bigr\}_{l=1}^d \Bigr)$  %\COMMENT{sometimes known as multinomial. Number of components to flip}
    
    \STATE $\iX_{t_k}^{\theta, l^\star} \gets 1 - \iX_{t_k}^{\theta, l^\star}$  %\COMMENT{bit flip} 
    
    \STATE $\Lambda \gets 0$  %\COMMENT{Reset integral}
    \STATE $E \sim \mathcal{E}(1)$
  \ENDIF
\vspace{1ex}
\STATE $\iXtheta_{t_{k+1}} \gets \iXtheta_{t_{k}}$ % \COMMENT{Update state}

\ENDFOR

\textbf{Output:} $\iXtheta_{\Tf}$ % \COMMENT{Final state}

\end{algorithmic}
\end{algorithm}





\begin{algorithm}[H]
\caption{Backward sampling of DMPM with piecewise-constant score and flip-schedule}
\label{alg:dmpm_sampler_flip_schedule}
\begin{algorithmic}[1]

\REQUIRE 

  Time horizon $\Tf>0$ and rate $\lambda>0$; 
  
  $K>0$ number of reverse steps and time-schedule $0=t_0 < t_1<\cdots <t_K=\Tf$;
  
  Flip-schedule, i.e., sequence of positive integers $\{M_{t_k}\}_{k=0}^{K}$;
  
  Discrete denoiser model $d^{\theta}$;
  
  Derived score function $s_{t}^{\theta} := \frac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}-\frac{4\alpha_{\Tf-t} d_t^{\theta}}{1-\alpha_{\Tf-t}^2}$ (score reparameterization \eqref{eq:discrete_score_reparameterization});

  Define $\alpha_t$ as in \eqref{eq:def_alpha};
  
\vspace{1ex}
\STATE $\iXtheta_0 \sim \mathrm{Unif}(0,1)^{\otimes d}$ % \COMMENT{initialize in $\{0,1\}^d$}
\STATE $E \sim \mathcal{E}(1)$  %\COMMENT{exponential(1) random variable}
\STATE $\Lambda \gets 0$   %\COMMENT{Rate integral}

\FOR{$k = 0$ to $K-1$} 
  \STATE $\overline{\lambda}_{t_k} \gets \lambda \sum_{l=1}^d \bigl(1 - s^{\theta, \ell}_{t_k}\bigr)$  %\COMMENT{Compute rate}
  \STATE $\Delta t_{k} \gets t_{k+1}-t_k$ 
  \STATE $\Lambda \gets \Lambda + \overline{\lambda}_{t_k}\,\Delta t_{k}$  %\COMMENT{Piecewise constant approximation of the rate integral}

  \IF{ $\Lambda > E$ }
    \STATE $\displaystyle [\,\ell_1^\star,\dots,\ell_M^\star\,] 
       \;\sim\; \mathrm{Hypergeometric} \ \!\Bigl(\Bigl\{\dfrac{\lambda \,\bigl(1 - s^{\theta,l}_{t_k}\bigr)}{\overline{\lambda}_{t_k}}\Bigr\}_{l=1}^d,\ M_{t_{k}} \Bigr)$  %\COMMENT{sometimes known as multinomial. Number of components to flip}
    \FOR{$i=1$ to $M_{t_{k}}$}
      \STATE $\iX_{t_k}^{\theta, l_i^\star} \gets 1 - \iX_{t_k}^{\theta, l_i^\star}$  %\COMMENT{bit flip}
      
    \ENDFOR
    \STATE $\Lambda \gets 0$  %\COMMENT{Reset integral}
    \STATE $E \sim \mathcal{E}(1)$
  \ENDIF
\vspace{1ex}
\STATE $\iXtheta_{t_{k+1}} \gets \iXtheta_{t_{k}}$ % \COMMENT{Update state}

\ENDFOR

\textbf{Output:} $\iXtheta_{\Tf}$ % \COMMENT{Final state}

\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Denoise--Noise Cycling with a Discrete Denoiser Model}
\label{alg:dmpm_dennoise_cycling}
\begin{algorithmic}[1]

\REQUIRE 

  Time horizon $\Tf>0$ and rate $\lambda>0$; 
  
  $K>0$ number of reverse steps and time-schedule $0=t_0 < t_1<\cdots <t_K=\Tf$;
  
  Discrete denoiser model $d^{\theta}$;  

\vspace{1ex}
\STATE $\iXtheta_{0} \sim \mathrm{Unif}(0,1)^{\otimes d}$  \COMMENT{initial sample in $\{0,1\}^d$}

\FOR{$k = 0 $ to $K-1$}
  %\COMMENT{Repeat denoise--noise cycles}
  \STATE \textbf{Denoise phase:}
       \STATE $d_{t_k} \gets d^\theta_{t_k}\bigl(\iXtheta_{t_k}\bigr)$  %\COMMENT{predict flipping probabilities}
       \STATE Compute $\iXtheta_{\Tf}$ by flipping each component $l$ of $\iX_{t_k}^{\theta}$ with probability $d_{t_k}^l$
       \vspace{1ex}
  \STATE \textbf{Noise phase:}
    \STATE Sample $\iXtheta_{t_{k+1}} \sim p_{\Tf - t_{k+1} | 0}(\cdot | \iXtheta_{\Tf})$, as in \cref{alg:training}  %\COMMENT{Renoised state}
\ENDFOR

\textbf{Output:}  $\iXtheta_{\Tf}$  %\COMMENT{Final denoised sample}

\end{algorithmic}
\end{algorithm}



% \subsection{Algorithms}\label{pseudo_code}
% We provide the following pseudo-code of estimating the score function:


% The objective of finding the approximate score function above comes from the fact that
% \begin{align}\label{eq:objective}
%     &\argmin_{\theta \in \Theta} \E\l[\sum_{\ell=1}^d(1-s_{\Tf-t}^{\theta,\ell})h\parenthese{\frac{1-s_{\Tf-t}^{\ell}}{1-s_{\Tf-t}^{\theta,\ell}}}(\foX_{t}) \r]\nonumber\\
%     =&\argmin_{\theta \in \Theta} \E\l[\sum_{\ell=1}^d(-1+s_{\Tf-t}^{\ell}(\foX_t))\log (1-s_{\Tf-t}^{\theta, \ell}(\foX_t) )-s_{\Tf-t}^{\theta,\ell}(\foX_t) \r] \eqsp.
% \end{align}
% Here we used the definition of the function $h$ and removed all the terms independent of $\theta$. Finally, plugging the conditional expectation formula of $s^{\ell}_{\Tf-t}(\foX_t)$ into \eqref{eq:objective}, we obtain the second quantity of the desired objective 
% \begin{equation*}
%      \argmin_{\theta \in \Theta} \E\l[\sum_{\ell=1}^d(-1+f_{\Tf-t}^{\ell}(\foX_t))\log (1-s_{\Tf-t}^{\theta, \ell}(\foX_t) )-s_{\Tf-t}^{\theta,\ell}(\foX_t) \r] \eqsp,
% \end{equation*}
% with $f_{\Tf-t}^{\ell}(\foX_{t})=\dfrac{2\alpha_{t}}{1+\alpha_{t}}-\dfrac{4(\overrightarrow{X}_{t}^\ell-\overrightarrow{X}_0^\ell)^2}{1-\alpha_{t}^2}$, where $\alpha_t=\rme^{-2\lambda t}$.

% {\color{purple}
%     \begin{algorithm}
%         \caption{Score Approximation (Discrete time scheme)}
%          \textbf{Input:} An underlying data sample $X \in \l\{0,1\r\}^d$, a time horizon $\Tf \gg 1$ large enough, a sequence of step sizes $\l\{h_k\r\}_{k=1}^M$, $N \ge 1$ associated with the time discretization $t_k=\sum_{i=1}^k h_i$ such that $t_M=\Tf$ together with a convention $t_0=0$, a prescribed jump rate $\lambda$, a parameterized vector field family $\l\{s^{\theta,\ell}: \theta \in \Theta ,~ \ell =1,\ldots,d \r\}$, a balance parameter $\zeta$
         
%          \textbf{Sample the forward process:}
%     \begin{algorithmic}
%     \STATE Draw $k\sim\unif(\{1,\ldots,M\})$
%     \STATE Draw the number of jump $N \sim \poissondist(\lambda t_k)$
    
%     % \STATE Draw the jump times $(T_i)_{i\in\{1,\ldots,N\}}|N\simiid \unif(\ccint{0,t_k})$ with $T_1 < \ldots < T_N$
%     \STATE Draw the jump times $(\ell_i)_{i\in\{1,\ldots,N\}}|N\simiid \unif(\{1,\ldots,M\})$
%     \STATE Initialize $Y_0=X$
%     %\STATE $i \gets 0$ and $j \gets 0$
%     \FOR{$i =0,\ldots, N-1$}
%         % \FOR{$j=1, \ldots, M$}
%         %     \IF{$T_i < t_j <T_{i+1}$}
%         %         \STATE Set $\foX_{t_j}=\foX_{T_i}$
%         %     \ENDIF
%         %     %\STATE $j \gets j+1$
%         % \ENDFOR
%         %\IF{$T_{i+1}<\Tf$}
%             % \STATE Draw $\ell_i \sim \unif(\{1,\ldots,d\})$
%             \STATE Set $Y_{i+1}=\varphi_{\ell_i}(Y_{i})$
%             %\STATE $i \gets i+1$
%         %\ENDIF
%     \ENDFOR
%     \STATE Set $\overrightarrow{X}_{t_k}=Y_{N}$
%     %\STATE Sample the forward Markov process $(\overrightarrow{X}_{t_k})_{k=1,\ldots,N}$ using the given generator matrix $\overrightarrow{Q}$ %\eqref{transitionmatrix}
%     \STATE Take gradient descent step on
%      \begin{align*}
%         &\nabla_\theta \Bigg\{\sum_{\ell=1}^d\Bigg[\l| s^{\theta,\ell}_{\Tf-t_k}(\overrightarrow{X}_{t_k}) -f^\ell_{\Tf-t_k}(\overrightarrow{X}_{t_k}) \r|^2\\
%         &\hspace{3.8cm}+\zeta(-1+f_{\Tf-t_k}^{\ell}(\foX_{t_k}))\log (1-s_{\Tf-t_k}^{\theta, \ell}(\foX_{t_k}) )-s_{\Tf-t_k}^{\theta,\ell}(\foX_{t_k})  \Bigg]\Bigg\}
%         \eqsp,
%     \end{align*}
%     to update $\theta^\star$,
%     with $f_{\Tf-t_k}^{\ell}(\foX_{t_k})=\dfrac{2\alpha_{t_k}}{1+\alpha_{t_k}}-\dfrac{4(\overrightarrow{X}_{t_k}^\ell-\overrightarrow{X}_0^\ell)^2}{1-\alpha_{t_k}^2}$, where $\alpha_t=\rme^{-2\lambda t}$
%     \end{algorithmic}
    
    
%     % \textbf{Learn the approximate score $s^{\theta^\star}$:  }
%     % \begin{algorithmic}
%     % \STATE Solve the following minimization 
%     % \begin{equation*}
%     %     \theta^\star=\argmin_{\theta \in \Theta} \sum_{k=0}^M \sum_{\ell=1}^d\l[\l| s^{\theta,\ell}_{\Tf-t_k}(\overrightarrow{X}_{t_k}) -s^\ell_{\Tf-t_k}(\overrightarrow{X}_{t_k}) \r|^2+\zeta(1-s_{\Tf-t_k}^{\theta,\ell})h\parenthese{\frac{1-s_{\Tf-t_k}^{\ell}}{1-s_{\Tf-t_k}^{\theta,\ell}}}(\foX_{t_k}) \r]
%     % \end{equation*}
%     % with $s_{\Tf-t_k}^{\ell}(\foX_{t_k})=\dfrac{2\alpha_{t_k}}{1+\alpha_{t_k}}-\dfrac{4(\overrightarrow{X}_{t_k}^\ell-\overrightarrow{X}_0^\ell)^2}{1-\alpha_{t_k}^2}$, where $\alpha_t=e^{-2\lambda t}$ 
%     % \end{algorithmic}
%     \textbf{Output:} $\theta^\star$
%     \end{algorithm}
% }
% \np{how to simulate the real score?}



% Once we achieve the approximate score, we can construct an algorithm to produce fresh samples that closely mimic the observed data. The pseudo-code of backward sampling is as follows:
% % \np{modify}
% \begin{algorithm} 
%     \caption{DMPMs Algorithm (Continuous time scheme)}\label{alg:backward_approximation_continuous}
%     \textbf{Input:} a time horizon $\Tf \gg 1$ large enough, a prescribed jump rate $\lambda$, an approximate score function $s^{\theta^\star}$
% \begin{algorithmic}
% \STATE \textbf{Backward process:}
% \STATE Set $T_0=0$ and initialize $\overleftarrow{X}_0 \sim \gamma^d$ 
% \STATE $i \gets 0$ 
% \WHILE{$T_i\leq \Tf$}
%     \STATE Draw $E_i\sim \text{Exp}(1)$
%     \STATE Solve $\Delta T_{i+1} = \inf \{t \geq 0\, :\, \int_0^t \lambda^{\theta^\star}_{T_i+r}(\baX_{T_i})\rmd r \geq E_i \} $, with $\lambda^{\theta^\star}_t(x)=\lambda\sum_{\ell=1}^d(1-s^{\theta^\star,\ell}_{t}(x))$
%     \STATE Set $T_{i+1}=T_i+\Delta T_{i+1}$
%     % \FOR {$j=1,\ldots, M$}
%     \IF{$T_i < t <\min(T_{i+1},\Tf)$}
%         \STATE Set $\baX_{t}=\baX_{T_i}$
%     \ENDIF
%     % \ENDFOR
%     \IF{$T_{i+1}<\Tf$}
%         \STATE Draw $\ell_i\in\{1,\ldots,d\}$ which is distributed according to $\categorial(\{  \lambda(1- s_{T_{i+1}}^{\theta^\star,\ell}(\baX_{T_i})) / \lambda^{\theta^\star}_{T_{i+1}}(\baX_{T_i}) \}_{\ell=1}^d)$
%         \STATE Set $\baX_{T_{i+1}}=\varphi^{(\ell_i)}(\baX_{T_i})$
%     \ENDIF
%      \STATE $i \gets i+1$
% \ENDWHILE

% %\STATE Sample the backward process $(\overleftarrow{X}^\star_{t_k} )_{k=1,\ldots,N}$ , using the parametrized score $s^{\theta^\star}$
% \end{algorithmic}
% \textbf{Output:} $\overleftarrow{X}_{\Tf}$
% \end{algorithm}


\newpage

\section{Experiments}

\label{app:experiment}

All experiments are conducted using PyTorch. All the training and experiments are conducted on four NVIDIA RTX8000 GPU.

We use the score parameterization introduced in \eqref{eq:score_reparameterization}: 
\begin{equation}
    s_{t}^{\theta, \ell}(x)= \dfrac{2\alpha_{\Tf-t}}{1+\alpha_{\Tf-t}}-\dfrac{4\alpha_{\Tf-t} d_t^{\theta, \ell}(x)}{1-\alpha_{\Tf-t}^2}\eqsp,
\end{equation}
where the neural network $d_t^{\theta, \ell}(x)$ aims to approximate $d_t^{\ell}(x) = \mathbb{P} (\fX_0^\ell \neq x^\ell \big| \fX_{\Tf - t} = x)$. Since the output of the neural network is $d_t^{\theta}(x) \in (0, 1)^d$, we add a sigmoid activation function at the last layer.

We consider various loss configurations $\losslinear, \losslinearw$ as introduced in \eqref{eq:linear_loss}, \eqref{eq:linear_loss_gamma}, with $6$ choices of coefficients $(\varpi_1, \varpi_2, \varpi_3)$ normalized in the 2-simplex $\Delta_2 \subset \R^3$. We test all $2^3 - 1 = 7$ possible non-empty combinations, minus the single $\lossKL$ loss combination ($\varpi_2 = 1$), as the latter only acts as entropic regularization and does not perform well by itself. This lets us study the synergies between the different loss terms. 

\subsection{Small dimension data}
\label{app:experiment_small}

We first conduct experiments on a discrete data distribution $p$ supported on $\{0,1\}^d$. Each component of $X = (X_i)_{i=1}^d \sim p$ is independently distributed as a Bernoulli distribution with parameter $p_i$:
\begin{equation}
    p(x) = \prod_{i = 1}^d p_i(x_i) \eqsp,
\end{equation}
where the map $i \mapsto p_i$ forms a sawtooth-like pattern, oscillating linearly between $0.05$ and $0.95$, as can be seen in \cref{fig:sawtooth_pattern}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.3\linewidth]{other_img/saws_1d.png}
    \caption{Sawtooth pattern used to define $i \mapsto p_i$, plotted with $d = 16$. Values oscillate linearly from $0.05$ to $0.95$, and back.}
    \label{fig:sawtooth_pattern}
\end{figure}


For training, we use $20\,000$ datapoints resampled at each epoch, and a batch size of $1024$. We train each model for $300$ epochs, using AdamW with a learning rate of 1e-3.
We employ a network composed of multiple MLP blocks: $4$ residual blocks, each consisting of two feed-forward layers of width $256$; layer normalization and SiLU activations in each block; a feed-forward embedding for the timesteps, mapping $\mathbb{R}$ to a hidden dimension of $256$, whose output is then injected into each residual block by an additional MLP of dimension $256\times 256$.

For evaluation, we estimate each distribution with $20{,}000$ samples, and draw $1000$ vectors uniformly on the simplex $\Delta_{d}$ to compute our $\text{SWD}$ metric (see \cref{app:experiment_metric}).

\subsection{Image data}
\label{app:experiment_image}

We work on the binarized MNIST dataset, which we scale from $28\times28$ to $32\times32$ in order to fit in the U-Net architecture. We set the pixel value to $0$ if its intensity is below $0.5$, and to $1$ otherwise.

We compare DMPM to MD4 (masked diffusion, as in \citet{shi2024simplified}) and DFM (discrete flow matching, as in \citet{gat2024discrete}). We reimplement MD4 with the cosine schedule and the algorithms given in Appendix F of \citet{shi2024simplified}. We implement DFM based on the Pytorch implementation in \url{https://github.com/gle-bellier/discrete-fm}, and we use corrector sampling for better results. 

For DMPM, we are using the cosine time-schedule and time horizon $\Tf = 3$. For both MD4 and DFM, we set the mask value to the integer $2$.

To establish a fair comparison, we use the same network model for every method. We use a U-Net following the implementation of \cite{nichol2021improved} available in \url{https://github.com/openai/improved-diffusion}. We dimension the network as follows.

The first layer is an embedding layer of output dimension $32$ and input dimension $d_{\text{input}}$, where $d_{\text{input}} = 2$ for DMPM (input values are either $0$ and $1$) and $d_{\text{input}} = 3$ for MD4 and DFM (input values are either $0, 1$ or the mask value $2$).

We set the hidden layers to $[128, 256, 256, 256]$, fix the number of residual blocks to $2$ at each level, and add self-attention block at resolution $16\times16$, using 4 heads. We use an exponential moving average with a rate of $0.99$. We use the silu activation function at every layer. Timestep $t$ is fed to the model through the Transformer sinusoidal position embedding.

For DMPM and MD4, we set the number of output channels to $1$ and add a sigmoid activation at the last layer. For DFM, we set the output channels to $3$ and apply softmax channel-wise.

The optimizer is AdamW with learning rate 5e-4. We use the StepLR scheduler which scales the learning rate by $\gamma= .99$ every $400$ steps. We train on MNIST for $120\,000$ steps with batch size $256$. A single training run on MNIST takes approximately 6 hours per GPU, and requires about 6-12GB of VRAM for our settings.

To assess the quality of our generative models, we compute our metrics between $4\,000$ real images and $4\,000$ generated images. Generating $4\,000$ images with $1\,000$ reverse steps takes approximately $2$ hours on one GPU.

\subsection{Metrics}
\label{app:experiment_metric}

For low-dimensional data, we use a custom sliced Wasserstein metric. For image data, in addition to the classical FID metric, we use a $\text{F}_1^{\text{DC}}$ summary score, based on the density and coverage metrics.

\paragraph{$\text{F}_1^{\text{DC}}$ as summary metric of density-coverage} The density and coverage metrics are introduced in the setting of generative models by \cite{naeem2020reliablefidelitydiversitymetrics}. They assess the overlap of sample distributions using local geometric structures. Density measures how much the generated distribution is contained in the original data distribution (measuring quality), and coverage measures how much of the original data distribution is covered by the generated distribution (diversity).

These metrics are improvements of the precision and recall metrics for generative models \citep{2019improvedprecisionrecallmetric}. They offer different measures to characterize the performance of generative models. For instance they can decorrelate the negative effect of mode collapse from the negative effect of noisy/blurry generations, each of them decreasing respectively coverage and density, and have been of importance in recent studies, e.g., in heavy-tailed generative modeling \citep{shariatian2024denoisinglevyprobabilisticmodels, yoon2023lim}.

We consider a single summary $\text{F}_1^{\text{DC}}$ score, which we define as the harmonic mean of these two values:
\begin{equation}
    \text{F}_1^{\text{DC}} = 2\cdot \dfrac{\text{density}\cdot \text{coverage}}{\text{density} + \text{coverage}}\eqsp.
\end{equation}

\paragraph{Sliced Wasserstein metric $\text{SWD}$}

Since the state space of our dataset over $\{0, 1\}^d$ is of size $2^d$, we cannot work with histogram-based metrics, which would require exponentially many samples when $d$ increases.

We address this issue with our sliced Wasserstein metric $\text{SWD}$. This metric is defined between distributions $\mu, \nu$ on $\{0, 1\}^d$ as:
\begin{equation}
    \text{SWD}(\mu, \nu) = \int_{\Delta_{d}} \text{W}\left( u_{\#}\mu,  u_{\#}\nu \right) \text{d} u\eqsp,
\end{equation}
where, for $u \in \Delta_{d}$, the pushforward $u_{\#}$ is derived from the function
\begin{equation}
    x \in \{0, 1\}^d \mapsto \langle u, x \rangle \in [0, 1] \eqsp.
\end{equation}
Simple Monte-Carlo averages are used to evaluate the integral with respect to the uniform distribution over the simplex $\Delta_{d}$, and we compute the Wasserstein distance between the pushforward measures with the \texttt{pyemd} package \citep{pyemd}. 

% \begin{figure*}[ht]
% \centering
% \includegraphics[width=0.75\textwidth]{experiment_results_img/mode_collapse_hist.png}
% \caption{Illustration of mode collapse for large reverse steps: histograms of generated data with $d=16$.}
% \label{fig:mode-collapse-hist}
% \end{figure*}


\section{Additional results}

In this section, we give grid images of generated samples for DMPM models trained on binarized MNIST, with the loss $\loss_{1/3, 1/3, 1/3}^{w}$.

\begin{figure}[ht]
    \centering
    \begin{minipage}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{mnist_experiment_results_img/dmpm_default.png}
        \caption{Default DMPM sampler, 25 reverse steps, cosine time-schedule, linear flip-schedule dimensioned for $1000$ total bit flips.}
        \label{fig:dmpm_default_grid}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{mnist_experiment_results_img/dmpm_denoise_renoise.png}
        \caption{Denoise-renoise sampler, 200 reverse steps, cosine time-schedule.}
        \label{fig:dmpm_denoise_renoise_grid}
    \end{minipage}
\end{figure}

\newpage

\section{Convergence of DMPMs}
\label{appendix:convergence}
The proof of DMPMs' convergence requires understanding the backward dynamic under the canonical process point of view equivalent to the transition matrix point of view we provided in \cref{matrix_POV}. 
\subsection{Canonical process point of view}
We now want to give a description of the time reversal process as the solution of an optimal control process like in the continuous setting in \citet{conforti2024klconvergenceguaranteesscore}. To this purpose, we consider the following canonical setting. Let $\Omega = \D(\ccint{0,\Tf};\msx)$ be the canonical space of all càdlàg (right continuous and left limited) paths from $\ccint{0,\Tf}$ to $\msx$. With abuse of notation, we denote as  $(\overrightarrow{X}_t)_{t\in \ccint{0,\Tf}}$ the canonical process defined by
\begin{align*}
    \overrightarrow{X}_t(\omega) = \omega_t, \quad \text{ for }t\in \ccint{0,\Tf},(\omega_s)_{s\in \ccint{0,\Tf}}\in \Omega\eqsp.
\end{align*}
Denote by $\mu_t=\mathrm{Law}(X_t)$ the law of $\overrightarrow{X}_t$, $\Delta \overrightarrow{X}_t = \overrightarrow{X}_t - \overrightarrow{X}_{t-}$ the size of jump at time $t$ for $t\in \ccint{0,\Tf}$. Consider one-hot vectors $(\rme_i)_{i=1}^d$. Denote by $\Qc_d$ the set of all effective jumps, $i.e.$
$$\Qc_d=\l\{(-1)^p\rme_i \quad\text{for } i=1,...,d \text{ and } p=0,1 \r\}\eqsp.$$
Denote $|q|:=(\big|q^j\big|)^d_{j=1}$ for $ q\in \Qc_d$.
Endow $\Omega$ with the $\sigma$-field $\sigma(X_t;t\in\ccint{0,\Tf})$ generated by the canonical projections and consider the associated canonical filtration. 
 We denote the set 
 $$\Ac:=\l\{(x,q)\in \msx \times \Qc_d \text{ such that } (x+q)\in \msx\r\}\eqsp, $$
 %(|q|^Tx,|q|^Tq) \in \l\{ (0,1),(1,-1) \r\}\r\}\eqsp,$$ 
 and the rate of jump $\varepsilon_t$ by
\begin{align*}
    \varepsilon_t(\foX_{t-},\foX_{t-}+q):&=\begin{cases}
        1\eqsp, \quad &\text{if } (\foX_{t-},q )\in \Ac\eqsp,\\
        0 \eqsp, \quad &\hspace{1cm}\text{else}\eqsp.
    \end{cases}
\end{align*}
 We then denote the kernel $\bar L$ of the canonical process $(\foX_t)_{t\in \ccint{0,\Tf}}$ as

\begin{align*}
    \bar L_\omega(\rmd q\rmd t):&=\rmd t\sum_{i=1}^d( \1_{\l\{\foX_{t-}^i=0\r\}}\varepsilon_t(\foX_{t-},\foX_{t-}+q)\delta_{\l\{\rme_i\r\}}(\rmd q)+\1_{\l\{\foX_{t-}^i=1\r\}}\varepsilon_t(\foX_{t-},\foX_{t-}+q)\delta_{\l\{-\rme_i\r\}}(\rmd q))\\
     &=\rmd t\sum_{i=1}^d( \1_{\l\{\foX_{t-}^i=0\r\}}\delta_{\l\{\rme_i\r\}}(\rmd q)+\1_{\l\{\foX_{t-}^i=1\r\}}\delta_{\l\{-\rme_i\r\}}(\rmd q))\\
     &=:L_\omega (t,\rmd q)\rmd t \eqsp.
\end{align*}

We now follow the approach for time reversal used in \citet{leonard2012girsanov} to characterize the evolution of $X$ with the use of the following martingale problem.


\begin{definition}[Martingale problem]
    We say that $\P\in \Pc(\Omega)$ solves \emph{the martingale problem} $MP(\bar L)$, and write $\P \in MP(\bar L)$, if the integrability assumption
    $$\mathbb E_{\P}\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}(|q|^2\wedge 1)\bar L(\rmd q\rmd t)<\infty$$
    holds, and if the process
    \begin{align*}
        \sum_{ x \in \msx}f( x)\mu_t(x)-\sum_{x\in \msx}f( x)\mu_0( x)-\int_0^t \sum_{x\in \msx}\sum_{q\in \Qc_d} (f( x+q)-f(x) )\mu_{s-}(x)\bar L(\rmd s\rmd q)
    \end{align*}
    is a local $\P$-martingale for any function $f: \msx\to \R$, where $\mu_t$ is the marginal distribution of $\foX_t$.
\end{definition}
Note that $\P \in MP(\bar L)$ is equivalent to  $\P \in \Pc(\Omega)$ such that 
    \begin{align*}
        \mathbb E_{\P} \sum_{0\leq s\leq t: \Delta \foX_s \neq 0} g(s,\Delta \foX_s)=\mathbb E_{\P} \int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d} g(s,q)\bar L (\rmd q\rmd s)\eqsp,
    \end{align*}
    for all measurable functions $g$. 

    \begin{definition}[Condition (U)]\cite{leonard2012girsanov}
        One says that $\P \in MP(\bar L)$  satisfies the \emph{uniqueness condition (U)} if for any probability measure $\P'$ on $\Omega$ such that the initial laws $\P'_0=\P_0$ are equal, $\P'\ll \P$ and $\P'\in MP(\bar L)$, we have $\P=\P'$.
    \end{definition}
    For the sake of this paper, let $\overrightarrow{R}$ be an invariant probability measure on $\Omega$ that fulfills the uniqueness condition (U) , and solves the martingale problem $ MP(\bar L)$ w.r.t. the canonical process $(\foX_t)_{t\in \ccint{0,\Tf}}$. 

    Following \citep[Corollary 2.7]{leonard2012girsanov}, the process $(\foX_t)_{t\in \ccint{0,\Tf}}$ can be decomposed as
    $$\foX=\foX_0+q\odot  \mu^{X}_L, \quad \overrightarrow{R}-\text{a.s.}\eqsp,$$
    with $\mu^X_L:= \sum_{t\in \ccint{0,\Tf}: \Delta \foX_t \neq 0} \delta_{(t,\Delta \foX_t)}$. 



%\ao{Martingale problem}

\subsubsection{Girsanov's theorem}\label{sec:girsanov}

% \alain{replace $\varrho$ by $\varrho$ here}
From \citet{leonard2012girsanov} we see that, for jump processes, the relative entropy of two path measures can be decomposed with the help of the function $ \varrho(a):=\rme^a-a-1$, for $a\in \R$, and its convex conjugate $\varrho^*(b)=(b+1)\log(b+1)-b$ for $b>-1$ with convention $\varrho^*(-1)=1$ and $\varrho^*(b)=\infty$ for $b<-1$. This is proven by the following theorem.

\begin{theorem}[Girsanov's theorem]\label{girsanovtheorem}
    Let $\P \in \Pc(\Omega)$ verifying $\KL(\P|\overrightarrow{R})<\infty$. Then, there exists a unique predictable non-negative process $u:\Omega \times \ccint{0,\Tf}\times \Qc_d\to [0,\infty)$ satisfying the integrability condition
    \begin{align*}
        \mathbb E_{\P} \int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d} \varrho^*(|u-1|)\rmd\bar L <\infty \eqsp,
    \end{align*}
    and $\P \in MP(u\bar L)$. Moreover, we have that
    \begin{align*}
        \KL(\P|\overrightarrow{R})=\KL(\P_0|\overrightarrow{R}_0)+\mathbb E_{\P} \int_{\ccint{0,\Tf}}\sum_{q\in\Qc_d} h(u_t(\foX_{t},q))\rmd t\eqsp,
    \end{align*}
    with $h(a):=\varrho^*(a-1)=a\log a-a+1$.
\end{theorem}

The proof of Theorem \ref{girsanovtheorem} relies on several following technical lemmas. Let us introduce their framework first. Let $\P \in \Pc(\Omega)$ such that $\P \in MP(\bar K)$, and $\chi$ a $\R$-valued predictable process on $\Omega \times \ccint{0,\Tf}\times \Qc_d$ such that $\int \sum_{\Qc_d}\varrho(\chi)d\bar K<\infty$. We define

$$Z_t:=\exp\l(\chi \odot \tilde \mu^K_t-\int_{\ccint{0,\Tf}}\sum_{\Qc_d} \varrho(\chi)\rmd\bar K \r) \eqsp, \quad \text{for } t\in \ccint{0,\Tf} \eqsp,$$
and the stopping time for $k, j \ge 1$,
$$\sigma^k_j:=\inf \left\{t\in \ccint{0,\Tf};\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(\chi)\rmd\bar K\ge k \text{ or } ( \Delta \foX_t \neq 0 \text{ and } \rho_t(\Delta \foX_t) \notin [-j,k]) \right\}\eqsp.$$

\begin{lemma}\label{Zmartingale}
    Assume further that $\chi$ satisfies
    \begin{align}\label{eq:h}
        \mathbb E_{\P} \int_{\ccint{0,\Tf}}\sum_{q\in\Qc_d}\varrho(\rho_t(q))\bar K(\rmd q\rmd t)<\infty \eqsp.
    \end{align}
    Then $\chi\odot \tilde \mu^K$ is a $\P-$martingale with $\tilde \mu^K:=\mu^X_K-\bar K= \sum_{t\in \ccint{0,\Tf}: \Delta \foX_t \neq 0} \delta_{(t,\Delta \foX_t)}-\bar K$. Moreover, the process $Z$ defined as above is a local $\P-$martingale and a positive $\P-$supermartingale, which satisfies
    $$\rmd Z_t=Z_{t-}\left[(e^{\chi(q)}-1)\odot \rmd\tilde \mu_t^K \right].$$
\end{lemma}
\begin{proof}[Proof of Lemma \ref{Zmartingale}]
   This result is an adaptation of Lemma 6.1 in \citet{leonard2012girsanov}. From its definition, we have that $\tilde \mu^K$ is a $\P-$martingale measure. Therefore the stochastic integral 
    $$M_t^\chi:=\chi \odot \tilde \mu^K_t=\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\lambda_s(q)\rmd\tilde\mu^K$$ is a local $\P-$martingale. Denote $Y_t:=M^\rho_t-\int_{\ccint{0,\Tf}}\beta_s \rmd s$ with $\beta_t:=\sum_{q \in \Qc_d}\varrho(\rho_t(q))K(t,q)$.
    Recalling It\^o's formula for the jump process $(Y_t)_{t\in \ccint{0,\Tf}}$, we have
    \begin{align*}
        \rmd f(Y_t)&=\l[\sum_{q\in \Qc_d}\l[f(Y_{t-}+\chi(q))-f(Y_{t-})\r]K(t,q)\r]\rmd t\\
        &\hspace{4cm}+\nabla f(Y_{t-})\cdot \beta_t\rmd t+\rmd M_t \eqsp, \quad \P\text{-a.s.} \eqsp,
    \end{align*}
    for $f$ a measurable function, with $M$ a local $R-$martingale. Using this formula for $f(y)=\rme^y$, we obtain
    \begin{align*}
        \rmd \rme^{Y_t}&=\left[\sum_{q\in \Qc_d} (\rme^{Y_{t-}+\chi(q)}-\rme^{Y_{t-}})K(t,q)  \right]\rmd t-\rme^{Y_{t-}} \beta_t \rmd t+\rmd M_t \\
        &=\rme^{Y_{t-}}\beta_t \rmd t-\rme^{Y_{t-}}\beta_t\rmd t+\rmd M_t=\rmd M_t \eqsp,
    \end{align*}
    $\P$-a.s., with $M$ a local $\P-$martingale. This implies $Z=\rme^Y$ is a local $\P-$martingale and, since $Z$ is positive, we can conclude that $Z$ is a $\P-$supermartingale. Moreover, we have
    \begin{align*}
        \rmd \rme^{Y_t}&=\rme^{Y_{t-}}[\varrho(\Delta Y_t)+\rmd Y_t]\\
        &=\rme^{Y_t-}\left[\varrho(\chi(q))\odot \rmd\tilde\mu^K_t+(\sum_{q\in \Qc_d}\varrho(\rho_t(q))K(t,q))\rmd t-\beta_t \rmd t+\chi(q)\odot \rmd\tilde\mu_t^K\right]\\
        &=\rme^{Y_t-}\left[\varrho(\chi(q))\odot \rmd\tilde\mu^K_t+\chi(q)\odot \rmd\tilde\mu_t^K\right]\\
        &=\rme^{Y_{t-}}\left[(\rme^{\chi(q)}-1 )\odot \rmd\tilde\mu^K_t \right] \eqsp,
    \end{align*}
    which finishes the proof of Lemma \ref{Zmartingale}.
\end{proof}

\begin{lemma}\label{importantlemma}
    For all $j, k\ge 1$, $Z^{\sigma^k_j}$ is a genuine $\P-$martingale and the measure
    $$Q^k_j:=Z_1^{\sigma^k_j}\P^k_j$$
    is a probability measure on $\Omega$ which satisfies
    $$Q^k \in MP(\1_{\l[0,\sigma^k_j\r]}\rme^\chi \bar K) \eqsp.$$
\end{lemma}
\begin{proof}[Proof of Lemma \ref{importantlemma}]
    Fix $j, k \ge 1$. We have 
    $$Z^{\sigma^k_j}=\exp (\chi^k_j \odot \tilde \mu^K -\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d} \varrho(\chi^k_j)d\bar K ),$$
    where $\chi^k_j=\1_{\l[0,\sigma^k_j\r]}\chi$ is predictable since $\chi$ is predictable and $\1_{\l[0,\sigma^k_j\r]}$ is left continuous, and $\chi^k_j \odot \tilde \mu^K:=\int_{\ccint{0,\Tf}} \sum_{q\in \Qc_d} \rho_s(q)\rmd\tilde \mu^K$. For simplicity, we write $\chi=\1_{\left\{\lambda_j^k \in [-j,k]\right\}}\chi^k_j$ and $Z=Z^{\sigma^k_j}$ in the rest of the proof. From the definition of $\sigma^k_j$, we obtain
    \begin{align}\label{inequality}
        \int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d} \varrho(\chi) \rmd\bar K \leq k \eqsp, \quad \text{for } \chi \in [-j,k], \quad \P^k_j-\text{a.s.} 
    \end{align}
    First, we prove that $Z$ is a $\P^k_j-$martingale. From \cref{Zmartingale}, $Z$ is a local martingale, it is enough to show that
    \begin{align*}
        \mathbb E_{\P^k_j}Z_T^p<\infty \quad \text{for some }p>1 \eqsp.
    \end{align*}
    For all $p\ge 0$, we have
    \begin{align*}
        Z^p=\exp\l(p\chi \odot \tilde \mu^K-p\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(\chi)\rmd\bar K \r)\leq \exp\l(p\chi\odot \tilde \mu^K\r) \eqsp,
    \end{align*}
    and 
    \begin{align*}
       \exp\l(p\chi\odot \tilde \mu^K-\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d} \varrho(p\chi)\rmd\bar K \r)\ge \exp\l(p\chi\odot \tilde \mu^K\r)/C(k,p) \eqsp,
    \end{align*}
    for some finite deterministic constant $C(k,p)>0$ since $\varrho(p\chi)\leq c(k,p)\varrho(\chi)$ holds for all $\chi \leq k$ and some constant $0<c(k,p)<\infty$. Then we have
    \begin{align*}
        \exp\l(\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d} \varrho(p\chi)\rmd\bar K\r)&\leq \exp\l(\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d} c(k,p) \varrho(\chi) \rmd\bar K\r)\\
        &\leq \exp ( kc(k,p))=:C(k,p) \eqsp.
    \end{align*}
    This implies
    \begin{align*}
        Z^p \leq \exp\l(p\chi \odot\tilde \mu^K\r)\leq C(k,p) \exp\l(p\chi\odot \tilde \mu^K-\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d} \varrho(p\chi)\rmd\bar K \r) \eqsp.
    \end{align*}
    Applying Lemma \ref{Zmartingale} for $p\chi$, we deduce that $\exp\l(p\chi\odot \tilde \mu^K_t-\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d} \varrho(p\chi)\rmd\bar K \r)$ is a $\P^k_j$-supermartingale. This yields
    $$\mathbb E_{\P^k_j} \exp\l(p\chi\odot \tilde \mu^K-\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d} \varrho(p\chi)\rmd\bar K \r) \leq \exp\l(p\chi\odot \tilde \mu^K_0-\int_{[0,0]}\sum_{q\in \Qc_d} \varrho(p\chi)\rmd\bar K \r)=1 \eqsp.$$
    Therefore,
    \begin{align*}
        Z^p \leq C(k,p)<\infty \eqsp,
    \end{align*}
    which allow us to conclude that $Z$ is a $\P^k_j-$martingale \citep[see, $e.g$,][]{zitkovic2015uniform}. Then since $\mathbb E_{\P^k_j}(Z_T)=Z_0=1$, we have $1=\int Z_1 \P^k_j=\int Q^k_j$, $i.e.$ $Q^k_j$ is a probability measure on $\Omega$.\\
    Now, we show that 
    $$Q^k_j\in MP(\1_{\l[0,\sigma^k_j\r]}\rme^\chi \bar K) \eqsp.$$
    Let $\tau$ be a finitely valued stopping time which will be specified later, and for any measurable function $f$, we denote $g(t,\Delta \foX_t):=f(\foX_{t-}+\Delta \foX_t)-f(\foX_{t-})$ with $g(t,0)=0$ for all $t\in \ccint{0,\Tf}$. Denote
    $$F_t:=\sum_{0\leq s\leq t\wedge \tau} g(s,\Delta \foX_s) \eqsp.$$
    By Lemma \ref{Zmartingale}, the martingale $Z$ satisfies
    $$\rmd Z_t=\1_{\l[0,\sigma^k_j\r]}(t)Z_{t-}\left[(\rme^\chi-1)\odot \tilde \mu^K \right] \eqsp,$$
    and
    \begin{align*}
        \rmd F_t=\1_{[0,\tau]}(t)g(t,\Delta \foX_t) \eqsp.
    \end{align*}
    Therefore,
    \begin{align*}
        \rmd[Z,F]_t=\1_{[0,\sigma^k_j \wedge \tau]}(t)Z_{t-}(\rme^{\chi(\Delta \foX_t)}-1 )g(t,\Delta \foX_t) \eqsp, \quad \P^k_j-\text{a.s.}
    \end{align*}
    Combining these above with the fact that $Z$ is $\P^k_j-$martingale, we get
    \begin{align*}
       &\mathbb E_{Q^k_j} \sum_{0\leq s\leq t\wedge \tau}g(s,\Delta \foX_s)\\
        &=\mathbb E_{P^k_j}(Z_{t\wedge \tau}F_{t\wedge \tau}-Z_0F_0)\\
        &=\mathbb E_{P^k_j}\int_{[0,t\wedge \tau]} (F_s\rmd Z_s+Z_s\rmd F_s+\rmd[Z,F]_s)\\
        &=\mathbb E_{P^k_j} \left[\int_{[0,\tau]}F_s\rmd Z_s+\sum_{0\leq s\leq t\wedge \tau}Z_{s-}g(t,\Delta \foX_s)+\sum_{0\leq s\leq t\wedge \tau}Z_{s-}(\rme^{\chi(s,\Delta \foX_s)}-1)g(s,\Delta \foX_s) \right]\\
        &=\mathbb E_{P^k_j} \sum_{0\leq s\leq t\wedge \tau}Z_{t-}\rme^{\chi(s,\Delta \foX_s)}g(s,\Delta \foX_s)\\
        &=\mathbb E_{P^k_j} \int_{[0,t\wedge \tau]}\sum_{q\in \Qc_d}Z_{s-}g(s,q)\rme^{\chi(s,q)}\bar K(\rmd q\rmd s)\\
        &=\mathbb E_{Q^k_j}\int_{[0,t\wedge \tau]}\sum_{q\in \Qc_d}g(s,q)\rme^{\chi(s,q)}\bar K(\rmd q\rmd s) \eqsp,
    \end{align*}
    where we used $\P \in LK(\bar K)$ in the last equality. Choosing $\tau$ such that the above terms are meaningful, we can conclude that $Q^k_j \in MP(\rme^{\chi^k_j}\bar K)$.
\end{proof}
\begin{proof}[Proof of \Cref{girsanovtheorem}]

 This proof is an adaptation of Theorem 2.6 in \citet{leonard2012girsanov} based on technical lemmas provided above. By Lemma \ref{Zmartingale}, we have $0<\mathbb E_{\overrightarrow{R}} Z_t\leq 1$ for all $\chi$ satisfies integrability condition \eqref{eq:h}. Note that the relative entropy admits the following variational representation 
 \begin{align}\label{variational}
     \KL(\P|\overrightarrow{R})=\sup\l\{\int u \rmd\P-\log \int \rme^u \rmd\overrightarrow{R}; \quad u: \int \rme^u \rmd\overrightarrow{R}<\infty \r\} \eqsp,
 \end{align}
 for any $\P \in \Pc(\Omega)$ such that $\KL(\P|\overrightarrow{R})<\infty$. Using this expression of $\KL(\P|\overrightarrow{R})$ , we have
    \begin{align*}
        \mathbb E_{\P}(\chi \odot \tilde \mu_T^L-\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d} \varrho(\lambda_t(q))\bar L(\rmd q\rmd t) ) \leq \KL(\P|\overrightarrow{R}) \eqsp, 
    \end{align*}
    for any $\chi$ satisfying \eqref{eq:h}. Therefore,
    \begin{align*}
        \mathbb E_\P(\chi\odot \tilde \mu^L) \leq \KL(\P|\overrightarrow{R})+\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d} \varrho(\chi) \rmd\bar L \eqsp.
    \end{align*}
    Consider $\|.\|_\varrho$ defined as 
    $$\|\chi\|_\varrho:=\inf \left\{a>0; \mathbb E_\P \int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d} \varrho(\chi/a)\bar L(\rmd q\rmd t)\leq 1 \right\} \eqsp.$$
    This norm is the Luxemburg norm of the small Orlicz space
    \begin{align*}
        S_\varrho:=\left\{\chi:\Omega \times\ccint{0,\Tf}\times \Qc_d\to \R; \text{measurable s.t. }\mathbb E_\P \int_{\ccint{0,\Tf}}\sum_{q\in\Qc_d}\varrho (b|\chi|)\rmd\bar L<\infty, \forall b\ge 0\right\} \eqsp,
    \end{align*}
    Taking $\phi:=\dfrac{\chi}{\|\chi\|_\varrho}$ implies
    \begin{align}\label{eq:continuous}
        \mathbb E_\P(\phi\odot \tilde \mu^L) \leq (\KL(\P|\overrightarrow{R})+1 )\|\phi\|_\varrho, \quad \forall \phi \eqsp.
    \end{align}
    Consider now the space $\mathcal{B}$ of all bounded processes such that 
    $$\mathbb E_\P \int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d} \varrho(|\phi|)\rmd\bar L<\infty \eqsp,$$
    respectively its subspace $\mathcal H\subset \mathcal B$ of the predictable processes. Since $\mathcal{B}\subset S_\varrho$ and any $\phi \in \mathcal{H}$ satisfies \eqref{eq:h}, Lemma \ref{Zmartingale} entails \eqref{eq:continuous} for all $\phi\in \mathcal{H}$, as $\KL(\P|\overrightarrow{R})<\infty$. This implies the linear mapping $\phi \mapsto \mathbb E_\P(\phi\odot \tilde \mu_T^L)$ is continuous on $\mathcal{H}$ equipped with the norm $\|\cdot \|_\varrho$.
    
    Note that the convex conjugate of the Young function $\varrho(|a|)$ is $\varrho^*(|b|)$. Thus, the dual space of $(S_\varrho,\|\cdot\|_\varrho)$ is isomorphic to the space
    \begin{align*}
        L_{\varrho^*}:=\left\{ k:\Omega \times \ccint{0,\Tf}\times \Qc_d\to \R;\text{ measurable s.t. } \mathbb E_\P\int_{\ccint{0,\Tf}}\sum_{q\in\Qc_d} \varrho^*(|k|)\rmd\bar L<\infty \right\} \eqsp.
    \end{align*}
    Using the Riesz theorem, there exists a function $k\in L_{\varrho^*}$ such that
    \begin{align}\label{eq:k}
        \mathbb E_\P (\phi\odot \tilde \mu^L)=\mathbb E_\P \int_{\ccint{0,\Tf}}\sum_{q\in\Qc_d}k\phi \rmd\bar L, \quad \text{for } \phi\in \mathcal{H} \eqsp.
    \end{align}
    We now prove the uniqueness and predictability. Introduce the predictable projection of $k\in L_{\varrho^*}$ as $k^{pr}:=\mathbb E_\P(k|X_{[0,t)})$, for $t\in\ccint{0,\Tf}$. Since $\mathcal{B}$ is dense in $S_\varrho$, $\mathcal{H}$ is dense in the subspace of all the predictable processes in $S_\varrho$. Then, any two functions $g$, $k \in L_{\varrho^*}$ satisfying \eqref{eq:k} must share the same projection, $i.e.$ $g^{pr}=k^{pr}$. It follows that there exists a unique predictable process $k$ in the space
    \begin{align*}
        \mathcal{K}(P):=\left\{k:\Omega\times\ccint{0,\Tf}\times \Qc_d\to\R; \text{ predictable s.t. } \mathbb E_\P \int_{\ccint{0,\Tf}}\sum_{q\in\Qc_d}\varrho^*(|k|)\rmd\bar L<\infty \right\}\eqsp,
    \end{align*}
    which satisfies \eqref{eq:k}. Moreover,
    \begin{align*}
        \phi \odot \tilde \mu^L-\phi \odot k\bar L=\phi \odot(\mu^X-\bar L-k\bar L)=\phi\odot (\mu^X -(k+1)\bar L)=\phi\odot (\mu^X-u \bar L), \quad \text{for } \phi\in \mathcal{H} \eqsp,
    \end{align*}
    with $u=k+1$. This means that the equation \eqref{eq:k} is equivalent to
    \begin{align}\label{propertyl}
        \E_{\P} \l[\phi \odot (\mu^X-u\bar L) \r]=0, \quad \text{for } \phi \in \Hc \eqsp.
    \end{align}
    Thus, $u\bar L$ is a positive measure and $u$ is nonnegative. Furthermore, $\P \in MP(l\bar L)$ since equation \eqref{propertyl} implies
\begin{align*}
    \mathbb E_\P \l[\sum_{0\leq s\leq t} \phi(s,\Delta \foX_s)\r]=\mathbb E_\P \l[\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d} \phi(s,q)u \bar L(\rmd q\rmd s)\r], \quad \forall \phi \in \Hc \eqsp.
\end{align*}

We now prove the second claim regarding the expression of the relative entropy $\KL(\P|\overrightarrow{R})$. When $\P \sim \overrightarrow{R}$, we define the stopping time $\tau_j^k$ as
\begin{align*}
    \tau_j^k:=\inf\left\{t \in \ccint{0,\Tf};\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(\log u)\rmd\bar L\ge k \text{ or }( \Delta \foX_t \neq 0 \text{ and } \log u_t(\Delta \foX_t)\notin [-j,k] )\right\} \eqsp.
\end{align*}
    By conditioning w.r.t. $\foX_0$, we can assume without loss of generality that $\overrightarrow{R}_0=\P_0$, $i.e.$ $\frac{d\P_0}{d\overrightarrow{R}_0}(\foX_0)=1$. Applying Lemma \ref{importantlemma} with $\P \in MP(u\bar L)$ and $\chi=-\log u$, we obtain
    \begin{align}\label{Qkj}
        Q^{\tau_j^k}:&=\exp\l(-\log u\odot \tilde \mu^{uL}_1-\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(-\log u)u\bar L(\rmd q\rmd s)\r)\P^{\tau_j^k}\nonumber\\
        &\in MP(\1_{[0,\tau_j^k]}\rme^{-\log u}u \bar L)=MP(\1_{[0,\tau_j^k]} \bar L ) \eqsp.
    \end{align}
    Since $\overrightarrow{R}^{\tau^k_j}$ fulfills the uniqueness condition (U), from \eqref{Qkj}, we derive
    $$Q^{\tau^k_j}=\overrightarrow{R}^{\tau^k_j} \eqsp.$$
   First, we apply \ref{importantlemma} with $\overrightarrow{R} \in MP(\bar L)$ and $\chi = \log u$ to get
    \begin{align*}
        \tilde \P^{\tau_j^k}:&=\exp\l(\log u\odot \tilde \mu^{L}_1-\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(\log u)\bar L(\rmd q\rmd s)\r)\overrightarrow{R}^{\tau_j^k}\\
        &\in MP(\1_{[0,\tau^k_j]}\rme^{\log u}u \bar L)= MP(\1_{[0,\tau^k_j]}l \bar L)\eqsp.
    \end{align*}
    Secondly, applying Lemma \ref{importantlemma} with $\tilde \P^{\tau^k_j}\in MP(\1_{[0,\tau^k_j]}u \bar L)$ and $\chi = -\log u$ yields
    \begin{align*}
        \tilde Q^{\tau^k_j}:&=\exp\l(-\log u\odot \tilde \mu^{uL}_1-\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(-\log u)u\bar L(\rmd q\rmd s)\r)\tilde \P^{\tau_j^k}\\
        &\in  MP(\1_{[0,\tau_j^k]}\rme^{-\log u} u \bar L)=MP(\1_{[0,\tau_j^k]} \bar L ) \eqsp.
    \end{align*}
    From the uniqueness condition (U) satisfied by $\overrightarrow{R}^{\tau^k_j}$, it follows that $\tilde Q^{\tau^k_j}=\overrightarrow{R}^{\tau^k_j}$. Combining it with $Q^{\tau^k_j}=\overrightarrow{R}^{\tau^k_j}$ implies
    $$Q^{\tau^k_j}=\tilde Q^{\tau^k_j} \eqsp,$$
    which means
    \begin{align*}
        &\exp\l(-\log u\odot \tilde \mu^{uL}_1-\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(-\log u)u\bar L(\rmd q\rmd s)\r) \P^{\tau_j^k}\\
        =&\exp\l(-\log u\odot \tilde \mu^{uL}_1-\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(-\log u)u\bar L(\rmd q\rmd s)\r)\tilde \P^{\tau_j^k} \eqsp.
    \end{align*}
    Notice that $\exp\l(-\log u\odot \tilde \mu^{uL}_1-\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(-\log u)u\bar L(\rmd q\rmd s)\r)>0$, we finally conclude that $\P^{\tau^k_j}=\tilde \P^{\tau^k_j}$ $i.e.$
    \begin{align*}
        \1_{[0,\tau_j^k\wedge 1]}\dfrac{d\P}{d\overrightarrow{R}}=\1_{[0,\tau_j^k\wedge 1]}\dfrac{d\P_0}{d\overrightarrow{R}_0}(\foX_0)\exp ((\1_{[0,\tau^k_j\wedge 1]}\log u)\odot \tilde \mu^L-\int_{[0,\tau^k_j\wedge 1]}\sum_{q\in \Qc_d}\varrho(\log u)\rmd\bar L )\eqsp.
    \end{align*}
    
    Letting $k$ and $j$ tend to infinity, since $\tau:=\lim_{k,j\to \infty}\tau_j^k=\infty$, we get
\begin{align*}
    \dfrac{\rmd\P}{\rmd\overrightarrow{R}}=\dfrac{\rmd\P_0}{\rmd\overrightarrow{R}_0}(\foX_0)\exp\l(\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\log u_t(q)\tilde \mu^L(\rmd t\rmd q)-\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(\log u_t(q))\bar L(\rmd t\rmd q) \r), \quad \P-\text{a.s.}
\end{align*}
    We now extend the result above to the case when $\P$ might not be equivalent to $\overrightarrow{R}$. The idea is to approximate $\P$ by a sequence $(\P_n)$, which satisfies $\P_n \sim \overrightarrow{R}$ for all $n\ge 1$. Denoting 
    $$\P_n=(1-\dfrac{1}{n})\P+\dfrac{\overrightarrow{R}}{n} \eqsp, \quad n \ge 1 \eqsp,$$
    we have $\P_n \sim \overrightarrow{R}$ and $\lim_{n\to\infty} \KL(\P|\P_n)=0$. For simplicity, write $\chi=\log u$ and $\chi^n=\log u^n$, well-defined $\P-$a.s. From \eqref{variational} and using $\P\in MP(u\bar L)$ combined with Lemma \ref{Zmartingale}, we obtain
    \begin{align*}
        \KL(\P|\P_n)&\ge \mathbb E_\P ((\chi-\chi^n)\odot \tilde \mu^{u^nL}-\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(\chi-\chi^n)u^n \rmd\bar L ) \eqsp.
    \end{align*}
    By definition, we have $$\tilde \mu^{u^nL}=\mu^X-u^n\bar L=\mu^X-u\bar L+(u-u^n)\bar L=\tilde \mu^{uL}+(u-u^n)\bar L, \quad \P-\text{a.s.} \eqsp,$$
    which yields
    \begin{align*}
        \KL(\P|\P_n)&\ge\mathbb E_\P ((\chi-\chi^n)\odot(\tilde \mu^{uL}+\bar L(u-u^n))-\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}(\dfrac{u}{u^n}-\log \dfrac{u}{u^n}-1)u^n \rmd\bar L )\\
        &=\mathbb E_\P ((\chi-\chi^n)\odot \tilde \mu^{uL}+\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\log \dfrac{u}{u^n}u \rmd\bar L -\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}(\dfrac{u}{u^n}-1 )u^n \rmd\bar L )\eqsp.
    \end{align*}
    Since $\P \in MP(u\bar L)$, we deduce that the stochastic integral $(\chi-\chi^n)\odot \tilde \mu^{uL}$ is a local $\P-$martingale. Therefore,
    \begin{align*}
        \KL(\P|\P_n)&\ge\mathbb E_\P (\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}( u^n-u-u\log \dfrac{u^n}{u})\rmd\bar L )\\
        &=\mathbb E_\P (\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}( \dfrac{u^n}{u}-\log \dfrac{u^n}{u}-1)u\rmd\bar L )\\
        &=\mathbb E_\P \int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(\chi^n-\chi)\rmd(u\bar L) \eqsp.
    \end{align*}
    Since $\lim_{n \to \infty}\KL(\P|\P_n)=0$, we obtain
    \begin{align}\label{lim}
        \lim_{n\to \infty}\mathbb E_\P \int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(\chi^n-\chi)\rmd u\bar L=0 \eqsp.
    \end{align}
    Moreover, the fact that $\P_n\sim \overrightarrow{R}$ yields
    \begin{align*}
        \dfrac{d\P_n}{d\overrightarrow{R}}=\dfrac{d\P_{n,0}}{d\overrightarrow{R}_0}(\foX_0)\exp\l(\chi^n\odot \tilde \mu^L- \int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(\chi^n)\rmd\bar L\r) \eqsp.
    \end{align*}
    Taking the limit for $n\to \infty$, and using dominated convergence theorem, we get
    \begin{align*}
        \KL(\P|\overrightarrow{R})=\KL(\P_0|\overrightarrow{R}_0)+\lim_{n\to \infty}\mathbb E_\P (\chi^n\odot \tilde \mu^L- \int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(\chi^n)\rmd\bar L) \eqsp.
    \end{align*}
    By direct computation, we can show 
    \begin{align*}
        \mathbb E_\P (\chi^n\odot \tilde \mu^L- \int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(\chi^n)\rmd\bar L)&=\mathbb E_\P (\chi\odot \tilde \mu^L- \int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(\chi)\rmd\bar L)\\
        & \hspace{2cm}-\mathbb E_\P \int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d} \varrho(\chi^n-\chi)\rmd u\bar L \eqsp.
    \end{align*}
    Taking $n\to \infty$ and using \eqref{lim}, we deduce that
\begin{align*}
    \KL(\P|\overrightarrow{R})=\KL(\P_0|\overrightarrow{R}_0)+\mathbb E_\P (\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\log u \rmd\tilde \mu^L-\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(\log u)\rmd\bar L ) \eqsp.
\end{align*}
Applying \eqref{propertyl} to the function $\phi=\log u$, we get
\begin{align*}
    \KL(\P|\overrightarrow{R})&=\KL(\P_0|\overrightarrow{R}_0)+\mathbb E_\P (\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}(l-1)\log u \rmd\bar L-\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(\log u)d\bar L )\\
    &=\KL(\P_0|\overrightarrow{R}_0)+\mathbb E_\P \int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}[(u-1)\log u-u+\log u+1]\rmd\bar L\\
    &=\KL(\P_0|\overrightarrow{R}_0)+\mathbb E_\P \int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}(u\log u-u+1)\rmd\bar L \eqsp.
\end{align*}
Replacing the formula of $\bar L$ and extending the function $u$ to $u_t(\foX_{t-},q)=0$ if $(\foX_{t-},q)\notin \Ac$, the previous equality can be rewritten as
\begin{align*}
    \KL(\P|\overrightarrow{R})&=\KL(\P_0|\overrightarrow{R}_0)+\mathbb E_\P \int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}(u\log u-u+1)(\foX_{t-},q)\rmd t\\
    &=\KL(\P_0|\overrightarrow{R}_0)+\mathbb E_\P \int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}(u\log u-u+1)(\foX_{t},q)\rmd t \eqsp,
\end{align*}
where the last equality above relies on the fact that $\foX_{t-}=X_t$ for Lebesgue almost all $t \in \ccint{0,\Tf}$. We conclude that the relative entropy admits the following expression
\begin{align*}
    \KL(\P|\overrightarrow{R})=\KL(\P_0|\overrightarrow{R}_0)+\mathbb E_\P \int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}h(u(\foX_{t},q))\rmd t \eqsp,
\end{align*}
with $h(a):=\varrho^*(a-1)=a\log a-a+1$ for $a>0$. The proof of \cref{girsanovtheorem} is then finished.
\end{proof}


\subsubsection{Optimal Control problem of the time reversal process}

In the continuous case, \citet{conforti2024klconvergenceguaranteesscore} showed that the time reversal process satisfies an optimal control problem, which characterizes its evolution and provides us a power tool to prove the convergence of the algorithm simulating the backward process. In this section, using Girsanov'stheorem \ref{girsanovtheorem}, we aim to characterize this entropic optimization problem. Let $\overrightarrow{R}$ be a reversible path probability measure on path space, $i.e.$ $\overleftarrow{R}=\overrightarrow{R}$. Let $\overrightarrow{\P}^{\mustar}$ be the forward probability measure on $\ccint{0,\Tf}$ started at ${\mustar}$, and denote by $\overleftarrow{\P}^{\mustar}$ the backward probability measure with the final distribution $\mustar$.  In \cref{invariant_measure}, we showed that the forward invariant measure is $\gamma^d$.

\begin{proposition}\label{prop:2}
    Let $\P \in \Pc(\Omega)$ verifying $\KL(\P|\overrightarrow{R})<\infty$. We then have that its time reversal $\overleftarrow{\P}^{\mustar}$ satisfies the following optimization problem
    \begin{align*}
        \overleftarrow{\P}^{\mustar}=\argmin_{\P \text{ s.t. }\P_{\Tf}=\mustar} ( \KL(\P|\overrightarrow{R})+\int g\rmd\P_{\Tf}),\quad \text{with } g=-\log\dfrac{\rmd\mustar}{\rmd\gamma^d} \eqsp.
    \end{align*}
\end{proposition}
\begin{proof}[Proof of \cref{prop:2}]
   From \citet[Proposition 3.1]{leonard2012girsanov}, we have that the relative entropy $\KL(\P|\overrightarrow{R})$ admits the following variational representation 
    \begin{align*}
        \KL(\P|\overrightarrow{R})=\sup_{f \text{ s.t. } \int \rme^f \rmd\overrightarrow{R}<\infty} (\int f \rmd\P-\log \int \rme^f \rmd\overrightarrow{R}), \quad \text{for } \P \in \Pc(\Omega) \eqsp.
    \end{align*}
     Taking $f(X.)=-g(\overleftarrow{X}_{\Tf})$, for all $\P \in \Pc(\Omega)$ satisfying the finite relative entropy condition, we get
    \begin{align*}
        \KL(\P|\overrightarrow{R})&\ge \int -g\rmd\P_{\Tf}-\log \int \rme^{-g}\rmd\overrightarrow{R}=-\int g \rmd\P_{\Tf}-\log \int \rmd\mustar=-\int g \rmd\P_{\Tf},
    \end{align*}
    since $\int \rmd\mustar=1$. As $\ovl{\P}^{\mustar}$ is the backward process ended at ${\mustar}$ and $\overrightarrow{R}$ is a reversible path probability measure on $\ccint{0,\Tf}$, $i.e.$ $\ovr{R}=\ovl{R}$, we have
%\begin{align*}
%    \dfrac{\rmd\overrightarrow{\P}^{\mustar}}{\rmd\overrightarrow{R}}=\dfrac{\rmd\mustar}{\rmd\gamma^d}(\overrightarrow{X}_0).
%\end{align*}

\begin{align*}
     \dfrac{\rmd\overleftarrow{\P}^{\mustar}}{\rmd\ovr{R}}=\dfrac{\rmd\overleftarrow{\P}^{\mustar}}{\rmd\overleftarrow{R}}=\dfrac{\rmd\mustar}{\rmd\gamma^d}(\overleftarrow{X}_{\Tf})=\rme^{-g(\overleftarrow{X}_{\Tf})} \eqsp.
\end{align*}
    This implies
    \begin{align*}
    \KL(\overleftarrow{\P}^{\mustar}|\overrightarrow{R})=\KL(\mustar|\gamma^d)=\int \log \dfrac{\rmd\mustar}{\rmd\gamma^d}\rmd\mustar=-\int g \rmd\mustar=-\int g \rmd\P_{\Tf} \eqsp.
    \end{align*}
    Combining the previous results, we obtain that the time reversal $\overleftarrow{\P}^{\mustar}$ is the optimal solution to the following problem
\begin{align*}
    \overleftarrow{\P}^{\mustar}=\argmin_{\P \text{ s.t. }\P_{\Tf}=\mustar} (\KL(\P|\overrightarrow{R})+\int g \rmd\P_{\Tf}) \eqsp,
\end{align*}
which is the desired conclusion.
\end{proof}

Now, using the expression of $\KL(\P|\overrightarrow{R})$ in Girsanov's theorem \ref{girsanovtheorem}, we can derive the following Optimal Control problem.
\begin{definition}[Admissible controls]
    We say that a measurable function $u: \Omega \times \ccint{0,\Tf}\times \msx \times \Qc_d \to \R^+$ is an admissible control if $u_t(x,q)=0$ if $(x,q)\in \msx \setminus \Ac$. We denote by $\Uc$ the set of all admissible controls.
\end{definition}
\begin{theorem}\label{theo:3}
    $\overleftarrow{\P}^{\mustar}$ is the law of $\overleftarrow{X}^{u^*}$ with $u^*$ is the optimal solution to 
    \begin{align}\label{controlproblem}
        V(0,\overleftarrow{X}_0)=&\inf_{u\in \Uc} \mathbb E \left[\int_{\ccint{0,\Tf}}\sum_{q\in\Qc_d} h(u_t(\overleftarrow{X}_{t}^u,q))\rmd t+g(\overleftarrow{X}^u_{\Tf}) \right] \eqsp,\\
        &\text{s.t. } \begin{cases}
            \overleftarrow{X}^u=\overleftarrow{X}^u_0+q\odot \mu^X_{uL} \eqsp,\nonumber\\
            \overleftarrow{X}^u_{\Tf}\sim \mustar \eqsp.\nonumber
        \end{cases}
    \end{align}
\end{theorem}
\begin{proof}[Proof of \cref{theo:3}]
    Theorem \ref{theo:3} is a consequence of Theorem \ref{girsanovtheorem} and Proposition \ref{prop:2} together with the note that $\overleftarrow{X}_{t-}^u=\overleftarrow{X}_{t}^u$ for Lebesgue almost all $t \in \ccint{0,\Tf}$.
\end{proof}

\subsubsection{Hamilton--Jacobi--Bellman equation}

The goal of this section is to characterize the previous optimization problem via the Hamilton--Jacobi--Bellman (HJB) equation. To this purpose, we first consider the generalization of the previous control problem. Let $J$ be the following cost 
\begin{align*}
    J(t,x,u)&:=\mathbb E \l[\int_{[t,\Tf]}\sum_{q\in \Qc_d} h(u_s(\overleftarrow{X}_{s}^{t,x,u},q))\rmd s+g(\overleftarrow{X}_{\Tf}^{t,x,u})\r] \eqsp,\\
    &\text{s.t. } \begin{cases}
        \overleftarrow{X}^{t,x,u}=\overleftarrow{X}^{t,x,u}_t+q\odot  \mu^X_{uL},\\
        \overleftarrow{X}^{t,x,u}_t=x,
    \end{cases} \quad \text{for } (x,t,u)\in \msx\times \ccint{0,\Tf}\times \Uc \eqsp.
\end{align*}
Consider $V(t,x)$ to be the value function of the previous cost function, $i.e.$,
\begin{align*}
    V(t,x):=\inf_{u\in \Uc} J(t,x,u) \eqsp.
\end{align*}
The following Dynamic Programming Principle is the main tool to derive the HJB equation. 
\begin{lemma}\label{lem:1}
    For any stopping time $\kappa \in [t,\Tf]$, the Dynamic Programming Principle (DPP) implies
\begin{align}\label{dpp}
    V(t,x)=\inf_{u\in \Uc} \mathbb E \l[\int_{(t,\kappa]} \sum_{q\in \Qc_d}h(u_s(\overleftarrow{X}_{s}^{t,x,u},q))\rmd s+V(\kappa,\overleftarrow{X}^{t,x,u}_\kappa)\r] \eqsp.
\end{align}
\end{lemma}
\begin{proof}[Proof of \cref{lem:1}]
    Refer to \citet[Section 2.2]{touzi2012optimal}.
\end{proof}
The expression of $V$ given in Lemma \ref{lem:1} leads us to the following HJB equation, which is a characterization of the optimal control to the problem \eqref{controlproblem}.

\begin{theorem}\label{theo:4}
    Assume that $V$ is continuously differentiable in time. Then, the optimal control $u^*$ to the problem \eqref{controlproblem} is 
    \begin{align*}
        u^*_t(x,q)=\begin{cases}
            \rme^{V(t,x)-V(t,x+q)} \eqsp, \quad &\text{ if }  (x,q) \in \Ac \eqsp,\\
            \hspace{1cm}0\eqsp,\quad  &\text{ otherwise \eqsp, } 
        \end{cases}
    \end{align*}
    with $V$ satisfies the following HJB equation
    \begin{align*}
         \begin{cases}
        \partial_t V(t,x)+\inf_{u \in \Uc}\sum_{q\in \Qc_d}\l[ h(u_t(x,q))+[V(t,x+q)-V(t,x)]u_t(x,q) \r]=0 \eqsp,\\
        V(\Tf,x)=g(x) \eqsp,
    \end{cases} \quad \text{for }(t,x) \in \ccint{0,\Tf} \times \msx \eqsp.
    \end{align*}
   The HJB equation can be rewritten as 
    \begin{align}\label{hjb}
    \begin{cases}
        \partial_t V(t,x)-\sum_{q\in \Qc_d}(\rme^{V(t,x)-V(t,x+q)}\1_{\Ac}(x,q))+d=0 \eqsp,\\
        V(\Tf,x)=g(x) \eqsp,
    \end{cases} \quad \text{for } (t,x) \in \ccint{0,\Tf}\times \msx \eqsp.
    \end{align}
    
\end{theorem}
\begin{proof}[Proof of \cref{theo:4}]


The DPP formula \eqref{dpp} for $\kappa=t+\alpha$ with $\alpha>0$ leads to 
\begin{align*}
     \mathbb E \l[\int_{(t,t+\alpha]} \sum_{q\in \Qc_d}h(u_s(\overleftarrow{X}_{s-}^{t,x,u},q))\rmd s+V(t+\alpha,\overleftarrow{X}^{t,x,u}_{t+\alpha})-V(t,x)\r]\ge 0 \eqsp,
\end{align*}
for any admissible control $u \in \Uc$. Using It\^o's formula on the process $\overleftarrow{X}^{t,x,u}$, we get
\begin{align*}
    \mathbb E \Bigg[&\int_{(t,t+\alpha]} \sum_{q\in \Qc_d}h(u_s(\overleftarrow{X}_{s-}^{t,x,u},q))\rmd s\\
    +&\int_{(t,t+\alpha]}(\partial_t V(s,\overleftarrow{X}^{t,x,u}_{s-})+\sum_{q\in \Qc_d}(V(s,\overleftarrow{X}_{s-}^{t,x,u}+q)-V(s,\overleftarrow{X}^{t,x,u}_{s-}))u_s(\overleftarrow{X}^{t,x,u}_{s-},q) )\rmd s\Bigg]\ge 0 \eqsp.
\end{align*}
Multiplying the both hand sides by $\frac{1}{\alpha}$ and pushing $\alpha \to 0$, we derive that
\begin{align*}
    \sum_{q\in \Qc_d}h(u_t(x,q))+\partial_t V(t,x)+\sum_{q\in \Qc_d}\l[V(t,x+q)-V(t,x)\r]u_t(x,q) \ge 0 \eqsp,
\end{align*}
for any admissible control $u \in \Uc$. Taking the infimum w.r.t. $u$, we get
\begin{align*}
    \partial_t V(t,x)+\inf_{u\in \Uc} \sum_{q\in \Qc_d}\l[ h(u_t(x,q))+[V(t,x+q)-V(t,x)]u_t(x,q) \r] \ge 0 \eqsp, \quad \text{for }(t,x) \in \ccint{0,\Tf} \times \msx \eqsp.
\end{align*}
We prove next the equality by contradiction. Assume that there exists $(t_0,x_0)\in \ccint{0,\Tf}\times \msx$ such that
\begin{align*}
    \partial_t V(t_0,x_0)+\inf_{u\in \Uc} \sum_{q\in \Qc_d}\l[ h(u_{t_0}(x_0,q))+[V(t_0,x_0+q)-V(t_0,x_0)]u_{t_0}(x_0,q) \r]>0 \eqsp.
\end{align*}
Denote $\Delta V(t_0,x_0,q):=V(t_0,x_0+q)-V(t_0,x_0)$. The previous inequality implies that there exists $\varepsilon>0$ such that 
\begin{align}\label{varepsilon}
     \partial_t V(t_0,x_0)+\inf_{u\in \Uc} \sum_{q\in \Qc_d}\l[ h(u)+u\Delta V \r](t_0,x_0,q)\ge\varepsilon>0 \eqsp.
\end{align}
Take $\xi>0$ small enough such that 
\begin{align}\label{xi}
    \sum_{q\in \Qc_d}(\rme^{-\Delta V+\xi}-\rme^{-\Delta V})(t_0,x_0,q)<\dfrac{\varepsilon}{2} \eqsp,
\end{align}
and define the function $\varphi \leq V$ as
\begin{align*}
    \varphi(t,x):=V(t,x)-\xi\l[|\Tf-t_0|^2+\delta_{\l\{x_0\r\}}(x)\r] \eqsp, \quad \text{for } (t,x)\in \ccint{0,\Tf} \times \msx \eqsp.
\end{align*}
It is clear that
\begin{align*}
    \varphi(t_0,x_0)=V(t_0,x_0) \eqsp, \quad \partial_t \varphi(t_0,x_0)=\partial_t V(t_0,x_0) \eqsp,\quad \text{and} \quad \varphi(t_0,x)-V(t_0,x)=-\xi \text{ for } x \neq x_0 \eqsp. 
\end{align*}
Therefore,
\begin{align*}
    &\partial_t \varphi(t_0,x_0)+\inf_{u\in \Uc}  \sum_{q\in \Qc_d}\l[ h(u_{t_0}(x_0,q))+[\varphi(t_0,x_0+q)-\varphi(t_0,x_0 )]u_{t_0}(x_0,q) \r]\\
    =& \partial_t V(t_0,x_0)+\inf_{u\in \Uc} \sum_{q\in \Qc_d}\l[ h(u_{t_0}(x_0,q))+\l[V(t_0,x_0+q)-V(t_0,x_0)-\xi\r]u_{t_0}(x_0,q) \r]\\
    =& \partial_t V(t_0,x_0)+\inf_{u\in \Uc} \sum_{q\in \Qc_d}\l[ h(u)+(\Delta V-\xi)u \r](t_0,x_0,q) \eqsp.
\end{align*}
The minimum above is attained at $u \in \Uc$ such that $u(t_0,x_0,q)=e^{-\Delta V+\xi}(t_0,x_0,q)$ , thus
\begin{align*}
    &\partial_t \varphi(t_0,x_0)+\inf_{u\in \Uc}  \sum_{q\in \Qc_d}\l[ h(u_{t_0}(x_0,q))+[\varphi(t_0,x_0+q)-\varphi(t_0,x_0 )]u_{t_0}(x_0,q) \r]\\
    =& \partial_t V(t_0,x_0)+\sum_{q\in \Qc_d}\l[h(e^{-\Delta V+\xi})+( \Delta V-\xi)e^{-\Delta V+\xi} \r](t_0,x_0,q)\\
    =& \partial_t V(t_0,x_0)+\sum_{q\in \Qc_d} (1-e^{-\Delta V+\xi})(t_0,x_0,q)\\
    =& \partial_t V(t_0,x_0)+\sum_{q\in \Qc_d} (1-e^{-\Delta V})(t_0,x_0,q)+ \sum_{q\in \Qc_d} (e^{-\Delta V}-e^{-\Delta V+\xi})(t_0,x_0,q)\\
    >& \varepsilon -\dfrac{\varepsilon}{2}=\dfrac{\varepsilon}{2}>0 \eqsp,
\end{align*}
where the last inequality relies on \eqref{xi} and \eqref{varepsilon} with $u=e^{-\Delta V}$. Therefore, we obtain
\begin{align*}
    \partial_t \varphi(t_0,x_0)+\inf_{u\in \Uc}  \sum_{q\in \Qc_d}\l[ h(u_{t_0}(x_0,q))+[\varphi(t_0,x_0+q)-\varphi(t_0,x_0 )]u_{t_0}(x_0,q) \r]>0 \eqsp.
\end{align*}
From the continuity in time of the Hamiltonian, the previous inequality yields that
\begin{align}\label{v(t,x_0)}
    &\partial_t \varphi(t,x)+ \inf_{u\in \Uc} \sum_{q\in \Qc_d}\l[ h(u_{t}(x,q))+[\varphi(t,x+q)-\varphi(t,x)]u_{t}(x,q) \r]\ge0 \eqsp,\nonumber \\
    &\hspace{7cm}\text{for } (t,x) \in (t_0-r,t_0+r)\times \l\{x_0\r\} \eqsp,
\end{align}
for some $r>0$. Defining the stopping time $\kappa^u$ as
\begin{align*}
    \kappa^u:=\inf\l\{t\in (t_0,\Tf]: \overleftarrow{X}_{t-}^{t_0,x_0,u}\neq x_0\r\}\wedge (t_0+r) \eqsp,
\end{align*}
for an arbitrary control $u$, we have
\begin{align*}
    \varphi(\kappa^u,\overleftarrow{X}^{t_0,x_0,u}_{\kappa^u})=\begin{cases}
        \varphi(t_0+r,\overleftarrow{X}^{t_0,x_0,u}_{t_0+r} ) \eqsp, \quad &\text{if } \quad \overleftarrow{X}^{t_0,x_0,u}_{\kappa^u-}=x_0 \eqsp,\\
        \varphi(\kappa^u, \overleftarrow{X}^{t_0,x_0,u}_{\kappa^u}) \eqsp, \quad &\text{if } \quad \overleftarrow{X}^{t_0,x_0,u}_{\kappa^u-} \neq x_0 \eqsp.
    \end{cases}
\end{align*}
This implies that
\begin{align*}
    \varphi(\kappa^u,\overleftarrow{X}^{t_0,x_0,u}_{\kappa^u})-V(\kappa^u,\overleftarrow{X}^{t_0,x_0,u}_{\kappa^u})&=\begin{cases}
        \hspace{1cm}-\xi r^2 \eqsp, \quad &\text{if } \quad \overleftarrow{X}^{t_0,x_0,u}_{\kappa^u-}=x_0 \eqsp,\\
        -\xi (|\kappa^u-t_0|^2 +1) \eqsp, \quad &\text{if } \quad \overleftarrow{X}^{t_0,x_0,u}_{\kappa^u-} \neq x_0 \eqsp,
    \end{cases}\\
    &\leq -\xi r^2 \eqsp.
\end{align*}
Therefore,
\begin{align*}
    &\mathbb E \l[\int_{(t_0,\kappa^u]} \sum_{q\in \Qc_d}h(u_s(\overleftarrow{X}_{s-}^{t_0,x_0,u},q))ds+V(\kappa^u,\overleftarrow{X}^{t_0,x_0,u}_{\kappa^u})\r]\\
    \ge&\mathbb E \l[\int_{(t_0,\kappa^u]} \sum_{q\in \Qc_d}h(u_s(\overleftarrow{X}_{s-}^{t_0,x_0,u},q))ds+\varphi(\kappa^u,\overleftarrow{X}^{t_0,x_0,u}_{\kappa^u})+\xi r^2\r]\\
    =&\mathbb E \l[\int_{(t_0,\kappa^u]} \sum_{q\in \Qc_d}h(u_s(\overleftarrow{X}_{s-}^{t_0,x_0,u},q))ds+\varphi(\kappa^u,\overleftarrow{X}^{t_0,x_0,u}_{\kappa^u})-\varphi(t_0,x_0)\r]+\varphi(t_0,x_0)+\xi r^2 \eqsp.
\end{align*}
Using It\^o's formula and the fact that $V(t_0,x_0
)=\varphi(t_0,x_0)$, we obtain
\begin{align*}
    &\mathbb E \l[\int_{(t_0,\kappa^u]} \sum_{q\in \Qc_d}h(u_s(\overleftarrow{X}_{s-}^{t_0,x_0,u},q))ds+V(\kappa^u,\overleftarrow{X}^{t_0,x_0,u}_{\kappa^u})\r]\\
    =&\mathbb E \int_{(t_0,\kappa^u]}\Bigg[\partial_t \varphi(s,\overleftarrow{X}^{t_0,x_0,u}_{s-})+ \sum_{q\in \Qc_d}h(u_s(\overleftarrow{X}_{s-}^{t_0,x_0,u},q))\\
    &\hspace{2cm}+(\varphi(s,\overleftarrow{X}_{s-}^{t_0,x_0,u}+q)-\varphi(s,\overleftarrow{X}^{t_0,x_0,u}_{s-}))u_s(\overleftarrow{X}^{t_0,x_0,u}_{s-},q) \Bigg]ds+V(t_0,x_0)
    +\xi r^2 \eqsp.
\end{align*}
This together with \eqref{v(t,x_0)} yields
\begin{align}\label{contradiction}
    \mathbb E \l[\int_{(t_0,\kappa^u]} \sum_{q\in \Qc_d}h(u_s(\overleftarrow{X}_{s-}^{t_0,x_0,u},q))ds+V(\kappa^u,\overleftarrow{X}^{t_0,x_0,u}_{\kappa^u})\r]\ge V(t_0,x_0)+\xi r^2 \eqsp.
\end{align}
Since the above control $u$ is arbitrary, \eqref{contradiction} is indeed a contradiction to DPP formula \eqref{dpp}.

Consequently, we can deduce the following HJB equation satisfied by the value function
\begin{align}\label{hjb1}
    \begin{cases}
        \partial_t V(t,x)+\inf_{u\in \Uc} \sum_{q\in \Qc_d}\l[ h(u_t(x,q))+[V(t,x+q)-V(t,x)]u_t(x,q) \r]=0 \eqsp,\\
        V(\Tf,x)=g(x) \eqsp,
    \end{cases}\text{ for } (t,x)\in \ccint{0,\Tf}\times \msx \eqsp.
\end{align}
We now find the minimum in \eqref{hjb1} as done before. By direct computation, we easily obtain the optimal solution as follows
\begin{align*}
    u^*_t(x,q)=\begin{cases}
        \rme^{V(t,x)-V(t,x+q)} \eqsp, \quad &\text{for } (x,q)\in \Ac \eqsp,\\
        \hspace{1cm}0 \eqsp, \quad &\text{otherwise} \eqsp.
    \end{cases}
\end{align*}
Hence, replacing $u^*$ into \eqref{hjb1} yields
\begin{align*}
\begin{cases}
    \partial_t V(t,x)-\sum_{q\in \Qc_d}(\rme^{V(t,x)-V(t,x+q)}\1_{\Ac}(x,q))+d=0 \eqsp,\\
    V(\Tf,x)=g(x) \eqsp,
\end{cases} \quad \text{for } (t,x)\in \ccint{0,\Tf}\times \msx \eqsp.
\end{align*}
and we complete the proof of Theorem \ref{theo:4}.
\end{proof}   


The previous HJB equation will be instrumental in the proof of our convergence bound. To do this, we first consider the following martingale and monotone property.

\begin{proposition}\label{prop:3}
    With all the notations above, $\sum_{q\in \Qc_d} u^*_t(\overleftarrow{X}_{t}^{u^*},q)$ is a martingale. Furthermore, for any $0\leq s\leq t\leq \Tf$, it holds
     \begin{align*}
         y_s \leq \rme^{-4(t-s)}y_t \eqsp, 
     \end{align*}
     with $y_r:=\mathbb{E} \l[\sum_{q\in \Qc_d}h(u^*_r(\overleftarrow{X}_{r}^{u^*},q))\r]$, for $ r \in \ccint{0,\Tf}$ \eqsp.
\end{proposition}
\begin{proof}[Proof of \Cref{prop:3}]

% \begin{proposition}\label{prop:3}
%     With all the notations above, $u^*_t(\overleftarrow{X}_{t}^{u^*},q)$ is a martingale if $q\in \Qc_d$ is fixed. Furthermore, for any $0\leq s\leq t\leq \Tf$, it holds
%      \begin{align*}
%          y_s \leq \rme^{-4(t-s)}y_t \eqsp, 
%      \end{align*}
%      with $y_r:=\mathbb{E} \l[\sum_{q\in \Qc_d}h(u^*_r(\overleftarrow{X}_{r}^{u^*},q))\r]$, for $ r \in \ccint{0,\Tf}$ \eqsp.
% \end{proposition}
% \begin{proof}[Proof of \Cref{prop:3}]

Fix $t \in \ccint{0,\Tf}$, we have $\overleftarrow{\P}^{\mu_0}(\overleftarrow{X}_{t-}^{u^*}=\overleftarrow{X}_{t}^{u^*})=1$. Applying It\^o's formula on
$$\varphi(t,\overleftarrow{X}_{t}^{u^*} ):=\sum_{q\in \Qc_d} u^*_t(\overleftarrow{X}_{t}^{u^*},q)=\sum_{q\in \Qc_d}e^{V(t,\overleftarrow{X}_{t}^{u^*})-V(t,\overleftarrow{X}_{t}^{u^*}+q)} \eqsp,$$ 
we obtain that the process
\begin{align*}
    &\varphi(t,\overleftarrow{X}_{t}^{u^*} )-\varphi(0,\overleftarrow{X}_{0}^{u^*} )+\int_0^t\l[ \partial_t \varphi(s,\overleftarrow{X}_{s}^{u^*} )+\sum_{\bar q \in \Qc_d}\l[\varphi(s,\overleftarrow{X}_{s-}^{u^*}+\bar q )-\varphi(s,\overleftarrow{X}_{s-}^{u^*} ) \r]u^*_s( \overleftarrow{X}^{u^*}_{s-},\bar q)\r]\rmd s
\end{align*}
is a martingale. Since $\baX_t=\baX_{t-}$ for Lebesgue almost all $t \in \ccint{0,\Tf}$, then the process
\begin{align*}
    &\varphi(t,\overleftarrow{X}_{t}^{u^*} )-\varphi(0,\overleftarrow{X}_{0}^{u^*} )+\int_0^t\l[ \partial_t \varphi(s,\overleftarrow{X}_{s}^{u^*} )+\sum_{\bar q \in \Qc_d}\l[\varphi(s,\overleftarrow{X}_{s}^{u^*}+\bar q )-\varphi(s,\overleftarrow{X}_{s}^{u^*} ) \r]u^*_s( \overleftarrow{X}^{u^*}_{s},\bar q)\r]\rmd s
\end{align*}
is a martingale. Denote
\begin{align*}
    b_s:=\partial_t \varphi(s,\overleftarrow{X}_{s}^{u^*} )+\sum_{\bar q \in \Qc_d}\l[\varphi(s,\overleftarrow{X}_{s}^{u^*}+\bar q )-\varphi(s,\overleftarrow{X}_{s}^{u^*} ) \r]u^*_s( \overleftarrow{X}^{u^*}_{s},\bar q), \quad \text{for } s\in \ccint{0,\Tf} \eqsp.
\end{align*}
By the definition of $\varphi$ and the HJB equation \eqref{hjb}, we get that
\begin{align*}
    b_s &=\sum_{q\in \Qc_d} u^*_s(\overleftarrow{X}_{s}^{u^*},q)\l[\partial_t V(s,\overleftarrow{X}_{s}^{u^*})-\partial_t V(s,\overleftarrow{X}_{s}^{u^*}+q) \r]\\
    &\hspace{2cm}+ \sum_{\bar q \in \Qc_d}\l[\sum_{q\in \Qc_d} u^*_t(\overleftarrow{X}_{t}^{u^*}+\bar q,q)-\sum_{q\in \Qc_d} u^*_t(\overleftarrow{X}_{t}^{u^*},q) \r]u^*_s( \overleftarrow{X}^{u^*}_{s},\bar q)\\
    &= \sum_{q\in \Qc_d} u^*_s(\overleftarrow{X}_{s}^{u^*},q)\l[\sum_{\bar q\in \Qc_d} u^*_s(\overleftarrow{X}_{s}^{u^*},\bar q)-\sum_{\bar q\in \Qc_d} u^*_t(\overleftarrow{X}_{t}^{u^*}+q,\bar q) \r]\\
    &\hspace{2cm}+ \sum_{\bar q \in \Qc_d}\l[\sum_{q\in \Qc_d} u^*_t(\overleftarrow{X}_{t}^{u^*}+\bar q,q)-\sum_{q\in \Qc_d} u^*_t(\overleftarrow{X}_{t}^{u^*},q) \r]u^*_s( \overleftarrow{X}^{u^*}_{s},\bar q) \eqsp.
\end{align*}
Swapping $q$ with $\bar q$ in the first term implies $b_s=0$. Therefore $\sum_{q\in \Qc_d} u^*_t(\overleftarrow{X}_{t}^{u^*},q)$is a martingale. Combining this with the fact that $h$ is a convex function, we thus obtain $\sum_{q\in \Qc_d}h(u^*_t(\overleftarrow{X}_{t}^{u^*},q))$ is a submartingale and the monotonicity follows. 


% 

We now want to go further to see the monotonicity in details. Define 
$$f(t,\overleftarrow{X}_{t}^{u^*}):=\sum_{q\in \Qc_d}h(u^*_t(\overleftarrow{X}_{t}^{u^*},q))=\sum_{q\in \Qc_d}h(\rme^{V(t,\overleftarrow{X}_{t}^{u^*})-V(t,\overleftarrow{X}_{t}^{u^*}+q)} )\1_{\Ac}(\overleftarrow{X}_{t}^{u^*},q ) \eqsp.$$
Applying It\^o's formula on $f(t,\overleftarrow{X}_{t}^{u^*})$, we get that the process
\begin{align*}
    &f(t,\overleftarrow{X}_{t}^{u^*})-f(0,\overleftarrow{X}_{0}^{u^*})-\int_0^t \partial_t f(s,\overleftarrow{X}_{s}^{u^*})+\sum_{\bar q\in \Qc_d}\l[f(s,\overleftarrow{X}_{s-}^{u^*}+\bar q)-f(s,\overleftarrow{X}_{s-}^{u^*}) \r]u^*_s(\overleftarrow{X}_{s-}^{u^*},\bar q)\rmd s
\end{align*}
is a martingale. Since $\baX_t=\baX_{t-}$ for Lebesgue almost all $t \in \ccint{0,\Tf}$, then the process
\begin{align*}
       f(t,\overleftarrow{X}_{t}^{u^*})-f(0,\overleftarrow{X}_{0}^{u^*})-\int_0^t \partial_t f(s,\overleftarrow{X}_{s}^{u^*})+\sum_{\bar q\in \Qc_d}\l[f(s,\overleftarrow{X}_{s}^{u^*}+\bar q)-f(s,\overleftarrow{X}_{s}^{u^*}) \r]u^*_s(\overleftarrow{X}_{s}^{u^*},\bar q)\rmd s
\end{align*}
is a martingale. Denote
\begin{align*}
    c_s:=\partial_t f(s,\overleftarrow{X}_{s}^{u^*})+\sum_{\bar q\in \Qc_d}\l[f(s,\overleftarrow{X}_{s}^{u^*}+\bar q)-f(s,\overleftarrow{X}_{s}^{u^*}) \r]u^*_s(\overleftarrow{X}_{s}^{u^*},\bar q), \quad \text{for } s\in \ccint{0,\Tf} \eqsp.
\end{align*}
By definition of $f$ and $h$, we have that
\begin{align*}
    c_s&= \sum_{q\in \Qc_d}h'(u^*_s(\overleftarrow{X}_{s}^{u^*},q))u^*_s(\overleftarrow{X}_{s}^{u^*},q)\l[\partial_t V(s,\overleftarrow{X}_{s}^{u^*})-\partial_t V(s,\overleftarrow{X}_{s}^{u^*}+q) \r]\\
    &+\sum_{\bar q \in \Qc_d}\sum_{q\in \Qc_d}\l[h(u^*_s(\overleftarrow{X}_{s}^{u^*}+\bar q,q) )-h(u^*_s(\overleftarrow{X}_{s}^{u^*},q) )\r]u^*_s(\overleftarrow{X}_{s}^{u^*},\bar q) \eqsp.
\end{align*}
Using the HJB equation \eqref{hjb} and the definition of $h$ and $u$, we obtain
\begin{align*}
    c_s&=\sum_{q\in \Qc_d}\log u^*_s(\overleftarrow{X}_{s}^{u^*},q)u^*_s(\overleftarrow{X}_{s}^{u^*},q)\l[\sum_{\bar q\in \Qc_d}u^*_s(\overleftarrow{X}_{s}^{u^*},\bar q)-\sum_{\bar q\in \Qc_d}u^*_s(\overleftarrow{X}_{s}^{u^*}+q,\bar q) \r]\\
    &+\sum_{q\in \Qc_d}\Bigg[u^*_s(\overleftarrow{X}_{s}^{u^*}+ q,-q)\log u^*_s(\overleftarrow{X}_{s}^{u^*}+q,-q)-u^*_s(\overleftarrow{X}_{s}^{u^*}+ q,-q)\\
    &\hspace{4cm}-u^*_s(\overleftarrow{X}_{s}^{u^*},q)\log u^*_s(\overleftarrow{X}_{s}^{u^*},q)+u^*_s(\overleftarrow{X}_{s}^{u^*},q)\Bigg]u^*_s(\overleftarrow{X}_{s}^{u^*},q)\\
    &= \sum_{q_\in \Qc_d} \Bigg[ \log u^*_s(\overleftarrow{X}_{s}^{u^*},q)(u^*_s(\overleftarrow{X}_{s}^{u^*},q))^2-\log u^*_s(\overleftarrow{X}_{s}^{u^*},q)u^*_s(\overleftarrow{X}_{s}^{u^*},q)u^*_s(\overleftarrow{X}_{s}^{u^*}+ q,-q)\\
    &\hspace{1cm}+\Bigg[u^*_s(\overleftarrow{X}_{s}^{u^*}+ q,-q)\log u^*_s(\overleftarrow{X}_{s}^{u^*}+q,-q)-u^*_s(\overleftarrow{X}_{s}^{u^*}+ q,-q)\\
    &\hspace{4cm}-u^*_s(\overleftarrow{X}_{s}^{u^*},q)\log u^*_s(\overleftarrow{X}_{s}^{u^*},q)+u^*_s(\overleftarrow{X}_{s}^{u^*},q)\Bigg]u^*_s(\overleftarrow{X}_{s}^{u^*},q)\Bigg] \eqsp.
\end{align*}
Note that for $(\overleftarrow{X}_{t}^{u^*},q) \in \Ac$, we have
\begin{align*}
    u^*_s(\overleftarrow{X}_{s}^{u^*}+ q,-q)=\rme^{V(s,\overleftarrow{X}_{s}^{u^*}+ q)-V(s,\overleftarrow{X}_{s}^{u^*})}=\dfrac{1}{\rme^{V(s,\overleftarrow{X}_{s}^{u^*})-V(s,\overleftarrow{X}_{s}^{u^*}+q)}}=\dfrac{1}{u^*_s(\overleftarrow{X}_{s}^{u^*},q)} \eqsp.
\end{align*}
Therefore,
\begin{align*}
    c_s&= \sum_{q_\in \Qc_d} \Bigg[ \log u^*_s(\overleftarrow{X}_{s}^{u^*},q)(u^*_s(\overleftarrow{X}_{s}^{u^*},q))^2-\log u^*_s(\overleftarrow{X}_{s}^{u^*},q)-\log u^*_s(\overleftarrow{X}_{s}^{u^*},q)\\
    &\hspace{4cm}-1-(u^*_s(\overleftarrow{X}_{s}^{u^*},q))^2\log u^*_s(\overleftarrow{X}_{s}^{u^*},q)+(u^*_s(\overleftarrow{X}_{s}^{u^*},q))^2\Bigg]\\
    &=\sum_{q\in \Qc_d}\l[(u^*_s(\overleftarrow{X}_{s}^{u^*},q))^2-2\log u^*_s(\overleftarrow{X}_{s}^{u^*},q)-1  \r] \eqsp.
\end{align*}
For simplicity, we write $u^*_s(\overleftarrow{X}_{s}^{u^*},q)=u$, then $c_s$ can be rewritten as
\begin{align*}
    c_s=\sum_{q\in \Qc_d}( u^2-2\log u-1) \eqsp.
\end{align*}
We now show that $c_s\ge 4f(s,\overleftarrow{X}_{s}^{u^*})$, meaning that $u^2-2\log u-1\ge 4(u\log u-u+1)$. To this purpose, we consider a function
\begin{align*}
    g(u)=u^2-2\log u-1-4(u\log u-u +1) \eqsp, \quad \text{ for } u>0 \eqsp.
\end{align*}
We have
\begin{align*}
    g'(u)=2u-\dfrac{2}{u}-4\log u \quad \text{and }\quad g''(u)=2+\dfrac{2}{u^2}-\dfrac{4}{u}=2(\dfrac{1}{u}-1 )^2\ge 0 \eqsp. 
\end{align*}
Thus, $g$ is a convex function. Combining this with the fact that $u=1$ is a solution of $g'$ implies that the minimum of $g$ is attained at $u=1$. Therefore $g(u)\ge g(1)=0$, for any $u>0$. This yields $c_s\ge 4f(s,\overleftarrow{X}_{s}^{u^*})$, which is our desired estimate.

Since $f(t,\overleftarrow{X}_{t}^{u^*})-f(0,\overleftarrow{X}_{0}^{u^*})-\int_0^t c_s\rmd s$ is a martingale, for $0\leq s\leq t \leq \Tf$, we have
\begin{align*}
    \E \l[f(t,\overleftarrow{X}_{t}^{u^*})-f(0,\overleftarrow{X}_{0}^{u^*})-\int_0^t c_r\rmd r \r]=\E\l[f(s,\overleftarrow{X}_{s}^{u^*})-f(0,\overleftarrow{X}_{0}^{u^*})-\int_0^s c_r \rmd r \r] \eqsp.
\end{align*}
It is equivalent to
\begin{align*}
    \E \l[f(t,\overleftarrow{X}_{t}^{u^*}) \r]=\E\l[f(s,\overleftarrow{X}_{s}^{u^*}) \r]+\E \l[\int_s^t c_r \rmd r \r]\eqsp.
\end{align*}
Denote $y_t:=\E \l[f(t,\overleftarrow{X}_{t}^{u^*}) \r]$. By the dominated convergence theorem and the fact that $c_r\ge 4f(r,\overleftarrow{X}_{r}^{u^*})$, we get
\begin{align*}
    y_t\ge y_s+\int_s^t 4y_r \rmd r \eqsp,
\end{align*}
which means $y'_t\ge 4y_t$. Combining this with Gr\"onwall's inequality implies our claim
\begin{align*}
    y_t \ge \rme^{4(t-s)}y_s \quad \text{or} \quad y_s \leq \rme^{-4(t-s)}y_t \eqsp, \quad \text{for } 0\leq s\leq t\leq \Tf \eqsp.
\end{align*}
\end{proof}


\subsection{Connection between the transition matrix and canonical process point of view}
%\np{Need to modify after knowing the explicit formula of forward matrix Q.}
 
As we see in previous sections, the time reversal process can be understood not only via the backward transition matrix but also via the process corresponding to the optimal control problem. The transition matrix point of view provides an approximation of the score to simulate the backward process, which is very useful in practice. In parallel, the canonical process point of view gives us a better understanding of the evolution of the time reversal process, which allows us to show a theoretical guarantee on our algorithm. These two points of view in fact have a strong relation, which will be specified in this section. 

\begin{proposition}\label{prop:connection}
    We see that the optimal control $u^*$ satisfies the following relation with respect to the score function defined in \eqref{eq:function_score_d} as 
    \begin{align}\label{approu}
    u^*_t(x,q)&=\begin{cases}
            1-s^\ell_t(x) \eqsp, \quad &\text{ if } (x,q) \in \Ac \eqsp,\\
            \hspace{0.5cm}0\eqsp,\quad  &\text{ otherwise \eqsp,} 
        \end{cases} \quad \text{with } \ell=1,\ldots,d \text{ such that } q^\ell \neq 0 \eqsp.
    \end{align}
    
\end{proposition}

\begin{proof}[Proof of \cref{prop:connection}]
    The proof relies on the fact that the function $\psi(t,x)=-\log\frac{\rmd\mu_{\Tf-t}}{\rmd\gamma^d}(x)$, for $(t,x)\in \ccint{0,\Tf}\times \msx$ is a solution to the HJB equation \eqref{hjb}. Indeed, for $(t,x) \in \ccint{0,\Tf}\times \msx$, we have that 
\begin{align*}
    \partial_t \psi(t,x)-\sum_{q\in \Qc_d}e^{\psi(t,x)-\psi(t,x+q)}\1_{\Ac}(x,q)&= \dfrac{\partial_t \mu_{\Tf-t}(x)}{\mu_{\Tf-t}(x)}-\dfrac{1}{\mu_{\Tf-t}(x)}\sum_{q\in \Qc_d} \mu_{\Tf-t}(x+q)\1_{\Ac}(x,q)\\
    &=\dfrac{1}{\mu_{\Tf-t}(x)}(\partial_t \mu_{\Tf-t}(x)-\sum_{q\in \Qc_d} \mu_{\Tf-t}(x+q)\1_{\Ac}(x,q) ) \eqsp.
\end{align*}
Using the formula of marginal distribution in \eqref{marginaldist}, we can compute the numerator as follows
\begin{align*}
    &\hspace{0.5cm}\partial_t \mu_{\Tf-t}(x)-\sum_{q\in \Qc_d} \mu_{\Tf-t}(x+q)\1_{\Ac}(x,q)\\
    &=\sum_{z\in \msx}\mu_0(z)\sum_{i=1}^d( \partial_t \ovr{p}^1_{\Tf-t}(z^i,x^i)\prod_{\substack{j=1\\j\neq i}}^d \overrightarrow{p}^1_{\Tf-t}(z^j,x^j))-\sum_{z\in \msx}\sum_{i=1}^d\mu_0(z)\overrightarrow{p}_{\Tf-t}(z,\varphi_i(x)) \eqsp,
\end{align*}
with the map $\varphi_i: \msx \to \msx$ defined for any $x\in \msx$ as the vector where the $i$-bit is flipped compared to $x$. Therefore
\begin{align*}
    &\hspace{0.5cm}\partial_t \mu_{\Tf-t}(x)-\sum_{q\in \Qc_d} \mu_{\Tf-t}(x+q)\1_{\Ac}(x,q)\\
    &=\sum_{z\in \msx}\mu_0(z)\sum_{i=1}^d (\dfrac{\partial_t \ovr{p}^1_{\Tf-t}(z^i,x^i)}{\ovr{p}^1_{\Tf-t}(z^i,x^i)}\overrightarrow{p}_{\Tf-t}(z,x)-\dfrac{\ovr{p}^1_{\Tf-t}(z^i,\varphi^i_i(x))}{\ovr{p}^1_{\Tf-t}(z^i,x^i)}\overrightarrow{p}^1_{\Tf-t}(z,x) )\\
    &=\sum_{z\in \msx}\mu_0(z)\overrightarrow{p}^1_{\Tf-t}(z,x)\sum_{i=1}^d \dfrac{\partial_t \ovr{p}^1_{\Tf-t}(z^i,x^i)-\ovr{p}^1_{\Tf-t}(z^i,\varphi^i_i(x))}{\ovr{p}^1_{\Tf-t}(z^i,x^i)} \eqsp.
\end{align*}
We know further that either $z^i=x^i$ or $z^i=\varphi_i^i(x)$, and in both cases, together with the formula of $\ovr{p}^1_{\Tf-t}$, we obtain that 
\begin{align*}
    \dfrac{\partial_t \ovr{p}^1_{\Tf-t}(z^i,x^i)-\ovr{p}^1_{\Tf-t}(z^i,\varphi_i^i(x))}{\ovr{p}^1_{\Tf-t}(z^i,x^i)}=-1 \eqsp.
\end{align*}
Replacing this into the previous equality, we get
\begin{align*}
    \partial_t \mu_{\Tf-t}(x)-\sum_{q\in \Qc_d} \mu_{\Tf-t}(x+q)\1_{\Ac}(x,q)=-d\mu_{\Tf-t}(x) \eqsp,
\end{align*}
and therefore 
\begin{align*}
     \partial_t \psi(t,x)-\sum_{q\in \Qc_d}\rme^{\psi(t,x)-\psi(t,x+q)}\1_{\Ac}(x,q)=-d \eqsp.
\end{align*}
Moreover, $\psi$ also satisfies the final condition
\begin{align*}
    \psi(\Tf,x)=-\log\dfrac{\rmd\mustar}{\rmd\gamma^d}(x)=g(x) \eqsp,
\end{align*}
meaning that $\varphi$ solves the HJB equation \eqref{hjb}.
Consequently, the optimal control is
\begin{align*}
    u^*_t(x,q)=\begin{cases}
            \dfrac{\mu_{\Tf-t}(x+q)}{\mu_{\Tf-t}(x)} \eqsp, \quad &\text{ if } (x,q)\in \Ac \eqsp,\\
            \hspace{0.5cm}0 \eqsp,\quad  &\text{ otherwise \eqsp. } 
        \end{cases}
\end{align*}
In other words, the optimal control admits the following formula
\begin{align*}
     u^*_t(x,q)=\begin{cases}
            \dfrac{\mu_{\Tf-t}(\varphi^{(\ell)}(x))}{\mu_{\Tf-t}(x)} \eqsp, \quad &\text{ if } (x,q)\in \Ac \eqsp,\\
            \hspace{0.5cm}0 \eqsp,\quad  &\text{ otherwise \eqsp, } 
        \end{cases} \quad \text{with } \ell=1,\ldots,d \text{ such that } q^\ell\neq 0 \eqsp.
\end{align*}
This implies the relation between the optimal control and the score function as
\begin{align*}
    u^*_t(x,q)&=\begin{cases}
            1-s^\ell_t(x) \eqsp, \quad &\text{ if } (x,q)\in \Ac \eqsp,\\
            \hspace{0.5cm}0 \eqsp,\quad  &\text{ otherwise \eqsp,} 
        \end{cases}\quad \text{with } \ell=1,\ldots,d \text{ such that } q^\ell\neq 0 \eqsp,
\end{align*}
which means the transition matrix and the canonical process point of view are equivalent.
\end{proof}

\subsection{Convergence of DMPMs}\label{proof_theo:5}
Based on the canonical process point of view, we can characterize the backward evolution using martingale and optimal control problems. These tools equip us to prove the error's bound in \cref{theo:5}.

\begin{proof}[Proof of \cref{theo:5}]
We show first the bound for the "distance" between the backward path measure in the continuous time $\overleftarrow{\P}^{\mustar}$ of $\overleftarrow{X}^{u^*}$ (we write $\overleftarrow{\P}$ for short) and the path measure $\ovl{\P}^\star$ of the simulated backward process $\baX^\star$ generated in \cref{alg:backward_approximation_continuous}. By Girsanov's theorem \ref{girsanovtheorem}, we have
\begin{align*}
    \dfrac{\rmd\overleftarrow{\P}}{\rmd\overrightarrow{R}}=\dfrac{\rmd\overleftarrow{\P}_0}{\rmd\overrightarrow{R}_0}(\overleftarrow{X}_0) \exp\l( \log u_t^* \odot \tilde \mu ^L-\int_{\ccint{0,\Tf}}\sum_{q\in \Qc_d}\varrho(\log u_t^*)\rmd\bar L\r) \eqsp.
\end{align*}
With a partition $0=t_0<...<t_K=\Tf$ for $M \ge 1$ of $\ccint{0,\Tf}$ and $\tau=\max \{h_k, k=1,\ldots,K\}$
, the previous expression implies
\begin{align*}
    \dfrac{\rmd\overleftarrow{\P}}{\rmd\overrightarrow{R}}=\dfrac{\rmd\overleftarrow{\P}_0}{\rmd\overrightarrow{R}_0}(\overleftarrow{X}_0) \exp\sum_{k=0}^{K-1}\l( \int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d} \log u_t^* \rmd\tilde \mu ^L-\int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d}\varrho(\log u_t^*)\rmd\bar L\r) \eqsp.
\end{align*}
Apply Girsanov's theorem \ref{girsanovtheorem} again for the path measure $\ovl{\P}^\star$ of the process $\baX^\star$ in \eqref{alg:backward_approximation_continuous}, we obtain
\begin{align*}
    \dfrac{\rmd{\ovl{\P}^\star}}{\rmd\overrightarrow{R}}=\dfrac{\rmd{\ovl{\P}_0^\star}}{\rmd\overrightarrow{R}_0}(\overleftarrow{X}_0) \exp\sum_{k=0}^{K-1}\l( \int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d} \log u_{t_k}^{\theta^\star} \rmd\tilde \mu ^L-\int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d}\varrho(\log u_{t_k}^{\theta^\star})\rmd\bar L\r) \eqsp.
\end{align*}
Combining the previous quantities, we see that
\begin{align*}
    \dfrac{\rmd\overleftarrow{\P}}{\rmd\ovl{\P}^\star}=\dfrac{\rmd\overleftarrow{\P_0}}{\rmd\ovl{\P}_0^\star}(\overleftarrow{X}_0)\exp\sum_{k=0}^{K-1}\l( \int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d} (\log u_t-\log u_{t_k}^{\theta^\star}) \rmd\tilde \mu ^L-\int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d}(\varrho(\log u_{t})-\varrho(\log u_{t_k}^{\theta^\star}))\rmd\bar L\r) \eqsp.
\end{align*}
This leads to the following expression of the relative entropy
\begin{align*}
    & \KL(\overleftarrow{\P}| \ovl{\P}^\star)=\KL(\mu_{\Tf}|\gamma^d)+\sum_{k=0}^{K-1}\E_{\overleftarrow{\P}}\Bigg[\int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d} (\log u_t-\log u_{t_k}^{\theta^\star}) \rmd\tilde \mu ^L\\
    &\hspace{5cm}-\int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d}(\varrho(\log u_{t})-\varrho(\log u_{t_k}^{\theta^\star}))\rmd\bar L \Bigg] \eqsp.
\end{align*}
Using equation \eqref{eq:k} and the definition of  $\varrho$, we derive
\begin{align}\label{eq:3}
    \KL(\overleftarrow{\P}|  \ovl{\P}^\star)&=\KL(\mu_{\Tf}|\gamma^d)+\sum_{k=0}^{K-1}\E_{\overleftarrow{\P}} \Bigg[ \int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d}\Bigg((\log u_t-\log u_{t_k}^{\theta^\star})(u_t-1)\nonumber\\
    &\hspace{4cm}-(u_t-\log u_t-1)+(u_{t_k}^{\theta^\star}-\log u_{t_k}^{\theta^\star}-1)\Bigg)\rmd\bar L \Bigg]\nonumber\\
    &=\KL(\mu_{\Tf}|\gamma^d)+\sum_{k=0}^{K-1}\E_{\overleftarrow{\P}} \l[ \int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d} \l(\dfrac{u_t}{u_{t_k}^{\theta^\star}}\log \dfrac{u_t}{u_{t_k}^{\theta^\star}}-\dfrac{u_t}{u_{t_k}^{\theta^\star}}+1 \r)u_{t_k}^{\theta^\star}\rmd\bar L \r]\nonumber\\
    &=\KL(\mu_{\Tf}|\gamma^d)+\sum_{k=0}^{K-1}\E_{\overleftarrow{\P}} \l[ \int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d} u_{t_k}^{\theta^\star}h\l(\dfrac{u_t}{u_{t_k}^{\theta^\star}}\r)\rmd t\r]\nonumber\\
    &= \KL(\mu_{\Tf}|\gamma^d)+\sum_{k=0}^{K-1}\E_{\overleftarrow{\P}} \l[ \int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d} u_{t_k}h\l(\dfrac{u_t}{u_{t_k}}\r)\rmd t\r]\nonumber\\
    &\hspace{2.3cm}+\sum_{k=0}^{K-1}\E_{\overleftarrow{\P}} \l[ \int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d} \l(u_{t_k}^{\theta^\star}h\l(\dfrac{u_t}{u_{t_k}^{\theta^\star}}\r)-u_{t_k}h\l(\dfrac{u_t}{u_{t_k}}\r)\r)\rmd t\r] \eqsp.
\end{align}
By definition of the function $h$ and the tower property, the last term can be computed as
\begin{align*}
    I:&=\sum_{k=0}^{K-1}\E_{\overleftarrow{\P}} \l[ \int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d} \l(u_{t_k}^{\theta^\star}h\l(\dfrac{u_t}{u_{t_k}^{\theta^\star}}\r)-u_{t_k}h\l(\dfrac{u_t}{u_{t_k}}\r)\r)\rmd t\r]\\
   & = \sum_{k=0}^{K-1}\E_{\overleftarrow{\P}} \l[ \int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d} \l(u_t\log \dfrac{u_{t_k}}{u_{t_k}^{\theta^\star}}+u_{t_k}^{\theta^\star}-u_{t_k}\r)\rmd t \r]\\
    &=\sum_{k=0}^{K-1}\E_{\overleftarrow{\P}} \l[ \int_{[t_k,t_{k+1}]} \E_{\overleftarrow{\P}}\l[\sum_{q\in \Qc_d}u_t\log \dfrac{u_{t_k}}{u_{t_k}^{\theta^\star}}\middle|\Fc_{t_k}\r]+\sum_{q\in \Qc_d}(u_{t_k}^{\theta^\star}-u_{t_k}) \rmd t\r] \eqsp,
\end{align*}
with $\Fc_{t_k}$ the $\sigma$-algebra of $\baX_{t_k}$. \cref{prop:3} implies that $\sum_{q\in \Qc_d}u_{t}$ is a $\overleftarrow{\P}$-martingale, hence
\begin{align*}
    I&=\sum_{k=0}^{K-1}\E_{\overleftarrow{\P}} \l[ \int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d} \l(u_{t_k}\log \dfrac{u_{t_k}}{u_{t_k}^{\theta^\star}}+u_{t_k}^{\theta^\star}-u_{t_k}\r)\rmd t \r]\\
    &=\sum_{k=0}^{K-1}\E_{\overleftarrow{\P}} \l[ \int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d}u_{t_k}^{\theta^\star}h\l(\dfrac{u_{t_k}}{u_{t_k}^{\theta^\star}} \r)\rmd t\r]\\
    &= \sum_{k=0}^{K-1}(t_{k+1}-t_k)\E_{\overleftarrow{\P}} \l[ \sum_{q\in \Qc_d}u_{t_k}^{\theta^\star}h\l(\dfrac{u_{t_k}}{u_{t_k}^{\theta^\star}} \r)\r] \eqsp.
\end{align*}

\cref{ass:approx_score} combined with the fact that the backward partition $(t_k)_{k=0}^K$ is associated with the forward sampling imply
\begin{align*}
    I&\leq \epsilon\sum_{k=0}^{K-1}(t_{k+1}-t_k)=\epsilon (t_N-t_0)=\epsilon\Tf \eqsp,
\end{align*}
since $ \sum_{k=0}^{K-1}(t_{k+1}-t_k)$ is a telescoping sum. Replacing this into \eqref{eq:3} yields
\begin{align*}
    \KL(\overleftarrow{\P}| \ovl{\P}^\star)\leq \KL(\mu_{\Tf}|\gamma^d)+\sum_{k=0}^{K-1}\E_{\overleftarrow{\P}} \l[ \int_{[t_k,t_{k+1}]}\sum_{q\in \Qc_d} u_{t_k}h\l(\dfrac{u_t}{u_{t_k}}\r)\rmd t\r]+\epsilon\Tf \eqsp.
\end{align*}

We now use the tower property of conditional expectations combined with the monotonicity showed in \cref{prop:3} to bound the second term above as follows
\begin{align*}
    \KL(\overleftarrow{\P}| \ovl{\P}^\star)&\leq \KL(\mu_{\Tf}|\gamma^d)+\sum_{k=0}^{K-1}\E_{\overleftarrow{\P}} \l[ \int_{[t_k,t_{k+1}]} \E_{\overleftarrow{\P}}\l[\sum_{q\in \Qc_d}u_{t_k} h\l(\dfrac{u_t}{u_{t_k}}\r)\Bigg|\Fc_{t_k}\r]\rmd t\r]+\epsilon \Tf\\
    &\leq \KL(\mu_{\Tf}|\gamma^d)+\sum_{k=0}^{K-1}\E_{\overleftarrow{\P}} \l[ \int_{[t_k,t_{k+1}]}\E_{\overleftarrow{\P}}\l[\sum_{q\in \Qc_d} u_{t_k} h\l(\dfrac{u_{t_{k+1}}}{u_{t_k}}\Bigg|\Fc_{t_k}\r)\r]\rmd t\r]+\epsilon \Tf\\
    &\leq \KL(\mu_{\Tf}|\gamma^d)+ \sum_{k=0}^{K-1}(t_{k+1}-t_k)\E_{\overleftarrow{\P}}\l[\sum_{q\in \Qc_d}u_{t_k}h\l(\dfrac{u_{t_{k+1}}}{u_{t_k}}\r) \r]+\epsilon \Tf\\
    &\leq \KL(\mu_{\Tf}|\gamma^d)+\tau \sum_{k=0}^{K-1}\E_{\overleftarrow{\P}}\l[\sum_{q\in \Qc_d}u_{t_k}h\l(\dfrac{u_{t_{k+1}}}{u_{t_k}}\r) \r]+\epsilon \Tf \eqsp,
\end{align*}
where the last inequality comes from the fact that $\tau=\max \{h_k, k=1,\ldots,K\}$. By definition of the function $h$, we have
\begin{align*}
    h\l(\dfrac{u_{t_{k+1}}}{u_{t_k}}\r) =\dfrac{1}{u_{t_k}}\l(h(u_{t_{k+1}})-h(u_{t_k})-(u_{t_k}-u_{t_{k+1}})\log (u_{t_k}) \r) \eqsp,
\end{align*}
which leads to
\begin{align*}
    \KL(\overleftarrow{\P}| \ovl{\P}^\star)&\leq \KL(\mu_{\Tf}|\gamma^d)+ \tau\sum_{k=0}^{K-1} \l( \E_{\overleftarrow{\P}}\l[\sum_{q\in \Qc_d}h(u_{t_{k+1}})\r]-\E_{\overleftarrow{\P}}\l[\sum_{q\in \Qc_d}h(u_{t_{k}})\r]\r)\\
    &\hspace{3cm}-\tau\sum_{k=0}^{K-1} \l( \E_{\overleftarrow{\P}}\l[\sum_{q\in \Qc_d} (u_{t_k}-u_{t_{k+1}})\log (u_{t_k}) \r]\r)+\epsilon \Tf \eqsp.
\end{align*} 

Using the tower property and the fact that $\sum_{q\in \Qc_d}u_{t}$ is a $\overleftarrow{\P}$-martingale proved in Proposition \ref{prop:3}, we obtain that
\begin{align*}
     \E_{\overleftarrow{\P}}\l[\sum_{q\in \Qc_d} (u_{t_k}-u_{t_{k+1}})\log (u_{t_k}) \r]&=  \E_{\overleftarrow{\P}}\l[ \l(\sum_{q\in \Qc_d}u_{t_k}\log (u_{t_k})-\E_{\overleftarrow{\P}}\l[\sum_{q\in \Qc_d}u_{t_{k+1}}\log (u_{t_k})|\Fc_{t_k}\r]\r) \r]=0 \eqsp.
\end{align*}
Combine this with the fact that $\gamma^d$ is the invariant measure of the forward process, that satisfies a log-Sobolev inequality and therefore an exponential entropy decays \citep[Theorem 5.2.1]{bakry2014analysis}, we get
\begin{align*}
     \KL(\overleftarrow{\P}| \ovl{\P}^\star)&\leq \rme^{-\Tf}\KL(\mustar|\gamma^d)+\tau \sum_{k=0}^{K-1}\l( \E_{\overleftarrow{\P}}\l[\sum_{q\in \Qc_d}h(u_{t_{k+1}})\r]-\E_{\overleftarrow{\P}}\l[\sum_{q\in \Qc_d}h(u_{t_{k}})\r]\r)+\epsilon \Tf \eqsp.
\end{align*}
We obtain a telescoping sum on the right hand side, which yields
\begin{align*}
    \KL(\overleftarrow{\P}| \ovl{\P}^\star)&\leq \rme^{-\Tf}\KL(\mustar|\gamma^d)+\tau \l( \E_{\overleftarrow{\P}}\l[\sum_{q\in \Qc_d}h(u_{t_{M}})\r]-\E_{\overleftarrow{\P}}\l[\sum_{q\in \Qc_d}h(u_{t_{0}})\r]\r)+\epsilon \Tf\\
     &=\rme^{-\Tf}\KL(\mustar|\gamma^d)+\tau \l( \E_{\overleftarrow{\P}}\l[\sum_{q\in \Qc_d}h(u_{\Tf})\r]-\E_{\overleftarrow{\P}}\l[\sum_{q\in \Qc_d}h(u_{0})\r]\r)+\epsilon \Tf \eqsp.
\end{align*}
Since $h \ge 0$, we obtain
\begin{align}\label{eq:5}
     \KL(\overleftarrow{\P}| \ovl{\P}^\star)&\leq \rme^{-\Tf}\KL(\mustar|\gamma^d)+\tau \E_{\overleftarrow{\P}}\l[\sum_{q\in \Qc_d}h(u_{\Tf})\r]+\epsilon \Tf\nonumber\\
     &=\rme^{-\Tf}\KL(\mustar|\gamma^d)+\tau \beta_{\gamma^d}(\mustar)+\epsilon \Tf \eqsp. 
\end{align}
Finally, notice that $\mustar=\mathrm{Law}(\overleftarrow{X}^{u^*}_T)$, therefore
\begin{align*}
    \KL(\mustar|\mathrm{Law}(\baX^\star_{\Tf} ) )=\KL(\mathrm{Law}(\overleftarrow{X}^{u^*}_{\Tf})|\mathrm{Law}(X^\star_{\Tf}) ) \leq \KL(\mathrm{Law}(\overleftarrow{X}^{u^*}_.)|\mathrm{Law}(\baX^\star_.) )= \KL(\overleftarrow{\P}| \ovl{\P}^\star) \eqsp,
\end{align*}
where the inequality is known as \textit{Data processing} inequality for relative entropy \citep[Lemma 1.6]{nutz2021introduction}. Combining this with \eqref{eq:5}, we can conclude that
\begin{align*}
    \KL(\mustar|\mathrm{Law}(\baX^\star_{\Tf} ) ) \leq \rme^{-\Tf}\KL(\mustar|\gamma^d)+\tau \beta_{\gamma^d}(\mustar)+\epsilon \Tf \eqsp,
\end{align*}
and finish the proof of the convergence bound.
\end{proof}

\subsection{Convergence of DMPMs with early stopping strategy}
\begin{proof}[Proof of \cref{prop:bound_tv_mustar_and_noise}]\label{proof_prop:bound_tv_mustar_and_noise}
Recall the definition of total variation distance of $\mu_\eta$ and $\mustar$ for any $\eta>0$ is
\begin{equation*}
    \tvnorm{\mu_\eta - \mustar} = \sum_{x\in \msx} |\mu_\eta(x)-\mustar(x)| \eqsp.
\end{equation*}
By the triangle inequality, we obtain 
\begin{equation*}
    \tvnorm{\mu_\eta - \mustar} \leq \sum_{x\in \msx} |\mu_\eta(x) -\mustar(x)\ovr{p}_\eta(x,x)|+|\mustar(x)-\mustar(x)\ovr{p}_\eta(x,x)| \eqsp,
\end{equation*}
where the transition probability $\ovr{p}_\eta$ is defined in \eqref{eq:transition_d}. The two terms above are nonnegative as 
\begin{equation*}
    \mu_\eta(x) = \sum_{z \in \msx} \mustar(z) \ovr{p}_\eta (z,x) \geq \mustar(x) \ovr{p}_\eta (x,x) \eqsp,
\end{equation*}
and the transition probability $\ovr{p}_\eta(x,x) \leq 1$ for any $x\in \msx$. This together with the formula of $\ovr{p}_\eta$ in \eqref{eq:transition_d} yield
\begin{align*}
    \tvnorm{\mu_\eta - \mustar} &\leq \sum_{x \in \msx} \l[\mu_\eta(x)+\mustar(x)-2\mustar(x)\l(\frac{1}{2}+\frac{1}{2}\rme^{-2\lambda\eta}\r)^d \r]\\
    &\leq 2-2\l(\frac{1}{2}+\frac{1}{2}\rme^{-2\lambda\eta}\r)^d \eqsp.
\end{align*}
To simplify this upper bound, we use the exponential inequality for $\rme^{-2\lambda\eta}$ as follows
\begin{align*}
    \tvnorm{\mu_\eta - \mustar} \leq 2-2\l(\frac{1}{2}+\frac{1}{2}(-2\lambda\eta+1)\r)^d = 2-2(1-\lambda\eta)^d \eqsp,
\end{align*}
and the proof is completed.


\end{proof}

\begin{proof}[Proof of \cref{coro:early_stopping}]\label{proof_coro:early_stopping}
% This proof is a consequence of \cref{theo:early_stopping} and \cref{prop:bound_tv_mustar_and_noise}. 
We evaluate first the behavior of the Fisher-like information at time $\eta$ as follows
\begin{align*}
    \beta_{\gamma^d}(\mu_\eta) &=\E\l[ \sum_{\ell=1}^d h\l(\rme^{-\log\l(\frac{\rmd \mu_\eta}{\rmd \gamma^d}(\foX_{\eta})\r)+\log\l(\frac{\rmd\mu_\eta}{\rmd \gamma^d}(\varphi^{(\ell)}(\foX_\eta))\r)}\r) \r]\\
    &= \E\l[ \sum_{\ell=1}^d h\l(\rme^{\log\l(\frac{\rmd \mu_\eta(\varphi^{(\ell)}(\foX_\eta))}{\rmd \mu_\eta(\foX_{\eta})}\r)} \r)\r]\\
    &= \E \l[\sum_{\ell=1}^d h\l( \frac{\rmd \mu_\eta(\varphi^{(\ell)}(\foX_\eta))}{\rmd \mu_\eta(\foX_{\eta})}\r)\r] \eqsp.
\end{align*}
By definition of the function $h$, we have
\begin{align*}
    \beta_{\gamma^d}(\mu_\eta) &= \E \l[\sum_{\ell=1}^d \l( \frac{\rmd \mu_\eta(\varphi^{(\ell)}(\foX_\eta))}{\rmd \mu_\eta(\foX_{\eta})}\log \frac{\rmd \mu_\eta(\varphi^{(\ell)}(\foX_\eta))}{\rmd \mu_\eta(\foX_{\eta})}-\frac{\rmd \mu_\eta(\varphi^{(\ell)}(\foX_\eta))}{\rmd \mu_\eta(\foX_{\eta})}+1\r)\r]\\
    &\leq \E \l[\sum_{\ell=1}^d \l( \frac{1}{\rmd \mu_\eta(\foX_{\eta})}\log \frac{1}{\rmd \mu_\eta(\foX_{\eta})}+1\r)\r]\\
    &= d+d\sum_{x\in \msx} \log\frac{1}{\rmd\mu_\eta(x)} \eqsp.
\end{align*}
Using the inequality $\log\frac{1}{\rmd\mu_\eta(x)}\leq \frac{1}{\rmd\mu_\eta(x)}-1$, we obtain
\begin{align*}
    \beta_{\gamma^d}(\mu_\eta) &\leq d+d\sum_{x\in \msx}\l(\dfrac{1}{\rmd\mu_\eta(x)}-1\r) \leq d\sum_{x\in \msx} \frac{1}{\rmd\mu_\eta(x)} \eqsp.
\end{align*}
Replacing the marginal distribution $\mu_\eta$ above by \eqref{marginaldist} and note that $\ovr{p}^1_t(x,y) \geq \frac{1}{2}-\frac{1}{2}\rme^{-2\lambda t}$ for any $t \in (0,\Tf)$ and $x,y \in \{0,1\}$, we get
\begin{align*}
    \beta_{\gamma^d}(\mu_\eta) &\leq d\sum_{x\in \msx} \frac{1}{\sum_{z\in \msx}\mustar(z)\l(\frac{1}{2}-\frac{1}{2}\rme^{-2\lambda\eta}\r)^d} = d\sum_{x\in \msx} \frac{1}{\l(\frac{1}{2}-\frac{1}{2}\rme^{-2\lambda\eta}\r)^d} = \frac{2^{2d}d}{(1-\rme^{-2\lambda\eta})^d} \eqsp.
\end{align*}
We now use the exponential inequality to arrive at
\begin{align*}
    \beta_{\gamma^d}(\mu_\eta) \leq \frac{2^{2d}d}{\l(\frac{2\lambda\eta}{2\lambda\eta+1}\r)^d} = \frac{2^d d(2\lambda\eta+1)^d}{(\lambda\eta)^d} \eqsp.
\end{align*}
Multiplying both hand sides with the fixed step size $\bh$ conditioned by \eqref{cond:early_stopping} implies
\begin{align}\label{eq:8}
    \bh \beta_{\gamma^d}(\mu_\eta) \leq \frac{\varespilon^2}{2} \eqsp.
\end{align}

Return to the main estimate of \cref{coro:early_stopping} and apply the triangle inequality, we have
\begin{align*}
    \tvnorm{\mustar- \mathrm{Law}(\overleftarrow{X}_{\Tf-\eta}^\star ) } 
    &\leq \tvnorm{\mustar - \mu_\eta } + \tvnorm{\mu_\eta - \mathrm{Law}(\overleftarrow{X}_{\Tf-\eta}^\star )} \\
    &\leq \tvnorm{\mustar - \mu_\eta } + \sqrt{2\KL(\mu_\eta| \mathrm{Law}(\overleftarrow{X}_{\Tf-\eta}^\star ))}  \eqsp,
\end{align*}
where the last line used Pinsker's inequality. Then using \cref{theo:early_stopping} and \cref{prop:bound_tv_mustar_and_noise} implies
\begin{align}\label{eq:9}
    \tvnorm{\mustar- \mathrm{Law}(\overleftarrow{X}_{\Tf-\eta}^\star ) } \leq  2-2(1-\lambda \eta)^d + \sqrt{2\rme^{-\Tf}\KL(\mustar|\gamma^d)} + \sqrt{2\bh \beta_{\gamma^d}(\mu_{\eta})}+\sqrt{2\epsilon(\Tf-\eta)} \eqsp.
\end{align}
Furthermore, the conditions of $\eta$ and $\bK_f$ in \eqref{cond:early_stopping} follow
\begin{equation}\label{eq:10}
    2-2(1-\lambda\eta)^d \leq \varespilon \quad \text{and } \quad {2\rme^{-\Tf}\KL(\mustar|\gamma^d)} \leq  \frac{\varespilon}{2} \eqsp.
\end{equation}
Finally, substituting estimates \eqref{eq:8} and  \eqref{eq:10} into \eqref{eq:9} yields the desired claim
\begin{align*}
    \tvnorm{\mustar- \mathrm{Law}(\overleftarrow{X}_{\Tf-\eta}^\star ) } \leq 2\varespilon+\sqrt{2\epsilon\Tf} \eqsp,
\end{align*}
and the proof is finished.
\end{proof}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
