\section{Related Works}
\label{Related}
\textbf{Adversarial robustness} \quad To defend against adversarial attacks, researchers have developed various techniques aimed at enhancing the robustness of DNNs.
\citet{goodfellow2014explaining} propose AT technique to defend against adversarial attacks by retraining classifiers using adversarial examples. In contrast, AP methods \citep{shi2021online,srinivasan2021robustifying} aim to purify adversarial examples before classification without retraining the classifier. Currently, the most common AP methods \citep{nie2022diffusion,bai2024diffusion} rely on pre-trained generative models as purifiers, which are trained on specific datasets and hard to generalize to data distributions outside their training domain. \citet{lin2024adversarial} propose applying AT technique to AP, optimizing the purifier to adapt to new data distributions.
Although our framework employs AP strategy, it fundamentally deviates from these works as we develop a model-free framework that relies solely on the information of the input example for adversarial purification, without requiring any additional priors from pre-trained models or training the purifiers.


\textbf{Tensor network and TN-based defense methods} \quad Tensor network (TN) is a classical tool in signal processing, with many successful applications in image completion and denoising \citep{kolda2009tensor,cichocki2015tensor}.
Compared to traditional TN methods such as TT \citep{oseledets2011tensor} and TR \citep{zhao2016tensor}, we employ the quantized technique \citep{khoromskij2011d} and develop a coarse-to-fine strategy. Recently, PuTT \citep{loeschcke2024coarse} also employs a coarse-to-fine strategy, aiming to achieve better initialization for faster and more efficient TT decomposition by minimizing the reconstruction error.
In comparison, our method progresses from low to high resolution, explicitly targeting perturbation removal and analyzing the impact of downsampling on perturbations. Furthermore, we propose a novel optimization objective that goes beyond simply minimizing the reconstruction error, focusing instead on preventing the reappearance of adversarial perturbations.

With the growing concern over adversarial robustness, a line of work has attempted to leverage TNs as robust denoisers to defend against adversarial attacks. In particular, \citet{yang2019me} reconstruct images and retrain classifiers to adapt to the new reconstructed distribution. \citet{entezari2022tensorshield} analyze vanilla TNs and show their effectiveness in removing high-frequency perturbations.
Additionally, \citep{bhattarai2023robust} extend the application of TNs beyond data to include classifiers, a concept similar to the approaches of \citep{rudkiewicz2024robustness,phan2023cstar}. Furthermore, \citep{song2024training} employ training-free techniques while incorporating ground truth information to defend against adversarial attacks. However, the aforementioned methods rely on additional prior or are limited to specific attacks.
In this paper, we aim to achieve robustness solely by optimizing TNs themselves, establishing them as a plug-and-play and promising adversarial purification technique.