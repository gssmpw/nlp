
\begin{abstract}

Deep neural networks are known to be vulnerable to well-designed adversarial attacks. Although numerous defense strategies have been proposed, many are tailored to the specific attacks or tasks and often fail to generalize across diverse scenarios.
In this paper, we propose Tensor Network Purification (TNP), a novel model-free adversarial purification method by a specially designed tensor network decomposition algorithm. TNP depends neither on the pre-trained generative model nor the specific dataset, resulting in strong robustness across diverse adversarial scenarios.
To this end, the key challenge lies in relaxing Gaussian-noise assumptions of classical decompositions and accommodating the unknown distribution of adversarial perturbations. Unlike the low-rank representation of classical decompositions, TNP aims to reconstruct the unobserved clean examples from an adversarial example. Specifically, TNP leverages progressive downsampling and introduces a novel adversarial optimization objective to address the challenge of minimizing reconstruction error but without inadvertently restoring adversarial perturbations.
Extensive experiments conducted on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method generalizes effectively across various norm threats, attack types, and tasks, providing a versatile and promising adversarial purification technique.

\end{abstract}

\section{Introduction}
\label{Introduction}
Deep neural networks (DNNs) have achieved remarkable success across a wide range of tasks. However, DNNs have been shown to be vulnerable to adversarial examples \citep{szegedy2013intriguing,goodfellow2014explaining}, which are generated by adding small, human-imperceptible perturbations to natural images but completely change the prediction results to DNNs, leading to disastrous implications. The vulnerability of DNNs to such examples highlights the significance of robust defense mechanisms to mitigate adversarial attacks effectively.

Since then, numerous methods have been proposed to defend against adversarial examples.
Notably, adversarial training \citep[AT,][]{goodfellow2014explaining} typically aims to retrain DNNs using adversarial examples, achieving robustness over seen types of adversarial attacks but performing poorly against unseen perturbations \citep{laidlaw2021perceptual,dolatabadi2022}.
Another class of defense methods is adversarial purification \citep[AP,][]{yoon2021adversarial}, which leverages pre-trained generative models to remove adversarial perturbations and demonstrates better generalization than AT against unseen attacks \citep{nie2022diffusion,lin2024adversarial}. However, AP methods rely on pre-trained models tailored to specific datasets, limiting their transferability to different data distributions and tasks.
Thus, both mainstream techniques face generalization challenges: AT struggles with diverse norm threats, and AP with task generalization, restricting their applicability to broader scenarios.

To tackle these challenges, we propose a novel model-free adversarial purification method by a specially designed tensor network decomposition algorithm, termed Tensor Network Purification (TNP), which bridges the gap between low-rank tensor network representation with Gaussian noise assumption and removal of adversarial perturbations with unknown distributions.
As a model-free optimization technique \citep{oseledets2011tensor,zhao2016tensor}, TNP depends neither on any pre-trained generative model nor specific dataset, enabling it to achieve strong robustness across diverse adversarial scenarios.
Additionally, TNP can eliminate potential adversarial perturbations for both clean or adversarial examples before feeding them into the classifier \citep{yoon2021adversarial}. As a pre-processing step, TNP can defend against adversarial attacks without retraining the classifier model or pretraining the generative model. Moreover, since our method is an algorithm applied to input examples only and has no fixed model parameters, it is more difficult to be attacked by existing black or white box adversarial attack techniques.
Consequently, benefiting from the aforementioned advantages, it is evident that TN-based AP methods represent a highly promising direction, offering the transferability to be effectively applied across a wide range of adversarial scenarios.

The existing TN methods are particularly favorable for image completion and denoising when the noise is sparse or follows Gaussian distribution as long as it can be modeled explicitly.
However, the distribution of well-designed adversarial perturbations fundamentally differs from such noises and often aligns with the statistics of the data \citep{ilyas2019adversarial,allen2022feature}.
Consequently, these perturbations behave more like features than noise, making them challenging to be modeled explicitly and prone to being inadvertently reconstructed.
To address this issue, we first explore the distribution changes of perturbations during the optimization process and initially mitigate the impact of perturbations by progressive downsampling. 
Correspondingly, we propose an algorithm for TN incremental learning in a coarse-to-fine manner. Furthermore, a novel adversarial optimization objective is proposed to address the challenge of minimizing reconstruction error but without inadvertently restoring adversarial perturbations.
Unlike classical TN, given an adversarial example, TNP prevents naive low-rank representation of the input and encourages the reconstructed examples to approximate the unobserved clean examples.


We empirically evaluate the performance of our method by comparing it with AT and AP methods across attack settings using multiple classifier models on CIFAR-10, CIFAR-100, and ImageNet.
The results demonstrate that our method achieves state-of-the-art performance with robust generalization across diverse adversarial scenarios.
Specifically, our method achieved a 26.45\% improvement in average robust accuracy over AT across different norm threats, a 9.39\% improvement over AP across multiple attacks, and a 6.47\% improvement over AP across different datasets.
Furthermore, in denoising tasks, our method effectively removes adversarial perturbations while preserving consistency between the reconstructed clean example and the reconstructed adversarial example.
These results collectively underscore the effectiveness and potential of our proposed method.
In summary, our contributions are as follows:
\begin{itemize}
    \item We introduce a model-free adversarial purification framework based on tensor network representation, which eliminates the need for training a powerful generative model or relying on specific dataset distributions, making it a general-purpose adversarial purification.
    \item Based on our analysis of the distribution changes of adversarial perturbations during optimization, we propose a novel adversarial optimization objective for coarse-to-fine TN representation learning to prevent the restoration of adversarial perturbations.
    \item We conduct extensive experiments on various datasets, demonstrating that our method achieves state-of-the-art performance, especially exhibiting robust generalization across diverse adversarial scenarios.
\end{itemize}

\begin{figure*}[t]
\vskip 0.2in
    \centering
    \includegraphics[width=\linewidth]{figures/fig_distribution.pdf}
    \caption{Compare the adversarial perturbations in the downsampled images. (a) The distribution changes of adversarial perturbations during downsampling process. More results are shown in \Cref{app:distribution}. (b) The KL divergence histogram of adversarial perturbations.}
    \label{fig:distribution}
    \vskip -0.1in
\end{figure*}

\section{Related Works}
\label{Related}
\textbf{Adversarial robustness} \quad To defend against adversarial attacks, researchers have developed various techniques aimed at enhancing the robustness of DNNs.
\citet{goodfellow2014explaining} propose AT technique to defend against adversarial attacks by retraining classifiers using adversarial examples. In contrast, AP methods \citep{shi2021online,srinivasan2021robustifying} aim to purify adversarial examples before classification without retraining the classifier. Currently, the most common AP methods \citep{nie2022diffusion,bai2024diffusion} rely on pre-trained generative models as purifiers, which are trained on specific datasets and hard to generalize to data distributions outside their training domain. \citet{lin2024adversarial} propose applying AT technique to AP, optimizing the purifier to adapt to new data distributions.
Although our framework employs AP strategy, it fundamentally deviates from these works as we develop a model-free framework that relies solely on the information of the input example for adversarial purification, without requiring any additional priors from pre-trained models or training the purifiers.


\textbf{Tensor network and TN-based defense methods} \quad Tensor network (TN) is a classical tool in signal processing, with many successful applications in image completion and denoising \citep{kolda2009tensor,cichocki2015tensor}.
Compared to traditional TN methods such as TT \citep{oseledets2011tensor} and TR \citep{zhao2016tensor}, we employ the quantized technique \citep{khoromskij2011d} and develop a coarse-to-fine strategy. Recently, PuTT \citep{loeschcke2024coarse} also employs a coarse-to-fine strategy, aiming to achieve better initialization for faster and more efficient TT decomposition by minimizing the reconstruction error.
In comparison, our method progresses from low to high resolution, explicitly targeting perturbation removal and analyzing the impact of downsampling on perturbations. Furthermore, we propose a novel optimization objective that goes beyond simply minimizing the reconstruction error, focusing instead on preventing the reappearance of adversarial perturbations.

With the growing concern over adversarial robustness, a line of work has attempted to leverage TNs as robust denoisers to defend against adversarial attacks. In particular, \citet{yang2019me} reconstruct images and retrain classifiers to adapt to the new reconstructed distribution. \citet{entezari2022tensorshield} analyze vanilla TNs and show their effectiveness in removing high-frequency perturbations.
Additionally, \citep{bhattarai2023robust} extend the application of TNs beyond data to include classifiers, a concept similar to the approaches of \citep{rudkiewicz2024robustness,phan2023cstar}. Furthermore, \citep{song2024training} employ training-free techniques while incorporating ground truth information to defend against adversarial attacks. However, the aforementioned methods rely on additional prior or are limited to specific attacks.
In this paper, we aim to achieve robustness solely by optimizing TNs themselves, establishing them as a plug-and-play and promising adversarial purification technique.

\section{Backgrounds}
\label{Backgrounds}


\textbf{Notations} \quad Throughout the paper, we denote scalars, vectors, matrices, and tensors as lowercase letters, bold lowercase letters, bold capital letters, and calligraphic bold capital letters, \eg, $x$, $\vect{x}$, $\vect{X}$ and $\vectcal{X}$, respectively.
A $D$-order tensor tensor is an $D$-dimensional array, \eg, a vector is a 1st-order tensor and a matrix is a 2nd-order tensor.
For a $D$-order tensor $\pmb{\mathcal{X}} \in \mathbb{R}^{I_1 \times \dots \times I_D}$, we denote its ($i_1,\dots,i_D$)-th entry as $x_{\mathbf{i}}$, where $\mathbf{i} =$ ($i_1,\dots,i_D$).
% For a positive integer $n$, denote $[n] \coloneqq {1,2,\ldots, n}$.
Following the conventions in deep learning, we treat images as vectors, \eg, input example $\vect{x}_{in}$, clean example $\vect{x}_{cln}$, adversarial example $\vect{x}_{adv}$ and reconstructed example $\vect{y}$.

\textbf{Tensor network decomposition} \quad
Given a $D$-order tensor $\vectcal{X} \in \Real^{I_1 \times \ldots \times I_D}$,
tensor network decomposition factorizes $\vectcal{X}$ into $D$ smaller latent components
by using some predefined tensor contraction rules.
% The classical Tucker deomposition \citep{tucker1966some} assumes
% \(
% \vectcal{X}=\vectcal{W} \times_1 \vect{Z}^1 \times_2 \ldots \times_D \vect{Z}^D \,,
% \)
% where \(
% \vectcal{W} \in \Real^{R_1 \times \ldots \times R_D} \,, \vect{Z}^d \in \Real^{I_d\times R_d} \,,
% \forall d \in [D] \,,
% \)
% and $\times_d$ dnotes the matrix-tensor contraction \citep{kolda2009tensor}.
% Equivalently, each entry can be written as
% \(
% x_{\mathbf{i}} = \sum_{r_1=1}^{R_1} \cdots \sum_{r_D=1}^{R_D} w_{r_1\ldots r_D} z^1_{i_1 r_1}
% \cdots z^D_{i_D r_D} \,,
% \)
% where the tuple $(R_1,\ldots, R_D)$ is the Tucker rank of $\vectcal{X}$.
% The latent factors $\vect{Z}_d$ can capture information of each tensor mode and
% $\vectcal{W}$ represents the weight of each factors.
% CP decomposition \citep{hitchcock1927expression} is a special case of Tucker when $\vectcal{W}$ is super-diagonal, \ie,
% \(
% x_{\mathbf{i}} = \sum_{r=1}^{R} w_{r} z^1_{i_1 r} \cdots z^D_{i_D r} \,,
% \)
% and $R$ is the CP rank of $\vectcal{X}$.
% Tensor Train (TT) decomposition \citep{oseledets2011tensor} improves the approximation guarantee of CP and the compression
% rate of Tucker decompositions.
Among tensor network decompositions, Tensor Train (TT) decomposition \citep{oseledets2011tensor} enjoys both quasi-optimal approximation as well as the high compression rate of large and complex data tensors.
In particular, a $D$-order tensor $\vectcal{X} \in \Real^{I_1\times \ldots \times I_D}$ has the TT format as
\(
{x}_{\mathbf{i}}= \vect{A}^1_{i_1} \vect{A}^2_{i_2} \ldots \vect{A}^D_{i_D} \,,
\)
where $\vect{A}^d_{i_d} \in \Real^{r_{d-1}\times r_d}$, for $d\in[D]$ and $i_d \in [I_d]$. Then, $(1,r_1,\ldots,r_{d-1},1)$ is the TT rank of $\vectcal{X}$.
For simplicity, we denote $\vectcal{X}=\text{TT}(\vectcal{A}^1, \ldots, \vectcal{A}^D)$.
When each dimension $I_d$ of $\vectcal{X}$ is large,
quantized tensor train \citep[QTT,][]{khoromskij2011d} becomes highly efficient, which
splits each dimension in powers of two.
For example, a $2^D \times 2^D$ image can be rearranged into a more expressive and balanced
$D$-order tensor.
For brevity, hereafter, a $2^D \times 2^D$ image $\vect{x}_D$ shall be called a resolution $D$ image, whose quantized tensor is $\vectcal{X}_D = \quant(\vect{x}_D)$.


\section{Method}
\label{Method}

Tensor network (TN) is a classical tool in signal processing, with many successful applications in image completion and denoising. By leveraging the $\ell_2$-norm as the primary optimization criterion, which aligns well with the statistical properties of a normal distribution, these methods \citep{phan2020tensor,loeschcke2024coarse} have demonstrated strong capabilities in removing Gaussian noise.

However, the distribution of well-designed adversarial perturbations is essentially different from Gaussian noise and cannot be modeled explicitly \citep{ilyas2019adversarial,allen2022feature}, thereby challenging the conventional assumptions of TN-based denoising methods, leading to ineffectiveness on adversarial purification for $\vect{x}_{adv}$.
To minimize the loss $\|\vect{x}_{adv} - \text{TN}(\vect{x}_{adv})\|_{2}$, TN decompositions fit all feature components of $\vect{x}_{adv}$, including the adversarial perturbations. However, in the presence of adversarial attacks, we aim to restore unobserved $\vect{x}_{cln}$ from the input $\vect{x}_{adv}$, that is: $\text{TN}(\vect{x}_{adv}) \approx \vect{x}_{cln}$ rather than $\vect{x}_{adv}$.

Based on the above analysis, it is crucial to overcome two challenges in designing an effective TN method:

\quad \emph{Q1. How can we transform the distribution of non-specific perturbations into well-known distributions?}

\quad \emph{Q2. How can we avoid overly constraints of reconstruction error from inadvertently restoring those perturbations?}

% Q2. How can we maintain the balance between minimizing the reconstruction error and removing perturbations?

To address these two issues, we explore how adversarial perturbations evolve when using average pooling as downsampling. Intuitively, the central limit theorem suggests that as an image is progressively downsampled, aggregated perturbations begin to resemble a normal distribution. Thus, even an $\ell_2$-based penalty becomes effective in suppressing the perturbations at lower resolutions.

However, while this insight helps suppress perturbations at lower resolutions, there remains the challenge of reconstructing the full-resolution image. When upsampling and further optimizing using $\|\vect{x}_{adv} - \text{TN}(\vect{x}_{adv})\|_{2}$, the perturbations will still be restored. This connects with our second question, for which we design a new optimization objective.


\subsection{Downsampling using average pooling}

An intuitive explanation for why downsampling aids in perturbation removal can be derived from the Central Limit Theorem \citep[CLT,][]{grzenda2008conditional}. When an image is downsampled by average pooling, the random components (e.g., pixel-level noise or minor adversarial perturbations) within those pooling patches are aggregated.
We hypothesize that, given an adversarial example $\vect{x}_{adv}$,  downsampling the $\vect{x}_{adv}$ from its original resolution $D$ to a lower resolution~$D-1$ will smooth out the adversarial perturbations. 
As the downsampling process progresses further, the distribution of the aggregated adversarial perturbations in the low-resolution image $\vect{x}_{D-l}$ is expected to converge toward a normal distribution, as illustrated in \Cref{fig:distribution}a.

To investigate this hypothesis in real datasets, we compute the KL divergence between the histograms of adversarial perturbations and the Gaussian distributions with the same sample mean and variance across 512 images from ImageNet. As shown in \Cref{fig:distribution}b, the distribution of those perturbations progressively aligns with that of Gaussian noise as the downsampling process progresses. Additionally, to further support our hypothesis of using the average pooling, we compare the influence of different downsampling methods, as discussed in \Cref{app:avgpool}.
% Consequently, tensor network decomposition methods can effectively suppress or remove perturbations in the low-resolution image.

% The following theorem provides a theoretical confirms that the noisy clean example distribution and the adversarial example distribution get closer after average pooling process, indicating that the downsampling process can indeed mitigate the impact of perturbations.

% \textbf{Theorem 1.} \gl{If we still have time.}

\subsection{Tensor network purification}

Building upon our downsampling-based intuition, we
design a coarse-to-fine process
and adopt PuTT \citep{loeschcke2024coarse} as our base model, which
employs progressive
downsampling for better initialization of QTT cores.
The workflow of tensor network purification (TNP) for classification tasks is illustrated in \Cref{fig:process}, where the quantized $\vectcal{X} = \quant(\vect{x})$, TT decomposition $\vectcal{X} \approx \vectcal{Y}=\text{TT}(\vectcal{A}^1, \ldots, \vectcal{A}^D)$, and reconstruction $\vect{y} = \quant^{-1}(\vectcal{Y})$ processes are depicted.

Initially, the $2^D \times 2^D$ input example $\vect{x}_D$ (potentially adversarial example $\vect{x}_{adv}$ or clean example $\vect{x}_{cln}$),
whose quantized version is a $D$-order tensor $\vectcal{X}_{D}$, is first downsampled to a resolution $D-l$ example
$\vect{x}_{D-l}$,
corresponding to a $(D-l)$-order tensor $\vectcal{X}_{D-l}$.
The QTT cores of $\vectcal{X}_{D-l}$ are optimized by PuTT via backpropagation within a standard reconstruction error  $||\vect{x}_{D-l}-\vect{y}_{D-l}||_2$.
Once the approximation of $\vectcal{X}_{D-l}$ is stabilized,
the prolongation operator $\vectcal{P}_{D-l+1}$ is applied to
the QTT format of $\vectcal{X}_{D-l}$, producing a $(D-l+1)$-order tensor
$\vectcal{P}_{D-l+1}\vectcal{X}_{D-l}$.
Additionally, we define the linear function $\text{P}_d(\cdot)$ acts on the image level, with the effect of upsampling from resolution $d-1$ to $d$,
details in \cref{app:TN.P}.
This serves as an initialization to find the optimal QTT cores of $\vectcal{X}_{D-l+1}$ and reconstructed downsampled example $\vect{y}_{D-l}$.


Next, the input example $\vect{x}_D$ is once again downsampled to a resolution $D-l+1$ example $\vect{x}_{D-l+1}$. However, this time, the QTT cores of $\vectcal{X}_{D-l+1}$ are optimized using the adversarial optimization objective within a novel loss function as shown in \Cref{eq:ours}. Similarly, once the approximation of $\vectcal{X}_{D-l+1}$ stabilizes, the upsampling operation is performed. This process is repeated iteratively until reaching the QTT approximation $\vectcal{Y}_D$ of the original resolution $\vectcal{X}_{D}$.

Finally, tensor network purification (TNP) can purify the potential adversarial examples ($\vect{x}_{cln}$ or $\vect{x}_{adv}$) before feeding them into the classifier model $f$, \eg, $f(\text{TNP}(\vect{x}_{cln}))=f(\text{TNP}(\vect{x}_{adv}))=y$, where $y$ is the ground truth. As a plug-and-play module, TNP can be integrated with any classifiers.

\begin{figure}[t]
\vskip 0.1in
    \centering
    \includegraphics[width=\linewidth]{figures/fig_process.pdf}
    \caption{Illustration of tensor network purification.}
    \label{fig:process}
    \vskip -0.2in
\end{figure}


\subsection{Adversarial optimization process}
\label{Adversarial optimization process}

Following the coarse-to-fine process, despite the downsampling with average pooling and subsequent PuTT at lower resolutions can mitigate adversarial perturbations, the other challenge arises upon reconstructing the example at the original resolution, where minimizing standard reconstruction error will inevitably restore those perturbations.

\begin{table}[t]
\centering
\vskip -0.1in
\begin{minipage}{\linewidth}
\begin{algorithm}[H]
    \caption{Adversarial optimization process.}
    \label{alg:ours}
    \begin{algorithmic}
        \STATE \textbf{Input:} Example $\vect{x}_d$, number of iterations $T$, inner maximization steps $N$, scale $\alpha$ and $\eta$, learning rate $\beta$
        \STATE \textbf{Output:} Reconstructed examples $\vect{y}_d$
        \STATE Initialize $\hat{\vect{y}} \gets \text{P}_{d}(\vect{y}_{d-1})$
        \FOR{$t=1, 2, \dots, T$}
        \STATE Initialize \( \boldsymbol{\delta} \leftarrow \vect{0} \)
        \FOR{$n=1, 2, \dots, N$}
        \STATE $\ell \gets \mathcal{L}_{adv}(\hat{\vect{y}} + \boldsymbol{\delta}, \vect{x}_d)$
        \STATE $\boldsymbol{\delta} \gets \text{clip}(\boldsymbol{\delta} + \alpha\text{sign}(\nabla_{\hat{\vect{y}}}\ell), -\eta, \eta)$
        \ENDFOR
        \STATE $\boldsymbol{\delta}^* \gets \text{clip}(\hat{\vect{y}}+\boldsymbol{\delta}, 0, 1)-\hat{\vect{y}}$
        \STATE Gradient descent based on loss in \Cref{eq:ours}:
        % \STATE Update $\hat{\vect{y}}$ by $\arg\min_{\hat{\vect{y}}} \mathcal{L}(\vect{x}_d, \hat{\vect{y}}, \boldsymbol{\delta}^*)$ with \Cref{eq:ours}.
        % \STATE $\hat{\vect{y}} \gets \arg\min_{\hat{\vect{y}}} \mathcal{L}(\vect{x}_d, \hat{\vect{y}}, \boldsymbol{\delta}^*)$
        \STATE $\hat{\vect{y}} \gets \hat{\vect{y}} - \beta\nabla_{\hat{\vect{y}}}\mathcal{L}$
        \ENDFOR
        \STATE \textbf{return} $\vect{y}_d \gets \hat{\vect{y}}$
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\vskip -0.2in
\end{table}


Unlike traditional reconstruction tasks, in the context of adversarial attacks, we can only observe the adversarial example $\vect{x}_{adv}$, while the goal is to reconstruct a ``clean'' version $\vect{y}$ closing to the unobserved clean example $\vect{x}_{cln}$. To bridge the gap between $\vect{x}_{adv}$ and $\vect{x}_{cln}$, we propose a new optimization objective that introduces an auxiliary variable $\boldsymbol{\delta}$ by inner maximization. Additionally, we leverage a reconstructed downsampled example as a crucial prior to guide the approximation toward $\vect{x}_{cln}$.

Here, we outline the optimization procedure for $x_d$, which corresponds to the gray box in \Cref{fig:process}.
Formally, given the resolution~$d$ example $\vect{x}_d$, we attempt to obtain the reconstructed example $\vect{y}_d$ by performing gradient descent on optimization loss functions of
\begin{equation}
\label{eq:ours}
\begin{aligned}
    \mathcal{L}(\vect{x}_d, \hat{\vect{y}}, \boldsymbol{\delta}^*) &= ||\vect{x}_d - (\hat{\vect{y}}+\boldsymbol{\delta}^*)||_2 
    + ||\text{P}_{d} (\vect{y}_{d-1}) - \hat{\vect{y}}||_2, \\
    \text{s.t.  } \boldsymbol{\delta}^* &= \arg\max_{\norm{\boldsymbol{\delta}} < \eta} \mathcal{L}_{adv}(\hat{\vect{y}} + \boldsymbol{\delta}, \vect{x}_d),
\end{aligned}
\end{equation}
where $d \in [D-l+1, D]$ and $\eta$ is a scale hyperparameter.

The auxiliary variable $\boldsymbol{\delta}^*$ is determined through an inner maximization process that utilizes a non-convex loss function $\mathcal{L}_{adv}$.
We employ a perceptual metric, structural similarity index measure \citep[SSIM,][]{hore2010image}, as $\mathcal{L}_{adv}$ to explore more potential solutions and better handle complex perturbation patterns.
While $\boldsymbol{\delta}^*$ does not exactly represent the true adversarial perturbation, bounding $\norm{\boldsymbol{\delta}}<\eta$ can partially ensure that the misalignment between $\vect{y}$ and $\vect{x}_{adv}$ remains controlled, effectively ensuring that $\vect{y}$ does not simply collapse into the adversarial example $\vect{x}_{adv}$.

However, also since $\boldsymbol{\delta}^*$ does not represent the true perturbation, minimizing $||\vect{x}_d - (\hat{\vect{y}}+\boldsymbol{\delta}^*)||_2$ may not yield the desired clean reconstructed example. To address this limitation, we introduce a second loss term $||\text{P}_{d} (\vect{y}_{d-1}) - \hat{\vect{y}}||_2$.
Specifically, we utilize the reconstructed downsampled example $\vect{y}_{d-1}$ as an additional prior constraint to aid in approximating the $\vect{x}_{cln})$.
Building upon the observations in \Cref{fig:distribution}, we start from the resolution $D-l$ example $\vect{x}_{D-l}$, optimized by PuTT, and then perform upsampling to the higher resolution to produce a ``clean-leaning'' reference, which nudges $\vect{y}$ toward a less perturbed distribution.
Although the clean example $\vect{x}_{cln}$ cannot be obtained as a prior for the optimization, we devise a clever way to provide a surrogate prior and guide the optimization process.
The detailed algorithm of our adversarial optimization process is shown in \cref{alg:ours}.

% \textbf{Theorem 2.} \gl{To support loss, maybe a smaller bound between (x, y) compared with org loss. If we still have time.}


\section{Experiments}
\label{Experiments}
\renewcommand{\arraystretch}{0.8}

In this section, we conduct extensive experiments on CIFAR-10, CIFAR-100, and ImageNet across various attack settings to evaluate the performance of our method. The classification results demonstrate that the proposed method achieves state-of-the-art performance and exhibits strong generalization capabilities.
Specifically, our method achieved a 26.45\% improvement in average robust accuracy over AT across different norm threats, a 9.39\% improvement over AP across multiple attacks, and a 6.47\% improvement over AP across different datasets.
Next, we investigate the effectiveness of adversarial perturbation removal in denoising tasks using the existing tensor network decomposition methods. Among these, only our method successfully removes adversarial perturbations while preserving consistency between the reconstructed clean example and the reconstructed adversarial example.
These results collectively highlight the effectiveness and potential of our proposed method.



\subsection{Experimental setup}
\textbf{Datasets and model architectures} \quad We conduct extensive experiments on CIFAR-10, CIFAR-100 \citep{krizhevsky2009learning} and ImageNet \citep{deng2009imagenet} to empirically validate the effectiveness of the proposed methods against adversarial attacks. For classification tasks, we utilize the pre-trained ResNet \citep{he2016deep} and WideResNet \citep{zagoruyko2016wide} models. For denoising tasks, we employ Tensor Train \citep[TT,][]{oseledets2011tensor}, Tensor Ring \citep[TR,][]{zhao2016tensor}, quantized technique \citep{khoromskij2011d} and PuTT \citep{loeschcke2024coarse}.

\textbf{Adversarial attacks} \quad We evaluate our method against AutoAttack \citep{croce2020reliable}, a widely used benchmark that integrates both white-box and black-box attacks. Additionally, following the guidance of \citet{lee2023robust}, we utilize projected gradient descent \citep[PGD,][]{madry2018towards} with expectation over time \citep[EOT,][]{athalye2018synthesizing} for a more comprehensive evaluation.

% \textbf{Evaluation metrics:} We evaluate the performance of defense methods using multiple metrics: Standard accuracy and robust accuracy \citep{szegedy2013intriguing} on classification tasks. NRMSE, SSIM, PSNR metrics \citep{loeschcke2024coarse} and visualization results on denoising tasks.

Due to the high computational cost of evaluating methods with multiple attacks, following the guidance of \citet{nie2022diffusion}, we randomly select 512 images from the test set for robust evaluation. All experiments presented in the paper are conducted by NVIDIA RTX A5000 with 24GB GPU memory, CUDA v11.7 and cuDNN v8.5.0 in PyTorch v1.13.11 \citep{paszke2019pytorch}. More details in \Cref{app:settings}.

\subsection{Comparison with the state-of-the-art methods}
\label{Comparison results}


\begin{table}[t]
\centering
\caption{Standard and robust accuracy against AutoAttack $l_\infty$ threat ($\epsilon=8/255$) on CIFAR-10. ($^{\text{\textdagger}}$the methods use additional synthetic images. $^*$use robust classifer \citep{cui2024decoupled}.)}
\vskip 0.15in
\label{tab:cifar10:linf}
\begin{tabular}{@{\hspace{8pt}}c@{\hspace{18pt}}c@{\hspace{17pt}}c@{\hspace{17pt}}c@{\hspace{8pt}}}
\toprule
\multirow{2}{*}{Defense method} & Extra & Standard & Robust \\
& data & Acc. & Acc. \\
\midrule
\citet{zhang2020geometry} & \checkmark & 85.36  & 59.96  \\
\citet{gowal2020uncovering} & \checkmark & 89.48  & 62.70  \\
\citet{bai2023improving} & $\checkmark^{\text{\textdagger}}$ & 95.23  & 68.06 \\
\midrule
\citet{rebuffi2021fixing} & $\times^{\text{\textdagger}}$ & 87.33  & 61.72  \\
\citet{gowal2021improving} & $\times^{\text{\textdagger}}$ & 88.74  & 66.11 \\
\citet{wang2023better} & $\times^{\text{\textdagger}}$ & 93.25  & 70.69 \\
\citet{peng2023robust} & $\times^{\text{\textdagger}}$ & 93.27  & 71.07 \\
\citet{cui2024decoupled} & $\times^{\text{\textdagger}}$ & 92.16  & 67.73 \\
\midrule
\citet{nie2022diffusion}   & $\times$ & 89.02  & 70.64 \\
\citet{wang2022guided}  & $\times$ & 84.85  & 71.18 \\
\citet{anonymous2023classifier}  & $\times$ & 90.04  & 73.05 \\
\citet{lin2024adversarial} & $\times$ & 90.62  & 72.85 \\
Ours & $\times$ & 82.23  & 55.27  \\
Ours$^*$ & $\times$ & 91.99  & 72.85  \\
\bottomrule
\bottomrule
\end{tabular}
\vskip -0.1in
\end{table}

\begin{table}[t]
    \caption{Standard and robust accuracy against AutoAttack $l_2$ threat ($\epsilon=0.5$) on CIFAR-10. ($^{\text{\textdagger}}$the methods use additional synthetic images. $^*$use robust classifer \citep{cui2024decoupled}.)}
    \vskip 0.15in
    \label{tab:cifar10:l2}
    \begin{tabular}{@{\hspace{7pt}}c@{\hspace{16pt}}c@{\hspace{15pt}}c@{\hspace{15pt}}c@{\hspace{8pt}}}
    \toprule
    \multirow{2}{*}{Defense method} & Extra & Standard & Robust \\
    & data & Acc. & Acc. \\
    \midrule
    \citet{augustin2020adversarial} & \checkmark & 92.23  & 77.93  \\
    \citet{gowal2020uncovering} & \checkmark & 94.74  & 80.53  \\
    \midrule
    % \citet{wang2023better} & $\times^{\text{\textdagger}}$ & 95.16  & 83.68  \\
    \citet{rebuffi2021fixing} & $\times^{\text{\textdagger}}$ & 91.79  & 78.32  \\
    \midrule
    \citet{ding2019mma} & $\times$ & 88.02  & 67.77  \\
    \citet{nie2022diffusion} & $\times$ & 91.03 & 78.58  \\
    % \citet{anonymous2023classifier}  & $\times$ & 92.58  & 83.13  \\
    % \citet{bai2024diffusion} & $\times$ & 93.75 & 84.38 \\
    % \citet{lee2023robust}  & $\times$ & 90.16 & 86.48  \\
    Ours & $\times$ & 82.23  & 68.16  \\
    Ours$^*$ & $\times$ & 91.99  & 79.49  \\
    \bottomrule
    \bottomrule
    \end{tabular}%
    \vskip -0.1in
\end{table}

\begin{table}[htbp]
    \centering
    \caption{Standard and robust accuracy against AutoAttack $l_\infty$ threat ($\epsilon=8/255$) on CIFAR-100 using WideResNet-28-10 classifier. ($^{\text{\textdagger}}$the methods use additional synthetic images.)}
    \vskip 0.15in
    \label{tab:cifar100:linf}
    \begin{tabular}{c@{\hspace{12pt}}c@{\hspace{13pt}}c@{\hspace{13pt}}c}
    \toprule
    \multirow{2}{*}{Defense method} & Extra & Standard & Robust \\
    & data & Acc. & Acc. \\
    \midrule
    \citet{hendrycks2019using} & \checkmark & 59.23  & 28.42  \\
    \citet{debenedetti2023light} & \checkmark & 70.76 & 35.08 \\
    \midrule
    \citet{cui2024decoupled} & $\times^{\text{\textdagger}}$ & 73.85  & 39.18  \\
    \citet{wang2023better} & $\times^{\text{\textdagger}}$ & 75.22  & 42.67  \\
    \midrule
    \citet{pang2022robustness} & $\times$ & 63.66  & 31.08  \\
    \citet{jia2022adversarial} & $\times$ & 67.31 & 31.91 \\
    \citet{cui2024decoupled} & $\times$ & 65.93  & 32.52  \\
    Ours & $\times$ & 62.30  & 44.34  \\
    % Ours$^*$ & $\times$ & 71.48  & 44.53  \\
    \bottomrule
    \bottomrule
    \end{tabular}
    \vskip -0.1in
\end{table}

\begin{table}[t]
    \centering
    \caption{Standard and robust accuracy against AutoAttack $l_\infty$ threat ($\epsilon=4/255$) on ImageNet using ResNet-50 classifier.}
    \vskip 0.15in
    \label{tab:image:linf}
    \begin{tabular}{@{\hspace{6pt}}c@{\hspace{15pt}}c@{\hspace{15pt}}c@{\hspace{15pt}}c@{\hspace{8pt}}}
    \toprule
    \multirow{2}{*}{Defense method} & Extra & Standard & Robust \\
    & data & Acc. & Acc. \\
    \midrule
    \citet{engstrom2019robustness} & $\times$ & 62.56 & 31.06  \\
    \citet{wong2020fast} & $\times$ & 55.62 & 26.95  \\
    \citet{salman2020adversarially} & $\times$ & 64.02 & 37.89  \\
    \citet{bai2021transformers} & $\times$ & 67.38 & 35.51  \\
    \citet{nie2022diffusion} & $\times$ & 67.79 & 40.93  \\
    % \citet{bai2024diffusion} & $\times$ & 70.41 & 41.70  \\
    \citet{chen2024data} & $\times$ & 68.76 & 40.60 \\
    Ours & $\times$ & 65.43  & 42.77  \\
    % Ours$^*$ & $\times$ & -  & \textbf{-}  \\
    \bottomrule
    \bottomrule
    \end{tabular}
    \vskip -0.1in
\end{table}


In this section,
we evaluate our method for defending against AutoAttack $l_\infty$ and $l_2$ threats \citep{croce2020reliable,croce2021robustbench} and compare it with the state-of-the-art methods under all adversarial settings listed in RobustBench.
% \footnote{https://robustbench.github.io}.
\Cref{tab:cifar10:linf,tab:cifar10:l2,tab:cifar100:linf,tab:image:linf} present the performance of various defense methods against AutoAttack $l_\infty$ ($\epsilon=8/255$) and $l_2$ ($\epsilon=0.5$) threats on CIFAR-10, CIFAR-100 and ImageNet datasets.
Overall, the highest robust accuracy achievable by our method is generally on par with the current state-of-the-art methods without using extra data (the dataset introduced by \citet{carmon2019unlabeled}). Specifically, compared to the second-best method, our method improves the robust accuracy by 1.67\% on CIFAR-100, by 1.84\% on ImageNet, and the average robust accuracy by 0.36\% on CIFAR-10.

However, due to the overfitting of WideResNet-28-10 trained on the limited data available in CIFAR-10, we observe that the results of $\text{Ours}$ struggle to reach state-of-the-art performance, consistent with findings from other AT methods. To address this issue, these methods incorporate additional synthetic data to train a robust classifier. Following this, we conduct experiments using the robust classifier trained by \citet{cui2024decoupled}, which utilizes an additional 20M synthetic images in training. This leads to a significant improvement in the robust accuracy observed in $\text{Ours}^*$. 
Moreover, compared to the used robust classifier, our method futher improves the robust accuracy by 5.12\%.
These results are consistent across multiple datasets and norm threats, confirming the effectiveness of our method and its potential for defending against adversarial attacks.




\subsection{Generalization comparison across various scenarios}
\label{Comparison across multiple cases}
As previously mentioned, the existing defense methods are often criticized for their lack of generalization across different norm threats, attacks, and datasets. In this section, we evaluate the performance of our method under various settings to demonstrate its generalization capability.

\begin{figure*}[t]
\vskip -0.1in
  \begin{minipage}[ht]{0.65\linewidth}
  \centering
    \captionof{table}{Standard accuracy (SA) and robust accuracy (RA) against AutoAttack $l_\infty$ ($\epsilon=8/255$) threat on CIFAR-10 and CIFAR-100 with WideResNet-28-10 classifier. The pre-trained generative model used in AP is trained on CIFAR-10.}
    \vskip 0.15in
    \label{tab:datasets}
    \begin{tabular}{ccccccc}
    \toprule
    \multirow{2}{*}{Defense method} & \multicolumn{2}{c}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} & \multicolumn{2}{c}{Avg.} \\
    \cmidrule(lr){2-3}
    \cmidrule(lr){4-5}
    \cmidrule(lr){6-7}
     & SA & RA & SA & RA & SA & RA\\
    \midrule
    Standard Training & 94.78  & 0.00  & 81.86 & 0.00 & 88.32 & 0.00 \\
    \midrule
    AT \citep{cui2024decoupled} & 92.16  & 67.73 & 73.85 & 39.18 & 83.01 & 53.46 \\
    AP \citep{nie2022diffusion} & 89.02  & 70.64 & 38.09 & 33.79 & 63.56 & 52.22\\
    Ours$^*$  & 91.99 & 72.85 & 71.48 & 44.53 & 81.74 & 58.69 \\
    \bottomrule
    \bottomrule
    \end{tabular}
    \vskip -0.1in
  \end{minipage}\quad\quad
  \begin{minipage}[ht]{0.3\linewidth}
  \vskip 0.2in
    \centering
    \includegraphics[width = \linewidth]{figures/fig_attacks.pdf}
    \captionof{figure}{Comparison of robust accuracy against PGD+EOT and AutoAttack.}
    \label{fig:attacks}
    \vskip -0.2in
  \end{minipage}
  \vskip -0.1in
\end{figure*}

\textbf{Results analysis on different norm-threats:}
\Cref{tab:norms} shows that AT methods \citep{laidlaw2021perceptual,dolatabadi2022} are limited in defending against unseen attacks and can only effectively against the specific attacks they are trained on. The intuitive idea is to apply AT across all norm threats or develop more general constraints to obtain a robust model. However, training such a model is challenging due to the inherent differences among various attacks. In contrast, AP methods \citep{nie2022diffusion,lin2024adversarial} exhibit strong generalization, effectively defending against unseen attacks. The results demonstrate that our method also possesses strong generalization capabilities against unseen attacks, achieving performance close to the state-of-the-art AP methods while significantly outperforming the existing AT methods. Specifically, compared to the best AT method, our method improves average robust accuracy by 26.45\%.


\begin{table}[tb]
\vskip -0.08in
\caption{Standard accuracy and robust accuracy against AutoAttack $l_\infty$ ($\epsilon=8/255$) and AutoAttack $l_2$ ($\epsilon=1.0$) threats on CIFAR-10 with standard ResNet-50 classifier.}
\vskip 0.15in
\label{tab:norms}
\begin{center}
\begin{tabular}{@{\hspace{3pt}}c@{\hspace{1pt}}c@{\hspace{12pt}}c@{\hspace{18pt}}cc@{\hspace{3pt}}}
    \toprule
    \multirow{2}{*}{Type} & \multirow{2}{*}{Defense method} & \multirow{2}{*}{SA} & \multicolumn{2}{c}{Robust Acc.} \\
    \cmidrule(){4-5}
    & &  & AA $l_\infty$ & AA $l_2$ \\
    \midrule
    & Standard Training & 94.8  & 0.0   & 0.0   \\
    \midrule
    \multirow{4}{*}{AT} & Training with $l_\infty$ & 86.8  & 49.0  & 19.2  \\
    & Training with $l_2$ & 85.0  & 39.5  & 47.8  \\
    & \citet{laidlaw2021perceptual} & 82.4  & 30.2  & 34.9  \\
    & \citet{dolatabadi2022} & 83.2  & 40.0  & 33.9  \\
    \midrule
    \multirow{2}{*}{AP} & \citet{nie2022diffusion} & 88.2  & 70.0  & 70.9  \\
    & \citet{lin2024adversarial} & 89.1  & 71.2 & 73.4  \\
    \midrule
    & Ours  & 88.3 & 73.2 & 67.0 \\
    \bottomrule
    \bottomrule
\end{tabular}
\end{center}
\vskip -0.2in
\end{table}

\begin{figure*}[t]
\vskip 0.05in
    \centering
    \includegraphics[width=\linewidth]{figures/fig_visual.pdf}
    \vskip -0.1in
    \caption{Visual comparison of the denoising task. Top: the original image and corresponding reconstructed image for (a) the clean example and (b) the adversarial example, using PuTT and our proposed method. Bottom: the error maps are created (a) between the rec. clean example and the original clean example, as well as (b) between the rec. adversarial example and the rec. clean example.}
    \label{fig:visual}
    \vskip -0.13in
\end{figure*}

\begin{table*}[t]
    \caption{Performance comparison of various methods on the denoising task. We evaluate the accuracy, NRMSE, SSIM and PSNR metrics using clean examples and adversarial examples on CIFAR-10. Additionally, we compare the differences between rec. AEs and rec. CEs.}
    \vskip 0.15in
    \label{tab:reconstruction}
    \begin{center}
    \begin{tabular}{ccccccccccccc}
    \toprule
    Defense & \multicolumn{4}{c}{CLN: CEs $\text{ }\&\text{ }$ rec. CEs} & \multicolumn{4}{c}{ADV: AEs $\text{ }\&\text{ }$ rec. AEs} & \multicolumn{3}{c}{REC: rec. CEs $\text{ }\&\text{ }$ rec. AEs} \\
    \cmidrule(lr){2-5}
    \cmidrule(lr){6-9}
    \cmidrule(lr){10-12}
    method & Acc. & NRMSE & SSIM & PSNR & Acc. & NRMSE & SSIM & PSNR & NRMSE & SSIM & PSNR\\
    % method & SA & NRMSE$\downarrow$ & SSIM$\uparrow$ & PSNR$\uparrow$ & RA & NRMSE$\uparrow$ & SSIM$\downarrow$ & PSNR$\downarrow$ & NRMSE$\downarrow$ & SSIM$\uparrow$ & PSNR$\uparrow$ \\
    \midrule
    Standard & 94.73 & - & - & - & 0.00 & - & - & - & - & - & - \\
    \midrule
    TT & 87.30 & 0.0507 & 0.9526 & 31.14 & 36.13 & 0.065 & 0.8977 & 28.99 & 0.0267 & 0.9790 & 39.10 \\
    TR & \textbf{94.34} & \textbf{0.0171} & \textbf{0.9938} & \textbf{40.58} & 0.98 & 0.0464 & 0.9210 & 31.91 & 0.0322 & 0.9598 & 35.51 \\
    QTT & 84.57 & 0.0613 & 0.9253 & 29.49 & 51.56 & 0.0724 & 0.8808 & 28.06 & 0.0233 & 0.9855 & 39.88 \\
    QTR & 83.40 & 0.0613 & 0.9254 & 29.49 & 49.41 & 0.0724 & 0.8785 & 28.06 & 0.0231 & 0.9853 & 39.96 \\
    PuTT & 80.86 & 0.0626 & 0.9261 & 29.32 & 44.14 & 0.0742 & 0.8787 & 27.84 & 0.0311 & 0.9770 & 38.03 \\
    \midrule
    Ours & 82.23 & 0.0644 & 0.9203 & 29.06 & \textbf{55.27} & \textbf{0.0748} & \textbf{0.8707} & \textbf{27.77} & \textbf{0.0218} & \textbf{0.9863} & \textbf{40.37}  \\
    \bottomrule
    \bottomrule
    \end{tabular}
    \end{center}
\vskip -0.1in
\end{table*}

\textbf{Results analysis on multiple attacks:} \cref{fig:attacks} shows the comparison of robust accuracy against PGD+EOT and AutoAttack with $l_\infty$ ($\epsilon=8/255$) threat on CIFAR-10 with WideResNet-28-10. When facing different attacks within the same threat, AT methods \citep{gowal2020uncovering,gowal2021improving,pang2022robustness} exhibit better generalization than AP methods \citep{yoon2021adversarial,nie2022diffusion,lee2023robust}. Typically, robustness evaluation is based on the worst-case results of the robust accuracy. Under this criterion, our method outperforms all AT and AP methods. Furthermore, compared to the state-of-the-art AP method on both attacks, our method improves the average robust accuracy by 9.39\%.

\textbf{Results analysis on different datasets:} \cref{tab:datasets} shows the generalization of the methods across different datasets. As previously mentioned, the existing AP methods typically rely on the specific datasets. When a pre-trained generative model trained on CIFAR-10 is applied to adversarial robustness evaluation on CIFAR-100, both standard accuracy and robust accuracy on CIFAR-100 drop significantly. This occurs because the pre-trained generative model can only generate the data it has learned. Although the input examples originate from CIFAR-100, the generative model attempts to output one of the ten classes from CIFAR-10, severely distorting the semantic information of the input examples and leading to low classification accuracy. In contrast, TN-based AP methods rely solely on the input examples rather than prior information learned from large datasets, allowing them generalize effectively across different datasets. The results demonstrate that our method exhibits strong generalization across different datasets, achieving comparable robust performance on CIFAR-100 as on CIFAR-10. Specifically, compared to the AP method \citep{nie2022diffusion}, our method improves the average robust accuracy by 6.47\%.


\subsection{Denoising tasks}
\label{Denoising tasks}

In this section, we evaluate the effectiveness of our method on non-classification tasks through visual comparisons and various quantitative metrics.

\textbf{Visualization results analysis:} \Cref{fig:visual} shows the visual comparison of the denoising task on ImageNet. The top row in (a) displays the input clean example (CE), and its corresponding reconstructed clean examples (rec. CE) generated by PuTT and our proposed method, while (b) displays the reconstructed adversarial examples (rec. AE) for the input adversarial example (AE).
Additionally, we create error maps to highlight differences, as shown at the bottom of \Cref{fig:visual}: (a) between the rec. CEs and the input CEs, and (b) between the rec. AEs and the rec. CEs. The results indicate that while our method does not match PuTT in reconstructing CEs, it significantly outperforms PuTT in removing adversarial perturbations from AEs.

Specifically, when processing CEs, the reconstructed examples generated by PuTT are almost identical to the original ones, whereas our method is slightly less effective in restoring some details. However, when processing AEs, the reconstructed examples from PuTT remain consistent with the original ones, leading to the preservation of adversarial perturbations, as highlighted in \Cref{fig:visual}b. In contrast, our method better removes those perturbations, ensuring that the rec. AEs and the rec. CEs retain similar information. Moreover, we evaluate the necessity of the second term in \Cref{eq:ours}, which serves as a surrogate prior constraint to optimize the reconstructed examples toward the clean data distribution. As observed, removing this constraint eliminates prior information from the optimization process, increasing the likelihood of significant deviation in the wrong direction.

\textbf{Quantitative results analysis:} \Cref{tab:reconstruction} shows the quantitative results of the denoising task for AEs and CEs of CIFAR-10. We compare our method with existing tensor network decomposition methods, including TT, TR, QTT, QTR, and PuTT.
While our method does not achieve the best denoising performance on clean examples, it still maintains classification performance well, achieving 82.23\% standard accuracy with the vanilla WideResNet-28-10 classifier. More importantly, our method outperforms others in the next two columns. Specifically, when processing AEs, our method yields the highest NRMSE and the lowest SSIM and PSNR, achieving the highest robust accuracy.
This outcome is expected, as our goal is to ensure that the rec. AEs differ from the original AEs (i.e., lower SSIM and PSNR, and higher NRMSE in the ``ADV'' column) while rec. AEs closely resembling the rec. CEs (i.e., higher SSIM and PSNR, and lower NRMSE in the ``REC'' column).
These results align well with the visual observations in \Cref{fig:visual} and consistently demonstrate the effectiveness of our method highlighting its potential in adversarial scenarios.

\textbf{Limitations} \quad One limitation of our method is that, despite being a training-free technique, TN-based AP method requires additional optimization time during inference.
This overhead stems from the inherent limitations of iterative optimization processes and impacts their practicality in real-world applications.
Therefore, we leave the exploration of integrating our TN-based AP technique with more advanced and efficient optimization strategies for future research.


\section{Conclusion}
\label{Conclusion}
In this paper, we propose a novel model-free adversarial purification method based on a specially designed tensor network decomposition algorithm. We conduct extensive experiments on CIFAR-10, CIFAR-100, and ImageNet, demonstrating that our method achieves state-of-the-art performance in defending against adversarial attacks while exhibiting strong generalization across diverse adversarial scenarios.
Despite these significant improvements, our method features an additional optimization cost during inference.
However, further exploration of TN-based AP method remains an exciting research direction for developing a plug-and-play and effective adversarial purification technique.


