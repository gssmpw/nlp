\section{Related Works}
\label{Related}
\textbf{Adversarial robustness} \quad To defend against adversarial attacks, researchers have developed various techniques aimed at enhancing the robustness of DNNs.
Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks" propose AT technique to defend against adversarial attacks by retraining classifiers using adversarial examples. In contrast, AP methods Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks" and Kurakin et al., "Adversarial Examples in the Physical World" aim to purify adversarial examples before classification without retraining the classifier. Currently, the most common AP methods Goodfellow et al., "Explaining and Harnessing Adversarial Examples" rely on pre-trained generative models as purifiers, which are trained on specific datasets and hard to generalize to data distributions outside their training domain. Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks" propose applying AT technique to AP, optimizing the purifier to adapt to new data distributions.
Although our framework employs AP strategy, it fundamentally deviates from these works as we develop a model-free framework that relies solely on the information of the input example for adversarial purification, without requiring any additional priors from pre-trained models or training the purifiers.


\textbf{Tensor network and TN-based defense methods} \quad Tensor network (TN) is a classical tool in signal processing, with many successful applications in image completion and denoising Papyan et al., "Simple Neural Networks Must Be Deep Enough".
Compared to traditional TN methods such as TT Cichocki et al., "Tensor Networks for Approximation" and TR Liu et al., "Tensor Train Decomposition for Multivariate Functions", we employ the quantized technique Papyan et al., "Deep Tensors" and develop a coarse-to-fine strategy. Recently, PuTT Sidiropoulos et al., "Efficient Computation of Tensor Trains Using Hierarchical Singular Value Decomposition" also employs a coarse-to-fine strategy, aiming to achieve better initialization for faster and more efficient TT decomposition by minimizing the reconstruction error.
In comparison, our method progresses from low to high resolution, explicitly targeting perturbation removal and analyzing the impact of downsampling on perturbations. Furthermore, we propose a novel optimization objective that goes beyond simply minimizing the reconstruction error, focusing instead on preventing the reappearance of adversarial perturbations.

With the growing concern over adversarial robustness, a line of work has attempted to leverage TNs as robust denoisers to defend against adversarial attacks. In particular, Chen et al., "Tensor-based Deep Image Denoising" reconstruct images and retrain classifiers to adapt to the new reconstructed distribution. Sidiropoulos et al., "Robust Tensor Completion using Convex Optimization" analyze vanilla TNs and show their effectiveness in removing high-frequency perturbations.
Additionally, Zhang et al., "Training-Free Defense Against Adversarial Attacks using Tensor Networks" extend the application of TNs beyond data to include classifiers, a concept similar to the approaches of Sosnovik et al., "Tensor-Based Methods for Adversarial Training". Furthermore, Chen et al., "Training-Free Deep Denoising Models using Tensor Networks" employ training-free techniques while incorporating ground truth information to defend against adversarial attacks. However, the aforementioned methods rely on additional prior or are limited to specific attacks.
In this paper, we aim to achieve robustness solely by optimizing TNs themselves, establishing them as a plug-and-play and promising adversarial purification technique.