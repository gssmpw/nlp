UI adaptation can either be offline, to computationally design an interface, or online, to adapt the UI according to users' goals. We will focus on online adaptive UIs and refer readers to \cite{combinatorialoptimizationdesign2020oulasvirta, functionalityselection2017oulasvirta} for an overview of computational UI design.
\subsection{Control of Adaptive Interfaces} We introduce an overview of different control strategies for adaptive interfaces. Note that we use control strategy and policy in the context of adaptive interfaces interchangeably.

\subsubsection{Heuristics, Bayesian Networks \& Combinatorial Optimization}
In early works, heuristic- or knowledge-based approaches are used to adapt the UI \cite{Browne1990,Stephanidis1997,Smith2010}. Similarly, multi-agent systems employ rule-based and message-passing approaches \citep{Rich1998,Rich2005,Yorke2012}. Another popular technique for AUIs is domain-expert-designed Bayesian networks \citep{Horvitz1998, Bosma2004}. More recently, combinatorial optimization was used to adapt interfaces dynamically \cite{park2018adam, lindlbauer2019context}. The downside of these approaches is that experts need to specify user goals using complex rule-based systems or mathematical formulations.
Creating them comprehensively and accurately requires developers to foresee all possible user states, which is tedious and requires expert knowledge.Commonly, these approaches also get into conflict when multiple rules or objectives apply. This conflict often results in unintuitive adaptations. In contrast, \marlui only requires the layout of the UI.
From its representation as an RL environment, we learn policies that meaningfully adapt the UI and realistically reproduce user behavior.

\subsubsection{Supervised Learning}
Leveraging machine learning can overcome the limitations of heuristic-, network-, and optimization-based systems by learning appropriate UI adaptations from user data.
Traditional machine learning approaches commonly learn a mapping from user input to UI adaptation.

Algorithms like nearest neighbor \cite{Maes1995, Lashkari1997}, Na√Øve Bayes \cite{McCreath2006,Faulring2010}, perceptron \cite{Shen2009a, Shen2009b}, support vector machines \cite{Berry2011}, or random forests \cite{Pejovic2014, Mehrotra2015} are used and models are learned offline \cite{Berry2011} and online \cite{Shen2009a}.
Due to the problem setting, these approaches require users' input to be highly predictive of the most appropriate adaptation. Furthermore, it restricts the methods to work in use cases where myopic planning is sufficient, i.e., a single UI adaptation leads users to their goal.
In contrast, \marlui considers multiple goals when selecting an adaptation and can lead users to their goal using sequences of adaptations.

More recent work overcomes the limitations stemming from simple input-to-adaptation mapping by following a two-step approach. They (1) infer users' intention based on observations and (2) choose an appropriate adaptation based on the inferred intent \cite{oulasvirta2018computational}. Such work uses neural networks, and user intention is modeled either explicitly \cite{kolekar2010learning, soh2017deep} or as a low-dimensional latent representation \cite{RIZZOGLIO2021}.

However, these approaches are still highly dependent on the quality of the training data, which may not even be available for emerging technologies. In contrast, \marlui can learn supportive policies without pre-collected user data by just observing simulated user behavior.

\subsubsection{Bandits \& Bayesian Optimization}
Bandit systems are a probabilistic approach often used in recommender systems \cite{glowacka2019bandit}. In a multi-armed bandit setting, each adaptation is modeled as an arm with a probability distribution describing the expected reward. The Bayes theorem updates the expectation, given a new observation and prior data. Related work leverages this approach for AUIs \cite{lomas2016interface,koch2019may,kangas2022scalable}.

Bayesian optimization is a sample-efficient global optimization method that finds optimal solutions in multi-dimensional spaces by probing a black box function \cite{shahriari2015taking}. In the case of AUIs, it is used to find optimal UI adaptations by sampling users' preferences \cite{Koyama2014, Koyama2016}. Both approaches trade off exploration and exploitation when searching for appropriate adaptations (i.e., exploration finds entirely new solutions, and exploitation improves existing solutions), rendering them suitable approaches to the AUI problem. However, such methods are not able to plan adaptations over a sequence of interaction steps, i.e., they plan myopic strategies.

In addition, these approaches need to sample user feedback to learn or optimize for meaningful adaptations and, hence, also rely on high-fidelity user data.
Furthermore, as users themselves learn during training or optimization, solutions can converge to sub-optimal user behavior as such methods reduce exploration with convergence.
In contrast, \marlui can plan adaptations over a sequence of interaction steps learned from realistic, simulated user data.

\subsubsection{Reinforcement Learning}
Reinforcement learning is a natural approach to solving the AUI problem, as its underlying decision-making formalism implicitly captures the closed-loop iterative nature of HCI \cite{howes2018interaction}. It is a generalization of bandits and learns policies for longer horizons, where current actions can influence future states. This generalization enables selecting UI adaptations according to user goals that require multiple interaction steps. Its capability makes RL a powerful approach for AUIs with applications in dialog systems \cite{Gasic2014, Su2017}, crowdsourcing \cite{PengCrowdsourcing2013, Hu2018}, sequential recommendations \cite{Chen2019, Liu2018, Liebman2015}, information filtering \cite{seo2000reinforcement}, personalized web page design \cite{ferretti2014exploiting}, and mixed reality \cite{gebhardt2019learning}. Similar to our work is a model-based RL method that optimizes menu adaptations \cite{todi2021adapting}.

Current RL methods sample predictive models \cite{todi2021adapting, Gasic2014, Hu2018} or logged user traces \cite{gebhardt2019learning}. However, these predictive models and offline traces represent user interactions with non-adaptive interfaces. Introducing an adaptive interface will change user behavior; so-called co-adaptation \cite{mackay2000responding}. Hence, it is unclear if the learned model can choose meaningful adaptations when user behavior changes significantly due to the model's introduction. In contrast, our user agent learns to interact with the adapted UI; hence, our interface agent learns on behavioral traces from the adapted setting.

\subsubsection{Multi-Agent Reinforcement Learning}
MARL is a generalization of RL in which multiple agents act, competitively or cooperatively, in a shared environment \cite{zhang2021multi}.Multi-agent systems are common in games \cite{baker2019emergent, jaderberg2019quake}, robotics \cite{ota2006multiagent,sariff2018multiagent}, or modeling of social dilemmas \cite{chao2015social,leibo2017multi}. MARL is challenging since multiple agents change their behavior as training progresses, making the learning problem non-stationary. Common techniques to address this issue are via implicit \cite{tian2020implicit} or explicit \cite{foerster2016learning} communication, centralized critic functions \cite{lowe2017multi}, or curricula \cite{epciclr2020,wang2020curriculum}.