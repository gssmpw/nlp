Control theory deals with the behavior of dynamic systems with inputs and how their behavior is modified by feedback \cite{thummala2007, seal2019}. The goal of control theory is to develop methods for influencing the behavior of dynamical systems to achieve desired outcomes \cite{seal2019}. The key idea is to measure the system's output, compare it to a reference or desired state, and adjust the system's input accordingly to minimize the difference between the actual and desired outputs \cite{grigas2023}. This feedback loop allows the system to adapt and maintain stability in the presence of disturbances or uncertainties \cite{seal2019, vizvarova2021}. Control theory provides a mathematical framework for modeling, analyzing, and optimizing such feedback systems \cite{babblenewt2023, seal2019}.
In an HCI context, a human interacts with a machine. While the human interaction can be modeled as a control system, the machine uses control theory to achieve desired outcomes given the human input. Together they form a closed-loop system (\figref{fig:control_loop}) \cite{murray2018control}. First, we will discuss control theory from a machine perspective. Then, we will discuss modeling human behavior from a control theory perspective. For a mathematical introduction, see the background chapter (\secref{sec:ocp}).
\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{chapters/02_related_work/figures/control_loop.pdf}
\caption{Human-Computer Interaction as a closed-loop system, adapted from \cite{murray2018control}}
\label{fig:control_loop}
\end{figure}
\subsection{Control of Machines}
There has been extensive research on optimal control techniques and their applications to various domains. Proportional-integral-derivative (PID) control is one of the most widely used approaches due to its simplicity and effectiveness. For example, PID control has been used to control haptic devices \cite{abut2018interface, ramos2016wavenet, pothi2014design}. However, PID control has some shortcomings, such as poor adjustment and follow-up when the system has strong interference or high nonlinearity and uncertainty \cite{krstic2017applicability}. It can also struggle with precise control and overshoot in some applications \cite{somefun2021dilemma}.

Model Predictive Control (MPC) is another optimal control technique that has gained popularity in recent years. MPC addresses some of the limitations of PID control by explicitly taking into account the nature of the actuators and system dynamics over a certain time horizon before deciding the control action to be applied \cite{dasilva:2008:mpc, qin1997overview}. This allows MPC to handle constraints, nonlinearities, and uncertainties more effectively. MPC has been used, for instance, in computer numerical controlled machines \cite{lam2010model, lam2013model}, the control of drones \cite{Gebhardt:2018}, or autonomous vehicles \cite{liu2017path}. However, MPC also has some shortcomings, such as the need for an accurate system model and the computational cost associated with solving the optimization problem in real-time \cite{holkar2010overview}. Furthermore, MPC is a time-dependent control strategy, in which the setpoint moves at a fixed pace. This limits user creativity and freedom.

Reinforcement Learning (RL) is an increasingly popular approach for optimal control in intelligent systems that interact with humans. RL addresses some of the limitations of MPC by learning control policies directly from interaction with the environment, without requiring an explicit system model \cite{sutton1998introduction}. This allows RL to handle complex, uncertain, and nonlinear systems more effectively. RL has been applied to robotics, where it has been used for learning complex behaviors such as manipulation and locomotion \cite{christen2019guided, christen2023synh2r}. In self-driving cars, RL has been employed for learning driving policies that can handle diverse traffic scenarios and road conditions \cite{kiran2021deep}.

In this dissertation, we look at a time-independent MPC for haptic devices and RL for adaptive user interfaces. In both scenarios, we embedded user dynamics into the control loop. To that end, we must have models of human behavior.

\subsection{Human Behavior as Control Strategy}
The specific area of control systems relating to human users became a major focus starting in the 1950s \cite{McRuer1967, Costello1968}. Our discussion of the relevant work is based on \cite{murray2018control}; we refer to that for a more extensive discussion. In general, it is accepted that models of human behavior enable simulation. Simulating human behavior, in turn, has many advantages in theory crafting, design, novel interaction paradigms, and safety \cite{murray2022simulation}.

Manual control theory \cite{McRuer1967, Costello1968}, which seeks to model the interaction of humans with machines like aircraft or cars, grew out of Craik's early, war-related work \cite{Craik1947}. It became more well-known in the broader framing of Wiener's Cybernetics \cite{Wiener1948}. According to Wickens and Hollands \cite{Wickens1999}, modeling human control behavior emerged from two main schools: the skills researchers, who focused on learning and acquisition in undisturbed environments, and the dynamic systems or manual control theory researchers \cite{Kelley1968, Sheridan1974}, who modeled the interaction of humans with machines in vehicle control and complex industrial process control, driven by engineering motivations to eliminate error in closed-loop systems. Poulton \cite{Poulton1974} reviews the early tracking literature, while Jagacinski and Flach \cite{Jagacinski2003} provide an accessible textbook review of manual control approaches.

Early work relies on heuristics \cite{card1986model, card1980klm, card1983the, kieras1997overview, anderson1997act} and on low-parameter mathematical models \cite{fitts1954information, hick1952rate}. More recent work extends these models and, for instance, predicts the operating time for a linear menu \cite{10.1145/1240624.1240723}, gaze patterns \cite{salvucci2001integrated}, pointing \cite{muller2017control, martin2021intermittent, murray2021forward, ikkala2022breathing}, or cognitive load \cite{duchowski2018index}.

Recently, reinforcement learning gained popularity within the research area of computational user models. This popularity is due to its neurological plausibility \cite{botvinick2012hierarchical, frank2012mechanisms}, allowing it to serve as a model of human cognitive functioning. The underlying assumption of RL in HCI is that users behave rationally within their bounded resources \cite{gershman2015computational, oulasvirta2022computational}. There is evidence that humans use such a strategy across domains, such as in causal reasoning \cite{denison2013rational} or perception \cite{gershman2012multistability}. In human-computer interaction, researchers have leveraged RL to automate the sequence of user actions in a keystroke-level-model framework \cite{leino2019computer} or to predict fatigue in volumetric movements \cite{cheema2020predicting}. It was also used to explain search behavior in user interfaces \cite{yang2020predicting} or menus \cite{chen2015emergence} and as a model for multitasking \cite{jokinen2021multitasking}. Most similar to our work is research on hierarchical reinforcement learning for user modeling. \citeauthor{jokinen2021touchscreen} \cite{jokinen2021touchscreen} show that human-like typing can emerge with the help of Fitts' Law and a gaze model. Other works show that HRL can elicit human-like behavior in task interleaving \cite{gebhardt2020hierarchical} or touch interactions \cite{jokinen2021touchscreen}.

Specifically, Multi-Agent Reinforcement Learning (MARL) is of interest \cite{debard2020multiagent}. MARL is interesting as it captures the implicit closed-loop iterative nature of HCI. We can see both the human and computer as two agents that interact together in an environment. MARL is popular in robotics \cite{yang2004multiagent}. Furthermore, MARL is closely related to alignment theory as MARL deals with training multiple AI agents to behave in desired ways. The challenge of aligning multiple agents' objectives with each other and with human values relates closely to fundamental alignment problems \cite{ma2022elign, rodriguez2022instilling}. MARL is also highly relevant to cooperative AI, as it provides frameworks for training multiple agents to work together toward common goals. Techniques from MARL, like centralized training with decentralized execution \cite{chen2019new}, can be used to develop AI systems that cooperate effectively \cite{tan1993multi}.

In this dissertation, we first use simple heuristics to model human behavior in a pen-based haptic feedback guidance system. We embedded this human behavioral model into an MPC that controls the haptic system. Afterwards, we use RL to model human behavior in adaptive interfaces. This is used to implicitly learn a behavioral model by the adaptive interface, to show optimal adaptations to the user.

\subsection{Shared Control}
Traditionally, there is a clear distinction between assistance systems, where the machine only supports the human, and automation, where the machine takes over the main task, replacing the human. Sheridan recognized that this distinction should not be so black and white, and proposed levels of automation \cite{sheridan1978human}. There are many situations where both the human and the machine should act together simultaneously, and where authority and tasks need to be shifted or adapted \cite{sheridan2011adaptive, miller2003beyond}.
Human-machine cooperation is a broader concept that includes shared control but also extends to cooperation at higher tactical and strategic levels like guidance and navigation. Shared control, specifically, is an approach where control is shared between human users and automated components, such as robots or computer systems. It involves cooperation at the control level between humans and machines \cite{flemisch2016shared}. Shared control is often discussed in the context of physical systems, such as (semi-)autonomous vehicles \cite{marcano2020review} and human-robot interaction \cite{losey2018review}.