%!TEX root = ../main.tex

\section{Related Work}
\label{sec:related}
This paper proposes multi-agent RL as a framework for adaptive UIs. Our method features a \useragent that models human interaction behavior and an \interfaceagent that adapts the UI to support the \useragent.
Most related to our work is research on computational user modeling, methods for adaptive UIs, and (MA)RL.

\subsection{Computational User Modelling}
Computational user modeling has a long tradition in HCI. These models predict user performance and are essential for UI optimization \cite{oulasvirta2018computational}. Early work relies on heuristics \cite{card1986model, card1980klm, card1983the, kieras1997overview, anderson1997act} and on simple mathematical models \cite{fitts1954information, hick1952rate}.
More recent work extends these models and, for instance, predicts the operating time for a linear menu \cite{10.1145/1240624.1240723}, gaze patterns \cite{salvucci2001integrated}, or cognitive load \cite{duchowski2018index}.

Recently, reinforcement learning gained popularity within the research area of computational user models.
This popularity is due to its neurological plausibility \cite{botvinick2012hierarchical, frank2012mechanisms}, allowing it to serve as a model of human cognitive functioning.
The underlying assumption of RL in HCI is that users behave rationally within their bounded resources \cite{gershman2015computational, oulasvirta2022computational}.
There is evidence that humans use such strategy across domains, such as in causal reasoning \cite{denison2013rational} or perception \cite{gershman2012multistability}.
In human-computer interaction, researchers have leveraged RL to automate the sequence of user actions in a KLM framework \cite{leino2019computer} or to predict fatigue in volumetric movements \cite{cheema2020predicting}.
It was also used to explain search behavior in user interfaces \cite{yang2020predicting} or menus \cite{chen2015emergence} and as a model for multitasking \cite{jokinen2021multitasking}.
Most similar to our work is research on hierarchical reinforcement learning for user modeling.
\citeauthor{jokinen2021touchscreen} \cite{jokinen2021touchscreen} show that human-like typing can emerge with the help of Fitts' Law and a gaze model.
Other works show that HRL can elicit human-like behavior in task interleaving \cite{gebhardt2020hierarchical} or touch interactions \cite{jokinen2021touchscreen}.
Inspired by this work, we design our user agent by decomposing high-level decision-making from motor control.
Using two hierarchical levels yields simulated behavior.

\subsection{Methods for Adaptive UIs}
UI adaptation can either be offline, to computationally design an interface, or online, to adapt the UI according to users' goals. We will focus on online adaptive UIs and refer readers to \cite{combinatorialoptimizationdesign2020oulasvirta, functionalityselection2017oulasvirta} for an overview of computational UI design.

\subsubsection{Heuristics, Bayesian Networks \& Combinatorial Optimization}
In early works, heuristic- or knowledge-based approaches are used to adapt the UI  \cite{Browne1990,Stephanidis1997,Smith2010}.
Similarly, multi-agent systems employ rule-based and message-passing approaches  \citep{Rich1998,Rich2005,Yorke2012}. Another popular technique for AUIs is domain-expert-designed Bayesian networks  \citep{Horvitz1998, Bosma2004}.
More recently, combinatorial optimization was used to adapt interfaces dynamically \cite{park2018adam, lindlbauer2019context}.
The downside of these approaches is that experts need to specify user goals using complex rule-based systems or mathematical formulations. 
Creating them comprehensively and accurately requires developers to foresee all possible user states, which is tedious and requires expert knowledge.
Commonly, these approaches also get into conflict when multiple rules or objectives apply.
This conflict often results in unintuitive adaptations.
In contrast, our method only requires the layout of the UI.
From its representation as an RL environment, we learn policies that meaningfully adapt the UI and realistically reproduce user behavior.

\subsubsection{\del{Machine Learning} \add{Supervised Learning}}
Leveraging machine learning can overcome the limitations of heuristic-, network-, and optimization-based systems by learning appropriate UI adaptions from user data.
Traditional machine learning approaches commonly learn a mapping from user input to UI adaptation. 
Algorithms like nearest neighbor  \cite{Maes1995, Lashkari1997}, Na\"ive Bayes  \cite{McCreath2006,Faulring2010}, perceptron \cite{Shen2009a, Shen2009b}, support vector machines \cite{Berry2011}, or random forests \cite{Pejovic2014, Mehrotra2015} are used and models are learned offline \cite{Berry2011} and online \cite{Shen2009a}.
Due to the problem setting, these approaches require users' input to be highly predictive of the most appropriate adaptation. 
Furthermore, it restricts the methods to work in use cases where myopic planning is sufficient, i.e., a single UI adaptation leads users to their goal. 
In contrast, our method considers multiple goals when selecting an adaptation and can lead users to their goal using sequences of adaptations. 

More recent work overcomes the limitations stemming from simple input-to-adaptation mapping by following a two-step approach. They (1) infer users' intention based on observations and (2) choose an appropriate adaptation based on the inferred intent \cite{oulasvirta2018computational}.
Such work uses neural networks, and user intention is modeled either explicitly \cite{kolekar2010learning, soh2017deep} or as a low-dimensional latent representation \cite{RIZZOGLIO2021}.
However, these approaches are still highly dependent on \deliui{the quality of the} training data, which may not even be available for emerging technologies. \deliui{In contrast, our method does not depend on pre-collected user data \emph{and} can learn supportive policies just by observing simulated user behavior. } \addiui{In contrast, our method can learn supportive policies without pre-collected user data by just observing simulated user behavior.}

\subsubsection{Bandits \& Bayesian Optimization} 
Bandit systems are a probabilistic approach often used in recommender systems \cite{glowacka2019bandit}.
In a multi-armed bandit setting, each adaptation is modeled as an arm with a probability distribution describing the expected reward.
The Bayes theorem updates the expectation, given a new observation and prior data.
Related work leverages this approach for AUIs \cite{lomas2016interface,koch2019may,kangas2022scalable}.
Bayesian optimization is a sample-efficient global optimization method that finds optimal solutions in multi-dimensional spaces by probing a black box function \cite{shahriari2015taking}. In the case of AUIs, it is used to find optimal UI adaptations by sampling users' preferences \cite{Koyama2014, Koyama2016}.
Both approaches trade off exploration and exploitation when searching for appropriate adaptations (i.e., exploration finds entirely new solutions, and exploitation improves existing solutions), rendering them suitable approaches to the AUI problem.

However, such methods are not able to plan adaptations over a sequence of interaction steps, i.e., they plan myopic strategies.
In addition, these approaches need to sample user feedback to learn or optimize for meaningful adaptations and, hence, also rely on high-fidelity user data.
Furthermore, as users themselves learn during training or optimization, solutions can converge to sub-optimal user behavior as such methods reduce exploration with convergence. 
In contrast, our method can plan adaptations over a sequence of interaction steps learned from realistic, simulated user data.  

\subsubsection{Reinforcement Learning} 
Reinforcement learning is a natural approach to solving the AUI problem, as its underlying decision-making formalism implicitly
captures the closed-loop iterative nature of HCI \cite{howes2018interaction}. 
It is a generalization of bandits and learns policies for longer horizons, where current actions can influence future states.
This generalization enables selecting UI adaptations according to user goals that require multiple interaction steps.
Its capability makes RL a powerful approach for AUIs with applications in dialog systems \cite{Gasic2014, Su2017}, crowdsourcing \cite{PengCrowdsourcing2013, Hu2018}, sequential recommendations \cite{Chen2019, Liu2018, Liebman2015}, information filtering \cite{seo2000reinforcement}, personalized web page design \cite{ferretti2014exploiting}, and mixed reality \cite{gebhardt2019learning}.
Similar to our work is a model-based RL method that optimizes menu adaptations \cite{todi2021adapting}.

Current RL methods sample predictive models \cite{todi2021adapting, Gasic2014, Hu2018} or logged user traces  \cite{gebhardt2019learning}. However, these predictive models and offline traces represent user interactions with non-adaptive interfaces. Introducing an adaptive interface will change user behavior; so-called co-adaptation \cite{mackay2000responding}. Hence, it is unclear if the learned model can choose meaningful adaptations when user behavior changes significantly due to the model's introduction. In contrast, our user agent learns to interact with the adapted UI; hence, our interface agent learns on behavioral traces from the adapted setting.

\subsubsection{Multi-Agent Reinforcement Learning}
MARL is a generalization of RL in which multiple agents act, competitively or cooperatively, in a shared environment \cite{zhang2021multi}.  
%
Multi-agent systems are common in games \cite{baker2019emergent, jaderberg2019quake}, robotics \cite{ota2006multiagent,sariff2018multiagent}, or modeling of social dilemmas \cite{chao2015social,leibo2017multi}. MARL is challenging since multiple agents change their behavior as training progresses, making the learning problem non-stationary. Common techniques to address this issue is via implicit \cite{tian2020implicit} or explicit \cite{foerster2016learning} communication, centralized critic functions \cite{lowe2017multi,yu2021surprising}, or curricula \cite{epciclr2020,wang2020curriculum}. We take inspiration from the latter and use a curriculum during the training of our agents.

Closest to our work is \cite{debard2020multiagent}, which proposes a multi-agent system that maps 2D interface trajectories to actions for navigating 3D virtual environments. A user agent learns interactions on a 2D interface. A decoder that is trained on real user data maps the user agent's actions to 2D touch gestures. A second agent then translates these 2D touch gestures into 3D operations. 
In their setting, the interface agent does not observe the environment itself but receives the actions of the user agent as its state.
Our work extends their setting to the case where the user agent and the interface agent observe and manipulate the same UI. Furthermore, we do not rely on real world user data.
