\section{Implementation}
We train the user and \interfaceagent's policies simultaneously in a shared environment (the AUI). All policies receive an independent reward, and the actions of the policies influence a shared environment. We execute actions in the following order: (1) the \interfaceagent's action, (2) the \useragent's high-level action, followed by (3) the \useragent's low-level motor action. The reward for the two learned policies is computed after the low-level motor action has been executed. The episode is terminated when the \useragent has either completed the task or exceeded a time limit.

We implement our framework in Python 3.8 using RLLIB \cite{liang2018rllib} and Gym \cite{brockman2016openai}. We use PPO \add{\cite{schulman2017proximal, yu2022surprising}} to train our policies. We use 3 cores on an Intel(R) Xeon(R) CPU @ 2.60GHz during training \add{and an NVIDIA TITAN Xp GPU}. Training takes $\sim$36 hours. \del{We utilize an NVIDIA TITAN Xp GPU for training.} The \useragent's high-level decision-making policy $\policy_d$ is a 3-layer MLP with 512 neurons per layer and ReLU activation functions. The  \interfaceagent's policy  $\policy_I$ is a two-layer network with 256 neurons per layer and ReLU activation functions. \addiui{We sample the full state initialization (including goal) from a uniform distribution. We use stochastic sampling for our exploration-exploitation trade-off.} 

We use curriculum learning to increase the task difficulty and improve learnability. Specifically, we adjust the difficulty level every time a criteria has been met by increasing the mean number of initial attribute differences. More initial attribute differences result in longer action sequences and are therefore more complex to learn. We increase the mean by 0.01 every time the successful completion rate is above 90\% and the last level up was at least 10 epochs away.

We randomly sample the number of attribute differences from a normal distribution with standard deviation $1$, normalize the sampled number into the range $[1, n_a]$ and round it to the nearest integer, where $n_a$ is the number of attributes of a setting (in the case of game character $n_a=5$).

\del{The difference between agents of different applications is their respective state- and action spaces.}