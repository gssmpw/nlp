% \section{Conclusion}

The question addressed in this paper is whether it is possible to develop a \emph{general framework} for point-and-click AUIs that does not depend on task-specific heuristics or data to generate policies offline. To this end, we have introduced \marlui, a multi-agent reinforcement learning approach. Our method features a \useragent and an \interfaceagent. The \useragent aims to achieve a task-dependent goal as quickly as possible, while the \interfaceagent learns the underlying task structure by observing the interactions between the \useragent and the UI. Since the \useragent is RL-based and thus learns through trial-and-error interactions with the interface, it does not require real user data. We have evaluated our approach in simulation and with participants, by replacing the \useragent with real users, across five different interfaces and various underlying task structures. The tasks ranged from assigning items to a toolbar, handing out-of-reach objects to the user, selecting the best-performing interface, providing the correct object to the user, and enabling more efficient interaction with a hierarchical menu. 
Results show that our framework enables the development of AUIs with minimal adjustments while being able to assist real users in their task.
We believe that \marlui, and a multi-agent perspective in general, is a promising step towards tools for developing adaptive interfaces, thereby reducing the overhead of developing adaptive strategies on an interface- and task-specific basis.
