\section{Applications}
To demonstrate the versatility of our framework, we introduce four additional point-and-click interfaces to demonstrate how our approach generalizes to different scenarios. Every scenario offers a distinct adaption, task, and interface. Our method requires minimal to no adaptations for the different scenarios (i.e., mostly a change in the dimensions of the observation and action spaces). We showcase both 2D interfaces as well as Mixed Reality interfaces. We show how the \interfaceagent selects from a set of pre-designed UI widgets, hand user's the correct blocks when building a tower, move out-of-reach items closer, and make hierarchical menus more efficient during photo editing. 

Please refer to our supplementary video for visual demonstrations of the tasks. Due to the diverse nature of use cases, we will report either number of clicks or task completion time as success metric. All scenarios are showcased through the interaction of a real user with a trained \interfaceagent. The numerical results are obtain in simulation.

\subsection{Number Entry}
\label{sec:number}
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.6\columnwidth]{chapters/05_shared_control/rl/figures/keypad.pdf}
    \caption{Adaptive keypad: the \useragent is asked to enter a randomly initialized price by using a keypad (1). The \interfaceagent selects from three pre-designed different widgets either a normal keypad,  a digits-only keypad or a non-digits-only keypad. The \useragent selects a button of the chosen widget. The task ends when the \useragent presses enter (4).}
    \label{fig:price}
\end{figure}

We introduce a price entry task on a keypad. The \interfaceagent selects a widget from a pre-designed set. We show that our approach can support applications requiring users to issue command sequences and provide meaningful help given a user's progress in the task (see \Fig{price}). 
The task assumes a setting where the simulated user must enter a product price between $10.00$ and $99.99$.
To complete the task, the \useragent has to enter the first two digits, the decimal point, the second two digits, and then press enter. 
The \interfaceagent can select one of three different interface layouts: i) a standard keypad, ii) a keypad with only digits and iii) a widget with only the decimal point and the enter key. 

The goal difference penalty (\Eq{rd}) in this case is based on whether the current price $\tools$ matches the target price $\gattr$: 
\begin{equation}
    \error_{gd} = -\sum_t\mathbbm{1}_{\tools_t \neq \gattr_t},
\end{equation}
where $\mathbbm{1}$ is an indicator that is 1 if $\tools_t \neq \gattr_t$ and $0$ otherwise, and $t$ is the current timestep. Every time a button is hit, $t$ increases by 1. This is similar to the penalty in all other tasks. However, it considers that the order of the entries matters. On average, the \useragent needs $4.0$ seconds to complete the task in cooperation with the \interfaceagent, compared to $4.9$ seconds when using a static keypad. The number of clicks is identical, since the full task can be solved on the standard keypad.

\subsubsection*{Qualitative Policy Inspection} We observe that the \interfaceagent learns to select the UI that has the biggest buttons for an expected number entry (e.g., only digits or only non-digits). From this we can conclude that the \interfaceagent implicitly learns the concept of Fitt's law and prioritizes larger buttons where appropriate. 

\subsection{Block Building}
\label{sec:building}
The second scenario is a block-building task (\Fig{building}) where the user constructs various castle-like structures from blocks. Compared to the game character task, only the dimensionality of the observation and action space needs to be changed. It can choose between 4 blocks (wall, gate, tower, roof) and a delete button. The agent needs to move the hand to a staging place for the blocks (see \Figure{building}) and then place the block in the corresponding location. The block cannot be placed in the air, i.e., it always needs another block on the floor below. The \interfaceagent suggests a next block every time the user places a block. However, the user can put the block down, in case it is unsuitable. An action is picking or placing a block. 

This task represents a subset of tasks that do not have a Heads-Up-Display-like UI to interact with, but are situated directly in the virtual world. This is a common interactive experience of AR/VR systems. The user needs on average $1.1$ actions with our framework, compared to $2.0$ actions without the \interfaceagent. Thus 1.1 indicates that the \interfaceagent suggests the correct next block, most of the time. 

\subsubsection*{Qualitative Policy Inspection} We observe that the policy learns to always suggest a block that is usable given the current state of the tower. This indicates that the policy has an implicit understanding of the order of blocks and can distinguish between those belonging to the foundation versus the upper parts of a tower.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.6\columnwidth]{uist_2023/figures/building.pdf}
%     \vspace{-3mm}
%     \caption{Block Building: The user is building a castle from blocks (1). The user places the first block (2). The \interfaceagent suggests a next block to place (3). This is repeated till the castle is built (4). }
%     \Description{
%         A series of four figures is presented that describes each step for block building task. Different building blocks with different colors are presented in each figure. The target brush is green circle. The first figure describes the initialization. The second figure shows that the \useragent selects and places a block. The third figure shows that the \interfaceagent suggests a block to \useragent and \useragent grabs it. The fourth figure shows that this process is continued until the task is completed.
%     }
%     \label{fig:building}
% \end{figure}



    % Second figure
\begin{figure}[!t]

        \centering
        \includegraphics[width=0.5\columnwidth]{chapters/05_shared_control/rl/figures/building.pdf} % Ensure this height matches the first image
        \caption{Block Building: The user is building a castle from blocks (1). The user places the first block (2). The \interfaceagent suggests a next block to place (3). This is repeated till the castle is built (4). }
        \label{fig:building}
\end{figure}
    % First figure
\begin{figure}[!t]
        \centering
        \includegraphics[width=0.5\columnwidth]{chapters/05_shared_control/rl/figures/item_grab.pdf} % Adjust height as needed
        \caption{Out-of-reach object grabbing: the \useragent attempts to grab a specific object, that is initially out of reach, in a space containing multiple objects (1). The \useragent learned to move towards an object to indicate its intention to grab it (2). Based on that, the \interfaceagent learned to move the intended object within the \useragent's reach (3). The \useragent then grabs the object to finish the task (4).}
        \label{fig:objectgrab}
\end{figure}

\subsection{Out-of-reach Item Grabbing}
\label{sec:out-of-reach}

In the third usage scenario, the user needs to use their hand to grab an object that is initially out of reach. Thus, the \interfaceagent needs to move an object within reach of the user, which can then grab it. The \interfaceagent observes the location of the user's hand. The task environment includes several objects. Uniquely, in this scenario the user and the \interfaceagent are forced to collaborate to select the correct target object and complete the task (see \Figure{objectgrab}); as it is impossible for the \useragent to complete the task on its own. Compared to the game character task, only the dimensionality of the observation and action space needs to be changed. 

In this use case, we changed the lower level of our user to learn motor control with RL instead of using the Fitts-Law-based motor controller. This highlights the modularity of our approach and can be useful in scenarios where existing models, such as Fitts' Law, are not sufficient.
The low-level motor control policy controls the hand movement. In particular, given a target slot, the policy selects the parameters of an endpoint distribution. Given the current position and the endpoint parameters (mean and standard deviation), we compute the predicted movement time using the WHo Model \cite{guiard2015mathematical}. The low-level policy needs to learn i) the coordinates and dimensions of menu slots, ii) an optimal speed-accuracy trade-off given a target slot, and its current position. Refer to \Appendix{learned} for more details.

\subsubsection*{Qualitative Policy Inspection} \del{We qualitatively evaluate the learned policy.} We find that the the policy selects objects positioned in the direction of the user's arm movement rather than the closest ones. This indicates that the policy implicitly learns the correlation between directionality of movement and intent. 

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.6\columnwidth]{uist_2023/figures/item_grab.pdf}
%     \caption{Out-of-reach object grabbing: the \useragent attempts to grab a specific object, that is initially out of reach, in a space containing multiple objects (1). The \useragent learned to move towards an object to indicate its intention to grab it (2). Based on that, the \interfaceagent learned to move the intended object within the \useragent's reach (3). The \useragent then grabs the object to finish the task (4).}
%     \Description{
%         A series of four figures is presented that describes out-of-reach object grabbing task. The first figure describes the initialization, where there is a bookshelf and there are two cupboards, one which has plates inside it and the other one is empty. The second figure shows that the \useragent moves its hand towards the cupboard filled with plates. The third figure shows how the \interfaceagent reacts and removes a plate from the cupboard and bring it forward. The fourth and the last figure shows that the \useragent grabs the object brought forward by the \interfaceagent and completes the task.
%     }
%     \label{fig:objectgrab}
% \end{figure}

\subsection{2D Hierarchical Menu}
\label{sec:photo}
In this task, a user edits a photo by changing its attributes. A photo has five distinct attributes with three states per attribute: i) filter (color, sepia, gray), ii) text (none, Lorem, Ipsum), iii) sticker (none, unicorn, cactus), iv) size (small, medium large), and v) orientation (original, flipped horizontal, and vertical). The photo's attribute states are limited to one per attribute, i.e., the photo cannot be in grayscale and color simultaneously. This leads to a total of 15 attribute states and 243 photo configurations. 

The graphical interface is a hierarchical menu, where each attribute is a top-level menu entry, and each attribute state is in the corresponding submenu. By clicking a top-level menu, the submenu expands and thus becomes visible and selectable. Only one menu can be expanded at any given time. 

The photo attribute states correspond to the current input state and the target state. The \interfaceagent selects an attribute menu to open. Its goal is to reduce the number of clicks necessary to change an attribute, e.g., from two user interactions (filter->color) to one (color). For the \emph{\useragent}, the higher level selects a target slot, and the lower level moves to the corresponding location. 

\subsubsection*{Qualitative Policy Inspection} 
We observe that the \interfaceagent intelligent\add{ly} decides which submenu to open next. We notice that this is never a menu the user recently interacted with as the probability of having to change, e.g., the color twice in a row is minimal and only a result of errors.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{chapters/05_shared_control/rl/figures/hierarchical_good.pdf}
    \caption{We introduce a photo editing task where (1)~a user matches a photo to a target by operating a hierarchical menu. (2)~The user selects the submenu `size`. (3)~The user then selects the attribute `small`, which alters the image. (4)~After the user has changed an attribute, the interface observes the new state of the photo and finds the most likely submenu for the next user action. (5)~The user clicks on an item in the submenu to complete the task.}
    \label{fig:hierarchical_good}
\end{figure}