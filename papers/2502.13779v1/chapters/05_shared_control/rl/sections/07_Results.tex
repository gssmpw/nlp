\section{Evaluation}
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.6\columnwidth]{chapters/05_shared_control/rl/figures/character.pdf}
    \caption{In our proposed task the \interfaceagent and the \useragent interact in a turn-based manner, in which the user (agent) matches a game character selection to a target state  (1). First, the user operates a toolbar with three slots (2). Second, The interface agent assigns the most relevant items to the available slots (3). Finally, This cycle continues till the two characters match (4).}
    \label{fig:task}
\end{figure}

MARLUI aims to learn UI adaptations from simulated users that can support real users in the same task. Specifically, we want our framework to produce AUIs that are competitive with baselines that require carefully collected real user data. In this section, we evaluate if our approach achieves this goal. Thus, we first conduct an \emph{in silico study} to analyze how the \interfaceagent and the \useragent solve the UI adaptation problem in simulation. Then, we conduct a \emph{user study} to investigate if policies of the \interfaceagent that were learned in simulation benefit real users in terms of number of actions needed to complete a task.

\subsection{Task \& Environment}
\label{sec:task}
To conduct the evaluation, we introduce the \emph{character-creation task} (see \Fig{task}).
In this task, a user creates a virtual reality game character by changing its attributes. A character has five distinct attributes with three items per attribute: i) shoes (red, blue, white), ii) shirt (orange, red, blue), iii) glasses (reading, goggles, diving), iv) backpack (pink, blue, red), and v) dance (hip hop, break, silly). The characters' attribute states are limited to one per attribute, i.e., the character cannot be dancing hip hop and break simultaneously. This leads to a total of 15 attribute items and 243 character configurations. We sample the goal uniformly from the different configurations. 

The game character's attributes can be changed by selecting the corresponding items in a toolbar-like menu with three slots. The user can cycle through the items by selecting "Next." The static version of the interface has all items of an attribute assigned to the three slots, and every attribute has its own page (e.g., all shoes, if the user presses next, all backpacks). The character's attribute states correspond to the current input state $\tools$ and the target state $\gattr$, where $\gattr$ is only known to the \useragent. The goal of the \interfaceagent is to reduce the number of clicks necessary to change an attribute, by assigning the relevant items to the available menu slots. For the \emph{user agent}, the higher level selects a target slot, and the lower level moves to the corresponding location. 

\subsection{In Silico}
\paragraph{Training}

\begin{figure}[!t]
    \centering
    % First figure
        \centering
        \includegraphics[width=\textwidth]{chapters/05_shared_control/rl/figures/training.pdf} % Adjust height as needed
        \caption{We train our agent till convergence. Left: the fraction of successfully completed episodes per epoch. Ours and Static reach a 100\% successful completion rate. Random does not converge. Right: The number of actions needed on average during a successful episode. Our framework needs less actions compared to Static and Random.}
        \label{fig:training}
\end{figure}

\begin{figure}[t]{!t}
        \centering
        \includegraphics[width=0.5\textwidth]{chapters/05_shared_control/rl/figures/subset_done.pdf}
        \caption{The fraction of successfully completed episodes as function of the fraction of the total number of goals. The graph shows that it is sufficient to see half of the goals to learn policies that generalize to all goals.}
        \label{fig:goal}
\end{figure}


We evaluate the training of our framework against a static and a random interface. In the random interface, items are randomly assigned to the slots. \add{Our method would perform on par, or worse, with random if it was incapable of learning. Hence, the random baseline provides a lower bound for performance. The static baseline has no adaptations, allowing us to evaluate the general effectiveness of our adaptive interface. For most tasks, a task-specific heuristic could be found that presents an upper baseline. However, this has to be designed specifically for each task. On the other hand, our goal is to provide a general framework that works across tasks with minimal changes to the reward function and observation spaces.}

\Figure{training} shows the \useragent's task completion rate and number of actions per task of all three interfaces during training. Ours and the static baseline converge, whereas the random baseline does not. Furthermore, the mean number of actions of ours is lower than the mean of the static interface.

\paragraph{Generalization to unseen goals} 
% \begin{figure}
%     \centering
%     \vspace{0.8cm}
%     \includegraphics[width=0.4\columnwidth]{uist_2023/figures/subset_done.pdf}
%     \caption{
%     % We investigate the fraction of total goals our method needs to have access to. 
%     The fraction of successfully completed episodes as function of the fraction of the total number of goals. The graph shows that it is sufficient to see half of the goals to learn policies that generalize to all goals.}
%     \Description{A graph shows the relation between task completion rate and the fraction of goals the proposed framework needs to access. It is showed that the task completion rate increases until top and stays stable after the fraction of goals reach half of the goal subset.}
%     \label{fig:goal}
% \end{figure}
To understand how well our approach can generalize to unseen goals, we ablate the fraction of goals the agents have access to during training. We then evaluate the learned policies against the full set of goals, which is defined as all possible combinations of character attributes. 
The results are presented in \Figure{goal}. We find that having access to roughly half of the goals is sufficient to not impact the results. This indicates that our approach generalizes to unseen goals of the same set for this task. 

\paragraph{Understanding policy behavior}
We qualitatively analyzed the learned policies of our \interfaceagent to understand how it supports the \useragent in its task.
In \Figure{sequence}, we show a snapshot of two sequences with identical initialization. 
To reach the target character configuration, the user agent can either select the blue bag or the purple glasses (both are needed). 
Depending on which item the user selects at this time step, the \interfaceagent proposes different suggestions in subsequent steps. For instance, it might happen that the \interfaceagent suggest two relevant items; while the user can only select a single on. As example, the blue backpack the user did not select initially (\Figure{sequence}, bottom) gets suggested again later; as the user has not selected a single backpack during the interaction cycle.
This behavior shows that the \interfaceagent implicitly reasons about the attribute that the user intends to select based on previous interactions. \addiui{In short, the \interfaceagent learns to suggest items that the user is not wearing, or that the user has not interacted with.}


\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{chapters/05_shared_control/rl/figures/EXAMPLE.pdf}
    \caption{With our framework, multiple relevant items can be assigned simultaneously; yet the user can only select one (left). Depending on the user's action (top: select backpack, bottom: select glasses), other item gets assigned later (top: shoes, bottom: backpack). Note that the user has not selected any backpack in those two actions. This shows that our framework actively adapts to user input.}
    \label{fig:sequence}
\end{figure}

\subsection{User Study}
\label{sec:userstudy}
Our goal was to create a \useragent whose behavior resembles that of real users, so the \interfaceagent can support them in the same task. To this end, we evaluated the sim-to-real transfer capabilities of our framework by conducting a user study where the \interfaceagent interacted with participants instead of the \useragent.

\subsubsection{Baselines}
We compared our framework to two supervised learning approaches and the static interface (see \Sec{task}).
In line with previous work \cite{gebhardt2019learning}, we used a Support Vector Machine (SVM) with a Radial basis function (RBF) kernel as a baseline. \add{It represents a direct competitor to our approach, as it generalizes on a method level. However, it needs data recollection for every interface and task.}

We used the implementations of scikit-learn \cite{scikit-learn} and optimized the hyperparameters for performance. The feature vector of the baseline was identical to that of our framework. The baseline learned the probability with which a user will select a certain character attribute next. We assigned the three attributes with the highest probability to the menu slots. Note that we did not consider "Next" to be an item. 

\paragraph{Dataset} We collected data from 6 participants to train the supervised baselines. These participants did not take part in the user study. They interacted with the static interface, which resulted in a dataset with over 3000 logged interactions. \del{We found that more data points did not improve the performance of the SVM classifier through k-fold cross-validation}\add{We saw that the SVM performance saturates when increasing to more than 2700 data points} and reached around 91\% \addiui{top-3 classification accuracy (i.e., the percentage of how often the users' selected item was in the top three of the SVM output)} on a test set. Furthermore, we found that the baseline generalize well to unseen participants (again through cross-validation). \add{We used all data points for the final model.} 

\paragraph{Metrics}
We used two metrics (dependent variables) to evaluate our approach. 
\begin{enumerate}
    \item \emph{Number of Actions:} the number of clicks a user needed to complete a task, which is a direct measure of user efficiency \cite{card1980klm}. 
    \item \emph{Task Completion Time:} the total time a user needed to complete a task.
\end{enumerate}

\subsubsection{Procedure}
Participants interacted with the \interfaceagent and the two baselines. The three settings were counterbalanced with a Latin square design, and the participants completed 30 trials per setting. In each condition, we discarded the first six trials for training. The participants were instructed to solve the task as fast as possible while reducing the number of redundant actions. They were allowed to rest in-between trials. We ensured that the number of initial attribute differences (IAD), which refers to the number of clothing attributes that differ between the current selection and the target at the start of a trial, was uniformly distributed within the participant's trials. Participants used an Oculus Quest 2 with its controller. 

We recruited 12 participants from staff and students of an institution of higher education (10 male, 2 female, aged between 23 and 33). All participants were right-handed and had a normal or correct-to-normal vision. On average, they needed between 35 to 40 minutes to complete the study.

\subsubsection{Results}
\begin{figure}[!t]
     \centering
     \begin{subfigure}{0.3\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{chapters/05_shared_control/rl/figures/n_actions_v2-1.pdf}
        \label{fig:userstudy_actions}
    \end{subfigure}
    \hspace{0.5cm}
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \includegraphics[width=\textwidth]{chapters/05_shared_control/rl/figures/time_v2-1.pdf}
        \label{fig:userstudy_time}
    \end{subfigure}    
    \caption{The average number of actions (left) and the task completion time (right) participants needed to finish the tasks of our user study, plotted with the 95\% confidence interval. We compare Ours against Static, and an SVM. Our approach outperforms both baselines on the number of actions. Averaged over all participants and all trials.}
    \label{fig:userstudy}
\end{figure}

We present a summary of our results in \Figure{userstudy}. We analyzed the effect of conditions on the performance of participants with respect to the number of actions and task completion time. 

Participants needed on average $3.34$ actions to complete a task with our framework, compared to $5.73$, and $3.87$ for the static, and SVM baselines respectively. \add{The normality and sphericity were not violated.} We performed a repeated-measures ANOVA We found a significant effect on method ($F(2, 22)=209.68, p<.001$). With a Holm-corrected post-hoc we find that both the SVM and Ours significantly outperform the static interface (both $p<.001$) in terms of number of actions. We also find that ours significantly outperforms the SVM ($p=0.006$).  

It is important to decompose the results per initial attribute difference. This is crucial because a task with a single IAD is expected to be shorter in duration than tasks with multiple IADs. We report the values in \Table{desc}. Using a repeated-measures ANOVA, we found a significant effect ($F(8, 0.25)=35.537, p<.001$) on the interaction between the approach and the IAD. We limited our analysis to scenarios where the interaction involves "Ours" and only considered scenarios where the IADs between the methods were identical (i.e., we do not discuss, for example, the SVM with five initial attribute differences versus the static method with a single difference). Our findings from a Holm-corrected post-hoc analysis showed no difference between the static baseline and the other approach for a single initial attribute difference (all $p=1.00$) and a significant difference for more than one IAD (all $p<.001$). When comparing the SVM versus our method, we found no difference for one IAD (p=$1.00$) or for five (p=$0.26$); for all other cases, a difference was found (all $p<.001$). We observed that the \interfaceagent's strategy, which groups similar category items (e.g., showcasing pink and blue backpacks if the character has a red one cf. \Fig{sequence}), increases the chance of selecting a useful item. However, this strategy doesn't apply when all attributes need changing.

\begin{table}[!t]
\centering
\caption{User study result reported by IAD and method (Mean $\pm$ SD).}
\label{tab:desc}
\begin{adjustbox}{width=\textwidth}
{
\begin{tabular}{lccccc}
\toprule
& \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} \\
\cmidrule[0.4pt]{2-6}
\textbf{Heuristic} & $1.708 \pm 0.396$ & $3.889 \pm 0.451$ & $6.139 \pm 0.797$ & $7.903 \pm 0.845$ & $9.000 \pm 0.000$ \\
\textbf{SVM} & $1.456 \pm 0.219$ & $3.747 \pm 0.515$ & $4.607 \pm 0.174$ & $5.439 \pm 0.259$ & $5.827 \pm 0.389$ \\
\textbf{Our} & $1.625 \pm 0.569$ & $\textbf{2.036} \pm \textbf{0.297}$ & $\textbf{3.281} \pm \textbf{0.657}$ & $\textbf{4.719} \pm \textbf{0.613}$ & $5.311 \pm 0.186$ \\
\bottomrule
\end{tabular}
}
\end{adjustbox}
\end{table}

When looking at the task completion time, participants using our framework needed $12.14$ seconds to complete the task. The completion time was $12.36$ for the static interface and $12.71$ \add{for the SVM}. The task completion time was normally distributed (Shapiro-Wilk $p>0.05$). We found no significant difference in overall task completion time with a Greenhouse-Geisser (for sphericity) corrected repeated-measures ANOVA ($F(1.568, 17.243)=0.86, p=0.42$). This could be, because despite requiring more action for a static baseline, the participants formed a strategy.

\subsection{Discussion}
To analyze if our multi-agent framework is competitive with a baseline that requires carefully collected real user data, we compared it against a supervised SVM. We did not find significant differences between the two approaches in the task performance metrics of the number of actions and task completion time. This suggests that our approach is a competitive alternative to data-driven approaches for creating adaptive user interfaces. 

The adaptive approaches significantly reduce the number of actions necessary to complete the task compared to the static interface. However, no significant differences in task completion times were found. We argue that this could be due to real users being more familiar with the ordering of items in the static interface that is kept constant across trials. This familiarity is not captured by our current cognitive model or incentivized in the reward function. In future work, we will model familiarity and investigate its effect on task completion time. 

We have shown qualitatively that our \interfaceagent learns to take previous user actions into account. This characteristic is core to meaningful adaptations. At the moment, our agent's capabilities are limited by the size of the stack $\stack$. In the future, recurrent approaches such as LSTM could be investigated to overcome this limitation. 

Furthermore, we presented evidence that our framework can generalize to goals that were not seen during training. 
It is important to mention that the results of this experiment are subject to its task and that seen and unseen goals are from the same distribution. Nevertheless, the study provides first indications that our approach generalizes to applications where not all users' goals might not always be encountered during training.