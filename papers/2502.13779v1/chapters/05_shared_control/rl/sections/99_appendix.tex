\section{Learned Lower Level}
\label{app:learned}
We detail how to learn the low-level motor policy. The low-level policy needs to learn i) the coordinates and dimensions of menu slots, ii) an optimal speed-accuracy trade-off given a target slot, and its current position. 

To prevent the low-level motor control policy from correcting wrong high-level decisions and to increase general performance, we limit the state space $\StatePerPolicy_M$ to strictly necessary elements with respect to the motor control task \cite{christen2021hide}: 

\begin{equation}
    \StatePerPolicy_M = \left (\pos, \target \right ),
\end{equation}
with the current position $\pos \in I^2$, the target slot $\target \in \mathbb{Z}_2^{\nslots}$. 

The action space $\ActionPerPolicy_M$ is defined as follows:

\begin{equation}
    \ActionPerPolicy_M = \left(\mu_{\pos}, \sigma_{\pos} \right).
\end{equation}
It consists of $\mu_{\pos} \in I^2$ and $\sigma_{\pos} \in I$, i.e., the mean and standard deviation which describes the endpoint distribution in the unit interval. We scale the standard deviation linearly between a min and max value where the minimum value is the size of normalized pixel width and the max value is empirically chosen to be 15\% of the screen width. Once an action is taken, we sample a new end-effector position from a normal distribution: $\pos \sim \mathcal{N}\left(\mu_{\pos}, \sigma_{\pos}\right)$.

Given the predicted actions, we compute the expected movement time via the WHo model \cite{guiard2015mathematical}, similar to our non-learned low-level motor control policy in the main paper. 

The reward for the low-level motor control policy is based on the \emph{motoric} speed-accuracy trade-off. Specifically, we penalize: i) missing the target supplied by the high-level $(\miss)$, and ii) the movement time ($\mt$). Furthermore,  we add a penalty iii)  which amounts to the squared Euclidean distance between the center of the target $\target$ and $\mu_\pos$. This incentivizes the policy to hit the desired target in the correct location. Since the penalty only considers the desired point $\mu_\pos$, it will not impact the speed-accuracy trade-off (which is a function of $\sigma_\pos$). The total reward is defined as follows:

\begin{equation}
    \RewardPerPolicy_M = \underbrace{\satweight (\miss)}_{i)} - \underbrace{(1-\satweight) \mt}_{ii)} - \underbrace{\beta ||\mu_{\pos} - \mu_\target||_2^2}_{iii)},
\end{equation}
where $\miss$ equals $0$ when the target button is hit and $-1$ on a miss. A hit occurs when the newly sampled user position $\pos$ is within the target $\target$, while a miss happens if the user position is outside of the target. $\satweight$ is a speed-accuracy trade-off weight and $\beta$ is a small scalar weight to help with learning.

% \section{2D Hierarchical Menu}
% \label{app:hierarchicalmenu}

% \begin{figure}
%     \centering
%     \includegraphics[width=\columnwidth]{uist_2023/figures/hierarchical_good.pdf}
%     \caption{We introduce a photo editing task where (1)~a user matches a photo to a target by operating a hierarchical menu. (2)~The user selects the submenu `size`. (3)~The user then selects the attribute `small`, which alters the image. (4)~After the user has changed an attribute, the interface observes the new state of the photo and finds the most likely submenu for the next user action. (5)~The user clicks on an item in the submenu to complete the task.}
%     \Description{
%         A series of five figures that describe each step of a photo editing task is presented. A toolbar, photo, and cursor are presented in each figure, which has five components: 1) Filter, 2) Text, 3) Sticker, 4) Size, and 5) Orri., which is the abbreviation of orientation. The first figure describes initialization, where the cursor is on the photo. The second figure shows how the user opens the size menu, so the cursor is on the size button on the toolbar. The third figure shows that the user selects a small-size option by moving the cursor over the small choice. The fourth figure shows that the filter menu is opened by the interface agent. Therefore, the cursor is on the photo, not on the toolbar. The fifth and last figure describes that the user selected the grey filter from the previously opened filter menu by moving the cursor. 
%     }
%     \label{fig:hierarchical_good}
% \end{figure}

% In this task, a user edits a photo by changing its attributes. A photo has five distinct attributes with three states per attribute: i) filter (color, sepia, gray), ii) text (none, Lorem, Ipsum), iii) sticker (none, unicorn, cactus), iv) size (small, medium large), and v) orientation (original, flipped horizontal, and vertical). The photo's attribute states are limited to one per attribute, i.e., the photo cannot be in grayscale and color simultaneously. This leads to a total of 15 attribute states and 243 photo configurations. 

% The graphical interface is a hierarchical menu, where each attribute is a top-level menu entry, and each attribute state is in the corresponding submenu. By clicking a top-level menu, the submenu expands and thus becomes visible and selectable. Only one menu can be expanded at any given time. 

% The photo attribute states correspond to the current input state  $\tools$ and the target state $\gattr$, where $\gattr$ is only known to the \useragent. The \interfaceagent selects an attribute menu to open. Its goal is to reduce the number of clicks necessary to change an attribute, e.g., from two user interactions (filter->color) to one (color). %, by inferring what the user most likely intends to do next. 
% For the \emph{user agent}, the higher level selects a target slot, and the lower level moves to the corresponding location. 
