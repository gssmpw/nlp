%!TEX root = ../main.tex

\section{Method}
We first present an outline the model of our \useragent, consisting of a high- and low-level policy. Then we present the \interfaceagent (\Fig{overview}).

\subsection{General Task Description}
We model tasks to be completed if the \useragent achieves their desired goal. For game character creation, a goal can be the desired configuration of a character with a certain shirt (red, green, blue), and backpack (pink, red, blue). We represent the goal as a one-hot vector encoding $\gattr$ of these attributes. A one-hot vector can be denoted as  $\mathbb{Z}_{2}^{j}$, where $j$ is the number of items. For the previous example, $\gattr$ would be in $\mathbb{Z}_2^6$ as it possesses six distinct items. 

Furthermore, the \useragent can access an input observation denoted by $\tools$, e.g., this can correspond to the current character configuration. The current input observation, $\tools$, and the goal state $\gattr$ are identical in dimension and type. 

The \useragent interacts with the interface and attempts to match the input observation and goal state as fast as possible, such that $\tools = \gattr$. Each interaction updates $\tools$ accordingly, and a trial terminates once they are identical. In the character creation example, this would be the case if the shirt and backpack of the edited character are the same as the desired configuration. The \interfaceagent makes online adaptations to the interface. It does \emph{not} know the specific goal of a user. Instead, it needs to observe user interactions with the interface to learn the underlying task structure that will yield the optimal adaptations, e.g., the user likely wants to configure the shoes after configuring the backpack.

\add{The \interfaceagent learns to adapt the UI to the \useragent by maximizing the same expected discounted reward. Specifically, the \interfaceagent learns to infer optimal next adaptations over an infinite horizon by updating implicit probabilities of likely next actions of the \useragent, given its current sequence of past actions. As such it does not learn an explicit or implicit goal probability distribution, but a distribution of the most likely next actions of the \useragent. An example is to suggest a white and blue shoe to the \useragent in the tool, as the \useragent has not interacted with the category of shoes thus far (\Fig{fig:teaser_rl}). }

\subsection{User Agent}
\label{sec:user_agent}
% The \useragent interacts with an environment to achieve a certain goal (e.g., select the intended attributes of a character). The agent tries to accomplish this as fast and accurately as possible. Thus, the \useragent first has to compare the goal state and input observation and then plan movements to reach the target. We model the user as a hierarchical agent with separate policies. 


The \useragent interacts with an environment to achieve a certain goal (e.g., select the intended attributes of a character). The agent tries to accomplish this as fast and accurately as possible, hence it minimizes task completion time. Thus, the \useragent first has to compare the goal state and input observation and then plan movements to reach the target. We model the \useragent as a hierarchical agent with separate policies for a two-level hierarchy \cite{Langerak:2021:Generalizing}. First, we introduce a high-level decision-making policy $\policy_d$ that computes a target for the agent (high-level decision-making), we approximate visual cost with the help of existing literature \cite{10.1145/1240624.1240723}. Second, a WHo Model Fitts'-Law-based low-level motor policy $\policy_m$ that decides on a strategy to reach this target. We now explain both policies in more detail.

\subsubsection{High-level Decision-Making Policy}
The high-level decision-making policy of the hierarchy is responsible to select the next target item in the interface. The overall goal of the policy is to complete a given task while being as fast as possible. Its actions are based on the current observation of the interface, the goal state, and the agent's current state. More specifically, the high-level state space $\StatePerPolicy_d$ is defined as:

\begin{equation}
    \StatePerPolicy_d = \left (\pos, \menu, \tools, \gattr \right ),
    \label{eq:sd}
\end{equation}
which comprises: i) the current position of the \useragent's end-effector normalized by the size of the UI, $\pos \in I^n$ (where $n$ denotes the dimensions, e.g., 2D vs 3D), ii) an encoding of the assignment of each item $\menu \in \mathbb{Z}_2^{\nitems \times \nslots}$, with $\nitems$ and $\nslots$ being the number of menu items and environment locations, respectively, iii) the current input state $\tools \in \mathbb{Z}_2^{\nitems}$, and iv) the goal state $\gattr \in \mathbb{Z}_2^{\nitems}$. Here, $I$ denotes the unit interval $[0,1]$, and 
$\mathbb{Z}_{2}^{n}$ is the previously described set of integers.
The item-location encoding $m$ represents the current state of a UI. It can be used, for instance, to model item-to-slot assignments. The action space $\ActionPerPolicy_D$ is defined as:
\begin{equation}
    \ActionPerPolicy_d = \target,
    \label{eq:ad}
\end{equation}
which indicates the next target slot $\target \in \mathbb{N}_{\nslots}$. The reward for the high-level decision-making policy consists of two weighted terms to trade-off between task completion accuracy and task completion time: i) how different the current input observation $\tools$ is from the goal state $\gattr$, and ii) the time it takes to execute an action. Therefore, the high-level policy needs to learn how items correlate with the task goal as well as how to interact with any given interface. With this, we define the reward as follows: 

\begin{equation}
    \RewardPerPolicy_d =  \alpha \underbrace{\error_{gd}}_{i)} - (1-\alpha)\underbrace{\left(\dect + \mt\right)}_{ii)} + \mathbbm{1}_{\text{success}},
    \label{eq:rd}
\end{equation}
where $\error_{gd}$ is the difference between the input observation and the goal state, $\alpha$ a weight term, $\mt$  the movement time as an output of the low-level policy, $\dect$ the decision time, and $\mathbbm{1}_{\text{success}}$ an indicator function that is 1 if the task has been successfully completed and 0 otherwise. 

In addition to movement time, we also need to determine the decision time $\dect$. To this end, we are \addiui{inspired by} the SDP model \cite{10.1145/1240624.1240723}. This model interpolates between an \addiui{approximated} linear visual search-time component \del{($T_s$)} and the Hick-Hyman decision time \cite{hick1952rate} \del{($T_{hh}$)}, both functions take into account the number of menu items and user parameters. 

We define the difference $\error_{gd}$ between the input observation $\tools$ and the goal state $\gattr$ as the number of mismatched attributes:
\begin{equation}
    \error_{gd} = - \sum_{x \in \gattr, y \in \tools }\frac{\mathbbm{1}_{x \neq y}}{n_{attr}},
\end{equation}
where $\mathbbm{1}$ is an indicator function that is $1$ if $x\neq y$ and else $0$,  $x$ and $y$ are individual entries in the vectors $\gattr$ and $\tools$ respectively, and $n_{attr}$ is the number of attributes (e.g., shirt, backpack, and glasses).  


\subsubsection{Low-Level Motor Control Policy}
\label{sec:ll}
The low-level motor control policy is a non-learned controller for the end-effector movement. In particular, given a target, it selects the parameters of an endpoint distribution (mean $\mu_\pos$ and standard deviation $\sigma_{\pos}$) . We set $\mu_\pos$ to the center of the target. The target $\target$ is the action of the higher-level decision-making policy ($\ActionPerPolicy_D$). 
Following empirical results \cite{fitts1954information}, we set $\sigma_{\pos}$ to 1/6th of a menu slot width to reach a hitrate of 96\%.

Given the current position and the endpoint parameters (mean and standard deviation), we compute the predicted movement time using the Fitts' Law derived WHo Model \cite{guiard2015mathematical}.
\begin{equation}
        \mt = \left ( \frac{k}{(\sigma_{\pos}/d_{\pos}-y_0)^{1-\beta}} \right ) ^{1/\beta} + \mt^{(0)},
\end{equation}
where $k$ and $\beta$ are parameters that describe a group of users, $\mt^{(0)}$ is the minimal movement time, and $y_0$ is equal to the minimum standard deviation. The term $d_{\pos}$ indicates the traveled distance from the current position to the new target position $\mu_\pos$. We follow literature for the values of other parameters \cite{guiard2015mathematical, jokinen2021touchscreen}. We sample a new position from a normal distribution: $\pos \sim \mathcal{N}\left(\mu_{\pos}, \sigma_{\pos}\right)$.


\subsection{Interface Agent}
The \interfaceagent makes discrete changes to the UI to maximize the performance of the \useragent. 
% For instance, it assigns items to a toolbar to simplify their selection for the \useragent.
In our running example of character customization, it assigns items to a toolbar to simplify their selection for the \useragent.
Unlike the \useragent, we model the \interfaceagent as a flat RL policy. The state space $\StatePerPolicy_I$ of the interface agent is defined as:

\begin{equation}
    \StatePerPolicy_I = \left (\pos, \tools, \menu, \stack \right ),
    \label{eq:si}
\end{equation}

which includes: i) the position of the user $\pos \in I^2$, ii) the input observation $\tools \in \mathbb{Z}_2^{\nitems}$, iii) the current state of the UI $\menu \in \mathbb{Z}_2^{\nitems \times \nslots}$, and iv) a vector including the history of interface elements the \useragent interacted with (commonly referred to as stacking). The action space $\ActionPerPolicy_I \in \mathbb{Z}$ and its dimensionality is application-specific. 
The goal of the \interfaceagent is to support the \useragent. Therefore, the reward of the \interfaceagent is directly coupled to the performance of the \useragent. We define the reward of the \interfaceagent to be the reward of the \useragent's high-level policy:

\begin{equation}
    \RewardPerPolicy_{I} = \RewardPerPolicy_{D}.
    \label{eqn:ri}
\end{equation}

Note that the \interfaceagent does \emph{not} have access to the \useragent's goal $\gattr$ or target $\target$. To accomplish its task, the \interfaceagent needs to learn to help the \useragent based on an implicit understanding of i) the objective of the \useragent, and ii) the underlying task structure. Our setting allows the \interfaceagent to gain this understanding solely by observing the changes in the interface as the result of the \useragent's actions. This makes the problem more challenging but also more realistic. 

The \interfaceagent learns an implicit distribution of possible goals, and by observing the \useragent it narrows down the distribution over goals. At every time step the \interfaceagent suggests the items that are most likely needed, given the goal distribution. 