\section{Framework Overview}
% Point-and-click AUIs dynamically assign relevant interactive elements to an interface, thereby decreasing number of decisions and actions users need to take. This, in turn, increases the overall usability \cite{gebhardt2019learning, lindlbauer2019context}. 

\add{We define ''adaptation'' as the online alteration of an interface given current and past user input to support users in completing their task more efficiently. As such AUIs dynamically assign relevant interactive elements to an interface, thereby decreasing number of decisions and actions users need to take.}
MARLUI is a framework to create AUIs \del{that dynamically assign the most relevant interactive elements to a point-and-click interface.
MARLUI} and can learn adaptive policies, \add{that is neural networks that given an input predict suitable adaptations}. \add{MARLUI learns} without needing data and only necessitates minimal adjustments to learn these policies for different AUIs. \add{We specifically, assume the users to be Computational Rational \cite{oulasvirta2022computational} to make the problem tractable.}
In this section, we provide a high-level overview of the workings of our proposed framework.
As shown in \Fig{overview}, MARLUI consists of three main components: a user interface as a shared environment \add{(which gets adapted)},  a \useragent \add{(which learns to interact with the AUI)}, and an \interfaceagent that learns an adaptation policy. 
Referencing the game character customization example illustrated in \Fig{teaser_rl}, we first describe the function of each component individually, followed by an explanation of how MARLUI can support real users.


\paragraph*{User Interface / Shared Environment}
Our approach models UI adaptation as a MARL problem, where an \interfaceagent learns from a simulated \useragent's interactions with a user interface, i.e., the shared environment. In this shared environment, user- and \interfaceagent take actions in a turn-based manner. The \useragent performs a point-and-click maneuver, which changes the state of the UI, e.g., altering the clothing of the game character. The \interfaceagent observes the action and the change to the interface and based on that selects a new subset of items to display, e.g., adjusting the clothing items displayed in the toolbar.
In turn, the \useragent observes this adaptation and takes a new action. This cycle continues till the task is complete, e.g., the desired clothing configuration is reached. 

\paragraph*{User Agent}
The \useragent aims to achieve its goal as fast as possible. In the context of game character creation, goals might involve selecting specific attributes for a character, such as the color of a shirt or backpack. The \useragent can observe the visible parts of the interface, and knows its internal state. Based on these information, the \useragent acts on the interface with point-and-click maneuvers, aiming to align the current state of the UI with its goal, e.g., aligning the current- with the desired clothing configuration. 
By constraining its motor behavior in a physiologically plausible manner, the \useragent is bound to exhibit human-like behavior for pointing and selecting items.
Through trial and error, the \useragent will eventually learn a policy that allows it to realize this alignment.
% The interactions continue until the task is completed, e.g., the desired clothing configuration is reached.

\paragraph*{Interface Agent}
The \interfaceagent adapts the UI in a turn-based manner to minimize the number of actions the \useragent must perform to complete a task. Despite not knowing the \useragent's specific goal, it learns the task structure \add{(i.e., sequence of states)} through observing the \useragent's interactions with the UI. 
It can then learn to select the subset of items that are most relevant to the \useragent at its current state, e.g., dynamically populating the toolbar with the cloting items that are most likely to be picked.
Through trial and error, it can learn UI adaptation policies without relying on pre-collected user data or predefined heuristics.

% \paragraph*{Joint Training}
% We train the \useragent and \interfaceagent jointly in the shared environment. This process allows the \interfaceagent to learn the underlying task structure and make relevant adjustments to the interface. Through observation and trial and error, the \interfaceagent learns to support the \useragent's efforts to achieve their goals more efficiently.

% \paragraph*{Real World Application}
% By simulating a realistic \useragent, the \interfaceagent can provide meaningful adaptations in real-world tasks. This is demonstrated through the MARLUI learning procedure, where the \useragent's goal is defined but unknown to the \interfaceagent. For instance, in game character customization, the \useragent selects attributes like backpacks, glasses, and garments using a toolbar dynamically populated by the \interfaceagent. The goal state is randomly determined, and the \useragent learns to achieve this goal through interaction by simulating human-like behavior. By assigning items to menu slots and observing the \useragent's performance, measured by the time taken for the task to be completed, the \interfaceagent learns the task structure from observing the \useragent's interactions. This collaborative exploration between the \useragent and \interfaceagent allows the latter to guide the \useragent's actions, paving the way for applications with real users.

\paragraph*{Interaction with Real Users}
By mimicking human-like point-and-click behavior through the \useragent, the \interfaceagent can learn to adapt UIs such that it also assist real users in accomplishing the same task.
To apply the learned adaptive interface to real users, the setting is changed such that \interfaceagent interacts with the actual users instead of the \useragent. 
In a turn-based fashion, it selects the most relevant next items after each click or selection of the actual user according to what it has learned through interactions with the \useragent.

% \begin{delfigure}[t]
%     \centering
%     \includegraphics[width=0.9\textwidth]{uist_2023/figures/method.pdf}
%     \caption{\textcolor{red}{\sout{Our \interfaceagent and \useragent act in the same environment. The \useragent is modeled as a two-level hierarchy with a high-level decision-making policy $\policy_d$ and a low-level motor control policy $\policy_m$. The \useragent interacts with the UI. The high-level policy of the \useragent observes that state of the environment (\Eq{sd}) and chooses a specific menu slot as target accordingly (\Eq{ad}). The lower level receives this action and computes a movement (\Sec{ll}). The \interfaceagent policy $\policy_I$ adapts the interface to assist the \useragent in achieving its task more efficiently. It observes user actions in the UI (\Eq{si}) and decides on adaptations. Note that the \interfaceagent cannot access the goal, making the problem partially observable.}}}
    
%     \Description{
%         Flow Chart with three columns labeled Hierarchical User Agent, shared Environment, and \interfaceagent.
%         The shared environment contains a GUI with different selection possibilities for different clothing. The hierarchical user agent is subdivided into 1) decision-making policy. This observes the GUI and outputs an action. 2) A motor control policy, this takes as input the action and interacts subsequently with the GUI. The \interfaceagent has a single component. This component observes the user interaction and adapts the GUI.  
%     }
%     \label{fig:overview}
% \end{delfigure}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\textwidth]{chapters/05_shared_control/rl/figures/method_cr.pdf}
    \caption{Our \interfaceagent and \useragent act in the same environment. The \useragent is modeled as a two-level hierarchy with a high-level decision-making policy $\policy_d$ and a low-level motor control policy $\policy_m$. The \useragent interacts with the UI. The high-level policy of the \useragent observes that state of the environment (\Eq{sd}) and chooses a specific menu slot as target accordingly (\Eq{ad}). The lower level receives this action and computes a movement (\Sec{ll}). The \interfaceagent policy $\policy_I$ adapts the interface to assist the \useragent in achieving its task more efficiently. It observes user actions in the UI (\Eq{si}) and decides on adaptations. Note that the \interfaceagent cannot access the goal of the user, making the problem partially observable.}
    \label{fig:overview}
\end{figure*}
