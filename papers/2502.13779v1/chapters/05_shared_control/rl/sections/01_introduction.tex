\section{Introduction}
In the previous chapter, we demonstrated the use of model predictive control to determine the actuation of a haptic feedback device, relying on known system dynamics that included predictive models of user behavior. However, these models have limitations. To overcome this, we investigate a model-free reinforcement learning approach for learning system, task, and user dynamics. In this chapter, we shift our focus from haptics to adaptive point-and-click interfaces, allowing us to emphasize control aspects, as actuation and sensing in graphical user interfaces are deterministic.

Point-and-click interfaces form the core interaction paradigm in modern Human-Computer Interaction (HCI) \cite{reilly2005just, kemp2008point, lafuente2023comparing}. These interactions include clicking toolbar items, navigating hierarchical menus, and selecting objectsâ€”from UIs on traditional 2D desktops to emerging spatial UIs in immersive Mixed Reality. While individual point-and-click interactions are simple, complexity increases with the number of available interactive items. More items require users to process more information and evaluate more options, increasing cognitive load and diminishing the user experience, potentially extending task completion times.

Adaptive User Interfaces (AUIs) aim to mitigate this complexity by dynamically adjusting the interface to display the most relevant items for the user's current task while omitting less relevant information. This reduces cognitive load and simplifies navigation through complex interfaces, such as hierarchical menus.

Developing effective AUIs poses numerous challenges: AUIs must (1) infer users' intentions by observing their behavior and input, and (2) adapt the user interface based on these inferred intentions~\cite{oulasvirta2018computational}. Previous research has predominantly relied on machine learning (ML) techniques~\cite{Shen2009a, Shen2009b, todi2021adapting, gebhardt2019learning}, or heuristics~\cite{Browne1990,Stephanidis1997,Smith2010}. However, these methods depend on manually collected data or carefully hand-crafted rules, which is a significant limitation. Each new user interface or task requires new data collection and updated or redesigned adaptations, even when the interaction paradigm remains unchanged.

For example, customizing a game character through a toolbar interface illustrates these challenges \figref{fig:teaser_rl}. The toolbar has limited slots, but the set of clothing items is much larger. An AUI must assign the most relevant items to the slots based on the user's previous selections and likely final outfits. Similarly, in a VR game where a user builds a castle from different blocks, the AUI recommends the most likely block to be selected for each operation, considering the user's past choices and probable final designs. Despite the similar point-and-click paradigm, ML or heuristic approaches would require separate data collection and rule design for each task. This requirement makes developing more complex AUIs time-consuming and costly.

In this paper, we propose \marlui, a proof-of-concept framework for easy development of AUIs across various point-and-click UIs. We frame the point-and-click interaction paradigm in a Multi-Agent Reinforcement Learning (MARL) framework. An \interfaceagent learns to adapt a UI by selecting the most relevant subset of items at each interaction step. The \interfaceagent observes the \useragent and minimizes its task completion time, while the \useragent learns human-like point-and-click behavior to interact with the UI and train the \interfaceagent. Unlike previous approaches requiring real user data to train an \interfaceagent, our method relies on a human-like \useragent. The \useragent and the \interfaceagent jointly learn by exploring the interface through trial-and-error. Our \interfaceagent then transfers the learned adaptation strategy to real scenarios, selecting the most relevant items after a real user's click or selection. Switching tasks within the \marlui framework, such as from dressing a game character to constructing a block castle, requires minimal modification and only training on the respective interface. This approach eliminates the need for developers to gather or create task- and interface-specific data or heuristics, streamlining AUI development. Future work can focus on user personalization, extending to more complex interfaces, adapting to changing goals, and addressing different tasks.

To model the point-and-click paradigm as a MARL problem, we propose a simulated \useragent to learn interaction with a UI for task completion. We model the \useragent hierarchically, decomposing decision-making, such as selecting a toolbar item, from motor control actions, such as moving the cursor to a desired menu slot. Targeting human-like behavior in the \useragent, we incorporate cognitive and motor control bounds, following advancements in Computational Rationality~\cite{oulasvirta2022computational}. Concurrently, we train an \interfaceagent. The \useragent aims to reach a goal state, while the \interfaceagent adapts the UI to help the \useragent achieve its goal more efficiently, minimizing the number of clicks. The \useragent and \interfaceagent operate in a turn-based manner: the \useragent acts, followed by the \interfaceagent's adaptation, continuing until the task is completed. This novel approach enables our data- and heuristic-free method.

To demonstrate our framework's effectiveness across various tasks and interfaces, we explore five use cases: (1) customizing a game character using a toolbar, (2) adaptive numeric keypad design, (3) building a block tower, (4) selecting initially out-of-reach objects, and (5) photo editing with anticipatory menu opening. These use cases cover a broad range of interface types, from heads-up displays to world-anchored interfaces, showcasing the versatility of our approach.

We evaluate our framework in two stages: in-silico studies on the character creation task, showing our \interfaceagent can generalize to unseen goals, and evaluations with real participants, comparing \marlui against data-driven baselines. Our findings indicate that training the \interfaceagent with our simulated \useragent enables real users to reduce the number of actions needed compared to previous frameworks.

In summary, we make three key contributions in this paper:
\begin{enumerate}
\item The first attempt at a multi-agent Reinforcement Learning approach that does not rely on real user data or hand-crafted heuristics to adapt point-and-click user interfaces in real-time.
\item A \useragent that learns to operate a user interface and enables an \interfaceagent to learn the task's underlying structure purely by observing the \useragent's actions.
\item An \interfaceagent that learns the task's underlying structure by observing actions in the interface by the \useragent.
\item Results from evaluations demonstrating the effectiveness of our approach and five use cases showcasing its general applicability for point-and-click tasks.
\end{enumerate}
