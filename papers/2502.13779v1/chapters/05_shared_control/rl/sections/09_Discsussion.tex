\section{Discussion}
\label{sec:limitations}
MARLUI models the interaction with point-and-click adaptive interfaces as a multi-agent cooperative game by teaching a simulated \useragent and an \interfaceagent to cooperate. Learned policies of the interface agent have shown their capability to effectively assist real users. Demonstrating our approach in a wide variety of use cases is a first step towards general methods that are not tied to specific applications nor dependent on manually crafted rules or offline user data collection. However, there are limitations that require further research.

In this work, we have focused on point-and-click interfaces. However, it would be interesting to extend the user agent to model other interaction paradigms. By enhancing our user agent to replicate behavior for other interaction types than clicking, we could extend the possible use cases that MARLUI can support. For example, research has shown that gaze-based selection, similar to cursor movement, follows Fitts' Law \cite{schuetz2019explanation}. \add{Furthermore, similar concepts can also be applied to human-robot interactions, such as using simulated humans to train human-robot handshakes or human-to-robot handovers \cite{christen2023learning, christen2019guided, christen2023synh2r}.} 

Moreover, user goals can change during human-computer interaction, particularly in creative tasks where users constantly adjust their objective based on intermediate results. This presents challenges for standard RL approaches, which assume goals to remain stationary. Future research on MARL for AUIs needs to focus on finding strategies to easily adapt trained interfaces to changing or new user goals. This is required to establish more robust and flexible adaptive interfaces that can support real-world use cases.

We have demonstrated that our formulation solves problems with up to 5 billion possible states, as in the character creation application (\Sec{task}). However, the complexity of the problem grows exponentially with the number of states. This makes it challenging for MARLUI to scale to interfaces with even larger state spaces. To overcome this, we could explore different input modalities, such as representing the state of the UI as an image instead of using one-hot encoding. This approach is similar to work on RL agents playing video games \cite{mnih2013playing}, which showed that image representations can effectively cope with large state spaces.

We have shown that the simulated \useragent's behavior was sufficiently human-like to enable the \interfaceagent to learn helpful policies that transfer to real users. The \interfaceagent's performance is inherently limited by the \useragent. Therefore, increasing realism in the model of the simulated user is an interesting future research direction, for instance, modeling human-like search \cite{chen2015emergence} or motor control with a biomechanical model \cite{fischer2021reinforcement}. \add{Along similar lines, our work helps with the creation of policies that adapt interfaces given user interactions. However, it does not adapt to the user themselves (e.g., different levels of expertise). Such personalization is an interesting direction for future research.}

Our framework has theoretical appeal because it provides a plausible model of the bilateral nature of AUIs: the adaptation depends on the user, whereas also the user action depends on the adaptation. Modeling this unilaterally as in supervised learning does not reflect reality well. Related to ongoing research \cite{murray2022simulation}, we believe that future work can leverage our framework to gain a better theoretical understanding of how users interact with a UI.  Our setup has the potential to scale to multiple users with different skills and intentions. This could lead to bespoke assistive UIs for users with specific needs or UIs for users with specific expertise levels. 

Finally, our proposed framework enables adaptive policies for different point-and-click tasks and interfaces. We have shown how our framework produces policies that support users in a variety of these interfaces and tasks. Building on this, future work can investigate the transition from framework to developer tool. Tools that enable developers to use our framework easily and consistently will streamline the development process of AUIs.
