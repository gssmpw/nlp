\chapter{Summary \& Insights}
\label{ch:control:conclusion}

\paragraph{Summary}
In summary, we introduced two optimal control strategies: \magpen and \marlui. In ''\magpenTitle'' (\chapref{ch:control:optimal}), we developed an optimization method for electromagnetic haptic guidance systems using the MPCC framework. This method balanced user input and system control, allowing for trajectory and speed adjustments as needed. It optimized system states and inputs over a receding horizon by solving a stochastic optimal control problem at each timestep. Our design dynamically adjusted forces and automatically modified magnet position and strength, suitable for real-time optimization. We implemented our method on a prototype hardware platform, \magpen, and demonstrated that users responded well to the feedback, achieving higher accuracy compared to open-loop and time-dependent closed-loop methods. This approach offered a principled method for haptic guidance, broadly applicable in areas such as drawing, sketching, writing, or virtual haptic tools. However, our MPCC-based framework relied on known system dynamics, which are not always available, especially when considering user behavior.

In our second project, ''\marluiTitle'' (\chapref{ch:control:multi}), we addressed this limitation by introducing a model-free multi-agent reinforcement learning strategy for adaptive user interfaces. \marlui incorporated a \useragent and an \interfaceagent. The \useragent aimed to achieve a task-specific goal quickly, while the \interfaceagent learned the task structure by observing the \useragent's interactions with the UI. This method did not require real user data, as the \useragent learned through trial and error. We evaluated our approach through simulations and real user tests across five different interfaces and various task structures. The results indicated that our framework supported adaptive UI development with minimal adjustments, effectively assisting real users in their tasks. \marlui, and the multi-agent approach in general, represent a promising direction for developing adaptive interfaces, reducing the need for interface- and task-specific strategies.

Overall, our methods enabled an autonomy-automation trade-off, resulting in better user performance in terms of speed, accuracy, and number of actions compared to alternatives. Our research is a starting point for considering and investigating the role of predictive user models in control loops.

\paragraph{Implications}
Previous systems commonly employ open-loop control \cite{yamaoka2013depend} to manage intelligent systems. This method does not take the user into account, thereby preventing the balancing of user agency with system automation. Alternatively, some systems rely on heuristics \cite{Lopes16, Browne1990, Smith2010, Stephanidis1997}, which involve the creation of rules by experts. Additionally, supervised learning \cite{Maes1995, Lashkari1997, McCreath2006, Faulring2010, Shen2009a, Shen2009b, Berry2011, Pejovic2014, Mehrotra2015} is widely used for controlling intelligent systems. However, supervised learning focuses on short-term decisions. By only optimizing for the immediate next step, they limit the system's ability to intelligently balance user agency with system automation.

In contrast, our approach minimizes a cost function over a receding horizon constrained by user and system dynamics. We have demonstrated how to integrate models of human behavior explicitly and implicitly into optimal control strategies for intelligent systems. This approach allows systems to consider future states and actions and optimize inputs accordingly. In \magpen, we embedded a user model explicitly in the control strategy, considering user behavior over a horizon, though mathematically formulating this behavior can be challenging. Conversely, in \marlui, we took a multi-agent RL approach where user behavior is implicitly learned, eliminating the need for explicit behavioral models or system dynamics descriptions, and allowing the method to generalize across interfaces and tasks.

Our method builds on existing cognitive models in the literature, leveraging previous research. This grounding in cognitive models enables advancements on two fronts: 1) developing cognitive models for user bounds, and 2) applying RL methods to more complex scenarios. Treating HCI, and specifically the interaction with intelligent systems, as a multi-agent reinforcement problem is intuitive and facilitates future extensions of our work.

\paragraph{Limitations}
Our research has limitations in the context of shared variable interfaces.

First, both \magpen and \marlui require an objective function. In task-driven scenarios, these objective functions are relatively intuitive, such as accuracy or task completion time. However, in open-ended scenarios, such as browsing the web or engaging in creative tasks, defining an appropriate objective is challenging. Further research, such as Inverse Reinforcement Learning, is required to extract objective functions from user data.

Second, our work does not explicitly account for how human behavior changes based on the system's actions. In \magpen, we assume a compliant user, and in \marlui, the interaction might be implicitly learned. However, explicit models focusing on the interaction and the influence of system actions on user actions are vital.

Finally, neither \magpen nor \marlui considers the user's learning phase. During interaction, users develop a mental model of the system. Ideally, the system would be aware of the user's mental model. Embedding theory-of-mind models into a control strategy could create methods that work for both novice and expert users and personalize interactions based on user behavior, resulting in more intuitive and efficient interactions.