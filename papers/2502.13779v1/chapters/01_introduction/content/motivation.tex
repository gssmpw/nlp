\section{Motivation}
Although human-computer interaction may appear relatively unchanged since the post-war era, as we still mainly rely on the mouse and keyboard, today's intelligent systems depend on much more than just explicit user input. Autonomous vehicles utilize computer vision to anticipate human intent and movement \cite{janai2020computer, maqueda2018event, hee2013motion}. GitHub Copilot automatically adapts and generates code using large language models \cite{wermelinger2023using, imai2022github}. Smart home devices, like Nest thermostats, learn user preferences to automatically optimize comfort and energy efficiency \cite{yang2013learning, pisharoty2015thermocoach}, and advanced software like photo editing tools amplify human creativity \cite{liu20233dall}. These modern systems leverage contextual understanding to adapt to user needs. However, the shift from explicit to implicit user input, which comes from contextual understanding, is not without challenges. Systems that take action without explicit inputs reduce user agency. Thus, despite significant advances in the contextual understanding that these intelligent systems leverage, interaction with these systems remains challenging. In this dissertation, we focus on algorithmic methods to balance the automation provided by implicit contextual understanding with user agency achieved through explicit input in the area of Human-Computer Interaction (HCI).

In intelligent systems, a user interacts with an artificial agent that has contextual knowledge. The incorporation of contextual understanding aims to make these systems more natural and beneficial for real-world human tasks \cite{xu2023transitioning}. The goal of the artificial agent is to assist users in completing tasks, perform tasks on the users' behalf, or otherwise help them. In contrast to non-context-aware devices (e.g., \cite{engelbart1962augmenting, engelbart1968research}) that rely on explicit and precise user input, intelligent systems are always available. This crucial property allows intelligent systems to proactively engage with their environment and users. In turn, this property enables implicit and imprecise input from the user. Humans leverage these kinds of interactions on a daily basis -- e.g., saying "can you hand me \emph{this}?" \cite{lee2024gazepointar}. Furthermore, the proactive capabilities enable intelligent systems to act on behalf of the user by inferring their intent -- e.g., a robot handing you a glass of water without being told to do so \cite{christen2023learning}. Yet, intelligent systems are still capable of acting based on explicit user commands, seemingly losing no ground to classical forms of human-machine interaction. However, as intelligent systems are capable of both implicit (system automation) and explicit (user agency) interactions, they need to strike a balance between user agency and the automation of tasks.

This agency-automation trade-off becomes even more crucial in interactions where the system and user operate on a shared variable, that is, a variable that can be manipulated simultaneously by both the user and the system. For instance, consider the control of a semi-autonomous car. The user changes the car's acceleration, while the intelligent system simultaneously adjusts it. The artificial agent might increase safety and driving efficiency. However, it might not include passenger preferences. Similarly, a drawing assistance system might take control of a pen, thereby increasing accuracy yet constraining the user's creative expression. Also consider a virtual reality intelligent adaptive user interface. The interface may show optimal adaptations and decrease task completion time, but it does not necessarily align with user expectations. In all these scenarios, the ownership of the variable is ambiguous, and therefore, the control is ambiguous.

This ambiguity and agency-automation balance is where the primary challenge for interaction with intelligent systems lies. Fully automated systems could potentially provide greater efficiency and measurably better performance but at the cost of compromising user agency and diminishing perceived usability and overall experience. On the other hand, systems that rely solely on user agency provide complete control but may be sub-optimal, inefficient, and fail to augment the user's abilities. This dissertation explores the question: \emph{How can we algorithmically control intelligent systems with shared variables to balance user agency and system automation?}

Researchers have explored the balance between agency and automation in intelligent systems. In the 1960s, \citeauthor{BarHillel1960} \cite{BarHillel1960} addressed optimizing human-computer labor division in translation tasks. The 1990s saw debates over user agency versus system automation \cite{Shneiderman1997}, leading to a consensus on automation that enhances productivity while maintaining control. Additionally, research on human-AI collaboration emphasizes the importance of bidirectional communication, trust, and shared goals to enhance team effectiveness \cite{demir2017team, shively2017human}. To facilitate bidirectional communication, both models of human behavior and intelligent control strategies are necessary. Traditional methods to model human-computer interaction have focused on manual control theory \cite{McRuer1967, Costello1968} using heuristics \cite{card1986model, card1980klm, card1983the, kieras1997overview, anderson1997act} and low-parameter mathematical models \cite{fitts1954information, hick1952rate}. More recent approaches leverage reinforcement learning (RL) due to its ability to predict user behavior \cite{jokinen2021multitasking, jokinen2021touchscreen, gebhardt2020hierarchical}. These models of human behavior need to be integrated into the control strategies of systems. However, most haptic devices use a form of open-loop control \cite{yamaoka2013depend}, and thus do not take the user into account. Alternatively, some devices use proportional-integral-derivative control (PID) \cite{abut2018interface, ramos2016wavenet, pothi2014design}. However, PID, among other shortcomings, optimizes for a fixed setpoint, thereby limiting user freedom. This dissertation builds upon these foundations by integrating simple heuristics and RL to model human behavior in both the control of pen-based haptic feedback systems and the policy of adaptive user interfaces. We introduce novel kinesthetic haptic devices, essential for exploring the agency-automation spectrum, and employ model-based control strategies and multi-agent RL to dynamically adjust to user interactions, providing real-time optimal adaptations and enhancing user experience.

In summary, AI-driven systems are becoming increasingly ubiquitous in people's work and lives. These systems are no longer just "conventional" computing systems, but intelligent systems that exhibit unique characteristics that bring new interaction paradigms. This shift presents a challenge in balancing user agency and system automation, particularly when the user and system share control over a variable. This dissertation aims to explore how to strike an optimal balance between user control and system efficiency in interactions with intelligent systems and shared variables. We approach this question by introducing novel haptic devices, model-based control, and reinforcement learning approaches.