\chapter{Outlook and Future Work}
\label{ch:outlook}
This dissertation has demonstrated the potential of shared control for interaction with intelligent systems that have shared variables, but further exploration is needed. We identify three important directions for future research: improving the system's understanding of human behavior, enabling systems to reason from a human perspective, and enhancing control strategies to incorporate models of human behavior.

\section{Understanding Humans}
\subsection{Human Sensing \& Inference}
The shift towards contextual intelligent systems will integrate the world at large—including the users themselves—into the interface. For instance, in a scenario where a robot and a human collaborate to clean an apartment, the robot should infer and predict the user's intent from the state of the world and the user's actions without explicit instructions.

Accurate human state estimation is essential for this, including physical states captured through computer vision and, more challengingly, latent states such as expertise, fatigue, and intent. Advances in machine learning, particularly in areas such as human latent state inference \cite{li2017inferring}, are needed to tackle these challenges. Additionally, the system should be aware of its environment and capable of extrapolating and predicting future states. Physics-based machine learning holds promise for understanding physical world dynamics \cite{christen2023learning}, while behavioral models are necessary for predicting human behavior. A first project could be to learn a classifier that predicts the extent to which a user is an expert in complex software such as CAD based on their cursor movement.

\subsection{Human Behavioral Models}
Sensing the human state alone is insufficient. To apply current state estimations to intelligent control, we need to predict future human states. This requires behavior models that capture the complex dynamics of human behavior. Challenges include stochastic system dynamics, changing user optimization goals, and the impact of system actions on users.

Data-driven approaches such as imitation learning and inverse reinforcement learning, combined with cognitive models, can help tackle these problems \cite{kwon2020inverse}. Reinforcement learning, assuming computational rationality, can approximate optimal human-like behavior. By incorporating data-driven priors, we can model more complex behaviors \cite{Langerak:2024:rile}. Integrating these approaches will enhance our understanding of human behavior and improve interface design and interaction paradigms. An interesting direction could be to use text-conditioned models that generate realistic user behavior given a prompt (e.g., "A novice user browses the website to look for new sneakers").

\subsection{Non-Stationary and Abstract Goals}
Our research has assumed specific, concrete goals, but this may not translate well to more abstract tasks such as creative activities or web browsing. Goals can also change dynamically during an activity.

Future research should focus on hierarchically structuring goals and dynamically adjusting their probabilities. Recognizing interactions as stochastic decisions at various levels, a hierarchical goal model could better adapt to uncertainty. Additionally, understanding the necessary accuracy of higher-level goals is crucial. In \magpen, we showed that a low-level polynomial approximation of a sketch was sufficient when updated iteratively.

\section{Understanding what Humans Understand}
The systems and humans might observe the world differently. Whether this is due to occlusions, quality of sensors, or other reasons, the system should be able to infer i) what the user has observed and ii) what the user believes about the world state. These capabilities might allow the system to predict future user actions and expectations more accurately.

\subsection{Human-Like Scene Understanding}
Humans do not have perfect scene understanding. They might miss something outside their field of view or something that does not match their prior knowledge. Current user models often assume complete scene awareness, which can be problematic for human-like user models.
Future studies should integrate insights from human cognitive perception with advanced computer vision techniques. Research questions include determining what a person can see from a monocular RGB image or video and estimating their field of view given a map and location. Extending gaze estimation techniques to real-world scenarios could provide valuable insights \cite{akinyelu2020convolutional}.

\subsection{Theory-of-Mind User Models}
Understanding user beliefs is crucial, but integrating these beliefs into control systems remains a challenge. Future research could explore incorporating probabilistic belief inferences into control frameworks. This requires precise belief inference, representation, and dynamics, along with optimization criteria that meaningfully utilize these beliefs.

Bayesian models offer a tractable path, keeping track of belief states at each inference step \cite{baker2011bayesian}. However, this requires knowing probability matrices. Recurrent networks such as LSTM can approximate complex dynamics, but the exact solution to the belief model problem remains an open question. A first project could predict human-belief states in simple games in which the user cooperates with an agent to solve a task (e.g., finding the exit of a maze), where the belief is limited to the agent state (e.g., position) and end-goal (the maze exit).

\section{Cooperative Control}
Finally, we need to integrate human sensing, inference, and behavioral models into intelligent control strategies, paving the way for more intuitive and seamless interactions between humans and machines that respect and enhance human decision-making processes.

\subsection{Embedding User Models in System Control}
Core to useful cooperative control strategies is embedding models of human behavior into the system control. However, how to embed human behavioral models is not entirely clear. We could embed these human models hierarchically into the system control strategy. However, this would be computationally expensive as we need to solve multiple control problems per timestep. Alternatively, we can learn implicit user models either from data or in simulation. While this might be a more scalable approach, how to learn these models implicitly remains an open question. A first project would be to directly compare explicit versus implicit embedded user models in a simple task to investigate the trade-offs involved.

\subsection{Combine Model-based and Model-free approaches}
Model-based methods like MPC and model-free methods like RL each have unique advantages. MPC can adjust to new and different objectives without retraining, while RL excels at developing policies for unpredictable system behaviors. Combining these approaches could greatly improve computational assistance for users.

Future developments might involve using MPC to offer initial direction in novel situations based on user goals, with RL continuously refining and customizing the system. Learning complex environmental dynamics and integrating them into model-predictive optimization can compute policies without retraining for new objectives. Alternatively, inverse reinforcement learning might offer cost functions to use in MPC when system dynamics are known.

