\chapter{Conclusion}
\label{ch:summary}
In this dissertation, we investigate the trade-off between user agency and system automation in interactive intelligent systems, focusing specifically on interactions involving shared variables. These interactions allow both a user and an artificial agent to manipulate the same variable. Despite rapid advancements in contextual intelligent systems, research on their interaction remains insufficient. We approach this problem from two angles: first, by investigating a haptic interface as the interaction medium, and second, by leveraging optimal control strategies to balance agency and automation more effectively.

\section*{The Design of Shared Variable Interfaces}
In a shared variable interface, both the user and the system act on a shared variable, which can take forms such as kinesthetic haptic devices or graphical menus. Kinesthetic haptic feedback serves as a special instance of this shared variable, where both action and perception occur in the same modality from the user's perspective, such as using hands to change or perceive a joystick's position. This contrasts with interfaces where perception and interaction modalities differ, like touch interfaces. Previous kinesthetic haptic feedback systems often involved complex devices requiring user instrumentation, which limited full user agency. To overcome these limitations, we introduce novel haptic interfaces that elicit grounded forces for system automation and are untethered for full user agency.

\subsection*{1. \omniHapTitle}
In \chapref{ch:shared:contact}, we introduce \omniHap, a contact-free, non-planar haptic feedback device with a spherical electromagnetic actuator. The actuator consists of three orthogonal coils integrated into a single sphere, capable of eliciting radial and tangential forces on a permanent magnet embedded in a tool. Despite the tool being untethered, the force remains grounded via the electromagnet. Our device can produce forces up to 1N, which can be increased by enhancing the volume of the embedded permanent magnet, albeit at a weight trade-off. Users were able to discriminate at least 25 points spread evenly on the device's sphere (\figref{fig:confusion_circle_error}), with mean errors of $2.5mm \pm 1.4$, $5.7mm \pm 4.6$, $6.5mm \pm 5.2$, and $7.2mm \pm 5.1$ across different azimuth angles (\figref{fig:error_vs_angle}). \omniHap showed that enabling feedback in the same modality as the user's exertion and with a shared variable results in natural interactions. However, the reliance on external tracking for tool position remained a limitation due to its expense and complexity.

\subsection*{2. \omniUISTTitle}
\chapref{ch:shared:volumetric} introduces on-device sensing with \omniUIST, integrating Hall sensors into the \omniHap base. This core contribution allows us to decompose the natural interference caused by simultaneous magnetic tracking and actuation. Our novel gradient-based optimization method minimizes the difference between estimated and observed magnetic fields, affording 3D tracking capabilities with a mean error of $6.9mm$ during actuation (\figref{fig:optitrack_eval}). Furthermore, we improved the actuator design compared to \omniHap, doubling the peak force to $\pm 2N$ at 15A (\figref{fig:Fz_vs_iz}). We demonstrated \omniUIST's capabilities through three Virtual Reality applications: sculpting, gaming, and soft object exploration (\figref{fig:usecases}). \omniUIST showed how it is possible to sense the tool state and take it into account in the control loop without user instrumentation or cumbersome setups.

\subsection*{Implications}
Our work lays the foundation for investigating haptic devices from a new perspective, that of \interfacesLower. We have taken the first step towards understanding haptic \interfacesLower and demonstrate new applications in gaming, object exploration, and design. This integration of sensing and actuating the tool through electromagnetism is both cost-effective and efficient. The primary advantages of \omniHap and \omniUIST are their high accuracy and substantial force output, achieved without mechanically moving parts, thereby avoiding wear and increasing user agency. By eliciting forces on the tool manipulated by the user, we enable haptic feedback in the same modality as the user's exertion, resulting in natural interactions. However, more intelligent system control is needed for complex dynamics.

\section*{The Control of Shared Variable Interfaces}
The intelligent control of interfaces with shared variables is crucial to trade off user agency with system automation. Previous systems typically used methods that either do not take the user into account at all or do not predict user behavior. Therefore, previous methods have no or limited ability to intelligently trade off user agency and system automation. In contrast, we embed, explicitly and implicitly, a predictive model of user behavior into our control strategy. A predictive model allows the system to take future human decisions into account in the control loop. Using future predictions enables a better balance between user agency and system automation.

\subsection*{3. \magpenTitle}
In \chapref{ch:control:optimal}, we explore Model Predictive Contour Control to enable haptic guidance for electromagnetic systems. Our real-time approach assists users in tasks like calligraphy (\figref{fig:caligraphy}), sketching (\figref{fig:dragon}), or designing (\figref{fig:tablet}) by iteratively predicting the motion of an input device (such as a pen) and adjusting the position and strength of an underlying dynamic electromagnetic actuator. In addition to the control strategy, we introduced a prototypical haptic feedback device that allowed us to evaluate our approach (\figref{fig:hardware}). Our user study showed that users were more accurate with our method ($6.2mm \pm 0.8$) compared to open-loop ($38.0mm \pm 56.9$) or time-dependent closed-loop methods ($45.0mm \pm 50.8$), as illustrated in \figref{fig:single_user_control} and \tabref{tab:strategy_results}. \magpen demonstrates that an explicit predictive user model and system dynamics with a receding horizon cost function enable the control of a haptic system that trades off user autonomy with system automation. However, the reliance on known system dynamics posed a challenge, especially in embedding user behavior.

\subsection*{4. \marluiTitle}
In \chapref{ch:control:multi}, we introduce a model-free reinforcement learning approach to overcome the need for explicit system dynamics. We also proposed adaptive user interfaces that automatically adjust based on user context and previous inputs. In our MARL formulation, a user agent simulates real user interactions, while an interface agent learns to optimize the UI based on the user agent's performance. This approach significantly reduced the number of actions users needed ($3.34$) compared to supervised-learning ($3.87$) and static baselines ($5.73$), as illustrated in \figref{fig:userstudy} and \tabref{tab:desc}. Unique to our approach is that it requires minimal tuning between tasks and scenarios. To show this, we introduced four additional use cases: Interface Selection (\figref{fig:price}), Block Building (\figref{fig:building}), Out-of-reach Item Grabbing (\figref{fig:objectgrab}), and Photo Editing (\figref{fig:hierarchical_good}). \marlui demonstrates that it is possible to learn implicit models of human behavior and tasks by treating interaction as a multi-agent game. These implicit models can then be integrated into intelligent control strategies that take human decision-making into account.

\subsection*{Implications}
We demonstrate how to integrate models of human behavior explicitly and implicitly into optimal control strategies for intelligent systems. This approach allows systems to consider future states and actions and optimize inputs accordingly. In \magpen, we embed a user model explicitly in the control strategy, considering user behavior over a horizon, though mathematically formulating this behavior can be challenging. Conversely, in \marlui, we take a multi-agent RL approach where user behavior is implicitly learned, eliminating the need for explicit behavioral models or system dynamics descriptions, and allowing the method to generalize across interfaces and tasks. Both approaches successfully balance user agency and automation, assisting users in their tasks.

\section*{Conclusion}
We started with the question: \emph{``How can we algorithmically control intelligent systems with shared variables to balance user agency and system automation?''} To answer this question, we provide four distinct contributions in two parts: the design and control of shared variable interfaces. We demonstrate that integrating models of human behavior into control strategies, either explicitly or implicitly, enables intelligent systems to take user agency better into account. Furthermore, we show that user agency versus system automation is not just an algorithmic problem, and that this trade-off must also be taken into account during the engineering of physical devices and the design of interfaces. Hence, we advocate for an integrated end-to-end approach for the interaction with intelligent systems that takes an algorithmic, engineering, and design perspective.