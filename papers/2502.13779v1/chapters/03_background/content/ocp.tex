\section{Optimal Control}
\label{sec:ocp}

This chapter offers an overview to introduce the reader to the problem formulations and methods of optimal control for discrete-time dynamic systems, which are the main formulations and methods used in the second half of this thesis chapters. This discussion is partially is based on \citeauthor{gebhardt2020optimal}'s earlier work and we refer to their publication for an overview \cite{gebhardt2020optimal}. 
% 
We start by discussing the optimal control problem (OCP) for scenarios where the dynamic system's model is known. We proceed to examine model predictive control, an optimization technique employed to address these scenarios. Next, we introduce Markov decision processes (MDPs), which serve to frame OCPs when the system model is not known. We extend MDPs to their partial-observable counterparts (POMDPs), these enable scenarios where the system does not have access to the full state space. From there, we explore reinforcement learning (RL), methodologies applied to solve (PO)MDPs. Finally, we review multi-agent reinforcement learning (MARL) for scenarios with multiple agents in a shared environment. 

\subsection{Optimal Control}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{chapters/03_background/figures/ocp.pdf}
    \caption{An overview of the control problem. An agent/system takes action $\action$ or input $\mathbf{u}$ based on the state $\mathbf{x}$ or $\mathbf{s}$. The action/input updates the state. Furthermore the agent receives a reward $r$.}
    \label{fig:ocp}
\end{figure}


 Control theory is a field that studies how to influence the behavior of dynamic systems using external inputs, with the goal of achieving a target state. Optimal control, a subfield of control theory, involves finding the best possible control inputs and state trajectories for a dynamic system. This is done by optimizing a measure of performance or cost. Optimal control can be applied to systems with either continuous or discrete dynamics. In this context, we concentrate on discrete dynamic systems.

First, we introduce the OCP formulation for the case of a discrete system model which inputs are optimized according to a cost function (\figref{fig:ocp}). Therefor, we consider the following discrete-time linear time-invariant system
\begin{equation}
\label{eq:linear_system}
    \mathbf{\statex}(\timet+1) = \mathbf{A\statex}(\timet) + \mathbf{B\inputu}(\timet),
\end{equation}
\noindent where $\statex \in \mathbb{R}^{n}$ is the state vector, $\inputu \in \mathbb{R}^{m}$ the input vector, $\mathbf{A} \in \mathbb{R}^{n \times n}$ the system matrix, $\mathbf{B} \in \mathbb{R}^{n \times m}$ the input matrix, $\timet \in \mathbb{N}_0$ the discrete time, and the pair $(\mathbf{A},\mathbf{B})$ is assumed stabilized.

We define $\statex(\timet)$ as the state vector measured at discrete time $\timet$ and $\statex_{\timet+k}$ be the state vector predicted at discrete time $\timet+k$ using state equation \ref{eq:linear_system} with initial condition $\statex_\timet = \statex(\timet)$.
We constraint the problem with:
\begin{equation}
\label{eq:constraints}
    \statex(\timet) \in \mathbb{X} \subseteq \mathbb{R}^{n}, \inputu(\timet) \in \mathbb{U} \subseteq \mathbb{R}^{M},
\end{equation}
\noindent where $\mathbb{X}$ and $\mathbb{U}$ are the sets of all possible states and inputs respecively.

Then, the a general cost function can be defined as:
\begin{equation}
\label{eq:cost_function}
    \valuev_N(\statex(\timet)) = r_T(\statex_{\timet+N}, \inputu_{\timet+N}) + \sum^{N-1}_{k=0} \gamma^k r(\statex_{\timet+k}, \inputu_{\timet+k}),
\end{equation}
\noindent with the stage cost:
\begin{equation}
\label{eq:stage_cost}
    r(\statex_{ime\timet+k}, \inputu_{\timet+k}) = \statex_{\timet+k}^T\mathbf{Q}\statex_{\timet+k} + \inputu_{\timet+k}^T\mathbf{R}\inputu_{\timet+k},
\end{equation}
\noindent and terminal cost:
\begin{equation}
\label{eq:terminal_cost}
    r_T(\statex_{\timet+N}, \inputu_{\timet+N}) = \statex_{\timet+N}^T\mathbf{P}\statex_{\timet+N} ,
\end{equation}
\noindent where $N$ is the prediction horizon, $\gamma \in (0,1]$ a discount factor to give less weight to errors and inputs further in the future, $\mathbf{Q} \in \mathbb{R}^{n \times n}$ and $\mathbf{R} \in \mathbb{R}^{m \times m}$, and $\mathbf{P} \in \mathbb{R}^{n \times n}$ are the state, input and terminal weighting matrices respectively. The weighting matrices are required to be symmetric and positive definite to ensure the objective function is convex. The terminal cost matrix $\mathbf{P}$ is obtained by solving an algebraic Riccati equation, which arises from the infinite-horizon linear quadratic regulator problem \citep{Camacho2013}.

Optimal control problems can be categorized as either finite-horizon (FHOCPs) or infinite-horizon (IHOCPs) based on the time period considered for optimization.
In FHOCPs, the control inputs are optimized over a fixed, finite number of future states or time steps. The goal is to find the best control sequence that minimizes the objective function from the initial state to a specified terminal state. We will use a FHOCP for haptic control.  
In IHOCPs, the control inputs are optimized assuming the system will continue to operate indefinitely. The objective is to find a control policy that minimizes the cost over the entire unbounded future, often expressed as an infinite sum of stage costs. The optimal control law is time-invariant - it provides the optimal input as a function of the current state, regardless of the time index. We will use an infinite horizon in our work on \marluiTitle. 

The FHOCP is defined as:
\begin{align}
\label{eq:finite_horizon}
    \valuev_N^*(\statex(\timet)) = \underset{\inputu(\timet)}{\min}~&r_T(\statex_{\timet+N}, \inputu_{\timet+N}) + \sum^{N-1}_{k=0} \gamma^k r(\statex_{\timet+k}, \inputu_{\timet+k}) \\ \nonumber
    \text{subject to} & \\ \nonumber
    &\statex_{\timet+k+1} = \mathbf{Ax}_{\timet+k} + \mathbf{Bu}_{\timet+k},~k = 0,..., N - 1 \\ \nonumber
    &\statex_{\timet+k} \in \mathbb{X},~k = 1,..., N  \\ \nonumber
    &\inputu_{\timet+k} \in \mathbb{U},~k = 0,..., N - 1  \\ \nonumber
    &\statex_{\timet} = \statex(\timet) \nonumber
\end{align}
with input sequence $\inputu(\timet) = (\inputu^{T}_\timet, ..., \inputu^{T}_{t+N-1})^T \in \mathbb{R}^{Nm}$.

Similarly, the IHOCP is defined as
\begin{align}
\label{eq:infinite_horizon}
    V_\infty^*(\statex(\timet)) = \underset{\inputu(\timet)}{\min}~&\sum^{\infty}_{k=0} \gamma^k r(\statex_{\timet+k}, \inputu_{\timet+k}) \\
    subject~to~ &\statex_{\timet+k+1} = \mathbf{Ax}_{\timet+k} + \mathbf{Bu}_{\timet+k},~k = 0, 1,... \\ \nonumber
    &\statex_{\timet+k} \in \mathbb{X},~k = 1, 2,...  \\ \nonumber
    &\inputu_{\timet+k} \in \mathbb{U},~k = 0, 1,... \\ \nonumber
    &\statex_\timet = \statex(\timet) \nonumber
\end{align}
with input sequence $\inputu(\timet) = (\inputu^{T}_\timet, \inputu^{T}_{t+1}, ...)^T$.

We introduced the OCP for a linear system so far. However, not all systems are linear or can be linearized; such as human behavior. In the nonlinear case, the right part of \eqref{eq:linear_system} is substituted by a nonlinear function that describes the evolution of the dynamic system:
\begin{equation}
\label{eq:nonlinear_system}
    \statex(t+1) = F(\statex(\timet), \inputu(\timet)).
\end{equation}
The terminal penalty for non-linear stytem is given by the Lypunov equation \citep{Johansen2004}, rather than the Ricatti equation.

\subsection{Model Predictive Control}
\label{sec:back:mpc}
Model predictive control (MPC) is a practical implementation of (in)finite horizon optimal control for the measured state vector $\statex(\timet)$ to attain the predicted optimal input sequence $\inputu^(\timet)$. Specifically, it is a closed-loop control method that measures the state ($\statex$) at every time step ($\timet$), solves the optimal control equations \eqref{eq:finite_horizon}, and then applies its first element ($\inputu^(\timet)$) to the system. This is repeated at each discrete time $\timet$ with a receding prediction horizon. Thus, MPC is also denoted as receding horizon control (RHC). 

Both the FHOCP and the IHOCP can be reformulated as a quadratic programming (QP) problem for linear systems with quadratic cost functions and linear constraints (\citep{Gorges2017}):

\begin{align}
	\label{eq:quadratic_program}
	\underset{\mathbf{X}}{\text{minimize}} \ & \frac{1}{2} \mathbf{X}^{T} \mathbf{H} \mathbf{X} + \mathbf{f}^{T} X \\
	\text{ subject to } &\mathbf{A}_{\mathit{ineq}} \statex \leq \mathbf{b}_{\mathit{ineq}} \nonumber \\
	\text{ and } & \mathbf{A}_{\mathit{eq}} \statex = \mathbf{b}_{\mathit{eq}} \nonumber~,
\end{align}

Here, $\mathbf{X}$ represents the stacked state vectors $\statex_{t}$ and inputs $\inputu_{t}$ at each time point. The matrices $\mathbf{H}$ and $\mathbf{f}$ contain the quadratic and linear cost coefficients, respectively, as defined in \eqref{eq:cost_function}. The matrices $\mathbf{A}{\mathit{ineq}}$ and $\mathbf{b}{\mathit{ineq}}$ describe the linear inequality constraints on states and inputs from \eqref{eq:constraints}, while $\mathbf{A}{\mathit{eq}}$ and $\mathbf{b}{\mathit{eq}}$ represent the linear equality constraints from our model in \eqref{eq:linear_system} for each time point $k \in {0,\ldots,N}$.

To solve nonlinear dynamic systems and cost functions, we can use nonlinear programming (NLP) or sequential quadratic programming (SQP) methods. NLP addresses the nonlinear optimization problem directly, whereas SQP iteratively solves local quadratic programming (QP) approximations of the nonlinear problem. The selection of horizon length, terminal cost, and constraints in these formulations is crucial for ensuring closed-loop stability and performance. We use numerical solvers to address MPC problems \cite{forcespro}.

\subsection{Markov Decision Processes}
\label{sec:mdp}
Model Predictive Control (MPC) addresses the OCP of a system assumed to behave deterministically according to a known dynamic model. However, many OCPs inherently have stochastic state transitions, making it impossible to model the underlying system dynamics accurately. Markov Decision Processes (MDPs) offer a mathematical framework for these situations, where a system model and suitable control policies are learned from the system's interactions with its environment. Generally, MDPs can be used to represent a wide range of sequential decision-making problems in stochastic settings \citep{kaelbling1998planning}.

An MDP satisfies the Markov property, which states that state transitions depend only on the current state and action, not on any prior states or actions. An MDP is defined by a five-tuple ($\StatePerPolicy$, $\ActionPerPolicy$, $\Transitions$, $\RewardPerPolicy$, $\discount$), where $\StatePerPolicy$ is the set of states, $\ActionPerPolicy$ is the set of actions, $\Transitions: \StatePerPolicy \times \ActionPerPolicy \times \StatePerPolicy \rightarrow [0,1]$ is the transition probability function, and $\Transitions (\state', \action, \state)$ represents the probability of transitioning from state $\state'$ to state $\state$ after taking action $\action$. $\RewardPerPolicy : \StatePerPolicy \times \ActionPerPolicy \rightarrow \mathbb{R}$ is the reward function, with rewards discounted by a factor $\discount$. The expected discounted reward for taking action $\action$ in state $\state$ under policy $\policy$ is called the Q value: 

\begin{equation}
\label{eq:q_value}
    Q^{\policy}(\state,\action) = \mathbb{E}_\policy\left[\sum_{\timet=0}^{\infty}\gamma^t \RewardPerPolicy(\state_\timet, \action_\timet) \mid | \state_\timet=\state, \action_\timet=\action \right ],
\end{equation}

The value function $V^\policy(s)$ is the expected return starting from state $s$ and following policy $\policy$: 
\begin{equation}
V^\policy(s) = \mathbb{E}_\policy\left[\sum_{t=0}^\infty \discount^t \RewardPerPolicy(\state_\timet,\action_\timet)\mid | \state_\timet = \state \right  ]
\end{equation} 

The Q values and value function are related via: 
\begin{equation}
V^\policy(\state) = \sum_\action \policy(\action|\state) Q^\policy(\state,\action)
\end{equation}

The Q values of following states are related via the Bellman equation:
\begin{equation}
\label{eq:bellman}
    Q^\policy(\state, \action) = \sum_{\state'} P(\state'|\state,\action)[\RewardPerPolicy(\state',\state,\action) + \discount Q^\policy(\state',\policy(\state'))].
\end{equation}
The optimal policy can then be computed as $\policy^* = \arg\max_a Q^\policy(s,a)$.
 
\subsection{Partially Observable Markov Decision Processes}
\label{sec:back:pomdp}

Partially Observable Markov Decision Processes (POMDP)  extent the MDP, and is a mathematical framework for single-agent decision-making in stochastic partially observable environments \cite{aastrom1965optimal}, which is a generalization over Markov Decision Processes \cite{howard1960dynamic}. A POMDP is a seven-tuple ($\StatePerPolicy, \ObservationPerPolicy, \ActionPerPolicy, \Transitions, \ObservationTransitions, \RewardPerPolicy, \gamma$). In POMDPs, the exact states ($s \in \StatePerPolicy$) of the evolving environment may or may not be captured fully. Therefore, observations ($\observation \in \ObservationPerPolicy$) represent the observable states, which may differ from the exact state. Similar to $\Transitions$, $\ObservationTransitions: \StatePerPolicy \times \ActionPerPolicy \times \ObservationPerPolicy \rightarrow [0,1]$ is an observation probability function, where $\ObservationTransitions (\observation, \action, \state')$ is the probability of observing $\observation$ while transitioning to $\state'$ after taking action $\action$. 

The Q-value function for a POMDP is defined over belief states and actions, rather than exact states and actions. Let $b$ denote a belief state, which is a probability distribution over states, so that $\belief'(\state')$ is the updated belief of being in state $\state'$. To compute the belief state in a Partially Observable Markov Decision Process (POMDP), we use a recursive state estimation process based on the previous belief state, the action taken, and the current observation received. Then, the belief state update equation is: 
\begin{equation}
    \belief'(\state')=\eta \ObservationTransitions(\observation | \state',\action)\sum_{\state \in \StatePerPolicy}\Transitions(\state'| \state, \action) \belief(\state)
\end{equation}
\noindent where $\eta = 1/P(\observation|\belief,\action)$ is a normalizing constant with:
\begin{equation}
    P(o|b,a) = \sum_{\state' \in \StatePerPolicy} \ObservationTransitions(\observation, \state',\action) \sum_{\state \in \StatePerPolicy} \Transitions(\state'|\state,\action) \belief(\state).
\end{equation}

This process is repeated after each action and observation to maintain a current belief state that summarizes all information received so far. The belief state forms a sufficient statistic for the entire action-observation history and allows the POMDP to be cast as a continuous-state MDP, called a belief MDP

The value function for a POMDP policy $\policy$ over belief states is defined as: \begin{equation}
V^\policy(b) = \mathbb{E}_\policy\left[\sum_{t=0}^\infty \gamma^t R(\state_\timet,\action_\timet) \mid \belief_0 = \belief\right]
\end{equation} 

Then the Q-value function for a POMDP policy $\policy$ (casted as belief MDP) is defined as:
\begin{equation}
Q^{\policy}(\belief,\action) = \sum_{\state \in \StatePerPolicy} \belief(\state) \left[\RewardPerPolicy(\state,\action) + \gamma \sum_{\observation \in \ObservationPerPolicy} \ObservationTransitions(\observation, \state,\action) V^{\policy}(\belief^\action_\observation) \right],
\end{equation}
\noindent where $\belief^\action_\observation$ is the updated belief state after taking action $\action$ and observing $\observation$ and $V^{\policy}(\belief^\action_\observation)$ is the value of the updated belief state $\belief^\action_\observation$ under policy $\policy$. The optimal policy can then be computed as $\policy^* = \arg \max_{\action} Q^{\policy}(\belief,\action)$

\subsection{Stochastic Games}
MDPs or POMDPs assume a single policy. Stochastic games generalize MDPs for multiple policies \cite{shapley1953stochastic}. When players do not have perfect information about the environment, stochastic games become partially observable stochastic games. A partially observable stochastic game is defined as an eight-tuple $\left(N, \SetOfStates, \SetOfObservations, \SetOfActions, T, \SetOfObservationTransitions, \SetOfRewards, \gamma \right )$, where $N$ is the number of policies. $\SetOfStates = \StatePerPolicy_1 \times ... \times \StatePerPolicy_N$ is a finite set of state sets, and $\SetOfObservations = \ObservationPerPolicy_1 \times ... \times \ObservationPerPolicy_N$ is a finite set of observation sets, with subscripts indicating different policies. $\SetOfActions = \ActionPerPolicy_1 \times ... \times \ActionPerPolicy_N$ is a finite set of action sets. $\SetOfObservationTransitions = \ObservationTransitions_1 \times ... \times \ObservationTransitions_N$ defines a set of observation probability functions of different players. A set of reward functions is defined as $\SetOfRewards = \RewardPerPolicy_1, ... \RewardPerPolicy_N $. Furthermore, we define a set of policies as $\SetOfPolicies = {\policy_1, ... \policy_N}$.

All policies have their individual actions, states, observations, and rewards. In this paper, we optimize each policy individually, while the observations are influenced by each other's actions, hence we can treat this as multiple distinct POMDPs as outlined in \secref{sec:back:pomdp}.

Interactive POMDPs (I-POMDPs) are an alternative framework for multi-agent decision making \cite{gmytrasiewicz2004interactive}. A core different with partially observable stochastic games is the I-POMDPs explicitly model belief states, and take actions base on them. Whereas stochastic games involve less cognitive modeling and have an implicit representation. 

\subsection{Reinforcement Learning}
\label{sec:back:rl}
Reinforcement learning (RL) solves MDPs by learning a state-action value function $Q(s,a)$ (or $Q(b,a)$) that approximates the Q value defined by the Bellman equation $Q^\policy(s,a)$. RL algorithms fall into two categories: model-based and model-free. In model-based algorithms, state transition probabilities are known, and policies are determined by enumerating possible state sequences following an initial state and action, summing the expected rewards along these sequences. This dissertation focuses on model-free RL algorithms for solving MDPs, so for an overview of model-based approaches, see \cite{moerland2023model, polydoros2017survey}. In model-free RL, the transition probability functions are unknown but can be sampled. These algorithms learn the approximate state-action value function $Q(s,a)$. In Deep RL, the policy $\policy_\theta$ is learned as a multi-layered perceptron (MLP), where $\theta$ represents the learnable parameters, or the weights of the MLP.

Trust Region Policy Optimization (TRPO) \cite{schulman2015trust} is a policy optimization algorithm designed to achieve stable and efficient training in reinforcement learning by maintaining a trust region around the current policy. The algorithm optimizes the policy by solving the following constrained optimization problem:
\begin{equation}
\label{eq:trpo}
\arg\max_{\theta} \mathbb{E}_{\state_\timet,\action_\timet \sim \policy_{\theta_{\text{old}}}} \left[ \frac{\policy_{\theta}(\action_\timet|\state_\timet)}{\policy_{\theta_{\text{old}}}(\action_\timet|\state_\timet)} A^{\policy_{\theta_{\text{old}}}}(\state_\timet,\action_\timet) \right],
\end{equation}

subject to a constraint on the Kullback-Leibler (KL) divergence between the new and old policies:
\begin{equation}
\label{eq:trpo_kl}
\mathbb{E}_{\state_\timet \sim \rho{\theta_{\text{old}}}} \left[ D_{\text{KL}} \left( \policy_{\theta_{\text{old}}}(\cdot|\state_\timet) | \policy_{\theta}(\cdot|\state_\timet) \right) \right] \leq \delta,
\end{equation}
where $\policy_{\theta}$ represents the policy parameterized by $\theta$, and $A^{\policy_{\theta_{\text{old}}}}(\state_\timet,\action_\timet)$ is the advantage function estimated under the old policy $\policy_{\theta_{\text{old}}}$:

\begin{equation}
A^\policy(\state, \action) = Q^\policy(\state, \action) - V^\policy(\state),
\end{equation}

The constraint ensures that each policy update is conservative, limiting the divergence between successive policies, thereby enhancing the stability and performance of the learning process. This approach mitigates the risk of drastic policy changes, which can lead to performance degradation or instability in the learning process.

In this dissertation we use Proximal Policy Optimization (PPO) \cite{schulman2017proximal}, which is a simplified variant of TRPO designed to retain its benefits while reducing computational complexity. PPO maximizes a clipped objective function:
\begin{equation}
\label{eq:ppo}
\arg\max_\theta \mathbb{E}_\timet \left[ \min \left( r_t(\theta) \action_\timet, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \action_\timet \right) \right],
\end{equation}
where $r_t(\theta) = \frac{\policy{\theta}(\action_\timet|\state_\timet)}{\policy_{\theta_{\text{old}}}(\action_\timet|\state_\timet)}$ is the probability ratio between the new and old policies, and $\action_\timet$ is the estimated advantage function. The clipping function constrains $r_t(\theta)$ within a range of $[1 - \epsilon, 1 + \epsilon]$, with $\epsilon$ being a small hyperparameter. This restriction prevents excessively large updates that could destabilize training, striking a balance between exploration and exploitation. PPO's design simplifies the implementation of policy updates by eliminating the need for complex constraints on KL divergence, thus making it a practical and widely adopted algorithm in reinforcement learning applications.

% \subsection{Model Predictive Control vs Reinforcement Learning}
% RL and MPC are both solve optimal control problems, but they differ in some key ways. RL typically solves an OCP with a potentially infinite horizon, depending on the discount factor $\discount$. In contrast, MPC usually solves a finite-horizon OCP at each time step, using the current state as the initial condition. From a control perspective, RL evaluates a state feedback control law $\inputu(\timet)=\mathbf{h}(\statex(\timet))$. It then applies the resulting control input $\inputu(\timet)$ to the system and measures the next state $\statex(t+1)$. The resulting costs or rewards $r(\statex(\timet),\inputu(\timet),\statex(t+1))$ capture the difference between desired and actual system behavior and are used to update the controller $\mathbf{h}$. The control procedures of RL and MPC are similar in that they both use the current state to compute an optimal control input. However, a key difference is that model-free RL methods do not require a model of the system dynamics. They learn the optimal policy directly through interaction with the environment. In contrast, MPC relies on an explicit model of the system dynamics to predict the future state trajectory and optimize the control inputs over a finite horizon. At each time step, MPC solves a constrained optimization problem using the current state as the initial condition. In summary, while RL and MPC share the goal of optimizing a control policy, model-free RL learns the policy through trial-and-error interaction with the system, while MPC uses a given model to plan an optimal trajectory at each time step. Recent work has explored combining the strengths of both approaches, e.g. using MPC to generate data for RL \cite{bohn2021reinforcement} or using RL to tune MPC parameters \cite{pan2022model}. 

