
% Define a new boolean flag named "shortversion"
\newif\ifshortversion

% Set the flag: use \shortversiontrue for the article (ICLR) class,
% or \shortversionfalse for the acm (EC) class.
\shortversionfalse

% Use the flag to choose the document class.
\ifshortversion
    \documentclass{article}
    % For LaTeX2e
    \usepackage{iclr_files/iclr2025_conference,times,relsize,microtype}

    \title{STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models}

    % Authors must not appear in the submitted version. They should be hidden
    % as long as the \iclrfinalcopy macro remains commented out below.
    % Non-anonymous submissions will be rejected without review.
    
    \def\CurrentAudience{iclr}
    \author{Narun K.~Raman, Taylor Lundy, Kevin Leyton-Brown  \\
    Department of Computer Science\\
    University of British Columbia\\
    Vancouver, BC, Canada \\
    \texttt{\{narunram,tlundy,klb\}@cs.ubc.ca} \\
    \And
    Jesse Perla \\
    Department of Economics \\
    University of British Columbia \\
    Vancouver, BC, Canada \\
    \texttt{jesse.perla@ubc.ca} \\
    }
    \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\else
    % !TEX program = pdflatex
    % !TEX root = main.tex
    \documentclass[format=acmsmall]{acmart}
    \usepackage{acm-ec-25}
    \usepackage{booktabs} % For formal tables
    \usepackage[ruled]{algorithm2e} % For algorithms
    \renewcommand{\algorithmcfname}{ALGORITHM}
    \SetAlFnt{\small}
    \SetAlCapFnt{\small}
    \SetAlCapNameFnt{\small}
    \SetAlCapHSkip{0pt}
    \IncMargin{-\parindent}

    % Choose a citation style by commenting/uncommenting the appropriate line:
    %\setcitestyle{acmnumeric}
    \setcitestyle{authoryear}
    
    % Title. Note the optional short title for running heads. In the interest of anonymization, please do not include any acknowledgements.
    \title[STEER-ME]{STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models}
    
    \def\CurrentAudience{arxiv}
    
    % Anonymized submission.
    % \author{Submission 1228}
    \author{Narun K.~Raman}
    \affiliation{%
      \institution{University of British Columbia}
      \city{Vancouver}
      \country{Canada}
    }
    \email{narunram@cs.ubc.ca}
    
    \author{Taylor Lundy}
    \affiliation{%
      \institution{University of British Columbia}
      \city{Vancouver}
      \country{Canada}
    }
    \email{tlundy@cs.ubc.ca}
    
    \author{Thiago Amin}
    \affiliation{%
      \institution{University of British Columbia}
      \city{Vancouver}
      \country{Canada}
    }
    \email{thiagoamin22@gmail.com}
    
    \author{Jesse Perla}
    \affiliation{%
      \institution{University of British Columbia}
      \city{Vancouver}
      \country{Canada}
    }
    \email{jesse.perla@ubc.ca}
    
    \author{Kevin Leyton-Brown}
    \affiliation{%
      \institution{University of British Columbia}
      \city{Vancouver}
      \country{Canada}
    }
    \email{kevinlb@cs.ubc.ca}





    \begin{abstract}
        How should one judge whether a given large language model (\model) can reliably perform economic reasoning? Most existing \model benchmarks focus on specific applications and fail to present the model with a rich variety of economic tasks. A notable exception is \citet{ramansteer}, who offer an approach for comprehensively benchmarking strategic decision-making; however, this approach fails to address the non-strategic settings prevalent in microeconomics, such as supply-and-demand analysis. We address this gap by taxonomizing microeconomic reasoning into \num{58} distinct elements, focusing on the logic of supply and demand, each grounded in up to \num{10} distinct \domains, \num{5} \Tags, and \num{3} \qtypes. The generation of benchmark data across this combinatorial space is powered by a novel \model-assisted data generation protocol that we dub \autosteer, which generates a set of questions by adapting handwritten templates to target new domains and perspectives. Because it offers an automated way of generating fresh questions, \autosteer mitigates the risk that \models will be trained to over-fit evaluation benchmarks; we thus hope that it will serve as a useful tool both for evaluating and fine-tuning models for years to come. We demonstrate the usefulness of our benchmark via a case study on \num{27} \models, ranging from small open-source models to the current state of the art. We examined each model's ability to solve microeconomic problems across our whole taxonomy and present the results across a range of prompting strategies and scoring metrics.
    \end{abstract}
    
\fi

\input{preamble}


\usepackage{hyperref}
\usepackage{url}
\usepackage{subcaption}
\usepackage{placeins}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \PassOptionsToPackage{dvipsnames}{xcolor}
% \usepackage[dvipsnames]{xcolor}
% \usepackage{xspace}


\usepackage{environ}
\usepackage{etoolbox}

\usepackage{fontawesome}


\begin{document}

\input{notation}



    % Title page for title and abstract only.


\maketitle
    





% \begin{shownto}{iclr}

%     \maketitle
%     \begin{abstract}
%     Large language models (\models) are increasingly being applied to economic tasks like stock picking and financial analysis. Existing \model benchmarks tend to focus on specific applications and often fail to describe a rich variety of economic tasks. \cite{ramansteer} offer a blueprint for comprehensively benchmarking strategic decision-making. However, their work failed to address the non-strategic settings prevalent in micro-economics. We address this gap by taxonomizing micro-economic reasoning into $58$ distinct elements, each grounded in up to $10$ distinct \domains, $5$ \Tags, and $3$ \qtypes. The generation of benchmark data across this combinatorial space is powered by a novel LLM-assisted data generation protocol that we dub \autosteer, which generates a set of questions by adapting handwritten templates to target new \domains and \Tags. By generating fresh questions for each element, \autosteer  helps reduce the risk of data contamination, ensuring that \model evaluations remain valuable over time. We leveraged our benchmark to evaluate $27$ \models over each of the instantiated elements, examined their ability to reason through and solve microeconomic problems and compared \model performance across a suite of adaptations and metrics. Our work provides insights into the current capabilities and limitations of \models in non-strategic economic decision-making and a tool for fine-tuning these models to improve performance. 
%     \end{abstract}
% \end{shownto}



\section{Introduction}

There is much recent interest in using language models (\models) to reason about economic topics. Some prominent examples include financial sentiment analysis, where \models are tasked with analyzing the sentiment information of financial texts \citep{malo2013gooddebtbaddebt, maia201818, finbert, yang2020finbert}; question answering, where \models are tasked with answering an economic question based on the provided information \citep{maia201818, finqa_benchmark, chen2022convfinqaexploringchainnumerical, flue, xie2023pixiu, ramansteer}; financial text summarization, which entails condensing long unstructured financial texts into short summaries that capture crucial information and maintain factual consistency with the original long texts \citep{mukherjee2022ectsum, zhou2021trade}; and Named Entity Recognition, which asks the model to detect critical financial entities such as persons, organizations, and locations \citep{salinas-alvarado-etal-2015-domain, flue}. More open-ended applications are also starting to emerge. \models such as WallStreetBERT, TradingGPT, FinGPT, FinTral, and BloombergGPT are already giving advice to investors and financial advisors \citep{xie2023wall, li2023tradinggpt, yang2023fingptopensourcefinanciallarge, bhatia2024fintralfamilygpt4level, bloomberggpt}. \models can help to automate budgetary planning and allocation \citep{chen2023emergenceeconomicrationalitygpt}. \models are also being deployed as agents in simulations to analyze the impact of policy changes on key indicators like inflation and GDP growth \citep{carriero2024macroeconomicforecastinglargelanguage, li2024econagentlargelanguagemodelempowered}.

Before \models should be trusted in such open-ended applications, they should demonstrate robustly strong performance on the fundamentals of economic reasoning (just as, e.g., financial advisors, budget planners, and economists are required to do). Many existing benchmarks have been proposed, many of which were introduced in papers cited above. However, most of these are quite narrowly focused on a single task and/or application, rather than assessing economic reasoning more broadly. A second---useful but insufficient---category of benchmarks tests foundational concepts in mathematics, ranging from basic arithmetic to complex problem-solving tasks \citep{huang2016well, ling2017program, amini2019mathqa, lample2019deep, zhao2020ape210k}. Notable benchmarks include GSM8K \citep{cobbe2021training}, a small but varied dataset that contains moderately difficult math problems and MATH \citep{hendrycks2021measuring}, a challenging benchmark for which no evaluated model has yet attained expert-level performance across any of the $57$ tested scenarios.

What might it look like to assess an \model's economic reasoning more comprehensively? Economics encompasses a wide array of problems, such as determining optimal consumption bundles, forecasting profit in the face of uncertainty, or analyzing how a shift in supply impacts equilibrium prices and quantities. Each of these problems can occur in a wide range of contexts such as labor markets, consumer product markets, financial markets, or public policy. Beyond the breadth of inputs that must be considered, evaluating \models presents further challenges to benchmark designers. There is no guarantee that an \model will perform equally well on problems that appear similar or are conceptually related \citep[e.g.,][]{hendrycks2021measuringmassivemultitasklanguage}. For instance, an \model that excels at maximizing profit may struggle with minimizing cost. Similarly, \models can be susceptible to perturbations in the text of a question, which can impact their performance on otherwise similar problems \citep{ribeiro-etal-2020-beyond}. For example, \models may excel in allocating budgets as a doctor, but struggle to allocate budgets as an educator. Finally, \models may reason correctly about their own incentives, but fail to apply this logic to other participants and hence have difficulty understanding market or aggregate level responses (e.g., total supply, demand, and prices). Therefore, in order to be comprehensive, a microeconomic benchmark must exhibit broad variation across problems, contexts, and textual perturbations.
%
It is similarly nontrivial actually to conduct experiments that comprehensively assesses how well different \models perform at economic reasoning tasks. Different models may leverage distinct architectures, driving performance differences \citep{sanh2020distilbertdistilledversionbert, islam2023comprehensivesurveyapplicationstransformers, ramansteer}. Additionally, adaptation strategies---such as fine-tuning, prompt engineering, and output distribution modification---can dramatically influence a model's effectiveness \citep{gpt3, lester2021powerscaleparameterefficientprompt, kojima2023largelanguagemodelszeroshot}. Under the right adaptations, models with as few as 7B parameters can achieve state-of-the-art performance \cite[e.g.,][]{bhatia2024fintralfamilygpt4level}. Furthermore, robustness across multiple task formats (e.g., multiple-choice QA, free-text QA, etc.) is crucial for understanding the gaps in an \model's reasoning capabilities. A model that performs well on one task format may underperform on others, which suggests gaps in its reasoning processes. Finally, scoring performance using only a single metric can give a skewed understanding of an \model's abilities and limitations \citep{schaeffer2023emergentabilitieslargelanguage}, or obscure tradeoffs that are relevant to practitioners \citep{ethayarajh-jurafsky-2020-utility}. Without a comprehensive evaluation, we risk misattributing performance to a \model when it is instead driven by an adaptation strategy or is an artifact of the metric used. 

A recent paper by \citet{ramansteer} developed a benchmark distribution for assessing economic reasoning in strategic settings that aims for comprehensiveness in the senses just described. This work serves as a starting point for our own paper, and so we describe it in detail. First, they developed a taxonomy that divided the space of game theory and foundational decision theory into $64$ distinct ``elements of economic rationality,'' ensuring that the elements in the benchmark covered a wide range of strategic contexts and decision-making problems. Second, they formalized a hierarchy across elements so that an \model's performance could be better understood in the context of its dependent subtasks. They generated a huge set of questions from this taxonomy, dubbed \steer, which vary in their difficulty and domain (e.g., finance, medicine, public policy). Finally, they evaluated a spectrum of \models over two adaptation strategies and scored with a suite of metrics. They defined this evaluation framework as a \reportcard (\src), a flexible scoring rubric that can be tuned by the user for their particular needs. 

A key drawback of \steer is that, in its focus on game-theoretic reasoning, it neglects much of the subject matter of microeconomics: multiagent settings in which agents nevertheless act nonstrategically. Such reasoning is widespread in competitive markets, where each agent's impact on the market is too small to affect prices unilaterally. For example, while a mobile phone manufacturer might make a strategic decision about the number of handsets to produce and the price to sell them at, a small farm's decision to produce wheat instead of corn given market prices is non-strategic. We employ---and expand upon---the \steer blueprint to construct a benchmark for testing \models on economics in non-strategic environments. Following \citet{ramansteer}, we built a taxonomy of non-strategic economics consisting of $58$ elements. We then instantiated each element in the taxonomy across $8$--$10$ domains. From here, we expanded on the blueprint in two ways. First, we increased the diversity of the questions in the dataset and instantiated each element in $5$ different \emph{\Tags} and up to $3$ \emph{\qtypes} (as defined in \Cref{subsec:dataset}). Second, we expanded their evaluation framework to include newer \models ($27$ in total), some new adaptations ($3$ that we developed and $2$ more from the literature), and many new scoring metrics (a family of $4$ calibration metrics). We dub our benchmark \benchmark, reflecting both its conceptual links to the original \steer and its novel focus on microeconomics.

Even given the best possible \model benchmark, data contamination poses an increasingly important challenge \citep{sainz-etal-2023-nlp, deng2023investigating, ravaut2024much}. Data contamination occurs when the test data used to evaluate an \model is similar or identical to data the \model encountered during training, leading to inflated performance metrics that do not accurately reflect the \model's true capabilities. To tackle this issue, we introduce a new dynamic data generation process called \autosteer which we used to generate all of the questions in \benchmark. \autosteer combines many of the features present in existing dynamic and modular frameworks \citep{gioacchini2024agentquest, wang2024benchmark, white2024livebench} that we detail in \Cref{app:autosteer}. 

In what follows, \Cref{sec:taxonomy} gives an overview of our taxonomy; for space reasons we defer definitions and examples of each element---which are extensive---to \Cref{app:taxonomy}. \Cref{sec:steer_benchmark} describes how we used this taxonomy to build the benchmark distribution. For $37$ elements, we have written {\model} prompts to synthetically generate \num{1000}--\num{5000} multiple-choice questions and manually validated \num{500} generations per element. \Cref{sec:exp_setup} describes the setup of an experiment in which we generated full \srcs for $27$ \models, ranging from Llama-\num{2} \num{7}B to o1-preview, evaluated on a total of \num{21000} test questions. We spent \$\num{12439.54} making requests to OpenAI and Anthropic's API and \num{9.81} GPU years of compute to evaluate open-source models. 
%
Finally, we survey our experimental results in \Cref{sec:results}. Here, we offer a few highlights. We observed a significant variation in performance across both \models and elements. Even among large models, most underperform on at least a few tasks, indicating that a model's size alone is not sufficient to predict its degree of success across our benchmark. The one exception is o1-preview, which consistently achieved top performance on every element we tested, standing out as the most robust and accurate model in our evaluations. Across \domains and \Tags, \models generally exhibited stable performance, but key error patterns emerged throughout the benchmark. In some instances, \models would ignore the question asked and instead provide ``near-miss'' solutions---those that were economically relevant but addressed a simpler or different problem. In addition, they also frequently bypassed the intended reasoning process by relying on answer choices to reach the correct response rather than deriving solutions independently. Finally, our richest \parent produced a particularly large number of errors: even models as large as GPT-4o and Claude 3.5 Sonnet consistently miscalculated a straightforward concept like deadweight loss by applying incorrect formulas and misinterpreting marginal cost. 

We release all model outputs to support evaluation research and contributions via our website \url{steer-benchmark.cs.ubc.ca}, allowing users to deeply probe all of our experimental results and the underlying model prediction details. Finally, we will release an extensible codebase to support the community in taking \benchmark further.
% In the de-anonymized version of this paper we will offer a link to all model outputs to support evaluation research and contributions. We will also point to a public website allowing users to deeply probe all of our experimental results and the underlying model prediction details. Finally, we will release an extensible codebase to support the community in taking \benchmark further.









\end{document}