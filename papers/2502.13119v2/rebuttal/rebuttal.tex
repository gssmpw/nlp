\documentclass[12pt]{rebuttal_style}
% \title{A LaTeX class to create rebuttal documents for journal reviews}
% \author{Narun}
% \journal{Journal Name}
% \manuscriptid{0000-0000-0000}

\usepackage{verbatim}
% \usepackage{natbib}

%% You can define Note commands using the \ColorNote command 
%% provided in the class. 
\newcommand{\narun}[1]{\ColorNote{red}{Narun}{#1}}


\begin{document}
%\pagenumbering{arabic}
% \maketitle

% The JournalRebuttal class extends the \LaTeX~\verb|article| class to include environments and commands that assist in preparing rebuttal documents to reviews received for journal submissions. 



\begin{response}
    We would like to thank all reviewers for their thoughtful comments and for highlighting areas where our work could be further improved. We recognize that some aspects of our benchmark's insights may not have been fully clear, and we appreciate the opportunity to elaborate. Here, we outline some of the key insights our initially submitted work offered:
\begin{enumerate}
    \item Reasoning over Options: One particularly interesting finding is that LLMs demonstrate an ability to recognize when a question can be resolved using the provided options. This allows them to "short-circuit" difficult computations by leveraging the structure of multiple-choice questions, a behavior that suggests nuanced reasoning strategies.

    \item Challenges with Real-Valued Exponents: Real-valued exponent computations, which are prevalent in economic reasoning (e.g., interest rates, discounting, elasticities), present a significant challenge for LLMs. This reveals a gap in the models' ability to handle key mathematical operations that are critical for solving common economics problems.

    \item Domain-Specific Conditioning: We also observe that certain domains influence LLM performance. For example, when questions are framed in more realistic, real-life scenarios, models often fail to recognize what is being asked. They tend to equivocate or misinterpret the problem, indicating potential biases introduced by the contextual framing of the task.
\end{enumerate}
\end{response}

\vspace{0.5cm}
\makerule

\begin{revcomment}
    The primary contribution—expanding benchmarks to cover non-strategic microeconomic settings—leans more toward an economic contribution than a machine learning one, which may affect the paper's resonance with ICLR's core audience.
\end{revcomment}
\begin{response}
We appreciate the reviewer’s perspective and would like to clarify that our work, while grounded in economic concepts, is fundamentally aimed at advancing machine learning applications. By evaluating LLMs in non-strategic microeconomic contexts, we tackle unique challenges in structured decision-making under constraints. These tasks provide a rich test bed for assessing LLMs' reasoning capabilities, particularly their ability to integrate mathematical reasoning, domain-specific knowledge, and contextual understanding. This focus aligns with the broader machine learning goal of understanding and improving general reasoning abilities in AI systems.

Economic reasoning, like logical reasoning, is implicit in many decision-making problems for agents across diverse domains. If an LLM is deployed as an agent reasoning about markets or changes in prices, the capabilities evaluated by this benchmark would directly inform its effectiveness. Furthermore, our benchmark provides a framework for examining LLMs’ reasoning in a domain that combines abstraction with real-world complexity, making it broadly relevant to machine learning research.

Our methodology aligns with the trend in the ML community of using domain-specific benchmarks to assess reasoning capabilities. Similar to SWEBench (software engineering tasks, ICLR 2023), BEND (biology-focused reasoning, ICLR 2024), and ISARSTEP (high-level mathematics, ICLR 2021), STEER-ME leverages a complex domain to rigorously evaluate model generalization and reasoning limitations. These works demonstrate that domain-specific benchmarks can yield insights into model capabilities that extend beyond the evaluated domain, and we view STEER-ME as a meaningful contribution within this tradition.
\end{response}

\begin{revcomment}
    The paper asserts that auto-STEER addresses data contamination but lacks empirical evidence or a detailed explanation of how it mitigates this problem effectively.
\end{revcomment}
\begin{response}
Thank you for raising this concern. While a direct investigation of auto-STEER's ability to mitigate data contamination is beyond the scope of this paper, we appreciate the need to provide context for its effectiveness. In Section 3.2, we describe how auto-STEER generates diverse questions by systematically varying templates, domains, and perspectives. To address your comment, we have added a discussion in Appendix B that draws on related work to support our design. For instance, Mirzadeh et al. (2024) show that modifying question templates by swapping names and numbers—an approach similar to auto-STEER—significantly reduces performance on the GSM8K benchmark, highlighting its susceptibility to contamination. This evidence reinforces the potential of auto-STEER to address data contamination effectively, and we hope this additional context clarifies its motivation and design.
\end{response}

\begin{revcomment}
    The benchmark is limited to multiple-choice questions, whereas real-world financial assistant LLMs are often required to handle open-ended generation tasks relevant to economic analysis. I would recommend expanding more experiments with more types of datasets, such as the free-text generation QA datasets.
\end{revcomment}
\begin{response}
    We appreciate the reviewer's suggestion to incorporate open-ended generation tasks as part of our evaluation framework. In the resubmitted paper, we have addressed this by performing a case study on the performance of closed-source models in a free-text generation QA adaptation, applied to a subset of elements.
    
    To examine the effect of multiple-choice formats, we selected 4 elements (one from each non-Foundations setting) with the largest gap in accuracy between the hidden and shown adaptations to evaluate in free-text QA. Our analysis revealed that performance is consistently worse under the free-text adaptation compared to the hidden adaptation. This performance gap appears to stem from the models' tendency to approximate correct answers: in the multiple-choice setting, they often recognize and select the closest option, whereas in free-text generation, this heuristic is less effective.
\end{response}

\begin{revcomment}
    The extensive number of elements and perspectives, while thorough, could complicate the task of providing a clear, overall model recommendation.
\end{revcomment}
\begin{response}
    Our goal is not to recommend a single best model but to offer a variety of perspectives that allow users to select a model that best suits their particular use case. The extensive elements and cross-cutting analyses are intended to provide insights into model performance across different economic reasoning tasks. That being said, we highlight that o1-preview is the best performer in all of the elements we tested, but ultimately, the goal is to offer a nuanced understanding of strengths and weaknesses rather than a one-size-fits-all recommendation.
\end{response}

% \begin{revcomment}
%     A substantial portion of the paper focuses on introducing economic rationality elements, which might not align well with the technical expectations of ICLR’s machine-learning audience. But overall, the idea is pretty interesting.
% \end{revcomment}
% \begin{response}
%     \narun{mix into 1.}
% \end{response}


\begin{revcomment}
    I am willing to raise my score if the paper could include more analysis (e.g., more evaluations on different types of datasets) on the perspectives of LLM-wise or machine learning-wise.
\end{revcomment}
\begin{response}
    We appreciate this suggestion and have made revisions to address this feedback. In the resubmitted paper, we include additional analysis focused on free-text QA. If this additional analysis does not fully align with the reviewer's expectations, we would be happy to discuss any further evaluations or clarify specific aspects of interest.
\end{response}

\nextreviewer

\begin{revcomment}
    The paper fails to discuss the correlation between the results of the proposed benchmark and those of related benchmarks, highlighting the significance of this research.
\end{revcomment}
\begin{response}
    Thank you for your comment. While direct comparisons of results are challenging due to the distinct focus of STEER-ME, we agree that providing a discussion of how it relates to other benchmarks would strengthen the paper. In the revised version, we have better highlighted how STEER-ME fits within the broader ecosystem of benchmarks. This discussion emphasizes STEER-ME's unique contributions to evaluating reasoning in non-strategic microeconomics and clarifies its role and significance in advancing research in this area.
\end{response}

\begin{revcomment}
    The analysis in the results section appears incomplete. Why is there only a separate section on "Domain Robustness" without any mention of "type robustness" or "perspective robustness"?
\end{revcomment}
\begin{response}
    Thank you for pointing out this omission. In the resubmitted paper, we have conducted an analysis on type robustness. We offer a brief summary of the results. We investigated model performance across different functional forms in the questions. We observed that performance on Cobb-Douglas functions, which are widely used in economics and involve real-valued exponents, was worse compared to functions with integer-valued exponents. However, despite challenges with real-valued exponents, models performed equally well on both Cobb-Douglas and linear functions for the output elasticity and returns to scale elements. We hypothesize that this is because there are simple solutions to Cobb-Douglas production functions in these elements: the output elasticity is simply the exponent, and the returns to scale is the sum of the exponents.
\end{response}

\begin{revcomment}
    Although the paper generates a large number of questions for testing purposes, is this amount really necessary? For instance, the paper creates 100 templates by rephrasing, only modifying exact words or objects in the questions. Will these modifications lead to significant differences in results?
\end{revcomment}
\begin{response}
    Our results indicate that varying the text of questions, even when conditioning on fixed attributes such as element, type, domain, and perspective, leads to significant variation in LLM performance. This suggests that rephrased templates are not trivial modifications and provide meaningful insights into the model's ability to generalize across semantically equivalent formulations. To quantify this, we conducted experiments (detailed in Appendix G) and observed that models exhibit variability in performance across these rephrased templates, underscoring the importance of including such variability in evaluations.

    One of the main motivations for generating a large number of templates is to capture this variability, as studies (e.g., Mirzadeh et al., 2024) have shown that rephrasings can significantly affect model performance. Additionally, when sampling answers from models at temperature 0 (standard for analytical tasks), responses are deterministic. To achieve statistical power, it is essential to test the model with numerous rephrased questions. This is akin to asking 100 humans the same question phrased differently to evaluate robustness and consistency.
\end{response}

\nextreviewer

\begin{revcomment}
    In the AUTO-STEER section, the authors may need to clarify which type of LLM was used for data generation. Additionally, it is important to discuss the impacts of the benchmark results by different LLMs for data generation.
\end{revcomment}
\begin{response}
    Thanks for catching this! We used gpt-4o-2024-05-13 and have updated that in the paper.
    We also have added a case study on 4 elements chosen at random (one from each setting) generated by claude-3-5-sonnet in Appendix K.2. We find that the claude-3-5-sonnet generated elements do not induce a performance difference downstream.
\end{response}

\begin{revcomment}
    The authors note that LLMs struggle with basic mathematical problems, like calculating the deadweight loss of a monopoly[Line 445-450], and suggests that this may be attributed to the use of incorrect formulas. To strengthen this claim, the authors should present specific evidence, such as analyses of error cases.
\end{revcomment}
\begin{response}
    This is a great point. We add a more in-depth discussion into the error cases for deadweight loss in the results section and the appendix. At a high level, we computed answers that would have arisen given specific incorrect formulas and identify when the model's answer is closest to the correct answer or the incorrect answer (with some minimum tolerance). We found that in the vast majority of cases ($\sim82$\% of the time) claude-3-5-sonnet used four easily-identifiable incorrect formulas and in the majority of cases ($\sim54$\%) gpt-4o used those same incorrect formulas.
    % At a high level, we add the answer the incorrect formula would have generated to the list of options and re-evaluated the models. When we re-scored (i.e., gave a point if a model reached the correct answer OR the answer the incorrect formula gives) we found that claude-3-5-sonnet had a exact-match performance gain of ~$80\%$ and gpt-4o had a gain of ~$40\%$. We also found that these two models often incorrectly computed a sub-step in their reasoning which accounted for the majority of the rest of the incorrect cases.
    % \narun{an analysis that would be interesting here is running gpt-4o on the CoTs and asking if it is using the formula that we know to be correct or not. and then spot-checking.
    
    % OR compute the answer with the wrong formula and replace a non-answer with that answer.}
\end{response}

\begin{revcomment}
    The authors need to provide more information on which open-source LLMs were tested (they did not list the LLMs), particularly whether they included models from the fields of mathematics and economics. It would also be important to discuss any differences in results between these specialized LLMs and general LLMs.
\end{revcomment}
\begin{response}
    In Table 2 in the appendix we listed the 15 LLMs we evaluated. We have updated that number since the original submission to be XX total LLMs with XX-5 being open-source LLMs. This did include some LLMs tuned specifically for mathematics (e.g., Mathstral) and we offer an analysis on the performance differences between the more general LLMs in its parameter size in Appendix H.
\end{response}

\begin{revcomment}
    It would be better to explore the effects of LLMs under in-context learning conditions.
\end{revcomment}
\begin{response}
    We appreciate the reviewer's suggestion and agree that exploring the effects of in-context learning is essential to understanding LLM performance. In fact, our benchmark explicitly incorporates in-context learning scenarios by presenting LLMs with prompts that frame economic reasoning tasks within a broader context. These prompts include relevant background information to simulate realistic economic problem-solving conditions. Additionally, we analyze performance across different domains to identify whether and how contextual framing influences model behavior.
\end{response}

\nextreviewer

\begin{revcomment}
    Section 2: There are numerous economic concepts that may be challenging for machine learning researchers to follow, particularly the rationale behind constructing the taxonomy shown in Figure 1.
\end{revcomment}
\begin{response}
    We acknowledge that the summary level in the main text is brief. To address this, we provide a more detailed rationale in Appendix A, where we explain how we selected and grouped the elements included in our benchmark. While this discussion offers additional depth, it is not intended to exhaustively capture the full scope of the economic literature. For readers interested in further exploration, we include references to foundational texts that provide a comprehensive treatment of these concepts and classifications.
\end{response}


\begin{revcomment}
    Section 3.1: Details about the dataset are lacking. Key statistics, such as the distribution of questions and question lengths, are not provided. Additionally, there is no discussion on how the dataset's quality is ensured.
\end{revcomment}
\begin{response}
    We appreciate the reviewer's comment and have addressed these concerns in the resubmitted paper. To provide greater transparency, we have included additional summary statistics about the dataset in Section 3.1. These statistics detail the total number of benchmark elements, the average and median character counts per question, and the distribution of questions across domains and types, offering a clearer picture of the dataset’s composition and scope.

    To ensure the quality of the dataset, we implemented a two-step verification process. First, all style-transferred templates were manually reviewed by a human to confirm their economic and semantic correctness. Second, we spot-checked a sample of the generated templates to assess their validity. While we did not have the resources to verify every generated question, the high success rate observed during spot-checking gives us confidence in the overall quality of the dataset. We have clarified these quality assurance measures in the auto-STEER section to highlight their role in maintaining the reliability of the benchmark. 
\end{response}

\begin{revcomment}
    Section 5: The analysis of experimental results is limited. There is no case study or error analysis presented to offer deeper insights into model performance.
\end{revcomment}
\begin{response}
We appreciate the reviewer’s feedback regarding the need for a deeper analysis of experimental results. In response, we have expanded Section 5 in the revised version to include detailed case studies and error analyses, highlighting both the strengths and limitations of LLMs on our benchmark.

One key addition is our analysis of free-text QA. This adaptation revealed notable challenges: while models often approximate correct answers in multiple-choice settings by recognizing and selecting the most plausible option, their performance in free-text QA declines. This is particularly evident in tasks requiring precise economic reasoning, where models frequently struggle to articulate complete and accurate responses.

We also observed that including multiple-choice options can positively condition models toward engaging with the question more effectively, as these options serve as cues that guide the reasoning process. In contrast, the open-ended format removes these cues, exposing areas where models lack depth in reasoning or misinterpret economic concepts.
\end{response}

\begin{revcomment}
    Why is it necessary to develop these benchmarks for LLMs? Existing finance-related benchmarks, such as FinQA, FinEval, FinBench, FinanceBench, and FAMMA, already appear to cover similar topics to those in STEER. Additionally, general reasoning datasets in common scenarios may address some overlapping subjects.
\end{revcomment}
\begin{response}
    We appreciate the opportunity to clarify the motivation for developing STEER and how it differs from existing benchmarks.
    
    First, many existing finance-related benchmarks, such as FinanceBench, FiQA, and FinEval, focus primarily on classification-oriented tasks, such as sentiment analysis, named entity recognition, document classification, or knowledge-based question answering. While these benchmarks are valuable for specific NLP applications, they do not evaluate a model’s reasoning capabilities in economic contexts. STEER, by contrast, is designed to assess reasoning skills, requiring models to engage with complex economic problems rather than simply categorizing or retrieving information.
    
    Finally, while general reasoning datasets may overlap with some of the reasoning types STEER investigates, they lack the depth and specif icity required for evaluating economic reasoning. These datasets often focus on general problem-solving or abstract reasoning but do not delve into domain-specific economic concepts. STEER goes beyond testing definitional recall to evaluate whether models can apply economic principles to answer complex, context-dependent questions. This includes rephrased and domain-specific questions that require adaptability and reasoning in specialized economic contexts, making STEER distinct from broader reasoning benchmarks.
\end{response}

\end{document}
