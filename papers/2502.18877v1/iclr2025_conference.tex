
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\usepackage{hyperref}
\usepackage{url}
\usepackage{natbib}

\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amssymb}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage[smallerops]{newtxmath}
\usepackage{xspace}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{xspace}
\usepackage{subcaption}
\usepackage{footmisc}
\usepackage{colortbl}

\usepackage{textcomp}
\definecolor{Gray}{gray}{0.9}
\definecolor{LightCyan}{rgb}{0.75,1,1}


\addtolength{\textheight}{\topskip}

\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\DeclareMathAlphabet{\mathbb}{U}{msb}{m}{n}

\newcommand{\enc}{\mathbf{F}}
\newcommand{\tpath}{\mathbf{p}}

\newcommand{\bfq}{\mathbf{q}}
\newcommand{\bfd}{\mathbf{d}}
\newcommand{\bfD}{\mathbf{D}}
\newcommand{\calL}{\mathcal{L}}

\newcommand{\mbase}{\texttt{base}}
\newcommand{\mlarge}{\texttt{large}}
\newcommand{\mxxl}{\texttt{xxl}}

\newcommand{\GenRet}{\textsc{GenRet}\xspace}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}


\setlist[itemize]{leftmargin=4mm}

\renewcommand{\algorithmiccomment}[1]{\hfill\textcolor{gray}{$\triangleright$ \textit{#1}}}


\title{Hierarchical Corpus Encoder: \\Fusing Generative Retrieval and Dense Indices}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{%
  Tongfei Chen\textsuperscript{\dag$\alpha$} \quad\quad Ankita Sharma\textsuperscript{\dag$\beta$} \quad\quad Adam Pauls\textsuperscript{\dag$\gamma$} \quad\quad Benjamin Van Durme\textsuperscript{*} \\
  \textsuperscript{*}~Microsoft \quad \textsuperscript{$\alpha$} Augment Code \quad \textsuperscript{$\beta$} Apple \quad \textsuperscript{$\gamma$} Scaled Cognition \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
  Generative retrieval employs sequence models for conditional generation of document IDs based on a query (DSI \citep{DSI22}; NCI \citep{NCI22}; \textit{inter alia}).
  While this has led to improved performance in zero-shot retrieval, it is a challenge to support documents not seen during training. 
  %We identify that the performance gain comes from contrastive training against sibling nodes in the hierarchy of document candidates, and port these back to traditional dense encoders by proposing our \emph{hierarchical corpus encoder} (HCE).
  We identify the performance of generative retrieval lies in contrastive training between sibling nodes in a document hierarchy.
  This motivates our proposal, the \emph{hierarchical corpus encoder} (HCE), which can be supported by traditional dense encoders.
  Our experiments show that HCE achieves superior results than generative retrieval models under both unsupervised zero-shot and supervised settings, while also allowing the easy addition and removal of documents to the index.
  \blfootnote{\textsuperscript{\dag}~Work performed at Microsoft.}
\end{abstract}

\section{Introduction}
Recent work on neural information retrieval (IR) has broadly fallen into two categories: \emph{dense encoders}, and \emph{generative retrieval}. For dense retrieval \citep[][\emph{i.a.}]{LeeCT19,KarpukhinOMLWEC20,IzacardCHRBJG22}, a fixed-dimension vector representation is created for each document by an encoder, and searching is performed with techniques such as approximate nearest neighbor (ANN) search or maximum inner product search (MIPS; \cite{Shrivastava014}) with an external index (e.g., FAISS \citep{FAISS}). Such encoders are generally trained in a supervised fashion by learning to distinguish  relevant  documents (positive examples) against irrelevant documents (negative examples) given a query. This is known as contrastive training.

Generative retrieval \citep[][\emph{i.a.}]{DSI22,NCI22} takes a different approach: directly outputting the identifier of the document with an encoder-decoder sequence model without an explicit vector representation for each document. Purported advantages include (a) there is no need to deploy an explicit MIPS index; and (b) better performance in unsupervised settings when adapted to a new corpus without any labeled training data. However, they also suffer from known problems such as the addition of new documents requires continued training, which is both computationally expensive and is prone to catastrophic forgetting \citep{KishoreWLAW23}.%; \textbf{and (b) some version of it cannot easily scale to a large document set.}

Here we examine the innovations of generative retrieval and identify the key important distinction with dense retrieval approaches to date: \emph{tiered hierarchical negative samples}. This motivates our proposal, the \emph{hierarchical corpus encoder} (HCE). %, which jointly learns to embed texts together within a hierarchy of  documents.
At training time, positive samples are contrasted against siblings on a document hierarchy as negative samples, mimicking the loss employed in generative retrieval (\S\ref{sec:gr}). At test time, retrieval falls back to MIPS with an external index. %such that documents can easily be added or removed. 
HCE provides the best of both worlds: (1) zero-shot adaptation to new domains; and (2) efficient addition and removal to the index without fine-tuning. 


Our experimental results demonstrate  that HCE achieves superior performance over a variety of popular dense and generative retrieval methods under both supervised and unsupervised scenarios, illustrating the effectiveness of HCE's modeling of the document set as a hierarchy. 


\section{Background}
\paragraph{Dense Retrieval}
In the dense retrieval paradigm of information retrieval, one seeks to learn an \emph{encoder}  $\enc: V^* \to \mathbb{R}^n$ that maps a string of tokens (from vocabulary set $V$) to a point in a $n$-dimensional vector space. Retrieval can then be performed in by some nearest neighbor search (NNS) or maximum inner product search (MIPS) in this space $\mathbb{R}^n$. A common instantiation of this encoder is a Transformer with a pooling operation on top \citep{IzacardCHRBJG22}, followed by an optional normalization step (that makes the norm of the vector 1, as is done in \cite{NiQLDAMZLHCY22}). 

Under the condition where query-document relevance judgments are present,
it is common practice to train such an encoder with a \emph{constrastive loss} \citep{Sohn16}, where the model is trained to discriminate positive candidates $d^+$ that are relevant to the query $q$ from irrelevant negative candidates $d^- \in D^-$. We denote the vector representation of a query $q$ as $\bfq =\enc(q)$, and similarly $\bfd = \enc(d)$. For a set of documents $D$, we denote $\bfD = \{\enc(d)\}_{d \in D}$, which is a set of vectors. A common form of the loss is
\begin{equation}
      \calL_{C}(\bfq, \bfd^+, \bfD^-) = -\log \frac{\exp S(\bfq, \bfd^+)}{\exp S(\bfq, \bfd^+) +\displaystyle\sum_{\bfd^- \in \bfD^-} \exp S(\bfq, \bfd^-)}\ , 
\end{equation}
where $S(\bfq, \bfd)$ is the scoring function between vectors. This scoring function is usually just  an inner product (optionally scaled by a temperature $\tau$)  between vector embeddings $S(\bfq, \bfd) = \bfq \cdot \bfd/\tau$, or a normalized version $S(q, d) = \frac{\bfq \cdot \bfd}{\tau \cdot \left\|\bfq\right\| \cdot \left\|\bfd\right\|}$ where cosine similarity is computed. 

This general framework is first pioneered by \citet{LeeCT19} and \citet{KarpukhinOMLWEC20} for dense retrieval in NLP, where it is successfully applied to open-domain question answering. 


\paragraph{Generative Retrieval} Generative retrieval is a new paradigm that directly generates document identifiers by the model's \emph{parametric memory}, without using an external index. Specifically, one first assigns a unique string-valued identifier to each document, and when decoding, constrain the decoding process so that its output falls within the set of unique identifiers. Top-$k$ retrieval is done by using beam search in the decoding process. Such method removes the need for any external index, making the whole memory of the corpus \emph{parametrized by} a neural network.

The earliest work in this thread is GENRE \citep{CaoIRP21}, where the correct entity name (e.g. Wikipedia article title) is generated through a sequence-to-sequence model for entity linking. DSI \citep{DSI22} and NCI \citep{NCI22} applied the generative retrieval approach to \emph{ad hoc} document retrieval. \GenRet \citep{GenRet23} learns the document IDs without the initial preprocessing step. Such methods suffer from two drawbacks: (a) \emph{scalability}: challenging to scale to massive scale of documents since all memory is parametrized in the model, absent of external storage; (b) \emph{extensibility}: hard to expand the document set if new documents are going to be indexed, since it is usually pre-trained on a static collection of documents.


Adding new documents in generative retrieval usually requires continued training. There are methods proposed to alleviate the problem: DSI++ \citep{MehtaGT0TRNSM23} sought to mitigate catastrophic forgetting for continued training; IncDSI \citep{KishoreWLAW23} proposed a constrained optimization method to find optimal vectors for new documents, but it only applies to the \emph{atomic} version of DSI. 

Our experiments were done in 2024, which is contemporaneous with similar findings presented in \citet{0003WZC0RRR24}. \citet{0003WZC0RRR24} shows that generative retrieval exhibits behavior analogous to hierarchical search within
a tree index in dense retrieval when using hierarchical semantic
identifiers. Their investigation revealed that generative retrieval can be understood as a special case of multi-vector dense retrieval, which coincides with our theoretical findings presented here.

\section{A Closer Look at Generative Retrieval}
  \label{sec:gr}

Generative retrieval seems to be a departure from traditional contrastive learning, but upon scrutiny, we found that the underlying loss is similar in many ways.
Here we take a closer look at arguably the seminal work that started the research thread on generative retrieval for documents: the \emph{differentiable search index} \citep[DSI;][]{DSI22}. There were two proposed versions of DSI, namely the \emph{atomic}-ID version and the \emph{hierarchical} version. Later work that followed (e.g. NCI; \cite{NCI22}) prioritized the hierarchical version.

\paragraph{Atomic Version}
In the \emph{atomic} version of DSI, each document's ID is added to the vocabulary of the Seq2Seq decoder, and the decoder is run exactly 1 step to generate the correct document ID. The Seq2Seq negative log-likelihood loss would be\footnote{~Bias term omitted for more succinct exposition, as one can always rewrite $(\mathbf{w}^{\rm T}\mathbf{x} + b)$ as $(\mathbf{w}, b)^{\rm T} (\mathbf{x}, 1)$.}
\begin{equation}
\label{eq:gr-atom-1}
    \calL_{\textrm{GR-atom}}(q, d^+) = -\log \left(\mathrm{softmax} (\mathbf{W}_{\rm LMHead}\cdot\mathbf{s})[d^+] \right).
\end{equation}
Note here $\mathbf{s} \in \mathbb{R}^{n_{\rm dec}}$ is the decoder output state (\autoref{fig:dsi} Left), and $\mathbf{W}_{\rm LMHead}$ is the weight matrix of the \emph{language model head} of the decoder, and $\mathbf{x}[d^+]$ selects the probability term from vector $\mathbf{x}$ that corresponds to the document $d^+$. Since the token set of this decoder is the \emph{document ID set}, $\mathbf{W}_{\rm LMHead} = \left[ \mathbf{v}_d^{\rm T} \right]_{d \in \mathcal{D}} \in \mathbb{R}^{|\mathcal{D}| \times n_{\rm dec}}$, and the logits $(\mathbf{W}_{\rm LMHead}\cdot \mathbf{s}) \in \mathbb{R}^{|\mathcal{D}|}$. Expanding \autoref{eq:gr-atom-1}:
\begin{align}
        \calL_\textrm{GR-atom}(q, d^+) &= -\log \frac {\exp \mathbf{s} \cdot \mathbf{v}_{d^+}} {\displaystyle\sum_{d \in \mathcal{D}} \exp \mathbf{s} \cdot \mathbf{v}_{d^+}}  = \calL_C\left(\,\mathbf{s}, \mathbf{v}_{d^+}, \{\mathbf{v}_{d^-}\}_{d^- \in \mathcal{D} \setminus \{d^+\}}\right).
\end{align}
Thus we see that in the \emph{atomic} version of generative retrieval, 
\begin{itemize}
    \item a fixed-length vector $\mathbf{v}_d$ is actually trained for each document $d$, in the form of rows in the weight matrix of the language model head;
    \item a fixed-length vector is actually created for the query, in the form of the decoder state vector $\mathbf{s}$;
    \item and the decoding process of $\arg\max_{d \in \mathcal{D}} \exp \mathbf{s} \cdot \mathbf{v}_d$ is in fact maximum inner product search (MIPS).
\end{itemize} 
Hence the atomic version of generative retrieval can be considered as a form of contrastive learning, where the positive document is contrasted with \emph{all} other documents in the corpus, with the embedding for all documents saved as \emph{parametric memory} in $\mathbf{W}_{\rm LMHead}$ (\autoref{fig:dsi} Left). These embeddings are updated under gradient descent for every training iteration. This is different from contrastive learning in dense retrievers, where usually a small set of negative samples are \emph{sampled} from the corpus.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{dsi.pdf}
    \vspace{-0cm}
    \caption{Left: DSI with atomic IDs. Right: DSI with a document hierarchy.}
    \label{fig:dsi}
    \vspace{-0.4mm}
\end{figure}

\paragraph{Hierarchical Version}
The atomic version above obviously does not scale efficiently beyond hundreds of thousands of candidate documents, as the size of its parametric memory would scale linearly with respect to the number of documents. Therefore the authors of DSI proposed a hierarchical method that limits the number of tokens that the decoder can generate. In the hierarchical version of DSI, a hierarchy of documents is computed before training via divisive $k$-means clustering.  The set of documents are arranged as leaves in a tree, where intermediate nodes are clustering centroids.
In this tree $\mathcal{T}$, each document $d$ is assigned a path from root $\tpath_{d} = (p_{d}^{(1)}, \cdots, p_d^{|\tpath_d|})$ (\autoref{fig:dsi} Right).

The DSI decoder is expected to output this path $\tpath_d$ as the sequence output. Since DSI is trained with a sequence objective with a softmax head, this sequence loss can therefore be expressed as
\begin{align}
  \label{eq:gr-hier}
        \calL_\textrm{GR-hier}(q, d^+) &= -\sum_{t=1}^{|\tpath_{d^+}|} \log \frac {\exp \mathbf{s}^{(t)} \cdot \mathbf{v}(p^{(t)}_{d^+})} {\displaystyle\sum_{p \in \Sigma^{(t)}} \exp \mathbf{s}^{(t)} \cdot \mathbf{v}(p)} = \sum_{t=1}^{|\tpath_{d^+}|} \calL_C \left( \mathbf{s}^{(t)}, \mathbf{v}(p_{d^+}^{(t)}), \{\mathbf{v}_p\}_{p \in \Sigma^{(t)}}\right),
\end{align}
\noindent where $t$ is the decoder step, $\mathbf{s}^{(t)}$ is the decoder state at step $t$, and $\Sigma^{(t)}$ is the set of symbols allowed on depth $t$ of the hierarchy.
Here we see that the hierarchical version of generative retrieval performs contrastive learning at each step in the decoding process: at each step $t$, the decoder state $\mathbf{s}^{(t)}$, acting as a \emph{query}, is matched with all possible tokens $\Sigma^{(t)}$ at this step $t$: the correct action at this step $p_{d^+}^{(t)}$ is contrasted against all other steps. In essence, DSI is taking \emph{tiered hierarchical negative samples}. %In NCI, the authors proposed a prefix-aware weight-adaptive (PAWA) decoder, where the weights for path symbols $p^{(i)}$ and $p^{(j)}$ $(i \ne j)$ are decoupled.

It can be seen that the loss function of generative retrieval essentially consists of one (under the atomic version) or multiple (under the hierarchical version) steps of contrastive loss, with the decoder state at each step functioning as the query vector. We make the observation that \emph{contrastive loss} is a sampled estimation of the full log-likelihood  loss; and the hierarchical version is a computationally efficient way of working with a very large candidate set. Indeed, the same idea has been explored in many applications of NLP. For example, in the early days of neural language modeling, \cite{MorinB05} proposed a \emph{hierarchical softmax} function to speed up the softmax computation over a large vocabulary set.  This was then adopted in Word2Vec \citep{MikolovSCCD13}.

In generative retrieval, decoding is done in a coarse-to-fine manner with constrained beam search. This is akin to the approach of \cite{ChenCD20}, who classified textual entity mentions into a constraining type hierarchy. 
Hierarchical decoding is also reminiscent of indexing algorithms such as hierarchical $k$-means trees \citep{NisterS06,MujaL09}: first build a tree of samples based on clustering  as index, and when decoding, apply beam search over this tree.

With this understanding of generative retrieval, we translate the innovations to a dense retriever.

\section{Model}

We propose HCE, which jointly learns an encoder $\enc: V^* \to \mathbb{R}^{n}$ (the $n$-dimensional hypersphere $\mathcal{S}^{n-1} \subset \mathbb{R}^n$ if normalized) with a hierarchical tree $\mathcal{T}$ of the document set, with a novel loss that takes the hierarchy of documents into account.

\begin{wrapfigure}[9]{r}{0.5\textwidth}
\vspace{-1.3cm}
\begin{center}
    \includegraphics[height=3.4cm]{hierarchy.pdf}
    \vspace{-0.25cm}
    \caption{A document hierarchy with depth 3.}
    \label{fig:hierarchy}
\end{center}
\end{wrapfigure}


\subsection{Learning with Document Hierarchy}

\paragraph{Clustering}
Given an initial encoder $\enc_0$, we can compute all embeddings in the document set $\mathcal{D}$: $\{\enc_0(d)\}_{d \in \mathcal{D}}$. We perform an agglomerative version of  hierarchical clustering (Algorithm \ref{alg:hier-sph-k-means}) on these vectors to form a tree. This  differs from DSI, where divisive clustering is performed. The reason is that we keep the path from the root the same length for all documents (see \autoref{fig:hierarchy}), and easier parallelization on GPUs.

Starting with $|\mathcal{D}|$ vectors for the entire corpus, we perform spherical $K$-means clustering,\footnote{~Spherical $K$-means is used instead of normal $K$-means since most pretrained retriever maximize inner product or cosine similarity between relevant query-document pairs, instead of minimizing $L_2$ distance.} where $K = \lceil |\mathcal{D}|/b \rceil$. Here $b$ is a \emph{branching factor}. We recurse until $K = 1$, when all clusters are collected into a single root node. Note that for each tree node there is no guarantee that it has exactly $b$ children, and  $b$ can be understood as the expected number of vectors in each cluster (see \autoref{fig:hierarchy}). For each clustering step, the spherical $K$-means clustering \citep{DhillonM01} is used (Algorithm \ref{alg:sph-k-means}).

\begin{figure}[t]
\vspace{-0.5cm}
\begin{minipage}[t]{0.48\textwidth}%
\begin{algorithm}[H]
\caption{\textsc{HierAggCluster}}\label{alg:hier-sph-k-means}
\begin{algorithmic}
\small
\Require vectors $\mathbf{v}_i \in \mathbb{R}^n, ~ 1 \le i \le |\mathcal{D}|$
\Require branching factor $b$
\State $t \leftarrow \lceil \log_b |\mathcal{D}|\rceil$
\State $T^{(t)} \leftarrow \{\textsc{Tree}(\mathbf{v}_i, \varnothing)\}$ \Comment{\small A forest of leaf nodes}
\State $K \leftarrow \lceil |\mathcal{D}| / b \rceil$
\While{$t > 0$}
   \State {$T^{(t-1)} \leftarrow \textsc{SphKMeans}(T^{(t)}, K)$} \\ \Comment{\small Clusters into a forest of subtrees}
   \State $t \leftarrow t  - 1$
   \State $K \leftarrow \lceil K/b \rceil$
\EndWhile \\
\Return $T^{(0)}$ \Comment{\small Returns a tree with a single root}
\end{algorithmic}
\end{algorithm}
\end{minipage}%
\hfill%
\begin{minipage}[t]{0.48\textwidth}%
\begin{algorithm}[H]
\small
\caption{\textsc{SphKMeans}}\label{alg:sph-k-means}
\begin{algorithmic}
\Require vectors $\mathbf{v}_i \in \mathbb{R}^n, ~ 1 \le i \le \left|\mathcal{D}\right|$
\Require number of clusters $K$
\State $\forall i,~ a_i \sim \mathrm{Unif} \{1, \cdots, K\}$ \Comment{{\small \it Random init}}
\While{$a_i$ not converged}
   \State $\forall k,~ \mathbf{c}_k \leftarrow \dfrac{{\displaystyle\sum}_{i : a_i = k} \mathbf{v}_i} {\left\|{\displaystyle\sum}_{i : a_i = k} \mathbf{v}_i \right\|}$ \Comment{{\small \it E step}}
   \State $\forall i,~ a_i \leftarrow \arg\max_k \mathbf{v}_i \cdot \mathbf{c}_k$ \Comment{{\small \it M step}}
\EndWhile \\
\Return $\left\{ \textsc{Tree}(\mathbf{c}_k, \{i : \mathbf{v}_i\}_{a_i = k}) \right\}_{1 \le k \le K}$ \\ \Comment{\small Returns a forest of clusters} \\
\Comment{\small $\textsc{Tree}(r, C)$ is a tree with root $r$ and children $C$}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\vspace{-0.6cm}
\end{figure}

The resulting hierarchical tree $\mathcal{T} = T^{(0)}$ has depth $L = \lceil \log_b |\mathcal{D}|\rceil $, so that each document $d$ can be encoded as a fixed-length path  $\tpath_{d} = (p_d^{(1)}, \cdots, p_d^{(L)})$ from root (see \autoref{fig:hierarchy}, where the highlighted leaf node has its path to root $\tpath = (1, 1, 0)$ bolded). 
Each prefix $(p^{(1)}, \cdots, p^{(l)})~(l < L)$ of this path points to a \emph{nonterminal} node $c$ of this tree, and corresponds to a centroid from the hierarchical clustering process. We denote the vector of the centroid as $\mathbf{c}(p^{(1)}, \cdots, p^{(l)}) \in \mathbb{R}^n$.

\paragraph{Hierarchy-aware Loss}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{hce.pdf}
  \vspace{-0.5cm}
  \caption{Illustration of HCE training, where the query is contrasted with tiered negative samples. Query and documents here are taken from the NQ320k dataset \citep{KwiatkowskiPRCP19}.}
  \vspace{-0.2cm}
  \label{fig:hce}
\end{figure}

Provided that the hierarchy is constructed, we adapt the sequence decoding loss ($\calL_{\textrm{GR-hier}}$, \autoref{eq:gr-hier}) in generative retrieval to our encoder-based method. Recall that $\calL_{\textrm{GR-hier}}$ contrasts the vector of the positive path against the vector of the negative paths on each layer of the hierarchy, given the decoder state at each step $\mathbf{s}^{(t)}$ as the query vector. We make two modifications:
\begin{itemize}
    \item Since there is no decoder in our case, the query vector \emph{stays the same across steps}: it will always be the vector embedding $\mathbf{q} = \enc(q)$ of the query $q$ across levels.
    \item Vectors for intermediate nodes are the \emph{centroid vectors of the prefixes} from  $K$-means clustering.
\end{itemize}

As such, given query $q$ and its relevant document $d^+$, at step $t$ on hierarchy $\mathcal{T}$, we contrast the positive prefix $(p_{d^+}^{(0)}, \cdots, p_{d^+}^{(t)})$ against \emph{all its siblings} $\left\{(p_{d^+}^{(0)}, \cdots, p_{d^+}^{(t-1)}, p^-) ~\Big|~ p^- \ne p_{d^+}^{(t)} \right\}$ (\autoref{fig:hce}):
\begin{equation}
  \label{eq:hce-hier}
    \calL_H^{(t)}(q, d^+) = \calL_C\left(\bfq, ~\mathbf{c}(p_{d^+}^{(0)}, \cdots, p_{d^+}^{(t)}), ~\left\{ \mathbf{c}(p_{d^+}^{(0)}, \cdots, p_{d^+}^{(t-1)}, p^-) ~\Big|~ p^- \ne p_{d^+}^{(t)} \right\}\right)
\end{equation}

Naturally we could do this for each layer of the hierarchy: $\calL_H(q, d^+) = {\displaystyle\sum}_{t = 1}^{L} \calL_H^{(t)}(q, d^+)$, taking \emph{tiered hierarchical negative samples}.
However, doing this for the full hierarchy is not memory-efficient: it requires storing the embedding of all documents (leaf nodes of the hierarchy) as parameters! This gets us back to the situation of generative retrieval with atomic IDs (\S\ref{sec:gr}). We adopt a simple solution: retain the vectors of the first $M (M<L)$ layers of the $L$ layers in memory. For the first $M$ layers, we apply the hierarchy loss in \autoref{eq:hce-hier}; for the last $(L-M)$ layers, we fall back to constrastive loss, where negative samples are sampled within the children of the prefix (\autoref{fig:hce}):
\begin{equation}
  \label{eq:hce}
    \calL_{\rm HCE}(q, d^+) = \underbrace{\sum_{t = 1}^{M} \calL_H^{(t)}(q, d^+)}_\text{Hierarchy loss} + \underbrace{\sum_{t=M+1}^L\calL_C^{\leftrightarrow} \left(q, d^+, \mathrm{Sample}_{n_{\rm NS}} \left(p_{d^+}^{(1)}, \cdots, p_{d^+}^{(t)}\right) \setminus \{d^+\} \right)}_\text{Contrastive loss with negative samples}.
\end{equation}
Here $\calL_C^{\leftrightarrow}$ is a bi-directional contrastive loss elaborated below, and $\mathrm{Sample}_{n_{\rm NS}}(p)$ samples $n_{\rm NS}$ documents that are the children of the prefix $p$. In our experiments we take $n_{\rm NS} = 4$.



\paragraph{Contrastive Loss with Tricks}
It has been shown that the \emph{in-batch negative trick} \citep{HendersonASSLGK17,ChenK0H20} improves the performance by including positive documents for other queries $j \ne i$ in the same \emph{mini-batch}\footnote{~Batch size can be increased by the number of GPUs by using the \texttt{AllGather} operation, e.g. in \texttt{NCCL}.} $B$ as negative documents for query $i$. 
Additionally, \cite{NiQLDAMZLHCY22} shows that one can do a bi-directional loss: viewing the candidate as the query and the queries in the batch as candidates. We combine these tricks. For details, see \autoref{app:contrastive}.

\subsection{Training under Different Scenarios}

Our proposed method HCE can be used in both supervised training where query-document relevance judgments are present; and in zero-shot cases where query dataset is absent.

\paragraph{Supervised Training} Given a training set of $\mathcal{S}_{\rm train} = \{(q, d^+)\}$ (there is no need for labeled irrelevant samples) and a corpus $\mathcal{D}$, we simply optimize the HCE loss (\autoref{eq:hce}) proposed above.
\paragraph{Unsupervised Training} In this scenario only the corpus $\mathcal{D}$ is present, and our goal of this training step is to align the encoder's output distribution with the distribution of the corpus.

The goal is creating a pseudo-query for each document. We experimented with two approaches:
\begin{itemize}
    \item \textbf{Inverse Cloze Task} (ICT; \cite{LeeCT19}): A random segment $\tilde q$ of text $d^+$ is used as a pseudo-query: $\calL_U(d^+) = \calL_{\rm HCE}(\tilde\bfq, \bfd^+)$. In NCI \citep{NCI22} it is referred to as \emph{document-as-query}, and we follow their choice of taking a random span of 64 tokens;
    \item \textbf{SimCSE} \citep{GaoYC21}: The text $d^+$ is encoded again with a different randomized dropout mask, yielding a different encoding $\tilde\bfd^+$ from the original $\bfd^+$. Then the variant $\tilde\bfd^+$ is used as the query vector: $\calL_U(d^+) = \calL_{\rm HCE}(\tilde\bfd^+, \bfd^+)$.
\end{itemize}
We found that these two methods perform similarly. For all experiments described, ICT is used.

\paragraph{Joint Training} Additionally, the supervised and the unsupervised objective can be \emph{jointly} optimized:
\begin{equation}
  \label{eq:hce-joint}
    \calL_J(\mathcal{S}_{\rm train}, \mathcal{D}) = \mathbb{E}_{(q, d^+) \sim \mathcal{S}_{\rm train}} \left[\calL_S(q, d^+)\right] + \alpha \cdot \mathbb{E}_{d \sim \mathcal{D}} \left[\calL_U(d)\right].
\end{equation}

This achieves the two goals outlined in \cite{WangI20}: (a) aligning the query with its relevant document; (b) ensures that the distribution of the corpus is nicely distributed.

\begin{wrapfigure}[17]{r}{0.5\textwidth}
\vspace{-0.9cm}
\begin{minipage}{0.5\textwidth}
\begin{algorithm}[H]
\small
\caption{\textsc{EM-Style-Train}}\label{alg:em-style-training}
\begin{algorithmic}
\Require Training dataset $\mathcal{S}_{\rm train}$
\Require Validation dataset $\mathcal{S}_{\rm dev}$
\Require Document collection $\mathcal{D}$
\Require Initial model checkpoint $\enc_0$
\State $\enc \leftarrow \enc_0$
\State $\mathcal{T} \leftarrow \textsc{HierAggCluster}(\{\enc_0(d)\}_{d \in \mathcal{D}})$
\State $m \leftarrow \textsc{Metric}(\enc, \mathcal{S}_{\rm dev}, \mathcal{D})$ \Comment{\small Some metric on $\enc_0$}
\While{early stopping criteria not met}
  \State $\enc^\prime \leftarrow \textsc{Optimize}(\enc, \mathcal{T}, \mathcal{S}_{\rm train}, \mathcal{D})$
  \State $m^\prime \leftarrow \textsc{Metric}(\enc^\prime, \mathcal{S}_{\rm dev}, \mathcal{D})$
  \If{$m^\prime > m$} \Comment{\small A better representation found}
    \State $\mathcal{T} \leftarrow \textsc{HierAggCluster}(\enc^\prime(d)_{d \in \mathcal{D}})$
    \State $m \leftarrow m^\prime$
  \EndIf
  \State $\enc \leftarrow \enc^\prime$
\EndWhile \\
\Return $\enc$
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{wrapfigure}
\paragraph{Co-Training of Encoder and Hierarchy}
Finally, we introduce a co-training approach for jointly optimizing the encoder and the hierarchy. In existing generative retrievers (e.g. DSI, NCI), the construction of the hierarchy is a preprocessing step, and is usually computed from another encoder.\footnote{~In both DSI and NCI, document vectors used to generate the hierarchy is computed with BERT-\mbase~whereas their model is based on T5, causing a discrepancy. HCE uses the same underlying model.} We seek to jointly optimize these in an EM-style (coordinate descent with alternating maximization) co-training setup (Alg. \ref{alg:em-style-training}).

In essence, after each epoch, if the metric on the validation set increases (i.e., a better representation of the corpus is obtained), a re-clustering of the corpus will be triggered.  We  stress that after training completes, the hierarchy $\mathcal{T}$ can be \emph{safely discarded} as only the encoder $\enc$ is needed for downstream indexing and retrieval.

\section{Experiments \& Discussions}

\paragraph{Setup}
We start from the pre-trained dense encoder GTR\footnote{~GTR normalizes the embeddings so that their $L_2$ norm is 1, thus mapping into the hypersphere $\mathcal{S}^{n-1}$.} \citep{NiQLDAMZLHCY22} as both a baseline and the initial checkpoint where we continue training, as this is also used as a baseline in DSI \citep{DSI22}. Additionally GTR, DSI, and NCI are all fine-tuned versions of T5, facilitating fair comparison. % that is, taking the scoring function between the query and document vector as $S(\bfq, \bfd) = \frac{\bfq\cdot\bfd}{\tau\cdot\|\bfq\|\cdot\|\bfd\|}.$

We mostly follow the configurations as specified in GTR, where  the temperature for contrastive loss is set as $\tau = 0.01$, with the AdaFactor optimizer \citep{ShazeerS18} with linear decay in accordance with GTR's hyperparameters.\footnote{~A minor difference is that we employ an initial learning rate $\eta = 10^{-4}$, since we found that the original $\eta = 10^{-3}$ causes unstable oscillation under unsupervised learning.} We use machines with $8\times$ NVidia A100 GPUs with \texttt{bfloat16} precision, with total batch size across GPUs 512 (for the ArguAna dataset, 192). No experiment ran for more than 1 day.

\textbf{HCE-\{U, S, J\}} stands for unsupervised, supervised, and joint training respectively. For HCE-J, the loss weight for unsupervised learning in \autoref{eq:hce-joint} is set at $\alpha = 0.5$.

\subsection{Supervised Setting: NQ320k \& TriviaQA}

\paragraph{Datasets} We employ two datasets commonly used in prior work for supervised retrieval training: NaturalQuestions \citep{KwiatkowskiPRCP19} and TriviaQA \citep{JoshiCWZ17}. NaturalQuestions collects real-user queries from Google and each query is paired with an answer span from a Wikipedia article. A common version of the dataset is NQ320k, where there are 320k query-passage pairs.
TriviaQA is a reading comprehension dataset whose candidate passages also come from Wikipedia. We follow the preprocessing scripts in NCI \citep{NCI22}. For details see \autoref{app:stat}.


\paragraph{Baselines and Metrics}
We include the following methods as baselines, grouped by their categories: \begin{itemize}
    \item \textbf{Sparse retrieval: } BM25 \citep{RobertsonWJHG94} and a version with augmented generated queries by DocT5Query \citep{nogueira2019doc2query}. 
    \item \textbf{Pre-trained dense retrievers:} These are dense retrievers pre-trained or fine-tuned with other datasets such as MS MARCO \citep{NguyenRSGTMD16}, but not fine-tuned on the datasets here. These include Contriever fine-tuned on MS MARCO \citep{IzacardCHRBJG22} (based on BERT) and GTR (based on T5). 
    \item \textbf{Fine-tuned dense retrievers:} Dense retrievers fine-tuned on NQ320k or TriviaQA. These include DPR \citep{KarpukhinOMLWEC20} and ANCE \citep{XiongXLTLBAO21}. We also fine-tune our initial checkpoint, GTR, with ANCE-style contrastive training only (without hierarchy, denoted with ``+CT'') to illustrate the gain of our novel hierarchy loss. Our full HCE also falls into this category.
    \item \textbf{Generative retrievers:} DSI \citep{DSI22}; NCI \citep{NCI22}; SEAL \citep{BevilacquaOLY0P22} where substrings are generated as document IDs; and \GenRet~\citep{GenRet23}, the state-of-the-art generative retriever that learns document IDs without clustering as preprocessing.
\end{itemize}
\noindent We use the NIST  \texttt{trec\_eval}\footnote{~\url{https://github.com/usnistgov/trec_eval}.} tool, reporting the union of the common metrics employed in prior work, including recall at various cutoff $k$ (R@$k$), and mean reciprocal rank (MRR).\footnote{~In some prior work, hit@$k$ (a.k.a. success@$k$) is reported, which is defined to be the ratio of queries where at least 1 relevant document is found within the top-$k$ retrieved set. Note that for NQ320k, each query is paired with at most 1 relevant document, thus recall@$k$ = hit@$k$ for NQ320k. However, for TriviaQA, there may be more than 1 relevant document for a query, hence recall@$k$ $\ne$ hit@$k$. }

\paragraph{Hyperparameters} Learning rate, temperature, etc. are set in the prior section. Here the only parameter we tune is the branching factor $b$, which impacts the performance (see analysis below).

\paragraph{Query Generation} In some of the baseline methods, query generation (QG) is applied to the document set --- these retrievers are marked with ``+QG'' in \autoref{tab:results-supervised}. In all of these retrievers, DocT5Query \citep{nogueira2019doc2query} fine-tuned by MS MARCO \citep{NguyenRSGTMD16} is employed. For sparse retrievers, generated queries are appended to the document to be indexed. For dense \& generative retrievers, generated queries and corresponding documents are added to the training set.
\begin{table}[!t]
\small
  \centering
    \adjustbox{max width=0.9\linewidth}{
      \begin{tabular}{l|ccccc|ccc}
        \toprule
           \bf Model & \multicolumn{5}{c}{\bf NQ320k} & \multicolumn{3}{c}{\bf TriviaQA} \\
            \cmidrule(lr){2-6}\cmidrule(lr){7-9}
                            &R@1    & R@5 & R@10& R@100 & MRR & R@5 & R@20 & R@100 \\
        \midrule  
          \multicolumn{7}{l}{\bf Sparse retrievers} \\
          BM25              & 0.297 & 0.508 & 0.603 & 0.821 & 0.402 & 0.569 & 0.695 & 0.802  \\
          BM25 (+QG)        & 0.380 & 0.591 & 0.693 & 0.861 & 0.489 & 0.597 & 0.721 & 0.827\\
        \midrule
          \multicolumn{7}{l}{\bf Pre-trained dense retrievers} \\
          Contriever        &  0.436 &  0.717 &  0.796 &0.944 &  0.561 & 0.565 & 0.706 & 0.832 \\
          Contriever-MSMARCO& 0.597 &  0.840 &  0.902 & 0.973 &  0.705 & 0.645 & 0.762 & 0.869  \\
          GTR (\mbase) & 0.560 & 0.792 & 0.844 & 0.937 & 0.662 & 0.610 & 0.724 & 0.832 \\ 
          GTR (\mlarge) & 0.565 & 0.801 & 0.854 & 0.944 & 0.677 & 0.625 & 0.742 & 0.852 \\
        \midrule  
          \multicolumn{7}{l}{\bf Supervisedly trained dense retrievers} \\
          DPR               & \it 0.463 & \it 0.695 & \it 0.756 & \it 0.909 & \it 0.569 & \it 0.512 & \it 0.622 & \it 0.741 \\
          ANCE              & 0.526 &       & 0.804 &  0.913 & 0.628 &       & 0.803 & 0.853 \\
          GTR (\mlarge), +CT & 0.622 & 0.869 & 0.926 & 0.968 & 0.735 & 0.747 & 0.876 & 0.948 \\  
          GTR (\mlarge), +CT, +QG & 0.672 & 0.896 & 0.927 & 0.972 & 0.775 & 0.763 & 0.887 & 0.953 \\
        \midrule
          \multicolumn{7}{l}{\bf Generative retrievers} \\
          DSI (\mbase)      & 0.274 &       & 0.566 &        &  \\
          DSI (\mlarge)     & 0.356 &       & 0.626 &        &  \\
          NCI (\mbase)      & 0.602 & \it 0.766 & 0.802 &  0.909 & 0.679  \\
          NCI (\mbase), +QG & 0.659 & \it 0.821 & 0.852 & 0.924 & 0.731 & 0.782 & 0.873 & 0.928 \\
          NCI (\mlarge), +QG & 0.662 & \it 0.817 & 0.853  & 0.925 & 0.734 & 0.802 & 0.886 & 0.936  \\
          \GenRet~(+QG)     & 0.681 & \it 0.861 & 0.888 &  0.952 & 0.759 & \it 0.789 & \it 0.881 & \it 0.940 \\
        \midrule 
          \multicolumn{7}{l}{\bf Ours: Hierarchy-aware trained dense retrievers} \\
          \rowcolor{LightCyan}
          HCE-J (\mbase)      & 0.620 & 0.876 & 0.906 & 0.969 & 0.705 & 0.764 & 0.881 & 0.946 \\
          \rowcolor{LightCyan}
          HCE-J (\mbase), +QG & 0.640 & 0.878 & 0.914 & 0.973 & 0.742 & 0.785 & 0.898 & 0.954 \\
          \rowcolor{LightCyan}
          HCE-J (\mlarge)     & 0.695 & 0.899 & 0.933 & 0.976 & 0.789 & 0.768 & 0.890 & 0.956 \\
          \rowcolor{LightCyan}
          HCE-J (\mlarge), +QG & \bf 0.712 & \bf 0.906 &\bf 0.939 & \bf 0.979 &\bf 0.801 &\bf 0.803 &\bf 0.906 &\bf 0.962 \\ 
          \rowcolor{LightCyan}
          ~~\it Hierarchy co-training removed & 0.700 & 0.899 & 0.931 & 0.977 & 0.793 & 0.783 & 0.894 & 0.957 \\
        \bottomrule
    \end{tabular}
  }
  \caption{Performance on NQ320k and TriviaQA. Numbers typeset in italics are recomputed if model output is public; or rerun from their public implementations. Missing numbers due to our inability to reproduce the exact method to our best effort (may be due to unreleased source code). All rows labeled with \mbase/\mlarge~ are based on T5-\mbase/\mlarge, thus sharing the same number of parameters for fair comparison. }
  \vspace{-0.5cm}
  \label{tab:results-supervised}
\end{table}


\paragraph{Results} Jointly trained HCE outperformed all dense and generative retrieval baselines across both datasets (\autoref{tab:results-supervised}). This demonstrates the effectiveness of HCE's learned vector representations.\footnote{~Note that HCE is an \emph{encoder-only} model: it has only half of the parameters of DSI and NCI models.} HCE is especially good at recall@$k$ where $k > 5$, with a 6--7\% boost against NCI and 2\% gain against \GenRet~under NQ320k, and a 2\% boost against NCI for TriviaQA.

HCE's performance gain is less in recall@$k$ when $k$ is small, as compared to a large $k$. Our hypothesis is that when discriminating between similar documents under the same prefix in the hierarchy, HCE uses a negative-sampling approach for computational efficiency, resulting in less negative samples than the full set under the prefix being trained. This in turn causes good performance on the first $M$ levels of the hierarchy but not as good at the last layer of the hierarchy.

\paragraph{Discussion: Ablation Studies} 
Here we remove certain components from our best setup to see the contribution of each component. From \autoref{tab:results-supervised}, we see that by removing query generation or hierarchy co-training (\autoref{alg:em-style-training}), the performance of R@1 drops about 1\%, while for recall at $k>=10$ the results are largely the same. Comparing HCE with GTR+CT we found that our novel hierarchy-aware loss improves the performance of R@1 about 4\% across datasets.

\paragraph{Discussion: Effect of Tree Depth} A critical hyperparameter of HCE is $b$, the \emph{branching factor} that controls the tree depth and the expected number of children for each intermediate node. Intuitively, the deeper the hierarchy is, the finer-grained tiered negative samples each relevant document receives. 

\begin{wrapfigure}[14]{r}{0.4\textwidth}
  \vspace{-0.4cm}
  \includegraphics[width=\linewidth,trim={0 0 0cm 0.2cm},clip]{b.pdf}
  \caption{Recall@$\{1,5,10,100\}$ for various branching factors $b$ under NQ320k.}
  \vspace{-0.2cm}
  \label{fig:branching-factors}
\end{wrapfigure}
We ran experiments for NQ320k under the best setup (HCE-J, \mlarge, +QG), with $b \in \{2,4,8,16,32,64,128,256\}$. Recall at $\{1,5,10,100\}$ with varying $b$ are plotted in \autoref{fig:branching-factors}, with the tree depth also shown ($L = \lceil \log_b |\mathcal{D}|\rceil$). We set $M = L - 1$: the last layer is computed via negative sampling.

It can be seen that recall at 10/100 remains mostly the same for all $b$. A significant difference can be observed for recall@1: The best result, $b=8$, has R@1 = 0.712, whereas for $b \ge 64$, R@1 $\approx 0.67$. This 4\% gap can be attributed to the finer-grained hierarchy: the sampled negative siblings are much closer to the positive document, generating harder negative samples for contrastive learning.


\subsection{Unsupervised Setting: BEIR}
We evaluate on the BEIR benchmark \citep{BEIR} that contains a diverse set of IR tasks. We use a subset (BEIR-14) that consists of 14 of these datasets following prior work in \cite{IzacardCHRBJG22}: MS MARCO \citep{NguyenRSGTMD16}, which is already used as a fine-tuning dataset in our initial checkpoint GTR, and also datasets without a public license, are excluded.

The branching factor $b$ is universally set to be the smallest power of 2 such that the hierarchy depth $L = 3$, i.e., $\tilde b = \sqrt[3]{|\mathcal{D}|};~ b = 2^{\lceil \log_2 \tilde b\rceil}$. For example, given a document set of 1M documents, we have $\tilde b = \sqrt[3]{1000000} = 100$, hence we set $b = 128$. In the HCE loss, we choose $M = 2$, i.e., the loss of the first two layers are computed via the hierarchy, whereas the last year through contrastive sampling. 
\newcolumntype{y}{>{\columncolor{LightCyan}}c}
\begin{table}[!t]
\small
    \centering
    \adjustbox{max width=0.85\linewidth}{
        \begin{tabular}{l|c|ccc|c|cycy}
          \toprule
            \bf Dataset  & BM25 & DPR & ANCE & TAS-B & \GenRet & \multicolumn{2}{c}{T5-\tt base} & \multicolumn{2}{c}{T5-\tt large} \\
            \cmidrule(lr){7-8}\cmidrule(lr){9-10}
                          &      & & & & +QG & GTR & HCE-U & GTR & HCE-U \\
          \midrule
            TREC-COVID    & 0.656 & 0.332 & 0.654 & 0.481 & 0.718 & 0.589 & 0.688 & 0.591 & \bf 0.724 \\
            NFCorpus      & 0.325 & 0.189 & 0.237 & \bf 0.383 & 0.316 & 0.304 & 0.328 & 0.329 & 0.349 \\
            NQ            & 0.329 & 0.474 & 0.446 & 0.463 &       & 0.495 & 0.514 & 0.547 & \bf 0.561 \\
            HotpotQA      & \bf 0.603 & 0.391 & 0.456 & 0.584 &       & 0.535 & 0.567 & 0.579 & 0.590 \\
            FiQA-2018     & 0.236 & 0.112 & 0.295 & 0.300 & 0.302 & 0.352 & 0.388 & 0.424 & \bf 0.473 \\
            ArguAna       & 0.315 & 0.175 & 0.415 & \bf 0.429 & 0.343 & 0.363 & 0.374 & 0.380 & 0.387 \\
            Touch\'e-2020 & \bf 0.367 & 0.131 & 0.240 & 0.162 &       & 0.303 & 0.321 & 0.326 & 0.339 \\
            CQADupStack   & 0.299 & 0.153 & 0.296 & 0.314 &       & 0.118 & 0.141 & 0.127 & \bf 0.153 \\
            Quora         & 0.789 & 0.248 & 0.852 & 0.835 &       & 0.880 & 0.874 & \bf 0.885 & 0.880 \\
            DBPedia-entity& 0.313 & 0.263 & 0.281 & 0.384 &       & 0.347 & 0.358 & 0.391 & \bf 0.408 \\
            SciDocs       & 0.158 & 0.077 & 0.122 & 0.149 & 0.149 & 0.140 & 0.160 & 0.158 & \bf 0.183 \\
            FEVER         & \bf 0.753 & 0.562 & 0.669 & 0.700 &       & 0.646 & 0.671 & 0.682 & 0.714 \\
            Climate-FEVER & 0.213 & 0.148 & 0.198 & 0.228 &       & 0.241 & 0.269 & 0.262 & \bf 0.277 \\
            SciFact       & 0.665 & 0.318 & 0.507 & 0.643 & 0.639 & 0.595 & 0.640 & 0.631 & \bf 0.674 \\
          \midrule
            Average ($<$200k)& 0.393& 0.201 & 0.372 & 0.398 & 0.411 & 0.391 & 0.430 & 0.420 & \bf 0.465 \\
            Average       & 0.430 & 0.255 & 0.405 & 0.432 &       & 0.422 & 0.450 & 0.451 & \bf 0.479 \\
          \bottomrule
    \end{tabular}}
    \caption{NDCG@10 for BEIR-14 datasets under unsupervised training.}
    \vspace{-0.5cm}
    \label{tab:ndcg_beir}
\end{table}
\begin{table}[!t]
\small
    \centering
    \adjustbox{max width=0.75\linewidth}{
        \begin{tabular}{l|cyyy|cyyy}
          \toprule
            \bf Dataset  & \multicolumn{4}{c}{T5-\tt base} & \multicolumn{4}{c}{T5-\tt large} \\
            \cmidrule(lr){2-5}\cmidrule(lr){6-9}
                     & GTR & HCE-U & HCE-S & HCE-J & GTR & HCE-U & HCE-S & HCE-J \\
          \midrule
            NFCorpus & 0.304 & 0.328 & \bf 0.337 & 0.331 & 0.329 & 0.349 & \bf 0.423 & 0.403 \\
            HotpotQA & 0.535 & 0.567 & 0.595 & \bf 0.605 & 0.579 & 0.590 & 0.663 & \bf 0.676 \\
            FiQA-2018& 0.352 & 0.388 & 0.392 & \bf 0.401 & 0.424 & 0.473 & 0.482 & \bf 0.491 \\
            SciFact  & 0.595 & 0.640 & 0.746 & \bf 0.764 & 0.631 & 0.674 & 0.805 & \bf 0.811 \\
          \bottomrule
    \end{tabular}}
    \caption{NDCG@10 for BEIR datasets with a training set under different training scenarios.}
    \vspace{-5mm}
    \label{tab:ndcg-beir-joint}
\end{table}


\paragraph{Baselines and Metrics} BEIR is designed for \emph{zero-shot retrieval}: as such, we compare our unsupervised HCE-U with various baselines. Baselines include BM25, dense retrievers DPR, ANCE, GTR, and TAS-B \citep{HofstatterLYLH21}, a dense retriever trained with balanced topic aware sampling. Additionally, \GenRet, the state-of-the-art generative retriever, reported its performance on the 6 BEIR datasets whose corpus has $<$200k documents. \footnote{~We found that \GenRet does not scale to corpora larger than 200k documents owing to memory issues.}
%We report performance on all BEIR-14, with some datasets containing >5M documents for retrieval, showcasing the scalability of HCE. 
Following the setup in BEIR, the preferred metric is NDCG@10 and recall@100 (R@100). 


\paragraph{Results}
 Unsupervised HCE consistently outperform GTR (with the exception of Quora), with an improvement of $\approx$ 1--11\% (on average 3\%) for NDCG@10 (\autoref{tab:ndcg_beir}) and $\approx$ 3\% for R@100 ( \autoref{app:recall}). We see that the distribution of the corpus plays an important role here: for corpus like TREC-COVID (biomedical literature) which deviates from the distribution of normal web search text, we see a 10\% improvement after unsupervised fine-tuning; for datasets like NQ, HotpotQA, and DBPedia-entity where the documents come from Wikipedia, the performance gain is less, presumably because the pre-trained GTR has already seen text from Wikipedia. Unsupervised fine-tuning does not yield a gain in Quora, because the documents being retrieved are also queries (the task asks the model to find potentially duplicate questions asked on Quora), which are  short --- the inverse cloze task (ICT) sampling that takes a short excerpt does not work properly in this case.

\paragraph{Discussion: Effect of Supervision}
We study the effect of (un)supervised HCE training. We take the subset of BEIR where training data is provided publicly, and run HCE under 3 scenarios: unsupervised, supervised, and joint training. Results are reported in \autoref{tab:ndcg-beir-joint}. 

We see that with supervision, adding unsupervised training as an auxiliary task results in $\approx$1\% gain in NDCG@10, showing that the joint loss in \autoref{eq:hce-joint} is effective. The performance drop in NFCorpus is due to the dataset's imbalanced training set.\footnote{~NFCorpus has more than 100k training pairs but only 3k documents in the corpus --- setting the loss weight $\alpha = 1$ (balanced between supervised and unsupervised objective) may be detrimental here.}




\paragraph{Discussion: Scalability and Complexity for Clustering}

At training time, apart from the encoder, the centroids of the first $M$ layers of the hierarchy are optimized as parameters, but can be discarded at indexing and retrieval time. This requires $O(b^M n)$ extra space at training, where $n$ is the dimensionality of the embeddings. In practice, with a corpus with $N \approx 5,\!000,\!000$ documents (e.g. Wikipedia articles), we have $b = 256, M = 2, n = 768$. This space consumption is tolerable under this scenario.

The $k$-means algorithm runs in $O(INkn)$ time, where $I$ is the number of iterations. We observed in all datasets the iteration process  converged before $I < 256$. Considering $I$ a constant, the total time complexity for constructing a document hierarchy is 
$\sum_{i=1}^{\lceil \log_b N \rceil} O\left(\frac{N}{b^{i-1}} \cdot \frac{N}{b^{i}} \cdot n \right) \simeq O(N^2 n/b)$. This complexity is high, but ameliorated since it can be parallelized on GPUs, and it can be made to a streaming version for lower memory consumption.\footnote{~Our method runs easily on the whole Wikipedia as corpus, with $>$5M documents to be indexed.} 

\begin{wrapfigure}[24]{r}{0.33\textwidth}
  \vspace{-1.6cm}
  \includegraphics[width=\linewidth,trim={0 0 0cm 0.2cm},clip]{h1.pdf}
  \smallskip\par
  \includegraphics[width=\linewidth,trim={0 0 0cm 0.2cm},clip]{h10.pdf}
  \caption{Recall@$\{1,10\}$ for incremental updates on NQ320k.}
  \label{fig:inc}
\end{wrapfigure}
\subsection{Incrementally Updated Retrieval}

Our proposed HCE supports on-the-fly update of documents to the index without training, since the new documents can just be encoded and added to the index. However, if too many documents are added to the extent that the corpus distribution is distorted, a re-training and re-indexing might be needed. We study this scenario of a streaming document set under the setting of IncDSI \citep{KishoreWLAW23}, who proposed the setting of \emph{incrementally updated retrieval}. A dataset $\mathcal{D}$ is partitioned into 3 subsets: the old set $\mathcal{D}_0$ (90\%) which is used to train the retriever;  the new set $\mathcal{D}^\prime$ (9\%), in a streaming fashion after training;and a tuning set $\mathcal{D}^*$ (1\%) for validation purposes.

We use the dataset partitions for NQ320k provided in \citet{KishoreWLAW23}. Following their setup, we add $k \in \{10^1, 10^2, 10^3, 10^4\}$ new documents in $\mathcal{D}^\prime$ to an index trained with the same old set $\mathcal{D}_0$. Since our method of adding new documents simply involves encoding the new documents, there is no parameter to tune so $\mathcal{D}^*$ is discarded. We measure R@1 and R@10 at different $k$ (number of documents added) for both the original test set in $\mathcal{D}_0$ and the queries associated with the new documents in $\mathcal{D}^\prime$ (denoted ``old/new'' in \autoref{fig:inc}).

We take the results reported in \citet{KishoreWLAW23}, including methods that do not require long-time post-training after adding new documents. These include IncDSI, a constrained optimizer that finds a new embedding for a new document in the embedding space of \emph{an atomic DSI} (\S\ref{sec:gr}); and DPR, a contrastively-trained retriever. \footnote{~Note that these are \emph{not} directly comparable since IncDSI/DPR are based on BERT while HCE is on T5.}

Results are shown in \autoref{fig:inc}. We see from the trends that all methods' performance dropped for new documents as expected (especially when $k$ goes from $10^2$ to $10^4$), but HCE's performance drop for new documents is comparable to IncDSI, a method specifically designed for incremental additions. At $k \in \{10^3, 10^4\}$, HCE performs comparably with IncDSI under R@1 for new documents in $\mathcal{D}^\prime$, and significantly outperforms under R@10.

\section{Conclusion}
We present \emph{hierarchical corpus encoder} (HCE) that jointly learns a dense encoder and a document hierarchy for information retrieval. HCE contrasts positive samples against siblings nodes on the document hierarchy as negative samples, mimicking the training dynamics in hierarchical generative retrieval. HCE is shown to achieve superior performance over a variety of dense encoder and generative retrieval baselines, under both supervised and unsupervised scenarios, demonstrating the effectiveness of jointly learning a document hierarchy.

HCE easily scales to corpora with $>$5M documents. Additionally, as a dense encoder, HCE supports easy addition and removal of documents to an MIPS index without the need for continued training, as is demonstrated in our incrementally updated retrieval experiments.


\section*{Limitations}
\label{sec:limitations}

HCE learns a hierarchy of documents as a by-product. At indexing time, this hierarchy is discarded and encodings are fed to an MIPS index (e.g. FAISS \citep{FAISS}), where product quantization (PQ) methods are commonly employed to reduce the memory of the index. PQ itself uses $k$-means clustering. One might study training an \emph{index-aware} encoder: after training an optimal index is also constructed as a by-product for fast retrieval.

The agglomerative clustering algorithm, although parallelizable on GPUs, has a high time complexity. A divisive hierarchical $k$-means clustering algorithm (that should run in $\sum_{i=1}^{\lceil \log_b N \rceil} b^{i-1} O(\frac{N}{b^{i-1}} \cdot b \cdot n ) \simeq O(Nnb \log N)$ time) that is also parallelizable is desired with very large datasets (e.g. $>$100M documents).


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix

\section{Details of the Contrastive Loss}
\label{app:contrastive}
The contrastive loss employed utilizes both the \emph{in-batch negative trick} and \emph{bidirection contrastive loss}. Mathematically it can be written as
\begin{align}
\label{eq:contrastive-all}
    \calL_{C}^{\leftrightarrow}(B) &= -\frac{1}{|B|}\sum_{i \in B} \underbrace{\left[\log \frac{\exp S(q_i, d_i^+)}{\displaystyle\sum_{j \in B} \exp S(q_i, d_j^+) + \sum_{d_i^- \in D_i^-} \exp S(q_i, d_i^-)}\right.}_\text{query to doc} + \underbrace{\left.\log \frac{\exp S(q_i, d_i^+)}{\displaystyle\sum_{j \in B}\exp S(q_j, d_i^+)}\right]}_\text{doc to query (no neg. samples)}.
\end{align}

\section{Statistics of the Datasets}
\label{app:stat}

Statistics of the datasets used is provided for reference. Note that the ``NQ'' dataset in BEIR is a differently preprocessed version of NaturalQuestions \citep{KwiatkowskiPRCP19} than NQ320k.

\begin{table}[H]
\small
  \centering
    \adjustbox{max width=\linewidth}{
      \begin{tabular}{l|rrr}
        \toprule
        Dataset     & \# Training pairs & \# Test queries & \# docs in corpus \\
        \midrule  
        NQ320k      & 152,144\textsuperscript{\dag} & 7,830 & 109,739 \\
        TriviaQA    & 110,647 & 7,701 & 73,970 \\
        \midrule
        \multicolumn{2}{l}{\textbf{BEIR}} \\
        TREC-COVID  & -       & 50 & 171332 \\
        NFCorpus    & 110,575  & 323 & 3633 \\
        NQ         & -        & 3452 & 2681468 \\
        HotpotQA    & 170,000 & 7,405 & 5,233,329 \\
        FiQA-2018   & 14,166  & 648  & 57,638 \\
        ArguAna     &  -      & 1,406 & 8,674 \\
        Touch'e-2020 & -      & 49 & 382,545 \\
        CQADupStack & -       & 13,145 & 457,199 \\
        Quora       & -       & 10,000 & 522,931 \\
        DBPedia-entity & -    & 400 & 4,635,922 \\
        SciDocs     & -       & 1,000 & 25,657 \\
        FEVER       & -       & 6,666 & 5,416,568 \\
        Climate-FEVER & -     & 1,535 & 5,416,593 \\
        SciFact     & 920     & 300 & 5,183 \\
        \bottomrule
    \end{tabular}
  }
  \vspace{0.2cm}
  \caption{Statistics on various datasets used. ~\textsuperscript{\dag} Negative pairs are discarded.}
  \label{tab:dataset-stats}
\end{table}


\section{Additional Metrics of BEIR}
\label{app:recall}

For zero-shot retrieval in BEIR, prior work also reported recall@100 (R@100). We report the performance of our unsupervised model (HCE-U) here, as compared to various baselines. \GenRet does not report R@100.

\newcolumntype{y}{>{\columncolor{LightCyan}}c}
\begin{table}[H]
\small
    \centering
    \adjustbox{max width=0.95\linewidth}{
        \begin{tabular}{l|c|ccc|cycy}
          \toprule
            \bf Dataset  & BM25 & DPR & ANCE & TAS-B & \multicolumn{2}{c}{T5-\tt base} & \multicolumn{2}{c}{T5-\tt large} \\
            \cmidrule(lr){6-7}\cmidrule(lr){8-9}
                          &      & & & & GTR & HCE-U & GTR & HCE-U \\
          \midrule
            TREC-COVID    & \bf 0.498 & 0.457 & 0.457 & 0.387 & 0.411 & 0.444 & 0.434 & 0.457 \\
            NFCorpus      & 0.250 & 0.208 & 0.232 & 0.280 & 0.275 & 0.305 & 0.298 & \bf 0.334 \\
            NQ            & 0.760 & 0.880 & 0.836 & 0.903 & 0.893 & 0.918 & 0.930 & \bf 0.945 \\
            HotpotQA      & \bf 0.740 & 0.591 & 0.578 & 0.728 & 0.676 & 0.704 & 0.725 & 0.739 \\
            FiQA-2018     & 0.539 & 0.342 & 0.581 & 0.593 & 0.670 & 0.702 & 0.734 & \bf 0.789 \\
            ArguAna       & 0.942 & 0.751 & 0.937 & 0.942 & 0.974 & 0.976 & 0.978 & \bf 0.979 \\
            Touch\'e-2020 & \bf 0.538 & 0.301 & 0.458 & 0.431 & 0.488 & 0.510 & 0.500 & 0.519 \\
            CQADupStack   & 0.606 & 0.403 & 0.579 & 0.622 & 0.221 & 0.251 & 0.234 & \bf 0.265 \\
            Quora         & 0.973 & 0.470 & 0.987 & 0.986 & 0.994 & 0.994 & \bf 0.995 & \bf 0.995 \\
            DBPedia-entity& 0.398 & 0.365 & 0.319 & 0.499 & 0.418 & 0.440 & 0.480 & \bf 0.511 \\
            SciDocs       & 0.356 & 0.360 & 0.269 & 0.335 & 0.319 & 0.384 & 0.354 & \bf 0.429 \\
            FEVER         & 0.931 & 0.840 & 0.900 & 0.937 & 0.923 & 0.935 & 0.941 & \bf 0.947 \\
            Climate-FEVER & 0.436 & 0.427 & 0.445 & 0.534 & 0.522 & 0.565 & 0.552 & \bf 0.582 \\
            SciFact       & 0.908 & 0.727 & 0.816 & 0.891 & 0.860 & 0.904 & 0.894 & \bf 0.932 \\
          \midrule
            Average       & 0.634 & 0.509 & 0.600 & 0.648 & 0.617 & 0.645 & 0.646 & \bf 0.673 \\
          \bottomrule
    \end{tabular}}
    \vspace{0.2cm}
    \caption{Recall@100 for BEIR-14 datasets under unsupervised training.}
    \label{tab:r100_beir}
\end{table}



\end{document}
