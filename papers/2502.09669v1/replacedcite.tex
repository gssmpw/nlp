\section{Related Work}
\vspace{-0.025in}

{\bf INR for scientific visualization.}
Using deep learning techniques for data representation____ has been extensively studied recently. 
One solution uses INR, which inputs spatial coordinates and outputs corresponding voxel values to achieve the generation and reduction of scientific data. 
Typically, INR leverages the MLP architecture to represent volumetric data.
For example, Lu et al.\ ____ compressed a single scalar field by optimizing an INR with weight quantization.
Han and Wang____ proposed CoordNet, a coordinate-based network to tackle diverse data and visualization generation tasks.
Tang and Wang____ presented STSR-INR, leveraging an INR to generate simultaneous spatiotemporal super-resolution for time-varying multivariate volumetric data.
Han et al.\ ____ proposed KD-INR to handle large-scale time-varying volumetric data compression when volumes are only sequentially accessible during training.
Tang and Wang____ designed ECNR to achieve efficient time-varying data compression by combining INR with the Laplacian pyramid for multiscale fitting.
Li and Shen____ leveraged Gaussian distribution to model the probability distribution of an INR network to achieve efficient isosurface extraction.

Besides the vanilla MLP architecture, multiple works integrate grid parameters into INR to achieve efficient encoding and rendering.
%For instance, 
Weiss et al.\ ____ implemented fV-SRN, achieving significant rendering speed gain over____ using a volumetric latent grid.
Wurster et al.\ ____ proposed APMGSRN, which uses multiple spatially adaptive feature grids to represent a large volume.
Xiong et al.\ ____ designed MDSRN to simultaneously reconstruct the data and assess the reconstruction quality in one INR network.
Tang and Wang____ developed StyleRF-VolVis, leveraging a grid-based encoding INR to represent a volume rendering scene that supports various editings.
Yao et al.\ ____ proposed ViSNeRF, utilizing a multidimensional INR representation for visualization synthesis of dynamic scenes, including changes of transfer functions, isovalues, timesteps, or simulation parameters. 
Gu et al.\ ____ and Lu et al.\ ____ presented NeRVI and FCNR, respectively, utilizing INRs for the effective compression of a large collection of visualization images. 
%
Unlike existing works that mainly focus on network architecture design, this paper aims to develop a pretraining strategy for optimizing the initial parameters of an INR network to enhance the representation generalizability.

{\bf Meta-learning.}
Meta-learning is a deep learning technique primarily aiming for few-shot learning____.
Numerous recent studies have explored using it to optimize the initialization of neural networks, enabling them to adapt to new tasks with just a few steps of gradient descent.
For instance, 
Sitzmann et al.\ ____ leveraged meta-learners to generalize INR across shapes.
Tancik et al.\ ____ employed meta-learning to initialize INR network parameters according to the underlying class of represented signals.
Emilien et al.\ ____ proposed COIN++, a neural compression framework that supports encoding various data modalities with a meta-learned base network.
Similar to these works, we develop Meta-INR based on existing meta-learning algorithms____.
However, we focus on applying meta-learning in INR for volume data representation, setting it apart from existing studies.