\vspace{-0.075in}
\section{Meta-INR}
\vspace{-0.025in}

%\subsection{Overview}
Meta-INR is a pretraining approach designed to optimize the initial parameters of an INR network for efficient finetuning on unseen volumes.
The training pipeline of Meta-INR consists of two sequential stages: {\em meta-pretraining} and {\em volume-specific finetuning}. 
\begin{myitemize}
\vspace{-0.05in}
    \item The {\em meta-pretraining} stage optimizes a meta-model on a sparse subsampled volumetric dataset, utilizing less than 1\% of the original data, to learn the initial parameters of an INR network that supports rapid adaptation to other unseen volumes within the dataset. 
    \item The {\em volume-specific finetuning} stage finetunes the initial parameters of the meta-model on a specific volume, resulting in a volume-specific adapted INR for high-fidelity volume reconstruction.
\vspace{-0.05in}
\end{myitemize}
The adapted INR shares the same network architecture, $\Phi: \mathbb{R}^3 \rightarrow \mathbb{R}$ as the meta-model, which maps a 3D coordinate to the corresponding voxel value.
Let $\theta_{m}$ denote the parameters of the meta-model.
For a volumetric dataset $\mathbf{D} = \{ d_1, d_2, \dots, d_i, \dots, d_T \}$ that contains $T$ timesteps or ensembles, one volume $d_i$ consists of a set of coordinate-value pairs $(\mathbf{C}_i, \mathbf{V}_i)$, where $\mathbf{C} = \{ (x_1, y_1, z_1), (x_2, y_1, z_1), \dots \}$ is a set of spatial coordinates and $\mathbf{V} = \{ v_1, v_2, \dots \}$ is the corresponding voxel values at these positions.
After meta-pretraining, we finetune $\theta_{m}$ on each set of coordinate-value pairs in $\mathbf{D}$ independently, resulting in a series of volume-specific adapted INRs with parameters $\{ \theta_{1}, \theta_{2}, \dots, \theta_{T} \}$ that can represent each volume within the temporal or ensemble sequence.

%--------------------------------------
\begin{algorithm}[htb]
    \caption{Meta-Pretraining}
    \label{alg:meta-pretraining}
        \KwIn{Volumetric dataset $\mathbf{D}$ with $T$ timesteps or ensembles, spatial and temporal downsampling intervals $\lambda_s$ and $\lambda_t$, inner and outer loop learning rates $\alpha$ and $\beta$, number of inner-loop steps $K$.}
        \KwOut{Optimized meta-model parameters $\theta_{m}$.}
        Subsample $\mathbf{D}$ under $\lambda_s$ and $\lambda_t$ to obtain downsampled dataset $\hat{\mathbf{D}} = \{ \hat{d}_1, \dots, \hat{d}_{T'} \}$ with $T'$ timesteps or ensembles
        
        Randomly initialize meta-model parameters $\theta_{m}$
        
        \While{not done}{
            Initialize gradients: $\nabla \theta_{m} = 0$\;
            \For{all $\hat{d}_i$}{
                Clone parameters: $\theta' \leftarrow \theta_{m}$\;
                Randomly sample batches of coordinate-value pairs $(\mathbf{C}_i, \mathbf{V}_i)$ from $\hat{d}_i$\;
                \For{$k = 1 \text{ to } K$}{
                    Compute loss: $\mathcal{L} \leftarrow \mse(\Phi(\mathbf{C}_i; \theta'), \mathbf{V}_i)$\;
                    $\theta' \leftarrow \theta' - \alpha \nabla_{\theta'} \mathcal{L}$\;
                }
                $\nabla \theta_{m} \leftarrow \nabla \theta_{m} + (\theta - \theta')$\;
            }
            Update meta-model parameters: $\theta_{m} \leftarrow \theta_{m} - \beta \frac{\nabla \theta_{m}}{T'}$\;
        }
        \Return $\theta_{m}$
\end{algorithm}
%--------------------------------------

\vspace{-0.05in}
\subsection{Meta-Pretraining}
\label{subsec:mp}

The meta-pretraining stage optimizes the parameters of $\theta_{m}$, serving as the initial parameters in the volume-specific finetuning stage. 
Meta-pretraining is data efficient, requiring only a small portion of the data to derive sufficiently generalizable $\theta_{m}$.
In this paper, we achieve this by spatiotemporally subsampling the original dataset $\mathbf{D}$.
Specifically, we subsample $\mathbf{D}$ using an interval of $\lambda_s$ on each spatial dimension and $\lambda_t$ on the temporal dimension, resulting in a downsampled dataset $\hat{\mathbf{D}}$.

% We empirically set $\lambda_s=4$ and $\lambda_t=2$ and find this setting works for most datasets without significant accuracy loss.
The spatial subsampling interval $\lambda_s=4$ and temporal subsampling interval $\lambda_t=2$ are chosen empirically, balancing pretraining efficiency and quality of the prior. This configuration retains structural patterns for most datasets without significant accuracy loss.
It further implies that only $\frac{1}{(4^3\times2)}\times100\%\thickapprox0.78\%$ original data samples are used for pretraining.

Then, we consider optimizing $\theta_{m}$ as a meta-learning problem and propose to leverage a MAML-like algorithm~\cite{Finn-ICML17}.
In particular, we randomly initialize the meta-model parameters and iteratively update $\theta_{m}$ through a nested inner and outer loop. In the inner loop, for each subsampled volumetric dataset $\hat{d}_i$ in $\hat{\mathbf{D}}$, we finetune a cloned set of parameters $\theta'$ using $K$ gradient steps on randomly sampled batches of coordinate-value pairs $(\mathbf{C}_i, \mathbf{V}_i)$. The inner-loop loss is computed as the mean squared error (MSE) between the predicted and ground-truth (GT) voxel values. After processing all volumes in $\hat{\mathbf{D}}$, the outer loop accumulates the gradients from the inner loop and updates $\theta_{m}$ using the average gradient across the volumes. The optimization continues until $\theta_{m}$ converges.

\vspace{-0.05in}
\subsection{Volume-Specific Finetuning}
\label{subsec:vsf}

Once the meta-pretraining stage finishes, we utilize $\theta_{m}$ as initial parameters to finetune each adapted INR on a specific volume in $\mathbf{D}$.
%
The finetuning process mirrors the inner-loop adaptation during meta-pretraining, where the pre-trained initial parameters $\theta_{m}$ are taken and updated in $K$ gradient steps. 
The only difference is that we utilize all available data points in this stage instead of a subsampled version of the volumetric dataset.
This ensures that the finetuned parameters follow a more accurate gradient descent direction from $\theta_{m}$, leading to improved reconstruction accuracy.
After the volume-specific finetuning stage, we can obtain a series of volume-wise adapted INRs, each representing a specific volume within the target time-varying or ensemble sequence.
