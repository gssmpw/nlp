\vspace{-0.075in}
\section{Conclusions and future work}
\vspace{-0.025in}

We have presented Meta-INR, a pretraining method designed to optimize a meta-model that can adapt to unseen volume data efficiently.
%
The generalizability of the meta-model allows for fast convergence during volume-specific finetuning while retaining the model interpretability within its parameters. 
The evaluation of various volumetric data representation tasks demonstrates better quantitative and qualitative performance of the meta-pretraining than other training strategies.  

For future work, we would like to explore continual learning to improve the capabilities of the meta-model for handling more complex time-varying volume data with significantly larger timesteps and variations.
Moreover, we want to investigate meta-pretraining on grid-based INRs such as fV-SRN~\cite{Weiss-CGF22} or APMGSRN~\cite{Wurster-TVCG24}.
Finally, we plan to incorporate transfer learning techniques to learn potentially more challenging variations across variables for multivariate datasets, allowing a meta-model trained on one variable sequence to be effectively used in finetuning another.
