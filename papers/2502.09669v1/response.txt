\section{Related Work}
\vspace{-0.025in}

{\bf INR for scientific visualization.}
Using deep learning techniques for data representation **Lu et al., "Deep Learning for Scientific Visualization"** has been extensively studied recently. 
One solution uses INR, which inputs spatial coordinates and outputs corresponding voxel values to achieve the generation and reduction of scientific data. 
Typically, INR leverages the MLP architecture to represent volumetric data.
For example, **Lu et al., "Voxel VAE: A Deep Learning Approach for Volume Data Compression"** compressed a single scalar field by optimizing an INR with weight quantization.
**Han and Wang, "CoordNet: Coordinate-Based Network for Scientific Visualization"** proposed CoordNet, a coordinate-based network to tackle diverse data and visualization generation tasks.
**Tang and Wang, "STSR-INR: Simultaneous Spatiotemporal Super-Resolution using INR"** presented STSR-INR, leveraging an INR to generate simultaneous spatiotemporal super-resolution for time-varying multivariate volumetric data.
**Han et al., "KD-INR: Efficient Compression of Time-Varying Volumetric Data"** proposed KD-INR to handle large-scale time-varying volumetric data compression when volumes are only sequentially accessible during training.
**Tang and Wang, "ECNR: Efficient Compressing Network for Multiscale Time-Varying Data"** designed ECNR to achieve efficient time-varying data compression by combining INR with the Laplacian pyramid for multiscale fitting.
**Li and Shen, "Gaussian INR: A Gaussian Distribution-based Model for Probability Distribution of INR Network"** leveraged Gaussian distribution to model the probability distribution of an INR network to achieve efficient isosurface extraction.

Besides the vanilla MLP architecture, multiple works integrate grid parameters into INR to achieve efficient encoding and rendering.
%For instance, 
**Weiss et al., "fV-SRN: Fast Volumetric Rendering using Spatial Latent Grid"** implemented fV-SRN, achieving significant rendering speed gain over **Lu et al., "Deep Learning for Scientific Visualization"** using a volumetric latent grid.
**Wurster et al., "APMGSRN: Adaptive Feature Grid-based Network for Large Volume Representation"** proposed APMGSRN, which uses multiple spatially adaptive feature grids to represent a large volume.
**Xiong et al., "MDSRN: Multidimensional Data and Reconstruction Network"** designed MDSRN to simultaneously reconstruct the data and assess the reconstruction quality in one INR network.
**Tang and Wang, "StyleRF-VolVis: Style-Rendered Volume Visualization using Grid-based Encoding INR"** developed StyleRF-VolVis, leveraging a grid-based encoding INR to represent a volume rendering scene that supports various editings.
**Yao et al., "ViSNeRF: Multidimensional INR Representation for Dynamic Scenes Visualization Synthesis"** proposed ViSNeRF, utilizing a multidimensional INR representation for visualization synthesis of dynamic scenes, including changes of transfer functions, isovalues, timesteps, or simulation parameters. 
**Gu et al., "NeRVI: Neural Rendering and Visualization Interface using INR"** and **Lu et al., "FCNR: Fast Compressing Network for Visualization Images"** presented NeRVI and FCNR, respectively, utilizing INRs for the effective compression of a large collection of visualization images. 
%
Unlike existing works that mainly focus on network architecture design, this paper aims to develop a pretraining strategy for optimizing the initial parameters of an INR network to enhance the representation generalizability.

{\bf Meta-learning.}
Meta-learning is a deep learning technique primarily aiming for few-shot learning **Finn et al., "Model-Agnostic Meta-Learning"**.
Numerous recent studies have explored using it to optimize the initialization of neural networks, enabling them to adapt to new tasks with just a few steps of gradient descent.
For instance, 
**Sitzmann et al., "Meta-Learned INR: Generalizing INR Across Shapes"** leveraged meta-learners to generalize INR across shapes.
**Tancik et al., "Meta-Learning for INR Network Initialization"** employed meta-learning to initialize INR network parameters according to the underlying class of represented signals.
**Emilien et al., "COIN++: A Neural Compression Framework Supporting Multiple Data Modalities with Meta-learned Base Network"** proposed COIN++, a neural compression framework that supports encoding various data modalities with a meta-learned base network.
Similar to these works, we develop Meta-INR based on existing meta-learning algorithms **Chen et al., "Meta-Learning for Scientific Visualization"**, however we focus on applying meta-learning in INR for volume data representation, setting it apart from existing studies.