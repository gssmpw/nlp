\section{Additional Background}

\subsection{Model Access Levels} In Figure~\ref{fig:model_access}, we show a schematic overview of different model access levels discussed in Section \ref{sec:related-word}.

\begin{figure*}[ht]
    \centering
    
    \begin{tikzpicture}[]

\node[draw, ultra thick, minimum width=\linewidth, minimum height=4cm, anchor=north west] (border_black) at (current page.north west) {};
\node[anchor=north west, draw, ultra thick, fill=black, text=white] (title) at (border_black.north west) {Black box};

\node[below=of title, yshift=3mm] (input) {Input};
\node[right=of input, draw, thick] (preprocess) {Preprocess};
\node[right=of preprocess, draw, thick, fill=black, text=white] (model_s) {$\mathcal{M}_S$};
\node[right=of model_s] (output_s) {Output$_S$};
\node[right=of output_s, draw, thick] (postprocess) {Postprocess};
\node[right=of postprocess, draw, thick] (deferral) {Deferral};

\node[below=of model_s, draw, thick, fill=black, text=white] (model_l) {$\mathcal{M}_B$};
\node[right=of model_l] (output_l) {Output$_L$};

\draw[dotted, thick] ($(border_black.north west) + (0, -2.25cm)$) -- ($(border_black.north east) + (0,-2.25cm)$);
\node[anchor=north west, yshift=1.25cm, xshift=-1.75cm] (title) at (border_black.south east) {Remote};
\node[anchor=north west, yshift=-1.25cm, xshift=-1.75cm] (title) at (border_black.north east) {Local};


\draw[->, thick] (input) -- (preprocess);
\draw[->, thick] (preprocess) -- (model_s);
\draw[->, thick] (model_s) -- (output_s);
\draw[->, thick] (output_s) -- (postprocess);
\draw[->, thick] (postprocess) -- (deferral);

\draw[->, thick] (input) |- (model_l);
\draw[->, thick] (model_l) -- (output_l);

\draw[->, ultra thick] (deferral) |- node[anchor=south east] {Yes} (output_l);

\draw[->, ultra thick] (deferral) |- node[anchor=north east] {No} ++(0, 1) -| (output_s);

\end{tikzpicture}
    \vskip10pt
    \begin{tikzpicture}[]

\node[draw, ultra thick, minimum width=\linewidth, minimum height=4cm, anchor=north west] (border_black) at (current page.north west) {};
\node[anchor=north west, draw, ultra thick, fill=gray, text=white] (title) at (border_black.north west) {Gray box};

\node[below=of title, yshift=3mm] (input) {Input};
\node[right=of input, draw, thick, fill=gray, text=white] (model_s) {$\mathcal{M}_S$};
\node[right=of model_s] (logits) {Logits};
\node[right=of logits, draw, thick] (decode) {Decode};
\node[right=of decode] (output_s) {Output$_S$};
\node[right=of output_s, draw, thick] (deferral) {Deferral};

\node[below=of model_s, draw, thick, fill=black, text=white] (model_l) {$\mathcal{M}_B$};
\node[below=of output_s] (output_l) {Output$_L$};

\draw[dotted, thick] ($(border_black.north west) + (0, -2.25cm)$) -- ($(border_black.north east) + (0,-2.25cm)$);

\node[anchor=north west, yshift=1.25cm, xshift=-1.75cm] (title) at (border_black.south east) {Remote};
\node[anchor=north west, yshift=-1.25cm, xshift=-1.75cm] (title) at (border_black.north east) {Local};


\draw[->, thick] (input) -- (model_s);
\draw[->, thick] (model_s) -- (logits);
\draw[->, thick] (logits) -- (decode);
\draw[->, thick] (decode) -- (output_s);
\draw[->, ultra thick] (deferral) -- node[anchor=south] {No} (output_s);

\draw[->, thick] (input) |- (model_l);
\draw[->, thick] (model_l) -- (output_l);

\draw[->, ultra thick] (deferral) |- node[anchor=south east] {Yes} (output_l);

\draw[->, thick] (logits) |- ++(0, 1) -| (deferral);

\end{tikzpicture}

\vskip10pt
    \begin{tikzpicture}[]

\node[draw, ultra thick, minimum width=\linewidth, minimum height=4cm, anchor=north west] (border_black) at (current page.north west) {};
\node[anchor=north west, draw, ultra thick] (title) at (border_black.north west) {White box};

\node[below=of title, yshift=3mm] (input) {Input};
\node[right=of input, draw, thick] (model_s) {$\mathcal{M}_S$};
\node[right=of model_s, draw] (tuning) {Tuning};
\node[right=of tuning] (logits) {Logits};
\node[right=of logits, draw, thick] (decode) {Decode};
\node[right=of decode] (output_s) {Output$_S$};
\node[right=of output_s, draw, thick] (deferral) {Deferral};

\node[below=of model_s, draw, thick, fill=black, text=white] (model_l) {$\mathcal{M}_B$};
\node[below=of output_s] (output_l) {Output$_L$};

\draw[->, thick, dashed] (input) |- (model_l);
\draw[->, thick, dashed] (input) -- (model_s);
\draw[->, thick, dashed] (model_s) -- (tuning);
\draw[->, thick, dashed] (tuning) |- ++(0, 1) -| (model_s);
\draw[->, thick, dashed] (model_l) -| (tuning);

\draw[->, thick] (input) to [out=300, in=150] (model_l);
\draw[->, thick] (input) to [out=30, in=150] (model_s);
\draw[->, thick] (model_s) to [out=30, in=150] (logits);
\draw[->, thick] (model_l) to [out=15, in=180] (output_l);


\draw[->, thick] (logits) -- (decode);
\draw[->, thick] (decode) -- (output_s);
\draw[->, ultra thick] (deferral) -- node[anchor=south] {No} (output_s);
\draw[->, thick] (logits) |- ++(0, 1) -| (deferral);
\draw[->, ultra thick] (deferral) |- node[anchor=south east] {Yes} (output_l);

\draw[dotted, thick] ($(border_black.north west) + (0, -2.25cm)$) -- ($(border_black.north east) + (0,-2.25cm)$);
\node[anchor=north west, yshift=1.25cm, xshift=-1.75cm] (title) at (border_black.south east) {Remote};
\node[anchor=north west, yshift=-1.25cm, xshift=-1.75cm] (title) at (border_black.north east) {Local};

\end{tikzpicture}
    \caption{\textbf{An overview of different uncertainty quantification strategies depending on model access level}.}
    \label{fig:model_access}
\end{figure*}

\subsection{Ideal Deferral Curve}
\label{app:ideal_deferral}

We present the functional form of the \emph{ideal deferral} curve, denoted
\(\mathrm{acc}_{\mathrm{ideal}}(r)\), for a small (student) model \(\mathcal{M}_S\) and a large (teacher) model \(\mathcal{M}_L\). Recall that \(r \in [0,1]\) denotes the deferral ratio, i.e., the fraction of inputs that \(\mathcal{M}_S\) “defers” to \(\mathcal{M}_L\). Let $p_s = \text{acc}(\mathcal{M}_S)$, and $p_l = \text{acc}(\mathcal{M}_L)$ with \(0 \le p_s \le p_l \le 1\). Our goal is to describe the maximum achievable joint accuracy if exactly a fraction \(r\) of the data is deferred to the large model.

\paragraph{Intuition and Setup}
Since \(\mathcal{M}_S\) achieves accuracy \(p_s\), it misclassifies a fraction \((1 - p_s)\) of the inputs. In an \emph{ideal} scenario, we defer exactly those inputs that \(\mathcal{M}_S\) is going to misclassify. Because \(\mathcal{M}_L\) is more accurate (\(p_l \ge p_s\)) every example misclassified by \(\mathcal{M}_S\) benefits from being passed to \(\mathcal{M}_L\).

\begin{itemize}
    \item \textbf{Case 1:} \(r \le (1 - p_s)\).\\
    We can use our entire deferral “budget” \(r\) to cover only those inputs \(\mathcal{M}_S\) would get wrong. Hence, deferring a fraction \(r\) of the data (all from \(\mathcal{M}_S\)'s mistakes) raises the overall accuracy by substituting \(\mathcal{M}_S\)'s errors with \(\mathcal{M}_L\)'s accuracy \(p_l\) on that fraction.
    \item \textbf{Case 2:} \(r > (1 - p_s)\).\\
    We have enough capacity to defer \emph{all} of \(\mathcal{M}_S\)'s mistakes, so the joint accuracy saturates at \(p_l\). Deferring \emph{additional} examples (which \(\mathcal{M}_S\) would have classified correctly) will not improve the overall accuracy beyond \(p_l\).
\end{itemize}

\paragraph{Piecewise Functional Form}
Thus, the \emph{ideal deferral} curve can be expressed as:
\begin{equation}
\mathrm{acc}_{\mathrm{ideal}}(r) \;=\;
\begin{cases}
p_s + \dfrac{p_l - p_s}{\,1 - p_s\,} \; r,
& \quad 0 \;\le\; r \;\le\; (1 - p_s), \\[1em]
p_l,
& \quad (1 - p_s) \;<\; r \;\le\; 1.
\end{cases}
\end{equation}
When \(0 \le r \le (1 - p_s)\), the overall accuracy grows linearly from \(\mathrm{acc}_{\mathrm{ideal}}(0) = p_s\) to \(\mathrm{acc}_{\mathrm{ideal}}(1-p_s) = p_l\). Past \(r = (1 - p_s)\), it remains constant at \(p_l\). 

Figure~\ref{fig:metrics_illustration} (b) in the main paper plots this ideal deferral curve (green line). It serves as an upper bound on how effective any real deferral strategy can be. In contrast, a purely random deferral strategy produces a linear interpolation (the red line), which is strictly below the ideal curve for most \(r\). Consequently, the difference
\(\mathrm{acc}_{\mathrm{ideal}}(r) - \mathrm{acc}_{\mathrm{rand}}(r)\)
represents the \emph{maximum possible} gain one can achieve by carefully selecting which examples to defer rather than choosing them at random.

\paragraph{Summary} We summarize the key take-aways below:
\begin{itemize}
    \item \textbf{Ideal Deferral Routes All Mistakes:} Only the inputs misclassified by \(\mathcal{M}_S\) get deferred, guaranteeing the highest possible joint accuracy at each deferral level \(r\).
    \item \textbf{Piecewise Definition:} Accuracy increases linearly from \(p_s\) to \(p_l\) over the interval \(r \in [0,\, (1 - p_s)]\), then remains at \(p_l\).
    \item \textbf{Upper Bound on Realized Deferral:} No actual strategy can exceed this ideal curve, as it assumes perfect knowledge of which specific inputs \(\mathcal{M}_S\) would misclassify.
\end{itemize}



\section{Additional Experimental Details}

\subsection{CNN Used in Image Classification Experiments}

Below we include a representation of the \texttt{SmallCNN} model used as \smallmodel in image classification experiments discussed in Section \ref{sec:class_exp}:

\begin{lstlisting}[]
SmallCNN(
  (features): Sequential(
    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU(inplace=True)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=2048, out_features=64, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=64, out_features=10, bias=True)
  )
)
\end{lstlisting}

\subsection{Reduce Confidence and Answer ``N'' Baselines}
\label{app:uncertainty_appendix}

In addition to the baseline model in Section \ref{sec:lang_exp} (i.e., a model that was not fine-tuned with our specialized \(\mathcal{L}_{\text{def}}\) loss but from which we still compute predictive entropy as a deferral signal), we also examine two additional methods aimed at eliciting uncertainty from the model directly via prompt modifications. Both methods are \textit{black box} approaches that only rely on a query interface to the model via prompt injection, and we provide their implementation details below.

\paragraph{Reduce Confidence.}
In this setting, we modify the original prompt \(\mathbf{x}\) by appending an additional instruction \(\mathbf{x}'\) that encourages the model to respond with lower confidence when it is uncertain: $\mathbf{x} \;\leftarrow\; \mathbf{x} \;\big\vert\; \mathbf{x}'$.
For instance, the instruction we add is:
\[
    \mathbf{x}' = \texttt{``Respond with low confidence if you are uncertain.''}
\]
We treat this appended text as a hint to the model to self-regulate its confidence when producing an answer. This is similar in spirit to other black box approaches such as confidence quantification, rejection awareness, remote model notice, and self-critiquing. Although \citet{xiong2024can} show that large language models can express aspects of their confidence via prompting, our experiments indicate that simply prompting the model to express lower confidence does not reliably improve the separation of correct versus incorrect predictions, nor does it offer advantages in a deferral setting. These findings are in line with those reported in \cite{kadavath2022language}.

\paragraph{Answer ``N.''}
We also consider an alternate prompt modification, in which the appended instruction is:
\[
    \mathbf{x}' = \texttt{``Respond with `N' if you are uncertain.''}
\]
This approach explicitly instructs the model to produce a special ``N'' token to indicate uncertainty or lack of confidence. The intuition is that by introducing a designated ``uncertain'' response, one might isolate uncertain cases for deferral. However, our results in Section \ref{sec:lang_exp} similarly show that the model’s ability to follow this instruction is inconsistent and does not substantially improve performance as a deferral model. The model often remains overconfident and fails to produce ``N'' in cases where it is in fact incorrect.

\subsection{Additional metrics}
\label{app:add_metrics}

In addition to the metrics outlined in Section~\ref{sec:experiments}, we also consider the \textbf{Area Under the Receiver Operating Characteristic Curve} (AUROC) ($s_\text{AUROC}$). The AUROC quantifies the model's ability to discriminate between correctly and incorrectly classified data points by evaluating the trade-off between the True Positive Rate (TPR) and the False Positive Rate (FPR) across various confidence thresholds $\tau$. Formally, given the confidence sets $\mathcal{C}_\text{corr}$ and $\mathcal{C}_\text{incorr}$, the AUROC is defined as
    \begin{equation}
        s_\text{AUROC} = \int_{0}^{1} \text{TPR}(\tau) \, \mathrm{d}\text{FPR}(\tau),
    \end{equation}
    where for each threshold $\tau \in [0,1]$ we compute $\text{TPR}(\tau) = \frac{|\{c \in \mathcal{C}_\text{corr} \mid c \geq \tau\}|}{|\mathcal{C}_\text{corr}|}$ and $\text{FPR}(\tau) = \frac{|\{c \in \mathcal{C}_\text{incorr} \mid c \geq \tau\}|}{|\mathcal{C}_\text{incorr}|}$. Note that $s_\text{AUROC} = 1$ indicates perfect separability and  $s_\text{AUROC} = 0.5$ corresponds to a random guessing baseline.

\subsection{Factuality Scoring}
\label{app:fac_scoring}

Factuality scoring with Gemini for a reference caption $r$ and a candidate caption $c$ is computed as follows:
\begin{enumerate}
    \item \textbf{Compute the log-likelihoods.} Let $\ell_{\text{Same}}(c, r)$ be the log-likelihood that the model outputs ``Same'' for a given candidate caption $c$ and reference $r$, and let $\ell_{\text{Diff}}(c, r)$ be the log-likelihood that the model outputs ``Different''.

    \item \textbf{Apply softmax.} To convert these log-likelihoods into probabilities, we exponentiate and normalize:
    \[
        p(\text{Same} \mid c, r) = \frac{\exp\!\bigl(\ell_{\text{Same}}(c, r)\bigr)}
        {\exp\!\bigl(\ell_{\text{Same}}(c, r)\bigr) + \exp\!\bigl(\ell_{\text{Diff}}(c, r)\bigr)},
    \]
    \[
        p(\text{Diff} \mid c, r) = \frac{\exp\!\bigl(\ell_{\text{Diff}}(c, r)\bigr)}
        {\exp\!\bigl(\ell_{\text{Same}}(c, r)\bigr) + \exp\!\bigl(\ell_{\text{Diff}}(c, r)\bigr)}.
    \]

    \item \textbf{Interpret the probability.} The value $p(\text{Same} \mid c, r)$ is then taken as the factual alignment score, expressing how confidently the model believes the candidate caption is factually aligned with the reference.
\end{enumerate}

\subsection{Additional Experimental Results}
\label{app:additional_exp_class}

In this section, we provide additional experimental results further supporting our findings reported for image classification experiments in Section \ref{sec:class_exp}. In particular, we show ROC curves in Figure \ref{fig:roc_class} and distributional overlap in Figure \ref{fig:dist_overlap_class}, both demonstrating that \loss increases the separation of correct/incorrect confidence scores. Similarly, the deferral curves in Figure \ref{fig:deferral_class} clearly show that \loss successfully pushed the realized deferral (black line) closer to the ideal one (marked with dashed upper line). Lastly, we report the joint accuracy of \smallmodel across varying $\alpha$ parameter in Figure \ref{fig:joint_acc_clss}. As discussed in Section \ref{sec:experiments}, we observe that \smallmodel's accuracy generally decreases with $\alpha \rightarrow 0$.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/rocs.pdf}
    \vspace{-20pt}
    \caption{\textbf{ROC curves for image classification experiments}. Each figure shows the ROC curves for each of the datasets considered in Section \ref{sec:class_exp}. We observe that \loss consistently increases separation of correct and incorrect confidence scores across varying $\alpha$ (colored curves) compared to the baseline (denoted with black dashed line).}
    \label{fig:roc_class}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/dist.pdf}
    \vspace{-20pt}
    \caption{\textbf{Distributional overlap for image classification experiments}. Left-most column shows the results obtained using the untuned baseline, while the remaining columns correspond to the results obtained using \loss with decreasing $\alpha$ values. Rows correspond to the datasets considered in Section \ref{sec:class_exp}. We see that \loss increases separation of correct and incorrect confidence scores compared to the baseline.}
    \label{fig:dist_overlap_class}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figs/deferrals.pdf}
    \vspace{-20pt}
    \caption{\textbf{Deferral curves for image classification experiments}. Left-most column shows the results obtained using the untuned baseline, while the remaining columns correspond to the results obtained using \loss with decreasing $\alpha$ values. Rows correspond to the datasets considered in Section \ref{sec:class_exp} The results show that \loss brings the realized deferral (black line) closer to the ideal deferral (dashed upper line).}
    \label{fig:deferral_class}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/deferrals_2.pdf}
    \vspace{-20pt}
    \caption{\textbf{Joint accuracy across different levels of $\alpha$}. For varying fixed deferral ratios, we observe that the accuracy of \smallmodel generally decreases as $\alpha \rightarrow 0$.}
    \label{fig:joint_acc_clss}
\end{figure*}
