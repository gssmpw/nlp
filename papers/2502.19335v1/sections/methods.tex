\section{Method}

\subsection{Overview \& Setup}

Our framework consists of a large, highly capable model \textbf{\bigmodel} and a smaller, resource-efficient model \textbf{\smallmodel}. We assume that $S \in \mathbb{N}$ and $L \in \mathbb{N}$ represent the parameter count of each model with $S \ll L$. Both models can either function as classifiers (i.e., $\mathcal{M}: \mathbb{R}^D \rightarrow [C]$ with $\mathbb{R}^D$ denoting the input space and $C$ the number of total classes), or (multi-modal) sequence models (i.e., $\mathcal{M}: \mathbb{R}^D \rightarrow [V]^{T}$ where $V$ is the vocabulary and $T$ is the sequence length). We include experiments on all of these model classes in Section~\ref{sec:experiments}. Furthermore, we do not require a shared model family to be deployed on both \smallmodel and \bigmodel; for example, \smallmodel could be a custom convolutional neural network optimized for efficient inference and \bigmodel a vision transformer~\citep{dosovitskiy2020image}. The primary objective is to design a deferral mechanism that enables \smallmodel to decide when to return its predictions without the assistance of \bigmodel and when to instead defer to it.

\looseness=-1
Deferral decisions are made using signals derived from the small model \smallmodel as this approach is typically more cost-effective than employing a separate routing mechanism~\citep{teerapittayanon2016branchynet}. Approaches that involve querying the large model \bigmodel to assist in making deferral decisions at test time are excluded from our setup. Such methods --- common in domains like LLMs --- are counterproductive to our goal since querying \bigmodel defeats the purpose of making a deferral decision in the first place?. Examples of these inapplicable methods include collaborative LLM frameworks~\citep{mielke2022reducing} and techniques that rely on semantic entropy for uncertainty estimation~\citep{kuhn2023semantic}. As part of our setup, we assume that \smallmodel is strictly less capable than \bigmodel --- a realistic scenario in practice supported by scaling laws~\citep{kaplan2020scaling}. Under this assumption, mistakes made by \bigmodel are also made by \smallmodel; however, \smallmodel may make additional errors that \bigmodel would avoid. This reflects the general observation that larger models tend to outperform smaller models across a wide range of tasks.

As discussed in Section~\ref{sec:related-word}, the choice of deferral strategy often depends on the level of access available to \smallmodel. We assume white box access with full access to \smallmodel's internals. As such, deferral mechanisms can be directly integrated into the model's architecture and parameters. This involves fine-tuning \smallmodel to predict deferral decisions or to incorporate rejection mechanisms within its predictive process. Our work falls into this category as it proposes a new loss function to fine-tune \smallmodel. 

Our goal is to train a small model that can effectively distinguish between correct and incorrect predictions. While many past works have considered the question of whether it is possible to find proxy measures for prediction correctness, the central question we ask is:
\begin{center}
\textbf{Can we \emph{optimize} the small model \smallmodel to separate correct from incorrect predictions?}
\end{center}
\noindent We show that this is indeed achievable through a carefully designed fine-tuning stage that does not require any architectural modifications. This ensures that the ability to separate correct from incorrect decisions is integrated seamlessly into \smallmodel's existing structure.


\subsection{Confidence-Tuning for Deferral}

\begin{figure}
    \centering
    \resizebox{\linewidth}{!}{
    \input{figs/loss}
    }
    \vspace{-15pt}
    \caption{\textbf{Overview of \loss}: We want correctly predicted samples maintain their current prediction by ensuring that cross entropy is decreased (top, green). At the same time, we want incorrectly predicted samples to yield a uniform confidence across all classes, leading to a low overall confidence score (bottom, red).}
    \label{fig:opt_goal}
\end{figure}

\textbf{Stage 1: Standard Training.} We begin with a \smallmodel that has already been trained on the tasks it is intended to perform upon deployment. However, due to its limited capacity, \smallmodel cannot achieve the performance levels of \bigmodel. Importantly, we make no assumptions about the training process of \smallmodel—whether it was trained from scratch without supervision from an external model or through a distillation approach.

\sloppy
\textbf{Stage 2: Correctness-Aware Finetuning with \loss.} Next, we introduce a correctness-aware loss, dubbed \loss, to fine-tune \smallmodel for improved confidence calibration. Specifically, the model is trained to make correct predictions with high confidence while reducing the confidence of incorrect predictions (see Figure~\ref{fig:opt_goal}). This loss can either rely on true labels or utilize the outputs of \bigmodel with soft probabilities as targets. 


For a standard classification model, the calibration loss is defined as the following hybrid loss
\begin{align}
\mathcal{L} &= \alpha \mathcal{L}_\text{corr} + (1 - \alpha) \mathcal{L}_\text{incorr} \\
\mathcal{L}_\text{corr} &= \frac{1}{N} \sum_{i=1}^{N} \mathds{1}\{ y_i = \hat{y}_i \} \text{CE}(p_i(\mathbf{x}_i), y_i) \\
\mathcal{L}_\text{incorr} &= \frac{1}{N} \sum_{i=1}^{N} \mathds{1}\{ y_i \neq \hat{y}_i \} \text{KL}\left(p_i(\mathbf{x}_i) \parallel \mathcal{U}\right)
\end{align}
where  \( y_i \) and \( \hat{y}_i \) are the true and predicted labels for $\mathbf{x}_i$, respectively, \( p_i \) is the predicted probability distribution of \smallmodel over classes, \( \mathcal{U} \) represents the uniform distribution over all classes, \( N \) denotes the number samples in the current batch, \( \alpha \in (0, 1) \) is a tunable hyperparameter controlling the emphasis between correct and incorrect predictions, and the cross-entropy function and KL divergence are defined as \( \text{CE}(p, y) = -\sum_{c} y_c \log p_c \) and \( \text{KL}(p \parallel q) = \sum_{c} p_c \log ( \frac{p_c}{q_c}) \), respectively. We note that a similar loss has previously been proposed in Outlier Exposure (OE)~\citep{hendrycks2018deep} for out-of-distribution (OOD) sample detection. Here, the goal is to make sure that OOD examples are assigned low confidence scores by tuning the confidence on a auxiliary outlier dataset. However, to the best of our knowledge, this idea has not previously been used to improve deferral performance of a smaller model in a cascading chain.

We emphasize that the trade-off parameter $\alpha$ plays a critical role as part of this optimization setup as it directly influences model utility and deferral performance. A lower value of \(\alpha\) emphasizes reducing confidence in incorrect predictions by pushing them closer to the uniform distribution, making the model more cautious in regions where it may make mistakes. Conversely, a higher value of \(\alpha\) encourages the model to increase its confidence on correct predictions, sharpening its decision boundaries and enhancing accuracy where it is already performing well. Thus, \(\alpha\) serves as a crucial hyperparameter that balances the trade-off between improving calibration by mitigating overconfidence in errors and reinforcing confidence in accurate classifications. By appropriately tuning \(\alpha\), practitioners can control the model’s behavior to achieve a desired balance between reliability in uncertain regions and decisiveness in confident predictions, tailored to the specific requirements of their application.

We further generalize this loss to token-based models (e.g., LMs and VLMs), formulated as
\ifarxiv
\small
\fi
\begin{align}
    \mathcal{L}_\text{corr} & = \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \mathds{1}\{ y_{i,t} = \hat{y}_{i,t} \} \text{CE}(p_{i,t}(\mathbf{x}_i), y_{i,t}) \\
    \mathcal{L}_\text{incorr} & = \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \mathds{1}\{ y_{i,t} \neq \hat{y}_{i,t} \} \text{KL}\left(p_{i,t}(\mathbf{x}_i) \parallel \mathcal{U}\right)
\end{align}
\normalsize
where \( y_{i,t} \) and \( \hat{y}_{i,t} \) denote the true and predicted tokens at position \( t \) for sample \( i \), \( p_{i,t} \) is the predicted token distribution at position \( t \) for sample \( i \), and \( T \) is the sequence length for the token-based model. The token-level loss ensures that correct token predictions are made confidently while incorrect tokens are assigned smaller confidences.

\sloppy
\textbf{Stage 3: Confidence Computation \& Thresholding.} After fine-tuning \smallmodel with \loss, we apply standard confidence- and entropy-based techniques for model uncertainty to obtain a deferral signal. We use the selective prediction framework to determine whether a query point~$\mathbf{x} \in \mathbb{R}^D$ should be accepted by \smallmodel or routed to \bigmodel. Selective prediction alters the model inference stage by introducing a deferral state through a \textit{gating mechanism}~\citep{yaniv2010riskcoveragecurve}. At its core, this mechanism relies on a deferral function $g:\mathbb{R}^D \rightarrow \mathbb{R}$ which determines if \smallmodel should output a prediction for a sample~$\mathbf{x}$ or defer to \bigmodel. Given a targeted acceptance threshold $\tau$, the resulting predictive model can be summarized as:
\begin{equation}
\label{eq:deferral}
    (\mathcal{M}_S,\mathcal{M}_L,g)(\mathbf{x}) = \begin{cases}
  \mathcal{M}_S(\mathbf{x})  & g(\mathbf{x}) \geq \tau \\
  \mathcal{M}_L(\mathbf{x}) & \text{otherwise.}
\end{cases}
\end{equation}

\emph{Classification Models (Max Softmax).} Let \(\mathcal{M}_S\) produce a categorical distribution
\(\{p(y=c \mid \mathbf{x})\}_{c=1}^C\) over \(C\) classes. 
Then we define the gating function as
\begin{align}
g_{\text{CL}}(\mathbf{x}) \;=\; \max_{1 \,\le\, c \,\le\, C}\;p\bigl(y = c \,\big\vert\, \mathbf{x}\bigr).
\end{align}

\emph{Token-based Models (Negative Predictive Entropy).} 
Let \(\mathcal{M}_S\) produce a sequence of categorical distributions 
\(\{p(y_t = c \mid \mathbf{x})\}_{c=1}^C\) for each token index \(t \in T\). Then we define the gating function as
\begin{equation}
\footnotesize
g_{\text{NENT}}(\mathbf{x}) 
= \; \frac{1}{T} \sum_{t=1}^{T} \sum_{c=1}^{C} 
    p\bigl(y_t = c \,\big\vert\, \mathbf{x}\bigr)\,\log p\bigl(y_t = c \,\big\vert\, \mathbf{x}\bigr),
\end{equation}
where \(y_t \in [C]\) is the predicted token at time step \(t\), \(p(y_t=c \mid \mathbf{x})\) is the (conditional) probability of token \(k\) at step \(t\), and \(T\) is the total number of token positions for the sequence. Across both model classes, higher values of either $g_{\text{CL}}$ or $g_{\text{NENT}}$ indicate higher confidence in the predicted class or sequence generation, respectively.