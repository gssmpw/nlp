\section{Introduction}

\begin{figure}[t]
\centering
    \subfigure{%
        \resizebox{0.35\linewidth}{!}{
    \input{figs/cascade_schema}}
        \label{fig:cascade_schema}
    }
    \subfigure{%
        \includegraphics[width=0.55\linewidth]{figs/deferral_example_intro.pdf}%
        \label{fig:cascade_performance}
    }
    \caption{\textbf{Overview of the cascading setup (left) and performance trade-off (right)}. \emph{Left}: Cascading determines which inputs should be predicted by a small model \smallmodel or routed to a large model \bigmodel. \emph{Right}: Performance is measured as a trade-off between joint accuracy across \smallmodel and \bigmodel and deferral ratio. Ideal deferral strategies optimize this trade-off and push the realized deferral curve closer to the ideal deferral depicted in (d). (a) depicts full deferral; (b) depicts no deferral; and (c) depicts excessive deferral of requests that could have been correctly handled by \smallmodel.
    }
    
    \label{fig:deferral_example_intro}
\end{figure}

In recent years, large-scale machine learning models such as Gemini~\citep{team2023gemini}, GPT-4~\citep{achiam2023gpt} or Claude~\citep{antropicmodels} have gained significant traction due to their remarkable ability to address a wide array of tasks. These tasks range from natural language understanding and generation, including machine translation, summarization, and conversational agents, to computer vision applications like image recognition, object detection, and image captioning. The versatility and high performance of these expansive models make them invaluable tools across diverse domains, including healthcare~\citep{llm_healthcare}, finance~\citep{llm_finance}, education~\citep{llm_education}, and entertainment~\citep{llm_games}.

\looseness=-1
\sloppy
Deploying and operating such large models presents significant challenges in terms of latency, memory, compute and storage \citep{MLSYS2023_c4be71ab}. Optimizing inference costs is an active research area which includes both techniques for reducing the size of the existing large model such as model compression ~\citep{hoefler2021sparsity}, model pruning~\citep{ma2023llmpruner, pruning_survey} and distillation~\citep{knowledgedistilation_survey}, and those aiming to leverage a sequence of models such as speculative decoding~\citep{leviathan2023fast} and model cascades~\citep{dohan2022language, CheZahZou2023,gupta2024languagemodelcascadestokenlevel,Chen:2024}. However, due to scaling laws showing that the performance of a Large Language Model (LLM) increases with its size \citep{kaplan2020scaling}, the latter category of methods leveraging a sequence of models is currently a more promising direction to lower inference costs without sacrificing the capabilities of large models.

\looseness=-1
Both speculative decoding and model cascading  rely on the existence of a large performant model \textbf{\bigmodel} and a small model \textbf{\smallmodel} that is cheap, fast, and less accurate. Speculative decoding leverages \textbf{\smallmodel} for generating a set of draft tokens that are then validated by \textbf{\bigmodel} in parallel, a technique successfully deployed in industry applications \citep{blogpost}. In contrast, model cascades leverage a deferral rule for selecting the most suitable model to process a given request (see Figure~\ref{fig:deferral_example_intro} left). While the success of the speculative decoding necessitates a highly performant \textbf{\smallmodel} to generate quality draft tokens, model cascades allow the deployment of a less capable \textbf{\smallmodel} by invoking \textbf{\bigmodel} only for inference requests outside the small model's scope. In this work, we contribute to the advancement of the model cascades. 

Model cascades achieve efficient deferral by optimizing for two objectives: compute budget and joint accuracy. We illustrate their trade-off on the example shown in Figure~\ref{fig:deferral_example_intro} (right). Assume we have \textit{x} inference requests and a small model \textbf{\smallmodel} that only requires \textit{20\%} of the compute budget of the large model \textbf{\bigmodel}. There are three worst case scenarios: (a) the small model \textbf{\smallmodel} defers all requests to \textbf{\bigmodel} and the system achieves the best joint accuracy (equal to the accuracy of \textbf{\bigmodel}) but the worst compute budget (\textit{1.2x}) since all requests are run on both models; (b) \textbf{\smallmodel} never sends a request to \textbf{\bigmodel}, resulting in the smallest compute budget (\textit{0.2x}) but also the lowest joint accuracy (equal to the accuracy of \textbf{\smallmodel}); (c) \textbf{\smallmodel} only sends requests that it could have answered correctly to \textbf{\bigmodel}, requiring an increased compute budget compared to (b) but still resulting in the lowest joint accuracy (equal to the accuracy of \textbf{\smallmodel}). On the other hand, an ideal case is the scenario (d) where the small model \textbf{\smallmodel} only sends requests for which it would be incorrect, requiring a compute budget between \textit{0.2-1x} but resulting in the optimal joint accuracy given that budget. We call the approximation of the ideal case the \textit{deferral performance}.

In this paper we address the following research question: 

\begin{center}
\textbf{How can we optimize model cascades to maximize deferral performance?} 
\end{center}


In other words, we focus on designing effective model cascades by making the small model more aware of what it does not know. We achieve this by introducing a \textit{general-purpose} loss function, called \loss, that calibrates the small modelâ€™s confidence in its predictions. By fine-tuning \textbf{\smallmodel} to output high confidence for correct predictions and low confidence for incorrect ones, we enhance the reliability of its uncertainty estimates and facilitate learning of common tasks, thereby directly improving the deferral performance.  Crucially, \loss includes an inherent mechanism for managing the trade-off between \emph{model} performance and \emph{deferral} performance that can be applied to an arbitrary architecture, making our work directly applicable to Vision-Language Models (VLMs). 

We demonstrate the efficacy of the \loss loss across various model architectures, including encoder-only vision models for image classification, decoder-only LMs for closed- and open-form text generation, and encoder-decoder setups for VL tasks for open set classification and captioning. Our main results show that models trained with \loss outperform an untuned baseline by a factor of 0.72x/2x on CIFAR-100/TinyImagenet and 7x/10x on ARC-e/c, respectively, in terms of deferral performance. This advancement paves the way for more scalable and efficient deployment strategies, leveraging the strengths of both local and large-scale models to deliver high-quality results in real-time applications.
