\section{Related Work} \label{sec:related-word}

\sloppy
Our proposed method improves model cascades through uncertainty-aware finetuning. Next, we describe related work for both research areas.

\textbf{Model Cascades:} 
A cascade consists of a series of models and a deferral rule which determines the appropriate model given an input request. The concept of model cascades has first been proposed by \citet{990517}, where it is used to accelerate object detection models. Cascades have been extensively studied for classification-based computer vision \citep{Wang2017IDKCF, pmlr-v31-trapeznikov13a, Bolukbasi2017AdaptiveNN, NEURIPS2023_1f09e1ee} and in models for natural language processing \citep{dohan2022language, mamou2022tangobertreducinginferencecost, varshney-baral-2022-model}. 

\looseness=-1
Cascades are particularly promising in the context of generative models such as LLMs and VLMs since they can significantly reduce inference costs. In contrast to speculative decoding~\citep{leviathan2023fast}, they aim to invoke the large model only for difficult examples. However, the two approached can also be combined \citep{narasimhan2025faster}. While~\citet{chen2024cascade} combine the deferral logic with speculative decoding to generate initial tokens using larger models and later tokens using a smaller model, the majority of research on model cascades has focused on using pre-trained LLMs with a post-hoc deferral logic~\citep{NEURIPS2022_bc8f76d9, NEURIPS2023_1f09e1ee, yue2024large}. \citet{kolawole2024agreementbasedcascadingefficientinference} use agreement across multiple models to make deferral decisions, while~\citet{gupta2024languagemodelcascadestokenlevel} present a method to learn a deferral rule based on quantiles of per-token log probabilities. 

Model cascades can be further improved through training and fine-tuning. \citet{wang2024cascadeawaretraininglanguagemodels} train the small model only on easier examples by masking tokens for which large and small model are incorrect. \citet{Enomoro_Eda_2021} extend the training objective of image classification models with confidence calibration. In contrast to previous research, our work extends cascades to VLMs and improves overall inference performance by making smaller models less confident when they are incorrect.

\textbf{Uncertainty-Aware Models:} Extensive research has been conducted in the field of uncertainty quantification in deep learning and we refer to~\citet{abdar2021review} for a detailed survey. While many methods have been proposed for classification-based models, measuring uncertainty for generative models is still an active area of research.
Based on the assumed level of access to model internals, existing methods can be summarized into three main categories:

\emph{Black box} methods operate solely via the model’s query interface by injecting tailored instructions into prompts. These modify the prompt $\mathbf{x}$ by appending instructions $\mathbf{x}'$ for the model to respond less confidently: $\mathbf{x} \leftarrow \mathbf{x} | \mathbf{x}'$. Related methods are confidence quantification~\citep{shrivastava2023llamas}, rejection and remote model awareness~\citep{kadavath2022language}, and self-critiquing~\citep{gou2023critic}. \citet{xiong2024can} show that LLMs can express their confidence through prompting and sampling strategies and their experiments indicate that these models tend to be overconfident.
 
\emph{Gray box} approaches employ confidence-based strategies centered on post-processing the model’s logits. Many uncertainty techniques such as ensembling~\citep{lakshminarayanan2017simple} and Bayesian methods~\citep{blundell2015weight}) are not scalable. Related techniques are  max confidence~\citep{hendrycks2016baseline}, predictive entropy, and confidence reduction prompting. \citet{malinin2021uncertainty} uses token-entropy as a measure of uncertainty in auto-regressive models and~\citet{kuhn2023semantic} leverages linguistic invariances via semantic entropy.
 
\emph{White box} methods utilize uncertainty-aware fine-tuning in order to produce more accurately calibrated models. \citet{chuang2024learningrouteconfidencetokens} introduces Self-REF, a framework which leverages confidence tokens during fine-tuning to improve performance in downstream routing. \citet{krishnan2024enhancingtrustlargelanguage} proposes an uncertainty-aware causal language modeling loss function, which captures the trade-off between predictive accuracy and uncertainty calibration. In contrast to previous work, our method aims to calibrate the model in a way such that correctly generated predictions are assigned low predictive uncertainty and incorrectly generated predictions are assigned high predictive uncertainty. We consider the uncertainty-aware model in the context of cascade inference system, where it helps to improve overall performance.

Prior to this work, \citet{rawat2021doubtsummontitansefficient} has proposed pre-partitioning the data into easy and hard sets before training. One example of such a partitioning is based on the \bigmodel's confidence. The small model \smallmodel is then trained to identify easy/hard examples with explicit labels. We improve on this static one-time partitioning of easy/hard examples by deciding the assignment dynamically based on \smallmodel's current state during training. 
