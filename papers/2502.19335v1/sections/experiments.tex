\section{Experiments}
\label{sec:experiments}

In this section we detail the experiments used to evaluate our deferral strategies across three distinct model architectures: encoder-only classification models, decoder-only LMs, and encoder-decoder VLMs. Each setup leverages a deferral setup from smaller to larger models, assessing the efficacy of deferring hard queries to more capable models. 

\sloppy
\subsection{Encoder-only Setup (Classification Models)}
\label{sec:class_exp}

We comprehensively assess the performance of our deferral strategies across different model architectures and task types, starting with image classification. We train both a large model and a small model on the following datasets: CIFAR-10/100~\citep{krizhevsky2009learning}, Food-101~\citep{bossard14}, and TinyImageNet200~\citep{Le2015TinyIV}. For both CIFAR datasets we use a ResNet-18~\citep{he2016deep} as \bigmodel and a custom CNN as \smallmodel. For Food-101 and TinyImageNet200 we instead use a ResNet-50~\citep{he2016deep} as \bigmodel and a Mobilenet V3 Small \citep{howard2019searching} as \smallmodel, where the latter is trained using knowledge distillation from the big model.

\paragraph{Evaluation Metrics.}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/metrics_example_class.pdf}
    \vspace{-20pt}
    \caption{ 
    \textbf{Performance metrics overview}: \textbf{(a)}~Distributional Overlap~$s_o$: the densities of confidence scores for correctly (green) and incorrectly classified (red) samples, with the overlap area shaded in blue. Smaller values are better ($\downarrow$).
    \textbf{(b)}~Deferral Performance~$s_d$: how joint accuracy between \smallmodel and \bigmodel varies with deferral ratio, showing random (red), ideal (green), and realized (black) deferral strategies. 
    The blue region shows the realized performance gain, the hatched portion represents the range of useful deferral functions, and the green region indicates the potential headroom over the realized deferral. Larger values are better ($\uparrow$).}
    \label{fig:metrics_illustration}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figs/summary_metrics.pdf}
    \vspace{-20pt}
    \caption{\textbf{Performance on image classification tasks}. We observe that lower levels of $\alpha$ lead to decreased distributional overlap between correct/incorrect predictions (left), increased deferral performance (center) and generally decreased performance over the full data distribution (right). These results support our conclusion that the small model \smallmodel learns to refocus on easier subsets of the distribution while understanding more reliably when it should abstain and defer to the large model \bigmodel.}
    \label{fig:image_class_results}
\end{figure*}

We measure the performance of our confidence tuning method and the resulting deferral function~$g(\cdot)$ using the following performance metrics (see example in Figure~\ref{fig:metrics_illustration} for a visual overview):

\begin{enumerate}[leftmargin=1.25em]
    \item The \textbf{Distributional Overlap of Confidences of Correct and Incorrect Predictions} $s_o$ is defined as the integral of the minimum of the probability density functions (PDFs) of confidence scores for correctly classified samples, $\hat{p}_{\text{corr}}(c)$, and incorrectly classified samples, $\hat{p}_{\text{incorr}}(c)$ (see Figure \ref{fig:metrics_illustration}a). Formally, given the confidence sets $\mathcal{C}_\text{corr}$ and $\mathcal{C}_\text{incorr}$, the overlap $s_o$ is computed as
\begin{equation}
    s_o = \int_{0}^{1} \min\left\{ \hat{p}_{\text{corr}}(c),\ \hat{p}_{\text{incorr}}(c) \right\} \, \mathrm{d}c,
\end{equation}
where the PDFs are estimated using Kernel Density Estimation (KDE). If $s_o = 1$, then \smallmodel cannot distinguish the confidence distribution of correct and incorrect predictions; if $s_o = 0$, then \smallmodel can perfectly separate correct and incorrect predictions. Note that a related way of capturing the distributional separability is given by the Area Under the Receiver Operating Characteristic Curve (AUROC) which we discuss in Appendix~\ref{app:add_metrics}.

    
    \item \textbf{Deferral Performance $s_d$}:  
To formally quantify how well \smallmodel defers difficult inputs to \bigmodel, we examine the joint performance across all possible deferral ratios $r \in [0,1]$, where $r$ denotes the fraction of inputs sent to \bigmodel based on a particular threshold $\tau$ (recall Equation~\eqref{eq:deferral}). Figure~\ref{fig:metrics_illustration} b) illustrates how, as $r$ increases from $0$ to $1$, the overall (joint) accuracy $\text{acc}(r)$ increases from the accuracy of \smallmodel (blue circle, no deferral) to the accuracy of \bigmodel (orange square, full deferral). Useful deferral models are constrained to operate between random deferral ($\mathrm{acc}_{\mathrm{rand}}$, red line) and ideal deferral ($\mathrm{acc}_{\mathrm{ideal}}$, green line). The ideal deferral $\mathrm{acc}_{\mathrm{ideal}}$ corresponds to the oracle solution that perfectly defers examples misclassified by \smallmodel and we discuss its exact functional form in Appendix~\ref{app:ideal_deferral}. We also define the realized deferral curve, $\mathrm{acc}_{\mathrm{real}}$, as the joint accuracy obtained under the learned deferral strategy $g(\cdot)$ employed by \smallmodel and \bigmodel. The deferral performance metric \( s_d \) is then given as:
\begin{equation}
\label{eq:deferral_performance}
\small
s_d = \frac{A_{\mathrm{perf}}}{A_{\mathrm{useful}}} = \frac{\int_{0}^{1} \left( \mathrm{acc}_{\mathrm{real}}(r) - \mathrm{acc}_{\mathrm{rand}}(r) \right) \, \mathrm{d}r}{\int_{0}^{1} \left( \mathrm{acc}_{\mathrm{ideal}}(r) - \mathrm{acc}_{\mathrm{rand}}(r) \right) \, \mathrm{d}r}.
\end{equation}
This ratio quantifies the fraction of the potential improvement over random deferral that has been realized by the achieved deferral strategy. Note that $s_d = 1$ indicates perfect deferral, matching the ideal strategy, while an $s_d = 0$ implies no improvement over random deferral.
    \item \textbf{Accuracy of the small model} $\text{acc}(\mathcal{M}_S)$: Finally, since \loss emphasizes patterns for distinguishing correct/incorrect examples, the model is no longer encouraged to minimize the classification loss over the full population. As a result, improving on the correct/incorrect separation task can lead to drops in utility. Hence, practically useful deferral methods need to balance both deferral performance and the accuracy of \smallmodel.
\end{enumerate}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/tradeoff_class_cut.pdf}
    \vspace{-20pt}
    \caption{\textbf{Performance trade-off between small model accuracy $\text{acc}(\mathcal{M}_S)$ and deferral performance $s_d$}. The baseline model obtained without fine-tuning using \loss is often the most accurate model over the full data distribution. With the introduction of \loss we can improve distinguishability of correct/incorrect predictions (left) as well as deferral (right) at the expense of model utility. Successful cascading solutions in practice need to balance both model accuracy and deferral performance.}
    \label{fig:tradeoffs}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/summary_metrics_l.pdf}
    \vspace{-20pt}
    \caption{\textbf{Performance on language modeling tasks}. Similar as Figure~\ref{fig:image_class_results}. In addition to a non-tuned baseline, we also add an uncertainty prompting baseline as well as an Answer ``N'' option. Both these approaches fail to meaningfully improve deferral.}
    \label{fig:l_results}
\end{figure*}

\textbf{Results.} We document our main results in Figure~\ref{fig:image_class_results}. We report performance results on both a baseline model, i.e., an instance of \smallmodel that was not trained with \loss, and a set of small models trained with \loss at various $\alpha$s. Across all models we compute the deferral performance as well as the model's correct/incorrect separation ability (center, left). We see that the strongest performance is achieved at low $\alpha$s as the model strongly emphasizes pushing the outputs of incorrect examples closer to a uniform distribution. However, this strong performance comes at a cost: the accuracy of the small model consistently degrades for small $\alpha$s (right). This highlights the fact that the model effectively ``unlearns'' to perform well on some part of the data distribution and re-focuses its classification ability of easier data points. On the other hand, for large values of $\alpha$ we observe consistency in model accuracy or even slight improvements as the model now emphasizes most of its training on maintaining good performance on already well-predicted points. 

This result highlights a critical trade-off which is directly controlled by $\alpha$: \emph{how strongly do we want to degrade model performance over the full data distribution in order to obtain a better deferral model?} We note that this compromise between raw model utility and deferral performance is not surprising and similar trade-offs exist in fairness~\citep{dutta2020there, yaghini2023learning} and privacy~\citep{abadi2016deep, rabanser2023training}. We study this trade-off explicitly in Figure~\ref{fig:tradeoffs} showing (i) a clear negative correlation between deferral performance and the small model's accuracy; and (ii) a clear positive correlation between the overlap of correct/incorrect confidences and the accuracy of \smallmodel. 

\subsection{Decoder-only Setup (Language Models)}
\label{sec:lang_exp}

In the decoder-only setup, we explore the application of LLMs. Our primary models of interest are the scalable LMs from the Gemma model class~\citep{team2024gemma}. We choose Gemma2B as \smallmodel and Gemma7B as \bigmodel with 2 billion and 7 billion parameters, respectively. Similar to the encoder-only setup, we employ smaller LMs as the initial classifiers to manage simpler next-token prediction tasks. The deferral strategy involves routing only those token sequences that exhibit high uncertainty --- as determined by high predictive entropy --- to the more powerful model \bigmodel.

Our experiments start by taking the instruction-tuned checkpoints of Gemma2B and Gemma7b and fine-tuning both models on the training split of a respective dataset to ensure that the model (i) performs well on the task; and (ii) is familiar with the desired response format. Note that this fine-tuning stage is performed using standard perplexity minimization. Then, we finetune \smallmodel with \loss using the same training split to decrease confidence on incorrect next-token predictions. Finally, we evaluate the model yielded by \loss on a validation/test split. The datasets we consider are ARC-e/c~\citep{clark2018think}, MMLU~\citep{hendrycks2020measuring}, and GSM8K~\citep{cobbe2021training}. We use the same evaluation metrics as previously used in Section~\ref{sec:class_exp}.

\textbf{Results.} We document our main results in Figure~\ref{fig:l_results} where we compare the baseline model's deferral and correct/incorrect separation ability against our fine-tuned model at different $\alpha$s. We generally observe a similar trend as in the image classification results: higher $\alpha$s maintain raw prediction performance closer to the baseline model but do not significantly improve correct/incorrect separation. At the same time, low $\alpha$s improve deferral more substantially at the cost of accuracy on the full data distribution. In addition to the baseline model (i.e., a model that was not fine-tuned with \loss but from which we still compute the predictive entropy as a deferral signal), we also include results for two other uncertainty prompting baselines (details in Appendix~\ref{app:uncertainty_appendix}): (i) \emph{Reduce Confidence}: where we append additional instructions to the input prompt to encourage the model to reduce confidence when it is uncertain; and (ii) \emph{Answer ``N''}: where we instruct the model to answer with ``N'' if it is uncertain about the answer. Consistent with recent findings in \cite{kadavath2022language}, we find that these approaches do not reliably improve separation of correct-incorrect predictions or offer advantages as deferral models.

\begin{figure*}

    \centering
    \subfigure{%
        \includegraphics[width=0.48\textwidth]{figs/summary_metrics_vl_class.pdf}%
        \label{fig:vl_results_class}
    }
    \hfill
    \vrule width 0.5pt
    \hfill
    \subfigure{%
        \includegraphics[width=0.48\textwidth]{figs/summary_metrics_vl_gen.pdf}%
        \label{fig:vl_results_gen}
    }
    \caption{\textbf{Performance on VLM classification (left) and captioning tasks (right)}. Consistent with results in Figures~\ref{fig:image_class_results} and \ref{fig:l_results}, we see that smaller $\alpha$s lead to improved deferral performance in both classification and generation tasks.}
    \label{fig:vl_results}
\end{figure*}

\subsection{Encoder-Decoder Setup (Vision-Language Models)}
Finally, we examine models that incorporate both visual and textual processing capabilities, making it ideal for tasks that require a comprehensive understanding of image content in conjunction with language generation. We consider the PaliGemma~\citep{steiner2024paligemma} model family which are encoder-decoder models designed to perform VL tasks such as image captioning, visual question answering, and image classification with descriptive outputs. In this setup, the encoder component processes the input images to extract rich feature representations, while the decoder generates corresponding textual classifications or descriptions. We use PaliGemma1B as \smallmodel and PaliGemma7B as \bigmodel. The deferral strategy involves deploying a smaller VLM to handle the majority of classification tasks, reserving the more resource intensive 7B model for instances where \smallmodel's predictive entropy falls below a predefined threshold.

Similarly to our experiments on LMs in Section~\ref{sec:lang_exp}, we employ two stages of fine-tuning. First, we take the instruction-tuned checkpoints of PaliGemma1B and PaliGemma7B and then fine-tune both models on the training split of a given dataset. Next, we fine-tune only \smallmodel using \loss before evaluating the model on a validation/test split of the dataset. The datasets we consider are two classification datasets (VQAv2~\citep{goyal2017making}, AI2D~\citep{hiippala2021ai2d}) and two captioning datasets (Cococap~\citep{lin2014microsoft}, Screen2Words~\citep{wang2021screen2words}). This allows us to evaluate \loss in both closed-form vision-language classification setups as well as open-form text generation.

\textbf{Factuality Scoring.}
For classification tasks we apply our analysis in the same way as in Section~\ref{sec:lang_exp}. However, for captioning datasets we need to evaluate the quality of a caption generated by PaliGemma. To do that, we compute a factuality score which judges whether the generated caption is semantically coherent with respect to a reference caption using the Gemini LLM~\citep{team2023gemini}. Specifically, the Gemini LLM is prompted with an instruction of the form: \emph{``Are these captions semantically equivalent?''}, followed by both the candidate caption and the reference caption. The model then responds with either \emph{``Yes''} or \emph{``No''}. Finally, we compute the log-likelihood of each response and normalize it to a probability, reflecting the LLM's confidence in the captions being factually aligned. We detail this process in Appendix~\ref{app:fac_scoring} and denote the factuality score for input point $\mathbf{x}_i$ with candidate caption $\hat{\mathbf{y}}_i$ and ground truth caption $\mathbf{y}_i$ as $s_{\text{Fac}}(\hat{\mathbf{y}}_i, \mathbf{y}_i)$.


\textbf{Measuring Correlation Between Factuality and Negative Predictive Entropy.} Since the result of evaluating $s_{\text{Fac}}(\hat{\mathbf{y}}_i, \mathbf{y}_i)$ is no longer binary, our evaluation metrics which previously relied on accuracy cannot be used directly to evaluate deferral performance and the correct/incorrect entropy distribution separation. We address this issue by replacing the distributional overlap computation with the Pearson correlation $\rho(g_{\text{NENT}}(\mathbf{x}_i), s_{\text{Fac}}(\hat{\mathbf{y}}_i, \mathbf{y}_i))$ between the negative predictive entropy of a caption $g_{\text{NENT}}(\mathbf{x}_i)$ and its associated factuality score $s_{\text{Fac}}(\hat{\mathbf{y}}_i, \mathbf{y}_i))$. We also adapt our deferral performance metric from Equation~\eqref{eq:deferral_performance} to rely on factuality measures instead of accuracy. 

\textbf{Results.}  We document our results in Figure~\ref{fig:vl_results} where we compare the baseline model's deferral ability against our fine-tuned models at different $\alpha$s. For the classification results (Figure~\ref{fig:vl_results} left), we observe the same trends as outlined in our classification and language modeling experiments. For the captioning results (Figure~\ref{fig:vl_results_gen} right) we observe that \loss measurably increases the correlation between factuality and negative predictive entropy, yielding better deferral from \smallmodel to \bigmodel with decreasing $\alpha$. This demonstrates that our method does not just work on classification problems but also generalizes to sequence generation tasks. Note that while we also benchmarked the prompting baselines from Section~\ref{sec:lang_exp} for these experiments, the PaliGemma model did not return any responses for these modified prompts (likely due to PaliGemma's rigid pretraining and prompting instructions~\citep{beyer2024paligemma}).