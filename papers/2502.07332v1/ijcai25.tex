%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

\usepackage{xcolor}
\usepackage{subcaption} % For subfigures


% Comment out this line in the camera-ready submission
%\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{The Combined Problem of Online Task Assignment and Lifelong Path Finding\\ in Logistics Warehouses: A Case Study}


% Single author syntax
%\author{
%    Author Name
%    \affiliations
%    Affiliation
%    \emails
%    email@example.com
%}

%\author{Anonymous}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
%\iffalse
\author{
Fengming Zhu$^1$
\and
Fangzhen Lin$^1$
\and
Weijia Xu$^2$
\And
Yifei Guo$^2$\\
\affiliations
$^1$CSE Department, HKUST\\
$^2$Meituan Academy of Robotics Shenzhen\\
\emails
fzhuae@connect.ust.hk,
flin@cse.ust.hk,
\{xuweijia, guoyifei02\}@meituan.com
}
%\fi

\begin{document}

\maketitle

%\vspace{-3mm}

\begin{abstract}
%We study the online problem of task assignment and path finding in modern automated warehouses, which has significant applications in the logistics industry.
%%%% what assumption
%%The existing literature mostly considers ideal abstractions of such problems by imposing potentially unrealistic assumptions.
%%To this end, we propose a system that aims at mitigating those gaps between simulation and real-world deployment.
%%%%
%%The combined problem of task assignment and path finding for robot swarms is one of the key challenges in modern automated warehouses, especially in the logistics .
%The existing literature 
%(1)~mostly considers idealized formulations of such problems by assuming
%robot models that neglect rotational costs and focusing on well-formed layouts,
%and (2)~has not fully explored the benefit of deliberate task assignment.
%To address the above issues, our proposed system redesigns these two modules, namely
%(i)~a lifelong path planner that is directly tailored for a more practical robot model in possibly \textit{non}-well-formed layouts, and
%%(ii)~online task assignment that reacts on real-time states.
%(ii)~an task assigner that can adapt to the underlying path planner.
%Simulation experiments conducted in warehouse scenarios at \textit{Meituan}, one of the largest shopping platforms in China, demonstrate that
%(a)~\textit{in terms of time efficiency},
%our system takes only 83.77\% of the execution time needed for the currently deployed system at Meituan, outperforming other SOTA algorithms by 8.09\%;
%(b)~\textit{in terms of economic efficiency},
%ours can achieve the same throughput with only 60\% of the agents of the current scale.
%%%%
%% Simulation experiments conducted in warehouse scenarios at \textit{Meituan}, one of the biggest shopping platforms in China, demonstrate that our system, in terms of throughput, 
%% outperforms other SOTA algorithms by 10.2\%, and outperforms the currently deployed system at Meituan by 19.4\%.
%% From the economical side, our system can achieve the same throughput with only 60\% agents of the current scale.
%%%%
%Our observations also imply that rule-based methods, though may not be general, are sometimes effective enough in practice.




We study the combined problem of online task assignment and lifelong path finding, which is crucial for the logistics industries.
However, most literature either (1) focuses on lifelong path finding assuming a given task assigner, or (2) studies the offline version of this problem where tasks are known in advance.
We argue that, to maximize the system throughput, the online version that integrates these two components should be tackled directly.
To this end, we introduce a formal framework of the combined problem and its solution concept.
Then, we design a rule-based lifelong planner under a practical robot model that works well even in environments with severe local congestion.
Upon that, we automate the search for the task assigner with respect to the underlying path planner.
Simulation experiments conducted in warehouse scenarios at \textit{Meituan}, one of the largest shopping platforms in China, demonstrate that
(a)~\textit{in terms of time efficiency},
our system requires only 83.77\% of the execution time needed for the currently deployed system at Meituan, outperforming other SOTA algorithms by 8.09\%;
(b)~\textit{in terms of economic efficiency},
ours can achieve the same throughput with only 60\% of the agents currently in use.
\end{abstract}



\section{Introduction}

%% background problem
%We consider the problem of real-world warehouse automation, where a fleet of robots are programmed to pick up and deliver packages without any collision.
%The investigation of such a problem will significantly benefit logistics companies. 
%However, the problem is inherently hard, primarily for two reasons:
%(1)~the computational complexity of multi-agent path finding is notorious especially when the size of the robot fleet is considerably large, and
%(2)~the stream of tasks arrive in an online fashion of which the accomplishment also depends on the subsequent execution by the path finding module. 

% Lin's version
We consider the problem present in highly automated real-world warehouses where a fleet of robots is programmed to pick up and deliver packages without any collision.
This is a significant problem for logistics companies as it has a major impact on their operational efficiency.  
It is a difficult problem for at least the following  two reasons:
(1)~the computational complexity of multi-agent path finding is notoriously high, especially when the number of robots is large, and
(2)~the dynamic and real-time assignment of tasks to the robots both depends on and affects the subsequent path finding.


% Existing mainstream solutions and main issues
There is a vast literature that studies idealized abstractions of such real-world problems.
The most commonly seen formulation is to assume a given (or naive) task assigner, and therefore, the focus is merely on the path-finding part, which is usually termed as one-shot \textit{Multi-Agent Path Finding} (MAPF)~\cite{yu2013structure,erdem2013general,sharon2015conflict,li2021eecbs,okumura2022priority,okumura2023lacam}
or its lifelong version \textit{Multi-Agent Pickup and Delivery} (MAPD)~\cite{ma2017lifelong,vsvancara2019online,li2021lifelong,okumura2022priority}.
%Note that a task assigner tightly depends on the underlying path finding. In other words, to maximize the throughput of the whole pipeline, one should adapt a task assigner to a path planner.
However, to maximize the throughput of the whole production pipeline, the task assigner should also be deliberately designed with respect to the particular underlying path planner.
To this end, some recent work has further investigated the combined problem of \textit{Task Assignment and Path Finding} (TAPF)~\cite{yu2013multi,ma2016optimal,honig2018conflict,liu2019task,chen2021integrated,tang2023solving}.
%Nevertheless, this line of work is mostly restricted to  offline scenarios, i.e., tasks (and/or their release times) are priorly known.
Nevertheless, this line of work is mostly restricted to  offline scenarios, i.e., tasks (and/or their release times) are assumed to be known.
%However, so far only offline scenarios, i.e. tasks (and/or their release times) are assumed upfront, are considered.
In practice, for example in a sorting center, orders may come dynamically in real-time.

% Other implementation-related problems
Besides, we draw attention to two seemingly minor but indeed fundamental aspects.
\textbf{For one}, the robots are usually abstracted to agents doing unit-cost unit-distance cardinal actions,
i.e., \{\texttt{stop}, \texttt{$\uparrow$}, \texttt{$\downarrow$}, \texttt{$\leftarrow$}, \texttt{$\rightarrow$}\}, what we term as the \texttt{Type}$\oplus$ robot model.
The planned paths are later post-processed to executable motions regarding kinematic constraints~\cite{honig2016multi} and action dependencies~\cite{honig2019persistent}, as a real-world robot has to rotate before going in a different direction.
Imaginably, when the rotational cost is not negligible compared to the translational cost, the quality of the plans computed for the \texttt{Type}$\oplus$ robot model will be largely compromised when instantiated to motions.
A candidate solution is to revisit and reimplement the existing algorithms over an alternative set of atomic actions \{\texttt{stop}, \texttt{forward}, \texttt{$\circlearrowright$90}, \texttt{$\circlearrowleft$90}\}, which we advocate in this paper as the \texttt{Type}$\odot$ robot model.
\textbf{For another}, most of the literature assumes the problem instance to be \textit{well-formed}~\cite{ma2017lifelong,liu2019task,xu2022multi} to guarantee completeness of their methods, which is actually a strong condition requiring that every agent can find a collision-free path to her current goal even if the others are stationary.
%In fact, \textit{non-well-formed} instances are also commonly seen in modern warehouse, illustrated as an example in Figure~\ref{fig:eg_non_wf}.
%The planner that we will present later makes no use of this potentially unrealistic assumption.
However, this assumption is often not met in modern warehouses. In particular, the instance (Figure~\ref{fig:eg_non_wf}) that we consider in this work does not satisfy this condition.

%wellformed instance too strong!

%\textit{
%Given a potentially non-well-formed layout with the number of agents varying,
%is there a good way for online TAPF that can take the advantage of the layout.
%}
%\textbf{That is why this is a case study}


%Might be minor for academia, but critical for industrial deployment

%MAPF-POST and Temporal Plan Graph (TPG) ~\cite{honig2016multi}

%allow task swapping~\cite{okumura2023solving}

%robust warehouse execution~\cite{honig2019persistent}

%capacitated mapd~\cite{chen2021integrated}

%probably no need for a general algorithm, but a practical one for certain structured layouts


\begin{figure*}[tb]
\centering
\vspace{-3mm}
\includegraphics[width=180mm]{fig/warehouse3-compressed}
\caption{A \textit{non-well-formed} instance 
%(see~\protect\cite{xu2022multi}) 
currently deployed in Meituan warehouses. The white cells near \textsc{Green} dots are delivery ports, while the ones near \textsc{Red} dots are pickup ports. Colored circles heading to different directions with numbers are agents. The colored box (blue) is a pickup port currently assigned to the agent in the same color (ID 45 in the lower right area). Congestion happens a lot near the pickup ports.}
\vspace{-1mm}
\label{fig:eg_non_wf}
%\vspace{-2mm}
\end{figure*}

% Our Contribution
Considering the aforementioned issues,
%we here propose a system that organically integrates the task assignment and the path planning in an online manner.
%so that the mismatch between algorithmic simulation and real-world deployment shall be alleviated. 
we introduce a formal framework to study the combined problem that organically integrates task assignment and path finding in an online manner.
%\textcolor{red}{Rigorously formalize the online problem}
%More specifically,
To solve the formalized problem,
we \textbf{first} develop lifelong path finding algorithms directly for the \texttt{Type}$\odot$ robot model (assuming an arbitrary task assigner), including those adapted from the existing literature and our new rule-based planner which performs both efficiently and effectively, even for non-well-formed instances.
%Note that an additional strong assumption of the aforementioned work is that the layout should better be \textit{well-formed}~\cite{ma2017lifelong},
%which is dropped in our system, since non-well-formed layouts are commonly seen in modern warehouses.
%We illustrate a non-well-formed example in Figure~\ref{fig:eg_non_wf}.
\textbf{Secondly}, we propose a novel formulation that addresses the online problem of task assignment as optimally solving a Markov Decision Process~(MDP), with path planners as hyper-parameters.
Due to the complex state space and transition of the formulated MDP, we resort to approximated solutions by reinforcement learning (RL), as well as other non-trivial rule-based ones with insightful observations.
%To our best knowledge, this is the first compound system that directly tackles both task assigner and path planner simultaneously in an online fashion.
\textit{Simulation experiments on warehouse scenarios at Meituan, one of the largest shopping platforms in China, have shown that our system
(1)~takes only 83.77\% of the execution time needed for the currently deployed system at Meituan, outperforming other SOTA algorithms by 8.09\%; and
(2)~can achieve the same throughput with only 60\% of the agents of the current scale.}
We also draw an important lesson from this study
that both path finding and task assignment should fully exploit the warehouse layout, as it is normally fixed in a relatively long period of time after deployment, though the number of agents may still vary.
To this end, it might be more worthwhile to investigate layout-dependent-agent-independent solutions instead of entirely general-purpose solutions.




This paper is organized as follows.
We first list a few related areas in Section~\ref{sec:related}.
Then the problem formulation is provided in Section~\ref{sec:problem}.
We later present our system in two parts: the path planners in Section~\ref{sec:pf}, and the task assigners in Section~\ref{sec:ta}.
Experimental results are shown in Section~\ref{sec:exp}, mainly conducted for Meituan warehouse scenarios with various scales of agents.
We conclude the paper with a few insightful discussions in Section~\ref{sec:conclusion}.

% Our system


% Organization


\section{Related Work}
\label{sec:related}
%Our work pertains to a broad range of research areas, mainly on planning from the multi-agent community and scheduling from the operations research community.

% 1. path finding
% MA-PF/PD
\textbf{Path Finding.}
The study of MAPF aims to develop centralized planning algorithms.
In spite of the computational complexity being NP-hard in general~\cite{yu2013structure},
researchers have developed practically fast planners that can even solve instances with hundreds of agents within seconds.
Exemplars can be found via reduction to logic programs~\cite{erdem2013general},
prioritized planning~\cite{silver2005cooperative,ma2019searching,okumura2022priority},
conflict-based search~\cite{sharon2015conflict,li2021eecbs}, 
depth-first search~\cite{okumura2023lacam}, etc.
Most of them can be extended to the online version of the problem, i.e. MAPD,
where the goals assigned to agents are priorly unknown~\cite{ma2017lifelong,vsvancara2019online,li2021lifelong}.
%Usually, adaptations are then made via communication between robots and broadcasting by the central controller for the purpose of synchronization.
%A notable drawback is that both MAPF and MAPD assume a task assigner should be given.


% 2. task assignment
% TAPF, Anon-MAPF
% Multi-Goal MA-PF/PD
\textbf{Task Assignment.}
The earliest attempt is the formulation of \textit{Anonymous}-MAPF (AMAPF) that does not specify the exact goal that an agent must go to~\cite{stern2019multi}.
%, but the total number of anonymous goals should be less than or equal to the number of agents
Compared with the labeled version, AMAPF can be solved in polynomial time,
via reduction to max-flow problems~\cite{yu2013multi}, or target swaps~\cite{okumura2023solving}.
As a generalization,
TAPF explicitly associates each agent with a team~\cite{ma2016optimal}, or with a set of candidate goals~\cite{honig2018conflict,tang2023solving},
and eventually computes a set of collision-free paths as well as the corresponding assignment matrix.
Another analogous formulation is called \textit{Multi-Goal} (MG-)MAPF~\cite{surynek2021multi,ijcai2024p0028} and its lifelong variant MG-MAPD~\cite{xu2022multi}, which also associates each agent with a set of goals, but the visiting order is pre-specified.
%Imaginably, the further generalized model should consider that tasks arrive as a priorly unknown sequence and are assigned to agents for real-time path finding.
%To the best of our knowledge, this problem is so far open,
%for which we will present a solution system in this paper.

%Hungarian method~\cite{kuhn1955hungarian}
%allow task swapping~\cite{okumura2023solving}

%% 3. comb. opt., scheduling 
%% Job shop scheduling, vehicle routing
%\textbf{Scheduling.}
%One may notice the analogy between TAPF and job-shop scheduling problems (JSSP)~\cite{manne1960job,jain1999deterministic} or vehicle routing problems (VRP)~\cite{toth2014vehicle,braekers2016vehicle}.
%However, there are two key differences: (1) job durations in JSSP and route lengths in VRP are usually known in advance and (2) the execution of jobs or routes are independent of each other.
%Neither of the two conditions holds in TAPF, especially when the tasks are released online.

%\textcolor{red}{some postponed to Appendix~\ref{apd:more_related_work}}
\textit{We also append some discussion on other less related areas to Appendix~\ref{apd:more_related_work}.}
Despite the rich literature, none of the above directly solves the online problem that a real-world automated warehouse is faced with, which well motivates this work.



\section{Problem Definition}
\label{sec:problem}

%\textcolor{red}{N as a number and a set}

We consider a set of agents $N$, moving along a 4-neighbor grid map given as an undirected graph $G = (V, E)$,
where $V$ is the set of vertices and $E$ is the set of unit-cost edges.
Let $P, D\subseteq V$ denote the set of pickup ports and the set of delivery ports, respectively.
Usually, these two sets are disjoint and are specified alongside the map graph.


Let $k$ starting from 0 denote any arbitrary timestep (to be distinguished from the later notations of tasks).
At each timestep $k$, the local \textit{agent-state} of agent $i$, denoted as $\phi_i^k$, is a 3-tuple consisting of
her current location $l_i^k\in V$,
direction $d_i^k\in \{n,s,w,e\}$,
and goal $g_i^k \in P\cup D$.
$\Phi_i$ is the set of all possible local states of agent $i$, and consequently $\Phi = \prod_{i\in N} \Phi_i$ is the set of all possible \textit{joint agent-states}.
Each agent is associated with a set of four unit-cost actions $A=\{\texttt{stop}, \texttt{forward}, \texttt{$\circlearrowright$90}, \texttt{$\circlearrowleft$90}\}$ (called the \texttt{Type}$\odot$ robot model), with their usual meaning specified using the deterministic function $move$.
For example,
\[
\begin{split}
	move(((3,28), w),\ \ \ \ \ \ \ \ \ \texttt{$\circlearrowright$90}) &\to ((3,28), n) \\
	move(((3,28), e), \texttt{forward}) &\to ((3,29), e)
\end{split}
\]
While planning paths for agents, we need to avoid the following types of collisions.
\begin{definition}[Collision types~\cite{stern2019multi}]
Let $i$ and $j$ be any arbitrary pair of agents,
	\begin{itemize}
		\item Vertex-collision: $l_i^k = l_j^k$,
		\item Edge-collision: $l_i^k = l_j^{k+1} \land l_j^{k} = l_i^{k+1}$,
%		\item Following-conflict: $l_i(k+1) = l_j(k)$.
	\end{itemize}
\end{definition}

%In fact, there is an additional type of conflicts, namely \textit{cycle-conflict}, which is a special case when the group of agents that run into a following-conflict form a cycle.
%Mathematically, a cycle-conflict happens when there exists a subset of agents
%$\{i, i+1, \cdots, j-1, j\}$,
%such that $l_i(k+1) = l_{i+1}(k)\land l_{i+1}(k+1) = l_{i+2}(k)\land \cdots\land l_{j}(k+1) = l_{i}(k)$.
%Note that we do not explicitly include cycle-conflicts in our discussion because forbidding following-conflicts implies forbidding cycle-conflicts.

%\begin{definition}[Lock types]
%ss
%	\begin{itemize}
%		\item Dead-lock.
%		\item Live-lock.
%	\end{itemize}
%\end{definition}

%The so-called tasks are composed of a sequence $I$ of typed items. 
%The type of an item $\iota \in I$ is also known as a \textit{stock keeping unit}\footnote{https://en.wikipedia.org/wiki/Stock\_keeping\_unit} (SKU).
%We let $T$ denote the set of all possible types (SKUs).
The so-called tasks are composed of a sequence $I$ of typed items. 
Each item $\iota \in I$ is associated with a type $t\in T$.
A back-end demand database specifies for each type a subset of delivery ports that items of the type need to be delivered to. 
Here we model such a database as a lookup table $L: T\mapsto 2^D$.
As $L$ will be changed in real-time, we also let $L^k$ denote the demand database at timestep~$k$.
When an agent has finished her last delivery job and returned to a pickup port at timestep $k$,
an item, say of type $t$, will be loaded onto this idle agent.
The system will then check the lookup table $L^k$, choose one target delivery port $g\in L^k(t)$ to assign to this agent, and delete this demand, i.e. $L^{k+1}(t) = L^k(t)\backslash \{g\}$.

Also, we have an assignment table $\eta$ that keeps track of which one of the loaded items is assigned to which agent, i.e., $\eta(\iota) = i$ means the item $\iota$ is currently carried by agent $i$.
An item $\iota$ is \textit{delivered} if there exists a timestep $k$ such that $l_{\eta(\iota)}^k = g_{\eta(\iota)}^k$, i.e., the agent carrying this item has reached her goal.
Upon successful delivery, the item $\iota$ will be deleted from the entry of $\eta$.
As $\eta$ is being changed  over time, we also use the time-indexed version $\eta^t$.
 
%Once an agent is assigned a goal, she cannot swap her goal with that of anyone else, as in real-world warehouses those robots are not equipped with arms and cannot swap packages.
We assume (1) $L$ has recorded the demands of a long enough period of time, and therefore, will not be enlarged;
and (2) an item will be appended to the system
only when it is loaded onto an agent.
%We also assume that no two agents simultaneously arrives at two pickup ports or two delivery ports due to the continuous-time nature of the real-world problem, but we allow the case when one is at a pickup port and the other is at a delivery port.
%Note that this does not imply any two agents will not appear simultaneously at the same location and hence no impossibility of any collisions.
%The above assumption is merely about no two agents will be switched to such a status waiting for assignment.

%(3)~no two agents arrive at their pickup ports simultaneously.
%Therefore, the path planner has no knowledge of the forthcoming items and their types.

With the above notations, we formally define the dynamics of the whole system as a deterministic \textit{system-transition} function over \textit{system-states}.
\begin{enumerate}
	\item
	A \textit{system-state} $\psi$ is a tuple consisting of the joint agent-state $\phi = \{\phi_i\}_{i \in N}$, the lookup table $L$, and the assignment table $\eta$ at that moment. % illegal state?
	Let $\Psi$ denote all possible system states.
	Among them, there is an initial system-state $\psi^0 = (\phi_1^0, \cdots, \phi_N^0, L^0, \eta^0)$.
%	A system state is \textit{legal} if no two agents are at pickup locations simultaneously.
	
	\item
	The space of \textit{system-actions} is $\Omega = (A\cup P\cup D)^N$. That is, a \textit{system-action} $\omega\in \Omega$ is an ordered tuple of the atomic actions of agents where any of them can be substituted by an assignment decision.
	A system-action $\omega$ is \textit{executable} under a legal system-state $\psi$ iff
	\[
	\begin{split}
	 \forall i \in N.\ 
	 & [\l_i \in V\backslash (P\cup D) \land \omega_i \in A] \lor \\
	 & [\l_i \in P \land \omega_i \in D] \lor
	[\l_i \in D \land \omega_i \in P]
	\end{split}
	\]
	
	\item $\Gamma: \Psi \times \Omega \mapsto \Psi$ is the \textit{system-transition} function, which means (1) if no agents are at the pickup/delivery ports, then the system proceeds by deterministically moving agents by their reported actions, which will not change the goal component $g_i$ in each agent-state $\phi_i$; or (2) if any agent arrives at any pickup (resp. delivery) port, then the  system  needs to re-assign the agent the next delivery (resp. pickup) port, which will change the goal of that particular agent to the corresponding new location and temporarily force her to stay in-place, and change the demand table $L$ as well as the assignment table $\eta$ accordingly.
\end{enumerate}
%\textcolor{red}{one item at a time}
An additional minor assumption is, even if two agents arrive at two different pickup (resp. delivery) ports simultaneously,
%they will not be ``switched'' to the status of waiting for new-delivery (resp. pickup) assignments asimultaneously, as in real-world both time and motion execution are continuous.
they will eventually get assigned certain new ports within the next one single timestep one by one in a random order. We do not care about the case where one is waiting for a new-delivery assignment while another is waiting for a new-pickup assignment.

In summary, a \textit{problem instance} is a following tuple $$<N, G, P, D, A, I, L>,$$
and consequently a \textit{principle solution} is threefold:
\begin{enumerate}
	\item $\pi_N: \Phi\mapsto A^N$ is the routing policy that outputs the next action for each agent given their current states. It is unnecessary to compute the entire $\pi_N$ completely upfront, instead, execution can be interleaved with replanning.
	\item $\pi_{D}: \Psi \times 2^D \mapsto D$ is the delivery selection policy which assigns an agent a delivery port among the candidates according to $L(t)$ when she is at one of the pickup ports and given an item of type $t$.
%	Note that $t$ is not one of the input parameters as the loaded item is not within control. 
	\item $\pi_{P}: \Psi \mapsto P$ is the pickup selection policy which assigns an agent a pickup port to return to when she has finished the delivery.
\end{enumerate}
Note that (1) both $\pi_D$ and $\pi_P$ assign one new goal at a time, as we assumed before;
(2) both $\pi_D$ and $\pi_P$ will change the goal of that particular agent to the corresponding port,
while $\pi_N$ will not;
(3) if an agent is at a pickup or delivery port, then her agent-action, even if specified by $\pi_N$, will be overwritten to $\texttt{stop}$ by the decision of $\pi_D$ or $\pi_P$.

%condition for feasible policy%
\begin{definition}[Feasibility]
Given any system-state $\psi$ and the system-transition $\Gamma$, the application of the above solution policy $(\pi_N, \pi_D, \pi_P)$ deterministically outputs a successor system-state $\psi'$. If there is no aforementioned collision between $\psi$ and $\psi'$, then $(\pi_N, \pi_D, \pi_P)$ is a feasible solution.
\end{definition}

%A goal $g\in P\cup D$ is reached under $\pi_N$ if there exists an agent $i$ and a timestep $k$, such that $l_i(k) = g$.
%An item of type $t$ is delivered if the target delivery port assigned by $\pi_{D}$ is reached. 

%TODO: two items with identical goals

%The system ends immediately once it has delivered every item in the online sequence $I$, in a given period of time.
We finally define \textit{makespan} as our evaluation metric.
\begin{definition}[Makespan]
	Given a problem instance with its initial system-state $\psi^0$, and a feasible solution $(\pi_N, \pi_D, \pi_P)$, an execution trajectory will be generated by the sequential applications of the solution policy $\{\psi^0, \psi^1, \cdots\}$. The makespan is the minimum timestep $k$ such that every item in I is delivered at $\psi^k$. 
\end{definition}

However, in real-world warehouses, the pickup ports are usually concentrated in a restricted local area for operational convenience, e.g., in the top right corner of Figure~\ref{fig:eg_non_wf}. Therefore, $\pi_{P}$ is normally implemented for the purpose of balancing the loads over all pickup ports.
\textbf{In this work, we merely aim at a \textit{practical solution} consisting of only $(\pi_N, \pi_D)$, assuming $\pi_{P}$ is given and is not part of the desired solution.}







%\textbf{Evaluation Protocol.}
%Since usually the underlying order database $\mathcal{T}$ is quite large, we will run the system through relatively small sets of tasks to test the elapsed \textit{makespans}, i.e., the number of timesteps to accomplish the given set of tasks.

%\begin{example}[System pipeline]
%As in Figure~\ref{fig:eg_non_wf},
%$\textsc{Robot}_1 \sim \textsc{Robot}_{50}$ initially rest in random locations upon the completion of the system execution the last time. Now the system is launched and every robot starts moving towards the pickup ports, namely $\textsc{Red}_1$ and $\textsc{Red}_2$. Suppose $\textsc{Robot}_{31}$ first reaches $\textsc{Red}_2$, the human operator loads one item, say a dozen of eggs, onto $\textsc{Robot}_{31}$, and the system checks the demand database and finds out that there are three orders requesting a dozen of eggs, with corresponding delivery ports $\textsc{Green}_{69}$, $\textsc{Green}_{142}$, and $\textsc{Green}_{83}$. After deliberate consideration, the system decides to send $\textsc{Robot}_{31}$ to $\textsc{Green}_{83}$ this time (and later ones to the remaining two delivery ports), and plans a path for $\textsc{Robot}_{31}$ to go from $\textsc{Red}_2$ to $\textsc{Green}_{83}$ while avoiding potential collisions with others, and so forth.
%\end{example}

\begin{example}[System pipeline]
As shown in Figure~\ref{fig:eg_non_wf}, $\textsc{Robot}_1 \sim \textsc{Robot}_{50}$ initially rest in random locations after the last system execution. Once the system is launched, each robot moves towards the pickup ports, $\textsc{Red}_1$ and $\textsc{Red}_2$. When $\textsc{Robot}_{31}$ reaches $\textsc{Red}_2$, the human operator loads a dozen eggs onto it. The system checks the demand database and finds three orders for a dozen eggs, with delivery ports $\textsc{Green}_{69}$, $\textsc{Green}_{142}$, and $\textsc{Green}_{83}$. After consideration, the system decides to send $\textsc{Robot}_{31}$ to $\textsc{Green}_{83}$ this time, planning a path while avoiding potential collisions, with subsequent deliveries to the other two ports.
\end{example}

One may see potential connections between our problem and the standard formulation MAPD in the existing literature, \textit{we postpone some remarks elaborating on the differences to Appendix~\ref{apd:relate_to_mapd}, due to the limited space}.
%\begin{remark}[Relation to MAPD]
%In MAPD, an online task $t_i$ is characterized by a pickup port $s_i$ and a delivery port $g_i$ with a priorly unknown release time. Once an agent becomes idle, she will select one task $t^* = (s^*, g^*)$ of her best interest from the released ones, and then plan a path from her current location to $g^*$ through $s^*$.
%Mapping to our settings, an agent becomes idle only when she arrives at a pickup port, and shall be assigned one delivery port from the candidate ones, say $\{g_1, \cdots, g_k\}$.
%Suppose the system will simply pair each delivery port with a pickup port immediately, for which the particular agent will return to after the delivery. Then it is equivalent to, in the language of MAPD, releasing $k$ tasks $\{(g_1, \pi_P(g_1)), \cdots, (g_k, \pi_P(g_k)\}$.
%However, after choosing one from the $k$ tasks and assigning it to an agent, the rest $(k-1)$ tasks will be temporarily removed, or say ``deactivated'', from the pool of released tasks until the next item of the same type arrives.
%\end{remark}


We clarify that in the rest of the paper, by ``path finding'' we mean to compute $\pi_N$, and by ``task assignment'' we mean to compute $\pi_D$.


\section{Path Finding}

\label{sec:pf}

In this section, we first review several representative algorithms that can plan collision-free paths in a lifelong fashion.
However, they are not always effective for resolving collisions under the \texttt{Type}$\odot$ robot model for non-well-formed instances like Figure~\ref{fig:eg_non_wf}.
To this end, we propose a simple-yet-powerful rule-based planner
%that takes advantage of the map layout,
 which is capable of efficiently and robustly moving robots without collisions or deadlocks.

%\begin{definition}[Well-formed instance~\cite{xu2022multi}]
%	The vertices in $P\cup D$ are defined as task endpoints, while the initial locations of agents $\{l_i^0\}_{i \in N}$ are defined as non-task endpoints. A problem instance is well-formed iff (1)~the set of non-task endpoints is disjoint with the set of task endpoints, and (2)~there exists a path between any two endpoints that traverses no other endpoints.
%\label{def:wellform}
%\end{definition}

%\textcolor{red}{why this layout is hard}
%
%\textcolor{red}{not effective examples in Appendix~\ref{apd:pf_notgood_eg}}


\subsection{Existing Lifelong Path Finding Algorithms}

\textbf{Prioritized Planning.}
A straightforward way is to prioritize path finding for each agent by assigning them  distinct priorities, known as \textit{Cooperative A$^*$} (CA$^*$)~\cite{silver2005cooperative}, which can also be extended to lifelong situations.
In descending order of priority, the agents will plan their paths one by one.
Once an agent with a higher priority has found her path, those $(state, time)$ pairs along the path will be \textit{reserved} for this particular agent.
All subsequent agents with lower priorities will view those reservations as states that are unreachable at the corresponding timesteps, i.e. as spatio-temporal obstacles.
Therefore, each agent will need to conduct optimal search over the joint space of state and time.
Understandably,
there is a chance that an agent with a lower priority cannot compute any feasible path given the preceding path computed by a higher-priority, which makes the algorithm itself incomplete.
This situation is even worse under the \texttt{Type}$\odot$ robot model, as an agent often needs to rotate in-place before going to an adjacent vertex, which adds extra difficulty of avoiding collisions. \textit{An illustrative example is provided in Appendix~\ref{apd:pf_notgood_eg}}.

\textbf{Rolling Horizon Collision Resolution.}
A more systematic approach for lifelong path finding is to \textit{window} the search process~\cite{silver2005cooperative}.
This idea is further developed by~\cite{li2021lifelong} as 
the \textit{Rolling Horizon Collision Resolution} (RHCR) framework.
The framework takes two use-specified parameters: (1)~the replanning frequency $h$ and (2)~the length of the collision resolution window $w\geq h$, ensuring that no collisions occur within the next $w$ timesteps.
The framework is general enough to encompass most MAPF algorithms.
An example is to
extend conflict-based search (CBS)~\cite{sharon2015conflict,li2021eecbs} to the lifelong version using this RHCR framework, where the high-level constraint tree is expanded only if there are still collisions within the first $w$ timesteps, resulting in a much smaller constraint tree.
However, under the \texttt{Type}$\odot$ robot model, neighboring agents often require more timesteps to resolve collisions, especially in crowded local areas.
\textit{An example is provided in
Appendix~\ref{apd:pf_notgood_eg}.}

%
%\subsection{Prioritized Planning}
%
%A straightforward way of extending single-agent path finding to multi-agent path finding is to assign each agent a priority ordering, known as \textit{Cooperative A$^*$} (CA$^*$)~\cite{silver2005cooperative}.
%According to descending order of the priorities, the agents will plan their path one by one.
%Once an agent of a higher priority has found her path, those $(state, time)$ pairs along the path will be \textit{reserved} for this particular agent.
%All subsequent agents with lower priorities will see those reservations as states that are unreachable at corresponding timesteps, i.e. as spatio-temporal obstacles.
%In principle, each agent will need to conduct optimal search over the joint space of state-and-time, adding a bit overhead compared with optimal search over only states.
%
%% greedy
%
%Despite of being easy to implement and thus highly extendable, this paradigm is understandably incomplete, as it may not find any collision-free solution given a certain priority ordering whereas the problem instance is indeed solvable.
%A recent study~\cite{ma2019searching} also shows that there exists certain problem instances that are unsolvable for any order of static priorities.
%The authors therefore propose a method called \textit{Priority-Based Search} (PBS), which resembles the two-level method \textit{Conflict-Based Search} (CBS) but searches for a feasible priority ordering in its high level.
%
%\textcolor{red}{incomplete, if higher pri plans first while the lower one got no way to go}
%
%Note that all of the above mentioned algorithms assume that (1) each agent will serve as an obstacle when she reaches the goal, and (2) the system should terminate only when all agents arrives at their goals.
%Adaptations need to be made in situations of lifelong path finding, where agents are repeatedly assigned new goal locations.
%For decoupled algorithms like CA$^*$, one can easily reuse the reservation table for the previously planned paths, and plan new paths for the currently idle agents that are assigned new goals.
%For centralized algorithms like PBS, the central controller has to replan for \textit{all the agents} even if only one agent is assigned a new goal.
%% must be a different one from anyone else
%
%In this paper, we only implement the lifelong CA$^*$ for the \texttt{Type}$\odot$ robot model as one of the baselines of prioritized planning due to its flexibility, later denoted as \textbf{PP}.
%
%
%
%\subsection{Rolling Horizon Collision Resolution}
%A more systematic approach to extending one-shot MAPF algorithms to lifelong ones is to \textit{window} the search process~\cite{silver2005cooperative}.
%This idea is further developed by~\cite{li2021lifelong} as 
%the \textit{Rolling Horizon Collision Resolution} (RHCR) framework.
%The framework takes two use-specified parameters: (1)~the replanning frequency $h$ and (2)~the length of the collision resolution window $w$, ensuring $h \leq w$.
%The framework is general enough to encompass most of the MAPF algorithms, for example
%\begin{enumerate}
%	\item For CA$^*$, the state-time reservations made for the previously planned agents with higher priorities will only be effective within the first $w$ timesteps, and therefore, the size of the reservation table will reduce from $|states| \times T_{max}$ to $|states| \times w$. 
%	\item For CBS, the high-level contraint tree will be extended only if there are still collisions happening within the first $w$ timesteps, and therefore, the depth the constraint tree will be much smaller, similar for PBS.
%\end{enumerate}
%In this paper, we only implement RHCR upon CBS for our \texttt{Type}$\odot$ robot model due to 
%its completeness\footnote{CBS is complete for one-shot MAPF instances. Note that even with certain deadlock avoidance mechanisms mentioned in~\cite{li2021lifelong} the RHCR framework is still incomplete.} in theory, later denoted as \textbf{RHCR-CBS}.
%
%may need to reduce to $w\gets \infty$ when congestion around pickup ports
%

\subsection{Our Solution: Touring With Early Exit}



% traffic jam near pickup ports leads to failures or time-outs for the existing algorithms.




Instead of doing deliberate planning, we here present a simple-yet-effective rule-based planner, named \textit{Touring With Early Exit} (later denoted as \textbf{Touring} for short).
As summarized in Algorithm~\ref{alg:touring},
this planner consists of three main rules \textsc{Rule1-touring}, \textsc{Rule2-early-exit}, and \textsc{Rule3-safe}.
We will explain them one by one.

%\vspace{-3mm}
\begin{algorithm}[tb]
\small
    \caption{Touring with early exit}
    \label{alg:touring}
    \textbf{Input}: $states = (\{l_i\}_{i\in N}, \{d_i\}_{i\in N}, \{g_i\}_{i\in N})$\\
    \textbf{Parameter}: for any arbitrary timestep $k$, omitted below\\
    \textbf{Output}: next joint-action $actions$
    \begin{algorithmic}[1] %[1] enables line numbers
        \STATE $actions \gets \textsc{Rule1-touring}(states)$
        %\textcolor{red}{double-check}
        \IF{$\textsc{Exists-deadlock}(states)$}
        	\STATE $actions \gets \textsc{Rule3-safe}(states, actions)$
        	\STATE \textbf{return} actions
        \ENDIF
        \STATE $actions \gets \textsc{Rule2-early-exit}(states, actions)$
        \STATE $actions \gets \textsc{Rule3-safe}(states, actions)$
        \STATE \textbf{return} actions
    \end{algorithmic}
\end{algorithm}

\textbf{Firstly}, a tour $\tau$ is defined as a simple cycle in the map graph $G$.
Let $V_\tau\subset V$ denote the vertices in $\tau$.
For \textsc{Rule1-touring}, we partition the graph into disjoint tours $\{\tau_p\}_{p\in P}$, ensuring that each tour covers distinct pickup ports, i.e., 
\[
\begin{split}
& \big( \forall p_1, p_2 \in P.\
p_1 \in \tau_1 \land p_2\in \tau_2 \land \tau_1 \neq \tau_2 \iff p_1 \neq p_2 \big) \\
%\land
%& \big( \forall p_1, p_2 \in P.\
%p_1 \in \tau \land p_2\in \tau \implies p_1 = p_2 \big) \\
& \land
\bigcup_{p\in P} V_{\tau_p} = V \land \bigcap_{p\in P} V_{\tau_p} = \emptyset\\
\end{split}
\]
\textsc{Rule1-touring} further specifies the fixed direction along which agents will traverse the tour regardless of their goal locations, i.e. blind touring.
Figure~\ref{fig:touring}(a) shows an example with two tours (in dashed orange), one covering \textsc{F2} clockwise and the other covering \textsc{G2} counter-clockwise.
An agent may need more than one action at certain cells for touring, e.g., need a \texttt{$\circlearrowleft$90} followed by a \texttt{forward} at the corner \textsc{A4}.

\textbf{Secondly}, for each tour $\tau$, $\textsc{Rule2-early-exit}$ marks certain vertices as turnings, where an agent currently in $\tau$ can exit the tour. The set of turnings is denoted as $V_\tau^{turn} \subseteq V_\tau$. 
An agent $i$ can \textit{exit} her tour $\tau$ if she is at the turning
%\textcolor{red}{along the touring direction}
that is the closest to her goal by choosing the next action of her shortest plan towards the goal,
%i.e., $l_i \in \arg\min_{l\in V_\tau^{turn}}\textsc{ShortestDistance}(l, g_i)$, 
or continue touring otherwise. Note that it may not be the case that $g_i \in V_\tau$, which may require agents to go across tours.
An exiting action is prioritized over a touring action.
Two agents who are exiting their own tours simultaneously but moving towards each other will be prioritized by their IDs:
the one with the larger ID will exit, while the other continues touring until reaching the next second-best turning.
The blue cells in Figure~\ref{fig:touring} represent the turnings of the respective tours, with \ref{fig:touring}(b) and \ref{fig:touring}(c) illustrating the two aforementioned prioritized cases.
These turnings can be either user-specified, or searched in terms of minimizing the makespan.
\textit{We provide an illustration of how the frequency of the turnings affects the eventual makespan
in Appendix~\ref{apd:param_search}}.

\textbf{Finally}, \textsc{Rule3-safe}
is applied to revise those actions to collision-free ones.
%Imaginably, if a preceding agent is rotating, then the following agent should not go \texttt{forward}, otherwise collisions will happen.
For example, if a preceding agent is rotating, the following agent should not move \texttt{forward}; otherwise, collisions may occur.
Thus,
we design \textsc{Rule3-safe} conservatively: 
for each agent $i$ (1)~she observes her adjacent agents but assumes their actions specified by the prior rules may or may not be executed successfully, (2)~for either outcome, she checks whether her next action, if it is \texttt{forward}, will lead to a collision, (3) if any potential collision is detected, she revises her action to \texttt{stop}. 
Intuitively, this ensures that every agent maintains a safe distance from one another.
%Each agent is associated with a fixed priority, say her identity number, and each action is also tagged by the number of the last rule that revise it, i.e. 1 or 2.
%The eventual priority is an ordered tuple of (agent-priority, action-priority) and the comparison reduces to the comparison between ordered tuples. Suppose two agents are going against each other, possible cases are (1) if they are both about to cross tours, then only the one with higher agent-priority will make it and the other one continues touring until the next second-best turning; (2) if one is about to turn and the other one is touring, then the latter one with a lower action-priority will wait until the former one finishes her turn.
%\textcolor{red}{actually implies no following-collision~\cite{stern2019multi}}
\textit{In fact, this conservative rule also prevents potential following-collisions} (which will not be discussed in this paper; see~\cite{stern2019multi}).

\textbf{Last but not least},
one may notice that
if there is a subset of agents forming a cycle where each one is about to go to the next location that is currently occupied by another agent in the cycle, \textsc{Rule3-safe} will overwrite the actions of all involved agents to \texttt{stop}, resulting in a deadlock.
Since the planner consisting of only the three main rules is merely a one-step reactive planner, the identical planning step will repeat indefinitely once a deadlock is formed.
Therefore, additional inspections need to be made (Line 2 in Algorithm~\ref{alg:touring}),
within which a depth-first search is conducted to see if any cycles (and thus the deadlock) exist.
 Once a deadlock is found, all the \textit{exiting} agents will be interrupted and resume \textit{touring}.

%
%\textcolor{red}{As we mentioned, in real-world warehouses, agents may encounter a variety of contingencies like failing to execute planed actions or (un)load packages.}
%Hence, \textsc{Rule3-safe} is to simply revise those action profiles that may cause following-conflict to safe ones. Formally, given an action profile $\{a_i\}_{i\in N}$ at timestep $k$,
%if 
%$\exists j.\ l_j^k = move(i, a_i)$, then $a_i$ will be revised to \texttt{stop}.
%Note that this rule will only affect \texttt{forward} actions, as in-place spinning does not move agents to different locations.
%
%Given the above three main rules, the planner is still incomplete as it may stuck in local areas.
%We further polish the rules in two aspects.
%\begin{enumerate}
%	\item \textbf{Priorities.} Each agent is associated with a fixed priority, say her identity number, and each action is also tagged by the last rule that revise it. The eventual priority is an ordered tuple of (agent-priority, action-priority) and the comparison reduces to the comparison between ordered tuples. That is, suppose there are two agents go against each other, (1) if they are both about to cross tours, then only the one with higher agent-priority will make it and the other one continues touring until the next second-best turning; (2) if one is about to turn and the other one is touring, then the latter one with a lower action-priority will wait until the former one finishes her turn.
%
%	\item \textbf{Deadlocks.} \textcolor{red}{like how?} Since the planner is a purely \textcolor{red}{one-step planner} reactive one, understandably it may cause deadlocks. Once a deadlock is formed, it will never be escaped automatically as the identical planning step will repeat forever. Therefore, additional inspections are made (Line 2 in Algorithm~\ref{alg:touring}). Once a deadlock is detected \textcolor{red}{by DFS}, all the turnings will be interrupted and everyone continues touring (along a fixed direction and thus no deadlock).
%\end{enumerate}
%


\begin{figure}[tb]
\vspace{-3mm}
\centering
\includegraphics[width=80mm]{fig/touring}
\caption{Illustration of \textsc{Rule1-Touring} (a) and two prioritized cases (b, c).
%\textcolor{red}{more elaboration}
Colored boxes are the goals.}
\label{fig:touring}
\vspace{-3mm}
\end{figure}

Our \textbf{Touring} planner  eliminates potential collisions by implementing safety rules and avoids deadlocks in real-time. The worst case is to continue touring until the goal is reached.
Hence, our \textbf{Touring} planner is both \textit{sound} and \textit{complete} as
(1)~it will not cause any vertex-collision or edge-collision, %(or even following-collision),
and (2)~every item will be delivered in finite number of steps.

%
%PRIMAL~\cite{sartoretti2019primal,damani2021primal}
%
%\textcolor{red}{Although our experiments only focus on the specific layout, that is because there is only reference value at Meituan. But the design principle is obvious and to some extent general, though simple}


\subsection{Comparison for Path Finding Algorithms}
Before adding task assignment to the context, here we first conduct a brief comparison among the above path finding algorithms, assuming a sequence of goals arrives online.
We implement the lifelong CA$^*$ as a baseline for prioritized planning (denoted as \textbf{PP}),
and CBS under the RHCR framework with $h=1, w=5$ as a baseline for windowed search (denoted as \textbf{RHCR-CBS}).
We also implement two heuristics for the underlying single-agent search, namely $h_{slow}$, which merely computes the Manhattan distance between the current location and the goal, and $h_{fast}$, which additionally counts the minimum number of \texttt{$\circlearrowleft$90}/\texttt{$\circlearrowright$90} needed. Hence, here we have $2\times2=4$ combined baselines. \textit{We report the computing time, even for various scales, in Appendix~\ref{apd:comp_time}.} It turns out \textbf{RHCR-CBS-$h_{slow}$} is too costly for a multi-run evaluation.

As we mentioned, our testing environment (Figure~\ref{fig:eg_non_wf}) may not be well-formed.
\textbf{PP} may fail due to improper priority orderings.
\textbf{RHCR-CBS} may also take a long time for collision resolution, especially when there is a traffic jam near the pickup ports.
We treat a replanning of \textbf{RHCR-CBS} as failure if the number of leaf nodes in the high-level constraint tree exceeds 50, indicating severe congestion.
Once these two methods fail, they will be temporarily switched to \textbf{Touring}, and later be switched back.

In Figure~\ref{fig:pf_vp}, we present the entire distributions of the tested makespans over multiple runs.
As one can clearly see that our \textbf{Touring} planner largely outperforms the other three, and the computing time is entirely negligible compared to the others, as reported in Appendix~\ref{apd:comp_time}.
Besides, \textbf{RHCR-CBS} outperforms \textbf{PP} in most cases, though the average performances are close, as it poorly handles some extreme cases.



%\begin{table}[tb]
%\centering
%\begin{tabular}{@{}lr@{}}
%\toprule
%(50 agents)                      & \textbf{Planning Time (s)} \\ \midrule
%\textbf{Touring (ours)}             &             0.017                    \\
%\textbf{PP $h_{fast}$}       &             0.112               \\
%\textbf{PP $h_{slow}$}       &             0.170               \\
%\textbf{RHCR-CBS $h_{fast}$} &             1.581               \\
%\textbf{RHCR-CBS $h_{slow}$} &             5.995                \\ \bottomrule
%\end{tabular}
%\caption{Average (re)planning time \textit{per step} in 50-agent Meituan warehouse simulation throughout multiple runs.}
%\label{tab:plan_time}
%\end{table}

\begin{figure}[tb]
\vspace{-4mm}
\centering
\includegraphics[width=80mm]{fig/pf_vp}
\caption{The tested makespans of lifelong path finding algorithms in 50-agent Meituan warehouse scenarios. Dotted lines represent the 25-/75-quantiles, and white dots are the means. The means correspond to the leftmost column of the 50-agent scenario in Table~\ref{tab:eval_full}. \underline{416.09} is the reference makespan by Meituan's current system.}
\label{fig:pf_vp}
\vspace{-3mm}
\end{figure}




\section{Task Assignment}
\label{sec:ta}

In the offline setting where tasks are known \textit{a priori}
the assignment problem is well-studied~\cite{ma2016optimal,honig2018conflict,liu2019task,tang2023solving}, 
where the combinatorial search of the best task assignment is coupled with path finding.
However, when it comes to the online setting, it seems that the best approach so far is to greedily assign tasks at each decision point~\cite{ma2017lifelong,okumura2022priority}, i.e., to pick up the task so as to minimize the path costs from the current location to starting location of the task.
Projecting onto our settings, a greedy assignment is to select among those candidates the delivery port that is closest to the current location.
However, no evidence has witnessed that greedy assignments are rational and effective, given the fact that forthcoming tasks are totally unknown.
To this end, we extend this greedy strategy into a broader class of strategies, divided into three categories
(1)~stateless assignment, (2)~adaptive assignment, and (3)~predictive assignment.

%By a classic lesson taken from CPU process scheduling in operating systems,
%an obvious drawback of such greedy assignment is that there might always be certain tasks that keep being preempted. 
%One can also simply argue that committing to a greedy strategy does not make any sense if the future is even unknown.
%To this end, we extend this naive greedy strategy to a broader class of strategies, in three categories (1) stateless assignment, (2) adaptive assignment, and (3) predictive assignment.

\subsection{Stateless Assignment}

%A stateless assignment makes no use of any system-state information.
As shown in Algorithm~\ref{alg:statless}, \textsc{MeasureFunc} is a user-specified function that encodes a measure between the location of the current agent waiting for assignment and the candidate delivery ports. Straightforward options are
\begin{enumerate}
	\item \textbf{Shortest distances}. This reduces to the greedy strategies that select the closest delivery port.
	\item \textbf{Negative shortest distances}. This is equivalent to selecting the farthest delivery port. It is usually counter-intuitive, but makes some sense since it may alleviate congestion around the pickup ports, especially when the scale of the agents is large.
	\item \textbf{Random numbers}. It reduces to random assignments.
\end{enumerate}

%\vspace{-5mm}
\begin{algorithm}[tb]
%\vspace{-5mm}
\small
    \caption{Stateless assignment}
    \label{alg:statless}
    \textbf{Input}: agent $i$ waiting for assignment, item $\iota$ of type $t$, candidate delivery ports $L(t)$\\
    \textbf{Parameter}: any arbitrary timestep $k$ (omitted below)\\
    \textbf{Output}: A selected goal $\in L(t)$ % $g^* \in L(t)$
    \begin{algorithmic}[1] %[1] enables line numbers
%        \STATE $g^* \gets 
        \STATE \textbf{return} $\arg\min_{g \in L(t)} \textsc{MeasureFunc}(g, l_i)$
%        \STATE \textbf{return} $g^*$
    \end{algorithmic}
\end{algorithm}
%\vspace{-3mm}





\subsection{Adaptive Assignment}

Stateless assignments do not make use of system-state information, e.g., the current locations of all agents.
As revealed in Figure~\ref{fig:occu_ratio}, the occupation ratio, defined as the fraction of the number of agents over the number of passable cells,
of the left half differs significantly from that of the right half.
The \textit{closest-first} strategy will inevitably cause high-level congestion around the pickup ports, while \textit{farthest-first} strategy unnecessarily sends agents to farther locations, even though it alleviates traffic jams.
The random strategy somehow
%unconsciously
balances between the former two.

Inspired by this insight, Algorithm~\ref{alg:adaptive} further develops an adaptive version, which takes in a congestion threshold $\alpha$ and makes dynamic assignment decisions based on the current state. If there is a heavy traffic in the right half of the map, the system will send agents to farther goals, and similarly otherwise. One can clearly see in Figure~\ref{fig:occu_ratio}(d) that the occupation ratio fluctuates more responsively.

The threshold parameter $\alpha$ can be specified by users or searched in terms of minimizing the makespan.
\textit{We report comprehensive search results in
Figure~\ref{fig:alpha_bp} in Appendix~\ref{apd:param_search}.}


\begin{algorithm}[t]
\small
    \caption{Adaptive assignment}
    \label{alg:adaptive}
    \textbf{Input}: agent $i$ waiting for assignment, item $\iota$ of type $t$, candidate delivery ports $L(t)$, all agents' locations $\{l_i\}_{i\in N}$\\
    \textbf{Parameter}: A threshold $\alpha$, any timestep $k$ (omitted below)\\
    \textbf{Output}:  A selected goal $\in L(t)$ % $g^* \in L(t)$
    \begin{algorithmic}[1] %[1] enables line numbers
    	\STATE $occu_{l}, occu_{r} \gets \textsc{OccupationRatio}(\{l_i\}_{i\in N})$
    	\IF{$occu_{r} \leq \alpha$}
%    		\STATE $g^* \gets
    		\STATE \textbf{return} $\arg\min_{g \in L(t)} \textsc{ShortestDistance}(g, l_i)$
%    	\ELSIF{s}
%    		\STATE s
		\ELSE
%			\STATE $g^* \gets
			\STATE \textbf{return} $\arg\max_{g \in L(t)} \textsc{ShortestDistance}(g, l_i)$
    	\ENDIF
%        \STATE \textbf{return} $g^*$
    \end{algorithmic}
\end{algorithm}

\begin{figure}[t]
\vspace{-2mm}
\centering
\includegraphics[width=85mm]{fig/occu_ratio}
\caption{The dynamics of the occupation ratios for different assignment strategies in 50-agent cases. Dashed lines represent the means.}
\vspace{-1mm}
\label{fig:occu_ratio}
\vspace{-1mm}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\vspace{5mm}
\begin{table*}[tb]
\vspace{-6mm}
\small
\begin{tabular}{@{}lrrrrrrrrr@{}}
\toprule
(30 agents)                  & \textbf{Random}      & \textbf{Closest}      & \textbf{Farthest}     & \textbf{Adapt$^{1st}$} & \textbf{Adapt$^{2nd}$} & \textbf{Adapt$^{3rd}$} & \textbf{RL$^{avg}$}  & \textbf{RL$^{best}$}  & \textbf{RL$^{worst}$}  \\ \midrule
\textbf{Touring (ours)}      & 467.00               & \textcolor{orange}{$\textbf{412.90}$}               & 530.85               & $443.45^{0.158}$       & $454.15^{0.152}$       & $454.15^{0.155}$       & 425.00                  & $425.00^{412}$          & $425.00^{439}$          \\
\textbf{PP $h_{slow}$}       & 659.45               & $\textbf{550.75}$               & 678.50               & $587.70^{0.158}$        & $590.10^{0.149}$        & $605.30^{0.146}$        & \cellcolor[HTML]{EFEFEF}569.45               & \cellcolor[HTML]{EFEFEF}$569.45^{482}$       & \cellcolor[HTML]{EFEFEF}$569.45^{762}$       \\
\textbf{PP $h_{fast}$}       & 641.30               & $\textbf{561.50}$               & 681.50               & $589.10^{0.152}$        & $589.50^{0.155}$        & $610.20^{0.149}$        & \cellcolor[HTML]{EFEFEF}588.40                & \cellcolor[HTML]{EFEFEF}$588.40^{492}$        & \cellcolor[HTML]{EFEFEF}$588.40^{800}$        \\
\textbf{RHCR-CBS $h_{fast}$} & 645.00               & $\textbf{539.75}$               & 726.05               & $645.20^{0.152}$        & $646.20^{0.155}$        & $655.10^{0.146}$        & \cellcolor[HTML]{EFEFEF}641.95               & \cellcolor[HTML]{EFEFEF}$641.95^{495}$       & \cellcolor[HTML]{EFEFEF}$641.95^{800}$     \vspace{1.5mm}  \\
%                             &                      &                      &                      &                        &                        &                        &                      &                      &                      \\
(40 agents)                  &                      &                      &                      &                        &                        &                        &                      &                      &                      \\ \midrule
\textbf{Touring (ours)}      & 392.10               & 376.30               & 422.70               & $382.40^{0.219}$        & $387.30^{0.211}$        & $387.30^{0.215}$        & \textcolor{orange}{$\textbf{372.05}$}               & $372.05^{348}$       & $383.65^{399}$       \\
\textbf{PP $h_{slow}$}       & 474.50               & $\textbf{427.10}$               & 518.35               & $443.50^{0.215}$        & $447.20^{0.211}$        & $449.05^{0.207}$       & \cellcolor[HTML]{EFEFEF}452.80                & \cellcolor[HTML]{EFEFEF}$452.80^{425}$        & \cellcolor[HTML]{EFEFEF}$473.90^{565}$        \\
\textbf{PP $h_{fast}$}       & 467.00               & $\textbf{426.70}$               & 516.40               & $443.70^{0.219}$        & $449.15^{0.215}$       & $451.25^{0.211}$       & \cellcolor[HTML]{EFEFEF}445.20                & \cellcolor[HTML]{EFEFEF}$445.20^{417}$        & \cellcolor[HTML]{EFEFEF}$475.45^{535}$       \\
\textbf{RHCR-CBS $h_{fast}$} & 463.00               & 444.95               & 523.75               & $438.85^{0.211}$       & $438.85^{0.215}$       & $443.10^{0.219}$        & \cellcolor[HTML]{EFEFEF}$\textbf{431.55}$               & \cellcolor[HTML]{EFEFEF}$431.55^{394}$       & \cellcolor[HTML]{EFEFEF}$447.05^{481}$    \vspace{1.5mm}   \\
%                             &                      &                      &                      &                        &                        &                        &                      &                      &                      \\
\textcolor{teal}{\textbf{(50 agents)}}         &                      &                      &                      &                        &                        &                        &                      &                      &                      \\ \midrule
\textcolor{teal}{\textbf{Touring (ours)}}      & 362.55               & 358.35               & 375.15               & \textcolor{orange}{$\textbf{348.55}^{0.235}$}       & $349.35^{0.265}$       & $349.80^{0.240}$         & 350.15               & $352.70^{316}$        & $350.15^{363}$       \\
\textcolor{teal}{\textbf{PP $h_{slow}$}}       & 424.65               & 392.85               & 435.45               & $\textbf{388.35}^{0.280}$        & $392.65^{0.275}$       & $400.55^{0.255}$       & \cellcolor[HTML]{EFEFEF}409.95               & \cellcolor[HTML]{EFEFEF}$397.10^{359}$        & \cellcolor[HTML]{EFEFEF}$409.95^{509}$       \\
\textcolor{teal}{\textbf{PP $h_{fast}$}}       & 410.70               & $\textbf{390.15}$               & 434.20               & $396.70^{0.280}$         & $399.40^{0.265}$        & $400.15^{0.260}$        & \cellcolor[HTML]{EFEFEF}398.25               & \cellcolor[HTML]{EFEFEF}$402.70^{361}$        & \cellcolor[HTML]{EFEFEF}$398.25^{444}$       \\
\textcolor{teal}{\textbf{RHCR-CBS $h_{fast}$}} & 409.60               & 401.00               & 415.90               & $384.25^{0.280}$        & $385.20^{0.275}$        & $386.10^{0.265}$        & \cellcolor[HTML]{EFEFEF}384.90                & \cellcolor[HTML]{EFEFEF}$\textbf{382.20}^{363}$        & \cellcolor[HTML]{EFEFEF}$384.90^{468}$    \vspace{1.5mm}    \\
%                             &                      &                      &                      &                        &                        &                        &                      &                      &                      \\
(60 agents)                  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}   & \multicolumn{1}{l}{}   & \multicolumn{1}{l}{}   & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\ \midrule
\textbf{Touring (ours)}      & 350.60                & 352.50                & 352.40                & \textcolor{orange}{$\textbf{335.50}^{0.281}$}        & $337.10^{0.293}$        & $337.10^{0.299}$        & 342.70               & $342.70^{308}$        & $342.70^{359}$        \\
\textbf{PP $h_{slow}$}       & 390.90                & 380.45               & 411.00                & $\textbf{369.35}^{0.287}$       & $370.2^{0.293}$        & $373.10^{0.329}$        & \cellcolor[HTML]{EFEFEF}375.10                & \cellcolor[HTML]{EFEFEF}$375.10^{345}$        & \cellcolor[HTML]{EFEFEF}$375.10^{403}$        \\
\textbf{PP $h_{fast}$}       & 394.15               & 382.80                & 397.15               & $\textbf{371.75}^{0.299}$       & $372.65^{0.329}$       & $378.45^{0.311}$       & \cellcolor[HTML]{EFEFEF}391.05               & \cellcolor[HTML]{EFEFEF}$391.05^{356}$       & \cellcolor[HTML]{EFEFEF}$391.05^{499}$       \\
\textbf{RHCR-CBS $h_{fast}$} & 372.50                & 370.00                & 375.90                & $\textbf{357.35}^{0.305}$       & $360.55^{0.287}$       & $360.85^{0.323}$       & \cellcolor[HTML]{EFEFEF}372.85               & \cellcolor[HTML]{EFEFEF}$372.85^{354}$       & \cellcolor[HTML]{EFEFEF}$372.85^{469}$   \vspace{1.5mm}    \\
%                             &                      &                      &                      &                        &                        &                        &                      &                      &                      \\
(70 agents)                  &                      &                      &                      &                        &                        &                        &                      &                      &                      \\ \midrule
\textbf{Touring (ours)}      & 346.45               & 354.65               & 344.50                & \textcolor{orange}{$\textbf{333.40}^{0.353}$}        & $333.60^{0.339}$        & $334.10^{0.360}$         & 338.80                & $338.80^{308}$        & $338.80^{354}$        \\
\textbf{PP $h_{slow}$}       & 375.95               & 381.15               & 393.85               & $374.95^{0.325}$       & $375.40^{0.388}$        & $375.95^{0.304}$       & \cellcolor[HTML]{EFEFEF}$\textbf{373.50}$                & \cellcolor[HTML]{EFEFEF}$373.50^{347}$        & \cellcolor[HTML]{EFEFEF}$373.50^{394}$        \\
\textbf{PP $h_{fast}$}       & 371.25               & 372.10                & 372.10                & $\textbf{364.95}^{0.332}$       & $364.95^{0.360}$        & $365.35^{0.304}$       & \cellcolor[HTML]{EFEFEF}390.90                & \cellcolor[HTML]{EFEFEF}$390.90^{340}$        & \cellcolor[HTML]{EFEFEF}$390.90^{513}$        \\
\textbf{RHCR-CBS $h_{fast}$} & 362.50                & 377.20                & 365.55               & $\textbf{351.90}^{0.367}$        & $353.30^{0.353}$        & $354.10^{0.381}$        & \cellcolor[HTML]{EFEFEF}362.20                & \cellcolor[HTML]{EFEFEF}$362.20^{337}$        & \cellcolor[HTML]{EFEFEF}$362.20^{435}$        \\ \bottomrule
\end{tabular}
\vspace{-1mm}
\caption{Evaluation results. The numbers are the average makespans ($\downarrow$).
The reference makespan is \underline{416.09} by the currently deployed system at Meituan. 
For each path planner, the result performed by the best task assigner is marked in \textbf{bold}. 
The scenario in \textcolor{teal}{teal} is the current scale at Meituan. 
Those in \textcolor{orange}{orange} represent the best combinations at each scale.
Some cells are marked in \textcolor{gray}{gray} as the RL policies are not explicitly trained for those scenarios.
For the adaptive assignment strategies, we report the top-3 ones along with the corresponding thresholds in superscripts.
For the RL strategies, besides the best ones of average performances, we also present the best ones of best-case (resp. worst-case) performances, along with the corresponding best-case (resp. worst-case) makespans in superscripts.}
\label{tab:eval_full}
\vspace{-1mm}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Predictive Assignment}
\label{sec:ta_rl} 
One can further argue that purely reactive assignments like the above ones do not capture the dynamics of the system.
%A natural debate is, \textit{even if the system has monitored that a certain area is currently of low-level congestion, (1) is it a good idea to send agents there, and (2) will it be still vacuum when the agent gets there?}
To this end, one needs to make good use of the underlying path finding module which may hint about the dynamic flow of the agent swarm.

A systematic way is to formulate the assignment problem as a Markov Decision Process (MDP), taking the path finding module, i.e. the routing policy $\pi_N$, as a hyperparameter.
The MDP is defined as a 5-tuple
$<\mathcal{S}, \mathcal{A}, T, R, \gamma>$:
\begin{enumerate}
	\item \textbf{States} $\mathcal{S}$: each $S \in \mathcal{S} \subset \Psi$ is a collection of all system-states where there exists an agent at a pickup port waiting for a new-delivery assignment. We call these \textit{assignment states} to avoid ambiguity.
%	\item \textbf{States} $\mathcal{S}$: each $S \in \mathcal{S}$ is a collection of all individual agents' instantaneous local states, including their locations, directions, etc. Not all such joint-states are in $\mathcal{S}$, only those where there exists an agent waiting for task assignment are valid ones. We call them \textit{assignment states} to avoid ambiguity.
	\item \textbf{Actions} $\mathcal{A} = D$: all possible delivery ports. Given a loaded item of type $t$, the available actions are the delivery ports in $L(t)$.
%	\item \textbf{Transition function} $T: \mathcal{S}\times \mathcal{A} \mapsto \Delta(\mathcal{S})$. Once a valid task is assigned to an idle agent, the system will proceed according to the underlying path finding module until the next assignment state. The transition is implicitly stochastic since there might be intermediate contingencies between two assignment states, e.g., failing to execute plans or unload packages by certain robots.
	\item \textbf{Transition function} $T: \mathcal{S}\times \mathcal{A} \mapsto \Delta(\mathcal{S})$. Once a new delivery port is assigned to an agent, the system will proceed according to $\pi_N$ (and the given $\pi_P$) until the next assignment state. 
	\item \textbf{Reward function} $R: \mathcal{S}\times \mathcal{A} \times \mathcal{S} \mapsto \mathbb{R}$. The reward is the negative time cost spent between two assignment states. Note that the reward signals are quite ``delayed'', in the sense that for two adjacent assignment states, the immediate reward received at the latter one might not reflect the delivery cost for the task assigned in the former state (it is usually not delivered yet). Nevertheless, the total accumulated rewards of an episode is indeed the negative makespan to complete the attended item sequence.
	\item \textbf{Discount factor} $\gamma \in (0, 1)$.
\end{enumerate}
Note that to solve the above MDP is to search for the policy $\pi_D$ while fixing $\pi_N$.
%The solution concept here is an assignment (possibly randomized) policy $\pi: \mathcal{S}\mapsto \mathcal{A}$ that maximizes the expected accumulated rewards. 
By definition, once the optimal policy $\pi_D^*$ is found, it will instruct the system to assign tasks at any possible assignment state, and therefore, the initial resting locations of agents
%(where the system is launched from)
do not matter.
We adopt PPO~\cite{schulman2017proximal} as the RL algorithm to solve the above MDP, \textit{and will postpone further training details to Appendix~\ref{apd:rl_train}}. 

% online?





\section{Experimental Results}
\label{sec:exp}

%\textcolor{red}{why ours is way better: coincide with universal plans}

In this section, we report the main experimental results in Table~\ref{tab:eval_full}, conducted on Meituan simulated warehouse scenarios.
The online sequences of items are retrieved from roughly 5-minute segments of the system's log containing around $140$ items % 135
of approximately 50 types, % 46
while the demand database $L$ is made from mobile orders made by the customers in a longer period of around 6 hours. % 23:40-17:52
The two dimensions of this table represent the choices of path planners and task assigners, respectively.
%\textcolor{red}{tested by Meituan log data}
The numbers are the average \textit{makespans} over multiple runs launched with agents initialized in random locations.
In each run, the system is required to deliver a sequence of items of a fixed length that arrives online.
In other words, we evaluate the average cost it takes to accomplish the same amount of throughput, for each pair of path planners and task assigners.
For the adaptive assignment strategies, we report the top-3 ones along with the corresponding thresholds in superscripts.
For the RL strategies, besides the best ones for average performances, we also present the best ones for best-case (resp. worst-case) performances, along with the corresponding best-case (resp. worst-case) makespans in superscripts.
We only train RL policies over the \textbf{Touring} planner, since the others are not fast enough and will take a tremendous amount of time for RL training.
However, we can slightly abuse a trained assignment policy by testing it with the other three path planners as the state spaces are same.

\underline{As an overview},
our \textbf{Touring} planner outperforms the other three regardless of the task assigner.
We highly conjecture that this planner, to some extent, coincides with a near-optimal universal plan~\cite{zhu2023computing}; \textit{see more discussion in Appendix~\ref{apd:more_related_work}}.
As for the task assigner,
(1)~when the number of agents is $\geq 50$,
adaptive strategies are surprisingly effective, even slightly better than RL ones, and the \textit{closest-first} strategy is not necessarily better than the \textit{farthest-first} strategy.
(2)~when the number of agents is $< 50$,
it might be redundant to use adaptive strategies as the density of agents is quite low; instead, stateless ones or RL ones are better choices.
Although RL strategies can achieve comparable performances in practice (even optimal performance in theory if trained well),
it depends on the user whether the training cost is a worthwhile effort.

%\textcolor{red}{farthest v.s. closest not much difference}

\underline{Looking closer}, we point out two insights:
\begin{enumerate}
\item \textit{Time efficiency}.
Regarding the current scale of Meituan (50 agents),
our system only needs 83.77\% of the makespan to deliver the same amount of throughput, compared to their current system $^{(348.55 / 416.09)}$, outperforming the best$^{(382.20 / 416.09)}$ among the rest by 8.09\%.

\item \textit{Economic efficiency}.
Note that there is a continuing improvement$^{(348.55 \rightarrow 335.50)}$ while increasing the number of agents to 60. However, the marginal gain of further increasing to 70 agents is negligible$^{(335.50 \rightarrow 333.40)}$.
In fact, only 30 agents can fulfill the current throughput with even slightly shorter time$^{(412.90)}$, resulting in a 40\% reduction in fixed costs for purchasing robots. 

\end{enumerate}

%	\item \textbf{RQ1} (time efficiency): Given the currently deployed layout with 50 agents, what is the best combined option of those path finding and task assignment modules?
%	\item \textbf{RQ2} (economic efficiency): Is it profitable to deploy more agents to further increase the throughput, or the other way around, to deploy less agents while still maintaining the same throughput?
%
%Regarding \textbf{RQ1},
%
%Regarding \textbf{RQ2},

%\textcolor{red}{why ours is way better: coincide with universal plans}



\section{Conclusion and Discussion}
\label{sec:conclusion}



In this paper, we conduct a case study on the real-world problem of warehouse automation by combining lifelong multi-robot path finding and dynamic task assignment in an online fashion.
As a result, we manage to speed up package delivery given the current scale at Meituan,
and also identify potential profitable upgrades of the system.

An important lesson from this study is that given the layout of the warehouse, once deployed, is normally fixed in a relatively long period of time, it is worthwhile to have both the routing module and the assignment module that take advantage of the layout.
However, both modules should be general enough to account for the varying number of robots available.

A limitation of this work is that we search an assignment policy with respect to a fixed routing policy, which is an open-loop control. A natural next step is to couple the search of these two, though it will be computationally challenging.

%PRIMAL
%
%Another lesson is 
%
%1. The design of warehouse layouts should not be oblivious to the design of robot control strategies in later phases.
%
%2. policy search for a specific warehouse
%
%draw two lessons 
%
%robust RL
%
%Contextual RL

%\section*{Ethical Statement}
%
%There are no ethical issues.

%\section*{Acknowledgments}
%
%The preparation of these instructions and the \LaTeX{} and Bib\TeX{}
%files that implement them was supported by Schlumberger Palo Alto
%Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
%Preparation of the Microsoft Word file was supported by IJCAI.  An
%early version of this document was created by Shirley Jowell and Peter
%F. Patel-Schneider.  It was subsequently modified by Jennifer
%Ballentine, Thomas Dean, Bernhard Nebel, Daniel Pagenstecher,
%Kurt Steinkraus, Toby Walsh, Carles Sierra, Marc Pujol-Gonzalez,
%Francisco Cruz-Mencia and Edith Elkind.

%\clearpage
%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}


\newpage
\onecolumn
\appendix

\section{Relation to MAPD}
\label{apd:relate_to_mapd}


In MAPD, an online task $t_i$ is characterized by a pickup port $s_i$ and a delivery port $g_i$ with a priorly unknown release time. Once an agent becomes idle, she will select one task $t^* = (s^*, g^*)$ of her best interest from the released ones, and then plan a path from her current location to $g^*$ through $s^*$.
Mapping to our settings, an agent becomes idle only when she arrives at a pickup port, and shall then be assigned one delivery port from the candidates, say $\{g_1, \cdots, g_k\}$.
Suppose the system will simply pair each delivery port with a pickup port immediately, for which the particular agent will return to after the delivery. Then it is equivalent to, in the language of MAPD, releasing $k$ tasks $\{(g_1, \pi_P(g_1)), \cdots, (g_k, \pi_P(g_k)\}$.
However, after choosing one from the $k$ tasks and assigning it to an agent, the rest $(k-1)$ tasks will be temporarily removed, or ``deactivated'', from the pool of released tasks until the next item of the same type arrives.


\section{More Discussion on Related Work}
\label{apd:more_related_work}

The problem presented in this paper pertains to some other important areas in planning and operations research.

\textbf{Universal Planning.}
Unlike the idealized one-shot MAPF, fully automating real-world warehouses requires lifelong path finding.
However, most of the existing work~\cite{ma2017lifelong,vsvancara2019online,li2021lifelong,xu2022multi} still focuses on the solution concept as a set of collision-free paths, which is a sequence of joint actions.
Such a solution concept is vulnerable if there is any uncertainty, e.g. unknown future goals, or even system contingencies.
We argue that one can turn to the solution concept of universal plans~\cite{schoppers1987universal,ginsberg1989universal}.
Although universal plans are even harder to compute, there are some exemplars using multi-agent reinforcement learning~\cite{sartoretti2019primal,damani2021primal}, or via reduction to logic programs~\cite{zhu2023computing}.

% 3. comb. opt., scheduling 
% Job shop scheduling, vehicle routing
\textbf{Scheduling.}
One may also notice the analogy between TAPF and job-shop scheduling problems (JSSP)~\cite{manne1960job,jain1999deterministic} or vehicle routing problems (VRP)~\cite{toth2014vehicle,braekers2016vehicle}.
However, there are at least two key differences: (1) job durations in JSSP and route lengths in VRP are usually known in advance and (2) the execution of jobs or routes is independent of each other.
Neither of these two conditions holds in TAPF, especially when the tasks are released online.


\section{Side Effects of the \texttt{Type}$\odot$ Robot Model}
\label{apd:pf_notgood_eg}

When the state of an agent is lifted from pure locations to (location, direction) pairs, there will be extra difficulty resolving collisions. We here show three examples for (a)~\textit{cooperative A$^*$} (CA$^*$)~\cite{silver2005cooperative}, (b)~\textit{conflict-based search} (CBS)~\cite{sharon2015conflict}, and (c)~\textit{priority inheritance with backtracking} (PIBT)~\cite{okumura2022priority}, respectively.
\begin{enumerate}
	\item Figure~\ref{fig:pf_notgood_eg}(a) shows a case where CA$^*$ fails for the \texttt{Type}$\odot$ robot model. Suppose agent 2 is prioritized over agent 1, then agent 1 will move away immediately under the \texttt{Type}$\oplus$ robot model. However, under the \texttt{Type}$\odot$ robot model, agent 1 has to rotate first and thus cannot manage to avoid collision at the very next timestep.
	\item Figure~\ref{fig:pf_notgood_eg}(b) shows a case where it takes CBS a longer time  to resolve collisions under the \texttt{Type}$\odot$ robot model. The main reason is still due to the rotational cost. Similarly for the execution of \textit{priority-based search} (PBS)~\cite{ma2019searching}. 
% move to appendix?
%Another successful exemplar in designing dynamic priorities is to toggle priority update and inheritance at each planning step, bringing the solver named \textit{Priority Inheritance With Backtracking} (PIBT)~\cite{okumura2022priority}.
	\item Figure~\ref{fig:pf_notgood_eg}(c) shows a failed case due to deeper theoretical reasons.
	Instead of performing any best-first search, PIBT repeats one-timestep planning until the terminal state, and therefore, it needs a crucial lemma to make sure the total number of execution steps is always bounded (see \textbf{Lemma 1} in~\cite{okumura2022priority}), i.e., \textit{at each timestep the agent with the highest priority will manage to move one step closer to her goal}.
Nevertheless, when the states of each agent are lifted from only locations to (location, direction) pairs, this lemma no longer holds as a counter-example is provided in Figure~\ref{fig:pf_notgood_eg}(c).
\end{enumerate}



%\begin{figure}[!ht]
%\includegraphics[width=110mm]{fig/pp}
%
%\includegraphics[width=176mm]{fig/cbs}
%
%\includegraphics[width=132mm]{fig/pibt}
%\caption{A counter-example that shows the reachability lemma holds for the \texttt{Type}$\oplus$ robot model (upper), but fails for the \texttt{Type}$\odot$ robot model (lower).}
%\label{fig:pibt_fail}
%\end{figure}

\begin{figure*}[!ht]
%    \centering
    % First subfigure
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=110mm]{fig/pp} 
        \caption{Cooperative A$^*$ may fail for the \texttt{Type}$\odot$ Robot Model}
    \end{subfigure}
    \vspace{0.5mm}
    
    % Second subfigure
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=176mm]{fig/cbs}
        \caption{Conflict-based search works but takes more timesteps (same execution results by priority-based search in this particular case).}
    \end{subfigure}
    \vspace{0.5mm}
    
    % Third subfigure
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=132mm]{fig/pibt}
        \caption{PIBT fails.}
    \end{subfigure}


    \caption{Examples showing the added difficulty of resolving collisions with the \texttt{Type}$\odot$ robot model.}
    \label{fig:pf_notgood_eg}
\end{figure*}

%\newpage
\section{Computing Time}
\label{apd:comp_time}

We report the planning time per step in Table~\ref{tab:plan_time}.
Experiments are conducted on a
MacBook Air with Apple M2 CPU and 16 GB memory.
The planners are all implemented in Python, therefore, those numbers are merely for relative comparisons within this work.

\begin{table}[!ht]
\centering
\begin{tabular}{@{}lrrrrr@{}}
\toprule
                             & \textbf{30} & \textbf{40} & \textbf{50} & \textbf{60} & \textbf{70} \\ \midrule
\textbf{Touring (ours)}      & $\textbf{0.008}$       & $\textbf{0.012}$       & $\textbf{0.018}$       & $\textbf{0.025}$       & $\textbf{0.032}$       \\
\textbf{PP $h_{fast}$}       & 0.066       & 0.115       & 0.225       & 0.500       & 0.713       \\
\textbf{PP $h_{slow}$}       & 0.169       & 0.287       & 0.448       & 0.892       & 1.028       \\
\textbf{RHCR-CBS $h_{fast}$} & 0.765       & 0.718       & 1.842       & 2.642       & 2.448       \\
\textbf{RHCR-CBS $h_{slow}$} & 3.023       & 3.070       & 7.081       & 7.821       & 8.952       \\ \bottomrule
\end{tabular}
\caption{Planning time per step in seconds, implemented in Python.}
\label{tab:plan_time}
\end{table}


%\newpage
\section{Parameter Search}
\label{apd:param_search}
In the design of both the \textbf{Touring} and adaptive task assignment, there are certain hyper-parameters.
We here show how the best option is searched in terms of minimizing the eventual makespan.
\begin{enumerate}
	\item Turning frequency (Figure~\ref{fig:turning_freq}). Figure~\ref{fig:eg_non_wf} has presented the extreme where every possible cell that can be a turning is set as a turning, i.e., of frequency 1. One can gradually ``sparsify'' the turnings to see if the overall makespan gets worse. It turns out, the more turnings you have, the better the makespan on average will be. 
	\item Adaptive Threshold (Figure~\ref{fig:alpha_bp}). As the occupation ratio is defined as the number of agents over the number of passable cells in that part of area, the spectrum of tested thresholds in $N$-agent scenarios will be considerably less than those in $N'$-agent scenarios if $N < N'$.
	One can clearly observe that our \textbf{Touring} planner significantly outperforms the other three, and the threshold that makes the lowest box plot is the most desired one. Another observation from Figure~\ref{fig:alpha_bp} is that ours is also more stable than the other three, as the variations (the length of those boxes) are relatively small in most cases.
\end{enumerate}


%\subsection{Turning Frequencies}
\begin{figure*}[!ht]
\centering
\includegraphics[width=170mm]{fig/turning_freq}
\caption{Makespans over different turning frequency in various scales of agents in Meituan warehouse simulation. The X-axis means ``there will be a turning every $x$ cells''.}
\label{fig:turning_freq}
\end{figure*}



%\newpage
%\subsection{Adaptive Thresholds}

\begin{figure*}[!ht]
\centering
\includegraphics[width=167mm]{fig/alpha_bp_30}
\includegraphics[width=167mm]{fig/alpha_bp_40}
\includegraphics[width=167mm]{fig/alpha_bp_50}
\includegraphics[width=167mm]{fig/alpha_bp_60}
\includegraphics[width=167mm]{fig/alpha_bp_70}
\caption{The box-plot of makespans over different adaptive thresholds with various scales of agents in Meituan warehouse simulation.}
\label{fig:alpha_bp}
\end{figure*}


\newpage
\section{RL Training Details}
\label{apd:rl_train}

Here we reveal the details of RL training skipped in Section~\ref{sec:ta_rl}.

\textbf{Actions.} We directly mask out unavailable actions (those delivery ports that do not need the item) at each assignment state, instead of signaling large negative rewards. In principle, these two are equivalent in terms of the value of the eventual optimal policy, but the former one will guide the policy optimization to converge faster~\cite{huang2022closer}.

\textbf{State features.} As defined in Section~\ref{sec:ta_rl}, assignment states contain necessary information from system-states. Here we make each state of size $num\_of\_agents\times (2 + 1)$,
	which means to mark each agent's location and direction (converted to [0, 90, 80, 270]). The location feature is further normalized by the layout shape, and the direction feature is normalized by 360.
	
\textbf{Episodes.} We train the RL agents over one set of item sequences while evaluate it over another set of item sequences.


\textbf{Hyper-parameters.} Both the value network and the policy network are MLPs of size $H \times H \times H \times H$ followed by respective value/policy heads.
We attach some training samples in Figure~\ref{fig:rl_log}, for $H$ chosen from [128, 256, 1024]. The number of total training steps can also be seen in this figure. It turns out networks with $H=1024$ tend to overfit in most cases.

%\subsection{Training Protocol}
%
%\subsection{Different Network Sizes}
\begin{figure*}[!ht]
\centering
\includegraphics[width=170mm]{fig/train_log_30a}
\includegraphics[width=170mm]{fig/train_log_40a}
\includegraphics[width=170mm]{fig/train_log_50a}
\includegraphics[width=170mm]{fig/train_log_60a}
\includegraphics[width=170mm]{fig/train_log_70a}
\caption{Training processes for different network sizes in various scales of agents. }
\label{fig:rl_log}
\end{figure*}

\newpage
\section{Running Example Recordings}
We attach three video clips for the \textbf{Touring} planner coupled with the closest-first assigner, the adaptive assigner with $\alpha~=~0.235$, and the RL assignment with the best best-case performance, respectively.
\begin{enumerate}
	\item \texttt{warehouse\_50\_touring\_closest.mp4} with makespan 355.
	\item \texttt{warehouse\_50\_touring\_alpha0235.mp4} with makespan 325.
	\item \texttt{warehouse\_50\_touring\_rl.mp4} with makespan 316.
\end{enumerate}
In all the above instances, the initial states are the same, i.e. the corresponding agents are in the same locations and towards the same directions at the beginning, and the online item sequences are also the same.

\end{document}






