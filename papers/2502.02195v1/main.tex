\documentclass[a4paper,fleqn]{cas-sc}
\usepackage[authoryear]{natbib}
\usepackage{graphicx} 
\usepackage{float}
\usepackage{algorithm}  
\usepackage{algpseudocode}
\usepackage{color}
\usepackage{setspace}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
% \usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[switch]{lineno}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{url}
\usepackage[nomarkers,figuresonly]{endfloat}


\newcommand{\colorComments}{black} 
 
%%%Author definitions
% \def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
% \tsc{WGM}
% \tsc{QE}
% \tsc{EP}
% \tsc{PMS}
% \tsc{BEC}
% \tsc{DE}
%%%

\usepackage{lineno}
% \linenumbers 

\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}
\shorttitle{EFKAN}
\shortauthors{F.~Wang, H.~Qiu, Y.~Huang, X.~Gu, R.~Wang, and B.~Yang}

\title [mode = title]{EFKAN: A KAN-Integrated Neural Operator For Efficient Magnetotelluric Forward Modeling}

\author[1]{Feng~Wang}
% \credit{Conceptualization, Data curation, Investigation, Methodology, Software, Validation, Visualization, Writing – original draft, Writing – review \& editing}

\author[1]{Hong~Qiu}
% \credit{Writing – review \& editing}

\author[1]{Yingying~Huang}
% \credit{Writing – review \& editing}

\author[1]{Xiaozhe~Gu}
% \credit{Writing – review \& editing}

\author[1]{Renfang~Wang}
% \credit{Funding acquisition, Project administration, Supervision, Writing – review \& editing}

\author[2]{Bo~Yang}
% \credit{Project administration, Supervision, Writing – review \& editing}

\address[1]{College of Big Data and Software Engineering, Zhejiang Wanli University, Ningbo, 315200, China}
\address[2]{Key Laboratory of Geoscience Big Data and Deep Resource of Zhejiang Province, School of Earth Sciences, Zhejiang University, Hangzhou, 310027, China}

\begin{abstract}
Magnetotelluric (MT) forward modeling is fundamental for improving the accuracy and efficiency of MT inversion. Neural operators (NOs) have been effectively used for rapid MT forward modeling, demonstrating their promising performance in solving the MT forward modeling-related partial differential equations (PDEs). Particularly, they can obtain the electromagnetic field at arbitrary locations and frequencies. In these NOs, the projection layers have been dominated by multi-layer perceptrons (MLPs), which may potentially reduce the accuracy of solution due to they usually suffer from the disadvantages of MLPs, such as lack of interpretability, overfitting, and so on. Therefore, to improve the accuracy of MT forward modeling with NOs and explore the potential alternatives to MLPs, we propose a novel neural operator by extending the Fourier neural operator (FNO) with Kolmogorov-Arnold network (EFKAN). Within the EFKAN framework, the FNO serves as the branch network to calculate the apparent resistivity and phase from the resistivity model in the frequency domain. Meanwhile, the KAN acts as the trunk network to project the resistivity and phase, determined by the FNO, to the desired locations and frequencies. Experimental results demonstrate that the proposed method not only achieves higher accuracy in obtaining apparent resistivity and phase compared to the NO equipped with MLPs at the desired frequencies and locations but also outperforms traditional numerical methods in terms of computational speed. 
\end{abstract}
 
% \begin{coverletter}

% Dear Editors-in-Chief,
% \newline

% We hereby submit our manuscript entitled “EFKAN: A KAN-Integrated Neural Operator for Efficient Magnetotelluric Forward Modeling” for consideration for publication in Computers \& Geosciences.  We confirm that the submission follows all the requirements and includes all the items of the submission checklist.
% \newline

% In this manuscript, we report a novel approach that extends the Fourier neural operator with Kolmogorov-Arnold networks (a kind of novel and powerful neural network) to alleviate the limitations of fully connected neural networks in terms of interpretability, overfitting, and flexibility to improve the accuracy of neural operator equipped with fully connected neural networks. The proposed method can obtain apparent resistivity and phase at desired locations and frequencies and has good generalization ability to the small-scaled data and unseen data, demonstrating the potential for efficient magnetotelluric forward modeling.
% \newline

% We provide the source codes in a public repository with details listed in the section "Code availability".
% \newline

% This manuscript presents our original work and is not under consideration by any other journal. All authors approved the manuscript and this submission. Thank you for receiving our manuscript and considering it for review. Any efforts from you and the reviewers to accelerate the review process will be highly appreciated. We thank for your time and look forward to hearing from you!
% \newline

% Sincerely,
% \newline

% Feng~Wang, Hong~Qiu, Yingying~Huang, Xiaozhe~Gu, Renfang~Wang, and Bo~Yang

% Key Laboratory of Geoscience Big Data and Deep Resource of Zhejiang Province, School of Earth Sciences, Zhejiang University, Hangzhou, 310027, China (bo.yang@zju.edu.cn)
% \newline

% \textbf{Delete before submission:}

% Please confirm that your submission follows all the requirements of the guidelines, including the submission checklist:

% - Cover letter

% - Highlights

% - Authorship statement

% - The manuscript must be single column and double spaced

% - Reference must be in the author-date format

% - Code availability section 

% *The manuscripts that do meet the requirement guidelines will be desk-rejected.



% \end{coverletter}

 
% \begin{highlights}
% \item We propose a novel neural operator that extends the Fourier neural operator by integrating the Kolmogorov-Arnold network for magnetotelluric forward modeling.
% \item The proposed method demonstrates not only higher accuracy in obtaining apparent resistivity and phase compared to neural operators equipped with multilayer perceptrons but also outperformed traditional numerical methods in terms of computational speed.
% \item The proposed method shows good performance in generalization.
% \end{highlights}

\begin{keywords}
Magnetotelluric forward modeling \sep Kolmogorov-Arnold network \sep Neural operator
\end{keywords}

\maketitle 

\printcredits

\doublespacing


\section{Introduction}
Magnetotelluric (MT) has been widely used to illuminate the deep earth by inverting the subsurface resistivity. This method utilizes the natural electromagnetic fields induced by solar and ionospheric currents, which interact with the resistivity of the earth to produce secondary fields measurable at the surface. Since its inception, MT has played a crucial role in various applications, including natural resource exploration \citep{jiang_application_2022}, geothermal energy assessment \citep{cheng_imaging_2022}, environmental monitoring \citep{romano_sensitivity_2014}, and deep geological structure characterization \citep{egbert_fluid_2022}.

The interpretation of MT data is grounded in the inverse problem of determining the resistivity of the earth from the observed electromagnetic field variations. The effectiveness and efficiency of MT inversion are usually significantly determined by forward modeling. Over the last decades, numerical computational methods such as finite difference method (FDM) \citep{varilsuha_3d_2018}, finite element method (FEM) \citep{zhu_scalable_2022}, and finite volume method (FVM) \citep{guo_modular_2020} have dominated the solution of MT forward-related PDEs. These methods solve the PDEs with physical laws, thus they can obtain reasonable solutions. However, their accuracy largely depends on the spatial discretization; finer mesh grids yield higher accuracy but also increase computational costs.

Deep neural networks (DNNs) have been used for rapid MT forward modeling. This approach aims to learn the mapping function between the resistivity and the responses on the surface, such as the electromagnetic field or apparent resistivity, by training DNNs with supervision. \citet{conway_inverting_2019} trained a DNN that can output apparent resistivity and phase for three-dimensional (3-D) MT forward modeling. \citet{shan_application_2022} applied multitask learning (that is, a DNN has two branches) to predict the apparent resistivity and the phase of the mode $xy$. \citet{wang_three_2024} employed the 3-D Swin Transformer \citep{liu_swin_2021} for 3-D MT forward modeling, predicting the apparent resistivity and phase in different polarization directions. One main disadvantage of these methods is that their efficiency is limited by the spatial discretization of the resistivity model, meaning that the trained DNN only works for specific resolutions and frequencies. Thus, it is difficult to achieve satisfactory results on meshes or at frequencies that differ from the training data.

To increase the efficiency of solving PDEs, surrogate models have been developed to approximate solutions, replacing the need for numerical computational methods. Modern machine learning techniques have been used as surrogate models for PDE solvers. Representative models are physics-informed neural networks (PINNs) \citep{raissi_physics-informed_2019} and neural operators (NOs) such as Fourier neural operators (FNOs) \citep{li_fourier_2021} and deep operator networks (DeepONets) \citep{lu_learning_2021}.

The PINNs solve PDEs by modeling the sought solution with a DNN by minimizing the physics-informed loss function through automatic differentiation. \citet{zhang_maxwells_2020} reported that they used PINNs for the simulation of time-domain electromagnetic fields and obtained the electromagnetic fields with a high degree of accuracy, without discretization or interpolation in space or time. Although PINNs are mesh independent, they have limited generalization capabilities, as they require retraining for different conductivities and frequencies, which hinders their use in rapid MT forward modeling.

In contrast, NOs have been proposed to solve PDEs, where NNs map functions to functions \citep{shukla_comprehensive_2024}. FNOs represent a novel deep learning architecture designed to learn mappings between function spaces, especially for solving PDEs. In addition, FNOs are mesh independent and can solve PDEs directly by learning the underlying patterns in the data. At the heart of FNOs is the Fourier layer, which leverages the Fourier transform to convert functions from the spatial domain to the frequency domain. This layer focuses on low frequency that capture the essential features of the functions, applies a linear transformation, and then reverts back to the spatial domain with an inverse Fourier transform. This approach is not only efficient but also aligns well with the global and continuous nature of PDEs. For example, \citet{peng_rapid_2023} proposed a surrogate model for MT forward modeling in the frequency domain by combining FNO with a PDE loss function, eliminating the need for labeled data in network training.

However, FNOs have limitations in MT forward modeling because they require the output resolution to match the input resolution, which restricts their ability to obtain responses at arbitrary locations and frequencies. To address this shortcoming, DeepONets introduced the trunk net to map the output of the branch net to the desired solution based on the universal approximation theorem of operators \citep{lu_learning_2021}. \citet{peng_rapid_2022} proposed the extended Fourier neural operator (EFNO) for MT forward modeling in the frequency domain by extending FNO with MLP. The architecture of EFNO is well suited for the demands of responses at arbitrary locations and frequencies for MT forward modeling.

As an alternative to MLPs, KANs, as proposed by \citet{liu_kan_2024}, address the shortcomings of MLPs, including a lack of interpretability \citep{cranmer_interpretable_2023} and overfitting. Unlike traditional NNs that use fixed activation functions at the nodes, KANs introduce learnable activation functions on the edges, which connect the nodes. These activation functions are typically parameterized by certain functions, such as spline functions, providing a high degree of flexibility and adaptability to model complex relationships with potentially fewer parameters. KANs have been explored for solving PDEs and operator learning. \citet{liu_kan_2024} combined PINNs with KANs to solve the 2-D Poisson equation. \citet{abueidda_deepokan_2024} proposed a radial basis functions (RBF)-based KAN operator for solving orthotropic elasticity problems. \citet{shukla_comprehensive_2024} conducted a comprehensive and fair comparison between MLP and KAN for PDE and operator learning.

In this study, we exploit the potential of KANs to improve the accuracy of MT forward modeling by developing a novel approach that leverages the advantages of FNOs, DeepONets, and KANs. Specifically, based on the basic framework of DeepONets, we utilize FNO as the branch network to handle the frequency information of the resistivity model, and we take the KAN as the trunk net to obtain the apparent resistivity and phase at the desired locations and frequencies. Furthermore, we employ the spectral method to generate the resistivity model with anomalies, rather than simply embedding anomalies within homogeneous half-space underground to evaluate the effectiveness of the proposed method. This paper is organized as follows. In Section~\ref{cpt2}, we set up the problem formulation, briefly describe neural operators and KANs, and then provide details about the proposed method. In Section~\ref{cpt3}, we demonstrate the effectiveness of the proposed method through numerical experiments. In Section~\ref{cpt4}, we provide a discussion about the characteristics of the proposed method. Finally, we draw our conclusions in Section~\ref{cpt5}.

\section{Methodology}
\label{cpt2}
In this section, we first set up the MT forward problem with a surrogate model, followed by describing the proposed approach comprehensively.

\subsection{Problem Formulation}
In the frequency domain, the source-free propagation of the natural electric field $E$ and magnetic field $H$ can be expressed by the simplified Maxwell’s equations,

\begin{equation}
    \left\{\begin{array}{l}
    \nabla \times H=(\sigma - i \omega \varepsilon) E, \\
    \nabla \times E=i \omega \mu H,
    \end{array}\right.
\label{2-1}
\end{equation}

\noindent where $\sigma (S/m)$ represents the conductivity of the subsurface medium, which is the reciprocal of resistivity $\rho (\Omega \cdot m)$, $i$ is the imaginary unit, $\varepsilon(F/m)$ denotes the permittivity, $\omega=2 \pi f$ indicates the angular frequency of the electromagnetic (EM) field, and $\mu(H/m)$ denotes the permeability. For MT, it is commonly assumed that the permittivity and permeability of the earth are constants, e.i., $\varepsilon_0=8.85 \times 10^{-12} \mathrm{~F} / \mathrm{m}$ and $\mu_0=4 \pi \times 10^{-7} \mathrm{H} / \mathrm{m}$. Due to $\omega \varepsilon \ll \sigma$, the displacement current can be neglected. Therefore, the EM fields studied by the EM equations will satisfy the quasi-stationary field assumption within the frequency range of MT, and Equation~\ref{2-1} can be rewritten as

\begin{equation}
    \begin{cases}\nabla^{2} E -k^2 E =0, \\ \nabla^{2} H - k^2 H  =0,\end{cases}
\label{2-2}
\end{equation}

\noindent where $k=\sqrt{-iw\mu \sigma}$. Thus, the number of PDEs to be solved can be reduced to those related to either $E$ or $H$, instead of solving all PDEs concerning both $E$ and $H$. For 2-D MT forward modeling, the conductivity varies only in the vertical direction $z$ and the horizontal direction $y$; therefore, we can decouple Equation~\ref{2-2} into two modes: mode $xy(E_x, H_y, H_z)$ and mode $yx(H_x, E_y, E_z)$ by

\begin{equation}
\left\{\begin{array}{lll}
\nabla \cdot \nabla E_x-k^2 E_x =0, & \text{for mode}\ xy \\ 
\nabla \cdot \nabla H_x  - k^2 H_x =0. & \text{for mode}\ yx
\end{array}\right.
\label{2-3}
\end{equation}

Equation~\ref{2-3} can be solved by cooperating with the appropriate boundary condition, such as the Dirichlet boundary condition,

\begin{equation}
    u_{b c}=g(y, z),
\label{2-4}
\end{equation}

\noindent where $u_{b c}$ presents the electric or magnetic field at the boundary, and $g(y, z)$ denotes the Dirichlet boundary condition at coordinates $y$ and $z$. Once we obtain $E$ and $H$, we can determine the apparent resistivity $\rho_{xy}$ and $\rho_{yx}$, as well as the phase $\phi_{xy}$ and $\phi_{yx}$, using

\begin{equation}
\begin{aligned}
    & \rho_{x y}=\frac{1}{\omega \mu}\left|\frac{E_x}{H_y}\right|^2, \quad \phi_{x y}=\arctan \left(\frac{\operatorname{Im}\left(E_x / H_y\right)}{\operatorname{Re}\left(E_x / H_y\right)}\right), \\
    & \rho_{y x}=\frac{1}{\omega \mu}\left|\frac{E_y}{H_x}\right|^2, \quad \phi_{y x}=\arctan \left(\frac{\operatorname{Im}\left(E_y / H_x\right)}{\operatorname{Re}\left(E_y / H_x\right)}\right),
\end{aligned}
\label{2-5}
\end{equation}

\noindent where Re denotes the real part and Im denotes the imaginary part, respectively. Additionally, our primary concern is the electromagnetic field on the earth surface, as this is where we are typically able to deploy instruments for the collection of electromagnetic field data. To obtain the apparent resistivity and phase on the surface, conventional numerical modeling methods, such as the FDM, are often used to solve Equation~\ref{2-3}, which can be represented as

\begin{equation}
    G_\theta:\left\{\begin{array}{l}
    \sigma(y, z) \\
    \left(y, z, f\right)
    \end{array}\right\} \rightarrow\left\{\begin{array}{l}
    \rho_{x y}\left(y, z, f\right) \\
    \phi_{x y}\left(y, z, f\right) \\
    \rho_{y x}\left(y, z, f\right) \\
    \phi_{y x}\left(y, z, f\right)
    \end{array}\right\},
\label{2-6}
\end{equation}

\noindent where $G_\theta$ indicates the solver with parameter $\theta$ for the desired apparent resistivity and phase. Classical linear MT forward modeling method, such as FDM, usually require discretizing the conductivity, followed by constructing linear systems for different frequencies. Hence, the computational cost of conventional methods is very high, as their performance depends on the number of grids, and they require constructing a separate operator for each frequency. Therefore, it is necessary to develop a surrogate model of $G_\theta$ to improve the efficiency of MT forward modeling.

\subsection{Solving the Problem}
\subsubsection{Neural Operators}
Neural operators have been used to solve PDEs with modern machine learning technique, which  the solution operator. Representative models are FNOs \citep{li_neural_2020} and DeepONets \citep{lu_learning_2021}. 

To construct a NN for solving PDEs, \citet{li_neural_2020} proposed the NO, which can be written as

\begin{equation}
\begin{array}{r}
    v_{j+1}(x)=\alpha\left(W v_j(x)+\int_D \kappa(x, y) v_j(y) d y\right),     j=0,1,2, \ldots, h.
\end{array}
\label{2-7}
\end{equation}

\noindent where $v_{j+1}$ denotes the output of the $j+1$-th layer of the NN, $\alpha$ represents the activation function, $v_{j}$ indicates the output from the previous layer, and $W$ is the weight. Compared to the standard NN, the right-hand side of Equation~\ref{2-7} includes an additional integral term, where $\kappa$ signifies the integral kernel function that needs to be learned from data. By incorporating this integral term, the NO can extract non-local features from the data. Due to the fact that the computation of the integral is time-consuming, \citet{li_fourier_2021} further proposed the FNO by Fourier layer that transforms the integral operation of Equation~\ref{2-7} into frequency domain. The Fourier layer can be expressed as

\begin{equation}
    v_{j+1}=\alpha\left(W v_j+\mathcal{F}^{-1}\left(\mathcal{F}(\kappa) \cdot \mathcal{F}\left(v_j\right)\right)\right),
\label{2-8}
\end{equation}

\noindent where $\mathcal{F}$ denotes the Fourier transform, and $\mathcal{F}^{-1}$ is the inverse Fourier transform. As shown in Fig.~\ref{fno}, FNO consists of lifting layer $P$, a Fourier layer, and projecting layer $Q$. The lifting layer $P$ maps the input $a$ into a high-dimensional channel space. The Fourier layer transforms the input $v$ into frequency and filtering the Fourier coefficients, followed by inverse Fourier transform. The $W$ represents linear transformations, such as those performed by MLPs. The projecting layer $Q$ projects the data to the desired dimension.

Although FNO can share the same parameters regardless of the discretization employed for both input and output \citep{kovachki_neural_2024}, it is not suitable for MT forward modeling. This is because FNO typically requires that the input and output data have consistent spatial dimensions. This means that if the input data is a multi-dimensional array with a specific resolution, then the output data should also have the same dimensions and resolution. For MT forward modeling, the results of the forward modeling depend not only on the resistivity model but also on the observation frequency and the location of the electromagnetic field observation.

Unlike FNO, DeepONet is not restricted to a specific architecture and can incorporate various types of neural network architectures in its branch and trunk nets, providing greater flexibility. The formulation of DeepONet can be expressed as

\begin{equation}
    G_\theta(\sigma)(y) \approx \sum_{k=1}^p \underbrace{b_k\left(\sigma\left(x_1\right), \sigma\left(x_2\right), \ldots, \sigma\left(x_m\right)\right)}_{\text {branch}} \underbrace{t_k(y)}_{\text {trunk }},
\label{2-9}
\end{equation}

\noindent where $b_k$ represents $k$-th branch net, $t_k$ denotes $k$-th trunk net, and $p$ is the number of branches. In DeepONet, branch and trunk nets facilitate the approximation of infinite-dimensional mappings from inputs to outputs, offering a highly flexible and efficient approach. The branch network provides the necessary input features, while the trunk network maps these features to the output space, enabling DeepONet to handle complex and high-dimensional operator learning tasks \citep{tianping_chen_universal_1995}. This architecture allows DeepONet to generalize well across a range of scenarios, including different domain geometries, input parameters, and initial and boundary conditions \citep{kontolati_learning_2024}.


\begin{figure*}
\centering
\includegraphics[width=0.7\textwidth]{figs/FNO.png}
\caption{(a) The full architecture of FNO \citep{li_fourier_2021}: $P$ is the lifting layer, and $Q$ represents the projecting layer. (b) Fourier layers: $\mathcal{F}$ and $\mathcal{F}^{-1}$ are the Fourier and inverse Fourier transform, respectively, and $W$ denotes linear transform.}
\label{fno}
\end{figure*}


\subsubsection{KANs}
KANs \citep{liu_kan_2024} is a novel type of neural network architecture inspired by the Kolmogorov-Arnold representation theorem  \citep{kolmogorov_representation_1957,braun_constructive_2009}, which posits that any multivariate continuous function can be represented as a superposition of continuous functions of one variable. This foundational theorem underlies the unique structure of KANs, where traditional fixed activation functions are replaced by learnable functions on the network edges, effectively eliminating the need for linear weight matrices.

The core mathematical formula of the Kolmogorov-Arnold representation theorem is given by

\begin{equation}
    f(\mathbf{x}) = \sum_{q=1}^{Q} g_q\left(\sum_{p=1}^{P} \psi_{p,q}(x_p)\right),
\label{2-10}
\end{equation}

\noindent where $f: [0, 1]^d \rightarrow \mathbb{R}$ is a continuous function, $g_q$ and $\psi_{p,q}$ are continuous univariate functions, $P$ and $Q$ are integers that depend on $d$, the number of variables in $\mathbf{x}$.

In KANs, the activation functions are not fixed; instead, they are learned during the training process, allowing for a more flexible and efficient approximation of complex functions. Each weight parameter in a KAN is replaced with a univariate function. In the original implementation \citep{liu_kan_2024}, $\psi(x)$ is defined as a weighted combination of a basis function $b(x)$ and B-splines,

\begin{equation}
    \psi(x)=w_{b}b(x)+w_s\text{spline}(x),
    \label{2-11}
\end{equation}

\noindent where $b(x)$ and spline$(x)$ are defined as follows,

\begin{equation}
    b(x)=\frac{x}{1+e^{-x}},
\label{2-12}
\end{equation}

\begin{equation}
    \text{spline}(x) = \sum_{i} c_i B_i(x),
\label{2-13}
\end{equation}

\noindent where $w_b$,$w_s$ and $c_i$ are the weights that are optimized during training, and $B_i(x)$ are the B-spline basis functions defined over a grid. 

Thus, the proposed method is able to improve the interpretability, as the learnable functions can be visualized and understood more intuitively. The training process involves optimizing the weights of these splines to minimize the loss function, adjusting the shape of the spline to best fit the training data. KANs offer a promising alternative to traditional MLPs by leveraging the theoretical foundation of the Kolmogorov-Arnold representation theorem, leading to potentially more efficient and interpretable models for approximating complex functions.

\subsubsection{EFKAN}

\begin{figure*}
\centering
\includegraphics[width=0.8\textwidth]{figs/EFKAN.pdf}
\caption{The diagram of EFKAN: (a) The branch network consist of lifting layer $P$, Fourier layer, and projecting layer $Q$. The input of the branch network is the resistivity model; (b) the input of the trunk network is the frequencies and coordinates. The output of EFKAN contains of apparent resistivity $\{\rho_{xy}, \rho_{yx}\}$ and phase $\{\phi_{xy}, \phi_{yx}\}$.}
\label{efkan}
\end{figure*}

Considering the characteristics of MT forward modeling, as well as the advantages of FNOs, DeepONets, and KANs, we propose extending FNO with KAN to improve the accuracy of MT forward modeling, which we name EFKAN (Fig.~\ref{efkan}). Specifically, we employ FNO as the branch net to map the resistivity $\rho(y, z)$ to $U(y,z)$; for the trunk net, we utilize the desired frequencies and coordinates of the electromagnetic field observation as inputs to KAN to obtain $D(y,z,f)$. Note that $D(y,z,f)$ has the same dimension with $U(y,z)$. We can therefore achieve the desired apparent resistivity and phase ($\rho_{xy}, \phi_{xy}, \rho_{yx}, \phi_{yx}$) by the matrix multiplication of $u_i$ and $d_i$. For the 2-D case, the mathematical expression of MT forward modeling with EFKAN can be written as

\begin{equation}
\begin{aligned}
     G_\theta(\sigma)(y, z, f)  \approx \sum_{k=1}^p \underbrace{b_k\left(\sigma\left(y_1, z_1\right), \sigma\left(y_2, z_2\right), \ldots, \sigma\left(y_m, z_m\right)\right)}_{\text {FNO}} \underbrace{t_k(y, z, f)}_{\text {KAN}}.
\label{2-14}    
\end{aligned}
\end{equation}

In the FNO of EFKAN, we utilize the fast Fourier transformer (FFT), 2-D convolution, and GELU to construct the Fourier layer. In the frequency domain, it is necessary to truncate the high modes to obtain $\mathcal{F}\left(v_t\right) \in \mathbb{C}^{k_{\max} \times d_v}$ because we convolve $v_t \in \mathbb{R}^{n \times d_v}$ with a function that has only $k_{\max}$ Fourier modes. We set $k_{\max}$ to 18. For the 2-D convolution, we define the kernel size to $1\times 1$, and keep the input channels equal to the output channels. Additionally, we place linear layers at the beginning and end of the FNO to determine the dimension of output, and we employ the Gaussian error linear unit (GELU) as the activation function. More details related to the FNO can be found in Table~\ref{table-fno}. For the KAN of EFKAN, we use the original KAN implemented in \citep{liu_kan_2024} that adopts B-spline function as the activation function. The trunk net-KAN has three layers, and the details about each layer are listed in Table~\ref{table-kan}.     

\begin{table}
\renewcommand{\arraystretch}{1.5}
\centering
\caption{FNO architecture overview.}
\label{table-fno}
\begin{tabular}{ccc}
\hline
Layer                                             & Operation                                                                                              & Shape of output                                   \\ \hline
$P$                                                 & Linear                                                                                                           & batch size$\times 64\times 64 \times 1$                                \\ \hline
Fourier layer 1                                   & \begin{tabular}[c]{@{}c@{}}2-D FFT\\ 2-D Convolution\\ GELU \end{tabular} & bacth size$\times 64\times 64\times 32$                               \\ \hline
\begin{tabular}[c]{@{}c@{}}\vdots \end{tabular} & \begin{tabular}[c]{@{}c@{}}\vdots \end{tabular}                                                                & \begin{tabular}[c]{@{}c@{}}\vdots \end{tabular} \\ \hline
Fourier layer 6                                   & \begin{tabular}[c]{@{}c@{}}2-D FFT\\ 2-D Convolution\\ GELU \end{tabular} & bacth size$\times64 \times64 \times 32$                               \\ \hline
$Q1$                                                & Linear                                                                                                           & batch size$\times 64 \times 64 \times 128$                              \\ \hline
Activation                                                & GELU                                                                                                           & batch size$\times 64 \times 64 \times 128$                            
                \\ \hline
$Q2$                                                & Linear                                                                                                           & batch size$\times 64\times 64\times 4$                              \\ \hline
\end{tabular}
\end{table}

\begin{table}
\renewcommand{\arraystretch}{1.5}
\centering
\caption{KAN architecture overview.}
\label{table-kan}
\begin{tabular}{ccccc}
\hline
Layer & Number of neurons & Grid size & Spline order & Grid range  \\ \hline
1     & 2                & 5         & 3            & [-1, 1] \\
2     & 256              & 5         & 3            & [-1, 1] \\
3     & 4096              & 5         & 3            & [-1, 1] \\ \hline
\end{tabular}
\end{table}

\section{Computational Experiments}
\label{cpt3}
In this section, we use synthetic data to evaluate the effectiveness of the proposed method. To make the experiments as close as possible to field MT forward modeling, we utilize the Gaussian random field (GRF)-based approach to simulate resistivity models for training and testing the EFKAN. We present a series of computational experiments comparing the efficiency and accuracy of EFNO and EFKAN in solving 2-D MT forward problems. Specifically, we focus on using EFKAN to improve the accuracy of solutions for the 2-D Helmholtz equation and explore its applicability to spatial-temporal coordinates and frequencies not included in the training data, as well as its performance on small-scale datasets. All experiments are implemented with the PyTorch platform, and we train and test EFKAN and its competitive approach on a NVIDIA Tesla K80 GPU. It should be pointed out that the colorbars associated with EFKAN errors differ from those of EFNO errors in this study. Consequently, when comparing the prediction errors of EFNO and EFKAN, it is essential to take into account the differences between their respective colorbars.


\subsection{Data Generation}
To make the conductivity $\sigma$ used for forward modeling more closely resemble the field geological structure, we employ the spectral method to generate the conductivity model instead of simply embedding anomalies within a homogeneous half-space underground.

We set the area of interest for conductivity to 200 km $\times$ 100 km (width $\times$ height). We discretize the model into 64 grids along the horizontal direction and 64 grids along the vertical direction, with grid intervals that increase in size. Specifically, we divide the space between 0 km and -1 km into 20 grids at fixed intervals, and discretize the area between -1 km and -20 km and the domain between -20 km and -100 km into 20 and 24 grids, respectively, with intervals that increase logarithmically. To ensure FDM forward modeling fits the boundary conditions of PDEs, we expand the width to 600 km by repeating the conductivity at the margins, extend the depth to -200 km by linearly decaying the conductivity at the bottom, and add an air layer of 600 km $\times$ 200 km. We set the number of grids to 10 for both the expanded area and the air layer. It is worth pointing out that the air layer is only required in mode $xy$.

To make the synthetic conductivity more realistic, we utilize the spectral method to generate the random field as the conductivity. In the spectral method, the spectrum $P(k)$ is proportional to

\begin{equation}
P(k) \propto|k|^{-\beta / 2},
\label{3-1}
\end{equation}

\noindent where $k$ denotes the wavenumber, and $\beta$ represents the scale. Based on common scales of conductivity anomalies found in the earth, we use average conductivity values for five sections of conductivity, with $\beta$ of 3, 4, 5, 6, and 7, respectively. We define the conductivity interval from $10^{-4}$ S/m to 1 S/m, set the conductivity of the air layer to $10^{-9}$ S/m, and the conductivity at the lower boundary to $10^{-2}$ S/m. An example demonstrating the synthetic conductivity model is shown in Fig.~\ref{conduct_examp}.

\begin{table}
\renewcommand{\arraystretch}{1.5}
\centering
\caption{Statistics of training and testing datasets. The frequencies and coordinates of TeData-\uppercase\expandafter{\romannumeral1} are same with TrData. The frequencies and coordinates of TeData-\uppercase\expandafter{\romannumeral2} are the downsampled frequencies and coordinates of TrData. The frequencies of TeData-\uppercase\expandafter{\romannumeral3} are different from TrData, and its coordinates are same with TrData.}
\label{data_detail}
\begin{tabular}{lcccc}
\hline
Dataset               & TrData                     & TeData-\uppercase\expandafter{\romannumeral1} & TeData-\uppercase\expandafter{\romannumeral2} & TeData-\uppercase\expandafter{\romannumeral3} \\ \hline
Smooth                & \Checkmark  & \Checkmark                                                                    & \Checkmark                                                                    & \Checkmark                                                                    \\
Rectangular anomalies & \ding{56} & \ding{56}                                                                   & \Checkmark                                                                    & \Checkmark                                                                    \\
Number of samples     & 15000                      & 100                                                                                          & 100                                                                                          & 100                                                                                          \\
Number of frequencies & 64                         & 64                                                                                           & 32                                                                                           & 64                                                                                           \\
Number of coordinates & 64                         & 64                                                                                           & 32                                                                                           & 64                                                                                           \\ \hline
\end{tabular}
\end{table}

We uniformly arrange 64 sites within the range of $y = -100$ km to $100$ km along the surface at $z = 0$ km. We employ the FDM to simulate the electric and magnetic fields. We generate 15,000 pairs ${(\sigma, y, z, f), (\rho_{xy}, \rho_{yx}, \phi_{xy}, \phi_{yx})}$, comprising conductivity model, apparent resistivity, and phase, for training EFKAN at 64 frequencies spaced evenly on a logarithmic scale from 0.049 Hz to 10 Hz, referring to this dataset as TrData. Additionally, we generate three datasets to test the performance of EFKAN: (1) TeData-\uppercase\expandafter{\romannumeral1}. A dataset containing 100 pairs of ${(\sigma, y, z, f), (\rho_{xy}, \rho_{yx}, \phi_{xy}, \phi_{yx})}$ with the same frequencies and spatial coordinates as TrData; (2) TeData-\uppercase\expandafter{\romannumeral2}. This dataset includes 100 resistivity sections with rectangular anomalies embedded in a smooth background, and the electrical and magnetic fields are computed using the same frequencies and spatial coordinates as TrData; (3) TeData-\uppercase\expandafter{\romannumeral3}. This dataset contains 100 resistivity profiles with rectangular anomalies, and the electrical and magnetic fields are simulated using 64 different frequencies (from 0.005 Hz to 12.589 Hz). The statistics of the training and testing datasets are listed in Table~\ref{data_detail}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figs/conductivity_sample.pdf}
    \caption{An example of the resistivity model generated through the GRF method.}
\label{conduct_examp}
\end{figure}


\subsection{Prediction by the EFKAN Trained by TrData}
In this section, we explore the performance of EFKAN trained by the TrData. For both EFNO and EFKAN, we set the batch size to 50 and the number of training epochs to 200 for both training and testing. We optimize both EFNO and EFKAN using the AdamW optimizer with a learning rate of 0.001, aiming to minimize the relative $\ell_1$-norm loss,

\begin{equation}
\begin{aligned}
        \mathcal{L}=\sum_{i=1}^N \frac{\left\|\rho_{x y, i}-\hat{\rho}_{x y, i}\right\|_1+\left\|\rho_{y x, i}-\hat{\rho}_{y x, i}\right\|_1}{\left\|\rho_{x y, i}\right\|_1+\left\|\rho_{y x, i}\right\|_1+\left\|\phi_{xy, i}\right\|_1+\left\|\phi_{y x, i}\right\|_1}+  \frac{\left\|\phi_{x y, i}-\hat{\phi}_{x y, i}\right\|_1+\left\|\phi_{y x, i}-\hat{\phi}_{yx, i}\right\|_1}{\left\|\rho_{x y, i}\right\|_1+\left\|\rho_{y x, i}\right\|_1+\left\|\phi_{xy, i}\right\|_1+\left\|\phi_{y x, i}\right\|_1},
\label{lossfnc}
\end{aligned}
\end{equation}

\noindent where $\hat{\rho}{xy}, \hat{\rho}{yx}, \hat{\phi}{xy}$, and $\hat{\phi}{yx}$ respectively denote the predicted resistivity and phase for models $xy$ and $yx$, and $N$ indicates the batch size. Additionally, we apply early stopping during training, terminating it when the number of epochs with higher average error on the training samples than on the testing samples exceeds 10. The average loss is calculated as

\begin{equation}
  \epsilon=\frac{1}{M}\sum_{i=1}^{K}\mathcal{L}_{i},
\end{equation}

\noindent where $M$ is the number of samples, and $K$ indicates the number of iterations per epoch. The loss curves on TrData and TeData-\uppercase\expandafter{\romannumeral1} are shown in Fig.~\ref{loss}. It can be observed that the loss curves for both EFNO and EFKAN converge rapidly during training. However, the training and testing loss curves of EFNO fluctuate significantly due to early stopping, and the training of EFNO is terminated at the 106-th epoch. Nevertheless, the training and testing loss curves of EFKAN are much smoother and converge to a smaller value than that of EFNO. The quantitative results are listed in Table~\ref{quantive}, which proves that EFKAN can achieve higher accuracy than EFNO.

To further explore the capability of EFKAN, we randomly select a resistivity mode from each of the three datasets TeData-\uppercase\expandafter{\romannumeral1}, TeData-\uppercase\expandafter{\romannumeral2}, and TeData-\uppercase\expandafter{\romannumeral3} (Fig.~\ref{t1_models}). The predicted apparent resistivity and phase for the smooth resistivity (Fig.~\ref{t1_models}(a)) are shown in Fig.~\ref{t1_random_smooth}, it can be observed that the apparent resistivity and phase predicted by EFNO and EFKAN have high similarities with FDM; however, the error by EFKAN is weaker than that of EFNO. The relative $\ell_1$-norm error of $\rho_{xy}, \rho_{yx}, \phi_{xy}$, and $\phi_{yx}$ by EFNO is 0.0081, 0.0150, 0.0178, and 0.0153, respectively. The relative $\ell_1$-norm error of $\rho_{xy}, \rho_{yx}, \phi_{xy}$, and $\phi_{yx}$ by EFKAN is 0.0043, 0.0056, 0.0059, and 0.0093, respectively. The 1-D profiles at 0.049 Hz and 10 Hz (Fig.~\ref{t1_1d_smooth}) further demonstrate that EFKAN achieves higher accuracy than EFNO.

For the smooth resistivity with rectangular anomalies (Fig.~\ref{t1_models}(b)) from TeData-\uppercase\expandafter{\romannumeral2}, both EFNO and EFKAN can obtain satisfactory results (Fig.~\ref{t1_random_block}). The relative $\ell_1$-norm error of $\rho_{xy}, \rho_{yx}, \phi_{xy}$, and $\phi_{yx}$ by EFNO is 0.0826, 0.0867, 0.0701, and 0.0852, respectively. The relative $\ell_1$-norm error of $\rho_{xy}, \rho_{yx}, \phi_{xy}$, and $\phi_{yx}$ by EFKAN is 0.0908, 0.0969, 0.0412, and 0.0758, respectively. These errors are higher than those of TeData-\uppercase\expandafter{\romannumeral1} due to the absence of rectangular anomalies in the training dataset (i.e., TrData). As shown in Fig.~\ref{t1_1d_block}, although the predictions from EFNO and EFKAN are essentially the same, EFKAN fits the ground truth (i.e., the results by FDM) better, especially in the phase at 10 Hz (Fig.~\ref{t1_1d_block}(b2) and (b4)).

For the smooth resistivity with rectangular anomalies (Fig.~\ref{t1_models}(c)) from TeData-\uppercase\expandafter{\romannumeral3}, although the frequencies used for prediction are entirely different from TrData, both EFNO and EFKAN predict the apparent resistivity and phase precisely. The relative $\ell_1$-norm errors for $\rho_{xy}$, $\rho_{yx}$, $\phi_{xy}$, and $\phi_{yx}$ by EFNO are 0.0391, 0.0845, 0.0799, and 0.1349, respectively. The relative $\ell_1$-norm errors for $\rho_{xy}$, $\rho_{yx}$, $\phi_{xy}$, and $\phi_{yx}$ by EFKAN are 0.0451, 0.0877, 0.0666, and 0.1485, respectively. As shown in Fig.~\ref{t1_1d_block_df}, the prediction by EFKAN are closer to the ground truth in some areas, such as the apparent resistivity between 0 and 75 km at 12.589 Hz (Fig.~\ref{t1_1d_block_df}(b1) and (b3)).


\begin{table}
\renewcommand{\arraystretch}{1.5}
\centering
\caption{Quantitative results for TrData dataset.}
\label{quantive}
\begin{tabular}{lcccccc}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{Model}} & \multicolumn{2}{c}{TeData-\uppercase\expandafter{\romannumeral1}} & \multicolumn{2}{c}{TeData-\uppercase\expandafter{\romannumeral2}} & \multicolumn{2}{c}{TeData-\uppercase\expandafter{\romannumeral3}} \\ \cline{2-7} 
\multicolumn{1}{c}{}                       & $\epsilon$          & Time (s)          & $\epsilon$          & Time (s)          & $\epsilon$          & Time (s)          \\ \hline
EFNO                                       & 0.0059                               & 0.2302                       & 0.0819                               & 0.2325                       & 0.0785                               & 0.2844                       \\
EFKAN                                      & 0.0037                               & 1.1387                       & 0.0884                               & 1.1141                       & 0.0776                               & 1.2609                       \\ \hline
\end{tabular}
\end{table}


\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{figs/loss.pdf}
\caption{(a) The training loss of EFNO and EFKAN on the TrData dataset; (b) The testing loss of EFNO and EFKAN on the TeData-\uppercase\expandafter{\romannumeral1} dataset.}
\label{loss}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{figs/test1_models.pdf}
\caption{Examples for demonstrating the effectiveness of EFKAN: (a) The smooth resistivity model randomly sampled from TeData-\uppercase\expandafter{\romannumeral1}; (b) The smooth resistivity model with rectangular anomalies sampled from TeData-\uppercase\expandafter{\romannumeral2}; (c) The smooth resistivity model with rectangular anomalies sampled from TeData-\uppercase\expandafter{\romannumeral3}.}
\label{t1_models}
\end{figure}

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/random_smooth.pdf}
\caption{Comparison of the ground truth obtained by FDM and the predicted apparent resistivity and phase by EFNO and EFKAN for the smooth resistivity model (Fig.~\ref{t1_models}(a)).}
\label{t1_random_smooth}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/test1_smooth_1d.pdf}
\caption{The 1-D profiles in Fig.~\ref{t1_random_smooth}: (a1)$\sim$(a4) show the apparent resistivity and phase at 0.049 Hz; (b1)$\sim$(b4) represent the apparent resistivity and phase at 10 Hz.}
\label{t1_1d_smooth}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/random_block.pdf}
\caption{Comparison of the ground truth obtained by FDM and the predicted apparent resistivity and phase by EFNO and EFKAN for the smooth resistivity model with rectangular anomalies (Fig.~\ref{t1_models}(b)).}
\label{t1_random_block}
\end{figure*}


\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/test1_block_1d.pdf}
\caption{The 1-D profiles in Fig.~\ref{t1_random_block}: (a1)$\sim$(a4) show the apparent resistivity and phase at 0.049 Hz; (b1)$\sim$(b4) display the apparent resistivity and phase at 10 Hz.}
\label{t1_1d_block}
\end{figure*}


\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/random_block_df.pdf}
\caption{Comparison of the ground truth obtained by FDM and the predicted apparent resistivity and phase by EFNO and EFKAN for the smooth resistivity model with rectangular anomalies (Fig.~\ref{t1_models}(c)).}
\label{t1_random_block_df}
\end{figure*}


\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/test1_block_df_1d.pdf}
\caption{The 1-D profiles in Fig.~\ref{t1_random_block_df}: (a1)$\sim$(a4) demonstrate the apparent resistivity and phase at 0.005 Hz; (b1)$\sim$(b4) show the apparent resistivity and phase at 12.589 Hz.}
\label{t1_1d_block_df}
\end{figure*}


\subsection{Prediction by the EFKAN Trained by the TrData with downsampled frequencies and coordinates}
In this section, we downsample the frequencies and coordinates of TrData by a factor of 3 to train EFNO and EFKAN, aiming to compare their effectiveness in predicting high-resolution data. Specifically, we use the network trained with apparent resistivity and phase at 22 frequencies and 22 coordinates to predict data at 64 frequencies and 64 coordinates. In Fig.~\ref{sp_loss}, we display the training loss on the downsampled TrData and the testing loss on the TeData-\uppercase\expandafter{\romannumeral1}. The loss curves of EFNO show fluctuations and termination at the 82nd epoch. The training of EFKAN is terminated at the 149-th epoch, and it achieved smaller testing error than EFNO. Table~\ref{quantive-sp} clearly shows that EFKAN achieves smaller average $\ell_1$-norm error on TeData-\uppercase\expandafter{\romannumeral1}, TeData-\uppercase\expandafter{\romannumeral2}, and TeData-\uppercase\expandafter{\romannumeral3}, verifying the effectiveness of EFKAN for predicting high-resolution data. Besides, the quantitative results (Table~\ref{quantive-sp}) demonstrate that EFKAN outperforms EFNO in terms of prediction accuracy, proving its effectiveness for three different kinds of testing data.

Also, we randomly sample a resistivity model (Fig.~\ref{sp_models}) from each of the three datasets TeData-\uppercase\expandafter{\romannumeral1}, TeData-\uppercase\expandafter{\romannumeral2}, and TeData-\uppercase\expandafter{\romannumeral3} for illustrating the effectiveness of EFKAN. In Fig.~\ref{sp_random_smooth}, we present the predicted apparent resistivity and phase for the smooth resistivity (Fig.~\ref{sp_models}(a)). We can observe that the apparent resistivity and phase predicted by EFNO and EFKAN have high precision, and the $\ell_1$-norm error of EFKAN is lower than that of EFNO, as corroborated by the color bar. The 1-D profiles at 0.049 Hz and 10 Hz (Fig.~\ref{sp_1d_smooth}) further demonstrate that EFKAN fits the ground truth with higher accuracy than EFNO. The relative $\ell_1$-norm error of $\rho_{xy}, \rho_{yx}, \phi_{xy}$, and $\phi_{yx}$ by EFNO is 0.0107, 0.0123, 0.0190, and 0.0210, respectively. The relative $\ell_1$-norm error of $\rho_{xy}, \rho_{yx}, \phi_{xy}$, and $\phi_{yx}$ by EFKAN is 0.0069, 0.0069, 0.0076, and 0.0110, respectively.

For the smooth resistivity model with rectangular anomalies (Fig.~\ref{sp_models}(b)) from TeData-\uppercase\expandafter{\romannumeral2} dataset, although the complexity of resistivity has been increased, both EFNO and EFKAN predict the apparent resistivity and phase with high resolution (Fig.~\ref{sp_random_block}) and their error is slight. The relative $\ell_1$-norm error of $\rho_{xy}, \rho_{yx}, \phi_{xy}$, and $\phi_{yx}$ by EFNO is 0.0646, 0.1093, 0.0569, and 0.0969, respectively. The relative $\ell_1$-norm error of $\rho_{xy}, \rho_{yx}, \phi_{xy}$, and $\phi_{yx}$ by EFKAN is 0.0662, 0.0899, 0.0498, and 0.0982, respectively. In the 1-D profiles (Fig.~\ref{sp_1d_block}), we can further see that the prediction by EFKAN meets the ground truth essentially at 0.049 Hz (Fig.~\ref{sp_1d_block}(a1)$\sim$(a4)) and fits the true apparent resistivity and phase very well at 10 Hz (Fig.~\ref{t1_1d_block}(b1)$\sim$(b4)), outperforming EFNO.

We downsample the original frequencies and coordinates of TeData-\uppercase\expandafter{\romannumeral3} (Fig.~\ref{sp_models})(c) to testing the effectiveness of EFKAN for predicting the the apparent resistivity and phase at 64 frequencies and 64 coordinates. As shown in Fig.~\ref{sp_random_block_df}, EFNO and EFKAN present satisfactory predicted results. Additionally, the $\ell_1$-norm error of EFKAN is obviously slighter than that of EFNO. The relative $\ell_1$-norm error of $\rho_{xy}, \rho_{yx}, \phi_{xy}$, and $\phi_{yx}$ of EFNO is 0.0397, 0.0856, 0.0802, and 0.1401, respectively. The relative $\ell_1$-norm error of $\rho_{xy}, \rho_{yx}, \phi_{xy}$, and $\phi_{yx}$ of EFKAN is 0.0415, 0.0904, 0.0604, and 0.1499, respectively. Fig.~\ref{t1_1d_block_df} shows that the predictions by EFNO and EFKAN are similar at 0.005 Hz, while the predictions by EFKAN are more precise than those of EFNO at 12.589 Hz.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{figs/sp_loss.pdf}
\caption{(a) The training loss of EFNO and EFKAN on the downsampled TrData dataset; (b) The testing loss of EFNO and EFKAN on the TeData-\uppercase\expandafter{\romannumeral1} dataset.}
\label{sp_loss}
\end{figure}

\begin{table}
\renewcommand{\arraystretch}{1.5}
\centering
\caption{Quantitative Results for the Down-Sampled TrData.}
\label{quantive-sp}
\begin{tabular}{lcccccc}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{Model}} & \multicolumn{2}{c}{TeData-\uppercase\expandafter{\romannumeral1}} & \multicolumn{2}{c}{TeData-\uppercase\expandafter{\romannumeral2}} & \multicolumn{2}{c}{TeData-\uppercase\expandafter{\romannumeral3}} \\ \cline{2-7} 
\multicolumn{1}{c}{}                       & $\epsilon$          & Time (s)          & $\epsilon$          & Time (s)          & $\epsilon$          & Time (s)          \\ \hline
EFNO                                       & 0.0773                               & 0.9933                       & 0.0903                               & 0.9552                       & 0.0795                               & 0.9497                       \\
EFKAN                                      & 0.0041                               & 1.0920                       & 0.0885                               & 1.1158                       & 0.0773                               & 1.1331                       \\ \hline
\end{tabular}
\end{table}


\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{figs/sp_models.pdf}
\caption{Examples for demonstrating the effectiveness of EFKAN trained by the downsampled TrData: (a) The smooth resistivity model randomly sampled from TeData-\uppercase\expandafter{\romannumeral1}; (b) The smooth resistivity model with rectangular anomalies sampled from TeData-\uppercase\expandafter{\romannumeral2}; (c) The smooth resistivity model with rectangular anomalies sampled from TeData-\uppercase\expandafter{\romannumeral3}.}
\label{sp_models}
\end{figure}

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/sp_smooth_pre.pdf}
\caption{Comparison of the ground truth obtained by FDM and the predicted apparent resistivity and phase by EFNO and EFKAN for the smooth resistivity model (Fig.~\ref{sp_models}(a)).}
\label{sp_random_smooth}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/sp_smooth_1d.pdf}
\caption{The 1-D profiles in from Fig.~\ref{sp_random_smooth}: (a1)$\sim$(a4) display the apparent resistivity and phase at 0.049 Hz; (b1)$\sim$(b4) show the apparent resistivity and phase at 10 Hz.}
\label{sp_1d_smooth}
\end{figure*}


\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/sp_block_pre.pdf}
\caption{Comparison of the ground truth obtained by FDM and the predicted apparent resistivity and phase by EFNO and EFKAN for the smooth resistivity model with rectangular anomalies (Fig.~\ref{sp_models}(b)).}
\label{sp_random_block}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/sp_block_1d.pdf}
\caption{The 1-D profiles in Fig.~\ref{sp_random_block}: (a1)$\sim$(a4) show the apparent resistivity and phase at 0.049 Hz; (b1)$\sim$(b4) exhibit the apparent resistivity and phase at 10 Hz.}
\label{sp_1d_block}
\end{figure*}


\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/sp_block_df_pre.pdf}
\caption{Comparison of the ground truth obtained by FDM and the predicted apparent resistivity and phase by EFNO and EFKAN for the smooth resistivity model with rectangular anomalies (Fig.~\ref{sp_models}(c)).}
\label{sp_random_block_df}
\end{figure*}


\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/sp_block_df_1d.pdf}
\caption{The 1-D profiles in Fig.~\ref{sp_random_block_df}: (a1)$\sim$(a4) show the apparent resistivity and phase at 0.005 Hz; (b1)$\sim$(b4) show the apparent resistivity and phase at 12.589 Hz.}
\label{sp_1d_block_df}
\end{figure*}


\subsection{Prediction by the EFKAN Trained by the Small-Scale TrData}
The performance of neural networks is typically limited by the amount of the training data. However, the larger the training data, the higher the computational cost. In this section, we decrease the number of samples in TrData to 5000 to assess the generalization ability of EFKAN. Fig.~\ref{sd_loss} displays the training loss on the small-scale TrData and the testing loss on the TeData-\uppercase\expandafter{\romannumeral1}. The training of EFNO is early stopped at the 108-th epoch, while EFKAN is terminated at the 149-th epoch with a smaller loss value. The testing loss curves show that EFKAN achieves a smaller loss than EFNO on TeData-\uppercase\expandafter{\romannumeral1}. As shown in Table~\ref{quantive-sd}, EFKAN achieves the lowest average $\ell_1$-norm error across all three testing datasets, demonstrating its strong generalization ability.

We randomly sample a resistivity model (Fig.~\ref{sd_models}) from each of the three datasets TeData-\uppercase\expandafter{\romannumeral1}, TeData-\uppercase\expandafter{\romannumeral2}, and TeData-\uppercase\expandafter{\romannumeral3}  to show the effectiveness of EFKAN. Fig.~\ref{sd_random_smooth} presents the predicted apparent resistivity and phase for the smooth resistivity (Fig.~\ref{sd_models}(a)), and we can observe that both EFNO and EFKAN obtain solutions with high precision. The $\ell_1$-norm error by EFNO and EFKAN are close to each other except for the error of $\rho_{xy}$, where the amplitude of the error by EFNO is much higher than that of EFKAN. The 1-D profiles at 0.049 Hz and 10 Hz (Fig.~\ref{sd_1d_smooth}) further demonstrate that EFKAN can fit the ground truth better than EFNO. The relative $\ell_1$-norm error of $\rho_{xy}, \rho_{yx}, \phi_{xy}$, and $\phi_{yx}$ by EFNO is 0.0240, 0.0278, 0.0298, and 0.0321, respectively. The relative $\ell_1$-norm error of $\rho_{xy}, \rho_{yx}, \phi_{xy}$, and $\phi_{yx}$ by EFKAN is 0.0088, 0.0123, 0.0163, and 0.0185, respectively.

As shown in Fig.~\ref{sd_random_block}, both EFNO and EFKAN obtain satisfactory results, though the resistivity with rectangular anomalies is not included in TrData. Additionally, the errors are relatively minor and are mainly concentrated in the low-frequency. The relative $\ell_1$-norm error of $\rho_{xy}, \rho_{yx}, \phi_{xy}$, and $\phi_{yx}$ by EFNO is 0.1396, 0.1588, 0.0933, and 0.1231, respectively. The relative $\ell_1$-norm error of $\rho_{xy}, \rho_{yx}, \phi_{xy}$, and $\phi_{yx}$ by EFKAN is 0.1161, 0.1425, 0.0702, and 0.1160, respectively. In Fig.~\ref{sd_1d_block}, we present the 1-D profiles, and we can observe that the predictions by EFKAN meet the ground truth well between -100 and -35 km at 0.049 Hz (Fig.~\ref{sd_1d_block}(a1) to (a4)), and it exhibits superior performance to EFNO at 10 Hz (Fig.~\ref{sd_1d_block}(b1) to (b4)).

We evaluate the performance for frequencies that are completely different from TrData. The example is shown in Fig.~\ref{sd_models}, which also includes rectangular anomalies. As shown in Fig.~\ref{sd_random_block_df}, both EFNO and EFKAN are able to provide reasonable apparent resistivity and phase. The errors corresponding to phase are more pronounced than those of apparent resistivity, as well as their distribution. The relative $\ell_1$-norm error of $\rho_{xy}, \rho_{yx}, \phi_{xy}$, and $\phi_{yx}$ by EFNO is 0.0844, 0.0797, 0.0911, and 0.1402, respectively. The relative $\ell_1$-norm error of $\rho_{xy}, \rho_{yx}, \phi_{xy}$, and $\phi_{yx}$ by EFKAN is 0.0529, 0.0767, 0.0683, and 0.1403, respectively. From the 1-D profiles (Fig.~\ref{sd_1d_block_df}), it can be observed that both EFNO and EFKAN can essentially fit the apparent resistivity and phase at 0.005 Hz as well as the phase at 12.589 Hz. Furthermore, the apparent resistivity predicted by the two methods is very close to the ground truth, yet EFKAN is more accurate than EFNO at 12.589 Hz (Fig.~\ref{sd_1d_block_df}(b1) and (b3)).

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{figs/sd_loss.pdf}
\caption{(a) The training loss of EFNO and EFKAN on the small-scale TrData dataset; (b) The testing loss of EFNO and EFKAN on the TeData-\uppercase\expandafter{\romannumeral1} dataset.}
\label{sd_loss}
\end{figure}

\begin{table}
\renewcommand{\arraystretch}{1.5}
\centering
\caption{Quantitative Results for the Reduced-Scale TrData}
\label{quantive-sd}
\begin{tabular}{lcccccc}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{Model}} & \multicolumn{2}{c}{TeData-\uppercase\expandafter{\romannumeral1}} & \multicolumn{2}{c}{TeData-\uppercase\expandafter{\romannumeral2}} & \multicolumn{2}{c}{TeData-\uppercase\expandafter{\romannumeral3}} \\ \cline{2-7} 
\multicolumn{1}{c}{}                       & $\epsilon$          & Time (s)          & $\epsilon$          & Time (s)          & $\epsilon$          & Time (s)          \\ \hline
EFNO                                       & 0.0110                               & 0.9537                       & 0.0949                               & 0.9694                       & 0.0844                               & 0.9742                       \\
EFKAN                                      & 0.0070                               & 1.1269                       & 0.0932                               & 1.1053                       & 0.0825                               & 1.1407                       \\ \hline
\end{tabular}
\end{table}

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{figs/sd_models.pdf}
\caption{Examples for demonstrating the effectiveness of EFKAN trained on the small-scale TrData: (a) The smooth resistivity model randomly sampled from TeData-\uppercase\expandafter{\romannumeral1}; (b) The smooth resistivity model with rectangular anomalies sampled from TeData-\uppercase\expandafter{\romannumeral2}; (c) The smooth resistivity model with rectangular anomalies sampled from TeData-\uppercase\expandafter{\romannumeral3}.}
\label{sd_models}
\end{figure}

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/sd_smooth_pre.pdf}
\caption{Comparison of the ground truth obtained by FDM and the predicted apparent resistivity and phase by EFNO and EFKAN for the smooth resistivity model (Fig.~\ref{sd_models}(a)).}
\label{sd_random_smooth}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/sd_smooth_1d.pdf}
\caption{The 1-D profiles in Fig.~\ref{sd_random_smooth}: (a1)$\sim$(a4) show the apparent resistivity and phase at 0.049 Hz; (b1)$\sim$(b4) show the apparent resistivity and phase at 10 Hz.}
\label{sd_1d_smooth}
\end{figure*}


\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/sd_block_pre.pdf}
\caption{Comparison of the ground truth obtained by FDM and the predicted apparent resistivity and phase by EFNO and EFKAN for the smooth resistivity model with rectangular anomalies (Fig.~\ref{sd_models}(b)).}
\label{sd_random_block}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/sd_block_1d.pdf}
\caption{The 1-D profiles in Fig.~\ref{sd_random_block}: (a1)$\sim$(a4) show the apparent resistivity and phase at 0.049 Hz; (b1)$\sim$(b4) display the apparent resistivity and phase at 10 Hz.}
\label{sd_1d_block}
\end{figure*}


\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/sd_block_df_pre.pdf}
\caption{Comparison of the ground truth obtained by FDM and the predicted apparent resistivity and phase by EFNO and EFKAN for the smooth resistivity model with rectangular anomalies (Fig.~\ref{sd_models}(c)).}
\label{sd_random_block_df}
\end{figure*}


\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figs/sd_block_df_1d.pdf}
\caption{The 1-D profiles in Fig.~\ref{sd_random_block_df}: (a1)$\sim$(a4) demonstrate the apparent resistivity and phase at 0.005 Hz;  (b1)$\sim$(b4) represent the apparent resistivity and phase at 12.589 Hz.}
\label{sd_1d_block_df}
\end{figure*}

\subsection{Computational Cost}
From Tables~\ref{quantive}, \ref{quantive-sp}, and \ref{quantive-sd}, it is evident that EFKAN requires approximately 0.01 seconds to predict the EM solutions for a single resistivity model. In contrast, the traditional and widely-used numerical algorithm FDM takes about 10 seconds per resistivity model for forward modeling. This indicates that the computational speed of EFKAN is nearly 1000 times faster than that of FDM. Additionally, EFKAN consumes slightly more time per resistivity model compared to EFNO, with training times per epoch being 93 seconds and 114 seconds for EFNO and EFKAN, respectively. This discrepancy may arise from the fact that the KAN employs a separate function for each variable in the input, and the standard B-spline functions utilized in the KAN are not optimized for parallel computing on GPUs. To enhance computational efficiency, one could consider replacing the B-spline function with alternative univariate functions, such as radial basis functions (RBFs) \citep{li_kolmogorov-arnold_2024}, wavelets \citep{bozorgasl_wav-kan_2024}, and Jacobi polynomials \citep{ss_chebyshev_2024}.

\section{Discussion}
\label{cpt4}
The numerical results indicate that EFKAN achieves higher accuracy in apparent resistivity and phase measurements compared to the EFNO \citep{peng_rapid_2022} that consists of FNO and MLP. We believe that is due to the advantage of KAN for science tasks. Based on the Kolmogorov-Arnold representation theorem, KAN has no linear weights at all–every weight parameter is replaced by a univariate function \citep{liu_kan_2024}, such as B-spline functions. Due to the locality and adjustable number of grids in univariate function, KANs can achieve a certain degree of dynamic network architecture and continual learning \citep{yu_kan_2024}.  In reference \citep{yu_kan_2024}, \citet{yu_kan_2024} proved that KAN significantly outperforms MLP in symbolic formula representation, while MLP generally outperforms KAN in machine learning, computer vision, natural language processing, and audio processing. Therefore, KANs have demonstrated promising performance in many science tasks including Poisson equation, knot theory, and Anderson localization.

Furthermore, we believe that KAN can endow EFKAN with a certain degree of interpretability. The Kolmogorov-Arnold representation theorem posits that any multivariate continuous function can be represented as a superposition of continuous functions of a single variable. Thus, it is easy to visualize and understand these learnable activation functions, allowing for a more intuitive grasp of how inputs are transformed into outputs, examples are shown in Fig.~\ref{efkan}. This design not only overcomes the limitations of fixed activation functions in terms of interpretability and training efficiency but also aligns well with the global and continuous nature of the problems KAN are designed to solve, such as partial differential equations and operator learning tasks. The parameterization of these functions and their learnable nature make KAN more transparent and understandable, compared to the \emph{black-box} nature of traditional MLPs.

From Fig.~\ref{loss}, Fig.~\ref{sp_loss}, and Fig.~\ref{sd_loss}, we can observer that the testing loss of both EFNO and ENKAN is lower than the training loss, which is contrary to that in many deep learning tasks. By analyzing the training and testing dataset, we speculate that is due to the to the number of samples in the test dataset being significantly fewer than the number of samples in the training dataset. Specifically, the training dataset TrData contains 15000 samples, which is 150 times the number of samples of test dataset TeData-\uppercase\expandafter{\romannumeral1}, TeData-\uppercase\expandafter{\romannumeral2} and TeData-\uppercase\expandafter{\romannumeral3}. Therefore, the accumulated error on the dataset with more samples may be greater than the accumulated error on the dataset with fewer samples. It is worth mentioning that this phenomenon has also occurred in our previous work \citep{peng_rapid_2022}. We also find this phenomenon existing in the Fashion MNIST classification, which consisting of a training set of 60000 examples and a test set of 10000 examples. It can be observed that regardless of whether an MLP or CNN is used, the validation loss (i.e., the testing loss in our experiments) is lower than the training loss (\url{https://nvsyashwanth.github.io/machinelearningmaster/fashion-mnist/}).

\section{Conclusion}
\label{cpt5}
In this study, we present the development of a novel neural operator, EFKAN, for rapid MT forward modeling by extending the FNO with KAN. Within the EFKAN framework, we employ the FNO as the branch network to map the resistivity model to apparent resistivity and phase in the frequency domain, and we replace the MLPs with the KAN to explore the feasibility which utilizes the KAN to map these apparent resistivity and phase to the desired frequencies and locations. Furthermore, we use the spectral method to obtain the smooth and stochastically varying resistivity model, rather than merely embedding anomalies within a homogeneous half-space underground. We conduct nine numerical experiments to validate the effectiveness of EFKAN. The experimental results demonstrate that EFKAN not only achieves higher accuracy compared to EFNO equipped with MLPs but also exhibits faster computational speed compared to the conventional numerical computational method FDM, which suggests that EFKAN could serve as a potential surrogate model for rapid MT forward modeling. 

\section{Acknowledgments}
This work was supported in part by the National Natural Science Foundation of China under Grant 42474103 and Grant 61906170, in part by the Food Science and Engineering, the Most Important Discipline of Zhejiang Province under Grant ZCLY24F0301, in part by the Basic Public Welfare Research Program of Zhejiang Province under Grant LGF21F020023, in part by the Natural Science Foundation of Ningbo Municipality under Grant 2022Z233, Grant 2021Z050, Grant 2022S002 and 2023J403.

\newpage

\textbf{Code availability}
The source codes are available for downloading at the link: https://github.com/linfengyu77/EFKAN.

\textbf{Declaration of competing interest}
The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

\textbf{Data availability}
No data was used for the research described in the article.

\bibliographystyle{cas-model2-names}
\bibliography{ref} 

\end{document}

