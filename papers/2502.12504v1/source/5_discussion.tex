\section{Discussion}
\subsection{Generalizable Mechanisms to Enable Unbounded Behaviors in Simulation}

Simulations of real-world scenarios are particularly valuable when they produce realistic human behaviors that were not explicitly prompted. We refer to these as "unbounded behaviors" because they occur outside the expected framework of the simulation. Such behaviors are difficult to predict with traditional models, but multi-agent LLM systems have the unique potential to generate them. In Study 3, we observed unbounded behaviors such as cheating in a classroom simulation and shoppers neglecting to return carts in a parking lot.%, and littering in a graffiti simulation.

However, in these scenarios, additional mechanisms were needed to enable the emergence of unbounded behaviors. For instance, we had to introduce private rooms to observe cheating in the classroom, and we had to add clear stakes (such as "you have an infant waiting in the car") to see agents failing to return shopping carts. %In the graffiti simulation, littering occurred without any modifications, but not in the proportions observed in real-world studies.
If every simulation requires a custom mechanism to produce specific unbounded behaviors, the value of simulations diminishes —they would only reflect behaviors that researchers expect, thus undermining the purpose of simulation. However, if the mechanisms are generalizable, they can be widely implemented, allowing simulations to still produce meaningful unbounded behaviors. The question remains: were the mechanisms we added general or specific? With only two examples, it's hard to draw definitive conclusions, but there is reason to believe they could be broadly applicable.

In the classroom simulation, the introduction of private rooms was necessary to observe cheating. In hindsight, it seems obvious that students would only cheat if they had a private space to communicate without getting caught. Broadly speaking, communication is a fundamental mechanism in society that enables many unexpected outcomes. The invention of the telephone, the internet, and even home-delivered mail has had significant effects on the economy (increasing global trade and commerce), employment (enabling remote work), and culture (connecting distant people). Any simulation that tests cooperation will likely need diverse communication channels (private, public, synchronous, asynchronous) to allow for various forms of interaction and collaboration.

In the shopping cart simulation, adding stakes was essential. Studies show that people with a child waiting in the car are less likely to return their cart. However, we initially did not see this effect in the simulation. Upon reflection, the condition "having a child" was too vague. If the child was 12 years old, there was little pressure to skip returning the cart. But if the child was 5 months old, there was significant urgency. When we clarified the child's age in the simulation, the unbounded behavior emerged for parents with a 5-month-old more frequently than for parents with a child of unspecified age. This outcome makes sense, as it was a flaw in the initial setup to specify only "having a child" without including age. To simulate unbounded behaviors effectively, simulations need enough contextual details for agents to understand the stakes and reason for their actions.

In the future, more work is needed to discover a set of mechanisms that enable simulations to exhibit rich, bounded human behavior. Communication channels and detailed contextual stakes are likely two such mechanisms, but there are probably many more that need to be discovered.

\subsection{How can policymakers benefit from imperfect simulations with multi-agent systems?}
These experiments showed that multi-agent LLM simulations can replicate the direction of effects (though perhaps not the magnitude) and extend to situations not explicitly covered in the literature. They also have the capability to exhibit unbounded actions. These are all encouraging signs that these simulations could be a valuable tool for policymakers, but the reality is that they will never be perfect. The question is: can they still be useful despite their imperfections?

Even if simulations are not completely accurate, they can still serve as ideation tools to imagine different possible outcomes. Prospection — the ability to think and reason about the future—is a hallmark of human intelligence, yet it is something that many people struggle with. When thinking about the future, people often find it difficult to account for the complexity of multiple independent actors' decisions, fall prey to cognitive biases (such as optimism bias when they want a policy to succeed), and struggle to imagine novel situations for which they have little prior experience. Although future thinking is important, it is cognitively taxing, and any aids might be better than relying solely on our limited cognition. LLM systems, while imperfect, are relatively fast, low-cost, and can run multiple scenarios. In many cases, a reasonable simulation is better than none at all.

If simulations are accurate in predicting the direction of effects and relative effect sizes (even if not their exact magnitude), they can still be valuable for testing different policies to determine whether they will be effective (even if not precisely how effective). This can be especially useful because some policies are known to fade quickly over time, becoming ineffective unless frequently renewed, while others persist by establishing new norms—such as making organ donation the default option instead of requiring an opt-in. A simulation, particularly one that tracks effects over time, could reveal these policy limitations. Additionally, simulations can expose potential negative externalities, such as added bureaucracy or unintentionally excluding participants (for example, when new market regulations make it harder for newcomers to enter).

Simulations can also serve as a boundary object that helps individuals from diverse backgrounds integrate their knowledge. Many complex scenarios involve various areas of expertise and perspectives. In a city, a proposal like introducing a bike lane affects many departments —- transportation, waste management, emergency route planning, and legal services. Instead of each department prioritizing its own concerns, a simulation might help different groups see how their concerns intersect, fostering a more collaborative discussion. Thus, simulations could function as a social glue for large organizations.

While there is much work to be done with policymakers to determine the value of imperfect simulations, there are reasons to believe they could be useful for tackling a challenging and important problem that currently has few affordable and accessible feedback mechanisms.

% These experiments showed that MA-LLM simulations can replication the direct of effects (although not the magnitude), and can extend to situations not explicitly seen before the the literature. They also have the capability to exhibit unbounded actions. These are all encouraging signs that these simulations could be a tool for policymakers, but the obvious truth is that these simulations will never be perfect. Can they still be useful, even if they are imperfect?

% Even if simulations are not completely accurate, In the worst case they can act as ideation tools to imagine different possible outcomes. Prospection - the ability to think and reason about the future - is one of the hallmarks of human intelligence - and yet it is something most people struggle with. When trying to think about the future people struggle with the complexity of thinking through multiple independent actors' decisions, they struggle with cognitive biases, such as optimism bias when they want a policy to have the desired effect, and they struggle to imagine novel situations for which the brain has less experience to draw from. Although future thinking is important, it is hugely cognitively taxing, and any aids might be better than our own limited cognition. AI systems - albeit imperfect - are fast, low cost, and can be done many times. In many situations, any reasonable simulation might be better than none.

% If simulations are accurate in their effect direction and the relative effect size (but not magnitude), this can still be a good tool to test different policies to at least say if they will be effective (even if they can’t say how effective). This is useful because some policys are known to fade quickly over time, rendering them ineffective unless repeated often. Whereasother policies are known to persist - because they create a new norm - like making organ donation the default (rather than an opt-in). A simulation  - particularly one that helps look at effects over time could expose these policy flaws. Additionally, simulations can expose potential negative externalities - such as like additional paperwork or inadvertently excluding participants (often adding regulation to markets makes it harder for new people to enter). 

% Simulations can also be a boundary object that help people from multiple backgrounds integrate their knowledge. Many complex scenarios involve many types of expertise and personas. Cities have many types of experts, and a proposal like introducing a bike lane will affect many of them - transportation, garbage collection, emergency route planning. Legal. Rather than every department seeing it’s concerns as the top priority, a simulation might help groups see how their concerns are integrated and start a conversation. Thus, simulations could function as  “AI as social glue.” for large organizations [AI as social glue]

% Whereas there is much work to be done with policymakers to find the value of imperfect simulations, there are reasons to believe they can have utility for a very hard and important problem that currently has few alternatives for cheap and easy feedback mechanisms. 

\subsection{Future interfaces for simulations}
Although the simulations in this study were set up and run in a console without a specialized user interface, there are many ways people might want to interact with simulations in the future that would require rich user interfaces. After a simulation has been run, users may want to step through it to better understand the sequence of events that led to a particular outcome. They might also wish to compare two runs of the simulation with different outcomes to hypothesize which factors contributed to those differences. While these simulations were run in a batch mode (without user interaction), many users may prefer interactive simulations that allow them to interrupt the process, adjust parameters, or intervene to try to create or prevent certain outcomes. Additionally, users may want to fork a simulation at a particular stage to explore multiple parallel scenarios.

Since designing and validating a comprehensive simulation environment with all the necessary mechanisms is challenging, there could be a future where people use or modify existing environments rather than building one from scratch. For example, a large city simulation could be utilized by various organizations: city governments could use it to prepare for natural disasters, hospitals to allocate resources during medical emergencies, businesses to explore store locations, and civic planners to mitigate the effects of climate change. Such environments could be developed collectively and made available to the public, much like open-source code. However, there are also ethical concerns; simulations could be misused in harmful ways, and there may be reasons to restrict access to simulations of public places or sensitive areas.  As LLM simulations become easier to use, it is critical to build safegaurds to limit "bad actors" from using simulations to model nefarious situations like war or voter/consumer manipulation. Furthermore, overreliance on simulations is a major concern - AI simulations are much cheaper and easier to conduct than real design and policy work. However, when designing policies for people, it is still important to run experiments with real humans and use traditional methods to think through all possible consequences; simulations should only be used as an additional thinking tool, as opposed to being viewed as a standalone representation of ground truth. \color{black}

% Although the simulations in this study were set up and run in a console with no special user interface, in the future there are many ways that people might want to interaction with simulations that would require rich user interfaces. After a simulation has been run, users might want to set through the simulation to better understand what steps lead to a particular outcome. They might want to compare two runs of the simulation with different outcomes to be able to hypothesize which steps led to a particular outcome. These simulations were run in a batch (without user interaction); many users want to run interactive simulations, allowing them to interrupt the simulation and change parameters to interfere to try to create or prevent certain outcomes. Users may also want to fork a simulation at a particular stage to explore two parallel patterns. 

% Since a rich simulation environment with all the necessary mechanisms in place is hard to design and validate, there may be a future where people can use or modify other users' environments rather than building one from scratch. An environment of a large city could be used by many different organizations - city governments can use it to prepare for natural disasters, hospitals can use it to allocate resources during medical emergencies, businesses can use it to explore store locations, and civic planners can use it to mitigate the effects of climate change. Environments can be created collectively and used by everyone like open source code. However, there are many unethical ways a simulation could be used, and there may be reasons to restrict access to simulations of public places or sensitive areas. 

% \color{red}\subsection{Extending the Results of Previous Research}
% % While the results of our studies show several new findings, there is also consistency with past research. Prior work has shown that LLM simulations are able to capture directional effects in published and non-published lab experiments with human subjects \cite{hewitt2024predicting}. Our studies support this finding, specifically studying the PGG as a case study - we replicate published PGG experiments in the first study, while we study "unpublished" experiments in the second study. Our third study finds that generative architectures are able to produce reasonable "emergent" behavior - behavior that LLMs were not explicitly instructed to engage in - in simulation, a finding that has been supported in past research of simulations in other real-world contexts \cite{park2023generativeagentsinteractivesimulacra}. Altogether, this consistency begins to compellingly demonstrate the promise that multi-agent LLM architectures show in being able to simulate human behavior to an acceptable degree and to be used by policymakers as low consequence testing and thinking tools. 

% Past work has shown LLM simulations accurately capture directional effects in both published and unpublished results – but it doesn’t study PPG and doesn’t use an architecture that would allow for "unbounded behaviors" that are necessary for policymaking \cite{hewitt2024predicting}. Past research has shown that generative architectures result in emergent behavior in simulation but do not rigorously evaluate its consistency with observed human behavior \cite{park2023generativeagentsinteractivesimulacra}. Past research has shown that multi-agent architectures perform better than single LLMs in economic games, but do not address the PGG \cite{sreedhar2024simulatinghumanstrategicbehavior}.
% \color{black}

\section{Limitations and future work}
This paper represents an initial exploration of multi-agent LLM systems as a tool for simulating human behavior and has several limitations. While the findings demonstrate that the proposed approach is feasible, there remain many open questions and challenges that need to be addressed.

%Hyperfocus on prosocial cooperative behavior and PGG. 
This paper focused on prosocial cooperative behavior and used the public goods game (PGG) as a vehicle for many of the preliminary studies (Studies 1 and 2). Although the PGG closely mimics real-world scenarios like taxation and charitable donations, it has limitations. First, the outcomes of the PGG depend on its multiplication factor for increasing the public pot. Typically, a factor of 1.6 is used, but if the multiplication factor is lower, the potential benefit is reduced, which could alter the game's dynamics. We lack human data for all possible values of the multiplication factor for comparison. Additionally, there are many variants of the PGG that were not tested: for example, punishment is an important mechanism that was not explored, though it has been shown to replicate in other games like the Ultimatum Game. There are countless mechanisms to replicate, and the more games we replicate, the more confidence we can have that multi-agent LLM systems are consistent with human behavior. At this point, there is a growing body of evidence from AI, HCI, economics, and psychology suggesting that LLM-agents can capture much of human behavior; what remains is to identify and refine the mechanisms that enable successful simulation.
% This paper focused on prosocial cooperative behavior, and used the public goods game as a vehicle for many of the preliminary studies (Studies 1 and 2). Although the PGG closely mimics many important real world scenarios like taxation and charity donation, it is also limited. The first the outcomes of the PPG game depend on its multiplication factor to increase the public pot. Typically 1.6 is used, but if the multiplication factor is lower, then the potential benefit is lower, and that could change the overall dynamics of the game. We don’t have human data for all possible values of the multiplication factor to compare to. Additionally, there are many variants of the PPG that we did not test: punishment is an important mechanism we did not test, but it has been shown to replicate for other games like the Ultimatum Game. There are an unending number of mechanisms to try to replicate. The more games replicate, the more confidence we have that MA-LLMs are consistent with human behavior, the more confidence we can have in AI simulations. At this point, there is a growing body of evidence from AI, HCI, Economics and Psychology that largely conclude that AI can capture much of human behavior; what remains to be done is capture and simplify the mechanisms that allow for successful simulation.

%No guarantees of  generalizability? 
% Our second study, along with other studies replicating multiple experiments not yet published [willer], indicate that LLMS are not simply echoing results of previous papers. Thus, there is potential for LLMs to have simulate human behavior for novel scenarios. It’s unclear how LLMs have these abilities. Possibly LLMs are using knowledge of past experiments and are transferring principles learned from one paper onto new scenarios. An extreme hypothesis is that LLMs are not using any experimental knowledge and have truly understood human nature from training data. It’s unclear how they have these abilities, and it’s also unclear what the limits on it might be. For any novel scenario put in a simulation, we will never know whether the LLM is capable of simulating those abilities or not.  Other work has shown that not all studies replicate [willer], but future work will have to test the bounds on when LLMs are more or less likely to simulate behavior. 

Our second study, along with other studies replicating multiple unpublished psychology experiments ~\cite{hewitt2024predicting}, indicates that LLMs are not simply echoing the results of previous papers - there is potential for LLMs to simulate human behavior in novel scenarios. It is unclear how LLMs possess these abilities since they don't have inherent motivations like humans. Perhaps \color{black} they are using knowledge from past experiments and transferring principles learned from one context to new situations. An extreme hypothesis is that LLMs may not be relying on experimental knowledge at all but have genuinely understood aspects of human nature from their training data. However, the limits of these abilities are also uncertain. For any novel scenario in a simulation, we cannot be certain whether the LLM is capable of simulating the required behaviors. While previous work has shown that not all studies replicate, future research will need to test the boundaries of when LLMs are more or less likely to simulate human behavior accurately.

%complex effects like interactions
Our studies primarily tested single interventions, but real-world scenarios are more complex, often involving multiple policies that can interact in unpredictable ways. For example, adding transparency might increase donations in the PGG, while negative priming might decrease them. If both policies are present, the outcome could vary depending on the specifics of the situation, and there may not be a definitive ground truth to compare against. In the future, more psychological experiments may need to be conducted to inform simulation design.
% Our studies mostly tested a single intervention, but in the real-world more complex things can happen - multiple policies can be present and combine and have a complex interaction. For example, adding transparency should increase donation in the PGG, but negative priming should decrease it. If both policies are present, what will happen? Psychology does not have a clear answer for this - the outcome depends on the specifics of the situation, and there might not be ground truth to compare to. In the future, more psychology experiments might have to be run to inform simulation. 

%POPULATIONS
Much of the experimental evidence from the social sciences is based on Western, Educated, Industrialized, Rich, and Democratic (WEIRD) populations. If LLMs do replicate human behavior, it is uncertain whether they can replicate all human behaviors, or only those represented in training data, such as experiments and online content. Further research is needed to understand to what extent LLMs can simulate behaviors of diverse groups, including children, marginalized communities, and others who are underrepresented in the training data. Past research has demonstrated that people from different types of communities act differently in the same strategic scenarios \cite{oosterbeek2004cultural, henrich2000does, kozitsina2020ethnicity} - it is important to ensure that simulations are representative of the population for which policymakers using them are designing for. \color{black} Additionally, people often behave differently toward their in-group versus an out-group, and it is unclear whether LLMs can capture these nuances.
% Much of the experimental results from the social sciences are based on Western, Educated, Industrialized, Rich, and Democratic (WEIRD) populations. If LLMs do replicate human behavior, it is unclear whether they can replicate all human behavior, or just those seen in training data such as experiments and online content. Further research would require understanding to what degree LLMs replicate WEIRD populations, children, marginalized communities, and other groups that are not well represented in the training data. People also act different to their in-group (or tribe) than they do to an out-group (or competitors). It’s unclear if LLMs capture these nuances of human populations.

% LLMs evolve.
The foundational LLMs used in these simulations continue to evolve. Simulations that replicate human behavior today may not do so in the future. As LLMs are trained on new datasets, their behavior could change for better or worse; high-quality human behavior data (e.g., from video sources) could improve the model, while low-quality or synthetic data could lead to degradation (e.g., mode collapse). Reinforcement Learning from Human Feedback (RLHF) can also cause certain LLMs, such as GPT-4, to display hyper-rational behaviors that exceed typical human responses, especially in emotionally charged situations where humans would behave more irrationally. Experimental results show that LLMs may react to induced emotions (e.g., happiness or anger) in disproportionately rational ways~\cite{mozikov2024good}. Future work may need to track how well simulation results replicate on different models over time.

% The foundational LLMs these simkluations are built on continue to evolve. Simulations that replicate human behavior today might not replicate in the future. LLMs change in many ways: As LLMs train on new datasets agent behavior could change for the better or worse - more data of high quality human behavior data (perhaps from video data) could enhance the model, but low quality, or synthesized data might make it worse (or exhibit mode collapse). Reinforcement Learning from Human Feedback (RLHF) can cause certain LLMs (such as GPT-4) to display hyperrational behaviors that exceed those of humans. This hyperrationality becomes more pronounced in more emotionally charged situations where humans would behave more irrationally. Experimental results show that LLMs might react to induced emotions (such as happiness or anger) in ways that are disproportionately rational ~\cite{mozikov2024good}.

%Actual human cognition.
Lastly, there is a gap between how role-playing tasks are simulated and actual cognitive processes. Naive priming in role-playing situations often leads to caricature-like behaviors instead of genuine cognitive modeling. This is because the underlying RLHF mechanism relies on human crowd workers providing feedback on what ~\emph{they think seems like} plausible behavior for a particular character in a particular situation. However, stereotypical perceptions of behavior can often not match real human behavior in moral dilemmas. For instance, many people may think a Buddhist monk would react to the trolley problem by avoiding action, when in reality surveys have shown the opposite. Moreover, the humans in the loop tend to belong to similar populations characterized as “poorly paid gig workers” ~\cite{casper2023open}, biasing the kinds of perceptions that are represented. Addressing these limitations may involve leveraging techniques like Bayesian induction and internal dialogue to foster more sophisticated representation of agent behaviors.


% 

% \subsection{Mechanisms \& Limitations of LLM-Based Simulations}

% We can predict mismatches between simulation outcomes and human behavior by analyzing the mechanisms underneath the hood in LLM systems. One such mechanism is the bias in the population that produces the content of the training data. Since LLMs are largely trained on content from WEIRD (Western, Educated, Industrialized, Rich, and Democratic) populations, they tend to replicate behaviors typical of these groups ~\cite{atari2023humans}. This bias becomes evident when simulating economic games for which we have experimental evidence of cross-cultural differences in behavior. For instance, the Orma of Kenya have been shown to exhibit the near-total cooperation in public goods games ~\cite{henrich2000does}, which is not captured accurately by out-of-the-box LLMs.

% In terms of reinforcement learning mechanisms, anti-hallucination incentives in Reinforcement Learning from Human Feedback (RLHF) can cause certain LLMs (such as GPT-4) to display hyperrational behaviors that exceed those of humans. This hyperrationality becomes more pronounced in more emotionally charged situations where humans would behave more irrationally. Experimental results show that LLMs might react to induced emotions (such as happiness or anger) in ways that are disproportionately rational ~\cite{mozikov2024good}.

% Beyond hyperrationality, other behaviors and values are also enforced through RLHF to mitigate safety and liability concerns for LLM developers. Most of the popular LLMs (GPT, Gemini, Claude, Llama) are guided by a set of safety guidelines that enforce certain moral values, such as avoiding generating offensive or hurtful content. These guidelines are then given to human evaluators in the RLHF process to enforce. This moral bias deviates from varieties of human behavior that do not reflect morality or follow the same moral paradigm. In our experiments, we found that sometimes LLMs struggle to emulate "bad" behaviors, such as punishing, antagonizing, or reprimanding. For example, in punishment scenarios within the Public Goods Game, GPT-4 models rarely resort to punishment, even when interacting with selfish agents. Moreover, dialogue generated by LLMs in adversarial situations tends to remain polite and sycophantic, consistent with other literature observing sycophantic behavior ~\cite{sharma2023towards}.

% Lastly, there is a gap between how role-playing tasks are simulated and actual cognitive processes. Naive priming in role-playing situations often leads to caricature-like behaviors instead of genuine cognitive modeling. This is because the underlying RLHF mechanism relies on human crowd workers providing feedback on what ~\emph{they think seems like} plausible behavior for a particular character in a particular situation. However, stereotypical perceptions of behavior can often not match real human behavior in moral dilemmas. For instance, many people may think a Buddhist monk would react to the trolley problem by avoiding action, when in reality surveys have shown the opposite. Moreover, the humans in the loop tend to belong to similar populations characterized as “poorly paid gig workers” ~\cite{casper2023open}, biasing the kinds of perceptions that are represented. Addressing these limitations may involve leveraging techniques like Bayesian induction and internal dialogue to foster more sophisticated representation of agent behaviors.

% \subsection{LLMs as simulacra?}
% % Maybe include some discussion on the amplification of human behavioral trends that we notice in experiments. Would it be useful to think of LLMs as simulacra that exaggerate certain features of behavior?

% \subsection{Utility of Simulations Across Contexts}
% % Can we draw any conclusions about the relative utilty of simulations in different scenarios (eg when human behavior is more consistent but we don’t know how things play out at scale or over time, simulation is useful; when cross cultural differences make human behavior difficult to predict, simulation could be difficult to use)?

% \subsection{Additional Cooperation Mechanisms}
% Beyond the validated theories we tested here, experiments have investigated a wide range of factors influencing cooperation in similar games like commons dilemmas, including but not limited to individual differences such as social motives and gender, perceptual factors about the causes and uncertainty of the situation, and social structure of the participants in terms of group size and power differentials. These factors can be used to inform potential policy designs and further evaluate how closely simulations match human behavior across more granular factors.

% \subsection{Ethical Concerns for Policy Designers}
% % It's probably important to explicitly discuss the ethical risks of using simulations for informing policy design, and how we can mitigate those risks. For example, we can explicitly present methods for mitigating stereotype-based biases in agent behaviors.

% Using LLM simulations to inform policy design presents a host of potential ethical risks. For example, when simulations are used to inform policies that impact diverse populations, there is a danger that these biases could reinforce harmful stereotypes or perpetuate systemic inequities. Potential technical and human system mechanisms can be used to mitigate these risks.

% For example, bias detection mechanisms could be implemented to identify and flag stereotypical behaviors exhibited by simulated agents. The tricky part here is accounting for cross-cultural and cross-demographic variations in behavior while not falling into exaggerated stereotypes. While some mitigation can be done, ultimately the tool itself must be used with sensitivity to these biases and avoid over-reliance for design making.

% Transparency and explainability help make possible this kind of appropriate usage. It is important to embed into the tool itself a clear understanding of the limitations and potential biases inherent in the models. Decision-makers should be informed about the mechanisms used to reduce bias, and uncertainties in the models' predictions should be openly communicated. This enables policymakers to make more informed decisions and reduces the risk of over-reliance.

% Ultimately, there is an ethical responsibility to consider the potential societal impact of policies informed by LLM simulations. Policies that influence sensitive domains---such as criminal justice, healthcare, or economic redistribution---should be subject to thorough ethical reviews before being implemented. LLM-based simulations should complement rather than replace human judgment and expertise, especially in contexts where the stakes are high. It is crucial to involve diverse stakeholders in the policy design process to ensure that multiple perspectives are considered, and to validate the fairness and appropriateness of simulated outcomes. Finally, it is important to guard against the misuse of LLM-based simulations for manipulative purposes, such as using simulated human behavior to justify controversial policy decisions or to sway public opinion through synthetic social proof.

% \subsection{Limitations}

% % If we talk about mechanisms and limitations in the discussion, we don't need to go into depth here, we can just mention briefly. I'm not sure how we should split topics between discussion & limitations/future-work/conclusion. @lydia up to you.
% LLMs are good at simulating behavior of experiment participants, but it may turn out they are best at simulating WEIRD people. The experiments we replicate and the knowledge and studies we use to inform the real-world scenarios are likely drawn from WEIRD people, because of the convenience sampling performed in many published experimental studies. An extension of this work might examine the ability of LLMs to simulate behavior in a wider range of demographics. 

% \bigskip

% The system we use is limited by the underlying model (GPT4). There are many other models to chose from. Moreover, as LLMs change, the outputs will change.

% \color{black}

