\section{Introduction}

% //Motivation: 
Human prosocial cooperation is essential for our collective health, education, and welfare, but cooperation is also difficult to maintain. 
When everyone pays taxes, societies can collectively benefit from public goods like roads, safety nets, and schools. However, individuals who do not contribute get to keep their wealth and still benefit from public goods. 
This "freeloading" behavior can eventually erode public trust and lead to a tragedy of the commons. 
Psychologists study this behavior in the lab using games like the public goods game (PPG) where a group of 3 or more people all decide how much to donate to a public fund. Public funds are multiplied then redistributed equally. If everyone donates, everyone is better off. However, if someone decides not to donate, they benefit more than others - they receive the public funds and get to keep their initial wealth. Eventually, this freeloading behavior discourages everyone else from donating, and public funds are emptied. 
Encouraging cooperation is thus important and psychologists have discovered many factors that affect prosocial behavior such as priming (positive framing), transparency (social pressure), and varying endowments (inequality).  

% //Problem:
However, human behavior in the lab is different than in the wild. In the lab, people’s choices and actions are bounded: they only get to choose from a small set of options, such as a choice from a menu of how much to donate. But in the wild, choices are unbounded. Humans exhibit more complex behaviors including lying, cheating, persuasion, and gaming the system. These are often the behaviors that cause programs and policies to fail. They’re also very difficult to foresee. Studying human behavior in the wild using experiments is very challenging. It requires large numbers of people interacting, creating a control group or a matched sample to compare to, and numerous runs a to achieve statistical significance. This is almost never done, and thus policies affecting human cooperative behavior are rarely informed by experimental evidence.

% //Approach:
Multi-agent LLM systems have the potential to simulate complex human behavior and interactions. 
They exhibit human-like behavior \cite{park2023generativeagentsinteractivesimulacra}, including doing actions they were not explicitly told to do (emergent behavior), thus displaying the potential for unbounded actions. 
Beyond human-like behavior, LLMs and multi-agent LLM systems have also shown a capacity to replicate actual human behavior seen in lab studies. This includes 70 psychology experiments \cite{aher2023usinglargelanguagemodels, hewitt2024predicting}, and multi-player economic games with strategic thinking about what other players will do, such as the 
%like in many game theoretic games, like the 
multi-turn Ultimatum Game \cite{sreedhar2024simulatinghumanstrategicbehavior} and auctions \cite{manning2024automated}. 
For a simulation to be useful, it does not necessarily have to replicate human behavior exactly, but it does have to exhibit a rich enough set of actions consistent with behaviors that are observed, and provide evidence that the results can generalize to previously unseen situations.
To lay the groundwork for a future where policy makers can ask LLM-agents to simulate human behavior in response to policy changes, we address the following  questions:
\begin{itemize}
    \item[(1)] Do multi-agent LLM systems understand human cooperative behavior well enough to simulate it, even in a lab setting?
    \item[(2)] Are multi-agent LLM systems simply echoing the results from previous papers, or can effects transfer across papers and apply more broadly?
    \item[(3)] To what extent are multi-agent LLMs complex enough to simulate the rich set of unbounded options, actions, and interactions people do in the real world, outside of the lab? 
    \item[(4)] To see the complex interactions observed in the real world, what simulation mechanisms do we have to add to the set up of the system?
\end{itemize}
% (1) Do multi-agent LLM systems understand human cooperative behavior well enough to simulate it, even in a lab setting?
% (2) Are multi-agent LLM systems simply echoing the results from previous papers, or can effects transfer across papers and apply more broadly?
% (3) To what extent are multi-agent LLMs complex enough to simulate the rich set of unbounded options, actions, and interactions people do in the real world, outside of the lab? 
% (4) To see the complex interactions observed in the real world, what simulation mechanisms do we have to add to the set up of the system?

% //Results:
Our experiments show that multi-agent LLM systems replicate findings of three lab studies of human cooperative prosocial behavior, showing that LLM-agents are consistent with humans in their responses to priming, transparency and varying endowments. The simulation accurately replicated the direction of the effect (positive or negative), but often exaggerated the effect size. In addition to replicating the findings of previous studies, we show that multi-agent LLM systems can apply insights from other games to the PGG. Specifically, we demonstrate that two priming effects previously studied in other games — but not tested in the PGG — also have a similar impact on behavior in the PGG. This indicates that multi-agent LLM systems are not simply echoing existing research but have a broader ability to simulate psychological mechanisms across different games, situations, and simulations.

We also show that for two real world situations, we do see complex, unbounded behavior consistent with anecdotal evidence. For a classroom scenario with a teacher testing various late policies, we find that when late policies are strict and assignments are hard, we can enable the behavior of cheating to emerge. For a shopping center parking lot scenario, we find that LLM-agents can be affected by external conditions like humans in returning or not returning their shopping cart. For both scenarios, simple mechanisms had to be added to the simulation to see these effects. For cheating, the simulation had to include communication channels like a private student room to initiate the communication needed for a behavior like cheating. For agents to not return their cart, their provided information had to allude to what was at stake as a result of their external condition.


% //Discussion:
Altogether, we conclude that multi-agent LLM systems show remarkable promise towards simulating human prosocial behavior.  
For broader real-world scenarios, we find that there remain additional mechanisms to identify in order for multi-agent LLM systems to simulate cooperative behavior, including (1) creating communication mechanisms that allow agents to coordinate behavior privately and (2) make sure the stakes (or incentives) driving behavior are clear. 
These are far from the only mechanisms needed, and there is much future work to discover more mechanisms, but there is reason to believe that these mechanisms
% might be fairly broad, and 
will be broadly applicable across domains. 
Our simulations show that in the future, there is a potential for multi-agent LLM simulations to be useful in informing policy makers. Currently, LLM-agents can simulate the general direction of human behavioral effects, although they struggle to accurately capture the magnitude. While fine-tuning may improve this, it likely never perfectly replicates human behavior. 
Nevertheless, the ability to simulate directional trends can still help policymakers anticipate potential policy outcomes. Given the complexity and high social stakes of such decisions, even preliminary simulations could provide a valuable framework for exploring possible scenarios and guiding decision-making.




% MOTIVATION 

% Human collaboration in its ideal form involves a  kind of fluid sharing of ideas and resources. However, humans working together is a tenuous proposition; it requires people to sometimes sacrifice closely held ideas or possessions. As a result, many situations lead to dysfunctional collaboration. Human collaboration is so important that psychologists study it in a well-defined setting called the public goods game (PGG). This game is directly related to many situations. Taxation is a case in point: A community can do big things that can benefit everyone if everyone contributes their fair share. However, when a single agent benefits from shared infrastructure and simply stops donating, they are essentially freeloading of the contributions of others. If the practice spreads, the infrastructure deteriorates, and the resulting situation is called the tragedy of the commons. Because this is so important, psychologists study the many ways to influence people's decisions to donate to the public good.  For example, priming and transparency are common techniques shown to increase contributions. 


% \bigskip

% \color{red}PROBLEM
% Doing experiments is hard, especially on large groups. Experiments also may not translate into practice. We seek to understand the way in which AI can simulate human behavior in the PGG. We do this in three ways. We see if AIs can simulate previous results of papers. We then see if AI can combine ...~


% An important assumption of lab experiments is that the observed behavior is generalizable to the real world \cite{labexperiments} - human behavior in the real world is more complex than in a lab setting. In lab experiments, the environment is mostly fixed; people's choices of actions are often limited, and defined by the experiments - for example, people can either PAY or NOT PAY. But in the real world, the environment and the action space is unbounded. People can do other actions like cheat - people can persuade people they've payed when they have not. People can even even make it seem like they are unable to pay when they can  (for example, tax havens, usually on boundary of legality, allow people to hide income in order to reduce the amount of tax they pay). Previous research suggests that LLMs, and multi-agent LLM systems in particular, have potential to display these "unscripted" actions in simulation \cite{horton2023largelanguagemodelssimulated} and therefore help policy makers forsee the ways in which actual people will react in response to their proposals. This is accomplished via a lower stakes method without the actual consequences that would ensue from enacting a new untested policy.

% % Human behavior in the real world is more complex than in a lab setting. In a lab, the environment is fixed, and user’s choices of actions are often limited, and defined by the experimenters - for example, users can either PAY or NOT PAY. But in the real world, the environment and the action space is unbounded. Users can do other actions like cheat, users can persuade people they’ve PAYED when they have not. Users have things in their environment like tax havens that allow them to avoid taxes semi-legally. LLMs are interesting way to potential see see of these “unscripted” actions in simulation, in order to help policy makers SEE REAL USER BEHAVIOR, BEYOND LAB BEHAVIOR.

% \color{black}

% \sout{When creating policies, plans, or designs for people, it is challenging for designers to foresee all of the ways in which people may reason and behave. Simulations can be very helpful in this context, but it is challenging to accurately model (1) the ways in which people collaborate, both allowed (i.e., working together to solve problems) and not (i.e., cheating/collusion) and (2) the ways in which people communicate (what and with whom). People do not collaborate or communicate uniformly – their personalities \cite{mccrae2008five}, experiences \cite{Kidd2013}, and circumstances \cite{mullainathan2013scarcity} all affect their decision making and behavior. However, these factors are important to accurately consider in simulations to enable designers to anticipate ways in which people may react to or circumvent proposed policies.}
 
% \color{red}APPROACH\color{black}

% Recently, multi-agent LLM systems have shown remarkable capabilities in simulating human-like knowledge and behavior in a range of domains, including but not limited to social systems \cite{park2023generativeagentsinteractivesimulacra, park2022socialsimulacracreatingpopulated}, entertainment \cite{Callison_Burch_2022}, education \cite{gpteach}, economics \cite{horton2023largelanguagemodelssimulated, aher2023usinglargelanguagemodels}, and judicial trials \cite{hamilton2023blindjudgementagentbasedsupreme}. Specifically, LLMs have also been shown to be able to replicate behavior observed in prior human experiments \cite{hewitt2024predicting}, allowing researchers to benchmark behavior in LLM-simulations with actual human behavior. We extend this by focusing on the interesting case of prosocial situations – where people can voluntarily act in a way that benefits another person or society as a whole. We (1) replicate treatments from prior lab experiments with human subjects in variations of the PGG to confirm LLMs capture observed human behavior; (2) simulate scenarios that combine treatments used in lab experiments of other games and the PGG to verify that LLMs are not parroting published literature; and (3) simulate two real-world scenarios (a classroom with late-policies and a grocery store shopping cart return) towards identifying mechanisms necessary to enable human-like behavior.
 
% The public goods game (PGG) is a classic economics experiment where players begin with a set endowment and can choose to voluntarily contribute a portion of it to a public pool, which will be multiplied by a factor then distributed evenly amongst the players. Players also keep the amount they do not contribute. In the PGG, the group’s total payoff is maximized when everyone contributes all of their initial endowment to the public pool. However, economic theory dictates that the most profitable strategy for individual players is to make zero contribution to the public pool, because they may benefit from the contributions of other more charitable people while at the same time hoarding their personal wealth. However, these decisions are rarely seen in human experiments – human players generally voluntarily contribute a portion of their endowment to the pot, although contribution levels can vary \cite{stuffinpgg}. Furthermore, priming \cite{Eriksson_Strimling_2014}, variations in endowment \cite{HARGREAVESHEAP20164}, and transparency of communication \cite{transparencyandcooperation} can greatly affect outcomes. \color{red}ITS LIKE TAXES\color{black}

% \color{red}RESULTS\color{black}
 
% We use the PGG to test if LLMs act like humans in prosocial contexts. We evaluate whether LLM-agents are affected by priming, transparency (i.e., social pressures), and varying endowments (i.e., unequal resources) similar to the ways in which humans are by extracting human actions from economics literature. We also verify the robustness of this replication by also testing combinations of existing literature, examining whether LLM simulations yield results consistent with reasonable extrapolation of what would happen based on the results from each experiment. We find that in replicating lab experiments of the PGG, multi-agent LLM systems display the differences observed as a result of experimental treatments (priming, transparency, and varying endowments), but with a larger effect-size. In combining experiments, we find that multi-agent LLM systems display the expected differences of experimental treatments.

% \color{orange}We try simulating two real-world scenarios as case studies towards beginning to identify specific mechanisms required for simulations to display plausible human communication and collaboration. First, we simulate a classroom setting with changing late policies and various perturbations to student lives, finding that even in the most pressing of conditions, student-agents do not ever consider cheating - until a separate room is introduced that only student-agents can enter and communicate with one another without the professor-agent overhearing. Second, we simulate shopping-cart returns in a store parking lot with various conditions affecting shopper-agents (ex. having a child with them, running late to a meeting, etc.), finding that shoper-agents almost always return their cart regardless of the conditions affecting them - until the prompts of the conditions allude to or explicitly state the consequences of them.\color{black}

% Altogether, we conclude that multi-agent LLM systems show remarkable promise towards simulating human prosocial behavior. However, for broader real-world scenarios, we find that there remain additional mechanisms to identify and introduce in order for multi-agent LLM systems to simulate plausible behavior, identifying two of them - partitioning agents into rooms to allow for private communication and defining consequences as part of the prompting - via case studies, a repeatable process for identifying further necessary mechanisms. Altogether, the results of our simulations of lab experiments and real-world case studies suggest a future in which simulations using multi-agent LLM systems will be a powerful tool to assist policy-makers.