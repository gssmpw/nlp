\section{Related Work}

\subsection{The Public Goods Game}

The public goods game (PGG) is a particularly important case of human cooperative prosocial behavior studied by psychologists and economists. In the PGG, a set of three or more players are each given an endowment of money, of which they choose a portion of to voluntarily contribute in-private to a public pool. The public pool will then be multiplied by a factor, say 2, then redistributed evenly amongst the players. In a game with four players, if every player has a \$20 endowment and contributes half of their endowment, \$10, each player will receive a payoff of \$30: \$10 from the amount of their endowment they kept and \$20 from the payoff from the public pool. If each player donates nothing, they all simply keep their initial endowment of \$20 without any additional payoff. In the PGG, working cooperatively can be mutually beneficial. However, if a player decides not to contribute to the public pool, the player can increase the payoff — in the example previously discussed, if one player decides not to contribute to the public pool, the payoff is \$35 — \$20 from the endowment the player kept, and \$15 from the payoff from the public pool. Hence, not contributing to the public pool can result in a greater payoff for an individual, despite contribution to the public pool by \textit{someone} being necessary for there to be any additional payoff for players. This game mimics situations involving prosocial behavior in the real-world, such as taxation — when paying taxes, people are forced to contribute some of their own money to a public pool to develop infrastructure and services for the broader community. People can even try to avoid paying taxes through tax havens or loopholes and freeload off public goods.

Lab experiments with human subjects show that in single-round (one-shot) PGGs, people do typically contribute a portion of their endowment — on average, 50\%. When the game is played for multiple rounds, the amount that players contribute decreases each round, until it is effectively zero \cite{labexperiments}. However, introducing variations to the game can increase or decrease prosocial behavior. Priming participants by presenting the game under different names \cite{Eriksson_Strimling_2014} or having participants primed with words alluding to cooperation before playing the PGG \cite{drouvelis2015priming} increases their average contribution amount. Introducing transparency of contributions also increases average contributions — having participants write their contribution on a blackboard after each round \cite{rege2004impact} or announce their contributions publicly \cite{transparencytwo} increases the average contributions made in the PGG. Finally, varying the endowments that players have to begin with in the PGG also has an effect on average contributions — in experiments with individuals given low (\$20), medium (\$50), and high (\$80) endowments, individuals with the low and medium endowments contributed the same amount in games where endowments were the same as in games where endowments were varied. However, individuals with the high endowment contributed much less in games where endowments were varied \cite{HARGREAVESHEAP20164}.
% were varied than in games where endowments were equal 

These effects are not limited to the PGG and can be seen in select real-world scenarios. In fundraisers, audiences may be primed to cooperate towards a common goal. In many settings, pledges are made publicly (that is, transparently), which increases donations \cite{Bhati2020-xw, Oppenheimer2011-zh}. Similar strategies are used by policy makers seeking to levy taxes for a bridge or other local public goods \cite{parks2013cooperation}. Furthermore, lower class individuals have been demonstrated to be more prosocial than their higher class counterparts in studies of generosity, charity, trust, and helpfulness \cite{piff2010having}. Overall, although free-loaders may be able to benefit off of public goods — whether in the PGG or in the real world — there are strategies that policy makers can use to increase prosocial behavior.

% \color{red}One can often see the above results play out in fund-raising events. Those doing fundraising will prime an audience to cooperate toward a common goal. In many settings, pledges are made publicly: this increases transparency, which in turn increases donations \cite{Bhati2020-xw, Oppenheimer2011-zh}. Similar advice is given to policy makers seeking to levy taxes for a bridge or other local public goods \cite{parks2013cooperation}. \color{black}


% \section{Background on the Public Goods Game}
% Background:
% A particularly important instance of human cooperative prosocial behavior studied by psychologists and economists is the public goods game (PGG). 
% In the PPG, a set of 3 or more players are each given an endowment of money, and they can choose to voluntarily contribute a portion to a public pool. 
% The public pool will then be multiplied by a factor (typically 2), and then redistributed evenly. 
% If the game has 4 players and every player has a $20 endowment and donates 50\% to the public good, each player has \$30 in the end 
% ($10 from their endowment + (\$20 from splitting the pot after multiplication). 
% But if each player donates nothing, they all simply keep the \$20. 
% In PPG, working collaboratively can be mutually beneficial. 
% However, if one player decided not to donate, the 3 players that donate all get \$25 (\$10+\$15 for the split pot), and the player that didn’t donate gets \$35 (\$20+\$15 from the split pot). 
% The PPG game can broadly mimic situations of prosocial behavior, such as taxation, ….., making it a valuable game to simulate. 


% Human subject studies show that when a single round of PPG is played, people do act prosoically and donate X\% of the money on average. 
% However, If the game is played for multiple rounds, the amount the players donate drops every round, until it is effectively zero. 
% However, factors like primincy and transparency can increase prosocial behavior.
% These lab results are important in that they show that humans do act prosocially and can be prompted to do so. 
% These results can guide policy makers, but don’t capture of complexity of most real world simulations or allow policy makers to compare two different policies. 

% BIG THING: IF YOU DON'T CONTRIBUTE, YOU GET MASSIVE PAYOFFS!

% \section{Related Work}

% \subsection{Human Prosocial Behavior?}

% \subsubsection{prosocial behavior / cooperation / big picture}

% \subsubsection{Human Behavior in Public Goods Game Experiments}

% \sout{Studies with human subjects demonstrate that humans often act "irrationally" in economic games, choosing to value social norms over profit maximization. This remains true for the PGG.} Humans are often better off when we collaborate with one another, when we all pay our taxes, when we shovel our sidewalks, when we pull over for ambulances.. but in any situation where everyone should contribute, there's ~stuff~ that happens. Someone prioritizes themselves over others, or someone cheats, or groups of people collude together. 

% In the PGG, total payoff is maximized when all participants contribute their entire endowment to the public pool, while individual profit is maximized when the individual participant contributes nothing \cite{6d947858-4a17-3462-9053-fc55b58ffee1}. However, in experiments with human subjects, participants typically contribute roughly half of their endowment in one-shot and the first round of multi-round PGGs \cite{labexperiments, stuffinpgg}.

% \color{red}explain, priming.... transparency..... endowments.....\color{black}

% Moreover, the behavior of human participants in such experiments is significantly affected by various treatments. More generous behavoir follows priming for teamwork, cooperation and unity; less generous behavior follows priming for taxation, competition, or proportionality concepts reduces their willingness to contribute \cite{Eriksson_Strimling_2014, moralsmatter, drouvelis2015priming}. Transparency in contributions (i.e, participants make their contributions publicly for all other participants to see) also results in higher contribution amounts as compared to without \cite{KHADJAVI2017468, transparencytwo}. Giving different players different endowment amount also leads to interesting results, with "rich" participants contributing less to the public pool when the remaining participants have lower endowments as compared to when everybody has the same endowment \cite{HARGREAVESHEAP20164}. \color{red} SO WHAT \color{black}

% \color{red}, Oppenheimer2011-zh}. Similar advice is given to policy makers seeking to levy taxes for a bridge or other local public goods \cite{parks2013cooperation}. \color{black}



\subsection{Replicating Bounded Scenarios with Human Subjects using LLM Simulations}

Prior research in psychology, economics, and artificial intelligence has compared results from LLM simulations to experimental data from studies with human subjects, finding that outcomes in LLM simulations generally mirror those from the real world. Using a single LLM, past work has been able to replicate results from economic and psychology experiments with humans, including the Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds \cite{aher2023usinglargelanguagemodels}. Single LLMs have also been able to replicate results from an archive of seventy social science experiments — both published and unpublished. In published studies, LLM simulation results had correlation coefficient of r = 0.85 with results from lab experiments with humans. In unpublished studies which could not have appeared in the LLM's training data, the correlation was also strikingly high (r = 0.90) \cite{hewitt2024predicting}. However, LLMs have been found to potentially miss nuances in human behavior in psychology experiments, potentially overestimating and giving false positives \cite{cui2024aireplacehumansubjects}. But overall, past work suggests that single LLMs do have a general sense of human behavior in tightly bounded experimental situations. 

Prior research has also used multi-agent LLM systems to replicate human behavior in economic and psychological settings. Prior work has used multi-agent LLM systems to simulate economic games like the Dictator Game, as well as agents' immediate responses to price increases or budget allocations, finding that it is possible to change LLM-agent behavior by giving them different endowments, information, and preferences \cite{horton2023largelanguagemodelssimulated}. Frameworks have also been presented to simulate scenarios such as negotiations, bail hearings, job interviews, and auctions, finding that LLM-agents are able to predict the signs of effects, but not the magnitude \cite{manning2024automated}. However, these studies often lack robust comparisons to human behavior in similar real-world situations. Multi-agent systems have also shown the ability to replicate human strategic behavior in the ultimatum game more accurately than single LLMs, as assessed by comparing simulation results to human experimental data  \cite{sreedhar2024simulatinghumanstrategicbehavior}. However, the ultimatum game is a single economic experiment that has been extensively studied and thus likely appears often in the training data of LLMs, leaving room for further exploration.

% \color{red}Past research suggests that LLMs, and multi-agent LLM systems in particular, have immense potential to replicate human behavior in simulation. Prior research has replicated bounded behaviors in lab experiments with positive results. Prior research has also shown that LLM simulations can demonstrate realistic and emergent human behaviors, BUT HAS NOT TIED TO THE BASELINE SUFFICIENTLY.

% THIS PARA IS WEAK --> BETTER JOB OF SAYING OURS IS DIFFERENT BECAUSE... OBVIOUSLY THERE IS A LOT OF POTENTIAL, THERE"S BEEN SOME PRIOR WORK SHOWING ITS GOOD, BUT WE're EXACTLY FOCUSING ON UNBOUNDED BEHAVIORS THAT NEED MULTI-AGENT SYSTEMS, SO WE REPLICATE EXPERIMENTS BUT THEN GO BEYOND TO UNBOUNDED SCENARIOS WHICH IS NECESSARY FOR POLICY MAKERS. \color{black}

Past research has demonstrated that LLMs, and multi-agent LLM systems in particular, show promise in replicating human behavior in lab experiments where behavior is bounded. However, policymakers require simulations of unbounded scenarios - where people have nearly unlimited choices on ways to think, act, and behave. Our work begins by confirming that multi-agent LLM systems could replicate prosocial behavior in lab experiments, but then extends it into select real-world situations where agents had a more unbounded action space.\color{black}

% The discussed work suggests that LLMs are able to emulate human decision making in fixed scenarios, but do not explore open-ended scenarios that are actually necessary for simulations to provide value in real-world use cases.

% \sout{Overall, past research suggests that LLMs, and multi-agent LLM systems in particular, have immense potential to replicate human behaviors in simulation. This naturally leads to the exploration of multi-agent LLM systems ability to simulate specific types of human behaviors, such as prosocial behavior. BUT it doesn’t study PPG and doesn’t use an architecture that would allow for "unbounded behaviors" that are necessary for policymaking.}

% Prior research has compared results from LLM simulations to experimental data from studies with human subjects, finding that outcomes in LLM simulations generally mirror those from the real world. LLM has been demonstrated to produce results in simulations of social science experiments that strongly correlate with results from both published (85\%) and unpublished (90\%) studies with human subjects \cite{hewitt2024predicting}. LLM has also replicated results from economic and psychology experiments with humans such as the Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds \cite{aher2023usinglargelanguagemodels, sreedhar2024simulatinghumanstrategicbehavior}. However, LLM does potentially miss nuances in human behavior in psychology experiments, potentially overestimating and giving false positives \cite{cui2024aireplacehumansubjects}.

% \color{red}THIS NEEDS TO BE MORE! WILLER STUDY IS SUPER HELPFUL — IF A READER COMES IN THINKING NO WAY AI CAN DO THIS,  THERE PAPER HELPS A TON

% PARAGRAPH ABOUT HORTON, PARAGRAPH ABOUT US, PARAGRAPH ABOUT WILLER

% Prior research in psychology, economics, and AI (non-HCI) have shown that LLMs do have some capacity to replicate human subjects. Using vanilla LLMs (no multi-agent), Kalai showed some consistency (GPS, UG). There was consistency with payoff but no the gender (OR WHATEVER IT WAS). WILLET REPLICATED A WHOLE BATTERY OF CLASSIC PSYCHOLOGY STUFF WITH A SINGLE LLM. This inlcuded zyshbsfdal, published and unpublished paper. This has a strong implication that LLMs have a general sense of human behavior. Unpublished studies being replicated is even greater evidence of that [ but somehow we are better], 

% A few people have also done this w multi-agent. Horton focuses on auction/bail hearing/etc. and show that people can haggle with a cup. But haggling with a cup isn't real, blah blah blah. One paper has looked at human strategic behavior in the Ultimatum Game, showing multi-agents is better than single agents, and GPT4 is better than GPT3.5. So we've reached this threshold of humans doing things thinking about other stuff. But this is just one game with well published results. So we need to extend these studies to see if LLMs can actually replicate broad enough of human psychology that they can be trusted to full simulations of society.

% \color{black}

\subsection{Multi-Agent LLM Systems}

Previously introduced multi-agent systems have varying capacities for allowing unbounded human behavior. Prior research has presented systems for simulating scenarios such as negotiations, bail hearings, job interviews, and auctions. In such systems, the action space is somewhat limited, with the system having rigid turn-taking guidelines and detailed information on how the simulation should proceed \cite{manning2024automated}. Similarly, ChatArena is a system which allows users to simulate turn-taking games: setups for tic-tac-toe, rock-paper-scissors, and chameleon are provided by default, while users can modify input information to simulate games such as the PGG. However, the system is again rigid in the actions agents can take and when agents can take such actions \cite{ChatArena}.

There are also multi-agent LLM systems in which LLM-agents are allowed more complexity and freedom in the ways they can "think" and act: these are referred to as "generative agents" \cite{park2022socialsimulacracreatingpopulated}. MetaAgents is a framework in which agents have four modules: perception, memory, reasoning, and execution. Using a simulated environment of a job fair, the research studies agents' abilities to create teams and workflows for collaborative tasks, finding that agents show promising results in simulation \cite{li2023metaagentssimulatinginteractionshuman}. Architectures have also been presented to simulate the inhabitants of a town. Agents have modules for observation, memory, reflection, planning, and reacting. Multiple locations can also be defined for agents to move between. In simulation, agents are found to demonstrate emergent behaviors, but not compared to observations of human actions explicitly \cite{park2023generativeagentsinteractivesimulacra}.

Past work has also explored coupling large language models with simulation models to better represent human behavior and decision-making \cite{Ghaffarzadegan_2024}, as well as using LLMs to mediate between end-users and simulations \cite{giabbanelli2024broadening}. However, there remains further room for exploration in identifying mechanisms within multi-agent systems themselves to achieve similar goals.

% These systems show more promise in simulating unbounded human behavior, so we focus our work around them. 

% \color{purple} Previous research shows that multi-agent LLM systems can simulate realistic human behavior, but does not rigorously compare the results to actually observed human behavior WHICH WE DO. \color{black}

Whereas previous research shows that multi-agent LLM systems can simulate realistic human behavior, our work goes further showing that multi-agent LLM systems can simulate real observed human behavior in the lab. Additionally, we show that LLMs can simulate observed human behavior outside the lab, where there is an unbounded action space. For example, people can cheat.\color{black}

% In addition, such systems sometimes require added mechanisms to be able to observe specific behaviors. For example, for agents representing students that make a decision to cheat, agents must have a way of doing so without an agent representing an instructor knowing. 

% Recent work has explored coupling large language models with simulation models to better represent human behavior and decision-making \cite{Ghaffarzadegan_2024}, as well as using LLMs to mediate between end-users and simulations \cite{giabbanelli2024broadening}. 

% In contrast, we focus on identifying mechanisms within multi-agent systems themselves to achieve similar goals.

% turn-taking, fixed action spaces
% — chatarena
% — johns stuff

% not turn-taking, unbounded action spaces
% — park
% — ghaffarzedegan
% — limeta2023

% we are interested in using systems of the latter group. these have unbounded action spaces and allow the user to define the environments to varying levels of specificity. we identify specific mechansims that allow for unbounded behavior but put bounds on the environment


 
% \color{red}THIS SHOUDL NOT BE FIRST: Previous research suggests that multi-agent LLM systems especially show promise in simulating human behavior. Multi-agent LLM systems are able to successfully simulate simple games such as tic-tac-toe, rock-paper-scissors, and chameleon \cite{ChatArena}.\color{black} They have also been used as economic agents, being given endowments, information and preferences, and generating reasonable human behavior in scenarios via simulation \cite{horton2023largelanguagemodelssimulated}. Multi-agent LLM systems have also shown promise in simulating human behavior in more open-ended scenarios. Such systems have been used to simulate interactions within social systems \cite{Ghaffarzadegan_2024, li2023metaagentssimulatinginteractionshuman, park2023generativeagentsinteractivesimulacra}, demonstrated to create human-like negotiation strategies \cite{gandhi2023strategicreasoninglanguagemodels} and shown to adhere to human trust behaviors \cite{xie2024largelanguagemodelagents}. They have demonstrated the ability to simulate human-like behavior in a variety of contexts, including supreme court decisions \cite{hamilton2023blindjudgementagentbasedsupreme} as well as bail hearings, job interviews, and auctions (Manning, 2024). Multi-agent LLM systems have also been used to simulate human behavior in larger-scale settings, such as the inhabitants of a town \cite{park2023generativeagentsinteractivesimulacra} and people’s behavior during an epidemic \cite{williams2023epidemicmodelinggenerativeagents}.

% \color{red}THIS FEELS LIKE A LIST... NO MENTION OF MECHANISMS, THOUGHTS FEELINGS BEHAVIORS IN PARK THAT HORTONS/CHATARENA DOESNT. SETUP A SPACE. pARK HAS TAHIS PAPER TAHT GIVE PEOPLE XYZ AND THEY EXHIBITAED BEHAVIORS. LESS ABOUT THEY REPLICATE HUMANS BUT SHOULD BE ABOUT THE TECH. PARK IS NOT TURN-TAKING, CHATARENA IS, ETC. SO THESE SYSTEMS HAVE VARIOUS COMPONENTS ALL TOGETHER. SO PARK IS GOOD CAUSE IT CAN DO UNBOUNDED BUT THERE'S NO LIKE COMPARISON TO REAL WORLD. WE WANT UNBOUNDED ACTION SPACE BUT BOUNDED ENVIRONMENT TO SEE WHAT HAPPENS. WE CHOOSE TO USE THESE UNBOUNDED SYSTEMS BECAUSE THEY GIVE US GOOD ACTIONS, BUT THEN WE HAVE TO FIND THE MECHANISMS THAT DO APPROPRIATELY BOUNDED\color{black}

% Notably, multi-agent LLM systems (that is, systems in which agents have separate context windows and are thus partitioned from each other) have also specifically been demonstrated to outperform single LLMs in simulating human behavior. Previous research has found that multi-agent LLM systems more often replicate human-like outcomes in simulation than single LLMs in economic experiments \cite{sreedhar2024simulatinghumanstrategicbehavior}. Multi-agent LLM systems built with collaborative frameworks are able outperform single LLMs in task accomplishment \cite{chen2023agentversefacilitatingmultiagentcollaboration}.