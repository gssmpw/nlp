%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf, natbib=True, screen=False]{acmart}

%for tables
\usepackage{multirow}

%for teaser plots
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{shapes.misc}

%for pseudocodes
\usepackage{algorithm2e}


\usepackage{cleveref}
\Crefname{equation}{Eq.}{Eqs.}
\Crefname{figure}{Fig.}{Figs.}
\Crefname{table}{Tab.}{Tabs.} 
\Crefname{appendix}{App.}{Apps.} 
\crefformat{section}{\S#2#1#3}
\crefformat{subsection}{\S#2#1#3}

%for the definition environment
\theoremstyle{definition}
\newtheorem{definition}{Def.}

\makeatletter


%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.


\setcopyright{cc} 
\copyrightyear{2025} 
\acmYear{2025} 
\setcctype{by-sa} 
\acmConference[WWW '25]{Proceedings of the ACM Web Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia} 
\acmBooktitle{Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May 2, 2025, Sydney, NSW, Australia} 
\acmDOI{10.1145/3696410.3714589} 
\acmISBN{979-8-4007-1274-6/25/04}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}
\DeclareMathOperator*{\argminA}{arg\,min}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Joint Evaluation of Fairness and Relevance in Recommender Systems with Pareto Frontier}
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Theresia Veronika Rampisela}

\orcid{0000-0003-1233-7690}
\affiliation{%
  \institution{University of Copenhagen}
  \city{Copenhagen}
  \country{Denmark}}
\email{thra@di.ku.dk}

\author{Tuukka Ruotsalo}
\orcid{0000-0002-2203-4928}
\affiliation{%
 \institution{University of Copenhagen}
 \city{Copenhagen}
 \country{Denmark}
}
\affiliation{%
 \institution{LUT University}
 \city{Lahti}
 \country{Finland}
}
\email{tr@di.ku.dk}

\author{Maria Maistro}
\orcid{0000-0002-7001-4817}
\affiliation{%
 \institution{University of Copenhagen}
 \city{Copenhagen}
 \country{Denmark}
}
\email{mm@di.ku.dk}

\author{Christina Lioma}
\orcid{0000-0003-2600-2701}
\affiliation{%
 \institution{University of Copenhagen}
 \city{Copenhagen}
 \country{Denmark}
}
\email{c.lioma@di.ku.dk}



\begin{abstract}
Fairness and relevance are two important aspects of recommender systems (RSs). Typically, they are evaluated either (i) separately by individual measures of fairness and relevance, or (ii) jointly using a single measure that accounts for fairness with respect to relevance. 
However, approach (i) often does not provide a reliable joint estimate of the goodness of the models, as it has two different best models: one for fairness and another for relevance. 
Approach (ii) is also problematic because these measures tend to be ad-hoc and do not relate well to traditional relevance measures, like NDCG. 
Motivated by this, we present a new approach for jointly evaluating fairness and relevance in RSs: Distance to Pareto Frontier (DPFR). 
Given some user-item interaction data, we compute their Pareto frontier for a pair of existing relevance and fairness measures, and then use the distance from the frontier as a measure of the jointly achievable fairness and relevance. Our approach is modular and intuitive as it can be computed with existing measures. 
Experiments with 4 RS models, 3 re-ranking strategies, and 6 datasets show that existing metrics have inconsistent associations with our Pareto-optimal solution, making DPFR a more robust and theoretically well-founded joint measure for assessing fairness and relevance.
Our code: \href{https://github.com/theresiavr/DPFR-recsys-evaluation}{github.com/theresiavr/DPFR-recsys-evaluation}.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002944.10011123.10011130</concept_id>
<concept_desc>General and reference~Evaluation</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003347.10003350</concept_id>
<concept_desc>Information systems~Recommender systems</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10002951.10003317.10003359</concept_id>
<concept_desc>Information systems~Evaluation of retrieval results</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}
\ccsdesc[500]{Information systems~Evaluation of retrieval results}
\ccsdesc[300]{General and reference~Evaluation}
\ccsdesc[300]{Information systems~Recommender systems}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{evaluation, relevance, fairness, 
pareto frontier, recommendation}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\begin{figure}[!h]
    \centering
    \begin{minipage}{.6\columnwidth}
        \centering
        \scalebox{.6}{
        \begin{tikzpicture}
            \tikzstyle{blue rectangle}=[fill={rgb,255: red,1; green,115; blue,178}, draw=black, shape=rectangle]
            \tikzstyle{green circle pareto}=[fill={rgb,255: red,2; green,158; blue,115}, draw=black, shape=rectangle]
            \tikzstyle{orange rectangle}=[fill={rgb,255: red,222; green,132; blue,5}, draw=black, shape=rectangle]
            \tikzstyle{pink rectangle}=[fill={rgb,255: red,204; green,120; blue,188}, draw=black, shape=rectangle]
            \tikzset{cross/.style={cross out, draw=black, minimum size=2*(#1-\pgflinewidth), inner sep=0pt, outer sep=0pt}, 
            cross/.default={5pt}}
            \begin{axis}
            [
                xmin = 0,
                ymin = 0,
                xmax = 1+0.1,
                ymax = 1+0.1,
                xlabel={\large Relevance (\textsc{Rel})}, 
                ylabel={\large Fairness (\textsc{Fair})}, 
                ticks=none,
                clip=false,
                axis lines=left,
                axis line style=thick,
                x label style={at={(axis description cs:0.5,0)},anchor=north},
                y label style={at={(axis description cs:0,.5)},anchor=south},
                legend style={at={(0.83,0.87)}, anchor=south,legend columns=1},
                legend style={font=\small, draw=none},
                legend cell align={left}
            ]

            \addlegendimage{}
            \addlegendimage{dashed,gray!50}
    
        
            \node (0) at (0.2, 1) {};
            \node (1) at (1, 0.2) {};
            \node [style=green circle pareto] (9) at (0.5, 0.5) {};
            \node [style=orange rectangle] (bestF) at (0.2, 0.9) {};
            \node [style=blue rectangle] (bestR) at (0.65, 0.2) {};
            \draw [in=90, out=0] (0.center) to (1.center);
            \addlegendentry{\large Pareto Frontier (PF)};
            \draw (0.7656854249492, 0.7656854249492) 
            node[cross,red, ultra thick,
            label={[align=center]0:{\normalsize(0.766, 0.766)}}, 
            label={[align=center, xshift=5pt]88:{\large\textbf{PF-midpoint}}\\[-2pt]{\footnotesize($\alpha=0.5$)}}] (10) {};
            \draw[dashed, color=gray!50] (9) -- (10);
            \addlegendentry{\large Euclidean distance};
            \draw[dashed, color=gray!50] (10) -- (bestF);
            \draw[dashed, color=gray!50] (10) -- (bestR);
            \node[label={\large \textbf{Model C}}, label={0:{\large(0.5, 0.5)}}]  at (9) {};
            \node[label={\large \textbf{Model A}}, label={0:{\large(0.2, 0.9)}}]  at (bestF) {};
            \node[label={\large \textbf{Model B}}, label={0:{\large(0.65, 0.2)}}]  at (bestR) {};
            \end{axis}%
        \end{tikzpicture}
        }
    \end{minipage}%
    \begin{minipage}[t][][b]{0.4\columnwidth}

     \scalebox{.8}{
        \begin{tabular}{ccc}
            \toprule
            Model & $\downarrow$ Dist.~to PF & $\uparrow$ Avg\\
            \midrule
            A & 0.582 & \textbf{0.55} \\
            B & 0.578 & 0.425 \\
            C & \textbf{0.376} & 0.5 \\
            \bottomrule
        \end{tabular}}
    \end{minipage}
    
    \caption{
    $(x, y)$ denotes the pair of relevance and fairness score. 
    Example: 
    Model A is best for fairness, 
    Model B is best for relevance, and Model C is the closest 
    to the Pareto Frontier (PF) midpoint, when relevance and fairness are equally weighted ($\alpha=0.5$). 
    Averaging relevance and fairness (Avg) leads to falsely concluding that Model A is best for both aspects. Note that distance to PF also beats other existing measures of fairness and relevance (see $\S$\ref{ss:corr}).
    }
    \label{fig:pareto_teaser}

\end{figure}

\section{Introduction
}
\label{s:intro}

Relevance and fairness are important aspects of recommender systems (RSs). Relevance is typically evaluated using common ranking measures (e.g., NDCG), while various fairness measures for RSs exist \cite{Wang2023ASystems,Amigo2023ASystems}. 
Some fairness measures integrate relevance, so that they evaluate fairness w.r.t.~relevance. 
The problem with these joint measures is that they tend to be ad-hoc, unstable, and they do not account very well for both aspects simultaneously \cite{Rampisela2024CanRelevance}. 
Another way of evaluating relevance and fairness is to use a different measure for each aspect. However, this does not always provide a reliable joint estimate of the goodness of the models, as it may have two different best models: one for fairness and another for relevance.   
This can be avoided by aggregating the scores of the two measures into a single score, or by aggregating the resulting model rankings into one using rank fusion. These approaches are also problematic because: 
(i) the scores of the two measures may 
have different distributions and different scales, making them hard to combine; 
(ii) the two measures may not even be computed with the same input, making their combination hard to interpret (relevance scores are computed for individual users and then averaged, while fairness measures for individual items are typically based on individual item recommendation frequency); and 
(iii) the resulting scores are less understandable as it is unknown how close the models are to an ideal balance of fairness and relevance, e.g., an acceptable trade-off between fairness and relevance scores. 

To address the above limitations, we contribute an approach that builds on the set of all Pareto-optimal solutions \cite{Censor1977ParetoProblems}. Our approach addresses issue (i) and (ii) above by avoiding direct combination of measures. We directly address (iii) by computing the distance of the model scores to a desired fairness-relevance balance. 
Our approach uses Pareto-optimality, a popular concept in multi-objective optimization problems across domains, including RSs \cite{Ribeiro2015MultiobjectiveSystems}. 
A recommendation is Pareto-optimal if there are no other possible recommendations with the same \textsc{Rel} score that achieve better fairness.\footnote{The opposite is also true, but in RS scenario the \textsc{Rel} score is usually the primary objective, 
not the \textsc{Fair} score.} 
In other words, given Pareto-optimal solutions, we cannot get other recommendations that empirically perform better, 
unless relevance is sacrificed. 
In our approach, we combine existing \textsc{Fair} measures and \textsc{Rel} measures as follows. We build a Pareto Frontier (PF) that first maximises relevance, finds the best fairness achievable under the relevance constraint, and then jointly quantifies fairness and relevance as the distance from an optimal solution, see Fig.~\ref{fig:pareto_teaser}. 

Our approach, \emph{Distance to PF of Fairness and Relevance} (DPFR) has several strengths. First, DPFR is \emph{modular}; it can be used with well-known existing measures of relevance and fairness. DPFR is also \emph{tractable} as one can control the weight ($\alpha$) of fairness w.r.t.~relevance. As the resulting score is the distance to the scores of a traditional relevance measure and a well-known fairness measure, DPFR is also \emph{intuitive} in its interpretation. Most importantly, DPFR is a principled way of jointly evaluating relevance and fairness based on an empirical best solution that uses Pareto-optimality. Experiments with different RS models, re-ranking approaches and datasets show that there exists a noticeable gap between using current measures of relevance and fairness and our Pareto-optimal joint evaluation of relevance and fairness. This gap is bigger in larger datasets and when using rank-based relevance measures (i.e., MAP, NDCG), as opposed to set-based relevance measures (i.e., 
Precision, Recall). 

In this work, we focus on \textbf{individual item fairness}. This type of fairness is commonly defined as all items having equal exposure, where exposure typically refers to the frequency of item appearance in the recommendation list across all users~\cite{Patro2020FairRec:Platforms, Mansoury2020FairMatch:Systems, Rampisela2024EvaluationStudy}. Individual item fairness is important in ensuring that each item/product in the system has a chance to be recommended to any user \cite{Lazovich2022MeasuringMetrics}.


\section{Related work}\label{s:prev_work}

Evaluating fairness and relevance together is a type of multi-aspect evaluation. 
However, none of the existing multi-aspect evaluation methods \cite{Maistro2021PrincipledRankings, Lioma2017EvaluationLists,Palotti2018MM:Engines} can be used in this case as 
these methods require separate labels that are unavailable in RS scenarios. 
Specifically, it is not possible to label an item as `fair', because item fairness depends on other recommended items. The same item can be a fair recommendation in one ranking, but unfair in another. In RSs, fairness is typically defined as treating users or items without discrimination \cite{Biega2018EquityRankings}. This is often quantified as the opportunity for 
having equal relevance (for users) or exposure (for items) 
\cite{Biega2018EquityRankings, Wang2022ProvidingSystems}, computed either individually or for
groups of items/users \cite{Raj2022MeasuringResults, Zehlike2022FairnessSystemsc}. 

The problem of jointly evaluating RS relevance and fairness is further aggravated by the fact that improved fairness is often achieved at the expense of relevance to users \cite{Mehrotra2018TowardsSystems}. 
We posit that this trade-off makes multi-objective optimization a suitable solution. Pareto optimality is a well-known objective for such optimization, 
and it has been previously used in RS but only to recommend items to users \cite{Ribeiro2015MultiobjectiveSystems,  Zheng2022AOptimization, Ge2022TowardLearning, Xu2023P-MMF:System}. 
Because the true PF is often unknown due to the problem complexity \cite{Laszczyk2019SurveyMeasures,Audet2020PerformanceOptimization}, prior work has used the model's training loss w.r.t.~two different aspects \cite{Lin2019ARecommendation} or scores from different models \cite{Nia2022RethinkingNetworks,Paparella2023Post-hocRecommendation} to generate the PF. 
Our work differs from this in terms of both the purpose of using Pareto-optimal solutions, and the nature of the PF. Specifically, we exploit Pareto-optimality through PF as a robust \textit{evaluation} method, instead of as a recommendation method. 
In addition, our generated PF is based on the ground truth (i.e., the test set), a common RS evaluation approach, instead of the recommender models' empirical performance, which may not be optimal. Thus, our PF is also model-agnostic, as opposed to the PF in \cite{Xu2023P-MMF:System}. 
Our approach differs also from FAIR~\cite{Gao2022FAIR:Evaluation} since the PF considers the empirically achievable optimal solution based on the dataset, while FAIR compares against the desired fairness distribution which might not be achievable. Lastly, \cite{Paparella2023Post-hocRecommendation} selects the optimal solution based on its distance to the utopia point (the theoretical ideal scores), whereas the utopia point may not be realistic due to dataset or measure characteristics~\cite{Rampisela2024EvaluationStudy,Moffat2013SevenMetrics}. Since our PF is generated based on test data, any of its solutions is empirically achievable. 


\section{Distance to Pareto Frontier(DPFR)}
\label{s:our_method}

We present definitions 
($\S$\ref{ss:motivation}),  
and then explain DPFR in different steps: 
given a \textsc{Fair} and a \textsc{Rel} measure, how to 
generate PF based on the ground truth data in the test set ($\S$\ref{ss:generation}); how to choose a reference point in the PF based on $\alpha$ (e.g., the midpoint for $\alpha=0.5$) ($\S$\ref{ss:pareto-for-eval}); and how to compute the distance of the \textsc{Fair} and \textsc{Rel} scores to the reference point with a distance measure $d$ ($\S$\ref{ss:pareto-for-eval}). 
Additionally, we present a computationally efficient adaptation of DPFR ($\S$\ref{ss:compute-eff}). 

\subsection{Definitions}
\label{ss:motivation}

We adapt the Pareto-optimality definition \cite{vanVeldhuizen1999MultiobjectiveInnovations}: the multi-objective problem is finding the optimum \textsc{Fair} score $s_f$, and \textsc{Rel} score, $s_r$ from a list of possible recommendations across all users. We define the tuple $s = (s_r, s_f) \in S$, where $S$ is the Cartesian product of all possible \textsc{Rel} and \textsc{Fair} scores. The relation $\geq_A$ ($>_A$) means `better or equal to' (`better to') according to an aspect $A \in \{\textsc{Rel}, \textsc{Fair}\}$. 

\begin{definition}[Pareto Dominance]
A tuple $s = (s_r, s_f)$ dominates $s'=(s_r', s_f')$ iff $s$ is partially better than $s'$, i.e., $s_r \geq_\textsc{Rel} s'_r$ and $s_f \geq_\textsc{Fair} s'_f$, in addition to $s_r >_\textsc{Rel} s'_r$ or $s_f >_\textsc{Fair} s'_f$. 
\end{definition}


\begin{definition}[Pareto Optimality]
A solution (recommendation list) that has \textsc{Rel} and \textsc{Fair} scores of $x = (x_r, x_f) \in S$ is Pareto-optimal iff there is no other solution with $x'=(x_r', x_f') \in S$ that dominates $x$.
\end{definition}

\begin{definition}[Pareto Frontier]
The set of all Pareto-optimal tuples.
\end{definition}

\subsection{Pareto Frontier generation}
\label{ss:generation}
Given user-item preference data (e.g., test set), the aim is to explore the empirical, maximum feasible fairness towards individual items, such that the recommendation satisfies Pareto-optimality w.r.t.~fairness scores across all items and an average relevance score across users, e.g., MAP@$10=0.9$.\footnote{This is how \textsc{Fair} and \textsc{Rel} measures are usually computed.} This is done to measure how far a model performance is, from these Pareto-optimal solutions.
Enumerating all possible recommendations for users and items to find the complete set of Pareto-optimal solutions is computationally infeasible, and there is no analytical solution either. Instead, we contribute an algorithm that iteratively builds upon a maximally relevant initial recommendation list. Our algorithm iteratively finds Pareto-optimal recommendations by prioritising relevance over fairness, as recommendations are usually optimised for relevance (with or without fairness). This prioritisation is known as lexicographic optimization \cite{Ryu2018Multi-objectiveWeight}. 
We call our algorithm {\sc Oracle2Fair} (full technical description in App.~\ref{app:algo}). 
Our algorithm generates the PF of fairness and relevance in two steps: 
 \textbf{(1) initialisation} of the recommendations with an \textit{Oracle} (App.\ref{app:algo}, Algorithm~\ref{alg:oracle}).  The \textit{Oracle} generates a recommendation with the highest empirical score for relevance, based on user interactions that are part of the test set. This step is followed by \textbf{(2) replacements} to make the recommendations as \textit{Fair} as possible; at the end of this algorithm, the \textsc{Fair} scores should reach the empirically fairest score while maintaining as much relevance as possible. 
Throughout the PF generation, items in a user's train/val split are not recommended to the same user. Henceforth, \emph{relevant items} refers to the items in a user's test split.

\noindent \textbf{(1) Initialisation}. 
The Oracle recommends at most $k=10$ relevant items, from the $n$ items in the dataset, to each of the $m$ users in the test split, one user at a time. 
The recommendation begins with users having exactly $k$ items in the test split; only these items can be recommended to those users to gain the maximum relevance. Recommendations to other users are made maximally relevant and fair as follows: if a user has $>k$ relevant items, we pick $k$ items with the least exposure among them. Item exposure is computed based on what has been recommended to other users who already have exactly $k$ items. Note that this process is not trivial (see App.~\ref{app:algo}, 
Algo~\ref{alg:oracle}, ll.~\ref{ln:startgtk}--\ref{ln:endgtk}). If a user has $<k$ relevant items, we recommend those items at the top (to maximise top-weighted \textsc{Rel} measures) and fill the rest of their recommendation slots with the least exposed items in the dataset (Algo~\ref{alg:oracle}, ll.~\ref{ln:startltk}--\ref{ln:endltk}). This least-exposure prioritisation strategy ensures that the solutions are Pareto-optimal. 

\noindent \textbf{(2) Replacements}. 
The algorithm iteratively replaces the recommended items to achieve maximum fairness, such that each replacement results in a fairer recommendation than the previous. 
We compute the \textsc{Fair} and \textsc{Rel} measures after each replacement as follows. The most popular item, which is recommended most often, is replaced with one of these item types, 
in decreasing order of priority: an unexposed item, then the least popular item in the recommendation; this increases fairness from the previous recommendations. We do this one item and one user at a time, starting with the users that have the most popular item at the bottom of their recommendation list, to ensure that the decrease in relevance is minimum as the replacement item is mostly not relevant to that user. Nonetheless, the \textsc{Oracle2Fair} prioritises replacing the recommendations of users for whom the replacement item is relevant (if any). As fairness increases and relevance decreases/stays the same from the previous recommendation, the new recommendation is also Pareto-optimal. 
We continue the replacement until the maximum times any item is recommended is $\left\lceil km/n \right\rceil$, i.e., the upper bound of how many times an item can be recommended, if all items in the dataset must appear in the recommendation as uniformly as possible. We explicitly used $\left\lceil km/n \right\rceil$ as a stopping condition for \textsc{Oracle2Fair}. 
To ensure maximum \textsc{Rel} scores (especially in top-weighted measures), each time a replacement takes place, we rerank the recommendations based on descending relevance.

The resulting (\textsc{Rel, Fair}) scores reflecting Pareto-optimal recommendations from this process make up the PF. If there are duplicates in the \textsc{Rel} value, we keep the best \textsc{Fair} score for a single value of \textsc{Rel}. While it cannot be reasonably verified that the resulting PF matches the theoretical PF, this is a close way to build the full PF, as opposed to building the PF from trained models scores ($\S$\ref{s:prev_work}). 




\subsection{Distance computation} 
\label{ss:pareto-for-eval}

For each pair of \textsc{Fair} and \textsc{Rel} measures, we find a reference point using a tunable parameter $\alpha \in [0,1]$; $\alpha=0$ means only relevance is accounted for, and $\alpha=1$ means only fairness is accounted for. Next, we explain how to compute the reference point. We first use the following equation to find the length of a subset $T$ of the PF: 
$
    lenPF(T) = \sum_{t=1}^{|T|-1} d_E(x^t, x^{t+1})
$.
 Given that $P$ is the set of all Pareto-optimal solutions, $x^t = (x_r^{t}, x_f^{t})$ is the pair of Pareto-optimal solutions $(x_r,x_f)$ with the $t$-th highest $x_r$ in $P$, and $d_E$ is the Euclidean distance. The overall PF length is $lenPF(P)$ or simply $lenPF$. 
 
 The reference point is $s_{\alpha} = x^{t'}$, where $t'$ is computed as follows:
 $
    t' = \argminA_{j \in \left[1,\dots,|P|-1\right]}\left|lenPF(T^j)-\ \alpha \cdot lenPF \right|
$. $T^{j}$ is a subset of $P$ containing the $j$ highest $x_r$ scores. So, the reference point is a point in the PF whose cumulative traversal distance is closest to the $\alpha$-weighted PF distance travelled from the first point in the PF. The reference point $s_\alpha$ is how far the PF is traversed, from the pair with the best \textsc{Rel} score to the one with the best \textsc{Fair} score, multiplied by $\alpha$. As the PFs 
may have different density of points along the frontiers, the reference point is not computed based on a percentile (e.g., median) to avoid bias towards the denser part. 
Next, the distance between each model's ($x_r, x_f$) scores and the reference point $s_\alpha$ is computed with a distance measure $d$ that accommodates 2d-vectors. The model with the closest distance is the best model in terms of both relevance and fairness, given the weight $\alpha$. 
We call this \emph{Distance to Pareto frontier of Fairness and Relevance} (DPFR). 

\subsection{Efficient computation of Pareto Frontier}
\label{ss:compute-eff}
  

Generating the PF as in $\S$\ref{ss:generation} is costly. An efficient alternative is to compute a subset of the PF. We pick a fixed amount of Pareto-optimal solutions to compute, $p$ (e.g., 10). However, to reliably approximate the PF, these solutions should be spread according to the PF distribution, as opposed to e.g., only computing the first $p$ points of the PF. The spread of the points is important, as the reference point in DPFR is computed based on the overall estimated PF. 
In the estimated PF, the first point corresponds to the measure scores of the initial recommendation given by the Oracle, and the rest are spread evenly throughout the PF generation. 
To select at which point of the \textsc{Oracle2Fair} algorithm the measures should be computed, we first estimate the total number of replacements needed by examining the distribution of recommended items frequency. This is done by getting the individual frequency count of all items in the recommendation, and subtracting the ideal upper bound of item count $\lceil km/n \rceil$ ($\S$\ref{ss:generation}) from each count. 
The number of expected replacements is the sum of the difference between the item frequency count and the ideal upper bound of item count in $\S$\ref{ss:generation}. Items with recommendation frequency counts less than the upper bound are excluded. With the estimated total number of replacement $numRep$, we compute the measures every $numRep\ \text{div} (p-1)$ replacements done by \textsc{Oracle2Fair}, such that the measures are computed a total of $p-1$ times during the replacement process + 1 time before the replacement starts. These $p$ points are spread evenly in terms of distance in the PF, which is important as DPFR is a distance-based measurement. 

\section{Experimental setup}
\label{s:experiments}

We study how our joint evaluation approach, DPFR, compares to existing single- and multi-aspect evaluation measures of relevance and fairness. 
Next, we present our experimental setup. 
The experiments are run in a cluster of CPUs and GPUs (e.g., Intel(R) Xeon(R) Silver 4214R CPU @ 2.40GHz, AMD EPYC 7413 and 7443, Titan X/Xp/V, Titan RTX, Quadro RTX 6000, A40, A100, and H100).

\noindent \textbf{Datasets.} We use six real-world datasets of various sizes and domains: 
e-commerce (A\-ma\-zon Luxury Beauty, i.e., Amazon-lb \cite{Ni2019JustifyingAspects}),
movies (ML-10M and ML-20M \cite{Harper2015TheContext}), 
music (Lastfm \cite{Cantador20112nd2011},  
videos (QK-video \cite{Yuan2022Tenrec:Systems}), 
and jokes (Jester \cite{Goldberg2001Eigentaste:Algorithm}). 
The datasets are as provided by \cite{Zhao2021RecBole:Algorithms}, except for QK-video, which we obtain from \cite{Yuan2022Tenrec:Systems}. 
Statistics of the datasets are in Tab.~\ref{tab:stats} and extended statistics are in App.~\ref{app:stat}.

\begin{table}[tb]
\caption{Statistics of the preprocessed datasets.}

\resizebox{0.98\columnwidth}{!}{
\begin{tabular}{lrrrr}
\toprule
\textbf{dataset} & \multicolumn{1}{l}{\textbf{\#users ($m$)}} & \multicolumn{1}{l}{\textbf{\#items ($n$)}} & \multicolumn{1}{l}{\textbf{\#interactions}} & \multicolumn{1}{l}{\textbf{sparsity (\%)}} \\ 
\midrule                                                                   
Lastfm \cite{Cantador20112nd2011}& 1,859 & 2,823 & 71,355 & 98.64\% \\
Amazon-lb \cite{Ni2019JustifyingAspects} & 1,054 & 791 & 12,397 & 98.51\% \\
QK-video \cite{Yuan2022Tenrec:Systems} & 4,656 & 6,423 & 51,777 & 99.83\% \\
Jester \cite{Goldberg2001Eigentaste:Algorithm} & 63,724 & 100 & 2,150,060 & 66.26\% \\ 
ML-10M \cite{Harper2015TheContext} & 49,378 & 9,821 & 5,362,685 & 98.89\% \\
ML-20M \cite{Harper2015TheContext} & 89,917 & 16,404 & 10,588,141 & 99.28\% \\
\bottomrule
\end{tabular}
}
\label{tab:stats}
\end{table}


\noindent \textbf{Preprocessing.} We remove duplicate interactions (we keep the most recent). We keep only users and items with $\geq5$ interactions. 
We convert ratings equal/above the following threshold to $1$ and discard the rest: for Amazon-lb and ML-*, the threshold is 3, as their ratings range between $[1,5]$ and $[0.5, 5]$ resp.; the threshold for Jester is $0$, as the ratings range in $[-10, 10]$. Lastfm and QK-video have no ratings. QK-video has several interaction types; we use the `sharing' interactions. 
For Jester, we remove users with $>80$ interactions, to provide a large enough number of item candidates for recommendation during testing.\footnote{\label{fn:jester}Some users in Jester have interacted with almost all 100 items. If a user has 80 items in the train/val set, there would only be 20 candidate items to recommend during test, which makes it easier to achieve higher relevance. 
} 

\noindent \textbf{Data splits.} To obtain the train/val/test sets for Amazon-lb and ML-*, we use global temporal splits \cite{Meng2020ExploringModels} with a ratio of 
6:2:2 on the preprocessed datasets. Global random splits with the same ratio are used for the other datasets that have no timestamps. 
From all splits, we remove users with $<5$ interactions in the train set. 

\noindent\textbf{Measures}.
We measure relevance (\textsc{Rel}) with Hit Rate (HR), MRR, Precision (P), Recall (R), MAP, and NDCG. We measure individual item fairness (\textsc{Fair}), as per  \cite{Rampisela2024EvaluationStudy}, with Jain Index (Jain) \cite{jain1984quantitative, Zhu2020FARM:APPs}, Qualification Fairness (QF) \cite{Zhu2020FARM:APPs}, Gini Index (Gini) \cite{Gini1912VariabilitaMutabilita, Mansoury2020FairMatch:Systems}, Fraction of Satisfied Items (FSat) \cite{Patro2020FairRec:Platforms}, and Entropy (Ent) \cite{Shannon1948ACommunication, Patro2020FairRec:Platforms}.\footnote{
These are set-based measures, but we do not expect the conclusions to differ for rank-based measures (App.~\ref{app:discussion}).
} 
We also use joint measures (\textsc{Fair+Rel}) as per \cite{Rampisela2024CanRelevance}: 
Item Better-Off (IBO) \cite{Saito2022FairRanking},\footnote{
The measure Item Worse-Off is not used as its formulation is highly similar to IBO.}
Mean Max Envy (MME) \cite{Saito2022FairRanking},
Inequity of Amortized Attention (IAA) \cite{Biega2018EquityRankings, Borges2019EnhancingAutoencoders}, Individual-user-to-individual-item fairness (II-F) \cite{Diaz2020EvaluatingExposure, Wu2022JointRecommendation}, and 
All-users-to-individual-item fairness (AI-F) \cite{Diaz2020EvaluatingExposure}. 
We denote by $\uparrow$/$\downarrow$ measures where higher/lower is better.
DPFR is computed with Euclidean distance and $\alpha=0.5$ (PF midpoint) for all datasets, so the midpoint is based on the empirically achievable scores, per dataset and measure pairs. 
For all runs, we use $k=10$. 

\noindent\textbf{Recommenders}. We use 4 
common collaborative filtering-based recommenders: ItemKNN \cite{Deshpande2004Item-basedAlgorithms},
BPR \cite{Rendle2009BPR:Feedback},
MultiVAE \cite{Liang2018VariationalFiltering}, and 
NCL \cite{Lin2022ImprovingLearning}, with RecBole \cite{Zhao2021RecBole:Algorithms} and tune their hyperparameters. We train for 300 epochs with early stopping, and keep the configuration with the best NDCG@10 during validation. Each user's train/val items are excluded from their recommendations during testing. 

\noindent\textbf{Fair Re-rankers.} 
To have fairer recommendations, we reorder the top $k'$ items that are pre-optimised for relevance. Ideally $k'>k$ to allow exposing items that are not in the top $k$. As there are few relevant items per user in RS data,\footnote{The median number of relevant items per user across all datasets is 2--53, see App.~\ref{app:stat}.}
$k'$ should not be too big (e.g., 100). So, we re-rank the top $k'=25$ items per dataset and model using three methods: GS, CM, and BC (explained below). 
We re-rank separately per user for CM and BC, or altogether for GS, for all $k'm$ recommended items, where $m$ is the number of users. 
Other fair ranking methods exist but cannot be used as they apply to group or two-sided fairness only (e.g., \cite{Zehlike2017FAIR:Algorithm, Zehlike2020ReducingApproach, Patro2020FairRec:Platforms}), or to stochastic rankings only (e.g., \cite{Wu2022JointRecommendation, Oosterhuis2021ComputationallyFairness}), or do not scale to larger datasets (e.g., \cite{Biega2018EquityRankings, Saito2022FairRanking}).
    
   \noindent \textit{1. Greedy Substitution (GS) \cite{Wang2022ProvidingSystems}} is a re-ranker for individual item fairness. We modify the GS algorithm, to replace the most popular items with the least popular ones, both considering how many times an item is at the top $k'$ recommendations for all users (App.~\ref{app:gs}). As such, items can be swapped across users. To determine which items are most popular (i.e., to be replaced) and least popular (replacement items), the parameter $\beta=0.05$ is used. We pair these two item types, and for each pair, we calculate the loss of (predicted) relevance if the items are swapped. We then replace up to 25\% of the initial recommendations, starting from item pairs with the least loss. 
    
    \noindent \textit{2. COMBMNZ (CM) \cite{Lee1997AnalysesCombination}} is a common rank fusion method. Two rankings are fused for each user: one based on the (min-max) normalised predicted relevance score and another based on the coverage of each top $k'$ item (to approximate fairness). We calculate item coverage only based on their appearance in the top $k$ across all users and min-max normalise the score across all users. 
    As favouring items with higher coverage would boost unfairness, we generate the ranking using $1$ minus the normalised coverage. CM uses a multiplier based on the item appearance count in the two rankings above; this count is also only based on the top $k$. 
    The resulting ranking is a fused ranking of fairness and relevance.
    
    \noindent \textit{3. Borda Count (BC)} is a common rank fusion method. For each user, we combine the original recommendation list and the rankings based on increasing item coverage, as in CM. Unlike CM, BC uses points. Higher points are given to items placed at the top. The result is a fused ranking of fairness and relevance. 

\section{Experimental results}
\label{ss:result-pareto}
We now present the evaluation scores of 16 runs (4 recommenders $\times$ 3 re-rankers, including no reranking) ($\S$\ref{sss:results-diversity}). The relevance and fairness scores of these runs are the input to our DPFR approach. 
Not all combinations of evaluation measures are suited for PF. We explain this in $\S$\ref{sss:no-fit}. We present the generated PF ($\S$\ref{sss:result-pf}) and compare existing measures to DPFR ($\S$\ref{ss:corr}). We compare the results of efficient DPFR to other joint evaluation approaches ($\S$\ref{ss:compare-eff}).

\subsection{Groundwork runs} \label{sss:results-diversity}

The scores of \textsc{Rel}, \textsc{Fair}, and \textsc{Fair+Rel} measures for our 16 runs are shown in the appendix (Tab.~\ref{tab:base-rerank-all-1}--\ref{tab:base-rerank-all-2}). 
Two main findings emerge from Tab.~\ref{tab:base-rerank-all-1}--\ref{tab:base-rerank-all-2}. First, for all six datasets, \textbf{none of the best models according to \textsc{Rel} are also the best according to \textsc{Fair} measures}. This is similar to our toy example (Fig.~\ref{fig:pareto_teaser}), where one model ranks highest for fairness and another for relevance. 
Second, \textbf{the five \textsc{Fair+Rel} measures have no unanimous agreement on the best model}. 
IBO has a different best model from the others in 4/6 times, but sometimes agrees with one or more \textsc{Fair} measures. 
MME and AI-F agree on the best model 5/6 times, and sometimes agree on the best model with \textsc{Fair} measures.
The best model according to IAA and II-F is always the same, and 4/6 times the same as the best model based on the \textsc{Rel} measures. 
The overall picture is inconclusive, with some \textsc{Fair+Rel} measures aligning more with \textsc{Fair} measures, and others aligning more with \textsc{Rel} measures. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{img/pairplot_lastfm.png}
    \includegraphics[width=\columnwidth]{img/pairplot_qkvideo.png}
    \caption{Pareto Frontier of fairness and relevance (in blue) and recommender scores for Lastfm and QK-video on exponential-like scales. 
    \textsc{Rel, Fair}, Avg (mean of \textsc{Rel, Fair}), and DPFR are the best model per evaluation approach.}
    \label{fig:pareto-pair-plot}
\end{figure}

\subsection{Measure compatibility with DPFR}
\label{sss:no-fit}

 Which pairs of \textsc{Rel} and \textsc{Fair} measures are suitable to generate the PF? We answer this based on the PF slope. The slope is calculated using the two endpoints of the PF, i.e., the start and end of the  \textsc{Oracle2Fair} algorithm. A slope of zero means the \textsc{Rel} scores of the PF vary, but the \textsc{Fair} scores do not. As we compute the PF for multiple measures simultaneously, we expect a zero gradient for cases where the initial recommendation according to a \textsc{Fair} measure is already the fairest, even if other \textsc{Fair} scores are not. An undefined gradient value occurs when the initial recommendation is already the fairest and the most relevant according to a pair of \textsc{Fair} and \textsc{Rel} measures. Thus, we posit that a PF with a gradient value other than zero or undefined makes the corresponding pair of measures fit for PF generation (it allows for trade-offs in both aspects). 
The \textsc{Rel}-\textsc{Fair} measure pairs that are fit for DPFR based on their gradient are: \{P, MAP, R, NDCG\} $\times$ \{Jain, Ent, Gini\} (App.~\ref{app:actual-scores}, Tab.~\ref{tab:app-gradient}). Only results from these pairs are shown henceforth. Next, we explain what causes an undefined or zero gradient. 

\noindent \textbf{Causes of zero/undefined gradient.}
Generating the PF requires a ranking of items. Any score that is based on a single relevant item, e.g., HR and MRR, is by design not suitable. 
Out of the \textsc{Fair} measures, QF and FSat sometimes behave inconsistently depending on the dataset properties, as follows. 
A dataset with relatively few relevant items can already be made maximally fair at the start of the PF generation, as QF quantifies fairness with ignorance to frequency of exposure; the score does not change as long as the same set of items appears in the top $k$ recommendations of all users, no matter how many times each. 
When all items in the dataset already occur in the initial recommendations of our Oracle, nothing can be done to improve QF. 
For FSat, in few cases, the score is already maximum at the start of the PF generation. A maximum FSat score is achieved when all items in the dataset have at least the maximum possible exposure, if the available recommendation slots are shared equally across all items.\footnote{$\left\lfloor km/n\right\rfloor$ times (the total number of recommendation slots across users divided by the number of items).} 
In principle, QF and FSat can still be used for DPFR when the initial recommendation by Oracle is not the maximum yet. Otherwise, the interpretation would be less meaningful in joint evaluation, as there is no trade-off between different aspects.


\subsection{The generated PF} \label{sss:result-pf}

Fig.~\ref{fig:pareto-pair-plot} shows the PF plots of the pairs of \textsc{Fair} and \textsc{Rel} measures that are suited for DPFR, only for Lastfm and QK-video, which  are representative of the overall trends in all our datasets (see Fig.~\ref{fig:app-pairplot} in the Appendix). The scores plotted are those computed in $\S$\ref{ss:generation}. The corresponding scores of our recommendation models are in App.~\ref{app:actual-scores}. 
We see that, as the recommendations are made fairer, the generated PF for all datasets is a series of monotonic scores of \textsc{Fair}, specifically monotonic increasing \textsc{Fair} scores (except $\downarrow$Gini), and the remaining measures are monotonic decreasing. 
The monotonic property  theoretically and empirically holds for the \textsc{Fair} measures, as we replace an item with the most exposure by another item with the least exposure, thereby making the recommendation fairer. 
Note that some users do not have exactly $k$ items in the test set, so the perfect relevance score cannot be reached for Precision@$k$ and Recall@$k$ \cite{Moffat2013SevenMetrics}. NDCG and MAP are implemented with normalisation\footnote{Only the first $\min{(|R^*_u|, k)}$ items in a user $u$'s recommendations are considered, where $R^*_u$ is the set of relevant items for user $u$.} so that they can still achieve a score of 1 in this situation. 

The datasets which were randomly split as they have no timestamps (QK-video, Jester) have relatively short, compact PF. This happens because the random split results in a uniform distribution of items in each split, which means that items in the test split are quite diverse (64--100\% of all unique items in the dataset). Considering that the \textsc{Oracle2Fair} algorithm starts by recommending items in the test split and stops when the recommendation reaches the fairest, there is not much room for change in \textsc{Fair} scores, as the initial recommendation is already rather fair. Additionally, there are not many relevant items per user in these datasets (i.e., the median for both datasets is 6 or less); random non-relevant items were chosen to make up for the remaining recommendations.\footnote{The randomly-split Lastfm does not have a short PF because on average it has more relevant items per user compared to QK-video and Jester (see App.~\ref{app:stat}).} 
Thus, the PF generation decreases relevance only marginally in 2/6 cases. 
Correspondingly, we find that in QK-video and Jester, there exist Pareto-optimal recommendations, that are close to maximally fair and maximally relevant, with the exception of P@10. These can be seen in the measure pairs of \{MAP, R, NDCG\} $\times$ \{Jain, Ent, Gini\}, where the PF is close to the coordinates of $(1,1)$, or $(1,0)$ for 
the measure pairs with Gini. Thus, in theory, a fair recommendation does not necessarily have to sacrifice relevance. 


\subsection{Agreement between measures} \label{ss:corr}

We study the agreement between DPFR and other evaluation approaches in ranking our 16 runs from best to worst. Low agreement means that the other approaches have few ties to the Pareto-optimal solutions that DPFR uses, and vice versa. 
We compare DPFR to (a) existing \textsc{Rel} and \textsc{Fair} measures, (b) existing joint \textsc{Fair+Rel} measures ($\S$\ref{s:experiments}), and also (c) the average (arithmetic mean) of \textsc{Fair} and \textsc{Rel} scores from the selected measure pairs that are used to generate the PFs. 
To compute the average for a measure where lower values are better (i.e., Gini), we compute $1-$the Gini score instead.

\subsubsection{Comparison of existing measures to DPFR}
We find that for all datasets and all measure pairs, \textbf{the best model as per  DPFR is always different from the best model as per \textsc{Rel} measures}. Moreover, \textbf{half the time, the best model as per DPFR is different from the best model as per a \textsc{Fair} measure}. 
Existing \textsc{Fair+Rel} measures tend to have the same best model as either \textsc{Fair} or \textsc{Rel} measures (73.3\% of the time), instead of having a more balanced evaluation of both aspects. These findings are expected as existing joint evaluation measures use relevance in their formulation differently than the \textsc{Rel} measures. 
Overall, the best model found with DPFR is less skewed towards relevance or fairness. 
 

\subsubsection{Correlation of measures}
For each dataset, we compute the Kendall's\footnote{Ties are handled, unlike in Spearman's $\rho$.} $\tau$ \cite{KENDALL1945THEPROBLEMS} correlations between the ranking given by DPFR and by the joint evaluation baselines (see Fig.~\ref{fig:corrplot}). 
Rankings are considered equivalent if $\tau \geq 0.9$ \cite{Maistro2021PrincipledRankings, Voorhees2001EvaluationDocuments}. 
We see similar agreement trends in datasets where recommenders have higher \textsc{Rel} scores (Lastfm and Jester) or lower (Amazon-lb and QK-video). 

\begin{figure}[htbp]
    \centering   \includegraphics[width=0.8\columnwidth, trim = 2.5mm 2.5mm 2mm 0mm, clip,]{img/corr_heatmap_all.pdf}
    \caption{ 
    Kendall's $\tau$ correlation heatmap between the rank ordering of existing joint evaluation measures (including the average of \textsc{Fair} and \textsc{Rel} scores, avg), and DPFR.}
    \label{fig:corrplot}
\end{figure}
 
Overall, most times DPFR orders models differently ($\tau<0.9$) than all \textsc{Fair+Rel} measures except AI-F. We see similar trends between IAA and II-F, and between MME and AI-F. IBO can have similar trends as MME (except for Amazon-lb and QK-video). For all datasets, IAA and II-F have overall either weak or negative $\tau$ with DPFR (e.g., $[-0.2, 0.25]$ for ML-10M and $[-0.62, -0.35]$ for QK-video). A notable exception is DPFR with \{MAP, R, NDCG\} $\times$ {Ent} for Lastfm and Jester, where we see moderate correlations, $\tau \in [0.42,0.68]$. 


Ent differs from this trend because DPFR with \{MAP, R, NDCG\} $\times$ \{Jain, Gini\} has PF gradients of greater magnitude. This only affects Lastfm and Jester (they have higher \textsc{Rel} scores than the other datasets). DPFR with P has different patterns from other \textsc{Rel} measures: the raw DPFR scores of pairs involving P are lower on average, as the scores from Oracle do not start from $1$, but much less, and therefore closer to the models' scores (Fig.~\ref{fig:pareto-pair-plot}). 
Meanwhile, IBO has varying $\tau$ across datasets: a huge range of $\tau$, i.e., $[0.00,0.9]$ for Lastfm, weak correlations $[0.01, 0.13]$ for Amazon-lb, and moderate to strong correlations $[0.67, 0.98]$ for ML-10M. These variations might be because IBO is based on the number of items satisfying a certain criterion, rather than an average of scores across users and/or items, i.e., how other \textsc{Fair+Rel} measures are defined. 

Among the joint measures, AI-F correlates the strongest with DPFR, as both AI-F and DPFR, indirectly or directly, consider 
the recommendation frequency of each item and compare it with that of other items. 
However, the rank orderings given by AI-F are not equivalent to DPFR, as $\tau < 0.9$ for 5/6 datasets (excl. Amazon-lb). 
For the same measure-pair and between datasets, the $\tau$ of AI-F and DPFR also varies a lot. E.g., $\tau=0.07$ for NDCG-Ent for Lastfm, but $\tau=0.9$ 
for Amazon-lb. We thereby do not recommend using any of the \textsc{Fair+Rel} measures (none correlates with Pareto optimality). 

Taking the mean of \textsc{Fair} and \textsc{Rel} scores (avg) at a glance seems to correlate highly with DPFR. However, while it gives equivalent rankings ($\tau\geq 0.9$) in some cases (e.g., for Amazon-lb, most of ML-10M and QK-video, and half of ML-20M), it only does so for (1) datasets with lower \textsc{Rel} scores (Amazon-lb, QK-video), i.e., in cases where all models perform poorly, we have low variance in \textsc{Rel}, which leads to fairness dominating both avg and DPFR; (2) datasets with low variance in \textsc{Fair} scores (ML-*).  
In such cases, quantifying the evaluation jointly is challenging as one aspect dominates over the other. 
In the other datasets, the rank ordering given by the average is inconsistent: sometimes $\tau \geq 0.9$ for one dataset, but not for the others. This inconsistency between datasets holds for all measure pairs, except for P-Jain and NDCG-Gini. Due to these inconsistencies, we discourage using the arithmetic mean. 

Overall, our correlation analysis shows that existing joint \textsc{Fair+Rel} evaluation measures cannot be used as a reliable proxy for DPFR. 

\subsubsection{Best model disagreement} 

We take a closer look at how DPFR relates to computing averages, as they are similar approaches in terms of combining scores from a measure pair. As comparing the raw scores of DPFR and the average is invalid, we instead count the disagreement between the best model based on DPFR and the mean of \textsc{Fair} and \textsc{Rel} scores  (Tab.~\ref{tab:best_model_disagreement}). 
The aim is to study whether one would come to the same conclusion regarding the best model, using the two different joint evaluation approaches.

Among the 12 measure pairs that are fit for DPFR, we find that \textbf{the best model according to DPFR is not always the same according to the average of \textsc{Fair} and \textsc{Rel} scores of the same pair}; in one case the disagreement is up to 58.33\% (for Lastfm). The disagreement is generally much higher in the more complex rank-based measures (0--83.33\%) compared to simpler set-based \textsc{Rel} measures (0--50.00\%). 
Therefore, there are many cases where the mean of \textsc{Fair} and \textsc{Rel} scores is not the best case, especially for Lastfm and Jester where \textsc{Rel} scores are higher and vary more. 
In these two datasets, more often than not, DPFR leads to different conclusions than a simple average. Yet, sometimes the average agrees with DPFR in the best model: for QK-video, disagreement is low (8.33\%), and there is a perfect agreement on the best model for Amazon-lb; we posit that these are due to equally poor and low variance in the \textsc{Rel} scores. This is in line with our correlation analysis. 
As there is a huge range of variability across datasets (0--58.33\%), we do not recommend using a simple average to get the same result as DPFR, as it is unreliable and inconsistent. 
Generally, averaging fails to reach the same conclusion as DPFR almost half the time, especially when the \textsc{Rel} and \textsc{Fair} scores vary highly.


\begin{table}[tbp]
\centering
\caption{The percentage of best model disagreement when taking the mean of \textsc{Fair} and \textsc{Rel} scores as opposed to using DPFR, separated by the \textsc{Rel} measure type.  P@$k$ and R@$k$ are set-based, NDCG and MAP are rank-based. We only consider the 12 measure pairs with a nonzero, defined gradient ($\S$\ref{sss:no-fit}).}
\label{tab:best_model_disagreement}
\resizebox{0.70\columnwidth}{!}{
\begin{tabular}{lrrr}
\toprule
{} &                    Set-based & Rank-based &    All \\
\hline
\midrule
Lastfm    &          50.00 &      66.67 &  58.33 \\
Amazon-lb &          0.00 &       0.00 &   0.00 \\
QK-video  &          16.67 &       0.00 &   8.33 \\
Jester    &          16.67 &      83.33 &  50.00 \\
ML-10M    &           0.00 &      66.67 &  33.33 \\
ML-20M       &       0.00 &       50.00 &  25.00 \\
\hline
\midrule
All datasets &      13.89 &       44.44 &  29.17 \\
\bottomrule
\end{tabular}
}
\end{table}


\subsection{Efficient DPFR} 
\label{ss:compare-eff}


\subsubsection{The efficiency of the PF generation} 
We study the efficiency of DPFR by comparing the PF, an estimated version of PF on a subset of points, and the \textsc{Fair+Rel} measures. The estimated version of PF uses 3--12 points as per $\S$\ref{ss:compute-eff}. 
We compute the amount of points in the estimated PF as \% of those in the PF, and the resulting computation times. One point in the PF translates to one round of computing all \textsc{Fair} and \textsc{Rel} measures, so fewer points mean faster. For brevity, we report the estimated PF with only 3, 6, and 12 points in Tab.~\ref{tab:big-est-time} 
(see App.~\ref{app:actual-scores}, Tab.~\ref{tab:corr_est} for extended results).
 \balance

\begin{table}[tbp]
    \centering
    \caption{Efficiency and effectiveness of PF, estimated PF (Est.~PF), and \textsc{Fair+Rel} measures: \% of data points in the Est.~PF (\% pts), computation time. Dist is the average distance between midpoints in the Est.~PF and PF over 12 measure pairs. Minimum agreement (Min $\tau$) is the Kendall's $\tau$ correlation between DPFR with PF and Est.~PF. 
    Both PFs compute 11 \textsc{Rel} and \textsc{Fair} measures simultaneously. 
    The times for other evaluation measures are averaged (Avg/model) and summed (All models) over 16 model combinations.}
    \label{tab:big-est-time}
\scalebox{0.65}{
     \begin{tabular}{lllrrrrrr}
\toprule
& & {} &  Lastfm &  Amazon-lb &  QK-video &  Jester &  ML-10M &  ML-20M \\
\midrule
\multicolumn{2}{c}{\#pts}& PF& 4882 & 847 & 499 & 16202 & 2781 & 3783 \\
\cline{1-9}
\multicolumn{2}{c}{\multirow[c]{3}{*}{\%pts}} & Est.~PF (12 pts) & 0.25 &      1.42 &     2.40 &   0.07 &   0.43 &   0.32 \\
& & Est.~PF (6 pts) &  0.12 &      0.71 &     1.20 &   0.04 &   0.22 &   0.16 \\
& & Est.~PF (3 pts) & 0.06 &      0.35 &     0.60 &   0.02 &   0.11 &   0.08\\
\hline
\midrule
\multirow[c]{14}{*}{\rotatebox[origin=c]{90}{$\downarrow$ Computation time (mins.)}}& & PF           &   19.18 &       0.56 &     10.49 &  847.42 &   28.99 &   75.77 \\
& & Est.~PF (12 pts) &    2.02 &       0.19 &      4.23 &  552.16 &    1.90 &    2.60 \\
& & Est.~PF (6 pts)  &    2.00 &       0.19 &      4.12 &  551.72 &    1.84 &    2.54 \\
& & Est.~PF (3 pts)  &    2.01 &      0.19 &     4.07 &  552.26 &   1.82 &   2.52\\
\cline{2-9}
& \multirow[c]{5}{*}{\rotatebox[origin=c]{90}{Avg/model}} & IBO    &    <0.3s &     <0.3s &      0.01 &    0.01 &   <0.3s &     0.01  \\
& & MME    &    2.04 &       0.03 &     19.51 &    0.09 &   15.25 &    89.13 \\
& & IAA    &    <0.3s &     <0.3s &      0.01 &    0.02 &    0.01 &     0.02 \\
& & II-F   &    <0.3s &     <0.3s &      0.01 &    0.01 &    0.01 &     0.02 \\
& & AI-F   &    <0.3s &     <0.3s &      0.01 &    0.01 &   <0.3s &     0.01 \\
\cline{2-9}
& \multirow[c]{5}{*}{\rotatebox[origin=c]{90}{All models}} & IBO  &    0.02 &     <0.3s &      0.10 &    0.12 &    0.06 &     0.15 \\
& & MME  &   32.63 &       0.49 &    312.14 &    1.38 &  244.03 &  1426.10 \\
& & IAA  &    0.03 &     <0.3s &      0.10 &    0.36 &    0.13 &     0.30  \\
& & II-F &    0.04 &     <0.3s &      0.16 &    0.14 &     0.1 &     0.25 \\
& & AI-F &   0.03 &     <0.3s &      0.11 &    0.13 &    0.07 &     0.17 \\
\hline
\midrule
\multicolumn{2}{c}{\multirow[c]{3}{*}{$\uparrow$ 
Min $\tau$}} & Est.~PF (12 pts)& 0.95 &       1.00 &      1.00 &    0.98 &    0.98 &    0.97\\
\multicolumn{2}{c}{} &  Est.~PF (6 pts)&  0.90 &       0.97 &      1.00 &    0.98 &    0.95 &    0.92\\
\multicolumn{2}{c}{} &  Est.~PF (3 pts)&  0.78 &       0.98 &      1.00 &    1.00 &    0.97 &    0.75\\
\midrule
\multicolumn{2}{c}{\multirow[c]{3}{*}{$\downarrow$ Dist.}} & Est.~PF (12 pts)& 0.01 &      0.02 &     0.00 &   0.00 &   0.01 &   0.01 \\
\multicolumn{2}{c}{} &  Est.~PF (6 pts)& 0.03 &      0.05 &     0.00 &   0.00 &   0.02 &   0.02 \\
\multicolumn{2}{c}{} &  Est.~PF (3 pts)& 0.03 &      0.05 &     0.00 &   0.00 &   0.03 &   0.05 \\
\bottomrule
\end{tabular}
}
\end{table}


The PF (Fig.~\ref{fig:pareto-pair-plot}) has hundreds to tens of thousands of points each, while the estimated PF only contains 0.02--2.40\% of the points, which means reduced computational complexity for the PF generation. In terms of actual computation time (Tab.~\ref{tab:big-est-time}), computing the PF with \textsc{Oracle2Fair} take 0.56--75.77 mins to compute, but only 0.19--4.07 mins for the PFs estimated with 3 points for all datasets except Jester. For Jester, it takes $\sim$14 hours and the estimation only takes $\sim$9 hours. 
However, this is expected for Jester as it has 62K users in the test split, as opposed to the 3.5K or fewer in the other datasets (i.e., see Tab.~\ref{tab:data_split} in App.~\ref{app:stat}). 
While computing the estimated PFs is on average slower than computing the joint measures IBO, IAA, II-F, and AI-F, it is expected as the (estimated) PFs compute 11 measures simultaneously. Yet, in most cases (except Amazon-lb and Jester), the estimated PFs are still faster to compute than the time to compute MME for one model per dataset, let alone to compute MME for all models. For ML-20M, computing the estimated PF is even up to 35 times faster than computing MME of one model.  





\subsubsection{The effectiveness of efficient DPFR} 
To what extent is the DPFR from the efficiently generated PF (estimated PF) a reasonable proxy for fairness-relevance joint evaluation using the PF, in terms of giving a similar ordering of models? 
We compare the DPFR from the PF and estimated PF using Kendall's $\tau$ correlations. Further, as DPFR is computed based on a $\alpha$-based reference point lying on the PF, to quantify possible accuracy loss of the estimated PF, in Tab.~\ref{tab:big-est-time} we show the error of the midpoint estimation. This error is the Euclidean distance between the reference point in the PF and estimated PF (i.e., the midpoint in our case), as per \cite{Wang2016AEngineering}.

 
We first analyse the error of the midpoint estimation. For the 12 measure pairs and 6 datasets, the midpoint coordinates on average do not move much: the distance is 0.00--0.05, even when the PF is only estimated with 3 points. 
Ergo, the correlations between the rank ordering of models given by the DPFR of PF and its estimation, are still equivalent ($\tau \geq 0.9$) when estimated with 6 or 12 points \cite{Maistro2021PrincipledRankings,Voorhees2001EvaluationDocuments}. Even the 3-point estimation maintains high agreement ($\tau \in [0.75,1]$), with only 5 cases having $\tau<0.9$ across 6 datasets and 12 measure-pairs. 
Therefore, it is possible to only compute a small number of points in the PF, e.g., 6 or 12 points, and still make a reliable PF estimation, evidenced by the small shift of the PF midpoint and the rank ordering of the models remaining equivalent ($\tau \geq 0.9$), if not identical ($\tau=1$) for all measure pairs and datasets. 

\section{Discussion and conclusions}
Recommendation evaluation has long used measures that quantify only relevance, but has recently shifted to include fairness. However, there exists no de-facto way to robustly quantify these two aspects. We propose a novel approach (DPFR) that uses fairness and relevance measures under a joint evaluation scheme for RSs. DPFR computes the empirical best possible recommendation, jointly accounting for a given pair of relevance and fairness measures, in a principled way according to Pareto-optimality. DPFR is modular, tractable, and intuitively understandable. It can be used with existing measures for relevance and fairness, and allows for different trade-offs of relevance and fairness. We empirically show that existing evaluation measures of fairness w.r.t.~relevance \cite{Biega2018EquityRankings,Diaz2020EvaluatingExposure,Wu2022JointRecommendation, Saito2022FairRanking, Borges2019EnhancingAutoencoders} behave inconsistently: they disagree with optimal solutions based on DPFR computed on more robust and well-understood measures of relevance, such as NDCG, and fairness, such as Gini. 
We uncover some weaknesses of these measures, but more research is warranted to study their behaviour. Admittedly, existing joint measures are not originally defined to be aligned with existing relevance and fairness measures \cite{jain1984quantitative,Zhu2020FARM:APPs,Gini1912VariabilitaMutabilita,Mansoury2021ASystems,Mansoury2020FairMatch:Systems,Patro2020FairRec:Platforms}, so it is not surprising that they have different results from DPFR. However, existing measures show varying performance also from each other and from well-understood relevance and fairness measures. Thus, DPFR can be a viable alternative for robust, interpretable, and provenly optimal evaluation in offline scenarios. We also show that DPFR can be computed fast while reaching equivalent conclusions. 
Overall, DPFR demonstrates distinct benefits in mitigating false conclusions by up to 50\% compared to basic aggregation methods like averaging. Surprisingly, simple averaging aligns more with our Pareto-optimal based DPFR, than existing joint measures. 
We recommend combining either MAP-Ent or NDCG-Ent: the conclusions are distinguishable from simply averaging, or taking the best model 
based on fairness or relevance measures. 

\clearpage
\begin{acks}
The work is supported by the Algorithms, Data, and Democracy project (ADD-project), funded by Villum Foundation and Velux Foundation. Qiuchi Li contributed to the idea of computing the reference point. We thank the DIKU IR Lab and the anonymous reviewers, who have provided helpful feedback to improve earlier versions of the manuscript.
\end{acks}


%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}


\appendix

\section{Extended dataset statistics}\label{app:stat}
Tab.~\ref{tab:data_split} presents the statistics of each dataset split. For several datasets (e.g., Amazon-lb and ML-*), the number of users in the test split is significantly less than the number of users in the train split. Tab.~\ref{tab:rel_item_stat} presents the statistics of items in the test split, per user.


\begin{table}[htbp]
    \centering
        \caption{Number of [users, items, and interactions] in the train, validation, and test split after preprocessing. 
    } 
    \label{tab:data_split}
\resizebox{\columnwidth}{!}{
\begin{tabular}{llll}
\toprule
{} &                   train &                     val &                    test \\
\midrule
Lastfm    &     [1842, 2821, 42758] &     [1831, 2448, 14248] &     [1836, 2476, 14237] \\
Amazon-lb &       [1054, 552, 8860] &        [470, 204, 1811] &        [437, 209, 1726] \\
QK-video  &     [4656, 6245, 34345] &      [3470, 4095, 8726] &      [3514, 4101, 8706] \\
Jester    &   [63724, 100, 1294511] &    [62137, 100, 427623] &    [62167, 100, 427926] \\
ML-10M    &  [49378, 6838, 4944064] &    [2695, 7828, 296914] &    [1523, 7880, 121707] \\
ML-20M    &  [89917, 8719, 9882504] &    [4987, 10742, 472243] &    [2178, 13935, 233394] \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[htbp]
    \caption{Statistics of items in the test split, per user, i.e., the number of relevant items per user.}
    \label{tab:rel_item_stat}
    \centering
    \resizebox{0.65\columnwidth}{!}{
\begin{tabular}{lrrrr}
\toprule
{} &    mean &  min &  median &   max \\
\midrule
Lastfm    &    7.75 &    1 &       8 &    19 \\
Amazon-lb &    3.95 &    1 &       3 &    16 \\
QK-video  &    2.48 &    1 &       2 &    16 \\
Jester    &    6.88 &    1 &       6 &    29 \\
ML-10M    &   79.91 &    1 &      46 &  1632 \\
ML-20M    &  107.16 &    1 &      53 &  2266 \\
\bottomrule
\end{tabular}
}
\end{table}


\section{Algorithms for generating Pareto Frontier}\label{app:algo}
We present the pseudocodes of the algorithms for generating the Pareto Frontier: the Oracle (Algorithm \ref{alg:oracle}) and  \textsc{Oracle2Fair} (Algorithm \ref{alg:oracle2fair}). We provide the worst-case time complexity analysis for both algorithms and discuss a possible edge case. 
We denote as $k$ the recommendation cut-off, as $m$ the number of users, as $n$ the number of items, as $H$ the maximum number of items in a user's interaction history ($H_u$) across all users, and as $R$ the maximum number of items in a user's test split ($R_u^*$) across all users. We use the binary logarithm ($\log_2{}$). For brevity, we omit lines with $O(1)$, and we state the reasoning for each line (L). 



\SetKwComment{Comment}{/* }{ */}
\RestyleAlgo{ruled}
\LinesNumbered


\begin{algorithm*}[htbp]
\caption{Oracle \\ Create recommendations with the highest relevance}\label{alg:oracle}

\KwData{ \\
$I$: all items in the dataset; \\
$H_u$: items in train-val split for each user $u \in U$;  \\
$R_u^*$: items in test split (relevant items) for each user $u \in U$; \\
$k$: number of recommended items
}
\KwResult{\\
$rec$: most relevant recommendation\\\\
$result$: a list of relevance and fairness scores\\
$itemNotInRec$: items that are not in the recommendation}

\Comment{Handle users with exactly $|R_u^*|=k$}
\lForEach{$u \in U$ where $|R_u^*| = k$}{{$rec[u] \gets R_u^*$}}

\Comment{Handle users with $|R_u^*| > k$}

\For{$K=k+1$ \KwTo $max(|R_u^*|)$}{\label{ln:startgtk}
    $userWithK \gets$ get users where $|R_u^*|=K$ \\
    \ForEach{$u \in userWithK$}{
        $takenItem[u] \gets {R_u^* \cap rec}$ \\
        $weight[u] \gets sum(countInRec(takenItem[u]))$ \\
        }
    $sortUserWithK \gets$ sort $userWithK$ by the least weight \\
    $tempRec[u] \gets R_u^* \setminus takenItem[u]$ \\
    keep only max $k$ items in $tempRec[u]$ \\

    \ForEach{$u \in sortUserWithK$}{
        $rec[u] \gets tempRec[u]$ \\
        $numItemToAdd \gets k - |tempRec[u]|$ \\
        sort $takenItem[u]$ by the least item count \\
        $rec[u].append(takenItem[u][:numItemToAdd])$  
    }
}  \label{ln:endgtk}

\Comment{Handle users with $|R_u^*| < k$}
$remainUser \gets$ get users where $|R_u^*|<k$ \\ \label{ln:startltk}
\lForEach{$u \in remainUser$}{$rec[u] \gets |R_u^*|$} 
$itemNotInRec \gets I \setminus rec$ \\
\ForEach{$u \in remainUser$}{
    \While{$|rec[u]|<k$ and $itemNotInRec \neq \emptyset$}{
        \For{$item \in itemNotInRec$}{
         \If{$item \notin H_u$}{
         $rec[u].append(item)$ \\
         $itemNotInRec \gets itemNotInRec\setminus \{item\}$
         }
        }    
    }

    \If{$|rec[u]|<k$}{
        \While{$|rec[u]|<k$}{

        $candItem \gets$ least popular item in $rec$ that is not in $H_u \cup R_u^* \cup rec[u]$ \\
        $rec[u].append(candItem)$
    }
    }
} \label{ln:endltk}
$result \gets calculateScores(rec)$ 
\end{algorithm*}



\begin{algorithm*}[htbp]
\caption{\textsc{Oracle2Fair} \\After recommending maximally relevant items, iteratively change the recommendation list to increase fairness until maximum fairness is reached}\label{alg:oracle2fair}
\KwData{$H_u, R_u^*,I, k$}
\KwResult{\\
$rec$: most fair possible recommendation;\\
$result$: a list of relevance and fairness scores}

$rec, result, itemNotInRec \gets Oracle(I, H_u, R_u^*)$

\Comment{Get the most popular item in the recommendations and its frequency count}
 $newMostPop \gets mostPop \gets getMostPopItem(rec)$  \\ \label{ln:updatecountbegin}
 $newCntPop \gets cntPop \gets cnt(mostPop, rec)$ \\

$uWithMostPop \gets$ all users with $mostPop \in rec[u]$ \\
sort $uWithMostPop$ by largest index of $mostPop$ in $rec[u]$ \\ \label{ln:updatecountend}
\For{$i \in itemNotInRec$}{
\lIf{$cntPop=1$}{break}    
\If{$newMostPop \neq mostPop$}{
    $mostPop \gets newMostPop$ \\ \label{ln:mostpopstart}
    update $uWithMostPop$ following $mostPop$ \\ \label{ln:mostpopend}
}
\If{$newMostPop = mostPop$}{
    $candU \gets$ all $u$ in $uWithMostPop$ where $i \notin H_u$  \\
    \If{$\exists u \in candU$ with $i \in R_u^*$ \label{ln:recommendstart}}{
        recommend $i$ to the top $u$ from $candU$ with $i \in R_u^*$
    }
    \lElse{recommend $i$ to the top $u$ from $candU$}
    reorder $rec[u]$ so all relevant items are at the top \\
    $result.append(calculateScores(rec))$ \\ \label{ln:recommendend}
    $itemNotInRec \gets itemNotInRec \setminus \{i\}$ \\
    $newMostPop \gets getMostPopItem(rec)$ \\
    $newCntPop \gets cnt(mostPop, rec)$ 
    }
  }
\Else{
    do lines \ref{ln:updatecountbegin}--\ref{ln:updatecountend} \\ \label{ln:dolines}
    $i \gets leastPop \gets getLeastPopItem(rec)$ \\ \label{ln:getleastpop}
    $m, n \gets |U|, |I|$ \\
    \While{$cntPop > \lceil km/n \rceil$}{
    \lIf{$newMostPop \neq mostPop$}{do lines \ref{ln:mostpopstart}--\ref{ln:mostpopend}}
    \If{$newMostPop = mostPop$}{
    $candU \gets$ all $u$ in $uWithMostPop$ where $i \notin H_u \cup rec[u]$  \\
    do lines \ref{ln:recommendstart}--\ref{ln:recommendend} \\
    do lines \ref{ln:dolines}--\ref{ln:getleastpop}
    }
    }
}

\end{algorithm*}




\subsection{Time complexity of the Oracle (\Cref{alg:oracle})}
\label{app:analyse-oracle}

The line-by-line time complexity analysis of Oracle (\Cref{alg:oracle}) is as follows:

\begin{itemize}
    \item L1: $O(km)$, creating list of size $k$ for $m$ users
    \item L2--17: $O(R^2m + Rm \log{m} + Rmk\log{k})$, resulting from at most $R$ iterations of:
    \begin{itemize}
        \item L3: $O(m)$, linear search on list of size $m$
        \item L4--7: $O(mR)$, at most $m$ times looking up at most $R$ values
        \item L8: $O(m \log{m})$, sorting a list of size $m$
        \item L11--16: $O(mk \log{k})$, at most $m$ times sorting list of size $k$
    \end{itemize}
    
    \item  L18: $O(m)$, linear search on list of size $m$
    \item  L19: $O(m)$, $m$ assignment operations
    \item  L21--36: $O(k^2m^2)$, resulting from at most $m$ iterations of: 
    \begin{itemize}
        \item L22--29: $O(kH)$, at most $k$ times of linear search on list of size $H$
        \item L30--L35: $O(k^2m)$, $k$ times of counting in a list of size $km$.
    
        In most cases, the number of users $m >> H$, so the time complexity of this block is dominated by $O(k^2m)$.
    \end{itemize}
    \item L37: $O(km)$ computing relevance/fairness measures for $m$ users based on recommendation list of size $k$

\end{itemize}

Overall, the time complexity of Oracle is dominated by that of L2 and L21--36. Hence, the time complexity of Oracle is $O(R^2m + Rm \log{m} + Rmk \log{k} + k^2m^2)$.


\subsection{Time complexity of \textsc{Oracle2Fair} (\Cref{alg:oracle2fair})}

The line-by-line time complexity analysis of \textsc{Oracle2Fair} (\Cref{alg:oracle2fair}) is as follows:

\begin{itemize}
    \item L1: $O(R^2m + Rm \log{m} + Rmk \log{k} + k^2m^2)$ (derived in \Cref{app:analyse-oracle})
    \item L2--4: $O(km)$, $m$ times of counting and linear search on list of size $k$
    \item L5: $O(m \log{m})$, sorting a list of size $m$ (e.g., with Tim Sort)
    \item L6--24: $O(max(Hmn, kmn))$, resulting from at most $n$ iterations of: 
    \begin{itemize}
        \item L10: $O(km)$, $m$ times of linear search on list of size $k$
        \item L13: $O(Hm)$, $m$ times linear search on list of size at most $H$
        \item L14: $O(km)$
        \item L18: $O(k \log{k})$, sorting a list of size $k$
        \item L21--L22: $O(km)$; the term $O (k \log{k})$ is dominated by $O(km)$, as typically the cut-off $k$ is much smaller than the number of users $m$ (and hence $\log{k} < m$).
    \end{itemize}

     \item L26: $O(km + m \log{m})$, combining L2--4 and L5
     \item L27: $O(km)$, $m$ times counting on list of size $k$
     \item L29--34: $O(max(Hkm^2,k^2m^2)+km^2\log{m})$, resulting from at most $km$ iterations (derived below) of:
     \begin{itemize}
         \item L30: $O(km)$, from L10
         \item L32: $O(Hm)$, $m$ times linear search on list of size at most $H$
         \item L33: $O(km)$, from L14 and L18
         \item L34: $O(km+m \log{m})$, from L26--27
     \end{itemize}
\end{itemize}

We estimate the worst-case complexity of the number of iterations of the code block in L29--34, by assuming that the initial recommendation is the unfairest. The unfairest recommendation happens when the same $k$ unique items are recommended to all $m$ users. 
This code block aims to reduce the max frequency count of all items to $\left\lceil km/n \right\rceil$. Hence, the number of iterations is $\sum^{k} (m-\left\lceil km/n \right\rceil) \leq k(m-\frac{km}{n}-1) = km-\frac{k^2m}{n}-k \leq km$.

All in all, the time complexity of \textsc{Oracle2Fair} is dominated by that of L1, L6--24, and L29--34. Hence, the overall time complexity of \textsc{Oracle2Fair} is $O(R^2m + Rm \log{m} + Rmk \log{k} +  k^2m^2 + km^2\log{m} + kmn )$ if $k\geq H$, or $O(R^2m + Rm \log{m} + Rmk \log{k} +  Hkm^2 + km^2\log{m} + Hmn )$ otherwise. To simplify the time complexity, further assumptions need to be made for one or more variables.

\subsection{Possible edge cases}
There might be edge cases, for example, datasets where the train/val set of each user contains almost all items in the dataset (each user has rated/clicked most items in the dataset). In this case, if we do not want to re-recommend items in the train/val set to users, some items may have item counts more than
$\left\lceil km/n \right\rceil$ at the end of the process, and \Cref{alg:oracle2fair} might not halt. However, such datasets are rare in recommender systems.


\section{Modifications to the GS algorithm}\label{app:gs}
The original GS algorithm \cite{Wang2022ProvidingSystems} increases individual item fairness within clusters of similar items. The item similarity is determined based on the item embedding. As our experiments and the \textsc{Fair} measures do not deal with the additional constraint of item similarity, we consider all items as similar. Therefore, we only have a single cluster of items.

On top of that, we also modify GS to increase computational efficiency. In the original GS algorithm, for each pair of candidate items for replacement $i$ and candidate items to be replaced $i'$, the algorithm finds all users that have $i$ in the original recommendation list. The algorithm then computes the loss in relevance (computed using predicted relevance value) if item $i$ is replaced by $i'$. Until this point, our modified algorithm does the same. The difference is that we save each $i, i', u$, and the loss associated, while the original algorithm only saves the information for the one user $u^*$, whose recommendation list will suffer the least loss when we replace $i$ with $i'$. The original GS then proceeds to make the replacement, update the pool of candidate items for replacement and to be replaced, and go through the entire process again. Initially, we found that with the GS algorithm, around 20\% of the initial recommendations are replaced during the process, meaning that for Amazon-lb, there are at least $437 \times 10 \times 0.2 \geq 800$ iterations of the process (Tab.~\ref{tab:data_split}). The number of iterations is much bigger for ML-10M, which has more than three times the  recommendation slots as Amazon-lb, and hence it is very costly to use the GS algorithm as is. 

Our modified GS utilises the saved information earlier. After going through all pairs of $(i,i')$, we sort the saved list from the smallest to the largest loss, and (attempt to) perform the replacement using the first $P$ pairs, where $P$ is 25\% of the number of recommendation slots. During the replacement process, if the item that is supposed to be replaced no longer exists in the user's recommendation list, we simply skip the replacement.



\section{Extended results}\label{app:actual-scores}
We present the actual scores of the recommender models in Tab.~\ref{tab:base-rerank-all-1}--\ref{tab:base-rerank-all-2}. In Tab.~\ref{tab:app-gradient}, we present the gradient values of the PF, used in determining which pair of measures are suitable for DPFR. In Fig.~\ref{fig:app-pairplot} we present the Pareto Frontier (PF) of fairness and relevance together with recommender model scores in Tab.~\ref{tab:base-rerank-all-1}--\ref{tab:base-rerank-all-2} for Amazon-lb, Jester, and ML-*. In \Cref{tab:dpfr-scores,tab:dpfr-scores2} we present the DPFR scores for all datasets. 
In Tab.~\ref{tab:corr_est} we present the Kendall's $\tau$ correlation scores of the DPFR from estimated PF and the PF.


\input{tab/bigtable} 


\begin{table*}[tbp]
\centering
\caption{The gradient values of the PF, based on the extreme points (starting and ending points). We consider a gradient to be `good' if it is not zero or undefined (-).}\label{tab:app-gradient} 
\begin{tabular}{lrrrrrrrl}
\toprule
{} & Lastfm & Amazon-lb & QK-video & Jester &  ML-10M &  ML-20M &  \# good &    conclusion \\
\midrule
HR-Ent    &   -97.57 &      -1.86 &     -0.31 &      - &  -14.74 &   -6.95 &        5 &  inconsistent \\
HR-FSat   & -1439.17 &     -19.92 &      0.00 &      - &  -30.48 &  -18.97 &        4 &  inconsistent \\
HR-Gini   &   561.63 &       6.23 &      3.71 &      - &  117.19 &   43.44 &        5 &  inconsistent \\
HR-Jain   &  -979.86 &     -18.77 &     -5.80 &      - & -157.73 &  -78.22 &        5 &  inconsistent \\
HR-QF     &     0.00 &       0.00 &      0.00 &      - &  -30.48 &  -18.97 &        2 &  inconsistent \\
MAP-Ent   &    -0.17 &      -0.17 &     -0.03 &  -0.07 &   -0.14 &   -0.18 &        6 &   always good \\
MAP-FSat  &    -2.46 &      -1.81 &      0.00 & -44.47 &   -0.29 &   -0.48 &        5 &  inconsistent \\
MAP-Gini  &     0.96 &       0.56 &      0.34 &   1.42 &    1.12 &    1.10 &        6 &   always good \\
MAP-Jain  &    -1.68 &      -1.70 &     -0.54 &  -0.37 &   -1.51 &   -1.98 &        6 &   always good \\
MAP-QF    &     0.00 &       0.00 &      0.00 &    0.0 &   -0.29 &   -0.48 &        2 &  inconsistent \\
MRR-Ent   &   -97.57 &      -1.86 &     -0.31 &      - &  -14.74 &   -6.95 &        5 &  inconsistent \\
MRR-FSat  & -1439.17 &     -19.92 &      0.00 &      - &  -30.48 &  -18.97 &        4 &  inconsistent \\
MRR-Gini  &   561.63 &       6.23 &      3.71 &      - &  117.19 &   43.44 &        5 &  inconsistent \\
MRR-Jain  &  -979.86 &     -18.77 &     -5.80 &      - & -157.73 &  -78.22 &        5 &  inconsistent \\
MRR-QF    &     0.00 &       0.00 &      0.00 &      - &  -30.48 &  -18.97 &        2 &  inconsistent \\
NDCG-Ent  &    -0.24 &      -0.22 &     -0.04 &   -0.1 &   -0.20 &   -0.25 &        6 &   always good \\
NDCG-FSat &    -3.50 &      -2.32 &      0.00 & -68.56 &   -0.42 &   -0.68 &        5 &  inconsistent \\
NDCG-Gini &     1.37 &       0.73 &      0.47 &    2.2 &    1.62 &    1.55 &        6 &   always good \\
NDCG-Jain &    -2.38 &      -2.19 &     -0.73 &  -0.57 &   -2.18 &   -2.79 &        6 &   always good \\
NDCG-QF   &     0.00 &       0.00 &      0.00 &    0.0 &   -0.42 &   -0.68 &        2 &  inconsistent \\
P-Ent     &    -0.20 &      -0.33 &     -0.07 &  -0.08 &   -0.16 &   -0.20 &        6 &   always good \\
P-FSat    &    -2.95 &      -3.53 &      0.00 & -51.41 &   -0.33 &   -0.55 &        5 &  inconsistent \\
P-Gini    &     1.15 &       1.10 &      0.89 &   1.65 &    1.26 &    1.26 &        6 &   always good \\
P-Jain    &    -2.01 &      -3.33 &     -1.40 &  -0.43 &   -1.70 &   -2.27 &        6 &   always good \\
P-QF      &     0.00 &       0.00 &      0.00 &    0.0 &   -0.33 &   -0.55 &        2 &  inconsistent \\
R-Ent     &    -0.17 &      -0.17 &     -0.03 &  -0.07 &   -0.26 &   -0.30 &        6 &   always good \\
R-FSat    &    -2.57 &      -1.83 &      0.00 & -48.04 &   -0.53 &   -0.82 &        5 &  inconsistent \\
R-Gini    &     1.00 &       0.57 &      0.34 &   1.54 &    2.04 &    1.88 &        6 &   always good \\
R-Jain    &    -1.75 &      -1.73 &     -0.54 &   -0.4 &   -2.75 &   -3.39 &        6 &   always good \\
R-QF      &     0.00 &       0.00 &      0.00 &    0.0 &   -0.53 &   -0.82 &        2 &  inconsistent \\
\bottomrule
\end{tabular}
\end{table*}

\input{tab/dpfr}

\begin{figure*}[tbp]
    \centering
    \includegraphics[width=\columnwidth]{img/pairplot_amazonlb.png}
    \includegraphics[width=\columnwidth]{img/pairplot_jester.png}
    \includegraphics[width=\columnwidth]{img/pairplot_ml10m.png}
    \includegraphics[width=\columnwidth]{img/pairplot_ml20m.png}
    
    \caption{Pareto Frontier of fairness and relevance (in blue), together with recommender model scores for Amazon-lb, Jester, and ML-*. \textsc{Fair} measures are on the $y$-axis and \textsc{Rel} measures are on the $x$-axis. We implement exponential-like scales to enhance the visibility of the model plots. The \textsc{Rel, Fair}, Avg, and DPFR denote the best model based on each evaluation approach.}
    \label{fig:app-pairplot}

\end{figure*}


\begin{table*}[tbp]
    \centering
    \caption{Range of agreement $\tau$ between estimated PF and PF across 12 measure pairs, using the estimated PF with 3--12 points.}
    \label{tab:corr_est}
    \resizebox{0.72\textwidth}{!}{
\begin{tabular}{lrrrrrr}
\toprule
{\#pts} &      Lastfm &   Amazon-lb &    QK-video &      Jester &      ML-10M &      ML-20M \\
\midrule
3  &  0.78--1.00 &  0.98--1.00 &  1.00--1.00 &  1.00--1.00 &  0.97--1.00 &  0.75--1.00 \\
4  &  0.88--1.00 &  0.98--1.00 &  0.98--1.00 &  0.98--1.00 &  0.98--1.00 &  0.93--1.00 \\
5  &  0.78--1.00 &  0.98--1.00 &  1.00--1.00 &  1.00--1.00 &  0.97--1.00 &  0.92--1.00 \\
6  &  0.90--1.00 &  0.97--1.00 &  1.00--1.00 &  0.98--1.00 &  0.95--1.00 &  0.92--1.00 \\
7  &  0.88--1.00 &  1.00--1.00 &  1.00--1.00 &  1.00--1.00 &  0.98--1.00 &  0.93--1.00 \\
8  &  0.90--1.00 &  0.98--1.00 &  1.00--1.00 &  0.98--1.00 &  1.00--1.00 &  0.95--1.00 \\
9  &  0.98--1.00 &  1.00--1.00 &  1.00--1.00 &  1.00--1.00 &  0.97--1.00 &  0.98--1.00 \\
10 &  0.88--1.00 &  1.00--1.00 &  1.00--1.00 &  0.98--1.00 &  1.00--1.00 &  0.95--1.00 \\
11 &  0.92--1.00 &  1.00--1.00 &  1.00--1.00 &  1.00--1.00 &  0.98--1.00 &  0.97--1.00 \\
12 &  0.95--1.00 &  1.00--1.00 &  1.00--1.00 &  0.98--1.00 &  0.98--1.00 &  0.97--1.00 \\
\bottomrule
\end{tabular}}
\end{table*}



\section{Further discussions}
\label{app:discussion}

\subsection{The impact of replacing frequently recommended items}
In this work, we replace frequently recommended items in two cases: during the Pareto Frontier generation (PF) with the {\sc Oracle2Fair} algorithm (\Cref{ss:generation}) and as part of the fair rerankers (\Cref{s:experiments}). In both cases, none of the replacements significantly affect the overall recommendation performance:

\noindent\textbf{1. Replacement in the {\sc Oracle2Fair} algorithm.} The {\sc Oracle2Fair} algorithm is used to generate the recommendation lists whose scores make up the PF (not the scores from the model-generated recommendation lists, that we compare to a point in the PF). The replacements are done on lists separate from the model's recommendation lists. Therefore, the replacements in the {\sc Oracle2Fair} algorithm do not affect the recommendation performance based on relevance and fairness measures. 

\noindent\textbf{2. Replacement in the fair rerankers.} We look at recommendation performance based on NDCG (relevance) and Gini (fairness). In all six datasets, when comparing the best non-reranked model to its reranked versions (e.g., for Lastfm, it is NCL vs NCL-BC, NCL-CM, and NCL-GS), the decrease in NDCG is not more than 0.26, and the decrease is even below 0.15 in all datasets excluding Jester, (\Cref{app:actual-scores}, \Cref{tab:base-rerank-all-1,tab:base-rerank-all-2}). For Jester, the 0.26 decrease in NDCG is exchanged for an improvement in Gini by 0.218. Hence, we believe that the impact of item replacement is reasonable.

\subsection{Using rank-based fairness measures}

Suppose we have two \textsc{Fair} measures, e.g., set-based Gini (Gini) and rank-based Gini (Gini-w). In \cite{Rampisela2024EvaluationStudy}, the absolute scores of Gini and Gini-w do not differ considerably and the two measures correlate strongly with Kendall's $\tau \geq 0.9$. Hence, generating the PF with rank-based fairness measures such as Gini-w is not expected to result in significantly different conclusions from the set-based version.

\end{document}

