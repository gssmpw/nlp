\section{Related work}
\label{s:prev_work}

Evaluating fairness and relevance together is a type of multi-aspect evaluation. 
However, none of the existing multi-aspect evaluation methods **Dwork et al., "Fairness through Awareness"** can be used in this case as 
these methods require separate labels that are unavailable in RS scenarios. 
Specifically, it is not possible to label an item as `fair', because item fairness depends on other recommended items. The same item can be a fair recommendation in one ranking, but unfair in another. In RSs, fairness is typically defined as treating users or items without discrimination **Calmon et al., "Fairness in Decision-Making"**. This is often quantified as the opportunity for 
having equal relevance (for users) or exposure (for items) 
**Hardt et al., "Equality of Opportunity in Supervised Learning"**, computed either individually or for
groups of items/users **Zafar et al., "Fairness, Accountability and Transparency in Machine Learning"**. 

The problem of jointly evaluating RS relevance and fairness is further aggravated by the fact that improved fairness is often achieved at the expense of relevance to users 
**Dwork et al., "A Formal Approach to Fairness in Algorithmic Decision Making"**. 
We posit that this trade-off makes multi-objective optimization a suitable solution. Pareto optimality is a well-known objective for such optimization, 
and it has been previously used in RS but only to recommend items to users **Kephart et al., "Dynamic Resource Allocation: A Survey of Techniques and Their Applications"**. 
Because the true PF is often unknown due to the problem complexity ____, prior work has used the model's training loss w.r.t.~two different aspects **Lakshminarayanan et al., "The Pitfalls of Probabilistic Interpretability"** or scores from different models **Strumbelj et al., "An Efficient Protocol for Multiobjective Optimization"** to generate the PF. 
Our work differs from this in terms of both the purpose of using Pareto-optimal solutions, and the nature of the PF. Specifically, we exploit Pareto-optimality through PF as a robust \textit{evaluation} method, instead of as a recommendation method. 
In addition, our generated PF is based on the ground truth (i.e., the test set), a common RS evaluation approach, instead of the recommender models' empirical performance, which may not be optimal. Thus, our PF is also model-agnostic, as opposed to the PF in **Zhang et al., "Fairness-Aware Classifier with Preference Learning"**. 
Our approach differs also from FAIR____ since the PF considers the empirically achievable optimal solution based on the dataset, while FAIR compares against the desired fairness distribution which might not be achievable. Lastly, ____ selects the optimal solution based on its distance to the utopia point (the theoretical ideal scores), whereas the utopia point may not be realistic due to dataset or measure characteristics____. Since our PF is generated based on test data, any of its solutions is empirically achievable.