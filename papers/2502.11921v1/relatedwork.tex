\section{Related work}
\label{s:prev_work}

Evaluating fairness and relevance together is a type of multi-aspect evaluation. 
However, none of the existing multi-aspect evaluation methods \cite{Maistro2021PrincipledRankings, Lioma2017EvaluationLists,Palotti2018MM:Engines} can be used in this case as 
these methods require separate labels that are unavailable in RS scenarios. 
Specifically, it is not possible to label an item as `fair', because item fairness depends on other recommended items. The same item can be a fair recommendation in one ranking, but unfair in another. In RSs, fairness is typically defined as treating users or items without discrimination \cite{Biega2018EquityRankings}. This is often quantified as the opportunity for 
having equal relevance (for users) or exposure (for items) 
\cite{Biega2018EquityRankings, Wang2022ProvidingSystems}, computed either individually or for
groups of items/users \cite{Raj2022MeasuringResults, Zehlike2022FairnessSystemsc}. 

The problem of jointly evaluating RS relevance and fairness is further aggravated by the fact that improved fairness is often achieved at the expense of relevance to users \cite{Mehrotra2018TowardsSystems}. 
We posit that this trade-off makes multi-objective optimization a suitable solution. Pareto optimality is a well-known objective for such optimization, 
and it has been previously used in RS but only to recommend items to users \cite{Ribeiro2015MultiobjectiveSystems,  Zheng2022AOptimization, Ge2022TowardLearning, Xu2023P-MMF:System}. 
Because the true PF is often unknown due to the problem complexity \cite{Laszczyk2019SurveyMeasures,Audet2020PerformanceOptimization}, prior work has used the model's training loss w.r.t.~two different aspects \cite{Lin2019ARecommendation} or scores from different models \cite{Nia2022RethinkingNetworks,Paparella2023Post-hocRecommendation} to generate the PF. 
Our work differs from this in terms of both the purpose of using Pareto-optimal solutions, and the nature of the PF. Specifically, we exploit Pareto-optimality through PF as a robust \textit{evaluation} method, instead of as a recommendation method. 
In addition, our generated PF is based on the ground truth (i.e., the test set), a common RS evaluation approach, instead of the recommender models' empirical performance, which may not be optimal. Thus, our PF is also model-agnostic, as opposed to the PF in \cite{Xu2023P-MMF:System}. 
Our approach differs also from FAIR~\cite{Gao2022FAIR:Evaluation} since the PF considers the empirically achievable optimal solution based on the dataset, while FAIR compares against the desired fairness distribution which might not be achievable. Lastly, \cite{Paparella2023Post-hocRecommendation} selects the optimal solution based on its distance to the utopia point (the theoretical ideal scores), whereas the utopia point may not be realistic due to dataset or measure characteristics~\cite{Rampisela2024EvaluationStudy,Moffat2013SevenMetrics}. Since our PF is generated based on test data, any of its solutions is empirically achievable.