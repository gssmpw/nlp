\section{Related Work}
\paragraph{Token-Level Generation:}  
Generating output tokens using LLMs has been widely explored in tasks such as question answering **Vinyals et al., "Pointer Networks"**, summarization **See et al., "Get To The Point"**, and machine translation **Bahdanau et al., "Neural Machine Translation by Jointly Learning to Align and Translate"**. However, using token-level generation for supervised learning tasks like structured prediction remains underexplored. Recent studies **Zhang et al., "Generating Text with Long-range Recurrence Depending on Two Fully Trained RNNs"** show that token-level generation can be more effective than pooled representations by aligning with the LLMs' pre-training objective, leading to better efficiency and robustness against errors.

\paragraph{Mutual Information:}  
Mutual information (MI) helps measure dependencies between features in deep learning **Krishnan et al., "Deep Learning via Semi-Infinite Linear Programming"**. In language models, **Wang et al., "Chain-of-Thought Distillation for Visual Question Answering"** used MI for Chain-of-Thought Distillation, while the MIST framework **Chen et al., "MIST: A Framework for Mutual Information-based Short-Text Clustering"** applied it to short-text clustering. Unlike these works, we use MI to show that token-level generation retains more information than pooled representations.

\paragraph{Mitigating Exposure Bias and Format Mismatch:}  
Exposure bias occurs when an autoregressive model is trained with ground-truth tokens but must rely on its own predictions during inference. Scheduled sampling **Bengio et al., "Scheduled Sampling for Sequence Prediction"** helps reduce this gap. Format mismatch arises when generated tokens do not align with the required structured output. **Chen et al., "Learning to Generate Abstracts from Text"** improved coherence by extracting structured information from LLMs, while **Rao et al., "Structured Output Learning with Deep Conditional Random Fields"** modeled structured outputs as action sequences to preserve dependencies.


Unlike previous works, we extend token-level generation to both regression and classification and provide theoretical and empirical proof of its advantages. We also integrate scheduled sampling with a task adapter to ensure generated tokens match numerical or categorical outputs, addressing exposure bias and format mismatch.