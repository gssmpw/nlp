% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
@article{belghazi2018mine,
  title={Mine: mutual information neural estimation},
  author={Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeswar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, R Devon},
  journal={arXiv preprint arXiv:1801.04062},
  year={2018}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{kenton2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of naacL-HLT},
  volume={1},
  year={2019},
  organization={Minneapolis, Minnesota}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@inproceedings{zhang2020pegasus,
  title={Pegasus: Pre-training with extracted gap-sentences for abstractive summarization},
  author={Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
  booktitle={International conference on machine learning},
  pages={11328--11339},
  year={2020},
  organization={PMLR}
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, P},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@article{yang2019xlnet,
  title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  author={Yang, Zhilin},
  journal={arXiv preprint arXiv:1906.08237},
  year={2019}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan},
  journal={arXiv preprint arXiv:1907.11692},
  volume={364},
  year={2019}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{li2024language,
  title={What Do Language Models Learn in Context? The Structured Task Hypothesis},
  author={Li, Jiaoda and Hou, Yifan and Sachan, Mrinmaya and Cotterell, Ryan},
  journal={arXiv preprint arXiv:2406.04216},
  year={2024}
}

@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3843--3857},
  year={2022}
}

@article{tishby2000information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={2000}
}

@article{beaudry2011intuitive,
  title={An intuitive proof of the data processing inequality},
  author={Beaudry, Normand J and Renner, Renato},
  journal={arXiv preprint arXiv:1107.0740},
  year={2011}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{zhang2023adalora,
  title={AdaLoRA: Adaptive budget allocation for parameter-efficient fine-tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and Karampatziakis, Nikos and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  journal={arXiv preprint arXiv:2303.10512},
  year={2023}
}

@article{kowsher2024rocoft,
  title={RoCoFT: Efficient Finetuning of Large Language Models with Row-Column Updates},
  author={Kowsher, Md and Esmaeilbeig, Tara and Yu, Chun-Nam and Soltanalian, Mojtaba and Yousefi, Niloofar},
  journal={arXiv preprint arXiv:2410.10075},
  year={2024}
}

@article{liu2024dora,
  title={Dora: Weight-decomposed low-rank adaptation},
  author={Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
  journal={arXiv preprint arXiv:2402.09353},
  year={2024}
}

@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}

@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  pages={7432--7439},
  year={2020}
}

@article{sap2019socialiqa,
  title={Socialiqa: Commonsense reasoning about social interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09728},
  year={2019}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}



@article{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}

@inproceedings{marelli2014semeval,
  title={Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment},
  author={Marelli, Marco and Bentivogli, Luisa and Baroni, Marco and Bernardi, Raffaella and Menini, Stefano and Zamparelli, Roberto},
  booktitle={Proceedings of the 8th international workshop on semantic evaluation (SemEval 2014)},
  pages={1--8},
  year={2014}
}

@article{cer2017semeval,
  title={Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation},
  author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, Inigo and Specia, Lucia},
  journal={arXiv preprint arXiv:1708.00055},
  year={2017}
}


@book{thomas2006elements,
  title={Elements of information theory},
  author={Thomas, MTCAJ and Joy, A Thomas},
  year={2006},
  publisher={Wiley-Interscience}
}

@inproceedings{vinayakumar2017deepcybernet,
  title={deepcybernet at emoint-2017: Deep emotion intensities in tweets},
  author={Vinayakumar, R and Premjith, B and Kumar, Sachin and Kp, Soman and Poornachandran, Prabaharan},
  booktitle={Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis},
  pages={259--263},
  year={2017}
}

@inproceedings{marelli-etal-2014-sick,
    title = "A {SICK} cure for the evaluation of compositional distributional semantic models",
    author = "Marelli, Marco  and
      Menini, Stefano  and
      Baroni, Marco  and
      Bentivogli, Luisa  and
      Bernardi, Raffaella  and
      Zamparelli, Roberto",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf",
    pages = "216--223",
}
@article{bengio2015scheduled,
  title={Scheduled sampling for sequence prediction with recurrent neural networks},
  author={Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@article{hu2023llm,
  title={Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models},
  author={Hu, Zhiqiang and Wang, Lei and Lan, Yihuai and Xu, Wanyu and Lim, Ee-Peng and Bing, Lidong and Xu, Xing and Poria, Soujanya and Lee, Roy Ka-Wei},
  journal={arXiv preprint arXiv:2304.01933},
  year={2023}
}
@article{huang2024scalingnote,
  title={ScalingNote: Scaling up Retrievers with Large Language Models for Real-World Dense Retrieval},
  author={Huang, Suyuan and Zhang, Chao and Wu, Yuanyuan and Zhang, Haoxin and Wang, Yuan and Wang, Maolin and Cao, Shaosheng and Xu, Tong and Zhao, Xiangyu and Qin, Zengchang and others},
  journal={arXiv preprint arXiv:2411.15766},
  year={2024}
}
@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}
@article{oh2022don,
  title={Don't Judge a Language Model by Its Last Layer: Contrastive Learning with Layer-Wise Attention Pooling},
  author={Oh, Dongsuk and Kim, Yejin and Lee, Hodong and Huang, H Howie and Lim, Heuiseok},
  journal={arXiv preprint arXiv:2209.05972},
  year={2022}
}

@article{hossain2019president,
  title={" President Vows to Cut< Taxes> Hair": Dataset and Analysis of Creative Text Editing for Humorous Headlines},
  author={Hossain, Nabil and Krumm, John and Gamon, Michael},
  journal={arXiv preprint arXiv:1906.00274},
  year={2019}
}```

@article{crossley2023large,
  title={A large-scaled corpus for assessing text readability},
  author={Crossley, Scott and Heintz, Aron and Choi, Joon Suh and Batchelor, Jordan and Karimi, Mehrnoush and Malatinszky, Agnes},
  journal={Behavior Research Methods},
  volume={55},
  number={2},
  pages={491--507},
  year={2023},
  publisher={Springer}
}

@article{shardlow2020complex,
  title={Complex: A new corpus for lexical complexity prediction from likert scale data},
  author={Shardlow, Matthew and Cooper, Michael and Zampieri, Marcos},
  journal={arXiv preprint arXiv:2003.07008},
  year={2020}
}
@article{mackiewicz1993principal,
  title={Principal components analysis (PCA)},
  author={Ma{\'c}kiewicz, Andrzej and Ratajczak, Waldemar},
  journal={Computers \& Geosciences},
  volume={19},
  number={3},
  pages={303--342},
  year={1993},
  publisher={Elsevier}
}
@article{roy2016solving,
  title={Solving general arithmetic word problems},
  author={Roy, Subhro and Roth, Dan},
  journal={arXiv preprint arXiv:1608.01413},
  year={2016}
}

@article{roy2015reasoning,
  title={Reasoning about quantities in natural language},
  author={Roy, Subhro and Vieira, Tim and Roth, Dan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={3},
  pages={1--13},
  year={2015},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{hosseini2014learning,
  title={Learning to solve arithmetic word problems with verb categorization},
  author={Hosseini, Mohammad Javad and Hajishirzi, Hannaneh and Etzioni, Oren and Kushman, Nate},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={523--533},
  year={2014}
}

@article{koncel2015parsing,
  title={Parsing algebraic word problems into equations},
  author={Koncel-Kedziorski, Rik and Hajishirzi, Hannaneh and Sabharwal, Ashish and Etzioni, Oren and Ang, Siena Dumas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={3},
  pages={585--597},
  year={2015},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{patel2021nlp,
  title={Are NLP models really able to solve simple math word problems?},
  author={Patel, Arkil and Bhattamishra, Satwik and Goyal, Navin},
  journal={arXiv preprint arXiv:2103.07191},
  year={2021}
}

@book{freud1997dora,
  title={Dora: An Analysis of a Case of Hysteria},
  author={Freud, Sigmund},
  year={1997},
  publisher={Simon and Schuster}
}

@inproceedings{ren2024melora,
  title={MELoRA: mini-ensemble low-rank adapters for parameter-efficient fine-tuning},
  author={Ren, Pengjie and Shi, Chengshun and Wu, Shiguang and Zhang, Mengqi and Ren, Zhaochun and Rijke, Maarten and Chen, Zhumin and Pei, Jiahuan},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3052--3064},
  year={2024}
}

@article{zhang2023lora,
  title={Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning},
  author={Zhang, Longteng and Zhang, Lin and Shi, Shaohuai and Chu, Xiaowen and Li, Bo},
  journal={arXiv preprint arXiv:2308.03303},
  year={2023}
}

@article{wu2024mixture,
  title={Mixture-of-Subspaces in Low-Rank Adaptation},
  author={Wu, Taiqiang and Wang, Jiahao and Zhao, Zhe and Wong, Ngai},
  journal={arXiv preprint arXiv:2406.11909},
  year={2024}
}
@article{kowsher2024propulsion,
  title={Propulsion: Steering LLM with Tiny Fine-Tuning},
  author={Kowsher, Md and Prottasha, Nusrat Jahan and Bhat, Prakash},
  journal={arXiv preprint arXiv:2409.10927},
  year={2024}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{peters2018dissecting,
  title={Dissecting contextual word embeddings: Architecture and representation},
  author={Peters, Matthew E and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv preprint arXiv:1808.08949},
  year={2018}
}


@article{ashish2017attention,
  title={Attention is all you need},
  author={Ashish, Vaswani},
  journal={Advances in neural information processing systems},
  volume={30},
  pages={I},
  year={2017}
}

@article{sutskever2014sequence,
  title={Sequence to Sequence Learning with Neural Networks},
  author={Sutskever, I},
  journal={arXiv preprint arXiv:1409.3215},
  year={2014}
}



@article{ran2019numnet,
  title={NumNet: Machine reading comprehension with numerical reasoning},
  author={Ran, Qiu and Lin, Yankai and Li, Peng and Zhou, Jie and Liu, Zhiyuan},
  journal={arXiv preprint arXiv:1910.06701},
  year={2019}
}

@article{press2019improving,
  title={Improving transformer models by reordering their sublayers},
  author={Press, Ofir and Smith, Noah A and Levy, Omer},
  journal={arXiv preprint arXiv:1911.03864},
  year={2019}
}

@article{trask2018neural,
  title={Neural arithmetic logic units},
  author={Trask, Andrew and Hill, Felix and Reed, Scott E and Rae, Jack and Dyer, Chris and Blunsom, Phil},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{dong2019unified,
  title={Unified language model pre-training for natural language understanding and generation},
  author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{tishby2015deep,
  title={Deep learning and the information bottleneck principle},
  author={Tishby, Naftali and Zaslavsky, Noga},
  booktitle={2015 ieee information theory workshop (itw)},
  pages={1--5},
  year={2015},
  organization={IEEE}
}

@article{jain2019attention,
  title={Attention is not explanation},
  author={Jain, Sarthak and Wallace, Byron C},
  journal={arXiv preprint arXiv:1902.10186},
  year={2019}
}
@article{chen2023token,
  title={Token prediction as implicit classification to identify LLM-generated text},
  author={Chen, Yutian and Kang, Hao and Zhai, Vivian and Li, Liangze and Singh, Rita and Raj, Bhiksha},
  journal={arXiv preprint arXiv:2311.08723},
  year={2023}
}
@article{yu2024breaking,
  title={Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling},
  author={Yu, Yao-Ching and Kuo, Chun-Chih and Ye, Ziqi and Chang, Yu-Cheng and Li, Yueh-Se},
  journal={arXiv preprint arXiv:2406.12585},
  year={2024}
}
@inproceedings{shi2023towards,
  title={Towards a Unified Framework for Reference Retrieval and Related Work Generation},
  author={Shi, Zhengliang and Gao, Shen and Zhang, Zhen and Chen, Xiuying and Chen, Zhumin and Ren, Pengjie and Ren, Zhaochun},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={5785--5799},
  year={2023}
}

@article{amin2024private,
  title={Private prediction for large-scale synthetic text generation},
  author={Amin, Kareem and Bie, Alex and Kong, Weiwei and Kurakin, Alexey and Ponomareva, Natalia and Syed, Umar and Terzis, Andreas and Vassilvitskii, Sergei},
  journal={arXiv preprint arXiv:2407.12108},
  year={2024}
}

@inproceedings{lu2017unified,
  title={A unified framework for structured prediction: From theory to practice},
  author={Lu, Wei},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts},
  year={2017}
}
@inproceedings{wang2025extracting,
  title={Extracting structure from an LLM-how to improve on surprisal-based models of Human Language Processing},
  author={Wang, Daphne P and Sadrzadeh, Mehrnoosh and Stanojevi{\'c}, Milo{\v{s}} and Chow, Wing-Yee and Breheny, Richard},
  booktitle={Proceedings of the 31st International Conference on Computational Linguistics},
  pages={4938--4944},
  year={2025}
}

@article{raj2024faster,
  title={Faster Speech-LLaMA Inference with Multi-token Prediction},
  author={Raj, Desh and Keren, Gil and Jia, Junteng and Mahadeokar, Jay and Kalinli, Ozlem},
  journal={arXiv preprint arXiv:2409.08148},
  year={2024}
}
@article{wang2022code4struct,
  title={Code4struct: Code generation for few-shot event structure prediction},
  author={Wang, Xingyao and Li, Sha and Ji, Heng},
  journal={arXiv preprint arXiv:2210.12810},
  year={2022}
}
@article{liu2022autoregressive,
  title={Autoregressive structured prediction with language models},
  author={Liu, Tianyu and Jiang, Yuchen and Monath, Nicholas and Cotterell, Ryan and Sachan, Mrinmaya},
  journal={arXiv preprint arXiv:2210.14698},
  year={2022}
}

@article{chen2024learning,
  title={Learning to Maximize Mutual Information for Chain-of-Thought Distillation},
  author={Chen, Xin and Huang, Hanxian and Gao, Yanjun and Wang, Yi and Zhao, Jishen and Ding, Ke},
  journal={arXiv preprint arXiv:2403.03348},
  year={2024}
}
@book{cover1999elements,
  title={Elements of information theory},
  author={Cover, Thomas M},
  year={1999},
  publisher={John Wiley \& Sons}
}

@inproceedings{covert2023learning,
  title={Learning to maximize mutual information for dynamic feature selection},
  author={Covert, Ian Connick and Qiu, Wei and Lu, Mingyu and Kim, Na Yoon and White, Nathan J and Lee, Su-In},
  booktitle={International Conference on Machine Learning},
  pages={6424--6447},
  year={2023},
  organization={PMLR}
}
@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}
@inproceedings{kamthawee2024mist,
  title={MIST: mutual information maximization for short text clustering},
  author={Kamthawee, Krissanee and Udomcharoenchaikit, Can and Nutanong, Sarana},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={11309--11324},
  year={2024}
}





