\section{Related Work}
\paragraph{Token-Level Generation:}  
Generating output tokens using LLMs has been widely explored in tasks such as question answering \cite{rajpurkar2016squad, yang2019xlnet}, summarization \cite{lewis2019bart, zhang2020pegasus}, and machine translation \cite{vaswani2017attention, wu2016google}. However, using token-level generation for supervised learning tasks like structured prediction remains underexplored. Recent studies \cite{chen2023token, yu2024breaking} show that token-level generation can be more effective than pooled representations by aligning with the LLMs' pre-training objective, leading to better efficiency and robustness against errors.

\paragraph{Mutual Information:}  
Mutual information (MI) helps measure dependencies between features in deep learning \cite{cover1999elements, covert2023learning}. In language models, \citet{chen2024learning} used MI for Chain-of-Thought Distillation, while the MIST framework \cite{kamthawee2024mist} applied it to short-text clustering. Unlike these works, we use MI to show that token-level generation retains more information than pooled representations.

\paragraph{Mitigating Exposure Bias and Format Mismatch:}  
Exposure bias occurs when an autoregressive model is trained with ground-truth tokens but must rely on its own predictions during inference. Scheduled sampling \cite{bengio2015scheduled} helps reduce this gap. Format mismatch arises when generated tokens do not align with the required structured output. \citet{wang2022code4struct} improved coherence by extracting structured information from LLMs, while \citet{liu2022autoregressive} modeled structured outputs as action sequences to preserve dependencies.


Unlike previous works, we extend token-level generation to both regression and classification and provide theoretical and empirical proof of its advantages. We also integrate scheduled sampling with a task adapter to ensure generated tokens match numerical or categorical outputs, addressing exposure bias and format mismatch.