\section{Related Work}
\paragraph{Token-Level Generation:}  
Generating output tokens using LLMs has been widely explored in tasks such as question answering ____, summarization ____, and machine translation ____. However, using token-level generation for supervised learning tasks like structured prediction remains underexplored. Recent studies ____ show that token-level generation can be more effective than pooled representations by aligning with the LLMs' pre-training objective, leading to better efficiency and robustness against errors.

\paragraph{Mutual Information:}  
Mutual information (MI) helps measure dependencies between features in deep learning ____. In language models, ____ used MI for Chain-of-Thought Distillation, while the MIST framework ____ applied it to short-text clustering. Unlike these works, we use MI to show that token-level generation retains more information than pooled representations.

\paragraph{Mitigating Exposure Bias and Format Mismatch:}  
Exposure bias occurs when an autoregressive model is trained with ground-truth tokens but must rely on its own predictions during inference. Scheduled sampling ____ helps reduce this gap. Format mismatch arises when generated tokens do not align with the required structured output. ____ improved coherence by extracting structured information from LLMs, while ____ modeled structured outputs as action sequences to preserve dependencies.


Unlike previous works, we extend token-level generation to both regression and classification and provide theoretical and empirical proof of its advantages. We also integrate scheduled sampling with a task adapter to ensure generated tokens match numerical or categorical outputs, addressing exposure bias and format mismatch.