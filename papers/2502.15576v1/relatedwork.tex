\section{Related Works}
\vspace{-0.25cm}
Modern large language models have shown promising text-generation abilities, prompting researchers to explore their internal mechanisms. 
One approach~\citep{belinkov2018evaluating,jawahar2019does,rogers2021primer} develops contrastive datasets to probe hidden states for specific features, but it is limited by the polysemantic nature of neurons~\citep{elhage2022toy,olah2020zoom}, making the explanations non-concise and difficult to apply in downstream tasks. To overcome this, researchers~\citep{brickentowards} propose learning orthogonal basis vectors to better understand LLMs. Early works~\citep{Beren2022SVD,wu2024language} applied singular vector analysis to identify concise, interpretable directions in neuron activations. Soon after, sparse autoencoders~\citep{brickentowards,cunningham2023sparse} were introduced, allowing for more flexible settings. 
Sparse autoencoders, initially used to analyze image data~\citep{olshausen1997sparse,makhzani2013k}, are now being applied to LLMs. Researchers from Anthropic~\citep{brickentowards} and EleutherAI~\citep{cunningham2023sparse} demonstrated that activations from smaller models like GPT-2 and Pythia yield highly interpretable features. Subsequent studies showed these features help interpret model behaviors in tasks like indirect object identification~\citep{makelov2024sparse}, translation~\citep{dumas2024llamas}, and circuit detection~\citep{marks2024sparse}. Recent works~\citep{templeton2024scaling,gao2024scaling,lieberum2024gemma} confirm this technique's success with larger LLMs. 
Our study follows this path, and advances by developing a method for generating discourse-level explanations to steer LLM representations.

\vspace{-0.1cm}