\begin{figure}[tb]
  \centering
  \includegraphics[scale=0.65]{Fig/Fig1.pdf}
  \vspace{-0.5cm}
  \caption{ Fig.(a) shows the general pipeline of existing non-blind video inpainting. 
  Such pipeline require manual annotation of corrupted regions of each frame, limiting its application scope. 
  In this paper, we formulate a new task: blind video inpainting, which can directly learn a mapping from corrupted video to inpainted result without any corrupted region annotation (Fig.(b)).
  Fig.(c) shows an example of our blind video inpainting method in scratch restoration and bullet removal.}
  \label{Fig1}
  \vspace{-0.5cm}
\end{figure}
\section{Introduction}
\label{Introduction}
Video inpainting is a fundamental visual restoration task in computer vision, aiming to fill corrupted regions in videos with plausible and coherent contents~\cite{Wu_2023_CVPR,9967838,Wu_2023_CVPR1}. 
Existing video inpainting methods~\cite{cai2022devit,liu2021fuseformer,Ren_2022_CVPR,yan2020sttn,zhang2022flow,cherel2023infusion,yu2023deficiency,Wu_2023_CVPR,Zheng_2023_ICCV} 
typically take corrupted videos along with masks indicating corrupted regions as input, and generate corrupted contents based on spatial texture and temporal information of valid (uncorrupted) regions.
This means that these methods require manual annotation of corrupted regions using binary masks to indicate “where to inpaint”.
We refer to this case as a \emph{non-blind setting}.
Nevertheless, accurate mask annotations are not available in many scenarios.
On the one hand, the boundary between corrupted regions and valid regions is often blurred, making it unrealistic to accurately annotate corrupted regions. 
On the other hand, manual annotation is labor-intensive and resource-consuming, especially for corrupted videos with high frame rates and high resolutions.

In this paper, we formulate a novel task: \textbf{\emph{blind video inpainting}}, where a mapping from corrupted videos to inpainted results is directly learned without any corrupted region annotation.
Unlike the non-blind setting, which only focuses on “how to inpaint”, our proposed blind video inpainting setting requires considering both “where to inpaint” and “how to inpaint” simultaneously. 
This blind video inpainting setting is more effective in real-world applications.
In fact, the “corruption” faced by video inpainting can be divided into two types.
The first type of “corruption” is not present in the original video.
This type of “corruption” is caused by external factors and destroys the original structure of the video, such as scratches~\cite{chang2019free,wu2023flow}, watermarks~\cite{9967838,10222097}, and bullets~\cite{Kim_2019_CVPR}, etc.
Another type of “corruption” exists in the original video itself, such as undesired objects and occlusions~\cite{9010390,10447901}. 
In this paper, we mainly focus on the first type of “corruption”.

For blind video inpainting task, a naive solution is to process corrupted video frame by frame using existing blind image inpainting methods~\cite{schmalfuss2022blind,wang2020vcnet,10147235}. 
However, such a solution will neglect the motion continuity between frames, resulting in flicker artifacts in the completed video.
In this paper, we propose an end-to-end blind video inpainting framework consisting of a mask prediction network and a video completion network.
The former aims to estimate the corrupted regions that need to be completed, while the latter focuses on completing the corrupted video.
Specifically, we first develop a mask prediction network to predict the corrupted regions of whole video by detecting semantic-discontinuous regions of the frame and utilizing temporal consistency prior of the video.
Then, we design a video completion network to perceive valid context information from uncorrupted regions using predicted mask to generate corrupted contents.
To precisely capture the accurate correspondence between the mask prediction network and video completion network, we introduce a consistency loss to regularize their parameters, enabling mutual constraint and maximizing the overall model performance.

Furthermore, existing video inpainting datasets~\cite{lee2019cpnet,chang2019free,yan2020sttn,Kang2022ErrorCF} usually introduce specific prior knowledge during the construction process, such as corrupted content, clear border, and fixed shape.
These priors make corrupted regions easily distinguishable from natural video frame by the mask prediction network, failing to realistically simulate the complex real-world scenarios in blind video inpainting.
In this paper, we exploit free-form strokes~\cite{chang2019free} as corrupted regions, and fill these regions with natural images as corrupted contents. 
Such operations allow us to construct the corrupted contents with variable shapes, complex motions and faithful texture.
To avoid the introducing border priors, we also utilize iterative Gaussian smoothing~\cite{wang2018image} to extend the edges of corrupted regions, making these boundaries difficult to distinguish.
Such strategy can enforce the network to infer the corrupted regions by the semantic context of the video frame rather than merely fitting to the training data.
Further, to enhance the generalization ability of our model in practical applications, we collect a bullet removal dataset consisting of 1,250 video clips. 
Extensive experimental results demonstrate that our method can achieve comparable performance to non-blind baselines. 
An example result of our method in scratch restoration and bullets removal is shown in Fig.\ref{Fig1}(c).

To sum up, our contributions are summarized as follows:
\begin{itemize}

\item A novel blind video inpainting task is formulated. It directly learns a mapping from corrupted videos to inpainted results without any corrupted region annotation. To the best of our knowledge, this is the first blind inpainting work in the field of video inpainting.
  
\item An end-to-end blind video inpainting framework consisting of a mask prediction network and a video completion network is designed, and a consistency loss is introduced to regularize the training parameters of this framework.
  
\item A dataset suitable for blind video inpainting task is customized. This dataset consists of $2,400$ synthesized video clips and $1,250$ real-world video clips, and will be published to facilitate subsequent research. 
\end{itemize}










