\section{Proposed Method}
\label{Method}
\subsection{Problem Formulation}
\label{Problem Formulation}
Given a corrupted video sequence $\textbf{\emph{X}}=\{\textbf{\emph{x}}_1,\textbf{\emph{x}}_2,\dots,\textbf{\emph{x}}_T\}$ with sequence length $T$. 
The corrupted regions of video sequence $X$ are denoted as a binary mask $\textbf{\emph{M}}=\{\textbf{\emph{m}}_1,\textbf{\emph{m}}_2, \dots,\\
\textbf{\emph{m}}_T\}$. 
For each mask $\textbf{\emph{m}}_i$, “0” indicates that corresponding pixel is corrupted, and “1” denotes the valid regions.  
The goal of video inpainting is to generate a complete video sequence $\widehat{\textbf{\emph{Y}}}=\{\widehat{\textbf{\emph{y}}}_1,\widehat{\textbf{\emph{y}}}_2 ,\dots,\widehat{\textbf{\emph{y}}}_T\}$, which should be spatially and temporally consistent with the ground truth $\textbf{\emph{Y}}=\{\textbf{\emph{y}}_1, \textbf{\emph{y}}_2,\dots, \textbf{\emph{y}}_T\}$. Formally, existing video inpainting methods~\cite{cai2022devit,liu2021fuseformer,Ren_2022_CVPR,yan2020sttn,zhang2022flow} aim
to model the conditional distribution $p(\widehat{\textbf{\emph{Y}}}|\textbf{\emph{X}},\textbf{\emph{M}})$ by training a deep neural network $\mathcal{D}$, \emph{i.e.}, $\widehat{\textbf{\emph{Y}}}=\mathcal{D}(\textbf{\emph{X}},\textbf{\emph{M}})$. 
Nevertheless, these methods are only applicable
when the corrupted region mask $\textbf{\emph{m}}_i$ of each video frame $\textbf{\emph{x}}_i$ is available. 
In practice,
accurate mask of corrupted regions is difficult to manually annotate due to the blurred boundary between the corrupted regions and the valid regions.
Furthermore, collecting these manual annotation mask is not only labor-intensive and time-consuming,
but also often subjective and error-prone.


Different from existing video inpainting~\cite{cai2022devit,liu2021fuseformer,Ren_2022_CVPR,yan2020sttn,zhang2022flow}, 
we focus on video inpainting without any manual annotation mask of corrupted regions,
called \emph{\textbf{blind video inpainting}}.
This task aims to learning a direct mapping {$\mathcal{G}$} from the corrupted video $\textbf{\emph{X}}$ to the completed video $\widehat{\textbf{\emph{Y}}}$ 
\emph{i.e.}, $\widehat{\textbf{\emph{Y}}}=\mathcal{G}(\textbf{\emph{X}})$.
It means that the mapping $\mathcal{G}$ needs not only to automatically locate the corrupted regions, but also complete them.
Towards this end,
we decompose the blind video inpainting task into two sub-tasks: \emph{mask prediction} and \emph{video completion}.
The former aims to estimate the corrupted regions of video, indicating “where to inpaint”, while the latter focus to complete the corrupted regions of video, addressing “how to inpaint”.
The dual sub-tasks are defined as follows:
\begin{itemize}
    \item \textbf{mask prediction}: 
    given a corrupted video sequence $\textbf{\emph{X}}$, it learns the mapping $\bm\Phi$ to predict the corrupted region masks $\textbf{\emph{M}}$, \emph{i.e.}, $\textbf{\emph{M}} = \bm \Phi(\textbf{\emph{X}})$;
    
    \item \textbf{video completion}:
    based on the predicted region masks $\textbf{\emph{M}}$,
    it learns the mapping $\bm\Psi$ to generate the completed video sequence $\widehat{\textbf{\emph{Y}}}$, \emph{i.e.}, 
    $\widehat{\textbf{\emph{Y}}} = \bm \Psi(\textbf{\emph{X}},\textbf{\emph{M}})$.
\end{itemize}

\subsection{Network Design}
\label{Network Design}

In this paper, we design an end-to-end trainable framework to tackle blind video inpainting task. 
As shown in Fig.\ref{Fig_KT}(a), our framework consists of a mask prediction network and a video completion network.
The former aims to predict the corrupted regions of entire video by detecting semantic-discontinuous regions of the frame and utilizing temporal consistency prior of the video, while the latter perceive valid context information from uncorrupted regions using predicted masks to generate corrupted contents.

\subsubsection{Mask Prediction Network}
As shown in Fig.\ref{Fig_KT}(a), 
our mask prediction network sub-network consists of a short-term prediction module and a long-term refinement module.
The short-term prediction module takes the individual video frame $\textbf{\emph{x}}_i$ as input to predict the binary mask $\textbf{\emph{m}}_i^s$ of visual inconsistency regions, thus providing initial input for the long-term refinement module.
The long-term refinement module is used to refine $T$ predicted masks
$\textbf{\emph{M}}^s = \{\textbf{\emph{m}}_1^s, \textbf{\emph{m}}_2^s, \dots, \textbf{\emph{m}}_T^s\}$
using the temporal consistency priors of videos.

\noindent\textbf{Short-Term Prediction Module}.
The short-term prediction module is designed as an encoder-decoder structure. 
Specifically, the encoder consists of three stages, which generate feature maps at three different scales.
All stages share a similar structure, which consists of multiple convolutional layers and residual blocks with ReLU activation functions. 
In contrast, the decoder is used to decode the learned features into the binary masks, and its architecture is similar to that of the encoder.
Furthermore, researchers have demonstrated that noise can significantly reduce image segmentation performance~\cite{9156335,9508165}. To enhance the noise immunity of CNN, Li et al.~\cite{9156335,9508165} replaced the max-pooling, strided-convolution, and average-pooling in CNN with DWT, thereby achieving higher image segmentation accuracy. 
Motivated by such observation, we employ DWT to perform the down-sampling operation in the short-term prediction module. 
In this way, the short-term prediction module can obtain a more accurate mask of corrupted regions.
Formally, the entire mask prediction process can be expressed as:
\begin{equation}
\textbf{\emph{m}}_i^s=STP(\textbf{\emph{x}}_i),
\label{STPM}
\end{equation}
where 
$STP$ denotes short-term prediction module.
$\textbf{\emph{m}}_i^s$ is the predicted mask of the $i$-th video frame $\textbf{\emph{x}}_i$.

\noindent\textbf{Long-Term Refinement Module}.
The long-term refinement module consists of three main parts: encoder, sequence-to-sequence transformer, and decoder, 
where the encoder and decoder have the same backbone as the encoder and decoder of the short-term refinement module.
Sequence-to-sequence transformer is the core component of the long-term refinement module. 
It aims to refine $\textbf{\emph{M}}^s = \{\textbf{\emph{m}}_1^s, \textbf{\emph{m}}_2^s, \dots, \textbf{\emph{m}}_T^s\}$ using the temporal consistency priors.



As shown in Fig.\ref{Fig_KT}(b), for deep features $\textbf{\emph{E}}=\{\textbf{\emph{e}}_1, \textbf{\emph{e}}_2, \cdots,\textbf{\emph{e}}_T\}$ extracted with encoder through cascading the corrupted video sequence $\textbf{\emph{X}}$ and mask $\textbf{\emph{M}}^s$, 
we first project them
into query (${\widetilde{\textbf{\emph{Q}}}}$), key (${\widetilde{\textbf{\emph{K}}}}$) and value (${\widetilde{\textbf{\emph{V}}}}$) using a $1\times1\times1$ convolutional layer.
After obtaining ${\textbf{\emph{Q}}}$, ${\textbf{\emph{K}}}$, and ${\textbf{\emph{V}}}$, they are split into $N$ groups \{$\widetilde{ \textbf{\emph{Q}}}_n, \widetilde{\textbf{\emph{K}}}_n, \widetilde{\textbf{\emph{V}}}_n\} \in \mathbb{R}^{T\times{H}\times{W}\times{\frac{C}{N}}}$
along the channel dimension, 
where $n \in \{1,2,\cdots,N\}$. 

Then, we measure the relevance between $\widetilde{\textbf{\emph{Q}}}_n$ and $\widetilde{\textbf{\emph{K}}}_n$ to extract the spatial-temporal relationship between video frames.
However, the positions of corrupted regions in adjacent frames are similar~\cite{gu2020pyramid,ji2021progressively}. Therefore, we use the surrounding neighborhood obtained from a response window to compute a spatial-temporal affinity matrix ${\textbf{\emph{H}}}_n$ of the target pixel, instead of computing the response between a query position and the features at all positions. 
Such a relevance measurement captures more relevance related to the target regions within $T$ frames. 

Having the affinity matrix ${\textbf{\emph{H}}}_i$, we calculate the spatial-temporally aggregated features ${\textbf{\emph{D}}}_n$ within the surrounding neighborhood by matrix multiplication between the response window of $\widetilde{\textbf{\emph{V}}}_n$ and the affinity matrix ${\textbf{\emph{H}}}_n$. 
Finally, a soft-attention block is used to synthesize features from the group of affinity matrix ${\textbf{\emph{H}}}_i$ and aggregated features ${\textbf{\emph{D}}}_i$. During the synthesis process, relevant spatial-temporal patterns should be enhanced while less relevant ones should be suppressed. To achieve that, a soft-attention map ${\textbf{\emph{G}}}$ is generated by taking the channel-wise maximum value on ${\textbf{\emph{H}}}$, which is obtained by concatenating a group of the affinity matrix ${\textbf{\emph{H}}}_i$ along the channel dimension.
In summary, the synthesized features $\widehat{\textbf{\emph{E}}}$ are calculated as follows:
\begin{equation}
\widehat{\textbf{\emph{E}}}=\textbf{\emph{E}}+Conv(\textbf{\emph{D}})\odot\textbf{\emph{G}},
\label{Inpainted}
\end{equation}
where $Conv$ and $\odot$ represent the convolutional and element-wise multiplication operation, respectively.
$\textbf{\emph{D}}$ is obtained by concatenating a group of the spatial-temporally aggregated features ${\textbf{\emph{D}}}_i$ along the channel dimension.


\subsubsection{Video Completion Network}
\label{Video Completion Network}
Video completion network aims to perceive valid context information from uncorrupted regions using predicted mask to generate corrupted contents, which is in line with the goal of video inpainting task in non-blind settings. 
Recently, benefiting from the advantages of long-range feature capture capacity, 
transformer-based video inpainting methods~\cite{cai2022devit,liu2021fuseformer,Ren_2022_CVPR,yan2020sttn,zhang2022flow}
have achieved unprecedented performance in non-blind settings.
These methods typically retrieve relevant contents to fill the corrupted regions by a self-attention mechanism.
However, they still suffer from two major limitations. 
Firstly, these methods always aggregate the features using all attention relations based on query-key pairs to generate the corrupted contents. 
Such a aggregation will leads to the redundant or irrelevant contents being filled into the corrupted regions, causing blurry or compromised results~\cite{chen2023learning,zhou2024adapt}. 
Secondly, these methods typically utilize the whole features to calculate self-attention.
Such a setting ignores the impact of noise in the features on attention calculation, significantly reducing the accuracy of attention retrieval~\cite{wu2024waveformer}.
Based on this, we design a wavelet sparse transformer as our video completion network to generate corrupted contents.

As shown in Fig.\ref{Fig_KT}(c), for the features $\textbf{\emph{F}}=\{\textbf{\emph{f}}_1, \textbf{\emph{f}}_2, \cdots,\textbf{\emph{f}}_T\}$ extracted by encoder, we first map each $\textbf{\emph{f}}_i\in\mathbb{R}^{{h}\times{{w}}\times{c}}$ into query ($\textbf{\emph{Q}}_i$), key ($\textbf{\emph{K}}_i$), and value ($\textbf{\emph{V}}_i$) to establish deep correspondences for each region in different semantic spaces. 
Having $\textbf{\emph{Q}}_i$, $\textbf{\emph{K}}_i$, and $\textbf{\emph{V}}_i$, we decompose them into different frequencies by Discrete Wavelet Transform (DWT)~\cite{1989A}.
Such a strategy can isolate the noise into the high-frequency components $\textbf{\emph{Q}}_i^H, \textbf{\emph{K}}_i^H, \textbf{\emph{V}}_i^H \in \mathbb{R}^{3\times{\frac{h}{2}}\times{{\frac{w}{2}}}\times{c}}$,  allowing the low-frequency components $\textbf{\emph{Q}}_i^L, \textbf{\emph{K}}_i^L, \textbf{\emph{V}}_i^L\in\mathbb{R}^{{\frac{h}{2}}\times{{\frac{w}{2}}}\times{c}}$ only contain relatively clean basic features.

After obtaining all low-frequency components, we use them to calculate standard dense self-attention (DSA):
\begin{equation}
DSA=Softmax\left(\frac{\textbf{\emph{Q}}^L\cdot{(\textbf{\emph{K}}^L)^T}}{\sqrt{d}}+\textbf{\emph{B}}\right),
\label{SSS}
\end{equation}
where $Softmax$ denotes the softmax layer. $\textbf{\emph{B}}$ refers to the learnable relative positional bias. Since not all query tokens are closely relevant to corresponding ones in keys, the utilization of all similarities is ineffective for generation corrupted contents. Instead, it leads to the redundant or irrelevant contents being filled into the corrupted regions, causing blurry or compromised results. 

Intuitively, developing a sparse self-attention (SSA) mechanism to aggregate the most relevant features could improve inpainting performance. 
Inspired by adaptive sparse self-attention~\cite{zhou2024adapt}, we utilize ReLu to remove negative similarities, preserving the most significant contents:
\begin{equation}
SSA=Softmax\left(ReLu\left(\frac{\textbf{\emph{Q}}^L\cdot{(\textbf{\emph{K}}^L)^T}}{\sqrt{d}}\right)+\textbf{\emph{B}}\right),
\label{SSS}
\end{equation}
where $ReLu$ denotes the ReLu layer. 
Note that simply using $SSA$ will impose over sparsity, making the learned feature representation insufficient to produce the complete corrupted contents. 
Conversely, using $DSA$ will inadvertently introduce irrelevant features into corrupted regions, leading to blurry results.
Therefore, we design a two-branch self-attention mechanism to calculate attention, maximizing the advantages of both two paradigms. 
The completed features of different frequencies can obtained by a shared attention manner:
\begin{equation}
\begin{split}
\widehat{\textbf{\emph{V}}}^L=&(\omega_1\odot DSA+\omega_2\odot SSA)\textbf{\emph{V}}^L,\\
\widehat{\textbf{\emph{V}}}^H=&(\omega_1\odot DSA+\omega_2\odot SSA)\textbf{\emph{V}}^H,
\label{SSS}
\end{split}
\end{equation}
where $\omega_1$ and $\omega_2$ are two normalized weights for adaptively modulating two-branch. 
$\widehat{\textbf{\emph{V}}}^L$ and $\widehat{\textbf{\emph{V}}}^H$ denote the completed low-frequency components and completed high-frequency components, respectively.  
In this way, the negative impact of noise on attention calculation can be significantly mitigated, thereby achieving the goal of improving the inpainting performance.
Finally, we utilize Inverse Discrete Wavelet Transform (IDWT) to obtain the final completed features $\widehat{\textbf{\emph{V}}}$.
Note that we only borrow information from valid regions to generate corrupted contents. 
Therefore, we set the $DSA$ and $SSA$ in corrupted regions to 0.



\subsection{Consistency Constraints}
\label{Consistency Constraints}
In our framework, mask prediction network and video completion network are closely correlated and mutually constrained.
The former helps the latter to locate the corrupted regions, while the latter regularizes the the former by the reconstruction loss, enforcing it to focus on corrupted regions. 

Ideally, if mask prediction network and video completion network both can capture accurate correspondence, the difference between the corrupted video frame $\textbf{\emph{x}}_i$ and the completed result $\widehat{\textbf{\emph{y}}}_i$ should exist only in the corrupted regions. 
This means that the following relationship holds:
\begin{equation}
\textbf{\emph{m}}_i=\mathcal{B}(\widehat{\textbf{\emph{y}}}_i-\textbf{\emph{x}}_i),
\label{STPM}
\end{equation}
\begin{equation}
\textbf{\emph{m}}_i^l=\mathcal{B}(\widehat{\textbf{\emph{y}}}_i-\textbf{\emph{x}}_i),
\label{STPM1}
\end{equation}
where $\mathcal{B}$ denotes binarization operation. $\textbf{\emph{m}}_i^{l}$ and $\textbf{\emph{m}}_i$ are the mask obtained by mask prediction network and ground truth mask, respectively. 


Eq.(\ref{STPM}) and Eq.(\ref{STPM1}) give us the solution to further constraint mask prediction network and video completion network by a consistency loss. The consistency loss $\mathcal{{L}}_{c}$ is formulated as follows:
\begin{equation}
\mathcal{{L}}_{c}={\parallel{\textbf{\emph{m}}_i^l-\mathcal{B}(\widehat{\textbf{\emph{y}}}_i-\textbf{\emph{x}}_i)}\parallel_1+\parallel{\textbf{\emph{m}}_i-\mathcal{B}(\widehat{\textbf{\emph{y}}}_i-\textbf{\emph{x}}_i)}\parallel_1}.
\label{L-cycle_Y}
\end{equation}


\subsection{Loss Functions}
\label{Loss Functions}
We train our network by minimizing the following loss:
\begin{equation}
\begin{aligned}
\mathcal{{L}}=\lambda_{m}\mathcal{{L}}_{m}+\lambda_{v}\mathcal{{L}}_{v}+\lambda_{c}\mathcal{{L}}_{c},
\label{L-total}
\end{aligned}
\end{equation}
where $\mathcal{{L}}_{m}$, $\mathcal{{L}}_{v}$, and $\mathcal{{L}}_{c}$ denote
mask prediction loss, video completion loss and consistency loss, respectively. 
$\lambda_{m}$, $\lambda_{v}$ and $\lambda_{c}$ are non-negative trade-off parameters. 
In our experiments, 
$\lambda_{m}$, $\lambda_{v}$ and $\lambda_{c}$ are set to $3$, $5$ and $0.02$, determined by grid searching. More details of our framework and loss function can be found in the \textcolor{magenta}{supplementary materials}.

\begin{table*}[!t]
%\resize box
\caption{Quantitative results of video inpainting on YouTube-VOS and DAVIS datasets. The term $\emph{Blind}$ denotes `Blind setting' for short.}
  \centering
\small
  %\renewcommand\tabcolsep{0.2pt}
  %\renewcommand\arraystretch{1}
    \begin{tabular}{c||c||c|c|c|c||c|c|c|c}
    \hline
    \hline
     ~   &   & \multicolumn{4}{c||}{\textbf{YouTube-VOS}}      & \multicolumn{4}{c}{\textbf{DAVIS}}  \\
     \cline{3-10}  \multirow{-2}{*}{ \textbf{Methods}} & \multirow{-2}{*}{ \textbf{Blind}}
       &  PSNR$\uparrow$ &  SSIM$\uparrow$ &   $E_{warp}\downarrow$  &  LPIPS$\downarrow$  &   PSNR$\uparrow$ &  SSIM$\uparrow$ &   $E_{warp}\downarrow$  &  LPIPS$\downarrow$  \\
    \hline
    \hline
    TCCDS~\cite{huang2016temporally} &\usym{1F5F4} &23.418&0.8119 &0.3388&1.9372 &28.146&0.8826   &0.2409&1.0079  \\
    \hline
    VINet~\cite{Kim_2019_CVPR,8931251} &\usym{1F5F4} &26.174 &0.8502 &0.1694 &1.0706 &29.149 &0.8965 &0.1846 &0.7262 \\
    \hline
    DFVI~\cite{xu2019deep}  &\usym{1F5F4} &28.672 &0.8706 &0.1479 &0.6285 &30.448&0.8961    &0.1640&0.6857 \\
    \hline
    FGVC~\cite{Gao-ECCV-FGVC} &\usym{1F5F4} &24.244&0.8114 &0.2484&1.5884&28.936&0.8852   &0.2122&0.9598 \\
    \hline
    CPVINet~\cite{lee2019cpnet} &\usym{1F5F4} &28.534    &0.8798&0.1613&0.8126&30.234&0.8997 &0.1892&0.6560  \\
    \hline
    STTN~\cite{yan2020sttn} &\usym{1F5F4} &28.993&0.8761 &0.1523&0.6965&28.891&0.8719   &0.1844&0.8683 \\
    \hline
    FuseFormer~\cite{liu2021fuseformer} &\usym{1F5F4} &29.765&0.8876 &0.1463&0.5481&29.627&0.8852&0.1767&0.6706 \\
    \hline 
    E2FGVI~\cite{li2022towards} &\usym{1F5F4} &30.064&0.9004&0.1490&0.5321  &31.941&0.9188&0.1579&{0.6344} \\
    \hline
    FGT~\cite{zhang2022flow} &\usym{1F5F4} &30.811  &0.9258&0.1308&0.4565  &32.742&0.9272 &0.1669&0.4240 \\
    \hline
     ProPainter~\cite{zhou2023propainter}   &\usym{1F5F4} &29.906    & 0.9050   &0.1458    & 0.4962   & 31.967   &0.9250    &0.1655    &0.4370  \\
     \hline
     WaveFormer~\cite{wu2024waveformer} &\usym{1F5F4} &{33.264}&{0.9435}&{0.1184}&{0.2933}&{34.169}&{0.9475}&{0.1504}&{0.3137} \\
     \hline
    \textbf{VCNet~(Ours)} &\usym{1F5F4} &\textbf{34.107}&\textbf{0.9521}&\textbf{0.1102}&\textbf{0.2145}&\textbf{34.936}&\textbf{0.9561}&\textbf{0.1362}&\textbf{0.2706} \\
    \hline
    \hline
    MPNet+CPVINet &\usym{2714}  &25.206   &0.8115   &0.2045   &1.2269   &26.024   &0.8296   &0.2178  &1.2543\\
    \hline
    MPNet+STTN &\usym{2714}  &25.049  &0.8295   &0.2032   &1.6490  &25.009   &0.8505   &0.2882  &1.4868\\
    \hline
    MPNet+FuseFormer &\usym{2714}  &25.269  &0.8302  &0.2062   &1.6772   &24.752  &0.8260  &0.2887  &1.4351\\
    \hline
    MPNet+E2FGVI &\usym{2714}  &26.422   &0.8368  &0.1832   &1.1145 & 27.318  &0.8536   &0.1871  &0.9523\\
    \hline
    MPNet+FGT &\usym{2714}  &27.032   &0.8755  &0.1609   &0.8667 & 28.387  &0.8963   &0.1759  &0.7392\\
    \hline
    MPNet+WaveFormer &\usym{2714}  &29.185   &0.8902  &0.1508   &0.7153 & 29.794  &0.9016   &0.1607  &0.7669\\
    \hline
    \textbf{Ours} &\usym{2714} &\textbf{30.528}&\textbf{0.9088}&\textbf{0.1362}&\textbf{0.6556}&\textbf{30.961}&\textbf{0.9107}&\textbf{0.1565}&\textbf{0.7338} \\
    \hline
    \hline
    \end{tabular}
  \label{tab_Q}
  \vspace{-0.2cm}
\end{table*}

