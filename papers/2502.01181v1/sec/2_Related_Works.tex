\section{Related Works}
\label{Related Work}

\noindent\textbf{Non-Blind Video Inpainting.}
% \subsection{Video Inpainting}
% With the rapid development of deep learning, video inpainting has made great progress. 
The video inpainting methods can be roughly divided into three lines: 3D convolution-based~\cite{chang2019free,Kim_2019_CVPR,9558783}, flow-based~\cite{Gao-ECCV-FGVC,Kang2022ErrorCF,Ke2021OcclusionAwareVO,li2022towards,xu2019deep,Zhang_2022_CVPR,zou2020progressive}, and attention-based methods~\cite{cai2022devit,lee2019cpnet,Li2020ShortTermAL,liu2021fuseformer,Ren_2022_CVPR,9010390,srinivasan2021spatial,yan2020sttn,zhang2022flow}. 

%\noindent\textbf{3D convolution-based methods.}
The methods~\cite{chang2019free,Kim_2019_CVPR,9558783} based on 3D convolution usually reconstruct the corrupted contents by directly aggregating complementary information in a local temporal window through 3D temporal convolution. 
%For example,
%Wang et al.~\cite{wang2018video} proposed the first deep learning-based video inpainting network using a 3D encoder-decoder network.
%Further,
%Kim et al.~\cite{Kim_2019_CVPR} aggregated the temporal information of the neighbor frames into missing regions of the target frame by a recurrent 3D-2D feed-forward network.
Nevertheless, they often yield temporally inconsistent completed results due to the limited temporal receptive fields.
%\noindent\textbf{Flow-based methods.} 
The methods~\cite{Gao-ECCV-FGVC,Kang2022ErrorCF,Ke2021OcclusionAwareVO,li2022towards,xu2019deep,Zhang_2022_CVPR,zou2020progressive} based on optical flow treat the video inpainting as a pixel propagation problem. 
Generally, they first introduce a deep flow completion network to complete the optical flow, and then utilize the completed flow to guide the valid pixels into the corrupted regions. 
%For instance, 
%Xu et al.~\cite{xu2019deep} used the flow field completed by a coarse-to-fine deep flow completion network to capture the correspondence between the valid regions and the corrupted regions, and guide relevant pixels into the corrupted regions. 
%Based on this, 
%Gao et al.~\cite{Gao-ECCV-FGVC} further improved the performance of video inpainting by explicitly completing the flow edges. 
%Zou et al.~\cite{zou2020progressive} corrected the spatial misalignment in the temporal feature propagation stage by the completed optical flow. 
However, these methods fail to capture the visible contents of long-distance frames, thus reducing the inpainting performance in the scene of large objects and slowly moving objects. 


%\noindent\textbf{Attention-based methods.} 
Due to its outstanding long-range modeling capacity, attention-based methods, especially transformer-based methods, have shed light on the video inpainting community. 
These methods~\cite{cai2022devit,lee2019cpnet,Li2020ShortTermAL,liu2021fuseformer,Ren_2022_CVPR,9010390,srinivasan2021spatial,yan2020sttn,zhang2022flow} first find the most relevant pixels in the video frame with the corrupted regions by the attention module, and then aggregate them to complete the video frame. 
%For example,
%Zeng et al.~\cite{yan2020sttn} Ô¨Ålled the missing regions of multi-frames simultaneously by learning a spatial-temporal transformer network. 
%Further, Liu et al.~\cite{liu2021fuseformer} improved edge details of missing contents by novel soft split and soft composition operations.
Although the existing video inpainting methods have shown promising results, 
%they usually assume that the corrupted regions of the video are known, 
they usually need to elaborately annotate the corrupted regions of each frame in the video,
limiting its application scope.
Unlike these approaches, we propose a blind video inpainting network in this paper, which can automatically identify and complete the corrupted regions in the video.


\begin{figure*}[tb]
\centering%height=3.0cm,width=15.5cm
\includegraphics[scale=0.66]{Fig/Fig_KT.pdf}
\vspace{-0.15cm}
\caption{\textbf{The overview of the proposed blind video inpainting framework}. Our framework are composed of a mask prediction network (MPNet) and a video completion network (VCNet). The former aims to predict the masks of corrupted regions by detecting semantic-discontinuous regions of the frame and utilizing temporal consistency prior of the video, while the latter perceive valid context information from uncorrupted regions using predicted mask to generate corrupted contents.
}
\label{Fig_KT}
\vspace{-0.5cm}
\end{figure*}

\noindent\textbf{Blind Image Inpainting.}
% \subsection{Blind Image Inpainting}
% \subsection{Image Inpainting}
In contrast to video inpainting, image inpainting solely requires consideration of the spatial consistency of the inpainted results. In the few years, the success of deep learning has brought new opportunities to many vision tasks, which promoted the development of a large number of deep learning-based image inpainting methods~\cite{shamsolmoali2023transinpaint,dong2022incremental,liu2022reduce,li2022misf,cao2022learning}. 
As a sub-task of image inpainting, blind image inpainting~\cite{wang2020vcnet,zhao2022transcnn,li2024semid,li2023decontamination,10147235} has been preliminarily explored. 
For example,
Nian et al.~\cite{cai2017blind} proposed a novel blind inpainting method based on a fully convolutional neural network. 
Liu et al.~\cite{BII} designed a deep CNN to directly restore a clear image from a corrupted input. However, these blind inpainting work assumes that the corrupted regions are filled with constant values or Gaussian noise, which may be problematic when corrupted regions contain unknown content. To improve the applicability, Wang et al.~\cite{wang2020vcnet} relaxed this assumption and proposed a two-stage visual consistency network. 
%Jenny et al.~\cite{schmalfuss2022blind} improved inpainting quality by integrating theoretically founded concepts from transform domain methods and sparse approximations into a CNN-based approach. 
Compared with blind image inpainting, blind video inpainting presents an additional challenge in preserving temporal consistency. Naively applying blind image inpainting algorithms on individual video frame to fill corrupted regions will lose inter-frame motion continuity, resulting in flicker artifacts. Inspired by the success of deep learning in blind image inpainting task, we propose the first deep blind video inpainting model in this paper, which provides a strong benchmark for subsequent research.
