\section{Experiments}
\label{Experiments}

\subsection{Dataset Customization}
\noindent\textbf{Training Dataset.}
Existing datasets~\cite{lee2019cpnet,chang2019free,yan2020sttn,Kang2022ErrorCF} usually generate corrupted video by ${\textbf{\emph{x}}_i}=(1-{\textbf{\emph{m}}_i})\odot{\textbf{\emph{y}}_i}+{\textbf{\emph{m}}_i}\odot{\textbf{\emph{u}}_i}$, where ${\textbf{\emph{u}}_i}$ is a pre-defined noise,  $\odot$ is the element-wise multiplication operation.
The non-blind setting always assume that the noise ${\textbf{\emph{u}}_i}$ is the “0" pixel value.
In this scenario, the corrupted regions $\textbf{\emph{m}}_i$ in corrupted video frame ${\textbf{\emph{x}}_i}$ can be easily identified and predicted by the prior knowledge introduced in the dataset, such as corrupted contents, clear border, and fixed shape.
Therefore, these datasets fail to realistically simulate the complex real-world scenarios in blind video inpainting.

In this paper, we customize a dataset suitable for blind video inpainting task. 
Specifically, we first utilize free-form strokes~\cite{chang2019free} as our corrupted regions ${\textbf{\emph{m}}_i}$, and fill ${\textbf{\emph{m}}_i}$ with real-world image patches as corrupted contents ${\textbf{\emph{u}}_i}$.
In this way, the generated corrupted contents have variable shapes, complex motions and faithful texture, effectively avoiding the introduction of shape and contents prior in the dataset.

After obtaining the corrupted regions ${\textbf{\emph{m}}_i}$ and corrupted contents ${\textbf{\emph{u}}_i}$, 
the corrupted video frame ${\textbf{\emph{x}}_i}$ can generated by ${\textbf{\emph{x}}_i}=(1-{\textbf{\emph{m}}_i})\odot{\textbf{\emph{y}}_i}+{\textbf{\emph{m}}_i}\odot{\textbf{\emph{u}}_i}$. 
However, the corrupted frame ${\textbf{\emph{x}}_i}$ obtained in this way often results in noticeable edges, which can serve as a strong indicator for identifying corrupted regions.
It may inadvertently encourage the model to learn and rely on this prior knowledge, ultimately compromising its ability to understand the semantic content of the video.
To avoiding the introduction of obvious edge priors, an iterative Gaussian smoothing~\cite{wang2018image} is introduce to extend the contact regions between ${\textbf{\emph{u}}_i}$ and ${\textbf{\emph{y}}_i}$. 
Such strategy can enforce the network to infer the corrupted regions by the semantic context of the video frame, rather than merely fitting to the training data.

Finally, to enhance the generalization ability of the model in practical applications, we also collect $1,250$ bullet removal video clips. 
We customized dataset consists of $2,400$ synthesized video clips and $1,250$ real-world video clips for blind video inpainting taks. 
This dataset will be published to facilitate subsequent research. 




\noindent\textbf{Testing Dataset.}
To verify the effectiveness of the proposed blind video inpainting method, 
we employ two widely used datasets, namely Youtube-vos~\cite{Xu2018YouTube} and DAVIS~\cite{Perazzi2016A}, for evaluation. Following the previous works~\cite{Li2020ShortTermAL,xu2019deep}, $508$ and $60$ video clips of the Youtube-vos and the DAVIS are used as test sets to calculate metrics, respectively. Meanwhile, the corrupted contents of these test sets are synthesized using a similar manner to the training set. 

\subsection{Baselines and Metrics}
To the best of our knowledge, there is no work focusing on deep blind video inpainting task. Therefore, we use sixteen methods as our baselines to evaluate the blind video inpainting ability of our model, including eleven non-blind video inpainting methods, \emph{i.e.}, TCCDS~\cite{huang2016temporally}, VINet~\cite{Kim_2019_CVPR,8931251}, DFVI~\cite{xu2019deep}, CPVINet~\cite{lee2019cpnet}, FGVC~\cite{Gao-ECCV-FGVC}, STTN~\cite{yan2020sttn}, FuseFormer~\cite{liu2021fuseformer}, E2FGVI~\cite{li2022towards}, FGT~\cite{zhang2022flow}, ProPainter~\cite{zhou2023propainter}, and WaveFormer~\cite{wu2024waveformer}, and six blind video inpainting methods, \emph{i.e.}, MPNet+CPVINet~\cite{lee2019cpnet}, MPNet+STTN~\cite{yan2020sttn}, MPNet+FuseFormer~\cite{liu2021fuseformer}, MPNet+E2FGVI~\cite{li2022towards}, MPNet+FGT~\cite{zhang2022flow}, and MPNet+WaveFormer~\cite{wu2024waveformer}.
To ensure the fairness of the experimental results, non-blind video inpainting baselines follow their original non-blind settings (including model input and test datasets), while blind video inpainting baselines are fine-tuned on our synthesized dataset using their released models and codes.
Similar to the previous works~\cite{9967838,9446636}, we employ four metrics to report quantitative results, \emph{i.e.}, PSNR~\cite{9008384}, SSIM~\cite{9010390}, LPIPS~\cite{zhang2018unreasonable} and flow warping error $E_{warp}$~\cite{lai2018learning}.

\begin{figure}[tb]
\centering%height=3.0cm,width=15.5cm
\includegraphics[scale=0.67]{Fig/Fig_VV.pdf}
\vspace{-0.5cm}
\caption{Three example of inpainting results with our method. The top row shows corrupted video frame. The completed results are shown in the bottom row, where green box denotes the mask generated by the model.}
\label{Fig6}
\end{figure}

\begin{figure}[tb]
\centering%height=3.8cm,width=17.5cmscale=0.75
\includegraphics[scale=0.67]{Fig/Fig_B.pdf}
\vspace{-0.6cm}
\caption{Qualitative results compared with OGNet~\cite{phutke2023blind} and RAVUNet~\cite{agnolucci2022restoration} on bullet removal.}
\label{Fig_BIRC}
\vspace{-0.2cm}
\end{figure}

\subsection{Experimental Results on Synthesized Dataset}
We report the quantitative evaluation results of our method and other baselines in Tab.~\ref{tab_Q}. As shown in this table, the PSNR, SSIM, $E_{warp}$ and LPIPS of our method achieve state-of-the-art results under blind inpainting settings on two datasets, and have obtained comparable performance with non-blind video inpainting methods. In particular, for blind inpainting settings, our method outperforms the best baseline (MPNet+FGT~\cite{zhang2022flow}) with a large margin in all evaluation metrics. 
Compared with video inpainting methods under non-blind settings, our blind inpainting model achieves a comparable performance to the sub-optimal baseline E2FGVI~\cite{li2022towards}. 
This verifies the superiority and effectiveness of the proposed blind video inpainting approach.
To further compare the visual qualities of inpainted results,
we show the inpainted results for our model on three examples in Fig.~\ref{Fig6}. 
As can be observed, our method can obtain spatial-temporally consistent inpainted results without any mask annotations. 
This demonstrates the effectiveness of the proposed method. 
More blind video inpainting results can be found in the \textcolor{magenta}{supplementary materials}.


\subsection{Experimental Results on Real Cases}
The proposed blind video inpainting method is beneficial for many practical applications, such as bullet removal. Fig·~\ref{Fig_BIRC} compares the results of bullet removal between OGNet~\cite{phutke2023blind}, RAVUNet~\cite{agnolucci2022restoration}, and our method. As shown in Fig.~\ref{Fig_BIRC}, our method effectively eliminates bullets in videos without the need for any mask annotations, and generates better details than the baselines. This results further demonstrate the effectiveness and superiority of our method in practical applications. More real cases can be found in the \textcolor{magenta}{supplementary materials}.

\subsection{Ablation Studies}
\label{Ablation Studies}
\begin{table}[tb]
\footnotesize
\renewcommand\tabcolsep{3.5pt}
  \centering
  % \vspace{-0.2cm}
  \caption{Ablation study of MPNet.}
  \vspace{-0.2cm}
    \begin{tabular}{c|cc||c|cc}
    \hline
    \hline
     \rowcolor{mygray} \textbf{Methods} &BCE$\downarrow$ & IOU$\uparrow$   & \textbf{Methods} &BCE$\downarrow$ & IOU$\uparrow$  \\
    \hline
    \hline
    STP &1.1251       &0.8437      & DWT\_STP &1.0785  &0.8682  \\
    DWT\_STP+LTR &0.9176    &0.8829       & Full MPNet &\textbf{0.8052}       &\textbf{0.9017}  \\
    \hline
    \hline
    \end{tabular}%
  \label{tab_M}%
  \vspace{-0.3cm}
\end{table}%
\begin{figure}[tb]
\centering%height=3.0cm,width=15.5cm
\includegraphics[scale=0.67]{Fig/Fig_MPNet.pdf}
 \vspace{-0.6cm}
\caption{Example of corrupted regions segmentation.}
\label{Fig_MV}
\vspace{-0.1cm}
\end{figure}
\begin{figure}[!t]
\centering%height=3.8cm,width=17.5cmscale=0.75
\includegraphics[scale=0.67]{Fig/Fig_VCNet.pdf}
\vspace{-0.7cm}
\caption{Comparison of qualitative results under non-blind settings. Better viewed at zoom level 400\%.}
\vspace{-0.2cm}
\label{Fig_VQ}
\end{figure}

\begin{figure*}[!t]
\centering%height=3.8cm,width=17.5cmscale=0.75
\includegraphics[scale=0.708]{Fig/Fig_RCM.pdf}
\vspace{-0.5cm}
\caption{(a) Visual evaluations with random masks filled with different content (first four examples). (b) Visual comparison of different masks filled with the same content (last four examples). Better viewed at zoom level 400\%.}
\label{Fig_RCM}
\vspace{-0.2cm}
\end{figure*}


\noindent\textbf{Effectiveness of MPNet.}
We conduct an ablation study on the proposed MPNet. 
As shown in Tab.~\ref{tab_M}, we compare the full MPNet model with its three variants,
\emph{i.e.}, “STPM”, “DWT\_STPM”, and “DWT\_STPM+LTRM”.
First two variants refer to down-sampling in the short-term prediction module using the strided-convolution and the DWT, separately.
The third variant refines the prediction result of the second variant using strided-convolution for down-sampling in the long-term refinement module.
The results indicate that the full MPNet model achieves the best prediction performance of corrupted regions, 
with the values of 0.8052 and 0.9017 for binary cross entropy (BCE) and intersection over union (IOU), respectively. 
Besides, we also show the predicted results of the full MPNet model and the second variant. 
As revealed in Fig.~\ref{Fig_MV}, the corrupted regions predicted by the full MPNet model are closer to ground-truth. This demonstrates that the effectiveness of MPNet.

\noindent\textbf{Effectiveness of VCNet.}
To demonstrate the effectiveness of VCNet, we compare it with ten non-blind video inpainting methods.
As shown in Tab.~\ref{tab_Q}, our VCNet achieves substantial improvements on all four quantitative metrics.
Furthermore, we also display some qualitative comparisons with two representative methods, including FGT~\cite{zhang2022flow} and WaveFormer~\cite{wu2024waveformer}. As shown in Fig.~\ref{Fig_VQ}, our VCNet can generate faithful texture and structure information, which verifies the effectiveness and superiority of VCNet.




\noindent\textbf{Effectiveness of Sparse Self-Attention.}
In Tab.~\ref{tab_msfn}, we perform an ablation study for sparse self-attention. From the table, we can observe that: 1) The model using only the dense self-attention (DSA) branch achieves the worst inpainting performance due to the introduction of irrelevant features. 2) Using sparse self-attention (SSA) branches improves the inpainted performance of the model. 
These results further demonstrates the necessity of an adaptive selection strategy for video inpainting task.


\noindent\textbf{Effectiveness of Consistency Loss $\mathcal{{L}}_{c}$.}
$\mathcal{{L}}_{c}$ is introduced to regularize the training parameters, enforcing mask prediction network and video completion network to capture accurate correspondence. Tab.~\ref{tab_msfn} presents an ablation study on $\mathcal{{L}}_{c}$. As shown in Tab.~\ref{tab_msfn}, the models with $\mathcal{{L}}_{c}$ participating in training can obtain better inpainted results.
\begin{table}[!t]
\vspace{-0.2cm}
\footnotesize
\centering
\caption{Ablation study of sparse self-attention and $\mathcal{{L}}_{c}$.}
\vspace{-0.2cm}
\begin{tabular}{ccc||cccc}
\hline
\hline
 \rowcolor{mygray} \multirow{1}{*}{DSA}                            & 
\multirow{1}{*}{SSA}                            & 
\multirow{1}{*}{$\mathcal{{L}}_{c}$}                            & 
\multirow{1}{*}{PSNR$\uparrow$} & 
\multirow{1}{*}{SSIM$\uparrow$} & 
\multirow{1}{*}{$E_{warp}\downarrow$} & 
\multirow{1}{*}{LPIPS$\downarrow$} \\ 
\hline
\hline
\usym{2714} & 
\multicolumn{1}{c}{} &  
&29.172       &0.8897      &0.1529       & 0.7264 \\ 
\usym{2714}  & 
\usym{2714} &  
&29.885      &0.8962      & 0.1454      &0.6891  \\ %\hline
\usym{2714}  & 
\usym{2714}  & \usym{2714} 
 &\textbf{30.528}&\textbf{0.9088}&\textbf{0.1362}&\textbf{0.6556}\\ 
\hline
\hline
\end{tabular}
\vspace{-0.3cm}
\label{tab_msfn}
\end{table}

\noindent\textbf{Robustness against Various Degradation Patterns.}
In Fig.~\ref{Fig_RCM}(a), we filled the corrupted regions of the video with different contents to perform the inpainting experiment. As shown in figure, our model can handle Gaussian noise or constant color fills, even though these patterns are not included in the training dataset. 
These results suggest that our model can learn to identify and inpaint the visually inconsistent regions in video frames, rather than simply memorizing the data distribution of the synthetic dataset.


\noindent\textbf{Robustness against Various Corrupted Patterns.}
We conduct an ablation study for corrupted patterns. As illustrated in Fig.~\ref{Fig_RCM}(b), our blind video inpainting model can effectively handle corrupted regions of various patterns, and generates visually pleasing results. This indicates that our method is highly robust to the patterns of corrupted regions.


\begin{table}[!t]
\footnotesize
\renewcommand\tabcolsep{0.5pt}
  \centering
  \caption{Efficiency analysis.}
  \vspace{-0.2cm}
    \begin{tabular}{c||cccccc}
    \hline
    \hline 
    \rowcolor{mygray} {Metrics}  & STTN~\cite{yan2020sttn}  &FuseFormer~\cite{liu2021fuseformer}   & E2FGVI~\cite{li2022towards}  &FGT~\cite{zhang2022flow} & VCNet(Ours)\\
    \hline
    \hline
    FLOPs &477.91G &579.82G   &442.18G   & 455.91G  &\textbf{396.35G} \\
    Time  &0.22s   &0.30s     &0.26s &0.39s   &\textbf{0.21s}\\
    \hline
    \hline
    \end{tabular}%
    \vspace{-0.3cm}
  \label{AS_ETC}%
\end{table}%


\noindent\textbf{Efficiency Analysis.}
We compare the efficiency of our VCNet with STTN~\cite{yan2020sttn}, FuseFormer~\cite{liu2021fuseformer}, E2FGVI~\cite{li2022towards} and FGT~\cite{zhang2022flow} on two metrics: FLOPs and inference time. 
Following previous works \cite{liu2021fuseformer,yan2020sttn,zhang2022flow}, we measure FLOPs with a temporal size (number of frames) of 20.
The inference time is measured on a single Titan RTX. 
As shown in Tab.~\ref{AS_ETC}, our VCNet achieves the fastest inference speed and lowest FLOPs compared to baselines. 
This also further indicates that our full model (MPNet+VCNet) is more efficient than MPNet+STTN~\cite{yan2020sttn}, MPNet+FuseFormer~\cite{liu2021fuseformer}, MPNet+E2FGVI~\cite{li2022towards}, and MPNet+FGT~\cite{zhang2022flow}. 

%More ablation experiments can be found in the \textcolor{magenta}{supplementary materials}.

