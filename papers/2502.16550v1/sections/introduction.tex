\section{Introduction}
\label{sec:introduction}
%The proliferation of propagandistic content in online and social media poses a significant challenge to information credibility, shaping public opinion through manipulative rhetorical strategies~\cite{da-san-martino-etal-2019-fine}. Propaganda detection has been an active area of research, with studies focusing on textual~\cite{barron2019proppy}, multimodal~\cite{ACL2021:propaganda:memes}, and multilingual~\cite{piskorski-etal-2023-multilingual,zhang2022cross} approaches to identifying deceptive and persuasive tactics. With such progress, automated systems have been developed to assist fact-checkers, journalists, and end-users~\cite{aaai2019:proppy,zhang2019tanbih}. However, these systems lack the ability to provide a justification as a form of explanation, which could greatly benefit end-users. The importance of providing an explanation have been studied in similar research areas.~\cite{russo2023benchmarking} proposed extractive and abstraction summarization approach to generation an explanation to classification decision for a claim. Such approaches has previously been studied for fact-checking~\cite{atanasova2024generating,kotonya-toni-2020-explainable-automated}. Very recently Large Language Models (LLMs) based explanation has been studied in different studies, such as hate speech detection and its explanation~\cite{yang2023hare}, reliability of LLMs based explanation for hate-speech~\cite{wang_evaluating_2023}, and generating LLM based explanation~\cite{huang_chain_2023}.  


%%%% Interpretability and explanation --
The proliferation of propagandistic content in online and social media poses a significant challenge to information credibility, shaping public opinion through manipulative rhetorical strategies~\cite{da-san-martino-etal-2019-fine}. Automatic propaganda detection has been an active area of research, with studies focusing on textual~\cite{barron2019proppy}, multimodal~\cite{ACL2021:propaganda:memes}, and multilingual approaches~\cite{piskorski-etal-2023-multilingual,zhang2022cross}. However, majority of existing systems lack the ability to provide a justification as a form of model prediction explanation, which could greatly benefit end-users, improving their critical media literacy and increasing their trust in system's predictions.  
%For propaganda detection and its explanation,
 
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.22]{figures/expl_example.png}
    \vspace{-0.3cm}
    \caption{Example of a news sentence and its explanation and quality assessment process.}
    \label{fig:expl_example}
    \vspace{-0.6cm}
\end{figure}

 ~\citet{RANLP2021:propaganda:interpretable} developed interpretable models for propaganda detection in news articles, combining qualitative features with pre-trained language models to enhance transparency. More recently,~\citet{10.1145/3613904.3642805} proposed an LLM-based approach for propaganda detection and natural explanations. However, their study fully relies on GPT-4 for detection and explanation generation. This approach has limitations, such as over-reliance on GPT-4, which may not perform well for non-English, medium- to low-resource languages. Additionally, to our knowledge, there are no datasets for propaganda detection that accompany explanations with annotated labels. To address this gap, we propose a large multilingual (i.e., Arabic and English) explanation-enhanced dataset for propaganda detection. We build upon existing datasets, including ArPro~\cite{hasanain2024can} and the SemEval-2023 English dataset~\cite{piskorski-etal-2023-semeval}, enhancing them with explanations. Given the complexity of manually generating explanations and the higher reliability reported for GPT-4-based explanation generation~\cite{wang_evaluating_2023}, we opted to use a stronger LLM for explanation generation and manually checked for quality assurance. Figure \ref{fig:expl_example} demonstrate an example of news sentence, its explanation, and human evaluation process. The developed dataset can be used to train specialized LLMs for propaganda detection and to provide explanations for their predictions.  
To this end, our contributions to this study are as follows:
\begin{itemize}[noitemsep,topsep=0pt,labelsep=.5em]
    \item We introduce an explanation-enhanced dataset for propaganda detection, consisting of approximately 21k and 6k news paragraphs and tweets for Arabic and English, respectively.  
    \item To ensure the quality of the LLM-generated explanations, we manually evaluate a sample of explanations for each language.  
    \item Through comparative experiments, we demonstrate that our proposed LLM achieves similar performance to transformer-based model while also generating explanations for its predictions.  
\end{itemize}
