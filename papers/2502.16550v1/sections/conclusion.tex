\section{Conclusions and Future Work}
\label{sec:conclusions}
In this study, we introduce a multilingual dataset for propaganda detection and explanation, which is the \textit{first} large dataset accompanied by explanations for the task. For Arabic, we have created a new propaganda-labeled dataset of size 13$K$ samples, consisting of tweets and news paragraphs. Using GPT-4o1, we generated explanations for this dataset, as well as for ArPro (consisting of 8$K$ instances), and for English starting from the SemEval-2023 dataset. 
To ensure the quality of the explanations, we manually evaluated them, and our findings suggest that these explanations can serve as gold-standard references. % for developing specialized LLMs. 
 We propose an explanation-enhanced LLM based on Llama-3.1 (8B) and demonstrate its comparable performance to strong baselines, while also providing high-quality explanations. 
For future work, we plan to extend the task to multilabel classification and span-level propaganda techniques detection.


% manually annotated dataset for detecting propaganda in Arabic memes. We have annotated $\sim$ 6K memes with four different categories, making it the first such resource for Arabic content. To facilitate future annotation efforts for this type of content, we developed annotation guidelines in both English and Arabic and are releasing them to the community. Our work provides an in-depth analysis of the dataset and includes extensive experiments focusing on different modalities and models, including pre-trained language models (PLMs), large language models (LLMs), and multimodal LLMs. Our results indicate that fine-tuned models significantly outperform LLMs.

% In future work, we plan to extend the dataset with further annotations that include hateful, offensive, and propagandistic techniques.
