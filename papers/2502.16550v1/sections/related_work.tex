\section{Related Work}
\label{sec:related_work}


Automatic detection of misinformation and propagandistic content has gained significant attention over the past years. Research has explored various problems, including cross-lingual propaganda analysis~\cite{barron2019proppy}, news article propaganda detection~\cite{da-san-martino-etal-2019-fine}, and misinformation and propaganda related to politics and war. Building on the seminal work of~\citet{da-san-martino-etal-2019-fine}, resources have been developed for multilingual~\cite{piskorski-etal-2023-semeval,hasanain-etal-2023-araieval} and multimodal setups~\cite{SemEval2021-6-Dimitrov,araieval:arabicnlp2024-overview}. 
% Most tasks have been framed within a classification framework, including binary, multiclass, multilabel, and span-level technique detection~\cite{hasanain2024can,piskorski-etal-2023-semeval}.
Reasoning-based explanations in NLP have advanced fact-checking~\cite{russo2023benchmarking}, hate speech detection~\cite{huang-etal-2024-chatgpt}, and propaganda detection~\cite{10.1145/3613904.3642805}. While binary classifiers effectively identify propaganda, they often lack transparency, making interpretation difficult~\cite{atanasova2024generating}.~\citet{RANLP2021:propaganda:interpretable} showed that qualitative reasoning aids deception detection, while~\citet{atanasova2024generating} emphasized explanation generation for better interpretability. Yet, explicit prediction reasoning for propaganda detection remains under-explored, particularly in multilingual settings. Our work addresses the gap by developing a multilingual explanation-enhanced dataset and proposing a specialized LLM.

% a \textit{first} Arabic dataset with reasoning-based explanations
% a resource and propose an specialized LLM fine-tuning Llama-3.1-8B to generate explanations alongside classifications, improving both accuracy and interpretability. 
% Additionally, we introduce the \textit{first} Arabic dataset with reasoning-based explanations, contributing to explainable AI in misinformation detection.

% \paragraph{Persuasion Techniques Detection.}
% The study of misinformation and propagandistic content has received significant attention in the past year. Research efforts have explored various aspects, including cross-lingual propaganda analysis~\cite{barron2019proppy}, news article propaganda analysis~\cite{da-san-martino-etal-2019-fine}, and misinformation and propaganda related to politics and war. Following the seminal work by~\citet{da-san-martino-etal-2019-fine}, resources have been developed for multilingual~\cite{piskorski-etal-2023-semeval,hasanain-etal-2023-araieval} and multimodal setups~\cite{SemEval2021-6-Dimitrov,araieval:arabicnlp2024-overview}. Tasks have primarily been defined within a classification framework, including binary, multiclass, multilabel, and span-level technique detection~\cite{hasanain2024can,piskorski-etal-2023-semeval}.  

% The study of reasoning-based explanations in NLP has gained attention in fact-checking~\cite{russo2023benchmarking}, hate speech detection~\cite{huang-etal-2024-chatgpt}, and propaganda detection~\cite{10.1145/3613904.3642805}. While binary classification models effectively detect propagandistic content, they often lack transparency, making interpretation challenging~\cite{atanasova2024generating}.~\citet{yu2021interpretable} demonstrated that qualitative reasoning improves deception detection, while~\citet{atanasova2024generating} emphasized the role of explanation generation in enhancing interpretability. Despite these recent advancements, explicit reasoning remains underexplored, particularly in multilingual settings. Our work bridges this gap by developing a resource and fine-tuning Llama-3.1-8B to generate explanations alongside classifications, improving both accuracy and interpretability. Additionally, we introduce the \textit{first} Arabic dataset with reasoning-based explanations, contributing to explainable AI in misinformation detection.

% \paragraph{Explanation.}
% The study of reasoning-based explanations in NLP has gained attention in fact-checking~\cite{russo2023benchmarking}, hate speech detection~\cite{huang-etal-2024-chatgpt}, and propaganda detection~\cite{10.1145/3613904.3642805}. While binary classification models effectively detect propagandistic content, they often lack transparency, making interpretation challenging~\cite{atanasova2024generating}.~\citet{yu2021interpretable} demonstrated that qualitative reasoning improves deception detection, while~\citet{atanasova2024generating} emphasized the role of explanation generation in enhancing interpretability. Despite these recent advancements, explicit reasoning remains underexplored, particularly in multilingual settings. Our work bridges this gap by developing a resource and fine-tuning Llama-3.1-8B to generate explanations alongside classifications, improving both accuracy and interpretability. Additionally, we introduce the \textit{first} Arabic dataset with reasoning-based explanations, contributing to explainable AI in misinformation detection.
