%\section{Experiments}
\section{LLM for Detection and Explanation}
\label{sec:experiments}

% In this study, we propose to fine-tune an open LLM to perform both propaganda detection and explanation. 
% \todo[inline]{to edit}
% \todo[inline]{need to describe instruction generation}
% \todo[inline]{need to describe baselines training and testing}
% I
%\subsection{LLM Fine-tuning}
\paragraph{Model}
For developing an explanation-enhanced LLM, we adapted Llama 3.1 8B Instruct, a robust open-source model with strong multilingual capabilities \cite{dubey2024llama}. We selected the 8B variant over larger versions (70B, 45B) due to the high computational cost of fine-tuning and inference. Llama-3.1 8B has also shown strong performance in relevant multilingual tasks \cite{pavlyshenko2023analysisdisinformationfakenews, kmainasi2024llamalensspecializedmultilingualllm}.

%We base our experiments on Llama 3.1, one of the most effective \textit{open} LLMs to date, with strong multilingual performance~\cite{dubey2024llama}. Fine-tuning larger scales of the model (e.g., 70B version) holds a great overhead in terms of time and computational cost. Moreover, These models may be inaccessible to many researchers, so we focus on the smaller Llama 3.1-8B version. We base LlamaLens on Llama 3.1-8B-Instruct, as it is already aligned with various user tasks.
% We detail our method to model training next. 
\noindent
\paragraph{Instruction-following dataset.}
We constructed the instruction-following datasets with the aim of enhancing the model's generalizability and to guide the LLM to follow user instructions, which is a standard approach to fine-tune an LLM~\cite{zhang2023instruction}. To create versatile instructions, we prompt state-of-the-art LLMs including GPT-4o, and Claude-3.5-sonnet to generate instructions (See Appendix~\ref{apndix:prompts}). Using each LLM, we created ten diverse English instructions per language. Each instruction is uniformly distributed across dataset samples. Each sample is structured as below equation.

\footnotesize
\begin{equation}
    \begin{aligned}
        a &= \text{"Label: "} f(x) + \text{" Explanation: "} g(x), \\
        &\quad \text{where } x = I(\text{input\_text})
    \end{aligned}
\end{equation}
\normalsize
% \begin{equation}
%     \begin{aligned}
%         a &= \text{"Label: "} f(x) + \text{" Explanation: "} g(x), \\
%         &\quad \text{where } x = I(\text{input\_text})
%     \end{aligned}
% \end{equation}
% Specifically, twenty diverse English instructions were created per training setup, uniformly distributed across dataset samples. Each sample is structured as follows:
% \begin{lstlisting}
% system_message: system_prompt
% user_message: Instruction + {input_text}
% assistant_message: Label: {class_label} Explanation: {explanation}
% \end{lstlisting}
% Two state-of-the-art LLMs were employed to generate instructions, GPT-4o, and Claude-3.5-sonnet. The primary objective of this approach is to improve the generalizability ability across various instruction formats. During evaluations, a randomly selected instructions is used to test the model's performance, measuring its generalization capability. 
\noindent
\paragraph{Training.}
Due to limited computational resources, we adopted Low-rank Adaptation (LoRA)~\cite{hu2021lora} for training as a parameter-efficient fine-tuning technique. LoRA captures task-specific updates through low-rank matrices that approximate full weight updates. %This approach reduces memory and computational overhead while also mitigating the risk of overfitting compared to full fine-tuning~\cite{mao2025survey}. 

\noindent
\textbf{Parameters Setup}  
We fine-tune the model for two epochs using mixed-precision training with bfloat16 (bf16). LoRA hyperparameters are set with a rank and \(\alpha\) of 128, a dropout rate of 0.1, and a learning rate of 2e-4. Optimization is performed using AdamW~\cite{loshchilov2017decoupled}, with a weight decay of 0.01 to regularize the model and mitigate overfitting by penalizing weights during optimization. The learning rate follows a cosine decay schedule, gradually decreasing over time to aid model convergence. We maintain a consistent LoRA learning rate of 2e-4 across all trained models. Training is conducted on four NVIDIA A100 GPUs using Distributed Data Parallel (DDP)~\cite{bai2022modern}. We set a per-device batch size of 4 and use gradient accumulation with 2 steps, effectively achieving an overall batch size of 32.

\noindent
\textbf{Evaluation.} For the evaluation, we used a zero-shot approach and selected a random instruction from our instruction sample as a prompt, which is a common approach reported in a prior study~\cite{kmainasi2024llamalensspecializedmultilingualllm}. The temperature parameter was set to zero to ensure result reproducibility. Additionally, we implemented post-processing function to extract the labels and corresponding explanations.

\noindent
\textbf{Evaluation Metrics.} To assess classification performance, we used macro and micro F$_1$ scores. For evaluating explanations, we used BERTScore~\cite{zhang2020bertscoreevaluatingtextgeneration}, which leverages contextual embeddings. Specifically, we computed the F$_1$ score using AraBERT (v2)~\cite{antoun-etal-2020-arabert} for Arabic and BERT-base-uncased~\cite{devlin2019bert} for English.\footnote{BERTScore was chosen over BLEU and ROUGE as it captures semantics, better reflecting explanation quality.}



% To assess the performance of classification, we employed standard metrics: macro and micro-F$_1$ scores. To evaluate the model's generated explanations, we used a semantic-based metric. Specifically, we used F$_1$ within BERTScore~\cite{zhang2020bertscoreevaluatingtextgeneration}, which uses contextual embedding to compute the scores. For Arabic we used AraBERT (v2)~\cite{antoun-etal-2020-arabert} and for English we used bert-base-uncased~\cite{devlin2019bert}.\footnote{We used BERTScore instead of traditional metrics like BLEU and ROUGE, as it focuses on capturing the semantics of the explanations, which reflects the quality of the explanations rather than exact lexical overlap.} 

% The score is derived from the BERT contextual embeddings; AraBERT (v2)~\cite{antoun-etal-2020-arabert} was used to assess Arabic explanations, and bert-base-uncased~\cite{devlin2019bert} was used for English explanations. 

