\section{Dataset}
\label{sec:dataset}
In addition to investigating LLMs' ability in explainable propaganda detection for a high-resource language, English, we also consider a lower-resource language, Arabic. In this work, we extend existing datasets with natural language annotation explanations generated by GPT-4o1, and evaluated by humans. This section gives an overview of the dataset creation approach.      
\begin{comment}
\begin{table}[t]
\centering
\setlength{\tabcolsep}{2pt} 
\scalebox{0.71}{%
\begin{tabular}{lrrrrrr}
\toprule
\textbf{Split} & \multicolumn{1}{l}{\textbf{\# Articles}} & \multicolumn{1}{l}{\textbf{\#items}} & \multicolumn{1}{l}{\textbf{(W)}} & \multicolumn{1}{l}{\textbf{Avg (W)}} & \multicolumn{1}{l}{\textbf{Avg Exp. (W)}} & \multicolumn{1}{l}{\textbf{\% Prop.}} \\
\hline
\multicolumn{7}{c}{\textbf{Arabic}}\\
\hline
Train & 8,103 &18,453    &597,604 & 32.4     & 48.1 &63.8\%  \\
Dev   & 822   &1,318     &42,944  & 32.6     & 47.9 &64.4\%  \\
Test  & 835   &1,326     &46,484  & 35.1     & 48.7 &61.3\%  \\ \midrule
\textbf{Total} & \textbf{8,913$^{*}$}        & \textbf{21,097}  & \textbf{687,032} & \textbf{32.6}     & 48.1 &\textbf{63.7\%}\\
\hline
\multicolumn{7}{c}{\textbf{English}}\\
\hline
Train & 250   &4,472     &107,517 & 24.0     & 61.2 &26.9\%  \\
Dev   & 204   &621       &14,871  & 23.9     & 61.6 &27.9\%  \\
Test  & 225   &922       &21,862  & 23.7     & 61.2 &27.9\%  \\ \midrule
\textbf{Total} & \textbf{250$^{*}$} & \textbf{6,015}   & \textbf{144,250} & \textbf{24.0}     & \textbf{61.2} &\textbf{27.2\%}\\
\bottomrule
\end{tabular}%
}
\caption{Distribution of Arabic and English datasets. Exp.: explanation. Data items: annotated data elements including paragraphs and tweets. $^{*}$ Total unique articles. Prop.: Propagandistic. W: \# Words}
  \label{tab:data_stats}
\end{table}
\end{comment}

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lrrrrr}
\toprule
\textbf{Split} & \multicolumn{1}{c}{\textbf{\# Articles}} & \multicolumn{1}{c}{\textbf{\#items}} & \multicolumn{1}{c}{\textbf{Avg (W)}} & \multicolumn{1}{c}{\textbf{Avg Exp. (W)}} & \multicolumn{1}{c}{\textbf{\% Prop.}} \\
\hline
\multicolumn{6}{c}{\textbf{Arabic}}\\
\hline
Train & 8,103 &18,453    & 32.4     & 48.1 &63.8\%  \\
Dev   & 822   &1,318     & 32.6     & 47.9 &64.4\%  \\
Test  & 835   &1,326     & 35.1     & 48.7 &61.3\%  \\ \midrule
\textbf{Total} & \textbf{8,913$^{*}$} & \textbf{21,097}  & \textbf{32.6}     & 48.1 &\textbf{63.7\%}\\
\hline
\multicolumn{6}{c}{\textbf{English}}\\
\hline
Train & 250   &4,472     & 24.0     & 61.2 &26.9\%  \\
Dev   & 204   &621       & 23.9     & 61.6 &27.9\%  \\
Test  & 225   &922       & 23.7     & 61.2 &27.9\%  \\ \midrule
\textbf{Total} & \textbf{250$^{*}$} & \textbf{6,015}   & \textbf{24.0}     & \textbf{61.2} &\textbf{27.2\%}\\
\bottomrule
\end{tabular}%
}
\vspace{-0.3cm}
\caption{Distribution of Arabic and English datasets. Exp.: explanation. Data items: annotated data elements including paragraphs and tweets. $^{*}$ Total unique articles. Prop.: Propagandistic. W: \# Words}
  \label{tab:data_stats}
  \vspace{-0.3cm}
\end{table}



\subsection{Arabic Propaganda Dataset}
Building upon the ArPro Arabic dataset~\cite{hasanain2024can}, we follow the same annotation approach to build a larger dataset by collecting and annotating 7$K$ paragraphs. %, aiming at reducing label distribution skewness. 
 Furthermore, this extension includes collecting and annotating tweets, to examine propaganda use in social media. Eventually, our Arabic dataset comprises two types of annotated documents: tweets and news paragraphs. The news paragraphs are extracted from articles published by 300 distinct news agencies, capturing a broad spectrum of Arabic news sources. It covers a diverse range of writing styles and topics including 14 different topics such as  news,  politics, human rights, and science and technology.
As for the tweets subset, we start from a manually constructed set of 14 keywords and phrases, covering the topic of Israeli-Palestinian war, targeting sub-topics popular during October and early November 2023. We use Twitter's search API to search for tweets posted during the second week of November 2023 and matching the collected phrases, avoiding collecting retweets and replies, resulting in 5.7$K$ tweets to annotate. 

Data was annotated following a two-phase approach~\cite{hasanain2024can}. In the first phase, $3$ annotators independently examine each data item (paragraph or tweet) and label it by whether it is propagandistic. In the second phase, $2$ expert annotators examine annotations from the first phase and resolve any conflicts. Finally, the dataset set was split into training, development, and testing subsets following a stratified sampling approach. 


\subsection{English Propaganda Dataset}

The English dataset is composed of $250$ articles, collected from $42$ unique news sources, coming from all political positions. The articles are manually cleaned of any artifacts mistakenly included during collection, such as links. The articles include topics that trended in the late 2023 and early 2024, with discussions of politics and the Israeli-Palestinian war covering $60\%$ of the articles.

%The articles are annotated with $24$ persuasion techniques. 
Each article is annotated by at least $2$ annotators and reviewed by $1$ curator, whose task is to resolve inconsistencies between annotations.
During the whole process, random checks of the annotations are carried out to verify the quality and give feedback on inaccuracies.
To create the dataset, the articles are divided into sentences and split into three subsets: training, development and testing.
%To create the dataset, the articles are divided into sentences. %Only annotations completely contained in a sentence are retained. A more specific processing is done for the technique "Repetition",  which is maintained only in sentences in which it appeared at least twice.
%The sentences are divided into train, dev and test sets following a $75-10-15$ split, while ensuring, when possible, that there are sufficient samples of each technique for evaluation.
%The sentences are divided into train, dev and test sets following a $60-20-20$ split for sentences with techniques with less than $30$ total occurrences and a $75-10-15$ for more common techniques and sentences with no techniques, to ensure that there are sufficient samples of each technique for evaluation.

\subsection{Explanation Generation}
We use GPT-4o1 to generate natural language explanations for gold propaganda annotations. This LLM is designed to have superior reasoning capabilities\footnote{\url{https://openai.com/index/introducing-openai-o1-preview/}} which we believe are required for the task at hand. During pilot studies, we experimented with another highly-effective LLM, GPT-4o and a variety of prompts. Our manual evaluation of different samples in English and Arabic %by the authors 
revealed that explanations generated by GPT-4o1 are better on average (following the quality assessment described in the next section). Eventually, the following prompt is used for explanation generation: \textit{``Generate one complete explanation shorter than 100 words on why the paragraph as a whole is [gold label (propagandistic/not propagandistic)]. Be very specific in this full explanation to the paragraph at hand. Your explanation must be fully in [language].''}


\paragraph{Quality of Generated Explanations}
% \todo[inline]{Arid, can you add table here and complete this section?}
% We computed annotation agreement using various evaluation measures, including Fleiss' kappa, Krippendorff’s alpha, average observed agreement, and majority agreement. The resulting scores were 0.529, 0.528, 0.755, and 0.873, respectively. Based on the value of Krippendorff’s alpha, we can conclude that our annotation agreement score indicates moderate agreement.
%We computed the average 5-point Likert scale value for human evaluation metrics. 
%\todo[inline]{@Arid: Verify and add missing details in highlights.}
We verify the quality of the generated explanations by human evaluation. We used a 5-point Likert scale for various evaluation metrics selected from relevant studies on natural language explanation evaluation~\cite{huang-etal-2024-chatgpt,10.1145/3543873.3587368,10.1145/3613904.3642805}, including \textit{informativeness, clarity, plausibility, and faithfulness}. %This approach is in-line with relevant studies on (e.g., ~\cite{huang-etal-2024-chatgpt}).
Evaluation was carried out for Arabic and English datasets using a random sample of 200 generated explanations. 
We provided detailed annotation instructions %guidelines (see in Appendix \ref{sec:app_annotation_guideline}) 
for the human evaluators and 
each explanation assessed by three evaluators (see in Appendix \ref{sec:app_annotation_setup}).
% We develop an extensive annotation guideline (see in Appendix \ref{sec:app_annotation_guideline}) to be followed by the human evaluators. Each explanation was evaluated by 3 evaluators.
% (see in Appendix \ref{sec:app_annotation_setup}). 

In Table \ref{tab:likert_score}, we report the average scores for all evaluation metrics. We first compute the average across annotators for each explanation and then across all explanations. Results show that GPT-4o1 generally generates explanations that are of high quality, considering the metrics at hand, especially in terms of clarity. 


%The average agreement scores for faithfulness, clarity, plausibility, and informativeness in the Arabic propaganda dataset are 4.201, 4.351, 4.236, and 4.274, respectively, indicating high agreement for all evaluation metrics. Similarly, the average Likert scale agreement scores range from 4.648 to 4.715 for the English propaganda dataset. 

\begin{table}[h]
\centering
\setlength{\tabcolsep}{1pt} 
\scalebox{0.85}{%
\begin{tabular}{@{}lcccc@{}}
\toprule
\multicolumn{1}{l}{\textbf{Data}} & \multicolumn{1}{c}{\textbf{Faithfulness}} & \multicolumn{1}{c}{\textbf{Clarity}} & \multicolumn{1}{c}{\textbf{Plausibility}} & \multicolumn{1}{c}{\textbf{Informative.}} \\ \midrule
Arabic & 4.20 & 4.35 & 4.24 & 4.27 \\
English & 4.67 & 4.71 & 4.66 & 4.65 \\ \bottomrule
\end{tabular}
}
\vspace{-0.1cm}
\caption{Average Likert scale value for each human evaluation metric across different sets of explanations.}
\label{tab:likert_score}
\vspace{-0.3cm}
\end{table}