\section{Approach}
\label{sec:approach}


\begin{figure}[]
	\centering
 \includegraphics[width=0.7\linewidth,trim=0cm 0cm 0cm 0cm]{figures/overview.pdf}
	\vspace{-10pt}
	\caption{Overview of \nick.}
	\label{fig:overview}
\end{figure}

Figure~\ref{fig:overview} presents a high-level overview of the approach used in \nick, which aims to repair data-driven errors in large language models. The approach consists of two primary components: computing the relevant model slice and then repairing the identified slice selectively. In the first stage, we apply the concept of program slicing to language models to identify the slice that requires repair. In the second stage, we repair the identified slice selectively, focusing on the most error-prone sections of the model while minimizing any impact on its general performance. The following sections will explore the detailed steps involved in \nick.

\subsection{Problem Formulation}

Let \( \pi_\theta: X \rightarrow Y \) denote a pre-trained language model that maps input texts from \( X \) and a set of parameters, $\theta$, to corresponding output texts in \( Y \). Consider a bad demonstration dataset, denoted as \( D^E = (X^E, Y^E) \), where the response \( Y^E \) to a prompt \( X^E \) is undesirable. The goal is to ensure that the model \( \pi_\theta \) does not produce responses similar to \( Y^E \) when given prompts similar to \( X^E \). Additionally, consider a curated or refined dataset, denoted as \( D^R = (X^E, Y^R) \), which demonstrates the desirable response \( Y^R \) for those error-evoking prompts \( X^E \). In practice, these refined responses can be obtained through various methods, such as human annotation or conditioning a language model~\cite{solaiman2021process,lee2024mechanistic, wang2022exploring, pan2023automatically}. Once the refined data is acquired, the model \( \pi_\theta  \) is typically repaired via domain-adaptive training either by directly optimizing the model with the curated dataset~\cite{lee2024mechanistic,wang2022exploring,gehman2020realtoxicityprompts}, or the model is explicitly trained to prefer good demonstrations over bad ones via preference optimization~\cite{rafailov2024direct}.

However, repairing the errors in LLMs often involves a trade-off between overall model performance and repair quality~\cite{korbak2023pretraining,wang2022exploring}. Higher repair quality tends to come at the cost of reduced general performance~\cite{ding2022delta}. Our experiments also observed this trade-off across various techniques. Domain-adaptive repair methods are particularly susceptible to this issue because they update model parameters indiscriminately without considering their relevance to specific errors. This indiscriminate updating can lead to inefficient error repair, where the reduction in general performance may not justify the extent of error correction achieved.

To address these challenges, we propose focusing repairs on the sections of the model with the highest concentration of errors while leaving unrelated parameters unchanged. This approach aims to enhance repair efficiency, potentially achieving greater repair quality with less disruption to general performance compared to "intent-unaware" or indiscriminate techniques.

To that end, in this paper, we propose using examples from the bad demonstration dataset to identify and slice the most relevant sections of the model, referred to as intent, for selective repair. Specifically, we aim to pinpoint the most error-prone blocks within the transformer architecture for targeted intervention. Additionally, we hypothesize that errors may not be confined to a single block. To accommodate this, we introduce a dynamic slicing mechanism that allows for the selection of the most error-prone sections during the course of training. This approach enables a more nuanced and precise repair of errors throughout the model. 








\subsection{Slicing Intent}

Drawing upon the concept of relevant program slicing~\cite{weiser1984program}, in this step, we aim to slice off the most error-prone sections of the model for a selective repair. Such a focused repair approach is critical for LLMs, as updating all parameters using limited repair data may lead to overfitting and knowledge degradation~\cite{ding2022delta}. By selectively slicing the model based on bad data, our aim is to address the root cause of errors in the model while preserving the model's overall knowledge.

\subsubsection{Challenges in Slicing LLM}
Existing slicing techniques for deep learning models are primarily designed for networks using \textit{ReLU} activations~\cite{zhang2022remos,zhang2020dynamic}. These techniques rely on activation status or their magnitudes to identify relevant parts of the model. However, transformer-based language models employ an attention mechanism with linear transformations, making these methods inapplicable.  In contrast to \textit{ReLU} activations, the magnitude of a linear transformation in \textit{attention} doesn't directly correspond to its importance due to subsequent matrix multiplications. As an illustration, consider the following simplified example of unmasked attention scores for a sequence with three tokens, $T_1, T_2$ and $T_3$ and an embedding dimension, $d_k=1$:

\[
Q=
  \begin{bmatrix}
    3 \\
    -3 \\
    2
  \end{bmatrix}
  \text{ and }
  K=
  \begin{bmatrix}
    -5 \\
    2 \\
    1
  \end{bmatrix}
\]

\begin{align*}
\text{Score} &= \sigma\left(\frac{QK^T}{\sqrt{d_k}}\right) = \sigma\left(
  \begin{bmatrix}
    -15 & 6 & 3\\
    15 & -6 & -3 \\
    -10 & 4 & 2
  \end{bmatrix}\right) \\
&=
   \begin{bmatrix}
    0 & 0.95 & 0.05\\
    1 & 0 & 0 \\
    0 & 0.88 & 0.12
  \end{bmatrix} \text{ (simplified)}
\end{align*}

Here, we observe that in predicting the next token for $T_2$, $T_1$ is the most influential. Thus, the multiplication of two negative values of \textit{Q} and \textit{K}, $-3 \times -5$, yields the highest score for $T_2$. Unlike \textit{ReLU}, where a node value less than zero might indicate irrelevance~\cite{zhang2022remos}, in this context, negative values do not reliably signify insignificance. Additionally, considering the magnitude of the activation level is inapplicable~\cite{zhang2020dynamic}, as the sign plays a crucial role in the score computation.

In addition, existing techniques require careful calibration of a threshold to select the slice~\cite{zhang2022remos,zhang2020dynamic}, which is a challenging task for large-scale models with billions of parameters, such as LLMs. Tuning for the optimal threshold can be very challenging and time-consuming for such models, highlighting the need for a threshold-free slicing approach for LLMs.



\subsubsection{Our Approach}
To address these challenges, we propose a \textit{gradient-based} approach for determining the relevance of model parameters to the slicing criterion, which does not rely on the activation of neurons. Specifically, we identify the relevant transformer block, referred to as \textit{intent}, of the model by assessing the sensitivity of the blocks to the slicing criterion. The approach works by treating a sample of bad data, ($D^E=(X^E, Y^E)$),  as criteria for slicing the intent that requires fixing. Figure~\ref{fig:overview} shows the overview of our proposed algorithm for slicing the LLM, which involves two major steps: assessing the relevance of parameters to the slicing criteria and intent or slice selection, which will be discussed below:

\input{algoSensitivity}

\paragraph{Computing sensitivity to slicing criteria}
As previously discussed, activation-based approaches are not applicable in the context of the \textit{transformer}. Instead, we identify the \textit{intent} by assessing the sensitivity of blocks to \textit{slicing criteria}. To that end, we propose leveraging \textit{negative log-likelihood (NLL)} of the model response to assess the relevance of blocks to the slicing criteria. Specifically, we first measure the impact of all parameters on the model's response by calculating the first-order gradient of the \textit{NLL} of the generated response. Then, we compute the sensitivity of the block by taking the \textit{L2-norm} of the gradients of all parameters within the block. Without loss of generality, the sensitivity of a block to a slicing criterion, $x$, can be represented as:




\[
\label{eq:sen1}
S\{\text{block}\} \approx \left\| \nabla_{\theta_{\text{block}}} \left( -\sum_{t=1}^{T} \log p_{\theta}(x_t \mid x_{1:t-1}) \right) \right\|_2
\]

Here, \( \mathbf{\theta}_{\text{block}} \) represents the parameters within a specific block of the transformer, \( p_\theta(x_t \mid x_{1:t-1}) \) represents the probability of the token \( x_t \) given the prior tokens \( x_{1:t-1} \), \( T \) denotes the total number of tokens in \( x \), and \( \mathbf{\theta} \) refers to the overall parameter space. The notation \( \| \cdot \|_2 \) denotes the L2 norm, which is applied to the gradient of the NLL with respect to the block's parameters.



The \textit{NLL} reflects the model's confidence in generating the target response. A lower NLL score indicates higher confidence in accurately predicting the target response. Taking the gradients of parameters with respect to the \textit{NLL} provides a measure of their sensitivity to the inputs or criteria provided. If a small increase or perturbation in a parameter leads to a noticeable impact on the model output, that parameter is likely important or relevant to the criteria~\cite{kirkpatrick2017overcoming}. The greater the magnitude or norm of the gradient for a parameter, the more relevant it is to the criteria.




The \textit{Sensitivity} method (in Algorithm~\ref{algo:compute_nll}) provides our approach for computing sensitivity to slicing criteria. The method takes an instance of the model ($\pi_{\theta}$) and slicing criteria as $X$ and $Y$. It first calculates the \textit{negative log-likelihood} by invoking the \textit{NLL} method in line~\ref{algocs:3}.

To achieve this, the \textit{NLL} method first obtains the model logits (the output of the last layer) by forward passing the input through the model (Line~\ref{algocn:2}). The last generated token is then discarded, as the corresponding token in the ground truth response does not exist (Line~\ref{algocn:3}). Using the \textit{softmax} activation function, the probability distribution for all output tokens in the vocabulary is calculated (Line~\ref{algocn:4}). To compute the \textit{NLL}, a loss mask is obtained where the special padding tokens are skipped to eliminate their impact on the calculation (Line~\ref{algocn:1}). The \textit{NLL} for each token in the sequence is calculated individually by multiplying the probability distribution of ground truth response ($Y$) with the log-likelihood of the output ($\log(P)$) (Line~\ref{algocn:5}). Next, the \textit{NLL} for the entire sequence is calculated by summing the individual NLLs for every token in the sequence, with padding tokens eliminated by multiplying by the \textit{mask} (Line~\ref{algocn:6}). Finally, in Line~\ref{algocn:7}, the mean \textit{NLL} is computed by dividing by the count of non-padding tokens in the sequence. The final averaged \textit{NLL} approximates the model's confidence in generating the target responses, $Y$, for the given inputs, $X$, and returned from the method as the final outcome.



In the \textit{Sensitivity} method, after obtaining the \textit{NLL} for the given criteria, the magnitude of the gradients for each transformer block is calculated by taking the \textit{L2 norms} of all parameters within the block with respect to the \textit{NLL} (Lines~\ref{algocs:4}–\ref{algocs:5}). Specifically, the first-order gradients for all the model parameters are computed with respect to the \textit{NLL} (i.e., $\nabla_\theta (L)$). For each parameter within a transformer block ($\vartheta \in \theta_{\text{block}} \land \theta_{\text{block}} \subset \theta$), the gradients are squared, and the square root of their summation is taken to yield the overall magnitude or sensitivity of the block ($S$) (Line~\ref{algocs:4}). This measure indicates the relevance of the block to the given slicing criteria and is used in the \textit{Slice} method to identify and slice the most error-prone block.



\paragraph{Selecting Intent}


The final method, \textit{Slice}, in Algorithm~\ref{algo:compute_nll}, slices the most error-prone block of the model for the provided criteria by leveraging the other two methods. The method takes as input an instance of the model, denoted by \( \pi_{\theta} \), where $\theta$ represents the parameter space of the model, and a set of slicing criteria, \( X \) and \( Y \). It first computes the sensitivity of every block in the model by invoking the \textit{Sensitivity} method (Line~\ref{algoid:1}). Then, in Line~\ref{algoid:2}, the block with the highest sensitivity—deemed most relevant to the provided criteria—is selected. When the provided criteria correspond to a sample from poor demonstration data \( D^E \), this block represents where the most error is concentrated. This step effectively eliminates the need for thresholding to identify the slice. Next, in Line~\ref{algoid:3}, the parameters within the selected block are sliced off and returned by the method.



\input{algoModularization}

\subsection{Repairing Intent}


In this step, the identified slice or intent, $\theta_{\text{slice}}$, which is primarily responsible for undesirable generation, is addressed through two optimization objectives, as shown in Figure~\ref{fig:overview}. Specifically, our loss function includes an NLL term as repair loss and a KL term to preserve the model's normal utility, as shown in Equation~\ref{eq:rpob}.



\begin{equation}
\label{eq:rpob}
\begin{aligned}
Loss = & \alpha \cdot \text{NLL}\left( p_{\theta_{\text{slice}}}(\cdot \mid X^R) \right) \\
& + KL\left( p_{\theta_{\text{slice}}}(\cdot \mid X^N) \parallel p_{\theta_{\text{ref}}}(\cdot \mid X^N) \right)
\end{aligned}
\end{equation}


Here, \( \alpha \) represents the strength of the repair or NLL loss, \( \theta_{\text{slice}} \) denotes the set of sliced parameters from the model \( \pi_\theta \), \( \theta_{\text{ref}} \) represents the parameter space of the reference model \( \pi_{\theta_{\text{ref}}} \), and \( p_{\theta_{\text{slice}}} \) denotes the probability distribution of \( \pi_\theta \) conditioned on the sliced parameters. The key components of the repair process are briefly described below:

\subsubsection{Repair Loss}

We use the negative log-likelihood (\textit{NLL}) as the repair loss for our technique. This loss aims to maximize the log-likelihood of the curated responses (\(Y^R\)) for fault-evoking prompts (\(X^E\)). \textit{NLL} is a commonly employed loss function for repairing models via continued pre-training or supervised fine-tuning~\cite{wang2022exploring, gehman2020realtoxicityprompts, lee2024mechanistic,geva2022transformer}. However, unlike existing techniques, we only optimize the sliced parameters (\(\theta_{\text{slice}}\)) of the patient model, \(\pi_\theta\). This selective approach allows for more focused and aggressive updates of the error-prone parameters, potentially leading to more effective repairs. Additionally, updating a smaller portion of the total parameters reduces general performance degradation, as most of the model retains its original parameters. The relative importance of this term is regulated by the $\alpha$ coefficient. 

\subsubsection{KL Loss}
\textit{KL loss} is used to preserve the general performance of the model during the repair process. Specifically, this term aims to minimize the divergence between a reference distribution (the output of the reference model, $\pi_{\theta_{\text{ref}}}$) and the target distribution (output of $\pi_{\theta}$ on the pre-training corpus \(D^N\)). This term essentially encourages the model to maintain similar generation capabilities to the reference model on unrelated aspects.


\subsubsection{Dynamic Slicing}
Finally, we employ a dynamic slicing mechanism that selects the most error-prone block of the model during the course of training for an adaptive repair. This design decision is motivated by three key factors:
\paragraph{Error concentration} First, our threshold-free slicing technique selects only the most relevant or error-prone block of the model based on the criteria. However, a single block may not be solely responsible for undesirable responses to certain prompts. Other parts of the model might also significantly contribute to erroneous outputs, as we empirically confirm in our analysis. We find that errors can span multiple blocks, necessitating the repair of more than one block (details in \S~\ref{sec:rq3}). In such cases, repairing only a predetermined fixed block may not be sufficient.

\paragraph{Error movement} Second, a fixed selection strategy, like those used in existing works~\cite{zhang2020dynamic, zhang2022remos}, may fail to adapt to the effects of training dynamics on the model. While an area of the model might appear most responsible for undesirable responses before repair, it may not remain the most error-prone block throughout the course of training. Once training adequately addresses the initially selected area, another unselected area may appear more problematic, deserving more repair effort at that point. A dynamic slicing technique that accounts for the impact of training dynamics can more effectively address such shifts in error concentration.


\paragraph{Local error correction}

As demonstrated in our algorithm for repairing intent (Algorithm~\ref{algo:repair}), we use corresponding bad examples for each batch of good examples (Line~\ref{algorp:1}) from the bad demonstration dataset, \( D^E \), as criteria to slice the most relevant block of the model (Line~\ref{algorp:2}). This approach ensures that repair efforts focus on the block that most amplifies errors for the current batch of data. In contrast to pre-selection strategies, which often use all or a sample of data to determine which part to slice~\cite{zhang2020dynamic, zhang2022remos}, our method enables a more nuanced repair by allowing for localized error correction.





\subsubsection{Algorithm Overview}


The \textit{Repair} method in Algorithm~\ref{algo:repair} outlines the procedure for repairing the intent using our proposed optimization objectives. The method takes as input the affected or to-be-repaired model \( \pi_\theta \), a reference model \( \pi_{\theta_{\text{ref}}} \), which is the same model as initial \( \pi_\theta \) and is used to maintain similar performance on unrelated aspects post-correction, and references to bad examples (\(D^E\)), good examples (\(D^R\)), and normal examples (\(D^N\)). Additionally, a hyper-parameter \( \alpha \) is provided, which is used as a measure of the strength of the repair loss.


The repair process begins by sampling a batch of good examples in Line~\ref{algorp:1} during each training iteration. It then constructs a batch of corresponding bad examples to use as slicing criteria for the current iteration (Line~\ref{algorp:2}). Additionally, a random batch from the normal examples is obtained to compute a KL term (Line~\ref{algorp:3}). Next, the \textit{Slice} method is invoked to extract the most error-inducing block of the current model, \( \pi_\theta \) (Line~\ref{algorp:4}). The \textit{NLL} loss for the sliced parameters, \( \theta_{\text{slice}} \), is computed using the good batch in Line~\ref{algorp:5}. Similarly, a KL term is calculated for both the currently repaired model and the reference model using the batch of normal examples (Line~\ref{algorp:6}). In Line~\ref{algorp:7}, the combined loss is obtained, with the \textit{NLL} term regulated by a user-defined coefficient (\(\alpha\)). After computing the loss, gradients with respect to the sliced parameters are computed in Line~\ref{algorp:8} and updated in Line~\ref{algorp:9}. The repair process continues until convergence or early stopping is triggered and the repaired model is returned.








 




 





