\section{Evaluation}
\label{sec:evaluation}

In this section, we introduce our evaluation setup, outline our research questions, and discuss the experimental results in detail. As previously mentioned, we evaluate our technique within a model detoxification framework, where our goal is to repair toxic models using the principles outlined in \nick. To this end, we examine our framework across three research questions:

\begin{itemize} 
    \item \textbf{RQ1:} \textit{How effectively can \nick repair or detoxify the model?} This research question evaluates \nick's effectiveness in eliminating toxicity from models and compares it against several state-of-the-art baseline techniques.

    \item \textbf{RQ2:} \textit{What is the computational overhead of \nick?} This research question measures the computational overhead of \nick by assessing total floating point operations (FLOPS), peak memory usage, and convergence duration and compares these metrics against baseline techniques.
    
    \item \textbf{RQ3:} \textit{Does the dynamic selection employed by \nick offer any advantage?} In this research, we conduct ablation studies and empirical analyses of error concentration in the model to assess the impact and necessity of selective and dynamic repair.
\end{itemize}


\subsection{Experimental Setup}

\subsubsection{Model}
\label{sec:model}

We evaluate \nick across three models from the GPT family, namely GPT-2 Large (812M parameters), GPT-2 XL (1.61B parameters), and GPT-Neo (1.3B parameters). The GPT-2 models, developed by \textit{OpenAI}, were trained on 8 million web pages from the \textit{WebText} dataset~\cite{radford2019language}. The GPT-Neo model, developed by \textit{EleutherAI}, was trained on the \textit{PILE} dataset~\cite{gptneo}. 
We load these pre-trained models from the official Hugging Face repositories of OpenAI and EleutherAI~\cite{gptneohf,gpthf}.


\subsubsection{Dataset}

To evaluate \nick, we used the detoxification dataset developed by Lee \textit{et al.}~\cite{lee2024mechanistic}, consisting of 24,576 toxic and non-toxic pairs generated by GPT-2 models~\cite{radford2019language} from prompts sampled from the WikiText-2 train split~\cite{merity2016pointer}. The dataset is balanced with an equal number of toxic and non-toxic examples for detoxification. In our experimental setup, we treat the toxic continuations as a bad demonstration dataset, $D^E$, and the non-toxic continuations as a good dataset, $D^R$. The evaluation data includes a test set and a small development sample, which consists of 50 challenge prompts from REALTOXICITYPROMPTS~\cite{gehman2020realtoxicityprompts} and a subset of the WikiText-2 test split, totaling around 32.7K tokens. This development set is used for tuning hyperparameters through repeated runs with various combinations.


Additionally, for each model, we construct an individual normal dataset, \(D^N\), a curated pre-training subset that preserves diversity while minimizing targeted error symptoms. We leverage the notion of `unconditional generation' to construct this dataset~\cite{wang2022exploring}. Specifically, starting with the special start-of-sequence token (GPT models use `<|endoftext|>` as the start-of-sequence token~\cite{wang2022exploring}), we generate approximately 15,000 texts for each model using different random seeds. Following prior work, we employ nucleus sampling with a temperature of 1 and \(p = 0.9\) during generation~\cite{wang2022exploring}. The unconditionally generated text corpus is considered a good representative of the model's training corpus~\cite{wang2022exploring}. Then, we score each generation using perspective API and remove the ones with toxicity scores higher than 0.5~\cite{gehman2020realtoxicityprompts}, removing approximately 1.2\% of the initial dataset.
By minimizing the KL divergence in these examples, we aim to preserve the model's ability to generate random text similarly to its pre-repair state, thereby reducing the impact on its general performance.


\subsubsection{Baseline}
\label{sec:baseline}

We compare the performance of two variants of \nick: the standard \nick, which does not enforce a KL constraint, and \nickkl, which does, against several representative state-of-the-art baselines within domain-adaptive training, as introduced below:


\paragraph{Domain-Adaptive Pretraining (DAPT)} DAPT is a framework introduced by Gururangan \textit{et al.}~\cite{gururangan2020don}, which involves continuing the pretraining of a model on domain-specific texts. Gehman \textit{et al.}~\cite{gehman2020realtoxicityprompts} applied the DAPT framework to further train GPT-2 models on nontoxic texts to detoxify them. In our setup, we evaluate the effectiveness of DAPT in detoxifying models and compare it with \nick.

\paragraph{Direct Preference Optimization (DPO)} DPO is a cutting-edge algorithm designed to replace RLHF (Reinforcement Learning from Human Feedback~\cite{ouyang2022training}) due to its complex and unstable training process. It directly steers the model towards desirable generations over undesirable ones~\cite{rafailov2024direct}. The DPO algorithm has been shown to effectively eliminate toxicity from models, as demonstrated by Lee \textit{et al.}~\cite{lee2024mechanistic}. We also compare our method against DPO.


\paragraph{Domain-Adaptive Pretraining with KL Constraint (DAPT+KL)}
We also compare \nick against a variant of the DAPT method that includes a KL constraint to preserve the general model performance~\cite{liu2023chain}. While DAPT alone may cause the model to deviate when training on a domain-specific corpus, adding a KL term helps evaluate the repair quality of this approach by ensuring that the model maintains its general performance while focusing on domain-specific adjustments.

Additionally, in RQ3, we compare \nick against two of its variants to assess the effectiveness of the components employed by \nick, as described below:

\paragraph{\nick (Min)} In this variant of \nick, during each training iteration, instead of selecting the block with the highest error concentration, the block with the least concentration is chosen. Comparing \nick against this variant allows us to assess the impact of selective repair.


\paragraph{\nick (Fixed)} In this variant of \nick, a fixed slice of the model is pre-selected for repair. Specifically, using a substantial random sample of bad data ($D^E$) consisting of 2000 examples, average sensitivities for all parameters are computed. Then, using the same technique described in Algorithm~\ref{algo:compute_nll}, block sensitivity is computed, and the block with the highest sensitivity is selected for repair. Comparing \nick against this variant allows us to assess the impact of dynamic selection.

\subsubsection{Metric}

Following the prior works~\cite{gehman2020realtoxicityprompts,lee2024mechanistic,geva2022transformer}, we use the following two metrics to evaluate the toxicity and general performance of the model after detoxification.

\paragraph{Toxicity}Gehman \textit{et al.} developed a dataset called \textit{REALTOXICITYPROMPTS} to evaluate toxicity in LLMs~\cite{gehman2020realtoxicityprompts}. This dataset consists of sentence-level prompts that are provided to LLMs to generate continuations, which are likely to elicit toxic responses from the models. They also created a \textit{challenge} subset of this dataset, which includes 1,199 prompts that consistently caused all models in their experiments to generate toxic responses~\cite{gehman2020realtoxicityprompts}. This \textit{challenge} subset has been used in previous studies to evaluate the effectiveness of detoxification methods~\cite{lee2024mechanistic,geva2022transformer}. Similarly, we leverage this subset to assess the detoxification quality of our repaired models. Additionally, following prior works~\cite{geva2022transformer,lee2024mechanistic,gehman2020realtoxicityprompts}, we use the widely adopted toxicity detection tool, \textit{PERSPECTIVE API}~\cite{papi}, to assign a toxicity score to each generation. The score ranges from 0 to 1, with higher scores indicating more toxic responses. The toxicity score for the test data is calculated as the average toxicity score across the entire test set, following prior works~\cite{lee2024mechanistic}.



\paragraph{PPL} \textit{Perplexity} (PPL) is a widely used metric to evaluate the generation quality of language models. It has been employed to assess degradation in model generation after the repair process~\cite{gehman2020realtoxicityprompts, lee2024mechanistic, geva2022transformer, wang2022exploring}. \textit{PPL} measures how uncertain or "perplexed" the model is in predicting the next word. Higher PPL indicates that the model is worse at predicting the next word, meaning its generation quality is lower. When evaluated on a test corpus, it reflects how well the model's generated text aligns with the test data. Following prior works on GPT models~\cite{geva2022transformer, lee2024mechanistic}, we compute the \textit{PPL} of the model before and after repair using two benchmarks. \textit{PPL (WikiText2)} is computed on the test split of \textit{WikiText-2}~\cite{merity2016pointer}, which contains 4,358 rows and approximately 241K words. \textit{PPL (Lambada)} is evaluated on the LAMBADA benchmark~\cite{paperno2016lambada}, which assesses the text-understanding capabilities of language models. To calculate PPL, we split the test corpus into 1024-token segments, compute the NLL for target tokens in each segment, and weigh the NLL values by the number of target tokens. The perplexity is then computed as the exponentiation of the mean NLL per token.



Additionally, we evaluate the computational overhead of the techniques using the following three metrics, as described in the literature~\cite{kaplan2020scaling,shoeybi2019megatron,lee2019energy}:

\paragraph{TFLOPs} \textit{TFLOPs} (Tera Floating-Point Operations per Second) represents the total number of floating-point operations performed in the trillions during the course of training or inference~\cite{kaplan2020scaling}. This metric is commonly used to gauge the computational demand of a method. To compute the \textit{TFLOPs}, we utilize an open-source tool~\cite{casson2023transformerflops} that applies the formula derived by Kaplan \textit{et al.} for GPT models~\cite{kaplan2020scaling}.

\paragraph{Peak Memory Usage} Memory consumption is a critical factor when developing techniques for large language models (LLMs)\cite{shoeybi2019megatron}. To evaluate this, we measure the peak memory usage of all techniques during training. We employ the method used by Lee \textit{et al.}~\cite{lee2019energy}, where an independent process queries the GPU using the \textit{nvidia-smi} command at 1-second intervals to record the highest memory usage observed.

\paragraph{GPU Time} We report the total GPU time required for training each technique until convergence. This measurement is obtained using \textit{PyTorch's CUDA} API~\cite{torchcuda}, which tracks the time spent on the GPU throughout the training process.


\paragraph{Total Iteration} Additionally, we report the total iteration needed until the model converges or early stopping is triggered. 




% \paragraph{F1} In addition to \textit{perplexity}, we also measure the \textit{F1} score using 2,000 \textit{Wikipedia} sentences as prompts, as done in prior works~\cite{lee2024mechanistic}, to further assess the impact of repair on the general performance of the model. The \textit{F1} score measures the harmonic mean between precision and recall of the model's output, where precision is the percentage of generated tokens that are present in the original \textit{Wikipedia} sentences, and recall is the percentage of tokens in the original \textit{Wikipedia} sentences that are also contained in the model's generation. Ideally, we aim to see minimal change in both the \textit{perplexity} and \textit{F1} scores after repair, ensuring that general performance is maintained.




\subsubsection{Training Details}

As discussed in \S~\ref{sec:model}, we used pre-trained models from the official \textit{Hugging Face} repositories of \textit{OpenAI} and \textit{EleutherAI}~\cite{gpthf, gptneohf}. We leveraged Python's deep learning library \textit{PyTorch}~\cite{paszke2017automatic} for further training these models across all techniques. All hyperparameters in our study were fine-tuned using a small development dataset produced by Lee \textit{et al.}~\cite{lee2024mechanistic}. For \textit{DPO}, \textit{DAPT}, and \textit{DAPT + KL}, we used the implementation provided by Lee \textit{et al.}~\cite{lee2024mechanistic} as a reference. 

Similarly, we fine-tuned the hyperparameters for all techniques using the development set. Specifically, the final tuned learning rates for standard \nick, \nickkl, \textit{DPO}, \textit{DAPT}, and \textit{DAPT+KL} are $2e^{-5}$, $5e^{-5}$, $1e^{-6}$, $1e^{-6}$, and $5e^{-6}$, respectively. Through trial and error, we found that a higher learning rate tends to achieve better repair quality at the expense of general performance and vice versa. Since \nick only modifies a small portion of the model, it can accommodate a larger learning rate with less adverse impact on general performance compared to other indiscriminate techniques. Similarly, \textit{DAPT+KL} allows a slightly higher learning rate than \textit{DPO} and \textit{DAPT} as it explicitly aims to maintain general performance during repair. We also set the value of $\alpha$ to 0.5 for both \nick and \textit{DAPT+KL} after tuning.

For training models using all techniques, we used the memory-efficient \textit{RMSProp} optimizer with 150 warmup steps and a linear learning rate scheduler. A batch size of four was used for the techniques, with a validation split of eight batches, each with a batch size of eight. Models were trained with a validation loss patience of 30 iterations. All models were trained on an \textit{NVIDIA A100} GPU with 40GB of memory. We conducted all the training using the same random seed to ensure reproducibility and enable a fairer comparison.
 

\input{tbRQ1}
\subsection{RQ1: How Effectively Can \nick Repair the Model?}
\label{sec:rq1}



In this research question, we evaluate the repair effectiveness of \nick and compare it against several baselines. Table~\ref{tab:rq1} provides a comparative overview of \nick's performance across all models. The results show that both variants of \nick consistently outperform all other techniques on every model tested, achieving a higher repair score with better general performance stability. Specifically, standard \nick achieves an average 81.6\% reduction in toxicity, with an 8.3\% and 7.4\% increase in PPL on the \textit{WikiText2} and \textit{Lambada} benchmarks, respectively, across all models. The \nickkl variant reduces toxicity by 88.7\% with an 11\% and 8.3\% increase in PPL in \textit{WikiText2} and \textit{Lambada}. 

In contrast, \textit{DPO}, \textit{DAPT+KL}, and \textit{DAPT} reduce toxicity by 61.8\%, 56.3\%, and 36.2\%, while increasing \textit{PPL (WikiText2)} by 20.3\% (\textit{PPL (Lambada)}: 22.7\%), 15.3\% (\textit{PPL (Lambada)}: 17\%), and 30.2\% (\textit{PPL (Lambada)}: 33.3\%), respectively. Thus, both \nick variants clearly outperform all baseline techniques in both metrics. For example, compared to \textit{DPO}, standard \nick and \nickkl are 32\% and 43.6\% more effective in reducing toxicity while incurring 59.2\% and 46\% less increase in \textit{PPL (WikiText2)}. Similarly, against \textit{DAPT+KL}, standard \nick and \nickkl are 44.8\% and 57.5\% more effective in reducing toxicity while showing 45.8\% and 28.2\% less increase in \textit{PPL (WikiText2)}. Additionally, we find that all techniques, including \nick, significantly outperform \textit{DAPT}. A similar effect is observed in the \textit{PPL (Lambada)} benchmark as well.


Understandably, the ability of \textit{DAPT} to repair the model is limited by its tendency to lose general performance more rapidly. Overall, its PPL increases by 30.2\% (33.3\%) compared to increases of 15.3\% (17\%), 20.3\% (22.7\%), 8.3\% (7.4\%), and 11\% (8.3\%) for \textit{DAPT+KL}, \textit{DPO}, \nick,and \nickkl, respectively, in \textit{WikiText2} and \textit{Lambada}. This shows that adding a KL term to the \textit{DAPT} loss for self-generated random data helps better preserve unrelated model knowledge. Furthermore, the results demonstrate that compared to \textit{DAPT+KL}, which operates on all parameters indiscriminately, \nick's selective approach is more effective at controlling performance degradation with 45.8\% (56.4\%) and 28.2\% (51.2\%) less \textit{PPL (WikiText2)} (\textit{PPL (Lambada)}) increase than \textit{DAPT+KL}, for instance, despite using a higher learning rate. Although in the GPT-Neo model, PPL is slightly better for \textit{DAPT+KL} and \textit{DPO} on \textit{WikiText2} and \textit{Lambada}, \nick methods achieve better trade-offs between PPL and repair quality. For instance, the toxicity-to-PPL ratio for \textit{DAPT+KL} is 2.34 and 1.39 on \textit{WikiText2} and \textit{Lambada}, respectively, while for \textit{DPO}, it is 0.73 and 0.46. In contrast, standard \nick achieves 0.34 and 0.21, while \nickkl achieves 0.3 and 0.18—both lower than their counterparts—suggesting that \nick achieves a better trade-off between PPL and toxicity, even though a particular learning rate (LR) may result in higher PPL.

The results also demonstrate that KL-enabled techniques achieve better repair or toxicity scores, as they allow for more aggressive model repair at higher LR. For instance, \textit{DAPT+KL} reduces the toxicity score by 55\% compared to its non-KL counterpart, \textit{DAPT}. Similarly, \nickkl achieves a 9\% greater reduction in toxicity and exhibits 93\% lower standard error, indicating greater stability compared to standard \nick. Even with KL-enabled \textit{DAPT}, both \nick variants significantly outperform it in toxicity reduction by 44.8\% and 57.5\%, while incurring proportionally less disruption to general performance. \nick's ability to support higher LRs is a crucial factor in its effectiveness. However, this added efficiency is also largely driven by \nick's focused repair approach, as demonstrated in \S~\ref{sec:rq3}. Despite having a lower LR, the other approaches result in high PPL due to altering a large fraction of unrelated parameters. While a small LR implies minor individual changes, their cumulative effect can lead to a significant shift, causing high PPL. However, \nick focuses only on the areas with the highest error concentration, leaving most parameters untouched during each training pass. As a result, the model tends to converge 
on the error validation dataset before significantly affecting unrelated parameters, leading to highly precise adjustments and limiting large parameter shifts overall.


\input{tbRQ2}

\subsection{RQ2: What Is the Computational Overhead of \nick?}
\label{sec:rq2}

In this research question, we evaluate the computational overhead of \nick and compare it against baseline techniques. Table~\ref{tab:rq2} presents the overhead of various techniques across four metrics. Among the two \nick variants, the overhead of standard \nick is more amenable to the other three baseline techniques across all metrics. For example, overall, it ranks first in memory consumption and second in GPU time despite incurring higher TFLOPs and requiring more iterations to converge.

The additional compute units (TFLOPs) consumed by \nick variants are due to the extra forward pass and a higher number of iterations required for convergence. \nickkl involves four forward passes: one for a toxic batch of data to assess the sensitivity ($D^E$), one for a non-toxic batch ($D^R$), and for normal data, one pass through the model under repair ($\pi_\theta$) and another through the reference model ($\pi_{\theta_{\text{ref}}}$). In contrast, standard \nick does not compute the KL term, thereby eliminating two forward passes for normal data, which results in significantly lower TFLOPs overall—53\% less than \nickkl. \textit{DPO} also requires four forward passes; however, it converges in fewer iterations, leading to lower total TFLOPs.

On a per-token basis, the TFLOPs required for \nick are comparable to other baselines (with standard \nick ranking second and \nickkl requiring the most, though comparable to \textit{DPO}). However, due to the higher number of iterations needed for convergence to address a smaller part of the model, total TFLOPs are higher. Despite this, \nick’s GPU time remains proportionally lower, and it trains faster or comparably to some baseline techniques, such as \textit{DAPT} and \textit{DAPT+KL}. This could be attributed to better GPU utilization in \nick variants, which process more TFLOPs per iteration, while \textit{DAPT} takes longer to converge, spreading out its TFLOPs and leading to lower average GPU utilization.

In terms of memory consumption, we find that standard \nick uses the least memory, while \nickkl ranks third. This is because the backward pass in \nick is more constrained than in other techniques. First, to calculate sensitivity, it only computes gradients for the transformer blocks, excluding the \textit{embedding} and final output layers. After slicing the layer, it zeroes out the gradients, freeing memory. In the second backward pass, it only computes gradients for the required smaller slice, resulting in lower peak memory consumption compared to other techniques. \nickkl requires slightly more memory than \textit{DAPT} due to storing extra logits for normal data from two forward passes.

Overall, while \nick incurs higher TFLOPs due to longer iterations, it remains memory-efficient and trains reasonably faster by fully utilizing available GPU power. In exchange for additional compute units, \nick offers better repair efficiency than the other techniques. Between \nick and \nickkl, although the latter is more computationally intensive, it provides greater stability in model repair, as observed in \S~\ref{sec:rq1}.


\input{tbRq3}
\subsection{RQ3: Does the Dynamic Selection Employed by \nick Offer Any Advantage?}
\label{sec:rq3}





In the final research question, we investigate the effectiveness of dynamic slicing in delivering focused model repair. As described in \S~\ref{sec:baseline}, we evaluated two additional variants of both the standard \nick and \nickkl: \nick\textit{+ Min} and \nick\textit{+ Fixed}. The \nick\textit{+ Min} variant selects the transformer block with the lowest error concentration, as opposed to the highest in the regular \nick. This baseline allows us to assess the impact of selection on repair efficacy. Similarly, \nick\textit{+ Fixed} disables dynamic slicing and instead pre-selects the block with the highest error concentration for repair. This variant enables us to assess the impact of dynamic selection on repair effectiveness. The impact on model performance in this RQ is evaluated solely on the \textit{WikiText2} benchmark.

\begin{figure}
\centering
\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/gpt_neo_sen.png}
  \caption{GPT Neo 1.3B}
  \label{fig:gpt2xl}
\end{subfigure}%
\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/gpt2_large_sen.png}
  \caption{GPT2 812M}
  \label{fig:gpt2large}
\end{subfigure}
\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figures/gpt2_xl_sen.png}
  \caption{GPT2 1.61B}
  \label{fig:gptneo}
\end{subfigure}
\caption{Relative Toxicity Levels of Transformer Blocks Across Different Models}
\label{fig:tox}
\end{figure}

Table~\ref{tab:rq3} presents the comparative results of these variants against the regular \nick. It demonstrates that both regular \nick variants significantly outperform their \textit{Min} and \textit{Fixed} counterparts. For instance, standard \nick and \nickkl reduce toxicity by 80.5\% and 88.6\% more than their \textit{Min} variants while maintaining a similar level of PPL (with regular \textit{IRepairs} showing 3.3\% and 1.3\% more reduction, respectively). It clearly shows the impact of selection made by regular \nick in repairing the model. 

Similarly, both regular \textit{IRepairs} outperform their \textit{Fixed} counterparts by a clear margin. Standard \nick and \nickkl reduce toxicity by 80.4\% and 82.5\% more than their \textit{Fixed} counterparts while achieving slightly lower PPL (with \textit{Fixed} variants scoring 2.7\% and 0.8\% less in PPL). Additionally, pre-selecting the most error-prone blocks and focusing repair efforts on them slightly outperforms the dynamic \textit{Min} variants, with the difference being more noticeable in the \textit{Fixed + KL} variant (scoring 35\% less in toxicity than \textit{Min}).

We particularly observed that the \textit{Fixed + KL} approach performs comparably to the \nickkl method on the GPT2 1.61B model. To understand why fixed selection was effective for this model, we analyzed the toxicity levels of all transformer blocks across different models. We calculated toxicity by randomly sampling 2000 examples and computing the average sensitivity for each block. Figure~\ref{fig:tox} displays the distribution of relative toxicity across all blocks for the models studied.

For the GPT2 1.61B model, the most toxic block is about 28\% more toxic than the second most toxic block. In contrast, for the GPT2 812M model, the difference between the most and second most toxic blocks is only 8\% and 9.5\% with the third most toxic blocks. Similarly, for the GPT Neo 1.3B model, the difference between the most and second most toxic blocks is just 6.5\%. This indicates that the GPT2 1.61B model has a higher concentration of toxicity in the top block, making fixed targeted repair of this block more effective. In other models, errors are more evenly distributed among the top few blocks, reducing the effectiveness of a top-block-only repair.

This suggests that errors can be dispersed throughout the model, and a fixed selection technique may require costly tuning to determine the optimal selection threshold. In contrast, our dynamic slicing approach avoids this need for tuning and allows for model-wide repair by dynamically focusing on the most error-prone areas during training.


Figure~\ref{fig:tox} also shows that GPT-2 1.61B, GPT-2 812M, and GPT-Neo 1.3B have 245.3\%, 120.8\%, and 1137.7\% higher average error density in the top 20\% of blocks compared to the remaining 80\% of blocks. Error density was measured by dividing the total toxicity within \( N \) blocks by \( N \). This clearly indicates that the repair process should give more priority to these highly error-inducing regions than to others, which may lead to superior outcomes, as observed in our results.
















