\section{Threats To Validity and Limitations}
\label{sec:threats}
An internal threat to the study is the quality of the detoxification and evaluation datasets. To address this, we utilize both the training dataset and evaluation setup from a recent reputable work on LLM detoxification~\cite{lee2024mechanistic}. Additionally, we employ the implementations provided in the same study to address concerns about the construct validity of baseline techniques. Another internal threat arises from the reliability of the evaluation metrics. To mitigate this concern, we measure repair or toxicity scores using the widely used Perspective API~\cite{papi} and evaluate model quality post-repair using perplexity, as employed in many prior works~\cite{lee2024mechanistic, wang2022exploring, gehman2020realtoxicityprompts}. Similarly, our evaluation metrics for measuring computational overheads are based on well-established metrics in the literature~\cite{kaplan2020scaling, lee2019energy}. An external threat is the relevance of the models used. To address this, we have selected three models from the GPT and GPT-Neo families with billions of parameters, all of which have previously been employed for evaluating detoxification techniques~\cite{leong2023self, gehman2020realtoxicityprompts, xu2022leashing,korbak2023pretraining, lee2024mechanistic, yang2022unified}.

While the case study performed in this paper shows that \nick can effectively address the data-driven errors in large language models (LLMs), it is demonstrated within the context of detoxification. Further research is encouraged to explore its effectiveness and generalizability to other data-driven error scenarios, which will enhance the understanding and potential applications of this approach. Furthermore, although we evaluated \nick on models with billions of parameters—similar to those frequently used in evaluating prior repair techniques—its performance in ultra-large-scale LLMs remains an area for further investigation.








