\section{Introduction}
\label{sec:intro}
The recent advancement in large language model (LLM) capabilities marks a transformative moment in natural language processing (NLP). Owing to the effectiveness of \textit{transformer} in scaling efficiently to the large corpus, LLMs now excel in tasks such as question-answering, text summarization, and code generation~\cite{zhao2023survey}. However, despite their impressive capabilities, LLMs are not without their shortcomings. Akin to traditional software, LLMs can exhibit bugs or generate unintended outputs, manifesting as toxicity, harmful responses, factual errors, or hallucinations~\cite{ji2023survey,korbak2023pretraining}. 
The root cause of these issues often lies in the training data itself~\cite{ji2023survey}. LLMs are typically trained on vast, unfiltered datasets, primarily sourced from the internet, using semi-supervised learning techniques~\cite{zhao2023survey}. It is impractical to validate such a vast corpus, eliminating biases, factual inconsistencies, and other issues~\cite{ji2023survey}. As a result, LLMs inadvertently inherit and propagate these issues in their outputs.



Existing approaches to mitigate such issues in LLMs primarily operate in three stages~\cite{wang2022exploring,korbak2023pretraining,pan2023automatically}: the pre-training stage~\cite{korbak2023pretraining,liu2024exposing}, alignment stages following pre-training, also known as domain-adaptive training (DAT) methods~\cite{wang2022exploring,gehman2020realtoxicityprompts,lee2024mechanistic,liu2023chain,rafailov2024direct}, and runtime methods~\cite{leong2023self,yang2022unified,xu2022leashing,niu2024parameter}.
Runtime methods, while impactful, often address symptoms rather than the root causes of model errors and can introduce computational overhead, limiting their applicability in low-latency scenarios~\cite{wang2022exploring,korbak2023pretraining,dathathri2019plug,huang2023survey}. Pre-training methods, such as Liu et al.'s attention-sharpening regularizer~\cite{liu2024exposing}, though effective, are costly due to the need for training from scratch, making them more suitable for new models~\cite{wang2022exploring,huang2023survey}. DAT methods, on the other hand, directly repair pre-trained models and provide an offline approach to error mitigation, making them particularly useful in real-time applications and situations requiring more invasive repair procedures~\cite{wang2022exploring,huang2023survey}.


DAT has two main paradigms: fine-tuning the model with curated data and preference optimization, such as reinforcement learning from human feedback (RLHF)~\cite{ouyang2022training}. However, both approaches update model parameters indiscriminately without considering their relevance to the problem at hand. This can decrease the effectiveness of the repair and increase the likelihood of negatively impacting the model's general performance by altering unrelated parameters. To address this, we introduce \nick, a dynamic slicing-based technique for selectively repairing only the intended part of the model. 


Our motivation for targeted LLM repair is inspired by the successful application of the `fault localization followed by program repair' paradigm in traditional software engineering (SE). This approach has demonstrated effectiveness in producing optimal repairs while preserving the program's original structure as much as possible~\cite{mechtaev2015directfix,nguyen2013semfix,wen2018context}. For instance, Mechtaev \textit{et al.} employ partial MaxSAT constraint solving and component-based program synthesis to localize bugs and generate repairs, focusing on minimizing alterations to the program's structure~\cite{mechtaev2015directfix}. Inspired by these works, we adapt and evaluate this paradigm in the context of LLMs to address data-driven errors. Specifically, we propose a method that first localizes the source of errors within the model and then selectively repairs it. This approach aims to produce optimal repairs while preserving model performance by targeting only the relevant parts of the model and leaving unrelated sections unaffected.


To localize the source of errors within the model, we build upon the concept of relevant program slicing in software engineering~\cite{weiser1984program}. Relevant slicing identifies a subset of program statements that could impact a specified slicing criterion~\cite{gyimothy1999efficient, zhang2022remos}. Similarly, we apply these principles to identify and isolate the parts of the model that are most relevant to the errors being addressed.

Recently, slicing techniques have been adapted for deep learning models, offering advantages such as model protection and simplification~\cite{zhang2020dynamic} and vulnerability mitigation during transfer learning~\cite{zhang2022remos}. These approaches use a subset of data as the `slicing criteria' and analyze the model's activations to identify relevant parts of the model. However, these techniques rely on activation values to determine relevance, which is not directly applicable to the transformer architecture used in LLMs (as discussed in \S~\ref{sec:approach}). Additionally, these methods are more akin to static slicing, where the relevant sections of the model are selected after training. This results in a 'fixed selection,' which can be useful for various applications as demonstrated in previous works~\cite{zhang2020dynamic, zhang2022remos}. In contrast, we hypothesize that a dynamic selection technique, applied during the training process, will enable a more nuanced and precise repair of LLMs through domain-adaptive training with curated data.


Inspired by these works, \nick treats faulty data as a `slicing criterion' to identify error-prone sections of the model during each training pass. By analyzing the gradients of parameters with respect to the negative log-likelihood (NLL) of the faulty data, we pinpoint the components responsible for the unintended faulty responses. We then repair only the identified area while freezing the rest of the model, subject to a Kullback-Leibler (KL) divergence~\cite{csiszar1975divergence} constraint. This approach allows for focused repair efforts on the most critical sections, minimizing disruption to the existing knowledge stored in most model parameters. Moreover, \nick employs a dynamic slicing approach for repairing LLMs, enabling more nuanced and adaptive model repair compared to existing slicing methods that pre-select a fixed area.

To evaluate the effectiveness of our proposed technique, we conducted a case study focused on mitigating toxicity in LLMs. Given their pre-training on extensive corpora in a semi-supervised manner, LLMs are known to perpetuate biases and toxicity present in the data~\cite{xu2022leashing}. To assess the efficacy of our repair approach, we detoxified three models from the GPT-2 and GPT-Neo families, ranging from 800M to 1.6B parameters, using \nick. We compared our results against state-of-the-art baselines, employing the pairwise detoxification dataset developed by Lee \textit{et al.}~\cite{lee2024mechanistic}. Specifically, our baselines include representative techniques from different paradigms within domain-adaptive methods, such as Domain-Adaptive Pretraining (DAPT)~\cite{gururangan2020don,gehman2020realtoxicityprompts}, DAPT with a regularization term on the pre-training mixture to retain general performance during repair~\cite{liu2023chain}, and Direct Preference Optimization (DPO)~\cite{rafailov2024direct}, an RL-based preference optimization technique. Note that this study focuses on evaluating pre-trained language models (PLMs) that have not undergone further alignment. The term `LLM' used throughout the paper specifically refers to these PLMs. In this context, the general performance of the model refers to the quality of language generation from these models, measured in terms of perplexity metric.


We summarize the key contributions and findings of this paper as follows:

\begin{itemize} 
\item We propose using sensitivity as a measure of relevance for slicing transformer-based language models, addressing architectural challenges unique to these models.

\item Our framework not only facilitates targeted repair of LLMs but also adapts dynamically during training by slicing the model as needed. 
\item Unlike prior techniques, our method introduces a threshold-free slicing approach, eliminating the need for costly tuning, which can be expensive for large models such as LLMs.

\item Our analysis shows that the source of errors can be more pronounced in specific areas of the model than in others, and targeted interventions can deliver more efficient repairs compared to indiscriminate approaches.
\item Our approach, \nick, significantly outperforms state-of-the-art techniques, demonstrating greater efficiency in error elimination while preserving model generation quality. Specifically, \nick reduces toxicity by 43.6\% more than the closest baseline, \textit{DPO}, while causing 46\% less disruption to overall performance.

\item We also demonstrate that a dynamic selection approach is essential for addressing errors dispersed throughout the model, ensuring a robust and efficient repair.
\end{itemize}


