\section{Background}
\label{sec:background}

LLMs have revolutionized NLP and SE tasks due to their ability to capture complex patterns and generate human-like text and code. In this section, we provide an overview of the GPT (Generative Pre-trained Transformer) architecture, which serves as a foundation for many modern LLMs~\cite{zhao2023survey}, including the models used in our study.

The GPT architecture, introduced by OpenAI \cite{radford2019language}, is based on the transformer model \cite{vaswani2017attention}. It consists of multiple layers of transformer blocks, which are the fundamental units of computation in the model. These blocks are also the primary focus of our slicing technique in \nick. Each block contains two main components:

\begin{itemize}
    \item \textbf{Multi-Head Attention}: This mechanism allows the model to focus on different parts of the input sequence simultaneously. Multi-head attention splits the input into multiple `heads,' each learning to attend to different input aspects. The attention function is defined as:

    \begin{equation}
        \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
    \end{equation}
    
    \noindent where $Q$, $K$, and $V$ are query, key, and value matrices, respectively, obtained from linear projections of the input sequence. $d_k$ is the dimension of the key vectors. The attention output is then passed through a feed-forward network (FFN).

    \item \textbf{Feed-Forward Neural Network (FFN)}: This component processes the output of the attention mechanism. It is a neural network that operates on each position in the input sequence independently. The FFN typically consists of two linear transformations with a non-linear activation function in between:
    
    \begin{equation}
        \text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
    \end{equation}
    
    \noindent where $W_1$, $W_2$, $b_1$, and $b_2$ are learnable parameters.
    
\end{itemize}

Each transformer block applies layer normalization and residual connections around these components.

