Software engineering research has proposed various techniques to repair bugs in deep neural networks (DNNs) that arise during training or within the network structure itself~\cite{zhang2019apricot,zhang2021autotrainer,wardat2022deepdiagnosis,ma2018mode}. Examples include Zhang \textit{et al.} 's method for monitoring DNN training and suggesting corrective actions for anomalies~\cite{zhang2021autotrainer}, and Wardat \textit{et al.}'s work on identifying and fixing structural bugs in DNNs~\cite{wardat2022deepdiagnosis}. However, these techniques primarily address issues stemming from the DNN itself. In contrast, data-driven errors in LLMs, such as toxicity or hallucinations, can stem from biases and inconsistencies within the training data itself~\cite{ji2023survey}, requiring solutions beyond structural or training bug fixes.

On the other hand, machine learning research offers several strategies to mitigate such errors, primarily operating in three stages~\cite{wang2022exploring,korbak2023pretraining,pan2023automatically}: runtime methods~\cite{liu2021dexperts,dathathri2019plug,wang2022exploring,schick2021self,leong2023self,yang2022unified,xu2022leashing,niu2024parameter,weng2023large},  the pre-training stage~\cite{korbak2023pretraining,liu2024exposing,huang2023survey}, and alignment stage~\cite{wang2022exploring,gehman2020realtoxicityprompts,lee2024mechanistic,liu2023chain,rafailov2024direct}. These approaches are often complementary, each with its own scope and applicability~\cite{huang2023survey}. 
Runtime methods aim to control model outputs through non-invasive techniques such as context augmentation, vocabulary modification, word filtering, and output post-processing~\cite{dathathri2019plug,wang2022exploring,huang2023survey}. These methods provide flexible runtime control, though they face certain constraints - they do not modify the underlying model architecture or weights when such modifications would be beneficial~\cite{wang2022exploring}. The additional computational overhead also presents challenges in latency-sensitive applications~\cite{wang2022exploring}. Prompt-based approaches for steering instruction-following models fall into this category as well, relying on prompt engineering techniques to mitigate undesirable responses ~\cite{ganguli2023capacity, xie2023defending}.

In contrast to runtime methods, pretraining-based approaches suggest removing problematic data from the training corpus or modifying model architecture~\cite{korbak2023pretraining,
liu2024exposing}. While effective, this can be prohibitively expensive, making them suitable for new model developments~\cite{wang2022exploring}. DAT methods that operate during the alignment stage, like ours, offer a preemptive approach to error mitigation. This is particularly suitable in scenarios where these limitations are especially impactful and require a more invasive repair~\cite{wang2022exploring, lee2024mechanistic}.



DAT methods aim to continue pretraining or fine-tuning models on domain-specific texts~\cite{gehman2020realtoxicityprompts,lee2024mechanistic}. For instance, Gehman \textit{et al.} \cite{gehman2020realtoxicityprompts} applied a framework called Domain-Adaptive Pretraining (DAPT)~\cite{gururangan2020don} to further pretrain GPT-2 on curated non-toxic data, reducing its toxicity. Similarly, Wang \textit{et al.} \cite{wang2022exploring} used the DAPT framework with self-generated data to detoxify models. However, their technique is model-dependent and relies on self-generated data, which may not be applicable in all error scenarios, such as factual inaccuracies or hallucinations. Solaiman and Dennison~\cite{solaiman2021process} proposed a general framework for aligning language models to specific target values, but it involves expensive iterative training~\cite{si2022prompting}. These methods aim to optimize pre-trained model parameters using domain-specific texts, mitigating errors while minimizing the impact on overall performance.

Reinforcement learning (RL) from human feedback (RLHF) is another paradigm within DAT methods that relies on human demonstration datasets to align language models, and it has been shown to mitigate errors in LMs~\cite{ouyang2022training}. The recently proposed Direct Preference Optimization (DPO) is a cutting-edge RL-inspired algorithm designed to overcome the instability and complexity of RLHF~\cite{rafailov2024direct}. In recent work, Lee \textit{et al.} \cite{lee2024mechanistic} demonstrated the effectiveness of the DPO framework in detoxifying the GPT-2 model using paired datasets of toxic and non-toxic examples.


However, these existing domain-adaptive techniques treat all model parameters uniformly during repair. This indiscriminate approach increases the risk of altering parameters unrelated to the specific errors being addressed, which can disrupt the general knowledge stored in those parameters. Such an ``intent-unaware'' approach not only risks harming overall performance but is also limited in effectively targeting the error-prone parts of the model. A more focused strategy could address these issues more efficiently by concentrating the repair effort where it is most needed. Our proposed technique, \nick, addresses these concerns by enabling a selective repair strategy.

Parameter-efficient fine-tuning (PEFT)~\cite{ding2023parameter} refers to a family of model alignment methods, such as LoRA~\cite{hu2021lora}, designed to optimize resource and computational efficiency during fine-tuning. In contrast, \nick emphasizes targeted adjustments to address specific errors in a modelâ€™s behavior. Like existing repair methods, intent awareness is not an explicit goal of PEFT methods either.


Knowledge editing (KE) is a related area within machine learning that focuses on updating a model's factual knowledge, allowing developers and end-users to modify the model beyond the training setup~\cite{mitchell2021fast,meng2022locating,wang2024detoxifying}. These techniques complement training-time methods by enabling model fixes during test time~\cite{mitchell2021fast}. While training-time methods aim for global correction, KE techniques provide localized fixes by updating the model's knowledge with a single instance~\cite{mitchell2021fast,meng2022locating,wang2024detoxifying}.









