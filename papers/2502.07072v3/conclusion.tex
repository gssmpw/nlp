\section{Conclusion}
\label{sec:conclusion}

In this paper, we introduce \nick, an intent-aware technique for selectively repairing data-driven errors in LLMs through dynamic model slicing. While domain-adaptive training with curated data has shown promise, it tends to optimize model parameters indiscriminately, which can limit repair efficacy and increase the risk of negatively affecting general model performance by altering unrelated parameters. To address these limitations, \nick identifies the relevant portions of the model responsible for the errors, allowing for more targeted repair and making it an intent-aware approach. Our method employs a gradient-based technique to select the most relevant parts of the model by analyzing sensitivity to slicing criteria. Unlike existing slicing routines, our technique is specifically designed to address transformer-related challenges and to avoid the need for expensive tuning of selection thresholds. In a case study focused on model detoxification, \nick demonstrated its effectiveness in addressing the root causes of toxicity while minimizing the impact on general performance, outperforming state-of-the-art baselines. Our empirical results also suggest that errors can be highly concentrated in very limited regions of the model, highlighting the need for selective repair. We further demonstrate that a dynamic selection-based repair strategy is essential for effectively addressing errors dispersed throughout the model.




