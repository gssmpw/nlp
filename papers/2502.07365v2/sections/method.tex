\section{Approach}
\label{sec:method}


Inspired by the above findings, we propose \textbf{Long} Context Pre-training with \textbf{Re}storation \textbf{D}istillation (\textbf{LongReD}), a novel context window extension framework to improve short-text capacities of long-context LLMs through decreasing distribution drift and mitigating catastrophic forgetting. Unlike only training on long texts, our strategy combines three different training objectives at each training step, \ie \textbf{long-text training}, \textbf{short-text distillation}, and \textbf{short-to-long distillation}. For the three objectives, we employ three datasets $\mathcal D_1,\mathcal D_2,\mathcal D_3$ with different lengths $T_l$, $T_s(<T)$, and $T$, respectively, where $T$ and $T_l$ are the original and target long context window length, and $T_s$ is the short text length. 
The overall architecture is displayed in Figure~\ref{fig:architecture}.


\begin{figure*}[htb]
    \centering
    \includegraphics[width=\linewidth]{figs/main.pdf}
    \caption{Overview of our proposed Long Context Pre-training with Restoration Distillation (LongReD). The method consists of three parts, \ie long-text training, short-text distillation, and short-to-long distillation.}
    \label{fig:architecture}
    \vspace{-0.3cm}
\end{figure*}
% Inspired by the observation that decreasing the distributional shifts during the context window extension can better preserve the short-text abilities, we propose a long context training strategy to balance the long and short-text capacities. 


\subsection{Long Text Training}

To extend the context window of an LLM, we follow previous work to first scale positional encodings via ABF~\cite{xiong-naacl-2024-effective} or PI~\cite{chen-arxiv-2023-extending} and directly continually training on the long text dataset. Given any long text $\bm{x}$ from the dataset $\mathcal D_1$, the model $\Theta_e$  is trained with the language modeling objective to minimize the cross-entropy loss $\mathcal{L}_{long}$ as follows:
\begin{equation}
    \mathcal{L}_{long} = - \sum_{\bm{x}\in \mathcal D_1} \frac 1 {T_{l}}\sum_{t=1}^{T_{l}} \log \operatorname{Pr}(x_t |\bm{x}_{<t}; \Theta_{e}).
\end{equation}
Through training on long texts, the model can learn to model long-term dependencies and adapt to the extended context window. 




\subsection{Short-Text Distillation}
Section~\ref{sec:analysis} demonstrates the negative effect of distribution drift and catastrophic forgetting on short-text capacities while only training on long texts. Though replaying short text data can partly mitigate the issue, the performance restoration is limited. Therefore, we propose short-text distillation, a knowledge distillation method to 
minimize the distribution discrepancy between the extended model $\Theta_e$ and the original model $\Theta_o$. 

\paratitle{Distillation Layer Selection.} To decrease the distribution discrepancy, we choose to distill the inner hidden states of LLMs. Yet, ensuring all the hidden states are the same as the original model is impossible and may significantly decrease the long text modeling capacities. Thus, we only select  $M(<L)$ layers as the distill layers $\mathbf{L}_M=\{l_1, \dots, l_M\},\forall l_i\in \{0,\dots, L\}$ and distill the outputs of these layers. Specifically, we propose an attention-based layer selection method where we first compute the KL divergence of attention scores between the extended and original models and then select layers with the largest KL divergences for distillation. We also select the critical last layer that directly determines the output~\cite{men-arxiv-2024-shortgpt}.


% We introduce two layer selection methods: (1) KL-based: We select the last layer and leverage the attention KL divergence introduced in Section~\ref{sec:metric} as the metric to find the layers whose attention distribution is most different from the original models. (2) Uniform-based: We first select the initial two layers and the last layer which have been found to be the most important layers~\cite{men-arxiv-2024-shortgpt}. Then, we uniformly select the middle layers between them.

% We also chose the critical last layer that directly determines the output~\cite{men-arxiv-2024-shortgpt}.

\paratitle{Short-Text Distillation Loss.}
Given an input sample $\bm{x}$ from the short text dataset $\mathcal D_2$, we input the sample into the two models with its original positional indices $\bm p=[0,\dots, T_s-1]$. Then, we obtain the output hidden states of the selected layers of the two models, \ie $\{\mathbf H_{l_1; \bm p, \Theta_{e}},\dots, \mathbf H_{l_M; \bm p, \Theta_{e}}\}$ and $\{\mathbf H_{l_1; \bm p, \Theta_{o}},\dots, \mathbf H_{l_M; \bm p, \Theta_{o}}\}$. We 
compute the cosine similarity between the hidden states of the two models at the same layer and position and sum up them as the short-text distillation loss $\mathcal{L}_{short}$,  formulated as follows:
\begin{equation}
    \mathcal{L}_{short} = -\sum_{\bm{x}\in \mathcal D_2} \sum_{i=1}^M\operatorname{Sim}( \mathbf H_{l_i; \bm p, \Theta_{e}}, \mathbf H_{l_i; \bm p,\Theta_{o}}).
\end{equation}
% where $\operatorname{Sim}$ denotes the cosine similarity operation in Section~\ref{sec:metric}. 



\subsection{Short-to-Long Distillation}
\label{sec:s2l_distillation}
% Though short-text distillation can mimic the working mechanism of the original model on short texts. However, the language modeling capacities on short texts can hardly transfer to long texts, and excessive constraints introduced by this stage may even harm the model's performance on long texts. Thus, to bridge the gap between long and short input, we add an extra short-to-long distillation loss.

% Though short-text distillation effectively mimics the working mechanism of the original model on short texts, its language modeling capabilities often fail to generalize to long texts. Moreover, the excessive constraints introduced during this stage may even degrade performance on long-text tasks. To address this limitation, we introduce an additional short-to-long distillation loss to bridge the gap between short and long texts.

The two objectives of long-text training and short-text distillation separately optimize the model on long and short texts, resulting in a challenging performance trade-off between long and short texts. We argue that long and short texts share some inherent linguistic commonalities. Thus, we propose an additional short-to-long distillation to transfer the short-text capabilities to long texts, further bridging the gap between short and long texts.

% During long-text training and short-text distillation, the model has learned to model long-term dependency and to mimic the working mechanism of the original model. However, there exists a large gap in that the two objectives may only focus on solely improving the long-text or short-text abilities while the two capacities may compete during the training. In addition, the general capacities of short texts may hardly transfer to long texts. To bridge the gap between long and short input, we add an extra short-to-long distillation loss.

\paratitle{Skipped Positional Indices.} To distill short-text capabilities to long texts, we take the short text with the original context window size $T$ as input but employ the skipped positional indices method, which can simulate long-text positions and capture longer-term dependencies with short texts~\cite{zhu-iclr-2024-pose,wu-nips-2024-cream}.
To be specific, for a sequence $\bm{x}$ drawn from $\mathcal{D}_3$, we first define a threshold $T_b$ to split the positional indices into three parts, \ie $\bm p_{head}$, $\bm p_{mid}$, and $\bm p_{tail}$:
% we first randomly split the sequences into 3 non-overlapped segments with the least length of 32 tokens ($K=8$ in our settings):
\begin{align}
    \bm p  &= \bm p_{head}\cup\bm p_{mid}\cup\bm p_{tail},\\ \bm p_{head} &= \{0, \dots, T_b-1\},\\ \bm{p}_{mid} &= \{T_b, \dots, T-T_b-1\},\\\bm p_{tail} &= \{T-T_b, \dots, T-1\}.
\end{align}
Then, we modify the positional indices of the three segments. We keep $\bm p_{head}$ unchanged but modify the end positional index of $\bm p_{tail}$ to be equal to the target long text length $T_l$. For the middle segment, we employ uniform sampling or CREAM~\cite{wu-nips-2024-cream} which upsamples middle positions to determine the end positional index of $\bm p_{mid}$ as $T_{me}\in \{T-T_b,\dots,  T_l-T_b-1\}$. And the start positional index of $\bm p_{mid}$ is equal to $T_{me}$ minus the length of this middle segment $T - 2T_b$, \ie $T_{me} - T + 2T_b$. The modified positional indices of the input text are formulated as follows:
\begin{align}
    \hat{\bm{p}}  &= \hat{\bm{p}}_{head}\cup\hat{\bm{p}}_{mid}\cup\hat{\bm{p}}_{tail},\\ 
    \hat{\bm{p}}_{head} &= \{0, \dots, T_b-1\},\\
    \hat{\bm{p}}_{mid} &=\{T_{me}-T+2T_b,\dots, T_{me}\},\\
    \hat{\bm{p}}_{tail} &= \{T_l-T_b, \dots, T_l-1\}.
\end{align}
Details of the skipped positional indices methods are introduced in Appendix~\ref{app:cream}.
% For the middle segments, we employ the truncated Gaussian function to determine the start position of $\mathbf \hat{p}_{mid}$, which 
% Then, we modify the positional indices of these segments. We employ a recursive approach to divide the target context window $T_l$ into 
% $K$ subspaces in a top-down manner. The beginning of each subspace $u_i$ will be set as the position indice of each segment. The positional indice corresponding to the $j$-th token of the $i$-th segment can be defined as follows:
% \begin{equation}
%     \operatorname{pos}_{i,j} = u_i+j.
% \end{equation}


\paratitle{Short-to-Long Distillation Loss.}
After obtaining skipped positional indices $\hat{\bm {p}}$, we input these indices into the extended model $\Theta_e$ while using the normal positional indices $\bm p$ for teacher model $\Theta_o$. Then, we obtain the outputs of the $L$-th last layer of both models and compute the cosine similarity between them. Then, we regard the negative cosine similarity as the loss to distill the output distribution into longer positions. 
The training objective $\mathcal{L}_{s2l}$ can be represented as follows:
% For a sample $\mathbf x_m$ from $\mathcal D_3$, we input the original positional indices $\mathbf p=\{0,\dots, T-1\}$ to the teacher model $\Theta_o$, while employing skipped positional indices to simulate the long context for the extended model $\Theta_e$. Following \citet{XXX}, we split the positional indices into three parts, a.
% \begin{equation}
%     \mathbf{p}' = \{\mathbf p_1, \mathbf p_2, \mathbf p_3\}
% \end{equation}
% Different from the short-text distillation, we only distill the output of the last layer in the loss to avoid the disturbance of latent positional information in the middle layers. 
% The training objective $\mathcal{L}_{s2l}$ can be represented as follows:
\begin{equation}
    \mathcal{L}_{s2l} = -\sum_{\bm{x}\in \mathcal D_3}  \operatorname{Sim}( \mathbf H_{L; \hat{\bm{p}}, \Theta_{e}}, \mathbf H_{L; \bm p,\Theta_{o}}).
\end{equation}
Notably, we only distill the output of the last layer instead of selected layers in short-text distillation to avoid the disturbance of latent positional information in the middle layers. 
% \paragraph{Skipped Positional Indices.}

\subsection{Joint Training Objective}

Finally,  we aggregate losses from these three training objectives. For each batch of training data, we sample data points from the three datasets with a fixed ratio. Then, we independently compute loss through the three different objectives on the corresponding data and sum them up to balance the long and short text capacities. The final loss $\mathcal{L}_{final}$ can be represented as follows:
\begin{equation}
    \mathcal{L}_{final} = \mathcal{L}_{long} + \alpha_1 \mathcal{L}_{short} + \alpha_2 \mathcal{L}_{s2l},
\end{equation}
where $\alpha_1$ and $\alpha_2$ are hyper-parameters that control the proportion of the three parts.