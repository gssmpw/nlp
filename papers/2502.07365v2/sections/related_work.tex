\section{Related Work}
\label{sec:related_work}
\paratitle{Context Window Extension.}
LLMs typically have constrained context windows owing to positional encodings and attention mechanisms~\cite{press-iclr-2022-alibi,han-arxiv-2023-lm}. In order to meet the growing demand for processing long texts, various methods have been proposed to extend the context windows of LLMs based on modification of RoPE~\cite{su-neurocomputing-2024-roformer}. Positional Interpolation down-scale positional indices to prevent out-of-distribution positions. In addition, NTK~\cite{bloc97-reddit-2023-ntk,emozilla-reddit-2023-dynamicntk}, ABF~\cite{xiong-naacl-2024-effective}, Yarn~\cite{peng-arxiv-2023-yarn}, and LongRoPE~\cite{ding-icml-2024-longrope} modify the base of RoPE to control the rotation angles of different dimensions. After modifying positional encodings, LLMs are lightweightly continued pre-training on long texts~\cite{fu-icml-2024-data}. Through this pipeline, the context windows of LLMs can be extended to 128K or even 1M tokens~\cite{Dubey-arxiv-2023-llama3,Zeng-arxiv-2024-glm}. Typically, they can achieve utilization of long texts within their context windows at the cost of degradation of general capacities on short texts~\cite{Dubey-arxiv-2023-llama3}. Our work mainly focuses on the reasons for the degradation of short-text capacities and approaches to mitigating the decline. 


\paratitle{Knowledge Distillation.} Knowledge distillation serves as an approach to transferring knowledge and abilities of teacher models to student models~\cite{Hinton-arxiv-2015-Distilling,Xu-arxiv-2024-Survey}. During the era of LLMs, leveraging black-box and powerful LLMs, \eg GPT-4~\cite{openai-arxiv-2023-gpt4}, to generate instructions and even fine-grained reasoning path to training smaller LLMs is a common and effective method to enhance general and domain-specific capacities~\cite{vicuna2023,peng-arxiv-2023-instruction,mukherjee-arxiv-2023-orca,ho-acl-2023-large}. In addition, distilling the inner distribution of white-box LLMs can be another effective way. Divergence-based distillation methods minimize the divergence of the output probability distribution of teacher and student models~\cite{Gu-ICLR-2024-Minillm,agarwal-iclr-2024-on,jiang-arxiv-2024-MixCPT}. Similarity-based distillation methods align the intermediate hidden states, enabling the student models to process input like the teacher models~\cite{liang-icml-2023-less, Muralidharan-arxiv-2024-Compact}. Different from these methods of transferring knowledge of powerful models to small or compressed models, our method aims to restore and preserve the original capacities of LLMs during the context window extension stage. 
