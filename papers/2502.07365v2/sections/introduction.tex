\section{Introduction}
\label{sec:introduction}


\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/degradation.pdf}
    \caption{Comparison of the original and long-context models via ABF~\cite{xiong-naacl-2024-effective} or PI~\cite{chen-arxiv-2023-extending} on common short-text benchmarks.}
    \label{fig:drop}
    % \vspace{-0.3cm}
\end{figure}
Large language models (LLMs) have exhibited remarkable performance across a wide range of text and multimodal tasks~\cite{brown-nips-2020-gpt3,openai-arxiv-2023-gpt4,zhao-arxiv-2023-survey,Touvron-arxiv-2023-llama2,Dubey-arxiv-2023-llama3}. However, their abilities to process long contexts are constrained by the positional encodings and the attention mechanism, which define the context window size based on the length of the pre-training data~\cite{su-neurocomputing-2024-roformer,press-iclr-2022-alibi}. When the input text exceeds this context window, the model encounters out-of-distribution positional information, resulting in significant performance degradation~\cite{chen-arxiv-2023-extending,peng-arxiv-2023-yarn,dong-arxiv-2024-exploring}.


% \vspace{-10pt}

To address the challenges of long context processing, various methods have been proposed to extend the context window of LLMs~\cite{bloc97-reddit-2023-ntk,xiao-arxiv-2023-streaming,xiong-naacl-2024-effective,chen-arxiv-2023-extending}. A prominent approach involves scaling positional encodings combined with lightweight continual pre-training. Leveraging the properties of RoPE~\cite{su-neurocomputing-2024-roformer}, these methods interpolate positional indices or adjust the RoPE base to prevent out-of-distribution rotary angles beyond the original context window~\cite{chen-arxiv-2023-extending, ding-icml-2024-longrope, peng-arxiv-2023-yarn,xiong-naacl-2024-effective}. Subsequently, lightweight continual pre-training adapts LLMs to the extended context and modified encodings, enabling context window up to 128K or even 1M tokens~\cite{xiong-naacl-2024-effective,fu-icml-2024-data,Zeng-arxiv-2024-glm}. 
However, as shown in Figure~\ref{fig:drop}, these long context window extension techniques come at the cost of degraded performance on short-text tasks~\cite{xiong-naacl-2024-effective}. Yet, the root causes of this performance decline and potential mitigation strategies remain under-explored.


% Though the context window can be extended to 128k or even 1M tokens, these strategies will introduce degradation performances on common short-text tasks, as shown in Figure~\ref{fig:drop}.  
% Positional Interpolation (PI) scales down positional indices to keep them within the original window size. In contrast, methods such as NTK~\cite{bloc97-reddit-2023-ntk}, ABF~\cite{xiong-naacl-2024-effective}, Yarn~\cite{peng-arxiv-2023-yarn}, and LongRoPE~\cite{ding-icml-2024-longrope} modify the RoPE base to adjust the rotation angles across different dimensions. Following such modifications, lightweight continual pre-training is applied to adapt LLMs to longer context windows and altered positional encodings~\cite{xiong-naacl-2024-effective,fu-icml-2024-data}. These strategies enable efficient expansion of the context window from a few thousand tokens to as many as 128K or even 1M tokens~\cite{Dubey-arxiv-2023-llama3,Zeng-arxiv-2024-glm}. However, it will introduce degraded performance on short-text tasks~\cite{xiong-naacl-2024-effective}, while the underlying causes of performance decline and effective strategies to mitigate it remain insufficiently explored.


% These approaches can be directly applied to existing LLMs without training or with only lightweight continual pre-training, enabling them to handle texts exceeding their original context window~\cite{openai-arxiv-2023-gpt4,xiong-naacl-2024-effective}. By introducing simple modifications to the positional encodings or attention mechanisms, these training-free methods achieve effective expansions of the context window while suffering from poor performances or heavy computational costs~\cite{bloc97-reddit-2023-ntk,emozilla-reddit-2023-dynamicntk,dong-arxiv-2024-exploring,xiao-arxiv-2023-streaming,wang-arxiv-2024-length}. In practice, lightweight continual pre-training with scaled positional encodings is the mainstream approach for building long-context LLMs.


In this study, we aim to demystify the factors contributing to the performance degradation of short-text tasks after context window extension. Our analysis identifies two critical factors: \textbf{distribution drift} and \textbf{catastrophic forgetting}. The key findings of our work include: (1) Continual pre-training seeks to recover the original model's internal distribution, but the restoration is inherently imperfect. (2) Distribution shift in hidden states potentially leads to performance degradation. (3) During continual pre-training, the performance on short-text tasks initially improves but subsequently declines as training progresses, highlighting the presence of catastrophic forgetting. (4) Replaying short text data
% and maintaining a balanced data mixture 
is effective in mitigating forgetting and improving performance stability.



% In this work, we first analyze the effect of continual pre-training in context window extension. We hypothesize that LLMs have obtained powerful capacities and diverse knowledge during the pre-training stage and store these in the parameters. The role of continual pre-training during context window extension includes three dimensions, \ie restoration of disturbed attention scores, rematching of parameters and hidden states, and adaptation for extended context window. 

Based on our observations, we propose a novel approach called \textbf{Long} Context Pre-training with \textbf{Re}storation \textbf{D}istillation (\textbf{LongReD}) to mitigate the degradation in short-text capabilities of long-context LLMs. The central idea is that the short-text capacities of the extended model can be better preserved if it accurately simulates the original distributions before extension. To achieve this, in addition to typical \textbf{long-text training}, we introduce a \textbf{short-text distillation} objective, which employs the original model as a teacher to distill its hidden states on short texts into the extended model. This training objective minimizes distribution drift and alleviates catastrophic forgetting.
Moreover, we propose a \textbf{short-to-long distillation} training objective to bridge the gap between short-text distillation and long-text training. In this setup, the original and extended models are fed with normal positional indices and skipped positional indices, respectively. 
By applying the distillation on the output distributions of the last layer, the short-text capacities can be effectively transferred and integrated into long-text processing.


To the best of our knowledge, this work represents the first systematic analysis of the reasons behind the performance degradation of long-context LLMs on short-text tasks. Furthermore, we propose a general method to mitigate this short-text degradation. To assess the effectiveness of our method, we extend the context window of Llama-3-8B and Mistral-7B-v0.3 and assess their performance on both short-text and long-text tasks. Experimental results demonstrate that our method preserves the original models' performance on short-text tasks while maintaining or even improving their long-context modeling capabilities.

% We design an auxiliary loss, which minimizes the distribution shift of the hidden states of the original and extended models. Specially, the loss maximizes the cosine similarity of the hidden states of selected layers between t


% Our contributions can be summarized as follows:
% \begin{itemize}
%     \item We are the first to analyze the reasons for short-text abilities drop for long-context LLMs. We attribute the issues to two factors, \ie distribution drift and catastrophic forgetting. 
%     \item We propose Layer-wise Restoration Knowledge Distillation (LRKD), an auxiliary training method to minimize the distribution shift of hidden states between extended and original models.
%     \item Experiments demonstrate that our method can effectively alleviate the degradation of performances on short text tasks without harming the long text modeling capacities during the context window extension stages.
% \end{itemize}