\section{Background}
\label{sec:background}

\subsection{Transformer}
Owing to strong capabilities and scalability, Transformer decoders have served as the backbone of most LLMs~\cite{vaswani-nips-2017-attention, Touvron-arxiv-2023-llama2,Dubey-arxiv-2023-llama3}. Given a Transformer decoder with $L$ layers and $N$ heads and an input sequence $\bm{x} = \{ x_1, \dots, x_T \}$ consisting with $T$ tokens, the output of the $l$-th layer can be denoted as $\mathbf{H}_l = \{\bm{h}_{l,1}, \dots, \bm{h}_{l,T}\}$. Each Transformer layer consists of a multi-head attention (MHA) module and a feed-forward network (FFN) module with residual connections connecting them, as shown by the following formula:
\begin{align}
    \widetilde {\mathbf H}_l &= \mathrm{MHA}(\mathbf H_{l-1})+\mathbf H_{l-1},\\\mathbf H_l &= \mathrm{FFN}(\widetilde {\mathbf H}_l) + \widetilde {\mathbf H}_l.
\end{align}

In the $i$-th head of the MHA module at the $l$-th layer, the hidden states are first projected into query, key, and value matrices, \ie $\mathbf{Q}_l^{i}$, $\mathbf{K}_l^{i}$, and $\mathbf{V}_l^{i}$. Positional information is then incorporated into the query and key matrices through the RoPE with the rotation matrix $\mathbf{R}_{\theta}$ ($\theta$ is the RoPE base)~\cite{su-neurocomputing-2024-roformer}. These matrices are subsequently processed through a dot product followed by a softmax operation to compute the attention scores $\mathbf{A}_l^{i}$. Finally, the value representations are weighted by the attention scores, and all attention heads are concatenated and projected to produce the attention output of the $l$-th layer as follows:
\begin{align}
    \mathbf A_l^{i} &= \mathrm{Softmax}({\mathbf Q_{l}^{i}}^\intercal \mathbf R_{\theta} {\mathbf K_{l}^{i}}/\sqrt d ),\\
    \mathrm{MHA}(\mathbf H_{l-1}) &= \mathrm{Concat}(\{\mathbf A_l^{i} \mathbf V_l^{i}\}_{i=1}^N)\mathbf W^O,
\end{align}
where $d$ is the dimension of query and key, $\mathbf W^O$ is the projection matrix, and $\mathrm{Concat}$ is the concatenation of hidden states.

% \subsection{RoPE Extension(can delete)}
% \label{sec:rope_extension}
% Rotary position embedding (RoPE)~\cite{su-neurocomputing-2024-roformer} is a widely adopted positional encoding of current LLMs. Specifically, RoPE assigns each query and key a rotary matrix according to the absolute positions, and the products of the keys and queries acquire the relative positional information. The rotary matrix $\mathbf R_{\mathbf \Theta,t}$ takes each consecutive pair
% of elements in queries and keys as a sub-space and each sub-space $i$ has a corresponding basis $\theta_i$. Then, the rotation angle in each sub-space is equal to the product of basis $\theta_i$ and the positional indices $t$, \ie $\theta_i \cdot t$. For most LLMs, the basis is defined as the exponent of the base $b$:
% \begin{equation}
% \mathbf \Theta = \{\theta_i = b^{-2i/d}|i \in \{0,1,\dots, d/2-1\}\},
% \end{equation}
% where $b=10000$ for Llama-2 and $b=500000$ for Llama-3~\cite{Touvron-arxiv-2023-llama2, Dubey-arxiv-2023-llama3}.

% However, larger positional indices will lead to larger rotation angles, further leading to sharp performance degradation~\cite{chen-arxiv-2023-extending}. To extend the context window, positional interpolation (PI) down-scale the positional indices $t$ to $t/\lambda$ according to the scale factor $\lambda$~\cite{chen-arxiv-2023-extending}. Focusing on sub-spaces without experiencing a rotation circle during pre-training, NTK~\cite{bloc97-reddit-2023-ntk} and Yarn~\cite{peng-arxiv-2023-yarn} decrease the basis $\theta_i$ on these sub-spaces to prevent larger angles than training. To be specific, NTK directly increases the base $b$, while Yarn takes different operations on different sub-spaces.


\subsection{Measures of Distributional Discrepancy}
\label{sec:metric}


After extending the context window, the parameters of LLMs undergo changes, resulting in a shift in the distribution of hidden states. To quantify such distributional discrepancy, we propose two measures, \ie \textbf{hidden state similarity} and \textbf{attention Kullback-Leibler (KL) divergence}. Specifically, given the hidden states $\mathbf{H}_l$, $\mathbf{\widehat{H}}_l$ from the original and extended models, hidden state similarity measures the average cosine similarity between hidden states at the $l$-th layer across all positions. Similarly, attention KL divergence calculates the average KL divergences between the $i$-th attention head distributions $\mathbf{A}^i_l$, $\mathbf{\widehat{A}}^i_l$ from the original and extended models at the $l$-th layer. Higher hidden state similarity or lower attention KL divergence indicates less discrepancy between the two models. These metrics are formulated as follows:
% The formal definitions of these metrics are provided below:
\begin{align}
    \operatorname{Sim}(\mathbf{H}_l, \mathbf{\widehat{H}}_l) &= \frac 1 T\sum_{t=1}^T \frac { {\bm{h}_{l,t}}^\intercal \bm{\widehat{h}}_{l,t}}{\lVert \bm h_{l,t}\rVert\lVert \bm{\widehat{h}}_{l,t}\rVert},\\
    \operatorname{KL}(\mathbf{A}_l^{i},\widehat{\mathbf{A}}_l^{i}) &= \frac 1 T \sum_{t=1}^T \sum_{j=1}^t a^{i}_{l,t,j} \log \frac {a^{i}_{l,t,j}}{\hat a^{i}_{l,t,j}}.
\end{align}

% In addition, we also introduce attention entropy, a metric to measure the uniformity of attention distribution, as shown in the following:
% \begin{equation}
%     \operatorname{Entropy}(\mathbf {a}^{s,i}_{l,t}) = -\sum_{j=1}^t a^{s,i}_{l,t,j}\log a^{s,i}_{l,t,j}
% \end{equation}
% Typically, a lower attention entropy indicates that most attention is focused on a small number of tokens.

% \begin{itemize}
%     \item \emph{hidden state similarity}: 

%     \begin{equation}
        
%     \end{equation}
% \end{itemize}