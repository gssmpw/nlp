\section{Empirical Analysis}
\label{sec:analysis}

Prior work~\cite{xiong-naacl-2024-effective,ding-icml-2024-longrope} demonstrated that the short-text capacities will degrade after lightweight continual pre-training for extending the context window of LLMs. However, the secrets behind the performance decline have not been fully explored. In this section, through conducting empirical analysis, we attribute the performance degradation to two factors, \ie \emph{distribution drift} and \emph{catastrophic forgetting}. 


% \subsection{Experiment Settings}

% We conduct our experiments with Llama-3-8B. We extend its context windows to 32K and 128K with different RoPE scaling 




\subsection{Distribution Drift}
\label{sec:drift}

% Previous work demonstrates that LLMs gain long context utilization capacities during the pre-training stage and are only adapted for extended context windows during the extension stage~\cite{fu-icml-2024-data}. Thus, only lightweight continual pertaining is enough for adapting to long context windows. Here, we hypothesize that the knowledge and capacities of LLMs are stored in the parameters, and the modifications of RoPE prevent the model from fully utilizing the knowledge and capabilities within their parameters. We further hypothesize that the lightweight continual pre-training serves three roles, \ie adapting for long context windows, restoration of hidden states, and catastrophic forgetting.

% restoration of disturbed attention scores, rematching of parameters and hidden states, and adaptation
% for extended context window. 

% \paragraph{Experimental Settings.} We conduct our experiments with Llama-3-8B~\cite{Dubey-arxiv-2023-llama3}. We extend its context window on data selected from SlimPajama dataset~\cite{soboleva-huggingface-2023-slimpajama} following~\citet{fu-icml-2024-data}. We modify the RoPE theta to $2e7$ and $1e8$, and train Llama-3-8B on sequences with lengths of 32K and 128K, respectively, denoted as Llama-3-8B-32K and Llama-3-8B-128K. We also train models on data with a length of 32K with different window extension methods and training strategy.


\paratitle{Imperfect Distribution Restoration.}
In previous work~\citep{chen-arxiv-2023-extending, ding-icml-2024-longrope}, RoPE configurations are simply modified to extend the context window, undoubtedly leading to the change of distributions. Although continual pre-training is proposed to adapt LLMs to the extended context, a natural question is whether continual pre-training can eliminate the distributional discrepancy. To verify this, we modify 
% $\theta$ in RoPE 
RoPE base to $2e7$ and $1e8$, and train Llama-3-8B (with context window size of 8192 tokens) on 1B tokens with the sequence length of 32K and 128K, respectively, denoted as Llama-3-8B-32K and Llama-3-8B-128K.  Besides, we select Llama-3-8B-Instruct-262K~\cite{gradient-hf-2024-longcontextllama3}, which is trained on Llama-3-8B-Instruct. 
To measure the distributional discrepancy, we employ these models before and after continual pre-training for inference on 1000 test samples with the length of 8192 tokens from SlimPajama~\cite{soboleva-huggingface-2023-slimpajama}. The results of hidden state similarity and attention KL divergence are shown in Table~\ref{tab:restoration}. As can be seen, as the 
% RoPE $\theta$ 
RoPE base increases, the extended LLMs before continual pre-training exhibit lower hidden state similarity and higher KL divergence from the original models. After training on long-context data, the similarity increases, and the KL divergence declines to some extent. However, \textbf{despite the LLM striving to restore the inner distribution resembling the original model during continual pre-training, there still exists a certain degree of distributional discrepancy.}




% We directly employ different RoPE scaling methods with different scaling factors on Llama-3-8B-Instruct without further training and compare them with models after training. We infer these models on 1000 samples with the length of 8192 tokens from RedPajama~\cite{together-github-2023-redpajama} to obtain the hidden states and attention scores. Then, we assess the shift of hidden states through these metrics. Figure~\ref{} shows ....

\begin{table}[htb]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccc}
    \toprule
         \multirow{2}{*}{Model} & \multirow{2}{*}{Base} & \multicolumn{2}{c}{Simi $\uparrow$} & \multicolumn{2}{c}{KLD($\times 10^{-5}$) $\downarrow$}   \\
         & & before &after&before&after \\\midrule
         Llama-3-8B-32K&2e7& 0.92&0.95&3.51&1.51\\
         Llama-3-8B-128K&1e8&  0.83&0.94&6.74&1.68\\
         Llama-3-8B-Ins-262K&2.8e8& 0.76&0.91&8.75&2.50\\\bottomrule
    \end{tabular}}
        \caption{Results of hidden state similarity and KL divergence before and after continual pre-training.}
    \label{tab:restoration}
\end{table}



\paratitle{Relationship between Distribution Drift and Performance Degradation.} Owing to the distribution drift of the extended model, the inner working mechanisms may also change when processing the original short text data. 
% Thus, the drift of distribution may unavoidably lead to the performance change on short-text tasks.
% This naturally raises the question: \textbf{Does this imperfect restoration contribute to the observed performance decline?} 
To answer the question of whether the distributional discrepancy contributes to the performance decline in short text, we train the Llama-3-8B model using various context window extension methods and training strategies (detailed in Appendix~\ref{app:drift_models}). Additionally, we incorporate existing open-source long-context models, which were extended from Llama-3-8B-Instruct by Gradient AI~\cite{gradient-hf-2024-longcontextllama3}. We compute the average hidden state similarity across all layers between these extended models and their original counterparts and assess their overall performance on the MMLU benchmark (mainly focused on short-text tasks)~\cite{Hendrycks-iclr-2021-mmlu}. Then, we compute the MMLU performance preservation ratio of long-context models compared to the original models \emph{w.r.t.} the hidden state similarity between them, as shown in Figure~\ref{fig:mmlu_restoration}. Generally, extended models that maintain higher similarity to their original versions preserve more short-text performance. This finding highlights that \textbf{distribution shifts in extended models are a significant factor contributing to performance degradation in short text.}

%
\begin{figure}[htb]
   \centering
    \includegraphics[width=\linewidth]{figs/mmlu_restoration.pdf}
    \caption{Relationship between MMLU performance preservation of long-context models \emph{w.r.t.} the hidden states similarity.}
    \label{fig:mmlu_restoration}
\end{figure}





% We choose three metrics introduced in Section~\ref{} to assess the discrepancy of hidden states with the original models. 





\subsection{Catastrophic Forgetting}
\label{sec:forgetting}

Catastrophic forgetting is always a critical problem during continual pre-training~\cite{wu-arxiv-2024-continual}. Owing to the distributional difference of short texts and long texts, adapting for long context window may unavoidably leads to a trade-off on short texts. 
% Owing to the difference in data lengths and the mixture of domains, the process of context window extension may lead to the forgetting issue. 
In this section, we examine the effect of the training steps and training data on the short-text performances to verify the forgetting phenomenons. 

\paratitle{Effect of Training Steps.} We train Llama-3-8B with the RoPE base of $2e7$ on the SlimPajama dataset and obtain checkpoints with different training steps, where the batch size is $64$ and the training length is 32K. Then, we evaluate the models on four short-text benchmarks, \ie MMLU~\cite{Hendrycks-iclr-2021-mmlu}, HumanEval~\cite{chen-arxiv-2021-humaneval}, TriviaQA~\cite{Joshi-acl-2017-triviaqa}, and PIQA~
\cite{bisk-aaai-2020-piqa}. Figure~\ref{fig:step} presents the performance change with different training steps. It is clear that the performance is restored at the initial steps (usually less than 32 steps) in spite of fluctuations in some datasets. However, the performance gradually drops with the increase of training steps, which demonstrates that\textbf{ the continual adaptation for long context window will lead to catastrophic forgetting issues on short-text tasks.}


\begin{figure}[htb]
    \centering
    \includegraphics[width=
\linewidth]{figs/step.pdf}
    \caption{Results of models with different training steps.}
    \label{fig:step}
    % \vspace{-0.5cm}
\end{figure}

% Figure~\ref{fig:step} also presents the MMLU performance of checkpoints with different steps. It's clear that the performance on MMLU drops with the increase in training steps, which demonstrates \textbf{the catastrophic forgetting of short text capacities serves the cost of adapting for long contexts.}

\paratitle{Effect of Training Data Mixture.} Beyond the training steps, we also evaluate the effect of mixtures of training data. 
% We replace the domain-mixed data obtained from SlimPajama following~\citet{fu-icml-2024-data} with web data from book data from PG-19~\cite{rae-iclr-2020-pg19} and code data from Stack-v2~\cite{Lozhkov-arxiv-2024-stack}, which are most common domains of long training data. 
We replace half of the long text data with short text data with a length of 8K. We train Llama-3-8B on 1B tokens and evaluate them on different benchmarks. As shown in Table~\ref{tab:data_effect}, the length of training data plays a critical role in downstream tasks. 
% Replaying data from a single domain will only improve the related capacity while harming other abilities. However, keeping a balanced domain mixture of data can balance different capacities. In addition, 
Introducing short-text data achieves better short-text performances than only training on long-text data. This verifies that \textbf{long texts are critical for forgetting problems, which can be mitigated by replaying short texts.}

% replaying short texts during context window extension can migrate the forgetting issue and the .

% achieve better restoration of the short-text capacities. 

\begin{table}[htb]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cccc}
    \toprule
     Length & MMLU & HUMANEVAL & PIQA &TriviaQA\\\midrule
       Long & 62.0&14.02& 74.10 & 70.67\\
        Long+Short & 62.5&16.46& 78.24 & 72.82 
       % Web & Long & 62.4 \\
       % Book & Long & 61.3&12.80\\
       % Code & Long & 62.5&24.39
         \\\bottomrule
    \end{tabular}}
    \caption{Results of models trained with data mixtures with different lengths.}
    \label{tab:data_effect}

\end{table}

% \vspace{-0.3cm}
%