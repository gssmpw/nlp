\section{Experiment}
\label{sec:experiment}


% Please add the following required packages to your document preamble:
% \usepackage{multirow}

\begin{table*}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccccccccc|c|c} \toprule
         Model&  CW&  PE&  Data&  Method&  General&  Coding&  Math&  RC&   Common&Short Avg.&RULER &Avg.\\\midrule
 \multirow{13}{*}{\makecell{Llama-3\\-8B}}& 8K& -& -& -& 68.18& 41.20& 30.12& 63.78& 72.54& 55.16&- &-\\ \cmidrule{2-13}
 & 32K& ABF& Long& CPT& 65.39& 29.32& 29.50& 60.98& 69.83& 51.00&82.80&56.30\\ 
 & 32K& ABF& Mix& CPT&66.04 & 27.40& 29.32& 34.65& 71.88& 45.86&\textbf{85.04} &52.39\\
 & 32K& ABF& Mix& LongReD-C& \textbf{67.65}& \textbf{39.90}& \textbf{31.52}& \textbf{62.84}& 72.35& \textbf{54.85}&84.98&\textbf{59.87}\\
 & 32K& ABF& Mix& LongReD-U& 67.05& 39.71& 30.90& 62.51& \textbf{72.61}&  54.56 &84.08&59.48\\\cmidrule{2-13}
 & 32K& PI& Long& CPT& 63.84& 27.13& 26.18& 61.95& 67.71& 49.36&\textbf{82.17} &54.83\\ 
 & 32K& PI& Mix& CPT& 64.22& 27.29& 29.16& 36.63& 71.09& 45.68&81.63&51.67\\ 
 & 32K& PI& Mix& LongReD-C& \textbf{66.45}& {38.39}& {30.78}& \textbf{62.79}& {71.30}& 53.94&81.19&58.48\\
 & 32K& PI& Mix& LongReD-U& 66.34&\textbf{ 38.83}& \textbf{31.68}& 62.76& \textbf{72.36}& \textbf{54.39}&79.30&\textbf{58.54}\\\cmidrule{2-13}
 & 128K& ABF& Long& CPT& 64.06 & 30.04& 27.72& 60.49& 68.37& 50.14&\textbf{69.70} &53.40\\
 & 128K& ABF& Mix& CPT& \textbf{66.39}& 14.03& 29.02& 35.28& \textbf{71.91}& 43.33&69.64&47.71\\
 & 128K& ABF& Mix& LongReD-C& 65.99& \textbf{40.03}& \textbf{30.26}& 62.39& 71.50& \textbf{54.03}&64.93&55.85\\
 & 128K& ABF& Mix& LongReD-U& 65.61& 39.87& 29.67& \textbf{62.47}& 71.64& 53.85&68.41&\textbf{56.28}\\
 \midrule
 \multirow{5}{*}{\makecell{Mistral\\-7B-v0.3}}& 32K& -& -& -& 65.36& 33.20& 26.98
& 68.07& 61.86& 51.09& -&-\\\cmidrule{2-13}
  & 128K& ABF& Long& CPT& 55.29& 26.37& 15.79
& 52.93& 53.07& 40.68& 44.63&41.35\\
 & 128K& ABF& Mix& CPT& 55.99& 26.36& 15.30& 60.21& 52.74& 40.66& 45.76&41.51\\
 & 128K& ABF& Mix& LongReD-C& 61.45& 26.66& \textbf{19.88}
& 65.25& 58.26& 46.30& \textbf{58.37}&48.31\\
  & 128K& ABF& Mix& LongReD-U& \textbf{62.42}& \textbf{29.64}& 19.56& \textbf{66.86}& \textbf{59.98}& \textbf{47.69}& 53.60&\textbf{48.68}\\
 \bottomrule
    \end{tabular}}
    \caption{Comparison of performances of short-text and long-text benchmarks of our methods with other baselines. CW denotes the context window length, PE denotes the scaling method of RoPE, RC denotes reading comprehension, Common denotes commonsense question answering, Short Avg. denotes averaged scores on short benchmarks, and Avg. denotes averaged scores of all benchmarks. LongReD-C and LongReD-U denote our method with different skipped positional indices methods, \ie CREAM and Uniform Sampling.}
    \label{tab:results}
    % \vspace{-0.3cm}
\end{table*}
\subsection{Experimental Settings}

\paratitle{Pre-training Setup.}
In the experiments, we choose Llama-3-8B~\cite{Dubey-arxiv-2023-llama3} and Mistral-7B-v0.3~\cite{jiang-arxiv-2023-mistral} to evaluate the effectiveness of our methods on context window extension. 
We sample the long text dataset $\mathcal{D}_1$ and short-to-long dataset $\mathcal{D}_3$ from SlimPajama following the setting of \citet{fu-icml-2024-data}. Inspired by~\citet{gao-arxiv-2024-how}, we employ higher-quality data as the short text dataset $\mathcal{D}_2$ with the length of 1K. Then, we trained our model on these three datasets with a token quantity ratio of 4:3:1 and a total of 1B tokens. The number of distillation layers $M$ is set to 3 for extending Llama-3-8B to 128K context window size and 6 for other settings for better balancing the short and long text modeling performances.
% After the continual pre-training, we randomly select 10k conversations from TULU-V2-mix and instruction-tuning these models. 
The details of training and parameter configurations and data mixture are displayed in Appendix~\ref{app:train_details}.



% All the models are continually pre-trained on sampled data on SlimPajama~\cite{soboleva-huggingface-2023-slimpajama} following~\citet {fu-icml-2024-data}, while the instructed models also undergo data replay with chat-style data to recover the instruction-following capacities following~\citet{}.


\paratitle{Evaluation Benchmarks.}
To thoroughly evaluate the short-text performance of long-context models, we choose 17 benchmarks covering 5 capacities, \ie general, coding, math, reading comprehension, and commonsense question answering. We also select RULER~\cite{Hsieh-arxiv-2024-RULER} to assess the long text processing abilities. The evaluation datasets and details are shown in Appendix~\ref{app:evaluation}.
% (MMLU~\cite{Hendrycks-iclr-2021-mmlu}, BBH~\cite{suzgun-acl-2023-bbh}, LAMBADA~\cite{paperno-acl-2016-lambada}), math (MATH~\cite{henderycksnips-2021-math}, GSM8K~\cite{cobbe-arxiv-2021-gsm8k}), coding (HumanEval~\cite{chen-arxiv-2021-humaneval}, MBPP~\cite{austin-arxiv-2021-mbpp}), reading comprehension (Squadv2~\cite{rajpurkar-EMNLP-2016-squad}, Quac~\cite{choi-acl-2018-quac}, TriviaQA~\cite{Joshi-acl-2017-triviaqa}, Drop~\cite{dua-naacl-2019-drop}, BoolQ~\cite{clark-naccl-2019-boolq}), Commonsense (Openbookqa~\cite{todor-emnlp-2018-openbookqa}, Commonsenseqa~\cite{talmor-naacl-2019-commonsenseqa}, ARC-C~\cite{clark-arxiv-2018-arc}, SIQA~\cite{Sap-emnlp-2019-siqa}, PIQA~\cite{bisk-aaai-2020-piqa}), and long text processing (RULER~\cite{Hsieh-arxiv-2024-RULER}). The evaluation details are shown in Appendix~\ref{app:evaluation}.


\paratitle{Baselines.}
In our experiments, we choose two baseline methods for comparative analysis: (1) \emph{Long+CPT}: Only continual pre-training on long text datasets $\mathcal D_1$;
(2) \emph{Mix+CPT}: Continually pre-training on a mixed-length dataset ($\mathcal{D}_1\cup \mathcal{D}_2\cup\mathcal{D}_3$),  keeping the training data the same as our method. 
% To evaluate the general capacities and long-context capacities of LLMs, we 

% 12 short-text and 2 long-text benchmarks. We evaluate the general (MMLU, AGIEVAL, BBH), math (MATH, GSM8K), coding (HumanEval, MBPP), reading comprehension (), and commonsense reasoning () capacities for short texts, and Needle in a haystack and RULER for long texts.


\subsection{Main Results}


Table~\ref{tab:results} presents the performance comparison between our proposed methods and various baselines on both short-text and long-text tasks. Detailed results for each dataset can be found in Appendix~\ref{app:all_results}.

First, LongReD performs better on short-text tasks and achieves competitive long-text modeling performances compared to baseline methods. Across various scaling methods and target context window sizes, LongReD consistently outperforms most baselines on short-text benchmarks. Specifically, when extending Llama-3-8B to a 32K-token context window using ABF and LongReD-C, the short-text capabilities are retained up to $99.4\%$ of the original model's performance, compared to $92.5\%$ achieved by naive long-text training.
% Additionally, our methods exhibit comparable performance for Llama-3-8B and better performance for Mistral-7B-v0.3 on the RULER benchmark evaluating long-text processing to continual pre-training approaches. These results strongly validate the effectiveness of our approach.
Furthermore, our methods demonstrate competitive performance with Llama-3-8B and superior performance with Mistral-7B-v0.3 on the RULER benchmark for long-text processing, when compared to continual pre-training approaches. These compelling results robustly validate the efficacy of our proposed methodology.

% Both the LongReD-KL and LongReD-Aver can achieve better performances on short-text capacities.


% Second, our method can achieve comparable long-text modeling performances with the baselines. In all extension methods and target context window sizes, our method's performance on the RULER benchmark is similar to that of the continual pre-training methods. 

Second, the skipped positional indices method is crucial for long-text performances. Comparing LongReD with CREAM and uniform sampling methods, we find that the short-text performances are similar while the long-text performances on RULER are largely different. When the extension ratio is relatively small (\eg, 4 times), CREAM demonstrates superior performance by effectively mitigating the lost-in-the-middle problem. However, on the length of 128K tokens, the model trained with CREAM performs largely worse than that with uniform sampling.  We hypothesize that the excessive focus on the middle positions introduced by CREAM results in some positions not being adequately trained.



Finally, our method benefits from the scaling techniques of positional encodings. When scaling LLaMA-3-8B to 32K with either ABF or PI, our LongReD consistently achieves superior short-text performances. However, a comparison between models with different extension methods reveals that PI exhibits inferior performance to ABF on both short-text and long-text tasks when combined with LongReD. 
We hypothesize that this performance gap arises due to the significant distribution discrepancy introduced by PI, which poses greater challenges for our method to mitigate effectively.

% the performances of LongReD with PI are underperformed by ABF, further demonstrating that our method can be beneficial from a better scaling method. 


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \begin{table*}[htb]
% \resizebox{\textwidth}{!}{
% \begin{tabular}{l|lllll|cccccc|c}
% \toprule
% Model                        & CW  &PE & Data & Method & Sft                       & General & Coding & Math & RC & Commonsense&Avg. & RULER \\\midrule
% \multirow{14}{*}{Llama-3-8B} & 8K   & -  & -  & -      & $\times$                  &         &      &      &        &     \\
%                              & 8K   & -  & -  & -      & \checkmark &         &      &      &        &     \\\cmidrule{2-13}
%                              & 32K  & ABF& Long  & CPT    & $\times$                  &         &      &      &        &     \\
%                              & 32K  & ABF& Long & CPT    & \checkmark &         &      &      &        &     \\
%                              & 32K  & ABF& Mix  & CPT    & $\times$                  &         &      &      &        &     \\
%                              & 32K  & ABF& Mix  & CPT    & \checkmark &         &      &      &        &     \\
%                              & 32K  & ABF& Mix  & LongReD    & $\times$                  &         &      &      &        &     \\
%                              & 32K  & ABF& Mix  & LongReD    & \checkmark &         &      &      &        &     \\\cmidrule{2-13}
%                              & 32K  & PI& Long  & CPT    & $\times$                  &         &      &      &        &     \\
%                              & 32K  & PI& Long & CPT    & \checkmark &         &      &      &        &     \\
%                              & 32K  & PI& Mix  & CPT    & $\times$                  &         &      &      &        &     \\
%                              & 32K  & PI& Mix  & CPT    & \checkmark &         &      &      &        &     \\
%                              & 32K  & PI& Mix  & LongReD    & $\times$                  &         &      &      &        &     \\
%                              & 32K  & PI& Mix  & LongReD    & \checkmark &         &      &      &        &     \\\cmidrule{2-13}
%                              & 128K & ABF& Long & CPT    & $\times$                  &         &      &      &        &     \\
%                              & 128K & ABF& Long & CPT    & \checkmark &         &      &      &        &     \\
%                              & 128K & ABF& Mix  & CPT    & $\times$                  &         &      &      &        &     \\
%                              & 128K & ABF& Mix  & CPT    & \checkmark &         &      &      &        &     \\
%                              & 128K & ABF& Mix  & LongReD    & $\times$                  &         &      &      &        &     \\
%                              & 128K & ABF& Mix  & LongReD    & \checkmark &         &      &      &        &   
% \\\midrule
% \multirow{8}{*}{Mistral-7B-v0.3} & 32K   & - & -    & -      & $\times$                  &         &      &      &        &     \\
%                              & 32K   & -   & -  & -      & \checkmark &         &      &      &        &     \\\cmidrule{2-13}
%                              & 128K & ABF& Long & CPT    & $\times$                  &         &      &      &        &     \\
%                              & 128K & ABF& Long & CPT    & \checkmark &         &      &      &        &     \\
%                              & 128K & ABF& Mix  & CPT    & $\times$                  &         &      &      &        &     \\
%                              & 128K & ABF& Mix  & CPT    & \checkmark &         &      &      &        &     \\
%                              & 128K & ABF& Mix  & LongReD    & $\times$                  &         &      &      &        &     \\
%                              & 128K & ABF& Mix  & LongReD    & \checkmark &         &      &      &        &   
%                              \\\bottomrule
% \end{tabular}}
% \caption{Main Results.}
%     \label{tab:my_label}
% \end{table*}

% \begin{table*}[htb]
% \centering
% \caption{Main results.}
% \label{tab:results}
% \resizebox{\textwidth}{!}{
% \begin{tabular}{l|cc|cccccc|c}\toprule
% Model                                  & Length & Method & General & Math & Coding & Reading & CommonSense & Avg & Long Context \\\midrule
% \multirow{5}{*}{Llama-3-8B}            & 8K     &  -      &         &      &        &         &             &     &        -      \\
%                                        & 32K    &   FT     &         &      &        &         &             &     &     83.17         \\
%                                        & 32K    &        &         &      &        &         &             &     &              \\
%                                        & 128K   &   FT     &         &      &        &         &             &     &              \\
%                                        & 128K   &        &         &      &        &         &             &     &              \\\midrule
% \multirow{3}{*}{Llama-3-8B-Instruct}   & 8K     &   -     &         &      &        &         &             &     &              \\
%                                        & 128K   &   FT     &         &      &        &         &             &     &              \\
%                                        & 128K   &        &         &      &        &         &             &     &              \\\midrule
% \multirow{3}{*}{Mistral-7B-v0.3-Instruct} & 32K    &   -     &         &      &        &         &             &     &              \\
%                                        & 128K   &    FT    &         &      &        &         &             &     &              \\
%                                        & 128K   &        &         &      &        &         &             &     &     \\\bottomrule        
% \end{tabular}}
% \end{table*}
\subsection{Detailed Analysis}
\label{sec:ablation}
In this section, we conduct further detailed analysis to investigate our approach.

% To further analyze our method, we conduct the ablation study. We also assess the effect of distillation layers and distillation length. 


\paratitle{Ablation Study.}
In addition to long-text training, our method incorporates both short-text distillation and short-to-long distillation. To evaluate the individual contributions of these two components, we employ half of the training data for long-text training and another half data for short-text distillation or short-to-long distillation.
Moreover, we also evaluate the effect of the hyper-parameters $\alpha_1$ and 
$\alpha_2$ on both short and long text tasks. The results are compared against the LongReD-C method with $\alpha_1=5, \alpha_2 = 10$, as summarized in Table~\ref{tab:ablation}. The results indicate that excluding short-text distillation significantly degrades the model's performance on short-text tasks. Conversely, omitting the short-to-long distillation stage results in noticeable declines in long-text performance. In addition, The adaptation of the hyper-parameters is essential. Increasing or decreasing $\alpha_2$ will lead to performance degradation on long-text tasks, while slightly decreasing $\alpha_1$ will lead to better long-text performances. When either $\alpha_1$ or $\alpha_2$ is set to an extremely large value, the model's performance on long-context tasks drops precipitously. 

% two hyper-parameters $\alpha_1$ and $\alpha_2$ is very important. Increasing or decreasing $\alpha_2$ will lead to performance degradation while a small $\alpha_1$ may lead to declined performance on short-text tasks. 

% These findings underscore the critical roles of the two methods in balancing both short-text and long-text capabilities.

\begin{table}[htb]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cc|cccccc}
         \toprule
        $\alpha_1$&$\alpha_2$ &General & Code&Math&Common&RC&RULER\\\midrule
         5&10&67.65 &39.90 &31.52& 72.35& 62.84 &84.98 \\\midrule
         -&15&66.54&39.93&31.68&70.64&62.47& 85.48\\
         5&-& 67.39 &41.25 &30.78 &71.50 &62.25&83.61\\\midrule
         5&100 & 66.96	&40.47	&33.12	&71.86	&62.47	&80.40\\
         5&30 & 67.00 & 40.14&31.59&72.26&62.29&83.82\\
         5&15&67.20&38.20&31.46&72.39&62.55&84.87 \\
         5&5&66.91& 41.84&32.67&72.36&62.42 & 83.75\\
         100&10& 65.33	&37.81&	29.24&	72.00&	61.85	&80.19\\
         2&10& 67.02&41.10 & 31.83& 72.25&63.04 & 86.03\\
         \bottomrule
    \end{tabular}}
    \caption{Ablation results on five tasks. ``-'' denotes the objective is not employed.}
    \label{tab:ablation}
    % \vspace{-0.5cm}
\end{table}

% \begin{table}[htb]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{cc|ccccc}
%          \toprule
%         $\alpha_1$&$\alpha_2$ &MMLU & Humaneval&PIQA&TriviaQA&RULER\\\midrule
%          5&10&64.4&35.37&77.80&74.24&84.98 \\\midrule
%          -&15&63.4&34.15&77.04&74.49& 85.48\\
%          5&-& 64.4 &36.59&77.64&75.18&83.61\\\midrule
%          5&30 & 64.1 & 35.37&76.99&74.48&83.82\\
%          5&15&64.2&31.70&77.58&74.19&84.87 \\
%          5&5&64.3& 35.37&77.53&75.18&83.75 \\
%          2&10& 64.3&37.19 & 76.99& 75.08&86.03\\
%          \bottomrule
%     \end{tabular}}
%     \caption{Ablation results on five tasks. ``-'' denotes the objective is not employed.}
%     \label{tab:ablation}
%     \vspace{-0.5cm}
% \end{table}

% \begin{table}[]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{cc|ccccc}
%          \toprule
%          $\mathcal{L}_{short}$ &$\mathcal{L}_{s2l}$ &MMLU & Humaneval&PIQA&TriviaQA&RULER\\\midrule
%          \checkmark&\checkmark&64.4&35.37&77.8&74.24&84.98 \\\midrule
%         $\times$&\checkmark& \\
%         \checkmark&$\times$& \\\bottomrule
%     \end{tabular}}
%     \caption{Ablation Study.}
%     \label{tab:ablation}
% \end{table}



\paratitle{Distillation Layers.} 
% We investigate the effect of distillation granularity by varying the number and selection strategy of distilled layers. While our earlier experiments employed six layers selected based on attention KL divergence, we now evaluate three alternative configurations: all layers, only the last layer, and selecting six layers uniformly. The results are summarized in Table~\ref{tab:distill_granularity}.
% Our findings indicate that distilling all layers imposes excessively rigid constraints, leading to a significant degradation of long-text abilities. Conversely, distilling only the last layer enhances long-text modeling capabilities but fails to retain short-text abilities effectiveness compared to multi-layer distillation. Notably, for the same number of distilled layers, the performance of uniformly selected layers is consistently inferior to that of layers selected based on attention KL divergence, for both short-text and long-text scenarios. This highlights the importance of layer selection methods and granularity.
We explore the impact of distillation layers by varying the number of distilled layers and their selection strategies. In comparison to our method of selecting six layers via KL divergence (denoted as \emph{KL(6)}), we design three variants: (1) \emph{All}: distilling all layers; (2) \emph{Last}: distilling only the last layer; and (3) \emph{Uniform(6)}: uniformly selecting six layers. The results are summarized in Table~\ref{tab:distill_granularity}.
We can observe that distilling either all the layers or the last layer leads to worse long-text performance and general capacities than selecting six layers based on KL divergence. Furthermore, uniformly selected layers consistently exhibit inferior performance to those selected via attention KL divergence across the long-text tasks for the same number of distilled layers. These findings underscore the critical role of layer selection methods and distillation granularity in the performances of long-context LLMs.

% These findings highlight the importance of selecting an appropriate distillation granularity to balance long-text modeling capacity and short-text capacity preservation.
% \begin{table}[htb]
%     \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{c|ccccc}
%          \toprule
%          Layers &RULER&MMLU & Humaneval&PIQA&TriviaQA\\\midrule
%          KL(6) & 84.98 & 64.4& 35.37& 77.80& 74.24\\
%          Uniform(6) & 82.53 & 64.1& 35.97& 77.64 & 75.19\\
%          All&  81.47& 63.8& 34.76& 77.15& 74.53\\
%          Last&   82.24& 63.0& 36.58 & 78.13&74.64\\\bottomrule
       
%     \end{tabular}}
%     \caption{Results with different distillation layers.}
%     \label{tab:distill_granularity}
%     \vspace{-0.5cm}
% \end{table}

\begin{table}[htb]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|cccccc}
         \toprule
         Layers &General & Code&Math&Common&RC&RULER\\\midrule
         KL(6)& 67.65 &39.90 &31.52& 72.35& 62.84 & 84.98 \\
         Uniform(6) &  66.86 & 40.24 & 32.19 & 72.33&60.27 & 82.53\\
         All&  66.89& 39.63 & 31.80& 72.33 & 62.61 & 81.47\\
         Last&   66.40 &  40.49 &30.69 &72.11 & 63.19& 82.24 \\\bottomrule
    \end{tabular}}
    \caption{Results with different distillation layers.}
    \label{tab:distill_granularity}
    % \vspace{-0.5cm}
\end{table}




\paratitle{Distillation Length.} In our experiments, we set the length of short-text distillation $T_s$ as $1024$. To explore the effect of distillation length, we set $T_s$ as $2048$ and $8192$, and check the performance changes. As shown in Table~\ref{tab:distill_length}, increasing the distillation length will harm the model's ability to handle long texts while the short-text capacities are generally the same. 
% In addition, performances on short-text datasets except TriviaQA also decline slightly with the increase in distillation length. 
Overfitting to the long distillation length will destroy the implicit positional information in hidden states, which may be the main reason for the performance drop (details are shown in Appendix~\ref{app:latent}). 




\begin{table}[htb]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|cccccc}
         \toprule
         $T_s$&General & Code&Math&Common&RC&RULER\\\midrule
         1024& 67.65 &39.90 &31.52& 72.35& 62.84 & 84.98 \\
         2048& 66.80 &40.13 & 31.01 & 72.63& 62.32 & 83.20 \\
         8192& 67.43& 39.26 & 31.41 & 71.75 & 63.04 &  75.54 \\\bottomrule
    \end{tabular}}
    \caption{Results with varying distillation length $T_s$.}
    \label{tab:distill_length}
    % \vspace{-0.5cm}
\end{table}

% \paragraph{Choices of Distillation Loss Function}

% \paragraph{No PoSE}


% \paragraph{No }


% \begin{table*}[]
% \begin{tabular}{lllllllll}
% \toprule
% Model                        & CW   & Data & Method & General     & commonsense & RC     & Code   & RULER                           \\\midrule
% \multirow{4}{*}{Llama-3-8B} & 8K   & -    & -      & 53.96       & 72.542      & 63.782 & 41.195 & -                               \\
%                              & 32K  & Long & CPT    & 51.33 & 69.83       & 60.98  & 29.33  & {82.80} \\
%                              & 32K  & Mix  & CPT    & 52.30 & 71.876      & 34.646 & 30.68  & 85.04                     \\
%                              & 32K  & Mix  & LongReD(ours)    & 53.12       & 72.354      & 62.84  & 39.895 & 84.98                     \\\bottomrule
% \end{tabular}
% \end{table*}

\paratitle{Comparison with Continual Learning Methods.} Beyond simple continual pre-training, we also compare LongReD with two continual learning methods that can effectively mitigate catastrophic forgetting issues: (1) \emph{Model Merging}: Following previous work~\cite{hu-arxiv-2024-longrecipe}, we average the checkpoints of the original model and the extended model; (2) \emph{Parameter-Efficient Tuning}: Since modifying RoPE directly affects attention computation without altering the knowledge stored in MLP parameters, we only tune the parameters in the attention blocks. We train Llama-3-8B on a 32K context length with these methods and evaluate their performances. As shown in Table~\ref{tab:continual}, though these methods can achieve better performances than naive continual pre-training, our method consistently performs better on most short-context capacities evaluations. 

\begin{table}[htb]
    \centering
    \resizebox{\linewidth}{!}{\begin{tabular}{c|cccccc}
        \toprule
         Method & General & Code & Math & Common & RC & RULER \\
        \midrule
          LongReD-C & 67.51 & 39.90 & 31.52 & 72.35 & 62.84 & 84.98 \\
          Merging & 65.92 & 35.04 & 32.38 & 71.61 & 62.65 & 83.38 \\
          PET & 66.82 & 39.20 & 30.16 & 70.78 & 62.26 & 85.86 \\
          CPT & 65.39 &29.32& 29.50 &69.83 & 60.98 & 82.80 \\        \bottomrule
    \end{tabular}}
    \caption{Comparison with continual learning methods, where Merging denotes model merging and PET denotes parameter-efficient tuning. }
    \label{tab:continual}
\end{table}



% This method adapts the attention block for long-context windows while mitigating the forgetting issue of knowledge in other blocks.




