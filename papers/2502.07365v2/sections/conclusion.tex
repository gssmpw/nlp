\section{Conclusion}
\label{sec:conclusion}

In this paper, we analyzed two reasons for the performance degradation in short-text tasks after context window extension, \ie {distribution drift}  and {catastrophic forgetting}. Based on the observations, we proposed an effective training method to better preserve short-text capacities, named {Long} Context Pre-training with {Re}storation {D}istillation ({LongReD}). Besides continual pre-training on long texts, LongReD introduced two additional training objectives: short-text distillation and short-to-long distillation. Experiments demonstrated that our method could achieve better performances on common short-text benchmarks while achieving comparable long-text modeling abilities on the RULER benchmark compared with continual pre-training. In future work, we will explore how to directly integrate continual training with the distillation of the original model on long texts, rather than applying them separately to texts of different lengths.


\section*{Limitations}
\label{sec:limitation}

In this paper, we present a novel perspective on the extension of context windows and the associated decline in the general capabilities of LLMs. We also propose a method to preserve general capabilities while enhancing long-context abilities. However, a notable degradation in short-text performance after lightweight continual pretraining on only several billion tokens. \citet{xiong-naacl-2024-effective} and \citet{Dubey-arxiv-2023-llama3} have shown that, with training on more than 100 billion tokens, some capabilities on short texts of models may remain largely unaffected or even improve. Furthermore, our proposed method offers a general training strategy that is compatible with different positional encoding extension techniques. A more refined extension method that minimizes disruption to the model's distribution could further enhance the effectiveness of our approach, a topic we leave for future investigation.

