



\section{Skipped Positional Indices}
\label{app:cream}

Skipped positional indices are widely employed for effective long-context training. The method simulates long-distance dependency via modifying the positional indices. In this section, we introduce two skipped positional indices employed in our methods.


\subsection{CREAM}
Continuity-Relativity indExing with gAussian Middle (CREAM)~\cite{wu-nips-2024-cream} is a method that modifies position indices to simulate long positions. The main idea is to make the model better focused on the middle part of the long sequences. 

As described in Section~\ref{sec:s2l_distillation}, the positional indices are first split into three non-overlapped parts, \ie $\bm p_{head}$, $\bm p_{tail}$, and $\bm p_{tail}$, where the length of the head and the tail are $T_b$. There are two segmentation methods and we randomly select one method to determine the number of $T_b$. To achieve the continuity of the middle part, $T_b$ is set as a small number, where we set it to four times the context window extension factor. To learn more relative positions, $T_b$ is set as one-third of the input length $T/3$. 

To better concentrate on the middle positions, the method proposes a truncated Gaussian function to determine the end position of the middle part $p_e$. Initially, the method defines a Gaussian distribution $f(x)$ and its cumulative distribution function $F(x)$:
\begin{align}
    f(x) &= \frac 1 {\sigma \sqrt 2\pi} \exp(-\frac {(x-\mu)^2}{2\sigma^2}), \\
    F(x) &= \int_{-\infty}^x f(t) dt,
\end{align}
where $\mu= 1+T_l/T$ and $\sigma=3$. Then, the method samples 1000 points $x_i$ uniformly from 1 to extension factor $T_l/T$ and calculates their cumulative distribution function $F(x_i)$. After that, the method samples $F(u)$ from the Gaussian distribution and uses interpolation and inverse transformation to obtain the corresponding $u$. The number $u$ then rounds to the nearest integer $\alpha$.
\begin{align}
    u &= x_{i-1} + \frac{(x_i-x_{i-1})(F(u)-F(x_{i-1}))}{F(x_i)-F(x_{i-1})},\\
    \alpha &= \operatorname{round}(u),
\end{align}
where $F(x_i)$ and $F(x_{i-1})$ are the two nearest numbers to $F(u)$ and $F(u)$ is sampled uniformly from 0 to 1. Finally, we sample an integer as the end of the middle part:
\begin{align}
    T_{me} &\sim \operatorname{Uniform}(T_b+\alpha (T_l-2T_b), \alpha T-T_b-1),\\
    T_{mb} &= T_{me}+2T_b-T.
\end{align}

\subsection{Uniform Sampling}

In the uniform sampling method, we keep the same as the CREAM method except the sampling method of the positions of the middle part. Instead, we employ a uniform sampling method to determine the end positions of the middle part, as formulated as follows: 
\begin{align}
    T_{me} &\sim \operatorname{Uniform}(T-T_b, T_l-T_b-1),\\
    T_{mb} &= T_{me}+2T_b-T,
\end{align}
where $\operatorname{Uniform}$ denotes uniformly sampling from this range.
\section{Training Details}
\label{app:train_details}
% In our method, there are three datasets $\mathcal D_1$, $\mathcal D_2$, and $\mathcal D_3$, which are employed for long-text training, short-text distillation, and short-to-long distillation, respectively. Following~\citet{fu-icml-2024-data}, we sample samples with the same ratio with Llama-2 from SlimPajama datasets~\cite{soboleva-huggingface-2023-slimpajama}, tokenize and pack them into target length. Inspired by~\cite{gao-arxiv-2024-how}, we use more high-quality data to better preserve the shor-text capacities during short-text distillation. We select Fineweb-edu~\cite{penedo-arxiv-2024-fineweb}, Stack-v2~\cite{Lozhkov-arxiv-2024-stack}, the math part of Fineweb-Edu~\cite{penedo-arxiv-2024-fineweb} and Proof-Pile-2~\cite{Azerbayev-ICLR-2024-proof}, PG19~\cite{rae-iclr-2020-pg19}, Arxiv~\cite{soboleva-huggingface-2023-slimpajama}, and Wikipedia~\cite{soboleva-huggingface-2023-slimpajama} datasets to balance the quality and diversity. We also add instruction data (Tulu-v2-sft-mixture~\footnote{\url{https://huggingface.co/datasets/allenai/tulu-v2-sft-mixture}}, MathInstruct~\footnote{\url{https://huggingface.co/datasets/TIGER-Lab/MathInstruct}} WizardLM-evol-instruct-V2~\footnote{\url{https://huggingface.co/datasets/WizardLMTeam/WizardLM\_evol\_instruct\_V2\_196k}}, and Magicoder-Evol-Instruct-110K~\footnote{\url{ise-uiuc/Magicoder-Evol-Instruct-110K}}), which have been found to be beneficial~\cite{hu-arxiv-2024-minicpm}. The detailed data proportions are shown in Table~\ref{tab:data}.

\subsection{Training Datasets}

In our proposed method, we employ three datasets: $\mathcal{D}_1$, $\mathcal{D}_2$, and $\mathcal{D}_3$, which are specifically designed for long-text training, short-text distillation, and short-to-long distillation, respectively. Following the sampling strategy of~\citet{fu-icml-2024-data}, we sample data in the same proportion as Llama-2 from the SlimPajama datasets~\cite{soboleva-huggingface-2023-slimpajama}, tokenize the samples, and pack them to the target length. To enhance the preservation of short-text capabilities during the short-text distillation phase, we adopt the approach proposed by~\cite{gao-arxiv-2024-how}, utilizing higher-quality datasets. Specifically, we select Fineweb-Edu~\cite{penedo-arxiv-2024-fineweb}, Stack-v2~\cite{Lozhkov-arxiv-2024-stack}, the mathematical subset of Fineweb-Edu~\cite{penedo-arxiv-2024-fineweb}, Proof-Pile-2~\cite{Azerbayev-ICLR-2024-proof}, PG19~\cite{rae-iclr-2020-pg19}, Arxiv~\cite{soboleva-huggingface-2023-slimpajama}, and Wikipedia~\cite{soboleva-huggingface-2023-slimpajama}. These datasets are carefully chosen to balance quality and diversity.
Additionally, we incorporate instruction-tuned datasets that have been demonstrated to be beneficial for model performance~\cite{hu-arxiv-2024-minicpm}, including Tulu-v2-sft-mixture\footnote{\url{https://huggingface.co/datasets/allenai/tulu-v2-sft-mixture}}, MathInstruct\footnote{\url{https://huggingface.co/datasets/TIGER-Lab/MathInstruct}}, WizardLM-evol-instruct-V2\footnote{\url{https://huggingface.co/datasets/WizardLMTeam/WizardLM\_evol\_instruct\_V2\_196k}}, and Magicoder-Evol-Instruct-110K\footnote{\url{https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K}}. The detailed proportions of these datasets utilized in our method are summarized in Table~\ref{tab:data}.

\begin{table}[htb]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cc}
    \toprule
         Dataset& Data Mixture\\\midrule
         $\mathcal D_1$& \tabincell{c}{Wikipedia: 0.034, CommonCrawl: 0.534, \\StackExchange: 0.032, C4: 0.266, \\Github: 0.050, ArXiv: 0.043, Book: 0.041}\\\midrule
         $\mathcal D_3$& \tabincell{c}{Wikipedia: 0.034, CommonCrawl: 0.534, \\StackExchange: 0.032, C4: 0.266, \\Github: 0.050, ArXiv: 0.043, Book: 0.041}\\\midrule
         $\mathcal D_2$& \tabincell{c}{Fineweb-Edu: 0.476, Fineweb-Edu-Math: 0.722\\ Proof-Pile-2: 0.230, Stack-v2: 0.095, PG19: 0.095, \\Wikipedia: 0.096, ArXiv: 0.095, Instructions: 0.047}\\\bottomrule
    \end{tabular}}
    \caption{Data Mixture of three datasets.}
    \label{tab:data}
\end{table}

\subsection{Evaluated Models}
We select Llama-3-8B and Mistral-7B-v0.3 as our evaluated models. For Llama-3-8B, we extend its context window to 32K and 128K, with the ABF and PI methods. For Mistral-7B-v0.3, we extend its context window to 128K with ABF method. The configuration and training length of these models are displayed in Table~\ref{tab:model_config}.


\begin{table}[htb]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccccc}
    \toprule
         Model&  CW&  PE&  Ratio&  $L_{\mathcal D_1}$&  $L_{\mathcal D_2}$&  $L_{\mathcal D_3}$\\\midrule
         Llama-3-8B&  32K&  ABF&  2e7&  32000&  1024&  8192\\
         Llama-3-8B&  32K&  PI&  4&  32000&  1024&  8192\\
         Llama-3-8B&  128K&  ABF&  1e8&  128000&  1024&  8192\\
         Mistral-7B-v0.3&  128K&  ABF&  2e7&  128000&  1024&  32768\\
         % Qwen1.5-1.8B&  128K&  ABF&  2e7&  128000&  1024&  32768\\
         \bottomrule
    \end{tabular}}
    \caption{Configuration of models and length of datasets. CW denotes the context window of target models, PE denotes the scaling methods of RoPE, Ratio denotes the RoPE theta for ABF methods, and the interpolation ratio for PI methods. $L_{\mathcal D_1}$, $L_{\mathcal D_2}$, and $L_{\mathcal D_3}$ denotes the length of the three datasets.}
    \label{tab:model_config}
\end{table}

\subsection{Training Configurations}
We employ ring flash attention~\cite{liu-arxiv-2023-ringattn} in EasyContext framework~\cite{zhang-arxiv-2024-easycontext} (Apache-2.0 License) to train our models with 8 A800 GPUs. The learning rate is  fixed as 2e-5 without warmup and the training tokens of each batch are about 2M tokens. We employ AdamW optimizer~\cite{Loshchilov-iclr-2019-adamw} with the weight decay of 0.1, $\beta_1$ of 0.9, and $\beta_2$ of 0.95.  The hyper-parameters $\alpha_1$ and $\alpha_2$ are set as $\alpha_1=5, \alpha_2=10$ for 32K, and $\alpha_1=2, \alpha_2=15$ for 128K. All the models are trained with 512 steps with about 1B tokens.

\subsection{Training Costs}

We report the training costs of our method and other baselines. All the models are trained on 8 A800 GPUs with ring attention. The time costs for training these models are shown in Table~\ref{tab:time}. Compared with baselines, our methods will cost about $10\%$ extra computation than the only continually pre-training on the same data. Additionally, comparing only training on long texts, LongReD costs similar time in 32K while saving about $20\%$ computations in 128K settings.
\begin{table}[htb]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cccc} \toprule
         Model&  CW&  Data&  Method&  Time(h)\\ \midrule
 \multirow{6}{*}{Llama-3-8B}& 32K& Long& CPT& 22.4\\ 
 & 32K& Mix& CPT&19.9\\ 
 & 32K& Mix& LongReD& 22.4\\
 & 128K& Long& CPT& 42.8\\
 & 128K& Mix& CPT& 30.4\\
 & 128K& Mix& LongReD& 33.4\\\bottomrule
    \end{tabular}}
    \caption{Training time of Llama-3-8B with different methods and target context windows.}
    \label{tab:time}
\end{table}



\section{Evaluation Details}
\label{app:evaluation}



To evaluate the performance of LLMs on short-text and long-text tasks, we employ multiple benchmarks. In this section, we introduce the datasets for evaluating different capacities:
\begin{itemize}
    \item \emph{General}: MMLU~\cite{Hendrycks-iclr-2021-mmlu}, BBH~\cite{suzgun-acl-2023-bbh}, LAMBADA~\cite{paperno-acl-2016-lambada}
    \item \emph{Math}: MATH~\cite{henderycksnips-2021-math}, GSM8K~\cite{cobbe-arxiv-2021-gsm8k}
    \item \emph{Coding}: HumanEval~\cite{chen-arxiv-2021-humaneval}, MBPP~\cite{austin-arxiv-2021-mbpp}
    \item \emph{Reading comprehension}: Squadv2~\cite{rajpurkar-EMNLP-2016-squad}, Quac~\cite{choi-acl-2018-quac}, TriviaQA~\cite{Joshi-acl-2017-triviaqa}, Drop~\cite{dua-naacl-2019-drop}, BoolQ~\cite{clark-naccl-2019-boolq}
    \item \emph{Commonsense question answering}: Openbookqa~\cite{todor-emnlp-2018-openbookqa}, Commonsenseqa~\cite{talmor-naacl-2019-commonsenseqa}, ARC-C~\cite{clark-arxiv-2018-arc}, SIQA~\cite{Sap-emnlp-2019-siqa}, PIQA~\cite{bisk-aaai-2020-piqa}),
    \item \emph{Long text processing}: RULER~\cite{Hsieh-arxiv-2024-RULER}
\end{itemize}
we also present details of our evaluation configurations, as shown in Table~\ref{tab:evaluation_details}.

\begin{table}[htb]
    \centering
    \small{
    \begin{tabular}{llcc}
    \toprule
         Dataset&   CoT&\#Shots& Metric\\\midrule
         MMLU&   $\times$&5& Probability\\
         BBH&   \checkmark&3& EM\\
         Lambada&   $\times$&0& accuracy\\
         MBPP&   $\times$&3& pass@1\\
         HumanEval&  $\times$ &0& pass@1\\
         GSM8K&  \checkmark &4& accuracy\\
         MATH&  \checkmark &4& accuracy\\
         PIQA&   $\times$&0& Probability\\
 SIQA&  $\times$&0&Probability\\
 OpenBookQA&  $\times$&0&Probability\\
 ARC(C)&  $\times$&25&Probability\\
 CommonSenseQA&  $\times$&7&PPL\\
 Quac& $\times$& 1&F1\\
 TriviaQA& $\times$& 5&F1\\
 DROP&  $\times$&3&F1\\
 BoolQ&  $\times$&3&F1\\
 Squad-v2&  $\times$&1&F1\\
 RULER&  $\times$&0&Accuracy\\
    \bottomrule
    \end{tabular}}
    \caption{Configurations of evaluated benchmarks. CoT denotes employing the Chain-of-Thought prompt, \#Shot denotes the number of shots in prompts, and Metric denotes the evaluation metric for the benchmark.}
    \label{tab:evaluation_details}
\end{table}
\begin{table*}[htb]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lllllccc}
    \toprule
         Model&  RoPE&  CW&  Base Model&  Tokens&  MMLU&  MMLU Ratio& Simi\\\midrule
 \multicolumn{8}{c}{GradientAI Models}\\\midrule
         Llama-3-8B-Ins-G-262k&  ABF(2.8e8)&  262K&  -&  -&  61.9&  0.945& 0.920\\
         Llama-3-8B-Ins-G-1048k&  ABF(3.6e9)&  1048K&  -&  -&  59.7&  0.911& 0.900\\
         Llama-3-8B-Ins-G-4194k&  ABF(4.5e10)&  4194K&  -&  -&  56.1&  0.856& 0.872\\\midrule
         \multicolumn{8}{c}{Our Models (Directly Extension)}\\\midrule
 Llama-3-32K-5e6& ABF(5e6)& 32K& Llama-3-8B& 256M& 62.2& 0.960&0.943\\
 Llama-3-32K-5e7& ABF(5e7)& 32K& Llama-3-8B& 256M& 62.0& 0.957&0.934\\
 Llama-3-32K-5e8& ABF(5e8)& 32K& Llama-3-8B& 256M& 60.9& 0.940&0.924\\
         Llama-3-32K-5e9&  ABF(5e9)&  32K&  Llama-3-8B& 256M&  60.4&  0.932& 0.915\\
         Llama-3-32K-5e10&  ABF(5e10)&  32K&  Llama-3-8B& 256M&  59.8&  0.923& 0.908\\
 Llama-3-32K-5e20& ABF(5e20)& 32K& Llama-3-8B& 256M& 56.5& 0.872&0.856\\
 Llama-3-32K-PI4& PI(4)& 32K& Llama-3-8B& 256M& 60.9& 0.940&0.917\\
         Llama-3-32K-PI16&  PI(16)&  32K&  Llama-3-8B& 256M&  56.1&  0.866& 0.862\\\midrule
 \multicolumn{8}{c}{Our Models (Gradually Extension)}\\\midrule
 Llama-3-16K-5e6& ABF(5e6)& 16K& Llama-3-8B& 256M& 62.3& 0.961&0.946\\
 Llama-3-32K-5e7& ABF(5e7)& 32K& Llama-3-16K-5e6& 256M& 60.7& 0.938&0.927\\
 Llama-3-64K-5e8& ABF(5e8)& 64K& Llama-3-32K-5e7& 256M& 60.3& 0.930&0.912\\
 Llama-3-128K-5e9& ABF(5e9)& 128K& Llama-3-64K-5e8& 256M& 57.2& 0.883&0.901\\\bottomrule
    \end{tabular}}
    \caption{Details of evaluated models for the analysis of the relationship between distribution drift and short-text performances. }
    \label{tab:drift}
\end{table*}
The details of metrics employed to evaluate the performance are described as follows:
\begin{itemize}
    \item \emph{Probability}: For a choice task, all the choices are provided in the prompt and the Probability of each choice is calculated. The choice with the maximum Probability will be compared with the correct choice and the accuracy is reported.
    \item \emph{PPL}: For a choice task, the choices are not provided. Instead, the content of each choice is input after the question, and the average perplexity of each choice is calculated. The choice with the lowest PPL is selected as the correct answer.
    \item \emph{Pass@1}:  For coding tasks, the model is asked to give one solution. Then, we evaluate the solution with the unit testing and count the numbers that pass the testing. 
    \item \emph{EM}:  For the question-answering task, the exact matching metric measures whether the output is the same as the golden answer.
    \item \emph{F1}:  For the question-answering task, the F1 first splits the golden answer and output of the model into words, and calculates the F1 score of these word lists. 
    \item \emph{Accuracy}:   Accuracy measures the number of correctly answered samples out of all the samples. For RULER, the sample is correct if the answer is in the output or each part of the answer (only for qa1 and qa2).  

\end{itemize}


\section{Models for Evaluating Distribution Drift}
\label{app:drift_models}
To evaluate the relationship between the distribution drift and the short performance, we first employ open-sourced three long-context models from Gradient AI~\cite{gradient-hf-2024-longcontextllama3}, which are continually pre-trained from Llama-3-8B-Instruct with only several hundred million tokens. Then, we train Llama-3-8B with ABF and PI methods with different extension ratios on 256M tokens with a length of 32K. We also follow~\citet{gradient-hf-2024-longcontextllama3}, gradually extending the context window length to twice its original length. For each extension, we train the model with 256M tokens and 
enlarge the RoPE theta ten times. The full model list and their MMLU scores and hidden states similarity can be seen in Table~\ref{tab:drift}. Typically, a larger extension ratio will cause a greater shift in the model's distribution, further leading to a decline in the model's performance on short texts. We provide theoretical proof of the impact of the RoPE bases on the drifts of attention distributions.

\subsection{Proof of RoPE Base Impact on Attention Scores}
Given a query $\mathbf{q}^m$ and a key $\mathbf{k}^n$, their inner product with RoPE encoding can be expressed as:

\begin{align}
   & (\mathbf{R}^d_{\Theta, m} \mathbf{q}^m)^\top (\mathbf{R}^d_{\Theta, n} \mathbf{k}^n)\\ =& \mathrm{Re}\left[\sum_{i=0}^{d/2-1} \mathbf{q}^m_{[2i:2i+1]} {\mathbf{k}^n_{[2i:2i+1]}}^* e^{\mathrm{i}(m-n)\theta_i}\right]
\end{align}


where $[2i:2i+1]$ denotes the $i$-th subspace. Following the definition from \citet{su-neurocomputing-2024-roformer}, we let $h_i = \mathbf{q}^m_{[2i:2i+1]} {\mathbf{k}^n_{[2i:2i+1]}}^*$ and $S^\Theta_j = \sum_{k=0}^{j-1} e^{\mathrm{i}(m-n)\theta_k}$. We extend these definitions with $h_{d/2} = 0$ and $S^\Theta_0 = 0$. Applying Abel transformation, we rewrite the summation:

\begin{align}
\sum_{i=0}^{d/2-1} h_i e^{\mathrm{i}(m-n)\theta_i} 
&= \sum_{i=0}^{d/2-1} h_i (S^\Theta_{i+1} - S^\Theta_i) \\
&= -\sum_{i=0}^{d/2-1} S^\Theta_{i+1}(h_{i+1} - h_i)
\end{align}

\textbf{Theorem.} Let $\Theta = b^{-2i/d}$ and $\Theta' = {b'}^{-2i/d}$ be two RoPE bases with $b > b' > 1$. For fixed $\mathbf{q}^m$ and $\mathbf{k}^n$, their attention score difference satisfies:
\[
\left| \textit{Attn}(b) - \textit{Attn}(b') \right| \leq C \cdot \sum_{i=0}^{d/2-1} \left(|S^{\Theta}_{i+1}| + |S^{\Theta'}_{i+1}|\right)
\]
where $C = \max\limits_{0 \leq i < d/2} |h_{i+1} - h_i|$.

\textbf{Proof.}  
The attention scores under different bases are:
\begin{align}
    \textit{Attn}(b) = \mathrm{Re}\left[-\sum_{i=0}^{d/2-1} S^\Theta_{i+1}(h_{i+1} - h_i)\right],\\ \textit{Attn}(b') = \mathrm{Re}\left[-\sum_{i=0}^{d/2-1} S^{\Theta'}_{i+1}(h_{i+1} - h_i)\right]
\end{align}

Their difference satisfies:
\begin{align}
&\left| \textit{Attn}(b) - \textit{Attn}(b') \right|\\ 
\leq & \left|\sum_{i=0}^{d/2-1} \left(S^\Theta_{i+1} - S^{\Theta'}_{i+1}\right)(h_{i+1} - h_i)\right| \\
\leq & \sum_{i=0}^{d/2-1} \left| S^\Theta_{i+1} - S^{\Theta'}_{i+1}\right| \cdot \left| h_{i+1} - h_i\right| \\
\leq & C \sum_{i=0}^{d/2-1} \left(|S^{\Theta}_{i+1}| + |S^{\Theta'}_{i+1}|\right)
\end{align}
where the last inequality follows from $|S^\Theta_{i+1} - S^{\Theta'}_{i+1}| \leq |S^\Theta_{i+1}| + |S^{\Theta'}_{i+1}|$ by triangle inequality.

To analyze the expected difference across positions, consider:
\begin{align}
&\mathbb{E}\left[\left| \textit{Attn}(b) - \textit{Attn}(b') \right|\right] \\
\leq & \frac{1}{N} \sum_{q,k,t} C^{q,k} \sum_{i=0}^{d/2-1} \left(|S^{\Theta,t}_{i+1}| + |S^{\Theta',t}_{i+1}|\right) \\
\leq & \frac{C}{N} \sum_{t=0}^{T} (T-t) \sum_{i=0}^{d/2-1} \left(|S^{\Theta,t}_{i+1}| + |S^{\Theta',t}_{i+1}|\right)
\end{align}

\begin{table*}[htb]
    \centering
    \begin{tabular}{cccccccccc}
    \toprule
         Base&  5e5&  5e6&  5e7&  5e8&  5e9&  5e10&  5e11&  5e12& 5e13\\\midrule
         $B(\Theta)$&  1.3e6&  1.5e6&  1.9e6&  2.3e6&  3.0e6&  3.3e6&  3.6e6&  3.9e6& 4.1e6\\\bottomrule
    \end{tabular}
    \caption{Upper bound of different RoPE bases.}
    \label{tab:upbound}
\end{table*}


where $T$ is the maximum sequence length and $t = |m-n|$ is the relative position. Let $B(\Theta) = \sum_{t=0}^{T} (T-t) \sum_{i=0}^{d/2-1} |S^{\Theta,t}_{i+1}|$. Previous work has demonstrated that RoPE has long-term decay and a large base will lead to slow attention decay. For bases $b_3 > b_2 > b_1$, we have $B(\Theta_3) > B(\Theta_2) > B(\Theta_1)$ due to slower attention decay with larger bases. As shown in Table~\ref{tab:upbound}, empirical measurements on Llama-3-8B confirm that $B(\Theta)$ increases with $b$, proving that larger bases induce greater attention score changes during context window extension.


\section{Analysis of Distillation Length}
\label{app:latent}

As discussed in Section~\ref{sec:ablation}, increasing the distillation length will harm the long text modeling capacities. To explore the reason for the performance decline, we employ positional vectors~\cite{dong-arxiv-2024-exploring}, a method to extract latent positional information from the hidden states. Specifically, given the hidden states from different samples, we average the hidden states at the same position to obtain vectors that 
representing positional information, denoted as positional vectors:
\begin{equation}
   \bm  p_{l,i} = \frac 1 N\sum_{s=1}^N \bm h_{l,i}^s 
\end{equation}
Then, we compute the cosine similarities of positional vectors with different places. All the positional vectors are calculated on samples from SlimPajama~\cite{soboleva-huggingface-2023-slimpajama} with a length of 32000 tokens. The similarity matrices of the positional vectors of models with distillation lengths of 1024, 2048, and 8192 as well as the models trained with only long texts are shown in Figure~\ref{fig:pe}. We can observe that a large distillation length will cause a significant discontinuity in the implicit positional information inside and outside the original context window.


\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{figs/pe.pdf}
    \caption{Simialrity matrices of positional vectors inside and outside the context window.}
    \label{fig:pe}
\end{figure}


\section{Results Details}
\label{app:all_results}
In this section, we display the details of performances on these benchmarks. The results of benchmarks evaluating general, coding, and math capacities are shown in Table~\ref{tab:results_detail1}. Table~\ref{tab:results_detail2} and Table~\ref{tab:results_detail3} show the performance on commonsense question answering and reading comprehension benchmarks respectively. 


\begin{table*}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|ccccccccccc} \toprule
         Model&  CW&  PE&  Data&  Method&  MMLU&  BBH&  LAMBADA&  HUMANEVAL&   MBPP&GSM8K&MATH\\\midrule
 \multirow{13}{*}{Llama-3-8B}& 8K& -& -& -& 64.80& 63.97& 75.78& 34.75& 48.28& 50.04&10.20\\ \cmidrule{2-12}
 & 32K& ABF& Long& CPT& 62.00& 59.90& 74.27& 14.02& 44.62& 49.20&9.80\\ 
 & 32K& ABF& Mix& CPT&62.60& 60.54& 74.97& 16.46& 38.34& 49.43&9.20\\
 & 32K& ABF& Mix& LongReD-C& 64.40& 62.78& 75.35& 35.37& 44.42& 52.84&10.20\\
 & 32K& ABF& Mix& LongReD-U& 64.40& 61.75& 75.02& 34.76& 44.66& 52.69&9.20\\\cmidrule{2-12}
 & 32K& PI& Long& CPT& 61.40& 56.20& 73.92& 12.20& 42.06& 44.96&7.40\\ 
 & 32K& PI& Mix& CPT& 60.50& 57.17& 74.99& 16.46& 38.12& 49.13&9.20\\ 
 & 32K& PI& Mix& LongReD-C& 63.30& 60.83& 75.22& 34.76& 42.02& 51.55&10.00\\
 \cmidrule{2-12}
 & 128K& ABF& Long& CPT& 61.40& 56.22& 74.56& 17.07& 43.00& 47.23&8.20\\
 & 128K& ABF& Mix& CPT& 62.40& 61.62& 75.16& 21.95& 6.12& 50.04&8.00\\
 & 128K& ABF& Mix& LongReD-C& 63.60& 60.25& 74.11& 35.37& 44.68& 52.92&7.60\\
  & 128K& ABF& Mix& LongReD-U& 63.60& 59.45& 73.78& 35.96& 43.78& 49.13&10.20\\\midrule
 \multirow{5}{*}{\makecell{Mistral\\-7B-v0.3}}& 32K& -& -& -& 62.30& 58.49& 75.28& 24.39& 42.00& 45.56& 8.40\\\cmidrule{2-12}
  & 128K& ABF& Long& CPT& 51.90&	43.3&	70.68 & 21.34	&31.4&26.38	&5.20\\
 & 128K& ABF& Mix& CPT& 54.70	&44.87	&68.39 & 19.51&	33.2 & 27.60 &	3.00\\
 & 128K& ABF& Mix& LongReD-C& 58.90&	51.58	&73.86 & 20.12	&33.2 & 33.36 &6.40\\
 & 128K& ABF& Mix& LongReD-U& 60.1&	52.08&	75.1& 23.78	&35.5& 34.12&	5.00\\
 \bottomrule
    \end{tabular}}
    \caption{Comparison of performances of short-text and long-text benchmarks of our methods with other baselines. CW denotes the context window length, PE denotes the scaling method of RoPE, RC denotes reading comprehension, and Common denotes commonsense question answering. }
    \label{tab:results_detail1}
\end{table*}



\begin{table*}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|ccccccccc} \toprule
         Model&  CW&  PE&  Data&  Method&  CommonsenseQA&  OpenBookQA&  PIQA&  SIQA&   ARC-C\\\midrule
 \multirow{13}{*}{Llama-3-8B}& 8K& -& -& -& 73.22& 71& 77.97& 61& 79.52\\ \cmidrule{2-10}
 & 32K& ABF& Long& CPT& 71.50& 66.40& 74.1& 59.42& 77.73\\ 
 & 32K& ABF& Mix& CPT&72.65& 69.60& 77.09& 62.74& 77.30\\
 & 32K& ABF& Mix& LongReD-C& 72.56& 71.80& 77.80& 60.34& 79.27\\
 & 32K& ABF& Mix& LongReD-U& 72.89& 71.80& 77.91& 61.46& 79.01\\\cmidrule{2-10}
 & 32K& PI& Long& CPT& 71.25& 62.80& 72.03& 55.58& 76.88\\ 
 & 32K& PI& Mix& CPT& 70.19& 69.00& 77.04& 62.28& 76.96\\ 
 & 32K& PI& Mix& LongReD-C& 71.91& 70.80& 75.90& 59.98& 77.90\\
 \cmidrule{2-10}
 & 128K& ABF& Long& CPT& 65.19& 64.00& 77.15& 59.83& 75.68\\
 & 128K& ABF& Mix& CPT& 72.65& 70.00& 77.53& 61.62& 77.73\\
  & 128K& ABF& Mix& LongReD-C& 72.40& 69.20& 76.01& 61.16& 78.75\\
 & 128K& ABF& Mix& LongReD-U& 72.40& 69.80& 77.04& 61.00& 77.99\\\midrule
 \multirow{5}{*}{\makecell{Mistral\\-7B-v0.3}}& 32K& -& -& -& 71.50	&66.8	&66.65	&56.4	&79.01\\\cmidrule{2-10}
  & 128K& ABF& Long& CPT& 66.99&	34.8	&56.8&	38.89	&67.15\\
 & 128K& ABF& Mix& CPT& 66.99&	34.8	&56.8	&38.89&	67.15\\
 & 128K& ABF& Mix& LongReD-C& 69.04	&65.4	&62.68	&53.89&	75.26\\
& 128K& ABF& Mix& LongReD-C& 70.84	&65.6	&63.87	&55.48	&78.5\\
 \bottomrule
    \end{tabular}}
    \caption{Comparison of performances of short-text and long-text benchmarks of our methods with other baselines. CW denotes the context window length, PE denotes the scaling method of RoPE, RC denotes reading comprehension, and Common denotes commonsense question answering. }
    \label{tab:results_detail2}
\end{table*}

\begin{table*}
\centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|ccccccccc} \toprule
         Model&  CW&  PE&  Data&  Method&  SquadV2&  quacQ&  TriviaQA&  BoolQ&   DROP\\\midrule
 \multirow{13}{*}{Llama-3-8B}& 8K& -& -& -& 72.06& 35.65& 75.42& 82.6& 53.18\\ \cmidrule{2-10}
 & 32K& ABF& Long& CPT& 66.56& 34.03& 70.67& 83.12& 50.52\\ 
 & 32K& ABF& Mix& CPT&0.89& 16.97& 72.60& 82.60& 0.17\\
 & 32K& ABF& Mix& LongReD-C& 74.18& 34.39& 74.24& 82.29& 49.10\\
 & 32K& ABF& Mix& LongReD-U& 72.45& 34.50& 74.16& 82.26& 49.18\\\cmidrule{2-10}
 & 32K& PI& Long& CPT& 73.26& 33.09& 70.95& 81.16& 51.29\\ 
 & 32K& PI& Mix& CPT& 7.53& 21.43& 72.55& 81.44& 0.21\\ 
 & 32K& PI& Mix& LongReD-C& 71.96& 34.73& 74.87& 82.94& 49.47\\
 \cmidrule{2-10}
 & 128K& ABF& Long& CPT& 66.01& 32.81& 73.13& 81.16& 49.35\\
 & 128K& ABF& Mix& CPT& 1.08& 18.41& 74.06& 82.66& 0.21\\
 & 128K& ABF& Mix& LongReD-C& 70.67& 33.97& 75.19& 82.51& 49.63\\
 & 128K& ABF& Mix& LongReD-U& 70.26& 34.85& 75.13& 82.84& 49.29\\\midrule
 \multirow{5}{*}{\makecell{Mistral\\-7B-v0.3}}& 32K& -& -& -& 72.47&	31.84	&76.1	&83.43&	45.46\\\cmidrule{2-10}
  & 128K& ABF& Long& CPT& 55.15	&29.07	&62.9	&80.83	&37.4\\
 & 128K& ABF& Mix& CPT& 53.92	&31.88	&63.85	&78.32&	35.73\\
 & 128K& ABF& Mix& LongReD-C& 70.45	&30.16	&68.16	&80.61&	41.9\\
 & 128K& ABF& Mix& LongReD-U& 72.6	&30.82&	71.84	&81.16&	43.5\\
 \bottomrule
    \end{tabular}}
    \caption{Comparison of performances of short-text and long-text benchmarks of our methods with other baselines. CW denotes the context window length, PE denotes the scaling method of RoPE, RC denotes reading comprehension, and Common denotes commonsense question answering. }
    \label{tab:results_detail3}
\end{table*}