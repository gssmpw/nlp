%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\pdfoutput=1

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2025}
\usepackage[preprint]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives}

\begin{document}

\twocolumn[

\icmltitle{Position: Scaling LLM Agents Requires \\Asymptotic Analysis with LLM Primitives}
%\icmltitle{Position: Scaling LLM Agents Requires Deanthropomorphizing Them}
%\icmltitle{Position: Scaling LLM Agents Requires \\Complexity Analysis with LLM Primitives}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Elliot Meyerson}{yyy}
\icmlauthor{Xin Qiu}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Cognizant AI Labs, San Francisco, USA}

\icmlcorrespondingauthor{Elliot Meyerson}{elliot.meyerson@cognizant.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{LLMs, Algorithm Analysis, Agents, Multi-agent systems}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
Decomposing hard problems into subproblems often makes them easier and more efficient to solve.
With large language models (LLMs) crossing critical reliability thresholds for a growing slate of capabilities, there is an increasing effort to decompose systems into sets of LLM-based agents, each of whom can be delegated sub-tasks.
However, this decomposition (even when automated) is often intuitive, e.g., based on how a human might assign roles to members of a human team.
How close are these role decompositions to optimal?
This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such decomposed systems, and that insights from such analysis will unlock opportunities for scaling them.
By treating the LLM forward pass as the atomic unit of computational cost, one can separate out the (often opaque) inner workings of a particular LLM from the inherent efficiency of how a set of LLMs are orchestrated to solve hard problems.
In other words, if we want to scale the deployment of LLMs to the limit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM primitives should be used to reason about and develop more powerful decompositions of large problems into LLM agents.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

The turn to agents is well underway \cite{guo2024large, wang2024survey}.
Now that large language models (LLMs) have crossed critical thresholds of general capability and reliability \cite{bubeck2023sparks}, it is natural to ask how they can be integrated into computational systems larger than themselves, in order to \emph{do things}: ``Stop speaking and act!''
The term \emph{agent} has quickly crystallized as the term for LLM-based programs that carry out subprocesses within larger systems or act ``autonomously'' in virtual or physical environments.
Unlike the general purpose LLMs from which they descend, LLM agents usually have a specific scope in which they are intended to operate, i.e., specific kinds of inputs they are expected to receive and corresponding kinds of outcomes they are expected to produce.
Agent scopes vary in the precision with which they are specified, but the very act of specifying a scope, or \emph{role}, for an agent allows it to be treated as a computational object that can serve as a component in the development of a larger system.
The promise of LLM agents is not only that they have the capacity to affect change in the real world, but that by constructing increasingly larger systems of many agents, where each has its own complementary focus, we can dramatically expand the scale at which LLM-based AI is applied.

There has been a rush to start building such systems \cite{xi2025rise}.
In academia, LLM agents have been used in applications across the board, for example, in improving performance on standard benchmarks \cite{duimproving}, building teams for more effective software development \cite{qian2024chatdev, wu2024autogen}, simulating communities and human behavior \cite{park2023generative, yan2024opencity}, and tackling the research process itself \cite{huang2023benchmarking, lu2024ai, yu2024researchtown}.
In industry, the impact (at least on strategy and priorities) has been arguably greater: while academics usually prefer minimal systems in which agent behavior can be most clearly understood, industry leaders find themselves faced daily with huge organizations with countless interconnected actors and components, and thus countless opportunities for agents to be applied \cite{hodjat2024ai, urlana2024llms, sypherd2024practical}.

The core focus thus far in the development of such systems has been \emph{``What is possible with the resources we have today?''}, not \emph{``How efficient can it be in the limit of scale?''}
Efficiency at scale can be understandably an afterthought when the goal is to build something that works and is impressive as quickly as possible, but
\textbf{this paper argues that understanding the efficiency of LLM-agent-based systems at a fundamental algorithmic level is critical to achieving the scale such systems promise.}
Specifically, missing from the study of such systems are \emph{asymptotic analyses} that give a fundamental characterization of the \emph{cost} (temporal, energy, or monetary) of running the system at \emph{scale}, e.g., as the size, complexity, and number of problem instances increases to the limit.
Although such analysis may seem esoteric when folks are just at the point of trying to get \emph{something} to work, if such systems are really to scale to millions of users, millions of distinct task instances being solved, increasingly large and complicated tasks to be solved, and beyond, a formal understanding of the asymptotic behavior of these systems will be a key ingredient to guide research and development in the directions most likely to make such scale practically achievable.
Put loosely, this is because solutions that are intuitive or off-the-shelf and appear to work well for initial problems being solved today may be inherently limited when considering the question of scale, and there may be alternative directions that require different kinds of research, directions which careful asymptotic analysis can illuminate.
Asymptotic analysis has been critical to the scaling of systems and algorithms throughout the history of computation \cite{landau1909handbuch, turing1936computable, knuth1968art, aho1974design, sedgewick1996introduction, montanaro2016quantum, cormen2022introduction}, and we see no reason why the scaling of LLM agents should be different.

\textbf{We argue that key to such analysis is the concept of the \emph{LLM primitive}}, i.e., treating the forward pass of an LLM as the atomic unit of computation we are interested in counting in our analysis.
By abstracting away the internals of any particular LLM, the analysis can focus on the higher-level algorithmic behavior that emerges from the roles of agents within larger systems.
Aside from clarifying the analysis, this abstraction ensures that any algorithmic efficiency improvements discovered at the agentic level are orthogonal to the ongoing efficiency improvements of LLMs themselves.

%Aside from discussing the essential value of asymptotic analysis at a high level and the key role it should play in many important topics, the core argumentative approach taken in this paper is \emph{proof by example}.
%That is, by showing three distinct cases where an asymptotic analysis leads to valuable insights that might otherwise go undiscovered, we hope to inspire the reader to pursue this form of analysis in their own work, and we look forward to seeing what novel insights emerge.

To flesh out the argument, this paper proceeds as follows:
Section~\ref{sec:asymptotic_analysis} sketches a basic framework for undertaking asymptotic analysis with LLM primitives (AALPs);
Section~\ref{sec:examples} presents three example analyses using this framework, showing distinct cases where an asymptotic analysis leads to valuable insights that might otherwise go undiscovered;
Section~\ref{sec:alternative_views} engages with alternative views, i.e., ways AALPs might \emph{not} in fact be required;
Section~\ref{sec:research_directions} describes some of the critical research directions for developing AALPs into as useful a tool as possible;
and Section~\ref{sec:conclusion} concludes.

%Aside from enabling the scale-up of LLM-based agentic systems, stimulating the research area around asymptotic analysis with LLM primitives brings several other benefits to the research community, including the following:

%lots of work for academics and low-resource labs to do
%reintegration of algorithmists
%reclaiming of LLM systems research as a science

%\section{Background: LLM Agents}
%\label{sec:llm_agents}

%\section{Reminder: Asymptotic Analysis}
%\label{sec:complexity_theory}


\section{Asymptotic Analysis with LLM Primitives}
\label{sec:asymptotic_analysis}

In order to make the argument of this paper concrete, this section sketches a minimal framework for asymptotic analysis with LLM primitives (\textbf{AALPs}).
Although it elides many phenomena critical to the complete understanding of agentic LLM systems, this basic framework is sufficient to instantiate clear examples of the advantages of an asymptotic analysis-driven approach to system development, as will be shown in Section~\ref{sec:examples}.
This framework can then serve as a seed for future research that develops a more complete approach, as discussed in Section~\ref{sec:research_directions}.
%Importantly, we are interested not in \emph{whether} a problem can be solved (the occupation of the majority of LLM-agents research today), but the cost of producing such solutions at scale.

Let us first define the central object of interest:

\begin{definition}[Language-based Algorithm]
    A language-based algorithm (or LLM-based Algorithm \cite{chen2024design}), \textbf{LbA} for short, is any algorithm in which one or more of the computational steps are performed by an LLM.
\end{definition}

%For simplicity in this paper, let us assume the input and output space of any LbA is the space of token strings $V^*$ for some fixed-sized vocab $V$.

In other words, an LbA consists of a set of LLM-based agents that work together to complete tasks.

In the classical asymptotic analysis of algorithms, the basic unit of computation is a \emph{primitive}, an atomic operation whose executions can be counted characterize the behavior of the algorithm at scale.
Standard choices of a primitive include addition or variable assignment, i.e., operations that can be performed in a single CPU cycle.
This paper argues that the appropriate primitive for asymptotic analysis of agentic LLM-based systems is a single execution of an LLM, i.e., a forward pass that generates a single token $v \in V$.
By treating LLMs as atomic, we can focus on the fundamental behavior of the LbA, orthogonal to ongoing improvements inside the LLMs themselves.
We count only LLM operations because the computational resources to run LLMs are usually the most salient limiting factor in scaling LbAs.
Since our focus is on scaling the deployment of LbAs, we can also ignore training cost, with the expectation that the cost of running inference with fixed models over a long period of time and massive scale will rapidly dominate the cost of training \cite{sardanabeyond} (a loosening of this assumption is discussed in Section~\ref{sec:online}).

For convenience, we assume the cost of an LLM execution depends linearly on the number of input tokens $n$ and the size of the model $m$.
This assumption is based on the fact that industry APIs like those from OpenAI and Anthropic charge linearly per token \cite{openai_api_pricing, anthropic_api_pricing}.
Although in principle the cost of a single forward pass through a transformer model scales quadratically with the input size \cite{vaswani2017attention}, core optimizations like sparse attention \cite{tay2020sparse} and representation caching \cite{luohekeep} have moved the practical cost closer to linear, as reflected in the API prices.
If the cost is in fact superlinear, asymptotic separations between LbAs will generally be even sharper.
So, for the purposes of this paper, we use the following definition:
\begin{definition}[LLM Primitive]
    An LLM primitive is an operation defined by an LLM $M$ of size $m$, such that a single application of $M$ to a string of length $n$ has cost $mn$.
\end{definition}
Key to this definition is the idea that different LLMs $M_1, M_2, \ldots$ can have drastically different sizes $m_1, m_2, \ldots$ and thus drastically different costs.
Notice that we use ``cost'' here in a general sense, it could refer to an estimate of the raw FLOPS, the economic cost \cite{chen2023frugalgpt, shekhar2024towards}, or the environmental cost \cite{bender2021dangers, faizllmcarbon}, and we expect all of these to be highly correlated.
Why wouldn't one always use the smallest LLM available?
Different LLMs have different sets of capabilities they can reliably perform.
For an LLM primitive to be reliably applied in an LbA, the operation performed must be within the \emph{capabilities set} of the LLM being used:

\begin{definition}[Capabilities Set]
    Every LLM $M$ has a corresponding capabilities set $C_M = \{c_1, c_2, \ldots\}$, which characterizes the scope of tasks $M$ can reliably perform.
\end{definition}

In an LbA, different LLM-based operations might be performed by different LLMs with different capabilities.
This separation of labor motivates the specialization of LLMs, i.e., one need not rely on a single giant AGI-like model to solve all tasks that can be specified in text, especially the most basic ones.
A fully optimal LbA will use LLMs with minimal capabilities sets, since increasing the size of the set cannot decrease the minimal size of the required model.

Making the capabilities set explicit in the design of an algorithm has the auxiliary benefit of raising awareness of the trade-offs between two algorithms, i.e., if two algorithms have similar complexity, but one requires more accessible capabilities, that algorithm should be preferred.
Note also that specifying a capability set is only necessary for the purpose of designing the abstract algorithm for complexity analysis; one need not enumerate all the capabilities of a real-world LLM, rather just verify that it has the required capabilities in a particular implementation.
This idea is in line with the call for LLM \emph{specifications} \cite{stoica2024specifications}.

The central tool for achieving asymptotic improvements in LbAs over a na\"ive application of LLMs is the expectation that large and complex jobs can be meaningfully decomposed, and that the impact of this decomposition will scale in a manner not accessible by the na\"ive approach.
Sometimes the precise decomposition can be described analytically or deduced programmatically, but sometimes, especially when reasoning about abstract problems, it can be useful to simply assume we have a way of decomposing a problem, in order to make asymptotic analysis possible.
Any meaningful benefits uncovered in the analysis then motivate the development of a practical implementation of the decomposition.
We call this tool an \emph{oracle decomposition}:

\begin{definition}[Oracle Decomposition]
    An oracle decomposition is an assumed non-LLM operation that decomposes a problem in a specific way, without regard to how the operation is implemented.
\end{definition}

Armed with this basic set of tools, one can proceed to comparatively analyze the asymptotic behavior of different LbAs on different problems, highlighting the kinds of places asymptotic improvements can be found.

Note that there has already been one promising first attempt at a more detailed and formal treatment of AALPs \cite{chen2024design}, and we refer the reader to that work for further example analyses, which complement those in Section~\ref{sec:examples}.
However, the goal of the present paper is not to develop a complete framework, but to present a clear argument with illustrative examples to bring the basic idea of AALPs to as broad an audience as possible.
In other words, the present work is a call to action.

%Not that we propose the use of LLM Primitives instead of \emph{LLM Oracles}, as has been advocated in prior work.
%This choice of term is because our fundamental goal is to be able to determine the overall efficiency of algorithms with respect to LLMs, i.e. we care about the cost of these operations, we are not using them for purely theoretical reasons to show separations between complexity classes.
%By choosing the word \emph{primitive}, we emphasize that they are the building blocks of larger orchestrated algorithmic processes and systems; their contents are abstracted away so that we can focus on the fundamental behavior of these higher-level processes that are key to scaling the impact of AI.





\section{Example Asymptotic Separations}
\label{sec:examples}

This section provides concrete examples of how AALPs can be applied to demonstrate massive asymptotic separations between LbA's when applied at scale.
The goal of these examples is not to show challenging math, but simple results that immediately highlight the importance of such analysis.
Our hope is that these simple examples will inspire future work that tackles more complex LLM-agent systems.

The three example problems in this section highlight different kinds of opportunities that would arise from having a solid complexity theory for LLM agents.
The first highlights the criticality of understanding where to focus LLM size optimization in many-agent systems; the second highlights critical pitfalls of na\"ive anthropomorphic multi-agentization; and the third highlights issues that arise from applying LLMs to large and creative optimization tasks.
Note that the analysis in this section is \emph{intentionally loose}, and intended to be as rudimentary as possible, as the goal is simply to highlight the kinds of insights that can emerge, and argue that further development of AALPs will be critical in understanding how to scale real-world systems.
%Thus, it is one of our primary goals to make the following analyses as rudimentary as possible, to highlight how impactful implications can arise even from such simple analysis, and motivate the application of more powerful analytic techniques.
To that end, we invite the reader to critique any assumption (implicit or explicit) made in the following analyses, with the hope that such critique will lead to more practical AALPs methodologies and thus more powerful LbAs.
%The goal here is to show that there is a latent potential here that is worth critiquing to unlock its impact by developing a complete complexity theory with LLM primitives, and use it to develop and analyze better LbA's.

\subsection{$k$-Task Routing}
\label{sec:routing}

The section highlights potential scaling advantages of having specialized agents of asymptotically smaller size than highly-capabable generalist agents.
For this problem, suppose the input size is $n$ and the output size is $\Theta(1)$.
Many common tasks like information retrieval, classification, and agentic single-action tasks fall under this specification.
Suppose there are $k$ such distinct tasks of this form that we would like the system to be able to perform.

\subsubsection{Generalist Approach}
The simplest approach would be to use a single generalist many-task LLM agent $M_g$ of size $m_g$ capable of solving any of the tasks on its own.
Processing the entire input and returning a constant sized answer will then cost
\begin{equation}
\Theta(m_gn).
\end{equation}

\subsubsection{Delegator and Specialists Approach}

Suppose instead that we have a system consisting of a delegator $M_d$ of size $m_d$ and $k$ specialists $M_1, \ldots, M_k$, one for each of the $k$ distinct tasks.
For simplicity, assume the specialists are all of equal size $m_s$.
Suppose the delegator can determine which of the $k$ tasks the input string belongs to by observing only a constant number of metadata tokens prepended to the input.
Then, the cost of identifying the task and solving it with the designated specialist is
\begin{equation}
    \Theta(m_d) + \Theta(m_sn).
\end{equation}

\subsubsection{Implications}

We can safely assume $m_d \leq m_g$, since solving all $k$ tasks almost certainly requires (implicitly or explicitly) identifying which task is being solved as a sub-capability.
So, as $n$ grows, the comparison of interest is $\Theta(m_gn)$ vs. $\Theta(m_sn)$, i.e., $m_g$ vs. $m_s$.  
From analysis of the neural scaling laws of LLMs \cite{kaplan2020scaling}, we can assume the required size of the LLM scales with the number of required capabilities.
For example, let us assume that in the present setting, the required size of the LLM scales linearly with the number of tasks it can solve.
Then,
\begin{equation}
    \Theta(m_g) = \Theta(km_s) \implies \Theta(m_gn) = \Theta(km_sn).
\end{equation}
That is, the speed-up from delegation and specialization scales linearly with the number of tasks.
As LLM-agent-based systems become larger and larger, many hope that they will be able to tackle thousands of distinct tasks, if not more.
So, the advantage of having many specialist agents becomes enormous.

Importantly, notice that this analysis shows that, for those developing the LLMs themselves, it should be much more impactful to focus on minimizing the size of the specialist LLMs than minimizing the size of the delegator.
This is because, even if the delegator is relatively huge, it is called so few times relative to the specialists.
In a real-world team of human developers, without such analysis, equal research and development resources might be allocated to both reducing the size of the delegator and reducing the size of specialists.
For example, it could be very tempting to try to develop a relatively tiny classifier as the delegator, since it might seem like a relatively simple classification problem.
Analysis like the above can preempt research on shrinking the delegator: It would be okay to use a giant off-the-self maximum-capability LLM as the delegator, and focus on shrinking the specialists.
This observation supports the more general idea that, in an optimal LLM \emph{ecology}, we should expect orders of magnitude more executions of smaller models than of larger models \cite{nisioti2024text}.
%The delegate-and-specialise idea in this section also has an interesting connection to results on Mixture-of-Experts (MoE) systems that have become critical to scaling the internals of LLMs (see Section~\ref{sec:research_directions} for a further discussion).


\subsection{Iterative Code Debugging}

This section looks at another increasingly common and critical task for LLM-agents: code debugging.
At a high level, the problem is that we have a large code-base with many bugs, and we would like to clean it up.
For clarity of analysis, suppose the code is $n = kl$ tokens long, consisting of $k$ functions, the implementation of each being $l$ tokens long, and suppose there are $b$ bugs somewhere in the code, with no more than one bug per function.
The goal is to provide an efficient LbA that fixes all the bugs.

\subsubsection{Na\"ive Multi-agent Approach}

Suppose we take an anthropomorphic multi-agent approach, akin to the kind of approach that has become currently quite popular for pushing the problem-solving performance of coding LLMs \cite{park2023generative, qian2024chatdev, wu2024autogen, song2024adaptive}.
In such an approach, we instantiate multiple agents by prompting them with different roles, mapping each on to an employee in a human organization.
%For example, frameworks like autogen \cite{wu2024autogen} and others \cite{qian2024chatdev, park2023generative, yu2024researchtown} take such an approach.
For simplicity in the present analysis, we will use only two roles: quality assurance (QA) engineer $M_q$ and debugging specialist $M_d$ (we leave it as an exercise to the reader to extend/revise the analysis for other common roles found in multi-agent coding systems).

In this approach, we alternate between $M_q$ (of size $m_q$) identifying a bug and $M_d$ (of size $m_d$) fixing it.
Suppose $M_q$ has the capacity to identify exactly one bug at a time, and $M_d$ has the capacity to fix exactly one identified bug.
In the standard approach, the entire conversation history, including prior versions of the code are fed as input to each agent at each iteration.
From an asymptotic perspective, such accumulation in the input may seem clearly inefficient, but this is an approach taken in popular multi-agent frameworks today, such as autogen \cite{wu2024autogen}.
Suppose the QA engineer identifies bugs by producing constant size messages, while the debugging specialist rewrites the entire code every time it fixes a bug (as is also standard in current systems).
Then, for the $i$th bug, the QA agent looks at all $i$ previous versions of the code and previous QA comments, yielding a total input length of $\Theta(in)$.
So, the total cost of QA over all $b$ bugs is
\begin{equation}
    m_q(\Theta(n) + \Theta(2n) + \ldots + \Theta(bn)) = \Theta(m_qb^2n).
\end{equation}
Meanwhile, the debugging expert looks at $in$ tokens at each iteration and produces $n$ new tokens of code, yielding
\begin{equation}
    m_d(in + (in + 1) + \ldots + (in + n)) = \Theta(m_din^2) 
\end{equation}
at each iteration, and over all $b$ bugs:
\begin{equation}
    \Theta(m_dn^2) + \ldots \Theta(m_dbn^2) = \Theta(m_db^2n^2).
\end{equation}
So, the total cost of QA plus writing the debugged code is
\begin{multline}
    \Theta(m_qb^2n) + \Theta(m_db^2n^2) =\\ \Theta(m_qb^2kl) + \Theta(m_db^2k^2l^2).
\end{multline}

We notice immediately that, due to the additional factor of $n$ for the debugging expert compared to QA engineer, similar to the implication highlighted in Section~\ref{sec:routing}, it is much more important to focus on minimizing the size of the debugging expert than the QA engineer.
It could be fine to use the biggest LLM available for QA, since its relative asymptotic cost is so small, especially when the code grows extremely large.
However, the central implication from this section comes not from relative model size but from comparison to a more focused approach to the multi-agent problem decomposition, as is discussed next.

\subsubsection{Focused Multi-agent Approach}
Let's suppose all functions in the code have completely correct, precise and constant size specification (e.g., in their docstring).
Now, instead of having the QA agent look at the entire code every iteration, suppose it looks only at a single function.
Then, it will overall look at the entire code only a single time, resulting in a cost of
\begin{equation}
    %\mathrm{FocusedQACost} = \Theta(m_qkl).
    \Theta(m_qkl).
\end{equation}
Whenever a bug is identified in a function, the debugging expert then fixes the bug by looking at and rewriting only that single function, yielding a total debugging expert cost of
\begin{equation}
    %\mathrm{FocusedDebuggerCost} = \Theta(m_dl^2b),
    \Theta(m_dl^2b),
\end{equation}
and thus the cost for the whole system is
\begin{equation}
    \Theta(m_qkl) + \Theta(m_dl^2b).
\end{equation}

\subsubsection{Implications}

Since both the QA engineer and debugging expert in the focused version of the system have smaller jobs and less to keep track of, it would be reasonable to assume that they could be of a smaller size than their na\"ive counterparts.
However, even supposing they are no smaller, we have relative speed-ups of $\Theta(b^2)$ for QA and $\Theta(bk^2)$ for the debugging expert.
Since $n \geq b$, and the job of the debugging expert is intuitively more challenging than that of QA (since it has to actually produce correct code), the performance improvement that is likely to dominate the relative cost of the two approaches is that of the debugging expert:
\begin{equation}
    \Theta(bk^2).
\end{equation}

This is a huge speed-up, and highlights the power of breaking down solutions into the minimal possible bite-sized chunks that can be precisely specified for agents to operate on.
This improvement becomes astronomical as the size of the code increases to the size of large industrial codebases.
The apparent limitations of most existing multi-agent LLM approaches to relatively small codebases could be due in part to this scaling behavior, and thus pursuing the direction of extreme decomposition could be critical to scaling robust LLM-based coding systems.

This implication also generalizes to other applications where the goal is to iteratively refine large objects of interest, e.g., design or construction applications.
The insights from a properly developed asymptotic analysis with LLM primitives could be critical to making such applications scale.

Notice that we made a strong and critical assumption of perfect code specification in the description of the focused multi-agent approach.
The fact that this leads to such efficiency improvements is motivation to investigate whether LLMs are capable of generating such precise and correct specifications; if such focused precision is possible, it could have an enormous impact.

%Is it fair to wonder that Microsoft, the creators of autogen, with its inherent inefficiencies, have an interest in increased usage of the OpenAI api?
%This interesting economic conflict-of-interest makes it even more essential for academics to understand the fundamental algorithmic asymptotics of LLM-agent-based systems, and not simply rely directly on the off-the-shelf tools of the major industry labs.



\subsection{Evolutionary Optimization}

Another area where LLM agents are increasingly being applied is in optimization, where the LLM is used as the engine of variation, i.e., given some existing solutions it is used to generate variations on these solutions that have a chance of being improvements with respect to some evaluation function \cite{lehman2023evolution, meyerson2024language, bradleyquality, yang2024large, romera2024mathematical, lee2025evolving}.
Many of these applications have been developed under evolutionary optimization frameworks \cite{back1997handbook, wu2024evolutionary}.
In such scenarios, there is some evaluation, or \emph{fitness}, function $f(x)$ that we would like to maximize, and the LLM is responsible for generating solutions $x \in \mathcal{X}$.
If $x$ is representable by text, then an LLM can be applied for this role, and, in theory, any solution type is representable by text \cite{meyerson2024language}.

Let us suppose $f$ is of a special form that is common for analysis of evolutionary algorithms, i.e., it is a variant of the \textsc{OneMax} function \cite{doerr2020probabilistic, witt2013tight}.
Specifically, suppose each solution $x$ has length $n = kl$ tokens, such that it consists of $k$ blocks each of length $l$.
Suppose each block is either ``correct'' or ``incorrect'', and the value of $f(x)$ is the number of correct blocks in $x$.
Then, the maximum value of $f(x)$ is $k$, in the case that all blocks are correct.
Note that in practice there may be many ways for a specific block to be considered correct.% (corresponding to a multi-valued variant of \textsc{OneMax} \cite{}

\subsubsection{Global Mutation}

Suppose we have an LLM-based mutation agent $M_u$ of size $m_u$, that, when applied to an input string $x$, alters every block of $x$ in \emph{some} way, resulting in a 50\% chance of that block now being ``correct''.
Suppose we start with an arbitrary initial solution $x$, and run a greedy algorithm, where we iteratively apply the mutator to get a new solution $x'$, and replace $x'$ as our current solution if it has higher fitness $f(x') > f(x)$.
Note, this is an instantiation of a (1+$\lambda$)-EA \cite{droste2002analysis}, the most often analyzed algorithm in the EA literature \cite{doerr2021survey}, which has been used to analyze the optimization of deep architectures in other settings \cite{muir2019, ee2022}.

Now, if $M_u$ is applied to the entire string $x$, the expected number of times it needs to be applied to generate a completely correct solution is $2^k$.
Since each application of $M_u$ is to the whole solution, the generation of each new solution costs $\Theta(m_un^2)$, and thus the total expected cost is
\begin{equation}
    \Theta(m_u2^kn^2) = \Theta(m_u2^kk^2l^2),
\end{equation}
which is completely impractical even for any decently sized value of $k$.
This huge expected cost could be a reason we have seen LLM-based optimization methods being applied mainly to small (though interesting and semantically complex) problems: They are usually applied to the entire solution all at once.

\subsubsection{Local Mutation}

Suppose instead we have a programmatic way of determining the boundaries of blocks (oracle decomposition), and, instead of applying $M_u$ to the entire solution at once, it is applied to each block individually and in sequence, i.e., generating one updated block at a time.
Then, each application will cost $\Theta(m_ul^2)$, and the total expected cost will be
\begin{equation}
    \Theta(m_ukl^2).
\end{equation}


\subsubsection{Implications}

The relative performance improvement of the local mutation approach over the global one is enormous:
\begin{equation}
    \Theta(2^kk).
\end{equation}
Notice also that this improvement is before any consideration of model size, i.e., before considering that a capable local mutator agent might be substantially smaller than a global one.
Of course, local mutations will not be sufficient for all evaluation functions, particularly those that are non-convex.
However, they can still be quite powerful and practical tools, and gradient descent itself is in the most basic sense a local variation operator.

Again, we invite the reader to look back at the assumptions made in this section and see where there is room for improvement.
For example, we assume that solutions here can be broken down into components programmatically, but in practice, one may need to assume something fuzzier and more approximate.

In any case, this example points again to the criticality of problem decomposition in a dramatic case where it leads to super-exponential improvements at scale.
This simple analysis shines a light on the potential of a specific subfield (i.e., optimization driven by LLMs) to benefit from careful asymptotic analysis of their constituent LbAs.

Finally, notice that, in contrast to the prior two problems, this section considered a case where the LLM agent acts reliably, but not deterministically, hinting at further opportunities to generalize this kind of analysis.

\subsection{Exercises}

The above three examples are intended as representatives of a vast space of possible LbAs for which asymptotic analysis is critical.
To become more deeply acquainted with the position of this paper, we invite the reader to conduct their own analysis on variants of these problems as well as other problems, such as the below:
\begin{itemize}
    \item Sorting sets of large documents, e.g., for organizing legal arguments or prioritizing resumes (especially long and plentiful resumes of AI agents themselves).
    \item Solving a text-based puzzle, e.g., reordering the shuffled sentences of a novel to their original order.
    \item Writing and refining a paper for submission to ICML.
    \item Creating a policy proposal, e.g., to fight climate change or for pandemic response.
    \item Modifying the analyses in Section~\ref{sec:examples} with the assumption that the cost of an LLM forward pass is quadratic in the input size instead of linear.
    \item Designing and booking an optimal $k$-night vacation given a set of constraints.
\end{itemize}
We are curious to see what themes emerge from researchers with differing backgrounds approaching asymptotic analysis with LLM primitives in different ways on these problems and others.
Further examples of problems ripe for such analysis can be found in recent work \cite{chen2024design}.

%\subsection{Classification}

%\subsection{Sorting}

%\subsection{Mixture-of-Experts}
%\label{sec:moe}

\section{Alternative Views}
\label{sec:alternative_views}

Now that we have made the case for the adoption of asymptotic analysis with LLM primitives in the understanding and development of scaled LLM-based agentic systems, it is worth reflecting on possible counterarguments to this position.
Such an engagement can yield the identification of strengths and weaknesses of the position, and further promising directions of thinking thereby.
We invite the reader to come up with alternative views beyond the ones discussed here, as there are surely more and stronger ones.

\paragraph{Alternative View 1:} \emph{All the implications of such analysis are things people would do anyway.}

The intuition behind this view is that the algorithmic properties of agentic LLM systems are not that complicated, and optimal cost optimization of such systems will follow naturally from the kinds of refinements researchers and engineers tend to do anyway.
If true, this would be a huge blow to the argument for AALPs, since it would not add any practical value to LLM-based systems, it would just serve as a source of esoteric exercises for the algorithmically inclined.
However, we believe the evidence provided in Section~\ref{sec:examples} is sufficient to dispel this view, i.e., insights from the analysis will have real-world impact on where human and computational resources are focused, and thus have an impact on the timeline of deployment for scaled LLM agents.

\paragraph{Alternative View 2:} \emph{Direct optimization for cost is good enough, without thinking about asymptotic behavior.}

The view here is that for any given real-world LLM agentic system, the general aymptotic limiting case is not as important as the simple reality of optimizing for the problems the particular instantiation of the system faces today.
This view is similar to Alternative View 1, except that it's focused on particular instantiations of a system.
For example, we might imagine an automated external optimization process that refines the design of an agentic system, e.g., modifying agent scopes, modifying the communication channels between agents, and swapping out the underlying LLMs of different agents.
However, although such a process could yield meaningful speed-ups in special cases, the innovations discovered in such a system would likely not generalize to greater problem scales, i.e., in the scaling limit.
This failure of generalization is common in systems overly-optimized to overly-specific use cases \cite{tan2019efficientnet}.
Although it can be possible to mine general insights from such specialized innovations, most of the optimizations should be expected to lead to dead-ends at scale.
AALPs is the tool to free cost optimization from such tempting dead-ends.

\paragraph{Alternative View 3:} \emph{We don't need this for superintelligence, and once we get superintelligence, it will optimize everything better than humans ever could.}

We believe this is currently the strongest case against AALPs.
The view here is that progress is being made so fast at the core of LLMs that we are bound to hit a level of general intelligence and automated AI research capabilities beyond that of human AI researchers before we need to scale modular and compute-optimized many-agent systems in the real world.
Once such superintelligence is achieved, the superintelligent AI itself will be capable of performing asymptotic analysis as it sees fit, and will be otherwise responsible for developing future algorithmic insights leading to scaled agentic systems.
As there is some evidence of slowing progress at the core of LLM development \cite{cyran2024aimodels, booth2024hasai}, and there is always uncertainty of whether a particular research direction will hit a fundamental wall, we believe that the orthogonal AALPs approach, which foregrounds the advantages of many-agentness, is a worthwhile complement to the direct approach of LLM improvement.
We also believe that since AALPs is such a fundamental tool for understanding the behavior of agentic LLM-based systems, and agentic decomposition is fundamentally essential to the optimal use of resources in the limit (i.e., a self-optimized superintelligence will not use its full power to do single-digit addition), such a superintelligence must rely on AALPs as it decides how to optimize its usage of computational resources in the physical universe, and, for those human researchers developing and applying AALPs methods, having a superintelligence use your conceptual technology is not such a bad academic legacy.

%\begin{enumerate}
%    \item Direct optimization for cost is good enough, without thinking about asymptotic complexity.
%    \item We don't need decomposition, LLMs are just getting more capable and efficient all the time.
%    \item We don't need this for super-intelligence, and once we get super-intelligence, it can do whatever it wants to optimize itself.
%    \item We don't need a new complexity theory, the old one is good enough.
%    \item All the implications of such a theory are things people would do anyway.
%\end{enumerate}

\section{Research Directions}
\label{sec:research_directions}

The goal of this paper is to serve as a catalyst for the adoption and development of asymptotic analysis with LLM primitives into the research and development of LLM-based agentic systems.
As a result, the discussion has focused on high-level principles and minimal motivating examples to highlight the central advantages of such analysis as clearly as possible, and many critical considerations have been thus far ignored.
This section enumerates several such considerations, to serve as a scaffolding for future research.

\subsection{Stochasticity and Error-correction}
\label{sec:stochasticity}

For simplicity, the analysis in Sections~\ref{sec:asymptotic_analysis} and \ref{sec:examples} assumed that the constituent LLMs can execute capabilities in their scope reliably, i.e., the scenario where the LLM does not successfully fulfill its role is not considered.
Of course, LLMs today do make errors, even surprisingly simple ones \cite{basmov2023simple, williams2024easy, lehman2025evolution}.
A more complete analytical framework will take the likelihood of such errors into account.
Initial work has developed core ideas in how error probabilities can compound even in the roll-outs of single LLMs \cite{dziri2024faith}; further work is needed to extend such insights to the realm of LLM agents.
For example, ideas of error correction, e.g., from information theory \cite{hamming1950error, pless2011introduction}, could be integrated into agentic systems to reduce the impact of errors at scale, akin to the centrality of error-correction in quantum computing \cite{lidar2013quantum, roffe2019quantum}.
One open problem key to making such error correction work is developing LLM agents whose errors are decorrelated.
Established theory of randomized algorithms will be an essential resource for this project \cite{motwani1996randomized, mitzenmacher2017probability}.


\subsection{Asynchrony and Distributed Algorithms}
\label{sec:asynchrony}

Similarly, the examples in Section~\ref{sec:examples} did not consider potential (temporal) cost benefits from agents running asynchronously, in a distributed manner, or otherwise in parallel.
Such parallelization is extremely natural, especially if the central mechanism for achieving asymptotic improvements is problem decomposition.
Similar to the case of randomized algorithms above, existing methodologies for asymptotic analysis in parallel, asynchronous, and distributed systems \cite{akl1989design, baudet1978design, santoro2006design}, as well as classical multi-agent systems \cite{ferber1999multi, van2008multi, hodjat1998adaptive}, will be an invaluable resource, and the incorporation of such aspects will be required to clarify the full scope of the advantages of carefully designed LLM-agent systems.

%\subsection{Maximal Specialization}
%\label{sec:specialization}

\subsection{Automatic Decomposition}
\label{sec:decomposition}

A central motivating theme throughout this paper is the expectation that the jobs of large and complex LLM-based agentic systems can be usefully decomposed into subproblems, and some such decompositions lead to asymptotic improvements over others.
In the examples in Section~\ref{sec:examples} we assumed that effective decompositions were available a priori or could be deduced programmatically (i.e., without the use of LLMs).
In practice, especially in more open-ended systems where the full spectrum of tasks the system might need to solve is not known beforehand, having automatic decomposition methods will be critical to maintaining/maximizing asymptotic performance.
Such methods will likely rely on LLMs themselves, in which case their cost must be incorporated into AALPs.
There has been some initial work on automatic agent decomposition \cite{wu2024autogen, song2024adaptive}, but most is based on intuitive zero-shot approaches; much more asymptotically advantageous approaches should be possible.

\subsection{Extensions to Other Modalities}
\label{sec:other_modalities}

This paper argues that AALPs is required to scale LLM agents, but as agentic systems naturally grow alongside AI models that incorporate a greater and greater range of modalities \cite{team2023gemini, liang2024survey}, the capabilities afforded by these modalities will naturally be incorporated into agentic systems; this process has already begun \cite{jiang2024multi, gao2024multi, sarch2024vlm}.
The same techniques used for AALPs will naturally extend to other modalities, by encapsulating the usage of each modality as a primitive with an assigned cost.
A simple implication could be that many visual tasks (when performed at scale) should be executed by compact vision-only models instead of full-fledged multi-modal foundation models.

\subsection{Online Learning and Adaptation}
\label{sec:online}

This paper has focused on the case where all LLM training happens beforehand, and thus can be viewed as a fixed cost in comparison to the inference (forward-pass) costs of running an LLM-agent system at scale over a long timeframe.
However, for more adaptive systems, it may be essential to allow some form of agent learning during deployment.
Such learning could consist of the collection and refinement of memories (represented in text) as the system encounters new scenarios \cite{park2023generative, wang2024voyager}, or fine-tuning updates through gradient descent \cite{hu2023meta, rannen2024revisiting, ding2023parameter, han2024parameter}.
In any case, the computational cost of such learning, dependent on the size of the constituent LLM, will need to be carefully incorporated into AALPs.

\subsection{Ethics and Sentience}
\label{sec:ethics}

Finally, one side benefit of highly-decomposed modular many-agent LLM systems is a potential massive reduction in future machine suffering.
As centralized AI systems become larger and more capable, there are compelling arguments that at some threshold of capabilities \emph{sentience has to emerge}, at which point the AI system will likely endure astronomical levels of suffering \cite{metzinger2021artificial, saad2022digital, chiang2021ai}.
One avenue to minimizing the likelihood of such suffering is to build highly-decomposed many-agent systems that achieve the same positive impact of a centralized system, but, by minimizing the capabilities and scope of each AI, reducing the chance that sentience will emerge.
This argument is related to a position paper from last year, arguing that reducing the scope of AI memory will reduce the chance of suffering, since memory is such a critical ingredient for suffering in humans \cite{tkachenkoposition}.
If complemented by further research in how sentience can emerge, AALPs could thus play a role in reducing the chance of immeasurable machine suffering. 

%\section{Discussion}

%\begin{enumerate}
%    \item Orthogonality to improvements in LLMs themselves.
%    \item Relation to economic and environmental analysis.
%    \item Relation to classical MAS analysis \cite{} (cite AAOSA)
%    \item Relation to LLM error analysis \cite{}
%    \item Relation to existing LbA paper \cite{}
%    \item Relation to AI Consciousness
%    \item Relation to Ecologies, biological and human/economic
%\end{enumerate}

\section{Conclusion}
\label{sec:conclusion}

This paper has claimed that asymptotic analysis with LLM primitives will be critical to the project of scaling LLM-based agentic systems.
The argument for this position was developed through contextualization with existing work, concrete examples of how such analysis can lead to impactful insights, and engagement with alternative views.
Several important directions were then highlighted as a guide for future research.
Our hope is that this discussion will serve as a catalyst, motivating developers of LLM agents to carefully consider how their systems might scale, establishing an exciting research project within the cutting edge of AI for algorithmists and other academics who do not have access to hyperscale computing resources, and, most of all, to accelerate the real-world positive impact of LLM agents.


% Acknowledgements should only appear in the accepted version.
%\section*{Acknowledgements}

%\textbf{Do not} include acknowledgements in the initial version of
%the paper submitted for blind review.

%If a paper is accepted, the final camera-ready version can (and
%usually should) include acknowledgements.  Such acknowledgements
%should be placed at the end of the section, in an unnumbered section
%that does not count towards the paper page limit. Typically, this will 
%include thanks to reviewers who gave useful comments, to colleagues 
%who contributed to the ideas, and to funding agencies and corporate 
%sponsors that provided financial support.

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of artificial intelligence.
There are many societal consequences that potentially follow from such work.
One main advantage of the approach advocated in this paper is the potential reduction in future machine suffering, as discussed in Section~\ref{sec:ethics}.
Another advantage of the fact that AALPs leads naturally to maximally decomposed problem-solving is that decomposed systems can be much more interpretable, auditable, and therefore safer than a single opaque centralized AI carrying out the same task, a point that has been acknowledged in prior work \cite{khot2021text, sharkey2025openproblemsmechanisticinterpretability}.

\bibliography{scaling_agents}
\bibliographystyle{icml2025}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
