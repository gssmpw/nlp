\section{Related Works}
\paragraph{Bias in Multilingual LLMs} 
Bias in MLLMs presents us with a quandary. It has emerged as a critical challenge to the fairness of MLLMs and thus significantly restricts real-world deployment~\cite{xu-etal-2023-language-representation}.
Numerous studies have been conducted to measure language bias, which refers to the unidentical performance of MLLMs across different languages in terms of race, religion, nationality, gender, and other factors~\cite{zhao2024gender, mihaylov-shtedritski-2024-elegant, mukherjee-etal-2023-global, neplenbroek2024mbbq, li-etal-2024-land, vashishtha-etal-2023-evaluating, naous-etal-2024-beer,hofmann2024ai}. Most of these studies primarily focus on the lexical preferences of models, either by assigning the descriptions of specific groups with positive or negative meanings or by assessing the modelâ€™s ability to infer the identity of a subject described in an objectively neutral manner. Among the most relevant studies in this line of research, \citet{narayanan-venkit-etal-2023-nationality} examine whether the use of adjectives by language models defined by nationalities in English are positive or negative. \citet{zhu-etal-2024-quite} further extend this analysis to a Chinese context. Additionally, \citet{kamruzzaman-etal-2024-investigating}, \citet{nie-etal-2024-multilingual} and \citet{parrish-etal-2022-bbq} constructed multiple-choice selection evaluations in English. Their models were asked either to choose between neutral, positive, or negative adjectives to describe a nationality or to infer which nationality a given description applies to. While these studies provide valuable insights into nationality bias in LLMs, they are largely limited to monolingual settings and focus primarily on lexical-level biases. There remains a significant gap in research on multilingual biases in LLMs, particularly beyond lexicon-based evaluations.

\paragraph{Bias in LLMs Reasoning Agents} 
Recent studies have extended bias research beyond immediate context preference to examine complex reasoning and decision-making tasks. Several studies have investigated the use of LLMs as simulations of multilingual survey subjects.
\citet{jin2024language} examined LLM performance in moral reasoning tasks, particularly in responding to variations of the Trolley Problem. \citet{durmus2023towards} explored the subjective global opinions of LLMs by prompting models to answer binary-choice questions under explicit persona settings in a multilingual context. \citet{kwok2024evaluating} further advanced this approach by developing the Simulation of Synthetic Personas, and designing questionnaires based on real-world news to assess biases in model-generated responses. 
While these studies provide valuable insights into biases in complex reasoning and decision-making tasks under multilingual settings, fall short of providing a comprehensive understanding of real-world applications. 
Other studies addressed tasks such as hiring screening agents~\cite{armstrong2024silicon} and university application agents~\cite{zheng2024dissecting} in English. Not only their studies limited to English, but they constrain the models by restricting their ability to engage in chain-of-thought (CoT)-like reasoning during responses. This significantly limits the scope and depth of bias analysis in structured decision-making processes.