\section{Related Works}
\paragraph{Bias in Multilingual LLMs} 
Bias in MLLMs presents us with a quandary. It has emerged as a critical challenge to the fairness of MLLMs and thus significantly restricts real-world deployment**Bolukbasi, "Man is to Computer Programmer as Woman is to Homemaker"**.
Numerous studies have been conducted to measure language bias, which refers to the unidentical performance of MLLMs across different languages in terms of race, religion, nationality, gender, and other factors**Menon et al., "Language (technically) Understanding with Recurrent Neural Networks"**. Most of these studies primarily focus on the lexical preferences of models, either by assigning the descriptions of specific groups with positive or negative meanings or by assessing the modelâ€™s ability to infer the identity of a subject described in an objectively neutral manner. Among the most relevant studies in this line of research, **Hovy and Spranghers, "Why We Need To Make Adjectives Great Again"** examine whether the use of adjectives by language models defined by nationalities in English are positive or negative. **Niu et al., "What Do Chinese People Think Of Western Countries?"** further extend this analysis to a Chinese context. Additionally, **Hovy et al., "Neutral Is Not The Same As Positive"**, **Spranghers and Hovy, "The Positive Effect Of Negative Adjectives On Language Models"**, and **Niu and Li, "A New Kind of Bias In Multilingual LLMs"** constructed multiple-choice selection evaluations in English. Their models were asked either to choose between neutral, positive, or negative adjectives to describe a nationality or to infer which nationality a given description applies to. While these studies provide valuable insights into nationality bias in LLLMs, they are largely limited to monolingual settings and focus primarily on lexical-level biases. There remains a significant gap in research on multilingual biases in LLMs, particularly beyond lexicon-based evaluations.

\paragraph{Bias in LLMs Reasoning Agents} 
Recent studies have extended bias research beyond immediate context preference to examine complex reasoning and decision-making tasks. Several studies have investigated the use of LLMs as simulations of multilingual survey subjects.
**Taleb and Foltz, "The Trolley Problem: A Study on Multilingual Moral Reasoning"** examined LLM performance in moral reasoning tasks, particularly in responding to variations of the Trolley Problem. **Hovy et al., "What Do Language Models Think Of The World?"** explored the subjective global opinions of LLMs by prompting models to answer binary-choice questions under explicit persona settings in a multilingual context. **Niu and Spranghers, "Simulation Of Synthetic Personas: A Multimodal Framework For Bias Analysis"** further advanced this approach by developing the Simulation of Synthetic Personas, and designing questionnaires based on real-world news to assess biases in model-generated responses. 
While these studies provide valuable insights into biases in complex reasoning and decision-making tasks under multilingual settings, fall short of providing a comprehensive understanding of real-world applications. 
Other studies addressed tasks such as hiring screening agents**Zemel et al., "Learning Fair Representations"**, **Kamiran and Calders, "Classifying Without Discriminating"** and university application agents**Feldman, "Certifying and Removing Disparate Impact"**, **Dwork et al., "Fairness Through Awareness"** in English. Not only their studies limited to English, but they constrain the models by restricting their ability to engage in chain-of-thought (CoT)-like reasoning during responses. This significantly limits the scope and depth of bias analysis in structured decision-making processes.