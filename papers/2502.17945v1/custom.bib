% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{ferrara2023should,
  title={Should chatgpt be biased? challenges and risks of bias in large language models},
  author={Ferrara, Emilio},
  journal={arXiv preprint arXiv:2304.03738},
  year={2023}
}


@article{kwok2024evaluating,
  title={Evaluating cultural adaptability of a large language model via simulation of synthetic personas},
  author={Kwok, Louis and Bravansky, Michal and Griffin, Lewis D},
  journal={arXiv preprint arXiv:2408.06929},
  year={2024}
}

@article{thurstone1927law,
  title={A law of comparative judgment.},
  author={Thurstone, LL},
  journal={Psychological Review},
  volume={34},
  number={4},
  pages={273},
  year={1927},
  publisher={Psychological Review Company}
}

@inproceedings{armstrong2024silicon,
  title={The Silicon Ceiling: Auditing GPTâ€™s Race and Gender Biases in Hiring},
  author={Armstrong, Lena and Liu, Abbey and MacNeil, Stephen and Metaxa, Dana{\"e}},
  booktitle={Proceedings of the 4th ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
  pages={1--18},
  year={2024}
}

@article{zhao2024gender,
  title={Gender Bias in Large Language Models across Multiple Languages},
  author={Zhao, Jinman and Ding, Yitian and Jia, Chen and Wang, Yining and Qian, Zifan},
  journal={arXiv preprint arXiv:2403.00277},
  year={2024}
}

@article{shi2022language,
  title={Language models are multilingual chain-of-thought reasoners},
  author={Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang, Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung, Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.03057},
  year={2022}
}

@article{durmus2023towards,
  title={Towards measuring the representation of subjective global opinions in language models},
  author={Durmus, Esin and Nyugen, Karina and Liao, Thomas I and Schiefer, Nicholas and Askell, Amanda and Bakhtin, Anton and Chen, Carol and Hatfield-Dodds, Zac and Hernandez, Danny and Joseph, Nicholas and others},
  journal={arXiv preprint arXiv:2306.16388},
  year={2023}
}

@article{neplenbroek2024mbbq,
  title={MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs},
  author={Neplenbroek, Vera and Bisazza, Arianna and Fern{\'a}ndez, Raquel},
  journal={arXiv preprint arXiv:2406.07243},
  year={2024}
}

@article{jin2024language,
  title={Language Model Alignment in Multilingual Trolley Problems},
  author={Jin, Zhijing and Kleiman-Weiner, Max and Piatti, Giorgio and Levine, Sydney and Liu, Jiarui and Gonzalez, Fernando and Ortu, Francesco and Strausz, Andr{\'a}s and Sachan, Mrinmaya and Mihalcea, Rada and others},
  journal={arXiv preprint arXiv:2407.02273},
  year={2024}
}

@article{zheng2024dissecting,
  title={Dissecting bias of ChatGPT in college major recommendations},
  author={Zheng, Alex},
  journal={Information Technology and Management},
  pages={1--12},
  year={2024},
  publisher={Springer}
}


@article{hofmann2024ai,
	Abstract = {Hundreds of millions of people now interact with language models, with uses ranging from help with writing1,2 to informing hiring decisions3. However, these language models are known to perpetuate systematic racial prejudices, making their judgements biased in problematic ways about groups such as African Americans4--7. Although previous research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time, particularly in the United States after the civil rights movement8,9. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice, exhibiting raciolinguistic stereotypes about speakers of African American English (AAE) that are more negative than any human stereotypes about African Americans ever experimentally recorded. By contrast, the language models'overt stereotypes about African Americans are more positive. Dialect prejudice has the potential for harmful consequences: language models are more likely to suggest that speakers of AAE be assigned less-prestigious jobs, be convicted of crimes and be sentenced to death. Finally, we show that current practices of alleviating racial bias in language models, such as human preference alignment, exacerbate the discrepancy between covert and overt stereotypes, by superficially obscuring the racism that language models maintain on a deeper level. Our findings have far-reaching implications for the fair and safe use of language technology.},
	Author = {Hofmann, Valentin and Kalluri, Pratyusha Ria and Jurafsky, Dan and King, Sharese},
	Da = {2024/09/01},
	Date-Added = {2025-02-21 16:06:23 +0900},
	Date-Modified = {2025-02-21 16:06:23 +0900},
	Doi = {10.1038/s41586-024-07856-5},
	Id = {Hofmann2024},
	Isbn = {1476-4687},
	Journal = {Nature},
	Number = {8028},
	Pages = {147--154},
	Title = {AI generates covertly racist decisions about people based on their dialect},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/s41586-024-07856-5},
	Volume = {633},
	Year = {2024},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41586-024-07856-5}}


@inproceedings{blasi-etal-2022-systematic,
    title = "Systematic Inequalities in Language Technology Performance across the World`s Languages",
    author = "Blasi, Damian  and
      Anastasopoulos, Antonios  and
      Neubig, Graham",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.376/",
    doi = "10.18653/v1/2022.acl-long.376",
    pages = "5486--5505",
    abstract = "Natural language processing (NLP) systems have become a central technology in communication, education, medicine, artificial intelligence, and many other domains of research and development. While the performance of NLP methods has grown enormously over the last decade, this progress has been restricted to a minuscule subset of the world`s $\approx$6,500 languages. We introduce a framework for estimating the global utility of language technologies as revealed in a comprehensive snapshot of recent publications in NLP. Our analyses involve the field at large, but also more in-depth studies on both user-facing technologies (machine translation, language understanding, question answering, text-to-speech synthesis) as well as foundational NLP tasks (dependency parsing, morphological inflection). In the process, we (1) quantify disparities in the current state of NLP research, (2) explore some of its associated societal and academic factors, and (3) produce tailored recommendations for evidence-based policy making aimed at promoting more global and equitable language technologies. Data and code to reproduce the findings discussed in this paper areavailable on GitHub (\url{https://github.com/neubig/globalutility})."
}

@article{gallegos-etal-2024-bias,
    title = "Bias and Fairness in Large Language Models: A Survey",
    author = "Gallegos, Isabel O.  and
      Rossi, Ryan A.  and
      Barrow, Joe  and
      Tanjim, Md Mehrab  and
      Kim, Sungchul  and
      Dernoncourt, Franck  and
      Yu, Tong  and
      Zhang, Ruiyi  and
      Ahmed, Nesreen K.",
    journal = "Computational Linguistics",
    volume = "50",
    number = "3",
    month = sep,
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.cl-3.8/",
    doi = "10.1162/coli_a_00524",
    pages = "1097--1179",
    abstract = "Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs."
}

@inproceedings{xu-etal-2023-language-representation,
    title = "Language Representation Projection: Can We Transfer Factual Knowledge across Languages in Multilingual Language Models?",
    author = "Xu, Shaoyang  and
      Li, Junzhuo  and
      Xiong, Deyi",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.226/",
    doi = "10.18653/v1/2023.emnlp-main.226",
    pages = "3692--3702",
    abstract = "Multilingual pretrained language models serve as repositories of multilingual factual knowledge. Nevertheless, a substantial performance gap of factual knowledge probing exists between high-resource languages and low-resource languages, suggesting limited implicit factual knowledge transfer across languages in multilingual pretrained language models. This paper investigates the feasibility of explicitly transferring relatively rich factual knowledge from English to non-English languages. To accomplish this, we propose two parameter-free $\textbf{L}$anguage $\textbf{R}$epresentation $\textbf{P}$rojection modules (LRP2). The first module converts non-English representations into English-like equivalents, while the second module reverts English-like representations back into representations of the corresponding non-English language. Experimental results on the mLAMA dataset demonstrate that LRP2 significantly improves factual knowledge retrieval accuracy and facilitates knowledge transferability across diverse non-English languages. We further investigate the working mechanism of LRP2 from the perspectives of representation space and cross-lingual knowledge neuron."
}

@inproceedings{mihaylov-shtedritski-2024-elegant,
    title = "What an Elegant Bridge: Multilingual {LLM}s are Biased Similarly in Different Languages",
    author = "Mihaylov, Viktor  and
      Shtedritski, Aleksandar",
    editor = "Peled-Cohen, Lotem  and
      Calderon, Nitay  and
      Lissak, Shir  and
      Reichart, Roi",
    booktitle = "Proceedings of the 1st Workshop on NLP for Science (NLP4Science)",
    month = nov,
    year = "2024",
    address = "Miami, FL, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.nlp4science-1.3/",
    doi = "10.18653/v1/2024.nlp4science-1.3",
    pages = "16--23",
    abstract = "This paper investigates biases of Large Language Models (LLMs) through the lens of grammatical gender. Drawing inspiration from seminal works in psycholinguistics, particularly the study of gender`s influence on language perception, we leverage multilingual LLMs to revisit and expand upon the foundational experiments of Boroditsky (2003). Employing LLMs as a novel method for examining psycholinguistic biases related to grammatical gender, we prompt a model to describe nouns with adjectives in various languages, focusing specifically on languages with grammatical gender. In particular, we look at adjective co-occurrences across gender and languages, and train a binary classifier to predict grammatical gender given adjectives an LLM uses to describe a noun. Surprisingly, we find that a simple classifier can not only predict noun gender above chance but also exhibit cross-language transferability. We show that while LLMs may describe words differently in different languages, they are biased similarly."
}

@inproceedings{mukherjee-etal-2023-global,
    title = "{G}lobal {V}oices, Local Biases: Socio-Cultural Prejudices across Languages",
    author = "Mukherjee, Anjishnu  and
      Raj, Chahat  and
      Zhu, Ziwei  and
      Anastasopoulos, Antonios",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.981/",
    doi = "10.18653/v1/2023.emnlp-main.981",
    pages = "15828--15845",
    abstract = "Human biases are ubiquitous but not uniform: disparities exist across linguistic, cultural, and societal borders. As large amounts of recent literature suggest, language models (LMs) trained on human data can reflect and often amplify the effects of these social biases. However, the vast majority of existing studies on bias are heavily skewed towards Western and European languages. In this work, we scale the Word Embedding Association Test (WEAT) to 24 languages, enabling broader studies and yielding interesting findings about LM bias. We additionally enhance this data with culturally relevant information for each language, capturing local contexts on a global scale. Further, to encompass more widely prevalent societal biases, we examine new bias dimensions across toxicity, ableism, and more. Moreover, we delve deeper into the Indian linguistic landscape, conducting a comprehensive regional bias analysis across six prevalent Indian languages. Finally, we highlight the significance of these social biases and the new dimensions through an extensive comparison of embedding methods, reinforcing the need to address them in pursuit of more equitable language models."
}

@inproceedings{li-etal-2024-land,
    title = "This Land is {Your, My} Land: Evaluating Geopolitical Bias in Language Models through Territorial Disputes",
    author = "Li, Bryan  and
      Haider, Samar  and
      Callison-Burch, Chris",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.213/",
    doi = "10.18653/v1/2024.naacl-long.213",
    pages = "3855--3871",
    abstract = "Do the Spratly Islands belong to China, the Philippines, or Vietnam? A pretrained large language model (LLM) may answer differently if asked in the languages of each claimant country: Chinese, Tagalog, or Vietnamese. This contrasts with a multilingual human, who would likely answer consistently. In this paper, we show that LLMs recall certain geographical knowledge inconsistently when queried in different languages{---}a phenomenon we term geopolitical bias. As a targeted case study, we consider territorial disputes, an inherently controversial and multilingual task. We introduce BorderLines, a dataset of territorial disputes which covers 251 territories, each associated with a set of multiple-choice questions in the languages of each claimant country (49 languages in total). We also propose a suite of evaluation metrics to precisely quantify bias and consistency in responses across different languages. We then evaluate various multilingual LLMs on our dataset and metrics to probe their internal knowledge and use the proposed metrics to discover numerous inconsistencies in how these models respond in different languages. Finally, we explore several prompt modification strategies, aiming to either amplify or mitigate geopolitical bias, which highlights how brittle LLMs are and how they tailor their responses depending on cues from the interaction context. Our code and data are available at https://github.com/manestay/borderlines."
}

@inproceedings{vashishtha-etal-2023-evaluating,
    title = "On Evaluating and Mitigating Gender Biases in Multilingual Settings",
    author = "Vashishtha, Aniket  and
      Ahuja, Kabir  and
      Sitaram, Sunayana",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.21/",
    doi = "10.18653/v1/2023.findings-acl.21",
    pages = "307--318",
    abstract = "While understanding and removing gender biases in language models has been a long-standing problem in Natural Language Processing, prior research work has primarily been limited to English. In this work, we investigate some of the challenges with evaluating and mitigating biases in multilingual settings which stem from a lack of existing benchmarks and resources for bias evaluation beyond English especially for non-western context. In this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models by extending DisCo to different Indian languages using human annotations. We extend various debiasing methods to work beyond English and evaluate their effectiveness for SOTA massively multilingual models on our proposed metric. Overall, our work highlights the challenges that arise while studying social biases in multilingual settings and provides resources as well as mitigation techniques to take a step toward scaling to more languages."
}

@inproceedings{naous-etal-2024-beer,
    title = "Having Beer after Prayer? Measuring Cultural Bias in Large Language Models",
    author = "Naous, Tarek  and
      Ryan, Michael J  and
      Ritter, Alan  and
      Xu, Wei",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.862/",
    doi = "10.18653/v1/2024.acl-long.862",
    pages = "16366--16393",
    abstract = "As the reach of large language models (LMs) expands globally, their ability to cater to diverse cultural contexts becomes crucial. Despite advancements in multilingual capabilities, models are not designed with appropriate cultural nuances. In this paper, we show that multilingual and Arabic monolingual LMs exhibit bias towards entities associated with Western culture. We introduce CAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities spanning eight types that contrast Arab and Western cultures. CAMeL provides a foundation for measuring cultural biases in LMs through both extrinsic and intrinsic evaluations. Using CAMeL, we examine the cross-cultural performance in Arabic of 16 different LMs on tasks such as story generation, NER, and sentiment analysis, where we find concerning cases of stereotyping and cultural unfairness. We further test their text-infilling performance, revealing the incapability of appropriate adaptation to Arab cultural contexts. Finally, we analyze 6 Arabic pre-training corpora and find that commonly used sources such as Wikipedia may not be best suited to build culturally aware LMs, if used as they are without adjustment. We will make CAMeL publicly available at: https://github.com/tareknaous/camel"
}

@inproceedings{narayanan-venkit-etal-2023-nationality,
    title = "Nationality Bias in Text Generation",
    author = "Narayanan Venkit, Pranav  and
      Gautam, Sanjana  and
      Panchanadikar, Ruchi  and
      Huang, Ting-Hao  and
      Wilson, Shomir",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.9/",
    doi = "10.18653/v1/2023.eacl-main.9",
    pages = "116--122",
    abstract = "Little attention is placed on analyzing nationality bias in language models, especially when nationality is highly used as a factor in increasing the performance of social NLP models. This paper examines how a text generation model, GPT-2, accentuates pre-existing societal biases about country-based demonyms. We generate stories using GPT-2 for various nationalities and use sensitivity analysis to explore how the number of internet users and the country`s economic status impacts the sentiment of the stories. To reduce the propagation of biases through large language models (LLM), we explore the debiasing method of adversarial triggering. Our results show that GPT-2 demonstrates significant bias against countries with lower internet users, and adversarial triggering effectively reduces the same."
}

@inproceedings{zhu-etal-2024-quite,
    title = "Quite Good, but Not Enough: Nationality Bias in Large Language Models - a Case Study of {C}hat{GPT}",
    author = "Zhu, Shucheng  and
      Wang, Weikang  and
      Liu, Ying",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1180/",
    pages = "13489--13502",
    abstract = "While nationality is a pivotal demographic element that enhances the performance of language models, it has received far less scrutiny regarding inherent biases. This study investigates nationality bias in ChatGPT (GPT-3.5), a large language model (LLM) designed for text generation. The research covers 195 countries, 4 temperature settings, and 3 distinct prompt types, generating 4,680 discourses about nationality descriptions in Chinese and English. Automated metrics were used to analyze the nationality bias, and expert annotators alongside ChatGPT itself evaluated the perceived bias. The results show that ChatGPT`s generated discourses are predominantly positive, especially compared to its predecessor, GPT-2. However, when prompted with negative inclinations, it occasionally produces negative content. Despite ChatGPT considering its generated text as neutral, it shows consistent self-awareness about nationality bias when subjected to the same pair-wise comparison annotation framework used by human annotators. In conclusion, while ChatGPT`s generated texts seem friendly and positive, they reflect the inherent nationality biases in the real world. This bias may vary across different language versions of ChatGPT, indicating diverse cultural perspectives. The study highlights the subtle and pervasive nature of biases within LLMs, emphasizing the need for further scrutiny."
}

@inproceedings{kamruzzaman-etal-2024-investigating,
    title = "Investigating Subtler Biases in {LLM}s: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models",
    author = "Kamruzzaman, Mahammed  and
      Shovon, Md.  and
      Kim, Gene",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.530/",
    doi = "10.18653/v1/2024.findings-acl.530",
    pages = "8940--8965",
    abstract = "LLMs are increasingly powerful and widely used to assist users in a variety of tasks. This use risks introducing LLM biases into consequential decisions such as job hiring, human performance evaluation, and criminal sentencing. Bias in NLP systems along the lines of gender and ethnicity has been widely studied, especially for specific stereotypes (e.g., Asians are good at math). In this paper, we investigate bias along less-studied but still consequential, dimensions, such as age and beauty, measuring subtler correlated decisions that LLMs make between social groups and unrelated positive and negative attributes. Although these subtler biases are understudied they follow people as much as gender and ethnicity do. So, we want to see whether they also follow one with LLMs.We introduce a template-generated dataset of sentence completion tasks that asks the model to select the most appropriate attribute to complete an evaluative statement about a person described as a member of a specific social group. We also reverse the completion task to select the social group based on an attribute. We report the correlations that we find for 4 cutting-edge LLMs. This dataset can be used as a benchmark to evaluate progress in more generalized biases and the templating technique can be used to expand the benchmark with minimal additional human annotation."
}

@inproceedings{nie-etal-2024-multilingual,
    title = "Do Multilingual Large Language Models Mitigate Stereotype Bias?",
    author = {Nie, Shangrui  and
      Fromm, Michael  and
      Welch, Charles  and
      G{\"o}rge, Rebekka  and
      Karimi, Akbar  and
      Plepi, Joan  and
      Mowmita, Nazia  and
      Flores-Herr, Nicolas  and
      Ali, Mehdi  and
      Flek, Lucie},
    editor = "Prabhakaran, Vinodkumar  and
      Dev, Sunipa  and
      Benotti, Luciana  and
      Hershcovich, Daniel  and
      Cabello, Laura  and
      Cao, Yong  and
      Adebara, Ife  and
      Zhou, Li",
    booktitle = "Proceedings of the 2nd Workshop on Cross-Cultural Considerations in NLP",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.c3nlp-1.6/",
    doi = "10.18653/v1/2024.c3nlp-1.6",
    pages = "65--83",
    abstract = "While preliminary findings indicate that multilingual LLMs exhibit reduced bias compared to monolingual ones, a comprehensive understanding of the effect of multilingual training on bias mitigation, is lacking. This study addresses this gap by systematically training six LLMs of identical size (2.6B parameters) and architecture: five monolingual models (English, German, French, Italian, and Spanish) and one multilingual model trained on an equal distribution of data across these languages, all using publicly available data. To ensure robust evaluation, standard bias benchmarks were automatically translated into the five target languages and verified for both translation quality and bias preservation by human annotators. Our results consistently demonstrate that multilingual training effectively mitigates bias. Moreover, we observe that multilingual models achieve not only lower bias but also superior prediction accuracy when compared to monolingual models with the same amount of training data, model architecture, and size."
}

@inproceedings{parrish-etal-2022-bbq,
    title = "{BBQ}: A hand-built bias benchmark for question answering",
    author = "Parrish, Alicia  and
      Chen, Angelica  and
      Nangia, Nikita  and
      Padmakumar, Vishakh  and
      Phang, Jason  and
      Thompson, Jana  and
      Htut, Phu Mon  and
      Bowman, Samuel",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.165/",
    doi = "10.18653/v1/2022.findings-acl.165",
    pages = "2086--2105",
    abstract = "It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model`s biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model`s outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested."
}