\section{Related Works}
\subsection{StarCraft-II Decision Environment}
Since the inception of StarCraft II, it has been receiving a great deal of
attention. With its characteristics of huge state space and action space, multi-agent, incomplete information, long time series, real-time, and good software support, it is recognized as one of the best environments for studying decision-making algorithms. 

In 2017, the SC2LE learning environment was proposed, followed by the proposal of the SMAC environment\cite{SMAC} two years later. However, these environments do not provide textual observations and cannot recognize textual instructions, limiting the ability to interact with LLMs.

\begin{figure}
\centerline{\includegraphics[width=320pt]{fig-HEP-LLM-env.pdf}}
\caption{\textbf{StarCraft II.} In this decision-making environment, players need to control units, collect resources, build and upgrade technology, and confront opponents with incomplete observation information, making it one of the most complex decision-making environments.} 
\end{figure}

To provide an environment for LLM-based decision-making methods, TextStarCraft II was proposed. This environment separates macro decision-making and micro-operations, liberates LLM from high-speed micro-operations by handing over these operations automatically, and enables LLM to focus on macro decision-making. In interactive interfaces, the environment organizes the observed information into textual summaries and can recognize textual instructions, providing favorable conditions for LLM-based methods.

\subsection{Decision Making in Multi-Agent Games}

Multi-agent games include two directions of research. One is the study of a simple game with many agents, such as 27Marine-vs-30Marine task, controlling a group of agents to achieve maximum combat capability in the SC2LE . The second is the study of complex decision tasks with fewer agents, such as playing Go\cite{AlphaGo}, Texas Poker\cite{Alphaholdem}, Mujoco Football Game\cite{MujocoFootball}, and complete battle in StarCraft II. 

In the study of the first direction, many multi-agent reinforcement learning algorithms have emerged, such as value decomposition (VD) methods such as VDN\cite{VDN}, Qmix\cite{QMIX}, Weighted QMIX\cite{WQMIX} and so on, and centralized training decentralized execution (CTDE) methods such as MAPPO\cite{MAPPO} and MADDPG\cite{MADDPG}. These algorithms focus on solving the credit assignment problem of agents in a team, avoiding lazy agent problem, and aiming at maximizing the collaborative efficiency of the group of agents.

In the study of the second direction, algorithms such as AlphaGo\cite{AlphaGo}, AlphaStar, and AlphaHoldem\cite{Alphaholdem} showcase the ability to surpass top human players in complex multi-agent games. As a milestone in StarCraft II decision-making, AlphaStar uses two-stage training to acquire decision-making abilities beyond humans, first training on expert trajectories for about two weeks, then applying to more than one month of reinforcement learning to obtain higher abilities. 

However, all these algorithms mentioned above face the same problem: the demand for computing resources and training time. Some also require a large amount of high-quality trajectory data for imitation. These problems hinder further development of these methods and may also lead to the Sim-to-Real problem when migrating to other tasks.

\subsection{Large Language Model}
LLM usually refers to a language processing network with billions of parameters. In November 2022, OpenAI released the LLM-based chat application ChatGPT. In April 2023, OpenAI released GPT-4, which further improved the performance of LLM with more parameters and training data. After that, more and more LLM emerge, such as Claude-2\cite{Claude-2} and 3\cite{Claude-3}, LLAMA-2\cite{LLAMA-2} and 3\cite{LLAMA-3}.

LLMs have demonstrated remarkable skills in handling textual data, yet LLM-based decision-making systems have not been well advanced. In March 2024, an LLM enhanced with CoS achieved a milestone by securing a 50\% winning rate against Harder opponent in TextStarCraft II for the first time. However, this approach cannot defeat the VeryHard and Elite opponents. On their basis, we constructed Hierarchical Expert Prompt, introducing expert knowledge and enhancing the ability to deal with tasks with different priorities for LLM. For the first time, it defeated the TextStarCraft II agent under Elite difficulty, suggesting that the LLM-based method is a feasible solution to apply to decision-making tasks with greater complexity.
%
%
%
%
%