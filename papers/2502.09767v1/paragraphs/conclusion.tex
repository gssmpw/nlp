\section{Conclusion}
We introduced \emph{CaDDi}, a causal discrete diffusion framework that relaxes the traditional Markovian assumptions in favor of an autoregressive inference process. By explicitly conditioning each denoising step on the entire future trajectory, CaDDi captures richer temporal dependencies and leverages iterative refinement. Critically, our approach can also be built atop existing causal language models---bridging standard sequence modeling with powerful diffusion capabilities---while preserving both knowledge from large-scale training and the flexibility of iterative editing.

% Quantitative results on protein and text-generation tasks underscore that CaDDi attains superior or on-par generative quality at substantially fewer steps compared to prior work. Furthermore, we showed how semi-speculative decoding can accelerate inference without compromising quality. Overall, CaDDi offers a unified framework for discrete diffusion that naturally extends autoregressive modeling to more controllable and structured generation modes, opening several avenues for future research on accelerated sampling, advanced conditioning strategies, and broader applications in biology and language processing.
