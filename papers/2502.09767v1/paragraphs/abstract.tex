% Discrete diffusion models have gained attention as flexible and controllable frameworks for structured sequence modeling. However, their expressiveness remains limited compared to that of causal language models. In this work, we present \method{}, a causal discrete diffusion model that integrates sequential and temporal modeling within a unified non-Markovian diffusion framework. Unlike traditional diffusion models, which operate stepwise and lack access to prior states, \method{} explicitly incorporates the historical trajectory of previous timesteps, enhancing both expressiveness and controllability. Moreover, \method{} subsumes causal language models as a special case, facilitating natural adoption of pretrained large language models (LLMs) into discrete diffusion frameworks without architectural modifications. Empirical evaluations on natural language and biological sequence modeling tasks demonstrate that \method{} achieves state-of-the-art performance among discrete diffusion models while offering a more flexible and structured generative paradigm.




Discrete diffusion models have emerged as a flexible and controllable paradigm for structured sequence modeling, yet they still lag behind causal language models in expressiveness. 
% Motivated by the observation that the inverse of a Markovian process is often non-Markovian—as seen, for example, in Langevin dynamics—
To bridge the gap between two paradigms,
we introduce \textbf{\method{}}, a causal discrete diffusion model that unifies sequential and temporal modeling within a non-Markovian diffusion framework. Unlike conventional diffusion models that operate step by step with no access to prior states, \method{} integrates the temporal trajectory, enabling more expressive and controllable generation. Our approach also treats causal language models as a special case, allowing seamless adoption of pretrained large language models (LLMs) for discrete diffusion without the need for architectural modifications. Empirically, we demonstrate that \method{} outperforms state-of-the-art discrete diffusion models on both natural language and biological sequence tasks, narrowing the gap between diffusion-based methods and large-scale autoregressive transformers.

% Key point: DD with history (non-Markovian), implemented with a causal language model. 




