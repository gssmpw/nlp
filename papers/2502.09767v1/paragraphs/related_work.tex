\section{Related Work}

\paragraph{Discrete Diffusion.}
Diffusion models \citep{ho2020denoisingdiffusionprobabilisticmodels} generate data by learning a reverse (denoising) process to invert a fixed forward (noising) Markov chain. \citet{d3pm} first extended such models to discrete data (D3PM) by defining uniform and absorbing diffusion kernels on finite state spaces. Subsequent work introduced improved parameterizations, such as data distribution ratio estimation \citep{lou2024discretediffusionmodelingestimating}, drawing parallels with score matching \citep{song2021scorebasedgenerativemodelingstochastic}. 
Despite their efficacy, these methods typically rely on a Markov chain, focusing on denoising from a single noisy state \(\mathbf{x}_t\). By contrast, our approach \textbf{breaks} the Markovian assumption and conditions on the entire future trajectory \(\mathbf{x}_{t:T}\), providing more robust denoising and broader generative capabilities.

\paragraph{Discrete Flow Matching.}
Flow matching \citep{lipman2023flowmatchinggenerativemodeling, tong2024improvinggeneralizingflowbasedgenerative} learns a continuous transformation from noise to data via an ODE governed by a vector field. Recent extensions handle discrete data \citep{gat2024discreteflowmatching, davis2024fisherflowmatchinggenerative, stark2024dirichletflowmatchingapplications}. While these methods circumvent explicit Markovian noising, they often require continuous flow formulations and specialized training objectives. In contrast, our \textbf{non-Markovian discrete diffusion} remains within the discrete diffusion paradigm, retains a straightforward variational objective, and integrates naturally with causal modeling.

\paragraph{Autoregressive Models.}
Autoregressive transformers \citep{vaswani2017attention, chowdhery2023palm, touvron2023llama} remain a cornerstone in language processing, generating tokens sequentially given prior context. Such models excel at unidirectional left-to-right tasks but can be inflexible for intermediate edits or bidirectional generation. Our framework unifies causal (autoregressive) decoding with diffusion-based iterative denoising, thus benefiting from both paradigmsâ€”left-to-right token generation and multi-step refinements.

\paragraph{Integrating Autoregression with Diffusion and Flow Matching.}
Several works~\citep{DART, he2024calmflowvolterraflowmatching} try to combine diffusion or flow matching with causal transformers for improved generation. Specifically, DART \citep{DART} employs a non-Markovian trajectory to let a transformer model entire sequences of diffusion states. Our approach {further refines} this idea in two ways: (\textit{i}) we focus on discrete non-Markovian diffusion with explicit multi-step conditioning, and (\textit{ii}) we provide a direct path for adapting \textit{pretrained} LLMs, thus combining the strengths of large-scale language model pretraining with the controllability of discrete diffusion.

\paragraph{Non-Markovian Reverse Process in Physical System.}
Using a Non-Markov reverse process to recover the distribution introduced by Makrovian forward process is not a new idea. In physics, many systems exhibit this property. \textit{Langevin Dynamics:} Although the forward motion of a Brownian particle (with velocity and position) can be Markovian in the full state space, attempts to reverse the position-only dynamics often require the history of the system to account for friction or random kicks \citep{gardinerstochastic, van1992stochastic}. \textit{Quantum Processes:} Tracing out environmental degrees of freedom can yield a Markovian forward evolution, but reconstructing the entire global state upon reversal introduces non-Markovian memory effects.


% \paragraph{Integrating Autoregression with Diffusion and Flow Matching.}

% Several works combine diffusion or flow matching with causal transformers for improved generation. For instance, CaLMFlow \citep{he2024calmflowvolterraflowmatching} uses Volterra Integral Equations in a causal LM, capturing multiple past time points in an ODE-like framework. Similarly, DART \citep{DART} employs a non-Markovian trajectory to let a transformer model entire sequences of diffusion states. Our approach \textbf{further refines} this idea in two ways: (\emph{i}) we focus on discrete non-Markovian diffusion with explicit multi-step conditioning, and (\emph{ii}) we provide a direct path for adapting \emph{pretrained} LLMs, thus combining the strengths of large-scale language model pretraining with the controllability of discrete diffusion.
