\section{Mutual Information Analysis}
\label{sec:mutual_information}

To formally understand the limitations of traditional Markovian discrete diffusion models and the advantages of our proposed non-Markovian framework, we analyze the mutual information between the original data \(\mathbf{x}_0\) and the latent trajectory \(\mathbf{x}_{1:T}\).

\subsection{Mutual Information in Markovian Processes}

In Markovian discrete diffusion, each latent state \(\mathbf{x}_t\) depends solely on the immediate previous state \(\mathbf{x}_{t-1}\), adhering to the memoryless property. The mutual information between the original data and the latent trajectory is given by:
\[
I_{\text{Markov}}(\mathbf{x}_0; \mathbf{x}_{1:T}) = \sum_{t=1}^T I(\mathbf{x}_0; \mathbf{x}_t \mid \mathbf{x}_{t-1})
\]
This linear summation constrains the total mutual information, potentially limiting the model's capacity to capture long-range dependencies.

\subsection{Enhancing Mutual Information with Non-Markovian Processes}

Conversely, our non-Markovian discrete diffusion framework allows each latent state \(\mathbf{x}_t\) to condition on the entire history \(\mathbf{x}_{0:t-1}\). This richer dependency structure enhances the mutual information:
\[
I_{\text{Non-Markov}}(\mathbf{x}_0; \mathbf{x}_{1:T}) \geq I_{\text{Markov}}(\mathbf{x}_0; \mathbf{x}_{1:T})
\]
By maximizing \(I(\mathbf{x}_0; \mathbf{x}_{1:T})\), our model retains more information from the original data, thereby improving the capture of long-range dependencies and enhancing the robustness of the inference procedure.

\subsection{Implications for Model Performance}

The increased mutual information in non-Markovian processes translates to:
\begin{itemize}
    \item \textbf{Enhanced Information Preservation:} More comprehensive retention of original data information across latent states.
    \item \textbf{Improved Long-Range Dependency Capture:} Ability to model dependencies that span multiple timesteps effectively.
    \item \textbf{Increased Robustness:} More stable and reliable inference due to richer information flow.
\end{itemize}

\subsection{Training Objective Incorporating Mutual Information}

To explicitly encourage higher mutual information, we augment the training objective with a mutual information maximization term:
\[
\mathcal{L}' = \mathcal{L}_{\mathrm{NOMAD}} - \lambda I(\mathbf{x}_0; \mathbf{x}_{1:T})
\]
where \(\lambda\) is a hyperparameter balancing reconstruction fidelity and information preservation. Utilizing variational bounds, we approximate \(I(\mathbf{x}_0; \mathbf{x}_{1:T})\) to make this objective tractable in practice.

\section{Conclusion}

By leveraging mutual information as a theoretical foundation, we demonstrate that non-Markovian discrete diffusion models offer significant advantages over traditional Markovian approaches. Our framework not only preserves more information from the original data but also effectively captures complex, long-range dependencies, leading to more robust and flexible inference processes.
