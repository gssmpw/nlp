\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{paragraphs/figures/model_9.pdf}
    \caption{(a). \textbf{Inference paradigm for a standard causal language model versus \method{}.} In \method{}, each timestep first autoregressively denoises the tokens into $\widetilde{\mathbf{x}}_0$, then re-applies noise via the diffusion kernel to obtain A traditional autoregressive model emerges as the special case of $T=1$, which can be adapted to discrete diffusion by fine-tuning. (b). \textbf{Extending 1D to 2D Rotary Positional Encoding.} Standard rotary encodings for token positions are seamlessly generalized to also encode diffusion timesteps, remaining fully backward-compatible with existing language model architectures.}
    \label{fig:model}
\end{figure*}

\section{Non-Markovian Discrete Diffusion}
\label{sec:framework}

Previous methods have modeled the discrete diffusion process as a Markovian process, where the model learns an instantaneous reverse process to denoise \(\mathbf{x}_t\) and reconstruct \(\mathbf{x}_{t-1}\) by \(p_\theta (\mathbf{x}_{t-1} \mid \mathbf{x}_{t})\)~\citep{d3pm}. Despite efficient generation, this Markovian constraint can limit the model’s ability to capture long-range dependencies within the latent chain. All relevant information is compressed into single state \(\mathbf{x}_t\), potentially leading to a non-robust inference procedure.

In this paper, we extend the non-Markovian diffusion process to discrete data modeling following~\citep{DART}. Specifically, recent studies have demonstrated that the Markovian assumption is not strictly necessary in the inference process. Breaking this assumption allows for the incorporation of the entire temporal trajectory \(\mathbf{x}_{t:T}\) to denoise \(\mathbf{x}_{t-1}\) by \(p_\theta (\mathbf{x}_{t-1} \mid \mathbf{x}_{t:T})\) in autoregressive manner, leading to a more expressive and robust inference process. 

In the following, we will describe how the non-Markovian discrete diffusion process is constructed. Crucially, we will see that the resulting non-Markovian autoregressive inference mechanism essentially aligns with a causal language model plus an additional temporal dimension—laying the groundwork for our unified spatial-temporal framework (Section~\ref{sec:method}).



% \rex{i think it would be clearer if we first lay out the spatial temporal setting and the big picture}



\subsection{Hybrid Non-Markovian Forward Trajectory}
A key challenge in realizing non-Markovian inference is how to design the forward trajectory so that future states \(\mathbf{x}_{t+1: T}\) carry more complementary information about\(\mathbf{x}_t\). The straightforward Markovian absorbing process \(q(\mathbf{x}_{0:T}) = \prod_{t=1}^{T} q(\mathbf{x}_t | \mathbf{x}_{t-1})\) is ill-suited for this purpose, as information is highly limited and redundant. To ensure each timestep retains complementary information about the original data, we (1). \textit{construct the forward trajectory by independently corrupting \(\mathbf{x}_0\).}
(2). \textit{Mix an abosrbing kernel with uniform kernal to produce more diverse noisy states.}

\paragraph{Independent Corruption.}
As shown in Fig.~\ref{fig:non-markov}, the diffusion trajectory \(\mathbf{x}_{0:T}\) is created  where we add \textit{independent} noise to \(\mathbf{x}_0\) at different timesteps, rather than relying on the previous state as the noise source. The forward trajectory is constructed as:
    \begin{align}
    q(\mathbf{x}_{0:T}) :=&  q(\mathbf{x}_0, \mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_T) \\
    =& q(\mathbf{x}_0) \prod_{t=1}^{T} q(\mathbf{x}_t | \mathbf{x}_{0:t-1}) \\
    =& q(\mathbf{x}_0) \prod_{t=1}^{T} q(\mathbf{x}_t | \mathbf{x}_{0}),
    \end{align} 
where \(q(\mathbf{x}_t | \mathbf{x}_{0})\) is the marginal conditional distribution, which is obtained from a standard Markovian diffusion kernel but applied directly to \(\mathbf{x}_0\). For example, in absorbing or uniform diffusion processes, we can write:
    \begin{align}
        q\left(\mathbf{x}_t \mid \mathbf{x}_0\right)&=\operatorname{Cat}\left(\mathbf{x}_t ; \mathbf{x}_0 \overline{\mathbf{Q}}_t\right) \\
        &= \operatorname{Cat}\left(\mathbf{x}_t ; \mathbf{x}_0 \prod_{k=1}^{t} \mathbf{Q}_k\right)
    \end{align}
    where \(\mathbf{Q}_k\) is the transition matrix at step \(k\), and \(\overline{\mathbf{Q}}_t\) is the product of all such transitions up to \(t\). This construction generalizes the Markovian forward trajectory (see Appendix for further details) and creates a sequence of noisy states that better preserve intermediate information across timesteps.

    \begin{proposition}[Discrete Non-Markovian Information Gain]
        \label{thm:non_markov_gain}
        Let \(\{\mathbf{x}_0,\mathbf{x}_1,\ldots,\mathbf{x}_T\}\) be a discrete forward diffusion process that is \emph{not} strictly Markovian.  Suppose there exists at least one timestep \(t\) such that\footnote{it is trivial to find such \(t\) in our case with indendent noise corruption.}
        \[
        \mathbf{x}_{t-1}
        \;\not\!\!\perp\!\!\!\!\perp\;
        \mathbf{x}_{t+1:T}
        \;\big|\;
        \mathbf{x}_t.
        \]
        Then the conditional mutual information 
        \[
        I\bigl(\mathbf{x}_{t-1}; \mathbf{x}_{t+1:T}\,\big|\;\mathbf{x}_t\bigr)
        \;>\; 0,
        \]
        which implies that conditioning on \(\mathbf{x}_{t+1:T}\) in the reverse process \(\,p_\theta(\mathbf{x}_{t-1}\mid \mathbf{x}_{t:T})\) strictly reduces the uncertainty about \(\mathbf{x}_{t-1}\) compared to conditioning on \(\mathbf{x}_t\) alone.
    \end{proposition}
% \rex{discuss its significance}
In other words, when the non-Markovian forward trajectory is designed with independent corruption, future noisy states can complement each other, bolstering the reverse inference process.

\paragraph{Hybrid Diffusion Kernel.} Most prior discrete diffusion methods rely on a single kernel, such as purely absorbing (where tokens are replaced by \texttt{[MASK]}) or purely uniform corruption. However, sticking to one kernel can lead to monotonic or insufficiently diverse noise patterns in the generative trajectory. We therefore mix an absorbing kernel and a uniform kernel:

\begin{equation}
    \overline{\mathbf{Q}}_t = (1 -  \alpha_{t} -\beta_{t})\mathbf{I} 
    + \alpha_{t}\overline{\mathbf{Q}}_T^{\text{absorb}} 
    + \beta_{t}\overline{\mathbf{Q}}_T^{\text{uniform}},
\end{equation}
where $\alpha_t, \beta_t$ are time-dependent schedules with boundary conditions $\alpha_0=0, \alpha_T=1, \beta_0=$ $0, \beta_T=0$. The terms $\overline{\mathbf{Q}}_T^{\text {absorb }}$ and $\overline{\mathbf{Q}}_T^{\text {uniform }}$ are the marginal diffusion kernels (absorbing and uniform, respectively) at the final step. By mixing these kernels, we introduce a spectrum of corruption modes-some tokens may be masked out, while others might be replaced by random symbols, thus yielding a more informative trajectory. 

\begin{algorithm}[t]
    \caption{Inference for Non-Markovian Discrete Diffusion}
    \label{alg:non_markov_inference}
    \begin{algorithmic}[1]
    \State \textbf{Input:} Prior distribution $q(\mathbf{x}_T)$, model parameters $\theta$
    \State \textbf{Output:} Sampled data $\mathbf{x}_0$
    \State Initialize $\mathbf{x}_T \sim q(\mathbf{x}_T)$ as the noisy input data at the final timestep
    \For{$t = T \textbf{ down to } 1$}
    
        % \Comment{Using $x_0$ parameterization}
        \State Predict the clean data $\mathbf{x}_{0}$ from the historical trajectory $\mathbf{x}_{t:T}$ using the model:
        $p_\theta\left( \mathbf{x}_{0} \mid \mathbf{x}_{t:T} \right)$
        \State Sample from the predicted distribution to obtain the clean data at timestep $t-1$: 
        $\widetilde{\mathbf{x}}_{0} \sim p_\theta\left( \mathbf{x}_{0} \mid \mathbf{x}_{t:T} \right)$
        \State Add noise to the predicted clean data to get the next timestep $t-1$:
        $\mathbf{x}_{t-1} \sim q\left( \mathbf{x}_{t-1} \mid \widetilde{\mathbf{x}}_{0} \right)$
        \EndFor
    \State \Return $\mathbf{x}_0$
    \end{algorithmic}
    \end{algorithm}

% \paragraph{Combining Uniform and Absorbing forward diffusion process}

% \TODO{Mixing uniform and absorbing diffusion forward process}
    
\subsection{Non-Markovian Inference Process}
We train the non-Markovian reverse model 
\(
p_\theta\bigl(\mathbf{x}_{t-1} \mid \mathbf{x}_{t:T}\bigr)
\)
by minimizing a weighted ELBO objective derived from the variational perspective of diffusion:

\begin{equation}
    \mathcal{L}_{\text{non-markov}} = 
    \mathbb{E}_{\mathbf{x}_{1: T} \sim q\bigl( \mathbf{x}_{1:T} \mid \mathbf{x}_0\bigr)}
    \Biggl[
        \sum_{t=1}^T \tilde{\omega}_t \,\log p_\theta\bigl(\mathbf{x}_{t-1} \mid \mathbf{x}_{t:T}\bigr)
    \Biggr].
\end{equation}
Here, the key difference from standard discrete diffusion is that each term conditions on \(\mathbf{x}_{t+1: T}\). By Theorem~\ref{thm:non_markov_gain}, this inclusion \textit{strictly reduces} the conditional entropy when the forward process is indeed non-Markovian. Consequently, \(p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t: T}\right)\) has a more robust denoising pathway.

Once trained, sampling proceeds in an autoregressive manner, iterating backward over time by conditioning on the entire future trajectory \(\mathbf{x}_{t:T}\).

% TODO: maybe move to next section
\paragraph{\(\textbf{x}_0\)-Parameterization.}
Similar to standard discrete diffusion approaches~\citep{udlm, gat2024discreteflowmatching}, we adopt an \(x_0\)-parameterization to simplify training. In this view, we directly predict the clean sequence \(\mathbf{x}_0\) at each step, which leads to a simpler denoising objective:
\begin{equation}
    \mathcal{L}_{\text{non-markov}}
    \;=\;
    \mathbb{E}_{\mathbf{x}_{1: T} \sim q\bigl( \mathbf{x}_{1:T} \mid \mathbf{x}_0\bigr)}\!\Biggl[
        \sum_{t=1}^T  
        \log p_\theta\bigl(\mathbf{x}_0 \mid \mathbf{x}_{t:T}\bigr)
    \Biggr].
\end{equation}
(See Appendix for the derivation.) At inference time, we first sample \(\widetilde{\mathbf{x}}_0\) from the learned denoiser \(p_\theta(\mathbf{x}_0 \mid \mathbf{x}_{t:T})\) and then use the forward kernel \(q(\cdot \mid \widetilde{\mathbf{x}}_0)\) to obtain \(\mathbf{x}_{t-1}\). This procedure iterates backward through time until we reach \(\mathbf{x}_0\). Pseudocode is presented in Algorithm~\ref{alg:non_markov_inference}.


\section{\method{}: \underline{Ca}usal \underline{D}iscrete \underline{Di}ffusion Model}
\label{sec:method}

The autoregressive property of non-Markovian inference (Section~\ref{sec:framework}) naturally lends itself to a causal model that predicts tokens in a left-to-right fashion. Motivated by this insight, we propose \textbf{\method{}}, a causal language model that unifies the \textit{sequential} dimension (i.e., token order) with the \textit{temporal} dimension (i.e., discrete diffusion timesteps). Concretely, \method{} leverages a standard left-to-right structure while conditioning on multiple timesteps of the diffusion chain, enabling it to handle non-Markovian discrete diffusion in a single unified framework.

\subsection{Unified Sequential and Temporal Modeling}

In the non-Markovian setting (Section~\ref{sec:framework}), the reverse process is inherently autoregressive: we decode the entire sequence of latent states \(\mathbf{x}_{t:T}\) to retrieve \(\mathbf{x}_{t-1}\). This naturally aligns with the standard left-to-right generation of language models. However, unlike a conventional \textit{single}-dimensional token sequence, we must now account for a {temporal} dimension.

\paragraph{Constructing the Training Trajectory.} 
To accommodate both sequential and temporal dependencies in a single model, \method{} constructs a \textit{non-Markovian} forward trajectory for each data instance:
\begin{equation*}
    \bigl(\mathbf{x}_T^{(0)}, \ldots, \mathbf{x}_T^{(L)},\; \mathbf{x}_{T-1}^{(0)}, \ldots, \mathbf{x}_{0}^{(L)}\bigr),
\end{equation*}
where the upper index \((i)\) denotes the token position in the original sequence of length \(L\), and the subscript denotes the diffusion timestep. We then train a causal language model to predict {the next token} at each position via a standard next-token prediction loss. Crucially, the target for each position \(i\) is always the original clean token \(\mathbf{x}_0^{(i)}\) under \(\textbf{x}_0\)-parameterization.

\paragraph{Context Window.}
In practice, feeding all timesteps into the model can be prohibitively large. Therefore, we restrict the model’s temporal context to the most recent \(n\) timesteps. More formally, for each timestep \(t\) and token position \(i\), we define the context \(\mathcal{C}_{t,i}\) as follows:
\begin{align}
    \mathcal{C}_{t,i} &= 
    \left\{ \mathbf{x}_\tau^{(j)} \;\middle|\; t-1 \leq \tau \leq \min(t+n, T),\; 0 \leq j \leq L \right\} \notag \\
    &\cup 
    \left\{ \widetilde{\mathbf{x}}_0^{(j)} \;\middle|\; 0 \leq j < i \right\}
\end{align}
where \(\widetilde{\mathbf{x}}_0^{(j)}\) is the predicted clean token\footnote{In teacher-forced training, this is the ground truth clean token} at position \(j\) at current timestep \(t\). The token-level training objective is then:
\begin{equation}
    \mathcal{L}^{t, i}_{\text{token}} = \mathbb{E}\left[ -\log p_\theta\left( \mathbf{x}_0^{(i)} \;\middle|\; \mathcal{C}_{t,i} \right) \right].
\end{equation}
This design ensures that \method{} captures both local temporal structure and token-level dependencies in a scalable manner.
In practice, there is a inherent trade-off between scalability and the ability to model long-range temporal dependencies. We found \(n=4\) a reasonable choice through empirical evaluation.
% TODO: add some ending sentence, possibly with parallel training
% \rex{how do we pick $n$ in practice? is model performance sensitive to this?}


\subsection{2D Rotary Positional Encoding for Sequence Position \& Diffusion Timestep}
\label{subsec:2d_rotary}

A modern causal language model typically encodes \textit{only} the sequential dimension, via rotary positional encodings~\citep{roformer}. However, for non-Markovian discrete diffusion, we must capture not just the standard token-level sequence but also a temporal dimension corresponding to diffusion timesteps. To address this, we extend the original 1D rotary scheme to a {2D} variant 
% \rex{how does it compare with the fourier feature https://arxiv.org/abs/2106.02795 ?}
, allowing the model to incorporate positional information across both the sequence index \(i\) and the diffusion timestep \(t\). This enhanced encoding enables the model to more effectively learn joint dependencies between tokens and their progression through multiple diffusion steps.



Specifically, standard RoPE in modern language models like Pythia~\citep{biderman2023pythia} rotates a subset
% \footnote{For models that rotate the full set of dimensions, we could also extend 1D RoPE to 2D RoPE by composing two rotation matrixs.}
of the query/key dimensions according to the token position $i$. If $\mathbf{R}^{(i)}$ denotes the rotation matrix parameterized by $i$, the attention weight between positions $i$ and $j$ becomes $\left(\mathbf{R}^{(i)} \mathbf{q}^{(i)}\right)^{\top}\left(\mathbf{R}^{(j)} \mathbf{k}^{(j)}\right)$, where

% Specifically, in modern language models like Pythia~\citep{biderman2023pythia}, position is integrated by rotating a subset of the query/key dimensions in calculating attention weight\footnote{In some architectures, all dimensions of query/key are roateted. In that case, we can also extend 1D RoPE to 2D RoPE by composing rotation matrix introduced by two dimensions respectively.}:
% \begin{equation*}
% \alpha_{i,j}=\left(\mathbf{R}^{(i)} \mathbf{q}^{(i)}\right)^{\top}\left(\mathbf{R}^{(j)} \mathbf{k}^{(j)}\right)={\mathbf{q}^{(i)}}^{\top} \mathbf{R}^{(j-i)} \mathbf{k}^{(j)},
% \end{equation*}
% where
\begin{equation*}
\label{eq:1d_rotary}
\resizebox{.70\columnwidth}{!}{$
\mathbf{R}^{(i)}=\left(\begin{array}{ccccc}
\cos \left(i \theta_{12}\right) & -\sin \left(i \theta_{12}\right) & 0 & \cdots \\
\sin \left(i \theta_{12}\right) & \cos \left(i \theta_{12}\right) & 0 & \cdots \\
0 & 0 & 0 & \cdots\\
\vdots & \vdots &  \vdots & \ddots
\end{array}\right).
$}
\end{equation*}

As shown in Figure~\ref{fig:model}, we generalize this approach by introducing additional rotation for the timestep dimension:
\begin{equation*}
\label{eq:2d_rotary}
\resizebox{.95\columnwidth}{!}{$
\mathbf{R}_t^{(i)} = \begin{pmatrix}
    \cos \left(i \theta_{12}\right) & -\sin \left(i \theta_{12}\right) & 0 & 0 & \cdots\\
    \sin \left(i \theta_{12}\right) & \cos \left(i \theta_{12}\right)  & 0 & 0 & \cdots\\
    0 & 0 & \cos \left(t \theta_{34}\right) & -\sin \left(t \theta_{34}\right) & \cdots\\
    0 & 0 & \sin \left(t \theta_{34}\right) & \cos \left(t \theta_{34}\right)  & \cdots\\
    \vdots & \vdots & \vdots & \vdots & \ddots
\end{pmatrix}
$}
\end{equation*}
This 2D encoding allows the model to disentangle positional-based rotation (the token dimension \(i\)) from temporal-based rotation (the temporal dimension\(t\)), letting \method{} jointly reason about sequential and temporal positions.

\paragraph{Consistency with Standard Language Modeling.}
By interleaving temporal based rotation in these additional dimensions, it's easy to observe that when two tokens share the same timepoint $t$:
\begin{equation*}
\begin{aligned}
\left(\mathbf{R}_t^{(i)} \mathbf{q}_t^{(i)}\right)^{\top}\left(\mathbf{R}_t^{(j)} \mathbf{k}_t^{(j)}\right) & ={\mathbf{q}_t^{(i)}}^{\top} \mathbf{R}_0^{(j-i)} \mathbf{k}_t^{(j)} \\
& =\left(\mathbf{R}^{(i)} \mathbf{q}_t^{(i)}\right)^{\top}\left(\mathbf{R}^{(j)} \mathbf{k}_t^{(j)}\right)
\end{aligned}
\end{equation*}
which means that in the same timepoint the sequential attention pattern is identical to that of a conventional causal language model, and \(\mathbf{R}_t^{(i)}\) reduces to the usual (1D) rotation in \(i\).  

Hence, \method{} remains \textit{backward-compatible}: if no temporal dimension is present or within the same timepoint, it behaves like a standard causal language model. This design ensures that \method{} smoothly unifies the standard sequential modeling paradigm with the demands of non-Markovian discrete diffusion.

\input{paragraphs/tables/example_openweb}
\subsection{Adapt LLMs for Discrete Diffusion}
% Unlocking capability
A key observation is that standard causal language modeling can be seen as a \textbf{special case} of our proposed framework under particular settings: namely, a single-step diffusion (\(T=1\)) and a minimal context window restricted to the current timestep. When \(T=1\), the forward trajectory is simply \(\mathbf{x}_0\), and the reverse process is a single-step denoising process which autoregressively predicts next clean token, closly mirroring the standard language modeling paradigm.

Moreover, as shown in ~\ref{subsec:2d_rotary}, the 2D rotary positional encoding can be seamlessly integrated into existing language models, allowing for a unified treatment of both sequential and temporal dimensions. Given these equivalence, one can take a pretained LLM (trained in a standard causal fashion) and further fine-tune it under our non-Markovian diffusion objective. By allowing the model to condition on the previous timesteps in the diffusion chain \(\mathbf{x}_{t+1:T}\), we equip it with iterative denoising capabilities beyond standard next-token prediction. This straightforward adaptation:
\begin{itemize}
    \item \textbf{Expands Generation Modes:} The LLM can perform text infilling or partial prompting from arbitrary positions, rather than strictly appending text at the end, as shown in Figure~\ref{fig:sample_text_infilling}.
    \item \textbf{Leverages Pretraining Knowledge:} Since large LLMs are already trained on vast corpora, fine-tuning under our discrete diffusion objective benefits from a strong initialization and broad linguistic knowledge.
    \item \textbf{No Architectural Changes:} We only replace the original (causal) loss with a non-Markovian diffusion loss and provide noise-corrupted sequences as training data, preserving the underlying transformer structure.
\end{itemize}

%TODO{Talk about some related experiments}

\subsection{Inference Bottleneck of Naive \method{}}
Naive \method{} inference can be slower than standard discrete diffusion, typically requiring $\mathcal{O}(L \times T)$ function evaluations for a sequence of length $L$ over $T$ timesteps. However, by leveraging the unique properties of causal language modeling, we propose a \textit{semi-speculative decoding} strategy that substantially reduces inference time while maintaining generation quality.

\paragraph{Semi-Speculative Decoding.}

\begin{algorithm}[h]
    \caption{Semi-Speculative Decoding for Non-Markovian Discrete Diffusion}
    \label{alg:semi_spec_decoding}
    \begin{algorithmic}[1]
    \State \textbf{Input:}
        model parameters $\theta$, prior distribution $q(\mathbf{x}_T)$
    \State \textbf{Output:}
        Sampled data $\mathbf{x}_0$
    \State Initialize $\mathbf{x}_T \sim q(\mathbf{x}_T)$ \Comment{Noisy data at final timestep}
    \For{$t = T \textbf{ down to } 1$}
        \State $i \gets 0$
        % \State \textbf{// Verification in parallel}
        \If{$\widetilde{\mathbf{x}}_0^{\text{prev}} \text{ is available}$}
        \Comment{If previous step is available, use it as drafted tokens}
        \State $i \gets \textsc{Verify} (p_\theta, \mathbf{x}_{t:T}, \widetilde{\mathbf{x}}_0^{\text{prev}})$ 
        \Comment{Predict drafted tokens's probability in parallel, verify to find the first rejection on index $i$}
        \State $\widetilde{\mathbf{x}}_0^{i, \text{prev}} \gets \textsc{Correct} (p_\theta, \mathbf{x}_{t:T}, \widetilde{\mathbf{x}}_0^{\text{prev}})$
        \Comment{Correct the first rejection on index $i$ based on criterion}
        \State $\widetilde{\mathbf{x}}_0^{0:i} \gets \widetilde{\mathbf{x}}_0^{0:i, \text{prev}}$
        \EndIf
    

        \While{$i < L$} 
            \State $\widetilde{\mathbf{x}}_0^{i+1} \gets p_\theta\bigl(\mathbf{x}_0 \mid \mathbf{x}_{t:T}, \widetilde{\mathbf{x}}_0^{0:i}\bigr)$
            \State $ i \gets i + 1$
        \EndWhile
        
        \State $\mathbf{x}_{t-1} \sim q\bigl(\mathbf{x}_{t-1} \mid \widetilde{\mathbf{x}}_0\bigr)$

        \State $\widetilde{\mathbf{x}}_0^{\text{prev}} \gets \widetilde{\mathbf{x}}_0$
    \EndFor
    \State \textbf{return} $\mathbf{x}_0$ \Comment{Final predicted clean data}
    \end{algorithmic}
    \end{algorithm}

Although causal language models generate tokens sequentially, they can verify the probabilities of any \textit{pre-drafted} sequence in parallel. Notably, \method{} shares the same denoising target \(\mathbf{x}_0\) across all timesteps. This observation suggests a natural procedure: reuse the previous timestep's predictions \(\widetilde{\mathbf{x}}_0^{\text {prev }}\) as a \textit{draft} for the current timestep (see Algorithm~\ref{alg:semi_spec_decoding}). The model then \textit{verifies} these drafted tokens in parallel, accepting those that meet a specified confidence threshold (e.g. high probability).

This approach closely resembles speculative decoding~\citep{leviathan2023fast}, with one key difference: we do not rely on a separate, smaller model to propose the draft sequence. Instead, \method{}’s own predictions from the preceding timestep serve as the draft. Like speculative decoding, various verification and correction strategies (e.g. greedy, nucleus sampling) can be employed, ensuring either a comparable or identical sampling distribution while significantly reducing the total number of sampling steps.

\input{paragraphs/tables/protein}

\paragraph{Further Acceleration.}
Beyond semi-speculative decoding, \method{} also benefits from \textit{key-value caching (KV-Cache)}, a hallmark of causal generation that is unavailable in bidirectional discrete diffusion models. Additionally, the \(\textbf{x}_0\)-parameterization enables efficient timestep skipping, further accelerating inference. We believe these techniques only begin to illustrate the potential for more advanced optimization and scaling in \method{}, which we leave for future exploration.




% Analyze complexity of inference of naive CaDDI vs standard discrete diffusion vs standard LLM?


% \rex{complexity analysis}