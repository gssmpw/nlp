\section{Introduction}
Autoregressive transformers have become a dominant approach for sequence modeling~\citep{vaswani2017attention, chowdhery2023palm, touvron2023llama}, achieving state-of-the-art performance in many natural language and biological tasks. Their left-to-right decoding paradigm simplifies training via next-token prediction and is supported by large-scale pretraining, unlocking broad linguistic (or domain) knowledge. However, these models can be less flexible for {bidirectional or partially specified} generation, such as text infilling or prompting from arbitrary locations.

By contrast, discrete diffusion models~\citep{dieleman2022continuous, d3pm, gulrajani2024likelihood, gat2024discreteflowmatching} naturally accommodate {controllable} generation scenarios where tokens can be iteratively refined and sampled in a bidirectional manner~\citep{shen2023film}. Recent advances have extended discrete diffusion to continuous time~\citep{campbell2022continuous, shi2024simplified}, improved its training objectives~\citep{lou2024discretediffusionmodelingestimating, mdlm, udlm}, and accelerated inference~\citep{park2024textit, liu2024discrete}. Yet, these models often lag behind autoregressive approaches in generation quality~\citep{zheng2024masked}, due in part to their reliance on a \textit{single} latent state for denoising—leading to fragile inference where small decoding errors can accumulate over time~\citep{xu2024energy, zheng2024masked, hu2024mask}.

\begin{figure}[t]
    \label{fig:non-markov}
    \centering
    \includegraphics[width=1\linewidth]{paragraphs/figures/diffusion_process.pdf}
    \vspace{-13pt}
    \caption{(a). \textbf{Framework of non-Markovian discrete diffusion}, where noise is added independently to \(\mathbf{x}_0\) at each timestep. The reverse pass leverages the entire generative trajectory for denoising. (b). \textbf{$\mathbf{x}_0$-parameterization.} The reverse model predicts the clean data \(\widetilde{\mathbf{x}}_0\) and then re-applies noise to obtain \(\mathbf{x}_{t-1}\).}
    \label{fig:enter-label}
    \vspace{-10pt}
\end{figure}

In this work, we aim to bridge the gap between both paradigms. Specifically, we extend the non-Markovian diffusion process of \citet{DART} to the discrete domain, allowing each denoising step to use information from the entire generative trajectory rather than from a single state. This holistic view mitigates error accumulation and makes the backward (denoising) process naturally align with \textit{causal} language modeling. Leveraging this insight, we propose \method{}, a causal discrete diffusion model that unifies \textit{sequential} (left-to-right) and \textit{temporal} (multi-step) dimensions in a single transformer architecture. As a result, \method{} can be trained efficiently via a simple next-token prediction loss—similar to a causal language model—while preserving the bidirectional control and iterative refinement of diffusion. 

Additionally, we show that \method{} can be viewed as a generalization of traditional autoregressive models ($T=1$ is the special case), making it straightforward to fine-tune a pretrained LLM for discrete diffusion. Such adaptation unlocks flexible generation modes (e.g., text infilling) without sacrificing the rich knowledge encoded by large-scale pretraining. Finally, \method{} can be further accelerated at inference via semi-speculative decoding.

% \rex{previous paragraph mentions the motivation of better bidirectional / controllable generation. it seems that here if we only mention standard next token prediction, the main motivation becomes quite limited. we should make our main motivation very clear}
% \rex{if we want to claim unification as the main selling point, maybe we should discuss advantages and issues of both discrete diffusion and Transformer autoregression paradigm in the first paragraph, and say how the unification can bring the best of both worlds?}

% \rex{figure needed}

In summary, our key contributions are:
\begin{itemize}
    \item We first extend the non-Markovian diffusion process to the discrete space, where the model integrates the generative trajectory of the preceding states, enabling a more robust inference paradigm.

    \item We propose \method{}, a causal discrete diffusion model that unifies sequential and temporal modeling within a non-Markovian diffusion framework. \method{} generalizes traditional causal language models as a special case and can seamlessly adopt pretrained LLMs for discrete diffusion, enabling more controllable and structured generation.

    % \rex{any theoretical expressiveness results we can claim in intro as a contribution? }
    
    \item Quantitative results show that CaDDi surpasses recent discrete diffusion models on both biological sequences and natural language tasks. Additionally, it provides greater control over generation compared to standard autoregressive models, making it a powerful alternative for structured sequence modeling.
    
\end{itemize}





