
\section{Experiments}

\paragraph{Baseline.} We compare \method{} with several established discrete diffusion models, including D3PM \cite{d3pm}, SEDD \cite{lou2024discretediffusionmodelingestimating}, MDLM \cite{mdlm}, UDLM \cite{udlm}, and Discrete Flow Matching \cite{gat2024discreteflowmatching}. For MDLM and UDLM, we utilized their official implementations, which also include implementations of D3PM and SEDD. Additionally, we implemented Discrete Flow Matching based on the MDLM repo with the discrete path and denoising loss defined in ~\citep{gat2024discreteflowmatching}.

Our core causal language model is based on Pythia-160M \cite{biderman2023pythiasuiteanalyzinglarge}, with a customized tokenizer and embedding layers tailored to the specific task. For baseline models, we follow their original implementations, using Diffusion Transformers \cite{peebles2023scalablediffusionmodelstransformers} with rotary positional encoding and timestep embeddings.
To ensure a fair comparison, we match the number of learnable parameters across all models, with \method{} using slightly fewer parameters (see Table~\ref{tab:parameter_count} for details). All discrete diffusion and flow-matching models use 1000 diffusion steps, while \method{} is configured with a context window of 4 time points and 64 diffusion steps (see Section~\ref{sec:ablation} for an ablation study).
All models are trained with a learning rate of 3e-4, 2500 warm-up steps, and linear learning rate annealing. For evaluation, we sample the same number of sequences across all models.



\input{paragraphs/tables/lm1b}
\subsection{Biological Sequence Generation}
We test the sequence generation capability of \method{} on the AcyP protein dataset, comprising 26,878 protein sequences from the Acylphosphatase family, each containing 64 to 128 residues, sourced from UniProt \citep{10.1093/nar/gkae1010}.

\paragraph{Metrics}
We assess the quality of generated sequences using the following metrics: pLDDT \citep{Jumper2021} and self-consistency perplexity (scPPL), which measure the feasibility of sequences to fold into stable protein structures; and TM-score, RMSD, and H-prob, which evaluate the structural similarity of generated sequences to known structures in the PDB database \citep{Berman2000-ql}. Detailed descriptions of these metrics are provided in \ref{sec:appendix-metrics}.

As shown in Table \ref{tab:protein}, \method{} consistently outperforms all baseline models across all metrics. High pLDDT scores and low scPPL values indicate that sequences generated by \method{} are highly likely to fold into viable protein structures, as visualized in Figure~\ref{fig:protein_sample_result}. Furthermore, TM-score, RMSD, and H-prob demonstrate that \method{} generates realistic sequences with strong homology to known structures in the PDB database.

% Original caption by Yangtian:
% Comparison of Models by NFE and Generative Perplexity. Using semi-speculative decoding, our model, CaDDi-Spec, achieves a relatively lower NFE of \raisebox{-0.55ex}{\textasciitilde{}} 656 while attaining superior generative perplexity compared to other models.

We test \method{}'s capability in modeling more complicated sequences on One Billion Words \cite{chelba2014billionwordbenchmarkmeasuring}, a large, real world natural language dataset consisting over 30M English language sentences with varying lengths. We follow the tokenization and training setup in DiffusionBert 
\citep{he2022diffusionbertimprovinggenerativemasked}. For UDLM, we directly use pretrained weights hosted by the authors.

\subsection{Unconditional Text Generation}
\paragraph{Metrics}
To evaluate the quality of model-generated texts, we report {guided generative perplexity}, a refined version of generative perplexity that evaluates texts within a natural language context. This adjustment helps mitigate the degenerate behaviors observed with standard generative perplexity \citep{holtzman2020curiouscaseneuraltext} The guided generative perplexity is computed using various large language models, including GPT-2 \citep{radford2019language}, Llama-2 (7B parameters) \citep{touvron2023llama2openfoundation}, and Llama-3 (3B parameters) \citep{dubey2024llama}, all of which are pretrained on large natural language corpora. To further evaluate the diversity of generated texts, we compute the self-BLEU \citep{zhu2018texygenbenchmarkingplatformtext} score of the set of generated texts. Details of the metrics can be found in \ref{sec:appendix-metrics}).

\method{} achieves strong generative perplexities across all three large language models evaluated, outperforming baselines in all but one case with only a marginal difference. This demonstrates \method{}'s capability in generating coherent text unconditionally. Additionally, \method{} achieves a comparable self-BLEU score with other models, highlighting its ability to generate diverse and coherent text samples. As shown in Figure~\ref{fig:NFE}, with the help of semi-speculative decoding, our model can achieve better performance while maintaining an inference cost comparable to other models. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{paragraphs/figures/model_comparison_with_blue_palette.pdf}
    \vspace{-10pt}
    \caption{\textbf{Comparison of several discrete diffusion models on LM1B by number of function evaluations (NFE, x-axis) and generative perplexity (y-axis)}. Each curve traces how perplexity changes as the NFE increases. CaDDi-Spec represents \method{} with semi-speculative decoding by nucleus sampling~\citep{leviathan2023fast}.
    % By incorporating semi-speculative decoding, our method CaDDi-Spec achieves a notably lower NFE ($\approx$ 656) while still outperforming other models in perplexity. The dashed horizontal line indicates the dataset-level perplexity on LM1B.
    }
    \label{fig:NFE}
    \vspace{-10pt}
\end{figure}



\subsection{Conditional Text Generation}
We evaluate conditional text generation on the Amazon Polarity dataset~\citep{mcauley2013hidden}, which consists of 3.6M Amazon reviews labeled as positive or negative. We adapt this task as text infilling by prepending a label-based prompt to each review (see Appendix~\ref{sec:amazon_process} for details). We train the conditional generator \(p_\theta\left(\mathbf{x}_0 \mid \mathbf{x}_t, y\right)\) alongside unconditional one  \(p_\theta\left(\mathbf{x}_0 \mid \mathbf{x}_t\right)\), by preserving certain parts of the text as fixed.

We measure sentiment accuracy (SA) using a fine-tuned DistilBERT classifier. As shown in Table~\ref{tab:sentiment-comparison}, our approach achieves performance comparable to a fine-tuned GPT-2 on the same dataset while offering more flexible generation (unlike GPT2 only allow prompting from begining, our method allows for prompting from arbitrary parts of the text, such as the middle or the title. Examples are shown in Figure~\ref{fig:amazon_result_negative}). Furthermore, by applying classifier-free guidance (CFG)~\citep{ho2022classifier} with different guidance scales \(\gamma\), we generate reviews that better align with the given prompts.
\vspace{-5pt}


\input{paragraphs/tables/amazon}
