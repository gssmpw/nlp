\section{Preliminary}
% \subsection{Variational Perspective of Diffusion Models}
% Diffusion models can be viewed as a specific type of hierachical variational autoencoder (HVAE), where the latent variable is a series of corrupted data. HVAE is trained to optimize the evidence lower bound (ELBO) loss, which is defined as:
% \begin{align}
%     \max \mathcal{L}_{\theta, \phi}^{\mathrm{ELBO}} = \mathbb{E}_{\mathbf{x}_{1: T} \sim q_\phi(\mathbf{x})} & \bigg[ \sum_{t=1}^T \log p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t: T}\right) \nonumber \\
%     & \quad + \log p_\theta\left(\mathbf{x}_T\right) \nonumber \\
%     & \quad - \log q_\phi\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right) \bigg]
% \end{align}

% As a special case of HVAE, traditional diffusion models assume 
% \begin{itemize}
%     \item A Non-learnable Markovian forward process \(q_{\phi}\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right)= \prod_{t=1}^T q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\right)\),
%     \item A Markovian reverse process \(p_\theta\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t: T}\right)=p_\theta\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t\right)\)
% \end{itemize}
% By enforcing the Markovian property in reverse process, the model ensure that at each step, the model only look back at the current state and predict the previous state. 

\subsection{Variational Perspective of Diffusion Models}
% \rex{i think we can shorten this entire section by moving some to appendix. there are a lot more that are worth expanding in later sections..}
Diffusion models can be viewed as a special class of hierarchical variational autoencoders (HVAEs), where the latent variables consist of progressively corrupted versions of the original data. HVAEs are trained by maximizing the evidence lower bound (ELBO) on the data log-likelihood. Concretely, let \(\mathbf{x}_0\) be the original data and \(\mathbf{x}_{1:T}\) be the latent variables at timesteps \(1,\dots,T\). The ELBO objective can be written as:
\begin{align}
    \max_{\theta, \phi} \;\; \mathcal{L}_{\theta, \phi}^{\mathrm{ELBO}} 
    &= \mathbb{E}_{\mathbf{x}_{1: T} \sim q_\phi(\mathbf{x}_{1: T} \mid \mathbf{x}_0)} \Bigg[ 
        \sum_{t=1}^T \log p_\theta\bigl(\mathbf{x}_{t-1} \mid \mathbf{x}_{t:T}\bigr) \nonumber \\
        & \qquad\quad + \log p_\theta\bigl(\mathbf{x}_T\bigr)
         \;-\; \log q_\phi\bigl(\mathbf{x}_{1:T} \mid \mathbf{x}_0\bigr) 
    \Bigg],
\end{align}
where \(q_\phi(\mathbf{x}_{1:T} \mid \mathbf{x}_0)\) is the variational posterior distribution, and \(p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_{t:T})\) is the generative (reverse) model.

\paragraph{Markovian Assumptions.}
Traditional diffusion models (viewed as a special case of HVAEs) make the following Markovian assumptions:
\vspace{-0.5em}
\begin{itemize}
    \item \textbf{Forward Process:} 
    A non-learnable Markov chain \(q_\phi\bigl(\mathbf{x}_{1:T} \mid \mathbf{x}_0\bigr)=\prod_{t=1}^T q\bigl(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\bigr)\).
    \item \textbf{Reverse Process:} 
    A learnable Markov chain \(p_\theta\bigl(\mathbf{x}_{t-1} \mid \mathbf{x}_{t:T}\bigr)\!=\!p_\theta\bigl(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\bigr)\).
\end{itemize}
\vspace{-0.5em}
Enforcing the reverse process to be Markovian simplifies the sampling process: at each timestep, the model only conditions on the current latent variable \(\mathbf{x}_t\) to predict \(\mathbf{x}_{t-1}\). While this constraint facilitates efficient generation, it can limit the model's capacity to capture long-range dependencies within the latent chain. % Cite



\subsection{Discrete Diffusion Language Models}
Recently, discrete diffusion models~\citep{d3pm} have recently emerged as a powerful framework. In contrast to their continuous counterparts, which corrupt data by adding Gaussian noise in a real-valued space, discrete diffusion models operate on categorical variables, gradually corrupting tokens before reconstructing them through a learned denoising process.

\paragraph{Forward Process.}
Let \(\mathbf{x}_0 = (\mathbf{x}_0^1, \mathbf{x}_0^2, \ldots, \mathbf{x}_0^L)\) be a sequence of discrete tokens from a vocabulary \(\mathcal{V}\) of size \(|\mathcal{V}|\). The forward (noising) process produces \(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_T\) by independently corrupting each token according to a time-dependent transition matrix \(\mathbf{Q}_t\):
\begin{align}
    q\bigl(\mathbf{x}_t \mid \mathbf{x}_{t-1}\bigr)
    &= \operatorname{Cat}\Bigl(\mathbf{x}_{t};\, \mathbf{Q}_t \,\mathbf{x}_{t-1}\Bigr),
\end{align}
where \(\operatorname{Cat}(\cdot;\,\pi)\) denotes the categorical distribution with parameter \(\pi\), and \([\mathbf{Q}_t]_{ij}\) gives the probability of transitioning from state \(i\) to state \(j\) at time \(t\).
%TODO: Possibly wrong


\paragraph{Reverse Process.}
The reverse (denoising) model \(p_\theta\bigl(\mathbf{x}_{t-1} \mid \mathbf{x}_t\bigr)\) is learned to invert the corruption process. Employing the \textit{x\textsubscript{0}-parameterization}~\cite{d3pm}, one can write:
\begin{equation}
    p_\theta\bigl(\mathbf{x}_{t-1} \mid \mathbf{x}_t\bigr) 
    \;\;\propto\;\; 
    \sum_{\widetilde{\mathbf{x}}_0} 
    q\bigl(\mathbf{x}_{t-1}, \mathbf{x}_t \mid \widetilde{\mathbf{x}}_0\bigr)\;
    p_\theta\bigl(\widetilde{\mathbf{x}}_0 \mid \mathbf{x}_t\bigr),
\end{equation}
where \(p_\theta\bigl(\widetilde{\mathbf{x}}_0 \mid \mathbf{x}_t\bigr)\) is a time-dependent denoiser mapping \(\mathbf{x}_t\) back to an estimate of the original \(\mathbf{x}_0\).

\paragraph{Training Objectives.}
Training commonly involves maximizing the variational lower bound on \(\log p_\theta(\mathbf{x}_0)\). In practice, ~\citep{d3pm} have shown that a simple denoising objective can achieve better generative quality under \textit{x\textsubscript{0}-parameterization}:
\begin{equation}
    \label{eq:vanilla_loss}
    \mathcal{L}_{\text{simple}}
    = \mathbb{E}_{\mathbf{x}_0,\, \mathbf{x}_t \sim q}
      \bigl[
        -\log p_\theta\bigl(\mathbf{x}_0 \mid \mathbf{x}_t\bigr)
      \bigr],
\end{equation}
In this work, we also adopt this simplified objective. Various refinements of this objective exist, such as implicit maximizing the variational lower bound by score entropy~\citep{lou2024discretediffusionmodelingestimating}, utilizing a reweighted denoising objective specifically designed for absorbing discrete diffusion~\citep{mdlm}, but the Markovian nature of the forward and reverse processes remains a key component of discrete diffusion models.


% TODO: add shorgate of Markovian diffusion model here to explain the motivation


% TODO{possibly use (i, j) notation}

