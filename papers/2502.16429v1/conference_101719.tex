\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts


\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic} 
\usepackage{graphicx} 
\usepackage{textcomp} 
\usepackage{xcolor} 
\usepackage{booktabs} % 表格
\usepackage{tabularx} % 表格美化
\usepackage{algorithm} % 算法
\usepackage{hyperref} % 超链接
\usepackage{placeins} % figure*和table*阻止跨越浮动的宏包
\usepackage{nicefrac} % 提供 \nicefrac 命令
\usepackage[caption=false]{subfig} % 子图
\usepackage{newtxtext,newtxmath}  % 使用新的 Times 字体包
% \usepackage[table,xcdraw]{xcolor}

\usepackage{fontenc}  % 加载该宏包


\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm  
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em


    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% 作者居中
\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother

\makeatletter
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}
\makeatother

\begin{document}


\sloppy %用于调整文档中的文本断行处理方式

\title{A Joint Learning Framework for Bridging Defect Prediction and Interpretation}
\author{

\IEEEauthorblockN{ Guifang Xu}
\IEEEauthorblockA{\textit{software school} \\
\textit{Yunnan University }\\
Yunnan, China \\
x07191822@163.com} \\

\IEEEauthorblockN{ Xingcheng Guo}
\IEEEauthorblockA{\textit{software school} \\
\textit{Yunnan University}\\
Yunnan, China \\
rxij4vcol3@mail.com} 

\and

\IEEEauthorblockN{ Zhiling Zhu}
\IEEEauthorblockA{\textit{software school} \\
\textit{Yunnan University }\\
Yunnan, China \\
leoniddankt@mail.com} \\

\IEEEauthorblockN{Wei Wang \textsuperscript{*}}
\IEEEauthorblockA{\textit{software school } \\
\textit{Yunnan University }\\
Yunnan, China \\
wangwei@ynu.edu.cn} 

}
\maketitle

\begin{abstract}
Over the past fifty years, numerous software defect prediction (SDP) approaches have been proposed. However, the ability to explain why predictors make certain predictions remains limited. Explainable SDP has emerged as a promising solution by using explainable artificial intelligence (XAI) methods to clarify the decision-making processes of predictors. Despite this progress, there is still significant potential to enhance the reliability of \textcolor{blue}{existing} approaches. To address this limitation, we treat defect prediction and the corresponding interpretation as two distinct but closely related tasks and propose a joint learning framework that allows for the simultaneous training of the predictor and its interpreter. The novelty of our approach lies in two main aspects:
1. We design feedback loops that convey the decision-making logic from the predictor to the interpreter. This ensures a high level of conciseness in decision logic and feature engineering for both the predictor and the interpreter, enabling the interpreter to achieve reliable local and global interpretability.
2. We incorporate the interpretation results as a penalty term in the loss function of the joint-learning framework. This not only improves the accuracy of the predictor but also imposes a stronger constraint on the reliability of the interpreter.
We validated our proposed method against several \textcolor{blue}{existing} explainable SDPs across multiple datasets. The results demonstrate its effectiveness in both interpretation and defect prediction.
The source code for the proposed method is available at:
\noindent \href{https://github.com/BugPredictor/software-defect-prediction.git}{https://github.com/BugPredictor/software-defect-prediction.git}
\end{abstract} 
\begin{IEEEkeywords} 
software defect prediction, interpretability, knowledge distillation, joint learning framework
\end{IEEEkeywords} 



%-------------------------1. introduction------------------------------------------------
\section{Introduction}
For nearly half a century, software defect prediction (SDP) has been a significant research focus in the field of software engineering \cite{1,2}. Its primary aim is to identify defective code early in the software development process, helping practitioners prioritize quality assurance tasks, especially when resources are limited or deadlines are tight \cite{55}. Inspired by the success of machine learning (ML) in various disciplines, such as computer vision and machine translation, researchers have explored applying different ML algorithms to improve the accuracy of SDP. However, despite numerous efforts, existing SDP models often fail to provide actionable guidance for practitioners, making practical application challenging \cite{57}.

Recent studies have highlighted that the limited applicability of SDP in real-world scenarios is closely linked to the lack of interpretability \cite{3,4}. Interpretability refers to the ability to provide comprehensible insights into the decision-making logic for humans. Without it, developers struggle to understand the logic behind SDP predictions, leading to distrust and reluctance to adopt these techniques \cite{3, 5}. Explaining prediction results can be particularly challenging for ML-based predictors, especially those using deep learning (DL). The complex structures and large number of parameters in such models obscure straightforward interpretation. 

To address this issue, researchers have investigated the feasibility of explainable SDP, incorporating explainable artificial intelligence (XAI) into SDP. Efforts in explainable SDP are generally categorized into two groups \cite{murdoch2019definitions}: \textit{global} and \textit{local} interpretations. 
\textcolor{blue}{\textit{Global} approaches focus on explaining the overall behavior of a defect predictor, offering insights into how the model makes decisions across all instances \cite{7}. Local approaches provide clear evidence for why a given instance is predicted as buggy or clean. The prediction results provide actionable guidance for practitioners to address key questions, such as why a specific prediction was made and how to mitigate associated risks \cite{YU2024}. As a result, practitioners often prefer to use local interpretation techniques in practice \cite{3}. Nevertheless, local techniques often encounter challenges with reliability under different settings \cite{58}.}
%============A Formal Explainer for Just-In-Time Defect Predictions============
%  Local approaches provide clear evidence for why a given instance is predicted as buggy or clean. They focus on individual predictions to address key questions from practitioners and domain experts, such as why a specific prediction was made and how to mitigate associated risks.

% However, these methods often fail to explain why a specific code fragment is deemed buggy or clean. \textcolor{blue}{Local interpretation techniques, due to their focus on individual predictions, are considered to provide better explainability in practice compared to global methods. They can generate intuitive explanations, helping practitioners understand why a specific code fragment is predicted as defective or non-defective, while also offering actionable insights for defect resolution. For instance, by identifying the most influential metrics or features leading to a defect prediction, practitioners can prioritize high-risk components, allocate resources more efficiently, and improve overall software quality.} 
% Practitioners often prefer to use {\textit{local}} interpretation techniques in practice \cite{3}. Different from \textit{global} approaches, \textit{local} techniques offer explanations for the predictions of individual code fragments \cite{8}. Typical techniques of this category involve LIME \cite{lime}, SHAP \cite{10} and BreakDown \cite{34}.
% Nevertheless, \textit{local} techniques often encounter challenges with reliability under different settings \cite{58}.

\begin{figure*}[t]
    \centering
    \captionsetup[subfloat]{labelformat=empty} % 去掉子图的序号
    \subfloat[(a) The interpretations by different sampling techniques]{
    \label{fig_1:LIME_RUS}
        \subfloat[The interpretation for the SMOTE-based predictor]{
            \includegraphics[width=0.4\textwidth, height=0.12\textheight]{images/output_SMOTE_2.png}
        }
        \label{fig1:LIME_two}
        \subfloat[The interpretation for the RUS-based predictor]{
            \includegraphics[width=0.4\textwidth, height=0.12\textheight]{images/output_RUS_2.png}
        }
    }

    \subfloat[\textcolor{blue}{(b) Two interpretations produced by LIME}]{
    \subfloat[The first interpretation]{
            \includegraphics[width=0.4\textwidth, height=0.12\textheight]{images/output_LIME_two.png}
        }
        \subfloat[The second interpretation]{
            \includegraphics[width=0.4\textwidth, height=0.12\textheight]{images/output_LIME_one.png}
        }
    }
    \caption{Interpretations of LIME for a code fragment.}
    \label{fig_1}
\end{figure*}

To illustrate the challenges associated with \textit{local} approaches, we take the interpretation for "\textit{AbstractCommand.java}" from the Apache ActiveMQ project as an example.
The explanations generated by LIME with different samplers (i.e., SMOTE and RUS) are shown in Fig.~\ref{fig_1}~\subref{fig_1:LIME_RUS}. The explanations generated by applying LIME twice to the same file are shown in Fig.~\ref{fig_1}~\subref{fig1:LIME_two}.
These figures present the rankings of features that contribute to the predictions. Features displayed in red indicate their contribution to predicting buggy code, while those displayed in green indicate their contribution to the prediction of clean code.

In Fig.~\ref{fig_1}~\subref{fig_1:LIME_RUS}, we can see that although the SDP model with different samplers predicts the file as buggy, their explanations vary significantly. Among the ten features selected by LIME for the SMOTE-based predictor, only two are also selected for the RUS-based predictor. Fig.~\ref{fig_1}~\subref{fig1:LIME_two} further demonstrates that the two explanations generated by running the same predictor twice show considerable variation. For instance, in the first explanation, seven features contribute to the prediction of buggy code, whereas in the second explanation, the number increases to eight.

To address this issue, we consider defect prediction and the corresponding interpretation as two distinct but strongly related tasks and propose a joint-learning framework in this paper. Specifically, we design multiple feedback loops between the interpreter and predictor to simultaneously achieve reliable interpretation and high prediction accuracy. The specific contributions are as follows:

\begin{itemize}
    \item To improve the reliability of interpretation, we revised the loss function of the knowledge distillation (KD) framework and introduced output fidelity loss and feature fidelity loss. This strategy constructs a knowledge transfer loop between the interpreter and predictor, ensuring a high level of conciseness in decision-making logic and feature engineering power.
    \item To improve the accuracy of the predictor, we incorporate the interpretation results as a penalty term within the loss function of the joint-learning framework. This enables the predictor to capture more discriminative features guided by the interpretation results while imposing stronger restrictions on the fidelity of the interpreter to the predictor.
    \item Extensive experiments are conducted on 20 datasets with an in-depth analysis. The results indicate that the joint-learning framework improves both the reliability of interpretation and the accuracy of the predictor.
\end{itemize}

The paper is organized as follows: \textcolor{orange}{Section II} provides an overview of the existing research on SDP and interpretation methods used in SDP. In \textcolor{orange}{Section III}, we describe our proposed method in detail, while in \textcolor{orange}{Section IV}, we provide the details of the experiment. In \textcolor{orange}{Section V,we discuss the threats to validity. Finally, in Section VI, we present our conclusion and future works.}


%----------------------2. related work---------------------------------------------------
\section{Related Work}
This section provides an overview of SDP and explores existing techniques for explainable SDP.
\subsection{Software defect prediction}\label{subsec_2}
The primary goal of SDP is to classify code fragments (e.g., method or class) as either clean or buggy. Constructing a typical SDP model involves three steps. \textcolor{blue}{The first step is to transform software metrics, such as cohesion, coupling, complexity, encapsulation, inheritance, and size, into corresponding numerical representations \cite{12}.} \textcolor{blue}{The second step labels code fragments as buggy or clean based on post-release defects.} These defects can be collected from a Bug Tracking System (BTS) by connecting bug reports to their bug-fixing activities. Code fragments associated with these bug-fixing activities are regarded as buggy, while all others are labeled as clean. Finally, the labeled code fragments serve as training data for an ML-based classifier, which is then used to predict whether code fragments in a later version are buggy or clean.

Numerous ML algorithms, including Fuzzy self-organizing maps, K-means, SVM, random forest, logistic regression, etc., have been extensively employed in previous research \cite{20, 21, 22, 23, 24, 25, 26}. However, metrics used in SDP consistently exhibit correlations and redundancies \cite{13}, compromising the effectiveness of traditional ML-based predictors \cite{14}. To address this issue, various DL models have been investigated to improve SDP performance. The initial DL-based defect predictor, Deeper, introduced by Yang et al. \cite{27}, employed a deep belief network to identify discriminative information in the input data. Experimental results showed that Deeper could identify 32.22\% more defects than many traditional ML-based approaches.
Inspired by Yang’s work, many researchers have proposed different DL-based solutions. Specifically, Hoang et al. proposed a convolutional neural network (CNN)-based defect prediction model \cite{28}, which experimental results indicated outperformed Deeper. Qiao et al. proposed another DL-based model called DPNN to predict the number of defects \cite{29}. Experimental results showed a 14\% reduction in mean square error and an 8\% increase in the squared correlation coefficient. Li et al. proposed another CNN-based defect predictor, DP-CNN \cite{30}, which achieved better performance than other methods. As demonstrated, DL has significantly improved the performance of SDP.

To ensure the generalization, we employ the DP-CNN proposed in \cite{30} as the predictor and evaluate the difference in accuracy before and after integrating it into the joint-learning framework.

\subsection{Interpretation of defect prediction}
In many practical application scenarios, practitioners often ask: What is the underlying decision logic of the predictor? What is the reason for predicting a code fragment as buggy? How do different metrics influence the prediction? These questions highlight an urgent requirement for explainable defect prediction. However, highly accurate SDP models (e.g., Random Forest, DL) are often black-box in nature, making them difficult to interpret.

Efforts to improve the interpretability of predictions can be broadly divided into two main categories \cite{murdoch2019definitions}: \textit{global} and \textit{local} approaches. The \textit{global} approaches offer a high-level view of how inputs influence the model’s predictions and reveal patterns that are consistent across the model’s entire operation, rather than focusing on individual predictions. Many ML algorithms (e.g., decision trees, logistic regression techniques) and statistical methods (e.g., ANOVA, variable importance) fall into this category\cite{31,32}. These methods share two common aspects: 1. Simulatability: Every step of the decision-making process is comprehensible. For example, decision trees are simulatable since we can observe the state of every node as well as the feature thresholds used to separate samples. 2. Transparency: The decision-making process can be broken down into interpretable steps. For instance, the interpretability of a decision tree is defined by the computational path from the root node to the leaf nodes.
However, \textit{global} approaches cannot provide detailed interpretations for each code fragment, and their relatively simple structure often fails to guarantee optimal predictive accuracy. Consequently, many practical applications have adopted \textit{local} methods.

In contrast to \textit{global} approaches, \textit{local} methods such as LIME \cite{lime} and BreakDown \cite{34}, usually interpret the prediction of each code fragment as a list of ordered features. Feedback from practitioners indicates that the interpretations produced by \textit{local} methods can offer more actionable guidance \cite{58}.
However, recent studies have reported that \textit{local} techniques lack reliability under various conditions \cite{36,37,58}. Firstly, \textit{\textbf{ the interpretations \textcolor{blue}{of the same code can be inconsistent} when different data sampling techniques are used.}} 
In SDP, sampling is frequently used due to the scarcity of buggy codes. It is an effective way to mitigate the effects of imbalanced data on the ML algorithms \cite{59}. In \cite{58}, Jiho Shin et al., investigated the consistency of \textit{local} methods across five commonly used data sampling techniques. Their analysis revealed that both the features and the ranking of features in the interpretation vary significantly when different data sampling techniques are employed.
Secondly, \textit{\textbf{the interpretations of a code fragment do not exhibit consistency when multiple interpretations are executed.}} The underlying intuition of \textit{local} methods regards the interpreter $g$ and the predictor $f$ as two independent ML algorithms and leverages $g$ to approximate the behavior of $f$ within the local area of an instance $x$, denoted as $\pi_x$. In general, \textit{local} methods cannot guarantee that the $\pi_x$ required for multiple interpretations of the same instance are the same. So different $\pi_x$ can lead to distinct interpretations for the same code fragment \cite{37}. Finally, \textit{\textbf{the predictions produced by the interpreter and the predictor for the same code fragment are not consistent.}} To reduce the difficulty of interpretation, many \textit{local} methods define the interpreter as a simple-structure ML model. Taking the most widely used local interpretation technique LIME as an example, it sets a linear regression model as the interpreter, and the interpretation of SDP is oversimplified as a linear transformation between input metrics and prediction outcomes.
Recently, \cite{37} emphasized that attempting to comprehend an intricate model by employing a simple model might be overly optimistic. The inconsistency between the predictions generated by the interpreter and the predictor indicates that the interpretation fails to capture the underlying decision-making logic of the predictor.

\subsection{Knowledge distillation}
Knowledge distillation (KD) is a typical model compression technique that transfers knowledge from a teacher model (e.g., DL models) to a student model (e.g., a shallow neural network) \cite{38,39}. KD is regarded as a promising framework for improving the reliability of interpretation due to the faithful knowledge transfer channel between the student and teacher models. In the health care domain, \cite{che2016interpretable} leveraged distilled knowledge to discover disease patterns. Similarly, in the business domain, \cite{biggs2021model} distilled dynamical pricing knowledge from a complex black-box DL model, optimizing revenue while maintaining interpretability. In this paper, we designate the predictor as the teacher model and the interpreter as the student model, and transfer the decision-making knowledge of predictor to interpreter based on the KD principle. The main difference between our approach and existing methods lies in the revised loss functions of KD framework, which enable us to train the interpreter and predictor in a collaborative manner.


%-------------------------------3. proposed method ---------------------------------------
\section{Proposed Method}
Fig.~\ref{fig1} provides an overview of our approach, which consists of four modules: metric selection, predictor and interpreter design, joint learning framework, and local interpretation.

\begin{figure}[tb]
\centerline{\includegraphics[width=\columnwidth]{images/framework_new.png}}
\caption{The framework of our approach.}
\label{fig1}
\end{figure}

\subsection{Metric selection}
% Through the careful investigation \cite{11,42,43,12,44}, we select 16 metrics for our method, detailed information in Table \ref{tab1}. We chose these metrics based on the following criteria: (1) the metrics are expressive enough to distinguish as much similar source code as possible; and (2) their effectiveness has been confirmed in previous studies \cite{22,27,45,46}.

\textcolor{blue}{We carefully selected 16 metrics for our method after a thorough investigation of existing literature and metric suites, including the works of \cite{12,43,44}. The details of metrics are presented in Table \ref{tab1}. The selection of these metrics was guided by two primary criterias, ensuring their relevance and effectiveness in the context of defect prediction: (1) Expressiveness: The metrics needed to be sufficiently expressive, enabling them to capture subtle variations in code quality, structure, and complexity between similar pieces of source code. (2) Proven Effectiveness: Each selected metric has been validated in prior studies \cite{22,27,45,46}, demonstrating its utility in software defect prediction tasks. By adhering to these criterias, we constructed a dataset that covered a broad spectrum of code attributes, ranging from complexity measures to process-related characteristics.}

\begin{table}[tb]
\centering
\caption{Metrics suite}
\begin{tabular}{ll}
\toprule
% \textbf{Feature} & \textbf{Description} \\ 
\multicolumn{1}{c}{\textbf{Feature}} & \multicolumn{1}{c}{\textbf{Description}} \\ % 列名使用multicolumn命令居中
\midrule
WMC & Weighted methods per class  \\
\midrule
DIT & Depth of Inheritance Tree  \\
\midrule
NOC & Number of Children \\
\midrule
CBO & Coupling between object classes  \\
\midrule
RFC & Response for a Class \\
\midrule
LCOM & Lack of cohesion in methods  \\
\midrule
NPM & Number of Public Methods  \\
\midrule
DAM & Data Access Metric \\
\midrule
MOA & Measure of Aggregation  \\
\midrule
MFA & Measure of Functional Abstraction  \\
\midrule
CAM & Cohesion Among Methods of Class  \\
\midrule
IC & Inheritance Coupling  \\
\midrule
CBM & Coupling Between Methods  \\
\midrule
AMC & Average Method Complexity  \\
\midrule
LOC & Lines of Code  \\
\midrule
CC & McCabe's cyclomatic complexity \\
\bottomrule
\end{tabular}
\label{tab1}
\end{table}


\subsection{Defect prediction and interpretation model}
In this section, we provide the detailed information about the predictor and interpreter.

\subsubsection{Defect predictor}
To ensure the generality of our method, we chose a DL-based model, DP-CNN \cite{30} as the predictor. It consists of three main components: the convolution layer, max pooling layer, and classification layer. To further improve accuracy, we added an attention layer to the original model. The overall architecture of DP-CNN is presented in Fig.~\ref{fig2}. 
\begin{figure}[!tb]
\centerline{\includegraphics[width=\columnwidth]{images/DP-CNN_new.png}}
\caption{Model Architecture of DP-CNN.}
\label{fig2}
\end{figure}

\textcolor{blue}{The working procedure of DP-CNN can be summarized as follows:
First, the input defect metrics are fed into the convolution layer to extract correlations between features. Then feature maps are then passed through the max pooling layer, which reduces the dimensionality while retaining the most significant features. To further enhance the model's ability to focus on critical patterns, the attention layer dynamically assigns weights to the feature maps. Finally, the processed features are passed through a fully connected neural network and a softmax classifier to generate the prediction result.}
The correlations between metrics are crucial for defect prediction. Two characteristics of the convolution layer, sparse connectivity and shared weights, make it particularly suitable for this task \cite{30}. Sparse connectivity enables CNNs to have a wider receptive field, capturing non-linear correlations between metrics. Shared weights ensure that each CNN kernel has the same parameters, allowing predictors to capture these correlations regardless of their position in the input metrics. The max pooling layer retains the most significant features by selecting the maximum local element in the convolutional output, enhancing the model’s robustness to changes in the position of metrics in the input. The attention layer dynamically assigns weights to the inputs, providing a flexible way to capture correlations between feature maps. The fully connected neural network is responsible for converting the feature maps generated by previous layers into a logit. The softmax block is positioned at the end of the entire neural network, transforming the logits into the probabilities.
% To ensure the generality of our method, we chose a DL-based model, DP-CNN \cite{30} as the predictor. It consists of three main components: the convolution layer, max pooling layer, and classification layer. To further improve accuracy, we added an attention layer to the original model. The overall architecture of DP-CNN is presented in Fig.~\ref{fig2}.

% \begin{figure}[!tb]
% \centerline{\includegraphics[width=\columnwidth]{images/DP-CNN_new.png}}
% \caption{Model Architecture of DP-CNN.}
% \label{fig2}
% \end{figure}

% The correlations between metrics are crucial for defect prediction. The two characteristics of convolutional layer, sparse connectivity and shared weights are particularly suitable for this task \cite{30}. Sparse connectivity enables CNNs to have a wider receptive field, capturing non-linear correlations between metrics. Shared weights ensure that each CNN kernel has the same parameters, allowing predictors to capture these correlations regardless of their position in the input metrics.

% Given input matrix $ X \in R^{M X N}$follows.the output $\mathcal{H}$ of the convolution layer can be formalized asfollows.
% \begin{equation}
% \mathcal{H} = \sigma((W * X) + b)
% \end{equation}
% where $W$ and $b$ are the learnable weights and bias of the kernel, $*$ is the convolution operator, and the $\sigma(\cdot)$ is the activation function.

% One limitation of the convolution operation is its sensitivity to positional variations in the input metrics. The max-pooling layer addresses this issue by performing a down-sampling operation that selects the maximum value within each region of the convolutional output. This process not only reduces the spatial dimensions of the feature map but also filters out less significant features, retaining only the most prominent ones. By focusing on these key features, max pooling effectively suppresses noise and irrelevant information, enhancing the model's robustness to positional variations in the input metrics and improving its ability to generalize.

% The attention layer dynamically assigns weights to the inputs, highlighting the most relevant features while reducing the influence of less important ones. This flexible mechanism enables the model to better capture correlations between feature maps by focusing on the key patterns that contribute most to defect prediction. The fully connected neural network is responsible for converting the feature maps generated by previous layers into a logit. The softmax block is positioned at the end of the entire neural network, transforming the logits into the probabilities.


\subsubsection{Interpretation model}
Following the KD principle, we employ the soft decision tree (SDT) as the interpreter \cite{47}. The SDT is a variant of the fuzzy decision tree, sharing a similar structure with the ordinary decision tree but differing in the definition of nodes.
% Unlike decision trees, which select features based on information gain or Gini index, the SDT selects features by assigning weights to each feature.
In SDT, nodes are defined as neurons with learnable weight $W$ and bias $b$. Weights reflect the importance of each feature in the decision-making process at the node. The output of each node is defined as $P_i(x_i) = \sigma(x_iW_i + b_i)$, which determines the probability of transitioning to the right subtree. The $\sigma(\cdot)$ denotes the activation function and $x_i$ is the input of the $i$-th node. The output at the $l$-th leaf is defined as:
\begin{equation}
     Q_k^l=\frac{{\rm exp}(\phi_k^l)}{\sum_k {\rm exp}(\phi_k^l)}
\label{eq1}     
\end{equation}      
where $Q_k^l$ is the probability at the $l$-th leaf of type $k$ defect, and $\phi_k^l$ is the learned feature at that leaf.     

The reasons we use SDT as an interpreter are as follows:
Owing to its neural-like node architecture, SDT exhibits comparable feature engineering capabilities to CNN, enabling it to accurately retain the decision-making knowledge of CNN. Furthermore, in comparison to CNN, the complexity of SDT has been reduced, rendering the decision-making process more comprehensible.


\subsection{Joint learning framework}
% The architecture of the joint learning framework is presented in Fig.~\ref{fig3}, each feedback loop corresponds to a specific loss function. \textcolor{blue}{Training stops when the model reaches the maximum number of iterations or triggers an early stopping setting.} Given training data set $D=\{\left(x_i,y_i\right)_{i=1}^N\}$, $x_i\in R^n$, $y_i\in \{0,1\}$, the predictor $f$ and interpreter $g$, the loss function of joint learning framework is formalized as follows: 
The architecture of the joint learning framework is presented in Fig.~\ref{fig3}, each feedback loop corresponds to a specific loss function. \textcolor{orange}{Training stops when the model reaches the maximum number of iterations or meets the early stopping condition.} Given training data set $D=\{\left(x_i,y_i\right)_{i=1}^N\}$, $x_i\in R^n$, $y_i\in \{0,1\}$, the predictor $f$ and interpreter $g$, the loss function of joint learning framework is formalized as follows:
\begin{figure}[tb]
\centering
\includegraphics[width=\columnwidth]{images/joint_learning_model_new.png}
\caption{The architecture of joint learning framework.}
\label{fig3}
\end{figure}

\begin{equation}
    L=   \alpha L_{pred}(f, D) + \beta  L_{pred}(g, D)+ L_{int}(f,g,D)
\label{eq2}
\end{equation}
where $\alpha$ and $\beta$ are hyperparameters. 

The original loss function of KD consists of two essential parts: the distillation loss $L_{soft}$ (which is equivalent to $L_{int}(f,g,D)$ in \textcolor{blue}{Eq. \eqref{eq2})} and the student loss $L_{hard}$ (which is equivalent to $L_{pred}(g,D)$ in \textcolor{blue}{Eq. \eqref{eq2})}). $L_{soft}$ quantifies the difference between the teacher model and the student model, which serves as a feedback loop for transferring knowledge from the teacher model to the student model. Minimizing $L_{soft}$ will enforce the student model to adopt similar decision logic to the teacher model.
$L_{hard}$ evaluates the difference between the student model’s predictions and the ground truth. Minimizing $L_{hard}$ ensures that the student model retains the appropriate decision logic of the teacher model.
In this paper, we introduce two key modifications to the existing loss function: (1) added the output fidelity loss $L_{pred}(f, D)$, and (2) revised the $L_{soft}$.

$L_{pred}(f, D)$ shares a similar definition with $L_{pred}(g, D)$ indicating the consistency of the predictor's output to the ground truth.
\begin{equation}
    L_{pred}(f, D)=\sum_{i=1}^{|D|}\left(f(x_i)-y_i\right)^2
\label{eq4}
\end{equation}
\begin{equation}
    L_{pred}(g, D)=\sum_{i=1}^{|D|}\left(g(x_i)-y_i\right)^2
\label{eq5}
\end{equation}
We introduce output fidelity loss guided by two critical considerations: (1) Optimizing the predictor and interpreter simultaneously. Unlike the original KD framework, which keeps the parameters of teacher model (predictor) fixed during the distillation process, minimizing the output fidelity loss will enforce the predictor to capture the discriminative features of the input metrics.
(2) Improving the fidelity of the interpreter. Within the KD framework, predictor and interpreter are mutually dependent. Improving the accuracy of the predictor is equivalent to indirectly enforcing the interpreter to provide a more accurate explanation of the predictor's decision logic.
Therefore, introducing the output fidelity loss can boost the performance of the predictor and provide a more robust and insightful knowledge based on the KD principle.

Moreover, we revised the loss function $L_{soft}$ (denoted as $L_{int}(f,g, D)$ in \textcolor{blue}{Eq. \eqref{eq2}}). According to \textcolor{blue}{Eq. \eqref{eq_5}}, the $L_{int}(f,g,D)$ involves two parts, $L_{of}(f,g,D)$ and $L_{fef}(f,g,D)$. 
$L_{of}(f,g,D)$ is equivalent to the distillation loss in the original KD framework. The feature fidelity loss $L_{fef}(f,g,D)$ evaluates the similarity between the feature maps generated by predictor and interpreter.
Minimizing $L_{of}(f,g,D)$ and $L_{fef}(f,g,D)$ can enforce the interpreter to adopt similar decision logic and feature engineering power of predictor.
\begin{equation}
L_{int}(f,g,D) = \lambda L_{of}(f,g,D) + \gamma L_{fef}(f,g,D) 
\label{eq_5}
\end{equation}
where $\lambda$ and $\gamma$ are hyperparameters.
$L_{of}(f,g,D)$ shares a similar definition with $L_{soft}(f,g,D)$, which ensures the output of $g$ should be similar to that of $f$. 
\begin{equation}
L_{of}(f,g,T) = -\sum_{i=1}^{|D|} q_i^T \log(p_i^T)
\label{eq6}
\end{equation}
where $ q_i^T = \frac{{\rm exp}(\nicefrac{z_i}{T})}{\sum_{k}^{N} {\rm exp}(\nicefrac{z_k}{T})}$ and $p_i^T = \frac{{\rm exp}(\nicefrac{v_i}{T})}{\sum_{k}^{N} {\rm exp}(\nicefrac{v_k}{T})}$ are the classification probability of instance $x_i$ generated by $g$ and $f$, $z_i$ and $v_i$ are the logits of $x_i$. $T$ is the temperature hyperparameter \cite{47}. 
% By aligning the outputs of $g$ and $f$, the interpreter can inherent the decision logic of the predictor, which is critical for enhancing the fidelity of the interpreter.
Furthermore, the feature  fidelity loss $L_{fef}$ is defined as follows:
\begin{equation}
L_{fef} = \sum_{i}^{|D|} \left(
d(z_i)-v_i
\right)^2
\label{eq7}
\end{equation}
where function $d(\cdot)$ adjusts $z_i$ to the same dimension of $v_i$.

\textcolor{blue}{To further clarify the proposed joint learning framework, we take the PC2 dataset as an example. The data in PC2 involves software metrics and labels. Software metrics serve as the input for both the predictor $f$ and interpreter $g$. Labels indicate whether a code fragment is buggy or clean. The predictor and interpreter were optimized jointly using the loss function defined in Eq. \eqref{eq2}. Through the joint training process, the predictor was continuously refined to improve its classification accuracy. This process enables the predictor to effectively distinguish between buggy and clean code fragments. Simultaneously, the interpreter was trained to generate a decision tree that aligns with the predictor's decision-making logic. Through this process, the predictor was also influenced by the interpreter, as minimizing $L_{int}(f,g,D)$ required the predictor to refine its feature extraction and decision logic to better align with the explanations provided by the interpreter. In Fig.~\ref{PC2_tree}, the decision tree generated by the interpreter reflects the optimized decision logic and highlights key decision paths for defect prediction.}
\begin{figure}[t!]\color{blue}
    \centering
    \includegraphics[width=\columnwidth]{images/PC2_tree.pdf}
    \label{fig5:our_method}
    \caption{The interpretations of the proposed method on PC2 dataset.}
    \label{PC2_tree}
\end{figure}


% \textcolor{blue}{To facilitate a better understanding of our framework, we present a concrete example to demonstrate its application. Consider a software program characterized by various software metrics, such as lines of code (LOC), cyclomatic complexity, and code churn. The predictor uses these metrics to predict whether the program contains defects, while the interpreter generates a set of ``IF-THEN'' rules to explain the decision-making process behind the prediction. Suppose the model predicts that the software program is buggy or clean. The interpreter then provides an explanation, helping developers understand which metrics (e.g., LOC, cyclomatic complexity, etc.) played a key role in the prediction. The interpreter, through a series of ``IF-THEN'' rules, reveals how these software metrics influence the model's decision. These explanations help developers focus on the critical metrics affecting the prediction, thereby gaining a deeper understanding of the underlying causes of defects.}

\subsection{Local interpretation}
It is obvious that SDT obtains the global interpretability as decision tree does \cite{sivaprasad2023evaluation}, and interpretation for a dataset can be abstracted as a set of ``IF-THEN'' rules.
In this section, we focus on the interpretability for an instance. 
% Given a training dataset $D$, 
Given a metric set $A=\{a_1,a_2,...a_d\}$, the sensitivity of SDT for an instance $x$ on the $a_i$ is defined as follows.
\begin{equation}
S(a_i) = \frac{\Delta g_i(x)}{\Delta a_i}
\label{eq8}
\end{equation}
$\Delta g_i(x) = \lvert g(x) - g(x + {\rm One}(a_i)\Delta a_i) \rvert$
is the changes of the output of interpreter aroused by the perturbation of the metric $a_i$. $\Delta a_i$ is the standard variance of $a_i$. ${\rm One}(a_i)$ is the onehot encoder of $i$-the metric. 
\begin{equation}
\Delta a_i = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (a_i - \bar{a})^2}
\label{eq9}
\end{equation}
\begin{equation}
\bar{a}_i = \frac{1}{N} \sum_{j}^{N} a_{j}^{i} 
\label{eq10}
\end{equation}
where $N$ is the number of training data, and $a_{j}^{i}$ is the $i$-the metric of $x_j$.
According to \textcolor{blue}{Eq. \eqref{eq8}}, we interpret the correlation between the instance and predictive result as the sensitivity of feature perturbation. Given an instance $x_i$, if a metric has a greater sensitivity, that means such a metric has a stronger influence on the prediction result.

Since the $\Delta a_i$ is a constant and the parameters of interpreter are fixed after training, the sensitivity $S$ can provide a stable interpretation for any instance. The algorithm for local interpretation is presented below:

    \begin{algorithm}[htbp]
        \caption{Local Interpretation}
        \begin{algorithmic}[1]
            \REQUIRE interpreter $g$, instance $x$, metric set $A=\{a_1, a_2, \dots, a_d\}$, and  hyperparameters: $\lambda, \gamma$.
            \ENSURE Sensitivity index $SI$
            % \STATE {Set DP-CNN as $f$}
            % \STATE {Set SDT as $g$}
            % \STATE \algorithmicfor\ $e \in E$ \algorithmicdo\
            %     \STATE  \quad $\text{Joint training $f$ and $g$ under the constrain of} \ L$ \hfill \eqref{eq2}
            % \STATE \algorithmicend\ \algorithmicfor\ 
            \STATE \algorithmicfor\ $a_i \in A$ \algorithmicdo\
                \STATE \quad $\text{Calculate }\bar{a}_i$ \hfill \textcolor{blue}{Eq. \eqref{eq10}}
                \STATE \quad $\text{Calculate } \Delta a_i $ \hfill \textcolor{blue}{Eq. \eqref{eq9}}
                \STATE \quad $\text{Calculate }\Delta g_i(x)$
                \STATE \quad $S =  \cup \ S(a_i)$  \hfill \textcolor{blue}{Eq. \eqref{eq8}}
            % \STATE {set $r(t)=x(t)$}  
            \STATE \algorithmicend\ \algorithmicfor\ 
            \STATE $SI \leftarrow$ \text{ Sorts $S$ in descending order}
            \STATE \textbf{return} $SI$
        \end{algorithmic}
    \end{algorithm}


The loop structure (lines 2 to 5 in the algorithm) calculates the sensitivity of each metric and
generates the sensitivity set $S$. Line 7 rearranges the sensitivity set $S$ in descending order and puts the index of $S$ to $SI$.

% ---------------------------------实验—————————————————————————————————
\section{Experiment}\label{sec4}
To objectively evaluate the performance of our method, we conducted an experiment to quantify the performance of the proposed method and the baselines on \textcolor{blue}{20} datasets, aiming to answer the following research questions:

% \textbf{RQ1:} What reliability-related improvements does the proposed method make?
\textcolor{blue}{\textbf{RQ1:} What reliability-related improvements does the proposed method make in terms of consistency and faithfulness?}

As mentioned in \textcolor{orange}{Section II}, many \textit{local} methods often yield inconsistent interpretations by adopting different data sampling techniques as well as interpreting the same code fragment multiple times. These two factors greatly affect the reliability of explainable SDP. To evaluate the consistency of interpretation, we adopt the Coincidence Degree (CD) in RQ1 \cite{parekh2021framework}. Given an interpreter $g$, the CD of an instance $x_i$ is defined as follows :
\begin{align}
    CD(x_i) =  \frac{1}{|M|} \left| \bigcap_{j=1}^{|M|} g_j(x_i) \right| \label{eq13}
    % CD = \frac{1}{|D|} \sum_{(x_i,y_i) \in D} CD(x_i)  \label{eq14}
\end{align}
where $\bigcap_{j=1}^{|M|}{g_j(x_i)}$ is the intersection of the multiple interpretations for one instance, $|M|$ represents the number of interpretations or the number of data sampling techniques used. $CD-k\%$ indicates the coincidence of the top $k$ metrics. A greater $CD$ indicates greater consistency.

Furthermore, as discussed in \textcolor{orange}{Section II}, another issue related to the reliability of interpretation is that predictor and interpreter may generate different predictive results for the same instance. To evaluate the consistency between predictor and interpreter, we introduce Fidelity of Interpretation (FI) \cite{parekh2021framework,lakkaraju2020robust,bang2021explaining} here. Given dataset $D$, predictor $f$ and interpreter $g$, the fidelity is defined as:

\begin{equation}
    FI=\frac{1}{|D|}\sum_{(x_i,y_i)\in D} \mathbb{I}\left(f(x_i),g(x_i)\right)
\label{eq12}
\end{equation}
$ \mathbb{I}(\cdot)$ is an indicator function, which generates 1 when $f(x_i)=g(x_i)$, 0 otherwise.

In addition to the FI, we argue that the prediction produced by an interpreter should also align with the ground truth. This alignment indicates how much correct decision knowledge is obtained from the predictive model.
In this paper, we use the Accuracy of Interpreter (AI) as another indicator to measure this alignment. Given the dataset $D$ and interpreter $g$, we define the accuracy as:
\begin{equation}
    AI=\frac{1}{|D|}\sum_{(x_i,y_i)\in D} \mathbb{I}\left(g(x_i),y_i\right)
    \label{eq111}
\end{equation}
where $\left|D\right|$ denotes the cardinality of $D$, $g\left(x_i\right) $ is the prediction made by the interpreter for the instance $x_i$.

\textbf{RQ2:} What predictive accuracy-related improvements does the proposed method make?

There are two goals in validating the predictor trained by our framework (denoted as Joint\_CNN): 
% (1) The joint learning framework has a positive impact on the accuracy of predictor, 
\textcolor{orange}{(1) The joint learning framework has had a positive impact on the multiple evaluation metrics of the predictor,} and (2) its predictive performance is comparable to other notable predictors. 
% For the first goal, we compare the accuracy of Joint\_CNN with the DP-CNN trained not by our framework (denoted as Base\_CNN). 
\textcolor{orange}{For the first goal, we compare the F-measure and AUC of Joint\_CNN with the DP-CNN trained not by our framework (denoted as Base\_CNN). In addition, we incorporate three additional metrics from \cite{sharma2023}: Percent of Perfect Cleans (PPC), Percent of Non-Perfect Cleans (PNPC), and False Omission Rate (FOR). These indices offer valuable insights into the framework's ability to balance classification accuracy with practical project management benefits, such as budget savings, remaining time estimates, and failure rate predictions. }
For the second goal, we compare the F-measure and AUC of Joint\_CNN with SVM, Random Forest (RF), and Deep Belief Network (DBN) based predictors. \textcolor{blue}{Additionally, although SVM, RF, and DBN are commonly used for classification tasks, they are not interpretable prediction models. To better demonstrate the effectiveness and advantages of our model, we also included AutoSpearman \cite{18}, a widely recognized interpretable prediction model, in the comparison.}
% \textcolor{blue}{Additionally, we conducted a focused comparison between Joint\_CNN and AutoSpearman \cite{18}, a representative interpretable prediction model.}

\textcolor{orange}{The Percent of Pure Cleanings (PPC) measure is defined in \cite{sharma2023} as the percentage of correctly predicted negative cases, which indicates the amount of saved budget in the target project. PPC is calculated as:
\begin{equation}
    PPC = \frac{\left| TN \right|}{\left| nt \right|} 
    \label{eq_PPC}
\end{equation}
where TN represents the number of true negatives and $\left| nt \right|$ denotes the total number of test instances.
}

\textcolor{orange}{The Percent of Non-Perfect Cleans (PNPC) measure is defined in \cite{sharma2023} as the percentage representing the sum of true positives, false positives, and false negatives over the total number of instances tested. This measure is defined as follows:
\begin{equation}
    PNPC = \frac{\left| TP \right| + \left| FP \right| + \left| FN \right|}{\left| nt \right|}
    \label{eq_PNPC}
\end{equation}
where $\left| TP \right|$ represents the number of true positives, $\left| FP \right|$ denotes the number of false positives, and $\left| FN \right|$ indicates the number of false negatives.
}

\textcolor{orange}{The False Omission Rate (FOR) measure is defined in \cite{sharma2023} as the ratio of false negatives over the number of predicted cleans. This is used to estimate the percentage of potential failures that may occur in the target software. FOR is defined as:
\begin{equation}
    FOR = \frac{\left| FN \right|}{\left| TN \right| + \left| FN \right|}
    \label{eq_FOR}
\end{equation}
where $\left| FN \right|$ represents the number of false negatives, and $\left| TN \right| +  \left| FN \right|$ is the total number of predicted cleans.
}

\textcolor{blue}{The F-measure, also referred to as the F1-score, assesses the balance between precision and recall. It is the harmonic mean of these two values. The F-measure is defined as:}
\begin{equation}\color{blue}
    F\text{-measure} = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
    \label{eq_Fmeasure}
\end{equation}

\textcolor{blue}{The Area Under the Curve (AUC) is used to evaluate the performance of binary classification models by measuring the area under its Receiver Operating Characteristic (ROC) curve, which plots the trade-off between true positive rate and false positive rate across different thresholds. The AUC measures the effectiveness of a classification model in distinguishing between defective and non-defective instances. A higher AUC indicates better discrimination, where a value closer to 1 reflects excellent predictive performance.}
% Given the dataset \textit{D} and predictor \textit{f}, we define the accuracy as:
% \begin{equation}
%     Acc=\frac{1}{|D|}\sum_{(x_i,y_i)\in D} \mathbb{I}\left(f(x_i),y_i\right)
%     \label{eq11}
% \end{equation}
% where $\left|D\right|$ denotes the cardinality of $D$, $f\left(x_i\right) $ is the prediction for the instance $x_i$.
% , and $\mathbb{I}(.)$ is an identity function where outputs 1 if $P(x_i)=y_i$ and 0 otherwise.

% \textbf{RQ2:} What fidelity-related improvements does the proposed method make?

% Existing researches use the portion of instances where the predictor and the interpreter reach the same predictive results to measure the fidelity of interpreter \cite{parekh2021framework,lakkaraju2020robust,bang2021explaining}. Given dataset $D$, predictor $P$ and interpreter $I$, the fidelity is defined as:

% \begin{equation}
%     FI=\frac{1}{|D|}\sum_{(x_i,y_i)\in D} \mathbb{I}\left(P(x_i),I(x_i)\right)
% \label{eq12}
% \end{equation}
% where $I\left(x_i\right)$ is the prediction of the instance $x_i$.

% In addition to the $FI$, we argue that the prediction produced by an interpreter should also align with the ground truth. This alignment not only enables the interpreter to have similar decision logic as the predictor but also provides meaningful insights from the dataset. Based on the belief above, we use the accuracy of the interpreter as another indicator to measure fidelity.


% \textbf{RQ3:}What stability-related improvements does the proposed method make?

% We assess stability by measuring the consistency of the ranking of metrics across multiple interpretations. Given the set of metrics $A$ and the interpreter $I$, the definition of stability is as follows \cite{parekh2021framework} :
% \begin{align}
%     CD(x_i) =  \frac{1}{|A|} \left| \bigcap_{i=1}^{|A|} I_i(x_i) \right| \label{eq13}\\
%     CD = \frac{1}{|D|} \sum_{(x_i,y_i) \in D} CD(x_i)  \label{eq14}
% \end{align}
% \textcolor{red}{where} $\bigcap_{i=1}^{|A|}{I_i(x_i)}$ is the intersection of the multiple interpretations for one instance, $|\cdot|$ is the cardinality of a set. $CD-k\%$ indicates the coincidence of the top $k$ metrics. The greater $CD$ means the greater the stability.


\textbf{RQ3:} What is the global interpretability of the proposed method?

% \textcolor{blue}{The objectives of RQ3 are threefold: (1) to demonstrate that our approach obtains global interpretability, (2) to verify its superior computational efficiency, and (3) to demonstrate its reliability. Although local interpretation methods explain the decision logic for a single instance, they fail to reveal the complete structure of predictors \cite{60}. Global interpretable methods can fill in the gaps left by local interpretable methods. 
% For the first objective, we aim to demonstrate our approach is a decision tree-like interpretable method. 
% The substantial computational expense is a primary disadvantage of current global interpretation methods \cite{61}. For the second objective, we want to quantify the computational efficiency of our method and SP-Lime \cite{lime} by comparing the training time of them. The rationale for using SP-Lime as the baseline is as follows. First, SP-Lime is a representative example of perturbation-based global interpretable methods and most global explainable methods are based on the perturbation algorithm \cite{60}. Second, SP-Lime is the global version of Lime (one of baselines of RQ1). Selecting it as baseline is helpful to maintain the consistency of the experiment.}
\textcolor{blue}{The objectives of RQ3 are threefold: (1) to demonstrate that our approach obtains global interpretability, (2) to verify its superior computational efficiency, and (3) to demonstrate its reliability. Although local interpretation methods explain the decision logic for a single instance, they fail to reveal the complete structure of predictors \cite{60}. Global interpretable methods can fill in the gaps left by local interpretable methods. 
For the first objective, we aim to demonstrate our approach is a decision tree-like interpretable method. 
The substantial computational expense is a primary disadvantage of current global interpretation methods \cite{61}. For the second objective, we want to quantify the computational efficiency of our method and SP-Lime \textcolor{orange}{\cite{60}} by comparing the training time of them. The rationale for using SP-Lime as the baseline is as follows. First, SP-Lime is a representative example of perturbation-based global interpretable methods and most global explainable methods are based on the perturbation algorithm \cite{60}. Second, SP-Lime is the global version of Lime (one of baselines of RQ1). Selecting it as baseline is helpful to maintain the consistency of the experiment.}

% \textbf{RQ4:} What are the differences in rationality between our approach and the baselines?
% To explore the rationality of interpretation, we carried out an ablation experiment. Specifically, we take Join\_CNN as the predictor and eliminated metrics individually. The impacts on the predictor was evaluated by observing changes in the F1 score, AUC, and MCC. A metric is considered to have a significant impact on the predictor if the F1 score, ROC, AUC, and MCC exhibit substantial changes when the metric is removed. If the performance degradation results align with the interpretation results, we consider the interpretation rational.  

\textcolor{blue}{\textbf{RQ4:} What is the effect of different features on the performance and interpretability of our approach?  \\
\indent To assess the impact of individual features on the model's performance and interpretability, we carried out an ablation experiment. Specifically, we took Joint\_CNN as the predictor and eliminated metrics individually. The impacts on the predictor was evaluated by observing changes in the F-measure, AUC, and MCC, and compared these results with the model's interpretability. MCC is an acronym for Matthews Correlation Coefficient that is used to measure the quality of binary classifications, particularly in imbalanced datasets. The MCC is defined as follows:}
\begin{equation}\color{blue}
    MCC = \frac{TP \cdot TN - FP \cdot FN}{\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}
    \label{eq_Mcc}
\end{equation}
\textcolor{blue}{where $TP$, $TN$, $FP$ and $FN$ represent the number of true positives, true negatives, false positives, and false negatives, respectively. The MCC value ranges from -1 to 1, where -1 indicates complete disagreement between predictions and actual outcomes, 1 represents perfect agreement, and 0 implies performance equivalent to random guessing.}

\subsection{Dataset}
The \textcolor{blue}{20} datasets used in the experiment were collected from the AEEEM \footnote{ https://zenodo.org/record/3362613\#.YmuGoNpByUk}, NASA \footnote{http://promise.site.uottawa.ca/SERepository/datasets-page.html} and \textcolor{blue}{PROMISE} \footnote{http://promise.site.uottawa.ca/SERepository} repositories. The effectiveness of these datasets has been demonstrated by numerous studies \cite{51, 52, 53, 54}. Further details about each dataset are provided in Table \ref{tab2}. We observe that these datasets exhibit significant class imbalance, with defective instances accounting for only 0.41\% of the PC2 dataset.


% \begin{table}[tb]
% \centering
% \caption{Details of the datasets used in the experiment}
% \begin{tabular}{ccccc}
% \toprule
% % Group & Dataset & \multicolumn{1}{l}{ of instances} & &\multicolumn{1}{l}{ of metric} \\
% % \cline{3-5}
% %  &  & \multicolumn{1}{c}{All} & \multicolumn{1}{c}{Buggy} \\
% Group & Dataset & \multicolumn{1}{l}{Number of Instances} & Buggy    &\multicolumn{1}{l}{Number of Metrics} \\
% \midrule
% AEEEM       & JDT & 997  & 206(20.66\%) &    21 \\
%             & LC  & 691  &  64(9.26\%)  &    21 \\
%             & ML  & 1862 & 245(13.16\%) &    21 \\
%             & PDE & 1492 & 209(14.01\%) &    21 \\
% \midrule
% NASA        & kc1 & 2109&  326(15.46\%) &    21 \\
%             & mc1 & 9466&  68(0.72\%)   &    38 \\
%             & jm1 & 7782&  1672(21.49\%) &   21 \\
%             & PC1 & 549 &   77(14.03\%) &    21 \\
%             & PC2 & 5589&    23(0.41\%) &    36 \\
%             & PC3 & 1563&  160(10.24\%) &    37 \\
%             & PC4 & 1458&  178(12.21\%) &    37 \\
% \midrule      
% PROMISE     & ant-1.7       & 745&  166(22.28\%) &    20 \\
%             & camel-1.6     & 965&  188(19.48\%) &    20 \\
%             & ivy-1.2       & 352&  40(11.36\%)  &    20 \\
%             & jedit-4.1     & 312&  79(25.32\%)  &    20 \\
%             & log4j-1.0     & 135&  34(25.19\%)  &    20 \\
%             & lucene-2.4    & 340&  203(59.71\%) &    20 \\
%             & poi-3.0       & 442&  281(63.57\%) &    20 \\
%             & synapse1.2    & 256&  86(33.59\%)  &    20 \\
%             & xerces-1.3    & 453&  69(15.23\%)  &    20 \\
% \bottomrule
% \end{tabular}
% \label{tab2}
% \end{table}
\begin{table}[tb]
\centering
\caption{Details of the datasets used in the experiment}
\begin{tabular}{lllll}
\toprule
Group & Dataset & \textcolor{blue}{Instances} & \textcolor{blue}{Defects}    & \textcolor{blue}{\%Defects} \\
\midrule
AEEEM       & JDT           & 997       & 206        &    20.66\% \\
            & LC            & 691       & 64         &    9.26\% \\
            & ML            & 1862      & 245        &    13.16\% \\
            & PDE           & 1492      & 209        &    14.01\% \\
\midrule
NASA        & kc1           & 2109      & 326        &    15.46\% \\
            & mc1           & 9466      & 68         &    0.72\% \\
            & jm1           & 7782      & 1672       &    21.49\% \\
            & PC1           & 549       & 77         &    14.03\% \\
            & PC2           & 5589      & 23         &    0.41\% \\
            & PC3           & 1563      & 160        &    10.24\% \\
            & PC4           & 1458      & 178        &    12.21\% \\
\midrule 
\textcolor{blue}{PROMISE}     & \textcolor{blue}{ant-1.7}       & \textcolor{blue}{745}       & \textcolor{blue}{166}        &    \textcolor{blue}{22.28\%} \\
            & \textcolor{blue}{camel-1.6}     & \textcolor{blue}{965}       & \textcolor{blue}{188}        &    \textcolor{blue}{19.48\%} \\
            & \textcolor{blue}{ivy-1.2}       & \textcolor{blue}{352}       & \textcolor{blue}{40}         &    \textcolor{blue}{11.36\%} \\
            & \textcolor{blue}{jedit-4.1}     & \textcolor{blue}{312}       & \textcolor{blue}{79}         &    \textcolor{blue}{25.32\%} \\
            & \textcolor{blue}{log4j-1.0}     & \textcolor{blue}{135}       & \textcolor{blue}{34}         &    \textcolor{blue}{25.19\%} \\
            & \textcolor{blue}{lucene-2.4}    & \textcolor{blue}{340}       & \textcolor{blue}{203}        &    \textcolor{blue}{59.71\%} \\
            & \textcolor{blue}{poi-3.0}       & \textcolor{blue}{442}       & \textcolor{blue}{281}        &    \textcolor{blue}{63.57\%} \\
            & \textcolor{blue}{synapse1.2}    & \textcolor{blue}{256}       & \textcolor{blue}{86}         &    \textcolor{blue}{33.59\%} \\
            & \textcolor{blue}{xerces-1.3}    & \textcolor{blue}{453}       & \textcolor{blue}{69}         &    \textcolor{blue}{15.23\%} \\ 
\bottomrule
\end{tabular}
\label{tab2}
\end{table}

\subsection{Setting}
In the experiment, some parameters were set as follows: the initial learning rate $lr$ = 1e-06 and the batch size  $batch\_size$ is set to 16. The depth of SDT $tree\_depth$ is set to 4. The penalty strength of SDT $penalty\_strength$ is set to 1e+1 and the penalty decay rate $penalty\_decay$ is set to 0.25. The hyperparameters $\gamma$, $\lambda$, $\alpha$ and $\beta$ are set to 0.8, 0.5, 1.4 and 0.6 respectively. The temperature hyperparameter \textit{T} is set to 100. The window size for the Exponential Moving Average (EMA) $ema\_win\_size$ is set to  1000. 
% \textcolor{blue}{We carefully considered the characteristics of the dataset and conducted preliminary experiments to fine-tune certain parameters, ensuring a fair comparison across all methods.} The dataset was split into 7:2:1 for training, testing, and validation. The experiments are implemented by TensorFlow 2.3.0 with the NVIDIA GeForce RTX 3090 GPU.
\textcolor{orange}{We applied the same set of parameters across all datasets to ensure a fair comparison.} The dataset was split into 7:2:1 for training, testing, and validation. The experiments are implemented by TensorFlow 2.3.0 with the NVIDIA GeForce RTX 3090 GPU.

\subsubsection{Baseline}
We utilize LIME \cite{lime} and BreakDown \cite{34} as baselines for RQ1 and RQ3. We selected these techniques due to the following reasons: (1) These two methods are extensively employed by the SDP research community; (2) LIME, BreakDown, and our approach belong to feature-based interpretation techniques that strive to quantify the correlations between input features and predictive outputs; (3) The theoretical foundations of LIME and BreakDown differ substantially, thereby enriching the diversity of comparisons. LIME approximates the behavior of predictors in a local context, whereas BreakDown calculates the cumulative contribution of each feature to the prediction result.

\subsubsection{Data preprocessing}
We leverage normalization and different data sampling techniques to process the imbalanced data. Specifically, normalization is defined as follows:
\begin{equation}
    \widetilde{x}=\frac{x-\min{\left(D\right)}}{\max{\left(D\right)}-\min{\left(D\right)}}
\end{equation}
% where $x$ is the sample of dataset, and ${\rm min}(\cdot), {\rm max}(\cdot)$ are the minimum and maximum operations of the dataset. We use SMOTE (Synthetic Minority Oversampling Technique) here to alleviate the imbalance of the dataset, and the technical details could refer to \cite{55}.
where $x$ is the sample of a dataset, and ${\rm min}(\cdot), {\rm max}(\cdot)$ are the minimum and maximum operations applied column-wise across the dataset. \textcolor{blue}{The features in the dataset exhibit significant differences in value range. Without normalization, features with larger numerical ranges might unfairly influence the model. Normalization ensures that all features are on the same scale, so the model treats them equally in terms of their impact. Moreover, normalization helps the algorithm converge faster and more smoothly. As shown in the Table \ref{tab:data_preprocess}, normalizing the data significantly improves the model's performance compared to using the raw features.} We adopt two sampling techniques to the dataset: SMOTE (Synthetic Minority Oversampling Technique) and RUS (Random Undersampling). The technical details of SMOTE and RUS can be found in \cite{55} and \cite{RUS}. \textcolor{blue}{These techniques address class imbalance in defect prediction tasks but may also affect model interpretability. SMOTE balances the dataset by generating synthetic samples, which may subtly alter the distribution of dataset and distort the contribution of feature to the prediction results. In contrast, RUS reduces the dataset size by randomly removing majority-class samples, potentially simplifying the decision process but at the risk of discarding important information, which may affect the stability of the generated explanations.}

% \begin{table}[!tb]
% \centering
% \caption{Average Performance of the Models with and without Data Preprocessing Across All Datasets}
% \begin{tabular}{lll}
% \toprule
% & F-measure &AUC   \\
% \midrule
% Raw Data              & 0.5822  & 0.6041    \\
% Preprocessed Data     & 0.6704  & 0.7109   \\
% % \midrule
% % Improvement & 15.15\%\textcolor{blue}{\textuparrow} & 17.68 \%\textcolor{blue}{\textuparrow}  \\
% \bottomrule
% \end{tabular}
% \label{tab:data_preprocess}
% \end{table}
\begin{table}[!tb]\color{blue}
\centering
\caption{Average Performance of the Models with and without Data Preprocessing Across All Datasets}
\begin{tabular}{lll}
\toprule
& F-measure &AUC   \\
\midrule
Raw Data              & 0.5822  & 0.6041    \\
Preprocessed Data     & 0.6704  & 0.7109   \\
\bottomrule
\end{tabular}
\label{tab:data_preprocess}
\end{table}


%-============有关联合模型的计算复杂度描述===============
% \subsubsection{\textcolor{red}{computational complexity associated with the joint learning framework}}
% The computational complexity of the joint learning framework arises from training both the predictor and interpreter models simultaneously. For the predictor, convolutional layers, attention mechanisms, and fully connected layers contribute to the majority of the cost, with complexity influenced by the depth, number of filters, and attention layers. The interpreter has lower complexity, determined primarily by hyperparameters such as depth and regularization strength, which affect the tree structure and decision path construction. Both components experience linear growth in complexity as dataset and batch sizes increase. To balance efficiency and accuracy, hyperparameters were carefully tuned, enhancing the framework's overall performance.

\subsubsection{\textcolor{blue}{Statistical significance test}}
\textcolor{blue}{To verify the significance of the improvements achieved by our approach, we introduced $Cohen's\ d$ in this section. $Cohen's\ d$ is used to measure the difference between two groups, which can take any value between 0 and infinity. When $Cohen's\ d$ takes values in the ranges $[0,0.2), [0.2,0.8), and\ [0.8, +\infty)$, it indicates that the differences between the two groups are small, medium, and large \footnote{ https://en.wikipedia.org/wiki/Effect\_size}.
The definition of $Cohen's\ d$ is as follows:
\begin{equation}
    Cohen's\ d = \frac{M_1 - M_2}{\sqrt{(SD_1^2 + SD_2^2)/2}}
\end{equation}
where $M_1$ and $M_2$ represent the means of the two group data, and $SD_1$ and $SD_2$ represent their standard deviations.}
% Table \ref{tab7} presents the $Cohen's\ d$ values of the proposed method relative to the LIME and BreakDown for the two indicators $FI$ and $CD$-10\%. The results indicate that our approach exhibits a significant improvement compared to LIME. Since the $FI$ of BreakDown cannot be calculated, we are unable to provide the corresponding $Cohen's\ d$ values (denoted as `-' in the table). Additionally, the proposed method demonstrates a significant improvement in $CD$-10\% compared to BreakDown.
\begin{table}[!t]
\centering
\caption{The AI and FI of Joint\_SDT and LIME}
\begin{tabular}{lllll}
\toprule
& \multicolumn{2}{l}{$AI$} & \multicolumn{2}{l}{$FI$} \\
\cmidrule(ll){2-3} \cmidrule(ll){4-5}
& LIME & \textcolor{blue}{Joint\_SDT} & LIME  & \textcolor{blue}{Joint\_SDT} \\
\midrule
JDT       & 0.8200  & 0.8200           & 0.8100   & \textbf{0.9300}  \\
LC        & 0.8143  & \textbf{0.8429}  & 0.8000   & \textbf{0.8714}  \\
ML        & 0.6684  & \textbf{0.9037}  & 0.6791   & \textbf{0.9786}  \\
PDE       & 0.7200  & \textbf{0.7533}  & 0.6533   & \textbf{0.7867}  \\
jm1       & 0.6239  & \textbf{0.8010}  & 0.8793   & \textbf{0.9910}  \\
kc1       & 0.6540  & \textbf{0.8531}  & 0.5355   & \textbf{0.9431}  \\ 
mc1       & 0.8754  & \textbf{0.9916}  & 0.9155   & \textbf{0.9979}  \\
PC1       & 0.8727  & 0.8727           & 0.7091   & \textbf{0.9273}  \\
PC2       & 0.9821  & \textbf{0.9982}  & 0.9714   & \textbf{0.9803}  \\
PC3       & 0.7834  & \textbf{0.9172}  & 0.8471   & \textbf{0.9682}  \\
PC4       & 0.8151  & \textbf{0.9178}  & 0.9041   & \textbf{0.9932}  \\
\textcolor{blue}{ant-1.7}     & \textcolor{blue}{0.6667}  & \textbf{\textcolor{blue}{0.7467}}  & \textcolor{blue}{0.6667}  & \textbf{\textcolor{blue}{0.9200}}  \\
\textcolor{blue}{camel1.6}    & \textcolor{blue}{0.7010}  & \textcolor{blue}{0.5361}           & \textcolor{blue}{0.7010}  & \textbf{\textcolor{blue}{0.8144}}  \\
\textcolor{blue}{ivy-1.2}     & \textcolor{blue}{0.8056}  & \textbf{\textcolor{blue}{0.8333}}  & \textcolor{blue}{0.8056}  & \textbf{\textcolor{blue}{0.9167}}  \\
\textcolor{blue}{jedit-4.1}   & \textcolor{blue}{0.5312}  & \textbf{\textcolor{blue}{0.7500}}  & \textcolor{blue}{0.5312}  & \textbf{\textcolor{blue}{0.5938}}  \\
\textcolor{blue}{log4j-1.0}   & \textcolor{blue}{0.7857}  & \textcolor{blue}{0.6429}           & \textcolor{blue}{0.7857}  & \textcolor{blue}{0.5714}  \\
\textcolor{blue}{lucene-2.4}  & \textcolor{blue}{0.5000}  & \textbf{\textcolor{blue}{0.7059}}  & \textcolor{blue}{0.5000}  & \textbf{\textcolor{blue}{0.7059}}  \\
\textcolor{blue}{poi-3.0}     & \textcolor{blue}{0.4222}  & \textbf{\textcolor{blue}{0.7778}}  & \textcolor{blue}{0.4222}  & \textbf{\textcolor{blue}{0.8222}}  \\
\textcolor{blue}{synapse-1.2} & \textcolor{blue}{0.5769}  & \textbf{\textcolor{blue}{0.6538}}  & \textcolor{blue}{0.5769}  & \textcolor{blue}{0.5769}  \\
\textcolor{blue}{xerces-1.3}  & \textcolor{blue}{0.6522}  & \textbf{\textcolor{blue}{0.7174}}  & \textcolor{blue}{0.6522}  & \textbf{\textcolor{blue}{0.8478}}  \\
\midrule
\textcolor{orange}{Average}     & \textcolor{blue}{0.7135}  & \textbf{\textcolor{blue}{0.8018}}  & \textcolor{blue}{0.7173}  & \textbf{\textcolor{blue}{0.8568}}  \\
\midrule
\textcolor{blue}{Improvement} & \textcolor{blue}{12.38\%\textuparrow} & \textcolor{blue}{-} & \textcolor{blue}{19.45\%\textuparrow} & \textcolor{blue}{-} \\
\midrule
\textcolor{blue}{$Cohen's\ d$}& \textcolor{blue}{0.6792}  & \textcolor{blue}{-}      & \textcolor{blue}{0.9451}  & \textcolor{blue}{-}      \\
\bottomrule
\end{tabular}
\label{tab:lime_JointSDT}
\end{table}

\begin{table}[!t]\color{blue}
\centering
\caption{Average \textit{CD}s of LIME, BreakDown and proposed method}
\begin{tabular}{llll}
\toprule
        &LIME & BreakDown & Joint\_SDT \\
\midrule
\textcolor{orange}{Average}  &65\% &74.5\% &100\% \\
\midrule
Improvement &53.85\%\textcolor{blue}{\textuparrow} &34.23\%\textcolor{blue}{\textuparrow} &- \\
% \midrule
% $Cohen's\ d$    &5.2328 & 2.1205 &-\\
\bottomrule
\end{tabular}
\label{CDs}
\end{table}

\subsection{Experimental results}
The experimental results are presented as follows.
\subsubsection{The results of RQ1}
To evaluate the consistency of \textcolor{blue}{the interpreter trained by our framework (denoted as Joint\_SDT)}, LIME and BreakDown, we employed each of them to interpret an instance ten times, and calculate the average \textit{CD} for each method. As shown in Table \ref{CDs}, the average \textit{CD}-10\% of LIME, BreakDown and Joint\_SDT are \textcolor{blue}{65\%},  \textcolor{blue}{74.5\%}, and \textcolor{blue}{100\%}. This result suggests that LIME exhibits poor consistency.
To further investigate the consistency of LIME, we visualized its outputs in Fig.~\ref{fig4} and calculated the coefficient of variation (CV) for the weights assigned to different metrics across the ten interpretations. The results show that the CV is 42.18\%. The high CV suggests that LIME is unable to consistently assign similar weights to the same metric across multiple explanations. We believe this inconsistency is partly due to the sampling technique and the hyperparameters used by LIME.

\begin{figure*}[!t]
    \centering
    \subfloat[The first interpretation]{
        \includegraphics[width=0.35\textwidth, height=0.12\textheight]{images/lime_explanation_1.pdf}
        \label{fig4:Interpretation1}
    }
    \hspace{0.05\textwidth} % Space between the figures
    \subfloat[The second interpretation]{
        \includegraphics[width=0.35\textwidth, height=0.12\textheight]{images/lime_explanation_2.pdf}
        \label{fig4:Interpretation2}
    }

    \subfloat[The third interpretation]{
        \includegraphics[width=0.35\textwidth, height=0.12\textheight]{images/lime_explanation_3.pdf}
        \label{fig4:Interpretation3}
    }
    \hspace{0.05\textwidth} % Space between the figures
    \subfloat[The fourth interpretation]{
        \includegraphics[width=0.35\textwidth, height=0.12\textheight]{images/lime_explanation_4.pdf}
        \label{fig4:Interpretation4}
    }

    \subfloat[The fifth interpretation]{
        \includegraphics[width=0.35\textwidth, height=0.12\textheight]{images/lime_explanation_5.pdf}
        \label{fig4:Interpretation5}
    }
    \hspace{0.05\textwidth} % Space between the figures
    \subfloat[The sixth interpretation]{
        \includegraphics[width=0.35\textwidth, height=0.12\textheight]{images/lime_explanation_6.pdf}
        \label{fig4:Interpretation6}
    }
 
    \subfloat[The seventh interpretation]{
        \includegraphics[width=0.35\textwidth, height=0.12\textheight]{images/lime_explanation_7.pdf}
        \label{fig4:Interpretation7}
    }
    \hspace{0.05\textwidth} % Space between the figures
    \subfloat[The eighth interpretation]{
        \includegraphics[width=0.35\textwidth, height=0.12\textheight]{images/lime_explanation_8.pdf}
        \label{fig4:Interpretation8}
    }
 
    \subfloat[The ninth interpretation]{
        \includegraphics[width=0.35\textwidth, height=0.12\textheight]{images/lime_explanation_9.pdf}
        \label{fig4:Interpretation9}
    }
    \hspace{0.05\textwidth} % Space between the figures
    \subfloat[The tenth interpretation]{
        \includegraphics[width=0.35\textwidth, height=0.12\textheight]{images/lime_explanation_10.pdf}
        \label{fig4:Interpretation10}
    }
    % \caption{Multiple interpretations of LIME for a certain instance in the LC dataset.}
    \caption{\textcolor{blue}{Multiple interpretations of LIME for a certain instance in the PC2 dataset.}}
    \label{fig4}
\end{figure*}

% Furthermore, we conducted a comparison between the interpretations generated by LIME and \textcolor{red}{Join\_SDT} in terms of \textit{AI} and \textit{FI}. Since BreakDown does not provide an explicit surrogate function like LIME \cite{lime}, its \textit{AI} and \textit{FI} of BreakDown cannot be calculated. As shown in Table \ref{tab:lime_JointSDT}, the average \textit{AI} of \textcolor{red}{Join\_SDT} and LIME are 80.18\% and 71.35\% respectively and the average \textit{FI} of \textcolor{red}{Join\_SDT} and LIME are 85.68\% and 71.73\% respectively. The results indicate that the interpretations of \textcolor{red}{Join\_SDT} achieve much more consistency than LIME.
Furthermore, we conducted a comparison between the interpretations generated by LIME and Joint\_SDT in terms of \textit{AI} and \textit{FI}. Since BreakDown does not provide an explicit surrogate function like LIME \cite{lime}, its \textit{AI} and \textit{FI} of BreakDown cannot be calculated. As shown in Table \ref{tab:lime_JointSDT}, across all projects, \textcolor{blue}{Joint\_SDT} achieves an average improvement of 12.38\% in AI and 19.45\% in FI compared to LIME. \textcolor{blue}{The $Cohen's\ d$ results in the table further confirm that \textcolor{blue}{Joint\_SDT} outperforms LIME, with a large effect size in AI and a moderate effect size in FI}.

% \begin{table}[!tb]
% \centering
% \caption{The AI and FI of Joint\_SDT and LIME}
% \begin{tabular}{lllll}
% \toprule
% & \multicolumn{2}{l}{$AI$} & \multicolumn{2}{l}{$FI$} \\
% \cmidrule(ll){2-3} \cmidrule(ll){4-5}
% & LIME & Joint\_SDT & LIME  & Joint\_SDT \\
% \midrule
% JDT       & 0.8200  & 0.8200  & 0.8100   & 0.9300  \\
% LC        & 0.8143  & 0.8429  & 0.8000   & 0.8714  \\
% ML        & 0.6684  & 0.9037  & 0.6791   & 0.9786  \\
% PDE       & 0.7200  & 0.7533  & 0.6533   & 0.7867  \\
% jm1       & 0.6239  & 0.8010  & 0.8793   & 0.9910  \\
% kc1       & 0.6540  & 0.8531  & 0.5355   & 0.9431  \\
% mc1       & 0.8754  & 0.9916  & 0.9155   & 0.9979  \\
% PC1       & 0.8727  & 0.8727  & 0.7091   & 0.9273  \\
% PC2       & 0.9821  & 0.9982  & 0.9714   & 0.9803  \\
% PC3       & 0.7834  & 0.9172  & 0.8471   & 0.9682  \\
% PC4       & 0.8151  & 0.9178  & 0.9041   & 0.9932  \\
% ant-1.7   & 0.6667  & 0.7467  & 0.6667   & 0.9200  \\
% camel1.6  & 0.7010  & 0.5361  & 0.7010   & 0.8144  \\
% ivy-1.2   & 0.8056  & 0.8333  & 0.8056   & 0.9167  \\
% jedit-4.1 & 0.5312  & 0.7500  & 0.5312   & 0.5938  \\
% log4j-1.0 & 0.7857  & 0.6429  & 0.7857   & 0.5714  \\
% lucene-2.4& 0.5000  & 0.7059  & 0.5000   & 0.7059  \\
% poi-3.0   & 0.4222  & 0.7778  & 0.4222   & 0.8222  \\
% synapse-1.2 & 0.5769 & 0.6538  & 0.5769   & 0.5769  \\
% xerces-1.3 & 0.6522  & 0.7174  & 0.6522   & 0.8478  \\
% \midrule
% average   & 0.7135  & 0.8018  & 0.7173   & 0.8568  \\
% \midrule
% Improvement & 12.38\%\textuparrow & - & 19.45\%\textuparrow & - \\
% \midrule
% $Cohen's\ d$ & 0.6792 & - & 0.9451 & - \\
% \bottomrule
% \end{tabular}
% \label{tab:lime_JointSDT}
% \end{table}
\subsubsection{The results of RQ2}
% \begin{table}[t!]
%   \centering
%   \caption{Prediction Accuracies of Join\_CNN and Base\_CNN}
%     \begin{tabular}{lccc}
%     \toprule
%     Dataset & Join\_CNN   & Base\_CNN  \\
%     \midrule
%     JDT   & 0.8700 & 0.8100  \\
%     LC    & 0.8286 & 0.8286  \\
%     ML    & 0.9144 & 0.8503  \\
%     PDE   & 0.8867 & 0.8733  \\
%     jm1   & 0.7997 & 0.7060  \\
%     kc1   & 0.8152 & 0.7583  \\
%     mc1   & 0.9937 & 0.9873  \\
%     PC1   & 0.9091 & 0.8545  \\
%     PC2   & 0.9982 & 0.9982  \\
%     PC3   & 0.8981 & 0.8662  \\
%     PC4   & 0.9247 & 0.9247  \\
%     ACC   & 0.8944 & 0.8598  \\
%     \bottomrule
%     \end{tabular}%
%   \label{tab3}
% \end{table}%
\begin{table}[!t]\color{blue}
\centering
\caption{Prediction Performance of Joint\_CNN and Base\_CNN in Terms of F-measure and AUC}
\begin{tabular}{lllll}
\toprule
& \multicolumn{2}{l}{F-measure} & \multicolumn{2}{l}{AUC} \\
\cmidrule(ll){2-3} \cmidrule(ll){4-5}
& Base\_CNN &Joint\_CNN  &Base\_CNN  &Joint\_CNN  \\
\midrule
JDT & 0.6802 & \textbf{0.8544} & 0.7297 & \textbf{0.8360} \\
LC  & 0.5686 & \textbf{0.8285} & 0.6923 & 0.6862 \\
ML  & 0.5686 & \textbf{0.7123} & 0.6897 & 0.6418 \\
PDE & 0.7475 & 0.7111          & 0.7039 & 0.7032 \\
jm1 & 0.5755 & \textbf{0.8621} & 0.6240 & \textbf{0.6280} \\
kc1 & 0.6443 & \textbf{0.7303} & 0.7545 & \textbf{0.8087} \\
mc1 & 0.7352 & \textbf{0.8621} & 0.8082 & 0.6280 \\
PC1 & 0.7475 & \textbf{0.8621} & 0.7039 & 0.6280 \\
PC2 & 0.4996 & \textbf{0.8621} & 0.5000 & \textbf{0.6280} \\
PC3 & 0.5588 & \textbf{0.7957} & 0.7634 & \textbf{0.8324} \\
PC4 & 0.7021 & \textbf{0.7472} & 0.8169 & 0.7843 \\
ant-1.7      & 0.7991 & 0.7907          & 0.8276 & 0.8022 \\
camel-1.6    & 0.6932 & \textbf{0.7337} & 0.5992 & \textbf{0.6687} \\
ivy-1.2      & 0.8273 & \textbf{0.8347} & 0.6016 & \textbf{0.8594} \\
jedit-4.1    & 0.5862 & \textbf{0.6946} & 0.5429 & \textbf{0.8125} \\
log4j-1.0    & 0.7755 & 0.6520          & 0.8500 & 0.8500 \\
lucene-2.4   & 0.5623 & \textbf{0.6947} & 0.5482 & \textbf{0.7643} \\
poi-3.0      & 0.7156 & 0.6951          & 0.7263 & \textbf{0.7414} \\
synapse1.2   & 0.6616 & 0.6486          & 0.8235 & 0.7516 \\
xerces-1.3   & 0.7921 & \textbf{0.8407} & 0.8498 & 0.7875 \\
\midrule
\textcolor{orange}{Average}   & 0.6720  & \textbf{0.7706}  & 0.7078   & \textbf{0.7421} \\
\midrule
Improvement & 14.67\%\textcolor{blue}{\textuparrow} & - & 4.85\%\textcolor{blue}{\textuparrow} & - \\
\midrule
$Cohen's\ d$  & 1.1321 &- &0.3553 & - \\
\bottomrule
\end{tabular}
\label{tab:joint_Base_CNN}
\end{table}

\begin{table*}[!t]\color{orange}
\centering
\caption{Average Performances of Joint\_CNN and Base\_CNN in Terms of PPC, PNPC and FOR}
\begin{tabular}{lllllll}
\toprule
Group & \multicolumn{2}{l}{PPC} & \multicolumn{2}{l}{PNPC} & \multicolumn{2}{l}{FOR}\\
\cmidrule(ll){2-3} \cmidrule(ll){4-5} \cmidrule(ll){6-7}
& Base\_CNN &Joint\_CNN  &Base\_CNN  &Joint\_CNN  &Base\_CNN  &Joint\_CNN  \\
\midrule
AEEEM     & 0.6570 &\textbf{0.7394}  & 0.3430  &\textbf{0.2606}  &0.2597 &\textbf{0.1662} \\
NASA      & 0.6811 &\textbf{0.7708}  & 0.3189  &\textbf{0.2292}  &0.0503 &\textbf{0.0256}\\
PROMISE   & 0.4974 &\textbf{0.5167}  & 0.5026  &\textbf{0.4833}  &0.1867 &\textbf{0.1398}\\
\midrule
Average   & 0.6118 &\textbf{0.6756}  &0.3882   &\textbf{0.3244}  &0.1656 & \textbf{0.1105}\\
\midrule
Improvement &10.43\%\textcolor{blue}{\textuparrow} &- &19.67\%\textcolor{blue}{$\downarrow$} &- &49.86\%\textcolor{blue}{$\downarrow$} &-  \\
\bottomrule
\end{tabular}
\label{tab:joint_Base_CNN_PPC}
\end{table*}


According to Table \ref{tab:joint_Base_CNN}, we observed that Joint\_CNN outperforms Base\_CNN in terms of both F-measure and AUC. Across all datasets, Joint\_CNN achieves an average improvement of 14.67\% in F-measure and 4.85\% in AUC compared to Base\_CNN. 
\textcolor{orange}{Table \ref{tab:joint_Base_CNN_PPC} presents the average performance of Base\_CNN and Joint\_CNN on three groups: PROMISE, NASA, and AEEEM, evaluated using the metrics PPC, PNPC, and FOR. The results in Table \ref{tab:joint_Base_CNN_PPC} show that, compared to Base\_CNN, Joint\_CNN achieves improvements of 10.43\% in PPC, 19.67\% in PNPC, and 49.86\% in FOR. This indicates that Joint\_CNN is more accurate in predicting clean modules, more effective in predicting defective instances on average, and better at reducing false negative instances across the target projects.}
\textcolor{blue}{The $Cohen's\ d$ results indicate a large effect size in F-measure and a moderate effect size in AUC, further confirming the superiority of Joint\_CNN over Base\_CNN.} According to Table \ref{tab:JointCNN_SVM_DBN_RF_AutoSpearman}, joint\_CNN outperforms SVM, RF, DBN and AutoSpearman in terms of both F-measure and AUC. Specifically, Joint\_CNN achieves average improvements of 42.86\%, 30.63\%, 61.75\% and 2.13\% in F-measure compared to SVM, RF, DBN and AutoSpearman, respectively. Similarly, in AUC, Joint\_CNN demonstrates average improvements of 10.93\%, 15\%, 20.75\% and 0.97\%. \textcolor{blue}{The $Cohen's\ d$ results further validate the performance advantages of Joint\_CNN. In terms of F-measure, the effect sizes are large, indicating significant improvements over SVM, RF and DBN. While the effect size for AutoSpearman is smaller, Joint\_CNN still demonstrates overall better performance. For AUC, the effect sizes vary across different comparisons. Specifically, the effect size for SVM is medium, while the effect sizes for RF and DBN are large, indicating significant improvements. Although the effect size for AutoSpearman is small, Joint\_CNN still demonstrates overall robustness.}
\begin{table*}[!t]\color{blue}
\centering
\caption{The F-measure and AUC performance of different models on all datasets}
\begin{tabular}{lllllllllll}
\toprule
& \multicolumn{5}{l}{F-measure} & \multicolumn{5}{l}{AUC} \\
\cmidrule(lllll){2-6} \cmidrule(lllll){7-11}
Dataset    & SVM    & RF     & DBN    & AutoSpearman & Joint\_CNN   & SVM    & RF     & DBN    & AutoSpearman & Joint\_CNN \\
\midrule
JDT        & 0.6239 & 0.6989 & 0.7162 & 0.7824    & \textbf{0.8544}      & 0.7554 & 0.7419 & 0.7852 & 0.8598           & 0.8360    \\
LC         & 0.3467 & 0.7177 & 0.3467 & 0.7871    & \textbf{0.8285}      & 0.5846 & 0.8462 & 0.5846 & 0.5046           & 0.6862    \\
ML         & 0.4919 & 0.5958 & 0.3219 & 0.8054    & 0.7123               & 0.6732 & 0.7522 & 0.4596 & 0.7339           & 0.6418    \\
PDE        & 0.5565 & 0.6209 & 0.4874 & 0.7837    & 0.7111               & 0.6875 & 0.6601 & 0.6130 & 0.6957           & \textbf{0.7032}    \\
jm1        & 0.4476 & 0.5849 & 0.4476 & 0.7580    & \textbf{0.8621}      & 0.5030 & 0.5762 & 0.5030 & 0.6269           & \textbf{0.6280}    \\
kc1        & 0.5695 & 0.5371 & 0.6918 & 0.5814    & \textbf{0.7303}      & 0.6992 & 0.6141 & 0.7724 & 0.8111           & 0.8087    \\
mc1        & 0.4754 & 0.5513 & 0.4742 & 0.5814    & \textbf{0.8621}      & 0.8914 & 0.5577 & 0.8326 & 0.9373           & 0.6280    \\
PC1        & 0.3870 & 0.7556 & 0.3939 & 0.8276    & \textbf{0.8621}      & 0.6562 & 0.8557 & 0.6057 & 0.6815           & 0.6280    \\
PC2        & 0.4736 & 0.4996 & 0.4213 & 0.9819    & 0.8621               & 0.4507 & 0.5000 & 0.3647 & 0.4194           & \textbf{0.6280}    \\
PC3        & 0.5580 & 0.5745 & 0.5224 & 0.8345    & 0.7957               & 0.7914 & 0.5772 & 0.7356 & 0.8336           & 0.8324    \\
PC4        & 0.5700 & 0.6694 & 0.3535 & 0.8617    & 0.7472               & 0.7380 & 0.6154 & 0.5651 & 0.7814           & \textbf{0.7843}    \\
ant-1.7    & 0.7900 & 0.7121 & 0.5446 & 0.7760    & \textbf{0.7907}      & 0.8409 & 0.7477 & 0.6591 & 0.8266           & 0.8022    \\
camel-1.6  & 0.4920 & 0.5367 & 0.6314 & 0.6710    & \textbf{0.7337}      & 0.6199 & 0.5621 & 0.6923 & 0.6356           & 0.6687    \\
ivy-1.2    & 0.6244 & 0.7317 & 0.5000 & 0.8721    & 0.8347               & 0.6719 & 0.8125 & 0.7500 & 0.6250           & \textbf{0.8594}    \\
jedit-4.1  & 0.3914 & 0.3273 & 0.3431 & 0.7585    & 0.6946               & 0.4657 & 0.3743 & 0.4771 & 0.8698           & 0.8125    \\
log4j-1.0  & 0.4286 & 0.3538 & 0.4286 & 0.6533    & 0.6520               & 0.6364 & 0.5909 & 0.6364 & 0.7750           & \textbf{0.8500}    \\
lucene-2.4 & 0.7153 & 0.3585 & 0.5983 & 0.7059    & 0.6947               & 0.7140 & 0.5000 & 0.6333 & 0.8250           & 0.7643    \\
poi-3.0    & 0.6584 & 0.6786 & 0.5139 & 0.7819    & 0.6951               & 0.6463 & 0.6624 & 0.5392 & 0.8082           & 0.7414    \\
synapse-1.2& 0.5439 & 0.6067 & 0.3210 & 0.5247    & \textbf{0.6486}      & 0.5437 & 0.6062 & 0.4125 & 0.6732           & \textbf{0.7516}    \\
xerces-1.3 & 0.6430 & 0.6865 & 0.4697 & 0.7611    & \textbf{0.8407}      & 0.7747 & 0.6758 & 0.6465 & 0.7766           & \textbf{0.7875}    \\

\midrule
\textcolor{orange}{Average}    & 0.5394 & 0.5899 & 0.4764 & 0.7545 & \textbf{0.7706}  &0.6690  &0.6453  &0.6146 & 0.7350  & \textbf{0.7421} \\
\midrule
Improvement& 42.86\%\textcolor{blue}{\textuparrow} & 30.63\%\textcolor{blue}{\textuparrow} & 61.75\%\textcolor{blue}{\textuparrow} & 2.13\%\textcolor{blue}{\textuparrow} & -   &10.93\%\textcolor{blue}{\textuparrow} &15.00\%\textcolor{blue}{\textuparrow} &20.75\%\textcolor{blue}{\textuparrow} & 0.97\%\textcolor{blue}{\textuparrow} & - \\
\midrule
$Cohen's\ d$  &  2.3697 & 1.7196 & 2.9703  &0.1712 & - & 0.7337 & 0.9543 & 1.1906 &0.0657& - \\
\bottomrule
\end{tabular}
\label{tab:JointCNN_SVM_DBN_RF_AutoSpearman}
\end{table*}

We believe there are two reasons related to the improvement: (1) We explicitly integrate the prediction error $L_{pred}(f,D)$ into the loss function of the joint learning, enabling the predictor to capture discriminative information contained in the data during the training process, and subsequently improving the corresponding accuracy. (2) The $L_{int}$ can be regarded as a penalty term of $L$, which effectively encourages the framework to learn more generalizable representations of the data, leading to improved predictive accuracy for the data that is not contained in training dataset.

%  ---------------------------------------------------------------
% \begin{table*}[!tb]\color{blue}
% \centering
% \caption{Comparison of F-measure and AUC for AutoSpearman\_CNN and Joint\_CNN across Various Projects}
% \begin{tabular}{lcccc}
% \toprule
% & \multicolumn{2}{l}{F-measure} & \multicolumn{2}{l}{AUC} \\
% \cmidrule(ll){2-3} \cmidrule(ll){4-5}
% \textbf{Dataset} & \textbf{AutoSpearman\_CNN} & \textbf{Joint\_CNN} & \textbf{AutoSpearman\_CNN} & \textbf{Joint\_CNN} \\
% \midrule
% JDT          & 0.7824  & 0.8544  & 0.8598  & 0.836   \\
% LC           & 0.7871  & 0.8285  & 0.5046  & 0.6862  \\
% ML           & 0.8054  & 0.7123  & 0.7339  & 0.6418  \\
% PDE          & 0.7837  & 0.7111  & 0.6957  & 0.7032  \\
% jm1          & 0.7580  & 0.8621  & 0.6269  & 0.6280  \\
% kc1          & 0.5814  & 0.7303  & 0.8111  & 0.8087  \\
% mc1          & 0.5814  & 0.8621  & 0.9373  & 0.6280  \\
% PC1          & 0.8276  & 0.8621  & 0.6815  & 0.6280  \\
% PC2          & 0.9819  & 0.8621  & 0.4194  & 0.6280  \\
% PC3          & 0.8345  & 0.7957  & 0.8336  & 0.8324  \\
% PC4          & 0.8617  & 0.7472  & 0.7814  & 0.7843  \\
% ant-1.7      & 0.7760  & 0.7907  & 0.8266  & 0.8022  \\
% camel-1.6    & 0.6710  & 0.7337  & 0.6356  & 0.6687  \\
% ivy-1.2      & 0.8721  & 0.8347  & 0.6250  & 0.8594  \\
% jedit-4.1    & 0.7585  & 0.6946  & 0.8698  & 0.8125  \\
% log4j-1.0    & 0.6533  & 0.6520  & 0.7750  & 0.8500  \\
% lucene-2.4   & 0.7059  & 0.6947  & 0.8250  & 0.7643  \\
% poi-3.0      & 0.7819  & 0.6951  & 0.8082  & 0.7414  \\
% synapse-1.2  & 0.5247  & 0.6486  & 0.6732  & 0.7516  \\
% xerces-1.3   & 0.7611  & 0.8407  & 0.7766  & 0.7875  \\
% \midrule
% Average  & 0.7545  & 0.7706  & 0.7350  & 0.7421  \\
% \midrule
% Improvement & 2.13\%\textcolor{blue}{\textuparrow} & - & 0.97\textcolor{blue}{\textuparrow}  & - \\
% \midrule
% $Cohen's\ d$  &0.1712  & - &0.0657   & - \\
% \bottomrule
% \end{tabular}
% \label{tab:autospearman_our}
% \end{table*}

%----------------Comparison of Average Metric----------------------
% \begin{table}[t!]
%   \centering
%   \caption{Comparison of Average Metrics Across Joint\_CNN, Base\_CNN, SVM, RF, and DBN}
%     \begin{tabular}{lccccc}
%     \toprule
%     Metric     &Join\_CNN  &Base\_CNN & SVM     & RF      &DBN  \\
%     \midrule
%     F1-score   & 0.7706    & 0.6720   & 0.5394  & 0.5899  & 0.4764\\
%     AUC        & 0.7421    & 0.7109   & 0.6690  & 0.6453  & 0.6146\\
%     \bottomrule
%     \end{tabular}%
%   \label{tab44}
% \end{table}%
\subsubsection{The results of RQ3}
To demonstrate the global interpretability of our approach, we take the PC2 dataset as an example. As presented in Fig.~\ref{PC2_tree}, the decision logic can be abstracted as the path from the ``\texttt{DECISION\_DENSITY}'' metric (denoted as the root) to the leaf node (denoted as 1 or 0). For example, one feasible decision path can be presented as \texttt{DECISION\_DENSITY -> MAINTENANCE\_SEVERITY -> PERCENT\_COMMENTS -> PERCENT\_COMMENTS -> 0}, \textcolor{blue}{which is similar to the form of decision tree's interpretation. So, we can conclude our approach achieves the global interpretability.}

\textcolor{blue}{We measure the computational cost by calculating the average training time. Specifically, we employ the Python $\mathrm{time}$ module to record the start and end times of the training process for both our approach and SP-Lime. To ensure statistical reliability, this procedure is repeated ten times for each method. The results show that the average computational costs for our approach and SP-Lime are 744.7 seconds and 2301.4 seconds, respectively. These findings indicate that our method achieves a significant reduction in computational cost compared to SP-Lime. The substantial difference can be attributed to the optimized design of our method, which abstracts the decision logic as the path of SDT from the root node to the leaf node. In contrast, SP-Lime requires extensive perturbations and evaluations across the feature space, leading to higher computational requirements.}

\textcolor{blue}{Furthermore, we calculate the CD, FI and AI values for LC dataset to evaluate the reliability of our approach and SP-Lime. As presented in table \ref{tab77}, our method achieves a better performance in all three indicators, indicating our method is more reliable than SP-Lime. The reason for this result can be partially attributed to the random sampling involved in perturbation processes of SP-Lime \cite{60}.}
\begin{table}[!t]\color{blue}
  \centering
  \caption{The CD, FI and AI values of Joint\_SDT and SP-Lime}
    \begin{tabular}{lcccc}
    \toprule
    Method    &CD   &FI  & AI    \\
    \midrule
    Joint\_SDT   & 1.00  & 0.8714   &  0.8429 \\
    SP-Lime   & 0.81    &  0.7933  &  0.8012 \\
    \bottomrule
    \end{tabular}%
  \label{tab77}
\end{table}%

\subsubsection{The results of RQ4}
% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=\columnwidth]{images/PC2_tree.pdf}
%     \label{fig5:our_method}
%     \caption{The interpretations of the proposed method on PC2 dataset.}
%     \label{fig5}
% \end{figure}

We conducted an ablation experiment on 5,589 instances in the PC2 dataset to analyze the impact of different features on model performance. The interpretations generated by our approach are presented in Fig.~\ref{PC2_tree}. In the interpretations of our approach, the metrics that have a greater impact on the prediction results are located closer to the root node. 

Meanwhile, the impacts on the F-measure, AUC, and MCC after removing certain metrics are presented in Fig.~\ref{fig6}.  
We can see that the accuracy of Joint\_CNN went down significantly across MCC, F-measure and AUC three indicators after removing the ``\texttt{DECISION\_DENSITY}''. 
This indicates that this metric is critical for the accuracy of the predictor. This finding is consistent with the interpretation of our approach, as our approach placed it at the root node of SDT, indicating its decisive role in the model’s decision-making process. Thus, the significant decline in performance after removing this feature further corroborates its key role in the model. The importance of this feature is strongly evidenced both by the performance change and its position in the explanation.

To enhance the comprehensiveness of the analysis, the ablation experiment was extended to all datasets. The most important features interpreted by our approach for each dataset, as well as the ranking of performance degradation on the F-measure, AUC, and MCC after removing these features, are presented in Table \ref{tab66}. \textcolor{blue}{The results demonstrate that the performance degradation is consistent with our explanatory results, further validating that our method effectively captures key features to improve prediction performance and provide accurate model interpretations.}

\begin{table*}[!t]
  \centering
  \caption{Ranking of Feature Importance Based on SDT Interpretations and Ablation Results}
    \begin{tabular}{lccccc}
    \toprule
    Dataset    &Top Feature Identified by SDT   &F-measure Rank  & AUC Rank    &MCC Rank  \\
    \midrule
    JDT   & WCHU\_numberOfMethodsInherited      & 4   & 9  & 5  \\
    LC    & WCHU\_cbo                           & 2   & 1  & 1  \\
    ML    & ck\_oo\_numberOfPublicMethods       & 5   & 9  & 5  \\
    PDE   & WCHU\_fanOut                        & 3   & 3  & 5  \\
    jm1   & HALSTEAD\_DIFFICULTY                & 7   & 7  & 1  \\
    kc1   & v                                   & 1   & 2  & 2  \\
    mc1   & LOC\_TOTAL                          & 1   & 1  & 1  \\
    PC1   & branchCount                         & 2   & 1  & 1  \\
    PC2   & DECISION\_DENSITY                   & 1   & 4  & 1  \\
    PC3   & DESIGN\_DENSITY                     & 4   & 3  & 3  \\
    PC4   & LOC\_TOTAL                          & 2   & 2  & 1  \\

\textcolor{blue}{ant-1.7}     & \textcolor{blue}{mfa}     & \textcolor{blue}{1}       & \textcolor{blue}{5}       & \textcolor{blue}{1}  \\ 
\textcolor{blue}{camel-1.6}   & \textcolor{blue}{lcom}    & \textcolor{blue}{4}       & \textcolor{blue}{3}       & \textcolor{blue}{4}  \\ 
\textcolor{blue}{ivy-1.2}     & \textcolor{blue}{cam}     & \textcolor{blue}{2}       & \textcolor{blue}{7}       & \textcolor{blue}{4}  \\ 
\textcolor{blue}{jedit-4.1}   & \textcolor{blue}{lcom3}   & \textcolor{blue}{6}       & \textcolor{blue}{10}      & \textcolor{blue}{4}  \\ 
\textcolor{blue}{log4j-1.0}   & \textcolor{blue}{max\_cc} & \textcolor{blue}{4}       & \textcolor{blue}{4}       & \textcolor{blue}{3}  \\ 
\textcolor{blue}{lucene-2.4}  & \textcolor{blue}{dam}     & \textcolor{blue}{1}       & \textcolor{blue}{1}       & \textcolor{blue}{1}  \\
\textcolor{blue}{poi-3.0}     & \textcolor{blue}{wmc}     & \textcolor{blue}{2}       & \textcolor{blue}{3}       & \textcolor{blue}{2}  \\
\textcolor{blue}{synapse-1.2} & \textcolor{blue}{cbo}     & \textcolor{blue}{1}       & \textcolor{blue}{1}       & \textcolor{blue}{4}  \\ 
\textcolor{blue}{xerces-1.3}  & \textcolor{blue}{max\_cc} & \textcolor{blue}{2}       & \textcolor{blue}{8}       & \textcolor{blue}{2}  \\ 
\bottomrule
\end{tabular}%
\label{tab66}
\end{table*}%
% \begin{table*}[!t]
%   \centering
%   \caption{Ranking of Feature Importance Based on SDT Interpretations and Ablation Results}
%     \begin{tabular}{lccccc}
%     \toprule
%     Dataset    &Top Feature Identified by SDT   &F1-score Rank  & AUC Rank    &MCC Rank  \\
%     \midrule
%     JDT   & WCHU\_numberOfMethodsInherited      & 4   & 9  & 5  \\
%     LC    & WCHU\_cbo                           & 2   & 1  & 1  \\
%     ML    & ck\_oo\_numberOfPublicMethods       & 5   & 9  & 5  \\
%     PDE   & WCHU\_fanOut                        & 3   & 3  & 5  \\
%     jm1   & HALSTEAD\_DIFFICULTY                & 7   & 7  & 1  \\
%     kc1   & v                                   & 1   & 2  & 2  \\
%     mc1   & LOC\_TOTAL                          & 1   & 1  & 1  \\
%     PC1   & branchCount                         & 2   & 1  & 1  \\
%     PC2   & DECISION\_DENSITY                   & 1   & 4  & 1  \\
%     PC3   & DESIGN\_DENSITY                     & 4   & 3  & 3  \\
%     PC4   & LOC\_TOTAL                          & 2   & 2  & 1  \\
%     ant-1.7   & mfa                             & 1       & 5      & 1  \\ 
%     camel-1.6 & lcom                            & 4       & 3      & 4  \\ 
%     ivy-1.2   & cam                             & 2       & 7      & 4  \\ 
%     jedit-4.1 & lcom3                           & 6       & 10     & 4  \\ 
%     log4j-1.0 & max\_cc                         & 4       & 4      & 3  \\ 
%     lucene-2.4 & dam                            & 1       & 1      & 1  \\
%     poi-3.0    & wmc                            & 2       & 3      & 2  \\
%     synapse-1.2 & cbo                           & 1       & 1      & 4  \\ 
% xerces-1.3  & max\_cc                           & 2       & 8      & 2  \\
% \bottomrule
% \end{tabular}%
% \label{tab66}
% \end{table*}%

\begin{figure}[tb]
    \centering
    \subfloat[F-measure]{
        \includegraphics[width=0.4\textwidth, height=0.2\textheight]{images/PC2_f1.png}
        \label{fig:F1}
    } \\

    \subfloat[AUC]{
        \includegraphics[width=0.4\textwidth, height=0.2\textheight]{images/PC2_AUC.png}
        \label{fig:AUC}
    } \\
% \hspace{0.05\textwidth} % Space between the figures
    \subfloat[MCC]{
        \includegraphics[width=0.4\textwidth, height=0.2\textheight]{images/PC2_MCC.png}
        \label{fig:MCC}
    }
    \caption{Ablation study results across multiple indicators. The gray dashed line indicates the performance of DP-CNN before removing metrics.}
    \label{fig6}
\end{figure}



% \subsection{Statistical significance}
% To verify the significance of the improvements achieved by our approach, we introduced $Cohen's\ d$ d in this section. $Cohen's\ d$ is used to measure the difference between two groups, which can take any value between 0 and infinity. When $Cohen's\ d$ takes values in the ranges $[0,0.2), [0.2,0.8), and\ [0.8, +\infty)$, it indicates that the differences between the two groups are small, medium, and large \footnote{ https://en.wikipedia.org/wiki/Effect\_size}.
% The definition of $Cohen's\ d$ is as follows:

% \begin{equation}
%     {\rm Coh} = \frac{M_1 - M_2}{\sqrt{(SD_1^2 + SD_2^2)/2}}
% \end{equation}
% where $M_1$ and $M_2$ represent the means of the two group data, and $SD_1$ and $SD_2$ represent their
% standard deviations. 

% Table \ref{tab7} presents the $Cohen's\ d$ values of the proposed method relative to the LIME and BreakDown for the two indicators $FI$ and $CD$-10\%. The results indicate that our approach exhibits a significant improvement compared to LIME. Since the $FI$ of BreakDown cannot be calculated, we are unable to provide the corresponding $Cohen's\ d$ values (denoted as `-' in the table). Additionally, the proposed method demonstrates a significant improvement in $CD$-10\% compared to BreakDown.


% \begin{table}[t]
%   \centering
%   \caption{$Cohen's\ d$ values of our method relative to the LIME and BreakDown for the $FI$ and $CD$}
%     \begin{tabular}{lccc}
%     \toprule
%      $Cohen's\ d$  & $FI$   & $CD$  \\
%     \midrule
%     ${\rm Coh}({\rm our}, {\rm LIME})$          & 1.11 & 1.42 \\
%     ${\rm Coh}({\rm our}, {\rm BreakDown})$     &-     &2.20     \\
%     \bottomrule
%     \end{tabular}%
%   \label{tab7}
% \end{table}%

\section{Threats to validity}
\textcolor{blue}{While the proposed model demonstrates advancements in both prediction accuracy and interpretability, certain limitations may affect its generalizability and reliability. The details are as follows.
\begin{itemize}
    \item  Choice of dataset: The experiment was conducted on the publicly available datasets. These datasets may not fully capture the diversity of software systems in real-world scenarios, potentially limiting the robustness of the model.
    \item Hyperparameter setting: The model’s performance is related with the selection of hyperparameters for both the predictor and interpreter. Suboptimal tuning may lead to overfitting or underfitting, resulting in less accurate predictions or interpretations.
    \item Application scenarios: In this paper, we focus on the within-project defect prediction that the training data and the test data are generated from the same project. However, the performance of our model in the context of the cross-project setting is not evaluated. 
\end{itemize}
}

% \section{Conclusion}\label{sec5}
\section{\textcolor{blue}{Conclusion and future works}}\label{sec5}
In this paper, we explore the possibility of designing a defect predictor and its corresponding interpreter collaboratively. Unlike most existing approaches, which treat defect prediction and interpretation as separate tasks, we consider them strongly correlated. Based on this assumption, we introduce a framework aimed at improving the reliability of interpretation while simultaneously enhancing predictive accuracy by utilizing interpretation results.
\textcolor{blue}{Based on the proposed framework, we conducted extensive empirical evaluations, which revealed the following key findings:
\begin{itemize}
    \item  Compared to existing interpretation methods such as LIME and BreakDown, the proposed framework significantly improves the consistency and reliability of interpretations, providing robust explanations from a local perspective. This improvement is attributed to the incorporation of the Knowledge Distillation (KD) principle, which enables the accurate transfer of decision-making knowledge from the predictor to the interpreter.
    \item The framework demonstrates superior performance on widely used datasets in terms of both F-measure and AUC, outperforming baseline models such as SVM, RF, DBN, and Base\_CNN. By explicitly incorporating interpretation results into the loss function, the framework captures more discriminative information, leading to more robust predictive outcomes.
    \item The proposed method offers global interpretability, providing developers with transparent global insights and decision-making rationales. This capability ensures that the framework not only achieves high accuracy but also delivers comprehensive explanatory support for real-world defect prediction tasks.
    \item Ablation analysis further demonstrates that the framework effectively identifies key features that have the greatest impact on prediction outcomes, showcasing its ability to provide reliable explanations that align closely with the actual importance of these features.
\end{itemize}}

% \textcolor{blue}{Additionally, ablation analysis further demonstrates that the framework effectively identifies key features that have the greatest impact on prediction outcomes, showcasing its ability to provide reliable explanations that align closely with the actual importance of these features.}
% Future studies will explore the potential of applying this framework in various defect prediction scenarios, such as just-in-time defect prediction and cross-project defect prediction. Furthermore, we will endeavor to design different constraints on the loss functions to promote mutual benefits for both the predictor and interpreter.

\textcolor{blue}{Future research will investigate several possibilities to further improve the proposed model, including the following: 
\begin{itemize}
    \item Expand the research by including a wider array of datasets that accurately reflect real-world software systems. Evaluate the robustness and generalizability of our model accordingly. 
    \item Incorporate automatic hyperparameter optimization methods into our model to reduce the likelihood of suboptimal tuning. 
    \item Assess the model's performance in several defect prediction scenarios, such as the cross-project defect prediction.
\end{itemize}}
% \textcolor{blue}{Future research will investigate several possibilities to further improve the proposed model. First, expand the research by including a wider array of datasets that accurately reflect real-world software systems. Evaluate the robustness and generalizability of our model accordingly. Second, incorporate automatic hyperparameter optimization methods into our model to reduce the likelihood of suboptimal tuning. Third, assess the model's performance in several defect prediction scenarios, such as the cross-project defect prediction.}

\begin{thebibliography}{00}
\bibitem{1}
W. Zhang et al., ``Research on software defect prediction method based on machine learning,'' Applied Mechanics and Materials, vol. 687, pp. 2182--2185, 2014.

\bibitem{2}
Q. Wang, S. Wu, and M. S. Li, ``Software defect prediction,'' Journal of Software, vol. 19, no. 1, 2008.


\bibitem{3}
J. Jiarpakdee, C. K. Tantithamthavorn, and J. Grundy, ``Practitioners' perceptions of the goals and visual explanations of defect prediction models,'' in 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR), pp. 432--443, 2021.

\bibitem{4}
C. K. Tantithamthavorn and J. Jiarpakdee, ``Explainable AI for software engineering,'' in 2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE), pp. 1--2, 2021.\

\bibitem{5}
H. K. Dam, T. Tran, and A. Ghose, ``Explainable software analytics,'' in Proceedings of the 40th International Conference on Software Engineering: New Ideas and Emerging Results, pp. 53--56, 2018.

\bibitem{murdoch2019definitions} % 6
W. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu, ``Definitions, methods, and applications in interpretable machine learning,'' Proceedings of the National Academy of Sciences, vol. 116, no. 44, pp. 22071--22080, 2019.

\bibitem{7}
J. Jiarpakdee, C. Tantithamthavorn, and A. E. Hassan, ``The impact of correlated metrics on defect models,'' arXiv preprint arXiv:1801.10271, 2018.

% \bibitem{8}
% M. T. Ribeiro, S. Singh, and C. Guestrin, ``Model-agnostic interpretability of machine learning,'' arXiv preprint arXiv:1606.05386, 2016.

\bibitem{lime} % 9
M. T. Ribeiro, S. Singh, and C. Guestrin, ``Why should I trust you? Explaining the predictions of any classifier,'' Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1135--1144, 2016.

\bibitem{10}
S. M. Lundberg and S.-I. Lee, ``A unified approach to interpreting model predictions,'' Advances in Neural Information Processing Systems, vol. 30, 2017.

% \bibitem{11}
% S. R. Chidamber and C. F. Kemerer, ``A metrics suite for object oriented design,'' IEEE Transactions on Software Engineering, vol. 20, no. 6, pp. 476--493, 1994.

\bibitem{12}
J. Bansiya and C. G. Davis, ``A hierarchical model for object-oriented design quality assessment,'' IEEE Transactions on Software Engineering, vol. 28, no. 1, pp. 4--17, 2002.


\bibitem{13}
J. Jiarpakdee, C. Tantithamthavorn, A. Ihara, and K. Matsumoto, ``A study of redundant metrics in defect prediction datasets,'' in 2016 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW), pp. 51--52, 2016.

\bibitem{14}
J. Jiarpakdee, C. Tantithamthavorn, and A. E. Hassan, ``The impact of correlated metrics on defect models,'' arXiv preprint arXiv:1801.10271, 2018.

% \bibitem{15}
% M. A. Hall, ``Correlation-based feature selection for machine learning,'' PhD thesis, The University of Waikato, 1999.

% \bibitem{16}
% G. H. John, R. Kohavi, and K. Pfleger, ``Irrelevant features and the subset selection problem,'' in Machine Learning Proceedings 1994, Elsevier, pp. 121--129, 1994.

% \bibitem{17}
% R. Kohavi and G. H. John, ``Wrappers for feature subset selection,'' Artificial Intelligence, vol. 97, no. 1-2, pp. 273--324, 1997.

\bibitem{18}
J. Jiarpakdee, C. Tantithamthavorn, and C. Treude, ``AutoSpearman: Automatically mitigating correlated software metrics for interpreting defect models,'' in 2018 IEEE International Conference on Software Maintenance and Evolution (ICSME), pp. 92--103, 2018, organization={IEEE Computer Society}.

\bibitem{19}
L. Cai, Y. Fan, M. Yan, and X. Xia, ``Just-in-time software defect prediction: Literature review,'' Journal of Software, vol. 30, no. 5, pp. 1288--1307, 2019.

\bibitem{20}
I.-G. Czibula, G. Czibula, Z. Marian, and V.-S. Ionescu, ``A novel approach using fuzzy self-organizing maps for detecting software faults,'' Studies in Informatics and Control, vol. 25, no. 2, pp. 207--216, 2016.

\bibitem{21}
D. Kaur, A. Kaur, S. Gulati, and M. Aggarwal, ``A clustering algorithm for software fault prediction,'' in Proceedings of the 2010 International Conference on Computer and Communication Technology (ICCCT), IEEE, pp. 603--607, 2010.

\bibitem{22}
Y. Kamei et al., ``A large-scale empirical study of just-in-time quality assurance,'' IEEE Transactions on Software Engineering, vol. 39, no. 6, pp. 757--773, 2012, IEEE.

\bibitem{23}
X. Yang, D. Lo, X. Xia, and J. Sun, ``TLEL: A two-layer ensemble learning approach for just-in-time defect prediction,'' Information and Software Technology, vol. 87, pp. 206--220, 2017.

\bibitem{24}
J. Nam, ``Survey on software defect prediction,'' Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Tech. Rep, 2014.
\bibitem{25}
B. Eken, ``Assessing personalized software defect predictors,'' in Proceedings of the 40th International Conference on Software Engineering: Companion Proceedings, pp. 488--491, 2018.

\bibitem{26}
T. Jiang, L. Tan, and S. Kim, ``Personalized defect prediction,'' in 2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE), pp. 279--289, 2013, organization={IEEE}.

\bibitem{27}
X. Yang, D. Lo, X. Xia, Y. Zhang, and J. Sun, ``Deep learning for just-in-time defect prediction,'' in 2015 IEEE International Conference on Software Quality, Reliability and Security, pp. 17--26, 2015, organization={IEEE}.


\bibitem{28}
T. Hoang, H. K. Dam, Y. Kamei, D. Lo, and N. Ubayashi, ``Deepjit: An end-to-end deep learning framework for just-in-time defect prediction,'' in 2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR), pp. 34--45, 2019, organization={IEEE}.

\bibitem{29}
L. Qiao, X. Li, Q. Umer, and P. Guo, ``Deep learning based software defect prediction,'' Neurocomputing, vol. 385, pp. 100--110, 2020.

\bibitem{30}
J. Li, P. He, J. Zhu, and M. R. Lyu, ``Software defect prediction via convolutional neural network,'' in 2017 IEEE International Conference on Software Quality, Reliability and Security (QRS), pp. 318--328, 2017, organization={IEEE}.


% \bibitem{31}
% L. St, S. Wold, et al., ``Analysis of variance (ANOVA),'' Chemometrics and Intelligent Laboratory Systems, vol. 6, no. 4, pp. 259--272, 1989.
%-----------------修改成引用最新的文献-----------------
\bibitem{31}
M. N. R. Chowdhury, W. Zhang, and T. Akilan, ``ANOVA-based Automatic Attribute Selection and a Predictive Model for Heart Disease Prognosis,'' arXiv preprint arXiv:2208.00296, 2022.


\bibitem{32}
P. Wei, Z. Lu, and J. Song, ``Variable importance analysis: A comprehensive review,'' Reliability Engineering \& System Safety, vol. 142, pp. 399--432, 2015.

\bibitem{34}
A. Gosiewska and P. Biecek, ``Do not trust additive explanations,'' arXiv preprint arXiv:1903.11420, 2019.

\bibitem{35}
M. T. Ribeiro, S. Singh, and C. Guestrin, ``Anchors: High-precision model-agnostic explanations,'' in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, 2018.

\bibitem{36}
J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson, ``Understanding neural networks through deep visualization,'' arXiv preprint arXiv:1506.06579, 2015.


\bibitem{37}
D. Alvarez-Melis and T. S. Jaakkola, ``On the robustness of interpretability methods,'' arXiv preprint arXiv:1806.08049, 2018.

\bibitem{38}
J. Gou, B. Yu, S. J. Maybank, and D. Tao, ``Knowledge distillation: A survey,'' International Journal of Computer Vision, vol. 129, no. 6, pp. 1789--1819, 2021.


\bibitem{39}
G. Hinton, O. Vinyals, and J. Dean, ``Distilling the knowledge in a neural network,'' arXiv preprint arXiv:1503.02531, 2015.


\bibitem{che2016interpretable} % 40
Z. Che, S. Purushotham, R. Khemani, and Y. Liu, ``Interpretable deep models for ICU outcome prediction,'' in Proceedings of the AMIA Annual Symposium Proceedings, vol. 2016, p. 371, 2016, American Medical Informatics Association.


\bibitem{biggs2021model} % 41
M. Biggs, W. Sun, and M. Ettl, ``Model distillation for revenue optimization: Interpretable personalized pricing,'' in Proceedings of the International Conference on Machine Learning, pp. 946--956, 2021, PMLR.

% \bibitem{42}
% T. J. McCabe, ``A complexity measure,'' IEEE Transactions on Software Engineering, no. 4, pp. 308--320, 1976.

\bibitem{43}
M. Jureczko and L. Madeyski, ``Towards identifying software project clusters with regard to defect prediction,'' in Proceedings of the 6th International Conference on Predictive Models in Software Engineering, pp. 1--10, 2010.

\bibitem{44}
M.-H. Tang, M.-H. Kao, and M.-H. Chen, ``An empirical study on object-oriented metrics,'' in Proceedings Sixth International Software Metrics Symposium (Cat. No.PR00403), pp. 242--249, 1999, organization={IEEE}.

\bibitem{45}
L. Qiao and Y. Wang, ``Effort-aware and just-in-time defect prediction with neural network,'' PloS One, vol. 14, no. 2, pp. e0211359, 2019.

\bibitem{46}
K. Zhu, N. Zhang, S. Ying, and D. Zhu, ``Within-project and cross-project just-in-time defect prediction based on denoising autoencoder and convolutional neural network,'' IET Software, vol. 14, no. 3, pp. 185--195, 2020.

\bibitem{47}
N. Frosst and G. Hinton, ``Distilling a neural network into a soft decision tree,'' arXiv preprint arXiv:1711.09784, 2017.

\bibitem{sivaprasad2023evaluation} % 48刚加入的
A. Sivaprasad, E. Reiter, N. Tintarev, and N. Oren, ``Evaluation of human-understandability of global model explanations using decision tree,'' in \emph{Proceedings of the European Conference on Artificial Intelligence}, Springer, pp. 43--65, 2023.

\bibitem{parekh2021framework} % 48
J. Parekh, P. Mozharovskyi, and F. d'Alch\'e-Buc, ``A framework to learn with interpretation,'' Advances in Neural Information Processing Systems, vol. 34, pp. 24273--24285, 2021.

\bibitem{lakkaraju2020robust} % 49
H. Lakkaraju, N. Arsov, and O. Bastani, ``Robust and stable black box explanations,'' in Proceedings of the International Conference on Machine Learning, pp. 5628--5638, 2020, PMLR.

\bibitem{bang2021explaining} % 50
S. Bang, P. Xie, H. Lee, W. Wu, and E. Xing, ``Explaining a black-box by using a deep variational information bottleneck approach,'' in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 13, pp. 11396--11404, 2021.

\bibitem{51}
J. Nam, S. J. Pan, and S. Kim, ``Transfer defect learning,'' in 2013 35th International Conference on Software Engineering (ICSE), pp. 382--391, 2013, organization={IEEE}.

\bibitem{52}
R. Wu, H. Zhang, S. Kim, and S.-C. Cheung, ``Relink: recovering links between bugs and changes,'' in Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering, pp. 15--25, 2011.


\bibitem{53}
M. D’Ambros, M. Lanza, and R. Robbes, ``Evaluating defect prediction approaches: a benchmark and an extensive comparison,'' Empirical Software Engineering, vol. 17, pp. 531--577, 2012.
\bibitem{54}
F. Peters and T. Menzies, ``Privacy and utility for defect prediction: Experiments with morph,'' in 2012 34th International Conference on Software Engineering (ICSE), pp. 189--199, 2012, organization={IEEE}.

\bibitem{55}
N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, ``SMOTE: synthetic minority over-sampling technique,'' Journal of Artificial Intelligence Research, vol. 16, pp. 321--357, 2002.

\bibitem{56}
Yan, M., Fang, Y., Lo, D., Xia, X., Zhang, X. . ``File-level defect prediction: Unsupervised vs. supervised models,'' In 2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement, pp. 344-353. 2017.

\bibitem{57}
Lewis, Chris, Zhongpeng Lin, Caitlin Sadowski, Xiaoyan Zhu, Rong Ou, and E. James Whitehead. "Does bug prediction support human developers? findings from a google case study." In 2013 35th International Conference on Software Engineering (ICSE), pp. 372-381. IEEE, 2013.

\bibitem{58}
Shin J, Aleithan R, Nam J, et al. "Explainable software defect prediction: Are we there yet?". arXiv preprint arXiv:2111.10901, 2021.

\bibitem{59}
Wang, Song, Taiyue Liu, and Lin Tan. “Automatically Learning Semantic Features for Defect Prediction.” In Proceedings of the 38th International Conference on Software Engineering. 2016.

\bibitem{RUS}
Feng, S., Keung, J., Xiao, Y., Zhang, P., Yu, X., and Cao, X. "Improving the undersampling technique by optimizing the termination condition for software defect prediction," Expert Systems with Applications, vol. 235, pp. 121084, 2024.

\bibitem{60}
Saleem R, Yuan B, Kurugollu F, Anjum A, Liu L. Explaining deep neural networks: A survey on the global interpretation methods. Neurocomputing. 2022;513:165-80.

\bibitem{61}
Bassan S, Amir G, Katz G. Local vs. Global Interpretability: A Computational Complexity Perspective. arXiv preprint arXiv:2406.02981. 2024 Jun 5.

\bibitem{YU2024}
Yu, J., Fu, M., Ignatiev, A., Tantithamthavorn, C., and Stuckey, P. "A Formal Explainer for Just-In-Time Defect Predictions," ACM Transactions on Software Engineering and Methodology, 2024, ACM New York, NY.

\bibitem{sharma2023}
Sharma, U., Sadam, R. "How far does the predictive decision impact the software project? The cost, service time, and failure analysis from a cross-project defect prediction model," Journal of Systems and Software, vol. 195, p. 111522, 2023, Elsevier.





\end{thebibliography}

\end{document}
