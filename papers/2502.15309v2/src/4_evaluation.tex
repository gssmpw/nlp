\section{Experiment}
\label{sec:experiment}
\subsection{Experiment Setups}
\label{subsec:experiment_setup}

\renewcommand{\arraystretch}{1.2}
\begin{table}[!t]
    \centering
    \begin{tabular}{lccc}\toprule
\multirow{2}{*}{\textbf{Methods}}      & \multicolumn{3}{c}{\textbf{Metrics}} \\
& mAcc$\uparrow$        & mIoU$\uparrow$   & F-mIoU$\uparrow$   \\ \midrule 

ConceptGraphs~\cite{gu2023conceptgraphsopenvocabulary3dscene}       & 39.43                    & 25.57   & 44.06     \\

ConceptGraphs-Detector~\cite{gu2023conceptgraphsopenvocabulary3dscene} & 41.18                        & 26.82       & 42.28             \\

HOV-SG~\cite{werby23hovsg}  & 39.95                     & 27.52        & \textbf{46.79}      \\

\textbf{DynamicGSG}   & \textbf{54.04}              & \textbf{31.06}               & 46.21               \\

\textbf{DynamicGSG} w/o feature loss   & 52.94                                & 25.97     & 37.32       \\
 \bottomrule
\end{tabular}
    \caption{\textbf{3D Open-vocabulary Semantic Segmentation on Replica~\cite{replica19arxiv}:} Attributed to 3D-2D Gaussian Object Association, DynamicGSG significantly outperforms the baselines in terms of both mAcc and mIOU. And the joint feature loss also effectively improve the mIOU and F-mIOU of semantic segmentation.}
    \label{tab:semseg}
    \vspace{-1em}
\end{table}

\begin{figure}[tb]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/4_evaluation/5.pdf}
  \caption{\textbf{Visualization of Feature Loss Ablation Experiments.}}
  \label{fig:feature_ablation}
      \vspace{-1em}
\end{figure}

To comprehensively evaluate DynamicGSG, we conduct a series of experiments using data sourced from Replica\cite{replica19arxiv}, ScanNet++\cite{yeshwanthliu2023scannetpp}, and real laboratory environments: (1) A quantitative comparison of 3D open-vocabulary semantic segmentation on the Replica dataset, contrasting our results with recent open-vocabulary scene graph construction methods, accompanied by an ablation study to investigate the contribution of joint feature loss. (2) A language-guided object retrieval experiment to evaluate the effectiveness of multi-layer scene graphs generated by DynamicGSG in capturing spatial-semantic object relationships. (3) Quantitative evaluation of scene reconstruction quality on Replica and ScanNet++ datasets. (4) Within our laboratory, we manually introduce environment changes to validate DynamicGSG's capability for dynamic updating of Gaussian scene graphs. 

All experiments are conducted on a desktop computer equipped with an Intel Core i7-14700KF CPU, an NVIDIA RTX 4090D GPU, and 32GB RAM. In all experiments, we set thresholds $ \delta_{\text{pix}} $ = 200, $ \delta_{\text{sim}} $ = 0.55 and $ \delta_{\text{change}} $ = 0.15.
%For real laboratory environments, we employ an Intel RealSense D455 to capture aligned RGB-D streams synchronized with inertial measurement unit (IMU) data. 

\renewcommand{\arraystretch}{0.9}
\begin{table}[!t]
    \centering
    % \scriptsize
    \setlength{\tabcolsep}{5pt}  
\begin{tabular}{llccccc}
    \toprule
    \textbf{Methods} & \textbf{Query}  & \textbf{Match} & \textbf{R@1} & \textbf{R@2} & \textbf{R@3}  \\
    \midrule
    \multirow{8}{*}{ConceptGraphs~\cite{gu2023conceptgraphsopenvocabulary3dscene}} & \multirow{3}{*}{Descriptive} & CLIP &  0.52 &  0.65 & 0.71  \\
                             &                              &  LLM  &  0.40 &  0.55 &  0.62 & \\
                             &                              & 
                             HSG  &  -- &  --   &  --   & \\
    \cdashmidrule{2-6}
                             & \multirow{2}{*}{Affordance}  &  CLIP &  0.60 &  0.63 &  \textbf{0.69} \\
                             &                              &  LLM  &  0.60 &  0.69 &  \textbf{0.80}& \\
    \cdashmidrule{2-6}
                             & \multirow{2}{*}{Negation}    &  CLIP &  0.17 &  0.49 &  0.60 \\
                             &                              &  LLM  &  0.77 &  \textbf{0.91} &  \textbf{0.97} & \\

    \midrule
    \multirow{8}{*}{\textbf{DynamicGSG}}     & \multirow{3}{*}{Descriptive} &  CLIP &  \textbf{0.64} &  \textbf{0.74}   &  \textbf{0.76}   \\
                             &                              &  LLM  &  \textbf{0.41} &  \textbf{0.57}   &  \textbf{0.64}  & \\
                             &                              & 
                              HSG  &  \cellcolor{Green!25}\textbf{0.71} &  \cellcolor{Green!25}\textbf{0.81}   &  \cellcolor{Green!25}\textbf{0.82}   & \\
    \cdashmidrule{2-6}                             
                             & \multirow{2}{*}{Affordance}  &  CLIP &  \textbf{0.60} &  \textbf{0.66} &  0.66  \\
                             &                              &  LLM  &  \textbf{0.65} &  \textbf{0.74} &  0.77 & \\
    \cdashmidrule{2-6}
                             & \multirow{2}{*}{Negation}    &  CLIP &  \textbf{0.34} &  \textbf{0.54}   &  \textbf{0.71}    \\
                             &                              &  LLM  &  \textbf{0.77} &  0.89   &  0.94   & \\

    \bottomrule
\end{tabular}

    \caption{\textbf{Language-guided Object retrieval on Replica\cite{replica19arxiv}.} CLIP, LLM, and HSG refer to Semantic-based match, LLM-based match, and Hierarchical scene graph-based match, respectively.}
    \label{tab:objret}
        \vspace{-1em}
\end{table}

\begin{figure}[tb]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/4_evaluation/6.pdf}
  \caption{\textbf{Qualitative Results of Object Retrieval:} DynamicGSG effectively locates objects that ConceptGraphs~\cite{gu2023conceptgraphsopenvocabulary3dscene} cannot retrieve through Hierarchical scene graph-based match.}
  \label{fig:obj_retrieval}
  \vspace{-1em}
\end{figure}


\subsection{3D Open-vocabulary Semantic Segmentation}
\label{subsec:semantic_segmentation}
To evaluate the quality of semantic embeddings in DynamicGSG and investigate how joint feature loss supervision influences Gaussian instance grouping, we perform an ablation experiment of the 3D open-vocabulary semantic segmentation on 8 scenes from the Replica dataset \cite{replica19arxiv} and quantitatively compare our results with recent scene graph construction methods. The primary baseline methods used for comparison are ConceptGraphs \cite{gu2023conceptgraphsopenvocabulary3dscene} and HOV-SG \cite{werby23hovsg}. For the ablation analysis, we include a variant of DynamicGSG without feature loss. All compared methods consistently adopt the ViT-H-14 CLIP backbone for semantic feature extraction.

To generate the semantic segmentation, we first calculate the CLIP text description vector for class-specific prompt formatted as ``an image of \{class label\}" corresponding to each class in the Replica dataset. For each scene, we compute the cosine similarity between the semantic feature of each object within the scene graph and the text description vector of each class. Each object's points or Gaussians are allocated to the class with the highest similarity score. Finally, the point clouds or Gaussians generated by all methods are transformed to the same coordinate as the ground-truth semantic point clouds. Quantitative evaluation is performed through standardized metrics, including mAcc, mIoU, and frequency-weighted mIoU. 

As shown in Tab. \ref{tab:semseg}, our method performs better than all baselines on mAcc and mIoU while achieving comparable performance to HOV-SG on F-mIoU. Object association in \cite{gu2023conceptgraphsopenvocabulary3dscene, werby23hovsg} relies on the overlap ratio between point clouds which suffers from a critical limitation: the potential association of small objects into nearby large objects due to high overlap ratios of 3D point clouds. Our method employs 3D-2D object association which effectively prevents spurious merging, as the small masks and large objects do not exhibit abnormal geometric similarity, ultimately yielding a significant enhancement in mAcc. And the joint feature loss also effectively regularizes the Gaussian instance grouping to improve the mIOU and F-mIOU. The qualitative results in Fig. \ref{fig:feature_ablation} further demonstrate that joint feature loss significantly enhances the regularization of intra-instance Gaussian grouping.

\renewcommand{\arraystretch}{1.2}
\begin{table}[!t]
    \centering
    \begin{tabular}{lccc}\toprule
\multirow{2}{*}{\textbf{Methods}}      & \multicolumn{3}{c}{\textbf{Metrics}}  \\
& PSNR$\uparrow$        & SSIM$\uparrow$   & LPIPS$\downarrow$  \\ \midrule 

HOV-SG~\cite{werby23hovsg}      & 19.69          & 0.821     & 0.284  \\

ConceptGraphs~\cite{gu2023conceptgraphsopenvocabulary3dscene} & 23.24                & 0.910             & 0.204      \\

SGS-SLAM~\cite{Li_2024}  & 35.43            & 0.978      & 0.077       \\

\textbf{DynamicGSG}   & \textbf{35.62}       & \textbf{0.979}              & \textbf{0.068}              \\
 \bottomrule
\end{tabular}
    \caption{\textbf{Quantitative Reconstruction Performance on Replica~\cite{replica19arxiv}:} DynamicGSG is comparable to the Gaussian-based semantic SLAM method~\cite{Li_2024} while significantly outperforming methods based on point cloud~\cite{gu2023conceptgraphsopenvocabulary3dscene,werby23hovsg}.}
    \label{tab:replica}
    % \vspace{-2em}
\end{table}



% \renewcommand{\arraystretch}{1.2}
% \begin{table}[t]
% \centering
% \scriptsize
% \setlength{\tabcolsep}{1.2pt}

% \begin{tabular}{@{}=l +c +c +c +c +c +c +c +c +c +c@{}}
% \toprule


% \textbf{Methods} & \textbf{Metrics} & \texttt{R0} & \texttt{R1} & \texttt{R2} & \texttt{Of0} & \texttt{Of1} & \texttt{Of2} & \texttt{Of3} & \texttt{Of4}  & \textbf{Avg.}\\


% \midrule

% \multirow{3}{*}{HOV-SG~\cite{werby23hovsg}} 
% & PSNR \higher         &  22.00 &                      21.88 &                      20.91 &  18.01 &                      15.66 &  18.39 &  20.97 &  19.77 &                      19.69\\
% & SSIM \higher         &                      0.881 &                      0.917 &  0.866 &  0.764 &                      0.548 &                      0.858 &  0.875 &                      0.866 &                      0.821\\
% & LPIPS \perflower         &  0.235 &  0.167 &                0.244 &                       0.367 &  0.568 &  0.242
%  &  0.213 &  0.239 &                      0.284\\

% \cdashmidrule{1-11}

% \multirow{3}{*}{ConceptGraphs~\cite{gu2023conceptgraphsopenvocabulary3dscene}}
% & PSNR \higher         &  22.79 &                      23.99 &                      24.54 &  24.00 &                      21.90 &  22.06 &  23.36 &  23.25 &                      23.24\\
% & SSIM \higher         &                      0.888 &                      0.927 &  0.921 &  0.928 &                      0.845 &                      0.931 &  0.927 &                      0.911 &                      0.910\\
% & LPIPS \perflower         &  0.216 &  0.158 &                      0.178 &                       0.222 &  0.353 &  0.147
%  &  0.152 &  0.207 &                      0.204\\

% % \multirow{3}{*}{Splatam~\cite{nice-slam}} 
% % & PSNR \higher         &  \cellcolor{tabsecond}36.64 &                      \cellcolor{tabsecond}34.55 &   35.85 &   37.95 &  \cellcolor{tabsecond}41.14 &  \cellcolor{tabsecond}40.89 &                      \cellcolor{tabsecond}33.51 &                     \cellcolor{tabsecond}33.13 &                      \cellcolor{tabsecond}36.14 \\
% % & SSIM \higher         &   0.980 &  \cellcolor{tabsecond}0.983 &  \cellcolor{tabsecond}0.978 &   0.988 &   0.990 &   0.987 &  \cellcolor{tabsecond}0.969 &  \cellcolor{tabsecond}0.972 &   0.975 \\

% % & LPIPS \perflower         &  \cellcolor{tabthird}0.064 &                      0.050 &  \cellcolor{tabthird}0.065 &  \cellcolor{tabthird}0.048 &  \cellcolor{tabthird}0.046 &  \cellcolor{tabthird}0.051 &  \cellcolor{tabthird}0.083 &  \cellcolor{tabthird}0.081 &  \cellcolor{tabthird}0.091 \\

% \cdashmidrule{1-11}

% \multirow{3}{*}{SGS-SLAM~\cite{Li_2024}} 
% & PSNR \higher  &  33.83 &  34.20 &  36.27 &  39.62 &  \textbf{40.60} &  32.82 &  31.93 &  \textbf{34.25} &  35.43\\
% & SSIM \higher  &  0.981 &  0.972 &  0.985 &  0.985 &  \textbf{0.987} &  0.969 &  \textbf{0.972} &  \textbf{0.969} &  0.978\\
% & LPIPS \perflower  & 0.062 &  0.092 &  0.065 &  0.067 &  \textbf{0.053} &  0.086 &  0.084 &  0.104 &  0.077\\

% \cdashmidrule{1-11}

% \multirow{3}{*}{\textbf{DynamicGSG}} 
% & PSNR \higher                           &  \textbf{33.88} & \textbf{34.66} & \textbf{36.31} & \textbf{40.42} &  40.45 &  \textbf{32.87} &  \textbf{32.12} &  34.21 &  \textbf{35.62}\\
% & SSIM \higher         &  \textbf{0.983} &  \textbf{0.976} &  \textbf{0.986} &  \textbf{0.989} &  0.986 &  \textbf{0.970} &  0.971 &  0.968  &  \textbf{0.979} \\
% & LPIPS \perflower                          &  \textbf{0.051} &  \textbf{0.075} &  \textbf{0.054} &  \textbf{0.045} &  0.057 &  \textbf{0.083} &  \textbf{0.079} &  \textbf{0.103} &  \textbf{0.068} \\

% \bottomrule
% \end{tabular}
% \caption{\textbf{Quantitative Reconstruction performance on Replica~\cite{replica19arxiv}:} DynamicGSG is comparable to the Gaussian-based semantic slam method\cite{Li_2024} while significantly outperforming methods based on point cloud\cite{werby23hovsg,gu2023conceptgraphsopenvocabulary3dscene}.}
% \label{tab:replica}
% \end{table}


\renewcommand{\arraystretch}{1.2}
\begin{table}[t]
\centering

\scriptsize
\setlength{\tabcolsep}{5.2pt}

\begin{tabular}{lcccccccc}
\toprule

\multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\textbf{Metrics}} & \multicolumn{3}{c}{\textbf{Train View}} & \multicolumn{3}{c}{\textbf{Novel View}} \\


 &   & \texttt{S1} & \texttt{S2} & \textbf{Avg.}  & \texttt{S1} & \texttt{S2} & \textbf{Avg.}\\


\midrule

\multirow{3}{*}{SplaTAM~\cite{keetha2024splatam}} & PSNR $\uparrow$   & 27.78 & 28.40 & 28.09  & 24.50 & 25.56 & 25.03\\
& SSIM $\uparrow$    & \textbf{0.946} & 0.944 & 0.945  & 0.896 & 0.892 & 0.894\\
& LPIPS $\downarrow$  & 0.121 & 0.129 & 0.125  & 0.210 & 0.255 & 0.233 \\

\cdashmidrule{1-8}

\multirow{3}{*}{\textbf{DynamicGSG}} & PSNR $\uparrow$  & \textbf{27.86} &  \textbf{28.47} &    \textbf{28.17}  & \textbf{24.81} & \textbf{25.65} & \textbf{25.19}\\
& SSIM $\uparrow$    &  0.945 &  \textbf{0.946} &  \textbf{0.946}  &  \textbf{0.902} &  \textbf{0.893} &  \textbf{0.898}\\
& LPIPS $\downarrow$   &  \textbf{0.120} &  \textbf{0.125} &  \textbf{0.123}  &  \textbf{0.202} &  \textbf{0.247} &  \textbf{0.225}\\


\bottomrule
\end{tabular}
\caption{\textbf{Novel \& Train View Synthesis Performance on ScanNet++~\cite{yeshwanthliu2023scannetpp}:} DynamicGSG not only provides photorealistic reconstruction on training views but also enables high-fidelity novel view synthesis at any camera pose.}
\label{tab:scannetpp}
      \vspace{-1em}
\end{table}

\subsection{Language-guided Object Retrieval}
\label{subsec:object_retrieval}

\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.95\linewidth]{figures/4_evaluation/7.pdf}
  \caption{\textbf{Visualization of Dynamic Updates}: (a) The backpack on the sofa and the bin are removed. (b) Holistic position update of the table and its contents (books and bottle). (c) The books and bottle exchange positions, and the backpack is moved to the chair. (d) The teacup disappears from the tea table and some fruits appear on the computer table.}
  \label{fig:dynamic}
\end{figure*}

To validate whether the multi-layer scene graphs constructed by DynamicGSG can effectively capture spatial and semantic object relationships, we conduct a language-guided object retrieval experiment employing diverse query types across three semantic complexity levels (\textbf{Descriptive}: E.g., ``These are some books and they are on the table."; \textbf{Affordance}: E.g., ``Something I can open with my keys."; \textbf{Negation}: E.g., ``Something to sit on other than a chair.") provided by ConceptGraphs \cite{gu2023conceptgraphsopenvocabulary3dscene} on the Replica dataset \cite{replica19arxiv}. Following ConceptGraphs, we select 20 Descriptive, 5 Affordance and Negation queries for each scene, ensuring each query corresponds to at least one ground-truth object.

We employ three distinct object retrieval methods: (1) Semantic-based match: Objects in the scene graph are selected based on the cosine similarity between their semantic features and the CLIP text embedding of the query. The object exhibiting the highest similarity is chosen. (2) LLM-based match: LLM (GPT-4o) is utilized to identify the object node within the scene graph that optimally corresponds to the query statement. (3) Hierarchical scene graph-based match: For descriptive queries incorporating inter-object relationships, DynamicGSG leverages multi-layer scene graphs and object semantic features to perform hierarchical matching.

% We employ three distinct methods to perform object retrieval: (1) Semantic matching through cosine similarity between CLIP text description vectors of query statements and object semantic features in scene graphs. (2) Use GPT-4o to determine which object node in the scene graph best matches the statement of the query. (3) For descriptive queries with relationship information between objects, DynamicGSG use its multi-layer scene graphs and node object CLIP semantic features for hierarchical matching.

Analysis of top-1, top-2 and top-3 recall across diverse query types in Tab. \ref{tab:objret} indicates that hierarchical matching substantially enhances accuracy for descriptive queries involving inter-object relationships, exhibiting a significant improvement over ConceptGraphs \cite{gu2023conceptgraphsopenvocabulary3dscene}. This performance improvement is qualitatively illustrated in Fig. \ref{fig:obj_retrieval},
where our hierarchical matching approach successfully localizes target objects that baseline fails to identify. Futhermore, LLMs demonstrate superior instruction comprehension for Affordance and Negation queries, which more closely approximate natural human language. DynamicGSG also facilitates earlier object localization within these queries. 

\subsection{Scene Reconstruction Quality}
\label{subsec:reconstruction_quality}

To evaluate reconstruction fidelity of DynamicGSG relative to recent scene graph construction methods, we establish an evaluation protocol across three metrics (PSNR, SSIM, and LPIPS) following SplaTAM \cite{keetha2024splatam}. For a fair comparison, all methods utilize the ground-truth camera poses provided by the datasets. 

% Tab. \ref{tab:replica} 中在 replica 数据集上的定量比较结果表明，our method significantly outperforms the approaches based on point cloud  while achieving comparable results to the advanced Gaussian semantic SLAM method SGS-SLAM\cite{Li_2024}, which utilizes ground-truth semantic annotations during optimization. 同时可以直观的从 Fig. 5 中看出，我们的方法与

The quantitative comparisons on the Replica dataset \cite{replica19arxiv}, presented in Table \ref{tab:replica}, demonstrate that our method significantly outperforms point cloud based methods \cite{gu2023conceptgraphsopenvocabulary3dscene,werby23hovsg} while achieving comparable results to the advanced Gaussian semantic SLAM method SGS-SLAM \cite{Li_2024}, which utilizes ground-truth semantic annotations during optimization. To further validate the reconstruction quality of DynamicGSG, we extend our evaluation to novel view synthesis with SplaTAM on two scenes (8b5caf3398, b20a261fdf) from the ScanNet++ dataset \cite{yeshwanthliu2023scannetpp}. The results in Tab. \ref{tab:scannetpp} show that our method marginally outperforms SplaTAM in challenging scenarios.

% 为了验证我们的方法对比近期的场景图重建工作有更好的细节重建质量，我们在 replica 上采取与类似 等SLAM 工作的验证方法, 所有的方法采用数据集的ground truth 位姿作为输入。如表 2 所示，我们的方法在 PSNR, SSIM, and LPIPS 上均远远高出基于点云的方法，并且与先进的高斯语义slam方法达到了相似的效果，并且SGS-slam使用了数据集提供的真实语义分割作为输入进行优化。 同时，我们在scannnt++数据集上进行了更近一步的实验，我们与splatam进行了新颖视角渲染的实验验证，如表3 我们的方法也同样达到了与其相当的水平。



\subsection{Real-world Dynamic Update}
\label{subsec:real_world}
% 我们在真实实验室环境中使用 realsense d455 部署视觉惯性里程计 vins-fusion 来获得带有粗略位姿估计的 RGB-D 数据。通过人为构建环境变化，来验证我们的方法能否实现对环境的动态更新。我们所构建的环境局部变化包括，物体消失，位置移动，新物体出现。如图 1,4,6 中的结果所示，DynamicGSG 能够正确发现环境中的局部变化，并能进行实例级别的，并在后续对高斯地图进行优化，从而构建一个动态，高保真度的 3D 高斯场景图。

% Table 发现并更新的成功率

\begin{table}[!t]
    \centering
    \begin{tabular}{lcc}\toprule
\multirow{1}{*}{\textbf{Types of Change}}      &            & \textbf{Success Rate (\%)}  \\ \midrule 

Object Disappearance      & 27 / 30              & 90.0  \\

Object Relocation & 25 / 30                     & 83.3      \\

Novel Object Emergence  & 19 / 20             & 95.0       \\
\midrule
\textbf{Total}   & 71 / 80                  & 88.8              \\
 \bottomrule
\end{tabular}
    \caption{\textbf{Success Rate of Dynamic Updates in Real-world.}}
    \label{tab:changes}
    \vspace{-2em}
\end{table}

To assess DynamicGSG's capability in adapting to dynamic environments, we establish 30 scenes in our lab with a total of 80 manual environment modifications, including object disappearance (30 instances), object relocation (30 instances), and novel object appearance (20 instances) and employ VINS-Fusion \cite{qin2019a} integrated with an Intel RealSense D455 to acquire aligned RGB-D streams at a resolution of 640×480, along with initial pose estimation.

As detailed in Tab. \ref{tab:changes}, DynamicGSG, while leveraging the method detailed in Sec. \ref{para:dynamic_update} to perform initial pose refinement and incrementally construct Gaussian scene graphs, successfully detects environment changes and executes corresponding dynamic updates, ensuring
temporal consistency between the scene graphs and the real-world environments. Some visualization results of the experiment, presented in Fig. \ref{fig:teaser} and \ref{fig:dynamic}, demonstrate DynamicGSG detects three types of environment changes and effectively performs instance-level local updates to construct dynamic, high-fidelity Gaussian scene graphs.

