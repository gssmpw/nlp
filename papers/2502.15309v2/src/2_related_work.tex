\section{Related Work}  \label{sec:related_work}
\subsection{3D Scene Graphs}
\label{sec:3d_scene_graph}
3D scene graph \cite{rosinol20203ddynamicscenegraphs,hughes2024foundations} offers a hierarchical graph-structured representation of the environment with nodes representing spatial concepts at multiple levels of abstraction (e.g., objects, places, rooms, buildings.) and edges preserving the spatial and semantic relationships between nodes. Taking advantage of the generalization abilities of vision foundation models \cite{cheng2024yoloworldrealtimeopenvocabularyobject,kirillov2023segment, liu2024groundingdinomarryingdino, zhang2023recognizeanythingstrongimage} and cross-modal grounding capabilities of vision-language models \cite{radford2021learningtransferablevisualmodels, zhai2023sigmoidlosslanguageimage} enables the construction of 3D scene graphs at the open-vocabulary level. Recent methods \cite{gu2023conceptgraphsopenvocabulary3dscene, werby23hovsg, linok2024barequeriesopenvocabularyobject, yan2025dynamicopenvocabulary3dscene, jiang2024roboexpactionconditionedscenegraph} construct 3D scene graphs embedded with VLM semantic features, facilitating object retrieval, robot manipulation and navigation. 
 
However, most of these methods \cite{gu2023conceptgraphsopenvocabulary3dscene, werby23hovsg,linok2024barequeriesopenvocabularyobject} rely on static environment assumptions and typically lack mechanisms for handling dynamic updates. RoboEXP \cite{jiang2024roboexpactionconditionedscenegraph} conducts dynamic scene updates based on object spatial relationships and the scene modifications resulting from robot operations. DovSG \cite{yan2025dynamicopenvocabulary3dscene} extends dynamic scene graphs construction to mobile agents and utilizes Large Language Models (LLMs) \cite{openai2024gpt4technicalreport} for task decomposition and planning, enabling robots to accomplish complex tasks in dynamic environments over the long term. To enhance computational efficiency, these methods often employ aggressive point cloud downsampling strategies, which significantly limits their capacity for high-fidelity geometric reconstruction of scene details.

\subsection{Gaussian-based Open-Vocabulary Scene Understanding}
\label{sec:gaussian_open_vocab_understand}
Recent progress in Gaussian Splatting~\cite{kerbl20233d, Huang2DGS2024, yu2024mip, guedon2024sugar, cheng2024gaussianpro} demonstrates outstanding performance in photo-realistic reconstruction with promising efficiency. The gradient-based optimization has also been applied in an online setting to incrementally construct the Gaussian map \cite{keetha2024splatam,yan2024gs, matsuki2024gaussian, wei2024gsfusiononlinergbdmapping} through differentiable rendering.

Concurrently, advancements in vision foundation models (such as SAM \cite{kirillov2023segment}, CLIP \cite{radford2021learningtransferablevisualmodels}, DINO \cite{oquab2024dinov2learningrobustvisual}) have motivated the exploration of integrating 2D semantic features into 3D Gaussians. LangSplat \cite{qin2024langsplat3dlanguagegaussian} learns hierarchical semantics using SAM and trains an autoencoder to distill high-dimensional CLIP features into low-dimensional semantic attributes of Gaussians.  GaussianGrouping \cite{ye2024gaussiangroupingsegmentedit} implements joint 3D Gaussian optimization based on 2D pre-matched SAM masks and 3D spatial consistency,  enabling high-quality scene reconstruction and open-vocabulary object segmentation. OpenGaussian \cite{wu2024opengaussianpointlevel3dgaussianbased} augments 3D Gaussian Splatting with point-level open-vocabulary understanding through SAM-based instance feature training, a two-level codebook for discretization, and instance-level 3D-2D feature association for geometric-semantic alignment. However,  Most of the above works require embedding semantic information into pre-trained Gaussian scenes or conducting data preprocess prior to gradient-based optimization. Such offline pipelines inherently conflict with robots' operational paradigms in dynamic settings.

% Feature3DGS\cite{zhou2024feature3dgssupercharging3d} proposes an N-dimensional Gaussian rasterizer to extract high-dimensional features from 2D foundation models for view-based tasks such as language-guided segmentation and editing.

To address these deficiencies, we incrementally construct the semantic Gaussian map using posed RGB-D sequences from public datasets or live cameras running Visual-Inertial Odometry (VIO) \cite{qin2017vins, qin2018online, qin2019a, qin2019b} frameworks, enabling it to achieve high-fidelity reconstruction and integrate rich semantic information to build a topological scene graph. Leveraging the rapid training and differentiable rendering capabilities of 3D Gaussians, our system can dynamically update both the Gaussian maps and scene graphs to adapt to changes in the real-world environments.