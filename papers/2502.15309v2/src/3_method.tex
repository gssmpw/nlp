\begin{figure*}
  \centering
  \includegraphics[width=1.0\textwidth]{figures/1_introduction/2.pdf}
  \caption{\textbf{Overview of DynamicGSG}: Our system processes posed RGB-D sequences, utilizes open-vocabulary object detection and segmentation models to obtain 2D masks, and extracts corresponding semantic features. In parallel, we employ instance-level rendering to get 2D masks and semantic features of objects in the map for object fusion. Subsequently, we perform Gaussian initialization and joint optimization to incrementally create a high-fidelity object-centric Gaussian map. Based on the spatial relationship of objects and the capabilities of LVLM, we construct a hierarchical scene graph to provide a structured description of the scene. In dynamic real-world scenarios, after refining the initial camera poses obtained from VINS-Fusion\cite{qin2019a}, we detect local changes and make corresponding modifications in the Gaussian map and scene graph for environment adaptation.%这句在实物实验里说好点？
}
  \label{fig: pipeline}
\end{figure*}

\section{Methodology}
\label{sec:methodology}
The proposed system constructs dynamic high-fidelity Gaussian scene graphs by splatting Gaussians onto 2D image planes in various forms, building semantic scene graphs, optimizing Gaussian maps, and making dynamic updates. An overview of DynamicGSG is presented in Fig.~\ref{fig: pipeline}.

\subsection{Gaussian Preliminaries}
\label{subsec:gaussians}
We densely represent the scene using isotropic Gaussian, which is an explicit representation parameterized by RGB color $\mathbf{c}$, center position $\boldsymbol{\mu} \in \mathbb{R}^3 $, radius $\mathbf{r}  \in \mathbb{R}^+$, and opacity $\mathbf{o} \in [0,1]$. The influence of each Gaussian on 3D space point $\mathbf{x} \in \mathbb{R}^3$ is defined as:
\begin{equation}
\label{eq:opacity}
f(\mathbf{x}) = \mathbf{o} \cdot \exp\left( \frac{\|\mathbf{x} - \boldsymbol{\mu}\|^2}{2\mathbf{r}^2}\right)
\end{equation}

The view synthesis and Gaussian parameters optimization are implemented through differentiable rendering with the Gaussian map and a camera pose $T \in SE(3)$. The color, depth, and visibility (accumulated opacity) of each pixel $\mathbf{u}$ at camera pose $T_t$ is determined by $\alpha$-blending contributions from depth-ordered projections of 3D Gaussians:
\begin{equation}
\label{eq:render_rgb}
\hat{C}_t(\mathbf{u})=\sum_{i=1}^n\mathbf{c}_if_i(\mathbf{u})\prod_{j=1}^{i-1}\left(1-f_j(\mathbf{u})\right).
\end{equation}
\begin{equation}
\label{eq:render_depth}
    \hat{D}_t(\mathbf{u}) =\sum_{i=1}^n d_if_i(\mathbf{u})\prod_{j=1}^{i-1}\left(1-f_j(\mathbf{u})\right),
\end{equation}
\begin{equation}
\label{eq:render_opacity}
\hat{S}_t(\mathbf{u}) =\sum_{i=1}^nf_i(\mathbf{u})\prod_{j=1}^{i-1}\left(1-f_j(\mathbf{u})\right),
\end{equation}
where $d_i$ is the depth of the i-th Gaussian center in the camera coordinate.

We preserve the basic properties of Gaussians while introducing two additional parameters: a low-dimensional instance feature $\mathbf{e} \in \mathbb{R}^3$ and an identifier $\mathbf{idx} \in \mathbb{N}^+$. We render the 2D instance feature $\hat{E} \in \mathbb{R}^{3\times H\times W}$ for each pixel in a differentiable manner similar to color blending:
\begin{equation}
\label{eq:render_feature}
\hat{E}_t(\mathbf{u})=\sum_{i=1}^n\mathbf{e}_if_i(\mathbf{u})\prod_{j=1}^{i-1}\left(1-f_j(\mathbf{u})\right).
\end{equation}


\subsection{Gaussian Objects Association}
\label{subsec:obj_association}
Our system processes posed RGB-D sequences $ I_t= \left \langle C_t, D_t, T_t \right \rangle  \in \{ I_1, I_2, \dots, I_t \} $ to construct a Gaussian scene graph: $\textbf{G}=\{O_{t}, E_t\}$, where $ O_t=\{ o_j \}_{j=1}^{J} $ and $ E_t=\{ e_k \}_{k=1}^{K} $ represent the sets of objects and edges, respectively. Each object $o_j$ is characterized by a set of Gaussians indexed by $ idx $ and a semantic feature vector $ f_{o_j} $.

\paragraph{Object Recognition}
\label{para:obj_recogniton}
We employ various advanced vision models to obtain instance-level semantic information from RGB images. For the current frame $I_t$, we first use an open-vocabulary object detection model (YOLO-World \cite{cheng2024yoloworldrealtimeopenvocabularyobject}) to obtain object bounding boxes $ \{\textbf{b}_{t,i}\}_{i=1}^{M}$. These detected proposals are subsequently processed by Segment Anything \cite{kirillov2023segment} to generate corresponding masks $ \{\textbf{m}_{t,i}\}_{i=1}^{M}$. Post-processing is then performed to ensure these masks do not overlap with each other. Finally, we use CLIP \cite{radford2021learningtransferablevisualmodels} to obtain each detection's semantic feature. Since CLIP's visual descriptors are image-aligned, each mask $\textbf{m}_{t,i}$ is passed to CLIP to extract an instance-level semantic feature $f_{t,i}$.

\paragraph{3D-2D Object Association}
\label{para:obj_association}

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/3_method/3.pdf}
  \caption{\textbf{3D-2D Gaussian Object Association.}}
  \label{fig:Association}
\end{figure}

For all detections $ D_t = \{ \textbf{d}_{t,i} \}_{i=1}^{M} $ in $ I_t $, we associate them with the map objects $ O_{t-1} $  through joint geometric and semantic similarity matching. As shown in %Fig. \ref{fig:Association}%, 
we first render all Gaussians into the current camera view at instance-level to obtain object masks  $\{\textbf{m}_{t,o_j}\}_{j=1}^{J}$. Objects $O_{t-1}$ whose projected masks contain more than $ \delta_{\text{pix}} $ pixels in the image plane are considered visible in the current frame. The geometric similarity is then formulated by the Intersection over Union (IoU) between all 2D detection masks $ \{\textbf{m}_{t,i}\}_{i=1}^{M}$ and projected masks of visible 3D objects $\{\textbf{m}_{t,o_j}\}_{j=1}^{J}$:
\begin{equation}
s_{\text{geo}}(i, j) = \frac{m_{t,i} \cap m_{t,o_j}}{m_{t,i} \cup m_{t,o_j}}
\end{equation}

The semantic similarity is calculated as the normalized cosine similarity between detection CLIP descriptors $\{f_{t, i}\}_{i=1}^{M}$ and visible object semantic features $\{f_{o_j}\}_{j=1}^{J}$. The joint similarity $ s(i,j) $ combines geometric and semantic similarity through weighted summation:
\begin{equation}
s_{\text{sem}}(i,j) = (f_{t,i})^\top f_{o_j} /2+1/2
\end{equation}
\begin{equation}
s(i,j) = w_g s_{\text{geo}}(i,j) + w_s s_{\text{sem}}(i,j)
\end{equation}
where $ w_g + w_s=1 $. Object association is performed through a greedy algorithm that assigns each detection to a visible object with the maximum similarity score. A new object is initialized when no visible object with similarity exceeding the threshold $\delta_{sim}$ is matched.

\paragraph{Object Fusion}
\label{para:obj_fusion}

If a detection $d_{t,i}$ is associated with a map object $o_j$, we fuse this detection with the corresponding object by updating the object’s semantic feature as:
\begin{equation}
f_{o_j}=(n_{o_j}f_{o_j}+f_{t,i})/(n_{o_j}+1)
\end{equation}
where $n_{o_j}$ denotes the number of $o_j$ that has been associated.

\subsection{Gaussian Map Optimization}
\label{subsec:gaussian_optim}
To incrementally construct the object-centric Gaussian map, new Gaussians must be initialized and subsequently optimized with observations to ensure our map is realistic and accurately group Gaussians belonging to each object.

\paragraph{Densification based on opacity}
\label{para:densification}

Firstly, new Gaussians will be dynamically initialized to cover newly observed regions and objects. We create a newly observed mask following~\cite{keetha2024splatam}, which identifies pixels exhibiting either insufficient transparency accumulation or emerging geometry occluding the existing scene structure: 
\begin{equation}
M_t = (\hat{S}_t < \lambda_s) \cup  ((D_t < \hat{D}_t) (|D_t - \hat{D}_t|) > \lambda_\text{MDE} )
\end{equation}
where $\lambda_s = 0.5$, $\lambda_{MDE}$ equals 50 times median depth error. For each pixel in $I_t$, we add a new Gaussian characterized by the pixel's color, aligned depth, and an opacity of 0.5. Each new Gaussian's $idx$ is assigned by the object's index after object association, and its instance feature $e$ is initialized using the value corresponding to $idx$ in the codebook, which contains 200 instance colors from ScanNet dataset~\cite{dai2017scannet}. As illustrated in the lower-right corner of Fig. \ref{fig: pipeline},
these instance features are then projected onto the 2D mask regions where the corresponding objects is located in $C_{t}$, forming a ground-truth instance feature $E_t  \in \mathbb{R}^{3\times H\times W}$.

\paragraph{Intra-instance Gaussian Regularization}
\label{para:regularization}

During gradient-based optimization, each attribute of Gaussians will be refined according to differentiable rendering. As shown in Fig. \ref{fig:feature_ablation},
we observe that Gaussians initially grouped by object indexes tend to progressively encroach into regions occupied by other objects, resulting in segmentation artifacts and reduced boundary precision. To address this limitation, we propose a novel feature consistency loss with GT instance feature $E_t$ to improve reconstruction fidelity and instance-level grouping accuracy:
\begin{equation}
L_\text{feature} = \lambda_1|E_t - \hat{E}_t| + \lambda_2(1-SSIM(E_t, \hat{E_t}))
\end{equation}
where $\lambda_1 = 0.8, \lambda_2 = 0.2$. As demonstrated in Fig. \ref{fig:feature_ablation}, our feature loss enforces intra-instance Gaussian consistency while boosting semantic segmentation precision.

\paragraph{Gaussian map optimization}
\label{para:map_optimization}
We minimize the joint loss, including color, depth, and features, to optimize the Gaussian map:
\begin{equation}
L_\text{color} = \lambda_3|C_t - \hat{C}_t| + \lambda_4(1-SSIM(C_t, \hat{C}_t))
\end{equation}
\begin{equation}
L_\text{depth} = |D_t - \hat{D}_t|
\end{equation}
\begin{equation}
L_\text{mapping} = w_c L_\text{color} + w_d L_\text{depth} + w_f L_\text{feature}
\end{equation}
where $\lambda_3 = 0.8, \lambda_4 = 0.2, w_c = w_f =0.5, w_d = 1.0$.

In the process of optimizing the Gaussian map, we perform multi-view scene optimization by collecting a list of keyframes to improve the quality of 3D reconstruction. A keyframe is selected and stored every $n$-th frame, and $m$ keyframes are chosen for multi-view optimization based on temporal distance and geometric constraints. Furthermore, we prune redundant Gaussians with near-zero opacity or large covariances as \cite{kerbl20233d}.

\subsection{Multi-layer Scene Graph Construction}
\label{subsec:scene_graph_generation}
With the well-reconstructed object-centric Gaussian map, we construct a hierarchical scene graph $\textbf{G}_t =\{O_{t}, E_t\}$ that reflects the structured description of the environment. As illustrated on the right side of Fig.~\ref{fig: pipeline}, objects are categorized into asset, ordinary, and standalone objects.

\paragraph{Asset objects}
\label{para:asset_layer}
We use a Large Vision Language Model (GPT-4o) to identify asset objects in indoor environments. The expected asset objects typically include furniture on the ground, such as chairs, tables, or cabinets, rather than small portable containers or decorative items (e.g., baskets, vases, or lamps). Our system takes the labels generated by Yolo-World\cite{cheng2024yoloworldrealtimeopenvocabularyobject} for objects in $O_t$ as input to GPT-4o. The asset objects, denoted as $ \tilde{O}_t  \subseteq O_t$, are classified through a specific prompt.

% Edges $E_t$ in the scene graph $\textbf{G}_t$ are determined by the spatial relationships between ordinary objects $\Bar{O}_{t}$ and asset objects . We define the edge $E_t(i,j)$ i

\paragraph{Ordinary objects}
\label{para:ordinary_layer}
We define ordinary objects $\Bar{O}_t$ through spatial relationships between an object and asset objects $\tilde{O}_{t}$. If the center position of an object is located above the spatial range of the asset object $\tilde{o}_{j} \in \tilde{O}_{t}$, we determine this object as $\Bar{o}_{i} \in \Bar{O}_t$ and  establish an edge $e_t(i,j) \in E_t$ between them. This edge not only represents the spatial relationship but also indicates that $\Bar{o}_{i}$ is carried by $\tilde{o}_{j}$.

\paragraph{Standalone objects}
\label{para:standalone_layer}
Standalone objects are defined as the complement of asset and ordinary objects, represented as $\hat{O}_t \subseteq (O_t-\tilde{O}_t - \Bar{O}_t)$. Typical instances include stools, windows, and similar entities that exhibit inherent independence in operational environments.

Additionally, asset and standalone objects are directly connected to the room node within the scene graph $\textbf{G}_t$.

\subsection{Dynamic Scene Update}
\label{para:dynamic_update}
% 在机器人的真实工作场景中，由于机器人与人和其他机器人在该环境中的协作，该场景中物体的位置和物体之间的关系经常发生改变，如果机器人不能及时更新指导其执行任务的场景图，那么他将很难成功的完成后续的任务，这在过去的工作中是未被考虑的。为了缓解这个问题，我们利用高斯的快速训练和实时渲染，结合实时的 RGBD 观测来进行高斯地图和场景图的动态更新，如图 5 所示。due to the collaboration between robots and humans.

% In dynamic human-robot collaborative environments, changes in positions of objects and their interrelations present significant challenges for robot task execution， 利用高斯的高保真度重建和快速可微分渲染结合实时的 RGBD 观测，我们实现了一种发现场景变化进而在实例级别对高斯地图与场景图进行局部更新的方法，保证机器人内存中的环境表征与真实场景之间的时间一致性。
In dynamic human-robot collaborative environments, the changes in object positions and their spatial interrelationships present significant challenges for long-term task execution. Leveraging high-fidelity reconstruction and fast differentiable rendering of 3D Gaussian, we implement a dynamic update mechanism to detect changes and locally update the Gaussian map and scene graph at the instance level with real-time RGB-D observations, ensuring temporal consistency between the environment representations in the robot's memory and the physical workspace, as shown in Fig. \ref{fig:teaser} and \ref{fig:dynamic}.

\paragraph{Refine Camera Tracking}
\label{para:camera_tracking}
% 在公开的数据集或仿真环境中，我们的系统可以较为轻松的读取带有相机GT位姿的 RGBD 帧作为构建高斯场景图的输入，但是在真实的动态场景中准确的相机位姿估计会对高斯场景图的构建和更新产生重大影响。在真实环境中，我们使用 realsense D455 运行视觉惯性里程计 vins-fusion 来得到每一帧的相机位姿初步估计 $est\_I$。我们以该估计位姿作为优化参数，通过计算当前帧深度与颜色和渲染得到的深度和颜色帧之间的 L1 Loss 来进一步优化高斯位姿参数，此时将高斯的参数固定：
In public datasets or simulation environments, our system can directly utilize RGB-D frames with ground-truth poses as input. In real-world scenarios, the lack of reliable pose priors introduces fundamental constraints on the construction and modification of Gaussian scene graphs. To address this sensing gap, we deploy a validated and efficient VIO\cite{qin2017vins} framework to acquire an initial estimate $T_{t,est}$ and execute an iterative pose refinement by minimizing the L1 loss between aligned color $C_{t}$ and depth $D_{t}$ from the camera and their rendered views to obtain precise camera pose $T_{t}$:
\begin{equation}
    L_\text{tracking} = (\hat{S}_t > \lambda_r)(\lambda_5|C_t - \hat{C}_t| + \lambda_6|D_t - \hat{D}_t|)
\end{equation}
where $\lambda_r = 0.99, \lambda_5 = 0.5, \lambda_6 = 1.0$. During this stage, the parameters of the Gaussians are fixed.

\paragraph{Update Local Gaussian Map}
\label{para:update_map}
Using the refined pose $T_{t}$, our system begins to detect local changes, including the disappearance, displacement, and emergence of objects. Subsequently, we will modify the map and alter the scene graph at the instance level.

To address the disappearance and movement of objects, we render all objects where over 50\% of their Gaussians appear within the current camera frustum to acquire visible object RGB masks $\{m_{t, o_j}^{rgb}\}_{j=1}^{J}$ and compute structural similarity (SSIM) between $\{m_{t, o_j}^{rgb}\}_{j=1}^{J}$ and the corresponding regions within RGB observation $C_t$:
\begin{equation}
S(j) = SSIM(m_{t, o_j}^{rgb}, C_t(m_{t, o_j}))
\end{equation}
If $ S(j) < \delta_{\text{change}} $, it indicates that object $o_j$ has either been moved or substituted by another object. We categorize all disappeared objects as $O_\text{delete}$ and remove their associated obsolete Gaussians. Next, new objects $O_\text{appear}$ will be instantiated following Sec. \ref{subsec:obj_association}, based on the recalculated similarity matrix $s(i, j)$ between $\{\textbf{d}_{t,i}\}_{i=1}^{M}$ and $\{\textbf{o}_{t-1,j}\}_{j=1}^{J'} \subseteq \{O_{t-1} - O_\text{delete}\}$. The union of deleted and new objects constitutes the update set $O_\text{update} = \{O_\text{appear}, O_\text{delete}\}$.

Finally, if $O_\text{update} \neq \varnothing$, our system will clear the keyframe list to prevent outdated keyframes from participating in the subsequent Gaussian map optimization detailed in Sec. \ref{subsec:gaussian_optim}. Otherwise, we shall skip this stage.



\paragraph{Update Scene graph}
\label{para:update_scene_graph} 
% 当高斯地图完成局部更新后，需要对整体场景图的一部分子图进行相应的局部更新，我们将局部地图中更新的物体记为 O_{update} = {O_delete, O_appear}。 , 对于被删除的物体 O_delete，如果该物体是一个普通物体，只需要将其和与父节点对应的边从场景图中删除；如果该物体是一个可承载物体，就需要将其所有子结点和对应的边从场景图中删除。对于新增加的物体，我们通过 3.C 中的方法判断该物体的类型，并首先在场景图中新增可承载物体，之后通过计算所有新增普通物体与可承载物体的空间关系来确定其在场景图中的位置，并更新或增加相应的边。

Following the local updates of the Gaussian map, the scene graph $\mathbf{G}_{t-1}$ requires corresponding adjustments based on the categories of objects in $O_\text{update}$. For the deleted objects $O_\text{delete}$, if an object is marked as ordinary or standalone, it is sufficient to delete it along with the parent edge $e_k$ from $\textbf{G}_{t-1}$; in contrast, if the object is an asset object, the removal must extend to all its child nodes and their associated edges. For the newly appeared objects $O_\text{appear}$, we determine their type using the method described in Sec.~\ref{subsec:scene_graph_generation}. Subsequently, we insert nodes of corresponding types and establish relational edges within $\textbf{G}_{t-1}$ to construct the updated scene graph $\textbf{G}_t$.

% . Then, we calculate the spatial relationships between all newly added ordinary objects and the asset ones to determine the edges that need to be added, 