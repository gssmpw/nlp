\section{Study 1: Instructor Survey}

To understand the use of unsanctioned apps and devices in K-12 and university settings, we conducted a survey in November and December 2024 targeting educators who rely on personally acquired technologies for teaching-related activities. The following sections detail the methodologies and findings of this study.

\subsection{Methods}

\paragraph{Survey design.}
We designed an online survey to capture the use of unsanctioned apps for teaching and grading activities. 
This survey included a mix of open-ended and multiple-choice questions, structured to gather comprehensive insights into app usage, institutional awareness, and data management practices.

After obtaining consent, participants were first asked to list at least three apps (for mobile, web browsers, or desktop computers) that they have used for teaching and grading-related activities and why.
Follow-up questions explored participants’ perceptions and experiences with their listed apps and their knowledge of institutional policies regarding unsanctioned app use.
The survey concluded by asking participants about the data management practices of discontinued apps, specifically whether these apps provided the option to delete user data and if participants had utilized this feature % Lastly, they were asked about using personal devices (e.g., mobile phones) for teaching activities 
(full questionnaire in \S~\ref{app:study1ques}).

%, and we adhered rigorously to ethical and privacy standards recommended for human subjects studies. %This included anonymizing personally identifiable information (PII) that participants may have added to their survey, and any other information that could reveal any identity during the survey intake. 

\paragraph{Participant recruitment.}
The survey was conducted using the Qualtrics \cite{Qualtrics} platform. We recruited participants through Prolific, a well-known platform for recruiting participants from a diverse pool. 
The study received Institutional Review Board (IRB) exempt status, as it posed minimal risk to participants. 

Selection criteria ensured that participants were current or former educators teaching at K-12 schools, community colleges, colleges, or universities, with prior experience using unsanctioned apps for teaching, grading, or related activities. 
% Since we do not plan to conduct any statistical hypothesis tests with this survey data, we did not have any minimum sample size requirement to achieve a certain effect size. Rather, our goal was to collect a reasonably large number of applications participants have used and diverse experiences and insights. Thus, we collected data in batches of 50 samples and reviewed the data before collecting the next batch. After 450 samples, data started to saturate....
Each participant was compensated $\$5$ for completing the survey. Our survey was designed and checked to take approximately 10 minutes. %an hour based on the length of time it took for them to complete the survey (\rakib{need to report the median time to complete, and how much we actually paid to each participant}).

% We opened the survey for responses in batches to allow for the authors to review incoming responses for duplicates or AI generated responses.
% In total, the survey acquired 432 responses, 283 being from K-12 educators and 149 from university educators between October 2024 and December 2024. 

% All of our participants had experience in using tools not acquired by their institution and integrating them into their workflow. 

 
% Surveys were conducted until data saturation in the reasons behind the use of technologies was reached along from both groups – K-12 and University educators, which signified the point where no new themes or insights emerged from the data. It was not based on the total number of applications or unique applications from participants as there are thousands of educational technologies for educators to use. \rakib{the last two sentences were unlcear to me. the reasons behind using apps probably are not very many, and would not require more than 50 samples. but with 432 responses, i think we at least saw many repetitions of the apps, and the popular apps were already identified?}

% \paragraph{Data analysis.}
% Completed surveys underwent a content analysis procedure for the short and long answer questions. 
% Applications that participants listed were sanitized \rakib{need to explain how did you sanitize/cleaned} and cleaned to acquire a list of all applications that listed and find the total unique applications that were used by participants. 
% Questions that asked the participants to list reasons for their choice were sanitized and ran through a program to pull the most used words to describe applications and reasons for using other applications.
% For the questions asking for participants to define in their own words security and privacy terms, the authors went through and systematically read each respondent and either confirmed or denied whether the statement was accurate to a standard definition \hlfixme{report these definitions}. 

% Given the qualitative nature of our study, we primarily present our findings qualitatively, occasionally supplementing with counts to highlight prevalent patterns

% \paragraph{Study limitations.}

% While our study provides insights into the prevalence of unsanctioned EdTechs at institutions, several limitations warrant consideration. \hlfixme{what are these?}

\subsection{Results}
