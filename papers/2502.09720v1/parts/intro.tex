\section{Introduction}

\ifisicml
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/perp_plot.pdf}
    \caption{Perplexity of quantized models for three regimes (weight-only, weights + KV cache, end-to-end) on wikitext2 vs number of bits per entry.}
    \label{fig:main-plot}
\end{figure}

\else
\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figures/perp_plot.pdf}
  \end{center}
  \caption{Perplexity of quantized models for three regimes (weight-only, weights + KV cache, end-to-end) on wikitext2 vs number of bits per entry.}
  \vspace{-5ex}
\label{fig:main-plot}
\end{wrapfigure}

\fi

\ifisicml
\begin{table*}[t]
\centering
\begin{adjustbox}{max width=2\columnwidth} % Ensures the table fits
\begin{tabular}{lccccccccccc}
    \toprule
\textbf{Model} & \textbf{Bits} $\downarrow$ & \textbf{Bits (no zstd)} $\downarrow$ & \textbf{ARC-C} $\uparrow$ & \textbf{ARC-E} $\uparrow$ & \textbf{Hellaswag} $\uparrow$ &  \textbf{PIQA} $\uparrow$ & \textbf{Winogrande} $\uparrow$ & \textbf{Zero-shot Avg} $\uparrow$ & \textbf{Wikitext2 ppl} $\downarrow$ \\ 
% \textbf{OBQA} $\uparrow$ &
\midrule
    Baseline (FP16) & 16 & 16 & 0.54 & 0.78 & 0.79 & 0.81 & 0.74 & 0.73& 6.1\\
    \midrule
    \textbf{Weights only} \\
    LLM-QAT & 4.00 & - & 0.51 & 0.77 & 0.48 & 0.79 & 0.72 & 0.65 & 7.7\\
    GPTQ & 4.00 & - & 0.47 & 0.72 & 0.74 & 0.77 & 0.71 & 0.68 & 7.2\\
    SpinQuant          & 4.00& -&\textbf{0.54}& 0.77& 0.78 & \textbf{0.80}& 0.72& \textbf{0.72}& 6.5\\
    NestQuant $q=14,k=4$ (ours)           & \textbf{3.99}& 4.06 & 0.53 & \textbf{0.78} & \textbf{0.79} & \textbf{0.80} & \textbf{0.73} & \textbf{0.72}& \textbf{6.3}\\
    \midrule
    \textbf{Weights + KV cache} \\
    SpinQuant & 4.00& - & 0.51& 0.77& 0.77& 0.78& 0.69& 0.70& 6.6 \\
    NestQuant $q=14,k=4$ (ours)           & \textbf{3.99}& 4.06 & \textbf{0.53} & \textbf{0.78} & \textbf{0.79} &  \textbf{0.79} &\textbf{ 0.74} & \textbf{0.72}& \textbf{6.4}\\
    \midrule
    \textbf{Weights, KV cache, activations} \\
    LLM-QAT & 4.00 & - & 0.27 & 0.41 & 0.38 & 0.60 & 0.53 & 0.44 & 52.5 \\
    Quarot & 4.00& - & 0.44 & 0.67 & 0.75 & 0.75 & 0.66 & 0.67 & 8.4 \\
    SpinQuant & 4.00& - & \textbf{0.51}& \textbf{0.75}& 0.75&0.77& 0.66& 0.68& 7.3 \\
    NestQuant $q=14,k=4$ (ours)     & \textbf{3.99}& 4.06 & \textbf{0.51} & \textbf{0.75} & \textbf{0.78} & \textbf{0.79} & \textbf{0.72} & \textbf{0.71}& \textbf{6.6}\\
    \bottomrule
\end{tabular}
\end{adjustbox}
\caption{4-bit quantization of Llama-3-8B. The bits column for NestQuant corresponds to actually measured average number of bits per entry (when a vector of auxiliary scaling coefficients $\beta$ is compressed via zstd) and the second column shows quantization rate when no compression step is used.} 
\label{tab:llama3_8b}
\end{table*}
\fi

There are three principal goals of post-training quantization (PTQ). First, reducing the number of bits per parameter allows for loading big models on cheap GPUs with limited memory, thus democratizing access to LLMs. This requires ``weights-only'' quantization algorithms of which the most popular are AWQ, GPTQ, and QuIP (see references in Section~\ref{sec:llm_review}). 

The second goal of PTQ is to accelerate inference. In LLMs most of the compute is spent multiplying matrices. Multiplying a pair of such matrices requires $2n^3$ FLOPs and ${3\over 8}Rn^2$ bytes to exchange between the core and memory (here and below $R$ designates the number of bits required to store each entry of a vector/matrix). So when matrices are large (such as during the pre-fill phase when the prompt is processed) the GPU is compute-bound, while when $n$ is small (such as during generation) the GPU becomes memory-bound. To achieve this goal one needs to reduce $R$ by quantizing both weights and the KV cache.

The third goal of PTQ is to accelerate inference of giant LLMs that require hosting each layer on
a separate GPU (pipelining parallelism). For this goal one needs to quantize activations passed
from one layer to the next to reduce the communication bottleneck.

While quantization of weights to $R=3,4$ and even $R=2$ bits has been achieved with minimal loss of quality, quantization of KV cache and activations has been much more challenging. 
Popular algorithms for full quantization are LLM.int8(), SmoothQuant and SpinQuant (see references in Section~\ref{sec:llm_review}),  the latter having state-of-the-art performance. This work proposes an alternative algorithm (NestQuant) for quantizing weights, KV-cache, and activations. The algorithm is motivated by recent theoretical work on approximate matrix multiplication and follows several classical ideas such as the Conway-Sloane algorithm for the Gosset lattice. 

\subsection{Summary of results}

\ifisicml
\else
\begin{table*}[h]
\centering
\begin{adjustbox}{max width=\textwidth} % Ensures the table fits
\begin{tabular}{lccccccccccc}
    \toprule
\textbf{Model} & \textbf{Bits} $\downarrow$ & \textbf{Bits (no zstd)} $\downarrow$ & \textbf{ARC-C} $\uparrow$ & \textbf{ARC-E} $\uparrow$ & \textbf{Hellaswag} $\uparrow$ &  \textbf{PIQA} $\uparrow$ & \textbf{Winogrande} $\uparrow$ & \textbf{Zero-shot Avg} $\uparrow$ & \textbf{Wikitext2 ppl} $\downarrow$ \\ 
% \textbf{OBQA} $\uparrow$ &
\midrule
    Baseline (FP16) & 16 & 16 & 0.54 & 0.78 & 0.79 & 0.81 & 0.74 & 0.73& 6.1\\
    \midrule
    \textbf{Weights only} \\
    LLM-QAT & 4.00 & - & 0.51 & 0.77 & 0.48 & 0.79 & 0.72 & 0.65 & 7.7\\
    GPTQ & 4.00 & - & 0.47 & 0.72 & 0.74 & 0.77 & 0.71 & 0.68 & 7.2\\
    SpinQuant          & 4.00& -&\textbf{0.54}& 0.77& 0.78 & \textbf{0.80}& 0.72& \textbf{0.72}& 6.5\\
    NestQuant $q=14,k=4$ (ours)           & \textbf{3.99}& 4.06 & 0.53 & \textbf{0.78} & \textbf{0.79} & \textbf{0.80} & \textbf{0.73} & \textbf{0.72}& \textbf{6.3}\\
    \midrule
    \textbf{Weights + KV cache} \\
    SpinQuant & 4.00& - & 0.51& 0.77& 0.77& 0.78& 0.69& 0.70& 6.6 \\
    NestQuant $q=14,k=4$ (ours)           & \textbf{3.99}& 4.06 & \textbf{0.53} & \textbf{0.78} & \textbf{0.79} &  \textbf{0.79} &\textbf{ 0.74} & \textbf{0.72}& \textbf{6.4}\\
    \midrule
    \textbf{Weights, KV cache, activations} \\
    LLM-QAT & 4.00 & - & 0.27 & 0.41 & 0.38 & 0.60 & 0.53 & 0.44 & 52.5 \\
    Quarot & 4.00& - & 0.44 & 0.67 & 0.75 & 0.75 & 0.66 & 0.67 & 8.4 \\
    SpinQuant & 4.00& - & \textbf{0.51}& \textbf{0.75}& 0.75&0.77& 0.66& 0.68& 7.3 \\
    NestQuant $q=14,k=4$ (ours)     & \textbf{3.99}& 4.06 & \textbf{0.51} & \textbf{0.75} & \textbf{0.78} & \textbf{0.79} & \textbf{0.72} & \textbf{0.71}& \textbf{6.6}\\
    \bottomrule
\end{tabular}
\end{adjustbox}
\caption{4-bit quantization of Llama-3-8B. The bits column for NestQuant corresponds to actually measured average number of bits per entry (when a vector of auxiliary scaling coefficients $\beta$ is compressed via zstd) and the second column shows quantization rate when no compression step is used.} 
\label{tab:llama3_8b}
\end{table*}
\fi

The NestQuant algorithm is described in Section \ref{sec:details}. NestQuant is a generic drop-in replacement for any matrix multiplication. Its performance for synthetic random Gaussian matrices comes pretty close to information-theoretic limits (see Fig.~\ref{fig:synth}) and significantly outperforms uniform quantization employed by SpinQuant. Switching from a scalar (uniform) quantization to vector quantization requires some price to pay computationally (Section~\ref{sec:runtime}), however, among vector quantizers NestQuant is rather economical as it operates in dimension 8 and leverages an elegant algorithm of~\citep{ConwaySloane83}.

Applying NestQuant to quantizing an actual LLM (Llama-3-8B) shows massive end-to-end improvement: Fig.~\ref{fig:main-plot} shows a significant reduction of perplexity compared to SpinQuant; and Table~\ref{tab:llama3_8b} confirms enhanced performance on standard LLM benchmarks.

The main source of improvement of NestQuant is demonstrated in Fig.~\ref{fig:shaping} (although NestQuant uses an 8-dimensional Gosset lattice, not a 2D hexagonal one). More details on this as well as directions for improvement are discussed in Section~\ref{sec:outline}.

Thus, we believe that NestQuant offers an excellent alternative to other algorithms. It quantizes weights, KV-cache and activations, achieves significant improvement on both synthetic and real data.

\subsection{Paper organization}

We start with a detailed review of classical work on vector quantization and modern LLM quantization (Section~\ref{sec:prior}). Then in Section~\ref{sec:outline} we explain the motivation for each step of the algorithm. Section~\ref{sec:details} contains the pseudocode of the algorithm and diagram of quantized LLM. Finally, Section~\ref{sec:exper} concludes with details about empirical performance. Further details and evaluations are relegated to the Appendices.
