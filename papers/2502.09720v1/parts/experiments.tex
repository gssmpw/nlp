\section{Experiments}\label{sec:exper}
\subsection{Simulated Data}\label{sec:synth}
    We compared the mean $L_2$ loss per entry of SpinQuant to the uniform $L_\infty$-scaling quantizer (used in SpinQuant and other methods).
    The mean $L_2$ loss per entry for the product of two matrices $A \in \mathbb{R}^{n\times k}, B\in \mathbb{R}^{m\times k}$ is computed as $\frac{\lVert AB^T - \hat{A}\hat{B}^T\rVert_2}{nm}$.
    We set $n=k=m=4096$ and sampled two matrices $A,B$ with unit normal distribution $A_{ij},B_{ij} \sim \mathcal{N}(0,1)$. 
    We compare to the lower bound from \eqref{eq:lbnd}.

For NestQuant, we do a grid search over $(q, k)$. For given $q$ and $k$, we find the best subset in $\{\frac{m}{2} \text{ for } m = 1, 2, \ldots, 50\}$ of scaling coefficients $\beta$ of size $k$ using the algorithm from Appendix \ref{dp-section}. Then we calculate the expected bits per entry computed as $\log_2{q}+\frac{1}{8}\sum_{i=1}^kp(\beta_{i})\log_2 p(\beta_{i})$ where $p(\beta_{i})$ is the probability that the $i$'th beta is used in quantization. In Figure \ref{fig:synth}, we plot the efficient frontier of bits per entry vs root mean $L_2$ loss.

\subsection{Llama results}

We quantize Llama-3-8B model \cite{grattafiori2024llama3herdmodels} using different values of $q$. We choose the number of scaling coefficients ($k$) to be equal to $4$, the Section \ref{sec:k-choice} explains the rationale behind this choice. More details on the hyperparameter choice of the experiments are in Appendix \ref{sec:hyperparams}. For each experiment, we compute the number of bits per entry similar to Section \ref{sec:synth}, but for the setup of compressed $\beta$ indices, we run the zstd compression algorithm instead of using the entropy of the distribution. As our evaluation metric, we use the perplexity on the validation split of wikitext2 with context length $2048$.

We also perform the evaluation of NestQuant on various zero-shot benchmarks: ARC-Easy and ARC-Challenge \cite{clark2018arc}, Hellaswag \cite{zellers2019}, \cite{bisk2019piqa}, and Winogrande \cite{sakaguchi2019winogrande}. The results on 4-bit models with comparisons to other models are summarized in Table \ref{tab:llama3_8b}.

\begin{table}
\centering
\scriptsize
\begin{tabular}{lcccccc}
    \toprule
\textbf{q}& \textbf{Bits} & \textbf{Bits (no zstd)} &  \textbf{W}&
\textbf{W + KV}&
\textbf{W + KV + A}\\    \midrule
    14 & 3.99& 4.06 & 6.308& 6.379& 6.633\\
    12 & 3.76& 3.83 & 6.376& 6.475& 6.841\\
    10 & 3.50& 3.57 & 6.486& 6.640& 7.251\\
    8 & 3.18& 3.25 & 6.700& 6.968& 7.989\\
    \bottomrule
\end{tabular}
\ifisicml\else\vspace{1em}\fi
\caption{Wikitext2 perplexity of NestQuant quantization of Llama-3-8B at different rates. The "bits" column is the bit rate per entry with zstd compression of scaling coefficients, and "bits (no zstd)" is the bit rate without compression. The "W", "W+KV", and "W+KV+A" describe the quantization regime (whether weights, KV cache, or activations are quantized). The perplexity of non-quantized model is $6.139$} 
\label{tab:llama3_ppl}
\end{table}


\subsection{Comparison to other methods}
We compare our method to the current state-of-the-art method \textit{SpinQuant}. On the WikiText2 dataset, we computed the perplexity scores of the quantized models on context size 2048. Our method demonstrates superior perplexity scores by a high margin. On W4KV4A4 (4-bit weights, KV-cache, and activations) quantization of Llama 3-8B we achieve a perplexity score of 6.6 compared to the reported score of 7.3 in \textit{SpinQuant} (See table \ref{tab:llama3_8b}). Impressively, our method outperforms \textit{SpinQuant}, without the need of learned rotations. Even without LDLQ, we achieve a perplexity score of 6.8, which is still better than SpinQuant.

\subsection{Results for other models}

Here, we show the results of NestQuant on the newer 1B parameter model LLama3.2-1B. We do experiments in the same setups as for the Llama-3-8B model, computing the wikitext2 perplexity.

\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{lcccccc}
    \toprule
\textbf{q}& \textbf{Bits} & \textbf{Bits (no zstd)} &  \textbf{W}&
\textbf{W + KV}&
\textbf{W + KV + A}\\    \midrule
    14 & 3.99& 4.06& 10.061& 10.529& 11.197\\
    12 & 3.76& 3.837& 10.178& 10.862& 11.910\\
    10 & 3.50& 3.57& 10.377& 11.552& 14.191\\
    8 & 3.18& 3.25& 10.850& 13.309& 18.710\\
    \bottomrule
\end{tabular}
\ifisicml\else\vspace{1em}\fi
\caption{Wikitext2 perplexity of NestQuant quantization of Llama-3.2-1B. The format of the table is the same as in Table \ref{tab:llama3_ppl}. The perplexity of non-quantized model is 9.749} 
\label{tab:llama1b_ppl}
\end{table}

\subsection{The choice of $k$}
\label{sec:k-choice}

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/k_plot.pdf}
    \caption{The perplexity-bitrate scaling of NestQuant with different values of $k$, all components of the model (weights, KV cache, activations) are quantized}
    \label{fig:k-plot}
\end{figure}

The value of $k$, i.e. the number of scaling coefficients is an important hyperparameter of the algorithm. With an increase of $k$, we decrease the quantization error by allowing each vector to be quantized to the lattice point with a proper scaling. However, it increases the bitrate and makes the encoding slower, since we need to try a larger number of scaling coefficients.

We used $k = 3,4,5,8$ to quantize Llama-3-8B across different values of $q$, plotting the resulting perplexity against bitrate in Figure \ref{fig:k-plot}. We can see that using $k=3$ leads to a suboptimal performance of the quantization scheme, while the performances of $k=4,5,8$ are comparable. In our experiments, we use $k=4$, because lower having $k$ results in faster encoding.

More ablation studies for LDLQ and the choice of rotation are described in Appendix \ref{sec:ablation}.
