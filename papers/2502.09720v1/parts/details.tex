\section{Detailed Method}\label{sec:details}

\subsection{Nested lattice codebook}

In this section, we describe the construction for a Vector Quantization (VQ) codebook of size $q^d$ for quantizing an $d$-dimensional vector, where $q$ is an integer parameter. To quantize a vector, we find the closest codebook element by Euclidean norm. We describe efficient encoding and decoding algorithms to a quantized representation in $\Z_q^d$.

Let $\Lambda$ be a lattice in $\RR^d$ with generator matrix $G$. We define the coordinates of a point $x \in \Lambda$ to be an integer vector $v$ such that $x = Gv$. Each point $P \in \Lambda$ has a corresponding Voronoi region $\m{V}_\Lambda(P)$, for which $P$ is the closest point in $\Lambda$ with respect to $L^2$ metric. To define the codebook, we consider the scaled lattice $q\Lambda$. Then:

\begin{definition}
    $x \in \Lambda$ belongs to codebook $C$ iff $x \in \m{V}_{q\Lambda}(0)$. Let $v$ be the coordinates of $x$. Then, the quantized representation of $x$ is $\mathcal{Q}(x) := v \mmod q$. Note that $\mathcal{Q}$ is a bijection between $C$ and $\Z_q^d$
\end{definition}

Using this representation, we can describe the encoding and decoding functions, assuming the point $x$ we are quantizing is in $\m{V}_{q\Lambda}(0)$. We will also need an oracle $Q_{\Lambda}(x)$, which maps $x$ to the closest point in $\Lambda$ to $x$.

\begin{algorithm}[h]
   \caption{Encode}
   \label{alg:encode}
\begin{algorithmic}
   \State {\bfseries Input:} $x \in V_{q\Lambda}(0)$, $Q_{\Lambda}$
   \State $p \leftarrow Q_{\Lambda}(x)$
   \State $v \leftarrow G^{-1}p$ \Comment{coordinates of $p$}
   \State {\bfseries return} {$v \mmod q$} \Comment{quantized representation of $p$}
\end{algorithmic}
\end{algorithm}




\begin{algorithm}[h]
\caption{Decode}
\label{decode-algo}
\begin{algorithmic}
   \State {\bfseries Input:} $c \in \Z_q^d$, $Q_{\Lambda}$
   \State $p \leftarrow Gc$ \Comment{equivalent to answer modulo $q\Lambda$}
   \State {\bfseries return} $p - q\,Q_{\Lambda}\!\bigl(\tfrac{p}{q}\bigr)$
\end{algorithmic}
\end{algorithm}

In practice, we will be using the Gosset ($E_8$) lattice as $\Lambda$ with $d = 8$. This lattice is a union of $D_8$ and $D_8 + \frac{1}{2}$, where $D_8$ contains elements of $\Z^8$ with even sum of coordinates. There is a simple algorithm for finding the closest point in the Gosset lattice, first described in \cite{1056484}. We provide the pseudocode for this algorithm together with the estimation of its runtime in Appendix \ref{sec:oracle}.

\subsection{Matrix quantization}

\label{matrix-quant}

When quantizing a matrix, we normalize its rows, and quantize each block of $d$ entries using the codebook. The algorithm \ref{alg:nestquant} describes the quantization procedure for each row of the matrix.

\begin{algorithm}[h]
\caption{NestQuant}
\label{alg:nestquant}
\begin{algorithmic}
   \State {\bfseries Input:} $A$ --- a vector of size $n = db$, $q$, array of $\beta$
   \State $QA$ --- $n$ integers \Comment{quantized representation}
   \State $B$ --- $b$ integers \Comment{scaling coefficient indices}
   \State \label{norm_nestquant} $s \leftarrow \lVert A_i\rVert_2$ \Comment{normalization coefficient}
   \State $A \leftarrow \frac{A\sqrt{n}}{s}$
   \For{$j = 0$ {\bfseries to} $b-1$}
        \State $err = \infty$
        \For{$p = 1$ {\bfseries to} $k$}
            \State $v \leftarrow A[dj+1..dj+d]$
            \State $enc \leftarrow \text{Encode}\left(\frac{v}{\beta_p}\right)$
            \State $recon \leftarrow \text{Decode}(enc) \cdot \beta_p$
            \If{$err > |recon - v|_2^2$}
                \State $err \leftarrow |recon - v|_2^2$
                \State $QA[dj+1..dj+d] \leftarrow enc$
                \State $B_{j} \leftarrow p$
            \EndIf
        \EndFor
   \EndFor
   \State {\bfseries Output:} $QA$, $B$, $s$
\end{algorithmic}
\end{algorithm}

We can take dot products of quantized vectors without complete dequantization using algorithm \ref{alg:dotproduct}. We use it in the generation stage on linear layers and for querying the KV cache.

\begin{algorithm}[h]
\caption{Dot product}
\label{alg:dotproduct}
\begin{algorithmic}
   \State {\bfseries Input:} $QA_1$, $B_1$, $s_1$ and $QA_2$, $B_2$, $s_2$ --- representations of two vectors of size $n = db$ from Algorithm \ref{alg:nestquant}, array $\beta$
   \State $ans \leftarrow 0$
   \For{$j = 0$ {\bfseries to} $b-1$}
        \State $p_1 \leftarrow \text{Decode}(QA_1[dj+1..dj+d])$
        \State $p_2 \leftarrow \text{Decode}(QA_2[dj+1..dj+d])$
        \State $ans \leftarrow ans + (p_1 \cdot p_2)\beta_{B_1[j]}\beta_{B_2[j]}$
   \EndFor
   \State {\bfseries return} $ans$
\end{algorithmic}
\end{algorithm}

\subsection{LLM quantization}

\label{subsec:llm-quant}

\ifisicml
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/kv.pdf}
    \caption{The quantization scheme of multi-head attention. $H$ is Hadamard rotation described in \ref{subsec:llm-quant}. $\mathcal{Q}$ is the quantization function described in \ref{matrix-quant}}
    \label{fig:scheme}
\end{figure}

\else
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/kv.pdf}
    \caption{The quantization scheme of multi-head attention. $H$ is Hadamard rotation described in \ref{subsec:llm-quant}. $\mathcal{Q}$ is the quantization function described in \ref{matrix-quant}}
    \label{fig:scheme}
\end{figure}

\fi

Recall that we apply a rotation matrix $H$ to every weight-activation pair of a linear layer without changing the output of the network. Let $n$ be the number of input features to the layer.

\begin{itemize}
    \item If $n = 2^k$, we set $H$ to be Hadamard matrix obtained by Sylvester's construction
    \item Otherwise, we decompose $n = 2^km$, such that $m$ is small and there exists a Hadamard matrix $H_1$ of size $m$. We construct Hadamard matrix $H_2$ of size $2^k$ using Sylvester's construction, and set $U = H_1 \otimes H_2$.
\end{itemize}

Note that it's possible to multiply an $r \times n$ matrix by $H$ in $O(rn \log n)$ in the first case and $O(rn(\log n + m))$ in the second case, which is negligible to other computational costs and can be done online.

In NestQuant, we quantize all weights, activations, keys, and values using Algorithm \ref{alg:nestquant}. We merge the Hadamard rotation with the weights and quantize them. We also apply the Hadamard rotation and quantization to the activations before linear layers. We also apply rotation to keys and queries, because it will not change the attention scores, and we quantize keys and values before putting them in the KV cache. Figure \ref{fig:scheme} illustrates the procedure for multi-head attention layers.

When quantizing a weight, we modify the NestQuant algorithm by introducing corrections to unquantized weights when a certain vector piece is quantized. We refer the reader to section 4.1 of \cite{tseng2024} for a more detailed description.

\subsection{Optimal scaling coefficients}

One of the important parts of the algorithm is finding the optimal set of $\beta_i$. Given the distribution of 8-vectors that are quantized via a codebook, it is possible to find an optimal set of given size exactly using a dynamic programming approach, which is described in Appendix \ref{dp-section}.

\subsection{Algorithm summary}
\label{algo-summary}

Here we describe the main steps of NestQuant.

\begin{enumerate}
    \item Collect the statistics for LDLQ. For each linear layer with in-dimension $d$, we compute a $d \times d$ matrix $H$.
    \item We choose an initial set of scaling coefficients $\hat{\beta}$, and for each weight we simulate LDLQ quantization with these coefficients, getting a set of 8-dimensional vectors to quantize.
    \item We run a dynamic programming algorithm described in Appendix \ref{dp-section} on the 8-vectors to find the optimal $\beta$-values for each weight matrix.
    \item We also run the dynamic programming algorithm for activations, keys, and values for each layer. To get the distribution of 8-vectors, we run the model on a small set of examples.
    \item We quantize the weights using LDLQ and precomputed $\beta$.
    \item During inference, we quantize any activation before it's passed to the linear layer, and any KV cache entry before it is saved.
\end{enumerate}
Note the complete lack of fine-tuning needed to make our method work.