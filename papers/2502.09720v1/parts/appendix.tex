\newpage
\appendix
\onecolumn


\section{Figures}

\begin{figure}[h]
  \centering    \includegraphics[width=0.4\textwidth]{GaussianMeasure.png}
  \caption{Complement Gaussian measure of a $8$-dimensional cube (corresponding to shaping using an $\ell_{\infty}$ ball), a Voronoi region of the Gosset lattice $E_8$ (corresponding to shaping using Voronoi codes with base lattice $E_8$), and a Euclidean ball (corresponding to shaping with a ball, which does not admit efficient implementation) }
  \label{fig:GaussMeasure}
\end{figure}

\section{Gosset oracle}
\label{sec:oracle}
In this section, we discuss the algorithm for finding the nearest neighbour in $E_8$ lattice and estimate its performance in FLOPs (Floating Point Operations). We note that $E_8 = D_8 \cup D_8 + \frac{1}{2}$, where $D_8$ contains vectors in $\Z_8$ with even sum of coordinates. To compute $V_{E_8}(x)$, we compute two candidate points: $x_1 = V_{D_8}(x)$ and $x_2 = V_{D_8 + \frac{1}{2}}(x)$, and choose the one that has smaller $L^2$ distance to $x$.

To get $V_{D_8}(x)$, we can round each coordinate to the nearest integer. If the sum of rounded coordinates is odd, we need to "flip" the rounding direction of the coordinate for which the flip would cost the least. Note that finding the closest point in $V_{D_8 + \frac{1}{2}}$ works the same, but the rounding grid now contains half-integers, not integers.

In algorithm \ref{alg:GossetDecoding}, we first round our vector down (getting $d$) and compute the mask ($g$) of whether it's optimal to round up for $D_8$. We note that the optimal rounding for $D_8 + \frac{1}{2}$ is $d + 0.5$, while the optimal rounding for $D_8$ is $d + g$. 

We want to understand whether rounding to $D_8$ or $D_8 + \frac{1}{2}$ is better. Let $dist_i$ be the absolute distance from the $i$-th entry $x_i \in [d_i, d_i+1]$ to the middle of this integer segment $d_i + 0.5 = x_{2, i}$. We note that the contribution of this point to the MSE for $D_8$ is $(0.5-dist_i)^2$, while for $D_8+\frac{1}{2}$ is $dist_i^2$. The difference is: $0.25 - dist_i + dist_i^2 - dist_i^2 = 0.25 - dist_i$. If the sum of this value over $i$ is negative (i.e. $\sum dist_i > 2$), it's optimal to quantize to $D_8$, otherwise to $D_8 + \frac{1}{2}$. In pseudocode, we store $\sum dist_i$ as $\Delta$

We note that we should check the constraint that the sum of coordinates in $D_8$ is even, and if it is not, ``flip" one of the rounding directions. The optimal coordinate to flip can be determined through $dist$, and the new value of flipped coordinate --- through $g$. We also need to update $\Delta$ given that the MSE difference changes.

The full pseudocode of the algorithm is in Algorithm \ref{alg:GossetDecoding}.

\begin{algorithm}[H]
\caption{Oracle for the Gosset lattice}
\label{alg:GossetDecoding}
\begin{algorithmic}[1]

\State \textbf{Input:} $x \in \mathbb{R}^8$
\State $d \leftarrow \text{floor}(x)$
\State $x_2 \leftarrow d + 0.5$
\State $g \leftarrow (x > x_2)$
\State $s \leftarrow 2 \cdot g - 1$
\State $x_1 \leftarrow d + g$
\State $dist \leftarrow (x - x_2) \cdot s$
\State $\Delta \leftarrow \sum_i dist_i$
\If{$\sum_i x_{1, i}$ is odd}
    \State $pos = \argmin dist$
    \State $x_{1, pos} \leftarrow x_{1, pos} - s_{1, pos}$
    \State $\Delta \leftarrow \Delta + 2 \cdot dist_{pos} - 1$
\EndIf
\If{$\sum_i x_{2, i}$ is odd}
    \State $pos = \argmax dist$
    \State $x_{2, pos} \leftarrow x_{2, pos} + g_{2, pos}$
    \State $\Delta \leftarrow \Delta + 1 - 2 \cdot dist_{pos}$
\EndIf

\If{$\Delta > 2$}
   \State \Return $x_1$
\Else
   \State \Return $x_2$
\EndIf

\end{algorithmic}
\end{algorithm}





\section{Crude estimate of runtime of quantized matrix}

\label{sec:runtime}

We give an estimate of how the NestQuant runtime affects the overall efficiency of the model. We note that the runtime of encoding and decoding one 8-vector with one scaled codebook is dominated by running the oracle for the Gosset lattice, described in Algorithm \ref{alg:GossetDecoding}. By counting the arithmetic operations and comparisons equally, we get that one run of the oracle takes approximately 15 operations per vector entry. We should also add no more than $3$ arithmetic operations in Algorithms \ref{alg:encode} and \ref{decode-algo}, and multiplication by the generator matrix $G$, or its inverse $G^{-1}$. Given the structure of the matrices, we can spend less operations on multiplying them by vectors using prefix sum technique for $G^{-1}$ and ignoring zeros for $G$. So, the multiplication takes around $2$ operations per vector entry. In total, we do 20 FLOPs per dequantizing one matrix entry, and $20 \cdot k$ FLOPs for quantization, since we need to run the algorithm for $k$ values of $\beta$. We note that in weight quantization the encoding time is not important since it's done offline, and in KV cache quantization encoding happens one time per token, while decoding happens with any additional query to the KV cache.

\begin{equation}
G^{-1}
=
\begin{pmatrix}
0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & -\tfrac{7}{2} \\
0 & 1 & 1 & 1 & 1 & 1 & 1 & -6 \\
0 & 0 & 1 & 1 & 1 & 1 & 1 & -5 \\
0 & 0 & 0 & 1 & 1 & 1 & 1 & -4 \\
0 & 0 & 0 & 0 & 1 & 1 & 1 & -3 \\
0 & 0 & 0 & 0 & 0 & 1 & 1 & -2 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & -1 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 2
\end{pmatrix} \; \; \;
G 
=
\begin{pmatrix}
2 & -1 & 0 & 0 & 0 & 0 & 0 & 0.5 \\
0 & 1 & -1 & 0 & 0 & 0 & 0 & 0.5 \\
0 & 0 & 1 & -1 & 0 & 0 & 0 & 0.5 \\
0 & 0 & 0 & 1 & -1 & 0 & 0 & 0.5 \\
0 & 0 & 0 & 0 & 1 & -1 & 0 & 0.5 \\
0 & 0 & 0 & 0 & 0 & 1 & -1 & 0.5 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0.5 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.5
\end{pmatrix}
\end{equation}

Recall that in generation phase the LLM runtime is memory-bound, i.e. getting the weights from memory takes significantly more time than computing the matrix-vector product. Let's compute the total running time of loading matrix of size $m \times n$ from memory, which can be decomposed into the following running times:

\begin{enumerate}
    \item Loading the matrix from memory $mnR T_{\mathrm{load}}/8$ seconds, where $R$ is the rate (bits/entry) and $T_{\mathrm{load}}$ is the time to load a byte (in seconds)
    \item Decoding the matrix $N_{\mathrm{decode}}\cdot mn\cdot T_{\mathrm{flop}}$ where $N_{\mathrm{decode}}$ is the number of operations per entry and $T_{\mathrm{flop}}$ is the time to apply a floating point operation to a byte.
\end{enumerate}

In total $T_{\mathrm{total}}=mn(R T_{\mathrm{load}}/8+T_{\mathrm{flop}}N_{\mathrm{decode}})$
compared to $mn \cdot 2T_{\mathrm{load}}$ (assume 16-bit baseline), so the speedup is $\frac{2T_{\mathrm{load}}}{RT_{\mathrm{load}}/8+T_{\mathrm{flop}}N_{\mathrm{decode}}}$. 

Defining the arithmetic intensity $\alpha = \frac{T_{\mathrm{load}}}{T_{\mathrm{flop}}}$, we get a theoretical speedup of $\frac{2\alpha}{R\alpha/8+N_{\mathrm{decode}}}$. In our implementation of quantization $N_{\mathrm{decode}}=20$ and $\alpha\approx 30$, For $R=4$  we should expect a theoretical speedup of $\frac{60}{35}\approx 1.71$. For $R=3$  we get a theoretical speedup of $\frac{62}{33.25} \approx 1.92$.

In summary, we expect NestQuant in to achieve speedup in two following ways:

\begin{itemize}
    \item By speeding up the retrieval of weights and KV cache entries
    \item By quantizing activations for large models, where weights are stored on different GPUs, and there are bandwidth limitations.
\end{itemize}

\section{Dynamic programming for optimal $\beta$}

\label{dp-section}

Recall that instead of using one instance of lattice codebook $C$, we use a union of codebooks $C$ scaled to different coefficients. Specifically, our final codebook $\mathcal{C}$ is parameterized by coefficients $\beta_1 \le \beta_2 \le \ldots \le \beta_k$, and is equal to:

\[
    \mathcal{C} = \beta_1 C \cup \beta_2 C \cup \ldots \cup \beta_k C
\]

Given a set of 8-vectors to quantize, we can find the set of $\beta$ that minimizes reconstruction error using a dynamic programming algorithm, which is described in Appendix \ref{dp-section}.

When quantizing a vector to the $i$-th scaled codebook, we could either get a small granular error when the vector is in $V_{\beta_i\Lambda}(0)$, or a large overload error otherwise. If we use a codebook with smaller $\beta$, we have larger chance of overload error, but the expected magnitude of granular error is smaller due to the volume of Voronoi region being smaller (Figure \ref{err}). We can have two strategies for encoding:

\begin{figure}[H]\label{fig:overload_and_granular}
\centering
\subfigure[Overload error probablity]{\includegraphics[width=60mm]{figures/beta_err.pdf}}
\subfigure[Granular RMSE for five sampled vectors]{\includegraphics[width=60mm]{figures/granular.pdf}}
\caption{Granular and overload error for standard Gaussian vectors, $q=16$}
\label{err}
\end{figure}

\begin{enumerate}
    \item \textbf{First-$\beta$:} Use the smallest $\beta$, which does not result in an overflow error.
    \item \textbf{Opt-$\beta$:} Try all the values of $\beta$, and choose the one that has the smallest reconstruction MSE.
\end{enumerate}

\begin{table}[h]
\label{rmse-beta}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
k & 2& 4& 6& 8& 10 \\
\midrule
Opt-$\beta$& 0.0878& 0.0795& 0.0708& 0.0669& 0.0646 \\
First-$\beta$& 0.0878& 0.0798& 0.0712& 0.0676& 0.0656 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\ifisicml\else\vspace{1em}\fi
\caption{Mean RMSE for reconstructed iid standard Gaussian 8-vectors, $q=16$, $k$ betas are uniform on $[0, 10]$.}
\end{table}

Even though Opt-$\beta$ should provide smaller error, the definition of First-$\beta$ will be useful for us. We can note that the difference between error for Opt-$\beta$ and First-$\beta$ is not very siginificant (Table \ref{rmse-beta}). Moreover, First-$\beta$ can be used to determine the optimal set of $\beta_i$ to use.

Let we have $n$ samples $v_1, v_2, \ldots, v_n$ from the distribution of vectors we are quantizing, and a large set of betas $B$, containing $\beta_1 < \beta_2 < \ldots < \beta_m$, from which we want to take the optimal subset of size $k$ which minimizes the loss under First-$\beta$ strategy. For each vector $v_i$ and beta $\beta_j$ we compute $mse_{ij}$ --- the MSE if we use $\beta_j$ to quantize $v_i$ and $overload_{ij}$ --- whether an overload error occurs in this scenario.

We solve the optimization problem with dynamic programming. Let's define $dp_{ij}$ be the mimimum sum of MSE we can get if we have to quantize all the vectors which do not yield an overload error for $\beta_i$, using $\beta_i$ and $j - 1$ smaller betas and First-$\beta$ strategy. If $i$ is large enough so that no vector has an overflow error on $\beta_i$, $dp_{ik}$ has the answer to the problem. To compute the value of $dp_{ij}$, we can iterate over $s$ --- the index of second largest beta in the set (the largest being $\beta_i$). Then, the recalculation works in the following way:

\begin{align*}
    dp_{ij} &\leftarrow \min\left(dp_{ij}, dp_{s, j-1} + \sum_{p, cond_p} mse_{pi} \right) \\
    \text{where } cond_p &= overload_{ps} \wedge \neg overload_{pi}
\end{align*}

By following the transtions in this dynamic programming, we can reconstruct the optimal set of $\beta$.

\begin{algorithm}[h]
\caption{Dynamic programming for finding the set of $\beta$}
\label{alg:DP}
\begin{algorithmic}[1]

\State \textbf{Input:} vectors $v_i$, beta set $B$, $mse_{ij}$, $overload_{ij}$
\State $dp_{i, j} = \infty$ for $i$ in $0\ldots m$, $j$ in $0\ldots k$
\State $from_{i, j} = \text{null}$ for $i$ in $0\ldots m$, $j$ in $0\ldots k$
\State $dp_{0, 0} = 0$
\For {$i=1$ {\bfseries to} $m$}
    \For {$j=1$ {\bfseries to} $k$}
        \For {$s=0$ {\bfseries to} $i-1$}
            \State $cond_p = overload_{ps} \wedge \neg overload_{pi}$ for $p \in 1\ldots n$
            \State $cost = \sum_p cond_p \cdot mse_{pi}$
            \If{$dp_{ij} > dp_{s, j-1} + cost$}
                \State $dp_{ij} \leftarrow dp_{s, j-1} + cost$
                \State $from_{ij} \leftarrow s$
            \EndIf
        \EndFor
    \EndFor
\EndFor
\State Let $pos$ is chosen so that $\beta_{pos}$ has no overflow errors
\State result = \verb|[]|
\For {$j=k$ {\bfseries downto} $1$}
\State result.append($pos$)
\State $pos \leftarrow from_{pos, j}$
\EndFor

\end{algorithmic}
\end{algorithm}


\section{Llama experiment details}

\label{sec:hyperparams}

We choose the train split of the Wikitext2 \cite{merity2016pointer} dataset as a calibration dataset for computing $H$, and evaluate the model on the validation split, computing the perplexity metric. For step 2 in the algorithm (Section \ref{algo-summary}), we select $\hat{\beta} = [3.5, 4.5, 6.0, 14.5, 25.0]/q$, because it is the $\beta$ we get when optimizing them for weight quantization without consideration of LDLQ. The overall universe of betas contains values from $1$ to $40$ with spacing ranging from $0.25$ to $2$. For running DP on activations, keys, and values, we run the model on a batch of $6$ full-length sequences, which is sufficient for this low-dimensional hyperparameter.

When choosing maximum beta for given distribution, we add a margin of $\frac{3.0}{q}$ for weights and $\frac{4.0}{q}$ to the maximum beta needed to have $0$ overload errors on known data to account for potential overload errors in unknown data. While small number of overload error does not affect perplxity significantly, we still aim to minimize their probability.

When computing perplexity for Wikitext2 with given context length, we average the perplexities for all the positions, which is standard for other works in quantization of LLMs.

\section{Ablation studies}

\label{sec:ablation}

We found LDLQ to be useful in improving the quality of quantized model. In table \ref{tab:ldlq_ppl}, we compare the wikitext2 perplexity of models with and without LDLQ.

\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{lccc}
    \toprule
Algorithm &  \textbf{W}&
\textbf{W + KV}&
\textbf{W + KV + A}\\    \midrule
    NestQuant & 6.308& 6.379& 6.633\\
    NestQuant (no LDLQ) & 6.528& 6.605& 6.849 \\
    \bottomrule
\end{tabular}
\ifisicml\else\vspace{1em}\fi
\caption{Effect of LDLQ on NestQuant ($q=14$ and $k=4$) wikitext2 perplexity} 
\label{tab:ldlq_ppl}
\end{table}

While Hadamard matrices from Sylvester construction are commonly used in other works (QuIP\#, Quarot), there are multiple ways to construct a fast rotation for the case when dimension is not a power of $2$ (such as the down projection in MLP of Llama-3). We tested three possible options for rotation on $q = 14$, $k = 4$, W + KV + A quantization.

\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{lccc}
    \toprule
Algorithm &
\textbf{W + KV + A}\\    \midrule
    Fourier & 6.773\\
    $S \otimes H$, $S$ --- orthogonal, $H$ --- Sylvester Hadamard & 6.770 \\
    $H_1 \otimes H$, $H_1$ --- hardcoded Hadamard, $H$ --- Sylvester Hadamard & \textbf{6.663} \\
    \bottomrule
\end{tabular}
\ifisicml\else\vspace{1em}\fi
\caption{Effect of rotation on NestQuant ($q=14$ and $k=4$) wikitext2 perplexity} 
\label{tab:rot_ppl}
\end{table}
