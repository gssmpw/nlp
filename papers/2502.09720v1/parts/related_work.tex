\section{Prior work}\label{sec:prior}

We briefly survey prior work, which we separate into work by information theorists and by the ML community.

\subsection{Information-theoretic quantization}

Rate $R$ quantization of an information source $X$ in $\RR^n$ is the operation of encoding it to $nR$ bits, from which a decoder can produce a reconstruction $\hat{X}\in\RR^n$ that has small \emph{distortion} with respect to $X$. The most popular distortion criterion is the quadratic loss, where the expected distortion is defined as $D=\frac{1}{n}\EE\|X-\hat{X}\|^2$, and here we restrict attention to this loss. Characterization of the optimal tradeoff between $R$ and $D$ is a classic topic in information theory, e.g.~\cite[Part V]{PWbook24}.

For a Gaussian source $X\sim\m{N}(0,I_n)$ the rate-distortion theorem states that any compressor at rate $R$ must satisfy 
$D\geq D(R)\triangleq 2^{-2R}$. Furthermore, as dimension $n$ increases there exist quantizers with distortion approaching $D(R)$. Notably, such quantizers can be made universal, in the sense that they attain distortion $D(R)$ not only for iid Gaussian $X$ but for any  (even adversarial) input as long as its Euclidean norm is $(1+o(1))\sqrt{n}$.
%

One way for constructing these universal quantizers is based on lattices~\cite{ramiBook} that admit much more structure than more classical random codes (and $\epsilon$-nets).

Arguably, the most notable lattice-based quantization scheme is the family of Voronoi codes~\cite{ConwaySloane83}, which we use in this work.

How does one convert a quantizer adapted to Gaussian inputs to work (with the same guaranteed loss) on non-Gaussian data? In a nutshell, the idea is simple: if $U$ is chosen to be a  random $n\times n$ orthogonal matrix then the entries of $UX$ will be distributed as iid Gaussian~\cite{stam1982limit}. 
This idea of applying random rotations to smooth out the distribution of the quantizer's input may be viewed as a special case of high-dimensional companding \cite{gersho1979asymptotically}, and has been applied for image compression~\cite{hung1998multidimensional}, and as a potential replacement for dithering~\cite{hadad2016dithered}, to name a few. 



In the context of LLMs, the goal in quantization is slightly different since quantization is used to facilitate approximate matrix multiplication with reduced burden on the memory bandwidth. For example, when quantizing two vectors  $X,Y\in\RR^n$ the goal \emph{is not} to approximate them but to approximate their inner product. Recently, information-theoretic characterization of this task was completed in~\citep{op2024}. Specifically, the authors show that if $X,Y\sim\m{N}(0,I_n)$ are independent then for any algorithm operating on the basis of rate-$R$ quantized representations of $X$ and $Y$ we must have 
\begin{equation}\label{eq:lbnd}
\EE(X^\top Y-\widehat{X^\top Y})^2\geq n\Gamma(R),    
\end{equation}
where
\begin{align}
\Gamma(R)=    \begin{cases}
1-\left(1-(2\cdot2^{-2R^*}-2^{-4R^*})\right)\frac{R}{R^*}   & R<R^* \\
2\cdot 2^{-2R}-2^{-4R}    & R\geq R^*
\end{cases}.
\label{eq:gammadef}
\end{align}
and $R^*\approx 0.906$ is a solution to a certain transcendental fixed-point equation.

The same paper also constructs \textit{universal quantizers} based on nested lattices that asymptotically (as $n\to \infty$) achieve this lower bound. Note that extension from vectors to matrices can be made trivially by observing that one can quantize each column separately and treat matrix product as a collection of inner products.


In this work we show that with appropriate tweaks Voronoi codes indeed can result in practical fast and efficient algorithms for LLM quantization. We emphasize that most of our work is on simply developing a drop-in replacement for quantized matrix product and as such is not specific to LLMs.


The idea of applying random rotations to ``Gaussianize'' inputs in the context of approximate inner product computation is even  more natural than in the standard quantization. Indeed, since one is only interested in the inner product, not vectors themselves, one does not need to store (even a seed used to generate the) random orthogonal matrix. This has been long exploited in locality-sensitive hashing (LSH) algorithms,\footnote{In LSH, one typically performs several random \emph{projections} of the vector and quantizes them. This is equivalent to performing random rotation and quantizing only a small number of entries of the rotated vector.} which can be viewed as an (extremely low rate) quantizers for inner product approximation~\cite{charikar2002similarity,datar2004locality,andoni2008near}. Unsurprisingly, as we will see next,  random rotations have also been found quite useful for quantizing LLMs.


\subsection{LLM quantization}\label{sec:llm_review}

One of the directions of prior research on LLM quantization is addressing the issue of activation outliers that hinder the quantization quality. These outliers are present in certain dimensions of activations, weights and KV cache. In LLM.int8() of \cite{dettmers2022}, these outlier dimension are kept unquantized. In SmoothQuant~\cite{xiao2024} authors balance the scale of outliers between weights and activations by modifying LayerNorm's diagonal matrices.

Going to random rotations, by rewriting matrix product $AB=(AU) (U^\top B)$ for an orthogonal matrix $U$, one gets matrices with much more Gaussian entries (few outliers) and can apply standard quantization algorithms. Some of the multiplications by $U$ can be merged with the weights (i.e. do not require additional runtime FLOPs), while the rest are applied at runtime. For the latter, matrices $U$ should have structure to enable fast multiplication. For example, QuaRot \cite{ashkboos2024} uses randomized Hadamard matrices as coordinate transformations, which can be applied to a vector of size $n$ in $O(n \log n)$ additions. SpinQuant \cite{liu2024} uses a rotation parametrization with four orthogonal matrices $R_1$, $R_2$, $R_3$, $R_4$, where $R_1$ and $R_2$ can be arbitrary orthonormal matrices, and $R_3$ and $R_4$ should have a fast multiplication algorithm. The authors use Cayley SGD \cite{li2020} to optimize $R_1$ and $R_2$ for minimization of the quantization error, while the matrices $R_3$ and $R_4$ are chosen to be random Hadamard. 

Starting from LLM.int8() most of the schemes used uniform quantization (i.e. where a floating point vector simply rounded to a nearest integer after an appropriate rescaling). 
To the best of our knowledge, so far non-scalar quantization has only been used for weight-only compression for LLMs in QuIP\# \cite{tseng2024}, which uses E8P codebook for 2-bit quantization, and applies Resdidual Vector Quantization \cite{Juang1982MultipleSV} to get a 4-bit quantization scheme; and QTIP \cite{tseng2024qtip} which uses trelis codebook. Unfortunately these methods appear to be too expensive to apply then in runtime, perhaps explaining why non-uniform quantization for activations and KV-cache was not attempted before this work.

Finally, when quantizing weight matrices, one may notice that MSE distortion loss should be replaced by a weighted-MSE loss dependent on the statistics of the incoming activations. We refer to this type of algorithms as LDLQ, following authors of QuIP~\citep{chee2024}, QuIP\#~\citep{tseng2024} and GPTQ~\citep{frantar2023_2}. Applying LDLQ-modified NestQuant does lead to improved performance, see results in Section \ref{sec:exper}.

\ifisicml
\begin{figure}[t]
\hbox{\includegraphics[width=.5\linewidth]{CubicShaping_waste32percent}\hfil\includegraphics[width=.5\linewidth]{HexShaping_waste15percent}}
    \caption{Demonstrating advantage of NestQuant in 2D. Typical weights and activations are vectors inside the black circle. Uniform quantization wastes about 32\% of allocated bitstrings for vectors outside of the circle, while nested hexagonal lattices only wastes 15\% (explicitly enumerating points inside the circle to avoid the waste is too slow to do at runtime). This allows NestQuant to use finer grid while quantizing to the same rate $R$. The gain becomes much more dramatic in higher dimensions.}\label{fig:shaping}
\end{figure}

\else
\begin{figure}[t]
    \centering
    \includegraphics[width=.3\textwidth]{CubicShaping_waste32percent}%
    \includegraphics[width=.3\textwidth]{HexShaping_waste15percent}
    \caption{Demonstrating advantage of NestQuant in 2D. Typical weights and activations are vectors inside the black circle. Uniform quantization wastes about 32\% of allocated bitstrings for vectors outside of the circle, while nested hexagonal lattices only wastes 15\% (explicitly enumerating points inside the circle to avoid the waste is too slow to do at runtime). This allows NestQuant to use finer grid while quantizing to the same rate $R$. The gain becomes much more dramatic in higher dimensions.}\label{fig:shaping}
\end{figure}

\fi