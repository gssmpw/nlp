\section{Outline of NestQuant approach}\label{sec:outline}

In this section we outline the main components of our approach. A detailed description is brought in the following section.

When designing a quantizer, one needs to make some assumptions about the distribution of the source that will be fed to it. While weights (and sometimes activations) can be well-approximated by Gaussians, their magnitude are wildly varied. Thus, one employs two ideas: normalization and random rotation. 


\textbf{Normalization:}
In most of the literature, the normalization is done by taking an input vector of large dimension $n$ (e.g. $n=4096$ for Llama-3), dividing by the $L_\infty$ norm to get entries to be in $[-1,1]$ and then applying uniform quantization. This is suboptimal for two reasons: one is that uniform quantization induces error that is distributed uniformly on the small cube, which is suboptimal from the MSE point of view. Second reason, much more serious, is known as the shaping gain and demonstrated on Fig.~\ref{fig:shaping}. When entries of the vector are Gaussian, it will typically lie inside the black circle. Thus those grid elements outside of it will almost never be used, wasting bitspace. 

Instead, we use normalization by the $L_2$-norm (see Algorithm \ref{alg:nestquant}) and then use points inside the Voronoi region of a Gosset lattice, which as Fig.~\ref{fig:shaping} (right) demonstrates wastes a lot fewer bitstrings for rare vectors, thus allowing us to use finer grids.

\textbf{Random rotation:} 
When input to the quantizer significantly differs from the presumed model (of iid Gaussians), performance can become quite poor. As discussed previously, multiplying by a random orthoghonal matrix $U$ provides an excellent fix. Specifically, $UX$ vector becomes uniform on the $n$-sphere of radius $\sqrt{n}$, and small chunks of this vector have distribution very close to Gaussian iid. In particular, the total variation between any subset of $d$ coordinates and $\calN(0, I_d)$ is $O(d^2/n)$~\cite{stam1982limit}, such that for $d=o(\sqrt{n})$ what we quantize is effectively iid Gaussian.


\textbf{Complexity of lattice quantization:} 
In order to explain our choice of nested lattice quantizer, we need to carefully balance several requirements. One of the key ones is complexity. It is known that finding (even approximating) a nearest lattice point is a well-known cryptographic assumption \cite{DBLP:journals/combinatorica/DinurKRS03}.
Thus, we are not suggesting to directly operate on $n$-dimensional lattices. Instead, we partition the $n$-vector into sections, each of dimension $d$ and apply lattice quantization to $d$-subvectors. Equivalently, our vector quantizers for $\RR^n$ are constructed as Cartesian products of vector quantizers of small dimension $d$ (we will take $d=8$ for all experiments).

\textbf{Granular and overload quantization errors:} There are two different sources of errors for lattice quantizers. The first is called \emph{granular} quantization error, and is related to the second moment of the lattice Voronoi region. A common way to measure the granular error corresponding to a lattice $\Lambda\subset\RR^d$ is via the normalized second moment (NSM) defined as
\begin{align}
G(\Lambda)=\frac{1}{\mathrm{vol}(\m{V}_{\Lambda})^{1+\frac{2}{d}}}\frac{1}{d}\int_{x\in \m{V}_{\Lambda}}\|x\|^2dx,
\end{align}
where $\m{V}_{\Lambda}\subset\RR^d$ is the Voronoi region of $\Lambda$, consisting of all point in $\RR^d$ closer to $0$ than to any other lattice point in $\Lambda$.
This quantity corresponds to the MSE when $\Lambda$ is normalized to have unit covolume and is then used as a quantizer. It is well known that the optimal (smallest) NSM among all lattices in $\RR^d$ approaches $\frac{1}{2\pi e}$ from above as $d$ increases~\cite{ramiBook}. Furthermore, for $d=1$ we get $G(\ZZ)=\frac{1}{12}$. Consequently, in terms of granular error, by using high-dimensional lattices instead of the simple scalar quantizer based on $\ZZ$ we can already gain a factor of $\frac{2\pi e}{12}\approx 1.42329$ in performance (the Gosset lattice achieves $1.22$ gain).

Notice, however, that representing $x$ as $Q_{\Lambda}(x)$, the nearest lattice point in $\Lambda$, requires infinitely many bits, since the lattice is infinite. Since we only have $2^{dR}$ bitstrings to allocate, we need to select a subset of $\Lambda$ that will be actually used. Selection of $\m{S}$ so that $|\Lambda\cap \m{S}|=2^{dR}$ is called \emph{shaping}. 
If $Q_{\Lambda}(x)\in\m{S}$, then the quantization error $x-Q_{\Lambda}(x)$ will be in $\m{V}_{\Lambda}$ and we will only suffer from a granular error. However, when $Q_{\Lambda}(x)\notin \m{S}$ the quantization error is no longer in $\m{V}_{\Lambda}$ and may be have much greater magnitude than a typical granular error. We refer to those type of errors, where $Q_{\Lambda}(x)\notin \m{S}$, as \emph{overload} errors. 

Generally speaking, in order to achieve a small quantization error, one must keep the probability of overload very small. This can be attained by scaling up the codebook to $\beta\m{C}=\beta\Lambda\cap \beta \m{S}$ with a large enough $\beta>0$ such that overload becomes very rare. However, increasing $\beta$ also increases the squared granular error by a factor of $\beta^2$. Thus, one would like to use the smallest possible $\beta$ for which overload is rare. In order to allow for smaller $\beta$, we would like to choose $\m{S}\subset\RR^n$ such that $\beta\m{S}$ captures as much Gaussian mass as possible.

Denote by $\mu=\calN(0,I_d)$ the standard Gaussian measure. Since we need $2^{dR}=|\Lambda\cap \m{S}|\approx \frac{\mathrm{vol}(\m{S})}{\mathrm{covol}(\Lambda)}$, a good shaping region $\m{S}$ maximizes $\mu(\m{S})$, which in turn minimizes the overload probability that is approximated by $1-\mu(\m{S})$, under a volume constraint. Clearly, the optimal $\m{S}$ under this criterion is $r\m{B}$ where $\m{B}=\{x\in\RR^d~:~\|x\|\leq r_{\text{eff}}(1)\}$ is a Euclidean ball with radius $r_{\text{eff}}(1)$ chosen such that $\mathrm{vol}(\m{B})=1$, and $r$ is chosen such that $\mathrm{vol}(\m{S})=r^d$ satisfies the required volume constraint. Unfortunately, for $d>1$ the codebook $\m{C}=\Lambda\cap r\m{B}$ loses much of the lattice structure, and does not admit an efficient enumeration, and consequently encoding and decoding require using a lookup table (LUT). QuIP\# used this approach with $\Lambda = E_8$ (same as we do) and $\m{S} = r\m{B}$. However, this seems to only be possible for quantizing weights and not activations as complexity makes runtime implementation too slow.\footnote{We note that QuIP\# cleverly exploits symmetries of $E_8$ to show that an $R=2$ bit quantizer can be implemented using an LUT of size $2^{d\frac{R}{2}}=2^8$, but we believe this is still too slow, and furthermore does not naturally extend to different quantization rates.}

\textbf{Using int8-multipliers:} One often mentioned advantage of uniform quantization compared to other approaches is the fact that it approximates any matrix as a product of diagonal matrix (of norms) and an integer matrix. Thus, during multiplication one can leverage faster int-cores rather than floating-point multiplication. Note that if there exists a  scaling coefficient $\alpha>0$ such that $\alpha \Lambda \subset \mathbb{Z}^d$, then one can still use int-multipliers even for lattice-quantized vectors.

\textbf{Voronoi codes/nested lattice codes:} In Voronoi codes~\cite{ConwaySloane83} the same lattice $\Lambda$ is used for both quantization and shaping. In particular, the shaping region is taken as $\m{S}=2^{R} \m{V}_{\Lambda}$, where $2^R$ is an integer. As elaborated below, if $Q_{\Lambda}(x)$ admits an efficient implementation, one can efficiently perform encoding and decoding to the codebook $\m{C}=\Lambda\cap (2^R \m{V}_{\Lambda})\cong \Lambda/2^R\Lambda$. Moreover, in stark contrast to ball-based shaping, the encoding and decoding complexity does not depend on $R$.

\textit{In summary,} a good choice of lattice $\Lambda$ should therefore have: 1) efficient lattice decoding algorithm; 2) small NSM; 3) large $\mu(\m{V}_{\Lambda})$; 4) be a subset of standard integer lattice $\Z^d$.


In this work, we use the Gosset lattice ($E_8$) that satisfies all these properties. It has a fast decoding algorithm (Algorithm~\ref{alg:GossetDecoding}), its NSM is $\approx 0.0716821\approx 1.2243\frac{1}{2\pi e}$~\cite{agrell2023best}, and its Gaussian mass $\mu(r\m{V}_{E_8})$ is very close to $\mu(r \m{B})$ (the Gosset lattice has unit covolume, so $\mathrm{vol}(r\m{V}_{E_8})=\mathrm{vol}(r\m{B})$). The last point is illustrated in Figure~\ref{fig:GaussMeasure}, where the large loss for cubic shaping with respect to lattice shaping is also evident. The gap between Voronoi/Ball shaping and cubic shaping becomes much more significant as the dimension increases. This follows since for large $d$ we have that for $X\sim \mu=\mu_d$ the $\ell_\infty$ norm $\|X\|_{\infty}$ concentrates around $\sqrt{2\ln{d}}$. Thus, for $r<2\sqrt{2\ln{d}}$ we have that $\mu(r\mathrm{CUBE})\to 0$, whereas for any $r>\frac{\sqrt{d}}{r_{\text{eff}}(1)}=\sqrt{2\pi e}(1+o(1))$ we have that $\mu(r\mathrm{B})\to 1$. Note that SpinQuant uses high-dimensional cubic shaping, and therefore its MSE distortion suffers a $O(\ln d)$ multiplicative gap with respect to the optimal distortion.


\textbf{Overload avoidance via union of Voronoi codes:} Because we rely on lattice quantizers of relatively small dimension ($d=8$), even if $\mu(r\m{V}_{\Lambda})$ is very close to $\mu(r\m{B})$, overload events are unavoidable. This follows because in small dimension the norm of a iid Gaussian vector is not sufficiently concentrated. Thus, if one is restricted to $\m{C}=\beta(\Lambda\cap (2^R \m{V}_{\Lambda}))$ the parameter $\beta$ must be taken quite large in order to keep the overload probability small. This in turn, incurs a significant penalty in the obtained distortion. As a remedy, rather than using a Voronoi code, we take $\m{C}$ as a union of (a small number) of Voronoi codes in different scales. Namely, we take $\m{C}=\cup_{t=1}^{k} \beta_t(\Lambda\cap (2^R \m{V}_{\Lambda}))$, where $\beta_1<\cdots<\beta_{k}$. The smallest values of $\beta_t$ are set such that overload is not too common but not extremely rare, such that for most realizations of a Gaussian vector $X\in\RR^d$ the distortion is close to the fundamental limit $D(R)$. Whenever $X$ is atypically large, there will be overload in $\beta_t(\Lambda\cap (2^R \m{V}_{\Lambda}))$ for small $t$, but not for large $t$, such that the quantization error will be in $\beta_t \m{V}_{\Lambda}$ for one of the larger values of $\{\beta_t\}$. 

The details of choosing $k$ and values of $\beta_t$ are described in Section~\ref{dp-section}. Here we only note that the overall compression rate becomes $R+{1\over d}{\log_2 k}$. In some cases, we are using nvcomp \cite{nvcomp} for compressing a vector of $n/8$ chosen betas, in which case rate penalty is reduced below ${1\over d}\log_2 k$. We note that in our comparisons, including Table~\ref{tab:llama3_8b}, we always use this effective rate for a fair comparison with 
other algorithms.

\ifisicml
\begin{figure}[t]
\includegraphics[width=\linewidth]{quantization_loss_comparison}
    \caption{RMSE for quantized matrix multiplication for iid $\calN(0,1)$ matrices. NestQuant algo is optimized over $q$ and multiple $\beta$'s.
    Also shown is information-theoretic lower bound from~\eqref{eq:lbnd}.}\label{fig:synth}
\end{figure}

\else
\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{center}
    \includegraphics[width=\linewidth]{quantization_loss_comparison}
  \end{center}
  \caption{RMSE for quantized matrix multiplication for iid $\calN(0,1)$ matrices. NestQuant algo is optimized over $q$ and multiple $\beta$'s.
    Also shown is information-theoretic lower bound from~\eqref{eq:lbnd}.}\label{fig:synth}
\end{wrapfigure}
\fi

\textbf{NestQuant, SpinQuant and theory:}
As mentioned above, the use of nested lattice codes is rooted in theory. In~\citep{op2024} it was shown that nested lattice quantizers of high-dimensions attain the optimal rate-distortion tradeoff for matrix multiplication. Since the lattices used for proving that result do not admit efficient lattice decoding, here we resort to $n$-dimensional lattices constructed as the Cartesian product of $n/8$ copies of the Gosset lattice, whose dimension is $d=8$. To understand how much loss in efficiency this leads, Fig.~\ref{fig:synth} compares NestQuant, SpinQuant (uniform quantization with cubic shaping) and information-theoretic lower bound~\eqref{eq:lbnd}. Details of this experiment can be found in Section~\ref{sec:synth}. We can see that NestQuant is reasonably close to the fundamental limit and significantly outperforms SpinQuant.