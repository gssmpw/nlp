\section{Related Work}
\label{sec:related}
\paragraph{Knowledge Distillation (KD).}
%KD was originally defined by Bucilu«é et al.~\citep{Modelcompression2006} and generalized by Hinton et al.~\citep{hinton2015distilling}. 
Representative avenues of KD can be generally classified into offline KD, online KD, and self KD, based on whether the teacher model is pre-trained and remains unchanged during the training process. Offline KD involves a one-way knowledge transfer in a two-phase training procedure. It primarily focuses on optimizing various aspects of knowledge transfer, such as designing the knowledge itself~\citep{hinton2015distilling,adriana2015fitnets}, and refining loss functions for feature matching or distribution alignment~\citep{huang2017like,asif2019ensemble,teacherassistant}.
%probabilisticKD,payingattention,li2020few,adversarialKD}
In contrast, online KD simplifies the KD process by training both teacher and student simultaneously and often outperforms offline KD. For instance, DML~\citep{deepMutual} implements bidirectional distillation between peer networks. %Further research explores balancing multiple teachers~\citep{multiTeacher} or teacher model construction~\citep{li2020explicit,liu2021semi,wu2021peer}. 
For self KD, the same network is used as both teacher and student~\citep{self-kd,das2023understanding,mobahi2020self,zhang2020self,yang2019snapshot,lee2019rethinking}.
%zhang2020self,yang2019snapshot,lee2019rethinking,phuong2019distillation,lan2019self,yuan2020revisiting}.
In this paper, the self KD we refer to is the distillation between different layers within the same network~\citep{self-kd,yan2024orchestrate,zhai2019lifelong}. 
%A recent work~\citep{allen2020towards} proved that it resembles an implicit ensembling strategy that improves test accuracy. 
However, existing methods for online KD and self KD often fail to effectively utilize high-capacity teachers over time, making it an intriguing topic to further explore the relationships between teacher and student models in these environments.

\paragraph{Adaptive Distillation.}\label{adaptiveKD}
Recent studies have found that the difference in model capacity between a much larger teacher network and a much smaller student network can limit distillation gains~\citep{KnowledgeGap_2020_CVPR,earlystop,liu2020search}. Current efforts to address this gap fall into two main categories: training paradigms~\citep{gao2018embarrassingly} and architectural adaptation~\citep{kang2020towards,gu2020search}. For instance, ESKD~\citep{earlystop} suggests stopping the training of the teacher early, while ATKD~\citep{mirzadeh2020improved} employs a medium-sized teacher assistant for sequential distillation. SHAKE~\citep{li2022shadow} introduces a shadow head as a proxy teacher for bidirectional distillation with students. However, existing methods usually implement adaptive distillation by adjusting teacher-student architecture from a spatial level. In contrast, Spaced KD provides an architecture- and algorithm-agnostic way to improve KD from a temporal level.

%is an architecture- and algorithm-agnostic method that opens a new direction for exploration from the temporal level.

\vspace{-.5em}
\paragraph{Flatness of Loss Landscape.}
The loss landscape around a parameterized solution has attracted great research attention~\citep{keskar2016large,hochreiter1994simplifying,izmailov2018averaging,dinh2017sharp,he2019asymmetric}. A prevailing hypothesis posits that the flatness of minima following network convergence significantly influences its generalization capabilities~\citep{keskar2016large}. In general, a flatter minima is associated with a lower generalization error, which provides greater resilience against perturbations along the loss landscape. This hypothesis has been empirically validated by studies such as~\cite{he2019asymmetric}. Advanced advancements have leveraged KD techniques to boost model generalization~\citep{deepMutual,zhao2023dot,self-kd}. Despite these remarkable advances, it remains a challenging endeavor to fully understand the impact of KD on generalization, especially in assessing the quality of knowledge transfer and the efficacy of teacher-student architectures.

\iffalse
\begin{figure*}
    %\vspace{-.1cm}
    \centering
    \includegraphics[width=0.8\linewidth]{./imgs/figs/fig2_Ucurve.pdf}
    \vspace{-.2cm}
    \caption{Alignment of spaced learning in BNNs and DNNs. \textbf{(a)} Computational cognitive model of spaced learning, modified from~\cite{landauer1969reinforcement}. \textbf{(b)} Overall performance of Spaced KD from different networks and benchmarks. R18: ResNet-18; R50: ResNet-50; R101: ResNet-101; C100: CIFAR-100; T200: Tiny-ImageNet. \textbf{(c)} Quadratic polynomial fitting of all performance from \textbf{(b)}. %All of them show that spaced training with appropriate interval can enhance learning performance. 
    %Depiction of the cognitive model was modified from~\cite{landauer1969reinforcement}.
    }
    \label{fig:main_result}
     \vspace{-.3cm}
\end{figure*}
\vspace{-.5em}
\paragraph{Spacing Effect in Biological Learning}
The spacing effect was first proposed by Ebbinghaus~\citep{ebbingham1913memory} in 1913. He conducted a self testing memory experiment by recalling lists of syllables and discovered that spacing repetitions over time was significantly more effective than cramming them all at once. Then, the superiority of spaced learning has been demonstrated across various species, including invertebrates (like \textit{Drosophila}~\citep{beck2000learning,pagani2009phosphatase}, bees~\citep{menzel2001massed}), rodents~\citep{anderson2008spaced,bello2013differential} and non-human primates~\citep{medin1974comparative,robbins1973memory}, as well as in various aspects of human learning, such as skill and motor learning~\citep{donovan1999meta,shea2000spacing}, classroom education~\citep{gluckman2014spacing,roediger2008learning,sobel2011spacing}, and the generalization of conceptual knowledge in children~\citep{vlach2014spacing}. To explain its effectiveness, Landauer~\citep{landauer1969reinforcement} was the first to propose and describe the dynamics of spaced learning (Fig.~\ref{fig:main_result}), such as the saturation of memory formation with mass trials, the failure of reinforcement with overly long trial spacing, and the way these factors contribute to creating an optimal inter-trial interval. Subsequently, some cognitive computing models~\citep{peterson1966short,wickelgren1972trace} inferred that a memory trace becomes not only stronger but also highly resistant to decay following spaced trials.
Inspired by this, we hypothesize that there may exist an optimal interval between the learning paces of teacher and student to improve the generalization of DNNs.
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Method