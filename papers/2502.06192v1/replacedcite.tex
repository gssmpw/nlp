\section{Related Work}
\label{sec:related}
\paragraph{Knowledge Distillation (KD).}
%KD was originally defined by Bucilu«é et al.____ and generalized by Hinton et al.____. 
Representative avenues of KD can be generally classified into offline KD, online KD, and self KD, based on whether the teacher model is pre-trained and remains unchanged during the training process. Offline KD involves a one-way knowledge transfer in a two-phase training procedure. It primarily focuses on optimizing various aspects of knowledge transfer, such as designing the knowledge itself____, and refining loss functions for feature matching or distribution alignment____.
%probabilisticKD,payingattention,li2020few,adversarialKD}
In contrast, online KD simplifies the KD process by training both teacher and student simultaneously and often outperforms offline KD. For instance, DML____ implements bidirectional distillation between peer networks. %Further research explores balancing multiple teachers____ or teacher model construction____. 
For self KD, the same network is used as both teacher and student____.
%zhang2020self,yang2019snapshot,lee2019rethinking,phuong2019distillation,lan2019self,yuan2020revisiting}.
In this paper, the self KD we refer to is the distillation between different layers within the same network____. 
%A recent work____ proved that it resembles an implicit ensembling strategy that improves test accuracy. 
However, existing methods for online KD and self KD often fail to effectively utilize high-capacity teachers over time, making it an intriguing topic to further explore the relationships between teacher and student models in these environments.

\paragraph{Adaptive Distillation.}\label{adaptiveKD}
Recent studies have found that the difference in model capacity between a much larger teacher network and a much smaller student network can limit distillation gains____. Current efforts to address this gap fall into two main categories: training paradigms____ and architectural adaptation____. For instance, ESKD____ suggests stopping the training of the teacher early, while ATKD____ employs a medium-sized teacher assistant for sequential distillation. SHAKE____ introduces a shadow head as a proxy teacher for bidirectional distillation with students. However, existing methods usually implement adaptive distillation by adjusting teacher-student architecture from a spatial level. In contrast, Spaced KD provides an architecture- and algorithm-agnostic way to improve KD from a temporal level.

%is an architecture- and algorithm-agnostic method that opens a new direction for exploration from the temporal level.

\vspace{-.5em}
\paragraph{Flatness of Loss Landscape.}
The loss landscape around a parameterized solution has attracted great research attention____. A prevailing hypothesis posits that the flatness of minima following network convergence significantly influences its generalization capabilities____. In general, a flatter minima is associated with a lower generalization error, which provides greater resilience against perturbations along the loss landscape. This hypothesis has been empirically validated by studies such as____. Advanced advancements have leveraged KD techniques to boost model generalization____. Despite these remarkable advances, it remains a challenging endeavor to fully understand the impact of KD on generalization, especially in assessing the quality of knowledge transfer and the efficacy of teacher-student architectures.

\iffalse
\begin{figure*}
    %\vspace{-.1cm}
    \centering
    \includegraphics[width=0.8\linewidth]{./imgs/figs/fig2_Ucurve.pdf}
    \vspace{-.2cm}
    \caption{Alignment of spaced learning in BNNs and DNNs. \textbf{(a)} Computational cognitive model of spaced learning, modified from____. \textbf{(b)} Overall performance of Spaced KD from different networks and benchmarks. R18: ResNet-18; R50: ResNet-50; R101: ResNet-101; C100: CIFAR-100; T200: Tiny-ImageNet. \textbf{(c)} Quadratic polynomial fitting of all performance from \textbf{(b)}. %All of them show that spaced training with appropriate interval can enhance learning performance. 
    %Depiction of the cognitive model was modified from____.
    }
    \label{fig:main_result}
     \vspace{-.3cm}
\end{figure*}
\vspace{-.5em}
\paragraph{Spacing Effect in Biological Learning}
The spacing effect was first proposed by Ebbinghaus____ in 1913. He conducted a self testing memory experiment by recalling lists of syllables and discovered that spacing repetitions over time was significantly more effective than cramming them all at once. Then, the superiority of spaced learning has been demonstrated across various species, including invertebrates (like \textit{Drosophila}____, bees____), rodents____ and non-human primates____, as well as in various aspects of human learning, such as skill and motor learning____, classroom education____, and the generalization of conceptual knowledge in children____. To explain its effectiveness, Landauer____ was the first to propose and describe the dynamics of spaced learning (Fig.~\ref{fig:main_result}), such as the saturation of memory formation with mass trials, the failure of reinforcement with overly long trial spacing, and the way these factors contribute to creating an optimal inter-trial interval. Subsequently, some cognitive computing models____ inferred that a memory trace becomes not only stronger but also highly resistant to decay following spaced trials.
Inspired by this, we hypothesize that there may exist an optimal interval between the learning paces of teacher and student to improve the generalization of DNNs.
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Method