@article{cifar100,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}
@article{imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International Journal of Computer Vision},
  volume={115},
  pages={211--252},
  year={2015},
  publisher={Springer}
}
@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={770--778},
  year={2016}
}
@article{vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Machine Learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}
@inproceedings{heo2021rethinking,
  title={Rethinking spatial dimensions of vision transformers},
  author={Heo, Byeongho and Yun, Sangdoo and Han, Dongyoon and Chun, Sanghyuk and Choe, Junsuk and Oh, Seong Joon},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11936--11945},
  year={2021}
}


@inproceedings{Modelcompression2006,
  title={Model compression},
  author={Bucilu«é, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
  booktitle={Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={535--541},
  year={2006}
}
@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}
@article{review_ST,
  title={Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks},
  author={Wang, Lin and Yoon, Kuk-Jin},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={6},
  pages={3048--3068},
  year={2021},
  publisher={IEEE}
}
@article{review_KDSurvey,
  title={Knowledge distillation: A survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume={129},
  number={6},
  pages={1789--1819},
  year={2021},
  publisher={Springer}
}
@inproceedings{mirzadeh2020improved,
  title={Improved knowledge distillation via teacher assistant},
  author={Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5191--5198},
  year={2020}
}
@article{gao2021residualKD,
  title={Residual error based knowledge distillation},
  author={Gao, Mengya and Wang, Yujun and Wan, Liang},
  journal={Neurocomputing},
  volume={433},
  pages={154--161},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{furlanello2018born,
  title={Born again neural networks},
  author={Furlanello, Tommaso and Lipton, Zachary and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
  booktitle={International Conference on Machine Learning},
  pages={1607--1616},
  year={2018},
  organization={PMLR}
}


@inproceedings{DML2018,
  title={Deep mutual learning},
  author={Zhang, Ying and Xiang, Tao and Hospedales, Timothy M and Lu, Huchuan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4320--4328},
  year={2018}
}
@article{DeepNetTriage,
  title={Deep net triage: Analyzing the importance of network layers via structural compression},
  author={Nowak, Theodore S and Corso, Jason J},
  journal={arXiv preprint arXiv:1801.04651},
  year={2018}
}
@article{cheapKD,
  title={Moonshine: Distilling with cheap convolutions},
  author={Crowley, Elliot J and Gray, Gavin and Storkey, Amos J},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@article{KnowledgeFlow,
  title={Knowledge flow: Improve upon your teachers},
  author={Liu, Iou-Jen and Peng, Jian and Schwing, Alexander G},
  journal={arXiv preprint arXiv:1904.05878},
  year={2019}
}
@inproceedings{ProgressiveKD,
  title={Progressive Blockwise Knowledge Distillation for Neural Network Acceleration.},
  author={Wang, Hui and Zhao, Hanbin and Li, Xi and Tan, Xu},
  booktitle={International Joint Conference on Artificial Intelligence},
  pages={2769--2775},
  year={2018}
}
@article{BetterStudents,
  title={Search for better students to learn distilled knowledge},
  author={Gu, Jindong and Tresp, Volker},
  journal={arXiv preprint arXiv:2001.11612},
  year={2020}
}


# offline
@article{adriana2015fitnets,
  title={Fitnets: Hints for thin deep nets},
  author={Adriana, Romero and Nicolas, Ballas and Ebrahimi, K Samira and Antoine, Chassang and Carlo, Gatta and Yoshua, Bengio},
  journal={International Conference on Learning Representations},
  volume={2},
  number={3},
  pages={1},
  year={2015}
}
@article{huang2017like,
  title={Like what you like: Knowledge distill via neuron selectivity transfer},
  author={Huang, Zehao and Wang, Naiyan},
  journal={arXiv preprint arXiv:1707.01219},
  year={2017}
}
@inproceedings{probabilisticKD,
  title={Learning deep representations with probabilistic knowledge transfer},
  author={Passalis, Nikolaos and Tefas, Anastasios},
  booktitle={Proceedings of the European Conference on Computer Vision},
  pages={268--284},
  year={2018}
}
@article{payingattention,
  title={Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1612.03928},
  year={2016}
}
@inproceedings{teacherassistant,
  title={Improved knowledge distillation via teacher assistant},
  author={Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5191--5198},
  year={2020}
}
@inproceedings{li2020few,
  title={Few sample knowledge distillation for efficient network compression},
  author={Li, Tianhong and Li, Jianguo and Liu, Zhuang and Zhang, Changshui},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14639--14647},
  year={2020}
}
@inproceedings{adversarialKD,
  title={Knowledge distillation with adversarial samples supporting decision boundary},
  author={Heo, Byeongho and Lee, Minsik and Yun, Sangdoo and Choi, Jin Young},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={3771--3778},
  year={2019}
}
@article{asif2019ensemble,
  title={Ensemble knowledge distillation for learning improved and efficient networks},
  author={Asif, Umar and Tang, Jianbin and Harrer, Stefan},
  journal={arXiv preprint arXiv:1909.08097},
  year={2019}
}
# online
@inproceedings{deepMutual,
  title={Deep mutual learning},
  author={Zhang, Ying and Xiang, Tao and Hospedales, Timothy M and Lu, Huchuan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4320--4328},
  year={2018}
}
@article{ONE,
  title={Knowledge distillation by on-the-fly native ensemble},
  author={Zhu, Xiatian and Gong, Shaogang and others},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@inproceedings{multiTeacher,
  title={Online knowledge distillation with diverse peers},
  author={Chen, Defang and Mei, Jian-Ping and Wang, Can and Feng, Yan and Chen, Chun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={3430--3437},
  year={2020}
}
@article{li2020explicit,
  title={Explicit connection distillation},
  author={Li, Lujun and Wang, Yikai and Yao, Anbang and Qian, Yi and Zhou, Xiao and He, Ke},
  year={2020}
}
@article{liu2021semi,
  title={Semi-online knowledge distillation},
  author={Liu, Zhiqiang and Liu, Yanxia and Huang, Chengkai},
  journal={arXiv preprint arXiv:2111.11747},
  year={2021}
}
@inproceedings{wu2021peer,
  title={Peer collaborative learning for online knowledge distillation},
  author={Wu, Guile and Gong, Shaogang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={12},
  pages={10302--10310},
  year={2021}
}
# self
@InProceedings{self-kd,
author = {Zhang, Linfeng and Song, Jiebo and Gao, Anni and Chen, Jingwei and Bao, Chenglong and Ma, Kaisheng},
title = {Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
month = {October},
year = {2019}
}
@article{yan2024orchestrate,
  title={Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation},
  author={Yan, HongWei and Wang, Liyuan and Ma, Kaisheng and Zhong, Yi},
  journal={arXiv preprint arXiv:2404.00417},
  year={2024}
}
@inproceedings{zhai2019lifelong,
  title={Lifelong gan: Continual learning for conditional image generation},
  author={Zhai, Mengyao and Chen, Lei and Tung, Frederick and He, Jiawei and Nawhal, Megha and Mori, Greg},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={2759--2768},
  year={2019}
}

@inproceedings{das2023understanding,
  title={Understanding self-distillation in the presence of label noise},
  author={Das, Rudrajit and Sanghavi, Sujay},
  booktitle={International Conference on Machine Learning},
  pages={7102--7140},
  year={2023},
  organization={PMLR}
}
@article{allen2020towards,
  title={Towards understanding ensemble, knowledge distillation and self-distillation in deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2012.09816},
  year={2020}
}

@inproceedings{yuan2020revisiting,
  title={Revisiting knowledge distillation via label smoothing regularization},
  author={Yuan, Li and Tay, Francis EH and Li, Guilin and Wang, Tao and Feng, Jiashi},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3903--3911},
  year={2020}
}



@article{zhang2020self,
  title={Self-distillation as instance-specific label smoothing},
  author={Zhang, Zhilu and Sabuncu, Mert},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2184--2195},
  year={2020}
}
@inproceedings{yang2019snapshot,
  title={Snapshot distillation: Teacher-student optimization in one generation},
  author={Yang, Chenglin and Xie, Lingxi and Su, Chi and Yuille, Alan L},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2859--2868},
  year={2019}
}
@article{lee2019rethinking,
  title={Rethinking data augmentation: Self-supervision and self-distillation},
  author={Lee, Hankook and Hwang, Sung Ju and Shin, Jinwoo},
  year={2019}
}
@inproceedings{phuong2019distillation,
  title={Distillation-based training for multi-exit architectures},
  author={Phuong, Mary and Lampert, Christoph H},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1355--1364},
  year={2019}
}
@inproceedings{lan2019self,
  title={Self-referenced deep learning},
  author={Lan, Xu and Zhu, Xiatian and Gong, Shaogang},
  booktitle={Asian Conference on Computer Vision},
  pages={284--300},
  year={2019},
  organization={Springer}
}
@article{mobahi2020self,
  title={Self-distillation amplifies regularization in hilbert space},
  author={Mobahi, Hossein and Farajtabar, Mehrdad and Bartlett, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3351--3361},
  year={2020}
}
# Flatness of minima
@inproceedings{zhao2023dot,
  title={DOT: A Distillation-Oriented Trainer},
  author={Zhao, Borui and Cui, Quan and Song, Renjie and Liang, Jiajun},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6189--6198},
  year={2023}
} 
@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}
@article{hochreiter1994simplifying,
  title={Simplifying neural nets by discovering flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Advances in Neural Information Processing Systems},
  volume={7},
  year={1994}
}
@article{izmailov2018averaging,
  title={Averaging weights leads to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1803.05407},
  year={2018}
}
@inproceedings{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1019--1028},
  year={2017},
  organization={PMLR}
}
@article{he2019asymmetric,
  title={Asymmetric valleys: Beyond sharp and flat local minima},
  author={He, Haowei and Huang, Gao and Yuan, Yang},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{EarlyPhase,
  title={The Early Phase of Neural Network Training},
  author={Frankle, Jonathan and Schwab, David J and Morcos, Ari S},
  booktitle={International Conference on Learning Representations}
}
@article{lewkowycz2020large,
  title={The large learning rate phase of deep learning: the catapult mechanism},
  author={Lewkowycz, Aitor and Bahri, Yasaman and Dyer, Ethan and Sohl-Dickstein, Jascha and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:2003.02218},
  year={2020}
}
@article{xie2020diffusion,
  title={A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima},
  author={Xie, Zeke and Sato, Issei and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2002.03495},
  year={2020}
}
@inproceedings{Jastrzebski2019Sharpest,
  title={On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length},
  author={ Jastrzebski, Stanislaw  and  Kenton, Zachary  and  Ballas, Nicolas  and  Fischer, Asja  and  Bengio, Yoshua  and  Storkey, Amos J. },
  booktitle={International Conference on Learning Representations},
  year={2019},
}



# knowledge gap
@InProceedings{KnowledgeGap_2020_CVPR,
author = {Liu, Yu and Jia, Xuhui and Tan, Mingxing and Vemulapalli, Raviteja and Zhu, Yukun and Green, Bradley and Wang, Xiaogang},
title = {Search to Distill: Pearls Are Everywhere but Not the Eyes},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
month = {June},
year = {2020}
}
@InProceedings{earlystop,
author = {Cho, Jang Hyun and Hariharan, Bharath},
title = {On the Efficacy of Knowledge Distillation},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
month = {October},
year = {2019}
}
@inproceedings{liu2020search,
  title={Search to distill: Pearls are everywhere but not the eyes},
  author={Liu, Yu and Jia, Xuhui and Tan, Mingxing and Vemulapalli, Raviteja and Zhu, Yukun and Green, Bradley and Wang, Xiaogang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7539--7548},
  year={2020}
}
@article{gao2018embarrassingly,
  title={An embarrassingly simple approach for knowledge distillation},
  author={Gao, Mengya and Shen, Yujun and Li, Quanquan and Yan, Junjie and Wan, Liang and Lin, Dahua and Loy, Chen Change and Tang, Xiaoou},
  journal={arXiv preprint arXiv:1812.01819},
  year={2018}
}
@inproceedings{kang2020towards,
  title={Towards oracle knowledge distillation with neural architecture search},
  author={Kang, Minsoo and Mun, Jonghwan and Han, Bohyung},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={4404--4411},
  year={2020}
}
@article{gu2020search,
  title={Search for better students to learn distilled knowledge},
  author={Gu, Jindong and Tresp, Volker},
  journal={arXiv preprint arXiv:2001.11612},
  year={2020}
}
@article{li2022shadow,
  title={Shadow knowledge distillation: Bridging offline and online knowledge transfer},
  author={Li, Lujun and Jin, Zhe},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={635--649},
  year={2022}
}
@article{liu2021learning,
  title={Learning to teach with student feedback},
  author={Liu, Yitao and Sun, Tianxiang and Qiu, Xipeng and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2109.04641},
  year={2021}
}
@article{zhou2021bert,
  title={BERT learns to teach: Knowledge distillation with meta learning},
  author={Zhou, Wangchunshu and Xu, Canwen and McAuley, Julian},
  journal={arXiv preprint arXiv:2106.04570},
  year={2021}
}
@article{liu2022meta,
  title={Meta knowledge distillation},
  author={Liu, Jihao and Liu, Boxiao and Li, Hongsheng and Liu, Yu},
  journal={arXiv preprint arXiv:2202.07940},
  year={2022}
}
# space effect
@article{ebbingham1913memory,
  title={Memory: A contribution to experimental psychology},
  author={Ebbinghaus, Hermann},
  journal={Annals of Neurosciences},
  volume={20},
  number={4},
  pages={155},
  year={2013},
  publisher={SAGE Publications}
}
@article{smolen2016right,
  title={The right time to learn: mechanisms and optimization of spaced learning},
  author={Smolen, Paul and Zhang, Yili and Byrne, John H},
  journal={Nature Reviews Neuroscience},
  volume={17},
  number={2},
  pages={77--88},
  year={2016},
  publisher={Nature Publishing Group UK London}
}

@misc{kurakin2017adversarial,
      title={Adversarial examples in the physical world}, 
      author={Alexey Kurakin and Ian Goodfellow and Samy Bengio},
      year={2017},
      eprint={1607.02533},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1607.02533}, 
}

@article{beck2000learning,
  title={Learning performance of normal and mutantdrosophila after repeated conditioning trials with discrete stimuli},
  author={Beck, CDO and Schroeder, Bradley and Davis, Ronald L},
  journal={Journal of Neuroscience},
  volume={20},
  number={8},
  pages={2944--2953},
  year={2000},
  publisher={Soc Neuroscience}
}
@article{pagani2009phosphatase,
  title={The phosphatase SHP2 regulates the spacing effect for long-term memory induction},
  author={Pagani, Mario R and Oishi, Kimihiko and Gelb, Bruce D and Zhong, Yi},
  journal={Cell},
  volume={139},
  number={1},
  pages={186--198},
  year={2009},
  publisher={Elsevier}
}
@article{menzel2001massed,
  title={Massed and spaced learning in honeybees: the role of CS, US, the intertrial interval, and the test interval},
  author={Menzel, Randolf and Manz, Gisela and Menzel, Rebecca and Greggers, Uwe},
  journal={Learning \& Memory},
  volume={8},
  number={4},
  pages={198--208},
  year={2001},
  publisher={Cold Spring Harbor Lab}
}
@article{anderson2008spaced,
  title={Spaced initial stimulus familiarization enhances novelty preference in Long-Evans rats},
  author={Anderson, Matthew J and Jablonski, Sarah A and Klimas, Diana B},
  journal={Behavioural Processes},
  volume={78},
  number={3},
  pages={481--486},
  year={2008},
  publisher={Elsevier}
}
@article{bello2013differential,
  title={Differential effects of spaced vs. massed training in long-term object-identity and object-location recognition memory},
  author={Bello-Medina, Paola C and S{\'a}nchez-Carrasco, Livia and Gonz{\'a}lez-Ornelas, Nadia R and Jeffery, Kathryn J and Ram{\'\i}rez-Amaya, V{\'\i}ctor},
  journal={Behavioural Brain Research},
  volume={250},
  pages={102--113},
  year={2013},
  publisher={Elsevier}
}
@article{medin1974comparative,
  title={The comparative study of memory},
  author={Medin, Douglas L},
  journal={Journal of Human Evolution},
  volume={3},
  number={6},
  pages={455--463},
  year={1974},
  publisher={Elsevier}
}
@article{robbins1973memory,
  title={Memory in great apes.},
  author={Robbins, Donald and Bush, Carol T},
  journal={Journal of Experimental Psychology},
  volume={97},
  number={3},
  pages={344},
  year={1973},
  publisher={American Psychological Association}
}
@article{donovan1999meta,
  title={A meta-analytic review of the distribution of practice effect: Now you see it, now you don't.},
  author={Donovan, John J and Radosevich, David J},
  journal={Journal of Applied Psychology},
  volume={84},
  number={5},
  pages={795},
  year={1999},
  publisher={American Psychological Association}
}
@article{shea2000spacing,
  title={Spacing practice sessions across days benefits the learning of motor skills},
  author={Shea, Charles H and Lai, Qin and Black, Charles and Park, Jin-Hoon},
  journal={Human Movement Science},
  volume={19},
  number={5},
  pages={737--760},
  year={2000},
  publisher={Elsevier}
}
@article{gluckman2014spacing,
  title={Spacing simultaneously promotes multiple forms of learning in children's science curriculum},
  author={Gluckman, Maxie and Vlach, Haley A and Sandhofer, Catherine M},
  journal={Applied Cognitive Psychology},
  volume={28},
  number={2},
  pages={266--273},
  year={2014},
  publisher={Wiley Online Library}
}
@article{roediger2008learning,
  title={Learning and memory: A comprehensive reference (Vol. 2)},
  author={Roediger, Henry L and Byrne, JH},
  journal={Cognitive Psychology of Memory},
  year={2008}
}
@article{sobel2011spacing,
  title={Spacing effects in real-world classroom vocabulary learning},
  author={Sobel, Hailey S and Cepeda, Nicholas J and Kapler, Irina V},
  journal={Applied Cognitive Psychology},
  volume={25},
  number={5},
  pages={763--767},
  year={2011},
  publisher={Wiley Online Library}
}
@article{vlach2014spacing,
  title={The spacing effect in children's generalization of knowledge: Allowing children time to forget promotes their ability to learn},
  author={Vlach, Haley A},
  journal={Child Development Perspectives},
  volume={8},
  number={3},
  pages={163--168},
  year={2014},
  publisher={Wiley Online Library}
}
@article{landauer1969reinforcement,
  title={Reinforcement as consolidation.},
  author={Landauer, Thomas K},
  journal={Psychological Review},
  volume={76},
  number={1},
  pages={82},
  year={1969},
  publisher={American Psychological Association}
}
@article{peterson1966short,
  title={Short-term verbal memory and learning.},
  author={Peterson, Lloyd R},
  journal={Psychological Review},
  volume={73},
  number={3},
  pages={193},
  year={1966},
  publisher={American Psychological Association}
}
@article{wickelgren1972trace,
  title={Trace resistance and the decay of long-term memory},
  author={Wickelgren, Wayne A},
  journal={Journal of Mathematical Psychology},
  volume={9},
  number={4},
  pages={418--455},
  year={1972},
  publisher={Elsevier}
}
@article{wu_alignment_2022,
	title = {The alignment property of {SGD} noise and how it helps select flat minima: A stability analysis},
	pages = {4680--4693},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Wu, Lei and Wang, Mingze and Su, Weijie},
	urldate = {2024-04-22},
	date = {2022-12-06},
	langid = {english},
}
@article{liu2021efficient,
  title={Efficient training of visual transformers with small datasets},
  author={Liu, Yahui and Sangineto, Enver and Bi, Wei and Sebe, Nicu and Lepri, Bruno and Nadai, Marco},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23818--23830},
  year={2021}
}
@inproceedings{li2022locality,
  title={Locality guidance for improving vision transformers on tiny datasets},
  author={Li, Kehan and Yu, Runyi and Wang, Zhennan and Yuan, Li and Song, Guoli and Chen, Jie},
  booktitle={European Conference on Computer Vision},
  pages={110--127},
  year={2022},
  organization={Springer}
}
@article{sun2024logit,
  title={Logit standardization in knowledge distillation},
  author={Sun, Shangquan and Ren, Wenqi and Li, Jingzhi and Wang, Rui and Cao, Xiaochun},
  journal={arXiv preprint arXiv:2403.01427},
  year={2024}
}
@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}
@misc{loshchilov2017sgdr,
      title={SGDR: Stochastic Gradient Descent with Warm Restarts}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2017},
      eprint={1608.03983},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International Conference on Machine Learning},
  pages={1139--1147},
  year={2013},
  organization={PMLR}
}
@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}
@inproceedings{blanc2020implicit,
  title={Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process},
  author={Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
  booktitle={Conference on Learning Theory},
  pages={483--513},
  year={2020},
  organization={PMLR}
}
@article{damian2021label,
  title={Label noise sgd provably prefers flat global minimizers},
  author={Damian, Alex and Ma, Tengyu and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={27449--27461},
  year={2021}
}
@article{zhou2020towards,
  title={Towards theoretically understanding why sgd generalizes better than adam in deep learning},
  author={Zhou, Pan and Feng, Jiashi and Ma, Chao and Xiong, Caiming and Hoi, Steven Chu Hong and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21285--21296},
  year={2020}
}
@inproceedings{orvieto2023explicit,
  title={Explicit regularization in overparametrized models via noise injection},
  author={Orvieto, Antonio and Raj, Anant and Kersting, Hans and Bach, Francis},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={7265--7287},
  year={2023},
  organization={PMLR}
}
@inproceedings{tsuzuku2020normalized,
  title={Normalized flat minima: Exploring scale invariant definition of flat minima for neural networks using pac-bayesian analysis},
  author={Tsuzuku, Yusuke and Sato, Issei and Sugiyama, Masashi},
  booktitle={International Conference on Machine Learning},
  pages={9636--9647},
  year={2020},
  organization={PMLR}
}
@article{michaelis2019dragon,
  title={Benchmarking Robustness in Object Detection: Autonomous Driving when Winter is Coming},
  author={Michaelis, Claudio and Mitzkus, Benjamin and 
    Geirhos, Robert and Rusak, Evgenia and 
    Bringmann, Oliver and Ecker, Alexander S. and 
    Bethge, Matthias and Brendel, Wieland},
  journal={arXiv preprint arXiv:1907.07484},
  year={2019}
}

@article{li2022distilling,
  title={Distilling a powerful student model via online knowledge distillation},
  author={Li, Shaojie and Lin, Mingbao and Wang, Yan and Wu, Yongjian and Tian, Yonghong and Shao, Ling and Ji, Rongrong},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={34},
  number={11},
  pages={8743--8752},
  year={2022},
  publisher={IEEE}
}

@inproceedings{qian2022switchable,
  title={Switchable online knowledge distillation},
  author={Qian, Biao and Wang, Yang and Yin, Hongzhi and Hong, Richang and Wang, Meng},
  booktitle={European Conference on Computer Vision},
  pages={449--466},
  year={2022},
  organization={Springer}
}

@inproceedings{DLB,
  title={Self-distillation from the last mini-batch for consistency regularization},
  author={Shen, Yiqing and Xu, Liwu and Yang, Yuzhe and Li, Yaqian and Guo, Yandong},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11943--11952},
  year={2022}
}

@inproceedings{PSKD,
  title={Self-knowledge distillation with progressive refinement of targets},
  author={Kim, Kyungyul and Ji, ByeongMoon and Yoon, Doyoung and Hwang, Sangheum},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6567--6576},
  year={2021}
}
@inproceedings{shi2021follow,
  title={Follow your path: a progressive method for knowledge distillation},
  author={Shi, Wenxian and Song, Yuxuan and Zhou, Hao and Li, Bohan and Li, Lei},
  booktitle={Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13--17, 2021, Proceedings, Part III 21},
  pages={596--611},
  year={2021},
  organization={Springer}
}
@article{rezagholizadeh2021pro,
  title={Pro-KD: Progressive distillation by following the footsteps of the teacher},
  author={Rezagholizadeh, Mehdi and Jafari, Aref and Salad, Puneeth and Sharma, Pranav and Pasand, Ali Saheb and Ghodsi, Ali},
  journal={arXiv preprint arXiv:2110.08532},
  year={2021}
}
