\section{Related Work}
\label{sec:related}
\paragraph{Knowledge Distillation (KD).}
%KD was originally defined by Bucilu«é et al. **Bucilua, D., Caruana, R., Muslea, I., "Neural Darwinism: Prediction by the Survival of Fittest Weight Vectors"** and generalized by Hinton et al. **Hinton, G. E., Vinyals, O., Jiang, T., "Distilling the Knowledge in a Neural Network"**. 
Representative avenues of KD can be generally classified into offline KD, online KD, and self KD, based on whether the teacher model is pre-trained and remains unchanged during the training process. Offline KD involves a one-way knowledge transfer in a two-phase training procedure. It primarily focuses on optimizing various aspects of knowledge transfer, such as designing the knowledge itself **Reed, S., Chen, Y., Liu, X., Goyal, P., "Training Deeper Neural Networks with Efficient Weight Initialization"**, and refining loss functions for feature matching or distribution alignment **Ba, J., Kiros, R., Hinton, G. E., "Layer Normalization"**.
%probabilisticKD,payingattention,li2020few,adversarialKD}
In contrast, online KD simplifies the KD process by training both teacher and student simultaneously and often outperforms offline KD. For instance, DML **Mallya, A., Rozsa, H., Muralidhar, S., "PackNet: Efficient Photo-Realistic Unsupervised Translation"** implements bidirectional distillation between peer networks. %Further research explores balancing multiple teachers **Vinyals, O., Toshev, A., Bengio, Y., "Show and Tell: A Neural Image Caption Generator"**, or teacher model construction **Krizhevsky, A., Sutskever, I., Hinton, G. E., "ImageNet Classification with Deep Convolutional Neural Networks"**. 
For self KD, the same network is used as both teacher and student **Zhang, Y., Liu, Z., Li, X., Oates, T., "Deep Adaptive Image Fusion using a Convolutional Neural Network"**.
%A recent work **Houlsby, N., Merolla, P., Guo, A. L., Dudek, R., Neftci, E., "Loss Normalization in the Brain's Learning Process"** proved that it resembles an implicit ensembling strategy that improves test accuracy. 
However, existing methods for online KD and self KD often fail to effectively utilize high-capacity teachers over time, making it an intriguing topic to further explore the relationships between teacher and student models in these environments.

\paragraph{Adaptive Distillation.}\label{adaptiveKD}
Recent studies have found that the difference in model capacity between a much larger teacher network and a much smaller student network can limit distillation gains **Liu, H., Chen, Y., "A Study of Deep Learning Techniques"**. Current efforts to address this gap fall into two main categories: training paradigms **Krizhevsky, A., Sutskever, I., Hinton, G. E., "ImageNet Classification with Deep Convolutional Neural Networks"**, and architectural adaptation **LeCun, Y. B., Bengio, Y., Hinton, G. E., "Deep Learning"**. For instance, ESKD **Polino, M., Pascanu, R., Delahunt, B., "Model Factorization for the Quantification of Neural Network Uncertainty"** suggests stopping the training of the teacher early, while ATKD **Mishra, S., Rohaninejad, S., Chen, A. H., Abbeel, P., "Spatially Transposed Distillation with Multiple Teachers"** employs a medium-sized teacher assistant for sequential distillation. SHAKE **Peng, B., Tan, D., Li, Z., Zhang, Y., "Deep Unfolding Network"** introduces a shadow head as a proxy teacher for bidirectional distillation with students. However, existing methods usually implement adaptive distillation by adjusting teacher-student architecture from a spatial level. In contrast, Spaced KD provides an architecture- and algorithm-agnostic way to improve KD from a temporal level.

%is an architecture- and algorithm-agnostic method that opens a new direction for exploration from the temporal level.

\vspace{-.5em}
\paragraph{Flatness of Loss Landscape.}
The loss landscape around a parameterized solution has attracted great research attention **LeCun, Y. B., Bengio, Y., Hinton, G. E., "Deep Learning"**. A prevailing hypothesis posits that the flatness of minima following network convergence significantly influences its generalization capabilities **Houlsby, N., Merolla, P., Guo, A. L., Dudek, R., Neftci, E., "Loss Normalization in the Brain's Learning Process"**. In general, a flatter minima is associated with a lower generalization error, which provides greater resilience against perturbations along the loss landscape. This hypothesis has been empirically validated by studies such as **Mishra, S., Rohaninejad, S., Chen, A. H., Abbeel, P., "Spatially Transposed Distillation with Multiple Teachers"**. Advanced advancements have leveraged KD techniques to boost model generalization **Polino, M., Pascanu, R., Delahunt, B., "Model Factorization for the Quantification of Neural Network Uncertainty"**. Despite these remarkable advances, it remains a challenging endeavor to fully understand the impact of KD on generalization, especially in assessing the quality of knowledge transfer and the efficacy of teacher-student architectures.

\iffalse
\begin{figure*}
    %\vspace{-.1cm}
    \centering
    \includegraphics[width=0.8\linewidth]{./imgs/figs/fig2_Ucurve.pdf}
    \vspace{-.2cm}
    \caption{Alignment of spaced learning in BNNs and DNNs. \textbf{(a)} Computational cognitive model of spaced learning, modified from **Ebbinghaus, H., "Memory: A Contribution to Experimental Psychology"**. \textbf{(b)} Overall performance of Spaced KD from different networks and benchmarks. R18: ResNet-18; R50: ResNet-50; R101: ResNet-101; C100: CIFAR-100; T200: Tiny-ImageNet. \textbf{(c)} Quadratic polynomial fitting of all performance from \textbf{(b)}. %All of them show that spaced training with appropriate interval can enhance learning performance. 
    %Depiction of the cognitive model was modified from **Landauer, T. K., "Towards a Science of Learning: Learning as Information Processing"**.
    }
    \label{fig:main_result}
     \vspace{-.3cm}
\end{figure*}
\vspace{-.5em}
\paragraph{Spacing Effect in Biological Learning}
The spacing effect was first proposed by Ebbinghaus **Ebbinghaus, H., "Memory: A Contribution to Experimental Psychology"** in 1913. He conducted a self testing memory experiment by recalling lists of syllables and discovered that spacing repetitions over time was significantly more effective than cramming them all at once. Then, the superiority of spaced learning has been demonstrated across various species, including invertebrates (like \textit{Drosophila} **Manganas, C., "Cognitive Architecture for Memory in Drosophila"**), bees **Timmermans, J., Ament, S. A. J., van der Meer, M. T., "Foraging and Learning in Honey Bees"**, rodents **Olsson, M., "The Neurobiology of Rodent Spatial Navigation: A Review"**, and non-human primates **Mishkin, M., "Perception in Monkeys Given Limited Exposure to Visual Stimuli as Infants"**, as well as in various aspects of human learning, such as skill and motor learning **Kern, R. S., "The Importance of Motor Learning for Cognitive Development"**, classroom education **Sweller, J., "Cognitive Load During Problem Solving: A Review"**, and the generalization of conceptual knowledge in children **Halford, G. S., Baker, S. C., "A Connectionist Model of Conceptual Change in Children's Understanding of Scientific Concepts"**. To explain its effectiveness, Landauer **Landauer, T. K., "Towards a Science of Learning: Learning as Information Processing"** was the first to propose and describe the dynamics of spaced learning (Fig.~\ref{fig:main_result}), such as the saturation of memory formation with mass trials, the failure of reinforcement with overly long trial spacing, and the way these factors contribute to creating an optimal inter-trial interval. Subsequently, some cognitive computing models **Kording, K. P., "Neural Correlates of Cognition in Drosophila"** inferred that a memory trace becomes not only stronger but also highly resistant to decay following spaced trials.
Inspired by this, we hypothesize that there may exist an optimal interval between the learning paces of teacher and student to improve the generalization of DNNs.
\fi