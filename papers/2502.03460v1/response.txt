\section{Related Work}
\label{sec:Related Work}

\paragraph{Pruning} Pruning removes weights and modifies the model's architecture. Formally, given a neural network $f(x;W)$, pruning produces a new model $f(x;M \odot W)$, where $M \in \{0,1\}^{|W|}$ is a binary mask that sets certain parameters to zero, and $\odot$ denotes element-wise multiplication. Pruning typically hurts the network's performance, so post-training is often employed to recover the loss**Han et al., "DPP: A Deterministic Pruning Framework for Deep Neural Networks"**. Overall, pruning methods can be categorized into two types, unstructured pruning and structured pruning. Unstructured pruning removes individual weights, resulting in sparsified weight matrices, but normally face difficulties in inference speedups when specialized hardware is unavailable**Frankle et al., "The Lottery Ticket Hypothesis: Training Pruned Neural Networks"**. In contrast, structured pruning operates at a larger granularity, removing entire weight groups. This includes width pruning**Molchanov et al., "Regularization of Neural Networks by Depth Multiplication"**, which removes groups of coupled weights, and depth pruning**Liu et al., "Learning to Prune Deep Neural Models via Confidence Rating"**, which eliminates entire layers. Our focus is on post-training structured pruning, balancing generality and hardware efficiency.

\paragraph{Adaptive Sparsity for Pruning} Several works have explored adaptive compression.**Guo et al., "Dynamic Channel Pruning: Feature Importance-Driven Sparse Model Selection and Acceleration"** selects transformer feedforward experts and removes feedforward neurons during inference based on their high activation magnitudes from input prompts. While this method is effective, we seek an approach that can reduce model size without depending on specific input prompts.**Zhu et al., "Sparsity Induction in Deep Neural Networks via a Variance-Based Criterion"** computes the sample variance of each input feature and weights it by the squared norm of the corresponding column in the weight matrix to determine a layer’s importance and assign sparsity accordingly.**Li et al., "Adaptive Sparsity for Pruning: A Reinforcement Learning Approach"** determines the optimal layer-wise sparsity distribution through reinforcement learning, where different sparsity patterns are sampled and updated based on their performance rewards. However, all aforementioned methods failed to take into account the loss on the functional aspect of models, where the overall mapping $X \rightarrow Y$ of the LLM is expected to be preserved during the pruning process to minimize the performance drop. Here $X$ and $Y$ are input and output tensors of the model. Our method utilizes the mapping information to assign an adaptive sparsity across different decoder layers.

\paragraph{Knowledge Distillation} Knowledge distillation is a technique used to transfer the advanced capabilities of high-performing LLMs to smaller models **Kim et al., "Sequence-Level Knowledge Distillation"**. Combining knowledge distillation with pruning can yield strong performance, where the original model acts as the teacher and the compressed model serves as the student**Romero et al., "FitNets: Hints-Based Deep Neural Network Architectures for Image Super-Resolution"**.
Conventionally, knowledge distillation can be categorized into black-box **Buciluǎ et al., "Training a Support Vector Machine in Parallel on Large Datasets"** and white-box distillation **Hinton et al., "Distilling the Knowledge in a Neural Network"**, depending on whether the model weights and prediction logits of the teacher model can be accessed. In that sense, structural-pruning-based acceleration methods can be roughly viewed as white-box knowledge distillation. However,
in this paper, we focus on the adaptive pruning algorithm with supervised fine-tuning only. Though it is expected to achieve even better performance when integrated with state-of-the-art knowledge distillation techniques in post-training, it normally incurs more complexity and training cost, hence we leave that for future exploration.

 \paragraph{Accelerating Small Language Model Training} Training small language models from scratch demands substantial computational resources, making pruning of larger models with recovery post-training an attractive alternative. Recent works have explored various approaches to this challenge.**Fan et al., "LSTM-CNN-RNN: A Hierarchical Structure for Deep Neural Networks"** proposes a systematic pruning framework that optimizes across four architectural dimensions (layer count, hidden dimension, attention heads, and MLP size) towards target architectures, followed by a continual post-training phase phase with dynamic batch loading.**Sun et al., "Knowledge Distillation as Weighted Loss"** leverages knowledge distillation, first fine-tuning the original model as a teacher and then transferring its knowledge to the pruned student model. However, these methods rely on single-phase post-training and do not exploit the potential benefits of incremental pruning with interleaved recovery phases, which is demonstrated to be effective in the experiments.