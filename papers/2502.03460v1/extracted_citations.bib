@misc{FLAP,
      title={Fluctuation-based Adaptive Structured Pruning for Large Language Models}, 
      author={Yongqi An and Xu Zhao and Tao Yu and Ming Tang and Jinqiao Wang},
      year={2023},
      eprint={2312.11983},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{MLSYS2020_6c44dc73,
 author = {Blalock, Davis and Gonzalez Ortiz, Jose Javier and Frankle, Jonathan and Guttag, John},
 booktitle = {Proceedings of Machine Learning and Systems},
 editor = {I. Dhillon and D. Papailiopoulos and V. Sze},
 pages = {129--146},
 title = {What is the State of Neural Network Pruning?},
 url = {https://proceedings.mlsys.org/paper_files/paper/2020/file/6c44dc73014d66ba49b28d483a8f8b0d-Paper.pdf},
 volume = {2},
 year = {2020}
}

@misc{SliceGPT,
      title={SliceGPT: Compress Large Language Models by Deleting Rows and Columns}, 
      author={Saleh Ashkboos and Maximilian L. Croci and Marcelo Gennari do Nascimento and Torsten Hoefler and James Hensman},
      year={2024},
      eprint={2401.15024},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.15024}, 
}

@article{agarwal2023gkd,
  title={Gkd: Generalized knowledge distillation for auto-regressive sequence models},
  author={Agarwal, Rishabh and Vieillard, Nino and Stanczyk, Piotr and Ramos, Sabela and Geist, Matthieu and Bachem, Olivier},
  journal={arXiv preprint arXiv:2306.13649},
  year={2023}
}

@misc{dery2024everybodyprunenowstructured,
      title={Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes}, 
      author={Lucio Dery and Steven Kolawole and Jean-Fran√ßois Kagy and Virginia Smith and Graham Neubig and Ameet Talwalkar},
      year={2024},
      eprint={2402.05406},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.05406}, 
}

@inproceedings{dong2024prompt,
  title={Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation},
  author={Dong, Harry and Chen, Beidi and Chi, Yuejie},
  booktitle={First Conference on Language Modeling},
  year={2024}
}

@article{gu2023knowledge,
  title={Knowledge distillation of large language models},
  author={Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  journal={arXiv preprint arXiv:2306.08543},
  year={2023}
}

@article{ho2022large,
  title={Large language models are reasoning teachers},
  author={Ho, Namgyu and Schmid, Laura and Yun, Se-Young},
  journal={arXiv preprint arXiv:2212.10071},
  year={2022}
}

@article{hsieh2023distilling,
  title={Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes},
  author={Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
  journal={arXiv preprint arXiv:2305.02301},
  year={2023}
}

@misc{kim2024shortenedllamadepthpruning,
      title={Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods}, 
      author={Bo-Kyeong Kim and Geonmin Kim and Tae-Ho Kim and Thibault Castells and Shinkook Choi and Junho Shin and Hyoung-Kyu Song},
      year={2024},
      eprint={2402.02834},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.02834}, 
}

@article{latif2023knowledge,
  title={Knowledge distillation of llm for education},
  author={Latif, Ehsan and Fang, Luyang and Ma, Ping and Zhai, Xiaoming},
  journal={arXiv preprint arXiv:2312.15842},
  year={2023}
}

@inproceedings{llm-pruner,
  title={LLM-Pruner: On the Structural Pruning of Large Language Models},
  author={Xinyin Ma and Gongfan Fang and Xinchao Wang},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023},
}

@article{shum2024first, title={First: Teach a reliable large language model through efficient trustworthy distillation}, author={Shum, KaShun and Xu, Minrui and Zhang, Jianshu and Chen, Zixin and Diao, Shizhe and Dong, Hanze and Zhang, Jipeng and Raza, Muhammad Omer}, journal={arXiv preprint arXiv:2408.12168}, year={2024} }

@misc{siddiqui2024deeperlookdepthpruning,
      title={A deeper look at depth pruning of LLMs}, 
      author={Shoaib Ahmed Siddiqui and Xin Dong and Greg Heinrich and Thomas Breuel and Jan Kautz and David Krueger and Pavlo Molchanov},
      year={2024},
      eprint={2407.16286},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.16286}, 
}

@misc{sreenivas2024llmpruningdistillationpractice,
      title={LLM Pruning and Distillation in Practice: The Minitron Approach}, 
      author={Sharath Turuvekere Sreenivas and Saurav Muralidharan and Raviraj Joshi and Marcin Chochowski and Mostofa Patwary and Mohammad Shoeybi and Bryan Catanzaro and Jan Kautz and Pavlo Molchanov},
      year={2024},
      eprint={2408.11796},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.11796}, 
}

@misc{xia2024shearedllamaacceleratinglanguage,
      title={Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning}, 
      author={Mengzhou Xia and Tianyu Gao and Zhiyuan Zeng and Danqi Chen},
      year={2024},
      eprint={2310.06694},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06694}, 
}

@misc{xu2024surveyknowledgedistillationlarge,
      title={A Survey on Knowledge Distillation of Large Language Models}, 
      author={Xiaohan Xu and Ming Li and Chongyang Tao and Tao Shen and Reynold Cheng and Jinyang Li and Can Xu and Dacheng Tao and Tianyi Zhou},
      year={2024},
      eprint={2402.13116},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.13116}, 
}

@article{zhou2023distillspec,
  title={Distillspec: Improving speculative decoding via knowledge distillation},
  author={Zhou, Yongchao and Lyu, Kaifeng and Rawat, Ankit Singh and Menon, Aditya Krishna and Rostamizadeh, Afshin and Kumar, Sanjiv and Kagy, Jean-Fran{\c{c}}ois and Agarwal, Rishabh},
  journal={arXiv preprint arXiv:2310.08461},
  year={2023}
}

