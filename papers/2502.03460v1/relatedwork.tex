\section{Related Work}
\label{sec:Related Work}

\paragraph{Pruning} Pruning removes weights and modifies the model's architecture. Formally, given a neural network $f(x;W)$, pruning produces a new model $f(x;M \odot W)$, where $M \in \{0,1\}^{|W|}$ is a binary mask that sets certain parameters to zero, and $\odot$ denotes element-wise multiplication. Pruning typically hurts the network's performance, so post-training is often employed to recover the loss~\citep{MLSYS2020_6c44dc73}. Overall, pruning methods can be categorized into two types, unstructured pruning and structured pruning. Unstructured pruning removes individual weights, resulting in sparsified weight matrices, but normally face difficulties in inference speedups when specialized hardware is unavailable~\citep{dery2024everybodyprunenowstructured}. In contrast, structured pruning operates at a larger granularity, removing entire weight groups. This includes width pruning~\citep{llm-pruner, SliceGPT}, which removes groups of coupled weights, and depth pruning~\citep{kim2024shortenedllamadepthpruning, siddiqui2024deeperlookdepthpruning}, which eliminates entire layers. Our focus is on post-training structured pruning, balancing generality and hardware efficiency.

\paragraph{Adaptive Sparsity for Pruning} Several works have explored adaptive compression.~\citet{dong2024prompt} selects transformer feedforward experts and removes feedforward neurons during inference based on their high activation magnitudes from input prompts. While this method is effective, we seek an approach that can reduce model size without depending on specific input prompts.~\citet{FLAP} computes the sample variance of each input feature and weights it by the squared norm of the corresponding column in the weight matrix to determine a layerâ€™s importance and assign sparsity accordingly.~\citet{RLPruner} determines the optimal layer-wise sparsity distribution through reinforcement learning, where different sparsity patterns are sampled and updated based on their performance rewards. However, all aforementioned methods failed to take into account the loss on the functional aspect of models, where the overall mapping $X \rightarrow Y$ of the LLM is expected to be preserved during the pruning process to minimize the performance drop. Here $X$ and $Y$ are input and output tensors of the model. Our method utilizes the mapping information to assign an adaptive sparsity across different decoder layers.

\paragraph{Knowledge Distillation} Knowledge distillation is a technique used to transfer the advanced capabilities of high-performing LLMs to smaller models \citep{xu2024surveyknowledgedistillationlarge}. Combining knowledge distillation with pruning can yield strong performance, where the original model acts as the teacher and the compressed model serves as the student~\citep{sreenivas2024llmpruningdistillationpractice}.
Conventionally, knowledge distillation can be categorized into black-box~\citep{ho2022large,hsieh2023distilling} and white-box distillation~\citep{gu2023knowledge,latif2023knowledge,agarwal2023gkd,zhou2023distillspec,shum2024first}, depending on whether the model weights and prediction logits of the teacher model can be accessed. In that sense, structural-pruning-based acceleration methods can be roughly viewed as white-box knowledge distillation. However,
in this paper, we focus on the adaptive pruning algorithm with supervised fine-tuning only. Though it is expected to achieve even better performance when integrated with state-of-the-art knowledge distillation techniques in post-training, it normally incurs more complexity and training cost, hence we leave that for future exploration.

 \paragraph{Accelerating Small Language Model Training} Training small language models from scratch demands substantial computational resources, making pruning of larger models with recovery post-training an attractive alternative. Recent works have explored various approaches to this challenge.~\citet{xia2024shearedllamaacceleratinglanguage} proposes a systematic pruning framework that optimizes across four architectural dimensions (layer count, hidden dimension, attention heads, and MLP size) towards target architectures, followed by a continual post-training phase phase with dynamic batch loading.~\citet{sreenivas2024llmpruningdistillationpractice} leverages knowledge distillation, first fine-tuning the original model as a teacher and then transferring its knowledge to the pruned student model. However, these methods rely on single-phase post-training and do not exploit the potential benefits of incremental pruning with interleaved recovery phases, which is demonstrated to be effective in the experiments.