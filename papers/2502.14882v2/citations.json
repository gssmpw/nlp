[
  {
    "index": 0,
    "papers": [
      {
        "key": "lin2024moe",
        "author": "Lin, Bin and Tang, Zhenyu and Ye, Yang and Cui, Jiaxi and Zhu, Bin and Jin, Peng and Huang, Jinfa and Zhang, Junwu and Pang, Yatian and Ning, Munan and others",
        "title": "Moe-llava: Mixture of experts for large vision-language models"
      },
      {
        "key": "abdin2024phi",
        "author": "Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others",
        "title": "Phi-3 technical report: A highly capable language model locally on your phone"
      },
      {
        "key": "chu2023mobilevlm",
        "author": "Chu, Xiangxiang and Qiao, Limeng and Lin, Xinyang and Xu, Shuang and Yang, Yang and Hu, Yiming and Wei, Fei and Zhang, Xinyu and Zhang, Bo and Wei, Xiaolin and others",
        "title": "Mobilevlm: A fast, strong and open vision language assistant for mobile devices"
      },
      {
        "key": "chu2024mobilevlm",
        "author": "Chu, Xiangxiang and Qiao, Limeng and Zhang, Xinyu and Xu, Shuang and Wei, Fei and Yang, Yang and Sun, Xiaofei and Hu, Yiming and Lin, Xinyang and Zhang, Bo and others",
        "title": "Mobilevlm v2: Faster and stronger baseline for vision language model"
      },
      {
        "key": "li2024mini",
        "author": "Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya",
        "title": "Mini-gemini: Mining the potential of multi-modality vision language models"
      },
      {
        "key": "yao2024minicpm",
        "author": "Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others",
        "title": "Minicpm-v: A gpt-4v level mllm on your phone"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zhang2024treat",
        "author": "Zhang, Zeliang and Pham, Phu and Zhao, Wentian and Wan, Kun and Li, Yu-Jhe and Zhou, Jianing and Miranda, Daniel and Kale, Ajinkya and Xu, Chenliang",
        "title": "Treat visual tokens as text? but your mllm only needs fewer efforts to see"
      },
      {
        "key": "shang2024llava-prumerge",
        "author": "Shang, Yuzhang and Cai, Mu and Xu, Bingxin and Lee, Yong Jae and Yan, Yan",
        "title": "Llava-prumerge: Adaptive token reduction for efficient large multimodal models"
      },
      {
        "key": "xing2024pyramiddrop",
        "author": "Xing, Long and Huang, Qidong and Dong, Xiaoyi and Lu, Jiajie and Zhang, Pan and Zang, Yuhang and Cao, Yuhang and He, Conghui and Wang, Jiaqi and Wu, Feng and others",
        "title": "Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction"
      },
      {
        "key": "chen2024fastv",
        "author": "Chen, Liang and Zhao, Haozhe and Liu, Tianyu and Bai, Shuai and Lin, Junyang and Zhou, Chang and Chang, Baobao",
        "title": "An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "kwon2023efficient",
        "author": "Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion",
        "title": "Efficient memory management for large language model serving with pagedattention"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, A",
        "title": "Attention is all you need"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhang2023h2o",
        "author": "Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\\'e}, Christopher and Barrett, Clark and others",
        "title": "H2o: Heavy-hitter oracle for efficient generative inference of large language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhang2023h2o",
        "author": "Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\\'e}, Christopher and Barrett, Clark and others",
        "title": "H2o: Heavy-hitter oracle for efficient generative inference of large language models"
      },
      {
        "key": "jin2024llm",
        "author": "Jin, Hongye and Han, Xiaotian and Yang, Jingfeng and Jiang, Zhimeng and Liu, Zirui and Chang, Chia-Yuan and Chen, Huiyuan and Hu, Xia",
        "title": "Llm maybe longlm: Self-extend llm context window without tuning"
      },
      {
        "key": "cai2024pyramidkv",
        "author": "Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and others",
        "title": "Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zhang2023h2o",
        "author": "Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\\'e}, Christopher and Barrett, Clark and others",
        "title": "H2o: Heavy-hitter oracle for efficient generative inference of large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "jin2024llm",
        "author": "Jin, Hongye and Han, Xiaotian and Yang, Jingfeng and Jiang, Zhimeng and Liu, Zirui and Chang, Chia-Yuan and Chen, Huiyuan and Hu, Xia",
        "title": "Llm maybe longlm: Self-extend llm context window without tuning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "cai2024pyramidkv",
        "author": "Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and others",
        "title": "Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhang2023h2o",
        "author": "Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\\'e}, Christopher and Barrett, Clark and others",
        "title": "H2o: Heavy-hitter oracle for efficient generative inference of large language models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "tu2024vl",
        "author": "Tu, Dezhan and Vashchilenko, Danylo and Lu, Yuzhe and Xu, Panpan",
        "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "liukivi",
        "author": "Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia",
        "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "hooper2024kvquant",
        "author": "Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Sophia and Keutzer, Kurt and Gholami, Amir",
        "title": "Kvquant: Towards 10 million context length llm inference with kv cache quantization"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zandieh2024qjl",
        "author": "Zandieh, Amir and Daliri, Majid and Han, Insu",
        "title": "Qjl: 1-bit quantized jl transform for kv cache quantization with zero overhead"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "tu2024vl",
        "author": "Tu, Dezhan and Vashchilenko, Danylo and Lu, Yuzhe and Xu, Panpan",
        "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration"
      }
    ]
  }
]