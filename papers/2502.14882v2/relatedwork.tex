\section{Related Work}
\paragraph{Efficient Inference of MLLMs.}
Multimodal large language models (MLLMs) typically contain billions of parameters, posing significant challenges in both memory consumption and computational efficiency during deployment. Numerous studies have explored cost reduction strategies for MLLM deployment, including designing compact multimodal models~\citep{lin2024moe,abdin2024phi,chu2023mobilevlm,chu2024mobilevlm,li2024mini, yao2024minicpm}, model pruning~\citep{zhang2024treat,shang2024llava-prumerge,xing2024pyramiddrop,chen2024fastv}, and hardware-software co-optimization~\citep{kwon2023efficient}. However, the self-attention mechanism, which has quadratic computational complexity, remains a bottleneck~\citep{vaswani2017attention}. As input sequence length increases, both memory usage and computational burden grow correspondingly. During decoding, every generated token involves computations over all preceding input tokens, exacerbating inefficiencies.

\paragraph{KV Cache Compression.}
The KV cache technique has been introduced to mitigate redundant computations~\citep{zhang2023h2o}. By caching key and value embeddings in memory, the KV cache allows the model to reuse stored information instead of recomputing attention scores for all previous tokens. This approach effectively trades off memory usage for computational efficiency, significantly improving inference speed. 
While the KV cache technique substantially reduces computational overhead, it introduces a new bottleneck: memory consumption. This issue becomes increasingly critical in scenarios involving long-context generation and multi-turn conversations, where growing input lengths negatively impact throughput.

A common approach to reducing the size of the KV cache is to remove or evict unimportant key and value vectors from the cache.
A line of research explores various importance scores to evict tokens~\cite{zhang2023h2o, jin2024llm, cai2024pyramidkv}. 
% Leveraging the inherent sparsity of the KV cache, many studies have demonstrated that numerous unimportant tokens can be discarded during inference with minimal impact on performance. For instance, Heavy-Hitter Oracle (H$_2$O)~\citep{zhang2023h2o} greedily removes tokens with the lowest attention values during decoding, while the sliding window approach~\citep{jin2024llm} retains only the most recent tokens in cache. PyramidKV~\citep{cai2024pyramidkv} optimizes memory usage by allocating different cache budgets across layers to balance efficiency and performance. Based on~\citep{zhang2023h2o}, ~\citep{tu2024vl} designed KV cache dropping strategy tailored for MLLMs.
Quantization techniques provide another path to reducing KV cache memory overhead by transforming floating-point representations into lower-precision integers, thus significantly reducing memory usage and potentially enhancing inference speed. Methods such as KIVI~\citep{liukivi} have introduced asymmetric quantization strategies.
KVQuant~\citep{hooper2024kvquant} introduces advanced KV cache quantization techniques, including per-channel, pre-RoPE, non-uniform, and per-vector quantization, significantly improving accuracy at low bitwidths.
QJL~\citep{zandieh2024qjl} leverages a Johnson-Lindenstrauss (JL) transform combined with sign-bit quantization to eliminate the storage overhead associated with quantization constants in KV cache quantization.
In contrast to these approaches developed primarily for general-purpose LLMs, our method specifically addresses the unique challenges of MLLMs, where visual tokens dominate the KV cache and present distinct statistical patterns. By carefully exploiting the distributional characteristics of visual activations, we introduce a specialized channel-wise quantization combined with attention-aware calibration, significantly reducing memory footprint while preserving multimodal reasoning capabilities.
%Most existing techniques focus on general-purpose LLMs, often overlooking the unique characteristics of MLLMs. Recent research suggests that leveraging post-vision attention values can more effectively identify important tokens compared to conventional attention-based strategies in MLLMs~\citep{tu2024vl}. In this work, we further explore KV-cache compression tailored for MLLMs, aiming to enhance efficiency while preserving model performance.