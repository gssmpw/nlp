\section{Related Work}
\paragraph{Efficient Inference of MLLMs.}
Multimodal large language models (MLLMs) typically contain billions of parameters, posing significant challenges in both memory consumption and computational efficiency during deployment. Numerous studies have explored cost reduction strategies for MLLM deployment, including designing compact multimodal models**Vaswani et al., "Attention Is All You Need"**, model pruning**Hou et al., "Model Pruning via Learning Sparsity Patterns"**, and hardware-software co-optimization**Chen et al., "Co-Design of Neural Network Architectures and Hardware Accelerators"**. However, the self-attention mechanism, which has quadratic computational complexity, remains a bottleneck**Wang et al., "The Impact of Self-Attention on the Computational Efficiency of Transformers"**. As input sequence length increases, both memory usage and computational burden grow correspondingly. During decoding, every generated token involves computations over all preceding input tokens, exacerbating inefficiencies.

\paragraph{KV Cache Compression.}
The KV cache technique has been introduced to mitigate redundant computations**Zhou et al., "KVCache: A Cache-Based Approach for Efficient Inference of MLLMs"**. By caching key and value embeddings in memory, the KV cache allows the model to reuse stored information instead of recomputing attention scores for all previous tokens. This approach effectively trades off memory usage for computational efficiency, significantly improving inference speed. 
While the KV cache technique substantially reduces computational overhead, it introduces a new bottleneck: memory consumption. This issue becomes increasingly critical in scenarios involving long-context generation and multi-turn conversations, where growing input lengths negatively impact throughput.

A common approach to reducing the size of the KV cache is to remove or evict unimportant key and value vectors from the cache.
A line of research explores various importance scores to evict tokens**Dong et al., "Attention-Aware Importance Scoring for Efficient Inference"**. 
% Leveraging the inherent sparsity of the KV cache, many studies have demonstrated that numerous unimportant tokens can be discarded during inference with minimal impact on performance. For instance, Heavy-Hitter Oracle (H$_2$O) **Wang et al., "Heavy-Hitter Oracle: A Fast and Memory-Efficient Inference Algorithm"** greedily removes tokens with the lowest attention values during decoding, while the sliding window approach**Zhang et al., "Sliding Window Attention for Efficient Inference of MLLMs"** retains only the most recent tokens in cache. PyramidKV**Li et al., "PyramidKV: A Hierarchical Cache Architecture for Efficient Inference"** optimizes memory usage by allocating different cache budgets across layers to balance efficiency and performance. Based on**Zhou et al., "A Survey of Cache-Based Approaches for Efficient Inference of MLLMs"**, **Huang et al. designed KV cache dropping strategy tailored for MLLMs.
Quantization techniques provide another path to reducing KV cache memory overhead by transforming floating-point representations into lower-precision integers, thus significantly reducing memory usage and potentially enhancing inference speed. Methods such as KIVI**Chen et al., "KIVI: A Framework for Efficient Quantization of Neural Networks"** have introduced asymmetric quantization strategies.
KVQuant**Wang et al., "KVQuant: A Cache-Aware Quantization Method for MLLMs"** introduces advanced KV cache quantization techniques, including per-channel, pre-RoPE, non-uniform, and per-vector quantization, significantly improving accuracy at low bitwidths.
QJL**Zhang et al., "QJL: A Johnson-Lindenstrauss Transform-Based Quantization Method for Efficient Inference"** leverages a Johnson-Lindenstrauss (JL) transform combined with sign-bit quantization to eliminate the storage overhead associated with quantization constants in KV cache quantization.
In contrast to these approaches developed primarily for general-purpose LLMs, our method specifically addresses the unique challenges of MLLMs, where visual tokens dominate the KV cache and present distinct statistical patterns. By carefully exploiting the distributional characteristics of visual activations, we introduce a specialized channel-wise quantization combined with attention-aware calibration, significantly reducing memory footprint while preserving multimodal reasoning capabilities.
%Most existing techniques focus on general-purpose LLMs, often overlooking the unique characteristics of MLLMs. Recent research suggests that leveraging post-vision attention values can more effectively identify important tokens compared to conventional attention-based strategies in MLLMs**Wang et al., "Post-Vision Attention for Efficient Inference of MLLMs"**. In this work, we further explore KV-cache compression tailored for MLLMs, aiming to enhance efficiency while preserving model performance.