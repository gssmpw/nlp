\section{Method}

In this section, we present \textbf{S}tep-level \textbf{T}raj\textbf{e}ctory \textbf{Ca}libration (\textbf{\model}), a novel learning framework for LLM agents.
First, we warm up agent training with supervised fine-tuning (\S\ref{sec3.1}), equipping LLM agents with necessary task planning capabilities. 
Then, we focus on calibration trajectory construction (\S\ref{sec3.2}), which detects deviated actions for an explored trajectory through step-level reward comparison and calibrates them by reflection. 
Finally, we utilize these calibrated trajectories as a crucial part of data for reinforced training (\S\ref{sec3.3}).
Figure~\ref{fig:overview} illustrates the overview of \model.




\subsection{Warm-up via Supervised Fine-tuning}
\label{sec3.1}

Supervised fine-tuning (SFT) on the expert trajectory data has demonstrated promising results, serving as an effective initial step for developing strong agents. We employ ReAct-style~\cite{yao2023react} trajectory to conduct SFT, which additionally generates a Chain-of-Thought (CoT)~\cite{wei2022chain} rationale before each action. Considering that the CoT and the corresponding action are generated together, we represent both as a single unit, denoted as $a_t$, for simplicity.
Given an expert trajectory dataset $\mathcal D = \Big\{(u, e)^{(i)}\Big\}_{i=1}^{|\mathcal D|}$, where each trajectory $e = (u, a_1, o_1, ..., a_m, o_m)$, $u$ represents the initial task instruction, $a_t$ denotes the action (including its rationale) at step $t$, $o_t$ is the corresponding observation, and $|\mathcal D|$ is the number of trajectories, the SFT loss function is formulated as:
\begin{equation}
    \mathcal L_{\text{SFT}}(\theta) = -\mathbb E_{e \sim \mathcal D}\bigg[\sum_{t=1}^n \log \pi_\theta(a_t|e_{t-1})\bigg].
\end{equation}
This warm-up process equips the LLM agent with the necessary task-planning capabilities, enabling it to generate both rationales and actions, resulting in a base agent $\pi_{\text{base}}$.



\subsection{Calibration Trajectory Construction}
\label{sec3.2}

To construct the calibration trajectories, we utilize the base agent $\pi_{\text{base}}$ to explore the environment through interaction. During this exploration, suboptimal actions often lead to a cascade of further suboptimal decisions, causing the trajectory to deviate from successful task completion. 
We define these actions, which are likely to cause deviations from the optimal trajectory and increase the risk of task failure, as \textit{deviated actions}. 
Below, we introduce the details of detecting deviated actions and constructing calibrated trajectories accordingly.


\paragraph{Deviated Action Detection via Step-level Reward Comparison.}

Since long-horizon tasks can be modeled as a partially observable Markov decision process (POMDP), where the future action in a task execution process depends on the current action, we must consider this Markov property when detecting deviated actions. 
To illustrate this, we define the probability of an agent successfully completing a task based on a ``good'' historical trajectory (e.g., an expert trajectory) at time step $t$ as $p(a_{>t}|a_{\leq t})$, where we omit the environmental states for simplicity.
After executing a subsequent action $a_{t+1}$, the probability of task completion becomes $p(a_{>t+1}|a_{\leq t+1})$. If $a_{t+1}$ is a ``good'' action (e.g., an expert action), $p(a_{>t+1}|a_{\leq t+1})$ will generally be greater than $p(a_{>t}|a_{\leq t})$. 
This is because agentic tasks typically consist of sequential actions, where each action contributes to task completion as the sequence progresses. 
Thus, by comparing the task completion probabilities before and after executing an action, we can determine whether the action is deviated.


Specifically, we employ step-level rewards, calculated via Monte Carlo (MC) sampling introduced in \S\ref{sec:task_form}, as an approximate estimation of the task completion probabilities.
An explored action $\hat{a}_{t+1}$ is classified as a deviated action if its step reward is significantly lower than that of the previous expert action $a_t$ by a predefined threshold $\delta$; otherwise, it is considered a non-deviated action.
The formal detection criterion is defined as follows:
\begin{equation}
    \begin{cases}
    \text{Deviated Action:} & \\
    \quad r_s(s_{t}, \hat{a}_{t+1}) - r_s(s_{t-1}, a_{t}) < \delta, \\
    \text{Non-deviated Action:} & \\
    \quad r_s(s_{t}, \hat{a}_{t+1}) - r_s(s_{t-1}, a_{t})\geq \delta,
    \end{cases}
\end{equation}
where $r_s(s_{t-1}, a_{t})$ represents the step reward for the expert action $a_{t}$ at the $t$-th step, $r_s(s_{t},\hat{a}_{t+1})$ denotes the step reward for the explored action $\hat{a}_{t+1}$, and $\delta \geq 0$ is a threshold parameter.


\paragraph{Calibrated Trajectory Collection with Reflective Thoughts.}
As shown in Figure~\ref{fig:overview}, after identifying a deviated action in an explored trajectory, our goal is to enable the LLM agent to ``know'' that the action is deviated and learn how to realign with the task objective. Achieving this goal requires calibrated trajectories for training the agent. Inspired by many previous studies on LLM reflections~\citep{shinn2023reflexion}, we employ off-the-shelf  LLMs to generate reflective thoughts for calibration. Formally, we concatenate the previously explored trajectory $e_{1:t-1}$, the deviated action $\hat{a}_t$, and the corresponding ground-truth action $a_t$ in the expert trajectory, and prompt a state-of-the-art LLM (e.g., GPT-4o~\citep{openai2024gpt4o}) for reflection, transforming the deviated action $\hat{a}_t$ into the ground-truth action along with its reflective thought, which is denoted as $a_t^{'}$. This formulates the subsequent \textit{calibrated trajectory} $e_{c(t:m)} = (a_t^{'}, e_{t+1:m})$, where $e_{t+1:m}$ represents the expert sub-trajectory from the step $t+1$ to the end step $m$. The detailed prompt for this reflection is provided in Appendix~\ref{ref:reflection prompt}. 

Our calibration dataset $\mathcal{D}_c$ is constructed as:
\begin{equation}
    \mathcal{D}_c = \{e_{c(t:m)}^{(i)}\} \cup \{e_{d(1:m)}^{(j)}\},
\end{equation}
where $e_{d(1:m)}=(e_{1:t-1}, \hat{a}_t, \hat{e}_{t+1:m})$ denotes a deviated trajectory, which will be used in subsequent reinforced training.
Note that we perform trajectory calibration immediately when detecting the first deviated action, rather than waiting until the trajectory concludes. This approach ensures timely calibration and reduces unnecessary exploration.


\subsection{Reinforced Training}
\label{sec3.3}

While training on calibration trajectories enhances an agent's calibration capability, relying exclusively on these trajectories may initially hinder their ability to recognize correctness. To mitigate this, we introduce two types of successful data during exploration.
First, we construct the \textit{explored successful trajectory} dataset, $\mathcal{D}_e$, by collecting successful trajectories $\tilde{e}_{1:m}$ that the base agent independently explores from the beginning, along with their corresponding expert trajectories $e_{1:m}$.
Second, we build the \textit{expert sub-trajectory} dataset, $\mathcal{D}_s$. Specifically, for a failed trajectory $\hat{e}_{t:m}$, where the first erroneous action occurs at step $t$, we extract the corresponding expert action and the subsequent trajectory as $\hat{e}_{t:m}$, following \citet{xiong2024watch}. These sub-trajectories guide the agent in learning from challenging cases more effectively.

Using the collected data, we perform reinforced training to enhance LLM agents. 
Our goal is to guide the agent toward generating optimal trajectories that maximize task performance while minimizing suboptimal outcomes. We introduce \textit{trajectory deviation distance} (TDD), a measure that quantifies how much a suboptimal trajectory deviates from an optimal one at the trajectory level. 
Drawing inspiration from ~\citet{xu2024flame}, we utilize the nDTW distance $d_{\text{nDTW}}$ (defined in \S\ref{sec:distance_measure}), to quantify the deviation distance between a suboptimal trajectory $e_s$ and its corresponding optimal trajectory $e_o$.
A smaller $d_{\text{nDTW}}(e_s,e_o)$ indicates a lower deviation. This deviation distance is incorporated as a reward signal during reinforced training.

To ensure balanced training across the datasets, we refine the reward mechanism by incorporating the trajectory deviation distance.
The reward functions for each type of data are defined as follows:
\begin{align}
 r_c &= 1+\eta \cdot d_{\text{nDTW}}(e_{c(t:m)}, {e}_{d(t:m)}), \\
 r_{s} &= 1+\eta \cdot d_{\text{nDTW}}(e_{t:m}, \hat{e}_{t:m}),  \\
  r_{e} &= 1-\eta \cdot d_{\text{nDTW}}(\tilde{e}_{1:m}, e_{1:m}), 
\end{align}
where for the calibration trajectory $e_{c(t:m)}$ and the expert sub-trajectory $e_{t:m}$, we increase the reward as the deviation distance grows, encouraging the agent to calibrate larger deviations.
For the explored successful trajectory $\tilde{e}_{1:m}$, we reduce the reward for unnecessary explorations when the deviation distance increases, discouraging deviations from optimal behavior.
$\eta$ is a temperature coefficient that controls the impact of deviation distance on the reward.
Finally, we integrate these rewards into reinforcement training using the policy gradient~\citep{peters2007reinforcement} algorithm. The overall training objective is given by:
\begin{align}
& \mathcal{L}(\theta) = \nonumber \\
& \mathbb{E}_{(e_{c(t:m)}, e_{1:t-1}) \sim \mathcal{D}_c}
\Big[
    r_c \cdot \log \pi_\theta(e_{c(t:m)} \mid e_{1:t-1})
\Big] \nonumber \\
& + \mathbb{E}_{(e_{t:m}, e_{1:t-1}) \sim \mathcal{D}_{s}}
\Big[
    r_{s} \cdot \log \pi_\theta(e_{t:m} \mid e_{1:t-1})
\Big] \\
& + \mathbb{E}_{(\tilde{e}_{1:m}, u) \sim \mathcal{D}_{e}}
\Big[
    r_{e} \cdot \log \pi_\theta\big(\tilde{e}_{1:m} \big| u\big)
\Big] \nonumber.
\end{align}
