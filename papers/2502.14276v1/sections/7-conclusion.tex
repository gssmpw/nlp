\section{Conclusion}

In this paper, we introduce STeCa, a novel agent learning framework designed to enhance the performance of LLM agents in long-horizon tasks. 
STeCa identifies deviated actions through step-level reward comparisons and constructs calibration trajectories via reflection. 
These trajectories serve as critical data for reinforced training. Extensive experiments demonstrate that STeCa significantly outperforms baseline methods, with additional analyses underscoring its robust calibration capabilities.