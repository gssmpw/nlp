\section{Introduction}

Large Language Models (LLMs) have shown remarkable reasoning and planning abilities in various real-world applications, including household assistance~\citep{puig2018virtualhome,shridhar2020alfworld}, web browsing~\citep{yao2022webshop,deng2023mind2web}, and complex scientific reasoning~\citep{wang-etal-2022-scienceworld}. These tasks require LLM-based agents to engage in long-horizon interactions with the environment, making sequential decisions to achieve a given goal. 
Recent research has revealed that agents still make mistakes, yet they struggle to dynamically adjust their subsequent task planning~\citep{xie2024revealing,wang-etal-2024-e2cl}. This underscores the need for effective methods that enhance an agent's ability to improve its decision-making process over time.


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.98\linewidth]{figure/Fig_intro.pdf}
    \caption{Step-level calibration enables LLM agents to construct calibrated trajectories and learn to mitigate the accumulation of suboptimal actions.}
    \label{fig:intro}
    \vspace{-9pt}
\end{figure} 


Previous work has improved agent learning by leveraging enhanced exploratory data ~\citep{chen2023fireact,yin2023lumos,zeng2023agenttuning,xiang2024language}. These methods primarily rely on behavior cloning from expert demonstrations, training agents exclusively on successful trajectories. However, this approach prevents agents from proactively self-correcting mistakes, leading to the accumulation of errors and ultimately suboptimal task performance~\citep{xie2024revealing}.
To address this limitation, another line of work focuses on preference learning~\citep{song2024trial, xiong2024watch} and reinforcement learning~\citep{carta2023grounding,tan2024true}, integrating failure trajectories additionally to refine decision-making. These approaches train LLM-based agents using explicit error signals or reward functions. 
However, many long-horizon agentic tasks involve multi-turn interactions, where errors often only become evident at the terminal state~\citep{yuan2025agent}. As a result, these methods fail to address early-stage deviations, which may not be immediately apparent but accumulate incrementally over time, ultimately leading to significant errors.



To address these limitations, we highlight the importance of \textbf{timely calibration}, which allows agents to adjust suboptimal actions as they arise rather than deferring corrections until the end of an exploration. 
As illustrated in Figure~\ref{fig:intro}, when an early suboptimal action occurs, the subsequent actions are prone to deviate from the optimal trajectory, significantly increasing the risk of task failure. If an agent can engage in self-reflection and calibrate its behavior in real time, it stands a much better chance of successfully completing the task. 
However, implementing step-level calibrations presents significant challenges. 
1) Unlike in mathematical reasoning tasks~\citep{kumar2024training,xi2025rise}, where well-defined rules simplify error detection, identifying deviations at each step in long-horizon agentic tasks is considerably more complex. This complexity stems from the dynamic and diverse nature of task execution in interactive environments. 
2) As far as we know, the lack of step-level calibration trajectory data poses a major obstacle to training agents to effectively recognize and mitigate deviations.

In this work, we propose \textbf{S}tep-level \textbf{T}raj\textbf{e}ctory \textbf{Ca}libration (\textbf{\model}), a novel agent learning framework that enables LLM agents to perform real-time calibration. 
\model operates by interacting with the environment to perform explorations and utilizes Monte Carlo (MC) sampling~\citep{kakade2002approximately} to estimate step reward for each action. 
By comparing the rewards of adjacent actions, \model effectively identifies deviated actions that lead to suboptimal performance.
Then, we utilize off-the-shelf LLMs for reflection, which revises a deviated action into its ground-truth counterpart while generating a reflective thought. The resulting action and its thought, along with subsequent expert trajectory, form a \textit{calibrated trajectory}.
These calibrated trajectories, combined with successful trajectories during exploration, are then used to reinforce the agent's training, optimizing its learning process.
We evaluate \model on two widely-used agent benchmarks~\citep{puig2018virtualhome,shridhar2020alfworld}. Extensive experimental results demonstrate that \model significantly outperforms existing methods, achieving higher success rates across a variety of tasks. 



In summary, our contributions are as follows:
\begin{itemize}[leftmargin=*, nolistsep]
\setlength{\itemsep}{1mm}
\item We highlight the importance of timely calibration in interactive agentic tasks, a crucial aspect largely overlooked by previous methods.
Unlike existing approaches that rely on terminal-state error signals or reward functions, we emphasize the need for real-time adjustments to prevent the accumulation of deviations, which can lead to significant errors in long-horizon tasks.
\item We introduce \model, a novel learning framework that enhances LLM agents by integrating an automated deviation detection mechanism and calibrated trajectory construction. It equips agents with essential calibration capabilities for improvement during task execution.
\item Extensive experiments demonstrate that \model significantly outperforms existing methods. By detecting deviations in real-time, \model enables agents to effectively mitigate the accumulation of suboptimal actions and handle long-horizon tasks more robustly.
\end{itemize}
