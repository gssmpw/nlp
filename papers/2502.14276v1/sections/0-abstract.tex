Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. 
Existing work primarily focuses on behavior cloning from expert demonstrations and preference learning through exploratory trajectory sampling. However, these methods often struggle in long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories.
To address this, we highlight the importance of \textit{timely calibration} and the need to automatically construct calibration trajectories for training agents. We propose \textbf{S}tep-Level \textbf{T}raj\textbf{e}ctory \textbf{Ca}libration (\textbf{\model}), a novel framework for LLM agent learning. 
Specifically, \model identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. These calibrated trajectories, together with successful trajectory data, are utilized for reinforced training.
Extensive experiments demonstrate that \model significantly outperforms existing methods. Further analysis highlights that step-level calibration enables agents to complete tasks with greater robustness. 
Our code and data are available at \url{https://github.com/WangHanLinHenry/STeCa}.