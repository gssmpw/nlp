\section{Preliminaries}

\paragraph{Task Formulation.}
\label{sec:task_form}
This work investigates how LLM-based agents tackle long-horizon tasks within specific environments through interactions.
Following previous studies~\citep{song2024trial, xiong2024watch}, we formalize these agentic tasks as a partially observable Markov decision process (POMDP), which contains the key elements ($\mathcal U, \mathcal S, \mathcal A, \mathcal O, \mathcal T, \mathcal R$). Here, $\mathcal U$ denotes the instruction space, $\mathcal S$ the state space, $\mathcal A$ the action space, $\mathcal O$ the observation space, $\mathcal T$ the transition function ($\mathcal T: \mathcal S \times \mathcal A \rightarrow \mathcal S$), and $\mathcal R$ the reward function ($\mathcal R: \mathcal S \times \mathcal A \rightarrow [0, 1]$). Since the task planning capability of LLM agents is our main focus, $\mathcal U, \mathcal A, \mathcal O$ are subsets of natural language space.

Given a task instruction $u\in\mathcal{U}$, the LLM agent $\pi_{\theta}$ at time step $t$ takes an action $a_t\sim \pi_\theta(\cdot|u, e_{t-1})$ and receives the environmental feedback as the observation $o_t\in\mathcal{O}$. $e_{t-1}$ denotes the historical interaction trajectory $(a_1, o_1, ... , a_{t-1}, o_{t-1})$. Each action $a_t$ incurs the environment state to $s_t\in\mathcal{S}$. The interaction loop terminates when either the agent completes the task or the maximum step is reached.
The final trajectory is $e_m = (u, a_1, o_1, ..., a_m, o_m)$, where $m$ denotes the trajectory length. The outcome reward $r_o(u, e_m) \in [0, 1]$ indicates the success or failure of the task.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\textwidth]{figure/Fig_overview.pdf}
    \caption{Overview of the \textbf{S}tep-level \textbf{T}raj\textbf{e}ctory \textbf{Ca}libration (\textbf{STeCa}) framework for LLM agent learning.}
    \label{fig:overview}
\end{figure*}


\paragraph{Step-level Reward Acquisition.}
\label{sec:step_reward_compute}

It is crucial to acquire step-level rewards as feedback to improve decision-making for LLM agents.
Following prior work~\citep{kakade2002approximately,salimans2018learning,xiong2024watch}, we leverage expert trajectories as demonstrations and ask an LLM agent to begin exploration from the initial state $s_0\in\mathcal{S}$ toward the target state for a given demonstration. At each $t$-step, the agent's policy $\pi_\theta$ generates an action $a_t$, and we define a step-level reward $r_s(s_t, a_t)$ to quantify the contribution of $a_t$ to future success. 
Specifically, at $t$-th step, the agent generates $N$ new subsequent trajectories $\{e_{t+1:m}^{(i)}\}_{i=1}^{N}$ using the widely-used Monte Carlo sampling, conditioned on the historical trajectory $e_t$. Each trajectory receives an outcome reward $r_o(u,e_m)$ from the environment. The step-level reward $r_s(s_t, a_t)$ is computed as the expected value of these outcome rewards:
\begin{equation}
    r_s(s_t, a_t) = \mathbb E_{e_m \sim \pi_{\theta}(e_{t+1:m}|e_{t})} [r_o(u, e_m)].
\end{equation}


\paragraph{Normalized Dynamic Time Warping.}
\label{sec:distance_measure}
The normalized Dynamic Time Warping (nDTW) algorithm~\cite{muller2007dynamic}, implemented via dynamic programming (DP), effectively measures the distance between two trajectories containing multiple time steps. Formally, given a pair of trajectories $(x, y)$, this computation process is computed as:
\begin{align}
    D(i,j) = d(x_i, y_j) + 
        \min \begin{cases} 
            D(i-1, j) \\ 
            D(i, j-1),\\ 
            D(i-1, j-1)
        \end{cases}
\label{eq:nDTW}
\end{align}
where $d(x_i,y_i)$ denotes a distance function such as $L_2$ or cosine distance, $D(0,0)=d(x_0,y_0)$. $x_i$ denotes the action at the $i$-th step in the trajectory $x$, while $y_j$ denotes the action at the $j$-th step in the trajectory $y$. With a normalization operation, the nDTW distance $d_{\text{nDTW}}$ is given by:
\begin{equation}
    d_{\text{nDTW}}(x,y) = \frac{D(x-1, y-1)}{\sqrt{n_{x}^2 + n_{y}^2}},
\end{equation}
where $d_{\text{nDTW}}\in [0,1]$, $n_{x}$ and $n_{y}$ denote the number of steps in the trajectory $x$ and $y$, respectively. 
