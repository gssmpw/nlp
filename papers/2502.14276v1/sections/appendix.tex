\section{Datasets and Preprocessing}
\label{ref:ds}

\paragraph{ALFWorld}
ALFWorld~\citep{shridhar2020alfworld} offers interactive TextWorld environments that are meticulously aligned with the embodied environments introduced in ALFRED~\citep{shridhar2020alfred}. This framework challenges agents to navigate complex household settings and execute high-level instructions, thereby testing their ability to perform practical tasks. The dataset is structured into two distinct evaluation sets: a seen set, designed to assess in-distribution generalization, and an unseen set, which comprises novel task instances to evaluate out-of-distribution generalization capabilities. At the conclusion of each trajectory, the environment provides a binary reward, indicating whether the agent has successfully completed the assigned task. This setup facilitates a clear and measurable assessment of agent performance in both familiar and novel scenarios.

\paragraph{VirtualHome} 

VirtualHome~\citep{puig2018virtualhome} is a comprehensive dataset comprising 292 high-level household tasks and 1,374 unique action plans, distributed across 6,201 diverse environments. The dataset was meticulously curated through manual annotations provided by Amazon Mechanical Turk workers, who labeled tasks and their corresponding action plans in detail. Each entry in the dataset is structured into three components: a high-level task, a descriptive explanation, and executable action programs compatible with the VirtualHome environment. To evaluate task completion, we executed all tasks and recorded the final state of the environment upon completion. A task is considered successfully completed if the state of the environment after exploration by the LLM agent matches the predefined target state. To ensure data quality, the dataset was filtered by retaining only trajectories with successful final outcome rewards and verifying that every action in the planning sequence is executable within the environment. Furthermore, to maintain an appropriate level of task complexity, the dataset was restricted to trajectories with planning lengths ranging from 3 to 10 steps. This rigorous filtering process ensures a robust and reliable subset of data, suitable for in-depth analysis and model training.

\paragraph{Dataset Construction}
Since the original trajectories do not include reasoning processes preceding each action, we adopt established methodologies from prior work~\citep{song2024trial,xiong2024watch} to enrich the data. Specifically, we incorporate relevant task information and expert action trajectories to prompt GPT-4o to generate plausible reasoning steps (thoughts) before each action. This approach ensures that the dataset captures the cognitive processes underlying decision-making. Ultimately, the datasets are structured in a thought-action format, following the ReAct framework~\citep{yao2023react}. Detailed statistics for the two datasets are provided in Table~\ref{tab:dataset}, highlighting their key characteristics and composition.



\section{Baseline Methods}
\label{appendix:baselines}

Our baseline methods are as follows: 
1) SFT~\citep{chen2023fireact}, which employs behavior cloning on expert trajectories alone, serving as the base agent for STeCa and other baseline methods.
2) PPO~\citep{schulman2017proximal}, a widely-used reinforcement learning algorithm, optimizes final trajectory rewards. Additionally, we apply PPO for stepwise action optimization.
3) RFT~\citep{yuan2023scaling}, which extends expert trajectories by incorporating successful trajectories discovered by the base agent, followed by fine-tuning on the expanded dataset.
4) ETO~\citep{song2024trial}, which constructs positive and negative trajectory pairs and optimizes them using Direct Preference Optimization (DPO)~\citep{rafailov2024direct}.
5) E$^2$CL~\citep{wang-etal-2024-e2cl}, which leverages planning data, feedback data, and correction data to supervise the fine-tuning of LLM agents.
6) IPR~\citep{xiong2024watch}, which enhances trajectory pairs by augmenting sub-trajectory pairs based on step rewards, building upon ETO's framework, and trains LLM agents using preference learning methods.

\begin{table}[t!]
    \centering
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l c c c c c}
    \toprule
    \textbf{Dataset}   & \textbf{Train} & \textbf{Test} & \textbf{\#Actions}  & \textbf{\#Avg./Max. Turns}\\
    \midrule
    ALFWorld & 2,851 & 274 & 13 & 8.0 / 20 \\
    VirtualHome & 4,920 & 494 & 40 & 11.5 / 20\\
    \bottomrule
    \end{tabular}
    }
    \caption{Statistics of the datasets for experiments.}
    \label{tab:dataset}
\end{table}


\section{Additional Implementation Details}
\label{appendix:implement}

During the construction of the base agent, we train the model for 3 epochs with a batch size of 16 and a learning rate of 3e-6, employing the AdamW optimizer and a cosine learning rate scheduler. For reinforced training, the model is fine-tuned for only 1 epoch.

During the inference phase, all methods are evaluated using the ReAct-style interaction format, where the agent generates a rationale before executing each action. Specifically, we include a one-shot example in the instruction prompt for each task. Detailed prompts are provided in Appendix~\ref{ref:prompt}. For text generation, we apply greedy decoding with the temperature set to 0. To accelerate inference, we utilize vLLM~\citep{kwon2023efficient} libribray to optimize the generation process of LLMs. 

All experiments were conducted on a computational cluster equipped with 8 NVIDIA A6000 48GB GPUs. For fine-tuning, we employed several open-source models, including Llama-2-7B-Chat~\citep{touvron2023llama}, Mistral-7B~\citep{jiang2023mistral}, and Llama-3-8B-Instruct. We strictly complied with the licensing terms for academic use associated with these models: Llama-2-7B-Chat is governed by the Llama 2 Community License\footnote{\url{https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/blob/main/LICENSE.txt}}, Mistral-7B is licensed under the Apache-2.0 License\footnote{\url{https://huggingface.co/mistralai/Mistral-7B-v0.1}}, and Llama-3-8B-Instruct adheres to the Llama 3 License\footnote{\url{https://huggingface.co/meta-llama/Meta-Llama-3-8B/blob/main/LICENSE}}. This adherence ensures that our use of these models aligns with their respective legal and ethical guidelines.

\section{Experimental Settings about Analyses}
\label{appendix:analysis}

\subsection{Variants of Step-level Reward Acquisition}
\label{appendix:reward_analysis}

In addition to the Monte Carlo (MC) sampling for step-level reward acquisition, we further employ the following two variants: 
(1) \textbf{GPT-4o Annotation:} In Section~\ref{sec3.2}, we collect Monte Carlo (MC) step rewards corresponding to various step actions. To annotate all explored step actions, we randomly select several samples as in-context examples and utilize GPT-4 for annotation. The detailed prompt used for this process is provided in Appendix~\ref{appdendix: reflection prompt for step rewards}.
(2) \textbf{Reward Model Prediction:} We also leverage the data collected in Section~\ref{sec3.2}, where each step action is associated with an MC step reward, to train a reward model capable of predicting scores for step actions. Specifically, we use the Llama-2-7B-Chat~\citep{touvron2023llama} model as the base architecture. To mitigate overfitting, we add a dropout layer to the output layer, followed by a linear layer to map the output to a scalar score. Additionally, we employ Low-Rank Adaptation (LoRA)~\citep{hu2021lora} for efficient fine-tuning. The model is trained for 3 epochs, and during testing, we set the random seed to 42 to ensure reproducibility and score all step actions.


\subsection{Detailed Settings for Calibration Analysis}
\label{appendix:calibration_analysis}

We randomly select 100 pieces of data from $D_c(e_{1:t-1}, \hat{a}_t, e_{c(t:m)}, \hat{e}_{t+1:m})$ for both ALFWorld and VirtualHome to serve as the seen test set. Additionally, we randomly select 100 pieces of data from the unseen test set. Following the procedure outlined in Section~\ref{sec3.2}, we construct the calibration dataset $(e_{1:t-1}, \hat{a}_t, e_{c(t:m)}, \hat{e}_{t+1:m})$ derived from unseen scenarios.
After assembling the calibration datasets for both seen and unseen scenarios in VirtualHome and ALFWorld, we use these datasets to evaluate the calibration performance of the LLM agent. Specifically, we traverse the step actions from $(e_{1:t-1}, \hat{a}_t)$ to obtain the initial environment state. We then deploy the LLM agent to explore the environment starting from this state and assess whether it can successfully complete the task.

For the second experiment, we reuse the previously collected calibration dataset. However, in this case, we traverse the step actions only from $e_{1:t-1}$, excluding the deviated action $\hat{a}_t$. We refer to this configuration as the ``w/o deviated action'' setting.




\section{Prompt Templates}
\label{ref:prompt}

\subsection{Inference Prompt}
\label{ref:inference prompt}

As shown in Figure~\ref{fig:Inference prompt}, we provide the inference prompt for each task, which include a general instruction, a one-shot example, the specific task instruction and history trajectory.

\begin{tcolorbox}[breakable,title=Inference Prompt]
\textbf{\textit{\# General Instruction:}} \\
\textbf{Human}: Interact with a household to solve a task. Imagine you are an intelligent agent in a household environment and your target is to perform actions to complete the task goal...\\
Your response should use the following format: \\
Thought: <your thoughts> \\
Action: <your next action> \\
\textbf{Agent}: OK\\

\textbf{\textit{\# In-Context Example:}} \\
\textbf{Human}: The task is Drink (Drink water). \\
... \\

\textbf{\textit{\# Task Instruction:}} \\
\textbf{Human}: The task is xxx. \\
\textit{\textbf{(History trajectory)}} \\
... \\

\end{tcolorbox}
\begin{figure}[ht]
    \centering
    \vspace{-8pt}
    \caption{Inference prompt template.}
    \label{fig:Inference prompt}
\end{figure}

\subsection{Reflection Prompt}
\label{ref:reflection prompt}

As shown in Figure~\ref{fig:reflection prompt for virtualhome}, the reflection prompt includes history trajectory (containing deviated action) and reflection instruction. This prompt is then used to request GPT-4o to generate reflective thoughts.


\begin{tcolorbox}[breakable,title=Reflection Prompt]
\textbf{\textit{\# Historical Trajectory:}} \\
\textbf{Human}: Interact with a household to solve a task. Imagine you are an intelligent agent in a household environment and your target is to perform actions to complete the task goal...\\
\textbf{Agent}: OK\\
\textbf{Human}: Your task is write an email... \\
\textbf{Agent}: Thought: ... Action: ... \\
... (Interaction with multi-turns)\\
\textbf{Agent}: Thought: ... Action: ... \textcolor{red}{(\# error action at this step)} \\

  
\textbf{\textit{\# Reflection Instruction:}} \\
Above is the interaction history. However, the last step is not optimal and may lead to a wrong direction. The next step ground-truth action is [\textcolor{blue}{\text{ground truth action at this step}}]. Please provide the thought which would lead the agent to generate the ground truth action and be aware of the last non-optimal action. The thought should follow the format of the interaction history.

\end{tcolorbox}
\begin{figure}[ht]
    \centering
    \vspace{-8pt}
    \caption{
    Prompt template for reflection.
    }
    \label{fig:reflection prompt for virtualhome}
\end{figure}


\subsection{Prompt for Step Reward Prediction}
\label{appdendix: reflection prompt for step rewards}
Figure~\ref{fig:reflection prompt for step rewards in vh} presents the prompt template designed for predicting step rewards, which consists of an instruction and several in-context examples.

\begin{tcolorbox}[breakable,title=Prompt for Step Reward Prediction]
\textbf{\textit{\# Instruction:}} \\
Based on the historical interaction trajectory, the agent has generated its final action. Evaluate whether this action is conducive to the completion of the ultimate task and provide a score. The score should range from 0 to 1, with scores closer to 1 indicating a higher likelihood of task completion. Generate only the score, without any additional commentary or explanation. \\

\textbf{\textit{\# History trajectory1:}} \\
xxx \\
\textbf{\textit{\# Score1:}} \\
xx \\
\textbf{\textit{\# History trajectory2:}} \\
xxx \\
\textbf{\textit{\# Score2:}} \\
xx \\
\textbf{\textit{\# History trajectory3:}} \\
xx \\
\textbf{\textit{\# Score3:}} \\
...
\end{tcolorbox}

\begin{figure}[ht]
    \centering
    \vspace{-8pt}
    \caption{
    Prompt template for step reward prediction.
    }
    \label{fig:reflection prompt for step rewards in vh}
\end{figure}


\section{Case study}
\label{appendix:case_study}

Figure~\ref{fig:alfworld_case} provides an example demonstrating STeCa's calibration capability during the planning process. As illustrated, STeCa autonomously identifies deviated actions, reflects on them, and successfully completes the task. In contrast, the other two agents fail to achieve the same level of performance, further highlighting the effectiveness of STeCa in addressing such challenges.

\onecolumn

\begin{tcolorbox}[breakable,title=Case Study on the ALFWorld Benchmark]
\columnseprule=0.5pt
\begin{multicols}{2}

\textcolor{case_purple}{\textbf{Task Instruction:}} Your task is to find two pans and put them on the countertop.\\
\textcolor{case_blue}{\textbf{Thought 1:}} I need to find two pans for the task. The first step is to search the cabinets as they are likely places where pans are stored.\\
\textcolor{case_blue}{\textbf{Action 1:}} go to cabinet 1\\
\textcolor{case_purple}{\textbf{Observation 1:}} On cabinet 1, you see: a plate 1, a bowl 1, and a glass 1.\\
\textcolor{case_blue}{\textbf{Thought 2:}} Cabinet 1 contains a plate, a bowl, and a glass, but no pan. I should check the next cabinet for a pan.\\
\textcolor{case_blue}{\textbf{Action 2:}} go to cabinet 2\\
\textcolor{case_purple}{\textbf{Observation 2:}} On cabinet 2, you see: a cuttingboard 1, a knife 1, and a pan 1.\\
\textcolor{case_blue}{\textbf{Thought 3:}} Cabinet 2 also doesn't have a pan, just a saltshaker. I'll continue searching the cabinets.\\
\textcolor{case_blue}{\textbf{Action 3:}} go to cabinet 3\quad \textcolor{red}{\textit{\textbf{(\# Deviated Action)}}}\\
\textcolor{case_purple}{\textbf{Observation 3:}} The cabinet 3 is closed.\\

\begin{center}\textbf{Agent w/ SFT}\end{center}
\textcolor{case_blue}{\textbf{Thought 4:}} Cabinet 3 is closed. I need to open it to see if there's a pan inside.\\
\textcolor{case_blue}{\textbf{Action 4:}} open cabinet 3\\
\textcolor{case_purple}{\textbf{Observation 4:}} You open the cabinet 3. The cabinet 3 is open. In it, you see a plate 2.\\
\textcolor{case_blue}{\textbf{Thought 5:}} Inside cabinet 3, there's only a plate. I should close this cabinet and keep looking for a pan.\\
\textcolor{case_blue}{\textbf{Action 5:}} close cabinet 3\\
\textcolor{case_purple}{\textbf{Observation 5:}} You close the cabinet 3.\\
. . .\\
\textcolor{red}{\textbf{Failure!}}

\begin{center}\textbf{Agent w/ IPR}\end{center}
\textcolor{case_blue}{\textbf{Thought 4:}} Cabinet 3 is closed. I'll open it to see if there's a pan inside.\\
\textcolor{case_blue}{\textbf{Action 4:}} open cabinet 3\\
\textcolor{case_purple}{\textbf{Observation 4:}} You open the cabinet 3. The cabinet 3 is open. In it, you see a plate 2.\\
\textcolor{case_blue}{\textbf{Thought 5:}} There's a plate 2 in cabinet 3, but no pan. I'll close it and check the next cabinet.\\
\textcolor{case_blue}{\textbf{Action 5:}} close cabinet 3\\
\textcolor{case_purple}{\textbf{Observation 5:}} You close the cabinet 3.\\
. . .\\
\textcolor{red}{\textbf{Failure!}}
\columnbreak

\begin{center}{\textbf{Agent w/ STeCa (Ours)}}\end{center}

\sethlcolor{green!50}{\textbf{\hl{Thought 4:}}} Cabinet 2 has a pan. I should take pan 1 from cabinet 2 instead of moving to cabinet 3. This will allow me to accomplish part of my task to find two pans. \textcolor{my_green}{\textit{\textbf{(\# Self-reflection)}}}\\
\sethlcolor{green!50}\textbf{\hl{Action 4:}} take pan 1 from cabinet 2\quad \\
\textcolor{case_purple}{\textbf{Observation 4:}} You pick up the pan 1 from cabinet 2.\\
\textcolor{case_blue}{\textbf{Thought 5:}} With pan 1 in hand, I need to place it on a countertop to keep it accessible while I search for the second pan.\\
\textcolor{case_blue}{\textbf{Action 5:}} go to countertop 1\\
\textcolor{case_purple}{\textbf{Observation 5:}} On the countertop 1, you see: a bread 1, a cup 1, a fork 2, a fork 1, a knife 3, . . .\\
\textcolor{case_blue}{\textbf{Thought 6:}} I am at countertop 1. I should place pan 1 here and ensure it is securely positioned so it doesn't fall off.\\
\textcolor{case_blue}{\textbf{Action 6:}} put pan 1 in/on countertop 1\quad \\
\textcolor{case_purple}{\textbf{Observation 6:}} You put the pan 1 in/on the countertop 1.\\
\textcolor{case_blue}{\textbf{Thought 7:}} Pan 1 is now on the countertop. I need to find the second pan. I will continue searching the cabinets.\\
\textcolor{case_blue}{\textbf{Action 7:}} go to cabinet 3\\
\textcolor{case_purple}{\textbf{Observation 7:}} On cabinet 3, you see: a pot 1 and a pan 2.\\
\textcolor{case_blue}{\textbf{Thought 8:}} In cabinet 3, I found another pan. I should take pan 2 to complete the task of finding two pans.\\
\textcolor{case_blue}{\textbf{Action 8:}} take pan 2 from cabinet 3\quad \\
\textcolor{case_purple}{\textbf{Observation 8:}} You pick up the pan 2 from cabinet 3.\\
\textcolor{case_blue}{\textbf{Thought 9:}} I have pan 2 now and need to place it on the same countertop as pan 1 to complete the task.\\
\textcolor{case_blue}{\textbf{Action 9:}} go to countertop 1\\
\textcolor{case_purple}{\textbf{Observation 9:}} On the countertop 1, you see: a bread 1, a cup 1, a fork 2, a fork 1, a knife 3, . . .\\
\textcolor{case_blue}{\textbf{Thought 10:}} I am back at countertop 1 with pan 2. I should place it next to pan 1, ensuring both are ready for use.\\
\textcolor{case_blue}{\textbf{Action 10:}} put pan 2 in/on countertop 1\quad \\
\textcolor{my_green}{\textbf{Success!}}
\end{multicols}
\end{tcolorbox}
\begin{figure}[ht]
    \centering
    \vspace{-8pt}
    \caption{
    Case study on the ALFWorld benchmark.
    }
    \label{fig:alfworld_case}
\end{figure}