\section{Related Work}

\begin{figure}[t!]
  \includegraphics[width=1.0\linewidth]{figure/deviation_success_rate.pdf}
  \caption{Correlation between the deviation distance and success rate (measured by average final reward).
  }
  \label{fig:deviation_success_rate}
\end{figure}

\paragraph{LLM Agent Learning.}
LLM agents are widely used for tackling complex real-world tasks~\citep{wang2023voyager,hu2024dawn,wang-etal-2024-e2cl}, relying on iterative interactions with their environment guided by task objectives and constraints. However, in long-horizon planning, excessive interactions make them prone to suboptimal actions, increasing the risk of failure. While closed-source LLMs demonstrate strong intelligence, open-source counterparts still lag behind~\citep{liu2023agentbench,wang2023mint}.
To address this gap, some studies focus on improving task success rates by increasing the likelihood of generating optimal actions~\citep{chen2023fireact, yuan2023scaling}. Alternatively, another line of research seeks to mitigate suboptimal actions by collecting them and applying preference learning methods to reduce their occurrence~\citep{song2024trial,xiong2024watch}.
Recently, researchers have explored the capacity of LLM agents to self-correct errors, enhancing their ability to ensure successful task completion~\citep{wang-etal-2024-e2cl, qu2024recursive}.
However, these methods primarily focus on self-correction after errors have already occurred, lacking the ability to detect suboptimal actions in advance and calibrate subsequent planning accordingly.

\paragraph{Process Supervision.}
 
Process supervision provides fine-grained guidance, making it a promising approach for addressing long-horizon problems~\citep{uesato2022solving}. Early studies have explored obtaining step-level rewards and using them to optimize intermediate processes through reinforcement learning~\citep{lightman2023let, deng2024novice, wang-etal-2024-math}. Others have focused on constructing step-level positive and negative data pairs and applying preference learning techniques to achieve more precise optimization~\citep{xiong2024watch, jiao2024learning}.
However, existing studies have yet to address the construction of step-level reflection data. Such data could empower LLM agents to detect suboptimal actions, analyze the reasons for their suboptimality, and determine how to calibrate them to ensure successful task completion.
