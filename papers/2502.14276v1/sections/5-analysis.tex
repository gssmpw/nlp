\section{Discussions and Analyses}
In this section, we provide detailed analyses of the \model framework from the following aspects.

\subsection{Effectiveness with Different Base Models}

To validate the broad effectiveness of our method, we evaluate STeCa with different base models, including Mistral-7B and Llama-3-8B-Instruct, on the ALFWorld environment. We compare its performance with SFT and IPR across both seen and unseen tasks. As shown in Table~\ref{tab:base model}, STeCa consistently outperforms both SFT and IPR across multiple base models. 
Notably, in unseen tasks, STeCa achieves a 17.1\% improvement over SFT on Mistral-7B, highlighting its generalization ability for developing LLM-based agents. 
Furthermore, with Llama-3-8B-Instruct, a stronger backbone model, STeCa further achieves better performance, underscoring its potential for building advanced agents in the future.






\subsection{Comparisons between Variants of STeCa}
\label{analyses: ablation study}

\paragraph{Variants of Step-Level Reward Acquisition.}

To investigate the impact of different methods for reward acquisition, we conducted experiments using GPT-4o and a trained reward model to annotate reward for each step action while keeping all other processes unchanged. 
The detailed process of step action reward acquisition is described in Appendix~\ref{appendix:reward_analysis}. 
As shown in Table~\ref{tab:ablation}, our method utilizing MC sampling for step reward acquisition achieves superior performance compared to alternative variants, highlighting the effectiveness of MC sampling for reward acquisition. Notably, employing GPT-4o to directly annotate rewards for step actions demonstrates performance comparable to our method, suggesting that step rewards can be effectively obtained through more computationally efficient approaches. This finding provides a promising direction for future research into optimizing the efficiency of reward acquisition.



\paragraph{Variants of Reflective Thought Generation.}

In STeCa, we employ GPT-4o to generate reflection thoughts for constructing calibration trajectories.
To evaluate the impact of reflection quality on performance, we attempted to prompt the base agent $\pi_{\theta}$ to generate reflections while keeping all other processes unchanged. The results, summarized in Table~\ref{tab:ablation}, reveal a significant performance degradation, underscoring the critical importance of high-quality reflection generation for achieving optimal results. This finding also suggests that the base agent, trained solely on expert trajectories, lacks effective reflection capabilities. 


\begin{table}[t!]
\centering
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\textbf{Variants}}&  \multicolumn{2}{c}{\textbf{VirtualHome}} & \multicolumn{2}{c}{\textbf{ALFWorld}} \\
\cmidrule(l){2-3}
\cmidrule(l){4-5}
      & Seen & Unseen & Seen & Unseen \\
\midrule
\multicolumn{5}{c}{\textbf{Step-level Reward Acquisition}} \\
\midrule
MC Sampling  &  69.6 &  63.6  & 74.3  & 76.1   \\
GPT-4o Annotation  &  69.1  &  62.5 &  74.1 & 74.9  \\
RM Prediction   &  68.2 &  61.8 &  74.0  & 73.3   \\
\midrule
\multicolumn{5}{c}{\textbf{Reflective Thought Generation}} \\
\midrule
GPT-4o Generation      &  69.6  &  63.6 &  74.3   & 76.1   \\
Self-generation      &  66.0  &  61.1 &  71.4  & 73.3   \\
\bottomrule
\end{tabular}
}
\caption{Comparisons between variants of \model.}
\label{tab:ablation}
\end{table}


\begin{figure}[t!]
  \includegraphics[width=1.0\linewidth]{figure/average_rewards_comparison.pdf}
  \caption{Variations in Monte Carlo (MC) step rewards with respect to the number of remaining steps until task completion for expert trajectories.}
  \label{fig:average_rewards_comparison}
\end{figure}


\subsection{Analyses of Deviated Action Detection}
\label{statistic of step rewards}

To validate the empirical Markov property introduced in Section~\ref{sec3.2}, i.e., non-deviated ``good'' actions increase the likelihood of task completion, we conducted a statistical analysis using expert trajectories. Specifically, we compared task completion probabilities at varying distances from task completion, employing MC step rewards as a proxy for these probabilities. 
As illustrated in Figure~\ref{fig:average_rewards_comparison}, MC step rewards monotonically increase as the agent progresses toward task completion in both environments. This trend demonstrates that the accumulation of optimal actions significantly contributes to task completion. 
Conversely, deviated actions consistently reduce the task completion probability, further supporting our approach of using step reward comparisons between adjacent steps as a reliable criterion for detecting deviated actions.

\subsection{Analyses of Calibration}

In this section, we compare the calibration capabilities of STeCa and baseline methods. We evaluate calibration performance using the average final reward achieved upon successful task completion in the presence of deviated actions. To enable this analysis, we constructed datasets containing historical trajectories with deviated actions, categorized into seen and unseen scenarios across both environments. Additional details are provided in Appendix~\ref{appendix:calibration_analysis}.
As shown in Figure~\ref{fig:calibration_performance}, STeCa outperforms baseline methods by a significant margin. For instance, it achieves a 14.8\% relative improvement over IPR on unseen tasks in the VirtualHome environment, highlighting its superior calibration performance. To comprehensively illustrate the performance of different agent learning methods, we provide some concrete examples in Appendix~\ref{appendix:case_study}.


To examine the impact of deviated actions on LLM agent performance, we evaluate these agents trained with different methods under two settings: one with deviated actions included in historical trajectories and another without. The construction process of the testing datasets is detailed in Appendix~\ref{appendix:calibration_analysis}.
As shown in Figure~\ref{fig:deviation_success_rate}, STeCa exhibits minimal performance variation between the two settings, unlike other methods, which show significant differences. 
This suggests that the presence of deviated actions has little effect on STeCa's performance, as it effectively calibrates subsequent trajectories. 
These findings highlight STeCa's robustness and its potential for reliable performance in diverse and challenging environments.


\begin{figure}[t!]
  \includegraphics[width=1.0\linewidth]{figure/calibration_performance2.pdf}
  \caption{Calibration performance of different methods on the VirtualHome and ALFWorld datasets.
  }
  \label{fig:calibration_performance}
\end{figure}

