\section{Experiments}

\subsection{Experimental Settings}

\paragraph{Datasets.}

We conduct experiments on two representative agentic task datasets: \textbf{VirtualHome}~\citep{puig2018virtualhome} and \textbf{ALFWorld}~\citep{shridhar2020alfworld}. 
For ALFWorld, we utilize datasets constructed by \citealp{song2024trial}. For the VirtualHome dataset, we leverage the predefined tasks from the ActivityPrograms knowledge base \citep{puig2018virtualhome} and construct a corresponding dataset in a manner closely aligned with the ALFWorld dataset.
Please refer to Appendix~\ref{ref:ds} for further details regarding the dataset construction process and associated statistical information.

\begin{table*}[t!]
    \centering
    \resizebox{0.98\textwidth}{!}{
    \begin{tabular}{l l c c c c c}
    \toprule
    \multirow{2}{*}{\textbf{Paradigm}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{VirtualHome}} & \multicolumn{2}{c}{\textbf{ALFWorld}} & \multirow{2}{*}{\textbf{Average}}\\
    \cmidrule(l){3-4} \cmidrule(l){5-6}
         & & Seen & Unseen & Seen & Unseen & \\
    \midrule
    \multirow{3}{*}{Prompting-based} 
    %& Llama-2-7B-Chat~\citep{touvron2023llama} & 0.4 & 0.0 & 0.0 & 0.0 & 0.1 \\
    & GPT-3.5-Turbo~\citep{ouyang2022training} & 6.3 & 2.6 & 7.9 & 10.5 & 6.8 \\
    & GPT-4~\citep{achiam2023gpt} & 34.2 & 9.4 & 42.9 & 38.1 & 31.2 \\
    \midrule
    \multirow{10}{*}{Tuning-based} 
    & Llama-2-7B-Chat + PPO~\citep{schulman2017proximal} & 23.9 & 25.0 & 22.1 & 29.1 & 25.0 \\
    & Llama-2-7B-Chat + SFT~\citep{chen2023fireact} & 64.9 & 57.7 & 60.0 & 67.2 & 63.3 \\
    & Llama-2-7B-Chat + RFT~\citep{yuan2023scaling} & 65.1 & 58.3 & 62.9 & 66.4 & 63.2 \\
    & Llama-2-7B-Chat + Step-PPO~\citep{wang-etal-2024-math} & 65.7 & 59.6 & 65.7 & 69.4 & 65.1 \\
    & Llama-2-7B-Chat + ETO~\citep{song2024trial} & 66.6 & 60.1 & 68.6 & 72.4 & 66.9 \\
    & Llama-2-7B-Chat + E$^2$CL~\citep{wang-etal-2024-e2cl} & 67.1 & 61.8 & 70.1 & 73.9 & 68.2 \\
    & Llama-2-7B-Chat + IPR~\citep{xiong2024watch} & 67.6 & 61.9 & 70.3 & 74.7 & 68.6 \\
    \cmidrule{2-7}
    & \cellcolor[gray]{0.9}\textbf{Llama-2-7B-Chat + STeCa (Ours)} & \cellcolor[gray]{0.9}\textbf{69.6} & \cellcolor[gray]{0.9}\textbf{63.6} & \cellcolor[gray]{0.9}\textbf{74.3} & \cellcolor[gray]{0.9}\textbf{76.1} & \cellcolor[gray]{0.9} \textbf{70.9} \\
    & \textbf{Llama-2-7B-Chat + STeCa w/ SFT+DPO} & 66.8 & 63.5 & 74.1 & 75.5 & 70.0 \\
    & \textbf{Llama-2-7B-Chat + STeCa w/o RT} & 68.8 & 62.4 & 72.1 & 74.9 & 69.6 \\
    \bottomrule
    \end{tabular}
    }
    \caption{Performance of different methods on two agent datasets. ``Seen'' refers to the held-out test set containing tasks present during training, while ``Unseen'' refers to the test set with unseen task variations.}
    \label{tab:main_results}
\end{table*}



\paragraph{Baseline Methods.}

We evaluate STeCa against the following two categories of baselines: (1) \textbf{prompting-based} approaches, including GPT-3.5-turbo~\citep{ouyang2022training} and GPT-4~\citep{achiam2023gpt}. 
(2) \textbf{tuning-based} methods, which include supervised fine-tuning (SFT) methods, such as pure SFT~\citep{chen2023fireact}, RFT~\citep{yuan2023scaling}, and E$^2$CL~\citep{wang-etal-2024-e2cl}, reinforcement learning-based methods such as PPO~\citep{schulman2017proximal} and Step-PPO~\citep{wang-etal-2024-math}, as well as preference learning methods like ETO~\citep{song2024trial} and IPR~\citep{xiong2024watch}. Additional details about the baselines are provided in Appendix~\ref{appendix:baselines}.





\paragraph{Implementation Details.}

We utilize Llama-2-7B-Chat~\citep{touvron2023llama} as the base model for training LLM agents. We set $\delta=0$ as the threshold to detect deviated actions in two environments. We use $\eta=1$ for VirtualHome and $\eta=0.01$ for ALFWorld to weight the contribution of trajectory deviation to the reward. To obtain step-level rewards with MC sampling, we set the temperature to 1 and the number of samples $N$ to 5. More details are presented in Appendix~\ref{appendix:implement}.



\paragraph{Evaluation Metrics.}
Following existing studies~\cite{song2024trial,xiong2024watch}, we adopt the \textbf{Average Final Reward} as our evaluation metric. This metric measures the success rate of test tasks. 
In ALFWorld, the environment provides binary final rewards, where a reward of 1 indicates task completion and 0 indicates failure. 
Similarly, in VirtualHome, a trajectory is deemed successful if the final environment state aligns with a predefined target state and yields a reward of 1; otherwise, the reward is 0. 
This metric ensures a consistent measure of task performance across both environments.



\subsection{Main Results}
Table~\ref{tab:main_results} summarizes the performance of various methods on long-horizon tasks in the VirtualHome and ALFWorld environments. The proposed method, STeCa, achieves the highest overall performance, with an average reward of 70.9, significantly outperforming the baseline methods.
Compared to prompting-based methods, which exhibit relatively poor performance, STeCa demonstrates a significant improvement.
These results highlight the inherent limitations of closed-source LLMs relying solely on prompt engineering.
As a tuning-based method, STeCa demonstrates consistent superiority over prior approaches. Specifically, it achieves an average final reward of 70.9, surpassing IPR, the previous state-of-the-art method with an average reward of 68.6, by 3.4\%. This improvement highlights the effectiveness of trajectory calibration in enhancing generalization and overall performance. Moreover, STeCa outperforms E$^2$CL, a method that incorporates self-reflection mechanisms, by 4.0\%. 
Notably, STeCa achieves this without requiring additional iterative training, underscoring its superior training efficiency.

To further validate our method, we conducted an ablation study. Using SFT combined with Direct Preference Optimization~\citep{rafailov2024direct} (w/ SFT+DPO), the average reward dropped slightly to 70.0, while SFT training alone (w/o RT) resulted in a further decline to 69.6. Although these variants performed competitively, neither matched the performance of STeCa, highlighting the effectiveness of our proposed mechanisms.

\begin{table}[t!]
    \centering
    \resizebox{0.85\linewidth}{!}{
    \begin{tabular}{l c c c}
    \toprule
    \textbf{Base Model}  &  \textbf{Method} & \textbf{Seen} & \textbf{Unseen}\\ \midrule
    \multirow{3}{*}{Mistral-7B}  & SFT & 67.1 & 58.2 \\
    & IPR & 71.4 & 73.9 \\
    \cmidrule{2-4}
    & \textbf{STeCa} & \textbf{73.3} & \textbf{75.3}  \\ 
    \midrule
    \multirow{3}{*}{Llama-3-8B-Instruct} & SFT & 68.6 & 62.7 \\
    & IPR & 72.3 & 75.8 \\
    \cmidrule{2-4}
    & \textbf{STeCa} & \textbf{74.9} &  \textbf{77.0}  \\
    \bottomrule
    \end{tabular}
    }
    \caption{Performance of SFT, IPR, and our \model with different base models on the ALFWorld dataset.}
    \label{tab:base model}
\end{table}

