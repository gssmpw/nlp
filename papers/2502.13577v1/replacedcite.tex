\section{Related Work}
\begin{description}
    \item[Geometry of LLM Embeddings] Previous studies on the geometric properties of LLM Embeddings suggest that the embeddings capture extensive semantic information but are often distributed anisotropically, which can lead to high similarity between unrelated words, limiting the expressive power of the model ____. ____ have shown that the LLM token embedding space indeed exhibits a stratified manifold structure.
    \item[Mixture-of-Experts] Originally introduced by ____, the technique uses a gating network that routes inputs to one or a sparse set of specialized expert models. In recent years, MoE has scaled up drastically to work with large language models to increase model capacity with minimal added computations ____. Our approach also uses an MoE architecture, but for a different purpose: instead of scaling capacity, we use MoE to cluster and capture structure in an existing embedding space with known expert hyperparameters (such as sparsity level in a dictionary learning algorithm). In essence, we employ an unsupervised MoE that operates on fixed LLM embeddings to find meaningful partitions of different geometrical structures.
    \item[Dictionary Learning \& Sparse Coding] Dictionary learning is an algorithm that learns a set of dictionary atoms that can sparsely represent data through a linear combination (sparse codes) of atoms ____. Various algorithms have been proposed over the years that can learn dictionary and sparse codes effectively include KSVD ____, Online Dictionary Learning ____, Orthogonal Matching Pursuit (OMP) and its variant Batch-OMP ____, Iterative Hard-Thresholding (IHT) and Iterative Soft-Thresholding Algorithm (ISTA) ____, Learnable-ISTA ____, \textit{etc}.
\end{description}