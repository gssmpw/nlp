In this article, we propose a scalable deep reinforcement learning algorithm, Atomic-PPO, to optimize the long-run average reward of a ride-hailing system with a robo-taxi fleet. In Atomic-PPO, we integrate our atomic action decomposition and state reduction into PPO, which is a state-of-art RL algorithm. Our atomic action decomposition reduces the action space from being exponential in fleet size to a constant, which significantly reduces the complexity of policy training. Moreover, our state reduction largely decreases the neural network input dimension, reducing the runtime and memory usage of our algorithm. We test our approach with extensive numerical experiments using the NYC for-hire vehicle dataset, where our algorithm significantly outperforms benchmarks in reward maximization. Additionally, we examine the impact of charger allocation, vehicle range, and charger outlet rates on reward. Our findings suggest that allocating chargers based on ridership patterns is more effective than a uniform distribution across regions. We also demonstrate that investing in fast chargers is crucial, while expanding vehicle range is less essential. For future research, we plan to extend Atomic-PPO to broader vehicle routing applications, such as delivery and logistics. 