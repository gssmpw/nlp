Before presenting the performance of our atomic PPO algorithm, in this section, we construct an upper bound on the optimal long-run average reward using fluid limit. This upper bound will be used to construct an upper bound of optimality gap of our atomic PPO algorithm, as shown in the next section. We reformulate our robo-taxi dispatching problem as a fluid-based linear optimization program, where the fluid limit is attained as the fleet size approaches infinity, with both trip demand volume and the number of chargers scaling up proportionally to the fleet size. Under the fluid limit, the system becomes deterministic, and the fleet dispatching policy, which is a probability distribution of vehicle flows across all actions, reduces to a deterministic vector that represents the fraction of fleet assigned to each action at each time of the day.

%In particular, we formulate the vehicle dispatching as a fluid-based optimization problem, where the fluid limit is attained as the fleet size approaches infinity, where both the mean of trip arrivals and the number of chargers are scaled up proportionally to the fleet size. At each time of a day, we make a decision on the fraction of vehicles taking each action. The objective is to maximize the reward obtained for a single day.

We define the decision variables of the fluid-based optimization problem as follows:
\begin{enumerate}
    \item[-] \emph{Fraction of fleet for trip fulfilling $\lptripfulfill{}{} := \left(\lptripfulfill{\cartype{}, \triptype{}}{\time}\right)_{\cartype{} \in \Cartype, \triptype{} \in \Triptype, \time \in [T]}$}, where $\lptripfulfill{\cartype{}, \triptype{}}{\time}$ is the fraction of vehicles with status $\cartype{}$ fulfilling trip requests of status $\triptype{}$ at time $\time$.
    \item[-] \emph{Fraction of fleet for repositioning $\lpreroute{}{} := \left(\lpreroute{\cartype{}, \destination}{\time}\right)_{\cartype{} \in \Cartype, \destination \in \Region, \time \in [T]}$}, where $\lpreroute{\cartype{}, \destination}{\time}$ is the fraction of vehicles with status $\cartype{}$ repositioning to $\destination$ at time $\time$. 
    \item[-] \emph{Fraction of fleet for charging $\lpcharge{}{} := \left(\lpcharge{\cartype{}, \type}{\time}\right)_{\cartype{} \in \Cartype, \type \in \Type, \time \in [T]}$}, where $\lpcharge{\cartype{}, \type}{\time}$ denotes the fraction of vehicles with status $\cartype{}$ charging with rate $\type$ at time $\time$.
    \item[-] \emph{Fraction of fleet for continuing the current action $\lppass{}{} := \left(\lppass{\cartype{}}{\time}\right)_{\cartype{} \in \Cartype, \time \in [\Horizon]}$}, where $\lppass{\cartype{}}{\time}$ is the fraction of fleet with status $\cartype{}$ taking the passing action at time $\time$.
\end{enumerate}

The fluid-based linear program aims at maximizing the total reward achieved by the fluid policy:
{
\begin{align*}
    &\max_{\lptripfulfill{}{}, \lpreroute{}{}, \lpcharge{}{}, \lpslack{}{}, \lppass{}{}}  \Size \sum_{\time \in [\Horizon]}^{} \sum_{\cartype{} \in \Cartype} \left\{ \sum_{\origin \in \Region} \sum_{\destination \in \Region} \left[ \tripfulfillreward{,\origin\destination}{\time} \sum_{\tripactivetime \in [\Lc]}^{} \lptripfulfill{\cartype{}, (\origin, \destination, \tripactivetime)}{\time}   \right.\right. \nonumber\\
    &\qquad+ \left.\left. \reroutingreward{,\origin\destination}{\time} \lpreroute{\cartype{}, \destination}{\time} \right] + \sum_{\type \in \Type} \chargingreward{,\type}{\time} \lpcharge{\cartype{}, \type}{\time} \right\}, \notag \\%\label{eq:fluid-obj}\\
    &\text{s.t.} \  \eqref{eq:fluid-ev-conservation-red}-\eqref{eq:fluid-nonneg-red}.
\end{align*}
}


The constraints are given as follows:
\begin{enumerate}
    \item The flow conservation for each vehicle status $\cartype{}:= (\destination, \timetoarrival, \battery) \in \Cartype$ at each time $\time$ of a day. \\
    In particular, the left-hand side represents the vehicle flows from $\time - 1$ transitioning to the vehicle status $\cartype{}$ according to \eqref{eq:setup-car-state-transition}. The right-hand side represents the assignment of vehicles of status $\cartype{}$ to trip-fulfilling, repositioning, charging, and idling/passing actions. 
    \begin{align}
        &\left(\sum_{\origin \in \Region} \sum_{\cartype{}'= (\origin, \timetoarrival', \battery') \in \Cartype} \sum_{ \triptype{}= (\origin, \destination, \xi') \in \Triptype} \right. \notag\\
        &\qquad \left. \lptripfulfill{\cartype{}', \triptype{}}{\time-1}\mathds{1}(\timetoarrival' + \timecost{\origin\destination}{\time-1} - 1 = \timetoarrival, \ \battery' - \batterycost{\origin \destination} = \battery) \right) \notag \\
        &\qquad+ \left(\sum_{\origin \in \Region} \sum_{\cartype{}'= (\origin, 0, \battery') \in \Cartype} \lpreroute{\cartype{}', \destination}{\time-1} \cdot \right. \notag\\
        &\qquad \left. \mathds{1}(\timecost{\origin\destination}{\time-1} - 1 = \timetoarrival, \  \battery' - \batterycost{\origin\destination} = \battery) \right) \notag \\
        &\qquad+ \left[ \left(\sum_{\type \in \Type} \lpcharge{(\destination, 0, \battery - \type \chargetime), \type}{\time-1} \mathds{1}(\timetoarrival = \chargetime - 1, \battery \geq \type \chargetime) \right) + \right. \notag\\
        &\qquad \left. \left(\sum_{\battery' > \battery - \type \chargetime} \sum_{\type \in \Type} \lpcharge{(\destination, 0, \battery'), \type}{\time-1} \mathds{1}(\timetoarrival = \chargetime - 1, \battery = \range) \right) \right] \notag\\
        &\qquad+ \lppass{(\destination, \timetoarrival, \battery)}{\time-1} \mathds{1}(\timetoarrival = 0) + \lppass{(\destination, \timetoarrival+1, \battery)}{\time-1} \mathds{1}(\timetoarrival < \maxtimecost{}) \notag \\
        &= \sum_{\triptype{} \in \Triptype} \lptripfulfill{\cartype{}, \triptype{}}{\time} + \sum_{\destination \in \Region} \lpreroute{\cartype{}, \destination}{\time} + \sum_{\type \in \Type} \lpcharge{\cartype{}, \type}{\time} + \lppass{\cartype{}}{\time},\notag\\ 
        &\qquad \forall \cartype{}:= (\destination, \timetoarrival, \battery) \in \Cartype, \   \time \in [\Horizon], \label{eq:fluid-ev-conservation-red}
    \end{align}
    We note that the time steps are periodic across days, and thus for $t=1$ in \eqref{eq:fluid-ev-conservation-red}, $t-1$ is the last time step $T$ of the previous day. Similarly, in all of the subsequent constraints \eqref{eq:fluid-passenger-flow-cap-red} -- \eqref{eq:fluid-charging-cap-red}, the time step $t$ on the superscript of a variable being negative indicates time step $T-t$ in the previous day and $t>T$ indicates time step $t-T$ of the next day. 
    \item The fulfillment of trip orders does not exceed their arrivals.
    \begin{align}
        &\sum_{\cartype{}=(\origin, \timetoarrival, \battery) \in \Cartype}^{}\sum_{\triptype{}=(\origin, \destination, \tripactivetime) \in \Triptype}^{} \lptripfulfill{\cartype{}, \triptype{}}{\time + \tripactivetime} \leq \frac{1}{\Size} \arrrate{\origin \destination}{\time}, \notag\\ 
        &\qquad \forall \origin, \destination \in \Region,\ \time \in [\Horizon].\label{eq:fluid-passenger-flow-cap-red}
    \end{align}
    \item The number of vehicles charging at a specific rate in a given region does not exceed the corresponding charging capacity at any time.
    \begin{align}
        \sum_{j \in [\chargetime]} \sum_{\cartype{}= (\region, 0, \battery) \in \Cartype} \lpcharge{\cartype{}, \type}{\time - j} \leq \frac{1}{\Size}\n{\region}{\type}, \forall \type \in \Type, \  \time \in [\Horizon]. \label{eq:fluid-charging-cap-red}
    \end{align}
    \item The battery is sufficient for vehicles to complete the trips for trip-fulfillment.
    \begin{align}
        &\ \lptripfulfill{\cartype{}, \triptype{}}{\time} \mathds{1}\{\battery < \batterycost{\origin \destination}\} = 0,\notag\\ 
        &\quad \forall \cartype{}= (\origin, \timetoarrival, \battery) \in \Cartype,\ \triptype{}= (\origin, \destination, \tripactivetime) \in \Triptype, \ \time \in [\Horizon]. \label{eq:fluid-passenger-flow-battery-sufficiency-red}
    \end{align}
    \item The battery is sufficient for vehicles to complete the trips for repositioning.
    \begin{align}
        &\ \lpreroute{\cartype{}, \destination}{\time} \mathds{1}\{\battery < \batterycost{\origin \destination}\} = 0, \notag\\
        &\quad \forall \cartype{}= (\origin, \timetoarrival, \battery) \in \Cartype,\ \destination \in \Region, \ \time \in [\Horizon]. \label{eq:fluid-rerouting-flow-battery-sufficiency-red}
    \end{align}
    \item The fractions of vehicles of all statuses should add up to 1 at all times.
    \begin{align}
        &\ \sum_{\cartype{} \in \Cartype} \left[ \sum_{\triptype{} \in \Triptype} \lptripfulfill{\cartype{}, \triptype{}}{\time} + \sum_{\destination \in \Region} \lpreroute{\cartype{}, \destination}{\time} + \sum_{\type \in \Type} \lpcharge{\cartype{}, \type}{\time} + \lppass{\cartype{}}{\time} \right] = 1,\notag \\
        & \qquad\qquad \qquad\qquad \qquad\qquad  \forall \time \in [\Horizon]. \label{eq:fluid-total-flow-red}
    \end{align}
    \item All decision variables are non-negative.
    \begin{align}
        \lptripfulfill{}{}, \lpreroute{}{}, \lpcharge{}{}, \lppass{}{} \geq 0. \label{eq:fluid-nonneg-red}
    \end{align}
\end{enumerate}

Let $\bar{R}$ be the optimal objective value from the fluid based LP. 

\begin{theorem} \label{thm:fluid-obj-val}
    $R^*(s) \leq \bar{R}, \quad \forall s \in \mathcal{S}$.
\end{theorem}

The proof of Theorem \ref{thm:fluid-obj-val} is deferred to the Appendix. Theorem \ref{thm:fluid-obj-val} shows that $\bar{R}$ is an upper bound on the long-run average daily reward that can be achieved by any feasible policy. In the numerical section, we assess the gap between the average daily reward achieved by our Atomic-PPO and the fluid upper bound $\bar{R}$. This gap is an upper bound of optimality gap achieved by Atomic-PPO algorithm. We note that under the current formulation of the fluid-based LP, the number of variables scales with $\vert \Region \vert (\Lp + \maxtimecost{}{}) \range \Horizon$, where $\maxtimecost{}{}$ can potentially be very large. We can reduce the size of this LP to $|V|L_pBT$ without loss of optimality by leveraging the fact that only vehicles with task remaining time $\eta < L_p$ can be assigned with new tasks, and therefore we only need to keep track of a fraction of fleet statuses when computing the optimal fluid policy. We delay the presentation of our simplified fluid-based LP to the appendix.
%As a high-level idea of the proof, we first show that it is without loss of optimality to restrict our attention to deterministic stationary policies. Then, we show that for every deterministic stationary policy that satisfies all the feasibility constraints in Sec. \ref{sec:model}, we can construct a feasible solution to \eqref{eq:fluid-lp} by using the expected value of the fraction of vehicles taking each action at stationarity. Lastly, we argue that by substituting the variables in the objective function of \eqref{eq:fluid-lp} with the expected value of the fraction of vehicles taking each action at stationarity, we can obtain the long-run average reward achieved by the policy. Therefore, it immediately follows that the objective value obtained from \eqref{eq:fluid-lp} is an upper bound on the long-run average reward achievable by any feasible policy. The formal proof of theorem \ref{thm:fluid-obj-val} is delayed to the appendix.