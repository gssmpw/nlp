\paragraph{Related Literature.} The problem of optimal fleet dispatch has been extensively studied in the context of ride-hailing systems, with the majority focuses on gasoline vehicles (and thus no need for scheduling charging) and several recent works on EVs \citep{varma2023electric, roberti2016electric, luke2021joint, boewing2020vehicle, zalesak2021real, shi2019operating, kullman2022, dean2022synergies, yu2023coordinating, singh2024dispatching, dong2022dynamic, su2024branch, ahadi2023cooperative, liu2022smart, chen2023electric, gao2024stochastic, li2024bm, ma2023prolific, li2024coordinating, zhu2023dynamic, provoost2022improving, al2020approximate, iacobucci2019optimization, la2019heuristics, turan2020dynamic, wang2022multi, song2022sumo}. The methods adopted in these literature fall into one of the three main categories: (1) deterministic optimization method; (2) queueing-based analysis; and (3) deep reinforcement learning. 
%All ride-hailing literature we encounter that use RL model the system as a finite-horizon MDP problem. In contrast, our algorithm is designed to solve vehicle dispatching problem in a periodic MDP with infinite horizon and a long-run average reward objective. We note that modeling the system with an infinite horizon and a long-run average reward objective is crucial when accounting for vehicle charging. A finite-horizon or discounted infinite-horizon setting will produce a dispatching policy that depletes the fleet's battery either at the end of the finite horizon or when the discounted reward becomes negligibly small, especially when the charging facilities are limited.

\medskip 
\noindent(1) Deterministic optimization. The first category of works models the ride hailing system as a deterministic system, where all future trip arrivals are known. In deterministic systems, the optimal fleet dispatch can be formulated as a linear program \citep{luke2021joint} or mixed integer programs (MIP) \citep{zalesak2021real, boewing2020vehicle, dean2022synergies, tuncel2023integrated, yu2023coordinating, su2024branch, chen2023electric, li2024bm, li2024coordinating, provoost2022improving, iacobucci2019optimization}. These methods cannot be applied to settings with uncertain trip demand. 

\medskip 
\noindent(2) Queueing-Based Analysis. The second category of works models ride-hailing systems using queueing theory. Some studies adopt a closed queueing system approach \citep{banerjee2018, braverman2019empty, zhang2018modeling, waserhole2012vehicle, afeche2023}, where drivers remain in the system indefinitely. Others utilize an open queueing system framework \citep{varma2023electric, dong2022dynamic, wang2022, ozkan2020joint, ozkan2020dynamic, xu2021generalized}, where drivers dynamically enter and exit the system. Notably, the majority assume time-homogeneity in the system, with exceptions such as \cite{ozkan2020joint, ozkan2020dynamic, afeche2023, xu2021generalized}, which address time-varying dynamics. Most of this research focuses on developing fleet control policies that are optimal under specific conditions or when the system reaches a fluid limit. Although these policies demonstrate provable optimality as fleet sizes approach infinity, their performance can suffer from significant optimality gaps when fleet sizes are moderate. Our construction of a reward upper bound builds on the fluid-based analytical methods established in this body of work.



\medskip 
\noindent(3) Deep Reinforcement Learning. The third category of research uses deep reinforcement learning (RL) to train fleet dispatching policies in complex environments that incorporate both spatial and temporal demand uncertainty. For a comprehensive review of these studies, see the recent survey by \cite{qin2022reinforcement}. Majority of studies in this category including ours model the ride-hailing system as a Markov Decision Process (MDP), where dispatch decisions are made for the entire fleet at discreet time steps. A few articles modeled the system as a semi-MDP, where decision making is triggered by trip arrival or a vehicle completing a task \citep{kullman2022, singh2024dispatching, heitmann2023combining, gao2024stochastic}. 

Existing research has primarily focused on two frameworks: finite-horizon settings, where decisions are optimized over a limited time frame, and discounted infinite-horizon settings, which prioritize near-term rewards. In contrast, we propose an infinite-horizon model with a long-run average reward objective. Finite-horizon or discounted settings may be effective in certain scenarios, such as with conventional vehicles, where the system resets each midnight during periods of low demand. However, these approaches are less suitable for managing electric fleets. The fleet control policies trained under these settings do not account for rewards in the long-run and therefore could lead to battery depletion across the fleet. If a ride-hailing company lacks sufficient charging facilities to recharge the entire fleet overnight, then the fleet with depleted batteries will be unable to fulfill trip requests the following day. Therefore, we argue that incorporating both infinite-horizon and average reward objectives is essential for electric vehicle operations because these features enable operators to optimize for steady-state battery levels across the fleet. 
%While effective for certain scenarios, these approaches can lead to suboptimal policies for electric vehicle fleets, as they often fail to account for battery depletion effectively. , which is critical for addressing vehicle charging constraints. By optimizing for steady-state operations, our model avoids policies that deplete fleet batteries toward the end of the planning horizon or discount future rewards to the point of irrelevance.

A major challenge in training fleet dispatching policies is managing the high dimensionality of state and action spaces. Consequently, most studies focus on specific subproblems, such as ride matching, vehicle repositioning, or charging, rather than developing integrated frameworks that address all these components simultaneously. A few studies have attempted to address multiple aspects of this challenge, including \cite{zhu2023dynamic, al2020approximate, wang2022multi, kullman2022, turan2020dynamic}. Building on these efforts, our model provides a comprehensive approach that unifies ride matching, repositioning, and charging into a holistic fleet management solution.

%  Deep reinforcement learning. The third category of works use deep RL to train fleet dispatching policies in complex environment that incorporates demand uncertainty both spatially and temporally. For a comprehensive review of these works, please refer to the recent survey paper by \cite{qin2022reinforcement}. Some researchers formulate the ride-hailing system as a semi-MDP, where each decision epoch is triggered by either a trip arrival or a vehicle completing a task \citep{kullman2022, singh2024dispatching, heitmann2023combining, gao2024stochastic}. 
% In contrast, the majority of literature adopt the MDP framework, where the system makes batched decisions at every decision epoch. All works we come across consider a finite time horizon while our work considers an infinite time horizon with a long-run average reward objective. We note that modeling the system with an infinite horizon and a long-run average reward objective is crucial when accounting for vehicle charging. A finite-horizon or discounted infinite-horizon setting will produce a dispatching policy that depletes the fleet's battery either at the end of the finite horizon or when the discounted reward becomes negligibly small, especially when the charging facilities are limited.

% The challenge of training fleet dispatching policies arises from the high-dimensionality of the state and action space. Due to this challenge, most papers do not consider all three aspects of ride matching, vehicle repositioning, and charging, except for \cite{zhu2023dynamic, al2020approximate, wang2022multi, kullman2022, turan2020dynamic}. We complement these works as our model can account for all these three aspects.


To tackle the curse of dimensionality arising from the large fleet size, prior works adopted various strategies that include (i) T-step look ahead approach \citep{wei2023reinforcement} that computes the optimal fleet dispatching actions in the first $T$ steps of the value iteration by solving a linear program, (ii) warm start approach \citep{yuan2022reinforcement} that initializes the fleet dispatching policy by imitation learning from mixed integer programs, (iii) restricting vehicles at the same region to take the same action \citep{qin2021optimizing, liu2022deep}, or (iv) relaxing integral constraints and treating the fleet action as continuous variables \citep{mao2020dispatch, zhou2023robust, filipovska2022anticipatory, turan2020dynamic, gammelli2022graph, skordilis2021modular, schmidt2024learning, xie2023two, si2024vehicle}. However, T-step look ahead approach and warm start approach are still computationally challenging as they need to repeatedly solve large-scale linear programs or mixed integer programs. The remaining two approaches may incur errors that increase in the system's complexity, especially with the integration of ride-matching, repositioning, and charging.


Another extensively studied method to reduce the dimensionality of the policy space is to use a decentralized reinforcement learning (RL) training approach \citep{xu2018large, wang2018deep, tang2019deep, qin2020ride, han2016routing, jin2019coride, li2019efficient, zhou2019multi, enders2023hybrid, cordeiro2023deep, xi2022hmdrl, chen2024rebalance, xi2021ddrl, ahadi2023cooperative, rong2016rich, yu2019integrated, shou2020optimal, zhou2018optimizing, woywood2024multi, verma2017augmenting, han2016routing, gao2018optimize, wen2017rebalancing, holler2019deep, jiao2021real, garg2018route, singh2021distributed, xu2023multi, wang2024reinforcement, liu2022smart, ma2023prolific, zhu2023dynamic, al2020approximate, wang2022multi}. In this approach, vehicles are treated as homogeneous and uncooperative agents that independently choose their own optimal actions based on a shared Q-function. While the decentralized RL approach effectively reduces the policy dimension, the induced policy is theoretically suboptimal because it does not consider the impact of their action on the state transition and system reward collectively. 
Our Atomic PPO algorithm addresses this issue by introducing a coordinated sequential assignment of atomic actions that incorporates state transitions after each step. This approach reduces the action space while ensuring the algorithm still accounts for the impact of each vehicle's action on the system state and learns the accumulated rewards from the dispatch actions of the entire fleet in each decision epoch.


%Our Atomic PPO algorithm addresses this issue by considering a coordinated sequential assignment of atomic actions that incorporates state transition after each assignment. This sequential assignment, although reduces the action space, still accounts for the state changes resulted from each vehicle's atomic action, and therefore learns the accumulated rewards from the joint dispatch action of the entire fleet idnuced by the sequential atomic action assignment in each decision epoch.


%\paragraph{Our contributions.} We model the problem of optimal dispatch policy of robo-taxi fleet that incorporates ride-matching, repositioning, and charging as an infinite horizon average reward Markov decision process. 
Finally, our Atomic-PPO algorithm builds on our previous work \citep{feng2021scalable}, which addressed ride matching and vehicle repositioning for conventional vehicle fleets within a finite-horizon framework. In this study, we extend the framework by incorporating charging and adopting an infinite-horizon, long-run average reward objective. Consequently, our algorithm introduces a new state reduction scheme to address these added complexities. Additionally, we provide a theoretical upper bound on the optimal long-run average reward, serving as a benchmark for our fleet dispatching policy. Experimental results show that our algorithm achieves a reward within 10\% of the theoretical upper bound while maintaining low training costs.




% Survey. MDP, semi-MDP. All are finite horizon discounted or average reward. Emphasize long run average reward is the right choice for EV and why it is hard. 

% Challenge of state and policy dimension. Due to this, most paper did not consider all three tasks except for a few works . 

% Most popular training approach is to consider decrentralized. 

% In centralzied training, there are other methods that include xx, xx, xx, continuous variables . Drawback and comments, finite and non EV 


% Restate what is our atomic PPO and why it makes sense (include Jim's previous work). atomic, PPO, long run average reward for battery. Our algorithm achieves small optimality gap with small training expense. Our optimality gap benchmark to upper bound with fluid limit, which is an unachievable upper bound of system reward. 

% To incorporate the spatiotemporal aspects of ride-hailing systems and the uncertainty of future demand into decision making, numerous literature has adopted the reinforcement learning (RL) approach (see the recent survey \citep{qin2022reinforcement}). Most of the papers do not address all three aspects of order matching, vehicle repositioning, and charging, except for \cite{zhu2023dynamic, al2020approximate, wang2022multi, kullman2022, turan2020dynamic}. We complement these works as our model can account for all these three aspects.

% Some researchers formulate the ride-hailing system as a semi-MDP, where each decision epoch is triggered by either a trip arrival or a vehicle completing an action \citep{kullman2022, singh2024dispatching, heitmann2023combining, gao2024stochastic}. Under the semi-MDP framework, the system only needs to assign an action to a single trip request or a single vehicle in each decision epoch. In contrast, our model makes batched decisions by assigning an action to the entire fleet at fixed time intervals. Making batched decisions enables us to increase market thickness and reduce the noises from the stochastic trip arrivals, which mitigates issues such as the ``wild goose chase" problem \citep{wgc} \footnote{This issue arises when numerous vehicles drive empty to pick up distant trip requests while leaving many nearby requests unfulfilled.}.

% Nevertheless, making batched decisions requires the system to simultaneously dispatch multiple vehicles, resulting in a combinatorial action space that scales with fleet size. To tackle with the curse of dimensionality arising from the large fleet size, the literature has been predominating on decentralized RL \citep{xu2018large, wang2018deep, tang2019deep, qin2020ride, han2016routing, jin2019coride, li2019efficient, zhou2019multi, enders2023hybrid, cordeiro2023deep, xi2022hmdrl, chen2024rebalance, xi2021ddrl, ahadi2023cooperative, rong2016rich, yu2019integrated, shou2020optimal, zhou2018optimizing, woywood2024multi, verma2017augmenting, han2016routing, gao2018optimize, wen2017rebalancing, holler2019deep, jiao2021real, garg2018route, singh2021distributed, xu2023multi, wang2024reinforcement, liu2022smart, ma2023prolific, zhu2023dynamic, al2020approximate, wang2022multi}. In decentralized settings, vehicles are treated as individual agents and the Q-functions are learned for each agent's actions. Although decentralized RL has demonstrated strong empirical performance in ride-hailing applications, it is theoretically suboptimal compared to centralized planning algorithms. This limitation arises because agent-based Q-functions fail to capture the effect of individual policies on other vehicles. On the other hand, the sequential assignment of atomic actions to individual vehicles, as proposed in our work, can account for the state changes and accumulated rewards from the joint effect of the fleet.

% Asides from the decentralized frameworks, there are various strategies adopted by existing literature to tackle with the high-dimensionality arising from the large fleet size. \citep{wei2023reinforcement} proposes a $T$-step look ahead approach for value iteration, where the optimal fleet dispatching policy of the first $T$ steps are obtained by solving a LP. \citep{yuan2022reinforcement} warms start their fleet dispatching policy by imitation learning from a MIP, and then improves the policy using policy gradient methods. However, repeatedly solving LPs or MIPs can be computationally burdensome when the planning horizon is large. \citep{qin2021optimizing, liu2022deep} partitions the service area into grids and defines the action space as the joint actions of each grid, but they require all vehicles in the same grids to take the same action. There has been a large number of literature that treats the fleet action as continuous variables \citep{mao2020dispatch, zhou2023robust, filipovska2022anticipatory, turan2020dynamic, gammelli2022graph, skordilis2021modular, schmidt2024learning, xie2023two, si2024vehicle}, where they group vehicles of the same status (i.e. region, time to arrival) and assign a fraction of them to each action. This approach can prevent the action dimension from scaling exponentially with respect to the fleet size and has demonstrated to have decent performance for ride-hailing systems with gasoline vehicles. However, the action dimension still scales with the number of vehicle status and thus makes the computation cumbersome when we keep track of the battery levels in the vehicle status. Moreover, as the number of vehicle status grows larger after the incorporation of battery levels, the fractional solution introduces larger rounding errors. The method proposed by \citep{feng2021scalable} is more similar to our work, where in each decision epoch, the system sequentially assigns atomic actions to individual vehicles. However, Feng et al. does not consider the charging scheduling of vehicles, while our work efficiently incorporates charging into the fleet dispatching system using a novel state reduction scheme. 

% Beyond our contribution to fleet dispatching algorithms, we also provide a provable theoretical upper bound on the reward to benchmark our fleet dispatching policy. Among all works that use RL for fleet dispatching, only \citep{zhou2023robust} and \citep{singh2024dispatching} provide an upper bound on the performance of the fleet dispatching policies. However, both of these works compute the upper bound by assuming the perfect information of the system (e.g. the number of trip arrivals across all times), and require to solve a MIP for every problem instance with realized demand in their simulations. On the other hand, we provide a fluid-based LP that is proved to give an upper bound on the long-run average reward achievable by any feasible fleet dispatching policy. Therefore, as long as the mean of trip arrivals remain the same among different simulation trajectories, we only need to solve the LP once in order to obtain the upper bound.