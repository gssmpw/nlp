Robo-taxi services have been deployed in several U.S. cities, including San Francisco, Phoenix, and Los Angeles \citep{robotaxi_news}. The efficient operations of robo-taxi fleets, comprised of electric vehicles, is challenged by the stochasticity and the spatiotemporal distribution of trip demand, as well as the scheduling of battery charging with limited charging infrastructure. Inefficient operations could render vehicles unavailable during high-demand periods, leading to decreased service quality, reliability issues, and revenue loss. %\textcolor{blue}{The complexity of this problem is further compounded with its high-dimensionality arising from the large fleet size.}

%We partition the service area into a finite number of regions. Each region has a non-negative number of charging facilities with various charger outlet rates. Passenger arrivals are modeled as a Poisson process, with a varying arrival rate that depends on the origin-destination pair and time of the day. 
In this work, we model the robo-taxi fleet dispatch problem as a Markov decision process with discrete state and action space. The state of system records the spatial distribution of vehicles, their current tasks, active trip requests, and the availability of chargers. Based on the state, the system dispatches the entire fleet to manage trip fulfillment, repositioning, charging, and passing (i.e. continuing with their current tasks), resulting in associated rewards or costs. Our goal is to find a fleet dispatching policy that maximizes the long-run average reward over an infinite time horizon.

The challenge of computing the fleet dispatching policy arises from the high-dimensionality of the state and action space. Since the state space contains all possible distributions of the fleet and the action space contains all feasible fleet assignments, both the state space and the action space scale exponentially with the fleet size. We develop a scalable deep reinforcement learning (RL) algorithm, which we refer as atomic proximal policy optimization (Atomic-PPO). The efficiency of Atomic-PPO is accomplished by decomposing the dispatching of the entire fleet into the sequential assignment of atomic action (tasks such as trip fullfillment, reposition, charge or pass (no new assignment)) to individual vehicles, which we refers to as ``atomic action decomposition". The dimension of the atomic action space equals to the number of feasible tasks that can be assigned to an individual vehicle. Hence, the atomic action decomposition reduces the action space from being exponential in fleet size to being a constant, significantly reducing the complexity of policy training. We integrate our atomic action decomposition into a state-of-art RL algorithm, PPO \citep{schulman2017proximal}, which possesses the monotonic policy improvement guarantee and has proven to be effective in control optimization across various applications \citep{vinyals2019grandmaster, berner2019dota, simm2020reinforcement, zoph2018learning, akkaya2019solving}. We approximate both the value function and the atomic action function in the PPO training using neural networks, and further reduces the dimension of state representation by clustering the vehicle battery levels and destination information of trip requests.

To obtain an upper bound of the optimality gap of our fleet dispatching policy, we derive an upper bound on the optimal long-run average reward using a fluid-based linear program (LP) (see Theorem \ref{thm:fluid-obj-val}). The fluid limit is attained as the fleet size approaches infinity, with both trip demand volume and the number of chargers scaling up proportionally to the fleet size. Moreover, we benchmark our fleet dispatching policy against two heuristics. One of the heuristics is derived by applying randomized rounding to the fractional solution obtained from the fluid-based LP. The second heuristic is the power-of-k dispatching policy rooted in the load-balancing literature \citep{vvedenskaya1996queueing, mitzenmacher1996load}. \footnote{In the load-balancing context, the power-of-k uniformly samples k number of queues at random and then routes the incoming arrival to the shortest queue among them.} The power-of-k dispatching heuristic selects k closest vehicles for a trip request and matches the request with the vehicle that has the highest battery level \citep{varma2023electric}. Varma et al. demonstrates that under the assumption that all trip requests and charging facilities are uniformly distributed across the entire service area, the power-of-k dispatching policy can achieve the asymptotically optimal service level. We note that this assumption is restrictive and is not satisfied in our setting.

We evaluate the performance of our Atomic PPO algorithm using the NYC For-Hire Vehicle dataset \citep{nyctlc} in Manhattan area with a fleet size of 300, which is comparable to the fleet size deployed in a city by Waymo \citep{waymo_news}.% or Cruise \footnote{Here, we refer to the time before Cruise slashed its fleet size due to the collision incidents \citep{cruise_slash_news}.} \citep{cruise_news}. 
The fleet size is approximately 5\% of the for-hire vehicles in Manhattan area and we scale down the trip demand accordingly. Furthermore, our simulation incorporates the nonlinear charging rate, time-varying charging, repositioning cost and trip reward. 

In our baseline setting, we assume that there are abundant number of DC Fast chargers (75 kW) in all regions. Our Atomic-PPO beats both benchmarks in terms of total reward by a large margin. In particular, Atomic-PPO can achieve as high as 91\% of fluid upper bound, while the power-of-k dispatching and the fluid policy can only achieve 71\% and 43\% of fluid upper bound respectively. Moreover, our Atomic-PPO can achieve a near-full utilization of fleet for trip fulfillment during rush hours, while both benchmark policies have a significant number of vehicles idling at all time steps. The training of our Atomic-PPO is very efficient, as it can be completed within 3 hours using 30 CPUs. 

Additionally, we investigate the impact of charging facility allocation, vehicle range, and charger outlet rate on the long-run average reward. We find that the uniform allocation of chargers across all regions is inefficient, as it requires 30 chargers (10\% of the fleet size) to attain comparable long-run average reward as with abundant chargers. On the other hand, by allocating chargers according to ridership patterns, we only need 15 chargers (5\% of the fleet size) to obtain the comparable performance. Additionally, we find that deploying fast chargers is crucial for the reward maximization. A fast charger reduces the amount of time it takes to refill a certain amount of battery, which then reduces the opportunity cost incurred by charging. However, in our setting, we find that investing in vehicles with a longer range is not important as most trips are relatively short and longer range does not reduces the time cost of charging.%it does not reduce the time to refill the same amount of battery.

% \textcolor{blue}{Additionally, we investigate the impact of charging facility allocation, vehicle range, and charger outlet rate on revenue. We attempt allocating chargers uniformly across all regions and allocating all chargers to a subset of regions. Moreover, we attempt doubling the vehicle range or reducing the charger outlet rate. We compare the revenue obtained by Atomic-PPO from all these scenario against the setting where we have abundant chargers. We obtain that it is more effective to allocate chargers according to ridership patterns, while the uniform allocation of charging facilities is very inefficient. Moreover, we find that it is crucial to deploy chargers with large charging outlet rate. We note that a vehicle plugged into a charger cannot take any trip requests, which incurs an opportunity cost. A fast charger reduces the amount of time it takes to refill a certain amount of battery, which then reduces the opportunity cost incurred by charging. On the other hand, investing in vehicles with a larger range is not effective in increasing the revenue, as it does not reduce the time to refill the same amount of battery.}

\input{literature_review}
% \paragraph{Related Literature.} There is a rich body of literature on ride-hailing that studies the optimal dispatching of fleets. We identify three main categories of methodologies: (1) deterministic optimization framework; (2) queueing or fluid-based approaches; and (3) RL-based algorithms. All ride-hailing literature we encounter that use RL model the system as a finite-horizon MDP problem. In contrast, our algorithm is designed to solve vehicle dispatching problem in a periodic MDP with infinite horizon and a long-run average reward objective. We note that modeling the system with an infinite horizon and a long-run average reward objective is crucial when accounting for vehicle charging. A finite-horizon or discounted infinite-horizon setting will produce a dispatching policy that depletes the fleet's battery either at the end of the finite horizon or when the discounted reward becomes negligibly small, especially when the charging facilities are limited.

% The first category of works uses mathematical programs to obtain the fleet dispatching policy. Most of the literature adopt mixed integer programs (MIP) \citep{zalesak2021real, boewing2020vehicle, dean2022synergies, tuncel2023integrated, yu2023coordinating, su2024branch, chen2023electric, li2024bm, li2024coordinating, provoost2022improving, iacobucci2019optimization}, while Luke et al. develops a linear program (LP) that jointly optimizes fleet dispatching and charger facility allocation \citep{luke2021joint}. However, models based on LP and MIP assume a deterministic ride-hailing system and therefore cannot account for the uncertainty in future trip arrivals.

% The second category of works adopts queueing or fluid-based approaches. The works by \cite{banerjee2018, braverman2019empty} model the ride-hailing system as a closed queueing network and they prove that their fleet dispatching policies are asymptotically optimal using the fluid limit. Some other works develop heuristic fleet dispatching policies by modeling the ride-hailing system either as a closed queueing network \citep{zhang2018modeling, waserhole2012vehicle} or an open queueing network \citep{varma2023electric, dong2022dynamic, wang2022}. However, all these works require the system to be time homogeneous. To account for the time-varying systems, some researchers develop fluid-based LPs to find the optimal fleet dispatching policies \citep{ozkan2020joint, ozkan2020dynamic, afeche2023, xu2021generalized}. Our work contributes to this stream of literature as we derive a fluid-based upper bound on the long-run average reward. While fluid-based policies are provably optimal when the fleet size approaches infinity, the optimality gap can be non-negligible when the fleet size is moderate.

% To incorporate the spatiotemporal aspects of ride-hailing systems and the uncertainty of future demand into decision making, numerous literature has adopted the reinforcement learning (RL) approach (see the recent survey \citep{qin2022reinforcement}). Most of the papers do not address all three aspects of order matching, vehicle repositioning, and charging, except for \cite{zhu2023dynamic, al2020approximate, wang2022multi, kullman2022, turan2020dynamic}. We complement these works as our model can account for all these three aspects.

% Some researchers formulate the ride-hailing system as a semi-MDP, where each decision epoch is triggered by either a trip arrival or a vehicle completing an action \citep{kullman2022, singh2024dispatching, heitmann2023combining, gao2024stochastic}. Under the semi-MDP framework, the system only needs to assign an action to a single trip request or a single vehicle in each decision epoch. In contrast, our model makes batched decisions by assigning an action to the entire fleet at fixed time intervals. Making batched decisions enables us to increase market thickness and reduce the noises from the stochastic trip arrivals, which mitigates issues such as the ``wild goose chase" problem \citep{wgc} \footnote{This issue arises when numerous vehicles drive empty to pick up distant trip requests while leaving many nearby requests unfulfilled.}.

% Nevertheless, making batched decisions requires the system to simultaneously dispatch multiple vehicles, resulting in a combinatorial action space that scales with fleet size. To tackle with the curse of dimensionality arising from the large fleet size, the literature has been predominating on decentralized RL \citep{xu2018large, wang2018deep, tang2019deep, qin2020ride, han2016routing, jin2019coride, li2019efficient, zhou2019multi, enders2023hybrid, cordeiro2023deep, xi2022hmdrl, chen2024rebalance, xi2021ddrl, ahadi2023cooperative, rong2016rich, yu2019integrated, shou2020optimal, zhou2018optimizing, woywood2024multi, verma2017augmenting, han2016routing, gao2018optimize, wen2017rebalancing, holler2019deep, jiao2021real, garg2018route, singh2021distributed, xu2023multi, wang2024reinforcement, liu2022smart, ma2023prolific, zhu2023dynamic, al2020approximate, wang2022multi}. In decentralized settings, vehicles are treated as individual agents and the Q-functions are learned for each agent's actions. Although decentralized RL has demonstrated strong empirical performance in ride-hailing applications, it is theoretically suboptimal compared to centralized planning algorithms. This limitation arises because agent-based Q-functions fail to capture the effect of individual policies on other vehicles. On the other hand, the sequential assignment of atomic actions to individual vehicles, as proposed in our work, can account for the state changes and accumulated rewards from the joint effect of the fleet.

% Asides from the decentralized frameworks, there are various strategies adopted by existing literature to tackle with the high-dimensionality arising from the large fleet size. \citep{wei2023reinforcement} proposes a $T$-step look ahead approach for value iteration, where the optimal fleet dispatching policy of the first $T$ steps are obtained by solving a LP. \citep{yuan2022reinforcement} warms start their fleet dispatching policy by imitation learning from a MIP, and then improves the policy using policy gradient methods. However, repeatedly solving LPs or MIPs can be computationally burdensome when the planning horizon is large. \citep{qin2021optimizing, liu2022deep} partitions the service area into grids and defines the action space as the joint actions of each grid, but they require all vehicles in the same grids to take the same action. There has been a large number of literature that treats the fleet action as continuous variables \citep{mao2020dispatch, zhou2023robust, filipovska2022anticipatory, turan2020dynamic, gammelli2022graph, skordilis2021modular, schmidt2024learning, xie2023two, si2024vehicle}, where they group vehicles of the same status (i.e. region, time to arrival) and assign a fraction of them to each action. This approach can prevent the action dimension from scaling exponentially with respect to the fleet size and has demonstrated to have decent performance for ride-hailing systems with gasoline vehicles. However, the action dimension still scales with the number of vehicle status and thus makes the computation cumbersome when we keep track of the battery levels in the vehicle status. Moreover, as the number of vehicle status grows larger after the incorporation of battery levels, the fractional solution introduces larger rounding errors. The method proposed by \citep{feng2021scalable} is more similar to our work, where in each decision epoch, the system sequentially assigns atomic actions to individual vehicles. However, Feng et al. does not consider the charging scheduling of vehicles, while our work efficiently incorporates charging into the fleet dispatching system using a novel state reduction scheme. 

% Beyond our contribution to fleet dispatching algorithms, we also provide a provable theoretical upper bound on the reward to benchmark our fleet dispatching policy. Among all works that use RL for fleet dispatching, only \citep{zhou2023robust} and \citep{singh2024dispatching} provide an upper bound on the performance of the fleet dispatching policies. However, both of these works compute the upper bound by assuming the perfect information of the system (e.g. the number of trip arrivals across all times), and require to solve a MIP for every problem instance with realized demand in their simulations. On the other hand, we provide a fluid-based LP that is proved to give an upper bound on the long-run average reward achievable by any feasible fleet dispatching policy. Therefore, as long as the mean of trip arrivals remain the same among different simulation trajectories, we only need to solve the LP once in order to obtain the upper bound.

% % We also contribute to the literature by providing insights into the efficient deployment of charging facilities and investment in vehicle models. Through extensive numerical experiments, we explore optimal strategies for charging facility allocation and evaluate the impact of charger outlet rates and vehicle ranges on the reward. To the best of our knowledge, only \citep{luke2021joint} has previously addressed these topics in the context of dispatching policies. However, the fleet dispatching policy proposed by Luke et al. is based on LP and therefore cannot deal with the stochasticity of trip arrivals.

% In summary, we have three main contributions to the literature:
% \begin{enumerate}
%     \item A scalable fleet dispatching algorithm for a robo-taxi fleet.\\
%     Firstly, we adopt a deep RL approach that takes care of both the time heterogeneity of the system and the uncertainties of the future trip arrivals. On the other hand, many ride-hailing literature either model the ride-hailing system as a queueing network or use some mathematical programming approaches to obtain the fleet dispatching strategies. The queueing-based models require the system to be time homogeneous and the mathematical programming approaches assume perfect information about trip arrivals, neither of which align with the realistic ride-hailing systems. Secondly, our algorithm can jointly optimizes the order matching, vehicle repositioning, and the charging scheduling of the fleet, while most of the prior works only consider one or two out of these three aspects. Thirdly, while there is a vast number of literature that employs decentralized RL to tackle with the curse of dimensionality arising from the large fleet size, our model enables the scalability of the algorithm in the centralized manner.
%     \item A theoretical upper bound on the expected revenue.\\
%     Among all ride-hailing literature we came across, only \citep{zhou2023robust} and \citep{singh2024dispatching} provide an upper bound on the performance of the fleet dispatching policies. However, both of these works compute the upper bound by assuming the perfect information of the system (e.g. the number of trip arrivals), and require to solve a MIP for every problem instance with realized demand in their simulations. On the other hand, we provide a fluid based LP that is proved to give an upper bound on the expected revenue achievable by any feasible fleet dispatching policy. Therefore, as long as the mean of trip arrivals remain the same among different simulation trajectories, we only need to solve the LP once in order to obtain the upper bound.
%     \item Insights on the investment of charging facilities and vehicle models.\\
%     Using our fleet dispatching policy, we conduct extensive numerical experiments to provide insights into efficient strategies for charging facility allocation and examine the impact of charger outlet rates and vehicle ranges on revenue. To the best of our knowledge, only \citep{luke2021joint} examines the effective investment of charging facilities and vehicle models based on their fleet dispatching policy.
% \end{enumerate}