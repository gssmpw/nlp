One challenge of computing the optimal control policy lies in the size of the action space $\vert\mathcal{A}\vert$, which grows exponentially with the number of vehicles $N$ and vehicle statuses $|\Cartype|$. As a result, the dimension of policy $\pi$ also grows exponentially with $N$ and $|\Cartype|$. The focus of this section is to address this challenge by introducing a policy reduction scheme, which decomposes the dispatching of a fleet to sequential assignment of tasks to individual vehicles, where the task for each individual vehicle is referred as an ``atomic action". We use the name ``atomic action policy" because each atomic action only changes the status of a single vehicle. In particular, for any vehicle of a status $\cartype{} \in \Cartype$, an atomic action can be any one of the followings: 
\begin{itemize}
    \item[-] $\afulfillred{\triptype{}}$ represents fulfilling a trip of status $\triptype{} \in \Triptype$.
    \item[-] $\areroutered{\destination}$ represents repositioning to destination $\destination \in \Region$.
    \item[-] $\achargered{\type}$ represents charging with rate $\type \in \Type$ at its current region.
    \item[-] $\apassred$ represents idling or continuing with its previously assigned actions. 
\end{itemize}
We use $\hat{\mathcal{A}}$ to denote the atomic action space that includes all of the above atomic actions, i.e. $\hat{a} \in \hat{\mathcal{A}} := \left\{\left(\afulfillred{\triptype{}}\right)_{\triptype{} \in \Triptype}, \left(\areroutered{\destination}\right)_{\destination \in \Region}, \left(\achargered{\type}\right)_{\type \in \Type}, \apassred \right\}$. The atomic action significantly reduces the dimension of the action function since $\hat{\mathcal{A}}$ does not scale with the fleet size or the number of vehicle statuses. 

We now present the procedure of atomic action assignment. In each decision epoch $(t, d)$, vehicles are arbitrarily indexed from $1$ to $\Size$, and are sequentially selected. For a selected vehicle $n$, the atomic policy $\hat{\pi}: \mathcal{S} \times \Cartype \rightarrow \Delta(\hat{\mathcal{A}})$ maps from the tuple of system state $s^{t,d}_n$ before $n$-th assignment and the selected vehicle's status $\cartype{n}$ to a distribution of atomic actions. The system state $s^{t,d}_n$ transitions after every single vehicle assignment with $s_1^{t,d}=s^{t,d}$, and $s_{\Size}^{t,d}$ transitions to $s^{t+1,d}$ after assigning the last vehicle and trip arrival at time $t+1$ is realized.

% To determine the sequence of vehicles receiving atomic action assignments, we rank order all vehicle status based on some pre-specified sequence. We use a counter $m := (m_{\cartype{}})_{\cartype{} \in \Cartype}$ to keep track of the number of vehicles $m_{\cartype{}}$ of each status $\cartype{} \in \Cartype$ that have not yet been assigned in the current decision epoch. We define the augmented state space $\hat{\mathcal{S}}$, where each augmented state $\hat{s} := (s, m) \in \hat{\mathcal{S}}$ stores the information of the current system state $s$ and the counter $m$ that reflects the assignment status of the vehicles. We denote the atomic action policy $\hat{\pi} : \hat{\mathcal{S}} \rightarrow \Delta(\hat{\mathcal{A}})$. Here, we note that the atomic policy $\hat{\pi}$ depends on the time of the day as it is recorded in the system state $s$.

% At the beginning of the decision epoch $(t, d)$, we denote the system state before taking any assignments as $s^{t,d}_1$. We construct the current counter $m^{t,d}_1$ by setting $(m^{t,d}_1)_{\cartype{}} = (s^{t,d}_1)_{\cartype{}}$ for all $\cartype{} \in \Cartype$ to reflect the fact that none of the vehicles have been assigned yet. Then, we augment the current system state $s^{t,d}_1$ with the counter and produce an augmented state $\hat{s}^{t,d}_1 := (s^{t, d}_1, m^{t,d}_1)$. Then, we use the augmented state $\hat{s}^{t,d}_1$ to query the atomic action policy $\hat{\pi}$, which generates an atomic action $\hat{a}^{t, d}_1 \in \hat{\mathcal{A}}$. The atomic action policy assigns $\hat{a}^{t,d}_1$ to a vehicle of status $\cartype{}$ such that $\cartype{}$ is the first vehicle status in the sequence with $m_{\cartype{}} > 0$. Here, the feasibility of an atomic action follows from the model setting in Sec. \ref{sec:model}. For example, a trip of status $\triptype{}$ can only be assigned to a vehicle of status $\cartype{}$ that is associated with high enough battery level and within $\Lp$ of duration of complete from the origin region of the trip. Then, we update the system state from $s^{t,d}_1$ to $s^{t,d}_2$ to reflect the change of status of a vehicle of status $\cartype{}$. Additionally, we subtract one from the entry $m_{\cartype{}}$ to reflect that we have assigned one vehicle of status $\cartype{}$ and we obtain a new counter $m^{t,d}_2$. We denote the procedure of assigning an atomic action to a single vehicle as an ``atomic step". We construct the new augmented state $\hat{s}^{t,d}_2 := (s^{t,d}_2, m^{t,d}_2)$ and we repeat the same procedure. We repeat the process until all entries of $m$ become $0$. Since we have $\Size$ vehicles in the system, there are exactly $\Size$ atomic steps in each decision epoch. After all vehicles have been assigned, the system transitions to the next decision epoch $(t+1,d)$ with new trip arrivals realized. We note that the atomic action policy reduces the action space $\mathcal{A}$, which scales combinatorially with the fleet size and number of vehicle status, to the atomic action space $\hat{\mathcal{A}}$, which is a constant.

The total reward for each decision epoch $(t, d)$ is the sum of all rewards generated from each atomic action assignment in $(t, d)$, where the reward generated by the atomic action $\hat{a}^{t,d} \in \hat{\mathcal{A}}$ is given by
\begin{align*}
    r^t(\hat{a}^{t,d}) =& \sum_{\triptype{} \in \Triptype} r^t_{f, \triptype{}}\mathds{1}\left\{\hat{a}^{t,d} = \afulfillred{\triptype{}}\right\}\\
    +& \sum_{(\origin,\destination) \in \Region \times \Region} r^t_{e, \origin\destination}\mathds{1}\left\{\hat{a}^{t,d} = \areroutered{\destination}\right\}\\ 
    +& \sum_{\delta \in \Delta} r^t_{q, \delta}\mathds{1}\left\{\hat{a}^{t,d} = \achargered{\delta}\right\}.
\end{align*}
The long-run average reward given the atomic action policy $\hat{\pi}$ and the initial state $s \in \mathcal{S}$ is as follows: 
\begin{align*}
    &R(\hat{\pi} \vert s) \\
    =& \lim_{\totaldays \rightarrow \infty} \frac{1}{\totaldays} \mathbb{E}_{\hat{\pi}}\left[ \sum_{\day = 1}^{\totaldays}\sum_{t=1}^T \sum_{n=1}^{N} r^{t}(\hat{a}^{\time, \day}_{n}) \Bigg\vert s \right],\quad \forall s \in \mathcal{S},
\end{align*}
where $\hat{a}_n^{t, d}$ is the atomic actions generated by the atomic action policy in the $n$-th atomic step in decision epoch $(t, d)$. Given any initial state $s \in \mathcal{S}$, our goal is to find the optimal atomic action policy such that $\hat{\pi}^{*} = \argmax_{\hat{\pi}} R(\hat{\pi} \vert s)$. 

Our atomic action policy can be viewed as a reduction of the original fleet dispatching policy in that any realized sequence of atomic actions corresponds to a feasible fleet dispatching action with the same reward of the decision epoch. This reduction makes the training of atomic action policy scalable because the output dimension of atomic action policy $\hat{\pi}^t$ equals to $\vert \hat{\mathcal{A}}\vert$, which is a constant regardless of the fleet size. 

% \subsection{Reduction of Atomic Action Space} \label{subsec:reduced-atomic-policy}
% Even with reduction using atomic actions, the atomic action space $\hat{\mathcal{A}}$ is still large due to the size of vehicle status $\Cartype$. To further reduce the policy dimension, we propose the {\em reduced atomic policy} $\tilde{\pi}: \mathcal{S} \times \Cartype \rightarrow \Delta(\tilde{\mathcal{A}})$ that takes a state $s$ and a vehicle status $\cartype{}$ as inputs and outputs a {\em reduced atomic action} $\tilde{a} \in \tilde{\mathcal{A}} := \left\{\left(\afulfillred{\triptype{}}\right)_{\triptype{} \in \Triptype}, \left(\areroutered{\destination}\right)_{\destination \in \Region}, \left(\achargered{\type}\right)_{\type \in \Type}, \apass \right\}$, where $\afulfillred{\triptype{}}$ is the reduced atomic action to fulfill a trip of type $\triptype{}$, $\areroutered{\destination}$ is the reduced atomic action to reroute to region $\destination$, and $\achargered{\type}$ is the reduced atomic action to charge with rate $\type$.  

% In particular, we rank order all vehicle status based on some pre-specified sequence. At the beginning of each time step $(\time, \day)$, let $s_1^{t}$ be the state before assigning any reduced atomic actions. We record the number of vehicles $s_{1, \cartype{}}^{t}$ of each type $\cartype{} \in \Cartype$. In this time step, we will query each $\cartype{} \in \Cartype$ in sequence according to its index. We obtain a reduced atomic action $\tilde{a} \in \tilde{\mathcal{A}}$ from $\tilde{\pi}$ with the current state and the vehicle status $\cartype{}$ as inputs. We assign a vehicle of type $\cartype{}$ the reduced atomic action $\tilde{a}$ and transition to the next state that reflects the update of this vehicle. In time step $t$, we query each vehicle status $\cartype{}$ for $s_{1, \cartype{}}^{t}$ times in total. If $s_{1, \cartype{}}^{t} = 0$ for some $\cartype{}$, then we skip this vehicle status and proceed to the next one. 

% The reduced atomic policy indicates that given state $s$ and vehicle status $\cartype{}$, any atomic action $\hat{a} \in \hat{\mathcal{A}}$ that assigns a reduced atomic action $\tilde{a} \in \tilde{\mathcal{A}}$ to the vehicle status $\cartype{}$ is equivalent to the pair $(\tilde{a}, \cartype{})$. In particular, for each pair of reduced atomic action $\tilde{a} \in \tilde{\mathcal{A}}$ and a vehicle status $\cartype{}$, we say that the atomic action $\hat{a} \in \hat{\mathcal{A}}$ is {\em equivalent} to $(\tilde{a}, \cartype{})$ if $\tilde{a} = \afulfillred{\triptype{}}$ (resp. $\areroutered{\destination}$, $\achargered{\type}$, $\apass$) and $\hat{a} = \afulfill{\cartype{}, \triptype{}}$ for some $\triptype{} \in \Triptype$ (resp. $\areroute{\cartype{}, \destination}$ for some $\destination \in \Region$, $\acharge{\cartype{}, \type}$ for some $\type \in \Type$, $\apass$). We remark that for any non-passing atomic action $\hat{a} \neq \apass \in \hat{\mathcal{A}}$, there is a unique pair of reduced atomic action $\tilde{a} \in \tilde{\mathcal{A}}$ and a vehicle status $\cartype{} \in \Cartype$ that is equivalent to $\hat{a}$.

% We note that the reduced atomic policy drops the vehicle status from the action space and adds that into the state space. The reduction drops the dimension of the action space by the cardinality of $\Cartype$ from $\vert \hat{\mathcal{A}} \vert$ to $\vert \tilde{\mathcal{A}} \vert$, with the cost of increasing the input dimension by $3$ for storing the information of a single vehicle status. In Sec. \ref{sec:deep-rl}, we will demonstrate that the increase in the input dimension is still computationally tractable by using function approximation. 

% Analogous to the atomic actions, we define the rewards for every pair of reduced atomic action $\tilde{a}^{t} \in \tilde{\mathcal{A}}$ and vehicle status $\cartype{}$ of each time step $t$ as $r^{t}(\tilde{a}^{t}, \cartype{}) = r^{t}(\hat{a}^{t})$, where $\hat{a}^{t} \in \hat{\mathcal{A}}$ is the atomic action that is equivalent to $(\tilde{a}^{t}, \cartype{})$. We can then write the long-run average reward given the reduced atomic action policy $\tilde{\pi}$ and the initial state $s \in \mathcal{S}$ as follows: 
% \begin{align*}
%     R(\tilde{\pi} \vert s) = \lim_{D \to \infty} \frac{1}{D} \mathbb{E}_{\tilde{\pi}}\left[\sum_{d=1}^D \sum_{t=1}^T \sum_{\vehnum=1}^N r^{\time}(\tilde{a}^{\time, \day}_{\vehnum}, \cartype{\vehnum}) \Bigg\lvert s \right],\quad \forall s \in \mathcal{S},
% \end{align*}
% where $\tilde{a}^{\time, \day}_n$ and $\cartype{n}$ are the reduced atomic action and vehicle status associated with the vehicle at the $n$-th atomic step of time step $(\time, \day)$.

% Let $\tilde{R}^*(s)$ be the maximum long-run average reward achievable by reduced atomic policies given the initial state $s \in \mathcal{S}$. Proposition \ref{proposition:reduced-atomic-optimal} demonstrates that this the reduced atomic policy is without loss of optimality.

% \begin{proposition} \label{proposition:reduced-atomic-optimal}
%     $\tilde{R}^*(s) = R^*(s),~\forall s \in \mathcal{S}$.
% \end{proposition}
% \noindent\begin{proof}{Proof of Proposition \ref{proposition:reduced-atomic-optimal}}
%     Recall from the proof of Theorem \ref{thm:atomic-optimal}, we can find an optimal atomic policy $\hat{\pi}^*$ that is a solution of \eqref{eq:orig-atomic-policy-construct-pi} and \eqref{eq:orig-atomic-policy-construct-h-body}. We note that such $\hat{\pi}^*$ may not be unique since the argmax operator in \eqref{eq:orig-atomic-policy-construct-pi} may not return a unique atomic action. We further consider a specific $\hat{\pi}^*$ that is a solution of \eqref{eq:orig-atomic-policy-construct-pi} and \eqref{eq:orig-atomic-policy-construct-h-body}, and chooses a tie breaking rule in \eqref{eq:orig-atomic-policy-construct-pi} that selects the optimal non-passing atomic action that is the one associated with a vehicle status with the smallest index.
    
%     We construct a reduced atomic policy $\tilde{\pi}^*: \mathcal{S} \times \Cartype \rightarrow \Delta(\tilde{A})$ from the atomic policy $\hat{\pi}^*$ as follows: For any pair of state $s \in \mathcal{S}$ and vehicle status $\cartype{} \in \Cartype$, we assign $\tilde{\pi}^*(\tilde{a} \vert s, \cartype{}) := \hat{\pi}^*(\hat{a} \vert s)$ for all non-passing reduced atomic actions $\tilde{a} \neq \apass \in \tilde{\mathcal{A}}$, where $\hat{a} \in \hat{\mathcal{A}}$ is the atomic action equivalent to $(\tilde{a}, \cartype{})$. We assign $\tilde{\pi}^*(\apass \vert s, \cartype{})$ with the remaining probability.
    
%     % and reduced atomic action $\tilde{a} \in \tilde{\mathcal{A}}$, we find the atomic action $\hat{a} \in \hat{\mathcal{A}}$ that is equivalent to $(\tilde{a}, \cartype{})$, then we set
%     % \begin{equation} \label{eq:pihat-2-pitilde}
%     %     \tilde{\pi}^*(\tilde{a} \vert s, \cartype{}) := \begin{cases}
%     %         \hat{\pi}^*(\hat{a} \vert s), & \text{if } \tilde{a} \neq \apass,\\
%     %         1 - \sum_{\tilde{a} \neq \apass} \tilde{\pi}^*(\tilde{a} \vert s, \cartype{}), & \text{if } \tilde{a} = \apass.\\
%     %     \end{cases}
%     % \end{equation}
%     Since $\hat{\pi}^*$ is deterministic, $\tilde{\pi}^*$ is also a deterministic policy. In each time step $t$, let the state before assigning any atomic actions be $s_1^{t}$. We query the atomic policy $\hat{\pi}^*$ with $s_1^{t}$ and obtain an atomic action $\hat{a}_1^{t} \in \hat{\mathcal{A}}$. First, we consider the case where $\hat{a}_1^{t} \neq \apass$. Then, $\hat{\pi}^*$ assigns $\tilde{a}^{t}_1$ to a vehicle of type $\cartype{} \in \Cartype$, where $(\tilde{a}^{t}_1, \cartype{})$ is equivalent to $\hat{a}_1^{t}$, transitions to $s_2^{t}$, and obtains the reward $r^t(\hat{a}_1^{t})$. In the meantime, the reduced atomic policy $\tilde{\pi}^*$ finds the vehicle status with the smallest index that has non-zero number of vehicles, which we labeled as $\cartype{1}$. If $\cartype{1} = \cartype{}$, then $\tilde{\pi}^*$ returns $\tilde{a}^{t}_1$, assigns it to a vehicle of type $\cartype{}$, transitions to $s_2^{t}$, and obtains the reward $r^t(\tilde{a}_1^{t}, \cartype{1}) = r^t(\hat{a}_1^{t})$. Otherwise, due to the tie breaking rule of $\hat{\pi}^*$ in terms of vehicle status, $\tilde{\pi}^*$ will return $\apass$ for all vehicle status whose indices are smaller than $\cartype{}$, and it will keep traversing the list of vehicle status until it hits $\cartype{}$. Repeating the same argument until the atomic action generated by $\hat{\pi}^*$ is $\apass$, we obtain that the sequence of non-passing atomic actions returned by $\hat{\pi}^*$ are equivalent to the sequence of non-passing reduced atomic actions returned by $\tilde{\pi}^*$, given the sequence of vehicle status $\tilde{\pi}^*$ traverses. Therefore, we can obtain the same sequence of state transitions and induced atomic rewards by $\hat{\pi}^*$ and $\tilde{\pi}^*$ in time step $t$. We then can obtain $R(\tilde{\pi}^* \vert s) = R(\hat{\pi}^* \vert s) = R^*(s),~ \forall s \in \mathcal{S}$. Hence, $\tilde{R}^*(s) \geq R^*(s),~\forall s \in \mathcal{S}$.
    
%     % In the meantime, the reduced atomic policy $\tilde{\pi}^*$ finds the first vehicle status in the sequence with non-zero number of vehicles, and we label it as $\cartype{1}$. If $\cartype{1} = \cartype{}$ is the vehicle status associated with the atomic action $\hat{a}_{1}^{t}$, then by \eqref{eq:pihat-2-pitilde}, $\tilde{\pi}^*$ will return a reduced atomic action $\tilde{a}_1 \neq \apass \in \tilde{\mathcal{A}}$ such that $(\tilde{a}_1^{t}, \cartype{1})$ is equivalent to $\hat{a}_1^{t}$. Then, $\tilde{\pi}^*$ assigns the atomic action $\tilde{a}_1^{t}$ a vehicle of type $\cartype{1}$, transitions to $s_2^{t}$, obtains the reward $r^t(\tilde{a}_1^{t}, \cartype{1}) = r^t(\hat{a}_1^{t})$, and subtract one from the counter for vehicle status $\cartype{1}$. On the other hand, if $\cartype{1} \neq \cartype{}$, then by \eqref{eq:pihat-2-pitilde}, $\tilde{\pi}^*$ returns $\apass$ with probability one. Then, it assigns a vehicle of type $\cartype{1}$ with the passing action, remains at state $s_1^{t}$, receives $0$ reward, and subtract one from the counter for vehicle status $\cartype{1}$. $\tilde{\pi}^*$ then finds the first vehicle status $\cartype{2}$ with non-zero vehicles in the new counter and query with inputs $s_1^{t}$ and $\cartype{2}$. It repeats the process until it finds the vehicle status $\cartype{}$ that associated with $\hat{a}_1^{t}$. Due to the tie-breaking rule of $\hat{\pi}^*$, if $\cartype{1}$ precedes $\cartype{}$ in the ordered sequence, then $\hat{\pi}^*$ will not assign non-passing actions to any vehicles of type $\cartype{1}$ in the current time step $t$. When $\hat{a}^{t}_1 = \apass$, both $\hat{\pi}^*$ and $\tilde{\pi}^*$ assign passing to all vehicles and transition to the next time step.
    
%     % Repeating the same argument to the remaining atomic steps in the time step $t$, we obtain that the sequence of non-passing atomic actions returned by $\hat{\pi}^*$ are equivalent to the sequence of non-passing reduced atomic actions returned by $\tilde{\pi}^*$, given the sequence of vehicle status $\tilde{\pi}^*$ traverses. Therefore, we can obtain the same sequence of state transitions and induced atomic rewards by $\hat{\pi}^*$ and $\tilde{\pi}^*$ in time step $t$. We then can obtain $R(\tilde{\pi}^* \vert s) = R(\hat{\pi}^* \vert s) = R^*(s),~ \forall s \in \mathcal{S}$. Hence, $\tilde{R}^*(s) \geq R^*(s),~\forall s \in \mathcal{S}$.

%     The proof of the converse is trivial. Consider any reduced atomic policy $\tilde{\pi}$, the sequential assignment of reduced atomic actions to vehicles in each time step has a feasible action that is equivalent. Therefore, we can construct an original policy $\pi$ that is equivalent to $\tilde{\pi}$. We then can obtain $R(\pi \vert s) = R(\tilde{\pi} \vert s)$, $\forall s \in \mathcal{S}$. Hence, $R^*(s) \geq \tilde{R}^*(s),~ \forall s \in \mathcal{S}$.
    
%     % For any time step $t$, let the state before assigning any atomic actions be $s^{t}$, we can identify the list of all vehicle status with non-zero number of vehicles and we denote them as $\cartype{1}, \dots, \cartype{\Size}$. We roll out the policy $\tilde{\pi}$ for $\Size$ steps and obtain a sequence of reduced atomic actions $\tilde{a}^{t}_1, \dots, \tilde{a}^{t}_{\Size}$. For any sequence of reduced atomic action $\tilde{a}^{t}_1, \dots, \tilde{a}^{t}_{\Size}$ rolled out from $\tilde{\pi}$, we can find a feasible action $a^{t}$ that is induced by sequentially assigning $\tilde{a}^{t}_1, \dots, \tilde{a}^{t}_{\Size}$ to vehicles of types $\cartype{1}, \dots, \cartype{\Size}$. Setting $\pi(a^{t} \vert s^{t})$ with the probability of rolling out the sequence $\tilde{a}^{t}_1, \dots, \tilde{a}^{t}_{\Size}$ by $\tilde{\pi}$ at $s^{t}$, we can obtain $R(\pi \vert s) = R(\tilde{\pi} \vert s)$, $\forall s \in \mathcal{S}$. Hence, $R^*(s) \geq \tilde{R}^*(s),~ \forall s \in \mathcal{S}$.
    
%     % To prove the converse. Consider any policy using concise representation of atomic actions $\tilde{\pi}$. For any time step $t$, given an initial state $s^{t}$, we can identify the list of all vehicle status with non-zero number of vehicles and we denote them as $\cartype{1}, \dots, \cartype{\Size}$. Let $P^{\tilde{\pi}}(\tilde{a}^{t}_1, \dots, \tilde{a}^{t}_{\Size} \vert s^{t})$ denote the probability that the sequence of induced concise atomic actions $(\tilde{a}^{t}_1, \dots, \tilde{a}^{t}_{\Size})$ given the initial state $s^{t}$. I.e. $P^{\tilde{\pi}}(\tilde{a}^{t}_1, \dots, \tilde{a}^{t}_{\Size} \vert s^{t}) = \tilde{\pi}(\tilde{a}^{t}_1 \vert s^{t}_1, \cartype{1}) \cdot \dots \cdot \tilde{\pi}(\tilde{a}^{t}_{\Size} \vert s^{t}_{\Size}, \cartype{\Size})$, where $s^{t}_1, \dots, s^{t}_{\Size}$ is the sequence of states induced by assigning atomic actions to vehicles of types $\cartype{1}, \dots, \cartype{\Size}$ sequentially. We construct the policy $\pi(a^{t} \vert s^{t}) := P^{\tilde{\pi}}(\tilde{a}^{t}_1, \dots, \tilde{a}^{t}_{\Size} \vert s^{t})$ for all $a^{t}$, where $(\tilde{a}^{t}_1, \dots, \tilde{a}^{t}_{\Size})$ is the sequence of concisely represented atomic actions that induces $a^{t}$ by applying them sequentially to vehicles of types $\cartype{1}, \dots, \cartype{\Size}$. It is then obvious that $\sum_{a} r^t(a) \pi(a \vert s) = \sum_{\tilde{a}_1, \dots, \tilde{a}_{\Size}} P^{\tilde{\pi}}(\tilde{a}_1, \dots, \tilde{a}_{\Size} \vert s) \sum_{n = 1}^{\Size} r^t(\tilde{a}_n, \cartype{n})$ and $\sum_{s', a} P(s' \vert s, a) \pi(a \vert s) = \sum_{s', \tilde{a}_1, \dots, \tilde{a}_{\Size}} P(s' \vert s, \tilde{a}_1, \dots, \tilde{a}_{\Size}) P^{\tilde{\pi}}(\tilde{a}_1, \dots, \tilde{a}_{\Size} \vert s)$. Construct $g$ and $h$ w.r.t $\pi$ according to proposition \ref{proposition:fixed-policy-gh}. Then, by proposition \ref{proposition:joint-atomic-reduced-policy-equal-reward}, we can obtain $R(\tilde{\pi} \vert s) = R(\pi \vert s),~ \forall s \in \mathcal{S}$. Hence, $R^*(s) \geq \tilde{R}^*(s),~ \forall s \in \mathcal{S}$.

%     Therefore, we can conclude that $\tilde{R}^*(s) = R^*(s),~\forall s \in \mathcal{S}$.
%     \hfill $\square$
% \end{proof}