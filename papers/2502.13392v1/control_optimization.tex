The adoption of atomic actions has significantly reduced the action dimension. However, the implementation of the MDP is still challenging due to the large state space, which scales significantly with the fleet size, number of regions, and battery discretization. In this section, we provide an efficient algorithm to train the fleet dispatching policy by incorporating our atomic action decomposition into PPO \citep{schulman2017proximal}. To tackle with the large state size, we use neural networks to approximate both the value function and the policy function, to be specified later. We also further reduce the state size in terms of battery discretization and the number of regions by using the following state reduction scheme:

\paragraph{Battery Level Clustering.} We map the state representation of all vehicle statuses into vehicle statuses with aggregated battery levels. We cluster the battery levels into 3 intervals, each of which denotes low battery level $\battery_L$, medium battery level $\battery_M$, and high battery level $\battery_H$, respectively. The cutoff points can be set based on charging rates and criticality of battery levels. It is also possible to cluster the battery levels differently. If computing resources allow, we can cluster the battery levels with finer granularity, e.g. into 10 levels instead of 3. 

\paragraph{Trip Order Status Clustering.} In the state reduction scheme, trip orders are aggregated by recording only the number of requests originating from or arriving at each region, instead of tracking the number of trip requests for each origin-destination pair.
% The current state representation records all trip information, i.e. the number of trip requests between each o-d pair. In the state reduction scheme, we record only the number of trip requests originating from and arriving at each region, and the aggregated trip order state is defined as 
% \begin{enumerate}
%     \item[-] $\state{\text{origin}}{\time} := \left(\state{\text{origin}}{\time}(\region, \tripactivetime) \Biggm\lvert \region \in \Region, \xi \in [\Lc] \right)$, where $\state{\text{origin}}{\time}(\region, \tripactivetime) = \sum_{\destination \in \Region} \state{(\region, \destination, \tripactivetime)}{\time}$ is the total number of trip orders originating from region $\region$ with trip active time $\tripactivetime$.
%     \item[-] $\state{\text{dest}}{\time} := \left(\state{\text{dest}}{\time}(\region, \tripactivetime) \Biggm\lvert \region \in \Region, \xi \in [\Lc] \right)$, where $\state{\text{dest}}{\time}(\region, \tripactivetime) = \sum_{\origin \in \Region} \state{(\origin, \region, \tripactivetime)}{\time}$ is the total number of trip orders whose destinations are in region $\region$ with trip active time $\tripactivetime$.
% \end{enumerate}
The clustering of trip order statuses reduces the state dimension from $O(\vert \Region \vert^2)$ to $O(\vert \Region \vert)$. While it loses some information about the trip distribution, in numerical experiments, we demonstrate that the vehicle dispatching policy trained using our state reduction scheme still achieves a very strong performance (see Section \ref{sec:numerical}). 

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{plots/ppo.png}
    \caption{Atomic-PPO Training Pipeline}
    \label{fig:ppo}
\end{figure}

We denote the state space after the reduction on the original state as $\bar{\mathcal{S}}$, which we refer to as the ``reduced state space". 
%We want to find a randomized atomic control policy $\hat{\pi}: \bar{\mathcal{S}} \rightarrow \Delta(\hat{\mathcal{A}})$ on the reduced state space $\bar{\mathcal{S}}$ that is dependent on the time of day but homogeneous across days. For each training episode, we truncate the horizon to be a finite number of single days when the policy reached stationarity. We use the roll-outs of single days after the policy has reached stationarity to estimate the long run average daily revenue of the policy. The Atomic-PPO is facilitated by two coupled chains. The full chain on the state space $\mathcal{S}$ is adopted to facilitate the transition of the MDP, while the reduced chain on the reduced state space $\bar{\mathcal{S}}$ is employed to find the fleet control policy.
We use neural networks $\hat{\pi}_{\theta}: \bar{\mathcal{S}} \rightarrow \Delta(\hat{\mathcal{A}})$ on the reduced state space to approximate the atomic action policy function, where $\theta$ is the parameter vector for the atomic policy network. 
\begin{algorithm}[h]
    \SetAlgoLined
    \caption{The Atomic-PPO Algorithm} \label{algo:ppo}
    \KwInputs{Number of policy iterations $M$, number of trajectories per policy iteration $K$, number of days per trajectory $D$, initial policy network $\hat{\pi}_{\theta_0}$}
    \For{policy iteration $m = 1, \dots, M$}{
        Run policy $\hat{\pi}_{\theta_{m - 1}}$ for $\totaldays$ days of $\Horizon$ time steps for $K$ trajectories of Monte-Carlo simulations and collect dataset \eqref{eq:ppo-data}.\\
        Construct empirical estimates of long-run average daily reward \eqref{eq:ppo-g}.\\
        Construct empirical estimates of relative value functions \eqref{eq:ppo-v-mc}.\\
        Update relative value network by minimizing the mean-squared norm \eqref{eq:ppo-v-norm}.\\
        Estimate advantage functions by \eqref{eq:advantage}.\\
        Obtain the updated policy network $\hat{\pi}_{\theta_m}$ by maximizing surrogate objective function \eqref{eq:ppo-obj}.
    }
    \Return{policy $\hat{\pi}_{\theta_M}$}
\end{algorithm}
Our Atomic-PPO algorithm (Figure \ref{fig:ppo}) is formally presented in Algo. \ref{algo:ppo}. For each policy iteration $m = 1, \dots, M$, we maintain a copy of the policy neural network parameters $\theta_{m-1}$ from the previous iteration and hold it fixed throughout the iteration. Then, we generate a dataset $\mathrm{Data}^{(K)}_{\theta_{m-1}}$ by rolling out the atomic action policy $\hat{\pi}_{\theta_{m-1}}$ using $K$ trajectories of Monte Carlo simulation. This dataset includes the reduced state $\bar{s}^{t,d, (k)}_{n}$, atomic action $\hat{a}^{t,d, (k)}_{n}$, and atomic reward $r^t(\hat{a}^{t,d, (k)}_{n})$ at $n$-th atomic step of the decision epoch $(t, d)$ of trajectory $k$:
\begin{align} \label{eq:ppo-data}
    &\mathrm{Data}^{(K)}_{\theta_{m-1}} := \nonumber\\ 
    &\left\{\left[\left[\left(\bar{s}^{t, d, (k)}_{n}, \hat{a}^{t, d, (k)}_{n}, r^t(\hat{a}^{t, d, (k)}_{n}) \right)_{n = 1}^{N} \right]_{t = 1}^{\Horizon} \right]_{d=1}^{\totaldays} \right\}_{k = 1}^K,
\end{align}
In each trajectory, we truncate the roll-out to $D$ days, with $T$ time steps in each day. Here, we set $D$ to be a large number that exceeds the days for the system to be stationary given the policy, see Sec. \ref{sec:numerical} for more details. The procedure for the sequential assignment of atomic actions to individual vehicles follows from Section \ref{sec:atomic-action}.
% The PPO algorithm updates the policy in iteration $m$ by maximizing the following objective function: 
% \begin{align}
%     &\hat{L}(\theta_m, \theta_{m-1}) := \frac{1}{K}\sum_{k = 1}^K \sum_{d = 1}^{\totaldays} \sum_{t = 1}^{\Horizon} \sum_{n = 1}^{\Size} \nonumber \\
%     &\qquad \min\left(\frac{\hat{\pi}_{\theta_m}(\hat{a}^{t,d, (k)}_{n} \vert \bar{s}^{t,d, (k)}_{n})}{\hat{\pi}_{\theta_{m-1}}(\hat{a}^{t,d, (k)}_{n} \vert \bar{s}^{t,d, (k)}_{n})} \cdot \right. \nonumber\\
%     &\qquad \left. \hat{A}_{\theta_{m-1}}(\bar{s}^{t,d, (k)}_{n}, \hat{a}^{t,d, (k)}_{n}), \right. \nonumber \\
%     &\qquad \left. \text{clip}\left(\frac{\hat{\pi}_{\theta_m}(\hat{a}^{t,d, (k)}_{n} \vert \bar{s}^{t,d, (k)}_{n})}{\hat{\pi}_{\theta_{m-1}}(\hat{a}^{t,d, (k)}_{n} \vert \bar{s}^{t,d, (k)}_{n})}, 1 - \epsilon, 1 + \epsilon \right) \cdot \right. \nonumber\\
%     &\qquad \left. \hat{A}_{\theta_{m-1}}(\bar{s}^{t,d, (k)}_{n}, \hat{a}^{t,d, (k)}_{n}) \right). \label{eq:ppo-obj}
% \end{align}
% The optimization of the parameterized atomic policy can be approached using gradient ascent methods. The gradient of the reward function with respect to parameter is given by \cite{sutton1999policy}: 
% \begin{align*}
%     &\triangledown_{\theta} R(\hat{\pi}_{\theta} \vert s) = \lim_{D \rightarrow \infty} \frac{1}{D} \mathbb{E}_{\hat{\pi}_{\theta}}\left[\sum_{d = 1}^D \sum_{t = 1}^T \sum_{n = 1}^N \right.\\
%     &\qquad \left. \triangledown_{\theta} \log \hat{\pi}_{\theta}(\hat{a}_n^{t,d} \vert \bar{s}_n^{t,d}) A_{\theta}(\bar{s}_n^{t,d}, \hat{a}_n^{t,d}) \Bigg\lvert s \right],
% \end{align*}where $A_{\theta}(\bar{s}_n^{t,d}, \hat{a}_n^{t,d})$ is the advantage function of the atomic policy $\hat{\pi}_{\theta}$ defined as 
% \begin{align*}
%     &A_{\theta}(\bar{s}_n^{t,d}, \hat{a}_n^{t,d}) = r^t(\hat{a}^{t,d}_n) - \frac{1}{TN} g_{\theta} \\
%     &\quad+ \sum_{s' \in \mathcal{S}} \hat{P}_n(s' \vert s^{t,d}_n, \hat{a}^{t,d}_n) h_{\theta, n+1}(s') - h_{\theta, n}(s^{t,d}_n),
% \end{align*}
% $g_{\theta}$ is the long-run average daily reward achieved by $\hat{\pi}_{\theta}$, and $h_{\theta, n}$ is the relative value function of $\hat{\pi}_{\theta}$ at $n$-th atomic step.
% Directly using the policy gradient method is known to be sample inefficient and unstable due to the high variance from the gradient estimation \citep{marbach2001simulation}.
% %Naive policy updates can lead to instability and divergence, especially when the policy updates are large \.
% To address this, \citep{schulman2017proximal} proposed Proximal Policy Optimization (PPO), which updates the policy by maximizing a clipped objective function 
% $L$ with respect to the current policy network parameters $\theta$, rather than directly applying gradient ascent to update the policy.%trust region policy optimization (TRPO) \citep{schulman2015trust} was introduced to ensure training stability by constraining the policy updates within a trust region, which possesses the monotonic policy improvement guarantee. Building on TRPO, 
% \begin{align}
%     &L(\theta, \theta_{old}) := \lim_{D \rightarrow \infty} \frac{1}{D} \mathbb{E}_{\hat{\pi}_{old}} \left[ \sum_{d = 1}^{\totaldays} \sum_{t = 1}^{\Horizon} \sum_{n = 1}^{\Size} \right. \nonumber \\
%     &\qquad \min\left(\frac{\hat{\pi}_{\theta}(\hat{a}^{t,d}_{n} \vert \bar{s}^{t,d}_{n})}{\hat{\pi}_{\theta_{old}}(\hat{a}^{t,d}_{n} \vert \bar{s}^{t,d}_{n})} \cdot \right. \nonumber\\
%     &\qquad \left. A_{\theta_{old}}(\bar{s}^{t,d}_{n}, \hat{a}^{t,d}_{n}), \right. \nonumber \\
%     &\qquad \left. \text{clip}\left(\frac{\hat{\pi}_{\theta}(\hat{a}^{t,d}_{n} \vert \bar{s}^{t,d}_{n})}{\hat{\pi}_{\theta_{old}}(\hat{a}^{t,d}_{n} \vert \bar{s}^{t,d}_{n})}, 1 - \epsilon, 1 + \epsilon \right) \cdot \right. \nonumber\\
%     &\qquad \left. \left. A_{\theta_{old}}(\bar{s}^{t,d}_{n}, \hat{a}^{t,d}_{n}) \right) \Bigg\lvert s \right], \label{eq:ppo-obj-expected}
% \end{align}
% where the clip function in \eqref{eq:ppo-obj-expected} ensures that the new policy $\hat{\pi}_{\theta}$ will stay close to the old policy $\hat{\pi}_{\theta_{old}}$.
Using the collected data, we construct the empirical estimate $\hat{g}$ of long-run average daily reward using\eqref{eq:ppo-g}\footnote{We assume that the system has a single recurrent class, so the long-run average reward is constant across all initial states.}.
\begin{equation} \label{eq:ppo-g}
    \hat{g} = \frac{1}{KD} \sum_{k = 1}^K \sum_{d = 1}^{\totaldays} \sum_{t = 1}^{\Horizon} \sum_{n = 1}^{\Size} r^t(\hat{a}^{t,d,(k)}_n).
\end{equation}

We also compute the empirical estimate of the relative value function of the current atomic policy $\hat{\pi}_{m-1}$. In particular, we define the relative value function $h_{n, m-1}$ of policy $\hat{\pi}_{m-1}$ at atomic step $n \in [N]$ as:
\begin{align} \label{eq:ppo-v-def}
    &h_{n, m-1}(s) = \mathbb{E}_{\hat{\pi}_{\theta_{m-1}}}\left[\sum_{i = n}^{N} \left(r^t(\hat{a}^{t,d, (k)}_{i}) - \frac{1}{TN}g_{m-1}\right) \right. \notag \\
    &+ \left. \sum_{\ell = t + 1}^{\Horizon} \sum_{i = 1}^{N} \left( r^{\ell}(\hat{a}^{\ell,d, (k)}_{i}) - \frac{1}{TN}g_{m-1} \right) \Bigg\lvert s^{t,1}_n = s \right] \notag \\
    &+ \sum_{d = 2}^{\infty} \sum_{\ell = 1}^{\Horizon} \sum_{i = 1}^{N} \notag\\
    &\qquad \mathbb{E}_{\hat{\pi}_{\theta_{m-1}}}\left[ \left( r^{\ell}(\hat{a}^{\ell,d', (k)}_{i}) - \frac{1}{TN}g_{m-1} \right) \Bigg\lvert s^{t,1}_n = s \right], \notag\\ 
    &\qquad\qquad \forall s \in \mathcal{S},\  \forall t \in [\Horizon],%\\
    % =& \mathbb{E}_{\hat{\pi}_{\theta_{m-1}}}\left[\sum_{i = n}^{N} \left(r^t(\hat{a}^{t,d, (k)}_{i}) - \frac{1}{TN}g_{m-1}\right) \right. \notag \\
    % &+ \left. \sum_{\ell = t + 1}^{\Horizon} \sum_{i = 1}^{N} \left( r^{\ell}(\hat{a}^{\ell,d, (k)}_{i}) - \frac{1}{TN}g_{m-1} \right) \Bigg\lvert s^{t,1}_n = s \right] \notag \\
    % &+ \sum_{\ell = 1}^{\Horizon} \sum_{i = 1}^{N} \sum_{d = 2}^{\infty} \notag\\
    % &\qquad \mathbb{E}_{\hat{\pi}_{\theta_{m-1}}}\left[ \left( r^{\ell}(\hat{a}^{\ell,d', (k)}_{i}) - g_{n, m-1}^t \right) \Bigg\lvert s^{t,1}_n = s \right],\notag \\ 
    % &\qquad \qquad \forall s \in \mathcal{S},\ \forall t \in [\Horizon], \label{eq:ppo-v-def-swapped}
\end{align}
where $g_{m-1}$ is the long-run average daily reward achieved by $\hat{\pi}_{m-1}$ and we recall that the state $s$ contains the time of day $t$ information. By Proposition 2 in our concurrent work \cite{dai2025optimal}, the infinite series in \eqref{eq:ppo-v-def} is well defined. Additionally, we remark that under the atomic action decomposition, our Markov chain has a period of $TN$, which is the total number of atomic steps in each day. Our definition of the relative value function is equivalent to the one defined using the Cesaro limit, as given by Puterman (see page 338 of \cite{PutermanMDP}) for periodic chains, up to an additive constant (see Proposition 3 in \cite{dai2025optimal}).

% where $g_{n, m-1}^t$ is the long-run average reward achieved by $\hat{\pi}_{m-1}$ at $n$-th atomic step and time $t$ across all days, $g_{m-1} := \sum_{t = 1}^T \sum_{n = 1}^N g_{n, m-1}^t$ is the long-run average daily reward achieved by $\hat{\pi}_{m-1}$, and we recall that the state $s$ contains the time of day $t$ information. We remark that under the atomic action decomposition, our Markov chain has a period of $TN$, which is the total number of atomic steps in each day. Hence, for any atomic step $i \in [N]$ and time $t \in [\Horizon]$, the Markov chain induced by the states at the $i$-th atomic step of time $\ell$ of all days is aperiodic. Additionally, with the condition that our state space is finite and the assumption that the system has a single recurrent class, the infinite series in \eqref{eq:ppo-v-def-swapped} converges (see Theorem 1.8.5 on page 44 of \cite{norris1998markov}) and therefore our relative value function given in \eqref{eq:ppo-v-def} is well defined.
For any state $s^{t,d,(k)}_n$ in the atomic step $n$ of trajectory $k$ of decision epoch $(t, d)$, we construct an empirical estimate $\hat{h}^{t,d,(k)}_n$ of its relative value function as:
\begin{align} \label{eq:ppo-v-mc}
    &\hat{h}^{t,d, (k)}_{n} := \sum_{i = n}^{N} \left(r^t(\hat{a}^{t,d, (k)}_{i}) - \frac{1}{TN}\hat{g}\right) \notag \\
    &\qquad+ \sum_{\ell = t + 1}^{\Horizon} \sum_{i = 1}^{N} \left( r^{\ell}(\hat{a}^{\ell,d, (k)}_{i}) - \frac{1}{TN}\hat{g} \right) \notag \\
    &\qquad+ \sum_{d' = d + 1}^{\totaldays} \sum_{\ell = 1}^{\Horizon} \sum_{i = 1}^{N} \left( r^{\ell}(\hat{a}^{\ell,d', (k)}_{i}) - \frac{1}{TN}\hat{g} \right).
\end{align} 
Due to the large state space, we use neural networks $h_{\psi_{m-1}}: \bar{\mathcal{S}} \rightarrow \mathbb{R}$ on the reduced state space to approximate the relative value function for all atomic steps, where $\psi_{m-1}$ is the network parameters. 
We learn $h_{\psi_{m-1}}$ by minimizing the mean-square loss given the empirical estimates:
\begin{equation} \label{eq:ppo-v-norm}
    \sum_{k = 1}^K \sum_{d = 1}^{\totaldays} \sum_{t = 1}^{\Horizon} \sum_{n = 1}^{\Size} \left( h_{\psi_{m-1}}(\bar{s}^{t,d, (k)}_n) - \hat{h}^{t,d, (k)}_n\right)^2.
\end{equation}

This allows us to compute the empirical estimates of the advantage functions for each atomic step. The advantage function quantifies how much better (or worse) a specific atomic action \( \hat{a}^{t,d, (k)}_{n}\) performs compared to following the previous stage policy \( \hat{\pi}_{\theta_{m-1}} \) at a given reduced state \( \bar{s}^{t,d, (k)}_{n}\).
\begin{align} \label{eq:advantage}
    &\hat{A}_{\theta_{m-1}}(\bar{s}^{t,d, (k)}_{n}, \hat{a}^{t,d, (k)}_{n}) := \nonumber\\ 
    &\quad \begin{cases}
        r^t(\hat{a}^{t,d, (k)}_{n}) - \frac{1}{TN}\hat{g} + \\
        h_{\psi_{m-1}}(\bar{s}^{t,d, (k)}_{n + 1}) - h_{\psi_{m-1}}(\bar{s}^{t,d, (k)}_{n}), \\
        \quad \text{if } n < N,\\
        r^t(\hat{a}^{t,d, (k)}_{n}) - \frac{1}{TN}\hat{g} + \\
        h_{\psi_{m-1}}(\bar{s}^{t + 1,d, (k)}_{1}) - h_{\psi_{m-1}}(\bar{s}^{t,d, (k)}_{n}), \\
        \quad \text{if } n = N, t < \Horizon,\\
        r^t(\hat{a}^{t,d, (k)}_{n}) - \frac{1}{TN}\hat{g} + \\
        h_{\psi_{m-1}}(\bar{s}^{1,d+1, (k)}_{1}) - h_{\psi_{m-1}}(\bar{s}^{t,d, (k)}_{n}), \\
        \quad \text{if } n = N, t = \Horizon.\\
    \end{cases}
\end{align}
%where $\hat{g}$ represents the empirical estimate of the long-run average daily reward achieved by the atomic action policy $\hat{\pi}_{\theta_{m-1}}$, and $h_{\psi_{m-1}}$ represents the relative value function of the policy approximated by neural networks.


% \begin{align} \label{eq:ppo-v-mc}
%     &\hat{h}^{t,d, (k)}_{n} := -\frac{1}{T}\hat{g} + \sum_{i = n}^{N} r^t(\hat{a}^{t,d, (k)}_{i}) \notag \\
%     &\qquad+ \sum_{\ell = t + 1}^{\Horizon} \left( - \frac{1}{T}\hat{g} + \sum_{i = 1}^{N} r^{\ell}(\hat{a}^{\ell,d, (k)}_{i}) \right) \notag \\
%     &\qquad+ \sum_{d' = d + 1}^{\totaldays} \sum_{\ell = t + 1}^{\Horizon} \left(-\frac{1}{T}\hat{g} + \sum_{i = 1}^{N} r^{\ell}(\hat{a}^{\ell,d', (k)}_{i}) \right)
% \end{align}

Using the estimated advantage function $\hat{A}_{\theta_{m-1}}$, PPO algorithm select the the atomic action policy function of the next iteration $\hat{\pi}_{\theta_m}$ by choosing parameter $\theta_m$ that maximizes the clipped objective function defined as follows:
\begin{align}
    &\hat{L}(\theta_m, \theta_{m-1}) := \frac{1}{K}\sum_{k = 1}^K \sum_{d = 1}^{\totaldays} \sum_{t = 1}^{\Horizon} \sum_{n = 1}^{\Size} \nonumber \\
    &\qquad \min\left(\frac{\hat{\pi}_{\theta_m}(\hat{a}^{t,d, (k)}_{n} \vert \bar{s}^{t,d, (k)}_{n})}{\hat{\pi}_{\theta_{m-1}}(\hat{a}^{t,d, (k)}_{n} \vert \bar{s}^{t,d, (k)}_{n})} \cdot \right. \nonumber\\
    &\qquad \left. \hat{A}_{\theta_{m-1}}(\bar{s}^{t,d, (k)}_{n}, \hat{a}^{t,d, (k)}_{n}), \right. \nonumber \\
    &\qquad \left. \text{clip}\left(\frac{\hat{\pi}_{\theta_m}(\hat{a}^{t,d, (k)}_{n} \vert \bar{s}^{t,d, (k)}_{n})}{\hat{\pi}_{\theta_{m-1}}(\hat{a}^{t,d, (k)}_{n} \vert \bar{s}^{t,d, (k)}_{n})}, 1 - \epsilon, 1 + \epsilon \right) \cdot \right. \nonumber\\
    &\qquad \left. \hat{A}_{\theta_{m-1}}(\bar{s}^{t,d, (k)}_{n}, \hat{a}^{t,d, (k)}_{n}) \right), \label{eq:ppo-obj}
\end{align}
where $\epsilon>0$ is a hyper parameter referred as the clip size of the training. The PPO policy update, as defined in \eqref{eq:ppo-obj}, was introduced by \citep{schulman2017proximal} to enhance the computational efficiency of trust region policy optimization (TRPO) \cite{schulman2015trust}. TRPO was developed to replace the original policy gradient method \cite{sutton1999policy}, offering the advantage of improved sample efficiency and the monotonic policy improvement guarantee. 
% $L$ with respect to the current policy network parameters $\theta$, rather than directly applying gradient ascent to update the policy.%trust region policy optimization (TRPO) \citep{schulman2015trust} was introduced to ensure training stability by constraining the policy updates within a trust region, which .
% The clip function in our objective function \eqref{eq:ppo-obj} maintains stability of the training by ensuring that the new policy $\hat{\pi}_{\theta_m}$ will stay close to the old policy $\hat{\pi}_{\theta_{m-1}}$. The PPO framework we adopt possesses the stability of trust-region methods \citep{schulman2015trust}, which has monotonic policy improvement guarantee for general stochastic policies.



% Finally, we optimize the atomic action policy function $\hat{\pi}_{\theta_m}$ by maximizing the clipped surrogate objective function \eqref{eq:ppo-obj} w.r.t parameter $\theta_m$ under the PPO framework in \cite{schulman2017proximal}. 
% \begin{align}
%     &\hat{L}(\theta_m, \theta_{m-1}, \mathrm{Data}_{\theta_{m-1}}^{(K)}) := \frac{1}{K}\sum_{k = 1}^K \sum_{d = 1}^{\totaldays} \sum_{t = 1}^{\Horizon} \sum_{n = 1}^{\Size} \nonumber \\
%     &\qquad \min\left(\frac{\hat{\pi}_{\theta_m}(\hat{a}^{t,d, (k)}_{n} \vert \bar{s}^{t,d, (k)}_{n})}{\hat{\pi}_{\theta_{m-1}}(\hat{a}^{t,d, (k)}_{n} \vert \bar{s}^{t,d, (k)}_{n})} \cdot \right. \nonumber\\
%     &\qquad \left. \hat{A}_{\theta_{m-1}}(\bar{s}^{t,d, (k)}_{n}, \hat{a}^{t,d, (k)}_{n}), \right. \nonumber \\
%     &\qquad \left. \text{clip}\left(\frac{\hat{\pi}_{\theta_m}(\hat{a}^{t,d, (k)}_{n} \vert \bar{s}^{t,d, (k)}_{n})}{\hat{\pi}_{\theta_{m-1}}(\hat{a}^{t,d, (k)}_{n} \vert \bar{s}^{t,d, (k)}_{n})}, 1 - \epsilon, 1 + \epsilon \right) \cdot \right. \nonumber\\
%     &\qquad \left. \hat{A}_{\theta_{m-1}}(\bar{s}^{t,d, (k)}_{n}, \hat{a}^{t,d, (k)}_{n}) \right). \label{eq:ppo-obj}
% \end{align}
% In each policy iteration $m$, PPO maintains a copy of the network parameters $\theta_{m-1}$ from the previous iteration and hold it fixed throughout the iteration. In each training epoch of this policy iteration, PPO computes the gradients of the surrogate loss $\hat{L}$ w.r.t the current network $\theta_m$ and then updates $\theta_m$ using stochastic gradient descent or its variants. %It conducts conservative policy updates by using the clip function. It possesses the reliability of trust-region methods \cite{schulman2015trust}, which has monotonic policy improvement guarantee for general stochastic policies.

% The clipping function given $\epsilon \in (0, 1)$ is defined as \[
%     \text{clip}(x, 1 - \epsilon, 1 + \epsilon) := \begin{cases}
%         \min(x, 1 + \epsilon) & \text{If } x \geq 1,\\
%         \max(x, 1 - \epsilon) & \text{If } x < 1.\\
%     \end{cases}
% \]

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{plots/policy_eval.png}
%     \caption{Atomic-PPO Policy Evaluation}
%     \label{fig:policy_eval}
% \end{figure}

%We remark that the transitions of the MDP process is tracked using the full state information (i.e. without aggregation), whereas the inputs to the policy network and the value network are aggregated states. The main storage bottleneck comes from \eqref{eq:ppo-data}, where the number of state transitions scales with the number of episodes, time horizon, and number of vehicles. For each state vector stored in the dataset, we also need to use it to query the value network \eqref{eq:ppo-v-norm}-\eqref{eq:advantage} and the policy network \eqref{eq:ppo-obj}. Therefore, a significant amount of space and runtime can be saved by using aggregated states for network inputs. On the other hand, the MDP transition process can be implemented by updating the state vector on the fly, while we do not have to store any extra snapshot of the full state for the MDP transition. As a result, we can use the full state information to make the MDP transitions consistent, with very minimal space and runtime overhead. 