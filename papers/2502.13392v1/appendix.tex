\section{Neural Network Parameters} \label{sec:appendix-dnn}
We use deep neural networks to approximate the policy function and the value function. Both policy and value networks consist of a list of 3-layer shallow feed-forward networks corresponding to each of the decision epochs. The activation functions of the 3-layer shallow networks for the policy network are tanh, tanh, and tanh. The activation functions of the shallow networks for the value network are tanh, relu, and tanh. 

We adopt a clipping with an exponential decay rate. Specifically, let $\epsilon$ be the initial clipping size, and $\gamma$ be the clipping decay factor, then at the policy iteration $m$, the clipping rate is $\max\{\epsilon \cdot \gamma^m, 0.01\}$. Table \ref{tab:nn-params} specifies the hyperparameters for policy network and value network. 

The policy and value networks are trained using the Adam solver. In each training epoch of a neural network, we randomly select a batch of training data and update the network parameters based on the gradient computed from this batch. %The learning rate has a scheduled decay rate such that it decays by $0.1$ after every $K$ training epochs. A decay rate of $1$ refers to no decays in the learning rate.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        Hyperparameter & Value \\
        \hline
        %Max number of policy iterations & 50 \\
        Number of trajectories per policy iteration & 30 \\%30 \\
        %Number of days per episode & 8\\
        Initial clipping size & 0.1 \\
        Clipping decay factor & 0.97\\
        \hline
        Learning rate for policy network & 5e-4\\
        % Learning decay rate for policy network & 1\\
        % Learning decay schedule for policy network & - \\
        Batch size for policy network & 1024 \\
        Number of epochs for policy network & 20 \\
        \hline
        Learning rate for value network & 3e-4 \\
        % Learning decay rate for value network & 0.1\\
        % Learning decay schedule for value network & 50\\
        Batch size for value network & 1024 \\
        Number of epochs for value network & 100 \\
        \hline
    \end{tabular}
    \caption{Hyperparameters for deep neural networks}
    \label{tab:nn-params}
\end{table}

\section{Fluid Based LP}
\subsection{Proof of Theorem \ref{thm:fluid-obj-val}}
\input{proof_of_fluid_reduced}

\subsection{A formulation with reduced number of variables}
\input{fluid_appendix}