\section{Related Work}
\subsection{Closed Domain Approaches}
Liu et al.~\cite{liu2023robust} devised a system that leverages both domain generalization and domain adaptation techniques to mitigate discrepancies between hidden domains and reduce the modality gap, respectively.
Shalabi et al.~\cite{shalabi2024leveraging} addressed OOC detection by fine-tuning MiniGPT-4's alignment layer but without message generation and confidence scoring. Zhang et al.~\cite{zhang2023detecting} introduced a novel approach to reasoning over (image, caption) pairs. Instead of directly learning patterns from the data distributions, as Liu et al. did~\cite{liu2020visual}, they extract abstract meaning representation graphs from the captions and use them to generate queries for a VLM. This sophisticated approach enables a nuanced consistency check between the visual features of the image and the extracted features of the caption, but with limited room for explainability given by analysis of the generated queries and respective answers of the VLM. The work by Zhang et al.~\cite{zhang2023detecting} made it possible to understand the main limitation of closed-domain approaches to this task: evaluation of the veracity of an (image, caption) pair is sometimes challenging because the image may not depict all the statements that can be extracted from the caption. 

\subsection{Open Domain Approaches}
Popat et al.~\cite{popat2018declare} introduced the concept of detecting textual misinformation using external evidence; however,
in this study evidence is not integrated simultaneously.
Interpretability of the predictions is provided in the form of attention weights over the words of the analyzed document. Abdelnabi et al.~\cite{abdelnabi2022open} extended the concept of leveraging external knowledge for fact-checking to a multimodal (image, text) domain while also computing the aggregated consistencies considering all evidence at the same time for each of the two modalities.
A serious limitation of this approach is the provision of explanations solely in the form of attention scores signaling the most relevant evidence for the purpose of prediction along with limited debunking capabilities. 

Yao et al.~\cite{yao2023end} overcome this limitation by introducing an end-to-end pipeline consisting of evidence retrieval, claim verification, and explanation generation, using a dataset built from fact-checking websites with annotated evidence. A drawback of their approach is the utilization of a large language model (LLM) to summarize the evidence content, potentially overlooking important clues observable in the image.
Two parallel works, ESCNet~\cite{zhang2024escnet} by Zhang et al. and SNIFFER~\cite{qi2024sniffer} by Qi et al., also explore this area. ESCNet lacks explanation generation, while SNIFFER employs a commercial VLM (ChatGPT-4) for generating explanations.

Our work extends the principles outlined above~\cite{abdelnabi2022open, yao2023end} by introducing a module that reasons upon the source pages of the evidence and the generation of a contextualizing explanation as a zero-shot learning task of a VLM.