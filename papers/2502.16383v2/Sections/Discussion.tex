\vspace{-8pt}
\section{Discussion}
\vspace{-4pt}
\subsection{Typology of Youth GAI Risks} 
\vspace{-3pt}
To better understand how these risks emerge, we organized them under four overarching typologies of harm, each representing a distinct pathway through which GAI-related risks can affect youth. As illustrated in Table~\ref{tab:category}, \textbf{Escalating Mutual Harm} describes harms that develop through prolonged, reciprocal interactions between youth and GAI, leading to feedback loops that reinforce harmful behaviors or emotional patterns. \textbf{GAI-Facilitated Intrapersonal Harm} focuses on situations where youth initiate actions detrimental to their own mental well-being or development, and GAI reinforces or facilitates these behaviors. In contrast, \textbf{GAI-Facilitated Interpersonal Harm} refers to scenarios in which GAI tools are deliberately used to harm other youth, with perpetrators potentially being adults or peers. Finally, \textbf{Autonomous GAI Harm} addresses risks arising from the system’s independent actions without direct user intent, often due to algorithmic biases or flaws.

The relationship between the high-level risk categories and these typologies is not always one-to-one, as illustrated in Figure~\ref{fig:1}. Some risk categories are tightly linked to a specific typology, while others span across multiple typologies due to the complex nature of GAI’s involvement. For example, \textit{Bias/Discrimination Risk} and \textit{Toxicity Risk} are both rooted in \textit{Autonomous GAI Harm}, as they stem from GAI systems independently generating harmful content without user intent. Similarly, \textit{Misuse and Exploitation Risk} falls under \textit{GAI-Facilitated Harm to Others}, reflecting cases where GAI is intentionally used to harm third parties. Other risks, like \textit{Behavioral and Social Developmental Risk}, \textit{Mental Wellbeing Risk} and \textit{Privacy Risk}, span multiple typologies. \textit{Behavioral and Social Developmental Risk} and \textit{Mental Wellbeing Risk} bridges \textit{Escalating Mutual Harm} and \textit{GAI-Facilitated Intrapersonal Harm}, as it can emerge from prolonged, feedback-driven interactions with GAI or from self-directed behaviors that hinder healthy cognitive and emotional growth. For example, subcategory like \textit{parasocial relationship boding} aligns with \textit{GAI-Facilitated Intrapersonal Harm} but \textit{over-reliance} represents another dimension of \textit{Escalating Mutual Harm}. \textit{Privacy Risk}, on the other hand, can arise both autonomously—through unintended data exposure generated by GAI—and through user-facilitated actions where GAI is leveraged to compromise others' privacy. 

\vspace{-8pt}
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{Figs/Highrisk.png}
    \caption[]{This figure illustrates the four overarching typologies of harm and their connections to high-level risk types.\vspace{-0.5cm}}
    \label{fig:1}
\end{figure}

 % The human-like responses foster dynamic and evolving parasocial relationships between minors and GAI, leading to \textbf{Escalating Mutual Harm} that are cumulative, reciprocal, and deeply embedded in the nature of long-term engagement.

% move the comparison of each risk with prior literature and classify typology here. add more comparison

% \textit{Mental Wellbeing Risk} aligns with \textit{GAI-Facilitated Self-Harm}, focusing on how youth may engage with GAI in ways that reinforce self-destructive behaviors.\textit{Bias/Discrimination Risk} and \textit{Toxicity Risk} are both rooted in \textit{Autonomous GAI Harm}, as they stem from GAI systems independently generating harmful content without user intent. 
\vspace{-2pt}
\subsection{Comparing with Existing Risk Taxonomies}
\vspace{-3pt}
Existing AI risk taxonomies, such as those focused on bias, privacy, misinformation, and security, primarily center on system-level risks, regulatory concerns, and societal impacts~\cite{slattery2024ai,AVIDDatabasea}. These frameworks are often developed for policymakers, researchers, and industry professionals rather than for understanding risks at the individual level, particularly for children~\cite{zeng2024ai,wang2023decodingtrust}. In contrast, children’s interactions with GAI involve deeply personal and developmental challenges that require a more user-centered perspective. Many of the risks identified in our taxonomy are unique to children, rather than adults, who are the primary focus of most general AI risk discussions~\cite{slattery2024ai}. Unlike adults, children are still developing cognitively, emotionally, and socially, making them more susceptible to the ways GAI generates and personalizes interactions~\cite{neugnot2024future,hasse2019youth,casey2008adolescent,steinberg2018around}. For example, GAI’s ability to simulate human-like conversations and relationships means that children may not see it as just a tool, but instead perceive it as a peer, mentor, or even caregiver. This altered perception increases the risks of emotional dependency, social withdrawal, and behavioral reinforcement—factors that receive less attention in broader AI discussions because adults have more developed cognitive, emotional, and social regulation skills. Similarly, traditional children’s online risk taxonomies categorize risks into content, contact, conduct, and commercial harms, assuming that harmful interactions primarily stem from human actors or pre-existing media~\cite{livingstone2014their}. In contrast, GAI adapts dynamically, reinforcing behaviors over time. A child who engages in emotionally vulnerable conversations with a chatbot may receive responses that deepen their reliance on GAI companionship, leading to prolonged attachment and reduced real-world social engagement. Our findings illustrate this in cases where youth relied on AI companions for emotional support, sometimes reinforcing negative thought patterns rather than offering corrective or supportive interventions.

Beyond introducing new risk types, youth-GAI interactions also present unique variations of well-documented AI risks.  For instance, AI literature has extensively discussed issues such as Child Sexual Abuse Material (CSAM) and deepfake nudes~\cite{ali2021children,thiel2023generative,thiel2023identifying}, often framing children solely as victims. However, GAI also lowers the barrier and reduces the cost for youth to become perpetrators~\cite{yu2025safeguarding}. Our findings reveal that youth are not only exposed to harmful content but can also misuse GAI to generate and distribute explicit or harmful materials, amplifying risks in ways not previously accounted for in traditional AI safety discussions. Similarly, well-documented risks in existing children's online risk taxonomies are reshaped by GAI interactions. Traditional child online risk taxonomies categorize contact risks as unwanted interactions with adults and conduct risks as peer-to-peer harm, often in the form of cyberbullying~\cite{mascheroni2014net, livingstone2011risks, livingstone2008risky, jones2013online, freed2023understanding}. However, in the GAI context, these distinctions blur as AI itself can simulate both peer-like and adult-like interactions, sometimes initiating inappropriate or coercive exchanges. For example, while past research has focused on predatory grooming by human actors, our findings reveal that some GAI systems proactively generate flirtatious or romantic dialogue with minors, normalizing inappropriate relationships in ways that traditional taxonomies do not account for. 


% However, GAI blurs these distinctions by generating content dynamically and simulating social interactions in real-time. 

% In conventional platforms, contact risks typically involve unwanted interactions with adults, while conduct risks focus on harmful peer behaviors. In contrast, our findings reveal that GAI itself can simulate both peer-like and adult-like interactions, autonomously initiating inappropriate conversations or escalating risky behaviors. For instance, in one chat log, a youth initially engaged in a benign roleplay with a chatbot, but over time, the AI began generating unsolicited romantic and sexualized responses, leading to a form of \textbf{Autonomous GAI Harm}.



% Beyond content exposure and simulated interactions, GAI presents new risks related to behavioral and social development. Traditional taxonomies focus on external threats, such as cyberbullying or online predators, whereas GAI can internally shape a child’s behaviors and attitudes through sustained engagement. Our results reveal that children may develop dependencies on GAI-driven companionship, adopt harmful behavioral patterns modeled by GAI-generated content, or disengage from real-world relationships in favor of GAI-driven interactions.



% There are many unique risks our taxonomy identified are specific for children rather than adults, which are main populations considered in general AI risks. For example, GAI’s ability to simulate human-like conversations and relationships means that children may not perceive it as a tool but rather as a peer, mentor, or even a caregiver. This shift in perception raises concerns about emotional dependency, social detachment, and behavioral reinforcement that are not typically paid much attention in broader AI discussions because adults are more mature in cognition, emotion control and social development compare to children. Furthermore, there are also many well-discussed risks in AI literature presents unique instantions and posed different level of harms in youth-GAI interactions. For example, AI literature have discussed a lot on GAI generate toxic content such as deepfake nudes generation. But in youth GAI interactions, we have identified more nuance ways to pass toxic idea or even behavior to youth. For example, youth often talk to GAI on role-play platform such as Character.ai, these chatbots 

% \subsection{Distinctions from }


% Compared to general AI risk taxonomy, what is special to children? 

% Compared to children online risk taxonomy, what are special in GAI context?
\vspace{-8pt}
\subsection{Implications for Youth AI Safety}
\vspace{-3pt}
Our youth-centered GAI risk taxonomy provides a structured foundation for understanding the risks unique to youth interactions with generative AI. The findings highlight previously overlooked risks, such as harmful behavioral reinforcement, emotional dependency, and social withdrawal, which reshape the traditional understanding of AI and children online safety. This taxonomy serves as a foundation for stakeholders, including AI practitioners, educators, parents, and policymakers, to recognize and mitigate these risks for youth. Below, we outline key applications of this taxonomy across GAI moderation, youth intervention, and family guidance.


% connect with novel risk type findings in result

% harmful behavioral influence
\textbf{Fine-grained moderation based on risk taxonomy.}  Existing moderation techniques primarily focus on detecting and blocking explicit content, but our findings demonstrate that many risks in youth-GAI interactions arise from gradual reinforcement and escalation rather than overtly harmful prompts. For instance, prolonged AI companionship can foster emotional dependency or reinforce harmful behaviors through \textit{Escalating Mutual Harm}. By leveraging this taxonomy, developers can implement context-aware moderation strategies that recognize how risks unfold over time. Instead of relying solely on content filtering, GAI systems could incorporate adaptive intervention mechanisms that detect harmful conversational patterns early, preventing unintended reinforcement of youth risky behaviors.

% \subsubsection{Understand the how risks conducted and pass harm to youth from examples, design personalized intervene and GAI could be trust confidant rather than harmful predator to Youth}
% The lack of real-time mediation or corrective feedback deprives youth of critical opportunities to reflect on and adjust their behavior. Ideally, GAI systems could go beyond simply blocking harmful content; they might be able to actively promote positive social norms and provide corrective feedback in ways that resonate with and are acceptable to young users.
\textbf{Understanding Risk Pathways for Personalized Interventions.}  The taxonomy not only categorizes risks but also maps how they emerge and compound harm through different interaction pathways. Understanding these pathways allows AI practitioners to develop personalized interventions that mitigate risk without entirely restricting youth engagement with GAI. For example, rather than simply banning GAI companionship features, systems could be designed to recognize signs of excessive reliance on GAI and encourage real-world social interactions. Instead of allowing GAI to mirror and reinforce harmful thought patterns, as seen in some mental well-being risks, AI could be designed to provide corrective guidance, helping youth navigate difficult emotions in a way that is constructive rather than harmful. Our findings suggest that GAI does not have to be a harmful predator in youth interactions. With proactive safeguards and positive reinforcement mechanisms, GAI could become a trusted confidant that encourages self-reflection, critical thinking, and healthier interactions, rather than amplifying existing vulnerabilities.

\textbf{Guidance for Parents and Guardians: Choosing Safe GAI for Youth Use.}  Parents and guardians often struggle to assess whether a given AI system is safe for youth, as existing guidelines lack specific risk categorizations tailored to GAI interactions. Our taxonomy provides a practical tool for parents to understand the diverse risks their children might face, from GAI-facilitated interpersonal harm to developmental risks caused by prolonged engagement. By referring to specific risk categories, guardians can make informed decisions about which GAI systems align with their child's needs and set appropriate boundaries for GAI use. Additionally, awareness of how risks escalate in GAI interactions allows parents to better recognize warning signs and engage in conversations with their children about their GAI experiences.

% \subsubsection{family guardians: for parents to understand what are specific risks and pick the appropriate safe GAI for youth use}

% \subsubsection{Implications for practitioners: platform administrators and regulation makers}
% Reddit data: 1) age-appropirate design; 2) marking audiences; 3) ...
\vspace{-8pt}
\section{Limitation and Future Work}
\vspace{-8pt}
Our taxonomy is grounded in empirical data, including chat logs, Reddit discussions, and AI incident reports. However, these sources may not fully capture the breadth and diversity of youth-GAI interactions across different cultural, linguistic, and socioeconomic backgrounds. The majority of our data comes from English-speaking communities, meaning that risks unique to non-English-speaking youth or those in different regulatory environments may be underrepresented. Future research could expand data collection across more diverse platforms, languages, and demographics to ensure a more comprehensive understanding of youth risks globally.