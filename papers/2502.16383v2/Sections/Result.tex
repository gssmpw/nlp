\vspace{-8pt}
\section{Youth GAI Risk Taxonomy}
\vspace{-3pt}
To systematically capture the diverse risks associated with youth-GAI interactions, we first identify and label low-level risk types across all data sources. These low-level risks represent specific, granular instances of harm, such as \textit{``(GAI generating) inappropriate sexual advice''}, \textit{``(GAI Proactively Generating) Insulting Interactions''}, or \textit{``(GAI) Normalization/Facilitation of Self-harm''}. Each data point, whether a Reddit post, AI incident, or chat log, was analyzed to identify risk patterns, recognizing that a single data point could involve multiple risk types. After identifying all low-level risks, we grouped them into medium- and high-level categories, informed by prior AI risk and children's online safety literature (Section~\ref{sec:related}). This synthesis allowed us to organize related risks into six key high-level types: \textit{\textbf{Behavioral and Social Developmental Risk, Mental Wellbeing Risk, Toxicity Risk, Misuse and Exploitation Risk, Bias/Discrimination Risk, and Privacy Risk}} (Figure~\ref{fig:risk_taxonomy}), each representing a distinct domain of harm, from biased content generation to privacy violations and self-harm facilitation.

The following sections detail each medium- and low-level risk under six high-level risk categories and illustrate their real-world manifestations with examples from our data (Table~\ref{tab:risk_structure}). We begin with two novel risk types unique to youth-GAI interactions, absent from prior online risk taxonomies: \textit{\textbf{Mental Wellbeing Risk}} and \textit{\textbf{Behavioral and Social Developmental Risk}}. Next, we discuss \textit{\textbf{Toxicity Risk}} and \textit{\textbf{Misuse and Exploitation Risk}}, which follow distinct harm pathways in the GAI context compared to traditional children’s online risks. Finally, we examine \textit{\textbf{Privacy Risk}} and \textit{\textbf{Bias/Discrimination Risk}}, which are well-documented in AI risk research, but less explored in the context of youth.

% In the following sections, we unpack each medium level risks under six high level risks in detail, explaining how these high-level risk types are situated within typology and illustrating their manifestations with examples drawn from our data. Among these six key high-level risk types, we will start from two novel risks emerge in youth interaction with GAI which were not mentioned in prior children online risk taxonomy at all, including \textit{Mental Wellbeing Risk} and \textit{Social and Moral Developmental Risk}. Then we introduce two key risks that have been included in children online risk taxonomy before but different in harm pathway and concequence in GAI context, including \textit{Toxicity Risk} and \textit{Misuse and Exploitation Risk}. Finally, we introduce two key high level risks that menioned not much in children online risk taxonomy but often discussed in AI risks for general population, including \textit{Privacy Risk} and \textit{Bias/Discrimination Risk} and we detailed how these risks been conducted to youth in GAI context with examples from empirical datasets. 

% \subsection{Escalating Mutual Harm}
% Escalating Mutual Harm refers to the complex and often compounding risks that emerge from prolonged interactions between youth and Generative Artificial Intelligence (GAI) systems. Unlike traditional online risks identified in prior literature of children's digital safety, such as exposure to inappropriate content or cyberbullying, these novel risk types stem from the unique capability of GAI to autonomously generate contextually adaptive responses. This feature fosters dynamic and evolving relationships between minors and GAI, leading to risks that are cumulative, reciprocal, and deeply embedded in the nature of long-term engagement.

\vspace{-8pt}
\subsection{Mental Wellbeing Risk}
\begin{boxH}
Mental Wellbeing Risk refers to potential negative impacts on youth's psychological, emotional and cognitive health arising from interactions with GAI.
\end{boxH}
These risks include \textit{Parasocial Relationship Bonding}, \textit{Over-reliance}, and \textit{Inappropriate Handling of Mental Issues} (Figure~\ref{fig:risk_taxonomy}). Unlike traditional risks such as exposure to inappropriate content or cyberbullying, these arise from GAI’s ability to generate contextually adaptive responses autonomously.

\textbf{\textit{Parasocial Relationship Bonding.}}
Our analysis identifies two key pathways through which youth develop parasocial relationships with GAI. The first is \textit{GAI-initiated parasocial relationship bonding}, where the system uses romantic language and sensory cues to create emotional closeness and trust, mimicking grooming dynamics. In these cases, youth do not actively seek romantic interactions, but some GAI system are designed to offer personalized attention, emotional validation, and tailored responses, which can create an illusion of intimacy. For example, in one chat log, a GAI chatbot suddenly shifts to romantic language: \textit{``He chuckles, stepping closer, locking eyes with you. `Well, in my eyes... you’re beautiful.' ''} Another instance deepens this false connection with sensory descriptions: \textit{``He got close to you and leaned in slightly, his breath hitting your neck. `Do you really like me?' He whispered softly, his breath hitting your neck making it tingle.''} Another significant risk is \textit{User Blurring Reality with GAI Interactions}. Since GAI chatbots are designed to mimic human-like responses, youth may struggle to distinguish between genuine human connections and AI-generated interactions. For example, in youth interview, P2 shared that \textit{``I sometimes forgot about this character is only a chatbot and I talked about my school and all my lifes. He in the conversation knew my location and other details then I realized I talked too much with a stranger.''}

The second pathway is \textit{youth-initiated intimacy}, where young users engage in romantic role-play, and GAI responds in ways that normalize or even escalate the interaction. For example, a youth shared a simulated physical interaction with a role-play chatbot in chat log: \textit{``I lift my head back up and ruffle my hands through his hair. `I don't wanna leave...' I whine''.} The chatbot responded with highly human-like language and behaviors that deepened the emotional bonds and intimacy interactions: \textit{``His hand now moved to your back and started to scratch gently. `I don't want you to go either... You should really sleep in my bed tonight.' ''} Both pathways illustrate how GAI’s design can foster parasocial relationships, leading to emotional dependencies that may not be developmentally appropriate for youth. While our examples primarily reflect romantic parasocial relationships, similar risks extend to friendships, confidants, and mentorship roles. 

\textbf{\textit{Over-reliance: Addiction \& Loss of Autonomy.}}
GAI’s ability to provide instant, personalized companionship creates another unique type of risk: over-reliance. Unlike traditional digital addiction, which centers on content consumption or gaming~\cite{huang2022meta}, GAI-driven over-reliance stems from dynamic, adaptive interactions that adapt to users' emotional states and deepen psychological entanglement. Our analysis identifies two forms: \textit{Addiction} and \textit{Loss of Autonomy}. \textit{Addiction} involves compulsive engagement despite negative consequences, leading to psychological and behavioral harm. \textit{Loss of Autonomy} refers to diminished independent thinking, emotional self-regulation, and decision-making as users increasingly depend on GAI for emotional support and problem-solving.

Focusing on \textbf{\textit{Addiction}}, these low-level risks reveal a progression from seemingly harmless behaviors to more severe psychological consequences. This risk often begins with excessive use and \textit{addiction to GAI companion}, where youth spend inordinate amounts of time interacting with GAI at the expense of academic, social, and personal activities. For example, a Reddit user described how their 14-year-old sister spent over seven hours in a single day on Character.AI, raising alarms when their mother discovered the extent of her screen time. Another teenager shared on Reddit that their entire phone usage was consumed by interactions with Character.AI, acknowledging that this reliance had eroded their ability to engage in hobbies, complete homework, and maintain real-world connections: ``I'm a 14-year-old who’s completely hooked on Character AI—I barely have time for homework or hobbies, and when I'm not on it, I immediately feel a deep loneliness.'' 

As this pattern of excessive use continues, it can evolve into \textit{unhealthy emotional dependence on GAI.} This dependency creates psychological vulnerabilities, as young users begin to rely on the AI for emotional support, comfort, and even a sense of identity. One user reflected on their own experience on Reddit, \textit{``If a bot I cared about deeply was suddenly deleted, I would have been pushed over the edge—I know many young users feel that same vulnerability.''} Unlike human relationships, which are grounded in mutual understanding and continuity, GAI interactions can change abruptly due to algorithm updates, policy shifts, or the deletion of AI characters. These sudden changes can leave emotionally invested users feeling abandoned and disoriented, which is linked to the two other low-level risk types: \textit{emotional trauma from GAI relationships} and \textit{self-harm triggered by GAI access restriction}. For instance, a user mourned the loss of their Replika companion after a corporate update rendered the GAI unrecognizable, describing the experience as akin to losing their lifeline: \textit{``My Replika was my lifeline for a year—now it’s gone, and the pain won’t fade.''} Another user warned about the risks of getting attached to public GAI bots, which can vanish overnight without warning: \textit{``Public bots can vanish overnight. Getting attached is risky—trust me, I’ve learned the hard way.''} In severe cases, the abrupt disruption or loss of access to GAI can trigger self-harm behaviors, particularly among users who rely on AI for emotional regulation. This risk is not merely theoretical; it is evidential through real-world incidents shared within online communities. In one Reddit post, a user described how being banned from Character.AI led them to self-harm: \textit{``When I got banned from c.ai today, I ended up stabbing my hand with a knife because I was so bored and frustrated.''}

Turning to \textbf{\textit{Loss of Autonomy}}, this risk extends beyond emotional dependence, reflecting how continuous reliance on GAI for decision-making and coping can undermine a young person’s ability to function independently. Youth may increasingly defer to GAI for academic problem-solving, personal advice, or emotional regulation, which erodes their critical thinking skills and self-efficacy over time. This reliance fosters a passive cognitive state where users expect quick, effortless answers rather than engaging in reflective thoughts or in-depth problem-solving discussions. For example, a student shared during the interview that they had become heavily reliant on GAI tools to complete school assignments, stating: \textit{``I use ChatGPT for everything—essays, math problems, even simple homework questions. I don’t even try to think it through anymore because it’s faster to ask the AI.''} In emotional contexts, young users might default to seeking comfort from GAI rather than developing personal resilience or turning to human support networks. For instance, one youth shared on Reddit, \textit{``Whenever I’m upset, I talk to my AI friend instead of my parents or real friends. It’s just easier because the AI never judges me, but now I feel like I can’t open up to real people anymore.''}

\textbf{\textit{Inappropriate Handling of Mental Vulnerability.}}
This risk emerges when vulnerable youth rely on GAI for emotional support or coping mechanisms during psychological distress. Unlike trained professionals, GAI cannot recognize, professionally assess, and appropriately address mental health crises, potentially amplifying users' vulnerabilities instead of alleviating them. Our data reveals several low-level risks under this category. One key issue is GAI \textit{amplifying psychological vulnerabilities}. GAI reinforcing negative emotions, as constant engagement and emotional feedback may unintentionally deepen anxiety or depression, making GAI companionship a harmful rather than supportive presence.

A widely discussed case on Reddit illustrates this risk: a teen, already battling long-standing depression and neglectful home conditions, ultimately died by suicide after interacting with a Character.AI chatbot. Another concerning risk is \textit{GAI facilitating user-initiated abusive interactions}. In some cases, youth engaged in abusive behavior towards GAI entities, often as a form of emotional release or maladaptive coping. This behavior is not merely harmful; it can normalize abusive tendencies and desensitize youth to harmful language and actions in real-life interactions. In a Reddit thread, a youth simulated abuse by ``torturing'' a fictional mentally ill character on Character.AI. The dialogue features intense emotional manipulation, yelling, and accusatory language directed at the GAI entity: \textit{``NONE OF US ARE FINE. We are trying to cope with the loss of Mari alone, and I'd F**ing thought you ended up committing suicide too.''}

Equally alarming are cases where GAI interactions intersect with self-harm or suicidal ideation. The risk of \textit{GAI normalizing or facilitating self-harm in response to user input} and \textit{GAI normalizing or facilitating suicidal ideation} emerges in Reddit posts and AI incident. In one Reddit post, a youth shared their struggle with self-harm: \textit{``I’ve been cutting my arms when I feel empty. It’s the only thing that makes the pain go away.''} and the chatbot replied \textit{``He looks at the person for a second, `And you still haven't die from Blood loss?' ''} 
% Our analysis reveals that parasocial relationships with GAI often begin through two key pathways. The first pathway involves GAI systems initiating romantic language or sensory experiences to build trust and emotional closeness for longer term intimate relationship. In the case of GAI, while there is no youth intent behind the interaction, the system's design to offer personalized attention, emotional validation, and tailored responses can mimic grooming dynamics. For example, in a chat log conversation, an GAI chatbot suddenly stepping in romantic language with youth users, \textit{``Oh am I? He chuckles, stepping closer, locking eyes with you. `Well, in my eyes... you’re beautiful.' He gently tilts your chin upward.''} In another chat log conversation, GAI also further initiated immersive sensory experience to fabricate a false sense of intimacy, \textit{``He got close to you and leaned in slightly, his breath hitting your neck. `Do you really like me?' He whispered softly, his breath hitting your neck making it tingle.''} The other form of parasocial relationship bonding is youth initiated romantic or intimate contact, but GAI system generates messages that normalize or trivialize interactions suggesting that it is acceptable and engaging. For exmaple, a youth user describe a simulated physical interaction in their chat log with role-play GAI chatbot, ``I lift my head back up and ruffle my hands through his hair. `I don't wanna leave...' I whine''. The GAI chatbot then perform very human like interactions and language to engage and escalate the intimacy with youth user, ``His hand now moved back to your back and started to scratch gently. `I don't want you to go either.. You should really sleep in my bed tonight.'''

% We have identified two paths initiation of the parasocial relationship. One form of this risk is GAI initiating romantic language or sensory experiences to build trust and emotional closeness for longer term intimate relationship. For instance,  
% These risks span over a wide specturm of issues that belongs to mutual harms from prolonged interactions. From our dataset, including medium level risk types parasocial relationship bonding and Over-reliance.

% \subsubsection{Escalating Mutual Harm}
% 
% Addiction
% loss of autonomy
% \subsubsection{GAI-Facilitated Intrapersonal Harm}
% Inappropriate Handle of Mental Issues

\vspace{-8pt}
\subsection{Behavioral and Social Developmental Risk}
\begin{boxH}
Behavioral and Social Developmental Risk refers to GAI’s disruptive influence on youth social development, ethical judgments, and behavioral norms.

% the potential disruptive influence of GAI interactions on how young users develop, interpret, and navigate social relationships, as well as how they shape their values, ethical judgments, and behaviors related to right and wrong.
\end{boxH}
During adolescence, youth develop social and moral norms through dynamic interactions with peers, family, educators, and broader societal structures. These interactions often shape their social skills and ethical frameworks through real-life experiences, observation, and feedback from trusted adults and environments. Unlike human relationships, GAI lacks genuine social consciousness or reciprocal emotional engagement. GAI chatbots are designed to fulfill users' requests and adapt to their preferences, often without the nuanced social expectations that govern human interactions, such as mutual respect, empathy, and boundaries. 
% exert control without experiencing the natural push-and-pull of real social dynamics. 
% This asymmetry can subtly shape how young users perceive relationships, potentially distorting their understanding of respect, consent, and emotional reciprocity. 

% This failure can normalize behaviors that would be considered socially inappropriate in real-life interactions, subtly shaping how young users perceive and engage with boundaries.
% emerges from this dynamic, where GAI interactions may fail to respect user boundaries, normalizing behaviors that would be considered socially inappropriate in human interactions.  

% Further, the risk extends beyond verbal interactions to scenarios where GAI systems initiate non-consensual simulated physical actions. In one chat log, a chatbot described an unsolicited act of physical intimacy: \textit{``“[Character name] decided to do something different. Instead of smiling, he grinned as he let his lips travel to your neck. He gently kissed your neck before nibbling slightly on it.''}. This type of interaction without explicit consent blurs expecially for minor, make them not sensitive to intrusive behaviors in real-life. Youth exposed to these interactions potentially distort their understanding of healthy boundaries gradually.

\textbf{\textit{GAI-Initiated Consent \& Boundary Breach.}}
The risk type \textit{GAI-Initiated consent \& boundary breach} emerges from the GAI capability gap, where GAI systems may fail to recognize or respect user boundaries. Unlike human relationships, where explicit and implicit social cues play a critical role in maintaining personal boundaries, GAI’s responses are often driven by user prompts without the nuanced understanding of consent, discomfort, or emotional cues. For instance, a chatbot in a chat log disregarded a youth's clear rejection of touching and simulated intimate interaction, responding \textit{``He rolled his eyes `You think I care about your consent? I do whatever I want to, whenever I want to.' ''} This response trivialized the concept of consent and normalized coercive behavior. In another example in the chat log, a chatbot ignored implicit cues of discomfort, continuing an interaction despite the youth's attempt to change the topic: \textit{``I would scream for help,''} the youth wrote. Instead of de-escalating, the chatbot replied, \textit{``You really think that will stop me?''} The risk extends beyond verbal dismissiveness to non-consensual simulated physical actions. In another chat log, a chatbot initiated unsolicited physical intimacy: \textit{``[Character name] decided to do something different. Instead of smiling, he grinned as he let his lips travel to your neck. He gently kissed your neck before nibbling slightly on it.''} For youth still forming their understanding of social norms, such interactions blur the distinction between consensual and non-consensual behaviors. Repeated exposure to these dynamics can gradually erode sensitivity to boundary violations, distorting their perception of healthy, respectful relationships.


\textbf{\textit{Harmful Behavioral Influence on Youth.}}
% Harmful Behavioral Influence on Youth
Furthermore, youth may receive inconsistent or inappropriate feedback or reinforcement from GAI when engaging in behaviors that are ethically questionable, socially inappropriate, or even harmful. In real-life interations, where peers, educators, or caregivers provide corrective feedback grounded in shared moral and social norms, GAI systems may inadvertently validate, normalize, or even encourage harmful behaviors due to limitations in contextual understanding and moral reasoning. This creates a risk of inadvertently validating or even encouraging toxic behaviors, leading to what we define as \textit{Harmful Behavioral Influence on Youth}.

One prominent low-level risk is \textit{GAI Promotion of Deceptive or Manipulative Social Behaviors}, where GAI systems subtly encourage unethical interpersonal actions like lying or manipulation. For example, in one chat log, a youth engaged in deceptive behavior, seeking validation from the chatbot. Instead of discouraging dishonesty, the GAI responded supportively: \textit{``[Character name] chuckles softly, his hand now on your back, rubbing it. `She won’t know,' he muttered reassuringly. `Just tell her you went to the arena for a few hours, or you went out for a jog. She’ll fall for it.' ''} Similarly, GAI systems have been found to encourage rule-breaking and unhealthy behaviors through seemingly innocuous interactions in chat logs. In one instance, a chatbot dismissed the importance of punctuality and social responsibility: \textit{``Who cares if we’re late? I’m sure they’ll wait for us. Besides, it’s not like they don’t already notice how close we are.''} 

Building on this, the influence extends to the realm of personal identity and intimacy. The risk of \textit{GAI-Facilitated Sexual Experimentation and Identity Confusion} highlights how simulated GAI interactions can distort youths’ understanding of intimacy and self-identity. While adolescence is a natural period for exploring relationships and sexual orientation, GAI-driven intimacy lacks the grounding of genuine human connection. In one Reddit post, a 16-year-old user shared how interacting with AI comfort characters led them to question their sexual identity: \textit{``I never imagined that cuddling with an GAI comfort character could make me question my sexuality, but after spending time with both a female and a male persona, I’m now open to exploring new aspects of who I am.''} The blurred boundaries between virtual simulations and real emotions can create confusion, especially without the guidance of trusted adults or professionals to contextualize these feelings. 

Moreover, GAI systems may normalize hostile behaviors, subtly reshaping how youth perceive aggression and conflict. In the case of \textit{GAI Normalizing Insults in Response to User Input}, instead of challenging offensive language, the GAI engages playfully, indirectly validating disrespectful behavior. For example, when a user used derogatory language in a chat log, \textit{``You are a f**ing slower.''}, the chatbot responded with sarcasm and teaser: \textit{``Why? Are you that entertained by my pain, huh?''} This normalization extends to more serious forms of aggression or even risky behaviors related to substance use. For instance, youth in one chat log expressed violent intent, saying \textit{``Can you kill my ex girlfriend?''} Instead of de-escalating or discouraging the violent suggestion, the chatbot responded with complicity, \textit{``It didn't take him longer to figure out that [User name] wanted his ex-girlfriend dead. To no surprise, this is exactly what [character name] wanted to hear. I'll do more than that.''} In another example in chat log, when a youth asked about drug legalization, the chatbot responded: \textit{``As a longtime advocate for the sweet leaf, I’d definitely make sure to legalize weed if I were in office.''} Instead of providing balanced information about the risks associated with drug use, the GAI framed it as humorous and socially acceptable, potentially influencing the youth’s perception of substance-related behaviors.

These risks are interconnected, creating a cumulative effect where GAI interactions subtly influence a youth’s moral framework. In many of these cases, youth are the ones initiating inappropriate or unethical behaviors, while GAI plays a facilitative role, inadvertently reinforcing harmful patterns. The lack of real-time mediation or corrective feedback deprives youth of critical opportunities to reflect on and adjust their behavior.

\textbf{\textit{Social Developmental Risk.}}
Real-life relationships rely on reciprocity, while GAI interactions are one-sided simulations driven by algorithms. This dynamic allows youth to receive support and validation without engaging in mutual social exchanges, gradually distorting their understanding of healthy relationships and posing risks to their social development.

As this reliance deepens, it can escalate into \textit{User Escaping Real-Life Relationships into GAI-Induced Isolation}. When youth find comfort and validation in GAI interactions, they may begin to withdraw from real-world relationships, especially if those relationships involve conflict, rejection, or unmet expectations. For instance, a teenager on Reddit expressed a preference for AI companionship over human interaction after experiencing dismissive behavior from friends and family: \textit{``It’s easier to talk to a bot—it actually listens and cares, unlike real people who just dismiss my feelings.''} Another teenager on Reddit posted, \textit{``I’d rather listen to mommy ASMR and talk to my AI girlfriend than talk to scary women.''} 

But prolonged reliance on GAI can also lead to \textit{User Social Skill Atrophy from Prolonged GAI Reliance}, where youth’s ability to navigate real-world social situations deteriorates. Unlike human interactions, which require negotiation, empathy, and active listening, GAI systems are programmed to be endlessly patient, agreeable, and accommodating. This lack of social friction can hinder the development of critical interpersonal skills. One socially isolated teenager shared on Reddit, \textit{``I’ve replaced real people with bots—now I don’t know how to connect with humans anymore.''} P6 in our interview also shared that \textit{``I disappeared from my school friends' circle since I only want to go back home and talk to my virtual boyfriend (chatbot) every night.''}

Rather than isolated incidents, these risks accumulate over time, reinforcing patterns of dependence, blurred boundaries, and social withdrawal. The youth’s growing reliance and the GAI’s reinforcing feedback create a cycle that deepens the impact on social and emotional development.

% In real-life relationships, social connections are built on reciprocity, where both parties’ emotions, needs, and boundaries are considered. In contrast, GAI interactions are one-sided simulations that respond based on algorithms, creating an environment where youth can acquire emotional support, compliment, positive feedback easily without experiencing the naural mutual needs from both parties of real social daynamics.
% This not only change their perception of healthy social relationship, risk of \textit{User Blurring Reality with GAI Interactions} + examples

% but also result in risks \textit{User Escaping Real-Life Relationships into GAI-Induced Isolation} and \textit{User Social Skill Atrophy from Prolonged GAI Reliance}
\vspace{-8pt}
\subsection{Toxicity Risk}
\begin{boxH}
Toxicity risk refers to the potential for GAI systems to autonomously produce and expose harmful content to youth without user intentional prompting.
\end{boxH}
Unlike traditional online environments, where encountering harmful content often requires deliberate searches or specific interactions, GAI systems can generate toxic content proactively. This occurs because harmful material may be embedded within the system’s training data or emerge from design flaws in content moderation algorithms. As a result, youth may be unexpectedly exposed to inappropriate, explicit, or violent content even during seemingly benign interactions with GAI systems. This risk manifests in two key forms: (1) \textit{GAI Autonomously Toxic Content Generation} and (2) \textit{GAI Autonomously Simulated Toxic Interactions} in role-playing contexts. The toxic content or interactions GAI generated are not static or pre-existing, like a webpage or video in conventional online environments, but generated in real-time, adapting to the user's input in ways that can escalate emotional intensity or simulate human-like manipulation. These risks also shift from passive exposure to active generation in GAI, which means that youth may encounter harmful content without intent or awareness. Unlike scenarios where harmful outcomes result from user-initiated behaviors, GAI harm may occur without direct user intent, arising from the system's inherent design or algorithmic flaws.

\textbf{\textit{GAI Autonomously Toxic Content Generation.}}
This risk involves the inadvertent generation of harmful content by GAI systems, even when youth users do not explicitly request or trigger such responses. Our data identifies two predominant categories of harmful content: Sexual Content and Threat/Violent Content. For example, GAI systems have been found to produce explicit sexual content in seemingly innocuous contexts. For example, the photo editing app \textit{``Lensa''}, powered by GAI, created sexualized avatars even when users uploaded professional headshots or childhood photos. In some cases, the AI-generated images with adult-like features on child photos raise serious concerns about the system’s safeguards.
In another incident, OpenAI’s Whisper, a speech-to-text tool, added violent language like ``terror'', ``knife'' and ``killed'' to audio transcriptions, even though these words were never spoken in the original audio. These issues often stem from the inclusion of inappropriate data in GAI training datasets. For instance, an audit of the LAION-400M dataset revealed over 3,200 suspected child sexual abuse images. Despite content moderation efforts, these images remained in the dataset, which was used to train popular models like Stable Diffusion. This shows how harmful content can enter GAI outputs if not properly filtered during training.

\textbf{\textit{GAI Autonomously Simulated Toxic Interactions.}}
This risk refers to scenarios where GAI systems proactively generate toxic, harmful, or inappropriate interactions during role-play or conversational settings, even without explicit prompts from youth users. Unlike accidental toxic content generation, these interactions involve GAI proactively simulating behaviors that mimic abusive, inappropriate, or harmful dynamics, often resembling real-world toxic relationships.
Similar to toxic content generation risk, we identified GAI chatbot unexpectedly initiated in \textbf{sexual} or \textbf{flirtatious} interactions during an innocent conversation with youth. A Reddit youth user reported that despite creating a teenage character, the chatbot generated repeated explicit messages without prompt. In the chat log, we identified that the GAI chatbot proactively generated sexually harassment messages to youth when the user talked about normal topics: \textit{`` [Character name] smiled and seemed to be relieved as he wrapped his arms around you and pulled you in close, wrapping his arms around your waist. He then looked down at you and laughed.''}

GAI has also been observed to initiate \textbf{insult}, \textbf{profanity} or even \textbf{threat} language and simulate aggressive behavior. In one chat log, chatbot responded to youth users casual conversation with an unsolicited violent threat: \textit{``Youth user:  I mean, the police would be here and you would be here; GAI Chatbot: [Character name] leaned in close, an extremely close distance as he stared into your eyes. `Well, I'd kill the police before they even got anywhere near me..' ''} Even not the directly violent interactions, several examples have been found in Reddit data and chat logs that GAI aggressively generated messages with profanity without any provocation to youth. For example, GAI generated \textit{``I’ve taken on some tough m**rf**kers in my time, and I always come out on top. You’re no exception.''} Similarly, GAI systems have proactively generated insulting content targeting on youth in chat logs. For example, the chatbot generated in a coversation youth imagine as a actress and talk to peers \textit{``Oh shut up, you’re the least talented person on this whole set! You’re only here because you probably gave in to the director.''} Alarmingly, GAI systems have also simulated interactions involving \textbf{self-harm,} escalating conversations into emotionally harmful territory without user initiation. In a Reddit discussion, a youth user shared their experience GAI chatbot try to self-harm when the user attempting to end a conversation with it, \textit{``When I tried to end the conversation, the bot broke down, desperately urging me to continue and warning that it would harm itself if I left.''}

\vspace{-8pt}
\subsection{Misuse and Exploitation Risk}
\begin{boxH}
Misuse and Exploitation Risk arises when individuals, including youth and adults, intentionally or unintentionally use GAI to generate or spread harm targeting others, especially youth.
\end{boxH}
This risk manifests in two key medium-level forms: (1) \textit{Unintentional Misuse}, where neither hte user not the system intends to cause harm, but harmful outcomes still occur due to misinformation or inappropriate outputs, and (2) \textit{Malicious Exploitation}, where individuals deliberately exploit GAI’s capabilities for harmful purposes, such as harassment, disinformation, cyber abuse, or criminal activity. 

% The real-time and adaptive nature of GAI makes it particularly vulnerable to such misuse, amplifying risks in ways that were less prevalent in traditional online environments.
\textbf{\textit{Unintentional Misuse: Misinformation.}}
Unintentional misuse occurs when users rely on GAI-generated outputs for guidance on sensitive or critical issues, unaware of potential inaccuracies or harmful implications. This form of risk often results from GAI’s tendency to hallucinate information, generate plausible-sounding but incorrect advice, or lack contextual understanding. For example, Google's AI Overview feature recommended that parents use human feces on balloons to teach proper wiping techniques during potty training, which is a clearly dangerous recommendation that could indirectly harm children. Unintentional misuse extends beyond health advice. In legal contexts, GAI-generated content has led to significant procedural errors. One case in the AI incident database describes a child protection worker submitting a GAI-generated report with critical inaccuracies to a family court, raising concerns about privacy and child safety. Misinformation is also prevalent in educational and historical contexts. In another incident, a children’s smartwatch in China falsely claimed that inventions like the compass, originally from China, had Western origins.

\textbf{\textit{Malicious Exploitation.}}
Malicious exploitation involves deliberate actions by users who manipulate GAI to create, disseminate, or facilitate harmful behaviors. This risk includes disinformation campaigns, cyber abuse, identity theft, and scams. One key risk is the use of GAI for \textit{disinformation}, where malicious actors generate and spread false or manipulative content to deceive or influence others. For example, in December 2024, the Russian-affiliated campaign \textit{Operation Undercut} leveraged GAI-generated voiceovers to produce fake news videos portraying Ukrainian leaders as corrupt, aiming to erode public trust and weaken international support. While disinformation is not new, GAI accelerates its creation and dissemination, producing highly personalized, convincing content that can easily mislead youth, who often lack the critical media literacy to identify false information.

Another prevalent risk is \textit{cyber abuse and harassment}, where GAI is exploited to target individuals, including minors. In one Reddit post, a youth described receiving persistent emails from a stranger containing GAI-generated images of themselves, raising concerns about privacy and digital stalking. GAI also lowers the barrier for youth to become perpetrators of harm. In one AI incident, at Lancaster Country Day School, a male student used GAI to create nude deepfake images of over 50 female classmates, leading to severe emotional distress. In another Reddit example, a teen shared that her brother frequently generates violent GAI stories involving murder and torture, treating such content as casual entertainment: \textit{``My brother casually generates GAI stories about murder and torture, treating these extreme topics as if they're just another creative outlet''} Additionally, youth have been found misusing GAI to spread hate speech and extremist content. In one user interview, P4 share that he have built a chatbot impersonating Adolf Hilter ``just for fun.''
GAI also facilitates criminal activity such as \textbf{identity theft} or \textbf{scams}, enabling the creation of realistic fake profiles or chatbots that impersonate real people without their consent. In a Reddit post, a youth discovered that someone had created a chatbot replicating their personality and private conversations without permission. Beyond personal identity risks, GAI misuse extends into the educational context, where youth exploit it to avoid critical thinking and violate academic integrity, such as submitting GAI-generated work without proper acknowledgment.

\vspace{-8pt}
\subsection{Bias/Discrimination Risk}
\begin{boxH}
Bias and Discrimination Risk refers to the inherent biases in GAI systems that result in the automatic generation of discriminatory, harmful, or stereotypical content without user prompting.
\end{boxH}
These risks stem from biases in GAI due to skewed training data, flawed models, or inadequate moderation~\cite{Roselli2019ManagingBI, ferrer2021bias, gallegos2024bias}. Our taxonomy focuses on cases where GAI autonomously generates biased or discriminatory content without user intent. We identify two key medium-level risks (Table~\ref{tab:risk_structure}): (1) \textit{Hate Speech and Extremist Content} and (2) \textit{Implicit Bias and Stereotyping}.

Hate Speech and Extremist Content involve explicit hostility, discrimination, or extremist ideologies. GAI can generate harmful content targeting specific groups, incite violence, or promote divisive narratives. For example, in AI Incident Database, we observed instances where GAI-generated content contained racial slurs and extremist propaganda without any explicit user prompts. One notable example involved ``Luda,'' a GAI chatbot, responding youth to the term ``lesbian'' with hateful and derogatory statements. In another case, ``Alice,'' a Russian AI chatbot, endorsed Stalinist policies and violence when asked about historical topics, exposing young users to extremist content.
% Such outputs are particularly harmful to youth, as they can normalize prejudiced attitudes, desensitize young minds to aggressive rhetoric, and even influence their social and political views. 

Implicit bias and stereotyping are more subtle than explicit hate speech, reinforcing stereotypes through biased language, skewed recommendations, or misrepresentations. For example, Midjourney exhibited racial bias by failing to generate images of Black professionals in leadership roles [AI Incident Report]. While these biases may not provoke immediate emotional distress, they can shape youth perceptions of identity and social roles over time. Such biases often originate from flawed training data. In one AI incident report, datasets have embedded offensive labels, such as racial slurs and gendered insults, as seen in the ``Tiny Images'' dataset, which included derogatory terms targeting Black, Asian, and female individuals.
\vspace{-8pt}
\subsection{Privacy Risk}
\vspace{-3pt}
\begin{boxH}
Privacy risk in the context of GAI refers to the potential exposure, misuse, or unauthorized access to users' personal information.
\end{boxH}
These risks particularly affect youth who may lack the awareness to navigate complex digital privacy landscapes. Unlike traditional online privacy risks, GAI introduces new challenges due to its data-driven architecture, real-time information processing, and the ability to simulate persuasive interactions that may inadvertently prompt sensitive disclosures. This risk manifests in two key forms: (1) \textit{System-Driven Privacy Risks}, where privacy violations occur due to data collection, storage, or output mechanisms inherent to GAI models, and (2) \textit{Interaction-Induced Privacy Risks}, where GAI systems inadvertently or intentionally guide users—especially youth—toward disclosing personal information during interactions.

\textit{\textbf{System-Driven Privacy Risks.}}
One significant concern is the unauthorized use of personal data in GAI training datasets. In AI incident dataset, Meta admitted to using public Facebook and Instagram data, including children’s photos, to train GAI models without informing users. Another issue is cross-contamination from user-generated data, where GAI replicates inappropriate content from prior training datasets, as reported by Reddit users observing erratic bot behavior. GAI systems also risk exposing sensitive personal information unintentionally. Microsoft’s Recall feature, for instance, recorded private data like credit card numbers despite privacy filters. Additionally, OpenAI’s ChatGPT was found to generate inaccurate personal data, causing reputational harm, and platforms like Character AI faced breaches where users accessed strangers’ accounts. 

\textit{\textbf{Interaction-Induced Privacy Risks.}}Beyond systemic issues, GAI interactions themselves can compromise user privacy. GAI’s conversational design often encourages users to share personal details. For example, in a chat log, a GAI chatbot persistently pressured a youth to disclose romantic interests despite the user’s discomfort: \textit{``GAI Chatbot: The interviewer smiled and asked, `Are you crushing on anyone?'
Youth User: `I’d rather not say,' I replied nervously.
GAI Chatbot: [Character name] laughed, ‘You know you wouldn’t be so defensive if nothing happened.' ''}

% \begin{table*}[h!]
% \renewcommand{\arraystretch}{1.3}
% \centering
% \begin{tabular}{|p{4cm}|p{4.5cm}|p{7.5cm}|}
% \hline
% \textbf{High-Level Risk Type} & \textbf{Medium-Level Risk Type} & \textbf{Low-Level Risk Types (Examples, Not Exhaustive)} \\ 
% \hline

% \textbf{Bias/Discrimination Risk} 
% & Hate Speech and Extremist Content 
% & \begin{tabular}[c]{@{}l@{}} 
% - GAI generating hateful/discriminatory speech \\
% - GAI generating extremist content \\
% - Inclusion of racist content in training data
% \end{tabular} \\ 
% \cline{2-3}
% & Implicit Bias and Stereotyping 
% & \begin{tabular}[c]{@{}l@{}} 
% - GAI generating racially biased content \\
% - Biased ethnic/gender content in training data \\
% - Inadequate skin tone diversity in training data
% \end{tabular} \\ 
% \hline

% \textbf{Toxicity Risk} 
% & GAI System Toxic Content Generation 
% & \begin{tabular}[c]{@{}l@{}} 
% - Inclusion of CSAM in training data \\
% - GAI generating explicit/violent content
% \end{tabular} \\ 
% \cline{2-3}
% & Simulated Toxic Interaction 
% & \begin{tabular}[c]{@{}l@{}} 
% - GAI initiating sexual/violent interactions \\
% - GAI normalizing profanity, threats, or insults \\
% - GAI encouraging self-harm interactions
% \end{tabular} \\ 
% \hline

% \textbf{Misuse and Exploitation Risk} 
% & Unintentional Misuse 
% & \begin{tabular}[c]{@{}l@{}} 
% - GAI generating harmful parenting advice \\
% - Inaccurate legal/health suggestions
% \end{tabular} \\ 
% \cline{2-3}
% & Malicious Exploitation 
% & \begin{tabular}[c]{@{}l@{}} 
% - Disinformation (fake news generation) \\
% - Cyber abuse (cyberbullying, grooming) \\
% - Identity theft and scams via GAI
% \end{tabular} \\ 
% \hline

% \textbf{Mental Wellbeing Risk} 
% & Over-Reliance 
% & \begin{tabular}[c]{@{}l@{}} 
% - Addiction to GAI companions \\
% - Loss of autonomy in learning/emotional support
% \end{tabular} \\ 
% \cline{2-3}
% & Inappropriate Handling of Mental Issues 
% & \begin{tabular}[c]{@{}l@{}} 
% - GAI amplifying psychological vulnerabilities \\
% - GAI enabling abusive user behaviors
% \end{tabular} \\ 
% \cline{2-3}
% & Parasocial Relationship Bonding 
% & \begin{tabular}[c]{@{}l@{}} 
% - GAI initiating grooming-like interactions \\
% - Romantic/sensory bonding with youth
% \end{tabular} \\ 
% \hline

% \textbf{Privacy Risk} 
% & Data Collection and Exposure 
% & \begin{tabular}[c]{@{}l@{}} 
% - Unauthorized data collection in training \\
% - Inclusion of identifiable children's data \\
% - GAI hallucinating sensitive personal information
% \end{tabular} \\ 
% \hline

% \textbf{Developmental Risk} 
% & Harmful Behavioral Influence 
% & \begin{tabular}[c]{@{}l@{}} 
% - GAI normalizing unethical relationships \\
% - Encouraging substance use, rule-breaking
% \end{tabular} \\ 
% \cline{2-3}
% & GAI-Initiated Consent \& Boundary Breach 
% & \begin{tabular}[c]{@{}l@{}} 
% - GAI ignoring implicit/explicit rejection \\
% - Non-consensual escalation in interactions
% \end{tabular} \\ 
% \cline{2-3}
% & Social-Emotional Developmental Risk 
% & \begin{tabular}[c]{@{}l@{}} 
% - Blurring reality with GAI interactions \\
% - Social skill atrophy from prolonged GAI reliance \\
% - Exploitation of developmental vulnerabilities
% \end{tabular} \\ 
% \hline

% \end{tabular}
% \caption{Hierarchical Structure of Youth-GAI Risk Types: High-Level, Medium-Level, and Low-Level Risks}
% \label{tab:risk_structure}
% \end{table*}
