\section{Related Work}
\label{sec:related}


\textbf{Image Tokenization}. 
Imgae tokenization aims at transforming the high-dimension images into more compact and structured latent representations. 
% It has served as a cornerstone in numerous applications of modern computer vision, such as latent diffusion models ____ and multi-modality models ____.
Early explorations mainly used autoencoders ____, which learn latent codes reduced dimensionality. 
These foundations soon inspired methods with variational posteriors,
% that balance detail-preserving compression with the ability to encapsulate complex visual concepts, 
such as VAEs ____ and VQ-GAN ____. 
% They introduced new ways to structure latent spaces. 
% By imposing priors or quantization constraints, these tokenizers yielded representations that can facilitate more efficient generation and editing processes. 
Recent work has further improved compression fidelity and scalability ____, showing the importance of latent structure.
% that preserves both low-level details and high-level semantics.
% Meanwhile, the growing large-scale pre-training and vision-language modeling ____ has further encouraged the alignment of latent representations with rich semantic signals. 
% For example, tokens derived from text-aligned embeddings enable more intuitive manipulation and classification ____. 
More recent efforts have shown methods that bridge high-fidelity reconstruction and semantic understanding within a single tokenizer ____. 
Complementary to them, we further highlight the importance of discriminative latent space, which allows us to use a simple AE yet achieve better generation. 



\textbf{Image Generation}.  
The paradigms of image generation 
%can be broadly divided into 
mainly categorize to autoregressive and diffusion models.
Autoregressive models initially relied on CNN architectures ____ and were later augmented with Transformer-based models ____ for improved scalability ____. 
% The latest models are designed in an autoregressive fashion on a scale that further improves efficiency .
% including VAR ____, MAR ____ further enhance efficiency and image quality by optimizing factorized likelihoods and leveraging large-scale training.
Diffusion models show strong performance since their debut ____. 
Key developments ____ refined the denoising process for sharper samples. 
A pivotal step in performance and efficiency came with latent diffusion ____, which uses tokenizers to reduce dimension and conduct denoising in a compact latent space ____. 
% Subsequent improvements, such as Transformer-based backbones ____, have broadened the range of achievable image resolutions and enabled more flexible conditioning schemes. 
Recent advances include designing better tokenizers ____
% , such as the proposed \method, 
and combining diffusion with autoregressive models ____. 

% These methodological strides have fueled a diverse spectrum of applications, from text-guided image generation ____ to photorealistic editing ____, culminating in commercial systems ____ that highlight diffusion’s practical impact. As diffusion models continue to evolve, understanding the role of latent spaces—particularly how they balance semantic structure and fidelity—has emerged as a central challenge in large-scale image generation research.