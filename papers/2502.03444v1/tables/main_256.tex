\setlength{\tabcolsep}{4pt}
\begin{table*}[t!]
\centering

\resizebox{0.85\linewidth}{!}{%
\begin{tabular}{@{}l c | c c c c | c c | c c@{}}
\toprule
\multirow{2}{*}{Model (G)} &
\multirow{2}{*}{\# Params (G)} &
\multirow{2}{*}{Model (T)} &
\multirow{2}{*}{\# Params (T)} &
\multirow{2}{*}{\# Tokens $\downarrow$} &
\multirow{2}{*}{rFID $\downarrow$} &
\multicolumn{2}{c|}{w/o CFG} &
\multicolumn{2}{c}{w/ CFG} \\
& & & & & & gFID $\downarrow$ & IS $\uparrow$ & gFID $\downarrow$ & IS $\uparrow$ \\
\toprule
\multicolumn{10}{l}{\textit{Auto-regressive}\vspace{0.02in}} \\
\pz\pz VQGAN \cite{esser2021taming} & 1.4B & VQ & 23M  & 256 & 7.94 & -- & -- & 5.20 & 290.3  \\
\pz\pz ViT-VQGAN \cite{yu2021vector} & 1.7B & VQ & 64M  & 1024 & 1.28 & 4.17 & 175.1 & -- & -- \\
\pz\pz RQ-Trans. \cite{lee2022autoregressive} & 3.8B & RQ & 66M & 256 & 3.20 & --  & -- & 3.80 & 323.7  \\
\pz\pz MaskGIT \cite{chang2022maskgitmaskedgenerativeimage} & 227M & VQ & 66M & 256 & 2.28 & 6.18 & 182.1 &  -- & -- \\
% \pz\pz MAGE \cite{li2023magemaskedgenerativeencoder} & 439M & VQ &  & 256 & -- & 6.93 & 195.8 & -- & -- \\
\pz\pz LlamaGen-3B \cite{sun2024autoregressive} & 3.1B & VQ & 72M & 576 & 2.19 & -- & -- & 2.18 & 263.3 \\
\pz\pz TiTok-S-128 \cite{yu2024an} & 287M & VQ & 72M & 128 & 1.61 & -- & -- & 1.97 & 281.8 \\
\pz\pz VAR \cite{tian2024visualautoregressivemodelingscalable} & 2B & MSRQ$^\dagger$ & 109M & 680 & 0.90 & -- & -- & 1.92  & 323.1  \\
\pz\pz ImageFolder \cite{li2024imagefolder} & 362M & MSRQ & 176M & 286 & 0.80 & -- & -- & 2.60  & 295.0 \\
\pz\pz MAGVIT-v2 \cite{yu2024language}  & 307M & LFQ & 116M & 256 & 1.61 & 3.07 & 213.1 & 1.78  & 319.4 \\
\pz\pz MaskBit \cite{weber2024maskbit} & 305M & LFQ & 54M & 256 & 1.61 & -- & -- & 1.52  & 328.6 \\
\pz\pz MAR-H \cite{li2024autoregressiveimagegenerationvector} & 943M & KL & 66M & 256 & 1.22 & 2.35 & 227.8 & 1.55 & 303.7 \\
\arrayrulecolor{gray}\cmidrule(lr){1-10}
\multicolumn{10}{l}{\textit{Diffusion-based}\vspace{0.02in}} \\
\pz\pz LDM-4 \cite{rombach2022highresolutionimagesynthesislatent} & 400M & KL$^\dagger$ & 55M & 4096 & 0.27 & 10.56 & 103.5 & 3.60 & 247.7 \\

\pz\pz U-ViT-H/2 \cite{bao2023all} 
 & 501M & \multirow{5}{*}{KL$^\dagger$} & \multirow{5}{*}{84M} & \multirow{5}{*}{1024} & \multirow{5}{*}{0.62}
 & -- & -- & 2.29 & 263.9 \\

\pz\pz MDTv2-XL/2 \cite{gao2023mdtv2} 
 & 676M &  & & & 
 & 5.06 & 155.6 & 1.58 & 314.7 \\

\pz\pz DiT-XL/2 \cite{peebles2023scalablediffusionmodelstransformers} 
 & 675M & & & & 
 & 9.62 & 121.5 & 2.27 & 278.2 \\

\pz\pz SiT-XL/2 \cite{ma2024sit} 
 & \multirow{2}{*}{675M}  & & & & 
 & 8.30 & 131.7 & 2.06 & 270.3 \\

\pz\pz\pz + REPA \cite{yu2024representation}
 &  & & & &
 & 5.90 & 157.8 & 1.42 & 305.7 \\

\pz\pz TexTok-256 \cite{zha2024language} 
 & 675M & KL & 176M & 256 & 0.69
 & -- & -- & 1.46 & 303.1 \\

\pz\pz LightningDiT \cite{yao2025reconstruction} 
 & 675M & KL & 70M & 256 & 0.28
 & 2.17 & 205.6 & 1.35 & 295.3 \\
\arrayrulecolor{gray}\cmidrule(lr){1-10}
\multicolumn{10}{l}{\textit{Ours}\vspace{0.02in}} \\
\grayrow
\pz\pz MAETok + LightningDiT
 & 675M &  &  &  & 
 & 2.21 & 208.3  & 1.73  & 308.4 \\
\grayrow
\pz\pz MAETok + SiT-XL 
 & 675M & \multirow{-2}{*}{AE} & \multirow{-2}{*}{176M} & \multirow{-2}{*}{\textbf{128}} & \multirow{-2}{*}{0.48}
 & 2.31 & 216.5  & 1.67  & 311.2 \\

\bottomrule


\end{tabular}%
}
\vspace{-0.1in}
\caption{System-level comparison on ImageNet 256$\times$256 conditional generation. 
SiT-XL and LightningDiT trained on \method achieves performance comparable to state-of-the-art using plain AE with only 128 tokens. 
``Model (G)'': the generation model. ``\# Params (G)'': the number of generator's parameters. ``Model (T)'': the tokenizer model.
``\# Params (T)``: the number of tokenizer's parameters.
``\# Tokens": the number of latent tokens used during generation. $^\dagger$ indicates that the model has been trained on other data than ImageNet. 
}
\label{tab:main_256}
\vspace{-0.1in}
\end{table*}




