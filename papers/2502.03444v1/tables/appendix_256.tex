\setlength{\tabcolsep}{4pt}
\begin{table*}[h]
\centering

\resizebox{0.98\linewidth}{!}{%
\begin{tabular}{@{}l c | c c c c | c c c c | c c c c@{}}
\toprule
\multirow{2}{*}{Model (G)} &
\multirow{2}{*}{\# Params (G)} &
\multirow{2}{*}{Model (T)} &
\multirow{2}{*}{\# Params (T)} &
\multirow{2}{*}{\# Tokens $\downarrow$} &
\multirow{2}{*}{rFID $\downarrow$} &
\multicolumn{4}{c|}{w/o CFG} &
\multicolumn{4}{c}{w/ CFG} \\
\cmidrule(lr){7-10}\cmidrule(lr){11-14}
& & & & & & gFID $\downarrow$ & IS $\uparrow$ & Prec $\uparrow$ & Recall $\uparrow$ 
  & gFID $\downarrow$ & IS $\uparrow$ & Prec $\uparrow$ & Recall $\uparrow$ \\
\toprule
\multicolumn{14}{l}{\textit{Auto-regressive}\vspace{0.02in}} \\
\pz\pz VQGAN \cite{esser2021taming} 
  & 1.4B 
  & VQ 
  & 23M  
  & 256 
  & 7.94 
  & -- & -- & -- & --
  & 5.20 & 290.3 & -- & --
  \\
\pz\pz ViT-VQGAN \cite{yu2021vector} 
  & 1.7B 
  & VQ 
  & 64M  
  & 1024 
  & 1.28 
  & 4.17 & 175.1 & -- & --
  & -- & -- & -- & --
  \\
\pz\pz RQ-Trans. \cite{lee2022autoregressive} 
  & 3.8B 
  & RQ 
  & 66M 
  & 256 
  & 3.20 
  & -- & -- & -- & --
  & 3.80 & 323.7 & -- & --
  \\
\pz\pz MaskGIT \cite{chang2022maskgitmaskedgenerativeimage} 
  & 227M 
  & VQ 
  & 66M 
  & 256 
  & 2.28 
  & 6.18 & 182.1 & 0.80 & 0.51
  & -- & -- & -- & --
  \\
\pz\pz MAGE \cite{li2023magemaskedgenerativeencoder} 
  & 439M 
  & VQ 
  & (N/A) 
  & 256 
  & -- 
  & 6.93 & 195.8 & -- & --
  & -- & -- & -- & --
  \\
\pz\pz LlamaGen-3B \cite{sun2024autoregressive} 
  & 3.1B 
  & VQ 
  & 72M 
  & 576 
  & 2.19 
  & -- & -- & -- & --
  & 2.18 & 263.3 & 0.80 & 0.58
  \\
\pz\pz TiTok-S-128 \cite{yu2024an} 
  & 287M 
  & VQ 
  & 72M 
  & 128 
  & 1.61 
  & -- & -- & -- & --
  & 1.97 & 281.8 & -- & --
  \\
\pz\pz VAR \cite{tian2024visualautoregressivemodelingscalable} 
  & 2B 
  & MSRQ$^\dagger$ 
  & 109M 
  & 680 
  & 0.90 
  & -- & -- & -- & --
  & 1.92 & 323.1 & 0.82 & 0.60
  \\
\pz\pz ImageFolder \cite{li2024imagefolder} 
  & 362M 
  & MSRQ 
  & 176M 
  & 286 
  & 0.80 
  & -- & -- & -- & --
  & 1.92 & 323.1 & 0.75 & 0.63
  \\
\pz\pz MAGVIT-v2 \cite{yu2024language}  
  & 307M 
  & LFQ 
  & 116M 
  & 256 
  & 1.61 
  & 3.07 & 213.1 & -- & --
  & 1.78 & 319.4 & -- & --
  \\
\pz\pz MaskBit \cite{weber2024maskbit} 
  & 305M 
  & LFQ 
  & 54M 
  & 256 
  & 1.61 
  & -- & -- & -- & --
  & 1.52 & 328.6 & -- & --
  \\
\pz\pz MAR-H \cite{li2024autoregressiveimagegenerationvector} 
  & 943M 
  & KL 
  & 66M 
  & 256 
  & 1.22 
  & 2.35 & 227.8 & 0.79 & 0.62
  & 1.55 & 303.7 & 0.81 & 0.62
  \\
\cmidrule(lr){1-14}
\multicolumn{14}{l}{\textit{Diffusion-based}\vspace{0.02in}} \\
\pz\pz LDM-4 \cite{rombach2022highresolutionimagesynthesislatent} 
  & 400M 
  & KL$^\dagger$ 
  & 55M 
  & 4096 
  & 0.27 
  & 10.56 & 103.5 & 0.71 & 0.62
  & 3.60 & 247.7 & 0.87 & 0.48
  \\

\pz\pz U-ViT-H/2 \cite{bao2023all} 
  & 501M 
  & \multirow{5}{*}{KL$^\dagger$}
  & \multirow{5}{*}{84M} 
  & \multirow{5}{*}{1024} 
  & \multirow{5}{*}{0.62}
  & -- & -- & -- & --
  & 2.29 & 263.9 & 0.82 & 0.57
  \\

\pz\pz MDTv2-XL/2 \cite{gao2023mdtv2} 
  & 676M 
  & 
  & 
  & 
  & 
  & 5.06 & 155.6 & 0.72 & 0.66
  & 1.58 & 314.7 & 0.79 & 0.65
  \\

\pz\pz DiT-XL/2 \cite{peebles2023scalablediffusionmodelstransformers} 
  & 675M 
  &
  &
  &
  &
  & 9.62 & 121.5 & 0.67 & 0.67
  & 2.27 & 278.2 & 0.83 & 0.53
  \\

\pz\pz SiT-XL/2 \cite{ma2024sit} 
  & \multirow{2}{*}{675M} 
  &
  &
  &
  &
  & 8.30 & 131.7 & 0.68 & 0.67
  & 2.06 & 270.3 & 0.82 & 0.59
  \\

\pz\pz\pz + REPA \cite{yu2024representation}
  &  
  & 
  & 
  & 
  & 
  & 5.90 & 157.8 & 0.70 & 0.69
  & 1.42 & 305.7 & 0.80 & 0.65
  \\

\pz\pz TexTok-256 \cite{zha2024language} 
  & 675M 
  & KL 
  & 176M 
  & 256 
  & 0.69
  & -- & -- & -- & --
  & 1.46 & 303.1 & 0.79 & 0.64
  \\

\pz\pz LightningDiT \cite{yao2025reconstruction} 
  & 675M 
  & KL$^\dagger$ 
  & 70M 
  & 256 
  & 0.28
  & 2.17 & 205.6 & -- & --
  & 1.35 & 295.3 & -- & --
  \\
\cmidrule(lr){1-14}
\multicolumn{14}{l}{\textit{Ours}\vspace{0.02in}} \\
\rowcolor{gray!10}
\pz\pz MAETok + LightningDiT
  & 675M 
  &  
  &  
  &  
  &  
  & 2.21 & 208.3 & 0.79 & 0.62
  & 1.73 & 308.4 & 0.80 & 0.63
  \\
\rowcolor{gray!10}
\pz\pz MAETok + SiT-XL 
  & 675M 
  & \multirow{-2}{*}{AE} 
  & \multirow{-2}{*}{176M} 
  & \multirow{-2}{*}{\textbf{128}} 
  & \multirow{-2}{*}{0.48}
  & 2.31 & 216.5 & 0.78 & 0.62
  & 1.62 & 310.6 & 0.81 & 0.63
  \\

\bottomrule
\end{tabular}%
}
\caption{System-level comparison on ImageNet 256$\times$256 conditional generation, now also reporting Precision and Recall under both CFG and no-CFG settings. 
``Model (G)'': generation model. 
``\# Params (G)'': the number of generator parameters. 
``Model (T)'': the tokenizer model. 
``\# Params (T)``: the number of tokenizer parameters. 
``\# Tokens": the number of latent tokens used during generation. 
$^\dagger$ indicates that the model has also been trained on data beyond ImageNet.}
\label{tab:appendix_256}
\end{table*}
