\appendix
\onecolumn

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{appendix_vis/appendix_512.pdf}
    \caption{Additional selected samples from 512$\times$512 SiT-XL model on MAETok. We use a classifier-free guidance scale of 2.0.}
    \label{fig:appendix_more_512}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{appendix_vis/appendix_256.pdf}
    \caption{Additional selected samples from 256$\times$256 diffusion models on MAETok. We use a classifier-free guidance scale of 2.0.}
    \label{fig:appendix_more_256}
\end{figure}

\section{Theoretical Analysis}
\label{app:theory-analysis}
\paragraph{Preliminary.}We begin the theoretical analysis by introducing the preliminaries of the problem and the necessary notation.

Following the empirical analysis setting, we first consider the latent data distribution is the GMM with $K$ equally weighted Gaussians:
\begin{align}
\label{eq:gmm}
    p_0 = \frac{1}{K} \sum_{i=1}^{K} \mathcal{N}(\boldsymbol{{\mu}}^*_i, \mathbf{I}),
\end{align}
Following the the training objective \cite{shah2023learning}, we consider  the score matching loss of DDPM at timestep $t$ is
\begin{align}
\label{eq:training}
  \min_{\mathbf w} \mathbb E[\|s_{\mathbf w}(\mathbf x,t)-\nabla_{\mathbf x} \log p_t(\mathbf x)\|^2] 
\end{align}
where $s_{\mathbf w}(\mathbf x,t)$ is the denoising network and $\log p_t(\mathbf x)$ is the oracle score. Under the GMM assumption, the explicit solution of score function $\nabla_{\mathbf x} \log p_t(\mathbf x)$ can be written as
\begin{align}
\label{eq:score}
    \nabla_{\mathbf x} \ln p_t(\mathbf x) = \sum_{i=1}^K w_{i,t}^*(\mathbf x) \boldsymbol{{\mu}}_{i,t}^* - \mathbf x,
\end{align}
where the weighting parameter is 
\begin{align}
    w_{i,t}^*(\mathbf x) := \frac{\exp(-\|\mathbf x - \boldsymbol{{\mu}}_{i,t}^*\|^2 / 2)}{\sum_{j=1}^K \exp(-\|\mathbf x - \boldsymbol{{\mu}}_{j,t}^*\|^2 / 2)}, \quad \boldsymbol{{\mu}}^*_{i,t} := \boldsymbol{{\mu}}^*_i \exp(-t).
\end{align}
Therefore, we can consider the denosing neural network with the following format, that is
\begin{align}
\label{eq:denosing}
    s_{\theta_t}(\mathbf x) = \sum_{i=1}^K w_{i,t}(\mathbf x) \boldsymbol{{\mu}}_{i,t} - \mathbf x,  
\end{align}
where
\begin{align}
    w_{i,t}(\mathbf x) := \frac{\exp(-\|\mathbf x - \boldsymbol{{\mu}}_{i,t}\|^2 / 2)}{\sum_{j=1}^K \exp(-\|\mathbf x - \boldsymbol{{\mu}}_{j,t}\|^2 / 2)}, \quad \boldsymbol{{\mu}}_{i,t} := \boldsymbol{{\mu}}_i \exp(-t).
\end{align}

\paragraph{Assumptions.}To ensure the denoising network approximates the score function with sufficient accuracy, we consider the following three common assumptions, which constrain the training process from the perspectives of data quality (separability), good initialization (warm start), and regularity (bounded mean of target distribution) \cite{chen2022sampling,chen2023improved,benton2024nearly}.
\begin{assumption}
\label{assum:1}
(Separation Assumption  in~\cite{shah2023learning})
 For a mixture of $K$ Gaussians given by Equation \ref{eq:gmm}, for every pair of components $i, j \in \{1, 2, \ldots, K\}$ with $i \neq j$, we assume that the separation between their means
\begin{align}
    \|\boldsymbol{{\mu}}^*_i - \boldsymbol{{\mu}}^*_j\| \geq C \sqrt{\log(\min(K, d))}
\end{align}
for sufficiently large absolute constant $C > 0$.
\end{assumption}
\begin{assumption}
\label{assum:2}
(Initialization Assumption  in~\cite{shah2023learning})
 For each component $i \in \{1, 2, \ldots, K\}$, we have an initialization $\boldsymbol{{\mu}}_i^{(0)}$ with the property that
\begin{align}
    \|\boldsymbol{{\mu}}_i^{(0)} - \boldsymbol{{\mu}}^*_i\| \leq C' \sqrt{\log(\min(K, d))}
\end{align}
for sufficiently small absolute constant $C' > 0$.
\end{assumption}
\begin{assumption}
\label{assum:3}
The maximum mean norm of the GMM in GMM~\ref{eq:gmm} is bounded as: \(\max_i \|\boldsymbol{{\mu}}_i\| \leq B\).
\end{assumption}

\begin{remark}
By Assumption \ref{assum:3}, we could derive the second movement bound  of $p_0$ as
\begin{align}
    \mathbb{E}_{\mathbf x\sim p_0}[\|\mathbf x\|^2] = \int p_0(\mathbf x) \|\mathbf x\|^2 \mathrm{d} \mathbf x \le d+B^2
\end{align}
\end{remark} 
Then, we can have the following analysis,
\paragraph{Step 1: From $K$ Modes to Training Loss.}The main conclusion required for our proof is derived from the following theorem, which provides the estimation error $\|\boldsymbol{\mu}_i - \boldsymbol{\mu}_i^*\|$ for DDPM with gradient descent under $\mathcal{O}(1)$-level noise, assuming that Assumptions \ref{assum:1} and \ref{assum:2} are satisfied.
\begin{theorem}
\label{app-theorem:16}
(Adopted from Theorem 16  in~\citet{shah2023learning})
Let $q$ be a mixture of Gaussians in \cref{eq:gmm} with center parameters $\theta^* = \{\boldsymbol{\mu}_1^*, \boldsymbol{\mu}_2^*, \ldots, \boldsymbol{\mu}_K^*\} \in \mathbb{R}^d$ satisfying the separation \ref{assum:1}, and suppose we have estimates $\theta$ for the centers such that the warm initialization Assumption \ref{assum:2} is satisfied. For any $\varepsilon > \varepsilon_0$ and noise scale $t$ where
\begin{align*}
    \varepsilon_0 = 1/\text{poly}(d) \quad t = \Theta(\varepsilon),
\end{align*}
gradient descent on the DDPM objective at noise scale $t$ outputs $\tilde{\theta} = \{\tilde{\boldsymbol{\mu}}_1, \tilde{\boldsymbol{\mu}}_2, \ldots, \tilde{\boldsymbol{\mu}}_K\}$ such that $\min_i \|\tilde{\boldsymbol{\mu}}_i - \boldsymbol{\mu}_i^*\| \leq \varepsilon$ with high probability. DDPM runs for $H \geq H'$ iterations and uses $n \geq n'$ number of samples where
\begin{align*}
    H' = \Theta(\log(\varepsilon^{-1} \log d)),\quad  n' = \Theta(K^4d^5B^6/\varepsilon^2).
\end{align*}
\end{theorem}
Theorem \ref{app-theorem:16} indicates that, to achieve the same estimation error $\epsilon$, a data distribution with more modes requires more training samples. \cref{fig:norm bound} demonstrates that different latent spaces exhibit nearly identical mean norm upper bounds, thus justifying our focus on analyzing the number of modes $K$.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/gmm_maxmean.pdf}
    \caption{We compare the maximum mean norm across different numbers of components and observe that AE, VAE, VAVAE, and our method MAETok exhibit similar maximum mean norms. This suggests that these latent spaces share a comparable prior upper bound \( B \), supporting the rationale for primarily considering the number of modes, i.e., $K$ in~\cref{theorem:2.2}.}
    \label{fig:norm bound} 
\end{figure}

Given $\epsilon$ in Theorem \ref{app-theorem:16} and based on \cref{assum:3}, \cref{eq:training}, \cref{eq:score}, \cref{eq:denosing}, we further have
\begin{align}
    \mathbb{E}[\|s_{\theta_t}(\mathbf x_t) - \nabla_{\mathbf x_t} \log p_t(\mathbf x_t)\|^2]
    &= \mathbb{E}\Big[\Big\|\sum_{i=1}^K \big(w_{i,t}(\mathbf x_t) \boldsymbol{{\mu}}_{i,t} - w_{i,t}^*(\mathbf x_t) \boldsymbol{{\mu}}_{i,t}^*\big)\Big\|^2\Big] \notag\\
    &\leq 2\mathbb{E}\Big[\Big\|\sum_{i=1}^K w_{i,t}^*(\mathbf x_t) (\boldsymbol{{\mu}}_{i,t} - \boldsymbol{{\mu}}_{i,t}^*)\Big\|^2\Big] 
    + 2\mathbb{E}\Big[\Big\|\sum_{i=1}^K (w_{i,t}(\mathbf x_t) - w_{i,t}^*(\mathbf x_t)) \boldsymbol{{\mu}}_{i,t}\Big\|^2\Big] \notag\\
    & \lesssim e^{-2t}(\epsilon^2 + B^2)
\end{align}
The $\lesssim$ hides constant term $2$ and $4$.

Therefore, consider a step size $h_k \le \gamma$, we can have the learned score function $s_{\theta_t}(\mathbf x)$ satisfies
\begin{align}
\label{eq:learned score function}
    \frac{1}{T} \sum^N_{k=1} h_k \mathbb{E}[\|s_{\theta_{t_k}}(\mathbf x_{t_k}) - \nabla_{\mathbf x_{t_k}} \log p_t(\mathbf x_{t_k})\|^2]  \lesssim \frac{N\gamma}{T}(\epsilon^2 + B^2)
\end{align}

\paragraph{Step 2: From Training Loss to Samlping Error.}
In the practical sampling process, we adopt an early stopping strategy to improve the generation quality. Specifically, we consider the interval $t \in [0,0.8]$ during the reverse process. Then, the following conclusion holds:
\begin{theorem}
\label{app-theorem:2.2-o}
(Theorem 2.2.  in~\citep{chen2023improved})
There is a universal constant \(C\) such that the following hold. Suppose that \cref{assum:3} and \cref{eq:learned score function} hold and the step sizes satisfy the following for some quantities $\sigma_{t_1}^2,\dots,\sigma_{t_k}^2,\dots,\sigma_{t_N}^2$,
\begin{align}
    \frac{h_k}{\sigma_{t_{k-1}}^2} \leq \frac{1}{Cd} \le \gamma, \quad k = 1, \ldots, N
\end{align}
Define \(\Pi := \sum_{k=1}^N \frac{h_k^2}{\sigma_{t_{k-1}}^4}\). For \(T \geq 2, \delta \leq \frac{1}{2}\), the exponential integrator scheme (6) with early stopping results in a distribution \(\hat{q}_{T-\delta}\) such that
\begin{align}
    \mathrm{KL}(p_\delta \| \hat{q}_{T-\delta}) \lesssim (d + B^2) \exp(-T) + T \epsilon^2_0 + d^2 \Pi.
\end{align}
\end{theorem}
In particular, when using proper choices of $h_k$, the quantity $\Pi$ can be as small as $o(1)$. For instance, as shown in \citet{chen2023improved}, it can be proved that $\Pi=O(1/N^2)$ when using exponentially decreasing stepsize.


Then combing \cref{app-theorem:16} and \cref{app-theorem:2.2-o}, we finally have
\begin{theorem}
\label{app-theorem:2.2}
Training DDPM  for $H \geq H'$ iterations and uses $n \geq n'$ number of samples where
\begin{align*}
    H' = \Theta(\log(\varepsilon^{-1} \log d)),\quad  n' = \Theta(K^4d^5B^6/\varepsilon^2).
\end{align*}
Then, there is a universal constant \(C\) such that the following hold. Suppose that Assumptions \ref{assum:3} and Equation \ref{eq:learned score function} hold and the step sizes satisfy
\begin{align}
    \frac{h_k}{\sigma_{t_{k-1}}^2} \leq \frac{1}{Cd} \le \gamma, \quad k = 1, \ldots, N
\end{align}
Define \(\Pi := \sum_{k=1}^N \frac{h_k^2}{\sigma_{t_{k-1}}^4}\). For \(T \geq 2, \delta \leq \frac{1}{2}\), the exponential integrator scheme (6) with early stopping results in a distribution \(\hat{q}_{T-\delta}\) such that
\begin{align}
    \mathrm{KL}(p_\delta \| \hat{q}_{T-\delta}) \lesssim (d + B^2) \exp(-T) + N\gamma (\epsilon^2+B^2) + d^2 \Pi.
\end{align}
where $p$ is the data distribution and $\hat{q}$ is the sampling distribution.
\end{theorem}

In Theorem \ref{theorem:2.2}, we establish a connection between the training process and the sampling process, using KL-divergence as a metric to quantify the distance between the true data distribution and the sampled generated data distribution. It should be noted that both KL divergence and Wasserstein Distance serve as tools for measuring the similarity between distributions. Under the specific assumption that the data distributions are Gaussian, the Wasserstein Distance reduces to FID (i.e., the metric used in our paper). Theorem \ref{theorem:2.2} demonstrates that achieving the same sampling error necessitates a larger number of training samples for data distributions with a greater number of modes ($K$). Consequently, under the constraint of limited training samples, the quality of images generated from training data distributions with more modes ($K$) tends to be worse compared to those with fewer modes.



\section{Experiments Setup}
\label{sec:appendix-exp}

\subsection{Training Details of AEs}
\label{sec:appendix-exp-ae}

We present the training details of \method in \cref{tab:ae_config_table}.

\input{tables/ae_config}


\subsection{Training Details of Diffusion Models}
\label{sec:appendix-exp-diffusion}

We present the training details of SiT-XL and LightningDiT in \cref{tab:sit_config_table,tab:dit_config_table}, which mainly follows their original setup.

\input{tables/sit_config}

\input{tables/dit_config}


\section{Experiments Results}
\label{sec:appendix-results}


\subsection{More Quantitative Generation Results}
\label{sec:appendix-results-gen}


We provide the additional precision and recall evaluation on 256$\times$256 and 512$\times$512 ImageNet benchmarks in \cref{tab:appendix_256} and \cref{tab:appendix_512}, respectively.

\input{tables/appendix_256}

\input{tables/appendix_512}

\subsection{Classifier-free Guidance Tuning Results}
\label{sec:appendix-results-cfg}

\input{tables/cfg_tune}

We provide our CFG scale tuning results in \cref{tab:cfg-tune}, where we found the gFID with CFG changes significantly even with small guidance scales. 
Applying CFG interval \cite{kynkaanniemi2024applying} to cutout the high timesteps with CFG can mitigate this issue. 
However, it is still extremely difficult to tune the guidance scale.

We use a guidance scale of 1.9 and an interval of {[}0, 0.75{]} for 256$\times$256 SiT-XL and a guidance scale of 1.8 and an interval of {[}0, 0.75{]} for 256$\times$256 LightningDiT to report the main results. 
For 512$\times$ models, we use a guidance scale of 1.5 and an interval of {[}0, 0.7{]} for SiT-XL and a guidance scale of 1.6 with an interval of {[}0, 0.65{]} 
for LightningDiT's main results.
\textit{Note that our models may present even better results with more fine-grained CFG tuning}.

We attribute the difficulty of tuning CFG to the semantics learned by the unconditional class, as we discussed in \cref{sec:exp-discuss}. 
Such semantics makes the linear scheme of CFG less effective, as reflected by the sudden change with small guidance values. 
Adopting and designing more advanced CFG schemes \cite{chung2024cfg++,karras2024guiding} may also be helpful with this problem, and is left as our future work.




\subsection{Latent Space Visualization}
\label{sec:appendix-results-latentvis}

More latent space visualization of \method variants is included in \cref{fig:latent_vis_appendix}.
\method in general learns more discriminative latent space with fewer GMM models with differente reconstruction targets.

\begin{figure}[h!]
\centering
    \hfill
    \subfloat[\method (Pixel)]{\includegraphics[width=0.24\textwidth]{figures/latent_vis_mae_pixel.png}}
    \hfill
    \subfloat[\method (HOG)]{\includegraphics[width=0.24\textwidth]{figures/latent_vis_mae_hog.png}}
    \hfill
    \subfloat[\method (DINOv2)]{\includegraphics[width=0.24\textwidth]{figures/latent_vis_mae_dino.png}}
    \hfill
    \subfloat[\method (CLIP)]{\includegraphics[width=0.24\textwidth]{figures/latent_vis_mae_clip.png}}
    \hfill
\caption{UMAP visualization on ImageNet of the learned latent space from (a) MAETok with raw pixel target; (b) MAETok with HOG target; (c) MAETok with DINOv2 target; (d) MAETok with CLIP target.
MAETok presents a more discriminative latent space. 
% We use different colors for classes in ImageNet.
}
\label{fig:latent_vis_appendix}
\end{figure}

\subsection{More Qualitative Generation Results}
\label{sec:appendix-results-gen-vis}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{appendix_vis/sit_256_33.png}
    \caption{Uncurated generation results of 256$\times$256 \method + SiT-XL. We use CFG of 3.0. Class label = ``Loggerhead'' (33).}
    % \label{fig:enter-label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{appendix_vis/sit_256_88.png}
    \caption{Uncurated generation results of 256$\times$256 \method + SiT-XL. We use CFG of 3.0. Class label = ``Macaw'' (88).}
    % \label{fig:enter-label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{appendix_vis/sit_256_89.png}
    \caption{Uncurated generation results of 256$\times$256 \method + SiT-XL. We use CFG of 3.0. Class label = ``Cacatua galerita'' (89).}
    % \label{fig:enter-label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{appendix_vis/sit_256_130.png}
    \caption{Uncurated generation results of 256$\times$256 \method + SiT-XL. We use CFG of 3.0. Class label = ``Flamingo'' (130).}
    % \label{fig:enter-label}
\end{figure}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{appendix_vis/512_250.png}
    \caption{Uncurated generation results of 512$\times$512 \method + SiT-XL. We use CFG of 2.0. Class label = ``Siberian husky'' (250).}
    % \label{fig:enter-label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{appendix_vis/512_279.png}
    \caption{Uncurated generation results of 512$\times$512 \method + SiT-XL. We use CFG of 2.0. Class label = ``Arctic fox'' (279).}
    % \label{fig:enter-label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{appendix_vis/512_360.png}
    \caption{Uncurated generation results of 512$\times$512 \method + SiT-XL. We use CFG of 2.0. Class label = ``Otter'' (360).}
    % \label{fig:enter-label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{appendix_vis/512_402.png}
    \caption{Uncurated generation results of 512$\times$512 \method + SiT-XL. We use CFG of 2.0. Class label = ``Guitar'' (402).}
    % \label{fig:enter-label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{appendix_vis/512_555.png}
    \caption{Uncurated generation results of 512$\times$512 \method + SiT-XL. We use CFG of 2.0. Class label = ``Fire Truck'' (555).}
    % \label{fig:enter-label}
\end{figure}



\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{appendix_vis/512_573.png}
    \caption{Uncurated generation results of 512$\times$512 \method + SiT-XL. We use CFG of 2.0. Class label = ``Go-kart'' (573).}
    % \label{fig:enter-label}
\end{figure}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{appendix_vis/512_620.png}
    \caption{Uncurated generation results of 512$\times$512 \method + SiT-XL. We use CFG of 2.0. Class label = ``Laptop'' (620).}
    % \label{fig:enter-label}
\end{figure}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{appendix_vis/512_705.png}
    \caption{Uncurated generation results of 512$\times$512 \method + SiT-XL. We use CFG of 2.0. Class label = ``Carriage'' (705).}
    % \label{fig:enter-label}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{appendix_vis/512_817.png}
    \caption{Uncurated generation results of 512$\times$512 \method + SiT-XL. We use CFG of 2.0. Class label = ``Sports Car'' (402).}
    % \label{fig:enter-label}
\end{figure}