%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath,amssymb} 
% \usepackage{subfigure}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{xcolor}         % colors
\usepackage{multirow}
\usepackage{diagbox}
% \usepackage{subfigure}
\usepackage{bbm}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{colortbl}
\usepackage{graphicx, amsmath, amssymb, caption, multirow, overpic, textpos}
\usepackage{floatflt}
\usepackage{subfig}
% \usepackage{subcaption}
\usepackage{dsfont}
\usepackage{amsmath}
\newcommand{\grayrow}{\rowcolor[gray]{.95}}
\newcommand{\cellc}{\cellcolor{lightgray!20}}


\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}
\newcolumntype{y}[1]{>{\raggedright\arraybackslash}p{#1pt}}
\newcolumntype{z}[1]{>{\raggedleft\arraybackslash}p{#1pt}}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\def\pz{{\phantom{0}}}

\newcommand{\ch}[1]{{\color{blue}{[(CH): #1]}}}
\newcommand{\chh}[1]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=red,size=\scriptsize]{(CH): #1}}

\newcommand{\yj}[1]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,size=\scriptsize]{(YJ): #1}}

\usepackage{todonotes}
\usepackage{xcolor}

\newcommand{\wjdd}[1]{\todo[linecolor=cyan,backgroundcolor=cyan!25,bordercolor=cyan,size=\scriptsize]{(Jindong) #1}}
\newcommand{\wjd}[1]{{\color{cyan}{[(Jindong) #1]}}}

\newcommand{\method}{MAETok\xspace}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Masked Autoencoders Are Effective Tokenizers for Diffusion Models}

\begin{document}

\twocolumn[

\icmltitle{Masked Autoencoders Are Effective Tokenizers for Diffusion Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Hao Chen$^*$}{cmu}
\icmlauthor{Yujin Han$^*$}{hku}
\icmlauthor{Fangyi Chen}{cmu}
\icmlauthor{Xiang Li}{cmu}
\icmlauthor{Yidong Wang}{pku}
\\
\icmlauthor{Jindong Wang}{wm}
\icmlauthor{Ze Wang}{amd}
\icmlauthor{Zicheng Liu}{amd}
\icmlauthor{Difan Zou}{hku}
\icmlauthor{Bhiksha Raj}{cmu,mbz}
\end{icmlauthorlist}

\icmlaffiliation{cmu}{Carnegie Mellon University}
\icmlaffiliation{mbz}{Mohamed bin Zayed University of AI}
\icmlaffiliation{hku}{The University of Hong Kong}
\icmlaffiliation{amd}{AMD}
\icmlaffiliation{pku}{Peking University}
\icmlaffiliation{wm}{William \& Mary}

\icmlcorrespondingauthor{Hao Chen}{haoc3@andrew.cmu.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}

Recent advances in latent diffusion models have demonstrated their effectiveness for high-resolution image synthesis. 
However, the properties of the latent space from tokenizer for better learning and generation of diffusion models remain under-explored. Theoretically and empirically, we find that improved generation quality is closely tied to the latent distributions with better structure, such as the ones with fewer Gaussian Mixture modes and more discriminative features.
Motivated by these insights, we propose \textbf{MAETok}, an autoencoder (AE) leveraging mask modeling to learn semantically rich latent space while maintaining reconstruction fidelity. 
Extensive experiments validate our analysis, demonstrating that the variational form of autoencoders is not necessary, and a discriminative latent space from AE alone enables state-of-the-art performance on ImageNet generation using only \textbf{128} tokens. 
MAETok achieves significant practical improvements, enabling a gFID of \textbf{1.69} with \textbf{76×} faster training and \textbf{31×} higher inference throughput for 512×512 generation. 
Our findings show that the structure of the latent space, rather than variational constraints, 
is crucial for effective diffusion models.
% Code and trained models will be released.
Code and trained models are released\footnote{\scriptsize{\url{https://github.com/Hhhhhhao/continuous_tokenizer}}.}.

% MAETok establishes a new xx with \textbf{76x} faster training speed and \textbf{31x} higher inference throughput at 512$\times$512 ImageNet generation.
% These findings suggest that the structure of latent space, rather than the specific form of regularization, is crucial for effective diffusion models.

% Our theoretical analysis establishes the connection between latent space complexity and generation quality, while empirical results show that MAETok enables  and better generation quality across multiple diffusion model architectures. These findings suggest that the structure of latent space, rather than the specific form of regularization, is crucial for effective latent diffusion.




% This paper investigates Masked Autoencoders (MAE) as effective tokenizers for diffusion models in image generation. 
% While latent diffusion models have become dominant for high-resolution image synthesis, the properties of latent space remains critical yet under-explored. 
% We propose training transformer-based autoencoders using mask modeling objectives with auxiliary shallow decoders to learn better latent representations. 
% Experiments on ImageNet demonstrate that our MAE-based tokenizers outperform both plain autoencoders and VAEs in generation fidelity while enabling more robust image synthesis in a latent diffusion framework.
\end{abstract}


\begin{figure*}[t!]
    \centering    \includegraphics[width=0.98\linewidth]{figures/teaser_vis.pdf}
    \vspace{-0.1in}
    \caption{Diffusion models with \method achieves state-of-the-art image generation on ImageNet of 512$\times$512 and 256$\times$256 resolution.}
    \label{fig:teaser_vis}
\vspace{-0.2in}
\end{figure*}


\section{Introduction}
\label{sec:intro}


Diffusion models \cite{sohl2015deep,ho2020denoising,rombach2022high,peebles2023scalablediffusionmodelstransformers} have recently emerged as a powerful class of generative models, achieving state-of-the-art (SOTA) performance on various image synthesis tasks \cite{deng2009imagenet,ghosh2024geneval}.
% Beyond this, they also hold the potential to be extended with multi-modality models \cite{zhou2024transfusion,xie2024show,wang2024emu3,wu2024vila,li2024dual,wu2024janus}, bridging vision with other modalities for applications such as image understanding \cite{yue2024mmmu}.

Although originally formulated in pixel space \cite{ho2020denoising,dhariwal2021diffusion}, subsequent research has shown that operating in a \textit{latent space} -- a compressed representation typically learned by a tokenizer -- can substantially improve the efficiency and scalability of diffusion models \cite{rombach2022high}. 
By avoiding the high-dimensional pixel domain during iterative diffusion and denoising steps, latent diffusion models dramatically reduce computational overhead and have quickly become the \textit{de facto} paradigm for high-resolution generation \cite{esser2024scaling}.


However, a key question remains: \textit{What constitutes a ``good'' latent space for diffusion}? 
Early work primarily employed \textit{Variational Autoencoders} (VAE) \cite{kingma2013auto} as tokenizers, which ensure that the learned latent codes follow a relatively smooth distribution \cite{higgins2017beta} via a Kullback–Leibler (KL) constraint. 
While VAEs can empower strong generative results \cite{ma2024sit,li2024autoregressiveimagegenerationvector,deng2024causal}, they often struggle to achieve high pixel-level fidelity in reconstructions due to the imposed regularization \cite{tschannen2025givt}. 
In contrast, recent explorations with \textit{plain Autoencoders} (AE) \cite{hinton2006reducing,vincent2008extracting} produce higher-fidelity reconstructions but may yield latent spaces that are insufficiently organized or too entangled for downstream generative tasks \cite{chen2024deep}. Indeed, more recent studies emphasize that high fidelity to pixels does not necessarily translate into robust or semantically disentangled latent representations \cite{esser2021taming,yao2025reconstruction}; leveraging latent alignment with pre-trained models can often improve generation performance further \cite{li2024imagefolder,chen2024softvq,qu2024tokenflow,zha2024language}.

In this work, we attempt to answer this question by investigating the interaction between \textit{the latent distribution learned by tokenizers}, and \textit{the training and sampling behavior of diffusion models} operating in that latent space. 
Specifically, we study AE, VAE and the recently emerging representation aligned VAE \cite{li2024imagefolder,chen2024softvq,zha2024language,yao2025reconstruction}, by fitting a Gaussian mixture model (GMM) into their latent space.
Empirically, we show that a latent space with more \textit{discriminative} features, whose GMM modes are \textit{fewer}, tends to produce a lower diffusion loss.
Theoretically, we prove that a latent distribution with fewer GMM modes indeed leads to a lower loss of diffusion models and thus to better sampling during inference.
% of diffusion models.\wjdd{What does GMM do with MAE?}

% Theoretically and empirically, we demonstrate that the latent-space structure, specifically, \ch{xx, xx} crucially impacts diffusion training and eventual generation fidelity. \ch{add more with our analysis}

Motivated by these insights, we demonstrate that diffusion models trained on \textit{AE}s with discriminative latent space are enough to achieve SOTA performance.
We propose to train AEs as as \textit{Masked Autoencoders} (MAE) \cite{he2022masked,xie2022simmim,wei2022masked}, a self-supervised paradigm that can discover more generalized and discriminative representations by reconstructing proxy features \cite{zhang2022mask}.
%transformer
% \wjdd{Motivation for transformer? Suddenly, transformer appears.} 
%AE \cite{yu2021vector,yu2024an,li2024imagefolder,chen2024softvq} as \textit{Masked Autoencoders} (MAE) \cite{he2022masked,xie2022simmim,wei2022masked}, a self-supervised learning method that can \ch{xxxx}.
% \wjdd{learn more generalized and discriminative representations by constructing pre-text tasks? This will connect to discrimination, serving as the motivation to introduce MAE. Another idea: if learning discriminative features is helpful, can we introduce adversarial learning to AE? Another paper?}
More specifically, we adopt the transformer architecture of tokenizers \cite{yu2021vector,yu2024an,li2024imagefolder,chen2024softvq} and randomly mask the image tokens at the encoder, whose features need to be reconstructed at the decoder \cite{assran2023self}. 
To maintain a pixel decoder with high reconstruction fidelity, we adopt auxiliary shallow decoders that predict the features of unseen tokens from seen ones to learn the representations, along with the pixel decoder which is normally trained as previous tokenizers.
The auxiliary shallow decoders introduce trivial computation overhead during training.
This design allows us to extend the MAE objective that reconstructs masked image patches, to simultaneously predict \textit{multiple targets}, such as HOG \cite{dalal2005histograms} features \cite{wei2022masked}, DINOv2 features \cite{oquab2023dinov2}, CLIP embeddings \cite{radford2021learning,zhai2023sigmoid}, and Byte-Pair Encoding (BPE) indices with text \cite{superclass_huang}.
% This multi-target formulation encourages the AE’s latent space to encode both \textit{rich semantic features} and \textit{essential local details}, ultimately aligning more effectively with the diffusion process.

Furthermore, we reveal an interesting decoupling effect: the capacity to learn a \textit{discriminative and semantically rich} latent space at the encoder can be separated from the capacity to \textit{achieve high reconstruction fidelity} at the decoder. 
In particular, a higher mask ratio (40--60\%) in MAE training often degrades immediate pixel-level quality. 
However, by \textit{freezing} the AE’s encoder, thus preserving its well-organized latent space, and \textit{fine-tuning only the decoder}, we can recover strong pixel-level reconstruction fidelity without sacrificing the semantic benefits of the learned representations.

Extensive experiments on ImageNet \citep{deng2009imagenet} demonstrate the effectiveness of \method. 
% that while plain AEs achieve excellent reconstruction fidelity with low \textit{reconstruction} FID (rFID), their latent spaces do not necessarily support low \textit{generation} FID (gFID) when used with diffusion models.
% In contrast, VAEs often make diffusion models to yield slightly better generative performance at the expense of weaker reconstruction and over-smoothed latent space. 
% Our method, termed \textbf{\method}, 
It addresses the trade-off between reconstruction fidelity and discriminative latent space by training the plain AEs with mask modeling, showing that the structure of latent space is more crucial for diffusion learning, instead of the variational forms of VAEs.
\method achieves improved reconstruction FID (rFID) and generation FID (gFID) using only \textbf{128} tokens for the 256$\times$256 and 512$\times$512 ImageNet benchmarks. 
% , and during inference, \method functions as a plain AE.
% \wjdd{what about the extra computation overhead by MAE?}

Our contributions can be summarized as follows:
\vspace{-0.1in}
\begin{itemize}[leftmargin=1em]
\setlength\itemsep{0em}
    \item \textbf{Theoretical  and Empirical Analysis}:
    % \wjdd{Empirical and Theoretical analysis would be better since we also have empirical study} 
    % We shed light on why low diffusion training loss in a given latent space does not always lead to improved sample quality, revealing how latent-space structure interacts with diffusion objectives.
    We establish a connection between latent space structure and diffusion model performance through both empirical and theoretical analysis. 
    We reveal that structured latent spaces with fewer \textit{Gaussian Mixture Model} modes enable more effective training and generation of diffusion models.
    % , leading to a theoretical framework for understanding the relationship between latent distribution complexity and generation quality.
    \item \textbf{\method}: We train plain AEs using mask modeling and show that simple AEs with more discriminative latent space empower faster learning, better generation, and higher throughput of diffusion models, showing that the variational regularization of VAE is not necessary.
    % striking a balance between semantic representation and reconstruction fidelity. 
    % \item \textbf{Decoupled Encoder-Decoder Optimization:} We demonstrate that freezing the encoder (latent space) and fine-tuning only the decoder enables us to maintain strong semantic features while boosting pixel-level fidelity.
    \item \textbf{SOTA Generation Performance}: 
    Diffusion models of 675M parameters trained on \method with 128 tokens achieve performance comparable to previous best models on 256$\times$ 256 ImageNet generation and outperform 2B USiT at 512 resolution with a 1.69 gFID and 304.2 IS.
    % Our experiments on ImageNet confirm that the proposed approach outperforms plain AEs and VAEs in generation FID, offering a novel perspective on building robust latent spaces for diffusion-based generative modeling.
\end{itemize}








\section{On the Latent Space and Diffusion Models}
\label{sec:gmm}

To study the relationship of latent space for diffusion models, we start with popular tokenizers, including
% To address the question raised in \cref{sec:intro}: \textit{ What constitutes a ``good'' latent space for diffusion}? We utilize popular tokenizers, 
AE \cite{hinton2006reducing}, VAE \cite{kingma2013auto}, representation aligned VAE, i.e., VAVAE \cite{yao2025reconstruction}. 
We train diffusion models on them and establish connections between latent space properties and the quality of the final image generation through empirical and theoretical analysis.
% \subsection{Empirical and Theoretical Analysis}
% \label{sec:empirical_phenomenon}

\textbf{Empirical Analysis}.
Inspired by existing theoretical work \cite{chen2022sampling,chen2023improved,benton2024nearly}, our investigation of the connection between latent space and generation quality starts with a high-level intuition. 
With optimal diffusion model parameters, such as sufficient total time steps and adequately small discretization steps, the generation quality of diffusion models is dominated by the denoising network's training loss \cite{chen2022sampling,chen2023improved,benton2024nearly}, while the effectiveness of training diffusion model via DDPM \cite{ho2020denoising} heavily depends on the hardness of learning the latent space distribution \cite{shah2023learning,diakonikolas2023sq,gatmiry2024learning}. Specially, when the training data distribution is too complex and multi-modal, i.e., not discriminative enough, the denoising network may struggle to capture such entangled global structure of latent space, resulting in degraded generation. 

\begin{figure}[t!]
\centering
    \hfill
    \subfloat[GMM Loss]{\label{fig:gmm_loss}\includegraphics[width=0.235\textwidth]{figures/gmm_losses.pdf}}
    \hfill
    \subfloat[Diffusion Loss]{\label{fig:training_loss}\includegraphics[width=0.235\textwidth]{figures/losses.pdf}}
    \hfill
\vspace{-0.1in}
\caption{GMM fitting on latent space of AE, VAE, VAVAE, and \method. Fewer GMM modes in latent space usually corresponds to lower diffusion losses and better generation performance.}
\vspace{-0.15in}
\label{fig:empirical_phenomenon}
\end{figure}

Building upon this intuition, we use the \textit{Gaussian Mixture Models} (GMM) to evaluate the number of modes in alternative latent space representations, where a higher number of modes indicates a more complex structure. 
\cref{fig:gmm_loss} analyzes the GMM fitting by varying the number of Gaussian components and comparing their negative log-likelihood losses (NLL) across different latent spaces, where a lower NLL indicates better fitting quality.
We observe that, to achieve comparable fitting quality, i.e., similar GMM losses, VAVAE requires fewer modes compared to VAE and AE.
% In other words, 
Fewer modes are sufficient to adequately represent the latent space distributions of VAVAE compared to those of AE and VAE, highlighting simpler global structures in its latent space.
Correspondingly, \cref{fig:training_loss} reports the training losses of diffusion models with AE, VAE, and VAVAE, which (almost) align with the GMM losses shown in \cref{fig:gmm_loss}, where fewer modes correspond to lower diffusion losses and better gFID. 
This alignment validates our intuition, confirming that latent spaces with fewer modes and thus more separated and discriminative features can reduce the learning difficulty and lead to better generation quality of diffusion models.

\textbf{Theoretical Analysis}.
After observing experimental phenomena that align with our high-level intuition, we further present a concise theoretical analysis here to justify the rationale behind it, with more details provided in \cref{app:theory-analysis}.

Following the empirical analysis setup, we first consider a latent data distribution in $d$ dimensions modeled as a GMM with $K$ equally weighted Gaussians:
\begin{align}
\label{eq:main-gmm}
    p_0 = \frac{1}{K} \sum_{i=1}^{K} \mathcal{N}(\boldsymbol{{\mu}}^*_i, \mathbf{I}),
\end{align}
Considering the classic diffusion model DDPM \cite{ho2020denoising} and following the training objective as \citet{shah2023learning}, the score matching loss of DDPM at timestep $t$ is
\begin{align}
\label{eq:training}
  \min_{\mathbf w} \mathbb E[\|s_{\mathbf w}(\mathbf x,t)-\nabla_{\mathbf x} \log p_t(\mathbf x)\|^2],
\end{align}
where $s_{\mathbf w}(\mathbf x,t)$ represents the denoising network and $\nabla_{\mathbf{x}}\log p_t(\mathbf x)$ denotes the oracle score function. 

Then, we establish the following theorem to show that more modes typically require larger training sample sizes for diffusion models to achieve comparable generation quality. 

% \begin{theorem}
% \label{theorem:2.2}
% (Informal, see \cref{app-theorem:2.2})  
% Let \( q \) be a mixture of \( K \) Gaussians as defined in \cref{eq:gmm}. For different latent spaces with the same dimension \( d \), total time steps \( T \), discretization step size upper bound $\gamma$ and discretization steps $N$, to achieve a sampling error of 
% \begin{align}
%     \text{KL}(p_\delta \| \hat{q}_{T-\delta}) \lesssim N\gamma \epsilon^2 + C(d, T, B,N,\gamma),
% \end{align} 
% where $\delta \leq \frac{1}{2}$, $C(d, T, B,N,\gamma)$ is a constant, \(\varepsilon > \varepsilon_0 = 1/\text{poly}(d)\) and \( \max_i \|\boldsymbol{\mu}_i\|^2 \leq B^2 \), the DDPM with gradient descent requires running for \( H \geq H' \) iterations and using \( n \geq n' \) number of samples, where
% \begin{align}
%     H' &= \Theta\left(\log\left(\frac{1}{\varepsilon} \log d\right)\right),
% \end{align}
% \begin{align}
% \label{eq:n}
%      n' &= \Theta\left(\frac{K^4 d^5 B^6}{\varepsilon^2}\right).
% \end{align}
% \end{theorem}
\begin{theorem}
\label{theorem:2.2}
(Informal, see \cref{app-theorem:2.2})  
Let the data distribution be a mixture of \( K \) Gaussians as defined in \cref{eq:main-gmm}. Then assume the norm of each mode is bounded by some constants, let $d$ be the data dimension,  $T$ be the total time steps, and $\epsilon$ be a proper target error parameter. In order to achieve a $O(T\epsilon^2)$ error in KL divergence between data  distribution and generation distirbution, the DDPM algorithm may require using \( n \geq n'\) number of samples:
\begin{align}\label{eq:n}
     n' = \Theta\left(\frac{K^4 d^5 B^6}{\varepsilon^2}\right),
\end{align}
where the upper bound of the mean norm satisfies $\max_i \|\boldsymbol{\mu}_i\| \leq B $.
\end{theorem}
\cref{theorem:2.2} combines Theorem 16 from~\cite{shah2023learning} and Theorem 2.2 from~\cite{chen2023improved}, showing that to achieve a comparable generation quality $O(T\epsilon^2)$, latent spaces with more modes (\( K \)) require a larger training sample size, scaling as \( \mathcal{O}(K^4) \).This theoretically help explain why, under a finite number of training samples, latent spaces with more modes (e.g., AE and VAE) produce worse generations with higher gFID. 
We provide additional experimental results in \cref{app:theory-analysis}, demonstrating that these latent distributions share comparable upper bounds $B$, thus justifying our focus primarily on the impact of mode number $K$.

% We then utilize Theorem 2.2 in~\citep{chen2023improved} to bridge the training and sampling processes of diffusion models, stating that the KL-divergence between real and synthesized distributions is bounded by terms proportional to the training loss, i.e., $\epsilon^2_0+B^2$. Therefore, larger training loss tends to result in worse generation quality. It should be noted that both KL divergence and Wasserstein Distance serve as tools for measuring the similarity between distributions. Under the specific assumption that the data distributions are Gaussian, the Wasserstein Distance reduces to FID i.e., the metric used in our study. More details about Theorem 2.2 in~\citet{chen2023improved} are provided in \cref{app:theory-analysis}.

\section{Method}
\label{sec:method}

Motivated by our analysis, we show that the variational form of VAEs may not be necessary for diffusion models, and simple AEs are enough to achieve SOTA generation performance with \textbf{128} tokens, as long as they have discriminative latent spaces, i.e., with fewer GMM modes.  
We term our method as \textbf{\method}, with more details as follows.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/architecture.pdf}
    \vspace{-0.1in}
    \caption{Model architecture of \method. We adopt the plain 1D autoencoder (AE) as tokenizer, with a vision transformer (ViT) encoder $\mathcal{E}$ and decoder $\mathcal{D}$. \method is trained using mask modeling at encoder, with a mask ratio of 40-60\%, and predict multiple target features, e.g., HOG, DINO-v2, and CLIP features, of masked tokens from the unmasked ones using auxiliary shallow decoders.}
    \label{fig:architecture}
\vspace{-0.15in}
\end{figure}


\subsection{Architecture}

We build \method upon the recent 1D tokenizer design with learnable latent tokens \cite{yu2024an,li2024imagefolder,chen2024softvq}. 
Both the encoder $\mathcal{E}$ and decoder $\mathcal{D}$ adopt the Vision Transformer (ViT) architecture \cite{dosovitskiy2021imageworth16x16words,yu2021vector}, but are adapted to handle both image tokens and latent tokens, as shown in \cref{fig:architecture}.

\textbf{Encoder}. 
The encoder first divides the input image $I \in \mathbb{R}^{H \times W \times 3}$ into $N$ patches according to a predefined patch size $P$, each mapped to an embedding vector of dimension $D$, resulting in image tokens $\mathbf{x} \in \mathbb{R}^{N \times D}$. 
In addition, we define a set of $L$ learnable latent tokens $\mathbf{z} \in \mathbb{R}^{L \times D}$. 
The encoder transformer takes the concatenation of image patch embeddings and latent tokens $\left[\mathbf{x} ; \mathbf{z}\right] \in \mathbb{R}^{(N+L) \times D}$ as its input, and outputs the latent representations $\mathbf{h} \in \mathbb{R}^{L \times H}$ with a dimension of $H$ from only the latent tokens:
\begin{equation}
\mathbf{h}=\mathcal{E}\left(\left[\mathbf{x} ; \mathbf{z} \right] \right).
\end{equation}

\textbf{Decoder}.
To reconstruct the image, we use a set of $N$ learnable image tokens $\mathbf{e} \in \mathbb{R}^{N \times H}$. 
We concatenate these mask tokens with $\mathbf{h}$ as the input to the decoder, and takes only the outputs from mask tokens for reconstruction: 
\begin{equation}
    \hat{\mathbf{x}} =\mathcal{D}([\mathbf{e} ; \mathbf{h}]]).
\end{equation} 
We then use a linear layer on top of $\hat{\mathbf{x}} \in \mathbb{R}^{N \times D}$ to regress the pixel values and obtain the reconstructed image $\hat{I}$.


\textbf{Position Encoding}. 
To encode spatial information, we apply 2D Rotary Position Embedding (RoPE) to the image patch tokens~$\mathbf{x}$ at the encoder and the image tokens~$\mathbf{e}$ at the decoder. In contrast, the latent tokens $\mathbf{z}$ (and their encoded counterparts $\mathbf{h}$) use standard 1D absolute position embeddings, since they do not map to specific spatial locations. This design ensures that patch-based tokens retain the notion of 2D layout, while the learned latent tokens are treated as a set of abstract features within the transformer architecture.

\textbf{Training objectives}. 
We train \method using the standard tokenizer losses as in previous work \cite{esser2021taming}:
\begin{equation}
\mathcal{L} = \mathcal{L}_{\textrm{recon}} 
 + \lambda_1 \mathcal{L}_{\textrm{percep}} + \lambda_2 \mathcal{L}_{\textrm{adv}},
\label{eq:train_loss}
\end{equation}
with $\mathcal{L}_{\textrm{recon}}$, $\mathcal{L}_{\textrm{percep}}$, and $\mathcal{L}_{\textrm{adv}}$ denoting as pixel-wise mean-square-error (MSE) loss, perceptual loss \cite{larsen2016autoencoding,johnson2016perceptual,dosovitskiy2016generating,zhang2018unreasonableeffectivenessdeepfeatures}, and adversarial loss \cite{goodfellow2020generative,isola2018imagetoimagetranslationconditionaladversarial}, respectively, and $\lambda_1$ and $\lambda_2$ being hyper-parameters.  
Note that \method is a plain AE architecture, therefore, it does not require any variational loss between the posterior and prior as in VAEs, which simplifies training.


\subsection{Mask Modeling}

\textbf{Token Masking at Encoder}. 
A key property of \method is that we introduce mask modeling during training, following the principles of MAE \cite{he2022masked,xie2022simmim}, to learn a more discriminative latent space in a self-supervised way. 
Specifically, we randomly select a certain ratio, e.g., 40\%-60\%, of the image patch tokens according to a binary masking indicator $M \in \mathbb{R}^N$, and replace them with the learnable mask tokens $\mathbf{m} \in \mathbb{R}^D$ before feeding them into the encoder.
\textit{All} the latent tokens are maintained to more heavily aggregate information on the unmasked image tokens and used to reconstruct the masked tokens at the decoder output.

% This approach forces the model to use the latent tokens to 

\textbf{Auxiliary Shallow Decoders}. 
In MAE, a shallow decoder \cite{he2022masked} or a linear layer \cite{xie2022simmim,wei2022masked} is required to predict the target features, e.g., raw pixel values, HOG features, and features from pre-trained models, of the masked image tokens from the remaining ones.
However, since our goal is to train MAE as tokenizers, the pixel decoder $\mathcal{D}$ needs to be able to reconstruct images in high fidelity.
Thus, we keep $\mathcal{D}$ as a similar capacity to $\mathcal{E}$, and incorporate auxiliary shallow decoders to predict additional feature targets, which share the same design as the main pixel decoder but with fewer layers. 
Formally, each auxiliary decoder $\mathcal{D}^j_{\mathrm{aux}}$ takes the latent representations $\mathbf{h}$ and concatenate with their own $\mathbf{d}^j$ as inputs, and output $\hat{\mathbf{y}}^j$ as the reconstruction of their feature target $\mathbf{y}^j \in \mathbb{R}^{N \times D^j}$:
\begin{equation}
    \hat{\mathbf{y}}^j =\mathcal{D}^j_{\mathrm{aux}}([\mathbf{e}^j ; \mathbf{h}] ; \theta),
\end{equation}
where $D^j$ denotes the dimension of target features.
We train these auxiliary decoders along with our AE using additional MSE losses at only the masked tokens according to the masking indicator $M$, similarly to \citet{xie2022simmim}:
\begin{equation}
    \mathcal{L}_{\textrm{mask}} = \sum_j \left\|  M \otimes  \left( \hat{\mathbf{y}}^j -\mathbf{y}^j \right) \right\|_2^2.
\end{equation}



\input{tables/ablation}


\subsection{Pixel Decoder Fine-Tuning}

While mask modeling encourages the encoder to learn a better latent space, high mask ratios can degrade immediate reconstruction. 
To address this, after training AEs with mask modeling, we \emph{freeze} the encoder, thus preserving the latent representations, and \emph{fine-tune} only the pixel decoder for a small number of additional epochs. 
This process allows the decoder to adapt more closely to frozen latent codes of clean images, recovering the details lost during masked training.
% We find it is crucial to curriculumly decrease the mask ratio to fine-tune the decoder, where directly training the decoder on clean images may result in overfitting to them and a degraded generation performance. 
We use the same loss as in \cref{eq:train_loss} for pixel decoder fine-tuning and discard all auxiliary decoders in this stage.







\section{Experiments}
\label{sec:exp}

We conduct comprehensive experiments to validate the design choices of \method, analyze its latent space, and benchmark the generation performance to show its superiority.


\subsection{Experiments Setup}
\label{sec:exp-setup}

\noindent \textbf{Implementation Details of Tokenizer}. 
We use XQ-GAN codebase \cite{li2024xq} to train \method.
We use ViT-Base \cite{dosovitskiy2021imageworth16x16words}, initialized from scratch, for both the encoder and the pixel decoder, which in total have 176M parameters. 
We set $L=128$ and $H=32$ for latent space.
Three \method variants are trained on 256$\times$256 ImageNet \cite{deng2009imagenet}, and 512$\times$512 ImageNet, and a subset of 512$\times$512 LAION-COCO \cite{schuhmann2022laion} for 500K iterations, respectively. 
In the first stage training with mask modeling on ImageNet, we adopt a mask ratio of 40-60\% , set by ablation, and 3 auxiliary shallow decoders for multiple targets of HOG \cite{dalal2005histograms}, DINO-v2-Large \cite{oquab2023dinov2}, and SigCLIP-Large \cite{zhai2023sigmoid} features.
We adopt an additional auxiliary decoder for tokenizer trained on LAION-COCO, which predicts the discrete indices of text captions for the image using a BPE tokenizer \cite{cherti2023reproducible,superclass_huang}. 
Each auxiliary decoder has 3 layers also set by ablation. 
We set $\lambda_1=1.0$ and $\lambda_2=0.4$.
For the pixel decoder fine-tuning, we linearly decrease the mask ratio from 60\% to 0\% over 50K iterations, with the same training loss.
More training details of tokenizers are shown in \cref{sec:appendix-exp-ae}.


\noindent \textbf{Implementation Details of Diffusion Models}.
We use SiT \cite{li2024scalable} and LightningDiT \cite{yao2025reconstruction} for diffusion-based image generation tasks after training \method. 
We set the patch size of them to 1 and use a 1D position embedding, and follow their original training setting for other parameters.
We use SiT-L of 458M parameters for the analysis and ablation study.
For main results, we train SiT-XL of 675M parameters for 4M steps and LightningDiT for 400K steps on ImageNet of resolution 256 and 512. 
More details are provided in \cref{sec:appendix-exp-diffusion}.

\textbf{Evaluation}.
For tokenizer evaluation, we report the reconstruction Frechet Inception Distance (rFID) \cite{heusel2017gans}, peak-signal-to-noise ratio (PSNR), and structural similarity index measure (SSIM) on ImageNet and MS-COCO \cite{lin2014microsoft} validation set. 
For the latent space evaluation of the tokenizer, we conduct linear probing (LP) on the flatten latent representations and report accuracy. 
To evaluate the performance of generation tasks, we report generation FID (gFID), Inception Score (IS) \cite{salimans2016improved}, Precision and Recall \cite{kynkaanniemi2019improved} (in \cref{sec:appendix-results-gen}), with and without classifier-free guidance (CFG) \cite{ho2022classifier}, using 250 inference steps.


\subsection{Design Choices of \method}
\label{sec:exp-ablation}


We first present an extensive ablation study to understand how mask modeling and different designs affect the reconstruction of tokenizer and, more importantly, the generation of diffusion models. 
We start with an AE and add different components to study both rFID of AE and gFID of SiT-L.

\textbf{Mask Modeling}. 
In \cref{tab:ablations-mm}, we compare AE and VAE with mask modeling and also study the proposed fine-tuning of the pixel decoder. 
For AE, mask modeling significantly improves gFID and slightly deteriorates rFID, which can be recovered through the decoder fine-tuning stage without sacrificing generation performance.
In contrast, mask modeling only marginally improves the gFID of VAE, since the imposed KL constraint may hinder latent space learning.

\textbf{Reconstruction Target}.
In \cref{tab:ablations-target}, we study how different reconstruction targets affect latent space learning in mask modeling. 
We show that using the low-level reconstruction features, such as the raw pixel (with only a pixel decoder) and HOG features, can already learn a better latent space, resulting in a lower gFID. 
Adopting semantic teachers such as DINO-v2 and CLIP instead can significantly improve gFID. 
Combining different reconstruction targets can achieve a balance in reconstruction fidelity and generation quality.

\textbf{Mask Ratio}. 
In \cref{tab:ablations-mr}, we show the importance of proper mask ratio for learning the latent space using HOG target, as highlighted in previous works \cite{he2022masked,wei2022masked,xie2022simmim}. 
A low mask ratio prevents the AE from learning more discriminative latent space.
A high mask ratio imposes a trade-off between reconstruction fidelity and the latent space quality, and thus generation performance. 


\textbf{Auxiliary Decoder Depth}. 
We study the depth of auxiliary decoder in \cref{tab:ablation-depth} with multiple reconstruction targets.
We show that a decoder that is too shallow or too deep could hurt both the reconstruction fidelity and generation quality. 
When the decoder is too shallow, the combined target features may confuse the latent with high-level semantics and low-level details, resulting in a worse reconstruction fidelity. 
However, a deeper auxiliary decoder may learn a less discriminative latent space of the AE with its strong capacity, and thus also lead to worse generation performance.








\subsection{Latent Space Analysis}

We further analyze the relationship between the latent space of the AE variants and the generation performance of SiT-L.  


\begin{figure}[t!]
\centering
    \hfill
    \subfloat[
    \footnotesize{AE}]{\label{fig:latent_vis_ae}\includegraphics[width=0.33\linewidth]{figures/latent_vis_ae.png}}
    \hfill
    \subfloat[\footnotesize{VAE}]{\label{fig:latent_vis_vae}\includegraphics[width=0.33\linewidth]{figures/latent_vis_vae.png}}
    \hfill
    \subfloat[\footnotesize{\method}]{\label{fig:latent_vis_mae256}\includegraphics[width=0.33\linewidth]{figures/latent_vis_mae_256.png}}
    % \hfill
    % \subfloat[\footnotesize{\method$^\dagger$}]{\label{fig:latent_vis_mae512}\includegraphics[width=0.24\linewidth]{figures/latent_vis_mae_512.png}}
    \hfill
\vspace{-0.1in}
\caption{UMAP visualization on ImageNet of the learned latent space from (a) AE; (b) VAE; (c) MAETok.
Colors indicate different classes.
% (d) MAETok trained on 512$\times$512 LAION subset with text information.
MAETok presents a more discriminative latent space. 
% We use different colors for classes in ImageNet.
}
\vspace{-0.2in}
\label{fig:latent_vis}
\end{figure}

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/latent_vis.png}
%     \vspace{-0.1in}
%     \caption{UMAP visualization on ImageNet of the learned latent space from (a) AE; (b) VAE; (c) MAETok trained on 256$\times$256 ImageNet; (d) MAETok trained on 512$\times$512 LAION subset.
%     MAETok present a more discriminative latent space \ch{with less GMM modes}. We use different colors for classes in ImageNet.}
%     \label{fig:latent_vis}
% \end{figure}

\textbf{Latent Space Visualization}.
We provide a UMAP visualization \cite{mcinnes2018umap} in \cref{fig:latent_vis} to intuitively compare the latent space learned by different variants of AE.  
Notably, both the AE and VAE exhibit more entangled latent embeddings, where samples corresponding to different classes tend to overlap substantially. 
In contrast, MAETok shows distinctly separated clusters with relatively clear boundaries between classes, suggesting that MAETok learns more discriminative latent representations.
% More interestingly, incorporating text information (\cref{fig:latent_vis_mae512}) appears to further sharpen these clusters in the latent space.
In line with our analysis in \cref{sec:gmm} and \cref{fig:empirical_phenomenon}, a more discriminative and separated latent representation of \method results in much fewer GMM modes and improve the generation performance.
More visualization is shown in \cref{sec:appendix-results-latentvis}.


\textbf{Latent Distribution and Generation Performance}. 
We assess the latent space's quality by studying the relationship between the linear probing (LP) accuracy on the latent space, as a proxy of how well semantic information is preserved in the latent codes,  and the gFID for generation performance. 
In \cref{fig:lp_vs_gfid}, we observe tokenizers with more discriminative latent distributions, as indicated by higher LP accuracy, correspondingly achieve lower gFID. 
This finding suggests that when features are well-clustered in latent space, the generator can more easily learn to generate high-fidelity samples.
We further verify this intuition by tracking gFID throughout training, shown in \cref{fig:gfid_train}, where \method 
%Tokenizers that produce more discriminative latents, e.g., \method, 
enables faster convergence, with gFID rapidly decreasing with lower values than the AE or VAE baselines. 
A high-quality latent distribution is shown to be a crucial factor in both achieving strong final generation metrics and accelerating training.




\begin{figure}[t!]
\centering
    \hfill
    \subfloat[gFID vs. LP Acc.]{\label{fig:lp_vs_gfid}\includegraphics[width=0.235\textwidth]{figures/lp_vs_gfid.pdf}}
    \hfill
    \subfloat[gFID during training]{\label{fig:gfid_train}\includegraphics[width=0.235\textwidth]{figures/gfid_train.pdf}}
    \hfill
\vspace{-0.1in}
\caption{The latent space from tokenizer correlates strongly with generation performance. More discriminative latent space (a) with higher linear probing (LP) accuracy usually leads to better gFID, and (b) makes the learning of the diffusion model easier and faster.}
\vspace{-0.25in}
\label{fig:latent_analysis}
\end{figure}




\subsection{Main Results}
\label{sec:exp-in1k}


\input{tables/main_256}
\input{tables/main_512}

\textbf{Generation}.
We compare SiT-XL and LightningDiT based on variants of \method in \cref{tab:main_256,tab:main_512} for the 256$\times$256 and 512$\times$512 ImageNet benchmarks, respectively, against other SOTA generative models. 
Notably, the \textbf{naive SiT-XL} trained on \method with only \textbf{128 tokens and plain AE architecture} achieves consistently better gFID and IS without using CFG: it outperforms REPA \cite{yu2024representation} by \textbf{3.59} gFID on 256 resolution and establishes a SOTA comparable gFID of \textbf{2.79} at 512 resolution. 
When using CFG, SiT-XL achieves a comparable performance with competing autoregressive and diffusion-based baselines trained on VAEs at 256 resolution. 
It beats the 2B USiT \cite{chen2024deep} with 256 tokens and also achieves a new SOTA of \textbf{1.69} gFID and \textbf{304.2} IS at 512 resolution. 
Better results have been observed with LightningDiT, trained with more advanced tricks \cite{yao2025reconstruction}, where it outperforms MAR-H of 1B parameters and USiT of 2B parameters without CFG, achieves a \textbf{2.56} gFID and \textbf{224.5} IS, and \textbf{1.72} gFID with CFG.
These results demonstrate that \textbf{the structure of the latent space} (see \cref{fig:latent_vis}), instead of the variational form of tokenizers, is vital for the diffusion model to learn effectively and efficiently.
We show a few generation samples in \cref{fig:teaser_vis}, with more visualization included in \cref{sec:appendix-results-gen-vis}.





\input{tables/tok_comp}

\textbf{Reconstruction}.
MAETok also offers strong reconstruction capabilities  on ImageNet and MS-COCO, as shown in \cref{tab:tok_comp}.
Compared to previous continuous tokenizers, including SD-VAE \cite{rombach2022high}, DC-AE \cite{chen2024deep}, VA-VAE \cite{yao2025reconstruction}, SoftVQ-VAE \cite{chen2024softvq}, and TexTok \cite{zha2024language}, MAETok achieves a favorable trade-off between the quality of the reconstruction and the size of the latent space.
On 256$\times$256 ImageNet, using \textbf{128 tokens}, MAETok attains an rFID of \textbf{0.48} and SSIM of \textbf{0.763}, outperforming methods such as SoftVQ in terms of both fidelity and perceptual similarity, while using half of the tokens in TexTok \cite{zha2024language}. 
On MS-COCO, where the tokenizer is not directly trained, MAETok still delivers robust reconstructions. 
At resolution of 512, \method maintains its advantage by balancing compression ratio and the reconstruction quality. 
% These results validate that a well‐structured yet compact latent space with decoder fine-tuning not only supports compelling generation but also ensures strong reconstruction performance.



\subsection{Discussion}
\label{sec:exp-discuss}

\textbf{Efficient Training and Generation}. 
A prominent benefit of the 1D tokenizer design is that it enables arbitrary number of latent tokens. 
The 256$\times$256 and 512$\times$512 images are usually encoded to 256 and 1024 tokens, while \method uses \textbf{128} tokens for both.
% Not only can 128 tokens achieve performance comparable to SOTA, but also 
It allows for much more efficient training and inference of diffusion models. 
For example, when using 1024 tokens of 512$\times$512 images, the Gflops and the inference throughput of SiT-XL are 373.3 and 0.1 images/second on a single A100, respectively.
MAETok reduces the Glops to \textbf{48.5} and increases throughput to \textbf{3.12} images/second.
With improved convergence, MAETok enables a \textbf{76x} faster training to perform similarly to REPA.

\textbf{Unconditional Generation}. 
An interesting observation from our results is that diffusion models trained on \method usually present significantly better generation performance without CFG, compared to previous methods, yet smaller performance gap with CFG. 
We hypothesize that the reason is that the unconditional class
%i.e., the empty condition, 
also learns the semantics in the latent space, 
\input{tables/uncond_gen}
as shown by the unconditional generation performance in \cref{tab:uncond_gen}.
As the latent space becomes more discriminative, the unconditional generation performance also improves significantly. 
This implies that the CFG linear combination scheme may become less effective, aligning with our CFG tuning results included in \cref{sec:appendix-results-cfg}.








\section{Related Work}
\label{sec:related}


\textbf{Image Tokenization}. 
Imgae tokenization aims at transforming the high-dimension images into more compact and structured latent representations. 
% It has served as a cornerstone in numerous applications of modern computer vision, such as latent diffusion models \cite{ramesh2021zero,rombach2022high,midjourney,sora} and multi-modality models \cite{zhou2024transfusion,xie2024show,wang2024emu3,wu2024vila,wu2024janus}.
Early explorations mainly used autoencoders \cite{hinton2006reducing,vincent2008extracting}, which learn latent codes reduced dimensionality. 
These foundations soon inspired methods with variational posteriors,
% that balance detail-preserving compression with the ability to encapsulate complex visual concepts, 
such as VAEs \cite{van2017neural,razavi2019generating} and VQ-GAN \cite{esser2021taming,razavi2019generatingdiversehighfidelityimages}. 
% They introduced new ways to structure latent spaces. 
% By imposing priors or quantization constraints, these tokenizers yielded representations that can facilitate more efficient generation and editing processes. 
Recent work has further improved compression fidelity and scalability \cite{lee2022autoregressive,yu2024language,mentzer2023finite,zhu2024scaling}, showing the importance of latent structure.
% that preserves both low-level details and high-level semantics.
% Meanwhile, the growing large-scale pre-training and vision-language modeling \cite{vaswani2023attentionneed} has further encouraged the alignment of latent representations with rich semantic signals. 
% For example, tokens derived from text-aligned embeddings enable more intuitive manipulation and classification \cite{dosovitskiy2021imageworth16x16words,zhu2010deformable}. 
More recent efforts have shown methods that bridge high-fidelity reconstruction and semantic understanding within a single tokenizer \cite{yu2024an,li2024imagefolder,chen2024softvq,wu2024vila,gu2023rethinkingobjectivesvectorquantizedtokenizers}. 
Complementary to them, we further highlight the importance of discriminative latent space, which allows us to use a simple AE yet achieve better generation. 



\textbf{Image Generation}.  
The paradigms of image generation 
%can be broadly divided into 
mainly categorize to autoregressive and diffusion models.
Autoregressive models initially relied on CNN architectures \cite{van2016conditional} and were later augmented with Transformer-based models \cite{vaswani2023attentionneed,yu2024randomized,lee2022autoregressive,liu2024customize,sun2024autoregressive} for improved scalability \cite{chang2022maskgitmaskedgenerativeimage,tian2024visualautoregressivemodelingscalable}. 
% The latest models are designed in an autoregressive fashion on a scale that further improves efficiency .
% including VAR \cite{}, MAR \cite{li2024autoregressiveimagegenerationvector} further enhance efficiency and image quality by optimizing factorized likelihoods and leveraging large-scale training.
Diffusion models show strong performance since their debut \citet{sohldickstein2015deepunsupervisedlearningusing}. 
Key developments \cite{nichol2021improveddenoisingdiffusionprobabilistic,dhariwal2021diffusion,song2022denoisingdiffusionimplicitmodels} refined the denoising process for sharper samples. 
A pivotal step in performance and efficiency came with latent diffusion \cite{vahdat2021scorebasedgenerativemodelinglatent,rombach2022highresolutionimagesynthesislatent}, which uses tokenizers to reduce dimension and conduct denoising in a compact latent space \cite{van2017neural,esser2021taming, peebles2023scalablediffusionmodelstransformers}. 
% Subsequent improvements, such as Transformer-based backbones \cite{}, have broadened the range of achievable image resolutions and enabled more flexible conditioning schemes. 
Recent advances include designing better tokenizers \cite{chen2024softvq,zha2024language,yao2025reconstruction}
% , such as the proposed \method, 
and combining diffusion with autoregressive models \cite{li2024autoregressiveimagegenerationvector}. 

% These methodological strides have fueled a diverse spectrum of applications, from text-guided image generation \cite{nichol2021glide,ding2021cogview} to photorealistic editing \cite{gafni2022make,saharia2022photorealistic}, culminating in commercial systems \cite{ramesh2021zero,rombach2022high,midjourney,sora} that highlight diffusion’s practical impact. As diffusion models continue to evolve, understanding the role of latent spaces—particularly how they balance semantic structure and fidelity—has emerged as a central challenge in large-scale image generation research.







\section{Conclusion}

We presented a theoretical and empirical analysis of latent space properties for diffusion models, demonstrating that fewer modes in latent distributions enable more effective learning and better generation quality. Based on these insights, we developed MAETok, which achieves state-of-the-art performance through mask modeling without requiring variational constraints. Using only 128 tokens, our approach significantly improves both computational efficiency and generation quality on ImageNet. Our findings establish that a more discriminative latent space, rather than variational constraints, is crucial for effective diffusion models, opening new directions for efficient generative modeling at scale.

\newpage

\section*{Impact Statement}

This work advances the fundamental understanding and technical capabilities of machine learning systems, specifically in the domain of image generation through diffusion models. While our contributions are primarily technical, improving efficiency and effectiveness of generative models, we acknowledge that advances in image synthesis technology can have broader societal implications. These may include both beneficial applications in creative tools and design, as well as potential concerns regarding synthetic media. We have focused on developing more efficient and robust methods for image generation, and we encourage ongoing discussion about the responsible deployment of such technologies.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}


\newpage
\bibliography{ref}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\input{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
