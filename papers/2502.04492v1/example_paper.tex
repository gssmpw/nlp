 %%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\DeclareMathOperator*{\argmin}{argmin} % no space, limits underneath in displays
\usepackage{nicefrac}
\usepackage{float}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{soul}
\usepackage{marvosym}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage{siunitx}
\usepackage{lipsum}
\usepackage{tabularx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{MARL with Focal Diversity Optimization}

\begin{document}

\twocolumn[
\icmltitle{Multi-Agent Reinforcement Learning with Focal Diversity Optimization}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Selim Furkan Tekin}{yyy}
\icmlauthor{Fatih Ilhan}{yyy}
\icmlauthor{Tiansheng Huang}{yyy}
\icmlauthor{Sihao Hu}{yyy}
\icmlauthor{Zachary Yahn}{yyy}
\icmlauthor{Ling Liu}{yyy}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Georgia Institute of Technology, USA}

\icmlcorrespondingauthor{stekin6, filhan3, thuang374, shu335, zachary.yahn, ll72}{@gatech.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
The advancement of Large Language Models (LLMs) and their finetuning strategies has triggered the renewed interests in multi-agent reinforcement learning. In this paper, we introduce a focal diversity-optimized multi-agent reinforcement learning approach, coined as MARL-Focal, with three unique characteristics. First, we develop an agent-fusion framework for encouraging multiple LLM based agents to collaborate in producing the final inference output for each LLM query. Second, we develop a focal-diversity optimized agent selection algorithm that can choose a small subset of the available agents based on how well they can complement one another to generate the query output. Finally, we design a conflict-resolution method to detect output inconsistency among multiple agents and produce our MARL-Focal output through reward-aware and policy-adaptive inference fusion. Extensive evaluations on five benchmarks show that MARL-Focal is cost-efficient and adversarial-robust. Our multi-agent fusion model achieves performance improvement of 5.51\% compared to the best individual LLM-agent and offers stronger robustness over the TruthfulQA benchmark. Code is available at \url{https://github.com/sftekin/rl-focal}
\end{abstract}

\section{Introduction}
\label{submission}
\vspace{-4pt}

In recent years, LLMs have been integrated into clouds due to their unmatched scalability, cost-efficiency, and accessibility where researchers can access the models through API calls \cite{achiam2023gpt, jiang2024mixtral, touvron2023llama, team2024gemma}. As the scaling law \cite{kaplan2020scaling} implies, the performance of the LLMs increases with the number of parameters and data used. In theory and practice, a user with this intent can create a system built upon the wisdom of numerous LLMs, where each is shaped by billions of parameters and terabytes of training data by connecting to multiple LLM services. In this paper, we test this theory by a widely recognized challenge: how to select among the large collection of close-sourced LLMs the best model combination, and how to combine possibly conflicting output answers from multiple LLMs to reach the the best output for the target learning task.

The recent approaches in the context of multi-agents carry similar motivations in terms of exploiting multiple agents to work collaboratively for a particular task. These methods usually divide a hard problem into sub-problems where each agent is responsible for one task. However, neither the grouping of tasks nor the aggregation of outputs for the same task has been thoroughly explored. Another promising research direction is LLM routing, where the goal is to identify the most suitable model for a given prompt. This approach aims to prevent unnecessary requests and, in turn, reduce API costs. However, routing is inherently limited by the performance of the models within its pool. Additionally, it requires understanding the intent of the query to match it with an appropriate model, which often necessitates the use of another LLM.

This goal can be categorized under the hood of ensemble learning in closed-sourced LLMs. However, recent works offer supervised solutions that are costly in terms of training and inference to each model in the pool. Similarly, the distillation and mixture of expert (MoE) methods demand significant computational resources and, moreover, require full access to the model parameters. In this paper, we demonstrate that these solutions have limitations in terms of generalization and advocate that an effective ensemble model should be both adaptive and cost-efficient. 

To this end, we formulate the problem as an infinite horizon Markov Decision Process and separate the model selection and aggregation into two stages. For the first stage, we train a Decider Agent that will perform simultaneous actions to decide which model should be prompted based on the diversity metrics of the current model pool. The agent adaptively prunes the possible ensemble combinations to create the best grouping by respecting the error correlation among the base models using the focal diversity score. For the second stage, we train the Aggregator Agent to generate the final decision based on the outputs generated by the current model pool. Through extensive evaluation of 5 benchmark datasets, we show that MARL-Focal can surpass the best-performing base model and beat supervised SOTA models. Moreover, we demonstrate that MARL-Focal can create a more helpful, safe, and truthful system by utilizing aligned models on the Alpaca-Eval, BeaverTails, and TruthfulQA datasets. 

\subsection{Related Work}
\vspace{-4pt}
\textbf{Ensemble Learning in LLMs.} Many works exploit majority voting to perform inference-time ensemble  \cite{wang2022self, fu2022complexity, li2022advance, wang2022rationale}. The downside of majority voting is the definition of equality between divergent answers. %Compared to math problems or multiple-choice problems, consensus-based approaches like weighted majority voting may do poorly for generative queries. 
Two threads of research further improve majority voting, one work utilizes the BLEU score as the heuristic to compare answers \cite{li2024more} another is to enhance the BLEU score-based answer combination method by either assigning weights \citep{yao2024tree} or by creating a debate environment \citep{liang2023encouraging, wan2024knowledge, du2023improving, chan2023chateval}. Due to the lengthy and complex prompt strategies of former works, supervised summarization LLM ensemble methods are proposed \cite{jiang2023llm, tekin-etal-2024-llm}.

\textbf{Ensemble Reinforcement Learning.} Creating an adaptive ensemble model with RL such as \cite{song2023ensemble, chua2018deep} is an extensive area covered in many contexts such as time-series prediction, \cite{liu2020new, nemeth2022split, perepu2020reinforcement}, ensemble pruning \cite{partalas2009pruning, liu2020instance}, and in context of LLMs \cite{ouyang2022training, liu2024rl, monea2024llms, zhang2021multi, sun2024llm, liu2024dynamic}. To the best of our knowledge, our work is the first approach in adapting reinforcement learning for both the pruning and generation stage in the context of LLMs.


\section{Preliminaries}
\label{sec:prem}
\vspace{-4pt}

Ensemble Learning (EL) is a widely adopted approach in Machine Learning (ML) and also in the domain of LLMs e.g. Mixture of Experts (MoE), Fusion models, and Multi-Agent Systems, where the fundamental idea involves training multiple estimators, combining outputs, and aggregating them to make refined decisions. To demonstrate the effectiveness of EL learning bias-variance decomposition of quadratic loss is often used \cite{song2023ensemble}. Even though the decomposition is defined for regression estimators, it is a fundamental concept that can also be generalized to any estimators, including LLMs.
\subsection{Bias-Variance Trade-off and Ensemble Learning}
\label{section-3.1}

Assume that an estimator $\hat{f}(x)$ aims to approximate the true relation $y=f(x)+\epsilon$ by reducing the expected quadratic loss for an input $x$ and label $y$ sampled from a dataset $\mathcal{D}$:
\vspace{-3pt}
\begin{align}
    \mathbb{E}[(y - \hat{f})^2] &= \mathbb{E}[(\hat{f} - \mathbb{E}[\hat{f}])^2] + (y - \mathbb{E}[\hat{f}])^2 + \sigma^2 \\
    &= \mathrm{Var}(\hat{f}) + \mathrm{Bias}(\hat{f})^2 + \mathrm{Var}(\epsilon).
\label{eq:bias_var}
\end{align}
The equation \ref{eq:bias_var} is the well-known bias-variance decomposition \cite{james2013introduction} of an estimator under a given noise $\epsilon$ with zero mean and $\sigma^2$ variance. Here, the $\sigma^2$ is irreducible and there is a trade-off between the estimator variance and the bias. As the estimator raises its complexity to approximate the true estimator, its variance will increase as it tries to capture more data points. EL methods aim to reduce the bias and variance jointly e.g. \cite{dietterich2000ensemble}, by representing the parts of the hypothesis space with each estimator. \citep{krogh1994neural} presents the \textit{ambiguity decomposition} by defining the ensemble model as the convex combination of component models: 
\begin{equation}
    \hat{f}_{\mathrm{ens}} = \sum_iw_i\hat{f}_i ,\; \mathrm{where} \sum_i w_i = 1.
\end{equation}
The ambiguity decomposition shows that the quadratic error of the ensemble estimator is guaranteed to be less than equal to the average quadratic estimators of the component estimators as put by the \cite{brown2005diversity}: 
\begin{equation}
    (y-\hat{f}_{\mathrm{ens}})^2 = \sum_i w_i(\hat{f}_i - y)^2 - \sum_i w_i(\hat{f}_i-\hat{f}_{\mathrm{ens}}).
\end{equation}
Here, the first term is the weighted average error of individual estimators and the second term is the \textit{ambiguity term} showing the variance between the individual estimators. Thus, the second result of this decomposition is that greater ambiguity, i.e., higher error correlation between individual estimators, leads to a lower overall error. More explicitly, \cite{brown2005diversity} substitute the ensemble estimator, $\hat{f}_{\mathrm{ens}}=\frac{1}{M}{\sum_i \hat{f}_i}$ in equation \ref{eq:bias_var} to break down the variance component even further to obtain \textit{bias-variance-covariance} decomposition:
\begin{align}
    \mathbb{E}[(\hat{f}_{\mathrm{ens}} - y)^2] =\overline{\mathrm{Bias}} + \frac{1}{N}\overline{\mathrm{Var}} + (1-\frac{1}{N})\overline{\mathrm{Covar}}.
\end{align}
As the averaged covariance term implies, the quadratic loss of ensemble networks depends the error correlation among its estimators. Thus, to achieve lower error, the selected estimators forming the ensemble \textit{must make uncorrelated errors}, and each estimator should cover a part of the hypothesis space to ensure that the average bias and variance are lower.

\subsection{Problem-Induced Instability}
\label{section-3.2}

Current Deep Learning models, with the rise of LLMs, target cross-domain generalization. The few-shot learning tasks are the pioneered objectives of this capability. The models are being able to learn the true relation between the input and the label for a task $\mathcal{T}$ with very few number of samples, i.e., $(x_i, y_i)\sim \mathcal{T}$ where $i$ is usually between 1 and 5. In zero-shot learning models go beyond this by producing the desired output with no $y$ given. LLMs achieve the task-agnostic capability with extreme size of model parameters and data. However, an ensemble estimator created by the convex combination of component models are fit to a particular task by:
\begin{equation}
    w_{\mathrm{\mathrm{best}}} = \argmin_{w} {\mathbb{E}_{(x, y)\sim \mathcal{T}}[({\sum_i w_i \hat{f}_i(x)} - y)^2]},
\end{equation}
where $w_{\mathrm{best}}$ is the best weight assigned to each estimator for the task $\mathcal{T}$. The problem with such an ensemble estimator arises when the task changes. The current weights, $w$, are fitted for the particular task $T$ but when the task changes the weights representing the importance of each estimator lose their value and create instability. For example, in a pool of LLMs, one particular LLM can be good at commonsense reasoning while the other is good at STEM-related topics. Therefore, the ensemble estimator $\hat{f}_{ens}$ must be task-agnostic or adaptive, leveraging the strengths of each model in the pool based on the given task. With a task-adaptive ensemble model, one can reduce the bias and the variance jointly and cover the entire hypothesis space by selecting the correct model pool for the incoming task. A task-agnostic ensemble model makes the final decision based on each individual model output regardless of the task.

\subsection{The Cost of Ensemble}
\label{section-3.3}

Recent model-based approaches, e.g. LLM-Blender \cite{jiang2023llm}, Fuse-LLM \cite{wan2024knowledge}, LLM-TOPLA \cite{tekin-etal-2024-llm} offer supervised solutions by training ensemble models using the base-model outputs. Not only causes high cost of money and computation power due to the requirement of inference for each model in the model pool to create a training dataset but also the trained model is not task-adaptive and limited by the training dataset. 
To improve adaptability and reduce inference costs, routing-based approaches (e.g., \cite{chen2023frugalgpt, ong2024routellm, zhao2024eagle}) offer a partial solution, since, they face several challenges:
\begin{itemize}[itemsep=0pt, parsep=0pt, topsep=0pt, left=0pt]
    \item The router must assess query difficulty, which often requires using another medium-sized LLM.
    \item The router must understand model capabilities, which involve paired model comparisons that do not scale linearly with the pool size.
    \item The router must be fast, cost-effective, and resilient to base model failures.
    \item Like model-based approaches, routers are typically trained in a supervised manner, limiting their performance in cross-domain tasks and reducing adaptability.
    \item The router's performance is inherently capped by the best-performing model in the pool.
\end{itemize}
Thus, this approach does not fully address adaptability or scalability. For the reasons detailed in this section and in \ref{section-3.2}, \ref{section-3.3}, we next show that we can decrease bias and variance adaptively by modeling the problem as an infinite-horizon discounted Markov Decision Process (MDP) and solving it with a few parameters and metrics.


\begin{figure*}[hbt!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/main.png}
    \vspace{-12pt}
    \caption{An overview of MARL-Focal.}
    \label{fig:main}
    \vspace{-12pt}
\end{figure*}

\section{Problem Definition}
\vspace{-4pt}
Let $x$ denote an input query for a task $\mathcal{T}$ under an LLM $\mathcal{M}$ and $y$ represent the desired output, and let $N$ be the number of available LLMs to which queries can be sent, denoted as $\mathcal{M}_1, \dots, \mathcal{M}_N$. For an input $x$, we aim to find the best combination of $m$ LLMs to send the query, obtaining a set of outputs $\hat{y}_1, \dots, \hat{y}_m$, where $1 \leq m \leq M$. Then our secondary goal is to make the final best decision, $\hat{y}_{\mathrm{final}}$ based on the generated outputs, such that $\hat{y}_{\mathrm{final}} - y$ is minimum. Based on these objectives and environment, first, we define MDP elements following the notation of \cite{zhang2021multi} and second, we define our agents for each objective.

Considering that the model selection and output generation as actions and the current model outputs based on input as the states, then we advocate that, this environment can be modeled as a sequence of actions and states by the tuple $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$, where $\mathcal{S}$ represents the state, $\mathcal{A}$ is the action, $\mathcal{P}:\mathcal{S}\times\mathcal{A}\rightarrow \Delta(\mathcal{S})$ is the transition probability of mapping the action state space to the set of probability distributions. For example $s_{t+1}\sim P(.|a_t,s_t)$ represents the next state for action $a_t$ and $s_t$ at time $t$. Here, $\mathcal{R}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}$ is the immediate reward value transitioning from $s_t$ to $s_{t+1}$ and $\gamma$ is the discount factor determining the importance we put to instantaneous and feature rewards. Our goal in MDP is to find a policy function $\pi:\mathcal{S}\rightarrow \mathcal{A}$ that determines the distribution of actions given the current state. In other words, the action $a_t$ is sampled from the distribution $a_t \sim \pi(.|s_t)$ at time $t$.   

For each objective, we define an agent as shown in Figure \ref{fig:main}: the first is the \textit{Decider Agent}, and the second is the \textit{Aggregator Agent}. The agents are fully cooperative in minimizing $\hat{y}_{\mathrm{final}} - y$; however, the second agent's state depends on the actions of the first agent, thus, it is an extensive-form game. Even though we define two different agents, their common goal is to find a policy ($\pi^{(\mathrm{Dec})}$ or $\pi^{(\mathrm{Agg})}$) that will maximize accumulated discounted reward:
\begin{equation}
    \mathbb{E}_{a_t\sim\pi(.|s_t)}[\sum_{t>0}\gamma^t R(s_t, a_t, s_{t+1})].
\label{eq:objective}
\end{equation}
The agents can share a common reward function, however, in our design, we used two different reward functions. As we show in Section \ref{sec:methodology}, we parametrize the policy functions and train them by maximizing equation \ref{eq:objective} using policy optimization algorithms, REINFORCE and PPO \cite{sutton1999policy, schulman2017proximal}.


\section{Methodology}
\label{sec:methodology}
\subsection{Decider Agent}
The decider agent is responsible for selecting the "best" model combination for the incoming data. Based on our insights in Section \ref{sec:prem}, the agent must perform selection by respecting the error correlation among its member models. Thus, we introduce diversity metrics to evaluate the current model pool. Second, we achieve adaptability with the nature of RL, where the selection policy updates its parameters periodically to adapt to the environment. Third, to reduce the costs, the agent makes decisions without performing inference, relying solely on the diversity metrics of the current pool. 

We define the elements of a Decider Agent as follows:
\begin{itemize}[itemsep=0pt, parsep=0pt, topsep=0pt, left=0pt]
\item State: For $n$ number of diversity metrics denoted by $\delta^1,\dots,\delta^n$, the agent observes the state $s_t = [\mathcal{E}_t, \overline{\mathcal{E}}_t,  \delta_t^1, \dots, \delta_t^n]$ at time $t$, where $\mathcal{E}\in\{0,1\}^N$ is the  binary vector representing the current model pool and $\overline{\mathcal{E}}_t$ is the current size of the pool. The diversity metrics are calculated based on the historical data with window size $T$. The details of the designed metrics are given in Section \ref{sec:div}.
\item Action: The agent simultaneously decides whether each model should be included in the model pool. Accordingly, we define the action at time $t$ as a binary vector $a_t\in\{0,1\}^N$, where each index indicates whether a model is included in the pool. 
\item Policy: At time $t$ the policy should provide a probability vector for each model action distribution in the pool:
\vspace{-5pt}
\begin{equation}
    \begin{split}
        &\pi_{\theta}(a_t\mid s_t)=[p_1, p_2,\dots,p_N],\\
        &\mathrm{where}\; p_i = P(a\mid s_t;\theta),
    \end{split}
\end{equation}
Here, $p_i$ is the probability distribution for model $i$ to be included in the model pool and $\theta$ is the decider policy parameters. Simultaneous multiple actions increase the complexity of the problem, as traditional reinforcement learning algorithms are primarily designed for single-action settings. For the multi-action setting, \cite{tavakoli2018action} proposed a branching solution by modeling each action branch with another Neural Network layer. In this paper, we use a Multi-layer Perceptron (MLP) containing multiple layers of fully connected weights with sigmoid activation functions as a Decider Agent policy network. At the last layer, we branch for each action and use different weights:
\begin{equation}
    \begin{split}
    z &= o(W_{L-1}(\dots o(W_{1} s_t)\dots)),\\
    p_1 &= o(W_{L}^{(1)}z), \dots, 
    p_N = o(W_{L}^{(N)}z),\\
    \end{split}
    \label{eq:fusion}
\end{equation}
where $W_j$ is the weight matrix at layer $j$, $o$ represents the sigmoid activation, $z$ is the penultimate layer outputs, and $L$ is total number of layers. While the first layers extract the information from the current state vector, the last layers decide the next action by modeling each action independently. Therefore, during training, the first layers are jointly trained while the last layers are tuned for each model. 

\item Reward: The reward is the most important metric in RL since it defines the objective of the agent. We define the reward function for the Decider Agent as follows:
\vspace{-5pt}
\begin{equation}
{\small
r^{(\mathrm{Dec})}_{t+1} = 
\begin{cases}
  1 & \text{if } \hat{y}_{\text{final}} = y, \\
  -1 - \alpha\cdot \frac{\overline{\mathcal{E}}_t}{N}  & \text{otherwise}
\end{cases}
}
\label{eq:rw1}
\end{equation}
where $\alpha\in[0,1]$ is the size-penalization constant to force the agent include less number of models in the pool. Note that, the reward requires the final decision, $\hat{y}_{\text{final}}$, which is generated by the Aggregator Agent. 

MARL systems fundamentally carry stability issues since agents exhibit mutual dependence. In our experiments, we observed that splitting the reward function for each agent and performing a warm start resulted in more stable training. However, to perform a warm start on the Decider Agent, we need an evaluator metric to evaluate the created model pool. Thus, we substitute $\hat{y}_{\text{final}}$ with an interim prediction using plurality voting, which chooses the most voted decision based on the current model pool. The interim prediction stabilized the training and helped the policy network to converge to the best selection faster.
\end{itemize}

As shown in Figure \ref{fig:main}, the Decider Agent gets the current observation state $s_t$ from the multi-agent environment to perform multi-action prediction to create the new pool. The input query is, then, sent to each model in the pool to generate outputs, which are subsequently passed to the Aggregator agent.

\subsection{Aggregator Agent}
\vspace{-4pt}
This agent is responsible for generating the final decision based on the generated outputs by each model in the pool. Thus, the success of the selector agent would be undervalued if the aggregator agent fails to exploit the diversity of the outputs to arrive at the correct decision. As demonstrated in \cite{dietterich2000ensemble}, ensemble models can computationally achieve the global optimum by leveraging the local optima of individual models as starting points. We advocate that the generated outputs by each model, which can be the probabilities assigned to each option in a multiple-choice question, may locate the vicinity of the global optimum and the agent can perform a convex combination to reach the optimum.

We define the elements of an Aggregator Agent as follows:
\begin{itemize}[itemsep=0pt, parsep=0pt, topsep=0pt, left=0pt]
\item State: The agent observes the outputs generated by the models in the $m$ sized pool, which is denoted as $s_t=[\hat{y}_1, \dots, \hat{y}_m]$, where each output can be a sequence of words for open-ended questions (OEQ). In the case of multiple choice question (MCQ), we can represent the state as the probabilities assigned to each option, i.e., $s_t=[q_1, \dots,q_m]$, where $q_i\in[0,1]^k$ and $k$ is the number of choices.
\item Action: Based on the current state, the agent must generate a new output $\hat{y}_{\mathrm{final}}$ which we define as the action that the agent can take. In the case of multiple-choice questions, $a_t\in\{0,\dots,k\}$ is the action at time $t$ where each index indicates a choice. For open-ended questions, $a_t=\{v_1, \dots, v_s\}$ is the generated text with size of $s$, where $v$ is a token.
\item Policy: Similar to the decider agent, the aggregator agent provides mapping to the next action based on the current state:
\vspace{-5pt}
\begin{equation}
    {\small
    \begin{split}
        \pi_\phi(a_t\mid q_1,\dots,q_m) &= P(a\mid  q_1,\dots,q_m;\phi)\\
        \pi_\phi(a_t\mid s_t) &= P(a\mid s_t;\phi)
    \end{split}
    }
\end{equation}
Here, the first equation represents the model for MCQ, and the second is for the OEQ. In this paper, we focus on the MCQ and use a Multi-layer Perceptron (MLP) containing multiple layers of fully connected weights with sigmoid activation functions as aggregator policy network. We recommend referring to the studies \cite{jiang2023llm, tekin-etal-2024-llm, tekin2024h} as foundational resources for developing an ensemble policy network for OEQ.

\item Reward: We adopt the reward equation presented in Equation \ref{eq:rw1} for our Aggregator agent, excluding the size-penalization constant. Additionally, we initialize the process with a warm start using the outputs from the warm-started Decider Agent. 
\end{itemize}


\begin{algorithm}[t]
    \caption{MARL-Focal Train Algorithm}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} Warm-start samples $\mathcal{D}$, number of episodes $K$, Policy Networks $\pi_{\theta}$, $\pi_\phi$, Reward Functions $R^{\text{Dec}}, R^{\text{Agg}}$
        \STATE \textbf{Output:} Trained policies $\pi_{\theta}$, $\pi_{\phi}$
        \FOR{$i = 1$ to $K$}
        \STATE Include all models in the pool $\mathcal{E}_0=[1,1,\dots1]$
        \FOR{$x_t, y_t$ in $\mathcal{D}$}
        \STATE Create the state $s_t^{\text{Dec}} \leftarrow \{\mathcal{E}_{t-1}, \overline{\mathcal{E}}_t, \sigma^{1}_{t}, ..., \sigma_{t}^{m}\}$
        \STATE Make selection with policy $\mathcal{E}_{t+1} \leftarrow \pi_{\theta}(a_t|s_t^{\text{Dec}})$
        \STATE Obtain interim predictions $\hat{y}_{\mathrm{inter}} \leftarrow \mathrm{Vote}(\mathcal{E}_{t+1})$
        \STATE Calculate reward $r_t^{\text{Dec}}\leftarrow R^{\text{Dec}}(\hat{y}_{\mathrm{inter}}, y)$
        \STATE Append rewards to $\tau^{\text{Dec}}$ 
        \IF {$i \geq K/2$}
        \STATE Obtain model outputs $s_t^{\text{Agg}} \leftarrow \mathcal{E}_{t+1}(x_t)$
        \STATE Get final output $y_{\mathrm{final}} \leftarrow \pi_{\phi}(a_t|s_t^{\text{Agg}})$
        \STATE Calculate reward $r_t^{\text{Agg}}\leftarrow R^{\text{Agg}}(\hat{y}_{\mathrm{final}}, y)$
        \STATE Append rewards to $\tau^{\text{Agg}}$ 
        \ENDIF
        \ENDFOR
        \IF{$i < K/2$}
            \STATE update policy $\pi_{\theta}$ via $\tau^{\text{Dec}}$
        \ELSE
            \STATE update policy $\pi_{\phi}$ via $\tau^{\text{Agg}}$
        \ENDIF
        \ENDFOR
    \end{algorithmic}
    \label{algorithm:main}
\end{algorithm}


\subsection{Update Rule by MARL-Focal Algorithm}
In this section, we first introduce the update rule to train the policy network parameters and second the training loop. As shown in Figure \ref{fig:main}, we obtain two rewards, one for the decider and one for the aggregator for every incoming query. We store the resulting rewards to create the trajectory $\tau$ and compute $R(\tau)=r_t + \gamma r_{t+1} + \gamma^{2}r_{t+3} + \dots$, referred as the cumulative discounted rewards. Then, we rewrite the objective in equation \ref{eq:objective} as follows:
\begin{equation}
    \mathop{\max}_{\theta}J(\theta) = \mathbb{E}_{\tau\sim\pi_\theta}[R(\tau)],
    \label{eq:tau}
\end{equation}
The equation denotes the process of optimizing the policy network parameters $\theta$ to achieve the highest possible discounted cumulative reward. The policy network parameters should be optimized to increase the probability of action-state pairs that yield positive rewards. To achieve this, we perform gradient ascent optimization of equation \ref{eq:tau} by calculating $\nabla_{\theta}J(\theta)$. However, the true calculation requires differentiation of the state distribution, which we do not know the state dynamics and all the transition probabilities. First, the objective can be rewritten  by the Policy Gradient Theorem \cite{sutton1999policy}:
\begin{equation}
    \nabla_{\theta}J(\theta) = \mathbb{E}_{\pi_{\theta}}[\nabla_{\theta}\log\pi_\theta(a_t\mid s_t)R(\tau)].
    \label{eq:theory}
\end{equation}
The objective is now fully differentiable and does not depend on the state distribution. We perform the updates for each sampled trajectory $\tau$ by following Monte Carlo Reinforce (REINFORCE) algorithm which approximates the equation \ref{eq:theory} by:
\begin{equation}
    \sum_{t}\nabla_{\theta}\log\pi_\theta(a_t\mid s_t)R(\tau).
\end{equation}
However, the policy may get stuck in a local optimum because of the step taken on the error surface during this update. Therefore, current approaches employ Proximal Policy Optimization (PPO) \cite{schulman2017proximal} by performing clipped policy updates to prevent destructive weight updates. In our experiments, we used both of the update rules and set the clip parameter as a hyperparameter.

Overall we show the training loop in Algorithm \ref{algorithm:main}. The input $\mathcal{D}$ is the warm-start dataset that we use to train the agents. To ensure stable training, we first perform the update on the decider agent's policy and second on the aggregator agent's policy. This way, the aggregator agent can have more stable ensemble model pools, which facilitates effective learning-to-combine for the final prediction. Once both agents are initially trained, we implement a periodic update mechanism, where the policies of both agents are updated every 10 queries to maintain stability and adaptability. Next, we show the diversity metric calculation represented at line 6 in Algorithm \ref{algorithm:main}.

\subsection{Diversity Metrics and Focal Diversity}
\label{sec:div}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/focal_v2.png}
    \vspace{-12pt}
    \caption{All candidate ensemble teams from the model pool are plotted with their focal diversity scores, Fleiss Kappa, and Accuracy using the 4 popular LLM evaluation datasets. We use cubic interpolation to create surface and the red represents a higher performance score.}
    \label{fig:focal_first}
    \vspace{-10pt} 
\end{figure}

Consider a pool of $N$ base models, the total number of possible ensemble teams with size $S$ ($2\leq S \leq N$) is $2^{N}-N-1$~\citep{wu2021boosting}. 
A key question is how to perform ensemble pruning efficiently. We propose that the decider agent enables effective ensemble pruning by adaptively selecting ensembles with lower correlation and higher error diversity. As we argue in Section \ref{sec:prem}, these properties contribute to improved generative performance of the ensemble model. Therefore, in this section, we introduce the focal negative correlation metric specifically designed to detect error correlation among candidate models. As shown in Figure \ref{fig:focal_first}, the focal diversity and the common metrics e.g. Fleiss' Kappa \cite{fleiss} measure the amount of agreement create a surface where the Decider Agent moves to find the best ensemble combination.

\textbf{Focal Negative Correlation \& Focal Diversity.\/}
The focal negative correlation metric, $\rho^{focal}$ is used to quantify the level of error diversity among the component models of an ensemble concerning each model within the ensemble. The focal diversity metric $\lambda^{focal}$ is used to quantify the general error diversity of the ensemble by taking into account all focal negative correlation scores of an ensemble.
Let $\mathcal{E}$ denote an LLM ensemble composed of $N$ models: $\mathcal{M}_1, \dots, \mathcal{M}_N$, we choose one of the $N$ base models each time as the focal model to compute the focal negative correlation score of this ensemble, denoted as $\rho^{focal}(\mathcal{M}_i; \mathcal{E})$. We define the focal diversity of this ensemble team by the average of the $N$ focal negative correlation scores.
The procedure of computing the focal negative correlation score of $\rho^{focal}$ is as follows: 
(i) select a model among the set of $N$ models as the \textit{focal} model, (ii) extract all queries from the historical data within a time window of length $T$ where the focal model has failed, and compute the focal negative correlation score (iii) repeat the previous steps until all $N$ focal negative correlation scores are obtained. $\rho^{focal}_1, \dots, \rho^{focal}_N$, and (iv) compute the average over the scores to obtain the focal diversity of ensemble $\mathcal{E}$, denoted by $\lambda^{focal}(\mathcal{E})$:
\vspace{-4pt}
\begin{equation}
{\small
\begin{split}
\lambda^{focal}(\mathcal{E})=\frac{1}{N}\sum_{\mathcal{M}_i \in \mathcal{E}} \rho^{focal}(\mathcal{M}_i; \mathcal{E})\\
\rho^{focal}(\mathcal{M}_i; \mathcal{E} ) = 1 - \frac{P(2)}{P(1)}\\
P(2)=\sum_{j=1}^{N}\frac{j(j-1)}{N(N-1)}p_j, \;
P(1)=\sum_{j=1}^{N}\frac{j}{N}p_j
\end{split}
}
\end{equation}
Here $p_i$ is the probability that $i$ number of models fail together on a randomly chosen episode. We calculate as $p_i={n_i}/T$ where $n_i$ is the total number of episodes that $i$ number of models failed together on the validation set and $T$ is the total number of queries. The term $P(2)$ represents the probability of two randomly chosen models simultaneously failing on an episode, while the denominator, $P(1)$, represents the probability of one randomly chosen model failing on an episode. The terms beneath $p_j$ values are the probability of the chosen model being one of the failures. For example, when $N=3$, there are three cases of model failures; one, two, or three models can fail simultaneously. If one model fails, the chance of selecting the failed model is $1/3$. Similarly, for two models, it is $2/3$, and for three models, it is $1$.
In the case of minimum diversity, the probability of two randomly chosen models failing together comes down to the probability of one of them failing, which makes the fraction term equal to 1 and $\rho^{focal} = 0$. Similarly, in the case of maximum diversity, there are no simultaneous failures. Hence, the nominator equals 0 and $\rho^{focal} = 1$. As shown in Figure \ref{fig:focal_first}, the focal diversity is correlated with the performance of the ensemble. Secondly, there are smaller ensemble teams with higher focal diversity, less agreement, and high accuracy. This supports the design of the Decider Agent with size penalty term.


\section{Experiments}

We validate the effectiveness of our MARL-Focal approach through extensive evaluations of benchmarks representing multiple-choice questions. In addition, we show that the decider agent is also applicable to open-ended questions. Experiments show that the MARL-Focal framework can efficiently improve the performance of base models by creating a generic and more balanced fusion model. Furthermore, we examine and report the performance and behavioral changes of our agents through training.  

\subsection{Dataset and Framework Parameters}

The experiments contain 4 different benchmarks in multiple-choice question format: MMLU\cite{hendrycks2020measuring}, BBH \cite{suzgun2022challenging}, MUSR \cite{sprague2023musr}, and GPQA \cite{rein2023gpqa} are the benchmarks present in the HuggingFace leaderboard \cite{open-llm-leaderboard}. However, we also add GSM8K\cite{cobbe2021training}, which contains open-ended math problems. For this dataset, we transform the outputs of the models into probability distributions by conducting multiple inference passes (10 times) shown in \cite{tekin-etal-2024-llm}. Specifically, we count the frequency of each predicted answer and normalize it by dividing the frequency by the total number of passes. This process yields a probability distribution over the possible outputs. While GSM8k contains a test set, the other datasets are not split as train-test, thus, we perform a 1:5 ratio of test and train split following \cite{liu2024dynamic, tekin-etal-2024-llm}. We use the training split to perform the warm start shown in Algorithm \ref{algorithm:main}. As the performance metric, we used accuracy in all 5 datasets. 

In our second experiment, we evaluate the adaptability of the MARL-Focal Decider Agent in the context of selecting the most appropriate model that aligns with the specific skill required by the query. Accordingly, we fine-tuned three Llama-2-7b models for helpfulness, safety, and truthfulness using Alpaca-cleaned \cite{alpaca_clean}, BeaverTails \cite{ji2024beavertails}, and TruthfulQA \cite{lin2021truthfulqa} datasets, respectively. Our goal is in this design to select the correct model for the incoming query via Decider Agent. To measure, whether the given answer is helpful, truthful, and safe we follow the evaluation details shown in \cite{tekin2024h}. For helpfulness, the alpaca-eval library calls GPT4 \cite{achiam2023gpt} to compare with the answer given by text-davinci-003  \cite{brown2020language} and selects a preference. Thus, we report the Win Rate (\%) against text-davinci-003. In the case of safety, we calculate the amount of flagged output (\%) by a safety model, beaver-dam-7b, \cite{ji2024beavertails}. The model flags an output if it fits under 14 different unsafe categories. Lastly, the truthfulness score is measured by the trained text-davinci-003 models called GPT-Judge as instructed in \cite{lin2021truthfulqa}. We report the amount of output that the trained GPT-Judge model found truthful (\%) and informative (\%) among test queries.

In our experiments, we used 2 layered MLP policy networks for both the Decider Agent and Aggregator Agent. We set the time window $T=500$, size penalty constant $\alpha=0.1$, learning rate $lr=0.001$, clip parameter for PPO $\epsilon=0.02$, and discount factor $\gamma=0.8$. 

\begin{table*}[hbt!]
  \begin{adjustbox}{width=0.8\textwidth, center}
    \centering
    \small
    \begin{tabular}{l c c c c c c}
        \hline
        \multirow{2}{1.7cm}{Model Name} & \multirow{2}{1.7cm}{Model ID} & MMLU & GSM8k & BBH & MUSR & GPQA \\
        & & (Acc \%)$\uparrow$ & (Acc \%)$\uparrow$ & (Acc \%)$\uparrow$ & (Acc \%)$\uparrow$ & (Acc \%)$\uparrow$  \\
        \hline
        Phi-2b & 1 & $55.82$ & $68.85$ & $44.55$ & $41.90$ & $28.89$ \\
        Gemma-2b & 2 & $40.26$ & $24.03$ & $11.76$ & $1.68$ & $11.43$ \\
        Gemma-7b & 3 & $63.87$ & $73.04$ & $36.23$ & $46.59$ & $27.78$ \\
        Llama-7b & 4 & $41.79$ & $10.87$ & $10.35$ & $3.76$ & $2.24$ \\
        Mistral-7b & 5 & $59.67$ & $56.21$ & $22.17$ & $10.68$ & $5.59$ \\
        Llama-13b & 6 & $53.40$ & $41.74$ & $39.66$ & $44.90$ & $28.89$ \\
        Llama-70b & 7 & $68.53$ & $58.89$ & $28.03$ & $41.54$ & $30.00$ \\
        Mixtral-8x7b & 8 & $70.42$ & $73.91$ & $41.87$ & $48.85$ & $31.11$ \\
        \hline
        MARL-Focal & Dynamic & $\mathbf{73.24}$ & $\mathbf{76.06}$ & $\mathbf{49.04}$ & $\mathbf{51.93}$ & $\mathbf{32.18}$ \\
        \hline
    \end{tabular}
    \end{adjustbox}
    \caption{MARL-Focal performance in popular LLM evaluation datasets. We create the ensemble sets using decider agent dynamically.}
    \label{table:gsm8k}
    \vspace{-12pt}
\end{table*}

\begin{table*}[hbt!]
  \begin{adjustbox}{width=0.9\textwidth, center}
    \centering
    \small
    \begin{tabular}{l c c c c c}
        \hline
        \multirow{2}{1.7cm}{Aligned Task} & \multirow{2}{1.5cm}{Model ID} & Helpfulness & Safety & Truthfulness & \multirow{2}{1.5cm}{Avg. $(\%)\uparrow$} \\
        \cline{3-5}
        & & Win Rate$(\%)\uparrow$ & Flagged$(\%)\downarrow$ & (Truth.+Info.)/2$(\%)\uparrow$ & \\
        \hline
        Llama-2-7b & 0 & $13.79$ & $42.00$ & $21.03$ & $-2.39$ \\
        Helpful Model & 1 & $\mathbf{61.80}$ & $48.40$ & $62.59$ & $25.33$ \\
        Safe Model & 2 & $58.40$ & $35.60$ & $63.81$ & $28.87$ \\
        Truthful Model & 3 & $0.78$ & $\mathbf{5.20}$ & $\mathbf{66.74}$ & $20.77$ \\
        \hline
        MARL-Focal (Decider) & Dynamic & $56.4$ & $33.3$ & $64.37$ & $\mathbf{29.16}$ \\
        \hline
    \end{tabular}
    \end{adjustbox}
    \vspace{-3pt}
    \caption{We compare MARL-Focal (Decider) with the standard fine-tuned Llama-2-7b on the helpfulness, safety, and truthfulness datasets as a baseline. We measure the performance of the Decider Agent whether it can select the correct aligned model based on the incoming query. Avg. score is calculated as (Helpfulness - Safety + Truthfulness) / 3}
    \label{table:ablation}
    \vspace{-12pt}
\end{table*}

\begin{figure*}[hbt!]
    \centering
    \includegraphics[width=1\textwidth]{figures/reward_cost.png}
    \caption{The first two plots show performance for Decider and Aggregator agents for each dataset. The shaded regions represent the one standard deviation distance to the mean for 5 experiments. The third plot shows the performance and cost analysis for the LLMs and MARL-Focal in the MMLU task.}
    \label{fig:macro}
    \vspace{-12pt}
\end{figure*}


\begin{table}[t]
    \begin{adjustbox}{width=0.5\textwidth, center}
    \centering
    \begin{tabular}{l c c c}
        \hline
        Method & Model ID & MMLU & GSM8k \\
        \hline
        More Agents \cite{li2024more} & 6 & $51.09$ & $61.00$ \\
        More Agents \cite{li2024more} & 7 & $60.05$ & $77.00$ \\
        LLM-Blender \cite{jiang2023llm}  & 12345678 & $44.01$ & $40.41$ \\
        Majority Voting  & 12345678 & $68.06$ & $72.31$ \\
        Mixtral-8x7b  & 8 & $70.53$ & $71.16$ \\
        DyLAN \cite{liu2024dynamic} & - & $70.5$ & - \\
        LLM-TOPLA \cite{tekin-etal-2024-llm} & $378\mid138$ & $72.77$ & $\mathbf{79.01}$ \\
        \hline
        MARL-Focal & Dynamic & $\mathbf{73.24}$ & $76.06$ \\
        \hline
    \end{tabular}
    \end{adjustbox}
    \caption{ We compare our approach with the other ensemble methods in the literature.}
    \label{table:comparison_ens}
    \vspace{-12pt}
\end{table}

\subsection{Performance of MARL-Focal}
\vspace{-4pt}

The main results for the 5 benchmarks are shown in Table \ref{table:gsm8k}. The pool contains 8 models ranging from 2b to 70b parameters. In all the benchmarks MARL-Focal successfully improves the best base model performance. Specifically, it improves Mixtral-8$\times$7b, by $2.8\%$ in MMLU, $2.15\%$ in GSM8k, $3.08\%$ in MUSR, and $1.07\%$ in GPQA datasets while improving Phi-2b $4.49\%$ in BBH. The results indicate that the Decider Agent creates an effective model pool such that the Aggregator Agent exploits the differences and generates more correct output. Due to the dynamic nature of MARL-Focal, we cannot provide the model IDs forming the ensemble set.

The performance of each agent during the training loop is shown in Figure \ref{fig:macro}. First, while MUSR, BBH, and GPQA require small steps with low learning rates to converge, the other datasets—GSM8K and MMLU—converge more quickly. Second, the accuracy achieved by the Decider Agent using the interim prediction method with plurality voting is lower than that of the Aggregator Agent. Finally, we compare the cost of each base model with that of MARL-Focal by plotting performance against the average inference cost (\$) for the MMLU task, as shown in the third plot of Figure \ref{fig:macro}. MARL-Focal shows the best performance while being the second most costly method among all the models. The reason is that the Decider Agent selects either the Mixtral or LLama model in its pool alongside a smaller model.  

In Table \ref{table:comparison_ens}, we compare the performance of MARL-Focal with the ensemble methods in the literature and a simple baseline using MMLU and GSM8k datasets. MARL-Focal shows the best performance in MMLU by $0.54\%$ improvement and shows third best performance in GSM8k. However, the LLM-TOPLA model is a supervised approach and More Agents requires 40 LLMs. MARL-Focal, on the other hand, offers an adaptive solution that is less costly. 

\subsection{Open-Ended Questions and Alignment Selection}

The results of aligned model selection are shown in Table \ref{table:ablation}. Comparing the performance of MARL-Focal with the pretrained LLama-2-7b and individually aligned models on each dataset, we observe that the  MARL-Focal model demonstrates the best average performance across all datasets, showing over $15\%$ improvement compared to the helpful model in safety task, more than $1.5\%$ improvement over the safe model in truthfulness task, and over $50\%$ better performance than the truthful model. Since the Decider model is solely responsible for selecting base models, its performance is inherently limited by the capabilities of the best-performing individual model for that specific task. 

\vspace{-4pt}
\section{Conclusion}
\vspace{-4pt}
In conclusion, we presented a novel approach to model selection and aggregation by formulating the problem as an infinite-horizon Markov Decision Process and introducing a two-stage framework. The first stage utilizes a Decider Agent to dynamically select and group models based on diversity metrics and error correlations, ensuring optimal ensemble formation using the focal diversity score. The second stage leverages an Aggregator Agent to produce final decisions by synthesizing outputs from the selected model pool. Extensive evaluations on five benchmark datasets demonstrate that our proposed MARL-Focal framework not only outperforms the best individual models but also surpasses state-of-the-art supervised approaches. Furthermore, experiments on the Alpaca-Eval, BeaverTails, and TruthfulQA datasets highlight MARL-Focal’s ability to construct systems that are more helpful, safe, and truthful, showcasing its potential for real-world applications.

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \appendix
% \onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
