\section{Related Work}
\vspace{-4pt}
\textbf{Ensemble Learning in LLMs.} Many works exploit majority voting to perform inference-time ensemble  \cite{wang2022self, fu2022complexity, li2022advance, wang2022rationale}. The downside of majority voting is the definition of equality between divergent answers. %Compared to math problems or multiple-choice problems, consensus-based approaches like weighted majority voting may do poorly for generative queries. 
Two threads of research further improve majority voting, one work utilizes the BLEU score as the heuristic to compare answers \cite{li2024more} another is to enhance the BLEU score-based answer combination method by either assigning weights \citep{yao2024tree} or by creating a debate environment \citep{liang2023encouraging, wan2024knowledge, du2023improving, chan2023chateval}. Due to the lengthy and complex prompt strategies of former works, supervised summarization LLM ensemble methods are proposed \cite{jiang2023llm, tekin-etal-2024-llm}.

\textbf{Ensemble Reinforcement Learning.} Creating an adaptive ensemble model with RL such as \cite{song2023ensemble, chua2018deep} is an extensive area covered in many contexts such as time-series prediction, \cite{liu2020new, nemeth2022split, perepu2020reinforcement}, ensemble pruning \cite{partalas2009pruning, liu2020instance}, and in context of LLMs \cite{ouyang2022training, liu2024rl, monea2024llms, zhang2021multi, sun2024llm, liu2024dynamic}. To the best of our knowledge, our work is the first approach in adapting reinforcement learning for both the pruning and generation stage in the context of LLMs.