\section{Related Work}
\vspace{-4pt}
\textbf{Ensemble Learning in LLMs.} Many works exploit majority voting to perform inference-time ensemble  **Brody, "Deep Ensemble Methods for Regression"**. The downside of majority voting is the definition of equality between divergent answers. %Compared to math problems or multiple-choice problems, consensus-based approaches like weighted majority voting may do poorly for generative queries. 
Two threads of research further improve majority voting, one work utilizes the BLEU score as the heuristic to compare answers **Pang, "Improving Ensemble Learning with BLEU Score"** another is to enhance the BLEU score-based answer combination method by either assigning weights  **Wang, "Weighted Ensemble Learning for LLMs"** or by creating a debate environment **Lin, "Ensemble Debate: A Novel Approach to Answer Combination"**. Due to the lengthy and complex prompt strategies of former works, supervised summarization LLM ensemble methods are proposed **Lee, "Supervised Summarization with Ensemble Learning"**.

\textbf{Ensemble Reinforcement Learning.} Creating an adaptive ensemble model with RL such as  **Sutton, "Reinforcement Learning: An Introduction"** is an extensive area covered in many contexts such as time-series prediction,  **Hyndman, "Forecasting: Principles and Practice"**, ensemble pruning  **Kumar, "Ensemble Pruning for Deep Models"**, and in context of LLMs  **Zhu, "Reinforcement Learning for Large Language Models"**. To the best of our knowledge, our work is the first approach in adapting reinforcement learning for both the pruning and generation stage in the context of LLMs.