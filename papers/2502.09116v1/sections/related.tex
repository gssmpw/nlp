\section{Related Work}

\paragraph{Random scheduling}
Similar assumptions to the \model have been explored by previous work. In message passing, Bracha and Toueg~\cite{BrachaT85} define the fair scheduler, which ensures that in each message round, there is a non-zero constant probability that every correct process receives messages from the same set of correct processes. Under this scheduler, they proposed deterministic asynchronous binary consensus protocols for crash and Byzantine fault models. 
More recently, Tusk~\cite{narwhal} and Mahi-Mahi~\cite{mahimahi} employ a form of random scheduling. Their random scheduler can be seen as a special case of ours: they only consider the $n=3f+1$ case and assume a standard round-based model with the subset of processes that a process ``hears from'' in a given round chosen \textit{uniformly} at random among all possibilities. They leverage the random scheduler to increase the probability to commit at each round, and thus to reduce latency, whereas our paper focuses on circumventing impossibility results in standard asynchrony, at different ratios of fault-tolerance. They conduct experiments without randomization on a wide-area network, without observing loss of liveness, which can serve as motivation for our work.

In shared memory, Aspnes~\cite{Aspnes02} defines noisy scheduling, in which the adversary may chose the schedule, but that adversarial schedule is perturbed randomly. Under this assumption, deterministic asynchronous consensus becomes achievable. Also in shared memory, previous work introduce a \textit{stochastic scheduler}~\cite{AlistarhSV15,AlistarhCS16}, which schedules shared memory steps randomly. This line of work shows that many lock-free algorithms are essentially wait-free when run against a stochastic scheduler, because the schedules that would break wait-freedom have negligible probability.

\paragraph{Randomized consensus}
A large body of research leverages random coins to circumvent the FLP impossibility result~\cite{FLP}, which states that deterministic consensus is impossible in crash-prone asynchronous systems.
In this approach, protocols relax deterministic termination to probabilistic termination, assuming processes have access a source of (cryptographically-secure) randomness that cannot be predicted by the adversary. Randomized consensus protocol employ either local coins~\cite{Ben-Or83, ritas, waterbear}---which produce randomness independently and locally at each process, without coordination with other processes---or common coins~\cite{Rabin83, oracles-constantinople, signature-free, narwhal, mahimahi, bullshark}---which, through the use of coordination and strong cryptographic primitives, ensure that all correct processes receive the same random output with some probability.

Our algorithms for the $n=3f+1$ setting resemble existing coin-based random consensus protocols, with the randomness moved from the process logic to the schedule. In fact, all of the coin-based consensus protocols we examined can be transformed, with minor changes, into deterministic (coin-less) algorithms in the \model. A natural question, then, is whether our model is equivalent to the standard asynchronous model with coin tosses. It is not: in the standard model, achieving safety \textit{whp} when $n < 3f$ is impossible, even if processes have access to randomness. This is because of a standard split-brain argument: the adversary can partition correct processes into two sets that never exchange messages, allowing Byzantine to force different decisions in each set. By contrast, our model makes long-lived network partitions occur with negligible probability, allowing safety \textit{whp} even for $n<3f$.

% The randomized consensus protocol by Ben-Or~\cite{Ben-Or83} uses a local coin and requires exponential expected time to converge in the worst case. Subsequent works like RITAS~\cite{ritas} and WaterBear~\cite{waterbear} showed local-coin-based protocols can be made practical. Rabin~\cite{Rabin83} showed that the same type of protocol could achieve
% termination in expected constant time by resorting to a common coin. The construction of this common coin typically requires strong cryptographic primitives. Some crypto-based work: Cachin et al.~\cite{oracles-constantinople}---leveraged threshold signatures to devise a protocol with constant expected time and $O(n^2)$ communication, MostÃ©faoui et al.~\cite{signature-free}---proposed a similar protocol that is signature-free, based on a weak common coin. 

\paragraph{Probabilistic quorum systems}
A line of work on probabilistic quorum systems~\cite{prob-quorums, Yu06} relaxes quorum  intersection to be probabilistic rather than deterministic, and allows for probabilistic correctness guarantees. However, they are vulnerable to an adversarial scheduler~\cite{AiyerAB05}. ProBFT~\cite{probft} addresses this through the use of verifiable random functions for quorum selection. These works are similar to ours in that correctness is probabilistic rather than deterministic, but their approach focuses on the $n=3f+1$ setting, and is mainly aimed at scalability and efficiency (e.g., communication complexity), whereas we aim to circumvent impossibilities in standard asynchrony across various fault tolerance ratios.

% \paragraph{Partially-synchronous consensus}
% \changed{Our work stands to impact practical implementations in the related area of partially-synchronous consensus protocols. These circumvent the FLP impossibility by assuming that the system eventually becomes synchronous: after an unknown global stabilization time (GST), messages are delivered within a known delay $\Delta$. However, the liveness of practical systems is notoriously difficult to prove in partial synchrony, leading many published systems to only provide informal liveness arguments~\cite{narwhal, mysticeti, pbft}. Furthermore, establishing the delay $\Delta$ is difficult in Byzantine systems, leading practitioners to often set conservative timeouts for fear of losing liveness, to the detriment of performance. However, this can be alleviated in our model. Asynchronous algorithms do not require periods of synchrony, and thus do not require timeouts, to ensure liveness. Practical implementations may still choose to employ timeouts to improve performance (e.g., waiting for more messages after having received $n-f$ messages in a round, up to a timeout, can improve the chance of committing sooner), but since timeouts cannot impact liveness, practitioners are free to explore various timeout strategies without the fear of breaking liveness. The same is true of coin-based algorithms for the standard asynchronous model, but they have the disadvantage of requiring cryptographically-secure randomness, which is slow in practice, defeating the purpose of resorting to asynchronous algorithms for improved  performance.}

% \begin{itemize}
% \item \url{https://drops.dagstuhl.de/storage/00lipics/lipics-vol324-opodis2024/LIPIcs.OPODIS.2024.20/LIPIcs.OPODIS.2024.20.pdf}
% \item The protocol in \Cref{sec:2f+1} is similar the Dolev-Strong synchronous protocol~\cite{dolev-strong}.
% \end{itemize}


