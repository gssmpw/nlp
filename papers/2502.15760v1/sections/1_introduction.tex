\vspace{-0.4cm}
\section{Introduction}
\vspace{-0.2cm}
Foundation models~\citep{openai2024gpt4technicalreport, geminiteam2024geminifamilyhighlycapable} open up the possibilities to build agents that make intelligent decisions in the real world~\citep{liu2023agentbenchevaluatingllmsagents}. While prompting off-the-shelf language models with specific instructions is one way to get them to make decisions, this is not good enough for attaining goals and maximizing rewards that are critical in downstream tasks~\citep{zeng2023agenttuningenablinggeneralizedagent, chen2023fireactlanguageagentfinetuning}. Part of the reason is the lack of sufficiently diverse decision-making data for training large models~\citep{gur2023understandinghtmllargelanguage}. But perhaps a more fundamental challenge is that simply imitating Internet data is not good enough for training models ``how'' to act intelligently, reduce uncertainty, and achieve goals in non-stationary real-world decision making settings~\citep{bai2024digirltraininginthewilddevicecontrol, ma2024cautionenvironmentmultimodalagents}.

Recently, the community has been turning towards using reinforcement learning (RL) methods for training agentic policies. RL avoids the shortcomings of imitation and prompting, by explicitly training the policy to solve tasks~\citep{zhou2024archertraininglanguagemodel, verma2022chaichatbotaitaskoriented, snell2023offlinerlnaturallanguage, abdulhai2023lmrlgymbenchmarksmultiturn}. That said, the best performing RL methods today for improving a policy in multi-step agentic tasks rely critically on interaction due to the use of policy gradient updates~\citep{yao2023webshopscalablerealworldweb} coupled with Monte-Carlo values~\citep{bai2024digirltraininginthewilddevicecontrol, putta2024agentqadvancedreasoning, shao2024deepseekmath},
which often require sufficient amounts of on-policy data to get a low-variance learning signal. The amount of on-policy data needed is likely only larger in non-stationary and dynamic environments~\citep{bai2024digirltraininginthewilddevicecontrol}. 

If on the other hand, we could train a critic (i.e., an \emph{action-value function}) that could score a policy's action reliably, \emph{without} needing to actually simulate the policy behavior over multiple steps several times, we could simplify our recipe for policy improvement without costly simulations. With this motivation, in this paper, we build a simple approach to train a VLM Q-function. In our problem setting of mobile device control, this corresponds to training a VLM-based Q-function that can provide a score for every snapshot of the phone screen and an action, represented by laying the cursor over this snapshot.
Our method, \ourmethod{}, trains a Q-function for device control entirely using historical data collected by (potentially suboptimal) agents.

\begin{figure*}[!t]
     \centering
     \begin{subfigure}[b]{1.00\textwidth}
         \centering
    \includegraphics[width=0.99\textwidth]{figures/teaser.pdf}
     \end{subfigure}
     ~ \vspace{-0.2cm}
     %%HB.2.13note don't remove the line above
     \caption{\footnotesize{\textbf{Comparing \ourmethod{} with on-policy policy-gradient methods.} $(s, a)$ rollout pairs that are learned are marked \textcolor{ForestGreen}{green} in the buffer. Typically, policy-based methods utilizes a state value function to filter out promising state-action pairs, and requires online data to improve. In contrast, \ourmethod{} learns a state-action (\textbf{Q}) value function through TD-learning on offline data, and re-sample an amount of actions for each state. This Q-function is then used to rank the re-sampled action to learn a policy using the best action under each state. Digi-Q results in much higher sample efficiency than policy-based methods, thus it can be applied even in a fully offline setting.}}
        \label{fig:teaser} 
\end{figure*}

To train Q-functions effectively, \ourmethod{} handles or circumvents (via algorithmic modifications) a number of challenges posed by value learning at scale: \textbf{(i)} instability in running temporal-difference (TD) learning for training value functions with large models~\citep{kumar2021dr3} and \textbf{(ii)} inefficiency of TD backups per unit amount of ``compute'' (i.e., gradient steps spent)~\citep{chebotar2023qtransformerscalableofflinereinforcement} (see~\Cref{app:compute_efficiency}).
\ourmethod{} does so by training on top of a frozen intermediate layer \emph{representation} of the VLM instead of training all parameters of the VLM. For attaining the best performance, representations from off-the-shelf VLMs are not good enough, since they often do not contain feature information crucial for predicting actions or their consequences. Therefore, \ourmethod{} prescribes running an initial phase of representation fine-tuning to prime representations of a VLM to be more amenable to TD-learning. 

To specifically understand benefits of learning the Q-function, we note that while state-only Monte-Carlo value functions~\citep{bai2024digirltraininginthewilddevicecontrol, zhai2024finetuninglargevisionlanguagemodels} can only evaluate the efficacy of a single action that was actually executed at any given state, a good Q-function provides us with reliable estimates of the future expected reward for \emph{multiple} actions at the same state, without needing to actually roll these candidates out. This allows us to develop a Best-of-N policy-extraction objective that trains the agentic policy to imitate the best-rated action per the Q-function without any additional interaction. The difference between policy-based methods and \ourmethod{} is illustrated in~\Cref{fig:teaser}.

The main contribution of this work is \ourmethod{}, an offline approach to train a VLM-based Q-function for building device-control agents. \ourmethod{} represents and trains Q-functions on top of intermediate representations from a VLM, fine-tuned to especially consist of actionable information. \ourmethod{} unlocks the use of a Best-of-N policy extraction objective to make the most effective use of a Q-function in obtaining a policy. The agent produced by running \ourmethod{} on offline data outperforms prior approaches that also extract policies from offline data in the problem setting of Android device control~\citep{rawles2023androidwildlargescaledataset} with 21.2\% of relative improvement over the best-performing prior method, even though these domains remain challenging for state-of-the-art proprietary models~\citep{liu2024visualagentbenchlargemultimodalmodels, bai2024digirltraininginthewilddevicecontrol}. 
\emph{To the best of our knowledge, this work is the first to successfully scale state-action Q-value functions to realistic agent tasks with VLMs and show significantly improved performance.}