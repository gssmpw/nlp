
\section{More Qualitative Examples}

\subsection{Environment Errors} \label{app:env-errors}

We observe that several tasks has problems working in the environment introduced in~\citet{bai2024digirltraininginthewilddevicecontrol}. We observe that (1) the \url{newegg.com} domain has a high probability of blocking the agent from accessing it, and (2) the \url{costco.com} domain prevents the agent from typing the \texttt{<ENTER>} key. Examples are shown in~\Cref{fig:qual-env-error}. These problems were not observed in ~\citet{bai2024digirltraininginthewilddevicecontrol}. This is the main reason why some scores on the AitW Webshop subset in this paper falls a little behind~\citet{bai2024digirltraininginthewilddevicecontrol}.

\begin{figure}[!htp]
     \centering
    \begin{subfigure}[b]{0.85\textwidth}
         \centering
    \includegraphics[width=\textwidth]{figures/qual-env-error.pdf}
     \end{subfigure}
        \caption{\textbf{Environment errors.} These errors are systematic and can not be removed by the agent.}
        \label{fig:qual-env-error}
\end{figure}


\subsection{Example Trajectory Comparing REINFORCE and Best-of-N Loss} \label{app:pg-example}

We show a typical trajectory produced by the agent trained with REINFORCE in~\Cref{fig:pg-example}. We observe that the agent frequently diverges from the target and is too ``stubborn" to recover from errors. 

In this task towards searching for an item on costco.com, the agent has successfully arrived at costco.com, but (1) it takes some bad actions and (2) cannot recover. Specifically, after the agent clicks the warehouse button, it keeps clicking on the same button for 10 times until it clicks on somewhere else. This situation rarely appear in any trajectories collect by the agent trained with the Best-of-N loss.

\begin{figure}[!t]
     \centering
    \begin{subfigure}[b]{0.85\textwidth}
         \centering
    \includegraphics[width=\textwidth]{figures/pg-bad-example.pdf}
     \end{subfigure}
        \caption{\footnotesize{\textbf{Example trajectory of the agent trained with REINFORCE and Best-of-N loss.} Results show that the agent trained with REINFORCE tends to get stuck at a specific state because it's ``stubborn", while agent trained with Best-of-N loss effectively solves the task.}}
        \label{fig:pg-example}
        \vspace{2mm}
\end{figure}

\subsection{Benefits of dynamic programming} \label{app:dyn-prog}

\begin{figure}[t]
     \centering
    \begin{subfigure}[b]{0.85\textwidth}
         \centering
    \includegraphics[width=\textwidth]{figures/stitching-example.pdf}
     \end{subfigure}
        \caption{\footnotesize{\textbf{Trajectory examples showing benefits of Q-functions.} Our method can combine the best of a successful but lengthy (\textit{A}) trajectory and a failed but short trajectory (\textit{B}), to produce successful and short trajectories (\textit{C}).}}
        \label{fig:stitching-example}
\end{figure}

An appealing property of value-based RL is \emph{dynamic programming}: the ability to identify optimal behaviors from overall suboptimal rollouts. We present a qualitative example in~\Cref{fig:stitching-example} that illustrates this ability of \ourmethod{} in learning optimal behaviors from sub-optimal data. 
%%AK.10.1: readers will wonder -- how cherry picked is this?
In this example, trajectory (A) and (B) are from the offline dataset where trajectory (A) successfully completes the task but has many redundant actions while trajectory (B) does not have redundant actions but fails to complete the task. It turns out that \ourmethod{} is able to learn a policy that performs dynamic programming with trajectory (A) and (B) to produce a trajectory (C) that completes the task in the most efficient way. Neither trajectory (A) nor (B) is the optimal trajectory for solving the task but this example shows the ability of \ourmethod{} to learn an optimal policy from sub-optimal data, which is theoretically impossible through imitation alone.
