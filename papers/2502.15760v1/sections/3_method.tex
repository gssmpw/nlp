\vspace{-0.2cm}
\section{Preliminaries and Problem Setup}
\label{sec:prelims}
\vspace{-0.1cm}

We aim to build value functions for training agents in the domain of device control, where we wish to accomplish pixel-based interactions on virtual devices, following similar protocol as past work~\citep{bai2024digirltraininginthewilddevicecontrol}. In this section, we will discuss the setup for this problem, followed by reviewing terminology, notation, and background that would be useful in developing our approach in the next section.

\vspace{-0.2cm}
\subsection{Problem Setup: Android Device Control}
\vspace{-0.1cm}
We scope our study in the domain of pixel-based Android device control~\citep{bai2024digirltraininginthewilddevicecontrol, zhang2023appagentmultimodalagentssmartphone, rawles2023androidwildlargescaledataset}. Each episode in this domain starts with a fully-functioning Android emulator reset to the home screen, and a task is randomly drawn from a task pool represented by natural language instructions. The agent needs to complete the task through pixel-based interactions with the device as illustrated in Figure~\ref{fig:teaser}. The actions that the agent can take are primitive pixel-level commands such as clicking at a coordinates and typing text. Concretely, given a screenshot of a phone, we want the agent to output a string command such as ``click (0.8, 0.2)'' to be executed in the environment at the current step, where 0.8 and 0.2 are 0-1 normalized x-y coordinates in the screen. This domain is known to be more general and challenging than web navigation alone or link-based device control~\citep{bai2024digirltraininginthewilddevicecontrol},
%%AK.1.21: can we also cite some other prior work here since just Bai et al. 2024 might be too closely identifiable to our work
and present many real-world challenges of device stochasticity and dynamism, such as unpredictable distractors like pop-ups and technical glitches like incomplete website loading. Following \citet{pan2024autonomousevaluationrefinementdigital,bai2024digirltraininginthewilddevicecontrol}, the agents are evaluated via binary 0/1 rewards from a proprietary model (i.e., Gemini 1.5 Pro~\citep{geminiteam2024geminifamilyhighlycapable}) that makes a verdict of whether the specific task has been completed at each step. We want to use this signal to learn a value function that can accurately predict the future outcome that the agent would attain if it were to execute a given action at a given snapshot, without actually executing this rollout. More importantly, we wish to learn this value function using a static dataset $\mathcal{D}$ storing historical past interaction data and use it to learn an agentic policy.

\iffalse
\vspace{-0.2cm}
\subsection{Formalizing Off-Policy Device Control in a Hierarchical MDP}
\label{sec:hierarchical_mdp}
\vspace{-0.2cm}
To design a value-based RL approach for training device-control GUI agents, we subscribe to the hierarchical MDP framework from \citet{zhou2024archertraininglanguagemodel}. This hierarchical MDP consists of MDPs at two levels: \textbf{(1)} a high-level MDP where each step is an interaction with the external environment, and \textbf{(2)} a few low-level MDPs embedded inside each action in the high-level MDP, where each step is an independent natural language token. Terminology wise, we define the \textbf{state} $s_t$ in the high-level MDP as the log of the interaction history of the agent with the environment thus far concatenated to the current observation. Each \textbf{action} $a_t$ in the high-level MDP is a sequence of tokens that are directly applied to interact with the environment such as ``type box [2]: wikipedia of chocolate''. Each action $a_t^h$ in the low-level MDP is an individual token output while each state in the low-level MDP contains the high-level state $s_t$ and all the action tokens $a_t^{1:h-1}$ output before the current token. Concretely, in our device control setting, we instantiate $s_t$ with the current screenshot, while omitting the history of past screenshots due to convenience of fine-tuning existing open-source VLMs~\citep{liu2023improvedllava} that can only accept one single image as input. $a_t$ is a single command such as clicking a specified location or typing into a text box.
\fi


\vspace{-0.3cm}
\subsection{Reinforcement Learning Definitions}
\vspace{-0.1cm}
There are two types of value functions we can model in our setting: \textbf{(1)} a ``turn-level'' value function that can score each natural language interaction with the external environment (e.g., ``type box [2]: wikipedia of chocolate''), and \textbf{(2)} a value function at the ``token-level'', where each step is an independent natural language token. Terminology wise, we define the \textbf{state} $s_t$ in the of the Markov decision process (MDP) in our setting to consist of the sequence of tokens denoting the log of the interaction history of the agent with the environment thus far concatenated to the current observation. Each \textbf{action} $a_t$ is a sequence of tokens that are directly applied to interact with the environment at the entire turn-level. 

The turn-level Q-function for a given policy $\pi$ is the expected cumulative reward of a particular action at the current step, and then following the policy $\pi$ thereafter: $Q^\pi(s_h,a_h) = \EE_{\pi} \left[\sum_{t=0}^\infty \gamma^t r(s_{h+t}, a_{h+t})\right]$. The value function of a policy $\pi$, $V^\pi(s_h)$, is defined as the expected Q-value, $Q^\pi(s_h, a_h)$, where actions $a_h$ are sampled from the policy $\pi$. The advantage function $A^\pi(s_h,a_h)$ corresponds to the relative benefit of taking action $a_h$ in state $s_h$, and is computed as the difference between the Q-value and the value of the state under the policy: $A^\pi(s_h,a_h) = Q^\pi(s_h, a_h) - V^\pi(s_h)$. The goal of RL is to train a policy that can produce token sequences that maximize discounted cumulative rewards over the course of a rollout. 

For training the agentic policy, our approach seeks to train an action-value Q-function $Q$ parameterized by parameters $\theta$, and a policy parameterized by $\phi$. Additionally, we maintain a state-only value-function $V$ parameterized by $\psi$ to stabilize training. Both Q-  and V- functions are instantiated by a small MLP layer on top of a VLM backbone. We use $\theta_\mathrm{VLM}, \psi_\mathrm{VLM}$ to represent the parameters of the VLM backbone and similarly $\theta_\mathrm{MLP}, \psi_\mathrm{MLP}$ for parameters of the MLP head. We will denote the last layer representations of these VLM backbones as $f_{\theta_\mathrm{VLM}}(s,a)$ and $f_{\psi_\mathrm{VLM}}(s)$.

\vspace{-0.2cm}
\subsection{Background: ArCHer Framework for Training Agentic Policies with RL Algorithms} \label{sec:method-archer}
\vspace{-0.1cm}
The ArCHer framework~\citep{zhou2024archertraininglanguagemodel} provides one conceptual way to pose training of foundation model value functions and agentic policies as a hierarchical RL problem. Although their framework is not specific to one particular RL algorithm, \citet{zhou2024archertraininglanguagemodel} show that a convenient way to instantiate this approach is to learn a value function at the turn-level and a policy to produce tokens in an autoregressive manner. The value function critic and the agentic actor are then optimized against each other similarly to standard actor-critic RL. The loss functions for training the cirtic are given by: 
\begin{align}
    J_Q(\theta) = \EE_{s, a, r, s' \sim \Dcal}\left[(Q_\theta(s,a) -r - \gamma V_{\bar{\psi}}(s'))^2\right]. & \label{equation: JQ}\\
    J_V(\psi) = \EE_{s \sim \Dcal}\left[\EE_{a \sim \pi_\phi(\cdot|s)}\left[(V_\psi(s) - Q_{\bar{\theta}}(s,a))^2\right]\right]. & \label{equation: JV}
\end{align}
$\bar{\theta}$ and $\bar{\phi}$ are the delayed target networks~\citep{mnih2013playingatarideepreinforcement} for stability and they are updated as an exponential moving average of $\theta$ and $\phi$. The instantiated algorithm from \citet{zhou2024archertraininglanguagemodel} supports policy extractions through REINFORCE policy gradient:
% \vspace{-4mm}
\begin{align*}   
          J_\phi(\pi) = \mathbb{E}_{\substack{s_{c} \sim \mathcal{D}, a_t^{1:L} \sim \pi_\phi}}\!\!\left[\sum_{i=1}^L A(s_{c}, a_t) \log \pi_{\phi}(a_t^i| s_{c}, a_t^{1:i-1}) \right].
        % \label{equation: reinforce}
\end{align*}
% \vspace{-4mm}
While our approach will utilize a similar framework to conceptualize the training of the value function critic and the agentic policy in our method, the design of actor and critic updates employed are substantially different in our method. As such, the actor update from \citet{zhou2024archertraininglanguagemodel}, which also corresponds to the standard policy gradient update, can also be unstable in certain problems.
% As such, the practical approach in \citet{zhou2024archertraininglanguagemodel} did not work very well in our experiments with VLMs.



\vspace{-0.1cm}
\section{\ourmethod: Training VLM Q-Value Functions for Agentic Policy Learning}
\vspace{-0.1cm}

To obtain an effective agentic policy for device control problems, \ourmethod{} trains a Q-value function on static data, which is then used to extract a policy. In the process of designing \ourmethod{}, we need to address challenges with running value-based RL at scale. First, to avoid pathological behavior of TD-backups with large models~\citep{zhou2024archertraininglanguagemodel,snell2023offlinerlnaturallanguage,abdulhai2023lmrlgymbenchmarksmultiturn,chebotar2023qtransformerscalableofflinereinforcement} and to avoid the computational costs associated with training a billion-parameter VLM end-to-end with TD-learning, we train Q-functions on top of frozen VLM representations. Since VLMs are not trained on substantial quantities of decision-making data, off-the-shelf VLMs largely do not accurately represent \emph{actionable} elements of an input scene. To address this, \ourmethod{} fine-tunes VLM representations before running Q-function training. This fine-tuning procedure is not the same as training on in-domain data via supervised fine-tuning, but rather is designed to emphasize actionable features that change from one snapshot to the other and hence help model the value function.

Unlike typical on-policy RL or filtered imitation learning (e.g., AWR~\citep{peng2019advantageweightedregressionsimplescalable}) that only updates the policy with one action per state, training a Q-function allows us to simultaneously estimate returns from multiple action candidates, all of which can then be used for improving the policy. Using multiple action candidates can be more efficient, especially if the critic predictions are more liable, and even if not, it offers variance reduction benefits. \ourmethod{} utilizes this insight to develop a Best-of-N reranking based policy extraction objective for training the policy. This policy improvement operator is stable and more effective than policy gradient or advantage-weighted regression in our expriments. The method is illustrated in~\Cref{fig:method}. We now describe each part of the method below.

\begin{figure}[t]
\centering
     \begin{subfigure}[b]{0.98\textwidth}
    \includegraphics[width=0.99\textwidth]{figures/method.pdf}
     \end{subfigure}
     \vspace{2mm}
     \caption{\footnotesize{\textbf{Overview of \ourmethod.} \textcolor{cyan}{Blue} arrows represent forward data flows, while \textcolor{red}{red} arrows represent how we get learning targets used for back propagation. Our method first goes through a representation fine-tuning stage to extract actionable features from the VLM. TD-learning is then performed on top of frozen VLM representations to learn a reliable Q value function, followed by Best-of-N policy extraction approach.}}
        \label{fig:method}
        \vspace{-0.3cm}
\end{figure}

\vspace{-0.1cm}
\subsection{Training VLM Q-Functions via TD-Learning and Representation Fine-Tuning} \label{sec:method-tune-vlm}
\vspace{-0.1cm}
As discussed above, fine-tuning large VLMs end-to-end to represent value functions can present pathologies and perhaps a more practical approach is to train a separate value function on top of the frozen representation from the VLM. However, most VLM backbones are largely not trained to model \emph{actionable} information for a given scene or make predictions about possible scenes that could result from taking actions in an environment. If the internal representations of the VLM do not pay attention to actionable information in a scene correctly, then training a state-action Q-function $Q(s, a)$ will either degenerate into learning a state-only value function $V(s)$ (i.e., it will ignore the action input) or diverge by incorrectly amplifying noise in partially-trained  Q-value estimates at unseen, out-of-distribution actions that appear in the right hand side of a TD-backup during training. Indeed, in our preliminary runs, we found that while open-source VLMs such as LLaVa-1.5~\citep{liu2024improvedbaselinesvisualinstruction} are able to answer questions about a scene, the same VLM does often fail at correctly answering questions about impact of current actions into the future, e.g., it answers ``does this clicking operation lead to a new page in ebay.com?'' incorrectly. 

To address this issue with VLM representations, \ourmethod{} first fine-tunes representations of a VLM with an binary classification objective to enable it to pay attention to actionable features of an input scene. Once fine-tuned, the representations of this VLM are used to train a Q-function represented using a small MLP on top of the frozen representation. Not only is this approach more stable and robust against pathologies that could emerge from fine-tuning, but it cuts down computational costs since only 1\% of all the VLM parameters (the head) are now trained via TD-learning.

\textbf{Approach.} Our representation fine-tuning objective is constructed as follows: given a transition pair $(s_t, a_t, s_{t+1})$ drawn from a replay buffer, the representation fine-tuning  objective attempts to model if and how the next state $s_{t+1}$ will change from the current state and action $(s_t,a_t)$. Our observation is that in device control problems, a useful action should lead to a substantial visual change in the pixel values of a scene (e.g., successfully typing a search query and pressing enter on ``google.com'' should cause the page to change substantially to now show a list of search results, whereas an unsuccessful search attempt, say due to imperfect clicks, with a very high probability will change none to few pixels of the original scene and remain stuck on the ``google.com'' page ). Equipped with this insight, we construct positive and negative tuples of transitions $(s_t, a_t, s_{t+1})$, where the positive tuples consists of transitions change the state significantly (i.e., larger than a threshold $\epsilon$ on the $\ell_2$ image distance) and the negative tuples are the remaining transitions. This is equivalent to assigning a binary \{0, 1\} label to a transition:
\begin{align*}
\vspace{-2mm}
y_t = \left\{
  \begin{array}{lr} 
      0, & d(s_t, s_{t+1}) < \epsilon \\
      1, & \mathrm{otherwise}
      \end{array}
\right.
\vspace{-2mm}
\end{align*}
Now, the VLM is trained to produce this 0-1 label $y_t$ given a state-action tuple $(s_t, a_t)$ using a binary cross-entropy loss on its parameters $\theta_{\mathrm{VLM}}$:
\begin{align}
        J_{\mathcal{P}}(\theta_{\mathrm{VLM}}) = -\EE_{s_t, a_t \sim \Dcal}[y_i \log \mathcal{P}_{\theta_{\mathrm{VLM}}}(\mathrm{'yes'}|s_t, a_t) + (1 - y_i)\log \mathcal{P}_{\theta_{\mathrm{VLM}}}(\mathrm{'no'}|s_t, a_t)  ], \label{equation:vlm}
\end{align}
where $\mathcal{P}_{\theta_{\mathrm{VLM}}}$ is the next-token distribution obtained from the VLM backbone. 

After this phase of representation fine-tuning, we freeze the parameters of VLM, and extract the embedding of the yes/no token output to serve as the input representation of $(s_t, a_t)$ to the Q function. We now run a TD-learning objective from Equation \ref{equation: JQ} in Section~\ref{sec:prelims} to train the Q-function.

Note that Equation~\ref{equation: JQ} also utilizes a parameterized value function. Since the value function does not depend on the action, we are able to directly use internal representations of an off-the-shelf VLM, without requiring any phase of initial fine-tuning. On top of the frozen representations from the VLMs, our value functions $\theta_{\mathrm{MLP}}, \psi_{\mathrm{MLP}}$ is optimized with the TD loss as in Equations~\ref{equation: JQ_MLP} and~\ref{equation: JV_MLP}.
\begin{align}
J_Q(\theta_{\mathrm{MLP}}) 
&= \EE_{s,a,r,s' \sim \Dcal}[
(Q_{\theta_{\mathrm{MLP}}}(f_{\theta_{\mathrm{VLM}}}(s,a)) - r - \gamma \,
  V_{\bar{\psi}_{\mathrm{MLP}}}(f_{\bar{\psi}_{\mathrm{VLM}}}(s')))^2].
\label{equation: JQ_MLP} \\  % <-- put label here, on the numbered line
J_V(\psi_{\mathrm{MLP}})
&= \EE_{s \sim \Dcal}\Bigl[\EE_{a' \sim \pi_\phi(\cdot|s)}[
\bigl(V_{\psi_{\mathrm{MLP}}}(f_{\psi_{\mathrm{VLM}}}(s)) 
 - Q_{\bar{\theta}_{\mathrm{MLP}}}(f_{\bar{\theta}_{\mathrm{VLM}}}(s,a'))\bigr)^2]\Bigr].
\label{equation: JV_MLP}
\end{align}

\vspace{-0.4cm}
\subsection{Best-of-N Policy Extraction Against the Learned Q-Function} \label{sec:method-actor}
\vspace{-0.1cm}
Given a learned Q-function, we will now use it to extract a policy in an efficient and reliable manner. Perhaps the most straightforward approach for doing so is to use the REINFORCE policy gradient estimator to train the learned policy, however, this approach can be brittle with off-policy data. The presence of a ``negative gradient'' term~\cite{tajwar2024preferencefinetuningllmsleverage} (i.e., a term where the policy gradient multiplies the log likelihood by a negative-valued advantage) means that careful tuning of learning rates and interleaving policy and critic updates must be done to attain good performance (see \citet{zhou2024archertraininglanguagemodel} Section 5.7 for a discussion of these challenges).
While advantage-weighted supervised learning (i.e., AWR~\citep{peng2019advantageweightedregressionsimplescalable}) avoids this instability issue, it can be quite conservative in terms of moving away from the data collection policy.

To build a stable yet non-conservative policy training method, \ourmethod{} modifies weighted regression to make it more ``aggressive'', by leveraging the insight that access to a model of the Q-function allows for estimating values for multiple $N$ actions at any given state. After computing Q-values for multiple action candidates, we can imitate the best action. This would produce updates that are substantially less conservative than single-action AWR, without needing a negative gradient, and only employing a relatively more stable supervised learning update. Theoretically, this is because the implicit KL constraint against the data-generating policy that makes AWR conservative, is now much less of a problem with our multiple-action approach, since this implicit constraint is enforced against the Best-of-N policy~\citep{cobbe2021training}, which already is much less conservative than the data collection policy for larger values of $N$. Moreover, akin to how verifiers have been used in reasoning problems, the value function critic can be used to score multiple possible actions in an offline manner without actually running the rollout or seeking a downstream correctness signal. This approach is similar to concurrent work \citet{sobolmark2024policy} from the domain of robotic policy learning.

Concretely, given any state $s$, we sample $N$ action token sequences from the learned policy: $a_1, \cdots, a_N \sim \pi_\beta(\cdot|s)$, where $\pi_\beta$ is a behavior-cloned policy from the offline dataset. Now, we rank these actions according to the Q-values obtained from the value function trained previously. The policy is then trained to imitate the highest Q-value action of these $N$ actions as long as this best action also attains a positive advantage. Intuitively, this serves as a proxy learning objective to maximize the advantage function per state without the risk of ``negative gradient''. Formally, this means that the policy is optimized as per the loss described in Equation~\ref{equation:best-of-n}. 
\begin{align}
   \label{equation:best-of-n}
    J_\pi(\phi) = \EE_{\substack{s_t \sim \Dcal, a_i \sim \pi_\beta(\cdot|s_t)}}\!\! \left[\sum_{i=1}^N \delta(a_i) \sum_{h=1}^L \log(a_i^h|s_t, a_i^{1:h-1})\right], 
\end{align}
where $\delta(a_i) = \mathds{1}\{a_i = \argmax_i Q(s_t, a_i) \text{~and~} Q(s_t, a_i) - V(s_t) > 0\}$. This approach allows us to make fairly non-conservative policy updates, while also being stable and efficient due to a log loss.

\vspace{-0.1cm}
\subsection{Putting it Together: Implementation Details}
\vspace{-0.1cm}
A pseudocode of our overall algorithm is shown in~\Cref{app:algorithm}. After initially fine-tuning the VLM through the representation fine-tuning scheme in Section~\ref{sec:method-tune-vlm}, \ourmethod{} trains Q and V-functions before performing gradient updates on the actor, where the VLM backbone for the V-function is kept frozen from the pre-trained checkpoint. The usage of V-functions follows from \citet{zhou2024archertraininglanguagemodel, snell2023offlinerlnaturallanguage} to improve training stability.
The actor is represented on top of a separate VLM and is trained end-to-end, unlike the use of frozen features for the critic to improve training stability of TD-learning. For our experiments, we sample $N=16$ actions for computing the Best-of-N reranking policy learning objective in Equation~\ref{equation:best-of-n}: while the choice of $N$ can differ from domain to domain, our runs show that $N=16$ is a good choice for our domain of Android device control. We use LLaVa-1.5~\citep{liu2024improvedbaselinesvisualinstruction} for the backbone VLM for our Q- and V- functions. The architecture details can be found in~\Cref{app: arch}.
