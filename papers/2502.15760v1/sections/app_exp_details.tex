\section{Experimental Details}

\vspace{-0.2cm}
\subsection{Compute Efficiency Comparison}\label{app:compute_efficiency}
\vspace{-0.2cm}

A common concern with deploying TD-learning methods to train large-scale foundation models is their compute inefficiency~\citep{abdulhai2023lmrlgymbenchmarksmultiturn,chebotar2023qtransformerscalableofflinereinforcement}. Therefore, we attempted to understand the compute-performance tradeoffs associated with \ourmethod{}  by comparing it against end-to-end TD-learning on VLMs without using any representation fine-tuning or frozen pre-trained representations.  We plot the performance-compute tradeoff curve for \ourmethod{} on the web-shopping subset of the AitW dataset in~\Cref{fig:flops}. We found it a bit hard to fine-tune an entire VLM with TD-learning, which required iteration on hyperparameters such as learning rate and soft update rates for target networks. Due to the compute-intensive nature, we use a 3B VLM (PaLiGemma~\citep{beyer2024paligemmaversatile3bvlm}) for these runs instead of our 7B VLM~\citep{liu2024llavanext}, and evaluate the performance of the critic as measured by the correlation between advantage predictions and ground-truth notion of human judgement on a held-out set of trajectories.  
In particular, we find that end-to-end TD-learning exhibits a much worse performance-compute frontier, to the extent that beyond a point more training FLOPS hurts performance. We conjecture that this behavior is likely a result of well-known pathologies of training large models with TD learning~\citep{kumar2022offline}, though we leave it for future work to fully understand these pathologies in our context. In contrast, while \ourmethod{} invests an initial amount of computation for representation fine-tuning, its accuracy quickly rises up and results in much better frontiers, with no instability. The calculation of the FLOPS is shown below.

% \begin{figure}[!htp]
%      \centering
%      \begin{subfigure}[b]{0.45\textwidth}
%          \centering
%     \includegraphics[width=\linewidth]{figures/ablation_flops.pdf}
%      \end{subfigure}
%         \caption{\textbf{Offline critic evaluation accuracy as a function of compute} measured in terms of training FLOPS, compared across \ourmethod{}, end-to-end TD-learning on a VLM, and MC return. Observe that the critic accuracy is much better for our approach over end-to-end TD-learning as the amount of compute increases.}
%         \label{fig:ablation-n-actions-2curves}
%         \vspace{-0.2cm}
% \end{figure}

\begin{wrapfigure}{r}{0.45\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/ablation_flops.pdf}
  \caption{\textbf{Offline critic evaluation accuracy as a function of compute} measured in terms of training FLOPS, compared across \ourmethod{}, end-to-end TD-learning on a VLM, and MC return. Observe that the critic accuracy is much better for our approach over end-to-end TD-learning as the amount of compute increases.}
  \label{fig:flops}
  \vspace{-0.2cm}
\end{wrapfigure}

\textbf{FLOPS Calculation.} The 3B VLM takes $45.6\times10^{12}$ FLOPS for \textit{each sample} for forward plus backward process. As the end-to-end TD learning contains one VLM as part of the Q function and one VLM as the target Q function (which only do forward pass), one sample takes $68.4\times10^{12}$ FLOPS (according to \citet{hoffmann2022training}, the FLOPS incurred by the forward prrcess is approximately half of the backward process). Thus, as the longest run takes 15k samples, the last point of the end-to-end run in~\Cref{fig:flops} takes around $1\times10^{18}$ FLOPS. Also, the first logged point takes 128 samples, so the starting point should have $8.3\times10^{15}$ FLOPS.

On the other hand, in \ourmethod{}, we first finetune the 3B VLM, which incurs only one forward and backward process. Thus, finetuning the 3B VLM on $2000$ samples takes $91.2\times 10^{15}$ FLOPS. After that, we infer the representations of these samples with the 3B VLM, which includes one forward pass. This sums up to $136.8\times10^{15}$ FLOPs, which explains the starting point of the \ourmethod{} curve. Then we only train the value head using the VLM representations.\footnote{In this experiment, we fix the BERT model when running Digi-Q.} The size of the value head is 0.07B, incurring $1.1\times10^{12}$ FLOPS for each sample. The longest run of \ourmethod{} takes 0.46M samples, thus incurring $506.9\times 10^{15}$ FLOPS ($10\times 10^{17}$).

Thus, the end-to-end TD learning should range from $0.0083\times10^{15}$ to $1\times10^{18}$ FLOPS, while \ourmethod{} should range from $0.137\times10^{18}$ FLOPS to $0.644\times10^{18}$ FLOPS, which is shown in~\Cref{fig:flops}.




\textbf{Critic Accuracy.} We manually label 483 states with binary advantages, and normalize the advantages produced by the agents to have a mean of zero before thresholding and calculating its accuracy with human annotations.

\subsection{Critic Model Architecture} \label{app: arch}

We show the details of the critic model architecture in~\Cref{fig:arch}. In our environment setting, the states are composed of task, observation (screenshot at step $t$), previous observation (screenshot at step $t-1$), and previous action (action at timestep $t-1$). The task and previous action are text strings, while observations are images. We encode the text strings with BERT and images with BLIP-2 model. Then we concatenate all these feature vectors and pass them through a MLP that tries to predict the V value. The target of the V value is calculated by Equation~\ref{equation: JV_MLP}.

The state-action features are modeled by the current action as well, which is a string passed into not only the BERT encoder but also a part of the prompt passed into the VLM. The prompt is described in~\Cref{app:vlm-prompts}. In the end, the Q features include the BERT embeddings, the BLIP-2 embeddings, and the VLM intermediate-layer representations. We concatenate all of these feature vectors and pass into the another MLP that predicts the Q value. The target of Q value is calculated by Equation \ref{equation: JQ_MLP}.

\begin{figure}[!t]
     \centering
    \begin{subfigure}[b]{1.0\textwidth}
         \centering
    \includegraphics[width=0.99\textwidth]{figures/arch.pdf}
     \end{subfigure}
     ~\vspace{-0.2cm}
        \caption{\textbf{Q-function architecture.} The modules marked \textcolor{orange}{orange} are trained, otherwise the module is kept fixed.}
        \label{fig:arch}
\end{figure}

\subsection{Training Dataset Construction} \label{app:offline-dataset-construction}

We use the pre-trained AutoUI checkpoint to collect offline trajectories. Specifically, to collect each trajectory, starting from the home screen, the agent generates an action, and then the environment takes the action and transitions to the next state. It iterates until a maximum number of steps have been reached or the autonomous evaluator has decided to be a success. We collect 1296 trajectories this way for both AitW Webshop and AitW General subsets. The horizon $H$ of the Webshop subset is set to 20, and the horizon of the General subset is set to 10, which aligns with~\citep{bai2024digirltraininginthewilddevicecontrol}. Each trajectory is composed of state-action-reward-next-state pairs $(s, a, r, s')$, which is also referred to as ``transitions".

The $N$ actions in the offline dataset used for Best-of-N loss are sampled post-hoc from the pre-trained AutoUI checkpoint. When training the actor offline, as we use the Best-of-N loss, we want to sample more than one action. From an engineering aspect, collecting actions each time we sample from the offline dataset $\Dcal$ during training is not efficient. Thus, in practice, we pre-collect $K=64$ actions for each state, and store them in the offline dataset. As $N\in\{1,2,4,8,16\}$ is much smaller than $64$, this strategy serves as a good approximation and results in good performance. It suffices to give enough variety compared to sampling the actions when training the actor model. Note that in this case, the original action will always appear in the offline dataset.

\subsection{Additional Method Details} \label{app:additional-exp-details}

\textbf{Task set formulations.} The two task sets (Webshop and General) in the AitW dataset have different horizons $H$ (maximum number of steps allowed) in a trajectory to improve computational efficiency. Specifically, $H=20$ for AitW Webshop and $H=10$ for AitW General. Following tradition~\citep{bai2024digirltraininginthewilddevicecontrol}, we keep $A>1/H$ (e.g. 0.05 for AitW Webshop) as a threshold for the actor model to learn the state-action pair.

\textbf{Ablation on representation fine-tuning and TD learning as opposed to MC.} In the ablation study on representation fine-tuning, for all configurations, we train the actor model with Best-of-N loss where $N=16$ to keep computation efficient. This is also the case for the ablation on the TD learning as opposed to MC ablations.

\textbf{Ablation on actor loss.} For the ablation study on the actor loss, we keep the same trained Q function, while we ablate only on the loss used to train the actor model. We use $30$ actor epochs for the Best-of-N loss and AWR loss, and $120$ epochs for the REINFORCE loss as the magnitide of the raw advantage is very small. We use $N=16$ for the Best-of-N loss, while REINFORCE and AWR both uses the original action in the offlin dataset.

\textbf{Value function}. In practice, we find the V function significantly easier to train, and it suffices to only use the representations of the state from the vision encoder of the VLM (CLIP) to train the V functions. This simplification significantly saves time and space required, and aligns with previous work~\citep{bai2024digirltraininginthewilddevicecontrol}.

% We call this strategy \textit{approximate random action sampling}.

% \textit{Determinisitc action sampling}, on the other hand, always sample the first $n$ actions from the pre-collected action set. In this case, if a state is sampled multiple times in one iteration, the action set will always be the same, and thus the critic will always produce the same action for the actor to learn. We observe that the performance of determinisitc action sampling is significantly worse than approximate random action sampling, as shown in~\Cref{fig:ablation-n-actions-2curves} (\textit{Left}).

% Note that the $n=1$ case for both approximate random and determinisitc action sampling uses the original action (that causes the transition) instead of sampling it from the action set. Also, for all cases where $n>1$, we always include the original action into the sampled actions.

