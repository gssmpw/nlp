
\section{VLM Prompts} \label{app:vlm-prompts}

The prompt we use for fine-tuning and inferring the VLM is shown in~\Cref{fig:vlm-prompt}. The prompt template is designed to be action-type-specific, in order to facilitate the VLM to better differentiate different types of actions, which promotes fine-grained representations within the same action type. The input to the VLM is constructed by the image and the text prompt. Note that the VLM only sees the current image (overlayed with a cursor if the action is to click), and the next image is only used to calculate whether the target should be ``yes" or ``no". The target is a single token to promote computational efficiency. In practice, we find that a long target sequence introduces challenges for the VLM to fine-tune the representations.

\begin{figure}[!htp]
     \centering
    \begin{subfigure}[b]{0.85\textwidth}
         \centering
    \includegraphics[width=\textwidth]{figures/vlm-prompt.pdf}
     \end{subfigure}
     \vspace{2mm}
        \caption{\textbf{Prompt template we use to fine-tune and infer the VLM.} The input prompt consists of an input image and text input. The text input include a template prompt concatenated with an action-specific prompt. The action-specific prompt includes specific information about the input image. The output (target) prompt is just a word ``yes'' or ``no''.}
        \label{fig:vlm-prompt}
\end{figure}

