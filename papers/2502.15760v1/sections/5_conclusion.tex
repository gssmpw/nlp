\vspace{-0.1cm}
\section{Conclusion and Future Work}
\vspace{-0.1cm}

We presented \ourmethod{}, an effective method for training VLM Q-value functions from offline data, specifically for training real-world device-control agents. At the core of our method is a representation fine-tuning procedure that induces actionable features from VLM useful for later TD-learning and a Best-of-N policy training method that makes the best use of the learned Q function from TD-learning. While we primarily focus on GUI agent tasks on Android devices, our methodology is general, compute efficient, and leads to substantial improvement in performance. We believe that these ideas and approach should transfer to new tasks as well and applying Digi-Q to new domains in an interesting avenue for future work. That said, using the critic in Digi-Q in an active online self-improvement loop will require a more sophisticated system design to speed up the experiment iterations and methods to robustify the critic as the distribution of the agent policy drifts far from the base data collection policy with more online improvement. Nonetheless, ideas from \citet{kalashnikov2018qtoptscalabledeepreinforcement} could provide a good starting point to build policy learning systems based on TD-learning during real-world interaction.
