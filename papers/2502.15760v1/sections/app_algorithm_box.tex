\section{Details on the Algorithm} \label{app:algorithm}

For completeness, we include a detailed pseudo-code of \ourmethod{} in~\Cref{alg:archer_detail}. After initializing the parameters, we perform the representation fine-tuning procedure on top of VLM to obtain actionable features for later TD-learning. Then the VLM parameters will be kept frozen and we train the Q- and V- functions using TD-learning on top of frozen VLM representations. After both value functions are trained, we perform gradient updates on the actor with Best-of-N policy extraction.

\definecolor{darkgreen}{rgb}{0, 0.5, 0}
\begin{algorithm}[!htp]
\caption{\ourmethod{}: Practical Framework}
\label{alg:archer_detail}
\begin{algorithmic}[1]
\State Initialize parameters $\phi, \psi_\mathrm{MLP}, \bar{\psi}_\mathrm{MLP}, \theta_\mathrm{MLP}, \bar{\theta}_\mathrm{MLP}$.
% \State Import pretrained parameters $\theta_{\mathrm{VLM}}$.
\State Initialize replay buffer $\mathcal{D}$ (from an offline dataset).
\For{each VLM iteration}
\State $\theta_\mathrm{VLM} \leftarrow \nabla J_\mathcal P(\theta_\mathrm{VLM})$
\Comment{Equation~\ref{equation:vlm}}
\EndFor
% \State Get representation $f_{\theta_{\mathrm{VLM}}}(s_t,a_t)$ for all $(s_t, a_t)$ in buffer $\mathcal D$.
% \For{each local iteration}
% \State \textcolor{darkgreen}{\#\# Data Collection.}\Comment[only online mode] 
% \For{each environment step}
% \State Execute $a_t \sim \pi_\phi(\cdot|s_t)$ , obtain the next state $s_{t+1}$, add to buffer $\Dcal$.
% \EndFor
\For{each critic step}
\State \textcolor{darkgreen}{\#\# Update high-level Q and V functions by target function bootstrapping.}
\State $\theta_\mathrm{MLP} \leftarrow \theta_\mathrm{MLP} - \nabla J_{\theta_\mathrm{MLP}}(Q)$ \Comment{Equation~\ref{equation: JQ_MLP}}
\State $\psi_\mathrm{MLP} \leftarrow \psi_\mathrm{MLP} - \nabla J_{\psi_\mathrm{MLP}}(V)$ \Comment{Equation~\ref{equation: JV_MLP}}
\State \textcolor{darkgreen}{\#\# Update target Q and V functions.}
\State $\bar{\theta}_\mathrm{MLP} \leftarrow (1 - \tau)\bar{\theta}_\mathrm{MLP} + \tau\theta_\mathrm{MLP}$
\State $\bar{\psi}_\mathrm{MLP} \leftarrow (1 - \tau)\bar{\psi}_\mathrm{MLP} + \tau\psi_\mathrm{MLP}$
\EndFor
\State \textcolor{darkgreen}{\#\# Update low-level actor with high-level critic.}
\For{each actor step}
\State $\phi \leftarrow \phi - \nabla J_\phi(\pi)$ \Comment{Equation~\ref{equation:best-of-n}}
% \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}
