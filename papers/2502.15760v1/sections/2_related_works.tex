\vspace{-0.25cm}
\section{Related Work}
\vspace{-0.1cm}

\textbf{RL for training GUI and device-control agents.} Due to their reasoning and perception capabilities, LLMs and VLMs have been applied extensively to build agents to navigate web pages~\citep{zhou2024webarenarealisticwebenvironment, koh2024visualwebarenaevaluatingmultimodalagents, deng2023mind2webgeneralistagentweb, zheng2024gpt4visiongeneralistwebagent, he2024webvoyagerbuildingendtoendweb} and GUI interfaces~\citep{bai2024digirltraininginthewilddevicecontrol, yan2023gpt4vwonderlandlargemultimodal, hong2023cogagentvisuallanguagemodel, rawles2023androidwildlargescaledataset, rawles2024androidworlddynamicbenchmarkingenvironment, zhang2024lookscreensmultimodalchainofaction}. In contrast to using off-the-shelf proprietary models~\citep{zheng2024gpt4visiongeneralistwebagent, yan2023gpt4vwonderlandlargemultimodal, zhang2023appagentmultimodalagentssmartphone, he2024webvoyagerbuildingendtoendweb} or fine-tuning them with a small amount of human demonstrations~\citep{hong2023cogagentvisuallanguagemodel, zhang2024lookscreensmultimodalchainofaction, zeng2023agenttuningenablinggeneralizedagent}, RL provides the advantage of optimizing task-specific reward and goal-oriented behavior, which is important in dynamic and non-stationary environments especially when human demonstrations are stale~\citep{bai2024digirltraininginthewilddevicecontrol, zhou2024archertraininglanguagemodel, putta2024agentqadvancedreasoning, pan2024autonomousevaluationrefinementdigital, song2024trialerrorexplorationbasedtrajectory}.
However, most successful applications of RL for real-world GUI agent tasks use less efficient RL algorithms such as (nearly) on-policy policy gradient or filtered imitation learning algorithms~\citep{bai2024digirltraininginthewilddevicecontrol, putta2024agentqadvancedreasoning, song2024trialerrorexplorationbasedtrajectory, koh2024treesearchlanguagemodel, shao2024deepseekmath}.
%%HB2.13: I added the GRPO reference here
This can be problematic for real-world GUI agent tasks where interaction with the actual environment presents a bottleneck and on-policy data is hard to collect due to practical issues such as privacy. 
In traditional RL, the approach to avoid variance and learn without on-policy interaction (or massive simulation) is to actually train a Q-value function that can score a given action at a given state (snapshot of the phone screen). Using this Q-value function for training the policy results in substantially better performance~\citep{mnih2013playingatarideepreinforcement, haarnoja2018softactorcriticoffpolicymaximum, fujimoto2018addressingfunctionapproximationerror} and can be done entirely from historical data~\citep{kumar2020conservativeqlearningofflinereinforcement, fu2021d4rldatasetsdeepdatadriven}. To the best of our knowledge, our work is the first to scale value-based Bellman backups to convert VLMs into device-control Q-functions, which serve as effective scoring functions to extract a good device-control policy.

From an algorithmic standpoint, the closest work to ours that trains agents with Q-functions is ArCHer~\citep{zhou2024archertraininglanguagemodel}, which builds a hierarchical framework for developing RL algorithms for training agents. Note that this prior work presents results on simplified environments~\citep{yao2023webshopscalablerealworldweb}. While our use of a VLM-based value function to train the policy can be interpreted as yet another algorithm under the hierarchical actor-critic abstraction in ArCHer, note that the methodology for running RL at scale is substantially different from this prior work. Specifically, we prescribe several important components along the use of frozen VLM representations and a policy extraction approach based on best-of-N policy extraction, which also enables scaling test-time compute. Our experiments in Section~\ref{sec:experiments} show that \ourmethod{} is much more effective (about a 20\% improvement as shown in~\Cref{sec:ablation}) than the policy-gradient algorithm used by \citet{zhou2024archertraininglanguagemodel}. This justifies the benefits of our seemingly simple, yet an effective design of using the value function. Other works~\citep{zhai2024finetuninglargevisionlanguagemodels} train VLMs with on-policy PPO~\citep{schulman2017proximalpolicyoptimizationalgorithms,chen2024visionlanguagemodelsprovidepromptable}. Finally, \citet{chen2024visionlanguagemodelsprovidepromptable} runs RL on top of frozen VLM representations as well, although unlike us they do not fine-tune the VLM to make these representations more amenable for fitting value functions. Instead, they use handcrafted prompts to prime the VLM into producing useful features. Our results highlight that the representation fine-tuning phase 
%%AK.2.12: make sure we call it representation fine-tuning throughout?
%%HB.2.13: ran a complete audit - all set now
in \ourmethod{} is critical to obtaining a good Q-function, but prompting alone is not as effective.

\textbf{Challenges of training an off-policy Q function with foundation models.} Despite the efficiency and data reuse benefits of training a Q function via off-policy TD-learning, it can be unstable and computationally inefficient if not treated carefully, particularly the case for large foundation models with billions of parameters. This instability stems from two aspects: \textbf{(1)} prior work has often found it hard and unstable to train value functions via Bellman backups and TD-learning~\citep{kumar2021dr3,kumar2022offline,chebotar2023qtransformerscalableofflinereinforcement}, which is challenging at scale. To address this, \citet{chebotar2023qtransformerscalableofflinereinforcement} had to employ a combination of conservative regularization~\citep{kumar2020conservativeqlearningofflinereinforcement} and regularization with n-step returns~\citep{hessel2018rainbow} resulting in a complex approach; \textbf{(2)} policy extraction from trained Q-functions often utilizes policy gradient approaches with a ``negative gradient'' term~\citep{tajwar2024preferencefinetuningllmsleverage} that can be unstable with offline data. This has largely resulted in the community focusing on on-policy or filtered imitation learning methods. However, \citet{park2024valuelearningreallymain} show that supervised regression methods such as AWR~\citep{peng2019advantageweightedregressionsimplescalable} can lead to slow convergence and poor asymptotic performance. To address challenge \textbf{(1)}, \ourmethod{} runs TD-learning on top of frozen VLM representations, but after a fine-tuning phase to make them more amenable to representing Q-functions and to address \textbf{(2)}, we introduce a Best-of-N based policy extraction loss, akin to concurrent work~\citep{sobolmark2024policy} in the domain of robotic learning.
%%AK.2.12: have a small line on why is this different from PA-RL?

\iffalse

%%AK.9.14: I didn't follow the main point of this para: is the goal to discuss related works that show challenges with value-based RL (if so, that's good, but then why are we explaining the 
\textbf{Value-based RL.} The use of value-based RL
has been extensively studied in the deep RL literature~\citep{haarnoja2018softactorcriticoffpolicymaximum, fujimoto2018addressingfunctionapproximationerror, kumar2020conservativeqlearningofflinereinforcement, kostrikov2021offlinereinforcementlearningimplicit, mnih2013playingatarideepreinforcement}.
The core idea behind value-based RL is to use Bellman bootstrapping per transition level instead of the Monte Carlo estimator on the trajectory level, to achieve lower variance~\citep{schulman2018highdimensionalcontinuouscontrolusing} and be able to reuse off-policy data.
Therefore, value-based RL methods are often found to have an improved sample efficiency~\citep{haarnoja2019softactorcriticalgorithmsapplications} compared to policy-based RL methods like PPO~\citep{schulman2017proximalpolicyoptimizationalgorithms} in various domains such as games~\citep{mnih2013playingatarideepreinforcement} and robotics~\citep{haarnoja2018softactorcriticoffpolicymaximum}. However, applying value-based RL to real-world foundation model tasks presents unique challenges including computational efficiency and \yifei{why we use best-of-n policy extraction} and this work is the first to address those challenges and show the advantage of value-based RL in real-world agent tasks.
\fi
