
\vspace{-0.2cm}
\section{Experimental Evaluation} \label{sec:experiments}
\vspace{-0.1cm}
The goal of our experiments is to evaluate the efficacy of \ourmethod{} in producing effective Q-functions that in turn are able to train strong Android device control agents. Our experiments will answer the following questions: 
\textbf{(1)} How does \ourmethod{} compare with other state-of-the-art agent training algorithms, previously studied in the context of Android device control tasks? and \textbf{(2)} Can \ourmethod{} learn effectively from past interaction data? 
% \textbf{(3)} Does \ourmethod{} exhibit a favorable computation-performance trade-off compared to end-to-end TD-learning? 
In addition, we perform several ablation experiments to understand the effects of various components of \ourmethod{}: to understand the benefits of using representation fine-tuning and to validate the efficacy of the Best-of-N reranking approach for training the policy using the value function.


\vspace{-0.3cm}
\subsection{Main Performance Results}
\vspace{-0.1cm}
\textbf{Comparisons.} We compare \ourmethod{} with prior methods for building Android device control agents. First, we compare \ourmethod{} with prompting-based methods that extend off-the-shelf proprietary VLMs such as GPT-4V~\citep{openai2024gpt4vtechnicalreport} and Gemini 1.5 Pro~\citep{geminiteam2024geminifamilyhighlycapable} with the Set-of-Marks~\citep{yang2023setofmarkpromptingunleashesextraordinary} and a chain-of-thought mechanism for producing actions.  We also compare with existing VLMs trained via imitation learning for device control: CogAgent~\citep{hong2023cogagentvisuallanguagemodel}, a 18B model. We keep results for these three approaches to be the same as scores previously reported in the DigiRL paper (evaluations were done in March 2024.\footnote{We expect the latest evaluations of all methods to largely underestimate results from evaluations in March 2024 on the AitW Web Shopping subset because certain websites have started blocking agents from taking actions on the website. Details are shown in~\Cref{app:env-errors}. This issue affects both the baseline approaches and our method, and is a direct consequence of the non-stationarity and dynamism of the web environment. This also means that for a given fixed offline dataset, baseline performance numbers from March 2024 are expected to be higher than if the prior approach were run again as of the time of writing this paper. Hence if \ourmethod{} outperforms March 2024 evaluations of a prior method, we can reliably expect it to outperform that prior approach today. We also collected trajectory dataset with higher success rate for offline training, which we expect should lead to better results than March 2024, but also expect results to be uniformly underestimated due to the evaluation issue.}). That said, we evaluate AutoUI-1B~\citep{zhang2023youonlylookatscreens} again. Finally, we compare to a state-of-the-art approach for training device control agents, DigiRL, which does not utilize a state-action Q-value function, but rather uses a state-only value function and MC return estimates to estimate advantages \emph{only} on on-policy actions. We evaluate our results on Android-in-the-Wild (AitW) with offline dataset containing 1296 trajectories for AitW Web Shopping subset and 1008 trajectories from AitW General subset from pre-trained AutoUI checkpoint, following \citet{bai2024digirltraininginthewilddevicecontrol}. More details on the offline dataset can be found in~\Cref{app:offline-dataset-construction}. To understand the ballpark of performance gains from \ourmethod{}, we also compare with DigiRL in the offline-to-online setting, which is given access to online interaction starting from a dataset of 512 initial trajectories.

\textbf{Results.} Our main results are presented in~\Cref{tab:main-table}. We find that \ourmethod{} outperforms all prompting-based methods substantially (53.5\% absolute improvement on average compared to the best prompting-based approach AppAgent with GPT-4V) and improves over the previous state-of-the-art in the offline setting, DigiRL by around 21.2\% relatively averaged on General and Web Shopping test subsets, and 31.5\% relatively over Filtered-BC, a simple but strong baseline. In fact, the performance of Digi-Q even roughly matches the performance of DigiRL when it is allowed to perform on-policy interaction. By visualizing the agent's rollouts on test examples, as we will show in~\Cref{sec:qual-examples}, we find that training a policy with value functions enhances the capability of RL to perform dynamic programming with sub-optimal data to learn a better policy in the environment. 

\begin{table*}[!t]
    \centering
    \small
    \setlength{\tabcolsep}{5.0pt}
        \begin{tabular}{ccccccc}
            \toprule
            &&& \multicolumn{2}{c}{\textbf{AitW General}} & \multicolumn{2}{c}{\textbf{AitW Web Shopping}} \\
            \cmidrule(lr){4-5} \cmidrule(lr){6-7}
            &&& \texttt{Train} &\texttt{Test} & \texttt{Train} &  \texttt{Test}\\
            \midrule
            \multirow{4}{*}{\textbf{Prompting}} & \multirow{2}{*}{\textsc{Set-Of-Marks}} & GPT-4V &  5.2 & $13.5$ &  3.1 & $8.3$ \\
            && Gemini 1.5 Pro & $32.3$ & $16.7$  &  $6.3$ & $11.5$ \\ 
            \cdashline{2-7}
            &\multirow{2}{*}{ \begin{tabular}{@{}c@{}} \textsc{AppAgent}\end{tabular}} & GPT-4V &   $13.5$ & $17.7$ & $12.5$ & $8.3$\\
             && Gemini 1.5 Pro &  $14.6$ & $16.7$ &      
              $5.2$ & $8.3$ \\
            \midrule
            \multirow{7}{*}{\textbf{Learning}} & \multirow{2}{*}{\begin{tabular}{@{}c@{}}\textsc{Supervised} \\ \textsc{Training}\end{tabular}}& CogAgent &  $25.0$ & $25.0$ &  $31.3$ & $38.5$\\
            && AutoUI &  $27.7$ & $22.9$ & $20.7$ & $25.0$ \\
            \cdashline{2-7}
            & \multirow{3}{*}{\textsc{Offline}}
            \rule{0pt}{2.5ex}
            & Filtered BC & $51.0$ \scriptsize{$ \pm\ 0.9$} & $54.5$ \scriptsize{$ \pm\ 1.3$} & $37.2$ \scriptsize{$ \pm\ 4.7$} & $43.8$ \scriptsize{$ \pm\ 1.7$} \\
            && DigiRL & $53.5$ \scriptsize{$ \pm\ 2.7$} & $59.0$ \scriptsize{$ \pm\ 4.7$} & $43.1$ \scriptsize{$ \pm\ 3.6$} & $47.6$ \scriptsize{$ \pm\ 4.2$} \\
            && \textbf{Digi-Q (Ours)} & $\mathbf{61.5}$ \scriptsize{$ \pm\ \mathbf{2.3}$} & $\mathbf{71.2}$ \scriptsize{$ \pm\ \mathbf{2.1}$} & $\mathbf{53.1}$ \scriptsize{$ \pm\ \mathbf{1.7}$} & $\mathbf{58.0}$ \scriptsize{$ \pm\ \mathbf{2.1}$} \\
            % running online exps
            \cdashline{2-7}
            \rule{0pt}{2.5ex}
            & \textsc{Online} & DigiRL & \textcolor{gray}{$63.5$ \scriptsize{$\pm 3.1$}} & \textcolor{gray}{$74.5$ \scriptsize{$\pm 2.6$}} & \textcolor{gray}{$52.6$ \scriptsize{$\pm 1.6$}} & \textcolor{gray}{$57.3$ \scriptsize{$\pm 3.1$}} \\
            \bottomrule
            
        \end{tabular}
        \caption{\footnotesize{\textbf{Main comparisons of different agents across various settings.} Each offline experiment is repeated three times and the mean and standard deviation are reported. To be consistent with prior work~\citep{bai2024digirltraininginthewilddevicecontrol}, results are evaluated with the autonomous evaluator with the first 96 instructions in the train and test set.}}
        \label{tab:main-table}
\end{table*}


\vspace{-0.3cm}
\subsection{Ablation Studies}\label{sec:ablation}
\vspace{-0.2cm}
% \begin{table}[!htp]
\begin{wraptable}{r}{0.5\textwidth}
    \centering
    \setlength{\tabcolsep}{5.0pt}
        \begin{tabular}{ll}
\toprule
\textbf{Representation} & \textbf{Performance} \\ 
\midrule
Behavior Policy & $25.0$\\
\midrule
 \ourmethod{} (w/ MC return) & $37.5$ \scriptsize{$ \pm\ 4.5$} \\
\hdashline
\ourmethod{} Off-the-shelf VLM & $31.9$ \scriptsize{$ \pm\ 1.3$} \\
\ourmethod{} w/ BLIP-2 + BERT & $47.6$ \scriptsize{$ \pm\ 5.2$} \\
% Action Prediction ($a|s$) &  \\
% Next State Prediction w/o Cursor & \\ 
\hdashline
\textbf{\ourmethod{} (Ours)} & $\mathbf{58.0}$ \scriptsize{$ \pm\ \mathbf{2.1}$} \\ 
\bottomrule
        \end{tabular}
        \caption{\footnotesize{\textbf{Efficacy of our representation fine-tuning procedure} on the Web-Shopping test set in AitW.}}
        \label{tab:exp-novlm}
\end{wraptable}
% \end{table}

Next, we will perform a series of controlled experiments to understand the reasons behind the efficacy of \ourmethod{}. In particular, we will attempt to understand \textbf{(1)} the effect of representation fine-tuning (Stage I) for seeding the VLM representations for subsequent Q-function training, \textbf{(2)} the behavior of Best-of-N reranking style policy extraction operator compared to AWR (used by DigiRL~\citep{bai2024digirltraininginthewilddevicecontrol}) and standard REINFORCE-style policy gradients~\citep{williams1992simpleREINFORCE}, \textbf{(3)} the benefits of TD-learning over the more conventional approach of supervised regression to Monte-Carlo return for training the value function, and \textbf{(4)} the scaling performance with more data of \ourmethod{} compared to other baselines. Experimental details of the ablation studies can be found in~\Cref{app:additional-exp-details}.

\textbf{(1) The effect of representation fine-tuning in \ourmethod{}.} We first analyze the effect of fine-tuning the VLM representations by training them to accurately detect actions that led to a substantial change in the scene. To do so, we compare \ourmethod{} with alternate approaches that train Q-functions on top of two other natural choices of representations: \textbf{(a)} not using a generative VLM (i.e., Llava-1.5), but instead using frozen BLIP-2~\citep{radford2021learningtransferablevisualmodels} and tuned BERT~\citep{devlin2019bertpretrainingdeepbidirectional} representations, following \citet{bai2024digirltraininginthewilddevicecontrol}; \textbf{(b)} using an off-the-shelf VLM, without any representation fine-tuning~\citep{chen2024visionlanguagemodelsprovidepromptable}. 

As shown in~\Cref{tab:exp-novlm}, observe that simply using an off-the-shelf VLM only leads to marginal improvement over the behavior policy (31.9\% compared to 25.0\%): this is perhaps expected because an off-the-shelf generative VLM introduces a representation that captures features about the scene holistically, but does not necessarily judge whether these features are actionable. As we will also qualitatively show in Section~\ref{sec:qual-examples}, off-the-shelf VLMs also do not pay enough attention to action information, resulting in a Q-function that degenerates to a similar solution as the state-only value function in the absence of aggressive action coverage in the data. In fact, Digi-Q using off-the-shelf VLM falls short of Digi-Q w/ BLIP-2 + BERT as well. In contrast, the representation fine-tuning procedure employed by \ourmethod{} is able to unlock the advantage of using rich VLMs and achieves more than 10\% absolute improvement over the counterpart with BLIP-2 + BERT.


\textbf{(2) The effect of Best-of-N reranking style policy extraction operator.} Next, we aim to understand the impact of using Best-of-N reranking for policy extraction. This operator differs from traditional policy extraction methods in several ways: \textbf{(i)} the use of multiple actions \textbf{(ii)} not using a ``negative gradient''~\citep{tajwar2024preferencefinetuningllmsleverage} as in REINFORCE~\citep{williams1992simpleREINFORCE}. To understand the effect of the number of actions in \textbf{(i)}, we ablate \ourmethod{} over multiple values of $N\in\{1, 4, 8, 16\}$ in~\Cref{fig:ablation-n-actions} (\textit{Left}). Observe that \ourmethod{} improves monotonically as $N$ increases, indicating a clear benefit of sampling more actions and reranking them against the Q-function during training. More discussions on the ablations of number of actions resampled can be found in~\Cref{app:offline-dataset-construction}.

\begin{figure}[!t]
     \centering
    \begin{subfigure}[b]{0.95\linewidth}
         \centering
    \includegraphics[width=0.45\linewidth]{figures/ablation_num_actions_1curve.pdf}~~~~\vline~~ 
    \includegraphics[width=0.45\linewidth]{figures/data_efficiency.pdf}
     \end{subfigure}
        \caption{\footnotesize{ \textbf{\textit{Left: }Performance of \ourmethod{} when varying the number of actions $N$ used for policy extraction.} Observe that the performance of \ourmethod{} improves when more actions are used for policy extraction, indicating the efficacy of our approach and the benefits of learning a Q-function.} \textbf{\textit{Right:} Data efficiency of Digi-Q and DigiRL.} The success rate of Digi-Q increases significantly faster than offline DigiRL given the same amount of more data.}
        \label{fig:ablation-n-actions}
\end{figure}

Next, we answer \textbf{(ii)} by comparing  \ourmethod{} with REINFORCE and supervised regression (AWR). Our results in~\Cref{tab:exp-actor-loss} show that while REINFORCE is able to achieve some improvements compared to the data collection policy (37.5\% compared to 25.0\%), it also suffers from the highest variance among all ablated methods. We hypothesize that this is a direct consequence of the negative gradient, which is known to sometimes destabilize training.  While AWR (\citet{bai2024digirltraininginthewilddevicecontrol}) does not suffer from this issue, it is also not able to stably improve the policy (19.4\% compared to 25.0\%), likely because it is conservative. On the other hand, \ourmethod{} is able to make substantial improvements. 

\begin{table}[!htp]
    \centering
    \setlength{\tabcolsep}{5.0pt}
        \begin{tabular}{ccc}
\toprule
\textbf{Actor Objective} & \textbf{Performance} & \textbf{KL v.s. Behavior Policy} \\ 
\midrule
Behavior Policy & $25.0$ & $0$ \\
\hdashline
\rule{0pt}{2.5ex}
REINFORCE & $37.5$ \scriptsize{$ \pm\ 4.7$} & $7.15$ \\
AWR & $19.4$ \scriptsize{$ \pm\ 1.3$} & $2.84$ \\
\hdashline
\rule{0pt}{2.5ex}
\ourmethod{} & $\mathbf{58.0}$ \scriptsize{$ \pm\ \mathbf{2.1}$} & $3.28$ \\
\bottomrule
        \end{tabular}
        \caption{\footnotesize{\textbf{(1) Performance and (2) token-level KL-divergence value between the learned policy and the dataset} when using different policy extraction methods on Web Shopping test set. We utilize the same critic for all the methods, and only train the policy differently.}}
        \label{tab:exp-actor-loss}
\end{table}

Next we attempt to understand how ``non-conservative'' the updates made by different approaches are since one concern with AWR-style updates in prior work is the extent to which they are conservative. We wish to understand if our Best-of-N reranking based policy extraction approach also admits conservative updates. To do so, we measured the KL-divergence between actions from the dataset and the fine-tuned policies produced by \ourmethod{}, AWR, and REINFORCE in~\Cref{tab:exp-actor-loss}. Note that \ourmethod{} incurs a larger KL-divergence value unlike AWR that incurs the smallest deviation and is most conservative. In contrast, REINFORCE attains larger divergence values but behaves unstably (see \Cref{app:pg-example} for some example rollouts). Some qualitative examples for these variants are shown in \Cref{sec:qual-examples}. Our results are also consistent with findings in concurrent work from robotic control problems~\citep{sobolmark2024policy}.

\textbf{(3) The effect of TD-learning as opposed to MC.} To understand the importance of TD-learning for training the critic over Monte-Carlo (MC) regression that previous work is based on, we run an ablation of \ourmethod{}, which uses MC regression. Observe in \Cref{tab:exp-novlm}, that this version underperforms \ourmethod{} by 20\% (58.0\% compared to 37.5\%). As we show in \Cref{sec:qual-examples}, value functions from MC regression exhibit high variance, which inhibits them from producing good policies even when used to rank multiple actions.

% \textbf{How good are different value estimation methods?} We investigate the effect of different value estimation methods, including ablations on (1) Bellman backup and Monte-Carlo estimation, and (2) regression and classification loss.\yifei{do we need this paragraph? somehow this is not motivated from before}

\textbf{(4) Scaling performance of \ourmethod{} with different amount of data.} We present the comparison between \ourmethod{} and DigiRL along the axis of the number of training trajectories in~\Cref{fig:ablation-n-actions} (\textit{Right}). For a fair comparison, for \ourmethod{} we rerun all stages of training while varying the amount of training data. As shown in~\Cref{fig:ablation-n-actions} (\textit{Right}), we observe that \ourmethod{} outperforms DigiRL in all regimes, even in the low-data regime with only 256 trajectories. We suspect this is due to the ability to reuse data and perform better per-step credit assignment, thanks to a reliable Q function.


\subsection{Qualitative Visualizations} \label{sec:qual-examples}
\begin{figure}[t]
     \centering
    \begin{subfigure}[b]{0.95\textwidth}
         \centering
    \includegraphics[width=\textwidth]{figures/qual-adv-value-est.pdf}
     \end{subfigure}
     \vspace{2mm}
     \caption{\footnotesize{\textbf{Qualitative examples showing the advantage estimations of several transitions of TD (ours), Monte-Carlo, and TD without VLM representation.} Advantage estimations using TD-learnt value functions top of VLM representation better align with human judgements compared to MC and TD without using VLM.}}
        \label{fig:qual-adv-value-est}
\end{figure}

\textbf{Qualitative comparisons between different value function learning approaches.}  To qualitatively understand the quality of the Q-function learned, in \Cref{fig:qual-adv-value-est}, we visualize advantage estimates $A(s,a) = Q_{\theta}(s,a) - V_{\phi}(s)$ computed from Q-functions produced by four methods: \textbf{(1)} \ourmethod{} (with representation fine-tuning and TD-learning), \textbf{(2)} Monte-Carlo regression, \textbf{(3)} \ourmethod{} but using BLIP-2 + BERT representations from \citet{bai2024digirltraininginthewilddevicecontrol}; and \textbf{(4)} \ourmethod{} without representation fine-tuning. We compare advantages with human judgments of whether the actions mark progress towards the desired task. Ideally, good actions should attain a positive advantage. We observe that advantage estimates from MC regression are often erroneous and uncorrelated with the human notion of good actions, perhaps because of the use of high-variance MC estimator. 
Moreover, we find that \textbf{(3)} converges to a degenerate $Q(s,a)$ that approximately matches a state-only value function, with limited meaningful sensitivity to the action input, which is problematic for policy learning. \textbf{(4)} mitigates this problem and produces different action values under the same state, but still fails at fine-grained differences like clicking at different positions on the screen.
Thus, all these ablation variants perform suboptimally at attaining a good correlation with human judgement, only \textbf{(1)} \ourmethod{} is able to produce advantage estimates that align well with human annotations and judgement.

% We demonstrate transition examples labeled by the Q and V values of introducing VLM representations in~\Cref{fig:qual-adv-value-est}.

% in~\Cref{fig:stitching-example}.


