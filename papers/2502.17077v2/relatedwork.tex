\section{Related work}
\label{section:related}

Rank aggregation is undoubtedly one of the most prevalent yet perhaps one of the most underappreciated topics in preference learning. 
%
Its ubiquity stems from its foundational role, as many concepts and techniques of rank aggregation are integral to framing either the learning problem or the learning methodology across all areas of preference learning.
%
This importance is somewhat analogous to measures of central tendency in classical probability theory, where measures like the mean or expected value serve as core principles. 
%
Moreover, rank aggregation is critical in numerous machine learning problems, often without being the primary focus, despite its fundamental impact on the overall learning~framework.
%
%

Similar to how the mean or expected value often serves as a gold standard in classical probability theory, the Kemeny consensus plays a similar role in learning problems involving preferences without ties \parencite{brinker_case-based_2006, jiao_controlling_2016, korba_structured_2018}. 
%
When ties are permitted, the optimal bucket order problem becomes the corresponding standard, as it remains the primary rank aggregation problem used for partial label ranking algorithms \parencite{alfaro_learning_2021, alfaro_mixture-based_2021, alfaro_ensemble_2023, alfaro_multi-dimensional_2023, alfaro_pairwise_2023}, except for the approach by \textcite{thies_more-plr_2024}.
%
This preference for the optimal bucket order problem and the Kemeny consensus largely stems from their natural definitions: both aim to identify the object, whether a total order or bucket order, that minimizes the distance to the underlying data among all possible objects.
%
%

However, this straightforward formulation has a significant drawback: computing a Kemeny consensus is NP-hard \parencite{bartholdi_computational_1989}, as is solving the optimal bucket order problem \parencite{lorena_biased_2021}.
%
To address these computational challenges, several approximation algorithms have been developed \parencite{aledo_utopia_2017, aledo_approaching_2019, aledo_highly_2021}.
%
An alternative strand of research in preference learning investigates the use of other aggregation methods for the underlying learning problem \parencite{bengs_preference-based_2021, hullermeier_preference_2024}.
%
These methods generally fall into two categories:
%
\begin{itemize}

    \item \textit{Scoring-based} approaches, such as the \textit{Borda} method \parencite{black_partial_1976} or \textit{Copeland} scores \parencite{copeland_reasonable_1951}.

    \item \textit{Probabilistic-based} approaches, which include \textit{parametric} approaches like \textit{maximum likelihood estimator} or \textit{Bayes estimator} of an assumed parametric probabilistic model \parencite{cheng_label_2010}, as well as \textit{non-parametric} methods such as \textit{Markov chain-based} approaches \parencite{dwork_rank_2001} or modified \textit{von-Neumann winner} concepts \parencite{dudik_contextual_2015}.
%
\end{itemize}
%
%
Recently, two approaches, both with label ranking as the main application, were suggested that do not belong in one of the two categories above: \textcite{adam_inferring_2024} used imprecise probabilities as the main mathematical tool for rank aggregation, while \textcite{zhou_heuristic_2024} considered heuristic search methods for that purpose.

% % % % % % % % % % Preliminaries % % % % % % % % % %