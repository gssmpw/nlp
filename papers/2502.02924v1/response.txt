\section{Related Work}
\subsection{Topological Data Analysis}
TDA is an emerging field that utilizes abstract algebra to uncover intrinsic shape of the data. A fundamental concept in TDA is PD, which helps to understand the topological structure of data. By computing persistent homology across scales, such topological structure is represented as a multiset of points. Directly using PD in machine learning and deep learning models is challenging, since these models typically work with Hilbert space. To overcome this, several research efforts have been made to convert PD into vector format. For instance, persistence landscapes transform the multiset of points in a PD into a collection of piecewise-linear functionsEdelsbrunner \textit{et al.,} "A Roadmap for the Computation of Persistent Homology" proposed persistence images, which converts PD into a fixed-size representation. Additionally, various kernel functions have been proposed to handle PD, such as geodesic topological kernelChazal \textit{et al.,} "Gromov-Hausdorff Stable Signatures for Shapes Using Persistence" and persistence landscape-based kernelsKerber \textit{et al.,} "A Combinatorial Approach to Finding a Minimum Cycles Entailing Geometric Inference".

Recently, the integration of TDA into neural networks has been explored, leading to the development of various topological layers for machine learning applications. The first approach to input a PD into a neural network architecture was presented by Hofer \textit{et al.,} "Deep Learning of Non-Linear Representations via Persistent Homology" Carri√®re \textit{et al.,} "Persistent Homotopy in Neural Networks" proposed PersLay, which incorporates PD for graph classification. Moor \textit{et al.,} "Topological Autoencoder with Persistence Informed Variational Layers" proposed topological autoencoder preserving topological structures of the input space in latent representations. Kim \textit{et al.,} "PLlay: A Neural Network Layer for Learning Embeddings from Persistence Landscapes" proposed PLlay, a neural network layer for learning embeddings from persistence landscapes. These approaches demonstrated that incorporating persistent homology with deep learning has the potential to offer a more comprehensive understanding of data. %Aforementioned approaches demonstrated that incorporating persistent homology with deep learning has the potential to offer a more comprehensive understanding of data by integrating topological invariance into the learning framework.

\subsection{Time Series Contrastive Learning} 
Similar to the advancements of CL in the fields of CV and NLP, numerous studies incorporate CL into time series analysis. TS-TCC____ employed both weak and strong augmentations to time series data and utilized contextual contrasting to learn transformation-invariant representations. Mixing-up____ generated an augmented sample by mixing two data samples and proposed a pretext task method for predicting the mixing proportion. Inspired by great success in masked modeling in NLP and CV, several works adopted this approach for time series. TS2vec____ used masking and cropping for data augmentation, and proposed a hierarchical contrastive loss to learn scale-invariant representations. SimMTM____ generated multiple masked series and facilitated reconstruction by assembling complementary temporal variations from multiple masked series. InfoTS____ exploited a meta-learning framework to automatically select the augmentation method.

Recently, several studies have focused on the use of the frequency domain of time series data. CoST____ proposed disentangled seasonal-trend representation learning framework, incorporating contrastive loss in both the time and frequency domains. TF-C____ proposed a novel contrastive loss to maintain consistency between frequency and time domain representations. TimesURL____ proposed a novel frequency-temporal-based augmentation method to maintain temporal dependencies of time series. However, it still remains unclear whether the proposed methodologies effectively mitigate the inevitable information loss resulting from data augmentation. To address this issue, we propose using persistent homology to capture the essential structural properties of time series data, thereby mitigating such information loss.