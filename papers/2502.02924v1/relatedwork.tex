\section{Related Work}
\subsection{Topological Data Analysis}
TDA is an emerging field that utilizes abstract algebra to uncover intrinsic shape of the data. A fundamental concept in TDA is PD, which helps to understand the topological structure of data. By computing persistent homology across scales, such topological structure is represented as a multiset of points. Directly using PD in machine learning and deep learning models is challenging, since these models typically work with Hilbert space. To overcome this, several research efforts have been made to convert PD into vector format. For instance, persistence landscapes transform the multiset of points in a PD into a collection of piecewise-linear functions~\cite{bubenik2015statistical}.~\cite{adams2017persistence} proposed persistence images, which converts PD into a fixed-size representation. Additionally, various kernel functions have been proposed to handle PD, such as geodesic topological kernel~\cite{padellini2021supervised} and persistence landscape-based kernels~\cite{zhu2016stochastic, bubenik2020persistence}.

Recently, the integration of TDA into neural networks has been explored, leading to the development of various topological layers for machine learning applications. The first approach to input a PD into a neural network architecture was presented by Hofer \textit{et al.} \cite{hofer2017deep}. Carri√®re \textit{et al.} \cite{carriere2020perslay} proposed PersLay, which incorporates PD for graph classification. Moor \textit{et al.} \cite{moor2020topological} proposed topological autoencoder preserving topological structures of the input space in latent representations. Kim \textit{et al.} \cite{kim2020pllay} proposed PLlay, a neural network layer for learning embeddings from persistence landscapes. These approaches demonstrated that incorporating persistent homology with deep learning has the potential to offer a more comprehensive understanding of data. %Aforementioned approaches demonstrated that incorporating persistent homology with deep learning has the potential to offer a more comprehensive understanding of data by integrating topological invariance into the learning framework.

\subsection{Time Series Contrastive Learning} 
Similar to the advancements of CL in the fields of CV and NLP, numerous studies incorporate CL into time series analysis. TS-TCC ~\cite{TSTCC} employed both weak and strong augmentations to time series data and utilized contextual contrasting to learn transformation-invariant representations. Mixing-up ~\cite{wickstrom2022mixing} generated an augmented sample by mixing two data samples and proposed a pretext task method for predicting the mixing proportion. Inspired by great success in masked modeling in NLP and CV, several works adopted this approach for time series. TS2vec~\cite{yue2022ts2vec} used masking and cropping for data augmentation, and proposed a hierarchical contrastive loss to learn scale-invariant representations. SimMTM~\cite{dong2024simmtm} generated multiple masked series and facilitated reconstruction by assembling complementary temporal variations from multiple masked series. InfoTS~\cite{luo2023time} exploited a meta-learning framework to automatically select the augmentation method.

Recently, several studies have focused on the use of the frequency domain of time series data. CoST ~\cite{woo2022cost} proposed disentangled seasonal-trend representation learning framework, incorporating contrastive loss in both the time and frequency domains. TF-C~\cite{zhang2022self} proposed a novel contrastive loss to maintain consistency between frequency and time domain representations. TimesURL~\cite{liu2024timesurl} proposed a novel frequency-temporal-based augmentation method to maintain temporal dependencies of time series. However, it still remains unclear whether the proposed methodologies effectively mitigate the inevitable information loss resulting from data augmentation. To address this issue, we propose using persistent homology to capture the essential structural properties of time series data, thereby mitigating such information loss.