\section{Empirical Interventions}
Our findings reveal that LLMs struggle to enforce instruction hierarchies, often defaulting to inherent biases instead of following system-user directives. We experiment with two potential interventions, prompt-based adjustments and fine-tuning, to determine their effectiveness to mitigate the issue. While both interventions improve prioritization to some extent, neither fully resolves instruction hierarchy enforcement, as models continue to exhibit biases and inconsistent constraint adherence (\Cref{fig:polar_plot_interventions}).

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{plots/intervention_radar_plot.pdf}
    \caption{Llama-8B Model performance based on improved prompts and fine-tuning. The radial length shows PAR, while the angular width shows $1-|\text{CB}|$, all with a square-root transformation, consistent with \Cref{fig:polar_plot_separation}.}
    \label{fig:polar_plot_interventions}
\end{figure}

\subsection{Prompting-based Adjustments}

We first examine whether models can be steered through explicit priority instructions and constraint marking. Simple priority guidance (e.g., \textit{Follow Constraint 1 over Constraint 2 when they conflict}) improves adherence but remains inconsistent.\footnote{Placing the guidance in the user message yields similar if not better performance than placing them in the system message, confirming our observations on failed system message authority. For detailed results, check \Cref{app:prompt_analysis}.} In contrast, constraint marking, where constraints are explicitly labeled in the prompt (e.g., \textit{Constraint 1: write in English}), leads to a clearer prioritization structure across models. However, even with strong directives, models frequently revert to inherent biases, ignoring priority designations (\Cref{fig:polar_plot_interventions} Left). This suggests that while prompting can shift model behavior, it does not establish a stable, generalizable instruction hierarchy. Moreover, explicit constraint marking is often impractical in real-world applications.

\subsection{Fine-tuning Approach} 
To test whether hierarchical control can be reinforced at the model level, we fine-tune a LoRA-adapted~\citep{hu2021loralowrankadaptationlarge} Llama-8B on constraint prioritization tasks. Using three-fold cross validation, we train on four conflict types while testing on the remaining two, maintaining the same base tasks across training and test sets. While fine-tuning yields improvements in handling certain constraint types (\Cref{fig:polar_plot_interventions} Right), the gains are inconsistent even in this highly controlled setting with simple, well-defined constraints and shared base tasks. These results suggest that robust hierarchy enforcement may not emerge naturally through conventional fine-tuning approaches alone (at least not this naive setting), and broader questions about maintaining general instruction-following capabilities remain open.\footnote{More details on LoRA fine-tuning and data set construction are in \Cref{app:finetune_data}.}
