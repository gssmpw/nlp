\section{Introduction}


\epigraph{In some cases, the user and developer will provide conflicting instructions; in such cases, the developer message should take precedence.}{\textit{2024 Model Spec \\ OpenAI}}

Large language models (LLMs) have revolutionized natural language processing through their versatile text generation capabilities \citep{brown2020language,touvron2023llama,achiam2023gpt}, and instruction tuning has further enhanced their practical utility by enabling more precise output control through natural language directives \citep{Wei2021FinetunedLM, mishra-etal-2022-cross, wang-etal-2023-self-instruct, wu-etal-2024-language}. The instruction-following capabilities have transferred LLMs from general-purpose language models into adaptable tools for specific applications  \citep{wang-etal-2022-super, zhou2023instruction}.

With widespread deployment of instruction-following LLMs, their design choices have evolved to reflect real-world usage patterns. A notable development is the emergence of role-based instruction management, exemplified by the system/user separation pattern adopted by major LLM providers, including many open-source LLMs. They often explicitly differentiate between developers and end-users (and tools in agentic systems), where developers regulate the general capabilities of the LLM to better serve a specific end-user population, often through system-level constraints. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{plots/framework.png}
    \caption{A systematic framework for studying and evaluating instruction hierarchies in LLMs through verifiable constraint prioritization.}
    \label{fig:framework}
\end{figure}

This deployment pattern reflects an underlying assumption that different instruction sources should have varying levels of authority over model behavior. For instance, OpenAI explicitly states in their 2024 Model Spec that developer (system) messages should take precedence when user and developer instructions.\footnote{OpenAI 2024 Model Spec: \url{https://cdn.openai.com/spec/model-spec-2024-05-08.html}} This hierarchy is crucial not only for model safety \cite{wallace2024instruction}, but also for LLM-based agentic systems serving third-party users \citep{AutoGPT}, where developers can employ meta-prompts to configure an LLM as an agent's core component, prompts that should neither be revealed to nor overridden by end-users.


To systematically investigate LLMs' handling of instruction hierarchies, we design a controllable framework (\Cref{fig:framework}) for examining the hierarchical authority in LLMs through constraint prioritization. Our initial experiments across six state-of-the-art LLMs reveal a concerning observation: even with basic formatting conflicts (such as contradictory length requirements or capitalization rules), models exhibit highly inconsistent behaviors in choosing which instruction to follow.

Motivated by these preliminary findings, we dive deeper into understanding model behaviors by proposing several specialized metrics that measure conflict awareness, instruction prioritization patterns, and behavioral tendencies. Through extensive experiments using these metrics, we uncover several concerning patterns: models rarely acknowledge the existence of conflicting instructions in their responses, and even when they do recognize conflicts, they frequently fail to maintain proper instruction hierarchies. Moreover, we discover that models exhibit strong inherent biases toward certain types of constraints, regardless of their priority designation.

Given these challenges, we explore two possible interventions: prompting-based adjustments and fine-tuning. While both interventions improve prioritization to some extent, neither fully resolves instruction hierarchy enforcement. These findings suggest that robust handling of instruction hierarchies remains a fundamental challenge in current LLM architectures.

