
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{plots/polar_single_n2.pdf}
    \caption{Model performance across conflict types under \textbf{Pure Separation Configuration}. The radial plot combines two metrics: the radial length shows Priority Adherence Rate (PAR), measuring priority following effectiveness, while the angular width shows normalized Constraint Bias ($1-|\text{CB}|$), indicating bias resistance. Both metrics range between 0-1.  Higher values are better; larger areas indicate more effective priority control. A square-root transformation is applied to highlight subtle differences.}
    \label{fig:polar_plot_separation}
\end{figure*}


\section{Model Behavior Analysis}\label{sec:analysis}

While the obedience rates establish the failure of system/user separation as a control mechanism, a more detailed characterization of this failure is needed. Non-compliance (R3) can stem from various reasons --- from imperfect instruction following to various forms of conflict recognition. To better characterize model behaviors,  we introduce three specialized metrics (detailed in \Cref{sec:ad_metrics}) that focus on clear response patterns: Explicit Conflict Acknowledgement Rate (ECAR) captures when models recognize conflicts, while Priority Adherence Ratio (PAR) and Constraint Bias (CB) measure model behaviors when instructions are successfully followed, isolating these patterns from the noisy non-compliance cases.

In this section, through these metrics, we reveal that models rarely acknowledge conflicts explicitly, fail to maintain hierarchies even when they do, and exhibit strong inherent biases toward certain constraints regardless of priority designation.

\subsection{Advanced Metrics for Behavior Analysis}\label{sec:ad_metrics}

\paragraph{Explicit Conflict Acknowledgement}
Models occasionally acknowledge conflicting constraints without prompting. Through few-shot prompting, we identify these explicit acknowledgments (e.g., \ex{I notice contradictory instructions\ldots}) and separate them from responses for two purposes: to ensure constraint evaluation focuses on task-relevant output, and to compute the Explicit Conflict Acknowledgement Rate (ECAR). ECAR measures how often models explicitly recognize conflicts through statements about contradictions, requests for clarification, or explanations of constraint-selection decisions.

\paragraph{Priority Adherence Ratio (PAR)} Priority Adherence Ratio (PAR) measures how well models respect priority designation when they successfully follow a constraint. By focusing only on cases where exactly one constraint is satisfied (excluding non-compliance cases), PAR isolates clear prioritization behavior from noisy failure modes:
\begin{equation}
\text{PAR} = \frac{R_1}{R_1 + R_2}
\label{eq:par}
\end{equation}
PAR ranges from 0 to 1, with a PAR of 1 indicating perfect priority adherence: whenever the model follows a constraint, it chooses the primary one. Conversely, a PAR of 0 shows complete priority inversion.


\paragraph{Constraint Bias (CB)} 

Constraint Bias (CB) captures models' inherent preferences between conflicting constraints, independent of priority designation. By measuring constraint following patterns when no priority mechanism is specified (the NP.\ Baseline from \Cref{sec:baselines}) and averaging across both possible constraint orderings, CB reveals default behavioral tendencies. For example, a model might have an inherent tendency to output English regardless of which language is designated as primary.  
\begin{equation}
\text{CB} = \frac{R_{c1} - R_{c2}}{R_{c1} + R_{c2}}
\label{eq:cb}
\end{equation}
where $R_{c1}$ ($R_{c2}$) is the obedience rate of constraint $c1$ ($c2$) regardless of priority designation. CB ranges from $-$1 to 1, where 0 indicates no bias and a score closer to 1 ($-$1) indicates increasing bias towards $c1$ ($c2$). Like PAR, this metric isolates clear behavioral patterns by excluding non-compliance cases.

To quantify a model's resistance to such bias, we normalize CB to $1 - |\text{CB}|$ (range from 0 to 1), where a score closer to 1 indicates high resistance to bias while a score closer to 0 indicates strong internal bias.


\subsection{Ineffective Conflict Acknowledgment} \label{sec:conflict_acknowledgement}

\begin{table}[t]
\centering
\small
\begin{tabular}{lrrrr}
\toprule
Model & ECAR & $R1_{ac}$ & $R2_{ac}$ & $R3_{ac}$ \\
\midrule
Qwen & 0.1 & 0.0 & 100.0 & 0.0 \\
Llama-8B & 15.9 & 20.4 & 50.3 & 29.3 \\
Llama-70B & 20.3 & 30.7 & 37.7 & 31.6 \\
Claude & 2.7 & 50.0 & 31.2 & 18.8 \\
GPT4o-mini & 2.2 & 46.2 & 0.0 & 53.8 \\
GPT4o & 12.0 & 47.9 & 0.7 & 51.4 \\
\bottomrule
\end{tabular}
\caption{Conflict acknowledgment and constraint following rates under the \textbf{Pure Separation Configuration}. ECAR means Explicit Conflict Acknowledgement Rate; $R1_{ac}$, $R2_{ac}$ and $R3_{ac}$ stand for constraint obedience rates when the conflict is explicitly acknowledged.}
\label{tab:conflict_acknowledgement_basic_separation}
\end{table}

Our analysis of ECAR in \Cref{tab:conflict_acknowledgement_basic_separation} shows that models rarely acknowledge instruction conflicts, with ECAR ranging from 0\% (Qwen) to 20.3\% (Llama-70B). Meanwhile, acknowledgment does not guarantee correct prioritization and there's a clear architectural influence: while Llama models frequently acknowledge conflicts but show mixed constraint following patterns, GPT4o variants and Claude maintain more consistent primary constraint adherence when they do acknowledge conflicts. Notably, when GPT4o models explicitly acknowledge conflicts, they almost never choose to follow the lower-priority constraint. This unique characteristic likely stems from their instruction hierarchy training, as reported in \citet{wallace2024instruction}, suggesting that  instruction hierarchy training does lead to more systematic handling of prioritization.

\subsection{Failure Modes in Priority Enforcement}\label{sec:priorityeffectiveness}


We use polar plots (\Cref{fig:polar_plot_separation}) to analyze how well models enforce instruction priorities while avoiding biases. The radial length (PAR) represents priority adherence, while the angular width ($1 - |\text{CB}|$) indicates bias resistance. Larger sectors indicate better priority control with minimal bias.

Most models fail to enforce instruction hierarchies consistently, as reflected in their small total areas. GPT-4o and GPT-4o-mini perform best, particularly in binary constraints (language, case), likely due to their explicit instruction hierarchy training. However, even these models show significant variation across constraints, suggesting that their prioritization ability remains inconsistent.

Distinct failure patterns emerge. Bias-dominated failures (thin spokes) occur when models favor one constraint regardless of priority, as seen in Qwenâ€™s language conflict, where it always follows the user constraint. Indecisive failures (short, wide sectors) arise when models fail to enforce priority even when unbiased (e.g., Claude Word Length).

In general, models follow categorical constraints (e.g., case, language) more reliably than constraints requiring reasoning along a continuous scale (e.g., keeping counts during generation). This suggests that current instruction-following approaches are better at simple pattern recognition but fail to generalize to more complex constraints.

These findings reinforce that LLMs lack a robust mechanism for enforcing instruction priorities across diverse constraints, and also highlights a fundamental limitation in current instruction tuning paradigms.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{plots/tendency_analysis.pdf}
    \caption{Constraint Bias (CB) across six dimensions. Positive values (blue) favor the right-side constraint, while negative values (red) favor the left-side constraint, with magnitude reflecting bias strength.}
    \label{fig:model_tendency}
\end{figure}

\subsection{Model-specific Constraint Biases}\label{sec:bias} 


Our analysis of Constraint Bias (CB) scores reveals that models exhibit strong inherent preferences when resolving conflicting instructions, often overriding designated priority structures. \Cref{fig:model_tendency} visualizes these biases, where each subplot represents a constraint pair, and bars indicate model-specific tendencies. 

Most models display strong but inconsistent biases across constraint types. Bias magnitudes often exceed 0.5, indicating a clear default tendency toward certain constraints even when models are explicitly instructed otherwise.

Notably, some biases are widely shared across models. All models favor lowercase over uppercase text, prefer generating texts with more than 10 sentences, and tend toward avoiding keywords. This consistency across different model architectures suggests these biases might stem from common patterns in pre-training data or fundamental architectural designs in current models. For instance, the preference for lowercase likely reflects the predominance of lowercase text in training corpora.

Despite these shared biases, other preferences vary sharply across models. Word length preferences are particularly diverse: Qwen strongly favors shorter texts ($<$50 words), while Llama-8B heavily prefers longer texts ($>$300 words). Language choice and keyword usage frequency similarly show model-specific variations, suggesting these aspects are likely more influenced by individual architectural choices and training approaches than by natural patterns in the data.



