\section{Conclusion}\label{sec:conclusion}

Our comprehensive investigation into instruction prioritization in LLMs has revealed critical limitations in current models' ability to consistently manage conflicting directives. Despite the widespread adoption of role-based instruction configurations in deployed LLM systems, our findings demonstrate that even state-of-the-art models lack robust mechanisms for maintaining proper instruction priorities, and often fail to acknowledge or resolve conflicts between system and user-level directives. While our attempts to address these issues through prompt engineering and fine-tuning showed modest improvements, they ultimately underscore the need for more fundamental advances in LLM architectures and training regimens to support reliable instruction priority management. These insights not only highlight an important gap in current LLM capabilities, but also provide concrete directions for future research in developing models with more sophisticated instruction-handling capabilities.


\section*{Limitations}\label{sec:limitation}

While our study provides a systematic evaluation of instruction hierarchy enforcement in LLMs, several opportunities for expansion remain. 

First, our analysis focuses on single-turn interactions with specific constraint phrasings. Real-world applications often involve multi-turn conversations with varied linguistic expressions of the same constraints, where instruction prioritization can evolve dynamically. Understanding how models handle such variations and extended interactions presents an exciting direction for practical applications.

Second, our evaluation is constrained to explicitly defined, programmatically verifiable constraints (e.g., formatting rules, keyword inclusion). More complex constraints—such as tone, reasoning depth, safety guidelines, role-playing character settings, or agentic system rules—require either extensive human annotation or evaluation by other LLMs, introducing additional methodological challenges. These qualitatively different constraints might exhibit distinct patterns of hierarchy enforcement, presenting an important direction for future investigation that could reveal new insights about how models handle different types of directives.

Third, our prompting and fine-tuning experiments use minimal settings. More extensive prompting, pretraining, or reinforcement learning approaches could yield different results. For example, the effectiveness of explicit constraint marking suggests a promising avenue for practical applications. If explicitly marking constraints in the user message improves prioritization, exploring explicit token-level priority encoding—where system and user instructions are assigned semantic priority markers—may offer a more robust solution for instruction hierarchy enforcement.

Last but not least, while our study reveals clear patterns in how models handle instruction hierarchies, the underlying mechanisms remain to be understood. Why do models show more consistent behavior with certain constraints than others? Is this related to the fundamental nature of next-token prediction, the way constraints influence token-level dependencies, or other architectural factors? Understanding these mechanisms could provide crucial insights for designing more robust instruction-following systems, and even for understanding how LLMs fundamentally process information.