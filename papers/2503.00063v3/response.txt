\section{Related Work}
\noindent \textbf{White-box attack on point cloud.}
Existing works of point cloud attacks can be roughly divided into white-box attacks and black-box attacks. %White box attack means that all information of the attacked classification model can be used to update perturbations, including the prediction and gradient of the model. 
Recently, most white-box attack works adopt \emph{point-based attacks}**Liu et al., "Point-Based Adversarial Attacks"**. Liu et al.** extended the gradient-based adversarial attack FGSM** strategy to point clouds.%, iteratively searching the desired point-wise perturbation under an $l_2$ norm constraint.
3D-Adv** introduced the C\&W attack framework** in point cloud attacks, producing adversarial examples by shifting point coordinates and adding additional points.
%with the guidance of the target classifier. Afterward, 
Tsai \etal** improved the C\&W attack framework by introducing a KNN regularization term to suppress outlier points and compact point cloud surface. GeoA\text{$^3$}** uses a combined geometry-aware objective to maintain local curvature consistency and a uniform surface.% on the adversarial point cloud. 
Zheng \etal** proposed that deleting a small number of points with high saliency can effectively cause misclassification.
Apart from the point-based methods mentioned above, several studies have explored \emph{shape-based attack}. Liu \etal** introduced shape-based attacks by adding new features to objects. %, as well as using a Gaussian kernel for sinking operations to deform point clouds. 
Zhang \etal** and Miao \etal** proposed directly attacking the mesh to generate smoother results.%, employing edge length regularization and Gaussian curvature regularization, respectively.
Tang \etal** proposed to adversarially stretch the latent variables in an auto-encoder, which can be decoded as smooth adversarial point clouds. %Lou \etal** proposed a shape-based attack HiT-ADV, which conducts a two-stage search for attack regions based on saliency and imperceptibility scores, and then adds deformation perturbations in each attack region using Gaussian kernel functions. 
HiT-ADV** is a shape-based attack, that first search attack regions based on saliency and imperceptibility scores, and then adds deformation perturbations in each attack region with Gaussian kernel. 
Besides, some works **attack point clouds in the feature space for imperceptible attack. While these methods eliminate outliers and ensure smoothness, they still require optimization for each point cloud, resulting in a high time cost.  To this end, universal attack** was proposed to compute universal perturbations for point clouds with specific patterns.

\noindent \textbf{Black-box attack on point cloud.}
The black-box attack can be further classified as transfer-based** and boundary-based black-box attacks**. For point cloud, most works focus on transfer-based black-box attacks. AdvPC** leverages a point cloud auto-encoder to enhance the transferability of adversarial point clouds, while AOF** targets the low-frequency components of 3D point clouds to disrupt general features. SI-Adv** projects points onto a tangent plane and introduces perturbations to create shape-invariant point clouds. Eidos** is a transfer-based attack that allows adversarial examples trained on one classifier to be transferred to another. 3DHacker** generates adversarial samples using only black-box hard labels. PF-Attack** and ANF** optimize perturbations and their subcomponents through adversarial noise factorization near decision boundaries, reducing dependency on surrogate models and enhancing transferability. SS-attack** applies random scaling or shearing to the input point cloud to prevent overfitting the white-box model, thus improving attack transferability. While these methods enhance model transferability, they still rely on iterative label-based generation of adversarial samples.

\noindent\textbf{No-box attacks.} The no-box approach is a classifier-free attack strategy that requires neither access to classifier details nor model queries. To date, only a few studies have addressed this challenging setup for images or skeletons. Li \etal** employed an autoencoding model to design an adversarial loss for no-box image attacks. Sun \etal** used a small subset of the training set to train an auxiliary model, leveraging this model to generate adversarial examples and attack the target model. Lu \etal** define an adversarial loss to maximize each adversary's dissimilarity with positive samples while minimizing its similarity with negative samples for the skeleton attack.
Zhang \etal** combined the low frequency of a clean image with the high frequency of a texture image to craft adversarial examples. Mou \etal** developed a decision-based attack strategy that generates universal adversarial perturbations and a set of texture-adversarial instances.

\noindent \textbf{Boundary-based attacks.}
Boundary-based attack method **Li et al., "Decision Boundary Attack"** is widely used in the 2D field, which is an efficient framework that uses the final decision results to implement black-box attacks. In the 2D field, the decision boundary attack process starts with two origin images called 
source-image and target-image with different labels. Then, it performs a binary search to obtain a boundary image on the decision boundary. 
Various 2D decision boundary-based attacks are proposed based on this general attack framework. Thomas \etal** and Vignesh \etal** propose to
choose more efficient random perturbation including Perlin noise and DCT in random walking steps instead of Gaussian perturbation. Chen \etal** conduct a gradient estimation method using the Monte-Carlo sampling strategy instead of random perturbation. Thereafter, several works **improve the gradient estimation strategy through sampling from representative low-dimensional subspace. Recently, 
Tao \etal** introduced boundary-based black-box attacks on point clouds, proposing 3DHacker, which leverages a developed decision boundary algorithm to attack point clouds using only black-box hard labels. He \etal** and Chen \etal** jointly optimize two sub-perturbations near decision boundaries via adversarial noise factorization, enhancing transferability. %However, these three boundary-based point cloud attack methods are all optimization-based, requiring optimization for each adversarial sample with guidances of specific models, leading to a higher time cost and imperfect transferability.
However, these boundary-based point cloud attack methods require optimization for each adversarial sample using model-specific guidance, resulting in higher time costs and limited transferability.