\section{Related Work}
\noindent \textbf{White-box attack on point cloud.}
Existing works of point cloud attacks can be roughly divided into white-box attacks and black-box attacks. %White box attack means that all information of the attacked classification model can be used to update perturbations, including the prediction and gradient of the model. 
Recently, most white-box attack works adopt \emph{point-based attacks}~\cite{liu2019extending,zhang2019defense,tsai2020robust,sun2021adversarially}. Liu et al.~\cite{liu2019extending} extended the gradient-based adversarial attack FGSM~\cite{goodfellow2014FGSM} strategy to point clouds.%, iteratively searching the desired point-wise perturbation under an $l_2$ norm constraint.
3D-Adv~\cite{xiang2019generating} introduced the C\&W attack framework~\cite{carlini2017cw} in point cloud attacks, producing adversarial examples by shifting point coordinates and adding additional points.
%with the guidance of the target classifier. Afterward, 
Tsai \etal~\cite{tsai2020robust} improved the C\&W attack framework by introducing a KNN regularization term to suppress outlier points and compact point cloud surface. GeoA\text{$^3$}~\cite{wen2020geometry} uses a combined geometry-aware objective to maintain local curvature consistency and a uniform surface.% on the adversarial point cloud. 
Zheng \etal~\cite{zheng2019saliency} proposed that deleting a small number of points with high saliency can effectively cause misclassification.
Apart from the point-based methods mentioned above, several studies have explored \emph{shape-based attack}. Liu \etal~\cite{liu2020sink} introduced shape-based attacks by adding new features to objects. %, as well as using a Gaussian kernel for sinking operations to deform point clouds. 
Zhang \etal~\cite{zhang2023meshattack} and Miao \etal~\cite{dong2022isometric} proposed directly attacking the mesh to generate smoother results.%, employing edge length regularization and Gaussian curvature regularization, respectively.
Tang \etal~\cite{tang2023manifold} proposed to adversarially stretch the latent variables in an auto-encoder, which can be decoded as smooth adversarial point clouds. %Lou \etal~\cite{lou2024hide} proposed a shape-based attack HiT-ADV, which conducts a two-stage search for attack regions based on saliency and imperceptibility scores, and then adds deformation perturbations in each attack region using Gaussian kernel functions. 
HiT-ADV~\cite{lou2024hide} is a shape-based attack, that first search attack regions based on saliency and imperceptibility scores, and then adds deformation perturbations in each attack region with Gaussian kernel. 
Besides, some works \cite{kim2021minimal,shi2022shape,dong2022isometric,tang2023manifold} attack point clouds in the feature space for imperceptible attack. While these methods eliminate outliers and ensure smoothness, they still require optimization for each point cloud, resulting in a high time cost.  To this end, universal attack~\cite{rampini2021universal,cheng2021universal} was proposed to compute universal perturbations for point clouds with specific patterns.

\noindent \textbf{Black-box attack on point cloud.}
The black-box attack can be further classified as transfer-based~\cite{hamdi2020advpc,liu2022imperceptible,huang2022siadv,liu2022boosting,zhang2024eidos,zhang2024improving,cai2024frequency} and boundary-based black-box attacks~\cite{tao20233dhacker,he2023generating,chen2024anf}. For point cloud, most works focus on transfer-based black-box attacks. AdvPC~\cite{hamdi2020advpc} leverages a point cloud auto-encoder to enhance the transferability of adversarial point clouds, while AOF~\cite{liu2022boosting} targets the low-frequency components of 3D point clouds to disrupt general features. SI-Adv~\cite{huang2022siadv} projects points onto a tangent plane and introduces perturbations to create shape-invariant point clouds. Eidos~\cite{zhang2024eidos} is a transfer-based attack that allows adversarial examples trained on one classifier to be transferred to another. 3DHacker~\cite{tao20233dhacker} generates adversarial samples using only black-box hard labels. PF-Attack~\cite{he2023generating} and ANF~\cite{chen2024anf} optimize perturbations and their subcomponents through adversarial noise factorization near decision boundaries, reducing dependency on surrogate models and enhancing transferability. SS-attack~\cite{zhang2024improving} applies random scaling or shearing to the input point cloud to prevent overfitting the white-box model, thus improving attack transferability. While these methods enhance model transferability, they still rely on iterative label-based generation of adversarial samples.

\noindent\textbf{No-box attacks.} The no-box approach is a classifier-free attack strategy that requires neither access to classifier details nor model queries. To date, only a few studies have addressed this challenging setup for images or skeletons. Li \etal~\cite{li2020practical} employed an autoencoding model to design an adversarial loss for no-box image attacks. Sun \etal~\cite{sun2022towards} used a small subset of the training set to train an auxiliary model, leveraging this model to generate adversarial examples and attack the target model. Lu \etal~\cite{lu2023hard} define an adversarial loss to maximize each adversary's dissimilarity with positive samples while minimizing its similarity with negative samples for the skeleton attack.
Zhang \etal~\cite{zhang2022practical} combined the low frequency of a clean image with the high frequency of a texture image to craft adversarial examples. Mou \etal~\cite{mou2024no} developed a decision-based attack strategy that generates universal adversarial perturbations and a set of texture-adversarial instances.
\iffalse
\noindent \textbf{Unsupervised 2D attacks.} Unsupervised attack~\cite{naseer2018task,zhao2019unsupervised,li2020practical,zhang2022unsupervised,yao2023unsupervised} refer to a type of attack method that does not rely on the category labels of samples. Naseer \etal~\cite{naseer2018task} used predictions of clear input images from a pre-trained classifier for supervision, eliminating the need for class labels. Yao \etal~\cite{yao2023unsupervised} employed the cycle consistency adversarial loss to achieve unsupervised attacks against object tracking models. %However, to our best knowledge, there has been no unsupervised attack in the 3D vision community so far, and directly adapting these 2D methods to the 3D field may face many challenges. 
A more typical unsupervised attack way is based on the no-box paradigm. Zhao \etal~\cite{zhao2019unsupervised} introduced an unsupervised adversarial image generation method with a generative adversarial network and deep feature-based loss. Li \etal~\cite{li2020practical} utilized an image-to-image auto-encoding model to devise an adversarial loss to achieve no-box image attacks.  Zhang \etal~\cite{zhang2022unsupervised} proposed the contrastive loss gradient attack, an unsupervised untargeted poisoning attack for attacking graph contrastive learning. 
While these methods enhance the transferability of 2D attacks, they cannot be directly applied to the 3D point cloud. To address this, we propose an unsupervised point cloud attack method called NoPain, which operates without requiring labels, thereby broadening the applicability of point cloud attacks.
\fi

\vspace{-1mm}
\noindent \textbf{Boundary-based attacks.}
Boundary-based attack method \cite{brendel2018decision} is widely used in the 2D field, which is an efficient framework that uses the final decision results to implement black-box attacks. In the 2D field, the decision boundary attack process starts with two origin images called 
source-image and target-image with different labels. Then, it performs a binary search to obtain a boundary image on the decision boundary. 
Various 2D decision boundary-based attacks are proposed based on this general attack framework. Thomas \etal~\cite{brunner2019guessing} and Vignesh \etal~\cite{srinivasan2019black} propose to
choose more efficient random perturbation including Perlin noise and DCT in random walking steps instead of Gaussian perturbation. Chen \etal~\cite{chen2020hopskipjumpattack} conduct a gradient estimation method using the Monte-Carlo sampling strategy instead of random perturbation. Thereafter, several works~\cite{li2020qeba,li2021nonlinear,li2022decision} improve the gradient estimation strategy through sampling from representative low-dimensional subspace. Recently, 
Tao \etal~\cite{tao20233dhacker} introduced boundary-based black-box attacks on point clouds, proposing 3DHacker, which leverages a developed decision boundary algorithm to attack point clouds using only black-box hard labels. He \etal~\cite{he2023generating} and Chen \etal~\cite{chen2024anf} jointly optimize two sub-perturbations near decision boundaries via adversarial noise factorization, enhancing transferability. %However, these three boundary-based point cloud attack methods are all optimization-based, requiring optimization for each adversarial sample with guidances of specific models, leading to a higher time cost and imperfect transferability.
However, these boundary-based point cloud attack methods require optimization for each adversarial sample using model-specific guidance, resulting in higher time costs and limited transferability.