\section{Introduction}

Recent advancements in large language models (LLMs)~\cite{openai2023gpt4, dubey2024llama3,yang2024qwen2_5,liu2024deepseekv3} have demonstrated remarkable capabilities in understanding, generating, and reasoning across diverse domains, significantly advancing their application in various fields. 
Among these applications, question answering (QA) based on provided contexts has emerged as one of the most prominent use cases for LLMs.

As LLMs' capabilities continue to evolve and user expectations grow, users increasingly supply multiple documents retrieved in Retrieval-Augmented Generation (RAG) scenarios or long-context reference documents to guide LLMs in generating contextually relevant responses.
However, in practice, such retrieved documents or long-context references often contain substantial noise, including information irrelevant to the user's query.
Recent studies~\cite{ye2025difftrans,liu2024lost} highlight a critical challenge that LLMs frequently struggle to accurately identify and extract key information from these noisy contexts, limiting their effectiveness in real-world applications.

\begin{figure}[!tb]
    \centering
    \includegraphics[width=0.928\linewidth]{figs/motivation.pdf} 
    \caption{Normalized attention score. Transformers often miss the golden document in a noisy context.} 
    \label{fig:motivation}
\end{figure}

As illustrated in \Cref{fig:motivation}, we visualize the normalized attention scores assigned to retrieved documents in the RAG scenario, which includes various noisy documents and a single golden document. 
The task involves identifying the correct answer within noisy contexts. 
Our analysis evaluates several LLMs, including Llama3.1-8B-base~\cite{meta2024llama3_1}, Llama3.1-8B-inst~\cite{meta2024llama3_1}, and Llama3-ChatQA2-8B~\cite{xu2024chatqa2}, the latter of which has been fine-tuned specifically for long-context and RAG applications. 
The visualization demonstrates that the Transformer architecture tends to allocate only a small proportion of attention scores to the golden document, while disproportionately focusing on irrelevant or later-positioned documents. 
% Notably, ChatQA2, despite its fine-tuning for long-context and RAG tasks, tends to over-attend to documents positioned later in the sequence rather than the golden document. 
% Similarly, the aligned LLM struggles to maintain focus on relevant information in noisy environments.
These findings highlight a persistent challenge for Transformer-based architectures including effectively identifying and prioritizing relevant documents in the presence of noise. 
The issue~\cite{ye2025difftrans} arises from the non-negligible allocation of attention scores to irrelevant content, which ultimately obscures the correct answer and undermines model performance.

\begin{figure}[tb!]
    \centering
    \includegraphics[width=0.928\linewidth]{figs/radar.pdf} 
    \caption{Qwen2.5-OpAmp-72B achieves the best average performance in various noisy-context benchmarks compared to current SOTA LLMs.}
    \label{fig:radar}
\end{figure}

\citet{ye2025difftrans} propose a differential attention mechanism designed to mitigate attention noise through differential denoising, inspired by the principles of differential amplifiers in electrical engineering. 
However, differential amplifiers are effective in scenarios requiring a high common-mode rejection ratio (CMRR) considering that they only focus on differential gain. 
This is unsuitable for attention denoising in the Transformer block.
Training a differential transformer from scratch entails great computation costs and introduces significant risks, further limiting its practical applicability.

Inspired by the operational amplifiers (OpAmp), we introduce OpAmp adaptation with adapters, an efficient approach for refining the attention mechanism to enhance focus on the most relevant context leveraging parameter-efficient fine-tuning (PEFT) techniques. 
The OpAmp adaptation enables simultaneous control of differential gain and common-mode gain through the management of the CMRR. 
Building on the OpAmp design, our approach facilitates the training of OpAmp models using pre-trained Transformer architectures, eliminating the need for training from scratch. 
This strategy significantly reduces computational costs compared to previous methods. 
As demonstrated in \Cref{fig:radar}, our Qwen2.5-OpAmp-72B model, trained with the OpAmp adaptation, achieves superior average performance across various noisy-context benchmarks compared to current state-of-the-art (SOTA) LLMs. 
Our contributions are as follows:
\begin{itemize}[itemsep=0pt,topsep=0pt,parsep=0pt]
    \item We introduce the OpAmp adaptation for zoom attention to the most relevant context in noisy contexts;
    \item Implement OpAmp adaptation with adapters, which are fine-tuned with our noisy context dataset, achieving significant improvements;
    \item Develop OpAmp models with our OpAmp adaptation method, surpassing current SOTA LLMs in various noisy-context benchmarks.
\end{itemize}