\section*{Limitation}
The OpAmp adaptation with adapters introduces a marginally higher number of parameters compared to the standard PEFT training process with QLoRA. 
Consequently, the supervised fine-tuning process for our OpAmp models demands slightly greater GPU memory allocation and computational time. 
Additionally, our OpAmp models incur a minor latency during inference when compared to the original pre-trained LLMs.