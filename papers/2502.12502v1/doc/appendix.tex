\begin{table}[!tb]
\centering
% \setlength\tabcolsep{3.2pt}
\footnotesize
\resizebox{0.988\linewidth}{!}{
\begin{tabular}{cccccc}
\toprule 
 lr & epoch & LoRA $r$ & LoRA $\alpha$ & Adapter Dim \\
\midrule
$2 \times 10^{-4}$   &1    &64    &16  &512 \\
\bottomrule
\end{tabular}
}
\caption{Hyperparameters of supervised fine-tuning.}
\label{table:hyperparameters}
\end{table}

\begin{table}[!tb]
\centering
\setlength\tabcolsep{3.2pt}
\footnotesize
\resizebox{0.988\linewidth}{!}{
\begin{tabular}{lccc}
\toprule 
 & LongCite-45k & Neural-Bridge-RAG & Tulu3-SFT-Mix \\
\midrule
NCFT    &30k    &20k    &450k \\
\bottomrule
\end{tabular}
}
\caption{The proportion of LongCite-45k, Neural-Bridge-RAG and Tulu3-SFT-Mix in the NCFT dataset.}
\label{table:proportion}
\end{table}

\section{Implementation Details}
\label{sec:training_details}
The training process entailed using a constant learning rate schedule with a warm-up ratio of 0.03, and the paged AdamW \cite{dettmers2024qlora, loshchilov2017adamw} optimizer with a learning rate of $2 \times 10^{-4}$, no weight decay, a batch size of 128, and a sequence length of 8192 tokens. 
The models underwent instruction tuning for one epoch on 16 A100 GPUs, each with 80G memory.

Moreover, we employed the QLoRA \cite{dettmers2024qlora} technique for efficient fine-tuning. 
As for the QLoRA configuration, we use a 4-bit quantization scheme for our experiments, which significantly reduces memory usage while preserving model performance.
We show the hyperparameters for supervised fine-tuning in \Cref{table:hyperparameters}.

\section{Training Datasets}
\label{sec:training_datasets}

As shown in Table~\ref{table:proportion}, we shows the proportion of LongCite-45k~\cite{zhang2024longcite}, Neural-Bridge-RAG~\cite{NeuralBridge2024ragdataset} and Tulu3-SFT-Mix~\cite{lambert2024tulu3} in the NCFT dataset.

Considering the original format and quantity of LongCite-45k and Neural-Bridge-RAG, we perform data processing to simulate the noisy context scenarios. 
Firstly, we filter the Chinese corpus and divide the context into several chunks.
Then we preserve the chunks with golden documents and introduce relevant or irrelevant chunks as noise.
Finally, we filter low-quality corpora (too long or too short).
We obtained our supervised fine-tuning dataset after data processing
which encompasses a wide range of topics, and the noise ratio in the dataset ranges from 0 to 1, aiming to cover a variety of real-world situations and use cases.

\section{Evaluation Benchmarks}
\label{sec:evaluation_benchmarks}

We show the details of the noisy-context evaluation benchmark in \Cref{table:noisy-context benchmark}. 
Qasper, HotpotQA, and MuSiQue are directly derived from the LongBench~\cite{bai2024longbench}.
In contrast, CoQA, QuAC, and QReCC are QA datasets selected from ChatQA~\cite{liu2024chatqa} and have been noise-augmented in a manner consistent with \Cref{sec:training_datasets} to align with the noisy-RAG format. 
For the QuALITY dataset, we retain only the subset labeled as ``hard''.
Similarly, for the NarrativeQA, Loogle, and MultiHopRAG datasets, we apply filters based on context length and response quality to further enhance the benchmark's ability to differentiate between models.

\begin{table}[!tb]
\centering
\setlength\tabcolsep{3.2pt}
\footnotesize
\resizebox{0.988\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule 
Benchmark & Source & Max Length & Metric &\# Data \\
\midrule
\multicolumn{5}{l}{\textit{Long-Context QA}} \\ 
\midrule
NarrativeQA & Literature, Film & 64K & EM& 1009 \\ 
Qasper  & Science & 8K  & PM & 200 \\ 
QuALITY & Literature & 8K  & Acc. & 1065 \\ 
LooGLE  & Science & 32K  & EM & 1427 \\
\midrule
\multicolumn{5}{l}{\textit{Multi-Hop QA}} \\
\midrule
HotpotQA & Wikipedia & 16K & EM & 200 \\  
MuSiQue & Wikipedia & 16K & EM & 200 \\ 
MultiHopRAG & News & 8K & EM & 2255 \\
\midrule
\multicolumn{5}{l}{\textit{Noisy-RAG QA}} \\
\midrule
CoQA & Multi-field & 4K & EM & 500 \\ 
QuAC & Wikipedia   & 4K & PM & 996 \\ 
QReCC & Multi-field & 4K & PM & 643 \\
\bottomrule
\end{tabular}
}
\caption{An overview of the dataset statistics for the noisy-context benchmark. The `Source' column indicates the origin of the context.}
\label{table:noisy-context benchmark}
\end{table}

% \section{Case Studies}

