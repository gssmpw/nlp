\section{Related Works}

\subsection{Question Answering with Noisy Contexts}
The internal knowledge of LLMs often fails to meet diverse application needs~\cite{he2022retrieval, Ji2023Survey}, driving research into integrating external knowledge. 
Among the proposed solutions~\cite{guu2020realm, beltagy2020longformer, wang2024knowledgeediting}, RAG~\cite{borgeaud2022retrieving, ren2024knowledgeboundary} and long-context modeling techniques~\cite{press2022trainshorttestlong, chen2023extendingcontextwindow} have emerged as two prominent strategies for incorporating external knowledge stored in long-text formats. 
However, recent studies~\cite{Shi2023distracted, liu2024lost, lv2024coarse, ye2025difftrans} have identified a significant challenge. 
Specifically, as the number of retrieved documents grows and the length of input contexts expands, the model is increasingly exposed to noise, which is often the non-critical information unrelated to the query.
This noisy-context scenario significantly degrades the performance of LLMs on QA tasks~\cite{chen2023benchmark}. 
Consequently, we propose the OpAmp adaptation with adapters, a plug-and-play solution that minimizes noisy context impact with low computation costs, enhancing the performance in such scenarios.

\subsection{Parameter Efficient Fine-Tuning}
Traditionally, full fine-tuning is the predominant approach for fine-tuning pre-trained models, including LLMs. 
However, this method entails substantial computational costs, particularly regarding time consumption and GPU memory usage. 
To address these challenges, a variety of PEFT methods have been developed \cite{houlsby2019adapter, hu2021lora, dettmers2024qlora, wu2024pesc, li2021prefixtuning, lester2021prompttuning}, enabling efficient fine-tuning without compromising performance compared to full fine-tuning. 
PEFT focuses on training a limited subset of parameters within the existing model or newly inserted modules.
Adapter-based methods \cite{houlsby2019adapter, hu2021lora, dettmers2024qlora, wu2024pesc} insert learnable modules into Transformer blocks, which contain a small number of parameters. 
These adapters are fine-tuned instead of the original model weights. 
Among these methods, QLoRA \cite{dettmers2024qlora} has gained significant attention for its efficiency in fine-tuning LLMs while maintaining performance comparable to full fine-tuning.
Another emerging trend in PEFT is prefix-tuning \cite{lester2021prompttuning, li2021prefixtuning}, which involves adding learnable token vectors to the input sequence. 
% However, prefix-tuning often exhibits inferior performance compared to adapter-based approaches. 
In this study, we introduce adapters to perform OpAmp adaptation. 
Specifically, adapters reformulate the computation of the original attention mechanism into the OpAmp attention mechanism.
% as shown in \Cref{eq:opampattn}.

\subsection{Adaptation of Pre-trained Models}
Recent studies~\cite{chen2015net2net, lin2021m610t, komatsuzaki2022sparseupcycle, wu2024pesc} have focused on improving training efficiency by leveraging pre-trained model weights for a warm start, thus accelerating convergence and minimizing training costs.
\citet{komatsuzaki2022sparseupcycle} and \citet{wu2024pesc} introduce methods to initialize sparse MoE models using weights from a pre-trained dense model.
These approaches significantly reduce the required training resources.
In this paper, we train our OpAmp models with OpAmp attention blocks using weights from pre-trained LLMs.