\section{Conclusion}
Inspired by the operational amplifiers, we introduce the OpAmp adaptation implemented with adapters in this study. 
By integrating this adapter into pre-trained Transformer blocks, our approach enhances the model's ability to focus on the most relevant context without expensive full-scale training from scratch. 
We implement our OpAmp models and other baselines with our noisy-context fine-tuning dataset, NCFT, for fair comparisons.
The OpAmp adaptation demonstrates significant performance gains across LLMs of varying model sizes. 
Extensive empirical evaluations are conducted on extensive noisy-context benchmarks. 
The results indicate that our Qwen2.5-OpAmp-72B model, fine-tuned with our OpAmp adaptation, outperforms current SOTA LLMs,
including DeepSeek-V3~\cite{liu2024deepseekv3} and GPT-4o~\cite{hurst2024gpt4o}.

