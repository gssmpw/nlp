\section{Experiments}

\begin{table*}[t]
\centering
\renewcommand{\arraystretch}{1.0}
% \setlength\tabcolsep{3.6pt}
\resizebox{0.808\linewidth}{!}{
\footnotesize
\begin{tabularx}{0.878\linewidth}{X|cc|cc|cc}
\toprule
& \makecell{Qwen2.5 \\ OpAmp-72B} & \makecell{Llama3 \\ ChatQA2-70B} & \makecell{Qwen2.5 \\ 72B inst} & \makecell{Llama3.3 \\ 70B inst} & \makecell{DeepSeek \\ V3} & \makecell{GPT-4o \\ 0806} \\
\midrule
\makecell[X]{LooGLE (EM) \newline \cite{li2023loogle}}
& \textbf{66.3} & 59.1 & 64.9 & 63.0 & 63.4 & 62.7 \\ 
\midrule
\makecell[X]{NarrativeQA (EM) \newline \cite{kovcisky2018narrativeqa}}
& \textbf{61.7} & 59.8 & 60.2 & 61.5 & 60.5 & 61.5 \\  
\midrule
\makecell[X]{MultiHopRAG (EM) \newline \cite{tang2024multihoprag}} 
& \textbf{89.6} & 78.2 & 89.2 & 83.7 & 88.6 & 87.7\\ 
\midrule
\makecell[X]{HotpotQA (EM) \newline \cite{yang2018hotpotqa}}    
& \textbf{77.5} & 70.5 & 76.0 & 74.5 & 77.0 & \textbf{77.5}\\ 
\midrule
\makecell[X]{MuSiQue (EM) \newline \cite{trivedi2022musique}} 
& 48.0 & 39.0 & 44.0 & 47.5 & 52.5 & \textbf{53.0}\\ 
\midrule
\makecell[X]{CoQA (EM) \newline \cite{reddy2019coqa}}   
& \textbf{92.4} & 80.2 & 85.8 & 88.2 & 88.4 & 88.6\\  
\bottomrule
\end{tabularx}}
\caption{Performance of Qwen2.5-OpAmp-72B on various noisy context benchmarks. 
We present a detailed comparison of the Qwen2.5-OpAmp-72B with current SOTA open-source and commercial LLMs. 
We bold the highest scores among all models.}
\label{table:main_best}
\end{table*}

\begin{table*}[t]
\centering
\renewcommand{\arraystretch}{1.0}
\setlength\tabcolsep{9.6pt}
\resizebox{0.808\linewidth}{!}{
\footnotesize
\begin{tabularx}{0.862\linewidth}{X|cc|ccc}
\toprule
& \makecell{Llama3.1 \\ OpAmp-8B} & \makecell{Llama3 \\ ChatQA2-8B} & \makecell{Mistral \\ 7B inst-v0.3} & \makecell{Llama3.1 \\ 8B inst} & \makecell{Qwen2.5 \\ 7B inst} \\
\midrule
\makecell[X]{LooGLE (EM)}  
& \textbf{56.6} & 50.7 & 51.6 & 56.1 & 53.8 \\ 
\midrule
\makecell[X]{NarrativeQA (EM)}
& \textbf{57.4} & 53.1 & 44.7 & 55.9 & 47.7 \\ 
\midrule
\makecell[X]{MultiHopRAG (EM)}    
& \textbf{70.5} & 50.9 & 69.5 & 63.9 & 66.9 \\ 
\midrule
\makecell[X]{HotpotQA (EM)}   
& \textbf{61.0} & 56.5 & 58.0 & 58.5 & 59.5 \\
\midrule
\makecell[X]{MuSiQue (EM)} 
& \textbf{35.0} & 23.0 & 28.5 & 29.5 & 31.5 \\ 
\midrule
\makecell[X]{CoQA (EM)}   
& \textbf{85.4} & 78.2 & 70.6 & 82.2 & 84.2 \\ 
\bottomrule
\end{tabularx}}
\caption{Performance of Llama3.1-OpAmp-8B on various noisy context benchmarks. 
We present a detailed comparison of the Llama3.1-OpAmp-8B with various open-source LLMs with similar parameters. 
We bold the highest scores among all models.}
\label{table:main_small}
\end{table*}

\subsection{Training Settings}

\minisection{Training Data} 
We incorporate some noisy context data into the general supervised fine-tuning dataset to enhance LLMs' denoising capability in noisy context scenarios.
This training involved integrating three distinct datasets: LongCite-45k~\cite{zhang2024longcite}, Neural-Bridge-RAG~\cite{NeuralBridge2024ragdataset} and Tulu3-SFT-Mix~\cite{lambert2024tulu3}.
% The contexts in the first two datasets are divided into several chunks, which include golden documents and noisy documents, to better simulate noisy context scenarios.
% Data augmentation is also performed to expand the noisy context data.
After data processing, we get the \textbf{N}oisy \textbf{C}ontext \textbf{F}ine-\textbf{T}uning (NCFT) dataset for supervised fine-tuning. 
We provide more details of the NCFT dataset in \Cref{sec:training_datasets}.

\minisection{OpAmp Models}
We select two pre-trained models with different model sizes, Qwen2.5-72B~\cite{yang2024qwen2_5} and Llama3.1-8B~\cite{dubey2024llama3}, as our base models to train our OpAmp models using the NCFT dataset.
Moreover, we use the QLoRA technique to update the other parameters in the pre-trained models instead of full fine-tuning.
Please refer to \Cref{sec:training_details} for more details.

\subsection{Evalutaion Settings}

\minisection{Baselines} 
We compare our OpAmp models with existing powerful LLMs in our evaluation benchmark.
These LLMs include Llama3-ChatQA2-70B~\cite{xu2024chatqa2}, Qwen2.5-72B-inst~\cite{yang2024qwen2_5}, Llama3.3-70B-inst~\cite{dubey2024llama3}, DeepSeek-V3~\cite{liu2024deepseekv3}, GPT-4o-0806~\cite{hurst2024gpt4o}, Llama3-ChatQA2-8B~\cite{xu2024chatqa2}, Mistral-7B-inst-v0.3~\cite{jiang2023mistral}, Llama3.1-8B-inst~\cite{meta2024llama3_1} and Qwen2.5-7B-inst~\cite{yang2024qwen2_5}.

\minisection{Evalution Benchmarks} 
Our evaluation benchmarks are designed using a spectrum of well-known datasets and benchmarks including LongBench~\cite{bai2024longbench} and ChatQA~\cite{liu2024chatqa}.
After some selection and filtration, these benchmarks can be categorized as follows:
\begin{itemize}[itemsep=0pt,topsep=0pt,parsep=0pt]
    \item \textbf{Long-Context QA:} The evaluation encompasses partial match (PM), exact match (EM), and accuracy (Acc.) metrics for various long-context QA benchmarks, including NarrativeQA~\cite{kovcisky2018narrativeqa}, Qasper~\cite{dasigi2021qasper}, QuALITY~\cite{pang2021quality}, and LooGLE~\cite{li2023loogle}.
    \item \textbf{Multi-Hop QA:} Assessment of multi-hop reasoning performance on various benchmarks, including HotpotQA~\cite{yang2018hotpotqa}, MuSiQue~\cite{trivedi2022musique}, and MultiHopRAG~\cite{tang2024multihoprag}, using the EM metric.
    \item \textbf{Noisy-RAG QA:} PM and EM scores for RAG scenarios using CoQA~\cite{reddy2019coqa}, QuAC~\cite{choi2018quac}, and QReCC~\cite{anantha2020qrecc} benchmarks.
\end{itemize}
For a more detailed composition of the evaluation benchmark, please refer to \Cref{sec:evaluation_benchmarks}.

\subsection{Evaluation on Noisy-Context Benchmarks}

We perform various experiments on LLMs with different sizes to evaluate the capabilities of our OpAmp adaptation.
For LLMs with more than 70B parameters, we compare Qwen2.5-OpAmp-72B with Llama3-ChatQA2-70B, Qwen2.5-72B-inst, Llama3.3-70B-inst, DeepSeek-V3, and GPT-4o-0806.
For LLMs with around 7B parameters, we compare Llama3.1-OpAmp-8B with Llama3-ChatQA2-8B, Mistral-7B-inst-v0.3, Llama3.1-8B-inst, and Qwen2.5-7B-inst.
The noisy-context benchmarks cover a wide range of tasks.
For long-context scenarios, LooGLE and NarrativeQA are selected.
We utilize MultiHopRAG, HotpotQA, and MuSiQue for Multi-Hop reasoning evaluation and CoQA for noisy-RAG scenarios.
\Cref{table:main_best} and \Cref{table:main_small} demonstrate the superior performance of our OpAmp models compared to other existing powerful LLMs, underscoring the significant capabilities and effectiveness of the OpAmp adaptation in noisy context scenarios.
% regardless of the base model utilized.

\begin{table*}[tb!]
\centering
% \renewcommand{\arraystretch}{1.06}
\setlength\tabcolsep{3.6pt}
\resizebox{0.948\linewidth}{!}{
\begin{tabular}{lc|c|cccc|ccc|ccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{$\mathcal{K}$} & \multirow{2}{*}{Avg.} & Qasper & LooGLE & NarrativeQA & QuALITY & MultiHopRAG & HotpotQA & MuSiQue & CoQA & QuAC & QReCC \\ 
&  &  & (PM) & (EM) & (EM) & (Acc.) & (EM) & (EM) & (EM) & (EM) & (PM) & (PM) \\ 
\midrule
QLoRA & - & 52.4 & 38.9 & 53.1 & 55.7 & 76.1 & 68.4 & 56.5 & 31.5 & 83.6 & 25.2 & 35.4 \\
\midrule
\multirow{4}{*}{\makecell{OpAmp \\ Adapter}}
& 1  & 54.1 (+1.7) & 40.8 & 56.0 & 56.4 & 79.2 & 68.5 & 57.5 & 32.5 & \textbf{85.8} & 26.1 & 38.3 \\
& 5  & 54.3 (+1.9) & 41.2 & 56.5 & 56.9 & 77.8 & 69.5 & \textbf{62.0} & 31.5 & 84.6 & 25.5 & 37.1 \\
& 10 & \textbf{55.4 (+3.0)} & \textbf{43.1} & \textbf{56.6} & \textbf{57.4} & 79.0 & 70.5 & 61.0 & \textbf{35.0} & 85.4 & \textbf{26.5} & \textbf{39.8} \\
& 20 & 54.4 (+2.0) & 41.5 & 55.4 & 56.4 & \textbf{79.3} & \textbf{71.4} & 59.0 & 33.0 & 84.0 & 26.2 & 37.0 \\
\bottomrule
\end{tabular}}
\caption{Ablation studies on various noisy context benchmarks using Llama3.1-8B-base as the base model.
We bold the highest scores for each benchmark.}
\label{table:ab_study_cmrr}
\end{table*}

\minisection{Long-Context Evaluation}
Long-context evaluation requires LLMs to disregard large volumes of context-related but question-irrelevant information within extensive texts, accurately identify the paragraphs relevant to the answer, and generate responses based on these pertinent segments. 
Our Qwen2.5-OpAmp-72B model achieves EM scores of up to 66.3\% on the LooGLE benchmark with a maximum context length of 32K tokens and 61.7\% on the NarrativeQA benchmark with a maximum context length of 64K tokens. 
Similarly, our Llama3.1-OpAmp-8B model attains the highest EM score of 56.6\% on the LooGLE benchmark and leads with a score of 57.4\% on the NarrativeQA benchmark. 
These experiment results underscore the robust capability of our OpAmp models to filter out context-related noise and accurately locate answers within long contexts. 
Furthermore, they demonstrate the strong generalization ability of our approach across different model sizes.

\begin{table}[tb!]
\centering
% \renewcommand{\arraystretch}{1.06}
\setlength\tabcolsep{2.4pt}
\resizebox{0.988\linewidth}{!}{
\begin{tabular}{ll|ccc|ccc|ccc}
\toprule
&  & \multicolumn{3}{c|}{CoQA (EM)} & \multicolumn{3}{c|}{QuAC (PM)} & \multicolumn{3}{c}{QReCC (PM)} \\
\midrule
% \multirow{2}{*}{Model} & \multirow{2}{*}{CMRR}
\multicolumn{2}{c|}{Noise Ratio} & 0.0 & 0.8 & 0.9 & 0.0 & 0.8 & 0.9 & 0.0 & 0.8 & 0.9 \\ 
\midrule
\multicolumn{2}{c|}{QLoRA} & 89.8 & 85.4 & 83.6 & 27.5 & 26.1 & 25.2 & 36.5 & 36.4 & 35.4 \\
\midrule
\multirow{4}{*}{\makecell{OpAmp \\ Adapter}}
& \multirow{4}{*}{$\mathcal{K} = \left\{
    \begin{tabular}{c}
    1 \\ 
    5 \\ 
    10 \\
    20
    \end{tabular}
\right.$}  
& 90.4 & 85.6 & \textbf{85.8} & 28.5 & 26.2 & 26.1 & 39.4 & 39.1 & 38.3 \\
& & 90.0 & 85.6 & 84.6 & 27.5 & 26.7 & 25.5 & 38.2 & 37.3 & 37.1 \\
& & 91.2 & \textbf{88.0} & 85.4 & 28.5 & 26.5 & \textbf{26.5} & \textbf{40.8} & \textbf{39.8} & \textbf{39.8} \\
& & \textbf{91.8} & 86.6 & 84.0 & \textbf{28.6} & \textbf{28.0} & 26.2 & 38.5 & 38.1 & 37.0 \\
\bottomrule
\end{tabular}}
\caption{Ablation studies on various benchmarks with different noise ratios using Llama3.1-8B-base as the base model.
We bold the highest scores.}
\label{table:ab_study_noise}
\end{table}

\minisection{Multi-Hop Evaluation}
Multi-hop evaluation is designed to assess the capability of LLMs to extract and synthesize relevant information from multiple documents for reasoning. 
This task requires LLMs to filter out irrelevant or noncritical documents to minimize interference during the reasoning process.
Our Qwen2.5-OpAmp-72B model demonstrates strong performance on multi-hop reasoning tasks, achieving high scores of 89.6\% on MultiHopRAG and 77.5\% on HotpotQA, with notable advantages over competing models. 
Although it performs slightly weaker than top-performing LLMs on the MuSiQue benchmark, its EM score of 48.0\% remains competitive for multi-hop reasoning tasks. 
Additionally, our Llama3.1-OpAmp-8B model also excels in multi-hop reasoning benchmarks, achieving top scores of 70.5\% on MultiHopRAG, 61.0\% on HotpotQA, and 35.0\% on MuSiQue, consistently surpassing other models. 
These results highlight the superior ability of our OpAmp models to handle complex, multi-step reasoning tasks across various benchmarks, underscoring its effectiveness in enhancing reasoning capabilities.

\minisection{Noisy-RAG Evaluation}
For the currently most widely adopted RAG technology, we conduct the noisy-RAG evaluation to assess the ability of LLMs to filter out irrelevant documents and accurately identify the document containing the correct answer in real-world RAG scenarios.
Our Qwen2.5-OpAmp-72B model achieves a top score of 92.4\% on the CoQA benchmark, surpassing the second-closest LLM, DeepSeek-V3, by a significant margin of 4\%. 
Our Llama3.1-OpAmp-8B model also attains a leading score of 85.4\% on the CoQA benchmark, outperforming Qwen2.5-7B-inst by 1.2\%. 
These experimental results highlight the superior performance of our OpAmp models in identifying correct answers within real-world RAG scenarios, exhibiting robust resistance to interference and noise.

\subsection{Ablation Studies}
\label{sec:ab}

To further investigate the contribution of $\mathcal{K}$, we perform a series of ablation studies. 
Additionally, we compare our OpAmp approach with the QLoRA technique. 
In brief, we denote the OpAmp adapter as adapters implemented for our OpAmp adaptation.
To ensure fair comparisons in these ablation studies, both OpAmp and QLoRA models are fine-tuned using the same dataset, NCFT.

\minisection{CMRR}
\Cref{table:ab_study_cmrr} presents a comparative analysis of QLoRA and the OpAmp adapter for enhancing the Llama3.1-8B-base model across various noisy context benchmarks. 
The OpAmp adapter demonstrates consistent superiority over QLoRA across all evaluated benchmarks. 
Specifically, QLoRA achieves an average score of 52.4\%, whereas the OpAmp adapter significantly enhances performance, with the best results observed at $\mathcal{K}=10$, yielding an average score of 55.4\%. 
% Notably, the OpAmp adapter with $\mathcal{K}=10$ attains the highest average score, highlighting its efficacy in managing noisy-context tasks.
When examining the impact of different values of $\mathcal{K}$, $\mathcal{K}=10$ emerges as the optimal configuration across multiple benchmarks. 
Larger value ($\mathcal{K}=20$) exhibits diminishing returns, while smaller values ($\mathcal{K}=1, 5$) perform adequately but are marginally less competitive. 
This suggests our statement that attention denoising requires only a modest $\mathcal{K}$ instead of the $\mathcal{K} \rightarrow \infty$ used in the differential transformer architecture~\cite{ye2025difftrans}.

\begin{table}[tb!]
\centering
% \renewcommand{\arraystretch}{1.06}
\setlength\tabcolsep{2.0pt}
\resizebox{0.988\linewidth}{!}{
\begin{tabular}{lc|ccc|c}
\toprule
\multirow{3}{*}{Method} & \multirow{3}{*}{$\mathcal{K}$} & \multicolumn{4}{c}{FaithEval} \\
\cmidrule{3-6}
& & Inconsistent & Unanswerable & Counterfactual & \multirow{2}{*}{Avg.}\\ 
& & (EM) & (EM) & (EM) & \\ 
\midrule
QLoRA & - & 24.1 & 46.1 & 71.6 & 47.3\\
\midrule
\multirow{4}{*}{\makecell{OpAmp \\ Adapter}}
& 1 & \textbf{45.5} & 53.1 & \textbf{76.3} & \textbf{58.3 (+11.0)} \\
& 5 & 42.1 & \textbf{53.7} & 75.9 & 57.2 (+9.90) \\
& 10 & 45.3 & 53.0 & 75.1 & 57.8 (+10.5) \\
& 20 & 22.3 & 58.8 & 73.8 & 51.6 (+4.30) \\
\bottomrule
\end{tabular}}
\caption{Ablation studies on FaithEval using Llama3.1-8B-base as the base model.
We bold the highest scores.}
\label{table:ab_study_faitheval}
\end{table}

\minisection{Noise Ratio}
The ablation study detailed in \Cref{table:ab_study_noise} assesses the performance of QLoRA and the OpAmp adapter across varying noise ratios (0.0, 0.8, 0.9) on noisy-RAG benchmarks, including CoQA, QuAC, and QReCC. 
The noise ratio is simulated by introducing noise documents into the original golden document, replicating increasingly challenging real-world RAG scenarios. 
As expected, performance across all methods generally degrades with increasing noise ratios, reflecting the growing difficulty of extracting relevant information from cluttered contexts.
QLoRA exhibits a steady decline in performance as noise levels increase. 
For instance, its score on CoQA drops from 89.8\% at a noise ratio of 0.0 to 83.6\% at 0.9. 
In contrast, the OpAmp adapter demonstrates greater robustness, particularly when configured with $\mathcal{K}=10$. 
Moreover, higher values of $\mathcal{K}$ occasionally underperform compared to $\mathcal{K}=10$, indicating that excessive attention denoise may compromise the capability.
Overall, the OpAmp adapter consistently outperforms QLoRA across all noise levels, with $\mathcal{K}=10$ emerging as the optimal configuration for balancing robustness and performance under noisy conditions. 
This underscores the effectiveness of our method in handling challenging RAG scenarios.

\minisection{Hallucination}
As shown in \Cref{table:ab_study_faitheval}, the ablation study on FaithEval~\cite{ming2024faitheval} demonstrates that OpAmp not only enhances robustness to noisy contexts but also reduces hallucinations as a valuable secondary benefit. 
While QLoRA achieves an average score of 47.3\%, OpAmp attains much higher averages, with the best results with $\mathcal{K}=1$ (58.3\%), indicating consistent improvements. 
Notably, $\mathcal{K}=1, 5, 10$ exhibit similar performance levels, suggesting that moderate values of $\mathcal{K}$ effectively balance denoising and model stability while mitigating hallucinations. 
However, performance declines significantly (51.6\%) when $\mathcal{K}=20$.
The degradation demonstrates an excessive attention-denoising process caused by excessive CMRR, which impairs the model's ability to avoid hallucination.
This analysis underscores that the optimal performance is achieved with moderate $\mathcal{K}$ values, highlighting the importance of balancing denoising intensity with model adaptability.

\subsection{Visualization of Attention}

\begin{figure}[!tb]
    \centering
    \includegraphics[width=0.968\linewidth]{figs/abstudy.pdf} 
    \caption{Normalized attention score. Our OpAmp model demonstrates significant attention denoise capability compared to the base model and QLoRA model.} 
    \label{fig:abstudy}
\end{figure}

\begin{figure}[!tb]
    \centering
    \includegraphics[width=0.968\linewidth]{figs/cmrr.pdf} 
    \caption{Normalized attention score with different values of $\mathcal{K}$ utilizing for OpAmp adaptation.} 
    \label{fig:cmrr}
\end{figure}

To provide deeper insights into the OpAmp mechanism, we perform some visualizations of $\vec{\bar{M}}$. 
As previously mentioned, transformer-based architectures tend to allocate disproportionate attention to irrelevant or later-positioned documents. 
In contrast, OpAmp can enhance LLMs' focus on the most relevant documents. 
We employ normalized attention scores based on Llama3.1-8B to trace the OpAmp mechanism in a noisy context to investigate this behavior.
As shown in \Cref{fig:abstudy}, Llama3.1-8B-base becomes completely lost in the noisy context, with its attention distribution across documents generally increasing sequentially from low to high.
Llama3.1-QLoRA-8B model performs relatively better, with a slight increase in attention to the golden document.
However, the limitation of a forced backward shift in attention still exists. 
In contrast, our Llama3.1-OpAmp-8B uniquely allocates the most attention to the golden document among all documents. 
This mechanism is a key factor contributing to the strong performance of our OpAmp model in noisy context scenarios.
Meanwhile, we also investigate the mechanism across different CMRR values.
As illustrated in \Cref{fig:cmrr}, only when $\mathcal{K}=10$ does the OpAmp model allocate the highest level of attention to the golden document, surpassing the other CMRR values and indirectly confirming that a moderate CMRR value is crucial for maximizing the effectiveness of the OpAmp mechanism instead of $\mathcal{K}\rightarrow \infty$ utilized in differential transformer~\cite{ye2025difftrans}.