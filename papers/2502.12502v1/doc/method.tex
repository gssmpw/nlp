\section{Methods}

\subsection{Preliminaries}

\minisection{Adapters}
\citet{houlsby2019adapter} introduced the concept of integrating adapters into pre-trained transformer-based models for PEFT. 
This approach only fine-tunes the parameters introduced by the adapters while maintaining the pre-trained weights with large parameters unchanged.
An adapter module comprises two trainable matrices, $\vec{W}_1 \in \mathbb{R}^{d_1 \times d_2}$ and $\vec{W}_2 \in \mathbb{R}^{d_2 \times d_1}$, along with a non-linear activation function  $\phi(\cdot)$. 
Here, $d_1$ represents the feature dimension of the pre-trained weights, while $d_2$ denotes the hidden dimension of the inserted adapter, typically satisfying $d_2 \ll d_1$.
Given an input feature $\vec{H} \in \mathbb{R}^{N \times d_1}$, the output of the adapter module is expressed as:
\begin{equation}
    \vec{H}' = \phi(\vec{H}\vec{W}_1)\vec{W}_2 + \vec{H}.
\label{eq:adapter}
\end{equation}

\minisection{Attention}  
The self-attention mechanism~\cite{vaswani2017attention} serves as the foundational building block for LLMs~\cite{openai2023gpt4, dubey2024llama3, yang2024qwen2_5, liu2024deepseekv3}. 
Given a query feature $\vec{Q} \in \mathbb{R}^{N \times d}$, a key feature $\vec{K} \in \mathbb{R}^{N \times d}$, and a value feature $\vec{V} \in \mathbb{R}^{N \times d}$, the attention mechanism is computed as follows:  
\begin{align}
    \mathrm{Attn}(\vec{Q}, \vec{K}, \vec{V}) &= \vec{M}\vec{V}, \nonumber\\
    \vec{M} &= \mathrm{Softmax}\left(\frac{\vec{Q}\vec{K}^{\top}}{\sqrt{d}}\right),
\label{eq:attn}
\end{align}
where $N$ represents the number of tokens and $d$ denotes the dimensionality of the query, key, and value features.

\minisection{Differential Amplifier}  
The differential amplifier~\cite{sansen2007analog} is an electronic device designed to amplify the voltage difference between its two input signals while rejecting any voltage common to both inputs. 
In an analog circuit with input voltages $V_{\text{in}}^{+}$ and $V_{\text{in}}^{-}$, the ideal output voltage $V_{\text{out}}$ is proportional to the difference between the two inputs, as expressed by:  
\begin{equation}  
    V_{\text{out}} = A_{d} (V_{\text{in}}^{+} - V_{\text{in}}^{-}),  
\label{eq:diffamp}  
\end{equation}  
where $A_{d}$ represents the differential gain. 
% This formulation aligns with the principles of the differential transformer \cite{ye2025difftrans}.

\begin{figure}[!tb]
    \centering
    \includegraphics[width=0.828\linewidth]{figs/electronic-opamp.pdf} 
    \caption{The operational amplifier with two input voltages $V_{\text{in}}^{+}$ and $V_{\text{in}}^{-}$. 
    The CMRR $\mathcal{K}$ is controlled by resistances $R_1$, $R_2$, $R_3$, $R_4$.
    } 
    \label{fig:electronic-opamp}
\end{figure}

\minisection{Operational Amplifier}
In practical applications, the desired output often deviates from the predictions of \Cref{eq:diffamp}.
For instance, when $V_{\text{in}}^{+}$ and $V_{\text{in}}^{-}$ are equal, the output voltage does not necessarily become zero.
However, according to \Cref{eq:diffamp}, the output voltage should theoretically be zero in such cases. 
To address this discrepancy, as shown in \Cref{fig:electronic-opamp}, the OpAmp~\cite{sansen2007analog} provides a more accurate and stable output expression, including an additional term accounting for common-mode effects:
\begin{align}  
    V_{\text{out}} &= V_{\text{in}}^{+}\cdot(\frac{R_4}{R_3+R_4}\cdot\frac{R_1 + R_2}{R_1}) - V_{\text{in}}^{-}\cdot\frac{R_2}{R_1} \nonumber\\
    &= A_{d}(V_{\text{in}}^{+} - V_{\text{in}}^{-}) + \frac{A_{c}}{2}(V_{\text{in}}^{+} + V_{\text{in}}^{-}),
\label{eq:opampamp}  
\end{align}  
where $A_{c}$ is the common-mode gain of the amplifier. 
The common-mode rejection ratio (CMRR) is defined as the ratio of the differential gain to the common-mode gain:
\begin{equation}
    \mathcal{K} = \frac{A_{d}}{A_{c}}.
\label{eq:cmrr}
\end{equation}
Obviously, $A_{c} \rightarrow 0$ and $\mathcal{K} \rightarrow \infty$ for an ideal differential amplifier. 

\subsection{OpAmp Adaptation}  
% In practical applications, the gains for the two input signals of an amplifier are not perfectly equal.
Inspired by the operational amplifier, we propose the OpAmp adaptation, which modifies the original attention mechanism into the OpAmp attention mechanism. 
Specifically, the operational amplifier is employed to denoise the input signals and produce a refined output in the analog circuit domain. 
Building on this concept, we design the OpAmp attention mechanism to denoise the attention matrices $\vec{M}$. 
As shown in \Cref{fig:opamp}, the original attention mechanism described in \Cref{eq:attn} is adapted using \Cref{eq:opampamp}:
\begin{equation}
    \vec{\bar{M}} = A_{d}(\vec{M}^{+} - \vec{M}^{-}) + \frac{A_{c}}{2}(\vec{M}^{+} + \vec{M}^{-}),
\label{eq:opampattn}   
\end{equation}
where $\vec{\bar{M}}$ is the denoised attention matrix via OpAmp adaptation, $\vec{M}^{+}$ and $\vec{M}^{-}$ are formulated through adapters, the detailed implementation of which will be elaborated in \Cref{sec:arch_des}.
As illustrated in \Cref{eq:opampattn}, we can adopt different $\mathcal{K}$ to adapt different scenarios using \Cref{eq:cmrr}.

Notably, the attention noise for LLMs after alignment is relatively small in noisy-context scenarios as shown in \Cref{fig:motivation}.
This suggests that attention denoising requires only a modest CMRR $\mathcal{K}$ instead of high CMRR values.
The experiment results presented in \Cref{sec:ab} further support our claim that excessively high CMRR values can lead to performance degradation. 
\begin{figure}[!tb]
    \centering
    \includegraphics[width=0.928\linewidth]{figs/opamp.pdf} 
    \caption{Overview of the OpAmp adaptation using \Cref{eq:opampattn} with adapters.} 
    \label{fig:opamp}
\end{figure}

\subsection{Architecture Design}
\label{sec:arch_des}

Given an input feature $\vec{X} \in \mathbb{R}^{N \times d}$, the query feature $\vec{Q} \in \mathbb{R}^{N \times d}$ and the key feature $\vec{K} \in \mathbb{R}^{N \times d}$ are computed as follows:
\begin{equation}
    \vec{Q} = \vec{X}\vec{W}^q, \vec{K} = \vec{X}\vec{W}^k,
\end{equation}
where $\vec{W}^q, \vec{W}^k \in \mathbb{R}^{d \times d}$ represent pre-trained weights used for linear projection.
As outlined in \Cref{eq:opampattn}, the computation of \(\vec{M}^+\) and \(\vec{M}^-\) is required to implement the OpAmp attention mechanism.
A straightforward approach involves duplicating $\vec{W}^Q$ and $\vec{W}^K$ to compute two sets of query and key features, denoted as $\vec{Q}_1, \vec{K}_1$ and $\vec{Q}_2, \vec{K}_2$
Subsequently, $\vec{M}^+$ and $\vec{M}^-$ can be calculated independently using \Cref{eq:attn} as follows:
\begin{align}
    \vec{M}^+ &= \mathrm{Softmax}\left(\frac{\vec{Q}_1\vec{K}_1^{\top}}{\sqrt{d}}\right), \\
    \vec{M}^- &= \mathrm{Softmax}\left(\frac{\vec{Q}_2\vec{K}_2^{\top}}{\sqrt{d}}\right),
\end{align}
However, this method incurs substantial computational overhead, particularly given the large parameter scale of LLMs. 

Consequently, we introduce an effective and efficient implementation of OpAmp adaptation to address this inefficiency.
Specifically, we employ adapters to avoid redundant weight computations as shown in \Cref{fig:opamp}.
For a given input $\vec{X}$, the query and key features $\vec{Q}_1, \vec{K}_1$ and $\vec{Q}_2, \vec{K}_2$ can be computed as follows:
\begin{align}
    \vec{Q}_1 = E^1_q(\vec{X}\vec{W}^q), \vec{Q}_2 = E^2_q(\vec{X}\vec{W}^q), \\
    \vec{K}_1 = E^1_k(\vec{X}\vec{W}^k), \vec{K}_2 = E^2_k(\vec{X}\vec{W}^k),
\end{align}
where $E^{i}_{j}(\vec{x})$ represents the adapters for OpAmp adaptation, defined according to \Cref{eq:adapter} as:
\begin{equation}
    E^{i}_{j}(\vec{x}) = \phi(\vec{x}\vec{W}_1)\vec{W}_2 + \vec{x},
\label{eq:opampadapter}
\end{equation}
with $i \in \{1, 2\}$ and $j \in \{q, k\}$.
This architecture ensures effective OpAmp adaptation while minimizing computational overhead.
Finally, the output of OpAmp attention can be computed as:
\begin{equation}
    \mathrm{OpAmpAttn}(\vec{Q}, \vec{K}, \vec{V}) = \vec{\bar{M}}\vec{V}.
\end{equation}
\minisection{Zero Initialization}  
At the onset of training, we employ zero initialization to promote identity mapping. 
Specifically, $\vec{W}_2$ is initialized to zero to guarantee that $E^{i}_{j}(\vec{x}) = \vec{x}$. 
Furthermore, to prevent any disruption to the original $\vec{M}$ during the initial phase of training, we set $A_c = 1$ and regulate $\mathcal{K} = \frac{A_d}{A_c}$ by adjusting the values of $A_d$. 
As a result, at the initial stage, \Cref{eq:opampattn} reduces to:
\begin{align}
    \vec{\bar{M}} &= A_{d} \cdot (\vec{M} - \vec{M}) + \frac{A_{c}}{2} \cdot (\vec{M} + \vec{M}), \nonumber \\
    &= A_{d} \cdot 0 + \frac{A_{c}}{2} \cdot 2 \vec{M} = \vec{M},
\end{align}
which aligns with the standard attention mechanism outlined in \Cref{eq:attn}. 
This strategy ensures that the model initiates training with a well-established mechanism before incorporating more sophisticated modifications.
Moreover, other modules, such as the normalization and FFN layers, are replicated directly from the original transformer block to ensure structural coherence.