\section{Literature Review}
\label{sxn:rw}
\subsection{Sentiment Analysis on Social Media}
Sentiment analysis (SA) emerged as a significant field early in the 2000s \cite{das2001yahoo, nasukawa2003sentiment}.  
SA \cite{dave2003mining, pang2008opinion} aims to determine the attitudes, opinions, or emotions expressed in text on specific topics or entities \cite{liu2022sentiment} and has become an increasingly popular research area. Due to higher user-generated content available on social media, understanding  sentiment in text cannot be overstated \cite{naseem2019dice}. 

Diverse strategies to accurately interpret and classify user sentiments have been employed. For example, lexicon-based approaches, like SENTIWORDNET \cite{baccianella2010sentiwordnet} and AFINN \cite{nielsen2011new}, used predefined word lists to classify text sentiment. While effective in some applications, these methods often struggled with context and nuance. Rule-based systems \cite{suttles2013distant} further enhanced this method by applying contextual rules to detect sentiment nuances, including handling negations \cite{taboada2011lexicon}.

Advancements in Machine learning (ML) \cite{pang2002thumbs}, such as supervised techniques trained on large amounts of labeled sentiment datasets, offer another powerful avenue for SA. 
Hence, the exploration of semi-supervised methods in SA could leverage unlabelled data to address the challenge of data annotation and labeling \cite{vo2015target, hwang2021semi}. Deep learning approaches such as Convolutional Neural Networks (CNN) \cite{chen2015convolutional} have significantly advanced SA capabilities. However, SA on social media poses unique challenges compared to more traditional domains due to the informal and conversational nature of the text \cite{medhat2014sentiment, naseem2019dice}. 

\subsection{Code-Switching on Low-resource}

Code-switching, the practice of alternating between two or more languages or dialects within a conversation, is particularly prevalent in multilingual communities and has become increasingly visible on social media platforms \cite{poplack2000toward,scotton1993social, danet2007multilingual}. It presents unique challenges and opportunities for NLP \cite{barman2014code}. Most NLP research traditionally focuses on high-resource languages like English, leaving low-resource languages underrepresented \cite{strassel2016lorelei, adelani2021masakhaner}. This gap is more pronounced in African and code-switched languages due to linguistic variability  \cite{adelani2021masakhaner}. Therefore, high-resource language techniques may underperform on low-resource language data \cite{lewis2014ethnologue}. The study in \cite{lee2015emotion} emphasizes the importance of analyzing emotions in code-switching data.  
The use of Generative Pre-trained Transformers (GPT) to generate synthetic code-switched data has been proposed to address data scarcity \cite{terblanche2024prompting}. A recent survey \cite{winata2022decades} revealed that until October 2022, only a few papers from the ACL Anthology and ISCA Proceedings focused on code-switching research in African languages. 
% Maria - not clear,  niesler2018first mentions the first dataset
For South African languages \cite{niesler2018first, niesler2008accent} 
% the latter has no dataset available. 
the first dataset was presented in 2018.
Even though Swahili-English code-switching has been studied in a few papers \cite{piergallini2016word, otundo2022intonation}, no datasets are available.

\subsection{ Transformer-based Pretrained Models}
Transformer-based architectures \cite{vaswani2017attention}, such as BERT  \cite{devlin2018bert}, have gained popularity owing to their effectiveness in learning general representations using large unlabelled datasets \cite{matthew2018peters} that can further be fine-tuned for downstream tasks \cite{gururangan2020don, bhattacharjee2020bert}. Hence, it has become the foundation for many NLP tasks \cite{bhattacharjee2020bert}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\columnwidth]{latex/Images/modeldiagram.png}
    \caption{ \textbf{Methodology:} \textit{Overview of the RideKE sentiment and emotion analysis framework. Unlabeled and labeled datasets are preprocessed and used to train supervised and semi-supervised models for sentiment and emotion prediction. The semi-supervised learning loop generates pseudo labels for evaluation of performance.}}
    \label{fig:rideke}
\end{figure}

Pretrained language models are trained on large, diverse datasets \cite{raffel2020exploring}. For example, RoBERTa \cite{liu2019roberta} was pretrained on over 160GB of uncompressed text, from BOOKCORPUS \cite{zhu2015aligning} and CommonCrawl English dataset \cite{commoncrawlCommonCrawl}. These models learn representations that perform well across various tasks, handling datasets of different sizes from diverse sources while remaining easily understandable \cite{wang2019superglue}. Examples of a few applications in low-resource include improving speech recognition accuracy (ASR) \cite{olatunji2023afrispeech}, machine translation (MT)  \cite{wang2024afrimte} and SA \cite{muhammad2023afrisenti}.