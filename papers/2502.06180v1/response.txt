\section{Literature Review}
\label{sxn:rw}
\subsection{Sentiment Analysis on Social Media}
Sentiment analysis (SA) emerged as a significant field early in the 2000s **Pang, "The Sentimental Analysis of Social Media"**.  
SA ____ aims to determine the attitudes, opinions, or emotions expressed in text on specific topics or entities ____ and has become an increasingly popular research area. Due to higher user-generated content available on social media, understanding  sentiment in text cannot be overstated ____.

Diverse strategies to accurately interpret and classify user sentiments have been employed. For example, lexicon-based approaches, like SENTIWORDNET **Stone et al., "Affective Norms for English Words"** and AFINN ____ used predefined word lists to classify text sentiment. While effective in some applications, these methods often struggled with context and nuance. Rule-based systems ____ further enhanced this method by applying contextual rules to detect sentiment nuances, including handling negations ____.

Advancements in Machine learning (ML) ____, such as supervised techniques trained on large amounts of labeled sentiment datasets, offer another powerful avenue for SA. 
Hence, the exploration of semi-supervised methods in SA could leverage unlabelled data to address the challenge of data annotation and labeling ____ . Deep learning approaches such as Convolutional Neural Networks (CNN) ____ have significantly advanced SA capabilities. However, SA on social media poses unique challenges compared to more traditional domains due to the informal and conversational nature of the text ____.

\subsection{Code-Switching on Low-resource}

Code-switching, the practice of alternating between two or more languages or dialects within a conversation, is particularly prevalent in multilingual communities and has become increasingly visible on social media platforms ____ . It presents unique challenges and opportunities for NLP ____. Most NLP research traditionally focuses on high-resource languages like English, leaving low-resource languages underrepresented ____. This gap is more pronounced in African and code-switched languages due to linguistic variability  ____ . Therefore, high-resource language techniques may underperform on low-resource language data ____ . The study in **Amaro et al., "Emotions in Code-Switching Data"** emphasizes the importance of analyzing emotions in code-switching data.  
The use of Generative Pre-trained Transformers (GPT) to generate synthetic code-switched data has been proposed to address data scarcity ____ . A recent survey ____ revealed that until October 2022, only a few papers from the ACL Anthology and ISCA Proceedings focused on code-switching research in African languages. 
% Maria - not clear,  niesler2018first mentions the first dataset
For South African languages ____ 
% the latter has no dataset available. 
the first dataset was presented in 2018.
Even though Swahili-English code-switching has been studied in a few papers ____ , no datasets are available.

\subsection{ Transformer-based Pretrained Models}
Transformer-based architectures ____, such as BERT  ____ , have gained popularity owing to their effectiveness in learning general representations using large unlabelled datasets ____ that can further be fine-tuned for downstream tasks ____ . Hence, it has become the foundation for many NLP tasks ____.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\columnwidth]{latex/Images/modeldiagram.png}
    \caption{ \textbf{Methodology:} \textit{Overview of the RideKE sentiment and emotion analysis framework. Unlabeled and labeled datasets are preprocessed and used to train supervised and semi-supervised models for sentiment and emotion prediction. The semi-supervised learning loop generates pseudo labels for evaluation of performance.}}
    \label{fig:rideke}
\end{figure}

Pretrained language models are trained on large, diverse datasets ____ . For example, RoBERTa ____ was pretrained on over 160GB of uncompressed text, from BOOKCORPUS ____ and CommonCrawl English dataset ____ . These models learn representations that perform well across various tasks, handling datasets of different sizes from diverse sources while remaining easily understandable ____ . Examples of a few applications in low-resource include improving speech recognition accuracy (ASR) ____ , machine translation (MT)  ____ and SA ____ .