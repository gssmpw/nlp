% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage{float}
\usepackage[table]{xcolor} % Add color grey color to the table
\usepackage{booktabs} % create boarder lines on Table2 
\usepackage{adjustbox}
\usepackage{tabularx}
\usepackage{subcaption}
\usepackage{array}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{ragged2e}
\newcolumntype{Y}{>{\RaggedRight\arraybackslash}X}
\usepackage{adjustbox} % Add this package for adjusting box sizes
\usepackage{appendix}



% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{array}
\usepackage{amsmath} 

\renewcommand{\textfraction}{0}          %% default .2
\renewcommand{\dbltopfraction}{1}        %% default .7
\renewcommand{\dblfloatpagefraction}{1} %% default .5


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{RideKE: Leveraging Low-Resource, User-Generated Twitter Content for Sentiment and Emotion Detection in Kenya\`{n} Code-Switched Dataset}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Naome A. Etori\and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Naome A. Etori\ and Maria L. Gini \\ 
  Department of Computer Science and Engineering  \\
  University of Minnesota -Twin Cities\\
  %Affiliation / Address line 3 \\
  \texttt\{etori001, gini\} @umn.edu} 



\begin{document}
\maketitle
% Maria -- the abstract is a bit too long.  You do not need to include all the numerical results in it.
\begin{abstract}
Social media has become a crucial open-access platform for individuals to express opinions and share experiences. %These platforms enable instant communication and feedback through user-generated content. 
However, leveraging low-resource language data from Twitter is challenging due to scarce, poor-quality content and the major variations in language use, such as  slang and code-switching.  Identifying tweets in these languages can be difficult as Twitter primarily supports high-resource languages. %; low-resource languages such as Sheng often lack strong linguistic and contextual support. 
We analyze Kenyan code-switched data and evaluate four state-of-the-art (SOTA) transformer-based pretrained models for sentiment and emotion classification, using supervised and semi-supervised methods. We detail the methodology behind data collection and annotation, and the challenges encountered during the data curation phase. Our results show that XLM-R outperforms other models; for sentiment analysis, XLM-R supervised model achieves the highest accuracy (69.2\%) and F1 score (66.1\%), XLM-R semi-supervised (67.2\% accuracy, 64.1\% F1 score). In emotion analysis, DistilBERT supervised leads in accuracy (59.8\%) and F1 score (31\%), mBERT semi-supervised (accuracy (59\% and F1 score 26.5\%). AfriBERTa models show the lowest accuracy and F1 scores. All models tend to predict neutral sentiment, with Afri-BERT showing the highest bias and unique sensitivity to empathy emotion. \footnote{\url{https://github.com/NEtori21/Ride_hailing_project}}

\end{abstract}

\section{Introduction}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{latex/Images/ken.png} 
    \caption {\textbf{Geographical representation of RideKE:} 
    \textit{diverse local accents collected in tweets, such as Rift Valley (e.g., Eldoret, Nakuru), Central (e.g., Nyeri, Kiambu), Nairobi (e.g., Kasarani, Kileleshwa), Western (e.g., Kakamega, Bungoma), Nyanza (e.g., Kisumu, Kisii), Eastern (e.g., Machakos, Embu) Coast (e.g., Mombasa, Malindi), and North-Eastern (e.g., Garissa, Mandera).}}
    \label{fig:ken}
\end{figure}

\begin{table*}[t!]
  \small
  \centering
  \begin{tabular}{p{1.4\columnwidth}cc}
    \hline
    \textbf{Tweets} & \textbf{Sentiment} & \textbf{Emotion} \\
    \hline
    \rowcolor{gray!25}
    Uber kenya did your App stop accepting cards for package deliveries? I have had two riders this morning cancel picking a package because they want me to pay cash. & Negative & Frustration \\
    Thank you for the love and support and for the feedback as well. Tell all your friends to ride a littleCab. Buy Kenyan, build Kenya. & Positive & Love \\
    \rowcolor{gray!25}
    Uber drivers are not employees of Uber Kenya Uber is only an app. The link between you as a rider and the driver. But yes they should look after them because the drivers keep them afloat. & Neutral & Neutral \\
    A ride will be canceled for one reason or another and both parties should have the liberty to. Sometimes clients will cancel due to the proximity of the driver and other times because the driver is unreachable. & Neutral & Neutral \\
    \rowcolor{gray!25}
    Hope everyone making the most of this awesome Uber kenya Jan offer! Spread the word! Loving it. \#Uber kenya & Positive & Happy \\
    Giving drivers right to refer the rider to another driver then that is totally not a good idea. Some drivers are connecting while he like really far from you, he wastes time, then after more than 5 mins refers another driver & Neutral & Happy \\
    \rowcolor{gray!25}
    Greater experience for Uber riders with new product & Positive & Happy \\
    I am reporting your driver for taking payment twice. I had ordered an Uber for a friend with payment with a card and then he tells the passenger to pay via Mpesa. & Negative & Frustration \\
    \rowcolor{gray!25}
    I also stopped using Uber kenya after I was charged for cancelling a trip as per the drivers request. Little cab iko tu sawa. & Negative & Frustration \\
    Crooked policies. Uber kenya. I think you need to sort out your service. & Negative & Angry \\
    \rowcolor{gray!25}
    Honestly, Am disappointed with them. kucancel trips ndio wanajua lately. & Negative & Frustration \\
    \hline
  \end{tabular}
  \caption{Sample Tweets with Sentiment and Emotion Labels.}
  \label{tab:tweets_sentiment_emotion}
\end{table*}

Kenya, reflecting Africa's extensive multilingual diversity, offers a unique insight into the continent's rich linguistic heritage, standing as a focal point of language contact, expansion, and diversity. It is home to many languages that bridge its vibrant storytelling, poetry, song, and literature and exemplifies Africa's linguistic wealth, albeit on a more localized scale. With over 40 languages grouped into Bantu, Nilotic, and Cushitic, Kenya's linguistic landscape is diverse and dynamic \cite{dwivedi2014linguistic,carter2007teaching,banks2002talk}.

Central to linguistic diversity is the co-official language status of English and Kiswahili, with the latter spoken by the majority and enjoying near-equal prominence with English. However, the linguistic equilibrium faces challenges from Sheng, a language that blends English, Kiswahili, and words from other ethnic languages that initially were used in Nairobi Eastlands slums. Sheng emerged as a sociolect among urban youth in the city's working-class neighborhoods and has since spread across various social and age groups. Hence, it is an integral part of Kenyan culture, influencing the traditional dominance of English and Kiswahili \cite{barasa2016spoken, momanyi2009effects,mazrui1995slang}. 

In recent years, language diversity has also been mirrored in the urban transportation sector, primarily due to the growth of Ride-Hailing Services (RHS) such as Uber, Bolt, and Little Cab. These services have rapidly transformed from urban novelties to essential components of daily mobility for many Kenyans, connecting remote areas with vibrant urban cities. 
However, with the entry of global giants like Uber in 2015, followed by Bolt and the local contender Little Cab, this transformation is not just physical; it extends into digital and social media platforms such as Twitter.

Since many  languages are spoken across Kenya, each population has its own dialect. Hence, code-switching is common in these new forms of communication, where speakers alternate between two or more languages in one conversation \cite{kanana2018functions,santy2021bertologicomix,angel2020nlp,thara2018code}. Analyzing sentiment and emotions in code-switched language context is critical in the broad natural language processing  (NLP) field, for example, creating systems that can predict emotional states from text to speech which can be applied in various use cases, such as measuring consumer satisfaction \cite{ren2012linguistic}, natural disasters \cite{vo2013twitter}, marketing strategy \cite{zamani2016eye}, e-learning \cite{ortigosa2014sentiment}, e-commerce\cite{jabbar2019real} and psychological states \cite{aytuug2018sentiment}. However, despite this linguistic richness, African languages remain significantly underrepresented in NLP research \cite{muhammad2023afrisenti}. Although NLP research has made extensive progress and demonstrated broad utility over the past two decades, the focus on African languages has been limited. This disparity is often attributed to the scarcity of high-quality, annotated datasets for these languages.

Recently, researchers \cite{muhammad2023afrisenti}\footnote{\url{https://github.com/afrisenti-semeval/afrisent-semeval-2023}} have focused on addressing this challenge by introducing a comprehensive benchmark with over 110,000 tweets across 14 African languages, Swahili among them, and introduced the first Africentric SemEval Shared task \cite{muhammad2023semeval}. Various studies have evaluated the performance of state-of-the-art (SOTA) transformer models on African languages, highlighting unique challenges and opportunities \cite{aryal2023sentiment}.

\begin{table*}[ht!]
\centering
\footnotesize
\begin{tabularx}{\textwidth}{Y Y}
\toprule
\textbf{Code-switched Reference} & \textbf{English Translation} \\
\midrule
I recently interacted with one Uber driver who told me that \textbf{\textcolor{blue}{huko ni mbali, lazima uongeze pesa}}. Different from the estimate on the app. He almost dropped me midway because I argued that it wasn't fair. \textbf{\textcolor{blue}{Hawa madere ni wazimu walai.}} & I recently interacted with one Uber driver who told me that \underline{\textcolor{red}{the place is far, you have to add money}}. Different from the estimate on the app. He almost dropped me midway because I argued that it wasn't fair. \underline{\textcolor{red}{These drivers are crazy, really.}} \\
\midrule
In Mombasa, they ask you how much the App has displayed as the cost, then tell you it's too low, madam \textbf{\textcolor{blue}{unaona utaongeza ngapi}}, \textbf{\textcolor{blue}{hiyo pesa ni kidogo}} & In Mombasa they ask you how much the app has displayed as the cost then tell you it's too low, madam \underline{\textcolor{red}{how much extra?}}, \underline{\textcolor{red}{That's little money}} \\
\bottomrule
\end{tabularx}
\caption{Example of code-switched sentences in Tweets}
\label{tab:codeswitched}
\end{table*}

However, research on social media NLP analysis for RHS datasets mainly targets high-resource languages. NLP for low-resource languages is constrained by factors like NLP research's geographical and language diversity \cite{joshi2020state}. Using pre-trained transformer models, we introduce RideKE, a sentiment and emotion analysis dataset for  African-accented English code switched with Swahili and Sheng. 

Our dataset contains over 29,000 tweets, each sentiment classified as either positive, negative, or neutral, and emotions classified as frustration, happy, angry, sad, empathy, fear, love, and surprise. The dataset represents one location, Kenya, as shown in Table~\ref{tab:tweets_sentiment_emotion}. Our goal is to advance research in low-resource languages.

The experiments in this paper are designed to allow us to answer the following specific questions:

\begin{enumerate}
    \item How do pretrained language models enhance the detection and representation of Kenyan low-resource languages and accents in modern NLP tools?
    \item How does the performance of sentiment and emotion detection varies across different pretrained transformer-based models?
    % \item What are the prevalent themes or topics in RideKE conversation?
    \item How effective are different transformer-based models in performing sentiment and emotion detection on the low-resource (RideKE) dataset using semi-supervised learning?
\end{enumerate}

Our paper makes the following contributions as we address these questions:

\begin{itemize}\setlength{\itemsep}{1pt} \setlength{\parskip}{0pt \setlength{\parsep}{0pt}}
    \item  We use semi-supervised learning to classify sentiments and emotions. We compare four SOTA transformer-based models and provide a detailed model performance analysis.
    \item We contribute a partially curated human-annotated labeled public dataset with over 29,000 tweets from the RHS domain. This is Kenya's first-ever code-switched sentiment and emotion dataset in the RHS domain. It contributes resources to low-resource areas, which can be used for other analyses.
\end{itemize}

\section{Literature Review}
\label{sxn:rw}
\subsection{Sentiment Analysis on Social Media}
Sentiment analysis (SA) emerged as a significant field early in the 2000s \cite{das2001yahoo, nasukawa2003sentiment}.  
SA \cite{dave2003mining, pang2008opinion} aims to determine the attitudes, opinions, or emotions expressed in text on specific topics or entities \cite{liu2022sentiment} and has become an increasingly popular research area. Due to higher user-generated content available on social media, understanding  sentiment in text cannot be overstated \cite{naseem2019dice}. 

Diverse strategies to accurately interpret and classify user sentiments have been employed. For example, lexicon-based approaches, like SENTIWORDNET \cite{baccianella2010sentiwordnet} and AFINN \cite{nielsen2011new}, used predefined word lists to classify text sentiment. While effective in some applications, these methods often struggled with context and nuance. Rule-based systems \cite{suttles2013distant} further enhanced this method by applying contextual rules to detect sentiment nuances, including handling negations \cite{taboada2011lexicon}.

Advancements in Machine learning (ML) \cite{pang2002thumbs}, such as supervised techniques trained on large amounts of labeled sentiment datasets, offer another powerful avenue for SA. 
Hence, the exploration of semi-supervised methods in SA could leverage unlabelled data to address the challenge of data annotation and labeling \cite{vo2015target, hwang2021semi}. Deep learning approaches such as Convolutional Neural Networks (CNN) \cite{chen2015convolutional} have significantly advanced SA capabilities. However, SA on social media poses unique challenges compared to more traditional domains due to the informal and conversational nature of the text \cite{medhat2014sentiment, naseem2019dice}. 

\subsection{Code-Switching on Low-resource}

Code-switching, the practice of alternating between two or more languages or dialects within a conversation, is particularly prevalent in multilingual communities and has become increasingly visible on social media platforms \cite{poplack2000toward,scotton1993social, danet2007multilingual}. It presents unique challenges and opportunities for NLP \cite{barman2014code}. Most NLP research traditionally focuses on high-resource languages like English, leaving low-resource languages underrepresented \cite{strassel2016lorelei, adelani2021masakhaner}. This gap is more pronounced in African and code-switched languages due to linguistic variability  \cite{adelani2021masakhaner}. Therefore, high-resource language techniques may underperform on low-resource language data \cite{lewis2014ethnologue}. The study in \cite{lee2015emotion} emphasizes the importance of analyzing emotions in code-switching data.  
The use of Generative Pre-trained Transformers (GPT) to generate synthetic code-switched data has been proposed to address data scarcity \cite{terblanche2024prompting}. A recent survey \cite{winata2022decades} revealed that until October 2022, only a few papers from the ACL Anthology and ISCA Proceedings focused on code-switching research in African languages. 
% Maria - not clear,  niesler2018first mentions the first dataset
For South African languages \cite{niesler2018first, niesler2008accent} 
% the latter has no dataset available. 
the first dataset was presented in 2018.
Even though Swahili-English code-switching has been studied in a few papers \cite{piergallini2016word, otundo2022intonation}, no datasets are available.

\subsection{ Transformer-based Pretrained Models}
Transformer-based architectures \cite{vaswani2017attention}, such as BERT  \cite{devlin2018bert}, have gained popularity owing to their effectiveness in learning general representations using large unlabelled datasets \cite{matthew2018peters} that can further be fine-tuned for downstream tasks \cite{gururangan2020don, bhattacharjee2020bert}. Hence, it has become the foundation for many NLP tasks \cite{bhattacharjee2020bert}.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\columnwidth]{latex/Images/modeldiagram.png}
    \caption{ \textbf{Methodology:} \textit{Overview of the RideKE sentiment and emotion analysis framework. Unlabeled and labeled datasets are preprocessed and used to train supervised and semi-supervised models for sentiment and emotion prediction. The semi-supervised learning loop generates pseudo labels for evaluation of performance.}}
    \label{fig:rideke}
\end{figure}

Pretrained language models are trained on large, diverse datasets \cite{raffel2020exploring}. For example, RoBERTa \cite{liu2019roberta} was pretrained on over 160GB of uncompressed text, from BOOKCORPUS \cite{zhu2015aligning} and CommonCrawl English dataset \cite{commoncrawlCommonCrawl}. These models learn representations that perform well across various tasks, handling datasets of different sizes from diverse sources while remaining easily understandable \cite{wang2019superglue}. Examples of a few applications in low-resource include improving speech recognition accuracy (ASR) \cite{olatunji2023afrispeech}, machine translation (MT)  \cite{wang2024afrimte} and SA \cite{muhammad2023afrisenti}. 

\section{Methods and Datasets}
\label{sxn:meth}
\subsection{Overview of RideKE Dataset}

RideKE dataset. as shown in Table~\ref{tab:tweets_sentiment_emotion} and ~\ref{tab:codeswitched}, includes a blend of Kenyan-accented English, approx. (70\%), with a minority mix of Swahili and Sheng (30\%). 
The dataset includes a total of 29,623 
entries across 12 distinct columns. See Table~\ref{tab:tweet_data_analysis} 
% Maria -- added
in the Appendix.

\subsection{Data Collection}
We used a systematic scraping process using the snscrape python library \footnote{\url{https://pypi.org/project/snscrape/l}} which allows for querying and retrieving tweets based on specified criteria. We targeted three keyword search terms—\#UBER-Kenya, \#BOLT-kenya, and \#LITTLECAB, from January 2017 to April 2023, capturing not only the tweet texts but also other essential metadata such as user engagement metrics (likes, retweets, replies), user account details (followers, following, tweet counts), and relational markers (hashtags, user mentions). Initially, the data was in a dictionary format but it was later converted to DataFrame using pandas and preserved in a CSV format to ensure reproducibility.

\subsubsection{Geo-based data collection}
The tweet's location metadata was crucial in determining the regional focus of our study. We referenced Kenya's location as shown in Table~\ref{table:tweet_counts_by_location}. To ensure uniformity, we used a simple yet effective keyword filtering normalization technique to address location inconsistencies as shown by the diverse representations of Nairobi in the dataset shown in Table~\ref{table:tweet_counts_by_location}.
To isolate the relevant tweets, we applied a filter on the \texttt{user\_location} field to include only locations mentioning \texttt{Kenya} and discard entries with missing data and all those with no location. 
We assessed the frequency distribution of different locations using value count function.

\begin{table}[ht]
\centering
\small 
\begin{tabular}{lr}
\toprule
\textbf{Location} & \textbf{Tweet Count} \\
\midrule
Kenya                      & 18974 \\
Nairobi, Kenya             & 11960 \\
\textit{Not specified}     & 10868 \\
Nairobi                    & 4776 \\
Nairobi, Kenya             & 620 \\
nairobery                  & 1 \\
Africa, Nairobi Kenya      & 1 \\
Mt. Meru                   & 1 \\
3rd Parklands              & 1 \\
New Jersey                 & 1 \\
\bottomrule
\end{tabular}
\caption { \textbf{Tweet Counts by location:}
\textit{We only included locations mentioning Kenya}
\label{table:tweet_counts_by_location}}
\end{table}


\subsection{Language Detection}
We used \texttt{langdetect} \footnote{\url{https://pypi.org/project/langdetect/l}} Python library to detect languages within text. It revealed diverse languages, \texttt{English}  being the most prevalent, then \texttt{Indonesian}, \texttt{Swahili} and others as shown in Table~\ref{tab:my_label}. For the Sheng language, native speakers manually detected the language. We only kept English (code-switched) for our analysis.

\subsection{Data Preprocessing}

Tweets often feature slang, abbreviations, and non-alphanumeric characters such as hashtags and emojis, contributing to the data's unstructured nature \cite{adebara2022towards}.  We implemented a refined text preprocessing pipeline to enhance data consistency and accurate analysis. The pipeline standardizes data by converting text to strings, trimming whitespace, lowering case, and expanding contractions to preserve semantic integrity. The text is then normalized by reducing repeated characters, removing punctuation, newlines, and tabs, and then tokenizing. 

\subsection{Data Annotation}
Inspired by \cite{raffel2020exploring} established guidelines, we created a set of annotation guidelines for emotion annotations to ensure a standardized and high-quality approach in our labeling efforts, as shown in Table~\ref{tab:emotion_guidelines}. We added a 'frustration' label and used 'happy' instead of 'joy.' For the sentiment annotation, we adhered to the established annotation framework detailed by \cite{mohammad2016practical}. However, human annotation is time-consuming and costly. We employed two Kenyan volunteer annotators fluent in English, Swahili, and Sheng. One holds a bachelor's degree in political science and the other in computer science. They received a small token of appreciation for their efforts. We ensured the annotator's comprehension of the task. Two annotators labeled the same dataset entries to enhance quality. Each labeled 1,554 tweets with sentiment labels (positive, negative, neutral) and emotion labels 
% Maria  happiness instead of happy?
(sadness, happy, love, anger, fear, surprise, frustration, and neutral).

\subsubsection{Annotation Quality Control}
 We used Cohen's Kappa \cite{artstein2017inter}\footnote{\url{https://github.com/zyocum/cohens_kappa}} as our primary metric for assessing the level of inter-annotator agreement between the two annotators. It is perfect for categorical items, such as sentiment and emotion labels. Cohen’s Kappa provides a means to compute an inter-rater agreement score that accounts for the probability of random agreement:

\begin{equation}
\kappa = \frac{P_o - P_e}{1 - P_e}
\end{equation}
where $P_o$ is the observed agreement, and $P_e$ is the expected agreement by chance. 

To assign the final sentiment and emotion label to each tweet, we employed a majority voting method \cite{davani2022dealing} to determine the final label of the tweet \cite{mohammad2022ethics}. Instances of complete disagreement among annotators were resolved by involving a lead annotator and applying a majority rule rather than omitting them from the dataset. We found a Cohen's Kappa coefficient of 0.60 for sentiment classification tasks. Cohen's Kappa score for the emotion annotations is approximately 0.67, which indicates a substantial level of agreement beyond chance and suggests a good degree of consistency in their annotations.

\subsubsection{Data Splits}
The dataset was split into three sets (A, B, and C) as shown in the dataset division Table~\ref{table:dataset_division}. We used ChatGPT \cite{brown2020language} for automatic labeling to augment the training dataset and increase training labels since we had only two human annotators. Set A provided Ground truth labels for initial supervised training. Set B is the test dataset that is manually annotated by human annotators. Set C represented the unlabelled dataset Used in a semi-supervised training loop, with empty rows and duplicates removed, labels standardized and encoded.

\begin{table}[ht]
\centering
\small
\begin{tabular}{c l l}
\toprule
\textbf{Set} & \textbf{Description} & \textbf{Details} \\ \midrule
Set A & 553 human, 636 ChatGPT & Supervised Train \\ 
Set B & 2,000 human & Testing \\ 
Set C & 27,090 unlabelled & Semi-supervised \\ 
\bottomrule
\end{tabular}
\caption{Dataset Division}
\label{table:dataset_division}
\end{table}

\subsection{Semi-supervised Learning Phase}
Semi-supervised learning (SSL) offers a framework for utilizing large amounts of unlabelled data when obtaining labels is expensive \cite{chapelle2006introduction, learning2006semi} as applied to our case. Research shows SSL improves performance on different machine learning tasks such as text classification and machine translation \cite{najafi2019robustness}. SSL connects supervised and unsupervised learning by utilizing a small fraction of labelled data alongside a larger pool of unlabeled data to improve learning accuracy. SSL has been widely studied to show effectiveness for a wide range of low-resource applications, such as in text-to-speech synthesis (TTS) \cite{saeki2023virtuoso}, speech recognition \cite{du2023semi,thomas2013deep}, machine translation\cite{pham2023semi,singh2022low}, POS-Taggers \cite{garrette2013real}, and sentiment classification \cite{gupta2018semi}. Our work extends the application of SSL to sentiment and emotion classification tasks. We seek to mitigate this limitation by leveraging labeled and unlabeled 
% Maria -- train pretrained models. Why? 
data to train pretrained models. We used accuracy, precision, recall, and F1 scores to evaluate the models' performance.


\begin{table*}[!ht]
    \centering
    \small
    \resizebox{1.0\textwidth}{!}{%
    \begin{tabular}{l|cccc|cccc}
    \toprule
     & \multicolumn{4}{c|}{Sentiment} & \multicolumn{4}{c}{Emotions} \\
    Model & Accuracy & Precision & Recall & F1-Score & Accuracy & Precision & Recall & F1-Score \\
    \midrule
    DistilBERT supervised & 0.578 & 0.598 & 0.629 & 0.546 & 0.598 & 0.334 & 0.315 & 0.310 \\
    DistilBERT semi-supervised & 0.553 & 0.585 & 0.598 & 0.516 & 0.544 & 0.264 & 0.266 & 0.252 \\
    mBERT supervised & 0.638 & 0.621 & 0.663 & 0.596 & 0.592 & 0.253 & 0.298 & 0.265 \\
    mBERT semi-supervised & 0.635 & 0.622 & 0.661 & 0.598 & 0.594 & 0.297 & 0.317 & 0.297 \\
    XLM-R supervised & 0.692 & 0.665 & 0.723 & 0.661 & 0.658 & 0.343 & 0.267 & 0.258 \\
    XLM-R semi-supervised & 0.672 & 0.644 & 0.702 & 0.641 & 0.620 & 0.334 & 0.248 & 0.230 \\
    AfriBERTa large supervised & 0.398 & 0.500 & 0.479 & 0.358 & 0.604 & 0.163 & 0.191 & 0.157 \\
    AfriBERTa semi-supervised & 0.413 & 0.534 & 0.491 & 0.366 & 0.556 & 0.145 & 0.177 & 0.142 \\
    \bottomrule
    \end{tabular}}
    \caption{\textbf{Model Performance Evaluation on Sentiment and Emotion Analysis Tasks.} \textit{Performance evaluation of supervised and semi-supervised training for sentiment and emotion analysis across models. Results represent averages over multiple runs.}}
    \label{tab:model_performance_sentiment_emotion}
\end{table*}

\begin{table*}[!ht]
    \centering
    \small
     \resizebox{1.0\textwidth}{!}{%
    \begin{tabular}{l|ccc|ccc|ccc}
    \toprule
     & \multicolumn{3}{c|}{Negative} & \multicolumn{3}{c|}{Neutral} & \multicolumn{3}{c}{Positive} \\
    Model & Precision & Recall & F1 & Precision & Recall & F1 & Precision & Recall & F1 \\
    \midrule
    DistilBERT supervised & 0.920 & 0.385 & 0.543 & 0.284 & 0.635 & 0.392 & - & - & - \\
    DistilBERT semi-supervised & 0.901 & 0.325 & 0.478 & 0.268 & 0.604 & 0.371 & - & - & - \\
    mBERT supervised & 0.906 & 0.467 & 0.616 & 0.330 & 0.587 & 0.423 & - & - & - \\
    mBERT semi-supervised & 0.873 & 0.443 & 0.588 & 0.363 & 0.628 & 0.460 & - & - & - \\
    XLM-R supervised & 0.921 & 0.563 & 0.699 & 0.417 & 0.714 & 0.526 & - & - & - \\
    XLM-R semi-supervised & 0.850 & 0.524 & 0.648 & 0.392 & 0.712 & 0.506 & 0.691 & 0.871 & 0.771 \\
    AfriBERTa large supervised & 0.794 & 0.100 & 0.178 & 0.144 & 0.492 & 0.223 & - & - & - \\
    AfriBERTa semi-supervised & 0.874 & 0.096 & 0.174 & 0.171 & 0.560 & 0.261 & 0.558 & 0.817 & 0.663 \\
    \bottomrule
    \end{tabular}}
    \caption{\textbf{Model Performance Evaluation on Sentiment classification Tasks Labels.} \textit{Performance evaluation for Negative, Neutral, and Positive sentiments across various models. A dash (-) indicates missing values, i.e., the models did not predict all positive sentiment instances. The results represent averages over multiple runs.}}
    \label{tab:model_performance_sentiment}
\end{table*}


 \section{Experiments}
\label{sxn:exp}
\subsection{Models and Architecture}
We evaluate four transformer-based models in our experiments: \textbf{DistilBERT} \cite{sanh2019distilbert}, a smaller and faster version of BERT;  \textbf{mBERT} \cite{devlin2018bert}, a multilingual version of BERT trained on 104 languages; \textbf{XLM-RoBERTa} \cite{conneau2019unsupervised}, a multilingual model trained on 100 languages with improved performance; and \textbf{AfriBERTa large} \cite{ogueji2021small}, a model specifically designed for African languages to address the unique linguistic challenges in this region. Each model was trained on supervised and semi-supervised learning on sentiment and emotion classification tasks. The initial supervised training and subsequent semi-supervised fine-tuning were conducted separately for each model.
 

\subsection{Experimental Setup }
\subsubsection{Supervised Learning Phase}
In supervised training, we utilized the human-annotated, well-curated labeled dataset. We used batches ranging from 16 to 64 depending on the model sizes, optimizing for computational efficiency. A combined categorical cross-entropy loss shown in Figure \ref{fig:TrainingLosses} function, with equal weighting for sentiment and emotion tasks, guided the model toward effective multitasking. We applied a dropout rate of 0.1 for each model to prevent overfitting and enhance generalization. 
We employed the Adam optimizer, with a learning rate $1e-5$ through 10 epochs of training and monitoring. 
Initially, the four transformer-based models were fine-tuned on a dataset with 1,189 labeled tweets. We then evaluated the model.

\subsubsection{Semi-supervised Learning Phase}

Our goal in using SSL is to leverage the vast, unlabeled datasets to mitigate the high cost of human annotations. Following an initial supervised learning phase, each transformer-based model underwent a semi-supervised training loop. In this loop, the models dynamically labeled the unlabeled dataset based on their predictions, generating a pseudo-labeled dataset. We employed a dynamic threshold, set at the 75th percentile of the models' probability predictions across all classes for each batch, to ensure only high-confidence predictions were used for labeling. Samples with predictions below this threshold were excluded to minimize the inclusion of erroneous labels in the training data.

We extended the semi-supervised training loop over 4 epochs, a duration we empirically selected to refine the models' generalization capabilities without causing performance degradation due to overtraining, as indicated by either worsening or plateauing loss. We carefully chose the hyperparameters to ensure optimal training dynamics and model performance.

We set the learning rate at 1e-5 and dynamically adjusted it using a learning rate scheduler during training to optimize generalization and reduce overfitting. The batch size varied between 16 and 64, depending on the specific transformer model, to ensure computational efficiency. We used a combined loss function shown in Figure \ref{fig:TrainingLosses} for sentiment and emotion analysis and applied a dropout rate of 0.1 to prevent overfitting. We employed the Adam optimizer with a learning rate of 1e-5 and no weight decay. 

\begin{figure}[ht!]
\centering
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{latex/Images/supervised_training_loss.png}
  \caption{Supervised Loss}
  \label{fig:SupervisedTrainingLoss}
\end{subfigure}
%\hfill
\begin{subfigure}{0.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{latex/Images/semi_supervised_learning_training_loss.png}
  \caption{Semi-supervised Loss}
  \label{fig:SemiSupervisedTrainingLoss}
\end{subfigure}
\caption{\textbf{Training loss} \textit{ (a) supervised and (b) semi-supervised learning.}}
\label{fig:TrainingLosses}
\end{figure}



\begin{table*}[!ht]
    \centering
    \small
    \resizebox{1.0\textwidth}{!}{%
    \begin{tabular}{l|ccc|ccc|ccc}
    \toprule
     & \multicolumn{3}{c|}{Neutral} & \multicolumn{3}{c|}{Frustration} & \multicolumn{3}{c}{Happy} \\
    Metrics & Precision & Recall & F1-score & Precision & Recall & F1-score & Precision & Recall & F1-score \\
    \midrule
    Distilbert\_supervised\ & 0.130 & 0.176 & 0.150 & 0.444 & 0.364 & 0.400 & 0.000 & 0.000 & 0.000 \\
    Distilbert\_semi\_supervised & 0.141 & 0.121 & 0.131 & 0.132 & 0.227 & 0.167 & 0.000 & 0.000 & 0.000 \\
    mBERT\_supervised\ & 0.043 & 0.059 & 0.050 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\
    mBERT\_semi\_supervised & 0.284 & 0.234 & 0.256 & 0.100 & 0.045 & 0.063 & 0.000 & 0.000 & 0.000 \\
    XLM\_R\_supervised\_training & 1.000 & 0.118 & 0.211 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\
    XML\_R\_semi\_supervised & 0.571 & 0.037 & 0.070 & 0.333 & 0.015 & 0.029 & 0.000 & 0.000 & 0.000 \\
    AfriBERTa\_large\_supervised & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\
    AfriBERTa\_semi\_supervised & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Model Performance Evaluation on Emotion classification Tasks.} \textit{Performance metrics of supervised and semi-supervised learning for (Neutral, Frustration, and Happy) emotion analysis across models. Showing poor performance of happy emotions.}}
    \label{tab:model_performance_sentiment_emotion_table1}
\end{table*}


\begin{table*}[!ht]
    \centering
    \small
    \resizebox{1.0\textwidth}{!}{%
    \begin{tabular}{l|ccc|ccc|ccc}
    \toprule
     & \multicolumn{3}{c|}{Anger} & \multicolumn{3}{c|}{Love} & \multicolumn{3}{c}{Fear} \\
    Model & Precision & Recall & F1-Score & Precision & Recall & F1-Score & Precision & Recall & F1-Score \\
    \midrule
    Distilbert\_supervised\ & 0.517 & 0.861 & 0.646 & 0.333 & 0.222 & 0.267 & 0.000 & 0.000 & 0.000 \\
    Distilbert\_semi\_supervised & 0.445 & 0.833 & 0.580 & 0.357 & 0.212 & 0.266 & 0.000 & 0.000 & 0.000 \\
    mBERT\_supervised\ & 0.524 & 0.795 & 0.632 & 0.408 & 0.444 & 0.426 & 0.000 & 0.000 & 0.000 \\
    mBERT\_semi\_supervised & 0.487 & 0.838 & 0.616 & 0.438 & 0.430 & 0.434 & 0.000 & 0.000 & 0.000 \\
    XLM\_R\_supervised\ & 0.553 & 0.943 & 0.697 & 0.489 & 0.489 & 0.489 & 0.000 & 0.000 & 0.000 \\
    XML\_R\_semi\_supervised & 0.506 & 0.918 & 0.652 & 0.427 & 0.461 & 0.443 & 0.000 & 0.000 & 0.000 \\
    AfriBERTa\_large\_supervised & 0.484 & 0.975 & 0.647 & 0.250 & 0.022 & 0.041 & 0.000 & 0.000 & 0.000 \\
    AfriBERTa\_semi\_supervised & 0.417 & 0.920 & 0.574 & 0.182 & 0.012 & 0.023 & 0.000 & 0.000 & 0.000 \\
    \bottomrule
    \end{tabular}}
    \caption{\textbf{Model Performance Evaluation on Emotion Classification Tasks.} \textit{Performance metrics of supervised and semi-supervised training for (Anger, Love, and Fear) emotion analysis across models. The model performed poorly on Fear emotions.}}
    \label{tab:model_performance_emotion_analysis_table2}
\end{table*}

\begin{table*}[!ht]
    \centering
    \small
    \resizebox{1.0\textwidth}{!}{%
    \begin{tabular}{l|ccc|ccc|ccc}
    \toprule
     & \multicolumn{3}{c|}{Sadness} & \multicolumn{3}{c|}{Empathy} & \multicolumn{3}{c}{Surprise} \\
    Model & Precision & Recall & F1-Score & Precision & Recall & F1-Score & Precision & Recall & F1-Score \\
    \midrule
    Distilbert\_supervised\ & 0.500 & 0.222 & 0.308 & 0.833 & 0.652 & 0.732 & 0.250 & 0.333 & 0.286 \\
    Distilbert\_semi\_supervised & 0.100 & 0.083 & 0.091 & 0.844 & 0.580 & 0.688 & 0.360 & 0.337 & 0.348 \\
    mBERT\_supervised\ & 0.200 & 0.222 & 0.211 & 0.865 & 0.660 & 0.749 & 0.237 & 0.500 & 0.321 \\
    mBERT\_semi\_supervised & 0.129 & 0.167 & 0.145 & 0.855 & 0.621 & 0.720 & 0.377 & 0.516 & 0.436 \\
    XLM\_R\_supervised\ & 0.250 & 0.111 & 0.154 & 0.791 & 0.747 & 0.768 & 0.000 & 0.000 & 0.000 \\
    XML\_R\_semi\_supervised & 0.154 & 0.083 & 0.108 & 0.767 & 0.701 & 0.733 & 0.250 & 0.021 & 0.039 \\
    AfriBERTa\_large\_supervised & 0.000 & 0.000 & 0.000 & 0.731 & 0.719 & 0.725 & 0.000 & 0.000 & 0.000 \\
    AfriBERTa\_semi\_supervised & 0.000 & 0.000 & 0.000 & 0.706 & 0.659 & 0.682 & 0.000 & 0.000 & 0.000 \\
    \bottomrule
    \end{tabular}}
    \caption{\textbf{Model Performance Evaluation on Emotion classification Tasks.} \textit{Performance metrics of supervised and semi-supervised training methods for emotion (Sadness, Empathy, and Surprise) analysis across various models. Showing outstanding performance on  Empathy emotions.}}
    \label{tab:model_performance_emotion_analysis_table3}
\end{table*}


\section{Results and Discussions}
\label{sxn:res}

\subsection{Sentiment Analysis}
Table~\ref{tab:model_performance_sentiment_emotion} summarizes the performance of all models on sentiment analysis. XLM-R supervised achieves the highest overall performance with an accuracy of 62.5\%  and an F1-score of 66.7\%. This is followed closely with semi-supervised XLM-R, which has an accuracy of 62.1\% and an F1-score of 68.3\%. However, DistilBERT supervised performance falls behind with an accuracy of 57.8\%  and an F1-score of 54.6\%. On the other hand, mBERT models show consistency between supervised and semi-supervised training, maintaining average F1-scores of 59.8\% and 59.6\%, respectively. AfriBERTa models struggled, with the supervised learning achieving an F1-score of 35.8\%, and overall poorest performance across all metrics.

The detailed performance metrics for negative, neutral, and positive sentiment classification are presented in Table~\ref{tab:model_performance_sentiment}.  For the negative sentiment, the supervised XLM-R achieves a high F1-score of 69.9\%, unlike the semi-supervised AfriBERTa, which has the worst F1-score of 17.4\%. In neutral sentiment classification, the supervised XLM-R again excels with an F1-score of 52.6\%. For the positive sentiment, the semi-supervised XLM-R stands out with an exceptional F1-score of 77.1\%, and the semi-supervised AfriBERTa shows robust performance with an F1-score of 66.3\%. 

\begin{figure*}[h]
    \centering
    \begin{subfigure}[t]{0.46\textwidth}
        \centering
        \includegraphics[width=\textwidth]{latex/Images/sentiment_comparison_heatmap.png}
        \caption{Sentiment Prediction Comparison Across Models}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{latex/Images/emotion_comparison_heatmap.png}
        \caption{Emotion Prediction Comparison Across Models}
    \end{subfigure}    
    \caption{\textit{Heatmaps comparing sentiment and emotion predictions across different models. AfriBERT model most frequently predicts neutral sentiment and shows the highest sensitivity for empathy emotions.}}
    \label{fig:heatmaps}
\end{figure*}

\subsection{Emotion Analysis}
Table~\ref{tab:model_performance_sentiment_emotion} summarizes the performance of all models on emotion analysis. The models generally show lower performance than sentiment analysis. Since emotions are complex \cite{mohammad2017word}. The supervised DistilBERT achieves the highest F1-score of 31\%, followed by mBERT semi-supervised, with an F1-score of 29.7\%.


Table~\ref{tab:model_performance_sentiment_emotion_table1} shows  performance for emotion classification across neutral, frustration, and happy. DistilBERT supervised leads in frustration with an F1-score of 40\%. All models perform poorly on happy emotion classification. In Table~\ref{tab:model_performance_emotion_analysis_table2}, XLM-R supervised leads for anger and love emotions with F1-scores of 69.7\% and 48.9\%, respectively, but all models struggle with fear emotion. Table~\ref{tab:model_performance_emotion_analysis_table3} shows low performance for sadness and surprise  but outstanding performance for empathy with XLM-R supervised, leading with an F1-score of 76.8\%.


\subsection{Pretrained Models performance}
As shown in Figure~\ref{fig:heatmaps}, XLM-R, particularly in its supervised form, consistently outperforms other models across sentiment and emotion analysis tasks. mBERT also performs reliably well in sentiment analysis and some emotion classifications. DistilBERT, while efficient, has limitations in handling a range of emotions. AfriBERTa shows lower performance across most metrics than other models. Despite being tailored to African languages, AfriBERTa models do not perform as well in sentiment and even worse in emotion analysis.


\subsection{Semi-Supervised Performance Analysis}
The detailed analysis of SSL models reveals mixed outcomes, with clear performance enhancements in certain models and tasks, particularly in sentiment analysis. For example, mBERT's semi-supervised version slightly improved sentiment analysis with an F1-score of 59.8\% compared to 59.6\% for supervised version. In emotion analysis, mBERT's semi-supervised version outperformed its supervised counterpart with an F1-score of 29.7\% versus 26.5\%. The semi-supervised AfriBERTa achieved an F1-score of 36.6\% in sentiment analysis, marginally higher than the supervised version's 35.8\%, and scored 15.7\% compared to 14.2\% in emotion task. 


\section{Limitations}
We acknowledge the subjective nature of sentiment and emotion analysis, which can be influenced by label bias, leading to inconsistencies in labeled data. We will publicly share our dataset to address this issue and facilitate further study on label bias and annotator disagreement. 
Secondly, the cost of obtaining labeled datasets, particularly from native speakers, can be challenging. Transformer models, SOTA for sentiment and emotion analysis, require large data and computational resources, which is still challenging in low-resource setting. Lastly, We recognize the ethical considerations of LLM use.

\section{Conclusions and Future Work}
\label{sec:conclusion}
We presented RideKE, a code-switched dataset from Twitter, with sentiment and emotion labels partially annotated for Kenyan-accented English mixed with Swahili and Sheng. Our semi-supervised learning shows mixed results, with clear performance enhancements in certain models and tasks, particularly in sentiment analysis, suggesting its potential to generally enhance model performance. We highlight the benefits of semi-supervised learning in improving model performance and reducing data annotation costs.

In the future, we aim to further enhance model performance by expanding the pool of human-labeled datasets, using other semi-supervised approaches, utilizing techniques like few-shot learning, and experimenting with different model architectures and hyperparameters tuning.

%\smallskip
%\noindent\textbf
\section*{Acknowledgments}
We thank the volunteer annotators who dedicated their time and expertise to this project, which would not have succeeded without their commitment. % to accurately labeling and reviewing our dataset.



% \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
% Maria -- is there a bibliography style you should use?

\bibliography{anthology, custom}


\clearpage

\appendix

%--------------------------------------------------------------------
\section{Appendix}
%--------------------------------------------------------------------

\subsection{Language Detection}

\begin{table}[ht]
\begin{tabular}{lll}
\toprule
\parbox{.75in}{\textbf{Language Code}} & \textbf{Occurrences} & \textbf{Language} \\
\midrule
en & 29845 & English \\
id & 3288 & Indonesian \\
sw & 624 & Swahili \\
no & 192 & Norwegian \\
da & 119 & Danish \\
tr & 95 & Turkish \\
nl & 81 & Dutch \\
af & 73 & Afrikaans \\
de & 71 & German \\
ca & 55 & Catalan \\
so & 46 & Somali \\
sv & 34 & Swedish \\
et & 26 & Estonian \\
tl & 15 & Tagalog (Filipino) \\
hu & 14 & Hungarian \\
fr & 14 & French \\
es & 10 & Spanish \\
hr & 9 & Croatian \\
it & 8 & Italian \\
cy & 8 & Welsh \\
fi & 6 & Finnish \\
pl & 4 & Polish \\
sl & 3 & Slovenian \\
lt & 3 & Lithuanian \\
ro & 3 & Romanian \\
% More rows can be added as needed
\bottomrule
\end{tabular}
\caption{Count of language detection in the RideKE dataset}
\label{tab:my_label}
\end{table}

% \newpage

\subsection{Tweets Per Location}

\begin{figure}[ht]
  \includegraphics[width=\columnwidth]{latex/Images/user_location_distribution_log_scale.png}
  \caption{Number of tweets per location on a logarithmic scale. Nairobi appears to be the most active location per dataset.}
  \label{fig:number of tweets per location using logarithmic scale}
\end{figure}


\subsection{\rule{0pt}{6ex}Sheng-to-English Sample Sentences}

\begin{table}[ht]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Sheng} & \textbf{English Translation} \\
\midrule
dere anadai  &  Driver demands \\
kuna some people eating  & people benefitting \\
ferry slay queens & Ferry divas \\
Mmemulikwaa oya  & on the spotlight ! \\
Mhesh  & honorable sir  \\
wazungu’s& white people \\
sikwembe ya Yesu & strong faith in Jesus \\
Hiyo pesa ni kadonye & That’s little money \\
fare noma & Expensive fare \\
kuweka ngata & To fuel\\
\bottomrule
\end{tabular}
\caption{Sheng to English Example Sentences}
\label{table:sheng_english}
\end{table}

\onecolumn
\newpage

\subsection{Annotation Guidelines}
\label{sec:appendix_annotation}
\begin{table*}[!htbp]
\centering
\small
\begin{tabular}{|l|p{\dimexpr\textwidth-2\tabcolsep-2in\relax}|} % Reduced from 1in to 2in
\hline
\textbf{Aspect} & \textbf{Details} \\
\hline
Title & Annotation Guidelines for RHS Conversation on Twitter \\
\hline
Task & Annotating emotions in tweets related to RHS experiences \\
\hline
Annotation Process & 
\begin{itemize} \setlength{\itemsep}{1pt} \setlength{\parskip}{0pt} \setlength{\parsep}{0pt}
    \item Emotion Definition: Annotators accurately identify and label the predominant emotion expressed in each tweet based on the emotional tone conveyed by the text.
    \item Keyword Identification: Pay attention to keywords or phrases that suggest the presence of a particular emotion.
    \item Context Matters: Consider the tweet's context, including any relevant hashtags, mentions, or user profiles, for a better understanding of the emotional context.
    \item Tweet Length: Emotions can be expressed differently in short and long tweets.
\end{itemize} \\
\hline
Emotion Labels Guidelines & 
\begin{enumerate}\setlength{\partopsep}{0pt} \setlength{\itemsep}{0pt} \setlength{\parskip}{0pt}
    \item \textbf{Anger:} Label when the tweet expresses frustration, annoyance, resentment, or strong displeasure toward RHS, drivers, or related issues. Look for keywords and tone indicative of anger. Keywords: angry, furious, annoyed, upset. Example: "Terrible experience with Uber driver! He was rude and refused to follow the GPS directions \#Angry".
    \item \textbf{Happy:} Label when the tweet reflects joy, satisfaction, contentment, or delight regarding RHS experiences. Look for expressions of happiness, appreciation, or positive feedback. Keywords: happy, delighted, thrilled, satisfied. Example: ``Just had the best ride ever with the friendliest driver! \#HappyCustomer \#GreatService''
    \item \textbf{Fear:} Label when the tweet expresses anxiety, worry, concern, or fear about RHS safety, incidents, or perceived risks. Identify cues of fear or apprehension. Keywords: afraid, scared, worried, nervous. Example: "My ride is taking an unfamiliar route, and I'm getting worried. Is this safe? \#Fear"
    \item \textbf{Suprise:} Label when the tweet indicates astonishment, amazement, or unexpected reactions to RHS experiences.Keywords: surprised, shocked, amazed, unexpected. Example: "Wow, my driver gave me a free upgrade to a luxury car! \#Surprised 
    \item \textbf{Love:} Label when the tweet reflects affection, appreciation, or strong positive emotions toward RHS, drivers, or related aspects. Look for expressions of love or admiration. Keywords: love, adore, appreciate, grateful. Example: "Wow, my driver gave me a free upgrade to a luxury car! \#Surprised \#Love"
    \item \textbf{Frustration:} Label when the tweet expresses dissatisfaction, irritation, or being fed up with RHS issues. Identify cues of frustration and annoyance. Keyword: frustrated, annoyed, fed up, irritated. Example: "Been waiting for my ride for ages. This is so frustrating!\#Frustrated \#LateAgain"
   
     \item \textbf{Neutral:} Label when the tweet does not exhibit any strong emotional sentiment or when the emotion is unclear or ambiguous. Use this label sparingly and only when other emotions are not evident. Example: "Just booked my ride for tomorrow morning. \#RideHail \#PlanningAhead
   
  \end{enumerate} \\
\hline
Quality Control & Monitor inter-annotator agreement to ensure consistency among annotators. Resolve disagreements through discussion and clarification. \\
\hline
Privacy and Ethical Considerations & Respect user privacy and report any offensive content appropriately. \\
\hline
\end{tabular}
\caption{Annotation guidelines for ride-hailing service conversation emotions on Twitter}
\label{tab:emotion_guidelines}
\end{table*}

\newpage

\subsection{Sample dataset structure}

\newcommand{\0}[1]{{\bf \begin{tabular}{@{}l@{}}#1\end{tabular}}}

\begin{table}[!htbp]
\centering
%\small
\resizebox{\textwidth}{!}{%
%\scalebox{0.5}
\begin{tabular}{|l|l|p{5cm}|l|l|l|l|l|l|l|l|l|}
\hline
\textbf{Keyword} & \textbf{Date} & \textbf{Tweets} & \0{reply\\count} & \0{retweet\\count} & \0{like\\count} & \textbf{verified} & \0{user\\followers} & \0{user\\following} & \0{user\\tweets} & \0{user\\location} & \textbf{country} \\ \hline
\#UBER-Kenya & 2023-04-10 & Did Nairobi ask you to double Nairobi fare price ? That's how Uber Kenya and bolt steal from us here. & 1 & 0 & 0 & 0 & 2104 & 981 & 23173 & Mombasa & Kenya \\ \hline
\#UBER-Kenya & 2023-03-30 & Uber Kenya made an order that was cancelled by a restaurant but I've already paid. How do I follow up on my refund? & 1 & 0 & 0 & 0 & 946 & 975 & 4642 & Nairobi & Kenya \\ \hline
\#UBER-Kenya & 2023-03-30 & Uber Kenya made an order that was cancelled by a restaurant but I've already paid. How do I follow up on my refund? & 1 & 0 & 0 & 0 & 946 & 975 & 4642 & Nairobi & Kenya \\ \hline
\#UBER-Kenya & 2023-04-02 & Uber is losing the Kenyan market to Nairobi apps, customers are tired of being asked by drivers where in Nairobi they are going. Nairobi apps show Nairobi drivers where the customer is, where is going and price hence drivers will decide to accept or decline the request. & 2 & 0 & 0 & 0 & 46 & 297 & 817 & Nairobi & Kenya \\ \hline
\#UBER-Kenya & 2023-03-27 & Uber Kenya how can your driver click not paid when he was paid? And Nairobi is proof of payment? & 2 & 0 & 0 & 0 & 5744 & 1338 & 208846 & Nairobi & Kenya \\ \hline
\#BOLT-Kenya & 2023-04-06 & Hello, thanks for writing in. Kindly do reach out to us via kenyabolt.eu and a member of our team will respond and assist accordingly. & 0 & 0 & 0 & 1 & 15093 & 447 & 16966 & Nairobi & Kenya \\ \hline
\#BOLT-Kenya & 2023-01-16 & Let's have an honest conversation here...this morning you lowered the base category to 8ksh per kilometer. We all know that fuel is still very high. What method did you use to reach this point, Did you involve drivers about the & 1 & 0 & 3 & 0 & 14 & 87 & 82 & Nairobi & Kenya \\ \hline
\#BOLT-Kenya & 2022-11-19 & If you don't communicate. Let us as drivers do what we feel like doing. Because bolt Kenya is manner less. & 0 & 0 & 0 & 0 & 11 & 69 & 66 & Nairobi & Kenya \\ \hline
\#BOLT-Kenya & 2019-10-27 & How come Bolt Kenya does not have an active customer service line for queries? & 0 & 0 & 0 & 0 & 23 & 180 & 35 & Nairobi & Kenya \\ \hline
\#BOLT-Kenya & 2019-05-20 & Thanks to Boltkenya been arriving at my studio sessions and interviews on time and with comfort. You too can enjoy this service by simply downloading boltkenya and using my code FEMIONEBOLT to get kshs250 off & 0 & 0 & 3 & 0 & 40629 & 528 & 24042 & Nairobi & Kenya \\ \hline
\#LITTLECAB & 2022-07-31 & Now \#Littlecab will not allow me to cancel a ride I did not take until I pay. Exhausting! & 0 & 0 & 0 & 0 & 636 & 2222 & 4085 & Nairobi & Kenya \\ \hline
\#LITTLECAB & 2022-07-31 & And they let me have their driver. The security officer at \#Carnivorekenya says that they do not verify the drivers. What is the whole point of telling us to use \#littlecab if you have no relationship with them. Just destroyed my whole experience attending a beautiful musical. & 1 & 0 & 0 & 0 & 636 & 2222 & 4085 & Nairobi & Kenya \\ \hline
\#LITTLECAB & 2022-05-10 & Use \#Littlecab. These other Apps are foreign and exploitive. & 0 & 0 & 1 & 0 & 480 & 987 & 4740 & Mombasa & Kenya \\ \hline
\#LITTLECAB & 2020-12-23 & Why do we always encounter cabs from \#LittleCab that arrive with different number plates from what is registered in your system? While I don't board them in principle for security concerns, it may one day be costly for a desperate client & 2 & 0 & 0 & 0 & 4712 & 3044 & 20683 & Nairobi & Kenya \\ \hline
\end{tabular}%
}
\caption{Original sample of the tweets data structure}
\label{tab:tweet_data_analysis}
\end{table}

\end{document}
