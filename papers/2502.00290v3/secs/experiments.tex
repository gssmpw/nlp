\section{Application I: Dynamic Decoding Strategy}\label{sec:case1}


\subsection{LogTokU-guided Decoding}
It is necessary to ensure that the generated response to be of diversity, especially in diversity-driven fields such as LLM-guided discovery researches~\cite{peng2024large}. However, higher diversity often means that the LLM's responses are more prone to hallucinations. For example, during the sampling process, a larger temperature tends to generate more unreliable responses from the model. Therefore, ensuring both the diversity and precision of LLM-generated results is a key challenge.

LogTokU offers a potential solution to this challenge. In this subsection, we propose a dynamic decoding strategy that can adjust its sampling approach according to LogTokU during response generation. The dynamic decoding strategy ensures diverse answers when the LLM has adequate knowledge (i.e., low model uncertainty), while adopting a more cautious sampling strategy when the model's knowledge is insufficient (i.e., high model uncertainty), thus maintaining both diversity and accuracy in the generated responses. Specifically, we hope that the sampling diversity and the LLM's EU about the next token denoted as $\text{EU}(a_t)$ are negatively correlated,
which means that the higher EU, the less chance for sampling tokens with lower scores.
Taking the temperature sampling strategy as an example, there is a smaller temperature when the model has larger EU.

\subsection{Experimental Analysis}
\subsubsection{Settings}
In this paper, we use the multi-label evaluation benchmark SemEval~\cite{mohammad-etal-2018-semeval}, which is a multi-tag NLP analysis task on tweet text. We evaluate models of different sizes, including LLaMA2 (7B)\footnote{\href{https://huggingface.co/meta-llama/Llama-2-7b-chat-hf}{https://huggingface.co/meta-llama/}}, LLaMA2 (13B), LLaMA3 (3B), LLaMA3 (8B) and LLaMA3 (70B). As shown in Fig.~\ref{fig:seting}, after providing an answer with the first class label, LLM dynamically decides whether to give the second class label based on the uncertainty indicator. We can evaluate the ability of uncertainty to guide the model in rejecting incorrect answers (the ability to avoid hallucinations in generating diverse responses).

\input{tabs/wagering_debug}

To evaluate the effectiveness of the dynamic decoding strategy, we verify whether it can enhance diversity while maintaining accuracy. Specifically, we evaluate whether the model can select as many correct answers as possible with the guidance of uncertainty on the multi-label LLM benchmark SemEval~\cite{mohammad-etal-2018-semeval}. As shown in Fig.~\ref{fig:seting}, for all test samples in the entire data set, when generating responses, the LLM selects the class with the highest output probability or the two tokens with the highest probabilities at the critical token (class) position based on uncertainty. The rule is similar to multiple-choice questions in exams. For any question, the LLM can decide whether to answer a second class. If it answers, there are two possibilities: (1) the second class is correct, the LLM will get one more point for its diversity; (2) the second class is incorrect, which may result in losing the points earned from the first correct answer. In other words, the LLM needs to estimate its own confidence and balance the trade-off between the penalty (hallucination) and award (diversity).

% \begin{figure}[!t] 
% \centering 
% \includegraphics[width=0.4\textwidth]{figs/gambling.png} 
% \caption{Illustration of gambling evaluation framework.} \label{fig:gambling}
% \end{figure}

\begin{figure}
\centering 
\includegraphics[width=0.8\textwidth]{figs/experimentset.pdf} 
\caption{Illustration of experimental setting in Table~\ref{tab:percent}.} \label{fig:seting}
\end{figure}



\textbf{Compared methods}. We introduce two baseline decoding strategies and two dynamic decoding strategies based on probability and entropy, including: $\bullet$ Greedy Search: the LLM sacrifices diversity and selects only one class for all samples with the highest probability. $\bullet$ Top-2: the LLM seeks diversity and selects two classes for all samples with the highest probability and the second highest probability. $\bullet$ Probability: the LLM dynamically chooses to select either one class or two classes for different samples, with the maximum probability as the indicator. The LLM selects one class when the maximum probability is low and two classes when the maximum probability is high. $\bullet$ Entropy: This is also a dynamic strategy with entropy as an indicator. The LLM selects one class when the entropy is low and two classes when the entropy is high. $\bullet$ LogTokU: This is also a dynamic strategy, with the indicator being $\text{EU}(a_t)$, as shown in Sec.~\ref{sec:case1}. The LLM selects one class when EU is high and two classes when EU is low.



\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figs/curve.pdf}
\caption{A close-up observation explains why LogTokU achieves the best performance. All samples are sorted by reliability from high to low. The ``performance'' is the accumulated score (i.e., the number of accumulated correct responses minus the number of accumulated incorrect responses). A trend of increasing and then decreasing represents a good reliability indicator, where the answer becomes more likely to be wrong as reliability decreases.}
\label{fig:curve}
\end{figure}


\subsubsection{Results Analyses}

We show the normalized performance of different methods in various LLM sizes (higher is better), as shown in Table~\ref{tab:percent}. We can find that using LogTokU achieves the best performance, with a significant improvement in the overall score of the model, with an improvement of more than 10\% on LLaMA2-13B. 

\input{tabs/relia_dubug}

As shown in Fig.~\ref{fig:seting}, when the model chooses to generate more than one answer, the score obtained may either increase (award of correct response) or decrease (penalty of error response), and a trustworthy response reliability indicator should have the ability to identify the error responses. Therefore, we estimate the accumulated score curve as LLM answers more and more questions with diverse answers. Specifically, we arrange all samples in ascending order of estimated uncertainty of all methods, then the samples with uncertainty lower than threshold will answer more than one answer, which the others just answer the choice with the highest prediction probability.
Ideally, the accumulated score change should follow a trend that first increases and then decreases, which implies that samples with lower uncertainty are more likely to gain additional score when challenged to give more diverse responses. As the uncertainty increases, the probability of giving the response to be error also increases. The results are shown in Fig.~\ref{fig:curve}, and it can be observed that the uncertainty indicated by LogTokU consistently satisfies the expectation. However, traditional probability-based uncertainty estimation methods show a decrease trend when the estimated reliability is high, indicating that these methods fail to estimate the risk of giving more diverse responses. We highlight the best performance points in terms of accumulated score for different methods, and it can be observed that our method significantly outperforms the comparison methods.


\section{Application II: Reliability Estimation}
\subsection{LogTokU-guided Response Uncertainty Estimation}

Another benefit that LogTokU brings is the estimation of the reliability of the response. In traditional probability-based token uncertainty estimation, a large number of uncritical tokens exhibit high uncertainty, making it difficult to map token uncertainty to sentence uncertainty. This issue requires manually applying weights to different tokens to overcome the problem caused by uncritical tokens being estimated with high uncertainty. LogTokU naturally overcomes this problem. As shown in the case study in Fig.~\ref{fig:cover2}, for uncritical tokens such as commas, the probability-based method labels them as unreliable tokens due to their low predictive probability (high entropy). In contrast, LogTokU accurately classifies them into the fourth quadrant, marking them as ``I know more than one answer''. Therefore, LogTokU can more easily use token uncertainty to represent the uncertainty of sentences. Inspired by~\cite{duan2024shifting}, we use the most uncertain tokens in a sentence to represent the overall reliability of the sentence. The response reliability can be represented as the averaged reliability on tokens with the $K$-lowest reliability values:
\begin{equation}
    \mathcal{R}_{\text{response}} = \frac{1}{K} \sum_{t \in \mathcal{T}_{K}} \mathcal{R}(a_{t}),
\end{equation}
where $ \mathcal{R}_{\text{response}}$ indicates the reliability of the response and $\mathcal{R}(a_{t})$ represents the reliability of token $a_t$, and \(\mathcal{T}_{K}\) represents the set of \(K\) tokens with the lowest reliability values (\(\text{rel}(a_{t})\)).



Following the uncertainty combination of AU and EU in discriminative models~\cite{abdar2021review}, in the experiments, we simply represent the reliability of the token as: 
\begin{equation}
    \mathcal{R}(a_{t}) =\frac{1}{\text{AU}(a_t)\cdot \text{EU}(a_t)},
\end{equation}
which indicates that the reliability of the token is low when both the estimated AU and EU are high. In this paper, we simply identify the response reliability in QA task according to the scenarios classified in Fig.~\ref{fig:cover1}: (1) high AU, high EU: LLM lacks knowledge of the question and has no idea of a suggested answer (unreliable); (2) low AU, high EU: The LLM lacks knowledge of the question, but it knows what should be an appropriate answer (reliable); (3) low AU, low EU: LLM knows precisely what is the most appropriate answer (reliable);
(4) high AU, low EU: LLM has encountered many similar samples during training and knows more than one suitable response (reliable). 



\subsection{Experimental Analysis}
\subsubsection{Settings}
To validate whether the estimated response reliability is trustworthy, we evaluate it on the TruthfulQA benchmark dataset~\cite{lin2021truthfulqa}. Following standard settings, we consider responses with $\text{BLEURT} > 0.5$ as correct answers, and use the estimated reliability as the score for the responses to calculate the AUROC. A higher AUROC indicates that responses estimated with lower reliability are more likely to be incorrect. We compare LogTokU with probability-based strategy, entropy-based strategy and recent sampling-based uncertainty estimation methods, including: LN-Entropy (LN-E)~\cite{malinin2021uncertainty}, Semantic Entropy (SE)~\cite{kuhn2023semantic}, D-Semantic Entropy (DSE)~\cite{kuhn2023semantic}, Lexical Similarity (LeS)~\cite{lin2024generating}. Specifically, the token reliability based on probability is represented as: $\mathcal{R}(a_t)=\log \left(p(a_t|\bm{q},\bm{a}_{t-1},\mathcal{M})\right)$, and the token reliability based on entropy is represented as: $\mathcal{R}(a_t)=\frac{1}{H(a_t)}$, where $H(a_t)$ indicates the entropy of prediction distribution of $a_t$.


\subsubsection{Results Analyses}

Table~\ref{tab:reliability} shows the AUROC performance of different estimation methods in the TruthfulQA benchmark, with a higher value indicating a more reliable response reliability estimation. The results show that LogTokU achieves the best performance in models of various sizes, suggesting that LogTokU provides a better estimation of the response reliability. Furthermore, we find that sampling methods do not perform well on this task, and similar observations have been reported in~\cite{xiong2024efficient}. To illustrate the performance gap between sampling-based methods and LogTokU, we calculate the average performance of current sampling-based methods termed ``Average''. One potential reason is because sampling-based methods cannot capture the situation in the Quadrant IV and thus fail to measure models' inherent uncertainty. For more experimental results, please refer to the appendix.
