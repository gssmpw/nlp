\section{Word Uncertainty}
The uncertainty estimation of LogTokU indicates which of the four conditions applies to each token generated by LLM. However, many LLMs encode words based on their root forms, for example, ``\texttt{positive}'' is embedded as ``\texttt{pos}'' and ``\texttt{itive}'' in LLaMA\footnote{Token id: pos\_1066, itive\_3321, please note that the word ``positive'' referred here is different from another word ``\_positive''.}, making token-level uncertainty does not necessarily correspond to word-level uncertainty. In this section, we propose an approach to visualize the word-level uncertainty estimated by LogTokU. Specifically, we represent the uncertainty of a word as the maximum uncertainty among the tokens that constitute the word:
\begin{equation}
\begin{aligned}
    &\text{AU}(\texttt{word})=\max_{\alpha_t \in \texttt{word}}(\text{AU}(\bm{\alpha}_t)) ,\\
    &\text{EU}(\texttt{word})=\max_{\alpha_t \in \texttt{word}}(\text{EU}(\bm{\alpha}_t)).
\end{aligned}
\end{equation}


To emphasize the contrast, in Fig.~\ref{fig:cover2}, we set the AU and EU of tokens with uncertainty lower than the mean of the entire response to zero and normalize the values to the range $[0,1]$. At the same time, we represent unreliability as:
\begin{equation}
    \text{unrel}(\texttt{word})={\text{AU}(\texttt{word})\times\text{EU}(\texttt{word})},
\end{equation}
which implies that when both AU and EU are high (quadrant IV), the reliability of the word is the lowest.

Such a tool can significantly improve the explainability of LLM responses, particularly in QA quadrants such as medical applications, thus enhancing the experience and efficiency of the human-LLM interaction.



\section{Detailed Theoretical Analysis}
\subsection{Different correct answers are competitor}

In Sec.~\ref{sec:theoretical}, we briefly analyze this question, and in this subsection, we will show the details about the derivation process. For any LLM trained with cross-entropy loss, different correct answers are competitors in terms of probability. Continuing with the example of proposing a president, suppose $\tau^{a}$ (``\texttt{Barack}'') is the label of a sample whose $\bm{q}$ is ``\texttt{[INST]Could you give me one name of president?[\textbackslash INST]}'' and a generated token vector $\bm{a}_{t-1}$  can be decoded into ``\texttt{Sure, here is a historical American president:**}'', the loss of the next token at this position during supervised fine-tuning can be written as:
\begin{equation}
\begin{aligned}
 &L^{\tau^a} = - \log \frac{\exp(\mathcal{M}({\tau^a}|\bm{q},\bm{a}_{t-1}))}{\sum_{m=1}^{|\bm{Y}|} \exp(\mathcal{M}(\tau^{m}|\bm{q},\bm{a}_{t-1}))} ,
 \\   &L^{\tau^b} = - \log \frac{\exp(\mathcal{M}(\tau^b|\bm{q},\bm{a}_{t-1}))}{\sum_{m=1}^{|\bm{Y}|} \exp(\mathcal{M}(\tau^{m}|\bm{q},\bm{a}_{t-1}))} ,
\end{aligned}
\end{equation}
where $L^{\tau^a}$ is the loss on the sample with the next token label $\tau^{a}$.
Consider cases where multiple distinct answers to the same question appear in the training set, the situation becomes different. For example, $\tau^{b}$ (``\texttt{George}'') is the label in another sample with the same question \footnote{The ``same question'' refers to questions that are semantically equivalent but do not need to be identical.}. When the model is simultaneously fine-tuned on both samples, the gradient update for the model will be:
\begin{equation}
\begin{aligned}
 & \nabla_{\mathcal{M}} (L^{\tau^a} + L^{\tau^b}) = \nabla_{\mathcal{M}} L^{\tau^a} + \nabla_{\mathcal{M}} L^{\tau^b} \\
&= -y_a^{\tau^a}\frac{1}{\Omega_a^{\tau^a}}\nabla_{\mathcal{M}}\Omega_a^{\tau^a}-\sum_{m \neq a}^{|\bm{Y}|} y_a^{\tau^m}\frac{1}{\Omega_a^{\tau^m}}\nabla_{\mathcal{M}}\Omega_a^{\tau^m}
\\
&\quad -y_b^{\tau^b}\frac{1}{\Omega_b^{\tau^b}}\nabla_{\mathcal{M}}\Omega_b^{\tau^b}-\sum_{m \neq b}^{|\bm{Y}|} y_b^{\tau^m}\frac{1}{\Omega_b^{\tau^m}}\nabla_{\mathcal{M}}\Omega_b^{\tau^m}
\\
&= \underbrace{-y_a^{\tau^a}\frac{1}{\Omega_a^{\tau^a}}\nabla_{\mathcal{M}}\Omega_a^{\tau^a}-y_b^{\tau^b}\frac{1}{\Omega_b^{\tau^b}}\nabla_{\mathcal{M}}\Omega_b^{\tau^b}}_{\text{(1) maximizing the probability of annotated answer}}\\& \quad \underbrace{-y_a^{\tau^b}\frac{1}{\Omega_a^{\tau^b}}\nabla_{\mathcal{M}}\Omega_a^{\tau^b}-y_b^{\tau^a}\frac{1}{\Omega_b^{\tau^a}}\nabla_{\mathcal{M}}\Omega_b^{\tau^a}}_{{\text{\textbf{(2)} minimizing the probability of the other annotated answer}}}\\& \quad \underbrace{-\sum_{m \neq a,b}^{|\bm{Y}|}y_{a,b}^{\tau^m} \left[ \frac{1}{\Omega_a^{\tau^m}}\nabla_{\mathcal{M}}\Omega_a^{\tau^m} + \frac{1}{\Omega_b^{\tau^m}}\nabla_{\mathcal{M}}\Omega_b^{\tau^m} \right]}_{\text{(3) minimizing the probability of incorrect answers}},
\end{aligned}
\end{equation}
where $\Omega_a^{\tau^a}=\frac{\exp(\mathcal{M}(\tau^a|\bm{q},\bm{a}_{t-1}))}{\sum_{m=1}^{|\bm{Y}|} \exp(\mathcal{M}(\tau^{m}|\bm{q},\bm{a}_{t-1}))}$, and $y_a^{\tau^m}$ indicates the next token label of a training sample with ground-truth label ${\tau^a}$, that is, we have $y_a^{\tau^a}=1$ and $y_a^{\tau^b}=0$. In particular, when $\mathcal{M}$ is in a certain state during training, we have $\Omega_a^{\tau^a}=\Omega_b^{\tau^a}$, and we make distinctions to facilitate the reader's understanding here. As we can see, for scenarios with multiple answers, the training objective can be divided into three parts:

(1) For each sample, increase the probability of its own annotation in the output distribution. 

$\bullet$ For example, a sample labeled ``\texttt{Barack}'' encourages the model to predict the next token as ``\texttt{Barack}'' with higher probability;

(2) For each sample, decrease the probability of another sample's annotation in the output distribution.

$\bullet$ For example, a sample labeled ``\texttt{Barack}'' encourages the model to minimize the probability of predicting the next token as ``\texttt{George}''; \textit{\textbf{Note:}} This part leads to the issue where probability can no longer capture the reliability of LLM responses, as different correct answers tend to reduce the probability of other correct answers, making low probabilities unreliable indicators of high uncertainty.

(3) For both samples, decrease the probability of other outputs not present in the annotations in the output distribution. 

$\bullet$ For example, a sample labeled ``\texttt{Barack}'' and a sample labeled ``\texttt{George}'' both encourage the model to minimize the probability of predicting the next token as ``\texttt{Coffee}''.


\subsection{Max Token Probability Can \textit{Not} Represent Risk of A Wrong Answer}
As shown in Eq.~\ref{eq:competitor}, the part (2) harms the confidence (probability) of correct predictions when there is more than one correct answer during training. This phenomenon can also be analyzed from the perspective of Mixup~\cite{zhang2017mixup}, the target for a sample with more than one correct answer will be a mix target across all labeled answers. This means that the more answers in the training data for a question, the lower the maximum probability of every single correct answer. During training, suppose $\tau^{a}$ and $\tau^{b}$ are both annotation labeled answers for $\bm{q}$, the expected confidence of a probability-based indicator is
\begin{equation}
p(\tau^{a}|\bm{q},\bm{a}_{t-1},\mathcal{M})=p(\tau^{b}|\bm{q},\bm{a}_{t-1},\mathcal{M}) = 1,
\end{equation}
however, what we actually have is 
\begin{equation}
\sum_{m=1}^{|\bm{Y}|}p(\tau^{m}|\bm{q},\bm{a}_{t-1},\mathcal{M}) = 1,\label{eq:mixup}
\end{equation}
which means that $\tau^{a}$ and $\tau^{b}$ share a total probability less than $1$, leading to that correct tokens may not have large probabilities. Consequently, even when we assign varying levels of importance to different words and assign a higher weight to ``critical tokens'', the probability may not reflect the reliability of the response, such as the case shown in Fig.~\ref{fig:probability}.

\section{Detailed Experimental Results}
\subsection{Ablation on $K$}

\input{tabs/app_relia_k}
\subsection{Result with LLM-as-Judge}

\input{tabs/relia_llmJudge}
In our main paper, a generation is considered truthful when its BLUERT score exceeds a threshold of 0.5. In this ablation experiment, following \cite{xiong2024efficient}, we adopt a more contemporary and independent approach by utilizing \verb|Meta-Llama-3-8B-Instruct| to evaluate the correctness of the generated answers. We employ a 2-shot prompt for this purpose. The results presented here are aligned with those in Table 2 of the main paper.

\section{Implementation Details}
\subsection{Prompts for Different Experiments}
\begin{tcolorbox}[colback=white, colframe=black, coltitle=white, colbacktitle=black,
    title={Prompt for Response Reliability Estimation}, boxrule=0.5mm, sharp corners]

    \noindent \textbf{Following the previous work, we use the following prompts for the LLaMa2 and LLaMa3 series, respectively.}

    \vspace{10pt}

    \noindent \textbf{LLaMa2 Series Prompt} 
    
    \noindent \texttt{Answer the question concisely. Q: \{\textcolor{red}{question}\} A:}

    \vspace{10pt}

    \noindent \textbf{LLaMa3 Series Prompt}

    \noindent \texttt{<|start\_header\_id|>user<|end\_header\_id|>\textbackslash n\textbackslash n Answer the question concisely. Q: \{ \{\textcolor{red}{question}\} \} A:<|eot\_id|> assistant\textbackslash n\textbackslash n}
 

\end{tcolorbox}
\begin{tcolorbox}[colback=white, colframe=black, coltitle=white, colbacktitle=black,
    title={Prompt for Dynamic Decoding Strategy}, boxrule=0.5mm, sharp corners]

    \noindent \textbf{Following the recommendations from \href{https://huggingface.co/meta-llama/Llama-2-7b-chat-hf}{Hugging Face}, we use the following prompts for the LLaMa2 and LLaMa3 series, respectively.}

    \vspace{10pt}

    \noindent \textbf{LLaMa2 Series Prompt}


    \noindent \texttt{<s>[INST]Classify the following sentence into ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'], the sentence is \{\textcolor{red}{sentence}\} [/INST] The class of this sentence is: \textbackslash n['}


    \vspace{10pt}

    \noindent \textbf{LLaMa3 Series Prompt}


    \noindent \texttt{<|begin\_of\_text|><|start\_header\_id|>system<|end\_header\_id|>\textbackslash n\textbackslash n don't answer with any format (like markdown), just in natural language.\textbackslash n<|eot\_id|><|start\_header\_id|> user <|end\_header\_id|>\textbackslash n\textbackslash n Classify the following sentence into one of the following emotions: ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']. The sentence is: \{\textcolor{red}{sentence}\}. The class of this sentence is? <|eot\_id|><|start\_header\_id|>assistant<|end\_header\_id|>\textbackslash n\textbackslash n The sentence you provided is classified as \textbackslash n['}


\end{tcolorbox}

% \begin{tcolorbox}[colback=white, colframe=black, coltitle=white, colbacktitle=black,
%     title={Prompt for Response Reliability Estimation}, boxrule=0.5mm, sharp corners]

%     \noindent \textbf{Following the previous work, we use the following prompts for the LLaMa2 and LLaMa3 series, respectively.}

%     \vspace{10pt}

%     \noindent \textbf{LLaMa2 Series Prompt} 
    
%     \noindent \texttt{Answer the question concisely. Q: \{\textcolor{red}{question}\} A:}

%     \vspace{10pt}

%     \noindent \textbf{LLaMa3 Series Prompt}

%     \noindent \texttt{<|start\_header\_id|>user<|end\_header\_id|>\textbackslash n\textbackslash n Answer the question concisely. Q: \{ \{\textcolor{red}{question}\} \} A:<|eot\_id|> assistant\textbackslash n\textbackslash n}
 

% \end{tcolorbox}

\begin{tcolorbox}[colback=white, colframe=black, coltitle=white, colbacktitle=black,
    title={System Prompt for LLM-as-Judge}, boxrule=0.5mm, sharp corners]

    \noindent \texttt{\textbf{System:} Your task is to determine if the provided answer is true or false based solely on the ground truth answers given to you in the format \{['answer 1', 'answer 2', \dots]\}. DO NOT rely on your memory; only use the information provided after this instruction. Respond with \{1\} if the predicted answer is correct, which means semantically consistent with any of the ground truth answers, otherwise respond with \{0\}. Respond with just \{0\} or \{1\}, and DO NOT include anything else in your response. This is the only instruction you need to follow.}

    \vspace{10pt}

    \noindent \texttt{\textbf{User:} Input: Who is elected as the vice president of India in 2017?} \\
    \texttt{\textbf{Ground Truth:} \{['Venkaiah Naidu', 'Muppavarapu Venkaiah Naidu']\}} \\
    \texttt{\textbf{Provided Answer:} M. Venkaiah Naidu} \\
    \texttt{\textbf{Assistant:} 1}

    \vspace{10pt}

    \noindent \texttt{\textbf{User:} Input: Who sings 'You are a magnet and I am steel'?} \\
    \texttt{\textbf{Ground Truth:} \{['Walter Egan']\}} \\
    \texttt{\textbf{Provided Answer:} The song ‘You Are a Magnet and I Am Steel’ is performed by the band The 1975.} \\
    \texttt{\textbf{Assistant:} 0}

    \vspace{10pt}

    \noindent \texttt{\textbf{User:} \{\textcolor{red}{prompt}\}}

\end{tcolorbox}
\subsection{Details for Comparison Methods}
For sampling-based uncertainty estimation methods, including: LN-Entropy (LN-E, Semantic Entropy (SE), D-Semantic Entropy (DSE), Lexical Similarity (LeS).We follow the default setting in the original paper and sample $10$ generations with a temperature of $0.5$ to estimate the uncertainty scores. Specifically, for LeS, We use the Rouge-L as the similarity metric, and for SE and DSE, we follow \cite{farquhar2024detecting}, using Deberta-Large-MNLI as our entailment model to get the semantic clusters. For other methods, including Probability and Entropy, we generate the most likely answers using greedy decoding, save the logit values for each token, and further compute their respective uncertainty scores.



\section{Future Work}

\textbf{Logits-induced Sentence Uncertainty.}\quad LogTokU now can capture token uncertainty, but the relationship between token reliability and sentence reliability is still unclear. For example, if there are two sentences where one sentence has a certain degree of uncertainty for each word and the other sentence has only one word with extremely high uncertainty, while the remaining words have very low uncertainty, which sentence is more reliable? The average strategy in this paper may not be the best approach and there may also be better methods to measure the relationship between tokens and sentences.

\textbf{More Modeling Techniques.}\quad Here, we model LogTokU according to logits and the Dirichlet distribution. Other strategies are also optional, such as using energy-based modeling to analyze logits or measuring EU and AU by observing network characteristics, such as the performance of attention.

\textbf{More Applications for Utilizing LogTokU.}\quad In this paper, we present two usage cases and highlight the potential to explore more usage scenarios. For example, we can fine-tune the model based on EU to inject missing knowledge, or improve the model's performance by using the Retrieval Augmented Generation (RAG) strategy to incorporate relevant knowledge as context.

\section{Limitations}

LogTokU demonstrates great potential and may even open up a new space for exploration. However, we must acknowledge that LogTokU has some unavoidable limitations:

\textbf{(1) It cannot estimate the uncertainty of black-box models.}\quad Compared to uncertainty estimation methods that rely on activation states and attention layers, LogTokU requires only logits and can be considered a gray-box uncertainty estimation approach. However, we note that most commercial models do not provide logits as output. Therefore, how to evaluate the uncertainty of black-box LLMs remains an issue for future research.

\textbf{(2) It cannot estimate the uncertainty of the distilled models.}\quad  Distilled models are optimized by using the output probability distribution of a larger model as the learning target, and they often rescale logits at each layer. As a result, small models trained through distillation lose the strength of evidence in their logits. In this case, since logits do not carry evidence strength information, LogTokU cannot be used for uncertainty modeling.

\clearpage
% \twocolumn

\section{Theoretical Proof of Why Logits can Capture Epistemic Uncertainty}

\begin{tcolorbox}[colback=gray!10,%gray background
                  colframe=black,% black frame color
                  width=\linewidth,% Use 8cm total width,
                  arc=1mm, auto outer arc,
                  boxrule=0.5pt,
                 ]
% \vspace{-8pt}
\begin{theorem}
For any LLM \(\mathcal{M}\) trained with the cross-entropy loss \(L_{\text{CE}}\) using gradient descent optimization (i.e., \(\nabla_\mathcal{M} L_{\text{CE}}\)), the total evidence \( \sum_{\tau^i \in \mathcal{T}} z_{\tau^i} \) will strictly accumulate (i.e., \( \Delta\sum_{\tau^i \in \mathcal{T}} z_{\tau^i}>0 \)), thus the epistemic uncertainty defined in Eq.~\ref{eq:eu} will strictly decrease (i.e., \( \Delta EU \leq 0 \)).
\label{thm:main}
\end{theorem}
\end{tcolorbox}


\textit{Proof.}
Let \( z_{\tau^1}, z_{\tau^2}, \dots, z_{\tau^{|\bm{Y}|}} \) denote the logits corresponding to the classes \( \bm{Y}=\{\tau^1,\tau^2,\cdots,\tau^{|\bm{Y}|}\} \). The softmax function computes the class probabilities \( p_{\tau^1}, p_{\tau^2}, \dots, p_{\tau^{|\bm{Y}|}} \) as follows:

\[
p_{\tau^i} = \frac{e^{z_{\tau^i}}}{\sum_{j=1}^{|\bm{Y}|} e^{z_{\tau^j}}}.
\]

Let \( y \) represent the true label of the input, encoded as a one-hot vector. The cross-entropy loss \( L \) for a single training example is given by:

\[
L_{\text{CE}} = -\sum_{i=1}^{|\bm{Y}|} y_{\tau^i} \log(p_{\tau^i}).
\]

Since \( y \) is one-hot encoded, only the term corresponding to the correct class \( k \) is non-zero. Thus, the loss simplifies to:

\[
L_{\text{CE}} = -\log(p_{\tau^k}).
\]

Substituting \( p_{\tau^k} \) with its expression from the softmax function yields:

\[
L_{\text{CE}} = -\log\left(\frac{e^{z_{\tau^k}}}{\sum_{j=1}^{|\bm{Y}|} e^{z_{\tau^j}}}\right).
\]

To analyze the effect of gradient updates, we compute the gradient of the loss \( L \) with respect to the logits \( z_i \). We consider two cases: (1) \( \tau^i = \tau^k \) (the correct class) and (2) \( \tau^i \neq \tau^k \) (incorrect classes).

\textit{1. Gradient for the correct class (\( \tau^i = \tau^k \)):}

\[
\frac{\partial L}{\partial z_{\tau^k}} = -\left(1 - p_{\tau^k}\right).
\]

\textit{2. Gradient for incorrect classes (\( \tau^i \neq \tau^k \)):}

\[
\frac{\partial L}{\partial z_{\tau^i}} = p_{\tau^i}.
\]

These gradients describe how the loss changes with respect to the logits. Using gradient descent with a learning rate \( \eta \), the update rules for the logits are as follows:

\textit{1. Update for the correct class (\( {\tau^i} = {\tau^k} \)):}

\[
z_{\tau^k} := z_{\tau^k} + \eta (1 - p_{\tau^k}).
\]

\textit{2. Update for incorrect classes (\( {\tau^i} \neq {\tau^k} \)):}

\[
z_{\tau^i} := z_{\tau^i} - \eta p_{\tau^i}.
\]

These updates ensure that the logit for the correct class increases, while the logits for incorrect classes decrease. 

Next, we analyze the change in the total evidence, defined as the sum of the $k$-largest logits (defined in Eq.~\ref{eq:evidence}). Let \( \mathcal{T} \) represent the set of classes corresponding to the top predicted classes (\(\tau^k \in \mathcal{T}\)). For any optimization step, the change in the sum of logits is given by:

\[
\Delta\sum_{\tau^i \in \mathcal{T}} z_{\tau^i} := \Delta z_{\tau^k} + \sum_{\tau^i \in \mathcal{T}, \tau^i \neq \tau^k} \Delta z_{\tau^i}.
\]

Substituting the update rules, we obtain:

\begin{equation}
\begin{aligned}
\Delta\sum_{\tau^i \in \mathcal{T}} z_{\tau^i} & = \eta(1 - p_{\tau^k}) + \sum_{\tau^i \in \textcolor{brown}{\mathcal{T}}, \tau^i \neq \tau^k} (-\eta p_{\tau^i}) \\ & \textcolor{red}{\bm{\geq}} ~\eta(1 - p_{\tau^k}) + \sum_{\tau^i \in \textcolor{brown}{\bm{Y}}, \tau^i \neq \tau^k} (-\eta p_{\tau^i}) \\ &= \eta(1 - p_{\tau^k}) - \eta(1 - p_{\tau^k})
\\&= 0.    
\end{aligned}    
\end{equation}

Since \( \mathcal{T} \) is a subset of the full set of classes \( \bm{Y} \), and \( |\mathcal{T}| \leq |\bm{Y}| \), the inequality \( \Delta\sum_{\tau^i \in \mathcal{T}} z_{\tau^i} \geq 0 \) holds. This implies that the total evidence increases, leading to a decrease in epistemic uncertainty, i.e., \( \Delta EU \leq 0 \). 

Thus, the theorem is proved.




% \textbf{Generalized Theorem:}



% \textit{Case 1: Under Label Smoothing}

% Label smoothing modifies the true label distribution \( y \) by mixing it with a uniform distribution over all classes. Let \( \alpha \in [0, 1] \) be the smoothing parameter. The smoothed label distribution \( \tilde{y} \) is defined as:

% \[
% \tilde{y}_{\tau^i} = (1 - \alpha) y_{\tau^i} + \frac{\alpha}{|\bm{Y}|},
% \]

% where \( y_{\tau^i} \) is the original one-hot encoded label. The cross-entropy loss with label smoothing becomes:

% \[
% L_{\text{smooth}} = -\sum_{i=1}^{|\bm{Y}|} \tilde{y}_{\tau^i} \log(p_{\tau^i}).
% \]

% Substituting \( \tilde{y}_{\tau^i} \), the loss can be rewritten as:

% \[
% L_{\text{smooth}} = -(1 - \alpha) \log(p_{\tau^k}) - \frac{\alpha}{|\bm{Y}|} \sum_{i=1}^{|\bm{Y}|} \log(p_{\tau^i}).
% \]


% \textit{1. Update for the correct class (\( \tau^i = \tau^k \)):}

% \[
% z_{\tau^k} := z_{\tau^k} + \eta \left[(1 - \alpha)(1 - p_{\tau^k}) + \frac{\alpha}{|\bm{Y}|} p_{\tau^k}\right].
% \]

% \textit{2. Update for incorrect classes (\( \tau^i \neq \tau^k \)):}

% \[
% z_{\tau^i} := z_{\tau^i} - \eta \left[\frac{\alpha}{|\bm{Y}|} (1 - p_{\tau^i}) + (1 - \alpha) p_{\tau^i}\right].
% \]

% The updates ensure that the logit for the correct class increases, while the logits for incorrect classes decrease. The smoothing term \( \alpha \) distributes some of the gradient updates uniformly across all classes, but the dominant effect remains the increase in evidence for the correct class. Thus, the total evidence \( \sum_{\tau^i \in \mathcal{T}} z_{\tau^i} \) still increases, and epistemic uncertainty decreases.



% \textit{Case 2: Under \(l_n\) Regularization}

% \(l_n\) regularization adds a penalty term to the loss function to control the magnitude of the logits. The regularized loss is given by:

% \[
% L_{\text{reg}} = L + \lambda \| \bm{z} \|_n^n,
% \]

% where \( \lambda > 0 \) is the regularization strength, and \( \| \bm{z} \|_n^n = \sum_{i=1}^{|\bm{Y}|} |z_{\tau^i}|^n \) is the \(l_n\) norm of the logits.


% \textit{1. Update for the correct class (\( \tau^i = \tau^k \)):}

% \[
% z_{\tau^k} := z_{\tau^k} + \eta \left[(1 - p_{\tau^k}) - \lambda n |z_{\tau^k}|^{n-1} \cdot \text{sign}(z_{\tau^k})\right].
% \]

% \textit{2. Update for incorrect classes (\( \tau^i \neq \tau^k \)):}

% \[
% z_{\tau^i} := z_{\tau^i} - \eta \left[p_{\tau^i} + \lambda n |z_{\tau^i}|^{n-1} \cdot \text{sign}(z_{\tau^i})\right].
% \]


% The \(l_n\) regularization term penalizes large logits, encouraging smaller magnitudes. However, the gradient updates for the correct class still dominate, as the term \( (1 - p_{\tau^k}) \) ensures an increase in the logit for the correct class. For incorrect classes, the regularization term further suppresses their logits. Thus, the total evidence \( \sum_{\tau^i \in \mathcal{T}} z_{\tau^i} \) increases, and epistemic uncertainty decreases.

