\section{Related Work}

\textbf{Sampling-based uncertainty estimation}.\quad  
These methods evaluate the randomness in the LLM generation process. Specifically, they allow LLMs to guess multiple times and evaluate their consistency. \cite{selectivelyanswering2023} introduce repetition and diversity into the measurement of consistency.
% Repetition is calculated as the fraction of times the sampled outputs match the greedy output, while diversity is inversely proportional to the number of distinct samples, with a value of zero assigned if all samples are different.  
% \cite{huang2024uncttp} treat the quantification of uncertainty in the LLM as a binary problem, where the LLM inconsistency is considered uncertain and vice versa certain after a definite number of samplings.  
\cite{zhang2024sac3reliable} introduce a mechanism that perturbs semantically equivalent questions to evaluate the consistency of LLM responses across variants of the same question.  
\cite{huang2024uncttp} inject correct and incorrect labels into the prompt during sampling, and the uncertainty level is defined based on whether the LLM's responses remain consistent across three samplings for each instance.  
A more continuous measurement of uncertainty is based on similarity. A recent method proposed by \cite{lin2024generating} calculates the similarity between multiple responses to indirectly quantify the dispersion of the model outputs.  
The most representative sampling-based method is Semantic Entropy (SE)~\cite{kuhn2023semantic}, which improves token-level measures by clustering sentences into equivalence classes based on their semantic similarity and computing the entropy over these classes. However, sampling-based methods can not evaluate the model's inherent uncertainty and are costly. Specifically, sampling-based methods only measure the consistency of multiple guesses but fail to identify whether the LLM lacks knowledge about the question.

\textbf{Probability-based uncertainty estimation}.\quad  
Probability-based uncertainty estimation methods can also be described as deterministic methods. These methods calculate the confidence (uncertainty) of the model based on the probability distribution of the prediction~\cite{arora2021prob2}. In addition to maximum probability, entropy is another common measure for estimating uncertainty. \cite{kadavath2022language} use the probability of the complete sequence to compute predictive entropy for assessing the sharpness of the output distributions. However, not all tokens in a sentence are equally critical~\cite{lin2024critical}. Recent work by \cite{duan2024shifting} highlights that not all tokens contribute equally to the underlying meaning, as linguistic redundancy often allows a few key tokens to capture the essence of longer sentences. However, even when focusing only on key tokens, these probability-based methods still cannot estimate the reliability of the answer generated by LLM. As shown in Fig.~\ref{fig:probability}, the key tokens do not yet provide a reliable estimation of the risk.


