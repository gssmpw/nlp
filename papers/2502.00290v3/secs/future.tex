\section{Theoretical Analysis}\label{sec:theoretical}

\textbf{Different correct answers are competitor.}\quad For any LLM trained with cross-entropy loss, different correct answers are competitors in terms of probability \footnote{The ``same question'' refers to questions that are semantically equivalent but do not need to be identical.}. Continuing with the example of proposing a president, suppose $\tau^{a}$ (``\texttt{Barack}'') is the label of a sample whose $\bm{q}$ is ``\texttt{[INST]Could you give me one name of president?[\textbackslash INST]}'' and a generated token vector $\bm{a}_{t-1}$  can be decoded into ``\texttt{Sure, here is a historical American president:**}'', the loss of the next token at this position during supervised fine-tuning can be written as:
\begin{equation}
\begin{aligned}
 &L^{\tau^a} = - \log \frac{\exp(\mathcal{M}({\tau^a}|\bm{q},\bm{a}_{t-1}))}{\sum_{m=1}^{|\bm{Y}|} \exp(\mathcal{M}(\tau^{m}|\bm{q},\bm{a}_{t-1}))} ,
 % \\   &L^{\tau^b} = - \log \frac{\exp(\mathcal{M}(\tau^b|\bm{q},\bm{a}_{t-1}))}{\sum_{m=1}^{|\bm{Y}|} \exp(\mathcal{M}(\tau^{m}|\bm{q},\bm{a}_{t-1}))} ,
\end{aligned}
\end{equation}
where $L^{\tau^a}$ is the loss on the sample with the next token label $\tau^{a}$.
Consider cases where multiple distinct answers to the same question appear in the training set, the situation becomes different. For example, $\tau^{b}$ (``\texttt{George}'') is the label in another sample with the same question. When the model is simultaneously fine-tuned on both samples, the gradient update for the model will be:
\begin{equation}
\begin{aligned}
 & \nabla_{\mathcal{M}} (L^{\tau^a} + L^{\tau^b}) = \nabla_{\mathcal{M}} L^{\tau^a} + \nabla_{\mathcal{M}} L^{\tau^b} \\
% &= -y_a^{\tau^a}\frac{1}{\Omega_a^{\tau^a}}\nabla_{\mathcal{M}}\Omega_a^{\tau^a}-\sum_{m \neq a}^{|\bm{Y}|} y_a^{\tau^m}\frac{1}{\Omega_a^{\tau^m}}\nabla_{\mathcal{M}}\Omega_a^{\tau^m}
% \\
% &\quad -y_b^{\tau^b}\frac{1}{\Omega_b^{\tau^b}}\nabla_{\mathcal{M}}\Omega_b^{\tau^b}-\sum_{m \neq b}^{|\bm{Y}|} y_b^{\tau^m}\frac{1}{\Omega_b^{\tau^m}}\nabla_{\mathcal{M}}\Omega_b^{\tau^m}
% \\
&\quad= \underbrace{-y_a^{\tau^a}\frac{1}{\Omega_a^{\tau^a}}\nabla_{\mathcal{M}}\Omega_a^{\tau^a}-y_b^{\tau^b}\frac{1}{\Omega_b^{\tau^b}}\nabla_{\mathcal{M}}\Omega_b^{\tau^b}}_{\text{(1) maximizing the probability of annotated answer}}\\& \quad \underbrace{-y_a^{\tau^b}\frac{1}{\Omega_a^{\tau^b}}\nabla_{\mathcal{M}}\Omega_a^{\tau^b}-y_b^{\tau^a}\frac{1}{\Omega_b^{\tau^a}}\nabla_{\mathcal{M}}\Omega_b^{\tau^a}}_{{\text{\textbf{(2)} minimizing the probability of the other annotated answer}}}\\& \quad \underbrace{-\sum_{m \neq a,b}^{|\bm{Y}|}y_{a,b}^{\tau^m} \left[ \frac{1}{\Omega_a^{\tau^m}}\nabla_{\mathcal{M}}\Omega_a^{\tau^m} + \frac{1}{\Omega_b^{\tau^m}}\nabla_{\mathcal{M}}\Omega_b^{\tau^m} \right]}_{\text{(3) minimizing the probability of incorrect answers}},
\end{aligned}\label{eq:competitor}
\end{equation}
where $\Omega_a^{\tau^a}=\frac{\exp(\mathcal{M}(\tau^a|\bm{q},\bm{a}_{t-1}))}{\sum_{m=1}^{|\bm{Y}|} \exp(\mathcal{M}(\tau^{m}|\bm{q},\bm{a}_{t-1}))}$, and $y_a^{\tau^m}$ indicates the next token label of a training sample with ground-truth label ${\tau^a}$, that is, we have $y_a^{\tau^a}=1$ and $y_a^{\tau^b}=0$. In particular, when $\mathcal{M}$ is in a certain state during training, we have $\Omega_a^{\tau^a}=\Omega_b^{\tau^a}$, and we make distinctions to facilitate the reader's understanding here. As we can see, for scenarios with multiple answers, the training objective can be divided into three parts:
(1) For each sample, increase the probability of its own annotation in the output distribution.
(2) For each sample, decrease the probability of another sample's annotation in the output distribution. \textit{\textbf{Note:}} This part leads to the issue where probability cannot anymore capture the reliability of LLM responses, as different correct answers tend to reduce the probability of other correct answers, making low probabilities cannot indicates low reliability.
(3) For both samples, decrease the probability of other outputs not present in the annotations in the output distribution.



