
\section{Introduction}
Over the past few years, Large Language Models (LLMs) have developed rapidly and LLM-driven systems are deployed across various domains. Despite their remarkable performance, LLMs remain prone to hallucinations~\cite{banerjee2024llms}, which causes them to generate unreliable responses when the models lack the corresponding knowledge. Hallucinations critically undermine the reliability of LLMs, particularly in professional applications such as medical and legal consultations~\cite{shah2024accuracy,dahl2024large}. Hallucinations have been considered as a major barrier to the broader deployment of LLMs~\cite{huang2023survey,liu2024exploring,perkovic2024hallucinations,zhou2024larger}.

\begin{figure*}
\centering 
\subfigure[Current probability-based methods fail in estimating response reliability.]{ \includegraphics[width=0.61\textwidth]{figs/probability.pdf} \label{fig:probability}
} \hfill 
\subfigure[Comparison of logits and probability.]{ \includegraphics[width=0.32\textwidth]{figs/softmax.pdf} 
\label{fig:softmax} }
\caption{\textbf{Why probability-based methods fail?} \emph{Left}: A pair of examples on LLaMA-2 shows that probability fails in estimating reliability. Since LLMs know the names of many presidents, the probability after normalization is very low; whereas for the future of the universe, since LLMs only know one hypothesis, the probability is very high. The probability-based reliability measure is counterintuitive, as the answers on common sense questions, where LLMs have rich knowledge, are less reliable than on unsolved physics problems. This is because probability cannot reflect whether a low probability is due to LLMs knowing multiple correct answers. These two cases are well characterized in this paper, corresponding to the fourth and second quadrants of Fig.~\ref{fig:cover1}, respectively. \emph{Right}: Normalization leads to the loss of evidence strength information.} 
\end{figure*}




Uncertainty estimation has shown promise in the identification of hallucinations in LLMs~\cite{xiao2021hallucination,huang2024survey}. High uncertainty often indicates the need for caution from users, as it suggests that the model is likely to be influenced by hallucinations~\cite{zhang2023enhancing,yoffe2024debunc}. In other words, high uncertainty signals unreliable responses. However, existing methods for uncertainty estimation in LLMs have limitations in estimating the inherent uncertainty of LLMs and lack efficiency. Including discussions of various heuristic methods, such as self-reflection~\cite{ji2023towards}, LLM uncertainty estimation can be categorized into sampling-based and token-based methods. Sampling-based methods estimate uncertainty by multiple sampling~\cite{liu2024exploring}, perturbations~\cite{zhang2024sac3reliable}, or contrastive analysis~\cite{huang2024uncttp}. Semantic Entropy (SE)~\cite{kuhn2023semantic} is a representative technique among these sampling-based methods. The core of these methods is estimating the consistency of LLM's multiple guesses, and the response will be marked as unreliable if inconsistency is detected. However, sampling-based methods suffer from several limitations: (1) it cannot assess the reliability of a single response; (2) it requires multiple sampling iterations, making it inefficient and impractical for deployment in real-world applications; and (3) it fails to account for the model's inherent uncertainty, such as consistently incorrect responses due to a lack of knowledge. 

Token-based methods can estimate the uncertainty of a single sentence without requiring multiple samplings as deterministic approaches~\cite{gupta2024language,fadeeva2024fact}. However, due to the lack of effective token-level uncertainty estimation, these methods often fail to achieve satisfactory results. For example, many tokens are correct but exhibit low probabilities (high entropy) in the model response.
% The limitations of probability and entropy force researchers to \textcolor{red}{apply complex transformations to them to reduce their impact on response uncertainty estimation}~\cite{gupta2024language,fadeeva2024fact}.
Many works have pointed out that the importance of different tokens in a sentence is not uniform~\cite{lin2024critical,duan2024shifting}, and the reliability of the model response depends mainly on a few critical tokens~\cite{duan2024gtbench,bigelow2024forking}, so they only focus on critical tokens when estimating reliability. However, as shown in Fig.~\ref{fig:probability}, although both ``\texttt{Barack Hussein Obama}'' and ``\texttt{Abraham Lincoln}'' could be correct answers, their maximum probability of the critical token (first token of their names) is only $0.377$, which indicates a very high uncertainty. Therefore, to obtain an LLM uncertainty estimation that is both effective and efficient, a reliable token-level uncertainty estimation method is urgently required.


In this paper, we find that the reason why the current token uncertainty estimation fails is that probability no longer captures the reliability of LLM responses. Specifically, probability can only reflect the relative strength relationship between different categories from the perspective of a discriminative model. However, generative LLMs are different from traditional discriminative models, and there may be more than one token that can be the suitable next-token. Therefore, measuring the probability of a single suitable candidate cannot capture the reliability of the model response. To capture the reliability of generating the next token, we need to assess whether the model has the knowledge to generate the next token, which can be referred to as the model's inherent uncertainty. In this paper, we reveal that the strength of evidence accumulation is lost during normalization, leading to the failure of probability-based reliability estimation. Therefore, we present a new uncertainty estimation framework based on logits named \textbf{Log}its-induced \textbf{Tok}en \textbf{U}ncertainty (LogTokU), which achieves both effectiveness and efficiency. Specifically, LogTokU introduces a more clear token-level uncertainty estimation, decoupling uncertainty into tokens relative aleatoric uncertainty (AU) and model inherent epistemic uncertainty (EU). As illustrated in Fig.~\ref{fig:cover1}, distinct combinations of these two types of uncertainty are treated as separate cases. Compared to probability-based uncertainty modeling, LogTokU can not only express states of ``I am sure'' and ``I do not know'', but it can also express ``Lack knowledge, but I suggest'' and ``I know more than one answer''. Moreover, this framework enables the estimation of real-time reliability of any response without the need for sampling, ensuring high efficiency. To realize the LogTokU framework, we adapt evidence modeling~\cite{sensoy2018evidential} in this paper, treating logits as parameters of a Dirichlet distribution to characterize aleatoric uncertainty and epistemic uncertainty. Then, we validate the reliability of the estimated uncertainty by using it to guide downstream tasks.
The main contributions of this paper can be summarized as: (1) We reveal why probability-based strategies fail and introduce \textbf{a new framework} to estimate the token uncertainty of LLMs, which shows that the token uncertainty is a promising way to estimate response reliability. (2) By employing evidential learning to estimate aleatoric uncertainty and epistemic uncertainty, we provide \textbf{a viable implementation} of the LogTokU framework. 
% \item To fill the gap in the LLM evaluation field regarding the lack of epistemic uncertainty estimation paradigm, we propose \textbf{a new evaluation paradigm} to estimate the reliability of LLM's EU estimation.
(3) We demonstrate \textbf{two usage cases} for the estimated uncertainty, thus validating its effectiveness and showcasing the significant potential of LogTokU. 




\begin{figure*}
\centering 
\subfigure[Logits-induced token uncertainty (LogTokU).]{ \includegraphics[width=0.43\textwidth]{figs/evidential.pdf} \label{fig:cover1}
} \hfill 
\subfigure[An example dialogue on LLaMA-2-Chat.]{ \includegraphics[width=0.51\textwidth]{figs/case.pdf} 
\label{fig:cover2} }
\caption{\textbf{Why LogTokU works?} \emph{Left}: Illustration of four different scenarios considered in LogTokU, where the gray bars represent the logits for predicting the next token, the triangular patterns represent the corresponding Dirichlet distribution, and the table below compares uncertainty estimation using probability with that using LogTokU. \emph{Right}: A case study from a medical QA, where the markings under each word reflect reliability estimated according to LogTokU, as well as the values of AU (gray) and EU (blue). \textbf{I}: Both AU and EU are high, where LLaMA recommends a metal ``\texttt{Chromium}'' for diabetes patients. \textbf{II}: The total logits are low, but one token's logit is larger than the others, indicating that the LLM lacks experience and knowledge but knows what should be the next token, where the LLM repeats the medicine ``\texttt{Glucomannan}'' that has been generated in the previous context. \textbf{III}: The LLM is very confident about the next token, where it generates the fixed phrase ``has \texttt{been}''. \textbf{IV}: The LLM has enough knowledge and knows more than one suitable answer. For example, the LLM generates ``\texttt{[comma]}'', which can be replaced by many other suitable words. The dilemma in Fig. 1 is addressed according to quadrant II and IV.} \label{fig:two_subfigures}
\end{figure*}

