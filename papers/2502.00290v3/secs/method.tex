\section{Logits-induced Token Uncertainty}

\subsection{Notations}
Consider that given a pre-trained LLM noted as $\mathcal{M}$ and its corresponding tokenizer dictionary $\bm{Y}=\{\tau^1,\tau^2,\cdots,\tau^{|\bm{Y}|}\}$, and $|\bm{Y}|$ indicates the size of the vocabulary dictionary. Specifically, the user inputs an instruction, then the instruction is transformed into a prompt by applying a chat template (for example, ``\texttt{[INST]Could you give me one name of president?[\textbackslash INST]}''. The prompt is encoded by the corresponding tokenizer as a vector $\bm{q}$ and input into LLM to perform the next token prediction under certain basic sampling strategies. The model continuously generates the next token $a_{t}$ based on the $\bm{q}$ and tokens that have been generated $\bm{a}_{t-1}=a_{1}a_{2}\cdots a_{t-1}$ (for example, $\bm{a}_{t-1}$ is a generated token vector can be decoded into ``\texttt{Sure, here is a historical American president:**}'') until they meet the stop rule (for example, meeting \texttt{[EOS]}), which can be formulated as:
\begin{equation}
\begin{aligned}
    \textbf{P}(\bm{Y}|\bm{q},&\bm{a}_{t-1},\mathcal{M}) = \{ p(\tau^{m}|\bm{q},\bm{a}_{t-1},\mathcal{M})\}_{m=1}^{|Y|}\\=&\left\{\frac{\exp(\mathcal{M}(\tau^{m}|\bm{q},\bm{a}_{t-1}))}{\sum_{j=1}^{|\bm{Y}|}\exp(\mathcal{M}(\tau^{j}|\bm{q},\bm{a}_{t-1}))}\right\}_{m=1}^{|\bm{Y}|},      
\end{aligned}
\end{equation}
where $\mathcal{M}(\tau^{m}|\bm{q},\bm{a}_{t-1})$ indicates the predicted logit (score before softmax layer) of $\tau^{m}$. Then the prediction token $a_{t}$ (for example, $a_{t}$ can be ``\texttt{Barack}''\footnote{The names generated here may consist of more than one token; for instance, generating ``Barack'' results in the next token being ``Bar''. For ease of understanding, we will directly represent the next token as the corresponding complete word in this paper.}, ``\texttt{George}'' and other tokens) will be sampled for the distribution of $\textbf{P}(\bm{Y}|\bm{q},\bm{a}_{t-1},\mathcal{M})$:
\begin{equation}
    a_t \sim \textbf{P}(\bm{Y}|\bm{q},\bm{a}_{t-1},\mathcal{M}),
\end{equation}
where $a_t \in \bm{Y}$, and the sampling probability satisfies $p(a_t=\tau^{m})={p}(\tau^{m}|\bm{q},\bm{a}_{t-1},\mathcal{M})$ (e.g., the probability of next token being ``\texttt{Barack}'' is $0.377$ as shown in Fig.~\ref{fig:probability}).
% we note $p(a_k=\tau^{m})$ as $p(\tau^{m})$ for convenient.

\subsection{Failures in Traditional Uncertainty Estimation}



Traditional discriminative models typically use probability to estimate reliability. Probability, in fact, is a normalization of the strength of evidence for different categories. Due to the \emph{mutually exclusive} properties of the different categories, the relative strength between different categories can accurately indicate the reliability of the prediction. However, in the case of LLMs, the situation is different. Although LLM's next token prediction can still be viewed as a classification task with $|\bm{Y}|$ categories, these categories are no longer mutually exclusive. Even different tokens that are mutually exclusive in a conversation may no longer be mutually exclusive in a different context. This is why recent LLM research suggests shifting from token-to-token training to a concept-to-concept training paradigm. There may be more than one suitable next token, so the relative relationship between different tokens to no longer reflect the reliability of the response. Therefore, the information of the strength of the evidence before normalization becomes important. During the LLM training process, the suitable token accumulates evidence (logits increase, similar to the discriminative model~\cite{wei2022mitigating}), specifically, the higher the strength of the evidence, the more similar scenarios the model has encountered during training.

Consider the two situations shown in Fig.~\ref{fig:softmax}: (1) left: LLM has encountered this question 3 times during training, with answers a, b, and c; (2) right: LLM has encountered this question 3,000 times during training, and the answers can be summarized into three situations, a, b, and c. 

The accumulation of evidence in these two scenarios is significantly different, and their reliability is completely different. However, after logits are normalized into probabilities, the strength information of the evidence is lost, so in probability-based uncertainty estimation, these two situations are considered the same. This is because probability loses the key information that can indicate the reliability of the response.

\subsection{LogTokU: Four-quadrant Framework}\label{sec:four}

The reason why probability-based methods fail to identify reliability is that probability is normalized. After logits are normalized, only the relative strength between different answers (should be either ``\texttt{Barack}'' or ``\texttt{George}'') is retained, while the original strength information of the logits is discarded, which results in the loss of the ability to indicate reliability (distinguish between ``I do not know'' and ``I know more than one answer''). To address this limitation, we propose a Logits-induced Token Uncertainty framework termed LogTokU. In addition to considering the relative relationships among tokens (AU), LogTokU also takes into account the strength of the model's response (EU). With the information of EU, ``I do not know'' and ``I know more than one answer'' can be characterized separately. As shown in Fig.~\ref{fig:cover1}, the four quadrants of uncertainty are described as:

\textbf{Quadrant I: high AU, high EU}.\quad
This quadrant indicates that the model exhibits a low strength of evidence for all tokens, potentially due to lack of relevant knowledge. For example, as shown in Fig.~\ref{fig:cover2}, the model might recommend an unfamiliar medication.

\textbf{Quadrant II: low AU, high EU}.\quad
In this quadrant, the model shows a low evidence strength for most tokens but there is a higher strength for one particular token, indicating a lack of diversity despite producing a relatively high probability token. For example, the model repetitively suggests a drug name that was recently mentioned.
\emph{\textbf{Failure of probability-based methods:} Probability-based methods may regard this quadrant as highly reliable. However, it still involves a certain degree of risk due to the lack of knowledge, and the high probability only indicates the model recommendation.}

\textbf{Quadrant III: low AU, low EU}.\quad
Here, the model exhibits very high strength for one specific token while maintaining a low strength of evidence for all other tokens. This reflects a strong certainty about a particular token, such as the fixed phrase ``has been''.

\textbf{Quadrant IV: high AU, low EU}.\quad
This quadrant indicates that the model assigns a high strength of evidence to multiple tokens. Although none of them achieves a high probability, these tokens collectively demonstrate strong evidence. For example, this situation might arise when predicting nouns or punctuation marks that can be expressed in multiple valid ways.
\emph{\textbf{Failure of probability-based methods:} Probability-based methods may interpret this quadrant as unreliable. However, the model intends to express that there is more than one suitable candidate for the next token.}

 
\subsection{Considering Logits as Evidence}
In this subsection, we present a viable implementation of the LogTokU framework, specifically by modeling AU and EU using a Dirichlet distribution. Inspired by Evidential Deep Learning~\cite{sensoy2018evidential}, we treat logits as evidence for each token and model them into a Dirichlet distribution. A naive approach is using the logits of all non-negative values as evidence, while setting those with negative values to no evidence by applying the ReLU activation function. However, unlike conventional classification networks, the number of candidates generated by an LLM (that is, tokenizer size) is significantly large, with a considerable proportion of tokens having extremely low logits, and these tokens should be discarded as noise~\cite{tang2024top}. Therefore, this paper focuses on the distribution of main candidates with high logits. Specifically, we sample the top $K$ tokens with the largest logits to model a Dirichlet distribution:
\begin{equation}
    \alpha_k=\mathcal{M}({{\tau_k}}|\bm{q},\bm{a}_{t-1}), \quad \alpha_{0}=\sum_{k=1}^{K}\alpha_k,
    \label{eq:evidence}
\end{equation}
where \(\tau_{k}\) is the token with the $k$-largest prediction logit, and $\alpha_0$ is the total evidence of the Dirichlet distribution (the sum of the largest $k$ logits).

\textbf{Aleatoric (data) uncertainty.}\quad To measure the data uncertainty, we evaluate the expected entropy of the data distribution. Since entropy captures the ``peakiness'' of the output distribution, a lower entropy indicates that the model concentrates most of the probability mass on a single class, while a higher entropy characterizes a more uniform distribution, indicating that the model is undecided about the prediction. For Dirichlet networks, this quantity has a closed-form solution:
\begin{equation}
    \text{AU}(a_t) = - \sum_{k=1}^K\frac{\alpha_k}{\alpha_0}\left(\psi(\alpha_k+1) -  \psi(\alpha_0+1)\right), 
\end{equation}
where $\psi$ denotes the digamma function, defined as $\psi(x) = \frac{d}{d x} \log \Gamma(x)$. 

\textbf{Epistemic (model) uncertainty.}\quad 
We define the EU by:
\begin{equation}
    \text{EU}(a_t) = K / \sum_{k=1}^K (\alpha_k + 1),\label{eq:eu}
\end{equation}
and the underlying intuition is that larger $\alpha_k$ produces a sharper density, and thus it indicates increased confidence in a prediction. For more information on Dirichlet distribution, please refer to the introduction from~\cite{ulmer2023prior}.


% logits到evidence建模    % sentence uncertainty
\subsection{Why Logits can Capture EU}


\textbf{Why evidential modeling can capture EU.} Due to there is no ground-truth of the EU, uncertainty-aware model training (such as evidential deep learning~\cite{sensoy2018evidential}) does not obtain the EU through direct alignment. Instead, it is typically achieved by assuming that all scenarios should intuitively have a high EU for the model, and the EU is gradually eliminated during the model's learning process. Therefore, there is a regularization constraint on the growth of the evidences (logits). This process consists of two parts: classification loss and regularization for constraining smaller logits. In other words, due to the trade-off between correct classification and logits regularization, logits are only allowed to be larger when the model is highly confident in its predictions, thereby characterizing the magnitude of the EU. Taking the training loss of typical evidential modeling ($\mathcal{L}_\text{EVD}$) as an example:
\begin{equation}
\begin{aligned}
    \mathcal{L}_\text{EVD} =\underbrace{-\left( \sum_{j}^{|\bm{Y}|} y_{j}\log \frac{z_{j}+1}{\sum_{j'}^{|\bm{Y}|} (z_{j'}+1)} \right)}_{\text{Evidential deep learning classification loss}} \underbrace{+ \mathcal{L}_{\text{reg}}}_{\text{logits constrainer}},
\end{aligned}
\end{equation}
where $z_j$ is the predictive logit of class $j$ and $\mathcal{L}_{\text{reg}}$ indicates the regularization for constraining the logits scale.

\textbf{Cross-entropy can be interpreted as evidential modeling.} Intuitively, the cross-entropy loss is not affected by the scale of logits; adding a constant to all elements of the logits vector does not change the probability values. For instance, both [20, 18] and [30, 28] are mapped to the probability distribution [0.88, 0.12] after applying the softmax function. However, intuition is not always trustworthy. During the actual gradient descent training of models, the scale of logits is constrained. In fact, as shown in Eq.~\ref{eq:loss_cp}, the cross-entropy loss can be decomposed into a format similar to the evidential deep learning loss function, which naturally penalizes shifting logits scales during gradient updates.








\begin{equation}
\begin{aligned}
    \mathcal{L}_\text{LLM} &= -\sum_{j}^{|\bm{Y}|}y_{j}\log p_{j} 
    \\&= -\sum_{j}^{|\bm{Y}|}y_{j}\log \frac{e^{z_{j}}}{\sum_{j'}^{|\bm{Y}|}e^{z_{j'}}} \\&= -\sum_{j}^{|\bm{Y}|}y_{j}\log \frac{e^{(z_{j}+1})}{\sum_{j'}^{|\bm{Y}|}e^{(z_{j'}+1})}  \\&= \underbrace{-\left( \sum_{j}^{|\bm{Y}|} y_{j}\log \frac{z_{j}+1}{\sum_{j'}^{|\bm{Y}|} (z_{j'}+1)} \right)}_{\text{Evidential deep learning classification loss}} \\&\quad - \sum_{j}^{|\bm{Y}|} y_{j}((z_{j}+1) - \log (z_{j}+1))  \\&\quad \underbrace{- \log \frac{\sum_{j'}^{|\bm{Y}|} (z_{j'}+1)}{\sum_{j'}^{|\bm{Y}|} e^{(z_{j'}+1)}} }_{\text{logits constrainer}},\label{eq:loss_cp}
\end{aligned}
\end{equation}

In other words, \textbf{absolute logits scale does not change probability distribution, but it affects optimization}, which is the theoretical reason why logits can be considered as evidence similar to uncertainty-aware modeling.

