@article{xu2023multimodal,
  title={Multimodal learning with transformers: A survey},
  author={Xu, Peng and Zhu, Xiatian and Clifton, David A},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={10},
  pages={12113--12132},
  year={2023},
  publisher={IEEE}
}

@article{chacko2024adversarial,
  title={Adversarial Attacks on Large Language Models Using Regularized Relaxation},
  author={Chacko, Samuel Jacob and Biswas, Sajib and Islam, Chashi Mahiul and Liza, Fatema Tabassum and Liu, Xiuwen},
  journal={arXiv preprint arXiv:2410.19160},
  year={2024}
}

@article{wallace2019universal,
  title={Universal adversarial triggers for attacking and analyzing NLP},
  author={Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
  journal={arXiv preprint arXiv:1908.07125},
  year={2019}
}

@inproceedings{wei2022towards,
  title={Towards transferable adversarial attacks on vision transformers},
  author={Wei, Zhipeng and Chen, Jingjing and Goldblum, Micah and Wu, Zuxuan and Goldstein, Tom and Jiang, Yu-Gang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={3},
  pages={2668--2676},
  year={2022}
}

@article{wang2024dual,
  title={Dual stage black-box adversarial attack against vision transformer},
  author={Wang, Fan and Shao, Mingwen and Meng, Lingzhuang and Liu, Fukang},
  journal={International Journal of Machine Learning and Cybernetics},
  pages={1--12},
  year={2024},
  publisher={Springer}
}

@article{baras2023quantattack,
  title={QuantAttack: Exploiting Dynamic Quantization to Attack Vision Transformers},
  author={Baras, Amit and Zolfi, Alon and Elovici, Yuval and Shabtai, Asaf},
  journal={arXiv preprint arXiv:2312.02220},
  year={2023}
}

@article{islam2024malicious,
  title={Malicious path manipulations via exploitation of representation vulnerabilities of vision-language navigation systems},
  author={Islam, Chashi Mahiul and Salman, Shaeke and Shams, Montasir and Liu, Xiuwen and Kumar, Piyush},
  journal={arXiv preprint arXiv:2407.07392},
  year={2024}
}


@article{vaswani2017attention,
  title={Attention is All You Need},
  author={Vaswani, Ashish},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{dong2018speech,
  title={Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition},
  author={Dong, Linhao and Xu, Shuang and Xu, Bo},
  booktitle={2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={5884--5888},
  year={2018},
  organization={IEEE}
}






@inproceedings{carlini2017adversarialexampleseasilydetected,
author = {Carlini, Nicholas and Wagner, David},
title = {Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods},
year = {2017},
isbn = {9781450352024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128572.3140444},
doi = {10.1145/3128572.3140444},
abstract = {Neural networks are known to be vulnerable to adversarial examples: inputs that are close to natural inputs but classified incorrectly. In order to better understand the space of adversarial examples, we survey ten recent proposals that are designed for detection and compare their efficacy. We show that all can be defeated by constructing new loss functions. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and the properties believed to be intrinsic to adversarial examples are in fact not. Finally, we propose several simple guidelines for evaluating future proposed defenses.},
booktitle = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
pages = {3â€“14},
numpages = {12},
location = {Dallas, Texas, USA},
series = {AISec '17}
}

@inproceedings{moosavi2016deepfool,
  title={Deepfool: a simple and accurate method to fool deep neural networks},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2574--2582},
  year={2016}
}

@incollection{kurakin2018adversarial,
  title={Adversarial examples in the physical world},
  author={Kurakin, Alexey and Goodfellow, Ian J and Bengio, Samy},
  booktitle={Artificial intelligence safety and security},
  pages={99--112},
  year={2018},
  publisher={Chapman and Hall/CRC}
}



@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  year={2021},
  organization={PMLR}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@inproceedings{carlini2017towards,
  title={Towards evaluating the robustness of neural networks},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={2017 ieee symposium on security and privacy (sp)},
  year={2017},
  organization={Ieee}
}

@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv preprint arXiv:1706.06083},
  year={2017}
}

@article{zou2023representation,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@article{salman2024intriguing,
  title={Intriguing Equivalence Structures of the Embedding Space of Vision Transformers},
  author={Salman, Shaeke and Shams, Md Montasir Bin and Liu, Xiuwen},
  journal={arXiv preprint arXiv:2401.15568},
  year={2024}
}

@article{vilas2024analyzing,
  title={Analyzing Vision Transformers for image classification in class embedding space},
  author={Vilas, Martina G and Schauml{\"o}ffel, Timothy and Roig, Gemma},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@inproceedings{liu2023understanding,
  title={Understanding and defending patched-based adversarial attacks for vision transformer},
  author={Liu, Liang and Guo, Yanan and Zhang, Youtao and Yang, Jun},
  booktitle={Proceedings of the 40th International Conference on Machine Learning},
  pages={21631--21657},
  year={2023}
}

@article{zhang2024towards,
  title={Towards General Conceptual Model Editing via Adversarial Representation Engineering},
  author={Zhang, Yihao and Wei, Zeming and Sun, Jun and Sun, Meng},
  journal={arXiv preprint arXiv:2404.13752},
  year={2024}
}

@inproceedings{biswas2022geometric,
  title={Geometric Analysis and Metric Learning of Instruction Embeddings},
  author={Biswas, Sajib and Barao, Timothy and Lazzari, John and McCoy, Jeret and Liu, Xiuwen and Kostandarithes, Alexander},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2022},
  organization={IEEE}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{papernot2016limitations,
  title={The limitations of deep learning in adversarial settings},
  author={Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z Berkay and Swami, Ananthram},
  booktitle={2016 IEEE European symposium on security and privacy (EuroS\&P)},
  pages={372--387},
  year={2016},
  organization={IEEE}
}


@article{naseer2021intriguing,
  title={Intriguing properties of vision transformers},
  author={Naseer, Muhammad Muzammal and Ranasinghe, Kanchana and Khan, Salman H and Hayat, Munawar and Shahbaz Khan, Fahad and Yang, Ming-Hsuan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23296--23308},
  year={2021}
}

@article{raghu2021vision,
  title={Do vision transformers see like convolutional neural networks?},
  author={Raghu, Maithra and Unterthiner, Thomas and Kornblith, Simon and Zhang, Chiyuan and Dosovitskiy, Alexey},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={12116--12128},
  year={2021}
}

@article{shao2021adversarial,
  title={On the adversarial robustness of vision transformers},
  author={Shao, Rulin and Shi, Zhouxing and Yi, Jinfeng and Chen, Pin-Yu and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:2103.15670},
  year={2021}
}

@article{mo2022adversarial,
  title={When adversarial training meets vision transformers: Recipes from training to architecture},
  author={Mo, Yichuan and Wu, Dongxian and Wang, Yifei and Guo, Yiwen and Wang, Yisen},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={18599--18611},
  year={2022}
}

@article{fu2022patch,
  title={Patch-fool: Are vision transformers always robust against adversarial perturbations?},
  author={Fu, Yonggan and Zhang, Shunyao and Wu, Shang and Wan, Cheng and Lin, Yingyan},
  journal={arXiv preprint arXiv:2203.08392},
  year={2022}
}

@inproceedings{kim2024exploring,
  title={Exploring Adversarial Robustness of Vision Transformers in the Spectral Perspective},
  author={Kim, Gihyun and Kim, Juyeop and Lee, Jong-Seok},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={3976--3985},
  year={2024}
}

@inproceedings{wang2022understanding,
  title={Understanding adversarial robustness of vision transformers via cauchy problem},
  author={Wang, Zheng and Ruan, Wenjie},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={562--577},
  year={2022},
  organization={Springer}
}

@article{yang2024robust,
  title={Robust contrastive language-image pretraining against data poisoning and backdoor attacks},
  author={Yang, Wenhan and Gao, Jingdong and Mirzasoleiman, Baharan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{zhang2023transferable,
  title={Transferable adversarial attacks on vision transformers with token gradient regularization},
  author={Zhang, Jianping and Huang, Yizhan and Wu, Weibin and Lyu, Michael R},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16415--16424},
  year={2023}
}

@article{rando2022exploring,
  title={Exploring adversarial attacks and defenses in vision transformers trained with DINO},
  author={Rando, Javier and Naimi, Nasib and Baumann, Thomas and Mathys, Max},
  journal={arXiv preprint arXiv:2206.06761},
  year={2022}
}

@inproceedings{li2023trade,
  title={Trade-off between robustness and accuracy of vision transformers},
  author={Li, Yanxi and Xu, Chang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7558--7568},
  year={2023}
}

@article{costa2024deep,
  title={How deep learning sees the world: A survey on adversarial attacks \& defenses},
  author={Costa, Joana C and Roxo, Tiago and Proen{\c{c}}a, Hugo and In{\'a}cio, Pedro RM},
  journal={IEEE Access},
  year={2024},
  publisher={IEEE}
}

@inproceedings{li2024harnessing,
  title={Harnessing Edge Information for Improved Robustness in Vision Transformers},
  author={Li, Yanxi and Du, Chengbin and Xu, Chang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  pages={3252--3260},
  year={2024}
}

@inproceedings{gong2024random,
  title={Random Entangled Tokens for Adversarially Robust Vision Transformer},
  author={Gong, Huihui and Dong, Minjing and Ma, Siqi and Camtepe, Seyit and Nepal, Surya and Xu, Chang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24554--24563},
  year={2024}
}

@inproceedings{prakash2018deflecting,
  title={Deflecting adversarial attacks with pixel deflection},
  author={Prakash, Aaditya and Moran, Nick and Garber, Solomon and DiLillo, Antonella and Storer, James},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={8571--8580},
  year={2018}
}

@inproceedings{cohen2019certified,
  title={Certified adversarial robustness via randomized smoothing},
  author={Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
  booktitle={international conference on machine learning},
  pages={1310--1320},
  year={2019},
  organization={PMLR}
}

@inproceedings{xiang2021patchguard,
  title={$\{$PatchGuard$\}$: A provably robust defense against adversarial patches via small receptive fields and masking},
  author={Xiang, Chong and Bhagoji, Arjun Nitin and Sehwag, Vikash and Mittal, Prateek},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2237--2254},
  year={2021}
}

@article{turner2023activation,
  title={Activation addition: Steering language models without optimization},
  author={Turner, Alex and Thiergart, Lisa and Udell, David and Leech, Gavin and Mini, Ulisse and MacDiarmid, Monte},
  journal={arXiv preprint arXiv:2308.10248},
  year={2023}
}

@article{liu2023context,
  title={In-context vectors: Making in context learning more effective and controllable through latent space steering},
  author={Liu, Sheng and Xing, Lei and Zou, James},
  journal={arXiv preprint arXiv:2311.06668},
  year={2023}
}

@inproceedings{papernot2016transferability,
  title={Transferability in Machine Learning: From Phenomena to Black-Box Attacks Using Adversarial Samples},
  author={Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian J.},
  booktitle={IEEE European Symposium on Security and Privacy (EuroS\&P)},
  pages={123--138},
  year={2016},
  publisher={IEEE}
}

@article{chen2017zoo,
  title={ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks Without Training Substitute Models},
  author={Chen, Jianxiong and Moreau, Thibault and Papernot, Nicolas},
  journal={arXiv preprint arXiv:1705.07183},
  year={2017},
  url={https://arxiv.org/abs/1705.07183}
}

@article{wei2024assessing,
  title={Assessing the brittleness of safety alignment via pruning and low-rank modifications},
  author={Wei, Boyi and Huang, Kaixuan and Huang, Yangsibo and Xie, Tinghao and Qi, Xiangyu and Xia, Mengzhou and Mittal, Prateek and Wang, Mengdi and Henderson, Peter},
  journal={arXiv preprint arXiv:2402.05162},
  year={2024}
}

@article{park2023linear,
  title={The linear representation hypothesis and the geometry of large language models},
  author={Park, Kiho and Choe, Yo Joong and Veitch, Victor},
  journal={arXiv preprint arXiv:2311.03658},
  year={2023}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@inproceedings{zheng2021rethinking,
  title={Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers},
  author={Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip HS and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6881--6890},
  year={2021}
}

@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European conference on computer vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}

@inproceedings{wang2019improving,
  title={Improving adversarial robustness requires revisiting misclassified examples},
  author={Wang, Yisen and Zou, Difan and Yi, Jinfeng and Bailey, James and Ma, Xingjun and Gu, Quanquan},
  booktitle={International conference on learning representations},
  year={2019}
}

@article{dar2022analyzing,
  title={Analyzing transformers in embedding space},
  author={Dar, Guy and Geva, Mor and Gupta, Ankit and Berant, Jonathan},
  journal={arXiv preprint arXiv:2209.02535},
  year={2022}
}

@article{geva2022transformer,
  title={Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space},
  author={Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2203.14680},
  year={2022}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@misc{imagenette,
  author    = "Jeremy Howard",
  title     = "imagenette",
  url       = "https://github.com/fastai/imagenette/",
  year      = {2019}
}

@article{krizhevsky2010cifar,
  title={Cifar-10 (canadian institute for advanced research)},
  author={Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey},
  journal={URL http://www. cs. toronto. edu/kriz/cifar. html},
  volume={5},
  number={4},
  pages={1},
  year={2010}
}