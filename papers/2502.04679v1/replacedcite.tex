\section{Related Work}
Transformers ____ have significantly advanced performance across multiple domains, including natural language processing ____, computer vision ____, and speech recognition ____, by efficiently capturing long-range dependencies through their self-attention mechanism and transformer architecture. Much of the ongoing research aims to refine Transformer models' internal representations ____, focusing on improving accuracy, robustness, and behavior control ____. These efforts have led to substantial improvements in model interpretability and performance across various tasks. The extension of Transformer models to visual tasks has led to the development of Vision Transformers (ViTs) ____, which have been analyzed for their ability to capture class-specific features ____ and their handling of input images within high-dimensional embedding spaces ____. Our work builds on this research to mechanistically explore how adversarial attacks influence the layer-wise representations in ViTs.

Despite these advancements, ViTs remain vulnerable to adversarial attacks, presenting significant challenges for their deployment in security-critical applications. Several attack methods, effective across various deep neural network models, have been adapted to target ViTs, including optimization-based methods ____, gradient-based techniques ____, and transfer attacks ____. Recent studies ____ have applied these techniques to investigate the adversarial robustness of ViTs, revealing unique vulnerabilities in their attention mechanisms and token-based processing. Notably, ____ demonstrated how vulnerabilities in vision-language models' embeddings can enable manipulation of navigation paths, while ____ leveraged ViTs' self-attention mechanisms for transferable attacks. Other works, such as ____, have explored dual-stage attacks for cross-architecture transferability, and ____ examined adversarial techniques specific to model quantization. Additionally, representation space-based techniques ____ have been proposed, including self-supervised learning for unlabeled data ____ and enhanced attack transferability using token gradient regularization ____. Novel optimization techniques, such as transferable triggers ____ and gradient-regularized relaxation ____, can enhance ViT attack methodologies, bridging insights across modalities.

Alongside adversarial attack research, significant focus has been on developing defenses to enhance model robustness without sacrificing accuracy ____, as maintaining both robustness and performance remains a key challenge. Adversarial training, introduced by ____, has been widely adopted in ViTs ____, with patch-based techniques ____ and edge information utilization ____ showing promising results in improving model resilience. Other defenses include random entangled tokens ____, prediction averaging over noisy inputs ____, and methods that reduce perturbation impact using small receptive fields and masking ____. Some strategies modify the representation space, such as robust representation sensors ____ or activation guidance ____, offering novel approaches to enhance model robustness. ____ assessed model vulnerability using parameter pruning and low-rank weight approximations, revealing fundamental insights into model brittleness and the trade-offs between model complexity and robustness. Our method differs by introducing neuron neutralization in ViTs to enhance adversarial robustness, without requiring adversarial training and outperforming traditional methods with a fraction of the dataset, providing a more efficient and effective approach to adversarial defense.