\section{Related Work}
Transformers \cite{vaswani2017attention} have significantly advanced performance across multiple domains, including natural language processing \cite{devlin2018bert}, computer vision \cite{dosovitskiy2020image}, and speech recognition \cite{dong2018speech}, by efficiently capturing long-range dependencies through their self-attention mechanism and transformer architecture. Much of the ongoing research aims to refine Transformer models' internal representations \cite{devlin2018bert, biswas2022geometric, zou2023representation, raghu2021vision}, focusing on improving accuracy, robustness, and behavior control \cite{zhang2024towards}. These efforts have led to substantial improvements in model interpretability and performance across various tasks. The extension of Transformer models to visual tasks has led to the development of Vision Transformers (ViTs) \cite{dosovitskiy2020image}, which have been analyzed for their ability to capture class-specific features \cite{vilas2024analyzing} and their handling of input images within high-dimensional embedding spaces \cite{salman2024intriguing}. Our work builds on this research to mechanistically explore how adversarial attacks influence the layer-wise representations in ViTs.

Despite these advancements, ViTs remain vulnerable to adversarial attacks, presenting significant challenges for their deployment in security-critical applications. Several attack methods, effective across various deep neural network models, have been adapted to target ViTs, including optimization-based methods \cite{carlini2017towards, carlini2017adversarialexampleseasilydetected, moosavi2016deepfool}, gradient-based techniques \cite{goodfellow2014explaining, madry2017towards, kurakin2018adversarial, papernot2016limitations}, and transfer attacks \cite{papernot2016transferability}. Recent studies \cite{shao2021adversarial, kim2024exploring, wang2022understanding} have applied these techniques to investigate the adversarial robustness of ViTs, revealing unique vulnerabilities in their attention mechanisms and token-based processing. Notably, \cite{islam2024malicious} demonstrated how vulnerabilities in vision-language models' embeddings can enable manipulation of navigation paths, while \cite{wei2022towards} leveraged ViTs' self-attention mechanisms for transferable attacks. Other works, such as \cite{wang2024dual}, have explored dual-stage attacks for cross-architecture transferability, and \cite{baras2023quantattack} examined adversarial techniques specific to model quantization. Additionally, representation space-based techniques \cite{salman2024intriguing} have been proposed, including self-supervised learning for unlabeled data \cite{rando2022exploring} and enhanced attack transferability using token gradient regularization \cite{zhang2023transferable}. Novel optimization techniques, such as transferable triggers \cite{wallace2019universal} and gradient-regularized relaxation \cite{chacko2024adversarial}, can enhance ViT attack methodologies, bridging insights across modalities.

Alongside adversarial attack research, significant focus has been on developing defenses to enhance model robustness without sacrificing accuracy \cite{li2023trade}, as maintaining both robustness and performance remains a key challenge. Adversarial training, introduced by \cite{goodfellow2014explaining}, has been widely adopted in ViTs \cite{mo2022adversarial, fu2022patch}, with patch-based techniques \cite{liu2023understanding} and edge information utilization \cite{li2024harnessing} showing promising results in improving model resilience. Other defenses include random entangled tokens \cite{gong2024random}, prediction averaging over noisy inputs \cite{cohen2019certified}, and methods that reduce perturbation impact using small receptive fields and masking \cite{xiang2021patchguard}. Some strategies modify the representation space, such as robust representation sensors \cite{zhang2024towards} or activation guidance \cite{turner2023activation, liu2023context}, offering novel approaches to enhance model robustness. \cite{wei2024assessing} assessed model vulnerability using parameter pruning and low-rank weight approximations, revealing fundamental insights into model brittleness and the trade-offs between model complexity and robustness. Our method differs by introducing neuron neutralization in ViTs to enhance adversarial robustness, without requiring adversarial training and outperforming traditional methods with a fraction of the dataset, providing a more efficient and effective approach to adversarial defense.