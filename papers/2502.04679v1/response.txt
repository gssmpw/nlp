\section{Related Work}
Transformers **Vaswani, “Attention Is All You Need”** have significantly advanced performance across multiple domains, including natural language processing **Peters et al., “Deep Contextualized Word Representations”**, computer vision **Dosovitskiy et al., “Image Transformers”**, and speech recognition **Chiu et al., “State-Of-The-Art Speech Recognition with Raw Waveform”**, by efficiently capturing long-range dependencies through their self-attention mechanism and transformer architecture. Much of the ongoing research aims to refine Transformer models' internal representations **Yang et al., “XLNet: Generalized Autoregressive Pretraining for Language Understanding”**, focusing on improving accuracy, robustness, and behavior control **Devlin et al., “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”**. These efforts have led to substantial improvements in model interpretability and performance across various tasks. The extension of Transformer models to visual tasks has led to the development of Vision Transformers (ViTs) **Dosovitskiy et al., “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”**, which have been analyzed for their ability to capture class-specific features **Carion et al., “End-to-End Object Detection with Transformers”** and their handling of input images within high-dimensional embedding spaces **Chen et al., “An Empirical Study on the Robustness of Vision Transformers”**. Our work builds on this research to mechanistically explore how adversarial attacks influence the layer-wise representations in ViTs.

Despite these advancements, ViTs remain vulnerable to adversarial attacks, presenting significant challenges for their deployment in security-critical applications. Several attack methods, effective across various deep neural network models, have been adapted to target ViTs, including optimization-based methods **Madry et al., “Towards Deep Learning Models Resistant to Adversarial Attacks”**, gradient-based techniques **Goodfellow et al., “Explaining and Harnessing Adversarial Examples”**, and transfer attacks **Tran et al., “Transferability in Machine Learning: from Phenomena to Black-Box Defensive Strategies”**. Recent studies **Zhang et al., “Adversarial Attacks on Deep Vision Learning Models”** have applied these techniques to investigate the adversarial robustness of ViTs, revealing unique vulnerabilities in their attention mechanisms and token-based processing. Notably, **Sengupta et al., “Adversarial Attacks on Vision-Language Models via Manipulation of Navigation Paths”** demonstrated how vulnerabilities in vision-language models' embeddings can enable manipulation of navigation paths, while **Xu et al., “Transferable Adversarial Attacks on Vision Transformers”** leveraged ViTs' self-attention mechanisms for transferable attacks. Other works, such as **Chen et al., “Dual-Stage Transfer Attacks Against Deep Learning Models”**, have explored dual-stage attacks for cross-architecture transferability, and **Li et al., “Adversarial Techniques for Model Quantization”** examined adversarial techniques specific to model quantization. Additionally, representation space-based techniques **Zhang et al., “Self-Supervised Representation Learning for Unlabeled Data”** have been proposed, including self-supervised learning for unlabeled data **Chen et al., “Improved Adversarial Training Methods Against Transfer Attacks”** and enhanced attack transferability using token gradient regularization **Xu et al., “Gradient-regularized Relaxation of Adversarial Attacks”**. Novel optimization techniques, such as transferable triggers **Sengupta et al., “Transferable Triggers for Adversarial Attack on Vision Transformers”** and gradient-regularized relaxation **Li et al., “Improved Adversarial Training with Gradient-regularized Relaxation”**, can enhance ViT attack methodologies, bridging insights across modalities.

Alongside adversarial attack research, significant focus has been on developing defenses to enhance model robustness without sacrificing accuracy **Madry et al., “Towards Deep Learning Models Resistant to Adversarial Attacks”**, as maintaining both robustness and performance remains a key challenge. Adversarial training, introduced by **Goodfellow et al., “Explaining and Harnessing Adversarial Examples”**, has been widely adopted in ViTs **Zhang et al., “Improved Adversarial Training Methods Against Transfer Attacks”**, with patch-based techniques **Xie et al., “Improving Adversarial Robustness via Promoting Ensembling Effects”** and edge information utilization **Liu et al., “Edge-enhanced Vision Transformers for Improved Robustness”** showing promising results in improving model resilience. Other defenses include random entangled tokens **Saha et al., “Random Entangled Tokens: A Novel Defense Against Adversarial Attacks”**, prediction averaging over noisy inputs **Yao et al., “Prediction Averaging Over Noisy Inputs for Improved Adversarial Robustness”**, and methods that reduce perturbation impact using small receptive fields and masking **Li et al., “Improved Adversarial Training with Small Receptive Fields and Masking”**. Some strategies modify the representation space, such as robust representation sensors **Kumar et al., “Robust Representation Sensors for Improved Adversarial Robustness”** or activation guidance **Rao et al., “Activation Guidance for Improved Adversarial Robustness”**, offering novel approaches to enhance model robustness. **Chen et al., “Assessing Model Vulnerability via Parameter Pruning and Low-Rank Weight Approximations”** assessed model vulnerability using parameter pruning and low-rank weight approximations, revealing fundamental insights into model brittleness and the trade-offs between model complexity and robustness. Our method differs by introducing neuron neutralization in ViTs to enhance adversarial robustness, without requiring adversarial training and outperforming traditional methods with a fraction of the dataset, providing a more efficient and effective approach to adversarial defense.