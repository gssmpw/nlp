\section{Introduction}

Large Language Models (LLMs) are essential for applications such as conversational agents and content generation. While they generate highly coherent outputs, LLMs also inherit inherent biases that influence their responses~\cite{perez-etal-2023-discovering}. Recent studies further reveal that these models exhibit biases in their expression of personality traits~\cite{chen2024extroversionintroversioncontrollingpersonality, mao2024editingpersonalitylargelanguage}. Recognizing the importance of understanding and managing these traits, previous research has turned to well-established psychological frameworks for guidance. In particular, personality traits are often assessed using the Myers-Briggs Type Indicator (MBTI)~\cite{10.1037/14404-000}, which categorizes individuals based on cognitive and behavioral preferences. 

\input{latex/figure/fig_1}

While prompting and prompt-based approaches such as IKE~\cite{ike} can induce temporary shifts in responses of LLMs, they often fail to create consistent
changes in decision-making style.
More importantly, as illustrated in Figure~\ref{fig:intro-challenge}, LLMs show a strong inherent bias towards \textit{Feeling}, making it challenging to induce a \textit{Thinking}-oriented reasoning style.

Prompt Induction post Supervised Fine-Tuning (PISF) has been shown to offer improved stability in personality control~\cite{chen2024extroversionintroversioncontrollingpersonality}. However, it still lacks consistency across diverse prompts. Alternatively, researchers have explored model editing techniques to modify LLM opinions. For example, \citet{mao2024editingpersonalitylargelanguage} introduced approaches that utilize MEND~\cite{mitchell2022fastmodeleditingscale} and SERAC~\cite{mitchell2022memorybasedmodeleditingscale}. Because opinions are highly context-sensitive, these methods often overfit to specific modifications, compromising their stability and adaptability. These limitations highlight the need for a more robust approach to personality control.

In this paper, we introduce \textbf{P}ersona \textbf{A}djustment by \textbf{L}LM S\textbf{e}lf-\textbf{T}argeted \textbf{T}rait Control via Relevant Knowledge \textbf{E}diting \textbf{(PALETTE)}, a model editing-based approach that targets personality bias at its core.
\input{latex/figure/fig_2}
Our approach leverages recent advances in model editing, such as Rank-One Model Editing~\cite{meng2023locatingeditingfactualassociations}, to modify specific aspects of an LLM’s internal knowledge without requiring full retraining.
By applying knowledge editing techniques, PALETTE systematically adjusts how a model responds to personality-related queries. Specifically, our method works by generating adjustment queries based on structured MBTI assessments and then applying a rank-one modification to the model’s internal representations.
For example, if the model initially responds to the question, “Which do you usually feel more persuaded by: \textbf{emotionally resonating things with you}, or by \textbf{factual arguments}?” with “I usually feel more persuaded by \textbf{emotionally resonating things},” our approach adjusts its representation so that the response aligns more with a T trait, replacing “\textbf{emotionally}” with “\textbf{factual}.”

Experimental results demonstrate that PALETTE effectively re-balances personality traits in LLMs. Specifically, our method increases the targeted trait ratio by 13\%–19\%, and when combined with prompt-based techniques, an additional 6\%–7\% improvement is observed. These findings confirm that PALETTE enables consistent and controlled personality adjustments, offering a robust solution for mitigating inherent biases in LLMs.