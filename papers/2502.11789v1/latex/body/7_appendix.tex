\appendix
\section{Extra Experiments}
\label{appendix:extra_experiments}
We also conduct experiments for  \textit{Qwen2.5-3b-inst.}, \textit{Qwen2.5-7b-inst.}. You can find detailed result in  Table~\ref{tab:qwen-2.5-3B-acc_results}, 
Table~\ref{tab:qwen-2.5-3B_wlt_results},  Table~\ref{tab:qwen-2.5-7B_acc_results}, and Table~\ref{tab:qwen-2.5-7B_wlt_results}. These results show PALETTE consistently shift the model’s personality traits in the intended direction. These results confirm that our personality editing method effectively mitigates intrinsic biases, achieves reliable personality shifts, and works synergistically with prompting in LLMs, regardless of model size.

\input{latex/table/acc_qwen_3B}
\input{latex/table/acc_qwen_7B}


\input{latex/table/wlt_qwen_3B}
\input{latex/table/wlt_qwen_7B}

\section{Prompts}
\label{appendix:prompts}


\subsection{Response Generation Prompts}
We design and use BASE prompt, T prompt, and F prompt as shown in Table~\ref{tab:generation_prompt}.
Specifically, \textbf{T prompt} is a prompt that encourages the expression of Thinking traits and \textbf{F prompt} is a prompt that encourages the expression of Feeling traits.  

\subsection{IKE Prompts}
Table~\ref{tab:IKE_prompt} demonstrates how our adjustment queries were applied using IKE (Implicit Knowledge Editing). It showcases with examples in three ways the model processes new information: COPY, UPDATE, and RETAIN.

\subsection{Evaluation Prompts}
We conduct pairwise comparisons based on alignment with the target personality traits, and calculate proportion of the target personality traits. Each can be shown in Table~\ref{tab:evaluation_prompt} and Table~\ref{tab:evaluation_prompt_ratio}.

\section{Adjustment Queries}
\label{appendix:request_prompts}

\subsection{Difference between Factual Knowledge Editing and Personality Editing}
\label{appendix:request_prompts_diff}

\begin{table}[ht]
\centering
\scalebox{0.85}{%
\begin{tabularx}{\linewidth}{X} % 가변 너비 사용
\hline
\\
\textbf{\{} \\
\textbf{\quad"prompt":} "\{\} plays the sport of", \\
\textbf{\quad"subject":} "LeBron James", \\
 \textbf{\quad"target\_new":} \{"str": "football"\}, \\
\textbf{\}} \\
\\
\hline
\end{tabularx}}
\caption{Knowledge-editing adjustment query example}
 \label{tab:knowledge_prompt}
\end{table}

\begin{table}[ht]
\centering
\scalebox{0.85}{%
\begin{tabularx}{\linewidth}{X} % 가변 너비 사용
\hline
\\
\textbf{\{} \\
\textbf{\quad"prompt":} "[Question] Which do you \\ \qquad usually feel more persuaded by: \\ 
\qquad emotionally resonating things with you, \\
\qquad or by factual arguments? Answer  \\
\qquad in one sentence. [Your answer] \\
\qquad \{\} usually feel more persuaded by", \\
\textbf{\quad"subject":} "I", \\
\textbf{\quad"target\_new":} \{"str": "factual"\}, \\
\textbf{\}} \\
\\
\hline
\end{tabularx}}
\caption{Personality-editing adjustment query example}
\label{tab:persona_prompt}
\end{table}

As shown in Table~\ref{tab:persona_prompt}, the \textit{target\_new} field is filled with the opposite of the model’s original response, different from Table~\ref{tab:knowledge_prompt}'s knowledge editing adjustment query. For instance, if the original output started with "emotionally," then the target word "factual" is assigned to \textit{target\_new}.

\subsection{Total adjustment queries}
\label{appendix:total_adjustment}

We design these queries based on the MBTI questionnaire. Total adjustment queries used for editing T personality is shown in Table~\ref{tab:total_request_T}, and F for Table~\ref{tab:total_request_F}. 

\input{latex/table/total_request_T}
\input{latex/table/total_request_F}


\input{latex/table/generation_prompt}
\input{latex/table/IKE_prompt}
\input{latex/table/evaluation_prompt_alignment}
\input{latex/table/evaluation_prompt_TraitRatio}


\section{Related Work}
\label{appendix:related_work}

Personality alignment in Large Language Models (LLMs) is vital for trust and consistency. Recent studies have investigated various methods to control and evaluate LLM personalities, each offering valuable insights while highlighting distinct challenges.

\subsection{Personality Control Methods} 
\citet{chen2024extroversionintroversioncontrollingpersonality} showed that prompt-based methods are effective but lack robustness over extended interactions. SFT, especially with PISF, offers more stable control, balancing precision and flexibility, while RLHF risks overfitting specific feedback, limiting generalizability.
\citet{mao2024editingpersonalitylargelanguage} highlighted that model editing techniques like MEND and SERAC effectively alter traits but often lead to overfitting and reduced adaptability. 
\citet{sorokovikova2024llmssimulatebigpersonality} revealed variability in personality simulation among LLMs. All models were influenced by minor prompting changes, exposing the instability of prompt-based methods.
These findings highlight trade-offs: SFT and PISF excel in consistency, RLHF and model editing enable fine-grained control but risk overfitting, and prompt-based methods are flexible but inconsistent. 


\subsection{Personality Evaluation Frameworks} 
\citet{wang-etal-2024-incharacter}'s \textit{INCHARACTER} framework provides a quantitative method for assessing personality fidelity in Role-Playing Agents (RPAs) using psychological scales. It focuses on external evaluation under controlled settings to measure alignment with predefined traits.
In contrast, \citet{mao2024editingpersonalitylargelanguage} introduced the PersonalityEdit benchmark, which evaluates both the alignment and stability of LLM outputs with target traits. 
\citet{sorokovikova2024llmssimulatebigpersonality} explored LLMs’ intrinsic ability to simulate Big Five traits, revealing variability in trait stability and responsiveness to input changes.



\section{Extra Implementation Details}
\label{appendix:extra_implementation}
\paragraph{Hyper-parameter Adjustment}
To adapt the r-ROME framework for personality editing on the \textit{Qwen2.5-1.5b-inst.}~\citep{yang2024qwen2}, several key hyperparameters were adjusted from the original GPT-2-XL configuration as shown in Table~\ref{tab:model_config}.

\begin{table*}[ht]
\centering
\small
\sisetup{table-format=1.4, separate-uncertainty}
\renewcommand{\arraystretch}{1.2} % 행 간격 조정
\begin{tabular}{l|l}
\toprule
\textbf{Parameter} & \textbf{Value} \\ 
\midrule
\textbf{layers} & [15] \\ 
\textbf{fact\_token} & subject\_first \\ 
\textbf{v\_num\_grad\_steps} & 20 \\ 
\textbf{v\_lr} & 2e-1 \\ 
\textbf{v\_loss\_layer} & 27 \\ 
\textbf{v\_weight\_decay} & 0.5 \\ 
\textbf{clamp\_norm\_factor} & 4 \\ 
\textbf{kl\_factor} & 0.0625 \\ 
\textbf{mom2\_adjustment} & false \\ 
\textbf{context\_template\_length\_params} & [[5, 10], [10, 10]] \\ 
\textbf{rewrite\_module\_tmp} & "model.layers.{}.mlp.down\_proj" \\ 
\textbf{layer\_module\_tmp} & "model.layers.{}" \\ 
\textbf{mlp\_module\_tmp} & "model.layers.{}.mlp" \\ 
\textbf{attn\_module\_tmp} & "model.layers.{}.attention.o\_proj" \\ 
\textbf{ln\_f\_module} & "model.final\_layernorm" \\ 
\textbf{lm\_head\_module} & "lm\_head" \\ 
\textbf{mom2\_dataset} & "wikipedia" \\ 
\textbf{mom2\_n\_samples} & 20 \\ 
\textbf{mom2\_dtype} & "float32" \\
\bottomrule
\end{tabular}
\caption{Configuration Parameters for Personality Editing in \textit{Qwen-2.5-1.5b-inst.}}
\label{tab:model_config}
\end{table*}

\begin{figure*}[!ht]
\centering
\includegraphics[width=1.1\textwidth]{latex/figure/human-eval-guide.pdf}
\caption{An example illustrating a structured assessment sheet used for human evaluation}
\label{fig:human-eval-guide}
\end{figure*}
These changes optimize the model's ability to express nuanced personality traits while aligning with the \textit{Qwen} model’s architecture.

\section{Human Evaluation Details}
\label{appendix:human_eval}

To assess the effectiveness of our personality editing approach, we conduct human evaluations using a structured assessment sheet, as shown in Figure~\ref{fig:human-eval-guide}. We recruited three fluent English-speaking judges for the evaluation, each compensated at approximately \$10 per hour. Three judges were provided with an explanation of the decision-making trait, along with the speaker's utterance and model's responses, allowing them to compare personality before and after editing. Originally, we conducted win/loss/tie evaluation; however, since tie results were minimal, we measured effectiveness using the win ratio instead. We computed two metrics to assess consistency among judges: the raw agreement and Cohen’s Kappa score. Agreement scores were 0.7, 0.57, and 0.6, respectively, resulting in an average of 0.605. Cohen’s Kappa scores were 0.4, 0.53, and 0.29, yielding a mean Kappa score of 0.406. These results support the reliability of our human evaluations while maintaining independent judgment.