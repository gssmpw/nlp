\tikzset{%
    parent/.style =          {align=center,text width=1cm,rounded corners=3pt, line width=0.3mm, fill=gray!10,draw=gray!80},
    child/.style =           {align=center,text width=2.3cm,rounded corners=3pt, fill=ttblue!10,draw=ttblue!80,line width=0.3mm},
    grandchild/.style =      {align=center,text width=2cm,rounded corners=3pt},
    greatgrandchild/.style = {align=center,text width=1.5cm,rounded corners=3pt},
    greatgrandchild2/.style = {align=center,text width=1.5cm,rounded corners=3pt},    
    referenceblock/.style =  {align=center,text width=1.5cm,rounded corners=2pt},
    %%%% Re-define
    % --------------------- value based ---------------------
    value_based/.style =           {align=center,text width=1.5cm,rounded corners=3pt, fill=ttblue!10,draw=ttblue!80,line width=0.3mm},   
    value_based1/.style =           {align=center,text width=3cm,rounded corners=3pt, fill=ttblue!10,draw=ttblue!80,line width=0.3mm},   
    value_based2/.style =           {align=center, text width=10.5cm,rounded corners=3pt, fill=ttblue!10,draw=ttblue!0,line width=0.3mm},  
    % --------------------- policy based---------------------
    policy_based/.style =           {align=center,text width=1.5cm,rounded corners=3pt, fill=violet!10,draw=violet!80,line width=0.3mm},   
    policy_based1/.style =           {align=center,text width=3cm,rounded corners=3pt, fill=violet!10,draw=violet!80,line width=0.3mm},   
    policy_secondary/.style =           {align=center,text width=2cm,rounded corners=3pt, fill=violet!10,draw=violet!80,line width=0.3mm},   
    policy_work/.style =           {align=center,text width=7.96cm,rounded corners=3pt, fill=violet!10,draw=violet!0,line width=0.3mm},    
    policy_based2/.style =           {align=center,text width=10.5cm,rounded corners=3pt, fill=violet!10,draw=violet!0,line width=0.3mm},    
    % --------------------- actor_critic ---------------------
    actor_critic/.style =           {align=center,text width=1.5cm,rounded corners=3pt, fill= cyan!10,draw= cyan!80,line width=0.3mm},   
    actor_critic1/.style =           {align=center,text width=3cm,rounded corners=3pt, fill= cyan!10,draw= cyan!80,line width=0.3mm},   
    actor_critic2/.style =           {align=center,text width=10.5cm,rounded corners=3pt, fill= cyan!10,draw= cyan!0,line width=0.3mm},      
    % --------------------- multi_agent ---------------------
    multi_agent/.style =           {align=center,text width=1.5cm,rounded corners=3pt, fill= orange!10,draw= orange!80,line width=0.3mm},   
    multi_agent1/.style =           {align=center,text width=3cm,rounded corners=3pt, fill= orange!10,draw= orange!80,line width=0.3mm},   
    multi_agent2/.style =           {align=center,text width=10.5cm,rounded corners=3pt, fill= orange!10,draw= orange!0,line width=0.3mm},   
    % % Multi-prompt 
    % multiple/.style =           {align=center,text width=1.8cm,rounded corners=3pt, fill= orange!10,draw= orange!80,line width=0.3mm},   
    % multiple_work/.style =           {align=center,text width=5cm,rounded corners=3pt, fill= orange!10,draw= orange!0,line width=0.3mm},        
    % % Tuning Strategy
    % tuning/.style =           {align=center,text width=1.8cm,rounded corners=3pt, fill= magenta!10,draw= magenta!80,line width=0.3mm},   
    % tuning_work/.style =           {align=center,text width=5cm,rounded corners=3pt, fill= magenta!10,draw= magenta!0,line width=0.3mm},          
}

\begin{figure*}
\footnotesize
        \begin{forest}
            for tree={
                forked edges,
                grow'=0,
                draw,
                rounded corners,
                node options={align=center,},
                text width=2.7cm,
                s sep=6pt,
                % calign=center,
                calign=child edge, 
                calign child=(n_children()+1)/2,
                % l sep = 10 pt,
                % l=40pt
                % dot/.style={tikz+={\draw[#1](.anchor)circle[radius=2pt];}},
            },
            [\textcolor{vision_color}{\Large R}\textcolor{lang_color}{\Large L}, fill=gray!45, parent, calign=center
            % Pre-trained model
                [Value Based \S\ref{sec:VB}, for tree={value_based}
                    [Deep Q Learning \S\ref{subsec:DQN},  value_based1
                        [DQN \cite{mnih2015human}
                        , value_based2]
                    ]
                    [Double DQN \S\ref{subsec:DoubleDQN}, value_based1
                        [Double DQN \cite{hasselt2010double}
                        , value_based2]
                    ]
                    [Dueling DQN \S\ref{subsec:DuelingDQN}, value_based1
                        [Dueling DQN \cite{wang2016dueling}
                        , value_based2]                        
                    ]
                    [Others \S\ref{subsec:DQNOthers}, value_based1
                        [DRQN \cite{hausknecht2015deep};
                        PER \cite{schaul2015prioritized};
                        C51 \cite{bellemare2017distributional};
                        QR-DQN \cite{dabney2018distributional};
                        IQN \cite{dabney2018implicit};
                        Noisy DQN \cite{fortunato2019noisy};
                        D3QN \cite{hu2020deep};
                        Multi-step Learning \cite{hernandez2019understanding};
                        Rainbow \cite{hessel2018rainbow};
                        Gorila \cite{parRL}
                        , value_based2]                        
                    ]
                ]
                % policy
                [Policy Based \S\ref{sec:PB}, for tree={fill=violet!45,policy_based}
                    [Vanilla Policy Gradient \S\ref{subsec:vpg},  policy_based1
                        [PG \cite{sutton1999policy}
                        , policy_based2]                            
                    ]
                    [Trust Region Policy Optimization \S\ref{subsec:trpo},  policy_based1
                        [NPG \cite{NIPS2001_4b86abe4};
                        TRPO \cite{schulman2015trust} 
                        , policy_based2]
                    ]
                    [Proximal Policy Optimization \S\ref{subsec:ppo},  policy_based1,
                        [PPO \cite{schulman2017proximal}
                        , policy_based2]
                    ]
                    [Others \S\ref{subsec:pgothers},  policy_based1,
                        [ACER \cite{DBLP:journals/corr/WangBHMMKF16} 
                        ACKTR \cite{wu2017scalable}
                        , policy_based2]
                    ]
                ]
                % planner
                [Actor Critic \S\ref{sec:AC}, for tree={fill=ttblue!45, actor_critic}
                    [Actor Critic \S\ref{subsec:ac}, actor_critic1
                        [AC \cite{konda1999actor}
                        , actor_critic2]  
                    ]
                    [Deterministic Policy Gradient \S\ref{subsec:dpg}, actor_critic1
                        [DPG \cite{silver2014deterministic}
                        , actor_critic2]  
                    ]
                    [Deep Deterministic Policy Gradient \S\ref{subsec:ddpg}, actor_critic1
                        [DDPG \cite{DBLPLillicrapHPHETS15}
                        , actor_critic2]  
                    ]
                    [Twin Delayed Deep Deterministic \S\ref{subsec:td3}, actor_critic1
                        [TD3 \cite{fujimoto2018addressing} 
                        , actor_critic2]  
                    ]
                    [Soft Actor-Critic \S\ref{subsec:sac}, actor_critic1
                        [SAC \cite{haarnoja2018soft}
                        , actor_critic2]  
                    ]
                    [Others \S\ref{subsec:ac_others}, actor_critic1
                        [IMPALA \cite{espeholt2018impala};
                        GA3C \cite{babaeizadeh2016reinforcement};
                        PAAC \cite{alfredo2017efficient};
                        UNREAL \cite{jaderberg2016reinforcement};
                        PGQ \cite{o2016combining};
                        Reactor \cite{gruslys2017reactor};
                        Retrace($\lambda$) \cite{munos2016safe}
                        , actor_critic2]  
                    ]
                ]
                % multi-agent
                [Multi-agent DRL \S\ref{sec:MA}, for tree={fill=ttblue!45, multi_agent}
                    [Multi-Actor Critic \S\ref{subsec:mac}, multi_agent1
                        [LOLA \cite{foerster2017learning};
                        IPPO  \cite{de2020independent};
                        COMA \cite{foerster2018counterfactual};
                        , multi_agent2]  
                    ]
                    [Multi-Critic Multi-DQN \S\ref{subsec:mdqn}, multi_agent1
                        [Hyper Q\cite{tesauro2003extending};
                        JAL \cite{claus1998dynamics};
                         IQL \cite{tan1993multi};
                         VDN \cite{sunehag2017value};
                         QMIX \cite{rashid2020monotonic};
                         QTran \cite{son2019qtran};
                         RODE \cite{wang2021rode};
                         QPLEX \cite{wang2021qplex}
                        , multi_agent2]  
                    ]
                    [Multi-Actor-Critic MADDPG \S\ref{subsec:maddpg}, multi_agent1
                        [ MADDPG \cite{lowe2017multi}
                        , multi_agent2]  
                    ]
                    [Multi-Actor-Critic MAPPO \S\ref{subsec:mappo}, multi_agent1
                        [MAPPO \cite{yu2022surprising}
                        , multi_agent2]  
                    ]
                ]  
            ]
        \end{forest}
        \caption{The taxonomy of DRL algorithms. Policy-based methods are not strictly devoid of critics, but policy-based approaches focus more on policy optimization, while Actor Critic methods emphasize systematic optimization, involving more efficient and parallel sampling.}
        \label{fig:taxonomy}
\end{figure*}
