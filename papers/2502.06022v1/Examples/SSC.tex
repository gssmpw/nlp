\subsection{The flag trick for spectral clustering problems}
Spectral clustering~\citep{ng_spectral_2001} is a baseline clustering technique. It can be applied to general cluster distributions compared to the classical k-means (round clusters) and Gaussian mixture models (ellipsoidal clusters).
Given a dataset $X := [x_1|\dots|x_n] \in \R^{p\times n}$, spectral clustering roughly consists in the eigenvalue decomposition of a Laplacian matrix $L\in\Sym_n$ associated to a pairwise similarity matrix, for instance $M_{ij} = \exp(\norm{x_i - x_j}^2 / 2\sigma^2)$. The eigenvectors are then used as new embeddings for the data points, on which standard clustering algorithms like k-means can be performed. This method is closely related to the celebrated Laplacian eigenmaps~\citep{belkin_laplacian_2003} which are used for nonlinear dimension reduction.
The good performances of such a simple method as spectral clustering are theoretically justified by the particular structure of the Laplacian matrix $L$ in an ideal case---block-diagonal with a multiple eigenvalue related to the number of clusters~\citep{ng_spectral_2001}.
The recent \textit{sparse spectral clustering} (SSC) method~\citep{lu_convex_2016} builds on such an ideal case and encourages the block-diagonality by looking for a sparse and low-rank representation:
\begin{equation}\label{eq:SSC_lu}
	\argmin{U \in \St(p, q)} \langle U U\T, L\rangle_F + \beta \norm{U U\T }_1,
\end{equation}
that they optimize over the convex hull of Grassmann manifolds with an ADMM algorithm.

\subsubsection{Application of the flag trick to spectral clustering}
The Grassmannian spectral clustering method~\citep{wang_grassmannian_2017} directly optimizes~\eqref{eq:SSC_lu} on Grassmann manifolds:
\begin{equation}\label{eq:SSC_Gr}
	\argmin{\S \in \Gr(p, q)} \langle \Pi_\S, L\rangle_F + \beta \norm{\Pi_\S }_1.
\end{equation}
The authors use a Riemannian trust region method~\citep{absil_optimization_2009} for optimization and show the interest of directly optimizing over the Grassmann manifold instead of convexifying the optimization space~\citep{lu_convex_2016}.
The following proposition applies the flag trick to SSC.
\begin{proposition}[Flag trick for SSC]\label{prop:SSC}
The flag trick applied to SSC~\eqref{eq:SSC_Gr} reads
\begin{equation}\label{eq:SSC_Fl}
	\argmin{\S_{1:d} \in \Fl(p, q_{1:d})} \langle \Pi_{\S_{1:d}}, L\rangle_F + \beta \norm{\Pi_{\S_{1:d}} }_1.
\end{equation}
and is equivalent to the following optimization problem:
\begin{equation}\label{eq:SSC_Fl_equiv}
	\argmin{U_{1:d} \in \St(p, q)} \sum_{k=1}^d (d - (k-1)) \tr{{U_k}\T L U_k} + \beta \norm{\sum_{k=1}^d (d - (k-1)) U_k {U_k}\T}_1.
\end{equation}
\end{proposition}
\begin{proof}
The proof is given in Appendix (\autoref{app:SSC}).
\end{proof}
We can see that the case $\beta=0$ corresponds to classical spectral clustering. Indeed, with a similar reasoning as in the proof of Theorem~\ref{thm:flag_trick}, we can easily show that the solution to problem~\eqref{eq:SSC_Fl} is explicit and corresponds to the flag of nested eigenspaces of $L$ (for increasing eigenvalues). Therefore, initializing the algorithm with the smallest $q$ eigenvectors of $L$ seems like a natural idea.
Moreover, one may intuitively analyze the relative weighting of the mutually-orthogonal subspaces $\operatorname{Span}(U_1)\perp\dots\perp\operatorname{Span}(U_d)$ in~\eqref{eq:SSC_Fl_equiv} as favoring a model with $q_1$ clusters, and then adding successively $q_{k} - q_{k_1}$ clusters to improve the modeling of the Laplacian matrix, with a tradeoff between too much and too few clusters.


\subsubsection{Nestedness experiments for spectral clustering}
We consider a 3D extension of the classical two-moons dataset for clustering with $n=100$.
The ambient dimension is $p = 3$ and the intrinsic dimensions that we try are $q_{1:2} = (1, 2)$.
We adopt a pre-processing strategy similar to~\citet{lu_convex_2016,wang_grassmannian_2017}, i.e. we compute the affinity matrix $W\in\R^{n \times n}$ using an exponential kernel with standard deviation being the median of the pairwise Euclidean distances between samples. Then we compute the normalized Laplacian matrix $L = I_n - D^{-\frac12} L D^{-\frac12}$ where $D\in\R^{n \times n}$ is a diagonal matrix with diagonal elements $D_{ii} = \sum_{j=1}^n w_{ij}$.
We run Algorithm~\ref{alg:GD} on Grassmann manifolds to solve the sparse optimization problem~\eqref{eq:SSC_Gr}, successively for $q_1 = 1$ and $q_2 = 2$. Then we plot the projections of the data points onto the optimal subspaces. We compare them to the nested projections onto the optimal flag output by running Algorithm~\ref{alg:GD} on $\Fl(3, (1, 2))$ to solve~\eqref{eq:SSC_Fl}. The results are shown in Figure~\ref{fig:SSC_nested}.
\begin{figure}
	\centering
    \includegraphics[width=\linewidth]{Fig/FT_exp_SC_synthetic_random.pdf}
    \caption{
    Illustration of the nestedness issue in sparse spectral clustering. Given a dataset with two half-moon-shaped clusters, we plot its projection onto the optimal 1D subspace and 2D subspace obtained by solving the associated Grassmannian optimization problem~\eqref{eq:SSC_Gr} or flag optimization problem~\eqref{eq:SSC_Fl}. 
    We can see that the Grassmann representations are not nested, while the flag representations are nested and much better clustered. 
    The last row corresponds to dividing the 2D embeddings by their norms, as commonly done in spectral clustering~\citep{ng_spectral_2001}.}
	\label{fig:SSC_nested}
\end{figure}
\begin{figure}
	\centering
    \includegraphics[width=.9\linewidth]{Fig/FT_exp_SC_breast.pdf}
    \caption{
    Illustration of the nestedness issue in sparse spectral clustering on the breast cancer dataset. For $q_k \in \qf := (1, 2, \dots, 99)$, we solve the Grassmannian optimization problem~\eqref{eq:SSC_Gr} on $\Gr(100, q_k)$ and plot the subspace angles $\Theta(\S_k^*, \S_{k+1}^*)$ (left) and explained variances ${\operatorname{tr}(\Pi_{\S_k^*} X X\T)} / {\operatorname{tr}(X X\T)}$ (right) as a function of $k$. We compare those quantities to the ones obtained by solving the flag optimization problem~\eqref{eq:SSC_Fl}. 
    We can see that the Grassmann-based method is highly non-nested while the flag of subspaces seems to be coherent with the cluster structure: the first axis is for the orange cluster while the second axis is for the blue cluster.
    }
    \label{fig:SC_nested}
\end{figure}
We can see that the Grassmann representations are not only non-nested, but also mix the two clusters in 2D, while the flag representations are nested and much better discriminate the clusters. This could be explained by the nestedness constraint of flag manifolds which imposes the 2D subspace to contain the 1D subspace.

Second, we consider the breast cancer dataset~\citep{wolberg_breast_1995}.
It contains $569$ samples---from which we extract a subset of $100$ samples for computational time---with $30$ numerical attributes and two classes. One has $n = 100$, $p=30$ and $C = 2$. 
Then we run the steepest descent algorithm~\ref{alg:GD} to solve the SSC problem~\eqref{eq:SSC_Fl}.
We choose the full signature $q_{1:99} = (1, 2, \dots, 99)$ and compare the output flag to the individual subspaces output by running optimization on $\Gr(p, q_k)$ for $q_k \in q_{1:d}$.
We perform scatter plots in $1D$ and $2D$ and subspace error computations as a function of the $q_k$. The results are illustrated in \autoref{fig:SC_nested}.
The subspace angle plot tells us that the Grassmann spectral clustering yields highly non-nested subspaces, while the flags are by nature nested. The scatter plots show that the Grassmann representations are totally inconsistent, while the flag representations are consistent with the cluster structures.

Third, we perform some classification experiments on the optimal subspaces for different datasets. For different datasets, we run the optimization problems on $\Fl(p, q_{1:d})$, then project the data onto the different subspaces in $\S_{1:d}^*$, normalize the rows of their Stiefel representative $U_k^*\in\St(p, q_k)$ to have unit Euclidean length and run a k-means algorithm with the number of clusters being equal to the number of classes.
We report the clustering results in \autoref{tab:SC_classif}.
\begin{table*}
  \caption{
  Clustering results for the SSC problem on real datasets. For each method (Gr: Grassmann optimization~\eqref{eq:SSC_Gr}, Fl: flag optimization~\eqref{eq:SSC_Fl}), we give the Rand index metric~\citep{rand_objective_1971} of the projected-predictions with respect to the true cluster labels.}
  \label{tab:SC_classif}
  \centering
  \begin{tabular}{cccccc}
    \toprule
    dataset & $n$ & $p$ & $q_{1:d}$ & Gr & Fl\\
    \midrule
    iris & $99$ & $4$ & $(1, 3, 5)$ & $0.8846$ & $0.9371$ \\
    breast & $100$ & $30$ & $(1, 2, 5)$ & $0.5533$ & $0.8181$ \\
    wine & $99$ & $13$ & $(1, 3, 5)$ & $0.6910$ & $0.7108$ \\
    digits & $100$ & $64$ & $(1, 2, 5, 10)$ & $0.9743$ & $0.9820$ \\
    \bottomrule
  \end{tabular}
\end{table*}
We can see that the flag approach yields more discriminating subspaces than the Grassmann approach. This can be explained by the multilevel nature of flags.



\subsubsection{Discussion on SSC optimization}\label{subsubsec:SSC_discu}
Similarly as in the previous examples, we can see that the steepest descent algorithm~\ref{alg:GD} might not be the most suited to solve the optimization problem~\eqref{eq:SSC_Fl}, notably due to the $\ell_1$ penalization. Contrary to the RSR case in which we want to avoid the non-differentiable points (cf. \autoref{subsubsec:RSR_discu}), here we would ideally like to attain those points, since they encode the desired sparsity of the Laplacian matrix. Therefore, we believe that more standard methods taking into account the $\ell_1$ norm to enforce the hierarchical sparsity of the flag of subspaces would be more adapted.