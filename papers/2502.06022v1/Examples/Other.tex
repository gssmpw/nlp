\subsection{The flag trick for other machine learning problems}
Subspace learning finds many applications beyond robust subspace recovery, trace ratio and spectral clustering problems, as evoked in~\autoref{sec:intro}. The goal of this subsection is to provide a few more examples in brief, without experiments.


\subsubsection{Domain adaptation}
In machine learning, it is often assumed that the training and test datasets follow the same distribution. However, some \textit{domain shift} issues---where training and test distributions are different---might arise, notably if the test data has been acquired from a different source (for instance a professional camera and a phone camera) or if the training data has been acquired a long time ago. \textit{Domain adaptation} is an area of machine learning that deals with domain shifts, usually by matching the training and test distributions---often referred to as \textit{source} and \textit{target} distributions---before fitting a classical model~\citep{farahani_brief_2021}. 
A large body of works (called ``subspace-based'') learn some intermediary subspaces between the source and target data, and perform the inference for the projected target data on these subspaces. The \textit{sampling geodesic flow}~\citep{gopalan_domain_2011} first performs a geodesic interpolation on Grassmannians between the source and target subspaces, then projects both datasets on (a discrete subset of) the interpolated subspaces, which results in a new representation of the data distributions, that can then be given as an input to a machine learning model. The higher the number of intermediary subspaces, the better the approximation, but the larger the dimension of the representation.
The celebrated \textit{geodesic flow kernel}~\citep{boqing_gong_geodesic_2012} circumvents this issue by integrating the projected data onto the continuum of interpolated subspaces. This yields an inner product between infinite-dimensional embeddings that can be computed explicitly and incorporated in a kernel method for learning. The \textit{domain invariant projection}~\citep{baktashmotlagh_unsupervised_2013} learns a \textit{domain-invariant} subspace that minimizes the maximum mean discrepancy (MMD)~\citep{gretton_kernel_2012} between the projected source $X_s := [x_{s1}|\dots|x_{s n_s}] \in \R^{p\times n_s}$ and target distributions $X_t := [x_{t1}|\dots|x_{t n_t}] \in \R^{p\times n_t}$:
\begin{equation}
	\argmin{U \in \St(p, q)} \operatorname{MMD}^2(U\T X_{s}, U\T X_{t}),
\end{equation}
where 
\begin{equation}
	\operatorname{MMD} (X, Y) = \norm{\frac 1 n \sum_{i=1}^n \phi (x_i) - \frac 1 m \sum_{i=1}^m \phi (y_i)}_\mathcal{H}.
\end{equation}
This can be rewritten, using the Gaussian kernel function $\phi(x)\colon y \mapsto \exp\lrp{-\frac{x\T y}{2\sigma^2}}$, as
\begin{multline}\label{eq:DIP}
	\argmin{\S \in \Gr(p, q)} 
	\frac 1 {n_s^2} \sum_{i,j=1}^{n_s} \exp\lrp{-\frac{(x_{si} - x_{sj})\T \Pi_\S (x_{si} - x_{sj})}{2 \sigma^2}}\\
	+ \frac 1 {n_t^2} \sum_{i,j=1}^{n_t} \exp\lrp{-\frac{(x_{ti} - x_{tj})\T \Pi_\S (x_{ti} - x_{tj})}{2 \sigma^2}}\\
	- \frac 2 {n_s n_t} \sum_{i=1}^{n_s} \sum_{j=1}^{n_t} \exp\lrp{-\frac{(x_{si} - x_{tj})\T \Pi_\S (x_{si} - x_{tj})}{2 \sigma^2}}.
\end{multline}
The flag trick applied to the domain invariant projection problem~\eqref{eq:DIP} yields:
\begin{multline}
	\argmin{\S_{1:d} \in \Fl(p, q_{1:d})} 
	\frac 1 {n_s^2} \sum_{i,j=1}^{n_s} \exp\lrp{-\frac{(x_{si} - x_{sj})\T \Pi_{\S_{1:d}} (x_{si} - x_{sj})}{2 \sigma^2}}\\
	+ \frac 1 {n_t^2} \sum_{i,j=1}^{n_t} \exp\lrp{-\frac{(x_{ti} - x_{tj})\T \Pi_{\S_{1:d}} (x_{ti} - x_{tj})}{2 \sigma^2}}\\
	- \frac 2 {n_s n_t} \sum_{i=1}^{n_s} \sum_{j=1}^{n_t} \exp\lrp{-\frac{(x_{si} - x_{tj})\T \Pi_{\S_{1:d}} (x_{si} - x_{tj})}{2 \sigma^2}},
\end{multline}
and can be rewritten as:
\begin{multline}
	\argmin{U_{1:d} \in \St(p, q)}
	\frac 1 {{n_s}^2} \sum_{i,j=1}^{n_s} \exp\lrp{-\sum_{k=1}^d \frac{d+1-k}{d} \frac{\norm{{U_k}\T (x_{si} - x_{sj})}_2^2}{2 \sigma^2}}\\
	+ \frac 1 {{n_t}^2} \sum_{i,j=1}^{n_t} \exp\lrp{-\sum_{k=1}^d \frac{d+1-k}{d} \frac{\norm{{U_k}\T (x_{ti} - x_{tj})}_2^2}{2 \sigma^2}}\\
	- \frac 2 {{n_s} {n_t}} \sum_{i=1}^{n_s} \sum_{j=1}^{n_t} \exp\lrp{-\sum_{k=1}^d \frac{d+1-k}{d} \frac{\norm{{U_k}\T (x_{si} - x_{tj})}_2^2}{2 \sigma^2}}.
\end{multline}
Some experiments similar to the ones of~\citet{baktashmotlagh_unsupervised_2013} can be performed. For instance, one can consider the benchmark visual object recognition dataset of~\citet{saenko_adapting_2010}, learn nested domain invariant projections, fit some support vector machines to the projected source samples at increasing dimensions, and then perform soft-voting ensembling by learning the optimal weights on the target data according to Equation~\eqref{eq:soft_voting}.

\subsubsection{Low-rank decomposition}
Many machine learning methods involve finding low-rank representations of a data matrix. 

This is the case of \textit{matrix completion}~\citep{candes_exact_2012} problems where one looks for a low-rank representation of an incomplete data matrix by minimizing the discrepancy with the observed entries, and which finds many applications including the well-known \href{https://en.wikipedia.org/wiki/Netflix_Prize}{Netflix problem}. Although its most-known formulation is as a convex relaxation, it can also be formulated as an optimization problem on Grassmann manifolds~\citep{keshavan_matrix_2010,boumal_rtrmc_2011} to avoid optimizing the nuclear norm in the full space which can be of high dimension. The intuition is that a low-dimensional point can be described by the subspace it belongs to and its coordinates within this subspace. More precisely, the SVD-based low-rank factorization $M = UW$, with $M \in \R^{p \times n}$, $U \in \St(p, q)$ and $W \in \R^{q \times n}$ is orthogonally-invariant---in the sense that for any $R\in\O(q)$, one has $(UR) (R\T W) = U W$. One could therefore apply the flag trick to such problems, with the intuition that we would try low-rank matrix decompositions at different dimensions. The application of the flag trick would however not be as straightforward as in the previous problems since the subspace-projection matrices $\Pi_\S := U U\T$ do not appear explicitly, and since the coefficient matrix $W$ also depends on the dimension $q$.

Many other low-rank problems can be formulated as a Grassmannian optimization. \textit{Robust PCA}~\citep{candes_robust_2011} looks for a low rank + sparse corruption factorization of a data matrix. \textit{Subspace Tracking}~\citep{balzano_online_2010} incrementally updates a subspace from streaming and highly-incomplete observations via small steps on Grassmann manifolds.

\subsubsection{Linear dimensionality reduction}
Finally, many other general dimension reduction algorithms---referred to as \textit{linear dimensionality reduction methods}~\citep{cunningham_linear_2015}---involve optimization on Grassmannians. For instance, linear dimensionality reduction encompasses the already-discussed PCA and LDA, but also many other problems like \textit{multi-dimensional scaling}~\citep{torgerson_multidimensional_1952}, \textit{slow feature analysis}~\citep{wiskott_slow_2002}, \textit{locality preserving projections}~\citep{he_locality_2003} and \textit{factor analysis}~\citep{spearman_general_1904}.