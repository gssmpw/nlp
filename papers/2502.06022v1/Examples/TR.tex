\subsection{The flag trick for trace ratio problems}\label{subsec:TR}
Trace ratio problems are ubiquitous in machine learning~\citep{ngo_trace_2012}. They write as:
\begin{equation}\label{eq:TR_St}
\argmax{U \in \St(p, q)} \frac{\tr{U\T A U}}{\tr{U\T B U}},
\end{equation}
where $A, B \in \R^{p\times p}$ are positive semi-definite matrices, with $\operatorname{rank}(B) > p - q$.

A famous example of TR problem is Fisher's linear discriminant analysis (LDA)~\citep{fisher_use_1936,belhumeur_eigenfaces_1997}.
It is common in machine learning to project the data onto a low-dimensional subspace before fitting a classifier, in order to circumvent the curse of dimensionality. It is well known that performing an unsupervised dimension reduction method like PCA comes with the risks of mixing up the classes, since the directions of maximal variance are not necessarily the most discriminating ones~\citep{chang_using_1983}. The goal of LDA is to use the knowledge of the data labels to learn a linear subspace that does not mix the classes.
Let $~{X := [x_1|\dots|x_n] \in \R^{p\times n}}$ be a dataset with labels $Y := [y_1|\dots|y_n] \in {[1, C]}^n$. Let $\mu = \frac{1}{n} \sum_{i=1}^n x_i$ be the dataset mean and $\mu_c = \frac{1}{\#\{i : y_i=c\}}\sum_{i : y_i=c} x_i$ be the class-wise means. 
The idea of LDA is to search for a subspace $\S \in \Gr(p, q)$ that simultaneously maximizes the projected \textit{between-class variance} $\sum_{c=1}^C \|\Pi_\S \mu_c - \Pi_\S \mu\|_2^2$ and minimizes the projected \textit{within-class variance} $\sum_{c=1}^C \sum_{i : y_i = c} \|\Pi_\S x_i - \Pi_\S \mu_c\|_2^2$. This can be reformulated as a trace ratio problem~\eqref{eq:TR_St}, with $A = \sum_{c=1}^C (\mu_c - \mu) (\mu_c - \mu)\T$ and $B = \sum_{c=1}^C \sum_{i : y_i = c} (x_i - \mu_c) (x_i - \mu_c)\T$.


More generally, a large family of dimension reduction methods can be reformulated as a TR problem. The seminal work of~\citet{yan_graph_2007} shows that many dimension reduction and manifold learning objective functions can be written as a trace ratio involving Laplacian matrices of attraction and repulsion graphs. Intuitively, those graphs determine which points should be close in the latent space and which ones should be far apart.
Other methods involving a ratio of traces are \textit{multi-view learning}~\citep{wang_trace_2023}, \textit{partial least squares} (PLS)~\citep{geladi_partial_1986,barker_partial_2003} and \textit{canonical correlation analysis} (CCA)~\citep{hardoon_canonical_2004}, although these methods are originally \textit{sequential} problems (cf. footnote~\ref{footnote:sequential}) and not \textit{subspace} problems.

Classical Newton-like algorithms for solving the TR problem~\eqref{eq:TR_St} come from the seminal works of~\citet{guo_generalized_2003, wang_trace_2007, jia_trace_2009}.
The interest of optimizing a trace-ratio instead of a ratio-trace (of the form $\tr{(U\T B U)^{-1}(U\T A U)}$), that enjoys an explicit solution given by a generalized eigenvalue decomposition, is also tackled in those papers. The \textit{repulsion Laplaceans}~\citep{kokiopoulou_enhanced_2009} instead propose to solve a regularized version $\tr{U\T B U} - \rho \tr{U\T A U}$, which enjoys a closed-form, but has a hyperparameter $\rho$, which is directly optimized in the classical Newton-like algorithms for trace ratio problems.

\subsubsection{Application of the flag trick to trace ratio problems}
The trace ratio problem~\eqref{eq:TR_St} can be straightforwardly reformulated as an optimization problem on Grassmannians, due to the orthogonal invariance of the objective function:
\begin{equation}\label{eq:TR_Gr}
\argmax{\S \in \Gr(p, q)} \frac{\tr{\Pi_\S A}}{\tr{\Pi_\S B}}.
\end{equation}
The following proposition applies the flag trick to the TR problem~\eqref{eq:TR_Gr}.
\begin{proposition}[Flag trick for TR]\label{prop:TR}
The flag trick applied to the TR problem~\eqref{eq:TR_Gr} reads
\begin{equation}\label{eq:TR_Fl}
	\argmax{\S_{1:d} \in \Fl(p, q_{1:d})} \frac{\tr{\Pi_{\S_{1:d}} A}}{\tr{\Pi_{\S_{1:d}} B}}.
\end{equation}
and is equivalent to the following optimization problem:
\begin{equation}\label{eq:TR_Fl_equiv}
\argmax{U_{1:d} \in \St(p, q)} \frac{\sum_{k=1}^{d} (d - (k-1)) \tr{{U_k}\T A {U_k}}}{\sum_{l=1}^{d} (d - (l-1)) \tr{{U_{l}}\T B {U_{l}}}}.
\end{equation}
\end{proposition}
\begin{proof}
The proof is given in Appendix (\autoref{app:TR}).
\end{proof}
Equation~\eqref{eq:TR_Fl_equiv} tells us several things. First, the subspaces $~{\operatorname{Span}(U_1) \perp \dots \perp \operatorname{Span}(U_d)}$ are weighted decreasingly, which means that they have less and less importance with respect to the TR objective.
Second, we can see that the nested trace ratio problem~\eqref{eq:TR_Fl} somewhat maximizes the numerator $\tr{\Pi_{\S_{1:d}} A}$ while minimizing the denominator $\tr{\Pi_{\S_{1:d}} B}$. Both subproblems have an explicit solution corresponding to our nested PCA Theorem~\ref{thm:flag_trick}. Hence, one can naturally initialize the steepest descent algorithm with the $q$ highest eigenvalues of $A$ or the $q$ lowest eigenvalues of $B$ depending on the application.
For instance, for LDA, initializing Algorithm~\ref{alg:GD} with the highest eigenvalues of $A$ would spread the classes far apart, while initializing it with the lowest eigenvalues of $B$ would concentrate the classes, which seems less desirable since we do not want the classes to concentrate at the same point.

\subsubsection{Nestedness experiments for trace ratio problems}
For all the experiments of this subsection, we consider the particular TR problem of LDA, although many other applications (\textit{marginal Fisher analysis}~\citep{yan_graph_2007}, \textit{local discriminant embedding}~\citep{chen_local_2005} etc.) could be investigated similarly.

First, we consider a synthetic dataset with five clusters.
The ambient dimension is $p = 3$ and the intrinsic dimensions that we try are $q_{1:2} = (1, 2)$.
We adopt a preprocessing strategy similar to~\citet{ngo_trace_2012}: we first center the data, then run a PCA to reduce the dimension to $n - C$ (if $n - C < p$), then construct the LDA scatter matrices $A$ and $B$, then add a diagonal covariance regularization of $10^{-5}$ times their trace and finally normalize them to have unit trace.
We run Algorithm~\ref{alg:GD} on Grassmann manifolds to solve the TR maximization problem~\eqref{eq:TR_Gr}, successively for $q_1 = 1$ and $q_2 = 2$. Then we plot the projections of the data points onto the optimal subspaces. We compare them to the nested projections onto the optimal flag output by running Algorithm~\ref{alg:GD} on $\Fl(3, (1, 2))$ to solve~\eqref{eq:TR_Fl}. The results are shown in Figure~\ref{fig:TR_nested}.
\begin{figure}
	\centering
    \includegraphics[width=.9\linewidth]{Fig/FT_exp_TR_synthetic.pdf}
    \caption{
    Illustration of the nestedness issue in linear discriminant analysis (trace ratio problem). Given a dataset with five clusters, we plot its projection onto the optimal 1D subspace and 2D subspace obtained by solving the associated Grassmannian optimization problem~\eqref{eq:TR_Gr} or flag optimization problem~\eqref{eq:TR_Fl}. 
    We can see that the Grassmann representations are not nested, while the flag representations are nested and well capture the distribution of clusters. In this example, it is less the nestedness than the \textit{rotation} of the optimal axes inside the 2D subspace that is critical to the analysis of the Grassmann-based method.
    }
	\label{fig:TR_nested}
\end{figure}
\begin{figure}
	\centering
    \includegraphics[width=.9\linewidth]{Fig/FT_exp_TR_digits.pdf}
    \caption{
    Illustration of the nestedness issue in linear discriminant analysis (trace ratio problem) on the digits dataset. For $q_k \in \qf := (1, 2, \dots, 63)$, we solve the Grassmannian optimization problem~\eqref{eq:TR_Gr} on $\Gr(64, q_k)$ and plot the subspace angles $\Theta(\S_k^*, \S_{k+1}^*)$ (left) and explained variances ${\operatorname{tr}(\Pi_{\S_k^*} X X\T)} / {\operatorname{tr}(X X\T)}$ (right) as a function of $k$. We compare those quantities to the ones obtained by solving the flag optimization problem~\eqref{eq:TR_Fl}. 
    We can see that the Grassmann-based method is highly non-nested and even yields an extremely paradoxical non-increasing explained variance (cf. red circle on the right).
    }
	\label{fig:TR_nested_digits}
\end{figure}
We can see that the Grassmann representations are non-nested while their flag counterparts perfectly capture the filtration of subspaces that best and best approximates the distribution while discriminating the classes. Even if the colors make us realize that the issue in this experiment for LDA  is not much about the non-nestedness but rather about the rotation of the principal axes within the 2D subspace, we still have an important issue of consistency.

Second, we consider the (full) handwritten digits dataset~\citep{alpaydin_optical_1998}. It contains $8 \times 8$ pixels images of handwritten digits, from $0$ to $9$, almost uniformly class-balanced. One has $n = 1797$, $p=64$ and $C = 10$.
We run a steepest descent algorithm to solve the trace ratio problem~\eqref{eq:TR_Fl}. We choose the full signature $q_{1:63} = (1, 2, \dots, 63)$ and compare the output flag to the individual subspaces output by running optimization on $\Gr(p, q_k)$ for $q_k \in q_{1:d}$.
We plot the subspace angles $\Theta(\S_k^*, \S_{k+1}^*)$ and the explained variance ${\operatorname{tr}(\Pi_{\S_k^*} X X\T)} / {\operatorname{tr}(X X\T)}$ as a function of the $k$. The results are illustrated in \autoref{fig:TR_nested_digits}.
We see that the subspace angles are always positive and even very large sometimes with the LDA. Worst, the explained variance is not monotonous. This implies that we sometimes \textit{loose} some information when \textit{increasing} the dimension, which is extremely paradoxical.

Third, we perform some classification experiments on the optimal subspaces for different datasets. For different datasets, we run the optimization problems on $\Fl(p, q_{1:d})$, then project the data onto the different subspaces in $\S_{1:d}^*$ and run a nearest neighbors classifier with $5$ neighbors.
The predictions are then ensembled (cf. Algorithm~\ref{alg:flag_trick}) by weighted averaging, either with uniform weights or with weights minimizing the average cross-entropy:
\begin{equation}\label{eq:soft_voting}
	w_1^*, \dots, w_d^* = \argmin{\substack{w_k \geq 0 \\ \sum_{k=1}^d w_k = 1}} - \frac 1 {n C} \sum_{i=1}^n \sum_{c=1}^C y_{ic} \ln\lrp{\sum_{k=1}^d w_k y_{kic}^*},
\end{equation}
where $y_{kic}^* \in [0, 1]$ is the predicted probability that $x_i \in \R^p$ belongs to class $c \in \{1 \dots C\}$, by the classifier $g_k^*$ that is trained on $Z_k := {U_k^*}\T X \in \R^{q_k \times n}$. One can show that the latter is a convex problem, which we optimize using the \href{https://www.cvxpy.org/index.html}{cvxpy} Python package~\citep{diamond2016cvxpy}.
We report the results in \autoref{tab:TR_classif}.
\begin{table}
  \caption{Classification results for the TR problem on real datasets. For each method (Gr: Grassmann optimization~\eqref{eq:TR_Gr}, Fl: flag optimization~\eqref{eq:TR_Gr}, Fl-U: flag optimization + uniform soft voting, Fl-W: flag optimization + optimal soft voting~\eqref{eq:soft_voting}), we give the cross-entropy of the projected-predictions with respect to the true labels.}
  \label{tab:TR_classif}
  \centering
  \begin{tabular}{ccccccccc}
    \toprule
    dataset & $n$ & $p$ & $q_{1:d}$ & Gr & Fl & Fl-U & Fl-W & weights\\
    \midrule
    breast & $569$ & $30$ & $(1, 2, 5)$ & $0.0986$ & $0.0978$ & $0.0942$ & $0.0915$ & $(0.754, 0, 0.246)$\\
    iris & $150$ & $4$ & $(1, 2, 3)$ & $0.0372$ & $0.0441$ & $0.0410$ & $0.0368$ & $(0.985, 0, 0.015)$\\
    wine & $178$ & $13$ & $(1, 2, 5)$ & $0.0897$ & $0.0800$ & $0.1503$ & $0.0677$ & $(0, 1, 0)$\\
    digits & $1797$ & $64$ & $(1, 2, 5, 10)$ & $0.4507$ & $0.4419$ & $0.5645$ & $0.4374$ & $(0, 0, 0.239, 0.761)$\\
    \bottomrule
  \end{tabular}
\end{table}
The first example tells us that the optimal $5D$ subspace obtained by Grassmann optimization less discriminates the classes than the $5D$ subspace from the optimal flag. This may show that the flag takes into account some lower dimensional variability that enables to better discriminate the classes. We can also see that the uniform averaging of the predictors at different dimensions improves the classification. Finally, the optimal weights improve even more the classification and tell that the best discrimination happens by taking a soft blend of classifier at dimensions $1$ and $5$. Similar kinds of analyses can be made for the other examples.

\subsubsection{Discussion on TR optimization and kernelization}
\paragraph{A Newton algorithm}
In all the experiments of this paper, we use a steepest descent method on flag manifolds (Algorithm~\ref{alg:GD}) to solve the flag problems.
However, for the specific problem of TR~\eqref{eq:TR_Fl}, we believe that more adapted algorithms should be derived to take into account the specific form of the objective function, which is classically solved via a Newton-Lanczos method~\citep{ngo_trace_2012}. 
To that extent, we develop in the appendix (\autoref{app:TR}) an extension of the baseline Newton-Lanczos algorithm for the flag-tricked problem~\eqref{eq:TR_Fl}.
In short, the latter can be reformulated as a penalized optimization problem of the form $\operatorname{argmax}_{\Sf\in\Fl(p, \qf)} {\sum_{k=1}^d \tr{\Pi_{\S_k} (A - \rho B)}}$, where $\rho$ is updated iteratively according to a Newton scheme. Once again, our central Theorem~\ref{thm:flag_trick} enables to get explicit expressions for the iterations, which results without much difficulties in a Newton method, that is known to be much more efficient than first-order methods like the steepest descent.

\paragraph{A non-linearization via the kernel trick}
The classical trace ratio problems look for \textit{linear} embeddings of the data.
However, in most cases, the data follow a \textit{nonlinear} distribution, which may cause linear dimension reduction methods to fail. The \textit{kernel trick}~\citep{hofmann_kernel_2008} is a well-known method to embed nonlinear data into a linear space and fit linear machine learning methods.
As a consequence, we propose in appendix (\autoref{app:TR}) a kernelization of the trace ratio problem~\eqref{eq:TR_Fl} in the same fashion as the one of the seminal graph embedding work~\citep{yan_graph_2007}.
This is expected to yield much better embedding and classification results.