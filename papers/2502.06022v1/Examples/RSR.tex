\subsection{The flag trick for robust subspace recovery problems}
Let us consider a dataset that is a union of \textit{inliers} and \textit{outliers}---the inliers are assumed to lie near a low-dimensional subspace $\S$ while the outliers live in the ambient space. The aim of robust subspace recovery (RSR) is to recover $\S$~\citep{lerman_overview_2018}. In that sense, RSR is an outlier-robust extension of classical dimension reduction methods like PCA.
Without further specifications, the RSR problem might not be well-posed. Therefore, the works in this domain often have to make some assumptions on the inlier and outlier distributions in order to obtain some convergence and recovery guarantees.
For instance, in \citet{lerman_overview_2018}, it is assumed that the inliers ``fill'' the lower-dimensional subspace and that the outliers are not much ``aligned''; this is rigorously defined in~\citet{lerman_robust_2015} and~\citet{maunu_well-tempered_2019} through \textit{permeance} and \textit{alignment} statistics.
A typical generative model following those assumptions is the \textit{Haystack model} introduced in~\citet{lerman_robust_2015}. The Haystack model assumes an isotropic Gaussian distribution on the subspace for the inliers and an isotropic Gaussian distribution on the (full) ambient space for the outliers. A more realistic model---the \textit{generalized Haystack model}---is introduced in~\citet{maunu_well-tempered_2019} to circumvent the simplistic nature of the Haystack model. This one assumes general (anisotropic) Gaussian distributions for the inliers and outliers. This makes the learning harder, since the anisotropy may keep the inliers from properly permeating the low-dimensional subspace---as discussed in \autoref{sec:intro}. Therefore, one has to make some stronger assumptions on the inlier-outlier ratio and the covariance eigenvalues distributions to derive some convergence and recovery guarantees.
\begin{remark}[Parametrization of RSR generative models]
The inlier distribution in the Haystack model follows the \textit{isotropic PPCA} model~\citep{bouveyron_hddc_2007, bouveyron_intrinsic_2011}, while it follows the \textit{PPCA} model~\citep{tipping_probabilistic_1999} in the case of the generalized Haystack model. Both models are a special case of the principal subspace analysis models~\citep{szwagier_curse_2024}. However, as argued in~\citet{szwagier_curse_2024}, while the Haystack model is parameterized with Grassmannians, the generalized Haystack model---which has more degrees of freedom accounting for the anisotropy---is parameterized with Stiefel manifolds. Therefore, from a statistical modeling perspective, it only makes sense to conduct subspace learning experiments on the Haystack model and not the generalized one.
\end{remark}


\subsubsection{Application of the flag trick to RSR}
Among the large family of methods for robust subspace recovery~\citep{lerman_overview_2018} we consider the one of \textit{least absolute deviation} (LAD) minimization, which can be explicitly formulated as an optimization problem on Grassmannians.
PCA minimizes the sum of \textit{squared} Euclidean distances between the points and the subspace. In the case of an outlier-contaminated dataset, the squared Euclidean distances might penalize too much the outliers, and therefore make the optimal subspace too much influenced by the outliers. To circumvent this well-known sensitivity of squared norms to outliers, many works propose to minimize the sum of \textit{absolute} Euclidean distances, which defines the LAD minimization problem:
\begin{equation}\label{eq:RSR_Gr}
    \argmin{\S \in \Gr(p, q)} \sum_{i=1}^n \norm{x_i - \Pi_{\S} x_i}_2.
\end{equation}
The latter has the interest of being rotationally invariant~\citep{ding_r1-pca_2006} but the drawback of being NP-hard~\citep{mccoy_two_2011, lerman_overview_2018} and obviously non-convex since Grassmannians are not.  % this sentence is maybe not very important here, but let's still keep it...
A first body of works relaxes the problem, for instance by optimizing on the convex hull of Grassmannians~\citep{mccoy_two_2011, xu_robust_2012, zhang_novel_2014, lerman_robust_2015}.
A second body of works directly optimizes the LAD on Grassmannians, either with an IRLS algorithm~\citep{lerman_fast_2018} or with a geodesic gradient descent~\citep{maunu_well-tempered_2019}, both achieving very good results in terms of recovery and speed.
The following proposition applies the flag trick to the LAD problem~\eqref{eq:RSR_Gr}.
\begin{proposition}[Flag trick for RSR]\label{prop:RSR}
The flag trick applied to LAD reads
\begin{equation}\label{eq:RSR_Fl}
    \argmin{\S_{1:d} \in \Fl(p, q_{1:d})} \sum_{i=1}^n \norm{x_i - \Pi_{\S_{1:d}} x_i}_2,
\end{equation} 
and is equivalent to the following optimization problem:
\begin{equation}\label{eq:RSR_Fl_equiv}
	\argmin{U_{1:d+1} \in \O(p)} \sum_{i=1}^n \sqrt{\sum_{k=1}^{d+1} \lrp{\frac {k-1} {d}}^2 \norm{{U_k}\T x_i}_2^2}.
\end{equation}
\end{proposition}
\begin{proof}
The proof is given in Appendix (\autoref{app:RSR}).
\end{proof}
Equation~\eqref{eq:RSR_Fl_equiv} tells us several things. First of all, due to the $((k-1)/d)^2$ weights, the points that are far from the center should be in the first principal subspaces. Second, the square root decreases the influence of the outlying points, compared to the nested PCA of Theorem~\ref{thm:flag_trick}.
Third, although less obvious, we can see that numerical issues are less prone to happen with the flag trick~\eqref{eq:RSR_Fl_equiv} than with the LAD~\eqref{eq:RSR_Gr}, since the quantity under the square root is zero only when the first subspace $~{\S_1 = \operatorname{Span}(U_1)}$ contains a data point. Therefore, whenever $q_1$ is smaller than the dimension $q$ that one would have tried for classical RSR, the non-differentiability and exploding-gradient issues caused by the square root are less likely.
Finally, since the nested LAD minimization~\eqref{eq:RSR_Fl} is nothing but a robust version of the nested PCA of Theorem~\ref{thm:flag_trick} ($\sum_{i=1}^n \|x_i - \Pi_{\Sf} x_i\|_2^2$), a natural idea can be to initialize the optimization algorithm with the nested PCA solution. This is what is done in~\citet{maunu_well-tempered_2019} for LAD minimization, and it is coming with stronger recovery guarantees.

\subsubsection{Nestedness experiments for RSR}
We first consider a dataset consisting in a mixture of two multivariate Gaussians: the inliers, with zero mean, covariance matrix $\diag{5, 1, .1}$, $n_\mathrm{in} = 450$ and the outliers, with zero mean, covariance matrix $\diag{.1, .1, 5}$, $n_\mathrm{out} = 50$. The dataset is therefore following the generalized haystack model of~\citet{maunu_well-tempered_2019}.
The ambient dimension is $p = 3$ and the intrinsic dimensions that we try are $q_{1:2} = (1, 2)$.
We run Algorithm~\ref{alg:GD} on Grassmann manifolds to solve the LAD minimization problem~\eqref{eq:RSR_Gr}, successively for $q_1 = 1$ and $q_2 = 2$. Then we plot the projections of the data points onto the optimal subspaces. We compare them to the nested projections onto the optimal flag output by running Algorithm~\ref{alg:GD} on $\Fl(3, (1, 2))$ to solve~\eqref{eq:RSR_Fl}. The results are shown in Figure~\ref{fig:RSR_nested}.
\begin{figure}
	\centering
    \includegraphics[width=\linewidth]{Fig/FT_exp_RSR_synthetic.pdf}
    \caption{
    Illustration of the nestedness issue in robust subspace recovery. Given a dataset consisting in a mixture of inliers (blue) and outliers (orange) we plot its projection onto the optimal 1D subspace and 2D subspace obtained by solving the associated Grassmannian optimization problem~\eqref{eq:RSR_Gr} or flag optimization problem~\eqref{eq:RSR_Fl}. 
    We can see that the Grassmann representations are not nested, while the flag representations are nested and robust to outliers.}
	\label{fig:RSR_nested}
\end{figure}
\begin{figure}
	\centering
    \includegraphics[width=\linewidth]{Fig/FT_exp_RSR_digits}
    \caption{Distributions of (increasing) Euclidean reconstruction errors $\|x_i - \Pi_{\S_{1:d}^*} x_i\|_{i=1\dots n}$ on the corrupted digits dataset for Grassmann and flag methods. While inliers and outliers are mixed with the subspace method, we can see a \textit{clear transition} with flags. This can be explained by the multilevel nature of the flag trick.}
	\label{fig:RSR_outlier}
\end{figure}
We can see that the Grassmann-based projections are non-nested while their flag counterparts are not only nested but also robust to outliers. This could be explained by the nestedness constraint of flag manifolds which imposes the 2D subspace to contain the 1D subspace.

Second, we perform an outlier detection experiment. A common methodology to detect outliers in a corrupted dataset is to first look for an outlier-robust subspace and then plot the distribution of distances between the data points and their projection onto the recovered subspace. This distribution is expected to show a clear gap between the inliers and outliers~\citep[Fig.~3.7]{vidal_generalized_2016}.
However, in practice, one does not know which subspace dimension $q$ to choose. If $q$ is too large, then the recovered subspace may contain both inliers and outliers, and therefore the distribution of distances might be roughly $0$. In contrast, if $q$ is too small, then some inliers may lie too far from the recovered subspace and be detected as outliers. An idea in the spirit of the flag trick is to perform an average ensembling of the reconstruction errors. More specifically, if $\norm{x_i - \Pi_\S x_i}_2$ is the classical measure of robust reconstruction error, then we compute $\norm{x_i - \Pi_{\S_{1:d}} x_i}_2$. Such a score extracts information from projections at different levels and might result in a richer multilevel analysis.
We consider a dataset where the inliers are images of $8 \times 8$ handwritten $0$'s and outliers correspond to other digits from $1$ to $9$, all extracted from the classical handwritten digits dataset~\citep{alpaydin_optical_1998}.
The ambient dimension is $p = 64$, the number of inliers is $n_\mathrm{in} = 90$ and the number of outliers is $n_\mathrm{out} = 10$. The intrinsic dimensions that we try are $q_{1:3} = (1, 2, 5)$.
We plot the reconstruction error for the points of the digits dataset on the optimal flag $\S_{1:3}^* \in \Fl(p, (1, 2, 5))$ in Figure~\ref{fig:RSR_outlier}. We compare it to the metric on $\S_3 \in \Gr(p, q_3)$.
We can see that the flag trick enables to clearly distinguish the inliers from the outliers compared to the Grassmann-based method, which is a consequence of the multilevel nature of flags.


\subsubsection{Discussion on RSR optimization and objective functions}\label{subsubsec:RSR_discu}
\paragraph{An IRLS algorithm}
In all the experiments of this paper, we use a steepest descent method on flag manifolds (Algorithm~\ref{alg:GD}) to solve the flag problems.
However, for the specific problem of RSR~\eqref{eq:RSR_Fl}, we believe that more adapted algorithms should be derived, notably due to the non-differentiability and exploding-gradient issues caused by the square root.
To that extent, we derive in appendix (\autoref{app:RSR}) an IRLS scheme (Algorithm~\ref{alg:FMF}) for RSR. In short, the RSR problem~\eqref{eq:RSR_Fl} can be reformulated as a weighted least squares problem $\sum_{i=1}^n w_i \norm{x_i - \Pi_{\Sf} x_i}_2^2$ with $w_i ={1}/{\norm{x_i - \Pi_{\Sf} x_i}_2}$ and optimized iteratively, with explicit expressions obtained via our central Theorem~\ref{thm:flag_trick}. We insist on the fact that such an IRLS algorithm could not be developed with the flagification of~\citet{pennec_barycentric_2018,mankovich_fun_2024}, since a sum of square roots does not correspond to a least-squares problem.

\paragraph{More RSR problems}
In this work, we explore one specific problem of RSR for conciseness, but we could investigate many other related problems, including robust PCA. 
Notably, drawing from the Grassmann averages (GA) method~\citep{hauberg_scalable_2016}, one could develop many new multilevel RSR and RPCA objective functions.
The idea behind GA is to replace data points with 1D subspaces ($\S_i = \operatorname{Span}(x_i)$) and then perform subspace averaging methods to find a robust prototype for the dataset. GA ends up solving problems of the form $\operatorname{argmin}_{\S \in \Gr(p, 1)} \sum_{i=1}^n w_i \, \operatorname{dist}_{\Gr(p, 1)}^2(\operatorname{Span}(x_i), \S)$, where $w_i$ are some weights and $\operatorname{dist}_{\Gr(p, 1)}$ is a particular subspace distance detailed in~\citet{hauberg_scalable_2016}. Using instead some multidimensional subspace distances, like the principal angles and its variants~\citep{hamm_grassmann_2008, ye_schubert_2016}, we can develop many variants of the Grassmann averages, of the form $\operatorname{argmin}_{\Sf \in\Gr(p, q)} \sum_{i=1}^n w_i \, \rho(\sqrt{{x_i}\T  \Pi_{\S} {x_i}})$, where $\rho\colon\R\to\R$ is a real function, like $\rho(x) = \arccos(x)$ if we want subspace-angle-like distances, $\rho(x) = - x^2$ if we want PCA-like solutions, and many other possible robust variants.
Applying the flag trick to those problems yields the following robust multilevel problem: $\operatorname{argmin}_{\Sf \in\Fl(p, \qf)} \sum_{i=1}^n w_i \rho(\sqrt{{x_i}\T  \Pi_{\Sf} {x_i}})$.