\section{Robust subspace recovery: extensions and proofs}\label{app:RSR}

\subsection{An IRLS algorithm for robust subspace recovery}
Iteratively reweighted least squares (IRLS) is a ubiquitous method to solve optimization problems involving $L^p$-norms. Motivated by the computation of the geometric median~\citep{weiszfeld_sur_1937}, IRLS is highly used to find robust maximum likelihood estimates of non-Gaussian probabilistic models (typically those containing outliers) and finds application in robust regression~\citep{huber_robust_1964}, sparse recovery~\citep{daubechies_iteratively_2010} etc.

The recent fast median subspace (FMS) algorithm~\citep{lerman_fast_2018}, achieving state-of-the-art results in RSR uses an IRLS scheme to optimize the Least Absolute Deviation (LAD)~\eqref{eq:RSR_Gr}.
The idea is to first rewrite the LAD as 
\begin{equation}
	\sum_{i=1}^n \norm{x_i - \Pi_{\S} x_i}_2 = \sum_{i=1}^n w_i(\S) \norm{x_i - \Pi_{\S} x_i}_2^2,
\end{equation}
with $w_i(\S) = \frac{1}{\norm{x_i - \Pi_{\S} x_i}_2}$, and then successively compute the weights $w_i$ and update the subspace according to the weighted objective.
More precisely, the FMS algorithm creates a sequence of subspaces $\S^1, \dots, \S^m$ such that 
\begin{equation}\label{eq:IRLS_FMF}
    \S^{t+1} = \argmin{\S \in \Gr(p, q)} \sum_{i=1}^n w_i(\S^t) \norm{x_i - \Pi_\S x_i}_2^2.
\end{equation}
This weighted least-squares problem enjoys a closed-form solution which relates to the eigenvalue decomposition of the weighted covariance matrix $\sum_{i=1}^n w_i(\S^t) x_i {x_i}\T$~\citep[Chapter~3.3]{vidal_generalized_2016}.

We wish to derive an IRLS algorithm for the flag-tricked version of the LAD minimization problem~\eqref{eq:RSR_Fl}.
In order to stay close in mind to the recent work of \citet{peng_convergence_2023} who proved convergence of a general class of IRLS algorithms under some mild assumptions, we first rewrite~\eqref{eq:RSR_Fl} as
\begin{equation}~\label{eq:RSR_Fl_IRLS}
    \argmin{\S_{1:d} \in\Fl(p, \qf)} \sum_{i=1}^n \rho(r(\S_{1:d}, x_i)),
\end{equation}
where $r(\S_{1:d}, x) = \norm{x - \Pi_{\Sf} x}_2$ is the \textit{residual} and $\rho(r) = |r|$ is the \textit{outlier-robust} loss function.
Following~\citet{peng_convergence_2023}, the IRLS scheme associated with~\eqref{eq:RSR_Fl_IRLS} is:
\begin{equation}
\begin{cases}
w_i^{t+1} = \rho'(r(\S_{1:d}^t, x_i)) /  r(\S_{1:d}^t, x_i) = 1 / \norm{x_i - \Pi_{\Sf} x_i}_2,\\
(\S_{1:d})^{t+1} = \argmin{\S_{1:d} \in\Fl(p, \qf)} \sum_{i=1}^n w_i^{t+1} \norm{x_i - \Pi_{\Sf} x_i}_2^2.
\end{cases}
\end{equation}
We now show that the second step enjoys a closed-form solution.
\begin{theorem}\label{thm:IRLS_FMF}
The RLS problem
\begin{equation}
    \argmin{\S_{1:d} \in\Fl(p, \qf)} \sum_{i=1}^n w_i \norm{x_i - \Pi_{\Sf} x_i}_2^2
\end{equation}
has a closed-form solution $\S_{1:d}^* \in\Fl(p, \qf)$, which is given by the eigenvalue decomposition of the weighted sample covariance matrix $S_w = \sum_{i=1}^n w_i x_i {x_i}\T = \sum_{j=1}^p \ell_j v_j {v_j}\T$, i.e.
\begin{equation}
    \S_k^* = \operatorname{Span}(v_1, \dots, v_{q_k}) \quad (k=1\twodots d).
\end{equation}
\end{theorem}
\begin{proof}
One has
\begin{equation}
	\sum_{i=1}^n w_i \norm{x_i - \Pi_{\Sf} x_i}_2^2 = \tr{(I - \Pi_{\Sf})^2 \lrp{\sum_{i=1}^n w_i x_i {x_i}\T}}.
\end{equation}
Therefore, we are exactly in the same case as in \autoref{thm:flag_trick}, if we replace $X X\T$ with the reweighted covariance matrix $\sum_{i=1}^n w_i x_i {x_i}\T$. This does not change the result, so we conclude with the end of the proof of \autoref{thm:flag_trick} (which itself relies on~\citet{szwagier_curse_2024}).
\end{proof}
Hence, one gets an IRLS scheme for the LAD minimization problem. 
One can modify the robust loss function $\rho(r) = |r|$ by a Huber-like loss function to avoid weight explosion. Indeed, one can show that the weight $w_i := 1 / \norm{x_i - \Pi_{\Sf} x_i}_2$ goes to infinity when the first subspace $\S_1$ of the flag gets close to $x_i$ .
Therefore in practice, we take 
\begin{equation}
    \rho(r) = 
        \begin{cases}
            r^2 / (2 p \delta) & \text{if } |r| <= p\delta,\\
            r - p \delta / 2 & \text{if } |r| > p\delta.
        \end{cases}
\end{equation}
This yields
\begin{equation}
    w_i = 1 / \max\lrp{p\delta,  1 / \norm{x_i - \Pi_{\Sf} x_i}_2}.
\end{equation}
The final proposed scheme is given in Algorithm~\ref{alg:FMF}, named \textit{fast median flag} (FMF), in reference to the fast median subspace algorithm of~\citet{lerman_fast_2018}.
\begin{algorithm}
\caption{Fast median flag}\label{alg:FMF}
\begin{algorithmic}
\Require $X\in \R^{p\times n}$ (data), $\quad q_1 < \dots < q_d$ (signature), $\quad t_{max}$ (max number of iterations), $\quad \eta$ (convergence threshold), $\quad \varepsilon$ (Huber-like saturation parameter)
\Ensure
$U \in \St(p, q)$
\State $t \gets 0, \quad \Delta \gets \infty, \quad U^0 \gets \operatorname{SVD}(X, q)$
\While{$\Delta > \eta$ and $t < t_{max}$}
    \State $t \gets t+1$
    \State $r_i \gets \norm{x_i - \Pi_{\Sf} x_i}_2$
    \State $y_i \gets {x_i} / {\max(\sqrt{r_i}, \varepsilon)}$
    \State $U^t \gets \operatorname{SVD(Y, q)}$
    \State $\Delta \gets \sqrt{\sum_{k=1}^{d} \Theta(U^t_{q_k}, U^{t-1}_{q_k})^2}$
\EndWhile
\end{algorithmic}
\end{algorithm}
We can easily check that FMF is a direct generalization of FMS for Grassmannians (i.e. when $d=1$).


\begin{remark}
This is far beyond the scope of the paper, but we believe that the convergence result of~\citet[Theorem~1]{peng_convergence_2023} could be generalized to the FMF algorithm, due to the compactness of flag manifolds and the expression of the residual function $r$.
\end{remark}

\subsection{Proof of Proposition~\ref{prop:RSR}}
Let $\Sf \in \Fl(p, \qf)$ and $U_{1:d+1} := [U_1|U_2|\dots|U_d|U_{d+1}] \in \O(p)$ be an orthogonal representative of $\Sf$. One has:
\begin{align}
	\norm{x_i - \Pi_{\S_{1:d}} x_i}_2 &= \sqrt{{(x_i - \Pi_{\S_{1:d}} x_i)}\T (x_i - \Pi_{\S_{1:d}} x_i)},\\
	 &= \sqrt{{x_i}\T {(I_p - \Pi_{\S_{1:d}})}^2 x_i},\\
	 &= \sqrt{{x_i}\T {\lrp{I_p - \frac1d \sum_{k=1}^d\Pi_{\S_k}}}^2 x_i},\\
 	 &= \sqrt{\frac1{d^2} {x_i}\T {\lrp{\sum_{k=1}^d (I_p - \Pi_{\S_k})}}^2 x_i},\\
 	 % &= \sqrt{\frac1{d^2} {x_i}\T U_{1:d+1} \diag{0, 1, \dots, d-1, d}^2 {U_{1:d+1}}\T  x_i},\\
 	 &= \sqrt{\frac1{d^2} {x_i}\T \lrp{\sum_{k=1}^{d+1} (k-1) U_k {U_k}\T}^2  x_i},\\
 	 &= \sqrt{\frac1{d^2} {x_i}\T \lrp{\sum_{k=1}^{d+1} (k-1)^2 U_k {U_k}\T}  x_i},\\
 	 &= \sqrt{\sum_{k=1}^{d+1} \lrp{\frac {k-1} {d}}^2 {x_i}\T \lrp{ U_k {U_k}\T}  x_i},\\
  	 \norm{x_i - \Pi_{\S_{1:d}} x_i}_2 &= \sqrt{\sum_{k=1}^{d+1} \lrp{\frac {k-1} {d}}^2 \norm{{U_k}\T x_i}_2^2},
\end{align}
which concludes the proof.