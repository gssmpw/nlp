\section{Trace Ratio problems: extensions and proofs}\label{app:TR}

\subsection{A Newton method for multilevel trace ratio problems}
We wish to solve the following optimization problem (cf. \autoref{eq:TR_Fl}):
\begin{equation}
	\argmax{\S_{1:d} \in \Fl(p, q_{1:d})} \frac{\tr{\Pi_{\S_{1:d}} A}}{\tr{\Pi_{\S_{1:d}} B}}.
\end{equation}
We draw from~\citet{ngo_trace_2012} (itself based on \citet{wang_trace_2007,jia_trace_2009}) to develop an efficient algorithm for solving this problem.

Let $\phi\colon \Fl(p, q_{1:d}) \ni \Sf \mapsto {\tr{\sum_{k=1}^d \Pi_{\S_k} A}}/{\tr{\sum_{k=1}^d \Pi_{\S_k} B}}$. One can see that $\phi$ is well defined on $\Fl(p, q_{1:d})$ if and only if $\operatorname{rank}(B) > p - q_d$, which we assume in the following. $\phi$ is continuous on $\Fl(p, \qf)$ which is a smooth compact manifold, therefore it admits a maximum $\rho^*$. As a consequence, for any ${\S_{1:d}} \in \Fl(p, \qf)$, one has $\phi(\Sf) \leq \rho^*$, which is equivalent to $~{\sum_{k=1}^d \tr{\Pi_{\S_k} (A - \rho^* B)} \leq 0}$, and since the maximum is attained, lets say at $\Sf^* := (\S_1^*, \dots, \S_d^*)$, one has ${\sum_{k=1}^d \tr{\Pi_{\S_k^*} (A - \rho^* B)} = 0}$.

Using a similar reasoning as in the proof of \autoref{thm:flag_trick} (itself based on~\citet{szwagier_curse_2024}), we can show that for any $\rho \in \R$, the function $\Sf \mapsto \sum_{k=1}^d \tr{\Pi_{\S_k} (A - \rho B)}$ is maximized in ${\S_{1:d}^*} = \lrp{\operatorname{Span}(v_1, \dots, v_{q_1}), \operatorname{Span}(v_1, \dots, v_{q_2}), \dots, \operatorname{Span}(v_1, \dots, v_{q_d})}$, where $v_1, \dots, v_p$ are the eigenvectors associated with the ordered-descending eigenvalues $\ell_1, \dots, \ell_p$. The maximum is simply given by $f(\rho) := \sum_{k=1}^d \tr{\Pi_{\S_k^*} (A - \rho B)} = \sum_{k=1}^d \sum_{j=1}^{q_k} \ell_j$ and it verifies $f(\rho^*) = 0$.

Let us now show that $\rho^*$ is the unique root of $f$. Let us introduce $\ell_j(\cdot)$, the function that maps a symmetric matrix to its $j$-th largest eigenvalue, and similarly $\S_k^*(\cdot)$, the function that maps a symmetric matrix to the subspace spanned by the eigenvectors associated with the $q_k$  largest eigenvalues. Then one can show that the derivative of $\sum_{j=1}^{q_k} \ell_j(A - \rho B)$ with respect to $\rho$ is $- \tr{\Pi_{\S_k^*(A - \rho B)} B}$~\citep{ngo_trace_2012}. Therefore, the derivative of $f$ is $f'(\rho) = - \sum_{k=1}^d \tr{\Pi_{\S_k^*(A - \rho B)} B}$, which is everywhere strictly negative since $B$ is positive semidefinite and $\operatorname{rank}(B) > p - q_d$. Therefore, f is a strictly decreasing function. Hence, $\rho^*$ is the unique root of $f$.

Now, the idea is to develop a Newton method to find the root $\rho^*$. The optimal flag will then be given by the eigenvalue decomposition of $A - \rho^* B$. One has the following iteration: 
\begin{equation}
    \rho^{t+1} = \rho^t - \frac{f(\rho^t)}{f'(\rho^t)} = \frac{\sum_{k=1}^d \tr{\Pi_{\S_k(A - \rho^t B)} A}}{\sum_{k=1}^d \tr{\Pi_{\S_k(A - \rho^t B)} B}}    
\end{equation}

Finally, in short, one can use similar considerations as in~\citet{ngo_trace_2012} to initialize the algorithm.
If we assume that $B$ is positive definite, then we can find $Z\in \operatorname{GL}_p$ such that $Z\T B Z = I_p$ and $Z\T A Z = \Lambda$, where $\Lambda := \diag{\lambda_1 \geq \dots \geq \lambda_p}$ are the (ordered descending) generalized eigenvalues of the pencil $(A, B)$.
Then, using Sylvester's law of inertia, one gets that the number of negative eigenvalues of $Z\T (A - \rho B) Z$ and of $\Lambda - \rho I_p$ are the same. Therefore, for $\rho = \lambda_1$, all the eigenvalues of $A - \rho B$ are nonpositive, so $f(\rho) = \sum_{k=1}^d \sum_{j=1}^{q_k} \lambda_j (A - \rho B) \leq 0$.
Similarly, for $\rho = \lambda_{q_d}$, the $q_d$ largest eigenvalues of $A - \rho B$ are nonnegative, so in particular for all $k \in [1,d]$, the $q_k$ largest eigenvalues of $A - \rho B$ are nonnegative, so in the end $f(\rho) \geq 0$. To conclude, if $B$ is positive definite, then one has $\rho^* \in [\lambda_{q_d}(A, B), \lambda_{1}(A, B)]$. This can be used as an initialization to the Newton method, although in practice a random initialization is performed in practice in~\citet[Algorithm~4.1]{ngo_trace_2012}.


The final proposed scheme is given in Algorithm~\ref{alg:IATR}, called \textit{flag iterative trace ratio} in reference to the iterative trace ratio of~\citet{wang_trace_2007}.
\begin{algorithm}
\caption{Flag iterative trace ratio}\label{alg:IATR}
\begin{algorithmic}
\Require $A, B \in \operatorname{SPD}_p$ (can find tighter restrictions), $\quad q_1 < \dots < q_d$ (signature), $\quad \eta$ (convergence threshold)
\Ensure
$U \in \St(p, q)$ 
\State $t \gets 0, \Delta \gets \infty, \rho^0 \in \lrb{\lambda_{q}(A, B), \lambda_{1}(A, B)}$
\While{$\Delta > \eta$}
	\State $U^t, \Lambda^t \gets \operatorname{EVD}(A - \rho^t B, q)$
	\State $\rho^t \gets \frac{\sum_{k=1}^d \tr{{U^t_{q_k}}\T A U^t_{q_k}}}{\sum_{k=1}^d \tr{{U^t_{q_k}}\T B U^t_{q_k}}}$
    \State $\Delta \gets \sqrt{\sum_{k=1}^{d} \Theta(U^t_{q_k}, U^{t-1}_{q_k})^2}$
\EndWhile
\end{algorithmic}
\end{algorithm}


\subsection{Kernelization (LDA and Graph Embedding)}
Similarly to~\citet{yan_graph_2007,wang_trace_2007}, if the matrices $A$ and $B$ can be rewritten as $X L_A X\T$ and $X L_B X\T$ where $L_A, L_B \in \Sym_n$ are Laplacian matrices of some connectivity graphs associated to the dataset, then one can apply the \textit{kernel trick} to non-linearly map the data to a high-dimensional Hilbert space $(\mathcal{H}, \langle \cdot, \cdot\rangle$. Let $\mathcal{K}$ be the chosen kernel, like for instance the RBF $\mathcal{K}(x, y) = e^{- \gamma \|x - y\|^2}$. Let ${K} := (K(x_i, x_j))_{i, j \in [1, n]} \in \Sym_n$. Then, by noting $\phi: \R^p \to \mathcal{H}$ the nonlinear map $\phi(x) = \mathcal{K}_x = \mathcal{K}(x, \cdot)$, one has $\langle \phi(x), \phi(y)\rangle = \mathcal{K}(x, y)$. 

Let $K := Q \Lambda Q\T$ be an eigendecomposition of $K$, with $Q\in \O(n)$ and $\Lambda\in\operatorname{diag}(\R^n)$.
Then the trace ratio problem in the RKHS can be rewritten as:
\begin{equation}
\argmax{{\S_{1:d}} \in \operatorname{Flag}(n, q_{1:d})} \frac{\sum_{k=1}^d \tr{\Pi_{\S_k} A}}{\sum_{k=1}^d \tr{\Pi_{\S_k} B}},
\end{equation}
with 
\begin{align*}
A &= \Lambda^{1/2} Q\T L_A Q \Lambda^{1/2}\\
B &= \Lambda^{1/2} Q\T L_B Q \Lambda^{1/2}
\end{align*}

Then, noting $\Gamma := [\Gamma_1|\dots|\Gamma_d] = [U_1|\dots|U_d]$, one get for any $x\in \R^p, \Pi^*(x) = \sum_i \Gamma_{\dot i} K(x_i, x)$, and therefore $\Pi^*(X) = \Gamma\T K \in \R^{q \times n}$ and for any new data points, $x'_1, \dots, x'_m \in \R^p$, writing ${K}' = (K(x'_i, x_j))_{i \in [1, m], j \in [1, n]} \in \R^{m\times n}$, one has ${\Pi^*}(X') = \Gamma\T {K'}\T \in \R^{q \times m}$.



\subsection{Proof of Proposition~\ref{prop:TR}}
Let $\Sf \in \Fl(p, \qf)$ and $U_{1:d+1} := [U_1|U_2|\dots|U_d|U_{d+1}] \in \O(p)$ be an orthogonal representative of $\Sf$. One has:
\begin{align}
	\frac{\tr{\Pi_{\S_{1:d}} A}}{\tr{\Pi_{\S_{1:d}} B}} &= \frac{\tr{\frac1d \sum_{k=1}^d \Pi_{\S_k} A}}{\tr{\frac1d \sum_{l=1}^d \Pi_{\S_l} B}},\\
	&= \frac{\tr{\sum_{k=1}^{d+1} (d - (k-1)) U_k {U_k}\T A}}{\tr{\sum_{l=1}^{d+1} (d - (l-1)) U_l {U_l}\T B}},\\
	\frac{\tr{\Pi_{\S_{1:d}} A}}{\tr{\Pi_{\S_{1:d}} B}} &= \frac{\sum_{k=1}^{d+1} (d - (k-1)) \tr{U_k {U_k}\T A}}{\sum_{l=1}^{d+1} (d - (l-1)) \tr{U_l {U_l}\T B}},
\end{align}
which concludes the proof.