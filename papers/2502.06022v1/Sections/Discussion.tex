\section{Discussion}\label{sec:discussion}

We introduced a general method to make subspace learning problems multilevel and nested.
The nestedness issue of Grassmannian-based methods was clearly demonstrated on a variety of machine learning problems, yielding inconsistent scatter plots and paradoxical learning curves. The flag trick was shown to both solve the nestedness issue and provide solutions that are close from the original objective of the problems. As a positive side-effect, the flag trick sometimes learned substantially better subspaces than the original subspace problems, due to its multilevel nature which enables to aggregate information from several dimensions.
When combined with ensemble learning, the flag trick showed improvement over subspace-based predictors that work at a fixed dimension, and raised interesting dimension-blending perspectives that challenge the manifold hypothesis. 
The code for reproducing the experiments can be found at \url{https://github.com/tomszwagier/flag-trick}.

One major limitation of this paper is the lack of in-depth study of the specific interest of the flag trick to each subspace learning method. We indeed focus on general practical results---the nestedness of flag-based methods with respect to classical subspace methods and the interest of ensembling dimensions instead of considering each dimension individually---and not on some application-specific questions, like the robustness of the flag to outliers for robust subspace recovery, the quality of the embedding for trace ratio problems, and the quality of the clustering for spectral clustering problems. We therefore strongly encourage the specialists to investigate with us the theoretical and practical interest of the flag trick for their subspace problems.
We also encourage the Riemannian optimization community to develop with us new optimization algorithms on flag manifolds that are adapted to each subspace learning problem---for instance with robustness or sparsity-inducing properties.
To that extent, we end this paper with a list of perspectives.

First, one should perform in-depth experiments to show the interest and the limitations of the flag trick for each specific subspace learning problem among---but not limited to---the list of examples developed in \autoref{sec:examples}, as well as their local, kernel and tangent space generalizations. 
Second, one should experiment with more complex and more efficient optimization algorithms on flag manifolds such as the ones developed in Appendix (IRLS in \autoref{app:RSR} and Newton in \autoref{app:TR}) or the ones proposed in~\citet{ye_optimization_2022,zhu_practical_2024} and develop new ones, like ADMM for sparse optimization.
Third, we derived a very general principle for transitioning from fixed-dimension subspace methods to multi-dimensional flag methods, but this principle could be revisited specifically for each problem. This includes the problems that are not specifically formulated as a Grassmannian optimization, as long as they somewhat involve low-dimensional subspaces, like in domain adaptation~\citep{gopalan_domain_2011, boqing_gong_geodesic_2012} and sparse subspace clustering~\citep{elhamifar_sparse_2013}.
Fourth, the question of how to choose the flag signature (i.e. the sequence of dimensions to try) remains open.
A first step for PCA has been achieved with the principal subspace analysis~\citep{szwagier_curse_2024}, where the dimensions are chosen based on the relative eigengaps of the covariance matrix. Other generative models should be designed specifically for each subspace learning problem in order to develop some appropriate guidelines to select the flag signature.