\section{Introduction}\label{sec:intro}

Finding low-dimensional representations of datasets is a quite common objective in machine learning, notably in virtue of the \textit{curse of dimensionality}~\citep{bellman_dynamic_1984}.
A classical way of building such a representation is by searching for a low-dimensional subspace that well represents the data, where ``well'' is defined by an application-dependent criterion~\citep{cunningham_linear_2015}. Here is a non-exhaustive list of examples of subspace learning problems.
\textit{Principal component analysis} (PCA)~\citep{jolliffe_principal_2002} searches for a low-dimensional subspace that minimizes the average squared Euclidean distance to the data.
\textit{Robust subspace recovery} (RSR)~\citep{lerman_overview_2018} minimizes the average (absolute) Euclidean distance to the data, which is less sensitive to outliers.
Many matrix decomposition methods, like \textit{robust PCA}~\citep{candes_robust_2011} and \textit{matrix completion}~\citep{keshavan_matrix_2010,candes_exact_2012} look for low-rank approximations of the data matrix, which can be decomposed into a product of subspace and coordinate matrices.
\textit{Trace ratio} (TR)~\citep{ngo_trace_2012} refers to a wide class of problems that look for subspaces making a tradeoff between desired yet antagonist properties of low-dimensional embeddings---for instance Fisher's \textit{linear discriminant analysis (LDA)}~\citep{fisher_use_1936} which seeks to maximize the between-class variance while minimizing the within-class variance.
\textit{Domain adaptation} methods learn some domain-invariant subspaces~\citep{baktashmotlagh_unsupervised_2013} by minimizing the projected maximum mean discrepancy (MMD)~\citep{gretton_kernel_2012} between the source and target distributions.
\textit{Subspace tracking} methods incrementally minimize a distance between the current subspace and the available data~\citep{balzano_online_2010}.
Subspaces can also be estimated not from the data but from their adjacency matrix or graph Laplacian, as done in the celebrated \textit{Laplacian eigenmaps}~\citep{belkin_laplacian_2003} and \textit{spectral clustering}~\citep{ng_spectral_2001}.
Subspaces can also be estimated beyond Euclidean data, for instance on symmetric positive definite (SPD) matrices datasets~\citep{harandi_dimensionality_2018}.
In all the previously cited examples, the search space---the space of all linear subspaces of dimension $q$ embedded in an ambient space of larger dimension $p$---is called the \textit{Grassmannian}, or the \textit{Grassmann manifold}~\citep{bendokat_grassmann_2024} and is denoted $\Gr(p, q)$. 
Consequently, many machine learning methods can be recast as an optimization problem on Grassmann manifolds.\footnote{
Some alternative methods to subspace learning via Grassmannian optimization exist.
Notably, a large family of methods---like \textit{partial least squares} (PLS)~\citep{geladi_partial_1986}, \textit{independent component analysis} (ICA)~\citep{hyvarinen_independent_2000}, \textit{canonical correlation analysis} (CCA)~\citep{hardoon_canonical_2004} or \textit{projection pursuit}~\citep{huber_projection_1985}---builds a low-dimensional subspace out of the data \textit{sequentially}. They first look for the best 1D approximation and then recursively add dimensions one-by-one via deflation schemes or orthogonality constraints~\citep[Section~II.C]{lerman_overview_2018}.
Those sequential methods however suffer from many issues---from a subspace estimation point of view---due to their greedy nature. Those limitations are well described in~\citet[Section~III.B]{lerman_overview_2018} and can be extended beyond robust subspace recovery.
In light of the fundamental differences between those methods and the previously described subspace methods, we decide not to address sequential methods in this paper.
\label{footnote:sequential}
}

Learning a low-dimensional subspace from the data through Grassmannian optimization requires to choose the dimensionality $q$ \textit{a priori}. 
This prerequisite has many important limitations, not only since a wrong choice of dimension might remove all the theoretical guarantees of the method~\citep{lerman_overview_2018}, but also because the observed dataset might simply {not} have a favored dimension, above which the added information is invaluable.\footnote{Many generative models assume that the data lies on a lower dimensional subspace, up to isotropic Gaussian noise~\citep{tipping_probabilistic_1999,lerman_robust_2015}. However, anisotropy is much more realistic~\citep{maunu_well-tempered_2019}, which nuances the theoretical guarantees and removes hopes for a clear frontier between signal and noise at the intrinsic dimension.}
For these reasons, one might be tempted to ``try'' several dimensions---i.e. run the subspace optimization algorithm with different dimensions and choose the best one \textit{a posteriori}, with cross-validation or statistical model selection techniques for instance.
In addition to being potentially costly, such a heuristic raises an important issue that we later refer to as the \textit{nestedness issue}: the optimal subspaces at different dimensions are \textit{not nested} in general.
This notably means that the data embeddings at two different dimensions might drastically differ, which is a pitfall for data analysis. For more details about the importance of nestedness in statistics, one can refer to~\citet{huckemann_intrinsic_2010, jung_analysis_2012, damon_backwards_2014, huckemann_backward_2018,pennec_barycentric_2018, lerman_overview_2018, dryden_principal_2019, yang_nested_2021, fan_nested_2022}.
We illustrate in Figure~\ref{fig:motivations} the nestedness issue on toy datasets related to three important machine learning problems: robust subspace recovery, linear discriminant analysis and sparse spectral clustering~\citep{lu_convex_2016,wang_grassmannian_2017}.\footnote{The methodology will be provided in due time.}
\begin{figure}
\centering
\includegraphics[width=\linewidth]{Fig/FT_intro.pdf}
\caption{Illustration of the subspace nestedness issue in three important machine learning problems: robust subspace recovery (left), linear discriminant analysis (middle) and sparse spectral clustering (right). For each dataset (top), we plot its projection onto the optimal 1D subspace (middle) and 2D subspace (bottom) obtained by solving the associated Grassmannian optimization problem. The 1D and 2D representations are \textit{inconsistent}---in the sense that the 1D plot is not the projection of the 2D plot onto the horizontal axis---which is a pitfall for data analysis.}
\label{fig:motivations}
\end{figure}
We can see that the scatter plots of the projected data for $q=1$ and $q=2$ are \textit{inconsistent}---in that the 1D representation is not the projection of the 2D representation onto the horizontal axis.



In this paper, we propose a natural solution to the nestedness issue with a geometrical object that has not been much considered yet in machine learning: \textit{flags}---i.e. sequences of nested subspaces of increasing dimension.
The idea is to first select a sequence of candidate dimensions---the \textit{signature} of the flag---then formulate a multilevel subspace learning criterion---with the \textit{flag trick}---and finally optimize over the space of flags of the chosen signature, which has a manifold structure similar to the one of Grassmannians.
The output subspaces are naturally nested, which solves the nestedness issue of subspace learning methods and provides the subspaces with a \textit{hierarchy} that is good for interpretability~\citep[Section~7]{huber_projection_1985}.
Moreover, the nested representations of the data can be fit to general machine learning algorithms and combined via \textit{ensembling} methods, in which case the weights can be interpreted as a measure of importance for the different dimensions, echoing the perspectives of the celebrated paper on the automatic choice of dimensionality in PCA~\citep{minka_automatic_2000}.
Beyond the change of paradigm (from subspaces to flags of subspaces), the main contribution of the paper is the \textit{flag trick}: a generic method to convert a \textit{fixed-dimension} subspace criterion into a \textit{multilevel} flag criterion, without any nestedness issue.


The paper is organized as follows.
In \autoref{sec:flags}, we provide some reminders on flag manifolds and describe a steepest descent optimization algorithm that we will use throughout this work.
In \autoref{sec:flag_trick}, we introduce the flag trick and propose an ensemble learning algorithm to leverage the hierarchical information of the flag into richer machine learning models.
In \autoref{sec:examples}, we show the interest of the flag trick to several subspace learning methods such as robust subspace recovery, trace ratio and spectral clustering.
In \autoref{sec:discussion}, we conclude and discuss the limits and perspectives of such a framework.
In appendix (\autoref{app:RSR}, \autoref{app:TR}, \autoref{app:SSC}), we show how the flag trick can be used to develop two \textit{advanced} optimization algorithms on flag manifolds (namely an iteratively reweighted least squares (IRLS) method for robust subspace recovery and a Newton-Lanczos method for trace ratio problems) that go beyond the limitations of simple algorithms like the steepest descent, and we provide the proofs that are not in the main body.