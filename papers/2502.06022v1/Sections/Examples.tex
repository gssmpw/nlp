\section{The flag trick in action}\label{sec:examples}
In this section, we provide some applications of the flag trick to several learning problems. We choose to focus on subspace recovery, trace ratio and spectral clustering problems. Other ones, like domain adaptation, matrix completion and subspace tracking are developed or mentioned in the last subsection but not experimented for conciseness.

\subsection{Outline and experimental setting}
For each application, we first present the learning problem as an optimization on Grassmannians. Second, we formulate the associated flag learning problem by applying the flag trick (Definition~\ref{def:flag_trick}). Third, we optimize the problem on flag manifolds with the steepest descent method (Algorithm~\ref{alg:GD})---more advanced algorithms are also derived in the appendix. 
Finally, we perform various nestedness and ensemble learning experiments via Algorithm~\ref{alg:flag_trick} on both synthetic and real datasets.

The general methodology to compare Grassmann-based methods to flag-based methods is the following one. For each experiment, we first choose a flag signature $~{q_{1\rightarrow d} := (q_1, \dots, q_d)}$, then we run independent optimization algorithms on $\Gr(p, q_1), \dots, \Gr(p, q_d)$~\eqref{eq:subspace_problem} and finally we compare the optimal subspaces $\S_k^* \in \Gr(p, q_k)$ to the optimal flag of subspaces $\Sf^* \in \Fl(p, q_{1\rightarrow d})$ obtained via the flag trick~\eqref{eq:flag_problem}. 
To show the nestedness issue in Grassmann-based methods, we compute the subspace distances $\Theta(\S_k^*, \S_{k+1}^*)_{k=1\dots d-1}$, where $\Theta$ is the generalized Grassmann distance of~\citet[Eq.~(14)]{ye_schubert_2016}. It consists in the $\ell_2$ norm of the principal angles, which can be obtained from the singular value decomposition (SVD) of the inner-products matrices ${U_k}\T {U_{k+1}}$, where $U_k \in \St(p, q_k)$ is an orthonormal basis of $\S_k^*$.

Regarding the implementation of the steepest descent algorithm on flag manifolds (Algorithm~\ref{alg:GD}), we develop a new class of manifolds in \href{https://pymanopt.org/}{PyManOpt}~\citep{boumal_manopt_2014,townsend_pymanopt_2016}, and run their \href{https://github.com/pymanopt/pymanopt/blob/master/src/pymanopt/optimizers/steepest_descent.py}{SteepestDescent} algorithm. Our implementation of the \texttt{Flag} class is based on the Stiefel representation of flag manifolds, detailed in \autoref{sec:flags}, with the retraction being the polar retraction. For the computation of the gradient, we use automatic differentiation with the \texttt{\href{https://github.com/HIPS/autograd}{autograd}} package. We could derive the gradients by hand from the expressions we get, but we use automatic differentiation as strongly suggested in PyManOpt's \href{https://pymanopt.org/docs/stable/quickstart.html}{documentation}.
Finally, the real datasets and the machine learning methods used in the experiments can be found in \href{https://scikit-learn.org/stable/}{scikit-learn}~\citep{pedregosa_scikit-learn_2011}.

\input{Examples/RSR}
\input{Examples/TR}
\input{Examples/SSC}
\input{Examples/Other}