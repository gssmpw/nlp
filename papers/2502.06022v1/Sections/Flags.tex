\section{Reminders on flag manifolds}\label{sec:flags}

A flag is a sequence of nested linear subspaces of increasing dimension.
This section introduces flag manifolds and provides the minimal tools to perform optimization on those spaces. Much more details and properties can be obtained in dedicated papers~\citep{ye_optimization_2022}.

\subsection{Flags in the scientific literature}
By providing a natural parametrization for the eigenspaces of symmetric matrices, flags have long been geometrical objects of interest in the scientific community, with traditional applications in physics \citep{arnold_modes_1972} and linear algebra \citep{ammar_geometry_1986}.
Modern computational methods on flag manifolds arose later with the seminal work of \citet{edelman_geometry_1998} who provided some optimization algorithms on matrix manifolds such as Grassmannians and Stiefel manifolds.
Later, \citet{nishimori_riemannian_2006} provided an optimization algorithm on flag manifolds for independent subspace analysis \citep{cardoso_multidimensional_1998, hyvarinen_emergence_2000}.
Recently, flag manifolds played a central role in the investigation of a so-called \textit{curse of isotropy} in principal component analysis~\citep{szwagier_curse_2024}.

Flags as a sequence of nested subspaces were first introduced in the machine learning literature as robust prototypes to represent collections of subspaces of different dimensions~\citep{draper_flag_2014,mankovich_flag_2022}.
They were obtained via a sequential construction (where one dimension is added at a time), which can be problematic for greediness reasons~\citep{huber_projection_1985,lerman_overview_2018}.
\citet{pennec_barycentric_2018} was the first to show that PCA can be reformulated as an optimization problem on flag manifolds by summing the unexplained variance at different dimensions. This principle was recently applied to several variants of PCA under the name of \textit{flagification} in~\citet{mankovich_fun_2024}, showing an important robustness to outliers.
Finally, some works compute distance-related quantities on flag-valued datasets, with notable applications in computer vision and hyperspectral imaging~\citep{ma_flag_2021, nguyen_closed-form_2022, mankovich_chordal_2023, szwagier_rethinking_2023}.


\subsection{Definition and representation of flag manifolds}
Let $p \geq 2$ %be an ambient dimension 
and $q_{1:d} := (q_1, q_2 ,\cdots, q_d)$ be a sequence of increasing integers such that $~{0 < q_1 < q_2 < \cdots < q_d < p}$.
A \textit{flag} of \textit{signature} $(p, q_{1:d})$ is a sequence of nested {linear} subspaces $\{0\} \subset \S_1 \subset \S_2 \subset \dots \subset \S_d \subset \R^p$ of respective dimension $q_1, q_2, \dots, q_d$, noted here $\S_{1:d} := (\S_1, \dots, \S_d)$.\footnote{Flags can equivalently be defined as sequences of \textit{mutually orthogonal} subspaces $\V_1 \perp \V_2 \dots \perp \V_d$, of respective dimension $q_1, q_2-q_1, \dots, q_d - q_{d-1}$, by taking $\V_k$ to be the orthonormal complement of $\S_{k-1}$ onto $\S_k$. This definition is more convenient for computations, but it won't be the one used in this paper.}
A flag $\S_{1:d}$ can be canonically represented as a sequence of symmetric matrices that are the \textit{orthogonal projection matrices} onto the nested subspaces, i.e. $\S_{1:d} \cong (\Pi_{\S_1}, \dots, \Pi_{\S_d})\in\Sym_p^d$. We call it the \textit{projection representation} of flags.

The set of flags of signature $(p, q_{1:d})$ is a smooth manifold~\citep{ye_optimization_2022}, denoted here $\Fl(p, q_{1:d})$. 
Flag manifolds generalize {Grassmannians}---since $~{\Fl(p, (q,)) = \Gr(p, q)}$---and therefore share many practical properties that are useful for optimization~\citep{edelman_geometry_1998}. In the following, we will frequently use the following notations: $q_0 := 0$, $q := q_d$ and $q_{d+1} := p$.


For computational and numerical reasons, flags are often represented as orthonormal $q$-frames. Those correspond to points on the Stiefel manifold $\St(p, q) = \{U\in\R^{p\times q}\colon U\T U = I_q\}$).
Let us define sequentially, for $k\in[1, d]$, $U_k\in\St(p, q_k - q_{k-1})$ such that $[U_1|\dots|U_k]$ is an orthonormal basis of $\S_k$ (this is possible thanks to the nestedness of the subspaces).
Then, $U_{1:d} := [U_1|\dots|U_d]\in\St(p, q)$ is a representative of the flag $\S_{1:d}$. We call it the \textit{Stiefel representation} of flags. Such a representation is not unique---contrary to the projection representation defined previously---due to the \textit{rotational-invariance} of orthonormal bases of subspaces. More precisely, if $U_{1:d}$ is a Stiefel representative of the flag $\S_{1:d}$, then for any set of orthogonal matrices $R_k\in\O(q_k - q_{k-1})$, the matrix $U'_{1:d} := [U_1 R_1|\dots|U_d R_d]$ spans the same flag of subspaces $\S_{1:d}$.
This provides flag manifolds with a quotient manifold structure~\citep{edelman_geometry_1998, absil_optimization_2009, ye_optimization_2022}: 
\begin{equation}\label{eq:Fl_quotient}
	\Fl(p, q_{1:d}) \cong \St(p, q) \big/ \lrp{\O(q_1) \times \O(q_2 - q_1) \times \dots \times \O(q_d - q_{d-1})}.
\end{equation}

\begin{remark}[Orthogonal representation]
For computations, one might have to perform the orthogonal completion of some Stiefel representatives.
Let $\Uf := [U_1|\dots|U_d] \in \St(p, q)$, then one denotes $U_{d+1} \in \St(p, p-q_d)$ to be any orthonormal basis such that $U_{1:d+1} := [U_1|\dots|U_d|U_{d+1}] \in \O(p)$. Such an orthogonal matrix $U_{1:d+1}$ will be called an \emph{orthogonal representative} of the flag $\Sf$. In the following, we may abusively switch from one representation to the other since they represent the same flag.
\end{remark}

\subsection{Optimization on flag manifolds}
There is a rich literature on optimization on smooth manifolds~\citep{edelman_geometry_1998,absil_optimization_2009,boumal_introduction_2023}, and the particular case of flag manifolds has been notably addressed in~\citet{ye_optimization_2022,zhu_practical_2024}. Since flag manifolds can be represented as quotient spaces of Stiefel manifolds, which themselves are embedded in a Euclidean matrix space, one can develop some optimization algorithms without much difficulty.
In this paper, we will use a \textit{steepest descent} algorithm, which is drawn from several works~\citep{chikuse_statistics_2003,nishimori_riemannian_2006,ye_optimization_2022,zhu_practical_2024}. 
Let $f\colon \Fl(p, \qf) \to \R$ be a smooth function on a flag manifold, expressed in the Stiefel representation (e.g. $f(\Uf) = \sum_{k=1}^d \norm{{U_k} {U_k}\T x} $ for some $x\in\R^p$). Given $\Uf \in \St(p, q)$, let $\operatorname{Grad} f (\Uf) = ({\partial f}/{\partial U_{ij}})_{i, j = 1}^{p, q}$ denote the (Euclidean) gradient of $f$. To ``stay'' on the manifold, one first computes the \textit{Riemannian gradient} of $f$ at $\Uf$, noted $\nabla f (\Uf)$. It can be thought of as a projection of the Euclidean gradient onto the tangent space and computed explicitly~\citep{nishimori_riemannian_2006,ye_optimization_2022}.
Then, one moves in the opposite direction of $\nabla f (\Uf)$ with a so-called \textit{retraction}, which is chosen to be the polar retraction of~\citet[Eq.~(49)]{zhu_practical_2024}, combined with a line-search.
We iterate until convergence.
The final steepest descent algorithm is described in Algorithm~\ref{alg:GD}.
\begin{algorithm}
\caption{Steepest descent on flag manifolds}\label{alg:GD}
\begin{algorithmic}
\Require $f\colon \Fl(p, \qf) \to \R$ a function, $\Uf \in \Fl(p, \qf)$ a flag (Stiefel representation)
\For{$t$ = 1, 2, \dots}
    \State $[G_1|\dots|G_d] \gets \operatorname{Grad} f (\Uf)$ \Comment{Euclidean gradient}
	\State $\nabla \gets \bigl[G_k - \bigl(U_k {U_k}\T G_k + \sum_{l \neq k} U_l {G_l}\T U_k\bigr)\bigr]_{k=1\dots d}$
	\Comment{Riemannian gradient}
	\State $\Uf \gets \operatorname{polar}(U_{1:d+1} - \alpha \nabla)$ \Comment{polar retraction + line search}
\EndFor
\Ensure $U_{1:d}^* \in \Fl(p, \qf)$ an optimal flag
\end{algorithmic}
\end{algorithm}
\begin{remark}[Initialization]
We can initialize Algorithm~\ref{alg:GD} randomly via~\citet[Theorem~1.5.5]{chikuse_statistics_2003}, or choose a specific flag depending on the application, as we will see in \autoref{sec:examples}.
\end{remark}
\begin{remark}[Optimization variants]\label{rk:optim}
Many extensions of Algorithm~\ref{alg:GD} can be considered: conjugate gradient~\citep{ye_optimization_2022}, Riemannian trust region~\citep{absil_optimization_2009} etc. We can also replace the polar retraction with a geodesic step~\citep{ye_optimization_2022} or other retractions~\citep{zhu_practical_2024}.
\end{remark}