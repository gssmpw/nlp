 \section{The flag trick in theory}\label{sec:flag_trick}
In this section, we motivate and introduce the flag trick to make subspace learning methods nested.
The key result is \autoref{thm:flag_trick}, which states that the classical PCA at a fixed dimension can be converted into a nested multilevel method using nested projectors.

In the remaining of the paper, we assume that the data has been already \textit{centered} around a point of interest (e.g. its mean or geometric median), so that we are only interested in fitting \textit{linear} subspaces and not \textit{affine} ones. 
One could directly include the center in the optimization variables---in which case the geometry would be the one of affine Grassmannians~\citep{lim_numerical_2019} or affine flags~\citep{pennec_barycentric_2018}---but we don't do it in this work for conciseness.


\subsection{From subspaces to flags of subspaces: the seminal example of PCA}
PCA is known as the eigendecomposition of the sample covariance matrix. Originally, it can be formulated as the search for a low dimensional subspace that minimizes the unexplained variance (or maximizes the explained variance).
Let $X:=[x_1|\dots|x_n] \in \R^{p\times n}$ be a data matrix with $n$ samples, let $\S \in \Gr(p, q)$ be a $q$-dimensional subspace, and let $\Pi_{\S} \in \R^{p\times p}$ be the orthogonal projection matrix onto $\S$.
Then PCA consists in the following optimization problem on Grassmannians:
\begin{equation}\label{eq:PCA_subspace}
\S_q^* = \argmin{\S \in \Gr(p, q)} \norm{X - \Pi_{\S} X}_F^2,
\end{equation}
where $\norm{M}_F^2 := \tr{M\T M}$ denotes the \textit{Frobenius norm}.
The solution to the optimization problem is the $q$-dimensional subspace spanned by the leading eigenvectors of the sample covariance matrix $S := \frac 1 n X X\T$, that we note $\S_q^* = \operatorname{Span}(v_1, \dots, v_q)$. 
It is unique when the sample eigenvalues $q$ and $q+1$ are distinct, which is almost sure when $q \leq \operatorname{rank}(S)$. We will assume to be in such a setting in the following for simplicity but it can be easily handled otherwise by ``grouping'' the repeated eigenvalues (cf.~\citet[Theorem~B.1]{szwagier_curse_2024}).
In such a case, the principal subspaces are \textit{nested} for increasing $q$, i.e., if $\S_q^*$ is the $q$-dimensional principal subspace, then for any $r > q$, one has $\S_q^* = \operatorname{Span}(v_1, \dots, v_q) \subset \operatorname{Span}(v_1, \dots, v_r) = \S_r^*$.

Another way of performing PCA is in a sequential manner (cf. footnote~\ref{footnote:sequential}). We first estimate the 1D subspace $\mathcal{V}_1^*$ that minimizes the unexplained variance, then estimate the 1D subspace $\mathcal{V}_2^*$ that minimizes the unexplained variance while being orthogonal to the previous one, and so on and so forth. This gives the following \textit{constrained} optimization problem on 1D Grassmannians:
\begin{equation}\label{eq:PCA_sequential}
\mathcal{V}_q^* = \argmin{\substack{\mathcal{V} \in \Gr(p, 1)\\ \mathcal{V} \perp \mathcal{V}_{q-1} \perp \dots \perp \mathcal{V}_1}} \norm{X - \Pi_{\mathcal{V}} X}_F^2.
\end{equation}
This construction naturally yields a sequence of nested subspaces of increasing dimension---i.e. a flag of subspaces---best and best approximating the dataset:
\begin{equation}
	\{0\} \subset \S_1^* \subset \S_2^* \subset \dots \subset \S_{p-1}^* \subset \R^p, \text{with } \S_k^* = \bigoplus_{l=1}^k \mathcal{V}_l^*.
\end{equation}
Those subspaces happen to be exactly the same as the ones obtained by solving the subspace learning optimization problem~\eqref{eq:PCA_subspace}, although the way they are obtained (in a greedy manner) is different. This is generally not the case for other dimension reduction problems (for instance in robust subspace recovery, as it is raised in the final open questions of~\citet{lerman_overview_2018}).

Hence, the optimal solution to the \textit{subspace} learning formulation of PCA~\eqref{eq:PCA_subspace} is equivalent to the \textit{sequential} formulation of PCA~\eqref{eq:PCA_sequential}, and both yield a flag of subspaces best and best approximating the data. One can wonder if this result could be directly obtained by formulating an optimization problem on flag manifolds. The answer is \textit{yes}, as first proven in~\citet[Theorem~9]{pennec_barycentric_2018} with an \textit{accumulated unexplained variance} (AUV) technique, but there is not a unique way to do it. Motivated by the recent principal subspace analysis~\citep{szwagier_curse_2024}, we propose in the following theorem a generic trick to formulate PCA as an optimization on flag manifolds.
\begin{theorem}[Nested PCA with flag manifolds]\label{thm:flag_trick}
	Let $X := [x_1|\dots|x_n]\in\R^{p\times n}$ be a centered $p$-dimensional ($p \geq 2$) dataset with $n$ samples. Let $q_{1:d} := (q_1, q_2 ,\cdots, q_d)$ be a sequence of increasing dimensions such that $0 < q_1 < q_2 < \cdots < q_d < p$.
	Let $S := \frac 1 n X X\T$ be the sample covariance matrix. Assume that it eigendecomposes as $S := \sum_{j=1}^p \ell_j v_j {v_j}\T$ where $\ell_1 \geq \dots \geq \ell_p$ are the eigenvalues and $v_1 \perp \dots \perp v_p$ are the associated eigenvectors.
	Then PCA can be reformulated as the following optimization problem on flag manifolds:
	\begin{equation}
		{\S_{1:d}^*} = \argmin{\S_{1:d} \in \Fl(p, q_{1:d})} \norm{X - \frac 1 d \sum_{k=1}^d \Pi_{\S_k} X}_F^2.
	\end{equation}
	More precisely, one has ${\S_{1:d}^*} = \lrp{\operatorname{Span}(v_1, \dots, v_{q_1}), \operatorname{Span}(v_1, \dots, v_{q_2}), \dots, \operatorname{Span}(v_1, \dots, v_{q_d})}$.
	The solution is unique if and only if $\ell_{q_k} \neq \ell_{q_{k+1}}, \forall k\in[1, d]$.
\end{theorem}
\begin{proof} 
One has:
\begin{align}
	\norm{X - \frac 1 d \sum_{k=1}^d \Pi_{\S_k} X}_F^2
	&= \tr{X\T \lrp{I_p - \frac1d\sum_{k=1}^d \Pi_{\S_k}}^2 X},\\
	&= \frac1{d^2} \tr{X\T \lrp{\sum_{k=1}^d \lrp{I_p - \Pi_{\S_k}}}^2 X},\\
	&= \frac n {d^2} \tr{W^2 S},
\end{align}
with $W = \sum_{k=1}^d (I_p - \Pi_{\S_k})$ and $S = \frac 1 n X X\T$.
	Let $U_{1:d+1} := [U_1|\dots|U_d|U_{d+1}]\in\O(p)$ be an orthogonal representative (cf. \autoref{sec:flags}) of the optimization variable $\S_{1:d} \in \Fl(p, q_{1:d})$. Then one has $~{\Pi_{\S_k} = U_{1:d+1} \diag{I_{q_k}, 0_{p - q_k}} {U_{1:d+1}}\T}$. Therefore, one has $W = U_{1:d+1} \Lambda {U_{1:d+1}}\T$, with $~{\Lambda = \diag{0 \, I_{q_1}, 1 \, I_{q_2 - q_1}, \dots, d \, I_{q_{d+1} - q_d}}}$.
	Hence, one has 
	\begin{equation}
		\argmin{\S_{1:d} \in \Fl(p, q_{1:d})} \norm{X - \frac 1 d \sum_{k=1}^d \Pi_{\S_k} X}_F^2 \Longleftrightarrow \argmin{U \in \O(p)} \frac n {d^2} \tr{U \Lambda^2 U\T S}.
	\end{equation}
	The latter problem is exactly the same as in~\citet[Equation~(19)]{szwagier_curse_2024}, which solves maximum likelihood estimation for principal subspace analysis.
	Hence, one can conclude the proof on existence and uniqueness using~\citet[Theorem~B.1]{szwagier_curse_2024}.
\end{proof}
The key element of the proof of \autoref{thm:flag_trick} is that averaging the nested projectors yields a \textit{hierarchical reweighting} of the (mutually-orthogonal) principal subspaces. More precisely, the $k$-th principal subspace has weight $(k-1)^2$, and this reweighting enables to get a hierarchy of eigenspaces~\citep{cunningham_linear_2015,pennec_barycentric_2018,oftadeh_eliminating_2020}.
In the following, we note $~{\Pi_{\S_{1:d}} := \frac 1 d \sum_{k=1}^d \Pi_{\S_k}}$ and call this symmetric matrix the \textit{average multilevel projector}, which will be central in the extension of subspace methods into multilevel subspace methods.



\subsection{The flag trick}
As we will see in the following (\autoref{sec:examples}), many important machine learning problems can be formulated as the optimization of a certain function $f$ on Grassmannians.
\autoref{thm:flag_trick} shows that replacing the subspace projection matrix $\Pi_{\S}$ appearing in the objective function by the average multilevel projector $\Pi_{\Sf}$ yields a sequence of subspaces that meet the original objective of principal component analysis, while being nested.
This leads us to introduce the \textit{flag trick} for general subspace learning problems.
\begin{definition}[Flag trick]\label{def:flag_trick}
Let $p \geq 2$, $0 < q < p$ and $q_{1:d} := (q_1, q_2 ,\cdots, q_d)$ be a sequence of increasing dimensions such that $0 < q_1 < q_2 < \cdots < q_d < p$.
The flag trick consists in replacing a subspace learning problem of the form:
\begin{equation}\label{eq:subspace_problem}
    \argmin{\S \in \Gr(p, q)} f(\Pi_\S)
\end{equation}
with the following optimization problem:
\begin{equation}\label{eq:flag_problem}
    \argmin{\S_{1:d} \in \Fl(p, q_{1:d})} f\lrp{\frac 1 d \sum_{k=1}^d \Pi_{\S_k}}.
\end{equation}
\end{definition}
Except for the very particular case of PCA (\autoref{thm:flag_trick}) where $f_X(\Pi) = \norm{X - \Pi X}_F^2$, we cannot expect to have an analytic solution to the flag problem~\eqref{eq:flag_problem}; indeed, in general, subspace problems do not even have a closed-form solution as we shall see in \autoref{sec:examples}. This justifies the introduction of optimization algorithms on flag manifolds like Algorithm~\ref{alg:GD}. 

\begin{remark}[Flag trick vs. AUV]
The original idea of accumulated unexplained variance~\citep{pennec_barycentric_2018} (and its subsequent application to several variants of PCA under the name of ``flagification''~\citep{mankovich_fun_2024}) consists in summing the subspace criteria at different dimensions, while the flag trick directly averages the orthogonal projection matrices that appear inside the objective function.
While both ideas are equally worth experimenting with, we believe that the flag trick has a much wider reach. Indeed, from a technical viewpoint, the flag trick appears at the covariance level and directly yields a hierarchical reweighting of the principal subspaces. This reweighting is only indirect with the AUV---due to the linearity of the trace operator---and is not expected to happen for other methods than PCA. Notably, as we shall see in \autoref{sec:examples}, the flag trick enables to easily develop extensions of well-known methods involving PCA, like IRLS~\citep{lerman_fast_2018} or Newton-Lanczos methods for trace ratio optimization~\citep{ngo_trace_2012}, and is closer in spirit to the statistical formulations of PCA~\citep{szwagier_curse_2024}.
\end{remark}


\subsection{Multilevel machine learning}
Subspace learning is often used as a preprocessing task before running a machine learning algorithm, notably to overcome the curse of dimensionality. One usually projects the data onto the optimal subspace $\S^* \in \Gr(p, q)$ and use the resulting lower-dimensional dataset as an input to a machine learning task like clustering, classification or regression~\citep{bouveyron_model-based_2019}. Since the flag trick problem~\eqref{eq:flag_problem} does not output one subspace but a hierarchical sequence of nested subspaces, it is legitimate to wonder what to do with such a multilevel representation.
In this subsection, we propose a general \textit{ensemble learning} method to aggregate the hierarchical information coming from the flag of subspaces.

Let us consider a dataset $X := [x_1|\dots|x_n]\in\R^{p \times n}$ (possibly with some associated labels $Y := [y_1|\dots|y_n]\in\R^{m \times n}$). In machine learning, one often fits a model to the dataset by optimizing an objective function of the form $R_{X, Y}(g) = \frac{1}{n} \sum_{i=1}^n L(g(x_i), y_i)$.
With the flag trick, we get a filtration of projected data points $Z_k = \Pi_{\S_k^*} X, k\in[1,d]$ that can be given as an input to different machine learning algorithms. This yields optimal predictors $g_k^* = \operatorname{argmin} \, R_{Z_k, Y}$ which can be aggregated via \href{https://scikit-learn.org/stable/modules/ensemble.html}{ensembling methods}.
For instance, \textit{voting} methods choose the model with the highest performance on holdout data; this corresponds to selecting the optimal dimension $q^* \in q_{1:d}$ \textit{a posteriori}, based on the machine learning objective. A more nuanced idea is the one of \textit{soft voting}, which makes a weighted averaging of the predictions. The weights can be uniform, proportional to the performances of the individual models, or learned to maximize the performance of the weighted prediction~\citep{perrone_when_1992}. Soft voting gives different weights to the nested subspaces depending on their contribution to the ensembled prediction and therefore provides a soft measure of the relative importance of the different dimensions. In that sense, it goes beyond the classical manifold assumption stating that data has one intrinsic dimension, and instead proposes a soft blend between dimensions that is adapted to the learning objective. This sheds light on the celebrated paper of Minka for the automatic choice of dimensionality in PCA~\citep[Section~5]{minka_automatic_2000}.
Many other ensembling methods are possible like gradient boosting, Bayesian model averaging and stacking.
The whole methodology is summarized in Algorithm~\ref{alg:flag_trick}.
\begin{algorithm}
\caption{Flag trick combined with ensemble learning}\label{alg:flag_trick}
\begin{algorithmic}
\Require $X := [x_1|\dots|x_n]\in\R^{p \times n}$ a data matrix; $q_{1:d} := (q_1, \dots, q_d)$ a flag signature; $f$ a subspace learning objective; (opt.) $Y := [y_1|\dots|y_n]\in\R^{m \times n}$ a label matrix
\State ${\S}_{1:d}^* \gets \operatorname{argmin}_{\S_{1:d} \in \Fl(p, q_{1:d})} \, f(\Pi_{\S_{1:d}})$  \Comment{flag trick \eqref{eq:flag_problem} + optimization (Alg.~\ref{alg:GD})}
\For{k = 1 \dots d}
	\State $g_k^* \gets \operatorname{fit}(\Pi_{\S_k^*} X, Y)$  \Comment{learning on $q_k$-dimensional projected data}
	\State $Y_k^* \gets g_k^*(\Pi_{\S_k^*} X)$ \Comment{prediction on $q_k$-dimensional projected data}
\EndFor
\State $Y^* \gets \operatorname{ensembling}({Y}_1^*, \dots, {Y}_d^*)$  \Comment{weighted predictions}
\Ensure $Y^*$ the ensembled predictions
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:flag_trick} is a general proposition of multilevel machine learning with flags, but many other uses of the optimal flag $\S_{1:d}^*$ are possible, depending on the application. For instance, one may directly use the reweighted data matrix $\Pi_{\S_{1:d}^*} X$ as an input to the machine learning algorithm. This enables to fit only one model instead of $d$.
One can also simply analyze the projected data \textit{qualitatively} via scatter plots or reconstruction plots as evoked in \autoref{sec:intro}. The nestedness will automatically bring consistency contrarily to non-nested subspace methods, and therefore improve interpretability.
Finally, many other ideas can be borrowed from the literature on subspace clustering and flag manifolds~\citep{draper_flag_2014,launay_mechanical_2021, ma_flag_2021, mankovich_flag_2022,mankovich_chordal_2023,mankovich_fun_2024}, for instance the computation of distances between flags coming from different datasets as a multilevel measure of similarity between datasets.