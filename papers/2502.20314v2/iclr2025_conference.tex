
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{mathrsfs}
\usepackage{subcaption}


\title{Adversarial Robustness in Parameter-Space Classifiers\thanks{Code available at \href{https://github.com/tamirshor7/Parameter-Space-Attack-Suite}{\textcolor{purple}{https://github.com/tamirshor7/Parameter-Space-Attack-Suite}}}}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{{Tamir Shor}\\
Department of Computer Science\\
Technion -- Israel Institute of Technology\\
Haifa, Israel\\
\texttt{tamir.shor@campus.technion.ac.il} \\
\And
Ethan Fetaya \\
Faculty of Engineering \\
Bar-Ilan University\\
Ramat Gan, Israel \\
\texttt{ethan.fetaya@biu.ac.il} \\
\And
Chaim Baskin \\
School of Electrical and Computer Engineering \\
Ben-Gurion University of the Negev\\
Be'er Sheva, Israel
\texttt{chaimbaskin@bgu.ac.il} \\
\AND
Alex Bronstein \\
Technion -- Israel Institute of Technology, Haifa, Israel \& Institute of Science and Technology, Austria  \\
\texttt{bron@cs.technion.ac.il}}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Implicit Neural Representations (INRs) have been recently garnering increasing interest in various research fields, mainly due to their ability to represent large, complex data in a compact and continuous manner. Past work further showed that numerous popular downstream tasks can be performed directly in the INR parameter-space.
Doing so can substantially reduce the computational resources required to process the represented data in their native domain. A major difficulty in using modern machine-learning approaches, is their high susceptibility to adversarial attacks, which have been shown to greatly limit the reliability and applicability of such methods in a wide range of settings. In this work, we show that parameter-space models trained for classification are inherently robust to adversarial attacks -- without the need of any robust training. To support our claims, we develop a novel suite of adversarial attacks targeting parameter-space classifiers, and furthermore analyze practical considerations of attacking parameter-space classifiers. 
\end{abstract}

\section{Introduction}
\label{intro}
%%%claims: 
% Intrinsic knowledge encapsulated in weight space classifiers helps robustness - each INR entry encapsulates cross-data information
%contributions: attacks suite, robustness claim, voxel grid attacks

Implicit Neural Representations (INRs) are representations of an arbitrary signal as a neural network predicting signal values under some conditioning. 
These representations have recently been raising increased interest due to their ability to efficiently and compactly encode high-dimensional data while maintaining high input fidelity, and have further shown to be capable of learning additional intrinsic knowledge latent within the original signal-domain representation \cite{sitzmann2020implicit, wang2021nerf}.
While INRs are inherently designed to fit one specific signal (rather than generalizing across an entire dataset), several recent works have proposed ways to use these representations to achieve a variety of downstream tasks by using some neural meta-network operating directly over the parameter-space of the INR of each signal within the dataset (\cite{chen2022transformers,dupont2022data,lee2021meta,pmlr-v202-navon23a}). 
In these settings, each data sample is received in the native signal space, converted to its compressed INR representation by fitting a neural model, and finally the meta-network performs the designated downstream task over the implicit representation encoded by the model's parameters.  The main advantage of such approaches is that they alleviate challenges in tasks involving high-dimensional data (videos, point-clouds, spectrograms), often requiring the incorporation of large, complex and problem-specific neural architectures for efficient data processing, and also potentially demand large computational resources for efficient optimization. 
Adversarial attacks are known to be a major concern for the use of neural networks across a variety of tasks. In this work, we explore -- to the best of our knowledge, for the first time --  the adversarial robustness of parameter-space classifiers. We start by framing the problem: any attack on a parameter-space classifier must be simultaneously optimized to reduce classification performance while also satisfy some constraint incurring the original, signal domain data fidelity. Since no prior studies have addressed this setting, we first formulate a novel set of possible white-box attacks over several data modalities. We then proceed to show that discussed parameter-space classification models are inherently robust to adversarial attacks in comparison to the signal-domain classification problem counterpart, as exemplified using our established attack suites. 
Our hypothesis of inherent robustness can be motivated by the following: Firstly, adversarial attacks commonly utilize the highly-nonlinear nature of modern deep classifiers to create 
perturbations with specific patterns that, while bounded, will be heavily amplified by the classifier to create significant data manipulations in deeper layers (\cite{ingle2021adversarial,romano2020adversarial}). In this setting of parameter-space classification, INR optimization is performed per-sample, and is embedded onto the classifier's inference process. 
This constrains the attacker to optimize perturbations in the signal-domain rather than directly manipulating the classifierâ€™s input. This optimization is disjoint from the downstream objective, and is dedicated to expressing global data features as a whole, rather than entangle specific entries with specific local signal features \cite{sitzmann2020implicit}. Therefore, this optimization process tends to obfuscate adversarial patterns introduced in signal domain.
Secondly, performing adversarial attacks over parameter-space classifiers poses significant computational difficulties for a white-box attacker, as attacking the INR given a perturbed signal requires backpropagation through an internal optimization loop.
In summary, this paper makes the following contributions -- First, we supply evidence of inherent robustness of parameter-space classifiers to adversarial attacks. Second, we propose several novel types of adversarial white-box attacks for parameter-space classifiers. Lastly, to demonstrate our claims on 3D data, we also formulate a novel adversarial attack for voxel-grid data representations.
\section{Related Work}
The analysis of trained neural network parameters had been raising increasing interest in recent years, demonstrating applicability in a wide, and at times surprising, range of machine learning tasks. \cite{eilertsen2020classifying} was one of the first works considering this idea, performing trained network parameter analysis to identify the optimization procedure hyperparameters. \cite{unterthiner2020predicting} showed that a trained CNN classifier model parameters can be used to directly attain a good assessment of classification accuracy, without running the full model's forward pass. \cite{dupont2022data} have been the first to demonstrate that neural network parameters, specifically of the SIREN \cite{sitzmann2020implicit} models (one of the first INRs to be proposed), could be used as the direct and only input for the training of a meta-learner model for classification, generative, data-imputation and novel-view synthesis tasks. \cite{peebles2022learning} employ generative modeling of deep-weight space to improve parameter update steps in model training.

\section{Method}
\label{method}

\subsection{Implicit neural representations}
\label{modeling}

Let $\mathcal{X}$ denote a class of signals of the form $x : \mathbb{R}^m \rightarrow \mathbb{R}^n$ (for clarity, we consider the domain and the co-domain to be finite Euclidean spaces; a more general formulation is straightforward). 
%
Given a specific signal $x$, the fitting of an implicit neural representation consists of fitting a neural model $F_\theta$ parameterized by $\theta \in \mathcal{Y}$ so to approximate $x$ in the sense of some norm, \citep{chen2022transformers,dupont2022data}
\begin{align} \label{inr_fit}
    \theta^*(x) = \argmin_{\theta  \in \mathcal{Y} } \quad & \|F_{\theta} -x\|_\mathcal{X}. 
\end{align}
%With a slight abuse of notation, we denote by $F_\theta(x)$ the INR output for all entries of $x \in \mathcal{X}$ collectively. 
In what follows, we denote the representation obtained by solving the minimization problem in equation (\ref{inr_fit}) as a map $R: \mathcal{X} \to \mathcal{Y}$, so that $R : x \mapsto\theta^*(x)$. 


\subsection{Parameter-Space Classifiers}
\label{psc}

In contrast to conventional classifiers operating directly on the signal space $\mathcal{X}$, parameter-space classifiers operate on the signal INRs, $R(\mathcal{X}) \subset \mathcal{Y}$. 
We denote a parameter-space classifier as $M_\psi$, parameterized by $\psi$; the model receives the representation parameters $\theta = R(x)$ as the input and outputs the predicted class label $\hat{c} = M_\psi(\theta)$ as the output.
Even for small neural representation models, the parameter space $\mathcal{Y}$ is often high-dimensional, increasing difficulties related to computational resources and overfitting when training the classification models on it \citep{navon2023equivariant,pmlr-v202-navon23a}. 
Functas \citep{dupont2022data} cope with this issue of increased parameter-space dimensionality by partitioning the parameter space into weights and biases, $\mathcal{Y} = \mathcal{Y}_\mathrm{w} \cup \mathcal{Y}_\mathrm{b}$, and sharing a single vector of weights $\theta_\mathrm{w} \in \mathcal{Y}_\mathrm{w}$ across a collection of represented signals, while allowing to fit individual \emph{modulation vectors} $v(x)  \in \mathbb{R}^d$ of a fixed dimension with a learned shared linear decoder $W$ such that each signal has an individual vector of biases, 
 $\theta_\mathrm{b}(x) = W v(x)$. This technique was successfully demonstrated with the SIREN (\cite{sitzmann2020implicit}) models. In what follows, we continue denoting the representation of $x$ as $\theta$, while taking into account that the actual INR $F$ may have additional parameters shared across multiple signals. 
\subsection{Parameter-Space Adversarial Attacks}
\label{secattacks}
Given a pre-trained parameter-space classification model $M_\psi$, our objective is to optimize an adversarial perturbation $\delta$ in signal domain under some $L_q(\mathcal{X})$-norm bound,
\begin{align}
    \label{attf}
    \max_{\delta} \quad & d( 
    M_\psi(R(x+\delta)), 
    M_\psi(R(x))
    )\,\,\, \text{s.t.} \,\,\, \|\delta\|_{L_q(\mathcal{X})} \leq \epsilon,
\end{align}
where $d$ denotes a distance in the class label domain.

Note that while the signal is attacked in its native domain, the classifier $M$ accepts its input in the representation domain. The two domains are related by the map $R$ -- a function whose pointwise value requires the solution of an optimization problem via a numerical algorithm. While backpropagating through this optimization process is possible (e.g. via automatic differentiation or implicit differentiation \cite{navon2020auxiliary}), as we later show in Section \ref{results}, doing so dramatically increases computational resources required for perturbation optimization, thus posing additional practical challenges for a potential attacker, in addition to the inherent difficulty of optimization such an attack that stems from the inference pipeline itself, as motivated in section \ref{intro}.

In the following sections, we propose five different adversarial attacks, addressing these challenges that are absent in the traditional signal-domain adversarial attack settings.



\subsubsection{Full PGD}
\label{fullpgd}
\begin{figure*}[htbp]


\centering
\includegraphics[width=0.8\linewidth]{pipeline.png}

\caption{\textbf{Parameter-space classifier adversarial attack pipeline} - for a single PGD iteration. Orange blocks are optimized, blue blocks remain frozen.}
\label{fig:PIPELINE}


\end{figure*}
A straightforward approach to solving equation (\ref{attf}) is by performing projected gradient descent (PGD) and backpropagating through both the classification model $M$ and the modulation optimization steps collectively denoted $R(x)$. While backpropagation through $M_\psi$ can be performed based on common first-order differentiation methods,  a potential attacker must allow gradient flow through the modulation optimization steps performed in the forward pass of $R$. This requires differentiation through those optimization steps, namely via second-order differentiation and optimization. As we further show in \cref{costs},  this may substantially increase the amount of computational resource required for the attacker.  \\
% This requirement imposes significant added difficulty for a potential attacker. As we demonstrate in section \ref{costs}, the optimization times required for a single PGD iteration grow linearly with the number of modulation optimization steps (which are controlled by the model, and not by the attacker). For "innocent" (non-attacker) users, however, the affect of adding modulation optimization steps is marginal, as gradient flow through modulation optimization steps $R(x)$ is not required. \\
The full PGD attack pipeline is portrayed in figure \ref{fig:PIPELINE} - in each PGD iteration, the current perturbation is added the original signal. Then, the perturbation is kept frozen and the perturbed signal is being optimized for the corresponding INR $\theta^*$, that is being fed into the parameter-space classifier $M$.

\subsubsection{TMO}
\label{tmo}
Given the potentially immense computational resources required for full PGD attack optimization, we explore several attacks bypassing the need for full second-order differentiation in perturbation optimization -- 
In \textit{Truncated Modulation Optimization} (TMO) we apply a principal similar to the Truncated Backpropagation Through Time algorithm, widely used in RNN optimization. Since computational resources required for full PGD linearly grow with the number of modulation optimization steps $n$, in this attack we fix a certain parameter $\tau$ controlling the number of modulation optimization steps through which gradients would flow in attack optimization. Namely, rather than performing the forward pass of $R$ consisting of $n$ modulation optimization iterations, only $\tau$ optimization steps are performed - all of which will be backpropagated through. \\
Since the full number of modulation optimization steps $n$ is inherent within the attacked model, an adversarial perturbation optimized for a duration of $\tau$ modulation steps might fail to fool the classifier at inference time (even if the attacked succeeded after $\tau$ steps). To alleviate this gap, when optimizing an adversarial perturbation $\delta$ for a given data sample $x$, we first run the full $n$ optimization steps of $R(x)$ to find the "clean" modulation vector. In every PGD iteration to follow, the $\tau$ optimization steps are initialized from the previously optimized "clean" modulation.


\subsubsection{BOTTOM}
\label{bottom}
TMO (\cref{tmo}), while computationally light, suffers from the limitation of performing less modulation optimization step per every PGD iteration. Model behavior observed by the attacking algorithm therefore deviates from the full forward process of the attacked model. 
In \textit{Backpropagation Over Truncation Through Optimization of Modulation Optimization} (BOTTOM) we address this, and propose an attack that performs the full $n$ modulation optimization steps per every sample $x$, while also alleviating the computational prices described in section \ref{fullpgd}.
 For every PGD iteration, the $n$ steps performed in the forward pass of $R$ are divided into $\left\lfloor \frac{n}{\tau} \right\rfloor$ optimization segments. In each segment, second order differentiation is performed through $\tau$ modulation optimization steps. \\
The attacker is given the flexibility to balance between computational resource consumption (lower $\tau$ values) and fidelity of gradients to the full forward pass in $R$ (higher $\tau$ values).

\subsubsection{ICOP}
\label{icop}
Proposed attacks in previous sections focus on perturbations applied in signal domain. In this section we further propose an attack applied in INR domain. The main challenge here is efficiently constraining the adversarial perturbation - while we perturb the neural representation of the signal, we still must constrain the response of every applied perturbation in signal domain. Namely, unlike the formulation in equation \ref{attf}, here we must impose $F(\delta) \leq B$. The need to query the Functa model $F$ to assess this response raises a difficulty to apply any trivial, immediate projection of a given perturbation $\delta$ onto the feasible set. We therefore propose \textit{Imposition of Constraints via Orthogonal Projection} (ICOP) -  after every PGD step we compute a projection loss, measuring constraint violation in $\mathcal{L}_1$: $\mathcal{L}_{proj.} = \|\mathcal{L}_q(F(\delta))-B\|_1$. We then iteratively take small gradient steps $\eta$: $\delta_{proj.} = \delta-\eta\frac{\partial \mathcal{L}_{proj.}}{\partial \delta}$, until constraint satisfaction. To discourage perturbation steps from severely breaking signal-domain constraints, we also found it useful to project the maximized classification loss to be orthogonal to the perturbation-induced deviation in signal-domain $\frac{\partial \|F(\theta+\delta) - F(\theta)\|_1}{\partial \delta}$ in every perturbation gradient step. 


\subsubsection{Implicit Differentiation}
\label{implicit}
In our adversarial setting (\cref{attf}), while $x$ and $M_\psi(R(x+\delta))$ are separable, any explicit differentiation method (e.g. computational graph-based gradient computation, as is the case in all previously proposed methods) would be heavily hindered by the nature of $R$. The successive optimization steps required for querying $R$ may vastly inflate the computational graph maintained for the adversarial objective, leading to increased computational costs and potential numerical instability commonly associated with large graphs. To this end, we also investigate implicit differentiation within our setting. Let $d(x,\delta)$ denote the classification error 
     (as featured in \cref{attf}), and $\mathscr{L}(\theta^*,x)$ represent the INR signal approximation error $\|F(\theta^*)-x\|_{\mathcal{X}}$. \\
Since $\theta^*=R(x)$, we elect the corresponding optimality conditions to represent local minima in $\theta^*$'s representation error of $x$:
\begin{equation*}
     \frac{\partial\mathscr{L}(\theta^*,x)}{\partial\theta^*} = 0 ;  
      \frac{\partial\mathscr{L}(\theta^* + d \theta,x + dx)}{\partial\theta^*} = 0
\end{equation*}
where $dx, d\theta$ represent infinitesimal steps in $x,\theta$ respectively.
Our implicit differentiation is thus guided by the following approximation of gradients for our entire optimization pipeline:

\begin{align}
\label{eq:impl}
\frac{\partial d}{\partial\delta} = -\frac{\partial d}{\partial \theta^*}\cdot\frac{\partial^2 \mathscr{L}}{\partial \theta^*\partial (x+\delta)}\cdot (\frac{\mathscr{L}}{\partial^2 \theta^*})^{-1} \cdot \frac{\partial(x+\delta)}{\partial\delta}
\end{align}

Note that unlike previous methods proposed in this paper (\cref{fullpgd,tmo,bottom,icop}), \cref{eq:impl} circumvents the evaluation of $\frac{\partial \theta^*}{\partial x}$ via the chain-rule , and therefore explicit backpropagation through the optimization steps applied when solving \cref{inr_fit} is not needed. In spite of its merits, we stress that implicit differentiation does not nullify the need of explicit methods -- the efficacy of implicit differentiation is heavily reliant on the extent of satisfaction of the underlying optimality conditions. Under our formulation -- when the set of parameters $\theta^*$ given by $R(x)$ satisfies sufficient signal-domain fidelity with $x$.

\subsection{BVA - Voxel-Grid Space Adversarial Attack}
\label{bva_form}
Out of the 3 most common conventions for 3D data representations -- voxel-grids, mesh-grids and point-clouds -- we find that due to the highly irregular structure of mesh-grids and point-clouds, fitting the dataset-shared set of INR weights ($\theta_w$ in notation from section \ref{modeling}) yields poor results, preventing achieving implicit representations of adequate signal fidelity. Therefore, in this work we focus on voxel-grid representations of 3D data. \\
While adversarial attacks over 3D have been studied in the past for mesh-grids (\cite{xu2022d3advm}) and point clouds \cite{liu2019extending}, to the best of our knowledge no similar work had so far been done in the context of voxel-grid representations. Since we wish to perform adversarial attacks over INR classifiers of voxel grids, and compare robustness to a corresponding signal-domain classifier, we develop Binary Voxel Attack (BVA) - a novel adversarial attack applied over voxel-grid data. \\
Voxel-grid representations are traditionally binary spatial occupancy maps. Therefore, an adversarial attack performing addition of a real-valued adversarial perturbation bounded in $\mathcal{L}_\infty$ is not applicable. Since our attacked signal is a binary map, we opt to establish an adversarial attack in the from of bit flipping (namely, bit attack). Given a voxel-grid input signal $x \in \{0,1\}^{H\times W\times D}$ (where $H,W,D$ are grid spatial dimensions), we formulate our attack as: 
\begin{align}
    \label{bva}
    \max_{\delta} \quad & \|M_\psi(x \oplus \delta)\|  \text{s.t.} \|p\|_{0} = B
\end{align}
where $\delta\in \{0,1\}^{H\times W\times D}$ is the optimized binary adversarial perturbation, and $\oplus$ denotes the bit-wise xor operation. $\delta$ is chosen to be bounded in $\mathcal{L}_0$ norm, namely bounding the number of bits flipped in $x$. To perform the required binary optimization of $\delta$ we follow \cite{iliadis2020deepbinarymask}, elaborated in \cref{app:binary_opt}.

\section{Results}
\label{results}
In the following sections we experiment with 3 different datasets - MNIST (\cite{deng2012mnist}), Fashion-MNIST (\cite{xiao2017fashion}) and ModelNet10 (\cite{wu20153d}). \\
Following the approach from \cite{dupont2022data}, for each dataset $\mathcal{X}$ we first optimize a set of modulation vectors $\{v_{x} \forall x \in \mathcal{X}\}$. Then, we train a single MLP classifier over this modulation dataset. Finally, every pre-trained classifier is attacked with each of the five attacks presented in \cref{secattacks}. We emphasize no robust are adversarial training is being done in any stage of this experimentation scheme. Attack formulations and results for 2D data are presented in section \ref{res2d}, and for 3D data in section \ref{res3d}. In section \ref{costs} we further demonstrate the added computational costs induced by performing adversarial attacks over deep weight-space classifiers, posing added practical challenges for potential attackers. Full training details are further specified in \cref{app:functa}.
\subsection{Robustness For 2D Data}
\label{res2d}
Clean and adversarial INR classification accuracy over 2D data classifiers is shown in table \ref{tab:2d}. In the notations of equation \ref{attf}, $q=\infty$ is chosen as $B=\frac{\varepsilon}{256}$ for every table entry. \textit{Clean} column represents classifier accuracy without introducing any adversarial perturbations.\\
Results indicate 3 major trends - First, both MNIST and Fashion-MNIST are adversarially robust. For MNIST, while the TMO and BOTTOM attacks are the only two that managed to incur any above-marginal decrease in classification accuracy, it is still far below the accuracy decrease of around $60\%$ that is traditionally observed in similarly-bounded PGD attacks over signal-domain classifiers without any robust training (\cite{wong2020fast}). While higher-bounded attacks over the Fashion-MNIST dataset have been more successful, their success is substantially lower than the effect of a similarly constrained attack over a signal-domain classifier. To show this, we train four baseline signal-domain classifiers and attack with PGD using similar $\mathcal{L}_\infty$ norm constraints and PGD iteration number. For a representative pool of common signal-domain classifiers we use ViT-Base/16, ViT-Large/16 (\cite{dosovitskiy2020image}), a Resnet18 and a smaller 2-layer CNN. Full architecture details are in \cref{aes}. We compare the robust accuracy of these four Signal-space classifiers under standard 100-iteration PGD attack, with that of our parameter-space classifier subjected to all attacks from \cref{fullpgd,tmo,bottom,icop,implicit}. Results, shown in figure \ref{fig:fmnist}, indicate parameter-space classifiers show significantly higher robustness, especially for lower attack bounds. \\
Furthermore, we observe that the full PGD attack demonstrates lower adversarial prowess compared to BOTTOM and TMO attacks in all experiments. We attribute this to the fact that in full-PGD the numerous steps of second-order optimization required for standard PGD in our case encourage vanishing gradients phenomena, namely attenuating gradient information backpropagated through the modulation optimization process $R$. While computationally lightest, implicit differentiation proves less potent than most explicit methods. We attribute this to the strong dependence of this method in fulfillment of specified optimality conditions. 
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=1\linewidth]{hist_short_32.png}

% \caption{\textbf{Implicit Attack Success/Failure distribution} - for optimized representation error in perturbed and clean data.}
% \label{fig:implicit_hist}
% \end{figure}
\begin{figure}[h]
    \centering
    % First subfigure
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fmnist_plot.png}
        \caption{Fashion-MNIST robust accuracy in $\mathcal{L}_\infty$.}
        \label{fig:fmnist}
    \end{subfigure}
    \hfill
    % Second subfigure
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{modelnet_plot.png}
        \caption{ModelNet10 robust accuracy in $\mathcal{L}_0$.}
        \label{fig:signal_acc}
    \end{subfigure}
    
    \caption{\textbf{Robust accuracy for parameter-space (solid curves) and signal-space (dotted curves) classifiers.}}
    \label{fig:side_by_side}
\end{figure}



\begin{table*}[htbp]
\centering
\caption{\textbf{MNIST \& Fashion-MNIST Clean/Robust Classification Accuracy} - for any $\varepsilon$ value, $\mathcal{L}_\infty$ attack bound (equation \ref{attf}) is $B=\frac{\varepsilon}{256}$.}
\resizebox{1\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{$\varepsilon$} & \multicolumn{6}{|c||}{\textbf{MNIST}} & \multicolumn{6}{|c|}{\textbf{Fashion-MNIST}} \\
\cline{2-13}
& Clean & Full & TMO & BOTTOM & ICOP & Implicit & Clean & Full & TMO & BOTTOM & ICOP & Implicit \\
\hline
\textbf{4} & 0.976 & 0.949 & 0.941 & 0.939 & 0.965 & 0.957 & 0.891 & 0.791 & 0.77 & 0.79 & 0.891 & 0.826 \\
\hline
\textbf{8} & 0.976 & 0.915 & 0.91 & 0.906 & 0.965 & 0.946 & 0.891 & 0.67 & 0.64 & 0.658 & 0.8904 & 0.792 \\
\hline
\textbf{16} & 0.976 & 0.866 & 0.825 & 0.821 & 0.961 & 0.926 & 0.891 & 0.54 & 0.44 & 0.438 & 0.89 & 0.745 \\
\hline
\textbf{32} & 0.976 & 0.803 & 0.7035 & 0.698 & 0.957 & 0.919 & 0.891 & 0.48 & 0.37 & 0.341 & 0.848 & 0.69\\
\hline
\end{tabular}
}

\label{tab:2d}
\end{table*}




\subsection{Robustness For 3D Data}
\label{res3d}

In section \ref{res2d} we survey adversarial robustness of classifiers INR for 2D data. The importance of this baseline is comparability to the widely studied adversarial robustness of 2D classifiers. Nonetheless, practical advantages of using INRs for 2D data are usually limited, since common grid representations suffice for most common tasks. Data modalities of larger dimensions, such as 3D data, however, more commonly gain from using INRs for achieving downstream tasks, including classification (\cite{jiang2020local,molaei2023implicit,niemeyer2020differentiable}).
In our experiments we use the ModelNet 10 dataset (\cite{wu20153d}) consisting of 4900 3D CAD models given in Mesh-grid representation. 
\subsubsection{Signal Space Attack}
\label{signal3d}
To demonstrate inherent adversarial robustness over 3D data under the BVA attack (section \ref{bva_form}), we must first demonstrate its prowess in attenuating signal-domain classification accuracy. To do so we train a voxel-grid classification model based on Point-Voxel CNN layers proposed by \cite{liu2019point}. We train the classifier over ModelNet10 (full optimization details brought in \cref{app:modelnet10det}) and then attack this trained model with BVA for a wide range of $\mathcal{L}_0$ bounds and assess attacked classification accuracy. Results are displayed in figure \ref{fig:signal_acc}, showing BVA is efficient over the tested signal-space classifier, inducing exponential decay of classification accuracy with growth in number of flipped voxel bits. 


\subsubsection{Parameter Space Attack}
\label{parameter3d}
Figure \ref{fig:signal_acc} compares the robust accuracy of the signal domain (black) and parameter-space classifiers, where the parameter-space classifier is attacked with TMO, BOTTOM, ICOP, and implicit attacks. We note that we do not experiment with the full PGD attack in parameter-space for the 3D INR classifier, since to optimize INR for the ModelNet10 dataset with sufficient input signal fidelity, we found it necessary to perform 500 modulation optimization iterations per sample. While computation times for doing so are not very high (several seconds), backpropagation through these 500 optimization steps calls for infeasible memory and compute time requirements, and also lead to severe vanishing/exploding gradient issues. We further elaborate on these considerations in \cref{costs}. \\
While the parameter-space classifier exhibits lower clean accuracy (73\% in parameter-space against 82\% in signal space), results demonstrate conclusive increased adversarial robustness for the signal-space classifier. We can specify two major trends - the first is that the absolute decrease in classification accuracy is much lower for the parameter-space classifier. The second is that for the attacked weight-space classifier, the decrease in accuracy attenuation across bound increase saturates significantly faster. 
The data from \cref{fig:signal_acc} is presented numerically in \cref{app:tab_3d}. 

\subsection{Qualitative Results}
\label{sec:qual}
In this section we provide qualitative evidence for the inherent robustness of parameter-space classifiers. As previously discussed, we argue that the enhanced robustness of parameter-space classifiers arises from the attacker's inability to directly perturb the classifier's input. Instead, the attacker must optimize a constrained signal-domain adversarial perturbation for which the response received as input to the classifier (after modulation optimization \cref{inr_fit}) is adversarially potent. This is deemed to be substantially more challenging for the attacker, as modulation optimization tends to attenuate adversarial prowess fed as input to the classifier. 
To illustrate this phenomenon, we report the mean amplification, measured as the difference in activation values between perturbed and clean input data, for each layer in our pipeline (\cref{fig:PIPELINE}). These measurements, shown in \cref{fig:amplification}, were calculated for classification over the MNIST dataset, with every sample propagated through the pipeline after being perturbed by each attack in our suite with a constraint of $\varepsilon=\frac{32}{256}$. To further validate the relationship between attack success and amplification, we differentiate between robust and non-robust samples. Successful attacks demonstrate an average amplification approximately three times greater than that of failed attacks. In both cases, amplification sharply plummets upon entering the first classifier layer (layer 10) and remains consistently low in subsequent layers.
These findings indicate that adversarial amplification within the INR layers has a negligible impact on the downstream classification model. This supports our claim that modulation optimization effectively obfuscates the adversarial potency of perturbations applied in the signal domain. Importantly, this effect is not observed for signal-domain classifiers, as we show in \cref{fig:signal_amplification}. Results for other attack constraints and datasets are reported in \cref{app:res}.
\begin{figure}[htbp]


\centering
\includegraphics[width=0.6\linewidth]{amplification_mnist.png}

\caption{\textbf{Adversarial amplification across pipeline layers} - showing modulation optimization's attenuation of adversarial amplification.}
\label{fig:amplification}


\end{figure}
In \cref{fig:tsnes} we demonstrate the affect of adversarial perturbations applied in signal-domain over the latent representations attained from perturbed data, and compare between parameter-space classifiers and traditional signal-space classifiers. To this end, as a signal-domain baseline, we first train a convolutional autoencoder over the MNIST dataset (\cref{aes}), disjointly from the downstream classification task. We then train a classifier over the learned latent space, and finally perform 100 iterations of signal-domain adversarial attack over this classifier. The bottom half of \cref{fig:tsnes} illustrates t-SNE projection of the trained autoencoder's latent representations before (left) and after (right) the adversarial attack. The top half of the plot corresponds to parameter-space classifiers, showing t-SNE projections of INR modulations optimized for clean (right) data and for data attacked with our full-PGD attack (\cref{fullpgd}. Prior to the attacks, the latent space of both signal and parameter-space domains is well-structured, with clear separation between the different classes. However, after the attack, the model operating strictly in signal-domain (i.e. the autoencoder baseline) experiences severe disruption of this structure -- classes of adversarial samples within the classifier's latent input space are no longer distinct and are instead intermixed. This indicates a significant deterioration in the class separability, highlighting the impact of the adversarial perturbations on the latent space. Adversarial attacks optimized for the parameter-space classifier, on the other hand, seem inconsequential to the input-space structure of the parameter-space classifier, including class separability. These conclusions are well-correlated with those from \cref{fig:amplification}.
\begin{figure}[htbp]


\centering
\includegraphics[width=0.4\linewidth]{tsnes.png}

\caption{\textbf{t-SNE projection of modulation vectors fitted for clean and adversarially-perturbed data} - for parameter-space and signal-space classifiers.}
\label{fig:tsnes}


\end{figure}







\section{Conclusion}
In this work, we investigate the adversarial robustness of parameter-space classifiers. We highlight the challenges in adapting commonly used PGD-based attacks to models that perform classification directly in the deep parameter space. To address these challenges, we propose a novel set of five distinct adversarial attacks tailored for parameter-space classification. Our results demonstrate that parameter-space classifiers exhibit enhanced inherent adversarial robustness compared to those operating strictly in the signal domain.
While this study serves as a significant initial exploration of adversarial robustness in parameter-space classificationâ€”a relatively under-explored areaâ€”we acknowledge several limitations. First, current state-of-the-art parameter-space classifiers still struggle to achieve satisfactory "clean" (non-attacked) classification performance on more complex datasets (\cite{navon2023equivariant,pmlr-v202-navon23a}). Consequently, we focus on a limited set of relatively simple datasets from both 2D and 3D modalities, where "clean" accuracy is sufficient for a reliable evaluation of adversarial attack effects. We hope that future advancements in non-adversarial parameter-space classification will broaden the range of datasets available for robustness studies.
Second, we note that the adversarial robustness observed in parameter-space classifiers often comes at the expense of reduced classification accuracy, as demonstrated in section \ref{results}.



\bibliography{./iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\section{Training Regime, Hyperparameters \& Technical Details}
\subsection{Functaset Generation}
\label{app:functa}
\subsubsection{MNIST \& Fashion-MNIST}
All experiments described in this section have been conducted on a single NVIDIA RTX-2080 GPU.
Following \cite{dupont2022data}, for each of the three datasets considered in this paper, we first train a SIREN \cite{sitzmann2020implicit} jointly over the entire training set. For MNIST and Fashion-MNIST we optimize modulation vectors of dimension $512$ for 6 epochs of 3 modulation optimization steps per-sample, with a learning-rate of $0.000005$ and a batch size of $128$. We use 10 SIREN layers with hidden dimension of $256$. We then proceed to creating the Functaset , which is the dataset of modulation vectors conditioning the SIREN for per-sample representation. For each sample we optimize for 10 modulation optimization steps, with a learning-rate of 0.01.
\subsubsection{ModelNet10}
\label{app:modelnet10det}
For ModelNet10 we similarly follow \cite{dupont2022data}, however we first convert the Mesh-grid data onto a voxelgrid, as previously mentioned (\cref{bva_form}). Each voxelgrid is resampled to dimension $15\times15\times15$. Due to the increased dimensionality, we train $2048$-dimensional Functa modulations with a SIREN similar to the one used for the MNIST dataset. After training the shared SIREN weights, each modulation vector is further optimized for $500$ modulation optimization steps. 
\subsection{Adversarial Attacks}
After the Functaset is generated, we proceed to executing our develop suite of adversarial attacks \cref{secattacks}. For MNIST and Fashion-MNIST, all attacks are executed over 10 modulation optimization steps per adversarial perturbation, similarly to optimization in the clean case. BOTTOM and TMO optimization segments are chosen at 5 steps per-sample. For implicit differentiation, we've found it most useful to optimize each perturbed image using L-BFGS, rather than gradient methods (which have been used in other attacks). Each L-BFGS optimization is carried for at most 20 iterations. For ModelNet10, ICOP,TMO and BOTTOM attacks are performed for a total of 500 modulation optimization steps per perturbed voxelgrid, with attack segments again chosen at 5 steps. Implicit differentiation attack is also performed with at most 20 iterations of L-BFGS. All attacks are carried-out for 100 PGD iterations, with early-stop of 3 iterations of constant loss for the implicit method. We use the Adam optimizer in all experiments.

\subsection{Binary Optimization in BVA}
\label{app:binary_opt}
In BVA \cref{bva} we formulate the attacker objective as:
\begin{align}
    \label{bva}
    \max_{\delta} \quad & \|M_\psi(x \oplus \delta)\|  \text{s.t.} \|p\|_{0} = B
\end{align}
Since voxel-grid data is represented by binary mask, our optimized perturbation $\delta$ in this case is a binary mask governing the bit-flip imposed over the original, "clean" voxel-grid data.
This formulation calls for the incorporation of binary optimization of the mask $\delta$. Trivial binarization of $\delta$ (e.g. by thresholding) is non-differentiable. To optimize $\delta$ as a binary mask we follow the binary optimization technique presented in \cite{iliadis2020deepbinarymask}. This method maintains two congruent masks - a binary mask $\Phi$ and a real mask $\Phi_c$. $\Phi$ is used for forward and backpropagation, where each gradient step applies the gradients calculated for $\Phi$ over $\Phi_c$. The latter is then binarized (without gradient tracking) to produce $\Phi$ for the next iteration.\\
In our experiments, we optimize under varying $\mathcal{L}_0$ constraints (\cref{fig:signal_acc}), using a learning-rate of $0.01$.

\subsection{Baseline Models}
\subsubsection{Classifier Baselines}
\label{cls_baselines}
In this section we specify implementation and training details related to the signal-space classifier baseline CNN and ViT architectures used in \cref{results}. In section \ref{res2d} we train 4 signal-domain classifiers -- a small CNN, a ResNet18, ViT-Large/16 and Vit-Base/16. For the ViTs we leverage pre-trained models from \cite{rw2019timm}, and finetune for our classification task using a a linear probe prior to a applying Softmax. For the Resnet we train from scratch, and project encoded data with a linear probe followed by a Softmax. For the small-CNN we use two convolutional layers with ReLU activations, and a single MaxPooling operation between them. For all baselines we train over the Fashion-MNIST training set for 5 epochs with learning-rate of 0.0001.
\subsubsection{Autoencoder Baselines}
\label{aes}
In \cref{sec:qual}, we present a qualitative analysis of adversarial perturbation amplification across pipeline layers, along with t-SNE projections of modulation spaces before and after adversarial perturbation. To connect these observations to the inherent adversarial robustness of parameter-space classifiers, we include a comparison with a signal-space classifier. In order to propose a signal-space classifier with an appropriate counterpart to the explored parameter-space model modulation space, we choose to train classifiers operating over latent spaces trained by autoencoders. We model each autoencoder as a 3-layer CNN with ReLU activations and train with a self-supervised reconstruction objective for 10 epochs. After the autoencoder is trained, we freeze it and train a linear classifier with 2 hidden layers and a single ReLU activation on top of it. The frozen encoder layers acts as the INR counterpart in \cref{fig:amplification}, and the MLP head is parallelized to the parameter-space classifier.  We use a latent dimension of $128$ for all encoders.
\section{Additional Results}
\label{app:res}
\subsection{Robustness For 2D Data}
\subsubsection{MNIST}
\cref{fig:signal_amplification} illustrates the mean amplification difference measured through model activation layers over MNIST data, similarly to \cref{fig:amplification}, however for the signal-domain classifier trained according to section \ref{aes}. This model uses a 3-layer convolutional encoder, followed by a classifier with a single activation. Clean classifier accuracy is $98.8\%$, robust accuracy is $11.28\%$. Differently from similar results displayed for parameter-space classifiers in \cref{fig:amplification}, upon transition from encoder to classifier layers (3rd layer, marked by the black vertical line), the adversarial amplification is not attenuated, but rather drastically inflated. 
\begin{figure}[htbp]


\centering
\includegraphics[width=1\linewidth]{signal_domain_amplification.png}

\caption{\textbf{Signal-Domain Amplification} - through encoder and downstream classifier layers} 
\label{fig:signal_amplification}


\end{figure}

\cref{fig:cons_amp} showcases adversarial amplification (similarly to \cref{fig:amplification}) across different attack bound for the full-PGD attack. Adversarial obfuscation drawn from \cref{fig:amplification} arises here as well.
\begin{figure}[htbp]


\centering
\includegraphics[width=1\linewidth]{amplification_constraints.png}

\caption{\textbf{Signal-Domain Amplification} - through encoder and downstream classifier layers, across different constraints $\frac{\varepsilon}{256}$ for full PGD attack} 
\label{fig:cons_amp}


\end{figure}
\subsubsection{Fashion-MNIST}

\cref{fig:amplification_fmnist} illustrates adversarial amplification across pipeline layers, similarly to \cref{fig:amplification}. Trends of correlation between amplification and adversarial success, as well as evident obfuscation of adversarial affect between INR and classifier segments, are consistent with findings from \cref{sec:qual}.

\begin{figure}[htbp]


\centering
\includegraphics[width=1\linewidth]{amplification_plots_fmnist.png}

\caption{\textbf{Adversarial amplification across pipeline layers} - showing modulation optimization's attenuation of adversarial amplification.}
\label{fig:amplification_fmnist}


\end{figure}

Similarly to \cref{fig:signal_amplification}, we test amplification patterns of a signal-domain classifier (\cref{aes}) for the Fashion-MNIST dataset as well. We again observe consistency across both datasets - the signal domain classifiers proves to be much more sensitive to adversarial examples propagated through the encoder-classifier pipeline.
\begin{figure}[htbp]


\centering
\includegraphics[width=1\linewidth]{signal_domain_amplification_fmnist.png}

\caption{\textbf{Signal-Domain Amplification} - through encoder and downstream classifier layers} 
\label{fig:fashion_signal_amplification}


\end{figure}


\cref{fig:tsnes_fmnist} exhibits the affect of adversarial attacks in signal-domain over the underlying modulation vectors (for parameter-space classifiers) and latent vectors (for signal-space classifiers) received as classifier input. Similarly to \cref{fig:tsnes}, for both parameter-space and signal-space model, a well-formed latent space is observed before the incorporation of adversarial perturbation. After attacking with PGD, the corresponding modulation vectors fed into the parameter-space classifier remain almost unchanged, whilst the latent space observed by the signal-domain classifier is heavily deformed.
\begin{figure}[htbp]


\centering
\includegraphics[width=0.8\linewidth]{fmnist_tsne_compare.png}

\caption{\textbf{t-SNE projection of modulation vectors fitted for clean and adversarialy-perturbed data} - for parameter-space and signal-space classifiers.}
\label{fig:tsnes_fmnist}


\end{figure}

\subsection{Robustness For 3D Data}
In this section we report additional results for the ModelNet10 dataset. \\
In \cref{app:tab_3d} we report numerical results for the data from \cref{fig:signal_acc}
The first two columns represent the same attack constraints in absolute bit numbers ($\varepsilon$) and bit numbers relative to the entire volume (matching horizontal axis in figure \ref{fig:signal_acc}).


\begin{table*}[htbp]
%\vspace{-0.1cm}
 %   \centering
 
\centering
\caption{\textbf{ModelNet10 Clean/Robust Classification Accuracy} - for any given $\varepsilon$ value, attack bound from equation \ref{attf} is $B=\varepsilon$ in $\mathcal{L}_0$ (i.e. number of flipped signal bits).}

\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$\varepsilon$ & Flip Ratio & Clean  & TMO & BOTTOM & ICOP & Implicit \\
\hline
10 & 0.003& 0.734 & 0.669 & 0.663 & 0.66  & 0.722 \\
\hline
20 &0.006 & 0.734 & 0.6475 & 0.634 & 0.6475   & 0.708 \\
\hline
40 &0.012& 0.734 & 0.647 & 0.623 & 0.641 & 0.696 \\
\hline
80 & 0.024&0.734 & 0.643 & 0.616 & 0.581 & 0.67\\
\hline
100 & 0.03&0.734 & 0.6412 & 0.609 & 0.557 & 0.657\\
\hline
200 & 0.06&0.734 & 0.594 & 0.576 & 0.41 & 0.613\\
\hline
\end{tabular}






\label{app:tab_3d}
\end{table*}

\section{Computational Resource Analysis}
\label{costs}

In sections \ref{res2d},\ref{res3d} we provide evidence for the adversarial robustness of parameter-space classifiers based on classification accuracies under the presence of adversarial perturbations. In this section, we further support these claims from a different, practical perspective. We show that orthogonally to the efficacy of adversarial attacks over parameter-space classifiers, attacking these classifiers also poses significant added computational difficulties for the potential attacker. These difficulties stem from the optimization process inherent within the classification pipeline's inference process. To illustrate this, we measure the computation times required for a single modulation optimization step during a single PGD iteration on an NVIDIA RTX-2080 GPU. Our results show that for the MNIST dataset with 512-dimensional modulation vectors, the compute time per step is approximately 0.015 seconds, while for the ModelNet10 dataset with 2048-dimensional modulation vectors, it is 0.02 seconds. Even for a small number of PGD steps, increasing the number of modulation optimization steps -- an aspect controlled by the model, not the attacker -- increases attack optimization times within several orders of magnitude. Importantly, this increase does not significantly impact the computation time required for "clean" (unperturbed) inference. For instance, with 100 modulation optimization steps set by the model to solve \cref{inr_fit}, "clean" inference would require 1.5 seconds per sample, while perturbation optimization of 100 PGD iterations would require the attacker to invest 150 seconds per-sample. 

On top of computation time increase, attacking parameter-space classifiers via explicit differentiation also poses a challenge for the attacker in terms of memory consumption. Tested on the same hardware, we find that GPU memory consumption for the full PGD attack over the ModelNet10 classifier grows linearly with number of modulation optimization steps, with a slope of $0.1705 [\frac{GB}{Mod. Steps}]$. The corresponding coefficient for the smaller MNIST data is  $0.049 [\frac{GB}{Mod. Steps}]$.  Namely, in order to perform a full PGD (\cref{fullpgd}) attack over the ModelNet10 data, every additional modulation optimization step included in "clean" model training induces roughly 170 additional megabytes for the attacker's backpropagation. Backpropagating through the  500  modulation optimization steps used in creation of the ModelNet10 INR dataset in our experiments would require 85 Gigabytes of GPU memory, greatly increasing optimization costs. The increase in modulation optimization steps holds no more than marginal prices for the "innocent" (non-attacker) user in INR dataset optimization, as this process does not require 2nd-order differentiation. Implicit differentiation (\cref{implicit}) offers the attacker constant-memory attacks (in our experiments - 526 MB for MNIST, 986 MB for ModelNet10), however at the cost of higher sensitivity to optimality condition satisfaction (as shown in \cref{res2d}).


\end{document}
