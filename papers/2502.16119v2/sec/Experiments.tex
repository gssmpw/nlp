\section{Experiments}
\label{sec:experiments}

\begin{table*}[t]
\caption{The test accuracy (\%) on four datasets in the pathological and practical settings using the HMG$_8$ model group.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|cccc|cccc|}
\hline
Settings  & \multicolumn{4}{c|}{Pathological Setting}                                                                                                                                & \multicolumn{4}{c|}{Practical Setting}                                                                                                                                   \\ \hline
Datasets  & \multicolumn{1}{c|}{Cifar-10}               & \multicolumn{1}{c|}{Cifar-100}              & \multicolumn{1}{c|}{Flowers102}            & \multicolumn{1}{l|}{TinyImagenet} & \multicolumn{1}{c|}{Cifar-10}               & \multicolumn{1}{c|}{Cifar-100}              & \multicolumn{1}{c|}{Flowers102}            & \multicolumn{1}{l|}{TinyImagenet} \\ \hline
FML       & \multicolumn{1}{c|}{82.56 ± 0.14}          & \multicolumn{1}{c|}{50.27 ± 0.09}          & \multicolumn{1}{c|}{52.17 ± 0.98}          & 33.37 ± 0.34                      & \multicolumn{1}{c|}{91.85 ± 0.24}          & \multicolumn{1}{c|}{46.45 ± 0.17}          & \multicolumn{1}{c|}{48.51 ± 0.57}          & 37.67 ± 0.19                      \\ \hline
LG-FedAvg & \multicolumn{1}{c|}{83.63 ± 0.09}          & \multicolumn{1}{c|}{55.51 ± 0.10}          & \multicolumn{1}{c|}{58.90 ± 0.22}          & 34.99 ± 0.29                      & \multicolumn{1}{c|}{92.34 ± 0.06}          & \multicolumn{1}{c|}{49.82 ± 0.16}          & \multicolumn{1}{c|}{53.65 ± 0.46}          & 38.35 ± 0.27                      \\ \hline
FedGen    & \multicolumn{1}{c|}{83.63 ± 0.24}          & \multicolumn{1}{c|}{55.37 ± 0.35}          & \multicolumn{1}{c|}{59.39 ± 0.19}          & 35.18 ± 0.12                      & \multicolumn{1}{c|}{92.43 ± 0.11}          & \multicolumn{1}{c|}{49.90 ± 0.21}          & \multicolumn{1}{c|}{54.59 ± 0.34}          & 38.44 ± 0.14                      \\ \hline
FedProto  & \multicolumn{1}{c|}{80.63 ± 0.05}          & \multicolumn{1}{c|}{51.89 ± 0.31}          & \multicolumn{1}{c|}{54.48 ± 0.20}          & 33.78 ± 0.20                      & \multicolumn{1}{c|}{79.32 ± 0.22}          & \multicolumn{1}{c|}{42.77 ± 0.13}          & \multicolumn{1}{c|}{26.66 ± 0.57}          & 24.57 ± 0.08                      \\ \hline
FedKD     & \multicolumn{1}{c|}{83.41 ± 0.66}          & \multicolumn{1}{c|}{53.01 ± 1.48}          & \multicolumn{1}{c|}{52.61 ± 0.74}          & 34.70 ± 0.91                      & \multicolumn{1}{c|}{91.85 ± 0.31}          & \multicolumn{1}{c|}{48.95 ± 1.24}          & \multicolumn{1}{c|}{51.44 ± 0.75}          & 39.13 ± 0.36                      \\ \hline
FedGH     & \multicolumn{1}{c|}{83.49 ± 0.29}          & \multicolumn{1}{c|}{55.25 ± 0.12}          & \multicolumn{1}{c|}{60.15 ± 1.08}          & 35.04 ± 0.16                      & \multicolumn{1}{c|}{92.66 ± 0.10}          & \multicolumn{1}{c|}{49.52 ± 0.05}          & \multicolumn{1}{c|}{54.24 ± 0.50}          & 38.67 ± 0.11                      \\ \hline
FedTGP    & \multicolumn{1}{c|}{84.75 ± 0.06}          & \multicolumn{1}{c|}{53.12 ± 0.24}          & \multicolumn{1}{c|}{58.60 ± 0.14}          & 32.41 ± 0.09                      & \multicolumn{1}{c|}{92.26 ± 0.68}          & \multicolumn{1}{c|}{49.09 ± 0.16}          & \multicolumn{1}{c|}{56.62 ± 0.40}          & 37.14 ± 0.16                      \\ \hline
FedORGP   & \multicolumn{1}{c|}{\textbf{85.71 ± 0.10}} & \multicolumn{1}{c|}{\textbf{61.12 ± 0.15}} & \multicolumn{1}{c|}{\textbf{64.15 ± 1.66}} & \textbf{37.04 ± 0.56}             & \multicolumn{1}{c|}{\textbf{94.01 ± 0.03}} & \multicolumn{1}{c|}{\textbf{55.33 ± 0.23}} & \multicolumn{1}{c|}{\textbf{60.34 ± 0.48}} & \textbf{40.15 ± 0.40}             \\ \hline
\end{tabular}
}
\label{tab:test all comparison_results} % 设置标签以便引用
\end{table*}

\subsection{Setup}
\textbf{Datasets.} To evaluate our model, we conduct experiments on four image classification datasets: CIFAR-10, CIFAR-100 \cite{Cifar}, Flowers102 \cite{flowers102} and TinyImagenet \cite{tinyimagenet}.\\
\textbf{Baselines.} To evaluate our proposed FedORGP, we compare it with seven popular methods that are applicable in HtFL, including LG-FedAvg \cite{lgfedavg}, FML \cite{fml}, FedGen \cite{FedGen}, FedKD \cite{fedkd}, FedProto \cite{tan2022fedproto}, FedGH \cite{yi2023fedgh} and FedTGP \cite{fedtgp}.\\
\textbf{Model heterogeneity.} To comprehensively evaluate the robustness and adaptability of our algorithm across different model architectures, we test our approach on four heterogeneous model groups (HMG): HMG$_3$ (4-layer CNN \cite{fedavg}, GoogleNet \cite{googlenet} and MobileNet\_v2 \cite{mobilenet}), HMG$_5$ (ResNet18/34/50/101/152 \cite{resnet}), HMG$_8$ (Combines HMG$_3$ and HMG$_5$), and HMG$_{10}$ (Extends HMG$_8$ with DenseNet121 \cite{desnet} and EfficientNet-B0 \cite{tan2019efficientnet}) \cite{fedtgp}. These HMGs span lightweight and complex convolutional networks, enabling evaluation of the proposed method across a broad range of model complexities.\\
\textbf{Statistical heterogeneity.} We conduct extensive experiments with two widely used statistically heterogeneous settings, the pathological setting and the practical setting \cite{fedavg, FedGen, FedDF}. For the pathological setting, we assign a fixed number of classes to the client. For the practical setting, we leverage Dirichlet distribution to simulate more realistic class imbalance across clients. The hyperparameter $\alpha$ controls the strength of heterogeneity. Notably, a smaller $\alpha$ implies a higher non-IID data distribution among clients.\\
\textbf{Training configuration.} Our experiments are conducted on an x86\_64 architecture with Ubuntu as the operating system. The platform is equipped with 8 NVIDIA V100 GPUs, each with 32 GB of memory, and CUDA version 12.2. Unless explicitly specified, we use the following settings. The federated learning setup includes a default configuration of 20 clients and the client participation ratio $\rho$ = 1. Both the client and server employ the SGD optimizer with a learning rate of 0.01, and both undergo training for a single epoch. The batch sizes for client $B$ and server $B_p$ are both set to 32. For model heterogeneity, we use the HMG$_8$ by default. For statistical heterogeneity testing, under the pathological setting, we distribute unbalanced data of 2/10/10/20 classes to each client from a total of 10/100/102/200 classes on Cifar-10/Cifar-100/Flowers102/TinyImagenet datasets. In the practical setting, a Dirichlet distribution with $\alpha$ = 0.05 is used to simulate real life scenario. Each client's private data is divided into a training set (75\%) and a test set (25\%). Each algorithm is trained three times, with each training run consisting of 100 epochs. For the evaluation, we count the best accuracy achieved in each training run, and the final result is calculated as the mean and variance of the accuracy of the three runs.\\
\textbf{Evaluation Metrics.} We measure the average test accuracy (\%) across all client models. We further evaluate the robustness of our algorithm under varying client participation rates, degrees of non-IID data, and feature dimension $K$.
% \textbf{Convergence rate:} We compare how the accuracy varies with the communication rounds.\\


\subsection{Performance Comparison}
The test accuracy of all methods across four datasets is presented in Tab. \ref{tab:test all comparison_results}. FedORGP consistently outperforms all baselines on these datasets, achieving up to a 8\% gain over FedTGP in pathological settings and 6.24\% in practical settings on Cifar-100. This improvement is due to the fact that FedORGP can separate features in terms of angles, which can be well integrated with the CE loss, thereby improving classification results more effectively than methods which only increase the Euclidean distance to perform prototype separation in high-dimensional space. In addition, FedORGP is also more efficient. While FedTGP requires 100 epochs of training on the server, FedORGP requires only 1 epoch of training with orthogonal regularisation.
%Unlike traditional prototype-based methods that rely solely on Euclidean distance to separate class prototypes, FedORGP enforces directional independence between classes by optimizing prototypes to be orthogonal. This directional independence mitigates overlap between classes, particularly in high-dimensional settings, where Euclidean distance alone may be insufficient for effective separation.

\begin{table}[ht]
\caption{Test on Cifar-100 (10 classes per client) with three different HMGs to test robustness to heterogeneous models.}
\label{tab:test HMG} % 设置标签以便引用
\resizebox{\linewidth}{!}{%
\begin{tabular}{|l|ccc|}
\hline
          & \multicolumn{3}{c|}{Heterogenous Model Groups}                                                                  \\ \hline
Settings  & \multicolumn{1}{c|}{HMG$_3$}               & \multicolumn{1}{c|}{HMG$_5$}               & HMG$_{10}$            \\ \hline
FML       & \multicolumn{1}{c|}{58.72 ± 0.15}          & \multicolumn{1}{c|}{42.84 ± 0.41}          & 45.50 ± 0.50          \\ \hline
LG-FedAvg & \multicolumn{1}{c|}{59.62 ± 0.24}          & \multicolumn{1}{c|}{51.46 ± 0.21}          & 50.19 ± 0.19          \\ \hline
FedGen    & \multicolumn{1}{c|}{59.82 ± 0.38}          & \multicolumn{1}{c|}{50.97 ± 0.26}          & 50.07 ± 0.13          \\ \hline
FedProto  & \multicolumn{1}{c|}{58.83 ± 0.52}          & \multicolumn{1}{c|}{49.08 ± 0.00}          & 48.04 ± 0.02          \\ \hline
FedKD     & \multicolumn{1}{c|}{\textbf{61.05 ± 0.04}} & \multicolumn{1}{c|}{46.24 ± 2.55}          & 47.57 ± 2.00          \\ \hline
FedGH     & \multicolumn{1}{c|}{59.21 ± 0.15}          & \multicolumn{1}{c|}{50.76 ± 0.30}          & 50.41 ± 0.19          \\ \hline
FedTGP    & \multicolumn{1}{c|}{60.41 ± 0.56}          & \multicolumn{1}{c|}{48.24 ± 0.47}          & 48.11 ± 0.07          \\ \hline
FedORGP   & \multicolumn{1}{c|}{60.64 ± 0.50}          & \multicolumn{1}{c|}{\textbf{61.58 ± 0.42}} & \textbf{58.54 ± 0.65} \\ \hline
\end{tabular}
}
\end{table}

\subsection{Impact of Model Heterogeneity}
To examine the impact of model heterogeneity in HtFL, we assess the performance of FedORGP on three additional HMG settings. We show results in Tab. \ref{tab:test HMG}, our findings indicate that all baselines perform well under the HMG$_3$ condition, as the reduced model heterogeneity significantly lessened its adverse impact on performance, while all methods perform worse with larger model heterogeneity. As the degree of model heterogeneity rises, the advantages of FedORGP becomes more pronounced, up to 10.12\% at HMG$_5$ and 8.13\% at HMG$_{10}$. In addition, FedORGP also has the smallest performance reduction when model heterogeneity increases, which shows that FedORGP is robust to model heterogeneity. 

\subsection{Robustness to Participation Rate}
To compare FedORGP against baselines under varying total client numbers $M$ and client participation rates $\rho$, we design three distinct settings. The results, shown in Tab. \ref{tab:pariticipation ratio}, ensure a consistent number of participating clients per round. FedORGP aims to promote directional independence, which is seamlessly integrated with CE loss. This separation minimizes inter-class overlap, thereby enabling the model to achieve robust generalization across clients with varying participation rates.

\begin{table}[ht]
\caption{Test on Cifar-100 (10 classes per client) under three participation rate settings with HMG$_8$. “-” means the baseline can't converge.}
\label{tab:pariticipation ratio} % 设置标签以便引用
\resizebox{\linewidth}{!}{%
\begin{tabular}{|l|ccc|}
\hline
          & \multicolumn{3}{c|}{Different $M$ Clients and Join Ratio}                                                       \\ \hline
Settings  & \multicolumn{1}{c|}{$M$=40, $\rho$=50\%}   & \multicolumn{1}{c|}{$M$=80, $\rho$=25\%}   & $M$=100, $\rho$=20\%  \\ \hline
FML       & \multicolumn{1}{c|}{33.96 ± 0.25}          & \multicolumn{1}{c|}{19.72 ± 0.11}          & 30.65 ± 0.25          \\ \hline
LG-FedAvg & \multicolumn{1}{c|}{47.05 ± 0.09}          & \multicolumn{1}{c|}{39.59 ± 0.23}          & 40.98 ± 0.51          \\ \hline
FedGen    & \multicolumn{1}{c|}{47.05 ± 0.40}          & \multicolumn{1}{c|}{39.76 ± 0.35}          & 41.10 ± 0.53          \\ \hline
FedProto  & \multicolumn{1}{c|}{38.30 ± 0.53}          & \multicolumn{1}{c|}{13.49 ± 0.84}          & 16.16 ± 0.55          \\ \hline
FedKD     & \multicolumn{1}{c|}{38.12 ± 3.36}          & \multicolumn{1}{c|}{21.76 ± 1.60}          & 34.59 ± 2.73          \\ \hline
FedGH     & \multicolumn{1}{c|}{46.82 ± 0.24}          & \multicolumn{1}{c|}{40.08 ± 0.35}          & -                     \\ \hline
FedTGP    & \multicolumn{1}{c|}{44.87 ± 0.58}          & \multicolumn{1}{c|}{37.51 ± 0.36}          & 36.75 ± 0.42          \\ \hline
FedORGP   & \multicolumn{1}{c|}{\textbf{54.88 ± 0.45}} & \multicolumn{1}{c|}{\textbf{44.86 ± 0.11}} & \textbf{43.22 ± 0.45} \\ \hline
\end{tabular}
}
\end{table}

% \subsection{Convergence rate}
% In this section, we test accuracy varies with the communication
% rounds. We can see Fig. \ref{fig:accvsepoch} that on both Cifar-100 and Flowers102 datasets, FedORGP consistently converges to the highest accuracy at the fastest rate, demonstrating its high efficiency. Such efficiency is particularly advantageous in FL scenarios, where reducing communication overhead is critical.

% \begin{figure}[ht]
%     \centering
%     \begin{subfigure}[b]{0.49\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{pictures/accuracy_vs_epoch_Cifar-100.pdf}
%         \caption{Cifar-100}
%         \label{fig:sub-fig1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.49\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{pictures/accuracy_vs_epoch_Flowers102.pdf}
%         \caption{Flowers102}
%         \label{fig:sub-fig2}
%     \end{subfigure}
%     \caption{Compare accuracy vs epoch of different algorithms}
%     \label{fig:accvsepoch}
% \end{figure}

\begin{table}[ht]
\caption{Test on two data distributions under various degrees of non-IID on the Cifar-100 dataset with HMG$_8$.}
\label{tab:noniid} % 设置标签以便引用
\resizebox{\linewidth}{!}{%
\begin{tabular}{|l|cc|cc|}
\hline
Settings  & \multicolumn{2}{c|}{Pathological Setting}                          & \multicolumn{2}{c|}{Practical Setting}                             \\ \hline
          & \multicolumn{1}{c|}{5 classes/client}      & 20 classes/client     & \multicolumn{1}{c|}{$\alpha$=0.1}          & $\alpha$=0.01         \\ \hline
FML       & \multicolumn{1}{c|}{66.40 ± 0.33}          & 33.13 ± 0.24          & \multicolumn{1}{c|}{37.30 ± 0.09}          & 61.17 ± 0.24          \\ \hline
LG-FedAvg & \multicolumn{1}{c|}{71.84 ± 0.17}          & 37.22 ± 0.36          & \multicolumn{1}{c|}{39.97 ± 0.11}          & 66.06 ± 0.14          \\ \hline
FedGen    & \multicolumn{1}{c|}{71.68 ± 0.09}          & 37.47 ± 0.09          & \multicolumn{1}{c|}{40.33 ± 0.37}          & 66.06 ± 0.08          \\ \hline
FedProto  & \multicolumn{1}{c|}{70.27 ± 0.07}          & 35.15 ± 0.18          & \multicolumn{1}{c|}{34.30 ± 0.16}          & 56.97 ± 0.36          \\ \hline
FedKD     & \multicolumn{1}{c|}{69.75 ± 1.35}          & 34.24 ± 0.79          & \multicolumn{1}{c|}{39.81 ± 1.12}          & 63.86 ± 1.46          \\ \hline
FedGH     & \multicolumn{1}{c|}{71.42 ± 0.29}          & 36.99 ± 0.23          & \multicolumn{1}{c|}{39.90 ± 0.21}          & 65.90 ± 0.27          \\ \hline
FedTGP    & \multicolumn{1}{c|}{69.86 ± 0.33}          & 35.98 ± 0.30          & \multicolumn{1}{c|}{38.60 ± 0.10}          & 67.12 ± 0.11          \\ \hline
FedORGP   & \multicolumn{1}{c|}{\textbf{75.95 ± 0.24}} & \textbf{45.45 ± 0.34} & \multicolumn{1}{c|}{\textbf{44.70 ± 0.29}} & \textbf{70.32 ± 0.36} \\ \hline
\end{tabular}
}
\end{table}

\subsection{Robustness to Statistical Heterogeneity}
To evaluate the robustness of all baseline methods under different non-IID data distributions, we conducted experiments on two dataset configurations. As demonstrated in Tab. \ref{tab:noniid}, FedORGP ensures optimal inter-class separation by leveraging orthogonality regularization, reducing embeddings overlap even in highly skewed data distributions. This demonstrates its effectiveness in promoting generalization in statistical heterogeneity environments.

\subsection{Impact of Feature Dimension}
To assess the robustness of all baselines to the $K$, we performe experiments with three feature dimensions: 128, 256, and 1024, as indicated in Fig. \ref{fig:k-dimension}. Results indicate that all baselines maintain a high degree of robustness, showing minimal variation in performance.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{pictures/algorithm_test_accuracy_K=128.pdf}
        \caption{K=128}
        \label{fig:K=128}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{pictures/algorithm_test_accuracy_K=256.pdf}
        \caption{K=256}
        \label{fig:K=256}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{pictures/algorithm_test_accuracy_K=512.pdf}
        \caption{K=512}
        \label{fig:K=512}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{pictures/algorithm_test_accuracy_K=1024.pdf}
        \caption{K=1024}
        \label{fig:K=1024}
    \end{subfigure}
    \caption{Test on Cifar-100 (10 classes per client) under different feature dimensions with HMG$_8$.}
    \label{fig:k-dimension}
\end{figure}

\subsection{Ablation Study}
This section analyzes the ablation study in Tab. \ref{tab:ablation}. FedORGP without orthogonality regularization (FedORGP w/o OC) aligns the embedding layer with the global prototype derived from weighted averaging. While the performance on CIFAR-10 is passable, FedORGP w/o OC exhibits a sharp performance decline on CIFAR-100 and Flowers102 datasets, likely due to substantial global prototype overlap. Introducing the orthogonality regularization in FedORGP markedly enhances prototype separation, improving performance by explicitly increasing angular separation across classes. FedORGP outperforms FedTGP, as reducing prototype similarity integrates more effectively with cross-entropy (CE) loss than merely increasing prototype distances.
% In this section, we present an analysis of the ablation study in Tab. \ref{tab:ablation}. FedProto aligns client feature representations with global prototypes, using the distance between these representations and prototypes for prediction. However, the approach of decreasing Euclidean distance between prototypes does not integrate effectively with the CE loss, which explains why FedProto underperforms compared to FedORGP without orthogonality regularization (FedORGP w/o OC) on the CIFAR-10 and Flowers102 datasets. On the CIFAR-100 dataset, however, FedProto exhibits slightly better performance than FedORGP w/o OC. This discrepancy may be attributed to the higher degree of feature overlap in the CIFAR-100 dataset, which limits the efficacy of separation techniques in the absence of orthogonality regularization. When orthogonality regularization are incorporated into FedORGP, the separation between prototypes is significantly enhanced, leading to a marked improvement in performance across datasets.

\begin{table}[ht]
\caption{Test on three datasets under the practical setting using the HMG$_8$ for ablation study.}
\label{tab:ablation} % 设置标签以便引用
\resizebox{\linewidth}{!}{%
\begin{tabular}{|l|l|l|l|}
\hline
           & \multicolumn{1}{c|}{FedTGP}     & \multicolumn{1}{c|}{FedORGP w/o OR} & \multicolumn{1}{c|}{FedORGP}               \\ \hline
Cifar-10    & 84.75 ± 0.06 & 78.05 ± 0.52                          & \textbf{85.71 ± 0.10} \\ \hline
Cifar-100   & 53.12 ± 0.24 & 35.11 ± 0.63                          & \textbf{61.12 ± 0.15} \\ \hline
Flowers102 & 58.60 ± 0.14 & 42.43 ± 0.57                          & \textbf{64.15 ± 1.66} \\ \hline
\end{tabular}
 }
\end{table}

\subsection{Hyperparameter Combination}
We conducted a grid search across $(\lambda_c, \lambda_s, \gamma)$ in Tab. \ref{tab:hyperpara} to determine the optimal combination of hyperparameters. The selected values for $\lambda_s$ and $\gamma$, specifically (1, 10), enable the orthogonality constraint to effectively enhance inter-class separation without sacrificing the integrity of class representations. This configuration fosters a more robust global prototype compared to the (10, 100) pairing at the same ratio. Additionally, a higher $\lambda_c$ imposes a stricter alignment constraint, thereby improving feature representation consistency across clients, which is crucial for maintaining a unified representation space against statistical heterogeneity. The highest accuracy 61.15\% is achieved with the (100, 1, 10) setting.

\begin{table}[ht]
\caption{Hyperparameter search on Cifar-100 with HMG$_8$.}
\label{tab:hyperpara} % 设置标签以便引用
\resizebox{\linewidth}{!}{%
\begin{tabular}{|cccc|cccc|cccc|}
\toprule
\huge $\lambda_c$ & \huge $\lambda_s$ &\huge  $\gamma$ &\huge  Acc &\huge  $\lambda_c$ & \huge $\lambda_s$ & \huge $\gamma$ & \huge Acc & \huge $\lambda_c$ & \huge $\lambda_s$ & \huge $\gamma$ & \huge Acc \\
\midrule
\huge 100.0 & \huge  100.0 & \huge  100.0 & \huge  40.34 & \huge  10.0 & \huge  100.0 & \huge  100.0 & \huge  50.67 & \huge  1.0 & \huge  100.0 & \huge  100.0 & \huge  53.94 \\
\huge 100.0 & \huge  100.0 & \huge  10.0 & \huge  40.32 & \huge  10.0 & \huge  100.0 & \huge  10.0 & \huge  48.12 & \huge  1.0 & \huge  100.0 & \huge  10.0 & \huge  54.43 \\
\huge 100.0 & \huge  100.0 & \huge  1.0 & \huge  48.32 & \huge  10.0 & \huge  100.0 & \huge  1.0 & \huge  51.52 & \huge  1.0 & \huge  100.0 & \huge  1.0 & \huge  54.82 \\
\huge 100.0 & \huge  10.0 & \huge  100.0 & \huge  59.17 & \huge  10.0 & \huge  10.0 & \huge  100.0 & \huge  56.60 & \huge  1.0 & \huge  10.0 & \huge  100.0 & \huge  53.94 \\
\huge 100.0 & \huge  10.0 & \huge  10.0 & \huge  55.37 & \huge  10.0 & \huge  10.0 & \huge  10.0 & \huge  54.76 & \huge  1.0 & \huge  10.0 & \huge  10.0 & \huge  54.34 \\
\huge 100.0 & \huge  10.0 & \huge  1.0 & \huge  51.96 & \huge  10.0 & \huge  10.0 & \huge  1.0 & \huge  53.47 & \huge  1.0 & \huge  10.0 & \huge  1.0 & \huge  54.22 \\
\huge 100.0 & \huge  1.0 & \huge  100.0 & \huge  58.97 & \huge  10.0 & \huge  1.0 & \huge  100.0 & \huge  55.14 & \huge  1.0 & \huge  1.0 & \huge  100.0 & \huge  50.64 \\
\huge \textbf{100.0} & \huge  \textbf{1.0} & \huge  \textbf{10.0} & \huge  \textbf{61.15} & \huge  10.0 & \huge  1.0 & \huge  10.0 & \huge  57.35 & \huge  1.0 & \huge  1.0 & \huge  10.0 & \huge  54.94 \\
\huge 100.0 & \huge  1.0 & \huge  1.0 & \huge  59.51 & \huge  10.0 & \huge  1.0 & \huge  1.0 & \huge  57.11 & \huge  1.0 & \huge  1.0 & \huge  1.0 & \huge  54.20 \\
\bottomrule
\end{tabular}
}
\end{table}