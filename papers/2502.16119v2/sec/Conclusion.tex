\section{Conclusion}
\label{sec:conclusion}

In this work, we propose a novel HtFL method, FedORGP, which employs orthogonality regularization on global prototypes at the server side to improve inter-prototype separation and maximize directional independence while preserving semantic integrity. On the client side, FedORGP aligns the embeddings with prototypes, enhancing its compatibility with CE loss. This approach overcomes the limitations of FedTGP by enhancing prototype separation through angular separation rather than merely increasing Euclidean distances between prototypes. Extensive experiments demonstrate that FedORGP consistently outperforms all baseline methods, and exhibits superior performance in scenarios where model and statistical heterogeneity coexist.