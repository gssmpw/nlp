\section{Introduction}
\label{sec:introduction}

In recent years, Federated Learning (FL) has emerged as a promising distributed machine learning paradigm \cite{promising}. FL eliminates the need for clients to exchange data, allowing data to remain decentralized, which makes it an favorable solution to data privacy challenges \cite{uncentralized, fedavg}.

Previous work mainly focuses on homogeneous FL, which assumes that all client models are identical. However, in large-scale real-world scenarios, considerable disparities exist in both client data distribution, often termed statistical heterogeneity, and hardware resources, known as system heterogeneity \cite{data_and_model_heterogenous, FedProx}. Data heterogeneity severely impacts the convergence and performance of global model \cite{model-training-on-noniid-data, fl_challenges_methods_directions}. Furthermore, discrepancies in hardware resources can result in model heterogeneity across clients, posing significant challenges to conventional FL approaches that rely on aggregating model parameters \cite{data_and_model_heterogenous, FLadvance}. In addition, homogeneous FL trains a shared global model by exchanging gradients, which further imposes significant communication costs as well as privacy exposure risks \cite{aggregating-vs-privacy, fedtgp}.

To tackle these challenges, Heterogeneous Federated Learning (HtFL) has emerged as a novel FL paradigm capable of handling both data heterogeneity and model heterogeneity simultaneously \cite{tan2022fedproto, yi2023fedgh, fedtgp}. HtFL incorporates prototype-based learning, which communicates client prototypes rather than model gradients to the server, thus alleviating issues related to model diversity and communication costs. However, existing weighted average HtFL solutions like FedProto \cite{tan2022fedproto} faces several limitations. First, aggregating global prototypes on the server side via weighted averaging requires clients to upload sample sizes, which may lead to leakage of data distribution information \cite{yi2023fedgh}. Second, as shown in Fig.\ref{fig:FedProto t-SNE}, the weighted average prototype lacks a well-defined decision boundary, resulting in overlapping feature distributions among different classes in the feature space \cite{fedtgp}.

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.33\linewidth}
        \includegraphics[width=\linewidth]{pictures/FedProto_visulization.pdf}
        \caption{FedProto}
        \label{fig:FedProto t-SNE}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
        \includegraphics[width=\linewidth]{pictures/FedTGP_visulization.pdf}
        \caption{FedTGP}
        \label{fig:FedTGP t-SNE}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
        \includegraphics[width=\linewidth]{pictures/FedOC_visulization.pdf}
        \caption{FedORGP}
        \label{fig:FedORGP t-SNE}
    \end{subfigure}
    \caption{We train models on the CIFAR-10 dataset and use t-SNE \cite{t-sne} to visualize their performance on previously unseen test samples (16 per class) within the feature space, with triangles indicating prototypes and circles denoting samples. The results indicate that FedProto \cite{tan2022fedproto} exhibits weak feature separation. FedTGP \cite{fedtgp} increases prototype margin but still lacks sufficient distinction in the feature space. In contrast, our FedORGP can reduce inter-sample similarity and increase intra-class compactness, thereby effectively classifying the majority of samples. This result suggests that FedORGP exhibits robust generalization performance under conditions of both statistical and model heterogeneity.}
    \label{fig:distributions}
\end{figure*}

While the recent HtFL solution FedTGP \cite{fedtgp} has successfully improved separation by increasing the Euclidean distance between prototypes, this approach still faces several limitations. First, contrastive learning, which works together with cross-entropy (CE) loss, primarily reduces the intra-class distance between prototypes but fails to explicitly enforce inter-class separation. Second, as shown in Fig.\ref{fig:FedTGP t-SNE}, in high-dimensional spaces, the Euclidean distance makes it difficult to effectively distinguish different samples as distance variations between samples diminish \cite{comprehensive-survey-of-distance}. Third, augmenting Euclidean distances fails to effectively leverage the angular separation characteristics inherent in CE loss \cite{opl}.

To address these limitations, we propose a novel HtFL algorithm called FedORGP, which optimizes global prototypes through orthogonality regularization. FedORGP improves intra-class prototype similarity while preserving semantic integrity and explicitly promotes angular separation between classes, ensuring that global prototypes achieve maximal directional independence in the feature space. Under the guidance of global prototypes, as demonstrated in Fig.\ref{fig:FedORGP t-SNE}, clients can ensure that the feature representations are directionally aligned with their corresponding global prototypes in the feature space, thereby facilitating a more effective integration with the angular properties inherent to CE loss.

We evaluate FedORGP against seven state-of-the-art HtFL methods across four datasets in scenarios where both data heterogeneity (practical distribution and Dirichlet distribution) and model heterogeneity (four levels of model heterogeneity) coexist. The experimental results show an improvement in accuracy by 10.12\% over the best baseline under both statistical and model heterogeneity. Our contributions can be summarized as follows:

\begin{itemize}
    \item We present a novel framework FedORGP for HtFL with orthogonality regularization. To the best of my knowledge, this is the first work to introduce orthogonality regularization into heterogeneous federated learning where data and model heterogeneity coexist.
    \item We observe that increasing the Euclidean distance between prototypes is incapable of effectively integrating with the angular characteristics inherent to CE loss. Our proposed FedORGP algorithm enhances intra-class clustering within the feature space while explicitly promoting angular separation between classes.
    \item We offer a theoretical convergence guarantee for FedORGP and rigorously establish the convergence rate under non-convex conditions.
\end{itemize}