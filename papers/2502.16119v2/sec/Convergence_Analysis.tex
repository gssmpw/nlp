\section{Convergence Analysis}
\label{sec:convergence analysis}

To analyse the convergence of FedORGP, we first introduce some additional notes as in the existing framework. Unless otherwise stated, we always write the client-side loss function $\mathcal{L}_k(\mathcal{D}_k, \omega_k, \mathcal{P})$ as $\mathcal{L}_k$. In order to analyse the convergence of FedORGP, we first introduce some additional assumptions \cite{tan2022fedproto, yi2023fedgh, FedNH}.\\
\textbf{Assumption 1.} (Lipschitz Smooth). The k-th client's gradient of local loss function is $L_1$-Lipschitz continuous:
\begin{equation}
% \scalebox{0.9}{$
    \|\nabla\mathcal{L}_k^{t_1}-\nabla\mathcal{L}_k^{t_2}\|_2 
    \leqslant L_1\|\omega_k^{t_1}-\omega_k^{t_2}\|_2, 
    \label{eq:lipschitz-continuous}
% $}
\end{equation}
where $\forall t_1, t_2 > 0, k \in \{1, \dots, m\}$. That also means that local objective function is $L_1$-Lipschitz smooth:
\begin{equation}
    % \scalebox{0.9}{$ % 缩放 0.9 倍
        \mathcal{L}_k^{t_1} - \mathcal{L}_k^{t_2} \leqslant \langle \nabla \mathcal{L}_k^{t_2}, (\omega_k^{t_1} - \omega_k^{t_2}) \rangle + \frac{L_1}{2} \|\omega_k^{t_1} - \omega_k^{t_2}\|_2^2,
    % $}
    \label{eq:lipschitz-smooth}
\end{equation}
\textbf{Assumption 2.} (Unbiased Gradient and Bounded Variance). The stochastic gradient $g_{k}^t=\nabla\mathcal{L}(\xi_i, \omega^t)$ is an unbiased estimator of the local gradient for k-th client, where $\xi_i$ is a random variable representing the randomness in the data (e.g., mini-batch). Suppose its expectation is
\begin{equation}
% \scalebox{0.9}{$
    \mathbb{E}_{\xi_i\thicksim D_i}[g_{k}^t]=\nabla\mathcal{L}_k(\omega_{k}^t),\forall k\in\{1,2,\ldots,m\},
    \label{eq:unbiased gradient}
% $}
\end{equation}
and there exists $\sigma^2 \geq 0$, then the variance of random gradient $g_{k}^t$ is bounded by:
\begin{equation}
% \scalebox{0.9}{$
        \mathbb{E}[\left\|g_{k}^t-\nabla\mathcal{L}_k(\omega_{k}^t)\right\|_2^2]\leqslant\sigma^2,\forall k\in\{1,2,\ldots,m\}.
    \label{eq:bounded variance}
% $}
\end{equation}
\textbf{Assumption 3.} (Bounded Gradient). The stochastic gradient of the global prototype is bounded by $G$:
\begin{equation}
\scalebox{0.9}{$
        \mathbb{E}[\|\nabla\mathcal{L}_{server}\|_2]\leqslant G
    \label{eq:bounded eulidean norm}
$}
\end{equation}
We denote $e \in \{0, 1, 2, \ldots, E\}$ as the local iteration, and t as the global round. Here, $tE$ indicates the time step before orthogonality regularization at $t$ round, and $tE + 0$ represents the interval between global prototype training and the first iteration of the $(t+1)$ round. Under above assumptions, we present the theoretical results at the non-convex condition. Since all client-side loss function share the same property, we omit the subscript $k$ and denote the loss function as $\mathcal{L}$. The detailed proof process is in the Appendix.\\
\textbf{Lemma 1.} Based on Assumption 1 and 2, after clients training one round, the local client model loss satisfies \cite{tan2022fedproto}:
\begin{equation}
    % \scalebox{0.9}{$
    \begin{aligned}
        \mathbb{E}\left[\mathcal{L}^{(t+1)E}\right] \leqslant &\mathcal{L}^{tE+0} - \left(\eta - \frac{L_1 \eta^2}{2}\right) \sum_{e=0}^{E-1} \|\mathcal{L}^{tE+e}\|_2^2 + \\
        &\frac{L_1 E \eta^2}{2} \sigma^2
    \end{aligned}
    % $}
\label{eq:lemma 4.1}
\end{equation}
\textbf{Lemma 2.} Based on Assumption 3, we further analyze how the introduction of global prototype orthogonality regularization influences the client-side loss:
\begin{equation}
    % \scalebox{0.9}{$
\mathbb{E}\left[\mathcal{L}^{(t+1)E+0}\right]\leqslant\mathbb{E}\left[\mathcal{L}^{(t+1)E}\right]+\lambda_c \eta G.
    \label{eq:lemma 4.2}
    % $}
\end{equation}
\textbf{Theorem 1.} Leveraging Lemma 1 and Lemma 2, we establish our main theoretical convergence results of the loss reduction over multiple steps in local training:
\begin{equation}
    % \scalebox{0.9}{$
    \begin{aligned}
        \mathbb{E}\left[\mathcal{L}^{(t+1)E+0}\right]\leqslant &\mathcal{L}^{tE+0}-\left(\eta-\frac{L_1\eta^2}2\right)\sum_{e=0}^{E-1}\|\mathcal{L}^{tE+e}\|_2^2 +\\
        &\frac{\eta^2 L_1E\sigma^2}2 +\lambda_c \eta G.
    \end{aligned}
    % $}
    \label{eq:theroem 1}
\end{equation}
\textbf{Theorem 2.} We extend the analysis to multiple rounds, providing a convergence rate for FedORGP in a non-convex setting. For an arbitrary client and any $\epsilon> 0$, the following inequality holds:
\begin{equation}
    \begin{aligned}
        \frac1T\sum_{t=0}^{T-1}\sum_{e=0}^{E-1}\mathbb{E}\left[\|\mathcal{L}^{tE+e}\|_2^2\right]\leqslant &\frac{2\left(\mathcal{L}^{t=0}-\mathcal{L}^*\right)}{T\eta\left(2-L_1\eta\right)}+\frac{L_1E\eta\sigma^2}{2-L_1\eta}+\\
        &\frac{2\lambda_c G}{2-L_1\eta}\leqslant \epsilon\\
    \end{aligned}
    \label{eq:theroem 2}
\end{equation}
when $\eta<\frac{2(\epsilon-\lambda_c G)}{L_1\left(\epsilon+E\sigma^2\right)}$ and $\lambda_c < \frac{\epsilon}{G}$.
 It shows that the model can converge to a solution with a rate that is proportional to $O(1/T)$, where $T$ is the number of communication rounds.
