\section{Discussion and Analysis}
\label{sec:discussion}
\subsection{Framework analysis}
\subsubsection{Ablation study shows improvements of components}
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./picture/ablation.pdf}
  \caption{The ablation studies on the ``T1: 3D Cardiac structures" and ``T3: 2D Chest structures" demonstrate the great contributions of the components in our framework. The ``C" and ``B" are the learning for continuity and bijection.}
  \label{Fig:ablation}
\end{figure}
The ablation studies on the ``T1: 3D Cardiac structures" and ``T3: 2D Chest structures" demonstrate the great contributions of our innovations. It has two observations: \textbf{1)} The innovations in our framework all contribute to the performance. When we add the GVS and GSS into the correspondence learning, the two tasks all achieve very significant improvement compared with the direct segmentation learning on few labeled images. The smoothness loss $\mathcal{L}_{smo}$ for the continuity further improves the deformation accuracy and the smoothness (topology-preservation ability), so that the two tasks all achieve further improvement. Finally, when adding the loss for bijection, these tasks all obtain the highest segmentation DSC. This illustrates that learning under the condition of medical images' topology will improve the representation of the backbone network $\mathcal{N}_{\theta}$, bringing better performance on target tasks. \textbf{2)} Compared between ``T1" and ``T3", their learning only with GVS have very different results due to the appearance of their images. The learning with GVS in T1 improves the segmentation performance, but it in T3 extremely weakens the performance. Because the cardiac CT images in T1 are enhanced by contrast agents, they have distinct regions and will provide guidance to learn the correspondence. However, the chest X-ray images in T3 are projected from 3D human body and have very low contrast, so they are unable to measure the alignment degree and interfere with the correspondence learning. When adding our GSS, the performances in these two tasks are all improved. Because the measurement of the features will avoid interference from the appearance limitation, thus achieving better optimization guidance for correspondence.
\subsubsection{Analysis of hyper-parameters}
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./picture/hyper.pdf}
  \caption{The ablation of the hyper-parameters on the ``T1: 3D Cardiac structures" show the effects from the weight of the smoothness loss $\lambda_{smo}$, the positive pairs $\lambda_{pos}$, and the GVS loss $\lambda_{GVS}$. The $|J_{\phi}|\leq0$ (\%) is  Jacobian matrix  \cite{he2021few} which evaluates smoothness of the deformation.}\label{Fig:hyper}
\end{figure}
We analyze the three hyper-parameters in our framework, i.e., the weight of the smoothness loss $\lambda_{smo}$, the positive pairs  $\lambda_{pos}$ and the GVS loss $\lambda_{GVS}$. With the enlarging of these three hyper-parameters, the segmentation performances of our framework are all improved and then weakened. Because: \textbf{a)} The smoothness loss improves the topology preservation ability and reduces the deformation degree. When the $\lambda_{smo}$ is small, the deformation accuracy (orange line) and topology preservation degree (blue line) are improved, promoting the reliability of correspondence. However, when the $\lambda_{smo}$ is large, excessive smoothing reduces the deformation accuracy, weakening the segmentation. \textbf{b)} The learning of positive pairs clusters the features in corresponding positions, making better representation for the same semantic features. However, when the $\lambda_{pos}$ is too large, the gradient from this loss will be much larger than the optimization for the negative pair ``OPT$()$", which makes the model tend to represent all features as consistent reducing their discrimination. The positive pairs will be further analyzed in Sec.\ref{sec:positive}. \textbf{c)} The training-free property of GVS improves the deformation accuracy and stabilizes the correspondence training when the $\lambda_{GVS}$ is small. However, the problem of appearance variation in the images is enlarged when the $\lambda_{GVS}$ is large, so the unreliable similarity will give an inaccurate optimization target, weakening the deformation accuracy and smoothness, and reducing the final segmentation performance. %We utilize the $\lambda_{pos}=0.1$ and $\lambda_{IL}=0.8$ in our all experiments.

\subsubsection{Analysis of deformation in learning process}
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./picture/registration.pdf}
  \caption{The visualization of the deformation in the learning process on the ``T1: 3D Cardiac structures". The first row is the line chart of the segmentation and deformation performance. The second row is the grids which demonstrates the deformation degree. The third row is the deformed image A during the learning process.}\label{Fig:registration}
\end{figure}
As shown in Fig.\ref{Fig:registration}, the deformation in our GEMINI-Semi demonstrates the great correspondence of the semantic regions between images. During the training, our model will quickly learn the correspondence of semantic regions between images in the beginning, so the deformation DSC is 78.1\% and the segmentation DSC is 90.3\% in the $8\times10^{3}$th iteration (1/5 of all iteration amount). These scores in this iteration have been very close to the final scores (78.4\% and 91.2\%). Visually, the deformed image in the $8\times10^{3}$th iteration also has a very high alignment degree to the target image B. In the later learning iterations, the segmentation DSC is improving slightly and up to 91.2\%, although there is no improvement in the deformation DSC. This is because the learn reliable correspondence is still promoting the representation learning via gradient, and the great alignment provides the reliable learning of positive pairs.
\subsubsection{Analysis of positive pairs $\mathcal{L}_{pos}$}
\label{sec:positive}
\begin{table}
\centering
\caption{The learning for positive pairs $\mathcal{L}_{pos}$ in different setting on the ``T1: 3D Cardiac structures". The ``No $\mathcal{L}_{pos}$" means the training without positive pairs. The ``Half $\mathcal{L}_{pos}$" means when training to half of the total iteration amount, the learning of positive pairs is added. The ``Full $\mathcal{L}_{pos}$" means the whole training process with positive pairs.} %The $|J_{\phi}|\leq0$ (\%) is  Jacobian matrix  \cite{he2021few} which evaluates smoothness (topological preservation) degree of the deformation.
%\resizebox{\linewidth}{!}{
\setlength{\tabcolsep}{4.7mm}{
\begin{tabular}{lcccccccccccccccc}
\toprule
\textbf{Type}&\textbf{No $\mathcal{L}_{pos}$}&\textbf{Half $\mathcal{L}_{pos}$}&\textbf{Full $\mathcal{L}_{pos}$}\\
\midrule
DSC$_{\pm std}\uparrow$ &90.3$_{\pm3.6}$ &90.5$_{\pm3.5}$ &\textbf{91.2$_{\pm3.6}$}\\
\bottomrule
\end{tabular}}
%}
\label{tab:pos}
\end{table}
As shown in Tab.\ref{tab:pos}, the learning of our positive pairs $\mathcal{L}_{pos}$ in different settings demonstrates its reliability. Due to the potential misalignment between images at the beginning of the training, the positive pairs will be constructed between misaligned regions, making some potential false positive pairs. Therefore, this experiment performs three situations to evaluate this potential problem. Without the learning of positive pairs (``No $\mathcal{L}_{pos}$"), our GEMINI-Semi has 90.3\% segmentation DSC. When adding the learning of positive pairs at half of the total iteration amount (``Half $\mathcal{L}_{pos}$"), it has 90.5\% DSC which has 0.2\% improvement. When directly adding the positive pairs in the whole process (``Full $\mathcal{L}_{pos}$"), it brings 0.9\% segmentation DSC improvement owing to the constraint for the feature consistency. As conclusion, the potential problem of false positive pairs in our task has less influence on learning, because the alignment accuracy is improved fast as demonstrated in Fig.\ref{Fig:registration}, and the potential interference is reduced in most of the training process. Therefore, we adopted the ``Full $\mathcal{L}_{pos}$" in our framework.
\subsubsection{Analysis of pre-training data amount in SS-MIP}
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./picture/line1.pdf}
  \caption{The pre-training and fine-tuning data amount analysis on SS-MIP.}\label{Fig:line}%a) The pre-training data amount evaluation shows our high representability in the pretext task due to our reliable correspondence discovery. b)-c) The fine-tuning data amount on the inner-scene transferring and inter-scene transferring show our significant improvement in smaller transferring data amount situations, demonstrating our great data efficiency.
\end{figure}
As shown in Fig.\ref{Fig:line} a), our GEMINI-MIP has a higher representation efficiency with fewer pre-training data. For the DenseCL, when enlarging the pre-training data amount to 75\%, it even has weaker performance than the 25\%. This is because when the enlarging of pre-training data amount, the contribution of the low-level texture knowledge is reducing and the high-level semantic knowledge is improving. The large-scale FP\&N problem makes the DenseCL learn the high-level semantics which deviate from reality, forbidding the improvement from larger datasets. The improvements of the BYOL and Model Genesis all slow down, owing to the limited representation ability in generation-based learning and the learning without negative pairs. Our GEMINI-MIP has achieved the best performance only with 25\% data which is higher than the BYOL with 100\% data, owing to our reliable learning of positive and negative pairs.

\subsubsection{Analysis of fine-tuning data amount in SS-MIP}
In the \emph{inner-scene} situation (Fig.\ref{Fig:line} b)), our GEMINI-MIP also has a better-transferring ability with fewer data, demonstrating our great data efficiency and cost-saving ability. In the T1: SCR, we evaluate the transferring performance with the enlarging of the downstream data amount in the SCR dataset (25\%, 50\%, 75\%, 100\%). Our GEMINI-MIP achieves the highest performance in all data amount settings. Especially with only 25\% data, it achieves higher performance than the DenseCL, Model Genesis, and Scratch models with 100\% data, and similar performance as the BYOL with 75\% data. Due to the FP\&N problem, although the DenseCL has higher performance than the ``Scratch", its improvement is very slight. Without the FP\&N problem, the BYOL shows a more competitive performance improvement, but it is still lower than ours owing to the lack of negative pairs in the BYOL learning, limiting the discrimination of features.

In the \emph{inter-scene} situation (Fig.\ref{Fig:line} c)), our GEMINI-MIP still has better transferring ability than other methods. We further evaluate the transferring performance with the enlarging of the downstream data amount in the KiPA22 dataset (25\%, 50\%, 75\%, 100\%) for an inter-scene evaluation. Although our GEMINI-MIP is unable to keep its performance when the data amount reduces which is different from it in the inner-scene situation, it still has the highest performance compared with the other four methods in each data amount. This is because our reliable learning of positive and negative feature pairs makes the networks pre-learn low-level patterns effectively which are shared in different tasks, so its representability of low-level features will transformed to different scenes, improving the transferring performance. The DenseCL, BYOL, and Model Genesis models only have similar or lower performance than the ``Scratch", because their pre-learned representations deviate from the reality due to their FP\&N problems, and this property is still transformed to the other scenes making large limitations.

\subsubsection{Analysis of the promotion for learning efficiency}
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./picture/learning.pdf}
  \caption{Our GEMINI-MIP has powerful learning efficiency both in the inner-scene and inter-scene transferring tasks.}\label{Fig:learning}
\end{figure}
As shown in Fig.\ref{Fig:learning}, in the transferring of downstream tasks, our GEMINI-MIP has a very powerful learning ability both in inner-scene and inter-scene situations. Compared with ``Scratch", our method has better performance both in accuracy (validation DSC) and learning speed. It only utilizes less than $5\times10^{3}$ iterations and can achieve the performance that exceeds ``Scratch"'s best model who has to train more than $20\times10^3$ iterations. Compared to the best models, our model also has more than 2.4\% DSC higher than the ``Scratch". This is because our model provides a reliable pre-trained initialization when learning the downstream tasks, promoting optimization efficiency. In the inner-scene transferring (T1: SCR$_{25}$), the DenseCL has a worse learning ability than the ``Scratch", because the large interference of the large scale FP\&N problem which makes the initial representation deviate from reality. All four pre-trained models (GEMINI-MIP, DenseCL, BYOL, Model Genesis) have better learning ability than the ``Scratch" in T3: CANDI (numerous small brain regions), because the pre-trained initialization will promote the perception for small structures which are challenging to learn by 2D network.
\subsubsection{Analysis of self-restoration in GEMINI-MIP}
\label{subsubsec:analysisiofrestor}
\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{./picture/opt.pdf}
  \caption{The necessity of the fundament in our GEMINI-MIP. When learning without the fundamental learning task, the GVS loss converges slowly due to the initial weak representation limiting the GSS and GVS for correspondence. When adding the fundament (self-restoration), warmup from the basic representation of semantic regions drives correspondence learning efficiently.}\label{Fig:opt}
\end{figure}
The self-restoration learns a basic representation for the features, thus providing an efficient similarity measurement in our GSS and driving more efficient correspondence learning. As shown in Fig.\ref{Fig:opt}, we evaluate the GVS loss in the training process to demonstrate the improvement of correspondence degree. When only learning our GEMINI-MIP without the fundamental task, the networkâ€™s initial weak representation makes inefficient learning of the correspondence, and until the late stage of the training, the GVS began to decline. When adding the fundamental pretext task, driven by the basic representation of semantic regions from the self-restoration, the GVS loss is converging continuously to learn the correspondence of semantic regions.
%\subsubsection{Analysis of our representation ability}

\subsection{Observations and discussions}
\subsubsection{FS-Semi v.s. SS-MIP}
With our homeomorphism prior, our GEMINI has powerful representation learning ability with little (FS-Semi) or no (SS-MIP) supervision. In our experiment (Sec.\ref{sec:task2}) of FS-Semi, the unlabeled images and the very few labeled images are learned together, so that the networks in the DCRL methods have a supervised optimization target which makes a basic distinction for the features of different segmentation regions. Therefore, this will improve the reliability of the correspondence discovery in the DCRL methods, and achieves more than 70\% DSC on all tasks in FS-Semi. But the reliability in the existing DCRL methods is still weak without our homeomorphism prior, and they all only have lower performance than our GEMINI-Semi. In our experiment (Sec.\ref{sec:task1}) of SS-MIP, the unlabeled images and the labeled images are trained separately in the pretext tasks and downstream tasks, so the networks in the DCRL methods have no supervision during their learning unlabeled images. Their correspondence discovery is extremely limited owing to the properties of medical images, constructing a lot of FP\&N pairs, and learning very poor representation. Therefore, in the downstream tasks, some of them even have worse performance than the ``Scratch". Based on our homeomorphism prior, our GEMINI-MIP has much more reliability to discover the correspondence even without supervision, achieving reliable positive pairs and implicit negative pairs for pre-training and powerful performance for transferring.
\subsubsection{GSS v.s. GVS}
\begin{table}
\centering
\caption{The comparison between our GEMINI-Semi with only GSS and with only GVS on the ``T1: 3D Cardiac structures" demonstrates the advantages of our geometric semantic similarity.}
%\resizebox{\linewidth}{!}{

\setlength{\tabcolsep}{2.5mm}{
\begin{tabular}{lcccccccccccccccc}
\toprule
\multirow{2}{*}{\textbf{Type}}&\textbf{Segmentation}&\multicolumn{2}{c}{\textbf{Deformation}}\\\cmidrule(r){2-2}\cmidrule(r){3-4}
&DSC$_{\pm std}\uparrow$&DSC$_{\pm std}\uparrow$ &$|J_{\phi}|\leq0$$\%\downarrow$  \\
\midrule
\cellcolor[gray]{0.9}Only $\mathcal{L}_{Seg}$&\cellcolor[gray]{0.9}84.3$_{\pm9.6}$&\cellcolor[gray]{0.9}-&\cellcolor[gray]{0.9}-\\
\cellcolor[gray]{0.9}No deformation&\cellcolor[gray]{0.9}-&\cellcolor[gray]{0.9}62.1$_{\pm8.7}$&\cellcolor[gray]{0.9}-\\
\cdashline{1-4}[0.8pt/2pt]
GVS only&89.3$_{\pm4.1}$&51.3$_{\pm10.3}$&40.8$_{\pm1.2}$\\
GSS only&\textbf{90.0$_{\pm3.4}$}&\textbf{84.1$_{\pm13.0}$}&\textbf{10.2$_{\pm2.2}$}\\
\bottomrule
\end{tabular}}

\label{tab:similarity}
\end{table}
The GSS has a very effective measurement ability to guide the learning of deformation (correspondence). As shown in Tab.\ref{tab:similarity}, we perform the learning of correspondence with the GVS or with the GSS, and the losses for the continuity and bijection are removed to avoid their potential influence. The ``GVS only" has 89.3\% DSC on the segmentation which is 5.0\% DSC higher than the ``Only $\mathcal{L}_{Seg}$", demonstrating the improvement from the correspondence learning. However, it only has 51.3\% DSC on the deformation which is 10.8\% lower than the images without deformation. This is because the appearance variation makes the GVS measurement unreliable, so it will train the network to learn the correspondence for those similar-but-different semantic regions (although it is still better than the direct correspondence discovery like \cite{li2021dense,o2020unsupervised,wang2022densecl,wang2022exploring}), finally the deformation will have very large folds (40.8\% if it has no smoothness loss) limiting its performance. The ``GSS only" achieves a significant 32.8\% DSC improvement and 30.6\% $|J_{\phi}|\leq0$ reduction on the deformation compared with the ``GVS only" owing to its measurement of the features which are robustness for the appearance variation. Therefore, better correspondence learning further promotes segmentation learning, achieving 90.0\% DSC which is 0.7\% higher.

\subsubsection{Non-homeomorphic medical images}\label{sec:non}
Our basic hypothesis, homeomorphism prior, limits our GEMINI learning only to be trained between the medical images meeting homeomorphism. However, in the real world, there are a large number of medical images that are unsatisfied with this prior, such as the images with lesions, whose features are unable to be paired only via deformation, hindering the learning of GEMINI. Fortunately, the homeomorphic medical images are easy to collect in real world. The physical examination or disease screening will accumulate a large number of medical images without lesions every year \cite{luo2016big}, providing a potential to collect a big dataset meeting the hypothesis of our framework. This provides data support for our model and hypothesis to have sufficient clinical significance and scope of application.

\subsubsection{Advancements of our preliminary work}\label{sec:adv}
Our preliminary efforts \cite{He_2023_CVPR} first presented the GVSL in 3D medical image SSP, learning the inter-image similarity for powerful representation and efficiently transferring to downstream tasks. This paper extends it (GVSL \cite{He_2023_CVPR}) substantially on self-supervised representation learning with the advancements on principle, method, and application.
\begin{enumerate}[leftmargin=*]
  \item We have proposed the GEMINI learning which is a novel paradigm for the large-scale FP\&N problem in the DCRL with detailed motivation in Sec.\ref{sec:intro}, and proposed a new principle concept, the homeomorphism prior, behind the GVSL in Sec.\ref{sec:hp}.
  \item We have conducted a more comprehensive review of the technological and theoretical research related to our task and provided a clear overview of the field in Sec.\ref{sec:related}.
  \item We have proposed a novel similarity measurement strategy, the GSS, enabling the learned representation in turn to promote the correspondence discovery during the learning process, and promoting learning efficiency for correspondence discovery in Sec.\ref{sec:methodology}.
  \item We have enlarged the application boundary of GVSL advancing the original model that only runs on 3D medical images to any dimension of medical images that satisfies homeomorphism prior.
  \item We have extended our method to more kinds of representation learning tasks, advancing the original model that was only used in pre-training to the variants with both the pre-training and few-shot semi-supervised learning in Sec.\ref{sec:task2} and Sec.\ref{sec:task1}.
  \item We have carried out more experiments for performance analysis and comparison, thus more completely demonstrating the power of our GEMINI learning in Sec.\ref{sec:discussion}.
\end{enumerate}

\subsubsection{Future works}
The future works of the proposed GEMINI and the GVSL are in three aspects:
\begin{enumerate}[leftmargin=*]
  \item As discussed in Sec.\ref{sec:non}, one of our important future works is to expand the learning of correspondence to some images without homeomorphic topology, like the images with lesions \cite{he2021meta}, to cope with the large-scale FP\&N problem in more images types.
  \item Further explore the homeomorphism mapping between images and non-images, like the medical images and the deformed grids for super-pixel segmentation \cite{yang2020superpixel}.
  \item Extend the pre-training to the datasets with multiple image categories, and evaluate the potential of the GVSL as foundation models \cite{liu2024imaging,10750441} for wider scenes.
  \item Design a lighter pre-training process to reduce the computing costs (Sec.C.5 of \emph{Appendix}).
\end{enumerate}


