\section{Methodology}
\label{sec:methodology}
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./picture/framework.pdf}
  \caption{Our GEMINI embedded the homeomorphism prior in medical images achieves a reliable correspondence discovery in DCRL. It has two aspects: a) The DHL (Sec.\ref{sec:dhl}) learns a deformable mapping for soft learning of negative pairs. b) The GSS (Sec.\ref{sec:gss}) fuses semantic similarity into the measurement of correspondence degree to construct the positive pairs reliably. The ``opt" is the optimization. More details are described in Sec.D of our \emph{Supplementary Materials}.}\label{method:framework}
\end{figure}
The proposed GEMINI paradigm (Fig.\ref{method:framework}) constructs soft learning of negative pairs (DHL, Sec.\ref{sec:dhl}) and reliable learning of positive pairs (GSS, Sec.\ref{sec:gss}) based on the homeomorphism prior knowledge for the large-scale FP\&N problem in DCRL for powerful representation.

\subsection{Preliminary of Homeomorphism and Formulation}\label{sec:hp}
In topologie \cite{alexandroff2013topologie}, the ``two objects are homeomorphic" means they have topological equivalence. There is a bijective and continuous function (like Fig.\ref{intro:1} A)) between them for a topological-preserved transformation (homeomorphism), and each point of one object corresponds to a point of the other object. Let's define two objects in topological space, $X$ and $Y$, where the $X$ and $Y$ are the point sets of the objects (e.g., the semantic regions in images). There is a mapping function $F:X\rightarrow Y$ that transforms the $X$ to the $Y$. If the $F$ is a \emph{bijection}, is \emph{continuous}, and its \emph{inverse function} $F^{-1}$ is also \emph{continuous}, this mapping function $F$ is a homeomorphism, the two objects are homeomorphic $X\cong Y$. Based on the above theory, the \emph{bijection} and \emph{continuity} are two key elements in constructing a homeomorphic mapping for our reliable correspondence discovery.

\textbf{Formulation:} Due to the consistency of human anatomy and the intrinsic topology property of images \cite{kong1989digital}, numerous categories of medical images scanned from the same human body region are approximately homeomorphic in topologie. Let's denote two same category medical images $x^{A}$ and $x^{B}$ sampled from the dataset $\mathcal{S}$ are homeomorphic $x^{A}\cong x^{B}$. The individual variations of human body, such as height, figure, and scanning posture, make the semantic regions' position in image space different between medical images. Therefore, this enables us to construct a homeomorphism mapping function $\psi_{\mathbb{R}^{n}}$ that transforms the pixel positions in the image space $\mathbb{R}^{n}$ with $n$ dimensions for the alignment of images. It is formulated as:
\begin{equation}\label{equ:trans}
\begin{aligned}
&\psi_{\mathbb{R}^{n}}(x^{A})=x^{B}\\
&\begin{array}{ll}
s.t. & \forall I\in x^{A},\psi_{\mathbb{R}^{n}}(I)\in x^{B}\\
     & \forall J\in x^{B},\psi^{-1}_{\mathbb{R}^{n}}(J)\in x^{A},
\end{array}
\end{aligned}
\end{equation}
where the $I$ and $J$ are subsets of $x^{A}$ and $x^{B}$. The $\forall I\in x^{A},\psi_{\mathbb{R}^{n}}(I)\in x^{B}$ means that the mapping $\psi_{\mathbb{R}^{n}}$ is topological-preserved (continuous), and the $\forall J\in x^{B},\psi^{-1}_{\mathbb{R}^{n}}(J)\in x^{A}$ further means that the $\psi_{\mathbb{R}^{n}}$ is bijective and there is a continuous inverse mapping $\psi^{-1}_{\mathbb{R}^{n}}$.

\textbf{Simple Discussion:} The hypothesis of the homeomorphism prior in medical images is based on an ideal situation, i.e., the images have the exactly same content (one-by-one correspondence of the semantic regions). However, due to the potential difference in the scanning fields and the human body, medical images do not always have the same content. For example, the cardiac CT images in a large scanning window will have whole lungs, but if they are in a small scanning window, the images will only have a small part of the lungs. This makes only the shared parts homeomorphic and the whole images are not. Fortunately, in our method, we construct this homeomorphism mapping function by \emph{deformation} and use 0 value to fill the hole caused by deformation. This means that we append a point set of all zeros on the original image point set so that the points between the medical images that do not meet homeomorphism will correspond to these zero points, thus finally making the whole point set meet the homeomorphism. It is formulated as $\{x^{A},\phi^{0}\}\cong\{x^{B},\phi^{0}\}$, where the $\phi^{0}$ is the zero set has no gradient in training. Therefore, it enables us only need to focus on the homeomorphic part between medical images.

\subsection{Deformable Homeomorphism Learning (DHL)}
\label{sec:dhl}
As shown in Fig.\ref{method:framework} a), the DHL leans a deformable mapping that transforms the space of one medical image to the other image, thus driving the soft learning of feature pairs. It has two shared-weight networks which learn representation (backbone) $\mathcal{N}_{\theta}$ and a network that learns deformation (deformer) $\mathcal{D}_{\xi}$. Two medical images sampled from the dataset $\{x^{A},x^{B}\}\sim\mathcal{S}$ are putted into the backbone networks to extract their features $f^{A}=\mathcal{N}_{\theta}(x^{A}),f^{B}=\mathcal{N}_{\theta}(x^{B})$. These features are put into the deformer networks $\mathcal{D}_{\xi}$ in order ($[f^{A},f^{B}]$) and reverse order ($[f^{B},f^{A}]$) to estimate their deformable mapping functions, i.e., DVF $\psi_{\mathbb{R}^{n}}^{AB}$ from $x^{A}$ to $x^{B}$, and DVF $\psi_{\mathbb{R}^{n}}^{BA}$ from $x^{B}$ to $x^{A}$. It is formulated as
\begin{equation}\label{equ:DHL}
\psi^{AB}_{\mathbb{R}^{n}}=\mathcal{D}_{\xi}(f^{A},f^{B}),\psi^{BA}_{\mathbb{R}^{n}}=\mathcal{D}_{\xi}(f^{B},f^{A}).
\end{equation}

According to the homeomorphism prior (Equ.\ref{equ:trans}) which constructs a continuous and bijective correspondence in DVF, we train the network to learn these properties: \textbf{1)} \emph{For correspondence}, we propose a geometric semantic similarity $\mathcal{L}_{GSS}$ for efficient measurement of the correspondence degree and drive the deformer to predict reliable DVF together with the original geometric visual similarity $\mathcal{L}_{GVS}$ in GVSL \cite{He_2023_CVPR} (Sec.\ref{sec:gss}). \textbf{2)} \emph{For continuity}, we utilize a smooth loss $\mathcal{L}_{smo}$ to constrain the DVF $\psi_{\mathbb{R}^{n}}$ to perform a topological-preserved (smooth) transformation, i.e., the deformable mapping, so the semantics of the regions inner medical images will be preserved, improving the reliability of correspondence:
\begin{equation}\label{equ:smo}
   \mathcal{L}_{smo}(\psi_{\mathbb{R}^{n}})=\sum_{p\in\mathbb{R}^{n}}\|\triangledown\psi_{p}\|^{2},
\end{equation}
where the $p$ is the position of the pixels in image space $\mathbb{R}^{n}$, the $\triangledown\psi_{p}$ is the gradient of position $p$. Therefore, it will avoid over-transformation which breaks the topological structures of semantic regions, and keep the homeomorphic property of the mapping. Therefore, the loss $\mathcal{L}_{DM}$ to learn a deformable mapping (DM) is
\begin{align}\label{equ:DHLloss}
&\mathcal{L}_{DM}(\theta,\xi,\{x^{A},x^{B}\})=\underbrace{\lambda_{smo}\mathcal{L}_{smo}(\psi^{AB}_{\mathbb{R}^{n}})}_{\textbf{\text{Continuity}}}\\
&+\underbrace{\lambda_{GVS}\mathcal{L}_{GVS}(x^{A},x^{B},\psi^{AB}_{\mathbb{R}^{n}})+\mathcal{L}_{GSS}(f^{A},f^{B},\psi^{AB}_{\mathbb{R}^{n}})}_{\textbf{\text{Correspondence}}},\notag
\end{align}
where the $\lambda_{smo}$ is the weights for the smooth loss and $\lambda_{GVS}$ is the weight of the geometric visual similarity. \textbf{3)} \emph{For bijection}, we simultaneously learn forward deformable mapping and inverse deformable mapping, thus training the deformers to learn the symmetry between two medical images and constructing a bidirectional optimization objective in our DHL $\mathcal{L}_{DHL}$:
\begin{equation}\label{equ:DHLloss2}
\mathcal{L}_{DHL}=\underbrace{\mathcal{L}_{DM}(\theta,\xi,\{x^{A},x^{B}\})+\mathcal{L}_{DM}(\theta,\xi,\{x^{B},x^{A}\}))}_{\textbf{\text{Bijectivity:}}\ x^A\rightarrow x^B,\ x^B\rightarrow x^A}
\end{equation}
Therefore, during the learning of bijective deformable mapping, the deformers will try to learn a reliable correspondence between images in the DVFs ($\psi^{AB}_{\mathbb{R}^{n}},\psi^{BA}_{\mathbb{R}^{n}}$). The gradient of the loss $\mathcal{L}_{DHL}$ will optimize the backbones $\mathcal{N}_{\theta}$ for soft learning of feature pairs implicitly, bringing a reliable optimization for representation (analyzed in Sec.\ref{sec:int}).

\textbf{Simple discussion of the property:} Our DHL serves as the framework of GEMINI, enabling homeomorphism mapping between images. By leveraging the key elements of continuity and bijection, it trains the backbone to extract distinct features for non-corresponding pairs and consistent features for corresponding pairs through deformable learning gradients. This soft feature learning mitigates false negatives caused by direct negative pair distinctions. However, the backbone's weak initial representation, stemming from the 'two-player game' learning process (Sec.\ref{sec:int}), is addressed by embedding foundational tasks in GEMINI variants to warm up learning (analyzed in Sec.\ref{subsubsec:analysisiofrestor}).

\subsection{Geometric Semantic Similarities (GSS)}
\label{sec:gss}
Our GSS measures the correspondence (alignment) degree via the represented dense features to promote the learning efficiency of deformation to improve the alignment, thus constructing and learning the positive pairs reliably. As shown in Fig.\ref{method:framework} a), the original GVS \cite{He_2023_CVPR} utilizes the DVF $\psi^{AB}_{\mathbb{R}^{n}}$ to transform the image $x^{A}$ to align the image $x^{B}$, and calculate the distance of pixel intensity between the aligned image $x^{AB}$ and the image $x^{B}$ for their similarity. Due to the limitation of the appearance, the distance will be interfered which makes an unreliable measurement. Our GSS measure the similarity between the represented features (Fig.\ref{method:framework} b)) further considering the important semantic information. It utilizes the DVF $\psi^{AB}_{\mathbb{R}^{n}}$ to align the dense features $f^{A}$ to the dense features $f^{B}$, and calculate the distance of the features between the aligned features $f^{AB}$ and the feature $f^{B}$ for their similarity. In our GEMINI, the original GVS still takes part in the measurement due to its training-invariability which will not be interrupted by the learning process for a basic optimization objective. Different from the similarity measured throughout the whole space \cite{wang2022densecl}, our GSS or GVS measures the similarities only between the corresponding positions of the features or images in the image grid. Therefore, this will make the measurement under the condition of semantic regions' inherent topology which significantly reduces the searching space of the correspondence discovery, so we call the GSS and GVS ``geometric".

As discussed in Sec.\ref{sec:hp}, during the calculation of the geometric similarities, we only focus on the homeomorphic regions. Therefore, we utilize an adaptive mask mechanism to remove the zero point set and measure the alignment degree of the potentially shared regions between two images. Specifically, it generates a mask $\epsilon$ with the same size as the image $x^{A}$ and the value of $1$, and then transforms it to the space of $x^{B}$ via the DVF $\psi^{AB}_{\mathbb{R}^{n}}$ for $\epsilon^{AB}$. The void region caused by the transformation in the mask is filled with ``$0$" which is the appended zero point set. Therefore, the regions with a value of ``$1$" are the shared regions between images, and those with a value of ``$0$" are the non-corresponding regions. Only the value ``$1$" regions highlighted in the mask will be calculated for similarity. We utilize the normalized cross-correlation \cite{he2021few} for the GVS and the cosine similarity for our GSS with the mask $\epsilon$ into the measurement process:
\begin{align}\label{equ:GSSGVS}
  &\mathcal{L}^{\epsilon}_{GVS}=\sum_{p\in\{\epsilon^{AB}=1\}}\frac{(\sum_{p_{i}}(x^{AB}_{p_{i}}-\hat{x}^{AB}_{p})(x^{B}_{p_{i}}-\hat{x}^{B}_{p}))^{2}}{(\sum_{p_{i}}(x^{AB}_{p_{i}}-\hat{x}^{AB}_{p}))(\sum_{p_{i}}(x^{B}_{p_{i}}-\hat{x}^{B}_{p}))}, \\
  &\mathcal{L}^{\epsilon}_{GSS}=\sum_{p\in\{\epsilon^{AB}=1\}}\frac{f^{AB}_{p}\cdot f^{B}_{p}}{\|f^{AB}_{p}\|\|f^{B}_{p}\|},
\end{align}
where the $\epsilon^{AB}=\psi^{AB}_{\mathbb{R}^{n}}(\epsilon)$, the $x^{AB}=\psi^{AB}_{\mathbb{R}^{n}}(x^{A})$, and the $f^{AB}=\psi^{AB}_{\mathbb{R}^{n}}(f^{A})$.
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./picture/gradient.pdf}
  \caption{The gradients from the loss of our GSS simultaneously train the explicit contrast of positive pairs and drive the implicit and soft learning in our DHL.}
  \label{method:grad}
\end{figure}

Our GSS will both drive the learning of homeomorphism mapping as a loss in the DHL and train the reliable learning of positive pairs for their representation due to the homeomorphism-based correspondence. As shown in Fig.\ref{method:grad}, the gradient from the GSS will be divided into two parts, including the gradient for positive pairs (red path) and the gradient for the soft learning of feature (both positive and negative) pairs (blue path). The rad path directly learns the consistency of features in corresponding positions predicted by the deformer $\mathcal{D}_{\xi}$. Due to the homeomorphism prior, the correspondence discovery is under the condition of topological preservation which will be reliable for the contrastive learning of positive pairs. The blue path utilizes the gradient form the deformer $\mathcal{D}_{\xi}$ to optimize the backbone network $\mathcal{N}_{\theta}$ indirectly, i.e., the learning in our DHL, bringing implicit and soft learning of feature pairs (analyzed in Sec.\ref{sec:int}).

The whole optimization of our GEMINI can be modeled as two parts, including the optimization of parameters $\theta$ for representation and the optimization of the parameters $\xi$ for deformable mapping:
\begin{align}\label{equ:opt}
&\xi^{*}=\underset{\xi}{\arg\min}\ \mathcal{L}_{DHL}(\theta^{*},\xi,\mathcal{S})\\
s.t.\ & \theta^{*}=\underbrace{\underset{\theta}{\text{OPT}}(\theta,\frac{\partial\mathcal{L}_{DHL}(\theta,\xi,\mathcal{S})}{\partial \theta})}_{\text{Gradient for soft negative pairs}}+\underbrace{\lambda_{pos}\underset{\theta}{\arg\min}\ \mathcal{L}_{pos}(\theta,\mathcal{S})}_{\text{Objective for positive pairs}},\notag
\end{align}
where the $\mathcal{L}_{pos}$ represents the red path in Fig.\ref{method:grad} for positive pairs, $\lambda_{pos}$ is the weight of $\mathcal{L}_{pos}$, and the OPT$()$ is the optimization strategy to minimize the $\mathcal{L}_{DHL}$ for negative pairs (yellow path). If it is SGD \cite{bottou2010large} with one step, it will be $\theta^{*}\leftarrow\theta-\eta\frac{\partial\mathcal{L}_{DHL}(\theta,\xi,\mathcal{S})}{\partial \theta}$, where the $\eta$ is learning rate. The two optimization parts correspond to the two forward paths (Fig.\ref{method:grad}) in the inference process, so the whole optimization process is compatible with the existing gradient descent methods. We utilize the Adam \cite{kingma2014adam} for optimization.

\textbf{Simple discussion of the property:} Our GSS, a specific loss in GEMINI, facilitates correspondence learning—key to homeomorphism mapping—by leveraging the deformer to align features based on the inherent topology of medical images. This enables reliable positive pair learning, addressing the large-scale false-positive problem. As part of DHL, GSS also supports soft feature learning via gradients. To overcome limitations in discovering positive pairs in non-homeomorphic regions, our adaptive mask mechanism highlights shared regions.

%Our GSS is one of the specific losses in our GEMINI which drives the learning of correspondence (one important aspect in homeomorphism mapping). It discovers the correspondence of features under the condition of medical images' inherent topology from the deformer, constructing a direct but reliable learning of positive feature pairs for the large-scale FP problem. Moreover, as one of the losses in the DHL, it also contributes to the soft learning of feature pairs via gradient in the DHL. The non-homeomorphic regions between images will limit the correspondence discovery of positive pairs, and our adaptive mask mechanism indicates the shared part.

\subsection{Intuitions on Behavior: Learning Reliable Positive and Implicit Negative Pairs for Dense Representation}\label{sec:int}
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./picture/intuition.pdf}
  \caption{Intuitions on behavior. a) The two optimization objectives in Equ.\ref{equ:opt} for $\theta$ drive the reliable learning of positive and implicit learning of negative pairs. b) The feature pairs are learned softly via the gradient from the DHL.}\label{method:int}
\end{figure}
The two optimization objectives in Equ.\ref{equ:opt} for $\theta$ train the backbones to learn reliable positive feature pairs and implicit negative feature pairs, bringing an effective dense representation learning to the continuous image signal.

\textbf{For positive pairs}, it effectively reduces the searching space of pairing, bringing reliable learning for positive pairs. Our homeomorphism prior makes the correspondence discovery under the condition of the consistent topology of images formulated as $c_{i}=\mathcal{D}_{\xi}(f^{A},f^{B})_{i}={\psi^{AB}_{\mathbb{R}^{n}}}_{i}$. In the training of GEMINI, the $\psi^{AB}_{\mathbb{R}^{n}}$ have to meet the minimization of $\mathcal{L}_{smo}$, $\mathcal{L}_{GSS}$, and $\mathcal{L}_{GVS}$ which make the $\psi^{AB}_{\mathbb{R}^{n}}$ have topology-preservation ability and good alignment degree. Therefore, as shown in Fig.\ref{method:int} a), it makes our positive feature pairing consider the topology, i.e., pairing features on a topology manifold, efficiently reducing the searching space and improving the reliability. The objective for positive pairs $\arg\min_{\theta}\ \mathcal{L}_{pos}(\theta,\mathcal{S})$ learns their consistency on this manifold. The feature $f^{A}_{q}$ ($q=7$ in Fig.\ref{method:int}) from the dense features $f^{A}$ and the feature $f^{B}_{*}$ from the dense features $f^{B}$ will be consistent, i.e., $f^{A}_{q}=f^{B}_{*}$.
%The existing whole-space similarity measurement \cite{wang2022densecl} calculates the similarity between all dense features being unreliable due to the semantic dependence in medical images. It is formulated as $c_{i}=\arg\max_{j} sim(f^{A}_{i},f^{B}_{j})$, where the $c_{i}$ is the correspondence index in $i$ position.

\textbf{For negative pairs}, it constructs implicit learning via the gradient $\text{OPT}_{\theta}(\theta,\frac{\partial\mathcal{L}_{DHL}(\theta,\xi,\mathcal{S})}{\partial \theta})$ from the DHL, avoiding the direct division of negative pairs and learning a soft contrast. Specifically, the deformer network $\mathcal{D}_{\xi}$ learns to discover the correspondence of the consistent features $\{f^{A},f^{B}\}$, thus driving the backbone represent distinct features for the features with inconsistent semantics. Fig.\ref{method:int} b) shows an example of this process. The features $f^{A}_{r^{2}}=\{f^{A}_{0},f^{A}_{1},f^{A}_{2},...,f^{A}_{r\times r}\}$ is the image $x^{A}$'s features in the receptive field of the Deformer network $\mathcal{D}_{\xi}$, where the $r$ is the width of the receptive field (it is 26 in our experiments). Due to the learning of positive pairs, the $f^{B}_{*}$ is constrained to $f^{A}_{q}$ where the $q$ is the corresponding position. Therefore, as demonstrated in Fig.\ref{method:int} a), in order to discover the corresponding position, it will train the features in non-corresponding position to distinct to the feature $f^{B}_{*}$ via a gradient.
\begin{equation}\label{equ:nega}
\begin{aligned}
&\mathcal{D}_{\xi}(f^{B}_{*},f^{A}_{r^2})=\psi^{BA}_{*},\\
&\begin{array}{lll}
s.t. & f^{B}_{*}=f^{A}_{q},&\psi^{BA}_{*}\ \text{is}\ *\rightarrow q
\end{array}
\end{aligned}
\end{equation}
Throughout the whole training process, the learning of representation in the backbone network $\mathcal{N}_{\theta}$ and the deformable mapping in the deformer network $\mathcal{D}_{\xi}$ is a two-player game \cite{saad2009coalitional}. The $\mathcal{D}_{\xi}$ learns to estimate the correspondence of semantic regions from the represented dense features $f^{A},f^{B}$ and measure their pixel displacement. The $\mathcal{N}_{\theta}$ learns to provide features of semantic regions to the $\mathcal{D}_{\xi}$ for their correspondence. To achieve more accurate correspondence, the deformer network $\mathcal{D}_{\xi}$ has to drive the backbone network $\mathcal{N}_{\theta}$ to output more distinct features in turn for different semantic regions via the gradient in backpropagation. Therefore, under this interaction, the $\mathcal{N}_{\theta}$ will provide more representative features for the $\mathcal{D}_{\xi}$ to improve the correspondence estimation, and the $\mathcal{D}_{\xi}$ will have a more powerful ability to learn the correspondence of pixel-wise features. This process needs a promising representation in backbone, but it is always unavailable at the beginning of training. Therefore, we train the GEMINI with a fundamental learning task, e.g., restoration \cite{zhou2019models,pathak2016context} in our implementation. 