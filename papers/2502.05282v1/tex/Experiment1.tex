\section{Experiment 2: Self-supervised Medical Image Pre-training (SS-MIP)}
\label{sec:task1}
We further implement our GEMINI learning on self-supervised medical image pre-training (SS-MIP) task (GEMINI-MIP), providing powerful tools to transfer potential tasks and giving complete experiments. Four public-available datasets are enrolled in our experiments for very effective evaluations.

\begin{table*}
\centering
\caption{The fine-tuning evaluations demonstrate our great transferring ability on SS-MIP tasks. Our GEMINI-MIP achieves the best performance compared with 18 methods on 3 downstream tasks. ``T1", ``T2", and ``T3" are the task 1, task 2, and task 3. ``AVG" is the average value of the row. The cells with gray backgrounds are the inner-scene (same image category) transferring and the others are the inter-scene (different image category) transferring.}
%\resizebox{\textwidth}{!}{
\begin{tabular}{clcccccccccc}
\toprule
\multirow{2}{*}{\textbf{Type}}
&\multirow{2}{*}{\textbf{Pre-training}}
&\multicolumn{2}{c}{\textbf{T1: SCR$_{25}$} \emph{Inner-scene}}
&\multicolumn{2}{c}{\textbf{T2: KiPA22} \emph{Inter-scene}}
&\multicolumn{2}{c}{\textbf{T3: CANDI} \emph{Inter-scene}}
&\textbf{AVG}
\\
\cmidrule(r){3-4}\cmidrule(r){5-6}\cmidrule(r){7-8}\cmidrule(r){9-9}
&
&DSC$_{\pm std}\uparrow$
&AVD$_{\pm std}\downarrow$
&DSC$_{\pm std}\uparrow$
&AVD$_{\pm std}\downarrow$
&DSC$_{\pm std}\uparrow$
&AVD$_{\pm std}\downarrow$
&DSC$_{\pm std}\uparrow$
\\
\midrule
-
&Scratch (2D U-Net)
&81.8$_{\pm8.2}$
&9.00$_{\pm6.37}$
&74.1$_{\pm12.3}$
&3.59$_{\pm1.97}$
&65.0$_{\pm4.4}$
&1.27$_{\pm0.21}$
&73.6$_{\pm8.3}$
\\
\cdashline{1-9}[0.8pt/2pt]
%\hline
Sup
&ImageNet \cite{deng2009imagenet}
&\color{blue}92.0$_{\pm3.1}$
&\color{red}\textbf{4.09$_{\pm1.64}$}
&72.6$_{\pm15.5}$
&4.78$_{\pm4.86}$
&71.1$_{\pm19.8}$
&1.35$_{\pm2.07}$
&78.6$_{\pm12.8}$
\\
%\hline
GRL
&Denosing \cite{vincent2010stacked}
&\cellcolor[gray]{0.9}83.9$_{\pm7.8}$
&\cellcolor[gray]{0.9}11.17$_{\pm7.81}$
&60.3$_{\pm17.7}$
&7.55$_{\pm5.18}$
&67.7$_{\pm2.1}$
&1.21$_{\pm0.08}$
&70.6$_{\pm9.2}$
\\
&In-painting \cite{pathak2016context}
&\cellcolor[gray]{0.9}85.1$_{\pm6.6}$
&\cellcolor[gray]{0.9}16.59$_{\pm10.74}$
&64.4$_{\pm16.4}$
&5.79$_{\pm4.13}$
&66.2$_{\pm2.3}$
&1.26$_{\pm0.08}$
&71.9$_{\pm8.4}$
\\
&Models Genesis \cite{zhou2019models}
&\cellcolor[gray]{0.9}86.1$_{\pm4.6}$
&\cellcolor[gray]{0.9}6.22$_{\pm2.27}$
&66.6$_{\pm16.3}$
&5.86$_{\pm3.14}$
&88.1$_{\pm3.1}$
&0.32$_{\pm0.10}$
&80.3$_{\pm8.0}$
\\
&Rotation \cite{komodakis2018unsupervised}
&\cellcolor[gray]{0.9}80.5$_{\pm7.7}$
&\cellcolor[gray]{0.9}20.62$_{\pm12.55}$
&69.7$_{\pm15.3}$
&6.45$_{\pm4.33}$
&78.3$_{\pm2.6}$
&0.75$_{\pm0.09}$
&76.2$_{\pm8.5}$
\\
%\hline
CRL
&SimSiam \cite{Chen2021CVPR}
&\cellcolor[gray]{0.9}87.2$_{\pm5.1}$
&\cellcolor[gray]{0.9}11.87$_{\pm8.10}$
&72.6$_{\pm13.3}$
&4.10$_{\pm3.25}$
&76.7$_{\pm2.1}$
&0.82$_{\pm0.06}$
&78.8$_{\pm6.8}$
\\
&BYOL \cite{grill2020bootstrap}
&\cellcolor[gray]{0.9}89.4$_{\pm4.9}$
&\cellcolor[gray]{0.9}8.48$_{\pm4.37}$
&74.1$_{\pm12.6}$
&3.87$_{\pm2.93}$
&70.5$_{\pm2.1}$
&1.08$_{\pm0.07}$
&78.0$_{\pm6.5}$
\\
&SimCLR \cite{chen2020simple}
&\cellcolor[gray]{0.9}89.0$_{\pm4.0}$
&\cellcolor[gray]{0.9}11.28$_{\pm6.53}$
&74.4$_{\pm11.3}$
&3.68$_{\pm2.65}$
&79.0$_{\pm2.6}$
&1.02$_{\pm0.40}$
&80.8$_{\pm6.0}$
\\
&MoCov2 \cite{chen2020improved}
&\cellcolor[gray]{0.9}84.3$_{\pm6.5}$
&\cellcolor[gray]{0.9}11.06$_{\pm5.19}$
&69.6$_{\pm14.4}$
&6.28$_{\pm4.70}$
&82.9$_{\pm3.6}$
&0.52$_{\pm0.12}$
&78.9$_{\pm8.2}$
\\
&DeepCluster \cite{caron2018deep}
&\cellcolor[gray]{0.9}84.0$_{\pm8.1}$
&\cellcolor[gray]{0.9}19.71$_{\pm13.15}$
&72.7$_{\pm15.1}$
&4.91$_{\pm3.50}$
&60.0$_{\pm2.2}$
&1.52$_{\pm0.07}$
&72.2$_{\pm8.5}$
\\
%\hline
DCRL
&VADeR \cite{o2020unsupervised}
&\cellcolor[gray]{0.9}85.2$_{\pm5.1}$
&\cellcolor[gray]{0.9}7.15$_{\pm3.05}$
&62.8$_{\pm15.6}$
&7.23$_{\pm4.73}$
&86.1$_{\pm3.4}$
&0.40$_{\pm0.12}$
&78.0$_{\pm8.0}$
\\
&DenseCL \cite{wang2022densecl}
&\cellcolor[gray]{0.9}85.0$_{\pm6.3}$
&\cellcolor[gray]{0.9}11.74$_{\pm7.02}$
&70.8$_{\pm14.8}$
&5.48$_{\pm3.95}$
&76.8$_{\pm2.9}$
&1.22$_{\pm0.68}$
&77.5$_{\pm8.0}$
\\
&SetSim \cite{wang2022exploring}
&\cellcolor[gray]{0.9}85.2$_{\pm5.1}$
&\cellcolor[gray]{0.9}9.63$_{\pm7.64}$
&70.8$_{\pm14.4}$
&4.92$_{\pm3.26}$
&74.9$_{\pm2.5}$
&0.89$_{\pm0.08}$
&77.0$_{\pm7.3}$
\\
&DSC-PM \cite{li2021dense}
&\cellcolor[gray]{0.9}90.5$_{\pm3.5}$
&\cellcolor[gray]{0.9}5.44$_{\pm2.87}$
&77.2$_{\pm12.2}$
&3.87$_{\pm3.34}$
&83.3$_{\pm2.4}$
&0.74$_{\pm0.64}$
&83.7$_{\pm6.0}$
\\
&PixPro \cite{xie2021propagate}
&\cellcolor[gray]{0.9}91.5$_{\pm3.3}$
&\cellcolor[gray]{0.9}9.83$_{\pm5.34}$
&73.6$_{\pm12.9}$
&4.00$_{\pm3.33}$
&63.9$_{\pm2.0}$
&1.35$_{\pm0.06}$
&76.3$_{\pm6.1}$
\\
&GLCL \cite{chaitanya2020contrastive}
&\cellcolor[gray]{0.9}87.3$_{\pm5.8}$
&\cellcolor[gray]{0.9}9.35$_{\pm4.68}$
&76.5$_{\pm11.9}$
&4.33$_{\pm2.94}$
&82.8$_{\pm2.6}$
&0.56$_{\pm0.09}$
&82.2$_{\pm6.8}$
\\
%\hline
\cdashline{1-9}[0.8pt/2pt]
\textbf{DCRL}
&\textbf{GVSL-MIP (CVPR)}\cite{He_2023_CVPR}
&\cellcolor[gray]{0.9}89.7$_{\pm3.7}$
&\cellcolor[gray]{0.9}10.52$_{\pm7.23}$
&\color{blue}78.9$_{\pm11.2}$
&\color{red}\textbf{2.95$_{\pm1.55}$}
&\color{blue}89.7$_{\pm2.6}$
&\color{blue}0.27$_{\pm0.08}$
&\color{blue}86.1$_{\pm5.8}$
\\
\textbf{(Ours)}
&\textbf{GEMINI-MIP}
&\color{red}\cellcolor[gray]{0.9}\textbf{92.1$_{\pm2.8}$}
&\cellcolor[gray]{0.9}\color{blue}5.38$_{\pm2.65}$
&\color{red}\textbf{79.1$_{\pm11.1}$}
&\color{blue}3.22$_{\pm2.24}$
&\color{red}\textbf{89.8$_{\pm2.6}$}
&\color{blue}0.27$_{\pm0.08}$
&\color{red}\textbf{87.0$_{\pm5.5}$}
\\
\bottomrule
\end{tabular}
%}
\label{tab:metrics1}
\end{table*}

\subsection{Experiments configurations}
\label{sec:configurations1}
\subsubsection{Variant design}
Our GEMINI-MIP task is the pretext task to pre-train the representation of the backbone network $\mathcal{N}_{\theta}$ and then the pre-trained network is transferred to the downstream tasks $\mathcal{L}_{DS}$. It also learns a self-restoration head $Res_{\tau}$ (fundamental task) on the dense features $f^{A},f^{B}$ due to the initial weak representation in the pretext task for a warm-up of our GSS. The self-restoration is based on the prior of edges and shapes in images and trains the network to capture the features from the broken distribution. Therefore, the variant framework for SS-MIP has an additional optimization for restoration $\mathcal{L}_{Res}$:
\begin{align}\label{equ:variant1}
&\textbf{\text{Pretext:}}\ \underset{\xi,\theta,\tau}{\arg\min}\ (\mathcal{L}_{DHL}(\theta,\xi,\mathcal{S}_{ul})+\mathcal{L}_{Res}(\theta,\tau,\mathcal{S}^{*}_{ul}),\notag\\
&\textbf{\text{Downstream:}}\ \underset{\kappa}{\arg\min}\ \mathcal{L}_{DS}(\theta,\kappa,\mathcal{S}_{l}),
\end{align}
where the $\mathcal{S}_{ul}$ is the unlabeled dataset, and the $\mathcal{S}^{*}_{ul}$ is the unlabeled dataset with the appearance transformation $\mathcal{T}(\mathcal{S}_{ul})=\mathcal{S}^{*}_{ul}$ for self-restoration, the $\mathcal{S}_{l}$ is the labeled dataset in the downstream task, $\mathcal{L}_{DS}$ is the loss for the downstream task, and the $\kappa$ is the parameters in the learning head of the downstream task. In our experiment, we utilize the mean square error as the loss for the self-restoration following \cite{zhou2019models}, $\mathcal{L}_{Res}(x,\mathcal{T}(x))=|x-Res_{\tau}(\mathcal{N}_{\theta}(\mathcal{T}(x)))|^{2}$, to train self-restoration objective. We utilize the random in-painting, local-shuffling, and non-linear transformation in the $\mathcal{T}$ to transform the unlabeled images.

\subsubsection{Datasets} We evaluate the representation learning ability of our GEMINI-MIP on four datasets with three downstream tasks to demonstrate the properties and advantages of our method in different aspects.

\textbf{Pretext datasets:} We utilize the \emph{ChestX-ray8} \cite{wang2017chestx} which has 112,120 frontal-view chest X-ray images with $1024\times1024$ resolution and 0 to 255 grayscale values. 44,810 of them are scanned from the anterior to posterior (AP) view and 67,310 of them are scanned from the posterior to anterior (PA) view. 63,340 of them are male and 48,780 of them are female. In our experiment, we resize the images into $512\times512$ and normalize them to [0, 1]. For a better homeomorphic property, we randomly pair these chest X-ray images with the same perspective (PA/AP) and gender (male/female).

\textbf{Downstream datasets:} Three publicly available datasets (SCR \cite{van2006segmentation}, KiPA22 \cite{he2021meta}, CANDI \cite{kennedy2012candishare}) are used to demonstrate the superiorities of our framework. \textbf{Task 1: SCR} dataset \cite{van2006segmentation} segments 3 chest-related structures on 247 X-ray images. We set 47 of them as the validation set, 100 of them as the training set, and 100 of them as the testing set. We utilize 25\% of the training set in this experiment (SCR$_{25}$) to build a limited data situation and more data amount evaluations are performed in our analysis (Sec.\ref{sec:discussion}). \textbf{Task 2: KiPA22} dataset \cite{he2021meta} segments four renal cancer-related structures on 130 3D CT images. We set 30 of them as the validation set, 70 of them as the training set, and 30 of them as the testing set. \textbf{Task 3: CANDI} dataset \cite{kennedy2012candishare} segments 28 brain tissues on 103 3D T1 MR images. We set 20 of them as the validation set, 40 of them as the training set, and 43 of them as the testing set. For the 3D datasets used in the 2D task, we train the networks on the 2D slices of the images, predict segmentation results at each 2D slice and evaluate the results for 3D volumes.

\subsubsection{Comparison setting} We benchmark GEMINI against 17 state-of-the-art or widely-used methods across four categories and compare with our previous GVSL to highlight advancements. \textbf{1)} A 2D supervised network pre-trained on ImageNet \cite{deng2009imagenet} evaluates the representation ability of supervised learning. \textbf{2)} Generative representation learning (GRL) methods (Denoising \cite{vincent2010stacked}, In-painting \cite{pathak2016context}, Models Genesis \cite{zhou2019models}, Rotation \cite{komodakis2018unsupervised}) provide baseline performance for classic approaches. \textbf{3)} Contrastive representation learning (CRL) methods (SimSiam \cite{Chen2021CVPR}, BYOL \cite{grill2020bootstrap}, SimCLR \cite{chen2020simple}, MoCov2 \cite{chen2020improved}, DeepCluster \cite{caron2018deep}) reveal limitations of global contrastive representation on MIDP tasks. \textbf{4)} Dense contrastive representation learning (DCRL) methods (VADeR \cite{o2020unsupervised}, GLCL \cite{chaitanya2020contrastive}, DSC-PM \cite{li2021dense}, PixPro \cite{xie2021propagate}, DenseCL \cite{wang2022densecl}, SetSim \cite{wang2022exploring}) highlight GEMINI's superior performance through reliable positive and negative pair learning. For downstream tasks, pre-trained 2D CNN feature extractors (backbone $\mathcal{N}_{\theta}$) are used in a 2D U-Net \cite{ronneberger2015u} (introduced in Sec.\ref{sec:task2}), trained with a combined Dice and cross-entropy loss ($\mathcal{L}_{DS}$).

\subsubsection{Implementation and evaluation metrics} All tasks are implemented by PyTorch \cite{paszke2019pytorch} on NVIDIA GeForce RTX 3090 GPUs with 24 GB memory, optimized by Adam \cite{kingma2014adam} whose learning rate is $10^{-4}$. The pretext task is trained with $2\times10^{5}$ iterations. We utilize the fine-tuning evaluation on the downstream tasks. The downstream tasks are trained with $4\times10^{4}$ iterations and validated every 200 iterations to save the best models on their validation sets. For a fair comparison, all methods in our experiment take the same basic training setting. Ten 2D X-ray images are randomly sampled in each iteration for stable pre-training, and five 2D images are randomly sampled in each iteration of downstream transferring. We use the same metrics as our experiment 1 (Sec.\ref{sec:task2}) for the evaluation of the performance.

\subsection{Results and Analysis}
\label{sec:results1}
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./picture/results1.pdf}
  \caption{Our GEMINI-MIP also has very significant visual superiority in the three downstream tasks.}\label{Fig:results1}
\end{figure}

\subsubsection{Quantitative evaluation shows metric superiority}
As shown in Tab.\ref{tab:metrics1}, the fine-tuning evaluation demonstrates the great transferring ability of our GEMINI-MIP due to the reliable positive and negative pairs discovery promoted by our homeomorphism prior. We can find two interesting observations in Tab.\ref{tab:metrics1}: \textbf{1)} The pre-training will bring better performance than random initialization (``Scratch") to most of the networks. This is because the learned representation from the pretext task with large-scale data will stimulate the network to learn diverse low-level patterns, although the FN or FP problem will interrupt the representation learning of high-level semantics, the diverse low-level patterns' knowledge will promote the transferring. \textbf{2)} Due to the large interference of FP or FN problem, the CRL and DCRL methods all have weaker performance than the network pre-trained by ImageNet in task 1. This is because the FP or FN problem interrupts the representation learning of high-level semantics and makes their representations deviate from reality. Therefore, even though these self-supervised pre-trained networks have improved the learning of downstream tasks, their upper limit is extremely limited.

Compared with the other DCRL methods and the conference vision (GVSL), our GEMINI-MIP achieves the best performance with three observations: \textbf{1)} Our GEMINI-MIP has great MIP ability with the highest DSC (92.1\%, 79.1\%, 89.8\%) in all tasks. Although the DSC-PM also achieves great performance in these three tasks (90.5\%, 77.2\%, 83.3\% DSC), its average DSC is still lower than our GVSL-MIP and GEMINI-MIP owing to the interference of FP\&N. Our homeomorphism prior brings reliable correspondence discovery and significantly weakens the FP\&N problem, thus greatly improving the representation. \textbf{2)} Our GEMINI brings significant improvements in both inner-scene and inter-scene transferring tasks. It achieves a very competitive 92.1\% DSC on task 1, and the best score (79.1\%, 89.8\% DSC) in the other inter-scene transferring tasks. The PixPro has reasonable performance on inner-scene tasks (91.5\% DSC), but it only has 73.6\% and 63.8\% DSC on the inter-scene tasks which is much lower than our framework. Because the reliable positive and negative pairs in our framework enable the network to pre-learn both low-level patterns and high-level semantics, this makes the pre-learned knowledge match the reality showing greater transferring ability. \textbf{3)} The GVSL-MIP has achieved similar performance as our GEMINI-MIP in inter-scene transferring because of its geometric visual similarity which will learn soft negative pairs. Our GEMINI-MIP further takes the geometric semantic similarity and has achieved 2.4\% DSC improvement in inner-scene transferring.

\subsubsection{Qualitative evaluation shows visual superiority}
As shown in Fig.\ref{Fig:results1}, the visualization of the segmentation results demonstrates our superiority in the SS-MIP tasks. Due to our reliable correspondence discovery, the pre-training process makes the network represent consistent and distinct features for the same and different semantic regions, having very effective initialization. Therefore, our GEMINI-MIP has a very fine visualization than the compared DCRL methods and conference vision, GVSL-MIP. In task 3, the VADeR, DenseCL, and SetSim lose some very small brain structures which are sensitive and easy to be interfered with by the misguidance from the FP\&N problem. The VADeR also have very poor performance on mixed kidney region in the tasks due to the FN problem which enlarges the network's challenge to distinguish these complex regions.
