

\documentclass[10pt,journal,compsoc]{IEEEtran}

\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi

\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{diagbox}
%\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{url}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{amsthm}
\usepackage{hyperref}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{marvosym}
\usepackage{threeparttable}
\usepackage{array,subfig}

\usepackage{mathrsfs}
\usepackage{color,xcolor}
\usepackage{colortbl}
\begin{document}
\title{Homeomorphism Prior for False Positive and Negative Problem in Medical Image Dense Contrastive Representation Learning}

\author{Yuting He,
        Boyu Wang,
        Rongjun Ge,
        Yang Chen,
        Guanyu Yang\IEEEauthorrefmark{1},
        Shuo Li
\thanks{\vspace*{-1\baselineskip} \newline {\emph{\IEEEauthorrefmark{1}Corresponding authors: G. Yang. (e-mail: yang.list@seu.edu.cn)}}}
\thanks{This research was supported by the National Natural Science Foundation of China (Grant No. 82441021), the Natural Science Foundation of Jiangsu Province (Grant No. BK20210291), the National Natural Science Foundation of China (Grant No. 62101249, T2225025), the Jiangsu Shuangchuang Talent Program (Grant No. JSSCBS20220202).}
\IEEEcompsocitemizethanks{
\IEEEcompsocthanksitem Y. He, Y. Chen and G. Yang\IEEEauthorrefmark{1} are with the Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, Nanjing, China, the Centre de Recherche en Information Biomédicale Sino-Français (CRIBs), and Jiangsu Provincial Joint International Research Laboratory of Medical Information Processing, Nanjing, China. (e-mail: yang.list@seu.edu.cn)
\IEEEcompsocthanksitem R. Ge is with the School of Instrument Science and Engineering, Southeast University, Nanjing, China (e-mail: rongjun\_ge@seu.edu.cn)
\IEEEcompsocthanksitem B. Wang is with the Department of Computer Science, Western University, London, ON N6A 3K7, Canada. (e-mail: bwang@csd.uwo.ca)
\IEEEcompsocthanksitem S. Li and Y. He are with the Department of Biomedical Engineering and the Department of Computer and Data Science, Case Western Reserve University, Cleveland, OH 44106 USA (e-mail: shuo.li11@case.edu).
}% <-this % stops an unwanted space
\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}

\IEEEtitleabstractindextext{%

\input{tex/abstract}
}

\maketitle

\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle


\input{tex/Introduction}
\input{tex/Related_work}
\input{tex/Methodology}

\input{tex/Experiment2}
\input{tex/Experiment1}


\input{tex/Discussion}
\input{tex/Conclusion}

\appendix



\begin{table*}[tb]
\centering
\caption{The fine-tuning evaluations demonstrate our great transferring ability on SS-MIP tasks which pre-trained on PPMI dataset. Our GEMINI-MIP achieves the best performance compared with 18 methods on two downstream tasks.}
\begin{tabular}{clcccccccccc}
\toprule
\multirow{2}{*}{\textbf{Type}}
&\multirow{2}{*}{\textbf{Pre-training}}
&\multicolumn{2}{c}{\textbf{T2: KiPA22} \emph{Inter-scene}}
&\multicolumn{2}{c}{\textbf{T3: CANDI} \emph{Inner-scene}}
%&\multicolumn{2}{c}{\textbf{T3: CANDI-mini} \emph{Inner-scene}}
&\textbf{AVG}
\\
\cmidrule(r){3-4}
\cmidrule(r){5-6}
%\cmidrule(r){7-8}
\cmidrule(r){7-7}
&
&DSC$_{\pm std}\uparrow$
&AVD$_{\pm std}\downarrow$
&DSC$_{\pm std}\uparrow$
&AVD$_{\pm std}\downarrow$
%&DSC$_{\pm std}\uparrow$
%&AVD$_{\pm std}\downarrow$
&DSC$_{\pm std}\uparrow$
\\
\midrule
-
&Scratch (3D U-Net)
&72.4$_{\pm16.3}$
&6.11$_{\pm5.91}$
&84.0$_{\pm3.2}$
&0.52$_{\pm0.14}$
%&62.1$_{\pm10.1}$
%&1.59$_{\pm0.83}$
&78.2$_{\pm9.8}$
\\
\cdashline{1-7}[0.8pt/2pt]
Sup
&Med3D \cite{chen2019med3d}
&81.7$_{\pm12.0}$
&\color{blue}2.61$_{\pm2.77}$
&72.7$_{\pm19.0}$
&1.57$_{\pm2.56}$
%&$_{\pm}$
%&$_{\pm}$
&77.2$_{\pm15.5}$
\\
GRL
&Denosing \cite{vincent2010stacked}
&70.0$_{\pm15.4}$
&7.60$_{\pm5.03}$
&\cellcolor[gray]{0.9}83.7$_{\pm3.3}$
&\cellcolor[gray]{0.9}1.71$_{\pm0.20}$
%&\cellcolor[gray]{0.9}unable%$_{\pm}$
%&\cellcolor[gray]{0.9}unable%$_{\pm}$
&76.9$_{\pm9.4}$
\\
&In-painting \cite{pathak2016context}
&69.7$_{\pm17.1}$
&7.57$_{\pm5.93}$
&\cellcolor[gray]{0.9}88.5$_{\pm3.1}$
&\cellcolor[gray]{0.9}0.32$_{\pm0.11}$
%&\cellcolor[gray]{0.9}$_{\pm}$
%&\cellcolor[gray]{0.9}$_{\pm}$
&79.1$_{\pm10.1}$
\\
&Models Genesis \cite{zhou2019models}
&75.8$_{\pm13.7}$
&4.64$_{\pm4.49}$
&\cellcolor[gray]{0.9}88.7$_{\pm3.1}$
&\cellcolor[gray]{0.9}0.31$_{\pm0.10}$
%&\cellcolor[gray]{0.9}$_{\pm}$
%&\cellcolor[gray]{0.9}$_{\pm}$
&82.3$_{\pm8.4}$
\\
&Rotation \cite{komodakis2018unsupervised}
&77.4$_{\pm14.3}$
&4.82$_{\pm6.29}$
&\cellcolor[gray]{0.9}89.4$_{\pm2.6}$
&\cellcolor[gray]{0.9}0.28$_{\pm0.08}$
%&\cellcolor[gray]{0.9}$_{\pm}$
%&\cellcolor[gray]{0.9}$_{\pm}$
&83.4$_{\pm8.5}$
\\
CRL
&SimSiam \cite{Chen2021CVPR}
&83.8$_{\pm11.9}$
&3.69$_{\pm7.47}$
&\cellcolor[gray]{0.9}87.3$_{\pm3.1}$
&\cellcolor[gray]{0.9}0.36$_{\pm0.10}$
%&\cellcolor[gray]{0.9}$_{\pm}$
%&\cellcolor[gray]{0.9}$_{\pm}$
&85.6$_{\pm7.5}$
\\
&BYOL \cite{grill2020bootstrap}
&83.6$_{\pm11.2}$
&2.78$_{\pm5.42}$
&\cellcolor[gray]{0.9}89.7$_{\pm2.4}$
&\cellcolor[gray]{0.9}\color{blue}0.27$_{\pm0.08}$
%&\cellcolor[gray]{0.9}$_{\pm}$
%&\cellcolor[gray]{0.9}$_{\pm}$
&\color{blue}86.7$_{\pm6.8}$
\\
&SimCLR \cite{chen2020simple}
&78.9$_{\pm13.9}$
&4.49$_{\pm5.15}$
&\cellcolor[gray]{0.9}89.2$_{\pm3.0}$
&\cellcolor[gray]{0.9}0.30$_{\pm0.14}$
%&\cellcolor[gray]{0.9}$_{\pm}$
%&\cellcolor[gray]{0.9}$_{\pm}$
&84.1$_{\pm8.5}$
\\
&MoCov2 \cite{chen2020improved}
&78.0$_{\pm15.3}$
&4.42$_{\pm5.67}$
&\cellcolor[gray]{0.9}89.7$_{\pm2.4}$
&\cellcolor[gray]{0.9}0.28$_{\pm0.11}$
%&\cellcolor[gray]{0.9}$_{\pm}$
%&\cellcolor[gray]{0.9}$_{\pm}$
&83.9$_{\pm8.9}$
\\
&DeepCluster \cite{caron2018deep}
&79.7$_{\pm13.7}$
&4.28$_{\pm5.76}$
&\cellcolor[gray]{0.9}89.8$_{\pm2.4}$
&\cellcolor[gray]{0.9}\color{blue}0.27$_{\pm0.08}$
%&\cellcolor[gray]{0.9}$_{\pm}$
%&\cellcolor[gray]{0.9}$_{\pm}$
&84.8$_{\pm8.1}$
\\
DCRL
&VADeR \cite{o2020unsupervised}
&72.1$_{\pm13.8}$
&6.56$_{\pm5.89}$
&\cellcolor[gray]{0.9}87.4$_{\pm3.6}$
&\cellcolor[gray]{0.9}0.35$_{\pm0.11}$
%&\cellcolor[gray]{0.9}$_{\pm}$
%&\cellcolor[gray]{0.9}$_{\pm}$
&79.8$_{\pm8.7}$
\\
&DenseCL \cite{wang2022densecl}
&74.0$_{\pm15.8}$
&6.42$_{\pm8.21}$
&\cellcolor[gray]{0.9}87.7$_{\pm3.8}$
&\cellcolor[gray]{0.9}0.34$_{\pm0.13}$
%&\cellcolor[gray]{0.9}$_{\pm}$
%&\cellcolor[gray]{0.9}$_{\pm}$
&80.9$_{\pm9.8}$
\\
&SetSim \cite{wang2022exploring}
&73.5$_{\pm15.9}$
&6.34$_{\pm6.68}$
&\cellcolor[gray]{0.9}88.4$_{\pm3.1}$
&\cellcolor[gray]{0.9}0.32$_{\pm0.10}$
%&\cellcolor[gray]{0.9}$_{\pm}$
%&\cellcolor[gray]{0.9}$_{\pm}$
&81.0$_{\pm9.5}$
\\
&DSC-PM \cite{li2021dense}
&79.0$_{\pm14.6}$
&4.90$_{\pm6.05}$
&\cellcolor[gray]{0.9}88.5$_{\pm3.4}$
&\cellcolor[gray]{0.9}0.32$_{\pm0.13}$
%&\cellcolor[gray]{0.9}$_{\pm}$
%&\cellcolor[gray]{0.9}$_{\pm}$
&83.8$_{\pm9.0}$
\\
&PixPro \cite{xie2021propagate}
&80.0$_{\pm14.4}$
&4.60$_{\pm6.25}$
&\cellcolor[gray]{0.9}\color{blue}89.9$_{\pm2.4}$
&\cellcolor[gray]{0.9}\color{blue}0.27$_{\pm0.07}$
%&\cellcolor[gray]{0.9}$_{\pm}$
%&\cellcolor[gray]{0.9}$_{\pm}$
&85.0$_{\pm8.4}$
\\
&GLCL \cite{chaitanya2020contrastive}
&70.7$_{\pm16.9}$
&7.33$_{\pm7.05}$
&\cellcolor[gray]{0.9}87.4$_{\pm3.2}$
&\cellcolor[gray]{0.9}0.34$_{\pm0.09}$
%&\cellcolor[gray]{0.9}$_{\pm}$
%&\cellcolor[gray]{0.9}$_{\pm}$
&79.1$_{\pm10.1}$
\\
\cdashline{1-7}[0.8pt/2pt]
\textbf{DCRL}
&\textbf{GVSL-MIP (CVPR)}\cite{He_2023_CVPR}
&\color{blue}84.3$_{\pm10.3}$
&2.85$_{\pm5.12}$
&\cellcolor[gray]{0.9}89.1$_{\pm2.8}$
&\cellcolor[gray]{0.9}0.31$_{\pm0.11}$
%&\cellcolor[gray]{0.9}$_{\pm}$
%&\cellcolor[gray]{0.9}$_{\pm}$
&\color{blue}86.7$_{\pm6.6}$
\\
\textbf{(Ours)}
&\textbf{GEMINI-MIP}
&\color{red}\textbf{85.0$_{\pm10.2}$}
&\color{red}\textbf{2.55$_{\pm5.71}$}
&\cellcolor[gray]{0.9}\color{red}\textbf{90.0$_{\pm2.4}$}
&\cellcolor[gray]{0.9}\color{red}\textbf{0.26$_{\pm0.07}$}
%&\cellcolor[gray]{0.9}76.3$_{\pm6.0}$
%&\cellcolor[gray]{0.9}0.78$_{\pm0.26}$
&\color{red}\textbf{87.5$_{\pm6.3}$}
\\
\bottomrule
\end{tabular}
\label{supp:tab:metrics}
\end{table*}
\section*{A SS-MIP on more datasets}
\label{aupp:sec:task1}
\subsection*{A.1 Self-supervised pre-training on PPMI dataset}
We further evaluate the SS-MIP task on another pretext dataset for pre-training to demonstrate our representation ability. We extracted 837 3D brain T1 MR images with Parkinson’s disease from the PPMI database\footnote{PPMI database: \url{https://www.ppmi-info.org/}} as our pretext dataset. In our experiment, we extract the brain regions via HD-BET \cite{isensee2019automated}, crop and resize the images to $160\times160\times128$, and finally normalize them via the zero-score. Due to the consistency of the human brain regions, we randomly pair these brain images to pre-train the frameworks. Following the Experiment 2 (Sec.5) in our manuscript, we take the Task 2: KiPA22 dataset and Task 3: CANDA as the downstream tasks to evaluate the inter-scene and inner-scene transferring abilities. (Because the Task 1: SCR$_{25}$ dataset is 2D and the pre-trained models are 3D, we exclude this task in this experiment.) We utilize the same implementation and evaluation metrics as the Sec.5 in this experiment.

As shown in Tab.\ref{supp:tab:metrics}, it achieves similar observations as the SS-MIP experiment in Sec.5. For most of the methods, the pre-training on the PPMI dataset will bring better performance than random initialization (“Scratch”) both in the T2: KiPA22 and T3: CANDI tasks. Especially in the T3 (inner-scene), most of the pre-training methods achieve more than 4.0\% DSC improvement compared with the “Scratch”. Even though the other CRL and DCRL methods have FP\&N problems in this experiment, they are still able to learn the representation of some domain features and promote their final performance to the upper limit of the task 3 (near 90\%). When transferring the pre-trained models to the T2 (inter-scene), the SimSiam, BYOL, our GVSL-MIP, and our GEMINI-MIP all still have significant improvement (more than 10\% DSC). This is because these methods learn the consistency of features and avoid the FP\&N problems. The other methods’ performance improvement is obviously decreased owing to the FP or FN problem which interrupts their representation learning of high-level semantics and makes their representations deviate from reality. On both two tasks, our GEMINI-MIP achieves the highest performance showing our superiority.
\begin{table}[tb]
\centering
\caption{The gap coefficient $G^{i}$ quantifies the gap between “pre-trained on chest X-ray images \& fine-tuning on brain T1 MR images” (inter-scene) and “pre-trained on brain T1 MR images \& fine-tuning on brain T1 MR images” (inner-scene).}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc}
\toprule
\textbf{Index}
&\multirow{3}{*}{\textbf{Method}}
&\textbf{Chest X-ray}
&\textbf{Brain T1 MR}
%&\multicolumn{2}{c}{\textbf{T3: CANDI-mini} \emph{Inner-scene}}
&\textbf{Gap}
\\
\cmidrule(r){1-1}
\cmidrule(r){3-3}
\cmidrule(r){4-4}
%\cmidrule(r){7-8}
\cmidrule(r){5-5}
\multirow{2}{*}{$i$}
&
&2D U-Net
&3D U-Net
&\multirow{2}{*}{$G^{i}$}
\\
&
&\emph{Inter-scene}
&\emph{Inner-scene}
&
\\
\midrule
0
&Scratch
&65.0$_{\pm4.4}$
&84.0$_{\pm3.2}$
&1
\\
\cdashline{1-5}[0.8pt/2pt]
1
&BYOL
&70.5$_{\pm2.1}$
&\cellcolor[gray]{0.9}89.7$_{\pm2.4}$
&1.01
\\
2
&DeepCluster
&60.0$_{\pm2.2}$
&\cellcolor[gray]{0.9}89.8$_{\pm2.4}$
&1.57
\\
3
&Model Genesis
&88.1$_{\pm3.1}$
&\cellcolor[gray]{0.9}88.7$_{\pm3.1}$
&0.03
\\
4
&DenseCL
&76.8$_{\pm2.9}$
&\cellcolor[gray]{0.9}87.7$_{\pm3.8}$
&0.57
\\
\cdashline{1-5}[0.8pt/2pt]
\textbf{5}
&\textbf{Our GEMINI-MIP}
&\textbf{89.8$_{\pm2.6}$}
&\cellcolor[gray]{0.9}\textbf{90.0$_{\pm2.4}$}
&\textbf{0.01}
\\
\bottomrule
\end{tabular}
}
\label{supp:tab:gap}
\end{table}
\subsection*{A.2 Analysis of the gap between the inner-scene and inter-scene transferring}
As shown in Tab.\ref{supp:tab:gap}, the quantitative evaluation of the gap between the inner-scene and inter-scene transferring show our great transferring ability both inner scene and inter scene. Here, we formulate a gap coefficient $G$ to quantify this gap:
\begin{equation}\label{equ:gap}
G^{i}=\frac{S^{i}_{inner}-S^{i}_{inter}}{S^{0}_{inner}-S^{0}_{inter}},
\end{equation}
where the $i$ is the index of the method, $S$ is the score of the method (here we take the DSC). The $S_{inner}^{0}-S_{inter}^{0}$ is the gap of the “Scratch” between the two settings which means the difference caused by the initial situation, such as network structure and dimension. The $S_{inner}^{i}-S_{inter}^{i}$ is the gap of the $i_{th}$ method between the two settings. Therefore, the $\frac{S_{inner}^{i}-S_{inter}^{i}}{S_{inner}^{0}-S_{inter}^{0}}$ means the gap of the model in two settings excluding the gap caused by the initial network. If the $G^{i}$ is larger than 1, it means that the pre-trained model has weaker inter-scene transferring ability than inner-scene transferring. If it is smaller than 1, it means that the model has great inter-scene transferring ability.

Most self-supervised learning methods have large gap between inner- and inter-scene transferring, and our GEMINI has great universal representation for different scenes. The BYOL and DeepCluster are limited in the inter-scene transferring ($G>1$) because they only take the image-level contrast which will represent the high-level semantic features and this representation is very different between scenes. The DenseCL has 0.57 gap coefficient which is better than the BYOL and DenseCL. Because it takes dense contrastive learning which also represent low-level detail features and this representation is shared in different scene. Our GEMINI and the Model Genesis all have good inter-scene transferring ability with very low gap coefficient (0.01 and 0.03), showing their great universal representation ability and demonstrating their potential as an initialization for more scenes.

\section*{B Discussion of the research problem and method}
\subsection*{B.1 Discussion of FP\&N problem}
\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{./picture/FPproblem.pdf}
  \caption{The evaluation of the large-scale FP problem. The true positive (TP) pairs constructed by the features’ similarity (used in DenseCL) only occupy the 5.79\% of the foreground region, and our GEMINI is able to bring 60.74\% TP pairs. }\label{supp:fig:fp}
\end{figure}
As analyzed in the Introduction section, medical images' semantic dependence property will make large-scale FP problem, and their semantic continuity and semantic overlap properties will make large-scale FN problem. In this section, we make an experiment to quantitatively count the percentage of FP and FN pairs in the pairing process.

For FP pairs, we utilize two cardiac CT images (image A and B), and extract their pixel-wise features via a random initialized 3D U-Net. Then, we utilize the pixel-wise feature similarity measurement method in the DenseCL \cite{wang2022densecl} to extract the positive pairs. Because the semantics of the background region are unclear, we count the accuracy of the feature pairs in the foreground regions. As shown in Fig.A, only 5.79\% of the positive pairs in the foreground region are accurate. Therefore, if we directly pair the features only according to their similarity, most of the contrasts (94.21\%) for positive pairs are inaccurate in the medical images and will interrupt the whole contrastive learning process. This is because medical images have very weak contrast due to their special imaging way, making the directly extracted features lack discrimination. Therefore, it makes the “Semantic dependence” one of the inherent properties in medical images constructing large-scale FP pairs.

For FN pairs, we further evaluate the percentage of FN pairs caused by the semantic continuity and semantic overlap properties, and the results show large potential limitations in the DCRL. a) For the PN pairs caused by the “Semantic continuity”, we follow the SimCLR \cite{chen2020simple} which pairs the negative features for each feature. We pair the features in different positions of image A’s foreground regions as negative pairs. The result shows that 17.79\% of the negative pairs are FN pairs which have the same semantics. Although the existing DCRL methods utilize attention \cite{wang2022exploring} or clustering \cite{li2021dense} to avoid directly dividing adjacent pixel-wise features as negative pairs, the FN caused by “semantic continuity” is still an open and challenging problem. b) For the FN pairs caused by the “Semantic overlap”, we follow the DenseCL \cite{wang2022densecl} which pairs the current features and the memory bank features as the negative pairs. We make the features of image B in the foreground as the memory bank features and the features of image A in the foreground as the current features. Then, we pair the current and memory bank features as negative pairs and calculate the accuracy. Finally, 17.53\% of the negative pairs are FN pairs which have the same semantics. The “Semantic overlap” property of the medical images makes it inevitable that there will be numerous consistent semantic regions between medical images. Therefore, it will produce 17.53\% FN pairs in the training process making the model learn in an unreliable direction.


\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{./picture/fitting.pdf}
  \caption{The FP and FN pairs have a serious impact on learning. a) The fitting process with FP and FN pairs on a cardiac CT image. b) The models' learned segmentation ability on the fitted case and their generalization ability on another testing case.}\label{supp:fig:fitting}
\end{figure}

According to the above probability of FP and FN pairs, we simulated the number of these FP and FN pairs in a supervised heart segmentation learning task. Specifically, we train a U-Net on the cardiac structures segmentation task with a cardiac CT image (Image A in Fig.\ref{supp:fig:fp}) to evaluate the fitting ability of the model with or without FP\&N pairs. a) In the non-FP\&N pairs setting, we utilize the contrastive segmentation learning like Wang et al. \cite{wang2019panet}. b) In the FP\&N pairs setting, we randomly generate FP (94\%) and FN (17\%) pairs in the contrastive segmentation learning. c) We further reduce the probabilities of FP and FN pairs to one-third of original (31\% and 6\%) to give an ablation of the false pairs’ degree. We take 1000 iterations, and draw the loss values of the learning process on a line chart to visualize the fitting process. We also evaluate the segmentation of the fitted case and another testing case (Image B in Fig.\ref{supp:fig:fp}) to evaluate the model learned representation with false pairs.

As shown in Fig.\ref{supp:fig:fitting}, the FP and FN pairs have a serious impact on learning. Without the FP\&N pairs, the model is able to be fitted to the target cardiac images, and learn the representation ability of the semantic regions. However, when learning with large-scale FP\&N pairs (94\%, 17\%), the model is unable to be fitted to the targets owing to the interference of the noisy optimization targets. When reducing the FP\&N degree to one-third, the model is able to be gradually fitted to the target image and has a certain generalization, but its performance is weaker than the ``no FP\&N" situation. Therefore, we can draw the following two conclusions in DCRL: a) the large-scale FP\&N problem will make the model unable to learn representation; b) alleviating the FP\&N degree, the model will be able to learn the representation ability of data with generalization ability. Therefore, our GEMINI embeds the homeomorphism prior to the DCRL for the large-scale FP\&N problem, enhancing the learning of true feature pairs. Although it is challenging to remove FP\&N pairs without annotation, reducing the FP\&N degree via our GEMINI is still able to guides the model to learn a generalizable representation.

\subsection*{B.2 Discussion of the novelty in GEMINI}
The proposed GEMINI is a novel dense contrastive representation learning paradigm in medical image analysis. Not only in the innovations, i.e., our DHL and GSS, it also achieved great novelty in principle.

\emph{In principle}, our GEMINI has advanced the theoretical foundation of homeomorphism for the dense contrastive representation learning, providing a principle inspiration to the community. It modeled the human consistent anatomy in medical images based on the principle of topologie \cite{hubbard1991differential}, proposed a new principal concept, homeomorphism prior, and formulated it in the DCRL task as a new paradigm. Therefore, the community will be further inspired by our principle of homeomorphism and make new scientific and technological progress in other tasks and fields.

\emph{In methodology}, our work has proposed a novel dense contrastive representation learning framework that enables the contrast of feature pairs under the condition of human inherent topology, thus promoting the DCRL in medical images. It modeled the consistency of human inherent topology (i.e., homeomorphism prior) as a learning for deformable mapping to overcome the reliability issue in DCRL’s feature correspondence process, giving one potential answer to the long-standing question of “how to achieve a reliable dense feature correspondence for unlabeled data?” Based on the modeling, the proposed DHL and GSS bring soft learning of feature pairs and reliable learning of positive pairs, promoting the contrast of features in DCRL. Finally, our work has achieved a new ability to learn reliable semi-supervised medical image segmentation and pre-training models.

\section*{C More framework analysis and experiment discussion}
\subsection*{C.1 Discussion of the receptive field $r$ in the Deformer network}
\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{./picture/receptfield.pdf}
  \caption{The ablation study of the receptive field size $r$ and the network parameter amounts. a) The segmentation performance on the T1 of FS-Semi setting with the increasing of the receptive field size $r$. b) The fine-tuning performance with the enlarging of the parameter amount (million, $M$) in the pre-trained networks.}\label{supp:fig:rece}
\end{figure}

The performance is robust for the receptive field $r$. As shown in Fig.\ref{supp:fig:rece} a), we enlarge the receptive field $r$ via adding the depth and down-sampling stages of the “Deformer” network and evaluate the model's performance on T1 of FS-Semi setting. With the enlarging of the receptive field, the models’ performance is stalely around 90\% DSC. Because the backbone network and “Deformer” network together constitute a whole network to learn the feature representation, and the features from the backbone network have been extracted from a large receptive field. Therefore, even the receptive field of the “Deformer” network is small, the final DVF is still calculated from a large receptive field. The layers inner the backbone is still optimized by the gradient with a big reception, so that our GEMINI keeps stable performance with the enlarging of $r$. Owing to the soft learning of feature pairs in our DHL, once added this module, the framework achieves more than 5\% DSC improvement.

\subsection*{C.2 Discussion of the parameter amount}
As shown in Fig.\ref{supp:fig:rece} b), we have evaluated our GEMINI on different settings of model parameters. We pre-trained our GEMINI-MIP on the ChestX-ray8 dataset for the networks with 0.12$M$, 0.49$M$, 1.97$M$, 7.85$M$, 31.39$M$, and 125.52$M$ ($M$ is million) parameters, and fine-tuned them on the T1: SCR25 task. With the enlarging of the network, the model performance is improving quickly. This is because the network capacity increases with the enlarging of the networks so that it will be able to learn the representation of more features in the pre-training process. When the parameter amount is 1.97M, the speed of performance improving is reduced, illustrating that the increase of network capacity has approached the upper bound of this task. Therefore, when the network is further enlarged to 125.52$M$ (more than 50 times compared with 1.97$M$), the performance is only improved 1.4\% DSC. %Considering the cost of model training, we all utilize the networks with 1.97$M$ parameters as the backbone network for all experiments.

\subsection*{C.3 Discussion of the feature distribution}
\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{./picture/points.pdf}
  \caption{The t-SNE visualization of the learned pixel representations. We provide the coordinates of pixels in a zoomed view, indicating their spatial relationship.}\label{supp:fig:distri}
\end{figure}

As shown in Fig.\ref{supp:fig:distri}, we visualize the learned representation by our framework to demonstrate its effectiveness in distinguishing different semantic regions. In the three tasks of the SS-MIP experiment, we randomly select the slices or patches from the test datasets and extract their pixel-wise features via the backbone network initialized from scratch (a) and our GEMINI-MIP (b). Then, these features are zoomed by t-SNE \cite{van2008visualizing} to two dimensions. As demonstrated in the enlarged region, the features from the ``Scratch" model is mixed owing to its initial weak representation. The pixel-wise features from our framework are clustered into several meaningful groups. Most of the pixels in each group are spatially close and in different groups are also spatially separated (indicated by their coordinates (c)) in the original image. Because our GEMINI discovers the correspondence of pixel-wise features based on the homeomorphism of human body and learns the representation according to the consistent context topology, the same semantic features which are spatially close will be clustered.

\subsection*{C.4 Discussion of the cross-architecture compatibility}
\begin{table*}[tb]
\centering
\caption{The FS-Semi evaluations on U-Net \cite{ronneberger2015u}, TransUNet \cite{chen2021transunet}, and SwinUNet \cite{cao2022swin} demonstrate the cross-architecture compatibility of our GEMINI. The ``-" means that the setting is unable to be implemented.}
%\resizebox{\textwidth}{!}
{
\begin{tabular}{clccccccccccccccc}
  \toprule
  \multirow{2}{*}{\textbf{Type}}
  &\multirow{2}{*}{\textbf{Method}}
  &\multicolumn{2}{c}{\textbf{T1: 3D cardiac structures}}
  &\multicolumn{2}{c}{\textbf{T2: 3D brain tissues}}
  &\multicolumn{2}{c}{\textbf{T3: 2D chest structures}}
  &\textbf{AVG}\\ \cmidrule(r){3-4}\cmidrule(r){5-6}\cmidrule(r){7-8}\cmidrule(r){9-9}
  &
  &DSC$_{\pm std}\uparrow$
  &AVD$_{\pm std}\downarrow$
  &DSC$_{\pm std}\uparrow$
  &AVD$_{\pm std}\downarrow$
  &DSC$_{\pm std}\uparrow$
  &AVD$_{\pm std}\downarrow$
  &DSC$_{\pm std}\uparrow$
  \\
  \midrule
  \textbf{Five}
  &U-Net \cite{ronneberger2015u}
  &84.3$_{\pm9.6}$
  &2.43$_{\pm2.14}$
  &69.5$_{\pm8.8}$
  &1.59$_{\pm0.84}$
  &83.4$_{\pm6.9}$%{\color{purple}$^{*}$}
  &10.34$_{\pm4.80}$
  &79.1$_{\pm8.4}$
  \\
  \textbf{(Lower)}
  & TransUNet \cite{chen2021transunet}
  & 74.5$_{\pm8.3}$
  & 4.41$_{\pm1.39}$
  & 67.4$_{\pm5.4}$
  & 2.02$_{\pm0.46}$
  & 76.5$_{\pm8.2}$
  & 16.59$_{\pm6.53}$
  & 72.8$_{\pm7.3}$
  \\
  & SwinUNet \cite{cao2022swin}
  & 40.8$_{\pm8.0}$
  & 11.59$_{\pm1.32}$
  & 67.8$_{\pm5.3}$
  & 4.04$_{\pm0.39}$
  & 63.9$_{\pm11.5}$
  & 14.26$_{\pm8.91}$
  & 57.5$_{\pm8.3}$
  \\
  \cdashline{1-9}[0.8pt/2pt]
  \textbf{Full}
  &U-Net \cite{ronneberger2015u}
  &-
  &-
  &88.7$_{\pm1.2}$
  &0.31$_{\pm0.04}$
  &96.1$_{\pm1.4}$%{\color{purple}$^{*}$}
  &2.28$_{\pm1.00}$
  &-
  \\
   \textbf{(Upper)}
  & TransUNet \cite{chen2021transunet}
  & -
  & -
  & 85.7$_{\pm1.2}$
  & 0.43$_{\pm0.05}$
  & 95.2$_{\pm2.1}$
  & 2.78$_{\pm1.35}$
  & -
  \\
  & SwinUNet \cite{cao2022swin}
  & -
  & -
  & 82.8$_{\pm2.7}$
  & 0.54$_{\pm0.15}$
  & 95.3$_{\pm1.2}$
  & 2.17$_{\pm0.65}$
  & -
  \\
  \cdashline{1-9}[0.8pt/2pt]
  \textbf{Semi}
  &\textbf{GEMINI+U-Net}
  &91.2$_{\pm3.6}$
  &0.97$_{\pm0.56}$
  &87.3$_{\pm1.0}$
  &0.35$_{\pm0.03}$
  &87.7$_{\pm5.2}$
  &7.14$_{\pm3.63}$
  &88.7$_{\pm3.3}$
  \\
   \textbf{(Ours)}
  &\textbf{GEMINI+TransUNet}
  &90.8$_{\pm3.4}$
  &0.94$_{\pm0.51}$
  &84.4$_{\pm1.3}$
  &0.45$_{\pm0.05}$
  &88.4$_{\pm5.7}$
  &8.63$_{\pm4.68}$
  &87.9$_{\pm3.5}$
  \\
  &\textbf{GEMINI+SwinUNet}
  &88.6$_{\pm4.2}$
  &1.28$_{\pm0.64}$
  &79.9$_{\pm5.0}$
  &0.62$_{\pm0.20}$
  &86.2$_{\pm7.8}$
  &6.34$_{\pm4.34}$
  &84.9$_{\pm5.7}$
  \\
  \bottomrule
\end{tabular}
}
\label{tab:arch}
\end{table*}
As shown in the Tab.\ref{tab:arch}, we perform the TransUNet \cite{chen2021transunet} (CNN+transformer), SwinUNet \cite{cao2022swin} (Transformer), and U-Net \cite{ronneberger2015u} (CNN) on the FS-Semi tasks with three datasets, and our GEMINI demonstrates a great compatibility across these model architectures. There are two observations: \textbf{a)} Our GEMINI has achieved significant improvement on both TransUNet \cite{chen2021transunet}, SwinUNet \cite{cao2022swin}, and U-Net \cite{ronneberger2015u}. Compared with the lower bound of the architectures that are trained only with five labeled images, our GEMINI has improved them more than 9\% DSC on AVG owing to our learning of the  homeomorphism mapping between medical images. Especially, for the 3D brain tissues segmentation, GEMINI achieves a similar performance compared with the FULL setting (83 labels) only with 5 labels in all architectures, demonstrating our great potential in reducing of annotation costs. \textbf{b)} Our GEMINI has great architecture compatibility across CNN-based (U-Net [3]), transformer-based (SwinUNet [2]), and CNN-transformer-based (TransUNet) networks. For U-Net and TransUNet that utilizes CNN to encode and decode features, our GEMINI has similar significant improvement that achieves 88.7\% and 87.9\% on AVG DSC. SwinUNet takes patch-embedding and four-times down sampling at the beginning, and utilizes the shifted window to learn global features. Therefore, it is challenging to represent fine-grained dense features and makes the whole network easy to overfit to the global features when the amount of training cases is small. As a result, SwinUNet has very poor performance on “FIVE” setting. When adding GEMINI, it learns the inter-image consistency for unlabeled images and effectively reduces the over-fitting, thus also achieving more than 20\% DSC improvement.

\subsection*{C.5 Discussion of the computing costs}
\begin{table}[tb]
\centering
\caption{Owing to the additional deformer networks, our GEMINI has relatively higher computing costs in the pre-training stage, but it has same computing costs in the fine-tuning for downstream tasks as other methods and achieves much higher performance.}
\resizebox{\linewidth}{!}
{
\begin{tabular}{clccccccccccccccc}
  \toprule
  \multirow{2}{*}{\textbf{Type}}
  &\multirow{2}{*}{\textbf{Method}}
  &\textbf{Pre-training}
  &\textbf{Downstream}
  &\textbf{T1: SCR$_{25}$}
  \\
  \cmidrule(r){3-3}\cmidrule(r){4-4}\cmidrule(r){5-5}
  &
  & FLOPs
  & FLOPs
  & DSC$_{\pm std}$
  \\
  \midrule
  1$\times$Encoder
  &Rotation \cite{komodakis2018unsupervised}
  &5.99G
  &20.15G
  &80.5$_{\pm7.7}$
  \\
  2$\times$Encoder
  &BYOL \cite{grill2020bootstrap}
  &11.98G
  &20.15G
  &89.4$_{\pm4.9}$
  \\
  1$\times$Encoder-decoder
  &Model Genesis \cite{zhou2019models}
  &19.74G
  &20.15G
  &86.1$_{\pm4.6}$
  \\
  2$\times$Encoder-decoder
  &VADeR \cite{o2020unsupervised}
  &39.67G
  &20.15G
  &85.2$_{\pm5.1}$
  \\
  \cdashline{1-5}[0.8pt/2pt]
  2$\times$Encoder-decoder
  &\textbf{Our GEMINI}
  &\textbf{52.59G}
  &20.15G
  &\textbf{92.1$_{\pm2.8}$}
  \\
  \bottomrule
\end{tabular}
}
\label{tab:computing}
\end{table}
As shown in Tab.\ref{tab:computing}, we compare the methods’ number of floating-point operations (FLOPs) in four architecture types in pre-training stage, i.e., ``1$\times$Encoder" that only runs an encoder in the pre-training, here, we take the Rotation method \cite{komodakis2018unsupervised}; ``2$\times$Encoder" that runs two encoders for contrastive representation learning in the pre-training, here we take the BYOL \cite{grill2020bootstrap}; ``1$\times$Encoder-decoder" that runs an encoder-decoder network, like the U-Net, in the pre-training, here we take the Model Genesis \cite{zhou2019models}; ``2$\times$Encoder-decoder" that runs two encoder-decoder networks for dense contrastive representation learning in the pre-training, here we take the VADeR \cite{o2020unsupervised}. Our GEMINI is also a ``2$\times$Encoder-decoder" method. In the pre-training stage, all methods take U-Net (for Encoder-decoder) or the encoder part of the U-Net (for Encoder) as their backbone. In the downstream adaptation stage, all methods’ pre-trained parameters are used to initialize the U-Net to learn segmentation task (T1: SCR$_{25}$) and the part without pre-training is initialized randomly. All methods utilize same input sizes with [300$\times$300] in pre-training stage and [512$\times$512] in downstream stage. Owing to two additional deformer networks to learning the homeomorphism mapping, our GEMINI has the highest FLOPs in the pre-training stage, but it greatly contributes to the pre-training performance. In the downstream stages, owing to all methods take the U-Net with same parameter amount, our GEMINI has same FLOPs as other methods. As a results, our GEMINI has very significant performance improvement on the SCR$_{25}$ task owing our reliable learning for positive and negative feature pairs. The large-scale FP\&N problem in VADeR makes it has worse performance than the BYOL even it has larger FLOPs in pre-training.

The additional the one-time cost of our GEMINI in the pre-training stage bring obtain better representation, effectively reducing the long-time computing costs in the downstream tasks. Because once the pre-training is completed, stronger representation will accelerate the convergence speed of the model on downstream tasks, thus reducing the long-time computing cost in the training of numerous downstream tasks. As illustrated in the Fig.12 of our paper, compared with the BYOL \cite{grill2020bootstrap}, DenseCL \cite{wang2022densecl}, Model Genesis \cite{zhou2019models}, our GEMINI achieved better performance with fewer iterations, illustrating its potential in reducing the computing costs in downstream tasks.

\subsection*{C.6 Discussion of the second-best models}
Compared with the second-best methods (BRBS \cite{he2022learning} in Tab.2 and our CVPR version, GVSL \cite{He_2023_CVPR}, in Tab.3), we can find these methods also fused the homeomorphism prior into their framework, and their great performance demonstrates the great potential of this prior knowledge in medical images. In our Experiment 1: FS-Semi (Sec.4), the BRBS is a “learning registration to learn segmentation” method whose registration part is based on the homeomorphism prior. Therefore, its powerful performance in the T1: 3D cardiac structures and T2: 3D brain tissues illustrate the advantages. However, the BRBS’s visual similarity make it unable to generalize to the chest X-ray (T3: 2D chest structures) that has relatively low contrast as illustrated in Fig.8. Our GEMINI utilize the semantic similarity based on features and achieves significant improvement on this task, demonstrating our superiority. In our Experiment 2: SS-MIP (Sec.4), our CVPR version, GVSL, benefits from our homeomorphism prior, achieving second-best performance on the T2: KiPA22 and T3: CANDI. However, it also utilizes the visual similarity which is limited on the low-contrast images, i.e., the chest x-ray images in the pre-training dataset. Therefore, its pre-trained representation for chest x-ray is relatively weaker and limits its performance in the inner-scene transferring. Our GSS improves the measurement of the correspondence degree, and drive the representation learning for low-contrast targets during pre-training. Therefore, our GEMINI has significantly improved the GVSL’s performance on the T1: SCR$_{25}$ task.

\subsection*{C.7 Discussion of the reliability}
\begin{table*}[tb]
\centering
\caption{The evaluation of our GEMINI's reliability on the tasks in Experiment 1. The \emph{Cor} is the Pearson correlation coefficient \cite{cohen2009pearson}, and the \emph{p} is the p-value.}
%\resizebox{\linewidth}{!}
{
\begin{tabular}{lccccccccccccccc}
  \toprule
  \diagbox{Evaluations}{Tasks}
  &\multicolumn{2}{c}{\textbf{T1: 3D cardiac structures}}
  &\multicolumn{2}{c}{\textbf{T2: 3D brain tissues}}
  &\multicolumn{2}{c}{\textbf{T3: 2D chest structures}}
  &\multicolumn{2}{c}{\textbf{AVG}}
  \\
  \midrule
  \multirow{2}{*}{\textbf{a) Reliability across samples}}
  &\textbf{DSC} $\uparrow$
  &\textbf{std} $\downarrow$
  &\textbf{DSC} $\uparrow$
  &\textbf{std} $\downarrow$
  &\textbf{DSC} $\uparrow$
  &\textbf{std} $\downarrow$
  &\textbf{DSC} $\uparrow$
  &\textbf{std} $\downarrow$
  \\
  &91.2
  &3.6
  &87.3
  &1.0
  &87.7
  &5.2
  &88.7
  &3.3
  \\
  \midrule
  \multirow{2}{*}{\textbf{b) Reliability across training}}
  &\textbf{\emph{Cor}} $\uparrow$
  &\textbf{\emph{p}} $\downarrow$
  &\textbf{\emph{Cor}} $\uparrow$
  &\textbf{\emph{p}} $\downarrow$
  &\textbf{\emph{Cor}} $\uparrow$
  &\textbf{\emph{p}} $\downarrow$
  &\textbf{\emph{Cor}} $\uparrow$
  &\textbf{\emph{p}} $\downarrow$
  \\
  &0.989
  & $<$0.001
  &0.999
  & $<$0.001
  &0.968
  & $<$0.001
  &0.985
  & $<$0.001
  \\
  \bottomrule
\end{tabular}
}
\label{tab:reliability}
\end{table*}
As shown in Tab.\ref{tab:reliability}, in the three tasks of our Experiment 1, we calculated the standard deviations (std) and the inter-training Pearson correlation coefficients (Cor) \cite{cohen2009pearson}. The results indicate that our GEMINI demonstrates strong reliability across different tested samples and training initializations. \emph{a) Reliability across samples:} We evaluated the DSC and std of the performance across the tested samples. Our GEMINI-Semi achieved an average of 88.7\% DSC with a 3.3 std, indicating high performance with robustness across diverse samples, which supports its reliability in real-world applications. \emph{b) Reliability across training:} We conducted a test-retest reliability analysis \cite{guttman1945basis} and reported the Cor for the performance when our GEMINI-Semi was trained twice from different initialization states. The Cors for all three tasks exceeded 0.95 demonstrating very high consistency between the two training sessions. Additionally, all p-values were below 0.001, indicating significant consistency. Thus, our model shows excellent reliability across different initialization states, which supports its reliability in model implementation.

\section*{D More details in experiments}

\subsection*{D.1 Details of the training diagram}
\begin{figure}[tb]
  \centering
  \includegraphics[width=0.9\linewidth]{./picture/training.pdf}
  \caption{The overall training diagram of our GEMINI. a) The inference process of the whole framework. The gray path in the last line is the additional learning part in the variants of our GEMINI in self-supervised pre-training (GEMINI-MIP) and semi-supervised segmentation (GEMINI-Semi). b) The loss calculation to optimize the whole framework.}\label{supp:fig:diagram}
\end{figure}
\begin{figure*}[!]
  \centering
  \includegraphics[width=\linewidth]{./picture/details_framework.pdf}
  \caption{The detailed architecture of our GEMINI. a) The backbone architecture utilizes the 3D U-Net in 3D image tasks and 2D U-Net in 2D image tasks. b) The deformer network architecture utilized a lightweight U-Net. c-d) The segmentation head in the variant of GEMINI-Semi and the self-restoration head in the variant of GEMINI-MIP.}\label{supp:fig:architecture}
\end{figure*}
As shown in Fig.\ref{supp:fig:diagram}, the training diagram introduces the details of our GEMINI's variants in SSP and Semi experiments. In the forward inference, as described in the ``Methodology" section of our paper, two images $x^{A}, x^{B}$ are put into two shared-weight backbones $\mathcal{N}_{\theta}$ separately to extract the features $f^A, f^B$. The features are further put into two shared-weight deformers together to predict two DVFs $\psi^{AB}, \psi^{BA}$ that are bidirectional. For the variant of GEMINI-Semi, a labeled image $x^C$ is put into the shared-weight backbone $\mathcal{N}_{\theta}$, and then put into an additional segmentation head $Seg_{\kappa}$ to predict the segmentation results $\hat{y}^{C}_{Seg}$. For the variant of GEMINI-MIP, an appearance transformed image $x^C$ (described in Sec.5.1.1) is put into the shared-weight backbones $\mathcal{N}_{\theta}$, and then put into an additional self-restoration head $Res_{\tau}$ to predict the restored image $\hat{y}^{C}_{Res}$. In the loss calculation, the smooth loss (Equ.3) is calculated on the DVFs $\psi^{AB}, \psi^{BA}$ to learn the continuity of the deformable mapping, the GVS loss ($\mathcal{L}_{GVS}$, Equ.6) and GSS loss ($\mathcal{L}_{GSS}$, Equ.7) are calculated on the deformed images $x^{AB}=\psi^{AB}(x^A), x^{BA}=\psi^{BA}(x^B)$ and deformed features $f^{AB}=\psi^{AB}(f^A), f^{BA}=\psi^{BA}(f^B)$ (described in Sec.3.3) to learn the correspondence. For the variant of GEMINI-Semi, a segmentation loss $\mathcal{L}_{Seg}$ is calculated on segmentation result $\hat{y}^{C}_{Seg}$ and the groundtruth $y^{C}_{Seg}$. For the variant of GEMINI-MIP, a self-restoration loss $\mathcal{L}_{Res}$ is calculated on the restored image $\hat{y}^{C}_{Res}$ and the original image $y^{C}_{Res}$. The learning of self-restoration in SSP is a fundamental task for a warm-up of our GSS, due to the initial weak representation in the pretext task.

\subsection*{D.2 Details of the architectures and implementation}
As shown in Fig.\ref{supp:fig:architecture}, we utilize the U-Net \cite{ronneberger2015u} architecture (3D U-Net for 3D images and 2D U-Net for 2D images) as our backbone architecture for great basic dense representation in our experiment. In the encoding path, it takes max pooling layers to reduce the feature maps' resolution and in the decoding path, it takes up-sampling layers (bilinear for 2D images and trilinear for 3D images) to restore the features' resolution. Skip connections are used to transmit features from the encoding path to the decoding path in each resolution stage. There are five resolution stages in the network and each stage utilizes Conv-GN-LeckyReLU\footnote{Conv is a convolution layer and GN is a group normalization layer \cite{wu2018group}.} modules to extract features. The deformer network also takes a lightweight U-Net architecture with very shallow depth to estimate the DVF. It only has three resolution stages and each stage has half of the Conv-GN-LeckyReLU module amount compared with the backbone. Both the segmentation head and self-restoration head take one Conv-GN-LeckyReLU module to project the input features and follow a convolution layer to predict the targets. The detailed hyper-parameters inner these architectures are marked in Fig.\ref{supp:fig:architecture}.






\bibliographystyle{IEEEtran}
\bibliography{mybib}
%\input{tex/bio}


\end{document}


