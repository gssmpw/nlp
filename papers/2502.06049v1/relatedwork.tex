\section{Related Work}
\paragraph{Memory augmented Transformers}
Various methods have been proposed to augment Transformers with memory. One direction is to optimize the attention mechanisms and use some global representations acting as memory points to ensure input coverage. Models like Longformer \cite{DBLP:journals/corr/abs-2004-05150}, Big Bird \cite{DBLP:journals/corr/abs-2007-14062}, GMAT \cite{DBLP:journals/corr/abs-2006-03274} and Extended Transformer Construction \cite{DBLP:journals/corr/abs-2004-08483} all proposed some sparse attention mechanisms to reduce the quadratic dependency of self-attention to linear and introduced global tokens to encode the information from the entire sequence.

Another line of work introduces memorization capabilities to Transformers through recurrence. Transformer-XL \cite{DBLP:journals/corr/abs-1901-02860} addresses the limitation of fixed-length context by introducing segment-level recurrence and relative position encodings. 
% It processes input sequences segment by segment, caching hidden states from previous parts to extend the context window efficiently without recomputing the entire sequence, improving performance on long-sequence tasks. 
However, during training, gradients are restricted to individual segments, limiting the model's ability to capture long-term temporal dependencies. 
% Moreover, using all hidden states from previous segments as memory for each layer significantly increases computational and memory demands during the forward pass. 
Recurrent Memory Transformer (RMT)~\cite{bulatov2022recurrentmemorytransformer} mitigates these limitations by introducing a more efficient memory mechanism. It adds recurrence to Transformers via a small number of special overlapping memory tokens between segments of long sequences, enabling gradients to propagate across them while significantly reducing memory usage. 
RMT outperforms Transformer-XL for sequence processing tasks and is on par with Transformer-XL on language modeling, but requires less memory.
Associative RMT (ARMT) \cite{rodkin2024associativerecurrentmemorytransformer} is a follow-up to RMT that addresses its time complexity issues. 
Similarly, MemReasoner \cite{ko2024memreasoner} introduces a memory-augmented LLM architecture designed for temporal reasoning. However, as demonstrated by \citet{kuratov2024babilong} and \citet{ko2024memreasoner}, RMT continues to outperform these subsequent models, maintaining its status as the state-of-the-art (SOTA) method. 
Therefore, we primarily consider RMT as the SOTA memory-based model and compare \name\ against it.

\paragraph{Retrieval-Augmented Generation (RAG)}
Retrieval-Augmented Generation (RAG) \cite{DBLP:journals/corr/abs-2005-11401} is a popular solution for language models to handle large amounts of text. The core architecture of RAG comprises a retriever module that identifies relevant information from a knowledge base, ensuring that the input to the generative model remains within the token limit while filtering out irrelevant noise, thereby improving efficiency and response quality.
% Hybrid search is a common approach for retrievers, combining dense embeddings generated by bi-encoders for semantic matching with sparse methods like BM25 for precise lexical retrieval, balancing contextual understanding and term accuracy. Following the retrieval phase, a re-ranking module, often implemented using a cross-encoder, can be introduced to filter out top-K from the retrieved documents, forming a two-stage retrieval pipeline.
While Retrieval-Augmented Generation (RAG) has proven effective for many tasks, it struggles with some complicated tasks like multi-hop question-answering \cite{mavi2024multihopquestionanswering}, which require retrieving and reasoning over multiple interconnected pieces of evidence.
%