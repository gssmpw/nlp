@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{huggingface_smollm,
  author       = {Loubna, Ben, Allal and Anton, Lozhkov and Elie, Bakouch},
  title        = {Small Language Models: Efficient, Accessible, and Effective},
  howpublished = {\url{https://huggingface.co/blog/smollm}},
  note         = {Accessed: 2025-01-16},
  year         = {2023}
}

@article{penedo2024fineweb,
  title={The fineweb datasets: Decanting the web for the finest text data at scale},
  author={Penedo, Guilherme and Kydl{\'\i}{\v{c}}ek, Hynek and Lozhkov, Anton and Mitchell, Margaret and Raffel, Colin and Von Werra, Leandro and Wolf, Thomas and others},
  journal={arXiv preprint arXiv:2406.17557},
  year={2024}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{kenton2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of naacL-HLT},
  volume={1},
  pages={2},
  year={2019},
  organization={Minneapolis, Minnesota}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{DBLP:journals/corr/abs-2303-08774,
  author       = {OpenAI},
  title        = {{GPT-4} Technical Report},
  journal      = {CoRR},
  volume       = {abs/2303.08774},
  year         = {2023}
}

@article{team2024gemma,
  title={Gemma 2: Improving open language models at a practical size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@misc{kuratov2024babilong,
      title={BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack}, 
      author={Yuri Kuratov and Aydar Bulatov and Petr Anokhin and Ivan Rodkin and Dmitry Sorokin and Artyom Sorokin and Mikhail Burtsev},
      year={2024},
      eprint={2406.10149},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{bulatov2022recurrentmemorytransformer,
      title={Recurrent Memory Transformer}, 
      author={Aydar Bulatov and Yuri Kuratov and Mikhail S. Burtsev},
      year={2022},
      eprint={2207.06881},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2207.06881}, 
}

@article{DBLP:journals/corr/abs-1901-02860,
  author       = {Zihang Dai and
                  Zhilin Yang and
                  Yiming Yang and
                  Jaime G. Carbonell and
                  Quoc V. Le and
                  Ruslan Salakhutdinov},
  title        = {Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
  journal      = {CoRR},
  volume       = {abs/1901.02860},
  year         = {2019},
  url          = {http://arxiv.org/abs/1901.02860},
  eprinttype    = {arXiv},
  eprint       = {1901.02860},
  timestamp    = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1901-02860.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1902-09113,
  author       = {Qipeng Guo and
                  Xipeng Qiu and
                  Pengfei Liu and
                  Yunfan Shao and
                  Xiangyang Xue and
                  Zheng Zhang},
  title        = {Star-Transformer},
  journal      = {CoRR},
  volume       = {abs/1902.09113},
  year         = {2019},
  url          = {http://arxiv.org/abs/1902.09113},
  eprinttype    = {arXiv},
  eprint       = {1902.09113},
  timestamp    = {Tue, 01 Oct 2024 17:31:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1902-09113.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2004-05150,
  author       = {Iz Beltagy and
                  Matthew E. Peters and
                  Arman Cohan},
  title        = {Longformer: The Long-Document Transformer},
  journal      = {CoRR},
  volume       = {abs/2004.05150},
  year         = {2020},
  url          = {https://arxiv.org/abs/2004.05150},
  eprinttype    = {arXiv},
  eprint       = {2004.05150},
  timestamp    = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2004-05150.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2007-14062,
  author       = {Manzil Zaheer and
                  Guru Guruganesh and
                  Avinava Dubey and
                  Joshua Ainslie and
                  Chris Alberti and
                  Santiago Onta{\~{n}}{\'{o}}n and
                  Philip Pham and
                  Anirudh Ravula and
                  Qifan Wang and
                  Li Yang and
                  Amr Ahmed},
  title        = {Big Bird: Transformers for Longer Sequences},
  journal      = {CoRR},
  volume       = {abs/2007.14062},
  year         = {2020},
  url          = {https://arxiv.org/abs/2007.14062},
  eprinttype    = {arXiv},
  eprint       = {2007.14062},
  timestamp    = {Mon, 16 Oct 2023 13:50:56 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2007-14062.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2006-03274,
  author       = {Ankit Gupta and
                  Jonathan Berant},
  title        = {{GMAT:} Global Memory Augmentation for Transformers},
  journal      = {CoRR},
  volume       = {abs/2006.03274},
  year         = {2020},
  url          = {https://arxiv.org/abs/2006.03274},
  eprinttype    = {arXiv},
  eprint       = {2006.03274},
  timestamp    = {Fri, 27 May 2022 16:07:27 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-03274.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2004-08483,
  author       = {Joshua Ainslie and
                  Santiago Onta{\~{n}}{\'{o}}n and
                  Chris Alberti and
                  Philip Pham and
                  Anirudh Ravula and
                  Sumit Sanghai},
  title        = {{ETC:} Encoding Long and Structured Data in Transformers},
  journal      = {CoRR},
  volume       = {abs/2004.08483},
  year         = {2020},
  url          = {https://arxiv.org/abs/2004.08483},
  eprinttype    = {arXiv},
  eprint       = {2004.08483},
  timestamp    = {Wed, 22 Apr 2020 12:57:53 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2004-08483.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{mavi2024multihopquestionanswering,
      title={Multi-hop Question Answering}, 
      author={Vaibhav Mavi and Anubhav Jangra and Adam Jatowt},
      year={2024},
      eprint={2204.09140},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2204.09140}, 
}

@article{DBLP:journals/corr/abs-2005-11401,
  author       = {Patrick S. H. Lewis and
                  Ethan Perez and
                  Aleksandra Piktus and
                  Fabio Petroni and
                  Vladimir Karpukhin and
                  Naman Goyal and
                  Heinrich K{\"{u}}ttler and
                  Mike Lewis and
                  Wen{-}tau Yih and
                  Tim Rockt{\"{a}}schel and
                  Sebastian Riedel and
                  Douwe Kiela},
  title        = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
  journal      = {CoRR},
  volume       = {abs/2005.11401},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.11401},
  eprinttype    = {arXiv},
  eprint       = {2005.11401},
  timestamp    = {Fri, 29 May 2020 09:57:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-11401.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{hendryckstest2021,
  title={Measuring Massive Multitask Language Understanding},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2021}
}

@inproceedings{westonBCM15,
  author       = {Jason Weston and
                  Antoine Bordes and
                  Sumit Chopra and
                  Tom{\'{a}}s Mikolov},
  title        = {Towards AI-Complete Question Answering: {A} Set of Prerequisite Toy
                  Tasks},
  booktitle    = {{ICLR} (Poster)},
  year         = {2016}
}

@misc{bills2023language,
 title={Language models can explain neurons in language models},
 author={
    Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William
 },
 year={2023},
 howpublished = {\url{https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html}}
}

@inproceedings{KangLYT0F24,
  author       = {Jikun Kang and
                  Romain Laroche and
                  Xingdi Yuan and
                  Adam Trischler and
                  Xue Liu and
                  Jie Fu},
  title        = {Think Before You Act: Decision Transformers with Working Memory},
  booktitle    = {{ICML}},
  publisher    = {OpenReview.net},
  year         = {2024}
}

@article{zhu2020incorporating,
  title={Incorporating bert into neural machine translation},
  author={Zhu, Jinhua and Xia, Yingce and Wu, Lijun and He, Di and Qin, Tao and Zhou, Wengang and Li, Houqiang and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2002.06823},
  year={2020}
}

@inproceedings{liu-lapata-2019-hierarchical,
    title = "Hierarchical Transformers for Multi-Document Summarization",
    author = "Liu, Yang  and
      Lapata, Mirella",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1500/",
    doi = "10.18653/v1/P19-1500",
    pages = "5070--5081",
}

@inproceedings{DosovitskiyB0WZ21,
  author       = {Alexey Dosovitskiy and
                  Lucas Beyer and
                  Alexander Kolesnikov and
                  Dirk Weissenborn and
                  Xiaohua Zhai and
                  Thomas Unterthiner and
                  Mostafa Dehghani and
                  Matthias Minderer and
                  Georg Heigold and
                  Sylvain Gelly and
                  Jakob Uszkoreit and
                  Neil Houlsby},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition
                  at Scale},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2021}
}

@article{li2023laffi,
  title={LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning Language Models},
  author={Li, Qianxi and Cao, Yingyue and Kang, Jikun and Yang, Tianpei and Chen, Xi and Jin, Jun and Taylor, Matthew E},
  journal={arXiv preprint arXiv:2401.00907},
  year={2023}
}

@article{SuALPBL24,
  author       = {Jianlin Su and
                  Murtadha H. M. Ahmed and
                  Yu Lu and
                  Shengfeng Pan and
                  Wen Bo and
                  Yunfeng Liu},
  title        = {RoFormer: Enhanced transformer with Rotary Position Embedding},
  journal      = {Neurocomputing},
  volume       = {568},
  pages        = {127063},
  year         = {2024}
}

@inproceedings{ko2024memreasoner,
  title={MemReasoner: A Memory-augmented LLM Architecture for Multi-hop Reasoning},
  author={Ko, Ching-Yun and Dai, Sihui and Das, Payel and Kollias, Georgios and Chaudhury, Subhajit and Lozano, Aurelie},
  booktitle={The First Workshop on System-2 Reasoning at Scale, NeurIPS'24},
  year={2024}
}

@article{DooleyArchivalAdvantage,
  author = {Dooley, Jackie},
  title = {The Archival Advantage: Integrating Archival Expertise into Management of Born-digital Library Materials},
  journal = {Archival Science Special Issue on Archiving Research Data},
  volume = {7},
  number = {1},
  year = {2007},
  month = {March}
}

@misc{rodkin2024associativerecurrentmemorytransformer,
      title={Associative Recurrent Memory Transformer}, 
      author={Ivan Rodkin and Yuri Kuratov and Aydar Bulatov and Mikhail Burtsev},
      year={2024},
      eprint={2407.04841},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.04841}, 
}