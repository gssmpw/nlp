\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{Photos/TerraTrace-fig3.pdf}
    \vspace{-0.2cm}
    \caption{\textbf{TerraTrace System.} TerraTrace finds the NDVI coordinates from our dataset, extracts a set of metrics to analyze the data, and passes these to GPT-4 Turbo for additional analysis.}
    \label{fig:Fig3}
    \vspace{-0.5cm}
\end{figure}
\section{TerraTrace Platform}
\label{system}
We combine our insights about NDVI signatures from the dataset developed in Sec.~\ref{signatures} to develop TerraTrace, an end-to-end AI powered land use analysis platform. TerraTrace takes in a set of geographic coordinates that define the target region. From the dataset, we filter coordinates within this geo-polygon by coarse latitude and longitude ranges to identify the dataset region. Next we calculate the Euclidean distance between a target coordinate and points in our dataset. We extract the corresponding signature curves within the prescribed polygon by computing the mean NDVI value per time point across valid coordinates. We then interpolate the NDVI values and fit a 3rd order polynomial. TerraTrace presents users with a GUI that plots the region using the Leaflet interactive map library which creates an interface to adjust the polygon and plot the NDVI signature curve. 

TerraTrace also analyzes the data to extract land use insights. First, we check that the data is valid and the curve has $>$10 points for robust classification. We then extract features such as the annual minimum and maximum NDVI, range, and median. We determine the growth and decline rates by calculating the maximum point difference in NDVI values. We then use these metrics to check if the curve is an annual crop. This means that the NDVI increases above 0.2 indicating healthy vegetation growth, reaches a peak between 0.2 and 0.8, followed by a decline. We check the growth and decline rates are $>$0.005 to help filter out perennial species which only have small NDVI fluctuations across seasons.

TerraTrace complements these metrics with an LLM based analysis. We pass in the statistics such as max, min and average, an image of the NDVI curve, and a classification of whether the region contains vegatation using a JSON format. We determine vegetation presence with thresholds: <0.1 is non-vegetative, 0.1-0.2 as some vegetation, and 0.2 as healthy vegetation \cite{eos}. We pass in images converted to grayscale, resized, and encoded as a base64 string within the JSON. Next we construct prompts like the following to query the model. "The area of interest is defined by the $[{coordinates}]$. Please analyze the land cover type at this location." We use GPT-4 Turbo (version 2024-04-09) which translates the curves and data into a detailed analysis table, providing an additional validation. Further explanation of the algorithm is presented in Fig.~\ref{fig:Fig3}.

%We integrate additional data sources as well. We use the CDL to calculate a percentage of total crop pixels of specific types within the target region and 

%\textcolor{red}{TODO: move Historic Wild-Fire Data to the end}

%We pass in the statistics such as max, min and average, an image of the NDVI curve, and a classification of whether the region contains vegatation. We determine this using a simple threshold defining <0.1 as non-vegetative, 0-0.2 as some vegetation, and 0.2 as healthy vegetation. We pass in images converted to grayscale, resized, and encoded as a base64 string within the JSON. Next we construct prompts like the following to query the model. "The coordinates for the area of interest are {coordinates}. Please analyze the land cover type at this location."


%VI: I'm removing this because I don't know if it's clear that the compute required to serve an LLM is lower than this. Maybe it is but without hard numbers I think it may raise questions.
%After the curves are plotted, a comprehensive data analysis is conducted. We reduce the platform's dependence on extensive data and compute power by combining mathematical modeling with LLM analysis.


%To enhance the robustness of our analysis, we employ a Large Language Model GPT-4 Turbo. Utilizing up to 2000 tokens per query, the LLM translates the visual representation of the curves into a detailed analysis table, providing an additional layer of validation. Further explanation of the algorithm is represent in Fig.~\ref{Fig3}.


