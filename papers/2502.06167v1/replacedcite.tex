\section{Related Work}
\label{sec:related_work}

\paragraph{AutoRegressive Models.} AutoRegressive models for visual generation ____ process 2D images by converting them into 1D token sequences. Early approaches, such as PixelCNN ____ and PixelSNAIL ____, introduced pixel-by-pixel image generation using a raster-scan order. Later advancements ____ adapted this idea to generate image tokens following a similar raster sequence. For instance, VQ-GAN ____ utilizes a decoder-only transformer akin to GPT-2 for generating images, while VQVAE-2 ____ and RQ-Transformer ____ enhance the method by incorporating hierarchical scales or stacked representations. Recently, Visual AutoRegressive ($\VAR$) modeling ____ proposed an innovative coarse-to-fine ``next-scale prediction'' strategy, significantly improving scalability, inference speed, and image quality, thus surpassing conventional autoregressive models and diffusion transformers.

\paragraph{Diffusion Models.} Diffusion models ____ excel in generating high-resolution images by iteratively refining noise into coherent visuals. Prominent examples, such as DiT ____ and U-ViT ____, leverage probabilistic frameworks to learn data distributions effectively. Recent progress in diffusion-based image generation has focused on enhancing sampling techniques and training efficiency ____, advancing latent-space learning ____, refining model architectures ____, and exploring applications in 3D generation ____.



\paragraph{Universality of Transformers.}
The universality of transformers refers to their capacity to function as universal approximators, meaning they can theoretically model any sequence-to-sequence function with arbitrary precision. ____ establish this property by demonstrating that transformers achieve universal approximation through the stacking of multiple layers of feed-forward and self-attention functions. Taking a different approach, ____ confirms transformer universality by leveraging the Kolmogorov-Albert representation theorem. Additionally, ____ extend the universality results to architectures featuring non-standard attention mechanisms. More recently, ____ shows that even a transformer with a single self-attention layer can act as a universal approximator. Of independent interest, ____ explore the generalization and approximation properties of transformers under assumptions of HÃ¶lder smoothness and low-dimensional subspaces. ____ showed that the looped transformer can approximate the Hypergraphs algorithm.