
\section{VAR Transformer Blocks}\label{sec:app_var}

In this section, we define the components in the VAR Transformer.

We first introduce the Softmax unit.

\begin{definition}[Softmax] \label{def:softmax}
    Let $z \in \R^{n}$. We define 
    $\mathsf{Softmax}: \R^{n} \to \R^{n}$ satisfying 
    \begin{align*}
        \mathsf{Softmax}(z):= \exp(z) / \langle \exp(z) , {\bf 1}_n \rangle.  
    \end{align*}
\end{definition}

Here, we define the attention matrix in the VAR Transformer as follows.

\begin{definition}[Attention Matrix]\label{def:attn_matrix}
    Let $W_Q, W_K \in \R^{d \times d}$ denote the model weights. Let $X \in \R^{n \times d}$ denote the representation of the length-$n$ input. Then, we define the attention matrix $A \in \R^{n \times n}$ by, For $i,j \in [n]$, 
    \begin{align*}
        A_{i,j} := & ~\exp( \underbrace{ X_{i,*} }_{1 \times d} \underbrace{ W_Q }_{d \times d} \underbrace{ W_K^\top }_{d \times d} \underbrace{ X_{j,*}^\top }_{d \times 1}).
    \end{align*}
\end{definition}

With the attention matrix, we now provide the definition for a single layer of Attention.

\begin{definition}[Single Attention Layer]\label{def:single_layer_transformer}
     Let $X \in \R^{n \times d}$ denote the representation of the length-$n$ sentence. Let $W_V \in \R^{d \times d}$ denote the model weights. As in the usual attention mechanism, the final goal is to output an $n \times d$ size matrix where $D:= \diag( A {\bf 1}_n) \in \R^{n \times n}$. Then, we define attention layer $\mathsf{Attn}$ as
    \begin{align*}
        \mathsf{Attn} (X) := & ~ D^{-1} A X W_V .
    \end{align*}
\end{definition}

Here we present the definition of the VAR Attention.

\begin{definition}[$\VAR$ Attention Layer]\label{def:single_layer_var_transformer}
    Let $r \geq 1$ be a positive integer. Let $h_r, w_r$ be two positive integers. 
     Let $\X \in \R^{h_r \times w_r \times d}$ denote the representation of the input token map. Let $W_V \in \R^{d \times d}$ denote the model weights. As in the usual attention mechanism, the final goal is to output an $n \times d$ size matrix where $D:= \diag( A {\bf 1}_n) \in \R^{h_rw_r \times h_rw_r}$. Then, we define attention layer $\mathsf{Attn}_r: \R^{h_rw_r \times d} \to \R^{h_rw_r \times d}$ as
    \begin{align*}
        \mathsf{Attn}_r (X) := & ~ D^{-1} A X W_V .
    \end{align*}
\end{definition}

We introduce the feed-forward layer in the VAR Transformer as follows.

\begin{definition}[Single Feed-Forward Layer]\label{def:ffn}
We define the FFN as follows:
    \begin{itemize}
        \item $X \in \R^{d\times L}$
        \item $k \in [n]$
        \item $c$ is the number of neurons
        \item $W^{(1)} \in \R^{c \times d}, W^{(2)} \in \R^{d \times c}$ are weight matrices
        \item $b^{(1)}\in \R^{ c}$, $ b^{(2)} \in \R^{d}$ are bias vectors.
        \item $\mathsf{FFN}: \R^{d\times L} \to \R^{d\times L}$ 
    \end{itemize}
    
    \begin{align*}
        \mathsf{FFN}(X)_{*,k} =
        & ~ \underbrace{X_{*,k}}_{d \times 1} +  \underbrace{W^{(2)}}_{d \times c} \mathsf{ReLU}( \underbrace{ W^{(1)} X_{*,k} }_{c\times 1} + \underbrace{b^{(1)}}_{c\times 1})  + \underbrace{b^{(2)}}_{d \times 1} 
    \end{align*}
    
\end{definition}

\subsection{Phase 2: Feature Map Reconstruction }\label{sec:phase_2}

In this section, we introduce the Phase Two of the VAR model.

\begin{definition}[Convolution Layer, Definition 3.9 from~\cite{kll+25} on Page 9]\label{def:conv_layer}
    The Convolution Layer is defined as follows:
    \begin{itemize}
        \item Let $h \in \mathbb{N}$ denote the height of the input and output feature map.
        \item Let $w \in \mathbb{N}$ denote the width of the input and output feature map.
        \item Let $c_{\rm in} \in \mathbb{N}$ denote the number of channels of the input feature map.
        \item Let $c_{\rm out} \in \mathbb{N}$ denote the number of channels of the output feature map.
        \item Let $X \in \R^{h \times w \times c_{\rm in}}$ denote the input feature map.
        
        \item For $l \in [c_{\rm out}]$, we use $K^l \in \R^{3 \times 3 \times c_{\rm in}}$ to denote the $l$-th convolution kernel.
        \item Let $p = 1$ denote the padding of the convolution layer.
        \item Let $s = 1$ denote the stride of the convolution kernel.
        \item Let $Y \in \R^{h \times w \times c_{\rm out}}$ denote the output feature map.
    \end{itemize}
    We use $\phi_{\rm conv}: \R^{h \times w \times c_{\rm in}} \to \R^{h \times w \times c_{\rm out}}$ to denote the convolution operation then we have $Y = \phi_{\rm conv}(X)$. Specifically, for $i \in [h], j \in [w], l \in [c_{\rm out}]$, we have
    \begin{align*}
        Y_{i,j,l} := \sum_{m=1}^3 \sum_{n=1}^3 \sum_{c = 1}^{c_{\rm in}} X_{i+m-1,j+n-1,c} \cdot K^l_{m,n,c} + b
    \end{align*}
\end{definition}

\begin{remark}
    Assumptions of kernel size, padding of the convolution layer, and stride of the convolution kernel are based on the specific implementation of \cite{tjy+24}.
\end{remark}

\subsection{Phase 3: VQ-VAE Decoder process}\label{sec:phase_3} 

In this section, we introduce Phase Three of the VAR model.

VAR will use the VQ-VAE Decoder Module to reconstruct the feature map generated in Section~\ref{sec:phase_2} into a new image. The Decoder of VQ-VAE has the following main modules \cite{kll+25}: (1) Resnet Blocks; (2) Attention Blocks; (3) Up Sample Blocks. We recommend readers to \cite{kll+25} for more details. 
