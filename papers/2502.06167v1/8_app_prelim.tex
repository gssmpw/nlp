{\bf Roadmap} In Section~\ref{sec:app_prelim}, we provide basic algebras that support our proofs. 
In Section~\ref{sec:app_var}, we provide other phases of the VAR Model. 
In Section~\ref{sec:training_of_flowar}, we give the definitions for training the FlowAR Model. 
In Section~\ref{sec:inference_of_flowar}, we give the definitions for the inference of the FlowAR Model.
In Section~\ref{sec:more_work}, we introduce more related work.

\section{Preliminary}\label{sec:app_prelim}

In this section, we introduce notations and basic facts that are used in our work. We first list some basic facts of matrix norm properties.

\subsection{Notations}
We denote the $\ell_p$ norm of a vector $x$ by $\| x \|_p$, i.e., $\|x\|_1 := \sum_{i=1}^n |x_i|$, $\| x \|_2 := (\sum_{i=1}^n x_i^2)^{1/2}$ and $\| x \|_{\infty} := \max_{i \in [n]} |x_i|$. For a vector $x \in \R^n$, $\exp(x) \in \R^n$ denotes a vector where $\exp(x)_i$ is $\exp(x_i)$ for all $i \in [n]$. For $n > k$, for any matrix $A \in \R^{n\times k}$, we denote the spectral norm of $A$ by $\| A \|$, i.e., $\| A \| := \sup_{x\in \R^k} \| Ax \|_2 / \| x \|_2$. We define the function norm as $\| f \|_\alpha :=  (\int \| f(X) \|_\alpha^\alpha \d X)^{1/\alpha}$ where $f$ is a function. We use $\sigma_{\min}(A)$ to denote the minimum singular value of $A$. Given two vectors $x, y \in \R^n$, we use $\langle x, y \rangle$ to denote $\sum_{i=1}^n x_iy_i$. Given two vectors $x, y \in \R^n$, we use $x \circ y$ to denote a vector that its $i$-th entry is $x_i y_i$ for all $i \in [n]$. We use $e_i \in \R^n$ to denote a vector where $i$-th entry is $1$, and all other entries are $0$. Let $x \in \R^n$ be a vector. We define $\diag (x) \in \R^{n \times n}$ as the diagonal matrix whose diagonal entries are given by $\diag(x)_{i, i} = x_i$ for $i = 1, \dots, n$, and all off-diagonal entries are zero. For a symmetric matrix $A \in \R^{n\times n}$, we say $A \succ 0$ (positive definite (PD)), if for all $x\in \R^n \setminus \{ {\bf 0}_n \}$, we have $x^\top A x > 0$. For a symmetric matrix $A \in \R^{n \times n}$, we say $A \succeq 0$ (positive semidefinite (PSD)), if for all $x \in \R^n$, we have $x^\top A x \geq 0$. The Taylor Series for $\exp(x)$ is $\exp(x) = \sum_{i=0}^{\infty} \frac{x^i}{i!}$. For a matrix $X \in \R^{n_1 n_2 \times d}$, we use $\X \in \R^{n_1 \times n_2 \times d}$ to denote its tensorization, and we only assume this for letters $X$ and $Y$.

\subsection{Basic Algebra}
In this section, we introduce the basic algebras used in our work.

\begin{fact}
Let $A$ denote the matrix. For each $i$, we use $A_{i,*}$ to denote the $i$-th row of $A$. For $j$, we use $A_{*,j}$ to denote the $j$-th column of $A$. 
We can show that
\begin{itemize}
    \item $\| A \| \leq \| A \|_F$
    \item $\| A \| \geq \| A_{i,*} \|_2$
    \item $\| A \| \geq \| A_{*,j} \|_2$
\end{itemize}
\end{fact}

Then, we introduce some useful inner product properties.

\begin{fact}
    For vectors $u,v,w \in \R^n$. We have 
    \begin{itemize}
        \item $\langle u, v \rangle = \langle u \circ v, {\bf 1}_n \rangle$
        \item $\langle u \circ v, w \rangle = \langle u \circ v \circ w, {\bf 1}_n \rangle$ 
        \item $\langle u, v \rangle = \langle v, u \rangle$
        \item $\langle u , v \rangle = u^\top v = v^\top u$
    \end{itemize}
\end{fact}

Now, we show more vector properties related to the hadamard products, inner products, and diagnoal matrices.
\begin{fact}
    For any vectors $u, v, w \in \R^n$, we have
    \begin{itemize}
        \item $u \circ v = v \circ u = \diag(u) \cdot v = \diag(v) \cdot u$
        \item $u^\top (v\circ w) = u^\top \diag(v) w$
        \item $u^\top (v\circ w) = v^\top (u \circ w) = w^\top (u \circ v)$
        \item $u^\top \diag(v) w = v^\top \diag(u) w = u^\top \diag(w) v$
        \item $\diag(u) \cdot \diag(v) \cdot {\bf 1}_n = \diag(u) v$
        \item $\diag(u \circ v) = \diag(u) \diag(v)$
        \item $\diag(u) + \diag(v) = \diag(u+v)$
    \end{itemize}
\end{fact}


