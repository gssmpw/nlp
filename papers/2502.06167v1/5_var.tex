\section{Universality of VAR Transformer}\label{sec:var_mainresult}

In this section, we present our proof for the universality of the VAR Transformer.
In Section~\ref{sec:universality_2}, we used a universality result from a previous work.
In Section~\ref{sec:two_layer_pertu}, we analyze how the error behaves when two consecutive layers in our composition are each replaced by their respective approximations.
In Section~\ref{sec:pertu_recursive_one}, we present the scenario when one of the composited layers got replaced by a different function.
In Section~\ref{sec:pertu_recursive_all}, we present the scenario when all of the composited layers got replaced. 
In Section~\ref{sec:var_universality}, we present our proof for the universality of the VAR Transformer.


\subsection{Universality of \texorpdfstring{$\mathcal{T}_A^{1,1,4}$}{} with \texorpdfstring{$O((1/ \epsilon)^{d 
n})$}{} FFN Layers}
\label{sec:universality_2}

We used a universality result from~\cite{hwg+24}.

\begin{lemma}[$\tau \in \mathcal{T}^{1,1,4}_A$ Transformer is Universal Seq2Seq Approximator, Theorem 2.3 in~\cite{hwg+24} on Page 11]
\label{lem:PT_uni_multi_layer_FF2}
    If the following conditions hold: 
    \begin{itemize}
        \item Let $1 \leq p < \infty$ and $\epsilon > 0$.
        \item Let a transformer with one self-attention layer defined as $\tau \in \mathcal{T}^{1,1,4}_A$
    \end{itemize}

    Then, there exists
    \begin{itemize}
        \item a transformer $\tau$ with single self-attention layer, such that for any $\mathcal{L} \in \mathcal{F}_{C}$ there exists
    $ \| \tau ( \cdot), \mathcal{L}\|_\alpha \leq \epsilon$.
    \end{itemize}
\end{lemma}


\subsection{Two Layers Perturbation}\label{sec:two_layer_pertu}

In this section, we analyze how the error behaves when two consecutive layers in our composition are each replaced by their respective approximations. Specifically, we consider the composition $f_i \circ g_i$ and replace $g_i$ with an up interpolation function $\Phi_{\mathrm{up},i}$ and $f_i$ with a one-layer transformer $\tau_i$. We show that under appropriate Lipschitz and approximation assumptions, the overall error of the approximated two-layer composition can be controlled in terms of the individual approximation errors.

\begin{assumption}[Target Function Class]\label{as:function_class}
    We assume the following things:
    \begin{itemize}
        \item Let $f_1, \ldots, f_r$ be $r$ $K$-Lipschitz functions from $\R^{h_r \times w_r \times d}$ to $\R^{h_r \times w_r \times d}$.
        \item For each $i \in [r]$, let $g_i$ be a $K$-Lipschitz function from $\R^{h_{i-1} \times w_{i-1} \times d}$ to $\R^{h_i \times w_i \times d}$.
        \item We assume that for each $i \in [r]$, $g_i$ can be approximated by some up interpolation function $\phi_{\mathrm{up},i}$.
        \item We assume that the target function $f_{\mathrm{word2img}}:\R^{1 \times 1 \times d} \to \R^{h_r \times w_r \times d}$ satisfies
    \begin{align*}
        f_{\mathrm{word2img}} := f_r \circ g_r \cdots \circ f_1 \circ g_1.
    \end{align*}
    \end{itemize}
\end{assumption}

With the Assumption~\ref{as:function_class}, we present the two layers of perturbation as follows. 

\begin{lemma}[Two Layers Perturbation]
    Let $\phi_{{\rm up},i}$ be the up interpolation function defined in~\ref{def:up_inter_layer_one_step}. Let $f_r$ be $r$ $K$-Lipschitz functions from Assumption~\ref{as:function_class}. Let $g_i$ be $r$ $K$-Lipschitz functions from Assumption~\ref{as:function_class}. Let $\tau_i$ be the one-layer transformer defined in Eq. 2.4 from~\cite{hwg+24}. If the following conditions hold:
    \begin{itemize}
        \item $\|g_i - \Phi_{\mathrm{up},i}\| \leq \epsilon_{1,i}$ from Assumption~\ref{as:function_class}.
        \item $\|f_i - \tau_i\| \leq \epsilon_{2,i}$ from Theorem~\ref{lem:PT_uni_multi_layer_FF2}.
        \item $f_i$ is $K_{1,i}$-Lipschitz.
    \end{itemize}
    Then we have
    \begin{align*}
        \|f_i \circ g_i - \tau_i \circ \Phi_{\mathrm{up},i}\| \leq K_{1,i} \epsilon_{1,i} + \epsilon_{2,i}.
    \end{align*}
\end{lemma}
\begin{proof}
    We can show that
    \begin{align*}
         \| f_i \circ g_i -  \tau_i \circ \Phi_{\mathrm{up},i} \| 
         = &~ \|f_i \circ g_i- f_i \circ \Phi_{\mathrm{up},i} + f_i \circ \Phi_{\mathrm{up},i}- \tau_i \circ \Phi_{\mathrm{up},i}\| \\
         \leq &~ \|f_i \circ g_i- f_i \circ \Phi_{\mathrm{up},i}\| + \|f_i \circ \Phi_{\mathrm{up},i}- \tau_i \circ \Phi_{\mathrm{up},i}\| \\
         = &~ \|f_i \circ (g_i- \Phi_{\mathrm{up},i})\| + \|(f_i -\tau_i) \circ \Phi_{\mathrm{up},i}\| \\
         \leq &~ \|f_i \circ (g_i- \Phi_{\mathrm{up},i})\| + \|f_i -\tau_i \| \\
         \leq &~ K_{1,i} \epsilon_{1,i} + \epsilon_{2,i}
    \end{align*}
    where the first step follows from basic algebra, the second step follows from triangle inequality, the third and fourth steps follow from basic algebra, and the fifth step follows from our conditions.
\end{proof}


\subsection{Perturbation of Recursively Composting Functions that One Layer is Different}\label{sec:pertu_recursive_one}

In this section, we consider a scenario where we have a composition of many layers, but only one of the layers is replaced by a different function. This setting helps us see how a single local perturbation can propagate through subsequent layers in a multi-layer composition. The lemma below quantifies this propagation by leveraging Lipschitz continuity.

\begin{lemma}[Perturbation of Recursively Composting Functions, One Layer is Different]\label{lem:one_layer_perturbation}
if the following conditions hold
\begin{itemize}
    \item Assume $\|  u_j(w) - v_j(w) \| \leq \epsilon $ for any $w$.
    \item $v_i(x) \leq K_2 \cdot \| x \|$
\end{itemize}
    Fix $j$, we have 
    \begin{align*}
        \| \circ_{i=j+1}^{n+1} v_i  \circ_{i=1}^j u_i - \circ_{i=j}^n v_i  \circ_{i=0}^{j-1} u_i \| \leq K_2^{n-j} \cdot \epsilon
    \end{align*}
\end{lemma}

\begin{proof}
We define $u$
\begin{align*}
    w = \circ_{i=0}^{j-1} u_i (x)
\end{align*}

    We can show that for any $x$
    \begin{align*}
        \| \circ_{i=j+1}^{n+1} v_i  \circ_{i=1}^j u_i (x) - \circ_{i=j}^n v_i  \circ_{i=0}^{j-1} u_i (x) \| 
        = & ~  \| \circ_{i=j+1}^{n+1} v_i  u_j(w) - \circ_{i=j+1}^n v_i  ( v_j(w) ) \| \\
        = & ~ \| \circ_{i=j+1}^{n+1} v_i ( u_j(w) - v_j(w)  ) \| \\
        \leq & ~ K_2^{n-j} \cdot \epsilon
    \end{align*}
    where the first step follows from basic algebra, the second step follows from linearity, and the third step follows from lemma assumptions.
\end{proof}

\subsection{Perturbation of Recursively Composting Functions that All Layer are Different}\label{sec:pertu_recursive_all}

In this section, we extend the analysis to the most general scenario in which all layers in the composition are replaced by different functions. This captures the situation where each layer $u_i$ is approximated by some other function $v_i$. We derive a cumulative bound that sums the individual perturbations introduced at each layer.

\begin{lemma}[Perturbation of Recursively Compositing Functions, All Layers are Different]
    If the following conditions hold:
    \begin{itemize}
        \item Let $\circ_{i=1}^n u_i  = u_n \circ \cdots \circ u_1$
        \item Let $\circ_{i=1}$
        \item Let $u_0(x) = x$ which is identity mapping
        \item Let $v_{n+1}(x) = x$ which is identity mapping
    \end{itemize}
    Then 
    \begin{align*}
        \| \circ_{i=1}^n u_i - \circ_{i=1}^n v_i \| \leq\sum_{j=1}^{n} \| \circ_{i=j+1}^{n+1} v_i  \circ_{i=1}^j u_i - \circ_{i=j}^n v_i  \circ_{i=0}^{j-1} u_i \|
    \end{align*}
\end{lemma}
\begin{proof}
    We can show
    \begin{align*}
        \| \circ_{i=1}^n u_i - \circ_{i=1}^n v_i \| 
        = & ~ \| \sum_{j=1}^{n} ( \circ_{i=j+1}^{n+1} v_i  \circ_{i=1}^j u_i - \circ_{i=j}^n v_i  \circ_{i=0}^{j-1} u_i ) \| \\
        \leq & ~  
        \sum_{j=1}^{n} \| \circ_{i=j+1}^{n+1} v_i  \circ_{i=1}^j u_i - \circ_{i=j}^n v_i  \circ_{i=0}^{j-1} u_i \|
    \end{align*}
    where the first step follows from adding intermediate terms, and the last step follows from the triangle inequality.
    
Thus, we complete the proof.
\end{proof}

\subsection{The Universality of VAR Transformer}\label{sec:var_universality}

In this section, with the established error bounds for replacing individual or multiple layers with alternative functions, we now prove the main universality result for the VAR Transformer. In essence, we show that a properly constructed VAR Transformer can approximate the target function $f_\mathrm{word2img}$ (from Assumption~\ref{as:function_class}) with arbitrarily small errors under suitable Lipschitz and approximation assumptions on each layer.

\begin{theorem}[Universality of VAR Transformer]\label{thm:var_universality}
    Assume $K_2 > 2$.
    For $f_\mathrm{word2img}$ satisfies Assumption~\ref{as:function_class}, there exists a VAR Transformer $\tau_{\mathsf{VAR}}$ such that
    \begin{align*}
        \|\tau_{\mathsf{VAR}} - f_\mathrm{word2img} \| \leq K_2^n( K_{1,i} \epsilon_{1,i} + \epsilon_{2,i} ).
    \end{align*}
\end{theorem}
\begin{proof}
    We can show that
    \begin{align*}
        \|\tau_{\mathsf{VAR}} - f_\mathrm{word2img} \|  = &~ \| \circ_{i=1}^r (f_i \circ g_i) - \circ_{i=1}^r (\tau_i \circ \Phi_{\mathrm{up},i}) \| \\
        = &~ \sum_{j=1}^n K_2^{n-j} ( K_{1,i} \epsilon_{1,i} + \epsilon_{2,i} )\\
        = &~ \frac{K_2^n - 1}{K_2 - 1}( K_{1,i} \epsilon_{1,i} + \epsilon_{2,i} ) \\
        \leq &~ K_2^n( K_{1,i} \epsilon_{1,i} + \epsilon_{2,i} ) .
    \end{align*}
    where the first step follows from Definition~\ref{def:var_function_class}, the second step follows from Lemma~\ref{lem:one_layer_perturbation}, the third step follows from basic algebra, and the fourth step follows from the basic inequality.  
\end{proof}
