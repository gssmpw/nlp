\section{Preliminary}\label{sec:prelim}
In this section, we introduce the fundamental definitions of our work. 
In Section~\ref{sec:notations}, we introduce all related math notations used in this paper.
In Section~\ref{sec:var_phase1}, we introduce the components in phase one of the VAR Model. 
In Section~\ref{sec:var_transformer}, we mathematically detail the VAR Transformer blocks. 

\subsection{Notations}\label{sec:notations}

We denote the $\ell_p$ norm of a vector $x$ by $\| x \|_p$, i.e., $\|x\|_1 := \sum_{i=1}^n |x_i|$, $\| x \|_2 := (\sum_{i=1}^n x_i^2)^{1/2}$ and $\| x \|_{\infty} := \max_{i \in [n]} |x_i|$. For a vector $x \in \R^n$, $\exp(x) \in \R^n$ denotes a vector where $\exp(x)_i$ is $\exp(x_i)$ for all $i \in [n]$. For $n > k$, for any matrix $A \in \R^{n\times k}$, we denote the spectral norm of $A$ by $\| A \|$, i.e., $\| A \| := \sup_{x\in \R^k} \| Ax \|_2 / \| x \|_2$. We define the function norm as $\| f \|_\alpha :=  (\int \| f(X) \|_\alpha^\alpha \d X)^{1/\alpha}$ where $f$ is a function. For a matrix $X \in \R^{n_1 n_2 \times d}$, we use $\X \in \R^{n_1 \times n_2 \times d}$ to denote its tensorization, and we only assume this for letters $X$ and $Y$.



\subsection{VAR Phase One}\label{sec:var_phase1} \label{sec:phase_1}

We first present Phase One of VAR model based on~\cite{kll+25}.

The VAR model uses the VAR Transformer to convert the initialized token map $X_{\mathrm{init}}$ into a series of pyramid-shaped token maps. 
The VAR Transformer alternates between up sample blocks and attention layers to get the output. 


\paragraph{Up Sample Blocks.} 
The $k$-th up sample block takes as input the initial token map $X_{\mathrm{init}}$ and the previous pyramid-shaped token maps $X_1, \ldots, X_k$, sets $Y_1 = X_{\mathrm{init}}$ and up samples each $X_i$ into a new token map $Y_{i+1}$, and outputs the new pyramid-shaped token maps $Y_1, \ldots, Y_{k+1}$.  

The upsampling on each token map $X_r (r \in [k])$ uses interpolation with a bicubic spline kernel.

% We first define some necessary operations used in VAR. 
\begin{definition}[Bicubic Spline Kernel, Definition 3.1 from~\cite{kll+25} on Page 7 ]\label{def:bi_spline_kernel}
    A bicubic spline kernel is a piecewise cubic function $W: \R \to \R$ that satisfies $W(x) \in [0,1]$ for all $x \in \R$.
\end{definition}

\begin{definition}[Up-interpolation Layer for One-Step Geometric Series]\label{def:up_inter_layer_one_step}
The Up-Interpolation layer is defined as follows:
\begin{itemize}
    \item Let $r \geq 2$ be an integer.
    \item Let $h_{r-1} < h_{r}$ denote two positive integers
    \item Let $w_{r-1} < w_{r}$ denote two positive integers.
    
    \item Let $d \in \mathbb{N}$ denote the number of channels.
    \item Let $\X \in \R^{h_{r-1} \times w_{r-1} \times d}$ denote the input feature map.
    \item Let $\mathsf{Y} \in \R^{h_{r} \times w_{r} \times d}$ denote the output feature map.
    \item Let $s,t \in \{-1,0,1,2\}$.
    \item Let $W: \R \to \R$ be a bicubic spline kernel as defined in~\ref{def:bi_spline_kernel}.
    
    We use $\phi_{{\rm up},r}: \R^{ h_{r-1} \times w_{r-1} \times c} \to \R^{h_{r} \times w_{r} \times c}$ to denote the up-interpolation operation then we have $\mathsf{Y} = \phi_{{\rm up}, r}(X)$. Specifically, for $i \in [h_{r}], j \in [w_{r}], l \in [c]$, we have
    \begin{align*}
        \mathsf{Y}_{i,j,l} := \sum_{s=-1}^2 \sum_{t=-1}^2 W(s) \cdot \mathsf{X}_{\frac{i \cdot h_{r-1}}{h_{r} }+s,\frac{j \cdot w_{r-1}}{w_{r}}+t,l} \cdot  W(t)
    \end{align*}
    
\end{itemize}
\end{definition}

After defining the Up-Interpolation Layer for a one-step geometric sequence, we can construct a Pyramid Up-Interpolation Layer, which applies multiple up-interpolation layers to generate token maps at different resolutions. Specifically, we can describe this Pyramid Up-Interpolation Layer through the following definition:

\begin{definition}[Pyramid Up-Interpolation Layer $\Phi_{{\rm}}$, $r=1$ Case]\label{def:pyramid_up_interpolation_layer_r1}
The Pyramid Up-Interpolation layer is defined as follows:
\begin{itemize}
    \item Let $d > 0$ denote one positive integer.
    \item  Let $\X_{\mathrm{init}} \in \R^{1\times 1 \times d}$ denote the initial token map.
\end{itemize}
We use $\Phi_{{\rm up}, 1} : \R^{ 1 \times 1 \times d} \to \R^{1 \times 1 \times d}$ such that
\begin{itemize}
    \item $\Phi_{\mathrm{up}, 1}(\X_{\mathrm{init}}) = \X_{\mathrm{init}}$.
\end{itemize}
\end{definition}

\begin{definition}[Pyramid Up-Interpolation Layer $\Phi_{{\rm}}$, $r \geq 2$ Case]\label{def:pyramid_up_interpolation_layer}
The Pyramid Up-Interpolation layer is defined as follows:
\begin{itemize}
    \item Let $d > 0$ denote one positive integer.
    \item Let $r \geq 2$.
    \item Let $\phi_{{\rm up},r}: \R^{ h_{r-1} \times w_{r-1} \times d} \to \R^{h_{r} \times w_{r} \times d}$ be defined in Definition~\ref{def:up_inter_layer_one_step}.
    \item  Let $\X_{\mathrm{init}} \in \R^{1\times 1 \times d}$ denote the initial token map.
\end{itemize}
We use $\Phi_{{\rm up}, r} : \R^{ h_{[r-1]} \times w_{[r-1]} \times d} \to \R^{h_{[r]} \times w_{[r]} \times d}$ such that
\begin{itemize}
    \item For all the $i \in [r] \setminus \{1\}$, we set $\mathsf{Y}_{i} = \phi_{{\rm up}, {i-1}} ( \X_{i-1} )$ (Here $\mathsf{Y}_i$ is the $i$-th layer of $Y$)
    \item For $i = 1$, we set $\mathsf{Y}_1 = \X_{\mathrm{init}}$
\end{itemize}
\end{definition}

\begin{figure}[!ht]\label{fig:iteration_trajectory}
    \centering
    \includegraphics[width=0.6\linewidth]{interpolation_pyramid.pdf}
    \caption{ One Pyramid Up-Interpolation Layer Instance $\Phi_{{\rm up},2}$, From Figure 1 in~\cite{kll+25}.}
    \label{fig:trajectory}
\end{figure}

\begin{remark}
    We have a pyramid-shaped token maps of size $ h_{[r+1]} \times w_{[r+1]} \times d $. To input this into the $\VAR$ Transformer, we merge the first two dimensions, transforming it into an input of size $ (\sum_{i=1}^{r+1} h_i w_i) \times d $. 
\end{remark}

Now, we are ready to introduce the VAR transformer. 
\begin{definition}[$\VAR$ Transformer]\label{def:var_transformer}
We define $\VAR$ transformer as follows:
\begin{itemize}
    \item Assume the $\VAR$ transformer has $m$ transformer layers.
    \item At the $i$-th transformer layer, let $g_i$ denote components excluding the attention layer, such as the LN layer or MLP layer.
    
    \item Let $\Phi_{\mathrm{up},r}$ denote the pyramid up-interpolation layer defined in Definition~\ref{def:pyramid_up_interpolation_layer}.
    \item  Let $\mathsf{Attn}_i$ stand for the self-attention layer, which is defined in Definition~\ref{def:single_layer_transformer}.
    \item Let $\X_{\mathrm{init}} \in \R^{1 \times 1 \times d}$ be an input token map and $X_{\mathrm{init}} \in \R^{1 \times d}$ be its matrix version.
    \item Let $n = \sum_{i=1}^m h_i w_i$.
\end{itemize}
     We define a $\VAR$ transformer as the following
\begin{align*}
    \mathsf{TF}(\X_{ \mathrm{init} }) := 
    & ~ g_m \circ \mathsf{Attn}_m \circ \Phi_{ \mathrm{up}, m} \circ \cdots \circ g_2 \circ \mathsf{Attn}_2 \circ \Phi_{ \mathrm{up}, 2} \\
    & ~ \circ g_1 \circ \mathsf{Attn}_1 \circ \Phi_{ \mathrm{up}, 1} (\X_{ \mathrm{init} }) \in \R^{n \times d},
\end{align*}
In this expression, $\circ$ stands for functional composition.
\end{definition}


\subsection{VAR Transformer Blocks}\label{sec:var_transformer}

Recall we have defined $\phi_{\mathrm{up}}: \R^{h \times w \times c} \to \R^{h' \times w' \times c}$ in Definition~\ref{def:up_inter_layer_one_step}. Since there is no non-linear operation in $\phi_{\mathrm{up}}$, $\phi_{\mathrm{up}}$ is equivalent to a matrix multiplication operation, where the dimension of the matrix is $\R^{h'w' \times hw}$. For simplicity, we view $\phi_{\mathrm{up}}$ as a $\R^{h'w' \times hw}$ dimension matrix in the following proofs. 


\begin{remark} [Applying $\phi_{\mathrm{up}}$ on $X \in \R^{n \times d}$, Remark 4.8 from~\cite{kll+25} on Page 8]
The actual input of VAR Transformer Layer are $r$ input token maps, $X_1 \in \R^{h_1 \times w_1 \times d}, \ldots, X_r \in \R^{h_r \times w_r \times d}$. We denote them as $X \in \R^{n \times d}$, where $n := \sum_{i = 1}^r h_i w_i$. We denote $\phi_{\mathrm{up}}(X) \in \R^{n' \times d}$ as applying $\phi_{\mathrm{up}}$ to each $X_i \in \R^{h_i \times w_i \times d}$ for $i \in [r]$, where $n' = \sum_{i=1}^r h_i' w_i'$. 
\end{remark}

Then, we can combine multiple attention layers with other components (up-interpolation layers, multilayer perceptron layers, layer-wise normalization layers) to create a complete VAR Transformer architecture.

\begin{definition}[Single VAR Transformer Layer, Definition 4.9 from~\cite{kll+25} on Page 9]\label{def:var_transformer_single_layer}
We define a VAR transformer block as the following.
\begin{itemize}
    \item Assume the VAR transformer has $m$ Transformer layers.
    \item Let $\mathsf{FFN}$ denotes a single Feed-forward Layer (see Definition~\ref{def:ffn}).
    \item Let $\mathsf{Attn}$ stands for a single self-attention layer (see Definition~\ref{def:single_layer_transformer}).
\end{itemize}
\begin{align*}
    \mathsf{TF}_{\mathrm{var}}(X) = \mathsf{FFN} \circ \mathsf{Attn} \circ \phi_{\rm up} \in \R^{n \times d},
\end{align*}

In this expression, $\circ$ stands for functional composition.
\end{definition}

Now, we present the VAR Transformer Network Function Class.

\begin{definition}[VAR Transformer Network Function Class]\label{def:var_function_class} We define VAR Transformer Network Function Class as follows.
    \begin{itemize}
        \item Assume the VAR transformer network has $m$ layers.
        \item for $i \in [m]$, $\mathsf{FFN}_i$ denotes the Feed-forward at $i$-th layer (see Definition~\ref{def:ffn}), $\mathsf{Attn}_i$ denotes the Attention at $i$-th layer (see Definition~\ref{def:single_layer_transformer}), and $\phi^{\mathrm{up}}_i$ denotes the Up interpolation at $i$-th layer (see Definition~\ref{def:up_inter_layer_one_step}).
        \item Let $\mathcal{T}^{a,s,c}$ denote the VAR transformer network function class
        \item each function $\tau \in \mathcal{T}^{a,s,c}$ consists of VAR transformer blocks $\mathsf{TF}_{\mathrm{var}}^m$ with $a$ heads of size $s$ and $c$ MLP hidden neurons
    \end{itemize}
    \begin{align*}
        \mathcal{T}^{a,s,c} := 
        & ~ \{ \tau: \R^{n\times d} \to \R^{n\times d} | \tau = \mathsf{TF}_{\mathrm{var}}^m \circ \mathsf{TF}_{\mathrm{var}}^{m-1} \circ \hdots \circ \mathsf{TF}_{\mathrm{var}}^1 (X)
        \} 
    \end{align*}
\end{definition}