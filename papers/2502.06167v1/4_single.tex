
\section{Any-Rank Single-Layer Attention is a Contextual Mapping Function}
\label{sec:context_map}
In this section, we show that Attention is a contextual mapping function. 
In Section~\ref{sec:cont_map}, we give the definition of contextual mapping.
In Section~\ref{sec:any_rank_attn_contmap}, we introduce any-rank single-layer attention as a contextual mapping function. 



\subsection{Contextual Mapping}\label{sec:cont_map}


{\bf Contextual Mapping.}
Let $X, Y \in \R^{n\times d}$ be the input embeddings and output label sequences, respectively. 
Let $X_{i} \in \R^{d}$ be the $i$-th token of each $X$ embedding sequence. 

\begin{definition}[Vocabulary, Definition 2.4 from~\cite{hwg+24} on Page 8]\label{def:vocab}
    We define the vocabulary.
    \begin{itemize}
        \item We define the $i$-th vocabulary set for $i \in [N]$ by $\mathcal{V}^{(i)}=\cup_{k \in[n]} X_k^{(i)} \subset \R^{d}$.
        \item We define the whole vocabulary set $\mathcal{V}$ as $\mathcal{V}=\cup_{i \in[N]} \mathcal{V}^{(i)} \subset \R^{d}$.
    \end{itemize}
\end{definition}
Note that while ``vocabulary'' typically refers to the tokens' codomain, here, it refers to the set of all tokens within a single sequence.
To facilitate our analysis, 
we introduce the idea of input token separation following~\cite{ks24,kkm22,ybr+20}.

\begin{definition}[Tokenwise Separateness, Definition 2.5 from~\cite{hwg+24} on Page 8] \label{def:token_seperate_new}
    We define the tokenwise separateness as follows.
    \begin{itemize}
        \item Let $X^{(1)}, \hdots, X^{(N)} \in \R^{n\times d}$ be embeddings. 
        \item Let $N$ be the number of sequences in the datasets.
        \item Let $n$ be the length of a sequence. i.e. $X^{(i)} \in \R^{n\times d}$
    \end{itemize}
    
    First, we state three conditions for $X^{(1)}, \hdots, X^{(N)}$ 
    \begin{itemize}
        \item [(i)] For any $i \in[N]$ and $k \in[n],\| X_k^{(i)}\|_2 > \gamma_{\min }$ holds. 
        \item [(ii)]
        For any $i \in[N]$ and $k \in[n], \|X_k^{(i)}\|_2<\gamma_{\max }$ holds.
        \item [(iii)]
        For any $i, j \in [N]$ and $k, l \in [n]$ if $X_k^{(i)} \neq X_ l^{(j)}, $ then $ \|X_k^{(i)}- X_l^{(j)}\|_2 > \delta$ holds.
    \end{itemize}
    Second, we define three types of separateness as follows, 
    \begin{itemize}
        \item {\bf Part 1.} If all conditions hold, then we call it  tokenwise $(\gamma_{\min }, \gamma_{\max }, \delta)$-separated
        \item {\bf Part 2.} If conditions (ii) and (iii) hold, then we denote this as $(\gamma, \delta)$-separateness.
        \item {\bf Part 3.} If only condition (iii) holds, then we denote it as $(\delta)$-separateness.
    \end{itemize} 
\end{definition}
To clarify condition (iii), we consider cases where there are repeated tokens between different input sequences. 
Next, we define contextual mapping. 
Contextual mapping describes a function's ability to capture the context of each input sequence as a whole and assign a unique ID to each input sequence.


\begin{definition} [$(\gamma,\delta)$-Contextual Mapping, Definition 2.6 from~\cite{hwg+24} on Page 8] \label{def:contextual_mapping_new}
A function $q:\R^{n\times d} \to \R^{n\times d}$ is said to be a $(\gamma, \delta)$-contextual mapping for a set of embeddings  $X^{(1)}, \hdots, X^{(N)} \in \R^{n\times d}$,  if the following conditions hold:
    \begin{itemize}
        \item {\bf Contextual Sensitivity $\gamma$.}
        For any $i \in[N]$ and $k \in[n],  \|q (X^{(i)})_k\|_2 < \gamma$ holds.
    
        \item {\bf Approximation Error $\delta$.}
        For any $i, j \in[N]$ and $k, l \in[n]$ such that $\mathcal{V}^{(i)} \neq \mathcal{V}^{(j)}$ or $X_k^{(i)} \neq X_l^{(j)}$, $\|q(X^{(i)})_k-q(X^{(j)})_l\|_2 > \delta$ holds.
    \end{itemize}
In addition, Note that $q (X^{(i)})$ for $i \in[N]$ is called a context ID of $X^{(i)}$.    
\end{definition}


\subsection{Any-Rank Single-Layer Attention is a Contextual Mapping Function}\label{sec:any_rank_attn_contmap}

Now we present the result showing that a softmax-based $1$-head, $1$-layer attention block with any-rank weight matrices is a contextual mapping.

\begin{lemma}[Any-Rank Attention as a $(\gamma, \delta)$-Contextual Mapping, Lemma 2.2 from~\cite{hwg+24} on Page 9]
\label{lem:contextual_map_self_attn_new}

    If the following conditions hold:
    \begin{itemize}
        \item Let $X^{(1)}, \hdots, X^{(N)} \in \R^{n \times d}$ be embeddings that are $(\gamma_{\min}, \gamma_{\max}, \epsilon)$-tokenwise separated, with the vocabulary set $\mathcal{V} = \cup_{i \in [N]} \mathcal{V}^{(i)} \subset \R^{d}$.
        \item $X_k^{(i)} \neq X_l^{(i)}$ for any $i \in [N]$ and $k, l \in [L]$.
        \item Let $
        \gamma = \gamma_{\max} + \frac{\epsilon}{4}$
        \item Let $
        \delta = \exp(-5 \epsilon^{-1} |{\cal V}|^4 d \kappa \gamma_{\max} \log L )$
        \item Let $\kappa := \gamma_{\max}/\gamma_{\min}$.
        \item Let $W^{(O)} \in \R^{d \times s}$ and $W_V, W_K, W_Q \in \R^{s \times d}$.
    \end{itemize}

    Then, we can show
    \begin{itemize}
        \item 1-layer, single-head attention mechanism serves as a $(\gamma, \delta)$-contextual mapping for the embeddings $X^{(1)}, \hdots, X^{(N)}$ with weight matrices $W^{(O)}$ and $W_V, W_K, W_Q$. 
    \end{itemize}
    
\end{lemma}


Lemma~\ref{lem:contextual_map_self_attn_new} indicates that any-rank self-attention function distinguishes input tokens $X_k^{(i)}=X_l^{(j)}$ such that $\mathcal{V}^{(i)} \neq \mathcal{V}^{(j)}$.
In other words, 
it distinguishes two identical tokens within a different context.