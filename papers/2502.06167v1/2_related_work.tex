\section{Related Work}\label{sec:related_work}

\paragraph{AutoRegressive Models.} AutoRegressive models for visual generation \cite{dyh+21,dzht22} process 2D images by converting them into 1D token sequences. Early approaches, such as PixelCNN \cite{vke+16} and PixelSNAIL \cite{cmr+18}, introduced pixel-by-pixel image generation using a raster-scan order. Later advancements \cite{rvav19,ero21,lkk+22} adapted this idea to generate image tokens following a similar raster sequence. For instance, VQ-GAN \cite{ero21} utilizes a decoder-only transformer akin to GPT-2 for generating images, while VQVAE-2 \cite{rvav19} and RQ-Transformer \cite{lkk+22} enhance the method by incorporating hierarchical scales or stacked representations. Recently, Visual AutoRegressive ($\VAR$) modeling \cite{tjy+24} proposed an innovative coarse-to-fine ``next-scale prediction'' strategy, significantly improving scalability, inference speed, and image quality, thus surpassing conventional autoregressive models and diffusion transformers.

\paragraph{Diffusion Models.} Diffusion models \cite{hja20,rbl+22} excel in generating high-resolution images by iteratively refining noise into coherent visuals. Prominent examples, such as DiT \cite{px23} and U-ViT \cite{bnx+23}, leverage probabilistic frameworks to learn data distributions effectively. Recent progress in diffusion-based image generation has focused on enhancing sampling techniques and training efficiency \cite{se19,sme20,lzb+22,hwl+24,cgl+25_high,ssz+25_dit}, advancing latent-space learning \cite{rbl+22,wsd+24,wxz+24,lzw+24}, refining model architectures \cite{hsc+22,px23,lsss24,wcz+23,xsg+24}, and exploring applications in 3D generation \cite{pjbm22,wlw+24,xlc+24,cgl+25_text}.



\paragraph{Universality of Transformers.}
The universality of transformers refers to their capacity to function as universal approximators, meaning they can theoretically model any sequence-to-sequence function with arbitrary precision. \cite{ybr+20} establish this property by demonstrating that transformers achieve universal approximation through the stacking of multiple layers of feed-forward and self-attention functions. Taking a different approach, \cite{jl23} confirms transformer universality by leveraging the Kolmogorov-Albert representation theorem. Additionally, \cite{adtk23} extend the universality results to architectures featuring non-standard attention mechanisms. More recently, \cite{ks24} shows that even a transformer with a single self-attention layer can act as a universal approximator. Of independent interest, \cite{hl24} explore the generalization and approximation properties of transformers under assumptions of HÃ¶lder smoothness and low-dimensional subspaces. \cite{lll+25_loop} showed that the looped transformer can approximate the Hypergraphs algorithm. 



