\section{Conclusion}\label{sec:conclusion}

In this paper, we established that both VAR Transformers and FlowAR architectures serve as universal approximators for Lipschitz sequence-to-sequence functionsâ€”even in their most minimal configurations. By dissecting the roles of self-attention, multi-scale up-sampling, and invertible flow transformations, we showed how these components collectively endow the models with sufficient expressive power to capture arbitrary continuous mappings. Our results unify previous theoretical findings on transformer universality with the practical enhancements brought by VAR and flow-based designs, providing a deeper theoretical underpinning for their empirical successes in high-quality image generation and structured prediction tasks. We hope that these findings inspire new explorations into more advanced architectural variants and guide future work on balancing model efficiency, interpretability, and expressive power.
