\section{Introduction}
Transformer-based architectures have reshaped the landscape of modern machine learning, demonstrating state-of-the-art performance across a wide range of tasks, including natural language processing (e.g., GPT-o3~\cite{gpto1}, Llama 3.3~\cite{llama3_arxiv,llama3_blog}, and Claude 3.5~\cite{claude3_pdf}), computer vision, and generative modeling. Their core mechanism of self-attention~\cite{vsp+17} allows for effective modeling of long-range dependencies in data, positioning transformers as a cornerstone of contemporary deep learning research. 
One particularly compelling variant is the Visual AutoRegressive (VAR) Transformer~\cite{tjy+24}, which adapts the transformer paradigm to structured image synthesis. By employing a coarse-to-fine ``next-scale prediction'' approach, VAR Transformers produce high-quality images more efficiently than many standard diffusion-based methods~\cite{sme20}. This iterative, pyramid-like generation process has demonstrated strong performance on large-scale visual tasks, indicating that multi-scale attention can capture hierarchical features in an image. Yet, despite promising empirical evidence, the theoretical underpinnings of VAR Transformers—specifically, whether they inherit the well-established universal approximation properties of classical transformers—remain an open question.

In parallel to VAR Transformers, flow-based generative methods (e.g., real-valued non-volume preserving (RealNVP) and Glow) have also garnered attention for their ability to generate high-fidelity samples in an invertible and tractable manner. Recent efforts have integrated autoregressive decompositions with flow-based designs, giving rise to Flow AutoRegressive (FlowAR) \cite{ryh+24} architectures. These models aim to blend the interpretability and stability of normalizing flows with the powerful representation learning of autoregressive transformers, potentially yielding more robust and scalable training dynamics. However, despite promising practical results, the theoretical investigation into the representational power of FlowAR remains similarly sparse.

This paper addresses two central gaps in our theoretical understanding of generative transformer architectures:
\begin{enumerate}
    \item {\bf Universality of VAR Transformers:} Although classic transformers are known to approximate arbitrary sequence-to-sequence functions~\cite{ks24,kkm22,ybr+20,hwg+24}, the additional pyramid up-sampling layers in VAR Transformers modify the input-output structure in ways that have not been theoretically dissected. We aim to rigorously determine whether VAR Transformers can still achieve universal approximation while relying on multi-scale token representations.
    \item {\bf Universality of FlowAR:} Normalizing-flow-inspired architectures equipped with autoregressive attention mechanisms promise both efficient sampling and tractable likelihood estimates. Yet, their approximation capabilities have not been formally established. Can a FlowAR model approximate arbitrary continuous transformations with any desired precision?
\end{enumerate}

In bridging these gaps, we seek to provide a unified view of how up-sampling, attention, and flow-based operations interact within transformer architectures to yield expressive function classes. Our contributions can be described as follows.
\begin{itemize}
    \item {\bf Universality of single-layer, single-head VAR Transformers (see Theorem~\ref{thm:var_universality}).} We establish that even minimal VAR Transformer designs can approximate any Lipschitz sequence-to-sequence function arbitrarily closely, extending classical universality results to the VAR setting. Our theoretical analysis demonstrates that the coarse-to-fine up-sampling process, in concert with self-attention, confers enough expressive power to realize complex transformations.
    
    \item {\bf Universality of FlowAR (see Corollary~\ref{corollary:flowar_universality}).} We further show that flow-based autoregressive transformers inherit similar approximation capabilities. In particular, we prove that FlowAR models can capture arbitrary Lipschitz transformations, illustrating how flow-based transformations and autoregressive attention can be combined without sacrificing the universality guarantees traditionally associated with transformers.
\end{itemize}

Our proofs highlight the complementary roles of self-attention, up-sampling layers, and flow-based transformations. We elucidate how these components—when arranged in a pyramid fashion (for VAR) or in an invertible mapping (for FlowAR)—enable powerful function approximation. These insights suggest a pathway for designing more flexible and efficient generative models that balance computational constraints with representational breadth.

By elucidating the theoretical foundations of both VAR Transformers and FlowAR, we advance a deeper understanding of why these models perform so effectively in practice. Our results reinforce that efficiency and expressiveness need not be at odds: even with seemingly minimal configurations (e.g., single-layer, single-head), these architectures can approximate highly complex functions. Moreover, our formal proofs set a stage for further explorations into the trade-offs between model depth, head multiplicity, and approximation efficiency, as well as studying domain-specific constraints in images, text, or other structured data formats.

\paragraph{Roadmap.} 
In Section~\ref{sec:related_work}, we survey related work on transformer universality and generative architectures. Section~\ref{sec:prelim} introduces the necessary background on VAR Transformers, focusing on up-interpolation layers and the structure of the VAR Transformer block. Section~\ref{sec:context_map} analyzes the contextual mapping properties of attention in VAR Transformers. Sections~\ref{sec:var_mainresult} present our main universality theorems, demonstrating that even simple VAR Transformers approximate Lipschitz functions with arbitrary accuracy. Section~\ref{sec:flowar_universality} provides universality results for FlowAR, a related autoregressive variant. Finally, Section~\ref{sec:conclusion} concludes with a summary of our findings and reflections on future directions. 
