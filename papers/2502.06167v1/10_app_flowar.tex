

\section{Training of FlowAR}\label{sec:training_of_flowar}

In this section, we introduce the training of FlowAR along with its definitions based on~\cite{ryh+24}.


We first introduce some notations used in FlowAR.
\subsection{Notations}\label{sub:notations}
For a matrix $X \in \R^{n_1 n_2 \times d}$, we use $\X \in \R^{n_1 \times n_2 \times d}$ to denote its tensorization, and we only assume this for letters $X, Y, Z, F, V$.

\subsection{Sample Function}
In this section, we introduce the sample functions used in FlowAR.

We first introduce the up sample function.

\begin{definition}[Up Sample Function]\label{def:up_sample_function}
    If the following conditions hold:
    \begin{itemize}
        \item Let $h, w \in \mathbb{N}$ denote the height and weight of latent $\X \in \R^{h \times w \times c}$.
        \item Let $r > 0$ denote a positive integer.
    \end{itemize}
    Then we define $\mathrm{Up}(\X,r) \in \R^{rh \times rw \times c}$ as the upsampling of latent $\X$ by a factor $r$.
\end{definition}

Then, we introduce the down sample function.
\begin{definition}[Down Sample Function]\label{def:down_sample_function}
    If the following conditions hold:
    \begin{itemize}
        \item  Let $h, w \in \mathbb{N}$ denote the height and weight of latent $\X  \in \R^{h \times w \times c}$.
        \item Let $r > 0$ denote a positive integer.
    \end{itemize}
    Then we define $\mathrm{Down}(\X ,r) \in \R^{\frac{h}{r} \times \frac{w}{r} \times c}$ as the downsampling of latent $\X $ by a factor $r$.
\end{definition}

\subsection{Linear Sample Function}
In this section, we present the linear sample function in FlowAR.

We first present the linear up sample function.
\begin{definition}[Linear Up Sample Function]\label{def:linear_up_sample_function}
    If the following conditions hold:
    \begin{itemize}
        \item Let $h, w \in \mathbb{N}$ denote the height and weight of latent $\X \in \R^{h \times w \times c}$. 
        \item Let $r > 0$ denote a positive integer.
        \item Let $\Phi_{\mathrm{up}} \in \R^{hw \times (rh \cdot rw)}$ be a matrix. 
    \end{itemize}
    Then we define the linear up sample function $\phi_{\mathrm{up}}(\cdot, \cdot)$ as it computes $\Y := \phi_{\mathrm{up}}(\X,r) \in \R^{rh \times rw \times c}$ such that the matrix version of $\X$ and $\Y$ satisfies
    \begin{align*}
        Y = \Phi_{\mathrm{up}}X \in \R^{(rh\cdot rw) \times c}.
    \end{align*}
\end{definition}

Then, we define linear down sample function as follows.
\begin{definition}[Linear Down Sample Function]\label{def:linear_down_sample_function}
    If the following conditions hold:
    \begin{itemize}
        \item Let $h, w \in \mathbb{N}$ denote the height and weight of latent $\X \in \R^{h \times w \times c}$.
        \item Let $r > 0$ denote a positive integer.
        \item Let $\Phi_{\mathrm{down}} \in \R^{((h/r) \cdot (w/r)) \times hw}$ be a matrix. 
    \end{itemize}
    Then we define the linear down sample function $\phi_{\mathrm{down}}(\X,r)$ as it computes $\Y := \phi_{\mathrm{down}}(\X,r) \in \R^{(h/r) \times (w/r) \times c}$ such that the matrix version of $\X$ and $\Y$ satisfies
    \begin{align*}
        Y = \Phi_{\mathrm{down}}X \in \R^{((h/r) \cdot (w/r)) \times c}.
    \end{align*}
\end{definition}


\subsection{VAE Tokenizer}\label{sub:vae_tokenizer}
In this section, we show the VAE Tokenizer.

\begin{definition}[VAE Tokenizer]\label{def:vae_tokenizer}
If the following conditions hold:
\begin{itemize}
    \item Let $\X \in \R^{h \times w \times c}$ denote a continuous latent representation generated by VAE.
    \item Let $K$ denote the total number of scales in FlowAR.
    \item Let $a$ be a positive integer.
    \item For $i \in [K]$, let $r_i := a^{K-i}$.
    \item For each $i\in [K]$, let $\phi_{\mathrm{down}, i}(\cdot , r_i) : \R^{h \times w \times c} \to \R^{(h / r_i) \times (w/r_i) \times c}$ denote the linear down sample function defined in Definition~\ref{def:linear_down_sample_function}.
\end{itemize}
For $i \in [K]$, we define the $i$-th token map generated by VAE Tokenizer be
\begin{align*}
    \Y^{i} := \phi_{\mathrm{down}, i}(\X, r_i) \in \R^{(h / r_i) \times (w/r_i) \times c},
\end{align*}
We define the output of VAE Tokenizer as follows:
    \begin{align*}
        \mathsf{Tokenizer}(\mathsf{X}) := \{\Y^{1},\Y^{2}, \dots,\Y^{K}\}.
    \end{align*}
\end{definition}
\begin{remark}
    In \cite{ryh+24}, they choose $a =2$ and hence for $i \in [n]$, $r_i := 2^{K-i}$.
\end{remark}

\subsection{Autoregressive Transformer}



Firstly, we give the definition of a single attention layer.
\begin{definition}[Single Attention Layer]\label{def:attn_layer}
    If the following conditions hold:
    \begin{itemize}
        \item Let $h,w \in \mathbb{N}$ denote the height and weight of latent $\X \in \R^{h \times w \times c}$.
        \item Let $W_Q, W_K, W_V \in \R^{c \times c}$ denote the weight matrix for query, key, and value, respectively.
    \end{itemize}
    Then we define the attention layer $\mathsf{Attn}(\cdot)$ as it computes  $\Y = \mathsf{Attn}(\X) \in \R^{h \times w \times c}$. For the matrix version, we first need to compute the attention matrix $A \in \R^{hw \times hw}$:
    \begin{align*}
        A_{i,j} := & ~\exp(  X_{i,*}   W_Q   W_K^\top   X_{j,*}^\top), \text{~~for~} i, j \in [hw].
    \end{align*}
    Then, we compute the output:
    \begin{align*}
        Y := D^{-1}AXW_V \in \R^{hw \times c}.
    \end{align*}
    where $D:=\diag(A {\bf 1}_n) \in \R^{hw \times hw}$.
\end{definition}


To move on, we present the definition of multilayer perceptron.
\begin{definition}[MLP layer]\label{def:mlp}
    If the following conditions hold:
    \begin{itemize}
        \item Let $h,w \in \mathbb{N}$ denote the height and weight of latent $\X \in \R^{h \times w \times c}$.
        \item Let $c$ denote the input dimension of latent $\X \in \R^{h \times w \times c}$.
        \item Let $d$ denote the dimension of the target output.
        \item Let $W \in \R^{c \times d}$ denote a weight matrix.
        \item Let $b \in \R^{1 \times d}$ denote a bias vector.
    \end{itemize}
    Then we define mlp layer as it computes $\Y := \mathsf{MLP}(X,c,d) \in \R^{h \times w \times d}$ such that the matrix version of $\X$ and $\Y$ astisfies, for each $j \in [hw]$,
    \begin{align*}
        Y_{j,:} = \underbrace{X_{j,:}}_{1\times c} \cdot \underbrace{W}_{c \times d} + \underbrace{b}_{1 \times d}
    \end{align*}
\end{definition}



We present the definition of layer-wise norm layer.
\begin{definition}[Layer-wise norm layer]\label{def:ln}
    Given a latent $\X \in \R^{h \times w \times c}$. We define the layer-wise as it computes $\Y := \mathsf{LN}(X) \in \R^{h\times w \times c}$ such that the matrix version of $\X$ and $\Y$ satisfies, for each $j \in [hw]$,
    \begin{align*}
        Y_{j,:} =  \frac{X_{j,:}-\mu_j}{\sqrt{\sigma_j^2}}
    \end{align*}
    where $\mu_j := \sum_{k=1}^c X_{j,k}/c$ and $\sigma_{j}^2 = \sum_{k=1}^c(X_{j,k}-\mu_j)^2/c$.
\end{definition}

\begin{definition}[Autoregressive Transformer]\label{def:ar_transformer}
    If the following conditions hold:
    \begin{itemize}
        \item Let $\X \in \R^{h \times w \times c}$ denote a continuous latent representation generated by VAE.
        \item Let $K$ denote the total number of scales in FlowAR.
        \item For $i \in [K]$, let $\Y_i \in \R^{(h / r_i) \times (w/r_i) \times c}$ be the $i$-th token map genereated by VAE Tokenizer defined in Definition~\ref{def:vae_tokenizer}.
        \item Let $a$ be a positive integer.
        \item For $i \in [K]$, let $r_i := a^{K-i}$.
        \item For $i \in [K-1]$, let $\phi_{\mathrm{up}, i}(\cdot, a):\R^{(h/r_i)\times(w/r_i)\times c} \to \R^{(h/r_{i+1})\times(w/r_{i+1})\times c}$ be the linear up sample function defined in Definition~\ref{def:linear_up_sample_function}.
        \item For $i \in [K]$, let $\mathsf{Attn}_i(\cdot):\R^{(\sum_{j=1}^i h/r_j)(\sum_{j=1}^i w/r_j) \times c} \to \R^{(\sum_{j=1}^i h/r_j)(\sum_{j=1}^i w/r_j) \times c}$ be the $i$-th attention layer defined in Definition~\ref{def:attn_layer}.
        \item For $i \in [K]$, let $\mathsf{FFN}_i(\cdot):\R^{(\sum_{j=1}^i h/r_j)(\sum_{j=1}^i w/r_j) \times c} \to \R^{(\sum_{j=1}^i h/r_j)(\sum_{j=1}^i w/r_j) \times c}$ be the $i$-th feed forward network defined in Definition~\ref{def:ffn}.
        \item Let $\Z_{\mathrm{init}} \in \R^{(h/r_1) \times (w/r_1) \times c}$ be the initial input denoting the class condition.
        \item Let $\Z^1 : = \mathsf{Z}_{\mathrm{init}} \in \R^{(h/r_1) \times (w/r_1) \times c}$.
        \item For $i \in [K] \setminus \{1\}$, Let $\Z^{i}$ be the reshape of the input sequence $\mathsf{Z}_{\mathrm{init}}, \phi_{\mathrm{up}, 1}(\Y^1, a), \ldots, \phi_{\mathrm{up}, i}(\Y^{i-1}, a)$ into the tensor of size $(\sum_{j=1}^i h /r_{j}) \times (\sum_{j=1}^i w /r_{j}) \times c$.
    \end{itemize}
    For $i \in [K]$, we define the Autoregressive transformer $\mathsf{TF}_i$ as 
    \begin{align*}
        \mathsf{TF}_i(\Z^i) = \mathsf{FFN}_i \circ \mathsf{Attn}_i (\Z^i) \in \R^{(\sum_{j=1}^{i} h /r_{j})(\sum_{j=1}^{i} w /r_{j}) \times c}.
    \end{align*}
    We denote $\wh{\Y}^i$ as the $i$-th block of size $(h/r_{i}) \times (w/r_{i}) \times c$ of the tensorization of $\mathsf{TF}_i(Z^i)$.
\end{definition}


\subsection{Flow Matching}
In this section, we introduce the flow matching definition.

\begin{definition}[Flow]\label{def:flow}
    If the following conditions hold:
    \begin{itemize}
        \item Let $\X \in \R^{h \times w \times c}$ denote a continuous latent representation generated by VAE.
        \item Let $K$ denote the total number of scales in FlowAR.
        \item For $i \in [K]$, let $\Y_i \in \R^{(h / r_i) \times (w/r_i) \times c}$ be the $i$-th token map genereated by VAE Tokenizer defined in Definition~\ref{def:vae_tokenizer}.
        \item For $i \in [K]$, let $\F^i_0 \in \R^{(h / r_i) \times (w/r_i) \times c}$ be a matrix where each entry is sampled from the standard Gaussian $\N(0,1)$.
    \end{itemize}
    We defined the interpolated input as follows:
    \begin{align*}
        \F^i_{t} := t \Y^i + (1-t)\F^i_0.
    \end{align*}
    The velocity flow is defined as
    \begin{align*}
        \V^i_t := \frac{\d \F^i_{t}}{\d t} = \Y^i -\F^i_0.
    \end{align*}
\end{definition}




Here, we define the architecture of the flow matching model defined in \cite{ryh+24}.
\begin{definition}[Flow Matching Architecture]\label{def:flow_matching_architecture}
    If the following conditions hold:
    \begin{itemize}

        \item Let $\X \in \R^{h \times w \times c}$ denote a continuous latent representation generated by VAE.
        \item Let $K$ denote the total number of scales in FlowAR.
        \item For $i \in [K]$, let $\Y_i \in \R^{(h / r_i) \times (w/r_i) \times c}$ be the $i$-th token map genereated by VAE Tokenizer defined in Definition~\ref{def:vae_tokenizer}.
        \item Let $i \in [K]$.
        \item Let $\wh{\Y}_i \in \R^{(h / r_i) \times (w/r_i) \times c}$ be the $i$-th block of the output of Autoregressive Transformer defined in Definition~\ref{def:ar_transformer}.
        \item Let $\F^i_t$ be the interpolated input defined in Definition~\ref{def:flow}.
        \item Let $\mathsf{Attn}_i(\cdot):\R^{(h / r_i) \times (w/r_i) \times c} \to \R^{(h / r_i) \times (w/r_i) \times c}$ be the $i$-th attention layer defined in Definition~\ref{def:attn_layer}.
        \item Let $\mathsf{MLP}_i(\cdot,c,d):\R^{(h / r_i) \times (w/r_i) \times c}  \to \R^{(h / r_i) \times (w/r_i) \times d}$ be the $i$-th attention layer defined in Definition~\ref{def:mlp}.
        \item Let $\mathsf{LN}_i(\cdot): \R^{(h / r_i) \times (w/r_i) \times c}  \to \R^{(h / r_i) \times (w/r_i) \times c}$ be the $i$-th layer-wise norm layer defined in Definition~\ref{def:ln}.
        \item Let $t_i \in [0,1]$ denote a time step.
    \end{itemize}
    Then we define the $i$-th flow matching model as $\mathsf{NN}_i(\F_t^i,\wh{\Y}_i,t_i): \R^{(h / r_i) \times (w/r_i) \times c} \times \R^{(h / r_i) \times (w/r_i) \times c} \times \R \to \R^{(h / r_i) \times (w/r_i) \times c}$. The tensor input needs to go through the following computational steps:
    \begin{itemize}
        \item {\bf Step 1:} Compute intermediate variables $\alpha_1, \alpha_2, \beta_1, \beta_2, \gamma_1, \gamma_2$. Specifically, we have
        \begin{align*}
            \alpha_1, \alpha_2, \beta_1, \beta_2, \gamma_1, \gamma_2 :=&~ \mathsf{MLP}_i(\wh{\Y}_i + t_i \cdot {\bf 1}_{(h / r_i) \times (w/r_i) \times c},c,6c)
        \end{align*}
        \item {\bf Step 2:} Compute intermediate variable $\wh{F_t}^{i'}$. Specifically, we have
        \begin{align*}
            \wh{\F_t}^{i'}:= \mathsf{Attn}_i (\gamma_1 \circ \mathsf{LN}(\F_t^i) + \beta_1) \circ \alpha_1
        \end{align*}
        where $\circ$ denotes the element-wise product for tensors.

        \item {\bf Step 3:} Compute final output $\F_t^{i''}$. Specifically, we have
        \begin{align*}
            \F_t^{i''} = \mathsf{MLP}_i(\gamma_2 \circ \mathsf{LN}(\wh{F_t}^{i'})+ \beta_2,c,c) \circ \alpha_2
        \end{align*}
        where $\circ$ denotes the element-wise product for tensors.
    \end{itemize}
\end{definition}



Then, we present our training objective.
\begin{definition}[Loss of FlowAR]
If the following conditions hold:
    \begin{itemize}
        \item Let $\X \in \R^{h \times w \times c}$ denote a continuous latent representation generated by VAE.
        \item Let $K$ denote the total number of scales in FlowAR.
        \item For $i \in [K]$, let $\Y_i \in \R^{(h / r_i) \times (w/r_i) \times c}$ be the $i$-th token map genereated by VAE Tokenizer defined in Definition~\ref{def:vae_tokenizer}.
        \item For $i \in [K]$, let $\wh{\Y}_i \in \R^{(h / r_i) \times (w/r_i) \times c}$ be the $i$-th block of the output of Autoregressive Transformer defined in Definition~\ref{def:ar_transformer}.
        \item For $i \in [K]$, let $\F^i_t$ be the interpolated input defined in Definition~\ref{def:flow}.
        \item For $i \in [K]$, let $\V^i_t$ be the velocity flow defined in Definition~\ref{def:flow}.
        \item For $i \in [K]$, let $\mathsf{NN}_i(\cdot,\cdot,\cdot):\R^{(h / r_i) \times (w/r_i) \times c} \times \R^{(h / r_i) \times (w/r_i) \times c} \times \R \to \R^{(h / r_i) \times (w/r_i) \times c}$ denote the $i$-th flow matching network defined in Definition~\ref{def:flow_matching_architecture}.
    \end{itemize}
    The loss function of FlowAR is 
    \begin{align*}
        L(\theta) = \sum_{i=1}^n \E_{t \sim \mathsf{Unif}[0,1]}\|\mathsf{NN}_i(\F^i_t, \wh{\Y}^{i}_t, t_i) - \V^i_t\|^2.
    \end{align*}
\end{definition}

\section{Inference of FlowAR}\label{sec:inference_of_flowar}
We define the architecture of FlowAR during the inference process as follows.
\begin{definition}[FlowAR Architecture in the Inference Pipeline]\label{def:flow_architecture_inference}
    If the following conditions hold:
    \begin{itemize}
        \item Let $K$ denote the total number of scales in FlowAR.
        \item Let $a$ be a positive integer.
        \item For $i \in [K]$, let $r_i := a^{K-i}$.
        \item For $i \in [K-1]$, let $\phi_{\mathrm{up}, i}(\cdot, a):\R^{(h/r_i)\times(w/r_i)\times c} \to \R^{(h/r_{i+1})\times(w/r_{i+1})\times c}$ be the linear up sample function defined in Definition~\ref{def:linear_up_sample_function}.
        \item For $i \in [K]$, let $\mathsf{Attn}_i(\cdot):\R^{(\sum_{j=1}^i h/r_j)(\sum_{j=1}^i w/r_j) \times c} \to \R^{(\sum_{j=1}^i h/r_j)(\sum_{j=1}^i w/r_j) \times c}$ be the $i$-th attention layer defined in Definition~\ref{def:attn_layer}.
        \item For $i \in [K]$, let $\mathsf{FFN}_i(\cdot):\R^{(\sum_{j=1}^i h/r_j)(\sum_{j=1}^i w/r_j) \times c} \to \R^{(\sum_{j=1}^i h/r_j)(\sum_{j=1}^i w/r_j) \times c}$ be the $i$-th feed forward network defined in Definition~\ref{def:ffn}.
        \item For $i \in [K]$, let $\mathsf{NN}_i(\cdot,\cdot,\cdot):\R^{(h / r_i) \times (w/r_i) \times c} \times \R^{(h / r_i) \times (w/r_i) \times c} \times \R \to \R^{(h / r_i) \times (w/r_i) \times c}$ denote the $i$-th flow matching network defined in Definition~\ref{def:flow_matching_architecture}.
        \item For $i \in [K]$, let $t_i \in [0,1]$ denote the time steps.
        \item For $i \in [K]$, let $\F^i_t$ be the interpolated input defined in Definition~\ref{def:flow}.
        \item Let $\Z_{\mathrm{init}} \in \R^{(h/r_1) \times (w/r_1) \times c}$ denote the initial input denoting the class condition.
        \item Let $\Z^1:=\Z_{\mathrm{init}} \in \R^{(h/r_1)\times(w/r_1) \times c}$.
    \end{itemize}
    Then, we define the architecture of FlowAR in the inference pipeline as follows:
    \begin{itemize}
        \item Layer 1: Given the initial token $\Z^1$, we compute
        \begin{align*}
            &~ s_1 = \mathsf{FFN}_1 \circ \mathsf{Attn}_1 (\Z^1) \in \R^{(h/r_1) \times (w/r_1) \times c}\\
            &~ \wh{s}_1 = \mathsf{NN}_1(F_t^1,s_1,t_1)
        \end{align*}
        \item Layer 2: Given the initial token $\Z^1$ and output of the first layer $\wh{s}_1$. Let $\Z^2$ be the reshape of the input sequence $\Z_{\mathrm{init}}, \phi_{\mathrm{up},1}(\wh{s}_1,a)$ into the tensor of size $(\sum_{i=1}^2 h/r_i) \times (\sum_{i=1}^2 w/r_i) \times c$. Then we compute
        \begin{align*}
            &~ s_2 = \mathsf{FFN}_2 \circ \mathsf{Attn}_2 (\Z^2)_{h/r_1:\sum_{i=1}^2 h/r_i,w/r_1 :\sum_{i=1}^2 w/r_i,0:c}\\
            &~ \wh{s}_2 = \mathsf{NN}_2(F_t^2,s_2,t_2)
        \end{align*}
        \item  Layer  $i \in [K]\setminus \{1,2\}$: Given the initial token $\Z^1$ and the output of the first $i-1$ layer $\wh{s}_1,\dots,\wh{s}_{i-1}$. Let $\Z^i$ be the reshape of the input sequence $\Z_{\mathrm{init}}, \phi_{\mathrm{up},1}(\wh{s}_1), \dots, \phi_{\mathrm{up},i-1}(\wh{s}_{i-1})$ into the tensor of size $(\sum_{j=1}^i h/r_j) \times (\sum_{j=1}^i w/r_j) \times c$. Then we compute
        \begin{align*}
            &~ s_i = \mathsf{FFN}_i \circ \mathsf{Attn}_i (\Z^i)_{\sum_{j=1}^{i-1} h/r_j : \sum_{j=1}^i h/r_j , \sum_{j=1}^{i-1} w/r_j : \sum_{j=1}^i w/r_j,0:c}\\
            &~ \wh{s}_i = \mathsf{NN}_i(F_t^i,s_i,t_i)
        \end{align*}
        Then the final output of FlowAR is $\wh{s}_K$.
    \end{itemize}
\end{definition}

\section{More Related Work}\label{sec:more_work}
In this section, we introduce more related work. 

\paragraph{Theoretical Machine Learning.}
Our work also takes inspiration from the following Machine Learning Theory work. Some works analyze the expressiveness of a neural network using the theory of circuit complexity~\cite{lls+25_gnn,kll+25_var_tc0,lls+24_rope_tensor_tc0,cll+24_mamba,cll+24_rope}. Some works optimize the algorithms that can accelerate the training of a neural network~\cite{llsz24,klsz24,dlms24,dswy22_coreset,haochen3,haochen4,dms23_spar,cll+25_deskreject,sy23,swyy23,lss+22,lsx+22,hst+22,hsw+22,hst+20,bsy23,dsy23,syyz23_weighted,gsy23_coin,gsy23_hyper,gsyz23,gswy23,syzz24,lsw+24,lsxy24,hsk+24,hlsl24}. Some works analyze neural networks via regressions~\cite{cll+24_icl,gms23,lsz23_exp,gsx23,ssz23_tradeoff,css+23,syyz23_ellinf,syz23,swy23,syz23_quantum,lls+25_grok}. Some works use reinforcement learning to optimize the neural networks~\cite{haochen1,haochen2,yunfan1,yunfan2,yunfan3,yunfan4,lswy23}. Some works optimize the attention mechanisms~\cite{sxy23,lls+24_conv}.


\paragraph{Accelerating Attention Mechanisms.}
The attention mechanism, with its quadratic computational complexity concerning context length, encounters increasing challenges as sequence lengths grow in modern large language models~\cite{gpto1,llama3_blog,claude3_pdf}. To address this limitation, polynomial kernel approximation methods \citep{aa22} have been introduced, leveraging low-rank approximations to efficiently approximate the attention matrix. These methods significantly enhance computation speed, allowing a single attention layer to perform both training and inference with nearly linear time complexity \citep{as23, as24b}. Moreover, these techniques can be extended to advanced attention mechanisms, such as tensor attention, while retaining almost linear time complexity for both training and inference \cite{as24_iclr}.~\cite{kll+25} provides an almost linear time algorithm to accelerate the inference of VAR Transformer. Other innovations include RoPE-based attention mechanisms~\cite{as24_rope,chl+24_rope} and differentially private cross-attention approaches~\cite{lssz24_dp}. Alternative strategies, such as the conv-basis method proposed in \cite{lls+24_conv}, present additional opportunities to accelerate attention computations, offering complementary solutions to this critical bottleneck. Additionally, various studies explore pruning-based methods to expedite attention mechanisms \cite{lls+24_prune,cls+24,llss24_sparse,ssz+25_prune,ssz+25_dit,hyw+23,whl+24,xhh+24,ssz+25_prune}.


\paragraph{Gradient Approximation.}
The low-rank approximation is a widely utilized approach for optimizing transformer training by reducing computational complexity \cite{lss+24,lssz24_tat,as24b,hwsl24,cls+24,lss+24_grad}. Building on the low-rank framework introduced in \cite{as23}, which initially focused on forward attention computation, \cite{as24b} extends this method to approximate attention gradients, effectively lowering the computational cost of gradient calculations. The study in \cite{lss+24} further expands this low-rank gradient approximation to multi-layer transformers, showing that backward computations in such architectures can achieve nearly linear time complexity. Additionally, \cite{lssz24_tat} generalizes the approach of \cite{as24b} to tensor-based attention models, utilizing forward computation results from \cite{as24_iclr} to enable efficient training of tensorized attention mechanisms. Lastly, \cite{hwsl24} applies low-rank approximation techniques during the training of Diffusion Transformers (DiTs), demonstrating the adaptability of these methods across various transformer-based architectures.
