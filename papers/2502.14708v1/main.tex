%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%% For EC %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

% \documentclass[format=acmsmall, review=false]{acmart}
% \usepackage{acm-ec-25}
% \usepackage{booktabs} % For formal tables
% \usepackage[ruled]{algorithm2e} % For algorithms
% \renewcommand{\algorithmcfname}{ALGORITHM}
% \SetAlFnt{\small}
% \SetAlCapFnt{\small}
% \SetAlCapNameFnt{\small}
% \SetAlCapHSkip{0pt}
% \IncMargin{-\parindent}

% % Choose a citation style by commenting/uncommenting the appropriate line:
% %\setcitestyle{acmnumeric}
% \setcitestyle{authoryear}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% Comment out these for EC %%%%%%%%%%%%%%%
\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{booktabs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{caption}
\usepackage{graphicx}
\usepackage[normalem]{ulem}
\usepackage{amsmath, amssymb}
\usepackage{amsthm}
\usepackage{wasysym}
\usepackage{tablefootnote}
\usepackage[labelformat=simple]{subcaption}
\renewcommand\thesubfigure{(\alph{subfigure})}
\usepackage{paralist}
\usepackage{multirow}
\usepackage{makecell}
% \usepackage{subcaption}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% Comment out these for EC %%%%%%%%%%%%%%%
\usepackage[round,longnamesfirst]{natbib}
\usepackage[hyphens]{url} 
\usepackage[
  bookmarks=true,
  bookmarksnumbered=true,
  bookmarksopen=true,
  pdfborder={0 0 0},
  breaklinks=true,
  colorlinks=true,
  linkcolor=black,
  citecolor=blue,
  filecolor=black,
  urlcolor=blue,
]{hyperref}

\usepackage{setspace}
\onehalfspacing
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[capitalize]{cleveref}



\hypersetup{pdftitle={Human Misperception of Generative-AI Alignment: A
Laboratory Experiment},pdfborderstyle=,pdfpagelayout=OneColumn,pdfnewwindow=true,pdfstartview=XYZ,plainpages=false,urlcolor=[rgb]{0.0430,0,0.5},linkcolor=[rgb]{0.0430,0,0.5},citecolor=[rgb]{0.0430,0,0.5},hypertexnames=false}


\Crefname{figure}{Figure}{Figures}

% define prompt environment
\usepackage{letltxmacro}
\usepackage{mdframed}
\usepackage{xcolor}
\LetLtxMacro\origttfamily\ttfamily
\DeclareRobustCommand*{\ttfamily}{%
  \origttfamily
  \hyphenchar\font=`\-\relax
  \fontdimen3\font=.25em\relax
  \fontdimen4\font=.167em\relax
  \fontdimen7\font=.167em\relax
}

\newmdenv[
  backgroundcolor=lightgray,
  font=\ttfamily\footnotesize,
]{prompt}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\definecolor{darkblue}{rgb}{0.0, 0.0, 0.55}
\usepackage{siunitx}

\theoremstyle{plain}
\newtheorem{prop}{\protect\propositionname}

\makeatother
\usepackage{pdflscape}
\usepackage{ulem}
\usepackage{adjustbox}

\usepackage{babel}
\providecommand{\propositionname}{Proposition}
\newtheorem{example}{Example}

\title{Human Misperception of Generative-AI Alignment:\\A Laboratory Experiment}
% \thanks{Krishna?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% Comment out these for EC %%%%%%%%%%%%%%%
\author{ \large Kevin He\thanks{University of Pennsylvania. Email: hesichao@gmail.com.} \ \ \ \  \ \ \ \  %
Ran Shorrer\thanks{The Pennsylvania State University. Email: rshorrer@gmail.com. Shorrer gratefully acknowledges support (for other projects) in the form of API credits from Anthropic, Google, and OpenAI.} \ \ \  \ \ \ \ \ %
Mengjia Xia\thanks{ University of Pennsylvania. Email: xiax@sas.upenn.edu.}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%% For EC %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% % Anonymized submission.
% \author{Submission 1151}


% % \date{November 27, 2024}

% \begin{abstract}
% We conduct an incentivized laboratory experiment to study people's perception of generative artificial intelligence (GenAI) alignment in the context of economic decision-making. Using a panel of economics problems spanning the domains of risk, time preference, social preference, and strategic interactions, we ask human subjects to make choices for themselves and to predict the choices made by GenAI on behalf of a human user. We find that people overestimate the degree of alignment between GenAI's choices and human choices. In every problem, the subjects’ average prediction about GenAI's choice is substantially closer to the average human-subject choice than it is to the GenAI choice. At the individual level, different subjects' predictions about GenAI’s choice in a given problem are highly correlated with their own choices in the same problem. We explore the implications of people overestimating GenAI alignment in a simple theoretical model.
% \end{abstract}

% \begin{document}

% % Title page for title and abstract only.
% \begin{titlepage}

% \maketitle
% \vspace{1cm}
% {\centering\textsc{Disclosure}

% }
% \vspace{1em}

% \noindent Other reseach projects by one of the authors are supported (in the form of API credits) by Anthropic, Google, and OpenAI.
% % % Optionally include a table of contents
%  \vspace{1cm}
%  \setcounter{tocdepth}{1} % adjust to 1 if desired
%  \tableofcontents

% \end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% Comment out these for EC %%%%%%%%%%%%%%%
\begin{document}

\maketitle

\begin{abstract}
We conduct an incentivized laboratory experiment to study people's perception of generative artificial intelligence (GenAI) alignment in the context of economic decision-making. Using a panel of economic problems spanning the domains of risk, time preference, social preference, and strategic interactions, we ask human subjects to make choices for themselves and to predict the choices made by GenAI on behalf of a human user. We find that people overestimate the degree of alignment between GenAI's choices and human choices. In every problem, human subjects’ average prediction about GenAI's choice is substantially closer to the average human-subject choice than it is to the GenAI choice. At the individual level, different subjects' predictions about GenAI’s choice in a given problem are highly correlated with their own choices in the same problem. We explore the implications of people overestimating GenAI alignment in a simple theoretical model.
\end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Individuals and organizations are increasingly using generative artificial intelligence (GenAI) to help with their economic decisions.\footnote{For example, SmartSimple Cloud  is a grant management software that helps grantmakers allocate funds across different philanthropic initiatives by summarizing and grading  grant applications \citep{SmartSimple_blog}. Fintech companies like Wealthfront and Betterment use AI for investment advisory, while academic researchers have demonstrated how large language models can be used to analyze market data and construct stock portfolios \citep{ko2024can, pelster2024can}.} This trend is accelerated by the rise of AI agents that can interact with the external environment and autonomously take actions on behalf of the user \citep{OpenAI_2025}, making it possible to even fully delegate economic decisions to GenAI. 

Unlike classification and prediction tasks, where machine-learning methods and AI systems have been traditionally   deployed,  economic decisions often do not have an objectively ``correct'' answer that applies to everyone. Instead, these economic problems confront agents with trade-offs (e.g., higher payoff vs. earlier payoff, efficiency vs. equity, riskier but potentially higher rewards vs. safer but lower rewards) and the optimal choices depend on the agent's preferences. To fully realize the potential gains from delegating  economic decisions to GenAI, people must hold correct beliefs about how this technology behaves when  instructed to act on their behalf. If people correctly anticipate GenAI's behavior, then judicious delegation of the appropriate decision problems to GenAI can save time and effort. But if people misperceive the degree of alignment between the GenAI choices and the user's preferences, they may make suboptimal delegation decisions and even end up worse off than without access to GenAI.   


This paper experimentally investigates the hypotheses that people overestimate the degree to which GenAI choices are aligned with human preferences in general (\emph{anthropomorphic projection}), and with their personal  preferences in particular  (\emph{self projection}).\footnote{The hypotheses and our main analyses were pre-registered. The  pre-registration can be found on
the registry website at \href{https://aspredicted.org/yd32-r96n.pdf}{https://aspredicted.org/yd32-r96n.pdf}.} To this end, we conduct an incentivized laboratory experiment where we focus on understanding people's beliefs about GenAI's choices.\footnote{
A related growing literature focuses on studying the responses produced by large language models (LLMs) instead of people's beliefs about these models. Some of the work in this area considers the possibility of using LLMs to simulate human subjects \citep[e.g.,][]{horton_large_2023,manning_automated_2024,GABE}, while others study LLMs as economic agents in order to understand how they behave in markets \citep[e.g.,][]{fish2024algorithmic,EconEvals}.} The experiment consists of two parts. In the first part, subjects are asked to make choices in an array of incentivized decision environments spanning the domains of risk, time preferences, social preferences, and strategic interactions.\footnote{\citet{Yariv} study most of these decision environments and compare behavior across different human subject pools. Our subjects' choices are in line with their findings.} In the second part, subjects are asked to predict the choices an AI chatbot would make when instructed to choose on behalf of a human user in the same decision environments. Subjects receive a bonus  if their prediction is sufficiently close to the average choice made by the large language model (LLM) GPT-4o.   

We find evidence of both anthropomorphic projection and self projection.  First, on average, human subjects' predictions about GenAI's choices in every decision environment are much closer to the average human-subject choice than to the average GenAI choice. Second, at the individual level, human subjects' predictions about GenAI's choices in a given environment are highly correlated with their own choices in the same environment. Additionally, consistent with subjects self-projecting preference parameters and not just specific choices onto the GenAI model, we find that a subject’s expectation of how GenAI chooses for a human user in a given problem can be predicted from the subject’s choices in related problems.


We explore the implications of anthropomorphic projection and self projection in a stylized theoretical model. Our theoretical analysis shows that anthropomorphic projection and self projection can lead to over-delegation to GenAI. More subtly, we also find that objectively improving AI alignment can harm agents who exhibit anthropomorphic projection (because they mistakenly adjust their delegation decisions in a detrimental fashion). Similarly, among agents who exhibit self projection, welfare may be higher for those who have more unusual preferences (since they are less likely to mistakenly delegate). These results contribute  to the literature on AI alignment \citep[e.g.,][]{gabriel2020artificial,hosseini2025} by analyzing the implications of \emph{misperceptions} of alignment when people selectively delegate to GenAI. 


% \citet{liang2022algorithmic} show hat 
% If members of different groups presents with different degrees of self projection, then the 
% We also find that different demographic present with different levels of self projection. Specifically, female subjects' predictions are substanially more correlated with their own choices when compared with males.   



Anthropomorphic projection and self projection may result from several causes.  First, humans may believe that GenAI models are designed to behave like humans, and a large literature documents that people project their current tastes and knowledge onto other people when they forecast others’ behavior and studies some implications of this bias \citep{danz2018biases,kaufmann2022projection,bushong2024failures,gagnon2024quality}. Furthermore, one may expect excessive projection even when people interact with personalized GenAI models, as the literature  shows that people also project their current tastes (which may be influenced by contextual information that the GenAI cannot observe or interpret) onto their future or past selves \citep[e.g.,][]{loewenstein2003projection,conlin2007projection}. Second, predicting the choices of GenAI in a specific environment is difficult, especially for individuals with less experience with GenAI products, and this may lead people to rely on a simple cognitive default \citep{woodford2020modeling}.
To assess the possibility that experience mitigates self projection,  we collect information on subjects' exposure to GenAI and conduct heterogeneity analysis. We find no evidence that the extent of self projection varies substantially by past experience with GenAI. Additionally, we find no evidence that experienced subjects make more accurate predictions about GenAI's choices.


% We find that \textbf{the average is the same. Furthermore, we find no evidence that the correlation with own choices is different, and in any case it does not account for much of the correlation we document(?)}.   

Our paper is closely related to studies that consider humans' belief formation about AI ability and their decision to delegate to AI.  
\citet{vafa_large_2024} and \citet{dreyfuss2024human} 
provide evidence that humans make anthropomorphic generalizations about LLM behavior in questions that involve factual answers. \citet{vafa_large_2024} show that when asked to guess how an agent will perform in one task based on the agent's performance in another task, human subjects do well when the agent is human, but they perform poorly when the agent is an LLM. \citet{dreyfuss2024human} show that human subjects project onto the LLM a notion of human difficulty and capability, even though it does not apply to the LLM. \citet{dell2023navigating}  coin the term ``jagged technological frontier'' to describe how GPT-4 performs well in some tasks but poorly in other seemingly similar tasks. They show that giving professional management consultants access to GPT-4 can be detrimental to their performance when the task is on the wrong side of the technological frontier.\footnote{There are ample evidences that LLMs can augment performance in a variety of tasks \citep[e.g.,][]{brynjolfsson2025generative,noy2023experimental}. } 
\citet{Noti} design an AI system that provides advice only when it is likely to be beneficial for the user and show that it can improve human decision-making relative to a design that always provides advice. 
We contribute to this literature by considering economic decision environments where agents' optimal choices vary based on their preference parameters. This setting lets us document a novel, distinct phenomenon: self projection. 



More broadly, our findings contribute to several strands of academic research. First, they contribute  the vast literature on mental models in decision-making.\footnote{Examples include \citet{mullainathan2008coarse,bordalo2012salience,hanna2014learning, bordalo2016stereotypes,enke2019correlation,enke2020you,imas2022impact,esponda2024mental,kendall2024complexity}; and \citet{rts}.} Second, they contribute to the growing literature on the interaction of algorithms with society.\footnote{Examples include  \citet{calvano2020protecting,calvano_artificial_2020,rambachan2020economic,aquilina2022quantifying,banchio2022artificial}; and \citet{liang2022algorithmic}. } Finally, they contribute to the Human+AI literature.\footnote{Examples include \citet{kleinberg2018human,green2019disparate},  \citet{raghu2019algorithmic}, and \citet{immorlica2024generative}.  }


% and by showing that selective delegation can overturn naive measures of AI-alignment.  



% Theory: 1) over delegation, 2) anthropomorphic projection --> more aligned AI can hurt humans (keeping the same level of bias -- ex. 1) 3) under self-projection it may be beneficial to have weird preferences (that the AI is less aligned with). 

% \section{abstract drafts}
% Understanding AI alignment is becoming increasingly critical as organizations and individuals deploy large language models (LLMs) and other AI algorithms across a wide array of contexts and environments. Alignment is particularly significant in economic decision-making; unlike many traditional problems in machine learning, economic decisions often lack a definitive "correct" answer and instead present trade-offs (e.g., efficiency vs. equality, risk vs. reward) where human agents vary in their preferred solutions.

% Through a laboratory experiment involving human and LLM subjects, we demonstrate that humans tend to anthropomorphize LLMs. On average, human subjects erroneously expect LLMs to make choices similar to those of the average human. Furthermore, each individual's predictions are highly correlated with their own choices in the same problem.

% Our findings underscore the importance of \emph{beliefs} about alignment for the realized alignment. A simplistic measure of alignment quantifies the coherence between the choices made by the LLM and the agent's preferences. However, in practice, agents select tasks for LLM deployment based on their beliefs about the LLM's alignment. Imprecise beliefs—such as those we document—can reverse the ranking of LLMs' alignment.

 




% \subsection{Kevin's attempt at an abstract}

% We conduct an incentivized laboratory experiment to study people's perception of generative artificial intelligence (GenAI) alignment in the context of economic decision-making. Using a panel of economics problems spanning the domains of risk, time preference, social preference, and strategic interactions, we ask human subjects to make choices for themselves and to predict the choices made by GenAI on behalf of a human user. We find that people overestimate the degree of alignment between GenAI's choices and human choices. In every problem, the subjects’ average prediction about GenAI's choice is substantially closer to the average human-subject choice than it is to the GenAI choice. At the individual level, different subjects' predictions about GenAI’s choice in a given problem are highly correlated with their own choices in the same problem. We explore the implications of people overestimating GenAI alignment in a simple theoretical model.

% \section{Introduction}
% \textbf{maybe: AI has the potential to help ppl in difficult tasks}
% The degree to which AI enhances human decision making depends not only on the abilities of the AI, but also on the subset of tasks where humans choose to deploy it. Correctly predicting the quality of AI performance can lead to a positive outcome. But inaccurate perceptions about AI-performance may lead to worse performance. For example, "Dell’Acqua et al. (2023) find that giving highly trained consultants access to GPT-4 results in a net productivity loss when they misjudge whether a task is outside AI’s competence; Agarwal et al. (2023) show that providing radiologists with a high-quality AI predictions does not increase diagnostic accuracy, as physicians misweight AI signals."

%  Forming accurate beliefs about the preformance of Large language models (LLMs) and other generative AI tools is especially challenging. LLMs have a wide diversity of uses and they can be used across contexts and in varying environments. For example, they are used to \textbf{examples with deterministic answer}. This diversity of uses, faces users with a challenge of evaluating LLM abilities in varying tasks in circumstances: LLMs may be better at some and then they are in others, and humans may struggle to understand what makes a problem hard for the LLM \textbf{DR} and to generalize from one context to another \textbf{vafa}.   


% Individuals and firms are increasingly delegating economic decisions to LLMs. 
% Unlike ``traditional" classification and predenction tasks, economic decisions often do not have a ``correct" answer. Instead, they introduce trade offs  (high payoff vs. earlier payoff, efficiency vs. equity, risk vs reward). This further complicates the prediction task: a user delegating to the AI needs to ask herself how well the AI is aligen with the users own tastes in the particular decision scenario. \textbf{something like: this compound the prediction challenge above, because it also adds the challenge for the AI to figure out the user's own preferences}




% \begin{itemize}


%     \item LLMs have a wide diversity of uses. They can be used across contexts and in varying environments. 
%     \item Individuals and companies increasingly delegate economic decisions to LLMs. 
%     \item Unlike answering questions or even standard prediction tasks, economic decisions don't have a ``correct" answer. Rather, they often introduce trade offs  (high payoff vs earlier payoff, efficiency vs equity, risk vs rewawrd). 
%     \item since agents slectively delegate to the LLM, their perception of LLM behavior will influence which tasks they delegate. To determine which LLMs enhance humans, we need to think not only about the LLM capabilities -- we need to compoud this on the human selection/delegation function. This creates a link between allignment and explainability/transparancy.   
% \end{itemize}

% Vafa et al:  "Crucially, in many instances, the decisions about where an
% LLM will be deployed are made by people. These decisions
% are often driven by where they believe a model will perform
% well (Lubars \& Tan, 2019; Lai et al., 2022). Assessing the
% real-world performance of LLMs therefore requires understanding how people form beliefs about their capabilities."


% Agarwal "Full automation using Artificial Intelligence (AI) predictions may not be optimal if humans can access contextual information. We study human-AI collaboration using an information experiment with professional radiologists. Results show that providing (i) AI predictions does not always improve performance, whereas (ii) contextual information does. Radiologists do not realize the gains from AI assistance because of errors in belief updating – they underweight AI predictions and treat their own information and AI predictions as statistically independent. Unless these mistakes can be corrected, the optimal human-AI collaboration design delegates cases either to humans or to AI, but rarely to AI assisted humans."

% \paragraph{Potential Example.} Each LLM has a distribution of outcomes in each on $N$ economic tasks. The (single) agent saves some effort cost $c>0$ for each task she delegates, but then gets the utility from the action chosen by the LLM agent (instead of the optimal action? Later we can introduce the idea that the agent may be biased and the AI debiases her). 
% Naive assessment: the most aligent LLM is the best. 
% Sophisticated assessment: The person will delegate only tasks where she thinks the LLM is sufficiently aligned (more than by $c$). 

% Now consider one LLM that is a little worse, but the agent thinks is more aligned. After we control for delegation, it will actually be better for the agent.  Can probably think of other stuff like that. 

% \section{Experimental Design and Deployment}

% In this Section we summarize the experimental protocol. All experimental materials are available in the Supplemental Materials Appendix.

% The experiment was advertised as requiring agents to make incentivized choices and predictions. The experiment began with a brief informed consent. Subjects that consented were told that the expeteriment consists in two part and that the first part involves making choices in various settings. 

% \paragraph{Part 1: Incentivized Choices.}

\section{Theoretical Implications of Anthropomorphic Projection and Self Projection}
\label{sec:theory}

In this section, we present a stylized theoretical model of anthropomorphic projection and self projection and show that these misperceptions can imply some unexpected comparative statics for GenAI users' welfare. This model also serves as the conceptual framework for guiding our empirical analysis of the experimental data. 


\subsection{A Model of Delegation under Misperceived Alignment}

Nature draws a decision problem $\omega\sim\mathcal{N}(0,\sigma_{\omega}^{2})$, which is not observed by the agent.
The agent observes their type $\theta\sim\mathcal{N}(0,\sigma_{\theta}^{2})$
and an attention cost $c>0$ , where $c$ is drawn from a strictly
positive density on $\mathbb{R}_{+}$ (and is independent of $\theta$ and  $\omega$). An action  $a \in \mathbb{R}$ must be taken and the agent with type $\theta$ gets decision utility $-(a-\omega-\theta)^{2}$ from action $a$ in decision problem $\omega$. 

The agent first chooses whether to costlessly
delegate their action to the GenAI. When the decision problem is $\omega$
and the agent  delegates, the GenAI will take the action  $\omega+b(\omega)$
on behalf of the agent (regardless of the agent's actual type $\theta$).
If the agent does not delegate, then they must choose an action themselves. Before doing so, they have the chance to  pay the attention cost $c$ and perfectly
learn the realization of $\omega$. If the agent does not delegate and does not pay the attention cost, then they must choose an action knowing only their type $\theta$. 


The agent is fully rational except for potentially misperceiving the GenAI's action.
In particular, a type $\theta$ agent believes that the GenAI
will take the action $\omega+rb(\omega)+\rho\theta$ in decision problem
$\omega,$ where $r\in[0,1]$ and $\rho\in[0,1].$ The agent maximizes expected total utility (i.e., decision utility minus any attention cost) given these beliefs. 

\subsection{Interpretation of the Model}


We interpret $\omega$ to capture the specific details of a decision
to be made, such as the rate of return on a risky investment or the
social benefit of a generous act. The agent's ideal action depends on both the decision problem $\omega$ and their type $\theta$, which
refers to a  personal trait such as risk attitude or social-preference
parameter. We assume that the agent knows their type and the distribution
of decision problems, but must pay a cost $c>0$ to understand the
details of the particular problem that they are currently facing.

The average ideal action within the population of agents for decision
problem $\omega$ is $\omega.$ We interpret the term $b(\omega)$
to be the bias of the GenAI relative to the humans for decision problem
$\omega$. We are agnostic about the source of such bias (for instance, biased training sample or issues with the model-training procedure) and allow the amount of bias to depend on the decision problem in an arbitrary way. 



The model accommodates both  anthropomorphic projection and  self projection. The parameter
$r$ relates to  anthropomorphic projection, where agents on average wrongly predict the GenAI action in problem $\omega$
to be $r\cdot(\omega+b(\omega))+(1-r)\cdot(\omega)=\omega+r\cdot b(\omega).$ Thus anthropomorphic projection becomes more severe as $r$ decreases, with people's  predictions of GenAI's action becoming more centered around the typical ideal human action and further away from  the actual GenAI action in each decision problem. The parameter $\rho$ models the extent of self projection, where agents partially project their
individual type realizations onto the GenAI. Correctly specified beliefs correspond to $r=1,$
$\rho=0$.

In practice, delegation to GenAI may lead to a partially personalized action that depends on the delegator's type. This may be because people choose to use one of several available GenAI models depending on their personal type realizations, or because the GenAI model has access to the agent's personal information and tailors its choice based on this information. We can view $\theta$ as the remaining idiosyncratic preference or contextual information that is orthogonal to the GenAI personalization. 



For the sake of clarity of results, we will separately consider the
effects of anthropomorphic projection and self projection on agent's delegation behavior and welfare.

\subsection{Implications of Anthropomorphic Projection}

Suppose $\sigma_{\theta}^{2}=0$, so there is no individual-level
variance in optimal actions. We show that projection bias causes over
delegation to the GenAI.

\begin{prop}\label{prop:group_level}
There is a threshold $\bar{r}\in[0,1]$ so  that when $r>\bar{r}$,  the agent never delegates to GenAI and behaves in the same way as a rational agent. When
$r\le\bar{r}$, the agent delegates to GenAI when $c>r^{2}\mathbb{E}[b(\omega)^{2}]$
and pays the attention cost when $c<r^{2}\mathbb{E}[b(\omega)^{2}]$,
and  the probability of over-delegation is  strictly decreasing in $r$
over the range $[0,\bar{r}].$ The threshold $\bar{r}$ is strictly
interior when $\mathbb{E}[b(\omega)^{2}]>\sigma_{\omega}^{2}$ and
it is equal to 1 when $\mathbb{E}[b(\omega)^{2}]<\sigma_{\omega}^{2}$. 
\end{prop}



In the case where the GenAI's bias is relatively large ($\mathbb{E}[b(\omega)^{2}]>\sigma_{\omega}^{2}$),
a rational agent never delegates to GenAI. Instead, a rational agent
either pays the attention cost to learn $\omega$ when $c$ is low
enough, or chooses the ex-ante optimal default action 0 when $c$
is too high. With sufficiently severe anthropomorphic projection,
the biased agent over delegates. For high $c,$ the biased agent delegates
to GenAI while the rational agent chooses the default action. For medium
$c,$ the biased agent delegates to GenAI while the rational agent pays
the attention cost.

Even in the case where the GenAI's bias is relatively small ($\mathbb{E}[b(\omega)^{2}]<\sigma_{\omega}^{2}$)
so that a rational agent sometimes delegates to GenAI, the biased agent
still uses a wrong threshold in cost realization to decide
between paying attention or delegating to GenAI.
For some medium realizations of $c$, a rational agent pays attention
but the biased agent delegates.

A corollary  of \cref{prop:group_level} is that an agent who suffers from anthropomorphic projection can be made strictly worse off when the GenAI becomes objectively more aligned on every problem. Of course, this cannot happen to a rational agent, and it also cannot happen under any fixed (even if irrational) delegation strategy that maps attention cost realizations to delegation decisions. As the following example illustrates, this phenomenon happens because the biased agent increases their GenAI delegation by too much in response to the GenAI's improved alignment, and this behavioral adjustment in delegation is what drives down their welfare.  


\begin{example}
Fix any $0<r<1$ and consider $b_{L}=(\sigma_{\omega}/r)-\epsilon$
and $b_{H}=(\sigma_{\omega}/r)+\epsilon$ for sufficiently small $\epsilon>0$
so that we still have $b_{L}>\sigma_{\omega}$. Consider a GenAI model with
$b(\omega)=b_{H}$ for every $\omega$ and another GenAI model with $b(\omega)=b_{L}$
for every $\omega$. For a rational agent, because both $b_{L}^{2}$
and $b_{H}^{2}$ are larger than $\sigma_{\omega}^{2}$, Proposition
\ref{prop:group_level} implies the rational agent never delegates to either GenAI model and has
the same welfare when they have access to either. By contrast, the biased
agent with parameter $r$ does not delegate for $b(\omega)=b_{H}$
(and gets the same welfare as the rational agent) but delegates with
positive probability for $b(\omega)=b_{L}$ (and gets strictly lower
welfare compared to the rational agent since they are always strictly better off choosing
$a=0$ instead of delegating). So, the biased agent has strictly lower
welfare when they have access to a GenAI model with the lower bias $b(\omega)=b_{L}$
than a GenAI model with the higher bias $b(\omega)=b_{H}$. 
\end{example}


\subsection{Implications of Self Projection}

Now suppose $b(\omega)=0$ for every $\omega$, so the GenAI takes the
optimal action for the average agent in every decision problem. If
an agent  exhibits self projection
bias with $\rho=1$, then they believe that the GenAI will take their
optimal action in every decision problem. So, they will make the mistake
of always delegating their decisions. The next proposition generalizes this special case: under any amount of self projection, agents whose $\theta$ types are not too extreme over-delegate to GenAI because they over-estimate the degree to which the
AI's decision matches their idiosyncratic preferences.

\begin{prop}
\label{prop:self_basic}
Suppose $\rho\in[0,1).$ For $|\theta|>\sigma_{\omega}/(1-\rho)$,
the agent  never delegates to GenAI for any
realization of $c$ and behaves as-if rationally.  For $\sigma_{\omega}<|\theta|<\sigma_{\omega}/(1-\rho)$,
the rational agent never delegates to GenAI but the biased agent delegates
to GenAI with positive probability. For $|\theta|<\sigma_{\omega}$, both the rational agent and the
biased agent delegate to GenAI with positive probability, but the 
biased agent does so for more realizations of the attention cost $c$. 
\end{prop}

The idea behind this result is that an agent with type $\theta$ partially
projects their type onto the GenAI's behavior, thus misperceiving the
expected decision utility from delegation to be $-(1-\rho)^{2}\theta^{2}$
instead of the objectively correct $-\theta^{2}$. This causes the
agent to over-delegate compared to the rational benchmark.

For rational agents in a world with GenAI, welfare is monotonically decreasing
in the distance of an agent's type to the group average. The intuition
is that the GenAI is more aligned with the average agent but less aligned
with agents with more unusual preferences, so the option of delegation
is less beneficial for the latter. But this result crucially depends
on agents holding correct beliefs about the GenAI behavior and need not hold when
agents suffer from self projection. Indeed, for $\rho\in(0,1),$ we
show that welfare jumps up discontinuously at the type $\theta=\sigma_{\omega}/(1-\rho)$.
The idea is that a biased agent who is subjectively almost indifferent between
delegating to GenAI and taking the default action equal to their type
is actually substantially better off taking the default action, since
the subjective indifference is driven by an overestimation of the
alignment between the GenAI's action and the agent's type.
\begin{prop}
\label{prop:monotonic}
For rational agents who can delegate to GenAI, welfare is monotonically
decreasing in $|\theta|.$ By contrast, for $\rho\in(0,1),$ the welfare
of agents who suffer from self projection is not monotonic around $\theta=\sigma_{\omega}/(1-\rho)$.
\end{prop}



\section{Experimental Design and Deployment}

\subsection{Overview}

We advertised the experiment as a study that requires agents to make  incentivized choices and predictions. The experiment began with a brief informed consent. Subjects who consented were told that the experiment consists of two parts, that they will earn ``tokens'' based on their answers, and that these tokens will be converted into a bonus payment at a rate of 1,000 tokens per US dollar at the end of the experiment (in addition to a base payment).



In the first part of the experiment (\emph{choice tasks}), subjects are asked 
to make choices in nine problems spanning four domains: risk, time
preference, social preference, and strategic interactions (see \cref{subsec:The-Decision-Problems} for details on the problems). Problems appear in a random order: for each subject,
we draw uniformly at random an order of the four domains, and within each domain we  also randomize the
order of the problems. Subjects earn tokens based
on their choices in every problem. Subjects receive no feedback during the experiment (specifically, they only learn how much they earned  after the end of the experiment).  

In the second part of the experiment (\emph{prediction tasks}),
subjects are told that an AI chatbot was asked to make  choices on the behalf of a human user in the same problems (and in an additional problem that the subjects have not seen before). They are also shown the exact instructions that were given to the chatbot before each choice: 

\begin{quote}
``You are a powerful decision-making agent and a helpful assistant that strictly follows the user’s instructions. The user is busy and requires you to provide an answer in exactly the requested format. The user may be given tokens depending on the answer you provide; each token is worth 0.001 US dollars. Here is the question that the user is facing:''    
\end{quote}

Subjects are told that the AI chatbot was asked about each problem thousands of times, and they are asked to predict the average AI response. The problems in the prediction tasks appear in a random order according to the same procedure we used in Part 1 (but using an independent random draw).  Subjects earn 100 tokens for each
prediction task where their prediction is sufficiently accurate (no more than 10\% off from
the average AI choice).






\subsection{\label{subsec:The-Decision-Problems}The Decision Problems}

We assembled a panel of ten economic decision problems across the four domains: risk, time preference, social preference, and strategic interactions. Eight of the problems came from \citet{Yariv}, who use these (and other) tasks to compare behavior across different experimental subject pools. We added an additional problem of strategic interaction (the beauty contest, Decision Problem 9) and an additional problem of social preference (Decision Problem 10) that uses slightly different numbers than those used in \citet{Yariv}. Each problem requires either a numerical answer or a binary answer. 


\paragraph{Decision Problem 1 (``risk100'').} The subject chooses how many tokens to wager out of an endowment of 100.  With 35\% probability, the subject receives three times the wagered tokens. With 65\% probability, the wagered tokens are lost.\footnote{Decision problems in the domain of risk follow \citet{gneezy1997experiment}. } 


\paragraph{Decision Problem 2 (``risk200'').}
The subject chooses how many tokens to wager out of an endowment of 200.  With 50\% probability, the subject receives 2.5 times the wagered tokens. Otherwise, the wagered tokens are lost.


\paragraph{Decision Problem 3 (``discounting'').} Subjects will receive either 150 tokens in 30 days or a larger number of tokens in 60 days. They are asked to report the minimal number of tokens that will make them choose the 60-days option. The number of tokens associated with the 60-days option is then randomly drawn from the interval between
150 and 400, and the subject receives the option that matches their reported threshold.\footnote{This problem is adapted from \citet{Yariv}, who use a similar but 
hypothetical comparison between money in 30 days versus 60 days. We
chose the range of 150 tokens to 400 tokens for the 60-days option
based on their finding that a vast majority of 
subjects give answers in this range.}

\paragraph{Decision Problem 4 (``dictator100'').} 
The subject chooses how many tokens, out of an endowment of 100, to give away to another randomly selected subject.

\paragraph{Decision Problem 5 (``dictator300'').} 
The subject chooses how many tokens, out of an endowment of 300, to give away to another randomly selected subject.

\paragraph{Decision Problem 6 (``dictator100x2'').} 
The subject chooses how many tokens, out of an endowment of 100, to give away to another randomly selected subject. For each token given, the other subject receives two tokens. 

\paragraph{Decision Problem 7 (``dictator100x0.5'').} 
The subject chooses how many tokens, out of an endowment of 100, to give away to another randomly selected subject. For each token given, the other subject receives half a token.  

\paragraph{Decision Problem 8 (``prisoner'').} Subjects play a one-shot
prisoner's dilemma  with another randomly selected subject from the same session.
If both players cooperate, then each gets 80 tokens. If one cooperates
and one defects, then the cooperator gets 60 tokens and the defector
gets 90 tokens. If both defect, then each gets 70 tokens. The two
actions in the game are given abstract names to avoid any connotations
of the words ``cooperate'' and ``defect.''

\paragraph{Decision Problem 9 (``beauty'').} Subjects play ``guess two-thirds
the average,'' an instance of a beauty-contest game. Subjects enter
whole numbers between 0 and 100, and the subject whose number is closest
to two-thirds of the average of the numbers entered by all subjects
in the session wins 5,000 tokens.

\paragraph{Decision Problem 10 (``dictator200'').} 
The subject chooses how many tokens, out of an endowment of 200, to give away to another randomly selected subject. This problem was not presented to the human subjects as a choice task, but they were 
asked to make a prediction about GenAI's choice in this problem during the prediction tasks (Part 2 of the experiment).
 
For easy reference,  \cref{tab:problems_summary} summarizes the descriptions of the decision problems. 


% \begin{table}
% \begin{centering}
% \caption{\label{tab:problems_summary}Summary of Decision Problems}
% \begin{tabular}{|c|>{\centering}p{10cm}|}
% \hline 
% Task & Description\tabularnewline
% \hline 
% \hline 
% risk100 & Wager some of 100 tokens: 35\% chance to receive 3 times the wagered
% tokens, 65\% chance to lose them.\tabularnewline
% \hline 
% risk200 & Wager some of 200 tokens: 50\% chance to receive 2.5 times the wagered
% tokens, 50\% chance to lose them.\tabularnewline
% \hline 
% discounting & A delayed payment of 150 tokens in 30 days would be equivalent to a delayed payment of how many tokens in 60 days for you? \tabularnewline
% \hline 
% dictator100 & Give tokens to a random subject from an endowment of 100.\tabularnewline
% \hline 
% dictator200 & Give tokens to a random subject from an endowment of  200.\tabularnewline
% \hline 
% dictator300 & Give tokens to a random subject from an endowment of  300.\tabularnewline
% \hline 
% dictator100x2 & Give tokens to a random subject from an endowment of  100. Recipient gets 2 tokens for each token given away. \tabularnewline
% \hline 
% dictator100x0.5 & Give tokens to a random subject from an endowment of  100. Recipient gets half a token for each token given away.\tabularnewline
% \hline 
% prisoner & Choose cooperate or defect in a prisoner's dilemma game.\tabularnewline
% \hline 
% beauty & Choose a number between 0 and 100 in a  beauty-contest game (guess two-thirds of the average guess).\tabularnewline
% \hline 
% \end{tabular}
% \par\end{centering}
% \end{table}

\begin{table}
\begin{centering}
\caption{\label{tab:problems_summary}Summary of Decision Problems}
\begin{tabular}{>{\raggedright\arraybackslash}m{2.5cm}m{3cm}m{11cm}}
\toprule \textbf{Domain} &
\textbf{Task} & \textbf{Description}\tabularnewline
\midrule
\multirow{2}{=}[-15pt]{{Risk\\Preference}} &
risk100 & Wager some of 100 tokens: 35\% chance to receive 3 times the wagered
tokens, 65\% chance to lose them.\tabularnewline\tabularnewline
% \hline 
&
risk200 & Wager some of 200 tokens: 50\% chance to receive 2.5 times the wagered
tokens, 50\% chance to lose them.\tabularnewline %\tabularnewline

 \midrule

\multirow{1}{=}[8pt]{{Time\\Preference}} &
discounting & A delayed payment of 150 tokens in 30 days would be equivalent to a delayed payment of how many tokens in 60 days for you? \tabularnewline%\tabularnewline
\midrule
\multirow{5}{=}[-35pt]{{Social\\Preference}} &
dictator100 & Give tokens to a random subject from an endowment of 100.\tabularnewline\tabularnewline
% \hline 
&
dictator200 & Give tokens to a random subject from an endowment of  200.\tabularnewline\tabularnewline
% \hline 
&
dictator300 & Give tokens to a random subject from an endowment of  300.\tabularnewline\tabularnewline
% \hline 
&
dictator100x2 & Give tokens to a random subject from an endowment of  100. Recipient gets 2 tokens for each token given away. \tabularnewline\tabularnewline
% \hline 
&
dictator100x0.5 & Give tokens to a random subject from an endowment of  100. Recipient gets half a token for each token given away.\tabularnewline%\tabularnewline
 \midrule 
 \multirow{2}{=}[-12pt]{{Strategic\\Interactions}} &
prisoner & Choose cooperate or defect in a prisoner's dilemma game.\tabularnewline\tabularnewline
% \hline 
&
beauty & Choose a number between 0 and 100 in a  beauty-contest game (guess two-thirds of the average guess).\tabularnewline
\bottomrule
\end{tabular}
\par\end{centering}


\end{table}

% \textcolor{red}{can we make the table more ``in format''}

\subsection{Querying GPT-4o}
The GenAI choices used to evaluate the correctness of subjects' predictions both for payment and for the main analysis were obtained from GPT-4o. We designed our prompts so that the GenAI model outputs a choice as the first token without offering detailed reasoning steps (see \cref{sec:prompts} for details). OpenAI provides the log probabilities for up to the 20 most likely tokens at each position. Accordingly, we recorded the log probabilities of the top 20 tokens at the first position and calculated a weighted average with weights proportional to their probabilities.\footnote{The  probabilities of the top 20 most likely tokens added up to $0.996$ on average.} The only exception is the prisoner's dilemma, which requires the GenAI models to make a binary choice between the two strategies ``A'' (cooperate) and ``B'' (defect). In this case, we specifically recorded the probability of token ``A.'' Since log probabilities are not fully deterministic, we repeated this process 100 times and took the average as the final choice.
%%%%%
\subsection{Deployment}
We implemented the experiment in oTree \citep{otree} and conducted it online using the Prolific platform in January  2025. We recruited 300 subjects who met the following three criteria: (1) live in the United States; (2) have previously
completed at least ten studies on Prolific; (3) have an approval rate
of at least 95\% on Prolific. Subjects were recruited in three sessions, with 100 subjects per session. 
Subjects had up to 67 minutes to complete the study. They took an average of 12.97 minutes (s.d. 10.05 minutes). On average, they earned \$4.15 (s.d. \$0.59), including a show-up fee of \$2.70.\footnote{A small part of this payment was delayed by 30 days or 60 days due to Decision Problem 3, which elicits time preferences. See \cref{subsec:The-Decision-Problems} for details.} Thus, the average  earning rate in the study was \$19.20 per hour.

\subsection{Auxiliary Measures and Questions}

At the end of the study, we asked subjects several questions about their degree of exposure, usage intensity, and attitudes towards GenAI (see \cref{fig:survey_results}). 
% conduct a survey about the subject's usage
% of GenAI tools in their daily life and their general beliefs about how GenAI choices compare
% with human choices. We ask subjects to indicate which of the common
% GenAI tools they have used before, how often they use GenAI tools
% in a typical week, and whether they have a paid subscription to any
% GenAI tool. We also ask subjects to what extent they agree with each
% of the following two statements: (1) ``Decisions made by AI are on average similar
% to decisions made by humans''; (2) ``On average, AI makes better
% decisions than humans.'' 
We also have access to demographic data on the subjects from their Prolific account registration.


In addition, throughout the experiment, we tracked  the amount of time that subjects spent on each task (choices and predictions). To mitigate the risk that subjects use LLMs in prediction tasks,  we also kept track
of subjects who copied text from the webpage during the tasks. Specifically,
subjects who pressed the keyboard combination Ctrl+C on Windows, Command+C on Mac, or used the copy function in their web browser during a task are flagged in our data. We found that 11\% of the subjects copied text at least once. 






%%%%%%

Finally, for robustness, and since subjects were not informed of the specific GenAI model used in the prediction tasks, we also queried three additional commercial models (GPT-4o-mini,\footnote{On average, the probabilities of the top 20 most common  tokens from the GPT-4o-mini model added up to $0.995$.} Gemini-1.5-Pro, and Gemini-1.5-Flash). The prompts provided to each model were identical, although the methods for eliciting choices varied. Specifically, since Google does not provide the distribution of the next token,  we queried the Gemini models 1,000 times for each task and computed the average result. These measurements were used for supplemental analyses, but not for determining subjects' 
compensation. 
 

\subsection{Pre-Registration}

We pre-registered our experimental protocol and primary analyses prior
to the start of the experiment. Our pre-registration specifies GPT-4o
as the model to be used to test the accuracy of subjects' predictions, the target sample size (300), a measure of the relative accuracy of aggregate subject predictions
about the GenAI choices (see \cref{subsec:PRA}), a regression specification to estimate individual-level
self projection (see \cref{subsec:self-projection}), and a similar regression specification with the subject's
prediction for a particular problem as the dependent variable and
the subject's choice in a related problem as the regressor. The
pre-registration also discussed our secondary analyses relating to subjects' experience with  and attitudes towards
GenAI tools, but we did not specify any particular hypotheses. The  pre-registration can be found on
the registry website at \href{https://aspredicted.org/yd32-r96n.pdf}{https://aspredicted.org/yd32-r96n.pdf}.

% \section{List of analyses/Tables}


% \begin{enumerate}
%     \item summary statistics: demographics, exposure to LLMs. May correlation between exposure and trust in LLMs. This should also include the information about copying.  
%     \item aggregate level results. 
    
%     \item regression results. Heterogeniety by various responses to AI questions (I would cut at the median of each answer)

%     \item cross predictions (generalization): First, we want to use the hold out question. Then, we want to use ``same domain" questions. I am still not sure how to use ``cross domain.'' 

%     \item are there individuals that are better at predicting than others (accross the board)? I am not sure how to measure this exactly (e.g., should each question get the same weight)?

%     \item Something to look at: the table Kevin sent (with average AI action) essential implies a discounting rate, a coefficient of risk aversion, etc. I am not at all sure that LLM has a consistent coefficient across questions in the same domain.Also for people. It looks like ppl give in dictator game 30\% no matters what. LLM does something very different (I think it cares more about efficiency and treats both players as almost symmetric. Note that "double" and "half" dictator are isomorphic if you're a social planner, and I think the LLM does something very similar across both. I lost access to the interface -- wondering what we asked exactly. Hope we were clear about "me vs. others") 
    
% \end{enumerate}
    

\section{Main Experimental Results}

\subsection{Descriptive Statistics}
Out of 300 subjects who participated in the experiment, 62.7\% identified as women, 35.3\% identified as men, and the rest did not provide an answer. Subjects' average age was 37 (s.d. 13). Consistent with our requirement that subjects live in the U.S., the majority of subjects were born in the U.S., with 64\% identifying as White, 14\% identifying as Black, 7.7\% identifying as Asian, and the rest identifying as mixed or as belonging to other racial groups.   


\cref{fig:survey_results} summarizes the subjects' answers to the survey questions regarding their exposure, usage, and attitudes towards GenAI (administered at the end of the study, after the subjects have completed the choice tasks and prediction tasks). \cref{fig:survey_results}(a) shows that, on average, subjects report using GenAI two days in a typical week. \cref{fig:survey_results}(b) displays the percentages of subjects who have used various GenAI models at least once before. The survey also asked the subjects whether they agree  that GenAI makes decisions similar to those of humans,  and whether they agree that GenAI makes  better decisions than humans.
Figures~\ref{fig:survey_results}(c) and (d) show the distributions of responses. The results reveal considerably heterogeneous attitudes among subjects, though few hold extreme views on either statement.

% At the end of the experiment, we surveyed participants about their experience with GenAI and their attitudes toward it. (In Section \ref{subsec:interact_GenAI_exp}, we study how these factors affect the strength of self projection.) Figure~\ref{fig:survey_results}(a) illustrates the distribution of GenAI usage intensity, measured by the number of days in a typical week that the person uses GenAI tools. On average, subjects use GenAI two days per week, with a median of one day. Notably, 29\% of subjects report not using GenAI at all in a typical week. We also asked subjects about their usage of specific GenAI models. Figure~
% \cref{fig:survey_results}(b) displays the percentages of subjects who have used various GenAI models at least once before. Over 80\% of subjects reported having used ChatGPT before. In addition to GenAI experience, we surveyed subjects' attitudes on whether GenAI makes decisions similar to those of humans and whether it makes better decisions than humans. Figures~\ref{fig:survey_results}(c) and \ref{fig:survey_results}(d) show the response distributions. The results reveal considerably heterogeneous attitudes among subjects, though few hold extreme views on either statement.
\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \includegraphics[width=\textwidth]{fig/intensity.pdf}
        \caption{Intensity of GenAI Use}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        % \vspace{-14.6em}  % Try small negative values like -0.1em, -0.2em
        \includegraphics[width=\textwidth]{fig/GenAI_dist.pdf}
        \caption{Experience with GenAI Models}
    \end{subfigure}
    
    \vspace{1em}
    
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{fig/survey_AI_similar.pdf}
        \caption{GenAI Makes Similar Decisions}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{fig/survey_AI_better.pdf}
        \caption{GenAI Makes Better Decisions}
    \end{subfigure}
    \caption{Experience and attitudes toward GenAI. (a) Distribution of responses to the question: ``In a typical week, on how many days do you use generative AI tools?'' (b) Percentage of subjects who have used various GenAI models before. (c) Degree of agreement with the statement: ``Decisions made by AI are on average similar to decisions made by humans.'' (d) Degree of agreement with the statement: ``On average, AI makes better decisions than humans.''}
    \label{fig:survey_results}
\end{figure}

% \begin{figure}[h]
%     \centering
%     \begin{minipage}[b]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{fig/intensity.pdf}
%         \caption{Distribution of responses to the question: ``In a typical week, on how many days do you use generative AI tools?''}
%         \label{fig:intensity}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[b]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{fig/GenAI_dist.pdf}
%         \caption{Models}
%         \label{fig:modeldist}
%     \end{minipage}
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \begin{minipage}[b]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{fig/survey_AI_similar.pdf}
%         \caption{Similar}
%         \label{fig:similar}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[b]{0.45\textwidth}
%         \includegraphics[width=\textwidth]{fig/survey_AI_better.pdf}
%         \caption{Better}
%         \label{fig:better}
%     \end{minipage}
% \end{figure}

\cref{tab:task_responses_summary} summarizes the distributions of subjects' choices and predictions,  along with the average choices by  GPT-4o. For ease of comparison, we also reproduce the average responses among the Amazon Mechanical Turk (MTurk) subjects in \citet{Yariv} whenever they are available. 
The table shows that subjects exhibit substantial variations in their choices and predictions. Additionally, the average responses in our Prolific subject pool are  similar to those documented among MTurk users in \citet{Yariv}. 

% We find that the average choices of our Prolific subjects are comparable to the average choices in the subject pools studied by \cite{Yariv}. We also find substantial variations in choices and in predictions for each problem, which may reflect heterogeneity in preference parameters like risk aversion and heterogeneity in beliefs about GenAI's behavior. 


\begin{table}[t]
    \centering
    \caption{Subjects' Choices and Predictions and GPT-4o's Choices.} 
     \input{table/task_responses_summary}
    \label{tab:task_responses_summary}
    \vspace{0.2cm}
    \parbox{\linewidth}{\small Note: For Decision Problem 8 (``prisoner,'' the prisoner's dilemma game), we report the percentage rate of cooperation for choices and predictions.  Decision Problem 10 (``dictator200'') served only as a prediction task (and not as a  choice task). ``GPT-4o Choice'' is the average choice that GPT-4o makes when instructed to act on behalf of a human user. The exact prompt is described in \cref{sec:prompts}. The last column, titled ``Mturk SY,'' reproduces responses from \citet{Yariv} whenever they are available. While \citet{Yariv} do not report the average choice in their ``discounting''  elicitation, they report an average monthly discounting rate of $0.67$. }
\end{table}

\subsection{Anthropomorphic Projection}\label{subsec:PRA}

To assess the degree of anthropomorphic projection, we need to  compare subjects' predictions about the average GenAI choice in each problem  to both the actual  average GenAI choice and the average human-subject choice. 
For this purpose, we pre-registered the \emph{relative prediction accuracy} (RPA) measure, which is given by the following formula: 
\begin{equation} \label{eq:RPA}
	RPA_j = 1 - \frac{|\bar{P}_j-\bar{Y}_j|}{|\bar{P}_j-\bar{Y}_j|+|\bar{P}_j-\bar{X}_j|}.
\end{equation}
Here, $j$ is a task,  $\bar{P}_j$ is the subjects' average prediction, $\bar{X}_j$ is the subjects' average choice, and $\bar{Y}_j$ is the GenAI's average choice (all quantities for task $j$). A relative prediction accuracy of 1 occurs when the average human prediction fully matches the average GenAI choice. A relative prediction accuracy of 0 occurs when the prediction fully matches the average human choice. A measure of 0.5 occurs when the average prediction is equidistant between the average GenAI choice and the average human choice. The relative prediction accuracy relates to the $r$ parameter from the theoretical model in \cref{sec:theory}. In a problem where GenAI's choice is $\omega + b(\omega)$ and the average human choice is $\omega$, if the average prediction about GenAI choice among a group of agents is $r \cdot \left(\omega + b(\omega)\right)+(1-r) \cdot \omega =\omega + r \cdot b(\omega)$ for $0\le r \le 1$, then RPA of this group would be $r$. 

\begin{table}[t]
    \centering
    \caption{Summary of the Main Results }
    % There is evidence of both anthropomorphic projection (low RPA) and self projection (significant coefficient estimates $\hat{\beta}_j$).} 
    \input{table/main_result}
    \label{tab:main_result}
    \vspace{0.2cm}
    \parbox{\linewidth}{\small Note: RPA is calculated according to the formula provided in \cref{eq:RPA}. RPA values lower than 0.5 indicate that the average prediction about GenAI's choice  is closer to the average human-subject choice than  the actual GenAI choice.  
     In the second column, $\hat{\beta}_j$ is an estimate of $\beta_j$, a linear regression coefficient that measures how subjects' predictions about GenAI choices correlate with their own choices in the same problem
      (see \cref{eq:main}).     The column ``Std Err'' contains the robust standard errors of $\hat{\beta}_j$. All $\hat{\beta}_j$'s  are statistically significant at the 1\% level.}
\end{table}

Column (1) of \cref{tab:main_result} presents the RPA for each problem.\footnote{Our pre-registration specifies that if the average GenAI choice is too close to the average human-subject choice in any problem (in particular, if the two are within 0.1 standard deviations of human subjects' choices), then we will exclude the problem from the RPA analysis. This did not happen for any of the problems.} Across all problems, the RPA ranges between 0.078 and 0.204. Namely, subjects' average predictions about the  GenAI choice are substantially closer to the average human-subject choice than they are to the actual average GenAI choice. Additionally,  \cref{tab:main_result_drop_copy} in the Appendix shows that the RPA decreases even further when we exclude the 11\% of subjects with detected copying behavior (some of whom may have queried an LLM to form their predictions).   Altogether, our findings support the hypothesis of anthropomorphic projection:  subjects, on average, overestimate the similarity between the average GenAI choice and the average human choice.





\subsection{Self Projection}\label{subsec:self-projection}
Next, we investigate to what extent subjects' predictions about GenAI's choices are positively correlated with their own choices in the same problem.  For each problem $j$, we  run a linear regression to estimate the following pre-registered model
\begin{equation} \label{eq:main}
	P_{ij} = \alpha_j + \beta_j \cdot X_{ij} + \varepsilon_{ij},
\end{equation}
where $P_{ij}$ is subject $i$'s prediction of GenAI's choice in problem $j$, and $X_{ij}$ is the de-meaned version of subject $i$'s own choice for problem $j$ (that is,  $X_{ij}$  is $i$'s choice minus $\bar{X}_j$, the average choice among all subjects for problem $j$). The coefficient of interest is $\beta_j$. It measures the correlation between subjects' choices and their predictions about GenAI, analogous to  the parameter $\rho$ from the theoretical model in \cref{sec:theory}. We interpret a positive estimate of $\beta_j$  as evidence of self projection in problem $j$. 

The two rightmost columns of \cref{tab:main_result} report our estimates of  $\beta_j$ (additional details are provided in Appendix \cref{tab:individual_regressions}). Across all problems, our estimates 
of $\beta_j$  are positive, substantial, and statistically different from zero at the 1\% level. 
These findings are consistent with subjects projecting their personal traits onto GenAI.  For example, subjects revealed to be more risk-seeking through their choices (i.e., those who wager more tokens in the two risk-domain problems) tend to also believe that GenAI will behave in a more risk-seeking way, and vice versa for the more risk-averse individuals.

% These findings are consistent with subjects projecting their personal traits onto GenAI.  For example, more risk-seeking individuals (i.e., those who wager more tokens in the two risk-domain problems) tend to believe that GenAI will behave in a more risk-seeking way, and vice versa for the more risk-averse individuals. Similar patterns are observed in problems concerning time preferences, where more impatient subjects perceive GenAI as more impatient, and social preferences, where more generous subjects perceive GenAI as more generous. Interestingly, we also observe positive and significant $\hat{\beta}_j$ in guess two-thirds the average and the prisoner's dilemma (to a smaller extent). This  may reflect people exaggerating how much GenAI shares their assessment of other subjects' reasoning depth in the beauty-contest game,  and people projecting their trade-off of social efficiency versus individual payoff in the prisoner's dilemma game.  

One may wonder if our findings result from  subjects memorizing their choices for every problem in the first part of the experiment and simply repeating them as their predictions or using them as anchors for their predictions in the second part of the experiment.  To rule out this possibility, we analyze predictions in dictator200, a problem that was not used as a choice task in the first part of the experiment. We estimate regression models of the form   
\begin{equation} \label{eq: cross predict}
	P_{ij} = \alpha_{jk} + \beta_{jk} \cdot X_{ik} + \varepsilon_{ij},
\end{equation}
where $P_{ij}$ is subject $i$'s prediction of GenAI's choice in problem $j$ and $X_{ik}$ is the de-meaned version of subject $i$'s own choice for a different problem $k$. 

We set $j= \text{dictator200}$. For regressors, we separately include the subjects' choices in four other dictator problems and two risk problems (as $k$).  \cref{tab:cross_give1} presents our results.  We find that subjects' choices from the dictator problems are highly correlated with their predictions of GenAI choice in dictator200, with all coefficient estimates $\hat{\beta}_{jk}$ being positive and statistically significant at the 1\% level. This is consistent with self projection operating through a channel where subjects project their social-preference parameter onto the GenAI, so a generous subject both chooses to give away more tokens in the four dictator-type choice tasks and predicts the GenAI would give away more tokens in the new prediction task that was previously unseen. By contrast, choices from the two  problems that belong to a different domain (risk problems) have much less explanatory power (as measured by $R^2$). Additionally, the estimated coefficient on one of the risk problems a  is not statistically significant at standard levels. 


\input{table/cross_give1}


We extend this analysis to problems that appeared as both choice tasks and prediction tasks. In Appendix \cref{tab:cross_give2,tab:cross_give3}, we regress predictions in one dictator problem on choices in another dictator problem. In Appendix~\cref{tab:cross_investment}, we regress predictions in one risk problem on choices in the other risk problem and on choices in the dictator problems. 
The results show that in every case, the coefficient estimate $\hat{\beta}_{jk}$ of own choices in a related problem is positive and statistically significant at the 1\% level. Furthermore, mirroring the findings from \cref{tab:cross_give1}, choices from dictator problems have much less explanatory power (as measured by $R^2$) compared to choices from the other risk problem in explaining the subjects' predictions in risk problems.

\paragraph{Summary.} We find that, as a group, human subjects overestimate the similarity between the average human choice and the average GenAI choice. Additionally, at the individual level, human subjects overestimate the correlation between their own choices and GenAI choices in every problem. We also provide evidence that suggests that this correlation may arise from human subjects projecting their traits (such as domain-specific preference parameters) onto the AI.    


\section{Heterogeneity Analyses }
In this section, we explore how the degree of self projection varies along several dimensions: experience with GenAI, attitudes toward GenAI, attention (as proxied by the amount of time spent on prediction tasks), and gender.  We find limited evidence of heterogeneity along any of these dimensions. 


% the robustness behind our finding of self projection and study factors that may affect the strength of self projection. The main messages are twofold. First, we show that the finding of self projection is not a mere artifact of subjects repeating the same answer for the same problem in the choice tasks and the prediction tasks. We find that subjects' predictions about GenAI choice in a given problem are positively correlated with their own choices in \emph{other} problems from the same domain, which is  consistent with subjects projecting a domain-specific deep preference parameter like risk aversion or social-preference weight. Second, we show that more experience with GenAI tools and longer deliberation time have either no effect or weak effect on mitigating  self projection. 


% \subsection{Correlation Between Predictions and Choices in  Other  Problems}

% The nine problems analyzed in the previous section are those that the subjects encountered twice in the experiment: first as a choice task, and later as a prediction task. Although we re-shuffled the order of the problems between the choice tasks and the predictions tasks, we would mechanically observe a correlation between choices and predictions if subjects memorized their choices for every problem in the first part of the experiment and simply repeated them as their predictions in the second part of the experiment.  

% To understand whether subjects could be  projecting some deep domain-specific preference parameter onto GenAI instead of repeating memorized choices, this subsection examines the correlation between subjects' choices in one problem $k$ and their predictions in a related problem $j$. Specifically, we estimate the following linear regression:
% \begin{equation*}
% 	P_{i, j} = \alpha_{j,k} + \beta_{j,k} \cdot X_{i,k} + \varepsilon_{i,j},
% \end{equation*}
% where $P_{i,j}$ is subject $i$'s prediction of GenAI's choice in problem $j$ and $X_{i,k}$ is the de-meaned version of subject $i$'s own choice for problem $k$. 

% %We conduct this cross-prediction analysis within three sets of related problems. Full regression results are provided in Tables~\ref{tab:cross_give1}, \ref{tab:cross_give2}, \ref{tab:cross_give3}, and \ref{tab:cross_investment} in the Appendix.

% Recall that one of the problems in the prediction tasks (dictator200) was not used earlier as a problem in the choice tasks. This helps eliminate the possibility of subjects simply recalling their previous responses. In \cref{tab:cross_give1} in the Appendix, we present the results of a regression using dictator200 as problem $j$.  For regressors, we separately include the subjects' choices in four other dictator problems and two risk problems.  When using choices from other problems that belong to the same domain (i.e., the dictator problems), the coefficient estimate $\hat{\beta}_{j,k}$ is positive and statistically significant at the 1\% level. This is consistent with self projecting operating through a channel where subjects project their social-preference parameter onto the GenAI, so a generous subject both chooses to give away more tokens in the four dictator-type choice tasks and predicts the GenAI would give away more tokens in the new prediction task that was previously unseen. By contrast, when using choices from the two other problems that belong to a different domain (risk problems) as the regressor, $\hat{\beta}_{j,k}$ is less than half in magnitude compared to similarly scaled dictator problems and it is not statistically significant at even the 10\% level for one of the risk problems. 


% In Tables~\ref{tab:cross_give2} and  \ref{tab:cross_give3}, we regress predictions in one dictator problem on choices in another dictator problem (among those problems that are used for both choice tasks and prediction tasks). We perform the analgous exercise for risk problems in Table  \ref{tab:cross_investment}.  The results show that in every case, the coefficient estimate $\hat{\beta}_{j,k}$ of own choices in a related problem is positive and statistically significant at the 1\% level. 

\subsection{Experience with GenAI and Attitudes Toward GenAI}
\label{subsec:interact_GenAI_exp}

As we discuss in the introduction, some of the possible explanations for self projection suggest that it will be attenuated as people gain more experience with GenAI. Additionally, self projection may also affect, and be affected by, people's beliefs about the quality of GenAI decision-making. This motivates us to assess the heterogeneity of our findings with respect to experience with GenAI and attitudes toward GenAI.  

% Our analysis reveals substantial self projection. We next turn to the question of whether subjects who have more experience with  GenAI tools suffer less from self projection due to their greater familiarity with the technology, and whether the extent of self projection varies among people who hold different beliefs about the quality of GenAI output. 

To this end, we split subjects to two groups based on their survey responses and  estimate regressions of the following form:
\begin{equation} \label{eq:hetero}
	P_{ij}= \alpha_j + \beta_{j} \cdot  X_{ij} +  \delta_j \cdot G_i + \gamma_j \cdot  G_i \times X_{ij} + \varepsilon_{ij} 
\end{equation}
where $G_i$ is an indicator variable for whether subject $i$ belongs to one of the groups. 
The coefficient of interest is 
% Here $\delta_j$ is the effect of group membership on prediction in problem $j$ and 
$\gamma_j$. It measures if group membership is associated with lower (if $\gamma_j < 0$) or higher (if $\gamma_j > 0$) levels of self projection in problem $j$. 

% We use each of the following  criteria for dividing subjects into two groups: 
We estimate the regression model of \cref{eq:hetero} using each of the following group classifications to define $G_i$:
\begin{enumerate}
	\item \texttt{Heavy User}: Subjects who reported using GenAI at least two days in a typical week (i.e., their answer was above the median). 
	\item \texttt{Text-Based LLM User}: Subjects who reported  that they have used ChatGPT, Gemini, Claude, or DeepSeek before.\footnote{This group excludes subjects who have only used AI image generators like Midjourney, but not LLMs that primarily output free-form text.}
	\item \texttt{Paid User}: Subject who reported having a paid subscription to a GenAI model or application.
	\item \texttt{Agree AI Similar}: Subject who agreed or strongly agreed with the statement: \textit{``Decisions made by GenAI are on average similar to decisions made by human.''}
	\item \texttt{Agree AI Better}: Subject who agreed or strongly agreed with the statement: \textit{``On average, GenAI makes better decisions than humans.''}
\end{enumerate}
% The first three conditions relate to the subject's usage of GenAI, allowing us to test whether experience with GenAI products affects the extent of self projection. The last two conditions relate to their attitudes about whether GenAI are aligned with human choices and whether they make better choices. 


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/group_experience.pdf}
    \caption{Heterogeneity: Experience with GenAI}
    \label{fig:group_experience}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/group_attitude.pdf}
    \caption{Heterogeneity: Attitudes Toward GenAI}
    \label{fig:group_attitude}
\end{figure}


 \cref{fig:group_experience,fig:group_attitude} plot the point estimates of $\gamma_j$ and the 95\% confidence interval for each problem  $j$  for each of the five group classifications (the full regression results are in \cref{Additional: experience}). The figures reveal that the point estimates are mixed and noisy. 

% with full regression results  in Tables~\ref{tab:group_heavy_user}-\ref{tab:group_ai_better} in the Appendix. in Figures~\ref{fig:group_experience} and \ref{fig:group_attitude}, we plot the point estimates of  $\gamma_j$ and the 95\% confidence interval for each problem  $j$  under each of the five group classifications. We see that these regressions yield highly noisy and inconsistent    results, with a large majority of the estimates not being statistically significant at the $5\%$ level and the interaction effect having different signs for different problems.  

To increase power, we also pool all problems together and estimate a model that imposes the assumption that $\gamma_j$ is constant across problems (recall from \cref{tab:main_result} that the magnitude of the coefficient estimate $\hat{\beta}_j$ was similar across most tasks). Specifically, we use all data to jointly estimate the following linear regression model for all problems $j$:
\begin{equation} \label{eq:hetero pooled}
	P_{i j}= \alpha_j +  \beta_{j} \cdot  X_{ij} +  \delta_j \cdot G_i + \gamma \cdot  G_i \times X_{ij} + \varepsilon_{ij} 
\end{equation}
with standard errors  clustered at the problem level.\footnote{For this analysis, we rescale $P$ and $X$ in every problem to the range $[0,100]$ (before we demean $X$) to avoid over-weighting problems that involve larger ranges of values. Specifically, for ``discounting,'' we apply the transformation $(\text{response}-150)\times \frac{100}{400-150}$; for ``risk200,'' we divide the response by 2; and for ``dictator300,'' we divide the response by 3.\label{fn:normalization}}

\input{table/group}

Estimates of $\gamma$ are reported in \cref{tab:group}. Three out of the five point estimates are positive, and one is essentially equal to zero. The only negative point estimate comes from \texttt{Heavy User}.  It is not statistically significant, and the lower  bound on the 95\% confidence interval of this estimate is $-0.16$ (to contextualize, the median estimate of $\beta_j$ across different problems is around $0.4$).  


% For each classification, the estimate of $\gamma$ is not significant at the $10\%$ level. Closest to being statistically significant and largest in magnitude is the effect of being a heavy GenAI user (i.e., uses GenAI tools two or more days per week). Being a heavy user mitigates the estimated effect size of self projection by 0.059 on average. Since the median estimated self projection coefficient across different problems is 0.401, having above-median experience with GenAI is estimated to  decrease self projection by  less than 15\%. Overall, we find little evidence that  experience with GenAI or having particular attitude about the quality of GenAI output can meaningfully mitigate self projection. 

\subsubsection{Individual Accuracy}
We also investigate if experience with GenAI correlates with more accurate predictions. To measure prediction accuracy at the individual level, we compute each subject's mean normalized absolute error (MAE), comparing  their predictions  with the GenAI's choices across all problems.\footnote{We use the same normalization as described in \cref{fn:normalization}, so that we equally weigh errors on all prediction tasks.} In this analysis, we compare subjects' predictions to  the choices of four different LLMs. 



We estimate the following linear regression model:
\begin{equation*}
\begin{split}
	MAE_{im} &= \alpha_m + \delta_{1m} \times GenAI\_exposure_i  
	+ \delta_{2m} \times agree\_AI\_similar_i \\
    &+  \delta_{3m} \times agree\_AI\_better_i + \delta_{4m} \times copier_i +  \varepsilon_{im},
\end{split}
\end{equation*}
where $i$ is a subject, $m$ is a GenAI model, and $copier_i$ is an indicator for whether we detected subject $i$ copying text from the website during the experiment. Finally, $GenAI\_exposure_i$ is a dummy variable indicating ``experience.'' We separately use three measures for this indicator: 1) \texttt{Heavy User}; 2) \texttt{Paid User}; and 3) $model\_user_{im}$, which is an indicator for subject $i$ reporting having experience with model $m$. 
 

% indicating whether subject $i$ has more experience with GenAI. For the analysis, we use three measures: 1) \texttt{Heavy User}; 2) \texttt{Paid User}; 3) $model\_user_{im}$, which is an indicator for subject $i$ reporting having experience with model $m$. . 

Appendix \cref{tab:mae_gpt-4o} presents our results with respect to GPT-4o. The coefficients of all experience measures are close to zero,  not statistically different from zero,  and  precisely estimated. Detected copying behavior is associated with an approximately  15 percent decrease in the MAE (statistically significant at the 1\% level across specifications). Finally, agreeing that AI  makes similar decisions to humans is associated  with an approximately 8 percent decrease in the MAE (statistically significant at the 5\% level across specifications), while the coefficient on agreeing that AI makes better decisions is small and not statistically significant.  In Appendix \cref{tab:mae_gpt-4o-mini,tab:mae_gemini-1.5-pro,tab:mae_gemini-1.5-flash}, we find similar results for the other larger GenAI model we study (Gemini-1.5-Pro), and weaker correlations for the two smaller models.    


\subsection{Self Projection and Response Time} \label{subsec:response time}

As response times are sometimes used to measure  attention or deliberation \citep[e.g.,][]{caplin2016measuring}, we also analyze heterogeneity along the lines of slow and fast response times. To this end, we follow a similar approach to  \cref{eq:hetero pooled} and estimate  the following linear regression model using all data:

\begin{equation*}
	P_{ij}= \alpha_j +   \beta_{j} \cdot  X_{ij} + \delta_j \cdot T_{ij} + \gamma \cdot  T_{ij} \times X_{ij} + \varepsilon_{ij} 
\end{equation*}
Here, $T_{i j}$ is an indicator for subject $i$ having spent longer than the \emph{median} time in the prediction task for problem $j$. We take two approaches for defining the median:
\begin{enumerate}
    \item \texttt{Problem Median}: Subject $i$ spent longer than the median response time across all subjects for problem $j$.
    \item \texttt{Personal Median}: Subject $i$ spent more time on prediction task $j$ than their personal median time across all prediction tasks. 
\end{enumerate}
We cluster standard errors at the problem level. 

% The regression results are in  
Appendix \cref{tab:time} displays our results. We find that having a response time  above the problem median is associated with slightly lower levels of self projection (point estimate $0.055$, s.e. $0.024$, $p<0.05$). The point estimate for personal median is similar, but the estimator is more noisy and is not statistically significant at standard levels. 

% which is statistically significant at the 10\% level. (Defining median response time based on personal median does not yield a statistically significant result at the 10\% level.) Again, this amounts to a less than 15\% mitigation of  self projection. We interpret these results to say that there is limited scope for longer deliberation time to substantially reduce self projection.  

\subsection{Self Projection and Gender} \label{subsec: gender}
Next, we ask if members of different demographic groups display different degrees of self projection  \citep[such a finding would have potential equity consequences, see][]{liang2022algorithmic}.
 We estimate the regression from  \cref{eq:hetero pooled}, using the group indicator $G_i$ to refer to whether the subject $i$ self-identified as female.  Appendix \cref{tab:gender} presents our results. Our estimate of $\gamma$ is close to zero and precisely estimated, suggesting limited heterogeneity along the dimension of gender. 

% Then, we ask whether different demographic groups vary in their extent of self projection. We estimate the average effect of gender on self projection by jointly estimating the following regression across all problems $j$, with standard errors clustered at the problem level:
% \begin{equation*}
% 	P_{i, j}= \alpha_j + \delta_j \cdot Female_i + \beta_{j} \cdot  X_{i,j} + \gamma \cdot  Female_i \times X_{i,j} + \varepsilon_{i,j} 
% \end{equation*}
% Here,  $Female_i$ is an indicator variable that equals 1 if subject $i$ identified as female and equals 0 if subject $i$ identified as male. (We exclude from this analysis subjects who did not identify as either female or male.) The estimated effect of gender on self projection (\cref{tab:gender}) is nearly zero and not statistically significant at the 10\% level. 
\paragraph{Summary.} We explored how the degree of self projection varies with exposure to  GenAI, with attitudes toward GenAI, with time spent on each prediction task, and with gender. We find limited evidence of heterogeneity along any of these dimensions. In particular, these results suggest that increased experience with GenAI and longer deliberation time are not associated with significant reductions in self projection. 

\section{Concluding Discussion}

This paper provides evidence  that people overestimate the degree to which GenAI choices are aligned with human preferences in general (anthropomorphic projection) and with their personal preferences in particular (self projection). We find limited evidence that experience attenuates these misperceptions. We show theoretically that these misperceptions lead to over-delegation to GenAI and interact with the true degree of AI alignment to produce complex welfare implications. 


We are not the first to study selective delegation to AI. We view the main contribution of our work as documenting the individual-level phenomenon of self projection. This  is facilitated by our focus on  economic decision environments that involve trade-offs, where agents' optimal actions depend on their preferences.


Our findings raise many interesting questions. For example, how can we  debias self projection?\footnote{\citet{dreyfuss2024human} report on measures for regulating the degree of anthropomorphic projection in problems that involve factual answers and not economic trade-offs.}  What are conducive design principles for GenAI agents in light of users who exhibit self projection and anthropomorphic projection? (Specifically, should GenAI sometimes defer to the user, similar to \citet{Noti}?) Will self projection persist in the long run? We leave these exciting questions for future research.    


\bibliographystyle{ecta}
% \bibliographystyle{ACM-Reference-Format}
\bibliography{bib}
\appendix

\section{Proofs}

\subsection{Proof of Proposition \ref{prop:group_level}}


\begin{proof}

The agent expects a utility of $-r^{2}\mathbb{E}[b(\omega)^{2}]$
from delegating to GenAI, $-c$ from paying attention and taking the
optimal action after learning $\omega$, and $-\sigma_{\omega}^{2}$
from not paying attention and taking the ex-ante optimal action $a=0$.
In the case where $\mathbb{E}[b(\omega)^{2}]>\sigma_{\omega}^{2}$,
the expected payoff from choosing $a=0$ is strictly higher than that
of delegation, so a rational agent never delegates. An agent with
$r>\frac{\sigma_{\omega}}{\sqrt{\mathbb{E}[b(\omega)^{2}]}}$ also
perceives the utility of delegation to be strictly lower than that
of choosing $a=0$, so they also never delegate. An agent with $r<\frac{\sigma_{\omega}}{\sqrt{\mathbb{E}[b(\omega)^{2}]}}$
perceives the utility of delegation to be strictly higher than that
of choosing $a=0$, so they will choose to delegate if the attention
cost is higher than $r^{2}\mathbb{E}[b(\omega)^{2}]$. 

In the case where $\mathbb{E}[b(\omega)^{2}]<\sigma_{\omega}^{2}$,
both the rational agent and the biased agent never choose to take
the action $a=0.$ They choose between delegating to GenAI or paying
the attention cost $c,$ depending on whether $c$ is lower than their
perceived loss from delegation, $r^{2}\mathbb{E}[b(\omega)^{2}]$. 
\end{proof}


\subsection{Proof of Proposition \ref{prop:self_basic}}


\begin{proof}
An agent with type $\theta$ expects a utility of $-(1-\rho)^{2}\theta^{2}$
from delegating to GenAI, $-c$ from paying attention and taking the
optimal action after learning $\omega$, and $-\sigma_{\omega}^{2}$
from not paying attention and taking the ex-ante optimal action $a=0$.
A rational agent with type $|\theta|>\sigma_{\omega}$ never delegates,
and for $|\theta|<\sigma_{\omega}$ the agent either delegates or
pays the attention cost depending on if $c$ is larger than $\theta^{2}$.
The biased agent does not delegate if $|\theta|>\sigma_{\omega}/(1-\rho)$.
For $|\theta|<\sigma_{\omega}/(1-\rho)$, the agent either delegates
or pays the attention cost depending on if $c$ is larger than $(1-\rho)^{2}\theta^{2}$.
Thus the biased agent sometimes delegates while the rational agent
never delegates for $|\theta|\in(\sigma_{\omega},\sigma_{\omega}/(1-\rho))$,
and the biased agent delegates with strictly higher probability than
the rational agent for $|\theta|<\sigma_{\omega}/(1-\rho).$ 
\end{proof}

\subsection{Proof of Proposition \ref{prop:monotonic}}


\begin{proof}
Consider two rational agents with types $0\le\theta_{1}<\theta_{2}$
(other cases are symmetric). For any optimal strategy $\sigma_{2}(c)$
of agent $\theta_{2}$ that maps the cost realization to a decision
between delegation, paying attention, or taking an action without
paying attention, consider the strategy $\sigma_{1}(c)$ of $\theta_{1}$
which (i) pays attention for every $c$ where $\sigma_{2}(c)$ pays
attention; (ii) chooses $\theta_{1}$ without paying attention for
every $c$ where $\sigma_{2}(c)$ chooses $\theta_{2}$ without paying
attention; (iii) delegates to the GenAI for every $c$ where $\sigma_{2}(c)$
delegates to the GenAI. Note that $\theta_{1}$ and $\theta_{2}$ get
the same payoff if they both pay attention, and they get the same
payoff of $-\sigma_{\omega}^{2}$ when they choose actions equal to
their types without paying attention. Delegation to GenAI has an expected
payoff of $-(\theta_{1})^{2}$ for type $\theta_{1}$ and $-(\theta_{2})^{2}$
for type $\theta_{2}$, so the former is higher. This shows $\theta_{1}$'s
welfare under the optimal strategy must be weakly higher than that
of $\theta_{2},$ so welfare is monotonically decreasing in $|\theta|.$

Now consider agents who suffer from self projection
with $\rho\in(0,1)$. All types to the right of $\sigma_{\omega}/(1-\rho)$
behave rationally. A type slightly to the left of $\sigma_{\omega}/(1-\rho)$
delegates to the GenAI when $c$ is higher than about $\sigma_{\omega}^{2}$,
but the true expected welfare from delegation is around $-\sigma_{\omega}^{2}/(1-\rho)^{2}$
whereas the true expected welfare from taking the the default action
$a=\theta$ is around $-\sigma_{\omega}^{2}$. Therefore the biased
agent with type slightly to the left of $\sigma_{\omega}/(1-\rho)$
has welfare that is discretely lower than that of the rational agent
of the same type. Since the rational agent's payoff is continuous
in type, this means there must be an upward jump in welfare at $\sigma_{\omega}/(1-\rho)$.
\end{proof}
    

\section{Relative Prediction Accuracy with Different GenAI Models}

In the main analysis, we followed our pre-registered plan and used  GPT-4o as the benchmark GenAI model. In this appendix, we replicate our analysis of  subjects' relative prediction accuracy  using other LLMs. Specifically, \cref{tab:llm_avg} presents the average GenAI choice for each problem and \cref{tab:llm_RPA} presents the corresponding RPA.


In most cases, the RPA is well below 0.5, but in a few cases the predictions align more closely with GenAI choices than with human subjects' choices. Comparing the smaller models (GPT-4o-mini and Gemini-1.5-Flash) with the larger models (GPT-4o and Gemini-1.5-Pro), we observe that human predictions are more aligned with the choices made by smaller models.


\begin{table}[ht]
    \centering
    \caption{GenAI Average Choices}
    \input{table/llm_avg}
    \label{tab:llm_avg}
\end{table}

\begin{table}[ht]
    \centering
    \caption{GenAI RPA}
    \input{table/llm_RPA}
    \label{tab:llm_RPA}
\end{table}



\section{Main Analysis Excluding Subjects with Detected Copying Behavior}


In this section, we replicate the main analyses excluding the 33 subjects (11\%) who were detected copying text at least once during the experiment. The results are summarized in \cref{tab:main_result_drop_copy}. Our measure of anthropomorphic projection---the RPAs---are slightly lower, while the $\hat{\beta}_j$ coefficients have hardly changed. 



\begin{table}[h]
    \centering
    \caption{Main Results Excluding Subjects with Detected Copying Behavior}
    \input{table/main_result_drop_copy}
    \label{tab:main_result_drop_copy}
    \vspace{0.2cm}
    \parbox{\linewidth}{\small Note: The column ``Std Err'' contains the robust standard errors of $\hat{\beta}_j$. All the $\hat{\beta}_j$ estimates are statistically significant at the 1\% level.}
\end{table}


\begin{landscape}
\section{Additional Materials}

\subsection{Additional Tables for \cref{subsec:self-projection}}

\input{table/individual_regressions}
\end{landscape}



\input{table/cross_give2}

\input{table/cross_give3}

\begin{landscape}
    \input{table/cross_investment}
\end{landscape}

\begin{landscape}
\subsection{Additional Tables for \cref{subsec:interact_GenAI_exp}}\label{Additional: experience}

    \input{table/group_heavy_user}

\end{landscape}

% \begin{landscape}
%     \input{table/group_llm_user}
% \end{landscape}


\begin{landscape}
     \input{table/group_llm_user}
    \input{table/group_paid_user}
\end{landscape}

\begin{landscape}
    \input{table/group_ai_similar}
    \input{table/group_ai_better}    
\end{landscape}

% \begin{landscape}
%     \input{table/group_ai_better}
% \end{landscape}

% \begin{landscape}
    % \input{table/group}
% \end{landscape}


% \input{table/individual_mape}
% \input{table/mape_gpt-4o}
% \input{table/mape_gpt-4o-mini}
% \input{table/mape_gemini-1.5-pro}
% \input{table/mape_gemini-1.5-flash}
% \input{table/individual_mae}

\input{table/mae_gpt-4o}

\input{table/mae_gpt-4o-mini}

\input{table/mae_gemini-1.5-pro}

\input{table/mae_gemini-1.5-flash}

\clearpage
\subsection{Additional Tables for \cref{subsec:response time}}
\input{table/time}
\subsection{Additional Tables for \cref{subsec: gender}}

\input{table/gender}

\clearpage
% \newpage
\section{Prompts} \label{sec:prompts}


The following paragraph is used as developer instructions in GPT and as system instructions in Gemini.
\begin{quote}
You are a powerful decision-making agent and a helpful assistant that strictly follows the user’s instructions. The user is busy and requires you to provide an answer in exactly the requested format. The user may be given tokens depending on the answer you provide; each token is worth 0.001 US dollars. Here is the question that the user is facing:   
\end{quote}

The prompts given to human subjects and GenAI models are nearly identical. Sentences included only in the GenAI prompts will be underlined. For human subjects, we emphasized the important parts of the problem using bold text, as shown below. 


Prompt for \textbf{risk100}
\begin{quote}
    You have \textbf{100} tokens. Please choose how many tokens out of the \textbf{100} to invest. The tokens you invest will be taken away, and you get to keep all the tokens that you choose not to invest. With \textbf{35}\% probability, the investment will be successful, and you will receive \textbf{3} tokens for every token that you invested. With \textbf{65}\% probability, the investment will be unsuccessful, and you will not receive anything for the tokens that you invested. How many tokens do you choose to invest? \uline{Your answer must contain only a number, nothing else. Answer:}
\end{quote}

Prompt for \textbf{risk200}
\begin{quote}
    You have \textbf{200} tokens. Please choose how many tokens out of the \textbf{200} to invest. The tokens you invest will be taken away, and you get to keep all the tokens that you choose not to invest. With \textbf{50}\% probability, the investment will be successful, and you will receive \textbf{2.5} tokens for every token that you invested. With \textbf{50}\% probability, the investment will be unsuccessful, and you will not receive anything for the tokens that you invested. How many tokens do you choose to invest? \uline{Your answer must contain only a number, nothing else. Answer:}
\end{quote}

Prompt for \textbf{discounting}
\begin{quote}
    After this study ends and you receive your base payment and bonus payment, you will also receive an additional bonus payment in either 30 days or 60 days. One option is to receive 150 tokens (to be converted into dollars) in 30 days. Another option is to receive a larger number of tokens (again, to be converted into dollars) in 60 days. \textbf{How many tokens do we need to give you in 60 days to make that option as good for you as getting 150 tokens in 30 days?} Enter a number between 150 and 400. 

    It is in your interest to answer accurately. After you enter your answer below (for example, let’s say you answer that N tokens in 60 days is as good as 150 tokens in 30 days), the computer will randomly draw a number X between 150 and 400, and this will be the number of tokens associated with the 60-days option. The computer will then choose between the option of “150 tokens in 30 days” and the option of “X tokens in 60 days”, based on your answer. If X is larger than N, then you will receive X tokens in 60 days. If X is smaller than N, then you will receive 150 tokens in 30 days. So, you will always get the option that you like better by accurately reporting how many tokens received in 60 days is equivalent (for you) compared to 150 tokens received in 30 days. 

    Please enter below \textbf{how many tokens we need to give you in 60 days to make that option as good for you as getting 150 tokens in 30 days}. \uline{Your answer must contain only a number, nothing else. Answer:}
\end{quote}

Prompt for \textbf{dictator100}
\begin{quote}
    You have \textbf{100} tokens. The computer has paired you with another randomly selected Prolific participant from this study. You must choose how many tokens out of the \textbf{100} to give away. The tokens that you do not give away are yours to keep. For each token that you give away, \textbf{the other participant will receive one token}. These received tokens will be converted into dollars and paid to the other participant as an extra bonus payment. How many tokens will you give away? \uline{Your answer must contain only a number, nothing else. Answer:}
\end{quote}

Prompt for \textbf{dictator300}
\begin{quote}
    You have \textbf{300} tokens. The computer has paired you with another randomly selected Prolific participant from this study. You must choose how many tokens out of the \textbf{300} to give away. The tokens that you do not give away are yours to keep. For each token that you give away, \textbf{the other participant will receive one token}. These received tokens will be converted into dollars and paid to the other participant as an extra bonus payment. How many tokens will you give away? \uline{Your answer must contain only a number, nothing else. Answer:}
\end{quote}

Prompt for \textbf{dictator100x2}
\begin{quote}
    You have \textbf{100} tokens. The computer has paired you with another randomly selected Prolific participant from this study. You must choose how many tokens out of the \textbf{100} to give away. The tokens that you do not give away are yours to keep. For each token that you give away, \textbf{the other participant will receive two tokens}. These received tokens will be converted into dollars and paid to the other participant as an extra bonus payment. How many tokens will you give away? \uline{Your answer must contain only a number, nothing else. Answer:}
\end{quote}

Prompt for \textbf{dictator100x0.5}
\begin{quote}
    You have \textbf{100} tokens. The computer has paired you with another randomly selected Prolific participant from this study. You must choose how many tokens out of the \textbf{100} to give away. The tokens that you do not give away are yours to keep. For each token that you give away, \textbf{the other participant will receive half of a token}. These received tokens will be converted into dollars and paid to the other participant as an extra bonus payment. How many tokens will you give away? \uline{Your answer must contain only a number, nothing else. Answer:}
\end{quote}

For \textbf{prisoner}, the prompts differ from the human subjects instructions. Human subjects are presented with a table of payoffs, while GenAI is provided with a description of the payoffs for different outcomes.

Human version:
\begin{quote}
    The computer has paired you with another randomly selected Prolific participant from this study to play a game. Each of you will choose A or B.

    \begin{table}[h!]
    \centering
    \begin{tabular}{|>{\centering\arraybackslash}m{5cm}|>{\centering\arraybackslash}m{4cm}|>{\centering\arraybackslash}m{4cm}|}
    \hline
    If... & \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}} Then: \\ You get \qquad \qquad \qquad They get \end{tabular}} \\ \hline
    You choose A and they choose A & 80 tokens & 80 tokens \\ \hline
    You choose A and they choose B & 60 tokens & 90 tokens \\ \hline
    You choose B and they choose A & 90 tokens & 60 tokens \\ \hline
    You choose B and they choose B & 70 tokens & 70 tokens \\ \hline
    \end{tabular}
    % \caption{Payoff Table}
    % \label{tab:payoff}
    \end{table}

    Do you choose A or B?
\end{quote}

GenAI version:
\begin{quote}
    The computer has paired you with another randomly selected Prolific participant from this study to play a game. Each of you will choose A or B. If you choose A and the other person chooses A, then you will get 80 tokens and the other person will get 80 tokens. If you choose A and the other person chooses B, then you will get 60 tokens and the other person will get 90 tokens. If you choose B and the other person chooses A, then you will get 90 tokens and the other person will get 60 tokens. If you choose B and the other person chooses B, then you will get 70 tokens and the other person will get 70 tokens. Do you choose A or B? Your answer must contain only a number, nothing else. Answer:
\end{quote}

Prompt for \textbf{beauty}
\begin{quote}
    You will play a guessing game with all other Prolific participants from this study. Everyone will enter a whole number between 0 and 100. The person whose number is the closest to \textbf{two-thirds of the average of the numbers} entered by all participants will win 5000 tokens. (If there is a tie for the closest number, then a winner will be randomly chosen among those who entered the closest number.) Enter your number below. \uline{Your answer must be either A or B and must contain nothing else. Answer:}
\end{quote}

Prompt for \textbf{dictator200}
\begin{quote}
    You have 200 tokens. The computer has paired you with another randomly selected Prolific participant from this study. You must choose how many tokens out of the 300 to give away. The tokens that you do not give away are yours to keep. For each token that you give away, the other participant will receive one token. These received tokens will be converted into dollars and paid to the other participant as an extra bonus payment. How many tokens will you give away? Your answer must contain only a number, nothing else. Answer:
\end{quote}

\newpage
\section{Screenshots from User Interface}

\begin{figure}[h]
    \centering
    %\includegraphics[width=0.9\linewidth]{fig/landing_anonymous.jpg}
    \includegraphics[width=0.9\linewidth]{fig/landing.jpg}
    \caption{Launch Page}
    \label{fig:landing}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/p1.png}
    \caption{Part 1 Instructions}
    \label{fig:part1}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{fig/choice_dictator300.png}
    \caption{ Example Choice Task (dictator300, Part 1)}
    % \label{fig:enter-label}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/p2.png}
    \caption{Part 2 Instructions }
    \label{fig:part2}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{fig/prediction_dictator300.png}
    \caption{ Example Prediction Task (dictator300, Part 2)}
    % \label{fig:enter-label}
\end{figure}

\end{document}
