Gaussian Processes (GPs) \citep{kolmogorov1940wienersche,rasmussen2006gaussian} are an important class of stochastic processes used in machine learning and statistics, with use cases including spatial data analysis \citep{liu2021missing}, time series forecasting \citep{girard2002gaussian}, bioinformatics \citep{luo2023diseasegps} and Bayesian optimization \citep{frazier2018tutorial}. GPs offer a non-parametric framework for modeling distributions over functions, enabling both flexibility and uncertainty quantification. These capabilities, combined with the ability to incorporate prior knowledge and specify relationships by choice of kernel function, make Gaussian Processes effective for both regression and classification.

However, GPs have substantial computational and memory bottlenecks. Both training and inference require computing the action of the inverse kernel Gram matrix, while training requires computing its log-determinant: both are $O(n^3)$ operations with sample size $n$. Further, storing the full Gram matrix requires $O(n^2)$ memory. These bottlenecks require scalable approximations for larger datasets.

Structured Kernel Interpolation (SKI) \cite{wilson2015kernel} helps scale Gaussian Processes (GPs) to large datasets by approximating the kernel matrix using interpolation on a set of inducing points. For stationary kernels, this requires $O(n+m \log m)$ computational complexity. The core idea is to express the original kernel as a combination of interpolation functions and a kernel matrix defined on a set of inducing points. However, despite its effectiveness, popularity (over $600$ citations, a large number for a GP paper) and high quality software availability (\cite{gardner2018gpytorch} has 3.5k stars on github), it currently lacks theoretical analysis. A key initial question is, given a fixed error bound for the SKI Gram matrix and use of cubic convolutional interpolation, how many inducing points are required to achieve that error bound? Given the required value of $m$ as a function of $n$, for what error tolerance is $O(n+m\log m)$ still linear? Following this, what do these errors imply for hyperparameter estimation and posterior inference?

\begin{table*}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Quantity} & \textbf{Bound} \\
\hline
SKI kernel error & $O(\frac{c^{2d}}{m^{3/d}})$ \\
\hline
SKI Gram matrix error & $O(\frac{nc^{2d}}{m^{3/d}})$ \\
\hline
SKI cross-kernel matrix error & $O(\frac{\max(n,T)c^{2d}}{m^{3/d}})$ \\
\hline
SKI score function error & $O(\frac{\sqrt{p}n^{2}c^{4d}}{m^{3/d}})$ \\
\hline
SKI posterior mean error & $O(c^{2d}\frac{\max(T,n)+\sqrt{Tn}n}{m^{3/d}})$ \\
\hline
SKI posterior covariance error & $O(\frac{Tn^{2}mc^{4d}+\sqrt{Tn}mc^{4d}\max(T,n)}{m^{3/d}})$ \\
\hline
\end{tabular}
\caption{Summary of Theoretical Results when using SKI with convolutional cubic interpolation. This shows the rate at which the error of using SKI (vs the exact kernel) grows as a function of important variables. Here $n$ and $T$ are the train/test sample sizes, $d$ is the dimensionality, $m$ the number of inducing points, $p$ is the number of hyperparameters and $c>0$ is a constant. Most importantly, the Gram matrix error grows linearly with the sample size, exponentially with the dimension while decaying at an $m^{3/d}$ rate in the inducing points.}
\label{table:theoretical_results}
\end{table*}

In this paper, we begin to bridge the gap between practice and a theoretical understanding of SKI. We have three primary contributions: 1) The first error analysis for the SKI kernel and relevant quantities, including the SKI gram matrix's spectral norm error. Based on this we provide \textit{a practical guide to select the number of inducing points}: they should grow as $n^{d/3}$ to control error. 2) SKI hyperparameter estimation analysis. 3) SKI inference analysis: the error of the GP posterior means and variances at test points. We find two interesting results: 1) we identify two dimensionality regimes relating SKI Gram matrix error to computational complexity. For $d\leq 3$, for \textit{any} fixed spectral norm error, we can achieve it in linear time using SKI with a sufficient sample size. For $d>3$, the error must \textit{increase} with the sample size to maintain our guarantee of linear time. 2) For a $\mu$-smooth log-likelihood, gradient ascent on the SKI log-likelihood will approach a neighborhood of a stationary point of the true log-likelihood at a $O\left(\frac{1}{K}\right)$ rate, with the neighborhood size determined by the SKI score function's error, which aside from the response variables grows \textit{linearly} with the sample size when increasing inducing points as we suggested. To obtain this, we leverage a recent result \cite{stonyakin2023stopping} from the inexact gradient descent \cite{daspremont2008smooth,devolder2014first} literature.

% By our downstream analysis, this implies that for small dimensionality and kernels exhibiting desirable regularity conditions, we can have arbitrarily small error in the SKI Gram matrix, in our estimated parameters and in posterior inference all in linear time as long as the sample size is sufficiently large.

In section \ref{sec:related} we describe related work. In section \ref{sec:ski-background} we give a brief background on SKI. In section \ref{sec:important-quantities} we bound the error of important quantities: specifically the SKI kernel, Gram matrix and cross-kernel matrix errors. In section \ref{sec:gp-applications} we use these to analyze the error of the SKI MLE and posteriors. We conclude in section \ref{sec:discussion} by summarizing our results and discussing limitations and future work.