
In this paper, we provided the first rigorous theoretical analysis for structured kernel interpolation. A key practical takeaway is that to control the SKI Gram matrix's spectral norm error, the number of inducing points should grow as $n^{d/3}$. Additionally, we showed the spectral norm error of the SKI gram and cross-kernel matrices, and how this impacts achieving a specific error in linear time. We then analyzed kernel hyperparameter estimation, showing that gradient ascent has an iterate approach a ball around a stationary point, where the ball's radius depends on the spectral error of the SKI score function. We concluded with analysis of the error of the SKI posterior mean and variance.

This work could be extended by analyzing the error of SKI with other interpolation schemes such as Lagrange interpolation \citep{lagrange1795lecons}, using potentially higher order polynomials. This would allow us to not only analyze how to vary $m$ for fixed $L=4$, but how to vary them jointly. Additionally, one could analyze the error of SKI in more complex settings, such as when the inducing points are not placed on a regular grid \citep{snelson2006sparse} or for non-stationary kernel functions, in which case the computational complexity would no longer be $O(n+m\log m)$. Further, we analyze the optimization properties under gradient ascent: it would be interesting to analyze it under stochastic gradient ascent, analogous to \cite{lin2024stochastic}, but now using inexact noisy SKI gradients. Finally, one could analyze the methods for extending SKI to higher dimensions \cite{kapoor2021skiing,yadav2022kernel} and for faster SKI inference \cite{yadav2021faster}.

This paper heavily used LLMs: particularly reasoning models. The paper idea and early error analysis of the kernel error and the spectral norm error came from the authors. Beyond that LLMs were used to outline the statements to be made, turn initial rough descriptions into more formal language, and attempt to prove the results. In general, LLM attempts at proofs were \textit{wrong}, but could drive insights into a working proof strategy. We also used the versions with internet access to help bring up relevant papers. While the LLMs sometimes hallucinated papers, the rate at which it did so was quite low and the usefulness of the papers it found was often very high.
