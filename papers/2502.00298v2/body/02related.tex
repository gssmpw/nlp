We can divide related works into three groups: those theoretically analyzing Gaussian process regression or kernel methods when using approximate kernels, SKI and its extensions, and papers developing techniques we use to obtain our guarantees. In the first group, the most relevant works are \cite{burt2019rates,burt2020convergence}, where they analyzed the sparse variational GP framework \citep{titsias2009variational,hensman2013gaussian} and derived bounds on the Kullback-Leibler divergence between the true posterior and the variational approximate posterior. \cite{moreno2023ski} gave bounds on the approximation error of the SKI Gram matrix. However, they only handled the case of univariate features and only bounded how much worse the SKI Gram matrix can be than the Nystr{\"o}m one. Further, they did not analyze the downstream effects on the approximate MLE or GP posterior. Also relevant are \cite{wynne2022variational,wild2021connections}, who gave a Banach space view of sparse variational GPs and connected them to the Nystr{\"o}m method, respectively. Finally, \cite{modell2024entrywise} provide entry-wise error bounds for low-rank approximations of kernel matrices: our approach also relies on entry-wise error bounds, but theirs are for the \textit{best} low-rank approximation to a given gram matrix, while ours are for the SKI gram matrix. Only one of these papers \cite{moreno2023ski} treated SKI specifically, and it only covered a very special case setting.

In the second group, the foundational work by \cite{wilson2015kernel} that we analyze introduced SKI as a scalable method for large-scale GP inference. \cite{kapoor2021skiing} extended SKI to high-dimensional settings using the permutohedral lattice. \cite{yadav2022kernel} developed a sparse grid approach to kernel interpolation that also helps address the curse of dimensionality. Most recently, \cite{ban2024malleable} proposed a flexible adaptation of SKI with a hyperparameter that adjusts the number of grid points based on kernel hyperparameters. We focus our analysis on the original technique of \cite{wilson2015kernel} in this paper, but future work could extend to the settings of the latter papers.

Also relevant are papers where we leverage or extend their results and proof techniques. We require a multivariate extension to the error analysis of \cite{keys1981cubic} for convolutional cubic interpolation, which we derive. We also use a recent result from the inexact gradient descent literature \cite{stonyakin2023stopping}, which allows us to analyze the effect of doing gradient ascent on the SKI log-likelihood instead of the true log-likelihood. Finally, we use a proof technique \cite{bach2013sharp,musco2017recursive} commonly used to bound the in-sample error of approximate kernel ridge regression to bound the test SKI mean function error.

% Further, when analyzing the hyperparameter MLE error of gradient ascent on the SKI log-likelihood, we use a strategy that is commonly used in the expectation maximization theory literature \cite{balakrishnan2017statistical,wu2016convergence} where they decompose the EM algorithm error into a population error, which decays geometrically, and a sample error, which involves a geometric sum. We instead analyze the error of gradient ascent on the SKI log-likelihood. We decompose into gradient ascent on the true log-likelihood, which under mild regularity conditions decays geometrically and the error due to using the SKI log-likelihood, which also involves a geometric sum.

