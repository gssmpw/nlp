\subsubsection{Proof of Proposition \ref{prop:spectral-norm}}\label{sec:proof-spectral-norm}

\spectralnorm*
\begin{proof}
    Recall that for any matrix $\textbf{A}$, $\Vert \textbf{A} \Vert_2 \leq \sqrt{\Vert \textbf{A} \Vert_1 \Vert \textbf{A} \Vert_\infty}$. Since $\textbf{K}-\tilde{\textbf{K}}$ is symmetric, we have
    \begin{align*}
        \Vert \textbf{K}-\tilde{\textbf{K}}\Vert_2&\leq \sqrt{\Vert \textbf{K}-\tilde{\textbf{K}}\Vert_1\Vert \textbf{K}-\tilde{\textbf{K}}\Vert_\infty} = \Vert \textbf{K}-\tilde{\textbf{K}}\Vert_\infty
    \end{align*}
    Furthermore, $\Vert \textbf{K}-\tilde{\textbf{K}}\Vert_\infty$ is the maximum absolute row sum of $\textbf{K}-\tilde{\textbf{K}}$. Since there are $n$ rows and, by Lemma \ref{lemma:ski-kernel-elementwise-error}, each element of $\textbf{K} - \tilde{\textbf{K}}$ is bounded by $\delta_{m,L}+\sqrt{L}c^d\delta_{m,L}$ in absolute value, we have
    \begin{align*}
        \Vert \textbf{K}-\tilde{\textbf{K}}\Vert_\infty &\leq n \left(\delta_{m,L}+\sqrt{L} c^d\delta_{m,L}\right) = \gamma_{n,m,L}.
    \end{align*}
    Therefore, $\Vert \textbf{K}-\tilde{\textbf{K}}\Vert_2 \leq \gamma_{n,m,L}$.
\end{proof}

\subsubsection{Proof of Lemma \ref{lemma:test-train-kernel-matrix-error}}\label{sec:proof-test-train-kernel-matrix-error}
\testtrainkernelmatrixerror*
\begin{proof}
    Using the same reasoning as in Proposition \ref{prop:spectral-norm}, we have
    \begin{align*}
        \Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_2&\leq \sqrt{\Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_1\Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_\infty} \\
        &\leq \max \left(\Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_1, \Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_\infty\right).
    \end{align*}
    Now, $\Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_1$ is the maximum absolute column sum, which is less than or equal to $T(\delta_{m,L} + \sqrt{L}c^d\delta_{m,L}) = \gamma_{T,m,L}$. Similarly, $\Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_\infty$ is the maximum absolute row sum, which is upper bounded by $n(\delta_{m,L} + \sqrt{L}c^d\delta_{m,L}) = \gamma_{n,m,L}$. Therefore,
    $$
    \Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_2 \leq \max(\gamma_{T,m,L},\gamma_{n,m,L}).
    $$
\end{proof}
\subsubsection{Additional Spectral Norm Bounds}
\begin{restatable}{lemma}{traintestbound}\label{lemma:test-train-bound}
    Let $\textbf{K}_{\cdot,\textbf{X}} \in \mathbb{R}^{T \times n}$ be cross kernel matrix between $T$ test points and $n$ training points, where the SKI approximation uses $m$ inducing points. If the kernel function $k$ is bounded such that $|k(\textbf{x}, \textbf{x}')| \leq M$ for all $\textbf{x}, \textbf{x}'\in \mathcal{X}$, then:
    \begin{align*}
        \Vert \textbf{K}_{\cdot,\textbf{X}}\Vert_2&\leq \sqrt{Tn}M
    \end{align*}
\end{restatable}
\begin{proof}
\begin{align*}
    \Vert \textbf{K}_{\cdot,\textbf{X}}\Vert_2&\leq \sqrt{\Vert \textbf{K}_{\cdot,\textbf{X}}\Vert_1\Vert \textbf{K}_{\cdot,\textbf{X}}\Vert_\infty}\\
    &\leq \sqrt{Tn}M
\end{align*}
\end{proof}
\begin{restatable}{lemma}{skitesttrainbound}\label{lemma:ski-test-train-bound}
    Let $\tilde{\textbf{K}}_{\cdot,\textbf{X}} \in \mathbb{R}^{T \times n}$ be the matrix of SKI kernel evaluations between $T$ test points and $n$ training points, where the SKI approximation uses $m$ inducing points. Let $\textbf{W}(\cdot) \in \mathbb{R}^{T \times m}$ and $\textbf{W}(\textbf{X}) \in \mathbb{R}^{n \times m}$ be the matrices of interpolation weights for the test points and training points, respectively. Assume that the interpolation scheme is such that the sum of the absolute values of the interpolation weights for any point is bounded by $c^d$, where $c>0$ is a constant. Let $\textbf{K}_{\textbf{U}} \in \mathbb{R}^{m \times m}$ be the kernel matrix evaluated at the inducing points. If the kernel function $k$ is bounded such that $|k(\textbf{x}, \textbf{x}')| \leq M$ for all $\textbf{x}, \textbf{x}'\in \mathcal{X}$, then:
    $$
    \Vert \tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_2 \leq \sqrt{Tn} m c^{2d} M
    $$
\end{restatable}
\begin{proof}
    By the definition of the SKI approximation and the submultiplicativity of the spectral norm, we have:
    $$
    \Vert \tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_2 = \Vert \textbf{W}(\cdot)\textbf{K}_{\textbf{U}}(\textbf{W}(\textbf{X}))^\top\Vert_2 \leq \Vert \textbf{W}(\cdot)\Vert_2 \Vert\textbf{K}_{\textbf{U}}\Vert_2 \Vert\textbf{W}(\textbf{X})\Vert_2
    $$

    We now bound each term.

    1.  **Bounding $\Vert \textbf{W}(\cdot)\Vert_2$ and $\Vert \textbf{W}(\textbf{X})\Vert_2$:**
        Since the spectral norm is induced by the Euclidean norm, and using the assumption that the sum of absolute values of interpolation weights for any point is bounded by $c^d$, we have
        $$\Vert \textbf{W}(\cdot)\Vert_2 \leq \sqrt{\Vert \textbf{W}(\cdot)\Vert_1 \Vert \textbf{W}(\cdot)\Vert_\infty} \leq \sqrt{T c^d \cdot c^d} = \sqrt{T} c^d.$$
        Similarly, $\Vert\textbf{W}(\textbf{X})\Vert_2 \leq \sqrt{n}c^d$.

    2.  **Bounding $\Vert\textbf{K}_{\textbf{U}}\Vert_2$:**
        Since $\textbf{K}_{\textbf{U}}$ is symmetric, $\Vert \textbf{K}_{\textbf{U}} \Vert_2 \leq \Vert \textbf{K}_{\textbf{U}} \Vert_\infty$. Each entry of $\textbf{K}_{\textbf{U}}$ is bounded by $M$ (by the boundedness of $k$), and each row has $m$ entries, so $\Vert \textbf{K}_{\textbf{U}} \Vert_\infty \leq mM$. Thus, $\Vert\textbf{K}_{\textbf{U}}\Vert_2 \leq mM$.

    Combining these bounds, we get:
    $$
    \Vert \tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_2 \leq (\sqrt{T} c^d) (mM) (\sqrt{n} c^d) = \sqrt{Tn} m c^{2d} M
    $$
    as required.
\end{proof}


% \amnote{might get rid of this}
\begin{lemma}\label{lemma:action-inverse-error}
    Let \(\mathbf{\tilde{K}}\) be the SKI approximation of the kernel matrix \(\mathbf{K}\), and let \(\sigma^2\) be the regularization parameter. The spectral error of the regularized inverse can be bounded as follows:
\begin{align*}
\left\|\left(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}\right)^{-1} - \left(\mathbf{K} + \sigma^2 \mathbf{I}\right)^{-1} \right\|_2 &\leq \frac{\gamma_{n, m, L}}{\sigma^4} 
% \\
% &=\frac{O(n\sqrt{L}\rho^{-L})}{\sigma^4}
\end{align*}
\end{lemma} 

\begin{proof}
    Note that
\[
\left(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}\right)^{-1} - \left(\mathbf{K} + \sigma^2 \mathbf{I}\right)^{-1} = \left(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}\right)^{-1} (\mathbf{K} - \mathbf{\tilde{K}}) \left(\mathbf{K} + \sigma^2 \mathbf{I}\right)^{-1}
\]

Taking the spectral norm, we have
\[
\begin{aligned}
\left\|\left(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}\right)^{-1} - \left(\mathbf{K} + \sigma^2 \mathbf{I}\right)^{-1}\right\|_2 &\leq \left\|\left(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}\right)^{-1}\right\|_2 \|\mathbf{K} - \mathbf{\tilde{K}}\|_2 \left\|\left(\mathbf{K} + \sigma^2 \mathbf{I}\right)^{-1}\right\|_2 \\
&\leq \gamma_{n, m, L}\left\|\left(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}\right)^{-1}\right\|_2 \left\|\left(\mathbf{K} + \sigma^2 \mathbf{I}\right)^{-1}\right\|_2\textrm{ by Proposition \ref{prop:spectral-norm}}\\
&\leq   \frac{\gamma_{n, m, L}}{\sigma^4}
\end{aligned}
\]

\end{proof}
