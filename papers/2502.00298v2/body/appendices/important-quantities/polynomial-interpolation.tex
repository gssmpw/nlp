\subsubsection{Proof of Lemma \ref{lemma:tensor-product-interpolation-error}}\label{sec:proof-tensor-product-interpolation}
\tensorproductinterpolationerror*
\begin{proof}
We define a sequence of intermediate interpolation functions. Let $g_0(\mathbf{x}) \equiv f(\mathbf{x})$ be the original function. For $i = 1, \ldots, d$, we recursively define $g_i(\mathbf{x})$ as the function obtained by interpolating $g_{i-1}$ along the $i$-th dimension using the cubic convolution kernel $u$:

$$
g_i(\mathbf{x}) \equiv \sum_{k=-1}^2 g_{i-1}\left(\mathbf{x} + \left( (\mathbf{c_x})_i - x_i + kh\right)\mathbf{e}_i \right) u\left(\frac{x_i - (\mathbf{c_x})_i - kh}{h}\right).
$$

Here, $\mathbf{c_x}$ is the grid point closest to $\mathbf{x}$, and $\mathbf{e}_i$ is the $i$-th standard basis vector. Thus, $g_1(\mathbf{x})$ interpolates $f$ along the first dimension, $g_2(\mathbf{x})$ interpolates $g_1$ along the second dimension, and so on, until $g_d(\mathbf{x}) = g(\mathbf{x})$ is the final tensor-product interpolated function.

We analyze the error accumulation across multiple dimensions using induction. Using \cite{keys1981cubic}, the error introduced by interpolating a thrice continuous differentiable function along a single dimension with the cubic convolution kernel is uniformly bounded over the interval domain by $Kh^3$ for some constant $K > 0$, provided the grid spacing $h$ is sufficiently small. This gives us the base case:
$$
|g_1(\mathbf{x}) - g_0(\mathbf{x})| \leq Kh^3.
$$

For the inductive step, assume that for some $i=k$ the error is uniformly bounded by
$$
|g_k(\mathbf{x}) - g_{k-1}(\mathbf{x})| \leq c^{k-1}Kh^3.
$$

We want to show that this bound also holds for $i=k+1$. We can express the difference $g_{k+1}(\mathbf{x}) - g_k(\mathbf{x})$ as follows:

\begin{align*}
g_{k+1}(\mathbf{x}) - g_k(\mathbf{x}) &= \sum_{k_{k+1}=-1}^2 g_k\left(\mathbf{x} + ((\mathbf{c_x})_{k+1} - x_{k+1} + k_{k+1}h) \mathbf{e}_{k+1} \right) u\left(\frac{x_{k+1} - (\mathbf{c_x})_{k+1} - k_{k+1}h}{h}\right) \\
&\quad - g_k(\mathbf{x}) \\
&= \sum_{k_{k+1}=-1}^2 \left[\sum_{k_k=-1}^2 g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k + ((\mathbf{c_x})_{k+1} - x_{k+1} + k_{k+1}h) \mathbf{e}_{k+1} \right) \right. \\
&\quad \left. u\left(\frac{x_k - (\mathbf{c_x})_k - k_k h}{h}\right)\right] u\left(\frac{x_{k+1} - (\mathbf{c_x})_{k+1} - k_{k+1}h}{h}\right) \\
&\quad - \sum_{k_k=-1}^2 g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k \right) u\left(\frac{x_k - (\mathbf{c_x})_k - k_k h}{h}\right) \\
&= \sum_{k_k=-1}^2 u\left(\frac{x_k - (\mathbf{c_x})_k - k_k h}{h}\right) \left[\sum_{k_{k+1}=-1}^2 g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k + ((\mathbf{c_x})_{k+1} - x_{k+1} + k_{k+1}h) \mathbf{e}_{k+1} \right) \right. \\
&\quad \left. u\left(\frac{x_{k+1} - (\mathbf{c_x})_{k+1} - k_{k+1}h}{h}\right) - g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k\right)\right].
\end{align*}

The inner term in the last expression represents the difference between interpolating $g_{k-1}$ along the $(k+1)$-th dimension and $g_{k-1}$ itself, evaluated at $\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k$. This can be written as:

\begin{align*}
&\sum_{k_{k+1}=-1}^2 g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k + ((\mathbf{c_x})_{k+1} - x_{k+1} + k_{k+1}h) \mathbf{e}_{k+1} \right) u\left(\frac{x_{k+1} - (\mathbf{c_x})_{k+1} - k_{k+1}h}{h}\right) \\
&\quad - g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k\right) \\
&= g_k\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k \right) - g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k \right).
\end{align*}

Therefore, we can bound the error as follows:

\begin{align*}
|g_{k+1}(\mathbf{x}) - g_k(\mathbf{x})| &\leq \left|\sum_{k_k=-1}^2 u\left(\frac{x_k - (\mathbf{c_x})_k - k_k h}{h}\right)\right| \cdot \left|g_k\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k \right) - g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k \right)\right|.
\end{align*}

Let $c>0$ be a uniform upper bound for $\sum_{k_k=-1}^2 \left|u\left(\frac{x_k - (\mathbf{c_x})_k - k_k h}{h}\right)\right|$, which exists because $u$ is bounded. By the inductive hypothesis, we have $\left|g_k\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k \right) - g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k \right)\right| \leq c^{k-1}Kh^3$. Thus,

$$
|g_{k+1}(\mathbf{x}) - g_k(\mathbf{x})| \leq c \cdot c^{k-1}Kh^3 = c^k Kh^3.
$$

This completes the inductive step.

Finally, we bound the total error $|g(\mathbf{x}) - f(\mathbf{x})| = |g_d(\mathbf{x}) - g_0(\mathbf{x})|$ by summing the errors introduced at each interpolation step:

$$
|g(\mathbf{x}) - f(\mathbf{x})| \leq \sum_{i=1}^d |g_i(\mathbf{x}) - g_{i-1}(\mathbf{x})| \leq \sum_{i=1}^d c^{i-1}Kh^3 = Kh^3 \sum_{i=0}^{d-1} c^i.
$$

The last sum is a geometric series, which evaluates to $Kh^3 \frac{1 - c^d}{1 - c}$. For a fixed $c>1$ (independent of $d$), this expression is $O(c^{d})$ when $d$ is large. Therefore, tensor-product cubic convolutional interpolation has $O(c^d h^3)$ error. Finally, noticing that $h=O\left(\frac{1}{m^{d/3}}\right)$ gives us the desired result.
\end{proof}

\subsubsection{Curse of Dimensionality for Kernel Regression}

The next lemma shows that when using a product kernel for $d$-dimensional kernel regression (where cubic convolutional interpolation is a special case), the sum of weights suffers from the curse of dimensionality. The proof strategy involves expressing the multi-dimensional sum as a product of sums over each individual dimension, leveraging the initial condition on the one-dimensional bound for each dimension, and taking advantage of the structure of the Cartesian grid.


\begin{restatable}{lemma}{cubicInterpolationWeights}
\label{lemma:cubic-interpolation-weights-curse-dimensionality}
Let $u: \mathbb{R} \rightarrow \mathbb{R}$ be a one-dimensional kernel function with constant $c>0$ defined as in \ref{def:sum-weight-upper-bound}. Let $u_d: \mathbb{R}^d \rightarrow \mathbb{R}$ be a d-dimensional product kernel defined as:
$$
u_d\left(\frac{x - x_i}{h}\right) = \prod_{j=1}^d u\left(\frac{x^{(j)} - x_i^{(j)}}{h}\right),
$$
where $x = (x^{(1)}, x^{(2)}, ..., x^{(d)}) \in \mathbb{R}^d$ and $x_i = (x_i^{(1)}, x_i^{(2)}, ..., x_i^{(d)}) \in \mathbb{R}^d$ are d-dimensional points. Assume the data points $\{x_i\}_{i=1}^n$ ($n$ may differ from the univariate case) lie on a fixed d-dimensional grid $G = G^{(1)} \times G^{(2)} \times ... \times G^{(d)}$, where each $G^{(j)} = \{p_1^{(j)}, p_2^{(j)}, ..., p_{n_j}^{(j)}\}$ is a finite set of $n_j$ grid points along the j-th dimension for $j = 1, 2, ..., d$. Then, for any $x \in \mathbb{R}^d$, the sum of weights in the d-dimensional kernel regression is bounded by $c^d$:
$$
\sum_{i=1}^n \left|u_d\left(\frac{x - x_i}{h}\right) \right|\leq c^d.
$$
\end{restatable}

\begin{proof}
Let the fixed d-dimensional grid be defined by the Cartesian product of d sets of 1-dimensional grid points: $G = G^{(1)} \times G^{(2)} \times ... \times G^{(d)}$, where $G^{(j)} = \{p_1^{(j)}, p_2^{(j)}, ..., p_{n_j}^{(j)}\}$ is the set of grid points along the j-th dimension.

We start with the sum of weights in the d-dimensional case:

$$
\sum_{i=1}^n u_d\left(\frac{x - x_i}{h}\right) = \sum_{i=1}^n \prod_{j=1}^d u\left(\frac{x^{(j)} - x_i^{(j)}}{h}\right)
$$

Since the data points lie on the fixed grid $G$, we can rewrite the outer sum as a nested sum over the grid points in each dimension:

$$
\sum_{i=1}^n \prod_{j=1}^d u\left(\frac{x^{(j)} - x_i^{(j)}}{h}\right) = \sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_d=1}^{n_d} \prod_{j=1}^d u\left(\frac{x^{(j)} - p_{k_j}^{(j)}}{h}\right)
$$

Now we can change the order of summation and product, as proven in Lemma \ref{lemma:switch-sum-product}:

$$
\sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_d=1}^{n_d} \prod_{j=1}^d u\left(\frac{x^{(j)} - p_{k_j}^{(j)}}{h}\right) = \prod_{j=1}^d \left( \sum_{k_j=1}^{n_j} u\left(\frac{x^{(j)} - p_{k_j}^{(j)}}{h}\right) \right)
$$

By the assumption of the lemma, we know that for each dimension $j$, the sum of weights is bounded by $c$. Note that $\{p_{k_j}^{(j)}\}_{k_j=1}^{n_j}$ is simply a set of points in $\mathbb{R}$, thus:

$$
\sum_{k_j=1}^{n_j} \left|u\left(\frac{x^{(j)} - p_{k_j}^{(j)}}{h}\right)\right| \leq c
$$

Therefore, we have:

$$
\prod_{j=1}^d \left(\left| \sum_{k_j=1}^{n_j} u\left(\frac{x^{(j)} - p_{k_j}^{(j)}}{h}\right)\right| \right) \leq \prod_{j=1}^d c = c^d
$$

Thus, we have shown that:

$$
\sum_{i=1}^n \left|u_d\left(\frac{x - x_i}{h}\right) \right|\leq c^d
$$

\end{proof}

