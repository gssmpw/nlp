\subsubsection{Proof of Lemma \ref{lemma:ski-kernel-elementwise-error}}\label{sec:proof-ski-kernel-elementwise-error}
\skikernelelementwiseerror*
\begin{proof}
Recall that SKI approximates the kernel as
\begin{align*}
k(\mathbf{x}, \mathbf{x}') &\approx \tilde{k}(\mathbf{x}, \mathbf{x}')\\
&= \boldsymbol{w}(\mathbf{x})^\top \mathbf{K}_{\textbf{U}} \boldsymbol{w}(\mathbf{x}'),
\end{align*}

Let $\textbf{K}_{\textbf{U},\textbf{x}'}\in \mathbb{R}^m$ be the vector of kernels between the inducing points and the vector $\textbf{x}'$
\begin{align}
\vert k(\mathbf{x}, \mathbf{x}') -\tilde{k}(\mathbf{x}, \mathbf{x}')\vert &= \vert k(\mathbf{x}, \mathbf{x}')-\boldsymbol{w}(\mathbf{x})^\top \textbf{K}_{\textbf{U},\textbf{x}'} +\boldsymbol{w}(\mathbf{x})^\top \textbf{K}_{\textbf{U},\textbf{x}'}-\boldsymbol{w}(\mathbf{x})^\top \mathbf{K}_{\textbf{U}} \boldsymbol{w}(\mathbf{x}')\vert\nonumber\\
&\leq  \vert k(\mathbf{x}, \mathbf{x}')-\boldsymbol{w}(\mathbf{x})^\top \textbf{K}_{\textbf{U},\textbf{x}'} \vert+\vert \boldsymbol{w}(\mathbf{x})^\top \textbf{K}_{\textbf{U},\textbf{x}'}-\boldsymbol{w}(\mathbf{x})^\top \mathbf{K}_{\textbf{U}} \boldsymbol{w}(\mathbf{x}')\vert\nonumber\\
&\leq \delta_{m,L}+\vert \boldsymbol{w}(\mathbf{x})^\top \textbf{K}_{\textbf{U},\textbf{x}'}-\boldsymbol{w}(\mathbf{x})^\top \mathbf{K}_{\textbf{U}} \boldsymbol{w}(\mathbf{x}')\vert \textrm{ since $\vert k(\mathbf{x}, \mathbf{x}')-\boldsymbol{w}(\mathbf{x})^\top \textbf{K}_{\textbf{U},\textbf{x}'} \vert$ is a single polynomial interpolation}\label{eqn:applying-single-poly-interp}
\end{align}
Now note that $\textbf{w}(x)\in \mathbb{R}^m$ is a sparse matrix with at most $L$ non-zero entries. Thus, letting $\tilde{\textbf{w}}(x)\in \mathbb{R}^L$ be the non-zero entries of $\textbf{w}(x)$ and similarly $\tilde{\textbf{K}}_{\textbf{U},\textbf{x}'}\in \mathbb{R}^L$ be the entries of $\textbf{K}_{\textbf{U},\textbf{x}'}$ in the dimensions corresponding to non-zero entries of $\textbf{w}(x)\in \mathbb{R}^m$, while $\tilde{\textbf{K}}_{\textbf{U}}\in \mathbb{R}^{L\times m}$ is the analogous matrix for $\textbf{K}_{\textbf{U}}$, we have
\begin{align}
    \vert \boldsymbol{w}(\mathbf{x})^\top \textbf{K}_{\textbf{U},\textbf{x}'}-\boldsymbol{w}(\mathbf{x})^\top \mathbf{K}_{\textbf{U}} \boldsymbol{w}(\mathbf{x}')\vert&=\vert \tilde{\textbf{w}}(\textbf{x})^\top \tilde{\textbf{K}}_{\textbf{U},\textbf{x}'}-\tilde{\textbf{w}}(\textbf{x})^\top \tilde{\textbf{K}}_\textbf{U}\textbf{w}(\textbf{x}')\vert\nonumber\\
    &\leq \Vert \tilde{\textbf{w}}(\textbf{x})\Vert_2\Vert \tilde{\textbf{K}}_{\textbf{U},\textbf{x}'}-\tilde{\mathbf{K}}_{\textbf{U}} \boldsymbol{w}(\mathbf{x}')\Vert_2\nonumber\\
    &\leq c^d\sqrt{L}\Vert \tilde{\textbf{K}}_{\textbf{U},\textbf{x}'}-\tilde{\mathbf{K}}_{\textbf{U}} \boldsymbol{w}(\mathbf{x}')\Vert_\infty\nonumber\text{ Lemma \ref{lemma:cubic-interpolation-weights-curse-dimensionality}}\\
    &\leq \sqrt{L}c^d\delta_{m,L}\label{eqn:applying-sparsity-single-kernel-evaluation}
\end{align}
where the last line follows as each element of $\mathbf{K}_{\textbf{U}} \boldsymbol{w}(\mathbf{x}')$ is a polynomial interpolation approximating each element of $\textbf{K}_{\textbf{U},\textbf{x}'}$. Plugging Eqn. \ref{eqn:applying-sparsity-single-kernel-evaluation} into Eqn. \ref{eqn:applying-single-poly-interp} gives us the desired initial result of
\begin{align*}
    \vert k(\textbf{x},\textbf{x}')-\tilde{k}(\textbf{x},\textbf{x}')\vert&\leq \delta_{m,L}+\sqrt{L}c^d\delta_{m,L}
\end{align*}
and Lemma \ref{lemma:tensor-product-interpolation-error} gives us the result when the convolutional kernel is cubic.
\end{proof}
