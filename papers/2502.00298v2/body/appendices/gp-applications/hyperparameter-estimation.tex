\subsubsection{Proof of Lemma \ref{lemma:ski_kernel_derivative_error_kernel}}\label{sec:proofski_kernel_derivative_error_kernelz}

\skikernelderivativeerrorkernel*
\begin{proof}
By assumption, $k'_{\theta_i}(x, x') = \frac{\partial k_{\theta}(x, x')}{\partial \theta_i}$ is a valid SPD kernel. The SKI approximation of $k'_{\theta_i}(x, x')$ using the same inducing points and interpolation scheme as $\tilde{k}_{\theta}(x, x')$ is given by $\tilde{k}'_{\theta}(x, x')$. For the kernel $k'_{\theta_i}(x, x')$, we have:

\begin{align*}
\left\vert k'_{\theta_i}(x, x') - \tilde{k}'_{\theta}(x, x') \right\vert \leq \delta_{m,L}',
\end{align*}

where $\delta_{m,L}'$ is the upper bound on the error of the SKI approximation of $k'_{\theta_i}(x, x')$ as defined in Lemma \ref{lemma:ski-kernel-elementwise-error}.

Now, we need to show that $\frac{\partial \tilde{k}_{\theta}(x,x')}{\partial \theta_i} = \tilde{k}'_{\theta}(x, x')$. Recall that the SKI approximation $\tilde{k}_{\theta}(x, x')$ is a linear combination of kernel evaluations at inducing points, with weights that depend on $x$ and $x'$:

\begin{align*}
\tilde{k}_{\theta}(x, x') = \sum_{j=1}^m \sum_{l=1}^m w_{jl}(x, x') k_{\theta}(u_j, u_l)
\end{align*}


where $w_{jl}(x, x')$ are the interpolation weights. Taking the partial derivative with respect to $\theta_i$, we get:
\begin{align*}
\frac{\partial \tilde{k}_{\theta}(x, x')}{\partial \theta_i} &= \sum_{j=1}^m \sum_{l=1}^m w_{jl}(x, x') \frac{\partial k_{\theta}(u_j, u_l)}{\partial \theta_i} \\
&= \sum_{j=1}^m \sum_{l=1}^m w_{jl}(x, x') k'_{\theta_i}(u_j, u_l).
\end{align*}


This is precisely the SKI approximation of the kernel $k'_{\theta_i}(x, x')$ using the same inducing points and weights:
\begin{align*}
\tilde{k}'_{\theta}(x, x') = \sum_{j=1}^m \sum_{l=1}^m w_{jl}(x, x') k'_{\theta_i}(u_j, u_l).
\end{align*}

Therefore, $\frac{\partial \tilde{k}_{\theta}(x,x')}{\partial \theta_i} = \tilde{k}'_{\theta}(x, x')$.

Substituting this into our inequality, we get:
\begin{align*}
\left\vert \frac{\partial k_{\theta}(x,x')}{\partial \theta_i}-\frac{\partial \tilde{k}_{\theta}(x,x')}{\partial \theta_i}\right\vert &= \left\vert k'_{\theta_i}(x, x') - \tilde{k}'_{\theta}(x, x') \right\vert \\
&\leq \delta_{m,L}'+\sqrt{L}c^d\delta_{m,L}'.
\end{align*}

\end{proof}

\subsubsection{Proof of Lemma \ref{lemma:partial_gradient_spectral_norm_bound}}\label{section:proof_partial_gradient_spectral_norm_bound}
\partialgradientspectralnormbound*
\begin{proof}
Let $K'_{\theta,l}$ be the kernel matrix corresponding to the kernel $k'_{\theta,l}(x,x') = \frac{\partial k_{\theta}(x,x')}{\partial \theta_l}$, and let $\tilde{K}'_{\theta,l}$ be the kernel matrix corresponding to its SKI approximation $\tilde{k}'_{\theta,l}(x,x')$.

From Lemma \ref{lemma:ski_kernel_derivative_error_kernel}, we have:

\begin{equation}
\frac{\partial \tilde{k}_{\theta}(x,x')}{\partial \theta_l} = \tilde{k}'_{\theta,l}(x, x')
\end{equation}

Therefore:
\begin{equation}
\frac{\partial K}{\partial \theta_l} - \frac{\partial \tilde{K}}{\partial \theta_l} = K'_{\theta,l} - \tilde{K}'_{\theta,l}
\end{equation}

By Proposition \ref{prop:spectral-norm}, we have a bound on the spectral norm difference between a kernel matrix and its SKI approximation. Let $\gamma'_{n,m,L,l}$ be the corresponding bound for the kernel $k'_{\theta,l}$ and its SKI approximation $\tilde{k}'_{\theta,l}$. Then:

\begin{equation}
\| K'_{\theta,l} - \tilde{K}'_{\theta,l} \|_2 \leq \gamma'_{n,m,L,l}
\end{equation}

Thus,

\begin{align*}
\left\| \frac{\partial K}{\partial \theta_l} - \frac{\partial \tilde{K}}{\partial \theta_l} \right\|_2 = \| K'_{\theta,l} - \tilde{K}'_{\theta,l} \|_2 \leq \gamma'_{n,m,L,l}
\end{align*}

This completes the proof.
\end{proof}

% \subsubsection{Proof of Lemma \ref{lemma:gradient_spectral_norm_bound}}\label{sec:proof_gradient_spectral_norm_bound}
% \gradientspectralnormbound*
% \begin{proof}
%     We represent the gradients $\nabla_\theta K$ and $\nabla_\theta \tilde{K}$ as $p \times n^2$ matrices using the vec-notation (denominator layout):
% \begin{align*}
% \nabla_\theta K = \begin{bmatrix} \frac{\partial \text{vec}(K)}{\partial \theta_1} & \frac{\partial \text{vec}(K)}{\partial \theta_2} & \cdots & \frac{\partial \text{vec}(K)}{\partial \theta_p} \end{bmatrix}^T, \\
% \nabla_\theta \tilde{K} = \begin{bmatrix} \frac{\partial \text{vec}(\tilde{K})}{\partial \theta_1} & \frac{\partial \text{vec}(\tilde{K})}{\partial \theta_2} & \cdots & \frac{\partial \text{vec}(\tilde{K})}{\partial \theta_p} \end{bmatrix}^T.
% \end{align*}

% Let $K'_{\theta,i}$ be the kernel matrix corresponding to the kernel $k'_{\theta,i}(x,x') = \frac{\partial k_{\theta}(x,x')}{\partial \theta_i}$, and let $\tilde{K}'_{\theta,i}$ be the kernel matrix corresponding to its SKI approximation $\tilde{k}'_{\theta,i}(x,x')$.

% From Lemma \ref{lemma:ski_kernel_derivative_error_kernel}, we have:

% \begin{equation}
% \frac{\partial \tilde{k}_{\theta}(x,x')}{\partial \theta_i} = \tilde{k}'_{\theta,i}(x, x')
% \end{equation}

% Thus, the $i$-th row of $\nabla_\theta K - \nabla_\theta \tilde{K}$ can be written as:

% \begin{equation}
% \frac{\partial \text{vec}(K)}{\partial \theta_i} - \frac{\partial \text{vec}(\tilde{K})}{\partial \theta_i} = \text{vec}(K'_{\theta,i}) - \text{vec}(\tilde{K}'_{\theta,i}).
% \end{equation}

% Therefore, we can write:
% \begin{equation}
%     \nabla_\theta K - \nabla_\theta \tilde{K} = \begin{bmatrix}
%     \text{vec}(K'_{\theta,1}) - \text{vec}(\tilde{K}'_{\theta,1}) \\
%     \vdots \\
%     \text{vec}(K'_{\theta,p}) - \text{vec}(\tilde{K}'_{\theta,p})
%     \end{bmatrix}^T
% \end{equation}
% Then, we have that:
% \begin{align}
%     (\nabla_\theta K - \nabla_\theta \tilde{K})^T = \begin{bmatrix}
%     \text{vec}(K'_{\theta,1}) - \text{vec}(\tilde{K}'_{\theta,1}) &
%     \cdots &
%     \text{vec}(K'_{\theta,p}) - \text{vec}(\tilde{K}'_{\theta,p})
%     \end{bmatrix}
% \end{align}

% Now we bound the spectral norm of $(\nabla_\theta K - \nabla_\theta \tilde{K})^T$:

% \begin{align*}
% \| (\nabla_\theta K - \nabla_\theta \tilde{K})^T \|_2 &\leq \| (\nabla_\theta K - \nabla_\theta \tilde{K})^T \|_F \\
% &= \left( \sum_{i=1}^p \| \text{vec}(K'_{\theta,i}) - \text{vec}(\tilde{K}'_{\theta,i}) \|_2^2 \right)^{1/2} \\
% &= \left( \sum_{i=1}^p \| K'_{\theta,i} - \tilde{K}'_{\theta,i} \|_F^2 \right)^{1/2} \\
% &\leq \left( \sum_{i=1}^p \left( \max_{j \in \{1,\ldots,p\}} \| K'_{\theta,j} - \tilde{K}'_{\theta,j} \|_2 \right)^2 \right)^{1/2} 
% \end{align*}

% \jmei{the term in the sum is already independent of $i$, so we could replace the sum by $p\times$ and then swap the norm for $\gamma$.}
% By Proposition \ref{prop:spectral-norm}, we have a bound on the spectral norm difference between a kernel matrix and its SKI approximation.  Let $\gamma'_{n,m,L,i}$ be the corresponding bound for the kernel $k'_{\theta,i}$ and its SKI approximation $\tilde{k}'_{\theta,i}$. Then:

% \begin{equation}
% \| K'_{\theta,i} - \tilde{K}'_{\theta,i} \|_2 \leq \gamma'_{n,m,L,i}
% \end{equation}

% for each $i \in \{1, \ldots, p\}$. Thus,

% \begin{align*}
% \| (\nabla_\theta K - \nabla_\theta \tilde{K})^T \|_2 &\leq \left( \sum_{i=1}^p (\gamma'_{n,m,L,i})^2 \right)^{1/2} \\
% &\leq \left( \sum_{i=1}^p \left(\max_{j \in \{1,\ldots,p\}} \gamma'_{n,m,L,j}\right)^2 \right)^{1/2} \\
% &= \left( p \left(\max_{j \in \{1,\ldots,p\}} \gamma'_{n,m,L,j}\right)^2 \right)^{1/2} \\
% &= \sqrt{p} \max_{j \in \{1,\ldots,p\}} \gamma'_{n,m,L,j}
% \end{align*}

% This completes the proof.
% \end{proof}

\subsubsection{Proof of Lemma \ref{lemma:score-function-bound}}\label{sec:proof-score-function-bound}
\scorefunctionbound*
\begin{proof}
    We start with the expressions for the gradients:

$$
\nabla \mathcal{L}(\theta) = \nabla \left( -\frac{1}{2} \mathbf{y}^\top (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log |\mathbf{K} + \sigma^2 \mathbf{I}| - \frac{n}{2} \log(2\pi) \right).
$$

$$
\nabla \tilde{\mathcal{L}}(\theta) = \nabla \left( -\frac{1}{2} \mathbf{y}^\top (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log |\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| - \frac{n}{2} \log(2\pi) \right).
$$

Thus, the difference is:

\begin{align*}
\| \nabla \mathcal{L}(\theta) - \nabla \tilde{\mathcal{L}}(\theta) \|_2 &= \left\| \nabla \left( -\frac{1}{2} \mathbf{y}^\top (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log |\mathbf{K} + \sigma^2 \mathbf{I}| \right) \right. \\
&\quad \left. - \nabla \left( -\frac{1}{2} \mathbf{y}^\top (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log |\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| \right) \right\|_2 \\
&\leq \underbrace{\left\| \nabla \left( \frac{1}{2} \mathbf{y}^\top \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right) \mathbf{y} \right) \right\|_2}_{T_1} \\
&\quad + \underbrace{\left\| \frac{1}{2} \nabla \left( \log |\mathbf{K} + \sigma^2 \mathbf{I}| - \log |\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| \right) \right\|_2}_{T_2}.
\end{align*}

We will bound $T_1$ and $T_2$ separately.

\textbf{Bounding $T_1$:}



\begin{align*}
    T_1 &= \frac{1}{2} \left\| \nabla_\theta \left( \mathbf{y}^\top \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right) \mathbf{y} \right) \right\|_2\\
    &=\frac{1}{2}\sqrt{\sum_{l=1}^p \left(\frac{\partial}{\partial \theta_l}\mathbf{y}^\top \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right) \mathbf{y}\right)^2}\\
    &\leq \frac{1}{2}\sqrt{p}\max_{1\leq l\leq p}\sqrt{\left(\frac{\partial}{\partial \theta_l}\mathbf{y}^\top \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right) \mathbf{y}\right)^2}\\
    &=\frac{1}{2}\sqrt{p}\max_{1\leq l\leq p} \left\vert \frac{\partial}{\partial \theta_l}\mathbf{y}^\top \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right) \mathbf{y}\right\vert
\end{align*}
We will then bound $\left\vert \frac{\partial}{\partial \theta_l}\mathbf{y}^\top \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right) \mathbf{y}\right\vert$. Using the following equality $\frac{\partial}{\partial \theta_l} \mathbf{X}^{-1} = -\mathbf{X}^{-1} (\frac{\partial \mathbf{X}}{\partial \theta_l}) \mathbf{X}^{-1}$, we can express this derivative as a quadratic form as a difference between two quadratic forms and apply standard techniques for bounding differences between quadratic forms.
% \begin{align*}
%     &\left\vert \frac{\partial}{\partial \theta_l}\mathbf{y}^\top \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right) \mathbf{y}\right\vert\\
%     &\leq\Vert \textbf{y}\Vert_2^2 \left\Vert \frac{\partial}{\partial \theta_l }\left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right) \right\Vert_2\text{ CS inequality}\\
%     &=\Vert \textbf{y}\Vert_2^2  \left\Vert \frac{\partial}{\partial \theta_l }\left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right) \right\Vert_2\\
%     &= \Vert \textbf{y}\Vert_2^2 \left\|  - (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial}{\partial \theta_l} \tilde{\mathbf{K}}\right) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} + (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial}{\partial \theta_l} \mathbf{K}\right) (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}  \right\|_2\text{ above equality}\\
%     &\leq  \frac{1}{\sigma^4}\Vert \textbf{y}\Vert_2^2\left(\left\Vert\frac{\partial}{\partial \theta_l} \tilde{\mathbf{K}}\right\Vert+\left\Vert\frac{\partial}{\partial \theta_l} \mathbf{K}\right\Vert\right)\text{ CS inequality}
% \end{align*}

% Since the kernel is $C^1$ wrt $\theta$ and $\mathcal{D}$ is compact, we can bound the entries of $\frac{\partial}{\partial \theta_l} \tilde{\mathbf{K}}$ and $\frac{\partial}{\partial \theta_l} \mathbf{K}$ uniformly over all $1\leq l\leq p$ with constants $\tilde{C},C>0$. Recalling the property $\Vert \cdot\Vert_2\leq \sqrt{\Vert \cdot\Vert_1\Vert \cdot\Vert_\infty}$, we can bound $T_1$ as:
\begin{align*}
    &\left\vert \frac{\partial}{\partial \theta_l}\mathbf{y}^\top \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right) \mathbf{y}\right\vert\\
    &\leq\Vert \textbf{y}\Vert_2^2 \left\Vert \frac{\partial}{\partial \theta_l }\left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right) \right\Vert_2\text{ CS inequality}\\
    &=\Vert \textbf{y}\Vert_2^2  \left\Vert \frac{\partial}{\partial \theta_l }\left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right) \right\Vert_2\\
    &= \Vert \textbf{y}\Vert_2^2 \left\|  - (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial}{\partial \theta_l} \tilde{\mathbf{K}}\right) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} + (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial}{\partial \theta_l} \mathbf{K}\right) (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}  \right\|_2 \\
    &= \Vert \textbf{y}\Vert_2^2 \left\|  - (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial}{\partial \theta_l} \tilde{\mathbf{K}} - \frac{\partial}{\partial \theta_l} \mathbf{K} + \frac{\partial}{\partial \theta_l} \mathbf{K}\right) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right.\\
    &\quad \left. + (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \frac{\partial}{\partial \theta_l} \mathbf{K} (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}  \right\|_2 \\
    &= \Vert \textbf{y}\Vert_2^2 \left\|  - (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial}{\partial \theta_l} \tilde{\mathbf{K}} - \frac{\partial}{\partial \theta_l} \mathbf{K}\right) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right.\\
    &\quad \left. - (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial}{\partial \theta_l} \mathbf{K}\right) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1}  + (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial}{\partial \theta_l} \mathbf{K}\right) (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}  \right\|_2 \\
    &\leq\Vert \textbf{y}\Vert_2^2  \left(\underbrace{\left\| (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial}{\partial \theta_l} \tilde{\mathbf{K}} - \frac{\partial}{\partial \theta_l} \mathbf{K}\right) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right\|_{2}}_{(a)} \right.\\
    &\quad \left.+ \underbrace{\left\| \left((\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\right)\left(\frac{\partial}{\partial \theta_l} \mathbf{K}\right) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right\|_{2}}_{(b)} \right.\\
    &\quad \left.+ \underbrace{\left\|(\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial}{\partial \theta_l} \mathbf{K}\right)\left((\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\right) \right\|_{2}}_{(c)}\right).
\end{align*}



We now explicitly bound (a), (b), and (c).
\begin{align*}
(a) &\leq \left\| (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right\|_2 \left\| \frac{\partial}{\partial \theta_l} \tilde{\mathbf{K}} - \frac{\partial}{\partial \theta_l} \mathbf{K} \right\|_2 \left\| (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right\|_2\\
&\leq \left\| (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right\|_2^2 \left\| \frac{\partial}{\partial \theta_l} \tilde{\mathbf{K}} - \frac{\partial}{\partial \theta_l} \mathbf{K} \right\|_2 \\
&\leq \frac{1}{\sigma^4} \left\| \frac{\partial}{\partial \theta_l} \mathbf{K} - \frac{\partial}{\partial \theta_l} \tilde{\mathbf{K}} \right\|_2\\
&\leq  \frac{1}{\sigma^4} \gamma'_{n,m,L,l} \quad \text{ (Using Lemma \ref{lemma:partial_gradient_spectral_norm_bound})}
\end{align*}

\begin{align*}
(b) &\leq \|(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\|_2 \left\|\frac{\partial}{\partial \theta_l} \mathbf{K}\right\|_2 \|(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1}\|_2 \\
&\leq \frac{1}{\sigma^2} \|(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\|_2 \left\|\frac{\partial}{\partial \theta_l} \mathbf{K}\right\|_2 \\
&\leq \frac{\gamma_{n,m,L}}{\sigma^4} \left\|\frac{\partial}{\partial \theta_l} \mathbf{K}\right\|_2 \quad \text{(Using Lemma \ref{lemma:action-inverse-error})}
\end{align*}
Since the kernel is $C^1$ wrt $\theta$ and $\mathcal{D}$ is compact, we can bound the entries of $\frac{\partial}{\partial \theta_l} \mathbf{K}$ uniformly over $\mathcal{D}$ and $l$ with some constant, say $C>0$. Then by Lemma \ref{lemma:test-train-bound}, reusing the training points instead of using the test points,
\begin{align*}
    (b)&\leq \frac{\gamma_{n,m,L}}{\sigma^4} \left\|\frac{\partial}{\partial \theta_l} \mathbf{K}\right\|_2\\
%    &\leq \frac{\gamma_{n,m,L}}{\sigma^4} \sqrt{\Vert \frac{\partial}{\partial \theta_l} \mathbf{K}\Vert_1\Vert \frac{\partial}{\partial \theta_l} \mathbf{K}\Vert_\infty}\\
    &\leq Cn\frac{\gamma_{n,m,L}}{\sigma^4}
\end{align*}
and finally 
\begin{align*}
(c) &\leq \|(\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\|_2 \left\|\frac{\partial}{\partial \theta_l} \mathbf{K}\right\|_2 \|(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\|_2 \\
&\leq \frac{1}{\sigma^2} \|(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\|_2 \left\|\frac{\partial}{\partial \theta_l} \mathbf{K}\right\|_2 \\
&\leq \frac{\gamma_{n,m,L}}{\sigma^4} \left\|\frac{\partial}{\partial \theta_l} \mathbf{K}\right\|_2 \quad \text{(Using Lemma \ref{lemma:action-inverse-error})} \\
&\leq \frac{\gamma_{n,m,L}}{\sigma^4} \gamma'_{n,m,L,l} \quad \text{(Using Lemma \ref{lemma:partial_gradient_spectral_norm_bound} )}
\end{align*}

Combining these, we obtain
\begin{align*}
    T_1 &\leq \frac{1}{2\sigma^4}\Vert \textbf{y}\Vert\sqrt{p}\max_{1\leq l\leq p} \left( \gamma'_{n,m,L,l}+Cn\gamma_{n,m,L}+\gamma_{n,m,L}\gamma'_{n,m,L,l} \right)
\end{align*}

% \begin{align*}
%     T_1&\leq \frac{\Vert \textbf{y}\Vert_2^2 n}{2\sigma^4}\sqrt{p}\left(\tilde{C}+C\right)
% \end{align*}

**Bounding $T_2$:**

Using the identity $\nabla \log |\mathbf{X}| = (\mathbf{X}^{-1})^\top$, we have

\begin{align*}
T_2 &= \frac{1}{2} \left\| \nabla_\theta \left( \log |\mathbf{K} + \sigma^2 \mathbf{I}| - \log |\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| \right) \right\|_2 \\
&= \frac{1}{2} \left\| (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} - (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right\|_2.
\end{align*}
We can rewrite the difference as:
$$
(\mathbf{K} + \sigma^2 \mathbf{I})^{-1} - (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} = (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} (\tilde{\mathbf{K}} - \mathbf{K}) (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}
$$
Then
\begin{align*}
T_2 &\leq \frac{1}{2} \| (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \|_2 \| \tilde{\mathbf{K}} - \mathbf{K} \|_2 \| (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \|_2 \\
    &\leq \frac{\gamma_{n,m,L}}{2\sigma^4}
\end{align*}

**Combining the Bounds:**

Combining the bounds for $T_1$ and $T_2$, we have

\begin{align*}
\| \nabla \mathcal{L}(\boldsymbol{\theta}) - \nabla \tilde{\mathcal{L}}(\boldsymbol{\theta}) \|_2 &\leq \frac{1}{2\sigma^4}\Vert \textbf{y}\Vert\sqrt{p}\max_{1\leq l\leq p} \left( \gamma'_{n,m,L,l}+Cn\gamma_{n,m,L}+\gamma_{n,m,L}\gamma'_{n,m,L,l} \right)+\frac{\gamma_{n,m,L}}{2\sigma^4}
\end{align*}
\end{proof}
