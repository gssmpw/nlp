
\begin{lemma}\label{lemma:switch-sum-product}
Given a function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ of the form $f(x_1, x_2, ..., x_d) = \prod_{j=1}^d f_j(x_j)$, where each $f_j: \mathbb{R} \rightarrow \mathbb{R}$. Let $G = G^{(1)} \times G^{(2)} \times ... \times G^{(d)}$ be a fixed d-dimensional grid, where each $G^{(j)} = \{p_1^{(j)}, p_2^{(j)}, ..., p_{n_j}^{(j)}\}$ is a finite set of $n_j$ grid points along the j-th dimension for $j = 1, 2, ..., d$. Then the following equality holds:

$$
\sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_d=1}^{n_d} \prod_{j=1}^d f_j(p_{k_j}^{(j)}) = \prod_{j=1}^d \left( \sum_{k_j=1}^{n_j} f_j(p_{k_j}^{(j)}) \right)
$$
\end{lemma}
\begin{proof}
\textbf{By Induction on d (the number of dimensions):}

\textbf{Base Case (d = 1):}

When $d=1$, the statement becomes:

$$
\sum_{k_1=1}^{n_1} f_1(p_{k_1}^{(1)}) = \sum_{k_1=1}^{n_1} f_1(p_{k_1}^{(1)})
$$

This is trivially true.

\textbf{Inductive Hypothesis:}

Assume the statement holds for $d = m$, i.e.,

$$
\sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_m=1}^{n_m} \prod_{j=1}^m f_j(p_{k_j}^{(j)}) = \prod_{j=1}^m \left( \sum_{k_j=1}^{n_j} f_j(p_{k_j}^{(j)}) \right)
$$

\textbf{Inductive Step:}

We need to show that the statement holds for $d = m+1$. Consider the left-hand side for $d = m+1$:

$$
\sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_{m+1}=1}^{n_{m+1}} \prod_{j=1}^{m+1} f_j(p_{k_j}^{(j)})
$$

We can rewrite this as:

$$
\sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_m=1}^{n_m} \left( \sum_{k_{m+1}=1}^{n_{m+1}} \left( \prod_{j=1}^m f_j(p_{k_j}^{(j)}) \right) f_{m+1}(p_{k_{m+1}}^{(m+1)}) \right)
$$

Notice that the inner sum (over $k_{m+1}$) does not depend on $k_1, k_2, ..., k_m$. Thus, for any fixed values of $k_1, k_2, ..., k_m$, we can treat $\prod_{j=1}^m f_j(p_{k_j}^{(j)})$ as a constant. Let $C(k_1, ..., k_m) = \prod_{j=1}^m f_j(p_{k_j}^{(j)})$. Then we have:

$$
\sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_m=1}^{n_m}  \left( C(k_1, ..., k_m) \sum_{k_{m+1}=1}^{n_{m+1}} f_{m+1}(p_{k_{m+1}}^{(m+1)}) \right)
$$

Now, the inner sum $\sum_{k_{m+1}=1}^{n_{m+1}} f_{m+1}(p_{k_{m+1}}^{(m+1)})$ is a constant with respect to $k_1, ..., k_m$. Let's call this constant $S_{m+1}$. So we have:

$$
\sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_m=1}^{n_m} C(k_1, ..., k_m) S_{m+1} = S_{m+1} \sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_m=1}^{n_m}  \prod_{j=1}^m f_j(p_{k_j}^{(j)})
$$

By the inductive hypothesis, we can replace the nested sums with a product:

$$
S_{m+1} \prod_{j=1}^m \left( \sum_{k_j=1}^{n_j} f_j(p_{k_j}^{(j)}) \right) = \left( \sum_{k_{m+1}=1}^{n_{m+1}} f_{m+1}(p_{k_{m+1}}^{(m+1)}) \right) \prod_{j=1}^m \left( \sum_{k_j=1}^{n_j} f_j(p_{k_j}^{(j)}) \right)
$$

Rearranging the terms, we get:

$$
\prod_{j=1}^m \left( \sum_{k_j=1}^{n_j} f_j(p_{k_j}^{(j)}) \right) \left( \sum_{k_{m+1}=1}^{n_{m+1}} f_{m+1}(p_{k_{m+1}}^{(m+1)}) \right) = \prod_{j=1}^{m+1} \left( \sum_{k_j=1}^{n_j} f_j(p_{k_j}^{(j)}) \right)
$$

This is the right-hand side of the statement for $d = m+1$. Thus, the statement holds for $d = m+1$.

\textbf{Conclusion:}

By induction, the statement holds for all $d \geq 1$. Therefore,

$$
\sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_d=1}^{n_d} \prod_{j=1}^d f_j(p_{k_j}^{(j)}) = \prod_{j=1}^d \left( \sum_{k_j=1}^{n_j} f_j(p_{k_j}^{(j)}) \right)
$$

\end{proof}


\begin{claim}\label{claim:convex-combo-eigenvalues}
Given a convex combination \(\mathbf{C} = \alpha \mathbf{A} + (1-\alpha) \mathbf{B}\), where \(\alpha \in [0,1]\), and \(\mathbf{A}\) and \(\mathbf{B}\) are symmetric matrices, the eigenvalues of \(\mathbf{C}\) lie in the interval \(\left[\min \left(\lambda_n(\mathbf{A}), \lambda_n(\mathbf{B})\right), \max \left(\lambda_1(\mathbf{A}), \lambda_1(\mathbf{B})\right)\right]\).
    % Given a convex combination $C=\alpha A+(1-\alpha)B,\alpha \in [0,1]$, the eigenvalues of $C$ lie in the interval $[\min(\lambda(A)_n, \lambda(B)_n), \max(\lambda(A)_1,\lambda(B)_1)]$
\end{claim}
\begin{proof}


First, recall that for a symmetric matrix \(\mathbf{A}\), the Rayleigh quotient \(R(\mathbf{A}, \mathbf{x}) = \frac{\mathbf{x}^{\top} \mathbf{A} \mathbf{x}}{\mathbf{x}^{\top} \mathbf{x}}\) is bounded by the smallest and largest eigenvalues of \(\mathbf{A}\):
\[
\lambda_n(\mathbf{A}) \leq R(\mathbf{A}, \mathbf{x}) \leq \lambda_1(\mathbf{A})
\]

Consider the Rayleigh quotient for the matrix \(\mathbf{C}\):
\[
R(\mathbf{C}, \mathbf{x}) = \frac{\mathbf{x}^{\top} (\alpha \mathbf{A} + (1-\alpha) \mathbf{B}) \mathbf{x}}{\mathbf{x}^{\top} \mathbf{x}} = \alpha R(\mathbf{A}, \mathbf{x}) + (1-\alpha) R(\mathbf{B}, \mathbf{x})
\]

Since \(R(\mathbf{A}, \mathbf{x})\) and \(R(\mathbf{B}, \mathbf{x})\) are bounded by their respective eigenvalues, we have:
\[
R(\mathbf{C}, \mathbf{x}) \leq \alpha \lambda_1(\mathbf{A}) + (1-\alpha) \lambda_1(\mathbf{B})
\]
which implies:
\[ 
R(\mathbf{C}, x)  \leq   \max(\lambda_1(\textbf{A}), \lambda_1(\textbf{B}))
\]

Similarly,
\[ 
R(\textbf{C}, \textbf{x})  \geq  \min(\lambda_n(\textbf{A}), \lambda_n(\textbf{B}))
\]

Thus, the eigenvalues of \(\textbf{C} = \alpha \textbf{A} + (1-\alpha)\textbf{B}\) are bounded by:
\[ 
\min(\lambda_n(\textbf{A}), \lambda_n(\textbf{B})) \leq  \lambda(\textbf{C}) \leq \max(\lambda_1(\textbf{A}), \lambda_1(\textbf{B}))
\]
\end{proof}
