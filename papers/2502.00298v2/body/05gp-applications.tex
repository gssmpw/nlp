In this section, we address how SKI affects Gaussian Processes Applications. In Section \ref{sec:kernel-hyperparameter-estimation} we address how using the SKI kernel and log-likelihood affect hyperparameter estimation, showing that gradient ascent on the SKI log-likelihood approaches a ball around a stationary point of the true log-likelihood. In section \ref{sec:posterior-inference} we describe how using SKI affects the accuracy of posterior inference.

\subsection{Kernel Hyperparameter Estimation}\label{sec:kernel-hyperparameter-estimation}

Here we show that, for a $\mu$-smooth log-likelihood, an iterate of gradient ascent on the SKI log-likelihood approaches a neighborhood of a stationary point of the true log-likelihood at an $O\left(\frac{1}{K}\right)$ rate, with the neighborhood size determined by the SKI score function's error. To show this, we leverage a recent result for non-convex inexact gradient ascent \cite{stonyakin2023stopping}, which requires an upper bound on the SKI score function's error. This requires bounding the spectral norm error of the SKI Gram matrix's partial derivatives. In order to obtain this, we note that for many SPD kernels, under weak assumptions, the partial derivatives are \textit{also} SPD kernels, and thus we can reuse the previous results directly on the partial derivatives.

Note that \cite{stonyakin2023stopping} does not actually imply \textit{convergence} to a neighborhood of a critical point, only that at least one iterate will approach it. Given the challenges of non-concave optimization and the fact that we leverage a fairly recent result, we leave stronger results to future work.

%In order to obtain this, we need to bound the errors of the partial derivatives of the SKI gram matrix. To do so, we note that in many cases, the partial derivatives of an SPD kernel are themselves SPD kernels. This allows us to express the gram matrix's gradient error as a different gram matrix error and thus apply our previous results. 

Let $\mathcal{D}\subseteq \Theta$ be a \textit{compact} subset that we wish to optimize over. In the most precise setting we would analyze projected gradient ascent, but for simplicity we analyze gradient ascent. Let let $\tilde{k}_{\theta}: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ be the SKI approximation of $k_{\theta}: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ using $m$ inducing points and interpolation degree $L-1$. We are interested in the convergence properties of \textit{inexact gradient ascent} using the SKI log-likelihood, e.g.
\begin{align*}
    \boldsymbol{\theta}_{k+1}&=\boldsymbol{\theta}_k+\eta \nabla \tilde{\mathcal{L}}(\boldsymbol{\theta}_k),
\end{align*}
where $\eta\in \mathbb{R}$ is the learning rate and $\nabla \tilde{\mathcal{L}}(\boldsymbol{\theta}_k)$ is the SKI score function (gradient of its log-likelihood). We assume: 1) a $\mu$-smooth log-likelihood. If we optimize on a bounded domain, then for infinitely differentiable kernels (e.g. RBF) this will immediately hold. 3) That the kernel's partial derivatives are themselves SPD kernels (this can be easily shown for the RBF kernel's lengthscale by noting that the product of SPD kernels are themselves SPD kernels). 
%We then mention an equivalence between strong concavity and the Hessian having an upper bound on its smallest eigenvalue.
% 2) That the kernel is continuously differentiable. 3) That the response vectors squared $l^2$ norm grows linearly with the sample size.
\begin{assumption}[$\mu$-smooth-log-likelihood]\label{assumption:mu-smoothness}
The true log-likelihood is $\mu$-smooth over $\mathcal{D}$. That is, for all $\boldsymbol{\theta},\boldsymbol{\theta}'\in \mathcal{D}$,
\begin{align*}
    \Vert \nabla \mathcal{L}(\boldsymbol{\theta})-\nabla \mathcal{L}(\boldsymbol{\theta}')\Vert &\leq \mu \Vert \boldsymbol{\theta}-\boldsymbol{\theta}'\Vert
\end{align*}
\end{assumption}

\begin{assumption}
    (Kernel Smoothness) $k_\theta(x,x')$ is $C^1$ in $\boldsymbol{\theta}$ over $\mathcal{D}$. That is, for each $l \in \{1, ..., p\}$, $k'_{\theta_l}(x, x') = \frac{\partial k_{\theta}(x, x')}{\partial \theta_l}$ exists and is continuous for $\boldsymbol{\theta}\in \mathcal{D}$.
\end{assumption}
\begin{assumption}
    (SPD Kernel Partials) For each $l \in \{1, ..., p\}$, the partial derivative of $k_{\theta}$ with respect to a hyperparameter $\theta_l\in \mathbb{R}$, denoted as $k'_{\theta_l}(x, x') = \frac{\partial k_{\theta}(x, x')}{\partial \theta_l}$, is also a valid SPD kernel.
\end{assumption}
% \begin{assumption}
%     (Linear Squared $l^2$ Response) $\Vert \textbf{y}\Vert_2^2=O(n)$.
% \end{assumption}

We next state several results leading up to our bound on the SKI score function's error. Here we argue that we can apply the same elementwise error we derived previously to the SKI partial derivatives.

\begin{restatable}{lemma}{skikernelderivativeerrorkernel}[Bound on Derivative of SKI Kernel Error using Kernel Property of Derivative]
\label{lemma:ski_kernel_derivative_error_kernel}
 Let $\tilde{k}'_{\theta_l}(x,x')$ be the SKI approximation of $k'_{\theta_l}(x,x')$, using the same inducing points and interpolation scheme as $\tilde{k}_{\theta}$. Then, for all $x, x' \in \mathcal{X}$ and all $\boldsymbol{\theta} \in \Theta$, the following inequality holds:

\begin{align*}
\left\vert \frac{\partial k_{\theta}(x,x')}{\partial \theta_l}-\frac{\partial \tilde{k}_{\theta}(x,x')}{\partial \theta_l}\right\vert &= \left\vert k'_{\theta_l}(x, x') - \tilde{k}'_{\theta}(x, x') \right\vert \\
&\leq\delta_{m,L}'+\sqrt{L}c^d\delta_{m,L}'\\
&=O\left(\frac{c^{2d}}{m^{3/d}}\right)
\end{align*}

where $\delta_{m,L}'$ is an upper bound on the error of the SKI approximation of the kernel $k'_{\theta_l}(x,x')$ with $m$ inducing points and interpolation degree $L-1$, as defined in Lemma \ref{lemma:ski-kernel-elementwise-error}.
\end{restatable}
\begin{proof}
    See Appendix \ref{sec:proofski_kernel_derivative_error_kernelz}
\end{proof}

We then use the elementwise bound to bound the spectral norm of the SKI gram matrix's partial derivative error. This again leverages Proposition \ref{prop:spectral-norm}, noting that these partial derivatives of the Gram matrices are themselves Gram matrices.

\begin{restatable}{lemma}{partialgradientspectralnormbound}[Partial Derivative Gram Matrix Difference Bound]
\label{lemma:partial_gradient_spectral_norm_bound}
For any $l \in \{1, \dots, p\}$,

\begin{align*}
\left\| \frac{\partial \mathbf{K}}{\partial \theta_l} - \frac{\partial \tilde{\mathbf{K}}}{\partial \theta_l} \right\|_2 &\leq \gamma'_{n,m,L,l} \\
&= O\left(\frac{nc^{2d}}{m^{3/d}}\right)
\end{align*}

where $\gamma'_{n,m,L,l}$ is the bound on the spectral norm difference between the kernel matrices corresponding to $k'_{\theta_l}$ and its SKI approximation $\tilde{k}'_{\theta_l}$ (analogous to Proposition \ref{prop:spectral-norm}, but for the kernel $k'_{\theta_l}$).
\end{restatable}
\begin{proof}
See Section \ref{section:proof_partial_gradient_spectral_norm_bound}.    
\end{proof}

% \begin{restatable}{lemma}{gradientspectralnormbound}[Bound on Spectral Norm of Gradient Difference]
% \label{lemma:gradient_spectral_norm_bound}
% Assume that $k'_{\theta,i}(x, x')$, is a valid SPD kernel. Let $\tilde{k}'_{\theta,i}(x,x')$ be the SKI approximation of $k'_{\theta,i}(x,x')$, using the same inducing points and interpolation scheme as $\tilde{k}_{\theta}$. Then, the spectral norm of the difference between the gradient of the true kernel matrix and the gradient of the SKI kernel matrix is bounded by:

% \begin{align*}
% \| \nabla_\theta K - \nabla_\theta \tilde{K} \|_2 &\leq \sqrt{p} \max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i}\\
% &=O\left(\sqrt{p}nc^{2d}h^3\right)\text{, convolutional cubic interpolation,}
% \end{align*}

% where $\nabla_\theta K$ and $\nabla_\theta \tilde{K}$ are represented as $p \times n^2$ matrices using the vec-notation (denominator layout), $p$ is the number of hyperparameters, and $\gamma'_{n,m,L,i}$ is the bound on the spectral norm difference between the kernel matrices corresponding to $k'_{\theta,i}$ and its SKI approximation $\tilde{k}'_{\theta,i}$ (analogous to Proposition \ref{prop:spectral-norm}, but for the kernel $k'_{\theta,i}$).
% \end{restatable}

% \begin{proof}
% See Section \ref{sec:proof_gradient_spectral_norm_bound}
% \end{proof}

We now bound the SKI score function. The key insight to the proof is that the partial derivatives of the difference between regularized gram matrix inverses is in fact a difference between two quadratic forms. We can then use standard techniques \citep{horn2012matrix} for bounding the difference between quadratic forms to obtain our result. The result says that, aside from the response vector's norm, the error grows quadratically in the sample size, at a square root rate in the number of hyperparameters and exponentially in the dimensionality. It further decays at an $m^{\frac{3}{d}}$ rate in the number of inducing points. Noting that to maintain linear time, $m$ should grow at an $n^{d/3}$ rate, we have that aside from the response vector, the error in fact grows linearly with the sample size when choosing the number of inducing points based on Theorem \ref{thm:inducing-points-count-alt}.
%by an expression that is \textit{quadratic} in the sample size. The reason for the quadratic dependency is that the difference in gradients of the quadratic forms in the log-likelihoods can itself be expressed as a gradient of a quadratic form involving the response variables. The response variables contribute a linear dependence and the gradient of the difference between the regularized inverses multiplicatively contribute a linear dependence: combined this is quadratic.
%\amnote{In an attempt to improve the bound, I got something that doesn't decrease with the number of inducing points, which is bad.}
\begin{restatable}{lemma}{scorefunctionbound}[Score Function Bound]\label{lemma:score-function-bound}
Let $\mathcal{L}(\boldsymbol{\theta})$ be the true log-likelihood and $\tilde{\mathcal{L}}(\boldsymbol{\theta})$ be the SKI approximation of the log-likelihood at $\boldsymbol{\theta}$. Let $\nabla \mathcal{L}(\boldsymbol{\theta})$ and $\nabla \tilde{\mathcal{L}}(\boldsymbol{\theta})$ denote their respective gradients with respect to $\boldsymbol{\theta}$. Then, for any $\boldsymbol{\theta}\in \mathcal{D}$,

\begin{align*}
&\| \nabla \mathcal{L}(\boldsymbol{\theta}) - \nabla \tilde{\mathcal{L}}(\boldsymbol{\theta}) \|_2 \\
&\leq \frac{1}{2\sigma^4}\Vert \textbf{y}\Vert\sqrt{p}\max_{1\leq l\leq p} \left( \gamma'_{n,m,L,l}+Cn\gamma_{n,m,L}\right.\\
&\qquad\left.+\gamma_{n,m,L}\gamma'_{n,m,L,l} \right)+\frac{\gamma_{n,m,L}}{2\sigma^4}\\
&=\Vert \textbf{y}\Vert_2 O\left(\frac{\sqrt{p}n^2c^{4d}}{m^{3/d}}\right)\\
&\equiv \epsilon_G
\end{align*}
where $C$ is a constants depending on the upper bound of the derivatives of the kernel function over $\mathcal{D}$.
\end{restatable}

% \begin{restatable}{lemma}{scorefunctionbound}[Score Function Bound]\label{lemma:score-function-bound}
% Let $\mathcal{L}(\theta)$ be the true log-likelihood and $\tilde{\mathcal{L}}(\theta)$ be the SKI approximation of the log-likelihood at $\theta$. Let $\nabla \mathcal{L}(\theta)$ and $\nabla \tilde{\mathcal{L}}(\theta)$ denote their respective gradients with respect to $\theta$. Then, for any $\theta\in \mathcal{D}$,
% \begin{align*}
% \| \nabla \mathcal{L}(\theta) - \nabla \tilde{\mathcal{L}}(\theta) \|_2 &\leq \frac{1}{\sigma^4}\left[\Vert y\Vert_2^2\sqrt{p}\max_{1\leq l\leq p}\left( \gamma'_{n,m,L,l}+Cn\gamma_{n,m,L}+\frac{\gamma_{n,m,L}}{\sigma^4} \gamma'_{n,m,L,l}\right)+\frac{1}{2}\gamma_{n,m,L}\right]\\
% &=O(\sqrt{p}n^3c^{2d}h^3)\\
% &\equiv \epsilon_G
% \end{align*}
% where $\gamma_{n,m,L}$ bounds the elementwise difference between $\mathbf{K}$ and $\tilde{\mathbf{K}}$, and $\gamma'_{n,m,L,i}$ is the bound on the spectral norm difference between the kernel matrices corresponding to $k'_{\theta,i}$ and its SKI approximation $\tilde{k}'_{\theta,i}$ and $C_n$ is a constant depending on the upper bound of the derivatives of the kernel function over $\mathcal{D}$ and the sample size.
% \end{restatable}

\begin{proof}
See Section \ref{sec:proof-score-function-bound}.
\end{proof}

We apply \cite{stonyakin2023stopping} below: the result is the same as in their paper (and assumes $\mu$-smoothness as we did on $\mathcal{L}$), but using gradient ascent instead of descent and using the score function error above. It says that at an $O\left(\frac{1}{K}\right)$ rate, at least one iterate of gradient ascent has its squared gradient norm approach a neighborhood proportional to the squared SKI score function's spectral norm error.

\begin{theorem} \citep{stonyakin2023stopping}
    For inexact gradient ascent on $\mathcal{L}$ with additively inexact gradients satisfying $\|\nabla \mathcal{L}(\boldsymbol{\theta}) - \nabla \tilde{\mathcal{L}}(\boldsymbol{\theta})\| \leq \epsilon_g$, we have:

\begin{equation}
    \max_{k=0,...,N-1} \|\nabla \mathcal{L}(\theta_k)\|^2 \leq \frac{2\mu(\mathcal{L}^* - \mathcal{L}(\boldsymbol{\theta}_0))}{K} + \frac{\epsilon_g^2}{2\mu}
\end{equation}

where $\mathcal{L}^*$ is the value at a stationary point, $\mathcal{L}(\boldsymbol{\theta}_0)$ is the initial, function value, $K$ is the number of iterations and $\epsilon_g$ is the gradient error bound in the previous Lemma.

\end{theorem}

\subsection{Posterior Inference}\label{sec:posterior-inference}
Finally, we treat posterior inference. As the current hyperparameter optimization results only say that \textit{some} iterate approaches a stationary point, we will focus on the error when the SKI and true kernel hyperparameter match.  We first add an assumption
\begin{assumption}
    (Bounded Kernel) Assume that the true kernel satisfies the condition that $|k(\mathbf{x}, \mathbf{x}')| \leq M$ for all $\mathbf{x}, \mathbf{x}'\in \mathcal{X}$.
\end{assumption}

% Consider on a compact domain,
% \begin{align*}
%     \vert k_\theta(x,x')-k_{\theta'}(x,x')\vert &\leq \rho \Vert \theta-\theta'\Vert
% \end{align*}

% which implies that
% \begin{align*}
%     \Vert \textbf{K}^{(\theta)}-\textbf{K}^{(\theta')}\Vert_2&\leq n\rho \Vert \theta-\theta'\Vert
% \end{align*}
% and we can then 
Now we bound the spectral error for the SKI mean function evaluated at a set of test points. The proof follows a standard strategy commonly used for approximate kernel ridge regression. See \cite{bach2013sharp,musco2017recursive} for examples. The result says that the $l^2$ error (aside from the response vector) grows exponentially in the dimensionality, super-linearly but sub-quadratically in the training sample size and at worst linearly in the test sample size. It decays at an $m^{\frac{3}{d}}$ rate in the number of inducing points. Similarly to for the score function error, if we follow Theorem \ref{thm:inducing-points-count-alt} for selecting the number of inducing points, the error in fact grows \textit{sublinearly} with the training sample size.
\begin{restatable}{lemma}{meaninference}\label{lemma:mean-inference} (SKI Posterior Mean Error)
    Let $\boldsymbol{\mu}(\cdot)$ be the GP posterior mean at a set of test points $\cdot\in \mathbb{R}^{T\times d}$ and $\tilde{\boldsymbol{\mu}}(\cdot)$ be the SKI posterior mean at those points. Then the SKI posterior mean $l^2$ error is bounded by:
{\footnotesize
\begin{align*}
    &\Vert \tilde{\boldsymbol{\mu}}(\cdot)- \boldsymbol{\mu}(\cdot)\Vert_2\\
    &\leq\left(\frac{\max(\gamma_{T,m,L},\gamma_{n,m,L})}{\sigma^2}+\frac{\sqrt{Tn}Mc^{2d}}{\sigma^4}\gamma_{n,m,L}\right)\Vert \textbf{y}\Vert_2\\
    &=\Vert \textbf{y}\Vert_2O\left(c^{2d}\frac{\max(T,n)+\sqrt{Tn}n}{m^{3/d}}\right)
    %\left(\frac{\max(\gamma_{T,m,L},\gamma_{n,m,L})}{\sigma^2}+\frac{\Vert \tilde{\mathbf{K}}_{\cdot, \mathbf{X}} \Vert_2}{\sigma^4}\gamma_{n,m,L}\right)\Vert \textbf{y}\Vert_2
\end{align*}
}
\end{restatable}


\begin{proof}
See Appendix \ref{sec:proof-mean-inference}.
\end{proof}

We now derive the spectral error bound for the test SKI covariance matrix. The proof involves noticing that a key term is a difference between two quadratic forms, and using standard techniques for bounding such a difference. The result shows that the error grows at worst super-linearly but subquadratically in the number of test points, quadratically in the training sample size and exponentially in the dimension. Interestingly, due to the use of standard techniques for bounding the difference between quadratic forms, the error is only guaranteed to decay with the number of inducing points at an $m^{3/d-1}$ rate, so that it is only guaranteed to decay at all if $d<3$. If we select the number of inducing points to be proportional to $n^{d/3}$, then the error grows at rate $n^{1+d/3}$ for $d<3$. An interesting question is whether alternate techniques can improve the result for higher dimensional settings e.g. $d\geq 3$.

\begin{restatable}{lemma}{skiposteriorcovarianceerror}[SKI Posterior Covariance Error]\label{lemma:ski-posterior-covariance-error}
Let $\boldsymbol{\Sigma}(\cdot)$ be the GP posterior covariance matrix at a set of test points $\cdot\in \mathbb{R}^{T\times d}$ and $\tilde{\boldsymbol{\Sigma}}(\cdot)$ be its SKI approximation. Then
\begin{align*}
    &\Vert \boldsymbol{\Sigma}(\cdot)-\tilde{\boldsymbol{\Sigma}}(\cdot)\Vert_2\\ &\leq \gamma_{T,m,L} + \frac{\sqrt{Tn}M}{\sigma^2} \max(\gamma_{T,m,L},\gamma_{n,m,L})\\
    &\quad+ \frac{\gamma_{n,m,L}}{\sigma^4}Tn m c^{2d} M^2 \\
    &\quad+ \frac{\sqrt{Tn} m c^{2d} M}{\sigma^2} \max(\gamma_{T,m,L},\gamma_{n,m,L}).\\
    &=O\left(\frac{Tn^2mc^{4d}+\sqrt{Tn}mc^{4d}\max(T,n)}{m^{3/d}}\right).
\end{align*}
where $\gamma_{T,m,L}$ is defined as in Proposition \ref{prop:spectral-norm}.
\end{restatable}

\begin{proof}
See Appendix \ref{sec:proof-ski-posterior-covariance-error}

\end{proof}

