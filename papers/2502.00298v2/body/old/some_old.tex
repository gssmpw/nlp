%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\jmei}[1]{\textcolor{magenta}{#1}}
\newcommand{\amnote}[1]{\textcolor{red}{#1}}
% Use the following line for the initial blind version submitted for review:
\newtheorem{claim}{Claim}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\usepackage{icml2021}
\newtheorem{assumption}{Assumption}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Error Bounds for Structured Kernel Interpolation via Multivariate Polynomial Interpolation Analysis}

\begin{document}

\twocolumn[
\icmltitle{Error Bounds for Structured Kernel Interpolation via Multivariate Polynomial Interpolation Analysis}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aeiau Zzzz}{equal,to}
\icmlauthor{Bauiu C.~Yyyy}{equal,to,goo}
\icmlauthor{Cieua Vvvvv}{goo}
\icmlauthor{Iaesut Saoeu}{ed}
\icmlauthor{Fiuea Rrrr}{to}
\icmlauthor{Tateu H.~Yasehe}{ed,to,goo}
\icmlauthor{Aaoeu Iasoh}{goo}
\icmlauthor{Buiui Eueu}{ed}
\icmlauthor{Aeuia Zzzz}{ed}
\icmlauthor{Bieea C.~Yyyy}{to,goo}
\icmlauthor{Teoau Xxxx}{ed}
\icmlauthor{Eee Pppp}{ed}
\end{icmlauthorlist}

\icmlaffiliation{to}{Department of Computation, University of Torontoland, Torontoland, Canada}
\icmlaffiliation{goo}{Googol ShallowMind, New London, Michigan, USA}
\icmlaffiliation{ed}{School of Computation, University of Edenborrow, Edenborrow, United Kingdom}

\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}
\onecolumn

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Structured Kernel Interpolation (SKI) \cite{wilson2015kernel} is helps scale Gaussian Processes (GPs) to large datasets by approximating the kernel matrix via interpolation at inducing points, leading to a computational complexity of $O(n+m\log m)$, where $n$ is the sample size and $m$ is the number of inducing points. Despite its widespread use and effectiveness, SKI lacks rigorous theoretical error analysis. In particular, for a given error tolerance for the approximate kernel matrix, how many inducing points $m$ do we need, and will $O(m\log m)\leq O(n)$ hold so that the computational complexity is linear as claimed? This paper addresses this gap by deriving error bounds for the SKI kernel approximation and analyzing the impact on GP training and inference. We: (1) establish error bounds for the SKI kernel and its gram matrix; (2) examine how these errors affect maximum likelihood estimation of mean function and kernel hyperparameters; (3) evaluate the error of SKI GP posterior means and  variances. Interestingly, we identify two dimensionality regimes relating SKI gram matrix error to computational complexity. For $d\leq 3$, as long as the sample size grows as we want to \textit{decrease} error, we maintain linear time. For $d>3$, the error must \textit{increase} with the sample size to maintain linear time. Our theoretical analysis thus provides crucial insights into the scalability-accuracy trade-offs of SKI and how they change with the feature dimensionality, establishing precise conditions under which linear-time GP inference can be achieved while maintaining controlled approximation error.
\end{abstract}

\section{Introduction}\label{sec:introduction}
Gaussian Processes (GPs) are an important class of stochastic processes used in machine learning and statistics, with use cases including spatial data analysis \citep{liu2021missing}, time series forecasting \citep{girard2002gaussian} and bioinformatics \citep{luo2023diseasegps}. GPs offer a non-parametric framework for modeling distributions over functions, enabling both flexibility and uncertainty quantification. These capabilities, combined with the ability to incorporate prior knowledge and specify relationships by choice of kernel function, make Gaussian Processes effective for regression, classification, and optimization.

However, GPs have substantial computational and memory bottlenecks. Both training and inference require computing the action of the inverse kernel gram matrix, while training requires computing its log-determinant: both are $O(n^3)$ operations, where $n$ is the sample size. Further, storing the full gram matrix requires $O(n^2)$ memory. These bottlenecks require scalable approximations to extend Gaussian Processes to larger datasets.

Structured Kernel Interpolation (SKI) \cite{wilson2015kernel} is an efficient method for scaling Gaussian Processes (GPs) to large datasets by approximating the kernel matrix using interpolation on a set of inducing points. For stationary kernels, this requires $O(n+m \log m)$ computational complexity. The core idea is to express the original kernel as a combination of interpolation functions and a kernel matrix defined on a set of inducing points. However, despite its effectiveness, popularity (over $600$ citations, a large number for a GP paper) and high quality software availability (\cite{gardner2018gpytorch} has 3.5k stars on github), it currently lacks theoretical analysis. A key initial question is, given a fixed error bound for the SKI gram matrix and their use of cubic interpolation, how many inducing points are required to achieve that error bound? Given the required value of $m$ as a function of $n$, for what error tolerance is $O(n+m\log m)$ still linear? Following this, how do these errors propagate to the SKI log-likelihood and posterior inference?

In this paper, we begin to bridge the gap between practice and a theoretical understanding of SKI. We have three primary contributions: 1) error bounds for the SKI kernel and relevant quantities, including the spectral norm of the difference between the true and SKI gram matrices. 2) SKI estimation analysis: maximum likelihood estimation (MLE) of kernel hyperparameters assuming a good initialization. 3) SKI inference analysis: the error of the GP posterior means and variances at test points. Interestingly, we identify two dimensionality regimes relating SKI gram matrix error to computational complexity. For $d\leq 3$, as long as the sample size grows as we want to \textit{decrease} error, we maintain linear time. For $d>3$, the error must \textit{increase} with the sample size to maintain linear time. \amnote{not sure this next statement is true need to check.}By our downstream analysis, this implies that for small dimensionality and kernels exhibiting desirable regularity conditions, we can have arbitrarily small error in the SKI gram matrix, in our estimated parameters and in posterior inference all in linear time as long as the sample size is sufficiently large.

In section \ref{sec:related} we describe related work. In section \ref{sec:ski-background} we give a brief background on SKI. In section \ref{sec:important-quantities} we bound the error of important quantities: specifically the SKI kernel error, the SKI gram matrix error and the error of the SKI inverse's action and the SKI log-determinant. In section \ref{sec:gp-applications} we use these to analyze the error of the SKI MLE and the KL divergence, total variation and Wasserstein distances  between the true and SKI predictive posteriors. We conclude in section \ref{sec:discussion} by summarizing our results and discussing limitations and future work.

For papers 
\section{Related Work}\label{sec:related}

We can divide related works into three groups: those theoretically analyzing Gaussian process regression or kernel methods when using approximate kernels, SKI and its extensions, and papers developing techniques we use to obtain our guarantees. In the first group, the most relevant works are \cite{burt2019rates,burt2020convergence}, where they analyzed the sparse variational GP framework and derived bounds on the Kullback-Leibler divergence between the true posterior and the variational approximate posterior. \cite{moreno2023ski} gave bounds on the approximation error of the SKI Gram matrix. However, they only handled the case of univariate features and only bounded how much worse the SKI Gram matrix can be than the Nystr{\"o}m one. Further, they did not analyze the downstream effects on the approximate MLE or GP posterior. Also relevant are \cite{wynne2022variational,wild2021connections}, who gave a Banach space view of sparse variational GPs and connected them to the Nystr{\"o}m method, respectively. Only one of these papers \cite{moreno2023ski} treated SKI specifically, and it only covered a very special case setting. % We briefly discuss and compare to the Nystr{\"o}m method based on their results, while leaving a full Banach space analysis to future work.
%We obtain analogous results when using the SKI kernel. 

In the second group, the foundational work by \cite{wilson2015kernel} that we analyze introduced SKI as a scalable method for large-scale GP inference. \cite{kapoor2021skiing} extended SKI to high-dimensional settings using the permutohedral lattice \cite{yadav2022kernel} developed a sparse grid approach to kernel interpolation that also helps address the curse of dimensionality. Most recently, \cite{ban2024malleable} proposed a flexible adaptation of SKI that allows for more adaptive kernel structures\amnote{check if these are correct interpretations}. We focus on the technique of \cite{wilson2015kernel} in this paper, but future work could extend to the settings of the latter papers.

Also relevant are papers where we leverage or extend their proof techniques. We require a multivariate extension to the error analysis of \cite{keys1981cubic} for convolutional cubic interpolation, which we derive. Further, when analyzing the hyperparameter MLE error of gradient ascent on the SKI log-likelihood, we use a strategy that is commonly used in the expectation maximization theory literature \cite{balakrishnan2017statistical,wu2016convergence,moreno2020robust} where they decompose the EM algorithm error into a population error, which decays geometrically, and a sample error, which involves a geometric sum. We instead analyze the error of gradient ascent on the SKI log-likelihood. We decompose into gradient ascent on the true log-likelihood, which decays geometrically and the error due to using the SKI log-likelihood, which also involves a geometric sum.


\section{Gaussian Processes, Structured Kernel Interpolation and Convolutional Cubic Interpolation}\label{sec:ski-background}

This section provides background on Gaussian Processes (GPs) and two key techniques for enabling scalable inference: Structured Kernel Interpolation (SKI) and Convolutional Cubic Interpolation. SKI \cite{wilson2015kernel} addresses GPs scalability issue by approximating the kernel matrix through interpolation on a set of inducing points, leveraging the efficiency of convolutional kernels. In particular, cubic convolutional kernels, as detailed in \cite{keys1981cubic}, provide a smooth and accurate interpolation scheme that forms the foundation of the SKI framework. In this paper, we focus on this cubic case as it is used by SKI. Future work may extend this to study higher-order interpolation methods. Here, we formally define these concepts and lay the groundwork for the subsequent error analysis.

\subsection{Gaussian Processes}\label{sec:gp-background}
A Gaussian process $\xi\sim \textrm{GP}(\nu,k_\theta)$ is a stochastic process $\{\xi(\textbf{x})\}_{\textbf{x}\in \mathcal{X}}$ such that any finite subcollection $\{\xi(\textbf{x}_i)\}_{i=1}^n$ is multivariate Gaussian distributed. In machine learning, we generally assume that we have index locations $\textbf{x}_i\in \mathbb{R}^d$ and observations $y_i\in \mathbb{R}$ for a set of training points $i=1,\ldots,n$ such that
\begin{align*}
y_i&=\xi(\textbf{x}_i)+\epsilon_i,\epsilon_i\sim \mathcal{N}(0,\sigma^2).
\end{align*}
where $\nu:\mathcal{X}\rightarrow\mathbb{R}$, $k_\theta:\mathcal{X}\times \mathcal{X}\rightarrow\mathbb{R}$ are the prior mean and covariance functions, respectively, with $\nu$ the mean function generally assumed to be $0$ everywhere and $k$ an SPD kernel with hyperparameters $\theta$. Given $\{\textbf{x}_i,y_i\}_{i=1}^n$ we are primariliy interested in two tasks: 1) estimate the kernel hyperparameters, given an assumed form for $k_\theta$ (e.g. RBF kernel) 2) do Bayesian inference for the posterior mean $\boldsymbol{\mu}(\cdot)\in \mathbb{R}^{T}$ and covariance $\boldsymbol{\Sigma}(\cdot)\in \mathbb{R}^{T\times T}$ at a set of test points $\{\textbf{x}_t\}_{t=1}^T$. Assuming $\nu\equiv 0$ (a mean-zero GP prior), for 1), one maximizes the log-likelihood
\begin{align}
    \mathcal{L}(\theta;X) = -\frac{1}{2}\textbf{y}^\top (\textbf{K}+\sigma^2 \textbf{I})^{-1}\textbf{y}-\frac{1}{2}\log \vert \textbf{K}+\sigma^2 \textbf{I}\vert -\frac{N}{2}\log (2\pi)\label{eqn:log-likelihood}
\end{align}
where $\textbf{K}\in \mathbb{R}^{n\times n}$ with entries $\textbf{K}_{ij}=k_\theta(\textbf{x}_i,\textbf{x}_j)$ is the Gram matrix for the training dataset. For 2), given the kernel function and known observation variance $\sigma^2$, the posterior mean and covariance are given by
\begin{align}
\boldsymbol{\mu}(\cdot) &= \mathbf{K}_{\cdot, \mathbf{X}}\left(\mathbf{K}+\sigma^{2} \mathbf{I}\right)^{-1} \mathbf{y}\label{eqn:posterior-mean}\\
\boldsymbol{\Sigma}(\cdot) &= \textbf{K}_{\cdot,\cdot}+\sigma^2I-\textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1}\textbf{K}_{\textbf{X},\cdot}\label{eqn:posterior-covariance}
\end{align}
where $\textbf{K}_{\cdot,\textbf{X}}\in \mathbb{R}^{T\times n}$ is the matrix of kernel evaluations between test and training points. However, between the log-likelihood and the posteriors, one first needs to compute the action of the inverse of the regularized Gram matrix, $(\textbf{K}+\sigma^2 \textbf{I})^{-1}\textbf{y}$. Second, one needs to compute the log-determinant $\log \vert \textbf{K}+\sigma^2 \textbf{I}\vert$. These are both $O(n^3)$ computationally and $O(n^2)$ memory.

\subsection{Structured Kernel Interpolation}\label{subsec:ski}
Structured kernel interpolation \citep{wilson2015kernel} or (SKI) addresses these computational and memory bottlenecks by approximating the original kernel function \(k_\theta:\mathcal{X}\times \mathcal{X}\rightarrow \mathbb{R},\mathcal{X}\subseteq\mathbb{R}^d\) by interpolating kernel values at a chosen set of inducing points \(\mathbf{U}=\left(\begin{array}{c}
     \textbf{u}_1^\top\\
     \vdots\\
     \textbf{u}_m^\top
\end{array}\right)\in \mathbb{R}^{m\times d}\). The approximate kernel function \(\tilde{k}:\mathcal{X}\times \mathcal{X}\rightarrow\mathbb{R}\) can be expressed as:
\[
\tilde{k}(\mathbf{x}, \mathbf{x}') = \mathbf{w}(\mathbf{x})^{\top} \mathbf{K}_{\mathbf{U}} \mathbf{w}(\mathbf{x}')
\]
where \(\mathbf{K}_{\mathbf{U}}\in \mathbb{R}^{m\times m}\) is the kernel matrix computed on the inducing points, and \(\mathbf{w}(\mathbf{x}),\mathbf{w}(\mathbf{x}')\in \mathbb{R}^m\) are vectors of interpolation weights using (usually cubic) convolutional kernel $u:\mathbb{R}\rightarrow \mathbb{R}$ for the points $\textbf{x}$ and $\textbf{x}'$, respectively. One can then form the SKI Gram matrix $\tilde{\textbf{K}}=\textbf{W}\textbf{K}_\textbf{U}\textbf{W}^\top$ with $\textbf{W}$ a \textit{sparse} matrix of $L$ interpolation weights per row for a polynomial of degree $L-1$. By exploiting the sparsity of each row, for stationary kernels this leads to a computational complexity of $O(nL+m\log m)$ and a memory complexity of $O(nL+m)$.

In order to learn kernel hyperparameters, one can maximize the SKI approximation to the log-likelihood (henceforth the SKI log-likelihood)
\begin{align*}
    \tilde{\mathcal{L}}(\theta;X)
&=-\frac{1}{2}\textbf{y}^\top (\tilde{\textbf{K}}+\sigma^2 \textbf{I})^{-1}\textbf{y}-\frac{1}{2}\log \vert \tilde{\textbf{K}}+\sigma^2 \textbf{I}\vert -\frac{N}{2}\log (2\pi)
\end{align*}
Given the SKI kernel $\tilde{k}:\mathcal{X}\times \mathcal{X}\rightarrow \mathbb{R}$ with learned hyperparameters, one can do posterior inference of the SKI approximations to the mean $\tilde{\boldsymbol{\mu}}(\cdot)$ and covariance $\tilde{\boldsymbol{\Sigma}}(\cdot)$ at a set of $T$ test points $\cdot$ as
\begin{align*}
    \tilde{\boldsymbol{\mu}}(\cdot)&=\tilde{\mathbf{K}}_{\cdot, \mathbf{X}}\left(\tilde{\mathbf{K}}+\sigma^{2} \mathbf{I}\right)^{-1} \mathbf{y}\\
    \tilde{\boldsymbol{\Sigma}}(\cdot)&=\tilde{\textbf{K}}_{\cdot,\cdot}+\sigma^2I-\tilde{\textbf{K}}_{\cdot,\textbf{X}}^\top (\tilde{\textbf{K}}+\sigma^2 I)^{-1}\tilde{\textbf{K}}_{\textbf{X},\cdot}
\end{align*}
where $\tilde{\textbf{K}}_{\cdot,\textbf{X}}\in \mathbb{R}^{T\times n}$ is the matrix of SKI kernels between test points and training points and $\tilde{\textbf{K}}_{\cdot,\cdot}\in \mathbb{R}^{T\times T}$ is the SKI Gram matrix for the test points.


\subsection{Convolutional Cubic Interpolation}\label{subsec:convolutional-cubic-interpolation}
Convolutional cubic interpolation \citep{keys1981cubic} is a method used to approximate the value of a function at arbitrary points within a domain, given its values on a regular grid. It leverages a cubic convolution kernel to produce a smooth, continuous estimate of the underlying function. The method calculates a weighted average of the known values, where the weights are determined by the cubic convolution kernel, which is a piecewise polynomial function specifically designed so that the approximation interpolates and is continuously differentiable. We formalize this using the definitions of the cubic convolutional interpolation kernel and the tensor-product cubic convolutional function below.

\begin{definition}\label{def:cubic-interpolation-kernel}

The cubic convolutional interpolation kernel $u:\mathbb{R}\rightarrow\mathbb{R}$ is given by
$$
u(s)\equiv\begin{cases}
1, & s=0\\
\frac{3}{2}\vert s\vert^3-\frac{5}{2}\vert s\vert^2+1, & 0<\vert s\vert <1\\
-\frac{1}{2}\vert s\vert^3+\frac{5}{2}\vert s\vert^2-4\vert s\vert+2, & 1<\vert s\vert<2\\
0, & \text{otherwise}
\end{cases}
$$
\end{definition}

\begin{definition}\label{def:tensor-product-cubic-interpolation-alt1}
Let $\mathbf{x} = (x_1, x_2, ..., x_d) \in \mathbb{R}^d$ be a d-dimensional point. Let $f:\mathbb{R}^d \rightarrow \mathbb{R}$ be a function defined on a regular grid with spacing $h$ in each dimension. Let $\mathbf{c_x}$ denote the grid point closest to $\mathbf{x}$.  The tensor-product cubic convolutional interpolation function $g:\mathbb{R}^d\rightarrow \mathbb{R}$ is defined as:

$$
g(\mathbf{x}) \equiv \sum_{\mathbf{k} \in \{-1, 0, 1, 2\}^d} f(\mathbf{c_x} + h\mathbf{k}) \prod_{j=1}^d u\left(\frac{x_j - (\mathbf{c_x})_j - h k_j}{h}\right),
$$
where $u$ is the cubic convolutional interpolation kernel (defined in Definition \ref{def:cubic-interpolation-kernel}), and $\mathbf{k} = (k_1, \ldots, k_d)$ is a vector of integer indices.
\end{definition}

\section{Important Quantities}\label{sec:important-quantities}
This section derives bounds for key quantities in Structured Kernel Interpolation (SKI). Section \ref{subsec:polynomial-interpolation-preliminaries} covers polynomial interpolation preliminaries, focusing on multivariate tensor-product cubic convolutional interpolation and its error characteristics. Section \ref{subsubsec:elementwise-error} provides a bound on the elementwise error between the true kernel and its SKI approximation, foundational for SKI accuracy. In Section \ref{subsubsec:spectral-norm-error}, we extend this to the spectral norm error of the SKI approximation for the training Gram matrix and train-test kernel matrix, key for expressing various downstream errors. Finally, we present conditions on the number of inducing points for achieving specific error tolerance $\epsilon$ and a theorem on error necessary for linear time complexity, noting linear time always holds for $d\leq 3$ with sufficiently large samples.

\subsection{Preliminaries for Polynomial Interpolation}\label{subsec:polynomial-interpolation-preliminaries}

This subsection analyzes the error of multivariate tensor-product cubic convolutional interpolation, a crucial part of the SKI framework. We define the interpolation kernel and function, and establish a lemma on how its error relates to grid spacing and input dimensionality, extending the work of \cite{keys1981cubic} to multivariate settings. Additionally, we highlight the curse of dimensionality for kernel interpolation weights.

The next lemma shows that multivariate tensor-product cubic convolutional interpolation retains error cubic in the grid spacing of \cite{keys1981cubic}, but exhibits exponential error growth with increasing dimensions. The proof uses induction on dimensions, starting with the 1D case from Keys.

\begin{lemma}\label{lemma:tensor-product-interpolation-error}
Assume that the grid spacing is fixed across all dimensions, denoted by $h$. The error of tensor-product cubic convolutional interpolation is $O(c^dh^3)$ for some constant $c>0$.
\end{lemma}

\begin{proof}
We define a sequence of intermediate interpolation functions. Let $g_0(\mathbf{x}) \equiv f(\mathbf{x})$ be the original function. For $i = 1, \ldots, d$, we recursively define $g_i(\mathbf{x})$ as the function obtained by interpolating $g_{i-1}$ along the $i$-th dimension using the cubic convolution kernel $u$:

$$
g_i(\mathbf{x}) \equiv \sum_{k=-1}^2 g_{i-1}\left(\mathbf{x} + \left( (\mathbf{c_x})_i - x_i + kh\right)\mathbf{e}_i \right) u\left(\frac{x_i - (\mathbf{c_x})_i - kh}{h}\right).
$$

Here, $\mathbf{c_x}$ is the grid point closest to $\mathbf{x}$, and $\mathbf{e}_i$ is the $i$-th standard basis vector. Thus, $g_1(\mathbf{x})$ interpolates $f$ along the first dimension, $g_2(\mathbf{x})$ interpolates $g_1$ along the second dimension, and so on, until $g_d(\mathbf{x}) = g(\mathbf{x})$ is the final tensor-product interpolated function.

We analyze the error accumulation across multiple dimensions using induction. Using \cite{keys1981cubic}, the error introduced by interpolating a thrice continuous differentiable function along a single dimension with the cubic convolution kernel is uniformly bounded over the interval domain by $Kh^3$ for some constant $K > 0$, provided the grid spacing $h$ is sufficiently small. This gives us the base case:
$$
|g_1(\mathbf{x}) - g_0(\mathbf{x})| \leq Kh^3.
$$

For the inductive step, assume that for some $i=k$ the error is uniformly bounded by
$$
|g_k(\mathbf{x}) - g_{k-1}(\mathbf{x})| \leq c^{k-1}Kh^3.
$$

We want to show that this bound also holds for $i=k+1$. We can express the difference $g_{k+1}(\mathbf{x}) - g_k(\mathbf{x})$ as follows:

\begin{align*}
g_{k+1}(\mathbf{x}) - g_k(\mathbf{x}) &= \sum_{k_{k+1}=-1}^2 g_k\left(\mathbf{x} + ((\mathbf{c_x})_{k+1} - x_{k+1} + k_{k+1}h) \mathbf{e}_{k+1} \right) u\left(\frac{x_{k+1} - (\mathbf{c_x})_{k+1} - k_{k+1}h}{h}\right) \\
&\quad - g_k(\mathbf{x}) \\
&= \sum_{k_{k+1}=-1}^2 \left[\sum_{k_k=-1}^2 g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k + ((\mathbf{c_x})_{k+1} - x_{k+1} + k_{k+1}h) \mathbf{e}_{k+1} \right) \right. \\
&\quad \left. u\left(\frac{x_k - (\mathbf{c_x})_k - k_k h}{h}\right)\right] u\left(\frac{x_{k+1} - (\mathbf{c_x})_{k+1} - k_{k+1}h}{h}\right) \\
&\quad - \sum_{k_k=-1}^2 g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k \right) u\left(\frac{x_k - (\mathbf{c_x})_k - k_k h}{h}\right) \\
&= \sum_{k_k=-1}^2 u\left(\frac{x_k - (\mathbf{c_x})_k - k_k h}{h}\right) \left[\sum_{k_{k+1}=-1}^2 g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k + ((\mathbf{c_x})_{k+1} - x_{k+1} + k_{k+1}h) \mathbf{e}_{k+1} \right) \right. \\
&\quad \left. u\left(\frac{x_{k+1} - (\mathbf{c_x})_{k+1} - k_{k+1}h}{h}\right) - g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k\right)\right].
\end{align*}

The inner term in the last expression represents the difference between interpolating $g_{k-1}$ along the $(k+1)$-th dimension and $g_{k-1}$ itself, evaluated at $\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k$. This can be written as:

\begin{align*}
&\sum_{k_{k+1}=-1}^2 g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k + ((\mathbf{c_x})_{k+1} - x_{k+1} + k_{k+1}h) \mathbf{e}_{k+1} \right) u\left(\frac{x_{k+1} - (\mathbf{c_x})_{k+1} - k_{k+1}h}{h}\right) \\
&\quad - g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k\right) \\
&= g_k\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k \right) - g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k \right).
\end{align*}

Therefore, we can bound the error as follows:

\begin{align*}
|g_{k+1}(\mathbf{x}) - g_k(\mathbf{x})| &\leq \left|\sum_{k_k=-1}^2 u\left(\frac{x_k - (\mathbf{c_x})_k - k_k h}{h}\right)\right| \cdot \left|g_k\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k \right) - g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k \right)\right|.
\end{align*}

Let $c>0$ be a uniform upper bound for $\left|\sum_{k_k=-1}^2 u\left(\frac{x_k - (\mathbf{c_x})_k - k_k h}{h}\right)\right|$, which exists because $u$ is bounded. By the inductive hypothesis, we have $\left|g_k\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k \right) - g_{k-1}\left(\mathbf{x} + ((\mathbf{c_x})_k - x_k + k_k h) \mathbf{e}_k \right)\right| \leq c^{k-1}Kh^3$. Thus,

$$
|g_{k+1}(\mathbf{x}) - g_k(\mathbf{x})| \leq c \cdot c^{k-1}Kh^3 = c^k Kh^3.
$$

This completes the inductive step.

Finally, we bound the total error $|g(\mathbf{x}) - f(\mathbf{x})| = |g_d(\mathbf{x}) - g_0(\mathbf{x})|$ by summing the errors introduced at each interpolation step:

$$
|g(\mathbf{x}) - f(\mathbf{x})| \leq \sum_{i=1}^d |g_i(\mathbf{x}) - g_{i-1}(\mathbf{x})| \leq \sum_{i=1}^d c^{i-1}Kh^3 = Kh^3 \sum_{i=0}^{d-1} c^i.
$$

The last sum is a geometric series, which evaluates to $Kh^3 \frac{1 - c^d}{1 - c}$. For a fixed $c>1$ (independent of $d$), this expression is $O(c^{d})$ when $d$ is large. Therefore, tensor-product cubic convolutional interpolation has $O(c^d h^3)$ error.
\end{proof}

The next lemma shows that when using a product kernel for $d$-dimensional kernel regression (where cubic convolutional interpolation is a special case), the sum of weights suffers from the curse of dimensionality. The proof strategy involves expressing the multi-dimensional sum as a product of sums over each individual dimension, leveraging the initial condition on the one-dimensional bound for each dimension, and taking advantage of the structure of the Cartesian grid.

\begin{lemma}\label{lemma:cubic-interpolation-weights-curse-dimensionality}
Let $u: \mathbb{R} \rightarrow \mathbb{R}$ be a one-dimensional kernel function such that for some constant $c > 0$, the sum of weights in a 1D kernel regression is bounded by $c$ over its domain. That is, for any $x \in \mathbb{R}$ and a set of data points $\{x_i\}_{i=1}^n \subset \mathbb{R}$,
$$
\sum_{i=1}^n u\left(\frac{x - x_i}{h}\right) \leq c,
$$
where $h > 0$ is the bandwidth.

Let $u_d: \mathbb{R}^d \rightarrow \mathbb{R}$ be a d-dimensional product kernel defined as:
$$
u_d\left(\frac{x - x_i}{h}\right) = \prod_{j=1}^d u\left(\frac{x^{(j)} - x_i^{(j)}}{h}\right),
$$
where $x = (x^{(1)}, x^{(2)}, ..., x^{(d)}) \in \mathbb{R}^d$ and $x_i = (x_i^{(1)}, x_i^{(2)}, ..., x_i^{(d)}) \in \mathbb{R}^d$ are d-dimensional points. Assume the data points $\{x_i\}_{i=1}^n$ ($n$ may differ from the univariate case) lie on a fixed d-dimensional grid $G = G^{(1)} \times G^{(2)} \times ... \times G^{(d)}$, where each $G^{(j)} = \{p_1^{(j)}, p_2^{(j)}, ..., p_{n_j}^{(j)}\}$ is a finite set of $n_j$ grid points along the j-th dimension for $j = 1, 2, ..., d$.

Then, for any $x \in \mathbb{R}^d$, the sum of weights in the d-dimensional kernel regression is bounded by $c^d$:
$$
\sum_{i=1}^n u_d\left(\frac{x - x_i}{h}\right) \leq c^d.
$$
\end{lemma}
\begin{proof}
Let the fixed d-dimensional grid be defined by the Cartesian product of d sets of 1-dimensional grid points: $G = G^{(1)} \times G^{(2)} \times ... \times G^{(d)}$, where $G^{(j)} = \{p_1^{(j)}, p_2^{(j)}, ..., p_{n_j}^{(j)}\}$ is the set of grid points along the j-th dimension.

We start with the sum of weights in the d-dimensional case:

$$
\sum_{i=1}^n u_d\left(\frac{x - x_i}{h}\right) = \sum_{i=1}^n \prod_{j=1}^d u\left(\frac{x^{(j)} - x_i^{(j)}}{h}\right)
$$

Since the data points lie on the fixed grid $G$, we can rewrite the outer sum as a nested sum over the grid points in each dimension:

$$
\sum_{i=1}^n \prod_{j=1}^d u\left(\frac{x^{(j)} - x_i^{(j)}}{h}\right) = \sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_d=1}^{n_d} \prod_{j=1}^d u\left(\frac{x^{(j)} - p_{k_j}^{(j)}}{h}\right)
$$

Now we can change the order of summation and product, as proven in Lemma \ref{lemma:switch-sum-product}:

$$
\sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_d=1}^{n_d} \prod_{j=1}^d u\left(\frac{x^{(j)} - p_{k_j}^{(j)}}{h}\right) = \prod_{j=1}^d \left( \sum_{k_j=1}^{n_j} u\left(\frac{x^{(j)} - p_{k_j}^{(j)}}{h}\right) \right)
$$

By the assumption of the lemma, we know that for each dimension $j$, the sum of weights is bounded by $c$. Note that $\{p_{k_j}^{(j)}\}_{k_j=1}^{n_j}$ is simply a set of points in $\mathbb{R}$, thus:

$$
\sum_{k_j=1}^{n_j} u\left(\frac{x^{(j)} - p_{k_j}^{(j)}}{h}\right) \leq c
$$

Therefore, we have:

$$
\prod_{j=1}^d \left( \sum_{k_j=1}^{n_j} u\left(\frac{x^{(j)} - p_{k_j}^{(j)}}{h}\right) \right) \leq \prod_{j=1}^d c = c^d
$$

Thus, we have shown that:

$$
\sum_{i=1}^n u_d\left(\frac{x - x_i}{h}\right) \leq c^d
$$

\end{proof}
\subsection{Error Bounds for the Ski Kernel}\label{subsec:ski-kernel-error-bounds}
This subsection analyzes the error introduced by the SKI approximation of the kernel function. We first derive bounds on the elementwise difference between the true kernel \(k(\textbf{x},\textbf{x}')\) and its SKI approximation \(\tilde{k}(\textbf{x},\textbf{x}')\). We then apply these elementwise bounds to derive further error bounds on the SKI kernel matrices, which will be crucial for understanding the downstream effects of the SKI approximation on Gaussian process hyperparameter estimation and posterior inference.
\subsubsection{Elementwise}\label{subsubsec:elementwise-error}

The following Lemma allows us to bound the absolute difference between the true and SKI kernels \textit{uniformly} in terms of the number of inducing points, an upper bound on the absolute sum of interpolation weights (in the Lagrange case this would be the Lebesgue constant), and the error due to polynomial interpolation.


\begin{lemma}\label{lemma:ski-kernel-elementwise-error}
    The SKI kernel $\tilde{k}:\mathcal{X}\times \mathcal{X}\rightarrow \mathbb{R}$ with $m$ inducing points and interpolation degree $L-1$ has error
    \begin{align*}
        \vert k(\textbf{x},\textbf{x}')-\tilde{k}(\textbf{x},\textbf{x}')\vert&\leq  \delta_{m,L}+\sqrt{L}c^d\delta_{m,L}
        % &=c^d\frac{2M_\rho(g)}{\rho-1}\rho^{-L}+\sqrt{L}c^{2d} \frac{2M_\rho(g)}{\rho-1}\rho^{-L}\\
        % &=O\left(c^d\sqrt{L}\rho^{-L}\right)
    \end{align*}
    where $\delta_{m,L}$ is an upper bound on the error of multivariate convolutional interpolation with $m$ inducing points and interpolation degree $L-1$ and $c>0$ is an upper bound on the absolute sum of the weights for 1d kernel interpolation. If the convolutional interpolation is cubic \cite{keys1981cubic}, then we have
    \begin{align*}
        \vert k(\textbf{x},\textbf{x}')-\tilde{k}(\textbf{x},\textbf{x}')\vert&\leq \sqrt{L}O(c^{2d}h^3)
    \end{align*}
\end{lemma}
\begin{proof}
Recall that SKI approximates the kernel as
\begin{align*}
k(\mathbf{x}, \mathbf{x}') &\approx \tilde{k}(\mathbf{x}, \mathbf{x}')\\
&= \boldsymbol{w}(\mathbf{x})^\top \mathbf{K}_{\textbf{U}} \boldsymbol{w}(\mathbf{x}'),
\end{align*}

Let $\textbf{K}_{\textbf{U},\textbf{x}'}\in \mathbb{R}^m$ be the vector of kernels between the inducing points and the vector $\textbf{x}'$
\begin{align}
\vert k(\mathbf{x}, \mathbf{x}') -\tilde{k}(\mathbf{x}, \mathbf{x}')\vert &= \vert k(\mathbf{x}, \mathbf{x}')-\boldsymbol{w}(\mathbf{x})^\top \textbf{K}_{\textbf{U},\textbf{x}'} +\boldsymbol{w}(\mathbf{x})^\top \textbf{K}_{\textbf{U},\textbf{x}'}-\boldsymbol{w}(\mathbf{x})^\top \mathbf{K}_{\textbf{U}} \boldsymbol{w}(\mathbf{x}')\vert\nonumber\\
&\leq  \vert k(\mathbf{x}, \mathbf{x}')-\boldsymbol{w}(\mathbf{x})^\top \textbf{K}_{\textbf{U},\textbf{x}'} \vert+\vert \boldsymbol{w}(\mathbf{x})^\top \textbf{K}_{\textbf{U},\textbf{x}'}-\boldsymbol{w}(\mathbf{x})^\top \mathbf{K}_{\textbf{U}} \boldsymbol{w}(\mathbf{x}')\vert\nonumber\\
&\leq \delta_{m,L}+\vert \boldsymbol{w}(\mathbf{x})^\top \textbf{K}_{\textbf{U},\textbf{x}'}-\boldsymbol{w}(\mathbf{x})^\top \mathbf{K}_{\textbf{U}} \boldsymbol{w}(\mathbf{x}')\vert \textrm{ since $\vert k(\mathbf{x}, \mathbf{x}')-\boldsymbol{w}(\mathbf{x})^\top \textbf{K}_{\textbf{U},\textbf{x}'} \vert$ is a single polynomial interpolation}\label{eqn:applying-single-poly-interp}
\end{align}
Now note that $\textbf{w}(x)\in \mathbb{R}^m$ is a sparse matrix with at most $L$ non-zero entries. Thus, letting $\tilde{\textbf{w}}(x)\in \mathbb{R}^L$ be the non-zero entries of $\textbf{w}(x)$ and similarly $\tilde{\textbf{K}}_{\textbf{U},\textbf{x}'}\in \mathbb{R}^L$ be the entries of $\textbf{K}_{\textbf{U},\textbf{x}'}$ in the dimensions corresponding to non-zero entries of $\textbf{w}(x)\in \mathbb{R}^m$, while $\tilde{\textbf{K}}_{\textbf{U}}\in \mathbb{R}^{L\times m}$ is the analogous matrix for $\textbf{K}_{\textbf{U}}$, we have
\begin{align}
    \vert \boldsymbol{w}(\mathbf{x})^\top \textbf{K}_{\textbf{U},\textbf{x}'}-\boldsymbol{w}(\mathbf{x})^\top \mathbf{K}_{\textbf{U}} \boldsymbol{w}(\mathbf{x}')\vert&=\vert \tilde{\textbf{w}}(\textbf{x})^\top \tilde{\textbf{K}}_{\textbf{U},\textbf{x}'}-\tilde{\textbf{w}}(\textbf{x})^\top \tilde{\textbf{K}}_\textbf{U}\textbf{w}(\textbf{x}')\vert\nonumber\\
    &\leq \Vert \tilde{\textbf{w}}(\textbf{x})\Vert_2\Vert \tilde{\textbf{K}}_{\textbf{U},\textbf{x}'}-\tilde{\mathbf{K}}_{\textbf{U}} \boldsymbol{w}(\mathbf{x}')\Vert_2\nonumber\\
    &\leq c^d\sqrt{L}\Vert \tilde{\textbf{K}}_{\textbf{U},\textbf{x}'}-\tilde{\mathbf{K}}_{\textbf{U}} \boldsymbol{w}(\mathbf{x}')\Vert_\infty\nonumber\\
    &\leq \sqrt{L}c^d\delta_{m,L}\label{eqn:applying-sparsity-single-kernel-evaluation}
\end{align}
where the last line follows as each element of $\mathbf{K}_{\textbf{U}} \boldsymbol{w}(\mathbf{x}')$ is a polynomial interpolation approximating each element of $\textbf{K}_{\textbf{U},\textbf{x}'}$. Plugging Eqn. \ref{eqn:applying-sparsity-single-kernel-evaluation} into Eqn. \ref{eqn:applying-single-poly-interp} gives us the desired initial result and Lemma \ref{lemma:tensor-product-interpolation-error} gives us the result when the convolutional kernel is cubic.
\end{proof}

\subsubsection{Spectral Norm Error}\label{subsubsec:spectral-norm-error}

We now transition from elementwise error bounds for the SKI kernel approximation to bounds on the spectral norm of the difference between the true Gram matrix and its SKI approximation. This is both of independent interest but will also be important to analyzing the error of the SKI log-determinant and action of the SKI regularized inverse: both important quantities for hyperparameter optimization and posterior inference. We also provide a bound on the spectral norms of both the SKI train/test kernel matrix's approximation error and the matrix itself. These are useful when analyzing the error of the predictive mean and variance in Gaussian process regression. 

\begin{proposition}\label{prop:spectral-norm}
    Let $\gamma_{n,m,L}\equiv n(\delta_{m,L}+\sqrt{L}c^d\delta_{m,L})$, where $\delta_{m,L}$ is an upper bound on the error of multivariate interpolation with $m$ inducing points and interpolation degree $L-1$, and $c>0$ is an upper bound on the absolute sum of the weights for 1D kernel interpolation. Then, for the SKI approximation $\tilde{\textbf{K}}$ of the true Gram matrix $\textbf{K}$, we have
    \begin{align*}
        \Vert \textbf{K}-\tilde{\textbf{K}}\Vert_2&\leq \gamma_{n,m,L}
    \end{align*}
    and for tensor-product cubic convolutional interpolation we have
    \begin{align*}
        \Vert \textbf{K}-\tilde{\textbf{K}}\Vert_2&\leq O(nc^{2d} h^3)
    \end{align*}
\end{proposition}
\begin{proof}
    Recall that for any matrix $\textbf{A}$, $\Vert \textbf{A} \Vert_2 \leq \sqrt{\Vert \textbf{A} \Vert_1 \Vert \textbf{A} \Vert_\infty}$. Since $\textbf{K}-\tilde{\textbf{K}}$ is symmetric, we have
    \begin{align*}
        \Vert \textbf{K}-\tilde{\textbf{K}}\Vert_2&\leq \sqrt{\Vert \textbf{K}-\tilde{\textbf{K}}\Vert_1\Vert \textbf{K}-\tilde{\textbf{K}}\Vert_\infty} = \Vert \textbf{K}-\tilde{\textbf{K}}\Vert_\infty
    \end{align*}
    Furthermore, $\Vert \textbf{K}-\tilde{\textbf{K}}\Vert_\infty$ is the maximum absolute row sum of $\textbf{K}-\tilde{\textbf{K}}$. Since there are $n$ rows and, by Lemma \ref{lemma:ski-kernel-elementwise-error}, each element of $\textbf{K} - \tilde{\textbf{K}}$ is bounded by $\delta_{m,L}+\sqrt{L}c^d\delta_{m,L}$ in absolute value, we have
    \begin{align*}
        \Vert \textbf{K}-\tilde{\textbf{K}}\Vert_\infty &\leq n \left(\delta_{m,L}+\sqrt{L} c^d\delta_{m,L}\right) = \gamma_{n,m,L}.
    \end{align*}
    Therefore, $\Vert \textbf{K}-\tilde{\textbf{K}}\Vert_2 \leq \gamma_{n,m,L}$.
\end{proof}

\begin{lemma}\label{lemma:test-train-kernel-matrix-error}
    Let $\textbf{K}_{\cdot,\textbf{X}}\in \mathbb{R}^{T\times n}$ be the matrix of kernel evaluations between $T$ test points and $n$ training points, and let $\tilde{\textbf{K}}_{\cdot,\textbf{X}}\in \mathbb{R}^{T\times n}$ be the corresponding SKI approximation. Then
    \begin{align*}
        \Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_2&\leq \max(\gamma_{T,m,L},\gamma_{n,m,L}).
    \end{align*}
\end{lemma}
\begin{proof}
    Using the same reasoning as in Proposition \ref{prop:spectral-norm}, we have
    \begin{align*}
        \Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_2&\leq \sqrt{\Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_1\Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_\infty} \\
        &\leq \max \left(\Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_1, \Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_\infty\right).
    \end{align*}
    Now, $\Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_1$ is the maximum absolute column sum, which is less than or equal to $T(\delta_{m,L} + \sqrt{L}c^d\delta_{m,L}) = \gamma_{T,m,L}$. Similarly, $\Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_\infty$ is the maximum absolute row sum, which is upper bounded by $n(\delta_{m,L} + \sqrt{L}c^d\delta_{m,L}) = \gamma_{n,m,L}$. Therefore,
    $$
    \Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_2 \leq \max(\gamma_{T,m,L},\gamma_{n,m,L}).
    $$
\end{proof}

\begin{lemma}\label{lemma:ski-test-train-bound}
    Let $\tilde{\textbf{K}}_{\cdot,\textbf{X}} \in \mathbb{R}^{T \times n}$ be the matrix of SKI kernel evaluations between $T$ test points and $n$ training points, where the SKI approximation uses $m$ inducing points. Let $\textbf{W}(\cdot) \in \mathbb{R}^{T \times m}$ and $\textbf{W}(\textbf{X}) \in \mathbb{R}^{n \times m}$ be the matrices of interpolation weights for the test points and training points, respectively. Assume that the interpolation scheme is such that the sum of the absolute values of the interpolation weights for any point is bounded by $c^d$, where $c>0$ is a constant. Let $\textbf{K}_{\textbf{U}} \in \mathbb{R}^{m \times m}$ be the kernel matrix evaluated at the inducing points. If the kernel function $k$ is bounded such that $|k(\textbf{x}, \textbf{x}')| \leq M$ for all $\textbf{x}, \textbf{x}'$ in the domain of interest, then:
    $$
    \Vert \tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_2 \leq \sqrt{Tn} m c^{2d} M
    $$
\end{lemma}

\begin{proof}
    By the definition of the SKI approximation and the submultiplicativity of the spectral norm, we have:
    $$
    \Vert \tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_2 = \Vert \textbf{W}(\cdot)\textbf{K}_{\textbf{U}}(\textbf{W}(\textbf{X}))^\top\Vert_2 \leq \Vert \textbf{W}(\cdot)\Vert_2 \Vert\textbf{K}_{\textbf{U}}\Vert_2 \Vert\textbf{W}(\textbf{X})\Vert_2
    $$

    We now bound each term.

    1.  **Bounding $\Vert \textbf{W}(\cdot)\Vert_2$ and $\Vert \textbf{W}(\textbf{X})\Vert_2$:**
        Since the spectral norm is induced by the Euclidean norm, and using the assumption that the sum of absolute values of interpolation weights for any point is bounded by $c^d$, we have
        $$\Vert \textbf{W}(\cdot)\Vert_2 \leq \sqrt{\Vert \textbf{W}(\cdot)\Vert_1 \Vert \textbf{W}(\cdot)\Vert_\infty} \leq \sqrt{T c^d \cdot c^d} = \sqrt{T} c^d.$$
        Similarly, $\Vert\textbf{W}(\textbf{X})\Vert_2 \leq \sqrt{n}c^d$.

    2.  **Bounding $\Vert\textbf{K}_{\textbf{U}}\Vert_2$:**
        Since $\textbf{K}_{\textbf{U}}$ is symmetric, $\Vert \textbf{K}_{\textbf{U}} \Vert_2 \leq \Vert \textbf{K}_{\textbf{U}} \Vert_\infty$. Each entry of $\textbf{K}_{\textbf{U}}$ is bounded by $M$ (by the boundedness of $k$), and each row has $m$ entries, so $\Vert \textbf{K}_{\textbf{U}} \Vert_\infty \leq mM$. Thus, $\Vert\textbf{K}_{\textbf{U}}\Vert_2 \leq mM$.

    Combining these bounds, we get:
    $$
    \Vert \tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_2 \leq (\sqrt{T} c^d) (mM) (\sqrt{n} c^d) = \sqrt{Tn} m c^{2d} M
    $$
    as required.
\end{proof}


\subsection{Achieving a Desired Error in Spectral Norm}\label{subsec:desired-error-spectral-norm}
Here, we derive a lower bound on the number of inducing points $m$ required to achieve a desired error tolerance $\epsilon > 0$ for the SKI Gram matrix when using cubic convolutional interpolation. We assume the domain is $[-D, D]^d$, where $D > 0$ is a constant that determines the size of the domain.

 We can directly determine a sufficient number of inducing points needed for a given $\epsilon > 0$ and domain $[-D, D]^d$.

\begin{corollary}\label{cor:inducing-points-count-alt}
    If $L=4$ (cubic interpolation), the domain is $[-D, D]^d$, and we use convolutional cubic interpolation \cite{keys1981cubic}, then to achieve a spectral norm error of $\Vert \textbf{K} - \tilde{\textbf{K}} \Vert_2 \leq \epsilon$, it is sufficient to choose the number of inducing points $m$ such that:
    $$
    m \geq \left( \frac{n}{\epsilon} (1 + 2c^d) K' (8 c^{2d} D^3) \right)^{d/3}
    $$
    for some constant $K'$ that depends only on the kernel function and the interpolation scheme.
\end{corollary}

\begin{proof}
    We want to choose $m$ such that the spectral norm error $\Vert \textbf{K} - \tilde{\textbf{K}} \Vert_2 \leq \epsilon$. From Proposition \ref{prop:spectral-norm}, we have:
    $$
    \Vert \textbf{K} - \tilde{\textbf{K}} \Vert_2 \leq n(1 + \sqrt{L}c^d) \delta_{m,L}
    $$
    For cubic interpolation ($L=4$), Lemma \ref{lemma:tensor-product-interpolation-error}, combined with the analysis in Lemma \ref{lemma:tensor-product-interpolation-error}, gives us:
    $$
    \delta_{m,L} \leq K' c^{2d} h^3
    $$
    where $K'$ is a constant that depends only on the kernel function (through its derivatives) and the interpolation scheme, but not on $n$, $m$, $h$, or $d$.

    Therefore, a sufficient condition to ensure $\Vert \textbf{K} - \tilde{\textbf{K}} \Vert_2 \leq \epsilon$ is:
    \begin{equation} \label{eq:sufficient_condition_final}
    n(1 + 2c^d) K' c^{2d} h^3 \leq \epsilon
    \end{equation}

    Since the inducing points are placed on a regular grid with spacing $h$ in each dimension, and the domain is $[-D,D]^d$ and assuming that $2D\mod h\equiv 0$, the number of inducing points $m$ satisfies:

    $$
    m = \left(\frac{2D}{h}\right)^d
    $$

    We can rearrange this to get:

    $$
    h = \frac{2D}{m^{1/d}}
    $$
    Substituting this into the sufficient condition \eqref{eq:sufficient_condition_final}, we get:

    $$
    n (1 + 2c^d) K' c^{2d} \left(\frac{2D}{m^{1/d}}\right)^3 \leq \epsilon
    $$

    Rearranging to isolate $m$, we obtain:

    $$
    m^{3/d} \geq \frac{n}{\epsilon} (1 + 2c^d) K' c^{2d} (8D^3)
    $$

    $$
    m \geq \left( \frac{n}{\epsilon} (1 + 2c^d) K' (8 c^{2d} D^3) \right)^{d/3}
    $$
\end{proof}

This result shows that the number of inducing points grows
\begin{itemize}
\item At a cube root rate with the number of observations $n$ and as error tolerance $\epsilon$ decreases. Thus, as we want a tighter error tolerance or have more observations we need more inducing points.
\item Linearly with the volume of the domain $(2D)^d$. Thus, if our observations are concentrated in a small region and we select an appropriately sized domain to cover it we need fewer inducing points.
\item Exponentially with the dimension $d$.
\end{itemize}

\subsection{Lower Bound on Error for Linear Time}\label{subsec:linear-time}

The next theorem establishes a condition on the spectral norm error, $\epsilon$, that ensures linear-time $O(n)$ computational complexity for SKI. The core idea is to choose the number of inducing points, $m$, large enough to satisfy the error bound based on Corollary \ref{cor:inducing-points-count-alt}, while simultaneously ensuring that $m$ does not grow faster than $O(n/\log n)$, thus ensuring linear-time complexity. By combining these two requirements, we can derive a lower bound for $\epsilon$ as a function of $n$, the dimensionality $d$, the domain size $D$, and the constant $c$ related to the interpolation weights.

Interestingly, the theorem implies a fundamental difference between two dimensionality regimes. For $d \leq 3$, the lower bound on $m$ required for a fixed $\epsilon$ grows more slowly than $n/\log n$. This means that for any fixed $\epsilon > 0$, SKI with cubic interpolation is guaranteed to be a linear-time algorithm for sufficiently large $n$. In contrast, for $d > 3$, the lower bound on $m$ required for a fixed $\epsilon$ eventually grows faster than $n/\log n$. Thus, to maintain linear-time complexity for $d > 3$, we must allow the error $\epsilon$ to increase with $n$. This demonstrates that the ``curse of dimensionality" significantly impacts the scalability of SKI, making it challenging to achieve both high accuracy and linear-time complexity in higher dimensions. The theorem quantifies this trade-off, showing precisely how much error growth is necessary as $n$ and $d$ increase.

\begin{theorem}[Sufficient Condition for Linear Time Complexity]
Consider $d\geq 3$. If the domain is $[-D, D]^d$, we use convolutional cubic interpolation ($L=4$), and we choose $\epsilon>0$ satisfying
\begin{equation} \label{eq:epsilon_condition}
\epsilon \geq \frac{(1 + 2c^d) K' 8 c^{2d} D^3}{C^{3/d}} \cdot \frac{n (\log n)^{3/d}}{n^{3/d}}
\end{equation}
for some constants $K,C>0$ that depend on the kernel function and the interpolation scheme. then if we choose
\begin{align*}
    m &= \left( \frac{n}{\epsilon} (1 + 2c^d) K' (8 c^{2d} D^3) \right)^{d/3}
\end{align*}
then we have both $\Vert \textbf{K}-\tilde{\textbf{K}}\Vert_2\leq \epsilon$ and
SKI computational complexity of $O(n)$.
\end{theorem}

\begin{proof}
The idea is that we want to have an error sufficiently large so that when we choose $m$ based on Corollary \ref{cor:inducing-points-count-alt}, we have $m=O\left(\frac{n}{\log n}\right)$. 

Assume that
$$
\epsilon \geq \frac{(1 + 2c^d) K' 8 c^{2d} D^3}{C^{3/d}} \cdot \frac{n (\log n)^{3/d}}{n^{3/d}}.
$$
Rearranging this we obtain
\begin{align*}
    \left( \frac{n}{\epsilon} (1 + 2c^d) K' (8 c^{2d} D^3) \right)^{d/3}&\leq C\frac{n}{\log n}.\\
    &=O\left(\frac{n}{\log n}\right).
\end{align*}
Now taking 
\begin{align*}
    m &= \left( \frac{n}{\epsilon} (1 + 2c^d) K' (8 c^{2d} D^3) \right)^{d/3}
\end{align*}
we have that $m=O\left(\frac{n}{\log n}\right)$ and by Corollary \ref{cor:inducing-points-count-alt}, $\Vert \textbf{K}-\tilde{\textbf{K}}\Vert_2\leq \epsilon$. Now plugging in $\frac{n}{\log n}$ into $m\log m$ we obtain
\begin{align*}
    O\left(m\log m\right)&=O\left(\frac{n}{\log n}\log \frac{n}{\log n}\right)\\
    &=O\left(\frac{n}{\log n}\log n-\frac{n}{\log n}\log \log n\right)\\
    &=O(n)
\end{align*}
as desired.
\end{proof}
\subsection{Additional Quantities}\label{subsec:additional-quantities}
\subsubsection{Action of Regularized Inverse}\label{subsubsec:action-regularized-inverse}

\begin{lemma}\label{lemma:action-inverse-error}
    Let \(\mathbf{\tilde{K}}\) be the SKI approximation of the kernel matrix \(\mathbf{K}\), and let \(\sigma^2\) be the regularization parameter. For a vector \(\mathbf{y}\), the error in the action of the regularized inverse can be bounded as follows:
\begin{align*}
\left\|\left(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}\right)^{-1} \mathbf{y} - \left(\mathbf{K} + \sigma^2 \mathbf{I}\right)^{-1} \mathbf{y}\right\|_2 &\leq \frac{\gamma_{n, m, L}}{\sigma^4} \|\mathbf{y}\|_2
% \\
% &=\frac{O(n\sqrt{L}\rho^{-L})}{\sigma^4}
\end{align*}


\end{lemma} 
\begin{proof}
    Note that
\[
\left(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}\right)^{-1} - \left(\mathbf{K} + \sigma^2 \mathbf{I}\right)^{-1} = \left(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}\right)^{-1} (\mathbf{K} - \mathbf{\tilde{K}}) \left(\mathbf{K} + \sigma^2 \mathbf{I}\right)^{-1}
\]

Taking the spectral norm, we have
\[
\begin{aligned}
\left\|\left(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}\right)^{-1} - \left(\mathbf{K} + \sigma^2 \mathbf{I}\right)^{-1}\right\|_2 &\leq \left\|\left(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}\right)^{-1}\right\|_2 \|\mathbf{K} - \mathbf{\tilde{K}}\|_2 \left\|\left(\mathbf{K} + \sigma^2 \mathbf{I}\right)^{-1}\right\|_2 \\
&\leq \gamma_{n, m, L}\left\|\left(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}\right)^{-1}\right\|_2 \left\|\left(\mathbf{K} + \sigma^2 \mathbf{I}\right)^{-1}\right\|_2\textrm{ by Lemma \ref{proposition:spectral-norm}}\\
&\leq   \frac{\gamma_{n, m, L}}{\sigma^4}
\end{aligned}
\]

Thus,
\[
\begin{aligned}
\left\|\left(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}\right)^{-1} \mathbf{y} - \left(\mathbf{K} + \sigma^2 \mathbf{I}\right)^{-1} \mathbf{y}\right\|_2 &\leq  \frac{\gamma_{n, m, L}}{\sigma^4}  ||\mathbf {y}||_2 
\end{aligned}
\]

The last inequality follows from the bounds on the inverse norms and the spectral norm error bound for \(K - \tilde{K}\).
\end{proof}


\subsubsection{Log Determinant}\label{subsubsec:log-determinant}

\begin{lemma}\label{lemma:log-determinant-error}
    Let \(\mathbf{\tilde{K}}\) be the SKI approximation of the kernel matrix \(\mathbf{K}\), and let \(\sigma^2\) be the regularization parameter. The error in the log-determinant of the regularized SKI matrix can be bounded as follows:
\[
\left|\log \det(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}) - \log \det(\mathbf{K} + \sigma^2 \mathbf{I})\right| \leq \frac{\gamma_{n, m, L}}{\sigma^2}.
\]



\end{lemma}

\begin{proof}
    We begin by applying the functional Mean Value Theorem for matrices, which implies that for some matrix \(Z = (\mathbf{\tilde{K}} + \alpha (\mathbf{K} - \mathbf{\tilde{K}})) + \sigma^2 \mathbf{I}\) with \(\alpha \in [0, 1]\),
\[
|f(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}) - f(\mathbf{K} + \sigma^2 \mathbf{I})| \leq \|f'(Z)\|_2 \|\mathbf{K} - \mathbf{\tilde{K}}\|_2
\]
where \(f(X) = \log |X|\) and thus \(f'(X) = X^{-\top}\).

Thus,
\[
|\log \det(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}) - \log \det(\mathbf{K} + \sigma^2 \mathbf{I})| \leq  \left\| (\mathbf{\tilde{K}} + \alpha (\mathbf{K} - \mathbf{\tilde{K}}) + \sigma^2 I)^{-1}  \right\|_2  ||\mathbf {K} -  {\tilde {\mathbf {K}}}||_2\label{eqn:initial-log-det-bound}
\]

Using Claim 1, which bounds the eigenvalues of a convex combination of matrices, we have:
\[
\lambda_{\min}(Z) =  (\lambda_{\min} (\mathbf{\tilde{K}})+ (1-\alpha)\lambda_{\min} (\mathbf {K}))+\sigma^2   > 0
\]

Therefore since $\mathbf{\tilde{K}}$ and $(\mathbf {K}$ are SPD and thus have positive eigenvalues,
\[
\left\| (\mathbf{\tilde{K}} +  (1-\alpha)(\mathbf {K}-  {\tilde {\mathbf {K}}})+  {\sigma^2 I})^{-1}  )^{-1}\right\|_2   <   {\frac {1}{ {\sigma^2}}}
\]

Substituting these bounds into Eqn. \ref{eqn:initial-log-det-bound} gives:
\[
|\log \det (\tilde{\mathbf {K}}+ {\sigma^2 I})- \log \det ({\mathbf {K}}+ {\sigma^2 I})| <   {\frac {1}{ {\sigma^2}}} ||{\mathbf {K}-  {\tilde {\mathbf {K}}}}||_2 
\leq   {\frac {1}{ {\sigma^2}}}  {\gamma_{n,m,L}}
\]

\end{proof}

\section{Gaussian Processes Applications}\label{sec:gp-applications}

\subsection{Kernel Hyperparameter Estimation and Posterior Inference}
\subsubsection{Difference in Log-Likelihoods}
\begin{lemma}\label{lemma:difference-in-log-likelihoods}
Let \( \mathbf{K} \) be the true kernel matrix, \( \tilde{\mathbf{K}} \) be the SKI approximation of the kernel matrix using \( m \) inducing points and interpolation degree \( L-1 \), and \( \sigma^2 \) be the regularization parameter (noise variance). Let \( \mathbf{y} \) be the vector of observed target values, and \( n \) be the number of data points. Let \( \mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) \) be the true log-likelihood and \( \tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y}) \) be the SKI approximation of the log-likelihood, both for a mean-zero Gaussian Process prior. Then, the absolute difference between the true and SKI log-likelihoods is bounded as follows:

\begin{equation}
| \mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) - \tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y}) | \leq \frac{1}{2} \left( \frac{\gamma_{n, m, L}}{\sigma^4} \| \mathbf{y} \|^2 + \frac{\gamma_{n, m, L}}{\sigma^2} \right)
\end{equation}

where \( \gamma_{n, m, L} \equiv n(\delta_{m, L} + \sqrt{L} c^d \delta_{m, L}) \), \( \delta_{m, L} \) is an upper bound on the error of multivariate interpolation with \( m \) inducing points and interpolation degree \( L-1 \), and \( c > 0 \) is an upper bound on the absolute sum of the weights for 1D kernel interpolation.

Furthermore, if the convolutional cubic interpolation kernel (\( L=4 \)) is used with grid spacing \( h \), and the domain is contained in $[-D, D]^d$, then:

\begin{equation}
| \mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) - \tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y}) | \leq \frac{1}{2} \left( \frac{n(2O(c^{2d}h^3))}{\sigma^4} \| \mathbf{y} \|^2 + \frac{n(2O(c^{2d}h^3))}{\sigma^2} \right)
\end{equation}
\end{lemma}

\begin{proof}
We begin by considering the log-likelihood of a mean-zero Gaussian Process, given by:

\begin{equation}
\mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) = -\frac{1}{2} \mathbf{y}^\top (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log|\mathbf{K} + \sigma^2 \mathbf{I}| - \frac{n}{2} \log(2\pi)
\end{equation}

The corresponding SKI approximation is:

\begin{equation}
\tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y}) = -\frac{1}{2} \mathbf{y}^\top (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log|\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| - \frac{n}{2} \log(2\pi)
\end{equation}

Our goal is to bound the absolute difference \( |\mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) - \tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y})| \). Applying the triangle inequality, we have:

\begin{align}
| \mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) - \tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y}) | &= \left| \frac{1}{2} \mathbf{y}^\top (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \mathbf{y}^\top (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} + \frac{1}{2} \log|\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| - \frac{1}{2} \log|\mathbf{K} + \sigma^2 \mathbf{I}| \right| \nonumber \\
&\leq \left| \frac{1}{2} \mathbf{y}^\top (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \mathbf{y}^\top (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} \right| + \left| \frac{1}{2} \log|\mathbf{K} + \sigma^2 \mathbf{I}| - \frac{1}{2} \log|\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| \right|
\end{align}

We now leverage Lemma \ref{lemma:action-inverse-error} and Lemma \ref{lemma:log-determinant-error} to bound the two terms on the right-hand side. Lemma \ref{lemma:action-inverse-error} provides a bound on the difference between the action of the true and approximated regularized inverses:

\begin{equation}
\| (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} \|_2 \leq \frac{\gamma_{n, m, L}}{\sigma^4} \| \mathbf{y} \|_2
\end{equation}

This allows us to bound the first term:

\begin{equation}
\left| \frac{1}{2} \mathbf{y}^\top (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \mathbf{y}^\top (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} \right| \leq \frac{1}{2} \frac{\gamma_{n, m, L}}{\sigma^4} \| \mathbf{y} \|^2
\end{equation}

Lemma \ref{lemma:log-determinant-error} bounds the difference in the log-determinants:

\begin{equation}
| \log|\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| - \log|\mathbf{K} + \sigma^2 \mathbf{I}| | \leq \frac{\gamma_{n, m, L}}{\sigma^2}
\end{equation}

Thus, the second term is bounded by:

\begin{equation}
\left| \frac{1}{2} \log|\mathbf{K} + \sigma^2 \mathbf{I}| - \frac{1}{2} \log|\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| \right| \leq \frac{1}{2} \frac{\gamma_{n, m, L}}{\sigma^2}
\end{equation}

Combining these bounds, we obtain the overall bound on the difference in log-likelihoods:

\begin{equation}
| \mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) - \tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y}) | \leq \frac{1}{2} \left( \frac{\gamma_{n, m, L}}{\sigma^4} \| \mathbf{y} \|^2 + \frac{\gamma_{n, m, L}}{\sigma^2} \right)
\end{equation}

Now, let us consider the specific case of using a convolutional cubic interpolation kernel (\( L=4 \)). From Lemma \ref{lemma:tensor-product-interpolation-error}, we have \( \delta_{m, L} = O(c^{2d}h^3) \). Consequently, from Corollary \ref{cor:spectral-norm-error-alt}:

\begin{equation}
\gamma_{n, m, L} \equiv n(\delta_{m, L} + \sqrt{L} c^d \delta_{m, L}) = n(O(c^{2d}h^3) + 2c^d O(c^{2d}h^3)) = n(2O(c^{2d}h^3))
\end{equation}

Substituting this into our general bound, we obtain the specific bound for cubic interpolation:

\begin{equation}
| \mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) - \tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y}) | \leq \frac{1}{2} \left( \frac{n(2O(c^{2d}h^3))}{\sigma^4} \| \mathbf{y} \|^2 + \frac{n(2O(c^{2d}h^3))}{\sigma^2} \right)
\end{equation}

This concludes the proof.
\end{proof}


\subsubsection{Difference in MLEs}


We formalize the intuition that a good initialization for gradient ascent on the SKI log-likelihood, combined with a sufficiently accurate SKI approximation, leads to a SKI MLE that is close to the true MLE. We achieve this by first bounding the difference in gradients between the true and SKI log-likelihoods. Then, under the assumption of strong concavity of the true log-likelihood, we show that if the initialization is close to the true MLE, the resulting SKI MLE will also be close to the true MLE.

\begin{assumption}[Strong Concavity of the Population Log-Likelihood]\label{assumption:strong-concavity}
The population log-likelihood $\bar{\mathcal{L}}(\theta) = \mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$ is strongly concave with parameter $\mu > 0$ in a neighborhood $\mathcal{D}$ of the true hyperparameters $\theta^*$. That is, for all $\theta_1, \theta_2 \in \mathcal{D}$,
$$
\bar{\mathcal{L}}(\theta_2) \leq \bar{\mathcal{L}}(\theta_1) + \nabla \bar{\mathcal{L}}(\theta_1)^\top (\theta_2 - \theta_1) - \frac{\mu}{2} \| \theta_2 - \theta_1 \|_2^2.
$$
\end{assumption}

\begin{proposition}\label{prop:strong_concavity_hessian}
    A function is strongly concave if and only if the Hessian matrix $H_f(\theta)$ satisfies $H_f(\theta) \preceq -\mu I$ for all $\theta \in \mathcal{D}$, where $\preceq$ denotes the negative semidefinite ordering.
\end{proposition}
\begin{proof}
    See \cite{boyd2004convex}.
\end{proof}
\begin{assumption}[Local Identifiability of Hyperparameters]\label{assumption:local-identifiability}
Let $P(\cdot | \mathbf{X}; \theta)$ denote the Gaussian process predictive distribution at a set of input points $\mathbf{X}$ given hyperparameters $\theta$. There exists a neighborhood $\mathcal{D}$ of the true hyperparameters $\theta^*$ such that for any $\theta \in \mathcal{D}$ with $\theta \neq \theta^*$, there exists a finite set of input points $\mathbf{X} = \{\mathbf{x}_1, \ldots, \mathbf{x}_n\}$, a set $A$ in the Borel sigma-algebra of $\mathbb{R}^n$, and a $\delta > 0$ such that:

$$
|P(\mathbf{y} \in A | \mathbf{X}; \theta) - P(\mathbf{y} \in A | \mathbf{X}; \theta^*)| > \delta
$$

and

$$
P^*_X(\mathbf{X}) > 0,
$$

where $\mathbf{y} = (y_1, \ldots, y_n)$ is the vector of corresponding output values, and $P^*_X$ is the marginal distribution of inputs under $P^*$.
\end{assumption}

\begin{lemma}[Bound on Derivative of SKI Kernel Error using Kernel Property of Derivative]
\label{lemma:ski_kernel_derivative_error_kernel}
Let $\mathcal{X} \subset \mathbb{R}^d$ be the input domain, and let $\Theta \subset \mathbb{R}^p$ be the domain of the kernel hyperparameters. Let $k_{\theta}: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ be a kernel function parameterized by $\theta \in \Theta$, and let $\tilde{k}_{\theta}: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ be its SKI approximation using $m$ inducing points and interpolation degree $L-1$.  Assume that the partial derivative of $k_{\theta}$ with respect to a hyperparameter $\theta_i$, denoted as $k'_{\theta}(x, x') = \frac{\partial k_{\theta}(x, x')}{\partial \theta_i}$, is also a valid SPD kernel. Let $\tilde{k}'_{\theta}(x,x')$ be the SKI approximation of $k'_{\theta}(x,x')$, using the same inducing points and interpolation scheme as $\tilde{k}_{\theta}$.

Then, for all $x, x' \in \mathcal{X}$, all $\theta \in \Theta$, the following inequality holds:

\begin{equation}
\left\vert \frac{\partial k_{\theta}(x,x')}{\partial \theta_i}-\frac{\partial \tilde{k}_{\theta}(x,x')}{\partial \theta_i}\right\vert = \left\vert k'_{\theta}(x, x') - \tilde{k}'_{\theta}(x, x') \right\vert \leq\delta_{m,L}'+\sqrt{L}c^d\delta_{m,L}',
\end{equation}

where $\delta_{m,L}'$ is an upper bound on the error of the SKI approximation of the kernel $k'_{\theta}(x,x')$ with $m$ inducing points and interpolation degree $L-1$, as defined in Lemma \ref{lemma:ski-kernel-elementwise-error}.
\end{lemma}

\begin{proof}
By assumption, $k'_{\theta}(x, x') = \frac{\partial k_{\theta}(x, x')}{\partial \theta_i}$ is a valid SPD kernel. The SKI approximation of $k'_{\theta}(x, x')$ using the same inducing points and interpolation scheme as $\tilde{k}_{\theta}(x, x')$ is given by $\tilde{k}'_{\theta}(x, x')$. For the kernel $k'_{\theta}(x, x')$, we have:

\begin{align*}
\left\vert k'_{\theta}(x, x') - \tilde{k}'_{\theta}(x, x') \right\vert \leq \delta_{m,L}',
\end{align*}

where $\delta_{m,L}'$ is the upper bound on the error of the SKI approximation of $k'_{\theta}(x, x')$ as defined in Lemma \ref{lemma:ski-kernel-elementwise-error}.

Now, we need to show that $\frac{\partial \tilde{k}_{\theta}(x,x')}{\partial \theta_i} = \tilde{k}'_{\theta}(x, x')$. Recall that the SKI approximation $\tilde{k}_{\theta}(x, x')$ is a linear combination of kernel evaluations at inducing points, with weights that depend on $x$ and $x'$:

\begin{align*}
\tilde{k}_{\theta}(x, x') = \sum_{j=1}^m \sum_{l=1}^m w_{jl}(x, x') k_{\theta}(u_j, u_l)
\end{align*}


where $w_{jl}(x, x')$ are the interpolation weights. Taking the partial derivative with respect to $\theta_i$, we get:
\begin{align*}
\frac{\partial \tilde{k}_{\theta}(x, x')}{\partial \theta_i} &= \sum_{j=1}^m \sum_{l=1}^m w_{jl}(x, x') \frac{\partial k_{\theta}(u_j, u_l)}{\partial \theta_i} \\
&= \sum_{j=1}^m \sum_{l=1}^m w_{jl}(x, x') k'_{\theta}(u_j, u_l).
\end{align*}


This is precisely the SKI approximation of the kernel $k'_{\theta}(x, x')$ using the same inducing points and weights:
\begin{align*}
\tilde{k}'_{\theta}(x, x') = \sum_{j=1}^m \sum_{l=1}^m w_{jl}(x, x') k'_{\theta}(u_j, u_l).
\end{align*}

Therefore, $\frac{\partial \tilde{k}_{\theta}(x,x')}{\partial \theta_i} = \tilde{k}'_{\theta}(x, x')$.

Substituting this into our inequality, we get:
\begin{align*}
\left\vert \frac{\partial k_{\theta}(x,x')}{\partial \theta_i}-\frac{\partial \tilde{k}_{\theta}(x,x')}{\partial \theta_i}\right\vert &= \left\vert k'_{\theta}(x, x') - \tilde{k}'_{\theta}(x, x') \right\vert \\
&\leq \delta_{m,L}'+\sqrt{L}c^d\delta_{m,L}'.
\end{align*}

\end{proof}
\begin{proposition}[Bound on Spectral Norm of Gradient Difference]
\label{prop:gradient_spectral_norm_bound}
Let $\mathcal{X} \subset \mathbb{R}^d$ be the input domain, and let $\Theta \subset \mathbb{R}^p$ be the domain of the kernel hyperparameters. Let $k_{\theta}: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ be a kernel function parameterized by $\theta \in \Theta$, and let $\tilde{k}_{\theta}: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ be its SKI approximation using $m$ inducing points and interpolation degree $L-1$. Let $K \in \mathbb{R}^{n \times n}$ and $\tilde{K} \in \mathbb{R}^{n \times n}$ be the corresponding kernel matrices for a dataset of $n$ data points $\{\mathbf{x}_i\}_{i=1}^n$.

Assume that for each $i \in \{1, ..., p\}$, the partial derivative of $k_{\theta}$ with respect to $\theta_i$, denoted as $k'_{\theta,i}(x, x') = \frac{\partial k_{\theta}(x, x')}{\partial \theta_i}$, is a valid SPD kernel. Let $\tilde{k}'_{\theta,i}(x,x')$ be the SKI approximation of $k'_{\theta,i}(x,x')$, using the same inducing points and interpolation scheme as $\tilde{k}_{\theta}$.

Then, the spectral norm of the difference between the gradient of the true kernel matrix and the gradient of the SKI kernel matrix is bounded by:

\begin{equation}
\| \nabla_\theta K - \nabla_\theta \tilde{K} \|_2 \leq \sqrt{p} \max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i},
\end{equation}

where $\nabla_\theta K$ and $\nabla_\theta \tilde{K}$ are represented as $p \times n^2$ matrices using the vec-notation (denominator layout), $p$ is the number of hyperparameters, and $\gamma'_{n,m,L,i}$ is the bound on the spectral norm difference between the kernel matrices corresponding to $k'_{\theta,i}$ and its SKI approximation $\tilde{k}'_{\theta,i}$ (analogous to Proposition \ref{prop:spectral-norm}, but for the kernel $k'_{\theta,i}$).
\end{proposition}

\begin{proof}
We represent the gradients $\nabla_\theta K$ and $\nabla_\theta \tilde{K}$ as $p \times n^2$ matrices using the vec-notation (denominator layout):
\begin{align*}
\nabla_\theta K = \begin{bmatrix} \frac{\partial \text{vec}(K)}{\partial \theta_1} & \frac{\partial \text{vec}(K)}{\partial \theta_2} & \cdots & \frac{\partial \text{vec}(K)}{\partial \theta_p} \end{bmatrix}^T, \\
\nabla_\theta \tilde{K} = \begin{bmatrix} \frac{\partial \text{vec}(\tilde{K})}{\partial \theta_1} & \frac{\partial \text{vec}(\tilde{K})}{\partial \theta_2} & \cdots & \frac{\partial \text{vec}(\tilde{K})}{\partial \theta_p} \end{bmatrix}^T.
\end{align*}

Let $K'_{\theta,i}$ be the kernel matrix corresponding to the kernel $k'_{\theta,i}(x,x') = \frac{\partial k_{\theta}(x,x')}{\partial \theta_i}$, and let $\tilde{K}'_{\theta,i}$ be the kernel matrix corresponding to its SKI approximation $\tilde{k}'_{\theta,i}(x,x')$.

From Lemma \ref{lemma:ski_kernel_derivative_error_kernel}, we have:

\begin{equation}
\frac{\partial \tilde{k}_{\theta}(x,x')}{\partial \theta_i} = \tilde{k}'_{\theta,i}(x, x')
\end{equation}

Thus, the $i$-th row of $\nabla_\theta K - \nabla_\theta \tilde{K}$ can be written as:

\begin{equation}
\frac{\partial \text{vec}(K)}{\partial \theta_i} - \frac{\partial \text{vec}(\tilde{K})}{\partial \theta_i} = \text{vec}(K'_{\theta,i}) - \text{vec}(\tilde{K}'_{\theta,i}).
\end{equation}

Therefore, we can write:
\begin{equation}
    \nabla_\theta K - \nabla_\theta \tilde{K} = \begin{bmatrix}
    \text{vec}(K'_{\theta,1}) - \text{vec}(\tilde{K}'_{\theta,1}) \\
    \vdots \\
    \text{vec}(K'_{\theta,p}) - \text{vec}(\tilde{K}'_{\theta,p})
    \end{bmatrix}^T
\end{equation}
Then, we have that:
\begin{align}
    (\nabla_\theta K - \nabla_\theta \tilde{K})^T = \begin{bmatrix}
    \text{vec}(K'_{\theta,1}) - \text{vec}(\tilde{K}'_{\theta,1}) &
    \cdots &
    \text{vec}(K'_{\theta,p}) - \text{vec}(\tilde{K}'_{\theta,p})
    \end{bmatrix}
\end{align}

Now we bound the spectral norm of $(\nabla_\theta K - \nabla_\theta \tilde{K})^T$:

\begin{align*}
\| (\nabla_\theta K - \nabla_\theta \tilde{K})^T \|_2 &\leq \| (\nabla_\theta K - \nabla_\theta \tilde{K})^T \|_F \\
&= \left( \sum_{i=1}^p \| \text{vec}(K'_{\theta,i}) - \text{vec}(\tilde{K}'_{\theta,i}) \|_2^2 \right)^{1/2} \\
&= \left( \sum_{i=1}^p \| K'_{\theta,i} - \tilde{K}'_{\theta,i} \|_F^2 \right)^{1/2} \\
&\leq \left( \sum_{i=1}^p \left( \max_{j \in \{1,\ldots,p\}} \| K'_{\theta,j} - \tilde{K}'_{\theta,j} \|_2 \right)^2 \right)^{1/2} 
\end{align*}

By Proposition \ref{prop:spectral-norm}, we have a bound on the spectral norm difference between a kernel matrix and its SKI approximation.  Let $\gamma'_{n,m,L,i}$ be the corresponding bound for the kernel $k'_{\theta,i}$ and its SKI approximation $\tilde{k}'_{\theta,i}$. Then:

\begin{equation}
\| K'_{\theta,i} - \tilde{K}'_{\theta,i} \|_2 \leq \gamma'_{n,m,L,i}
\end{equation}

for each $i \in \{1, \ldots, p\}$. Thus,

\begin{align*}
\| (\nabla_\theta K - \nabla_\theta \tilde{K})^T \|_2 &\leq \left( \sum_{i=1}^p (\gamma'_{n,m,L,i})^2 \right)^{1/2} \\
&\leq \left( \sum_{i=1}^p \left(\max_{j \in \{1,\ldots,p\}} \gamma'_{n,m,L,j}\right)^2 \right)^{1/2} \\
&= \left( p \left(\max_{j \in \{1,\ldots,p\}} \gamma'_{n,m,L,j}\right)^2 \right)^{1/2} \\
&= \sqrt{p} \max_{j \in \{1,\ldots,p\}} \gamma'_{n,m,L,j}
\end{align*}

This completes the proof.
\end{proof}


\begin{lemma}[Corrected Bound on Gradient Difference]\label{lemma:gradient-difference-bound-corrected}
Let $\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})$ be the true log-likelihood and $\tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y})$ be the SKI approximation of the log-likelihood. Let $\nabla \mathcal{L}(\theta)$ and $\nabla \tilde{\mathcal{L}}(\theta)$ denote their respective gradients with respect to $\theta$. Then, for any $\theta$,
$$
\| \nabla \mathcal{L}(\theta) - \nabla \tilde{\mathcal{L}}(\theta) \|_2 \leq \frac{1}{\sigma^4}\left[\Vert y\Vert_1^2\left(\max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i}+2 C_1(\theta)\gamma_{n,m,L}\right)+\frac{1}{2}\gamma_{n,m,L}\right],
$$
where $\gamma_{n,m,L}$ bounds the elementwise difference between $\mathbf{K}$ and $\tilde{\mathbf{K}}$, and $\gamma'_{n,m,L,i}$ is the bound on the spectral norm difference between the kernel matrices corresponding to $k'_{\theta,i}$ and its SKI approximation $\tilde{k}'_{\theta,i}$ and $C_1(\theta)$ are constants that depend on $\theta$ and the derivatives of the kernel function.
\end{lemma}

\begin{proof}
We start with the expressions for the gradients:

$$
\nabla \mathcal{L}(\theta) = \nabla \left( -\frac{1}{2} \mathbf{y}^\top (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log |\mathbf{K} + \sigma^2 \mathbf{I}| - \frac{n}{2} \log(2\pi) \right).
$$

$$
\nabla \tilde{\mathcal{L}}(\theta) = \nabla \left( -\frac{1}{2} \mathbf{y}^\top (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log |\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| - \frac{n}{2} \log(2\pi) \right).
$$

Thus, the difference is:

\begin{align*}
\| \nabla \mathcal{L}(\theta) - \nabla \tilde{\mathcal{L}}(\theta) \|_2 &= \left\| \nabla \left( -\frac{1}{2} \mathbf{y}^\top (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log |\mathbf{K} + \sigma^2 \mathbf{I}| \right) \right. \\
&\quad \left. - \nabla \left( -\frac{1}{2} \mathbf{y}^\top (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log |\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| \right) \right\|_2 \\
&\leq \underbrace{\left\| \nabla \left( \frac{1}{2} \mathbf{y}^\top \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right) \mathbf{y} \right) \right\|_2}_{T_1} \\
&\quad + \underbrace{\left\| \frac{1}{2} \nabla \left( \log |\mathbf{K} + \sigma^2 \mathbf{I}| - \log |\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| \right) \right\|_2}_{T_2}.
\end{align*}

We will bound $T_1$ and $T_2$ separately.

**Bounding $T_1$:**

Using the chain rule and the matrix derivative identity $\frac{\partial}{\partial \theta} \mathbf{X}^{-1} = -\mathbf{X}^{-1} (\frac{\partial \mathbf{X}}{\partial \theta}) \mathbf{X}^{-1}$, we have
\begin{align*}
    T_1 &= \frac{1}{2} \left\| \nabla_\theta \left( \mathbf{y}^\top \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right) \mathbf{y} \right) \right\|_2 \\
    &= \frac{1}{2} \left\| \sum_{i=1}^n \sum_{j=1}^n y_i y_j \nabla_\theta \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1}_{ij} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}_{ij} \right) \right\|_2 \\
    &\leq \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n |y_i y_j| \left\| \nabla_\theta \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1}_{ij} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}_{ij} \right) \right\|_2.
\end{align*}
Now, applying the matrix derivative identity, we get:
\begin{align*}
    &\left\| \nabla_\theta \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1}_{ij} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}_{ij} \right) \right\|_2 \\
    &= \left\| \left( -(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} (\nabla_\theta \tilde{\mathbf{K}}) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} + (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} (\nabla_\theta \mathbf{K}) (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right)_{ij} \right\|_2 \\
    &= \left\| \left( -(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} (\nabla_\theta \tilde{\mathbf{K}} - \nabla_\theta \mathbf{K} + \nabla_\theta \mathbf{K}) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right.\right.\\
    &\quad \left.\left. + (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \nabla_\theta \mathbf{K} (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right)_{ij} \right\|_2 \\
    &= \left\| \left( -(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} (\nabla_\theta \tilde{\mathbf{K}} - \nabla_\theta \mathbf{K}) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right.\right.\\
    &\quad \left.\left. - (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} (\nabla_\theta \mathbf{K}) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1}  + (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} (\nabla_\theta \mathbf{K}) (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right)_{ij} \right\|_2 \\
    &\leq \underbrace{\left\| (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} (\nabla_\theta \tilde{\mathbf{K}} - \nabla_\theta \mathbf{K}) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right\|_{2}}_{(a)} \\
    &\quad + \underbrace{\left\| \left((\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\right)(\nabla_\theta \mathbf{K}) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right\|_{2}}_{(b)} \\
    &\quad + \underbrace{\left\|(\mathbf{K} + \sigma^2 \mathbf{I})^{-1}(\nabla_\theta \mathbf{K})\left((\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\right) \right\|_{2, }}_{(c)}\\
\end{align*}

We now explicitly bound (a), (b), and (c).

\begin{align*}
(a) &\leq \| (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \|_2 \| \nabla_\theta \tilde{\mathbf{K}} - \nabla_\theta \mathbf{K} \|_2 \| (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \|_2\\
&\leq \| (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \|_2^2 \| \nabla_\theta \tilde{\mathbf{K}} - \nabla_\theta \mathbf{K} \|_2 \\
&\leq \frac{1}{\sigma^4} \| \nabla_\theta \mathbf{K} - \nabla_\theta \tilde{\mathbf{K}} \|_2\text{ Proposition }\ref{prop:gradient_spectral_norm_bound}\\
&\leq  \frac{1}{\sigma^4}\sqrt{p} \max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i}.
\end{align*}

\begin{align*}
(b) &\leq \|(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\|_2 \|\nabla_\theta \mathbf{K}\|_2 \|(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1}\|_2 \\
&\leq \frac{1}{\sigma^2} \|(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\|_2 \|\nabla_\theta \mathbf{K}\|_2 \\
&\leq \frac{\gamma_{n,m,L}}{\sigma^4} \|\nabla_\theta \mathbf{K}\|_2\text{ proof of Lemma \ref{lemma:action-inverse-error}}.
\end{align*}

\begin{align*}
(c) &\leq \|(\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\|_2 \|\nabla_\theta \mathbf{K}\|_2 \|(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\|_2 \\
&\leq \frac{1}{\sigma^2} \|(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\|_2 \|\nabla_\theta \mathbf{K}\|_2 \\
&\leq \frac{\gamma_{n,m,L}}{\sigma^4} \|\nabla_\theta \mathbf{K}\|_2\text{ proof of Lemma \ref{lemma:action-inverse-error}}.
\end{align*}

\amnote{We need some sort of assumption here, some kind of continuously differentiable}$\|\nabla_\theta \mathbf{K}\|$ can be bounded by a constant, say $C_1(\theta)$. Thus, we can bound $T_1$ as:
\begin{align*}
T_1 &\leq \sum_{i,j} \vert y_iy_j\vert \frac{1}{\sigma^4} \left(\max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i}+2 C_1(\theta)\gamma_{n,m,L}\right)\\
&\leq \frac{1}{\sigma^4}\left(\max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i}+2 C_1(\theta)\gamma_{n,m,L}\right)\sum_{i,j} \vert y_iy_j\vert \\
&=\frac{\Vert y\Vert_1^2}{\sigma^4}\left(\max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i}+2 C_1(\theta)\gamma_{n,m,L}\right)
\end{align*}

**Bounding $T_2$:**

Using the identity $\nabla \log |\mathbf{X}| = (\mathbf{X}^{-1})^\top$, we have

\begin{align*}
T_2 &= \frac{1}{2} \left\| \nabla_\theta \left( \log |\mathbf{K} + \sigma^2 \mathbf{I}| - \log |\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| \right) \right\|_2 \\
&= \frac{1}{2} \left\| (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} - (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right\|_2.
\end{align*}
We can rewrite the difference as:
$$
(\mathbf{K} + \sigma^2 \mathbf{I})^{-1} - (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} = (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} (\tilde{\mathbf{K}} - \mathbf{K}) (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}
$$
Then, applying Lemma \ref{lemma:action-inverse-error} we obtain
\begin{align*}
  T_2 &\leq \frac{1}{2} \| (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \|_2 \| \tilde{\mathbf{K}} - \mathbf{K} \|_2 \| (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \|_2 \\
    &\leq \frac{\gamma_{n,m,L}}{2\sigma^4}
\end{align*}
where $C_2(\theta)$ is another constant depending on $\theta$ (from the derivatives of the kernel when applying Lemma \ref{lemma:action-inverse-error}).

**Combining the Bounds:**

Combining the bounds for $T_1$ and $T_2$, we have

$$
\| \nabla \mathcal{L}(\theta) - \nabla \tilde{\mathcal{L}}(\theta) \|_2 \leq \frac{1}{\sigma^4}\left[\Vert y\Vert_1^2\left(\max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i}+2 C_1(\theta)\gamma_{n,m,L}\right)+\frac{1}{2}\gamma_{n,m,L}\right].
$$

\end{proof}
\begin{proposition}[One-Step Improvement on True Log-Likelihood]\label{prop:one-step-improvement-on-true-log-likelihood}
Let $\theta^*\in \mathcal{D}$ be the true MLE maximizing $\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})$. Assume $\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})$ is $\mu$-strongly concave and has $L$-Lipschitz continuous gradient in $\mathcal{D}$.

If the step size $\eta$ satisfies $\eta \leq \frac{1}{L}$ (where $L$ is the Lipschitz constant of the gradient of the true log-likelihood) and $\eta\mu < 1$,
then starting at $\theta_0\in \mathcal{D}$, after one step of projected gradient ascent on $\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})$, yielding $\theta_1$, we have:

$$
\| \theta_1 - \theta^* \|_2 \leq (1 - \mu \eta) \| \theta_0 - \theta^* \|_2
$$

\end{proposition}
\begin{proof}
By the optimality of $\theta^*$, we have $\nabla \mathcal{L}(\theta^*) = 0$.
By the $\mu$-strong concavity of $\mathcal{L}$, we have for any $\theta$:

$$
\mathcal{L}(\theta) \leq \mathcal{L}(\theta^*) + \nabla \mathcal{L}(\theta^*)^\top (\theta - \theta^*) - \frac{\mu}{2} \| \theta - \theta^* \|_2^2 = \mathcal{L}(\theta^*) - \frac{\mu}{2} \| \theta - \theta^* \|_2^2
$$

Rearranging, we get:

\begin{equation} \label{eq:strong_concavity_alt}
\| \theta - \theta^* \|_2^2 \leq \frac{2}{\mu} (\mathcal{L}(\theta^*) - \mathcal{L}(\theta))
\end{equation}
Since $\theta_1$ is obtained by one step of projected gradient ascent from $\theta_0$, we have:
$$
\theta_1 = \text{Proj}_{\mathcal{D}} (\theta_0 + \eta \nabla \mathcal{L}(\theta_0))
$$

By the properties of projection onto a convex set, we have:

$$
\| \theta_1 - \theta^* \|_2^2 \leq \| \theta_0 + \eta \nabla \mathcal{L}(\theta_0) - \theta^* \|_2^2
$$

Expanding the right-hand side, we get:

\begin{align*}
\| \theta_1 - \theta^* \|_2^2 &\leq \| \theta_0 - \theta^* \|_2^2 + 2 \eta (\theta_0 - \theta^*)^\top \nabla \mathcal{L}(\theta_0) + \eta^2 \| \nabla \mathcal{L}(\theta_0) \|_2^2 \\
&\leq \| \theta_0 - \theta^* \|_2^2 + 2 \eta (\mathcal{L}(\theta_0) - \mathcal{L}(\theta^*) + \frac{\mu}{2} \| \theta_0 - \theta^* \|_2^2) + \eta^2 \| \nabla \mathcal{L}(\theta_0) \|_2^2 \\
&\text{(by strong concavity, with } \nabla \mathcal{L}(\theta^*) = 0 \text{)}\\
&\leq \| \theta_0 - \theta^* \|_2^2 - \eta \mu \| \theta_0 - \theta^* \|_2^2 + \eta^2 \| \nabla \mathcal{L}(\theta_0) \|_2^2 \quad \text{(by \eqref{eq:strong_concavity_alt})} \\
&= (1 - \eta \mu) \| \theta_0 - \theta^* \|_2^2 + \eta^2 \| \nabla \mathcal{L}(\theta_0) \|_2^2
\end{align*}

By the Lipschitz continuity of the gradient, we have:

$$
\| \nabla \mathcal{L}(\theta_0) \|_2 = \| \nabla \mathcal{L}(\theta_0) - \nabla \mathcal{L}(\theta^*) \|_2 \leq L \| \theta_0 - \theta^* \|_2
$$
Thus,
$$
\| \theta_1 - \theta^* \|_2^2 \leq (1 - \eta \mu) \| \theta_0 - \theta^* \|_2^2 + \eta^2 L^2 \| \theta_0 - \theta^* \|_2^2 = (1 - \eta \mu + \eta^2 L^2) \| \theta_0 - \theta^* \|_2^2
$$
Taking the square root of both sides:
$$
\| \theta_1 - \theta^* \|_2 \leq \sqrt{1 - \eta \mu + \eta^2 L^2} \| \theta_0 - \theta^* \|_2
$$
If we choose $\eta \leq \frac{1}{L}$, we have:
$$
\sqrt{1 - \eta \mu + \eta^2 L^2} \leq \sqrt{1-2\eta \mu+\eta^2\mu^2} = 1 - \eta \mu
$$
Thus,
$$
\| \theta_1 - \theta^* \|_2 \leq (1 - \eta \mu) \| \theta_0 - \theta^* \|_2
$$
as long as $\eta \leq \frac{1}{L}$.
\end{proof}

\begin{proposition}[MLE Closeness via One-Step Improvement]\label{prop:mle-closeness-one-step}
Let $\theta^*$ be the true MLE maximizing $\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})$. Assume $\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})$ is $\mu$-strongly concave and has $L$-Lipschitz continuous gradient near $\theta^*$. Assume that the gradient difference is bounded:

$$
\| \nabla \tilde{\mathcal{L}}(\theta) - \nabla \mathcal{L}(\theta) \|_2 \leq \epsilon_g:= \frac{1}{\sigma^4}\left[\Vert y\Vert_1^2\left(\frac{1}{\sqrt{p}}\max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i}+2 C\gamma_{n,m,L}\right)+\frac{1}{2}\gamma_{n,m,L}\right]
$$

for all $\theta\in \mathcal{D}$.

If the step size $\eta>0$ satisfies $\eta \leq \frac{1}{\tilde{L}}$ (where $L$ is the Lipschitz constant of the gradient of the log-likelihood) $\mu\eta\leq 1$:

$$
\| \theta_0 - \theta^* \|_2 \leq (1 - \eta \mu) \| \theta_0 - \theta^* \|_2+\eta \epsilon_g
$$

Then, after one step of projected gradient ascent on $\tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y})$, yielding $\theta_1$, we have:

$$
\| \theta_1 - \theta^* \|_2 \leq (1 - \mu \eta) \| \theta_0 - \theta^* \|_2 + \eta \epsilon_g
$$
\end{proposition}
\begin{proof}
  Let $\theta_1$ be the result of one step of projected gradient ascent on $\tilde{\mathcal{L}}$ from $\theta_0$ with step size $\eta$:
  $$
  \theta_1 = \text{Proj}_{\mathcal{D}} (\theta_0 + \eta \nabla \tilde{\mathcal{L}}(\theta_0))
  $$
  where $\mathcal{D}$ is a convex set containing $\theta^*$.

  By the triangle inequality, we have:
  \begin{align*}
  \| \theta_1 - \theta^* \|_2 &= \| \text{Proj}_{\mathcal{D}} (\theta_0 + \eta \nabla \tilde{\mathcal{L}}(\theta_0)) - \theta^* \|_2 \\
  &\leq \| \theta_0 + \eta \nabla \tilde{\mathcal{L}}(\theta_0) - \theta^* \|_2 \\
  &= \| \theta_0 + \eta \nabla \mathcal{L}(\theta_0) - \theta^* + \eta (\nabla \tilde{\mathcal{L}}(\theta_0) - \nabla \mathcal{L}(\theta_0)) \|_2 \\
  &\leq \| \theta_0 + \eta \nabla \mathcal{L}(\theta_0) - \theta^* \|_2 + \eta \| \nabla \tilde{\mathcal{L}}(\theta_0) - \nabla \mathcal{L}(\theta_0) \|_2 \\
  &\leq \| \theta_0 + \eta \nabla \mathcal{L}(\theta_0) - \theta^* \|_2 + \eta \epsilon_g \quad \text{(by the gradient difference bound)}
  \end{align*}

  Now, let's consider the term $\| \theta_0 + \eta \nabla \mathcal{L}(\theta_0) - \theta^* \|_2$. This represents the distance to $\theta^*$ after taking one step of gradient ascent on the true log-likelihood $\mathcal{L}$. By Proposition \ref{prop:one-step-improvement-on-true-log-likelihood}, if $\| \theta_0 - \theta^* \|_2 \leq \frac{\eta(2\mu - \eta L)}{2}$ and $\eta \leq \frac{1}{L}$, we have:
  $$
  \| \theta_0 + \eta \nabla \mathcal{L}(\theta_0) - \theta^* \|_2 \leq (1 - \mu \eta) \| \theta_0 - \theta^* \|_2
  $$

  Therefore,
\begin{align*}
  \| \theta_1 - \theta^* \|_2 &\leq (1-\eta\mu)\|\theta_0-\theta^*\|_2+\eta\epsilon_g
\end{align*}
\end{proof}

\begin{theorem}[Closeness of SKI MLE to True MLE]\label{thm:mle-closeness-gradient-ascent}
Let $\theta^*$ be the true MLE maximizing $\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})$. Assume $\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})$ is $\mu$-strongly concave and has $L$-Lipschitz continuous gradient in $\mathcal{D}$. Assume that the gradient difference between the true log-likelihood and the SKI log-likelihood is bounded by
$$
\| \nabla \tilde{\mathcal{L}}(\theta) - \nabla \mathcal{L}(\theta) \|_2 \leq \epsilon_g := \frac{1}{\sigma^4}\left[\Vert y\Vert_1^2\left(\frac{1}{\sqrt{p}}\max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i}+2 C\gamma_{n,m,L}\right)+\frac{1}{2}\gamma_{n,m,L}\right]
$$
for all $\theta$ in a neighborhood of $\theta^*$, where $\gamma_{n,m,L}$, $\gamma'_{n,m,L,i}$ are defined as in Proposition \ref{prop:spectral-norm} and Proposition \ref{prop:gradient_spectral_norm_bound}, respectively, and $C$ is a bound on $C_1(\theta)$ in this neighborhood. Let $\tilde{\theta}$ be the SKI MLE obtained by running projected gradient ascent on $\tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y})$ from initialization $\theta_0$ with a fixed step size $\eta$.

If the step size $\eta$ satisfies $0 < \eta \leq \frac{1}{L}$ and $\eta \mu < 1$, then, after $t$ steps of projected gradient ascent, the iterate $\theta_t$ satisfies
$$
\| \theta_t - \theta^* \|_2 \leq (1 - \eta\mu)^t \| \theta_0 - \theta^* \|_2 + \frac{\epsilon_g}{\eta\mu}.
$$
\end{theorem}
\begin{proof}
    Consider that starting from $\theta_t$ and applying one step of gradient ascent, by Proposition \ref{prop:mle-closeness-one-step}
    \begin{align*}
        \Vert \theta_{t+1}-\theta^*\Vert_2 &\leq (1-\eta\mu)\Vert \theta_t-\theta^*\Vert_2+\eta \epsilon_g
    \end{align*}
    and iterating we have
    \begin{align*}
        \Vert \theta_t-\theta^*\Vert_2&\leq (1-\eta\mu)^t\Vert \theta_0-\theta^*\Vert_2+\sum_{s=0}^{t-1} (1-\eta\mu)^s \eta \epsilon_g\\
        &\leq (1-\eta\mu)^t\Vert \theta_0-\theta^*\Vert_2+\frac{\eta\epsilon_G}{\eta\mu}\text{ geometric series}
    \end{align*}
\end{proof}
\amnote{Now we would like to show that for some kernel (RBF probably), there exists a region around true parameter such that the log-likelihood is w.h.p. both strongly concave and has L-Lipschitz continuous gradients.}

\begin{theorem}[Local Strong Concavity of Population Log-Likelihood - RBF Kernel, Restricted Parameter Space]\label{thm:local-strong-concavity-restricted}
Let $P^*$ be the true data generating distribution, and let $\theta^* = (\sigma_{f*}, \ell_*)$ be the maximizer of the population log-likelihood $\mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$ for a Gaussian process with an RBF kernel:
$$
k_\theta(x, x') = \sigma_f^2 \exp\left(-\frac{\|x - x'\|_2^2}{2\ell^2}\right),
$$
where $\theta = (\sigma_f, \ell)$ are the signal variance and lengthscale parameters, respectively.

Assume the following:

1.  **Restricted Parameter Space:** We restrict the parameter space to a compact set $\Theta$ such that for all $\theta \in \Theta$, we have $\sigma_f \in [\sigma_{f, min}, \sigma_{f, max}]$ and $\ell \in [\ell_{min}, \ell_{max}]$, where $0 < \sigma_{f, min} < \sigma_{f, max} < \infty$ and $0 < \ell_{min} < \ell_{max} < \infty$. Furthermore, $\theta^* \in \Theta$.
2.  **Smoothness and Boundedness:**
    *   The true data generating process is of the form $y_i = \xi(x_i) + \epsilon_i$, where $\xi \sim GP(0, k_{\theta^*})$ and $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ are i.i.d. noise.
    *   The marginal distribution of the inputs $P^*_X$ has a density $p(x)$ with respect to the Lebesgue measure on $\mathbb{R}^d$ that is bounded away from zero on a compact set $\mathcal{X} \subset \mathbb{R}^d$, i.e., there exists $\rho > 0$ such that $p(x) \geq \rho$ for all $x \in \mathcal{X}$.
    *   The function $\xi$ and its first and second partial derivatives are bounded on $\mathcal{X}$.
3.  **Regularity of the RBF Kernel:** The RBF kernel $k_\theta(x,x')$ and its first, second, and third partial derivatives with respect to $\theta$ are continuous and bounded on $\mathcal{X} \times \mathcal{X}$ for $\theta \in \Theta$.

Then, there exists a neighborhood $\mathcal{D} \subseteq \Theta$ of $\theta^*$ and constants $\mu > 0$ and $L < \infty$ such that the following hold:

*   **(a) Local Strong Concavity:** The population log-likelihood $\mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$ is strongly concave with parameter $\mu$ within $\mathcal{D}$, i.e., for all $\theta_1, \theta_2 \in \mathcal{D}$:

    $$
    \mathbb{E}_{P^*}[\mathcal{L}(\theta_2; \mathbf{X}, \mathbf{y})] \leq \mathbb{E}_{P^*}[\mathcal{L}(\theta_1; \mathbf{X}, \mathbf{y})] + \nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta_1; \mathbf{X}, \mathbf{y})]^\top (\theta_2 - \theta_1) - \frac{\mu}{2} \| \theta_2 - \theta_1 \|_2^2.
    $$

*   **(b) Lipschitz Gradient:** The gradient of the population log-likelihood $\nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$ is Lipschitz continuous with constant $L$ within $\mathcal{D}$, i.e., for all $\theta_1, \theta_2 \in \mathcal{D}$:

    $$
    \| \nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta_1; \mathbf{X}, \mathbf{Y})] - \nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta_2; \mathbf{X}, \mathbf{Y})] \|_2 \leq L \| \theta_1 - \theta_2 \|_2.
    $$
\end{theorem}

\begin{proof}
The proof follows a similar structure to the previous attempts, but crucially, we now work within a restricted parameter space.

**1. Define the Population Log-Likelihood:**
As before, let $\bar{\mathcal{L}}(\theta) = \mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{Y})]$ denote the population log-likelihood.

**2. Consider the Hessian:**
Let $\mathbf{H}(\theta)$ be the Hessian of $\bar{\mathcal{L}}(\theta)$. We will show that $\mathbf{H}(\theta^*)$ is negative definite.

**3. Expected Fisher Information:**
The expected Fisher information matrix is $\mathbf{I}(\theta^*) = -\mathbf{H}(\theta^*)$. We will show that $\mathbf{I}(\theta^*)$ is positive definite.

**4. Positive Definiteness of $I(\theta^*)$:**
We want to show that for any non-zero vector $\mathbf{v} \in \mathbb{R}^2$, $\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} > 0$. Following the same steps as in the previous attempt, we arrive at:

$$
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} = \frac{1}{2} \mathbb{E}_{P^*_X}\left[ \text{tr}\left( (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \mathbf{A} (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \mathbf{A} \right) \right],
$$

where $\mathbf{A} = v_1 \frac{\partial \mathbf{K}_{\theta^*}}{\partial \sigma_f^2} + v_2 \frac{\partial \mathbf{K}_{\theta^*}}{\partial \ell}$.

As before, we can rewrite this as:

$$
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} = \frac{1}{2} \mathbb{E}_{P^*_X}\left[ \text{tr}\left( (\mathbf{B} \mathbf{A} \mathbf{B})^\top (\mathbf{B} \mathbf{A} \mathbf{B}) \right) \right],
$$

where $\mathbf{B} = (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1/2}$.

**5. Key Argument - Restricted Parameter Space and Linear Independence:**
Now, we leverage the restricted parameter space $\Theta$. Since $\sigma_f$ is bounded away from 0 and $\ell$ is bounded away from both 0 and $\infty$, the eigenvalues of $\mathbf{K}_{\theta^*}$ are bounded away from 0 and $\infty$. This implies that $\mathbf{B}$ and $\mathbf{B}^{-1}$ are well-defined and have bounded norms.

Suppose for contradiction that $\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} = 0$ for some non-zero $\mathbf{v}$. This implies that $\mathbf{B}\mathbf{A}\mathbf{B} = \mathbf{0}$ almost surely, and consequently, $\mathbf{A} = \mathbf{0}$ almost surely.

Thus, we have:

$$
v_1 \frac{\partial \mathbf{K}_{\theta^*}}{\partial \sigma_f^2} + v_2 \frac{\partial \mathbf{K}_{\theta^*}}{\partial \ell} = \mathbf{0} \quad \text{a.s.}
$$

Now, we need to show that this is impossible for non-zero $\mathbf{v}$ under our assumptions. We can write this as:
$$
v_1 \frac{\partial k_{\theta^*}}{\partial \sigma_f^2}(x_i,x_j) + v_2 \frac{\partial k_{\theta^*}}{\partial \ell}(x_i,x_j) = 0
$$
for all $x_i,x_j$ in the support of $P_X^*$.

Using the derivatives of the RBF kernel that we computed earlier, this becomes:
$$
v_1 \exp\left(-\frac{\|x_i - x_j\|_2^2}{2\ell_*^2}\right) + v_2 \sigma_{f*}^2 \exp\left(-\frac{\|x_i - x_j\|_2^2}{2\ell_*^2}\right) \left(\frac{\|x_i - x_j\|_2^2}{\ell_*^3}\right) = 0
$$
for all $x_i, x_j$ in the support of $P_X^*$.

Dividing by $\exp\left(-\frac{\|x_i - x_j\|_2^2}{2\ell_*^2}\right)$ (which is always non-zero), we get:
$$
v_1 + v_2 \sigma_{f*}^2 \frac{\|x_i - x_j\|_2^2}{\ell_*^3} = 0
$$

Since $\sigma_{f*} > 0$ and $\ell_* > 0$, the only way for this to hold for all $x_i, x_j$ in a set of non-zero measure under $P^*_X$ is if $v_1 = v_2 = 0$.
This is because if $v_2 \neq 0$, then the term $\frac{\|x_i - x_j\|_2^2}{\ell_*^3}$ can be varied continuously by changing $x_i$ and $x_j$, which makes it impossible for the equation to hold unless $v_1$ is also zero.

Thus, we have reached a contradiction. Therefore, for any non-zero $\mathbf{v}$, we must have $\mathbf{A} \neq \mathbf{0}$ almost surely, and hence $\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} > 0$.

**6. Negative Definite Hessian and Strong Concavity:**

Since $\mathbf{I}(\theta^*)$ is positive definite, $\mathbf{H}(\theta^*)$ is negative definite. By continuity of the Hessian (due to the regularity assumption on the kernel), there exists a neighborhood $\mathcal{D} \subseteq \Theta$ of $\theta^*$ such that $\mathbf{H}(\theta)$ is negative definite for all $\theta \in \mathcal{D}$. Moreover, there exists a constant $\mu > 0$ such that $-\mathbf{H}(\theta) \succeq \mu \mathbf{I}$ for all $\theta \in \mathcal{D}$. This implies that the population log-likelihood is strongly concave with parameter $\mu$ within $\mathcal{D}$.

**7. Lipschitz Continuity of the Gradient:**

The Lipschitz continuity of the gradient follows from the boundedness of the third-order derivatives of the population log-likelihood in the neighborhood $\mathcal{D}$. This can be shown using the regularity assumptions on the kernel (Assumption 3) and the boundedness assumption (Assumption 2). Since the third-order derivatives are bounded, there exists a constant $L < \infty$ such that:

$$
\| \nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta_1; \mathbf{X}, \mathbf{Y})] - \nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta_2; \mathbf{X}, \mathbf{Y})] \|_2 \leq L \| \theta_1 - \theta_2 \|_2
$$

for all $\theta_1, \theta_2 \in \mathcal{D}$.

**Conclusion:**

We have shown that under the given assumptions, there exists a neighborhood $\mathcal{D}$ of $\theta^*$ and constants $\mu > 0$ and $L < \infty$ such that the population log-likelihood is strongly concave with parameter $\mu$ and has a Lipschitz continuous gradient with constant $L$ within $\mathcal{D}$. This completes the proof.
\end{proof}
\begin{theorem}[Local Strong Concavity of Population Log-Likelihood - General Case]\label{thm:local-strong-concavity-general}
Let $P^*$ be the true data generating distribution, and let $\theta^*$ be the maximizer of the population log-likelihood $\mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$ for a Gaussian process with kernel $k_\theta(x, x')$.

Assume the following:

1.  **Identifiability:** The hyperparameters $\theta^*$ are identifiable from the data generating distribution $P^*$.
2.  **Smoothness and Boundedness:**
    *   The true data generating process is of the form $y_i = \xi(x_i) + \epsilon_i$, where $\xi \sim GP(0, k_{\theta^*})$ and $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ are i.i.d. noise.
    *   The marginal distribution of the inputs $P^*_X$ has a density $p(x)$ with respect to the Lebesgue measure on $\mathbb{R}^d$ that is bounded away from zero on a compact set $\mathcal{X} \subset \mathbb{R}^d$.
    *   The function $\xi$ and its first and second partial derivatives are bounded on $\mathcal{X}$.
3.  **Regularity of the Kernel:** The kernel $k_\theta(x,x')$ and its first, second, and third partial derivatives with respect to $\theta$ are continuous and bounded on $\mathcal{X} \times \mathcal{X}$ for $\theta$ in a neighborhood of $\theta^*$.
4. **Linear Independence of Derivatives:** For any non-zero vector $\mathbf{v}$ of the same dimension as $\theta$, the following holds almost surely with respect to $P^*_X$:
    $$
    \sum_{i} v_i \frac{\partial \mathbf{K}_\theta}{\partial \theta_i} \neq \mathbf{0}
    $$
    where $\mathbf{K}_\theta$ is the kernel matrix evaluated at a set of input points drawn from $P^*_X$.

Then, there exists a neighborhood $\mathcal{D}$ of $\theta^*$ and constants $\mu > 0$ and $L < \infty$ such that the following hold:

*   **(a) Local Strong Concavity:** The population log-likelihood $\mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$ is strongly concave with parameter $\mu$ within $\mathcal{D}$.
*   **(b) Lipschitz Gradient:** The gradient of the population log-likelihood $\nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$ is Lipschitz continuous with constant $L$ within $\mathcal{D}$.
\end{theorem}
\begin{proof}
Let $\bar{\mathcal{L}}(\theta) = \mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$ denote the population log-likelihood. The Hessian matrix $\mathbf{H}(\theta)$ has entries given by:

$$
\mathbf{H}_{ij}(\theta) = \frac{\partial^2 \bar{\mathcal{L}}(\theta)}{\partial \theta_i \partial \theta_j}.
$$

We want to show that $\mathbf{H}(\theta^*)$ is negative definite. It is equivalent to showing that the negative Hessian, $-\mathbf{H}(\theta^*)$, which is also known as the *expected Fisher information matrix*, is positive definite.

The log-likelihood for a set of observations $\{(\mathbf{x}_i, y_i)\}_{i=1}^n$ (assuming a zero mean function) is:

$$
\mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) = -\frac{1}{2} \mathbf{y}^\top (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log|\mathbf{K}_\theta + \sigma^2 \mathbf{I}| - \frac{n}{2} \log(2\pi),
$$

where $\mathbf{K}_\theta$ is the kernel matrix evaluated at the points in $\mathbf{X}$, and $\sigma^2$ is the noise variance.

\textbf{1. First-Order Derivatives:**}

The gradient of the log-likelihood has components:

$$
\frac{\partial \mathcal{L}}{\partial \theta_i} = \frac{1}{2} \mathbf{y}^\top (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \text{tr}\left((\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right).
$$

\textbf{2. Second-Order Derivatives:**}

The second-order derivatives can be computed using matrix calculus identities. After some calculations (details in Appendix A), we arrive at the following expression:

\begin{align*}
\frac{\partial^2 \mathcal{L}}{\partial \theta_i \partial \theta_j} &= -\frac{1}{2} \mathbf{y}^\top (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left[ \frac{\partial^2 \mathbf{K}_\theta}{\partial \theta_i \partial \theta_j} - \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right)(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right) - \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right)(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) \right] (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \mathbf{y} \\
&\quad + \frac{1}{2} \text{tr}\left[ (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right)(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right) \right] - \frac{1}{2} \text{tr}\left[ (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial^2 \mathbf{K}_\theta}{\partial \theta_i \partial \theta_j}\right) \right].
\end{align*}

\textbf{3. Population Log-Likelihood and Hessian:**}

The population log-likelihood is $\bar{\mathcal{L}}(\theta) = \mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$. The Hessian of the population log-likelihood is:

$$
\mathbf{H}(\theta) = \mathbb{E}_{P^*}[\nabla^2_\theta \mathcal{L}(\theta; \mathbf{X}, \mathbf{y})],
$$

where $\nabla^2_\theta$ denotes the Hessian with respect to $\theta$.

\textbf{4. Expected Fisher Information:**}

The expected Fisher information matrix is $\mathbf{I}(\theta^*) = -\mathbf{H}(\theta^*)$. We want to show that $\mathbf{I}(\theta^*)$ is positive definite.

\textbf{5. Positive Definiteness:**}

To show that $\mathbf{I}(\theta^*)$ is positive definite, we need to show that for any non-zero vector $\mathbf{v} \in \mathbb{R}^2$:

$$
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} > 0.
$$

Substituting $\theta = \theta^*$ and taking the expectation with respect to $P^*$, we get:

$$
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} = -\mathbb{E}_{P^*}[\mathbf{v}^\top \nabla^2_\theta \mathcal{L}(\theta^*; \mathbf{X}, \mathbf{y}) \mathbf{v}].
$$

Since $\mathbf{y}|\mathbf{X} \sim \mathcal{N}(\mathbf{0}, \mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})$, we have $\mathbb{E}_{P^*}[\mathbf{y}\mathbf{y}^\top | \mathbf{X}] = \mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I}$. Using this, we can simplify the expectation of the quadratic form involving $\mathbf{y}$ (details in Appendix B). After algebraic manipulation, we obtain:

$$
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} = \frac{1}{2} \mathbb{E}_{P^*_X}\left[ \text{tr}\left( (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \mathbf{A} (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \mathbf{A} \right) \right],
$$

where $\mathbf{A} = \sum_{i=1}^2 v_i \frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_i}$.

\textbf{6. Using Properties of Trace and Positive Semidefiniteness:**}

Now, note that $(\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1}$ is positive definite and thus has a positive definite square root. Let $\mathbf{B} = (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1/2}$. Then, we can rewrite the expression as:

$$
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} = \frac{1}{2} \mathbb{E}_{P^*_X}\left[ \text{tr}\left( \mathbf{B} \mathbf{A} \mathbf{B} \mathbf{B} \mathbf{A} \mathbf{B} \right) \right] = \frac{1}{2} \mathbb{E}_{P^*_X}\left[ \text{tr}\left( (\mathbf{B} \mathbf{A} \mathbf{B}) (\mathbf{B} \mathbf{A} \mathbf{B}) \right) \right] = \frac{1}{2} \mathbb{E}_{P^*_X}\left[ \text{tr}\left( (\mathbf{B} \mathbf{A} \mathbf{B})^\top (\mathbf{B} \mathbf{A} \mathbf{B}) \right) \right].
$$

Since $(\mathbf{B}\mathbf{A}\mathbf{B})^\top (\mathbf{B}\mathbf{A}\mathbf{B})$ is positive semidefinite, its trace is non-negative. Moreover, the trace is zero if and only if $\mathbf{B}\mathbf{A}\mathbf{B} = \mathbf{0}$.

\textbf{7. Using Identifiability and Smoothness:**}

Here, we use the identifiability assumption. If $\mathbf{B}\mathbf{A}\mathbf{B} = \mathbf{0}$, then $(\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1/2} \mathbf{A} (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1/2} = \mathbf{0}$, which implies $\mathbf{A} = \mathbf{0}$ (since the inverse square root is invertible).

$\mathbf{A} = \mathbf{0}$ implies:

$$
\sum_{i=1}^p v_i \frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_i}=\mathbf{0}
$$


This means that a linear combination of the derivatives of the kernel matrix with respect to the hyperparameters is zero. However, under the identifiability assumption and the smoothness of the data distribution, this can only happen if $v_1 = v_2 = 0$ (i.e., $\mathbf{v}$ is the zero vector). If different hyperparameters always lead to different predictive distributions, then their derivatives must be linearly independent.

Therefore, for any non-zero $\mathbf{v}$, we have $\mathbf{B}\mathbf{A}\mathbf{B} \neq \mathbf{0}$, and thus:

$$
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} = \frac{1}{2} \mathbb{E}_{P^*_X}\left[ \text{tr}\left( (\mathbf{B} \mathbf{A} \mathbf{B})^\top (\mathbf{B} \mathbf{A} \mathbf{B}) \right) \right] > 0.
$$

This shows that $\mathbf{I}(\theta^*)$ is positive definite, and consequently, $\mathbf{H}(\theta^*)$ is negative definite.

\textbf{Step 8: Establish Lipschitz continuity of the gradient.}

The Lipschitz continuity of the gradient follows from the boundedness of the third-order derivatives of the population log-likelihood in the neighborhood $\mathcal{D}$. This can be shown using the regularity assumptions on the kernel (Assumption 3) and the boundedness assumption (Assumption 2). Since the third-order derivatives are bounded, there exists a constant $L < \infty$ such that
\[
\| \nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta_1; \mathbf{X}, \mathbf{y})] - \nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta_2; \mathbf{X}, \mathbf{y})] \|_2 \leq L \| \theta_1 - \theta_2 \|_2
\]

for all $\theta_1, \theta_2 \in \mathcal{D}$.

\end{proof}

\subsection{Error of GP Predictive Posterior Parameters}

\subsubsection{Mean}
\amnote{Wrong now that it's a set of test points instead of a single test point.}
\begin{lemma}
    Let $\boldsymbol{\mu}(\cdot)$ be the GP posterior mean at a set of test points $\cdot$ and $\tilde{\boldsymbol{\mu}}(\cdot)$ be the SKI posterior mean at those points. Then the SKI posterior mean $l^2$ error is bounded by:
\begin{align*}
    \Vert \tilde{\boldsymbol{\mu}}(\cdot)- \boldsymbol{\mu}(\cdot)\Vert_2&\leq \frac{1}{\sigma^2}\left(\min(\gamma_{T,m,L},\gamma_{n,m,L})+\frac{M}{\sigma^2}\gamma_{n,m,L}\right)
\end{align*}

\end{lemma}

\begin{proof}
    We start by expressing the difference between the true and SKI posterior means:


\begin{align*}
& \left\|\mathbf{K}_{\cdot,\mathbf{X}} \left( \mathbf{K} + \sigma^{2} \mathbf{I} \right)^{-1} \mathbf{y} - \tilde{\mathbf{K}}_{\cdot, \mathbf{X}} \left( \tilde{\mathbf{K}} + \sigma^{2} \mathbf{I} \right)^{-1} \mathbf{y} \right\|_{2} \\
&= \left\| \left(\mathbf{K}_{\cdot,\mathbf{X}} - \tilde{\mathbf{K}}_{\cdot, \mathbf{X}} \right) \left( \tilde{\mathbf{K}} + \sigma^{2} \mathbf{I} \right)^{-1} \mathbf{y} + \tilde{\mathbf{K}}_{\cdot, \mathbf{X}} \left[ \left( \mathbf{K} + \sigma^{2} \mathbf{I} \right)^{-1} - \left( \tilde{\mathbf{K}} + \sigma^{2} \mathbf{I} \right)^{-1} \right] \mathbf{y} \right\|_{2}
\end{align*}


Applying the triangle inequality gives:

\begin{align*}
& \leq \left\| \left(\mathbf{K}_{\cdot,\mathbf{X}} - \tilde{\mathbf{K}}_{\cdot, \mathbf{X}} \right) \left( \tilde{\mathbf{K}} + \sigma^{2} \mathbf{I} \right)^{-1} \right\|_{2} \| \mathbf{y} \|_{2} + \left\| \tilde{\mathbf{K}}_{\cdot, \mathbf{X}} \left( \left( \mathbf{K} + \sigma^{2} \mathbf{I} \right)^{-1} - \left( \tilde{\mathbf{K}} + \sigma^{2} \mathbf{I} \right)^{-1} \right) \mathbf{y} \right\|_{2} \\
&\leq \|\mathbf{K}_{\cdot,\mathbf{X}} - \tilde{\mathbf{K}}_{\cdot, \mathbf{X}} \|_{2} \left\| \left( \tilde{\mathbf{K}} + \sigma^{2} \mathbf{I} \right)^{-1} \right\|_{2}  \| \mathbf{y} \|_{2} + \| \tilde{\mathbf{K}}_{\cdot, \mathbf{X}} \|_{2}  \left\| \left( \mathbf{K} + \sigma^{2} \mathbf{I} \right)^{-1} - \left( \tilde{\mathbf{K}} + \sigma^{2} \mathbf{I} \right)^{-1} \right\|_{2} \cdot \| \mathbf{y} \|_{2} \\
& \leq \frac{ \|\mathbf{K}_{\cdot,\mathbf{X}} - \tilde{\mathbf{K}}_{\cdot, \mathbf{X}} \|_{2} }{ \sigma^{2} } \| \mathbf{y} \|_{2} + \gamma_{n, m, L}\frac{ \| \tilde{\mathbf{K}}_{\cdot, \mathbf{X}} \|_{2} }{ \sigma^{4} }  \| \mathbf{y} \|_{2} \textrm{ by Lemma \ref{lemma:action-inverse-error}}\\
& \leq \left( \frac{ \|\mathbf{K}_{\cdot,\mathbf{X}} - \tilde{\mathbf{K}}_{\cdot, \mathbf{X}} \|_{2} }{ \sigma^{2} } + \frac{ \| \tilde{\mathbf{K}}_{\cdot, \mathbf{X}} \|_{2} }{ \sigma^{4} } \gamma_{n, m, L} \right) \| \mathbf{y} \|_{2}\\
&\leq \left(\frac{\max(\gamma_{T,m,L},\gamma_{n,m,L})}{\sigma^2}+\frac{\Vert \tilde{\mathbf{K}}_{\cdot, \mathbf{X}} \Vert_2}{\sigma^4}\gamma_{n,m,L}\right)\Vert \textbf{y}\Vert_2
\end{align*}

\end{proof}
\begin{lemma}[Bound on the Error of SKI Posterior Covariance]
Let $\boldsymbol{\Sigma}(\cdot)$ be the GP posterior covariance matrix at a set of test points $\cdot\in \mathbb{R}^{T\times d}$ and $\tilde{\boldsymbol{\Sigma}}(\cdot)$ be its SKI approximation. Assume that the true kernel satisfies the condition that $|k(\mathbf{x}, \mathbf{x}')| \leq M$ for all $\mathbf{x}, \mathbf{x}'$ in the domain of interest, so that $\Vert \textbf{K}_{\textbf{X},\cdot} \Vert_2 \leq \sqrt{Tn}M$. Then
\begin{align*}
    \Vert \boldsymbol{\Sigma}(\cdot)-\tilde{\boldsymbol{\Sigma}}(\cdot)\Vert_2 &\leq \gamma_{T,m,L} + \frac{\sqrt{Tn}M}{\sigma^2} \max(\gamma_{T,m,L},\gamma_{n,m,L})\\
    &+ \frac{\gamma_{n,m,L}}{\sigma^4}Tn c^{2d} M^2 + \frac{\sqrt{Tn} m c^{2d} M}{\sigma^2} \max(\gamma_{T,m,L},\gamma_{n,m,L}),
\end{align*}
where $\gamma_{T,m,L}$ and $\gamma_{n,m,L}$ are defined as in Proposition \ref{prop:spectral-norm}, $c$ is an upper bound on the absolute sum of the weights for 1D kernel interpolation, $m$ is the number of inducing points, and $\sigma^2$ is the observation noise variance.
\end{lemma}

\begin{proof}
First, note that
\begin{align*}
    \Vert \boldsymbol{\Sigma}(\cdot)-\tilde{\boldsymbol{\Sigma}}(\cdot)\Vert_2 &\leq \Vert \textbf{K}_{\cdot,\cdot}-\tilde{\textbf{K}}_{\cdot,\cdot}\Vert_2\\
    &+ \Vert \textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1}\textbf{K}_{\textbf{X},\cdot}-\tilde{\textbf{K}}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1}\tilde{\textbf{K}}_{\textbf{X},\cdot}\Vert_2 \\
    &\leq \gamma_{T,m,L} + \Vert \textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1}\textbf{K}_{\textbf{X},\cdot}-\tilde{\textbf{K}}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1}\tilde{\textbf{K}}_{\textbf{X},\cdot}\Vert_2,
\end{align*}
where we used Proposition \ref{prop:spectral-norm} and the fact that $\Vert \textbf{K}_{\cdot,\cdot}-\tilde{\textbf{K}}_{\cdot,\cdot}\Vert_2 \leq \gamma_{T,m,L}$.

Now, we bound the second term:
\begin{align*}
    &\Vert \textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1}\textbf{K}_{\textbf{X},\cdot}-\tilde{\textbf{K}}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1}\tilde{\textbf{K}}_{\textbf{X},\cdot}\Vert_2\\
    &\leq \Vert \textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1}\textbf{K}_{\textbf{X},\cdot}-\textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1}\tilde{\textbf{K}}_{\textbf{X},\cdot}\Vert_2 \\
    &+ \Vert\textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1}\tilde{\textbf{K}}_{\textbf{X},\cdot}-\tilde{\textbf{K}}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1}\tilde{\textbf{K}}_{\textbf{X},\cdot}\Vert_2\\
    &\leq \Vert \textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1}(\textbf{K}_{\textbf{X},\cdot}-\tilde{\textbf{K}}_{\textbf{X},\cdot})\Vert_2 + \Vert (\textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1}-\tilde{\textbf{K}}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1})\tilde{\textbf{K}}_{\textbf{X},\cdot}\Vert_2\\
    &\leq \Vert \textbf{K}_{\cdot,\textbf{X}} \Vert_2 \Vert (\textbf{K}+\sigma^2 I)^{-1} \Vert_2 \Vert \textbf{K}_{\textbf{X},\cdot}-\tilde{\textbf{K}}_{\textbf{X},\cdot} \Vert_2 + \Vert \textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1}-\tilde{\textbf{K}}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1} \Vert_2 \Vert \tilde{\textbf{K}}_{\textbf{X},\cdot} \Vert_2\\
    &\leq \frac{1}{\sigma^2} \Vert \textbf{K}_{\cdot,\textbf{X}}\Vert_2 \Vert \textbf{K}_{\textbf{X},\cdot}-\tilde{\textbf{K}}_{\textbf{X},\cdot}\Vert_2 + \Vert \textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1}-\tilde{\textbf{K}}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1} \Vert_2 \Vert \tilde{\textbf{K}}_{\textbf{X},\cdot} \Vert_2,
\end{align*}
where we used the fact that $(\textbf{K}+\sigma^2 I)^{-1} \preceq \frac{1}{\sigma^2}I$.

Next, we bound the term $\Vert \textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1}-\tilde{\textbf{K}}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1} \Vert_2$:
\begin{align*}
    &\Vert \textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1}-\tilde{\textbf{K}}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1} \Vert_2 \\
    &= \Vert \textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1} - \textbf{K}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1} + \textbf{K}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1} - \tilde{\textbf{K}}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1} \Vert_2 \\
    &\leq \Vert \textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1} - \textbf{K}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1}\Vert_2 + \Vert \textbf{K}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1} - \tilde{\textbf{K}}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1} \Vert_2\\
    &= \Vert \textbf{K}_{\cdot,\textbf{X}} [(\textbf{K}+\sigma^2 I)^{-1} - (\tilde{\textbf{K}}+\sigma^2 I)^{-1}] \Vert_2 + \Vert (\textbf{K}_{\cdot,\textbf{X}} - \tilde{\textbf{K}}_{\cdot,\textbf{X}}) (\tilde{\textbf{K}}+\sigma^2 I)^{-1} \Vert_2\\
    &\leq \Vert \textbf{K}_{\cdot,\textbf{X}} \Vert_2 \Vert (\textbf{K}+\sigma^2 I)^{-1} - (\tilde{\textbf{K}}+\sigma^2 I)^{-1} \Vert_2 + \Vert \textbf{K}_{\cdot,\textbf{X}} - \tilde{\textbf{K}}_{\cdot,\textbf{X}} \Vert_2 \Vert (\tilde{\textbf{K}}+\sigma^2 I)^{-1} \Vert_2\\
    &\leq \Vert \textbf{K}_{\cdot,\textbf{X}} \Vert_2 \frac{\gamma_{n,m,L}}{\sigma^4} + \Vert \textbf{K}_{\cdot,\textbf{X}} - \tilde{\textbf{K}}_{\cdot,\textbf{X}} \Vert_2 \frac{1}{\sigma^2},
\end{align*}
where we used Lemma \ref{lemma:action-inverse-error} in the last inequality.

Substituting this back into the main inequality, we get:
\begin{align*}
    &\Vert \textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1}\textbf{K}_{\textbf{X},\cdot}-\tilde{\textbf{K}}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1}\tilde{\textbf{K}}_{\textbf{X},\cdot}\Vert_2 \\
    &\leq \frac{1}{\sigma^2} \Vert \textbf{K}_{\cdot,\textbf{X}}\Vert_2 \Vert \textbf{K}_{\textbf{X},\cdot}-\tilde{\textbf{K}}_{\textbf{X},\cdot}\Vert_2 + \left(\Vert \textbf{K}_{\cdot,\textbf{X}} \Vert_2 \frac{\gamma_{n,m,L}}{\sigma^4} + \Vert \textbf{K}_{\cdot,\textbf{X}} - \tilde{\textbf{K}}_{\cdot,\textbf{X}} \Vert_2 \frac{1}{\sigma^2}\right) \Vert \tilde{\textbf{K}}_{\textbf{X},\cdot} \Vert_2\\
    &= \frac{1}{\sigma^2} \Vert \textbf{K}_{\cdot,\textbf{X}}\Vert_2 \Vert \textbf{K}_{\textbf{X},\cdot}-\tilde{\textbf{K}}_{\textbf{X},\cdot}\Vert_2 + \frac{\gamma_{n,m,L}}{\sigma^4}\Vert \textbf{K}_{\cdot,\textbf{X}} \Vert_2  \Vert \tilde{\textbf{K}}_{\textbf{X},\cdot} \Vert_2 + \frac{1}{\sigma^2} \Vert \textbf{K}_{\cdot,\textbf{X}} - \tilde{\textbf{K}}_{\cdot,\textbf{X}} \Vert_2 \Vert \tilde{\textbf{K}}_{\textbf{X},\cdot} \Vert_2.
\end{align*}

Using Lemma \ref{lemma:test-train-kernel-matrix-error} and the fact that $\Vert \textbf{K}_{\textbf{X},\cdot}-\tilde{\textbf{K}}_{\textbf{X},\cdot}\Vert_2 \leq \max(\gamma_{T,m,L},\gamma_{n,m,L})$ and that $\textbf{K}_{\cdot, \textbf{X}} = \textbf{K}_{\textbf{X},\cdot}^\top$, we have $\Vert \textbf{K}_{\cdot,\textbf{X}} \Vert_2 = \Vert \textbf{K}_{\textbf{X},\cdot} \Vert_2$. Also, by assumption, $\Vert \textbf{K}_{\textbf{X},\cdot} \Vert_2 \leq \sqrt{Tn}M$. Using Lemma \ref{lemma:ski-test-train-bound}, we have $\Vert \tilde{\textbf{K}}_{\textbf{X},\cdot} \Vert_2 \leq \sqrt{Tn}mc^{2d}M$. Substituting these bounds, we get:

\begin{align*}
    &\Vert \textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1}\textbf{K}_{\textbf{X},\cdot}-\tilde{\textbf{K}}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1}\tilde{\textbf{K}}_{\textbf{X},\cdot}\Vert_2 \\
    &\leq \frac{\sqrt{Tn}M}{\sigma^2} \max(\gamma_{T,m,L},\gamma_{n,m,L}) + \frac{\gamma_{n,m,L}}{\sigma^4}(\sqrt{Tn}M)(\sqrt{Tn} m c^{2d} M) + \frac{1}{\sigma^2} \max(\gamma_{T,m,L},\gamma_{n,m,L}) (\sqrt{Tn} m c^{2d} M) \\
    &= \frac{\sqrt{Tn}M}{\sigma^2} \max(\gamma_{T,m,L},\gamma_{n,m,L}) + \frac{\gamma_{n,m,L}}{\sigma^4}Tn m c^{2d} M^2 + \frac{\sqrt{Tn} m c^{2d} M}{\sigma^2} \max(\gamma_{T,m,L},\gamma_{n,m,L}).
\end{align*}

Finally, substituting this back into the original inequality, we obtain the desired bound:

\begin{align*}
    \Vert \boldsymbol{\Sigma}(\cdot)-\tilde{\boldsymbol{\Sigma}}(\cdot)\Vert_2 &\leq \gamma_{T,m,L} + \frac{\sqrt{Tn}M}{\sigma^2} \max(\gamma_{T,m,L},\gamma_{n,m,L})\\
    &+ \frac{\gamma_{n,m,L}}{\sigma^4}Tn m c^{2d} M^2 + \frac{\sqrt{Tn} m c^{2d} M}{\sigma^2} \max(\gamma_{T,m,L},\gamma_{n,m,L}).
\end{align*}
This completes the proof.
\end{proof}

\section{Discussion}\label{sec:Discussion}

\bibliography{main}
\bibliographystyle{icml2021}


\appendix


\section{Auxiliary Technical Results}

\begin{lemma}\label{lemma:switch-sum-product}
Given a function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ of the form $f(x_1, x_2, ..., x_d) = \prod_{j=1}^d f_j(x_j)$, where each $f_j: \mathbb{R} \rightarrow \mathbb{R}$. Let $G = G^{(1)} \times G^{(2)} \times ... \times G^{(d)}$ be a fixed d-dimensional grid, where each $G^{(j)} = \{p_1^{(j)}, p_2^{(j)}, ..., p_{n_j}^{(j)}\}$ is a finite set of $n_j$ grid points along the j-th dimension for $j = 1, 2, ..., d$. Then the following equality holds:

$$
\sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_d=1}^{n_d} \prod_{j=1}^d f_j(p_{k_j}^{(j)}) = \prod_{j=1}^d \left( \sum_{k_j=1}^{n_j} f_j(p_{k_j}^{(j)}) \right)
$$
\end{lemma}
\begin{proof}
\textbf{By Induction on d (the number of dimensions):}

\textbf{Base Case (d = 1):}

When $d=1$, the statement becomes:

$$
\sum_{k_1=1}^{n_1} f_1(p_{k_1}^{(1)}) = \sum_{k_1=1}^{n_1} f_1(p_{k_1}^{(1)})
$$

This is trivially true.

\textbf{Inductive Hypothesis:}

Assume the statement holds for $d = m$, i.e.,

$$
\sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_m=1}^{n_m} \prod_{j=1}^m f_j(p_{k_j}^{(j)}) = \prod_{j=1}^m \left( \sum_{k_j=1}^{n_j} f_j(p_{k_j}^{(j)}) \right)
$$

\textbf{Inductive Step:}

We need to show that the statement holds for $d = m+1$. Consider the left-hand side for $d = m+1$:

$$
\sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_{m+1}=1}^{n_{m+1}} \prod_{j=1}^{m+1} f_j(p_{k_j}^{(j)})
$$

We can rewrite this as:

$$
\sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_m=1}^{n_m} \left( \sum_{k_{m+1}=1}^{n_{m+1}} \left( \prod_{j=1}^m f_j(p_{k_j}^{(j)}) \right) f_{m+1}(p_{k_{m+1}}^{(m+1)}) \right)
$$

Notice that the inner sum (over $k_{m+1}$) does not depend on $k_1, k_2, ..., k_m$. Thus, for any fixed values of $k_1, k_2, ..., k_m$, we can treat $\prod_{j=1}^m f_j(p_{k_j}^{(j)})$ as a constant. Let $C(k_1, ..., k_m) = \prod_{j=1}^m f_j(p_{k_j}^{(j)})$. Then we have:

$$
\sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_m=1}^{n_m}  \left( C(k_1, ..., k_m) \sum_{k_{m+1}=1}^{n_{m+1}} f_{m+1}(p_{k_{m+1}}^{(m+1)}) \right)
$$

Now, the inner sum $\sum_{k_{m+1}=1}^{n_{m+1}} f_{m+1}(p_{k_{m+1}}^{(m+1)})$ is a constant with respect to $k_1, ..., k_m$. Let's call this constant $S_{m+1}$. So we have:

$$
\sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_m=1}^{n_m} C(k_1, ..., k_m) S_{m+1} = S_{m+1} \sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_m=1}^{n_m}  \prod_{j=1}^m f_j(p_{k_j}^{(j)})
$$

By the inductive hypothesis, we can replace the nested sums with a product:

$$
S_{m+1} \prod_{j=1}^m \left( \sum_{k_j=1}^{n_j} f_j(p_{k_j}^{(j)}) \right) = \left( \sum_{k_{m+1}=1}^{n_{m+1}} f_{m+1}(p_{k_{m+1}}^{(m+1)}) \right) \prod_{j=1}^m \left( \sum_{k_j=1}^{n_j} f_j(p_{k_j}^{(j)}) \right)
$$

Rearranging the terms, we get:

$$
\prod_{j=1}^m \left( \sum_{k_j=1}^{n_j} f_j(p_{k_j}^{(j)}) \right) \left( \sum_{k_{m+1}=1}^{n_{m+1}} f_{m+1}(p_{k_{m+1}}^{(m+1)}) \right) = \prod_{j=1}^{m+1} \left( \sum_{k_j=1}^{n_j} f_j(p_{k_j}^{(j)}) \right)
$$

This is the right-hand side of the statement for $d = m+1$. Thus, the statement holds for $d = m+1$.

\textbf{Conclusion:}

By induction, the statement holds for all $d \geq 1$. Therefore,

$$
\sum_{k_1=1}^{n_1} \sum_{k_2=1}^{n_2} ... \sum_{k_d=1}^{n_d} \prod_{j=1}^d f_j(p_{k_j}^{(j)}) = \prod_{j=1}^d \left( \sum_{k_j=1}^{n_j} f_j(p_{k_j}^{(j)}) \right)
$$

\end{proof}


\begin{claim}\label{claim:convex-combo-eigenvalues}
Given a convex combination \(\mathbf{C} = \alpha \mathbf{A} + (1-\alpha) \mathbf{B}\), where \(\alpha \in [0,1]\), and \(\mathbf{A}\) and \(\mathbf{B}\) are symmetric matrices, the eigenvalues of \(\mathbf{C}\) lie in the interval \(\left[\min \left(\lambda_n(\mathbf{A}), \lambda_n(\mathbf{B})\right), \max \left(\lambda_1(\mathbf{A}), \lambda_1(\mathbf{B})\right)\right]\).
    % Given a convex combination $C=\alpha A+(1-\alpha)B,\alpha \in [0,1]$, the eigenvalues of $C$ lie in the interval $[\min(\lambda(A)_n, \lambda(B)_n), \max(\lambda(A)_1,\lambda(B)_1)]$
\end{claim}
\begin{proof}


First, recall that for a symmetric matrix \(\mathbf{A}\), the Rayleigh quotient \(R(\mathbf{A}, \mathbf{x}) = \frac{\mathbf{x}^{\top} \mathbf{A} \mathbf{x}}{\mathbf{x}^{\top} \mathbf{x}}\) is bounded by the smallest and largest eigenvalues of \(\mathbf{A}\):
\[
\lambda_n(\mathbf{A}) \leq R(\mathbf{A}, \mathbf{x}) \leq \lambda_1(\mathbf{A})
\]

Consider the Rayleigh quotient for the matrix \(\mathbf{C}\):
\[
R(\mathbf{C}, \mathbf{x}) = \frac{\mathbf{x}^{\top} (\alpha \mathbf{A} + (1-\alpha) \mathbf{B}) \mathbf{x}}{\mathbf{x}^{\top} \mathbf{x}} = \alpha R(\mathbf{A}, \mathbf{x}) + (1-\alpha) R(\mathbf{B}, \mathbf{x})
\]

Since \(R(\mathbf{A}, \mathbf{x})\) and \(R(\mathbf{B}, \mathbf{x})\) are bounded by their respective eigenvalues, we have:
\[
R(\mathbf{C}, \mathbf{x}) \leq \alpha \lambda_1(\mathbf{A}) + (1-\alpha) \lambda_1(\mathbf{B})
\]
which implies:
\[ 
R(\mathbf{C}, x)  \leq   \max(\lambda_1(\textbf{A}), \lambda_1(\textbf{B}))
\]

Similarly,
\[ 
R(\textbf{C}, \textbf{x})  \geq  \min(\lambda_n(\textbf{A}), \lambda_n(\textbf{B}))
\]

Thus, the eigenvalues of \(\textbf{C} = \alpha \textbf{A} + (1-\alpha)\textbf{B}\) are bounded by:
\[ 
\min(\lambda_n(\textbf{A}), \lambda_n(\textbf{B})) \leq  \lambda(\textbf{C}) \leq \max(\lambda_1(\textbf{A}), \lambda_1(\textbf{B}))
\]
\end{proof}

\section{Proofs Related to Estimation}
\subsection{Second-Order Derivatives of Log-Likelihood (Details)}

Here, we provide the detailed calculations for the second-order derivatives of the log-likelihood with respect to $\theta_i$ and $\theta_j$.

We start with the first-order derivative:

$$
\frac{\partial \mathcal{L}}{\partial \theta_i} = \frac{1}{2} \mathbf{y}^\top (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \text{tr}\left((\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right).
$$

Now, we differentiate again with respect to $\theta_j$. We will use the following matrix calculus identities:

*   $\frac{\partial}{\partial \theta} (\mathbf{A}^{-1}) = -\mathbf{A}^{-1} (\frac{\partial \mathbf{A}}{\partial \theta}) \mathbf{A}^{-1}$
*   $\frac{\partial}{\partial \theta} \text{tr}(\mathbf{A}\mathbf{B}) = \text{tr}(\frac{\partial \mathbf{A}}{\partial \theta} \mathbf{B}) + \text{tr}(\mathbf{A} \frac{\partial \mathbf{B}}{\partial \theta})$
*   $\frac{\partial}{\partial \theta} \log|\mathbf{A}| = \text{tr}(\mathbf{A}^{-1} \frac{\partial \mathbf{A}}{\partial \theta})$

Applying these identities, we obtain:

\begin{align*}
\frac{\partial^2 \mathcal{L}}{\partial \theta_i \partial \theta_j} &= \frac{1}{2} \mathbf{y}^\top \frac{\partial}{\partial \theta_j} \left[ (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \right] \mathbf{y} \\
&\quad - \frac{1}{2} \frac{\partial}{\partial \theta_j} \left[ \text{tr}\left((\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) \right]
\end{align*}

Let's compute each term separately. For the first term:

\begin{align*}
&\frac{\partial}{\partial \theta_j} \left[ (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \right] \\
&= \frac{\partial}{\partial \theta_j} [(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}] \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} + (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \frac{\partial}{\partial \theta_j} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \\
&\quad + (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) \frac{\partial}{\partial \theta_j} [(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}] \\
&= -(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \\
&\quad + (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial^2 \mathbf{K}_\theta}{\partial \theta_i \partial \theta_j}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \\
&\quad - (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}
\end{align*}

For the second term:

\begin{align*}
&\frac{\partial}{\partial \theta_j} \left[ \text{tr}\left((\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) \right] \\
&= \text{tr}\left( \frac{\partial}{\partial \theta_j} [(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}] \frac{\partial \mathbf{K}_\theta}{\partial \theta_i} \right) + \text{tr}\left( (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \frac{\partial}{\partial \theta_j} \left[\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right] \right) \\
&= -\text{tr}\left( (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \frac{\partial \mathbf{K}_\theta}{\partial \theta_i} \right) + \text{tr}\left( (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \frac{\partial^2 \mathbf{K}_\theta}{\partial \theta_i \partial \theta_j} \right)
\end{align*}

Putting everything together, we get the expression for the second-order derivative:

\begin{align*}
\frac{\partial^2 \mathcal{L}}{\partial \theta_i \partial \theta_j} &= -\frac{1}{2} \mathbf{y}^\top (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left[ \frac{\partial^2 \mathbf{K}_\theta}{\partial \theta_i \partial \theta_j} - \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right)(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right) - \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right)(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) \right] (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \mathbf{y} \\
&\quad + \frac{1}{2} \text{tr}\left[ (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right)(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right) \right] - \frac{1}{2} \text{tr}\left[ (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial^2 \mathbf{K}_\theta}{\partial \theta_i \partial \theta_j}\right) \right].
\end{align*}

\subsection{Appendix B: Simplification of the Quadratic Form (Details)}

Here, we provide the detailed algebraic manipulations to arrive at the expression involving the trace. We start with the quadratic form:
Substituting the expression for the second-order derivative and evaluating at $\theta=\theta^*$, we get:

\begin{align*}
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} 
&= -\mathbb{E}_{P^*} \left[ \sum_{i=1}^2 \sum_{j=1}^2 v_i v_j \frac{\partial^2 \mathcal{L}}{\partial \theta_i \partial \theta_j}\bigg|_{\theta = \theta^*} \right].
\end{align*}

Now, we substitute the expression for the second-order derivatives and use the fact that $\mathbb{E}_{P^*}[\mathbf{y}\mathbf{y}^\top | \mathbf{X}] = \mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I}$:

\begin{align*}
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} &= \frac{1}{2} \mathbb{E}_{P^*_X} \left[ \sum_{i=1}^2 \sum_{j=1}^2 v_i v_j \left( \text{tr}\left[ (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_i}\right) (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_j}\right) \right] - \text{tr}\left[ (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \frac{\partial^2 \mathbf{K}_{\theta^*}}{\partial \theta_i \partial \theta_j} \right] \right. \right. \\
&\quad \left. \left. + \text{tr}\left[ (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \left\{ \frac{\partial^2 \mathbf{K}_{\theta^*}}{\partial \theta_i \partial \theta_j} - \left(\frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_i}\right)(\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_j}\right) - \left(\frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_j}\right)(\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_i}\right) \right\} \right] \right) \right].
\end{align*}

After simplifying and rearranging terms, we get:

\begin{align*}
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} &= \frac{1}{2} \mathbb{E}_{P^*_X} \left[ \text{tr}\left( (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \left( \sum_{i=1}^2 v_i \frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_i} \right) (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \left( \sum_{j=1}^2 v_j \frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_j} \right) \right) \right] \\
&= \frac{1}{2} \mathbb{E}_{P^*_X}\left[ \text{tr}\left( (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \mathbf{A} (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \mathbf{A} \right) \right],
\end{align*}

where $\mathbf{A} = \sum_{i=1}^2 v_i \frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_i}$. This is the expression used in the proof.

