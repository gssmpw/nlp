
\amnote{Now we would like to show that for some kernel (RBF probably), there exists a region around true parameter such that the log-likelihood is w.h.p. both strongly concave and has L-Lipschitz continuous gradients.}

\begin{theorem}[Local Strong Concavity of Population Log-Likelihood - RBF Kernel, Restricted Parameter Space]\label{thm:local-strong-concavity-restricted}
Let $P^*$ be the true data generating distribution, and let $\theta^* = (\sigma_{f*}, \ell_*)$ be the maximizer of the population log-likelihood $\mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$ for a Gaussian process with an RBF kernel:
$$
k_\theta(x, x') = \sigma_f^2 \exp\left(-\frac{\|x - x'\|_2^2}{2\ell^2}\right),
$$
where $\theta = (\sigma_f, \ell)$ are the signal variance and lengthscale parameters, respectively.

Assume the following:

1.  **Restricted Parameter Space:** We restrict the parameter space to a compact set $\Theta$ such that for all $\theta \in \Theta$, we have $\sigma_f \in [\sigma_{f, min}, \sigma_{f, max}]$ and $\ell \in [\ell_{min}, \ell_{max}]$, where $0 < \sigma_{f, min} < \sigma_{f, max} < \infty$ and $0 < \ell_{min} < \ell_{max} < \infty$. Furthermore, $\theta^* \in \Theta$.
2.  **Smoothness and Boundedness:**
    *   The true data generating process is of the form $y_i = \xi(x_i) + \epsilon_i$, where $\xi \sim GP(0, k_{\theta^*})$ and $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ are i.i.d. noise.
    *   The marginal distribution of the inputs $P^*_X$ has a density $p(x)$ with respect to the Lebesgue measure on $\mathbb{R}^d$ that is bounded away from zero on a compact set $\mathcal{X} \subset \mathbb{R}^d$, i.e., there exists $\rho > 0$ such that $p(x) \geq \rho$ for all $x \in \mathcal{X}$.
    *   The function $\xi$ and its first and second partial derivatives are bounded on $\mathcal{X}$.
3.  **Regularity of the RBF Kernel:** The RBF kernel $k_\theta(x,x')$ and its first, second, and third partial derivatives with respect to $\theta$ are continuous and bounded on $\mathcal{X} \times \mathcal{X}$ for $\theta \in \Theta$.

Then, there exists a neighborhood $\mathcal{D} \subseteq \Theta$ of $\theta^*$ and constants $\mu > 0$ and $L < \infty$ such that the following hold:

*   **(a) Local Strong Concavity:** The population log-likelihood $\mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$ is strongly concave with parameter $\mu$ within $\mathcal{D}$, i.e., for all $\theta_1, \theta_2 \in \mathcal{D}$:

    $$
    \mathbb{E}_{P^*}[\mathcal{L}(\theta_2; \mathbf{X}, \mathbf{y})] \leq \mathbb{E}_{P^*}[\mathcal{L}(\theta_1; \mathbf{X}, \mathbf{y})] + \nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta_1; \mathbf{X}, \mathbf{y})]^\top (\theta_2 - \theta_1) - \frac{\mu}{2} \| \theta_2 - \theta_1 \|_2^2.
    $$

*   **(b) Lipschitz Gradient:** The gradient of the population log-likelihood $\nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$ is Lipschitz continuous with constant $L$ within $\mathcal{D}$, i.e., for all $\theta_1, \theta_2 \in \mathcal{D}$:

    $$
    \| \nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta_1; \mathbf{X}, \mathbf{Y})] - \nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta_2; \mathbf{X}, \mathbf{Y})] \|_2 \leq L \| \theta_1 - \theta_2 \|_2.
    $$
\end{theorem}

\begin{proof}
The proof follows a similar structure to the previous attempts, but crucially, we now work within a restricted parameter space.

**1. Define the Population Log-Likelihood:**
As before, let $\bar{\mathcal{L}}(\theta) = \mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{Y})]$ denote the population log-likelihood.

**2. Consider the Hessian:**
Let $\mathbf{H}(\theta)$ be the Hessian of $\bar{\mathcal{L}}(\theta)$. We will show that $\mathbf{H}(\theta^*)$ is negative definite.

**3. Expected Fisher Information:**
The expected Fisher information matrix is $\mathbf{I}(\theta^*) = -\mathbf{H}(\theta^*)$. We will show that $\mathbf{I}(\theta^*)$ is positive definite.

**4. Positive Definiteness of $I(\theta^*)$:**
We want to show that for any non-zero vector $\mathbf{v} \in \mathbb{R}^2$, $\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} > 0$. Following the same steps as in the previous attempt, we arrive at:

$$
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} = \frac{1}{2} \mathbb{E}_{P^*_X}\left[ \text{tr}\left( (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \mathbf{A} (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \mathbf{A} \right) \right],
$$

where $\mathbf{A} = v_1 \frac{\partial \mathbf{K}_{\theta^*}}{\partial \sigma_f^2} + v_2 \frac{\partial \mathbf{K}_{\theta^*}}{\partial \ell}$.

As before, we can rewrite this as:

$$
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} = \frac{1}{2} \mathbb{E}_{P^*_X}\left[ \text{tr}\left( (\mathbf{B} \mathbf{A} \mathbf{B})^\top (\mathbf{B} \mathbf{A} \mathbf{B}) \right) \right],
$$

where $\mathbf{B} = (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1/2}$.

**5. Key Argument - Restricted Parameter Space and Linear Independence:**
Now, we leverage the restricted parameter space $\Theta$. Since $\sigma_f$ is bounded away from 0 and $\ell$ is bounded away from both 0 and $\infty$, the eigenvalues of $\mathbf{K}_{\theta^*}$ are bounded away from 0 and $\infty$. This implies that $\mathbf{B}$ and $\mathbf{B}^{-1}$ are well-defined and have bounded norms.

Suppose for contradiction that $\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} = 0$ for some non-zero $\mathbf{v}$. This implies that $\mathbf{B}\mathbf{A}\mathbf{B} = \mathbf{0}$ almost surely, and consequently, $\mathbf{A} = \mathbf{0}$ almost surely.

Thus, we have:

$$
v_1 \frac{\partial \mathbf{K}_{\theta^*}}{\partial \sigma_f^2} + v_2 \frac{\partial \mathbf{K}_{\theta^*}}{\partial \ell} = \mathbf{0} \quad \text{a.s.}
$$

Now, we need to show that this is impossible for non-zero $\mathbf{v}$ under our assumptions. We can write this as:
$$
v_1 \frac{\partial k_{\theta^*}}{\partial \sigma_f^2}(x_i,x_j) + v_2 \frac{\partial k_{\theta^*}}{\partial \ell}(x_i,x_j) = 0
$$
for all $x_i,x_j$ in the support of $P_X^*$.

Using the derivatives of the RBF kernel that we computed earlier, this becomes:
$$
v_1 \exp\left(-\frac{\|x_i - x_j\|_2^2}{2\ell_*^2}\right) + v_2 \sigma_{f*}^2 \exp\left(-\frac{\|x_i - x_j\|_2^2}{2\ell_*^2}\right) \left(\frac{\|x_i - x_j\|_2^2}{\ell_*^3}\right) = 0
$$
for all $x_i, x_j$ in the support of $P_X^*$.

Dividing by $\exp\left(-\frac{\|x_i - x_j\|_2^2}{2\ell_*^2}\right)$ (which is always non-zero), we get:
$$
v_1 + v_2 \sigma_{f*}^2 \frac{\|x_i - x_j\|_2^2}{\ell_*^3} = 0
$$

Since $\sigma_{f*} > 0$ and $\ell_* > 0$, the only way for this to hold for all $x_i, x_j$ in a set of non-zero measure under $P^*_X$ is if $v_1 = v_2 = 0$.
This is because if $v_2 \neq 0$, then the term $\frac{\|x_i - x_j\|_2^2}{\ell_*^3}$ can be varied continuously by changing $x_i$ and $x_j$, which makes it impossible for the equation to hold unless $v_1$ is also zero.

Thus, we have reached a contradiction. Therefore, for any non-zero $\mathbf{v}$, we must have $\mathbf{A} \neq \mathbf{0}$ almost surely, and hence $\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} > 0$.

**6. Negative Definite Hessian and Strong Concavity:**

Since $\mathbf{I}(\theta^*)$ is positive definite, $\mathbf{H}(\theta^*)$ is negative definite. By continuity of the Hessian (due to the regularity assumption on the kernel), there exists a neighborhood $\mathcal{D} \subseteq \Theta$ of $\theta^*$ such that $\mathbf{H}(\theta)$ is negative definite for all $\theta \in \mathcal{D}$. Moreover, there exists a constant $\mu > 0$ such that $-\mathbf{H}(\theta) \succeq \mu \mathbf{I}$ for all $\theta \in \mathcal{D}$. This implies that the population log-likelihood is strongly concave with parameter $\mu$ within $\mathcal{D}$.

**7. Lipschitz Continuity of the Gradient:**

The Lipschitz continuity of the gradient follows from the boundedness of the third-order derivatives of the population log-likelihood in the neighborhood $\mathcal{D}$. This can be shown using the regularity assumptions on the kernel (Assumption 3) and the boundedness assumption (Assumption 2). Since the third-order derivatives are bounded, there exists a constant $L < \infty$ such that:

$$
\| \nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta_1; \mathbf{X}, \mathbf{Y})] - \nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta_2; \mathbf{X}, \mathbf{Y})] \|_2 \leq L \| \theta_1 - \theta_2 \|_2
$$

for all $\theta_1, \theta_2 \in \mathcal{D}$.

**Conclusion:**

We have shown that under the given assumptions, there exists a neighborhood $\mathcal{D}$ of $\theta^*$ and constants $\mu > 0$ and $L < \infty$ such that the population log-likelihood is strongly concave with parameter $\mu$ and has a Lipschitz continuous gradient with constant $L$ within $\mathcal{D}$. This completes the proof.
\end{proof}
\begin{theorem}[Local Strong Concavity of Population Log-Likelihood - General Case]\label{thm:local-strong-concavity-general}
Let $P^*$ be the true data generating distribution, and let $\theta^*$ be the maximizer of the population log-likelihood $\mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$ for a Gaussian process with kernel $k_\theta(x, x')$.

Assume the following:

1.  **Identifiability:** The hyperparameters $\theta^*$ are identifiable from the data generating distribution $P^*$.
2.  **Smoothness and Boundedness:**
    *   The true data generating process is of the form $y_i = \xi(x_i) + \epsilon_i$, where $\xi \sim GP(0, k_{\theta^*})$ and $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ are i.i.d. noise.
    *   The marginal distribution of the inputs $P^*_X$ has a density $p(x)$ with respect to the Lebesgue measure on $\mathbb{R}^d$ that is bounded away from zero on a compact set $\mathcal{X} \subset \mathbb{R}^d$.
    *   The function $\xi$ and its first and second partial derivatives are bounded on $\mathcal{X}$.
3.  **Regularity of the Kernel:** The kernel $k_\theta(x,x')$ and its first, second, and third partial derivatives with respect to $\theta$ are continuous and bounded on $\mathcal{X} \times \mathcal{X}$ for $\theta$ in a neighborhood of $\theta^*$.
4. **Linear Independence of Derivatives:** For any non-zero vector $\mathbf{v}$ of the same dimension as $\theta$, the following holds almost surely with respect to $P^*_X$:
    $$
    \sum_{i} v_i \frac{\partial \mathbf{K}_\theta}{\partial \theta_i} \neq \mathbf{0}
    $$
    where $\mathbf{K}_\theta$ is the kernel matrix evaluated at a set of input points drawn from $P^*_X$.

Then, there exists a neighborhood $\mathcal{D}$ of $\theta^*$ and constants $\mu > 0$ and $L < \infty$ such that the following hold:

*   **(a) Local Strong Concavity:** The population log-likelihood $\mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$ is strongly concave with parameter $\mu$ within $\mathcal{D}$.
*   **(b) Lipschitz Gradient:** The gradient of the population log-likelihood $\nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$ is Lipschitz continuous with constant $L$ within $\mathcal{D}$.
\end{theorem}
\begin{proof}
Let $\bar{\mathcal{L}}(\theta) = \mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$ denote the population log-likelihood. The Hessian matrix $\mathbf{H}(\theta)$ has entries given by:

$$
\mathbf{H}_{ij}(\theta) = \frac{\partial^2 \bar{\mathcal{L}}(\theta)}{\partial \theta_i \partial \theta_j}.
$$

We want to show that $\mathbf{H}(\theta^*)$ is negative definite. It is equivalent to showing that the negative Hessian, $-\mathbf{H}(\theta^*)$, which is also known as the *expected Fisher information matrix*, is positive definite.

The log-likelihood for a set of observations $\{(\mathbf{x}_i, y_i)\}_{i=1}^n$ (assuming a zero mean function) is:

$$
\mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) = -\frac{1}{2} \mathbf{y}^\top (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log|\mathbf{K}_\theta + \sigma^2 \mathbf{I}| - \frac{n}{2} \log(2\pi),
$$

where $\mathbf{K}_\theta$ is the kernel matrix evaluated at the points in $\mathbf{X}$, and $\sigma^2$ is the noise variance.

\textbf{1. First-Order Derivatives:**}

The gradient of the log-likelihood has components:

$$
\frac{\partial \mathcal{L}}{\partial \theta_i} = \frac{1}{2} \mathbf{y}^\top (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \text{tr}\left((\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right).
$$

\textbf{2. Second-Order Derivatives:**}

The second-order derivatives can be computed using matrix calculus identities. After some calculations (details in Appendix A), we arrive at the following expression:

\begin{align*}
\frac{\partial^2 \mathcal{L}}{\partial \theta_i \partial \theta_j} &= -\frac{1}{2} \mathbf{y}^\top (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left[ \frac{\partial^2 \mathbf{K}_\theta}{\partial \theta_i \partial \theta_j} - \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right)(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right) - \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right)(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) \right] (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \mathbf{y} \\
&\quad + \frac{1}{2} \text{tr}\left[ (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right)(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right) \right] - \frac{1}{2} \text{tr}\left[ (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial^2 \mathbf{K}_\theta}{\partial \theta_i \partial \theta_j}\right) \right].
\end{align*}

\textbf{3. Population Log-Likelihood and Hessian:**}

The population log-likelihood is $\bar{\mathcal{L}}(\theta) = \mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$. The Hessian of the population log-likelihood is:

$$
\mathbf{H}(\theta) = \mathbb{E}_{P^*}[\nabla^2_\theta \mathcal{L}(\theta; \mathbf{X}, \mathbf{y})],
$$

where $\nabla^2_\theta$ denotes the Hessian with respect to $\theta$.

\textbf{4. Expected Fisher Information:**}

The expected Fisher information matrix is $\mathbf{I}(\theta^*) = -\mathbf{H}(\theta^*)$. We want to show that $\mathbf{I}(\theta^*)$ is positive definite.

\textbf{5. Positive Definiteness:**}

To show that $\mathbf{I}(\theta^*)$ is positive definite, we need to show that for any non-zero vector $\mathbf{v} \in \mathbb{R}^2$:

$$
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} > 0.
$$

Substituting $\theta = \theta^*$ and taking the expectation with respect to $P^*$, we get:

$$
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} = -\mathbb{E}_{P^*}[\mathbf{v}^\top \nabla^2_\theta \mathcal{L}(\theta^*; \mathbf{X}, \mathbf{y}) \mathbf{v}].
$$

Since $\mathbf{y}|\mathbf{X} \sim \mathcal{N}(\mathbf{0}, \mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})$, we have $\mathbb{E}_{P^*}[\mathbf{y}\mathbf{y}^\top | \mathbf{X}] = \mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I}$. Using this, we can simplify the expectation of the quadratic form involving $\mathbf{y}$ (details in Appendix B). After algebraic manipulation, we obtain:

$$
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} = \frac{1}{2} \mathbb{E}_{P^*_X}\left[ \text{tr}\left( (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \mathbf{A} (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \mathbf{A} \right) \right],
$$

where $\mathbf{A} = \sum_{i=1}^2 v_i \frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_i}$.

\textbf{6. Using Properties of Trace and Positive Semidefiniteness:**}

Now, note that $(\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1}$ is positive definite and thus has a positive definite square root. Let $\mathbf{B} = (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1/2}$. Then, we can rewrite the expression as:

$$
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} = \frac{1}{2} \mathbb{E}_{P^*_X}\left[ \text{tr}\left( \mathbf{B} \mathbf{A} \mathbf{B} \mathbf{B} \mathbf{A} \mathbf{B} \right) \right] = \frac{1}{2} \mathbb{E}_{P^*_X}\left[ \text{tr}\left( (\mathbf{B} \mathbf{A} \mathbf{B}) (\mathbf{B} \mathbf{A} \mathbf{B}) \right) \right] = \frac{1}{2} \mathbb{E}_{P^*_X}\left[ \text{tr}\left( (\mathbf{B} \mathbf{A} \mathbf{B})^\top (\mathbf{B} \mathbf{A} \mathbf{B}) \right) \right].
$$

Since $(\mathbf{B}\mathbf{A}\mathbf{B})^\top (\mathbf{B}\mathbf{A}\mathbf{B})$ is positive semidefinite, its trace is non-negative. Moreover, the trace is zero if and only if $\mathbf{B}\mathbf{A}\mathbf{B} = \mathbf{0}$.

\textbf{7. Using Identifiability and Smoothness:**}

Here, we use the identifiability assumption. If $\mathbf{B}\mathbf{A}\mathbf{B} = \mathbf{0}$, then $(\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1/2} \mathbf{A} (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1/2} = \mathbf{0}$, which implies $\mathbf{A} = \mathbf{0}$ (since the inverse square root is invertible).

$\mathbf{A} = \mathbf{0}$ implies:

$$
\sum_{i=1}^p v_i \frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_i}=\mathbf{0}
$$


This means that a linear combination of the derivatives of the kernel matrix with respect to the hyperparameters is zero. However, under the identifiability assumption and the smoothness of the data distribution, this can only happen if $v_1 = v_2 = 0$ (i.e., $\mathbf{v}$ is the zero vector). If different hyperparameters always lead to different predictive distributions, then their derivatives must be linearly independent.

Therefore, for any non-zero $\mathbf{v}$, we have $\mathbf{B}\mathbf{A}\mathbf{B} \neq \mathbf{0}$, and thus:

$$
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} = \frac{1}{2} \mathbb{E}_{P^*_X}\left[ \text{tr}\left( (\mathbf{B} \mathbf{A} \mathbf{B})^\top (\mathbf{B} \mathbf{A} \mathbf{B}) \right) \right] > 0.
$$

This shows that $\mathbf{I}(\theta^*)$ is positive definite, and consequently, $\mathbf{H}(\theta^*)$ is negative definite.

\textbf{Step 8: Establish Lipschitz continuity of the gradient.}

The Lipschitz continuity of the gradient follows from the boundedness of the third-order derivatives of the population log-likelihood in the neighborhood $\mathcal{D}$. This can be shown using the regularity assumptions on the kernel (Assumption 3) and the boundedness assumption (Assumption 2). Since the third-order derivatives are bounded, there exists a constant $L < \infty$ such that
\[
\| \nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta_1; \mathbf{X}, \mathbf{y})] - \nabla_\theta \mathbb{E}_{P^*}[\mathcal{L}(\theta_2; \mathbf{X}, \mathbf{y})] \|_2 \leq L \| \theta_1 - \theta_2 \|_2
\]

for all $\theta_1, \theta_2 \in \mathcal{D}$.

\end{proof}


\amnote{What if we don't have strong concavity? Can we still obtain a weaker guarantee?}
\subsubsection{MLE}

\begin{theorem}
    
\end{theorem}
\begin{proof}

Noting that $\theta^+=\theta+t\nabla \tilde{\mathcal{L}}(\theta)$
    \begin{align*}
        \mathcal{L}(\theta^+)&\geq  \mathcal{L}(\theta)+\nabla \mathcal{L}(\theta)^\top(\theta^+-\theta)-\frac{1}{2}L\Vert \theta^+-\theta\Vert^2\\
        &= \mathcal{L}(\theta)+\nabla \mathcal{L}(\theta)^\top(t\nabla \tilde{\mathcal{L}}(\theta))-\frac{1}{2}tL\Vert\nabla \tilde{\mathcal{L}}(\theta)\Vert^2\\
        &= \mathcal{L}(\theta)+t\nabla \mathcal{L}(\theta)^\top(\nabla\mathcal{L}(\theta)+\nabla \tilde{\mathcal{L}}(\theta)-\nabla\mathcal{L}(\theta))-\frac{1}{2}tL\Vert\nabla \mathcal{L}(\theta)+\nabla \tilde{\mathcal{L}}(\theta)-\nabla\mathcal{L}(\theta)\Vert^2\\
        &\geq \mathcal{L}(\theta)+\left(t-\frac{1}{2}L\right)\Vert \nabla \mathcal{L}(\theta)\Vert^2-t\Vert\mathcal{L}(\theta)\Vert\Vert \nabla\tilde{\mathcal{L}}(\theta)-\nabla\mathcal{L}(\theta)\Vert-\frac{1}{2}tL\Vert \nabla\tilde{\mathcal{L}}(\theta)-\nabla\mathcal{L}(\theta)\Vert^2
    \end{align*}
\end{proof}


% \subsubsection{Difference in Log-Likelihoods}
% \begin{lemma}\label{lemma:difference-in-log-likelihoods}
% Let \( \mathbf{K} \) be the true kernel matrix, \( \tilde{\mathbf{K}} \) be the SKI approximation of the kernel matrix using \( m \) inducing points and interpolation degree \( L-1 \), and \( \sigma^2 \) be the regularization parameter (noise variance). Let \( \mathbf{y} \) be the vector of observed target values, and \( n \) be the number of data points. Let \( \mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) \) be the true log-likelihood and \( \tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y}) \) be the SKI approximation of the log-likelihood, both for a mean-zero Gaussian Process prior. Then, the absolute difference between the true and SKI log-likelihoods is bounded as follows:

% \begin{equation}
% | \mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) - \tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y}) | \leq \frac{1}{2} \left( \frac{\gamma_{n, m, L}}{\sigma^4} \| \mathbf{y} \|^2 + \frac{\gamma_{n, m, L}}{\sigma^2} \right)
% \end{equation}

% where \( \gamma_{n, m, L} \equiv n(\delta_{m, L} + \sqrt{L} c^d \delta_{m, L}) \), \( \delta_{m, L} \) is an upper bound on the error of multivariate interpolation with \( m \) inducing points and interpolation degree \( L-1 \), and \( c > 0 \) is an upper bound on the absolute sum of the weights for 1D kernel interpolation.

% Furthermore, if the convolutional cubic interpolation kernel (\( L=4 \)) is used with grid spacing \( h \), and the domain is contained in $[-D, D]^d$, then:

% \begin{equation}
% | \mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) - \tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y}) | \leq \frac{1}{2} \left( \frac{n(2O(c^{2d}h^3))}{\sigma^4} \| \mathbf{y} \|^2 + \frac{n(2O(c^{2d}h^3))}{\sigma^2} \right)
% \end{equation}
% \end{lemma}

% \begin{proof}
% We begin by considering the log-likelihood of a mean-zero Gaussian Process, given by:

% \begin{equation}
% \mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) = -\frac{1}{2} \mathbf{y}^\top (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log|\mathbf{K} + \sigma^2 \mathbf{I}| - \frac{n}{2} \log(2\pi)
% \end{equation}

% The corresponding SKI approximation is:

% \begin{equation}
% \tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y}) = -\frac{1}{2} \mathbf{y}^\top (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log|\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| - \frac{n}{2} \log(2\pi)
% \end{equation}

% Our goal is to bound the absolute difference \( |\mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) - \tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y})| \). Applying the triangle inequality, we have:

% \begin{align}
% | \mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) - \tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y}) | &= \left| \frac{1}{2} \mathbf{y}^\top (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \mathbf{y}^\top (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} + \frac{1}{2} \log|\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| - \frac{1}{2} \log|\mathbf{K} + \sigma^2 \mathbf{I}| \right| \nonumber \\
% &\leq \left| \frac{1}{2} \mathbf{y}^\top (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \mathbf{y}^\top (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} \right| + \left| \frac{1}{2} \log|\mathbf{K} + \sigma^2 \mathbf{I}| - \frac{1}{2} \log|\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| \right|
% \end{align}

% We now leverage Lemma \ref{lemma:action-inverse-error} and Lemma \ref{lemma:log-determinant-error} to bound the two terms on the right-hand side. Lemma \ref{lemma:action-inverse-error} provides a bound on the difference between the action of the true and approximated regularized inverses:

% \begin{equation}
% \| (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} \|_2 \leq \frac{\gamma_{n, m, L}}{\sigma^4} \| \mathbf{y} \|_2
% \end{equation}

% This allows us to bound the first term:

% \begin{equation}
% \left| \frac{1}{2} \mathbf{y}^\top (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \mathbf{y}^\top (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} \right| \leq \frac{1}{2} \frac{\gamma_{n, m, L}}{\sigma^4} \| \mathbf{y} \|^2
% \end{equation}

% Lemma \ref{lemma:log-determinant-error} bounds the difference in the log-determinants:

% \begin{equation}
% | \log|\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| - \log|\mathbf{K} + \sigma^2 \mathbf{I}| | \leq \frac{\gamma_{n, m, L}}{\sigma^2}
% \end{equation}

% Thus, the second term is bounded by:

% \begin{equation}
% \left| \frac{1}{2} \log|\mathbf{K} + \sigma^2 \mathbf{I}| - \frac{1}{2} \log|\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| \right| \leq \frac{1}{2} \frac{\gamma_{n, m, L}}{\sigma^2}
% \end{equation}

% Combining these bounds, we obtain the overall bound on the difference in log-likelihoods:

% \begin{equation}
% | \mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) - \tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y}) | \leq \frac{1}{2} \left( \frac{\gamma_{n, m, L}}{\sigma^4} \| \mathbf{y} \|^2 + \frac{\gamma_{n, m, L}}{\sigma^2} \right)
% \end{equation}

% Now, let us consider the specific case of using a convolutional cubic interpolation kernel (\( L=4 \)). From Lemma \ref{lemma:tensor-product-interpolation-error}, we have \( \delta_{m, L} = O(c^{2d}h^3) \). Consequently, from Corollary \ref{cor:spectral-norm-error-alt}:

% \begin{equation}
% \gamma_{n, m, L} \equiv n(\delta_{m, L} + \sqrt{L} c^d \delta_{m, L}) = n(O(c^{2d}h^3) + 2c^d O(c^{2d}h^3)) = n(2O(c^{2d}h^3))
% \end{equation}

% Substituting this into our general bound, we obtain the specific bound for cubic interpolation:

% \begin{equation}
% | \mathcal{L}(\theta; \mathbf{X}, \mathbf{y}) - \tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y}) | \leq \frac{1}{2} \left( \frac{n(2O(c^{2d}h^3))}{\sigma^4} \| \mathbf{y} \|^2 + \frac{n(2O(c^{2d}h^3))}{\sigma^2} \right)
% \end{equation}

% This concludes the proof.
% \end{proof}

% \begin{assumption}[Local Identifiability of Hyperparameters]\label{assumption:local-identifiability}
% Let $P(\cdot | \mathbf{X}; \theta)$ denote the Gaussian process predictive distribution at a set of input points $\mathbf{X}$ given hyperparameters $\theta$. There exists a neighborhood $\mathcal{D}$ of the true hyperparameters $\theta^*$ such that for any $\theta \in \mathcal{D}$ with $\theta \neq \theta^*$, there exists a finite set of input points $\mathbf{X} = \{\mathbf{x}_1, \ldots, \mathbf{x}_n\}$, a set $A$ in the Borel sigma-algebra of $\mathbb{R}^n$, and a $\delta > 0$ such that:

% $$
% |P(\mathbf{y} \in A | \mathbf{X}; \theta) - P(\mathbf{y} \in A | \mathbf{X}; \theta^*)| > \delta
% $$

% and

% $$
% P^*_X(\mathbf{X}) > 0,
% $$

% where $\mathbf{y} = (y_1, \ldots, y_n)$ is the vector of corresponding output values, and $P^*_X$ is the marginal distribution of inputs under $P^*$.
% \end{assumption}

% \begin{lemma}[Local Identifiability of Signal Variance]
% Let $\theta = (\sigma^2, \ell^*)$ and $\theta^* = (\sigma^{*2}, \ell^*)$ be two sets of hyperparameters for a Gaussian process with an RBF kernel, where $\sigma^2 \neq \sigma^{*2}$ and the lengthscale $\ell^*$ is fixed. Let $\mathbf{X}_D = \{\mathbf{x}_1, \ldots, \mathbf{x}_N\}$ be a set of $N$ distinct training input points in $\mathbb{R}^d$. Then, there exists an additional input point $\mathbf{x}_{N+1}$ such that the predictive variances $\Sigma(\mathbf{x}_{N+1}; \theta)$ and $\Sigma(\mathbf{x}_{N+1}; \theta^*)$ are different, i.e., $|\Sigma(\mathbf{x}_{N+1}; \theta) - \Sigma(\mathbf{x}_{N+1}; \theta^*)| > 0$.
% \end{lemma}

% \begin{proof}
% \begin{enumerate}
%     \item \textbf{Predictive Variance:} The predictive variance at $\mathbf{x}_{N+1}$ under hyperparameters $\theta = (\sigma^2, \ell^*)$ is given by:
%     $$
%     \Sigma(\mathbf{x}_{N+1}; \sigma^2) = \sigma^2 - \sigma^2 \mathbf{k}(\mathbf{x}_{N+1}, \mathbf{X}_D; \ell^*) K(\mathbf{X}_D, \mathbf{X}_D; \ell^*)^{-1} \mathbf{k}(\mathbf{X}_D, \mathbf{x}_{N+1}; \ell^*)
%     $$
%     Similarly, under $\theta^* = (\sigma^{*2}, \ell^*)$, the predictive variance is:
%     $$
%     \Sigma(\mathbf{x}_{N+1}; \sigma^{*2}) = \sigma^{*2} - \sigma^{*2} \mathbf{k}(\mathbf{x}_{N+1}, \mathbf{X}_D; \ell^*) K(\mathbf{X}_D, \mathbf{X}_D; \ell^*)^{-1} \mathbf{k}(\mathbf{X}_D, \mathbf{x}_{N+1}; \ell^*)
%     $$

%     \item \textbf{Difference in Variances:} Subtracting the two variances, we get:
%     \begin{align*}
%         \Sigma(\mathbf{x}_{N+1}; \sigma^2) - \Sigma(\mathbf{x}_{N+1}; \sigma^{*2}) &= (\sigma^2 - \sigma^{*2}) - (\sigma^2 - \sigma^{*2}) \mathbf{c} \\
%         &= (\sigma^2 - \sigma^{*2})(1 - \mathbf{c})
%     \end{align*}
%     where $\mathbf{c} = \mathbf{k}(\mathbf{x}_{N+1}, \mathbf{X}_D; \ell^*) K(\mathbf{X}_D, \mathbf{X}_D; \ell^*)^{-1} \mathbf{k}(\mathbf{X}_D, \mathbf{x}_{N+1}; \ell^*)$ is a constant that does not depend on the signal variance.

%     \item \textbf{Non-Zero Difference:} Since $\sigma^2 \neq \sigma^{*2}$, the difference $\Sigma(\mathbf{x}_{N+1}; \sigma^2) - \Sigma(\mathbf{x}_{N+1}; \sigma^{*2})$ will be non-zero if $1 - \mathbf{c} \neq 0$.
%     By Cauchy-Schwarz, $\mathbf{c} \leq k(\mathbf{x}_{N+1},\mathbf{x}_{N+1}) = \sigma^{*2}$. Also, since $K$ is positive definite, $\mathbf{c} \ge 0$.
%     We need to choose $\mathbf{x}_{N+1}$ such that $\mathbf{c} \neq 1$.
%     To ensure this, we can choose $\mathbf{x}_{N+1}$ such that $\mathbf{k}(\mathbf{x}_{N+1}, \mathbf{X}_D; \ell^*)$ is not in the column space of $K(\mathbf{X}_D, \mathbf{X}_D; \ell^*)$. This is generally true if the training points are distinct and we choose $\mathbf{x}_{N+1}$ to be sufficiently far from the training points. For example, we can choose $\mathbf{x}_{N+1}$ such that $\mathbf{k}(\mathbf{x}_{N+1}, \mathbf{X}_D; \ell^*)$ is very close to zero.

%     \item \textbf{Conclusion:} Under the assumption of distinct training points, we can always find an $\mathbf{x}_{N+1}$ such that $1 - \mathbf{c} \neq 0$, and thus the predictive variances under $\sigma^2$ and $\sigma^{*2}$ are different. This implies local identifiability of the signal variance.
% \end{enumerate}
% \end{proof}


\begin{itemize}
    \item Describe the importance of Gaussian processes and kernel methods, and their limitations.
    \begin{itemize}
        \item Gaussian Processes (GPs) are an important class of stochastic processes used in machine learning and statistics, particularly spatial data analysis, time series forecasting and bioinformatics. GPs offer a non-parametric framework for modeling distributions over functions, thus enabling both flexibility and uncertainty quantification. These capabilities, combined with the ability to incorporate prior knowledge and specify relationships by choice of kernel function, make Gaussian Processes effective for regression, classification, and optimization.
        \item However, they have substantial computational and memory bottlenecks. Both training and inference require computing the action of the inverse kernel gram matrix, while training requires computing its log-determinant: both are $O(n^3)$ operations, where $n$ is the sample size. Further, storing the full gram matrix requires $O(n^2)$ memory. These bottlenecks necessitate scalable approximations to extend the Gaussian Processes to larger datasets.
    \end{itemize}
    \item Describe Nystr{\"o}m and its limitations
    \item Describe SKI, how its been used in many places, but lacks theoretical guarantees
    \begin{itemize}
        \item Structured Kernel Interpolation (SKI) is an efficient method for scaling Gaussian Processes (GPs) to large datasets by approximating the kernel matrix using interpolation on a set of inducing points. The core idea is to express the original kernel \( k(\mathbf{x}, \mathbf{x}') \) as a combination of interpolation functions and a structured kernel matrix \( \mathbf{K}_g \) defined on a set of inducing points. However, despite its effectiveness, popularity (it has over $600$ citations, a large number for a GP paper) and ready software availability (\cite{gardner2018gpytorch} has 3.5k stars on github), it currently lacks theoretical analysis of even the error bound of the kernel approximation, much less how those errors propagate and influence Gaussian process training and inference.
    \end{itemize}
    \item In this paper, we bridge the gap between practice and theory for SKI.
    \item Describe three contributions
    \begin{itemize}
        \item Error analysis for the kernel approximation, the action of the regularized Kernel inverse and the log determinant. We do this under both grid and Chebyshev spacing
        \item Comparison of these results to Nystr{\"o}m, describing how they are better \amnote{hopefully}.
        \item Analysis of important special cases, including stationary kernels and 
    \end{itemize}
\end{itemize}

% \begin{lemma}\label{lemma:ga-progress}
%     For a gradient ascent step on the SKI log-likelihood, the following inequality holds for the true log-likelihood:
%     \begin{align*}
%         \mathcal{L}(\theta_{k+1})&\geq \mathcal{L}(\theta_k) + \frac{3\eta}{4} \left( \Vert \nabla \mathcal{L}(\theta_k) \Vert - \frac{5\epsilon}{6} \right)^2 - \frac{13\eta\epsilon_G^2}{48}.
%     \end{align*}
%     Further, if $\Vert \nabla \mathcal{L}(\theta_k)\Vert>\frac{5+\sqrt{13}}{6}\epsilon_G$, then a gradient ascent step on the SKI log-likelihood will monotonically increase the true log-likelihood.
% \end{lemma}
% \begin{proof}
%     Define $\delta_k=\nabla\tilde{\mathcal{L}}(\theta_k)-\nabla \mathcal{L}(\theta_k)$. Recall the ascent (descent) Lemma from optimization, which states that due to $\mu$-smoothness,
%     \[ \mathcal{L}(\theta_{k+1}) \geq \mathcal{L}(\theta_k) + \nabla \mathcal{L}(\theta_k)^T (\theta_{k+1} - \theta_k) - \frac{\mu}{2} \| \theta_{k+1} - \theta_k \|^2 \].

%     Substituting the update rule:
%     \begin{align*}
%         \mathcal{L}(\theta_{k+1}) &\geq \mathcal{L}(\theta_k) + \eta \| \nabla \mathcal{\mu}(\theta_k) \|^2 + \eta \nabla \mathcal{L}(\theta_k)^T \delta_k - \frac{\mu \eta^2}{2} \| \nabla \mathcal{\mu}(\theta_k) + \delta_k \|^2 \\
%         &\geq \mathcal{L}(\theta_k) + \eta \| \nabla \mathcal{\mu}(\theta_k) \|^2 + \eta \nabla \mathcal{L}(\theta_k)^T \delta_k - \frac{\mu \eta^2 }{2}\| \nabla \mathcal{\mu}(\theta_k) \|^2 - \frac{\mu\eta^2}{2} \| \delta_k \|^2 - \mu\eta^2\nabla \mathcal{\mu}(\theta_k)^T \delta_k.
%     \end{align*}
% Using Cauchy-Schwarz and the bound on $\| \delta_k \|$:
%     \[ \nabla \mathcal{L}(\theta_k)^T \delta_k \geq - \| \nabla \mathcal{L}(\theta_k) \| \| \delta_k \| \geq - \epsilon_G \| \nabla \mathcal{L}(\theta_k) \| \]
%     Now letting $\eta=\frac{1}{2L}$, we have
%     \begin{align*}
%         \mathcal{L}(\theta_{k+1}) &\geq \mathcal{L}(\theta_k) + \eta \| \nabla \mathcal{L}(\theta_k) \|^2 -\epsilon_G\eta \Vert\nabla \mathcal{L}(\theta_k)\Vert - \frac{\mu \eta^2 }{2}\| \nabla \mathcal{L}(\theta_k) \|^2 - \frac{\mu\eta^2}{2} \epsilon_G^2 - \mu\eta^2\epsilon_G \Vert\nabla \mathcal{L}(\theta_k)\Vert\\
%         &=\mathcal{L}(\theta_k)+\eta\left(1-\frac{\mu\eta}{2}\right)\Vert \nabla \mathcal{L}(\theta_k)\Vert^2-\epsilon_G\eta(1+\mu\eta)\Vert \nabla \mathcal{L}(\theta_k)\Vert-\frac{\mu\eta^2}{2}\epsilon^2
%     \end{align*}
%     Now $\text{Substituting }\eta=\frac{1}{2\mu}\text{ we get:}$
%     \begin{align*}
%         \mathcal{L}(\theta_{k+1}) &\geq \mathcal{L}(\theta_k) + \frac{3\eta}{4} \Vert \nabla \mathcal{L}(\theta_k) \Vert^2 - \frac{3\epsilon_G\eta}{4} \Vert \nabla \mathcal{L}(\theta_k) \Vert - \frac{\eta\epsilon_G^2}{4}\\
%         &= \mathcal{L}(\theta_k) + \frac{3\eta}{4} \left( \Vert \nabla \mathcal{L}(\theta_k) \Vert^2 - \frac{5\epsilon_G}{3}\Vert \nabla \mathcal{L}(\theta_k) \Vert \right) - \frac{\eta\epsilon_G^2}{4}\\
%         &= \mathcal{L}(\theta_k) + \frac{3\eta}{4} \left( \Vert \nabla \mathcal{L}(\theta_k) \Vert^2 - \frac{5\epsilon_G}{3}\Vert \nabla \mathcal{L}(\theta_k) \Vert + \left(\frac{5\epsilon_G}{6}\right)^2 - \left(\frac{5\epsilon_G}{6}\right)^2 \right) - \frac{\eta\epsilon_G^2}{4}\\
%         &= \mathcal{L}(\theta_k) + \frac{3\eta}{4} \left( \Vert \nabla \mathcal{L}(\theta_k) \Vert - \frac{5\epsilon_G}{6} \right)^2 - \frac{3\eta}{4}\left(\frac{5\epsilon_G}{6}\right)^2 - \frac{\eta\epsilon_G^2}{4}\\
%         &= \mathcal{L}(\theta_k) + \frac{3\eta}{4} \left( \Vert \nabla \mathcal{L}(\theta_k) \Vert - \frac{5\epsilon_G}{6} \right)^2 - \frac{25\eta\epsilon_G^2}{48} - \frac{12\eta\epsilon_G^2}{48}\\
%         &= \mathcal{L}(\theta_k) + \frac{3\eta}{4} \left( \Vert \nabla \mathcal{L}(\theta_k) \Vert - \frac{5\epsilon_G}{6} \right)^2 - \frac{13\eta\epsilon_G^2}{48}
%     \end{align*}
%     and the term $\frac{3\eta}{4} \left( \Vert \nabla \mathcal{L}(\theta_k) \Vert - \frac{5\epsilon_G}{6} \right)^2 - \frac{13\eta\epsilon_G^2}{48}$ will be positive if $\Vert \nabla \mathcal{L}(\theta_k)\Vert>\frac{5+\sqrt{13}}{6}\epsilon_G$, implying the second claim.
% \end{proof}

% \begin{lemma}
%     For $k$ sufficiently large, we have
%     \begin{align*}
%         \Vert \nabla \mathcal{L}(\theta_k)\Vert &\leq \ldots \epsilon_G
%     \end{align*}
% \end{lemma}
% \begin{proof}
    
% \end{proof}
% \section{Convergence Analysis with Approximate Log-Likelihood (Formal Proof)}

% We formally prove that gradient ascent on an approximate log-likelihood with a bounded gradient difference converges to a neighborhood of a stationary point of the true log-likelihood.

% \subsection{Assumptions}

% \begin{itemize}
%     \item The true log-likelihood $\mathcal{L}(\theta)$ is $\mu$-smooth.
%     \item The gradient difference is bounded: $\| \nabla \mathcal{L}(\theta) - \nabla \tilde{\mathcal{L}}(\theta) \| \leq \epsilon$ for all $\theta \in \mathbb{R}^d$, where $\epsilon \geq 0$.
%     \item Gradient ascent updates on the approximate log-likelihood $\tilde{\mathcal{L}}(\theta)$:
%     \[ \theta_{k+1} = \theta_k + \eta \nabla \tilde{\mathcal{L}}(\theta_k) \]
%     \item Step size: $0 < \eta \leq \frac{1}{2\mu}$
%     \item $\mathcal{L}(\theta)$ is upper bounded by some constant $\mathcal{L}^*$.
% \end{itemize}

% \subsection{Goal}

% Show that gradient ascent on $\tilde{\mathcal{L}}(\theta)$ leads to a point where $\| \nabla \mathcal{L}(\theta_k) \|$ is approximately on the order of $\epsilon$.

% \subsection{Proof}

% \begin{enumerate}
%     \item \textbf{Applying Smoothness:}
%     From the $\mu$-smoothness of $\mathcal{L}(\theta)$, we have the following ascent lemma:
%     \[ \mathcal{L}(\theta_{k+1}) \leq \mathcal{L}(\theta_k) + \nabla \mathcal{L}(\theta_k)^T (\theta_{k+1} - \theta_k) + \frac{\mu}{2} \| \theta_{k+1} - \theta_k \|^2 \]
%     Using the update rule $\theta_{k+1} = \theta_k + \eta \nabla \tilde{\mathcal{L}}(\theta_k)$ and substituting $\nabla \tilde{\mathcal{L}}(\theta_k) = \nabla \mathcal{L}(\theta_k) + \delta_k$ where $\delta_k = \nabla \tilde{\mathcal{L}}(\theta_k) - \nabla \mathcal{L}(\theta_k)$ and $\| \delta_k \| \leq \epsilon$, we get:
%     \begin{align*}
%         \mathcal{L}(\theta_{k+1}) &\geq \mathcal{L}(\theta_k) + \eta \| \nabla \mathcal{L}(\theta_k) \|^2 + \eta \nabla \mathcal{L}(\theta_k)^T \delta_k - \frac{\mu \eta^2}{2} \| \nabla \mathcal{L}(\theta_k) + \delta_k \|^2 \\
%         &\geq \mathcal{L}(\theta_k) + \eta \| \nabla \mathcal{L}(\theta_k) \|^2 + \eta \nabla \mathcal{L}(\theta_k)^T \delta_k - \frac{\mu \eta^2 }{2}\| \nabla \mathcal{L}(\theta_k) \|^2 - \frac{\mu\eta^2}{2} \| \delta_k \|^2 - \mu\eta^2\nabla \mathcal{L}(\theta_k)^T \delta_k
%     \end{align*}

%     \item \textbf{Bounding the Difference Term:}
%     Using Cauchy-Schwarz and the bound on $\| \delta_k \|$:
%     \[ \nabla \mathcal{L}(\theta_k)^T \delta_k \geq - \| \nabla \mathcal{L}(\theta_k) \| \| \delta_k \| \geq - \epsilon \| \nabla \mathcal{L}(\theta_k) \| \]
%     Now letting $\eta=\frac{1}{2\mu}$, we have
%     \begin{align*}
%         \mathcal{L}(\theta_{k+1}) &\geq \mathcal{L}(\theta_k) + \eta \| \nabla \mathcal{L}(\theta_k) \|^2 -\epsilon\eta \Vert\nabla \mathcal{L}(\theta_k)\Vert - \frac{\mu \eta^2 }{2}\| \nabla \mathcal{L}(\theta_k) \|^2 - \frac{\mu\eta^2}{2} \epsilon^2 - \mu\eta^2\epsilon \Vert\nabla \mathcal{L}(\theta_k)\Vert\\
%         &=\mathcal{L}(\theta_k)+\eta\left(1-\frac{\mu\eta}{2}\right)\Vert \nabla \mathcal{L}(\theta_k)\Vert^2-\epsilon\eta(1+\mu\eta)\Vert \nabla \mathcal{L}(\theta_k)\Vert-\frac{\mu\eta^2}{2}\epsilon^2
%     \end{align*}
%     Now $\text{Substituting }\eta=\frac{1}{2\mu}\text{ we get:}$
%     \begin{align*}
%         \mathcal{L}(\theta_{k+1}) &\geq \mathcal{L}(\theta_k) + \frac{3\eta}{4} \Vert \nabla \mathcal{L}(\theta_k) \Vert^2 - \frac{5\epsilon\eta}{4} \Vert \nabla \mathcal{L}(\theta_k) \Vert - \frac{\eta\epsilon^2}{8}\\
%         &= \mathcal{L}(\theta_k) + \frac{3\eta}{4} \left( \Vert \nabla \mathcal{L}(\theta_k) \Vert^2 - \frac{5\epsilon}{3}\Vert \nabla \mathcal{L}(\theta_k) \Vert \right) - \frac{\eta\epsilon^2}{8}\\
%         &= \mathcal{L}(\theta_k) + \frac{3\eta}{4} \left( \Vert \nabla \mathcal{L}(\theta_k) \Vert^2 - \frac{5\epsilon}{3}\Vert \nabla \mathcal{L}(\theta_k) \Vert + \left(\frac{5\epsilon}{6}\right)^2 - \left(\frac{5\epsilon}{6}\right)^2 \right) - \frac{\eta\epsilon^2}{8}\\
%         &= \mathcal{L}(\theta_k) + \frac{3\eta}{4} \left( \Vert \nabla \mathcal{L}(\theta_k) \Vert - \frac{5\epsilon}{6} \right)^2 - \frac{3\eta}{4}\left(\frac{5\epsilon}{6}\right)^2 - \frac{\eta\epsilon^2}{8}\\
%         &= \mathcal{L}(\theta_k) + \frac{3\eta}{4} \left( \Vert \nabla \mathcal{L}(\theta_k) \Vert - \frac{5\epsilon}{6} \right)^2 - \frac{25\eta\epsilon^2}{48} - \frac{6\eta\epsilon^2}{48}\\
%         &= \mathcal{L}(\theta_k) + \frac{3}{8\mu} \left( \Vert \nabla \mathcal{L}(\theta_k) \Vert - \frac{5\epsilon}{3} \right)^2 - \frac{31\epsilon^2}{96\mu}
%     \end{align*}

%     \item \textbf{Rearranging:}
%     \[ \mathcal{L}(\theta_k) - \mathcal{L}(\theta_{k+1}) \leq \frac{31\epsilon^2}{96\mu} - \frac{3}{8\mu} \left( \| \nabla \mathcal{L}(\theta_k) \| - \frac{5\epsilon}{3} \right)^2 \]
%     \[ \frac{3}{8\mu} \left( \| \nabla \mathcal{L}(\theta_k) \| - \frac{5\epsilon}{6} \right)^2 \leq \mathcal{L}(\theta_{k+1}) - \mathcal{L}(\theta_k) + \frac{31\epsilon^2}{96\mu} \]

%     \item \textbf{Telescoping Sum:} Summing the inequality from $k=0$ to $K-1$:
%     \[ \sum_{k=0}^{K-1} \frac{3}{8\mu} \left( \| \nabla \mathcal{L}(\theta_k) \| - \frac{5\epsilon}{6} \right)^2 \leq \sum_{k=0}^{K-1} \left( \mathcal{L}(\theta_{k+1}) - \mathcal{L}(\theta_k) \right) + \sum_{k=0}^{K-1} \frac{31\epsilon^2}{96\mu} \]
%     \[ \frac{3}{8\mu} \sum_{k=0}^{K-1} \left( \| \nabla \mathcal{L}(\theta_k) \| - \frac{5\epsilon}{6} \right)^2 \leq \mathcal{L}(\theta_K) - \mathcal{L}(\theta_0) + \frac{31K\epsilon^2}{96\mu} \]
%     Since $\mathcal{L}(\theta)$ is upper bounded by $\mathcal{L}^*$, we have:
%     \[ \frac{3}{8\mu} \sum_{k=0}^{K-1} \left( \| \nabla \mathcal{L}(\theta_k) \| - \frac{5\epsilon}{6} \right)^2 \leq \mathcal{L}^* - \mathcal{L}(\theta_0) + \frac{31K\epsilon^2}{96\mu} \]

%     \item \textbf{Bounding the Average:} Dividing by $K\frac{3}{8\mu}$:
%     \[ \frac{1}{K} \sum_{k=0}^{K-1} \left( \| \nabla \mathcal{L}(\theta_k) \| - \frac{5\epsilon}{6} \right)^2 \leq \frac{8\mu(\mathcal{L}^* - \mathcal{L}(\theta_0))}{3K} + \frac{31\epsilon^2}{36} \]
%     Since this inequality holds for all $K$, we can take the limit inferior:
%     \[ \liminf_{K \to \infty} \frac{1}{K} \sum_{k=0}^{K-1} \left( \| \nabla \mathcal{L}(\theta_k) \| - \frac{5\epsilon}{6} \right)^2 \leq \frac{31\epsilon^2}{36} \]

%     \item \textbf{Implication:} The left-hand side is an average of non-negative terms. The limit inferior of an average of non-negative terms is 0 only if the terms themselves converge to 0. Thus, the above inequality implies that there exists a subsequence of $\{\theta_k\}$ such that:
%     \[ \lim_{j \to \infty} \left( \| \nabla \mathcal{L}(\theta_{k_j}) \| - \frac{5\epsilon}{6} \right)^2 = c^2 \]
%     where $c^2 \leq \frac{31\epsilon^2}{36}$.
%     This means there exists a subsequence that converges to a point where:
%     \[ \| \nabla \mathcal{L}(\theta_{k_j}) \| \leq c + \frac{5\epsilon}{6} \]
%     \[ -\left(c + \frac{5\epsilon}{6}\right) \leq \| \nabla \mathcal{L}(\theta_{k_j}) \| \leq c + \frac{5\epsilon}{6} \]
%     So we know that $c + \frac{5\epsilon}{6} \leq \sqrt{\frac{31}{36}}\epsilon + \frac{5\epsilon}{6} = \left( \frac{5}{6} + \frac{\sqrt{31}}{6} \right) \epsilon$
%     So we know that there exists a subsequence that converges to a point where:
%     \[ \| \nabla \mathcal{L}(\theta_{k_j}) \| \leq \left( \frac{5}{6} + \frac{\sqrt{31}}{6} \right) \epsilon \]
%     Since we are assuming we are in a compact space, then we know that the convergence of this subsequence implies the convergence of the entire sequence.
% \end{enumerate}

% \subsection{Implications}

% \begin{itemize}
%     \item \textbf{Convergence to a Neighborhood:} Gradient ascent on the approximate log-likelihood will converge to a region where the true gradient norm $\| \nabla \mathcal{L}(\theta_k) \|$ is approximately on the order of $\epsilon$.
%     \item \textbf{Accuracy Depends on $\epsilon$:} The accuracy is directly tied to the accuracy of the approximate log-likelihood's gradient.
%     \item \textbf{Step Size:} A step size $\eta = \frac{1}{2\mu}$ is used in this analysis.
% \end{itemize}

% \subsection{Further Considerations}

% \begin{itemize}
%     \item Smoothness of $\tilde{\mathcal{L}}(\theta)$ might also play a role.
%     \item Structure of the approximation error could lead to tighter bounds.
% \end{itemize}

% The inequality
% \[ \frac{3}{8\mu}\left(\Vert \nabla \mathcal{L}(\theta_k)\Vert-\frac{5\epsilon}{3}\right)^2-\frac{31\epsilon^2}{96\mu}>0 \]
% holds when:
% \[ \Vert \nabla \mathcal{L}(\theta_k) \Vert > \frac{5 + \sqrt{31}}{6}\epsilon \approx 1.76\epsilon \]

% This means that the norm of the true gradient needs to be sufficiently large compared to the error bound $\epsilon$ for the inequality to hold. In the context of gradient ascent, this condition ensures that the algorithm is still making progress in the right direction, despite the approximation, as long as the true gradient is not too small.

% In conclusion, using an approximate log-likelihood with a bounded gradient difference, gradient ascent can still make progress and converge to a neighborhood of a stationary point of the true log-likelihood. The size of this neighborhood is determined by the accuracy of the gradient approximation.

% \section{Convergence Rate Analysis for Gradient Norms in the Proxy Setting (Corrected)}

% We analyze the convergence rate of the sum of squared gradient norms when using projected gradient ascent on an approximate log-likelihood with a bounded gradient difference.

% \subsection{Assumptions}

% \begin{itemize}
%     \item The true log-likelihood $L(\theta)$ is $L$-smooth.
%     \item The gradient difference is bounded: $\| \nabla L(\theta) - \nabla L_{approx}(\theta) \| \leq \epsilon$ for all $\theta \in \mathcal{D}$.
%     \item Projected gradient ascent updates on the approximate log-likelihood $L_{approx}(\theta)$:
%     \[ \theta_{k+1} = \Pi_{\mathcal{D}}(\theta_k + \eta \nabla L_{approx}(\theta_k)) \]
%     \item Step size: $0 < \eta \leq \frac{1}{2L}$
%     \item $L(\theta)$ is upper bounded by some constant $L^*$ on the domain $\mathcal{D}$.
%     \item $\mathcal{D}$ is a convex set.
% \end{itemize}

% \subsection{Goal}

% Obtain a result analogous to $\frac{1}{K} \sum_{k=0}^{K-1} \| \nabla L(\theta_k) \|^2 = O\left(\frac{1}{K}\right)$, but in the proxy setting.

% \subsection{Derivation}

% \begin{enumerate}
%     \item \textbf{Smoothness and Update:} From the $L$-smoothness of $L(\theta)$ and our previous steps, we have:
%     \[ L(\theta_{k+1}) \geq L(\theta_k) + \frac{\eta}{2} \left( \| \nabla L(\theta_k) \| - \frac{\epsilon}{2} \right)^2 - \frac{3\eta\epsilon^2}{8} \]
%     Rearranging:
%     \[ \frac{\eta}{2} \left( \| \nabla L(\theta_k) \| - \frac{\epsilon}{2} \right)^2 \leq L(\theta_{k+1}) - L(\theta_k) + \frac{3\eta\epsilon^2}{8} \]

%     \item \textbf{Telescoping Sum:} Summing from $k=0$ to $K-1$:
%     \[ \frac{\eta}{2} \sum_{k=0}^{K-1} \left( \| \nabla L(\theta_k) \| - \frac{\epsilon}{2} \right)^2 \leq L(\theta_K) - L(\theta_0) + \frac{3K\eta\epsilon^2}{8} \]
%     Since $L(\theta) \leq L^*$, we have:
%     \[ \frac{\eta}{2} \sum_{k=0}^{K-1} \left( \| \nabla L(\theta_k) \| - \frac{\epsilon}{2} \right)^2 \leq L^* - L(\theta_0) + \frac{3K\eta\epsilon^2}{8} \]

%     \item \textbf{Bounding the Average:} Dividing by $K\eta/2$:
%     \[ \frac{1}{K} \sum_{k=0}^{K-1} \left( \| \nabla L(\theta_k) \| - \frac{\epsilon}{2} \right)^2 \leq \frac{2(L^* - L(\theta_0))}{K\eta} + \frac{3\epsilon^2}{4} \]

%     \item \textbf{Splitting the Sum:} Let $K'$ be the smallest index such that for all $k \geq K'$, $\| \nabla L(\theta_k) \| \leq \left( \frac{1}{2} + \frac{\sqrt{3}}{2} \right) \epsilon$. We can split the sum into two parts:
%     \[ \frac{1}{K} \sum_{k=0}^{K'-1} \left( \| \nabla L(\theta_k) \| - \frac{\epsilon}{2} \right)^2 + \frac{1}{K} \sum_{k=K'}^{K-1} \left( \| \nabla L(\theta_k) \| - \frac{\epsilon}{2} \right)^2 \leq \frac{2(L^* - L(\theta_0))}{K\eta} + \frac{3\epsilon^2}{4} \]

%     \item \textbf{Bounding the Second Term:} For $k \geq K'$, we have:
%     \[ \left( \| \nabla L(\theta_k) \| - \frac{\epsilon}{2} \right)^2 \leq \left( \left( \frac{1}{2} + \frac{\sqrt{3}}{2} \right) \epsilon - \frac{\epsilon}{2} \right)^2 = \frac{3\epsilon^2}{4} \]
%     Therefore:
%     \[ \frac{1}{K} \sum_{k=K'}^{K-1} \left( \| \nabla L(\theta_k) \| - \frac{\epsilon}{2} \right)^2 \leq \frac{1}{K} \sum_{k=K'}^{K-1} \frac{3\epsilon^2}{4} \leq \frac{3\epsilon^2}{4} \]

%     \item \textbf{Combining and Simplifying:} Substituting back into the inequality from step 4:
%     \[ \frac{1}{K} \sum_{k=0}^{K'-1} \left( \| \nabla L(\theta_k) \| - \frac{\epsilon}{2} \right)^2 + \frac{1}{K} \sum_{k=K'}^{K-1} \frac{3\epsilon^2}{4} \leq \frac{2(L^* - L(\theta_0))}{K\eta} + \frac{3\epsilon^2}{4} \]
%     \[ \frac{1}{K} \sum_{k=0}^{K'-1} \left( \| \nabla L(\theta_k) \| - \frac{\epsilon}{2} \right)^2 \leq \frac{2(L^* - L(\theta_0))}{K\eta} \]
%     We know that for $k < K'$, that the gradient norm is at least greater than $\frac{\epsilon}{2}$, so we can say that $\left( \| \nabla L(\theta_k) \| - \frac{\epsilon}{2} \right)^2 > 0$ for $k < K'$. Thus, for sufficiently large $K$:
%     \[ \frac{1}{K} \sum_{k=0}^{K'-1} \left( \| \nabla L(\theta_k) \| - \frac{\epsilon}{2} \right)^2 \leq \frac{2(L^* - L(\theta_0))}{K\eta} \]
%     This implies that $K' \sim O(\frac{1}{K})$ since the other term does not decrease with $K$. Now we can rewrite the equation from step 4:
%     \[ \frac{1}{K} \sum_{k=0}^{K-1} \| \nabla L(\theta_k) \|^2 \leq \frac{2(L^* - L(\theta_0))}{K\eta} + \frac{3\epsilon^2}{4} + \frac{1}{K} \sum_{k=0}^{K'-1} \epsilon \| \nabla L(\theta_k) \| + \frac{1}{K} \sum_{k=K'}^{K-1} \epsilon \| \nabla L(\theta_k) \| \]
%     Since $K' \sim O(\frac{1}{K})$ and since we can assume that $\| \nabla L(\theta_k) \|$ is upper bounded by some constant in a compact domain, then the term $\frac{1}{K} \sum_{k=0}^{K'-1} \epsilon \| \nabla L(\theta_k) \|$ goes to 0 as $K$ goes to infinity. Further, $\frac{1}{K} \sum_{k=K'}^{K-1} \epsilon \| \nabla L(\theta_k) \| \leq \frac{1}{K} \sum_{k=K'}^{K-1} \left( \frac{1}{2} + \frac{\sqrt{3}}{2} \right) \epsilon^2 \leq \left( \frac{1}{2} + \frac{\sqrt{3}}{2} \right) \epsilon^2$
% \end{enumerate}

% \subsection{Result}

% For sufficiently large $K$, we have:

% \[ \frac{1}{K} \sum_{k=0}^{K-1} \| \nabla L(\theta_k) \|^2 \leq \frac{2(L^* - L(\theta_0))}{K\eta} + \left( \frac{5 + 2\sqrt{3}}{4} \right) \epsilon^2 \]

% \subsection{Interpretation}

% \begin{itemize}
%     \item The result is essentially the same as before, but the derivation is more careful about when the inequality $\| \nabla L(\theta_k) \| \leq \left( \frac{1}{2} + \frac{\sqrt{3}}{2} \right) \epsilon$ holds.
%     \item We still have a $\frac{1}{K}$ term that goes to zero as $K$ increases and a constant error term that depends on $\epsilon^2$.
%     \item The convergence is still to a neighborhood where the average squared gradient norm is influenced by $\epsilon$.
% \end{itemize}

% In conclusion, by carefully accounting for the conditions under which the gradient norm bound holds, we arrive at a similar convergence result for the sum of squared gradient norms in the proxy setting. The analysis highlights the role of the approximation error $\epsilon$ in determining the final convergence behavior.

\subsection{Second-Order Derivatives of Log-Likelihood (Details)}

Here, we provide the detailed calculations for the second-order derivatives of the log-likelihood with respect to $\theta_i$ and $\theta_j$.

We start with the first-order derivative:

$$
\frac{\partial \mathcal{L}}{\partial \theta_i} = \frac{1}{2} \mathbf{y}^\top (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \text{tr}\left((\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right).
$$

Now, we differentiate again with respect to $\theta_j$. We will use the following matrix calculus identities:

*   $\frac{\partial}{\partial \theta} (\mathbf{A}^{-1}) = -\mathbf{A}^{-1} (\frac{\partial \mathbf{A}}{\partial \theta}) \mathbf{A}^{-1}$
*   $\frac{\partial}{\partial \theta} \text{tr}(\mathbf{A}\mathbf{B}) = \text{tr}(\frac{\partial \mathbf{A}}{\partial \theta} \mathbf{B}) + \text{tr}(\mathbf{A} \frac{\partial \mathbf{B}}{\partial \theta})$
*   $\frac{\partial}{\partial \theta} \log|\mathbf{A}| = \text{tr}(\mathbf{A}^{-1} \frac{\partial \mathbf{A}}{\partial \theta})$

Applying these identities, we obtain:

\begin{align*}
\frac{\partial^2 \mathcal{L}}{\partial \theta_i \partial \theta_j} &= \frac{1}{2} \mathbf{y}^\top \frac{\partial}{\partial \theta_j} \left[ (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \right] \mathbf{y} \\
&\quad - \frac{1}{2} \frac{\partial}{\partial \theta_j} \left[ \text{tr}\left((\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) \right]
\end{align*}

Let's compute each term separately. For the first term:

\begin{align*}
&\frac{\partial}{\partial \theta_j} \left[ (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \right] \\
&= \frac{\partial}{\partial \theta_j} [(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}] \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} + (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \frac{\partial}{\partial \theta_j} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \\
&\quad + (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) \frac{\partial}{\partial \theta_j} [(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}] \\
&= -(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \\
&\quad + (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial^2 \mathbf{K}_\theta}{\partial \theta_i \partial \theta_j}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \\
&\quad - (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}
\end{align*}

For the second term:

\begin{align*}
&\frac{\partial}{\partial \theta_j} \left[ \text{tr}\left((\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) \right] \\
&= \text{tr}\left( \frac{\partial}{\partial \theta_j} [(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}] \frac{\partial \mathbf{K}_\theta}{\partial \theta_i} \right) + \text{tr}\left( (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \frac{\partial}{\partial \theta_j} \left[\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right] \right) \\
&= -\text{tr}\left( (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right) (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \frac{\partial \mathbf{K}_\theta}{\partial \theta_i} \right) + \text{tr}\left( (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \frac{\partial^2 \mathbf{K}_\theta}{\partial \theta_i \partial \theta_j} \right)
\end{align*}

Putting everything together, we get the expression for the second-order derivative:

\begin{align*}
\frac{\partial^2 \mathcal{L}}{\partial \theta_i \partial \theta_j} &= -\frac{1}{2} \mathbf{y}^\top (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \left[ \frac{\partial^2 \mathbf{K}_\theta}{\partial \theta_i \partial \theta_j} - \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right)(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right) - \left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right)(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right) \right] (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1} \mathbf{y} \\
&\quad + \frac{1}{2} \text{tr}\left[ (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_i}\right)(\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_\theta}{\partial \theta_j}\right) \right] - \frac{1}{2} \text{tr}\left[ (\mathbf{K}_\theta + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial^2 \mathbf{K}_\theta}{\partial \theta_i \partial \theta_j}\right) \right].
\end{align*}

\subsection{Appendix B: Simplification of the Quadratic Form (Details)}

Here, we provide the detailed algebraic manipulations to arrive at the expression involving the trace. We start with the quadratic form:
Substituting the expression for the second-order derivative and evaluating at $\theta=\theta^*$, we get:

\begin{align*}
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} 
&= -\mathbb{E}_{P^*} \left[ \sum_{i=1}^2 \sum_{j=1}^2 v_i v_j \frac{\partial^2 \mathcal{L}}{\partial \theta_i \partial \theta_j}\bigg|_{\theta = \theta^*} \right].
\end{align*}

Now, we substitute the expression for the second-order derivatives and use the fact that $\mathbb{E}_{P^*}[\mathbf{y}\mathbf{y}^\top | \mathbf{X}] = \mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I}$:

\begin{align*}
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} &= \frac{1}{2} \mathbb{E}_{P^*_X} \left[ \sum_{i=1}^2 \sum_{j=1}^2 v_i v_j \left( \text{tr}\left[ (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_i}\right) (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \left(\frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_j}\right) \right] - \text{tr}\left[ (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \frac{\partial^2 \mathbf{K}_{\theta^*}}{\partial \theta_i \partial \theta_j} \right] \right. \right. \\
&\quad \left. \left. + \text{tr}\left[ (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \left\{ \frac{\partial^2 \mathbf{K}_{\theta^*}}{\partial \theta_i \partial \theta_j} - \left(\frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_i}\right)(\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_j}\right) - \left(\frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_j}\right)(\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1}\left(\frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_i}\right) \right\} \right] \right) \right].
\end{align*}

After simplifying and rearranging terms, we get:

\begin{align*}
\mathbf{v}^\top \mathbf{I}(\theta^*) \mathbf{v} &= \frac{1}{2} \mathbb{E}_{P^*_X} \left[ \text{tr}\left( (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \left( \sum_{i=1}^2 v_i \frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_i} \right) (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \left( \sum_{j=1}^2 v_j \frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_j} \right) \right) \right] \\
&= \frac{1}{2} \mathbb{E}_{P^*_X}\left[ \text{tr}\left( (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \mathbf{A} (\mathbf{K}_{\theta^*} + \sigma^2 \mathbf{I})^{-1} \mathbf{A} \right) \right],
\end{align*}

where $\mathbf{A} = \sum_{i=1}^2 v_i \frac{\partial \mathbf{K}_{\theta^*}}{\partial \theta_i}$. This is the expression used in the proof.



% We formalize the intuition that a good initialization for gradient ascent on the SKI log-likelihood, combined with a sufficiently accurate SKI approximation, leads to a SKI MLE that is close to the true MLE. We achieve this by first bounding the difference in partial derivatives between the true and SKI log-likelihoods, assuming that the partial derivatives themselves are SPD kernels. Then, under the assumption of one point strong concavity of the true log-likelihood at the true parameter, we show that if the initialization is close to the true MLE, the resulting SKI MLE will also be close to the true MLE. Finally, we show that for the Gaussian RBF kernel, for the signal variance parameter 1) the derivative of the kernel with respect to it is itself an SPD kernel 2) local strong concavity holds when learning it while fixing the lengthscale parameter. We leave the highly challenging case of the lengthscale to future work. 

\end{document}
