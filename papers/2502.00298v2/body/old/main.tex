 %%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{thmtools} 
\usepackage{thm-restate}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\jmei}[1]{\textcolor{magenta}{#1}}
\newcommand{\amnote}[1]{\textcolor{red}{#1}}
% Use the following line for the initial blind version submitted for review:
\newtheorem{claim}{Claim}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{theorem}{Theorem}
\usepackage{icml2021}


% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Error Bounds for Structured Kernel Interpolation}% via Multivariate Polynomial Interpolation Analysis}

\begin{document}

\twocolumn[
\icmltitle{Error Bounds for Structured Kernel Interpolation}% via Multivariate Polynomial Interpolation Analysis}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aeiau Zzzz}{equal,to}
\icmlauthor{Bauiu C.~Yyyy}{equal,to,goo}
\icmlauthor{Cieua Vvvvv}{goo}
\icmlauthor{Iaesut Saoeu}{ed}
\icmlauthor{Fiuea Rrrr}{to}
\icmlauthor{Tateu H.~Yasehe}{ed,to,goo}
\icmlauthor{Aaoeu Iasoh}{goo}
\icmlauthor{Buiui Eueu}{ed}
\icmlauthor{Aeuia Zzzz}{ed}
\icmlauthor{Bieea C.~Yyyy}{to,goo}
\icmlauthor{Teoau Xxxx}{ed}
\icmlauthor{Eee Pppp}{ed}
\end{icmlauthorlist}

\icmlaffiliation{to}{Department of Computation, University of Torontoland, Torontoland, Canada}
\icmlaffiliation{goo}{Googol ShallowMind, New London, Michigan, USA}
\icmlaffiliation{ed}{School of Computation, University of Edenborrow, Edenborrow, United Kingdom}

\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}
\onecolumn

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Structured Kernel Interpolation (SKI) \cite{wilson2015kernel} helps scale Gaussian Processes (GPs) to large datasets by approximating the kernel matrix via interpolation at inducing points, achieving linear computational complexity.  However, SKI lacks rigorous theoretical error analysis. This paper addresses this gap: we establish error bounds for the SKI kernel and its Gram matrix, examine their effect on hyperparameter estimation, and evaluate the resulting error in posterior means and variances. Crucially, using convolutional cubic interpolation, we identify two dimensionality regimes governing the trade-off between SKI Gram matrix spectral norm error and computational complexity. For $d \leq 3$, linear time is achievable for \textit{any} error tolerance, provided the sample size is sufficiently large. For $d > 3$, the error must \textit{increase} with sample size to maintain linear time. Our analysis provides key insights into SKI's scalability-accuracy trade-offs, establishing precise conditions for achieving linear-time GP inference with controlled approximation error for the Gram matrix, optimization and posterior inference.

% Structured Kernel Interpolation (SKI) \cite{wilson2015kernel} helps scale Gaussian Processes (GPs) to large datasets by approximating the kernel matrix via interpolation at inducing points, leading to a computational complexity of $O(n+m\log m)$, where $n$ is the sample size and $m$ is the number of inducing points. Despite its widespread use, SKI lacks rigorous theoretical error analysis. In particular, for a given error tolerance for the approximate kernel matrix, how many inducing points $m$ do we need, and will $m\log m=O(n)$ hold so that the computational complexity is linear as claimed? This paper addresses this gap by deriving error bounds for the SKI kernel approximation and analyzing the impact on GP training and inference. We: (1) establish error bounds for the SKI kernel and its gram matrix; (2) examine how these errors affect kernel hyperparameter estimation; (3) evaluate the error of SKI GP posterior means and  variances. Interestingly, when using convolutional cubic interpolation, we identify two non-obvious dimensionality regimes relating SKI gram matrix spectral norm error to computational complexity. For $d\leq 3$, as long as the sample size is sufficiently large, we maintain linear time for \textit{any} error tolerance. For $d>3$, the error must \textit{increase} with the sample size to maintain our guarantee of linear time. Our theoretical analysis thus provides crucial insights into the scalability-accuracy trade-offs of SKI and how they change with the feature dimensionality, establishing precise conditions under which linear-time GP inference can be achieved while maintaining controlled approximation error.
% \jmei{It looks like the regime boundary in general would end up being $d\leq L-1$, so maybe we could further explicitly link polynomial degree to the dimensionality, error, and complexity. Not sure if there's a generalization of Keys' result that leys us easily do this though.}
\end{abstract}

\section{Introduction}\label{sec:introduction}
\input{body/01intro}

\section{Related Work}\label{sec:related}
\input{body/02related}

\section{Gaussian Processes, Structured Kernel Interpolation and Convolutional Cubic Interpolation}\label{sec:ski-background}
\input{body/03background}


\section{Important Quantities}\label{sec:important-quantities}
\input{body/04important-quantities}

\section{Gaussian Processes Applications}\label{sec:gp-applications}
\input{body/05gp-applications}


\section{Discussion}\label{sec:discussion}


In this paper, we provided theoretical analysis for structured kernel interpolation. In particular, we analyzed the error of tensor-product cubic convolutional interpolation, showed the elementwise SKI kernel error and how that propagates to spectral norm error of the SKI gram and cross-kernel matrices, and showed how this impacts achieving a specific error in linear time. We then analyzed kernel hyperparameter estimation in Gaussian processes, showing that gradient ascent has an iterate approach a ball around a stationary point with size quadratic in the sample size.


\bibliography{main}
\bibliographystyle{icml2021}


\appendix


\section{Auxiliary Technical Results}
\input{body/appendices/01auxiliary}
\section{Proofs Related to Important Quantities}
\input{body/appendices/04important-quantities}

\section{Proofs Related to Gaussian Process Applications}
\input{body/appendices/05gp-applications}

\end{document}
