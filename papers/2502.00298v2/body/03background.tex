
This section provides background on Gaussian Processes (GPs) and two key techniques for enabling scalable inference: Structured Kernel Interpolation (SKI) and Convolutional Cubic Interpolation. SKI \cite{wilson2015kernel} addresses GPs scalability issue by approximating the kernel matrix through interpolation on a set of inducing points, leveraging the efficiency of convolutional kernels. In particular, cubic convolutional kernels, as detailed in \cite{keys1981cubic}, provide a smooth and accurate interpolation scheme that forms the foundation of the SKI framework. In this paper, we focus on this cubic case as it is used by SKI. Future work may extend this to study higher-order interpolation methods. Here, we formally define these concepts and lay the groundwork for the subsequent error analysis.

\subsection{Gaussian Processes}\label{sec:gp-background}
A Gaussian process $\xi\sim \textrm{GP}(\nu,k_\theta)$ is a stochastic process $\{\xi(\textbf{x})\}_{\textbf{x}\in \mathcal{X}}$ such that any finite subcollection $\{\xi(\textbf{x}_i)\}_{i=1}^n$ is multivariate Gaussian distributed. We assume that we have index locations $\textbf{x}_i\in \mathbb{R}^d$ and observations $y_i\in \mathbb{R}$ for a set of training points $i=1,\ldots,n$ such that
\begin{align*}
y_i&=\xi(\textbf{x}_i)+\epsilon_i,\epsilon_i\sim \mathcal{N}(0,\sigma^2).
\end{align*}
where $\nu:\mathcal{X}\rightarrow\mathbb{R}$, $k_\theta:\mathcal{X}\times \mathcal{X}\rightarrow\mathbb{R}$ are the prior mean and covariance functions, respectively, with $k$ an SPD kernel with hyperparameters $\theta$. Given $\{\textbf{x}_i,y_i\}_{i=1}^n$ we are primariliy interested in two tasks: 1) estimate hyperparameters $\boldsymbol{\theta}\in \Theta\subseteq \mathbb{R}^p$ of kernel $k_\theta$ (e.g. RBF kernel) 2) do Bayesian inference for the posterior mean $\boldsymbol{\mu}(\cdot)\in \mathbb{R}^{T}$ and covariance $\boldsymbol{\Sigma}(\cdot)\in \mathbb{R}^{T\times T}$ at a set of test points $\{\textbf{x}_t\}_{t=1}^T$. Assuming $\nu\equiv 0$ (a mean-zero GP prior), for 1), one maximizes the log-likelihood
\begin{align}
    \mathcal{L}(\boldsymbol{\theta};X) &= -\frac{1}{2}\textbf{y}^\top (\textbf{K}+\sigma^2 \textbf{I})^{-1}\textbf{y}\nonumber\\
    &\qquad-\frac{1}{2}\log \vert \textbf{K}+\sigma^2 \textbf{I}\vert -\frac{n}{2}\log (2\pi)\label{eqn:log-likelihood}
\end{align}
to find $\boldsymbol{\theta}\in \mathcal{D}\subseteq \Theta$ where $\textbf{K}\in \mathbb{R}^{n\times n}$ with entries $\textbf{K}_{ij}=k_\theta(\textbf{x}_i,\textbf{x}_j)$ is the Gram matrix for the training dataset. For 2), given the kernel function and known observation variance $\sigma^2$, the posterior mean and covariance are given by
\begin{align}
\boldsymbol{\mu}(\cdot) &= \mathbf{K}_{\cdot, \mathbf{X}}\left(\mathbf{K}+\sigma^{2} \mathbf{I}\right)^{-1} \mathbf{y}\label{eqn:posterior-mean}\\
\boldsymbol{\Sigma}(\cdot) &= \textbf{K}_{\cdot,\cdot}+\sigma^2I-\textbf{K}_{\cdot,\textbf{X}} (\textbf{K}+\sigma^2 I)^{-1}\textbf{K}_{\textbf{X},\cdot}\label{eqn:posterior-covariance}
\end{align}
where $\textbf{K}_{\cdot,\textbf{X}}\in \mathbb{R}^{T\times n}$ is the matrix of kernel evaluations between test and training points. Intuitively, the GP prior represents our belief about all possible functions before seeing any data. When we observe data points, the posterior represents our updated belief - it gives higher probability to functions that fit our observations while maintaining the smoothness properties encoded in the kernel. The posterior mean can be viewed as a weighted average of these functions, where the weights depend on how well each function fits the data and satisfies the prior assumptions. The posterior variance indicates our remaining uncertainty - it is smaller near observed points where we have more confidence, and larger in regions far from our data.

A challenge is that, between the log-likelihood and the posteriors, one first needs to compute the action of the inverse of the regularized Gram matrix, $(\textbf{K}+\sigma^2 \textbf{I})^{-1}\textbf{y}$. Second, one needs to compute the log-determinant $\log \vert \textbf{K}+\sigma^2 \textbf{I}\vert$. These are both $O(n^3)$ computationally and $O(n^2)$ memory.

\subsection{Structured Kernel Interpolation}\label{subsec:ski}
Structured kernel interpolation \citep{wilson2015kernel} or (SKI) addresses these computational and memory bottlenecks by approximating the original kernel function \(k_\theta:\mathcal{X}\times \mathcal{X}\rightarrow \mathbb{R},\mathcal{X}\subseteq\mathbb{R}^d\) by interpolating kernel values at a chosen set of inducing points \(\mathbf{U}=\left(\begin{array}{c}
     \textbf{u}_1^\top\\
     \vdots\\
     \textbf{u}_m^\top
\end{array}\right)\in \mathbb{R}^{m\times d}\). The approximate kernel function \(\tilde{k}:\mathcal{X}\times \mathcal{X}\rightarrow\mathbb{R}\) can be expressed as:
\[
\tilde{k}(\mathbf{x}, \mathbf{x}') = \mathbf{w}(\mathbf{x})^{\top} \mathbf{K}_{\mathbf{U}} \mathbf{w}(\mathbf{x}')
\]
where \(\mathbf{K}_{\mathbf{U}}\in \mathbb{R}^{m\times m}\) is the kernel matrix computed on the inducing points, and \(\mathbf{w}(\mathbf{x}),\mathbf{w}(\mathbf{x}')\in \mathbb{R}^m\) are vectors of interpolation weights using (usually cubic) convolutional kernel $u:\mathbb{R}\rightarrow \mathbb{R}$ for the points $\textbf{x}$ and $\textbf{x}'$, respectively. One can then form the SKI Gram matrix $\tilde{\textbf{K}}=\textbf{W}\textbf{K}_\textbf{U}\textbf{W}^\top$ with $\textbf{W}$ a \textit{sparse} matrix of $L$ interpolation weights per row for a polynomial of degree $L-1$. By exploiting the sparsity of each row, for stationary kernels this leads to a computational complexity of $O(nL+m\log m)$ and a memory complexity of $O(nL+m)$.

In order to learn kernel hyperparameters, one can maximize the SKI approximation to the log-likelihood (henceforth the SKI log-likelihood)
\begin{align*}
    \tilde{\mathcal{L}}(\boldsymbol{\theta};X)
&=-\frac{1}{2}\textbf{y}^\top (\tilde{\textbf{K}}+\sigma^2 \textbf{I})^{-1}\textbf{y}\\
&\qquad-\frac{1}{2}\log \vert \tilde{\textbf{K}}+\sigma^2 \textbf{I}\vert -\frac{n}{2}\log (2\pi)
\end{align*}
Given the SKI kernel $\tilde{k}:\mathcal{X}\times \mathcal{X}\rightarrow \mathbb{R}$ with learned hyperparameters, one can do posterior inference of the SKI approximations to the mean $\tilde{\boldsymbol{\mu}}(\cdot)$ and covariance $\tilde{\boldsymbol{\Sigma}}(\cdot)$ at a set of $T$ test points $\cdot$ as
\begin{align*}
    \tilde{\boldsymbol{\mu}}(\cdot)&=\tilde{\mathbf{K}}_{\cdot, \mathbf{X}}\left(\tilde{\mathbf{K}}+\sigma^{2} \mathbf{I}\right)^{-1} \mathbf{y}\\
    \tilde{\boldsymbol{\Sigma}}(\cdot)&=\tilde{\textbf{K}}_{\cdot,\cdot}+\sigma^2I-\tilde{\textbf{K}}_{\cdot,\textbf{X}} (\tilde{\textbf{K}}+\sigma^2 I)^{-1}\tilde{\textbf{K}}_{\textbf{X},\cdot}
\end{align*}
where $\tilde{\textbf{K}}_{\cdot,\textbf{X}}\in \mathbb{R}^{T\times n}$ is the matrix of SKI kernels between test points and training points and $\tilde{\textbf{K}}_{\cdot,\cdot}\in \mathbb{R}^{T\times T}$ is the SKI Gram matrix for the test points. Going forward, we may write $\mathcal{L}(\boldsymbol{\theta})$ and $\tilde{\mathcal{L}}(\boldsymbol{\theta})$, dropping the explicit dependence on the data but implying it.


\subsection{Convolutional Cubic Interpolation}\label{subsec:convolutional-cubic-interpolation}

Convolutional cubic interpolation \citep{keys1981cubic} gives a continuously differentiable interpolation of a function given its values on a regular grid, where its cubic convolutional kernel is a piecewise polynomial function designed to ensure continuous differentiability. We formalize this using the definitions of the cubic convolutional interpolation kernel and the tensor-product cubic convolutional function below. We also define an upper bound for the sum of weights for each dimension, which will be a useful constant going forward. Such a bound will exist for all continuous stationary kernels vanishing at infinity.

\begin{definition}\label{def:cubic-interpolation-kernel}

The cubic convolutional interpolation kernel $u:\mathbb{R}\rightarrow\mathbb{R}$ is given by
$$
u(s)\equiv\begin{cases}
1, & s=0\\
\frac{3}{2}\vert s\vert^3-\frac{5}{2}\vert s\vert^2+1, & 0<\vert s\vert <1\\
-\frac{1}{2}\vert s\vert^3+\frac{5}{2}\vert s\vert^2-4\vert s\vert+2, & 1<\vert s\vert<2\\
0, & \text{otherwise}
\end{cases}
$$
\end{definition}

\begin{definition}\label{def:tensor-product-cubic-interpolation-alt1}
Let $\mathbf{x} = (x_1, x_2, ..., x_d) \in \mathbb{R}^d$ be a d-dimensional point. Let $f:\mathbb{R}^d \rightarrow \mathbb{R}$ be a function defined on a regular grid with spacing $h$ in each dimension. Let $\mathbf{c_x}$ denote the grid point closest to $\mathbf{x}$.  The tensor-product cubic convolutional interpolation function $g:\mathbb{R}^d\rightarrow \mathbb{R}$ is defined as:
{\footnotesize
\begin{align*}
    g(\mathbf{x}) \equiv \sum_{\mathbf{k} \in \{-1, 0, 1, 2\}^d} f(\mathbf{c_x} + h\mathbf{k}) \prod_{j=1}^d u\left(\frac{x_j - (\mathbf{c_x})_j - h k_j}{h}\right)
\end{align*}
}
where $u$ is the cubic convolutional interpolation kernel and $\mathbf{k} = (k_1, \ldots, k_d)$ is a vector of integer indices.
\end{definition}

\begin{definition}\label{def:sum-weight-upper-bound}
    Given an interpolation kernel $u:\mathbb{R}\rightarrow\mathbb{R}$ and a fixed $n\in \mathbb{N}$, let $c>0$ be an upper bound such that, for any $x\in  \mathbb{R}$ and a set of data points $\{x_i\}_{i=1}^n \subset \mathbb{R}$,
$$
\sum_{i=1}^n \left\vert u\left(\frac{x - x_i}{h}\right) \right\vert\leq c,
$$
\end{definition}

Going forward, we always assume that we use convolutional cubic polynomial interpolation, so that $L=4$ as in \cite{wilson2015kernel}, but that we may vary the number of inducing points $m$. In particular, we will analyze how the number of inducing points affects error for different terms of interest, and how to choose the number of inducing points.