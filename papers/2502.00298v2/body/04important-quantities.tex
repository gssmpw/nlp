

This section derives bounds for key quantities in Structured Kernel Interpolation (SKI). Section \ref{subsubsec:elementwise-error} provides a bound on the elementwise error between the true kernel and its SKI approximation. In Section \ref{subsubsec:spectral-norm-error}, we extend this to the spectral norm error of the SKI approximation for the training Gram matrix and train-test kernel matrix. Finally, in section \ref{subsec:linear-time} we present conditions on the number of inducing points for achieving specific error tolerance $\epsilon>0$ and error needed to guarantee linear time complexity, noting linear time always holds for $d\leq 3$ with sufficiently large samples.


\subsection{Error Bounds for the Ski Kernel}\label{subsec:ski-kernel-error-bounds}
This subsection analyzes the error introduced by the SKI approximation of the kernel function. We start by extending the analysis of \cite{keys1981cubic} to the multivariate setting, deriving error bounds for multivariate cubic convolutional polynomial interpolation. We then use these to derive the elementwise error for the SKI approximation \(\tilde{k}(\textbf{x},\textbf{x}')\). We next apply these elementwise bounds to derive spectral norm error bounds for SKI kernel matrices, which will be crucial for understanding the downstream effects of the SKI approximation on Gaussian process hyperparameter estimation and posterior inference.


\subsubsection{Elementwise}\label{subsubsec:elementwise-error}


Our first lemma shows that multivariate tensor-product cubic convolutional interpolation retains error cubic in the grid spacing of \cite{keys1981cubic}, which is equivalent to $m^{-3/d}$ decay with the number of inducing points $m$, but exhibits exponential error growth with increasing dimensions. The proof uses induction on dimensions, starting with the 1D case from Keys.

\begin{restatable}{lemma}{tensorproductinterpolationerror}\label{lemma:tensor-product-interpolation-error}
The error of tensor-product cubic convolutional interpolation is $O(c^d h^3)$, or equivalently $O\left(\frac{c^d}{m^{3/d}}\right)$.
\end{restatable}
\begin{proof}
    See Appendix \ref{sec:proof-tensor-product-interpolation}.
\end{proof}


The following Lemma allows us to bound the absolute difference between the true and SKI kernels \textit{uniformly} with the same big-$O$ error as for the underlying interpolation itself. The proof uses the the triangle inequality to decompose the error into two parts: the first is the error from a single interpolation, while the second is the error of the nested interpolations.


\begin{restatable}{lemma}{skikernelelementwiseerror}\label{lemma:ski-kernel-elementwise-error}
    Let $\delta_{m,L}$ be the interpolation error for $m$ inducing points and interpolation degree $L-1$. The SKI kernel $\tilde{k}:\mathcal{X}\times \mathcal{X}\rightarrow \mathbb{R}$ with grid spacing $h$ in each dimension has error
    \begin{align*}
        \vert k(\textbf{x},\textbf{x}')-\tilde{k}(\textbf{x},\textbf{x}')\vert&= \delta_{m,L}+\sqrt{L}c^d\delta_{m,L}\\
        &=O\left(\frac{c^{2d}}{m^{3/d}}\right).
    \end{align*}

\end{restatable}
\begin{proof}
    See Appendix \ref{sec:proof-ski-kernel-elementwise-error}
\end{proof}


\subsubsection{Spectral Norm Error}\label{subsubsec:spectral-norm-error}

We now transition from elementwise error bounds to spectral norm bounds for the SKI gram matrix's approximation error, finding that it grows linearly with the sample size and exponentially with the dimension and decays as $m^{-3/d}$ with the number of inducing points. This is both of independent interest but will also be important to nearly all downstream analysis for estimation and inference. We also provide a bound on the spectral norms of the SKI train/test kernel matrix's approximation error. This is useful when analyzing the GP posterior parameter error.% Finally, we bound the error of the SKI gram matrix's regularized inverse, which is useful for analyzing the SKI MLE's error and the GP posterior parameter error.

For this next lemma we will express it both in the general interpolation setting and again give the specific big-$O$ for convolutional cubic interpolation, but going forward we sometimes only show the latter setting in the main paper and derive the general settings in the proof. In particular, \emph{\textbf{whenever we use big $O$-notation}} we are assuming convolutional cubic interpolation.
\begin{restatable}{proposition}{spectralnorm}\label{prop:spectral-norm}
    For the SKI approximation $\tilde{\textbf{K}}$ of the true Gram matrix $\textbf{K}$, we have
    \begin{align*}
        \Vert \textbf{K}-\tilde{\textbf{K}}\Vert_2&=n \left(\delta_{m,L}+\sqrt{L} c^d\delta_{m,L}\right)\\
        &\equiv \gamma_{n,m,L}\\
        &= O\left(\frac{nc^{2d}}{m^{3/d}}\right)
    \end{align*}
\end{restatable}
\begin{proof}
    See Appendix \ref{sec:proof-spectral-norm}
\end{proof}

\begin{restatable}{lemma}{testtrainkernelmatrixerror}\label{lemma:test-train-kernel-matrix-error}
    Let \(\textbf{K}_{\cdot,\textbf{X}}\in \mathbb{R}^{T\times n}\) be the matrix of kernel evaluations between \(T\) test points and \(n\) training points, and let \(\tilde{\textbf{K}}_{\cdot,\textbf{X}}\in \mathbb{R}^{T\times n}\) be the corresponding SKI approximation. Then
    \begin{align*}
        \Vert \textbf{K}_{\cdot,\textbf{X}}-\tilde{\textbf{K}}_{\cdot,\textbf{X}}\Vert_2=O\left(\frac{\max(n,T)c^{2d}}{m^{3/d}}\right)
    \end{align*}

\end{restatable}
\begin{proof}
    See Appendix \ref{sec:proof-test-train-kernel-matrix-error}.
\end{proof}


\subsection{Achieving Errors in Linear Time}\label{subsec:linear-time}

Here, we show how many inducing points $m$ are sufficient to achieve a desired error tolerance $\epsilon > 0$ for the SKI Gram matrix when using cubic convolutional interpolation. Based on the Theorem, we should grow the number of inducing points at an $n^{d/3}$ rate. We then show corollaries describing 1) how $\epsilon$ and $m$ must grow to maintain linear time 2) how the dimension affects whether the error must grow with the sample size to ensure linear time SKI.

The following theorem shows the number of inducing points that will guarantee a Gram matrix error tolerance. It says that the number of inducing points should grow as $n^{d/3}$ to achieve a fixed error. The proof starts by lower bounding the desired spectral norm error with the upper bound on the actual spectral norm error derived in Proposition \ref{prop:spectral-norm}: this is a sufficient condition for the desired spectral norm error to hold. It then relates the number of inducing points to the grid spacing in the SKI approximation, assuming a regular grid with equal spacing in each dimension. By substituting this relationship into the sufficient condition, the proof derives the sufficient number of inducing points to control error. 

\begin{restatable}{theorem}{inducingpointscountalt}\label{thm:inducing-points-count-alt}
    If the domain is $[-D, D]^d$, then to achieve a spectral norm error of $\Vert \textbf{K} - \tilde{\textbf{K}} \Vert_2 \leq \epsilon$, it is sufficient to choose the number of inducing points $m$ such that:
    $$
    m = \left( \frac{n}{\epsilon} (1 + 2c^d) K' (8 c^{2d} D^3) \right)^{d/3}
    $$
    for some constant $K'$ that depends only on the kernel function and the interpolation scheme.
\end{restatable}
\begin{proof}
    See Appendix \ref{sec:proof-inducing-points-count-alt}.
\end{proof}
This result shows that the number of inducing points should grow
\begin{itemize}
\item Sub-linearly with the sample size and decrease in error for $d<3$, linearly for $d=2$ and super-linearly for $d>3$. Thus, as we want a tighter error tolerance or have more observations we need more inducing points, but at very different rates depending on the dimensionality.
\item Linearly with the volume of the domain $(2D)^d$. Thus, if our observations are concentrated in a small region and we select an appropriately sized domain to cover it we need fewer inducing points.
\item Exponentially with the dimension $d$, as we have a $c^{2d}$ term.
\end{itemize}

The next Corollary establishes a condition on the spectral norm error, $\epsilon$, that ensures linear-time $O(n)$ computational complexity for SKI. The core idea is that $\epsilon$ should be such that if we choose $m$ based on the previous Theorem, $m=O(n/\log n)$ and thus $m\log m=O(n)$. 
    
\begin{restatable}{corollary}{corlineartime}\label{cor:linear-time}
    If
\begin{equation} \label{eq:epsilon_condition}
\epsilon \geq \frac{(1 + 2c^d) K' 8 c^{2d} D^3}{C^{3/d}} \cdot \frac{n (\log n)^{3/d}}{n^{3/d}}
\end{equation}
for some constants $K,C>0$ that depend on the kernel function and the interpolation scheme and we choose $m>0$ based on the previous theorem, then we have both $\Vert \textbf{K}-\tilde{\textbf{K}}\Vert_2\leq \epsilon$ and
SKI computational complexity of $O(n)$.
\end{restatable}
\begin{proof}
    See Appendix \ref{proof:cor-linear-time}.
\end{proof}

Interestingly, the previous Theorem and Corollary implies a fundamental difference between two dimensionality regimes. For $d \leq 3$, the choice of $m$ required for a fixed $\epsilon$ grows more slowly than $n/\log n$. This means that for any fixed $\epsilon > 0$, SKI with cubic interpolation is guaranteed to be a linear-time algorithm for sufficiently large $n$. In contrast, for $d > 3$, the choice of $m$ required for a fixed $\epsilon>0$ eventually grows faster than $n/\log n$. Thus, to maintain linear-time complexity for $d > 3$ and the guarantees from Theorem \ref{thm:inducing-points-count-alt}, we must allow the error $\epsilon$ to increase with $n$. This demonstrates that the curse of dimensionality impacts the scalability of SKI, making it challenging to achieve both high accuracy and linear-time complexity in higher dimensions. The next corollary formalizes this.
\begin{corollary}
    For $d\leq 3$, for any $\epsilon>0$, Corollary \ref{cor:linear-time} holds for any $n$ sufficiently large, so that choosing $m$ based on Theorem \ref{thm:inducing-points-count-alt} is sufficient to achieve linear complexity. For $d>3$, $\epsilon$ must grow with the sample size to maintain linear complexity.
\end{corollary}
\begin{proof}
    For $d\leq3 $, the RHS of Eqn. \ref{eq:epsilon_condition} decreases with $n$ with limit $0$ and thus for sufficiently large sample size will be $\leq \epsilon$, satisfying the conditions to guarantee small error and linear time. For $d>3$, the RHS of Eqn. \ref{eq:epsilon_condition} grows with $n$, so that $\epsilon$ must grow to satisfy the conditions for the guarantee. 
\end{proof}








% \subsubsection{Log Determinant}\label{subsubsec:log-determinant}

% \begin{lemma}\label{lemma:log-determinant-error}
%     Let \(\mathbf{\tilde{K}}\) be the SKI approximation of the kernel matrix \(\mathbf{K}\), and let \(\sigma^2\) be the regularization parameter. The error in the log-determinant of the regularized SKI matrix can be bounded as follows:
% \[
% \left|\log \det(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}) - \log \det(\mathbf{K} + \sigma^2 \mathbf{I})\right| \leq \frac{\gamma_{n, m, L}}{\sigma^2}.
% \]



% \end{lemma}

% \begin{proof}
%     We begin by applying the functional Mean Value Theorem for matrices, which implies that for some matrix \(Z = (\mathbf{\tilde{K}} + \alpha (\mathbf{K} - \mathbf{\tilde{K}})) + \sigma^2 \mathbf{I}\) with \(\alpha \in [0, 1]\),
% \[
% |f(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}) - f(\mathbf{K} + \sigma^2 \mathbf{I})| \leq \|f'(Z)\|_2 \|\mathbf{K} - \mathbf{\tilde{K}}\|_2
% \]
% where \(f(X) = \log |X|\) and thus \(f'(X) = X^{-\top}\).

% Thus,
% \[
% |\log \det(\mathbf{\tilde{K}} + \sigma^2 \mathbf{I}) - \log \det(\mathbf{K} + \sigma^2 \mathbf{I})| \leq  \left\| (\mathbf{\tilde{K}} + \alpha (\mathbf{K} - \mathbf{\tilde{K}}) + \sigma^2 I)^{-1}  \right\|_2  ||\mathbf {K} -  {\tilde {\mathbf {K}}}||_2\label{eqn:initial-log-det-bound}
% \]

% Using Claim 1, which bounds the eigenvalues of a convex combination of matrices, we have:
% \[
% \lambda_{\min}(Z) =  (\lambda_{\min} (\mathbf{\tilde{K}})+ (1-\alpha)\lambda_{\min} (\mathbf {K}))+\sigma^2   > 0
% \]

% Therefore since $\mathbf{\tilde{K}}$ and $(\mathbf {K}$ are SPD and thus have positive eigenvalues,
% \[
% \left\| (\mathbf{\tilde{K}} +  (1-\alpha)(\mathbf {K}-  {\tilde {\mathbf {K}}})+  {\sigma^2 I})^{-1}  )^{-1}\right\|_2   <   {\frac {1}{ {\sigma^2}}}
% \]

% Substituting these bounds into Eqn. \ref{eqn:initial-log-det-bound} gives:
% \[
% |\log \det (\tilde{\mathbf {K}}+ {\sigma^2 I})- \log \det ({\mathbf {K}}+ {\sigma^2 I})| <   {\frac {1}{ {\sigma^2}}} ||{\mathbf {K}-  {\tilde {\mathbf {K}}}}||_2 
% \leq   {\frac {1}{ {\sigma^2}}}  {\gamma_{n,m,L}}
% \]

% \end{proof}
