In this section, we address how SKI affects Gaussian Processes Applications. In Section \ref{sec:kernel-hyperparameter-estimation} we address how using the SKI kernel and log-likelihood affect hyperparameter estimation. In Section and posterior inference.

\subsection{Kernel Hyperparameter Estimation}\label{sec:kernel-hyperparameter-estimation}

\subsubsection{Difference in MLEs}

We formalize the intuition that a good initialization for gradient ascent on the SKI log-likelihood, combined with a sufficiently accurate SKI approximation, leads to a SKI MLE that is close to the true MLE. We achieve this by first bounding the difference in partial derivatives between the true and SKI log-likelihoods, assuming that the partial derivatives themselves are SPD kernels. Then, under the assumption of one point strong concavity of the true log-likelihood at the true parameter, we show that if the initialization is close to the true MLE, the resulting SKI MLE will also be close to the true MLE. Finally, we show that for the Gaussian RBF kernel, for the signal variance parameter 1) the derivative of the kernel with respect to it is itself an SPD kernel 2) local strong concavity holds when learning it while fixing the lengthscale parameter. We leave the highly challenging case of the lengthscale to future work. To start with, we define our notation. Let $\mathcal{X} \subset \mathbb{R}^d$ be the input domain and let $\Theta \subset \mathbb{R}^p$ be the domain of the kernel hyperparameters. Further, let $k_{\theta}: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ be a kernel function parameterized by $\theta \in \Theta$, and let $\tilde{k}_{\theta}: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ be its SKI approximation using $m$ inducing points and interpolation degree $L-1$.

We now state our assumptions: 1) one point strong concavity and gradient smoothness of the population log-likelihood 2) That the kernel's partial derivatives are themselves SPD kernels. 3) That the kernel is continuous differentiable. We then mention an equivalence between strong concavity and the Hessian having an upper bound on its smallest eigenvalue.

\begin{assumption}[One Point Strong Concavity of the Population Log-Likelihood]\label{assumption:strong-concavity}
The population log-likelihood $\bar{\mathcal{L}}(\theta) = \mathbb{E}_{P^*}[\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})]$ is strongly concave at $\theta^*$ with parameter $\mu > 0$ in a neighborhood $\mathcal{D}$ of the true hyperparameters $\theta^*$. That is, for all $\theta\in \mathcal{D}$,
$$
\bar{\mathcal{L}}(\theta) \leq \bar{\mathcal{L}}(\theta^*) + \nabla \bar{\mathcal{L}}(\theta^*)^\top (\theta - \theta^*) - \frac{\mu}{2} \| \theta - \theta^* \|_2^2.
$$
\end{assumption}
\begin{assumption}
    (One Point Gradient Smoothness of the Population Log-Likelihood) $\| \nabla \bar{\mathcal{L}}(\theta) - \nabla \bar{\mathcal{L}}(\theta^*) \|_2 \leq L \| \theta - \theta^* \|_2$ for some $L>0$.
\end{assumption}
\begin{assumption}
    (SPD Kernel Partials) For each $i \in \{1, ..., p\}$, the partial derivative of $k_{\theta}$ with respect to a hyperparameter $\theta_i$, denoted as $k'_{\theta_i}(x, x') = \frac{\partial k_{\theta}(x, x')}{\partial \theta_i}$, is also a valid SPD kernel.
\end{assumption}
\begin{assumption}
    (Kernel Smoothness) $k_\theta(x,x')$ is $C^1$ in $\theta$. That is, for each $i \in \{1, ..., p\}$, $k'_{\theta_i}(x, x') = \frac{\partial k_{\theta}(x, x')}{\partial \theta_i}$ exists and is Lipschitz for $\theta\in \mathcal{D}$.
\end{assumption}


\begin{proposition}\label{prop:strong_concavity_hessian}
    A function is strongly concave if and only if the Hessian matrix $H_f(\theta)$ satisfies $H_f(\theta) \preceq -\mu I$ for all $\theta \in \mathcal{D}$, where $\preceq$ denotes the negative semidefinite ordering.
\end{proposition}
\begin{proof}
    See \cite{boyd2004convex}.
\end{proof}

Here we argue that we can apply the same elementwise error we derived previously to the SKI partial derivatives.

\begin{restatable}{lemma}{skikernelderivativeerrorkernel}[Bound on Derivative of SKI Kernel Error using Kernel Property of Derivative]
\label{lemma:ski_kernel_derivative_error_kernel}
 Let $\tilde{k}'_{\theta}(x,x')$ be the SKI approximation of $k'_{\theta_i}(x,x')$, using the same inducing points and interpolation scheme as $\tilde{k}_{\theta}$. Then, for all $x, x' \in \mathcal{X}$, all $\theta \in \Theta$, the following inequality holds:

\begin{equation*}
\left\vert \frac{\partial k_{\theta}(x,x')}{\partial \theta_i}-\frac{\partial \tilde{k}_{\theta}(x,x')}{\partial \theta_i}\right\vert = \left\vert k'_{\theta_i}(x, x') - \tilde{k}'_{\theta}(x, x') \right\vert \leq\delta_{m,L}'+\sqrt{L}c^d\delta_{m,L}',
\end{equation*}

where $\delta_{m,L}'$ is an upper bound on the error of the SKI approximation of the kernel $k'_{\theta_i}(x,x')$ with $m$ inducing points and interpolation degree $L-1$, as defined in Lemma \ref{lemma:ski-kernel-elementwise-error}.
\end{restatable}
\begin{proof}
    See Appendix \ref{sec:proofski_kernel_derivative_error_kernelz}
\end{proof}

We then use the elementwise bound to bound the spectral norm of the SKI kernel's gradient error.

\begin{restatable}{lemma}{gradientspectralnormbound}[Bound on Spectral Norm of Gradient Difference]
\label{lemma:gradient_spectral_norm_bound}
Assume that  $k_{\theta}$ with respect to $\theta_i$, denoted as $k'_{\theta,i}(x, x') = \frac{\partial k_{\theta}(x, x')}{\partial \theta_i}$, is a valid SPD kernel. Let $\tilde{k}'_{\theta,i}(x,x')$ be the SKI approximation of $k'_{\theta,i}(x,x')$, using the same inducing points and interpolation scheme as $\tilde{k}_{\theta}$.

Then, the spectral norm of the difference between the gradient of the true kernel matrix and the gradient of the SKI kernel matrix is bounded by:

\begin{equation}
\| \nabla_\theta K - \nabla_\theta \tilde{K} \|_2 \leq \sqrt{p} \max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i},
\end{equation}

where $\nabla_\theta K$ and $\nabla_\theta \tilde{K}$ are represented as $p \times n^2$ matrices using the vec-notation (denominator layout), $p$ is the number of hyperparameters, and $\gamma'_{n,m,L,i}$ is the bound on the spectral norm difference between the kernel matrices corresponding to $k'_{\theta,i}$ and its SKI approximation $\tilde{k}'_{\theta,i}$ (analogous to Proposition \ref{prop:spectral-norm}, but for the kernel $k'_{\theta,i}$).
\end{restatable}

\begin{proof}
See Section \ref{sec:proof_gradient_spectral_norm_bound}
\end{proof}

Next, we extend the previous result on the SKI kernel's spectral norm gradient error to the SKI log-likelihood's spectral norm gradient error.

\begin{lemma}[Corrected Bound on Gradient Difference]\label{lemma:gradient-difference-bound-corrected}
Let $\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})$ be the true log-likelihood and $\tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y})$ be the SKI approximation of the log-likelihood. Let $\nabla \mathcal{L}(\theta)$ and $\nabla \tilde{\mathcal{L}}(\theta)$ denote their respective gradients with respect to $\theta$. Then, for any $\theta\in \mathcal{D}$,
$$
\| \nabla \mathcal{L}(\theta) - \nabla \tilde{\mathcal{L}}(\theta) \|_2 \leq \frac{1}{\sigma^4}\left[\Vert y\Vert_1^2\left(\max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i}+2 C_n\gamma_{n,m,L}\right)+\frac{1}{2}\gamma_{n,m,L}\right],
$$
where $\gamma_{n,m,L}$ bounds the elementwise difference between $\mathbf{K}$ and $\tilde{\mathbf{K}}$, and $\gamma'_{n,m,L,i}$ is the bound on the spectral norm difference between the kernel matrices corresponding to $k'_{\theta,i}$ and its SKI approximation $\tilde{k}'_{\theta,i}$ and $C_n$ is a constant depending on the derivatives of the kernel function and the sample size.
\end{lemma}

\begin{proof}
We start with the expressions for the gradients:

$$
\nabla \mathcal{L}(\theta) = \nabla \left( -\frac{1}{2} \mathbf{y}^\top (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log |\mathbf{K} + \sigma^2 \mathbf{I}| - \frac{n}{2} \log(2\pi) \right).
$$

$$
\nabla \tilde{\mathcal{L}}(\theta) = \nabla \left( -\frac{1}{2} \mathbf{y}^\top (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log |\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| - \frac{n}{2} \log(2\pi) \right).
$$

Thus, the difference is:

\begin{align*}
\| \nabla \mathcal{L}(\theta) - \nabla \tilde{\mathcal{L}}(\theta) \|_2 &= \left\| \nabla \left( -\frac{1}{2} \mathbf{y}^\top (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log |\mathbf{K} + \sigma^2 \mathbf{I}| \right) \right. \\
&\quad \left. - \nabla \left( -\frac{1}{2} \mathbf{y}^\top (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \mathbf{y} - \frac{1}{2} \log |\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| \right) \right\|_2 \\
&\leq \underbrace{\left\| \nabla \left( \frac{1}{2} \mathbf{y}^\top \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right) \mathbf{y} \right) \right\|_2}_{T_1} \\
&\quad + \underbrace{\left\| \frac{1}{2} \nabla \left( \log |\mathbf{K} + \sigma^2 \mathbf{I}| - \log |\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| \right) \right\|_2}_{T_2}.
\end{align*}

We will bound $T_1$ and $T_2$ separately.

**Bounding $T_1$:**

Using the chain rule and the matrix derivative identity $\frac{\partial}{\partial \theta} \mathbf{X}^{-1} = -\mathbf{X}^{-1} (\frac{\partial \mathbf{X}}{\partial \theta}) \mathbf{X}^{-1}$, we have
\begin{align*}
    T_1 &= \frac{1}{2} \left\| \nabla_\theta \left( \mathbf{y}^\top \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right) \mathbf{y} \right) \right\|_2 \\
    &= \frac{1}{2} \left\| \sum_{i=1}^n \sum_{j=1}^n y_i y_j \nabla_\theta \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1}_{ij} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}_{ij} \right) \right\|_2 \\
    &\leq \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n |y_i y_j| \left\| \nabla_\theta \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1}_{ij} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}_{ij} \right) \right\|_2.
\end{align*}
Now, applying the matrix derivative identity, we get:
\begin{align*}
    &\left\| \nabla_\theta \left( (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1}_{ij} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}_{ij} \right) \right\|_2 \\
    &= \left\| \left( -(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} (\nabla_\theta \tilde{\mathbf{K}}) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} + (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} (\nabla_\theta \mathbf{K}) (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right)_{ij} \right\|_2 \\
    &= \left\| \left( -(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} (\nabla_\theta \tilde{\mathbf{K}} - \nabla_\theta \mathbf{K} + \nabla_\theta \mathbf{K}) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right.\right.\\
    &\quad \left.\left. + (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \nabla_\theta \mathbf{K} (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right)_{ij} \right\|_2 \\
    &= \left\| \left( -(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} (\nabla_\theta \tilde{\mathbf{K}} - \nabla_\theta \mathbf{K}) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right.\right.\\
    &\quad \left.\left. - (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} (\nabla_\theta \mathbf{K}) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1}  + (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} (\nabla_\theta \mathbf{K}) (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \right)_{ij} \right\|_2 \\
    &\leq \underbrace{\left\| (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} (\nabla_\theta \tilde{\mathbf{K}} - \nabla_\theta \mathbf{K}) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right\|_{2}}_{(a)} \\
    &\quad + \underbrace{\left\| \left((\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\right)(\nabla_\theta \mathbf{K}) (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right\|_{2}}_{(b)} \\
    &\quad + \underbrace{\left\|(\mathbf{K} + \sigma^2 \mathbf{I})^{-1}(\nabla_\theta \mathbf{K})\left((\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\right) \right\|_{2, }}_{(c)}\\
\end{align*}

We now explicitly bound (a), (b), and (c).

\begin{align*}
(a) &\leq \| (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \|_2 \| \nabla_\theta \tilde{\mathbf{K}} - \nabla_\theta \mathbf{K} \|_2 \| (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \|_2\\
&\leq \| (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \|_2^2 \| \nabla_\theta \tilde{\mathbf{K}} - \nabla_\theta \mathbf{K} \|_2 \\
&\leq \frac{1}{\sigma^4} \| \nabla_\theta \mathbf{K} - \nabla_\theta \tilde{\mathbf{K}} \|_2\text{ Proposition }\ref{lemma:gradient_spectral_norm_bound}\\
&\leq  \frac{1}{\sigma^4}\sqrt{p} \max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i}.
\end{align*}

\begin{align*}
(b) &\leq \|(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\|_2 \|\nabla_\theta \mathbf{K}\|_2 \|(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1}\|_2 \\
&\leq \frac{1}{\sigma^2} \|(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\|_2 \|\nabla_\theta \mathbf{K}\|_2 \\
&\leq \frac{\gamma_{n,m,L}}{\sigma^4} \|\nabla_\theta \mathbf{K}\|_2\text{ proof of Lemma \ref{lemma:action-inverse-error}}.
\end{align*}

\begin{align*}
(c) &\leq \|(\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\|_2 \|\nabla_\theta \mathbf{K}\|_2 \|(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\|_2 \\
&\leq \frac{1}{\sigma^2} \|(\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} - (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}\|_2 \|\nabla_\theta \mathbf{K}\|_2 \\
&\leq \frac{\gamma_{n,m,L}}{\sigma^4} \|\nabla_\theta \mathbf{K}\|_2\text{ proof of Lemma \ref{lemma:action-inverse-error}}.
\end{align*}

As the kernel is $C^1$ with respect to $\theta$, $\|\nabla_\theta \mathbf{K}\|$ can be bounded by a constant over $\mathcal{D}$, say $C_n$. Thus, we can bound $T_1$ as:
\begin{align*}
T_1 &\leq \sum_{i,j} \vert y_iy_j\vert \frac{1}{\sigma^4} \left(\max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i}+2 C_n\gamma_{n,m,L}\right)\\
&\leq \frac{1}{\sigma^4}\left(\max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i}+2 C_n\gamma_{n,m,L}\right)\sum_{i,j} \vert y_iy_j\vert \\
&=\frac{\Vert y\Vert_1^2}{\sigma^4}\left(\max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i}+2 C_n\gamma_{n,m,L}\right)
\end{align*}

**Bounding $T_2$:**

Using the identity $\nabla \log |\mathbf{X}| = (\mathbf{X}^{-1})^\top$, we have

\begin{align*}
T_2 &= \frac{1}{2} \left\| \nabla_\theta \left( \log |\mathbf{K} + \sigma^2 \mathbf{I}| - \log |\tilde{\mathbf{K}} + \sigma^2 \mathbf{I}| \right) \right\|_2 \\
&= \frac{1}{2} \left\| (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} - (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \right\|_2.
\end{align*}
We can rewrite the difference as:
$$
(\mathbf{K} + \sigma^2 \mathbf{I})^{-1} - (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} = (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} (\tilde{\mathbf{K}} - \mathbf{K}) (\mathbf{K} + \sigma^2 \mathbf{I})^{-1}
$$
Then, applying Lemma \ref{lemma:action-inverse-error} we obtain
\begin{align*}
    T_2 &\leq \frac{1}{2} \| (\tilde{\mathbf{K}} + \sigma^2 \mathbf{I})^{-1} \|_2 \| \tilde{\mathbf{K}} - \mathbf{K} \|_2 \| (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} \|_2 \\
    &\leq \frac{\gamma_{n,m,L}}{2\sigma^4}
\end{align*}
where $C_2(\theta)$ is another constant depending on $\theta$ (from the derivatives of the kernel when applying Lemma \ref{lemma:action-inverse-error}).

**Combining the Bounds:**

Combining the bounds for $T_1$ and $T_2$, we have

$$
\| \nabla \mathcal{L}(\theta) - \nabla \tilde{\mathcal{L}}(\theta) \|_2 \leq \frac{1}{\sigma^4}\left[\Vert y\Vert_1^2\left(\max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i}+2 C_n\gamma_{n,m,L}\right)+\frac{1}{2}\gamma_{n,m,L}\right].
$$

\end{proof}
\begin{proposition}[One-Step Improvement on True Log-Likelihood]\label{prop:one-step-improvement-on-true-log-likelihood}
Let $\theta^*\in \mathcal{D}$ be the true MLE maximizing $\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})$. Assume $\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})$ is $\mu$-strongly concave and has $L$-Lipschitz continuous gradient in $\mathcal{D}$.

If the step size $\eta$ satisfies $\eta \leq \frac{1}{L}$ (where $L$ is the Lipschitz constant of the gradient of the true log-likelihood) and $\eta\mu < 1$,
then starting at $\theta_0\in \mathcal{D}$, after one step of projected gradient ascent on $\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})$, yielding $\theta_1$, we have:

$$
\| \theta_1 - \theta^* \|_2 \leq (1 - \mu \eta) \| \theta_0 - \theta^* \|_2
$$

\end{proposition}
\begin{proof}
By the optimality of $\theta^*$, we have $\nabla \mathcal{L}(\theta^*) = 0$.
By the $\mu$-strong concavity of $\mathcal{L}$, we have for any $\theta$:

$$
\mathcal{L}(\theta) \leq \mathcal{L}(\theta^*) + \nabla \mathcal{L}(\theta^*)^\top (\theta - \theta^*) - \frac{\mu}{2} \| \theta - \theta^* \|_2^2 = \mathcal{L}(\theta^*) - \frac{\mu}{2} \| \theta - \theta^* \|_2^2
$$

Rearranging, we get:

\begin{equation} \label{eq:strong_concavity_alt}
\| \theta - \theta^* \|_2^2 \leq \frac{2}{\mu} (\mathcal{L}(\theta^*) - \mathcal{L}(\theta))
\end{equation}
Since $\theta_1$ is obtained by one step of projected gradient ascent from $\theta_0$, we have:
$$
\theta_1 = \text{Proj}_{\mathcal{D}} (\theta_0 + \eta \nabla \mathcal{L}(\theta_0))
$$

By the properties of projection onto a convex set, we have:

$$
\| \theta_1 - \theta^* \|_2^2 \leq \| \theta_0 + \eta \nabla \mathcal{L}(\theta_0) - \theta^* \|_2^2
$$

Expanding the right-hand side, we get:

\begin{align*}
\| \theta_1 - \theta^* \|_2^2 &\leq \| \theta_0 - \theta^* \|_2^2 + 2 \eta (\theta_0 - \theta^*)^\top \nabla \mathcal{L}(\theta_0) + \eta^2 \| \nabla \mathcal{L}(\theta_0) \|_2^2 \\
&\leq \| \theta_0 - \theta^* \|_2^2 + 2 \eta (\mathcal{L}(\theta_0) - \mathcal{L}(\theta^*) + \frac{\mu}{2} \| \theta_0 - \theta^* \|_2^2) + \eta^2 \| \nabla \mathcal{L}(\theta_0) \|_2^2 \\
&\text{(by strong concavity, with } \nabla \mathcal{L}(\theta^*) = 0 \text{)}\\
&\leq \| \theta_0 - \theta^* \|_2^2 - \eta \mu \| \theta_0 - \theta^* \|_2^2 + \eta^2 \| \nabla \mathcal{L}(\theta_0) \|_2^2 \quad \text{(by \eqref{eq:strong_concavity_alt})} \\
&= (1 - \eta \mu) \| \theta_0 - \theta^* \|_2^2 + \eta^2 \| \nabla \mathcal{L}(\theta_0) \|_2^2
\end{align*}

By the Lipschitz continuity of the gradient, we have:

$$
\| \nabla \mathcal{L}(\theta_0) \|_2 = \| \nabla \mathcal{L}(\theta_0) - \nabla \mathcal{L}(\theta^*) \|_2 \leq L \| \theta_0 - \theta^* \|_2
$$
Thus,
$$
\| \theta_1 - \theta^* \|_2^2 \leq (1 - \eta \mu) \| \theta_0 - \theta^* \|_2^2 + \eta^2 L^2 \| \theta_0 - \theta^* \|_2^2 = (1 - \eta \mu + \eta^2 L^2) \| \theta_0 - \theta^* \|_2^2
$$
Taking the square root of both sides:
$$
\| \theta_1 - \theta^* \|_2 \leq \sqrt{1 - \eta \mu + \eta^2 L^2} \| \theta_0 - \theta^* \|_2
$$
If we choose $\eta \leq \frac{1}{L}$, we have:
$$
\sqrt{1 - \eta \mu + \eta^2 L^2} \leq \sqrt{1-2\eta \mu+\eta^2\mu^2} = 1 - \eta \mu
$$
Thus,
$$
\| \theta_1 - \theta^* \|_2 \leq (1 - \eta \mu) \| \theta_0 - \theta^* \|_2
$$
as long as $\eta \leq \frac{1}{L}$.
\end{proof}

\begin{proposition}[MLE Closeness via One-Step Improvement]\label{prop:mle-closeness-one-step}
Let $\theta^*$ be the true MLE maximizing $\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})$. Assume $\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})$ is $\mu$-strongly concave and has $L$-Lipschitz continuous gradient near $\theta^*$. Assume that the gradient difference is bounded:

$$
\| \nabla \tilde{\mathcal{L}}(\theta) - \nabla \mathcal{L}(\theta) \|_2 \leq \epsilon_g:= \frac{1}{\sigma^4}\left[\Vert y\Vert_1^2\left(\frac{1}{\sqrt{p}}\max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i}+2 C\gamma_{n,m,L}\right)+\frac{1}{2}\gamma_{n,m,L}\right]
$$

for all $\theta\in \mathcal{D}$.

If the step size $\eta>0$ satisfies $\eta \leq \frac{1}{\tilde{L}}$ (where $L$ is the Lipschitz constant of the gradient of the log-likelihood) $\mu\eta\leq 1$:

$$
\| \theta_0 - \theta^* \|_2 \leq (1 - \eta \mu) \| \theta_0 - \theta^* \|_2+\eta \epsilon_g
$$

Then, after one step of projected gradient ascent on $\tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y})$, yielding $\theta_1$, we have:

$$
\| \theta_1 - \theta^* \|_2 \leq (1 - \mu \eta) \| \theta_0 - \theta^* \|_2 + \eta \epsilon_g
$$
\end{proposition}
\begin{proof}
    Let $\theta_1$ be the result of one step of projected gradient ascent on $\tilde{\mathcal{L}}$ from $\theta_0$ with step size $\eta$:
    $$
    \theta_1 = \text{Proj}_{\mathcal{D}} (\theta_0 + \eta \nabla \tilde{\mathcal{L}}(\theta_0))
    $$
    where $\mathcal{D}$ is a convex set containing $\theta^*$.

    By the triangle inequality, we have:
    \begin{align*}
    \| \theta_1 - \theta^* \|_2 &= \| \text{Proj}_{\mathcal{D}} (\theta_0 + \eta \nabla \tilde{\mathcal{L}}(\theta_0)) - \theta^* \|_2 \\
    &\leq \| \theta_0 + \eta \nabla \tilde{\mathcal{L}}(\theta_0) - \theta^* \|_2 \\
    &= \| \theta_0 + \eta \nabla \mathcal{L}(\theta_0) - \theta^* + \eta (\nabla \tilde{\mathcal{L}}(\theta_0) - \nabla \mathcal{L}(\theta_0)) \|_2 \\
    &\leq \| \theta_0 + \eta \nabla \mathcal{L}(\theta_0) - \theta^* \|_2 + \eta \| \nabla \tilde{\mathcal{L}}(\theta_0) - \nabla \mathcal{L}(\theta_0) \|_2 \\
    &\leq \| \theta_0 + \eta \nabla \mathcal{L}(\theta_0) - \theta^* \|_2 + \eta \epsilon_g \quad \text{(by the gradient difference bound)}
    \end{align*}

    Now, let's consider the term $\| \theta_0 + \eta \nabla \mathcal{L}(\theta_0) - \theta^* \|_2$. This represents the distance to $\theta^*$ after taking one step of gradient ascent on the true log-likelihood $\mathcal{L}$. By Proposition \ref{prop:one-step-improvement-on-true-log-likelihood}, if $\| \theta_0 - \theta^* \|_2 \leq \frac{\eta(2\mu - \eta L)}{2}$ and $\eta \leq \frac{1}{L}$, we have:
    $$
    \| \theta_0 + \eta \nabla \mathcal{L}(\theta_0) - \theta^* \|_2 \leq (1 - \mu \eta) \| \theta_0 - \theta^* \|_2
    $$

    Therefore,
\begin{align*}
    \| \theta_1 - \theta^* \|_2 &\leq (1-\eta\mu)\|\theta_0-\theta^*\|_2+\eta\epsilon_g
\end{align*}
\end{proof}

\begin{itemize}
    \item \textbf{Not globally concave:} The GP log-likelihood with the RBF kernel is generally \textbf{not globally concave} with respect to the lengthscale and variance, even for large values of $l$.
    \item \textbf{Locally concave in regions:} However, as you suggest, it \textbf{can be locally concave in certain regions of the hyperparameter space, including regions with large lengthscales}. This means that if your optimizer starts in such a region, it might converge to a local optimum, but not necessarily the global optimum. The reason why it is locally concave is due to the fact that the kernel matrix is close to a rank-1 matrix. When this happens, the log determinant term can be approximated as $\log \det(\sigma^2 \mathbf{11}^T) = n\log(\sigma^2) + C$, where $C$ is some constant, and the quadratic term can be approximated as $\mathbf{y}^T(\sigma^2 \mathbf{11}^T)^{-1}\mathbf{y} = c/\sigma^2$, where $c$ is some constant. When $l$ is very large, the log likelihood is approximately $-1/2*(c/\sigma^2) - 1/2*(n*\log(\sigma^2)) = -1/2*(c/\sigma^2 + n\log(\sigma^2))$. The second derivative of $-1/2*(c/\sigma^2 + n\log(\sigma^2))$ with respect to $\sigma^2$ is $(c - n\sigma^4)/(2\sigma^6)$, which can be positive, but it can also be negative when $\sigma^4 > c/n$. This implies that when $\sigma^4 > c/n$, the log likelihood is locally concave. But $\sigma^4 < c/n$ means the log likelihood is convex.
    \item \textbf{Multiple local optima:} It's common to have multiple local optima in the GP log-likelihood landscape, especially when using flexible kernels like RBF. This is a well-known issue in GP hyperparameter optimization.
\end{itemize}

\begin{theorem}[Closeness of SKI MLE to True MLE]\label{thm:mle-closeness-gradient-ascent}
Let $\theta^*$ be the true MLE maximizing $\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})$. Assume $\mathcal{L}(\theta; \mathbf{X}, \mathbf{y})$ is $\mu$-strongly concave and has $L$-Lipschitz continuous gradient in $\mathcal{D}$. Assume that the gradient difference between the true log-likelihood and the SKI log-likelihood is bounded by
$$
\| \nabla \tilde{\mathcal{L}}(\theta) - \nabla \mathcal{L}(\theta) \|_2 \leq \epsilon_g := \frac{1}{\sigma^4}\left[\Vert y\Vert_1^2\left(\frac{1}{\sqrt{p}}\max_{i \in \{1,\ldots,p\}} \gamma'_{n,m,L,i}+2 C\gamma_{n,m,L}\right)+\frac{1}{2}\gamma_{n,m,L}\right]
$$
for all $\theta$ in a neighborhood of $\theta^*$, where $\gamma_{n,m,L}$, $\gamma'_{n,m,L,i}$ are defined as in Proposition \ref{prop:spectral-norm} and Proposition \ref{lemma:gradient_spectral_norm_bound}, respectively, and $C$ is a bound on $C_n$ in this neighborhood. Let $\tilde{\theta}$ be the SKI MLE obtained by running projected gradient ascent on $\tilde{\mathcal{L}}(\theta; \mathbf{X}, \mathbf{y})$ from initialization $\theta_0$ with a fixed step size $\eta$.

If the step size $\eta$ satisfies $0 < \eta \leq \frac{1}{L}$ and $\eta \mu < 1$, then, after $t$ steps of projected gradient ascent, the iterate $\theta_t$ satisfies
$$
\| \theta_t - \theta^* \|_2 \leq (1 - \eta\mu)^t \| \theta_0 - \theta^* \|_2 + \frac{\epsilon_g}{\eta\mu}.
$$
\end{theorem}
\begin{proof}
    Consider that starting from $\theta_t$ and applying one step of gradient ascent, by Proposition \ref{prop:mle-closeness-one-step}
    \begin{align*}
        \Vert \theta_{t+1}-\theta^*\Vert_2 &\leq (1-\eta\mu)\Vert \theta_t-\theta^*\Vert_2+\eta \epsilon_g
    \end{align*}
    and iterating we have
    \begin{align*}
        \Vert \theta_t-\theta^*\Vert_2&\leq (1-\eta\mu)^t\Vert \theta_0-\theta^*\Vert_2+\sum_{s=0}^{t-1} (1-\eta\mu)^s \eta \epsilon_g\\
        &\leq (1-\eta\mu)^t\Vert \theta_0-\theta^*\Vert_2+\frac{\eta\epsilon_G}{\eta\mu}\text{ geometric series}
    \end{align*}
\end{proof}
% $$
% \frac{\partial^2 L}{\partial \ell^2} = \frac{1}{2}\alpha^T K'' \alpha - \alpha^T K' K_y^{-1} K' \alpha + \frac{1}{2} \text{tr}\left( K_y^{-1} K' K_y^{-1} K' - K_y^{-1} K''\right)
% $$
\begin{lemma}[Strong Concavity of the Log-Likelihood for Signal Variance with Fixed Lengthscale]
Let $\mathbf{X}_D = \{\mathbf{x}_1, \ldots, \mathbf{x}_N\}$ be a set of $N$ distinct training input points in $\mathbb{R}^d$. Consider a Gaussian Process with an RBF kernel (or an RQ kernel with fixed shape parameter $\alpha$) where the lengthscale $\ell$ is fixed at its true value $\ell^*$. The negative log-likelihood function of the Gaussian Process, as a function of the signal variance $\sigma^2$, is locally strongly convex in a neighborhood of the true signal variance $\sigma^{*2}$.
\end{lemma}

\begin{proof}
\textbf{(Fixed Lengthscale):}

\begin{enumerate}
    \item \textbf{Kernel and Parameters:}

    Assume an RBF kernel (the argument for RQ is analogous):
    $$
    k(\mathbf{x}_i, \mathbf{x}_j; \sigma^2, \ell) = \sigma^2 \exp\left(-\frac{||\mathbf{x}_i - \mathbf{x}_j||^2}{2\ell^2}\right)
    $$
    Fix the lengthscale at its true value, $\ell = \ell^*$.
    The only parameter we are varying is $\theta = \sigma^2$.

    \item \textbf{Log-Likelihood:}

    The log-likelihood function for a GP with fixed hyperparameters is:
    $$
    \mathcal{L}(\sigma^2) = -\frac{N}{2} \log(2\pi) - \frac{1}{2} \log |K| - \frac{1}{2} \mathbf{y}^T K^{-1} \mathbf{y}
    $$
    where $K = K(\mathbf{X}_D, \mathbf{X}_D; \sigma^2, \ell^*)$ is the kernel matrix.

    \item \textbf{Simplifying the Kernel Matrix:}

    Since $\ell^*$ is fixed, we can write the kernel matrix as:
    $$
    K = \sigma^2 K_*
    $$
    where $K_* = K(\mathbf{X}_D, \mathbf{X}_D; 1, \ell^*)$ is a constant matrix that does not depend on $\sigma^2$.

    \item \textbf{Log-Likelihood in Terms of $\sigma^2$:}

    Substituting $K = \sigma^2 K_*$ into the log-likelihood, we get:
    $$
    \mathcal{L}(\sigma^2) = -\frac{N}{2} \log(2\pi) - \frac{1}{2} \log |\sigma^2 K_*| - \frac{1}{2} \mathbf{y}^T (\sigma^2 K_*)^{-1} \mathbf{y}
    $$
    $$
    \mathcal{L}(\sigma^2) = -\frac{N}{2} \log(2\pi) - \frac{N}{2} \log(\sigma^2) - \frac{1}{2} \log |K_*| - \frac{1}{2\sigma^2} \mathbf{y}^T K_*^{-1} \mathbf{y}
    $$

    \item \textbf{Derivatives:}

    Now, let's take the first and second derivatives of the log-likelihood with respect to $\sigma^2$:
    \begin{itemize}
        \item First derivative:
        $$
        \frac{d\mathcal{L}}{d\sigma^2} = -\frac{N}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \mathbf{y}^T K_*^{-1} \mathbf{y}
        $$
        \item Second derivative:
        $$
        \frac{d^2\mathcal{L}}{d(\sigma^2)^2} = \frac{N}{2(\sigma^2)^2} - \frac{1}{(\sigma^2)^3} \mathbf{y}^T K_*^{-1} \mathbf{y}
        $$
    \end{itemize}

    \item \textbf{Strong Concavity of the Negative Log-Likelihood:}

    To show strong concavity of the *negative* log-likelihood, we need to show that the second derivative of the log-likelihood is *negative*:
    $$
    \frac{d^2\mathcal{L}}{d(\sigma^2)^2} = \frac{N}{2(\sigma^2)^2} - \frac{1}{(\sigma^2)^3} \mathbf{y}^T K_*^{-1} \mathbf{y} < 0
    $$
    Rearranging, we get:
    $$
    \frac{N}{2} < \frac{\mathbf{y}^T K_*^{-1} \mathbf{y}}{\sigma^2}
    $$
    Since $K_*$ is positive definite, $\mathbf{y}^T K_*^{-1} \mathbf{y} > 0$. Thus, for a fixed dataset, we can always find a neighborhood of $\sigma^2$ values such that this inequality holds. This means that the negative log-likelihood is locally strongly convex/concave.

\end{enumerate}

\end{proof}


