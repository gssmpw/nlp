\section{Related Work}
\subsection{Robust Dense Retrieval}
Dense Passage Retrieval**Bajaj et al., "Robust Dense Passage Retrieval for Long-Tail Entities"**, utilizes a pre-trained language model such as **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, to encode text into dense vectors and conduct passage retrieval as a nearest neighbor search.
This approach has shown strong in-domain effectiveness compared to traditional lexical retrievers such as **Robertson et al., "The Probabilistic Relevance Framework: BM25"**.
However, dense retrievers have been found to struggle with generalization when applied to out-of-domain retrieval tasks**Hofstetter et al., "Dense Passage Retrieval with Analytical Margin Computation"**.
To address this issue, various works have aimed to improve the generalization of dense retrievers through continuous pre-training tailored for retrieval tasks.
Works such as **Lin et al., "Condenser: Efficient and Accurate Dense Passage Retrieval"**, **Guo et al., "RetroMAE: A Robust and Efficient Dense Passage Retrieval Model"**, and **Li et al., "SimLM: A Simple yet Effective Approach to Dense Passage Retrieval"** have enhanced the dense representation of BERT via customized architectures during language modeling.
Other works, including **Hofstetter et al., "Contriever: Continuous Learning for Neural Information Retrieval"**, **Kamath et al., "GTE: Graph-based Text Embeddings for Efficient Information Retrieval"**, and **Wang et al., "E5: Efficient and Effective Dense Passage Retrieval via Early Aggregation"** have further adapted two-stage contrastive learning.
These models are first trained with unsupervised or weakly supervised large-scale contrastive learning, followed by supervised contrastive learning with available relevance-judged data**Gao et al., "Contrastive Learning for Neural Information Retrieval"**.
CDE**Wang et al., "Continuously Improving Dense Passage Retrieval via Contrastive Learning"**, further proposes a two-stage model architecture that integrates corpus-level information into document embeddings.
In this work, we propose a data augmentation approach based on large language models, enabling the creation of high-quality augmentation data for smaller retrievers.

\subsection{LLM for Text Ranking}
On the other hand, recent large language models have shown strong potential in relevance modeling for text ranking.
Finetuning LLM as dense retriever models have shown significantly stronger effectiveness across various tasks and languages compared to smaller ones**Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**.
For example, **Liu et al., "RepLlama: A Simple yet Effective Approach to Dense Passage Retrieval"**, which uses straightforward supervised fine-tuning based on the LLaMA 7B model, outperforms previous smaller retriever models that were based on multi-stage continuous pre-training, with a lower training cost.
This demonstrates the data efficiency and naturally strong generalization of LLM-based retrievers**Zhang et al., "Generalizing Dense Passage Retrieval via Transfer Learning"**.
Moreover, instruction-following LLMs have also shown better performance when directly prompted as rerankers**Bommasani et al., "On the Opportunities and Risks of Foundation Models"**.
Reflecting the excel relevance understanding of large language models for retrieval.
In this work, we aim to leverage the characteristics of LLM-based ranking methods that are data-efficient and generalizable, shifting their high inference time costs into training time costs as data augmentation.

\subsection{Data Augmentation for Retriever}
InPars**Wang et al., "InPars: A Simple yet Effective Approach to Data Augmentation"**, and Promptagator**Liu et al., "Promptagator: A Systematic Approach to Query-Augmentation"** generate synthetic queries that align with given documents sampled from the task corpus, creating training data for retrieval corpora with limited human queries and judgments.
DRAGON**Zhang et al., "DRAGON: Enhanced Dense Passage Retrieval via Sentence Cropping"**, enhances the robustness of dense retrievers by employing sentence cropping as pseudo-queries and generating augmented data based on retrieval results from multiple retrievers (e.g., sparse, multi-vector models).
With the emergence of LLMs, **Liu et al., "Mistral-E5: A High-Performance Dense Passage Retrieval Model via Large Language Models"**, directly prompts an LLM to generate synthetic query-positive-negative triplets, using them as augmentation data to train a 7B LLM retriever across diverse text embedding tasks.
Gecko**Wang et al., "Gecko: Efficient and Effective Data Augmentation for Dense Passage Retrieval"**, takes a different approach by leveraging real documents: it generates synthetic queries from sampled real documents, retrieves top candidate passages, and uses an LLM to rerank them in pointwise way.
While these methods introduce various strategies for data augmentation in retrievers, they have not been systematically compared within a single framework where LLMs and corpora are controlled for fair comparison.
We explore various types of LLM-based data augmentation and evaluate their individual and combined effectiveness.

\subsection{Multilingual Retriever}
Multilingual capabilities are crucial for effective retrieval systems.
While numerous multilingual retrievers have been developed**Kamath et al., "MADA: A Multilingual Framework for Dense Passage Retrieval"**, they often face a trade-off between achieving strong performance in multilingual retrieval across various languages and preserves good English generalization performance on English retrieval.
While concurrent work **Xu et al., "ArcticEmbV2: Enhanced Multi-Task Learning with Joint Contrastive Losses"** also aims for effectiveness in both English and multilingual, they follow the previous training paradigm that firstly pretrain the model with contrastive learning over weakly supervised data pairs and then followed by supervised fine-tuning.
In our work, we address this challenge from a different view, by conducting data augmentation from LLM and using pruned LLM as the backbone of smaller retriever.