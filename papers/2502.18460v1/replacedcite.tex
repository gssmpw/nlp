\section{Related Work}
\subsection{Robust Dense Retrieval}
Dense Passage Retrieval____ utilizes a pre-trained language model such as BERT____, to encode text into dense vectors and conduct passage retrieval as a nearest neighbor search.
This approach has shown strong in-domain effectiveness compared to traditional lexical retrievers such as BM25____.
However, dense retrievers have been found to struggle with generalization when applied to out-of-domain retrieval tasks____.
To address this issue, various works have aimed to improve the generalization of dense retrievers through continuous pre-training tailored for retrieval tasks.
Works such as Condenser____, RetroMAE____, and SimLM____ have enhanced the dense representation of BERT via customized architectures during language modeling.
Other works, including Contriever____, GTE____, E5____ have further adapted two-stage contrastive learning.
These models are first trained with unsupervised or weakly supervised large-scale contrastive learning, followed by supervised contrastive learning with available relevance-judged data____.
CDE____ further proposes a two-stage model architecture that integrates corpus-level information into document embeddings.
In this work, we propose a data augmentation approach based on large language models, enabling the creation of high-quality augmentation data for smaller retrievers.

\subsection{LLM for Text Ranking}
On the other hand, recent large language models have shown strong potential in relevance modeling for text ranking.
Finetuning LLM as dense retriever models have shown significantly stronger effectiveness across various tasks and languages compared to smaller ones____.
For example, RepLlama____, which uses straightforward supervised fine-tuning based on the Llama2-7B model, outperforms previous smaller retriever models that were based on multi-stage continuous pre-training, with a lower training cost.
This demonstrates the data efficiency and naturally strong generalization of LLM-based retrievers____.
Moreover, instruction-following LLMs have also shown better performance when directly prompted as rerankers____.
Reflecting the excel relevance understanding of large language models for retrieval.
In this work, we aim to leverage the characteristics of LLM-based ranking methods that are data-efficient and generalizable, shifting their high inference time costs into training time costs as data augmentation.


\subsection{Data Augmentation for Retriever}
InPars____ and Promptagator____ generate synthetic queries that align with given documents sampled from the task corpus, creating training data for retrieval corpora with limited human queries and judgments.
DRAGON____ enhances the robustness of dense retrievers by employing sentence cropping as pseudo-queries and generating augmented data based on retrieval results from multiple retrievers (e.g., sparse, multi-vector models).
With the emergence of LLMs, Mistral-E5____ directly prompts an LLM to generate synthetic query-positive-negative triplets, using them as augmentation data to train a 7B LLM retriever across diverse text embedding tasks.
Gecko____ takes a different approach by leveraging real documents: it generates synthetic queries from sampled real documents, retrieves top candidate passages, and uses an LLM to rerank them in pointwise way.
While these methods introduce various strategies for data augmentation in retrievers, they have not been systematically compared within a single framework where LLMs and corpora are controlled for fair comparison.
We explore various types of LLM-based data augmentation and evaluate their individual and combined effectiveness.

\subsection{Multilingual Retriever}
Multilingual capabilities are crucial for effective retrieval systems.
While numerous multilingual retrievers have been developed____, they often face a trade-off between achieving strong performance in multilingual retrieval across various languages and preserves good English generalization performance on English retrieval.
While concurrent work ArcticEmbV2____ also aims for effectiveness in both English and multilingual, they follow the previous training paradigm that firstly pretrain the model with contrastive learning over weakly supervised data pairs and then followed by supervised fine-tuning.
In our work, we address this challenge from a different view, by conducting data augmentation from LLM and using pruned LLM as the backbone of smaller retriever.