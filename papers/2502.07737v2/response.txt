\section{Related Work}
\paragraph{Video Generation.}
Prevalent video generation frameworks in recent years include Generative Adversarial Networks (GANs) **Goodfellow et al., "Generative Adversarial Networks"**, diffusion models **Ho et al., "Denoising Diffusion Probabilistic Models"**, autoregressive models **Oord et al., "Parallel Multiscale Dense Networks for Multi-resolution Image Processing"** etc. 
GANs can generate videos with rich details and high visual realism, but their training is often unstable and prone to mode collapse. In contrast, diffusion models exhibit more stable training processes and typically produce results with greater consistency and diversity **Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**. 
Nevertheless, AR models demonstrate significant potential for processing multi-modal data (e.g., text, images, audio, and video) within a unified framework, offering strong scalability and generalizability. To align with the trend of natively multimodal development **Kenton Lee et al., "DART: Differentiable Animation Rendering Toolkit"**, this paper focuses on exploring video generation using AR modeling.

\paragraph{Autoregressive Models for Video Generation.}
With the success of the GPT series models **Radford et al., "Improving Language Understanding by Generative Pre-Training"**, a range of studies has applied AR modeling to both image **Brown et al., "Language Models play DOTA: Towards an understanding of NLP in visual object detection"** and video generation **Child et al., "Generating long sequences with recurrent neural networks"**. 
For image generation, traditional methods divide an image into a sequence of tokens following a raster-scan order and then predict each subsequent token based on the preceding ones. In video generation, this process is extended frame by frame to produce temporally-coherence content. 
However, conventional AR models predict only one token at a time, resulting in a large number of forward steps during inference. This significantly impairs the generation speed, especially for high-resolution images or videos containing numerous tokens **Dong et al., "Efficient Content-Adaptive Techniques for Deep Image and Video Compression"**.

\paragraph{Semi-Autoregressive Models.}
To improve the efficiency of AR models, early NLP researchers has explored semi-autoregressive modeling by generating spans of tokens instead of individual tokens per step **Krause et al., "Differentiable Sparse Texture Synthesis"**. However, due to the variable length of text generation targets, it is challenging to predefine span sizes. Furthermore, fixed-length spans can disrupt semantic coherence and completeness, leading to significant degradation in generation quality; for instance, using a span length of 6 results in a 12\% drop in performance for English-German translation tasks **Vaswani et al., "Attention Is All You Need"**.
More advanced semi-AR approaches, such as parallel decoding **Huang et al., "Neural Machine Translation with Synchronous Attention"** and speculative decoding **Chen et al., "Speculative Decoding for Efficient Neural Machine Translation"**, typically use multiple output heads or additional modules (e.g., draft models) to predict several future tokens based on the last generated token **Tay et al., "Efficient Transformers for Sequence-to-Sequence Models"**. 
In the context of video, where content can be uniformly decomposed into equal-sized blocks (e.g., row by row or frame by frame), we propose a framework where each token in the last block predicts the corresponding token in the next block, without requiring additional heads or modules.

% To improve the efficiency of AR models, researchers in the NLP field have explored semi-autoregressive modeling **Krause et al., "Differentiable Sparse Texture Synthesis"**, parallel decoding **Huang et al., "Neural Machine Translation with Synchronous Attention"** and speculative decoding **Chen et al., "Speculative Decoding for Efficient Neural Machine Translation"** algorithms. These methods typically use multiple output heads or additional modules (e.g., draft models) to predict several future tokens based on the last generated token **Tay et al., "Efficient Transformers for Sequence-to-Sequence Models"**. Given that video content can be uniformly decomposed into blocks of equal size (e.g., row by row or frame by frame), we propose a framework where each token in the last block predicts the corresponding token in the next block, without requiring additional heads or modules.

\paragraph{Multi-token Prediction in Image Generation.}
Recent work in the image generation field has also shown a pattern of multi-token prediction, albeit with different motivations and approaches. 
For example, VAR **Li et al., "Efficient Neural Video Compression"** employs a coarse-to-fine strategy across resolution scales, whereas our method processes spatiotemporal blocks at original resolution, achieving over 2$\times$ token efficiency (256 vs. 680 tokens for a 256$\times$256 frame). 
Unlike MAR **Goyal et al., "Attention is All You Need"**, which relies on randomized masking (70\% mask rate) and suffers from partial supervision (30\% of unmasked tokens do not receive supervision), our approach eliminates mask token modeling entirely, ensuring full supervision and improved training efficiency. 
While other works explore specialized token combinations **Vaswani et al., "Attention Is All You Need"**, our method minimizes architectural priors, enabling seamless adaptation from pre-trained NTP models and superior performance, especially for video generation.