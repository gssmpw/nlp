\section{Related Work}
\paragraph{Video Generation.}
Prevalent video generation frameworks in recent years include Generative Adversarial Networks (GANs)____, diffusion models____, autoregressive models____, etc. 
GANs can generate videos with rich details and high visual realism, but their training is often unstable and prone to mode collapse. In contrast, diffusion models exhibit more stable training processes and typically produce results with greater consistency and diversity____. 
Nevertheless, AR models demonstrate significant potential for processing multi-modal data (e.g., text, images, audio, and video) within a unified framework, offering strong scalability and generalizability. To align with the trend of natively multimodal development____, this paper focuses on exploring video generation using AR modeling.

\paragraph{Autoregressive Models for Video Generation.}
With the success of the GPT series models____, a range of studies has applied AR modeling to both image____ and video generation____. 
For image generation, traditional methods divide an image into a sequence of tokens following a raster-scan order and then predict each subsequent token based on the preceding ones. In video generation, this process is extended frame by frame to produce temporally-coherence content. 
However, conventional AR models predict only one token at a time, resulting in a large number of forward steps during inference. This significantly impairs the generation speed, especially for high-resolution images or videos containing numerous tokens____.

\paragraph{Semi-Autoregressive Models.}
To improve the efficiency of AR models, early NLP researchers has explored semi-autoregressive modeling by generating spans of tokens instead of individual tokens per step____. However, due to the variable length of text generation targets, it is challenging to predefine span sizes. Furthermore, fixed-length spans can disrupt semantic coherence and completeness, leading to significant degradation in generation quality; for instance, using a span length of 6 results in a 12\% drop in performance for English-German translation tasks____.
More advanced semi-AR approaches, such as parallel decoding____ and speculative decoding____, typically use multiple output heads or additional modules (e.g., draft models) to predict several future tokens based on the last generated token____. 
In the context of video, where content can be uniformly decomposed into equal-sized blocks (e.g., row by row or frame by frame), we propose a framework where each token in the last block predicts the corresponding token in the next block, without requiring additional heads or modules. 

% To improve the efficiency of AR models, researchers in the NLP field have explored semi-autoregressive modeling____, parallel decoding____ and speculative decoding____ algorithms. These methods typically use multiple output heads or additional modules (e.g., draft models) to predict several future tokens based on the last generated token____. Given that video content can be uniformly decomposed into blocks of equal size (e.g., row by row or frame by frame), we propose a framework where each token in the last block predicts the corresponding token in the next block, without requiring additional heads or modules. 

\paragraph{Multi-token Prediction in Image Generation.}
Recent work in the image generation field has also shown a pattern of multi-token prediction, albeit with different motivations and approaches. 
For example, VAR____ employs a coarse-to-fine strategy across resolution scales, whereas our method processes spatiotemporal blocks at original resolution, achieving over 2$\times$ token efficiency (256 vs. 680 tokens for a 256$\times$256 frame). 
Unlike MAR____, which relies on randomized masking (70\% mask rate) and suffers from partial supervision (30\% of unmasked tokens do not receive supervision), our approach eliminates mask token modeling entirely, ensuring full supervision and improved training efficiency. 
While other works explore specialized token combinations____, our method minimizes architectural priors, enabling seamless adaptation from pre-trained NTP models and superior performance, especially for video generation.