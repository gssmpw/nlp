\begin{table*}[]
\centering
\caption{Comparison of next-token prediction (NTP) and next-block prediction (\modelname) models in terms of performance and speed, evaluated on the K600 dataset (5-frame condition, 12 frames (768 tokens) to predict). Inference time was measured on a single A100 Nvidia GPU. All models are implemented by us under the same setting and trained for 20 epochs. FPS denotes ``frame per second''. The measurement of inference speed includes tokenization and de-tokenization processes. KV-cache is used for both models.}
\label{table:semiar-ar-scale}
% \setlength{\tabcolsep}{2.0pt}
% \vspace{0.04in}
\begin{adjustbox}{max width=\linewidth}

\begin{tabular}{@{}c|l|c|c|cc@{}}
\toprule
Model Size & Modeling Method & \# Block size & FVD $\downarrow$ & \# Forward steps & Inference speed (FPS) $\uparrow$ \\ \midrule
\multirow{2}{*}{700M}          & NTP     & 1 (1$\times$1$\times$1)   & 37.4 & 768           & 0.80                  \\
                               & \modelname (Ours)  & 16 (1$\times$1$\times$16) & \textbf{33.6} & \textbf{48}            & \textbf{8.89}                  \\ \midrule
\multirow{2}{*}{1.2B}          & NTP  & 1 (1$\times$1$\times$1)    & 31.4 & 768           & 0.75                  \\
                               & \modelname (Ours)  & 16 (1$\times$1$\times$16) & \textbf{28.6} & \textbf{48}            & \textbf{6.70}                  \\ \midrule
\multirow{2}{*}{3B}            & NTP  & 1 (1$\times$1$\times$1)    & 29.0 & 768           & 0.60                  \\
                               & \modelname (Ours)  & 16 (1$\times$1$\times$16)  & \textbf{26.5} & \textbf{48}            & \textbf{4.29}                  \\ \bottomrule
\end{tabular}


\end{adjustbox}
\end{table*}
