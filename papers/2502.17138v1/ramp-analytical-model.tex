\subsection{Choosing Replica Protection Strength}

A key challenge in applying \ramp is choosing the right hardware protection strength of individual replicas. 
Weakening hardware-level protection of individual replicas lowers storage cost but makes DUEs and NDEs more frequent, increasing UBER and SDC rates respectively.
We can recoup the lost UBER by correcting a DUE using available replicas, at the expense of a performance overhead to access and process additional replicas.
However, we cannot always rely on replicas to recoup the lost SDC rate. 
This is because a NDE that silently corrupts data may not trigger a correction by the storage-optimizer tier, unless the application can detect the error through other means, such as checksumming~\cite{zhang:pangolin:atc:2019}.
Hence, SDC rate may limit how much we can weaken individual replica strength.

To help application and system designers choose protection strength, we develop an analytical model that estimates the expected reliability and expected performance overhead when using available replicas to correct a DUE. 
For the reliability, we estimate the combined DUE resulting from using replicas to correct a memory error. 
Because NDE does not benefit from replication, we do not compute a combined NDE. 
However, we do estimate the NDE of each individual replica as a lower reliability bound. 
For the performance overhead, we compute the average number of additional replicas that are read to correct a memory error.
We assume that reading and processing each replica contributes fixed network bandwidth and CPU overhead per replica.

Our analytical model targets block-level replication, which is a common replication approach. The model differentiates between logical and physical blocks. The logical block is the unit of recovery, that is the smallest unit of data that can be recovered by the replication protocol. To enable recovery, a replication protocol maps a logical block to multiple physical block replicas stored across multiple memory nodes. Reading a logical block may involve reading one or more physical blocks.

\ignore{
Since we focus on protection techniques against random bit cell errors, our model focuses on read failures caused by uncorrectable memory errors due to random bit cell errors. 
Our model ignores other correlated failures that affect multiple bits and blocks, such as chip or channel failures due to logic circuit errors.
}

\ignore{
The model uses the following parameters: CPU cache-line size: $c$:, Physical-block size: $b$, Cache-line failure probability due to DUE: $p_c$, Physical-block failure probability due to DUE: $p_b$.
}

\begin{table}
\caption{Analytical model symbol notation}
\label{tab:model}
\centering
\begin{tabular}{lp{6.5cm}}
\textbf{Symbol(s)} & \textbf{Description} \\
$c$, $b$        & \scriptsize{Cache-line size and physical-block size}\\
$\pcdue$, $\pcnde$  & \scriptsize{Cache-line failure probability due to DUE and NDE}\\
$\pbdue$, $\pbnde$  & \scriptsize{Physical-block failure probability due to DUE and NDE}\\
%$\pldue$, $\plnde$  & \scriptsize{Logical-block failure probability due to DUE and NDE}\\
$\pldue$  & \scriptsize{Logical-block failure probability due to DUE}\\
\end{tabular}
\end{table}

The model uses the symbol notation shown in Table \ref{tab:model}.
%
Cache-line failure probability is the probability to fail when reading a cache-line worth of data from the memory system.
This probability depends on the memory protection scheme and the RBER of the underlying memory technology.
Although the RBER may vary over the lifetime of a memory technology, for the sake of simplicity, we adopt a single worst-case value based on the RBER at the end of a specified time period. 
For example, in NVM where the RBER may increase with the amount of time since the last write or refresh~\cite{zhang:pm-chipkill:micro:2018}\ignore{, which can range from a week to a year}, we can use the RBER at the end of the refresh period as the worst-case value.

Physical-block failure probability is the probability to fail when reading a physical-block worth of data from the memory system.
Successfully reading a physical block entails successfully reading all the cache lines that comprise the block. 
We can derive the physical-block failure probability as follows:
\ignore{from the cache-line failure probability:}

% \begin{equation}
% p_b&= 1 - (1-p_c)^{b/c} \quad \mathrm{and} \quad  p_b&= 1 - (1-p_c)^{b/c} 
% \end{equation}

\[
\pbdue= 1 - (1-\pcdue)^{b/c}
\]
\[
\pbnde= 1 - (1-\pcnde)^{b/c}
\]

We next provide analytical models for replication and erasure coding.

\mypar{Replication}
For each logical block, the replication protocol maintains a primary physical block and a sequence of N-1 backup physical replica blocks, with the physical block size equal to the logical block size. 
% For example, for a 4 KB virtual page, each physical block size is also 4KB.

When a compute node needs to read a logical block, it first reads the primary physical block. If the read fails because of a DUE, then it tries the next backup physical block in the sequence, continuing this
process until it successfully reads a block. 
When the compute node exhausts trying all available physical blocks without successfully reading one, the logical-block read fails with a DUE.

Hence, the probability to have a DUE when reading a logical block is the joint probability of all physical-block reads to fail:

\[
\pldue = \pbdue^{N}
\]

The probability to have a NDE when reading a logical block is the union probability of any physical-block reads to fail due to NDE:

\[
\plnde = \sum_{i=0}^{N-1} (1-\pbdue)^i \pbnde
\]

The average number of additional physical blocks that are read after failing to read the first physical block is:

\[
\begin{split}
a_r&= -1 + \sum_{i=0}^{N-1} \pbdue^i(1-\pbdue)(i+1)
\end{split}
\]

% \[
% \begin{split}
% a_r&= -1 + Np^{N-1}+\sum_{i=0}^{N-2} p_b^i(1-p_b)(i+1)
% \end{split}
% \]

\mypar{Erasure Coding}
Erasure coding provides redundancy without the overhead of complete replication.
Erasure coding divides a logical block into K physical blocks,
and then encodes these blocks using Reed-Solomon code RS(K, N)~\cite{reed:code:journal-applied-math:1960} to generate R parity blocks, for a total of N=K+R blocks.
It finally writes each of these N blocks to a different remote memory node. 
Physical block size is equal to $\frac{\text{logical block size}}{K}$.
% For example, for a 4 KB virtual page, each physical block size is $\frac{4}{K}$KB.

When a compute node needs to read a logical block, it can perform the read using any K physical blocks of the N physical blocks. If the compute node fails to read any of the physical blocks (due to a DUE), then it tries another physical block. 
When the compute nodes exhausts trying all available physical blocks without successfully reading K blocks, the logical-block read fails with a DUE.

Hence, the probability to have a DUE when reading a logical block is the probability to have at least N-K+1 physical blocks fail due to a DUE:

% \begin{equation*}
% \pldue = \sum_{i=N-K+1}^{N}\binom{N}{i}\pbdue^{i}(1-\pbdue)^{N-i}
% \end{equation*}

\begin{equation*}
\pldue = \sum_{i=N-K+1}^{N}f(i,N,\pbdue)
\end{equation*}

\begin{equation*}
\mathrm{where}\quad
f(k,n,p) = \binom{n}{k}p^{k}(1-p)^{n-k}
\end{equation*}

The probability to have a NDE is the number of ways in which we can read K physical blocks plus extra physical blocks due to DUE multiplied by the probability to have physical blocks fail due to DUE multiplied by the probability to have at least one physical block fail due to a NDE: 

\begin{multline*}
\plnde = \sum_{i=0}^{N-K}\binom{N}{K+i}f(i,K+i-1,\pbdue) \\ 
\times \sum_{j=1}^{K-1}f(j,K-1,\pbnde)
\end{multline*}

The average number of additional physical blocks that are read after failing to read any of the first K physical blocks is:

% \begin{equation*}
% a_r= -K + \sum_{i=0}^{N-K} \binom{N}{K+i} \binom{K+i-1}{i}\pbdue^{i}(1-\pbdue)^{K-1}(K+i)
% \end{equation*}

\begin{equation*}
a_r= -K + \sum_{i=0}^{N-K} \binom{N}{K+i} f(i, K+i-1, \pbdue) (K+i)
\end{equation*}

\noindent where each sum term is the number of ways in which we can read physical blocks multiplied by the probability to have physical blocks fail due to DUE multiplied by the number of physical blocks read.