\section{Background \& Motivation}
\label{sec:background}

\subsection{Memory Technologies}
DRAM is the dominant type of memory technology used for main memory in both server and client systems. 
Each DRAM cell stores a bit as a capacitor charge, accessed by a transistor. 
However, as DRAM technology scales to smaller cell geometries, it encounters significant challenges related to reliability~\cite{lee:ddr5:iedm:2023, patil:dve:isca:2021}.

Non-volatile memory (NVM) technologies, such as phase-change memory (PCM), spin-transfer torque RAM (STT-RAM), and resistive random access memory (ReRAM), are emerging as viable alternatives to DRAM for main memory due to their higher storage density and competitive performance~\cite{lee:pcm:isca:2009, kultursay:sttram:ispass:2013, xu:reram:hpca:2015}. 
For example, the now discontinued Intel Optane DC Persistent Memory (DCPMM) DCPMM offered increased capacity with 300ns read latency ~\cite{yang:optane-guide:fast:2020}, while Weebit ReRAM offers 20ns read latency \cite{weebit:skywater-ip}.


% and commercially available Intel Optane DC Persistent Memory (DCPMM), provide byte-addressable persistent storage accessible via load/store instructions, rather than I/O requests. In addition to non-volatility, these technologies provide the potential for increased memory density and increased energy efficiency relative to DRAM. 
% DCPMM has 2x higher read latency and 8x lower write bandwidth than DRAM, but it is up to 10x faster than Flash. %and supports up to 3 TB per processor. 

\subsection{Memory Systems}

\begin{figure}[!t]
\centering
\includegraphics[width=3.2in]{fig/direct-vs-disaggregated}
\caption{Memory system architectures}
\label{fig:memory-system-architectures}
\vspace{-0.5cm}
\end{figure}

DRAM server memory systems organize memory bit cells into arrays, which are grouped into chips, and further assembled into DIMM (Dual Inline Memory Module) modules. 
These modules are accessed by the memory controller through one or more channels, which coordinates data transfer between the processor and the memory system.
Data is transferred in memory blocks, with each block typically 64 bytes in size.
Although the microarchitecture design of NVM subsystems is more complex than conventional DRAM systems, at a high level, NVM systems follow similar chip structure and system organization as DRAM systems~\cite{yang:optane-guide:fast:2020, wang:model-nvm:micro:2020}.

Conventional Non-Uniform Memory Access (NUMA) systems consist of multiple sockets, each containing a processor with memory directly attached. 
Scaling up compute and memory capacity by adding more sockets is prohibitively expensive~\cite{barroso:wsc:book:2018, lim:memory-blade:isca:2009}, so datacenter systems typically scale out by adding additional server nodes, as shown in Figure~\ref{fig:memory-system-architectures}(a). 
However, since applications often exhibit imbalanced memory usage across server nodes, this can lead to underutilization of the aggregate memory, resulting in poor memory efficiency~\cite{lu:pond:asplos:2023, gu:infiniswap:nsdi:2017}.

Disaggregated memory architectures decouple the processor from memory into separate compute and memory nodes, which are interconnected by a high-performance system interconnect, as shown in  Figure \ref{fig:memory-system-architectures}(b)). 
Compute nodes mainly provide processing capability, but they may also include a small amount of local memory used as a local cache.
Memory nodes provide memory capacity by attaching standard memory subsystems to the network, either through an I/O interface, such as Remote Direct Memory Access (RDMA)~\cite{rdma-consortium}, or a memory semantics interface, such as Compute Express Link (CXL)~\cite{cxl-consortium}.
Overall, memory disaggregation allows separate evolution and scaling of processing and memory, which lets tailoring the compute-to-memory ratio to the specific needs of the workload, and improves memory utilization as memory is shared across multiple compute nodes.

\subsection{Memory Errors}
\label{sec:failure-model}


% Disaggregation provides separate fault domains between processing and memory, meaning that the failure of a compute node does not render disaggregated memory unavailable, and vice versa, that is when a memory node fails, compute and other memory nodes continue to function. 
% In this work, we focus on memory node failures caused by memory errors.

Data stored in a memory system is susceptible to memory errors, which can be classified into two main categories. 

\mypar{Cell Errors}
Memory cells are susceptible to both transient and permanent device faults.
DRAM cells have been traditionally susceptible to transient faults, such as radiation-induced errors~\cite{schroeder:dramfailures:sigmetrics:2009}. However, DRAM is increasingly vulnerable to permanent device faults due to manufacturing variability and defects arising from miniaturization~\cite{cha:dram-defects:hpca:2017}, as well as wear-out faults caused by aging~\cite{fieback:dram-aging:delft:2017}.
NVM cells can experience permanent faults due to limited and variable endurance, as well as transient faults caused by resistance drift and read disturb~\cite{yoon:freep:hpca:2011}.
These NVM cell errors are random in nature~\cite{zhang:pm-chipkill:micro:2018}. 
Raw bit error rate (RBER) in PCM and ReRAM is significantly higher than in DRAM and ranges from $10^{-3}$ to $10^{-5}$~\cite{zhang:pm-chipkill:micro:2018}, depending on the technology and time since last write or refresh.

\mypar{Non-Cell Errors}
Memory errors can also occur when other components of the memory system fail, including memory controller and memory channel failures. 
These can be caused by transient faults due to signal disturbances, as well as permanent failures resulting from faults in the logic and transmission circuitry~\cite{meza:dramfailures:dsn:2015}.
% Since NVM systems follow similar organization as DRAM systems, they will likely suffer from similar failures.

\subsection{Memory Error Protection}

To protect against memory errors, memory systems maintain error correcting codes (ECC) computed over data. 
These codes can detect and correct a small number of errors.
For example, single error correction double error detection (SEC-DEC) uses parity to detect up to two-bit errors or correct a single-bit error.
Chipkill uses wider ECC to protect against multi-bit errors and chip failures.
Detectable but uncorrectable memory errors (DUE), which are detected but cannot be corrected by ECC, can cause memory system failures.
Non-detectable memory errors (NDE), which are non-detected and potentially miscorrected by ECC, do not cause memory system failures but may cause silent data corruption (SDC), which is also higly undesirable. 

As memory technology continues to shrink and density increases, ECC storage overheads required to maintain reliability are growing. 
High-density DDR5 DRAM chips, for example, include on-die ECC, and DDR5 DIMMs double the number of error-correcting bits compared to DDR4 DIMMs~\cite{micron:ddr5:whitepaper}
Similarly, for dense NVM with high RBER, using stronger codes to achieve a low uncorrectable bit error rate (UBER) and low SDC rate incurs prohibitive storage overheads~\cite{zhang:pm-chipkill:micro:2018}.
These overheads remain significant ($\sim27\%$) despite recent efforts on improving storage
efficiency~\cite{zhang:pm-chipkill:micro:2018}. 
Additionally, stronger ECC may not be sufficient for addressing errors that occur when both ECC and ECC-protected data are colocated within the same failure domain, such as within a single memory channel or memory controller.

Fine-grain memory replication is emerging as an approach
to improving memory reliability by augmenting per-DIMM ECC with an additional layer of redundancy, ensuring that critical data has multiple copies across independent failure domains, such as different memory channels~\cite{zheng:raim:isca:2017} or memory controllers~ \cite{patil:dve:isca:2021}.
For example, Figure~\ref{fig:ramp-architecture} shows three applications A, B, and C with different degrees of replication.
Application A and B have two replicas per page so they can tolerate memory controller failures.


\ignore{
\subsection{Memory errors and their handling}
\label{sec:failure-model}
Disaggregation provides separate fault domains between processing and memory, meaning that the failure of a compute node does not render disaggregated memory unavailable, and vice versa, that is when a memory node fails, compute and other memory nodes continue to function.

In this work, we focus on memory node failures. These may happen due to several reasons. First, a memory node may fail due to a random NVM bit cell error. Bit cells are susceptible to permanent failures due to limited and variable endurance, and transient failures due to resistance drift and read disturb~\cite{yoon:freep:hpca:2011}. RBER in PCM and ReRAM is significantly higher than in DRAM, ranging from $10^{-3}$ to $10^{-5}$~\cite{zhang:pm-chipkill:micro:2018}.
%
Second, a memory node may fail due to a memory subsystem failure. 
At a very high level, NVM subsystems follow similar chip structure and system organization as DRAM subsystems, comprising a memory controller that is connected to multiple memory chips through one or more channels.
However, their microarchitecture design is more complex than conventional DRAM subsystems~\cite{wang:model-nvm:micro:2020}.
Hence, NVM subsystems will likely suffer from similar or more complex failures, including transient failures due to signal disturbances, and permanent failures due to faults in logic and transmission circuitry. 
%
Finally, a memory node may fail due to 
%failure in the power distribution units or 
failure in the network-interface card (NIC) that connects a memory node with the rest of the system.
}

\ignore{
Memory nodes incorporate hardware-level error-correcting code (ECC) mechanisms to protect against memory errors. Memory nodes fail when such protection mechanisms detect but cannot correct an underlying memory error. 

Tolerating NVM cell failures involves device- and architecture-level techniques that mitigate endurance-related permanent failures through write-efficient coding, memory remapping  and embedded redirection of failed lines8, , and mitigate transient failures through ECC8. However, maintaining a correctable error rate below 10-15 that is necessary with petabyte memory sizes expected in rack-scale DM requires using strong BCH ECC with high energy and die area overheads, up to 30% . Despite hardwareâ€™s best efforts, undetected memory errors may still trickle into software and cause disastrous silent data corruption12. 


Silent data corruption?

memory node failures can happen due to: cell, chip, controller, nic, failures
hardware-level protection techniques addrsss cell, chip failures
software-level replication addresses uncorrected errors


technology: persistent memory, nvram, 
failure model:
- disaggregation failures: compute nodes, memory nodes: memory controllers, bit errors, 
- memory bit errors: cite rber of pcm/reram

compute and memory nodes fail independently

How do memory nodes fail?

Bit errors in NVM, memory controller errors, circuit errors (how do these differ from bit errors?)
- sources of bit errors: nvm cells, circuit lines, etc. Here, we focus on nvm cells

ECC addresses bit errors

assumption: 
-memory nodes fail similarly to servers today?
-applications will rely on replication and erasure coding to recover from errors that are not recoverable with ECC 
-investigate bit errors as the primary failure model; won't model impact of memory controller and circuit failures. most likely, that will follow dram circuits. leave this as future work.

Hardware-level protection techniques, such as parity and chipkill-correct, employ redundancy to guard against bit corruption due to memory cell and/or chip failures. 

However, memory nodes can still fail due to memory controller 


Memory nodes can fail either due to 

why replication for performance? To avoid a single memory node becoming a performance bottleneck
why replication for availability? To tolerate failures that render the complete memory node unavailable and which cannot be addressed by bit and chip protection techniques. This includes memory controller failures, power delivery/supply?, NIC failures? 

Explore co-design of memory protection techniques for such applications
}