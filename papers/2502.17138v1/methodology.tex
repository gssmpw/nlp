\section{Methodology}
We outline the methodology used to evaluate \rampdm.

\mypar{Implementation}
We base our \rampdm prototype on the publicly available Hydra remote memory system~\cite{lee:hydra:github}, which we extend to handle synthetic memory faults as described below. 
We model the performance overhead of the chipkill design by throttling memory bandwidth to 8/9 of the available memory bandwidth utilizing the memory controller's thermal control registers~\cite{volos:quartz:middleware:2015} to account for the additional overhead to read 8B parity bits for every 64B block~\cite{zhang:pm-chipkill:micro:2018}.

\mypar{Fault Injection}
We model a target DUE rate through synthetic memory fault injection. Our aim is to inject faults that cause Machine Check Exceptions (MCEs) to exercise the complete hardware and software path for handling such interrupts. 
Although server systems and the Linux kernel provide various methods for injecting MCEs for testing, we find none of these methods suitable for our needs. 
Software-level simulation methods~\cite{linux:madvise-hwpoisson:2024, linux:mce-inject:2024} utilize virtual memory protection to poison the pages in a specified memory range, handling subsequent references to those pages like a hardware memory corruption. Hence, these methods do not simulate full MCE handling on the platform level.
In contrast, firmware-level simulation methods~\cite{linux:einj:2024} trigger actual MCEs, fully exercising MCE handling. Despite successfully triggering MCEs synchronously through direct API calls, we were unable to trigger them asynchronously through one-sided RDMA operations.
To address this gap, we developed a custom synthetic fault injection framework.
We extend the Resilience Manager with a fault injector that injects faults at a configurable rate. 
When a fault needs to be injected, the injector performs a two-sided send operation to the resilience monitor, triggering a synchronous MCE~\cite{linux:einj:2024}, rather than performing an one-sided RDMA operation.
This approach captures both the communication overhead of a one-sided RDMA operation that would fail due to a memory error and the overhead associated with handling the error-induced MCE.

\mypar{Cluster Setup}
We evaluate \rampdm on a cluster consisting of 12 machines interconnected via a 56 Gbps InfiniBand network, hosted on CloudLab~\cite{cloudlab:hardware}.
Each machine is equipped with a Xeon E5-2450 processor featuring 8 cores and 16 GB of physical memory. 
Each machine runs Linux kernel 4.4 with Mellanox OFED 4.1.
We run a single Resilience Manager on a single machine with a remote memory capacity of 1GB. 
The manager pages and replicates to remote memory allocated from six other machines, each running its own Resource Monitor.

\mypar{Workload Setup}
We evaluate \rampdm using Memcached~\cite{memcached}, a lightweight in-memory key-value store that is widely deployed as a distributed caching service to accelerate user-facing applications with stringent latency requirements~\cite{nishtala:memcached:nsdi:2013, yang:twemcache:osdi:2020}.
We run a single Memcached server process inside an LXC container on the machine running the Resilience Manager. The container limits the local memory capacity to a configurable amount, effectively forcing Memcached satisfy any additional memory requirements by paging to remote memory through the Resilience Manager.
We drive Memcached using an extended version of the Mutilate load generator~\cite{leverich:mutilate:eurosys:2014} configured to recreate the ETC workload from Facebook~\cite{berk:facebook-kv-workload:sigmetrics:2012}, using one master and two workload-generator clients, each running on a separate machine. 
We populate Memcached with 3 million records, for a total footprint of 1GB memory.
We limit local memory to 25\% of the total footprint.

\newcommand{\chipkill}{\emph{Chipkill}\xspace}
\newcommand{\chipkillrep}{\emph{Chipkill-REP}\xspace}
\newcommand{\chipkillec}{\emph{Chipkill-EC}\xspace}

\mypar{Configurations}
We study two protection schemes: 
(i) \chipkill, a baseline scheme that uses chipkill alone without rack-scale redundancy, 
(ii) \chipkillrep, a rack-scale redundancy scheme that combines rack-scale replication with chipkill, and 
(iii) \chipkillec, a rack-scale redundancy scheme that combines rack-scale erasure coding with chipkill. 
For the two schemes, we choose parameters so that they can both tolerate up to two replica failures (following standard practice), that is N=3 for replication and N=6 and K=4 for erasure coding.
We vary storage overhead by varying the protection strength of the BCH code to protect individual replica blocks.
We vary strength by varying the number of $t$ bit errors that can be corrected by the BCH code.
We assume uniform access to all logical blocks and that all physical blocks are equally vulnerable to memory errors.
% All replicas use the same ECC.

\mypar{Memory Technology}
Unless otherwise specified, we assume a memory technology with $RBER=2\times10^{-4}$, as in~\cite{zhang:pm-chipkill:micro:2018}.
