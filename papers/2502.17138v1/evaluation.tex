\section{Evaluation}

\subsection{Performance Overhead due to Error Handling}

\begin{figure}[tb]
\centering
\includegraphics[width=3.25in]{fig/fault_rate-qps.pdf}
\caption{Memcached throughput as completed queries per second (QPS) for different DUE fault rates.}
\label{fig:throughput-with-failures}
\end{figure}

\begin{figure}[tb]
\centering
\includegraphics[width=3.25in]{fig/fault_rate-avg.pdf}
\includegraphics[width=3.25in]{fig/fault_rate-p99.pdf}
\caption{Memcached average and P99 response latency for different DUE fault rates.}
\label{fig:latency-with-failures}
\end{figure}

Figures~\ref{fig:throughput-with-failures} and \ref{fig:latency-with-failures} illustrate the throughput and response latency performance of the two rack-scale configurations, \chipkillrep and \chipkillec, with DUE fault rates of individual replicas ranging from $10^{-2}$ to $10^{-9}$. 
Performance overhead includes the impact of replication and erasure coding.
We find that the chipkill bandwidth overhead does not have impact on end-to-end performance as network bandwidth (56 Gbps = 7GB/s) is the bottleneck, despite throttling memory bandwidth to 19GB/s to account for the chipkill overhead.

As predicted by the analytical model, at low DUE rates, accessing additional replicas is infrequent, leading to negligible performance overhead. The graphs demonstrate that this threshold is reached at a DUE rate of approximately $10^{-5}$, indicating that the increase in MCE rate due to the weaker protection strength of each replica does not significantly impact performance until the DUE rate becomes considerably higher. 
To better understand the performance overhead introduced by an MCE, we manually inject a synchronous MCE~\cite{linux:einj:2024} and measure the time required to handle it. Our results show that the average MCE handling latency is 200 $\mu$s, with occasional spikes up to 1 second. This latency helps explain why, under aggressive fault rates, the performance impact can become so pronounced.

Interestingly, the \chipkillec configuration, which uses erasure coding, outperforms \chipkillrep, despite the added computational complexity of erasure coding. This can be attributed to the memory-bound nature of Memcached, which leaves the Resilience Manager with ample computational resources to handle the erasure coding process. Furthermore, \chipkillecâ€™s erasure coding approach enables multiple small accesses to different replicas in parallel, rather than a single larger page access to a single replica. This parallelism better utilizes available bandwidth across replicas and helps avoid potential bottlenecks at any one replica.

\subsection{Storage Savings}
We use the analytical model to study availability and storage-saving trade-offs for the two rack-scale configurations, \chipkillrep and \chipkillec, against the baseline \chipkill configuration.


\begin{figure}[tb]
\centering
\includegraphics[width=3.5in]{fig/due_vs_storage_overhead.pdf}
\includegraphics[width=3.5in]{fig/nde_vs_storage_overhead.pdf}
\caption{DUE and NDE versus storage overhead for different chipkill protection schemes.
The solid rectangle (in the top figure) marks the DUE and storage overhead of the original chipkill design~\cite{zhang:pm-chipkill:micro:2018}.
NDE is shown only for baseline chipkill as it is independent of replication and identical for all chipkill schemes.
}
\label{fig:storage-overhead-due}
\end{figure}

\begin{figure}[tb]
\centering
\includegraphics[width=3.5in]{fig/storage_overhead_vs_replication_factor.pdf}
\caption{Storage overhead versus number of replicas for different chipkill protection schemes.
}
\label{fig:storage-overhead-replicas}
\end{figure}

\begin{figure}[tb]
\centering
\includegraphics[width=3.5in]{fig/storage_overhead_vs_rber.pdf}
\caption{Storage overhead versus RBER for different chipkill protection schemes.
}
\label{fig:storage-overhead-rber}
\end{figure}

Figure~\ref{fig:storage-overhead-due} plots combined DUE rate and baseline NDE rate of individual physical blocks as a function of storage overhead.
For each replication scheme, the storage overhead is calculated over a corresponding baseline that employs the same replication scheme but without chipkill protection. 
For \chipkillrep, we use a physical block size equal to the virtual page size, that is 4KB.
For \chipkillec, we use a physical block size equal to 1KB ($\frac{4KB}{4}$).
We observe that both replication and erasure coding can achieve the same level of DUE as the original ckipkill design ($\sim10^{-33}$ DUE rate), albeit at about $9\%$ less overhead.
\revisionhighlight{
For a target SDC rate of $10^{-22}$~\cite{zhang:pm-chipkill:micro:2018}, we need to provision an extra $2.4\%$ overhead, bringing the storage  savings down to $6.6\%$.
}

Figure~\ref{fig:storage-overhead-replicas} plots the storage overhead sustained to achieve the same level of DUE as the original chipkill design as we vary the number of replicas.
We observe diminishing returns in storage savings as we increase the number of replicas, suggesting that \ramp could be more beneficial with low replication factors. 

Finally, Figure~\ref{fig:storage-overhead-rber} plots the storage overhead sustained to achieve the same level of DUE as the original chipkill design as we vary the raw bit error rate (RBER) of the memory technology.
We observe growing returns in storage savings as we increase the RBER, suggesting that \ramp could be more beneficial with higher-density memories exhibiting higher memory error rates. 

Overall, these results confirm our main hypothesis: by weakening the protection of each individual replica, we can lower the storage overhead while we can rely on the combined protection conferred by multiple replicas to meet a stronger protection target.

