
\subsection{Tolerating Memory Errors}
\ramp assumes a computing system tolerates memory errors through a two-tier protection scheme. 
The first tier, or memory-protection tier, is a performance-optimized tier that reuses the memory chip failure protection bits (\eg, ECC) to detect and opportunistically correct memory errors at high performance, while ensuring that no miscorrection occurs.
The second tier, or memory-replication tier, is a storage-optimized tier that employs memory replication across multiple memory nodes to correct memory errors that are uncorrected by the memory-protection tier, including errors due to cell failures, chip failures, channel failures, and complete memory node failures

The memory-protection tier provides configurable protection against memory errors.
It requires that the memory controller be able to compute and decode different codes, as well as a mechanism to determine which ECC technique to use for each memory access. 
For example, the memory-protection tier could support page-granularity protection by augmenting the virtual memory page table and TLB to include protection information for each page~\cite{yoon:virtualized-ecc:asplos:2010}. 
The configuration capability can range from allowing upper tiers select a memory protection scheme from a fixed set of protection schemes (\eg, SEC-DEC, chipkill) to providing complete flexibility in choosing the protection method and strength~\cite{yoon:virtualized-ecc:asplos:2010}.

Memory nodes that fail correction at the memory-protection tier report DUEs to the replication engine for further handling. 
Memory nodes can report DUEs either by piggybacking on coherence requests~\cite{patil:dve:isca:2021} or by leveraging hardware error reporting mechanisms, such as Intel Machine Check Architecture (MCA)~\cite{intel:mce:book:2024, xu:nova-fortis:sosp:2017}, to raise a machine check exception (MCE) in response to uncorrectable memory errors.
After reporting, a memory node remains operational and continues to serve memory accesses to non-failed
memory regions, thus improving availability. 
Depending on the overhead of the error reporting mechanism, the ability to opportunistically correct errors in the memory-protection tier, in addition to detection, becomes crucial for preventing severe performance slowdowns due to error reporting~\cite{meza:dramfailures:dsn:2015, barroso:wsc:book:2018}.

For each replicated data item, the memory-replication tier maintains multiple replicas across memory nodes. 
The memory-replication tier maps each replica to a memory node and memory region, and configures the hardware protection strength of each replica to meet a target UBER and SDC rate, following application requirements. 
% The memory-replication tier configures memory protection method (\eg, SEC-DEC, chipkill) and code strength at the granularity of individual pages. 
The memory-replication tier may track and blacklist failed memory regions to avoid mapping replicas to regions with known errors. 
When the memory-replication tier accessing a data item faces a DUE, it attempts to correct the memory error using another replica.

The memory-replication tier can implement any block-level static homogeneous replication method, including primary-backup replication, chain replication, quorum-based replication, and erasure coding. 
Static requires a fixed number of replicas whose protection strength does not change dynamically, thus relieving the replication engine from having to support frequent protection changes.
Homogeneous requires all replicas to have the same protection strength, thus simplifying replica strength reasoning.

\ignore{
    A lightweight service processor on the memory node handles the exception and returns an error as a response to the RDMA request by piggybacking on the existing error reporting mechanism of RDMA.
}
\ignore{
    To serve control plane operations and support fine-grain error reporting, the memory nodes include a lightweight service processor.
}

\ignore{
On read failure, it redirects the request to another replica.
On write failure, if the memory node remains operational, then it may attempt to write to another memory region within the same memory node. Otherwise it the memory node fails completely, software issues the writes to another memory node, and also remaps/migrates (asynchronously) all other replicas of the failed memory node. 
}

\ignore{
operation:
-compute nodes access memory using RDMA; rdma offers robust failure semantics compared to ld/st; piggyback on existing error reporting mechanism
-when an RDMA causes a memory side error, memory nodes do not crash, report operation failure, compute nodes recover by trying another replica, memory nodes remain functional
-memory nodes do not crash; instead poison affected region and continue servicing other requests (rely on an extended from of intel machine check architecture); current mce raises exception on read errors only. report both reads and non-posted-writes. we expect replication protocols to use non-posted writes to ensure writes reach nvm (cite snia)
}

\ignore{
Application software running on compute nodes may tolerate memory errors using application-level redundancy in the form of replication and checksumming.
\ramp does not dictate or implement a specific redundancy scheme, leaving the implementation to the application for maximum flexibility. 
Because targeted applications already employ redundancy for performance and availability (\cref{sec:ramp:idea}), we do not expect this requirement to significantly burden applications. 

Applications may use replication to tolerate DUEs. 
For each replicated data item, an application maintains multiple replicas across memory nodes.
Applications map each replica to a memory node and memory region, and configure the hardware protection strength of each replica to meet a target UBER and SDC rate.
Applications may also track and blacklist failed memory regions to avoid mapping replicas to regions with known errors.
When an application trying to access a data item faces a DUE, it attempts to correct the memory error using another replica. 

Applications may use checksumming to tolerate NDEs, including non-detectable bit cell errors and scribbles, that would otherwise silently corrupt data.
With checksumming, an application maintains a checksum for each data item.
When the application writes a data item, it computes and stores a corresponding checksum. When the application later reads the data, it may recompute the checksum and verify that the computed checksum matches the stored checksum.
}
\ignore{Application-level checksumming increases CPU utilization, but provides end-to-end protection against silent data corruption.}

\ignore{
Software running on a compute node accesses disaggregated memory using one-sided remote DMA (RDMA) reads and writes. 
When the network interface card (NIC) at a memory node receives an RDMA request, it performs a local DMA request to the node's memory controller, which in turn issues memory accesses to memory media.
The controller uses hardware ECC to detect and correct memory errors, and leverages existing hardware error reporting mechanisms, such as Intel Machine Check Architecture (MCA), to report DUEs. 
The controller transparently corrects correctable errors, and silently returns invalid data for undetectable errors.
For DUEs, the controller raises a hardware exception in response to uncorrectable memory errors. 
A lightweight service processor on the memory node handles the exception and returns an error as a response to the RDMA request by piggybacking on the existing error reporting mechanism of RDMA. 
After reporting the error, the memory node continues normal operation by servicing other pending RDMA requests. 
For error reporting mechanisms, that do not provide a mechanism for detecting store failures, like Intel Machine Check Architecture (MCA), the NIC issues an additional read after a write to check success of the write. 
}

Overall, \ramp enables co-designing memory replication together with memory protection to trade-off protection strength, storage efficiency, and performance.
\ignore{Maintaining multiple replicas across memory nodes enables the power of many choices. Instead of trying hard to eliminate uncorrectable memory errors within a single memory node using strong but expensive codes, \ramp accepts the possibility of uncorrectable errors.}
Computing systems that maintain multiple replicas across memory nodes can employ weaker but lower-storage-overhead ECC within individual replicas. 
While weaker ECC increases failure rate of individual replicas, a memory system can rely on the multiple choices offered by available replicas in other memory nodes to correct a memory error. 
% Similarly, memory systems can rely on checksums to detect silent corruptions that may evade the weaker ECC.
For example, in Figure~\ref{fig:ramp-architecture}, applications A and B have two replicas per page, so we can use weaker ECC, relying on the collective protection of multiple replicas to tolerate the increased per-replica error rate.
% Application C uses no replication so we deploy stronger ECC, as we rely exclusively on ECC to tolerate memory errors.



% For example, Figure~\ref{fig:ramp-architecture} shows three applications A, B, and C with different degrees of replication and levels of protection.
% Application A and B maintain two replicas per data item so they can use weaker ECC, relying on the collective protection of multiple replicas to tolerate the increased per-replica error rate.
% Application C uses no replication so it deploys stronger ECC as it relies exclusively on ECC to tolerate memory errors.
