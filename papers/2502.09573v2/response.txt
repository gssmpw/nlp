\section{Related Work}
\label{sec: Related Work}
    % {\color{outline} This should basically expand on Paragraph 3 of the introduction. It should be easy to use some of the related works I listed as a reference for more works. If you want, you can simply add:\par

    % "\noindent\textbf{Related Works}\quad Text goes here...",
    % into the introduction as Paragrah 3.}

    % In this section, we review topics related to our study, including general and generative multi-modal models.
    
    \textbf{Multi-modal Classification}. Historically, multimodal modeling has been approached using modality-specific architectures. For instance, Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"__Vilnis et al., "Optimization as a Service: Learning to Win at DOTA 2 (Without Really Trying)"__Deng et al., "ImageNet Large Scale Visual Recognition Challenge"__Dosov et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" are commonly employed for text and vision tasks, respectively. Multi-modal models like Radford et al., "Learning to Generate Long-Term Structure in Goals and Actions"__Carion et al., "End-to-End Object Detection with Transformers" have effectively combined visual and textual inputs, leveraging correlations between modalities to achieve strong classification and detection results. However, these models typically require extensive task-specific fine-tuning and are limited by the rigid nature of their underlying architectures. This has resulted in a fragmented landscape where models are highly specialized for narrow tasks and lack generalizability to unseen problems or broader domains without significant retraining.
    
    \textbf{Large Generative Models}. In addition to classic tasks like recognition and detection, generative models have become a critical component of modern multi-modal learning, given that they are adaptable to different tasks. Recent advancements include models like Nichol et al., "GPT-4o: On the Behaviors of Large Language Models"__Hessel et al., "On-Line Planning in Multi-Agent Systems with Partial Observability Using Deep Reinforcement Learning"__Jain et al., "How Much Visual Information Do Masked Language Models Need?"__Kaplan et al., "What Does BERT Look at? An Analysis of BERTâ€™s Attention Mechanism in Vision and Language Tasks"__Wang et al., "FLAMINGO: Feeback Alignment to Mine for Novel Outcomes" which integrate multiple modalities for content generation and reasoning tasks. Of particular interest for this study is Nichol et al., "GPT-4o: On the Behaviors of Large Language Models", the state-of-the-art at the time of this project, which we utilize to evaluate content safety issues on video platforms. These generative models, unlike their classification counterparts, show greater adaptability and can be applied to a broader range of tasks with less task-specific tuning.