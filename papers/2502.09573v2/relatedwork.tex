\section{Related Work}
\label{sec: Related Work}
    % {\color{outline} This should basically expand on Paragraph 3 of the introduction. It should be easy to use some of the related works I listed as a reference for more works. If you want, you can simply add:\par

    % "\noindent\textbf{Related Works}\quad Text goes here...",
    % into the introduction as Paragrah 3.}

    % In this section, we review topics related to our study, including general and generative multi-modal models.
    
    \textbf{Multi-modal Classification}. Historically, multimodal modeling has been approached using modality-specific architectures. For instance, BERT~\citep{devlin2018bert}, RoBERTa~\citep{liu2019roberta}, ResNet~\citep{he2016deep}, and ViT~\citep{dosovitskiy2020image} are commonly employed for text and vision tasks, respectively. Multi-modal models like CLIP~\citep{radford2021learning} and GLIP~\citep{li2022grounded} have effectively combined visual and textual inputs, leveraging correlations between modalities to achieve strong classification and detection results. However, these models typically require extensive task-specific fine-tuning and are limited by the rigid nature of their underlying architectures. This has resulted in a fragmented landscape where models are highly specialized for narrow tasks and lack generalizability to unseen problems or broader domains without significant retraining.
    
    \textbf{Large Generative Models}. In addition to classic tasks like recognition and detection, generative models have become a critical component of modern multi-modal learning, given that they are adaptable to different tasks. Recent advancements include models like Flamingo~\citep{alayrac2022flamingo}, OpenFlamingo~\citep{awadalla2023openflamingo}, LLaVA~\citep{liu2024visual}, InstructBLIP~\citep{dai2023instructblip}, and GPT-4o~\citep{achiam2023gpt}, which integrate multiple modalities for content generation and reasoning tasks. Of particular interest for this study is GPT-4o, the state-of-the-art at the time of this project, which we utilize to evaluate content safety issues on video platforms. These generative models, unlike their classification counterparts, show greater adaptability and can be applied to a broader range of tasks with less task-specific tuning.