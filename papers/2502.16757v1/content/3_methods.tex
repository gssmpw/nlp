\section{Methods}

% \heng{Figure 2 is very difficult to understand. make it clear by adding labels to which one indicate system/ground truth?}


\subsection{Entailment Preserving Rate (EPR)}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/metrics.pdf}
    \caption{Comparison between EPR, EPR@K, and EPR@K-Oracle. $K=3$ FOL representations (orange) are generated from NL sentences (gray) using the translator $S$, and curved arrows represent entailment-preserving combinations. EPR only uses top 1 predictions for each sentence (red box), where EPR=0/2 because there are no entailment-preserving combinations. EPR@K uses all $K$ predictions (green box) which contain such combinations for both (a) and (b), having a value of 2/2. Finally, EPR@K-Oracle (blue) selects one parse from each sentence that maximizes the global EPR value. In this example, there is no such selection that preserves entailment in both (a) and (b), resulting in EPR@K-Oracle=1/2.}
    \label{fig:metrics}
\end{figure}

Evaluating the quality of FOL representations is challenging because (1) a sentence can have multiple semantically valid FOL representations, and (2) natural language entailment datasets do not usually provide reference FOL representations. In this study, we define \textbf{Entailment-Preserving Rate (EPR)} and its variants, a suite of reference-free metrics that only require entailment labels.

Given an RTE dataset consisting of premises-hypothesis pairs, the EPR of a \nltofol\ translator $S$ is measured by the ratio of the pairs where $S$ can preserve the entailment. Specifically, the translator $S$ translates each premise and hypothesis to FOL representations, obtaining the premise representations $S(P) = S(p_1), ..., S(p_N)$ and the hypothesis representation $S(h)$. Then, an automatic prover is used to determine if $S(h)$ can be proved from $S(P)$\footnote{Contradiction in RTE can be proved by proving $\lnot S(h)$ instead of $S(h)$, therefore it can be treated the same as entailment without loss of generality.}. Finally, a verification step is imposed to filter out spurious entailments caused by contradiction (Appendix \ref{sec:appendix-epr}).

Note that this definition is completely \textit{reference-free} because it does not require a comparison between predicted and gold FOL representations. Furthermore, EPR allows FOL representation to have arbitrary predicate names and logical structures as long as they combine with others to complete a proof, being robust to \textit{arbitrariness}.

% two natural 'loose' extensions
We also propose a natural loose extension of EPR to exploit multiple outputs that can be obtained by beam search and sampling. \textbf{EPR@K} allows up to $K$ different parses for each premises and hypothesis. If \textit{any} combination of FOL representations selected from each premise and hypothesis preserves the entailment, it is considered a success.

Finally, \textbf{EPR@K-Oracle} allows only one FOL representation per each premise and hypothesis, similar to EPR. However, instead of selecting the output with the highest model-assigned probability as EPR, outputs are selected from each sentence to maximize the global EPR. This can be viewed as adding an \textit{oracle reranker} to the model. As this constrained optimization problem is NP-complete, we use Answer Set Programming \citep{asp} to get an approximate solution (details in Appendix \ref{sec:appendix-oracle}).
% \heng{notation here is confusing because i is used for two different indices} \jinu{removed confusing $j_i$ notation}

This inequality holds by definition: EPR $=$ EPR@1 $\leq$ EPR@K-Oracle $\leq$ EPR@K. Notably, EPR@K-Oracle serves as an \textit{observable upper bound} for EPR. If a model achieves a specific EPR@K-Oracle score, it indicates the potential existence of a model that achieves an EPR score equal to that value by generating the parses selected by EPR@K-Oracle.

% \subsection{Baselines}

% As a primitive baseline, we train a FOL semantic parser by fine-tuning T5-base \citep{t5} using the NL-FOL pairs from the MALLS dataset \citep{malls}. As the MALLS dataset is out-of-distribution from the target NL-only datasets, one cannot expect this model to preserve entailment relations in unseen data. This base model is referred to as \texttt{T5-OOD}.

% We implement several LLM-based methods as our baseline. \texttt{LLM-Sentence} is a few-shot LLM that translates an NL sentence to its FOL representation. \texttt{LLM-Problem} takes the whole set of premises and hypotheses as the input and outputs all FOL representations at once.

% Improved versions of \texttt{LLM-Problem} are also thoroughly evaluated. \texttt{LINC} \citep{linc} adds majority voting to \texttt{LLM-Problem}, sampling $K$ FOL representations for each premises-hypothesis pair and deciding the RTE label. % LogicLM

\subsection{Iterative Learning-to-rank}

In this section, we describe the \textbf{iterative learning-to-rank}, a training method specifically designed for the EPF task.

\begin{figure*}[th]
    \centering
    \includegraphics[width=\linewidth]{figures/training_loop.pdf}
    \caption{Iterative Learning-to-rank approach to train an entailment-preserving \nltofol\ translator. (1) For each premise and hypothesis, multiple FOL representations are sampled using beam search. (2) An external solver counts all entailment-preserving combinations and assigns scores. (3) Finally, the learning-to-rank objective BRIO is applied to reward the outputs participating in the most entailment-preserving combinations, indirectly increasing the overall EPR. This training loop (1-3) is repeated for multiple iterations to maximize performance.}
    \label{fig:train-loop}
\end{figure*}

\subsubsection{Scoring function}
\label{sec:scoring-func}

% \heng{add NL sentences into Figure 3 to make it easier to understand} \jinu{it was already there, in the leftmost part of the figure}
The entailment-preserving rate is a dataset-wide metric, but a sentence-to-sentence translation model can only learn from sentence-level rewards. Therefore, we define a sentence-level scoring function such that optimizing it will naturally enhance the global EPR score.

Given a model output $S(p)_{j}$, we define the score as the \textit{number of entailment-preserving combinations} of parses that include $S(p)_{j}$. If the parse contains a syntax error, the score $-1$ is assigned. If a sentence is included in multiple premise-hypothesis pairs, we sum the values obtained from all pairs.

For instance, consider the example in Figure \ref{fig:train-loop}. There are two entailment-preserving combinations annotated with curved arrows. As the second output from \textit{Eruption produces ash clouds.} (darkest orange) is included in both combinations, the score of this output is 2. For other outputs included once (orange), the score 1 is assigned, and outputs that are not included in any combination (lightest orange) get a zero score. Finally, ones that have a syntax error (gray) are assigned a score of -1.

% Formally, the score can be expressed as follows:

% \small
% \[\
% \texttt{score}(S(p_i)_{j_i}) =
% \begin{cases} 
%   \mid \{ (\mathcal{P}, h_j)\ |\ \mathcal{P} \vdash h_j \ \land \ S(p_i)_{j_i} \in \mathcal{P} \}\mid \\
%   -1\ \ \ \ \ \ \  \text{if } S(p_i)_{j_i} \text{ has a syntax error} \\ 
% \end{cases}
% \]
% \normalsize

% where $\mathcal{P}$ is the specific combination of FOL representations selected one from each NL premise ($\mathcal{P} \in \{\{S(p_i)_{j_i} \mid i = 1...N \} \mid 1 \leq j_i \leq K)\}$. 

\subsubsection{Learning-to-rank}

If a score is assigned to each output, theoretically any feedback-based learning objective \citep{mrt, ppo, ppr} can be applied to maximize it. In this work, we specifically use BRIO \citep{brio}, a learning-to-rank training objective originally introduced for abstractive summarization models.

Intuitively, in Figure \ref{fig:train-loop}, BRIO tries to increase the average token probability of outputs with higher scores compared to ones with lower scores for each row (same input). Formally, the BRIO training objective $\mathcal{L}_{BRIO}$ is defined as:

\small\[
\mathcal{L}_{BRIO} = \sum_{i}\sum_{j}max(\hat{p}(y_j|x) - \hat{p}(y_i|x) + \Delta(j-i), 0)
\]\normalsize
where $1 \leq i < j \leq K$ denote the indices of outputs $y$ sorted in descending order of the scoring function ($y_1$ having the highest score), $\hat{p}(y|x)$ is the token log-probability normalized by sequence length, and $\Delta$ is the \textit{margin} hyperparameter.

Finally, the plain cross-entropy loss $\mathcal{L}_{CE}$ is added to prevent the model from losing original generation capability, resulting in the final loss function $\mathcal{L} = \mathcal{L}_{CE} + \lambda\mathcal{L}_{BRIO}$ where $\lambda$ is a \textit{mixing rate} hyperparameter. The details of training hyperparameters are included in Appendix \ref{sec:appendix-hyperparam}.

\subsubsection{Iterative training}

Iterative training, which repeats the process of sampling, evaluation, and training, is widely recognized for enhancing performance across various scenarios by enabling the model to deviate further from the original fine-tuned model \citep{iterrpo, multiturn-dpo}.

Initially, a base model $S_{0}$ is obtained by fine-tuning a sequence-to-sequence model on \nltofol\ parallel corpus using only the cross-entropy objective. Then, $S_0$ generates outputs using the training set, which is then evaluated using the scoring function presented in \ref{sec:scoring-func}. After that, a new model $S_{1}$ is trained on the outputs and scores obtained from $S_0$ using the BRIO loss. We repeat this iteration five times, resulting in six different models $S_{t=0..5}$.