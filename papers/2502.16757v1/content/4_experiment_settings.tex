\section{Experimental settings}
\subsection{Datasets}
\label{sec:datasets}

\input{table/datasets}

Three representative multi-premise RTE datasets, namely EntailmentBank \citep{entailmentbank}, eQASC \citep{eqasc}, and e-SNLI \citep{esnli}, are used for the experiments. The statistics of each data set are briefly introduced in Table \ref{tab:datasets}.

\textbf{EntailmentBank} provides \textit{entailment trees} with simple scientific facts as nodes. We decompose the trees into depth 1 subtrees which encode a single premises-hypothesis pair.

\textbf{eQASC} provides 2-hop explanations for a given hypothesis derived from QASC \citep{qasc}, a multiple-choice question dataset from the science domain.

\textbf{e-SNLI} extends the single-premise SNLI dataset \citep{snli} by adding \textit{explanations} to the original premise-hypothesis pairs. This can be viewed as the premise and explanation together entailing the hypothesis. Due to limited computation resources, we sample 100k premises-hypothesis pairs from the train set and use the original validation/test set without modification.

\subsection{\nltofol\ translator}

We use a sequence-to-sequence model, T5-base \citep{t5} with 220M parameters, as our \nltofol\ translator backbone. To obtain an initial model $S_0$, we take 34k pairs of natural language sentences and their LLM-generated FOL representation from MALLS \citep{malls},
% \heng{briefly explain how it works} \jinu{done!}
convert them into the NLTK format \citep{nltk} with a rule-based translator, and train the T5-base model with standard cross-entropy loss. We refer to this model ($S_0$) as \texttt{T5-Iter0}, and models obtained after the $N$-th iteration as \texttt{T5-iter}$N$. Note that the MALLS dataset is not intended to preserve the entailment relationship between sentences, which can be seen in the low EPR score of the initial model \texttt{T5-Iter0} (Table \ref{tab:main-results}).

\subsection{FOL Theorem prover}

For the automatic theorem prover that is used to check entailment, we use Vampire \citep{vampire}, one of the fastest provers currently available. As the generative models are trained on NLTK syntax, the model outputs are translated into Vampire-compatible format (TPTP \citep{tptp}) using a rule-based translator.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/main_results_merged.pdf}
    \caption{EPR (left), EPR@16 (right-solid), EPR@16-Oracle (right-dotted) per iteration. The continuous growth in all EPR metrics implies that the model extrapolated to unseen premises-conclusion pairs where BRIO loss is 0, demonstrating the strength of the proposed method.}
    \label{fig:main-results}
\end{figure*}

\subsection{Baselines}

As a baseline, we adopt a wide variety of methods that translate natural language to first-order logic.

First, a suite of classic meaning representation-based methods was included as a baseline. \texttt{CCG2Lambda} \citep{ccg2lambda} obtains Combinatory Categorical Grammar (CCG) parse trees using C\&C Parser \citep{cncparser} and converts them to FOL representations via Lambda calculus. \citet{amr2folbos} and \citet{amr2fol} both convert Abstract Meaining Representations (AMR) to FOL. The AMR graph was obtained using AMRBART \citep{amrparser} and translated to FOL representations using the respective implementations of rule-based translators. These methods are only evaluated by EPR and not by EPR@K(-Oracle) as they are designed to produce a single gold FOL parse for each sentence in a deterministic manner. Any errors that occurred during obtaining the FOL representation were removed for the evaluation.

Few-shot and fine-tuned LLMs were also evaluated as a baseline. First, we sample $K=16$ FOL representations using GPT-4o and GPT-4o-mini \citep{gpt4o} with 5 in-context examples temperature 1.0 (prompts shown in Appendix \ref{sec:appendix-prompt}). Also, we evaluate LogicLLaMA \citep{malls}, a LLaMA-7B \citep{llama1} checkpoint directly fine-tuned on the MALLS dataset. $K=16$ FOL representations per each sentence were sampled using temperature 0.1, following the original paper.
