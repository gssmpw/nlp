\section{Related Work}

\subsection{First-order logic for natural entailment}

Since the start of the RTE challenge \citep{rte}, multiple works have attempted using FOL representations to solve natural language entailment. These methods first obtain the syntactic/semantic parse tree and apply a rule-based transformation to get the FOL representation \citep{bos-markert-2005-recognising, bos-nli}. However, it was repeatedly shown that these FOL representations are not empirically effective in solving natural language entailment. For instance, \citet{bos-nli} reported that FOL representations translated from the discourse representation structure (DRS) yield only 1.9\% recall in detecting the entailment in the single-premise RTE benchmark \citep{rte}.

Independently from these works, multi-premise logical entailment benchmarks \citep{tafjord-etal-2021-proofwriter, logicnli, folio} were developed to evaluate the reasoning ability of generative models. These benchmarks adopt the classic 3-way entailment label classification format (\textit{entailment, contradiction, neutral}) of single-premise RTE tasks, in which both the NL sentences and their gold FOL representations point to the same entailment label. 

Recent works have applied LLMs to obtain FOL representations for these multi-premise logical entailment tasks \citep{logiclm, linc, divide-and-translate}, fueled by the code generation ability of LLMs. While they achieve significant performance in synthetic, controlled logical reasoning benchmarks, whether they can generalize to natural entailment has remained unanswered. Furthermore, \citet{linc} observed that LLMs are highly susceptible to \textit{arbitrariness}, as they fail to produce coherent predicate names or numbers of arguments even when generating FOL representations of premises and hypotheses in a single inference.

\subsection{Executable semantic representations}

Apart from FOL, a stream of research focuses on the \textit{executability} of semantic representations. From this perspective, semantic representations are \textit{program codes} that can be executed to solve downstream tasks, such as query intent analysis \citep{spider, dligach-etal-2022-exploring} and question answering \citep{semparse-qa}. The performance of the semantic parser is directly assessed by the accuracy of execution results for the downstream tasks, rather than the similarity between the prediction and the reference parse.

To improve the execution accuracy that is often non-differentiable, reinforcement learning (RL) and its variants have been applied to train neural semantic parsers \citep{cheng-etal-2019-learning, cheng-lapata-2018-weakly}. Using only the input sentence and the desired execution result, these methods learn to maximize the probability of the representations that lead to the correct execution result. However, these approaches are not directly applicable to EPF, as EPF requires taking account of \textit{interactions between premises and hypotheses} during execution (\textit{i.e.} theorem proving) while these methods assume that sentences are isolated.

