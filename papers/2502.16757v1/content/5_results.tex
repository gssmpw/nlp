% \heng{analysis is a bit dry. try to add more qualitative analysis}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/arbitrariness.pdf}
    \caption{While the FOL premises in \textbf{(a)} can entail the hypothesis (from Figure \ref{fig:entailment-preserving}), \textbf{(b)} cannot due to arbitrariness in predicate name and arity. During the iterative training, \textbf{(c)} the arbitrariness in predicate names decreases after the first iteration, and \textbf{(d)} the arity entropy is significantly reduced. These two results demonstrate that the proposed iterative learning-to-rank method can effectively reduce the arbitrariness of FOL parses.}
    \label{fig:arbitrariness}
\end{figure*}

\section{Results}
\label{sec:result}

% \subsection{Entailment-preserving rate (EPR)}
\label{sec:epr}


\input{table/epr_results}

The results for EPF on three benchmarks are presented in Table \ref{tab:main-results}.

The results show that classic meaning representations-based methods (\texttt{CCG2Lambda}, \texttt{AMR2FOL}) achieve extremely low EPR in multi-premise RTE datasets, consistently falling short under 0.1\% EPR in EntailmentBank and eQASC. This extends the previous negative results in single-premise RTE datasets \citep{bos-nli, whenlihelps} to multi-premise RTE. On the other hand, end-to-end generative models (\texttt{GPT-4o}, \texttt{LogicLLaMA}, \texttt{T5-Iter0}) also demonstrate low EPR score regardless of the model size or whether it is fine-tuned for \nltofol\ translation or not. Although LLM-based generative methods achieve strong performance in logical entailment tasks \citep{logiclm, linc}, the results show that they do not generalize well to natural language entailment.

As shown in Figure \ref{fig:main-results}, iterative learning-to-rank training can significantly increase the EPR score, resulting in +1.8-4.2p gain in EPR and +22.3-27.8p in EPR@16 after five iterations (\texttt{T5-Iter0}$\rightarrow$\texttt{T5-Iter5}). The BRIO objective provides training signals only for inputs with differing output scores, such as when one output preserves entailment and another does not. The increase of the EPR@K score implies that outputs that previously had zero scores now preserve entailment and have positive scores; indicating that the model \textit{extrapolated} to unseen cases. This demonstrates the effectiveness of the score function defined in Section \ref{sec:scoring-func} and the BRIO learning objective for EPF.

Notably, EPR@16-Oracle scores are significantly higher than EPR and close to the EPR@16 score with only a 1.7p difference in EntailmentBank\footnote{The EPR@16-Oracle scores are a conservative approximation of the \textit{true} EPR@16-Oracle score due to NP-complete evaluation, which favors EntailmentBank that has the smallest test set.}, even though the EPR@16-Oracle score only uses a single FOL parse for each sentence. This implies that the gap between the current state-of-the-art EPR score and the observable upper bound is large, leaving room for future improvement.

Examples of FOL representations sampled from \texttt{T5-Iter0} and \texttt{T5-Iter5} can be found in Appendix \ref{sec:appendix-examples}.

\section{Analysis}
\label{sec:analysis}

% \subsection{Reranking ability}

\subsection{Arbitrariness}
\label{sec:arbitrariness}

Arbitrariness can be defined as assigning inconsistent predicate signatures (name and arity (number of arguments)) for synonymous concepts. Arbitrariness is critical for generative \nltofol\ translators in the EPF task. For instance, FOL representations in Figure \ref{fig:arbitrariness}(b) cannot entail the hypothesis because predicate names (\texttt{ash}$\leftrightarrow$\texttt{ashCloud}) and arities (\texttt{block} having 1 and 2 arguments) do not match, even though the FOL representations are semantically plausible.
% As pointed out by \citet{linc}, an entailment-preserving parser should effectively reduce arbitrariness throughout the dataset. However, this optimization should be done \textit{globally}, as training signals can only be generated from the interaction between different FOL parses.
We show that the proposed method effectively reduces arbitrariness in both predicate names and their arity, which contributes to the overall EPR gain.

\subsubsection{Unique predicate names per sentence}

Unique predicate names per sentence can be measured by counting all predicate names and dividing by the number of NL sentences. If the number of unique predicates decreases, it implies that synonymous concepts are mapped to fewer predicates. For instance, unique predicate names per sentence are 1 in Figure \ref{fig:arbitrariness}(a), but 1.33 in Figure \ref{fig:arbitrariness}(b) due to \texttt{ashCloud} and \texttt{ash} being separated.

% Atomic propositions can be defined as predicate-argument structure, \textit{e.g.} \texttt{eruption(x)}, \texttt{block(ash, sunlight)}. An increasing number of atomic propositions implies that each FOL parse contains more information, which is a sign of reduced brittleness.

The results (Figure \ref{fig:arbitrariness}(b)) show that after the first iteration (\texttt{Iter1}), the number of unique predicate names constantly decreases in all datasets, indicating reduced arbitrariness.


\subsubsection{Arity entropy}
End-to-end generative models often fail to generate predicates with consistent \textit{arity}. As the same predicates with different arities cannot lead to a successful proof, it is important to suppress such divergence.

To measure such variance, we adopt a new metric, \textit{arity entropy}. For each predicate, the entropy of the probability distribution of a predicate's arities (\textit{ArityEnt}), \textit{i.e.}
\[
\textit{ArityEnt} = -\sum_{a=1}^{\textrm{max}(a)} p(a)\textrm{log}_2 p(a) \
\]
, was measured. Lower \textit{ArityEnt} indicates that the model prefers a single arity for a given predicate throughout the entire dataset. For instance, \texttt{T5-Iter0} generates \texttt{CausesCycles()} predicate with 2 and 3 arguments 10 and 4 times respectively within the EntailmentBank dataset, resulting in \textit{ArityEnt}=0.86. However, \texttt{T5-Iter5} only generates \texttt{CauseCycles()} with 2 arguments, having \textit{ArityEnt}=0.

Figure \ref{fig:arbitrariness}(c) shows that during iterative training, the average \textit{ArityEnt} of predicates decreases on all three datasets. The result also shows that our method can effectively reduce arbitrariness in order to preserve entailment.

\subsection{Inference types}

\input{table/entailmentbank_inference_types}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/reasoning_type.pdf}
    \caption{EPR@16 per iteration measured for each inference type in EntailmentBank. Deductive inference (\textit{blue}) and inductive inference (\textit{orange}) both achieve performance gain during the iterative training.}
    \label{fig:reasoning_type}
\end{figure}

To evaluate the robustness of the proposed method against diverse lexical and syntactic patterns, we analyze the EPR in various \textit{inference types}. \citet{entailmentbank} identified three \textit{deductive}, three \textit{inductive} inference types in the EntailmentBank dataset (Table \ref{tab:inference-types}). Deductive inferences are often purely lexical and syntactic as in \textit{An animal is a kind of orgasm. A dog is kind of animal. $\vdash$ A dog is a kind of orgasm.} (Sub), while inductive inferences exhibit non-trivial logical structures, \textit{e.g.} \textit{Hunting is a kind of method for obtaining food. Animals require food for survival. $\vdash$ Some animals must hunt to survive.} (IC) We manually labeled 264 examples from the EntailmentBank test set and measured the EPR@16 score for each reasoning template.


The results (Figure \ref{fig:reasoning_type}) demonstrate that our method consistently improves EPR@16 across all inference types, achieving gains ranging from 5.6p to 25.3p. This shows that our method is robust to the diversity of reasoning patterns (inductive or deductive), and low-resource settings where the proportion of a specific pattern in the training set is as low as 3\%.
% As FOL predicates are mostly defined at the word level, it might favor word-level inference types such as \texttt{Sub} over phrase-level inference (\textit{e.g.} \texttt{IR}).

% Notably, these improvements are not confined to deductive reasoning steps but also extend to \textit{inductive} reasoning steps (Orange lines). Even if the representations are not 

\subsection{Out-of-domain generalization}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/cross_dataset.pdf}
    \caption{Out-of-domain EPR@16. A model trained on one dataset (\textit{colored}) achieves better performance than the model trained solely on MALLS (\textit{gray}) in other datasets. Hatched items refer to in-domain performance, where the trained and evaluated data are the same.}
    \label{fig:cross_dataset}
\end{figure}

Out-of-domain generalization is crucial for semantic parsers to cover diverse use cases not seen in the training dataset. We evaluate the out-of-domain generalization by evaluating a model trained on one dataset (\textit{e.g.} EntailmentBank) on another dataset (\textit{e.g.} eQASC).

The results are shown in Figure \ref{fig:cross_dataset}. Models trained for five iterations in any dataset consistently outperform \texttt{Iter0} in all out-of-domain settings, which implies that (1) RTE datasets share similar logical structure and (2) the model was able to learn the common structure between multi-premise datasets using the BRIO objective.

The strong out-of-domain generalizability of \texttt{Iter5(e-SNLI)} and relatively low performance on e-SNLI of \texttt{Iter5(EB/eQASC)} demonstrates the effectiveness of e-SNLI for training a \nltofol\ translator. This highlights the importance of wide semantic coverage and large train data in data-driven approaches for the EPF task.
