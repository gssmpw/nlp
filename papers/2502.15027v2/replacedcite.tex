\section{Related Work}
\textbf{Large Multimodal Models.}
The LLaVA-series works____ demonstrate that training with supervised fine-tuning (SFT) multimodal data and expand the vision lens would produce compatible multimodal reasoning ability. By adopting a large-scale image-text corpus for instruction tuning, Qwen2-VL____, CogVLM____, InternVL2 ____ have achieved exceptional performance on various multimodal abilities. Moreover, Molmo ____ proposes to train an LMM from scratch with only the human-annotated data. Unlike these large models, MiniCPM-V ____ and Phi-3.5-Vision ____ propose to train lightweight yet SOTA LMMs. Despite their exceptional performance on multimodal benchmarks of varying difficulty, such as MMMU-Pro ____ and MathVista ____, it remains unclear how well these LMMs demonstrate interactive intelligence in Human-AI Interaction scenarios. In this paper, we conduct the evaluation of these LMMs to explore this basic yet vital capability (i.e., evolve through interactive human feedback).

\textbf{Multimodal Benchmarks.}
Traditional vision-language benchmarks focus on visual question answering____, image captioning____, as well as other benchmarks for specialized scenarios such as scene text understanding____, commonsense reasoning____, outside knowledge____. The recent development of LMM posts a strong need for modernized multimodal benchmarks____ such as MMBench____, MMMU-pro ____, and MathVerse ____ which involve comprehensively evaluating current LMMs on various multimodal abilities. However, these benchmarks primarily focus on static testing processes, overlooking the interactive testing process that is vital in human-AI interaction scenarios.

\textbf{Human-AI Interaction.}
Investigating how humans and AI systems communicate and collaborate is critical for shaping applications such as virtual assistants ____, personalized recommendations ____, autonomous vehicles ____, and healthcare diagnostics ____. Recent LLMs-driven techniques such as memory____ and iterative____ mechanisms offer expert-level collaboration. While LMMs ____ excel in multimodal tasks, their potential for HAI problem-solving ____ remains underexplored. By offering a unified framework and meticulously curated data, our \bench{} enables evaluation of LMMs on these capabilities and lays a foundation for advancing multimodal HAI problem-solving.

\textbf{User Stimulation with LLM.} Recently, previous work in order to build multi-agent system ____, stimulate human-AI interaction ____, evaluate LMMs in video analysis ____, stimulate the real user in a web shopping scenario ____, evaluate the conversational recommender systems ____ determine to use LLM or LMM to stimulate the user. However, previous works have overlooked the importance of ensuring the reliability of LLMs or LMMs that are used to stimulate the users. In this paper, we curate test data by selecting only the samples that LMMs correctly address, minimizing unreliable interaction results.



\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.92\linewidth]{figures/dataconstruct.pdf}
    \vspace{-1mm}
    \caption{Overview of the test data construction process for \bench{}. For each LMM serving as the feedback receiver, we process each instance from a target dataset (e.g., MathVerse) and collect the error cases to form a negative set. The feedback provider then processes the same instances to build a positive set. Finally, we curate test data by selecting the intersection of both sets.}
    \label{fig:dataconstruct}
\end{figure*}