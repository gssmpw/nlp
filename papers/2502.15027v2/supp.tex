\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\finalcopy
\input{math_commands.tex}

% \usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{xcolor}         % colors
\usepackage{color, colortbl}

\definecolor{backred}{RGB}{255, 190, 190}
\definecolor{backblue}{RGB}{210, 230, 250}

\newcommand{\mhy}[1]{\textcolor[rgb]{0,0,1}{#1}}

\definecolor{citecolor}{HTML}{2980b9}
\definecolor{linkcolor}{HTML}{c0392b}

\usepackage[hidelinks,breaklinks=true,colorlinks,bookmarks=false,citecolor=citecolor,linkcolor=linkcolor]{hyperref}


\definecolor{verylightgray}{gray}{0.95}

\newcommand\bench{InterFeedback-Bench}
\newcommand\agent{InterFeedback}
\newcommand\dataset{InterFeedback-Human}

% \title{\agent{}: Unveiling the Interactive Intelligence of Large Multimodal Models}

\title{\agent{}: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback (Supplementary Material)}

% \title{\agent{}: Can Large Multimodal Models Evolve through Interactive Human Feedback? (Supplementary Material)}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\section{Additional Experimental Details}
\subsection{Model Sources.} For different LMMs, we select their latest models with sizes around 7B for evaluation. Table \ref{tab:modelsource} presents the release time and model sources of LMMs used in \bench{}.

\begin{table}[ht]
\vspace{-3mm}
\caption{The release time and model source of LMMs used in our \bench{}.}
\label{tab:modelsource}
\centering
\vspace{1mm}
% \tabcolsep=0.2mm
\scalebox{0.68}{
\begin{tabular}{l|l|l}
\toprule
Model & Release Time & \multicolumn{1}{c}{Source} \\
\midrule
\multicolumn{3}{c}{\textit{Closed-source Models}} \\
\cmidrule{1-3}
GPT-4o \citep{gpt4o} & 2024-08-26 & \url{https://openai.com/index/hello-gpt-4o/}\\
OpenAI-o1 \citep{openaio1} & 2024-12-17 & \url{https://openai.com/o1/} \\
Gemini-1.5-Flash \citep{gemini-1.5} & 2024-09-24 & \url{https://deepmind.google/technologies/gemini/} \\
Gemini-2.0-Flash & 2025-01-21 & \url{https://deepmind.google/technologies/gemini/}\\
Claude-3.5-Sonnet & 2024-10-22 & \url{https://www.anthropic.com/claude/sonnet} \\
\midrule
\multicolumn{3}{c}{\textit{Closed-source Models}} \\
\cmidrule{1-3}
LLaVA-One-Vision & 2024-08-05 & \url{https://llava-vl.github.io/blog/2024-08-05-llava-onevision/} \\
InterVL2-8B & 2024-07-04 & \url{https://internvl.github.io/blog/2024-07-02-InternVL-2.0/}\\
Molmo-7B & 2024-09-24 & \url{https://huggingface.co/allenai/Molmo-7B-D-0924} \\
MiniCPM-V & 2024-08-03 & \url{https://huggingface.co/openbmb/MiniCPM-V} \\
GLM-4V-9B & 2024-11-01 & \url{https://huggingface.co/THUDM/glm-4v-9b}\\
Pih3.5-Vision-4.2B & 2024-08-20 & \url{https://huggingface.co/microsoft/Phi-3.5-vision-instruct}\\
LLaVA-1.5-7B & 2023-10-05 & \url{https://huggingface.co/liuhaotian/llava-v1.5-7b}\\
LLaVA-1.6-Mistral-7B & 2024-01-30 & \url{https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf} \\
Fuyu-8B & 2023-10-27 & \url{https://huggingface.co/adept/fuyu-8b}\\
Qwen2-VL-7B & 2024-08-30 & \url{https://huggingface.co/Qwen/Qwen2-VL-7B}\\
\bottomrule
\end{tabular}
}
\end{table}

\section{Qualitative Examples.} 
\textbf{Interactive process could improve the performance of leading LMMs.} In Figure \ref{fig:viscompare1}, we provide the qualitative results of different models. For the same question, Claude-3.5-Sonnet gives the correct answer C without human feedback, Gemini-2.0-Flash uses two rounds while OpenAI-o1 uses three rounds. It indicates that 1) even the SOTA models like OpenAI-o1 can not fully address the visual logic problem which is worse than Claude-3.5-Sonnet, 2) the responses can be corrected by human feedback which shows that the models have the capability of interpreting and incorporating the feedback into their reasoning, 3) Different models shows a different level of this capability. Additionally, we provide another example in Figure \ref{fig:viscompare2}.

\textbf{LMMs may not truly reasoning-They guess answers by elimination.} In Figure \ref{fig:visguess1}, we find that the model will guess the answer when we only have four options, the model tends to guess answers. For the same question, we conduct twice runs and find that OpenAI-o1 could not solve this problem at the beginning, but two different answers were given in these two runs. In the first run, the model outputs D at the beginning while in the second run, the model outputs the A at the beginning. In the following rounds, we provide the same prompts to ensure the fairness comparison, one can see that based on the same prompt, it outputs the same answer C in the second round. The left run in the figure shows the correct answer in the third round while the right run in the figure shows the incorrect answer D. We continue to give the third feedback for round 4, and the right run finally gives answer B. It is obvious that when a problem cannot solved by a model, it will 1) outcome answer randomly, and 2) outcome the answer through an elimination approach. These results may indicate that LMMs may not always truly reason they may give the answer by guessing. Additionally, we provide another example in Figure \ref{fig:visguess2} to illustrate that LMMs may guess answers when they can not solve the challenging problems.

\textbf{LMMs still fail when the GT answer is not provided in the level 3 feedback.} As discussed in the main submission, we include the GT answer in the level 3 feedback prompt to examine whether the model can generate the correct reasoning procedure that leads to the correct answer. When we remove the GT answer as in Figure \ref{fig:withoutgtanswer}, the model still fails to produce the correct answer, indicating its limited capability in solving challenging problems even when detailed feedback is provided as guidance.

\bibliography{main}
\bibliographystyle{iclr2025_conference}

\vspace{3cm}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/visfigure1.pdf}
    \caption{Qualitative results on different LMMs.}
    \label{fig:viscompare1}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/visfigure2.pdf}
    \caption{Qualitative results on different LMMs.}
    \label{fig:viscompare2}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/visguess1.pdf}
    \caption{An example that model tends to guess answers.}
    \label{fig:visguess1}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/visguess2.pdf}
    \caption{An example that model tends to guess answers.}
    \label{fig:visguess2}
\end{figure*}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/withoutgtanswer.pdf}
    \caption{Qualitative results by removing GT answer in level 3 feedback.}
    \label{fig:withoutgtanswer}
\end{figure}


\end{document}
