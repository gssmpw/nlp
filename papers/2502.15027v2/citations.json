[
  {
    "index": 0,
    "papers": [
      {
        "key": "liu2023llava",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual Instruction Tuning"
      },
      {
        "key": "llava1.5",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae",
        "title": "Improved baselines with visual instruction tuning"
      },
      {
        "key": "liu2024llavanext",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae",
        "title": "LLaVA-NeXT: Improved reasoning, OCR, and world knowledge"
      },
      {
        "key": "llava-onevision",
        "author": "Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan",
        "title": "LLaVA-OneVision: Easy Visual Task Transfer"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "qwen2-vl",
        "author": "Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin",
        "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wang2023cogvlm",
        "author": "Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang",
        "title": "CogVLM: Visual Expert for Pretrained Language Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "internvl2",
        "author": "OpenGVLab",
        "title": "InternVL2: Better than the Best\u2014Expanding Performance Boundaries of Open-Source Multimodal Models with the Progressive Scaling Strategy"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "molmo",
        "author": "Matt Deitke and Christopher Clark and Sangho Lee and Rohun Tripathi and Yue Yang and Jae Sung Park and Mohammadreza Salehi and Niklas Muennighoff and Kyle Lo and Luca Soldaini and Jiasen Lu and Taira Anderson and Erin Bransom and Kiana Ehsani and Huong Ngo and YenSung Chen and Ajay Patel and Mark Yatskar and Chris Callison-Burch and Andrew Head and Rose Hendrix and Favyen Bastani and Eli VanderBilt and Nathan Lambert and Yvonne Chou and Arnavi Chheda and Jenna Sparks and Sam Skjonsberg and Michael Schmitz and Aaron Sarnat and Byron Bischoff and Pete Walsh and Chris Newell and Piper Wolters and Tanmay Gupta and Kuo-Hao Zeng and Jon Borchardt and Dirk Groeneveld and Crystal Nam and Sophie Lebrecht and Caitlin Wittlif and Carissa Schoenick and Oscar Michel and Ranjay Krishna and Luca Weihs and Noah A. Smith and Hannaneh Hajishirzi and Ross Girshick and Ali Farhadi and Aniruddha Kembhavi",
        "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "yao2024minicpmvgpt4vlevelmllm",
        "author": "Yuan Yao and Tianyu Yu and Ao Zhang and Chongyi Wang and Junbo Cui and Hongji Zhu and Tianchi Cai and Haoyu Li and Weilin Zhao and Zhihui He and Qianyu Chen and Huarong Zhou and Zhensheng Zou and Haoye Zhang and Shengding Hu and Zhi Zheng and Jie Zhou and Jie Cai and Xu Han and Guoyang Zeng and Dahai Li and Zhiyuan Liu and Maosong Sun",
        "title": "MiniCPM-V: A GPT-4V Level MLLM on Your Phone"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "phi3model",
        "author": "Marah Abdin and Jyoti Aneja and Hany Awadalla and Ahmed Awadallah and Ammar Ahmad Awan and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and S\u00e9bastien Bubeck and Martin Cai and Qin Cai and Vishrav Chaudhary and Dong Chen and Dongdong Chen and Weizhu Chen and Yen-Chun Chen and Yi-Ling Chen and Hao Cheng and Parul Chopra and Xiyang Dai and Matthew Dixon and Ronen Eldan and Victor Fragoso and Jianfeng Gao and Mei Gao and Min Gao and Amit Garg and Allie Del Giorno and Abhishek Goswami and Suriya Gunasekar and Emman Haider and Junheng Hao and Russell J. Hewett and Wenxiang Hu and Jamie Huynh and Dan Iter and Sam Ade Jacobs and Mojan Javaheripi and Xin Jin and Nikos Karampatziakis and Piero Kauffmann and Mahoud Khademi and Dongwoo Kim and Young Jin Kim and Lev Kurilenko and James R. Lee and Yin Tat Lee and Yuanzhi Li and Yunsheng Li and Chen Liang and Lars Liden and Xihui Lin and Zeqi Lin and Ce Liu and Liyuan Liu and Mengchen Liu and Weishung Liu and Xiaodong Liu and Chong Luo and Piyush Madan and Ali Mahmoudzadeh and David Majercak and Matt Mazzola and Caio C\u00e9sar Teodoro Mendes and Arindam Mitra and Hardik Modi and Anh Nguyen and Brandon Norick and Barun Patra and Daniel Perez-Becker and Thomas Portet and Reid Pryzant and Heyang Qin and Marko Radmilac and Liliang Ren and Gustavo de Rosa and Corby Rosset and Sambudha Roy and Olatunji Ruwase and Olli Saarikivi and Amin Saied and Adil Salim and Michael Santacroce and Shital Shah and Ning Shang and Hiteshi Sharma and Yelong Shen and Swadheen Shukla and Xia Song and Masahiro Tanaka and Andrea Tupini and Praneetha Vaddamanu and Chunyu Wang and Guanhua Wang and Lijuan Wang and Shuohang Wang and Xin Wang and Yu Wang and Rachel Ward and Wen Wen and Philipp Witte and Haiping Wu and Xiaoxia Wu and Michael Wyatt and Bin Xiao and Can Xu and Jiahang Xu and Weijian Xu and Jilong Xue and Sonali Yadav and Fan Yang and Jianwei Yang and Yifan Yang and Ziyi Yang and Donghan Yu and Lu Yuan and Chenruidong Zhang and Cyril Zhang and Jianwen Zhang and Li Lyna Zhang and Yi Zhang and Yue Zhang and Yunan Zhang and Xiren Zhou",
        "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "mmmupro",
        "author": "Xiang Yue and Tianyu Zheng and Yuansheng Ni and Yubo Wang and Kai Zhang and Shengbang Tong and Yuxuan Sun and Botao Yu and Ge Zhang and Huan Sun and Yu Su and Wenhu Chen and Graham Neubig",
        "title": "MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "lu2024mathvista",
        "author": "Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng",
        "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "VQAv2",
        "author": "Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi",
        "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "cococaption",
        "author": "Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\\'a}r, Piotr and Zitnick, C Lawrence",
        "title": "Microsoft coco captions: Data collection and evaluation server"
      },
      {
        "key": "flickr_entity",
        "author": "Bryan A. Plummer and Liwei Wang and Christopher M. Cervantes and Juan C. Caicedo and J. Hockenmaier and Svetlana Lazebnik",
        "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"
      },
      {
        "key": "agrawal2019nocaps",
        "author": "Agrawal, Harsh and Desai, Karan and Wang, Yufei and Chen, Xinlei and Jain, Rishabh and Johnson, Mark and Batra, Dhruv and Parikh, Devi and Lee, Stefan and Anderson, Peter",
        "title": "nocaps: novel object captioning at scale"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "textvqa",
        "author": "Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus",
        "title": "Towards VQA Models That Can Read"
      },
      {
        "key": "sidorov2020textcaps",
        "author": "Sidorov, Oleksii and Hu, Ronghang and Rohrbach, Marcus and Singh, Amanpreet",
        "title": "Textcaps: a dataset for image captioning with reading comprehension"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zellers2019recognition",
        "author": "Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin",
        "title": "From recognition to cognition: Visual commonsense reasoning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "marino2019ok_okvqa",
        "author": "Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh",
        "title": "Ok-vqa: A visual question answering benchmark requiring external knowledge"
      },
      {
        "key": "AOKVQA",
        "author": "Dustin Schwenk and Apoorv Khandelwal and Christopher Clark and Kenneth Marino and Roozbeh Mottaghi",
        "title": "A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "worldgui",
        "author": "Henry Hengyuan Zhao and Difei Gao and Mike Zheng Shou",
        "title": "WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation"
      },
      {
        "key": "liu2023mmbench",
        "author": "Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others",
        "title": "Mmbench: Is your multi-modal model an all-around player?"
      },
      {
        "key": "li2023seedbench",
        "author": "Bohao Li and Rui Wang and Guangzhi Wang and Yuying Ge and Yixiao Ge and Ying Shan",
        "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension"
      },
      {
        "key": "yu2023mmvet",
        "author": "Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan",
        "title": "Mm-vet: Evaluating large multimodal models for integrated capabilities"
      },
      {
        "key": "yue2024mmmu",
        "author": "Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others",
        "title": "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi"
      },
      {
        "key": "lu2024mathvista",
        "author": "Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng",
        "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"
      },
      {
        "key": "mathverse",
        "author": "Renrui Zhang and Dongzhi Jiang and Yichi Zhang and Haokun Lin and Ziyu Guo and Pengshuo Qiu and Aojun Zhou and Pan Lu and Kai-Wei Chang and Peng Gao and Hongsheng Li",
        "title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "liu2023mmbench",
        "author": "Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others",
        "title": "Mmbench: Is your multi-modal model an all-around player?"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "mmmupro",
        "author": "Xiang Yue and Tianyu Zheng and Yuansheng Ni and Yubo Wang and Kai Zhang and Shengbang Tong and Yuxuan Sun and Botao Yu and Ge Zhang and Huan Sun and Yu Su and Wenhu Chen and Graham Neubig",
        "title": "MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "mathverse",
        "author": "Renrui Zhang and Dongzhi Jiang and Yichi Zhang and Haokun Lin and Ziyu Guo and Pengshuo Qiu and Aojun Zhou and Pan Lu and Kai-Wei Chang and Peng Gao and Hongsheng Li",
        "title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "virvou2022emerging",
        "author": "Virvou, Maria",
        "title": "The emerging era of human-AI interaction: Keynote address"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "dodeja2024towards",
        "author": "Dodeja, Lakshita and Tambwekar, Pradyumna and Hedlund-Botti, Erin and Gombolay, Matthew",
        "title": "Towards the design of user-centric strategy recommendation systems for collaborative Human--AI tasks"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "zhang2021human",
        "author": "Zhang, Jiehuang and Shu, Ying and Yu, Han",
        "title": "Human-machine interaction for autonomous vehicles: A review"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "mckinney2020international",
        "author": "McKinney, Scott Mayer and Sieniek, Marcin and Godbole, Varun and Godwin, Jonathan and Antropova, Natasha and Ashrafian, Hutan and Back, Trevor and Chesus, Mary and Corrado, Greg S and Darzi, Ara and others",
        "title": "International evaluation of an AI system for breast cancer screening"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "park2023generative",
        "author": "Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S",
        "title": "Generative agents: Interactive simulacra of human behavior"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "zhang2023human",
        "author": "Zhang, Tianyi and Tham, Isaac and Hou, Zhaoyi and Ren, Jiaxuan and Zhou, Liyang and Xu, Hainiu and Zhang, Li and Martin, Lara J and Dror, Rotem and Li, Sha and others",
        "title": "Human-in-the-loop schema induction"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "molmo",
        "author": "Matt Deitke and Christopher Clark and Sangho Lee and Rohun Tripathi and Yue Yang and Jae Sung Park and Mohammadreza Salehi and Niklas Muennighoff and Kyle Lo and Luca Soldaini and Jiasen Lu and Taira Anderson and Erin Bransom and Kiana Ehsani and Huong Ngo and YenSung Chen and Ajay Patel and Mark Yatskar and Chris Callison-Burch and Andrew Head and Rose Hendrix and Favyen Bastani and Eli VanderBilt and Nathan Lambert and Yvonne Chou and Arnavi Chheda and Jenna Sparks and Sam Skjonsberg and Michael Schmitz and Aaron Sarnat and Byron Bischoff and Pete Walsh and Chris Newell and Piper Wolters and Tanmay Gupta and Kuo-Hao Zeng and Jon Borchardt and Dirk Groeneveld and Crystal Nam and Sophie Lebrecht and Caitlin Wittlif and Carissa Schoenick and Oscar Michel and Ranjay Krishna and Luca Weihs and Noah A. Smith and Hannaneh Hajishirzi and Ross Girshick and Ali Farhadi and Aniruddha Kembhavi",
        "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models"
      },
      {
        "key": "qwen2-vl",
        "author": "Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin",
        "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "worldgui",
        "author": "Henry Hengyuan Zhao and Difei Gao and Mike Zheng Shou",
        "title": "WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation"
      },
      {
        "key": "swebenchmm",
        "author": "John Yang and Carlos E Jimenez and Alex L Zhang and Kilian Lieret and Joyce Yang and Xindi Wu and Ori Press and Niklas Muennighoff and Gabriel Synnaeve and Karthik R Narasimhan and Diyi Yang and Sida Wang and Ofir Press",
        "title": "{SWE}-bench Multimodal: Do Autonomous Programming Systems Generalize to New Software Domains?"
      },
      {
        "key": "li2024mediq",
        "author": "Shuyue Stella Li and Vidhisha Balachandran and Shangbin Feng and Jonathan S. Ilgen and Emma Pierson and Pang Wei Koh and Yulia Tsvetkov",
        "title": "MediQ: Question-Asking {LLM}s and a Benchmark for Reliable Interactive Clinical Reasoning"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "khan2024debatingpersuasivellmsleads",
        "author": "Akbir Khan and John Hughes and Dan Valentine and Laura Ruis and Kshitij Sachan and Ansh Radhakrishnan and Edward Grefenstette and Samuel R. Bowman and Tim Rockt\u00e4schel and Ethan Perez",
        "title": "Debating with More Persuasive LLMs Leads to More Truthful Answers"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "yao2025taubench",
        "author": "Shunyu Yao and Noah Shinn and Pedram Razavi and Karthik R Narasimhan",
        "title": "\\{\\${\\textbackslash}tau\\$\\}-bench: A Benchmark for {\\textbackslash}underline\\{T\\}ool-{\\textbackslash}underline\\{A\\}gent-{\\textbackslash}underline\\{U\\}ser Interaction in Real-World Domains"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "VideoAutoArena",
        "author": "Ziyang Luo and Haoning Wu and Dongxu Li and Jing Ma and Mohan Kankanhalli and Junnan Li",
        "title": "VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "chatshop",
        "author": "Sanxing Chen and Sam Wiseman and Bhuwan Dhingra",
        "title": "ChatShop: Interactive Information Seeking with Language Agents"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "yoon-etal-2024-evaluating",
        "author": "Yoon, Se-eun  and\nHe, Zhankui  and\nEchterhoff, Jessica  and\nMcAuley, Julian",
        "title": "Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation"
      }
    ]
  }
]