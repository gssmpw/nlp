\section{Related Work}
\textbf{Large Multimodal Models.}
The LLaVA-series works~\citep{liu2023llava, llava1.5, liu2024llavanext, llava-onevision} demonstrate that training with supervised fine-tuning (SFT) multimodal data and expand the vision lens would produce compatible multimodal reasoning ability. By adopting a large-scale image-text corpus for instruction tuning, Qwen2-VL~\citep{qwen2-vl}, CogVLM~\citep{wang2023cogvlm}, InternVL2 \citep{internvl2} have achieved exceptional performance on various multimodal abilities. Moreover, Molmo \citep{molmo} proposes to train an LMM from scratch with only the human-annotated data. Unlike these large models, MiniCPM-V \citep{yao2024minicpmvgpt4vlevelmllm} and Phi-3.5-Vision \citep{phi3model} propose to train lightweight yet SOTA LMMs. Despite their exceptional performance on multimodal benchmarks of varying difficulty, such as MMMU-Pro \citep{mmmupro} and MathVista \citep{lu2024mathvista}, it remains unclear how well these LMMs demonstrate interactive intelligence in Human-AI Interaction scenarios. In this paper, we conduct the evaluation of these LMMs to explore this basic yet vital capability (i.e., evolve through interactive human feedback).

\textbf{Multimodal Benchmarks.}
Traditional vision-language benchmarks focus on visual question answering~\citep{VQAv2}, image captioning~\citep{cococaption,flickr_entity,agrawal2019nocaps}, as well as other benchmarks for specialized scenarios such as scene text understanding~\citep{textvqa,sidorov2020textcaps}, commonsense reasoning~\citep{zellers2019recognition}, outside knowledge~\citep{marino2019ok_okvqa,AOKVQA}. The recent development of LMM posts a strong need for modernized multimodal benchmarks~\citep{worldgui,liu2023mmbench,li2023seedbench,yu2023mmvet, yue2024mmmu, lu2024mathvista, mathverse} such as MMBench~\citep{liu2023mmbench}, MMMU-pro \citep{mmmupro}, and MathVerse \citep{mathverse} which involve comprehensively evaluating current LMMs on various multimodal abilities. However, these benchmarks primarily focus on static testing processes, overlooking the interactive testing process that is vital in human-AI interaction scenarios.

\textbf{Human-AI Interaction.}
Investigating how humans and AI systems communicate and collaborate is critical for shaping applications such as virtual assistants \citep{virvou2022emerging}, personalized recommendations \citep{dodeja2024towards}, autonomous vehicles \citep{zhang2021human}, and healthcare diagnostics \citep{mckinney2020international}. Recent LLMs-driven techniques such as memory~\citep{park2023generative} and iterative~\citep{zhang2023human} mechanisms offer expert-level collaboration. While LMMs \citep{molmo,qwen2-vl} excel in multimodal tasks, their potential for HAI problem-solving \citep{worldgui, swebenchmm, li2024mediq} remains underexplored. By offering a unified framework and meticulously curated data, our \bench{} enables evaluation of LMMs on these capabilities and lays a foundation for advancing multimodal HAI problem-solving.

\textbf{User Stimulation with LLM.} Recently, previous work in order to build multi-agent system \citep{khan2024debatingpersuasivellmsleads}, stimulate human-AI interaction \citep{yao2025taubench}, evaluate LMMs in video analysis \citep{VideoAutoArena}, stimulate the real user in a web shopping scenario \cite{chatshop}, evaluate the conversational recommender systems \citep{yoon-etal-2024-evaluating} determine to use LLM or LMM to stimulate the user. However, previous works have overlooked the importance of ensuring the reliability of LLMs or LMMs that are used to stimulate the users. In this paper, we curate test data by selecting only the samples that LMMs correctly address, minimizing unreliable interaction results.



\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.92\linewidth]{figures/dataconstruct.pdf}
    \vspace{-1mm}
    \caption{Overview of the test data construction process for \bench{}. For each LMM serving as the feedback receiver, we process each instance from a target dataset (e.g., MathVerse) and collect the error cases to form a negative set. The feedback provider then processes the same instances to build a positive set. Finally, we curate test data by selecting the intersection of both sets.}
    \label{fig:dataconstruct}
\end{figure*}