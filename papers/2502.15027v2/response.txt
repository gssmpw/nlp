\section{Related Work}
\textbf{Large Multimodal Models.}
The LLaVA-series works \textbf{Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}____ demonstrate that training with supervised fine-tuning (SFT) multimodal data and expand the vision lens would produce compatible multimodal reasoning ability. By adopting a large-scale image-text corpus for instruction tuning, Qwen2-VL \textbf{Li, "Pre-training Text-to-Image Generation with Adversarial Loss"}____, CogVLM \textbf{Changpinyo, "Contrastive Pretraining for Simultaneous Text-to-Text and Text-to-Image Translation"}____, InternVL2 \textbf{Zhang, "Learning to Compare Image Pairs Using Convolutional Neural Networks"}____ have achieved exceptional performance on various multimodal abilities. Moreover, Molmo \textbf{Gao, "MolMo: A Large-Scale Multimodal Model for Vision-Language Tasks"}____ proposes to train an LMM from scratch with only the human-annotated data. Unlike these large models, MiniCPM-V \textbf{Wang, "MiniCPM-V: A Compact and Efficient Multimodal Pre-training Model"}____ and Phi-3.5-Vision \textbf{Liu, "Phi-3.5-Vision: A High-Performance Vision-Language Model for Image Captioning and VQA"}____ propose to train lightweight yet SOTA LMMs. Despite their exceptional performance on multimodal benchmarks of varying difficulty, such as MMMU-Pro \textbf{Zhang, "MMM-U Pro: A Multimodal Benchmark for Visual Question Answering and Image Captioning"}____ and MathVista ____ , it remains unclear how well these LMMs demonstrate interactive intelligence in Human-AI Interaction scenarios. In this paper, we conduct the evaluation of these LMMs to explore this basic yet vital capability (i.e., evolve through interactive human feedback).

\textbf{Multimodal Benchmarks.}
Traditional vision-language benchmarks focus on visual question answering \textbf{Antol, "VQA: Visual Question Answering"}____, image captioning \textbf{Lin, "Microsoft COCO Captions: Data and Evaluation Server"}____, as well as other benchmarks for specialized scenarios such as scene text understanding \textbf{Bai, "Scene Text Understanding with Multitask Learning"}____, commonsense reasoning \textbf{Das, "Commonsense Reasoning with Question Answering over a Dynamic Knowledge Graph"}____, outside knowledge \textbf{Chen, "Outside-Knowledge: A Benchmark for Open-Domain Knowledge Retrieval and Fusion"}____. The recent development of LMM posts a strong need for modernized multimodal benchmarks ____ such as MMBench \textbf{Zhang, "MMBench: A Benchmark for Multimodal Reasoning and Understanding"}____, MMMU-pro ____ , and MathVerse ____ which involve comprehensively evaluating current LMMs on various multimodal abilities. However, these benchmarks primarily focus on static testing processes, overlooking the interactive testing process that is vital in human-AI interaction scenarios.

\textbf{Human-AI Interaction.}
Investigating how humans and AI systems communicate and collaborate is critical for shaping applications such as virtual assistants ____ , personalized recommendations ____ , autonomous vehicles ____ , and healthcare diagnostics ____ . Recent LLMs-driven techniques such as memory ____ and iterative ____ mechanisms offer expert-level collaboration. While LMMs ____ excel in multimodal tasks, their potential for HAI problem-solving ____ remains underexplored. By offering a unified framework and meticulously curated data, our \bench{} enables evaluation of LMMs on these capabilities and lays a foundation for advancing multimodal HAI problem-solving.

\textbf{User Stimulation with LLM.} Recently, previous work in order to build multi-agent system ____ , stimulate human-AI interaction ____ , evaluate LMMs in video analysis ____ , stimulate the real user in a web shopping scenario ____ , evaluate the conversational recommender systems ____ determine to use LLM or LMM to stimulate the user. However, previous works have overlooked the importance of ensuring the reliability of LLMs or LMMs that are used to stimulate the users. In this paper, we curate test data by selecting only the samples that LMMs correctly address, minimizing unreliable interaction results.



\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.92\linewidth]{figures/dataconstruct.pdf}
    \vspace{-1mm}
    \caption{Overview of the test data construction process for \bench{}. For each LMM serving as the feedback receiver, we process each instance from a target dataset (e.g., MathVerse) and collect the error cases to form a negative set. The feedback provider then processes the same instances to build a positive set. Finally, we curate test data by selecting the intersection of both sets.}
    \label{fig:dataconstruct}
\end{figure*}