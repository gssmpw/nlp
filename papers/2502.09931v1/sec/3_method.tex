\section{Method}

\subsection{Encoder and Decoder in TransGUNet}

We used a Pyramid Pooling Transformer (P2T) \cite{wu2022p2t} comprising multiple pooling-based multi-head self-attention. P2T incurs a significantly lower computational cost and higher representation power than Vision Transformer (ViT) and Pyramid Vision Transformer (PVT), which are adopted in various medical image segmentation models \cite{dong2108polyp, zhang2022hsnet, liu2024cafe}. Inspired by previous studies, we utilized the same encoder architecture as the decoder to fully leverage global dependency. Although we primarily present the experimental results using P2T, we also provide various CNN and transformer backbones to demonstrate the versatility and robustness of the proposed approach across different backbone architectures in the Appendix (Tab. \ref{tab:ablation_backbone_networks}).

\subsection{ACS-GNN with EFS-based spatial attention for Skip Connection}

\noindent \textit{Motivation:} The human visual system (HVS) recognizes objects by dividing them into large parts and understanding them based on the connectivity strengths of each part \cite{majaj2015simple}. This process helps interpret complex scenes by identifying relationships between different parts of an image, leading to a holistic understanding of objects and their interactions \cite{palmer1999vision, marr2010vision}. Inspired by these principles, our approach employs a similar strategy of the HVS by transforming the cross-scale feature map into graphs to understand the complex anatomical structures in a high-dimensional feature space. However, significant noise and complex backgrounds create highly ambiguous visual signals that disturb the neural systems. The HVS mitigates this issue through signal filtering and attention processing \cite{posner1990attention, treue2001neural}. Therefore, we propose an entropy-based feature selection strategy that mimics these feature filtering and attention processes, called EFS-based spatial attention. The integration of these components enhances the preservation of global dependencies and the local details without ambiguity in attention mechanism. The overall architecture of the TransGUNet is illustrated in Fig. \ref{fig:TransGUNet}. The ACS-GNN with EFS-based spatial attention can be divided into four steps: \textit{1) Feature Preprocessing}, \textit{2) ACS-GNN (Fig. \ref{fig:TransGUNet} (c))}, \textit{3) EFS-based spatial attention (Fig. \ref{fig:ESA_based_spatial_attention})}, and \textit{4) Feature Postprocessing}.

\noindent \textbf{Feature Preprocessing.} Let $\mathbf{f}_{i} \in \mathbb{R}^{C_{i} \times \frac{H}{2^{i + 1}} \times \frac{W}{2^{i + 1}}}$ be the feature maps from the $i$-th encoder stage for $i = 1, 2, 3, 4$ where $(H, W)$ is the resolution of the input image. Because the number of channels in each stage primarily affects the complexity of the decoder, we employed a 2D convolution with a kernel size of $1 \times 1$ to reduce the number of channels to $C_{r}$. To obtain the cross-scale feature map $\mathbf{f}_{c}$, we resized them to the same resolution for $i = 1, 2, 3, 4$ as follows:
\begin{equation}
    \mathbf{f}^{'}_{i} = \textbf{Resize}_{(H_{t}, W_{t})} (\textbf{C2D}_{1 \times 1} (\mathbf{f}_{i})) \in \mathbb{R}^{C_{r} \times H_{t} \times W_{t}}
\end{equation}

\noindent where $\textbf{C2D}_{k \times k} ( \cdot )$, and $\textbf{Resize}_{(H_{t}, W_{t})} (\cdot)$ denote the 2D convolution with a kernel size of $k \times k$, and an operation to resize into the target spatial resolution $(H_{t}, W_{t})$, respectively. If the target and original resolutions of the input feature map differ, bilinear interpolation is used to upsample or downsample feature map to match the target resolution. Alternatively, if they have the same resolution, we do not apply resize, denoted as \say{\textit{Resolution Fix}} in Fig. \ref{fig:TransGUNet} (b). And then, we concatenate each resized feature map $\mathbf{f}_{c} = \left[ \mathbf{f}^{'}_{1}, \mathbf{f}^{'}_{2}, \mathbf{f}^{'}_{3}, \mathbf{f}^{'}_{4} \right] \in \mathbb{R}^{4C_{r} \times H_{t} \times W_{t}}$ where $\left[ \cdots \right]$ denotes concatenation between feature maps along the channel dimension. For convenience, we assume that $C = 4C_{r}$.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figure/UFSA}
    \caption{The overall block diagram of the Entropy-driven Feature Selection with spatial attention. Low and high transparency red indicates a high and low entropy score, respectively. We produce the spatial attention map using the indices corresponding to the $M$ channels with the lowest entropy scores, called Bottom-$M$ index.}
    \label{fig:ESA_based_spatial_attention}
\end{figure}

\noindent \textbf{Attentional Cross-Scale Graph Neural Network.} After obtaining the cross-scale feature $\mathbf{f}_{c} \in \mathbb{R}^{C \times H_{t} \times W_{t}}$, we apply 2D convolution with a kernel size of $1 \times 1$ and batch normalization (BN). Subsequently, the feature map is converted into a flattened vector $\mathbf{x} \in \mathbb{R}^{C \times N}$, where $N = H_{t}W_{t}$. Each pixel in $\mathbf{x}$ acts as a node in the graph. Additionally, a relative positional vector is added to each flattened vector element to preserve the position information. Next, we constructed the feature graph using the dilated $K$-nearest neighbors (KNN) algorithm. To implement the exchange of information between nodes, we adopted the Max-Relative graph convolution (MRConv) \cite{li2019deepgcns} owing to its simplicity and efficiency as it does not require learnable parameters for node aggregation. The MRConv and Update processes are implemented for graph convolution as $\mathbf{x}^{G} = \mathcal{G} (\mathbf{x})$. 

\noindent To improve feature aggregation by adaptively weighting the node importance, we applied node attention to prioritize critical features while learning their relevance. Based on ECANet \cite{wang2020eca}, we designed a node attention mechanism using a single 1D convolution operation with kernel size of $k$ and sigmoid function. Firstly, $\mathbf{x}^{G}$ is compressed into $\mathbf{z}_{\text{avg}}$ and $\mathbf{z}_{\text{max}}$ using Global Average Pooling and Global Max Pooling, respectively, and then each statistic is aggregated to produce a node attention map. Finally, such an attention mechanism is readily implemented as follows:
\begin{equation}
    \overline{\mathbf{x}}^{G} = \mathbf{x}^{G} \times \sigma \left( \sum_{d \in \{ \text{avg}, \text{max} \}} \textbf{C1D}_{k} (\mathbf{z}_{d}) \right)
\end{equation}

\noindent where $\textbf{C1D}_{k} ( \cdot )$ and $\sigma( \cdot )$ denote 1D convolution with a kernel size of $k$ and the sigmoid function, respectively. And, we reshape flattened vector $\overline{\mathbf{x}}^{G}$ into original feature map shape and apply 2D convolution with a kernel size of $1 \times 1$ and BN for more nonlinearity to obtain the refined feature map $\overline{\mathbf{X}}^{G} \in \mathbb{R}^{C \times H_{t} \times W_{t}}$. To address the oversmoothing problem \cite{li2018deeper, oono2020graph} in GNN, we also utilize Feed-Forward Networks with two consecutive convolutional layers and residual connections as follows:
\begin{equation}
    \overline{\mathbf{f}}^{G}_{c} = \textbf{BN} (\textbf{C2D}_{1 \times 1} (\delta(\textbf{BN} (\textbf{C2D}_{1 \times 1} (\overline{\mathbf{X}}^{G}))))) + \overline{\mathbf{X}}^{G}
\end{equation}

\noindent where $\delta( \cdot )$ and $\textbf{BN} (\cdot)$ denote the ReLU activation function for non-linearity and batch normalization, respectively.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figure/FeatureVisualization}
    \caption{(a) Input image, (b) Feature map from ACS-GNN, (c) Entropy map according to each feature map. We calculated entropy map using Shannon Entropy at pixel level.}
    \label{fig:FeatureVisualization}
\end{figure}

\noindent \textbf{Spatial Attention with Entropy-driven Feature Selection.} Although a GNN can maintain global dependencies, various noise and complex backgrounds in medical images still result in an uninformative feature map containing a high-entropy score, leading to a poor spatial attention map \cite{chen2021lesion, shawn2024ct}. To address this issue, we propose \textit{entropy-driven feature selection (EFS)-based spatial attention}, which illustrated in Fig. \ref{fig:ESA_based_spatial_attention}, that filters uninformative feature maps and generates a more reliable spatial attention map. We observed that the channel with a high entropy is uniformly or noisily activated as shown in Fig. \ref{fig:FeatureVisualization}. Therefore, inspired by previous studies \cite{vu2019advent, wang2019boundary}, we calculated the entropy score $\mathbf{E} \in \mathbb{R}^{C}$ at the pixel level for each channel of the input feature map using Shannon Entropy as follows:
\begin{equation}
    \mathbf{E} = \frac{1}{HW} \sum_{h = 1}^{H} \sum_{w = 1}^{W} (-\sigma(\overline{\mathbf{f}}^{G}_{c})_{:, h, w} \log (\sigma(\overline{\mathbf{f}}^{G}_{c})_{:, h, w}))
\end{equation}

\noindent Subsequently, we only retained the feature map channels with the $M$ lowest entropy scores and used them for spatial attention as follows:
\begin{equation}
    \hat{\mathbf{f}}^{G}_{c} = \overline{\mathbf{f}}^{G}_{c} \times \sigma(\textbf{C2D}_{1 \times 1} ((\overline{\mathbf{f}}^{G}_{c})_{\textbf{sorting} \left(  \mathbf{E} \right)[: M]}))
\end{equation}

\noindent where $\textbf{sorting}(\cdot)$ denotes the sorting algorithm into ascending. Hence, $\textbf{sorting} \left(  \mathbf{E} \right)[: M]$ means the Bottom-$M$ index of feature map $\overline{\mathbf{f}}^{G}_{c}$ with low entropy score. We used the Introselect algorithm, the default sorting algorithm in PyTorch. This dual process of ACS-GNN and EFS-based spatial attention allows our model to comprehend the intricate anatomical structures in medical images and generate more reliable spatial attention maps.

\noindent \textbf{Feature Postprocessing.} We first divided $\hat{\mathbf{f}}^{G}_{c}$ along the channel dimension with an equal number of channels $C_{r}$. Subsequently, each feature map was resized and applied residual connection between the original feature map to enhance the training stability for $i = 1, 2, 3, 4$ as follows:
\begin{equation}
    \hat{\mathbf{f}}^{G}_{i} = \mathbf{f}^{'}_{i} + \textbf{Resize}_{(\frac{H}{2^{i + 1}}, \frac{W}{2^{i + 1}})} \left( (\hat{\mathbf{f}}^{G}_{c})_{C_{r} \cdot (i - 1):C_{r} \cdot i} \right)
\end{equation}

\noindent For convenience, let $\mathbf{D}_{1} = \hat{\mathbf{f}}^{G}_{4}$. Then, we produce the decoder block output $\mathbf{D}_{i + 1}$ for $i = 1, 2, 3$ as $\mathbf{D}_{i + 1} = \textbf{Decoder}_{i} \left( \left[ \hat{\mathbf{f}}^{G}_{4 - i}, \textbf{Up}_{2} ( \mathbf{D}_{i} ) \right] \right)$ where $\textbf{Up}_{n} ( \cdot )$ and $\textbf{Decoder}_{i} ( \cdot )$ denote a bilinear upsampling with scale factor $2^{n - 1}$ and $i$-th transformer decoder block, respectively. We summarized the technical novelty and impact of TransGUNet in the Appendix.

\subsection{Training Procedure}
We employed a multi-task learning with deep supervision to enhance the representation power of the model and mitigate the gradient vanishing problem. To implement this, we obtained four predictions $\mathbf{O}_{i}$ from $\mathbf{D}_{i}$ for $i = 1, 2, 3, 4$ by applying 2D convolution with kernel size of $1 \times 1$ following sigmoid function and upsampling at each stage. We denote $\mathbf{O}_{i} = \{ \mathbf{R}_{i}, \mathbf{B}_{i} \}$ as the multiple outputs representing region $\mathbf{R}_{i}$ and boundary $\mathbf{B}_{i}$ predictions, respectively. The total loss function is defined as:
\begin{equation}
    \mathcal{L}_{total} = \sum_{i=1}^{4} \left( \mathcal{L}_{R} (\mathbf{R}_{t}, \mathbf{R}_{i}) + \mathcal{L}_{B} (\mathbf{B}_{t}, \mathbf{B}_{i}) \right)  
\end{equation}

\noindent where $\mathbf{R}_{t}$ and $\mathbf{B}_{t}$ denote the ground truths of region and the boundary, respectively. We obtained $\mathbf{B}_{t}$  by applying an anisotropic Sobel edge detection filter \cite{kanopoulos1988design} to $\mathbf{R}_{t}$. The loss function for region prediction was defined as $\mathcal{L}_{R} = \mathcal{L}^{w}_{IoU} + \mathcal{L}^{w}_{bce}$, where $\mathcal{L}^{w}_{IoU}$ and $\mathcal{L}^{w}_{bce}$ are the weighted IoU and binary cross entropy (BCE) loss functions, respectively. This loss function is identically defined in previous studies \cite{fan2020pranet, zhao2023m, nam2024modality}. Additionally, we defined boundary loss function $\mathcal{L}_{B}$ as the BCE loss function. 

\begin{table*}[t]
    \centering
    \scriptsize
    \setlength\tabcolsep{5.0pt} % default value: 6pt
    % \renewcommand{\arraystretch}{0.78} % Tighter
    \begin{tabular}{c|cc|cc|cc|cc|cc|cc|c}
    \hline
    \multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{2}{c|}{Synapse \cite{Synapse_dataset}} & \multicolumn{2}{c|}{ISIC2018 \cite{gutman2016skin}} & \multicolumn{2}{c|}{COVID19-1 \cite{ma_jun_2020_3757476}} & \multicolumn{2}{c|}{BUSI \cite{al2020dataset}} & \multicolumn{2}{c}{CVC-ClinicDB \cite{bernal2015wm}} & \multicolumn{2}{c|}{Kvasir-SEG \cite{jha2020kvasir}} & \multicolumn{1}{c}{\multirow{2}{*}{$P$-value}} \\ \cline{2-13}
     & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU & \\
     \hline
     UNet \cite{ronneberger2015u}         & 69.8 \tiny{(0.6)} & 58.9 \tiny{(0.4)} & 86.9 \tiny{(0.8)} & 80.2 \tiny{(0.7)} & 47.7 \tiny{(0.6)} & 38.6 \tiny{(0.6)} & 69.5 \tiny{(0.3)} & 60.2 \tiny{(0.2)} & 76.5 \tiny{(0.8)} & 69.1 \tiny{(0.9)} & 80.5 \tiny{(0.3)} & 72.6 \tiny{(0.4)} &  9.3E-06 \\
     UNet++ \cite{zhou2018unet++}         & 79.3 \tiny{(0.2)} & 69.8 \tiny{(0.1)} & 87.3 \tiny{(0.2)} & 80.2 \tiny{(0.1)} & 65.6 \tiny{(0.7)} & 57.1 \tiny{(0.8)} & 72.4 \tiny{(0.1)} & 62.5 \tiny{(0.2)} & 79.7 \tiny{(0.2)} & 73.6 \tiny{(0.4)} & 84.3 \tiny{(0.3)} & 77.4 \tiny{(0.2)} &  4.0E-05 \\
     CENet \cite{gu2019net}            & 75.2 \tiny{(0.3)} & 64.6 \tiny{(0.4)} & 89.1 \tiny{(0.2)} & 82.1 \tiny{(0.1)} & 76.3 \tiny{(0.4)} & 69.2 \tiny{(0.5)} & 79.7 \tiny{(0.6)} & 65.0 \tiny{(0.5)} & 89.3 \tiny{(0.3)} & 83.4 \tiny{(0.2)} & 89.5 \tiny{(0.7)} & 83.9 \tiny{(0.7)} &  7.1E-07  \\
     TransUNet \cite{chen2021transunet}      & 77.5 \tiny{(0.2)} & 67.1 \tiny{(0.6)} & 87.3 \tiny{(0.2)} & 81.2 \tiny{(0.8)} & 75.6 \tiny{(0.4)} & 68.8 \tiny{(0.2)} & 75.5 \tiny{(0.5)} & 68.4 \tiny{(0.1)} & 87.4 \tiny{(0.2)} & 82.4 \tiny{(0.1)} & 86.4 \tiny{(0.4)} & 80.1 \tiny{(0.4)} & 4.5E-09 \\
     MSRFNet \cite{srivastava2021msrf}          & 77.2 \tiny{(0.5)} & 67.6 \tiny{(0.3)} & 88.2 \tiny{(0.2)} & 81.3 \tiny{(0.2)} & 75.2 \tiny{(0.4)} & 68.0 \tiny{(0.4)} & 76.6 \tiny{(0.7)} & 68.1 \tiny{(0.7)} & 83.2 \tiny{(0.9)} & 76.5 \tiny{(1.1)} & 86.1 \tiny{(0.5)} & 79.3 \tiny{(0.4)} & 1.0E-07 \\
     DCSAUNet \cite{xu2023dcsau}         & 71.0 \tiny{(0.3)} & 55.8 \tiny{(0.1)} & 89.0 \tiny{(0.3)} & 82.0 \tiny{(0.3)} & 75.3 \tiny{(0.4)} & 68.2 \tiny{(0.4)} & 73.7 \tiny{(0.5)} & 65.0 \tiny{(0.5)} & 80.6 \tiny{(1.2)} & 73.7 \tiny{(1.1)} & 82.6 \tiny{(0.5)} & 75.2 \tiny{(0.5)} & 5.4E-08 \\
     M2SNet \cite{zhao2023m}         & 77.1 \tiny{(0.1)} & 67.5 \tiny{(0.4)} & 89.2 \tiny{(0.2)} & 83.4 \tiny{(0.2)} & 81.7 \tiny{(0.4)} & 74.7 \tiny{(0.5)} & 80.4 \tiny{(0.8)} & 72.5 \tiny{(0.7)} & 92.2 \tiny{(0.8)} & \textit{88.0} \tiny{(0.8)} & 91.2 \tiny{(0.5)} & 86.1 \tiny{(0.6)} & 2.2E-05 \\
     ViGUNet \cite{jiang2023vig}     & 72.3 \tiny{(1.5)} & 67.7 \tiny{(1.2)} & 88.3 \tiny{(1.2)} & 81.3 \tiny{(0.1)} & 70.2 \tiny{(0.3)} & 62.1 \tiny{(0.3)} & 70.9 \tiny{(0.2)} & 61.3 \tiny{(0.1)} & 77.5 \tiny{(0.4)} & 69.7 \tiny{(0.7)} & 79.6 \tiny{(0.6)} & 71.3 \tiny{(0.6)} & 3.8E-08 \\
     PVT-GCAS \cite{rahman2024g}    & 78.4 \tiny{(0.3)} & 68.9 \tiny{(0.2)} & \textit{90.6} \tiny{(0.3)} & \textit{84.2} \tiny{(0.5)} & 82.3 \tiny{(0.4)} & 74.8 \tiny{(0.5)} & \textit{82.0} \tiny{(0.5)} & \textit{73.6} \tiny{(0.5)} & 92.2 \tiny{(0.6)} & 87.6 \tiny{(0.6)} & 91.6 \tiny{(0.2)} & 86.8 \tiny{(0.4)} & 1.8E-04 \\
     CFATUNet \cite{wang2024cfatransunet}     & \textit{80.5} \tiny{(1.4)} & \textit{70.4} \tiny{(1.8)} & 90.3 \tiny{(0.4)} & 83.6 \tiny{(0.7)} & 80.4 \tiny{(0.5)} & 73.6 \tiny{(0.2)} & 80.6 \tiny{(0.3)} & 72.8 \tiny{(0.4)} & 91.0 \tiny{(0.2)} & 86.2 \tiny{(0.1)} & \textit{92.1} \tiny{(0.5)} & \textit{87.2} \tiny{(0.5)} & 3.2E-05 \\
     MADGNet \cite{nam2024modality}         & 79.3 \tiny{(1.2)} & 69.8 \tiny{(1.3)} & 90.2 \tiny{(0.1)} & 83.7 \tiny{(0.2)} & \textit{83.7} \tiny{(0.2)} & \textit{76.8} \tiny{(0.2)} & 81.3 \tiny{(0.4)} & 73.4 \tiny{(0.5)} & \textbf{\underline{93.9}} \tiny{(0.6)} & \textbf{\underline{89.5}} \tiny{(0.5)} & 90.7 \tiny{(0.8)} & 85.3 \tiny{(0.8)}  & 6.5E-02 \\ 
     GSENet \cite{li2025gse} & 79.1 \tiny{(1.6)} & 69.2 \tiny{(1.7)} & 90.7 \tiny{(0.2)} & \textit{84.2} \tiny{(0.2)} & 80.7 \tiny{(0.3)} & 73.7 \tiny{(0.4)} & 80.6 \tiny{(0.4)} & 72.8 \tiny{(0.5)} & 91.5 \tiny{(0.2)} & 86.4 \tiny{(0.1)} & \textit{92.1} \tiny{(0.3)} & \textit{87.2} \tiny{(0.4)} & 3.2E-02 \\
     \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{TransGUNet}}} & \textbf{\underline{80.9}}  \tiny{(0.4)} & \textbf{\underline{71.4}}  \tiny{(0.2)} & \textbf{\underline{91.1}}  \tiny{(0.3)} & \textbf{\underline{84.8}}  \tiny{(0.4)} & \textbf{\underline{84.0}} \tiny{(0.2)} & \textbf{\underline{77.0}} \tiny{(0.5)} & \textbf{\underline{82.7}} \tiny{(0.3)} & \textbf{\underline{74.7}} \tiny{(0.4)} & \textit{92.3} \tiny{(0.2)} & 87.7 \tiny{(0.4)} & \textbf{\underline{93.1}} \tiny{(0.1)} & \textbf{\underline{88.4}} \tiny{(0.2)} & \multicolumn{1}{c}{\multirow{2}{*}{-}} \\ \cline{2-13}
     & \textbf{+0.4} & \textbf{+1.0} & \textbf{+0.5} & \textbf{+0.6} & \textbf{+0.3} & \textbf{+0.2} & \textbf{+0.7} & \textbf{+1.1} & \textbf{-1.6} & \textbf{-1.8} & \textbf{+1.0} & \textbf{+1.2} & \\
    \hline
    \end{tabular}
    \caption{Segmentation results on six different datasets with \textit{seen} clinical settings. $(\cdot)$ denotes the standard deviations of multiple experiment results. We also provide the Wilcoxon signed rank test results ($P$-value) compared to our method and other methods.}
    \label{tab:comparison_sota_in_domain}
\end{table*}

\begin{table*}[t]
    \centering
    \scriptsize
    \setlength\tabcolsep{1.25pt} % default value: 6pt
    % \renewcommand{\arraystretch}{0.78} % Tighter
    \begin{tabular}{c|cc|cc|cc|cc|cc|cc|cc|cc|c}
    \hline
    \multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{2}{c|}{AMOS-CT \cite{ji2022amos}} & \multicolumn{2}{c|}{AMOS-MRI \cite{ji2022amos}} & \multicolumn{2}{c|}{PH2 \cite{mendoncca2013ph}} & \multicolumn{2}{c|}{COVID19-2 \cite{COVID19_2}} & \multicolumn{2}{c|}{STU \cite{zhuang2019rdau}} & \multicolumn{2}{c}{CVC-300 \cite{vazquez2017benchmark}} & \multicolumn{2}{c}{CVC-Colon \cite{tajbakhsh2015automated}} & \multicolumn{2}{c|}{ETIS \cite{silva2014toward}} & \multicolumn{1}{c}{\multirow{2}{*}{$P$-value}} \\ \cline{2-17}
     & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU & DSC & mIoU & \\
     \hline
     UNet \cite{ronneberger2015u}             & 56.3 \tiny{(2.1)} & 44.8 \tiny{(1.4)} & 8.3 \tiny{(2.5)} & 5.3 \tiny{(1.9)} & 90.3 \tiny{(0.1)}  & 83.5 \tiny{(0.1)} & 47.1 \tiny{(0.7)} & 37.7 \tiny{(0.6)} & 71.6 \tiny{(1.0)} & 61.6 \tiny{(0.7)} & 66.1 \tiny{(2.3)} & 58.5 \tiny{(2.1)} & 56.8 \tiny{(1.3)} & 49.0 \tiny{(1.2)} & 41.6 \tiny{(1.1)} & 35.4 \tiny{(1.0)} & 1.7E-08  \\
     UNet++ \cite{zhou2018unet++}             & 67.5 \tiny{(2.3)} & 56.6 \tiny{(2.8)} & 6.0 \tiny{(1.2)} & 3.8 \tiny{(1.5)} & 88.0 \tiny{(0.3)}  & 80.1 \tiny{(0.3)}  & 50.5 \tiny{(3.3)} & 40.9 \tiny{(3.7)} & 77.3 \tiny{(0.4)} & 67.8 \tiny{(0.3)} & 64.4 \tiny{(2.2)} & 58.4 \tiny{(2.0)} & 57.5 \tiny{(0.4)} & 50.2 \tiny{(0.4)} & 39.1 \tiny{(2.4)} & 34.0 \tiny{(2.1)} & 3.1E-07 \\
     CENet \cite{gu2019net}                   & 67.9 \tiny{(2.3)}  & 56.5 \tiny{(2.4)} & 14.5 \tiny{(1.4)} & 9.0 \tiny{(1.6)} & 90.5 \tiny{(0.1)} & 83.3 \tiny{(0.1)} & 60.1 \tiny{(0.3)} & 49.9 \tiny{(0.3)} & 86.0 \tiny{(0.7)} & 77.2 \tiny{(0.9)} & 85.4 \tiny{(1.6)} & 78.2 \tiny{(1.4)} & 65.9 \tiny{(1.6)} & 59.2 \tiny{(0.1)} & 57.0 \tiny{(3.4)} & 51.4 \tiny{(0.5)} & 1.8E-05 \\
     TransUNet \cite{chen2021transunet}       & 68.3 \tiny{(1.1)} & 57.7 \tiny{(1.2)} & 9.1 \tiny{(2.3)} & 5.8 \tiny{(2.5)} & 89.5 \tiny{(0.3)} & 82.1 \tiny{(0.4)} & 56.9 \tiny{(1.0)} & 48.0 \tiny{(0.7)} & 41.4 \tiny{(4.5)} & 32.1 \tiny{(4.2)} & 85.0 \tiny{(0.6)} & 77.3 \tiny{(0.3)} & 63.7 \tiny{(0.1)} & 58.4 \tiny{(0.3)} & 50.1 \tiny{(0.5)} & 44.0 \tiny{(2.3)} & 7.3E-06 \\
     MSRFNet \cite{srivastava2021msrf}        & 61.8 \tiny{(1.3)} & 51.3 \tiny{(1.7)} & 6.5 \tiny{(2.3)} & 4.2 \tiny{(1.9)} & 90.5 \tiny{(0.3)} & 83.5 \tiny{(0.3)} & 58.3 \tiny{(0.8)} & 48.4 \tiny{(0.6)} & 84.0 \tiny{(5.5)} & 75.2 \tiny{(5.2)} & 72.3 \tiny{(2.2)} & 65.4 \tiny{(2.2)} & 61.5 \tiny{(1.0)} & 54.8 \tiny{(0.8)} & 38.3 \tiny{(0.6)} & 33.7 \tiny{(0.7)} & 5.6E-06 \\
     DCSAUNet \cite{xu2023dcsau}              & 45.7 \tiny{(1.2)} & 36.3 \tiny{(1.5)} & 1.7 \tiny{(0.5)} & 1.1 \tiny{(0.2)} & 89.0 \tiny{(0.4)} & 81.5 \tiny{(0.3)} & 52.4 \tiny{(1.2)} & 44.0 \tiny{(0.7)} & 86.1 \tiny{(0.5)} & 76.5 \tiny{(0.8)} & 68.9 \tiny{(4.0)} & 59.8 \tiny{(3.9)} & 57.8 \tiny{(0.4)} & 49.3 \tiny{(0.4)} & 43.0 \tiny{(3.0)} & 36.1 \tiny{(2.9)} & 6.9E-07  \\
     M2SNet \cite{zhao2023m}                  & 69.6 \tiny{(1.3)} & 58.5 \tiny{(0.5)} & 22.0 \tiny{(0.6)} & 14.7 \tiny{(0.8)} & 90.7 \tiny{(0.3)} & 83.5 \tiny{(0.5)} & 68.6 \tiny{(0.1)} & 58.9 \tiny{(0.2)} & 79.4 \tiny{(0.7)} & 69.3 \tiny{(0.6)} & \textbf{\underline{90.0}} \tiny{(0.2)} & \textbf{\underline{83.2}} \tiny{(0.3)} & 75.8 \tiny{(0.7)} & 68.5 \tiny{(0.5)} & 74.9 \tiny{(1.3)} & 67.8 \tiny{(1.4)} & 1.3E-04 \\
     ViGUNet \cite{jiang2023vig}              & 50.8 \tiny{(2.7)} & 42.8 \tiny{(2.3)} & 6.5 \tiny{(0.2)} & 4.5 \tiny{(0.3)} & 90.1 \tiny{(0.7)} & 82.8 \tiny{(0.5)} & 49.7 \tiny{(0.3)} & 41.9 \tiny{(0.4)} & 75.5 \tiny{(1.2)} & 65.0 \tiny{(1.1)} & 72.9 \tiny{(0.4)} & 62.8 \tiny{(0.2)} & 53.7 \tiny{(0.6)} & 53.7 \tiny{(0.7)} & 38.6 \tiny{(3.1)} & 31.6 \tiny{(2.2)} & 3.9E-09 \\
     PVT-GCAS \cite{rahman2024g}          & 69.3 \tiny{(1.4)} & 58.5 \tiny{(1.6)} & 32.8 \tiny{(2.4)}  & 24.3 \tiny{(2.2)} & \textit{91.5} \tiny{(1.3)} & 84.9 \tiny{(1.5)} & 71.0 \tiny{(0.4)} & 60.4 \tiny{(0.4)} & 86.4 \tiny{(0.6)} & 76.6 \tiny{(0.2)} & 88.2 \tiny{(0.4)} & 81.0 \tiny{(0.5)} & 79.5 \tiny{(0.9)} & 71.6 \tiny{(0.9)} & \textit{79.5} \tiny{(0.7)} & \textit{71.6} \tiny{(1.0)} & 3.9E-04 \\
     CFATUNet \cite{wang2024cfatransunet} & 68.0 \tiny{(2.6)} & 56.7 \tiny{(2.1)} & 35.9 \tiny{(3.2)} & 25.9 \tiny{(3.6)} & \textit{91.5} \tiny{(0.6)} & \textit{85.0} \tiny{(0.7)} & 65.7 \tiny{(1.2)} & 56.2 \tiny{(1.1)} & \textit{87.9} \tiny{(0.1)} & \textit{79.2} \tiny{(0.2)} & \textit{89.1} \tiny{(0.6)} & 82.4 \tiny{(0.6)} & 78.0 \tiny{(0.9)} & 70.3 \tiny{(0.8)} & 77.0 \tiny{(0.6)} & 69.5 \tiny{(0.7)} & 8.8E-05 \\
     MADGNet \cite{nam2024modality}           & \textit{74.9} \tiny{(1.2)} & \textit{64.4} \tiny{(1.3)} & 14.8 \tiny{(2.5)} & 9.8 \tiny{(2.7)} & 91.3 \tiny{(0.1)}  & 84.6 \tiny{(0.1)} & \textit{72.2} \tiny{(0.1)} & \textbf{\underline{62.6}} \tiny{(0.3)} & \textbf{\underline{88.4}} \tiny{(1.0)} & \textbf{\underline{79.9}} \tiny{(1.5)} & 87.4 \tiny{(0.4)} & 79.9 \tiny{(0.4)} & 77.5 \tiny{(1.1)} & 69.7 \tiny{(1.2)} & 77.0 \tiny{(0.3)} & 69.7 \tiny{(0.5)} & 1.3E-02 \\ 
     GSENet \cite{li2025gse} & 74.2 \tiny{(0.2)} & 63.4 \tiny{(0.4)} & \textit{36.3} \tiny{(4.0)} & \textit{27.2} \tiny{(3.5)} & \textit{91.5} \tiny{(0.3)} & \textit{85.0} \tiny{(0.4)} & 66.3 \tiny{(0.3)} & 56.5 \tiny{(0.4)} & 86.3 \tiny{(0.4)} & 76.7 \tiny{(0.5)} & 89.0 \tiny{(0.1)} & 82.1 \tiny{(0.1)} & \textit{80.1} \tiny{(1.4)} & \textit{71.9} \tiny{(1.2)} & 79.3 \tiny{(0.4)} & 71.2 \tiny{(0.3)}  & 1.2E-02 \\
     \hline
     \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{TransGUNet}}} & \textbf{\underline{76.5}} \tiny{(0.5)} & \textbf{\underline{66.2}} \tiny{(0.4)} & \textbf{\underline{47.2}} \tiny{(1.1)} & \textbf{\underline{35.6}} \tiny{(1.2)} & \textbf{\underline{91.7}} \tiny{(0.3)} & \textbf{\underline{85.2}} \tiny{(0.2)} & \textbf{\underline{73.0}} \tiny{(0.2)} & \textit{62.4} \tiny{(0.2)} & 87.4 \tiny{(0.1)} & 78.2 \tiny{(0.4)} & \textbf{\underline{90.0}} \tiny{(0.3)} & \textit{83.1} \tiny{(0.1)} & \textbf{\underline{82.0}} \tiny{(0.2)} & \textbf{\underline{74.1}} \tiny{(0.3)} & \textbf{\underline{81.3}} \tiny{(0.3)} & \textbf{\underline{73.1}} \tiny{(0.2)} & \multicolumn{1}{c}{\multirow{2}{*}{-}} \\ \cline{2-17}
      & \textbf{+1.6} & \textbf{+1.8} & \textbf{+10.9} & \textbf{+8.4} & \textbf{+0.2} & \textbf{+0.2} & \textbf{+0.8} & \textbf{-0.2} & \textbf{-1.0} & \textbf{-1.7} & \textbf{+0.0} & \textbf{-0.1} & \textbf{+1.9} & \textbf{+2.2} & \textbf{+1.8} & \textbf{+1.5} & \\
    \hline
    \end{tabular}
    \caption{Segmentation results on eight different datasets with \textit{unseen} clinical settings. $(\cdot)$ denotes the standard deviations of multiple experiment results. We also provide the Wilcoxon signed rank test results ($P$-value) compared to our method and other methods.}
    \label{tab:comparison_sota_out_domain}
\end{table*}