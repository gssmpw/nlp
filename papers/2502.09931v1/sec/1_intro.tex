\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figure/Sup_GraphVisualization}
        \caption{Graph Visualization of TransGUNet. We selected three patches (\textcolor{red}{\textbf{Red}}, \textcolor{blue}{\textbf{Blue}}, \textcolor{ForestGreen}{\textbf{Green}}) and found the five nearest patches for each, based on the adjacency matrix of ACS-GNN, connecting them with lines to visualize the relationships using each color. This figure reveals that lesion patches exhibit high similarity with other lesion patches, while non-lesion patches similarly cluster together. This result demonstrates the modelâ€™s effectiveness in distinguishing lesion from non-lesion regions and maintaining strong intra-class correlations.}
    \label{fig:GraphVisualization}
\end{figure}

Medical image segmentation is crucial for the early detection of abnormal tissues and the development of treatment plans \cite{coates2015tailoring}. Traditional segmentation algorithms have received  considerable attention from medical experts \cite{otsu1979threshold, haralick1987image, kass1988snakes, tizhoosh2005image}. However, these methods still lack generalizability owing to the severe noise, inhomogeneous intensity distribution, and various clinical settings in medical images \cite{riccio2018new}. Consequently, this issue has raised concerns about the reliability of computer-based diagnostic procedures \cite{gaube2021ai}.

Recently, convolutional neural networks (CNNs) have been widely employed for medical image segmentation owing to their robustness in capturing local and spatial hierarchical features \cite{ronneberger2015u, zhao2021automatic, nam2023m3fpolypsegnet}. However, CNN-based models struggle to capture the global dependencies necessary to understand the complex anatomical structures in medical images \cite{hatamizadeh2022unetr}. This limitation has expanded the use of transformers to extract global dependencies for medical image segmentation \cite{chen2021transunet, cao2022swin}. However, despite their strengths, transformer-based models often fail to bridge the semantic gap between the encoder and decoder, hindering their ability to fully leverage global dependencies and resulting in sub-optimal segmentation performance \cite{wang2022uctransnet}.

Several models have been actively employed to improve skip connections to reduce this semantic gap for medical image segmentation. The most representative of these attempts is UNet++ \cite{zhou2018unet++}, which uses cross-scale feature fusion through dense connectivity in skip connection. Similarly, UCTransNet \cite{wang2022uctransnet}  and CFATransUNet \cite{wang2024cfatransunet} adopted a transformer-based approach to capture local cross-channel interactions of feature maps from a channel-wise perspective. However, these models suffer from increased computational complexity and ambiguous attention owing to their dense connectivity, extensive use of transformer blocks, and complex background in medical images with severe noise. Consequently, addressing the question, \say{\textit{How can we efficiently leverage global dependency without ambiguity while reducing the semantic gap between the encoder and decoder?}} is critical to overcoming these challenges and improving performance for medical image segmentation.

To answer this question, we focused on graph neural networks (GNNs), which are particularly suitable for flexibly and effectively capturing local and global dependencies, making them ideal for complex visual perception tasks \cite{han2022vision}. By leveraging this capability, we propose an \textit{attentional cross-scale GNN (ACS-GNN)} that can efficiently reduce the semantic gap between the encoder and decoder. It transforms cross-scale feature maps into graphs and applies attention to each node to facilitate robust feature integration. Additionally, we observed that deep learning models often produce uninformative feature maps that degrade the quality of spatial attention maps \cite{chen2021lesion, shawn2024ct}. To address this issue, we introduce an \textit{entropy-driven feature selection (EFS)}, which calculates entropy per channel and filters out high-entropy channels. By integrating \textit{ACS-GNN} and \textit{EFS-based spatial attention}, we designed a new medical image segmentation model called \textbf{TransGUNet} which effectively captures the relationships between patches, regardless of the lesion size and distance between patches (Fig. \ref{fig:GraphVisualization}). Extensive experimental results demonstrate that our graph-based approach consistently outperforms transformer- and convolution-based methods. Thus, TransGUNet represents a significant advancement in skip connection frameworks for medical image segmentation and offers a robust and efficient solution to the existing challenges. The main contributions of this study can be summarized as follows:

\begin{itemize}
    \item We propose \textbf{TransGUNet}, a novel medical image segmentation model that leverages the cross-scale GNN-based skip connection framework without ambiguous spatial attention and is applicable to various modalities and clinical settings. To the best of our knowledge, our novel skip connection framework is the first study to successfully and effectively exploit attentional cross-scale GNN with non-ambiguous spatial attention for medical image segmentation.

    \item The proposed \textit{attentional cross-scale graph neural network (ACS-GNN)} allows the model to comprehend the complex anatomical structures within medical images. Additionally, we incorporated \textit{entropy-driven feature selection (EFS)} with spatial attention to produce more reliable spatial attention maps.

    \item Our experimental results demonstrate that TransGUNet significantly outperforms transformer- and convolution-based approaches employed for medical image segmentation with various clinical settings.
\end{itemize}
