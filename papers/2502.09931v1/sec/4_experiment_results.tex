\section{Experiment Results}

\subsection{Experiment Settings}

Each model was trained and evaluated on five medical segmentation tasks, including multi-organ, skin cancer, COVID-19 infection, breast tumors, and polyp. For convenience, we denote the \textit{seen} clinical settings (Tab. \ref{tab:comparison_sota_in_domain}) as the test dataset, which has the same distribution with the training dataset. Moreover, we additionally evaluate the domain generalizability of each model on eight external segmentation datasets using different distributions for the training and test datasets, which are referred to as \textit{unseen} clinical settings (Tab. \ref{tab:comparison_sota_out_domain}). Due to the page limit, we present the detailed dataset description and split information in the Appendix (Tab. \ref{tab:seen_clinical_dataset} and \ref{tab:unseen_clinical_dataset}). To evaluate the performance of each model, we selected two metrics, the Dice Score Coefficient (DSC) and mean Intersection over Union (mIoU), which are widely used in medical image segmentation. Additionally, the quantitative results with more various metrics are also available in the Appendix.

We compared the proposed \textbf{TransGUNet (Ours)} with twelve representative medical image segmentation models, including UNet \cite{ronneberger2015u}, UNet++ \cite{zhou2018unet++}, CENet \cite{gu2019net}, TransUNet \cite{chen2021transunet}, MSRFNet \cite{srivastava2021msrf}, DCSAUNet \cite{xu2023dcsau}, M2SNet \cite{zhao2023m}, ViGUNet \cite{jiang2023vig}, PVT-GCAS \cite{rahman2024g}, CFATUNet \cite{wang2024cfatransunet}, MADGNet \cite{nam2024modality}, and GSENet \cite{li2025gse}. In all results, we report the mean performance of three trials for reliability. In all tables, \textbf{\underline{Bold}} and \textit{italic} are the first and second best performance results, respectively. And, the last row in Tab. \ref{tab:comparison_sota_in_domain} and \ref{tab:comparison_sota_out_domain} indicates the performance gap between \textbf{TransGUNet} and other second best method.

\subsection{Implementation Details}
We implemented TransGUNet on a single NVIDIA RTX 3090 Ti in Pytorch 1.8.

\noindent\textbf{Multi-Organ Segmentation.} Following the previous literature \cite{wang2024cfatransunet}, we employ an Adam optimizer with a learning rate of 0.001 for multi-organ segmentation. We optimize each model using a batch size of 24 and train them for 150 epochs. During training, we used flipping with a probability of 50\% and rotation between $-20^{\circ}$ and $20^{\circ}$. Because volumes in \textit{seen} and \textit{unseen} clinical settings have different resolutions, all images were resized to $224 \times 224$. We would like to clarify that we used identical settings in CFATUNet, which is the most recent multi-organ segmentation model. Note that we utilize the same settings on the multi-organ segmentation task to train all models for fair comparison.

\noindent\textbf{Binary Segmentation.} We started with an initial learning rate of $10^{-4}$ using the Adam optimizer and reduced the parameters of each model to $10^{-6}$ using a cosine annealing learning rate scheduler. We optimized each model using a batch size of 16 and trained them for 50, 100, 100, and 200 epochs on polyp, skin cancer, breast tumors, and COVID-19 infection segmentation tasks. In the training process, horizontal and vertical flips were applied with a 50\% probability, along with rotations ranging from $-5^{\circ}$ to $5^{\circ}$, as part of a multi-scale training strategy. This approach is commonly utilized in medical image segmentation models \cite{fan2020pranet, zhao2021automatic, zhao2023m, nam2024modality}. Because images in each dataset have different resolutions, all images were resized to $352 \times 352$. Additionally, we would like to clarify that we used identical settings in M2SNet and MADGNet, which are the most representative medical image segmentation methods.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figure/EfficiencyAnalysis}
        \caption{Comparison of parameters (M), FLOPs (G), inference speed (ms), and required GPU memory (G) vs segmentation performance (DSC) on average for all \textit{unseen} datasets.} \vspace{-0.50cm}
    \label{fig:EfficiencyAnalysis}
\end{figure}

\noindent\textbf{Hyperparameters of TransGUNet.} Key hyperparameters for TransGUNet on all datasets were set to $C_{r} = 64$ for efficiency and $(H_{t}, W_{t}) = (\frac{H}{8}, \frac{W}{8}), K = 11, k = 3$ in ACS-GNN and $M = 64$ in EFS-based spatial attention. In the Appendix, we provide the experiment results on various hyperparameter settings (Tab. \ref{tab:ablation_backbone_networks},
\ref{tab:ablation_target_resolution},
\ref{tab:ablation_eca_kernel_size}, \ref{tab:ablation_repetition_time}).

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure/QualitativeResults}
    \caption{Qualitative comparison of other methods and TransGUNet.  (a) Input images with ground truth. (b) UNet \cite{ronneberger2015u}. (c) UNet++ \cite{zhou2018unet++}. (d) CENet \cite{gu2019net}. (e) TransUNet \cite{chen2021transunet}. (f) MSRFNet \cite{srivastava2021msrf}. (g) DCSAUNet \cite{xu2023dcsau}. (h) ViGUNet \cite{jiang2023vig}. (i) M2SNet \cite{zhao2023m}. (j) PVT-GCASCADE \cite{rahman2024g}. (k) CFATransUNet \cite{wang2024cfatransunet}. (l) MADGNet \cite{nam2024modality}. (m) GSENet \cite{li2025gse}. (n) \textbf{TransGUNet (Ours)}. \textcolor{green}{\textbf{Green}} and \textcolor{red}{\textbf{Red}} lines denote the boundaries of the ground truth and prediction, respectively.} \vspace{-0.25cm}
    \label{fig:QualitativeResults}
\end{figure*}

\subsection{Comparison with State-of-the-art models}

We used the same model as that in Tab. \ref{tab:comparison_sota_in_domain} to evaluate the domain generalizability for unseen clinical settings Tab. \ref{tab:comparison_sota_out_domain}. For convenience, we denote $( \cdot, \cdot )$ as the performance improvement gap between TransGUNet and other models for seen and unseen clinical settings. As listed in Tab. \ref{tab:comparison_sota_in_domain} and \ref{tab:comparison_sota_out_domain}, TransGUNet achieved the highest segmentation performance across all various datasets on average. Compared to UNet++, M2SNet, and CFATransUNet, which focus on enhancing the skip connection framework, TransGUNet exhibited DSC improvements of (9.3\%, 22.3\%), (2.1\%, 7.2\%), and (1.6\%, 4.5\%), respectively. Additionally, compared to PVT-GCASCADE, which uses a single-scale GNN with spatial attention, TransGUNet demonstrated DSC improvement of (1.2\%, 3.8\%). Although MADGNet achieved the state-of-the-art performance in seen clinical settings, TransGUNet exhibited significant DSC improvement of 5.6\% for unseen clinical settings on average. Surprisingly, only TransGUNet achieved a DSC over 45\% on AMOS-MRI (MRI modality) when trained on Synapse (CT modality). These results indicate that employing an attentional cross-scale GNN-based skip connection with non-ambiguous spatial attention is crucial for understanding complex anatomical structures in medical images. Fig. \ref{fig:EfficiencyAnalysis} indicates that TransGUNet contains almost 25.0M parameters with 10.0G FLOPs, which has apparent advantages regarding computational efficiency. We provide detailed number of parameters, FLOPs, and Inference Time (ms) for each model in Appendix (Tab. \ref{tab:efficiency_analysis}).

\begin{table}[t]
    \centering
    \scriptsize
    \setlength\tabcolsep{2pt} % default value: 6pt
    % \renewcommand{\arraystretch}{0.8} % Tighter
    \begin{tabular}{c|ccc|cc|cc|c|c}
    \hline
    Setting & \multicolumn{3}{c|}{GNN} & \multicolumn{2}{c|}{\textit{Seen}} & \multicolumn{2}{c|}{\textit{Unseen}} & \multicolumn{1}{c|}{\multirow{2}{*}{Param (M)}} & \multicolumn{1}{c}{\multirow{2}{*}{FLOPs (G)}} \\ \cline{2-8}
    Name & Single & Cross & NA & DSC & mIoU & DSC & mIoU &  & \\
    \hline
    S0 &  &  &  & 90.6 & 84.5 & 82.8 & 75.4 & 24.3M & 8.9G \\
    \hline
    S1 & \cmark &  &  & 92.1 & 87.3 & 82.4 & 74.5 & 24.4M & 9.3G \\
    S2 & \cmark &  & \cmark & \textit{92.3} & \textit{87.6} & 82.7 & 75.4 & 24.4M & 9.3G \\
    \hline
    S3 &  & \cmark &  & 92.2 & \textit{87.6} & \textit{83.5} & \textit{75.8} & 25.0M & 10.0G \\
    \textbf{S4 (Ours)} &  & \cmark & \cmark & \textbf{\underline{92.7}} &\textbf{\underline{88.1}} & \textbf{\underline{84.4}} & \textbf{\underline{76.8}} & 25.0M & 10.0G \\
    \hline
    \end{tabular}
    \caption{Ablation study of ACS-GNN in skip connection on \textit{Seen} and \textit{Unseen} polyp segmentation datasets. \lq Single\rq, \lq Cross\rq, and \lq NA\rq\, denote Single-Scale GNN, Cross-Scale GNN, and Node Attention, respectively.} %\vspace{-0.25cm}
    \label{tab:ablation_on_GNN}
\end{table}

Fig. \ref{fig:QualitativeResults} illustrates the qualitative results of the various models. UNet, CENet, MSRFNet, and DCSAUNet, which do not incorporate global dependencies or reduce the semantic gap between the encoder and decoder, produce noisy and unreliable predictions. Despite the advantage of UNet++ and M2SNet for reducing the semantic gap through nested convolution and subtraction units, respectively, their predictions remain for unreliable in colonoscopy images comprising complex polyp structures or ultrasounds containing severe noise. Although TransUNet uses global dependency, it is not employed in the decoder, resulting in inaccurate predictions. CFATransUNet improves upon the previous methods by reducing the semantic gap and incorporating transformer blocks at the decoding stage. However, it overlooks crucial spatial relationships and misses the essential fine and local details needed to interpret medical images effectively. Although PVT-GCASCADE mitigates these deficiencies to a certain extent, it fails to consider cross-scale interactions, reliable spatial attention and the importance of each node in the graph. Furthermore, as illustrated in Fig. \ref{fig:GraphVisualization}, TransGUNet can understand the relationships between semantically similar patches regardless of their distance. Consequently, despite the severe noise, lesions of various sizes, and complex anatomical structures in various modalities, TransGUNet produces reliable predictions due to the dual utilization of ACS-GNN and EFS-based spatial attention. More various qualitative results are available in the Appendix (Fig. \ref{fig:Sup_QualitativeResults_Dermatoscopy}, \ref{fig:Sup_QualitativeResults_Radiology}, \ref{fig:Sup_QualitativeResults_Ultrasound}, \ref{fig:Sup_QualitativeResults_Colonoscopy}).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{figure/USA_Ablation}
    \caption{Comparison of spatial attention map quality according to various $M = \{ 8, 16, 32, 64, 128, 256 \}$. (a) Input image, (b) Spatial attention map according to various $M$.} \vspace{-0.25cm}
    \label{fig:USA_Ablation}
\end{figure}

\subsection{Ablation Study on TransGUNet}
We conducted ablation studies on polyp segmentation task to demonstrate the effectiveness of ACS-GNN and EFS-based spatial attention. We want to clarify that the experiment settings for the ablation study are identical to the main experiments for fair comparison.

\noindent \textbf{Ablation Study on ACS-GNN.} As listed in Tab. \ref{tab:ablation_on_GNN}, our approach (S4) exhibited the best performance across all settings. S0 represents TransGUNet without an ACS-GNN. The most notable result is that the application of skip connections in GNN, whether single-scale (S1) or cross-scale (S3), improves the performance on the seen dataset. However, on the unseen dataset, the single-scale GNN exhibits a 0.4\% performance decrease whereas the cross-scale GNN exhibits a 0.7\% performance improvement. These experimental results explain the reason behind TransGUNet outperforming PVT-GCASCADE. Additionally, NA improved the performance of both single-scale (S2) and cross-scale (S4) GNNs, with a negligible increase in the number of parameters and FLOPs owing to the ECA-style attention mechanism. Consequently, TransGUNet, which employs ACS-GNN, performs significantly better across various modalities and clinical settings.

\noindent \textbf{Ablation Study on EFS-based Spatial Attention.} As mentioned in the implementation details section, because we set $C_{r} = 64$, the total number of channels in the skip connection equals $4C_{r} = 256$. Therefore, when $4C_{r} = M$, where $M$ is the number of selected channels, all the feature maps are used to produce a spatial attention map. In Fig. \ref{fig:USA_Ablation}, $(\cdot, \cdot)$ denotes DSC on seen and unseen clinical setting of polyp segmentation. Fig. \ref{fig:USA_Ablation} shows that $M = 256$ results in worse performance than the cases utilizing the EFS-based spatial attention ($M = \{ 8, 16, 32, 64, 128\}$). These results demonstrate that our feature selection approach improves the quality of spatial attention, an aspect not previously addressed. If we select fewer channels ($M = \{ 8, 16, 32 \}$), uncertainty in the spatial attention map increases due to a lack of information.

\begin{table}[t]
    \centering
    \scriptsize
    \setlength\tabcolsep{4.0pt} % default value: 6pt
    \begin{tabular}{c|cc|cc|c|c}
    \hline
    Target Resolution & \multicolumn{2}{c|}{\textit{Seen}}  & \multicolumn{2}{c|}{\textit{Unseen}} & \multicolumn{1}{c|}{\multirow{2}{*}{Param (M)}}  & \multicolumn{1}{c}{\multirow{2}{*}{FLOPs (G)}} \\ \cline{2-5}
    $(H_{t}, W_{t})$  & DSC & mIoU & DSC & mIoU & & \\ 
    \hline
    $(H / 4, W / 4)$                 & \textit{87.1} & \textit{80.5} & \textit{78.5} & \textbf{\underline{69.7}} & 25.0M & 13.6G \\
    $(H / 8, W / 8)$ \textbf{(Ours)} & \textbf{\underline{87.3}} & \textbf{\underline{80.6}} & \textbf{\underline{78.6}} & \textbf{\underline{69.7}} & 25.0M & 10.0G \\
    $(H / 16, W / 16)$               & 86.3 & 79.6 & 77.2 & \textit{68.2} & 25.0M & 9.2G \\
    $(H / 32, W / 32)$               & 86.2 & 79.4 & 76.6 & 67.7 & 25.0M & 9.0G \\
    \hline
    \end{tabular}
    \caption{Quantitative results for each \textit{Seen} and \textit{Unseen} datasets according to various target resolution $(H_{t}, W_{t})$.} \vspace{-0.25cm}
    \label{tab:ablation_target_resolution}
\end{table}

\noindent \textbf{Ablation Study on Target Resolution in ACS-GNN.} In this section, we conduct an ablation study to compare the performance and efficiency according to various target resolution $(H_{t}, W_{t}) = \{ (\frac{H}{4}, \frac{W}{4}), (\frac{H}{8}, \frac{W}{8}), (\frac{H}{16}, \frac{W}{16}), (\frac{H}{32}, \frac{W}{32}) \}$. Each resolution correspond to the resolutions of the feature maps $\mathbf{f}_{i} \in \mathbb{R}^{C_{i} \times \frac{H}{2^{i + 1}} \times \frac{W}{2^{i + 1}}}$ extracted from the Transformer encoder for $i = 1, 2, 3, 4$. We used $(\frac{H}{8}, \frac{W}{8})$ as a target resolution $(H_{t}, W_{t})$. As listed in Tab. \ref{tab:ablation_target_resolution}, the experimental results indicate that the resolution $(\frac{H}{8}, \frac{W}{8})$ achieved the best performance. Although $(\frac{H}{4}, \frac{W}{4})$ also demonstrated high performance, the efficiency significantly decreased due to the increased FLOPs associated with the higher resolution of the input feature maps and the inherent characteristics of GNNs. This indicates that while higher resolutions can capture more detailed information, they come at the cost of increased computational resources. 