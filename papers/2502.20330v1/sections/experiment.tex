\section{Experimental Setup}

\begin{table*}[!t]
\centering
\caption{Comprehensive evaluation of RAPID against baseline methods across different target-draft model configurations. We report performance on $\infty$Bench and LongBench v2, along with prefill time and throughput speedup on LongBench v2 (Long, CoT) subset. LC and RAG denote evaluating the target model on long and retrieval contexts, respectively.
% Results are organized by target models (LLaMA-3.1-8B/70B and Qwen2.5-7B/72B) and methods (LC: Long Context, RAG, and RAPID).
For RAPID, we evaluate both self-speculation (using same-size RAG drafter) and upward-speculation (using larger RAG drafter) settings. \textcolor{green}{Green}/\textcolor{red}{red} highlighting indicates better/worse performance compared to LC baseline. \textbf{Bold} and \underline{underline} indicate best and second best metric score.}
\resizebox{0.95\textwidth}{!}{
\renewcommand{\arraystretch}{1.5} % Increase row height for better readability
\Huge % Use a slightly smaller font size for better fit
\begin{tabular}{@{}l l l c c c c c c c c r r@{}}

\toprule
\textbf{Target Model} & \textbf{Method} & \textbf{Draft Model} & & \multicolumn{4}{c}{\textbf{$\infty$Bench}} & \multicolumn{2}{c}{\textbf{LongBench v2}} & \multicolumn{2}{c}{\textbf{Efficiency}} \\ 
\cmidrule(lr){5-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12}
& & & & \textbf{En. QA} & \textbf{En. MC} & \textbf{En. Sum} & \textbf{AVG.} & \textbf{Overall} & \textbf{Overall (CoT)} & \textbf{Prefill Time (s)} & \textbf{Speedup} \\ 
\midrule
\multirow{6}{*}{LLaMA-3.1-8B} & LC & \centering - & & 34.58 & 53.28 & \underline{30.14} & 39.33 & 28.0 & 30.4 & \underline{25.89} & 1.00$\times$ \\
& RAG & \centering - & & \scorecolor{34.58}{31.91}{31.91} & \scorecolor{53.28}{62.01}{62.01} & \scorecolor{30.14}{27.27}{27.27} & \scorecolor{39.33}{40.40}{40.40} & \scorecolor{28.0}{29.2}{29.2} & \scorecolor{30.4}{33.4}{33.4} & \prefillcolor{25.89}{0.36}{\bestscore{0.36}} & \scorecolor{1.00}{3.35}{\bestscore{3.35}}$\times$ \\
& SD & \centering - & & \scorecolor{34.58}{32.90}{32.90} & \scorecolor{53.28}{55.90}{55.90} & \scorecolor{30.14}{30.11}{30.11} & \scorecolor{39.33}{39.64}{39.64} & \scorecolor{28.0}{29.4}{29.4} & \scorecolor{30.4}{31.0}{31.0} & \prefillcolor{25.89}{26.37}{26.37} & \scorecolor{1.00}{1.63}{1.63}$\times$ \\
& MagicDec & \centering - & & \scorecolor{34.58}{29.83}{29.83} & \scorecolor{53.28}{52.03}{52.03} & \scorecolor{30.14}{30.18}{30.18} & \scorecolor{39.33}{37.35}{37.35} & \scorecolor{28.0}{29.2}{29.2} & \scorecolor{30.4}{30.6}{30.6} & \prefillcolor{25.89}{26.05}{26.05} & \scorecolor{1.00}{0.71}{0.71}$\times$ \\
& RAPID & LLaMA-3.1-8B (RAG) & & \scorecolor{34.58}{34.90}{\secondbestscore{34.90}} & \scorecolor{53.28}{63.32}{\secondbestscore{63.32}} & \scorecolor{30.14}{30.27}{\bestscore{30.27}} & \scorecolor{39.33}{42.83}{\secondbestscore{42.83}} & \scorecolor{28.0}{32.4}{\secondbestscore{32.4}} & \scorecolor{30.4}{34.2}{\secondbestscore{34.2}} & \prefillcolor{25.89}{26.37}{26.37} & \scorecolor{1.00}{2.10}{\secondbestscore{2.10}}$\times$ \\
& RAPID & LLaMA-3.1-70B (RAG) & & \scorecolor{34.58}{40.94}{\bestscore{40.94}} & \scorecolor{53.28}{79.04}{\bestscore{79.04}} & \scorecolor{30.14}{29.96}{29.96} & \scorecolor{39.33}{49.98}{\bestscore{49.98}} & \scorecolor{28.8}{38.8}{\bestscore{38.8}} & \scorecolor{30.4}{40.2}{\bestscore{40.2}} & \prefillcolor{25.89}{28.04}{28.04} & \scorecolor{1.00}{1.14}{1.14}$\times$ \\

\midrule
\multirow{3}{*}{LLaMA-3.1-70B} & LC & \centering - & & 36.48 & 68.56 & \textbf{30.18} & 45.07 & 31.6 & 36.2 & \underline{160.54} & 1.00$\times$ \\
& RAG & \centering - & & \scorecolor{36.48}{38.66}{\secondbestscore{38.66}} & \scorecolor{68.56}{76.86}{\secondbestscore{76.86}} & \scorecolor{30.18}{27.17}{27.17} & \scorecolor{45.07}{47.56}{\secondbestscore{47.56}} & \scorecolor{31.6}{38.0}{\secondbestscore{38.0}} & \scorecolor{36.2}{39.4}{\secondbestscore{39.4}} & \prefillcolor{160.54}{2.81}{\bestscore{2.81}} & \scorecolor{1.00}{4.44}{\bestscore{4.44}}$\times$ \\
& RAPID & LLaMA-3.1-70B (RAG) & & \scorecolor{36.48}{40.56}{\bestscore{40.56}} & \scorecolor{68.56}{81.66}{\bestscore{81.66}} & \scorecolor{30.18}{29.64}{\secondbestscore{29.64}} & \scorecolor{45.07}{50.62}{\bestscore{50.62}} & \scorecolor{31.6}{40.2}{\bestscore{40.2}} & \scorecolor{36.2}{40.2}{\bestscore{40.2}} & \prefillcolor{160.54}{163.43}{163.43} & \scorecolor{1.00}{2.69}{\secondbestscore{2.69}}$\times$ \\
\midrule
\multirow{4}{*}{Qwen2.5-7B} & LC & \centering - & & 16.93 & 66.81 & 30.62 & 38.12 & 30.2 & 33.2 & \underline{20.32} & 1.00$\times$ \\
& RAG & \centering - & & \scorecolor{16.93}{20.28}{\secondbestscore{20.28}} & \scorecolor{66.81}{75.11}{75.11} & \scorecolor{30.62}{25.60}{25.60} & \scorecolor{38.12}{40.33}{40.33} & \scorecolor{30.2}{31.2}{31.2} & \scorecolor{33.2}{33.8}{33.8} & \prefillcolor{20.32}{0.34}{\bestscore{0.34}} & \scorecolor{1.00}{6.47}{\bestscore{6.47}}$\times$ \\
& RAPID & Qwen2.5-7B (RAG) & & \scorecolor{16.93}{19.81}{19.81} & \scorecolor{66.81}{75.98}{\secondbestscore{75.98}} & \scorecolor{30.62}{31.64}{\secondbestscore{31.64}} & \scorecolor{38.12}{42.48}{\secondbestscore{42.48}} & \scorecolor{30.2}{32.0}{\secondbestscore{32.0}} & \scorecolor{33.2}{35.4}{\secondbestscore{35.4}} & \prefillcolor{20.32}{21.62}{21.62} & \scorecolor{1.00}{2.65}{\secondbestscore{2.65}}$\times$ \\
& RAPID & Qwen2.5-72B (RAG) & & \scorecolor{16.93}{30.10}{\bestscore{30.10}} & \scorecolor{66.81}{83.84}{\bestscore{83.84}} & \scorecolor{30.62}{32.21}{\bestscore{32.21}} & \scorecolor{38.12}{48.72}{\bestscore{48.72}} & \scorecolor{30.2}{35.6}{\bestscore{35.6}} & \scorecolor{33.2}{41.2}{\bestscore{41.2}} & \prefillcolor{20.32}{23.45}{23.45} & \scorecolor{1.00}{0.93}{0.93}$\times$ \\
\midrule
\multirow{3}{*}{Qwen2.5-72B} & LC & \centering - & & \scorecolor{39.21}{39.21}{\secondbestscore{39.21}} & \scorecolor{81.66}{81.66}{\secondbestscore{81.66}} & \scorecolor{32.45}{32.45}{\secondbestscore{32.45}} & \scorecolor{51.11}{51.11}{\secondbestscore{51.11}} & \scorecolor{40.0}{40.0}{\secondbestscore{40.0}} & \scorecolor{43.9}{43.9}{\secondbestscore{43.9}} & \prefillcolor{162.42}{162.42}{\secondbestscore{162.42}} & \scorecolor{1.00}{1.00}{1.00}$\times$ \\
& RAG & \centering - & & \scorecolor{39.21}{30.72}{30.72} & \scorecolor{81.66}{80.22}{80.22} & \scorecolor{32.45}{28.63}{28.63} & \scorecolor{51.11}{46.52}{46.52} & \scorecolor{40.0}{38.8}{38.8} & \scorecolor{43.9}{39.8}{39.8} & \prefillcolor{162.42}{3.09}{\bestscore{3.09}} & \scorecolor{1.00}{3.60}{\bestscore{3.60}}$\times$ \\
& RAPID & Qwen2.5-72B (RAG) & & \scorecolor{39.21}{40.52}{\bestscore{40.52}} & \scorecolor{81.66}{85.59}{\bestscore{85.59}} & \scorecolor{32.45}{32.94}{\bestscore{32.94}} & \scorecolor{51.11}{53.02}{\bestscore{53.02}} & \scorecolor{40.0}{42.9}{\bestscore{42.9}} & \scorecolor{43.9}{44.1}{\bestscore{44.1}} & \prefillcolor{162.42}{164.80}{164.80} & \scorecolor{1.00}{1.98}{\secondbestscore{1.98}}$\times$ \\
\bottomrule
\end{tabular}
}
% \caption{Performance metrics comparison among different models and configurations, organized by target model and method.}

% \caption{Main results comparing RAPID with baseline approaches. We evaluate models on $\infty$Bench and LongBench v2. For $\infty$Bench, we report F1 score for En.QA, accuracy for En.MC, ROUGE-L-Sum for En.Sum, and their average (AVG.). For LongBench v2, we report overall accuracy w/ and w/o CoT. We also report the prefill time (seconds) and generation throughput (tokens/second) to measure efficiency. LC denotes long-context target LLM baseline, and RAG represents retrieval-augmented generation baseline. Best results for each target model are \underline{underlined}.}

  
\label{tab:main_results}
\end{table*}








% \subsection{Implementation Details}

% \paragraph{Target and Draft LLMs} 
% Given a long-context input of target LLM with a query, RAPID utilize a RAG module to retrieve the relevant text segments with the query in the long context. The retrieval context will be used to query  a draft LLM (RAG drafter) to generate $\gamma$ tokens each time, which are verified by the target LLM. Specifically, we choose LLaMA-3.1-8B, LLaMA-3.1-70B, Qwen2.5-7B and Qwen2.5-70B as the target LLMs in~\ourMethod{}, respectively. There are two settings of RAPID: (1) \textit{self-speculation} (Self-Spec), where long-context target LLM and RAG drafter are of the same scale; and (2) \textit{upward-speculation} (Upward-Spec), where RAG drafter involves larger parameter scale than target LLM. For the smaller target LLMs including LLaMA-3.1-8B and Qwen2.5-7B, we apply both settings of RAPID that utilizing the LLaMA-3.1-8B xxx as RAG drafter. For larger LLaMA-3.1-70B and Qwen2.5-70B, we apply the Self-Spec that xxx.


% \paragraph{RAG Setup.} We split the long-context input with a segment length of 512 tokens. The text segments and query are encoded by BGE-M3~\citep{chen-etal-2024-m3} model as embeddings. We subsequently sample the top-$k$ segments by the cosine similarity with the query embeddings. We set the maximum retrieval length as 1/24 long context length and minimun retrieval length as 4096. We filter the cosine similarity below the threshold of 0.3 in retrieved segments.


\subsection{Implementation Details}

\paragraph{Target and Draft LLMs.} 
RAPID is evaluated across different model scales using LLaMA-3.1 (8B, 70B) and Qwen2.5 (7B, 72B) as target LLMs. We implement two speculation settings: (1) \textit{self-speculation}, where the RAG drafter matches the target LLM's scale, and (2) \textit{upward-speculation}, where a larger RAG drafter assists a smaller target LLM. For smaller models (LLaMA-3.1-8B, Qwen2.5-7B), we evaluate both settings, while larger models (LLaMA-3.1-70B, Qwen2.5-72B) use self-speculation only. The RAG drafter generates $\gamma=10$ tokens per step for target LLM verification. We search $\eta$ in~\Cref{eq:new_target} between $\{5, 10, 20\} $  for self-speculation and $\{40, 50\} $ for upward-speculation, which would be further investigated in~\Cref{subsubsec:robustness}.

\paragraph{RAG Setup.} 
The long context is segmented into 512-token chunks and embedded using BGE-M3~\citep{chen-etal-2024-m3}. We retrieve top-$k$ segments based on cosine similarity with the query embedding, filtering out segments below a 0.3 similarity threshold. The retrieval context length is bounded between 4096 tokens and 1/24 of the input length.




% \paragraph{Hyperparameters.}



\subsection{Evaluation Protocol}



\paragraph{Baselines.} We compare our~\ourMethod{} with baselines including: (1) long-context target LLM (LC), where the target LLM in~\ourMethod{}  directly generates responses upon long context; (2) RAG, where the target LLM generates responses upon retrieval context of draft LLM input in~\ourMethod{}; (3) naive Speculative Decoding (SD), which involves identical target and draft LLMs with~\ourMethod{} but using the naive long-context target distribution; (4) MagicDec~\citep{chen2024magicdec}, which utilizes the StreamingLLM~\citep{xiao2023streamingllm} to compress the KV cache of draft model. We set the KV cache size as 4096 and sink tokens as 4.


\paragraph{Benchmarks.} We evaluate our \ourMethod{} with baselines on two benchmarks: (1) \textbf{$\infty$Bench}. We evaluate our method with baselines on three realistic tasks in this benchmark: long-book question answering (En.QA, metric: F1), multi-choice question-answering (En.MC, metric: accuracy), and summarization (En.Sum, metric: ROUGE-L-Sum). The context length in these tasks are beyond 100K. (2) \textbf{LongBench v2}, which involves multi-choice tasks across various context lengths from 8K to 2M words. We apply middle truncation following benchmark setup to ensure the context length within 128K tokens.


% \paragraph{Evaluation Setup.} 
% We measure the efficiency of our~\ourMethod{} and baselines across all expriments on the LongBench v2 (Long) subset with Chain-of-Thought (CoT)~\citep{Wei2022ChainOT} generation, which involves 120K context length and maximum 1K generation length. The prefill time and speedup is reported by the average over the subset, where speedup denotes RAPID (baselines) throughput / LC throughput for each target LLM. The temperature is set as 1 and 0.1 for $\infty$Bench and LongBench v2, respectively. More settings are listed in~\Cref{sec:extra_setup}.

\paragraph{Evaluation Setup} 
We conduct efficiency evaluations using the  LongBench v2 (Long, CoT) subset, where each example involves 120K (tokens) context length after truncation and 1K maximum
 generation tokens. Efficiency metrics include:
(1) \textit{prefill time} and (2) \textit{speedup}, computed as the ratio of method decoding throughput to LC throughput, both averaged across the subset. Additional experimental details are provided in~\Cref{sec:extra_setup}.



% For the self-specualtion evaluation of LLaMA-3.1-8B and Qwen2.5 7B, we 

% We use single A800 80GB GPU for the evaluation of LLaMA-3.1-8B and Qwen2.5. While for the evaluation involving LLaMA-3.1-8B



\begin{figure*}
    \centering
    \includegraphics[width=0.93\textwidth]{figures/gain_drop.pdf}
    % \caption{Accuracy gains and drops of RAG Drafter (Draft), naive SD, and RAPID relative to corresponding target LLM. Gains represent accuracy attained by each method (model) but not by the target LLM. Drops represent accuracy attained by the target LLM but not by compared methods. ``SD only'' and ``RAPID only'' denote accuracy gains or drops uniquely attained by SD and RAPID, respectively, in instances where the RAG Drafter does not attain.}
    \caption{Relative performance to target LLMs across different target-draft model configurations of LLaMA-3.1 series on LongBench v2 (Overall). RAPID integrates both benefits from target and draft LLMs, hence achieving higher relative success rate (benefits from draft) without increasing failure rate (benefits from target).
    Relative success represents correct predictions made by each method but missed by the target LLM. Relative failure represents correct predictions by the target LLM but missed by each method. ``SD Only'' and ``RAPID Only'' indicate correct (or wrong) predictions made exclusively by SD and RAPID where both target and draft models cannot attain.}
    \vspace{-0.7em}
    \label{fig:gains-drops}
\end{figure*}
















\section{Results and Analyses}


% \subsection{Main Results}
% This section compares our method with naive SD, long-context LLMs, RAG, MagicDec, and Triforce?

% \subsection{Main Results}
% We conduct comprehensive experiments to evaluate RAPID against baseline methods (LC and RAG) across different model configurations. The results in~\Cref{tab:main_results} demonstrate RAPID's effectiveness in both performance and efficiency.

% \paragraph{Comprehensive Performance Gains and Self-speculation Effectiveness.} In the self-speculation setting where identical models serve as both RAG drafter and target LLM, RAPID achieves an average score of 38.8 on $\infty$Bench with LLaMA-3.1-8B, substantially outperforming recent acceleration methods including MagicDec (\texttt{XX.X}), and naive SD (\texttt{XX.X}), while also showing significant performance gains over LC (30.0) and RAG (29.2) baselines respectively. This performance advantage is consistently observed across model architectures - for Qwen2.5-7B, RAPID attains 35.6 average score compared to TriForce (\texttt{XX.X}), MagicDec (\texttt{XX.X}), and naive SD (\texttt{XX.X}). The effectiveness of our test-time knowledge transfer mechanism is particularly evident in structured reasoning tasks, where RAPID achieves up to 77.8\% gains in Entity QA performance, and in its ability to maintain high performance even when RAG and LC baselines show varying strengths across different tasks - adapting to scenarios where either LC or RAG exhibits superior performance.

% \paragraph{Computational Efficiency.} \textit{RAPID delivers substantial throughput improvements while maintaining minimal computational overhead.} Through its innovative compressed context operations, RAPID achieves a 1.96$\times$ throughput improvement for LLaMA-3.1-8B (24.59 vs. 11.73 tokens/sec), significantly surpassing the speedups of existing methods like TriForce (\texttt{X.X}$\times$) and MagicDec (\texttt{X.X}$\times$). The efficiency gains are even more pronounced for Qwen2.5-7B, where RAPID demonstrates a 2.6$\times$ throughput enhancement (15.65 vs. 5.91 tokens/sec). Notably, these improvements come with only minimal increase in prefill time (26.37s vs. 25.89s for LLaMA-3.1-8B), validating our theoretical analysis regarding the efficiency benefits of the RAG drafter architecture.

% \paragraph{upward-speculation Benefits.} \textit{RAPID enables unprecedented performance gains through larger RAG drafters while maintaining practical inference speeds.} When employing LLaMA-3.1-70B as the RAG drafter for LLaMA-3.1-8B target model, RAPID achieves remarkable improvements across all metrics, with the $\infty$Bench average score reaching 40.2 - a 34\% improvement over the LC baseline (30.0). Similarly, using Qwen2.5-72B as drafter for Qwen2.5-7B yields a 32.8\% improvement on LongBench v2 (44.1 vs. 33.2). The effectiveness of our test-time knowledge transfer mechanism is particularly evident here, as these substantial performance gains are achieved while maintaining throughput (13.40 tokens/sec) comparable to the original LC configuration (11.73 tokens/sec) for LLaMA-3.1-8B.


% These comprehensive results demonstrate that RAPID successfully addresses the fundamental challenges in long-context language model deployment. By effectively integrating retrieval-augmented generation with speculative decoding, and enabling efficient knowledge transfer during inference, RAPID establishes a new paradigm for both high-performance and computationally efficient long-context language model inference.




% \subsection{Main Results}
% We conduct comprehensive experiments to evaluate RAPID against baseline methods (LC and RAG) across different model configurations. The results in~\Cref{tab:main_results} demonstrate RAPID's effectiveness in both performance and efficiency.

% \paragraph{Long-Context Generation Quality}
% RAPID consistently improves generation quality across different model scales. For LLaMA-3.1-8B, upward-speculation with LLaMA-3.1-70B drafter achieves substantial gains on both $\infty$Bench (49.98 vs. 39.33 AVG. score) and LongBench v2 (38.8 vs. 30.0 Overall score). Similar improvements are observed with Qwen2.5-7B, where upward-speculation enhances the Overall score from 30.2 to 35.6. Notably, even larger models benefit from RAPID - LLaMA-3.1-70B shows improved AVG. scores (50.62 vs. 45.07) and Qwen2.5-72B achieves better Overall performance (42.9 vs. 40.0).

% \paragraph{Generation Efficiency}
% RAPID demonstrates strong efficiency advantages compared to conventional long-context models. With LLaMA-3.1-8B, self-speculation achieves 2.1$\times$ throughput improvement (24.59 vs. 11.73 tokens/second) over LC baseline. While upward-speculation with larger drafters shows moderate throughput gains (13.40 tokens/second), it offers better quality-efficiency trade-offs. For larger models like LLaMA-3.1-70B, RAPID maintains competitive throughput (5.17 tokens/second) while improving generation quality.

% Our results suggest that RAPID effectively addresses the quality-efficiency trade-off in long-context generation through its speculative approach, with both self-speculation and upward-speculation offering distinct advantages depending on deployment requirements.

\subsection{Main Results}
We evaluate RAPID against baselines across different model scales and benchmarks. The results in~\Cref{tab:main_results} demonstrate the effectiveness of RAPID in both improving generation quality and efficiency for long-context inference.

\paragraph{RAPID integrates benefits from both target LLM and RAG drafter through self-speculation.} In the self-speculation setting, where RAPID uses same-scale models for target and draft, consistent improvements are observed across model families. For LLaMA-3.1-8B, RAPID with self-speculation achieves superior performance on $\infty$Bench (42.83 vs 39.33 LC, 40.40 RAG) and LongBench v2 (34.2\% vs 30.4\% LC, 33.4\% RAG). Similar improvements are seen for LLaMA-3.1-70B (50.62 vs 45.07 LC, 47.56 RAG on $\infty$Bench) and Qwen2.5 series. Notably, RAPID effectively integrates the complementary strengths of LC and RAG approaches - while RAG shows superior performance on certain tasks (e.g., En.MC: 79.04\% vs 53.28\% LC for LLaMA-3.1-8B), LC demonstrates advantages in others (e.g., En.QA: 34.58\% vs 31.91\% RAG). RAPID successfully captures these complementary benefits during inference, consistently achieving better or comparable performance to the stronger of its two components. Compared to existing speculative decoding approaches including naive SD and MagicDec, RAPID demonstrates superior performance through this effective integration mechanism.

\paragraph{Larger RAG drafters further boost performance through effective knowledge transfer.} Beyond self-speculation, RAPID enables a unique upward-speculation mechanism where larger models serve as RAG drafters while maintaining efficiency. This setting yields even more substantial improvements: LLaMA-3.1-8B with 70B RAG drafter achieves 49.98 on $\infty$Bench and 40.2\% overall accuracy on LongBench v2, surpassing not only its self-speculation results but even the LC performance of LLaMA-3.1-70B (36.2\%). Similar patterns emerge for Qwen2.5-7B with 72B RAG drafter, where the performance gains (48.72 vs 42.48 on $\infty$Bench) demonstrate the effectiveness of RAPID in leveraging and integrating knowledge from larger models through the retrieval-augmented speculation.


% \paragraph{Computational Efficiency.} 
\paragraph {RAPID demonstrates $>2\times$ speedup for long-context infernce.} In self-speculation settings, RAPID achieves significant speedup over LC baseline (2.10$\times$ for LLaMA-3.1-8B, 2.69$\times$ for LLaMA-3.1-70B), and significantly surpasses naive SD and MagicDec. When employing upward-speculation with larger drafters, RAPID still maintains comparable throughput~\footnote{Note that upward-speculation requires extra GPUs to serve the RAG drafter like regular SD.} (1.14$\times$ for LLaMA-3.1-8B with 70B drafter, 0.93$\times$ for Qwen2.5-7B with 72B drafter) while substantially improving generation quality. While pure RAG shows highest throughput (e.g., 3.35$\times$ speedup for LLaMA-3.1-8B), its performance can be significantly compromised in certain scenarios (e.g., En.QA accuracy drops from 39.21 to 30.72 for Qwen2.5-72B). In contrast, RAPID effectively maintains competitive throughput while consistently achieving superior generation quality across different settings.

% These results demonstrate that RAPID successfully addresses the core challenges of long-context inference by:
% (1) enabling efficient speculation through retrieval-augmented drafting, 
% (2) effectively integrating the strengths of both LC and RAG approaches during inference, and 
% (3) maintaining competitive computational efficiency while improving generation quality. The strong performance across different model scales validates the effectiveness of RAPID as a general approach for accelerating long-context inference.



\begin{figure}[!t]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/context_length.pdf}
    \vspace{-0.6em}
    \caption{Impact of context and retrieval lengths on RAPID (self-sepculation) performance and efficiency based on LLaMA-3.1-8B. RAPID consistently outperforms naive SD and achieves speedup beyond 32K context length with moderate retrieval lengths ($\le$16K).\textbf{Top:}$\Delta$Accuracy indicates the accuracy margins on LongBench v2 (Long, CoT) subset over target LLM. \textbf{Middle:} Acceptance rate indicating the proportion of accepted draft tokens. \textbf{Bottom:} Speedup ratio compared to target LLM inference ($>1$ indicates acceleration). 
    % RAPID consistently outperforms naive SD and achieves speedup beyond 32K context length with moderate retrieval lengths ($\le$16K).
    }
    \vspace{-0.5em}
    \label{fig:context-length}
\end{figure}








% \paragraph{Performance Comparison}

% \paragraph{Efficiency Comparison}


% \subsection{Analyses and Ablations}




\subsection{Benefits Integration Analysis}

\paragraph{RAPID incorporates benefits from RAG drafter while maintaining target model capabilities.}
To analyze how \ourMethod{} integrates the strengths of both RAG drafter and target LLM, we examine the relative success and failure of RAG drafter, SD, and~\ourMethod{} on LongBench v2. As shown in~\Cref{fig:gains-drops}, \ourMethod{} successfully handles additional cases where the target LLM fails by incorporating beneficial knowledge from the RAG drafter. Meanwhile, \ourMethod{} maintains the capabilities of target LLM, exhibiting significantly lower failure rates compared to using RAG drafter alone. This combination of gains from RAG drafter with minimal degradation of target LLM capabilities enables \ourMethod{} to outperform both target and draft models. Furthermore, the gains from RAG drafter in \ourMethod{} substantially exceed those in naive SD, demonstrating the effectiveness of our retrieval-augmented target distribution in~\Cref{eq:new_target}.



% demonstrates remarkable effectiveness in two aspects. First, compared to direct RAG application, \ourMethod{} achieves substantial accuracy gains over target LLMs while maintaining significantly lower accuracy drops. Second, RAPID substantially outperforms naive SD in terms of accuracy gains  with comparable drop rates, despite sharing identical target LLMs and RAG drafters. These results jointly validate that the retrieval-augmented target distribution (\Cref{eq:new_target}) in~\ourMethod{} effectively incorporates knowledge from RAG drafters while its inherent long-context target distribution prevents harmful deviations.

\paragraph{RAPID exhibits capabilities beyond individual target/draft LLMs.} Most notably, we observe an ``emergent phenomenon'' where RAPID successfully handles cases that both the target LLM and RAG drafter fail individually (shown as ``RAPID Only'' in~\Cref{fig:gains-drops}). Specifically, this emergent accuracy mass grows more pronounced as RAG drafters become stronger, from LLaMA-3.1-8B to LLaMA-3.1-70B. This suggests that RAPID not only combines the strengths of both models but also enables new capabilities through their synergistic interaction. The phenomenon becomes particularly evident in the upward-speculation setting, where the stronger RAG drafter facilitates more sophisticated knowledge transfer during inference.








\begin{table}[!t]
\centering
\caption{Evaluation on multi-turn dialogue generation with extended chat history for LLaMA-3.1-8B as both target and draft LLM. Quality scores (1-10) are rated by GPT-4-Turbo-1106 using LLM-as-a-Judge protocol.}
\resizebox{0.93\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
 & Quality & Acceptance Rate (\%) & Throughput \\
\midrule
Target LLM & 2.82 & - & 10.64$_{\pm 0.98}$ \\
RAG Drafter & \underline{3.95} & - & \textbf{40.49}$_{\pm 0.47}$ \\
SD & 2.94 & \underline{56.34}$_{\pm 0.13}$ & 14.07$_{\pm 3.08}$ \\
RAPID & \textbf{4.21} & \textbf{76.94}$_{\pm 0.13}$ & \underline{18.18}$_{\pm 3.23}$ \\
\bottomrule
\end{tabular}
}
\vspace{-0.5em}
    
\label{tab:generation-quality}
\end{table}




\begin{table}[!t]
\centering
\caption{Robustness study of RAPID with different draft influence parameter $\eta$. Results show performance gains ($\Delta$Accuracy) and speedup ratios on LongBench v2 (Long, CoT) subset using LLaMA-3.1-8B as target LLM, with LLaMA-3.1-8B and LLaMA-3.1-70B as RAG drafters under unrelated retrieval context.}
\resizebox{0.93\linewidth}{!}{
\begin{tabular}{l cccc}
\toprule
\multirow{2}{*}{$\eta$} & \multicolumn{2}{c}{LLaMA-3.1-8B (Draft)} & \multicolumn{2}{c}{LLaMA-3.1-70B (Draft)} \\
\cmidrule(lr){2-3} \cmidrule(l){4-5}
 & $\Delta$Accuracy & Speedup & $\Delta$Accuracy & Speedup \\
\midrule
% naive & 0.00 & 1.00 & 0.00 & 1.00 \\
0 & 1.20 & 1.62$\times$ & -1.30 & 0.67$\times$ \\
5 & 2.80 & 1.75$\times$ & 0.40 & 0.69$\times$ \\
10 & 1.60 & 1.77$\times$ & 1.20 & 0.72$\times$ \\
20 & 1.20 & 1.78$\times$ & 4.40 & 0.75$\times$ \\
30 & -2.40 & 2.07$\times$ & 6.60 & 0.80$\times$ \\
40 & -2.60 & 2.08$\times$ & 6.60 & 0.84$\times$ \\
50 & -6.30 & 2.10$\times$ & 6.00 & 0.87$\times$ \\
\bottomrule
\end{tabular}
}
\vspace{-0.5em}
    
\label{tab:robustness}
\end{table}





\subsection{Impact of Context and Retrieval Length.}

\paragraph{RAPID demonstrates effectiveness across various context configurations.}
We analyze how RAPID performs under varying target context lengths and RAG drafter retrieval lengths, as shown in~\Cref{fig:context-length}. The results demonstrate consistent advantages of RAPID over naive SD across all configurations. First, RAPID achieves significantly better performance gains (2-8\% $\Delta$Accuracy) over the long-context baseline compared to the marginal or negative gains (-5-2\%) of naive SD. This superior performance is accompanied by consistently higher acceptance rates (75-85\% versus 60-70\%) and better speedup ratios across all context  and retrieval lengths configurations.




\paragraph{RAPID achieves speedup for long-context inference beyond 32K.}
The impact of retrieval length reveals an interesting efficiency-effectiveness trade-off. In terms of computational efficiency, RAPID achieves acceleration (speedup $> 1.0\times$) when the target context length exceeds 32K, while SD requires contexts beyond 64K to demonstrate speedup. For retrieval length, while longer retrieval contexts generally lead to higher acceptance rates (up to 85\%), the speedup ratio is not necessarily increasing. Specifically, retrieval lengths of 4K and 8K achieve nearly identical speedup ratios, indicating minimal overhead in this scope. However, when retrieval length exceeds 16K, the increased computational overhead from longer draft contexts becomes apparent and impacts the overall speedup. These findings suggest that RAPID achieves remarkable efficiency when accelerating long-context inference beyond 32K tokens upon moderate retrieval length within 16K.


\subsection{Generation Quality Analysis}

\paragraph{RAPID achieves superior generation quality and throughput in real-world application.}
To evaluate the effectiveness of RAPID in practical long-context applications, we assess its performance on multi-turn dialogue generation. We construct a challenging evaluation dataset by adapting MT-Bench-101~\citep{bai-etal-2024-mt}: for each of the first 100 samples, we preserve their last-turn queries while distributing their previous conversation context within a longer chat history comprising additional dialogue turns from another 500 samples in MT-Bench-101. The resulting chat history is of around 122K tokens length. This setup tests the ability of models to maintain coherence and relevance while processing extensive dialogue history.

As shown in~\Cref{tab:generation-quality}, RAPID demonstrates substantial improvements across all metrics. Using GPT-4-Turbo-1106 as evaluator following LLM-as-a-Judge~\citep{Zheng2023JudgingLW}, RAPID achieves a generation quality score of 4.21, significantly outperforming the target LLM (2.82), RAG drafter (3.95) and naive SD (2.94). This quality improvement comes with a robust acceptance rate of 76.94\% (vs. 56.34\% for SD) and enhanced throughput of 18.18 tokens/second (1.7$\times$ speedup over target LLM), demonstrating practical advantages of RAPID in real-world long-context applications.



% \paragraph{A case study: when long-context/rag helps generation}





\subsection{Robustness to Retrieval Quality}
\label{subsubsec:robustness}
\paragraph{RAPID shows robustness to retrieval quality, which is further enhanced by stronger drafter.}
To assess the robustness of~\ourMethod{} regarding retrieval quality, we conduct stress tests by deliberately using unrelated retrieval context (using the context of first sample from LongBench v2 for all samples) while varying the knowledge transfer parameter $\eta$ in~\Cref{eq:new_target}. As shown in~\Cref{tab:robustness}, with self-speculation (LLaMA-3.1-8B drafter), RAPID maintains performance gains ($\Delta$Accuracy $>$ 0) and improved efficiency (speedup 1.62$\times$-1.78$\times$) when $\eta \leq 20$, even with irrelevant retrieval context. However, when $\eta > 20$, the RAG drafter may overly impact the target distribution, leading to performance degradation. Moreover, upward-speculation with LLaMA-3.1-70B as drafter demonstrates even better robustness, maintaining positive performance gains (up to 6.60\%) across all $\eta$ values despite totally unrelated retrieval context. This increased resilience suggests that RAPID effectively leverages the inherent capabilities of stronger RAG drafters, maintaining reliable performance even under suboptimal retrieval quality.





% potentially due to its superior ability to maintain coherence with the target context.

% These findings indicate that RAPID is particularly effective for long context scenarios ($>$16k tokens), where it simultaneously addresses both the performance degradation and computational efficiency challenges of traditional approaches. The scalability of both performance gains and speedup with context length demonstrates RAPID's potential as a practical solution for processing increasingly longer documents.





% These findings demonstrate RAPID's robust performance across different retrieval configurations and its ability to maintain high efficiency even with minimal context retrieval, offering practical advantages for real-world deployment scenarios.

% \paragraph{fix the context length, change the rag length to check the performance, acceptance rate, and efficiency.}


% \subsubsection{Generation Quality Analysis}

% Our \ourMethod{} can be applied in practical long-context applications such as multi-turn dialogue, with improved generation quality and efficiency. Due to the lack of long-context benchmark for real-life multi-turn dialogue, we curate a dataset consisting of data samples with a long chat history and a query related to previous turns of dialogue distributed in the history. This dataset comprising 100 samples, is built from MT-Bench-101~\citep{bai-etal-2024-mt}, where the query in our data samples is the last-turn query of the first 100 samples in MT-Bench-101, with the previous turns distributed in the chat history comprising another 500 turns in MT-Bench-101. The generated responses are evaluated by GPT-4-Turbo-1106 following LLM-as-a-Judge~\citep{Zheng2023JudgingLW}. As listed in~\Cref{tab:generation-quality}, our RAPID can achieve significant quality and throughput gains over the target LLM (LLaMA-3.1-8B), with reasonable acceptance rate that surpass naive SD by large margins.



% \subsubsection{Generation Quality Analysis}
% To evaluate RAPID's effectiveness in practical scenarios, we assess its performance on multi-turn dialogue generation. We construct a test set from MT-Bench-101~\citep{bai-etal-2024-mt} by embedding each query within extended chat histories of 500 turns, creating 100 challenging samples that require long-context understanding.




















% \subsection{Ablation Study}

% \begin{itemize}
%     \item difference RAG embedding
%     \item using unrelated RAG context
%     \item Test-Time distillation dynamic: for different m, compare the performance and efficiency.
% \end{itemize}




% \paragraph{Test-Time distillation dynamic:} error analyses

% \begin{itemize}
%     \item How to measure the speed? \jinjie{ report speed value + plot speed scaling curve when scaling the generation length and context length}

% \jinjie{Add some comparisons with baselines, like magicdec}

% \jinjie{ablation on the rejection logits and other design choices. What about flipping the main model and draft model? And two long-context models or two RAG models (with different decoding settings to enhance each other, report both performance and speed)?}

% \jinjie{scanning over the m}

% \jinjie{When does the long context/rag helps generation? A case study on edge cases where only \ourMethod works}

% \end{itemize}




% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \begin{table*}[!t]
% \resizebox{\textwidth}{!}{
% \renewcommand\arraystretch{1.2}
% \Huge
% \begin{tabular}{llllcccccccccc}
% \hline
%                          &                                            &                                             &                          & \multicolumn{4}{c}{$\infty$Bench}                                                                                             &                          & \multicolumn{2}{c}{LongBench v2}                            &                                              & \multicolumn{2}{c}{Efficiency}                                                                                      \\ \cline{5-8} \cline{10-11} \cline{13-14} 
% \multirow{-2}{*}{Method} & \multirow{-2}{*}{Target Model}             & \multirow{-2}{*}{Draft  Model}              &                          & En. QA                        & En. MC                        & En. Sum                       & AVG.                          &                          & Overall                      & Overall (CoT)                &                                              & Prefill Time                                              & Throughput                                              \\
% Long Context             & LLaMA-3.1-8B- LC                           & \multicolumn{1}{c}{-}                       &                          & 34.58                         & 53.28                         & 30.14                         & 39.33                         &                          & 30.0                         & 30.4                         &                                              & 25.89 (0.32)                                              & 11.73 (0.24)                                            \\
% RAG                      & LLaMA-3.1-8B (RAG)                         & \multicolumn{1}{c}{-}                       &                          & 31.91                         & 62.01                         & 27.27                         & 40.40                         &                          & 29.2                         & 33.4                         &                                              & 0.36 (0.05)                                               & 39.34 (1.02)                                            \\
%                          & \cellcolor[HTML]{DAE8FC}LLaMA-3.1-8B (LC)  & \cellcolor[HTML]{DAE8FC}LLaMA-3.1-8B (RAG)  & \cellcolor[HTML]{DAE8FC} & \cellcolor[HTML]{DAE8FC}34.90 & \cellcolor[HTML]{DAE8FC}63.32 & \cellcolor[HTML]{DAE8FC}30.27 & \cellcolor[HTML]{DAE8FC}42.83 & \cellcolor[HTML]{DAE8FC} & \cellcolor[HTML]{DAE8FC}32.4 & \cellcolor[HTML]{DAE8FC}34.2 & \cellcolor[HTML]{DAE8FC}                     & \cellcolor[HTML]{DAE8FC}26.37 (0.26)                      & \cellcolor[HTML]{DAE8FC}24.59 (3.42)                    \\
% \multirow{-2}{*}{RAPID}  & \cellcolor[HTML]{DAE8FC}LLaMA-3.1-8B (LC)  & \cellcolor[HTML]{DAE8FC}LLaMA-3.1-70B (RAG) & \cellcolor[HTML]{DAE8FC} & \cellcolor[HTML]{DAE8FC}40.94 & \cellcolor[HTML]{DAE8FC}79.04 & \cellcolor[HTML]{DAE8FC}29.96 & \cellcolor[HTML]{DAE8FC}49.98 & \cellcolor[HTML]{DAE8FC} & \cellcolor[HTML]{DAE8FC}38.8 & \cellcolor[HTML]{DAE8FC}40.2 & \cellcolor[HTML]{DAE8FC}                     & \cellcolor[HTML]{DAE8FC}28.04 (0.5)                       & \cellcolor[HTML]{DAE8FC}13.40 (0.68)                    \\
% Long Context             & LLaMA-3.1-70B (LC)                         & \multicolumn{1}{c}{-}                       &                          & 36.48                         & 68.56                         & 30.18                         & 45.07                         &                          & 31.6                         & 36.2                         &                                              &                                                           &                                                         \\
% RAG                      & LLaMA-3.1-70B (RAG)                        & \multicolumn{1}{c}{-}                       &                          & 38.66                         & 76.86                         & 27.17                         & 47.56                         &                          & 38.0                         & 39.4                         &                                              & 2.81 (0.36)                                               & 18.92 (0.07)                                            \\
% RAPID                    & \cellcolor[HTML]{DAE8FC}LLaMA-3.1-70B (LC) & \cellcolor[HTML]{DAE8FC}LLaMA-3.1-70B (RAG) & \cellcolor[HTML]{DAE8FC} & \cellcolor[HTML]{DAE8FC}40.56 & \cellcolor[HTML]{DAE8FC}81.66 & \cellcolor[HTML]{DAE8FC}29.64 & \cellcolor[HTML]{DAE8FC}50.62 & \cellcolor[HTML]{DAE8FC} & \cellcolor[HTML]{DAE8FC}40.2 & \cellcolor[HTML]{DAE8FC}40.2 & \cellcolor[HTML]{DAE8FC}                     & \cellcolor[HTML]{DAE8FC}163.43 (1.48)                     & \cellcolor[HTML]{DAE8FC}5.17 (0.57)                     \\ \hline
% Long Context             & Qwen2.5-7B (LC)                            & \multicolumn{1}{c}{-}                       &                          & 16.93                         & 66.81                         & 30.62                         & 38.12                         &                          & 30.2                         & 33.2                         &                                              & 20.32 (0.19)                                              & 5.91 (0.02)                                             \\
% RAG                      & Qwen2.5-7B (RAG)                           & \multicolumn{1}{c}{-}                       &                          & 20.28                         & 75.11                         & 25.60                         & 40.33                         &                          & 31.2                         & 33.8                         &                                              & 0.34 (0.05)                                               & 38.26 (0.87)                                            \\
%                          & \cellcolor[HTML]{DAE8FC}Qwen2.5-7B (LC)    & \cellcolor[HTML]{DAE8FC}Qwen2.5-7B (RAG)    & \cellcolor[HTML]{DAE8FC} & \cellcolor[HTML]{DAE8FC}19.81 & \cellcolor[HTML]{DAE8FC}75.98 & \cellcolor[HTML]{DAE8FC}31.64 & \cellcolor[HTML]{DAE8FC}42.48 & \cellcolor[HTML]{DAE8FC} & \cellcolor[HTML]{DAE8FC}32.0 & \cellcolor[HTML]{DAE8FC}35.4 & \cellcolor[HTML]{DAE8FC}                     & \cellcolor[HTML]{DAE8FC}21.62 (0.13)                      & \cellcolor[HTML]{DAE8FC}15.65 (3.53)                    \\
% \multirow{-2}{*}{RAPID}  & \cellcolor[HTML]{DAE8FC}Qwen2.5-7B (LC)    & \cellcolor[HTML]{DAE8FC}Qwen2.5-72B (RAG)   & \cellcolor[HTML]{DAE8FC} & \cellcolor[HTML]{DAE8FC}30.10 & \cellcolor[HTML]{DAE8FC}83.84 & \cellcolor[HTML]{DAE8FC}32.21 & \cellcolor[HTML]{DAE8FC}48.72 & \cellcolor[HTML]{DAE8FC} & \cellcolor[HTML]{DAE8FC}35.6 & \cellcolor[HTML]{DAE8FC}41.2 & \multicolumn{1}{l}{\cellcolor[HTML]{DAE8FC}} & \multicolumn{1}{l}{\cellcolor[HTML]{DAE8FC}23.45 (0.42)}  & \multicolumn{1}{l}{\cellcolor[HTML]{DAE8FC}5.51 (0.87)} \\
% Long Context             & Qwen2.5-72B (LC)                           & \multicolumn{1}{c}{-}                       &                          & 39.21                         & 81.66                         & 32.45                         & 51.11                         &                          & 40.0                         & 43.9                         & \multicolumn{1}{l}{}                         & \multicolumn{1}{l}{162.42 (0.19)}                         & \multicolumn{1}{l}{1.81 (0.01)}                         \\
% RAG                      & Qwen2.5-72B (RAG)                          & \multicolumn{1}{c}{-}                       &                          & 30.72                         & 80.22                         & 28.63                         & 46.52                         &                          & 31.6                         & 31.4                         & \multicolumn{1}{l}{}                         & \multicolumn{1}{l}{3.09 (0.09)}                           & \multicolumn{1}{l}{26.89 (0.49)}                        \\
% RAPID                    & \cellcolor[HTML]{DAE8FC}Qwen2.5-72B (LC)   & \cellcolor[HTML]{DAE8FC}Qwen2.5-72B (RAG)   & \cellcolor[HTML]{DAE8FC} & \cellcolor[HTML]{DAE8FC}40.52 & \cellcolor[HTML]{DAE8FC}85.59 & \cellcolor[HTML]{DAE8FC}32.94 & \cellcolor[HTML]{DAE8FC}53.02 & \cellcolor[HTML]{DAE8FC} & \cellcolor[HTML]{DAE8FC}42.9 & \cellcolor[HTML]{DAE8FC}44.1 & \multicolumn{1}{l}{\cellcolor[HTML]{DAE8FC}} & \multicolumn{1}{l}{\cellcolor[HTML]{DAE8FC}164.80 (0.47)} & \multicolumn{1}{l}{\cellcolor[HTML]{DAE8FC}3.58 (0.39)} \\ \hline
% \end{tabular}
% }
% \end{table*}









