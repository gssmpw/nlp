\section{Related Work}



% an ubiquitous method being used to accelerate the decoding stage of LLM inference. 
% It leverages the fact that the speed of calculating the logits of a group of generated tokens in parallel is comparable to generating one token. It uses a draft model of smaller scale, comparing to the main model, to generate a number of tokens for the main model to verify in parallel. If a token is rejected by the main model, it will then be corrected by sampling from a corrected distribution. Both TriForce~\citep{sun2024triforce} and MagicDec~\citep{chen2024magicdec} proposed that an accelerated version of the main model, which utilized the StreamingLLM framework~\citep{xiao2023streamingllm} and SnapKV~\citep{Li2024SnapKVLK}, can be used as the draft model, with minimal loss of performance and improvement in speed. Furthermore, designing mechanisms that allow the main model to skip intermediate layers during inference, can also be an alternative form of draft model~\citep{zhang-etal-2024-draft, Elhoushi_2024}.


% \paragraph{Speculative Decoding } 
% Speculative Decoding~\citep{chen2023acceleratinglargelanguagemodel, leviathan2023fastinferencetransformersspeculative} is designed to accelerate larger LLMs by smaller draft LLMs, xxx. To address the challenge of SD for long-context inference, TriForce~\citep{sun2024triforce} introduces hierarchical speculation upon compressed KV cache by retrieval and StreamingLLM~\citep{xiao2023streamingllm}. MagicDec~\citep{chen2024magicdec} similarly uses StreamingLLM~\citep{xiao2023streamingllm} to compress KV cache of draft LLMs, which demonstrates speedups on long-context inference of large batch. However, the compress KV cache using mechanism such as StreamingLLM may produce a weak drafter, resulting in low acceptance rate and speedup in complex real-world application. Our RAPID adopts the RAG drafter, has proven effective in many applications with superior efficiency, provide more high-quality speculation and speedup.




% \paragraph{Long-Context Inference Speedup}
% To overcome the efficiency bottleneck of long-context inference, various approaches have been proposed. One line of work focuses on optimizing KV cache operations through selective retention~\citep{xiao2023streamingllm,Kang2024GEARAE,Zhang2023H2OHO,Li2024SnapKVLK,Xiao2024InfLLMTL} or quantization~\citep{Sheng2023HighthroughputGI, Liu2024KIVIAT,he2024zipcache,Yang2024NoTL}. Another direction explores prompt compression techniques~\citep{Chevalier2023AdaptingLM,Jiang2023LLMLinguaCP,Pan2024LLMLingua2DD} to directly reduce input context length. However, these methods achieve efficiency gains by compressing contextual information without explicit quality guarantees, potentially leading to performance degradation in real-word applications~\citep{Zhang2024MoreTL}. In contrast, our~\ourMethod{} leverages SD to achieve acceleration while maintaining generation quality through explicit verification from long-context LLMs, providing a more reliable balance between efficiency and performance.


% \paragraph{RAG and Long-Context LLMs}
% The relative merits of long-context LLMs versus RAG have been extensively studied in recent literature. \citet{Li2024RetrievalAG} demonstrated that while long-context models generally outperform RAG, they share over 60\% prediction overlap, suggesting complementary strengths. \citet{Li2024LongCV} revealed that long-context LLMs excel particularly in document-based question-answering, while RAG demonstrates superior performance in tasks like dialogue-based question-answering. Recent efforts to integrate these complementary benefits have emerged: \citet{Li2024RetrievalAG} proposed using self-reflection to route between long-context and RAG paths, while \citet{Yue2024InferenceSF} introduced a step-by-step RAG-enhanced inference scaling paradigm. However, these approaches typically rely heavily on task-specific prompt engineering and fixed inference strategies, making them less flexible across different scenarios. In contrast, our \ourMethod{} provides a more principled approach by directly integrating RAG benefits into the decoding process, enabling dynamic adaptation while maintaining the advantages of both paradigms.





\paragraph{Speculative Decoding}
Speculative Decoding~\citep{chen2023acceleratinglargelanguagemodel, leviathan2023fastinferencetransformersspeculative} accelerates LLM inference by leveraging smaller draft models to propose multiple tokens for single-pass validation. Recent works like TriForce~\citep{sun2024triforce} and MagicDec~\citep{chen2024magicdec} attempt to extend SD to long-context scenarios through KV cache compression techniques~\citep{xiao2023streamingllm}. However, such compression approaches often result in weakened draft models with limited speedup in complex applications. In contrast, RAPID adopts RAG drafters that maintain both high-quality speculation and substantial speedup in various applications.

\paragraph{Long-Context Inference Speedup}
Research on accelerating long-context inference has primarily focused on two directions: optimizing KV cache operations through selective retention~\citep{xiao2023streamingllm,Kang2024GEARAE,Zhang2023H2OHO} or quantization~\citep{Sheng2023HighthroughputGI, Liu2024KIVIAT,he2024zipcache}, and exploring prompt compression methods~\citep{Chevalier2023AdaptingLM,Jiang2023LLMLinguaCP,Pan2024LLMLingua2DD}. While these approaches improve efficiency, they often compromise contextual information without quality guarantees~\citep{Zhang2024MoreTL}. RAPID addresses this limitation by leveraging SD to maintain generation quality through explicit verification from long-context LLMs, providing a more reliable balance between efficiency and performance.

\paragraph{RAG and Long-Context LLMs}
Recent studies have revealed complementary strengths between RAG and long-context LLMs, with substantial prediction overlap despite different performance characteristics~\citep{Li2024RetrievalAG,Li2024LongCV}. While long-context LLMs excel in document-based tasks, RAG shows advantages in scenarios like dialogue-based question-answering. Previous attempts to combine these approaches, such as self-reflection routing~\citep{Li2024RetrievalAG} and step-by-step RAG enhancement~\citep{Yue2024InferenceSF}, rely heavily on task-specific prompt engineering. RAPID provides a more principled solution by directly integrating RAG benefits into the decoding process, enabling dynamic adaptation while preserving advantages of both paradigms.
