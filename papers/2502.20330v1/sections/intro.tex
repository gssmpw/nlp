\section{Introduction}




% While this breakthrough has made direct long-context inference a tempting paradigm in place of RAG, significant challenges persist. In particular, long-context LLMs still struggle with precise information retrieval from broad contexts, particularly when identifying complex, specific details. Moreover, inference over long context suffers substantial efficiency bottlenecks due to memory-intensive key-value (KV) cache operations, rendering long-context generation throughput with smaller LLMs markedly slower than short-context inference even with larger counterparts.




% Recent advances in large language models (LLMs) have demonstrated unprecedented capabilities in processing extensive documents spanning millions of tokens, seemingly obviating the need for traditional retrieval-augmented generation (RAG) pipelines. While this breakthrough has made direct long-context processing an increasingly attractive alternative to RAG, there are significant challenges persist. In particular, long-context LLMs still struggle with precise information retrieval from broad contexts, particularly when identifying complex, specific details. Moreover, inference over long context suffers substantial efficiency bottlenecks due to memory-intensive key-value (KV) cache operations, rendering long-context generation throughput with smaller LLMs markedly slower than short-context inference even with larger counterparts.



% by two primary challenges. On the one hand, the capability of LLMs to effectively utilize context deteriorates with irrelevant information inclusion, potentially compromising applications that require precise information extraction and reasoning over diverse and broader corpus. On the other hand, long-context inference involves significant efficiency bottlenecks due to memory-bound key-value (KV) cache operations, resulting in slower generation throughput compared to shorter-context inference with even larger LLMs.
% This limitation potentially impacts long-context applications requiring precise information extraction and reasoning over broader documents. 
% On the other hand, inference over long context suffers substantial efficiency bottlenecks due to memory-bound key-value (KV) cache operations, rendering long-context generation throughput with smaller LLMs even slower than short-context inference with larger counterparts.

% Recent advances in large language models (LLMs) have emerged a debate between long-context LLMs and retrieval augmented generation (RAG). On the one hand, long-context LLMs have demonstrated unprecedented capabilities in processing extensive documents spanning millions of tokens, potentially obsoleting the complex RAG pipelines which limits the performance. On the other hand, RAG demonstrates superior efficiency over long-context LLMs


% Recently, long-context LLMs have emerged to directly process million-word documents, making the obsoleting of complex RAG pipelines become never more tempting. However, this breakthrough is hindered by the inference efficiency of long-context generation, where the inference over long key-value (KV) cache becomes strictly memory-bound and substantially impact inference latency.







% While previous research has investigated techniques such as KV cache compression to reduce the computational overhead associated with long KV cache, these approaches potentially compromise generation quality with efficiency.
% Speculative Decoding (SD), which utilizes a smaller (draft) LLM to generate multiple candidates for validation by a larger (target) LLM in a single forward pass,  has proven effective in reducing latency without quality degradation. The speedup of SD hinges on two critical factors: the computational efficiency of the draft model in generating candidates, and its capability to produce high-quality candidates that achieve higher acceptance rates. However, in long-context scenarios, the effectiveness of SD diminishes as memory-bound KV cache operations prevent smaller LLMs from maintaining significant speed benefits over larger models. \gcc{Evidences.}

Large language models (LLMs) have traditionally relied on retrieval-augmented generation (RAG) to process extensive documents by selectively retrieving relevant text segments.  While effective, the performance of RAG is inherently bounded by the capability of the retriever to extract pertinent information across diverse queries~\citep{Gao2023RetrievalAugmentedGF}. The recent emergence of long-context LLMs, capable of directly processing million-word documents~\citep{team2024gemini}, suggests a promising alternative to complex RAG pipelines. However, this breakthrough is bottlenecked by the computational efficiency of long-context inference, where processing extensive key-value (KV) caches becomes memory-bound and introduces substantial latency~\citep{Pope2022EfficientlyST}.


Speculative Decoding (SD)~\citep{chen2023acceleratinglargelanguagemodel, leviathan2023fastinferencetransformersspeculative} is a prevalent approach to accelerate LLM inference without compromising generation quality. By leveraging a smaller draft model to propose multiple candidates for single-pass validation by the target model, SD achieves significant speedup when candidates are accepted. The benefits of SD hinge on two critical factors: the computational efficiency of the draft model in generating candidates, as well as its capability to produce high-quality and acceptable candidates. However, SD will become less effective in long-context scenarios, as memory-bound KV cache operations prevent smaller LLMs from maintaining significant speed benefits over larger models~\citep{Pope2022EfficientlyST, ainslie2023gqa}. As depicted in~\Cref{fig:comparion_8b_70b}, the throughput gains of LLaMA-3.1-8B over LLaMA-3.1-70B diminish drastically (23.6 $\rightarrow$ 9.4) with increasing context lengths from 1K to 128K tokens.



\begin{figure}[!t]
    \centering
    % \includegraphics[width=0.5\linewidth]{}
    % \caption{A comparison between SFT, DPO, and LongPO.}
    \includegraphics[width=\linewidth]{figures/llama_comparison.pdf}
    \vspace{-1.6em}
    \caption{Performance (accuracy, left axis) and throughput (tokens/sec, right axis) of LLaMA-3.1-8B (serving on 1$\times$A800) and LLaMA-3.1-70B (serving on 8$\times$A800) on LongBench v2 (Long) across different retrieval context lengths.}
    \label{fig:comparion_8b_70b}
\end{figure}


In this work, we introduce \textbf{R}etrieval-\textbf{A}ugmented S\textbf{P}eculat\textbf{I}ve \textbf{D}ecoding (\textbf{\ourMethod}), to bridge the gap of SD for accelerating long-context inference while enhancing generation quality. \ourMethod{} employs a \textit{RAG drafter}—the draft LLM operating on shortened context from RAG—to speculate the generation of long-context LLM following the SD process. We propose that RAG drafter can serve as \textit{ideal draft model} for long-context target LLM, as it demonstrates the potential to approach the capabilities of long-context LLM~\citep{Li2024RetrievalAG} while offering superior computational efficiency. As illustrated in~\Cref{fig:comparion_8b_70b}, LLaMA-3.1-8B with RAG on 4K$\sim$16K tokens can recover most performance achieved with full 128K tokens. This indicates that the RAG drafter is capable of producing high-quality candidates for long-context target LLM with high acceptance rate, while eliminating the memory-bound KV cache operations over long-context  to accelerate the inference process.


In addition, our \ourMethod{} opens a new paradigm for SD that \textit{leveraging the same-scale or even larger LLMs as the RAG drafters to accelerate smaller target LLMs.} This paradigm shift is possible since RAG drafters, operating on shortened contexts (e.g., 4K), potentially maintain higher efficiency than target LLMs of the same or even larger scale on long contexts (e.g., 128K) as evidenced in~\Cref{fig:comparion_8b_70b}.
Therefore, our \ourMethod{} operates on two settings: (1) \textit{self-speculation}, where long-context target LLM and RAG drafter are of the same scale; and (2) \textit{upward-speculation}, where RAG drafter involves larger parameter scale than target LLM. Moreover, in the both settings, the generation quality of RAG drafter may surpass that of long-context target models in some scenarios~\citep{Li2024LongCV}. However, 
the native SD, utilizing target LLM prediction as ground-truth distribution to perform rejection sampling, may neglect the candidates of high quality from the stronger RAG drafter. This would result in unnecessary rejection of valid candidates, thereby impeding both efficiency and performance gains.

To address this limitation, \ourMethod{} implements a retrieval-augmented target distribution, which incorporates the native long-context target distribution in SD with an \textit{inference-time knowledge transfer}. Specifically, we reversely position the RAG drafter as teacher and long-context target LLM as the student, to derive a distilled logits shift towards the RAG drafter during inference. By incorporating the shift into the prediction logits of target LLM, we obtain an enriched target distribution that is more receptive to high-quality speculative candidates.



% The transfer is derived by leveraging the gradient of knowledge distillation loss regarding target LLM logits, where we reversely position the RAG drafter as teacher and the long-context target LLM as the student. Therefore, the dynamic indicates the transfer of prior knowledge from RAG drafter to the target LLM. By incorporating this dynamic into the the prediction logits of target LLM, we obtain an enriched target distribution that is more receptive to high-quality speculative candidates.

% even when they surpass the original prediction scope of the target LLM.

Our \ourMethod{} can serve as a drop-in decoding method during long-context inference. We conduct experiments on LLaMA-3.1 (8B, 70B)~\citep{dubey2024llama3herdmodels} and Qwen2.5 (7B, 72B)~\citep{yang2024qwen2} series on $\infty$Bench~\citep{zhang-etal-2024-bench} and LongBench v2~\citep{Bai2024LongBenchVT}. The experimental results demonstrate that \ourMethod{} successfully integrates the complementary strengths of long-context LLMs and RAG while maintaining significant inference speedups. In self-speculation settings, RAPID achieves consistent performance improvements (e.g., 42.83 vs 39.33 on InfiniteBench for LLaMA-3.1-8B) with significant speedup (up to 2.69$\times$) over the long-context target LLMs. The upward-speculation setting further boosts performance through effective knowledge transfer from larger RAG drafters (e.g., improving LLaMA-3.1-8B from 42.83 to 49.98 on InfiniteBench), with comparable efficiency with the smaller long-context target LLMs. With moderate retrieval length ($\le$16K) for RAG drafter, we found~\ourMethod{} consistently achieves speedup when target long-context length beyond 32K. Our analyses also indicate that RAPID demonstrates robustness to retrieval quality and potentially superior generation quality in real-world multi-turn dialogue tasks. These results validate RAPID as an effective decoding method for accelerating long-context inference and, at the same time, enhancing generation quality through retrieval-augmented speculation.



% Our \ourMethod{} can serve as a drop-in decoding method during long-context inference. The experimental results demonstrate ....


% Notably, out of the shortened context length, the RAG drafter inherently holds much higher efficiency without the requirement of ensuring smaller parameter scale than target LLM. Furthermore, LLMs on short context (e.g., 4k) may pose superior efficiency over smaller LLMs on long context (e.g., 128K), as exampled in~\Cref{fig:comparion_8b_70b}. Therefore, our \ourMethod{} opens a new paradigm for SD that \textit{leveraging the same scale or even larger LLMs as the RAG drafters to accelerate smaller target LLMs.}
% \ourMethod{} addresses the memory-bound limitations of SD for long-context inference by employing the RAG-based LLM as the draft model. We propose that RAG model can serve as \textit{ideal draft model} for long-context target LLM, as it demonstrate the potential to approach the capabilities of long-context LLMs while offering superior computational efficiency. \gcc{discuss self speculation}
% In such settings, RAG drafter may outperform long-context target models in terms of generation quality. However, the native SD, utilizing target LLM prediction as ground-truth distribution to perform rejection sampling, may neglect the overwhelming candidates of high quality from stronger RAG drafter. This would result in unnecessary rejection for candidates, which both hinders the speedup and performance improvement.
% since LLMs typically involve superior context utilization ability with shorter context as long as the required information is properly included. 



% To further leverage the retrieved information of draft model upon RAG

