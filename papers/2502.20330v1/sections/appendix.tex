
\section{Proof of Theorem 1}
\label{sec:proof1}
% The distillation loss is:
% % \begin{equation}
% %     \mathcal{L} = T^{2} \cdot \text{KL}(q(x_i) || p(x_i)),
% % \end{equation}
% where $T$ is the temperature parameter. The gradient of this loss with respect to the target model's logits reveals our dynamic:



% \begin{equation}
% \begin{aligned}
%     \mathcal{L} &= \mathcal{T}^{2} \cdot \mathrm{KL}(q(x) || p(x)) \\
%     &= \mathcal{T}^{2} \sum_j q(x_j) \log\frac{q(x_j)}{p(x_j)}
% \end{aligned}
% \end{equation}

% \text{Where:}
% \begin{equation}
% p(x_j) = \frac{\exp(z_j/\mathcal{T})}{\sum_k \exp(z_k/\mathcal{T})}
% \end{equation}

% % \text{Taking the derivative with respect to } z_i\text{:}



% \begin{equation}
% \begin{aligned}
% \frac{\partial \mathcal{L}}{\partial z_i} &= \mathcal{T}^{2} \sum_j q(x_j) \frac{\partial}{\partial z_i}[\log q(x_j) - \log p(x_j)] \\
% &= \mathcal{T}^{2} \sum_j q(x_j) \left(0 - \frac{\partial}{\partial z_i} \log p(x_j)\right) \\
% &= -\mathcal{T}^{2} \sum_j q(x_j) \frac{\partial}{\partial z_i} \log p(x_j) \\
% &= -\mathcal{T}^{2} \sum_j q(x_j) \frac{\partial}{\partial z_i} \left[\frac{z_j}{\mathcal{T}} - \log\sum_k \exp(z_k/\mathcal{T})\right] \\
% &= -\mathcal{T}^{2} \sum_j q(x_j) \left[\frac{\delta_{ij}}{\mathcal{T}} - \frac{1}{\mathcal{T}}\frac{\exp(z_i/\mathcal{T})}{\sum_k \exp(z_k/\mathcal{T})}\right] \\
% &= -\mathcal{T} \sum_j q(x_j) \left[\delta_{ij} - p(x_i)\right], \delta_{ij}=1 \text{ when i=j else 0} \\
% &= -\mathcal{T} \sum_j q(x_j) \left[\delta_{ij} - p(x_i)\right] \\
% &= -\mathcal{T} \left[q(x_i) - \sum_j q(x_j)p(x_i)\right] \\
% &= -\mathcal{T} \left[q(x_i) - p(x_i)\right]
% \end{aligned}
% \end{equation}



% \subsection*{A.4. Gradient Analysis of Distillation Loss}

We analyze the gradient of the knowledge distillation loss with respect to the target model's logits. The distillation loss with temperature $\mathcal{T}$ is defined as:

\begin{equation}
\begin{aligned}
    \mathcal{L} &= \mathcal{T}^{2} \cdot \mathrm{KL}(q(x) || p(x)) \\
    &= \mathcal{T}^{2} \sum_j q(x_j) \log\frac{q(x_j)}{p(x_j)}
\end{aligned}
\end{equation}

where the target distribution $p(x)$ is parameterized by logits $z$ through softmax:
\begin{equation}
    p(x_j) = \frac{\exp(z_j/\mathcal{T})}{\sum_k \exp(z_k/\mathcal{T})}
\end{equation}

\textbf{Theorem:} The gradient of the distillation loss with respect to logit $z_i$ is:
\begin{equation}
    \frac{\partial \mathcal{L}}{\partial z_i} = -\mathcal{T}[q(x_i) - p(x_i)]
\end{equation}

\textbf{Proof:} We derive this gradient through the following steps:

1) First, expand the derivative using the chain rule:
\begin{equation}
    \frac{\partial \mathcal{L}}{\partial z_i} = \mathcal{T}^{2} \sum_j q(x_j) \frac{\partial}{\partial z_i}[\log q(x_j) - \log p(x_j)]
\end{equation}

2) Note that $q(x_j)$ is independent of $z_i$:
\begin{equation}
    = -\mathcal{T}^{2} \sum_j q(x_j) \frac{\partial}{\partial z_i} \log p(x_j)
\end{equation}

3) Expand the log probability:
\begin{equation}
    = -\mathcal{T}^{2} \sum_j q(x_j) \frac{\partial}{\partial z_i} \left[\frac{z_j}{\mathcal{T}} - \log\sum_k \exp(z_k/\mathcal{T})\right]
\end{equation}

4) Apply the derivative using the Kronecker delta $\delta_{ij}$:
\begin{equation}
    = -\mathcal{T}^{2} \sum_j q(x_j) \left[\frac{\delta_{ij}}{\mathcal{T}} - \frac{1}{\mathcal{T}}\frac{\exp(z_i/\mathcal{T})}{\sum_k \exp(z_k/\mathcal{T})}\right]
\end{equation}

5) Simplify using the definition of $p(x_i)$:
\begin{equation}
    = -\mathcal{T} \sum_j q(x_j) [\delta_{ij} - p(x_i)]
\end{equation}

6) The sum over $j$ with $\delta_{ij}$ selects only $q(x_i)$:
\begin{equation}
    = -\mathcal{T} [q(x_i) - \sum_j q(x_j)p(x_i)]
\end{equation}

7) Since $\sum_j q(x_j) = 1$, we obtain our final result:
\begin{equation}
    = -\mathcal{T}[q(x_i) - p(x_i)]
\end{equation}

This gradient shows that the distillation loss pushes the target distribution $p(x)$ towards the draft distribution $q(x)$ with strength proportional to the temperature $\mathcal{T}$. \qed




\section{Correctness of RAPID's Residual Distribution}
\label{sec:proof2}

% \subsection*{A.3. Correctness of RAPID's Residual Distribution}

We prove that for RAPID's retrieval-augmented speculative decoding, when rejection occurs, sampling from the distribution
\begin{equation}
    x_i \sim \texttt{norm}(\max(p(x_i)-\hat{p}(x_i), p(x_i)-q(x_i)))
\end{equation}
maintains the target distribution $p(x_i)$, where:
\begin{equation}
    p(x_i) = p_\mphi(x_i \vert [\mathcal{C};x_{<i}]) \text{ (target distribution)}
\end{equation}
\begin{equation}
    q(x_i) = q_\mpsi(x_i \vert [\mathcal{C^{\text{S}}};x_{<i}]) \text{ (RAG drafter distribution)}
\end{equation}
\begin{equation}
    \hat{p}(x_i) = \softmax(\hat{z}(x_i)/T) \text{ (retrieval-augmented target)}
\end{equation}

\textbf{Proof:}
Let $x^\prime$ be a candidate token. Under RAPID's rejection sampling scheme:

1) For a token $x^\prime$ proposed by the draft model, the acceptance criterion is:
\begin{equation}
    r \leq \min(1, \frac{\hat{p}(x^\prime)}{q(x^\prime)})
\end{equation}
where $r \sim U(0,1)$

2) This leads to an acceptance probability:
\begin{equation}
    P(\text{accept}|x^\prime) = \min(q(x^\prime), \hat{p}(x^\prime))
\end{equation}

3) The residual probability mass that needs to be redistributed upon rejection is:
\begin{equation}
    p(x^\prime) - \min(q(x^\prime), \hat{p}(x^\prime)) = \max(p(x^\prime)-q(x^\prime), p(x^\prime) - \hat{p}(x^\prime))
\end{equation}

4) Let $\beta$ be the total acceptance probability:
\begin{equation}
    \beta = \sum_{x^\prime} \min(q(x^\prime), \hat{p}(x^\prime))
\end{equation}

5) Therefore, upon rejection, we must sample from:
\begin{equation}
    p^\prime(x^\prime) = \frac{p(x^\prime) - \min(q(x^\prime), \hat{p}(x^\prime))}{\sum_{x^\prime}(p(x^\prime) - \min(q(x^\prime), \hat{p}(x^\prime)))} = \frac{p(x^\prime) - \min(q(x^\prime), \hat{p}(x^\prime))}{1-\beta}
\end{equation}

This residual distribution ensures that for any token $x^\prime$:
\begin{equation}
    P(x = x^\prime) = \min(q(x^\prime), \hat{p}(x^\prime)) + (1-\beta)\frac{p(x^\prime) - \min(q(x^\prime), \hat{p}(x^\prime))}{1-\beta} = p(x^\prime)
\end{equation}



% The $\max(\cdot, 0)$ operation ensures non-negative probabilities in the residual distribution, as negative differences would indicate tokens that are overestimated by either the draft model or the retrieval-augmented target, and thus should have zero probability in the residual distribution. \qed







\section{Evaluation Setup}
\label{sec:extra_setup}
We conduct comprehensive evaluations across different model scales and configurations. We use temperature values of 1.0 and 0.1 for $\infty$Bench and LongBench v2, respectively. For base-scale models (LLaMA-3.1-8B and Qwen2.5-7B), we evaluate RAPID's self-speculation capabilities against multiple baselines including naive Speculative Decoding, MagicDec, Long Context (LC), and RAG implementations, using a single NVIDIA A800 80GB GPU. 

For large-scale models (LLaMA-3.1-70B and Qwen2.5-72B), self-speculation experiments are conducted using a distributed setup with 8$\times$A800 80GB GPUs. In upward-speculation settings, we employ a hybrid configuration where the target models (LLaMA-3.1-8B/Qwen2.5-7B) operate on a single A800 80GB GPU, while leveraging an additional 7$\times$A800 80GB GPUs to accommodate the larger RAG drafter.

% % Optional: Add infrastructure details if relevant
% All experiments are performed using [framework details] with [any relevant system configurations].








% \section{Effects of Retrieval Length}
% % \paragraph{Efficiency-Performance Trade-offs with Retrieval Length.}
% We now analyze how varying retrieval lengths (4k to 128k tokens) affect RAPID's effectiveness on LongBench V2 (Long) with 128k target context length, comparing against naive SD. As shown in~\Cref{fig:rag-length}, RAPID demonstrates several key advantages over naive SD. First, RAPID achieves consistently higher accuracy gains (6-8\%) and speedup ratios (1.4-2.0$\times$) across all retrieval lengths, while naive SD's performance deteriorates significantly with longer contexts. Most notably, RAPID with just 4k retrieval length ($\sim$80\% acceptance rate) matches the acceptance rate that naive SD achieves only when retrieving nearly the full context (128k), highlighting our method's superior efficiency in context utilization.

% The results also reveal important trade-offs as retrieval length increases. While longer retrieval lengths generally lead to higher acceptance rates (from 80\% at 4k to 92\% at 128k for RAPID), they come with diminishing returns in computational efficiency - the speedup ratio gradually decreases from 2.0$\times$ to 1.4$\times$ due to increased overhead in draft model processing. Moreover, we observe that extremely long retrieval lengths can potentially introduce performance degradation due to the inclusion of more hard negative retrieved chunks, though RAPID maintains positive accuracy gains throughout. This suggests an optimal operating point around 32k-64k retrieval length, where RAPID achieves a balanced trade-off between acceptance rate ($\sim$88\%), speedup (1.6$\times$), and accuracy gains (6\%).


% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{figures/rag_length.pdf}
%     \caption{Impact of retrieval length on RAPID and naive SD performance with 128k target context length.}
%     \label{fig:rag-length}
% \end{figure}
