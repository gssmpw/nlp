% \section{Method}




% \subsection{Preliminary: Speculative Decoding}
% Autoregressive generation with a LLM  $p_\mphi$ traditionally requires $n$ sequential forward passes to generate $n$ tokens, where each token $x_i$ is sampled from the distribution $p_\mphi(\cdot | x_{<i})$ for $i\in[1,n]$. This sequential process incurs substantial computational overhead due to model weight loading and KV cache updates in GPU SRAM.

% Speculative Decoding (SD) accelerates this process by employing a smaller, faster assistant model $q_\mpsi$ to generate $\gamma$ candidate tokens. These speculative tokens are then validated by the target model $p_\mphi$ in a single forward pass through rejection sampling, where $p_\mphi(\cdot | x_{<i})$ serves as the target distribution and $q_\mpsi(\cdot | x_{<i})$ as the proposal distribution.

% For each speculative token $x^{\prime}_i \sim q_\mpsi(\cdot | x_{<i})$, the acceptance criterion is:
% \begin{equation}
%     r \leq \min\left(1, \frac{p_\mphi(x^{\prime}_i| x_{<i})}{q_\mpsi(x^{\prime}_i| x_{<i})}\right),
% \end{equation}
% where $r \sim U(0,1)$. Upon acceptance, $x^{\prime}_i$ is incorporated into the final sequence as $x_i$. Otherwise, a new token is resampled according to:
% \begin{equation}
%     x_i \sim norm({\max(p_\mphi(\cdot|x_{<i}) - q_\mpsi(\cdot|x_{<i}), 0)}).
% \end{equation}
% % This procedure preserves the original target distribution while potentially achieving significant speedup with accepted draft predictions.
% This procedure guarantees that the resampled tokens follow the exact  distribution as direct sampling from the target model $p_\mphi$, while potentially achieving significant speedup when the speculative tokens are accepted.






% \subsection{Long Context Speculative Decoding with RAG Drafter}
% The speedup benefits of SD become marginal in long-context scenarios, even with smaller draft models, due to memory-bound KV cache operations. Formally, the target distribution over long context $\mathcal{C}$ and prefix $x_{<i}$ is $p_\mphi(\cdot \vert \left[\mathcal{C};x_{<i}\right])$,
% % \begin{equation}
% %     p(x) = p_\mphi(\cdot \vert \left[\mathcal{C};x_{<i}\right]),
% % \end{equation}
% where $\mphi$ is the target model parameters.

% Given that the KV cache for $\mathcal{C}$ occupies a significant amount of memory, draft models with fewer parameters than $\mphi$ might not surpass the target model in terms of speed.~\gcc{Give some evidence} To circumvent the bottleneck of long KV cache, our \ourMethod{} steers a draft model over compressed context $\mathcal{C^{\text{S}}}$, which is derived by applying RAG pipelines to retrieve relevant chunks in $\mathcal{C}$ regarding the query. Therefore, the draft distribution for model $\mpsi$ is $q_\mpsi(\cdot \vert \left[\mathcal{C^{\text{S}}};x_{<i}\right]).$
% % $q(x) = q_\mpsi(\cdot \vert \mathcal{C^{\text{S}}})$.
% % \begin{equation}
% %     q(x) = q_\mpsi(\cdot \vert \left[\mathcal{C^{\text{S}}};x_{<i}\right]).
% % \end{equation}
% We maintain the draft context length $\vert \mathcal{C^{\text{S}}} \vert \!=\! \vert \mathcal{C} \vert/\lambda$ with $\lambda \gg 1$ through controlled chunk retrieval.

% % by limiting the number of retrieved chunks, where $\lambda \gg 1$.

% Utilizing the RAG-based draft distribution, we can draw samples $x^{\prime}_i \sim q_\mpsi(\cdot \vert \left[\mathcal{C^{\text{S}}};x_{<i}\right])$, subsequently verified by the target model through rejection sampling following SD.
% \begin{equation}
%     r \leq \min\left(1, \frac{p_\mphi(x^{\prime}_i| \left[\mathcal{C};x_{<i}\right])}{q_\mpsi(x^{\prime}_i| \left[\mathcal{C^{\text{S}}};x_{<i}\right])}\right).
% \end{equation}









% \subsection{RAG-Augmented Target Distribution}
% % \jinjie{let's call this something like "speculative distillation"}
% The capability of LLMs to effectively utilize context may deteriorate with irrelevant information inclusion. Therefore, RAG potentially outperforms long-context LLMs in terms of generation quality. However, the speculative tokens in naive SD are accepted only when the draft distribution strictly aligns with target distribution, making the out-of-target-distribution tokens of higher quality from RAG be rejected. This not only impedes the utilization of benefits from RAG, but also resulting in unnecessary rejection and reducing the efficiency gains. To address this issue, \ourMethod{} utilizes RAG-Augmented Target Distribution, which seek to inject the xxx from RAG into the long-context LLMs during SD.

% Given a speculative token $x^{\prime}_i \sim q_\mpsi(\cdot \vert \left[\mathcal{C^{\text{S}}};x_{<i}\right])$, we define the RAG-Augmented Target Distribution as
% \begin{equation}
%     \hat{p} = \softmax\left(z_{\mphi}(x^{\prime}_i \vert \left[\mathcal{C};x_{<i}\right]) + m \cdot \left(q(x^{\prime}_i) - p(x^{\prime}_i)\right)\right),
% \end{equation}
% where
% \begin{equation}
%     p(x^{\prime}_i) = p_\mphi(x^{\prime}_i \vert \left[\mathcal{C};x_{<i}\right]) = \softmax\left(z_{\mphi}(x^{\prime}_i \vert \left[\mathcal{C};x_{<i}\right])\right),
% \end{equation}
% \begin{equation}
%     q(x^{\prime}_i) = q_\mpsi(x^{\prime}_i \vert \left[\mathcal{C};x_{<i}\right]).
% \end{equation}



% \paragraph{Theorem 1.} For a teacher model distribution $\mathcal{T}$ and a student model distribution $\mathcal{S}$, the gradient of knowledge distillation loss regarding student probability is $T \cdot (\mathcal{S} - \mathcal{T})$.


% We reversely let draft model $q(x^{\prime}_i)$ as the teacher model and target model as the student model $p(x^{\prime}_i)$, the distillation loss would be
% \begin{equation}
%     \mathcal{L} = T^{2} \cdot \mathrm{KL}(q(x^{\prime}_i) || p(x^{\prime}_i)).
% \end{equation}
% According to Theorem 1, the gradient of distillation loss regarding $z_i$ is
% \begin{equation}
%     \frac{\partial \mathcal{L}}{\partial z_i} = -T \cdot (q(x^{\prime}_i) - p(x^{\prime}_i)).
% \end{equation}
% Therefore, we can access an adjusted logit
% \begin{equation}
% \begin{aligned}
%     \hat{z_i} &= z_i - \eta \frac{\partial \mathcal{L}}{\partial z_i}  \\
%             &= z_i + \eta \mathcal{T} \left(q(x^{\prime}_i) - p(x^{\prime}_i)\right).
% \end{aligned}
% \end{equation}
% The dynamic is derived from the gradient of knowledge distillation loss regarding target LLM logits,
% where we reversely position the RAG-based draft model as teacher and the long-context target model as the student. Therefore,  a proportion of prior knowledge would be implicitly transferred from RAG to the target model during inference time, hence improving long-context generation quality. 



% \begin{equation}
% \begin{aligned}
%     \mathcal{L} &= \mathcal{T}^{2} \cdot \mathrm{KL}(q(x^{\prime}_i) || p(x^{\prime}_i)) \\
%     &= \mathcal{T}^{2} \sum_j q(x^{\prime}_j) \log\frac{q(x^{\prime}_j)}{p(x^{\prime}_j)}
% \end{aligned}
% \end{equation}

% \text{Where:}
% \begin{equation}
% p(x^{\prime}_j) = \frac{\exp(z_j/\mathcal{T})}{\sum_k \exp(z_k/\mathcal{T})}
% \end{equation}

% % \text{Taking the derivative with respect to } z_i\text{:}
% \begin{equation}
% \begin{aligned}
% \frac{\partial \mathcal{L}}{\partial z_i} &= \mathcal{T}^{2} \sum_j q(x^{\prime}_j) \frac{\partial}{\partial z_i}[\log q(x^{\prime}_j) - \log p(x^{\prime}_j)] \\
% &= \mathcal{T}^{2} \sum_j q(x^{\prime}_j) \left(0 - \frac{\partial}{\partial z_i} \log p(x^{\prime}_j)\right) \\
% &= -\mathcal{T}^{2} \sum_j q(x^{\prime}_j) \frac{\partial}{\partial z_i} \log p(x^{\prime}_j) \\
% &= -\mathcal{T}^{2} \sum_j q(x^{\prime}_j) \frac{\partial}{\partial z_i} \left[\frac{z_j}{\mathcal{T}} - \log\sum_k \exp(z_k/\mathcal{T})\right] \\
% &= -\mathcal{T}^{2} \sum_j q(x^{\prime}_j) \left[\frac{\delta_{ij}}{\mathcal{T}} - \frac{1}{\mathcal{T}}\frac{\exp(z_i/\mathcal{T})}{\sum_k \exp(z_k/\mathcal{T})}\right] \\
% &= -\mathcal{T} \sum_j q(x^{\prime}_j) \left[\delta_{ij} - p(x^{\prime}_i)\right], \delta_{ij}=1 \text{ when i=j else 0} \\
% &= -\mathcal{T} \sum_j q(x^{\prime}_j) \left[\delta_{ij} - p(x^{\prime}_i)\right] \\
% &= -\mathcal{T} \left[q(x^{\prime}_i) - \sum_j q(x^{\prime}_j)p(x^{\prime}_i)\right] \\
% &= -\mathcal{T} \left[q(x^{\prime}_i) - p(x^{\prime}_i)\right]
% \end{aligned}
% \end{equation}











\section{RAPID: Retrieval-Augmented Speculative Decoding}




\subsection{Background: Speculative Decoding}
Autoregressive generation with a LLM $p_\mphi$ traditionally requires sequential forward passes, where each token $x_i$ is sampled from the distribution $p_\mphi(x_i | x_{<i})$. 
This sequential nature incurs substantial computational overhead for LLM parameters loading and KV cache manipulation in GPU SRAM.
SD accelerates this process using a smaller draft model $q_\mpsi$ to generate $\gamma$ candidate tokens, which are then validated by the target model $p_\mphi$ in a single forward pass through rejection sampling. For each speculative token $x^{\prime}_i \sim q_\mpsi(x_i | x_{<i})$, the acceptance criterion is:
\begin{equation}
    r \leq \min\left(1, \frac{p_\mphi(x^{\prime}_i| x_{<i})}{q_\mpsi(x^{\prime}_i| x_{<i})}\right),
\end{equation}
where $r \sim U(0,1)$. Upon rejection, a new token is sampled from the residual distribution:
\begin{equation}
    x_i \sim \texttt{norm}({\max(p_\mphi(x_i|x_{<i}) - q_\mpsi(x_i|x_{<i}), 0)}),
\end{equation}
where \texttt{norm} is  to normalize the distribution by $\ell_1$ norm.

This procedure guarantees that the resampled tokens follow the exact distribution as direct sampling from the target model $p_\mphi$, while potentially achieving significant speedup when the speculative tokens are accepted. 
% However, in long-context scenarios where $x_{<i}$ includes extensive context $\mathcal{C}$, the efficiency gains become limited as both models must process and maintain the complete context in memory.


% RAPID addresses two critical challenges. First, traditional SD becomes inefficient with long contexts as both draft and target models must process complete context in memory, negating the computational advantages of smaller drafter. Second, the strict acceptance criterion in SD may reject high-quality candidates from RAG-based drafts that could potentially surpass the target model's capabilities.

\subsection{Overview}
\label{subsec:overview}
% Long-context LLMs face a fundamental trade-off between context utilization and computational efficiency. 
While traditional SD offers significant speedups for standard-length contexts, its benefits diminish substantially when handling extensive documents due to memory-bound KV cache operations. We present RAPID, a method that reimagines SD for long-context scenarios while enhancing generation quality. As demonstrated in~\Cref{alg:rapid}, RAPID comprises two critical components:

\paragraph{RAG Drafter.} SD becomes inefficient with long contexts as both draft and target LLMs must process complete context in memory, negating the computational advantages of smaller drafter. To overcome this challenge, \ourMethod{} utilizes a \textit{RAG drafter} to generate candidates for long-context LLMs as introduced in~\Cref{subsec:drafter}. The RAG drafter operates on selectively retrieved context segments, enabling significant speedups while maintaining access to relevant information.

\paragraph{Retrieval-Augmented Target Distribution.} The strict acceptance criterion in SD may reject high-quality candidates, as it requires strict match to the target LLM distribution for acceptance. This constraint becomes particularly limiting when using RAG drafters, which can potentially generate higher-quality outputs than long-context LLMs in certain scenarios~\citep{Li2024LongCV}. To incorporate the benefits from RAG drafters, \ourMethod{} steers a retrieval-augmented target distribution (\Cref{subsec:rag_target}), which enables knowledge transfer from RAG drafter to target model during inference. This mechanism allows the target distribution to incorporate valuable information while maintaining theoretical guarantees of the original SD.


% This constraint becomes particularly limiting when using RAG drafters, which can potentially generate higher-quality outputs than long-context LLMs in certain scenarios~\citep{Li2024LongCV}. To leverage these superior capabilities, \ourMethod{} introduces a retrieval-augmented target distribution (\Cref{subsec:rag_target}). This novel mechanism enables dynamic knowledge transfer from the RAG drafter to the target model during inference, effectively expanding the target distribution to embrace high-quality candidates while preserving theoretical guarantees of the original SD framework.



\begin{algorithm}[!t]
\caption{Retrieval-Augmented Speculative Decoding}
\label{alg:rapid}
\begin{algorithmic}[1]
\REQUIRE Target LLM $p_\mphi$, RAG drafter $q_\mpsi$, context $\mathcal{C}$, retrieval context $\mathcal{C^{\text{S}}}$, number of speculative tokens $\gamma$, temperature $T$, transfer strength $\eta$
\ENSURE Generated sequence $x_{1:n}$

\STATE $i \gets 1$ 
\WHILE{$i \leq n$}
    \STATE {\color{darkgreen2} // Generate $\gamma$ speculative tokens using RAG drafter}
    \FOR{$k \gets 1$ to $\gamma$}
        \STATE $x^{\prime}_{i+k-1} \!\sim\! q(x_{i+k-1}) \!=\! q_\mpsi(\cdot | [\mathcal{C^{\text{S}}};x_{<i};x^{\prime}_{i:i+k-1}])$ 
    \ENDFOR
    
    \STATE {\color{darkgreen2} // Validate speculative tokens sequentially}
    \FOR{$k \gets 1$ to $\gamma$}
        \STATE $j \gets i + k - 1$
        \STATE {\color{darkgreen2} // Compute target and draft distributions}
        \STATE $z(x^{\prime}_j) \gets \texttt{LogitsOf}(p_\mphi(\cdot | [\mathcal{C};x_{<j}]))$
        \STATE $p(x^{\prime}_j) \gets \softmax(z(x^{\prime}_j)/T)$
        \STATE $q(x^{\prime}_j) \gets q_\mpsi(x^{\prime}_j | [\mathcal{C^{\text{S}}};x_{<j}])$
        
        \STATE {\color{darkgreen2} // Compute retrieval-augmented target  distribution}
        \STATE $\hat{z}(x^{\prime}_j) \gets z(x^{\prime}_j) \!+\! \eta T(q(x^{\prime}_j) \!-\! p(x^{\prime}_j))$ (\Cref{eq:delta})
        \STATE $\hat{p}(x^{\prime}_j) \gets \softmax(\hat{z}(x^{\prime}_j)/T)$
        \STATE $r \sim U(0,1)$
        \IF{$r \leq \min(1, \frac{\hat{p}(x^{\prime}_j)}{q(x^{\prime}_j)})$}
            \STATE $x_{j} \gets x^{\prime}_{j}$
            \STATE $i \gets j + 1$
        \ELSE 
            \STATE \textbf{goto} line 26
        \ENDIF
    \ENDFOR
    
    \STATE {\color{darkgreen2} // Sample from residual if rejected}
    \STATE $x_i \sim \texttt{norm}(\max(p(x_i)-\hat{p}(x_i), p(x_i)-q(x_i), 0))$
    \STATE $i \gets i + 1$
\ENDWHILE
\STATE \textbf{return} $x_{1:n}$
\end{algorithmic}
\end{algorithm}



\subsection{RAG Drafter}
\label{subsec:drafter}
% Traditional SD faces significant challenges in long-context scenarios. 
When processing queries for extensive context $\mathcal{C}$, the target distribution of naive SD is 
\begin{equation}
    p(x_i) = p_\mphi(x_i \vert [\mathcal{C};x_{<i}]).
\end{equation}
Even with smaller draft models, the computational benefits diminish substantially due to memory-bound KV cache operations over the complete context $\mathcal{C}$.
To overcome this limitation, we propose to leverage RAG as the foundation for our draft model. 

% Instead of processing the entire context $\mathcal{C}$, our RAG drafter operates on a compressed context $\mathcal{C^{\text{S}}}$.  Following RAG pipelines, $\mathcal{C^{\text{S}}}$ comprises text segments deriving through selective retrieval of relevant chunks to the query, where the relevance is measured by the cosine similarity between sentence embeddings. 

Instead of processing the entire context $\mathcal{C}$, our RAG drafter operates on a compressed context $\mathcal{C^{\text{S}}}$. Specifically, $\mathcal{C^{\text{S}}}$ is constructed through selective retrieval: text segments from $\mathcal{C}$ are encoded into a dense vector space, where semantic similarity to the query is measured via cosine similarity, enabling efficient identification and extraction of the most relevant context chunks.



After deriving the compress context $\mathcal{C^{\text{S}}}$, the draft distribution is formally defined as
\begin{equation}
    q(x_i) = q_\mpsi(x_i \vert [\mathcal{C^{\text{S}}};x_{<i}]),
\end{equation}
where we maintain strict control over the compression ratio by enforcing $|\mathcal{C^{\text{S}}}| \le |\mathcal{C}|/\lambda$ with $\lambda \gg 1$. This compressed context enables our draft model to maintain significant speed advantages while preserving access to relevant information.

Based on the RAG drafter, the modified speculative decoding process proceeds as follows. For each generation step, we sample $\gamma$ speculative tokens from the RAG drafter as $x^{\prime}_i \narrowsim q(x_i)$.
% \begin{equation}
%     x^{\prime}_i \sim q_\mpsi(\cdot \vert [\mathcal{C^{\text{S}}};x_{<i}])
% \end{equation}
These candidates are validated against the target model using a modified acceptance criterion:
\begin{equation}
\label{eq:reject_sampling}
    r \leq \min\left(1, \frac{p(x_i)}{q(x_i)}\right) \!=\!  \min\left(1, \frac{p_\mphi(x^{\prime}_i| [\mathcal{C};x_{<i}])}{q_\mpsi(x^{\prime}_i| [\mathcal{C^{\text{S}}};x_{<i}])}\right)
\end{equation}
where $r \sim U(0,1)$. 

The RAG-based drafting mechanism offers two key advantages: (1) significant reduction in memory overhead and computational cost through compressed context operations ($|\mathcal{C^{\text{S}}}| \ll |\mathcal{C}|$), and (2) potentially enhanced speculation quality through selective retrieval of relevant information compared to processing diluted full context. Moreover, due to the remarkable efficiency on shorten context, \ourMethod{} even enables the use of same-scale or larger models as drafters to accelerate smaller target LLMs.


% This RAG-based drafting mechanism offers two key advantages. First, by operating on compressed context $\mathcal{C^{\text{S}}}$ where $|\mathcal{C^{\text{S}}}| \ll |\mathcal{C}|$, it significantly reduces memory overhead and computational cost compared to naive SD. Second, the selective retrieval ensures the drafter maintains access to the most relevant information, potentially enabling more accurate speculation than traditional drafters processing diluted full context. This design allows RAPID to maintain theoretical guarantees while making speculative decoding practical for long-context scenarios. Moreover, the reduced memory footprint opens up the possibility of using same-scale or even larger models as drafters while preserving computational efficiency.


% This design enables RAPID to maintain the theoretical guarantees of speculative decoding while significantly improving its practicality for long-context scenarios. Furthermore, by operating on compressed contexts, RAPID opens up the possibility of using same-scale or even larger models as drafters while maintaining computational efficiency.








\subsection{Retrieval-Augmented Target Distribution}
\label{subsec:rag_target}
The capability of LLMs to effectively utilize context often deteriorates with irrelevant information inclusion. Our empirical analysis in~\Cref{fig:comparion_8b_70b} shows that LLMs, by focusing on retrieved relevant chunks, can sometimes surpass full-context utilization in generation quality. However, the strict acceptance criterion of traditional SD may potentially result in unnecessary rejection for these superior generations when they deviate from the target distribution, leading to both quality degradation and computational inefficiency. 


To address this limitation, we introduce retrieval-augmented target distribution, which enables knowledge transfer from the RAG drafter to the long-context target model during inference. Formally, the retrieval-augmented target distribution in~\ourMethod{} is defined as:
\begin{equation}
\label{eq:new_target}
    \hat{p}(x_i) = \softmax(z(x_i) / T + \eta \cdot (q(x_i) - p(x_i))),
\end{equation}
where $\eta$ is a hyperparameter controlling the strength of knowledge transfer, $z(x_i)$ is the unnormalized logits of target LLM, namely $p(x_i) = \softmax \left(z(x_i)/T\right)$ and $T$ is the temperature.


% \begin{align}
%     p(x^{\prime}_i) &= p_\mphi(x^{\prime}_i | [\mathcal{C};x_{<i}]) = \text{softmax}(z_{\mphi}(x^{\prime}_i | [\mathcal{C};x_{<i}])), \\
%     q(x^{\prime}_i) &= q_\mpsi(x^{\prime}_i | [\mathcal{C^{\text{S}}};x_{<i}]).
% \end{align}

\vspace{1em}
% \textbf{Theorem 1.} \textit{For a student model distribution $p(x) \!=\! \softmax \left(z(x)/T\right)$ and the teacher model distribution $q(x)$, the gradient of knowledge distillation loss~\citep{Hinton2015DistillingTK} regarding $z(x)$ is: $T \cdot (p-q)$.} (See proof in~\Cref{sec:proof1}.)
\begin{proposition}
\label{theorem:distill}
Let $p(x) = \softmax(z(x)/T)$ be a student model distribution parameterized by logits $z(x)$ and temperature $T$, and $q(x)$ be a teacher model distribution. The gradient of the knowledge distillation loss $\mathcal{L} = T^2 \cdot \text{KL}(q(x) \| p(x))$ with respect to $z(x)$ is:
\begin{equation*}
    \frac{\partial \mathcal{L}}{\partial z(x)} = T \cdot (p(x) - q(x))
\end{equation*}
where $\text{KL}(\cdot \| \cdot)$ denotes the Kullback-Leibler divergence.
\end{proposition}
\begin{proof}
See~\Cref{sec:proof1}.
\end{proof}
\vspace{0.5em}




The design of retrieval-augmented target distribution in~\Cref{eq:new_target} implies a knowledge distillation step by positioning the RAG drafter as the teacher and the target model as the student, to infuse
a proportion of knowledge from RAG drafter into naive long-context target distribution.
% the naive long-context target distribution with knowledge from RAG drafter.

Specifically, for a distillation loss~\citep{Hinton2015DistillingTK} $\mathcal{L}$ between RAG draft distribution $q(x_i)$ (teacher) and long-context target distribution $p(x_i)$ (student), according to~\Cref{theorem:distill}, we have the distilled logits shift as
\begin{equation}
    \frac{\partial \mathcal{L}}{\partial z(x_i)} = T \cdot (p(x_i) - q(x_i)).
\end{equation}
% This formulation is theoretically grounded in knowledge distillation principles. Specifically, we reverse the traditional teacher-student roles, positioning the RAG drafter as the teacher and the target model as the student.

Now we can derive a ``distilled'' $z(x_i)$ augmented by RAG drafter through 
\begin{equation}
\label{eq:delta}
\begin{aligned}
    \hat{z}(x_i) &= z(x_i) - \eta \frac{\partial \mathcal{L}}{\partial z(x_i)} \\
              &= z(x_i) + \eta T(q(x_i) - p(x_i)),
\end{aligned}
\end{equation}
where $\eta$ controls the strength of knowledge transfer. Therefore, the retrieval-augmented target distribution in~\Cref{eq:new_target} is equivalent to the normalized $\hat{z}(x_i)$, i.e., $\hat{p}(x_i)\!=\!\softmax(\hat{z}(x_i)/T)$.

The retrieval-augmented target distribution $\hat{p}(x_i)$ enables flexible knowledge transfer from the RAG drafter while maintaining verification capability. Since the unnormalized logits $z(x_i) \in \mathbb{R}$ have larger magnitude compared to the normalized distributions $p(x_i), q(x_i) \in [0,1]$, the $\hat{p}(x_i)$ preserves the long-context ability of target LLM to verify candidates effectively. We empirically validate the robustness of this distribution in~\Cref{subsubsec:robustness}.

% For inference, we replace $p(x_i)$ with $\hat{p}(x_i)$ in the acceptance criterion (\Cref{eq:reject_sampling}). Let $p(x_i) \!=\! [\ervw_j]_{j=1}^{j=|V|}$ and $\hat{p}(x_i) \!=\! [\hat{\ervw}_j]_{j=1}^{j=|V|}$ with $|V|$ as the vocabulary size, we maintain $\hat{\ervw}_{k} = \ervw_{k}$ for $k \in \{v \vert \hat{\ervw}_{v} < 0.1 \cdot \max(\hat{\ervw}_{1:|V|})\}$ to avoid the affect of tail distribution following~\citet{li-etal-2023-contrastive}.

% For inference, we replace $p(x_i)$ with $\hat{p}(x_i)$ in the acceptance criterion (\Cref{eq:reject_sampling}). Let $p(x_i) = [\ervw_j]_{j=1}^{|V|}$ and $\hat{p}(x_i) = [\hat{\ervw}_j]_{j=1}^{|V|}$ denote the probability over vocabulary $V$. Following~\citet{li-etal-2023-contrastive}, we maintain 
% \begin{equation}
%     \hat{\ervw}_{k} = \ervw_{k}, k \in \{v \vert \hat{\ervw}_{v} < 0.1 \cdot \max(\hat{\ervw}_{1:|V|})\},
% \end{equation}
%  to prevent distortion in the tail of the distribution.

 For inference, we replace $p(x_i)$ with $\hat{p}(x_i)$ in the acceptance criterion (\Cref{eq:reject_sampling}). Let $p(x_i) = [\ervw_j]_{j=1}^{|V|}$ and $\hat{p}(x_i) = [\hat{\ervw}_j]_{j=1}^{|V|}$ denote the probability vectors over vocabulary $V$. Following~\citet{li-etal-2023-contrastive}, we maintain 
\begin{equation}
    \hat{\ervw}_{k} = \ervw_{k}, \quad \forall k \in \{v \in [|V|]: \hat{\ervw}_{v} < 0.1 \cdot \max_{j \in [|V|]} \hat{\ervw}_{j}\},
\end{equation}
to prevent distortion in the tail of the distribution.



When rejection occurs, we sample from an adjusted residual distribution:
\begin{equation}
    x_i \sim \texttt{norm}({\max(p(x_i)-\hat{p}(x_i), p(x_i)-q(x_i))}).
\end{equation}
This sampling strategy maintains theoretical guarantees, where we prove in~\Cref{sec:proof2} that the resulting tokens follow the same distribution as direct sampling from the original target model $p(x_i)$.






% effectively transfers knowledge from the RAG drafter to the target model during inference, allowing the target distribution to better accommodate high-quality candidates while maintaining the theoretical framework of speculative decoding.


% This gradient naturally leads to our augmented logits through a gradient ascent step:







% \subsection{Complete Algorithm}
% Algorithm 1 presents the complete \ourMethod{} procedure, integrating RAG-based speculation with test-time distillation:

% \begin{algorithm}[H]
% \caption{\ourMethod{}}
% \begin{algorithmic}[1]
% \State \textbf{Input}: Long context $\mathcal{C}$, prefix $x_{<i}$
% \State Retrieve compressed context $\mathcal{C^{\text{S}}}$ using RAG
% \State Generate $\gamma$ candidates using draft model: $x^{\prime}_i \sim q_\mpsi(\cdot | [\mathcal{C^{\text{S}}};x_{<i}])$
% \State Compute augmented target distribution $\hat{p}$
% \State Accept/reject based on modified criterion using $\hat{p}$
% \State \textbf{Return}: Generated token $x_i$
% \end{algorithmic}
% \end{algorithm}

% This integrated approach enables efficient long-context inference while benefiting from RAG's focused information access, achieving both computational efficiency and enhanced generation quality.













% However, the $p(x_{\textbf{candidate}})$ may be insufficient to model the ``accurate'' target, as long-context LLMs may be inferior to RAG. 

% Here we reversely consider the draft (RAG) model as the ``expert'' model and target (long-context) model as the ``amateur'' model, hence access the contrastive distribution as $q(x)-p(x)$. This distribution implies the probability mass where RAG model assigned higher than long-context model. We now define a new target distribution:
% \begin{equation}
%     \hat{p} = norm(p + m(q-p)).
% \end{equation}
% Accept when 
% \begin{equation}
%     r \leq \frac{\hat{p}(x_{\textbf{candidate}})}{q(x_{\textbf{candidate}})}.
% \end{equation}
% If got rejected, we need to resample from
% \begin{equation}
%     x_{\text{resample}} \sim \max(p(x)-q(x), p(x)-\hat{p}(x)).
% \end{equation}