%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

\usepackage{amssymb}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{gensymb}

% \usepackage{microtype}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\Crefname{figure}{Figure}{Figures}
\crefname{figure}{Fig.}{Figs.}

\newcommand{\condensedpara}[1]{\vspace{0.25em}\noindent\textbf{#1}} %\enspace}

% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\usepackage[misc,geometry]{ifsym} 

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{A Survey on Text-Driven 360-Degree Panorama Generation}

% Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Hai Wang$^1$$^{\textrm{\Letter}}$
\quad
Xiaoyu Xiang$^2$
\quad
Weihao Xia$^{1}$
\quad
Jing-Hao Xue$^1$\\
\affiliations
$^1$University College London
\quad
$^2$Meta Reality Labs \\
\emails
\{hai.wang.22, weihao.xia.21, jinghao.xue\}@ucl.ac.uk,
xiaoyu.xiang.ai@gmail.com
}
% \fi

% % \iffalse
% \author{
% First Author$^1$
% \and
% Second Author$^2$\and
% Third Author$^{2,3}$\And
% Fourth Author$^4$\\
% \affiliations
% $^1$First Affiliation\\
% $^2$Second Affiliation\\
% $^3$Third Affiliation\\
% $^4$Fourth Affiliation\\
% \emails
% \{first, second\}@example.com,
% third@other.example.com,
% fourth@example.com
% }
% % \fi

\begin{document}

\maketitle

\begin{abstract}
The advent of text-driven 360-degree panorama generation, enabling the synthesis of 360-degree panoramic images directly from textual descriptions, marks a transformative advancement in immersive visual content creation. This innovation significantly simplifies the traditionally complex process of producing such content. Recent progress in text-to-image diffusion models has accelerated the rapid development in this emerging field. This survey presents a comprehensive review of text-driven 360-degree panorama generation, offering an in-depth analysis of state-of-the-art algorithms and their expanding applications in 360-degree 3D scene generation. Furthermore, we critically examine current limitations and propose promising directions for future research. A curated project page with relevant resources and research papers is available at \textcolor{teal}{\href{https://littlewhitesea.github.io/Text-Driven-Pano-Gen/}{https://littlewhitesea.github.io/Text-Driven-Pano-Gen/}}.
\end{abstract}

\section{Introduction}

Rapid growth of immersive technologies, such as virtual reality (VR) and augmented reality (AR), has dramatically increased the demand for high-quality panoramic visual content. Among such content, 360-degree panoramas are pivotal in delivering realistic and immersive experiences by capturing a complete spherical view of an environment. Traditionally, producing these panoramas requires specialized camera equipment and considerable technical expertise. However, recent advances in text-driven 360-degree panorama generation \cite{text2light,immerseGAN,stitchdiffusion,diffpano} have introduced groundbreaking capabilities, enabling the synthesis of 360-degree panoramic images directly from textual descriptions. This innovation can potentially revolutionize content creation over diverse domains including VR/AR applications, gaming, and virtual tours.

Unlike conventional 2D images, 360-degree panoramas, often represented through equirectangular projection \cite{360survey}, encompass the entire $360\degree\times180\degree$ field of view (FoV). This distinctive format poses unique challenges for text-driven generation, requiring not only accurate image synthesis but also excellent preservation of geometric consistency and seamless visual coherence across the full $360\degree$ horizontal and $180\degree$ vertical extents.

The availability of large-scale paired image-text datasets has facilitated the development of text-to-image latent diffusion models (LDMs) \cite{latentdiffusion}, which excel at synthesizing high-quality, visually compelling images aligned with given text descriptions. Leveraging the powerful generative capabilities of pre-trained LDMs, researchers have developed methods specifically tailored to address the unique challenges of text-driven 360-degree panorama generation \cite{panodiff,stitchdiffusion,aognet,diffusion360,panfusion}. Despite the rapid progress in this field, there is currently no survey that provides a systematic analysis of these emerging methodologies, elucidates their key characteristics, and highlights their applications. To address this gap, this paper presents a holistic survey and analysis of the latest research advances in text-driven 360-degree panorama generation.

This survey is structured as follows: First, we establish a foundational understanding of this field by introducing the principal representations of 360-degree panoramas, presenting prominent datasets commonly used in this area, and outlining key evaluation metrics employed to assess the quality and fidelity of generated panoramic content. Next, we review state-of-the-art methods for text-driven 360-degree panorama generation, categorizing them into two primary paradigms: (a) \textit{Text-Only Generation} and (b) \textit{Text-Driven Narrow Field-of-View (NFoV) Outpainting}. Methods on text-only generation aim to synthesize entire 360-degree panoramas exclusively from textual prompts, typically utilizing fine-tuning techniques such as Low-Rank Adaptation (LoRA)~\cite{lora} or DreamBooth~\cite{dreambooth} on pre-trained LDMs~\cite{latentdiffusion}. In contrast, text-driven NFoV outpainting allows greater user control by conditioning the generation process on both textual descriptions and initial NFoV images. Following this, we explore emerging applications of text-driven 360-degree panorama generation in the realm of 360-degree 3D scene generation. Finally, we discuss the prevailing challenges in this developing field and propose potential directions for future research.

This paper offers the first survey on text-driven 360-degree panorama generation, systematically reviewing its state-of-the-art techniques, key datasets and evaluation metrics, as well as exploring its applications in 360-degree 3D scene generation. Furthermore, we identify critical challenges and outline future research directions, aiming to offer a valuable resource to researchers and practitioners in this area.


\section{Related Work}

\subsection{Text-to-Image Diffusion Models}

Text-to-image (T2I) diffusion models \cite{glide,imagen,dall-e2,latentdiffusion,sdxl} have achieved remarkable progress in generating high-fidelity and photorealistic images from textual descriptions. These models have garnered widespread attention because of their intuitive text-based conditioning as a user-friendly interface for diverse image generation tasks.

T2I diffusion models can be broadly categorized into pixel-space and latent-space models. Pixel-space models, such as GLIDE \cite{glide} and Imagen \cite{imagen}, operate directly in the pixel space, producing visually impressive results at the expense of substantial computational resources, limiting their scalability. In contrast, latent diffusion models (LDMs) \cite{latentdiffusion} address these limitations by leveraging pre-trained autoencoders like VQGAN \cite{vqgan} to map images into a compact latent space, where the diffusion process is conducted. This reduces computational overhead while maintaining high-quality outputs, making LDMs a preferred framework for text-driven 360-degree panorama generation, as surveyed in this work.

\subsection{3D Scene Representation}

Efficient and accurate 3D scene representation is a critical challenge in computer graphics and vision. Traditional explicit representations, including point clouds, meshes, and voxel grids, often suffer from high memory requirements and struggle with complex topologies and unbounded scenes.

Neural implicit functions \cite{sdf,occupancy,3dgen}, which represent 3D scenes as continuous functions encoded within neural network parameters, offer a compact and flexible paradigm for scene representation. Notably, Neural Radiance Fields (NeRFs) \cite{nerf} stand out for their ability to achieve high-quality novel view synthesis. However, NeRF's reliance on dense volumetric sampling along camera rays results in slow training, hindering its practicability.

Recently, 3D Gaussian Splatting (3DGS) \cite{3dgs} has emerged as an efficient alternative to 3D scene representation. By combining an explicit representation of 3D Gaussians with a highly efficient differentiable rasterization pipeline, 3DGS facilitates rapid scene reconstruction and rendering. This advancement has opened up new possibilities, including recent explorations in text-to-3D 360-degree scene synthesis~\cite{dreamscene360,scenedreamer360}, which leverage text-driven 360-degree panorama generation techniques.


\section{Preliminaries}

\subsection{Representations of 360-Degree Panoramas}

The representation of 360-degree panoramic content poses a fundamental challenge: How to accurately map spherical visual information onto a two-dimensional plane? To address this, a variety of projection methodologies \cite{360survey} have been developed, each with distinct advantages and trade-offs. Below, we outline two widely used formats for 360-degree panorama generation: Equirectangular Projection and Cubemap Projection.

\paragraph{Equirectangular Projection (ERP)} As the most prevalent representation format for 360-degree panoramas, ERP establishes a direct mapping between spherical and planar coordinates: longitude corresponds to the horizontal axis, spanning the full 360$\degree$ range, while latitude maps to the vertical axis, covering 180$\degree$ from -90$\degree$ (south pole) to +90$\degree$ (north pole). ERP's simplicity and compatibility with web viewers and VR headsets make it the preferred choice for numerous applications. Additionally, its representation as a single, continuous image allows the direct application of image manipulation techniques, such as text-driven 360-degree panorama-to-panorama translation \cite{360pant}. Despite these advantages, ERP introduces pronounced geometric distortions, particularly at the polar regions, where the visual content appears stretched. Furthermore, the texel density of a spherical image in ERP is non-uniformly distributed: it is comparatively lower in the equatorial regions and markedly higher towards the poles. This inhomogeneity can be particularly problematic in scenarios where critical visual information is predominantly located away from the poles, leading to inefficient utilization of image resolution for regions of interest. In this survey, unless explicitly stated otherwise, 360-degree panoramas are represented using ERP.

\paragraph{Cubemap Projection (CMP)} CMP offers an alternative representation that mitigates the distortions inherent in the ERP format, particularly at the poles. In CMP, the spherical image is projected onto the six faces of a cube, with each face representing a 90$\degree$$\times$90$\degree$ field of view. This division significantly reduces geometric distortions, making CMP more compatible with diffusion priors from text-to-image diffusion models trained on standard perspective images \cite{cubediff}. However, CMP introduces several challenges: (1) it increases the complexity of image manipulation compared to the single-image format of ERP; (2) it may necessitate additional conversion for compatibility with platforms or viewers that primarily support ERP. Despite these practical challenges, CMP is well-suited for applications that demand reduced distortion and higher fidelity. The width and height of an ERP image are four and two times the side length of the corresponding CMP, respectively, reflecting the geometric relationship between the two formats.


\subsection{Datasets}

360-degree panoramic image generation from text prompts presents unique challenges due to the complete 360$\degree$$\times$180$\degree$ field of view that these images encompass. Text-to-image diffusion models \cite{imagen,latentdiffusion,sdxl}, predominantly trained on perspective images with a narrower field of view, often struggle to synthesize high-quality 360-degree panoramas. To address this, several specialized datasets have been developed to facilitate research in this domain. \cref{tab:dataset} summarizes these datasets, with further details provided below.

\begin{table}[t]
\small
\centering
\begin{tabular}{lcrcl}
\toprule
Dataset (Category) & Year & \#Samples & Res.  & License \\ 
\midrule
SUN360 (I, O)    & 2012                   & 67,583       &  9K & Custom   \\ 
Matterport3D (I)     & 2017                 & 10,800       & 2K    & Custom         \\
Laval Indoor (I)      & 2017                 & 2,233       & 7K   & Custom   \\
Laval Outdoor (O)      & 2019                 & 205       & 7K   & Custom  \\
Structured3D (I)      & 2020                  & 196,515       & 1K   & Custom \\
Pano360 (I, O)      & 2021                & 35,000       & 8K   & Custom   \\
$\star$Polyhaven (I, O)      & 2025                  & 786       & 8K     & CC0       \\
$\star$Humus (I, O)      & 2025                  & 139       & 8K       & CC BY 3.0     \\
\bottomrule
\end{tabular}
    \caption{Summary of Text-Driven Panorama Generation Datasets: Includes categories, publication year, sample size, resolution (Res.), and license. Categories are indoor (I), outdoor (O), or hybrid (I, O). Datasets marked with $\star$ are sourced from public websites. 
    }
    \label{tab:dataset}
\end{table}

\condensedpara{SUN360}~\cite{sun360} is a comprehensive database comprising 67,583 high-resolution (9104$\times$4552) panoramic images sourced from the Internet. Each image covers a 360$\degree$$\times$180$\degree$ field of view in ERP format and is manually categorized into 80 distinct classes. Originally created for scene viewpoint recognition, SUN360 now serves as a valuable resource for a wide range of computer vision, computer graphics, and related research areas.

\condensedpara{Matterport3D}~\cite{matterport3d} offers 10,800 indoor 360-degree panoramic images with corresponding depth maps, all at a resolution of 2048$\times$1024 pixels. These panoramas are derived from 194,400 RGB-D images of 90 buildings, making it a rich dataset for studying indoor environments.

\condensedpara{Laval Indoor}~\cite{lavalindoor} consists of 2,233 high-dynamic-range, high-resolution (7768$\times$3884) 360-degree panoramic images, specifically curated for the study of extensive indoor scenes, such as factories, apartments, and houses.

\condensedpara{Laval Outdoor}~\cite{lavaloutdoor} complements its indoor counterpart, offering 205 high-dynamic-range, high-resolution (7768$\times$3884) 360-degree panoramic images that capture diverse outdoor environments, including urban and natural scenes.

\condensedpara{Structured3D}~\cite{structured3d} contains 196,515 360-degree panoramas with varying configurations and lighting conditions, representing 21,835 distinct rooms. Rendered at a resolution of 1024$\times$512 from 3D scenes of original house design files, Structured3D is ideal for research on structured 3D modeling and understanding.

\condensedpara{Pano360}~\cite{pano360} contains 35,000 360-degree panoramic images with a resolution of 8192$\times$4096. Of these, 34,000 are sourced from Flickr, with the remainder rendered from photorealistic 3D scenes. Pano360 was originally proposed for training camera calibration networks.

\condensedpara{Polyhaven}~\cite{polyhaven} contributes 786 real-world high-resolution (8192$\times$4096) 360-degree panoramas encompassing a variety of indoor and outdoor scenes.

\condensedpara{Humus}~\cite{humus} includes 139 real-world 360-degree panoramas represented using cubemap projection, with each face having a resolution of 2048$\times$2048 pixels. This dataset includes indoor and outdoor environments. 


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{demo.pdf}
    \caption{Paradigms for Text-Driven 360-Degree Panorama Generation. (a) Text-Only Generation synthesizes 360-degree panoramas from textual descriptions only. (b) Text-Driven NFoV Outpainting uses prompts and initial NFoV images as input to generate panoramic images.}
    \label{fig:demo}
\end{figure*}

\subsection{Evaluation Metrics}
\label{subsec:metrics}

A rigorous evaluation of text-driven 360-degree panorama generation methods typically requires to combine (a)~\textit{universal} and (b)~\textit{panorama-specific} metrics. The universal metrics, comprising Fréchet Inception Distance (FID), Kernel Inception Distance (KID), Inception Score (IS), and CLIP Score~(CS), are widely applicable to both perspective and panoramic images.

\condensedpara{FID}~\cite{fid} measures the distance between feature distributions of generated and real images using a pre-trained Inception-v3 network~\cite{inception}. Lower FID scores indicate better perceptual quality and closer alignment with the real image distribution.

\condensedpara{KID}~\cite{kid} measures the difference between real and generated image distributions by computing the maximum mean discrepancy of their features extracted from Inception-v3 \cite{inception}. Similar to FID, lower KID values indicate better image quality.

\condensedpara{IS}~\cite{is} measures both the quality and diversity of generated images by leveraging Inception-v3 \cite{inception}. It calculates the KL divergence between the conditional class distribution of generated images and the marginal distribution over all generated samples. Higher IS scores suggest better visual quality and diversity.

\condensedpara{CS}~\cite{clip} evaluates consistency between text prompts and generated images using the CLIP model~\cite{clip}. It calculates the cosine similarity between the text embedding of the prompt and the visual embedding of the generated image. A higher CS score reflects stronger text-image alignment and semantic coherence.


The universal metrics provide general assessments but often fail to capture nuances and geometric distortions unique to 360-degree panoramas.
Panorama-specific metrics designed specifically for ERP-based 360-degree panoramic images include Fréchet Auto-Encoder Distance (FAED), Omnidirectional FID (OmniFID), and Discontinuity Score (DS).

\condensedpara{FAED}~\cite{faed} computes Fréchet distances between features extracted from generated and real panoramas. Unlike FID, it employs an autoencoder~\cite{autoencoder} specifically trained on 360-degree panoramic images. Lower FAED scores reflect better perceptual and geometric quality tailored to the unique panoramic properties.

\condensedpara{OmniFID}~\cite{omnifid} adapts FID to specifically evaluate 360-degree panoramas. It converts equirectangular panoramas into cubemap representations and calculates FID across three disjoint subsets of cubemap faces, averaging the results. Lower OmniFID scores indicate higher geometric fidelity in 360-degree panorama generation.

\condensedpara{DS}~\cite{omnifid} measures the seam alignment across the borders of generated panoramas by applying a kernel-based edge detection algorithm. Lower discontinuity scores correspond to fewer visible seam artifacts, indicating better perceived consistencies across the seam.

We provide a comprehensive comparison of state-of-the-art methods, introduced in the following section, using the outlined metrics in~\cref{subsec:comparison}.


\section{State-of-the-Art Methods} 

Existing text-driven 360-degree panorama generation methods can be broadly categorized into two paradigms according to input modalities: (a)~\textit{Text-Only Generation} aims to synthesize 360-degree panoramas from textual prompts only, while (b) \textit{Text-Driven NFoV Outpainting} leverages both textual descriptions and initial NFoV images to guide the generation process, offering enhanced user control. \cref{fig:demo} provides an intuitive illustration for both paradigms. We detail the literature for both as follows.

\subsection{Text-Only Generation} 
\label{subsec:text_only}

This paradigm focuses on synthesizing 360-degree panoramas from textual descriptions. \cref{tab:only} provides a comparative overview of representative text-only methods.

Text2Light~\cite{text2light}, an early notable effort, explored a hierarchical framework utilizing VQGAN~\cite{vqgan} and CLIP~\cite{clip} to address this challenge. Recently, the advent of latent diffusion models (LDMs)~\cite{latentdiffusion} for text-to-image synthesis marked a significant advancement, enabling more sophisticated 360-degree panorama generation techniques. LDMs are typically trained on vast datasets consisting of perspective images with a limited field of view and corresponding text descriptions. 
Despite demonstrating robust capabilities in generating perspective images from text prompts, these models face significant difficulties when creating 360-degree panoramas with a complete 360$\degree$$\times$180$\degree$ field of view, which differ substantially from traditional perspective images.
 

\paragraph{Fine-Tuning.}  To adapt pre-trained LDMs for 360-degree panorama synthesis, a common strategy is to fine-tune these models with specialized 360-degree panorama datasets. Diffusion360 \cite{diffusion360} exemplifies this approach by leveraging the DreamBooth technique~\cite{dreambooth} to fine-tune a pre-trained LDM \cite{latentdiffusion} on SUN360 \cite{sun360}. To ensure geometric consistency of boundaries, Diffusion360 utilizes a circular blending strategy during both the denoising process and the VAE decoding stage, effectively reducing seam artifacts. In addition, it introduces a super-resolution module to enable the generation of high-resolution (6114$\times$3072) 360-degree panoramas. Notably, the DreamBooth fine-tuning approach is computationally demanding and time-consuming because it generally fine-tunes a significant portion of the model parameters.

In contrast, LoRA~\cite{lora} has recently gained attention as a parameter-efficient fine-tuning method. LoRA works by injecting trainable low-rank matrices into the pre-trained model's weights, allowing for rapid adaptation to new tasks with minimal additional parameters. For example, StitchDiffusion~\cite{stitchdiffusion} employs LoRA to fine-tune a pre-trained LDM on a dataset of paired 360-degree images (sourced from Polyhaven~\cite{polyhaven}) and corresponding textual descriptions generated using BLIP~\cite{blip}. It further introduces a tailored generation method based on MultiDiffusion~\cite{multidiffusion} to ensure boundary continuity in generated panoramas.

Other works have similarly adopted the LoRA fine-tuning technique. PanFusion \cite{panfusion} devises a dual-branch diffusion model, trained on Matterport3D~\cite{matterport3d}, with separate LoRA layers to integrate both global panoramic and local perspective views. It introduces an equirectangular-perspective projection attention module to facilitate information exchange between the two branches, aiming to alleviate visual inconsistencies in the generated 360-degree panoramas. However, PanFusion's output often exhibits blurriness at the top and bottom regions, due to its training dataset. To avoid this issue, DiffPano~\cite{diffpano} utilizes the Habitat Matterport 3D dataset~\cite{HM3D} to produce multi-view consistent 360-degree panoramas with clearer top and bottom details. For generating more precise textual descriptions, DiffPano adopts BLIP2 \cite{blip2} and Llama2 \cite{llama2} sequentially, resulting in a panoramic video-text dataset. Based on this dataset, it fine-tunes a pre-trained LDM using LoRA for single-view text-driven 360-degree panorama generation. Furthermore, to enable multi-view 360-degree panorama generation based on text prompts and camera viewpoints, DiffPano proposes a spherical epipolar-aware attention module, which effectively extends single-view panorama generation to multi-view consistent panorama generation.

\paragraph{Training-Free.} PanoFree \cite{panofree}, on the other hand, departs from fine-tuning approaches by introducing a tuning-free multi-view image generation framework based on a pre-trained LDM. Guided by textual descriptions, PanoFree leverages iterative warping and inpainting steps to produce multi-view perspective images, which are subsequently stitched into 360-degree panoramas, thus avoiding the need for specialized 360-degree panorama datasets.


\begin{table}[t]
\small
    \centering
    \begin{tabular}{llccc}
        \toprule
        Method & Publication & LDM-B & TF & Code \\
        \midrule
        Text2Light & TOG 2022 & $\times$    & $\times$ & $\checkmark$ \\
        Diffusion360 & arxiv 2023 & $\checkmark$    & $\times$ & $\checkmark$ \\
        StitchDiffusion & WACV 2024 & $\checkmark$   & $\times$ & $\checkmark$\\
        PanFusion     & CVPR 2024 & $\checkmark$    & $\times$ & $\checkmark$ \\
        PanoFree     & ECCV 2024 & $\checkmark$    & $\checkmark$ & $\times$ \\
        DiffPano     & NeurIPS 2024 & $\checkmark$     & $\times$ & $\times$ \\
        \bottomrule
    \end{tabular}
    \caption{Summary of Text-Only Generation. `LDM-B' indicates whether the method is based on latent diffusion models. `TF' specifies if it is training-free. `Code' denotes whether both the source code and the pre-trained model checkpoint are publicly accessible.}
    \label{tab:only}
\end{table}


\subsection{Text-Driven NFoV Outpainting} 
\label{subsec:outpainting}

This paradigm enhances user control by conditioning the generation process on both textual prompts and initial narrow NFoV images. The NFoV image, representing a limited portion of the scene, provides a visual starting point, which the generative model then extends into a complete panorama guided by the textual description. \cref{tab:outpainting} offers a summary of representative text-driven NFoV outpainting approaches.

An early attempt in this paradigm is ImmerseGAN~\cite{immerseGAN}, which uses a GAN-based inpainting architecture for this task. To achieve text-guided outpainting, ImmerseGAN adopts a pre-trained discriminative network to produce a latent vector representing the given textual description. This latent vector subsequently guides the generator in producing a 360-degree panorama that is semantically consistent with the text prompt. More recent approaches have primarily focused on leveraging the power of pre-trained latent diffusion models (LDMs) for their strong image generation priors acquired from training on large-scale datasets. These methods can be broadly classified into autoregressive-based (AR-based) and non-autoregressive-based (NAR-based) approaches according to their underlying frameworks.

\paragraph{AR-Based.} AOG-Net \cite{aognet} introduces an autoregressive framework, building upon a pre-trained LDM \cite{latentdiffusion}, to progressively outpaint NFoV images into complete panoramas under textual guidance. This stepwise approach enhances the generation of fine-grained visual content and improves alignment with the input textual descriptions. Moreover, AOG-Net incorporates a global-local conditioning mechanism to integrate text, omni-visual, NFoV and omni-geometry guidances at each autoregression step, thereby ensuring consistency across the generated panorama. AOG-Net is trained on Laval Indoor \cite{lavalindoor} and Laval Outdoor \cite{lavaloutdoor} for indoor and outdoor scenarios, respectively. Following the training dataset settings in AOG-Net, OPa-Ma~\cite{opa-ma} uses a pre-trained LDM with Mamba~\cite{mamba}, a state-space model known for its efficiency in handling long sequences, to iteratively outpaint local regions in each step. It introduces two modules: the Visual-textual Consistency Refiner, which enhances input utilization during generation, and the Global-local Mamba Adapter, which ensures global coherence across the generated panorama. 

\paragraph{NAR-Based.}  PanoDiff \cite{panodiff}, the first LDM-based method for text-driven NFoV outpainting, is trained on SUN360 \cite{sun360}. It first converts the input NFoV images into partial panoramas with visibility masks, and then employs a ControlNet-based LDM \cite{controlnet} for text-guided panorama completion. To ensure geometric continuity at the borders of the generated panorama, PanoDiff further implements a circular padding scheme during inference. Similarly, Diffusion360 \cite{diffusion360} adopts a ControlNet-based LDM \cite{controlnet} to generate 360-degree panoramas from perspective images and textual descriptions. However, instead of circular padding, Diffusion360 leverages a circular blending strategy during the denoising and VAE decoding stages for improved boundary continuity of the generated 360-degree panorama.

Unlike previous methods \cite{panodiff,diffusion360} that produce 360-degree panoramas using equirectangular representation, 
CubeDiff \cite{cubediff}, inspired by multi-view diffusion models \cite{mvdream,MVDiffusion}, generates 360-degree panoramas in cubemap format. This cubemap representation enables CubeDiff to more effectively leverage the diffusion priors learned by the LDM from extensive perspective images during the generation process. CubeDiff fine-tunes a pre-trained LDM on a mixed dataset of Structured3D~\cite{structured3d}, Pano360~\cite{pano360}, Polyhaven~\cite{polyhaven}, and Humus~\cite{humus}, using a single conditional view (NFoV image) and textual embeddings as input. To model inter-face relationships within the cubemap, CubeDiff inflates all 2D attention layers in the LDM, enabling effective learning of inter-view dependencies.


\begin{table}[t!]
\small
    \centering
    \begin{tabular}{llccc}
        \toprule
        Method & Publication & LDM-B & TF & Code \\
        \midrule
        ImmerseGAN & 3DV 2022 & $\times$     & $\times$ & $\times$ \\
        PanoDiff & MM 2023 & $\checkmark$    & $\times$ & $\checkmark$ \\
        Diffusion360 & arxiv 2023 & $\checkmark$    & $\times$ & $\checkmark$ \\
        AOG-Net & AAAI 2024 & $\checkmark$    & $\times$ & $\times$ \\
        OPa-Ma     & arxiv 2024 & $\checkmark$     & $\times$ & $\times$ \\
        CubeDiff     & ICLR 2025 & $\checkmark$   & $\times$ & $\times$ \\
        \bottomrule
    \end{tabular}
    \caption{Summary of Text-Driven NFoV Outpainting. 
    For explanations of the `LDM-B', `TF' and `Code' columns, see  \cref{tab:only}.
    }
    \label{tab:outpainting}
\end{table}


\begin{table*}[t]
    \centering
    % \resizebox{\textwidth}{!}
    { % Scale table to text width
\begin{tabular}{lcccccccc}
\toprule
Method          & FID $\downarrow$ & KID ($\times10^{-2}$) $\downarrow$ & IS $\uparrow$ & CS $\uparrow$  & FAED $\downarrow$ & OmniFID $\downarrow$ & DS $\downarrow$  & Inference (s) $\downarrow$\\ 
\midrule
Text2Light      & 72.63 & \underline{1.54}  & 5.35 & \textbf{19.20}    & 18.10       & 99.81  & 5.38 & 33   \\
Diffusion360    & \underline{70.32} & {2.00} & 5.29 & {18.74}    & \textbf{12.43}       & \underline{92.23}  & \underline{0.94} & \textbf{3}   \\
StitchDiffusion & 76.69 & 2.04  & \textbf{7.36} & \textbf{19.20}    & 15.58       & 108.63  & 1.07 & \underline{28}   \\
PanFusion       & \textbf{61.23} & \textbf{1.07}  & \underline{6.16} & \underline{18.96}    & \underline{13.16}       & \textbf{92.22}  & \textbf{0.85} & 30 \\
\midrule
PanoDiff & \underline{65.94} & \underline{2.44}  & \textbf{4.72} & \textbf{19.02}    & \underline{10.24}       & \underline{122.30}  & \underline{1.10} & \underline{48}   \\
Diffusion360       & \textbf{64.19} & \textbf{2.05}   & \underline{4.53} & \underline{17.92}   & \textbf{5.50}       & \textbf{101.39}  & \textbf{0.72} & \textbf{4} \\
\bottomrule
\end{tabular}
}
    \caption{Quantitative Comparison of Representative Text-Driven 360-Degree Panorama Generation. The first block of rows are for methods in the paradigm of Text-Only Generation, while the second block of rows are for Text-Driven NFoV Outpainting. We use metrics outlined in~\cref{subsec:metrics} for comprehensive evaluation. The inference time, required by each method to generate a 1024$\times$512 360-degree panorama, is also reported. The \textbf{best} and \underline{second-best} results are highlighted for the two paradigms, respectively.}
    \label{tab:comparison}
\end{table*}


\subsection{Quantitative Comparison}
\label{subsec:comparison}

To systematically evaluate strengths and weaknesses of representative methods of different kinds, we benchmark Text-Only Generation~(\cref{subsec:text_only}) and Text-Driven NFoV Outpainting~(\cref{subsec:outpainting}) methods with publicly available inference codes and model checkpoints.  For Text-Only Generation, we compare Text2Light \cite{text2light}, Diffusion360 \cite{diffusion360}, StitchDiffusion \cite{stitchdiffusion} and PanFusion \cite{panfusion}. For Text-Driven NFoV Outpainting, we compare PanoDiff \cite{panodiff} and Diffusion360. We conduct a comprehensive comparison using metrics outlined in~\cref{subsec:metrics}. 


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{demo2_text23d.pdf}
    \caption{The Framework for Text-Driven 360-Degree 3D Scene Generation using Text-Driven 360-Degree Panorama Generation. This framework accommodates both Text-Only Generation and Text-Driven NFoV Outpainting methods. The input NFoV image is omitted when employing text-driven NFoV outpainting methods for simplicity.}
    \label{fig:demo2}
\end{figure*}


To ensure an unbiased evaluation of the generalizability of the methods, we employ an out-of-domain dataset, ODI-SR~\cite{lau-net}, on which none of the models have been explicitly trained.
For generating text descriptions, we utilize BLIP2 \cite{blip2} to create textual captions for the 360-degree panoramas included in the ODI-SR dataset. These generated text prompts serve as inputs for both Text-Only Generation and Text-Driven NFoV Outpainting tasks. 
To simulate NFoV images, we first project the equirectangular 360-degree panoramas from the ODI-SR dataset into a cubemap format and then extract the front face of each cubemap. The original 360-degree panoramas from ODI-SR are designated as real images and used as ground truth for the computation of evaluation metrics.

The quantitative results obtained from this comparative evaluation, across the seven evaluation metrics, are presented in \cref{tab:comparison}. To provide insights into the computational efficiency of each method, we also report the inference time required to generate a 1024$\times$512 360-degree panorama on a consistent machine equipped with an RTX A6000 GPU.

\section{Emerging 3D Applications} 

Recent advances in text-driven 360-degree panorama generation~\cite{stitchdiffusion,diffusion360,panfusion} have catalyzed innovative methods for reconstructing 360-degree 3D scenes from textual descriptions. 360-degree panoramic images inherently capture both global contexts and geometric constraints of a scene, making them an essential intermediate representation for 3D scene generation. Consequently, recent text-driven 360-degree 3D scene generation methods~\cite{fastscene,holodreamer,dreamscene360,scenedreamer360,layerpano3d} utilize 360-degree panorama generation to bridge the gap between text prompts and 360-degree 3D scene reconstruction. 


As depicted in~\cref{fig:demo2}, these methods typically use a two-stage process: (1) 360-Degree Panorama Generation: generating a 360-degree panorama from the input text prompt using a fine-tuned LDM~\cite{latentdiffusion}, and (2) 3D Scene Reconstruction: inferring a 3D representation, typically with 3D Gaussian Splatting (3DGS)~\cite{3dgs}, from the generated panorama and corresponding multi-view perspective images. 
\cref{tab:t23d} provides a comparative summary.


Within this framework, emerging methods are primarily differentiated by their choice of (a) 360-degree panorama generators and their (b) strategies for extracting and utilizing 3D information.
For instance, FastScene~\cite{fastscene} and HoloDreamer~\cite{holodreamer} both employ Diffusion360~\cite{diffusion360} to generate the initial 360-degree panorama depicting a scene from a given text prompt. FastScene~\cite{fastscene} then synthesizes multi-view panoramas of this scene for specific camera poses using Coarse View Synthesis and Progressive Novel View Inpainting. With these synthesized multi-view panoramas, FastScene introduces Multi-View Projection to get their perspective views. The point clouds derived from these views are then used as input for 3DGS to reconstruct the 3D scene. HoloDreamer~\cite{holodreamer} enhances the Diffusion360-generated panorama with two distinct ControlNet-based LDMs~\cite{controlnet} and a super-resolution network to create a high-resolution, stylized output. Subsequently, HoloDreamer initializes 3D Gaussians using point clouds derived from a reverse equirectangular projection of the high-resolution panorama combined with its corresponding depth information. Finally, a two-stage 3DGS optimization process is developed to refine the scene rendering, resulting in the desired 3D scene reconstruction.

Furthermore, certain methods deviate from the reliance on Diffusion360~\cite{diffusion360}. DreamScene360~\cite{dreamscene360} utilizes StitchDiffusion~\cite{stitchdiffusion} to generate multiple 360-degree panorama candidates and then employs a self-refinement process to select the optimal candidate for initializing panoramic 3D Gaussians with a 3D geometric field. To facilitate visual feature correspondences between different views and maintain geometric consistencies during the 3DGS optimization process, semantic and geometric regularizations are applied. In contrast, SceneDreamer360~\cite{scenedreamer360} uses a fine-tuned PanFusion~\cite{panfusion} generator, coupled with a super-resolution module from~\cite{diffusion360}, to produce a high-resolution (6K) panorama aligned with the input text prompt. It then uses optimization-based viewpoint selection to extract multi-view images, which are subsequently used for improved point cloud initialization, ultimately leading to 3DGS-based scene reconstruction.

\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
Method & Publication & 360-Degree PG & 3D Lifting \\ 
\midrule
FastScene      & IJCAI 2024  & Diffusion360    & 3DGS                   \\ 
DreamScene360    & ECCV 2024      & StitchDiffusion         &  3DGS       \\
HoloDreamer         & arxiv 2024        & Diffusion360  & 3DGS \\           
LayerPano3D       & arxiv 2024        & Diffusion360 \& PanFusion   & 3DGS \\
SceneDreamer360    & arxiv 2024    &  PanFusion    &  3DGS      \\
\bottomrule
\end{tabular}
    }
    \caption{Summary of Text-Driven 360-Degree 3D Scene Generation. The column labeled `360-Degree PG' indicate which text-driven 360-degree panorama generation techniques are adopted to synthesize the intermediate panoramic representation of the scene. `3DGS' denotes 3D Gaussian Splatting.
    }

    \label{tab:t23d}
\end{table}


Other methods explore alternative panorama generation techniques. LayerPano3D \cite{layerpano3d} begins by generating four orthogonal perspective views with a fine-tuned text-to-image model \cite{sdxl}. These initial views are then combined with text-guided inpainting \cite{latentdiffusion}, and further processed by using a fine-tuned Diffusion360 \cite{diffusion360} model to outpaint the polar regions, resulting in a reference 360-degree panorama. To handle occlusions in complex scenes, LayerPano3D \cite{layerpano3d} decomposes the reference panorama into multiple depth-based layers and utilizes a fine-tuned inpainter \cite{panfusion} to complete unseen content at each layer. These inpainted, layered panoramas then provide supervision for panoramic 3D Gaussian scene optimization.



\section{Challenges and Future Directions}

Despite the impressive results achieved in text-driven 360-degree panorama generation, challenges remain in evaluation metrics, resolution, controllability, and model design. This section identifies these challenges and outlines potential directions for future research.

\paragraph{Evaluation Metrics} Although OmniFID \cite{omnifid} extends FID to account for geometric fidelity in generated 360-degree panoramas, it primarily focuses on geometric aspects, leaving other crucial dimensions unaddressed. Text-to-image consistency, commonly evaluated by using the CLIP Score \cite{clip}, is another critical metric for assessing text-driven 360-degree panorama generation methods. However, the CLIP Score, primarily designed for conventional 2D images, relies on an image encoder predominantly trained on standard perspective images. This encoder may not fully capture the inherent distortions and spatial relationships unique to 360-degree panoramas represented by equirectangular projections. Consequently, the CLIP Score may not accurately reflect the true semantic consistency in the context of 360-degree panorama generation. Future research could explore innovative evaluation metrics that effectively assess the semantic consistency between the input prompt and the generated 360-degree panoramic image.


\paragraph{Higher Resolution} While Diffusion360 \cite{diffusion360}, which uses a super-resolution module, is among the few methods that can currently achieve a maximum resolution of 6144$\times$3072 (6K), this remains inadequate for demanding applications like VR gaming and high-fidelity 3D scene reconstruction, which often necessitate resolutions of 8K or higher to capture intricate details of landscapes and architecture. Generating such high-resolution panoramas, however, introduces significant challenges related to computational latency and memory consumption. Addressing these limitations requires the development of more efficient model architectures and optimization techniques. Promising approaches include the use of window-based operations, model pruning, quantization, and advanced neural network designs tailored to resource-intensive tasks. Moreover, the availability of high-resolution, large-scale datasets will be critical for driving progress in this direction.


\paragraph{Multi-modal Generation} Existing text-driven methods, despite their ability to produce photorealistic 360-degree panoramas, often lack precise control over global semantic layout and spatial structure of the generated scene. This motivates exploring multi-modal approaches to enhance controllability. Although 360PanT \cite{360pant} demonstrates panorama-to-panorama translation using auxiliary modalities like edge and segmentation maps alongside text, its outputs deviate from the standard 360$\degree$$\times$180$\degree$ field of view when these additional modalities are incorporated. Future research should focus on developing multi-modal techniques that effectively integrate diverse inputs (\textit{e.g.}~depth maps, segmentation maps, or edge maps) with text prompts to achieve fine-grained spatial control in the generated 360-degree panoramas, while ensuring strict adherence to the standard equirectangular projection format.

\paragraph{Model Design} Most text-driven 360-degree panorama generation methods are built upon latent diffusion models (LDMs) \cite{latentdiffusion}. While LDMs have achieved remarkable success in text-to-image synthesis, recent advancements in autoregressive models indicate promising alternative architectures. Specifically, Visual Autoregressive (VAR) models, exemplified by Infinity~\cite{infinity}, have exhibited superior performance compared to the leading LDMs in standard text-to-image synthesis. This highlights an exciting avenue for future research: exploring VAR-based models for text-driven 360-degree panorama generation.

\section{Conclusion}

This survey has provided a comprehensive overview of the rapidly evolving field of text-driven 360-degree panorama generation. We began by introducing two primary representation methods of 360-degree panoramas, along with widely used datasets, and key evaluation metrics in this domain. Subsequently, we presented an in-depth discussion of prevalent methods for text-driven 360-degree panorama generation and their applications in generating 360-degree 3D scene. Despite the significant progress achieved in this field, several challenges remain. To address these challenges, we have articulated promising directions for future research.


% \section{\LaTeX{} and Word Style Files}\label{stylefiles}

% The \LaTeX{} and Word style files are available on the IJCAI--25
% website, \url{https://2025.ijcai.org/}.
% These style files implement the formatting instructions in this
% document.

% The \LaTeX{} files are {\tt ijcai25.sty} and {\tt ijcai25.tex}, and
% the Bib\TeX{} files are {\tt named.bst} and {\tt ijcai25.bib}. The
% \LaTeX{} style file is for version 2e of \LaTeX{}, and the Bib\TeX{}
% style file is for version 0.99c of Bib\TeX{} ({\em not} version
% 0.98i). .

% The Microsoft Word style file consists of a single file, {\tt
%         ijcai25.docx}. 
% %This template differs from the one used for IJCAI--23.

% These Microsoft Word and \LaTeX{} files contain the source of the
% present document and may serve as a formatting sample.

% Further information on using these styles for the preparation of
% papers for IJCAI--25 can be obtained by contacting {\tt
%         proceedings@ijcai.org}.

% \appendix

% \section*{Ethical Statement}

% There are no ethical issues.

% \section*{Acknowledgments}

% The preparation of these instructions and the \LaTeX{} and Bib\TeX{}
% files that implement them was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Preparation of the Microsoft Word file was supported by IJCAI.  An
% early version of this document was created by Shirley Jowell and Peter
% F. Patel-Schneider.  It was subsequently modified by Jennifer
% Ballentine, Thomas Dean, Bernhard Nebel, Daniel Pagenstecher,
% Kurt Steinkraus, Toby Walsh, Carles Sierra, Marc Pujol-Gonzalez,
% Francisco Cruz-Mencia and Edith Elkind.


%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{survey_360}

\end{document}

