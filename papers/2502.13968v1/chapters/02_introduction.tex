\section{Introduction}\label{sec:intro} 


The world we live in is compositional.
A typical office desk, for example, would consist of a monitor, a keyboard, a few cups of coffee, mobile phones, and so on.
Needless to say, we rarely encounter scenes comprising of one and only one object.
Yet, most 3D reconstruction research~\cite{mildenhall2020nerf, zhang2021stnerf, weng2022humannerf, rosu2023permutosdf, yariv2021volume, wang2021neus} has focused on scenes with only one object (e.g.~the famous caterpillar scene).
When more than one object is present in the scene (e.g.~the GTA Truck scene), the compositionality of the scene is ignored and the entire scene is reconstructed jointly.
\par
Recent works that addressed the challenge of compositional scene reconstruction have either used object templates~\cite{zhang2023neuraldome, bhatnagar22behave, fan2023arctic, GRAB:2020}, or parametric models of humans or hands~\cite{huang2022hhor, ye2022ihoi, Hi4D}.
A few works that propose a \textit{generalised} solution~\cite{wu2022object, Wu2023objectsdfplus} suffer from inter-penetration of the two or more interacting geometries.
In this work, we focus on the generalised compositional reconstruction setting (as shown in~\cref{fig:teaser}) while mitigating the penetration artefacts of the existing literature.
This mandates addressing the challenges posed by severe occlusion of the objects during interaction (e.g.~a person holding a cup), as well as accounting for the difference in object scales while sampling.

\par
With these considerations in perspective, we propose a new markerless, template-free approach for compositional 3D reconstruction of arbitrary objects undergoing interactions in a scene observed from 
multiple views. 
We represent the object geometries as separate Signed Distance Fields (SDFs) and the appearance with the corresponding Neural Radiance Fields. 
The 3D scene is encoded jointly for the objects by using a \textit{shared} multi-resolution hashgrid~\cite{mueller2022instant} which can be decoded into separate SDFs of the  target objects (e.g.~a hand and a book). 
Crucially, we propose a novel alpha-blending loss which enforces that a point lying inside one object has a high opacity only for the corresponding object's SDF while suppressing the opacity for the other. 
This incentivises clean separation boundaries and reduces the penetration volume between the two SDFs, even if the queried point is poorly observed (e.g.~due to occlusion). 
\par
To demonstrate the effectiveness of our method, we capture a new real-world dataset consisting of several scenes of human-object
interactions. 
For this, we ask the subjects to naturally interact with various small and mid-sized objects in a large capture dome. 
In summary, the technical contributions of this paper are as follows: 
\begin{itemize}[noitemsep]
    \item A novel markerless and category-agnostic approach for high-quality 3D reconstruction of two interacting objects from multi-view RGB inputs; 
    \item A shared neuro-implicit representation that can be jointly optimised for the geometries of interacting objects while also supporting separable free-viewpoint rendering; 
    \item An interaction-aware alpha-compositing of opacity values for each SDF enforcing clean separation boundaries and mitigating inter-object 
    penetration in 3D; 
    \item A new multi-view dataset for human-object interactions. 
\end{itemize} 

In addition to the captured dataset, we also evaluate our method on the publicly available WildRGB-D~\cite{xia2024rgbd} (object-object) and AffordPose~\cite{affordpose} (hand-object) datasets, and 
demonstrate its effectiveness on marker-based 3D reconstruction datasets like NeuralDome~\cite{zhang2023neuraldome}. 
We also evaluate human-human interactions on scenes of the ReMoCap~\cite{ghosh2024remos} dataset. 
These scenes involve practitioners performing martial arts poses, thereby leading to challenging interactions. 
The proposed approach performs better than previous state-of-the-art methods such as ObjectSDF++~\cite{Wu2023objectsdfplus} and NeuS2~\cite{neus2}. 
Although not the main objective of this work, we also observe better performance on the related task of segmented novel-view synthesis \cite{mildenhall2020nerf, mueller2022instant}. 
