\section{Background}
Our method is based on an implicit surface representation and uses volumetric rendering for image supervision.
It, therefore, builds on top of existing surface representation and volume rendering methods like NeuS~\cite{wang2021neus} 
and Instant-NGP~\cite{mueller2022instant}. 
We briefly discuss them below. 
\subsection{Neural Implicit Surfaces} 
NeuS \cite{wang2021neus} 
is an implicit multi-view reconstruction method that extends NeRF \cite{mildenhall2020nerf} by representing the surface and appearance of a scene as a Signed Distance Function (SDF),  $\Phi(\mathbf{x}): \mathbb{R}^3 \rightarrow \mathbb{R}$ and a radiance field $c(\mathbf{x}, \mathbf{v}): \mathbb{R}^5 \rightarrow \mathbb{R}^3$,  respectively. 
The surface $\mathcal{S}$ is defined by the zero level set of the SDF, $\mathcal{S} = \{ \mathbf{x} \in \mathcal{R}^3 | \Phi(\mathbf{x}) = 0 \}$ and the pixel colour is obtained by volumetrically rendering the colours $\hat{C}$ along the ray,  $\mathbf{p} = \mathbf{o} + i \cdot \mathbf{v}$, shot from the camera's origin $\mathbf{o}$ in the direction $\mathbf{v}$, through pixel $p$ using the rendering equation: 
\begin{equation} \label{eq:vol_render}
    \mathbf{\hat{C}}(p) = \sum_i^{N} T_i \alpha_i c_i, 
\end{equation}
where $T_i$ is the accumulated transmittance, $\alpha_i$ is the opacity and $c_i$ is the colour of $i^{th}$ sample along the ray. 
$\alpha_i$ and 
$T_i$ are obtained directly from 
$\Phi$ 
using 
\begin{equation}\label{eq:neus_opacity}
    \alpha_i = \max\left(\frac{\sigma(\Phi_i) - \sigma(\Phi_{i+1})}{\sigma(\Phi_i))}, 0 \right), 
\end{equation}
where $\sigma(\Phi_i) = (1 + e^{-\beta \Phi_i})^{-1}$ is the sigmoid function and $\beta$ is a learnable parameter.

We extend this formulation 
for two SDFs and, thereby, opacities of two interacting objects (see \cref{sec:Method}). 

\subsection{Hashgrid Encoding}
Training NeuS in the originally proposed fashion is slow and requires hours. 
Recent works~\cite{mueller2022instant, zhao2022instantnsr, neus2, rosu2023permutosdf} accelerate it using 
multi-resolution hash grid encoding of 3D points. 
MÃ¼ller \textit{et al.}~\cite{mueller2022instant} as first proposed a multi-resolution voxel grid such that the grids of different resolutions are represented by a hash table that maps a 3D point $\mathbf{x}$ to a learnable feature vector $h_l(\mathbf{x})$, with $l$ being the resolution level. 
All the feature vectors are concatenated to obtain the hash-encoded feature as $h(\mathbf{x}) = \{h_1(\mathbf{x}), h_2(\mathbf{x}), \ldots, h_L(\mathbf{x})\}$, where $L$ is the number of resolution levels. 
This representation (along with the CUDA implementation), speeds up the training substantially by three orders of magnitude and has been used to accelerate several surface reconstruction methods \cite{neus2, zhao2022instantnsr}.
We also adopt it in our method. 


\section{Method} 
\label{sec:Method} 
Our goal is to separately recover the 3D geometry and the appearance of each object in the scene from multiple RGB views. 
With close interactions (leading to severe mutual occlusions), the key challenge is to recover a clean surface boundary while preventing inter-penetrations. 
We address the problem with a new neural approach illustrated in \cref{fig:joint_sdf}.
The scene observed from multiple views is first encoded using a shared multi-resolution hash grid. 
In the second step, the scene features are decoded as two SDFs using two MLP heads, one for each interacting object (\cref{sec:Joint_SDF}). 
Next, the individual opacity of each object, the overall scene opacity and the scene colour for each point in the ray are forwarded to the volume renderer that renders the images. 
The separation boundaries between different objects are obtained using ray colour integration with $\alpha$-blending (\cref{sec:losses}). 
We next discuss each step in detail. 
\subsection{Scene Representation}
\label{sec:Joint_SDF}

We are given a set of $K$ calibrated multi-view images $\mathcal{I} = \{I_i\}_{i=1}^K$ capturing two interacting objects (subscripted with $1$ and $2$) along with their corresponding segmentation masks $\mathcal{M}_1 = \{M_{1}^{1}, M_{1}^{2}, \hdots,$ $ M_{1}^{K}\}$ and $\mathcal{M}_2 = \{M_{2}^{1}, M_{2}^{2} \hdots, M_{2}^{K}\}$.
One can recover the foreground mask as their union: $\mathcal{M} = \mathcal{M}_1 \cup \mathcal{M}_2$. 
A na\"ive way to perform 3D reconstruction would be to use the set of masks corresponding to each object in isolation in an attempt to recover the corresponding surfaces using a multi-view reconstruction method such as NeuS~\cite{wang2021neus} or VolSDF \cite{yariv2021volume}. 
Unfortunately, such a solution is sub-optimal as it does not jointly account for all the densities, resulting in large gaps due to occlusion and poor separation boundary as shown in \cref{fig:naive_sdf} and \cref{table:human_object_geometric}. 
\par
Hence, our approach uses a \textit{shared} hash-encoding for both objects in the scene. 
For any 3D point $\mathbf{x}$---encoded using a \textit{shared} multi-resolution hashgrid as $\mathbf{e} = (\mathbf{x}, h(\mathbf{x}))$ 
---we estimate the signed distance to the two objects in the scene using two separate SDFs, $\Phi_1$ and $\Phi_2$, respectively, parameterised using two separate MLP heads. 
As shown in~\cref{fig:joint_sdf}, the feature extraction network provides hashgrid features $\mathbf{g} \in \mathbb{R}^{d_g}$ for each encoded position $\mathbf{e}$.
These features, along with the hashgrid encodings are the input to the SDF MLPs that produce $\Phi_1(\mathbf{e}, \mathbf{g})$ and $\Phi_2(\mathbf{e}, \mathbf{g})$.
The SDF of the joint scene, $\Phi_s(\mathbf{e}, \mathbf{g})$, can now be composed using:
\begin{equation}
\label{eq:union_sdf}
    \Phi_s(\mathbf{e}, \mathbf{g}) = \min(\Phi_1(\mathbf{e}, \mathbf{g}), \Phi_2(\mathbf{e}, \mathbf{g})). 
\end{equation}
\par
We also recover the colour of the scene, $\mathcal{C}_s(\mathbf{x}, \mathbf{v}, \mathbf{n}, \Phi_s, \mathbf{g}): \mathbb{R}^{38} \rightarrow \mathbb{R}^3$, which can be encoded as an MLP and conditioned on the position $\mathbf{x}\in \mathbb{R}^{3}$, spherical-harmonic encoded view direction $\mathbf{v}\in \mathbb{R}^{16}$, the surface normal $\mathbf{n}\in \mathbb{R}^{3}$, SDF value $\Phi_s\in \mathbb{R}$ and the hashgrid features $\mathbf{g} \in \mathbb{R}^{15}$. 
For brevity, we denote this colour MLP as $\mathcal{C}_s(\mathbf{x}, \mathbf{v})$ in future sections. 
Using the scene SDF, $\Phi_s$, and the colour values, $\mathcal{C}_s$, one can apply the volume rendering proposed in~\cite{wang2021neus} to render the scene for each camera pose and each object separately, which is
visualised in our supplementary video. 
\subsection{Interaction-aware Training} 
\label{sec:losses}
Given the per-object segmentation masks, we optimise the scene parameters defined above using the following loss formulation. 
Let $\mathbf{C}$ represent the segmented foreground ground-truth colour, while 
$\mathbf{C}_1 = \mathbf{C} \circ \mathcal{M}_1$ and $\mathbf{C}_2 = \mathbf{C} \circ \mathcal{M}_2$ be the segmented objects' ground-truth colours, where ``$\circ$'' indicates the Hadamard product. 
The rendering loss function $\mathcal{L}_{\text{color}}$ is then defined as: 
\begin{equation} \label{eq:partial_color_loss}
    \mathcal{\hat{L}}_{\text{color}} = \sum_p|\mathbf{\hat{C}}_1(\mathbf{p}) - \mathbf{C}_1(\mathbf{p})|_s + \sum_p |\mathbf{\hat{C}}_2(\mathbf{p}) - \mathbf{C}_2(\mathbf{p})|_s, 
\end{equation}
where $|\cdot|_s$ denotes the  Smooth-$\operatorname{L1}$ loss. 
\par
\noindent \textbf{Training Stabilisation:}
In practice, since the individual objects can be relatively smaller than the overall scene scale, using only these segmented colours for SDF supervision makes the training unstable, especially in the beginning.
To stabilise the training, especially in the earlier stages, we use the entire scene colour for supervision as well, with predicted scene colour calculated using~\cref{eq:vol_render}.
The modified final loss now reads as:
\begin{align} 
    \mathcal{L}_{\text{color}} = \hat{\mathcal{L}}_{\text{color}} + \sum_p |\mathbf{\hat{C}}_s(\mathbf{p}) - \mathbf{C}(\mathbf{p})|_s. 
    \label{eq:color_loss} 
\end{align} 
\par 
While the estimated scene colour $\mathcal{C}_s(\mathbf{x}, \mathbf{v})$ can be directly supervised with the RGB colour loss, it is insufficient to enforce that the learned object and the human SDFs are \textit{separate}. 
Hence, the next question is how to ensure that the colour loss leads to separation between the two SDFs without arbitrarily entangling the two geometries.
Towards this goal, we introduce an $\alpha$-blending colour loss and a regularisation term that constrains the opacities of the two fields. 
Recall that we construct the scene SDF $\Phi_{s}(\mathbf{e})$ as a union of the individual object SDFs, $\Phi_1(\mathbf{e})$ and $\Phi_2(\mathbf{e})$, as in~\cref{eq:union_sdf}. 
We can, therefore, recover the opacity of the individual objects, $\alpha_1^i$ and $\alpha_2^i$, at position $i$ using the respective SDFs (as in ~\cref{eq:neus_opacity}). 
Now, to recover the joint scene opacity $\alpha_s^i$, we $\alpha$-composite the opacity contributions from both $\alpha_1^i$ and $\alpha_2^i$: 
\begin{equation} \label{eq:alpha_compositing}
    \alpha^{i}_{s} = \alpha^{i}_{1} + \alpha^{i}_{2} - \alpha^{i}_{1} \alpha^{i}_{2}. 
\end{equation}
After substituting $\alpha^{i}_{s}$ from~\cref{eq:alpha_compositing} in the rendering equation \eqref{eq:vol_render}, we obtain: 
\begin{align}
\mathbf{\hat{C}_{s}}(\mathbf{p}) &= \sum_{i=1}^{N} T^{i}_{s} \left( \alpha^{i}_{1} + \alpha^{i}_{2} - \alpha^{i}_{1} \alpha^{i}_{2} \right) c^{i}_{s}  \nonumber \\
&= \sum_{i=1}^{N} T^{i}_{s} \alpha^{i}_{1} c^{i}_{s} + T^{i}_{s} \alpha^{i}_{2} c^{i}_{s} -  T^{i}_{s} \alpha^{i}_{1} \alpha^{i}_{2} c^{i}_{s}. 
\label{eq:color_compositing}
\end{align}
Here, the first two terms,  $\mathbf{\hat{C}}_1(\mathbf{p}) = \sum_{i=1}^{N} T^{i}_{s} \alpha^{i}_{1} c^{i}_{s}$ and $\mathbf{\hat{C}}_2(\mathbf{p}) = \sum_{i=1}^{N} T^{i}_{s} \alpha^{i}_{2} c^{i}_{s}$, represent the visible part of the two objects. 
Note, however, that the transmittance $T_s^i$ and colour $c_s^i$ terms correspond to the entire scene, and $T_s^i$ reaches close to 0, when obstructed by either of the objects, thus ensuring that the final colour output is occlusion-aware.
\par 
\textbf{Alpha-Blending Regularisation.}
To achieve separable reconstruction, we assume that all the objects in a scene are opaque.
Therefore, at any point, at least one of the two opacities, ($\alpha_1^i, \alpha_2^i$), should be $0$, such that.~$\alpha_1^i \alpha_2^i = 0$ in~\cref{eq:color_compositing}. 
This observation is key to ensuring clean separation boundaries between the different objects.
However, we cannot \textit{explicitly} enforce this constraint as there is no way to know which of the two opacities should be $0$.
Thus, we introduce the following $\alpha$-regularisation which ensures that each position is opaque due to the influence of only one of the two SDFs, thereby preventing SDF penetration: 
\begin{equation}
\label{eq:alpha_reg}
    \mathcal{L}_{\text{alpha}} = \sum_p \left(\exp\bigg(\frac{\beta}{\lambda_t} \cdot \alpha_1(\mathbf{p}) \cdot \alpha_2(\mathbf{p})\bigg) - 1\right), 
\end{equation}
where $\beta$ is the learnable parameter from~\cref{eq:neus_opacity}, $\lambda_t$ is a hyperparameter controlling the temperature of the exponential curve and $\alpha_1, \alpha_2 \geq 0$. 
Here, $\beta$ increases as the training converges, thereby regularising more for overlapping opacities at the later stages of training. 
We empirically find that the above-proposed $\alpha$-regularisation performs the best and show an ablation in \cref{table:ablations}. 
Finally, we employ the commonly used Eikonal regularisation term 
to obtain the correct SDFs: 
\begin{align} \label{eq:eikonal_reg}
    \mathcal{L}_{\text{eik}} = &\sum_\mathbf{x} \norm{ \nabla_\mathbf{x} \Phi_1(\mathbf{e}, \mathbf{g}) - 1 }^2 + \norm{\nabla_\mathbf{x} \Phi_2(\mathbf{e}, \mathbf{g}) - 1}^2 + \nonumber \\ 
     & + \norm{\nabla_\mathbf{x} \Phi_s(\mathbf{e}, \mathbf{g}) - 1}^2. 
\end{align}
The resulting total loss can now be written as:
\begin{equation} \label{eq:total_loss}
    \mathcal{L}_{\text{recon}} = \mathcal{L}_{\text{color}} + \lambda_\alpha \mathcal{L}_{\text{alpha}} + \lambda_{\text{eik}} \mathcal{L}_{\text{eik}}. 
\end{equation}
We use $\lambda_{\alpha} = 0.1$, $\lambda_{\text{eik}} = 0.01$ and $\lambda_t = 100.0$ as hyperparameters in all our experiments. 
\par

\noindent \textbf{Opacity \textit{vs.}~direct SDF regularisation.} 
While we 
ensure separability by regularising the per-object opacities, another possible approach would be to 
regularise at the SDF level: 
Specifically, one could enforce both SDFs $\Phi_1$ and $\Phi_2$ to be not negative at the same point. 
We empirically observe that the proposed $\alpha$-regularisation performs best and show the corresponding ablation in~\cref{table:ablations}. 
