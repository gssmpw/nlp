\section{Related Works}
\label{sec:related_works}

We discuss the related works from three perspectives: 
(1) neural scene representations, (3) implicit models for multi-object segmentation and reconstruction and, finally, (3) generic human-object, hand-object and human-human interaction works. 

\subsection{Neural Scene Representations}

Recent advances in neural implicit representations and NeRF-based techniques \cite{mildenhall2020nerf, Tewari2022NeuRendSTAR} have enabled high-quality novel view synthesis and reconstruction of complex scenes from multi-view images. 
Extensions of those for surface reconstruction by \cite{wang2021neus,yariv2021volume,Oechsle2021ICCV_unisurf} and enhancements in speed by works like \cite{mueller2022instant, neus2, rosu2023permutosdf} have shown that it is possible to reconstruct high-quality geometry, in a reasonable amount of time, such that it can be applied to even short videos on per-frame basis.
They have also been 
applied for human rendering \cite{peng2021neuralbody,liu2021neuralactor,weng2022humannerf,zhao2022instantnsr, sun2024metacap}, that extend to dynamic scenes as well as provide pose-conditioned animation capabilities. 
Some works also extend it to multi-person scenarios~\cite{shuai2022multinb,zhang2021stnerf,Menapace2024ToG}.
The most relevant work to ours  \cite{zhang2023neuraldome, sun2021hoifvv} also uses implicit representation for the reconstruction of human-object interaction. 
They both use an SMPL prior \cite{SMPL:2015, SMPL-X:2019} for the human body and an object template for a layer-wise representation. 
HOI-FVV~\cite{sun2021hoifvv} uses sparse-view RGB input to predict occupancy values of the human undergoing interaction with objects.
However, the sparse inputs limit the reconstruction quality, and the object geometries have to be tracked assuming a template is available. 
On the other hand, Zhang et al.~\cite{zhang2023neuraldome} use dense RGB inputs, and obtain separate NeRFs for both human and object, using SMPL and an object template as priors, which are then blended 
to obtain the final reconstruction. 
In contrast, we implicitly learn to separate the two objects by using only the image segmentation masks as additional supervision. 
\par
While similar to Neus2~\cite{neus2} in employing hashgrid encoding with NeuS, we do not adopt their approximate second-derivative formulation. Instead of the coarse-to-fine training strategy, which caused missing reconstructions for small objects, we optimise all hashgrid levels from the start. Additionally, we condition the separate SDF MLP heads on hashgrid feature vectors derived via another MLP.


\subsection{Multi-Object Segmentation/Reconstruction}
While most methods focus on entire scene reconstruction, some methods~\cite{Zhi2021semanticnerf, wu2022object, Wu2023objectsdfplus} focus on individual objects in the scene. 
Semantic NeRF \cite{Zhi2021semanticnerf} uses a semantic head to predict labels for each position, supervised by the segmentation mask.
A similar idea is also used in \cite{huang2022hhor}, where the method predicts a label for each position and then uses it to separate the SDF of the human and the object. 
However, this approach---while recovering correct labels at the surface---produces incorrect labels \textit{within} the surface, making it feasible only for minimal occlusion and contact. 
The approach closest to ours is  ObjectSDF++~\cite{Wu2023objectsdfplus}, which predicts different SDFs for each object in the scene. 
Whereas they supervise the SDF separation using only opacity, we use the separately rendered colour of the two objects for supervision. 
Importantly, even though ObjectSDF++ proposes an extra SDF distinction regularisation term, it does not guarantee non-penetrating geometries. 
In contrast, we use an opacity regularisation term that incentivises the two SDFs to have disjoint opacities, thereby resulting in no (in most cases) or minimal interpenetration. 
Please refer to ~\cref{sec:Joint_SDF} for more details. 

\paragraph{Human-Object Interaction}


\begin{figure*}
    \centering
    \hspace*{10pt}\includegraphics[width=1.01\linewidth]{figures/others/method.pdf}
    \caption{\textbf{Schematic overview of our framework.} 
    We semantically segment the input multi-view images into the background and the areas corresponding to two interacting objects. 
    The scene is encoded using a shared, multi-resolution hash grid encoding $\mathbf{e}$ and 
    the shared features are decoded using two separate SDF MLPs to produce corresponding SDFs $\Phi_1$ and $\Phi_2$.
    The per-point colour $\mathcal{C}_s$ is estimated from the joint scene SDF  composed using $\Phi_s = \Phi_1 \cup \Phi_2$.
    Finally, we integrate the colours of the sampled points in the ray by $\alpha$-blending  the individual opacities, $\alpha_1$ and $\alpha_2$, ensuring clean separation boundaries between the two (see \cref{eq:color_compositing}). 
    The entire framework is supervised using the rendering loss and additional regularisers (see~\cref{eq:total_loss}).
    }
    \label{fig:joint_sdf}
    \vspace{-1em}
\end{figure*}
A widely arising scenario is human-object interaction, which our method can also handle since it is applicable to arbitrary objects. 
Human-object interaction has been extensively studied in the literature. 
While some previous works \cite{kwon2021h2o,hampali2020honnotate,Brahmbhatt2020contactpose,chao2021dexycb,Freihand2019,FirstPersonAction_CVPR2018,DecafTOG2023} focus only on hands interacting with objects, 
many of the recent works \cite{bhatnagar22behave,GRAB:2020,jiang2022fullbody,xie2022chore,fan2023arctic,huang2022intercap,zhang2020phosa,Li_3DV2022,zhang2023neuraldome,jiang2022neuralhofusion,tretschk2024scenerflow} consider whole body interacting with the object.
In many scenarios of humans represented by entire bodies interacting with objects, the latter are often substantially smaller than humans. 
Methods falling into this category can be broadly divided into two groups: Methods based on template fitting or multi-view reconstruction methods. 
For template fitting, most methods, leverage SMPL or SMPL-X \cite{SMPL:2015,SMPL-X:2019}, 
along with a pre-acquired template of the object to fit marker-based motion capture data \cite{GRAB:2020,fan2023arctic}
or (sparse) multi-view RGB-D data \cite{bhatnagar22behave, huang2022intercap}. 
Some methods even attempt to fit templates to a single RGB image \cite{zhang2020phosa,xie2022chore}.
On the other hand, multi-view reconstruction methods, most relevant to our work, 
can recover accurate geometry and high-quality textures using multi-view RGB(D) images. 
Similarly to our method, NeuralDome~\cite{zhang2023neuraldome} and Neural-HOFusion~\cite{jiang2022neuralhofusion} use segmentation masks to separate humans and objects from multi-view images. 
However, in contrast to our work, both use a layer-wise NeRF representation that reconstructs humans and objects in isolation and then fuses them to recover final outputs. 
In contrast, our formulation uses a unified density and color field for the whole scene, but decodes separate geometries (as Signed Distance Fields) for the human and object. 
NeuralDome \cite{zhang2023neuraldome} uses SMPL-X and object template as prior for tracking. 
NeuralHOFusion \cite{jiang2022neuralhofusion} also uses object templates for better reconstruction. 
Note that our method does not require any 3D object templates free per default (the supplement discusses the case when one is available). 
