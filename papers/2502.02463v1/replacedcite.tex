\section{Related Work}
Bayesian inference aims to determine the posterior distribution $p(x \mathop{|} z)$ of a latent variable $x$ given observations $z$, using Bayes' theorem: $p(x \mathop{|} z)=\frac{p(z \mathop{|} x)p(x)}{p(z)}$. While closed-form posteriors exist for specific conjugate prior scenarios, the general case involves an intractable evidence integral $p(z) = \int p(z \mathop{|} x)p(x)dx$ (along with others such as statistics of the predictive posterior distribution (PPD), for example), necessitating approximate inference methods.

Variational inference (VI) aims to approximate the posterior density through fitting a density of a particular family (typically a Gaussian) to the posterior by maximizing a variational lower bound on the evidence. Notable modern extensions include stochastic variational inference (SVI) ____, which utilizes backpropagation to optimize a stochastic approximation of the variational objective, black-box variational inference ____, which provides widely-applicable, model-agnostic tools for optimization of this variational objective, and amortized variational inference (AVI) ____, which efficiently solves many variational inference problems in parallel through the learning of an inference network mapping from observations to approximate posteriors.  These methods often provide acceptable estimates for the posterior density, but necessarily involve an expensive optimization routine at inference time, are not asymptotically exact, and often underestimate the posterior variance. Nonetheless, VI remains the most popular approximate inference technique in use today, and we will use a variant as a baseline in our empirical studies.

Recent neural approaches to approximate inference leverage the expressivity of deep learning architectures while aiming to maintain theoretical guarantees. Prior Fitted Networks (PFNs) ____ learn to perform approximate inference through sample-based supervised learning without the need for posterior densities (which are instead accessed via samples from the joint distribution over $x$ and $z$), enabling inference in a single forward pass. They provide rapid inference for changes in dataset, provide a posterior density estimate, and allow for almost arbitrary priors, but require complete retraining upon changes to the prior distribution, and use a Riemann Distribution, which can struggle to model smooth, heavy-tailed posteriors. As a close competitor to our method, we will also adopt a PFN baseline for our posterior studies. 

Continuous normalizing flows ____ learn a mapping from samples of some simple distribution to samples of the target posterior density, leveraging automatic differentiation to provide a flexible, closed-form density using a change of measure. However, the mapping they learn is fixed, meaning the prior is set at training time, and observations can only be incorporated via sufficient statistics of the simple distribution, limiting their suitability for our problem setting. Moreover, they must be constructed using only fully reversible components, which can limit expressivity. Classical approaches include the Laplace approximation ____ and expectation propagation (EP) ____. The Laplace approximation fits a Gaussian distribution based on the local geometry at the posterior mode, but lacks invariance to reparametrization. EP iteratively refines an isotropic Gaussian approximation to the posterior, but struggles with numerical stability.

Sampling-based methods, most notably Markov chain Monte Carlo (MCMC) methods, provide samples from the posterior which can be used to compute stochastic estimates for expectations with respect to said posterior. These are primarily based on access to the unnormalized posterior via the numerator of Bayes' theorem. When a differentiable prior and likelihood model are available, the state-of-the-art No U-Turn Sampler (NUTS) ____ can be used. NUTS is based on Hamiltonian Monte Carlo (HMC) ____, which provides more efficient generation of posterior samples without problem-specific tuning of the algorithm. MCMC methods are asymptotically exact and almost universally applicable, but are typically expensive due to random walk behavior. They also do not provide direct access to a posterior density, and empirical density estimates can be  sample inefficient.

Bayesian quadrature-based methods ____ assign a probabilistic model, typically a GP, to the integrand of the evidence $p(z)$ (and in some cases the predictive posterior distribution PPD $p(y|x,z)=\int p(y|x)p(x \mathop{|} z)$), and use probabilistic numerical integration to estimate the evidence and PPD directly. These methods provide accurate uncertainty measures, and can handle noisy, black-box likelihood models, but are typically computationally expensive and scale poorly to high-dimensional problems, although recent advances in sparse-GPs ____ provide promising avenues to alleviate these scaling issues.

\subsubsection{Sequential Inference}
Sequential inference over time series requires dedicated methods, referred to as Bayesian filtering (when causality is required) or smoothing (when not). The extended Kalman filter (EKF) linearizes dynamics and observation models, then approximates the distribution over the latent state as a closed-form Gaussian. With modern automatic differentiation, this method is widely applicable and computationally efficient, but struggles in practice due to the overly restrictive assumptions induced by linearization and Gaussian distributions. Particle filters (PFs), or sequential Monte Carlo methods ____, maintain a population of particles which are distributed according to the posterior distribution each time step, but can be expensive and only provide an empirical density. The unscented Kalman filter ____ maintains a minimal population of particles so is computationally efficient, but faces issues of numerical stability and also encodes a Gaussian assumption thus cannot handle multimodality. We compare our method against an EKF and a PF baseline.