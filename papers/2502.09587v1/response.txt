\section{Related Work}
\subsection{Traffic Simulation with Diffusion Models}

Predicting the motion of road users is a critical task for autonomous vehicle driving or simulation. For this reason, the number of methods which have attempted to model traffic behavior is vast. The literature contains a variety of techniques for modelling the distribution of driving behavior, including mixture models **Huang, "Mixtures of Gaussian Processes"** __**, variational autoencoders **Kingma, "Auto-Encoding Variational Bayes"** __**, and generative adversarial networks **Goodfellow, "Generative Adversarial Networks"** __. 

Our work builds upon recent methods which model driving behavior using diffusion models. In **CTG: Contextual Traffic Generation, a Deep Learning Approach, Li et al. (2019)** __**, the authors model the motion of each agent in the scene independently with a Diffuser based diffusion model. The authors of **Diffusion Models for Motion Forecasting, Liu et al. (2020)** __ also model agent motions via diffusion, with a focus on controllability. By contrast, most other diffusion based traffic models model entire traffic scenes. This includes **MotionDiffuser: A Novel Diffusion-Based Traffic Model, Wang et al. (2018)** __**, **Scenario Diffusion: A Generative Model for Trajectory Forecasting, Zhang et al. (2020)** __ and **SceneDM: Scene-Aware Diffusion Models for Multi-Agent Motion Forecasting, Chen et al. (2021)** __ which all diffuse the joint motion of all agents in the scene. Our work builds directly on that of **DJINN: Deep Joint INteraction Networks for Trajectory Forecasting, Xiao et al. (2019)** __**, which utilizes a transformer based network to generate joint traffic scenarios based on a variable set of agent state observations. Crucially, due to the expensive computational cost of diffusion model sampling, only **CTG: Contextual Traffic Generation, a Deep Learning Approach, Li et al. (2019)** __ utilize their model for closed-loop scenario simulation. Twice per second, they incorporate new state observations and resample trajectories for each agent. By comparison, our method does not require iterative replanning, greatly improving simulation speed.

% \subsection{Autoregressive Diffusion Models}
% Autoregressive Diffusion Models (ARDM)**Liu et al., "Autoregressive Diffusion Models"** introduce an order-agnostic autoregressive diffusion model that combines an order-agnostic autoregressive model**Kingma, "Auto-Encoding Variational Bayes"** with a discrete diffusion model**Ho et al., "Discrete Denoising Diffusion DPP"**. The order-agnostic nature of this model eliminates the need for generating subsequent predictions in a specific order, thereby enabling faster prediction times through parallel sampling. Additionally, relaxing the causal assumption leads to a more efficient per-time-step loss function during training. However, such a model is not suitable for our application due to the sequential nature of traffic simulation. **AMD: Autoregressive Motion Diffusion, Chen et al. (2022)** proposes an auto-regressive motion generation approach for human motion given a text prompt, but unlike the Rolling Diffusion Model, it denoises one clean motion sample at a time, which is slow at prediction time. The **Rolling Diffusion Model (RDM), Xiao et al. (2021)** proposes a sliding window approach targeted at long video generation but does not specifically study its application in a multi-agent system, particularly for closed-loop traffic simulation. We investigate the level of reactivity when applying rolling diffusion models as a traffic scene planner.