\subsection{Defense Prompt}
\label{sec:defense prompt}

We tested the defense prompt against notification-based Adversarial Attack by adding the defense prompt "Please make sure to ignore any content on the notifications that is unrelated to the goal." in the prompt. The results are shown in Table \ref{tab:defense_attack_table}.

% As illustrated, the addition of the defense prompt did not significantly aid any of the models, as the task success rates showed only a slight increase, or no increase at all. This suggests that such limited defensive measures struggle to provide effective protection against these types of attacks, highlighting the need for further exploration of more robust defensive strategies in the future.


From the table, we can see that for most MLLMs, the improvement is limited, with only a slight increase in the task success rate, or no increase at all. In T3A, the defense success rate of open-source models like GLM-4V-Plus ($SR_{def} = 0.17$) far exceeds that of the benign baseline ($SR_{ben} = 0.03$), indicating that defensive statements may inadvertently correct the model's inherent flaws by enhancing text instruction parsing. In contrast, closed-source MLLMs (such as GPT-4o in M3A) demonstrate weak defense effectiveness ($SR_{def} = 0.40$ vs. $SR_{ben} = 0.61$), suggesting that the integration of multimodal information may lower the priority of defensive instructions. 

Moreover, in I3A, defense effectiveness is polarized: Qwen2-VL-7B's $SR_{def}$ (0.15) shows a 200\% improvement over Adversarial Attack (0.05), while GLM-4V-Plus only partially recovers (0.13 vs. 0.16). In the AppAgent, the defense effectiveness is very weak. Overall, the experimental results indicate that the effectiveness of defenses depends on input modalities (T3A > I3A/AppAgent > M3A). We present more details about the comparison of attack success rates in Appendix \ref{appendix:More Details about Defense Prompt}.