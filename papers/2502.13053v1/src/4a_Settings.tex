\subsection{Settings}
\label{sec:evaluation_settings}

We structured the experiment setup around three components: benchmarks, models, and agents. The specific details of the settings are presented below.

\textbf{Benchmarks.} We evaluate the performance of the agents provided in the easy subset of the AndroidWorld benchmark \cite{rawles2024androidworld}, consisting of 61 tasks. Additionally, for the AppAgent, we utilize its own evaluation benchmark, which includes 45 popular application tasks. 

\textbf{Models.} We employed five advanced MLLMs for testing, including the closed-source models GPT-4o-2024-08-06 \citep{hurst2024gpt}, Qwen-VL-Max \citep{bai2023qwen} and GLM-4V-Plus \citep{hong2024cogvlm2}, as well as the open-source models Qwen2-VL-7B \citep{wang2024qwen2} and Llava-OneVision-7B \citep{li2024llava}.

\textbf{Agents.} We conduct experiments using agents provided by AndroidWorld \cite{rawles2024androidworld}, including mobile agents such as M3A, T3A, and a custom agent, I3A. Additionally, we tested the default configuration of AppAgent \citep{zhang2023appagentmultimodalagentssmartphone}, a mobile agent designed for user-defined tasks. The differences in the input data for each agent are presented in Table \ref{tab:agent_type}. The input data is categorized into two types: image data and element data. These inputs vary across different test benchmarks. Further details about these two types of input data can be found in Appendix \ref{appendix:details about input data}.

