\appendix
\section{Case Study}
\label{appendix:case study}
We present the reasoning details under two attack strategies in Figure \ref{fig:reasoning details}, both of which can be actively initiated by an adversary. This case study was conducted on M3A using GPT4o-2024-08-06 within the AndroidWorld benchmark, allowing us to thoroughly evaluate the impact of these attacks on multimodal large language models (MLLMs).

In the third step of the agent's task, we introduced an Adversarial Attack. By comparing the reasoning details of benign samples with those affected by the attack, we observed that the MLLMs were influenced by adversarial content present in the notifications. This led to a deviation from the intended reasoning process, causing the agent to make incorrect decisions and prematurely terminate the task. The attack's ability to inject misleading information into the agent's environment disrupted the flow of decision-making, showing how external disturbances could distort an agent's perception and execution of tasks.

In the sixth step, we implemented a Reasoning Gap Attack. At this point, the agent was required to click the save button to proceed with the task. By exploiting the reasoning gap, we sent a timely notification during the critical moment when the agent was supposed to perform the action. While the reasoning content itself remained unchanged between the benign and adversarial samples, the key difference was the alteration of the device state triggered by the notification. This manipulation caused the agent to misclick, inadvertently opening the SMS Messenger app. As a result, the agent entered a non-target state, deviating from the task’s intended flow. This illustrates how reasoning gaps, when exploited at the right time, can lead to system failures or misbehaviors that are difficult to predict or defend against. The results underscore the vulnerability of MLLMs to external manipulations and emphasize the need for improved defenses against such targeted attacks in real-world environments.

\section{Notification Design}
\label{appendix:Message Notification Design}
Mobile notifications have unique characteristics: they can appear on top of any application and often occupy a significant portion of the screen, making them effective in disrupting the agent's workflow. While mobile notifications are visible to human operators, the agent's ultimate goal is to complete task objectives autonomously without human intervention. Therefore, this type of attack behavior warrants in-depth investigation.

Message notifications on mobile devices are implemented in two main ways: app-based notifications and SMS-based notifications. App-based notifications lack a unified design standard and typically require users to download the app in advance. In the current era of advanced mobile security, malicious code in apps is easily detected by devices, limiting the applicability of this approach in many scenarios.  In contrast, SMS-based notifications offer better compatibility on Android devices and usually require no special configuration. This makes SMS a potential vector for malicious attacks. Given these considerations, we chose to design message notifications based on SMS. In all the elements of SMS, we will use the text content of message notifications to implement adversarial attacks. In addition, the accessibility (a11y) tree of message notifications also reflects the adversarial content. 

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/Notifiaction Design.pdf}
%     \caption{Adversarial Attack.}
%     \label{fig:Adversarial_Attack}
% \end{figure}

\section{Details about Input Data}
\label{appendix:details about input data}
There are certain differences in the data input to the Agent across different benchmarks. For AndroidWorld agents, image data includes both the current screenshot and Set-of-Mark (SoM) annotated screenshots \cite{yang2023setofmarkpromptingunleashesextraordinary}, where each UI element in the SoM screenshot is enclosed in a bounding box with a numeric label. For AppAgent, image data consists only of the SoM annotated screenshot. Additionally, element data refers to the text representation of the accessibility tree (a11y tree), which is a hierarchical structure outlining the web elements and their associated accessibility properties within a web page or application. The a11y tree is critical for understanding how screen readers and other assistive technologies interpret and interact with the content of a page, offering insights into the relationships between various elements and their accessibility attributes.

\section{More Details about Adversarial Attack}
\label{appendix:Details about Adversarial Attack}
In Figure \ref{fig:growth_rate_of_task}, we presented the growth rate of the number of tasks that ended prematurely among failed tasks after Adversarial Attack. From the figure, it is evident that GPT-4o-2024-08-06 is significantly affected by adversarial text, with the growth rate of prematurely ended failed tasks nearly doubling in the M3A context. This indicates that most of the failed tasks in GPT-4o-2024-08-06 were caused by the influence of adversarial text, leading to their premature termination. Most other models maintained a growth rate of at least 25\%. We believe that Qwen-VL-Max exhibits strong resistance to such Adversarial Attack, as it can identify this type of adversarial content in the M3A context, resulting in a negative growth rate. A negative growth rate was also observed in GLM-4V-Plus, indicating that it is difficult to interfere with GLM-4V-Plus using text alone.

% In addition, we also presented the proportion of tasks in adversarial samples where the number of steps is less than the number of steps for the same tasks in benign samples, as shown in Figure \ref{fig:growth_rate_of_step_less_benign}.




% \begin{figure*}[t]
%     \centering
%     \begin{minipage}[b]{0.32\textwidth}  % 每张图片占页面宽度的 30%
%         \centering
%         \includegraphics[width=\textwidth]{figures/i3a_intersect_Pvalue.pdf} % 图片路径
%         \subcaption{I3A Agent}  % 图片说明
%     \end{minipage}
%     \begin{minipage}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/m3a_intersect_Pvalue.pdf} % 图片路径
%         \subcaption{M3A Agent}
%     \end{minipage}
%     \begin{minipage}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/t3a_intersect_Pvalue.pdf} % 图片路径
%         \subcaption{T3A Agent}
%     \end{minipage}
%     \caption{The proportion of tasks in intersection tasks where adversarial sample tasks end earlier than benign sample tasks.}  % 总的标题
%     \label{fig:growth_rate_of_step_less_benign}
% \end{figure*}


\section{More Details about Defense Prompt}
\label{appendix:More Details about Defense Prompt}

% We present the attack success rates after using defense prompts in Table \ref{tab:defense_attack_ASR_table}. It can be observed that after the application of defense prompts, only a few MLLMs experienced a decrease in attack success rates, indicating that this defense design struggles to effectively counter the attack strategies proposed in this paper. Conversely, more models experienced an increase in attack success rates, suggesting that these defense statements may indirectly affect the judgment of MLLMs, resulting in highly unstable defense outcomes.


% The differences in response to defense strategies (defense prompts) among different models are significant. In closed-source models, GPT-4o shows outstanding defense performance in M3A and AppAgent scenarios, with ASR\_def reducing by 17\% and 31\% compared to ASR\_adv, respectively. However, in the case of I3A, the attack success rate increases by 4\% after defense, indicating that the defense strategy is insufficiently adaptable to single modalities. Qwen-vl-max exhibits weak defense performance with T3A, showing only a 2\% decrease in ASR, while glm-4v-plus remains unchanged in AppAgent but experiences catastrophic failure in T3A, with ASR rising from 0.11 to 0.80. Open-source models generally face defense failure issues; for instance, qwen2-vl shows an increase of 15\% in ASR after defense in AppAgent, and the M3A defense of llava-onevision only reduces ASR by 7\%. Overall, this indicates that the defense effects achieved through prompts are not stable and may be related to the input modalities of MLLMs.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/improved_bar_chart.pdf}
    \caption{The growth rate of tasks that were prematurely terminated in \textit{AndroidWorld}.}
    \label{fig:growth_rate_of_task}
\end{figure}

\input{tables/4e_Table_of_Defense_ASR}

We present the attack success rates after using the defense prompt in Table \ref{tab:defense_attack_ASR_table}, showing significant differences in how different models respond to the defense strategy. Overall, the defense prompt works best on T3A, followed by M3A, and then I3A and AppAgent. In terms of the number of models with reduced attack success rates, after using the defense prompt, only a few MLLMs showed a decrease in attack success rate, while more models exhibited an increase in attack success rate. This suggests that the defense prompt may indirectly influence the judgment of MLLMs, making the defense effect highly unstable and even resulting in attack gains. In the future, we will further explore more effective defense methods.




\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/Attack_Details2.pdf} % Adjust height as needed
    \caption{Case study about \textit{Adversarial Attack} and \textit{Reasoning Gap Attack}.}
    \label{fig:reasoning details}
\end{figure*}