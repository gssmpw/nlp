\paragraph{Theory of causation.} Extensive research into theories of causation spans various disciplines \citep{dalal2023calm} such as philosophy \citep{beebee2009oxford}, cognitive science \citep{waldmann2017oxford}, and probability and statistics \citep{pearl2009causality}. In this paper, we follow the counterfactual theory of causation \citep{lewis2013counterfactuals}, which entails three aspects: a \textit{relational} aspect (involving a cause and an effect components), a \textit{temporal} aspect (the cause precedes the effect), and a \textit{counterfactual} aspect (if the causing event had not taken place, the effect would not have occurred). 

\paragraph{Causal discovery in Statistics.} The task of causal discovery or structure learning is to recover the causal DAG using available observational or experimental data. It remains a challenging problem in statistics since the search space is super-exponential in the number of variables and the identifiability of the true DAG does not always exist. Causal discovery methods primarily fall into two categories: constraint-based and score-based approaches.  Constraint-based methods such as PC \cite{spirtes1991algorithm} and FCI \cite{spirtes2000causation} extract conditional independencies from the data distribution to detect edge existence and direction. Meanwhile, score-based methods search for model parameters in the DAG space by optimizing a scoring function \cite{chickering2002optimal,zheng2018dags,yu2019dag,bello2022dagma}.


\paragraph{Causal discovery in NLP.} Ample of work in NLP focuses on event causality identification (ECI), which identifies cause/effect spans from textual descriptions. ECI is commonly treated as a classification task that relies heavily on annotated data for supervised learning \cite{oh2013question, hashimoto2014toward, riaz2014recognizing, cheng2017classifying, gao2019modeling}, or at least partially for semi-supervised training \cite{zuo-etal-2021-improving, shen-etal-2022-event}. Machine learning models trained on mention-level annotations exploit event temporal links \cite{pustejovsky2003timebank, pustejovsky2006timebank} and/or lexical cues or semantics that signal causal information, including, but not limited to, prepositions e.g. \textit{because of, by, due to}, conjunctions/conjunctive adverbs e.g. \textit{because, since, therefore, as a result} or verb semantics \cite{wolff2003models, mirza-tonelli-2014-analysis} such as causation e.g. \textit{cause, force}. However, as these annotated benchmarks are relatively limited in scale, ECI models are prone to overfitting and tend to mishandle new and unseen cases \citep{zuo-etal-2021-improving, sun2023event}. 

 
\paragraph{Causal discovery with LLMs.} Despite impressive language skills and breakthroughs in AI capabilities, large language models (LLMs) are reported to exhibit the same difficulty where they fail to perform causal inference in out-of-distribution settings when variable names and textual expressions used in the queries are different to those in the training set \citep{jin2023can,zevcevic2023causal}. On the other hand, whether LLMs can perform causal discovery is a controversial topic. \citet{kiciman2023causal} show that in medical and climate domains, LLMs can achieve competitive performance in determining pairwise causal relationships with accuracies up to $97\%$, yet heavily relying on prompt engineering with occasional inconsistencies. Meanwhile, full graph discovery in LLMs remains excessively challenging, though proper prompting could yield some potential. However, when evaluated on datasets of real-world events, \texttt{GPT-3} and \texttt{GPT-4} are consistently surpassed by small fine-tuned small pre-trained language models on ECI tasks \citep{gao2019modeling} while under-performing greatly on binary pairwise causality inference \citep{romanou2023crab}. Since LLMs are trained on massive volume of natural language texts, they excel in identifying causal event pairs but not non-causal ones, raises concerns regarding the memorization of event knowledge rather than generalization \citep{jacovi2023stop,gao2023chatgpt,romanou2023crab}. 

\paragraph{Abstract reasoning.} The human brain is equipped with a remarkable capability of abstract reasoning: thinking of concepts and generalizations of concrete entities that exist in infinity. Conceptualization glues separate pieces of experiences into a mental world that forms commonsense knowledge and allows us to function in the complex reality \cite{murphy2004big}. By the same logic, events sharing a physical mechanisms should exhibit the similar causal dynamics. For examples, $broken \ window$ and $shattered \ glass$ both refers to an effect resulting from a hard physical object hitting against a glassy surface. \texttt{GLUCOSE} \cite{mostafazadeh-etal-2020-glucose} is one large-scale annotated corpus that explicitly facilitates causal commonsense knowledge. The dataset captures 10 dimensions of causal explanations in story events covering a wider range of entities and contexts. \texttt{GLUCOSE} provides rich translations of specific expressions into generic inferential rules dependent on story contexts. \texttt{GLUCOSE} is thus a rich resource for exploiting abstract causal knowledge, which remains a promising yet unexplored avenue. In the common pursuit of abstract knowledge, \citet{he2022acquiring} build an event conceptualization pipeline on top of \texttt{ATOMIC} \cite{sap2019atomic}, a large-scale annotated commonsense knowledge graph, wherein the mentioned textual entities are replaced with the corresponding abstract concepts.