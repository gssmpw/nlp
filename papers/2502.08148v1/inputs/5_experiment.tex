In this section, we conduct empirical analyses to demonstrate how the \texttt{ACCESS} benchmark is used for evaluating (1) the effectiveness of automatic event abstraction and causal discovery approaches, and (2) how a causal structure assists reasoning models on causal QA tasks.  All experimental results are averaged over $5$ random running seeds. The codes and data for reproducing our experiments are published at \url{github.com/isVy08/ACCESS}. 

\subsection{Abstract Event Identification}\label{sec:abstraction_exp}For abstract causal discovery and reasoning, a practical question is how one can identify abstract events from real-world corpora where the ground-truth is unknown. 
Given the advances of LLMs, a promising approach to use LLMs to generate abstractions. In this experiment, we explore two approaches to automatically extract event abstraction with \texttt{GPT-4o-mini}, using Open AI's official API.\footnote{\url{platform.openai.com/} (accessed between 30 Sept. 2024 and 14 Oct. 2024).} We then use \texttt{ACCESS} as ground-truth to evaluate the quality of abstraction. 

\paragraph{Generate abstract events in a Single Step.}
We have \texttt{GPT-4o-mini} directly generate the generalized expressions. We extract $9,495$ event mentions from \texttt{GLUCOSE} and ask the model to generate two generalized versions for every instance, corresponding to the levels of \textit{generalization} (level $1$) and \textit{abstraction} (level $2$) described in Figure \ref{fig:main}. We then compare the generated abstractions with the ground-true ones provided by \texttt{GLUCOSE} and \texttt{ACCESS}. The model achieves the BLEU score of $0.520$. The prompt for this task can be found in Appendix \ref{sup:reasoning}.

\paragraph{Identify abstract events in Two Steps.}
In the second approach\footnote{In this experiment, we exclude duplicated expressions to reduce biases, resulting in $3,713$ and $4,248$ generalizations.}, we obtain the produced \textit{generalizations} by \texttt{GPT-4o-mini} from the above step, then run automatic clustering to find the abstractions, following the setup in Section \ref{sec:abstraction}. For all instances in every output cluster, we retrieve the ground-true clusters given by \texttt{ACCESS} and take the majority one as the predicted assignment. We measure the level of agreement between the predicted and the true assignment, using the Rand index \citep{steinley2004properties} and mutual information \citep{vinh2009information}. 

In Appendix \ref{sup:clustering}, Table \ref{tab:clustering_abs} provides detailed numerical results for various clustering algorithms in this experiment. In all cases, the agreement scores are well below $1.0$ (perfect agreement). This indicates  vanilla automatic clustering is inadequate in identifying useful abstractions. While choosing a good clustering algorithm remains important, we find that the quality of the input generalizations plays a more critical role in the performance. When we conduct the same experiments on the ground-true generalizations from \texttt{GLUCOSE}, all metrics are significantly improved by at least $28\%$. 

We further observe that on average, with generalizations from \texttt{GPT-4o-mini}, an output cluster has more than $40\%$ of its instances belonging to a different cluster from the predicted one, and based on the ground-truth, a cluster should be further divided into at least $2$ sub-clusters to be considered correct. We find that the issue is mainly due to the fact the model produces over-generalized expressions, causing the clustering algorithm to form bigger clusters. For example, the mentions \textit{Amanda feels excited} and \textit{He is scared} are both generalized to \textit{A person feel an emotion} while we consider \textit{be excited} or \textit{be scared} to refer to different states. Another example is the mention \textit{Tom works hard} being one-step generalized to be \textit{A person do something}, which arbitrarily can be applied to any expressions. This reveals the difficulty in controlling the granularity of abstractions using LLMs, which substantiates the necessity for the benchmarks on event generalization and abstraction. 


\subsection{Pairwise Causal Discovery}\label{sec:CAUSAL}

We now describe how the data provided in \texttt{ACCESS} can be used for the causal discovery task. In the main text, we discuss the pairwise causal discovery task in LLMs. We examine how well LLMs can discern pairwise causal relations between two abstract events. Formally, given a pair of events $x$ and $y$, LLMs are asked to determine the relation between them by outputting one of the three possible relations: $x$ \textit{causes} $y$, $y$ \textit{causes} $x$, or \textit{no causal relation}. In addition to the $1,494$ causal relations in \texttt{ACCESS}, we also randomly generate $~1,000$ negative pairs to challenge the models. For our experiments, the LLMs used are \texttt{GPT-4o-mini}, \texttt{Llama3.2-3B-Instruct}, \texttt{Llama3.1-8B-Instruct} and \texttt{Llama2-chat-7B}\footnote{\url{llama.meta.com/} \& \url{huggingface.co/meta-llama/}}. The output from these models is post-processed to extract the final relation. The prompts can be found in Table \ref{tab:prompt_causal_discovery} of Appendix \ref{sup:reasoning}. 
 
The results are presented in Table \ref{tab:causal_discovery_results}, using Precision, Recall, and F1 score as evaluation metrics. We also report the performance of \texttt{random} choices and \texttt{majority} baseline, where the most frequent answer is selected for assessment based on the reference data.  LLMs achieve fairly humble accuracies, where \texttt{GPT-4o-mini} achieves the best performance, second to which is \texttt{Llama3.1-8B}. It is worth noting that the task is non-contextual since the goal is to assess the models' capability of intuitive or commonsense causal reasoning. Such intuition in humans is typically shaped by our observations and experiences from everyday life, enabling us to quickly identify scenarios where the causal relationships often hold. For example, we intuitively understand that speeding can frequently result in being the person being fined by the police. 

Appendix \ref{sup:causal_discovery} later demonstrates how the \texttt{ACCESS} pipeline facilitates the application of statistical structure learning algorithms. These methods are currently shown to under-perform on our benchmark, suggesting that there remains a large gap between theoretically grounded causal discovery and event causality identification research in NLP.   

\begin{table}[hbt!]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{l | r r r}
\hline
\multicolumn{4}{c}{\cellcolor[HTML]{C0C0C0}\textbf{Causal Discovery on \texttt{ACCESS}}}            \\ \toprule
                    & Precision $\uparrow$ & Recall $\uparrow$ & F1 $\uparrow$     \\ \toprule

\texttt{GPT-4o-mini}       & $\mathbf{0.705\pm.026}$    & $\mathbf{0.581\pm.028}$ & $\mathbf{0.559\pm.025}$ \\
\texttt{Llama3.2-3B} & $0.384\pm.015$    & $0.364\pm.006$ & $0.326\pm.007$ \\
\texttt{Llama3.1-8B} & $0.437\pm.006$    & $0.425\pm.006$ & $0.413\pm.006$ \\
\texttt{Llama2-7B} & $0.376\pm.006$    & $0.359\pm.006$ & $0.316\pm.007$ \\
 \midrule
\texttt{Random} & $0.340\pm.008$    & $0.330\pm.003$ & $0.330\pm.008$ \\
\texttt{Majority} & $0.114\pm.001$    & $0.333\pm.001$ & $0.170\pm.001$
 \\ 
\bottomrule
\end{tabular}%
}
\caption{Experiment results of causal discovery on \texttt{ACCESS} dataset. Precision, Recall, and F1 are computed under macro-average setting. \textbf{Bold} indicates best performance. $\uparrow$ Higher is better.}
\label{tab:causal_discovery_results}
\end{table}

\begin{table*}[!hbt]
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|lr|lr|lr}
\hline
\multicolumn{7}{c}{\cellcolor[HTML]{C0C0C0}\textbf{QA Reasoning on \texttt{GLUCOSE}}}            \\ 
\toprule
 & \multicolumn{2}{c}{\textbf{Specific QA}}  & \multicolumn{2}{c}{\textbf{Specific QA+}}  & \multicolumn{2}{c}{\textbf{Abstract QA+}}  \\ 
 \midrule
zero-shot COT & Accuracy $\uparrow$   & F1 $\uparrow$     & Accuracy $\uparrow$   & F1 $\uparrow$    &  Accuracy $\uparrow$  & F1$ \uparrow$   \\
 \midrule
\texttt{GPT-4o-mini}      & $0.790\pm.008$                                    & $0.809\pm.006$           & $0.719\pm.049$           & $0.720\pm.019$           & $0.561\pm.013$           & $0.554\pm.013$           \\
\texttt{GPT-4o-mini} + CG & $\mathbf{0.894\pm.067}$                           & $\mathbf{0.887\pm.046}$  & $\mathbf{0.912\pm.012}$  & $\mathbf{0.887\pm.006}$  & $\mathbf{0.731\pm.047}$  & $\mathbf{0.686\pm.017}$  \\
\midrule
\texttt{Llama3.2-3B}      & $0.723\pm.006$ & $0.501\pm.010$  &      $0.696\pm.022$ & $0.487\pm.010$  & 
$0.631\pm.006$ & $0.524\pm.008$          \\
\texttt{Llama3.2-3B} + CG & $\mathbf{0.803\pm.002}$ & $\mathbf{0.561\pm.014}$ &   $\mathbf{0.754\pm.001}$ & $\mathbf{0.533\pm.002}$ & $\mathbf{0.725\pm.005}$ & $\mathbf{0.551\pm.004}$ \\
\midrule
\texttt{Llama3.1-8B}      & $0.883\pm.005$ & $0.428\pm.003$         & $0.833\pm.018$ & $0.418\pm.010$         & 
$0.794\pm.012$ & $0.421\pm.004$         \\
\texttt{Llama3.1-8B} + CG & $\mathbf{0.924\pm.008}$ & $\mathbf{0.556\pm.018}$ & $\mathbf{0.885\pm.002}$ & $\mathbf{0.521\pm.011}$ & $\mathbf{0.875\pm.001}$ & $\mathbf{0.524\pm.009}$ \\
\midrule
\texttt{Llama2-7B}        & $0.530\pm.013$ & $0.509\pm.012$  &     $0.480\pm.003$ & $0.553\pm.007$       &   
$0.483\pm.015$ & $0.537\pm.005$          \\
\texttt{Llama2-7B} + CG   & $\mathbf{0.681\pm.004}$ & $\mathbf{0.601\pm.009}$ & $\mathbf{0.635\pm.015}$ & $\mathbf{0.632\pm.010}$ & $\mathbf{0.692\pm.010}$ & $\mathbf{0.663\pm.009}$
\\
\bottomrule
\end{tabular}
}
\caption{Experiment results of multi-choice causal reasoning on \texttt{GLUCOSE-QA} dataset. \textbf{QA+} indicates the setting where the stories are paraphrased. \textbf{+CG} refers to the experiment that prompts the causal information from \texttt{ACCESS}. }
\label{tab:causal_reasoning_qa}
\end{table*}

% \textbf{Bold} indicates best performance. $\uparrow$ Higher is better.

\begin{table}[!h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|l r}
\hline
\multicolumn{3}{c}{\cellcolor[HTML]{C0C0C0}\textbf{Abstract QA+ on \texttt{GLUCOSE}}}            \\ \toprule
bi-level COT             & Accuracy $\uparrow$ &  F1 $\uparrow$     \\ 
\toprule
\texttt{Llama3.2-3B} & $0.722\pm.025$ & $0.378\pm.004$ \\
\texttt{Llama3.2-3B} + CG & $\mathbf{0.740\pm.025}$ & $\mathbf{0.410\pm.014}$ \\
\midrule
\texttt{Llama3.1-8B} & $0.753\pm.011$ & $0.430\pm.009$  \\
\texttt{Llama3.1-8B} + CG & $\mathbf{0.813\pm.022}$ & $\mathbf{0.493\pm.018}$ \\
\midrule
\texttt{Llama2-7B} & $0.503\pm.028$ & $0.516\pm.007$ \\
\texttt{Llama2-7B} + CG & $\mathbf{0.605\pm.018}$ & $\mathbf{0.623\pm.005}$ \\
 \bottomrule
\end{tabular}%
}
\caption{Experiment results of multi-choice Abstract QA+ with bi-level COT prompting. }
\label{tab:causal_reasoning_abstract_qa}
\end{table}

% \textbf{Bold} indicates best performance. $\uparrow$ Higher is better.


\subsection{Reasoning with Causal Graphs}\label{sec:QA}

We now study how the causal graphs in \texttt{ACCESS} can be used to assist models in QA reasoning tasks. In connection with Section \ref{sec:CAUSAL}, this can essentially be viewed as a contextual causal discovery task. We construct a causal QA dataset from \texttt{GLUCOSE}, which provides a set of stories with annotated causal relations between events at both the \textit{mention} and \textit{generalization} levels. For every story, we create two alternative multi-choice questions about the cause and effect. We extract the sentences in the story as candidate answers. The event appears in the annotated causal pair is thus considered a correct answer. Furthermore, in some cases, there can exist multiple causes that co-occur and lead to an effect and vice versa. To address this, we have a human expert review the data to identify additional correct causal events. The judgment of causality is based on the same criteria of the annotation process described in Appendix \ref{sup:annotation_ph2}. It is worth highlighting again that we focus on \textit{direct} causal relations, meaning that we do not consider events that result from/in some intermediate causes/effects. The resulting dataset contains $480$ questions, some of which have multiple correct answers. 

Since the LLMs have the tendency to exploit the textual cues, we additionally generate the paraphrases for each story while retaining the event choices in their original version. We label this extra setting as \textbf{QA+}. We also experiment with two variants of question design. In this first one, the cause/effect event of question occurs at the original mention level, whereas in the second one, the cause/effect event is transformed into its generalized version. For example, the question \textit{"What could be the cause of the event 'Amy gets a job in a bank'?"} is replaced into \textit{"The story describes an event where 'a person gets a job'. What could be the cause of the event?"}. We label the two variants as \textbf{Specific QA} and \textbf{Abstract QA} respectively. To perform the second task, ideally the model should be able to first perform abstract reasoning, that is to map the generalized cause/effect event to its corresponding mention in the context, prior to retrieving the correct causal pair. Examples of this \texttt{GLUCOSE-QA} dataset are provided in Appendix \ref{sup:reasoning}. 

\paragraph{How \texttt{ACCESS} provides abstract causal information.} To evaluate whether the causal abstract knowledge from \texttt{ACCESS} can help QA reasoning, we extend the above experiment by adding the causal relations between two relevant abstractions as an additional context in the prompt. In our experiments, the corresponding abstraction of an event mention can be retrieved directly from \texttt{ACCESS}. However, for an arbitrary QA dataset, this should be done via two subsequent steps: (1) perform abstraction of the event described in the target cause/effect and (2) map the output abstraction to at least one abstract event in \texttt{ACCESS} and retrieve the corresponding causal relations. Section \ref{sec:abstraction_exp} has described two possible approaches to step (1). 

\paragraph{Results.}
Given a story and a causal question, we prompt LLMs to generate the answer from a set of provided candidates. We first adopt zero-shot chain-of-thought (COT) \citep{kojima2022large} with the basic \textit{``let's think step by step"} prompting. Given the \texttt{Llama} models are open-sourced, we consider a bi-level COT dedicated to abstract QA tasks. In this approach, we provide a brief instruction on how to perform abstract causal reasoning, which entails two steps: the first one is for abstract reasoning, that is to identify the mention corresponding to the generalized cause/effect of question; the second step is for causal reasoning, that is to retrieve the corresponding effect/cause mentioned in the story context. See Appendix \ref{sup:reasoning} for prompts and qualitative examples.  

The evaluation metrics include Accuracy (which measures how often the model successfully retrieves at least one correct answer) and F1 score under weighted-average setting (which considers the alignment of all predicted choices). 
Tables \ref{tab:causal_reasoning_qa} and \ref{tab:causal_reasoning_abstract_qa} summarize our experiment results. Each setting introduces an increased level of difficulty in abstract reasoning. In the first task of Specific QA, the models can draw the answers directly from the raw context. Meanwhile, Specific QA+ tasks obscure away the linguistic cues, which the models are known to heavily exploit for prediction. Finally, Abstract QA+ is the most challenging, where the models are expected to concretize the abstract events before deriving the answers.  

The findings reveal that the inclusion of causal graphs significantly enhances the performance of LLMs across all experimental settings. Except \texttt{Llama2} whose performance is consistently poor, the performance of all models degrade on Abstract QA+ tasks, which indicates their struggle in reasoning over abstract causality. However, while we use \texttt{ACCESS} to provide the LLMs with the causal relations only between event abstractions, large improvements have been observed. Therefore, we hypothesize that the model may possess a subtle capacity to reason abstractly that needs proper activating. Compared to \texttt{GPT-4o-mini}, the \texttt{Llama} family are more prone to temporal and lexical biases, resulting in low F1 scores due to a higher number of negative selections. Concretely, the models select on average $1.9$-$2.9$ more answers than the actual ones across the QA tasks. With the additional causal information, the ratios are reduced to $1.8-2.6$. Bi-level COT unfortunately yields undesirable results, with a slight gain in accuracy in smaller models yet at a cost of reduced F1 scores due to increased over-prediction. This implies potential errors in some reasoning steps, but tracing and evaluating lines of reasoning in complex COT is an open challenge. Nevertheless, our experiments show that a simple zero-shot COT plus relevant abstract causal knowledge can greatly benefit the models. This presents a straightforward alternative strategy to enhance performance by leveraging external knowledge bases.




% When we employ COT, the ratio goes up to $2.4-3.2$ yet goes down to $1.9-3.0$ when causal graphs are added. 




% \paragraph{Reasoning in T5 model.} 

% This study is conducted using the CICERO dataset \citep{ghosal2022cicero}, which is specifically designed for commonsense inference in dialogue scenarios. Each sample in the CICERO dataset involves a dialogue $D$. Within $D$, CICERO selects a sentence as the target $u_t$. Subsequently, CICERO poses various types of questions related to $u_t$, and in this study, we only consider questions $q$ asking about the cause: \texttt{``What is or could be the cause of target?"} and the effect event: \texttt{``What subsequent event happens or could happen following the target?"}. We explore both multi-choice and open-ended answer settings. For multiple choice answers, the Macro-F1 score is utilized for evaluation. Conversely, in the answer generation setting, BLEU metrics are applied.\footnote{We adopt standard implementation for BLEU and BERTScore from Hugging Face for all experiments.} Details on the experimental setup and prompt design can be found in Appendix \ref{sup:reasoning}. 

% When using LLMs like \texttt{GPT-3.5} or \texttt{Llama2}, if the models' pre-training corpus already includes content from \texttt{GLUCOSE} or CICERO, the LLMs themselves may have memorized relevant inference information. Consequently, during solving the downstream tasks, this can lead to issues of information leakage. To mitigate the interference caused by information leakage on test results, we employ the T5 model as the base model. Initially, we fine-tune T5 on the \texttt{GLUCOSE} corpus, resulting in \texttt{GLUCOSE-T5} \citep{ghosal2022cicero}. We refer to the configuration in \texttt{GLUCOSE}~\cite{mostafazadeh-etal-2020-glucose} to fine-tune the T5 model. Subsequently, we extract a subset from the CICERO dataset covered by the \texttt{ACCESS} causal graph i.e., set of records for which one can retrieve the causal relations corresponding to the abstraction of the target.
% After the abstraction, the numbers of the train/dev/test examples are $628$, $200$ and $232$ respectively. 

% The goal for both the tasks is to produce the response as the output in the sequence-to-sequence framework, where the natural language answer is expected in the open-ended QA task while the correct choice (such as `$x_1$ $x_3$') is the desired prediction in multiple-choice QA task.
% Teacher forcing is employed in the training process, and during inference, we utilize beam search.
% We employ \texttt{PyTorch-lightning}\footnote{\url{https://www.pytorchlightning.ai}} to implement the T5 model, and use the \texttt{Lightning Trainer} to automate the training process.
% We set the number of training epochs as $4$ in fine-tuning the \texttt{\texttt{GLUCOSE}-T5}, while set the epoch number as 1$0$ for further training the \texttt{\texttt{GLUCOSE}-T5} on the CICERO dataset.


% In Table~\ref{tab:causal_reasoning_qa}, we can observe that for \texttt{\texttt{GLUCOSE}-T5} with injected causal graph (\texttt{\texttt{GLUCOSE}-T5} w CG), its performance in multi-choice and open-ended QA tasks is higher than \texttt{\texttt{GLUCOSE}-T5} without using causal graph information. From this comparative result, we can conclude that for traditional non-LLM models, injecting causal graph information can also help the model enhance its causal reasoning ability. The qualitative examples on the responses of \texttt{\texttt{GLUCOSE}-T5} are presented in Tables \ref{tab:examples_causal_reasoning_t5_open} and \ref{tab:examples_causal_reasoning_t5_mc}. 


% \Devin{For every instance in the \texttt{GLUCOSE} dataset, when provided with a brief story and a sentence $X$ within that story, \texttt{GLUCOSE} encapsulates ten dimensions of causal statements associated with $X$.
% Therefore, aligning with the characteristics of the \texttt{GLUCOSE} dataset, we structure the fine-tuning training corpus so that the input consists of a story $S$, a sentence $X$ within the story, and a dimension of causal statement $d$.
% The model is expected to produce the specific and general rules as its output, where the rules explain why the antecedent event will cause the consequent event $X$.
% We thus employ such the corpus to finetune the T5 model and get the model \texttt{\texttt{GLUCOSE}-T5}.}


% \tao{For the direct answer generation scenario, the following prompt template was employed: \textit{[Demostration example] Given the following dialogue: [dialogue history]. The question is: [question]. The target is [target]. The following causal relations can help you answer this question: [causal graphs]. Let’s work this out in a step-by-step way to be sure that we have the right answer. Then provide your final answer within the tags <answer></answer>}. }

% \tao{For the direct answer selection scenario, the prompt was formulated as: \textit{[Demostration example] Given the following dialogue: [dialogue history]. The question is: [question]. The target is [target]. Please select the correct answers (one or more answers) from the following choices: [answer choices]. The following causal relations can help you answer this question: [causal graphs]. Let’s work this out in a step-by-step way to be sure that we have the right answer. Then provide your final answer within the tags <answer></answer>}. }

% The aim of this study is to investigate the impact of incorporating causal graphs on the performance of causal reasoning tasks within the context of dialogue. The dataset provides a completed dialogue, a target utterance selected from the dialogue, and a question about this target. 

% The answers can either be directly generated or  To assess the efficacy of causal graphs in enhancing task performance, we designed two experimental settings: 1) inputs without causal graphs, and 2) inputs with causal graphs. 

% \tao{In the multi-choice answer setting, the Macro-F1 score was utilized for evaluation. Conversely, in the answer generation setting, both BLEU and BERTScore-F1 metrics were applied. The findings, as depicted in Table~\ref{tab:causal_reasoning_qa}, reveal that the inclusion of causal graphs in the input significantly enhances performance across both GPT-3.5 and Llama2.}