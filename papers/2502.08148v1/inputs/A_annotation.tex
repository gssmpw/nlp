% \begin{table*}
% \centering
% \begin{tabular}{p{2.5cm} p{4.2cm}| p{2.5cm} p{5cm}}
% \toprule
% \multicolumn{2}{c}{\textbf{Cause event}} & \multicolumn{2}{c}{\textbf{Effect event}} \\
% \midrule
% Abstraction & Generalizations & Abstraction & Generalizations \\
% \midrule
% \textit{a person} & a person need money &  \textit{a person get} & a person take up a job  \\
% \textit{need money} & a person need cash &  \textit{a job} & a person get a good job  \\
%  & a person need to get money &  & a person get a job at a place  \\

% \midrule
% \textit{a person win} & a person win the contest & \textit{a person} & a person be celebrate an occasion  \\
%  & a person win something &  \textit{celebrate} & a person have a celebration  \\
 
% & a person end up winning &   & a person celebrate something  \\
% \midrule
% \textit{a person fall} & a person fall down & \textit{a person feel} & a person be in pain  \\
%  & a person fall to the floor &  \textit{pain} & a person experience pain in a body part  \\
 
% & a person fall on the ground &  & a person 's body be in pain \\
% \bottomrule
% \end{tabular}
% \caption{Examples of event causality on the abstraction and generalization level.}\label{tab:example}
% \end{table*}

We recruit in total $13$ university students in Malaysia aged $20-30$. The total hours are  $329.7$, where the hourly rate is RM$20$ (Malaysian ringgit), which is higher than the minimum wage of RM$7.1$.  

As for the annotation guidelines, we translate the technical terminologies in Section \ref{sec:setup} into layman language comprehensible to human annotators.  


\subsection{Abstract Event Extraction}\label{sup:annotation_ph1}
There are five steps in this annotation phase. Steps $1$ and $2$ are key to extracting abstract events, whereas Steps $3-5$ serve as post-processing to strengthen consistency among human annotators. 

\paragraph{Step 1: Sub-clustering.}
Each annotator is presented with a set of clusters generated from an automatic clustering algorithm. Each cluster contains multiple English sentences that describe events in daily life. Each word in every sentence is lemmatized to its base form so that the tense of the sentence does not influence the judgment of meaning. For every cluster, they are required to sub-group event sentences that are semantically similar or related together. There can exist clusters in which all sentences are related to one another; in this case no sub-clustering is needed. There can also be outlier events i.e., sentences that do not belong to any sub-clusters. For a sub-cluster to exist, it must contain at least two events. If an event cannot be sub-clustered, the annotator is to classify it as an outlier.	If a sentence is lexically or grammatically erroneous that makes it unjustifiable, the annotator is also asked to highlight and correct it whenever appropriate before clustering. 


Two event sentences are considered \textit{semantically related} or \textit{similar}\footnote{We use ``='' to denote semantic similarity and ``$\ne$" to denote semantic dissimilarity between two events.} if they describe the same event, and the decision must not be affected by the information about \texttt{location} and \texttt{time}. We note there is a difference between a \texttt{state/action} actually taking place with the prospect of the \texttt{state/action} taking place. In particular, we outline $11$ scenarios where word uses convey differences in meaning.	

\begin{enumerate}
    \item single participant vs. group of participants e.g., \emph{a person be playing in the park} $\ne$ \emph{a person and another person be playing in the park.}
    
    \item affirmation vs. negation e.g., \emph{a person be asleep}  $\ne$ \emph{a person do not sleep.}

    \item present vs. future tense e.g., \emph{a person go to sleep} $\ne$ \emph{a person will go to sleep.}

    \item ability e.g., \emph{a person do not eat} $\ne$ \emph{a person cannot eat.}

    \item intention or desire e.g., \emph{a person do not eat} $\ne$ \emph{a person do not want to eat.}

    \item deduction or possibility e.g., \emph{it rain} $\ne$ \emph{it may rain.}. 
    
    \item obligation, advice or prohibition	e.g., \emph{a person do not eat} $\ne$ \emph{a person should not eat}.

    \item offers, effort or decision e.g., \emph{a person help another person} $\ne$ \emph{a person offer to assist another person}; \emph{a person go to the gym} $\ne$ \emph{a person decide to go to the gym.}

    \item location as object. In some cases, the object receives an action from the verb refers to a place or location e.g., \emph{a person clean a place}. Here \textit{room} is considered an (spatial) item being taken action on and similar to any other items such as cup or a table $\rightarrow$ \emph{a person clean a place} \textcolor{red}{=} \emph{a person clean something.}

    \item multiple actions. Some sentences describe two actions happening at the same time e.g., \emph{a person take something and leave}. In order to evaluate its meaning, one  must select one of them to the key action. The key action is the action that is described by most of other events in the same cluster. This means that if most of the other events are about \textit{someone leaving somewhere}, the \textit{leave} action should be focused instead of \textit{take} action. 

    \item continuous vs. simple tense. Some sentences describe actions in the continuous state e.g., \emph{a person be go home}. We ignore the continuous state of the action and consider them equivalent to the action described simple tense $\rightarrow$  \emph{a person be go home} \textcolor{red}{=} \emph{a person go home.}	
\end{enumerate}

	
\paragraph{Step 2 : Topic identification.}

In this step, the annotator asked to identify the topic for every cluster or sub-cluster formed. The topic must first be an event, therefore it must contain at least two components: \texttt{participant(s)} and \texttt{action}. The topic must be specific about the state or action that takes place. At the same time, the topic must be written in a way that makes it general or abstract enough to include all event sentences.	Whenever possible, it is acceptable to use the most representative event sentence in a cluster as the topic.	

\paragraph{Intermediate processing.} In Steps $1$ and $2$, we divide the collection of clusters into $7$ batches. Each of the batch contains $60$ clusters and 
two workers are asked to annotate one same batch of clusters. This results in one cluster having two annotation results. Subsequently, an algorithm is run to automatically unify the results from two annotators. For every cluster in the original data, the algorithm starts by randomly selecting an event as a centroid. It then forms a sub-cluster around the centroid that contains all other events that are considered by both annotators to be semantically related to the centroid. The topic assigned to that sub-cluster is presented in the format \texttt{TOPIC : [Text 1] / [Text 2]} where \texttt{[Text 1]} is the topic assigned to events in this sub-cluster by the first annotator and \texttt{[Text 2]} is the topic assigned to them by the second annotator. Repeat the process with the other events until all instances are processed. Thereafter, any event that is not assigned to any cluster will exist as a stand-alone instance and temporarily be considered an outlier. 

The next steps focus on resolving the disagreements from two annotation results, which includes \textbf{Topic alignment} and \textbf{Outliers processing}. We assume that a sub-cluster is properly annotated if it (1) contain at least $2$ instances and (2) no annotators consider that sub-cluster to be an outlier.

\paragraph{Step 3: Topic alignment.}
Every cluster is now annotated with two topics. If both topics describe the same event, the annotator is asked to choose either or the one more representative. Otherwise, choose the one that fits most of the sentences in the cluster. If the chosen topic is already assigned to some previous cluster, merge the current cluster into that cluster. If at least one of the topics is Outliers (i.e., at least one annotator considers the sub-cluster as Outliers), temporarily view them as Outliers. 

\paragraph{Step 4: Outliers processing.}
The annotator moves on to process the outliers. For any event that is assigned by only one of the previous annotators to be outliers while assigned by the other to be associated some existing sub-cluster, the annotator is asked to merge it into the assigned sub-cluster if the event can be represented by the topic of that sub-cluster; otherwise, keep it as an outlier. For any event that is agreed by both annotators to be an outlier, the current annotator is asked to re-examine it for possible assignment to any existing sub-cluster. The merging decision must be again based on the conditions described in Step $1$. Any remaining stand-alone instances are discarded. 

\paragraph{Step 5: Topic matching.} This step aims to correct for potential mis-clustering from the automatic procedure. We obtain the outlier events and attempt re-categorize them into the post-annotated clustering results from all above steps. For each outlier, we present the annotators with a set of candidate clusters to which adding the outlier would not violate causal consistency. We ask them to select one cluster with whose topic the outlier is most semantically similar. The rules to determine semantic similarity of a sentence pair follows from Step 1. It is possible that there is no topic that matches the outlier. If there is any topic that is a word-by-word exact match, that topic must be selected. We also add another rule that requires the annotators to select the topic with the same level of abstraction (generality) or concreteness (specificity) as the outlier event, since there are some topics that are abstract or concrete versions of other topics. More specifically, if the outlier is concrete but the concrete topic is not presented for selection, select the abstract topic. If the outlier is abstract but the abstract topic is not presented for selection, the concrete topic must \underline{not} be selected. 

\subsection{Causal Relations Discovery}\label{sup:annotation_ph2}
The annotator is tasked with evaluating candidate pairs of clusters to determine whether a cause-and-effect relationship exists between them, based on their respective topics. Since each cluster's topic represents an event abstraction, and in essence, an event itself, the decision on causal relation hinges on whether the two topics describe causally linked events. Based on the cause-effect definition in Section \ref{sec:setup}, we provide them with the following criteria to guide their decision about whether an event $A$ causes another event $B$:
\begin{enumerate}[leftmargin=5.5mm]
    \item a causal relation must be temporal, but a temporal relation is \underline{not} always causal;

    \item the action/state of $A$ directly leads to the action/state of $B$ i.e., there must be no intermediate events or if there is one, it should be extremely rare in real-world scenarios;
    
    \item an event $B$ would not occur if $A$ did not occur.
\end{enumerate}

Initially, the workers provide non-contextual annotations based solely on their commonsense understanding of the abstractions. A relation is deemed valid if the annotator can envision a plausible scenario in daily life where the situation occurs frequently, commonly, and is highly likely. If no such scenario comes to mind, the clusters are considered unrelated. In the subsequent step, we identify the highly disagreed pairs, where the three annotators each make distinct decisions regarding causality i.e., $A$ causes $B$, $B$ causes $A$, $A$ and $B$ are unrelated. For these pairs, workers are presented with contextual information from stories in \texttt{GLUCOSE} and asked to reconsider their decisions. The final determination of the relationship is made through majority voting.

