
%%
%% This is file `sample-sigplan.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigplan')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigplan.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigplan,screen,nonacm,10pt]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2024}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[SPLASH 2024]{Proceedings of the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software}{October 20--25, 2024}{Pasadena, California, USA}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.

%--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\graphicspath{ {./images/} }
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{todonotes}
\usepackage{ulem}
%--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

\newcommand\el[1]{\textcolor{orange}{{#1}}} % Comments from Erick
\newcommand\ct[1]{\textcolor{blue}{{#1}}} % Comments from Christian
\newcommand\old[1]{\textcolor{lightgray}{{#1}}} % replaced text

%removing copyright
\setcopyright{none}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Censor Resistant Instruction Independent Obfuscation for Multiple Programs}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ali Ajorian}
\affiliation{%
 \institution{University of Basel}
 \country{Switzerland}}
\email{ali.ajorian@unibas.ch}

%\author{Erick Lavoie}
%\affiliation{%
 %\institution{University of Basel}
 %\country{Switzerland}}
%\email{erick.lavoie@unibas.ch}

%\author{Christian Tschudin}
%\affiliation{%
 %\institution{University of Basel}
% \country{Switzerland}}
%\email{christian.tschudin@unibas.ch}




%\renewcommand{\shortauthors}{Ali Ajorian}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
This work builds upon and optimizes our prior research on \textit{obfuscation as instruction decorrelation} which achieves multiple program obfuscation. Leveraging this infrastructure, we further achieve the property of sensor-resistant computation.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10010940</concept_id>
       <concept_desc>Software and its engineering~Software organization and properties</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software organization and properties}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Software Protection, Obfuscation, Instruction Independence, Instruction Decorrelation}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.


\received{25 April 2024}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section {Introduction}

Accessing the binaries of a computer program raises an important question of how much can we learn about the program beyond simply running it with different inputs and observing its behavior. There are numerous techniques available that allow adversaries to analyze these binaries or run the program to study its behavior and internal states, with the intention of reverse engineering the source code or extracting secrets stored within the program. To protect software from such attacks, computer science has employed various ad-hoc heuristics for nearly four decades \cite{xu2020layered} and formal models for over two decades \cite{barak2001possibility, goldwasser2007best, kuzurin2007concept, ajorianiio}, collectively referred to as \textit{program obfuscation} or shortly \textit{obfuscation}. Roughly speaking, obfuscation is a transformation that maintains a program's functionality while hindering the extraction of valuable information, such as algorithms, data structures, and secret keys, with an acceptable slowdown overhead. 

Ajorian et. al introduced \textit{obfuscation as instruction decorrelation}, which focuses on the internal structures of regular programs to hide essential instructions within a set of junk instructions \cite{ajorianiio}. They defined the unintelligibility of an obfuscator as the decorrelation of all possible pairs of essential and junk instructions, ensuring that an efficient adversary cannot identify a correlated set of instructions to reconstruct an obfuscated program. 
 
In this work, we enhance the generation of junk instructions by obfuscating a collection of programs rather than focusing on a single program. Specially, for each program the remaining programs serve as the source of junk instructions. Since the obfuscator ensures that all obfuscated instructions are decorrelated, no efficient adversary can determine the origin of each instruction to disrupt the execution of a program. This means that an \textit{evaluator} executing the output of such an obfuscator can run either all or none of the input programs, making it impossible to censor any of the input programs.

% -----------------------------------------------------------------------------------------------------------------------------------------------------------------
\section{Preliminaries}
\label{sec:preliminaries}
This section introduces the notation that will be used consistently across this work and offers foundational background information relevant to our study.
 
\subsection{Notation}

PPT stands for probabilistic polynomial time algorithms, which can make random choices during execution, and their running time is bounded by a polynomial function of the input size.

By $x \leftarrow D$ we mean that $x$ is a random variable drawn from the distribution $D$. The notation $A(.) \rightarrow x$ denotes that a probabilistic algorithm A generates an outcome $x$ from an input $(.)$.

The notation $|.|$, in addition to its standard mathematical meanings such as the length of a set, is also utilized for indicating program size, referring to the number of elements or instructions within the program.

If $X$ is a set, by $X^*$ we mean all possible sequences of any length defined on the set X.

A function $\epsilon : \mathbb{N} \rightarrow \mathbb{N}$ is defined as negligible function if its growth rate is slower than the inverse of any polynomial. In other words, for any positive polynomial $p(n)$, there exists a threshold $n_0$ such that for all $n>n_0$, $\epsilon(n)<\frac{1}{p(n)}$.

By $set(p)$, we mean the set of all instructions in the program $p$. 


\subsection{Background and Related Works}
\label{sec:background}

Let $\mathcal{P}$ be the set of all possible programs defined on an instruction space $\mathcal{I}$, Ajorian et. al defined an \textit{instruction-independent obfuscation} \cite{ajorianiio} as a PPT algorithm $IIO:\mathcal{P}\rightarrow \mathcal{P}$  with the following properties:

\begin{itemize}
	\item[--] \textbf{Functionality preservation}: $\hat p = IIO(p)$ computes the same functionality as input program $p$.
	\item[--] \textbf{Polynomial slowdown}: There exist polynomials $q$ and $r$ such that $|IIO(p)| \leq |p|$ and if $p$ halts within a maximum $t$ steps on input $x$, the obfuscated program halts within $r(t)$ steps on the same input.
	\item[--] \textbf{Unintelligibility}: for any auxiliary input $z$ of polynomial size, for any PPT adversary $A$ there must exist a negligible function $\epsilon$ such that:

	\begin{equation}
	\label{eq:s_unintelligibility}
	\begin{split}
		\big| & \Pr\nolimits_{A(\hat p, z)\rightarrow \hat s_1, \hat s_2; \hat s_1 \neq \hat s_2}[\{\hat s_1, \hat s_2\} \subseteq \mathbb{S} \text{ or } \{\hat s_1, \hat s_2\} \subseteq \mathbb{J}] - \\ 
      		& \Pr\nolimits_{\hat s_1, \hat s_2 \leftarrow \Omega ; \hat s_1 \neq \hat s_2}[\{\hat s_1, \hat s_2\} \subseteq \mathbb{S} \text{ or } \{\hat s_1, \hat s_2\} \subseteq \mathbb{J}] \big| \leq \epsilon(\lambda)
	\end{split}
	\end{equation}
\end{itemize}
where $\lambda$ is a user choice of a security parameter, $\mathbb{S}$ and $\mathbb{J}$ are the sets of essential and junk instructions respectively and $\Omega=set(\hat p)$.


\section{IIO with Multiple Programs}
An IIO obfuscator employs a set of junk instructions, denoted as $\mathbb{J}$, with specific requirements to conceal the set of essential instructions $\mathbb{S}$ of a source program $p$. Provided these requirements are satisfied, $\mathbb{J}$ can be constructed using various methods. Specially, $\mathbb{J}$ may consist of the union of several $\mathbb{S}_i$s where each $\mathbb{S}_i$ represents the essential instructions of a program $p_i$. From this perspective, an IIO obfuscator can be viewed as a transformation $\mathcal{O}$ that takes a set of programs $P=\{p_1, \ldots, p_n\}$ as inputs and produces a single obfuscated program $\mathcal{O}(P)$ that is semantically equivalent to the set $P$ while ensuring that an efficient adversary cannot correlate any pair of instructions originating from the same source program. 

The following definition encapsulates this concept of obfuscation:

\begin{definition}[Multiple Program IIO]
\label{RIObfs}
A PPT algorithm $\mathcal{O}$ is considered as a multiple program IIO for a set of input programs $P=\{p_1, \ldots, p_n\}$ if its output satisfies the following three conditions:
\begin{itemize}
	\item[--] \textbf{Functionality preservation}: $\mathcal{O}(P)$ computes the same functionality as running all individual programs in $P$.
	\item[--] \textbf{Polynomial slowdown}: There exist polynomials $q$ and $r$ such that $|\mathcal{O}(P)| \leq q(\sum_{i=1}^n |p_i|)$ and if $p_1, \ldots, p_n$ halts within a maximum $t$ of steps on inputs $x_1, \ldots, x_n$ respectively, the obfuscated program $\mathcal{O}(P)$ halts within $r(t)$ steps on the same input.
	\item[--] \textbf{Unintelligibility}: for any auxiliary input $z$ of polynomial size, for any PPT adversary $A$ and for all $i \in \{1, ..., n\}$ there exist a negligible function $\epsilon$ such that: 
	
	\begin{equation}
	\label{eq:mp_unintelligibility}
	\begin{split}
		\big| & \Pr\nolimits_{A(\mathcal{O}(P), z)\rightarrow \hat s_1, \hat s_2; \hat s_1 \neq \hat s_2}[\{\hat s_1, \hat s_2\} \subseteq \mathbb{S}_i ] - \\ 
      		& \Pr\nolimits_{\hat s_1, \hat s_2 \leftarrow \Omega ; \hat s_1 \neq \hat s_2}[\{\hat s_1, \hat s_2\} \subseteq \mathbb{S}_i ] \big| \leq \epsilon(\lambda)
	\end{split}
	\end{equation}
	where $\lambda$ is a user choice of a security parameter and $\mathbb{S}_i$ is the set of essential programs for the program $p_i$ and $\mathbb{J}_i =\bigcup_{j=1, j\neq i}^{n} \mathbb{S}_{j}$.
\end{itemize}
\end{definition}

Note that, what we have defined as unintelligibility in Equation  \ref{eq:mp_unintelligibility} is not directly derived from Equation \ref{eq:s_unintelligibility}. Rather, Equation \ref{eq:mp_bare_unintelligibility}, presented below, is a direct rewriting of Equation  \ref{eq:s_unintelligibility} for multi-program obfuscation. Furthermore, in Theorem \ref{thm:bare_unint_equivalence}, we prove that Equations \ref{eq:mp_unintelligibility} and \ref{eq:mp_bare_unintelligibility} are equivalent.

\begin{equation}
	\label{eq:mp_bare_unintelligibility}
	\begin{split}
		&\forall i \in \{1 \ldots n\}: \\
		\big| & \Pr\nolimits_{A(\mathcal{O}(P), z)\rightarrow \hat s_1, \hat s_2; \hat s_1 \neq \hat s_2}[\{\hat s_1, \hat s_2\} \subseteq \mathbb{S}_i \text{ or } \{\hat s_1, \hat s_2\} \subseteq \mathbb{J}_i] - \\ 
      		& \Pr\nolimits_{\hat s_1, \hat s_2 \leftarrow \Omega ; \hat s_1 \neq \hat s_2}[\{\hat s_1, \hat s_2\} \subseteq \mathbb{S}_i \text{ or } \{\hat s_1, \hat s_2\} \subseteq \mathbb{J}_i] \big| \leq \epsilon(\lambda)
	\end{split}
	\end{equation}

\begin{theorem}
\label{thm:bare_unint_equivalence}:
Equation \ref{eq:mp_unintelligibility} and Equation \ref{eq:mp_bare_unintelligibility} are equivalent.
\end{theorem}
\begin{proof}
\end{proof}

\subsection{Resistance to Censorship}
Equation \ref{eq:mp_unintelligibility} implies that no PPT adversary is able to correlate any two obfuscated instructions that originate from the same source program. This means that adversaries are unable to distinguish between input programs. 

In the context of delegating the execution of an IIO-obfuscated program to a computing agent, this property implies that the agent cannot identify or recognize the underlying program. As a result, the agent is unable to selectively halt or modify the computation based on the program's identity. In other words, the agent can either compute all of the programs or none of them, ensuring censor resistance of input programs.

\subsection{Achieving Verifiable Computation}
In scenarios where the computation of a function $f$ for an input $x$is delegated to a computing agent, a key challenge arises: how to ensure that the computing agent, also known as prover, has honestly computed the result using $f$. This problem, known as the \textit{Verifiable Computation Problem} (VCP), is of significant interest in the field of secure and trustworthy computation. The general solution to VPC,  involves having the prover provide a proof alongside the computed result. This proof enables the other party, referred to as the verifier, to efficiently confirm the correctness of the result without needing to recompute $f(x)$ as depicted in Figure \ref{fig:vcp}. 

\begin{figure}[th]
    \centering
    \includegraphics[width=0.3\textwidth]{vcp}
    \caption{Verifiable Computing Problem}
    \label{fig:vcp}
\end{figure}

We can achieve verifiable computing using multiple program obfuscation. To do this, we need a light weight unknown program $c$ which serves as a proof in this framework. In our obfuscator, we consider the program set $P=\{f, c\}$. Since any IIO obfuscated program is censor-resistant, the prover cannot distinguish between $f$ and $c$ and $c$ remains unknown to the prover. During the process, the verifier provides inputs $x$ and $a$ as inputs for $f$ and $c$ respectively. The prover executes the obfuscated program and returns both $f(x)$ and $c(a)$ to the verifier. For verification, the verifier independently computes $c(a)$ and compares it with the value of $c(a)$ returned by the prover. If the values match, the verifier accepts the result $f(x)$. 

\section{Tampering Attack}
\label{sec:TampAttack}
Consider an obfuscated program $\mathcal{O}(P)$ that takes input vector $X=(x_1, \ldots, x_n)$ and produces output values $Y=(y_1, \ldots, y_n)$ for the programs $p_1, \ldots, p_n$ respectively. Throughout this process, the obfuscated program encounters a sequence of internal states $S=(s_1, \ldots, s_m)$. An adversary with the capability of tampering with the instructions or the internal states of $\mathcal{O}(P)$ can monitors $X,Y$ and $S$ before and after the tampering. The differences in these observations could potentially provide the adversary some information to correlate instructions and violate security guarantee of Equation \ref{eq:mp_unintelligibility}. Specially, the adversary could remove an instruction from the obfuscated program, execute the resulting program, and determine which output value $y_i$ changes. By doing this for polynomially many times, the adversary can detect correlations between the instructions of the obfuscated program.

We can achieve verifiable computation using IIO. A light weight function $c$ 
%If an obfuscated program $\mathcal{O}(M)$ is unintelligible for PPT adversaries with the capability of tampering with the obfuscated program or its internal states then for each honest but curious PPT adversary $A$, each instruction $s \in \mathcal{O}(M)$ and each given tampered program $\mathcal{O}(M)'$ there exists a negligible function $\epsilon$ such that:

%\begin{equation}
%\label{eq:tamper}
%\begin{split}
%\Bigl\lvert Pr[A(\mathcal{O}(M))\rightarrow(s_1,s_2): s_1,s_2 \in M_i]- \\
%Pr[A(\mathcal{O}(M)')\rightarrow(s_1,s_2): s_1,s_2 \in M_i] \Big\rvert \leq \epsilon(.)
%\end{split}
%\end{equation}

%a weaker security guarantee is as the following where $s$ is a randomly selected instruction and $i\in \{1,\ldots, n\}$ :
%\begin{equation}
%\label{eq:tamper}
%\Bigl\lvert Pr[A(\mathcal{O}(M),s)=i]- Pr[A(\mathcal{O}(M)',s)=i] \Big\rvert \leq \epsilon(.)
%\end{equation}


\subsection{Tamper-Resistance Output}
\label{label:Choutput}
%A special tampering attack highlights an interesting aspect of IIO obfuscator. Consider a scenario in which a PPT adversary modifies one instruction at each step while monitoring the output vector $Y$. When such an instruction has an impact on the calculated output value $y_i$, which corresponds to program $M_i$, the adversary will observe a corresponding	change in $y_i$. Hence, he gains the ability to identify different sets of instructions which lead to change in the same output value. In other words, the adversary has the advantage to find the correlation between some instructions and distinguish them.  

To defend tampering attack, the obfuscator must guarantee that any attempt to tamper with $\mathcal{O}(P)$ either leaves the output vector $Y$ unchanged or or alters all its elements entirely. This can be accomplished through two approaches: (1) implementing a compensation mechanism that neutralizes the effects of tampered instructions, ensuring $Y$ remains unchanged, or (2) employing an encoding scheme for $Y$ that depends on all instructions of the obfuscated program, ensuring that any tampering affects the entire output vector uniformly. %We propose two different approaches to achieve this goal. In the first approach the obfuscated program must add each instruction redundantly enough and make sure that each class of redundant instructions executes only once correctly. In this case when an adversary tampers with an instruction one of the other equivalent instructions compensate the misbehavior of tampered instruction.  Although finding a solution to this problem is still an open problem, two different ways can provide insights into potential solutions. One way is to think of adding noise to the outputs of such instructions and nullify them as the execution of the program proceeds. The other way is to encode the vector $Y$ with a mechanism that relies on a parameter dependent on all instructions, such as a hash value computed from the entirety of the instruction set.

%\subsection{Data Footprint}
%\label{sec:Chinput}
%Data, including constant values, input or output data and intermediate values, can reveal correlations between instructions of a program.\footnote{Certain program classes, like Quine programs, are inherently learnable from their input-output behavior. A Quine takes no input and produces a copy of its own source code as its only output are inherently learnable from their input-output behavior. As protecting the algorithm in these cases is futile, we exclude them from our investigation.} An adversary can analyze the data flow within the static structure or dynamic states of a program to identify correlation between instructions. Data-dependent or control-dependent instructions often exchange data between each other to perform calculations. The mechanism facilitating this data exchange can reveal the correlation between these instructions. For instance, in many practical scenarios, data-dependent instructions may write to and read from the same memory location indicating a data dependency. Hence, an adversary can iterate over the entire instructions of the obfuscated program and find sets of correlated instructions have the same data items as their arguments. 

%In general, for an input vector $X$, an obfuscated program $\mathcal{O}(M)$ generates a sequence of intermediate vectors $(Z_i)_L$, where $L=\sum_{k=1}^n |M_k|$ and each $s_i$ is the internal state after running $i$th instruction, to produce the output vector $Y$. The obfuscated program has access to each data element within $X, Y$ and $(Z_i)_L$ through a set of labels $\mathcal{L}$. This label set along with the sequence of internal states corresponding to each input vector, can provide useful information to a PPT adversary to find correlation between instruction. Therefor, we can restate the Equation \ref{eq:unint} such that for any PPT adversary A and for all input vector ${X}$ there exists a negligible function $\epsilon$ such that: 
%	\begin{center}$\Bigl\lvert Pr[A(\mathcal{O}(M),X)\rightarrow(s_1,s_2): s_1, s_2 \in M_i | (Z_i)_L, \mathcal{L}]-Pr[A(\mathcal{O}(M),X)\rightarrow (s_1, s_2): s_1, s_2 \in M_i] \Bigl\rvert \leq \epsilon(.)$ \end{center}


%\subsection{Uniformity of Instructions Distributions}

%Our obfuscation approach preserves the basic operations performed by the source programs, e.g. addition and comparison, because they are ultimately implemented with the same hardware instructions. Adversaries may therefore try to leverage a priori knowledge about the distribution of instructions in the source programs: if they significantly differ, then observing certain instructions in the obfuscator output enables the adversary to associate these instructions back to the source programs. The obfuscator should therefore ensure that knowledge of the distribution of instructions of the programs provides no advantage. Formally:

%\begin{theorem}
%\label{th:distr-decorrelation} (source distribution uniformity):
%Theorem \ref{th:singleinst} implies that input programs must have identical probability distributions and almost the same size.
%\end{theorem}
%\begin{proof}

%(sketch) 
%Without prior knowledge, the adversary may only randomly guess the provenance of a randomly selected instruction with a probability of $1/n$. Having access to the probability distributions of input instructions for each program, the adversary makes educated guesses with a probability improved by $\frac{n-1}{n^2}$ compared to random guessing, providing a non-negligible advantage because it is a polynomial fraction. This contradicts Theorem \ref{th:singleinst}. The full proof can be found in Section \ref{app:proof1} of the appendix.
%\end{proof}

%The simplest approach we have found so far of ensuring Theorem~\ref{th:distr-decorrelation} is to simply add meaningless instructions to every input programs so that  they all follow a uniform distribution across all possible hardware instructions. 
 
\section {Implementation}
\label{sec:construction}
In this section, we present an implementation of an obfuscator that hides which source program an obfuscated instruction originated from (Theorem~\ref{th:singleinst}) and  data access dependencies between instructions (Theorem~\ref{th:data-access-decorrelation}). 


\subsection{Architecture Overview}
\label{sec:implementation:overview}

%\begin{figure}[th]
  %  \centering
   % \includegraphics[width=0.5\textwidth]{overview}
    %\caption{Architecture of our implementation}
    %\label{fig:overview}
%\end{figure}


\subsection{Adversarial Model}

\section{Security Analysis}
\label{sec:sec-analysis}

\section{Implementation Results}
\label{sec:evaluation}

\section{Conclusions and Open Problems}
\label{sec:conclusion}

\begin{acks}
We would like to express our sincere gratitude to Osman Bicer for numerous insightful discussions concerning the security properties of this work.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}


%%
%% If your work has an appendix, this is the place to put it.
\appendix

%massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-sigplan.tex'.
