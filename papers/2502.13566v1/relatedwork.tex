\section{Related Work}
The recent conversational LLMs have been applied to various NLP tasks in a zero-shot setting, where the task instructions are given directly in the model prompt. While many such studies report encouraging performance (e.g.\ reasoning and dialogue \citep{qin-etal-2023-chatgpt, abaskohi-etal-2024-benchmarking-large}, text classification \citep{tornberg2023chatgpt4, gilardi2023chatgpt, debess-etal-2024-good-bad}, and relation classification \citep{aguda-etal-2024-large-language}), also negative results have been reported \citep{edwards-camacho-collados-2024-language-models, koneru-etal-2024-large-language}, especially when compared against task-specific supervised models.

For example, in the area of named entity recognition (NER) and relation extraction (RE), both \citet{wei2024zeroshot} and \citet{qin-etal-2023-chatgpt} report the generative LLMs substantially lagging behind the state-of-the-art supervised models in both tasks when evaluated on English. \citet{abaskohi-etal-2024-benchmarking-large} report similar results when applying ChatGPT to Persian named entity recognition. While their NER results clearly lag behind supervised models, their experiments show the LLMs being highly competitive in some Persian tasks, indicating the model works well also on a language other than English. This is also supported by \citet{debess-etal-2024-good-bad}, who report GPT-4 performing remarkably well when compared to human annotators on Faroese sentiment analysis, as well as \citet{tarkka-etal-2024-automated} on Finnish emotion classification.

Given the previous work, our study focuses on a task close to named entity recognition, however, in contrast to works by \citet{wei2024zeroshot,qin-etal-2023-chatgpt,abaskohi-etal-2024-benchmarking-large}, we do not attempt to create a universal method capable of returning any given named entities from any given input text. Rather we focus only on two types of entities (namely social organizations and hobbies) from a specific text collection (Karelian refugee interviews). Therefore, we can design the prompt specifically for the targeted entity types and text collection.

Previously, a targeted information retrieval study has been conducted e.g.\ in \citet{polak2023extracting}, where a target-specific, multi-step pipeline was developed for extracting materials properties from English materials science articles, reaching close to 90\% F-score using GPT-4. However, the text domain (English scientific articles compared to Finnish historical interviews), as well as targeted entity types (materials properties compared to persons' social organizations and hobbies), greatly differ in our study.