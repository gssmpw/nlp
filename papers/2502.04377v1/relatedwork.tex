\section{Related Work}
\label{sec2}
Our work is highly related to map construction task (See  Fig.~\ref{fig1}) and multi-sensor fusion methods, which will be discussed thoroughly in the following.


\subsection{Map Construction Task}
\textbf{HD map construction.}
HD map construction is a critical and extensively researched area in autonomous driving.
Based on input sensor modalities,
HD map construction models can be categorized into camera-based~\cite{zhang2023online,ding2023pivotnet,qiao2023end,maptrv2,yuan2024streammapnet}, LiDAR-based~\cite{lang2019pointpillars,yin2021center} and camera-LiDAR fusion~\cite{li2022hdmapnet,liu2023vectormapnet,MapTR,hao2025msc,hao2024your} models.
Camera-only methods~\cite{zhang2023online,ding2023pivotnet,qiao2023end,maptrv2,yuan2024streammapnet} have increasingly adopted the Bird's-eye view (BEV) representation as an ideal feature space for multi-view perception, owing to its remarkable ability to mitigate scale ambiguity and occlusion challenges. 
Various techniques have been proposed to project perspective view (PV) features onto the BEV space by leveraging geometric priors, such as LSS~\cite{philion2020lift}, Deformable Attention~\cite{22eccvbevformer}, and GKT~\cite{2022GKT}.
Nevertheless, camera-only methods lack explicit depth information, which forces them to rely on higher resolution images or larger backbone models to achieve enhanced accuracy~\cite{liu2021Swin,liu2021swinv2,hao2023mixgen,wang2022internimage,22eccvbevformer,yang2022bevformer,xiong2023neural}. 
In contrast, LiDAR-only approaches~\cite{lang2019pointpillars,yin2021center} benefit from the accurate 3D geometric information provided by LiDAR input.
However, they face challenges related to data sparsity and sensing noise.






Recently, camera-LiDAR fusion methods~\cite{li2022hdmapnet,liu2023vectormapnet,MapTR} leverage the semantic richness of camera data and the geometric information from LiDAR in a collaborative manner. 
BEV-level fusion, which uses two independent streams to encode raw inputs from camera and LiDAR sensors into features within the same BEV space, has gained significant attention~\cite{liu2023bevfusion,bevfusion22mips}.
This approach incorporates complementary modality features, outperforming uni-modal input approaches.
Existing HD map construction multi-sensor fusion methods—HDMapNet~\cite{li2022hdmapnet}, VectorMapNet~\cite{liu2023vectormapnet}, and MapTR~\cite{MapTR}— utilize straightforward channel concatenation and convolution for multi-modal feature fusion. 
However, these methods overlook modality interaction and employ very simple fusion strategies, leading to issues of misalignment and information loss.




\textbf{BEV map segmentation.}
Semantic map construction methods \cite{roddick2020predicting,pan2020cross,gosala2022bird,liu2023bevfusion} take map construction as a BEV semantic segmentation task, assigning semantic labels to each pixel in the BEV plane.
Building on Perspective View (PV) segmentation~\cite{roddick2018orthographic,202eecvlls}, early approaches utilize homography transformations to convert camera images into bird's-eye view (BEV) representations, followed by the estimation of segmentation maps~\cite{ammar2019geometric,zhang2022beverse,garnett20193d,zhu2021monocular}.
However, homography transformation introduces strong artifacts, and BEV-based methods~\cite{zhou2022cross, xie2022m,liu2023bevfusion}, i.e. performing segmentation directly on BEV plane, have received extensive attention.
CVT~\cite{zhou2022cross} employs a learned map embedding and an attention mechanism between map queries and camera features.
Furthermore, BEVFusion~\cite{liu2023bevfusion}, BEVerse~\cite{zhang2022beverse} and M$^{2}$BEV~\cite{xie2022m} explore
multi-task learning with 3D object detection.
However, these approaches lack explicit utilization of depth information, resulting in unsatisfactory performance.


Existing fusion methods~\cite{2020cvprpointpainting,21nipsmultimodal} primarily focus on object-centric and geometry-oriented approaches. For instance, PointPainting~\cite{2020cvprpointpainting} enhances only the foreground LiDAR points, while MVP~\cite{21nipsmultimodal} concentrates solely on densifying foreground 3D objects. 
Both methods also assume that LiDAR is the more effective modality for sensor fusion, which may not be valid for map construction tasks~\cite{liu2023bevfusion}.
Additionally, X-Align~\cite{borse2023x} employs an integration method that combines the features of the two modalities before applying attention, neglecting modality interactions and relying on overly simplistic fusion strategies.
In summary, these methods utilize basic feature concatenation to merge multi-modal features, necessitating the network to implicitly reconcile information from misaligned features.






\subsection{Multi-sensor Fusion}
Multi-sensor fusion has garnered significant attention in the field of autonomous driving. Existing approaches can be broadly categorized into three types: point-level fusion, feature-level fusion, and BEV-level fusion.
Point-level fusion methods~\cite{21cvprpointaug,2020cvprpointpainting,21itscfusionpainting,22ijcaiautoalign,21nipsmultimodal} typically project image semantic features onto foreground LiDAR points, enabling LiDAR-based detection on the enhanced point cloud. While effective for 3D object detection tasks, these methods are less suitable for semantically driven tasks such as BEV map segmentation~\cite{liu2023bevfusion,bevfusion22mips,zhou2022cross,xie2022m} and HD map construction~\cite{li2022hdmapnet,liu2023vectormapnet,MapTR}. This limitation stems from the lossy projection of camera features to LiDAR, where only about 5\% of camera features align with points from a typical 32-beam LiDAR scanner, resulting in significant information loss.
Feature-level fusion methods~\cite{22cvprfocal,18eccvdeepcontinuous} first project LiDAR points into a feature space or generate proposals, query the corresponding camera features, and then concatenate them back into the feature space. However, both point-level and feature-level fusion approaches encounter generalization challenges. Specifically, point-level fusion is not easily extendable to other sensor modalities, while feature-level fusion struggles with generalization across different tasks.



Recently, multi-modal feature fusion in a unified BEV space has gained considerable attention~\cite{MapTR,liu2023vectormapnet,li2022hdmapnet,liu2023bevfusion,bevfusion22mips}. BEV-level fusion employs two independent streams to encode raw inputs from camera and LiDAR sensors into features within the same BEV space. This approach offers a straightforward yet effective means to integrate BEV-level features from both streams, facilitating their use in various downstream tasks.
However, existing BEV-level fusion methods often overlook modality interactions, relying on element-wise operations (such as summation or mean) or simple concatenation. This can lead to issues of misalignment and information loss. In this paper, we propose a simple and effective camera-LiDAR BEV feature fusion method that simultaneously integrates complementary information from different modalities, specifically targeting multi-modal map construction tasks.





\noindent\textbf{Comparison with Existing Works.}
This work differs from prior literature in \textit{three} key aspects. 
Firstly, we focus on the BEV-based multi-modal map construction task, distinct from other BEV perception tasks~\cite{chen2025stvit+,21cvprpointaug,21itscfusionpainting}, as it aims at predicting map elements, such as pedestrian crossing, lane divider, road boundaries, etc. 
In fact, the map construction task is a semantic-oriented task, which pays more attention to the semantic information in the image.
Therefore, the performance of directly using the fusion method on the 3D object detection task to the map task is not satisfactory.
Secondly, to solve the semantic misalignment problem between Camera and LiDAR BEV features, we propose Cross-modal Interaction Transform (CIT) module to enable the two BEV feature spaces to interact with each other and enhance feature representation through a self-attention mechanism.
Additionally, to further fuse features from different modalities, we propose an effective Dual Dynamic Fusion (DDF) module to adaptively select valuable information from different modalities.
To the best of our knowledge, \textit{MapFusion} is the first to explore the effectiveness of interactive modules on multi-modal map construction tasks.
Last but not least, the core components of \textit{MapFusion}, \textit{i.e.}, CIT module and DDF module, are simple yet effective plug-and-play techniques compatible with existing pipelines for various map tasks, such as HD map and semantic map construction.