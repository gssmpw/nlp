\section{Related Work}
\label{sec2}
Our work is highly related to map construction task (See  Fig.~\ref{fig1}) and multi-sensor fusion methods, which will be discussed thoroughly in the following.


\subsection{Map Construction Task}
\textbf{HD map construction.}
HD map construction is a critical and extensively researched area in autonomous driving.
Based on input sensor modalities,
HD map construction models can be categorized into camera-based____, LiDAR-based____ and camera-LiDAR fusion____ models.
Camera-only methods____ have increasingly adopted the Bird's-eye view (BEV) representation as an ideal feature space for multi-view perception, owing to its remarkable ability to mitigate scale ambiguity and occlusion challenges. 
Various techniques have been proposed to project perspective view (PV) features onto the BEV space by leveraging geometric priors, such as LSS____, Deformable Attention____, and GKT____.
Nevertheless, camera-only methods lack explicit depth information, which forces them to rely on higher resolution images or larger backbone models to achieve enhanced accuracy____. 
In contrast, LiDAR-only approaches____ benefit from the accurate 3D geometric information provided by LiDAR input.
However, they face challenges related to data sparsity and sensing noise.






Recently, camera-LiDAR fusion methods____ leverage the semantic richness of camera data and the geometric information from LiDAR in a collaborative manner. 
BEV-level fusion, which uses two independent streams to encode raw inputs from camera and LiDAR sensors into features within the same BEV space, has gained significant attention____.
This approach incorporates complementary modality features, outperforming uni-modal input approaches.
Existing HD map construction multi-sensor fusion methods—HDMapNet____, VectorMapNet____, and MapTR____— utilize straightforward channel concatenation and convolution for multi-modal feature fusion. 
However, these methods overlook modality interaction and employ very simple fusion strategies, leading to issues of misalignment and information loss.




\textbf{BEV map segmentation.}
Semantic map construction methods ____ take map construction as a BEV semantic segmentation task, assigning semantic labels to each pixel in the BEV plane.
Building on Perspective View (PV) segmentation____, early approaches utilize homography transformations to convert camera images into bird's-eye view (BEV) representations, followed by the estimation of segmentation maps____.
However, homography transformation introduces strong artifacts, and BEV-based methods____, i.e. performing segmentation directly on BEV plane, have received extensive attention.
CVT____ employs a learned map embedding and an attention mechanism between map queries and camera features.
Furthermore, BEVFusion____, BEVerse____ and M$^{2}$BEV____ explore
multi-task learning with 3D object detection.
However, these approaches lack explicit utilization of depth information, resulting in unsatisfactory performance.


Existing fusion methods____ primarily focus on object-centric and geometry-oriented approaches. For instance, PointPainting____ enhances only the foreground LiDAR points, while MVP____ concentrates solely on densifying foreground 3D objects. 
Both methods also assume that LiDAR is the more effective modality for sensor fusion, which may not be valid for map construction tasks____.
Additionally, X-Align____ employs an integration method that combines the features of the two modalities before applying attention, neglecting modality interactions and relying on overly simplistic fusion strategies.
In summary, these methods utilize basic feature concatenation to merge multi-modal features, necessitating the network to implicitly reconcile information from misaligned features.






\subsection{Multi-sensor Fusion}
Multi-sensor fusion has garnered significant attention in the field of autonomous driving. Existing approaches can be broadly categorized into three types: point-level fusion, feature-level fusion, and BEV-level fusion.
Point-level fusion methods____ typically project image semantic features onto foreground LiDAR points, enabling LiDAR-based detection on the enhanced point cloud. While effective for 3D object detection tasks, these methods are less suitable for semantically driven tasks such as BEV map segmentation____ and HD map construction____. This limitation stems from the lossy projection of camera features to LiDAR, where only about 5\% of camera features align with points from a typical 32-beam LiDAR scanner, resulting in significant information loss.
Feature-level fusion methods____ first project LiDAR points into a feature space or generate proposals, query the corresponding camera features, and then concatenate them back into the feature space. However, both point-level and feature-level fusion approaches encounter generalization challenges. Specifically, point-level fusion is not easily extendable to other sensor modalities, while feature-level fusion struggles with generalization across different tasks.



Recently, multi-modal feature fusion in a unified BEV space has gained considerable attention____. BEV-level fusion employs two independent streams to encode raw inputs from camera and LiDAR sensors into features within the same BEV space. This approach offers a straightforward yet effective means to integrate BEV-level features from both streams, facilitating their use in various downstream tasks.
However, existing BEV-level fusion methods often overlook modality interactions, relying on element-wise operations (such as summation or mean) or simple concatenation. This can lead to issues of misalignment and information loss. In this paper, we propose a simple and effective camera-LiDAR BEV feature fusion method that simultaneously integrates complementary information from different modalities, specifically targeting multi-modal map construction tasks.





\noindent\textbf{Comparison with Existing Works.}
This work differs from prior literature in \textit{three} key aspects. 
Firstly, we focus on the BEV-based multi-modal map construction task, distinct from other BEV perception tasks____, as it aims at predicting map elements, such as pedestrian crossing, lane divider, road boundaries, etc. 
In fact, the map construction task is a semantic-oriented task, which pays more attention to the semantic information in the image.
Therefore, the performance of directly using the fusion method on the 3D object detection task to the map task is not satisfactory.
Secondly, to solve the semantic misalignment problem between Camera and LiDAR BEV features, we propose Cross-modal Interaction Transform (CIT) module to enable the two BEV feature spaces to interact with each other and enhance feature representation through a self-attention mechanism.
Additionally, to further fuse features from different modalities, we propose an effective Dual Dynamic Fusion (DDF) module to adaptively select valuable information from different modalities.
To the best of our knowledge, \textit{MapFusion} is the first to explore the effectiveness of interactive modules on multi-modal map construction tasks.
Last but not least, the core components of \textit{MapFusion}, \textit{i.e.}, CIT module and DDF module, are simple yet effective plug-and-play techniques compatible with existing pipelines for various map tasks, such as HD map and semantic map construction.