\section{Related Work}
Aligning LLMs with human preferences has evolved from studies on RLHF, aiming to achieve human-aligned outcomes____. The RLHF process typically begins with SFT, followed by further fine-tuning to maximize expected reward scores. This requires the construction of a reward model based on the Maximum Likelihood Estimation (MLE) of the BT model to provide such reward scores. This fine-tuning process is referred to as RLHF, with the PPO algorithm being the most widely applied____. A series of works focus on self-training, where the workflow involves sampling online data from the model and training it using a two-player min-max game between two policies____. However, the use of online data in the learning process presents significant challenges, as it requires substantial computational resources and limits training efficiency.
%
To address these challenges, researchers have shifted their focus to offline preference alignment learning algorithms. These methods operate in a single stage and directly optimize a designed loss function to achieve optimal preference alignment based on pairwise datasets____. Another approach to alleviate the resource-intensive nature of training is proposed by Remax____, which introduces a variance reduction method for LLMs.
%
Most closely related to our work is ALoL____, which formulates the reinforcement learning process at the sequence level and derives its advantage-based offline objective. Unlike ALoL, which relies on clipping operations to ensure training stability, we formulate our method by directly approaching the optimal solution of RLHF, thereby achieving a more precise solution to the RLHF objective.