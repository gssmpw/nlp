\section{Related Work}
Aligning LLMs with human preferences has evolved from studies on RLHF, aiming to achieve human-aligned outcomes **Brown et al., "Language Models as Talent Agents"**. The RLHF process typically begins with SFT, followed by further fine-tuning to maximize expected reward scores. This requires the construction of a reward model based on the Maximum Likelihood Estimation (MLE) of the BT model to provide such reward scores. This fine-tuning process is referred to as RLHF, with the **Schulman et al., "Proximal Policy Optimization Algorithms"** algorithm being the most widely applied. A series of works focus on self-training, where the workflow involves sampling online data from the model and training it using a two-player min-max game between two policies **Fujita, "Self-Training for Pre-Trained Language Models"**. However, the use of online data in the learning process presents significant challenges, as it requires substantial computational resources and limits training efficiency.
%
To address these challenges, researchers have shifted their focus to offline preference alignment learning algorithms. These methods operate in a single stage and directly optimize a designed loss function to achieve optimal preference alignment based on pairwise datasets **Wu et al., "Offline Preference Alignment with Pairwise Datasets"**. Another approach to alleviate the resource-intensive nature of training is proposed by Remax, which introduces a variance reduction method for LLMs **Teng et al., "Remax: A Variance Reduction Method for Large-Scale Language Models"**.
%
Most closely related to our work is ALoL, which formulates the reinforcement learning process at the sequence level and derives its advantage-based offline objective. Unlike ALoL, which relies on clipping operations to ensure training stability, we formulate our method by directly approaching the optimal solution of RLHF, thereby achieving a more precise solution to the RLHF objective.