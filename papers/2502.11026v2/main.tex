%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% \usepackage[ruled, linesnumbered, lined]{algorithm2e}
% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

%%%% Custom
\input{math_commands}
\usepackage[ruled, linesnumbered, lined]{algorithm2e}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{tabularx}
\usepackage{xcolor}
\usepackage{stfloats}
\usepackage{makecell}
\usepackage{tcolorbox}
\usepackage{pifont}
\usepackage{algpseudocode}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% \usepackage{icml2025}
\usepackage[accepted]{icml2025}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Simplify RLHF as Reward-Weighted SFT: A Variational Method}

\begin{document}

\twocolumn[


\icmltitle{Simplify RLHF as Reward-Weighted SFT: A Variational Method}


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yuhao Du}{equal,sribd,cuhk}
\icmlauthor{Zhuo Li}{equal,sribd,cuhk}
\icmlauthor{Pengyu Cheng}{equal,tencent}
\icmlauthor{Zhihong Chen}{Stanford}
\icmlauthor{Yuejiao Xie}{cuhk}
\icmlauthor{Xiang Wan}{sribd}
\icmlauthor{Anningzhe Gao}{sribd}
\end{icmlauthorlist}

\icmlaffiliation{sribd}{Shenzhen Research Institute of Big Data}
\icmlaffiliation{cuhk}{The Chinese University of Hong Kong, Shenzhen}
\icmlaffiliation{tencent}{Tencent AI Lab}
\icmlaffiliation{Stanford}{Stanford University}

\icmlcorrespondingauthor{Anningzhe Gao}{anningzhegao@gmail.com}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{\icmlEqualContribution}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
% \printAffiliationsAndNotice{}

\begin{abstract}
% 1. RLHF
Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning Large Language Models (LLMs) with human values.
%
However, RLHF has been continuously challenged by its high complexity in implementation and computation consumption. 
%
Even with recent simplifications, such as Direct Preference Optimization (DPO) and Advantage Leftover Lunch (A-LoL), the problems of over-fitting and training instability remain hindering the alignment process from the expected optimal performance.
%
To address the existing challenges, we propose a novel simplification of RLHF from the perspective of variational inference, called \textbf{V}ariational \textbf{A}lignment with \textbf{R}e-weighting (\textbf{VAR}). More specifically, by directly minimizing the distribution gap between the learning LLM policy and the optimal solution of RLHF, we transform the alignment objective into a reward-driven re-weighted supervised fine-tuning (SFT) form, which only requires minor adjustment on the SFT loss to obtain noticeable improvement on training stability and effectiveness. On comprehensive alignment and generation benchmarks, our VAR method has numerically achieved competitive performance in LLM alignment helpfulness and harmlessness.
\end{abstract}


\section{Introduction}\label{sec:introduction}

Large Language Models (LLMs)~\cite{openai2024gpt4technicalreport,touvron2023llamaopenefficientfoundation,yang2024qwen2} have achieved remarkable success in extensive applications of artificial intelligence (AI), including dialogue generation~\cite{abdullin2024syntheticdialoguedatasetgeneration}, coding~\cite{cobbe2021training,shao2024deepseekmathpushinglimitsmathematical},  logical reasoning~\cite{suzgun2022challenging},  and AI agents\cite{wu2023autogen}. Among the diverse LLM training techniques, Reinforcement Learning from Human Feedback (RLHF) plays a core role in ensuring the LLM generation is helpful and harmless. ~\cite{ouyang2022traininglanguagemodelsfollow, rafailov2024direct}.
%
In particular, RLHF first learns a reward model (RM) from annotated human preferences, then trains LLMs within a reinforcement learning (RL) scheme via Proximal Policy Optimization~\citep{schulman2017proximalpolicyoptimizationalgorithms} to optimize the expected rewards from the learned RM~\cite{ouyang2022traininglanguagemodelsfollow}. 
%

Although recognized as the mainstream solution to LLM alignment~\cite{ouyang2022traininglanguagemodelsfollow, shao2024deepseekmathpushinglimitsmathematical, touvron2023llama, openai2024gpt4technicalreport, yang2024qwen2}, RLHF remains being challenged because of its expensive computational resource consumption~\cite{cheng2023adversarial, yuan2023rrhf} and complicated implementation~\cite{ouyang2022traininglanguagemodelsfollow,shao2024deepseekmathpushinglimitsmathematical,yuan2023rrhf} in which multiple models (\textit{e.g.} the learning policy, the reference, the critic model, and the reward model) are required to cooperate in the online RL training scheme.
%
Moreover, incorporating such a complicated pipeline significantly induces training complexity and instability, leading to the difficulty of training convergence and the high risk of collapse~\cite{song2023reward,go2023aligning}. 

Towards more stable training than online alignment, several ranking-based offline alternatives are proposed, primarily from the perspective of enlarging the likelihood margin between preferred and rejected response pairs in a contrastive approach. Direct Preference Optimization (DPO)~\cite{rafailov2024direct} implicitly maximizes the difference in sampling probabilities between good and bad answers. 
\citet{ethayarajh2024ktomodelalignmentprospect} introduces Kahneman-Tversky Optimization (KTO) to directly maximize the utility of generations instead of maximizing the log-likelihood of preferences.
Although methods like GPRO~\cite{shao2024deepseekmathpushinglimitsmathematical} forego the critic model, instead estimating the baseline from group scores and significantly reducing training resources, its online sampling strategy still challenges the practical implementation and training speed.
While effective, these methods usually rely on the collection of preferred / rejected response pairs with high quality, which introduces a substitution data collection consumption. Instead, Advantage Leftover Launch (A-LoL)~\citep{bahetileftover} formulates the reinforcement learning process at the sequence level and derives an advantage-based offline objective that exclusively utilizes preferred responses to achieve human-aligned results. However, it still relies on clipping the importance weights to ensure training stability, which prevents the optimization from reaching the true RLHF optima. 
Furthermore, approaches like DPO and ALoL could employ negative weights for potential dis-preferred responses, leading to an unstable training process due to the unbounded nature of loss landscape when negative weights are applied~\cite{pal2024smaug,yan20243d}.



In this paper, we address these limitations by proposing a reward-driven variational alignment framework that eliminates the need for clipping and avoids the instability introduced by negative weights. Our approach reformulates RLHF as a variational inference problem over positive measures, ensuring a stable and well-defined optimization landscape. Specifically, starting from the closed-form optimal solution of RLHF, we minimize the Kullback–Leibler (KL) divergence~\cite{kullback1951information} between the to-be-learned LLM and its optimal solution. The resulting loss function takes the form of a reward-driven weighted supervised fine-tuning (SFT) loss, where non-negative weights are derived through an exponential reward transformation. Furthermore, we introduce an efficient in-batch normalization technique to approximate the normalization term, enabling scalable and practical implementation. Experimental results demonstrate that our framework outperforms existing methods in both stability and alignment performance, providing a robust solution to the challenges of RLHF.

\section{Preliminary}

\paragraph{Reinforcement Learning from Human Feedback (RLHF)} is an essential approach to alignment LLMs with human values, especially from the perspectives of helpfulness and harmlessness~\citep{ouyang2022traininglanguagemodelsfollow}. 

RLHF first learns a reward model $r(\vx,\vy)$ from a given collection of human preference data $\mathcal{D}_\text{p}=\{(\vx,\vy_w,\vy_l)\}$, where $\vx$ is a user input prompt, $\vy_w, \vy_l$ are the preferred and rejected responses selected by annotators, respectively. To learn a representative RM, following Bradley-Terry~\cite{bradley1952rank} objective is usually utilized:
\begin{equation}\label{eq:bt-ranking-loss}
     -\mathbb{E}_{(\vx,\vy_w, \vy_l)\sim \gD_\text{p}} \Big[\log \sigma (r(\vx,\vy_w)-r(\vx,\vy_l))\Big],
\end{equation}
where $\sigma(\cdot)$ is the Sigmoid function.
%
Intuitively, \cref{eq:bt-ranking-loss} induces $r(\vx,\vy)$ to assign a higher reward score to the preferred response $\vy_w$ than the rejected response $\vy_l$ with respect to input $\vx$.

 
With a learned RM $r(\vx,\vy)$, RLHF optimizes the target LLM policy $\pi_\theta(\vy|\vx)$ by maximizing the expected reward:
\begin{equation}
\mathbb{E}_{\vx\sim\mathcal{D}, \vy\sim\pi_{\theta}(\vy|\vx)}
[r(\vx,\vy) ]- \beta \text{KL}[\pi_{\theta} \Vert\pi_{\text{ref}}],
\label{eq:RLHF}
\end{equation}
where $\text{KL}[\pi_{\theta} \Vert\pi_{\text{ref}}]$ is the KL divergence~\cite{kullback1951information} between the training policy $\pi_\theta(\vy|\vx)$ with a reference model $\pi_\text{ref}(\vy|\vx)$ to prevent $\pi_{\theta}(\vy|\vx)$ from the degeneration and preserve the generation diversity. $\beta>0$ is a hyper-parameter to re-weight the expected reward and the KL regularization term.

To solve the RLHF objective in \cref{eq:RLHF}, 
 Proximal Policy Optimization (PPO)~\cite{schulman2017proximalpolicyoptimizationalgorithms} has been recognized as the mainstream optimization algorithm~\citep{rafailov2024direct}. However, as mentioned in \cref{sec:introduction}, PPO suffers from training instability and high complexity in computation and implementation~\citep{yuan2023rrhf,cheng2023adversarial}. Therefore, many of recent works have been proposed to simplify and improve the original PPO algorithm. 
%
 \citet{rafailov2024direct} theoretically demonstrate that \cref{eq:RLHF} has a closed-form solution:
\begin{equation}    \label{eq:optimal_solution}
    \pi^*(\vy|\vx) = \frac{1}{Z(\vx)} \pi_\text{ref}(\vy|\vx)\exp\left(\frac{1}{\beta} r(\vx,\vy)\right),
\end{equation}
where $Z(\vx) = \mathbb{E}_{\vy\sim\pi_{\text{ref}}(\vx|\vy)}[\text{exp}(\frac{1}{\beta}r(\vx,\vy))]$ %= \sum_{\vy}\pi_{\text{ref}}(\vy|\vx)\exp\left(\frac{1}{\lambda}r(\vx,\vy)\right)$
is the denominator that normalizes the conditional distribution.
Based on the relation between the optimal policy $\pi^*(\vy|\vx)$ and the RM $r(\vx,\vy)$, \citet{rafailov2024direct} convert the RM learning objective~\cref{eq:bt-ranking-loss} to an optimal policy learning loss named Direct Preference Optimization (DPO):
\begin{equation}\label{eq:DPO}
- \mathbb{E}_{\gD_\text{p}}\Big[\log\sigma\Big( \beta\log \frac{\pi_\theta(\vy_{w}|\vx)}{\pi_\text{ref}(\vy_{w}|\vx)} - \beta \log \frac{\pi_\theta(\vy_{l}|\vx)}{\pi_\text{ref}(\vy_{l}|\vx)} \Big)\Big].
\end{equation}
%
\citet{bahetileftover} adopt the PPO objective into an offline scheme by using importance sampling and converting the expectation of $\pi_\theta(\vy|\vx)$ to the expectation of $\pi_\text{ref}(\vy|\vx)$, then propose Advantage-Leftover-Lunch (A-LoL) gradient estimation: 
\begin{equation}
 - \bbE_{\vx \sim\gD, \vy\sim \pi_\text{ref}(\vy|\vx)} \Big[
    \hat{A}^{\pi_\text{ref}}\cdot \frac{\pi_\theta(\vy|\vx)}{\pi_\text{ref}(\vy|\vx)} \cdot \nabla_\theta \log  \pi_\theta(\vy|\vx)  \Big],
\end{equation}
where $\hat{A}^{\pi_\text{ref}}$ is the estimated advantage value~\citep{schulman2015high} with respect to $\pi_{\bar{\theta}}$, also calculated offline. 


\paragraph{Variational Methods} provide a principled framework for approximating unknown probability distributions by leveraging optimization over a family of tractable parameterized distributions~\cite{kingma2022autoencodingvariationalbayes}. The fundamental idea of variational methods is to reformulate probabilistic inference as a functional optimization problem. More specifically, the goal is to find a surrogate distribution $q_\theta(\vy)$ from a parameterized family $\mathcal{Q} = \{q_\theta| \theta \in \Theta \}$, so that $q_\theta(\vy)$ can best approximates the target unknown distribution $p(\vy)$. This is usually achieved by minimizing the KL divergence $\text{KL} [q_\theta(\vy) \Vert p(\vy)]$ between $q_\theta(\vy)$ and $p(\vy)$. Mathematically, given the unknown target $p(\vy)$, variational methods minimize the following KL to find a distribution $ q_\theta(\vy) $ from a predefined family $ \mathcal{Q} $ that minimizes the KL divergence:
\begin{equation}
    \text{KL}\big(q_\theta \parallel p\big) = \mathbb{E}_{q_\theta(\vy)}\left[ \log \frac{q_\theta(\vy)}{p(\vy)} \right].
    \label{eq:kl_forward}
\end{equation}
This objective encourages $ q_\theta(\vy) $ to concentrate probability mass where $ p(\vy) $ is large. However, directly minimizing \cref{eq:kl_forward} is often intractable, as evaluating $ p(\vy) $ requires computing a normalization constant (e.g., a partition function). To bypass this intractability, variational methods maximize the \textbf{Evidence Lower Bound (ELBO)}~\cite{kingma2022autoencodingvariationalbayes}, derived by rearranging the log-evidence $ \log p(\vx) $:
\begin{align}
    \log p(\vx) &= \log \int p(\vx, \vy) d\vy \nonumber \\
    &\geq \underbrace{\mathbb{E}_{q_\theta(\vy)}\left[ \log \frac{p(\vx, \vy)}{q_\theta(\vy)} \right]}_{\text{ELBO}}.
    \label{eq:elbo}
\end{align}
Maximizing the ELBO is equivalent to minimizing $ \text{KL}\big(q_\theta \parallel p(\vy|\vx)\big) $, where $ p(\vy|\vx) = \frac{p(\vx, \vy)}{p(\vx)} $. The tightness of the bound depends on how well $ q_\theta(\vy) $ approximates the true posterior. In our offline policy optimization setting, the target distribution is the \textit{optimal policy} $\pi^*(\vy|\vx) $ and we seek a parametric policy $ \pi_\theta(\vy|\vx) $ to approximate $ \pi^* $. By minimizing $ \text{KL}\big(\pi^* \parallel \pi_\theta\big) $, we align $ \pi_\theta $ with high-reward regions of $ \pi^* $. The ELBO in this context becomes:
\begin{equation}
\mathbb{E}_{\pi_\theta(\vy|\vx)}\left[ \log \pi_{\text{ref}}(\vy|\vx) + \frac{1}{\lambda} r(\vx, \vy) - \log \pi_\theta(\vy|\vx) \right].
\end{equation}
Variational methods bridge the gap between tractable optimization and probabilistic inference by learning parametric approximations to complex distributions. In our work, this framework justifies using a learnable policy $ \pi_\theta(\vy|\vx) $ to approximate the optimal policy $ \pi^* $ in a reward-weighted manner, while leveraging offline data to estimate the ELBO efficiently through importance sampling.

\section{Method}
\subsection{Motivation}
Existing preference alignment methods exhibit two fundamental limitations. First, clipping-based approaches like PPO~\cite{schulman2017proximalpolicyoptimizationalgorithms} and A-LoL~\cite{bahetileftover} bound the importance ratio $\pi_\theta(\vy|\vx)/\pi_\text{ref}(\vy|\vx)$ within the interval $[1-\epsilon, 1+\epsilon]$. This flattens the reward distinctions between responses with similar values. For instance, when two responses have rewards $R_1 = 100$ and $R_2 = 99$, clipped methods assign nearly identical probabilities ($\sim 1/2$), failing to resolve fine-grained preferences (detailed analysis in \cref{app:clip}).

Second, existing methods that employ negative weights for the to-be-learned policy face intrinsic instability. For example, DPO's treatment of dis-preferred responses and A-LoL's alternative weighted SFT method, where the advantage value  $\hat{A}^{\pi_\text{ref}}$ can be negative, suffer from this issue. Consider the general weighted SFT objective:
\begin{equation}
\mathcal{L}_\text{W-SFT} = -\mathbb{E}\left[w(\vx,\vy)\log\pi_\theta(\vy|\vx)\right].
\end{equation}
When $w(\vx,\vy)$ takes negative values for dis-preferred responses, the loss becomes unbounded below. Minimizing the loss corresponds to maximizing $w(\vx,\vy)\log\pi_\theta(\vy|\vx)$. For negative weights ($w < 0$), this reduces to minimizing $\log\pi_\theta(\vy|\vx)$, creating a non-compact optimization landscape. While perfect performance ($\log\pi_\theta(\vy|\vx) \rightarrow 0$) is theoretically achievable, it is practically unreachable~\cite{gao2023scaling} (detailed analysis in \cref{app:theory_analysis}).

Our key insight is that \textit{reward-driven alignment should operate in the space of positive measures}. We therefore propose a variational method that naturally induces non-negative weights through exponential reward transformation:
\begin{equation}
w(\vx,\vy) \propto \pi_\text{ref}(\vy|\vx)\exp(r(\vx,\vy)/\lambda) > 0.
\end{equation}
This construction guarantees that the loss landscape has well-defined minima bounded by the reference policy's support. By reformulating RLHF as variational inference over positive measures, we achieve stable optimization without artificial clipping or negative weighting.


\subsection{KL Minimization as Variational Inference}
 This closed-form solution motivates our key insight: \textit{preference alignment can be reformulated as variational distribution matching}. Therefore, we formulate the policy optimization as a variational inference problem. In the variational inference paradigm~\cite{jordan1999introduction}, we approximate a complex target distribution (here $\pi^*$) by optimizing within a tractable family of distributions (here $\pi_\theta$). This can be achieved by minimizing the KL divergence between the target and the variational distributions:
\begin{align}
    \text{KL}(\pi^*\Vert \pi_\theta) &= \mathbb{E}_{\pi^*}\left[\log \frac{\pi^*(\vy|\vx)}{\pi_\theta(\vy|\vx)}\right] \\
    &= \underbrace{H(\pi^*)}_\text{Constant} - \mathbb{E}_{\pi^*} [\log \pi_\theta(\vy|\vx)],
\label{approximation_1}
\end{align}
where $H(\cdot)$ is the entropy function over $\pi^*$ and is a constant related to $\pi^*$. For conciseness, we adopt the expectation form of the KL divergence and let $\mathbb{E}_{\pi}$ denote $\mathbb{E}_{y\sim\pi(\vy|\vx)}$. As a result, the approximation of $\pi^*$ under minimizing KL divergence can be achieved by:
\begin{equation}
    \min_\theta\text{KL}(\pi^*\Vert \pi_\theta)  =  \max_\theta \mathbb{E}_{\pi^*} [\log \pi_\theta(\vy|\vx)].
    \label{KL_obj}
\end{equation}
Using importance sampling~\cite{goertzel1949quota,kahn1951estimation,kloek1978bayesian}, which effectively approximates an unknown distribution with a known one, we can rearrange \cref{KL_obj} to obtain the following objective by incorporating \cref{eq:optimal_solution}:
\begin{align}        
    \mathbb{E}_{\pi^*} [\log \pi_\theta(\vy|\vx)] &= \mathbb{E}_{\pi_\text{ref}} \left[\frac{\pi^*(\vy|\vx)}{\pi_\text{ref}(\vy|\vx)} \log \pi_\theta(\vy|\vx)\right] \\
    &=  \bbE_{\pi_\text{ref}} \left[\frac{\exp\left(\frac{1}{\lambda} r(\vx,\vy)\right)}{Z(\vx)} \log \pi_\theta(\vy|\vx)\right].
    \label{approximation_3}
\end{align}

\subsection{In-Batch Normalization Function Estimation}
\cref{approximation_3} implies that the key challenge in effectively approximating $\pi^*$ through a parameterized model $\pi_\theta$ lies in the computation of $Z(\vx)$. However, estimating $Z(\vx)$ involves summing over all possible outputs $y$ for a given $x$, which can be computationally expensive, as mentioned in previous work~\cite{rafailov2024direct}. To avoid directly computing $Z(\vx)$, some alignment methods adopt policy gradient algorithms (e.g., REINFORCE~\cite{williams1992simple} and PPO~\cite{schulman2017proximalpolicyoptimizationalgorithms}) that optimize $\pi_\theta$ without explicitly normalizing over all outputs. Here, we propose a novel approximation of $Z(\vx)$ within a mini-batch $\mathcal{B}$, leveraging the insight of ``cross-context importance sampling''. Specifically, our method uses pre-collected responses $y_j$ associated with inputs $x_j$ in the same batch to estimate $Z(\vx_i)$ efficiently, while implicitly suppressing low-reward responses through normalization. 
Given a batch $\mathcal{B}= \{ (x_i, y_i) \}_{i=1}^B$ and a reward model $r$, we assume each $y_j$ can be sampled from a uniform distribution $P_{\text{ref}}(y)$ instead of $\pi_{\text{ref}(\vy|\vx)}$, and thus estimate $Z(\vx_i)$ for each $\vx_i$ by:
\begin{align}
\small
    Z(\vx_i) &= \bbE_{\text{ref}} \left[\exp\left(\frac{1}{\lambda} r(\vx_i,\vy)\right)\right]  \\
    &= \bbE_{\vy\sim P_\text{ref}(\vy)} \left[ \frac{\pi_\text{ref}(\vy|\vx_i)}{P_\text{ref}(\vy)} \exp\left(\frac{1}{\lambda} r(\vx_i,\vy)\right) \right] \\
    &\approx \frac{1}{B} \sum_{j=1}^B \left[\frac{\pi_\text{ref}((\vy_j|\vx_i)}{P_\text{ref}(\vy_j)}\exp\left(\frac{1}{\lambda} r(\vx_i,\vy_j)\right)\right] \\
    &= \frac{1}{B} \sum_{j=1}^B \left[\frac{\pi_\text{ref}((\vy_j|\vx_i)}{\frac{1}{B}}\exp\left(\frac{1}{\lambda} r(\vx_i,\vy_j)\right)\right] \\
    &= \sum_{j=1}^B \left[\pi_\text{ref}(y_j |x_i) \exp\left(\frac{1}{\lambda} r(\vx_i,\vy_j)\right)\right].
\label{eq:zx}
\end{align}

%
% \begin{figure}[t]\vspace{-1em}
\begin{algorithm}[t]
\small
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\caption{Estimation of $Z(\vx_i)$ and computation of loss.}
\label{alg:main_estimation}
\Input{A mini-batch $\mathcal{B}=\{(\vx_i,\vy_i)\}_{i=1}^{B}$, policy LLM $\pi_{\theta}$ and reference LLM $\pi_{\text{ref}}$, reward model $r$ and a hyper-parameter $\lambda$.}
\Output{loss}
{Initialize $Z \leftarrow [0]\times B$.}\\
{Initialize $\text{loss} \leftarrow [0]\times B$.}\\
\For{$i \leftarrow 1$ \KwTo $B$}{
    {Compute log probability $\log \pi_{\theta}(\vy_i|\vx_i)$ for the pair $(\vx_i, \vy_i)$.}\\
\tcc{Estimate $Z(\vx_i)$ using Eq.~\ref{eq:zx}.}
    \For{$j \leftarrow 1$ \KwTo $B$}{
        {Compute ref. probability $\pi_{\text{ref}}(\vy_j|\vx_i)$ for the pair $(\vx_i, \vy_j)$.}\\
        {Compute the exponent of reward $\text{exp}(\frac{1}{\lambda}r(\vx_i, \vy_j))$ for the pair $(\vx_i, \vy_j)$.}\\
        {Add $\pi_{\text{ref}}(\vy_j|\vx_i)\cdot \text{exp}\left(\frac{1}{\lambda}r(\vx_i, \vy_j)\right)$ to $Z(\vx_i)$.}
    }
    {Compute coefficient $w_i = \frac{\pi_{\text{ref}}(\vy_i|\vx_i)\cdot\text{exp}(\frac{1}{\lambda}r(\vx_i, \vy_i))}{Z(x_i)}$.}\\
    {Update $\text{loss}_{i}\leftarrow -w_i \cdot\log \pi_{\theta}(\vy_i|\vx_i)$.}
}
\end{algorithm}
% \end{figure}


 
\subsection{Reward-Driven Weighted SFT}
Combining the above components, we obtain our final objective:
\begin{equation}
    \mathcal{L} = -\mathbb{E} \bigg [\underbrace{\frac{\pi_\text{ref}(\vy|\vx)\exp\left(\frac{1}{\lambda}r(\vx,\vy)\right)}{Z(\vx)}}_{\text{Variational Weights}} \underbrace{\log\pi_\theta(\vy|\vx)}_{\mathcal{L}_\text{SFT}} \bigg ],
\end{equation}
where each $Z(\vx_i)$ can be approximated by \cref{eq:zx}. By treating the term $\frac{\pi_\text{ref}(\vy|\vx)\exp\left(\frac{1}{\lambda}r(\vx,\vy)\right)}{Z(\vx)}$ as the variational weights, our objective becomes a reward-driven ($r(\vx,\vy)$) weighted SFT. Further analysis of the consistency of our objective with the RLHF objective is shown in \cref{app:consis}. In conclusion, we summarize the algorithm process of our in-batch estimation and loss computation in Alg.~\ref{alg:main_estimation}.
 


\section{Experiments}
To evaluate the effectiveness of our proposed approach, we conducted experiments under two primary settings: (1) the Helpful and Harmless Assistant Task (HHA)~\cite{bai2022training,ganguli2022red}; and (2) generative benchmarks, including MMLU~\cite{hendrycksmeasuring}, HumanEval~\cite{chen2021evaluating}, BigBench-Hard~\cite{srivastava2023beyond}, and GSM8k~\cite{cobbe2021training}.

\subsection{HHA Settings}
\paragraph{Dataset}
Our primary experiment utilizes the HHA dataset, which consists of user-assistant conversations paired with model-generated responses labeled as ``chosen'' or ``rejected'' based on human preferences. This dataset is divided into four subsets: (1) Harmless-base, containing red-teaming conversations designed to elicit harmful responses; (2) Helpful-base, (3) Helpful-online, and (4) Helpful-rejection, which focus on advice- and assistance-seeking interactions. We evaluate our method using the test sets of these subsets, comprising a total of 8.2K test conversations annotated with human preference labels.

For model training\footnote{Implementation details can be found in Appendix~\ref{app:HHA_implementation_details}.}, we employ the OffsetBias~\cite{park2024offsetbias} dataset, a preference dataset similar to HHA. We utilize OffsetBias because our method explicitly relies on reward scores during training. We observed that directly applying the HHA training set and its corresponding reward models often results in inappropriate reward scores, such as instances where the chosen response receives a lower reward score than the rejected response. This issue significantly compromises the effectiveness of our method. In contrast, OffsetBias addresses six common types of biases, including length bias—where models tend to assign higher scores to longer sequences—that can mislead reward models into assigning inaccurate reward scores. By mitigating these biases, OffsetBias provides more robust and reliable reward scores, making it better suited for training our model effectively. For all HHA experiments, we use the full training set of OffsetBias, which consists of 8.5K samples.
%%%%
\paragraph{Models}
To evaluate the scalability of our method, we conducted experiments on two model collections: Llama-\{1B, 3B, 8B, 13B\}\footnote{To ensure the use of the most updated models, we selected Llama3.2-\{1B, 3B\}, Llama3.1-8B, and Llama2-13B.}~\cite{dubey2024llama} and Qwen2.5-\{0.5B, 1.5B, 3B, 7B, 14B, 32B\}~\cite{yang2024qwen2}. Specifically, we benchmark our method against DPO across all models and consider two RL training settings: 1) starting from the pre-trained (base) model; (2) starting from the SFT model.
For the reward model, we employ a popular OffsetBiasRM~\cite{park2024offsetbias} that is trained on the OffsetBias preference dataset. OffsetBiasRM is designed to provide more accurate reward scores by addressing common biases, making it more suitable for our experiments.

% % ****************** Table: llama3 hh reward results ******************
\begin{table*}[t]\vspace{-1em}
\scriptsize
\centering
% \renewcommand{\arraystretch}{1.2} % Adjust row height
\setlength{\tabcolsep}{10pt} % Adjust column separation
% \resizebox{0.8\textwidth}{!}{
\caption{Reward scores obtained by aligning Llama model series using the OffsetBias training set and evaluating on the four subsets of the HHA benchmark. Additionally, we report the test reward on the split test set of OffsetBias. ``Avg. Helpful'' denotes the average reward across Helpful-base, Helpful-online, and Helpful-rejection, while ``Avg. All'' represents the average reward across all four subsets of HHA.}
\begin{tabular}{@{}ll|cccc |ccc@{}}
\toprule[1.5pt]
\multicolumn{2}{c|}{\multirow{2}{*}{Method}} & Harmless & \multicolumn{3}{c|}{Helpful} & \multirow{2}{*}{Avg. Helpful} & \multirow{2}{*}{Avg. All} & \multirow{2}{*}{OffsetBias} \\ 
% \specialrule{粗细}{上间距}{下间距}
\specialrule{0.0em}{0.0ex}{-0.1ex} % 减小行间距
\cmidrule(lr){4-6}
\specialrule{0.0em}{-0.8ex}{0.0ex} % 减小行间距 
&& base & base & online & rejection & \\
\midrule
\multirow{5}{*}{Llama3.2-1B}
&Base   & 37.03 & 20.51 & 24.04 & 21.93 & 22.16$_{\pm 1.34}$ & 25.88$_{\pm 1.59}$ & 21.00 \\
&DPO    & 45.50 & 44.45 & 47.07 & 45.61 & 45.71$_{\pm 0.16}$ & 45.66$_{\pm 0.08}$ & 37.31 \\
&VAR   & 52.48 & 57.35 & 60.58 & 59.38 & 59.10$_{\pm 0.24}$ & 57.44$_{\pm 0.14}$ & 56.81 \\ 
&SFT+DPO   & 56.43 & 64.65 & 64.95 & 65.90 & 65.16$_{\pm 0.43}$ & 62.98$_{\pm 0.30}$ & 59.09 \\
&SFT+VAR & \textbf{60.19} & \textbf{65.96} & \textbf{68.94} & \textbf{68.27} & \textbf{67.72}$_{\pm 0.11}$ & \textbf{65.84}$_{\pm 0.07}$ & \textbf{61.97} \\ \midrule
\multirow{5}{*}{Llama3.2-3B}
&Base   & 35.05 & 26.50 & 31.15 & 28.60 & 28.75$_{\pm 0.38}$ & 30.33$_{\pm 0.14}$ & 26.61 \\ 
&DPO    & 53.71 & 59.38 & 60.04 & 60.55 & 59.99$_{\pm 0.05}$ & 58.42$_{\pm 0.09}$ & 53.94 \\
&VAR   & 57.97 & 60.23 & 64.92 & 62.92 & 62.69$_{\pm 0.04}$ & 61.51$_{\pm 0.08}$ & 60.88 \\ 
&SFT+DPO   & \textbf{64.00} & \textbf{69.44} & 71.01 & \textbf{71.56} & \textbf{70.67}$_{\pm 0.08}$ & \textbf{69.00}$_{\pm 0.05}$ & \textbf{63.81} \\
&SFT+VAR & \textbf{64.00} & 67.93 & \textbf{71.32} & 70.83 & 70.02$_{\pm 0.22}$ & 68.52$_{\pm 0.12}$ & 63.72 \\ \midrule
\multirow{5}{*}{Llama3.1-8B}
&Base   & 38.73 & 34.74 & 39.96 & 37.30 & 37.33$_{\pm 0.65}$ & 37.68$_{\pm 0.46}$ & 30.42 \\ 
&DPO    & 56.17 & 59.89 & 61.02 & 60.86 & 60.59$_{\pm 0.02}$ & 59.48$_{\pm 0.03}$ & 51.38 \\
&VAR   & 57.18 & 61.13 & 65.57 & 64.33 & 63.68$_{\pm 0.20}$ & 62.06$_{\pm 0.11}$ & 63.91 \\ 
&SFT+DPO   & 62.38 & \textbf{69.16} & 70.00 & 70.61 & 69.93$_{\pm 0.07}$ & 68.04$_{\pm 0.11}$ & 60.88 \\
&SFT+VAR & \textbf{63.24} & 68.13 & \textbf{71.13} & \textbf{70.78} & \textbf{70.01}$_{\pm 0.47}$ & \textbf{68.32}$_{\pm 0.51}$ & \textbf{65.75} \\ \midrule
\multirow{5}{*}{Llama2-13B}
&Base   & 33.06 & 27.39 & 29.53 & 28.36 & 28.43$_{\pm 0.04}$ & 29.59$_{\pm 0.13}$ & 27.05 \\ 
&DPO    & 50.52 & 50.01 & 53.68 & 52.24 & 51.98$_{\pm 0.15}$ & 51.61$_{\pm 0.14}$ & 51.75 \\
&VAR   & 58.45 & 58.94 & 62.94 & 61.89 & 61.26$_{\pm 0.24}$ & 60.56$_{\pm 0.25}$ & 61.44 \\ 
&SFT+DPO   & 55.19 & 59.90 & 60.61 & 61.26 & 60.59$_{\pm 0.25}$ & 59.24$_{\pm 0.19}$ & 59.09 \\
&SFT+VAR & \textbf{61.29} & \textbf{63.27} & \textbf{66.07} & \textbf{65.75} & \textbf{65.03}$_{\pm 0.15}$ & \textbf{64.09}$_{\pm 0.09}$ & \textbf{62.59} \\
\bottomrule[1.5pt]
\end{tabular}
% }%
\label{table:llama3-hh-reward}\vspace{-1em}
\end{table*}
% % ****************** Table: llama3 hh reward results ******************
% % ****************** Table: qwen2.5 hh reward results ******************
\begin{table*}[t]\vspace{-1em}
\scriptsize
\centering
% \renewcommand{\arraystretch}{1.2} % Adjust row height
% \resizebox{1\textwidth}{!}{
\setlength{\tabcolsep}{10pt} % Adjust column separation
\caption{Reward scores obtained by aligning Qwen2.5 model series using the OffsetBias training set and evaluating on the four subsets of the HHA benchmark. Additionally, we report the test reward on the split test set of OffsetBias. ``Avg. Helpful'' denotes the average reward across Helpful-base, Helpful-online, and Helpful-rejection, while ``Avg. All'' represents the average reward across all four subsets of HHA.}
\begin{tabular}{@{}ll|cccc |ccc@{}}
\toprule[1.5pt]
\multicolumn{2}{c|}{\multirow{2}{*}{Method}} & Harmless & \multicolumn{3}{c|}{Helpful} & \multirow{2}{*}{Avg. Helpful} & \multirow{2}{*}{Avg. All} & \multirow{2}{*}{OffsetBias} \\ 
% \specialrule{粗细}{上间距}{下间距}
\specialrule{0.0em}{0.0ex}{-0.1ex} % 减小行间距
\cmidrule(lr){4-6}
\specialrule{0.0em}{-0.8ex}{0.0ex} % 减小行间距 
&& base & base & online & rejection & \\
\midrule
\multirow{5}{*}{Qwen2.5-0.5B}
&Base   & 33.03 & 25.44 & 30.86 & 26.94 & 27.75$_{\pm 0.30}$ & 29.06$_{\pm 0.14}$ & 40.38 \\
&DPO    & 55.21 & 55.50 & 56.24 & 56.75 & 56.17$_{\pm 0.53}$ & 55.93$_{\pm 0.45}$ & 53.44 \\
&VAR   & 55.22 & 58.09 & 62.32 & 60.38 & 60.26$_{\pm 0.21}$ & 59.00$_{\pm 0.07}$ & 59.50 \\ 
&SFT+DPO   & 56.42 & 58.02 & 60.38 & 59.91 & 59.44$_{\pm 0.03}$ & 58.68$_{\pm 0.02}$ & 55.88 \\
&SFT+VAR & \textbf{58.22} & \textbf{61.72} & \textbf{63.58} & \textbf{63.56} & \textbf{62.95}$_{\pm 0.14}$ & \textbf{61.77}$_{\pm 0.12}$ & \textbf{60.63} \\ \midrule
\multirow{5}{*}{Qwen2.5-1.5B}
&Base   & 35.01 & 26.18 & 32.11 & 28.13 & 28.81$_{\pm 0.12}$ & 30.36$_{\pm 0.32}$ & 26.52 \\ 
&DPO    & 53.40 & 57.00 & 57.38 & 57.85 & 57.41$_{\pm 0.17}$ & 56.41$_{\pm 0.13}$ & 56.63 \\
&VAR   & 61.33 & 64.72 & 68.87 & 68.01 & 67.20$_{\pm 0.09}$ & 65.73$_{\pm 0.05}$ & 64.75 \\ 
&SFT+DPO   & 54.51 & 61.13 & 61.54 & 62.26 & 61.64$_{\pm 0.32}$ & 59.86$_{\pm 0.28}$ & 57.63 \\
&SFT+VAR & \textbf{62.76} & \textbf{66.07} & \textbf{69.13} & \textbf{68.78} & \textbf{67.99}$_{\pm 0.13}$ & \textbf{66.69}$_{\pm 0.03}$ & \textbf{65.88} \\ \midrule
\multirow{5}{*}{Qwen2.5-3B}
&Base   & 47.07 & 34.27 & 41.86 & 36.61 & 37.58$_{\pm 0.14}$ & 39.96$_{\pm 0.18}$ & 45.06 \\ 
&DPO    & 60.58 & 63.37 & 63.84 & 64.90 & 64.03$_{\pm 0.53}$ & 63.17$_{\pm 0.51}$ & 58.97 \\
&VAR   & 63.30 & 66.29 & 70.07 & 69.32 & 68.56$_{\pm 0.08}$ & 67.24$_{\pm 0.05}$ & \textbf{66.06} \\ 
&SFT+DPO   & 54.61 & 58.45 & 58.69 & 60.19 & 59.11$_{\pm 0.17}$ & 57.98$_{\pm 0.17}$ & 52.63 \\
&SFT+VAR & \textbf{65.15} & \textbf{67.86} & \textbf{71.24} & \textbf{70.78} & \textbf{69.96}$_{\pm 0.06}$ & \textbf{68.75}$_{\pm 0.05}$ & 65.63 \\ \midrule
\multirow{5}{*}{Qwen2.5-7B}
&Base   & 42.49 & 42.02 & 47.96 & 44.21 & 44.73$_{\pm 0.53}$ & 44.17$_{\pm 0.28}$ & 54.09 \\ 
&DPO    & 61.51 & 66.09 & 66.50 & 67.28 & 66.62$_{\pm 0.18}$ & 65.35$_{\pm 0.10}$ & \textbf{65.81} \\
&VAR   & 64.86 & 66.41 & \textbf{70.87} & \textbf{69.78} & \textbf{69.02}$_{\pm 0.09}$ & \textbf{67.98}$_{\pm 0.03}$ & 65.38 \\ 
&SFT+DPO   & 60.78 & 62.02 & 61.62 & 62.67 & 62.10$_{\pm 0.25}$ & 61.77$_{\pm 0.30}$ & 56.91 \\
&SFT+VAR & \textbf{64.96} & \textbf{66.46} & 69.81 & 69.39 & 68.55$_{\pm 0.11}$ & 67.65$_{\pm 0.05}$ & 65.38 \\ \midrule
\multirow{5}{*}{Qwen2.5-14B}
&Base   & 39.29 & 32.73 & 39.03 & 35.16 & 35.64$_{\pm 0.08}$ & 36.55$_{\pm 0.27}$ & 45.16 \\ 
&DPO    & 55.83 & 55.63 & 59.04 & 57.61 & 57.43$_{\pm 0.32}$ & 57.03$_{\pm 0.26}$ & 62.94 \\
&VAR   & 64.24 & 65.23 & 69.79 & 68.50 & 67.84$_{\pm 0.14}$ & 66.94$_{\pm 0.03}$ & 65.63 \\ 
&SFT+DPO   & \textbf{66.97} & \textbf{67.83} & 68.16 & 68.94 & 68.31$_{\pm 0.42}$ & 67.97$_{\pm 0.38}$ & \textbf{67.94} \\
&SFT+VAR & 66.37 & 67.74 & \textbf{71.70} & \textbf{71.08} & \textbf{70.17}$_{\pm 0.21}$ & \textbf{69.22}$_{\pm 0.10}$ & 66.44 \\ \midrule
\multirow{5}{*}{Qwen2.5-32B-Int4}
&Base   & 38.80 & 34.36 & 39.13 & 36.78  & 36.77 & 37.27 & 38.97 \\ 
&DPO    & 37.09 & 31.38 & 34.58 & 33.04  & 33.00 & 34.02 & 32.38 \\
&VAR   & 50.03 & 45.36 & 51.77 & 47.77  & 48.30 & 48.73 & 57.69 \\ 
&SFT+DPO   & 37.95 & 27.70 & 28.63 & 28.16  & 28.16 & 30.61 & 30.23 \\
&SFT+VAR & \textbf{53.18} & \textbf{49.07} & \textbf{55.66} & \textbf{51.90} &  \textbf{52.21} & \textbf{52.45} &  \textbf{59.09} \\
\bottomrule[1.5pt]
\end{tabular}
% }%
\label{table:qwen2.5-hh-reward}\vspace{-1em}
\end{table*}
% % ****************** Table: qwen2.5 hh reward results ******************
\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\linewidth]{figures/aligned_model_v_sft_target-llama-v2.png}
    \caption{GPT-4 evaluation results on the HHA test set for the Llama series, reporting average win rates. Error bars are calculated across three different random seeds.}
    \label{fig:llama-win-rate}\vspace{-1em}
\end{figure*}

\paragraph{Evaluation}
By following previous work~\cite{rafailov2024direct, bahetileftover}, we adopt two popular evaluation strategies: 1) \textbf{Reward Score}: A higher reward score usually indicates more useful and helpful response with respective to the input. Specifically, we use the OffsetBiasRM reward model~\cite{park2024offsetbias} to calculate reward scores for sequences generated by the aligned models on the HHA test set. Additionally, we also evaluate reward scores on the split OffsetBias evaluation set to assess the in-distribution ability of the models. 2) \textbf{Pairwise Winrate Score}: Following the common practice of LLM-as-a-judge, we utilize GPT4 and adopt the evaluation template from MT-Bench~\cite{zheng2023judgingllmasajudgemtbenchchatbot}. To alleviate potential positional bias, we present the responses of two models to the judge in two different orders and compare their scores. A model is considered to win only if it does not lose in both orderings. Specifically, we define: \textit{Wins}: Outperforms in both orderings or wins in one and ties in the other. \textit{Tie}: Ties in both orderings or wins in one and loses in the other. \textit{Loses}: Lags in both orderings or ties in one and loses in the other.


\subsection{HHA Results}

\paragraph{Reward Evaluation}
\cref{table:llama3-hh-reward} present the reward scores on the HHA test sets. \textit{DPO} and \textit{VAR} denote models trained directly from the \textit{Base} model (pre-trained only), while \textit{SFT+} refers to models first fine-tuned via SFT and then further fine-tuned on the SFT model. From \cref{table:llama3-hh-reward}, we observe that our method outperforms DPO in both \textit{Avg. Helpful} and \textit{Avg. All} across all Llama models for the \textit{base} version, as well as for the \textit{SFT+} version, except for Llama3.2-3B, where it shows a marginal decrease of around 0.5\% compared to DPO. Additionally, our method achieves comparable results whether trained directly from the base model or the SFT model, particularly for larger LLMs such as Llama3.1-8B and Llama2-13B, whereas DPO struggles to achieve strong results when starting from the base model. Moreover, our method achieves performance comparable to the RLHF objective in a single training step, resembling the simplicity and efficiency of SFT. \cref{table:qwen2.5-hh-reward} further demonstrates the scalability of our method across different model sizes. Our approach consistently outperforms DPO across all average reward scores on both HHA and OffsetBias, even when starting from the base model. For Qwen2.5-32B, due to limited resources, we employ 4-bit quantization for training. Nevertheless, our method maintains its advantage over DPO.

% ****************** Table: llama3-instruct hh results ******************
\begin{table*}[t]\vspace{-1em}
\scriptsize
\centering
% \setlength{\tabcolsep}{5pt} % Adjust column separation
\caption{Winrate results for the Llama3 series instruct versions (1B, 3B, and 8B) on the HHA benchmark.}
\resizebox{0.99\textwidth}{!}{
\begin{tabular}{@{}ll|ccc|ccc|ccc|ccc|cc@{}}
\toprule[1.5pt]
\multicolumn{2}{c|}{\multirow{2}{*}{Method}} & \multicolumn{3}{c|}{Harmless-base} & \multicolumn{3}{c|}{Helpful-base} & \multicolumn{3}{c|}{Helpful-online} & \multicolumn{3}{c|}{Helpful-rejection} & \multicolumn{2}{c}{Avg. Winrate} \\
&& A win & B win & Tie & A win & B win & Tie & A win & B win & Tie & A win & B win & Tie & All & Helpful \\ \midrule
\multirow{2}{*}{Llama3.2-1B} 
& DPO & 20.2 & 59.6 & 20.2 & 49.49 & 32.32 & 18.18 & 12.46 & 71.38 & 16.16 & 34.68 & 47.47 & 17.84 & 29.21$_{\pm 0.17}$ & 32.21$_{\pm 0.41}$ \\
& VAR & \textbf{55.56} & 34.34 & 10.10 & \textbf{72.73} & 16.50 & 10.77 & \textbf{48.48} & 42.42 & 9.09 & \textbf{69.36} & 19.86 & 10.77 & \textbf{61.53}$_{\pm 0.22}$ & \textbf{63.53}$_{\pm 0.30}$ \\ \midrule
\multirow{2}{*}{Llama3.2-3B} 
& DPO & \textbf{76.10} & 20.54 & 3.37 & 76.77 & 14.81 & 8.42 & 49.83 & 44.10 & 6.06 & 62.97 & 25.59 & 11.45 & 66.42$_{\pm 0.58}$ & 63.19$_{\pm 0.74}$ \\
& VAR & 63.30 & 26.26 & 10.44 & \textbf{89.90} & 3.70 & 6.40 & \textbf{61.96} & 32.66 & 5.39 & \textbf{79.13} & 12.12 & 8.75 & \textbf{73.57}$_{\pm 0.95}$ & \textbf{76.99}$_{\pm 0.98}$ \\ \midrule
\multirow{2}{*}{Llama3.1-8B} 
& DPO & 42.42 & 42.42 & 15.15 & 68.69 & 19.19 & 12.12 & 33.33 & 54.21 & 12.46 & 53.20 & 31.65 & 15.15 & 49.41$_{\pm 0.17}$ & 51.74$_{\pm 0.11}$ \\
& VAR & \textbf{51.52} & 33.67 & 14.81 & \textbf{88.55} & 8.08 & 3.37 & \textbf{58.59} & 34.34 & 7.07 & \textbf{85.86} & 10.44 & 3.70 & \textbf{71.13}$_{\pm 0.34}$ & \textbf{77.67}$_{\pm 0.30}$ \\
\bottomrule[1.5pt]
\end{tabular}
}%
\label{table:llama3-instruct-hh}\vspace{-1em}
\end{table*}
% ****************** Table: llama3-instruct hh results ******************




\paragraph{Winrate Evaluation}
\cref{fig:llama-win-rate} present the win rates evaluated by GPT-4o for answers generated by aligned models compared to the SFT targets (chosen answers in the test set) for the Llama series. The Llama series show results consistent with the reward scores, where our method outperforms DPO across all models except Llama2-13B, which achieves comparable results. These findings further demonstrate that our method can achieve the RLHF objective in a single SFT-like step without the need for resource-intensive reinforcement learning. We provide additional results on Qwen series in \cref{app:additional_winrate_eval}.


\cref{table:llama3-instruct-hh} shows results on the Llama series instruct versions (i.e., models after RLHF). Our method outperforms DPO by a large margin across three scales (1B, 3B, and 8B) and consistently achieves higher win rates across all subsets of the HHA benchmark. This demonstrates the robustness of our method across models at different training stages, including those that have already undergone RLHF alignment. The results highlight the effectiveness of our approach in further refining and aligning models with human preferences, even when starting from pre-aligned instruct versions.


\begin{figure*}[!t]
    \centering
    \subfigure[Llama3.2-1B]{\includegraphics[width=0.3\textwidth]{figures/llama3.2_1b_val_reward.png}}
    \subfigure[Llama3.2-3B]{\includegraphics[width=0.3\textwidth]{figures/llama3.2_3b_val_reward.png}}
    \subfigure[Llama3.1-8B]{\includegraphics[width=0.3\textwidth]{figures/llama3.1_8b_val_reward.png}}
    \caption{Average validation reward during the training process for (a) Llama3.2-1B, (b) Llama3.2-3B, and (c) Llama3.1-8B on the OffsetBias dataset, comparing DPO and our method.}
    \label{fig:val-reward}\vspace{-1em}
\end{figure*}

%%%%%%%%%%%%%%%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/length.png}
    \caption{Average output length for aligned Llama3.1-8B on the HHA testset.}
    \label{fig:seq-length}\vspace{-1em}
\end{figure}
%%%%%%%%%%%%%%%%%

\paragraph{Training Stability}
\cref{fig:val-reward} illustrates the average validation reward during the training process for three Llama model collections. Comparing our method with DPO, we observe that DPO exhibits greater volatility and tends to decline from the early training steps. In contrast, our method demonstrates a gradual increase in validation reward, ultimately reaching a consistent level. This indicates that our approach is more robust over longer training steps and maintains a more stable training process compared to DPO.

\paragraph{Output Sequence Length Analysis}
We calculate the average output sequence lengths for models aligned on Llama3.1-8B across the four subsets of HHA, as shown in~\cref{fig:seq-length}. Starting from the base models, DPO generates longer sequences than our method. When starting from SFT, our models maintain output sequence lengths similar to the SFT version, while DPO produces sequences approximately twice as long as ours and the SFT version. Longer sequences tend to achieve higher reward scores and GPT-based scores~\cite{bahetileftover,ethayarajh2024ktomodelalignmentprospect}. Despite this, our method outperforms DPO in most reward evaluations and winrate evaluations.


\subsection{Generative Benchmark}
\label{sec:benchmark}
\paragraph{Settings}
Following prior settings~\cite{tunstall2023zephyr,ethayarajh2024ktomodelalignmentprospect}, we utilize UltraFeedback~\cite{cui2023ultrafeedback} as the training dataset. UltraFeedback is a large-scale preference dataset collected from diverse sources, where multiple LLMs generate four distinct responses for each prompt. The dataset comprises 64k prompts, resulting in 256k samples. Additionally, it includes GPT-4-evaluated scores for instruction-following, truthfulness, honesty, and helpfulness. For our experiments, we sampled 10k prompts, selecting the highest average-scored samples for training SFT, OURS, and ALoL, and using the highest-worst score pairs for training DPO. For training, we utilize Llama2-7B and Qwen2.5-7B as the base models. For comparison, we benchmark our method against ALoL and DPO. As for the reward model, we employ OffsetBiasRM for both ALoL and our method, with the same setting in HHA experiments.

%%%%%%%%%%%%%%%%%%%%%%%%% Table: benchmark %%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
\scriptsize
\centering
\setlength{\tabcolsep}{6pt}
\caption{Benchmark results for different methods on Llama2-7B and Qwen2.5-7B models. The table reports evaluation results on MMLU, GSM8k, HumanEval, and BBH benchmarks, along with the average performance (Avg.) across all benchmarks.}
\begin{tabular}{@{}ll|cccc|c@{}}
\toprule[1.5pt]
\multicolumn{2}{c|}{\multirow{2}{*}{Method}} & MMLU  & GSM8k  & HumanEval  & BBH  & \multirow{2}{*}{Avg.} \\ 
&& EM & EM & pass@1 & EM \\ \midrule
\multirow{4}{*}{Llama2-7B} 
& Base  & 37.46  & 1.90  & 3.05  & 12.77  & 13.79  \\
% & Rej. Samp. \\
& DPO   & 32.45  & 4.55  & 7.32  & \textbf{39.10} & 20.86  \\
& ALoL  & 35.78  & 4.09  & 12.80  & 38.16  & 22.71 \\
& VAR  & \textbf{38.57}  & \textbf{6.67}  & \textbf{14.02}  & 37.56 & \textbf{24.20}  \\ \midrule
\multirow{4}{*}{Qwen2.5-7B} 
& Base  & 67.13  & \textbf{86.13}  & 64.63  & 29.30  & 61.80  \\
% & Rej. Samp. \\
& DPO   & 68.64  & 74.53  & 33.54  & 53.06 & 57.44   \\
& ALoL  & 68.62  & 64.97  & 54.88  & 61.14 & 62.40   \\
& VAR  & \textbf{69.11}  & 74.30  & \textbf{68.90}  & \textbf{61.35}  & \textbf{68.42}   \\
\bottomrule[1.5pt]
\end{tabular}
% }%
\label{table:bench}
\vspace{-2em}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%% Table: benchmark %%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Generative Benchmark Results}
\cref{table:bench} presents the results of different methods on Llama2-7B and Qwen2.5-7B models across four benchmarks: MMLU, GSM8k, HumanEval, and BBH. For Llama2-7B, our method consistently outperforms both DPO and ALoL across most benchmarks, achieving the highest average results. On Qwen2.5-7B, our method also demonstrates strong results, achieving the best performance on multiple benchmarks while maintaining competitive results on others, highlighting the robustness and effectiveness  across various settings.
\subsection{Ablation Study}  
As per \cref{eq:zx}, we estimate $Z(\vx_i)$ with a mini - batch of data, making batch size $B$ crucial for training. We conduct an ablation study using batch sizes 2, 4, and 8 under the settings in \cref{sec:benchmark}, with UltraFeedback as the training set. \cref{table:ablation} shows the impact of different $B$ values on model performance. The model performs best at $B = 8$, but the improvement over $B = 4$ is just 0.14\%, suggesting that a larger $B$ can slightly boost performance. Thus, we use $B = 8$ for all experiments. However, our method is also robust to batch-size changes, which gives satisfactory results even at $B = 2$, showing stability and suitability for resource-constrained situations. 

%%%%%%%%%%%%%%%%%%%%%%%%% Table: Ablation %%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[h]
\scriptsize
\centering
\setlength{\tabcolsep}{11pt}
\caption{Ablation study of different batch sizes $B$ on Qwen2.5-7B.}
\begin{tabular}{@{}l|cccc|c@{}}
\toprule[1.5pt]
\multirow{2}{*}{$B$} & MMLU  & GSM8k  & HumanEval  & BBH & \multirow{2}{*}{Avg.} \\ 
& EM & EM & pass@1 & EM & \\ \midrule
2  & \textbf{69.59}  & 72.55  & 65.24  & \textbf{62.57} & 67.49 \\
4  & 69.09  & \textbf{75.44}  & 68.29  & 60.32 & 68.28 \\
8  & 69.11  & 74.30  & \textbf{68.90}  & 61.35 & \textbf{68.42} \\
\bottomrule[1.5pt]
\end{tabular}
\label{table:ablation}
\vspace{-1em}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%% Table: Ablation %%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Additional Analysis}
We provide case study in \cref{app:case_study}, from which we can observe our method effectively produce more comprehensive and helpful response with higher quality compared to the baseline methods.



\section{Related Work}
Aligning LLMs with human preferences has evolved from studies on RLHF, aiming to achieve human-aligned outcomes~\cite{stiennon2020learning,ouyang2022traininglanguagemodelsfollow,bai2022training,lee2023rlaif}. The RLHF process typically begins with SFT, followed by further fine-tuning to maximize expected reward scores. This requires the construction of a reward model based on the Maximum Likelihood Estimation (MLE) of the BT model to provide such reward scores. This fine-tuning process is referred to as RLHF, with the PPO algorithm being the most widely applied~\cite{schulman2017proximalpolicyoptimizationalgorithms}. A series of works focus on self-training, where the workflow involves sampling online data from the model and training it using a two-player min-max game between two policies~\cite{rosset2024directnashoptimizationteaching,swamy2024minimaximalistapproachreinforcementlearning,chen2024selfplayfinetuningconvertsweak}. However, the use of online data in the learning process presents significant challenges, as it requires substantial computational resources and limits training efficiency.
%
To address these challenges, researchers have shifted their focus to offline preference alignment learning algorithms. These methods operate in a single stage and directly optimize a designed loss function to achieve optimal preference alignment based on pairwise datasets~\cite{zhao2023slic,rafailov2024direct,azar2024general,ethayarajh2024ktomodelalignmentprospect,xu2024contrastivepreferenceoptimizationpushing}. Another approach to alleviate the resource-intensive nature of training is proposed by Remax~\cite{li2024remaxsimpleeffectiveefficient}, which introduces a variance reduction method for LLMs.
%
Most closely related to our work is ALoL~\cite{bahetileftover}, which formulates the reinforcement learning process at the sequence level and derives its advantage-based offline objective. Unlike ALoL, which relies on clipping operations to ensure training stability, we formulate our method by directly approaching the optimal solution of RLHF, thereby achieving a more precise solution to the RLHF objective.

\section{Conclusion}
In this paper, we proposed a reward-driven variational alignment framework to address the limitations of existing RLHF methods, such as instability from negative weights and suboptimal performance due to clipping. By reformulating RLHF as a variational problem over positive measures, our approach ensures a stable optimization landscape and derives a reward-driven weighted SFT loss through KL divergence minimization. The introduction of an efficient in-batch normalization technique further enables scalable and practical implementation. Experimental results demonstrate improved alignment performance and training stability, offering a robust and effective solution for preference alignment in RLHF.


\section*{Impact Statement}
Our work presents a reward-driven variational alignment framework that overcomes key challenges in RLHF, such as instability from negative weights and suboptimal clipping effects. By reformulating RLHF as a variational problem over positive measures, our method offers a stable and efficient optimization framework for aligning language models with human preferences, enhancing their reliability and scalability in applications like conversational AI, content moderation, and personalized recommendations. While our focus is on improving alignment performance and training stability, we recognize the broader societal implications, including risks of bias amplification, misuse of generative capabilities, and ethical concerns around automating human-like decision-making. We emphasize the need for ongoing research to address these challenges and ensure responsible deployment of RLHF advancements.



%%%%%%%%%%%%%%%% 到这里8页 %%%%%%%%%%%%%%



\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Future Work}
We will explore several promising directions to further enhance our framework. First, we aim to develop an online version of our method, enabling real-time interaction for calculating $Z(\vx)$ and updating the policy $\pi_\theta$ dynamically. Second, we plan to conduct extensive experiments across a broader range of tasks, such as multi-turn dialogue and long-form text generation, to validate the generalizability of our approach. Finally, scaling our framework to larger models and testing on more diverse and noisy preference datasets will provide deeper insights into its scalability and robustness. 




\section{Theoretical Analysis}
\label{app:theory_analysis}



\subsection{Loss Bound Analysis}
\paragraph{Lower Bound of Positive Weighted Loss}
\begin{theorem}
For any policy $\pi_\theta(\vy|\vx)$ satisfying $\sum_y \pi_\theta(\vy|\vx)=1$ and weights $w(\vx,\vy)>0$, the weighted SFT loss satisfies:
\begin{equation}
\mathcal{L}(\theta) = -\mathbb{E}_{x,y}\left[w(x,y)\log\pi_\theta(\vy|\vx)\right] \geq 0,
\end{equation}
with equality if and only if $\pi_\theta(\vy|\vx) = \delta_{y=y^*}$ where $\delta_{y=y^*}$ is the optimal policy when $y^* = \arg\max_y w(x,y)$.
\end{theorem}

\begin{proof}
Using the inequality $\log z \leq z - 1$ for $z > 0$:
\begin{align}
\mathcal{L}(\theta) &= -\mathbb{E}[w\log\pi_\theta] \\
&\geq -\mathbb{E}[w(\pi_\theta - 1)] \quad (\text{since } \log\pi_\theta \leq \pi_\theta - 1) \\
&= \mathbb{E}[w(1 - \pi_\theta)] \\
&\geq 0 \quad (\text{since } w>0 \text{ and } \pi_\theta \leq 1).
\end{align}

The equality holds when $\pi_\theta(\vy|\vx)=1$ for the $y$ maximizing $w(x,y)$.
\end{proof}


\paragraph{Unboundedness under Negative Weights}
\begin{theorem}
If there exists a pair $(\vx_0, \vy_0)$ such that $w(\vx_0, \vy_0) < 0$, then the loss $\mathcal{L}(\theta)$ is unbounded below:
\begin{equation}
\inf_\theta \mathcal{L}(\theta) = -\infty.
\end{equation}
\end{theorem}

\begin{proof}
Consider the loss component corresponding to the pair $(\vx_0, \vy_0)$:
\begin{equation}
\mathcal{L}_{y_0}(\theta) = -w(\vx_0, \vy_0) \log \pi_\theta(\vy_0 | \vx_0).
\end{equation}
Since $w(\vx_0, \vy_0) < 0$, let $|w(\vx_0, \vy_0)| = -w(\vx_0, \vy_0)$. Then:
\begin{equation}
\mathcal{L}_{y_0}(\theta) = |w(\vx_0, \vy_0)| \cdot \log \pi_\theta(\vy_0 | \vx_0).
\end{equation}
As $\pi_\theta(\vy_0 | \vx_0) \to 0^+$, we have $\log \pi_\theta(\vy_0 | \vx_0) \to -\infty$, and thus:
\begin{equation}
\mathcal{L}_{y_0}(\theta) \to |w(\vx_0, \vy_0)| \cdot (-\infty) = -\infty.
\end{equation}

Next, adjust the policy $\pi_\theta$ such that $\pi_\theta(\vy_0 | \vx_0)$ approaches 0 while maintaining normalization. For example, for some $\vy_1 \neq \vy_0$, set:
\begin{equation}
\pi_\theta(\vy_0 | \vx_0) = \epsilon, \quad \pi_\theta(\vy_1 | \vx_1) = 1 - \epsilon, \quad \epsilon \to 0^+.
\end{equation}
In this case, $\mathcal{L}_{y_0}(\theta)$ tends to $-\infty$, while the behavior of other loss terms is as follows:

(1) For $\vy \neq \vy_0$ and $\vy \neq \vy_1$, if $w(\vx, \vy) \geq 0$, the corresponding loss term is:
   \begin{equation}
   -w(\vx, \vy) \log \pi_\theta(\vy | \vx) \geq 0.
   \end{equation}
   If $\pi_\theta(\vy | \vx) \to 0$, these terms tend to $+\infty$. However, if their probabilities are kept at 0, the corresponding loss terms are 0.
   
(2) For $\vy_1$, if $w(\vx_1, \vy_1) \geq 0$, the corresponding loss term is:
   \begin{equation}
   -w(\vx_1, \vy_1) \log(1 - \epsilon) \approx w(\vx_1, \vy_1) \cdot \epsilon \to 0 \quad (\text{as } \epsilon \to 0).
   \end{equation}

In summary, the total loss $\mathcal{L}(\theta)$ can be decomposed as:
\begin{equation}
\mathcal{L}(\theta) = \underbrace{|w(\vx_0, \vy_0)| \cdot \log \epsilon}_{\to -\infty} + \underbrace{w(\vx_1, \vy_1) \cdot \epsilon}_{\to 0} + \text{other bounded terms}.
\end{equation}
Thus, $\mathcal{L}(\theta) \to -\infty$ as $\epsilon \to 0$, which implies $\inf_\theta \mathcal{L}(\theta) = -\infty$.
\end{proof}


\subsection{Analysis of Clip Operator and Policy Distinction}
\label{app:clip}
The clip operator:
$$\text{clip}(r(\theta,ref),1-\epsilon,1+\epsilon),$$
commonly used in methods like ALoL to stabilize training, bounds the importance ratio $r(\theta,\text{ref})=\frac{\pi_\theta(y_i|x)}{\pi_{\text{ref}}(y_i|x)}$ within $[1-\epsilon, 1+\epsilon]$. While effective in controlling gradient variance, clipping introduces bias by flattening reward distinctions between responses with similar values.

For instance, suppose for a given instruction $x$, we have a set of answers $\{y_1,y_2,...,y_n\}$, and the loss function of R-LoL (R-LoL used here as a simple example; the only difference between A-LoL and R-LoL is replacing $r(\vx,\vy)$ in R-LoL to $A(x,y) = r(\vx,\vy) - V(x)$) with clipping is:
\begin{align}
\mathcal{L}_\text{R-LoL} &= \sum_{i=1}^n \left(R(x,y_i) \cdot \text{clip}(r(\theta,ref),1-\epsilon,1+\epsilon)\log\pi_\theta(\vy|\vx) - \beta\log\pi_\theta(y_i|x)\right)\\
&=\sum_{i=1}^n(R(x,y_i)\cdot \text{clip}(r(\theta,ref),1-\epsilon,1+\epsilon)-\beta)\log\pi_\theta(y_i|x).
\end{align}
When $\epsilon$ is small, we assume that the parameter $\theta$ in the function $r(\theta,ref)$ is frozen when we do the update of the policy model $\pi_\theta$, i.e., the loss function becomes: 
\begin{equation}
\sum_{i=1}^n(R(x,y_i)\cdot \text{clip}(r(\theta_1,ref),1-\epsilon,1+\epsilon)-\beta)\log\pi_\theta(y_i|x),
\end{equation}
and we first update $\theta$, then when we do the next iteration we set $\theta_1 = \theta$. We denote $\eta_i = R(x,y_i)\cdot \text{clip}(r(\theta_1,ref),1-\epsilon,1+\epsilon)-\beta$ and we can see that $\eta_i\approx R(x,y_i)$ when $\epsilon,\beta$ are small. 

We write $\alpha_i = \frac{\eta_i}{\sum_{j=1}^n\eta_j}$, and since the denominator is independent of $x$, we just need to optimize
\begin{equation}
\mathcal{L}_\text{R-LoL} = \sum_{i=1}^n\alpha_i\log\pi_\theta(y_i|x),
\end{equation}
and $\sum\alpha_i = 1$. By using the Lagrange's method, construct (for simplicity, write $z_i=\pi_\theta(y_i|x)$)
\begin{equation}
F(z_1,z_2,...,z_n,\lambda) = \sum_{i = 1}^n\alpha_i\log z_i - \lambda(\sum_{i=1}^nz_i - 1),
\end{equation}
and the partial derivatives are
\begin{equation}
\frac{\partial F}{\partial z_i} = \frac{\alpha_i}{z_i} - \lambda,\frac{\partial F}{\partial \lambda} = -(\sum_{i=1}^nz_i - 1).
\end{equation}
Hence we can see that the optimal solution of the A-LoL loss is: (BY solving the partial derivatives the optimal solution must have the same distribution as $\alpha_i$)
\begin{equation}
\pi^*(y_i|x)/\pi^*(\{y_1,...,y_n\}|x)=\frac{R(x,y_i)\cdot \text{clip}(r(\theta_1,ref),1-\epsilon,1+\epsilon)-\beta}{\sum_{j=1}^n(R(x,y_j)\cdot \text{clip}(r(\theta_1,ref),1-\epsilon,1+\epsilon)-\beta)}.
\end{equation}
So we can see that for close rewards responses, this optimal solution will not distinguish their distributions. For example, if we have two responses $y_1,y_2$ with $R_1,R_2$ as their rewards, then the A-LoL method will give $\frac{R_1}{R_1+R_2}$ and $\frac{R_2}{R_1+R_2}$ as the optimal solution and could be closed to $1/2$ if $R_1/R_2$ is closed to $1$ (e.g. $R_1 = 100,R_2 = 99$). However, for our method, it will distinguished by $\exp(R_1-R_2)$, i.e. for $R_1 = 100,R_2 = 99$, our method gives us $\frac{e}{e+1}$ and $\frac{1}{e+1}$, which seems better.  



\subsection{Additional Winrate Evaluation on Qwen-series}\label{app:additional_winrate_eval}
\cref{fig:Qwen-win-rate} present the win rates evaluated by GPT-4o for answers generated by aligned models compared to the SFT targets (chosen answers in the test set) for the Qwen collections. our method outperforms DPO across all Qwen models except Qwen2.5-14B, where it shows a slight decrease. The Qwen collections exhibit slightly different trends compared to the reward scores, with our method starting from the base version outperforming the SFT+ version across scales from 0.5B to 14B and achieving comparable results at 32B. These findings further demonstrate that our method can achieve the RLHF objective in a single SFT-like step without the need for resource-intensive reinforcement learning.


\section{Implementation Details}\label{app:implementation_details}
\subsection{HHA Settings}\label{app:HHA_implementation_details}
For all comparisons, we ensure consistent settings between our method and DPO. We select learning rates of 1e-5 and 5e-6, employing the AdamW optimizer with a cosine learning rate scheduler. For the Winrate evaluation, we randomly sample 99 instances from each of the four subsets of the HHA test set and conduct experiments using three different random seeds to compute the mean and standard error. To mitigate potential positional bias in GPT-4's preferences, we randomly shuffle the positions of model-generated sequences and the SFT target during evaluation using \texttt{gpt-4o-2024-11-20}. For the Reward evaluation, we use the entire HHA test set, evaluating across three different random seeds to compute the mean and standard error. We employ generation parameters $\tau = 0.8$, $\text{top\_p} = 0.9$ and $\text{top\_k} = 50$ for all the generations. All models are trained on 4$\times$A100-80GB GPUs, with Llama3-8B and Qwen2.5-14B utilizing 8$\times$A100-80GB. Due to resource constraints, Qwen2.5-32B is trained using 4-bit quantization (bnb-4bit)\footnote{\url{https://huggingface.co/docs/bitsandbytes}} across 4 nodes, each with 8$\times$A100-80GB GPUs.


\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\linewidth]{figures/aligned_model_v_sft_target-qwen-v2.png}
    \caption{GPT-4 evaluation results on the HHA test set for the Qwen series, reporting average win rates. Error bars are calculated across three different random seeds.}
    \label{fig:Qwen-win-rate}
\end{figure*}




\subsection{Generative Benchmark Settings}\label{app:generative_implementation_details}
For all experiments, we use $4\times8$ A100-80GB GPUs, training with a learning rate of 5e-6 and the AdamW optimizer combined with a cosine learning rate scheduler for exactly two epochs. We evaluate the aligned models using the OpenCompass~\cite{2023opencompass} toolkit, with the following benchmarks: GSM8K (4-shot), MMLU (0-shot), HumanEval (0-shot), and BBH (3-shot chain-of-thought), following the default settings from OpenCompass. For GSM8K, MMLU, and BBH, we use exact match (EM) as the evaluation metric, while for HumanEval, we use pass@1.


\section{Consistency Proof of Our Objective with RLHF Optimal Solution}
\label{app:consis}
\begin{proof}
Our objective is to minimize:  
\begin{equation}
\mathcal{L} = -\mathbb{E} \left[ \frac{\pi_{\text{ref}}(\vy|\vx) \exp\left(\frac{1}{\lambda} r(\vx,\vy)\right)}{Z(\vx)} \log \pi_\theta(\vy|\vx) \right],
\end{equation}  
subject to $ \sum_y \pi_\theta(\vy|\vx) = 1 $. Using Lagrange multiplier method \cite{bertsekas2014constrained} with constant $\lambda$, we construct:  
\begin{equation}
F(\pi_\theta, \lambda) = \sum_y \frac{\pi_{\text{ref}}(\vy|\vx) \exp\left(\frac{1}{\lambda} r(\vx,\vy)\right)}{Z(\vx)} \log \pi_\theta(\vy|\vx) - \lambda \left( \sum_y \pi_\theta(\vy|\vx) - 1 \right).
\end{equation} 
Taking the derivative with respect to $ \pi_\theta(\vy|\vx) $ and setting it to zero:  
\begin{equation}
\frac{\partial F}{\partial \pi_\theta(\vy|\vx)} = \frac{\pi_{\text{ref}}(\vy|\vx) \exp\left(\frac{1}{\lambda} r(\vx,\vy)\right)}{Z(\vx) \pi_\theta(\vy|\vx)} - \lambda = 0.
\end{equation}
Solving for $ \pi_\theta(\vy|\vx) $, we obtain:  
\begin{equation}
\pi_\theta(\vy|\vx) = \frac{\pi_{\text{ref}}(\vy|\vx) \exp\left(\frac{1}{\lambda} r(\vx,\vy)\right)}{\lambda Z(\vx)}.
\end{equation} 
Since $ \lambda $ is a normalization constant, it can be absorbed into $ Z(\vx) $, yielding:  
\begin{equation}
\pi_\theta(\vy|\vx) = \frac{\pi_{\text{ref}}(\vy|\vx) \exp\left(\frac{1}{\lambda} r(\vx,\vy)\right)}{Z(\vx)},
\end{equation}
which matches our starting point:  
\begin{equation}
\pi^*(\vy|\vx) = \frac{1}{Z(\vx)} \pi_{\text{ref}}(\vy|\vx) \exp\left(\frac{1}{\lambda} r(\vx,\vy)\right).
\end{equation} 
\end{proof}




\section{Case Study}\label{app:case_study}
\cref{tab:case-study} compares responses from SFT, SFT+VAR, SFT+DPO, Base+VAR, and Base+DPO, along with their corresponding reward scores. Notably, the SFT+DPO response is excessively verbose and includes irrelevant details, yet it receives the highest reward score. This highlights a tendency of the reward model to favor longer sequences, even when the content is less helpful or accurate. In contrast, SFT+VAR provides a concise and accurate answer, demonstrating the effectiveness of our method in balancing response quality and length. Additionally, Base+DPO generates a highly structured but overly complex and repetitive response, while Base+VAR delivers a clear and straightforward answer. Importantly, Base+VAR achieves performance comparable to SFT+VAR, showing that our method can effectively align models even when starting from the base (pre-trained only) version, without the need for SFT.

\begin{table}[ht]
    \small
    \centering
    \caption{Sample generations from different aligned versions of Llama3.1-8B for a prompt about checking a car's gas tank capacity.}
    \begin{tabular}{lp{13cm}c}
    \toprule
    \textbf{Prompt} & How can I look up how much gas my car can hold? & \textbf{Reward}\\
    \midrule
    \textbf{SFT} & You can find out how much fuel your vehicle holds by checking the owner's manual or looking on the door jamb (where some vehicles display this information). You may also be able to use an online resource, such as Edmunds.com, which has detailed specifications for most cars.  & 31.36 \\
    \midrule
    \textbf{SFT+VAR} & You can find out how much gas your car can hold by looking at the vehicle's owner's manual or contacting the manufacturer. The amount of gas your car can hold will vary depending on the make and model, as well as any modifications that have been made to it.  & 31.69 \\
    \midrule
    \textbf{SFT+DPO} & The total amount of gas that a car can hold will depend on the type and size of the car, as well as local environmental conditions and safety regulations. To find out exactly how much gas a specific car can hold, it is best to check with the car manufacturer or local fuel suppliers. \newline In general, larger cars such as 4x4s and vans can store more gas than smaller cars like compact and economy cars. Most modern vehicles have a fuel tank capacity of between 50-120 litres (depending on vehicle size), but some heavy-duty vehicles may have larger tanks of 120-130L or more. \newline It's also important to note that many countries have limits on how much LPG or CNG a vehicle can store due to safety concerns - for example, most countries limit the storage of LPG to no more than 20kg, while CNG is usually limited to 70m3 (for light duty vehicles) and 100 m3 (for heavy duty vehicles). If you're not sure what these limits are in your country, please check with the relevant authorities or consult the car manufacturer. In addition, certain areas may restrict the use of alternative fuels such as LPG and CNG during air pollution season or other times when air quality is deemed unhealthy ...  & 60.75 \\
    \midrule
    \textbf{Base+VAR} & You can lookup the capacity of your car's fuel tank by consulting your vehicle's manual or by contacting the manufacturer. Alternatively, you can estimate the capacity based on the size and shape of the fuel tank. & 28.58 \\ \midrule
    \textbf{Base+DPO} & Sure, there is an easy way to look this up:\newline 1. Look up the cars make and model - This will give us its total capacity\newline 2. Find its fuel tank size - This will tell us what type of vehicle it is\newline 3. Check if it has any additional tanks or containers that may be used for gas\newline 4. Calculate the vehicles maximum capacity\newline 5. Add up any additional tanks or containers that may be used for gas\newline 6. Subtract from the maxiumum capacity anything not yet accounted for\newline 7. Finally, we'll add back any additional tanks or containers that were previously subtracted from\newline 8. Finally, we'll add back any additional tanks or containers that were previously subtracted from\newline 9. Finally, we'll calculate the final maximum capacity\newline 10. We'll now want to check that our calculated maximum capacity makes sense.\newline Let's go through each step:\newline 1: Make and model - To find out the vehicle's make and model (and year), let's run a Google search.\newline The results should include the following information about the vehicle:\newline - Make/model/trim level\newline - Fuel economy\newline - Transmission\newline - Cargo volume\newline - Total width\newline - Wheelbase\newline - Ground clearance\newline - Body style\newline - Exterior color scheme ... & 52.33 \\
    \midrule
    \end{tabular}
    \label{tab:case-study}
\end{table}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
