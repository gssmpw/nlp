\section{Related Work}
Aligning LLMs with human preferences has evolved from studies on RLHF, aiming to achieve human-aligned outcomes~\cite{stiennon2020learning,ouyang2022traininglanguagemodelsfollow,bai2022training,lee2023rlaif}. The RLHF process typically begins with SFT, followed by further fine-tuning to maximize expected reward scores. This requires the construction of a reward model based on the Maximum Likelihood Estimation (MLE) of the BT model to provide such reward scores. This fine-tuning process is referred to as RLHF, with the PPO algorithm being the most widely applied~\cite{schulman2017proximalpolicyoptimizationalgorithms}. A series of works focus on self-training, where the workflow involves sampling online data from the model and training it using a two-player min-max game between two policies~\cite{rosset2024directnashoptimizationteaching,swamy2024minimaximalistapproachreinforcementlearning,chen2024selfplayfinetuningconvertsweak}. However, the use of online data in the learning process presents significant challenges, as it requires substantial computational resources and limits training efficiency.
%
To address these challenges, researchers have shifted their focus to offline preference alignment learning algorithms. These methods operate in a single stage and directly optimize a designed loss function to achieve optimal preference alignment based on pairwise datasets~\cite{zhao2023slic,rafailov2024direct,azar2024general,ethayarajh2024ktomodelalignmentprospect,xu2024contrastivepreferenceoptimizationpushing}. Another approach to alleviate the resource-intensive nature of training is proposed by Remax~\cite{li2024remaxsimpleeffectiveefficient}, which introduces a variance reduction method for LLMs.
%
Most closely related to our work is ALoL~\cite{bahetileftover}, which formulates the reinforcement learning process at the sequence level and derives its advantage-based offline objective. Unlike ALoL, which relies on clipping operations to ensure training stability, we formulate our method by directly approaching the optimal solution of RLHF, thereby achieving a more precise solution to the RLHF objective.