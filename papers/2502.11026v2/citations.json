[
  {
    "index": 0,
    "papers": [
      {
        "key": "stiennon2020learning",
        "author": "Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F",
        "title": "Learning to summarize with human feedback"
      },
      {
        "key": "ouyang2022traininglanguagemodelsfollow",
        "author": "Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      },
      {
        "key": "lee2023rlaif",
        "author": "Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie and Mesnard, Thomas and Bishop, Colton and Carbune, Victor and Rastogi, Abhinav",
        "title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "schulman2017proximalpolicyoptimizationalgorithms",
        "author": "John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov",
        "title": "Proximal Policy Optimization Algorithms"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "rosset2024directnashoptimizationteaching",
        "author": "Corby Rosset and Ching-An Cheng and Arindam Mitra and Michael Santacroce and Ahmed Awadallah and Tengyang Xie",
        "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences"
      },
      {
        "key": "swamy2024minimaximalistapproachreinforcementlearning",
        "author": "Gokul Swamy and Christoph Dann and Rahul Kidambi and Zhiwei Steven Wu and Alekh Agarwal",
        "title": "A Minimaximalist Approach to Reinforcement Learning from Human Feedback"
      },
      {
        "key": "chen2024selfplayfinetuningconvertsweak",
        "author": "Zixiang Chen and Yihe Deng and Huizhuo Yuan and Kaixuan Ji and Quanquan Gu",
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhao2023slic",
        "author": "Zhao, Yao and Joshi, Rishabh and Liu, Tianqi and Khalman, Misha and Saleh, Mohammad and Liu, Peter J",
        "title": "Slic-hf: Sequence likelihood calibration with human feedback"
      },
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      },
      {
        "key": "azar2024general",
        "author": "Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele",
        "title": "A general theoretical paradigm to understand learning from human preferences"
      },
      {
        "key": "ethayarajh2024ktomodelalignmentprospect",
        "author": "Kawin Ethayarajh and Winnie Xu and Niklas Muennighoff and Dan Jurafsky and Douwe Kiela",
        "title": "KTO: Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "key": "xu2024contrastivepreferenceoptimizationpushing",
        "author": "Haoran Xu and Amr Sharaf and Yunmo Chen and Weiting Tan and Lingfeng Shen and Benjamin Van Durme and Kenton Murray and Young Jin Kim",
        "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "li2024remaxsimpleeffectiveefficient",
        "author": "Ziniu Li and Tian Xu and Yushun Zhang and Zhihang Lin and Yang Yu and Ruoyu Sun and Zhi-Quan Luo",
        "title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "bahetileftover",
        "author": "Baheti, Ashutosh and Lu, Ximing and Brahman, Faeze and Le Bras, Ronan and Sap, Maarten and Riedl, Mark",
        "title": "Leftover Lunch: Advantage-based Offline Reinforcement Learning for Language Models"
      }
    ]
  }
]