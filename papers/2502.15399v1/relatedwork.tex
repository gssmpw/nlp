\section{Related Work}
\label{sec:related_work}




There is a substantial body of literature dedicated to sampling. One research direction discusses the theory of sampling. For example, Berndt~\cite{berndtSamplingMethods2020} explains the advantages and disadvantages of different sampling methods. Likewise, Etikan et al.~\cite{etikanComparisonConvenienceSampling2016} conduct a comparative analysis between convenience and purposive sampling. These theoretical discussions informed our discussion of sampling methods in Section~\ref{sec:background}.

Another line of research examines sampling in different research methodologies~\cite{cashSamplingDesignResearch2022}, research fields~\cite{amirThereNoRandom2018, baltesSamplingSoftwareEngineering2022}, or a synthesis of both~\cite{hieblSampleSelectionSystematic2023}. Our work is in the category of sampling within a research field, i.e., cloud benchmarking.

The term \textit{sampling} is not common in cloud benchmarking, but appears under the "disguise" of other terms, such as "selection of workloads"~\cite{muhlbauerAnalysingImpactWorkloads2023}. Research in cloud benchmarking mostly deals with workload or benchmark design, often as benchmark suites~\cite{rajputEdgefaasbenchBenchmarkingEdge2022, baurleCombFlexibleApplicationoriented2022, wenCharacterizingCommodityServerless2023}. However, the rationale behind how researchers select and execute benchmarks remains largely unexplored.
The closest work to our research is that of Baltes and Ralph~\cite{baltesSamplingSoftwareEngineering2022} on sampling in software engineering. Similarly to recent cloud computing research, software engineering (SE) research shows limited use of probability sampling (15.3\% in SE vs. our 0.9\% in cloud computing for benchmark selection), ambiguities in sampling descriptions (24\% vs. 13\%), and a lack of transparency. However, their work does not address sampling in benchmark execution, and the areas of sample size or sample availability.

In the field of benchmarking, Bartz-Beielstein et al.~\cite{bartz-beielsteinBenchmarkingOptimizationBest2020} discuss best practices and formulate guidelines for researchers on benchmark selection. Their recommendations entail the properties of diversity, representativeness, scalability, and known solutions. Together with our proposed sampling guidelines, they achieve synergistic goals: Bartz-Beielstein et al. ensure that benchmarks are technically sound and diverse, while our guidelines ensure that the process of selecting and executing benchmarks is transparent.

To the best of our knowledge, in cloud benchmarking, sampling guidelines have not yet been formulated prior to our work. The closest work is by Papadopoulos et al.~\cite{papadopoulosMethodologicalPrinciplesReproducible2019}, who propose eight principles for reproducible performance evaluation in cloud computing. These principles primarily address designing experiments and reporting results. Interestingly, their open access principle aligns with our G4 guideline, and their emphasis on comprehensive experimental setup description is similar to our guideline on transparent sampling description (G2).