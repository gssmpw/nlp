\newpage
\appendix
\onecolumn

The appendix is structured into multiple sections, each offering supplementary information and further clarification on topics discussed in the main body of the manuscript. 

\startcontents[sections]  % 开始定义附录目录
\printcontents[sections]{}{1}{\setcounter{tocdepth}{3}}  % 打印附录目录
\vskip 0.2in
\hrule

\section{More Details for Method}\label{appendix_method}
\subsection{More Details for Outlier-aware Weighting}\label{app:outlier}
\paragraph{Interpretation of Dual Objectives for outlier weighting}
The mathematical framework achieves cross-model consensus and intra-model saliency through its hierarchical thresholding mechanism:

(i) \textbf{Cross-Model Consensus}:
The denominator in Eq. (3) normalizes each model's contribution by the total sparse outlier magnitude across all $n$ models:
\begin{equation}
    \sum_{j=1}^n \sum_{c=1}^{d_l} \|\textsc{Threshold}(\bm{\Delta}_{l,c}^{(j)}, \mu_c^{(j)}+3\sigma_c^{(j)})\|_1
\end{equation}
This forces models with greater sparse deviation magnitudes (potential task conflicts) to receive proportionally reduced aggregation weights $\alpha_l^{(i)}$, effectively suppressing outlier-dominated models in the merged output.

(ii) \textbf{Intra-Model Saliency}:
The $3\sigma$ threshold in $\textsc{Threshold}(\bm{\Delta}_{l,c}^{(i)}, \mu_c^{(i)}+3\sigma_c^{(i)})$ implements statistical outlier detection within each model's parameter distribution. For Gaussian-distributed $\Delta_{l,c,k}^{(i)}$ (per Central Limit Theorem), this retains only the top 0.3\% extreme deviations that likely correspond to:
\begin{itemize}
    \item Task-specific knowledge carriers ($\Delta > \mu+3\sigma$)
    \item Catastrophic interference sources ($\Delta < \mu-3\sigma$)
\end{itemize}
The $L_1$ norm aggregation $\sum_{c=1}^{d_l}\|\cdot\|_1$ then amplifies layers containing concentrated outlier parameters.

\textbf{Synergistic Effect}: The normalization in (i) prevents any single model's outliers from dominating the merger, while the saliency detection in (ii) preserves critical task-specific features within each model. This dual mechanism reduces interference by selectively blending statistically significant parameters across models.

\subsection{More Details for the Reasonability of R-TSVM}
\label{ssec:analysis}

Building on TSVM's theoretical framework, our method provides enhanced guarantees through statistical awareness and adaptive computation.

\paragraph{Conflict Probability Bound}
Let $p_{\text{conflict}}^{(l)}$ denote the probability of directional conflicts in layer $l$. Our rank adaptation yields as follows. We can observe that , compared to TSVM's fixed $\frac{1}{\sqrt{d_l}}$, our bound adapts to layer sparsity.

\begin{equation}
    \mathbb{E}[p_{\text{conflict}}^{(l)}] \leq \frac{1}{\sqrt{k_l}} \propto \frac{1}{\sqrt{d_l(1-\gamma s_l)}}
\end{equation}

\paragraph{Weight Concentration}
The 3$\sigma$ thresholding induces weight concentration on critical parameters. For any layer $l$:

\begin{equation}
    \frac{\mathbb{V}[w_l^{(i)}]}{\mathbb{E}[w_l^{(i)}]^2} \leq \frac{1}{\|\mathcal{T}_{3\sigma}(\bm{\tau}_l^{(i)})\|_0}
\end{equation}

This variance-to-mean ratio decreases as outliers become sparser, stabilizing training.

\begin{table}[ht]
    \centering
    \footnotesize  % 使用更小字号
    \setlength{\tabcolsep}{5pt}  % 压缩列间距
    \caption{Theoretical Comparison between our reweight optimization and TSVM.}
    \label{tab:theory}
    \begin{tabular}{@{} l >{\centering\arraybackslash}p{1.8cm} >{\centering\arraybackslash}p{2cm} @{}} % 自定义列宽
        \toprule
        \textbf{Property}    & \textbf{TSVM} & \textbf{R-TSVM}                    \\
        \midrule
        Layer adaptivity     & $\times$      & $\checkmark$                       \\
        Sparsity awareness   & $\times$      & $\checkmark$                       \\
        Conflict bound       & $O(d^{-1/2})$ & $O(d^{-1/2}(1{-}\gamma s)^{-1/2})$ \\
        Weight concentration & Uniform       & Heavy-tailed                       \\
        Comp.\ complexity    & $O(d^3)$      & $O(k d^2)$                         \\
        \bottomrule
    \end{tabular}
\end{table}

% \subsection{Orthogonal Subspace Mechanism}
% The key to TSVM's conflict resolution lies in the orthogonality of singular vectors:
% \begin{equation}
% \langle \bm{u}_r^{(i)}, \bm{u}_s^{(j)} \rangle \approx 0 \quad (r \neq s)
% \label{eq:orthogonality}
% \end{equation}
% This property enables:
% \begin{itemize}
%     \item \textbf{Constructive Alignment}: Shared directions ($\bm{u}_r^{(i)} \approx \bm{u}_r^{(j)}$) are amplified.
%     \item \textbf{Conflict Suppression}: Orthogonal directions ($\bm{u}_r^{(i)} \perp \bm{u}_s^{(j)}$) are naturally attenuated.
% \end{itemize}


\subsection{Order of Orthogonalization and Rank Truncation/Selection}
\label{sec:order}

A critical design choice in our R-TSVM algorithm lies in the sequential relationship between orthogonalization (Eq.~\ref{seek_orthogonal_u}-\ref{seek_orthogonal_v}) and rank truncation (Eq.~\ref{rank_selection}). Through theoretical analysis and empirical validation, we establish that \textbf{orthogonalization should precede truncation} to ensure optimal subspace alignment and information preservation. This ordering stems from three fundamental considerations:
\textbf{Global Orthogonality Constraints}: The orthogonal projection in Eq.~\ref{seek_orthogonal_u} minimizes the Frobenius norm difference $\| {U_{l}}_\bot - U_{l} \|_F$ under strict orthogonality constraints. Performing this projection \textit{before} truncation preserves the complete singular vector structure, enabling accurate modeling of cross-task interference patterns. Early truncation would discard directional components essential for constructing the orthogonal basis, particularly when task-specific updates exhibit heterogeneous rank distributions.

\textbf{Dynamic Rank Adaptation}: Our sparsity-adaptive rank selection (Eq.~\ref{rank_selection}) requires layer-wise sparsity measurement $\Omega_l$, computed from the full parameter deviation matrix $\bm{\Delta}_l^{(i)}$. Truncating $\bm{\Delta}_l^{(i)}$ prematurely would bias $\Omega_l$ by excluding contributions from low-magnitude parameters, thereby undermining the adaptive rank calculation. As shown in Algorithm~\ref{alg:OWLM}, orthogonalization (Step~4) utilizes the full-rank SVD decomposition to maintain statistical fidelity.

\textbf{Outlier Weighting Integrity}: The outlier-aware weighting mechanism (Eq.~6) operates on the complete parameter deviation matrix to identify statistically significant updates. Truncation prior to outlier detection would risk eliminating subtle yet critical features masked within lower-rank components, particularly in layers with heavy-tailed parameter distributions.

\section{More Details for Related Work}\label{more_relatedwork}
\subsection{Discussion with the Alignment Tax.}
\begin{figure}[htbp]
    \centering
        \setlength{\abovecaptionskip}{0cm}   %调整图片标题与图距离
    \setlength{\belowcaptionskip}{0cm}   %调整图片标题与
    \includegraphics[width=0.99\linewidth]{fig/subsequent_DPO.pdf} % 调整宽度为栏宽的 90%
    \caption{Illustration of Training Stage of 3H Optimization, which aims to further enhance LLMs alignment from three perspectives based on the existing Initially Aligned LLMs.}
    \label{fig:3H_stage}
\end{figure}
We would like to further clarify the main difference between 3H trade-off and previously defined alignment tax \cite{lin2024mitigating,lu2024online}. In general, the alignment tax describes the phenomenon of RLHF training leading to \emph{the forgetting of pre-trained abilities during the first alignment stage}. However, as shown in Figure \ref{fig:3H_stage}, we mainly focus on how can we further \emph{enhance the 3H-related abilities of the existing already-aligned model during the second or subsequent stages.} The trade-off mainly comes from the conflict of different alignment objects without dealing with the pre-trained knowledge. Take the Llama3 series for example, alignment tax mainly analyzes the pre-trained ability degradation on the SFT version of the Base LLM (e.g. train the Llama-3-8B on the Ultrachat) while performing DPO training, which refers to the \textbf{green arrow} of the Figure \ref{fig:3H_stage}. However, in this paper, we mainly focus on how can we further enhance the 3H-related abilities of the existing already aligned model (e.g. Llama3-8B-Instruct) during the second or subsequent alignment stages (\textbf{orange arrow} of the Figure \ref{fig:3H_stage}), which can meet more strict demands for specific applications.

\subsection{Discussion with the MOE-Based Merging Methods (e.g. H3 fusion).} To further distinguish our work from previous ones and strengthen our contribution, we provide more detailed discussions about the MOE-Based Merging methods \cite{zhao2024loraretriever,zhao2024merging,zhou2025mergeme}. Specifically, most of MOE-based merging works, such as SMILES \cite{tang2024smile}, Free-Merging \cite{zheng2024free}, and Twin-Merging \cite{zheng2024free}, aim to balance the performance and deployment costs through modular expertise identification and integration adapted to the input data, which is not designed for our setting about 3H optimization in LLM alignment. 

Recently,  we have noticed a concurrent MOE-fusion work called H3 fusion \cite{tekin2024h} related to our theme. It includes three main steps:(i) Adopt the instruction tuning and summarization fusion as two modern ensemble learning in the context of helpful-harmless-honest (H3) alignment (ii) \textbf{Merge} the aligned model weights with an expert router \textbf{according to the type of
input} instruction and dynamically select a subset of experts. (iii) Utilize the gating loss and regularization terms to enhance performance. But our work mainly focuses on how can we address the conflict issued for 3H optimization to construct a multi-object aligned LLM rather than dynamically adapted to the input data. Simultaneously, considering that the constraints of data availability and data leak will limit the generalization of existing merging methods for LLMs, in the paper we mainly adopt the well-known and latest \textbf{training-
free and data-free} merging strategies for dense LLM, while H3 fusion needs the data for training and only utilizes the merging techniques for efficiently adapting to the input data. Thus, \textbf{H3 fusion is indeed different from our work from the perspective of problem and technique contributions.}

% (e.g.as SMILES \cite{tang2024smile}, WEMOE \cite{shen2024efficient}, Free-Merging \cite{zheng2024free}, Twin-Merging \cite{zheng2024free}, H3 fusion \cite{cf208ab262f0affc4f5581b9a18901265d9728ab}}
% Notably, those mixtures of experts(MOE) based merging methods, such as SMILES \cite{tang2024smile}, WEMOE \cite{shen2024efficient}, Free-Merging \cite{zheng2024free} and Twin-Merging \cite{zheng2024free} are not designed for our settings, we provide discussions in the Appendix.


\section{More Details for Experiments}\label{training_details}
\subsection{The Training Details for Model Constructions and Baselines}
\textbf{Training hyperparameters for model constructions:} following SimPO \cite{meng2024simpo}, based on Llama-3-8B-Instruct and Mistral-7B-Instruct-V2, we conduct preference optimization adopting the fixed batch size 128 for 1 epoch training with the Adam optimizer. We set the max sequence length to 4096 and apply a cosine learning rate schedule with 10 percent warmup steps for each dataset. Specially, we adjust $\beta \in \left[ 0.1, 0.5, 1.0, 2.0 \right]$ and learning rate $lr \in \left[3e-7,5e-7 \right]$ for model constructions and report the best individual training models corresponding to different annotation dimensions.


\textbf{The Implementation of Baselines:} For Heuristic data mixture methods, we control the ratio between Honesty\&Harmlessness and Helpfulness to 1/5,1/10 and 1/20 by default and report the best average score (usually 1/10 according to our experiments). For ArmoRM, we follow the process of SimPO \cite{meng2024simpo} to achieve refined full mixture data. For hummer \cite{jiang2024hummer}, we refine the alignment dimension conflict (ADC) among preference datasets leveraging the powerful ability of AI feedback(e.g. GPT4) as the paper stated. For the full mixture datasets of Table \ref{tab:dpo_data_stats}, we control the ADC lower than 20 percent.




\textbf{Computation environment:} All of our experiments in this paper were conducted on 16×A100 GPUs based on the LLaMA-Factory \cite{zheng2024llamafactory}, MergeKit \cite{goddard2024arcee} and fusion\_bench \cite{tang2024fusionbench}.


\textbf{Reproducibility:} We have made significant efforts to ensure the reproducibility of our work. Upon acceptance, we will release all of the trained models and the complete training and testing code to facilitate the full reproducibility of our results. We are committed to advancing this work and will provide updates on its accessibility in the future. 



\subsection{The Evaluation Details for the Judged Models}\label{evaluation_datails}
We provide detailed descriptions for the evaluation that needs the judged models. For MT-Bench, we report scores following its evaluation
protocol to grade single answers from 1 to 10 scores assisted by GPT4. For HaluEval-Wild, given prompts to our trained model, we utilize the judged model to check whether the output of our trained model is a hallucination or not and then calculate the no hallucination rate. Similarly, we utilize the prompts from SaladBench and OR-Bench to instruct our trained models and then let the judged models check whether the replies of our trained models are safe/unsafe or refusal/answer. Based on the check results, we can naturally calculate the safe score and refusal score by counting all results. The detailed descriptions of the evaluation can be shown in Table \ref{tab:evaluation_comparison}.  More details can be shown in the original paper.

\begin{table}[ht]
    \centering
    \footnotesize  % 使用更小字号
    \setlength{\tabcolsep}{5pt}  % 压缩列间距
    \caption{Evaluation details corresponding judge models, scoring types, and metrics.}
    \label{tab:evaluation_comparison}
    \begin{tabular}{@{} l l c c c @{}} % 自定义列宽
        \toprule
        \textbf{Evaluation Datasets} &\textbf{Examples} & \textbf{Judge Models} & \textbf{Scoring Type} & \textbf{Metrics} \\
        \midrule
         MT-Bench \cite{zheng2023judging}          & 80    & GPT-4  & Single Answer Grade  & Rating of 1-10 \\
         HaluEval-Wild \cite{zhu2024halueval}         & 500   & GPT4   & Classify \& Calculate Ratio  & Rating of 0-100 \\
         SaladBench \cite{li2024salad}        & 1817  & MD-Judge-V0.2 & Classify \& Calculate Ratio  & Rating of 0-100 \\
         OR-Bench \cite{cui2024or}           & 1319  & GPT4-o & Classify \& Calculate Ratio  & Rating of 0-100 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table*}
    % \setlength{\abovecaptionskip}{0cm}
    % \setlength{\belowcaptionskip}{0cm}
    \captionsetup{font={small,stretch=1.25}, labelfont={bf}}
    \renewcommand{\arraystretch}{1}
    \caption{\textbf{3H Results on Mistral Under Continuous Optimization Setting where we sequentially perform DPO training using data with annotations about Helpfulness\&Honesty (Stage1), Helpfulness\&Harmlessness (Stage2) and Helpful (Stage3).For merging methods, we highlight the best score in bold and the second score with underlining.}}
    \label{continuous_mistral}
    \centering
    \resizebox{0.99\textwidth}{!}{
        \begin{tabular}{lcccccccc|c|cc|cccc}
            \toprule
            \multirow{2}{*}{\textbf{Methods}} & \multicolumn{8}{c|}{\textbf{Helpfulness}} & \textbf{Honesty} & \multicolumn{2}{c|}{\textbf{Harmlessness}} & \multirow{2}{*}{\textbf{Helpful\_Avg}} & \multirow{2}{*}{\textbf{Honest\_Avg}} & \multirow{2}{*}{\textbf{Harmless\_Avg}} & \multirow{2}{*}{\textbf{AVG}}                                                                                                          \\ \cmidrule{2-12}
                                              & Math                                      & GSM8K            & ARC-E                                      & ARC-C                                  & MMLU                                  & MBPP\_Plus                              & HumanEval\_Plus               & MT-Bench & HaluEval\_Wild & Salad\_Bench(↑) & OR-Bench(↑) &       &       &       &                   \\ \midrule
            \textbf{Mistral-7B-Instruct-V2}   & 9.54                                      & 46.17            & 82.36                                      & 72.88                                  & 59.97                                 & 26.46                                   & 28.66                         & 7.55     & 62.17           & 78.07           & 74.68       & 41.70 & 62.17 & 76.38 & 60.08             \\ \midrule
            Continual DPO Training Stage1     & 8.76                                      & 43.14            & 82.01                                      & 74.92                                  & 59.78                                 & 25.93                                   & 27.33                         & 7.59     & 61.33           & 78.74           & 77.23       & 41.18 & 61.33 & 77.99 & 60.17             \\
            Continual DPO Training Stage2     & 9.26                                      & 36.16            & 82.54                                      & 75.59                                  & 60.38                                 & 29.88                                   & 33.33                         & 7.86     & 56.40           & 82.76           & 78.54       & 41.88 & 56.40 & 80.65 & 59.64             \\
            Continual DPO Training Stage3     & 9.60                                      & 40.49            & 82.54                                      & 77.29                                  & 60.51                                 & 26.25                                   & 34.15                         & 7.46     & 57.40           & 80.77           & 83.16       & 42.29 & 57.40 & 81.97 & 60.52             \\ \midrule
            Weight Average                    & 10.04                                     & 45.72            & 82.36                                      & 75.25                                  & 61.03                                 & 26.46                                   & 31.71                         & 7.56     & 59.20           & 78.02           & 81.43       & 42.52 & 59.20 & 79.73 & 60.48             \\
            Rewarded Soup                     & 9.72                                      & 46.02            & 82.19                                      & 75.25                                  & 61.03                                 & 26.46                                   & 32.93                         & 7.61     & 58.60           & 77.94           & 81.34       & 42.65 & 58.60 & 79.64 & 60.30             \\
            Model Stock                       & 9.74                                      & 47.69            & 82.36                                      & 73.56                                  & 59.77                                 & 24.87                                   & 27.44                         & 7.68     & 61.00           & 78.51           & 76.44       & 41.64 & 61.00 & 77.48 & 60.04             \\
            Task Arithmetic                   & 9.76                                      & 43.06            & 82.54                                      & 75.93                                  & 61.27                                 & 25.66                                   & 32.93                         & 7.46     & 57.80           & 78.32           & 82.35       & 42.33 & 57.80 & 80.34 & 60.15             \\
            Ties                              & 10.48                                     & 41.55            & 84.66                                      & 76.27                                  & 61.60                                 & 26.19                                   & 30.49                         & 7.46     & 53.80           & 78.99           & 85.43       & 42.34 & 53.80 & 82.21 & 59.45             \\
            DARE                              & 10.40                                     & 42.99            & 85.36                                      & 75.93                                  & 61.54                                 & 24.60                                   & 33.54                         & 7.54     & 56.00           & 78.81           & 85.21       & 42.74 & 56.00 & 82.01 & 60.25             \\
            DARE Ties                         & 10.28                                     & 42.00            & 85.01                                      & 76.27                                  & 61.61                                 & 27.25                                   & 32.32                         & 7.43     & 53.00           & 79.17           & 86.50       & 42.77 & 53.00 & 82.84 & 59.54             \\
            DELLA                             & 10.18                                     & 43.14            & 84.83                                      & 75.25                                  & 61.46                                 & 26.46                                   & 31.71                         & 7.58     & 55.25           & 79.35           & 86.04       & 42.58 & 55.25 & 82.70 & 60.18             \\
            DELLA  Ties                       & 10.50                                     & 40.18            & 85.89                                      & 77.97                                  & 61.37                                 & 30.16                                   & 30.48                         & 7.30     & 54.80           & 79.90           & 87.49       & 42.98 & 54.80 & 83.70 & \underline{60.49} \\
            Breadcrumbs                       & 10.56                                     & 42.53            & 84.83                                      & 75.59                                  & 64.50                                 & 24.60                                   & 32.32                         & 7.53     & 52.40           & 79.42           & 84.34       & 42.81 & 52.40 & 81.88 & 59.03             \\
            Breadcrumbs Ties                  & 10.54                                     & 42.46            & 84.66                                      & 76.95                                  & 61.47                                 & 26.72                                   & 29.88                         & 7.45     & 53.40           & 79.80           & 84.57       & 42.52 & 53.40 & 82.19 & 59.37             \\
            TSVM                              & 10.52                                     & 41.25            & 85.28                                      & 77.21                                  & 61.57                                 & 29.22                                   & 30.48                         & 7.55     & 54.95           & 79.90           & 87.49       & 42.89 & 54.95 & 83.70 & \textbf{60.51}    \\


            \bottomrule
        \end{tabular}
    }
\end{table*}

\begin{table*}
  % \setlength{\abovecaptionskip}{0cm}
  % \setlength{\belowcaptionskip}{0cm}
  \captionsetup{font={small,stretch=1.25}, labelfont={bf}}
  \renewcommand{\arraystretch}{1}
  \caption{\textbf{The detailed 3H Results on Llama3 Under Static Optimization Setting adopting our proposed reweighting-based optimization.}}
  \label{detailed_reweight_llama3}
  \centering
  \setlength{\tabcolsep}{2pt}
  \resizebox{0.99\textwidth}{!}{
    \begin{tabular}{lcccccccc|c|cc|cccc}
      \toprule
      \multirow{2}{*}{\textbf{Methods}}      & \multicolumn{8}{c|}{\textbf{Helpfulness}} & \textbf{Honesty} & \multicolumn{2}{c|}{\textbf{Harmlessness}} & \multirow{2}{*}{\textbf{Helpful\_Avg}} & \multirow{2}{*}{\textbf{Honest\_Avg}} & \multirow{2}{*}{\textbf{Harmless\_Avg}} & \multirow{2}{*}{\textbf{AVG}}                                                                                                          \\ \cmidrule{2-12}
                                             & Math                                      & GSM8K            & ARC-E                                      & ARC-C                                  & MMLU                                  & MBPP\_Plus                              & HumanEval\_Plus               & MT-Bench & HaluEval\_Wild & Salad\_Bench & OR-Bench &       &       &       &                   \\ \midrule
      \textbf{llama3-8B-Instruct}            & 28.08                                     & 78.09            & 93.65                                      & 82.03                                  & 68.20                                 & 58.99                                   & 53.05                         & 8.25     & 53.50           & 91.16           & 26.97       & 58.79 & 53.50 & 59.07 & 57.12             \\ \midrule
      Hummer (best mixture training)          & 29.41                                     & 78.95            & 93.65                                      & 82.69                                  & 68.59                                 & 60.41                                   & 58.15                         & 8.58     & 55.60           & 92.10           & 50.11       & \textbf{60.35} & 55.60 & 73.21 & 63.05             \\ \midrule
      TSVM (best merging)                                   & 29.92                                     & 77.63            & 93.12                                      & 82.17                                  & 68.51                                 & 59.26                                   & 55.49                         & 8.29     & 56.20           & 89.43           & 67.76       & 59.30 & 56.20 & 78.60 & 64.70    \\
      \textbf{R-TSVM (ours)}                                   & 29.89                                     & 78.89            & 93.65                                      & 82.37                                  & 68.51                                 & 59.56                                   & 56.63                         & 8.29     & 57.20          & 89.92           & 69.27       & 59.72 & \textbf{57.20} & \textbf{79.60} & \textbf{65.51}    \\
      \bottomrule
    \end{tabular}
  }
\end{table*}

\begin{table*}
  % \setlength{\abovecaptionskip}{0cm}
  % \setlength{\belowcaptionskip}{0cm}
  \captionsetup{font={small,stretch=1.25}, labelfont={bf}}
  \renewcommand{\arraystretch}{1}
  \caption{\textbf{The detailed 3H Results on Mistral Under Static Optimization Setting adopting our proposed reweighting-based optimization.}}
  \label{detailed_reweight_mistral}
  \centering
  \setlength{\tabcolsep}{2pt}
  \resizebox{0.99\textwidth}{!}{
    \begin{tabular}{lcccccccc|c|cc|cccc}
      \toprule
      \multirow{2}{*}{\textbf{Methods}}      & \multicolumn{8}{c|}{\textbf{Helpfulness}} & \textbf{Honesty} & \multicolumn{2}{c|}{\textbf{Harmlessness}} & \multirow{2}{*}{\textbf{Helpful\_Avg}} & \multirow{2}{*}{\textbf{Honest\_Avg}} & \multirow{2}{*}{\textbf{Harmless\_Avg}} & \multirow{2}{*}{\textbf{AVG}}                                                                                                          \\ \cmidrule{2-12}
                                             & Math                                      & GSM8K            & ARC-E                                      & ARC-C                                  & MMLU                                  & MBPP\_Plus                              & HumanEval\_Plus               & MT-Bench & HaluEval\_Wild & Salad\_Bench & OR-Bench &       &       &       &                   \\ \midrule
      \textbf{Mistral-7B-Instruct-v0.2}        & 9.54                                      & 46.17            & 82.36                                      & 72.88                                  & 59.97                                 & 26.46                                   & 28.66                         & 7.55     & \textbf{62.17}           & 78.07           & 74.68       & 41.70 & 62.17 & 76.38 & 60.08             \\ \midrule
      Hummer (best mixture training)           & 9.79                                      & 44.50            & 83.72                                      & 74.89                                  & 60.53                                 & 25.85                                   & 33.15                         & 7.56     & 62.05           & 81.85           & 75.28       & 42.50 & 62.05 & 78.57 & 61.04             \\ \midrule
      TSVM (best merging)                                    & 10.40                                     & 44.88            & 84.29                                      & 75.24                                  & 60.87                                 & 28.50                                   & 32.32                         & 7.65     & 61.10           & 83.25           & 78.51       & 43.02 & 61.10 & 80.88 & 61.67  \\
        \textbf{R-TSVM (ours)}                                   & 10.44                                    & 45.00            & 84.35                                      & 75.79                                  & 60.87                                 & 28.50                                   & 32.52                         & 7.71     & 61.50           & 84.25           & 80.25       & \textbf{43.15} & 61.50 & \textbf{82.25} & \textbf{62.30}  \\
      \bottomrule
    \end{tabular}
  }
\end{table*}


\subsection{More Experiments under the Continual DPO Training Settings} \label{appendix_continual}
As shown in Table \ref{continuous_mistral}, we provide additional results under the continual training settings. Through comparison results between different training stages, we can observe the honesty, helpfulness, and harmlessness of LLMs are interactively enhanced due to forgetting during continual training. Moreover, model merging methods can achieve comparable results to these continual training methods without the need to consider the optimized status at a specific training stage. In other words, model merging paves a new way for continual DPO training, advocating training multiple models from the same start point and then merging them, rather than continually optimizing the model from the previous optimization. 

\subsection{The detailed results of Reweighting-based optimization.}\label{appendix_reweighting}
Due to the page limit in the main content, we provide the detailed results of Reweighting-based optimization over TSVM to further verify its effectiveness for 3H optimization in LLM alignment.



\subsection{Hyper-Parameter Analysis to Sparsity} \label{appendix_sparsity}
The sparsity-based strategy is closely related to the merging effect. As shown in Table \ref{static_llama3} and Table \ref{static_mistral}, apart from the SVD-based methods, the most effective merging methods are DARE and DELLA, both of which depend on random sparsification as shown in Table \ref{tab:methods}. However, we conduct extended studies to check the robustness and stability concerning random seed and sparsity factors. As shown in Figure \ref{seed} and Figure \ref{sparsity_sensitivity}, we can observe that R-TSVM can achieve better and more robust results than previous random sparsification-based methods, further verifying the effectiveness of our methods.


\begin{figure}[tb]
\centering
\subfloat[DARE-Ties]{
\includegraphics[width=0.3\linewidth]{fig/seed_dare_ties.pdf}
}
\subfloat[R-TSVM]{
\includegraphics[width=0.3\linewidth]{fig/seed_R_TSVM.pdf}
}
\caption{Comparisons between the random sparsification strategy (e.g.DARE-Ties) and SVD-based strategy (R-TSVM) on Mistral under static optimization settings adopting different seeds. R-TSVM can achieve more stable results than random sparsification methods. }
\label{seed}
\end{figure}

\begin{figure}[tb]
\centering
\subfloat[DARE-Ties]{
\includegraphics[width=0.3\linewidth]{fig/sparsity_dare_ties.pdf}
}
\subfloat[R-TSVM]{
\includegraphics[width=0.3\linewidth]{fig/sparsity_R_TSVM.pdf}
}
\caption{Paramerter sensitive analysis concerning sparsity factor for model merging methods on Mistral under static optimization settings.}
\label{sparsity_sensitivity}
\end{figure}
