
\section{Extened Study to Improve Task Singular Vector Merging for 3H Optimization}

Beyond the benchmarking effort in Sec. \ref{benchmark}, we also explore algorithmic advancements to improve model merging for 3H optimization. Specifically, we first select the most effective and stable merging algorithm, TSVM, to reform the 3H optimization problem and then discuss its drawbacks integrated with the ignored property of LLM (sparsity and heavy-tail) while merging LLM. Then, we leverage outlier weighting and sparsity-adaptive rank selection strategies to improve the effect of TSVM for 3H optimization. Finally, we provide a discussion for future optimized directions.

\subsection{Limitation for Task Singular Vector Merging}
As stated above, TSVM can achieve stable and good results for 3H Optimization. We reform the 3H optimization problem based on its implementation. Given $n$ alignment processes that produce model variants $\{\bm{\theta}^{i}\}_{i=1}^n$ from initial aligned model $\bm{\theta}^{0}$, for each layer $l \in \{1,...,L\}$, we can calculate alignment vector capturing optimization adjustments as Eq. \ref{alignment vector} and perform the singular value decomposition of the task vectors as Eq. \ref{eq:task_svd}, where $\bm{\theta}^{0}_{l}$ are the Parameters of the initial model at layer $l$, $\bm{U}^{(i)}_l, \bm{V}^{(i)}_l$ are respectively left and right singular vectors of task $i$'s parameter change matrix $\bm{\Delta}^{(i)}_l$, $\bm{S}^{(i)}_l$ are the singular values of $\bm{\Delta}^{(i)}_l$ and $d_l$ is the hidden dimension of layer and $\bm{\Delta}_l^{(i)}$ is a square matrix with row and column indices $(r,c) \in \{1,...,d_l\} \times \{1,...,d_l\}$. 
\begin{align}
    \label{alignment vector}
    \bm{\Delta}_l^{(i)} = \bm{\theta}^{i}_{l} - \bm{\theta}^{0}_{l} \in \mathbb{R}^{d_l \times d_l} \\[0.5ex] % 压缩行间距
    \label{eq:task_svd}
    \bm{\Delta}^{(i)}_{l} = \bm{U}^{(i)}_{l} \bm{S}^{(i)}_{l} \bm{V}_{l}^{(i)\top}
\end{align}

TSVM reduces interference through two steps: First, compress the layer task matrics with a fixed rank truncation $k_{\text{fixed}}$. Then, project task-specific parameter changes into orthogonal subspace by whitening these matrices to minimize their correlations as Eq. \ref{seek_orthogonal_u} and Eq. \ref{seek_orthogonal_v}, where $U_{l}$ and $V_{l}$ are the concatenated results of the left and right singular
vectors for different tasks. Thus, the layer-wise parameter of the merged model can be computed as Eq. \ref{eq:tsvm}
\vspace{-0.3cm}
\begin{align}
    \label{seek_orthogonal_u}
    \min_{U_{l\bot}} & \quad \| U_{l\bot} - U_{l} \|_F 
        \quad \text{s.t.} \quad U_{l\bot}^\top U_{l\bot} = I, \\[0.5ex] % 压缩行间距
    \label{seek_orthogonal_v}
    \min_{V_{l\bot}} & \quad \| V_{l\bot} - V_{l} \|_F 
        \quad \text{s.t.} \quad V_{l\bot}^\top V_{l\bot} = I, \\[0.5ex] % 压缩行间距
    \label{eq:tsvm}
    \bm{\theta}_{l}^{\text{TSVM}} & = \bm{\theta}^{0}_{l} + 
        \sum_{\mathclap{i=1}}^n U_{l\bot}^{(i)}[:,:k_{\text{fixed}}] \bm{S}^{(i)}_l V_{l\bot}^{(i)\top}[:k_{\text{fixed}},:]
\end{align}

\vspace{-0.3cm}


Based on Eq. \ref{eq:tsvm}, we can draw two limitations of TSVM while merging LLMs: \textbf{Limitations(i) Isotropic Treatment of Parameters}: TSVM ignores the heavy-tailed distribution of LLM parameters, where the outlier introduces additional challenges to capture true optimization direction through task vectors for model merging \cite{li2024owlore}; \textbf{Limitations(ii) Fixed Rank Selection}: TSVM's fixed rank truncation $k$ fails to adapt to layer-specific sparsity, ignoring significant structural heterogeneity across layers for LLM (Attention and FNN layers own different level of sparsity and parameter importance) \cite{li2024discovering}. We introduce reweighting-based optimization on TSVM (called R-TSVM), with theoretical foundations in outlier detection and sparse pattern analysis to address these problems.


\subsection{Our Reweight Task Singular Vector Merging}

% To cope with these limitations, as shown in Algorithm~\ref{alg:OWLM}, we propose a new reweight-based task singular vector merging algorithm (called R-TSVM), with theoretical foundations in outlier detection and sparse pattern analysis, aiming to further improve model merging for 3H optimization.


\textbf{Outlier-Aware Weighting for Limitation(i)}, we utilize layer-wise outlier detection to aggregate the Outlier-Aware Weight for subsequently weighting the singular values.  This means we should check which parts of the singular values can truly represent the optimization towards alignment process. Considering the heavy-tailed distribution of LLM parameter updates where few parameters undergo significant changes while most exhibit minor adjustments, we can refer to the 3 $\sigma$ principle compatible with this property. Thus, we adopt the statistical significance filtering and competitive weight normalization to identify significant true optimization adjustments as follows:
\begin{align}
    \mu_r^{(i)}    & = \mathbb{E}_c[|\Delta_{l,r,c}|]                            \\
    \sigma_r^{(i)} & = \sqrt{\mathbb{E}_c[|\Delta_{l,r,c}|^2] - (\mu_r^{(i)})^2}
\end{align}
\begin{equation}
    \alpha^{(i)}_l = \frac{\sum_{r=1}^{d_l} \|\textsc{Threshold}(\bm{\Delta}_{l,r,:}^{(i)}, \mu_r^{(i)}+3\sigma_r^{(i)})\|_1}{\sum_{j=1}^n \sum_{r=1}^{d_l} \|\textsc{Threshold}(\bm{\Delta}_{l,r,:}^{(j)}, \mu_r^{(j)}+3\sigma_r^{(j)})\|_1}
\end{equation}
where $\Delta_{l,r,c}^{(i)} \in \mathbb{R}$ denotes the weight deviation at row $r$, column $c$ of layer $l$ for model $i$ relative to initial model, $\mu_r^{(i)}$ and $\sigma_r^{(i)}$ represent the mean and standard deviation of deviations in row $r$, quantifying central tendency and dispersion, $\alpha_l^{(i)} \in [0,1]$ computes layer-wise aggregation weights via $L_1$-normalized sparse outlier magnitudes, and $\textsc{Threshold}(\bm{M}, \tau)$ applies hard-thresholding to suppress elements in matrix $\bm{M}$ with absolute values below $\tau$.

Compared with TSVM's direct utilization of full singular values, our outlier-aware weighting provides two key advantages:
\emph{Noise Suppression}: By thresholding parameter deviations via the $3\sigma$ rule, we filter out low-magnitude fluctuations that predominantly encode noise, forcing the singular vectors $\bm{u}_r^{(i)}$ to align with statistically significant task features.
\emph{Task Equilibrium}: The layer-wise aggregation weights $\alpha_l^{(i)}$ are globally normalized across all models, ensuring balanced contributions from diverse tasks and preventing dominance by high-magnitude updates that may obscure subtle yet critical features.

\textbf{Sparsity-Adaptive Rank Selection for Limitation (ii)}, we aim to adaptively decide the level of rank truncation based on the layer sparsity. We can first compute the sparsity consensus for all models first and then achieve the dynamic rank as Eq. \ref{rank_selection}, where $\gamma$ is the sparsity-rank coupling factor controlling the strength of rank reduction, $k_l$ is defined as the dynamic rank for layer $l$, adaptively determined by sparsity $\Omega_l$. The $\epsilon$ is set to 0.1 by default.
\begin{align}
    \label{sparsity}
    \Omega_l &= \frac{1}{n d_l^2}\sum_{i=1}^n\sum_{r,c=1}^{d_l} \mathbb{I}\left(|\Delta_{l,r,c}^{(i)}| < \epsilon\right) \\
    \label{rank_selection}
    k_l      &= \left\lfloor d_l(1-\gamma \Omega_l) \right\rfloor
\end{align}
% \begin{equation}
% \label{rank_selection}
% \bm{\Delta}_l^{(i)} \approx \bm{U}^{(i)}_l[:,:k_l] \bm{\tilde{S}}^{(i)}_l \bm{V}^{(i)\top}_l[:k_l,:]
% \end{equation}

Compared with TSVM's fixed-rank truncation, our sparsity-adaptive rank selection offers two enhancements:
\emph{Information Preservation}: The dynamic rank $k_l = \lfloor d_l(1-\gamma \Omega_l)\rfloor$ (Eq.~\ref{rank_selection}) adapts to layer-specific sparsity $\Omega_l$—retaining more singular directions in sparse layers (where updates concentrate on critical subspaces) while aggressively truncating redundant components in dense layers, thereby balancing information retention and noise suppression.
\emph{Conflict Mitigation}: By preserving dominant singular directions in sparse layers and enforcing orthogonality through Eq.~\ref{seek_orthogonal_u}, we reduce overlaps between task-specific parameters, decoupling interference-prone optimization trajectories.



\textbf{Overal:} With the above two reweighting designs, we can reform the layer-wise weight of the final merged model of Eq. \ref{eq:tsvm} to Eq.\ref{R-TSVM} with highlighting the improved parts:
{
\footnotesize
\begin{equation}\label{R-TSVM}
    \bm{\theta}^{\text{R-TSVM}}_l =
    \bm{\theta}^{0}_{l} + \sum_{i=1}^n
    \underbrace{
        \bm{{{{U}_l}_\bot}^{(i)}}[:,:\textcolor{blue}{k_l}]
    }_{\mathclap{\text{Adaptive rank selection}}}
    \underbrace{
        \biggl(
        \textcolor{red}{\alpha^{(i)}_l}
        \bm{S}^{(i)}_l
        \biggr)
    }_{\mathclap{\text{Outlier-aware weighting}}}
    \underbrace{
        \bm{{{{V}_l}_\bot}^{(i)\top}}[:\textcolor{blue}{k_l},:]
    }_{\mathclap{\text{Adaptive rank selection}}}
\end{equation}
}
More details of R-TSVM can be shown in Appendix \ref{appendix_method}.


\begin{algorithm2e}[t]
    \small
    \DontPrintSemicolon
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \Input{Initial model $\bm{\theta}^0$ and Further Aligned models $\{\bm{\theta}^i\}_{i=1}^n$ with same layers $L$, Sparsity factor $\gamma \in [0,1]$, $\epsilon > 0$}
    \Output{Merged model $\bm{\theta}^*$}
    % $\bm{\theta}^* \leftarrow \bm{\theta}_0$ \tcp*{\textcolor{gray}{Preserve base alignment}}

    \For{layer $l \leftarrow 1$ \KwTo $L$}{
    \tcp{Step1:Alignment Vector Extraction}
    $\bm{\Delta}_l^{(1:n)} \leftarrow [\bm{\theta}^{i}_{l} - \bm{\theta}^{0}_{l}]_{i=1}^n$

    \tcp{Step2:Outlier-Aware Weighting}
    \For{model $i \leftarrow 1$ \KwTo $n$}{
    Compute row-wise statistics: \\
    $\bm{\mu}^{(i)}, \bm{\sigma}^{(i)} \leftarrow \textsc{RowOutlierScore}(|\bm{\Delta}_l^{(i)}|)$ \\
    Calculate sparse aggregation weights: \\
    $\alpha^{(i)}_l \leftarrow \frac{\sum_{r=1}^{d_l} \|\textsc{Threshold}(\bm{\Delta}_{l,r,:}^{(i)}, \mu_r^{(i)}+3\sigma_r^{(i)})\|_1}{\sum_{j=1}^n \sum_{r=1}^{d_l} \|\textsc{Threshold}(\bm{\Delta}_{l,r,:}^{(j)}, \mu_r^{(j)}+3\sigma_r^{(j)})\|_1}$
    }
    \tcp{Step 3: Sparsity-Adaptive Rank Selection}
    Compute layer sparsity consensus: \\
    $\Omega_l \leftarrow \frac{1}{n d_l^2} \sum_{i=1}^n \sum_{r,c=1}^{d_l} \mathbb{I}(|\Delta_{l,r,c}^{(i)}| < \epsilon)$ \\
    Determine dynamic rank: \\
    $k_l \leftarrow \lfloor d_l(1 - \gamma \Omega_l) \rfloor$

    \tcp{Step 4:Reweight Optimization}
    \For{model $i \leftarrow 1$ \KwTo $n$}{
    Decompose:
    $[\bm{U}^{(i)}_l, \bm{S}^{(i)}_l, \bm{V}^{(i)}_l] \leftarrow \textsc{SVD}(\bm{\Delta}_l^{(i)})$ \\
    Compute orthogonal projections $\bm{{{U}_l}_\bot^{(i)}} $ and $\bm{{{V}_l}_\bot^{(i)}}$ via Eq.\ref{seek_orthogonal_u} via Eq.\ref{seek_orthogonal_v} \\
    Reweight for Outlier Weight:
    $\bm{S}^{(i)}_l \leftarrow \alpha^{(i)}_l \cdot \bm{S}^{(i)}_l$\\
    Reweight for Rank Selection:
    $\bm{{{U}_l}_\bot^{(i)}} \leftarrow \bm{{{U}_l}_\bot^{(i)}}[:,:k_l]$,
    $\bm{{{V}_l}_\bot^{(i)}} \leftarrow \bm{{{V}_l}_\bot^{(i)}}[:k_l,:]$,
    $\bm{S}^{(i)}_l \leftarrow \bm{S}^{(i)}_l[:k_l, :k_l]$}
    Merge Components:
    $\bm{M}_l \leftarrow \sum_{i=1}^n \bm{{{{U}_l}_\bot}^{(i)}}\bm{{S}_l}^{(i)} \bm{{{{V}_l}_\bot}^{(i)\top}}$ \\
    Update the Layer for the Merged Model:
    $\bm{\theta}^*_l \leftarrow \bm{\theta}^{0}_{l} + \bm{M}_l$}
    \caption{Reweight Task Singular Vector Merging}
    \label{alg:OWLM}
\end{algorithm2e}



\subsection{Comparsion Results and Discussions}

\textbf{Reweighting benefits TSVM for 3H optimization in LLM alignment}.
Our experiments demonstrate that the reweighting mechanisms—outlier weighting and sparsity-adaptive rank selection collectively enhance TSVM's capability for 3H optimization. As shown in Table \ref{tab:llama3_results} and \ref{tab:mistral_results}, we report the average score of helpfulness, honesty, and harmless under static optimization settings. The full results on various evaluation datasets can be shown in the Appendix \ref{appendix_reweighting}. From the results, we can observe that integrating both outlier weighting and sparsity-adaptive rank selection can collectively elevate the average score of 3H metric from 64.70 to 65.61 (Llama3) and from 61.67 to 62.30 (Mistral) on extensive evaluations. Notably, R-TSVM outperforms data mixture baselines on Mistral, achieving a higher relative 2.1 x improvement. These results validate model merging as a viable pathway for LLM alignment, particularly when balancing multi-dimensional objectives.

\textbf{Discussions}.
Our experimental results demonstrate that the main improvement of R-TSVM comes from the honest and harmless aspects. This can reflect the decrease in conflict between them, which can be defined as inter-aspect conflict reduction. But for helpfulness, R-TSVM is still worse than data mixture methods on Llama3 and the improvement on Mistral compared with existing merging strategy is also merely. Though the initial goal of honest and harmless training is not designed for helpfulness, modern preference datasets inherently encode helpfulness as a baseline annotation, forcing the alignment process to optimize towards this dimension regardless of their primary target (honesty/harmlessness). This means every alignment vector can represent helpfulness and one or more other dimensions' optimization directions, which may lead to conflict between alignment vectors only from the helpful dimension (e.g. code and commonsense QA abilities for LLM), which can be defined as intra-dimension conflict. This phenomenon necessitates a hierarchical conflict resolution framework to improve model merging for 3H optimization considering these two categories of conflicts simultaneously.




\begin{table}
    \centering
    \caption{Reweighting-Induced Improvements on Llama3 Under Static Optimization Settings.}
    \label{tab:llama3_results}
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{lcccc}
            \toprule
            \textbf{Method}              & \textbf{Helpfulness} & \textbf{Honesty} & \textbf{Harmlessness} & \textbf{Avg} \\ \midrule
            Llama3-8B-Instruct           & 58.79                & 53.50            & 59.07                 & 57.12        \\
            Hummer (best mixture)        & 60.35                & 55.60            & 73.21                 & 63.05        \\
            TSVM (best merging)          & 59.30                & 56.20            & 78.60                 & 64.70        \\
            R-TSVM w/o Outlier Weighting & 59.52                & 56.20            & 79.45                 & 65.06        \\
            R-TSVM w/o Rank Selection    & 59.45                & 56.80            & 79.05                 & 65.10        \\
            R-TSVM                       & 59.72                & 57.20            & 79.60                 & 65.51        \\
            \bottomrule
        \end{tabular}}
\end{table}

\begin{table}
    \centering
    \caption{Reweighting-Induced Improvements on Mistral Under Static Optimization Settings.}
    \label{tab:mistral_results}
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{lcccc}
            \toprule
            \textbf{Method}              & \textbf{Helpfulness} & \textbf{Honesty} & \textbf{Harmlessness} & \textbf{Avg} \\ \midrule
            Mistral-7B-Instruct-v0.2                & 41.70                & 62.17            & 76.38                 & 60.08        \\
            Hummer (best mixture)        & 42.50                & 62.05            & 78.57                 & 61.04        \\
            TSVM (best merging)          & 43.02                & 61.10            & 80.88                 & 61.67        \\
            R-TSVM w/o Outlier Weighting & 42.90                & 61.80            & 81.25                 & 61.98        \\
            R-TSVM w/o Rank Selection    & 43.32                & 61.20            & 81.75                 & 62.09        \\
            R-TSVM                       & 43.15                & 61.50            & 82.25                 & 62.30        \\
            \bottomrule
        \end{tabular}
    }
\end{table}

% \section{Methodology}
% \label{sec:method}

% \subsection{Task Singular Vector Merging Revisited}
% \label{ssec:tsvm_review}

% The TSVM framework \cite{ilharco2022editing} addresses task interference through geometric analysis of parameter subspaces. Given $n$ task-specific models $\{\bm{\theta}_i\}$ fine-tuned from base model $\bm{\theta}_0$, define task vectors as:

% \begin{equation}
% \bm{\tau}^{(i)} = \bm{\theta}_i - \bm{\theta}_0 \in \mathbb{R}^D
% \end{equation}

% The core innovation of TSVM lies in the singular value decomposition (SVD) of stacked task vectors:

% \begin{equation}
% [\bm{\tau}^{(1)}|\cdots|\bm{\tau}^{(n)}] = \bm{U}\bm{\Sigma}\bm{V}^\top
% \end{equation}

% Where the left singular vectors $\bm{U} = [\bm{u}_1,\cdots,\bm{u}_k]$ identify principal directions in parameter space. The merged model is constructed by:

% \begin{equation}
% \bm{\theta}^* = \bm{\theta}_0 + \sum_{r=1}^k \sigma_r \bm{u}_r
% \label{eq:tsvm_base}
% \end{equation}

% This achieves two key properties:
% \begin{itemize}
% \item \textbf{Constructive Alignment}: Directions shared across tasks ($\bm{u}_r$ with large $\sigma_r$) are amplified
% \item \textbf{Conflict Suppression}: Orthogonal components ($\bm{u}_r^\top\bm{u}_s \approx 0$) undergo natural cancellation
% \end{itemize}

% \subsection{Limitations and Our Improvements}
% \label{ssec:limitations}

% While effective, TSVM exhibits three limitations our method addresses:

% \paragraph{Isotropic Treatment of Parameters} 
% TSVM applies uniform SVD across all layers, ignoring the heavy-tailed distribution of LLM parameters \cite{dettmers2022sparsity}. We propose layer-wise outlier detection:

% \begin{equation}
% w_l^{(i)} = \frac{\|\mathcal{T}_{3\sigma}(\bm{\tau}_l^{(i)})\|_1}{\sum_j \|\mathcal{T}_{3\sigma}(\bm{\tau}_l^{(j)})\|_1},\ \ 
% \mathcal{T}_\tau(\bm{x}) = \begin{cases} 
% x_k & |x_k| \geq \mu + 3\sigma \\
% 0 & \text{otherwise}
% \end{cases}
% \end{equation}

% \paragraph{Fixed Rank Selection} 
% TSVM's fixed rank truncation ($k=0.8d$) fails to adapt to layer-specific sparsity. Our dynamic rank mechanism:

% \begin{equation}
% k_l = \lfloor d_l(1 - \gamma s_l)\rfloor,\ \ 
% s_l = \frac{1}{nd_l^2}\sum_{i,j,k} \mathbb{I}(|\tau_{l,jk}^{(i)}| < \epsilon)
% \end{equation}

% % \paragraph{Uniform Singular Value Scaling} 
% % Original TSVM equally weights surviving directions. We introduce competitive weighting:

% % \begin{equation}
% % \bm{\theta}^* = \bm{\theta}_0 + \sum_{l=1}^L \sum_{r=1}^{k_l} \underbrace{w_l^{(i)}\sigma_r^{(i)}}_{\text{Adaptive Scaling}} \bm{u}_r^{(i)}
% % \label{eq:ours}
% % \end{equation}










