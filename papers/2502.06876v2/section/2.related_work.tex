\section{Related Work}

% \textbf{Model Merging for LLM Alignment.}
% Recent advances in preference optimization have demonstrated the effectiveness of model merging techniques. Several approaches address key challenges through parameter space manipulation: \cite{model_soup_2025} and \cite{learn_ref_2024} propose dynamic weight averaging strategies to mitigate optimization drift in DPO by merging reference models from different training stages, while \cite{online_merge_2024} introduces alternating parameter fusion between SFT and DPO models to balance reward maximization with capability preservation. Notably, \cite{warm_2024} systematically demonstrates that weight averaging of reward models enhances robustness, showing 19\% improvement in human preference prediction accuracy. However, existing methods primarily focus on single-objective optimization scenarios, often yielding Pareto-suboptimal solutions when handling multiple alignment dimensions like helpfulness, harmlessness, and honesty. 
% Recent attempts to address multi-objective challenges remain limited in scope. Specifically, \cite{mix_data_2024} explores parameter fusion for multi-task learning but lacks comprehensive analysis of safety-critical dimensions, while \cite{h3fusion_2024}'s MoE-based ensemble suffers from input-dependent routing instability. Despite these advances, current model merging approaches lack theoretical foundations for handling complex trade-offs between competing objectives, particularly in maintaining the delicate balance between domain-specific knowledge and general capabilities \cite{domain_safety_2024}. Our work addresses these gaps by establishing principled optimization boundaries for three-dimensional alignment space.
% Model merging has emerged as a pivotal technique for LLM alignment \cite{yang2024model}, addressing challenges across three aspects:
% (a) \textit{Stabilizing reference policies} addresses the overoptimization problem in direct preference optimization. By merging reference models trained with varying initialization seeds or training phases, weight-space averaging constructs robust policy ensembles that constrain deviation from SFT baselines \cite{chegini2024model}. Dynamic trust-region updations refine this paradigm through iterative parameter blending \cite{gorbatovski2024learn}, while online gradient fusion between SFT and DPO objectives preserves foundational capabilities \cite{lu2024online}. Phased merging strategies in iterative preference learning \cite{tan2024language} demonstrate improved exploration-exploitation balance, though they remain limited by static reference distribution assumptions.
% (b) \textit{Cross-model capability transfer} confronts architectural and distributional mismatches during knowledge fusion \cite{wan2024knowledge}. Probabilistic token alignment bridges vocabulary gaps through embedding space reconciliation \cite{yang2024weighted}, complemented by reward-weighted fusion mechanisms that dynamically prioritize expert contributions \cite{yang2024weighted}. Vertical domain adaptation techniques graft task-specific parameters into general reward models \cite{lin2024dogerm}, while subspace projection preserves safety-critical features during specialization \cite{thakkar2024combining}. These approaches, however, remain vulnerable to toxic parameter propagation \cite{hammoud2024model} and catastrophic forgetting when integrating conflicting knowledge.
% (c) \textit{Avoiding forgetting after finetuning} include gradient-aware selective merging \cite{ju2024mitigating}, heterogeneous layer-wise merging\cite{lin2023speciality,lin2024mitigating} and disperse-data-then-merge  \cite{fu2024disperse} strategies to mitigate the general abilities' forgetting (alignment tax), while \cite{yi2024safety} provide a subspace method to enhance LLM's alignment after fine-tuning on downstream tasks; (d) \textit{Multi-Objective optimization} seeks to harmonize competing alignment criteria through model merging. Linear interpolation of reward-tuned models \cite{jang2023personalized,rame2024rewarded,rame2024warm,rame2024warp} and MoE-based expert routing \cite{tekin2024h} empirically approximate Pareto frontiers but lack theoretical guarantees for subspace conflict analysis. Location-based merging methods \cite{zhao2024towards} identify specific weights for alignment, but its performance heavily depends on the data used for parameter identification. Moreover, \cite{ahmadian2024mix} also compares the data mixture and merging methods but its scope is confined to multilingual scenarios without dealing with 3H optimization for LLM alignment.


% \textbf{Data Mixing in Helpfulness, Honesty, and Harmlessness.}
% Recent advancements in data mixing methods have significantly contributed to the alignment of language models toward achieving Helpfulness, Honesty, and Harmlessness (3H). These methods can be broadly categorized into heuristic approaches, reward model techniques, and Metric-evaluation methods, each offering unique strategies and benefits.
% (a) \textit{Heuristic Methods} Implementations such as Sparrow \cite{74eae12620bd1c1393e268bddcb6f129a5025166} and Constitutional AI \cite{3936fd3c6187f606c6e4e2e20b196dbc41cc4654} have demonstrated the effectiveness of using natural language rules for providing alignment feedback. These frameworks reduce the dependency on extensive human labeling, thereby enhancing efficiency. The focus remains on ensuring that the models are both harmless and helpful.
% Moreover, HonestLLM \cite{5c8245c04c902b0dd631b8633ca835967f395ad0} stands out by introducing honesty-specific datasets, such as HoneSet, and employing training methodologies like curriculum learning and uncertainty modeling. These approaches have shown marked improvements in honesty without compromising on helpfulness.
% (b) \textit{Reward Model Methods} Methods like Multi-Objective Direct Preference Optimization (MODPO) \cite{b7c01c660055dc91ac8de8621d5f823f0aa3c46e} and Controllable Preference Optimization (CPO) \cite{guo2024controllable} offer computationally efficient and stable methods to balance the 3H features. These approaches aim to avoid the instability commonly associated with Reinforcement Learning from Human Feedback (RLHF).
% Safe RLHF \cite{0f7308fbcae43d22813f70c334c2425df0b1cce1} decouples reward and cost models to balance helpfulness against harmlessness, whereas Elastic Reset \cite{15b8b6a8028b2b6e75b67dfb6aebaede36826cf8} addresses reward hacking challenges through dynamic resets. Additionally, BeHonest \cite{5c1e031b21bb8ea1bd652c5a4a1b2cf32149e521} provides essential metrics for evaluating honesty independently.
% Egalitarian methods such as MaxMin-RLHF \cite{dacc3a8d45968616f220628dc0db8d5d78c1a389} and contrastive reward learning \cite{78404d12d013f4f21dbf87a1f6721295ff7ac9e0} enhance alignment robustness by accommodating diverse user preferences.
% Recent work \cite{wang2024interpretable} proposes a method to enhance the interpretability of reward models by training an Absolute-Rating Multi-Objective Reward Model (ArmoRM) with multi-dimensional data and using a Mixture-of-Experts (MoE) strategy to select suitable reward objectives based on context.
%  (c) \textit{Metric Evaluation Methods} 
% Trade-offs among 3H objectives continue to pose significant challenges. Modular methods such as MODPO \cite{b7c01c660055dc91ac8de8621d5f823f0aa3c46e} and MetaAligner \cite{e36f36ffe6df823f50a54c525319926e3df5ead1} efficiently mitigate alignment tax, while others propose explicit trade-off measurements through representation engineering \cite{be9f699c4bfa0f29c9a0a920a6310ec70f5580e6} or hypervolume techniques \cite{165fdad3949b7abdb985cb8834c26c7baa7bd40f}. Despite these advancements, honesty remains less prioritized compared to helpfulness and harmlessness, highlighting the necessity for tools like HonestLLM \cite{5c8245c04c902b0dd631b8633ca835967f395ad0} and BeHonest \cite{5c1e031b21bb8ea1bd652c5a4a1b2cf32149e521}.
% Innovations in computational efficiency, including MODPO \cite{b7c01c660055dc91ac8de8621d5f823f0aa3c46e} and PMoL \cite{5f999f9c6745663e79b149a7d7dac39dab22081a}, have successfully balanced alignment efficacy with reduced training costs. New benchmarks like BeHonest \cite{5c1e031b21bb8ea1bd652c5a4a1b2cf32149e521} and HoneSet \cite{5c8245c04c902b0dd631b8633ca835967f395ad0} drive targeted evaluations of key alignment dimensions, paving the way for more refined and effective language model alignments.
% The Hummer dataset \cite{jiang2024hummer} introduces ADC to quantify conflict in preference datasets, reducing competition between alignment objectives and benefiting downstream applications by prioritizing values and reducing jailbreak vulnerabilities.

\textbf{Model Merging for LLM Alignment.}
Model merging has emerged as a pivotal technique for LLM alignment \cite{yang2024model}, addressing challenges across four aspects:
(a) \textit{Stabilizing reference policies} focuses on the over-optimization problem in direct preference optimization. Weight-space averaging of models with varying initializations constructs robust policy ensembles \cite{chegini2024model}, while dynamic trust-region updates \cite{gorbatovski2024learn} and online gradient fusion \cite{lu2024online} help preserve foundational capabilities.
(b) \textit{Cross-model capability transfer} resolves architectural mismatches during knowledge fusion \cite{wan2024knowledge} through probabilistic token alignment \cite{yang2024weighted}, vertical domain adaptation \cite{lin2024dogerm}, and subspace projection \cite{thakkar2024combining}. Persistent toxic parameter propagation \cite{hammoud2024model} remains a critical barrier, inducing biased representation transfer during integration.
(c) \textit{Avoiding forgetting after finetuning} develops gradient-aware selective merging \cite{ju2024mitigating}, heterogeneous layer-wise merging \cite{lin2023speciality,lin2024mitigating}, and subspace-based merging \cite{yi2024safety} methods to mitigate the alignment tax or realign the model after fine-tuning for downstream tasks. (d) \textit{Balancing multi-optimized objectives} employs linear interpolation of reward-tuned models \cite{jang2023personalized,rame2024rewarded,rame2024warm,rame2024warp} and MoE-based expert routing \cite{tekin2024h} to approximate Pareto frontiers but lacks theoretical guarantees for subspace conflict analysis. Location-based merging \cite{zhao2024towards} identifies specific weights for alignment, but its effect is highly dependent on the data used for parameter identification. Moreover, \cite{ahmadian2024mix} also compares the data mixture and model merging methods, yet critically limits their analysis to cross-lingual transfer scenarios without dealing with 3H optimization.

\textbf{Data Mixing in Helpfulness, Honesty, and Harmlessness.} Data mixing methods align the LLMs towards 3H dimension from  three aspects:
% have emerged as a pivotal methodology to align language models toward Helpfulness, Honesty, and Harmlessness (3H). These methods can be broadly categorized into three types:
(a) \textit{Heuristic Methods:} Sparrow \cite{74eae12620bd1c1393e268bddcb6f129a5025166} and Constitutional AI \cite{3936fd3c6187f606c6e4e2e20b196dbc41cc4654} initially adopt the rules for alignment feedback from 3H dimension, reducing dependency on extensive human labeling. Recently, \cite{bianchi2023safety,amballa2024safe} explore the heuristic mixture of instructions between helpful and safety-related data to balance multi-objects.
% HonestLLM \cite{5c8245c04c902b0dd631b8633ca835967f395ad0} introduces honesty-specific datasets and training methodologies that improve honesty while maintaining helpfulness.
(b) \textit{Reward Model Methods:} Beyond traditional Bradley-Terry models \citep{bradley1952rank, ouyang2022training}, many efforts have been devoted to exploring multi-objective reward models (RMs) to score the data for capturing the complicated human preferences \cite{touvron2023llama,wang2023helpsteer,wang2024arithmetic}. ArmoRM is a recent development aiming to promote LLMs aligned with human-interpretable multi-objective demands like honesty and helpfulness \cite{wang2024interpretable}.
(c) \textit{Metric Evaluation Methods:} Early metrics for preference data only focus on the quality and diversity dimensions \citep{cui2023ultrafeedback,wu2023fine}, hummer recently defined the alignment dimension conflict metric \cite{jiang2024hummer} to quantify the conflict among preference datasets to balance diverse alignment objectives effectively.


% Recent datasets like UltraFeedback \citep{cui2023ultrafeedback} and Hummer \citep{jiang2024hummer} introduce comprehensive evaluation dimensions and metrics to quantify alignment conflicts. New benchmarks like BeHonest \cite{5c1e031b21bb8ea1bd652c5a4a1b2cf32149e521} enable targeted evaluation of specific alignment dimensions while maintaining overall model effectiveness.



% H3Fusion \cite{tekin2024h} utilizes a Mixture-of-Experts (MoE) architecture to blend pre-aligned models, thereby improving robustness across 3H objectives.
% DeAL \cite{b899a28eb553800ce558cf8974a697f65103e591} enables real-time, flexible alignment adjustments for objectives like harmlessness and helpfulness, eliminating the need for retraining.
% HaM \cite{165fdad3949b7abdb985cb8834c26c7baa7bd40f} tackles conflicting objectives by covering the Pareto front, ensuring balanced improvements across 3H dimensions.