\section{LLM Merging Benchmark for 3H Optimization}\label{benchmark}

\begin{table*}
  \setlength{\abovecaptionskip}{0cm}
  \setlength{\belowcaptionskip}{0cm}
  \captionsetup{font={small,stretch=1.25}, labelfont={bf}}
  \renewcommand{\arraystretch}{1}
  \caption{\textbf{3H Results on Llama3 Under Static Optimization Setting where we perform DPO training using various datasets at once. For merging methods, we highlight the best score in bold and the second score with underlining.}}
  \label{static_llama3}
  \centering
  \setlength{\tabcolsep}{2pt}
  \resizebox{0.99\textwidth}{!}{
    \begin{tabular}{lcccccccc|c|cc|cccc}
      \toprule
      \multirow{2}{*}{\textbf{Methods}}      & \multicolumn{8}{c|}{\textbf{Helpfulness}} & \textbf{Honesty} & \multicolumn{2}{c|}{\textbf{Harmlessness}} & \multirow{2}{*}{\textbf{Helpful\_Avg}} & \multirow{2}{*}{\textbf{Honest\_Avg}} & \multirow{2}{*}{\textbf{Harmless\_Avg}} & \multirow{2}{*}{\textbf{AVG}}                                                                                                          \\ \cmidrule{2-12}
                                             & Math                                      & GSM8K            & ARC-E                                      & ARC-C                                  & MMLU                                  & MBPP\_Plus                              & HumanEval\_Plus               & MT-Bench & HaluEval\_Wild & Salad\_Bench & OR-Bench &       &       &       &                   \\ \midrule
      \textbf{llama3-8B-Instruct}            & 28.08                                     & 78.09            & 93.65                                      & 82.03                                  & 68.20                                 & 58.99                                   & 53.05                         & 8.25     & 53.50           & 91.16           & 26.97       & 58.79 & 53.50 & 59.07 & 57.12             \\ \midrule
      Individual Helpfulness                 & 29.08                                     & 79.30            & 93.65                                      & 81.69                                  & 68.58                                 & 57.94                                   & 58.54                         & 8.33     & 55.00           & 89.83           & 42.06       & 59.64 & 55.00 & 65.95 & 60.20             \\
      Individual Honesty                     & 28.52                                     & 78.77            & 93.65                                      & 81.36                                  & 68.34                                 & 58.47                                   & 54.27                         & 8.45     & 54.67           & 92.18           & 21.95       & 58.98 & 54.67 & 57.07 & 56.91             \\
      Individual Harmlessness                & 28.88                                     & 77.33            & 93.65                                      & 82.03                                  & 68.32                                 & 59.79                                   & 52.44                         & 8.15     & 53.33           & 92.36           & 27.92       & 58.82 & 53.33 & 60.14 & 57.43             \\
      Helpfulness\&Honesty                   & 29.60                                     & 77.63            & 93.47                                      & 82.71                                  & 68.33                                 & 59.79                                   & 59.15                         & 8.18     & 56.00           & 90.86           & 39.80       & 59.86 & 56.00 & 65.33 & 60.40             \\
      Helpfulness\&Harmlessness              & 30.02                                     & 77.26            & 93.47                                      & 82.37                                  & 68.31                                 & 58.99                                   & 56.11                         & 8.16     & 54.50           & 90.27           & 58.86       & 59.34 & 54.50 & 74.57 & 62.80             \\ \midrule
      3H Mixture Full Training (Heurisistic) & 28.21                                     & 78.85            & 93.65                                      & 81.69                                  & 68.38                                 & 60.85                                   & 57.32                         & 8.48     & 54.67           & 92.06           & 35.36       & 59.68 & 54.67 & 63.71 & 59.35             \\
      3H Mixture Full Training (ArmoRM)      & 28.81                                     & 78.97            & 93.65                                      & 82.39                                  & 68.42                                 & 60.55                                   & 58.22                         & 8.52     & 55.50           & 92.11           & 42.12       & 60.24 & 55.50 & 69.02 & 61.59             \\
      3H Mixture Training (Hummer)           & 29.41                                     & 78.95            & 93.65                                      & 82.69                                  & 68.59                                 & 60.41                                   & 58.15                         & 8.58     & 55.60           & 92.10           & 50.11       & 60.35 & 55.60 & 73.21 & 63.05             \\ \midrule
      Weight Average                         & 29.80                                     & 78.01            & 93.47                                      & 82.71                                  & 68.43                                 & 59.26                                   & 57.32                         & 8.02     & 57.78           & 91.72           & 41.48       & 59.63 & 57.78 & 66.60 & 61.34             \\
      Rewarded Soup                          & 29.64                                     & 77.94            & 93.47                                      & 82.71                                  & 68.54                                 & 60.85                                   & 57.93                         & 8.32     & 57.55           & 90.86           & 50.08       & 59.93 & 57.55 & 70.47 & 62.65             \\
      Model Stock                            & 28.72                                     & 78.24            & 93.47                                      & 82.71                                  & 68.41                                 & 59.79                                   & 56.10                         & 8.03     & 53.00           & 91.62           & 32.28       & 59.43 & 53.00 & 61.95 & 58.13             \\
      Task Arithmetic                        & 29.02                                     & 79.05            & 93.30                                      & 83.39                                  & 68.35                                 & 57.14                                   & 51.83                         & 8.37     & 57.33           & 91.39           & 28.29       & 58.81 & 57.33 & 59.84 & 58.66             \\
      Ties                                   & 29.30                                     & 78.17            & 93.30                                      & 83.05                                  & 68.52                                 & 56.61                                   & 53.05                         & 8.20     & 54.33           & 89.13           & 29.07       & 58.78 & 54.33 & 59.10 & 57.40             \\
      DARE                                   & 29.42                                     & 78.39            & 93.47                                      & 82.71                                  & 68.41                                 & 59.26                                   & 56.71                         & 8.28     & 57.00           & 91.85           & 38.80       & 59.58 & 57.00 & 65.33 & 60.64             \\
      DARE Ties                              & 29.64                                     & 78.01            & 93.47                                      & 82.71                                  & 68.43                                 & 59.26                                   & 57.32                         & 8.07     & 56.00           & 92.16           & 36.88       & 59.61 & 56.00 & 64.52 & 60.04             \\
      DELLA                                  & 29.08                                     & 78.92            & 93.30                                      & 83.73                                  & 68.41                                 & 54.76                                   & 52.44                         & 8.43     & 54.67           & 91.37           & 63.31       & 58.63 & 54.67 & 77.34 & 63.55             \\
      DELLA  Ties                            & 29.20                                     & 75.97            & 93.30                                      & 83.39                                  & 68.46                                 & 56.08                                   & 49.39                         & 8.16     & 54.00           & 87.95           & 70.02       & 57.99 & 54.00 & 78.99 & \underline{63.57} \\
      Breadcrumbs                            & 29.46                                     & 77.79            & 93.30                                      & 83.39                                  & 68.53                                 & 60.85                                   & 54.88                         & 8.24     & 59.33           & 91.60           & 44.85       & 59.56 & 59.33 & 68.23 & 62.37             \\
      Breadcrumbs Ties                       & 29.72                                     & 78.54            & 93.30                                      & 83.05                                  & 68.46                                 & 60.05                                   & 53.66                         & 8.14     & 55.50           & 90.00           & 58.52       & 59.37 & 55.50 & 74.26 & 63.04             \\
      TSVM                                   & 29.92                                     & 77.63            & 93.12                                      & 82.17                                  & 68.51                                 & 59.26                                   & 55.49                         & 8.29     & 56.20           & 89.43           & 67.76       & 59.30 & 56.20 & 78.60 & \textbf{64.70}    \\
      \bottomrule
    \end{tabular}
  }
\end{table*}
\subsection{Benchmark Setup}

\textbf{LLM DPO Training Datasets, Schemes, and Model Constructions.} To achieve the goal of 3H optimization, we select commonly used preference data shown in Table \ref{tab:dpo_data_stats}, which can be categorized into five groups from the annotation perspective, to perform DPO training. Following SimPO \cite{meng2024simpo}, we adopt two well-known off-the-shelf instruction-tuned models, Llama-3-8B-Instruct \cite{dubey2024llama} and Mistral-7B-Instruct-v0.2 \cite{jiang2023mistral}, as the SFT model and then fine-tune the entire network of LLMs utilizing the above preference data to construct five enhanced aligned models, which correspond to different annotation dimensions for further model merging.
% More detailed implementations can be shown in the Appendix.

\textbf{Setup and Implementation Details.} We conduct extensive experiments to compare model merging and data mixture methods for 3H optimization. These include \textit{Data Mixture Methods} where we first process the full training data of Table \ref{tab:dpo_data_stats} and then utilize these processed data for DPO training. As stated in Table \ref{tab:methods} and Appendix \ref{training_details}, we adopt (i) Heuristic-adjusted dataset mixing ratio (Heuristic) where we control the ratio between Honesty\&Harmlessness and Helpfulness, (ii) Data selection methods based on the multi-dimension score of the Reward model ArmoRM-Llama3-8B \cite{wang2024interpretable} (iii) Data selection based on the designed alignment conflict metric from Hummer \cite{jiang2024hummer}; As for \textit{Model Merging Methods} for LLM Alignment, considering that the constraints of data availability and data leak will limit the generalization of existing merging methods for LLMs, we adopt the well-known and latest training-free merging strategies for dense LLM from MergeKit \cite{goddard2024arcee}, which includes Weight Average \cite{wortsman2022model}, Task Arithmetic \cite{ilharco2022editing}, Ties-Merging \cite{yadav2024ties}, DARE\cite{yu2024language}, DELLA \cite{deep2024DELLA}, Model Stock \cite{jang2025model} and  Model Breadcrumbs \cite{davari2025model}. Moreover, from the perspective of Pareto-optimal front \cite{jang2023personalized,rame2024rewarded,rame2024warm,rame2024warp} and singular vector decomposition  \cite{stoica2024model,zhong2024panacea,gargiulo2024task}, we select Rewarded Soup \cite{rame2024rewarded} and TSVM \cite{gargiulo2024task} as two additional merging methods, which can meet the condition of training-free full parameters merging for 3H optimization. Notably, we leave those training-based and MOE-based merging methods \cite{tekin2024h} for further exploration and provide discussion in Appendix \ref{more_relatedwork}.

% (e.g. H3 Fusion \cite{cf208ab262f0affc4f5581b9a18901265d9728ab}) 

% apart from those mixtures of experts(MOE) based merging methods


% (e.g.as SMILES \cite{tang2024smile}, WEMOE \cite{shen2024efficient}, Free-Merging \cite{zheng2024free}, Twin-Merging \cite{zheng2024free}, H3 fusion \cite{cf208ab262f0affc4f5581b9a18901265d9728ab}}
% Notably, those mixtures of experts(MOE) based merging methods, such as SMILES \cite{tang2024smile}, WEMOE \cite{shen2024efficient}, Free-Merging \cite{zheng2024free} and Twin-Merging \cite{zheng2024free} are not designed for our settings, we provide discussions in the Appendix.
% including Task Arithmetic \cite{ilharco2022editing}, Ties-Merging \cite{yadav2024ties}, DARE\cite{yu2024language}, DELLA \cite{deep2024DELLA}, Rewarded Soup \cite{rame2024rewarded}, Model Breadcrumbs \cite{davari2025model}. 



% Notably, those mixtures of experts(MOE) based merging methods, such as SMILES \cite{tang2024smile}, WEMOE \cite{shen2024efficient}, Free-Merging \cite{zheng2024free} and Twin-Merging \cite{zheng2024free} are not designed for our settings, we provide discussions in the Appendix.

% \textbf{Datasets and Models:}To achieve the goal of 3H, we select commonly used preference data as shown in Table \ref{tab:dpo_data_stats}, which can be categorized from the annotation perspective, to perform DPO training. Similar to SimPO \cite{meng2024simpo}, we adopt two well-known off-the-shelf instruction-tuned models, Llama-3-8B-Instruct \cite{dubey2024llama} and Mistral-7B-Instruct-v0.2 \cite{jiang2023mistral}, as the SFT model and then utilize the above preference data to construct further enhanced aligned models. More details about the implementation can be shown in the Appendix.


% \begin{table}
% \centering
% \caption{\textbf{Necessary specifications for the strategy and scaling of each method.}}
% \resizebox{1\columnwidth}{!}{
% \begin{tabular}{cllc}
% \toprule
% \textbf{Type} & \textbf{Method} & \multicolumn{1}{c}{\textbf{Strategy}} & \textbf{Scaling} \\
% \midrule
% \multirow{3}{*}{Data Mixture-Based} & Heuristic~\cite{lambert2024t} & Empirically
% heuristic-adjusted ratio & Data Mixture ratio \\

% & ArmoRM~\cite{wang2024interpretable} & Reward Model  & Data Mixture ratio \\

% & Hummer~\cite{jiang2024hummer} & Alignment Conflict Metric  & Data Mixture ratio\\
% \midrule
% \multirow{8}{*}{Merging-Based} & Weight Average~\cite{wortsman2022model} & Linear Int. Consensus & Parameter Weight coeff. \\

% & Rewarded Soup~\cite{rame2024rewarded} & Linear Int. Consensus & Parameter Weight coeff.\\

% & Task Arithmetic~\cite{ilharco2022editing} &  Linear Int. Consensus & Parameter Scaling factor \\

% & Ties~\cite{yadav2024ties} & Top-k Sparsification  & Parameter Scaling factor \\

% & DARE~\cite{yu2024language} & Random Sparsification  & Parameter Scaling factor \\

% & DELLA~\cite{deep2024DELLA} & Random Sparsification  & Parameter Scaling factor \\
% & Breadcrumbs~\cite{davari2025model} & Top/Bottom-k Sparsification  & Parameter Scaling factor \\
% & Model Stock~\cite{jang2025model} & Geometric Sparsification & Parameter Adaptive ratio \\

% \bottomrule
% \end{tabular}}
% \label{tab:methods}
% \end{table}


\textbf{Settings:} We construct two different settings to verify the effectiveness of model merging for 3H optimization: \textbf{(i) Static Optimization for DPO Training at once}, where we aim to achieve an aligned model that simultaneously meets the 3H demands using various annotated preference data at once. \textbf{(ii) Continual Optimization for Sequential DPO Training}, which refers to the continual and dynamic circumstances with newly curated preference data and more customized demands compared to previously trained models. In this case, we need to simultaneously focus on the effectiveness and efficiency of constructing an aligned model.

%
\textbf{Evaluation:}  We conduct comprehensive and fair evaluations for LLMs' 3H-related abilities.
\textbf{(i) For Helpfulness:} we select  Math, GSM8K, ARC-C, ARC-E, MMLU, MBPP-Plus, and HumanEval-Plus \cite{liu2024your} to assess the general abilities and utilize MT-Bench \cite{zheng2023judging} to asses the instruction-following ability; \textbf{(ii) For Honesty:} we utilize the HaluEval-Wild \cite{zhu2024halueval} for evaluating hallucinations in real-world settings; \textbf{(iii) For Harmlessness:}, we conduct safety-related (SaladBench \cite{li2024salad}) and refusal-related (OR-Bench \cite{cui2024or}) evaluations to measure the effectiveness of harmless training. Notably, for all experiments in this paper, higher values are preferred. Moreover, we clarify the details of judged models for evaluation in the Appendix \ref{evaluation_datails}.


% on some evaluation datasets, the model's outcome is closely related to the selection of the evaluator(e.g. GPT version). We clarify these details in the Appendix \ref{evaluation_datails}.% Specifically, we choose the MD-Judge-v0.2 as the judge model of SaladBench and the GPT4-o as the the judge model of the remaining datasets to assess the models output.

% \textbf{Baselines:} Our methods for 3H optimization include:\textbf{(i) Data Mixture Methods}: Empirically heuristic-adjusted dataset mixing ratio (defined as Heuristic), data selection based on the multi-dimension score of the Reward model, e.g. ArmoRM-Llama3-8B \cite{wang2024interpretable} (defined as ArmoRM), data selection based on the evaluation metric for alignment conflict (such as Hummer \cite{jiang2024hummer});\textbf{(ii) Model Merging Methods} for LLMs' multi-task or multi-objective optimization including Task Arithmetic \cite{ilharco2022editing}, Ties-Merging \cite{yadav2024ties}, DARE \cite{yu2024language}, DELLA \cite{deep2024DELLA}, RewardSoup \cite{rame2024rewarded}, Model Breadcrumbs \cite{davari2025model}. Notably, those mixtures of experts(MOE) based merging methods, such as SMILES \cite{tang2024smile}, WEMOE \cite{shen2024efficient}, Free-Merging \cite{zheng2024free} and Twin-Merging \cite{zheng2024free} are not designed for our settings, we provide discussions in the Appendix.


% According to the operation stage, our compared baselines can be divided into three branches:\textbf{(i) Pre-processing Methods:} Empirically heuristic-adjusted dataset mixing ratio (defined as Heuristic), data selection based on the multi-dimension score of the Reward model, e.g. ArmoRM-Llama3-8B \cite{wang2024interpretable} (defined as ArmoRM), data selection based on the evaluation metric for alignment conflict (defined as Hummer \cite{jiang2024hummer}); \textbf{(ii) During Training Methods:} we mainly compare with the controllable preference direct optimization
% (CDPO) \cite{guo2024controllable}, which also focus on the 3H optimization for DPO training. \textbf{(iii) Post-Deployment Methods:} We mainly compare with previous model merging methods for LLMs' multi-task or multi-objective optimization including Task Arithmetic \cite{ilharco2022editing}, Ties-Merging \cite{yadav2024ties}, DARE \cite{yu2024language}, DELLA \cite{deep2024DELLA}, RewardSoup \cite{rame2024rewarded}, Model Breadcrumbs \cite{davari2025model}. Notably, those mixtures of experts(MOE) based merging methods,such as SMILES \cite{tang2024smile}, WEMOE \cite{shen2024efficient}, Free-Merging \cite{zheng2024free} and Twin-Merging \cite{zheng2024free} are not designed for our settings, we provide discussions in the Appendix.


\begin{table*}
  \setlength{\abovecaptionskip}{0cm}
  \setlength{\belowcaptionskip}{0cm}
  \captionsetup{font={small,stretch=1.25}, labelfont={bf}}
  \renewcommand{\arraystretch}{1}
  \caption{\textbf{3H Results on Mistral Under Static Optimization Setting where we perform DPO training using various datasets at once. For merging methods, we highlight the best score in bold and the second score with underlining.}}
  \label{static_mistral}
  \centering
  \setlength{\tabcolsep}{2pt}
  \resizebox{0.99\textwidth}{!}{
    \begin{tabular}{lcccccccc|c|cc|cccc}
      \toprule
      \multirow{2}{*}{\textbf{Methods}}      & \multicolumn{8}{c|}{\textbf{Helpfulness}} & \textbf{Honesty} & \multicolumn{2}{c|}{\textbf{Harmlessness}} & \multirow{2}{*}{\textbf{Helpful\_Avg}} & \multirow{2}{*}{\textbf{Honest\_Avg}} & \multirow{2}{*}{\textbf{Harmless\_Avg}} & \multirow{2}{*}{\textbf{AVG}}                                                                                                          \\ \cmidrule{2-12}
                                             & Math                                      & GSM8K            & ARC-E                                      & ARC-C                                  & MMLU                                  & MBPP\_Plus                              & HumanEval\_Plus               & MT-Bench & HaluEval\_Wild & Salad\_Bench & OR-Bench &       &       &       &                   \\ \midrule
      \textbf{Mistral-7B-Instruct-v0.2}        & 9.54                                      & 46.17            & 82.36                                      & 72.88                                  & 59.97                                 & 26.46                                   & 28.66                         & 7.55     & 62.17           & 78.07           & 74.68       & 41.70 & 62.17 & 76.38 & 60.08             \\ \midrule
      Individual Helpfulness                 & 9.44                                      & 47.38            & 84.48                                      & 75.25                                  & 60.70                                 & 22.49                                   & 32.31                         & 7.80     & 60.60           & 74.84           & 80.67       & 42.48 & 60.60 & 77.76 & 60.28             \\
      Individual Honesty                     & 9.34                                      & 46.63            & 82.54                                      & 71.19                                  & 59.04                                 & 24.34                                   & 23.78                         & 7.76     & 65.20           & 83.98           & 60.82       & 40.58 & 65.20 & 72.40 & 59.39             \\
      Individual Harmlessness                & 9.40                                      & 46.10            & 81.42                                      & 72.88                                  & 60.03                                 & 27.51                                   & 30.39                         & 7.43     & 59.00           & 85.43           & 69.54       & 41.90 & 59.00 & 77.49 & 59.46             \\
      Helpfulness\&Honesty                   & 8.76                                      & 43.14            & 82.01                                      & 74.92                                  & 59.78                                 & 25.93                                   & 27.33                         & 7.59     & 61.33           & 78.74           & 77.23       & 41.18 & 61.33 & 77.99 & 60.17             \\
      Helpfulness\&Harmlessness              & 9.96                                      & 41.77            & 83.42                                      & 75.59                                  & 61.13                                 & 34.92                                   & 29.27                         & 7.46     & 50.60           & 79.83           & 86.05       & 42.94 & 50.60 & 82.94 & 58.83             \\ \midrule
      3H Mixture Full Training (Heurisistic) & 9.56                                      & 41.70            & 83.06                                      & 74.24                                  & 60.23                                 & 22.75                                   & 34.15                         & 7.69     & 62.00           & 81.13           & 72.97       & 41.67 & 62.00 & 77.05 & 60.24             \\
      3H Mixture Full Training (ArmoRM)      & 9.71                                      & 43.70            & 83.65                                      & 74.54                                  & 60.33                                 & 25.11                                   & 33.58                         & 7.75     & 61.95           & 81.10           & 75.55       & 42.30 & 61.95 & 78.32 & 60.85             \\
      3H Mixture Training (Hummer)           & 9.79                                      & 44.50            & 83.72                                      & 74.89                                  & 60.53                                 & 25.85                                   & 33.15                         & 7.56     & 62.05           & 81.85           & 75.28       & 42.50 & 62.05 & 78.57 & 61.04             \\ \midrule
      Weight Average                         & 9.86                                      & 45.49            & 82.36                                      & 74.58                                  & 60.70                                 & 27.25                                   & 31.10                         & 7.47     & 60.60           & 81.51           & 72.90       & 42.35 & 60.60 & 77.21 & 60.50             \\
      Rewarded Soup                          & 9.74                                      & 45.11            & 82.54                                      & 74.58                                  & 60.65                                 & 26.72                                   & 30.49                         & 7.48     & 60.71           & 81.43           & 72.51       & 42.16 & 60.71 & 76.97 & 59.95             \\
      Model Stock                            & 9.80                                      & 46.93            & 82.36                                      & 74.24                                  & 60.34                                 & 25.13                                   & 30.49                         & 7.19     & 61.81           & 80.11           & 73.78       & 42.06 & 61.81 & 76.95 & 60.27             \\
      Task Arithmetic                        & 9.76                                      & 44.12            & 84.13                                      & 73.90                                  & 60.86                                 & 27.25                                   & 33.54                         & 7.44     & 61.01           & 83.91           & 71.04       & 42.63 & 61.01 & 77.48 & 60.37             \\
      Ties                                   & 10.22                                     & 41.47            & 85.19                                      & 74.92                                  & 61.34                                 & 27.25                                   & 31.10                         & 7.46     & 58.73           & 82.15           & 81.13       & 42.37 & 58.73 & 81.64 & 60.91             \\
      DARE                                   & 10.10                                     & 43.82            & 84.48                                      & 73.90                                  & 60.78                                 & 27.25                                   & 32.93                         & 7.47     & 61.05           & 83.80           & 71.04       & 42.59 & 61.05 & 77.42 & 60.35             \\
      DARE Ties                              & 10.10                                     & 42.46            & 85.00                                      & 74.58                                  & 61.05                                 & 26.98                                   & 31.71                         & 7.61     & 59.28           & 82.28           & 81.91       & 42.49 & 59.28 & 82.10 & \underline{61.49} \\
      DELLA                                  & 10.26                                     & 42.76            & 84.83                                      & 73.56                                  & 60.86                                 & 26.19                                   & 32.93                         & 7.47     & 61.00           & 84.36           & 77.20       & 42.35 & 61.00 & 77.78 & 60.38             \\
      DELLA  Ties                            & 10.14                                     & 40.71            & 84.48                                      & 75.93                                  & 61.56                                 & 30.95                                   & 32.32                         & 7.40     & 56.10           & 82.00           & 84.27       & 42.94 & 56.10 & 83.14 & 60.73             \\
      Breadcrumbs                            & 9.48                                      & 43.06            & 83.60                                      & 73.56                                  & 60.94                                 & 27.25                                   & 31.71                         & 7.52     & 60.20           & 83.88           & 70.91       & 42.14 & 60.20 & 77.40 & 59.91             \\
      Breadcrumbs Ties                       & 10.20                                     & 41.85            & 85.01                                      & 76.61                                  & 61.27                                 & 26.72                                   & 30.49                         & 7.48     & 60.04           & 81.83           & 80.49       & 42.45 & 60.04 & 81.16 & 61.22             \\
      TSVM                                   & 10.40                                     & 44.88            & 84.29                                      & 75.24                                  & 60.87                                 & 28.50                                   & 32.32                         & 7.65     & 61.10           & 83.25           & 78.51       & 43.02 & 61.10 & 80.88 & \textbf{61.67}    \\
      \bottomrule
    \end{tabular}
  }
\end{table*}

\begin{figure}[tb]
    \centering
    % \setlength{\abovecaptionskip}{0cm}   %调整图片标题与图距离
    % \setlength{\belowcaptionskip}{0cm}   %调整图片标题与
    \includegraphics[width=0.99\linewidth]{fig/3H_Trade_off.pdf} % 调整宽度为栏宽的 90%
    \caption{Illustration of the 3H Trade-Off circumstance, where Helpful Training benefits 3H performance simultaneously, but Honest and Harmless Training weaken each other. The grey line (Instruct LLM itself) serves as the reference. }
    \label{fig:3H_trade_off}
\end{figure}
\subsection{Experimental Results}

\textbf{Trade-off between Helpfulness, Honesty, and Harmlessness for LLM Alignment.} In Table \ref{static_llama3} and Table \ref{static_mistral}, we investigate the trade-off between 3H-related abilities of Llama3 \cite{dubey2024llama} and Mistral \cite{jiang2023mistral}. Several key results are summarized:

\underline{First}, \textit{heuristic data mixture strategies can lead to data conflicts that reduce the effectiveness of LLM in any single alignment objective}. By comparing the results of 3H Mixture Full Training (Heuristic) methods and direct DPO training on data with respective annotation dimensions, we can observe that heuristic mixture training without conflict-related design can not achieve good performance at any alignment object due to introduced optimization objects.

\underline{Second}, \textit{conflict-aware data selection methods can mitigate the trade-off in a way, but they are still worse than the results of individual training}. Specifically, compared with ArmoRM that provides multi-dimension scores to select data, Hummer is specially designed for evaluating conflict for preference data, which can achieve better 3H results for LLM alignment. Unfortunately, although we have adjusted different thresholds to help adjust the ratio of different types of preference data, their honesty and harmlessness are still lower than the results of individually trained models on preference data with respective annotation dimensions. For example, hummer can help increase the average score of 3H metrics for Llama3-8B-Instruct (from 57.2 to 63.5), but the honesty (55.60) is still lower than the models trained on the Helpfulness\&Honesty (56.00) and its harmlessness (73.21) is lower than the models trained Helpfulness\&Harmlessness data (74.57). The Mistral also exhibits a similar pattern.


\underline{Third}, \textit{there exist different collaborative relationships between helpfulness, honesty, and harmlessness while performing DPO Training, exhibiting that Helpful Training benefits 3H performance simultaneously, but Honest and Harmless Training weaken each other.} As shown in Figure \ref{fig:3H_trade_off}, set the results of Instruct LLMs as the start point (marks used the grey line in the Figure), we can compare the results of Honesty and Helpfulness after performing individual Helpful, Honest, and Harmless Training to distinguish the relationship between each optimization dimension. From the results, we can observe that only helpful training can optimize the LLM towards higher 3H values simultaneously, both Honest Training and Harmless Training lead to a drop in harmlessness or honesty. For example, honest training can enhance the honesty of Llama3 (orange bar), but weaken the harmless abilities (green bar). In contrast, harmless training leads to opposite conclusions.



% Tables~\ref{static_llama3} and~\ref{static_mistral} demonstrate that parameter-space conflict mitigation---through random sparsification, Top-$k$ filtering, and singular value decomposition---enables model merging techniques to surpass the state-of-the-art data mixture approach (Hummer) in comprehensive 3H performance. 

% \begin{table*}[!htbp]
%   \setlength{\abovecaptionskip}{-2pt}
%   \setlength{\belowcaptionskip}{2pt}
%   \captionsetup{font={small,stretch=1.2}, labelfont={bf}}
%   \renewcommand{\arraystretch}{0.95}
%   \centering
%   \setlength{\tabcolsep}{1.5pt}
%   \caption{\textbf{3H Results on Llama3 Under Static Optimization Setting where we perform DPO training using various datasets at once. For merging methods, we highlight the best score in bold and the second score with underlining.}}
%   \label{static_llama3}
%   \resizebox{\textwidth}{!}{
%     \begin{tabular}{@{}l*{8}{c}c@{}cc@{}cccc@{}}  % 与第一个表格列定义一致
%       \toprule
%       \multirow{2}{*}{\textbf{Methods}} & \multicolumn{8}{c}{\textbf{Helpfulness}} & \textbf{Honesty} & \multicolumn{2}{c}{\textbf{Harmlessness}} & \multirow{2}{*}{\textbf{Helpful\_Avg}} & \multirow{2}{*}{\textbf{Honest\_Avg}} & \multirow{2}{*}{\textbf{Harmless\_Avg}} & \multirow{2}{*}{\textbf{AVG}} \\
%       \cmidrule(lr){2-9} \cmidrule(lr){10-11}
%                                              & Math                                      & GSM8K            & ARC-E                                      & ARC-C                                  & MMLU                                  & MBPP\_Plus                              & HumanEval\_Plus               & MT-Bench & HaluEval\_Wild & Salad\_Bench & OR-Bench &       &       &       &                   \\ \midrule
%       \textbf{llama3-8B-Instruct}            & 28.08                                     & 78.09            & 93.65                                      & 82.03                                  & 68.20                                 & 58.99                                   & 53.05                         & 8.25     & 53.50           & 91.16           & 26.97       & 58.79 & 53.50 & 59.07 & 57.12             \\ \midrule
%       Individual Helpfulness                 & 29.08                                     & 79.30            & 93.65                                      & 81.69                                  & 68.58                                 & 57.94                                   & 58.54                         & 8.33     & 55.00           & 89.83           & 42.06       & 59.64 & 55.00 & 65.95 & 60.20             \\
%       Individual Honesty                     & 28.52                                     & 78.77            & 93.65                                      & 81.36                                  & 68.34                                 & 58.47                                   & 54.27                         & 8.45     & 54.67           & 92.18           & 21.95       & 58.98 & 54.67 & 57.07 & 56.91             \\
%       Individual Harmlessness                & 28.88                                     & 77.33            & 93.65                                      & 82.03                                  & 68.32                                 & 59.79                                   & 52.44                         & 8.15     & 53.33           & 92.36           & 27.92       & 58.82 & 53.33 & 60.14 & 57.43             \\
%       Helpfulness\&Honesty                   & 29.60                                     & 77.63            & 93.47                                      & 82.71                                  & 68.33                                 & 59.79                                   & 59.15                         & 8.18     & 56.00           & 90.86           & 39.80       & 59.86 & 56.00 & 65.33 & 60.40             \\
%       Helpfulness\&Harmlessness              & 30.02                                     & 77.26            & 93.47                                      & 82.37                                  & 68.31                                 & 58.99                                   & 56.11                         & 8.16     & 54.50           & 90.27           & 58.86       & 59.34 & 54.50 & 74.57 & 62.80             \\ \midrule
%       3H Mixture Full Training (Heurisistic) & 28.21                                     & 78.85            & 93.65                                      & 81.69                                  & 68.38                                 & 60.85                                   & 57.32                         & 8.48     & 54.67           & 92.06           & 35.36       & 59.68 & 54.67 & 63.71 & 59.35             \\
%       3H Mixture Full Training (ArmoRM)      & 28.81                                     & 78.97            & 93.65                                      & 82.39                                  & 68.42                                 & 60.55                                   & 58.22                         & 8.52     & 55.50           & 92.11           & 42.12       & 60.24 & 55.50 & 69.02 & 61.59             \\
%       3H Mixture Training (Hummer)           & 29.41                                     & 78.95            & 93.65                                      & 82.69                                  & 68.59                                 & 60.41                                   & 58.15                         & 8.58     & 55.60           & 92.10           & 50.11       & 60.35 & 55.60 & 73.21 & 63.05             \\ \midrule
%       Weight Average                         & 29.80                                     & 78.01            & 93.47                                      & 82.71                                  & 68.43                                 & 59.26                                   & 57.32                         & 8.02     & 57.78           & 91.72           & 41.48       & 59.63 & 57.78 & 66.60 & 61.34             \\
%       Rewarded Soup                          & 29.64                                     & 77.94            & 93.47                                      & 82.71                                  & 68.54                                 & 60.85                                   & 57.93                         & 8.32     & 57.55           & 90.86           & 50.08       & 59.93 & 57.55 & 70.47 & 62.65             \\
%       Model Stock                            & 28.72                                     & 78.24            & 93.47                                      & 82.71                                  & 68.41                                 & 59.79                                   & 56.10                         & 8.03     & 53.00           & 91.62           & 32.28       & 59.43 & 53.00 & 61.95 & 58.13             \\
%       Task Arithmetic                        & 29.02                                     & 79.05            & 93.30                                      & 83.39                                  & 68.35                                 & 57.14                                   & 51.83                         & 8.37     & 57.33           & 91.39           & 28.29       & 58.81 & 57.33 & 59.84 & 58.66             \\
%       Ties                                   & 29.30                                     & 78.17            & 93.30                                      & 83.05                                  & 68.52                                 & 56.61                                   & 53.05                         & 8.20     & 54.33           & 89.13           & 29.07       & 58.78 & 54.33 & 59.10 & 57.40             \\
%       DARE                                   & 29.42                                     & 78.39            & 93.47                                      & 82.71                                  & 68.41                                 & 59.26                                   & 56.71                         & 8.28     & 57.00           & 91.85           & 38.80       & 59.58 & 57.00 & 65.33 & 60.64             \\
%       DARE Ties                              & 29.64                                     & 78.01            & 93.47                                      & 82.71                                  & 68.43                                 & 59.26                                   & 57.32                         & 8.07     & 56.00           & 92.16           & 36.88       & 59.61 & 56.00 & 64.52 & 60.04             \\
%       DELLA                                  & 29.08                                     & 78.92            & 93.30                                      & 83.73                                  & 68.41                                 & 54.76                                   & 52.44                         & 8.43     & 54.67           & 91.37           & 63.31       & 58.63 & 54.67 & 77.34 & 63.55             \\
%       DELLA  Ties                            & 29.20                                     & 75.97            & 93.30                                      & 83.39                                  & 68.46                                 & 56.08                                   & 49.39                         & 8.16     & 54.00           & 87.95           & 70.02       & 57.99 & 54.00 & 78.99 & \underline{63.57} \\
%       Breadcrumbs                            & 29.46                                     & 77.79            & 93.30                                      & 83.39                                  & 68.53                                 & 60.85                                   & 54.88                         & 8.24     & 59.33           & 91.60           & 44.85       & 59.56 & 59.33 & 68.23 & 62.37             \\
%       Breadcrumbs Ties                       & 29.72                                     & 78.54            & 93.30                                      & 83.05                                  & 68.46                                 & 60.05                                   & 53.66                         & 8.14     & 55.50           & 90.00           & 58.52       & 59.37 & 55.50 & 74.26 & 63.04             \\
%       TSVM                                   & 29.92                                     & 77.63            & 93.12                                      & 82.17                                  & 68.51                                 & 59.26                                   & 55.49                         & 8.29     & 56.20           & 89.43           & 67.76       & 59.30 & 56.20 & 78.60 & \textbf{64.70}    \\
%       \bottomrule
%     \end{tabular}
%   }
%   \vspace*{-8pt}
% \end{table*}

\begin{table*}
    % \setlength{\abovecaptionskip}{0cm}
    % \setlength{\belowcaptionskip}{0cm}
    \captionsetup{font={small,stretch=1.25}, labelfont={bf}}
    \renewcommand{\arraystretch}{1}
    \caption{\textbf{3H Results on Llama3 Under Continuous Optimization Setting, where we sequentially perform DPO training using data with annotations about Helpfulness\&Honesety (Stage1), Helpfulness\&Harmlessnes (Stage2) and Helpful (Stage3).For merging methods, we highlight the best score in bold and the second score with underlining.}}
    \label{continuous_llama3}
    \centering
    \setlength{\tabcolsep}{2pt}
    \resizebox{0.99\textwidth}{!}{
        \begin{tabular}{lcccccccc|c|cc|cccc}
            \toprule
            \multirow{2}{*}{\textbf{Methods}} & \multicolumn{8}{c|}{\textbf{Helpfulness}} & \textbf{Honesty} & \multicolumn{2}{c|}{\textbf{Harmlessness}} & \multirow{2}{*}{\textbf{Helpful\_Avg}} & \multirow{2}{*}{\textbf{Honest\_Avg}} & \multirow{2}{*}{\textbf{Harmless\_Avg}} & \multirow{2}{*}{\textbf{AVG}}                                                                                                          \\ \cmidrule{2-12}
                                              & Math                                      & GSM8K            & ARC-E                                      & ARC-C                                  & MMLU                                  & MBPP\_Plus                              & HumanEval\_Plus               & MT-Bench & HaluEval\_Wild & Salad\_Bench & OR-Bench &       &       &       &                   \\ \midrule
            \textbf{Llama3-8B-Instruct}       & 28.08                                     & 78.09            & 93.65                                      & 82.03                                  & 68.20                                 & 58.99                                   & 53.05                         & 8.25     & 53.50           & 91.16           & 26.97       & 58.79 & 53.50 & 59.07 & 57.12             \\ \midrule
            Continual DPO Training Stage1     & 29.60                                     & 77.63            & 93.47                                      & 82.71                                  & 68.33                                 & 59.79                                   & 59.15                         & 8.18     & 56.00           & 90.86           & 39.80       & 59.86 & 56.00 & 65.33 & 60.40             \\
            Continual DPO Training Stage2     & 28.74                                     & 74.60            & 94.00                                      & 83.05                                  & 68.41                                 & 51.59                                   & 56.10                         & 8.25     & 52.20           & 90.55           & 77.95       & 58.09 & 52.20 & 84.25 & 64.85             \\
            Continual DPO Training Stage3     & 28.66                                     & 76.12            & 93.05                                      & 82.83                                  & 68.40                                 & 54.57                                   & 56.10                         & 8.03     & 53.20           & 90.63           & 71.61       & 58.72 & 53.20 & 81.12 & 64.34             \\ \midrule
            Weight Average                    & 29.78                                     & 79.82            & 93.65                                      & 82.37                                  & 68.40                                 & 58.47                                   & 53.65                         & 8.03     & 53.20           & 89.58           & 62.66       & 59.27 & 53.20 & 76.12 & 62.85             \\
            Rewarded Soup                     & 29.40                                     & 79.76            & 93.65                                      & 82.37                                  & 68.48                                 & 58.47                                   & 54.88                         & 8.15     & 54.20           & 89.33           & 62.75       & 59.40 & 54.20 & 76.04 & 63.21             \\
            Model Stock                       & 28.42                                     & 79.15            & 93.65                                      & 82.37                                  & 68.30                                 & 60.05                                   & 53.05                         & 8.25     & 50.60           & 91.27           & 28.96       & 59.16 & 50.60 & 60.12 & 56.63             \\
            Task Arithmetic                   & 28.72                                     & 73.16            & 92.95                                      & 83.05                                  & 68.32                                 & 52.11                                   & 46.34                         & 8.52     & 51.60           & 86.07           & 84.97       & 56.65 & 51.60 & 85.52 & 64.59             \\
            Ties                              & 29.18                                     & 76.50            & 93.65                                      & 83.39                                  & 68.61                                 & 56.35                                   & 43.78                         & 7.71     & 52.80           & 87.55           & 78.59       & 57.40 & 52.80 & 83.07 & 64.42             \\
            DARE                              & 28.18                                     & 73.92            & 92.95                                      & 83.05                                  & 68.30                                 & 51.85                                   & 49.39                         & 8.02     & 52.00           & 85.76           & 85.75       & 56.96 & 52.00 & 85.76 & 64.90             \\
            DARE Ties                         & 29.48                                     & 78.85            & 93.65                                      & 82.37                                  & 68.43                                 & 59.79                                   & 53.66                         & 7.67     & 52.40           & 89.46           & 71.38       & 59.24 & 52.40 & 80.42 & 64.02             \\
            DELLA                             & 27.68                                     & 71.19            & 93.12                                      & 83.05                                  & 68.31                                 & 48.15                                   & 46.34                         & 8.15     & 51.80           & 86.58           & 87.11       & 55.75 & 51.80 & 86.85 & 64.80             \\
            DELLA  Ties                       & 28.94                                     & 72.18            & 93.47                                      & 82.71                                  & 68.41                                 & 53.97                                   & 47.56                         & 8.21     & 52.20           & 87.24           & 84.38       & 56.93 & 52.20 & 85.81 & \textbf{64.98}    \\
            Breadcrumbs                       & 28.92                                     & 78.62            & 93.47                                      & 82.71                                  & 68.45                                 & 55.82                                   & 50.00                         & 8.48     & 52.40           & 87.88           & 72.69       & 58.31 & 52.40 & 80.29 & 63.67             \\
            Breadcrumbs Ties                  & 29.79                                     & 78.77            & 93.65                                      & 83.73                                  & 68.37                                 & 57.41                                   & 56.10                         & 8.57     & 53.40           & 88.26           & 67.64       & 59.55 & 53.40 & 77.95 & 63.63             \\
            TSVM                              & 29.86                                     & 78.99            & 93.65                                      & 83.71                                  & 68.37                                 & 58.51                                   & 55.40                         & 8.40     & 53.80           & 88.68           & 75.14       & 59.61 & 53.80 & 81.79 & \underline{64.91} \\
            % TSVM             & 29.79 & 78.77 & 93.65 & 83.73 & 68.37 & 57.41 & 56.10 & 8.57 &  53.40   & 88.26 & 67.64 & 59.55 & 53.40     & 77.95 & 63.63      \\
            \bottomrule
        \end{tabular}
    }
\end{table*}
\textbf{Comparison results between parameter-level model merging methods and data-aspect mixture methods.}
\underline{First}, \textit{model merging can achieve a better trade-off for 3H optimization in LLM alignment than various data mixture methods through phased optimization paradigm under static optimization settings.} From the perspective of implementation, while they utilize identical training datasets, compared with 3H mixture full training strategies, model merging advocates for phased optimization to negotiate competing alignment objectives through \textit{dimension-specific individual training}, where we first conduct individual training to obtain models for five different annotation dimensions respectively, and then adopt \textit{conflict-aware parameter fusion strategies}, such as random sparsification, Top-$k$ filtering, and singular value decomposition, to merge these models into an ideal one that can achieve close or superior results than full training methods. According to the average score of 3H results, the decoupled optimization of merging yields consistent improvements to data mixture methods across architectures. Take the experiment on Llama3 for example, compared with the best data mixture methods (Hummer), TSVM achieves 5.4 points increase for harmlessness and 0.6 points for honesty while sacrificing only about 1 point for helpfulness, verifying that TSVM can achieve a more favorable balance between 3H optimization. These results collectively confirm that model merging's phased optimization paradigm effectively negotiates competing alignment objectives, which provides new insights for addressing the trilemma of 3H optimization for LLM alignment.

\underline{Second}, \textit{model merging methods enable more effective and efficient 3H optimization in LLM alignment than continual training methods which sequentially optimize the model towards different alignment dimensions.} As shown in Table \ref{continuous_llama3} and Table \ref{continuous_mistral} in the Appendix \ref{appendix_continual}, we sequentially perform DPO training using data with annotations about Helpfulness\&Honesty (Stage1), Helpfulness\&Harmlessness (Stage2) and Helpful (Stage3). Through comparison results between different training stages, we can observe the honesty, helpfulness, and harmlessness of LLMs are interactively enhanced due to forgetting during continual training. For merging experiments, we merge the trained models on different datasets from the universe start point (Llama-3-8B-Instruct) rather than the checkpoints in the middle process of continual training. This can avoid hyper-parameter tuning to ensure the effectiveness of continual DPO training at a specific stage. Moreover, the intermediate checkpoint may overfit to previous optimization objects due to the over-training, which leads to difficulties in adapting new optimized targets. As a new training alternative, model merging can mitigate these issues. Take the TSVM for example, the model from TSVM can consistently outperform the one from the final training status (stage 3) in 3H metrics.



\underline{Third}, \textit{the effect of model merging for 3H optimization is closely related to their conflict-resolution strategies, TSVM consistently achieves better outcomes than other merging methods.} As shown in Table \ref{tab:methods}, we can divide existing parameter-level strategies into three categories: linear consensus, sparsification, and singular value decomposition. Among them, linearly interpolation of full model parameters or task vectors neglects the parameter conflict, limiting their performance for 3H optimization in LLM alignment, with fewer improvement compared with data mixture methods. The sparsification-based method holds the assumption that pruning redundant (DARE and DELLA) or outlier parameters (Breadcrumbs) that do not represent the direction of updations for task vectors can improve the effect of model merging, but the level of sparsity is difficult to control for LLM though already with different sparsification methods. From the results of Table \ref{static_llama3} and Table \ref{static_mistral}, we can observe that there is no fixed and stable trend
for the results of sparsification-based methods due to random sparsification.
For example, DELLA-Ties and DARE-Ties exhibit opposite phenomena in llama3 and mistral. More details about the sparsity that influences the effect of merging can be shown in Appendix \ref{appendix_sparsity}. In contrast, TSVM defines the task singular vector and mitigates the interference through de-correlations as stated above without heavily depending on sparsification, which provides a stable and effective merging strategy for 3H optimization in LLM alignments and consistently achieves better results than other methods.


% \begin{table*}[!htbp]
%   % \setlength{\abovecaptionskip}{0pt}
%   % \setlength{\belowcaptionskip}{-2pt}  % 减小标题下方间距
%   \captionsetup{font={small,stretch=1.2}, labelfont={bf}}
%   \renewcommand{\arraystretch}{0.95}  % 增加行紧凑性
%   \centering
%   \setlength{\tabcolsep}{1.5pt}       % 进一步减小列间距
%   \caption{\textbf{3H Results on Mistral Under Static Optimization Setting where we perform DPO training using various datasets at once. For merging methods, we highlight the best score in bold and the second score with underlining.}}
%   \label{static_mistral}
%   \resizebox{\textwidth}{!}{%         % 使用全宽度缩放
%     \begin{tabular}{@{}l*{8}{c}c@{}cc@{}cccc@{}}  % 优化列定义
%       \toprule
%       \multirow{2}{*}{\textbf{Methods}} & \multicolumn{8}{c}{\textbf{Helpfulness}} & \textbf{Honesty} & \multicolumn{2}{c}{\textbf{Harmlessness}} & \multirow{2}{*}{\textbf{Helpful\_Avg}} & \multirow{2}{*}{\textbf{Honest\_Avg}} & \multirow{2}{*}{\textbf{Harmless\_Avg}} & \multirow{2}{*}{\textbf{AVG}} \\
%       \cmidrule(lr){2-9} \cmidrule(lr){10} \cmidrule(lr){11-12}
%                                              & Math                                      & GSM8K            & ARC-E                                      & ARC-C                                  & MMLU                                  & MBPP\_Plus                              & HumanEval\_Plus               & MT-Bench & HaluEval\_Wild & Salad\_Bench & OR-Bench &       &       &       &                   \\ \midrule
%       \textbf{Mistral-7B-Instruct-V2}        & 9.54                                      & 46.17            & 82.36                                      & 72.88                                  & 59.97                                 & 26.46                                   & 28.66                         & 7.55     & 62.17           & 78.07           & 74.68       & 41.70 & 62.17 & 76.38 & 60.08             \\ \midrule
%       Individual Helpfulness                 & 9.44                                      & 47.38            & 84.48                                      & 75.25                                  & 60.70                                 & 22.49                                   & 32.31                         & 7.80     & 60.60           & 74.84           & 80.67       & 42.48 & 60.60 & 77.76 & 60.28             \\
%       Individual Honesty                     & 9.34                                      & 46.63            & 82.54                                      & 71.19                                  & 59.04                                 & 24.34                                   & 23.78                         & 7.76     & 65.20           & 83.98           & 60.82       & 40.58 & 65.20 & 72.40 & 59.39             \\
%       Individual Harmlessness                & 9.40                                      & 46.10            & 81.42                                      & 72.88                                  & 60.03                                 & 27.51                                   & 30.39                         & 7.43     & 59.00           & 85.43           & 69.54       & 41.90 & 59.00 & 77.49 & 59.46             \\
%       Helpfulness\&Honesty                   & 8.76                                      & 43.14            & 82.01                                      & 74.92                                  & 59.78                                 & 25.93                                   & 27.33                         & 7.59     & 61.33           & 78.74           & 77.23       & 41.18 & 61.33 & 77.99 & 60.17             \\
%       Helpfulness\&Harmlessness              & 9.96                                      & 41.77            & 83.42                                      & 75.59                                  & 61.13                                 & 34.92                                   & 29.27                         & 7.46     & 50.60           & 79.83           & 86.05       & 42.94 & 50.60 & 82.94 & 58.83             \\ \midrule
%       3H Mixture Full Training (Heurisistic) & 9.56                                      & 41.70            & 83.06                                      & 74.24                                  & 60.23                                 & 22.75                                   & 34.15                         & 7.69     & 62.00           & 81.13           & 72.97       & 41.67 & 62.00 & 77.05 & 60.24             \\
%       3H Mixture Full Training (ArmoRM)      & 9.71                                      & 43.70            & 83.65                                      & 74.54                                  & 60.33                                 & 25.11                                   & 33.58                         & 7.75     & 61.95           & 81.10           & 75.55       & 42.30 & 61.95 & 78.32 & 60.85             \\
%       3H Mixture Training (Hummer)           & 9.79                                      & 44.50            & 83.72                                      & 74.89                                  & 60.53                                 & 25.85                                   & 33.15                         & 7.56     & 62.05           & 81.85           & 75.28       & 42.50 & 62.05 & 78.57 & 61.04             \\ \midrule
%       Weight Average                         & 9.86                                      & 45.49            & 82.36                                      & 74.58                                  & 60.70                                 & 27.25                                   & 31.10                         & 7.47     & 60.60           & 81.51           & 72.90       & 42.35 & 60.60 & 77.21 & 60.50             \\
%       Rewarded Soup                          & 9.74                                      & 45.11            & 82.54                                      & 74.58                                  & 60.65                                 & 26.72                                   & 30.49                         & 7.48     & 60.71           & 81.43           & 72.51       & 42.16 & 60.71 & 76.97 & 59.95             \\
%       Model Stock                            & 9.80                                      & 46.93            & 82.36                                      & 74.24                                  & 60.34                                 & 25.13                                   & 30.49                         & 7.19     & 61.81           & 80.11           & 73.78       & 42.06 & 61.81 & 76.95 & 60.27             \\
%       Task Arithmetic                        & 9.76                                      & 44.12            & 84.13                                      & 73.90                                  & 60.86                                 & 27.25                                   & 33.54                         & 7.44     & 61.01           & 83.91           & 71.04       & 42.63 & 61.01 & 77.48 & 60.37             \\
%       Ties                                   & 10.22                                     & 41.47            & 85.19                                      & 74.92                                  & 61.34                                 & 27.25                                   & 31.10                         & 7.46     & 58.73           & 82.15           & 81.13       & 42.37 & 58.73 & 81.64 & 60.91             \\
%       DARE                                   & 10.10                                     & 43.82            & 84.48                                      & 73.90                                  & 60.78                                 & 27.25                                   & 32.93                         & 7.47     & 61.05           & 83.80           & 71.04       & 42.59 & 61.05 & 77.42 & 60.35             \\
%       DARE Ties                              & 10.10                                     & 42.46            & 85.00                                      & 74.58                                  & 61.05                                 & 26.98                                   & 31.71                         & 7.61     & 59.28           & 82.28           & 81.91       & 42.49 & 59.28 & 82.10 & \underline{61.49} \\
%       DELLA                                  & 10.26                                     & 42.76            & 84.83                                      & 73.56                                  & 60.86                                 & 26.19                                   & 32.93                         & 7.47     & 61.00           & 84.36           & 77.20       & 42.35 & 61.00 & 77.78 & 60.38             \\
%       DELLA  Ties                            & 10.14                                     & 40.71            & 84.48                                      & 75.93                                  & 61.56                                 & 30.95                                   & 32.32                         & 7.40     & 56.10           & 82.00           & 84.27       & 42.94 & 56.10 & 83.14 & 60.73             \\
%       Breadcrumbs                            & 9.48                                      & 43.06            & 83.60                                      & 73.56                                  & 60.94                                 & 27.25                                   & 31.71                         & 7.52     & 60.20           & 83.88           & 70.91       & 42.14 & 60.20 & 77.40 & 59.91             \\
%       Breadcrumbs Ties                       & 10.20                                     & 41.85            & 85.01                                      & 76.61                                  & 61.27                                 & 26.72                                   & 30.49                         & 7.48     & 60.04           & 81.83           & 80.49       & 42.45 & 60.04 & 81.16 & 61.22             \\
%       TSVM                                   & 10.40                                     & 44.88            & 84.29                                      & 75.24                                  & 60.87                                 & 28.50                                   & 32.32                         & 7.65     & 61.10           & 83.25           & 78.51       & 43.02 & 61.10 & 80.88 & \textbf{61.67}    \\
%       \bottomrule
%     \end{tabular}
%   }
%   \vspace*{-8pt}  % 表格底部负间距
% \end{table*}
% \subsection{An In-Depth Dissection on Model Merging}
% \textbf{Theoretical analysis}
% \textbf{Empirical result analysis}
% % #### drop掉的参数和outlier mask参数仅有部分重叠


% \begin{figure}[htbp]
%     \centering
% %     \setlength{\abovecaptionskip}{0cm}   %调整图片标题与图距离
% % \setlength{\belowcaptionskip}{0cm}   %调整图片标题与
%     \includegraphics[width=0.99\linewidth]{fig/subsequent_DPO.pdf} % 调整宽度为栏宽的 90%
%     \caption{Illustration of Traing Stage of 3H Optimization, which aims to further enhance LLMs alignment from three perspective based on the existing Initially Aligned LLMs. }
%     \label{fig:3H_stage}
% \end{figure}




