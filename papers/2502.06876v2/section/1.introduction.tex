\section{Introduction}
% (1)in training--->post training Model merging merits
% (2)3H optimization

Large language models (LLMs) have achieved excellent performance in various natural language processing tasks. However, their reliable deployment necessitates a balanced optimization across three critical dimensions: \textit{Helpfulness} (providing accurate and task-aligned responses), \textit{Honesty} (avoiding hallucinations and misinformation), and \textit{Harmlessness} (preventing toxic or unethical outputs), collectively termed as \textbf{3H optimization} \cite{bai2022training,guo2024controllable,sonkar2024pedagogical,yang2024dialectical}. While recent alignment techniques such as constitutional AI \cite{bai2022constitutional}, reinforcement learning from human feedback (RLHF) \cite{dai2023safe}, and Direct Preference Optimization (DPO) \cite{rafailov2024direct} have improved individual aspects of 3H, seeking a balance remains an open challenge. For instance, models optimized for helpfulness may inadvertently generate harmful content \cite{ji2024pku}, and there exits dishonesty in helpful and harmless alignment where LLMs tell lies in generating harmless responses \cite{huang2024dishonesty}. This tension underscores the need for systematic approaches to harmonize 3H objectives.


Traditional methods for enhancing 3H properties often rely on \textit{data mixture} strategies assisted by empirically heuristic rules \cite{lambert2024t}, multi-dimension scores by reward model \cite{wang2024interpretable}, alignment evaluation metric \cite{jiang2024hummer}, where diverse datasets are combined to fine-tune a single model. While effective, these approaches face practical limitations: (i) data curation requires substantial domain expertise and computational resources \cite{ji2024pku}, and (ii) conflicting optimization signals during fine-tuning may lead to difficulties
in prioritizing wanted alignment objectives without weakening others \cite{jiang2024hummer}. As a cost-effective alternative through integrating different specialized aligned models' abilities, model merging has achieved great attention for LLM alignment, addressing key challenges such as avoiding forgetting after fine-tuning \cite{yang2024model,zhu2024model}. But for 3H optimization, the effectiveness of existing merging methods remains underexplored. Simultaneously, systematic comparative analysis between data mixture and model merging methods is merely investigated. While preliminary investigations have emerged \cite{ahmadian2024mix}, these efforts remain narrowly focused on constrained scenarios (e.g., multilingual) or employ partial evaluations of 3H dimensions \cite{tekin2024h}. Thus, it remains elusive whether there are potential improvements in 3H metrics that can be achieved through a benchmarking study of model merging for 3H optimization in LLM alignment. This yields the primary question to be explored:
\begin{center}
\vspace*{-2mm}
\textit{(Q) Can we establish a benchmark for model merging in
3H optimization of LLM alignment, explore the overlooked optimization phenomena and
principles, and then advance the current state of the art?}
\vspace*{-2mm}
\end{center}
To address (Q), we introduce several key innovations that distinguish our work from the most relevant research  \cite{ahmadian2024mix}. We reveal previously overlooked collaborative relationships among the 3H dimensions.  Through establishing a model merging benchmark for 3H optimization in LLM alignment, we explore the effectiveness of a broader range of model merging methods in addressing conflict issues and examine various tasks, model types, and evaluation metrics. Through a detailed comparative analysis of merging and data mixture methods, we demonstrate that model merging paves a novel way to achieving effective 3H optimization for LLM alignment, and the effect of merging depends on the parameter-level conflict resolutions considering the redundant and outlier parameter components. Besides, we further propose a reweighting-based optimization to improve the effect of currently the most effective and stable merging method, leveraging the traits of intrinsic heavy-tailed parameter distribution and sparsity of LLMs. In summary, our key  \textbf{contributions} are listed below. 

$\bullet$ We create the first benchmark to explore the effect of model merging for 3H optimization in LLM alignment. This benchmark includes our investigations into 15 representative methods (12 training-free merging methods and 3 representative data mixture methods), 10 preference datasets associated with 5 annotation dimensions, 2 classific families of LLMs, and 2 different training settings.

$\bullet$ Assisted by our benchmark, we reveal a range of previously overlooked optimization principles and insights for 3H optimization in LLM alignment. These include: different collaborative relationships among 3H dimensions, the superiority of model merging to data mixture methods, and the factors affecting the effect of model merging.

$\bullet$ In addition to a holistic assessment of existing model merging methods, we develop novel enhancements to Task Singular Vector Merging (TSVM)-currently the most effective and stable merging method, through reweighting-based optimizations including outlier weighting and sparsity-aware rank selection. Extensive experiments can verify the effectiveness of these strategies to strengthen the effect of TSVM for better 3H optimization in LLM alignment. 








% For 3H optimization, merging models pre-aligned for distinct attributes (e.g., a helpfulness-optimized model and a harmlessness-focused one) could theoretically preserve complementary strengths while mitigating interference. 


% \begin{figure}[htbp]
%     \centering
%         \setlength{\abovecaptionskip}{0cm}   %调整图片标题与图距离
%     \setlength{\belowcaptionskip}{0cm}   %调整图片标题与
%     \includegraphics[width=0.99\linewidth]{fig/subsequent_DPO.pdf} % 调整宽度为栏宽的 90%
%     \caption{Illustration of Training Stage of 3H Optimization, which aims to further enhance LLMs alignment from three perspectives based on the existing Initially Aligned LLMs.}
%     \label{fig:3H_stage}
% \end{figure}
% Before discussing the trade-off between the 3H abilities of LLMs, we would like to clarify the main difference between our theme and previously defined alignment tax \cite{lin2024mitigating,lu2024online}. In general, the alignment tax describes the phenomenon of RLHF training leading to \emph{the forgetting of pre-trained abilities during the first alignment stage}. However, in this paper as shown in Figure \ref{fig:3H_stage}, we mainly focus on how can we further \emph{enhance the 3H-related abilities of the existing already-aligned model during the second or subsequent stages.} The trade-off mainly comes from the conflict of different alignment objects without dealing with the pre-trained knowledge. Task the Llama3 series for example, alignment tax mainly analyzes the pre-trained ability degradation on the SFT version of the Base LLM (e.g. train the Llama-3-8B on the Ultrachat) while performing DPO training. However, in this paper, we mainly focus on how can we further enhance the 3H-related abilities of the existing already aligned model (e.g. Llama3-8B-Instruct) during the second or subsequent alignment stages, which can meet more strict demands for real and specific applications.