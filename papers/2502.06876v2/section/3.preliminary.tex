\section{Reviewing Model Merging for Multi-Object Alignment Optimization}
\label{section:review_model_merging}
Model merging has emerged as an effective paradigm for cross-model knowledge integration without performance degradation \cite{yang2024model}.
The challenge of multi-objective alignment has been extensively studied in machine learning optimization, particularly in areas like multi-task learning~\cite{sener2018multi,liu2022auto,tang2024fusionbench}.
However, the intersection of model merging and alignment optimization presents unique challenges and opportunities that warrant dedicated investigation.

% Model merging methods have shown surprising effectiveness in combining model knowledge.
Existing merging methods for LLMs \cite{goddard2024arcee} include: Linear interpolation methods such as Rewarded Soups have demonstrated that simple weighted averaging of model parameters can be effective in learning the Pareto frontier of multiple objectives~\citep{rame2024rewarded}.
Given multiple models parameterized by $\theta^1, \theta^2, \cdots, \theta^n$, where each optimizes to a different objective, and a preference vector $w = (w_1, w_2, \cdots, w_n)$, the merged model using Rewarded Soups is defined as:
$\theta_{\text{Rewarded Soups}} = \sum_{i=1}^n w_i \theta^i$.
Building on simple weight interpolation methods such as Task Arithmetic~\citep{ilharco2022editing}, advanced merging approaches like TIES~\citep{yadav2024ties}, DARE~\citep{yu2024language} and Breadcrumbs~\citep{davari2025model} explore more nuanced ways to combine model parameters, often focusing on identifying and preserving crucial subspaces that capture different objectives and resolve the objective conflicts.
In general, these methods can be expressed as $\theta_{\text{Merged}} = \theta_0 + \sum_{i=1}^n w_i m_i \odot (\theta^i - \theta_0)$, where $m_i\in\mathbb{R}^{|\theta|}$ is a binary mask and $\odot$ is the element-wise multiplication.
Model stock \cite{jang2025model} identifies that model performance correlates strongly with proximity to the center of the weight space. Rather than averaging multiple models, Model Stock approximates this optimal center point geometrically.
Task Singular Vector Merge (TSVM) \cite{gargiulo2024task} represents an advanced approach to model merging that addresses the limitations of simpler methods like Task Arithmetic. While Task Arithmetic treats networks as flat parameter vectors, TSVM operates at a layer-wise level by analyzing the singular value decomposition (SVD) of task matrices.
The key innovation of TSVM lies in its treatment of task interference through  Singular Task Interference ($\text{STI}(\{\Delta_i\}_{i=1}^n) = \|(U^\top U - I)\Sigma(V^\top V - I)\|$ where $U$, $\Sigma$, and $V$ are the left singular vectors, singular values, and right singular vectors of the task matrices $\{\Delta_i\}_{i=1}^n$), which measures how task-specific features overlap in weight space. It reduces the task interference by decorrelating the TSVs, which can be formulated as an orthogonal Procrustes problem, seeking the orthogonal matrix $V_{\bot}$ and $U_{\bot}$ to reconstruct the layer-wise parameter for the merged model.
\vspace{-0.2cm}
% 修改后的表格代码
\begin{table}[H]  % 使用H强制定位
  \centering
  \caption{\textbf{Dataset statistics for our DPO training.}}
  \label{tab:dpo_data_stats}
  \resizebox{\columnwidth}{!}{%
    \begin{tabular}{@{}lll@{}}
      \toprule
      \textbf{Annotation Perspective}            & \textbf{Dataset}                       & \textbf{Judge} \\
      \midrule
      \multirow{4}{*}{Helpfulness}               & HelpSteer~\cite{yu2024lions}           & GPT4-Turbo     \\
                                                 & Py-Dpo~\cite{yu2024lions}              & GPT4-Turbo     \\
                                                 & Distilabel-Orca~\cite{yu2024lions}     & GPT4-Turbo     \\
                                                 & Distilabel-Capybara~\cite{yu2024lions} & GPT4-Turbo     \\ 
      \cmidrule(r){1-3}
      Harmlessness                               & UltraSafety~\cite{guo2024controllable} & GPT4-Turbo     \\ 
      \cmidrule(r){1-3}
      \multirow{2}{*}{Honesty}                   & Truthy-Dpo-v0.1~\cite{truthy_dpo_2024} & Human          \\
                                                 & GRATH~\cite{chen2024grath}             & Llama2-SelfGen \\ 
      \cmidrule(r){1-3}
      Helpfulness\&Honesty                       & UltraFeedback~\cite{yu2024lions}       & GPT4-Turbo     \\ 
      \cmidrule(r){1-3}
      \multirow{2}{*}{Helpfulness\&Harmlessness} & PKU-Safe-RLHF~\cite{yu2024lions}       & GPT4-Turbo     \\
                                                 & Nectar~\cite{yu2024lions}              & GPT4-Turbo     \\
      \bottomrule
    \end{tabular}%
  }
  \vspace*{-5pt}  % 表格底部负间距
\end{table}

\vspace{-0.5cm}
\begin{table}[H]  % 使用H强制定位
  \centering
  \caption{\textbf{Necessary specifications for the strategy and scaling of each method.}}
  \label{tab:methods}
  \setlength{\tabcolsep}{3pt}  % 优化列间距
  \renewcommand{\arraystretch}{0.95}  % 压缩行高
  \resizebox{\columnwidth}{!}{%
    \begin{tabular}{@{}lcc@{}}
      \toprule
      \textbf{Method}                           & \textbf{Strategy}                    & \textbf{Scaling}         \\
      \midrule
      \multicolumn{3}{@{}c@{}}{\textbf{Data Mixture-Based Methods}}                                                     \\
      \cmidrule(r){1-3}
      Heuristic~\cite{lambert2024t}             & Empirically heuristic-adjusted ratio & Data Mixture Ratio       \\
      ArmoRM~\cite{wang2024interpretable}       & Reward Model                         & Multi-object Data Selection       \\
      Hummer~\cite{jiang2024hummer}             & Alignment Conflict Metric            & Multi-object Data Selection       \\
      \cmidrule(r){1-3}
      \multicolumn{3}{@{}c@{}}{\textbf{Merging-Based Methods}}                                                          \\
      \cmidrule(r){1-3}
      Weight Average~\cite{wortsman2022model}   & Linear Int. Consensus                & Parameter Weight Coeff.  \\
      Rewarded Soup~\cite{rame2024rewarded}     & Linear Int. Consensus                & Parameter Weight Coeff.  \\
      Task Arithmetic~\cite{ilharco2022editing} & Linear Int. Consensus                & Parameter Scaling Factor \\
      Ties~\cite{yadav2024ties}                 & Top-k Sparsification                 & Parameter Scaling Factor \\
      DARE~\cite{yu2024language}                & Random Sparsification                & Parameter Scaling Factor \\
      DELLA~\cite{deep2024DELLA}                & Random Sparsification                & Parameter Scaling Factor \\
      Breadcrumbs~\cite{davari2025model}        & Top/Bottom-k Sparsification          & Parameter Scaling Factor \\
      Model Stock~\cite{jang2025model}          & Geometric Sparsification             & Parameter Adaptive Ratio \\
      TSVM~\cite{gargiulo2024task}              & Singular Value Decomposition         & Parameter Scaling Factor \\
      \bottomrule
    \end{tabular}%
  }
  \vspace*{-3pt}  % 表格底部负间距
\end{table}




