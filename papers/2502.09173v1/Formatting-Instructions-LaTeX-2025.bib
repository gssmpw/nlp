

@article{kueper_alzheimers_nodate,
	title = {The {Alzheimer}’s {Disease} {Assessment} {Scale}–{Cognitive} {Subscale} ({ADAS}-{Cog}): {Modifications} and {Responsiveness} in {Pre}-{Dementia} {Populations}. {A} {Narrative} {Review}},
	volume = {63},
	issn = {1387-2877},
	shorttitle = {The {Alzheimer}’s {Disease} {Assessment} {Scale}–{Cognitive} {Subscale} ({ADAS}-{Cog})},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5929311/},
	doi = {10.3233/JAD-170991},
	abstract = {The Alzheimer’s Disease Assessment Scale–Cognitive Subscale (ADAS-Cog) was developed in the 1980s to assess the level of cognitive dysfunction in Alzheimer’s disease. Advancements in the research field have shifted focus toward pre-dementia populations, and use of the ADAS-Cog has extended into these pre-dementia studies despite concerns about its ability to detect important changes at these milder stages of disease progression. If the ADAS-Cog cannot detect important changes, our understanding of pre-dementia disease progression may be compromised and trials may incorrectly conclude that a novel treatment approach is not beneficial. The purpose of this review was to assess the performance of the ADAS-Cog in pre-dementia populations, and to review all modifications that have been made to the ADAS-Cog to improve its measurement performance in dementia or pre-dementia populations. The contents of this review are based on bibliographic searches of electronic databases to locate all studies using the ADAS-Cog in pre-dementia samples or subsamples, and to locate all modified versions. Citations from relevant articles were also consulted. Overall, our results suggest the original ADAS-Cog is not an optimal outcome measure for pre-dementia studies; however, given the prominence of the ADAS-Cog, care must be taken when considering the use of alternative outcome measures. Thirty-one modified versions of the ADAS-Cog were found. Modification approaches that appear most beneficial include altering scoring methodology or adding tests of memory, executive function, and/or daily functioning. Although modifications improve the performance of the ADAS-Cog, this is at the cost of introducing heterogeneity that may limit between-study comparison.},
	number = {2},
	urldate = {2024-09-13},
	journal = {Journal of Alzheimer's Disease},
	author = {Kueper, Jacqueline K. and Speechley, Mark and Montero-Odasso, Manuel},
	pmid = {29660938},
	pmcid = {PMC5929311},
	pages = {423--444},
}

@article{kurlowicz_mini-mental_1999,
	title = {The {Mini}-{Mental} {State} {Examination} ({MMSE})},
	volume = {25},
	issn = {0098-9134, 1938-243X},
	url = {https://journals.healio.com/doi/10.3928/0098-9134-19990501-08},
	doi = {10.3928/0098-9134-19990501-08},
	language = {en},
	number = {5},
	urldate = {2024-09-13},
	journal = {Journal of Gerontological Nursing},
	author = {Kurlowicz, Lenore and Wallace, Meredith},
	month = may,
	year = {1999},
	pages = {8--9},
}

@misc{li_synthetic_2023,
	title = {Synthetic {Data} {Generation} with {Large} {Language} {Models} for {Text} {Classification}: {Potential} and {Limitations}},
	shorttitle = {Synthetic {Data} {Generation} with {Large} {Language} {Models} for {Text} {Classification}},
	url = {http://arxiv.org/abs/2310.07849},
	abstract = {The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLMgenerated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation1.},
	language = {en},
	urldate = {2024-09-12},
	publisher = {arXiv},
	author = {Li, Zhuoyan and Zhu, Hangxiao and Lu, Zhuoran and Yin, Ming},
	month = oct,
	year = {2023},
	note = {arXiv:2310.07849 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{xiao_c-pack_2023,
	title = {C-{Pack}: {Packaged} {Resources} {To} {Advance} {General} {Chinese} {Embedding}},
	author = {Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Muennighoff, Niklas},
	year = {2023},
	note = {\_eprint: 2309.07597},
}

@misc{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1711.05101},
	doi = {10.48550/arXiv.1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
	urldate = {2024-09-12},
	publisher = {arXiv},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = jan,
	year = {2019},
	note = {arXiv:1711.05101 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
}

@article{harris_array_2020,
	title = {Array programming with {NumPy}},
	volume = {585},
	doi = {10.1038/s41586-020-2649-2},
	journal = {Nature},
	author = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, Stéfan J and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and Fernández del Río, Jaime and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	year = {2020},
	pages = {357--362},
}

@inproceedings{mckinney_data_2010,
	title = {Data structures for statistical computing in python},
	volume = {445},
	booktitle = {Proceedings of the 9th {Python} in {Science} {Conference}},
	publisher = {Austin, TX},
	author = {McKinney, Wes and {others}},
	year = {2010},
	pages = {51--56},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} learning in {Python}},
	volume = {12},
	number = {Oct},
	journal = {Journal of machine learning research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and {others}},
	year = {2011},
	pages = {2825--2830},
}

@inproceedings{reimers_sentence-bert_2019,
	title = {Sentence-{BERT}: {Sentence} {Embeddings} using {Siamese} {BERT}-{Networks}},
	url = {http://arxiv.org/abs/1908.10084},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Reimers, Nils and Gurevych, Iryna},
	month = nov,
	year = {2019},
}

@inproceedings{wolf_transformers_2020,
	title = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Perric and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
	month = oct,
	year = {2020},
	pages = {38--45},
}

@incollection{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
	pages = {8024--8035},
}

@misc{noauthor_how_nodate,
	title = {How to cite {Pytorch} - {Cite} {Bay}},
	url = {http://citebay.com/how-to-cite/pytorch/},
	language = {en},
	urldate = {2024-09-11},
}

@misc{ansel_pytorch_2024,
	title = {{PyTorch} 2: {Faster} {Machine} {Learning} {Through} {Dynamic} {Python} {Bytecode} {Transformation} and {Graph} {Compilation}},
	shorttitle = {{PyTorch} 2},
	url = {https://pytorch.org/assets/pytorch2-2.pdf},
	abstract = {Tensors and Dynamic neural networks in Python with strong GPU acceleration},
	urldate = {2024-09-11},
	publisher = {ACM},
	author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, CK and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Suo, Michael and Tillet, Phil and Wang, Eikan and Wang, Xiaodong and Wen, William and Zhang, Shunting and Zhao, Xu and Zhou, Keren and Zou, Richard and Mathews, Ajit and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
	month = apr,
	year = {2024},
	doi = {10.1145/3620665.3640366},
	note = {Publication Title: 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24)
original-date: 2016-08-13T05:26:41Z},
}

@misc{noauthor_file_nodate,
	title = {File not found · pytorch/pytorch},
	url = {https://github.com/pytorch/pytorch},
	abstract = {Tensors and Dynamic neural networks in Python with strong GPU acceleration - File not found · pytorch/pytorch},
	language = {en},
	urldate = {2024-09-11},
	journal = {GitHub},
}

@misc{muennighoff_mteb_2023,
	title = {{MTEB}: {Massive} {Text} {Embedding} {Benchmark}},
	shorttitle = {{MTEB}},
	url = {http://arxiv.org/abs/2210.07316},
	abstract = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the ﬁeld difﬁcult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We ﬁnd that no particular text embedding method dominates across all tasks. This suggests that the ﬁeld has yet to converge on a universal text embedding method and scale it up sufﬁciently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https: //github.com/embeddings-benchm ark/mteb.},
	language = {en},
	urldate = {2024-09-11},
	publisher = {arXiv},
	author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Loïc and Reimers, Nils},
	month = mar,
	year = {2023},
	note = {arXiv:2210.07316 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@misc{noauthor_31110_nodate,
	title = {3.11.10 {Documentation}},
	url = {https://docs.python.org/3.11/},
	urldate = {2024-09-11},
}

@article{van_der_maaten_viualizing_2008,
	title = {Viualizing data using t-{SNE}},
	volume = {9},
	abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza-tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
	journal = {Journal of Machine Learning Research},
	author = {van der Maaten, Laurens and Hinton, Geoffrey},
	month = nov,
	year = {2008},
	pages = {2579--2605},
}

@misc{noauthor_sentence-transformersall-minilm-l12-v2_2024,
	title = {sentence-transformers/all-{MiniLM}-{L12}-v2 · {Hugging} {Face}},
	url = {https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-09-11},
	month = jan,
	year = {2024},
}

@misc{hoffer_deep_2018,
	title = {Deep metric learning using {Triplet} network},
	url = {http://arxiv.org/abs/1412.6622},
	doi = {10.48550/arXiv.1412.6622},
	abstract = {Deep learning has proven itself as a successful set of models for learning useful semantic representations of data. These, however, are mostly implicitly learned as part of a classification task. In this paper we propose the triplet network model, which aims to learn useful representations by distance comparisons. A similar model was defined by Wang et al. (2014), tailor made for learning a ranking for image information retrieval. Here we demonstrate using various datasets that our model learns a better representation than that of its immediate competitor, the Siamese network. We also discuss future possible usage as a framework for unsupervised learning.},
	urldate = {2024-09-11},
	publisher = {arXiv},
	author = {Hoffer, Elad and Ailon, Nir},
	month = dec,
	year = {2018},
	note = {arXiv:1412.6622 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ivan_when_2011,
	title = {When the {Web} meets the cell: using personalized {PageRank} for analyzing protein interaction networks},
	volume = {27},
	issn = {1367-4811, 1367-4803},
	shorttitle = {When the {Web} meets the cell},
	url = {https://academic.oup.com/bioinformatics/article/27/3/405/321946},
	doi = {10.1093/bioinformatics/btq680},
	abstract = {Motivation: Enormous and constantly increasing quantity of biological information is represented in metabolic and in protein interaction network databases. Most of these data are freely accessible through large public depositories. The robust analysis of these resources needs novel technologies, being developed today.},
	language = {en},
	number = {3},
	urldate = {2024-09-11},
	journal = {Bioinformatics},
	author = {Iván, Gábor and Grolmusz, Vince},
	month = feb,
	year = {2011},
	pages = {405--407},
}

@article{banky_equal_2013,
	title = {Equal {Opportunity} for {Low}-{Degree} {Network} {Nodes}: {A} {PageRank}-{Based} {Method} for {Protein} {Target} {Identification} in {Metabolic} {Graphs}},
	volume = {8},
	issn = {1932-6203},
	shorttitle = {Equal {Opportunity} for {Low}-{Degree} {Network} {Nodes}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0054204},
	doi = {10.1371/journal.pone.0054204},
	abstract = {Biological network data, such as metabolic-, signaling- or physical interaction graphs of proteins are increasingly available in public repositories for important species. Tools for the quantitative analysis of these networks are being developed today. Protein network-based drug target identification methods usually return protein hubs with large degrees in the networks as potentially important targets. Some known, important protein targets, however, are not hubs at all, and perturbing protein hubs in these networks may have several unwanted physiological effects, due to their interaction with numerous partners. Here, we show a novel method applicable in networks with directed edges (such as metabolic networks) that compensates for the low degree (non-hub) vertices in the network, and identifies important nodes, regardless of their hub properties. Our method computes the PageRank for the nodes of the network, and divides the PageRank by the in-degree (i.e., the number of incoming edges) of the node. This quotient is the same in all nodes in an undirected graph (even for large- and low-degree nodes, that is, for hubs and non-hubs as well), but may differ significantly from node to node in directed graphs. We suggest to assign importance to non-hub nodes with large PageRank/in-degree quotient. Consequently, our method gives high scores to nodes with large PageRank, relative to their degrees: therefore non-hub important nodes can easily be identified in large networks. We demonstrate that these relatively high PageRank scores have biological relevance: the method correctly finds numerous already validated drug targets in distinct organisms (Mycobacterium tuberculosis, Plasmodium falciparum and MRSA Staphylococcus aureus), and consequently, it may suggest new possible protein targets as well. Additionally, our scoring method was not chosen arbitrarily: its value for all nodes of all undirected graphs is constant; therefore its high value captures importance in the directed edge structure of the graph.},
	language = {en},
	number = {1},
	urldate = {2024-09-11},
	journal = {PLOS ONE},
	author = {Bánky, Dániel and Iván, Gábor and Grolmusz, Vince},
	month = jan,
	year = {2013},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Drug discovery, Drug metabolism, Graphs, Metabolic networks, Mycobacterium tuberculosis, Protein interaction networks, Protein metabolism},
	pages = {e54204},
}

@inproceedings{page_pagerank_1999,
	title = {The {PageRank} {Citation} {Ranking} : {Bringing} {Order} to the {Web}},
	shorttitle = {The {PageRank} {Citation} {Ranking}},
	url = {https://www.semanticscholar.org/paper/The-PageRank-Citation-Ranking-%3A-Bringing-Order-to-Page-Brin/eb82d3035849cd23578096462ba419b53198a556},
	abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.},
	urldate = {2024-09-11},
	author = {Page, Lawrence and Brin, Sergey and Motwani, R. and Winograd, T.},
	month = nov,
	year = {1999},
}

@misc{liu_autotimes_2024,
	title = {{AutoTimes}: {Autoregressive} {Time} {Series} {Forecasters} via {Large} {Language} {Models}},
	shorttitle = {{AutoTimes}},
	url = {https://arxiv.org/abs/2402.02370v2},
	abstract = {Foundation models of time series have not been fully developed due to the limited availability of time series corpora and the underexploration of scalable pre-training. Based on the similar sequential formulation of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, the inherent autoregressive property and decoder-only architecture of LLMs have not been fully considered, resulting in insufficient utilization of LLM abilities. To further exploit the general-purpose token transition and multi-step generation ability of large language models, we propose AutoTimes to repurpose LLMs as autoregressive time series forecasters, which independently projects time series segments into the embedding space and autoregressively generates future predictions with arbitrary lengths. Compatible with any decoder-only LLMs, the consequent forecaster exhibits the flexibility of the lookback length and scalability of the LLM size. Further, we formulate time series as prompts, extending the context for prediction beyond the lookback window, termed in-context forecasting. By adopting textual timestamps as position embeddings, AutoTimes integrates multimodality for multivariate scenarios. Empirically, AutoTimes achieves state-of-the-art with 0.1\% trainable parameters and over 5 times training/inference speedup compared to advanced LLM-based forecasters.},
	language = {en},
	urldate = {2024-09-11},
	journal = {arXiv.org},
	author = {Liu, Yong and Qin, Guo and Huang, Xiangdong and Wang, Jianmin and Long, Mingsheng},
	month = feb,
	year = {2024},
}

@misc{bian_multi-patch_2024,
	title = {Multi-{Patch} {Prediction}: {Adapting} {LLMs} for {Time} {Series} {Representation} {Learning}},
	shorttitle = {Multi-{Patch} {Prediction}},
	url = {https://arxiv.org/abs/2402.04852v2},
	abstract = {In this study, we present aLLM4TS, an innovative framework that adapts Large Language Models (LLMs) for time-series representation learning. Central to our approach is that we reconceive time-series forecasting as a self-supervised, multi-patch prediction task, which, compared to traditional contrastive learning or mask-and-reconstruction methods, captures temporal dynamics in patch representations more effectively. Our strategy encompasses two-stage training: (i). a causal continual pre-training phase on various time-series datasets, anchored on next patch prediction, effectively syncing LLM capabilities with the intricacies of time-series data; (ii). fine-tuning for multi-patch prediction in the targeted time-series context. A distinctive element of our framework is the patch-wise decoding layer, which departs from previous methods reliant on sequence-level decoding. Such a design directly transposes individual patches into temporal sequences, thereby significantly bolstering the model's proficiency in mastering temporal patch-based representations. aLLM4TS demonstrates superior performance in several downstream tasks, proving its effectiveness in deriving temporal representations with enhanced transferability and marking a pivotal advancement in the adaptation of LLMs for time-series analysis.},
	language = {en},
	urldate = {2024-09-11},
	journal = {arXiv.org},
	author = {Bian, Yuxuan and Ju, Xuan and Li, Jiangtong and Xu, Zhijian and Cheng, Dawei and Xu, Qiang},
	month = feb,
	year = {2024},
}

@misc{team_gemma_2024,
	title = {Gemma 2: {Improving} {Open} {Language} {Models} at a {Practical} {Size}},
	shorttitle = {Gemma 2},
	url = {https://arxiv.org/abs/2408.00118v2},
	abstract = {In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3 times bigger. We release all our models to the community.},
	language = {en},
	urldate = {2024-09-11},
	journal = {arXiv.org},
	author = {Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, Léonard and Mesnard, Thomas and Shahriari, Bobak and Ramé, Alexandre and Ferret, Johan and Liu, Peter and Tafti, Pouya and Friesen, Abe and Casbon, Michelle and Ramos, Sabela and Kumar, Ravin and Lan, Charline Le and Jerome, Sammy and Tsitsulin, Anton and Vieillard, Nino and Stanczyk, Piotr and Girgin, Sertan and Momchev, Nikola and Hoffman, Matt and Thakoor, Shantanu and Grill, Jean-Bastien and Neyshabur, Behnam and Bachem, Olivier and Walton, Alanna and Severyn, Aliaksei and Parrish, Alicia and Ahmad, Aliya and Hutchison, Allen and Abdagic, Alvin and Carl, Amanda and Shen, Amy and Brock, Andy and Coenen, Andy and Laforge, Anthony and Paterson, Antonia and Bastian, Ben and Piot, Bilal and Wu, Bo and Royal, Brandon and Chen, Charlie and Kumar, Chintu and Perry, Chris and Welty, Chris and Choquette-Choo, Christopher A. and Sinopalnikov, Danila and Weinberger, David and Vijaykumar, Dimple and Rogozińska, Dominika and Herbison, Dustin and Bandy, Elisa and Wang, Emma and Noland, Eric and Moreira, Erica and Senter, Evan and Eltyshev, Evgenii and Visin, Francesco and Rasskin, Gabriel and Wei, Gary and Cameron, Glenn and Martins, Gus and Hashemi, Hadi and Klimczak-Plucińska, Hanna and Batra, Harleen and Dhand, Harsh and Nardini, Ivan and Mein, Jacinda and Zhou, Jack and Svensson, James and Stanway, Jeff and Chan, Jetha and Zhou, Jin Peng and Carrasqueira, Joana and Iljazi, Joana and Becker, Jocelyn and Fernandez, Joe and van Amersfoort, Joost and Gordon, Josh and Lipschultz, Josh and Newlan, Josh and Ji, Ju-yeong and Mohamed, Kareem and Badola, Kartikeya and Black, Kat and Millican, Katie and McDonell, Keelin and Nguyen, Kelvin and Sodhia, Kiranbir and Greene, Kish and Sjoesund, Lars Lowe and Usui, Lauren and Sifre, Laurent and Heuermann, Lena and Lago, Leticia and McNealus, Lilly and Soares, Livio Baldini and Kilpatrick, Logan and Dixon, Lucas and Martins, Luciano and Reid, Machel and Singh, Manvinder and Iverson, Mark and Görner, Martin and Velloso, Mat and Wirth, Mateo and Davidow, Matt and Miller, Matt and Rahtz, Matthew and Watson, Matthew and Risdal, Meg and Kazemi, Mehran and Moynihan, Michael and Zhang, Ming and Kahng, Minsuk and Park, Minwoo and Rahman, Mofi and Khatwani, Mohit and Dao, Natalie and Bardoliwalla, Nenshad and Devanathan, Nesh and Dumai, Neta and Chauhan, Nilay and Wahltinez, Oscar and Botarda, Pankil and Barnes, Parker and Barham, Paul and Michel, Paul and Jin, Pengchong and Georgiev, Petko and Culliton, Phil and Kuppala, Pradeep and Comanescu, Ramona and Merhej, Ramona and Jana, Reena and Rokni, Reza Ardeshir and Agarwal, Rishabh and Mullins, Ryan and Saadat, Samaneh and Carthy, Sara Mc and Perrin, Sarah and Arnold, Sébastien M. R. and Krause, Sebastian and Dai, Shengyang and Garg, Shruti and Sheth, Shruti and Ronstrom, Sue and Chan, Susan and Jordan, Timothy and Yu, Ting and Eccles, Tom and Hennigan, Tom and Kocisky, Tomas and Doshi, Tulsee and Jain, Vihan and Yadav, Vikas and Meshram, Vilobh and Dharmadhikari, Vishal and Barkley, Warren and Wei, Wei and Ye, Wenming and Han, Woohyun and Kwon, Woosuk and Xu, Xiang and Shen, Zhe and Gong, Zhitao and Wei, Zichuan and Cotruta, Victor and Kirk, Phoebe and Rao, Anand and Giang, Minh and Peran, Ludovic and Warkentin, Tris and Collins, Eli and Barral, Joelle and Ghahramani, Zoubin and Hadsell, Raia and Sculley, D. and Banks, Jeanine and Dragan, Anca and Petrov, Slav and Vinyals, Oriol and Dean, Jeff and Hassabis, Demis and Kavukcuoglu, Koray and Farabet, Clement and Buchatskaya, Elena and Borgeaud, Sebastian and Fiedel, Noah and Joulin, Armand and Kenealy, Kathleen and Dadashi, Robert and Andreev, Alek},
	month = jul,
	year = {2024},
}

@book{sam_llama_2024,
	title = {Llama 3.1: {An} {In}-{Depth} {Analysis} of the {Next} {Generation} {Large} {Language} {Model}},
	shorttitle = {Llama 3.1},
	abstract = {Llama 3.1 405B, the first open model with 405 billion parameters, rivals top AI models in knowledge, math, tool use, and multilingual translation. Its release introduces opportunities for innovation, including synthetic data generation and model distillation. Upgraded 8B and 70B models feature a 128K context length and enhanced reasoning. Evaluated on over 150 benchmarks, Llama 3.1 excels in diverse tasks and competes with leading models like GPT-4. Openly available, it enables developers to customize and deploy advanced AI solutions, fostering broad-based AI innovation and application.},
	author = {Sam, Kira and Vavekanand, Raja},
	month = jul,
	year = {2024},
	doi = {10.13140/RG.2.2.10628.74882},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {https://arxiv.org/abs/2005.14165v4},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	language = {en},
	urldate = {2024-09-11},
	journal = {arXiv.org},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = may,
	year = {2020},
}

@article{rhanoui_forecasting_2019,
	title = {Forecasting financial budget time series: {ARIMA} random walk vs {LSTM} neural network},
	volume = {8},
	copyright = {Copyright (c) 2019 Institute of Advanced Engineering and Science},
	issn = {2252-8938},
	shorttitle = {Forecasting financial budget time series},
	url = {https://ijai.iaescore.com/index.php/IJAI/article/view/20275},
	doi = {10.11591/ijai.v8.i4.pp317-327},
	abstract = {Financial time series are volatile, non-stationary and non-linear data that are affected by external economic factors. There is several performant predictive approaches such as univariate ARIMA model and more recently Recurrent Neural Network. The accurate forecasting of budget data is a strategic and challenging task for an optimal management of resources, it requires the use of the most accurate model. We propose a predictive approach that uses and compares the Machine Learning ARIMA model and Deep Learning Recurrent LSTM model. The application and the comparative analysis show that the LSTM model outperforms the ARIMA model, mainly thanks to the LSTMs ability to learn non-linear relationship from data.},
	language = {en-US},
	number = {4},
	urldate = {2024-09-11},
	journal = {IAES International Journal of Artificial Intelligence (IJ-AI)},
	author = {Rhanoui, Maryem and Yousfi, Siham and Mikram, Mounia and Merizak, Hajar},
	month = dec,
	year = {2019},
	note = {Number: 4},
	keywords = {ARIMA, Deep learning, Financial time series, LSTM, Machine learning, RNN, Random walk},
	pages = {317--327},
}

@article{masini_machine_2023,
	title = {Machine learning advances for time series forecasting},
	volume = {37},
	issn = {1467-6419},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/joes.12429},
	doi = {10.1111/joes.12429},
	abstract = {In this paper, we survey the most recent advances in supervised machine learning (ML) and high-dimensional models for time-series forecasting. We consider both linear and nonlinear alternatives. Among the linear methods, we pay special attention to penalized regressions and ensemble of models. The nonlinear methods considered in the paper include shallow and deep neural networks, in their feedforward and recurrent versions, and tree-based methods, such as random forests and boosted trees. We also consider ensemble and hybrid models by combining ingredients from different alternatives. Tests for superior predictive ability are briefly reviewed. Finally, we discuss application of ML in economics and finance and provide an illustration with high-frequency financial data.},
	language = {en},
	number = {1},
	urldate = {2024-09-11},
	journal = {Journal of Economic Surveys},
	author = {Masini, Ricardo P. and Medeiros, Marcelo C. and Mendes, Eduardo F.},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/joes.12429},
	keywords = {bagging, boosting, deep learning, forecasting, machine learning, neural networks, nonlinear models, penalized regressions, random forests, regression trees, regularization, sieve approximation, statistical learning theory},
	pages = {76--111},
}

@article{rizvi_arima_2024,
	title = {{ARIMA} {Model} {Time} {Series} {Forecasting}},
	volume = {12},
	issn = {23219653},
	url = {https://www.ijraset.com/best-journal/arima-model-time-series-forecasting},
	doi = {10.22214/ijraset.2024.62416},
	abstract = {Time series forecasting is a critical component in various fields such as finance, economics, meteorology, and engineering. Among the multitude of methods available for time series forecasting, the Autoregressive Integrated Moving Average (ARIMA) model stands out for its simplicity and effectiveness. This paper provides a comprehensive review of ARIMA models, focusing on their application in forecasting time series data. We begin with an overview of time series analysis and the theoretical foundations of ARIMA models. Subsequently, we delve into the process of building and fitting ARIMA models, discussing the steps involved and the considerations for model selection. Furthermore, we explore advanced topics such as seasonal ARIMA (SARIMA) models and discuss their relevance in handling seasonal data patterns. Additionally, we review recent advancements and extensions of ARIMA models, including hybrid models and machine learning-based approaches. Finally, we discuss the challenges and limitations associated with ARIMA modeling and provide recommendations for future research directions.},
	language = {en},
	number = {5},
	urldate = {2024-09-11},
	journal = {International Journal for Research in Applied Science and Engineering Technology},
	author = {Rizvi, Mohd Faizan},
	month = may,
	year = {2024},
	pages = {3782--3785},
}

@article{kontopoulou_review_2023,
	title = {A {Review} of {ARIMA} vs. {Machine} {Learning} {Approaches} for {Time} {Series} {Forecasting} in {Data} {Driven} {Networks}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1999-5903},
	url = {https://www.mdpi.com/1999-5903/15/8/255},
	doi = {10.3390/fi15080255},
	abstract = {In the broad scientific field of time series forecasting, the ARIMA models and their variants have been widely applied for half a century now due to their mathematical simplicity and flexibility in application. However, with the recent advances in the development and efficient deployment of artificial intelligence models and techniques, the view is rapidly changing, with a shift towards machine and deep learning approaches becoming apparent, even without a complete evaluation of the superiority of the new approach over the classic statistical algorithms. Our work constitutes an extensive review of the published scientific literature regarding the comparison of ARIMA and machine learning algorithms applied to time series forecasting problems, as well as the combination of these two approaches in hybrid statistical-AI models in a wide variety of data applications (finance, health, weather, utilities, and network traffic prediction). Our review has shown that the AI algorithms display better prediction performance in most applications, with a few notable exceptions analyzed in our Discussion and Conclusions sections, while the hybrid statistical-AI models steadily outperform their individual parts, utilizing the best algorithmic features of both worlds.},
	language = {en},
	number = {8},
	urldate = {2024-09-11},
	journal = {Future Internet},
	author = {Kontopoulou, Vaia I. and Panagopoulos, Athanasios D. and Kakkos, Ioannis and Matsopoulos, George K.},
	month = aug,
	year = {2023},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {ARIMA, MAE, MAPE, MSE, RMSE, deep learning, finance, health, hybrid, machine learning, networks, weather},
	pages = {255},
}

@inproceedings{hill_semi-supervised_2022,
	title = {Semi-supervised {Embedding} for {Scalable} and {Accurate} {Time} {Series} {Clustering}},
	url = {https://ieeexplore.ieee.org/document/10020324},
	doi = {10.1109/BigData55660.2022.10020324},
	abstract = {While time series data are abundant in numerous real world applications, large labeled time series datasets are scarce. Semi-supervised models, which leverage small amounts of labeled data along with a large set of unlabeled data, have been shown to significantly outperform unsupervised learning models that only rely on unlabeled data for time series clustering. However, existing semi-supervised time series clustering algorithms suffer from lack of scalability as they are limited to perform learning operations within the original data space. We propose a scalable and accurate autoencoder-based semi-supervised learning model for time series clustering in the embedded space. With this model, we also introduce multiple semi-supervised objective functions that leverage only a small number of labeled examples but significantly improve the quality of the autoencoder’s learned latent space for clustering. Our experiments on a variety of datasets show that our methods can often improve performance of a typical clustering method (namely, k-means). We demonstrate that our methods achieve a maximum average Adjusted Rand Index (ARI) of 0.897, a 140\% increase over an unsupervised Convolutional Autoencoder (CAE) model. Finally, our proposed methods also achieve a maximum improvement of 44\% over an existing semi-supervised model.},
	urldate = {2024-09-11},
	booktitle = {2022 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Hill, Andrew and Bowler, Russell and Kechris, Katerina and Banaei-Kashani, Farnoush},
	month = dec,
	year = {2022},
	keywords = {Adaptation models, Big Data, Predictive models, Semisupervised learning, Time series analysis, Training, Veins},
	pages = {942--951},
}

@article{ye_lbp4mts_2023,
	title = {{LBP4MTS}: {Local} {Binary} {Pattern}-{Based} {Unsupervised} {Representation} {Learning} of {Multivariate} {Time} {Series}},
	volume = {11},
	issn = {2169-3536},
	shorttitle = {{LBP4MTS}},
	url = {https://ieeexplore.ieee.org/document/10292642/?arnumber=10292642},
	doi = {10.1109/ACCESS.2023.3327015},
	abstract = {Representation learning of multivariate time series is a crucial and complex task that offers valuable insights for numerous applications, including time series classification, trend analysis, and regression. Unsupervised learning approaches are often favored in practical scenarios due to the limited availability of labeled data. However, most existing studies focus more on the global information of time series and ignore the local information, especially the representation learning based on the self-attention mechanism. This affects representation performance and may lead to the failure of downstream tasks. This study proposed an unsupervised representation learning model for multivariate time series by comprehensively considering multivariate time series data’s global and local information. Specifically, a specially designed local binary pattern (LBP) method for multivariate time series (multivariate LBP) is introduced to the self-attention mechanism to improve the representation performance of modeling in terms of local information. Additionally, we propose a novel unsupervised approach for learning multivariate time series representations. The experimental results demonstrate significant advantages of our model over other representation learning methods and can be well applied in various downstream tasks.},
	urldate = {2024-09-11},
	journal = {IEEE Access},
	author = {Ye, Chengyang and Ma, Qiang},
	year = {2023},
	note = {Conference Name: IEEE Access},
	keywords = {Data models, Feature extraction, Representation learning, Task analysis, Time series analysis, Training, Transformers, Unsupervised learning, Unsupervised representation learning, global and local features, local binary pattern, multivariate time series},
	pages = {118595--118605},
}

@misc{franceschi_unsupervised_2020,
	title = {Unsupervised {Scalable} {Representation} {Learning} for {Multivariate} {Time} {Series}},
	url = {http://arxiv.org/abs/1901.10738},
	abstract = {Time series constitute a challenging data type for machine learning algorithms, due to their highly variable lengths and sparse labeling in practice. In this paper, we tackle this challenge by proposing an unsupervised method to learn universal embeddings of time series. Unlike previous works, it is scalable with respect to their length and we demonstrate the quality, transferability and practicability of the learned representations with thorough experiments and comparisons. To this end, we combine an encoder based on causal dilated convolutions with a novel triplet loss employing time-based negative sampling, obtaining general-purpose representations for variable length and multivariate time series.},
	language = {en},
	urldate = {2024-09-11},
	publisher = {arXiv},
	author = {Franceschi, Jean-Yves and Dieuleveut, Aymeric and Jaggi, Martin},
	month = jan,
	year = {2020},
	note = {arXiv:1901.10738 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{lyu_improving_2018,
	title = {Improving {Clinical} {Predictions} through {Unsupervised} {Time} {Series} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1812.00490},
	abstract = {In this work, we investigate unsupervised representation learning on medical time series, which bears the promise of leveraging copious amounts of existing unlabeled data in order to eventually assist clinical decision making. By evaluating on the prediction of clinically relevant outcomes, we show that in a practical setting, unsupervised representation learning can offer clear performance beneﬁts over endto-end supervised architectures. We experiment with using sequence-to-sequence (Seq2Seq) models in two different ways, as an autoencoder and as a forecaster, and show that the best performance is achieved by a forecasting Seq2Seq model with an integrated attention mechanism, proposed here for the ﬁrst time in the setting of unsupervised learning for medical time series.},
	language = {en},
	urldate = {2024-09-11},
	publisher = {arXiv},
	author = {Lyu, Xinrui and Hueser, Matthias and Hyland, Stephanie L. and Zerveas, George and Raetsch, Gunnar},
	month = dec,
	year = {2018},
	note = {arXiv:1812.00490 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{lee_spatio-temporal_2024,
	title = {Spatio-{Temporal} {Consistency} for {Multivariate} {Time}-{Series} {Representation} {Learning}},
	volume = {12},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/10445124},
	doi = {10.1109/ACCESS.2024.3369679},
	abstract = {Label sparsity in multivariate time series (MTS) makes using label information for practical applications challenging. Thus, unsupervised representation learning methods have gained attention to learn effective representations suitable for various MTS tasks without relying on labels. Recently, contrastive learning has emerged as a promising approach to generate robust representations by capturing underlying MTS information. However, the existing methods have some limitations, such as insufficient consideration of cross-variable relationships of MTS and high sensitivity to positive pairs. Therefore, we proposed a novel spatio-temporal contrastive representation learning method (STCR) designed to address these limitations. STCR focuses on learning robust representations by encouraging spatio-temporal consistency, which comprehensively considers spatial information as well as temporal dependencies in MTS. The results of extensive experiments on MTS classification and forecasting tasks demonstrate the efficacy of STCR in generating high-quality representations, achieving state-of-the-art performance on both tasks.},
	urldate = {2024-09-11},
	journal = {IEEE Access},
	author = {Lee, Sangho and Kim, Wonjoon and Son, Youngdoo},
	year = {2024},
	note = {Conference Name: IEEE Access},
	keywords = {Contrastive learning, Forecasting, Labeling, Multivariate regression, Representation learning, Self-supervised learning, Spatiotemporal phenomena, Task analysis, Time series analysis, Transformers, Vectors, cross-variable relations, multivariate time series, representation learning, temporal dependency},
	pages = {30962--30975},
}

@article{franceschi_unsupervised_2019,
	title = {Unsupervised {Scalable} {Representation} {Learning} for {Multivariate} {Time} {Series}},
	url = {https://www.semanticscholar.org/paper/Unsupervised-Scalable-Representation-Learning-for-Franceschi-Dieuleveut/1d514906fcc522aa08bc05156fdca68401173edf},
	abstract = {Time series constitute a challenging data type for machine learning algorithms, due to their highly variable lengths and sparse labeling in practice. In this paper, we tackle this challenge by proposing an unsupervised method to learn universal embeddings of time series. Unlike previous works, it is scalable with respect to their length and we demonstrate the quality, transferability and practicability of the learned representations with thorough experiments and comparisons. To this end, we combine an encoder based on causal dilated convolutions with a novel triplet loss employing time-based negative sampling, obtaining general-purpose representations for variable length and multivariate time series.},
	urldate = {2024-09-11},
	journal = {ArXiv},
	author = {Franceschi, Jean-Yves and Dieuleveut, Aymeric and Jaggi, Martin},
	month = jan,
	year = {2019},
}

@inproceedings{fan_semi-supervised_2021,
	title = {Semi-{Supervised} {Time} {Series} {Classification} by {Temporal} {Relation} {Prediction}},
	url = {https://ieeexplore.ieee.org/document/9413883},
	doi = {10.1109/ICASSP39728.2021.9413883},
	abstract = {Semi-supervised learning (SSL) has proven to be a powerful algorithm in different domains by leveraging unlabeled data to mitigate the reliance on the tremendous annotated data. However, few efforts consider the underlying temporal relation structure of unlabeled time series data in the semi-supervised learning paradigm. In this work, we propose a simple and effective method of Semi-supervised Time series classification architecture (termed as SemiTime) by gaining from the structure of unlabeled data in a self-supervised manner. Specifically, for the labeled time series, SemiTime conducts the supervised classification directly under the supervision of the annotated class label. For the unlabeled time series, the segments of past-future pair are sampled from time series, where two segments of pair from the same time series candidate are in positive temporal relation, while two segments from the different candidates are in negative temporal relation. Then, the temporal relation between those segments is predicted by SemiTime in a self-supervised manner. Finally, by jointly classifying labeled data and predicting the temporal relation of unlabeled data, the useful representation of unlabeled time series can be captured by SemiTime. Extensive experiments on multiple real-world datasets show that SemiTime consistently out-performs the state-of-the-arts, which demonstrates the effectiveness of the proposed method. Code and data are publicly available at https://haoyfan.github.io.},
	urldate = {2024-09-11},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Fan, Haoyi and Zhang, Fengbin and Wang, Ruidong and Huang, Xunhua and Li, Zuoyong},
	month = jun,
	year = {2021},
	note = {ISSN: 2379-190X},
	keywords = {Classification algorithms, Conferences, Feature extraction, Semantics, Semisupervised learning, Signal processing, Time series analysis, Time series classification, self-supervised, semi-supervised learning, temporal relation},
	pages = {3545--3549},
}

@article{kim_semi-supervised_2024,
	title = {Semi-supervised contrastive learning with decomposition-based data augmentation for time series classification},
	volume = {Preprint},
	issn = {1088-467X},
	url = {https://content.iospress.com/articles/intelligent-data-analysis/ida240002},
	doi = {10.3233/IDA-240002},
	abstract = {While time series data are prevalent across diverse sectors, data labeling process still remains resource-intensive. This results in a scarcity of labeled data for deep learning, emphasizing the importance of semi-supervised learning techniques. Appl},
	language = {en},
	number = {Preprint},
	urldate = {2024-09-11},
	journal = {Intelligent Data Analysis},
	author = {Kim, Dokyun and Cho, Sukhyun and Chae, Heewoong and Park, Jonghun and Huh, Jaeseok},
	month = jan,
	year = {2024},
	note = {Publisher: IOS Press},
	pages = {1--25},
}

@article{capstick2024representation,
  title={Representation Learning of Daily Movement Data Using Text Encoders},
  author={Capstick, Alexander and Cui, Tianyu and Chen, Yu and Barnaghi, Payam},
  journal={ICLR 2024 Workshop Time Series for Health},
  url={https://arxiv.org/abs/2405.04494},
  year={2024}
}
