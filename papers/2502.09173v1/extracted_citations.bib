@article{banky_equal_2013,
	title = {Equal {Opportunity} for {Low}-{Degree} {Network} {Nodes}: {A} {PageRank}-{Based} {Method} for {Protein} {Target} {Identification} in {Metabolic} {Graphs}},
	volume = {8},
	issn = {1932-6203},
	shorttitle = {Equal {Opportunity} for {Low}-{Degree} {Network} {Nodes}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0054204},
	doi = {10.1371/journal.pone.0054204},
	abstract = {Biological network data, such as metabolic-, signaling- or physical interaction graphs of proteins are increasingly available in public repositories for important species. Tools for the quantitative analysis of these networks are being developed today. Protein network-based drug target identification methods usually return protein hubs with large degrees in the networks as potentially important targets. Some known, important protein targets, however, are not hubs at all, and perturbing protein hubs in these networks may have several unwanted physiological effects, due to their interaction with numerous partners. Here, we show a novel method applicable in networks with directed edges (such as metabolic networks) that compensates for the low degree (non-hub) vertices in the network, and identifies important nodes, regardless of their hub properties. Our method computes the PageRank for the nodes of the network, and divides the PageRank by the in-degree (i.e., the number of incoming edges) of the node. This quotient is the same in all nodes in an undirected graph (even for large- and low-degree nodes, that is, for hubs and non-hubs as well), but may differ significantly from node to node in directed graphs. We suggest to assign importance to non-hub nodes with large PageRank/in-degree quotient. Consequently, our method gives high scores to nodes with large PageRank, relative to their degrees: therefore non-hub important nodes can easily be identified in large networks. We demonstrate that these relatively high PageRank scores have biological relevance: the method correctly finds numerous already validated drug targets in distinct organisms (Mycobacterium tuberculosis, Plasmodium falciparum and MRSA Staphylococcus aureus), and consequently, it may suggest new possible protein targets as well. Additionally, our scoring method was not chosen arbitrarily: its value for all nodes of all undirected graphs is constant; therefore its high value captures importance in the directed edge structure of the graph.},
	language = {en},
	number = {1},
	urldate = {2024-09-11},
	journal = {PLOS ONE},
	author = {Bánky, Dániel and Iván, Gábor and Grolmusz, Vince},
	month = jan,
	year = {2013},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Drug discovery, Drug metabolism, Graphs, Metabolic networks, Mycobacterium tuberculosis, Protein interaction networks, Protein metabolism},
	pages = {e54204},
}

@misc{bian_multi-patch_2024,
	title = {Multi-{Patch} {Prediction}: {Adapting} {LLMs} for {Time} {Series} {Representation} {Learning}},
	shorttitle = {Multi-{Patch} {Prediction}},
	url = {https://arxiv.org/abs/2402.04852v2},
	abstract = {In this study, we present aLLM4TS, an innovative framework that adapts Large Language Models (LLMs) for time-series representation learning. Central to our approach is that we reconceive time-series forecasting as a self-supervised, multi-patch prediction task, which, compared to traditional contrastive learning or mask-and-reconstruction methods, captures temporal dynamics in patch representations more effectively. Our strategy encompasses two-stage training: (i). a causal continual pre-training phase on various time-series datasets, anchored on next patch prediction, effectively syncing LLM capabilities with the intricacies of time-series data; (ii). fine-tuning for multi-patch prediction in the targeted time-series context. A distinctive element of our framework is the patch-wise decoding layer, which departs from previous methods reliant on sequence-level decoding. Such a design directly transposes individual patches into temporal sequences, thereby significantly bolstering the model's proficiency in mastering temporal patch-based representations. aLLM4TS demonstrates superior performance in several downstream tasks, proving its effectiveness in deriving temporal representations with enhanced transferability and marking a pivotal advancement in the adaptation of LLMs for time-series analysis.},
	language = {en},
	urldate = {2024-09-11},
	journal = {arXiv.org},
	author = {Bian, Yuxuan and Ju, Xuan and Li, Jiangtong and Xu, Zhijian and Cheng, Dawei and Xu, Qiang},
	month = feb,
	year = {2024},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {https://arxiv.org/abs/2005.14165v4},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	language = {en},
	urldate = {2024-09-11},
	journal = {arXiv.org},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = may,
	year = {2020},
}

@article{capstick2024representation,
  title={Representation Learning of Daily Movement Data Using Text Encoders},
  author={Capstick, Alexander and Cui, Tianyu and Chen, Yu and Barnaghi, Payam},
  journal={ICLR 2024 Workshop Time Series for Health},
  url={https://arxiv.org/abs/2405.04494},
  year={2024}
}

@article{ivan_when_2011,
	title = {When the {Web} meets the cell: using personalized {PageRank} for analyzing protein interaction networks},
	volume = {27},
	issn = {1367-4811, 1367-4803},
	shorttitle = {When the {Web} meets the cell},
	url = {https://academic.oup.com/bioinformatics/article/27/3/405/321946},
	doi = {10.1093/bioinformatics/btq680},
	abstract = {Motivation: Enormous and constantly increasing quantity of biological information is represented in metabolic and in protein interaction network databases. Most of these data are freely accessible through large public depositories. The robust analysis of these resources needs novel technologies, being developed today.},
	language = {en},
	number = {3},
	urldate = {2024-09-11},
	journal = {Bioinformatics},
	author = {Iván, Gábor and Grolmusz, Vince},
	month = feb,
	year = {2011},
	pages = {405--407},
}

@article{kontopoulou_review_2023,
	title = {A {Review} of {ARIMA} vs. {Machine} {Learning} {Approaches} for {Time} {Series} {Forecasting} in {Data} {Driven} {Networks}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1999-5903},
	url = {https://www.mdpi.com/1999-5903/15/8/255},
	doi = {10.3390/fi15080255},
	abstract = {In the broad scientific field of time series forecasting, the ARIMA models and their variants have been widely applied for half a century now due to their mathematical simplicity and flexibility in application. However, with the recent advances in the development and efficient deployment of artificial intelligence models and techniques, the view is rapidly changing, with a shift towards machine and deep learning approaches becoming apparent, even without a complete evaluation of the superiority of the new approach over the classic statistical algorithms. Our work constitutes an extensive review of the published scientific literature regarding the comparison of ARIMA and machine learning algorithms applied to time series forecasting problems, as well as the combination of these two approaches in hybrid statistical-AI models in a wide variety of data applications (finance, health, weather, utilities, and network traffic prediction). Our review has shown that the AI algorithms display better prediction performance in most applications, with a few notable exceptions analyzed in our Discussion and Conclusions sections, while the hybrid statistical-AI models steadily outperform their individual parts, utilizing the best algorithmic features of both worlds.},
	language = {en},
	number = {8},
	urldate = {2024-09-11},
	journal = {Future Internet},
	author = {Kontopoulou, Vaia I. and Panagopoulos, Athanasios D. and Kakkos, Ioannis and Matsopoulos, George K.},
	month = aug,
	year = {2023},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {ARIMA, MAE, MAPE, MSE, RMSE, deep learning, finance, health, hybrid, machine learning, networks, weather},
	pages = {255},
}

@misc{liu_autotimes_2024,
	title = {{AutoTimes}: {Autoregressive} {Time} {Series} {Forecasters} via {Large} {Language} {Models}},
	shorttitle = {{AutoTimes}},
	url = {https://arxiv.org/abs/2402.02370v2},
	abstract = {Foundation models of time series have not been fully developed due to the limited availability of time series corpora and the underexploration of scalable pre-training. Based on the similar sequential formulation of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, the inherent autoregressive property and decoder-only architecture of LLMs have not been fully considered, resulting in insufficient utilization of LLM abilities. To further exploit the general-purpose token transition and multi-step generation ability of large language models, we propose AutoTimes to repurpose LLMs as autoregressive time series forecasters, which independently projects time series segments into the embedding space and autoregressively generates future predictions with arbitrary lengths. Compatible with any decoder-only LLMs, the consequent forecaster exhibits the flexibility of the lookback length and scalability of the LLM size. Further, we formulate time series as prompts, extending the context for prediction beyond the lookback window, termed in-context forecasting. By adopting textual timestamps as position embeddings, AutoTimes integrates multimodality for multivariate scenarios. Empirically, AutoTimes achieves state-of-the-art with 0.1\% trainable parameters and over 5 times training/inference speedup compared to advanced LLM-based forecasters.},
	language = {en},
	urldate = {2024-09-11},
	journal = {arXiv.org},
	author = {Liu, Yong and Qin, Guo and Huang, Xiangdong and Wang, Jianmin and Long, Mingsheng},
	month = feb,
	year = {2024},
}

@article{masini_machine_2023,
	title = {Machine learning advances for time series forecasting},
	volume = {37},
	issn = {1467-6419},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/joes.12429},
	doi = {10.1111/joes.12429},
	abstract = {In this paper, we survey the most recent advances in supervised machine learning (ML) and high-dimensional models for time-series forecasting. We consider both linear and nonlinear alternatives. Among the linear methods, we pay special attention to penalized regressions and ensemble of models. The nonlinear methods considered in the paper include shallow and deep neural networks, in their feedforward and recurrent versions, and tree-based methods, such as random forests and boosted trees. We also consider ensemble and hybrid models by combining ingredients from different alternatives. Tests for superior predictive ability are briefly reviewed. Finally, we discuss application of ML in economics and finance and provide an illustration with high-frequency financial data.},
	language = {en},
	number = {1},
	urldate = {2024-09-11},
	journal = {Journal of Economic Surveys},
	author = {Masini, Ricardo P. and Medeiros, Marcelo C. and Mendes, Eduardo F.},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/joes.12429},
	keywords = {bagging, boosting, deep learning, forecasting, machine learning, neural networks, nonlinear models, penalized regressions, random forests, regression trees, regularization, sieve approximation, statistical learning theory},
	pages = {76--111},
}

@inproceedings{page_pagerank_1999,
	title = {The {PageRank} {Citation} {Ranking} : {Bringing} {Order} to the {Web}},
	shorttitle = {The {PageRank} {Citation} {Ranking}},
	url = {https://www.semanticscholar.org/paper/The-PageRank-Citation-Ranking-%3A-Bringing-Order-to-Page-Brin/eb82d3035849cd23578096462ba419b53198a556},
	abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.},
	urldate = {2024-09-11},
	author = {Page, Lawrence and Brin, Sergey and Motwani, R. and Winograd, T.},
	month = nov,
	year = {1999},
}

@article{rhanoui_forecasting_2019,
	title = {Forecasting financial budget time series: {ARIMA} random walk vs {LSTM} neural network},
	volume = {8},
	copyright = {Copyright (c) 2019 Institute of Advanced Engineering and Science},
	issn = {2252-8938},
	shorttitle = {Forecasting financial budget time series},
	url = {https://ijai.iaescore.com/index.php/IJAI/article/view/20275},
	doi = {10.11591/ijai.v8.i4.pp317-327},
	abstract = {Financial time series are volatile, non-stationary and non-linear data that are affected by external economic factors. There is several performant predictive approaches such as univariate ARIMA model and more recently Recurrent Neural Network. The accurate forecasting of budget data is a strategic and challenging task for an optimal management of resources, it requires the use of the most accurate model. We propose a predictive approach that uses and compares the Machine Learning ARIMA model and Deep Learning Recurrent LSTM model. The application and the comparative analysis show that the LSTM model outperforms the ARIMA model, mainly thanks to the LSTMs ability to learn non-linear relationship from data.},
	language = {en-US},
	number = {4},
	urldate = {2024-09-11},
	journal = {IAES International Journal of Artificial Intelligence (IJ-AI)},
	author = {Rhanoui, Maryem and Yousfi, Siham and Mikram, Mounia and Merizak, Hajar},
	month = dec,
	year = {2019},
	note = {Number: 4},
	keywords = {ARIMA, Deep learning, Financial time series, LSTM, Machine learning, RNN, Random walk},
	pages = {317--327},
}

@article{rizvi_arima_2024,
	title = {{ARIMA} {Model} {Time} {Series} {Forecasting}},
	volume = {12},
	issn = {23219653},
	url = {https://www.ijraset.com/best-journal/arima-model-time-series-forecasting},
	doi = {10.22214/ijraset.2024.62416},
	abstract = {Time series forecasting is a critical component in various fields such as finance, economics, meteorology, and engineering. Among the multitude of methods available for time series forecasting, the Autoregressive Integrated Moving Average (ARIMA) model stands out for its simplicity and effectiveness. This paper provides a comprehensive review of ARIMA models, focusing on their application in forecasting time series data. We begin with an overview of time series analysis and the theoretical foundations of ARIMA models. Subsequently, we delve into the process of building and fitting ARIMA models, discussing the steps involved and the considerations for model selection. Furthermore, we explore advanced topics such as seasonal ARIMA (SARIMA) models and discuss their relevance in handling seasonal data patterns. Additionally, we review recent advancements and extensions of ARIMA models, including hybrid models and machine learning-based approaches. Finally, we discuss the challenges and limitations associated with ARIMA modeling and provide recommendations for future research directions.},
	language = {en},
	number = {5},
	urldate = {2024-09-11},
	journal = {International Journal for Research in Applied Science and Engineering Technology},
	author = {Rizvi, Mohd Faizan},
	month = may,
	year = {2024},
	pages = {3782--3785},
}

