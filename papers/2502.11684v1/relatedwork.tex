\section{Related Works}
\begin{figure*}[htbp]
\centering
\includegraphics[width=1\textwidth]{figs/main.pdf}
\caption{An overview of our work. The upper part illustrates how we construct FIM training data from existing CoT data and train FIM models, \methodname, which works on chain-of-thought. The lower part demonstrates the process where \methodname is used to expand the steps of existing CoT data for more detailed reasoning.}
\label{fig:main}
\end{figure*}


\subsection{Mathematical Reasoning of LLMs}

Mathematical reasoning is one of the advanced capabilities of large language models (LLMs). By transforming real-world mathematical problems into a sequence of sub-problems and engaging in step-by-step thinking, the model’s ability to solve related mathematical tasks is enhanced~\citep{wei2023chainofthought}. Currently, the mathematical reasoning ability of models can be strengthened at various stages of LLM’s training. During the pre-training phase, reasoning-related knowledge texts, such as mathematical forum discussions, textbooks, and so on, are typically used for enhancement~\citep{paster2023openwebmatha, zhang2024autonomous}. Additionally, a large number of synthetic step-by-step reasoning question-answer pairs are used to train the model, allowing it to learn various reasoning patterns. In the instruction fine-tuning (SFT) phase, high-quality question-answer pairs are usually employed to help the model master the pattern of step-by-step thinking, thereby enabling it to solve reasoning problems~\citep{ding2024unleashing, zhou2024jiuzhang30, E-GSM2024Xu}. After SFT, researchers also use techniques such as outcome supervision and process supervision to reinforce the model's mathematical reasoning process, ensuring that the model generates more accurate reasoning steps during inference~\citep{lightman2023lets,pds2024xu, wang2024mathshepherd, zhang2025lessons}.



\subsection{Expansion of Reasoning Steps}
Just as the scaling law in model training applies, there is also a scaling law for LLMs during test-time. The former improves the model’s reasoning ability by providing more training data~\citep{hoffmann2022training}, while the latter increases the model’s computational load during inference to enhance calculation accuracy, thereby improving performance~\citep{brown2024large, snell2024scaling}. Expanding reasoning steps is one way to enhance the test-time computation of LLMs. By generating more detailed reasoning steps during inference, the model’s reasoning performance can be improved.

There are several ways to expand reasoning steps. For example, in a training-free approach, prompts like Chain-of-Thought~\citep{wei2023chainofthought} can guide the model to perform more detailed reasoning. Using self-consistency~\citep{wang2023selfconsistency} to perform multiple reasoning paths and vote on the most consistent answers is another option. Additionally, methods like tree-search combined with a verifier can be used to select the optimal reasoning path~\citep{chen2024alphamath, feng2024alphazerolike, guan2025rstarmath}. On the other hand, training-based approaches involve transforming training data into more detailed steps~\citep{jin2024impact, ying2024internlmmatha} or incorporating behaviors like planning~\citep{wang2023planandsolve} and self-correction~\citep{yan2024s$^3$cmath}, which can increase the model’s computation during test-time, thus improving reasoning performance.