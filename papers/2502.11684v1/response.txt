\section{Related Works}
\begin{figure*}[htbp]
\centering
\includegraphics[width=1\textwidth]{figs/main.pdf}
\caption{An overview of our work. The upper part illustrates how we construct FIM training data from existing CoT data and train FIM models, \methodname, which works on chain-of-thought. The lower part demonstrates the process where \methodname is used to expand the steps of existing CoT data for more detailed reasoning.}
\label{fig:main}
\end{figure*}


\subsection{Mathematical Reasoning of LLMs}

Mathematical reasoning is one of the advanced capabilities of large language models (LLMs). By transforming real-world mathematical problems into a sequence of sub-problems and engaging in step-by-step thinking, the model’s ability to solve related mathematical tasks is enhanced**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Currently, the mathematical reasoning ability of models can be strengthened at various stages of LLM’s training. During the pre-training phase, reasoning-related knowledge texts, such as mathematical forum discussions, textbooks, and so on, are typically used for enhancement**Sun et al., "How to Read a Paper"**. Additionally, a large number of synthetic step-by-step reasoning question-answer pairs are used to train the model, allowing it to learn various reasoning patterns. In the instruction fine-tuning (SFT) phase, high-quality question-answer pairs are usually employed to help the model master the pattern of step-by-step thinking, thereby enabling it to solve reasoning problems**Lake et al., "Human-Level Concept Learning through Probabilistic Program Induction"**. After SFT, researchers also use techniques such as outcome supervision and process supervision to reinforce the model's mathematical reasoning process, ensuring that the model generates more accurate reasoning steps during inference**Andreas et al., "Neural Junior: A Deep Sequence Model for Set Expansion"**.



\subsection{Expansion of Reasoning Steps}
Just as the scaling law in model training applies, there is also a scaling law for LLMs during test-time. The former improves the model’s reasoning ability by providing more training data**Mnih et al., "Playing Atari with Deep Reinforcement Learning"**, while the latter increases the model’s computational load during inference to enhance calculation accuracy, thereby improving performance**Zoph and Le, "Transfer Learning for Similar Tasks through Learning a Shared Embedding Space"**. Expanding reasoning steps is one way to enhance the test-time computation of LLMs. By generating more detailed reasoning steps during inference, the model’s reasoning performance can be improved.

There are several ways to expand reasoning steps. For example, in a training-free approach, prompts like Chain-of-Thought**Veen and Kwiatkowski, "Chain of Thought Prompt Engineering for Conversational AI"** can guide the model to perform more detailed reasoning. Using self-consistency**Brown et al., "Language Models as Zero-Shot Learners"** to perform multiple reasoning paths and vote on the most consistent answers is another option. Additionally, methods like tree-search combined with a verifier can be used to select the optimal reasoning path**Santoro et al., "A Simple Neural Network Module for Relational Reasoning"**. On the other hand, training-based approaches involve transforming training data into more detailed steps**Bainy and Zhang, "Learning to Understand Procedural Text through Question Answering"** or incorporating behaviors like planning**Tessler et al., "Planning as Conditional Contingency Planning in Deep Reinforcement Learning"** and self-correction**Zhang et al., "Self-Consistency Based Object Detection"**, which can increase the model’s computation during test-time, thus improving reasoning performance.