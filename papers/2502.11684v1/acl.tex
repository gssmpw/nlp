% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.


\usepackage{xspace}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}

\newcommand{\methodname}{MathFimer\xspace}
\newcommand{\modelsevenb}{MathFimer-7B\xspace}
\newcommand{\modelseventytwob}{MathFimer-72B\xspace}
\newcommand{\datasetname}{NuminaMath-FIM\xspace}

\title{\raisebox{-0.3cm}{\includegraphics[width=0.9cm,height=0.9cm,keepaspectratio]{figs/MathFimerLogo.png}}
\methodname: Enhancing Mathematical Reasoning by Expanding Reasoning Steps through Fill-in-the-Middle Task}

% \author{Zhejiang University \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
 \textbf{Yuchen Yan\textsuperscript{1,2}$\thanks{Contribution during internship at Meituan Group.}$},
 \textbf{Yongliang Shen\textsuperscript{1}$\thanks{Corresponding authors.}$},
 \textbf{Yang Liu\textsuperscript{2}},
 \textbf{Jin Jiang\textsuperscript{2,3}},
 \textbf{Xin Xu\textsuperscript{4}},\\
 \textbf{Mengdi Zhang\textsuperscript{2}},
 \textbf{Jian Shao\textsuperscript{1†}},
 \textbf{Yueting Zhuang\textsuperscript{1}}
\\
\\
 \textsuperscript{1}Zhejiang University 
 \textsuperscript{2}Meituan Group 
 \textsuperscript{3}Peking University \\
 \textsuperscript{4}Hong Kong University of Science and Technology
\\
 \texttt{\{yanyuchen,syl,jshao\}@zju.edu.cn}
}

\begin{document}
\maketitle
\begin{abstract}

Mathematical reasoning represents a critical frontier in advancing large language models (LLMs). While step-by-step approaches have emerged as the dominant paradigm for mathematical problem-solving in LLMs, the quality of reasoning steps in training data fundamentally constrains the performance of the models. 
Recent studies has demonstrated that more detailed intermediate steps can enhance model performance, yet existing methods for step expansion either require more powerful external models or incur substantial computational costs. 
In this paper, we introduce MathFimer, a novel framework for mathematical reasoning step expansion inspired by the "Fill-in-the-middle" task from code completion.
By decomposing solution chains into prefix-suffix pairs and training models to reconstruct missing intermediate steps, we develop a specialized model, MathFimer-7B, on our carefully curated NuminaMath-FIM dataset. 
We then apply these models to enhance existing mathematical reasoning datasets by inserting detailed intermediate steps into their solution chains, creating MathFimer-expanded versions.
Through comprehensive experiments on multiple mathematical reasoning datasets, including MathInstruct, MetaMathQA and etc., we demonstrate that models trained on MathFimer-expanded data consistently outperform their counterparts trained on original data across various benchmarks such as GSM8K and MATH.
Our approach offers a practical, scalable solution for enhancing mathematical reasoning capabilities in LLMs without relying on powerful external models or expensive inference procedures.


\end{abstract}

\section{Introduction}

\begin{figure}[t]
\centering
\includegraphics[width=0.49\textwidth]{figs/fim.pdf}
\caption{We developed \methodname inspired by the fill-in-the-middle task in code reasoning of LLMs. Panel \textbf{(a)} demonstrates an example where the FIM model completes a given code context, while Panel \textbf{(b)} shows how \methodname, as proposed in this paper, extends the steps of an existing step-by-step answer.}
\label{fig:fim}
\end{figure}

Recent advances in large language models (LLMs)~\citep{GPT42023openai, deepseekr12025deepseekai} have demonstrated remarkable capabilities across various reasoning tasks \citep{omnimath2024gao,ugphysics2025xu}, from logical deduction to complex problem-solving~\citep{phan2025humanity}. Among these, mathematical reasoning stands as a particularly challenging frontier~\citep{sun2024survey, ugmathbench2025xu}, serving as a critical benchmark for evaluating an LLM's ability to perform structured, multi-step reasoning processes.

A key breakthrough in improving LLMs' mathematical reasoning capabilities has been the introduction of chain-of-thought (CoT) prompting~\citep{wei2023chainofthought}, where models explicitly articulate intermediate steps in their problem-solving process. This approach has not only enhanced solution accuracy but has also provided valuable insights into the models' reasoning mechanisms. However, the effectiveness of CoT prompting raises a fundamental question: \textbf{\textit{What characteristics of training data are crucial for developing LLMs that can generate high-quality reasoning chains and arrive at correct mathematical solutions?}}

Prior research has revealed that the granularity and completeness of reasoning steps in training data significantly impact a model's reasoning capabilities~\citep{jin2024impact}. Models trained on more detailed step-by-step solutions tend to exhibit superior performance in mathematical reasoning tasks. This observation has led to various approaches for expanding reasoning steps in training data, including the use of stronger external models and sophisticated search algorithms like Monte Carlo Tree Search (MCTS)~\citep{zhou2023mcts1, rebase2024wu, liu2024mcts2}. However, these current approaches to improving reasoning steps face three main challenges. First, they rely on using even larger models to create better steps, which creates a cycle where we constantly need bigger models to make improvements~\citep{guan2025rstarmath, openmathinstruct2024toshniwal}. Second, these methods require substantial computing resources, particularly when using advanced techniques like MCTS to explore different reasoning paths. Third, instead of building upon existing human-verified steps, these methods often generate entirely new reasoning chains, which can introduce unexpected errors and reduce the reliability of solutions.

These limitations motivate our central research question: \textbf{\textit{Can we develop a more efficient and reliable method for expanding reasoning steps while preserving the validity of existing human-generated solutions?}}
Drawing inspiration from the "Fill-in-the-middle" task in code reasoning, where LLMs successfully complete missing code segments based on surrounding context, we propose a novel approach to this problem. Rather than generating entirely new reasoning chains, we explore whether the FIM paradigm can be adapted to supplement missing steps in existing reasoning processes or insert more detailed explanations into steps that are already sufficient.

Building on this insight, we propose MathFimer, a framework for enhancing mathematical reasoning through step expansion. We first construct NuminaMath-FIM by decomposing NuminaMath-CoT ~\citep{li2024numinamath} solutions into prefix-suffix pairs with missing intermediate steps.
Using this dataset, we train a step-expansion model MathFimer-7B on math-specialized base model Qwen2.5-Math-7B~\citep{yang2024qwen25math}. This model learns to supplement intermediate reasoning steps while preserving the original solution structure.

We apply MathFimer-7B to expand the reasoning steps in several existing mathematical reasoning datasets and evaluate their impact through comprehensive experiments. Our results demonstrate that training on MathFimer-expanded data consistently improves model performance across various mathematical reasoning benchmarks, including GSM8K and MATH. This improvement is observed across both general-purpose and math-specialized foundation models, with expanded datasets leading to more detailed reasoning steps and higher solution accuracy compared to the original training data.

Our main contributions are threefold:
\begin{itemize}
\item We propose a novel step expansion framework inspired by code completion techniques, introducing \methodname to enhance mathematical reasoning through targeted insertion of intermediate steps in existing solutions.
\item We develop and release a specialized training dataset (NuminaMath-FIM) along with a step-expansion model MathFimer-7B, providing a practical and scalable solution for improving mathematical reasoning datasets.
\item Through extensive experiments across multiple benchmarks and model architectures, we demonstrate that our approach consistently improves mathematical reasoning performance, offering new insights into the relationship between step granularity and reasoning quality in LLMs.
\end{itemize}

\section{Related Works}

\begin{figure*}[htbp]
\centering
\includegraphics[width=1\textwidth]{figs/main.pdf}
\caption{An overview of our work. The upper part illustrates how we construct FIM training data from existing CoT data and train FIM models, \methodname, which works on chain-of-thought. The lower part demonstrates the process where \methodname is used to expand the steps of existing CoT data for more detailed reasoning.}
\label{fig:main}
\end{figure*}


\subsection{Mathematical Reasoning of LLMs}

Mathematical reasoning is one of the advanced capabilities of large language models (LLMs). By transforming real-world mathematical problems into a sequence of sub-problems and engaging in step-by-step thinking, the model’s ability to solve related mathematical tasks is enhanced~\citep{wei2023chainofthought}. Currently, the mathematical reasoning ability of models can be strengthened at various stages of LLM’s training. During the pre-training phase, reasoning-related knowledge texts, such as mathematical forum discussions, textbooks, and so on, are typically used for enhancement~\citep{paster2023openwebmatha, zhang2024autonomous}. Additionally, a large number of synthetic step-by-step reasoning question-answer pairs are used to train the model, allowing it to learn various reasoning patterns. In the instruction fine-tuning (SFT) phase, high-quality question-answer pairs are usually employed to help the model master the pattern of step-by-step thinking, thereby enabling it to solve reasoning problems~\citep{ding2024unleashing, zhou2024jiuzhang30, E-GSM2024Xu}. After SFT, researchers also use techniques such as outcome supervision and process supervision to reinforce the model's mathematical reasoning process, ensuring that the model generates more accurate reasoning steps during inference~\citep{lightman2023lets,pds2024xu, wang2024mathshepherd, zhang2025lessons}.



\subsection{Expansion of Reasoning Steps}
Just as the scaling law in model training applies, there is also a scaling law for LLMs during test-time. The former improves the model’s reasoning ability by providing more training data~\citep{hoffmann2022training}, while the latter increases the model’s computational load during inference to enhance calculation accuracy, thereby improving performance~\citep{brown2024large, snell2024scaling}. Expanding reasoning steps is one way to enhance the test-time computation of LLMs. By generating more detailed reasoning steps during inference, the model’s reasoning performance can be improved.

There are several ways to expand reasoning steps. For example, in a training-free approach, prompts like Chain-of-Thought~\citep{wei2023chainofthought} can guide the model to perform more detailed reasoning. Using self-consistency~\citep{wang2023selfconsistency} to perform multiple reasoning paths and vote on the most consistent answers is another option. Additionally, methods like tree-search combined with a verifier can be used to select the optimal reasoning path~\citep{chen2024alphamath, feng2024alphazerolike, guan2025rstarmath}. On the other hand, training-based approaches involve transforming training data into more detailed steps~\citep{jin2024impact, ying2024internlmmatha} or incorporating behaviors like planning~\citep{wang2023planandsolve} and self-correction~\citep{yan2024s$^3$cmath}, which can increase the model’s computation during test-time, thus improving reasoning performance.



\section{Approach}
\begin{figure*}[htbp]
\centering
\includegraphics[width=1\textwidth]{figs/fim_example.pdf}
\caption{An example of NuminaMath-FIM. The left side represents a mathematical problem and its corresponding solution from NuminaMath-CoT, while the right side shows the FIM data constructed from it. The underlined portion represents a randomly selected step from all the steps, with the blue tokens \texttt{<|fim\_prefix|>}, \texttt{<|fim\_suffix|>}, and \texttt{<|fim\_middle|>} being three special tokens. During supervised fine-tuning, we only compute the loss for the underlined portion.}
\label{fig:fim_example}
\end{figure*}


In this paper, we propose a reasoning step expansion method that enhances the quality of existing data by filling in possible missing steps at the step level. This is achieved through the fill-in-the-middle (FIM) task, which supplements existing CoT data. Specifically, the work presented in this paper can be divided into two parts: the first part involves training the aforementioned FIM models, and the second part applies the trained FIM models to extend steps in existing data. Figure \ref{fig:main} shows an overview of our work.


\subsection{FIM Model Training}

The goal of this section is to train a Fill-in-the-Middle (FIM) model for mathematical reasoning tasks, which can generate the missing intermediate steps between a mathematical problem, its preceding steps, and its succeeding steps. This can be expressed as:
\[
\textbf{FIM}(\text{Q}, \text{P}, \text{S}) \Rightarrow \text{M}
\]
where $\textbf{FIM}$ refers to the model we are training, $Q$ (question) represents the mathematical problem, $P$ (prefix) refers to the preceding steps, $S$ (suffix) refers to the succeeding steps, and $M$ (middle) denotes the intermediate steps between $P$ and $S$.

We construct the data for training the FIM model using the existing high-quality mathematical reasoning dataset, NuminaMath-CoT. NuminaMath-CoT includes mathematical reasoning data of varying difficulty levels, containing 853K mathematical question-and-answer pairs, providing us with more generalizable data.


Specifically, we first performed a step-by-step decomposition of the NuminaMath-CoT data, transforming the standard answers into individual steps. Then, for each case, we randomly select one step and treat all the preceding steps as the prefix and all the succeeding steps as the suffix. This can be represented as:
\[
(P, S, M) = (y_{1 \ldots i-1}, y_{i+1 \ldots n}, y_i), y_i \in Y
\]
where \(y_i\) is a step randomly selected from the answer \(Y\), which contains \(n\) steps.

For the organization format of the FIM training data, we refer to the work of \citet{bavarian2022efficient} and adopt the PSM(Prefix-Suffix-Middle) sequence order. We use three special tokens: \texttt{<|fim\_prefix|>}, \texttt{<|fim\_suffix|>}, and \texttt{<|fim\_middle|>}, to construct the format for the FIM training data. An example of the FIM data construction is provided in Figure \ref{fig:fim_example}.


For each case in NuminaMath-CoT, we performed three rounds of random sampling as described above. As a result, for each mathematical problem, we constructed three FIM data entries, which together formed our FIM training set, NuminaMath-FIM, consisting of 2.5M training samples for FIM task. Next, we conducted SFT on a math-specialized base model, Qwen2.5-Math-7B\citep{yang2024qwen25math}. Specifically, we only computed the loss for the tokens after \texttt{<|fim\_middle|>}, ultimately obtaining the FIM model MathFimer-7B for step expansion.

\subsection{Expansion of Reasoning Steps}

After training MathFimer-7B, we can use it to expand the reasoning steps in existing mathematical solutions. Specifically, for each pair of consecutive steps in the original solution, we perform an inference using the FIM model to generate potentially missing intermediate steps or provide more detailed reasoning between them. This can be formally expressed as follows:
\[
\hat{y_i} = \textbf{FIM}(Q, y_1 \ldots y_{i-1}, y_i \ldots y_n)
\]
where i represents each position in the original answer, $n$ is the total number of steps in the original answer, $y_i$ is the i-th step in the original answer, $\textbf{FIM}$ is the trained \methodname model, $Q$ is the question for the sample, and $\hat{y_i}$ is the missing part generated by the FIM model between the i-th step and the subsequent steps.

In our experiments, we observed that when the original steps are already sufficiently detailed, the model tends to generate content that is very similar to the subsequent step  $y_i$ . Therefore, after the FIM model generates the supplementary step  $\hat{y_i}$ , we added a similarity calculation step. Specifically, we compute the sequence similarity between  $\hat{y_i}$  and  $y_i$ . We set a threshold $\eta$ and mark those generated steps with a similarity greater than  $\eta$ as \texttt{invalid}. In this paper, we set  $\eta$ = 0.8 .

Next, we insert the steps generated by the FIM model into the original steps. Specifically, if the similarity score in the previous step is not labeled as invalid, we will insert it into the original sequence. More precisely, for any step $\hat{y_i}$ where the similarity score is less than $\eta$, we insert it between step $y_{i-1}$ and step $y_i$. This insertion operation is carried out between each pair of original steps, ultimately constructing a more detailed answer with additional steps.

To evaluate the effectiveness and generalization of MathFimer-7B in expanding reasoning steps, we used it to extend the reasoning steps on several existing step-by-step reasoning datasets, including a mixture of GSM8K\citep{cobbe2021training} and MATH\citep{hendrycks2021measuring}, MathInstruct-CoT\citep{yue2023mammoth}, MetaMathQA\citep{yu2024metamatha}, NuminaMath-CoT\citep{li2024numinamath}, and ScaleQuestMath\citep{ding2024unleashing}. For all datasets, we only used the training set. We conducted instruction fine-tuning experiments on multiple foundation LLMs. For general-purpose LLMs, we selected Meta-Llama-3.1-8B and Meta-Llama-3.1-70B\citep{grattafiori2024llama}, and for math-specialized LLMs, we chose Qwen2.5-Math-7B and Qwen2.5-Math-72B\citep{yang2024qwen25math}. After instruction fine-tuning, we evaluated performance on multiple mathematical reasoning benchmarks, including GSM8K\citep{cobbe2021training}, MATH\citep{hendrycks2021measuring}, Math Odyssey\citep{fang2024mathodyssey}, and OlympiadBench-EN\citep{he2024olympiadbench}.

\section{Experiments}
\subsection{Settings}
We conducted supervised instruction fine-tuning experiments on both general-purpose and math-specialized foundation LLMs. To demonstrate the generalizability of the proposed method, we carried out experiments with different model sizes. We selected the original data before applying MathFimer-7B as the baseline for each experimental group and compared the performance improvements achieved after applying our proposed method for step expansion. In all experiments, we maintained identical training settings, only varying the data used for training. Specifically, we used Megatron-LM as the framework for SFT, with a model max\_length set to 8192 and a global batch size of 128 (GSM8K+MATH datasets were set to 32 due to their smaller sample sizes, as a large batch size would result in an insufficient number of optimization steps). The learning rate for training was set to 1e-5. We packed all training samples for faster training. All SFT experiments were conducted on 64 Ascend H910B-64G.

For evaluation, we employ vLLM\citep{kwon2023efficienta} as the inference framework. To reduce evaluation variance, each question is sampled 16 times with a temperature setting of 0.7, and the average accuracy is calculated. To determine whether the model-generated answers are correct, we utilize LLM-as-a-judge, thereby mitigating evaluation errors caused by answer extraction and rule-based comparison. All model inferences in this study are conducted on NVIDIA A100-80G GPUs, with 1-card inference for 7B/8B models and 4-cards inference for 70B/72B models.

\subsection{Main Results}

\begin{table*}[htbp]
\centering
\setlength{\tabcolsep}{4.0mm}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lllllll}
\toprule
\multicolumn{1}{l}{\multirow{2}[4]{*}{\textbf{Dataset}}} & \multicolumn{1}{l}{\multirow{2}[4]{*}{\textbf{FIM Model}}} & \multicolumn{2}{c}{\textbf{Elementary Math}} & \multicolumn{2}{c}{\textbf{Competition Math}} & \multicolumn{1}{c}{\multirow{2}[4]{*}{\textbf{AVERAGE}}} \\
\cmidrule{3-6}      &       & \multicolumn{1}{c}{\textbf{GSM8K}} & \multicolumn{1}{c}{\textbf{MATH}} & \multicolumn{1}{c}{\textbf{Odyssey}} & \multicolumn{1}{c}{\textbf{OB-EN}} &  \\
\midrule
\multicolumn{7}{c}{\textbf{Base Model: Meta-Llama3.1-8B}} \\
\midrule
GSM8K+MATH & /     & 67.55 & 18.32 & 21.59 & 1.78  & 27.31 \\
 & MathFimer-7B & 73.16\scriptsize{+5.61} & 21.84\scriptsize{+3.52} & 21.34\scriptsize{-0.25} & 2.52\scriptsize{+0.74} & 29.72\scriptsize{+2.41} \\
\midrule
MathInstruct-CoT & /     & 67.78 & 18.74 & 22.11 & 2.37  & 27.75 \\
& MathFimer-7B & 75.21\scriptsize{+7.43} & 22.9\scriptsize{+4.16} & 24.42\scriptsize{+2.31} & 3.56\scriptsize{+1.19} & 31.52\scriptsize{+3.77} \\
\midrule
MetaMathQA & /     & 84.15 & 34.66 & 29.05 & 6.37  & 38.56 \\
 & MathFimer-7B & 84.69\scriptsize{+0.54} & 35.12\scriptsize{+0.46} & 28.79\scriptsize{-0.26} & 6.81\scriptsize{+0.44} & 38.85\scriptsize{+0.29} \\
\midrule
NuminaMath-CoT & /     & 89.08 & 48.10 & 36.76 & 13.04 & 46.75 \\
 & MathFimer-7B & 91.21\scriptsize{+2.13} & 50.5\scriptsize{+2.4} & 38.3\scriptsize{+1.54} & 14.52\scriptsize{+1.48} & 48.63\scriptsize{+1.89} \\
\midrule
ScaleQuest-Math & /     & 91.21 & 59.52 & 38.82 & 20.74 & 52.57 \\
 & MathFimer-7B & 91.05\scriptsize{-0.16} & 59.56\scriptsize{+0.04} & 40.36\scriptsize{+1.54} & 21.63\scriptsize{+0.89} & 53.15\scriptsize{+0.58} \\
\midrule
\multicolumn{7}{c}{\textbf{Base Model: Meta-Llama3.1-70B}} \\
\midrule
GSM8K+MATH & /     & 89.23 & 40.22 & 38.30 & 8.74  & 44.12 \\
 & MathFimer-7B & 92.72\scriptsize{+3.49} & 44.36\scriptsize{+4.14} & 37.79\scriptsize{-0.51} & 12.15\scriptsize{+3.41} & 46.76\scriptsize{+2.63} \\
\midrule
MathInstruct-CoT & /     & 89.31 & 41.96 & 36.50 & 9.19  & 44.24 \\
 & MathFimer-7B & 90.98\scriptsize{+1.67} & 44.72\scriptsize{+2.76} & 39.33\scriptsize{+2.83} & 12.15\scriptsize{+2.96} & 46.8\scriptsize{+2.56} \\
\midrule
MetaMathQA & /     & 90.52 & 49.06 & 40.36 & 13.48 & 48.36 \\
 & MathFimer-7B & 92.57\scriptsize{+2.05} & 51.34\scriptsize{+2.28} & 38.3\scriptsize{-2.06} & 14.81\scriptsize{+1.33} & 49.26\scriptsize{+0.9} \\
\midrule
NuminaMath-CoT & /     & 96.44 & 66.36 & 47.30 & 31.70 & 60.45 \\
 & MathFimer-7B & 96.36\scriptsize{-0.08} & 67.82\scriptsize{+1.46} & 46.79\scriptsize{-0.51} & 33.33\scriptsize{+1.63} & 61.08\scriptsize{+0.63} \\
\midrule
ScaleQuest-Math & /     & 94.24 & 74.02 & 52.44 & 35.70 & 64.10 \\
 & MathFimer-7B & 95\scriptsize{+0.76} & 74.42\scriptsize{+0.4} & 49.36\scriptsize{-3.08} & 36.89\scriptsize{+1.19} & 63.92\scriptsize{-0.18} \\
\midrule
\multicolumn{7}{c}{\textbf{Base Model: Qwen2.5-Math-7B}} \\
\midrule
GSM8K+MATH & /     & 82.71 & 50.90 & 36.25 & 15.41 & 46.32 \\
 & MathFimer-7B & 85.37\scriptsize{+2.66} & 51.92\scriptsize{+1.02} & 34.7\scriptsize{-1.55} & 14.37\scriptsize{-1.04} & 46.59\scriptsize{+0.27} \\
\midrule
MathInstruct-CoT & /     & 86.28 & 59.80 & 44.22 & 20.59 & 52.72 \\
 & MathFimer-7B & 90.3\scriptsize{+4.02} & 58.86\scriptsize{-0.94} & 43.44\scriptsize{-0.78} & 20\scriptsize{-0.59} & 53.15\scriptsize{+0.43} \\
\midrule
MetaMathQA & /     & 93.18 & 70.22 & 49.10 & 34.81 & 61.83 \\
 & MathFimer-7B & 93.1\scriptsize{-0.08} & 79.08\scriptsize{+8.86} & 52.7\scriptsize{+3.6} & 41.04\scriptsize{+6.23} & 66.48\scriptsize{+4.65} \\
\midrule
NuminaMath-CoT & /     & 85.37 & 55.16 & 43.19 & 17.33 & 50.26 \\
 & MathFimer-7B & 87.72\scriptsize{+2.35} & 53\scriptsize{-2.16} & 42.16\scriptsize{-1.03} & 16.74\scriptsize{-0.59} & 49.91\scriptsize{-0.36} \\
\midrule
ScaleQuest-Math & /     & 93.78 & 70.52 & 50.13 & 34.81 & 62.31 \\
 & MathFimer-7B & 93.86\scriptsize{+0.08} & 79.38\scriptsize{+8.86} & 54.24\scriptsize{+4.11} & 40.44\scriptsize{+5.63} & 66.98\scriptsize{+4.67} \\
\midrule
\multicolumn{7}{c}{\textbf{Base Model: Qwen2.5-Math-72B}} \\
\midrule
GSM8K+MATH & /     & 93.25 & 70.74 & 50.13 & 30.37 & 61.12 \\
 & MathFimer-7B & 94.24\scriptsize{+0.99} & 75.16\scriptsize{+4.42} & 52.7\scriptsize{+2.57} & 36.3\scriptsize{+5.93} & 64.6\scriptsize{+3.48} \\
\midrule
MathInstruct-CoT & /     & 91.36 & 69.26 & 46.27 & 26.67 & 58.39 \\
 & MathFimer-7B & 92.49\scriptsize{+1.13} & 71.7\scriptsize{+2.44} & 46.02\scriptsize{-0.25} & 29.63\scriptsize{+2.96} & 59.96\scriptsize{+1.57} \\
\midrule
MetaMathQA & /     & 90.22 & 57.68 & 42.93 & 20.00 & 52.71 \\
 & MathFimer-7B & 92.95\scriptsize{+2.73} & 63.4\scriptsize{+5.72} & 47.3\scriptsize{+4.37} & 24.89\scriptsize{+4.89} & 57.14\scriptsize{+4.43} \\
\midrule
NuminaMath-CoT & /     & 96.29 & 77.54 & 55.27 & 43.26 & 68.09 \\
 & MathFimer-7B & 96.13\scriptsize{-0.16} & 77.4\scriptsize{-0.14} & 55.01\scriptsize{-0.26} & 44.15\scriptsize{+0.89} & 68.17\scriptsize{+0.08} \\
\midrule
ScaleQuest-Math & /     & 94.09 & 80.22 & 54.24 & 44.30 & 68.21 \\
 & MathFimer-7B & 94.47\scriptsize{+0.38} & 80.82\scriptsize{+0.6} & 55.27\scriptsize{+1.03} & 43.7\scriptsize{-0.6} & 68.57\scriptsize{+0.35} \\
\bottomrule
\end{tabular}%

}
\caption{Our main experimental results (\%) on four mathematical reasoning tasks (GSM8K, MATH, Math Odyssey and OlympiadBench-EN). The evaluation results are obtained by sampling the model 16 times with a temperature of 0.7 and calculating the average accuracy.
}
\label{tab:main_results}
\end{table*}


We conducted our experiments on base models of different sizes, including both general-purpose and math-specialized models. Specifically, we evaluated Meta-Llama-3.1-8B, Meta-Llama-3.1-70B, Qwen2.5-Math-7B, and Qwen2.5-Math-72B. We employed the MathFimer-7B model, which was trained based on Qwen2.5-Math-7B, to perform a single round of step expansion.
For comparative analysis, we selected five datasets: GSM8K+MATH, MathInstruct-CoT, MetaMathQA, NuminaMath-CoT, and ScaleQuest-Math, to examine whether step expansion via MathFimer-7B leads to improved performance on relevant mathematical reasoning benchmarks.
For evaluation, we used the GSM8K, MATH, Math Odyssey, and OlympiadBench-EN datasets. Among them, GSM8K and MATH primarily assess elementary-level mathematical problems, while Math Odyssey and OlympiadBench-EN consist of competition-level mathematics questions.

We present all our main results in Table \ref{tab:main_results}. As shown in the results, our method achieves consistent improvements across different base models and datasets. Specifically, for Meta-Llama3.1-8B, MathInstruct-CoT, when expanded using MathFimer, increases the average accuracy from 27.75\% to 32.52\%, yielding a 3.77 percentage point improvement. Similarly, for Qwen2.5-Math-72B, MetaMathQA, after step expansion via MathFimer, raises the average accuracy from 52.71\% to 57.14\%, achieving a gain of 4.43 percentage points.

Due to computational resource constraints, we perform only a single round of step expansion in our main experiment to observe the general applicability of our proposed MathFimer. However, MathFimer is capable of iterative step expansion, meaning that previously expanded steps can be further refined. We will explore the scalability of step expansion in more detail in the Analysis section \ref{sec:scaling}.


\section{Analysis}
\subsection{Disentangling Model Effects}
To disentangle the impact of our FIM methodology from model distillation effects, we conducted a systematic ablation study addressing a critical question: \textit{To what extent do our performance gains stem from the FIM-based step expansion versus knowledge transfer from the base model?}

We designed a controlled experiment using Qwen2.5-Math-7B as the base model. We first generated distillation datasets by fine-tuning the base model on NuminaMath-CoT and using it to generate solutions for GSM8k+MATH, MathInstruct, and MetaMathQA. We then applied MathFimer-7B's step expansion to these distilled datasets to isolate the contribution of our FIM approach.

The results in Table \ref{tab:ablation_distill} reveal several key insights. First, while distillation alone yields substantial improvements (e.g., GSM8K accuracy increases from 67.55\% to 81.58\% for G+M), MathFimer's step expansion provides additional gains even on distilled data (+1.13\%). This pattern is consistent across datasets, with MI-CoT showing similar additive benefits (+1.67\% on GSM8K). The smaller magnitude of improvements on distilled data compared to original data (e.g., +5.61 \% vs +1.13 \% for G+M on GSM8K) suggests that while knowledge transfer from the base model contributes significantly to overall performance, our FIM-based step expansion provides complementary benefits through structural enhancement of reasoning chains.

Specifically, we performed instruction fine-tuning on Qwen2.5-Math-7B, the same base model as MathFimer-7B, using the NuminaMath-CoT dataset. We then used the fine-tuned model to generate answers for questions from GSM8k+MATH, MathInstruct, and MetaMathQA, producing the distillation data: GSM8k+MATH(distill), MathInstruct(distill), and MetaMathQA(distill). Next, we applied MathFimer-7B to perform step expansion on these three datasets. Finally, we compared the performance differences between the distillation data and the step-expanded distillation data.

\begin{table}[htbp]
  \centering
  \setlength{\tabcolsep}{1.0mm}
  \resizebox{1.0\linewidth}{!}{
\begin{tabular}{llllll}
\toprule
\multicolumn{1}{l}{\textbf{Dataset}} & \multicolumn{1}{c}{\textbf{FIM}} & \multicolumn{1}{c}{\textbf{GSM8K}} & \multicolumn{1}{c}{\textbf{MATH}} & \multicolumn{1}{c}{\textbf{Odyssey}} & \multicolumn{1}{c}{\textbf{OB-EN}} \\
\midrule
\midrule
G+M   & /     & 67.55 & 18.32 & 21.59 & 1.78 \\
      & 7B    & 73.16\scriptsize{+5.61} & 21.84\scriptsize{+3.52} & 21.34\scriptsize{-0.25} & 2.52\scriptsize{+0.74} \\
\midrule
\multicolumn{1}{p{3.835em}}{G+M} & /     & 81.58 & 29.32 & 27.76 & 4.44 \\
(distill) & 7B    & 82.71\scriptsize{+1.13} & 31.88\scriptsize{+2.56} & 28.02\scriptsize{+0.26} & 5.78\scriptsize{+1.34} \\
\midrule
\midrule
MI-CoT & /     & 67.78 & 18.74 & 22.11 & 2.37 \\
      & 7B    & 75.21\scriptsize{+7.43} & 22.9\scriptsize{+4.16} & 24.42\scriptsize{+2.31} & 3.56\scriptsize{+1.19} \\
\midrule
\multicolumn{1}{p{3.835em}}{MI-CoT} & /     & 83.32 & 35.90 & 32.90 & 6.22 \\
(distill) & 7B    & 84.99\scriptsize{+1.67} & 36.18\scriptsize{+0.28} & 32.39\scriptsize{-0.51} & 7.56\scriptsize{+1.34} \\
\midrule
\midrule
MMQA  & /     & 84.15 & 34.66 & 29.05 & 6.37 \\
      & 7B    & 84.69\scriptsize{+0.54} & 35.12\scriptsize{+0.46} & 28.79\scriptsize{-0.26} & 6.81\scriptsize{+0.44} \\
\midrule
\multicolumn{1}{p{3.835em}}{MMQA} & /     & 84.23 & 35.18 & 24.42 & 6.81 \\
(distill) & 7B    & 85.29\scriptsize{+1.06} & 34.8\scriptsize{-0.38} & 26.99\scriptsize{+2.57} & 7.41\scriptsize{+0.6} \\
\bottomrule
\end{tabular}%
    }

\caption{Performance decomposition experimental results. For the abbreviations in the table, G+M refers to GSM8K+MATH, MI-CoT refers to MathInstruct-CoT, MMQA refers to MetaMathQA, Odyssey refers to Math Odyssey, and OB-EN refers to OlympiadBench-EN. Experiments are conduct on Meta-Llama-3.1-8B.}
\label{tab:ablation_distill}
\end{table}%

As shown in the results in Table \ref{tab:ablation_distill}, the performance improvement of MathFimer on the distillation data is smaller than on the original data, indicating that part of the effect of MathFimer comes from model distillation. However, even on the distillation data, using MathFimer still yields a noticeable performance gain, suggesting that the step expansion method proposed in this paper does indeed contribute to improving performance. 

\subsection{Analysis of Iteration Effects}
\label{sec:scaling}

Our iterative step expansion experiments demonstrate the robust scalability of MathFimer. As shown in Table \ref{tab:ablation_scaling}, each iteration of step expansion consistently improves reasoning performance across most benchmarks. Notably, on the GSM8K benchmark, MI-CoT achieves substantial gains of +7.43\%, +12.43\%, and +15.54\% percentage points over three iterations, reaching 83.32\% accuracy. Similar patterns emerge on MATH, with consistent improvements culminating in a +9.42\% percentage point gain. This iterative enhancement suggests that MathFimer effectively constructs increasingly sophisticated reasoning chains, where each expansion cycle introduces valuable intermediate steps that contribute to improved problem-solving capabilities. The consistent performance gains across different datasets and iteration counts validate the scalability of our approach and its ability to leverage extended reasoning chains for enhanced mathematical reasoning.

\begin{table}[tbp]
  \centering
  \setlength{\tabcolsep}{1.0mm}
  \resizebox{1.0\linewidth}{!}{
\begin{tabular}{llllll}
\toprule
\textbf{Dataset} & \textbf{Iter} & \multicolumn{1}{c}{\textbf{GSM8K}} & \multicolumn{1}{c}{\textbf{MATH}} & \multicolumn{1}{c}{\textbf{Odyssey}} & \multicolumn{1}{c}{\textbf{OB-EN}} \\
\midrule
\multirow{4}[2]{*}{G+M} & 0     & 67.55 & 18.32 & 21.59 & 1.78 \\
      & 1     & 73.16\scriptsize{+5.61} & 21.84\scriptsize{+3.52} & 21.34\scriptsize{-0.25} & 2.52\scriptsize{+0.74} \\
      & 2     & 77.03\scriptsize{+9.48} & 23.5\scriptsize{+5.18} & 21.08\scriptsize{-0.51} & 6.07\scriptsize{+4.29} \\
      & 3     & 78.7\scriptsize{+11.15} & 25.54\scriptsize{+7.22} & 22.37\scriptsize{+0.78} & 6.67\scriptsize{+4.89} \\
\midrule
\multirow{4}[2]{*}{MI-CoT} & 0     & 67.78 & 18.74 & 22.11 & 2.37 \\
      & 1     & 75.21\scriptsize{+7.43} & 22.9\scriptsize{+4.16} & 24.42\scriptsize{+2.31} & 3.56\scriptsize{+1.19} \\
      & 2     & 80.21\scriptsize{+12.43} & 26.68\scriptsize{+7.94} & 27.76\scriptsize{+5.65} & 4.44\scriptsize{+2.07} \\
      & 3     & 83.32\scriptsize{+15.54} & 28.16\scriptsize{+9.42} & 26.48\scriptsize{+4.37} & 6.67\scriptsize{+4.3} \\
\bottomrule
\end{tabular}%
}
\caption{Scaling analysis results. For the abbreviations in the table, G+M refers to GSM8K+MATH, MI-CoT refers to MathInstruct-CoT, Odyssey refers to Math Odyssey, and OB-EN refers to OlympiadBench-EN. Experiments are conduct on Meta-Llama-3.1-8B. Model used for step expansion is MathFimer-7B.}
\label{tab:ablation_scaling}
\end{table}%


\subsection{Impact of Model Scale}
To investigate the relationship between model capacity and step expansion capability, we conducted a systematic comparison between MathFimer-7B and MathFimer-72B. We trained MathFimer-72B on Qwen2.5-Math-72B using identical training data and hyperparameters as MathFimer-7B to ensure controlled comparison conditions.


% Table generated by Excel2LaTeX from sheet 'Sheet2'
\begin{table}[tbp]
  \centering
  \setlength{\tabcolsep}{1.0mm}
  \resizebox{1.0\linewidth}{!}{
% Table generated by Excel2LaTeX from sheet 'Sheet5'
\begin{tabular}{llllll}
\toprule
\textbf{Dataset} & \multicolumn{1}{c}{\textbf{FIM}} & \multicolumn{1}{c}{\textbf{GSM8K}} & \multicolumn{1}{c}{\textbf{MATH}} & \multicolumn{1}{c}{\textbf{Odyssey}} & \multicolumn{1}{c}{\textbf{OB-EN}} \\
\midrule
\multirow{3}[2]{*}{G+M} & /     & 67.55 & 18.32 & 21.59 & 1.78 \\
      & 7B    & 73.16\scriptsize{+5.61} & 21.84\scriptsize{+3.52} & 21.34\scriptsize{-0.25} & 2.52\scriptsize{+0.74} \\
      & 72B   & 73.09\scriptsize{+5.54} & 21.84\scriptsize{+3.52} & 23.39\scriptsize{+1.8} & 2.07\scriptsize{+0.29} \\
\midrule
\multirow{3}[2]{*}{MI-CoT} & /     & 67.78 & 18.74 & 22.11 & 2.37 \\
      & 7B    & 75.21\scriptsize{+7.43} & 22.9\scriptsize{+4.16} & 24.42\scriptsize{+2.31} & 3.56\scriptsize{+1.19} \\
      & 72B   & 73.92\scriptsize{+6.14} & 23.06\scriptsize{+4.32} & 24.68\scriptsize{+2.57} & 2.67\scriptsize{+0.3} \\
\midrule
\multirow{3}[2]{*}{MMQA} & /     & 84.15 & 34.66 & 29.05 & 6.37 \\
      & 7B    & 84.69\scriptsize{+0.54} & 35.12\scriptsize{+0.46} & 28.79\scriptsize{-0.26} & 6.81\scriptsize{+0.44} \\
      & 72B   & 84.91\scriptsize{+0.76} & 34.44\scriptsize{-0.22} & 28.02\scriptsize{-1.03} & 5.93\scriptsize{-0.44} \\
\bottomrule
\end{tabular}%

}
\caption{Different model size of MathFimer. For the abbreviations in the table, G+M refers to GSM8K+MATH, MI-CoT refers to MathInstruct-CoT, MMQA refers to MetaMathQA, Odyssey refers to Math Odyssey, and OB-EN refers to OlympiadBench-EN. Experiments are conduct on Meta-Llama-3.1-8B.}
\label{tab:ablation_model_size}
\end{table}%


Our experimental results, as presented in Table \ref{tab:ablation_model_size}, reveal an interesting finding: the performance gap between MathFimer-7B and MathFimer-72B is notably small across all benchmarks. For instance, on the GSM8K benchmark with MI-CoT data, MathFimer-7B achieves a +7.43\% improvement while MathFimer-72B shows a +6.14\% gain. This pattern of comparable performance persists across different datasets and evaluation metrics, suggesting that step expansion quality may not be significantly bottlenecked by model capacity. These results indicate that the step expansion task might be effectively addressed with relatively modest model sizes, potentially due to the structured nature of mathematical reasoning steps and the explicit decomposition in our approach.

\section{Conclusion}

In this paper, we introduce the Fill-in-the-middle (FIM) paradigm into mathematical reasoning chains. We construct NuminaMath-FIM by decomposing solutions into prefix-suffix pairs, where intermediate steps are held out for reconstruction. Through training on these prefix-middle-suffix triplets, we develop MathFimer models that can effectively expand reasoning steps while preserving solution coherence. Our comprehensive experiments across multiple mathematical reasoning datasets demonstrate that MathFimer-enhanced data consistently improves model performance with relative improvements of 7.43\% on GSM8K and 8.86\% on MATH.


\section*{Limitations}
While our MathFimer framework demonstrates promising results in enhancing mathematical reasoning through step expansion, we identify several important limitations that warrant careful consideration and future investigation.


\paragraph{Domain Generalization} While our approach demonstrates effectiveness in mathematical reasoning, its applicability to other reasoning domains remains uncertain. The current implementation and evaluation focus exclusively on mathematical problem-solving, leaving open questions about the framework's generalizability to domains such as code reasoning, logical deduction, and commonsense reasoning, where solution structures and validation requirements may differ significantly.

\paragraph{Generation Reliability} Our step expansion process inherently relies on model generation, introducing potential risks of error propagation. Despite overall improvements in reasoning quality, we currently lack robust mechanisms for verifying the logical consistency and mathematical correctness of inserted steps. This limitation becomes particularly critical when applying multiple iterations of step expansion, where errors could potentially accumulate.

\paragraph{Methodological Limitations} The framework's effectiveness inherently depends on the quality of initial training data and may inherit biases from base models. Additionally, the current approach primarily focuses on expanding existing solution patterns rather than generating novel solution approaches, potentially limiting its applicability to extremely complex or unconventional problems.

\bibliography{acl}
\appendix
\end{document}
