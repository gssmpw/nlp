[
  {
    "index": 0,
    "papers": [
      {
        "key": "wei2023chainofthought",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny",
        "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "paster2023openwebmatha",
        "author": "Paster, Keiran and Santos, Marco Dos and Azerbayev, Zhangir and Ba, Jimmy",
        "title": "OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text"
      },
      {
        "key": "zhang2024autonomous",
        "author": "Zhang, Yifan and Luo, Yifan and Yuan, Yang and Yao, Andrew Chi-Chih",
        "title": "Autonomous Data Selection with Language Models for Mathematical Texts"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ding2024unleashing",
        "author": "Ding, Yuyang and Shi, Xinyu and Liang, Xiaobo and Li, Juntao and Zhu, Qiaoming and Zhang, Min",
        "title": "Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch"
      },
      {
        "key": "zhou2024jiuzhang30",
        "author": "Zhou, Kun and Zhang, Beichen and Wang, Jiapeng and Chen, Zhipeng and Zhao, Wayne Xin and Sha, Jing and Sheng, Zhichao and Wang, Shijin and Wen, Ji-Rong",
        "title": "JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models"
      },
      {
        "key": "E-GSM2024Xu",
        "author": "Xu, Xin and Xiao, Tong and Chao, Zitong and Huang, Zhenya and Yang, Can and Wang, Yang",
        "title": "Can LLMs Solve longer Math Word Problems Better?"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "lightman2023lets",
        "author": "Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl",
        "title": "Let's Verify Step by Step"
      },
      {
        "key": "pds2024xu",
        "author": "Xu, Xin and Diao, Shizhe and Yang, Can and Wang, Yang",
        "title": "Can We Verify Step by Step for Incorrect Answer Detection?"
      },
      {
        "key": "wang2024mathshepherd",
        "author": "Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, R. X. and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Y. and Sui, Zhifang",
        "title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-Step without Human Annotations"
      },
      {
        "key": "zhang2025lessons",
        "author": "Zhang, Zhenru and Zheng, Chujie and Wu, Yangzhen and Zhang, Beichen and Lin, Runji and Yu, Bowen and Liu, Dayiheng and Zhou, Jingren and Lin, Junyang",
        "title": "The Lessons of Developing Process Reward Models in Mathematical Reasoning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "hoffmann2022training",
        "author": "Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent",
        "title": "Training Compute-Optimal Large Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "brown2024large",
        "author": "Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V. and R{\\'e}, Christopher and Mirhoseini, Azalia",
        "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling"
      },
      {
        "key": "snell2024scaling",
        "author": "Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral",
        "title": "Scaling LLM Test-Time Compute Optimally Can Be More Effective than Scaling Model Parameters"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wei2023chainofthought",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny",
        "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wang2023selfconsistency",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "chen2024alphamath",
        "author": "Chen, Guoxin and Liao, Minpeng and Li, Chengxi and Fan, Kai",
        "title": "AlphaMath Almost Zero: Process Supervision without Process"
      },
      {
        "key": "feng2024alphazerolike",
        "author": "Feng, Xidong and Wan, Ziyu and Wen, Muning and McAleer, Stephen Marcus and Wen, Ying and Zhang, Weinan and Wang, Jun",
        "title": "Alphazero-like Tree-Search Can Guide Large Language Model Decoding and Training"
      },
      {
        "key": "guan2025rstarmath",
        "author": "Guan, Xinyu and Zhang, Li Lyna and Liu, Yifei and Shang, Ning and Sun, Youran and Zhu, Yi and Yang, Fan and Yang, Mao",
        "title": "rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "jin2024impact",
        "author": "Jin, Mingyu and Yu, Qinkai and Shu, Dong and Zhao, Haiyan and Hua, Wenyue and Meng, Yanda and Zhang, Yongfeng and Du, Mengnan",
        "title": "The Impact of Reasoning Step Length on Large Language Models"
      },
      {
        "key": "ying2024internlmmatha",
        "author": "Ying, Huaiyuan and Zhang, Shuo and Li, Linyang and Zhou, Zhejian and Shao, Yunfan and Fei, Zhaoye and Ma, Yichuan and Hong, Jiawei and Liu, Kuikun and Wang, Ziyi and Wang, Yudong and Wu, Zijian and Li, Shuaibin and Zhou, Fengzhe and Liu, Hongwei and Zhang, Songyang and Zhang, Wenwei and Yan, Hang and Qiu, Xipeng and Wang, Jiayu and Chen, Kai and Lin, Dahua",
        "title": "InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "wang2023planandsolve",
        "author": "Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng",
        "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "yan2024s$^3$cmath",
        "author": "Yan, Yuchen and Jiang, Jin and Liu, Yang and Cao, Yixin and Xu, Xin and {zhang}, Mengdi and Cai, Xunliang and Shao, Jian",
        "title": "S\\${\\textasciicircum}3\\$c-Math: Spontaneous Step-Level Self-Correction Makes Large Language Models Better Mathematical Reasoners"
      }
    ]
  }
]