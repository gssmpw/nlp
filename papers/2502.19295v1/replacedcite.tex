\section{Background and Related Work}
We define a planning task specifically designed for solving with an LLM as $\{\mathcal{S}, s_0, \mathcal{G}, \mathcal{A}_\theta, T_\theta\}$. Here $\mathcal{S}$ represents the state space, containing all possible states. The initial state is denoted as $s_0 \in \mathcal{S}$, and the goal states are represented as $\mathcal{G} = \{ g_0,g_1, \cdots, g_m\}$, where each $g_i \in \mathcal{S}$ is a possible goal state. Note, in some cases, there may be only one goal state, i.e., $\mathcal{G} = \{ g_0\}$. The action space $\mathcal{A}_\theta(s)$ is generated by a pre-trained LLM with parameters $\theta$ based on the current state $s$, providing a set of valid actions for state $s$. The transition function $T_\theta : \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$, also parameterized by the LLM, predicts the outcome of applying an action $a \in \mathcal{A}_\theta(s)$ to a state $s$.  A solution to the planning task is represented as a state-action trace $\Pi = (s_0, a_1, s_1, a_2, \cdots, a_n,s_n )$ where $s_{i+1} = T_\theta(s_i, a_{i+1})$ and $s_n \in \mathcal{G}$.

\textbf{LLMs for Planning.} 
LLMs have demonstrated potential for reasoning and common-sense capabilities, driving their growing adoption in the planning tasks across various domains. Specifically, LLMs have been widely used as policy models for decision-making and planning in diverse interactive environments, including robotics____, multimodal games____, and text-based environments____. To further enhance their performance on planning problems, researchers have developed test-time inference techniques that adaptively guide the model's reasoning and decision-making processes during inference time. For example, Chain of Thought (CoT) prompting____ guides LLMs to explicitly generate intermediate reasoning steps, yielding state-action traces for planning tasks. Building on this, Tree of Thoughts (ToT)____ and Graph of Thoughts (GoT)____ explore multiple paths to improve decision-making and robustness. Similarly, XoT____ trains an auxiliary policy model to assign rewards to solutions. Techniques like RAP____ utilize Monte Carlo Tree Search to navigate the expansive action space, while FoR____ introduces a fine-tuning approach to discover diverse and creative solutions across multiple planning tasks. Despite their effectiveness, these methods often rely on self-evaluation, which can be prone to reliability issues____, or require additional models and fine-tuning, which entail significant computational overhead.

% In contrast, the second approach enhances reasoning capabilities by augmenting the LLM with task-specific adapters, external reasoning modules, or fine-tuning on domain-specific datasets. Supervised fine-tuning (SFT) with large-scale, high-quality datasets of reasoning chains has demonstrated remarkable effectiveness. For example, STaR____ uses online sampling with self-correction to generate positive samples for fine-tuning LLM. On the other hand, an extra verifier model is trained to select the best-of-N results, leading to better results on specific datasets____. This approach can be further improved by training a process-based verifier or a process reward model (PRM), which predicts the correctness of each intermediate step in a solution____. While this approach often achieves superior results on targeted tasks, it introduces additional computational requirements and may reduce the model's generalization capability.

\textbf{LLMs for Automated Heuristic Discovery.} 
% Funsearch, EoH, Petar's paper and Rao's paper on heuristic exploration with LLMs. 
Automatic heuristic design refers to the process of generating, optimizing, or adapting heuristic functions automatically to solve problems. Traditional heuristic functions are typically handcrafted based on domain expertise, but automatic methods use algorithms to create or refine these heuristic functions without manual intervention. Evolutionary algorithms like genetic programming____ have been widely used, allowing heuristic functions to evolve through processes inspired by natural selection. Recently, LLMs have emerged as powerful tools for code generation. Their extensive pretraining provides them with broad knowledge and contextual understanding, enabling them to assist in generating accurate code to solve various problems. For instance, ____ uses LLMs as optimizers to directly generate new trial solutions via in-context learning. Similarly, FunSearch____ uses LLMs to guide evolutionary procedures in mathematical discovery. Furthermore, ____ extend FunSearch by evolving scoring functions, significantly enhancing performance on combinatorial competitive programming tasks. ____ propose to use LLM with evolutionary computation methods for automatic heuristic design. However, automated heuristic discovery for LLM planning remains largely unexplored, and our work is the first to investigate this.

\begin{figure}[t]
    \begin{center}
    \begin{tabular}{@{\hspace{0cm}}c@{\hspace{-0.45cm}}c@{\hspace{-0.45cm}}c@{\hspace{-0.45cm}}c}
    \includegraphics[height=5.2cm]{FIG/COT.pdf} &
    \includegraphics[height=5.2cm]{FIG/COT_SC.pdf} &
    \includegraphics[height=5.2cm]{FIG/TOT.pdf} & 
    \includegraphics[height=5.2cm]{FIG/AutoHD.pdf} \vspace{-0.0cm} \\
    (a) CoT & (b) CoT-SC & (c) ToT & (d) AutoHD \\
    \end{tabular}
    \end{center}\vspace{-0.0cm}
    \caption{\small Comparison between existing methods and the proposed method. (a) CoT follows a single linear path, thereby constraining its exploratory capacity. (b) CoT-SC extends this approach by performing multiple CoT iterations, leading to a result with higher confidence scores. (c) ToT introduces a tree-based search mechanism, branching systematically through intermediate states to explore a broader solution space. (D) In contrast, our AutoHD uses a heuristic function generated by the LLM to guide exploration. The heuristic prioritizes promising states, enabling more efficient and effective navigation of the solution space. }
     \label{fig:comparison}
     \vspace{-0.0cm}
\end{figure}