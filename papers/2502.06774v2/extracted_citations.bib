@Article{Agrawal2019_DifferentiableConvexOptimization,
  author    = {Agrawal, Akshay and Amos, Brandon and Barratt, Shane and Boyd, Stephen and Diamond, Steven and Kolter, Zico},
  journal   = {33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada},
  title     = {Differentiable Convex Optimization Layers},
  year      = {2019},
  comment   = {Extension to OptNet (Amos and Kolter, 2017). Please see comments there. Here they basically expand to any convex program (not only quadratic).},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.1910.12430},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/Constrained NN/Differentiable COnvex Optimization Layer.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}

@Article{Amos2017_OptNetDifferentiableOptimization,
  author    = {Amos, Brandon and Kolter, J. Zico},
  journal   = {Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017},
  title     = {OptNet: Differentiable Optimization as a Layer in Neural Networks},
  year      = {2017},
  comment   = {They develop differentiable optimization layers for quadratic problems (quadratic objective and linear eq/ineq). The main contribution is a computationally cheap wayof obtaining the gradients in the backward pass. However, they have to solve a constrained convex optimization problem at every forward pass (they develop a batched interior point method).},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.1703.00443},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/Constrained optimization learning/OptNet Differentiable Optimization As a Layer in Neural Networks.pdf:PDF},
  groups    = {Constrained optimization learning, 00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}

@Article{Anil2018_SortingoutLipschitz,
  author    = {Anil, Cem and Lucas, James and Grosse, Roger},
  journal   = {Proceedings of the 36th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019},
  title     = {Sorting out Lipschitz function approximation},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.1811.05381},
  file      = {:https___doi.org_10.48550_arxiv.1811.05381 - Sorting Out Lipschitz Function Approximation.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Balestriero2022_POLICEProvablyOptimal,
  author    = {Balestriero, Randall and LeCun, Yann},
  journal   = {arXiv preprint},
  title     = {POLICE: Provably Optimal Linear Constraint Enforcement for Deep Neural Networks},
  year      = {2022},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {https://doi.org/10.48550/arXiv.2211.01340},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/Constrained NN/POLICE.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Beucler2019_EnforcingAnalyticConstraints,
  author     = {Beucler, Tom and Pritchard, Michael and Rasp, Stephan and Ott, Jordan and Baldi, Pierre and Gentine, Pierre},
  journal    = {Physical Review Letters},
  title      = {Enforcing Analytic Constraints in Neural-Networks Emulating Physical Systems},
  year       = {2019},
  issn       = {1079-7114},
  month      = mar,
  number     = {9},
  pages      = {098302},
  volume     = {126},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  doi        = {https://doi.org/10.1103/PhysRevLett.126.098302},
  file       = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/PINNs/Enforcing Analytic Constraints in Neural-Networks Emulating Physical Systems.pdf:PDF},
  groups     = {PINNs, 00-ESCAPE35, 00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords   = {Computational Physics (physics.comp-ph), Atmospheric and Oceanic Physics (physics.ao-ph), FOS: Physical sciences, FOS: Physical sciences},
  publisher  = {arXiv},
  readstatus = {read},
}

@Article{Bronstein2017_GeometricDeepLearning,
  author    = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  journal   = {IEEE Signal Processing Magazine},
  title     = {Geometric Deep Learning: Going beyond Euclidean data},
  year      = {2017},
  issn      = {1558-0792},
  month     = jul,
  number    = {4},
  pages     = {18--42},
  volume    = {34},
  doi       = {https://doi.org/10.1109/MSP.2017.2693418},
  groups    = {00-Paper-ENFORCE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{Brosowsky2020_SampleSpecificOutput,
  author    = {Brosowsky, Mathis and Dünkel, Olaf and Slieter, Daniel and Zöllner, Marius},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence, 35(8), 6812-6821},
  title     = {Sample-Specific Output Constraints for Neural Networks},
  year      = {2020},
  comment   = {They do constraint (convex polytopes, a.k.a., linear) on the output. So, they are essentially inequality constraints. They need additional inputs to the NN to enforce the constraints. The additional input is some parameter characterizing the constraints.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2003.10258},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Chen2021_Theoryguidedhard,
  author    = {Chen, Yuntian and Huang, Dou and Zhang, Dongxiao and Zeng, Junsheng and Wang, Nanzhe and Zhang, Haoran and Yan, Jinyue},
  journal   = {Journal of Computational Physics},
  title     = {Theory-guided hard constraint projection (HCP): A knowledge-based data-driven scientific machine learning method},
  year      = {2021},
  issn      = {0021-9991},
  month     = nov,
  pages     = {110624},
  volume    = {445},
  comment   = {Very interesting, they use same concept as KKT-hPINN but on differential equations (discretized). They use projection. They claim that is exacteven if they satisfy a discretized version (that is, an approximation by definition). However, they can sample MORE collocation points and CONTROL the approximation degree (smaller delta). This is not doable for our taylor method.},
  doi       = {https://doi.org/10.1016/j.jcp.2021.110624},
  groups    = {00-Paper: Hard constrained NN},
  publisher = {Elsevier BV},
}

@Article{Chen2024_PhysicsInformedNeural,
  author    = {Chen, Hao and Flores, Gonzalo E. Constante and Li, Can},
  journal   = {Computers \&  Chemical Engineering},
  title     = {Physics-Informed Neural Networks with Hard Linear Equality Constraints},
  year      = {2024},
  issn      = {0098-1354},
  month     = oct,
  pages     = {108764},
  volume    = {189},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.1016/j.compchemeng.2024.108764},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/PINNs/Physics-Informed Neural Networks with Hard Linear Equality Constraints.pdf:PDF},
  groups    = {PINNs, 00-ESCAPE35, 00-Paper: Hard constrained NN},
  keywords  = {Machine Learning (cs.LG), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}

@Article{Donti2021_DC3learningmethod,
  author    = {Donti, Priya L. and Rolnick, David and Kolter, J. Zico},
  journal   = {International Conference on Learning Representations},
  title     = {DC3: A learning method for optimization with hard constraints},
  year      = {2021},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {https://doi.org/10.48550/arXiv.2104.12225},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/Constrained optimization learning/DC3 A learning method for optimization with hard constraints.pdf:PDF},
  groups    = {Constrained optimization learning, 00-ESCAPE3, 00-ESCAPE35, 00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}

@Article{Erichson2019_PhysicsinformedAutoencoders,
  author    = {Erichson, N. Benjamin and Muehlebach, Michael and Mahoney, Michael W.},
  journal   = {Second Workshop on Machine Learning and the Physical Sciences (NeurIPS 2019), Vancouver, Canada},
  title     = {Physics-informed Autoencoders for Lyapunov-stable Fluid Flow Prediction},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.1905.10866},
  groups    = {00-Paper-ENFORCE},
  keywords  = {Computational Physics (physics.comp-ph), Machine Learning (cs.LG), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Frerix2020_HomogeneousLinearInequality,
  author    = {Frerix, Thomas and Niesner, Matthias and Cremers, Daniel},
  journal   = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  title     = {Homogeneous Linear Inequality Constraints for Neural Network Activations},
  year      = {2020},
  month     = jun,
  pages     = {3229--3234},
  volume    = {521},
  comment   = {They enforce linear INEQUALITY constraints of the form Ax>=b.
They have some interesting perspective on constrained generative models (like variational autoencoders).},
  doi       = {10.1109/cvprw50498.2020.00382},
  file      = {:Frerix_2020 - Homogeneous Linear Inequality Constraints for Neural Network Activations.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  publisher = {IEEE},
}

@Article{Giunchiglia2021_MultiLabelClassification,
  author    = {Giunchiglia, Eleonora and Lukasiewicz, Thomas},
  journal   = {Journal of Artificial Intelligence Research},
  title     = {Multi-Label Classification Neural Networks with Hard Logical Constraints},
  year      = {2021},
  issn      = {1076-9757},
  month     = nov,
  pages     = {759--818},
  volume    = {72},
  doi       = {https://doi.org/10.1613/jair.1.12850},
  groups    = {00-Paper-ENFORCE},
  publisher = {AI Access Foundation},
}

@Article{Karniadakis2021_Physicsinformedmachine,
  author     = {George Em Karniadakis and Ioannis G. Kevrekidis and Lu Lu and Paris Perdikaris and Sifan Wang and Liu Yang},
  journal    = {Nature Reviews Physics},
  title      = {Physics-informed machine learning},
  year       = {2021},
  month      = {may},
  number     = {6},
  pages      = {422--440},
  volume     = {3},
  doi        = {10.1038/s42254-021-00314-5},
  file       = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/PINNs/Physics-informed machine learning.pdf:PDF},
  groups     = {Review, 00-Paper: Hard constrained NN},
  publisher  = {Springer Science and Business Media {LLC}},
  ranking    = {rank4},
  readstatus = {read},
}

@Article{Konstantinov2023_NewComputationallySimple,
  author    = {Konstantinov, A. V. and Utkin, L. V.},
  journal   = {Doklady Mathematics},
  title     = {A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints},
  year      = {2023},
  issn      = {1531-8362},
  month     = dec,
  number    = {S2},
  pages     = {S233--S241},
  volume    = {108},
  comment   = {Very mathematical paper. They claim to do hard constraint with CONVEX constraint. They demonstrate with linear and quadratic constraints. They claim that the method is very computationally cheap. Apparently, they mainly talk about INEQUALITY constraints. Then they achieve equalities by using auxiliary variables.},
  doi       = {https://doi.org/10.1134/S1064562423701077},
  groups    = {00-Paper: Hard constrained NN},
  publisher = {Pleiades Publishing Ltd},
}

@Article{Kotary2021_EndEndConstrained,
  author    = {Kotary, James and Fioretto, Ferdinando and Van Hentenryck, Pascal and Wilder, Bryan},
  journal   = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI-21)},
  title     = {End-to-End Constrained Optimization Learning: A Survey},
  year      = {2021},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {https://doi.org/10.48550/arXiv.2103.16378},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Lastrucci2025_PicardKKThPINN,
  author    = {Lastrucci, Giacomo and Karia, Tanuj and Gromotka, Zoë and Schweidtmann, Artur M.},
  journal   = {arXiv preprint},
  title     = {Picard-KKT-hPINN: Enforcing Nonlinear Enthalpy Balances for Physically Consistent Neural Networks},
  year      = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2501.17782},
  groups    = {00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{LeCun1989_BackpropagationAppliedHandwritten,
  author    = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  journal   = {Neural Computation},
  title     = {Backpropagation Applied to Handwritten Zip Code Recognition},
  year      = {1989},
  issn      = {1530-888X},
  month     = dec,
  number    = {4},
  pages     = {541--551},
  volume    = {1},
  doi       = {https://doi.org/10.1162/neco.1989.1.4.541},
  groups    = {00-Paper-ENFORCE},
  publisher = {MIT Press},
}

@Article{Manek2020_LearningStableDeepa,
  author    = {Manek, Gaurav and Kolter, J. Zico},
  journal   = {33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada},
  title     = {Learning Stable Deep Dynamics Models},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2001.06116},
  groups    = {00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Dynamical Systems (math.DS), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}

@Article{Min2024_HardConstrainedNeural,
  author    = {Min, Youngjae and Sonar, Anoopkumar and Azizan, Navid},
  journal   = {ArXiv preprint},
  title     = {Hard-Constrained Neural Networks with Universal Approximation Guarantees},
  year      = {2024},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {https://doi.org/10.48550/arXiv.2410.10807},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/Constrained NN/HardNet.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Mukherjee2024_developmentsteadystate,
  author    = {Mukherjee, Angan and Bhattacharyya, Debangsu},
  journal   = {Computers \&  Chemical Engineering},
  title     = {On the development of steady-state and dynamic mass-constrained neural networks using noisy transient data},
  year      = {2024},
  issn      = {0098-1354},
  month     = aug,
  pages     = {108722},
  volume    = {187},
  comment   = {Only mass constrained!!},
  doi       = {https://doi.org/10.1016/j.compchemeng.2024.108722},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/PINNs/On the development of steady-state and dynamic mass-constrained neural networks using noisy transient data.pdf:PDF},
  groups    = {00-ESCAPE35, 00-Paper: Hard constrained NN},
  publisher = {Elsevier BV},
}

@Article{Pfrommer2020_ContactNetsLearningDiscontinuousa,
  author    = {Pfrommer, Samuel and Halm, Mathew and Posa, Michael},
  journal   = {Conference on Robot Learning 2020},
  title     = {ContactNets: Learning Discontinuous Contact Dynamics with Smooth, Implicit Representations},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2009.11193},
  groups    = {00-Paper-ENFORCE},
  keywords  = {Robotics (cs.RO), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Raissi2019_Physicsinformedneural,
  author     = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
  journal    = {Journal of Computational Physics},
  title      = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  year       = {2019},
  month      = {feb},
  pages      = {686--707},
  volume     = {378},
  comment    = {Constrained included as penalty term in the loss function using automatic differentiation},
  doi        = {https://doi.org/10.1016/j.jcp.2018.10.045},
  file       = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/PINNs/Physics-informed neural networks A deep learning...pdf:PDF},
  groups     = {PINNs, 00-ESCAPE34, 00-Paper: Hard constrained NN},
  publisher  = {Elsevier {BV}},
  readstatus = {read},
}

@Article{Schweidtmann2018_DeterministicGlobalOptimization,
  author    = {Schweidtmann, Artur M. and Mitsos, Alexander},
  journal   = {Journal of Optimization Theory and Applications},
  title     = {Deterministic Global Optimization with Artificial Neural Networks Embedded},
  year      = {2018},
  issn      = {1573-2878},
  month     = oct,
  number    = {3},
  pages     = {925--948},
  volume    = {180},
  doi       = {https://doi.org/10.1007/s10957-018-1396-0},
  groups    = {00-Paper-ENFORCE},
  publisher = {Springer Science and Business Media LLC},
}

@Article{Schweidtmann2021_Obeyvaliditylimits,
  author    = {Schweidtmann, Artur M. and Weber, Jana M. and Wende, Christian and Netze, Linus and Mitsos, Alexander},
  journal   = {Optimization and Engineering},
  title     = {Obey validity limits of data-driven models through topological data analysis and one-class classification},
  year      = {2021},
  issn      = {1573-2924},
  month     = may,
  number    = {2},
  pages     = {855--876},
  volume    = {23},
  doi       = {https://doi.org/10.1007/s11081-021-09608-0},
  groups    = {00-Paper: Hard constrained NN},
  publisher = {Springer Science and Business Media LLC},
}

@Article{Schweidtmann2024_reviewperspectivehybrida,
  author    = {Schweidtmann, Artur M. and Zhang, Dongda and von Stosch, Moritz},
  journal   = {Digital Chemical Engineering},
  title     = {A review and perspective on hybrid modeling methodologies},
  year      = {2024},
  issn      = {2772-5081},
  month     = mar,
  pages     = {100136},
  volume    = {10},
  doi       = {https://doi.org/10.1016/j.dche.2023.100136},
  groups    = {00-Paper-ENFORCE},
  publisher = {Elsevier BV},
}

@Article{Stoian2024_HowRealisticIs,
  author    = {Stoian, Mihaela Cătălina and Dyrmishi, Salijona and Cordy, Maxime and Lukasiewicz, Thomas and Giunchiglia, Eleonora},
  journal   = {Published as a conference paper at ICLR 2024},
  title     = {How Realistic Is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data},
  year      = {2024},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2402.04823},
  groups    = {00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Tordesillas2023_RAYENImpositionHard,
  author    = {Tordesillas, Jesus and How, Jonathan P. and Hutter, Marco},
  journal   = {ArXiv preprint},
  title     = {RAYEN: Imposition of Hard Convex Constraints on Neural Networks},
  year      = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2307.08336},
  file      = {:Tordesillas2023_RAYENImpositionHard - RAYEN_ Imposition of Hard Convex Constraints on Neural Networks.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Waechter2005_implementationinteriorpoint,
  author    = {Wächter, Andreas and Biegler, Lorenz T.},
  journal   = {Mathematical Programming},
  title     = {On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming},
  year      = {2005},
  issn      = {1436-4646},
  month     = apr,
  number    = {1},
  pages     = {25--57},
  volume    = {106},
  doi       = {10.1007/s10107-004-0559-y},
  groups    = {00-Paper-ENFORCE},
  publisher = {Springer Science and Business Media LLC},
}

@Article{Wang2020_Understandingmitigatinggradient,
  author     = {Wang, Sifan and Teng, Yujun and Perdikaris, Paris},
  journal   = {ArXiv},
  title      = {Understanding and mitigating gradient pathologies in physics-informed neural networks},
  year       = {2020},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  doi        = {https://doi.org/10.48550/arXiv.2001.04536},
  file       = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/PINNs/Understanding and Mitigating Gradient Pathologies in PINNs.pdf:PDF},
  groups     = {PINNs, 00-ESCAPE34, 00-Paper-ENFORCE},
  keywords   = {Machine Learning (cs.LG), Numerical Analysis (math.NA), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher  = {arXiv},
  ranking    = {rank5},
  readstatus = {read},
}

@Article{Wang2020_WhenwhyPINNs,
  author    = {Wang, Sifan and Yu, Xinling and Perdikaris, Paris},
  journal   = {ArXiv},
  title     = {When and why PINNs fail to train: A neural tangent kernel perspective},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2007.14527},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/PINNs/When and why PINNs fail to train A neural tangent kernel.pdf:PDF},
  groups    = {PINNs, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Numerical Analysis (math.NA), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}

@Article{Wang2021_Learningsolutionoperator,
  author    = {Wang, Sifan and Wang, Hanwen and Perdikaris, Paris},
  title     = {Learning the solution operator of parametric partial differential equations with physics-informed DeepOnets},
  year      = {2021},
  journal   = {ArXiv},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2103.10974},
  groups    = {00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Numerical Analysis (math.NA), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}

@Article{Wang2024_LinSATNetPositiveLinear,
  author    = {Wang, Runzhong and Zhang, Yunhao and Guo, Ziao and Chen, Tianyi and Yang, Xiaokang and Yan, Junchi},
  journal   = {In Proceedings of the 40th International Conference on Machine Learning (ICML'23)},
  title     = {LinSATNet: The Positive Linear Satisfiability Neural Networks},
  year      = {2024},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {https://doi.org/10.48550/arXiv.2407.13917},
  file      = {:Wang2024_LinSATNetPositiveLinear - LinSATNet_ the Positive Linear Satisfiability Neural Networks.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Artificial Intelligence (cs.AI), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}

@Article{Wu2021_ComprehensiveSurveyGraph,
  author    = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  title     = {A Comprehensive Survey on Graph Neural Networks},
  year      = {2021},
  issn      = {2162-2388},
  month     = jan,
  number    = {1},
  pages     = {4--24},
  volume    = {32},
  doi       = {https://doi.org/10.1109/TNNLS.2020.2978386},
  groups    = {00-Paper-ENFORCE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

