\section{Related work}
\label{sec:related-work}
This section reviews existing approaches to enforce constraints in NNs. According to the focus of this paper, a more in-depth analysis will be conducted on existing methods for enforcing algebraic equality constraints exactly.

\subsection{Soft-constrained neural networks}
% - Defined as early attempts to include constraints. Cite Raissi, Lu, maybe perdikaris and kardianakis.\\
One of the earliest approaches to embedding domain knowledge into NNs involves the use of \textit{soft} constraints. Soft constraints are incorporated as penalty terms appended to the loss function, penalizing residuals of algebraic~\cite{Erichson2019_PhysicsinformedAutoencoders, Pfrommer2020_ContactNetsLearningDiscontinuousa} or differential equations underlying the system~\cite{ Wang2021_Learningsolutionoperator}. Physics-informed neural networks (PINNs)~\cite{Raissi2019_Physicsinformedneural} represent a widely used framework designed to solve partial differential equations (PDEs) with NNs by employing soft constraints and collocation points.\ Although the soft-constrained approach places no restrictions on the complexity of the constraints, it has the drawback of not guaranteeing strict adherence to them. Furthermore, increasing the complexity of the loss function -- especially when the different terms vary in nature or scale -- can degrade the optimization performance of the neural network, often resulting in suboptimal accuracy~\cite{Wang2020_Understandingmitigatinggradient, Wang2020_WhenwhyPINNs}.

\subsection{Hard-constrained neural networks}
\label{subsec:related-work-hard}
Hard-constrained NNs refer to methodological approaches ensuring that neural network predictions strictly adhere to analytical constraints. These constraints, explicitly encoded within the architecture, act as inductive biases, guiding the learning process toward compliance with domain knowledge or restrictions~\cite{Karniadakis2021_Physicsinformedmachine}. Architectures such as convolutional neural networks (CNNs)~\cite{LeCun1989_BackpropagationAppliedHandwritten} and graph neural networks (GNNs)~\cite{Bronstein2017_GeometricDeepLearning, Wu2021_ComprehensiveSurveyGraph} encode inductive biases by guaranteeing invariance with respect to patterns and symmetries. Simple analytical constraints can be enforced using differentiable functions, such as sigmoids or ReLU for output bounding and softmax for simplex constraints (non-negativity and normalization). Recent literature includes significant contributions for enforcing analytical inequality constraints, such as convex polytopes and convex sets more generally~\cite{Frerix2020_HomogeneousLinearInequality, Donti2021_DC3learningmethod, Wang2024_LinSATNetPositiveLinear, Tordesillas2023_RAYENImpositionHard, Konstantinov2023_NewComputationallySimple}. One can also constrain the neural network to guarantee specific functional characteristics, such as being 1-Lipschitz~\cite{Anil2018_SortingoutLipschitz} or Lyapunov stable~\cite{Manek2020_LearningStableDeepa}, nevertheless, this is not a core objective of this study. For a broad and recent review on hard-constrained NNs, the reader is also referred to~\cite{Min2024_HardConstrainedNeural}.\\
Since this paper focuses on analytical equality constraints, the following literature review considers existing methods for this specific case. 

\paragraph{Projection methods}
Many methods for encoding hard equality constraints utilize projection techniques, which correct preliminary neural network predictions by appending a non-trainable layer to the output. Projections can be formulated as optimization problems (e.g., distance minimization) or derived from geometric principles. For example, in~\cite{Chen2021_Theoryguidedhard} NN predictions of physical systems governed by PDEs are projected to ensure solutions satisfy the finite difference discretization of the underlying linear PDEs. A more general approach is the KKT-hPINN~\cite{Chen2024_PhysicsInformedNeural}, which enforces linear equality constraints of the form $Ax+By=b$, though its applicability is limited to affine equations. Recently, HardNet~\cite{Min2024_HardConstrainedNeural} was introduced to enforce equality and inequality constraints affine in the output, without input restrictions, via a closed-form projection step. HardNet-Cvx~\cite{Min2024_HardConstrainedNeural} extends this approach to more general convex functions in the inputs, leveraging differentiable optimization layers supported by optimization solvers~\cite{Agrawal2019_DifferentiableConvexOptimization}.

\paragraph{Predict-and-complete}
Alternatively, the neural network can predict a partial set of output variables, $y_P \in \mathbb{R}^{N_O - N_C}$, and complete the prediction by solving the system of constraints based on this partial output. This approach ensures that the constraints are always satisfied. For instance, Beucler et al. introduced this concept to simulate physical systems such as climate modeling~\cite{Beucler2019_EnforcingAnalyticConstraints}. However, when the constraints are not explicitly defined, solving the system requires a root-finding solver. Similar approaches have been proposed within the hybrid modeling community, particularly in the \textit{serial} configuration, where a fully data-driven method is used to predict certain inputs to a mechanistic model~\cite{Schweidtmann2024_reviewperspectivehybrida}. While studies like DC3~\cite{Donti2021_DC3learningmethod} have developed efficient backpropagation techniques, scenarios involving implicit nonlinear constraints can be computationally expensive to tackle with predict-and-complete methods. Moreover, the predict-and-complete approach can reduce regression accuracy by hindering the representation capabilities of NNs~\cite{Beucler2019_EnforcingAnalyticConstraints}.


\paragraph{Constrained optimization}
One of the most popular fields of application of constrained learning is for solving (parametric) constrained optimization problems using NNs~\cite{Kotary2021_EndEndConstrained}. OptNet~\cite{Amos2017_OptNetDifferentiableOptimization} is an optimization layer developed to solve quadratic programs. Angrawal et al.~\cite{Agrawal2019_DifferentiableConvexOptimization} expands the methodology to convex programs. They develop efficient differentiation techniques through such layers. However, the forward pass always requires the solution of a constrained optimization problem. Recently, Mukherjee and Bhattacharyya~\cite{Mukherjee2024_developmentsteadystate} brute-forced the constrained learning paradigm by training a neural network using an NLP solver such as IPOPT~\cite{Waechter2005_implementationinteriorpoint} instead of standard unconstrained optimization algorithms. Also, NNs have been formulated as constrained optimization problems and solved using deterministic global solvers~\cite{Schweidtmann2018_DeterministicGlobalOptimization}. However, these approaches pose severe limitations in terms of NNs and dataset size.


\paragraph{Other methods}
Other methods have been proposed for constrained learning in NNs, mostly considering affine or convex regions. Many of them consider constraints only dependent on the input of the neural network~\cite{Schweidtmann2021_Obeyvaliditylimits, Tordesillas2023_RAYENImpositionHard, Balestriero2022_POLICEProvablyOptimal, Brosowsky2020_SampleSpecificOutput}, others design strategies to include the dependence on both inputs and outputs~\cite{Konstantinov2023_NewComputationallySimple, Lastrucci2025_PicardKKThPINN}. Recently, contributions to enforce general logic and linear constraints have been proposed by the neuro-symbolic AI community, developing constraining layers using logic programming to improve multi-label classification and synthetic data generation~\cite{Giunchiglia2021_MultiLabelClassification, Stoian2024_HowRealisticIs}.\\
However, to the best of our knowledge, there is no method designed to enforce any nonlinear equality constraints involving both the input and the output of the neural network by encoding them in the architecture and without relying on external solvers.