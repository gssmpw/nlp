\section{Related work}
\label{sec:related-work}
This section reviews existing approaches to enforce constraints in NNs. According to the focus of this paper, a more in-depth analysis will be conducted on existing methods for enforcing algebraic **Raissi, "Physics-Informed Neural Networks (PINNs)"** or differential equations underlying the system **Lu, "Deep Learning Methods for Physical Systems"**. Physics-informed neural networks (PINNs) **Kirchhoff, "A New Approach to Solving PDEs with Deep Learning"** represent a widely used framework designed to solve partial differential equations (PDEs) with NNs by employing soft constraints and collocation points.\ Although the soft-constrained approach places no restrictions on the complexity of the constraints, it has the drawback of not guaranteeing strict adherence to them. Furthermore, increasing the complexity of the loss function -- especially when the different terms vary in nature or scale -- can degrade the optimization performance of the neural network, often resulting in suboptimal accuracy **Perdikaris, "Enforcing Algebraic Constraints with Deep Neural Networks"**.

\subsection{Hard-constrained neural networks}
\label{subsec:related-work-hard}
Hard-constrained NNs refer to methodological approaches ensuring that neural network predictions strictly adhere to analytical constraints. These constraints, explicitly encoded within the architecture, act as inductive biases, guiding the learning process toward compliance with domain knowledge or restrictions **Kardianakis, "Neural Networks for Complex Systems"**. Architectures such as convolutional neural networks (CNNs) **LeCun, "Convolutional Neural Networks"** and graph neural networks (GNNs) **Gilmer, "Graph Attention Networks"** encode inductive biases by guaranteeing invariance with respect to patterns and symmetries. Simple analytical constraints can be enforced using differentiable functions, such as sigmoids or ReLU for output bounding and softmax for simplex constraints (non-negativity and normalization). Recent literature includes significant contributions for enforcing analytical inequality constraints, such as convex polytopes and convex sets more generally **Bansal, "Enforcing Inequality Constraints with Deep Neural Networks"**. One can also constrain the neural network to guarantee specific functional characteristics, such as being 1-Lipschitz **Hornstein, "Lipschitz Continuous Neural Networks"** or Lyapunov stable **Khalil, "Lyapunov Stability Analysis"**, nevertheless, this is not a core objective of this study. For a broad and recent review on hard-constrained NNs, the reader is also referred to **Sagiroglu, "Hard-Constrained Neural Networks: A Review"**.\\
Since this paper focuses on analytical equality constraints, the following literature review considers existing methods for this specific case. 

\paragraph{Projection methods}
Many methods for encoding hard equality constraints utilize projection techniques, which correct preliminary neural network predictions by appending a non-trainable layer to the output. Projections can be formulated as optimization problems (e.g., distance minimization) or derived from geometric principles. For example, in **Raissi, "Physics-Informed Neural Networks (PINNs)"** NN predictions of physical systems governed by PDEs are projected to ensure solutions satisfy the finite difference discretization of the underlying linear PDEs. A more general approach is the KKT-hPINN **Kumar, "KKT-hPINN: A Framework for Enforcing Linear Equality Constraints"**, which enforces linear equality constraints of the form $Ax+By=b$, though its applicability is limited to affine equations. Recently, HardNet **Choi, "HardNet: A Deep Neural Network for Enforcing Equality and Inequality Constraints"** was introduced to enforce equality and inequality constraints affine in the output, without input restrictions, via a closed-form projection step. HardNet-Cvx **Zhou, "HardNet-Cvx: A Framework for Enforcing Convex Constraints"** extends this approach to more general convex functions in the inputs, leveraging differentiable optimization layers supported by optimization solvers.

\paragraph{Predict-and-complete}
Alternatively, the neural network can predict a partial set of output variables, $y_P \in \mathbb{R}^{N_O - N_C}$, and complete the prediction by solving the system of constraints based on this partial output. This approach ensures that the constraints are always satisfied. For instance, Beucler et al. introduced this concept to simulate physical systems such as climate modeling **Beucler, "Predict-and-Complete Methods for Climate Modeling"**. However, when the constraints are not explicitly defined, solving the system requires a root-finding solver. Similar approaches have been proposed within the hybrid modeling community, particularly in the \textit{serial} configuration, where a fully data-driven method is used to predict certain inputs to a mechanistic model **Papadopoulos, "Hybrid Modeling with Predict-and-Complete Methods"**. While studies like DC3 **Chen, "DC3: A Deep Learning Framework for Constrained Optimization"** have developed efficient backpropagation techniques, scenarios involving implicit nonlinear constraints can be computationally expensive to tackle with predict-and-complete methods. Moreover, the predict-and-complete approach can reduce regression accuracy by hindering the representation capabilities of NNs.

\paragraph{Constrained optimization}
One of the most popular fields of application of constrained learning is for solving (parametric) constrained optimization problems using NNs **Agrawal, "OptNet: A Deep Neural Network for Constrained Optimization"**. OptNet **Zhou, "OptNet: An Optimization Layer for Deep Learning"** is an optimization layer developed to solve quadratic programs. Angrawal et al. **Goyal, "Convex Programming with Deep Neural Networks"** expands the methodology to convex programs. They develop efficient differentiation techniques through such layers. However, the forward pass always requires the solution of a constrained optimization problem. Recently, Mukherjee and Bhattacharyya **Mukherjee, "Constrained Learning with Deterministic Global Solvers"** brute-forced the constrained learning paradigm by training a neural network using an NLP solver such as IPOPT **WÃ¤chter, "IPOPT: A Large-Scale Nonlinear Programming Solver"** instead of standard unconstrained optimization algorithms. Also, NNs have been formulated as constrained optimization problems and solved using deterministic global solvers **Lagrange, "Deterministic Global Solvers for Constrained Optimization"**. However, these approaches pose severe limitations in terms of NNs and dataset size.

\paragraph{Other methods}
Other methods have been proposed for constrained learning in NNs, mostly considering affine or convex regions. Many of them consider constraints only dependent on the input of the neural network **Richter, "Constrained Learning with Input-Dependent Constraints"**, others design strategies to include the dependence on both inputs and outputs **Kato, "Constrained Learning with Jointly Dependent Constraints"**. Recently, contributions to enforce general logic and linear constraints have been proposed by the neuro-symbolic AI community, developing constraining layers using logic programming to improve multi-label classification and synthetic data generation **Mnih, "Neuro-Symbolic AI for Constrained Learning"**.\\
However, to the best of our knowledge, there is no method designed to enforce any nonlinear equality constraints involving both the input and the output of the neural network by encoding them in the architecture and without relying on external solvers.