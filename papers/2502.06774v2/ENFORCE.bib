@misc{tensorflow2015-whitepaper,
title={{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={http://tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dan~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@misc{paszke2019pytorchimperativestylehighperformance,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      year={2019},
      eprint={1912.01703},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.01703}, 
}

@misc{jax2018_github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/jax-ml/jax},
  version = {0.3.13},
  year = {2018},
}

@Article{Raissi2019_Physicsinformedneural,
  author     = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
  journal    = {Journal of Computational Physics},
  title      = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  year       = {2019},
  month      = {feb},
  pages      = {686--707},
  volume     = {378},
  comment    = {Constrained included as penalty term in the loss function using automatic differentiation},
  doi        = {https://doi.org/10.1016/j.jcp.2018.10.045},
  file       = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/PINNs/Physics-informed neural networks A deep learning...pdf:PDF},
  groups     = {PINNs, 00-ESCAPE34, 00-Paper: Hard constrained NN},
  publisher  = {Elsevier {BV}},
  readstatus = {read},
}

@Article{Karniadakis2021_Physicsinformedmachine,
  author     = {George Em Karniadakis and Ioannis G. Kevrekidis and Lu Lu and Paris Perdikaris and Sifan Wang and Liu Yang},
  journal    = {Nature Reviews Physics},
  title      = {Physics-informed machine learning},
  year       = {2021},
  month      = {may},
  number     = {6},
  pages      = {422--440},
  volume     = {3},
  doi        = {10.1038/s42254-021-00314-5},
  file       = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/PINNs/Physics-informed machine learning.pdf:PDF},
  groups     = {Review, 00-Paper: Hard constrained NN},
  publisher  = {Springer Science and Business Media {LLC}},
  ranking    = {rank4},
  readstatus = {read},
}

@Article{Beucler2019_EnforcingAnalyticConstraints,
  author     = {Beucler, Tom and Pritchard, Michael and Rasp, Stephan and Ott, Jordan and Baldi, Pierre and Gentine, Pierre},
  journal    = {Physical Review Letters},
  title      = {Enforcing Analytic Constraints in Neural-Networks Emulating Physical Systems},
  year       = {2019},
  issn       = {1079-7114},
  month      = mar,
  number     = {9},
  pages      = {098302},
  volume     = {126},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  doi        = {https://doi.org/10.1103/PhysRevLett.126.098302},
  file       = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/PINNs/Enforcing Analytic Constraints in Neural-Networks Emulating Physical Systems.pdf:PDF},
  groups     = {PINNs, 00-ESCAPE35, 00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords   = {Computational Physics (physics.comp-ph), Atmospheric and Oceanic Physics (physics.ao-ph), FOS: Physical sciences, FOS: Physical sciences},
  publisher  = {arXiv},
  readstatus = {read},
}


@Article{Chen2024_PhysicsInformedNeural,
  author    = {Chen, Hao and Flores, Gonzalo E. Constante and Li, Can},
  journal   = {Computers \&  Chemical Engineering},
  title     = {Physics-Informed Neural Networks with Hard Linear Equality Constraints},
  year      = {2024},
  issn      = {0098-1354},
  month     = oct,
  pages     = {108764},
  volume    = {189},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.1016/j.compchemeng.2024.108764},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/PINNs/Physics-Informed Neural Networks with Hard Linear Equality Constraints.pdf:PDF},
  groups    = {PINNs, 00-ESCAPE35, 00-Paper: Hard constrained NN},
  keywords  = {Machine Learning (cs.LG), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}

@Article{Fioretto2020_LagrangianDualityConstrained,
  author    = {Fioretto, Ferdinando and Van Hentenryck, Pascal and Mak, Terrence WK and Tran, Cuong and Baldo, Federico and Lombardi, Michele},
  title     = {Lagrangian Duality for Constrained Deep Learning},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2001.09394},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/PINNs/Lagrangian Duality for Constrained Deep Learning.pdf:PDF},
  groups    = {PINNs, 00-ESCAPE35, 00-Paper: Hard constrained NN},
  keywords  = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Amos2017_OptNetDifferentiableOptimization,
  author    = {Amos, Brandon and Kolter, J. Zico},
  journal   = {Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017},
  title     = {OptNet: Differentiable Optimization as a Layer in Neural Networks},
  year      = {2017},
  comment   = {They develop differentiable optimization layers for quadratic problems (quadratic objective and linear eq/ineq). The main contribution is a computationally cheap wayof obtaining the gradients in the backward pass. However, they have to solve a constrained convex optimization problem at every forward pass (they develop a batched interior point method).},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.1703.00443},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/Constrained optimization learning/OptNet Differentiable Optimization As a Layer in Neural Networks.pdf:PDF},
  groups    = {Constrained optimization learning, 00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}


@Article{Donti2021_DC3learningmethod,
  author    = {Donti, Priya L. and Rolnick, David and Kolter, J. Zico},
  journal   = {International Conference on Learning Representations},
  title     = {DC3: A learning method for optimization with hard constraints},
  year      = {2021},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {https://doi.org/10.48550/arXiv.2104.12225},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/Constrained optimization learning/DC3 A learning method for optimization with hard constraints.pdf:PDF},
  groups    = {Constrained optimization learning, 00-ESCAPE3, 00-ESCAPE35, 00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}


@Article{Pineda2022_TheseusLibraryDifferentiable,
  author    = {Pineda, Luis and Fan, Taosha and Monge, Maurizio and Venkataraman, Shobha and Sodhi, Paloma and Chen, Ricky T. Q. and Ortiz, Joseph and DeTone, Daniel and Wang, Austin and Anderson, Stuart and Dong, Jing and Amos, Brandon and Mukadam, Mustafa},
  title     = {Theseus: A Library for Differentiable Nonlinear Optimization},
  year      = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2207.09442},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/Constrained optimization learning/Theseus A library for differentiable nonlinear optimization.pdf:PDF},
  groups    = {Constrained optimization learning, 00-Paper: Hard constrained NN},
  keywords  = {Robotics (cs.RO), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}

@Article{Nellikkath2021_PhysicsInformedNeural,
  author    = {Nellikkath, Rahul and Chatzivasileiadis, Spyros},
  title     = {Physics-Informed Neural Networks for Minimising Worst-Case Violations in DC Optimal Power Flow},
  year      = {2021},
  copyright = {Creative Commons Attribution Share Alike 4.0 International},
  doi       = {https://doi.org/10.48550/arXiv.2107.00465},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/PINNs/Physics-Informed Neural Networks for Minimising Worst-Case Violations in DC Optimal Power Flow.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN},
  keywords  = {Systems and Control (eess.SY), Machine Learning (cs.LG), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Mukherjee2024_developmentsteadystate,
  author    = {Mukherjee, Angan and Bhattacharyya, Debangsu},
  journal   = {Computers \&  Chemical Engineering},
  title     = {On the development of steady-state and dynamic mass-constrained neural networks using noisy transient data},
  year      = {2024},
  issn      = {0098-1354},
  month     = aug,
  pages     = {108722},
  volume    = {187},
  comment   = {Only mass constrained!!},
  doi       = {https://doi.org/10.1016/j.compchemeng.2024.108722},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/PINNs/On the development of steady-state and dynamic mass-constrained neural networks using noisy transient data.pdf:PDF},
  groups    = {00-ESCAPE35, 00-Paper: Hard constrained NN},
  publisher = {Elsevier BV},
}

@Article{Pokharel2023_Effectsmassbalance,
  author    = {Pokharel, Sudan and Roy, Tirthankar and Admiraal, David},
  journal   = {Environmental Modelling \& amp; Software},
  title     = {Effects of mass balance, energy balance, and storage-discharge constraints on LSTM for streamflow prediction},
  year      = {2023},
  issn      = {1364-8152},
  month     = aug,
  pages     = {105730},
  volume    = {166},
  comment   = {Constrained included as penalty term in the loss function (no differentials, though)},
  doi       = {https://doi.org/10.1016/j.envsoft.2023.105730},
  groups    = {00-Paper: Hard constrained NN},
  publisher = {Elsevier BV},
}

@Article{Ullah2024_Physicsinformedneural,
  author    = {Ullah, Arif and Huang, Yu and Yang, Ming and Dral, Pavlo O.},
  journal   = {Digital Discovery},
  title     = {Physics-informed neural networks and beyond: enforcing physical constraints in quantum dissipative dynamics},
  year      = {2024},
  issn      = {2635-098X},
  doi       = {https://doi.org/10.1039/D4DD00153B},
  groups    = {00-Paper: Hard constrained NN},
  publisher = {Royal Society of Chemistry (RSC)},
}

@Article{Mueller2023_Exactconservationlaws,
  author    = {Müller, Eike Hermann},
  journal   = {Journal of Computational Physics},
  title     = {Exact conservation laws for neural network integrators of dynamical systems},
  year      = {2023},
  issn      = {0021-9991},
  month     = sep,
  pages     = {112234},
  volume    = {488},
  doi       = {https://doi.org/10.1016/j.jcp.2023.112234},
  groups    = {00-Paper: Hard constrained NN},
  publisher = {Elsevier BV},
}

@Article{Beucler2019_AchievingConservationEnergy,
  author    = {Beucler, Tom and Rasp, Stephan and Pritchard, Michael and Gentine, Pierre},
  title     = {Achieving Conservation of Energy in Neural Network Emulators for Climate Modeling},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.1906.06622},
  groups    = {00-Paper: Hard constrained NN},
  keywords  = {Atmospheric and Oceanic Physics (physics.ao-ph), Machine Learning (cs.LG), Computational Physics (physics.comp-ph), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Jagtap2020_Conservativephysicsinformed,
  author    = {Jagtap, Ameya D. and Kharazmi, Ehsan and Karniadakis, George Em},
  journal   = {Computer Methods in Applied Mechanics and Engineering},
  title     = {Conservative physics-informed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems},
  year      = {2020},
  issn      = {0045-7825},
  month     = jun,
  pages     = {113028},
  volume    = {365},
  doi       = {https://doi.org/10.1016/j.cma.2020.113028},
  groups    = {00-Paper: Hard constrained NN},
  publisher = {Elsevier BV},
}

@Article{Sturm2022_Conservationlawsneural,
  author    = {Sturm, Patrick Obin and Wexler, Anthony S.},
  journal   = {Geoscientific Model Development},
  title     = {Conservation laws in a neural network architecture: enforcing the atom balance of a Julia-based photochemical model (v0.2.0)},
  year      = {2022},
  issn      = {1991-9603},
  month     = apr,
  number    = {8},
  pages     = {3417--3431},
  volume    = {15},
  doi       = {https://doi.org/10.5194/gmd-15-3417-2022},
  groups    = {00-Paper: Hard constrained NN},
  publisher = {Copernicus GmbH},
}

@InProceedings{RichterPowell2022_NeuralConservationLaws,
  author    = {Richter-Powell, Jack and Lipman, Yaron and Chen, Ricky T. Q.},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Neural Conservation Laws: A Divergence-Free Perspective},
  year      = {2022},
  editor    = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
  pages     = {38075--38088},
  publisher = {Curran Associates, Inc.},
  volume    = {35},
  groups    = {00-Paper: Hard constrained NN},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/f8d39584f87944e5dbe46ec76f19e20a-Paper-Conference.pdf},
}

@Article{Fang2022_Datadrivensoliton,
  author    = {Fang, Yin and Wu, Gang-Zhou and Kudryashov, Nikolay A. and Wang, Yue-Yue and Dai, Chao-Qing},
  journal   = {Chaos, Solitons \& amp; Fractals},
  title     = {Data-driven soliton solutions and model parameters of nonlinear wave models via the conservation-law constrained neural network method},
  year      = {2022},
  issn      = {0960-0779},
  month     = may,
  pages     = {112118},
  volume    = {158},
  doi       = {https://doi.org/10.1016/j.chaos.2022.112118},
  groups    = {00-Paper: Hard constrained NN},
  publisher = {Elsevier BV},
}

@Article{Ozan2023_Hardconstrainedneural,
  author    = {Ozan, Defne E. and Magri, Luca},
  journal   = {Physical Review Fluids},
  title     = {Hard-constrained neural networks for modeling nonlinear acoustics},
  year      = {2023},
  issn      = {2469-990X},
  month     = oct,
  number    = {10},
  pages     = {103201},
  volume    = {8},
  comment   = {Very field-specific (they have periodicity propery in their phenomena). They both do soft and hard constraint. They do hard constraint using Galerkin neural networks. They claim to do interpolation from sparse and partial sensor data, extrapolation in time, and to require smaller architectures.},
  doi       = {https://doi.org/10.1103/PhysRevFluids.8.103201},
  groups    = {00-Paper: Hard constrained NN},
  publisher = {American Physical Society (APS)},
}

@Article{Konstantinov2023_NewComputationallySimple,
  author    = {Konstantinov, A. V. and Utkin, L. V.},
  journal   = {Doklady Mathematics},
  title     = {A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints},
  year      = {2023},
  issn      = {1531-8362},
  month     = dec,
  number    = {S2},
  pages     = {S233--S241},
  volume    = {108},
  comment   = {Very mathematical paper. They claim to do hard constraint with CONVEX constraint. They demonstrate with linear and quadratic constraints. They claim that the method is very computationally cheap. Apparently, they mainly talk about INEQUALITY constraints. Then they achieve equalities by using auxiliary variables.},
  doi       = {https://doi.org/10.1134/S1064562423701077},
  groups    = {00-Paper: Hard constrained NN},
  publisher = {Pleiades Publishing Ltd},
}

@Article{Frerix2020_HomogeneousLinearInequality,
  author    = {Frerix, Thomas and Niesner, Matthias and Cremers, Daniel},
  journal   = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  title     = {Homogeneous Linear Inequality Constraints for Neural Network Activations},
  year      = {2020},
  month     = jun,
  pages     = {3229--3234},
  volume    = {521},
  comment   = {They enforce linear INEQUALITY constraints of the form Ax>=b.
They have some interesting perspective on constrained generative models (like variational autoencoders).},
  doi       = {10.1109/cvprw50498.2020.00382},
  file      = {:Frerix_2020 - Homogeneous Linear Inequality Constraints for Neural Network Activations.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  publisher = {IEEE},
}


@Article{Mao2022_PhyTaylorPhysics,
  author    = {Mao, Yanbing and Sha, Lui and Shao, Huajie and Gu, Yuliang and Wang, Qixin and Abdelzaher, Tarek},
  title     = {Phy-Taylor: Physics-Model-Based Deep Neural Networks},
  year      = {2022},
  comment   = {They also use taylor, but to create augmented layers. Also, they do correction, but it is something else related to control. However, check.},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {https://doi.org/10.48550/arXiv.2209.13511},
  file      = {:Mao2022_PhyTaylorPhysics - Phy Taylor_ Physics Model Based Deep Neural Networks.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Chen2021_Theoryguidedhard,
  author    = {Chen, Yuntian and Huang, Dou and Zhang, Dongxiao and Zeng, Junsheng and Wang, Nanzhe and Zhang, Haoran and Yan, Jinyue},
  journal   = {Journal of Computational Physics},
  title     = {Theory-guided hard constraint projection (HCP): A knowledge-based data-driven scientific machine learning method},
  year      = {2021},
  issn      = {0021-9991},
  month     = nov,
  pages     = {110624},
  volume    = {445},
  comment   = {Very interesting, they use same concept as KKT-hPINN but on differential equations (discretized). They use projection. They claim that is exacteven if they satisfy a discretized version (that is, an approximation by definition). However, they can sample MORE collocation points and CONTROL the approximation degree (smaller delta). This is not doable for our taylor method.},
  doi       = {https://doi.org/10.1016/j.jcp.2021.110624},
  groups    = {00-Paper: Hard constrained NN},
  publisher = {Elsevier BV},
}

@Article{Cheng2024_ResidualEnhancedPhysics,
  author    = {Cheng, Haibo and He, Yunpeng and Zeng, Peng and Vyatkin, Valeriy},
  journal   = {IEEE Transactions on Geoscience and Remote Sensing},
  title     = {Residual-Enhanced Physics-Guided Machine Learning With Hard Constraints for Subsurface Flow in Reservoir Engineering},
  year      = {2024},
  issn      = {1558-0644},
  pages     = {1--9},
  volume    = {62},
  comment   = {Just skimmed, I think they use the same method as Chen et al. https://doi.org/10.1016/j.jcp.2021.110624},
  doi       = {https://doi.org/10.1109/TGRS.2024.3357797},
  groups    = {00-Paper: Hard constrained NN},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{Lee2023_Nonlinearconstraintsatisfaction,
  author     = {Lee, Jaemoon and Rangarajan, Anand and Ranka, Sanjay},
  title      = {Nonlinear constraint satisfaction for compressive autoencoders using instance-specific linear operators},
  year       = {2023},
  month      = aug,
  pages      = {562--571},
  booktitle  = {Proceedings of the 2023 Fifteenth International Conference on Contemporary Computing},
  collection = {IC3 2023},
  comment    = {They do constraints in image compression. Didn't understand exactly how, but they claim to transform nonlinear constraints into instance-specific linear constraints, and then they do projection as usual. However, I think that they express the constraint itself through a NN and then they kind of reformulate it linearly.},
  doi        = {https://doi.org/10.1145/3607947.3611644},
  groups     = {00-Paper: Hard constrained NN},
  publisher  = {ACM},
  series     = {IC3 2023},
}

@Article{Kahrs2007_validitydomainhybrid,
  author    = {Kahrs, O. and Marquardt, W.},
  journal   = {Chemical Engineering and Processing: Process Intensification},
  title     = {The validity domain of hybrid models and its application in process optimization},
  year      = {2007},
  issn      = {0255-2701},
  month     = nov,
  number    = {11},
  pages     = {1054--1066},
  volume    = {46},
  doi       = {https://doi.org/10.1016/j.cep.2007.02.031},
  groups    = {00-Paper: Hard constrained NN},
  publisher = {Elsevier BV},
}

@Article{Schweidtmann2021_Obeyvaliditylimits,
  author    = {Schweidtmann, Artur M. and Weber, Jana M. and Wende, Christian and Netze, Linus and Mitsos, Alexander},
  journal   = {Optimization and Engineering},
  title     = {Obey validity limits of data-driven models through topological data analysis and one-class classification},
  year      = {2021},
  issn      = {1573-2924},
  month     = may,
  number    = {2},
  pages     = {855--876},
  volume    = {23},
  doi       = {https://doi.org/10.1007/s11081-021-09608-0},
  groups    = {00-Paper: Hard constrained NN},
  publisher = {Springer Science and Business Media LLC},
}

@Article{Wang2024_LinSATNetPositiveLinear,
  author    = {Wang, Runzhong and Zhang, Yunhao and Guo, Ziao and Chen, Tianyi and Yang, Xiaokang and Yan, Junchi},
  journal   = {In Proceedings of the 40th International Conference on Machine Learning (ICML'23)},
  title     = {LinSATNet: The Positive Linear Satisfiability Neural Networks},
  year      = {2024},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {https://doi.org/10.48550/arXiv.2407.13917},
  file      = {:Wang2024_LinSATNetPositiveLinear - LinSATNet_ the Positive Linear Satisfiability Neural Networks.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Artificial Intelligence (cs.AI), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}


@Article{Kotary2021_EndEndConstrained,
  author    = {Kotary, James and Fioretto, Ferdinando and Van Hentenryck, Pascal and Wilder, Bryan},
  journal   = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI-21)},
  title     = {End-to-End Constrained Optimization Learning: A Survey},
  year      = {2021},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {https://doi.org/10.48550/arXiv.2103.16378},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}


@Article{Ferber2022_SurCoLearningLineara,
  author    = {Ferber, Aaron and Huang, Taoan and Zha, Daochen and Schubert, Martin and Steiner, Benoit and Dilkina, Bistra and Tian, Yuandong},
  title     = {SurCo: Learning Linear Surrogates For Combinatorial Nonlinear Optimization Problems},
  year      = {2022},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {https://doi.org/10.48550/arXiv.2210.12547},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/Constrained optimization learning/SurCo Learning Linear SURrogates for COmbinatorial nonlinear optimization problems.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}

@Article{Anil2018_SortingoutLipschitz,
  author    = {Anil, Cem and Lucas, James and Grosse, Roger},
  journal   = {Proceedings of the 36th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019},
  title     = {Sorting out Lipschitz function approximation},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.1811.05381},
  file      = {:https___doi.org_10.48550_arxiv.1811.05381 - Sorting Out Lipschitz Function Approximation.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}


@Article{Prach2023_1LipschitzNeural,
  author    = {Prach, Bernd and Lampert, Christoph H.},
  title     = {1-Lipschitz Neural Networks are more expressive with N-Activations},
  year      = {2023},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {https://doi.org/10.48550/arXiv.2311.06103},
  groups    = {00-Paper: Hard constrained NN},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Tan2024_Robustmachinelearning,
  author    = {Tan, Wallace Gian Yion and Wu, Zhe},
  journal   = {Computers \&  Chemical Engineering},
  title     = {Robust machine learning modeling for predictive control using Lipschitz-Constrained Neural Networks},
  year      = {2024},
  issn      = {0098-1354},
  month     = jan,
  pages     = {108466},
  volume    = {180},
  doi       = {https://doi.org/10.1016/j.compchemeng.2023.108466},
  groups    = {00-Paper: Hard constrained NN},
  publisher = {Elsevier BV},
}

@Article{GianYion2024_MachineLearningModeling,
  author    = {Gian Yion, Wallace Tan and Xiao, Ming and Wu, Guoquan and Wu, Zhe},
  title     = {Machine Learning Modeling of Nonlinear Processes with Lyapunov Stability Guarantees},
  year      = {2024},
  month     = jul,
  pages     = {528--535},
  booktitle = {2024 American Control Conference (ACC)},
  doi       = {https://doi.org/10.23919/ACC60939.2024.10644912},
  groups    = {00-Paper: Hard constrained NN},
  publisher = {IEEE},
}

@Article{Wang2019_SATNetBridgingdeep,
  author    = {Wang, Po-Wei and Donti, Priya L. and Wilder, Bryan and Kolter, Zico},
  title     = {SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.1905.12149},
  groups    = {00-Paper: Hard constrained NN},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Agrawal2019_DifferentiableConvexOptimization,
  author    = {Agrawal, Akshay and Amos, Brandon and Barratt, Shane and Boyd, Stephen and Diamond, Steven and Kolter, Zico},
  journal   = {33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada},
  title     = {Differentiable Convex Optimization Layers},
  year      = {2019},
  comment   = {Extension to OptNet (Amos and Kolter, 2017). Please see comments there. Here they basically expand to any convex program (not only quadratic).},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.1910.12430},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/Constrained NN/Differentiable COnvex Optimization Layer.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}


@Article{Yang2020_LearningPhysicalConstraints,
  author    = {Yang, Shuqi and He, Xingzhe and Zhu, Bo},
  journal   = {Advances in Neural Information Processing Systems 33 (NeurIPS 2020)},
  title     = {Learning Physical Constraints with Neural Projections},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2006.12745},
  file      = {:https___doi.org_10.48550_arxiv.2006.12745 - Learning Physical Constraints with Neural Projections.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Neural and Evolutionary Computing (cs.NE), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}


@Article{Min2024_HardConstrainedNeural,
  author    = {Min, Youngjae and Sonar, Anoopkumar and Azizan, Navid},
  journal   = {ArXiv preprint},
  title     = {Hard-Constrained Neural Networks with Universal Approximation Guarantees},
  year      = {2024},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {https://doi.org/10.48550/arXiv.2410.10807},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/Constrained NN/HardNet.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}


@Article{Balestriero2022_POLICEProvablyOptimal,
  author    = {Balestriero, Randall and LeCun, Yann},
  journal   = {arXiv preprint},
  title     = {POLICE: Provably Optimal Linear Constraint Enforcement for Deep Neural Networks},
  year      = {2022},
  copyright = {Creative Commons Attribution 4.0 International},
  doi       = {https://doi.org/10.48550/arXiv.2211.01340},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/Constrained NN/POLICE.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}


@Article{Tordesillas2023_RAYENImpositionHard,
  author    = {Tordesillas, Jesus and How, Jonathan P. and Hutter, Marco},
  journal   = {ArXiv preprint},
  title     = {RAYEN: Imposition of Hard Convex Constraints on Neural Networks},
  year      = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2307.08336},
  file      = {:Tordesillas2023_RAYENImpositionHard - RAYEN_ Imposition of Hard Convex Constraints on Neural Networks.pdf:PDF},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}


@Article{Gupta2021_Deeplearningobject,
  author    = {Gupta, Abhishek and Anpalagan, Alagan and Guan, Ling and Khwaja, Ahmed Shaharyar},
  journal   = {Array},
  title     = {Deep learning for object detection and scene perception in self-driving cars: Survey, challenges, and open issues},
  year      = {2021},
  issn      = {2590-0056},
  month     = jul,
  pages     = {100057},
  volume    = {10},
  doi       = {https://doi.org/10.1016/j.array.2021.100057},
  groups    = {00-Paper-ENFORCE},
  publisher = {Elsevier BV},
}

@Article{Cybenko1989_Approximationsuperpositionssigmoidal,
  author    = {Cybenko, G.},
  journal   = {Mathematics of Control, Signals, and Systems},
  title     = {Approximation by superpositions of a sigmoidal function},
  year      = {1989},
  issn      = {1435-568X},
  month     = dec,
  number    = {4},
  pages     = {303--314},
  volume    = {2},
  doi       = {https://doi.org/10.1007/BF02551274},
  groups    = {00-Paper-ENFORCE},
  publisher = {Springer Science and Business Media LLC},
}

@Article{Hornik1989_Multilayerfeedforwardnetworks,
  author    = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal   = {Neural Networks},
  title     = {Multilayer feedforward networks are universal approximators},
  year      = {1989},
  issn      = {0893-6080},
  month     = jan,
  number    = {5},
  pages     = {359--366},
  volume    = {2},
  doi       = {https://doi.org/10.1016/0893-6080(89)90020-8},
  groups    = {00-Paper-ENFORCE},
  publisher = {Elsevier BV},
}

@Article{Rahaman2018_SpectralBiasNeural,
  author    = {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred A. and Bengio, Yoshua and Courville, Aaron},
  journal   = {ArXiv},
  title     = {On the Spectral Bias of Neural Networks},
  year      = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.1806.08734},
  groups    = {00-Paper-ENFORCE},
  keywords  = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}


@Article{Wang2021_Learningsolutionoperator,
  author    = {Wang, Sifan and Wang, Hanwen and Perdikaris, Paris},
  title     = {Learning the solution operator of parametric partial differential equations with physics-informed DeepOnets},
  year      = {2021},
  journal   = {ArXiv},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2103.10974},
  groups    = {00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Numerical Analysis (math.NA), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}

@Article{Bronstein2017_GeometricDeepLearning,
  author    = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  journal   = {IEEE Signal Processing Magazine},
  title     = {Geometric Deep Learning: Going beyond Euclidean data},
  year      = {2017},
  issn      = {1558-0792},
  month     = jul,
  number    = {4},
  pages     = {18--42},
  volume    = {34},
  doi       = {https://doi.org/10.1109/MSP.2017.2693418},
  groups    = {00-Paper-ENFORCE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{Wu2021_ComprehensiveSurveyGraph,
  author    = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  title     = {A Comprehensive Survey on Graph Neural Networks},
  year      = {2021},
  issn      = {2162-2388},
  month     = jan,
  number    = {1},
  pages     = {4--24},
  volume    = {32},
  doi       = {https://doi.org/10.1109/TNNLS.2020.2978386},
  groups    = {00-Paper-ENFORCE},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{LeCun1989_BackpropagationAppliedHandwritten,
  author    = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  journal   = {Neural Computation},
  title     = {Backpropagation Applied to Handwritten Zip Code Recognition},
  year      = {1989},
  issn      = {1530-888X},
  month     = dec,
  number    = {4},
  pages     = {541--551},
  volume    = {1},
  doi       = {https://doi.org/10.1162/neco.1989.1.4.541},
  groups    = {00-Paper-ENFORCE},
  publisher = {MIT Press},
}


@Article{Wang2020_WhenwhyPINNs,
  author    = {Wang, Sifan and Yu, Xinling and Perdikaris, Paris},
  journal   = {ArXiv},
  title     = {When and why PINNs fail to train: A neural tangent kernel perspective},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2007.14527},
  file      = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/PINNs/When and why PINNs fail to train A neural tangent kernel.pdf:PDF},
  groups    = {PINNs, 00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Numerical Analysis (math.NA), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}

@Article{Wang2020_Understandingmitigatinggradient,
  author     = {Wang, Sifan and Teng, Yujun and Perdikaris, Paris},
  journal   = {ArXiv},
  title      = {Understanding and mitigating gradient pathologies in physics-informed neural networks},
  year       = {2020},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  doi        = {https://doi.org/10.48550/arXiv.2001.04536},
  file       = {:C\:/Users/glastrucci/OneDrive - Delft University of Technology/Università/TUDelft - PhD/Literature/PINNs/Understanding and Mitigating Gradient Pathologies in PINNs.pdf:PDF},
  groups     = {PINNs, 00-ESCAPE34, 00-Paper-ENFORCE},
  keywords   = {Machine Learning (cs.LG), Numerical Analysis (math.NA), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher  = {arXiv},
  ranking    = {rank5},
  readstatus = {read},
}

@Article{Brosowsky2020_SampleSpecificOutput,
  author    = {Brosowsky, Mathis and Dünkel, Olaf and Slieter, Daniel and Zöllner, Marius},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence, 35(8), 6812-6821},
  title     = {Sample-Specific Output Constraints for Neural Networks},
  year      = {2020},
  comment   = {They do constraint (convex polytopes, a.k.a., linear) on the output. So, they are essentially inequality constraints. They need additional inputs to the NN to enforce the constraints. The additional input is some parameter characterizing the constraints.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2003.10258},
  groups    = {00-Paper: Hard constrained NN, 00-Paper-ENFORCE},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}


@Article{Waechter2005_implementationinteriorpoint,
  author    = {Wächter, Andreas and Biegler, Lorenz T.},
  journal   = {Mathematical Programming},
  title     = {On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming},
  year      = {2005},
  issn      = {1436-4646},
  month     = apr,
  number    = {1},
  pages     = {25--57},
  volume    = {106},
  doi       = {10.1007/s10107-004-0559-y},
  groups    = {00-Paper-ENFORCE},
  publisher = {Springer Science and Business Media LLC},
}

@Article{Manek2020_LearningStableDeepa,
  author    = {Manek, Gaurav and Kolter, J. Zico},
  journal   = {33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada},
  title     = {Learning Stable Deep Dynamics Models},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2001.06116},
  groups    = {00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Dynamical Systems (math.DS), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  publisher = {arXiv},
}


@Article{Xu2021_Artificialintelligencepowerful,
  author    = {Xu, Yongjun and Liu, Xin and Cao, Xin and Huang, Changping and Liu, Enke and Qian, Sen and Liu, Xingchen and Wu, Yanjun and Dong, Fengliang and Qiu, Cheng-Wei and Qiu, Junjun and Hua, Keqin and Su, Wentao and Wu, Jian and Xu, Huiyu and Han, Yong and Fu, Chenguang and Yin, Zhigang and Liu, Miao and Roepman, Ronald and Dietmann, Sabine and Virta, Marko and Kengara, Fredrick and Zhang, Ze and Zhang, Lifu and Zhao, Taolan and Dai, Ji and Yang, Jialiang and Lan, Liang and Luo, Ming and Liu, Zhaofeng and An, Tao and Zhang, Bin and He, Xiao and Cong, Shan and Liu, Xiaohong and Zhang, Wei and Lewis, James P. and Tiedje, James M. and Wang, Qi and An, Zhulin and Wang, Fei and Zhang, Libo and Huang, Tao and Lu, Chuan and Cai, Zhipeng and Wang, Fang and Zhang, Jiabao},
  journal   = {The Innovation},
  title     = {Artificial intelligence: A powerful paradigm for scientific research},
  year      = {2021},
  issn      = {2666-6758},
  month     = nov,
  number    = {4},
  pages     = {100179},
  volume    = {2},
  doi       = {https://doi.org/10.1016/j.xinn.2021.100179},
  groups    = {00-Paper-ENFORCE},
  publisher = {Elsevier BV},
}

@Article{Feuerriegel2020_FairAIChallenges,
  author    = {Feuerriegel, Stefan and Dolata, Mateusz and Schwabe, Gerhard},
  journal   = {Business \& Information Systems Engineering},
  title     = {Fair AI: Challenges and Opportunities},
  year      = {2020},
  issn      = {1867-0202},
  month     = may,
  number    = {4},
  pages     = {379--384},
  volume    = {62},
  doi       = {https://doi.org/10.1007/s12599-020-00650-3},
  groups    = {00-Paper-ENFORCE},
  publisher = {Springer Science and Business Media LLC},
}

@Article{Hardt2016_EqualityOpportunitySupervised,
  author    = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
  journal   = {30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.},
  title     = {Equality of Opportunity in Supervised Learning},
  year      = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.1610.02413},
  groups    = {00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}



@Article{Wang2023_Scientificdiscoveryage,
  author    = {Wang, Hanchen and Fu, Tianfan and Du, Yuanqi and Gao, Wenhao and Huang, Kexin and Liu, Ziming and Chandak, Payal and Liu, Shengchao and Van Katwyk, Peter and Deac, Andreea and Anandkumar, Anima and Bergen, Karianne and Gomes, Carla P. and Ho, Shirley and Kohli, Pushmeet and Lasenby, Joan and Leskovec, Jure and Liu, Tie-Yan and Manrai, Arjun and Marks, Debora and Ramsundar, Bharath and Song, Le and Sun, Jimeng and Tang, Jian and Veličković, Petar and Welling, Max and Zhang, Linfeng and Coley, Connor W. and Bengio, Yoshua and Zitnik, Marinka},
  journal   = {Nature},
  title     = {Scientific discovery in the age of artificial intelligence},
  year      = {2023},
  issn      = {1476-4687},
  month     = aug,
  number    = {7972},
  pages     = {47--60},
  volume    = {620},
  doi       = {https://doi.org/10.1038/s41586-023-06221-2},
  groups    = {00-Paper-ENFORCE},
  publisher = {Springer Science and Business Media LLC},
}

@Article{Schweidtmann2018_DeterministicGlobalOptimization,
  author    = {Schweidtmann, Artur M. and Mitsos, Alexander},
  journal   = {Journal of Optimization Theory and Applications},
  title     = {Deterministic Global Optimization with Artificial Neural Networks Embedded},
  year      = {2018},
  issn      = {1573-2878},
  month     = oct,
  number    = {3},
  pages     = {925--948},
  volume    = {180},
  doi       = {https://doi.org/10.1007/s10957-018-1396-0},
  groups    = {00-Paper-ENFORCE},
  publisher = {Springer Science and Business Media LLC},
}

@Article{Mize2019_BestPracticesEstimating,
  author    = {Mize, Trenton},
  journal   = {Sociological Science},
  title     = {Best Practices for Estimating, Interpreting, and Presenting Nonlinear Interaction Effects},
  year      = {2019},
  issn      = {2330-6696},
  pages     = {81--117},
  volume    = {6},
  doi       = {10.15195/v6.a4},
  groups    = {00-Paper-ENFORCE},
  publisher = {Society for Sociological Science},
}

@Book{Nicolis1995_IntroductionNonlinearScience,
  author    = {Nicolis, G.},
  publisher = {Cambridge University Press},
  title     = {Introduction to Nonlinear Science},
  year      = {1995},
  isbn      = {9781139170802},
  month     = jun,
  doi       = {https://doi.org/10.1017/CBO9781139170802},
  groups    = {00-Paper-ENFORCE},
}

@Article{Cao2022_AIFinanceChallenges,
  author    = {Cao, Longbing},
  journal   = {ACM Computing Surveys},
  title     = {AI in Finance: Challenges, Techniques, and Opportunities},
  year      = {2022},
  issn      = {1557-7341},
  month     = feb,
  number    = {3},
  pages     = {1--38},
  volume    = {55},
  doi       = {https://doi.org/10.1145/3502289},
  groups    = {00-Paper-ENFORCE},
  publisher = {Association for Computing Machinery (ACM)},
}

@Article{Gerke2020_Ethicallegalchallenges,
  author    = {Gerke, Sara and Minssen, Timo and Cohen, Glenn},
  journal   = {Artificial Intelligence in Healthcare},
  title     = {Ethical and legal challenges of artificial intelligence-driven healthcare},
  year      = {2020},
  pages     = {295--336},
  booktitle = {Artificial Intelligence in Healthcare},
  doi       = {https://doi.org/10.1016/B978-0-12-818438-7.00012-5},
  groups    = {00-Paper-ENFORCE},
  isbn      = {9780128184387},
  publisher = {Elsevier},
}


@Article{Fletcher2002_Nonlinearprogrammingpenalty,
  author    = {Fletcher, Roger and Leyffer, Sven},
  journal   = {Mathematical Programming},
  title     = {Nonlinear programming without a penalty function},
  year      = {2002},
  issn      = {1436-4646},
  month     = jan,
  number    = {2},
  pages     = {239--269},
  volume    = {91},
  doi       = {https://doi.org/10.1007/s101070100244},
  groups    = {00-Paper-ENFORCE},
  publisher = {Springer Science and Business Media LLC},
}

@Article{Fletcher2002_GlobalConvergenceFilter,
  author    = {Fletcher, Roger and Leyffer, Sven and Toint, Philippe L.},
  journal   = {SIAM Journal on Optimization},
  title     = {On the Global Convergence of a Filter--SQP Algorithm},
  year      = {2002},
  issn      = {1095-7189},
  month     = jan,
  number    = {1},
  pages     = {44--59},
  volume    = {13},
  doi       = {https://doi.org/10.1137/S105262340038081X},
  groups    = {00-Paper-ENFORCE},
  publisher = {Society for Industrial & Applied Mathematics (SIAM)},
}

@article{boyd2003subgradient,
  title={Subgradient methods},
  author={Boyd, Stephen and Xiao, Lin and Mutapcic, Almir},
  journal={lecture notes of EE392o, Stanford University, Autumn Quarter},
  volume={2004},
  number={01},
  year={2003}
}

@Article{Lu2021_PhysicsInformedNeural,
  author    = {Lu, Lu and Pestourie, Raphaël and Yao, Wenjie and Wang, Zhicheng and Verdugo, Francesc and Johnson, Steven G.},
  journal   = {SIAM Journal on Scientific Computing},
  title     = {Physics-Informed Neural Networks with Hard Constraints for Inverse Design},
  year      = {2021},
  issn      = {1095-7197},
  month     = jan,
  number    = {6},
  pages     = {B1105--B1132},
  volume    = {43},
  doi       = {https://doi.org/10.1137/21M1397908},
  publisher = {Society for Industrial & Applied Mathematics (SIAM)},
}

@Article{Engel2017_LatentConstraintsLearning,
  author    = {Engel, Jesse and Hoffman, Matthew and Roberts, Adam},
  title     = {Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.1711.05772},
  groups    = {00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Article{Pfrommer2020_ContactNetsLearningDiscontinuousa,
  author    = {Pfrommer, Samuel and Halm, Mathew and Posa, Michael},
  journal   = {Conference on Robot Learning 2020},
  title     = {ContactNets: Learning Discontinuous Contact Dynamics with Smooth, Implicit Representations},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2009.11193},
  groups    = {00-Paper-ENFORCE},
  keywords  = {Robotics (cs.RO), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}


@Article{Erichson2019_PhysicsinformedAutoencoders,
  author    = {Erichson, N. Benjamin and Muehlebach, Michael and Mahoney, Michael W.},
  journal   = {Second Workshop on Machine Learning and the Physical Sciences (NeurIPS 2019), Vancouver, Canada},
  title     = {Physics-informed Autoencoders for Lyapunov-stable Fluid Flow Prediction},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.1905.10866},
  groups    = {00-Paper-ENFORCE},
  keywords  = {Computational Physics (physics.comp-ph), Machine Learning (cs.LG), FOS: Physical sciences, FOS: Physical sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}


@Article{Schweidtmann2024_reviewperspectivehybrida,
  author    = {Schweidtmann, Artur M. and Zhang, Dongda and von Stosch, Moritz},
  journal   = {Digital Chemical Engineering},
  title     = {A review and perspective on hybrid modeling methodologies},
  year      = {2024},
  issn      = {2772-5081},
  month     = mar,
  pages     = {100136},
  volume    = {10},
  doi       = {https://doi.org/10.1016/j.dche.2023.100136},
  groups    = {00-Paper-ENFORCE},
  publisher = {Elsevier BV},
}

@Article{Kingma2014_AdamMethodStochastic,
  author    = {Kingma, Diederik P. and Ba, Jimmy},
  journal   = {3rd International Conference for Learning Representations, San Diego, 2015},
  title     = {Adam: A Method for Stochastic Optimization},
  year      = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.1412.6980},
  groups    = {00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}


@Article{Stoian2024_HowRealisticIs,
  author    = {Stoian, Mihaela Cătălina and Dyrmishi, Salijona and Cordy, Maxime and Lukasiewicz, Thomas and Giunchiglia, Eleonora},
  journal   = {Published as a conference paper at ICLR 2024},
  title     = {How Realistic Is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data},
  year      = {2024},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2402.04823},
  groups    = {00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}


@Article{Giunchiglia2021_MultiLabelClassification,
  author    = {Giunchiglia, Eleonora and Lukasiewicz, Thomas},
  journal   = {Journal of Artificial Intelligence Research},
  title     = {Multi-Label Classification Neural Networks with Hard Logical Constraints},
  year      = {2021},
  issn      = {1076-9757},
  month     = nov,
  pages     = {759--818},
  volume    = {72},
  doi       = {https://doi.org/10.1613/jair.1.12850},
  groups    = {00-Paper-ENFORCE},
  publisher = {AI Access Foundation},
}

@Article{Lastrucci2025_PicardKKThPINN,
  author    = {Lastrucci, Giacomo and Karia, Tanuj and Gromotka, Zoë and Schweidtmann, Artur M.},
  journal   = {arXiv preprint},
  title     = {Picard-KKT-hPINN: Enforcing Nonlinear Enthalpy Balances for Physically Consistent Neural Networks},
  year      = {2025},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {https://doi.org/10.48550/arXiv.2501.17782},
  groups    = {00-Paper-ENFORCE},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  publisher = {arXiv},
}

