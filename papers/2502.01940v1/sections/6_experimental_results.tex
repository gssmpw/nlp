\section{Experimental Results and Analysis}
We apply the proposed approach to the Radelft dataset's RGB images and radar depth maps of Scene 2, which includes 3400 frames. We performed two main experiments: a test of the spatial spectrum generation using the proposed approach and an enhancement of these spatial spectrum images. 
\par We evaluate the performance of the data preprocessing by computing the Pearson correlation and mutual information metrics between the generated spectrums of RGB pixel semantic segmentation and the corresponding radar depth maps for $M_{radar}=50$ and $M_{cam}=200$, noting that a higher correlation value indicates a smaller discrepancy between the images, while mutual information indicates the learning potential of one modality from the other.

The performance of the depth map generation is measured by MAE, REL, RMSE, and UCD against the lidar point clouds and depth maps. MAE evaluates the average magnitude of errors in predictions, providing a straightforward measure of accuracy. REL normalizes the error by comparing it to the mean of the actual values, offering insight into the relative performance of the predictions. RMSE emphasizes larger errors by squaring the differences before averaging, making it sensitive to outliers. UCD measures the geometric similarity between the generated depth maps and the ground truth point clouds by calculating the average distance from each predicted point to its closest corresponding point in the lidar data. As our generation is dependent on semantic segmentations, this approach eliminates the average distance inflation that is caused by detectable object discrepancies between the semantic segmentation model, and radar and lidar point clouds when using Bidirectional Chamfer Distance (BCD).
% \vspace{-3px}

%---------------------------------------------%
\subsection{Data Preprocessing}
Our data preprocessing pipeline includes an input data conditioning sub-module followed by the proposed encoding approach explained in the previous sections. We performed experiments for $M = 10, 50, 70, 200$ and $\Phi=\Theta=(-70,70)$ degrees that truncate significant spectrum leakage at higher angles. The radar input is a simple data structure transformation from projected 3D point clouds coordinates to 2D depth maps with the pixel value being inversely proportional to depth. We test our data preprocessing against Pearson correlation and mutual information. The higher value of Pearson correlation represents a stronger linear relationship between the two variables, while the higher value of mutual information indicates a greater reduction in the entropy when predicting one variable from another.

\par In Figure \ref{fig:algorithm_results}, one can observe that a greater $M$ produces higher-resolution images that preserve the contours of objects. The ripples in both horizontal and vertical axes are due to spectral leakage. Figure \ref{fig:Correlation} plots the Pearson correlation values per frame between the camera and radar modalities for $M_{radar}=50$ and $M_{cam}=200$. It also shows that the correlation is still higher when both radar depth map and camera semantic segmentation are encoded with the proposed approach. It also shows that the correlation is significantly higher for single modality encoding. Figure \ref{fig: Mutual Information} shows that mutual information is significantly improved when we encode both modalities. It also shows that mutual information is still significantly improved when we encode only a single modality.

\par Tables \ref{tab:Correlation} and \ref{tab:Mutual Information} show the averages (mean values) of the plots in Figures \ref{fig:Correlation} and \ref{fig: Mutual Information}. The results show that there are improvements by factors of 3.88 and 76.69 in Pearson correlation and mutual information, respectively.
\vspace{-3px}

% \begin{table}[h]
% \renewcommand{\arraystretch}{1.2} % Adjust the row height (1.5x the default height)
% \parbox{.45\linewidth}{
% \centering
% \begin{tabular}{|c||c c|}
% \hline
%     & \textbf{I}_{cam} & \textbf{P}_{cam} \\ \hline\hline
% \textbf{I}_{radar} & 0.1646 & 0.5503 \\
% \textbf{P}_{radar} & 0.0809 & \textbf{0.6396} \\ \hline
% \end{tabular}
% \caption{Average Pearson Correlation for different pairs.}
% \label{tab:Correlation}
% }
% \hfill
% \parbox{.45\linewidth}{
% \centering
% \begin{tabular}{|c||c c|}
% \hline
%     & \textbf{I}_{cam} & \textbf{P}_{cam} \\ \hline\hline
% \textbf{I}_{radar} & 0.0359 & 0.6117 \\ 
% \textbf{P}_{radar} & 0.4863 & \textbf{2.7533} \\ \hline
% \end{tabular}
% \caption{Average Mutual Information for different pairs.}
% \label{tab:Mutual Information}
% }
% \vspace{-12px}
% \end{table}

% \begin{table}[h]
% \renewcommand{\arraystretch}{1.2} % Adjust the row height
% \parbox{.45\linewidth}{
% \centering
% \begin{tabular}{|c||c c|}
% \hline
%     & \textbf{I}_{cam} & \textbf{P}_{cam} \\ \hline\hline
% \textbf{I}_{radar} & 0.1646 & 0.5503 \\
% \textbf{P}_{radar} & 0.0809 & \textbf{0.6396} \\ \hline
% \end{tabular}
% \caption{Average Pearson Correlation for different pairs.}
% \label{tab:Correlation}
% }
% \hfill
% \parbox{.45\linewidth}{
% \centering
% \begin{tabular}{|c||c c|}
% \hline
%     & \textbf{I}_{cam} & \textbf{P}_{cam} \\ \hline\hline
% \textbf{I}_{radar} & 0.0359 & 0.6117 \\ 
% \textbf{P}_{radar} & 0.4863 & \textbf{2.7533} \\ \hline
% \end{tabular}
% \caption{Average Mutual Information for different pairs.}
% \label{tab:Mutual_Information}
% }
% \vspace{-10pt} % Slightly reduce spacing without excessive compression
% \end{table}

\begin{table}[h]
\renewcommand{\arraystretch}{1.2} % Adjust the row height
\parbox{.45\linewidth}{
\centering
\begin{tabular}{|c||c c|}
\hline
    & $\mathbf{I}_{\text{cam}}$ & $\mathbf{P}_{\text{cam}}$ \\ \hline\hline
$\mathbf{I}_{\text{radar}}$ & 0.1646 & 0.5503 \\
$\mathbf{P}_{\text{radar}}$ & 0.0809 & \textbf{0.6396} \\ \hline
\end{tabular}
\caption{Average Pearson Correlation for different pairs.}
\label{tab:Correlation}
}
\hfill
\parbox{.45\linewidth}{
\centering
\begin{tabular}{|c||c c|}
\hline
    & $\mathbf{I}_{\text{cam}}$ & $\mathbf{P}_{\text{cam}}$ \\ \hline\hline
$\mathbf{I}_{\text{radar}}$ & 0.0359 & 0.6117 \\ 
$\mathbf{P}_{\text{radar}}$ & 0.4863 & \textbf{2.7533} \\ \hline
\end{tabular}
\caption{Average Mutual Information for different pairs.}
\label{tab:Mutual_Information}
}
\vspace{-10pt} % Slightly reduce spacing without excessive compression
\end{table}



%---------------------------------------------%
\begin{figure*}[t!]
  \centering
  \includegraphics[width=1\linewidth]{figures/Results.pdf}
  \caption{Results from the training module for four example frames. From left to right for each example frame: Scene in RGB, Camera spatial spectrum ($\textbf{P}_{cam}$), original radar depth map, output from trained ResNet101. In each of the 4 frames, observe that our approach leads to sharper depth maps.}
  \vspace{-11px}
  \label{fig:training_results}
  \centering
  \vspace{-6px}
\end{figure*}

\subsection{Depth Map Generation}
% \vspace{-5px}
The depth map generation is achieved with the ResNet101 network trained as described in the previous section with $M_{cam}=200$, $M_{radar}=20$ and $\Phi=\Theta=(-70,70)$. The ResNet101 is trained for 10,000 epochs while the input data are compressed with the natural logarithm to make different features comparable in terms of scale. Qualitatively, from the results in Figure \ref{fig:training_results}, one can observe that the intensity locations in the radar depth maps are comparable to locations in camera spectrum images. Also, the output depth maps from the ResNet101 show object contours clearly compared to the original radar depth map. However, the magnitude of the ripples is significantly higher due to the logarithmic feature compression used at the input, which can be rescaled by exponentiation.
\par Quantitatively, we measure the performance of the output with MAE, REL, RMSE, and UCD. Specifically for UCD, we transform the spectrum images into 3D point clouds (in which the depth is inversely proportional to the pixel value) and apply the calculations. Table \ref{tab:UCD} presents the results for different methods including the state-of-the-art (SOTA) DNN detector \cite{roldan2024see} and the OS-CFAR. Figure \ref{fig: UCD} shows the per frame UCD for our method and SOTA. We observe that our approach improves MAE, REL, UCD by 21.13\%, 7.9\% and 27.95\%, respectively. However, the performance degrades by 12\% in terms of RMSE. We believe that this is due to the fact that, compared to other approaches, our generated depth maps are dense, inflating the RMSE result as there are gaps in the corresponding lidar depth maps.

\renewcommand{\arraystretch}{1.4}
\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Method} & \textbf{MAE} & \textbf{REL} & \textbf{RMSE} & \textbf{UCD ($m$)}\\
        \hline\hline
        Proposed Approach  & \textbf{0.026} & \textbf{0.025} & 0.111 & \textbf{3.48} \\
        SOTA DNN         & 0.033 & 0.027 & \textbf{0.099} & 4.83 \\
        OS-CFAR          & 0.029 & 0.152 & 0.258 & 18.02 \\
        % Peak Detector   &  -- \\
        \hline\hline
        \textbf{Improvement (\%)} & 21.13 & 7.9 & -12 & 27.95\\
        \hline
    \end{tabular}
    \caption{MAE, REL, RMSE, and UCD for different methods}
    \label{tab:UCD}
    \vspace{-5px}
\end{table}




