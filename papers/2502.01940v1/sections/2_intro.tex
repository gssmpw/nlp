\section{Introduction}

\par Autonomous Vehicles (AV) employ various sensors for comprehensive navigation and environmental perception, each contributing distinct advantages and limitations. RGB cameras are attractive due to their affordability and high accuracy in optimal lighting conditions but they struggle in low-visibility scenarios like darkness, heavy rain, or fog, which impede obstacle detection. Lidars offer precise 3D mapping and depth measurements, yet they consume high power, are susceptible to adverse weather conditions, and sometimes require mechanical parts for rotational scanning, raising the overall manufacturing and operational costs. Radars, however, stand out for their robustness in adverse weather, cost-effectiveness, low power consumption, and can offer a theoretical angular resolution comparable to lidars. 

\begin{figure}[t!]
  \centering
  \includegraphics[width=\linewidth]{figures/pixel_image_space1.pdf}
  \caption{Conceptual figure depicting the proposed spectrum-based transformation. The 'RGB' and 'Depth Maps' subspaces represent natural images captured by conventional cameras and the windshield view depth maps that lidars and radars produce, respectively. The 'Spatial Spectrum' subspace includes the special frequency 2D spectrum of bases that constitute images in the other subspaces.}
  \label{fig: image space}
  \vspace{-22px} % Adjust this value to reduce the space
\end{figure}

\par Automotive radars measure range, azimuth, and velocity, with 4D radars adding elevation to their traditional measurements. Unlike 3D radars, 4D radars are capable of estimating object heights without using speculative models. For such radars, extracting precise angular information involves a two-stage process: spatial spectrum estimations followed by Constant False Alarm Rate (CFAR) detectors like cell-averaging CFAR (CA-CFAR) and order-statistic CFAR (OS-CFAR) \cite{richards2010principles}. However, these conventional methods struggle in complex vehicular settings, producing sparse point clouds that limit accurate environmental representation \cite{khan2022comprehensive}. To address these limitations, data-driven approaches using deep neural networks (DNNs) have been reported \cite{brodeski2019deep, cheng2022novel, roldan2024see}. For instance, \cite{roldan2024see} employs a ResNet18 network \cite{chen2017rethinking} trained on dense lidar point clouds, generating denser radar point clouds that more accurately represent object shapes and sizes.

\par In this paper, we introduce a data-driven approach for generating radar depth maps by integrating radar point clouds with camera images. Leveraging the similar field-of-view (FoV) between radar and camera images, employing a non-linear frequency pixel positional encoding algorithm and Bartlett's spatial spectrum estimation \cite{Bartlett1948} transforms radar depth maps and camera RGB images into a shared spatial spectrum subspace, as shown in Figure \ref{fig: image space}. This transformation can resolve the differences between the 4D radar image and camera images, thus enabling spectrum-based learning. The method enables the use of high-resolution cameras to effectively train radar depth map generators. After this off-line training, the 4D radar model can operate independently of the camera, generating sharper and denser depth maps that are critical for perception, tracking, and rendering in AVs. Our contributions can be summarized as:

\begin{itemize}
\item We propose a pixel positional encoding algorithm that helps resolve the differences between a 4D radar image and RGB camera image, thus enabling spectrum-based learning for 4D radar images.

% \item We introduce a holistic framework for producing depth map generative models that are based on the newly developed DNN detector in \cite{roldan2024see} as a sub-module. Although the DNN detector is trained on lidar data, our framework does not require further training on lidar data and camera images for operation.

\item We present experimental results for high-resolution spectrum estimations and depth map generations. Our results show that our approach is capable of producing qualitatively sharp depth maps and significantly outperforms the state-of-the-art (SOTA), resulting in a reduction of 21.13\%, 7.9\% and 27.95\% in Mean Absolute Error (MAE), Relative Absolute Error (REL), and Unidirectional Chamfer Distance (UCD), respectively, which is quite significant.\cite{zhang2021unsupervised}. We also show that the estimated spectrum of camera and radar images results in an increase of the Pearson correlation and mutual information by a factor of 3.88 and 76.69, respectively.

\end{itemize}