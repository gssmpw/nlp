\section{Related Work}

% Look at survey: https://arxiv.org/pdf/2304.10410

\subsection{Datasets}
This work requires 4D radar data that include raw measurements in order to apply novel DNN detectors. Here, we review available 4D radar datasets.

The RaDelft, VoD, and K-Radar datasets are critical resources for automotive radar research, each offering unique capabilities. The RaDelft dataset provides synchronized 4D radar, lidar, camera, and odometry data from Delft, Netherlands, and has been used in deep learning to replicate lidar-like expressiveness with radar input \cite{roldan2024see}. Similarly, the VoD dataset integrates camera, BEV S3 lidar, 4D radar, and odometry data, offering annotations and bounding boxes for object detection, making it ideal for multimodal research and downstream tasks \cite{VoD}. The K-Radar dataset serves as a benchmark for object detection and classification with pre-processed radar data like range Doppler and range-angle maps, along with ground truth annotations. However, it lacks raw 4D radar signals, limiting its application in workflows requiring unprocessed radar data \cite{kradar}. In this work, we are using the RaDelft DL detector dataset, as VoD and K-radar do not provide raw radar signals that are sufficient for applying the detector developed in \cite{roldan2024see}.


%-------------------------------------------------------------------------
\subsection{Multimodal Data Fusion for Depth Map Generation}

Several advanced methods leverage radar and camera fusion to enhance autonomous driving perception tasks. For example, \cite{long2021radar} introduces a two-stage pixel-level fusion method for depth completion, where radar-to-pixel association in the first stage maps radar returns to pixels using lidar-generated ground truth, and the second stage fuses this output with camera images featuring projected 3D radar point clouds. Similarly, \cite{singh2023depth} adopts a two-stage approach that uses RadarNet to generate a confidence map from RGB images and noisy 3D radar point clouds in the first stage, followed by depth map enhancement using the confidence map and RGB images. In contrast, \cite{li2024radarcam} proposes a four-stage framework for dense depth estimation that enhances monocular depth prediction with accurate metric scale information derived from sparse 3D radar data, using global scale alignment, transformer-based quasi-dense scale estimation, and local refinement. Expanding on these concepts, \cite{li2024semantic} develops a semantic-guided depth completion framework that incorporates a Radar Point Correction Module (RPCM) for radar refinement, a Semantic Divide-and-Conquer Module (SDCM) for category-specific tasks, and a Region Depth Diffusion Module (RDDM) for further depth refinement. Additionally, \cite{radarocc} targets 3D occupancy prediction by directly processing radar tensors with Doppler bin descriptors and spherical-based feature encoding, excelling in adverse weather conditions on the K-Radar dataset. 
\par Unlike the aforementioned methods, our approach shows that using the DNN detector model in [5] as a module, we can improve the depth maps generated by further training this model with images generated by cheap cameras. This paves the way for eliminating the use of lidars for real time perception in AVs.




% \textbf{LXL:} LXL (LiDAR Excluded Lean 3D Object Detection) leverages 4D imaging radar and camera fusion for 3D object detection, addressing the limitations posed by the sparsity and noisiness of 4D radar point clouds. The LXL model introduces a novel "radar occupancy-assisted depth-based sampling" strategy, which enhances view transformation by incorporating predicted image depth distribution maps and radar 3D occupancy grids. This approach outperforms traditional methods like "depth-based splatting" used in Lift-Splat-Shoot (LSS). Experimental results on the View of Delft (VoD) and TJ4DRadSet datasets show that LXL significantly outperforms state-of-the-art 3D object detection methods, demonstrating the efficacy of integrating radar and camera data without relying on LiDAR. \cite{LXL}  Our approach is different from LXL, however, due to the introduction of a Bartlett-inspired method for generating higher-resolution depth maps via spectrum estimation. This generates a more descriptive environment for the network.

% \textbf{4DRVO-Net:} Four-dimensional radarâ€“visual odometry (4DRVO) addresses challenges in 4D radar-visual odometry, including radar point cloud sparsity, inaccurate data association, and disturbances from dynamic objects. It utilizes a feature pyramid, pose warping, and cost volume (PWC) network architecture. While the multi-scale feature extraction network, Radar-PointNet++, enhances learning from sparse radar points, the adaptive radar-camera fusion module (A-RCFM) facilitates effective cross-modal interaction. In contrast, we use a state-of-the-art attention-based module and ResNet101 for higher resolution depth-maps. In their work, 4DRVO-Net demonstrates superior performance on the VoD dataset, outperforming the SOTA by 27.5\% in the mean relative translation error and 69.3\% in the mean relative rotation error, approaching the performance of 64-beam LiDAR odometry. \cite{4DRVO} 

% \textbf{RCFusion:} This paper presents RCFusion, a network combining camera and 4D radar for precise 3D object detection in autonomous driving. Utilizing 4D radar's point clouds with elevation data, RCFusion operates in a unified BEV space. The network extracts multiscale camera features through image backbone and feature pyramid network (FPN), converts them to orthographic feature maps, and enhances them with a shared attention encoder. Simultaneously, radar features are processed with radar PillarNet to generate pseudo-images for point cloud backbone processing, enabling radar BEV features. Fusion is achieved via an interactive attention module (IAM), facilitating effective integration of both modalities for improved object detection performance on TJ4DRadSet and view-of-delft (VoD) datasets. \cite{RCFusion}

% \textbf{RadarOcc:} The paper introduces RadarOcc a recent approach for 3D occupancy prediction using 4D imaging radar. This method directly processes power radar tensors instead of relying on sparse radar point clouds. It employs advanced techniques such as Doppler bins descriptors, sidelobe-aware spatial sparsification, and spherical-based feature encoding to address challenges like data volume, noise, and spherical-to-Cartesian transformations. Tested on the K-Radar dataset, RadarOcc demonstrates robust performance against adverse weather. \cite{radarocc}

% \vspace{7px}
% \textbf{CRF-Net:} Nobis et al. proposed CameraRadarFusion Net (CRF Net), combining camera and radar to outperform state-of-the-art ImageNet classification. To do this, they concatenate the radar return as a channel (like those for RGB) and input it to a convolutional neural network. The reseachers use multiple datasets, especially NuScenes \cite{nuscenes}, for which they achieve a 13\% increase in mean Average Precision. \cite{CRFNet} Our proposed method for data fusion, however, is different, utilizing an attention mechanism for more adaptable integration of both modalities.