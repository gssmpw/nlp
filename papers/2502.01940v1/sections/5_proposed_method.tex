\section{Proposed Approach}
Unlike lidars, 4D imaging radars used in AV suffer from sparse scene representations. Our goal in this work is to bypass lidars and produce sharp 4D radar depth maps by passing the radar output to a data-driven depth map generator. Since the camera RGB images have different characteristics from radar depth maps (i.e., they come from different pixel image subspaces), we propose to compute their unified, constitutive basis vectors and transform them into their spatial spectrum representations using Bartlettâ€™s algorithm. This bridges the gap between their original characteristics. With non-linear frequency progression basis vectors, we propose to encode the semantic segmentations of camera images and their corresponding radar depth maps to estimate their spatial spectrum. Interestingly, the spatial spectrum of both images includes frequency components proportional to $M$ caused by spectral leakage. These frequency components complete the depth for sparse point clouds, as well as introduce frequency bias that helps with fitting highly oscillatory data (the sharp camera images), as shown in Figure \ref{fig:algorithm_results} \cite{xu2024overview}. Figure \ref{fig:RSUencountered1} depicts this transformation and network training pipeline, while \ref{fig:RSUencountered2} shows the deployment pipeline for real-time operation, noting that the 4D radar and camera work independently but synchronously.

%-----------------------------------------------------------%

\subsection{Pixel Positional Encoding and Spectrum Estimation}
This encoding method aims to facilitate the transformation into the spatial spectrum of both the radar and camera images. Fast implementation of this encoding process starts with an initialization of a non-linear phase progression, complex sinusoidal basis vectors for $M$ segments for horizontal and vertical axes, $\phi$ and $\theta$. Our basis functions differ from the standard Fourier basis functions, since we require higher resolution for the output \cite{priestley1981spectral}. For that, we change the phase progression across basis functions from standard linear to non-linear, resulting in changing frequency across receptors. This changing frequency leads to higher resolution spectrum \cite{richards2010principles}. Our segments are described as:
\begin{equation}
    x(m,\phi_n)=e^{-j\pi m sin(\phi_n)} , x(m,\theta_k)=e^{-j\pi m sin(\theta_k)}
    \label{eq:horizontal_basis}
\end{equation}
% \begin{equation}
%     x(m,\theta_i)=e^{-j\pi m sin(\theta_i)}
%     \label{eq:vertical_basis}
% \end{equation}
where $m$ is the segment index, noting that $M$ is proportional to spectrum resolution, and $\phi_n$ and $\theta_k \in \Phi$ and $\Theta$ are variation angles from the set $(-90, 90)$ with lengths $N$ and $K$, respectively. The covariance matrices of every $\phi$ and $\theta$ are defined as $\textbf{C}(\Phi)$ and $\textbf{C}(\Theta) \in \mathbb{C}^{N \times N}$ and $\mathbb{C}^{K \times K}$, in which each of their rows represents the periodograms $\textbf{y}(\phi_n)$ and $\textbf{y}(\theta_k)$. The joint 2D periodogram is:
\begin{equation}
    \textbf{Y}(\phi_n, \theta_k) = \textbf{y}(\theta_n)^T\textbf{y}(\phi_k)
\end{equation}
To calculate the final 2D spatial power spectrum $\textbf{P} \in \mathbb{R}^{N \times K}$ for an input image $\textbf{I}$, we iteratively encode all pixels of $\textbf{I}$ and calculate $P(n,k)$ as:
\begin{equation}
    P(n,k)= \sum_{n=0}^{N-1} \sum_{k=0}^{K-1} \left| \textbf{Y}(\phi_n, \theta_k) \circ \textbf{I}\right|
\end{equation}
where $\circ$ denotes an element-wise multiplication. Experimental results are presented in the next section.
%----------------------------

% \begin{algorithm}[tb]
% \caption{Pixel Encoding and Spectrum Estimation}
% \label{alg:algorithm}


% \textbf{Input}: \textbf{I}, $M$, $\boldsymbol{\Phi}$, and $\boldsymbol{\Theta}$\\
% \textbf{Output}: 2D spectrum estimation for basis $x(m,\phi)$ and $x(m,\theta)$, \textbf{P}
% \begin{algorithmic}[1] %[1] enables line numbers
% \STATE Initialize $x(m,\phi)$, $x(m,\theta)$, and \textbf{P}\\
%         \STATE Compute $\textbf{C}(\boldsymbol{\Phi})$ and $\textbf{C}(\boldsymbol{\Theta})$\\
%         \FOR{each row $\textbf{y}(\theta_k)$ in $\textbf{C}(\boldsymbol{\Theta})$}
%             \FOR{each row $\textbf{y}(\phi_n)$ in $\textbf{C}(\boldsymbol{\Phi})$}
%                 \STATE $P(n,k) \leftarrow sum(|(\textbf{y}(\theta_k)^T \textbf{y}(\phi_n)| \circ \textbf{I})$
%             \ENDFOR
%         \ENDFOR

% \STATE \textbf{return} \textbf{P}

% \end{algorithmic}
% \end{algorithm}
%-----------------------------------------------------------%

\subsection{Radar Data Preprocessing}
This module conditions the DNN detector's depth map, $\textbf{I}_{radar}$, in the predecessor radar pipeline. The objective is to transform 2D depth maps into a spatial spectrum representation of its constitutional bases, $\textbf{P}_{radar}$, to satisfy the input characteristics in the following module, the 'Training Process'. This process of spatial spectrum estimation is defined as $\mathscr{F}(\textbf{I}, M)$, noting that $M$ is proportional to the output resolution.
\begin{equation}
    \textbf{P}_{radar}=\mathscr{F}(\textbf{I}_{radar}, M_{radar})
\end{equation}
% Example input and output are found in Figure 4.

%-----------------------------------------------------------%

\subsection{Camera Image Preprocessing}
In order to obtain the spatial spectrum representations for objects of interest, we transform the RGB image, $\textbf{I}_{cam}$, into its semantic segmentations, $\textbf{Seg}$, considering that this is highly dependent on the semantic segmentation accuracy and classes of the model in use. We use Deeplab v3 \cite{yurtkulu2019semantic} with ResNet101 \cite{chen2017rethinking} trained on the PASCAL Visual Object Classes (VOC) 2012 dataset \cite{Everingham10}. We then transform the semantic image into its spatial spectrum representation $\textbf{P}_{cam}$ through $\mathscr{H}(\textbf{I}, M)$, which is the process of spatial spectrum estimation of camera images.
\begin{equation}
    \textbf{P}_{cam}=\mathscr{H}(\textbf{I}_{cam}, M_{cam})=\mathscr{F}(\textbf{Seg}, M_{cam})
\end{equation}
Note that we require $M_{cam} > M_{radar}$ so that the radar spectrum images have a lower resolution, which leaves room for enhancement with deep learning models. 
% Example results are present in Figure \ref{fig:algorithm_results}.

%-----------------------------------------------------------%

\subsection{Network Training Process}
This module focuses on training a generative model that produces a denser and contour-accurate version of $\textbf{P}_{radar}$. As there are some objects captured by the semantic segmentation model that are not detectable by radar, and vice versa, element-wise multiplication produces the mutuality between both spectrum images which, thereafter, is fed into the learning as ground truth for training a ResNet, being optimized to reduce the difference through L2 loss. The process is described as:
\begin{equation}
    \textbf{P}_{radar} \circ \textbf{P}_{cam} = \text{ResNet}(\textbf{P}_{radar})
\end{equation}