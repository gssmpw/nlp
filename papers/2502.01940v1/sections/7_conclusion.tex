\section{Conclusion}
\vspace{-3px}
% In this paper, we have introduced a novel approach for radar depth map generation in Autonomous Vehicles (AV) by integrating depth maps deep neural network (DNN) radar detectors with camera images. Our method leverages Bartlettâ€™s algorithm-inspired positional encoding to transform both radar point clouds and camera images into a common spectrum representation of the camera RGB image and the radar depth map. This technique facilitates the identification of correspondences between the two different data types, enabling the effective training of radar image generators using high-resolution camera images. Our experimental results demonstrate the potential of this integrated approach to significantly improve the accuracy and density of radar-generated depth maps. This enhancement not only addresses the limitations of conventional CFAR detectors but also offers a cost-effective alternative to expensive lidar-based sensor suites and leads to more robust and reliable environmental representations, making it a promising solution for scalable research and development in AV perception.

% This paper presents a new approach for generating 4D radar depth maps in Autonomous Vehicles by combining the depth maps generated by deep neural network (DNN) radar detectors with camera images. The approach uses Bartlett's algorithm-inspired positional encoding to convert radar point clouds and camera images into a common spectrum representation. This allows for better correspondence between the two data types and enables training radar image generators with high-resolution camera images. The results show improved accuracy and density in radar-generated depth maps, overcoming limitations of traditional CFAR detectors. The results presented show a 27.95\% improvement compared to the state-of-the-art in terms of unidirectional Chamfer distance which is quite significant. 

% We emphasize that the proposed approach can make the fusion of 4D radar depth maps and camera images even more compelling as an alternative to Lidar based AV perception. In terms of impact, the proposed work offers a cost-effective and significant alternative to expensive lidar systems. This, in turn, could be a game changer in AV perception and facilitate affordable AV system design in the near future.



This paper introduces a novel method for generating 4D radar depth maps for autonomous vehicles by integrating depth maps from DNN-based radar detectors with camera images using a positional encoding inspired by Bartlett's algorithm. This approach converts radar point clouds and camera images into a shared spectrum representation, enhancing their correspondence and enabling the training of radar image generators with high-resolution camera data. The method achieves a significant improvement of 27.95\% in unidirectional Chamfer distance compared to the state-of-the-art, delivering more accurate and denser radar-generated depth maps than traditional CFAR detectors. By offering a highly trained 4D Radar, the presented approach holds the promise of making perception in AVs much more cost-effective when used in conjunction with cheap cameras in real time, by avoiding the expense of Lidar.

\vspace{-3px}






