\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[font=small]{caption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{amsmath,algorithm}
\usepackage{mathrsfs}
\usepackage{subcaption}  % For subfigures
\usepackage{graphicx}    % For including graphics
\usepackage{xcolor}
\usepackage{booktabs} % For thicker lines
\usepackage{tabu}
\usepackage{tabularray}
% \usepackage{slashbox}






\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Toward a Low-Cost Perception System in Autonomous Vehicles: A Spectrum Learning Approach\\
}

\author{
    %Authors
    % All authors must be in the same font size and format.
    Mohammed Alsakabi\textsuperscript{\rm 1},
    Aidan Erickson\textsuperscript{\rm 1},
    John M. Dolan\textsuperscript{\rm 2},
    Ozan K. Tonguz\textsuperscript{\rm 1} \\
    
    \textsuperscript{\rm 1}Department of Electrical and Computer Engineering, College of Engineering \\
    \textsuperscript{\rm 2}The Robotics Institute, School of Computer Science \\
    Carnegie Mellon University, Pittsburgh, PA, United States\\
    \{malsakabi, aerickson\}@cmu.edu , \{jdolan, tonguz\}@andrew.cmu.edu
}

\maketitle
\input{sections/1_abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sections/2_intro}

% \begin{figure*}[!t]
%   \centering
%   \includegraphics[width=0.9\linewidth]{figures/Radar_Pipeline.pdf}
%   \caption{Radar Pipeline includes three modules: the hardware setup, the signal processor, and the detector.}
%   \label{fig:RSUencountered}
%   \vspace{-11px}
%   \centering
% \end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sections/3_related_work}

\begin{figure*}[!t]
  \centering
  \begin{minipage}{\linewidth}
    \centering
    \begin{subfigure}[b]{\textwidth}
      \centering
      \includegraphics[width=0.83\linewidth]{figures/our_pipeline.pdf}
      \caption{}
      \label{fig:RSUencountered1}
    \end{subfigure}
    \vspace{1em} % Space between images
    \begin{subfigure}[b]{\textwidth}
      \centering
      \includegraphics[width=0.83\linewidth]{figures/operational_scheme.pdf}
      \caption{}
      \label{fig:RSUencountered2}
      \vspace{-11px}
    \end{subfigure}
  \end{minipage}
  \caption{The pipeline for our proposed method: (a) The offline network training scheme is divided into four modules: the predefined radar signal processing and detection (point-clouds extractor) by \cite{roldan2024see}, the pre-processing of camera and radar data defined under 'Proposed Approach', and the training process which takes the 'Radar Data Pre-processing' output as input and the filtered 'Camera Image Pre-processing' output as the ground truth for training. (b) The proposed operational deployment of the trained network in (a) considering the required lower-level processes, where radar and camera work independently of each other and provide data for any required further process.}
  \label{our_pipeline}
  \vspace{-11px}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sections/4_background}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.92\linewidth]{figures/Data_preprocessing_results.pdf}
  \caption{Results for $M=10, 70$ and $200$, and $\Phi=\Theta=(-70,70)$ against the original RGB scene on the left.}
  \label{fig:algorithm_results}
  \vspace{-11px}
  \centering
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sections/5_proposed_method}

\begin{figure*}[!t]
	\centering
	\begin{subfigure}{0.30\linewidth}
		\includegraphics[width=\linewidth]{figures/Correlation.pdf}
		\caption{}
		\label{fig:Correlation}
	\end{subfigure}
        \hspace{0.01\textwidth}
	\begin{subfigure}{0.30\linewidth}
		\includegraphics[width=\linewidth]{figures/Mutual_Information.pdf}
		\caption{}
		\label{fig: Mutual Information}
	\end{subfigure}
        \hspace{0.01\textwidth}
	\begin{subfigure}{0.30\linewidth}
	        \includegraphics[width=\linewidth]{figures/UCD.pdf}
	        \caption{}
	        \label{fig: UCD}
         \end{subfigure}
	\caption{(a) Correlation and (b) mutual information between several depth map pairs. 'SoTA' refers to depth map obtained by \cite{roldan2024see} while 'Encoded' refers to the spectrum of encoded images using the described bases.}
	\label{fig:subfigures}
    \vspace{-14px}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sections/6_experimental_results}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\input{sections/7_conclusion}

\bibliographystyle{IEEEtran}
\bibliography{bibliography}

\end{document}
