
%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% For the deterministic simulator appendix %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{longtable} % for multi-page tables
\usepackage{bm} % bold symbols in math mode
\usepackage{booktabs}
\usepackage{xcolor}
\newcommand{\diag}[1]{\textnormal{diag}\left(#1\right)}
\definecolor{ibm0}{rgb}{0.3906, 0.5586, 0.9961}
\definecolor{ibm1}{rgb}{0.4688, 0.3672, 0.9375}
\definecolor{ibm2}{rgb}{0.8594, 0.1484, 0.4961}
\definecolor{ibm3}{rgb}{0.9922, 0.3789, 0.0000}
\definecolor{ibm4}{rgb}{0.9961, 0.6875, 0.0000}
\definecolor{ibm5}{rgb}{0.0000, 0.0000, 0.0000}
\definecolor{ibm6}{rgb}{0.9961, 0.9961, 0.9961}
\newcommand\pam[1]{{{\color{ibm4}{#1}}}}
\newcommand\inp[1]{{{\color{ibm0}{#1}}}}
\newcommand\oup[1]{{{\color{ibm1}{#1}}}}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% tikz
\usepackage{tikz}
%\usetikzlibrary{positioning}
%\usetikzlibrary{bayesnet}
%\usetikzlibrary{fit}
%\usetikzlibrary{shapes,arrows}
%\usetikzlibrary{shapes.multipart}
\usetikzlibrary{arrows.meta}

\usepackage{nicematrix}
\usepackage{bm}

\usepackage[normalem]{ulem} % we can remove this when we submit


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Comments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand\simon[1]{{{\color{orange} S: #1}}}
\newcommand\juan[1]{{\color{blue} J: #1}}
\newcommand\jakob[1]{{\color{green} JR: #1}}
\newcommand\outline[1]{{\color{magenta}#1}}
\newcommand\rmj[1]{{\color{gray}\tiny{#1}}} % to remove text
\newcommand\comm[1]{{\tiny #1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% cref names
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\subsectioncrefname}{Section}%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}
\usepackage{caption}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Sanity Checking Causal Representation Learning on a Simple Real-World System}

\begin{document}

\twocolumn[
\icmltitle{Sanity Checking Causal Representation Learning \\on a Simple Real-World System}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Juan L. Gamella}{equal,xxx}
\icmlauthor{Simon Bing}{equal,yyy}
\icmlauthor{Jakob Runge}{zzz,yyy}
\end{icmlauthorlist}

\icmlaffiliation{xxx}{Seminar for Statistics, ETH Zurich}
\icmlaffiliation{yyy}{Technische Universit√§t Berlin}
\icmlaffiliation{zzz}{ScaDS.AI Dresden/Leipzig, TU Dresden}
%\icmlaffiliation{zzz}{Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI) Dresden/Leipzig, TU Dresden}

\icmlcorrespondingauthor{Juan L. Gamella}{juangamella@gmail.com}
\icmlcorrespondingauthor{Simon Bing}{bing@campus.tu-berlin.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We evaluate methods for causal representation learning (CRL) on a simple, real-world system where these methods are expected to work. The system consists of a controlled optical experiment specifically built for this purpose, which satisfies the core assumptions of CRL and where the underlying causal factors---the inputs to the experiment---are known, providing a ground truth.
We select methods representative of different approaches to CRL and find that they all fail to recover the underlying causal factors. To understand the failure modes of the evaluated algorithms, we perform an ablation on the data by substituting the real data-generating process with a simpler synthetic equivalent. The results reveal a reproducibility problem, as most methods already fail on this synthetic ablation despite its simple data-generating process. Additionally, we observe that common assumptions on the mixing function are crucial for the performance of some of the methods but do not hold in the real data.
Our efforts highlight the contrast between the theoretical promise of the state of the art and the challenges in its application. We hope the benchmark serves as a simple, real-world sanity check to further develop and validate methodology, bridging the gap towards CRL methods that work in practice. We make all code and datasets publicly available at \href{https://github.com/simonbing/CRLSanityCheck}{\nolinkurl{github.com/simonbing/CRLSanityCheck}}.
\end{abstract}

\section{Introduction}
\label{s:intro}

\begin{figure*}[t]
\vskip 0.15in
    \centerline{
\includegraphics[width=172mm]{figures/tunnel.pdf}
}
\caption{The light tunnel (left) and a simplified schematic (right) showing its main components and variables. The tunnel consists of a controllable light source, linear polarizers mounted on rotating frames, a camera, and sensors to measure light intensity at different wavelengths and positions. The inputs to the system ($R,G,B,\theta_1,\theta_2$) are displayed in bold math print in this figure. The system's outputs---image data and numeric sensor measurements---are denoted by a tilde.}
\label{fig:tunnel}
\end{figure*}

Uncovering the underlying factors that determine the behavior of a system is a problem with a long history, both in theoretical treatise \citep{comon1994independent, hyvarinen1999nonlinear, bengio2013representation, lecun2015deep, lake2017building}, as well as practical applications \citep{johansson2022generalization, tibau2022spatiotemporal, lopez2023learning}. Causal representation learning is among the newest approaches in this line of work, and its enticing promise lies in recovering the \emph{causal} latent ground-truth model from high-level observations of a system. While this problem is provably hard \citep{locatello2019challenging}, there has been a considerable effort to advance the understanding of the theoretical constraints within which causal representation learning may be successfully applied \citep{hyvarinen2016unsupervised, khemakhem2020variational, gresele2021independent, brehmer2022weakly, ahuja2023interventional, zhang2023identifiability, lippe2022citris, lippe2022icitris, lippe2023biscuit, lachapelle2023synergies, varici2023scorebased, saengkyongam2024identifying, vonkugelgen2024nonparametric, yao2024unifying}. Applying established causal inference methods to unstructured, high-dimensional data, addressing the inherent limitations of machine learning methods in out-of-distribution scenarios, or learning mechanistic models of the world are among the multitude of promises that drive research in the field of causal representation learning \citep{scholkopf2021causal}.

A challenge that obfuscates the progress towards these goals is the lack of meaningful real-world benchmarks, to evaluate methods and identify theoretical approaches that hold potential for application. By and large, new methods are evaluated on synthetic datasets generated according to their own underlying assumptions \citep{brehmer2022weakly, ahuja2023interventional, lippe2022citris, lippe2022icitris, lippe2023biscuit, squires2023linear, lachapelle2023synergies, lachapelle2024nonparametric, liang2023causal, buchholz2023learning, varici2023scorebased, bing2024identifying, vonkugelgen2021self, vonkugelgen2024nonparametric, yao2024multi, xu2024sparsity}. This practice provides further validation for the theoretical foundations of these methods but yields limited insight into their applicability to real-world problems. More sophisticated synthetic benchmarks have been proposed \cite{lippe2022citris,lippe2022icitris,ahmed2021world,vonkugelgen2021self,liu2023triplet}, e.g., using visualizations of computer games or renderings of three-dimensional scenes. Because these do not cater to the assumptions of any particular method, they are extremely valuable for the standardized evaluation of CRL methods. However, their synthetic nature means they are limited as testbeds to discuss which CRL approaches---and their underlying theory---are applicable in the real world. While real-world datasets are often used to illustrate potential applications of new methods \cite{zhang2023identifiability, chen2024identifying, yao2024marrying}, the absence of a ground truth also precludes any in-depth analysis into this matter.

As an orthogonal contribution to existing synthetic benchmarks, we propose a real-world sanity check to test the applicability of CRL methods and their underlying theory. The test is based on a carefully designed, real, physical system whose data-generating process matches the core assumptions of causal representation learning (\cref{s:testbed}). Furthermore, the relationship between latent factors and observations is far simpler than in the synthetic benchmarks mentioned in the previous paragraph. Thus, we base our sanity check on the premise that a generic CRL method---designed to work in various settings---should also work on this simple but real system.

The physical system consists of a well-understood optical experiment involving the polarization of light, described in detail in \cref{s:testbed}. The control inputs to the experiment constitute the underlying causal factors, and its outputs---image and sensor data---correspond to the entangled observations. Because the control inputs are known, we have a ground truth to directly evaluate the performance of the tested methods in recovering the underlying causal factors.

As a first application of our sanity check, we evaluate a representative method from three different families of approaches: CRL methods requiring interventional (or counterfactual) data \cite{brehmer2022weakly, ahuja2023interventional, zhang2023identifiability, squires2023linear, buchholz2023learning, varici2023scorebased, liang2023causal, bing2024identifying, vonkugelgen2024nonparametric}, CRL considering multiple views \citep{gresele2019incomplete, locatello2020weakly, vonkugelgen2021self, daunhawer2023identifiability, ahuja2024multi, xu2024sparsity, yao2024multi}, and CRL based on time-series data \citep{yao2021learning, yao2022temporally, lippe2022citris, lippe2022icitris, lippe2023biscuit, lachapelle2024nonparametric}. The results and background needed to understand them are given for each method in \cref{s:results}.

% \paragraph{Wrap up} \outline{Echo of the discussion
% \begin{enumerate}    
%     \item Crucial: our testbed allows us to evaluate assumptions on the mixing transformation, but not the underlying assumptions on causal structure, e.g., whether one can successfully model the relationships between underlying factors using a SCM.
%     \item Furthermore, while the benchmark contains real-data, this is still a controlled environment, and success in this benchmark may not carry over to other real-world applications of methods.
%     \item Furthermore, without clear set applications of CRL, it is difficult to elucidate what other meaningful benchmarks may be. We focus on image data because a lot of the fields' algorithms are focused on such data.
%     \item However: our goal is that it serves as a sanity test for fundamental assumptions and algorithms designed to work in a variety of 
% \end{enumerate}}

To our knowledge, this sanity check is the first of its kind, and we hope that it can serve as a standardized benchmark to evaluate new methods and test the assumptions that underlie theoretical work. However, we find it important to clearly delimit the conclusions that can be drawn from this benchmark. First, our current setup is geared towards evaluating the assumptions concerning the mixing function, as they underpin the identifiability results of many methods, and they are the most straightforward to test. This means that the applicability of other assumptions, for example, regarding modeling the latent causal structure using a structural causal model \citep[SCM,][Chapter 3]{peters2017elements}, cannot be readily assessed using the datasets we currently provide. We further discuss this point in \cref{s:testbed}.
Second, as with any test, only a negative result is truly informative: a method that fails on this simple and tightly controlled testbed can be expected to fail in other, more complex, real-world scenarios. However, the converse does not hold. While our setup is representative of other systems that produce data with digital sensors, a method that succeeds on our benchmark is not guaranteed to do so on all other real-world systems that are the target of application.

The code to reproduce the results of this paper can be found at \href{https://github.com/simonbing/CRLSanityCheck}{\nolinkurl{github.com/simonbing/CRLSanityCheck}}. We make all datasets publicly available and document their data-collection procedures at \href{https://github.com/juangamella/causal-chamber}{\nolinkurl{github.com/juangamella/causal-chamber}}.

\section{Experimental Setup}\label{s:testbed}


Our physical system is a light tunnel like the one introduced by \citet{gamella2025chamber}.
% Change this in the preprint to "builds on the light tunnel"
We provide a schematic of its main components and relevant variables in \cref{fig:tunnel}. The tunnel is an elongated chamber with a controllable light source at one end, two linear polarizers mounted on rotating frames, a camera, and sensors to measure different physical quantities. As control inputs, the system takes the brightness level of the red, green, and blue LEDs of the light source ($R,G,B$) and the polarizer positions ($\theta_1, \theta_2$). Its outputs are the images captured by the camera (\cref{fig:images}) and readings of the infrared ($\tilde{I}_1, \tilde{I}_2, \tilde{I}_3$) and visible ($\tilde{V}_1, \tilde{V}_2, \tilde{V}_3$) light intensity at different positions, the electrical current drawn by the light source ($\tilde{C}$), and additional noisy measurements of the polarizer angles ($\tilde{\theta}_1, \tilde{\theta}_2$). A detailed description of all the variables can be found in \citet[Appendix II]{gamella2025chamber}.

As a data-generating process, the light tunnel satisfies the core assumptions of causal representation learning. In particular, it transforms some underlying causal factors---the control inputs---into observations consisting of images and numeric sensor measurements. Because we control the inputs to the system, we can sample them from any distribution or causal structure, as we do in the experiments of \cref{s:results}. However, this means that the underlying causal structure is synthetic, and the real-world applicability of the corresponding assumptions cannot be evaluated. Therefore, using the current setup we can only test the assumptions corresponding to the mixing function, but not those placed on the latent causal model generating the data.

\begin{figure}
    \centerline{
\includegraphics[width=82mm]{figures/sample_images.pdf}
}
\caption{\textbf{(A--D):} Real images collected from the light tunnel, with the corresponding control inputs overlaid in white. The light source color ($R,G,B$) is the same for images C and D, but the first polarizer is shifted by $90$ degrees in image D, showing the effect of the linear polarizers angles ($\theta_1, \theta_2$).
\textbf{(E, F):} Comparison between (E) a real image (at low resolution) and (F) the synthetic counterpart produced by a simple multi-layer perceptron given the same inputs.}
\label{fig:images}
\end{figure}

\paragraph{A real-world mixing function} The light tunnel provides an ideal testbed to evaluate assumptions concerning the mixing function that transforms the ground-truth causal factors into observations. In the tunnel, the process that produces the images is far simpler than those in common synthetic benchmarks, which arise, for example, from visualizations of computer games \cite{lippe2022citris,lippe2022icitris} or renderings of a physical simulator \cite{ahmed2021world, vonkugelgen2021self}. Evidence of this simplicity is that the process can be simulated to a high degree of visual realism with a simple multi-layer perceptron (MLP) (\cref{fig:images}, E/F). Furthermore, the relationship between the light source setting ($R,G,B$) and the pixel intensities is approximately linear up to the overexposure of the camera sensor, as happens in the central region of the images. The polarizer angles ($\theta_1, \theta_2$) scale the light intensity reaching the camera by a factor of $\cos^2(\theta_1 - \theta_2)$, as dictated by Malus' law \cite{collett2005field}. This dimming affects each color wavelength differently, resulting in a distortion of the color noticeable at relative angles close to  $90^\circ$ (see D/E in \cref{fig:images}). These effects are thoroughly documented in \citet[Appendix III]{gamella2025chamber}. The images also contain some other subtle artifacts, introduced by the chromatic aberration of the lens, the reflections of the polarizer frames, and the processor onboard the camera. Different inputs always result in different images.

A crucial difference with commonly used synthetic benchmarks is the noise present in the images and sensor measurements, which naturally arises from several processes. The first is the fluctuation of the light source, which modulates its brightness by switching on and off at a frequency of 2KHz, interacting with the photodiodes on the camera and the sensors in a complex way. The second is the measurement noise introduced by the sensors themselves, which is to be expected from any real system. For the camera and the third light sensor, which are placed behind the polarizers, the relative strength of these two noise sources is modulated by the polarizer positions $\theta_1$ and $\theta_2$. As a result, the ``mixing function'' of the light tunnel is not a deterministic process. Furthermore, this stochasticity is not captured by an additive noise model.

\paragraph{Synthetic ablation} To explore the effect that these subtle artifacts have on the performance of causal representation learning methods, we perform additional experiments where we substitute the real data-generating process of the light tunnel with a deterministic simulator that produces synthetic images and sensor measurements given the same inputs ($R,G,B,\theta_1,\theta_2$). The simulator produces synthetic measurements via a mechanistic model and synthetic images (\cref{fig:images}F) via a simple MLP trained on a separate dataset. Further details about the simulator and its construction can be found in \cref{s:simulator}.

\section{Results}
\label{s:results}

We apply our sanity check to methods representative of three different families of approaches: contrastive CRL (\cref{ss:contrastive}), multiview CRL (\cref{ss:multiview}), and CRL from temporal intervened sequences (\cref{ss:temporal}). As a result, the methods differ greatly in their setup and assumptions on the latent causal structure, as well as in their goals and the metrics to evaluate them. This makes a direct comparison between them difficult, and we instead perform our analysis method-by-method, introducing the necessary background together with the results.

In our efforts, we encounter a first challenge in the application of causal representation learning methods to real-world problems: a recurrent lack of public, well-documented, and tested code. As a result, our choice of methods is biased towards those with public code, or for which the authors provided code upon request.

We find that pre-processing steps and implementation decisions---such as network architectures and training procedures---have a drastic effect on the performance of some of the methods. This also highlights a potential difficulty in re-implementing CRL methods to reproduce their results. To minimize potential points of failure, we used the original implementations as much as possible. We thank the authors of the selected methods for their assistance, which was instrumental in getting them to run for this benchmark. To further ensure that a bug in our pipeline does not distort our results, we ran preliminary tests on synthetic datasets replicating the settings used in the respective original works \cite{buchholz2023learning, yao2024multi, lippe2022citris}. Our experimental pipeline for each method is described in detail in \cref{s:exp_details}.

\subsection{Contrastive CRL}
\label{ss:contrastive}

\begin{figure*}[t]
\vskip 0.15in
    \centering
    \begin{minipage}[c]{0.48\textwidth} % Left half for the table
        \centering
        \renewcommand{\arraystretch}{1.2} % Adjust row height for better readability
        \begin{tabular}{ccc}
        \toprule
         & MCC $\uparrow$ & SHD $\downarrow$\\
         \midrule
        Real & $0.285 \pm 0.054$ & $7.600 \pm 0.894$ \\
        Synth. ablation & $0.891 \pm 0.005$ & $2.000 \pm 0.000$\\
        \bottomrule 
    \end{tabular}
    \end{minipage}%
    \hfill
    \begin{minipage}[c][][b]{0.48\textwidth} % Right half for the figure
        \centering
        \includegraphics[width=\textwidth]{figures/graphs_ccrl.pdf} % Replace with your figure file
    \end{minipage}
    \vskip 0.15in
    \caption{Experimental results for the Contrastive CRL method applied to the real data and the data from the synthetic ablation described in \cref{s:testbed}. \textbf{Left:} MCC score of predicting the ground-truth factors, and SHD score between the estimated latent graph and the ground-truth graph (shown on the left). We provide the average and standard deviation ($\pm$) of the scores over five random initializations of the method. \textbf{Right:} Ground-truth graph and a summary of the estimated latent graphs produced by five random initializations of the method. The edges are shaded and labeled according to the frequency they appear in the estimates obtained in the five runs of the method, with darker edges appearing more often. The method performs well on the data from the synthetic ablation, where a deterministic simulator (\cref{s:simulator}) substitutes the data-generating process of the light tunnel. The method fails to produce meaningful results based on the real data.}
    \label{fig:ccrl_results}
\end{figure*}

The most prevalent class of CRL methods are those that assume access to data from different environments, stemming from interventions (or counterfactual observations) on some of the underlying causal factors \cite{brehmer2022weakly, ahuja2023interventional, zhang2023identifiability, squires2023linear, buchholz2023learning, varici2023scorebased, liang2023causal, bing2024identifying, vonkugelgen2024nonparametric}. We choose the method of \citet{buchholz2023learning} as a representative of this family of models. In the remainder of the text, we will refer to it as Contrastive CRL (CCRL).

\paragraph{Background.} As an underlying causal model, CCRL assumes a linear structural causal model (SCM) with additive Gaussian noise \citep[Assumption 2]{buchholz2023learning}. The underlying causal factors are transformed into observations through a nonlinear and deterministic mixing function \citep[Assumption 1]{buchholz2023learning}; while in theory their results can be extended to stochastic mixing functions with additive noise, both their experiments and implementation assume a deterministic mixing function. Provided with data from environments that correspond to a single-node intervention on each causal variable, the goal of the method is to recover the ground-truth causal factors and the graph encoding the causal structure between them \citep[Theorem 1]{buchholz2023learning}.

\paragraph{Data generation.} In line with the assumptions made by the method, we sample the values for the ground-truth factors---the tunnel inputs $R, G, B, \theta_1$ and $\theta_2$---from a linear SCM with additive Gaussian noise; the corresponding ground-truth graph is shown in \cref{fig:ccrl_results}. To generate the interventional data, we perform interventions that shift the mean of their target, closely following the assumptions described in \citet[Assumption 3]{buchholz2023learning}. We collect $10$K observations per environment, constituting a total of $60$K images provided as input to the method. A detailed exposition of the data-generating process is available in \cref{ss:ccrl_data_gen}.

\paragraph{Results.} In \cref{fig:ccrl_results}, we provide a summary of the results of applying CCRL to the real images and to the synthetic images (e.g., \cref{fig:images}F) produced by the deterministic simulator of the light tunnel described in \cref{s:testbed} and \cref{s:simulator}. We report the same metrics used in the original experiments of the method: the commonly used mean correlation coefficient \citep[MCC, ][]{hyvarinen2016unsupervised, khemakhem2020variational}, which is a measure of how well the method recovers the latent factors, and the structural Hamming distance \citep[SHD, ][]{tsamardinos2006max} that measures the recovery of the latent causal graph.

The method obtains fairly good metrics on the data from the deterministic simulator, indicating that this is a setting where most necessary assumptions for CCRL are met, allowing the method to recover the latent variables and graph reasonably well. However, the method exhibits a stark drop in performance when applied to the real data, failing to recover the latent variables and returning vastly different latent graphs in each of its five random initializations. Since the main difference between these two settings is the determinism of the mixing function (see \cref{s:testbed}), we conclude that CCRL is highly sensitive to noise in the mixing process. A possible explanation is that CCRL relies on detecting interventions in the latent space, a sensitive statistical problem for which it may lack power in the case of a noisy mixing process.

\subsection{Multiview CRL}
\label{ss:multiview}

\begin{figure}[t]
    \centerline{
\includegraphics[height=41mm]{figures/views.pdf}
}
\caption{Ground-truth graph relating the underlying causal factors, shown in bold print, to the different views employed in the Multiview CRL experiment. The views are disjoint sets of the output variables produced by the tunnel. The separate factors $(R,G,B)$ are shown as a tuple to avoid drawing additional edges. The graph is a subgraph of the complete causal ground-truth graph for the light tunnel, described in \citet[\href{https://www.nature.com/articles/s42256-024-00964-x/figures/3}{Figure 3b}]{gamella2025chamber}.}
\label{fig:views}
\end{figure}

\begin{figure*}[t]
\vskip 0.15in
    \centering
    \includegraphics[width=170mm]{figures/multiview_results.pdf}
    \vskip 0.15in
    \caption{Experimental results for the multiview CRL method over five random initializations. \textbf{(A, B, C):} Average $R^2$-score of predicting the ground-truth factors from the learned representation across different pairs of views. Success is indicated by simultaneously attaining a score near one for the factors in the content block and a significantly lower score for those in the style block. While content variables are consistently predicted better than style variables, the model fails to disentangle $\theta_1$ and $\theta_2$, evident in panel B \& C. \textbf{(D):} Scatter plot of the ground-truth factor $\theta_2$ vs. the corresponding sensor measurement in view 4 ($\tilde{\theta}_2$). Even though this view and the ground-truth factor are almost perfectly correlated, the model fails to recover the underlying factor $\theta_2$.}
    \label{fig:mv_real}
\end{figure*}

The multiview approach to CRL \citep{gresele2019incomplete, locatello2020weakly, vonkugelgen2021self, daunhawer2023identifiability, ahuja2024multi, xu2024sparsity, yao2024multi} relies on having different sets of observations, or views, that arise from mixtures of different subsets of the underlying causal factors. We choose the method introduced by \citet{yao2024multi} as a representative of this family of approaches.

\paragraph{Background.} As opposed to CCRL, the method from \citet{yao2024multi} places more flexible assumptions on the distribution of the underlying causal factors, where any smooth, continuous distribution with a positive density almost everywhere is allowed \citep[Assumption 2.1]{yao2024multi}. The underlying factors are transformed into observations via multiple diffeomorphic mixing functions that share (subsets of) these factors as their inputs, resulting in multiple views on the underlying causal model. For our testbed, we group the images and sensor measurements produced by the light tunnel into different views, as shown in \cref{fig:views}. Given a set of views, the goal of the method is to recover the underlying factors in the intersection of their inputs. These shared factors are referred to as the ``content block'' of a set of views, whereas those not in the intersection of inputs are called the ``style block''. Regarding identifiability, the theory guarantees partial (or block-) identifiability \citep{lachapelle2022partial} of the ground-truth factors in the content block up to an invertible transformation. The information about the factors in the style block is discarded as the model learns ``[...] \emph{all and only} information about [the content block]" \citep[Definition 2.3]{yao2024multi}.

\paragraph{Data generation.} To generate the ground-truth causal factors, we employ the same sampling procedure as for CCRL. Due to its flexible constraints on the underlying factors, the distribution entailed by the SCM and the interventions falls under the method's assumptions, and we pool the observations from the different interventional environments. See \cref{ss:multi_data} for an in-depth description of the data-generating process. The input to the method consists of $60$K observations, whose variables are grouped into the four views shown in \cref{fig:views}. The first view is the images ($\tilde{\text{I}}\text{m}$) captured by the camera, which depends on all ground-truth factors ($R, G, B, \theta_1, \theta_2$). The second view comprises the combined measurements of the current drawn by the light source ($\tilde{C}$) and the light intensity at the first two sensors ($\tilde{I}_1, \tilde{V}_1, \tilde{I}_2, \tilde{V}_2$); these depend only on the light-source setting ($R, G, B$). The third and fourth views are the measurements of the two angle sensors $\tilde{\theta}_1$ and $\tilde{\theta}_2$. These are simple linear functions (e.g., \cref{fig:mv_real}D) of the polarizer positions $\theta_1$ and $\theta_2$, respectively. In line with the experiments in the original paper \citep[Section 5.3]{yao2024multi}, during training we rely on the ground-truth content selection of the shared encodings, as opposed to learning it. More details can be found in \cref{ss:multi_data}.

\paragraph{Results. } In \cref{fig:mv_real}, we display the results of applying the multiview CRL method \citep{yao2024multi} on both the real data and the synthetic ablation described in \cref{s:testbed}. We report the same metric used in the original paper: the $R^2$ score \citep{wright1921correlation} of predicting each ground-truth factor using a non-linear estimator, fit on the encodings returned by the method for each view, and averaged for each pair of views shown in panels A-C of \cref{fig:mv_real}. Further details about the metric can be found in \cref{sss:mv_metric}. 

Success is indicated by simultaneously attaining a high score (near one) for the underlying factors shared by both views (the content block) and a score significantly lower than one for those factors that are not (the style block). In the case where the underlying factors are independent, the score for the style is expected to be near zero, but given the correlation between variables induced by the underlying SCM, this is not the case here. While the method consistently recovers the variables in the content block better than those in the style block, the difference in scores is not large,  as is particularly evident for the intersections of view 1 with views 3 and 4, where this difference is barely noticeable (\cref{fig:mv_real} B and C, respectively). This implies that the method fails to separate the information between the content and style variables, which is one of the method's goals \citep[Definition 2.3]{yao2024multi}. It is noteworthy that there is no prominent decrease in performance from the synthetic to the noisy real data. A hypothesis for this phenomenon lies in the subtlety of the trace of the polarizer angle information ($\theta_1, \theta_2$) in the image data and the model's failure to learn this pattern in the image encoder. Given the fact that image data is captured in distinct red, green and blue channels, it is unsurprising the ground-truth factors ($R,G,B$) are more easily learned by the image encoder than the polarizer angles. However, as we show in \cref{s:supervised} the information of the angles is indeed present in the images, giving us no reason to believe this task to be fundamentally ill-posed.

\subsection{CRL from temporal intervened sequences}
\label{ss:temporal}

\begin{figure*}
\vskip 0.15in
    \centering
    \centering
    \begin{minipage}[c]{0.42\textwidth}
        \centering
        \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{ccc}
    \toprule
         & $R^2$ diag $\uparrow$ & $R^2$ sep $\downarrow$\\
         \midrule
        Real & $0.092 \pm 0.056$ & $0.620 \pm 0.091$\\
        Synth. ablation & $0.120 \pm 0.068$ & $0.636 \pm 0.107$\\
    \bottomrule
    \end{tabular}
    \end{minipage}%
    \hfill
    \begin{minipage}[c][][b]{0.54\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/citris_r2.pdf}
    \end{minipage}
    \vskip 0.15in
    \captionsetup{width=172mm}
    \caption{Results for the CITRIS method on real and and the synthetic ablation as described in \cref{s:testbed}. \textbf{Left:} $R^2$ correlation metric over five random initializations. We report the average and ($\pm$) the standard deviation. The \emph{diag} columns refer to the average value of the diagonal of the $R^2$ correlation matrix (shown on the left) while the \emph{sep} columns denote the maximum off-diagonal value. \textbf{Right:} Average $R^2$ correlation matrices for real and synthetic ablation datasets. NA refers to the group of learned latents that are not assigned to any ground-truth causal factor during training \citep[Section 3.4]{lippe2022citris}. While there is a minor drop in performance from synthetic to the real data, this is negligible and the method fails to recover the ground-truth factors in both cases, as indicated by the low scores along the diagonal and (relatively) high off-diagonal scores, respectively.}
    \label{tab:citris_results}
\end{figure*}

As a third group of CRL approaches, we consider methods that exploit the causal temporal structure of a process evolving in time \citep{yao2021learning, yao2022temporally, lippe2022citris, lippe2022icitris, lippe2023biscuit, lachapelle2024nonparametric}. To represent this group of methods, we select CITRIS \citep{lippe2022citris}, for which well-documented code is publicly available.

\paragraph{Background.} CITRIS assumes that the underlying causal factors follow a first-order Markov, stationary dynamic Bayesian network \citep[DBN, ][]{thomas1989model, murphy2002dynamic} without instantaneous causal effects \citep[Section 2]{lippe2022citris}. Furthermore, the method assumes to have access to information about which underlying causal factor has undergone an intervention at each time step \citep[Section 3.1]{lippe2022citris}. In contrast to most other methods, CITRIS allows for multidimensional causal ground-truth variables. It explicitly models observations as variables with noise in its theoretical assumptions \citep[Section 3.1]{lippe2022citris}. 

\paragraph{Data generation.} We generate the ground-truth causal factors by sampling from the temporal causal process described in \cref{ss:citris_data}. As in the experiments of the original paper \citep[Appendix C.2]{lippe2022citris}, at each time step we randomly intervene on one of the factors (also including the possibility of intervening on none), and record the vector of the intervention targets at each time step. The original paper uses an additional loss metric \citep[the ``triplet" loss,][Section 6.1]{lippe2022citris} for model selection during training, which requires access to the ground-truth factors and requires the collection of an additional dedicated dataset. Since collecting data on a physical system is costly, and assuming access to the ground-truth factors during training is unrealistic, we forego the use of this metric. We use the CITRIS-VAE variant of this method \citep[Section 4.1]{lippe2022citris} for all experiments.

As in the original paper \citep[Section 6.1]{lippe2022citris}, we generate an additional dataset from independently sampled underlying factors to calculate the final metrics: the $R^2$ \citep{wright1921correlation} and Spearman's rank correlation coefficients \citep{spearman1904proof}, which show how well the model can predict the ground-truth factors from the latents learned during training. Please see \cref{sss:citris_metric} for details on these metrics.

\paragraph{Results.} We present the results for the $R^2$ score in \cref{tab:citris_results}; the results for the Spearman's rank coefficient are provided in \cref{sss:citris_add_fig}. Both metrics---employed in the original paper---produce a correlation matrix between the learned latents and the ground-truth factors. An optimal model displays values close to one on the diagonal and zeros everywhere else (see \cref{sss:citris_metric} for details). The results indicate that CITRIS fails catastrophically in recovering the ground-truth factors, both from the real data and the ablation with synthetic images and measurements. The metrics along the diagonal display a low score, while the off-diagonal values are relatively higher. This suggests that the model does not learn an encoding that separates the ground-truth causal factors nor a correct mapping between latents and causal factors.

CITRIS is a complex method with many sub-modules that interact during training. We hypothesize that a failure in one of these modules, such as the encoder struggling to extract information about one of the latents, can compound and drive down the method's performance. Although CITRIS and its variants have shown favorable performance on several visually complex synthetic datasets \citep{lippe2022citris, lippe2022icitris, lippe2023biscuit}, the exact reason for the failure in this setting remains elusive. Further analysis to pinpoint where things go wrong would require an in-depth study and additional ablations on the underlying data-generating process, e.g., by considering a simpler benchmark with only $R, G$, and $B$ as ground-truth factors. We believe this to be beyond the scope of this work.

\section{Discussion}
\label{s:discussion}

We have constructed a testbed for causal representation learning using a real and tightly controlled physical system built around a well-understood optical experiment. The data-generating process of this system satisfies the core assumptions of causal representation learning, and the straightforward mixing process is described by simple physical principles. Furthermore, we have access to ground-truth labels of the underlying causal factors. Therefore, the testbed serves as a sanity check where a CRL method that is expected to work on a variety of real-world scenarios should also succeed. A failure on this testbed indicates a potential failure on other, more complex real-world systems. However, the opposite does not hold, as success on this simple and controlled testbed is not guaranteed to carry over to more complex scenarios.

As a first application of our testbed, we evaluated three methods representative of different approaches to causal representation learning: contrastive CRL, multiview CRL, and CRL from temporal intervened sequences. Due to the different setups, assumptions, and goals of each method, we performed our analysis on a method-by-method basis. However, some general patterns emerge. First, all methods failed to attain their goals in recovering---up to their corresponding theoretical constraints---the ground-truth causal factors. Except for a single sub-result of the multiview method, the failure is catastrophic, and the output of the methods has a very weak or no association with the ground-truth causal factors. To better understand the reasons for this failure, we performed a synthetic ablation on the data by constructing a deterministic simulator to substitute the real-world data-generating process of the light tunnel.

However, save for the CCRL method, performance on this simpler synthetic data was not significantly better than on the real data. This highlights a deeper problem, underscoring the fragility of these methods and their dependence on pre-processing steps and other implementation decisions, such as model architectures, training regimes, and hyperparameter values. This constitutes a significant challenge for reproducibility and, ultimately, the application of these methods to real-world problems.

\subsection{Outlook}

Although we advocate for the use of real-world data, we believe that synthetic data is crucial for the development and validation of new methods. However, there is an inherent conflict of interest if the validation of a new method is limited to synthetic data produced by its authors.
Method-agnostic benchmarks have been catalysts of progress in various other fields \citep{lecun1998gradient, deng2009image, rajpurkar2016squad, runge2019inferring, lin2022truthfulqa}, especially if they include or resemble data that we may find in the real world. Thus, we applaud existing efforts in this regard, including the synthetic benchmarks mentioned throughout the manuscript \cite{lippe2022citris,lippe2022icitris,ahmed2021world,vonkugelgen2021self,liu2023triplet}.

While the community acknowledges that applications of CRL to real-world problems are lacking, the response has largely been to push the theoretical frontier and relax the assumptions needed to establish identifiability results.
We argue that, in many regards, the theory in the field is already quite advanced and is, therefore, not the only bottleneck. It is equally important to make a real effort to implement and apply the existing theory in a robust, reproducible, and efficient manner.
Without pursuing the engineering endeavor to develop actionable algorithms from the existing theory, we cannot hope to know where the true obstacles to application lie.

Unless we abandon the ultimate goal of applying causal representation learning to real-world problems, we cannot be satisfied with validating our methods on purely synthetic data. Our goal with this physical testbed is to provide researchers in the field with a stepping stone toward more complex real-world scenarios, with the hope of breaking down this hard problem into manageable steps.

The datasets we provide constitute a small fraction of all experiments possible with the light tunnel, which offers additional control inputs not described here that can be used to construct more complex tasks. We are open to suggestions for additional experiments that may prove useful---please reach out to the corresponding authors.

\section*{Acknowledgements}

We thank Francesco Locatello for valuable discussions, and Simon Buchholz and Dingling Yao for providing support for their respective model implementations. S.B. and J.R. received funding for this project from the European Research Council (ERC) Starting Grant CausalEarth under the European Union‚Äôs Horizon 2020 research and innovation program (Grant Agreement No. 948112). S.B. received support from the German Academic Scholarship Foundation.

\section*{Impact Statement}

This paper provides a sanity check to validate advances in causal representation learning. Because our contribution pertains to fundamental and theoretical research in machine learning, the potential societal consequences are difficult to ascertain and, in any case, not immediate. Thus, we do not feel any ethical concerns must be specifically highlighted here.

\newpage
\bibliography{refs}
\bibliographystyle{icml2025}

\newpage
\appendix
\onecolumn
\section{Experimental Details}
\label{s:exp_details}
In this section we provide details on all data generation procedures, hyperparameters, training procedures, and minor adjustments made to the original implementations required to reproduce the results of this work. Our general approach was to use each method's existing code as out-of-the-box as possible, and almost all hyperparameters and settings are the same as reported in the original papers. However, slight modifications were inevitable in making the code run for our data, and we thank the authors of the methods for their guidance. All implementations use the PyTorch machine learning library \citep{paszke2019pytorch}. The data used in this work is available through the \href{https://github.com/juangamella/causal-chamber/tree/main/datasets/lt_crl_benchmark_v1}{\nolinkurl{lt_crl_benchmark_v1}} dataset at \href{https://github.com/juangamella/causal-chamber}{\nolinkurl{github.com/juangamella/causal-chamber}} and the code, including all implementations of the considered methods, is available at \url{https://github.com/simonbing/CRLSanityCheck}.

\paragraph{Computational resources and compute time.} All experiments were run on a high-performance cluster with NVIDIA A100 GPUs. The total compute time required to reproduce all experiments in this work is approximately 100 GPU hours.


\begin{figure}[h]
    \centering
    \includegraphics[width=30mm]{figures/ccrl_gt_graph.pdf}
    \caption{The ground-truth graph for the Contrastive CRL dataset.}
    \label{fig:ccrl_gt_graph}
\end{figure}
\subsection{Contrastive CRL}
\subsubsection{Data Generation}
\label{ss:ccrl_data_gen}
We define a linear SCM with additive Gaussian noise between the variables ($R, G, B, \theta_1, \theta_2$) with causal relations encoded by the graph shown in \cref{fig:ccrl_gt_graph}. The independent Gaussian noise variables all have zero mean and a variance sampled from $\mathcal{U}([0.01, 0.02])$.
To generate interventional data, we consider interventions that shift the mean of their target by adding a factor $\eta$. We sample the value of the shift $\eta$ from $\mathcal{U}([1,2])$ for all interventions.
Each variable is intervened upon individually and we collect $n = 10000$ samples from each intervention, as well as the observational distribution. The full dataset is then split into train, validation, and test subsets according to the ratios (80/10/10) while ensuring that each subset contains the same fraction of samples from each environment.

\subsubsection{Training Details}
For our experiments with the CCRL method, we rely on the code kindly provided by the authors of the original work. As the original implementation only included a shallow convolutional encoder for images, we found it useful to extend the image encoder to a slightly deeper architecture, which we report in \cref{tab:ccrl_enc}. Otherwise, we use the exact same architecture and implementation as in the original paper. We report the hyperparameters used during training in \cref{tab:ccrl_hp}. 

Our initial experiments revealed it was crucial that the noise terms of the underlying SCM have mean zero. Attempts with a non-zero mean led to the failure of the method.

\subsubsection{Metrics}
\label{sss:ccrl_metric}

\paragraph{Mean Correlation Coefficient.} The Mean Correlation Coeefficent \citep[MCC, ][]{hyvarinen2016unsupervised, khemakhem2020variational} measures the pairwise correlation between the learned representation and the ground truth latents. If $\mathbf{C} \in \mathbb R^{d \times d}$ is the Pearson correlation matrix between the ground truth variables and the learned embeddings and $\mathfrak{S}_d$ denotes the set of $d$-permutations, the MCC score is formally defined as
\begin{align*}
    \textnormal{MCC} := \max_{\pi \in \mathfrak{S}_d} \frac{1}{d} \sum^d_{j=1} \lvert \mathbf{C}_{j, \pi(j)} \rvert.
\end{align*}

\paragraph{Structural Hamming Distance.} The Structural Hamming Distance \citep[SHD, ][]{tsamardinos2006max} measures how well a method recovers the ground-truth causal graph by comparing the adjacency matrices of the ground-truth graph $\mathbf{A} \in \{0, 1\}^{d \times d}$ and the learned graph $\hat{\mathbf{A}} \in \{0, 1\}^{d \times d}$, and counting the number of edges that are different. Incorrectly oriented edges count as two errors. We follow the convention that for an adjacency matrix $\mathbf{A}$, a nonzero entry $\mathbf{A}_{i, j} \neq 0$ implies an edge $i \rightarrow j$. Since the Contrastive CRL method outputs a continuous estimate of the adjacency matrix, it must the thresholded to obtain binary entries. We follow the heuristic in the original paper and choose the thresholding that results in the same number of edges as the ground truth $\mathbf{A}$. Naturally, this is not possible when the ground-truth graph is not known. The SHD is then defined as
\begin{equation*}
    \textnormal{SHD} := \sum_{i, j} \lvert \mathbf{A}_{i, j} - \hat{\mathbf{A}}_i,j \rvert.
\end{equation*}

\begin{table}[h]
    \centering
    \caption{Convolutional encoder architecture for Contrastive CRL.}
    \vskip 0.15in
    \begin{tabular}{ll}
    \toprule
        \textbf{Layer} & \textbf{Activation function}\\
    \midrule
        Conv (c\_in $=$ 3, c\_out $=$ 64, kernel $=$ 3, stride $=$ 2) & GroupNorm+SiLU\\
        Conv (c\_in $=$ 64, c\_out $=$ 64, kernel $=$ 3, stride $=$ 1) & GroupNorm+SiLU\\
        Conv (c\_in $=$ 64, c\_out $=$ 64, kernel $=$ 3, stride $=$ 2) & GroupNorm+SiLU\\
        Conv (c\_in $=$ 64, c\_out $=$ 64, kernel $=$ 3, stride $=$ 1) & GroupNorm+SiLU\\
        Reshape (1 $\times$ 1 $\times$ (16 $\cdot$ 16 $\cdot$ 64)) & -\\
        Linear ((16 $\cdot$ 16 $\cdot$ 64) $\times$ 1024) & LayerNorm+SiLU\\
        Linear (1024 $\times$ 5) & -\\
    \bottomrule
    \end{tabular}
    \label{tab:ccrl_enc}
\end{table}

\begin{table}[h]
    \centering
    \caption{Contrastive CRL hyperparameters.}
    \vskip 0.15in
    \begin{tabular}{ll}
         \toprule
        \textbf{Parameter} & \textbf{Value} \\
         \midrule
        Model latent dimension & 5\\
        Learning rate & 5e-4\\
        Optimizer & Adam \citep{kingma2015adam}\\
        Epochs & 100\\
        Batch size & 512\\
        Learning rate scheduler & Plateau Scheduler\\
        $\kappa$ & 0.1\\
        $\tau_1$ \citep[Eq. 9]{buchholz2023learning} & 1e-5\\
        $\tau_2$ \citep[Eq. 9]{buchholz2023learning} & 1e-4\\
         \bottomrule
    \end{tabular}
    \label{tab:ccrl_hp}
\end{table}

\newpage
\begin{figure}[h]
    \centerline{
\includegraphics[height=41mm]{figures/views.pdf}
}
\caption{Ground-truth graph relating the latent causal factors, shown in bold print, to the different views employed in the Multiview CRL experiment. The views are disjoint sets of the output variables produced by the tunnel. The graph is a subgraph of the complete causal ground-truth graph for the light tunnel, described in \citet[\href{https://www.nature.com/articles/s42256-024-00964-x/figures/3}{Figure 3b}]{gamella2025chamber}.}
\label{fig:views_app}
\end{figure}
\subsection{Multiview CRL}
\subsubsection{Data Generation}
\label{ss:multi_data}

The underlying causal model used in our experiments for the Multiview CRL method is identical to the one used for the Contrastive CRL method described in \cref{ss:ccrl_data_gen}. In addition to first view given by the images ($\tilde{\text{I}}\text{m}$)---which depend on ($R, G, B, \theta_1, \theta_2$)---we consider three additional views. The second view consists of the electrical current drawn by the light source and the infrared and visible light intensities from sensors one and two, i.e., ($\tilde{C}, \tilde{I}_1, \tilde{V}_1, \tilde{I}_2, \tilde{V}_2$). These depend on the underlying causal factors ($R, G, B$). The third and fourth views consist of the angle measurements of the first ($\tilde{\theta}_1$) polarizer and second polarizers, respectively. In turn, they depend on the corresponding polarizer angles ($\theta_1$) and ($\theta_2$). A graphical overview of the data-generating process is shown in \cref{fig:views_app}. These views consist of either real images and sensor measurements collected from the tunnel, or their synthetic and deterministic counterparts produced by the simulator given in \cref{s:simulator}. To increase the overall sample size, we include observational samples, as well as all interventional samples described in \cref{ss:ccrl_data_gen}, arriving at a dataset with $n=60000$ samples. The data is randomly split into train, validation, and test sets according to a 80/10/10 ratio.

\subsubsection{Training Details}
We use the publicly available implementation of the Multiview CRL method from \href{https://github.com/CausalLearningAI/multiview-crl}{\texttt{https://github.com/CausalLearningAI/multiview-crl}}. For our experiments, we use the variant of this implementation that relies on BarlowTwins \citep{zbontar2021barlow}, in accordance with the experiments that use image data in the original paper. The hyperparameter settings used in our experiments are reported in \cref{tab:mcrl_hp}. The number of training steps is chosen to obtain approximately the same number of epochs given our chosen batch size as the original paper uses for their image data experiments.

\begin{table}[h]
    \centering
    \caption{Multiview CRL hyperparameters.}
    \vskip 0.15in
    \begin{tabular}{ll}
         \toprule
        \textbf{Parameter} & \textbf{Value} \\
         \midrule
        Model latent dimension & 5\\
        Learning rate & 1e-4\\
        Optimizer & Adam \citep{kingma2015adam}\\
        Nr. of training steps & 28550\\
        Batch size & 512\\
        Similarity metric & Cosine similarity\\
         \bottomrule
    \end{tabular}
    \label{tab:mcrl_hp}
\end{table}

\subsubsection{Metrics}
\label{sss:mv_metric}
The theory of the Multiview CRL method states that the model achieves \emph{block-identifiability} \citep[Definition 2.3]{yao2024multi}, meaning that the information of the ground-truth variables belonging to the content block of a set of views is perfectly recovered by the model, while the information of the style variables belonging to this set of views is ignored. The authors measure recovery by looking at the $R^2$ score of an estimator trained to predict the values of the individual ground-truth latents from specific subsets of the learned representation. For each view in a given set, the view-specific encoder returns an encoded latent vector $\hat{\mathbf{z}} \in \mathbb R^d$. Let $[d] = (1, ..., d)$ denote the set of integers from one to $d$. Then, a subset of the view-specific encoding vector $\hat{\mathbf{z}}_{S}, S \subseteq [d]$ is selected as input to the estimator that predicts the learned latent variables. This subset differs between distinct sets of views and corresponds to what the model learns as the \emph{content-block} of that respective set of views. For each latent variable and view in a given set, we record the $R^2$ score \citep{wright1921correlation} of the estimator fit according to the aforementioned procedure, averaging the scores over the encodings obtained by different view-specific encoders in a given set. According to the theory, if the model achieves block identifiability, this metric will be close to one for the variables belonging to the content block in a set of views, while being significantly lower for variables that belong to the style partition.

\subsection{CITRIS}
\subsubsection{Data Generation}
\label{ss:citris_data}
We generate the data for the CITRIS experiments by defining a time-evolving process given by the following time-series graph and assignments:
\begin{center}
    \begin{minipage}[c]{0.42\textwidth}
        \centering        
            \begin{tikzpicture}
        \node[circle, inner sep=0.12em] (rt) at (1,-1.2) {$R^t$};
        \node[circle, inner sep=0.12em] (gt) at (0,1.2) {$G^t$};
        \node[circle, inner sep=0.12em] (bt) at (1,1.2) {$B^t$};
        \node[circle, inner sep=0.12em] (2t) at (2,1.2) {$\theta_2^t$};
        \node[circle, inner sep=0.12em] (1t) at (3,1.2) {$\theta_1^t$};
        

        \node[circle, inner sep=0.12em] (rtt) at (2.2,-1.2) {$R^{t+1}$};
        \node[circle, inner sep=0.12em] (gtt) at (0,0) {$G^{t+1}$};
        \node[circle, inner sep=0.12em] (btt) at (1,0) {$B^{t+1}$};
        \node[circle, inner sep=0.12em] (2tt) at (2,0) {$\theta_2^{t+1}$};
        \node[circle, inner sep=0.12em] (1tt) at (3,0) {$\theta_1^{t+1}$};
        
        \begin{scope}[]
            \draw[->] (rt) edge[] (rtt);            
            \draw[->] (gt) edge[] (gtt);
            \draw[->] (bt) edge[] (btt);
            \draw[->] (1t) edge[] (1tt);
            \draw[->] (2t) edge[] (2tt);

            \draw[->] (rt) edge[] (btt);
            \draw[->] (rt) edge[] (gtt);
            \draw[->] (rt) edge[] (2tt);

            \draw[->] (1t) edge[] (2tt);
            \draw[->] (bt) edge[] (2tt);
        \end{scope}
    \end{tikzpicture}
    \end{minipage}%
    \hfill
    \begin{minipage}[c][][b]{0.54\textwidth}
    \begin{align*}
        R^{t+1} \leftarrow\;& f\left(R^t + \epsilon_R\right)\\
        G^{t+1} \leftarrow\;& f\left(G^t + \frac{R^t - G^t}{2} + \epsilon_G\right)\\
        B^{t+1} \leftarrow\;& f\left(B^t + \frac{G^t - B^t}{4} + \epsilon_B\right)\\
        \theta_1^{t+1} \leftarrow\;& g\left(\theta_1^t + \epsilon_1\right)\\
        \theta_2^{t+1} \leftarrow\;& g\left(\theta_2^t + S(R^t, B^t)\left(\frac{\theta_1^t - \theta_2^t}{4}\right)\epsilon_2\right),\\
    \end{align*}
    \end{minipage}
\end{center}%
where the random innovations are sampled as $\epsilon_R, \epsilon_G, \epsilon_B \overset{\text{i.i.d.}}{\sim} \text{Unif}[-50,50]$, $\epsilon_1 \overset{\text{i.i.d.}}{\sim} \text{Unif}[-10,10]$, and $\epsilon_1 \overset{\text{i.i.d.}}{\sim} \text{Unif}[-5,5]$. Furthermore, the functions $S, f$ and $g$ are defined as
$$S(R^t, B^t) := \begin{cases}1 &\text{ if } B^t > R^t\\-1&\text{ if } B^t \leq R^t\end{cases}
\text{ , }
\; f(x) := \begin{cases}-x&\text{ if } x < 0\\510 - x &\text{ if } \geq 255\end{cases},
\; \text{and }g(x) := \begin{cases}-180-x&\text{ if } x < -90\\180 - x &\text{ if } x\geq 90\end{cases}.
$$
The purpose of $f$ and $g$ is to ensure that the values of $R^t, \ldots, \theta_2^t$ remain within the bounds of the control inputs for the light tunnel ($[0,255]$ for $R,G,B$ and $[-90,90]$ for $\theta_1, \theta_2$).

Intuitively, the process can be understood as follows: the variables $R$ and $\theta_1$ follow random walks. $G^t+1$ tries to approach $R^t$ in each step, and $B^t+1$ approaches $G^t$ in the same fashion. However, the innovations $\epsilon_R, \epsilon_G$, and $\epsilon_B$ prevent these variables from converging to their targets. Finally, $\theta_2$ tries to exploit the color-shifting effect of the polarizers to maximize the ratio of the blue to red pixels in the resulting images. It does so by approaching $\theta_1^t$---aligning the polarizer axes---if $B^t > R^t$, and receding if $R^t \geq R^t$. The behavior is regulated by the function $S(\cdot)$ above. At each time step, we perform no intervention with probability $0.3$, or we select an intervention target at random from $\{R^t, G^t, B^t, \theta_1^t, \theta_2^t\}$, setting it to a value sampled uniformly at random from its valid range. We encode the intervention target at each time step in a one-hot vector, with a vector of zeroes indicating that no variable is intervened upon. A Python implementation of the process is provided in \href{https://github.com/juangamella/causal-chamber/blob/main/datasets/lt_crl_benchmark_v1/generators/citris_1.py}{\nolinkurl{github.com/juangamella/causal-chamber/blob/main/datasets/lt_crl_benchmark_v1/generators/citris_1.py}}.

We collect a sequence of $n=100000$ samples from this process and use the first $n_{\text{train}} = 80000$ samples as the training dataset and the following $n_{\text{val}} = 10000$ and $n_{\text{test}} = 10000$ as validation and test datasets. Following the experiments in the original paper, to evaluate the final metrics we collect an additional dataset with $n = 1000$ samples, where the latent variables are sampled uniformly at random from their valid range.

\subsubsection{Training Details}
For the CITRIS experiments we rely on the code that is publicly available at \href{https://github.com/phlippe/CITRIS}{\texttt{https://github.com/phlippe/CITRIS}}. The hyperparameter settings we used in our experiments are shown in \cref{tab:citris_hp}. As we mention in the main text, we opted not to use the method's triplet loss for model selection, as this relies on accessing ground-truth latent values during training, which we find highly unrealistic. We replaced this validation loss with the same loss function used during training. Calculating the correlation metrics at intermediate steps during training does not suggest that this leads to a suboptimal model selection in terms of the final metric.

During initial experiments, we found that a number of different choices did not result in any improvement of the final results. We tried using the auto-regressive prior that does not rely on normalizing flows (as opposed to the one that does), setting the latent dimension of the model to match that of the ground truth, fixing the assignment of the model latents to the causal variables instead of learning it, as well as training for 500 instead of 250 epochs. None of these choices resulted in an improved final score.

\begin{table}[h]
    \centering
    \caption{CITRIS hyperparameters.}
    \vskip 0.15in
    \begin{tabular}{ll}
         \toprule
        \textbf{Parameter} & \textbf{Value} \\
         \midrule
         Model latent dimension & 16 \\
        Learning rate & 1e-4\\
        Optimizer & Adam \citep{kingma2015adam}\\
        Epochs & 250\\
        Batch size & 512\\
        Learning rate scheduler & Cosine Warmup (25 steps)\\
         \bottomrule
    \end{tabular}
    \label{tab:citris_hp}
\end{table}

\subsubsection{Metrics}
\label{sss:citris_metric}
Similar to the MCC score, the authors of CITRIS report the correlation between the learned representation and the ground-truth variables. However, since CITRIS allows for multiple learned latents to describe a single ground-truth variable, their model first learns a mapping that assigns latent dimensions to a ground-truth factor. To compute the final metrics, an MLP is trained to predict each ground-truth variable from the learned groupings of model latents. Each ground-truth factor is predicted from each respective grouping, resulting in a matrix. The authors then report the $R^2$ \citep{wright1921correlation} as well as Spearman's rank correlation coefficient \citep{spearman1904proof} for this matrix. As a summary metric, the diagonal of the resulting matrices is examined, considering its average value, which should be close to one in the optimal case, as well as the maximal off-diagonal value, which is expected to be close to zero.

\subsubsection{Additional Figures}
\label{sss:citris_add_fig}
We present additional results of the CITRIS experiments (cf. \cref{ss:temporal}) using Spearman's rank correlation coefficient in \cref{fig:citris_results_spear}. We see that this metric echoes the findings presented in \cref{tab:citris_results}.

\begin{figure*}
    \centering
    \centering
    \begin{minipage}[c]{0.42\textwidth} % Left half for the table
        \centering
        \renewcommand{\arraystretch}{1.2} % Adjust row height for better readability
    \begin{tabular}{ccc}
    \toprule
         & Spearman diag $\uparrow$ & Spearman sep $\downarrow$\\
         \midrule
        Real & $0.230 \pm 0.058$ & $0.801 \pm 0.052$\\
        Synth. ablation & $0.245 \pm 0.055$ & $0.809 \pm 0.072$\\
    \bottomrule
    \end{tabular}
    \end{minipage}%
    \hfill
    \begin{minipage}[c][][b]{0.54\textwidth} % Right half for the figure
        \centering
        \includegraphics[width=0.9\textwidth]{figures/citris_spearman.pdf} % Replace with your figure file
    \end{minipage}
        \captionsetup{width=172mm}
    \caption{Results for the CITRIS method on real and and the synthetic ablation as described in \cref{s:testbed}. \textbf{Left:} Spearman's rank correlation coefficient over five random initializations. We report the average and ($\pm$) the standard deviation. The \emph{diag} columns refer to the average value of the diagonal of the Spearman correlation matrix while the \emph{sep} columns denote the maximum off-diagonal value. \textbf{Right:} Average Spearman's rank correlation matrices for real and synthetic ablation datasets. NA refers to the group of learned latents that are not assigned to any ground-truth causal factor during training \citep[Section 3.4]{lippe2022citris}. Analogously to the $R^2$, the results here show that the method fails fails to recover the ground-truth factors both for real and synthetic data, as indicated by the low scores along the diagonal and (relatively) high off-diagonal scores, respectively.}
    \label{fig:citris_results_spear}
\end{figure*}

\newpage
\section{Supervised Sanity Check}
\label{s:supervised}
To ensure that the information of the latent variables ($R, G, B, \theta_1, \theta_2$) can indeed be recovered from the image data, we train a small MLP (architecture cf. \cref{tab:supervised_arch}) in a supervised fashion to predict the ground-truth causal factors from images on a held-out test set. We train on a subset of 5000 samples and test on 500 samples. Our results (cf. \cref{tab:supervised}) indicate that the latent variables do indeed leave a detectable trace in the data. We stress that this is merely a sanity check to exclude that our data is collected in a contrived way. The near-perfect performance of this supervised model should not be compared with the unsupervised methods we investigate in the main paper.

\begin{table}[h]
    \centering
    \caption{Test score of a supervised model trained to predict latents from images for the respective datasets.}
    \vskip 0.15in
    \begin{tabular}{ccc}
    \toprule
         & CCRL \& Multiview CRL Dataset & CITRIS Dataset\\
         \midrule
        $R^2$ & 0.976 & 0.914 \\
        \bottomrule
    \end{tabular}
    \label{tab:supervised}
\end{table}


\begin{table}[h]
    \centering
    \caption{Supervised MLP network architecture.}
    \vskip 0.15in
    \begin{tabular}{ll}
    \toprule
        \textbf{Layer} & \textbf{Activation function}\\
        \midrule
        Linear ((3 $\cdot$ 64 $\cdot$ 64) $\times$ 64) & LeakyReLU(0.01)\\
        Linear (64 $\times$ 256) & LeakyReLU(0.01)\\
        Linear (256 $\times$ 256) & LeakyReLU(0.01)\\
        Linear (256 $\times$ 64) & LeakyReLU(0.01)\\
        Linear (64 $\times$ 5) & -\\
        \bottomrule
    \end{tabular}
    \label{tab:supervised_arch}
\end{table}

\newpage
\section{Deterministic Simulator of the Light Tunnel}
\label{s:simulator}

In this section, we describe the deterministic simulator used for the synthetic ablations of the light-tunnel data in \cref{s:results}. The simulator consists of two components: a mechanistic model of the sensors (\cref{ss:sensors_sim}), derived from first principles and the technical datasheets of the sensors, and a neural simulator to produce synthetic images (\cref{ss:images_sim}). An implementation of both components, using Numpy and Pytorch, is provided by the \href{https://github.com/juangamella/causal-chamber-package}{\nolinkurl{causalchamber}} package; see simulators \texttt{lt.Deterministic} and \texttt{lt.DecoderSimple} in the \href{https://github.com/juangamella/causal-chamber-package/tree/main/causalchamber/simulators}{simulator index} for the source code and Jupyter notebooks with examples on how to run them.

For completeness, we provide a diagram of the light tunnel in \cref{fig:tunnel_appendix}, including additional variables used by the simulator that are not shown in \cref{fig:tunnel}. A detailed description of all involved variables is given in \cref{tab:sensor_vars}, and the simulator parameters are described in \cref{tab:sim_params}.

\begin{figure}[H]
\centerline{
\includegraphics[width=160mm]{figures/lt_simulator.pdf}
}
\caption{Diagram of the light tunnel with the variables involved in the deterministic simulator, where bold symbols correspond to the inputs to the simulator, and its outputs are denoted by a tilde. The inputs include variables not shown in \cref{fig:tunnel}, which control the behavior of the current sensor ($R_C$), the light-intensity sensors ($D^I_*, D^V_*, T^I_*, T^V_*$), and the angle sensors ($R_1, R_2$). All variables are described in detail in \cref{tab:sensor_vars} below, and a complete description of the light tunnel can be found in \citet{gamella2025chamber}.}
\label{fig:tunnel_appendix}
\end{figure}%

\renewcommand*{\arraystretch}{1.4}
\begin{longtable}{@{}lllp{7cm}@{}}
\caption{Description of the light tunnel variables involved in the deterministic simulator, including their symbol, value range, and the corresponding column name in the dataset files.}\label{tab:sensor_vars}\\
\toprule
Variable  & Type & Data column    & Description\\
\midrule
\endhead
\bottomrule
\endfoot
\bottomrule
\endlastfoot
%%%%%%%%%%%
$\tilde{I}_1, \tilde{I}_2, \tilde{I}_3 \in \{0,\ldots,2^{16}-1\}$
& \oup{Output} & \begin{tabular}[t]{@{}l@{}}
\nolinkurl{ir_1}\\[-1.2ex]
\nolinkurl{ir_2}\\[-1.2ex]
\nolinkurl{ir_3}
\end{tabular} & The uncalibrated measurement of the light-intensity sensors in the infrared part of the spectrum, placed before ($\tilde{I}_1$), between ($\tilde{I}_2$), and after ($\tilde{I}_3$) the polarizers, in reference to the light source.\\
%%%%%%%%%%%
$\tilde{V}_1, \tilde{V}_2, \tilde{V}_3 \in \{0,\ldots,2^{16}-1\}$
& \oup{Output} & \begin{tabular}[t]{@{}l@{}}
\nolinkurl{vis_1}\\[-1.2ex]
\nolinkurl{vis_2}\\[-1.2ex]
\nolinkurl{vis_3}
\end{tabular} & The uncalibrated measurement of the light-intensity sensors in the visible part of the spectrum, placed before ($\tilde{V}_1$), between ($\tilde{V}_2$), and after ($\tilde{V}_3$) the polarizers, about the light source.\\
%%%%%%%%%%%
$\tilde{C} \in [0,1023]$
& \oup{Output} & \nolinkurl{current} & The uncalibrated measurement of the electric current drawn by the light source. The calibrated measurement (in amperes) is given by
$$\tilde{C} \times \frac{R_C}{1023 \times 5} \times 2.5.$$\\
%%%%%%%%%%%
$\tilde{\theta}_1, \tilde{\theta}_2 \in [0,1023]$
& \oup{Output} & \begin{tabular}[t]{@{}l@{}}
\nolinkurl{angle_1}\\[-1.2ex]
\nolinkurl{angle_2}
\end{tabular} & The position of the polarizers is encoded into a voltage using a rotary potentiometer, which is then read by the control computer to produce the measurements $\tilde{\theta_1}$ and $\tilde{\theta_2}$. Given these measurements, the calibrated angle measurement is given as
$$(\tilde{\theta}_j - Z_j) \times \frac{720}{1023} \times \frac{\text{vref}(R_j)}{5} \;\text{ degrees},$$
where $R_j$ is the reference voltage of the corresponding sensor, and $Z_1 = 507, Z_2 = 512$ are the readings at angles $\theta_1 = \theta_2 = 0$ and reference voltages $R_1 = R_2 = 5$.\\
%%%%%%%%%%%
$R,G,B \in \{0,\ldots,255\}$
& \inp{Input} & \begin{tabular}[t]{@{}l@{}}
\nolinkurl{red}\\[-1.2ex]
\nolinkurl{green}\\[-1.2ex]
\nolinkurl{blue}
\end{tabular} & The brightness setting of the red, green, and blue LEDs on the tunnel's light source. Higher values correspond to higher brightness.\\
%%%%%%%%%%%
$\theta_1,\theta_2 \in \{-180,-179.9,\ldots,180\}$
& \inp{Input} & \begin{tabular}[t]{@{}l@{}}
\nolinkurl{pol_1}\\[-1.2ex]
\nolinkurl{pol_2}\\[-1.2ex]
\end{tabular}& Position of the tunnel's polarizers, in degrees.\\
%%%%%%%%%%%
$D^I_1, D^I_2, D^I_3 \in \{0,1,2\}$
& \inp{Input} & \begin{tabular}[t]{@{}l@{}}
\nolinkurl{diode_ir_1}\\[-1.2ex]
\nolinkurl{diode_ir_2}\\[-1.2ex]
\nolinkurl{diode_ir_3}
\end{tabular} & The size of the photodiode used by the light sensors to take infrared measurements, corresponding to the small $(D^I_j=0)$, medium $(D^I_j=1)$ and large $(D^I_j=2)$ photodiodes onboard.\\
%%%%%%%%%%%
$D^V_1, D^V_2, D^V_3 \in \{0,1\}$
& \inp{Input} & \begin{tabular}[t]{@{}l@{}}
\nolinkurl{diode_vis_1}\\[-1.2ex]
\nolinkurl{diode_vis_2}\\[-1.2ex]
\nolinkurl{diode_vis_3}
\end{tabular} & The size of the photodiode used by the light sensors to take visible-light measurements, corresponding to the small $(D^V_j=0)$, medium $(D^V_j=1)$ and large $(D^V_j=2)$ photodiodes onboard.\\
%%%%%%%%%%%
\begin{tabular}[t]{@{}l@{}}
$T^I_1, T^I_2, T^I_3,T^V_1, T^V_2, T^V_3 \in \{0,1,2,3\}$
\end{tabular}
& \inp{Input} & \begin{tabular}[t]{@{}l@{}}
\nolinkurl{t_ir_1/2/3}\\[-1.2ex]
\nolinkurl{t_vis_1/2/3}
\end{tabular} & The exposure time of the photodiode during a light-intensity measurement. Higher values correspond to longer exposure times.\\
%%%%%%%%%%%
$R_C,R_1,R_2 \in \{1.1, 2.56, 5\}$
& \inp{Input} & \begin{tabular}[t]{@{}l@{}}
\nolinkurl{v_c}\\[-1.2ex]
\nolinkurl{v_angle_1}\\[-1.2ex]
\nolinkurl{v_angle_2}
\end{tabular} & The reference voltage, in volts, of the sensors used to measure the current ($\tilde{C}$) and polarizer angles ($\tilde{\theta}_1, \tilde{\theta}_2$), respectively.\\
\end{longtable}

\subsection{Simulating the sensor measurements}
\label{ss:sensors_sim}

We derive a mechanistic model from first principles and the details provided in the technical datasheets of the involved components. The datasheets are provided at \href{https://github.com/juangamella/causal-chamber/tree/main/hardware/datasheets}{\nolinkurl{github.com/juangamella/causal-chamber/tree/main/hardware/datasheets}}. The expressions for the model, with color-coded variables to differentiate between the \inp{inputs}, \oup{outputs}, and \pam{parameters} of the simulator, are given below. A comparison of the resulting synthetic measurements and the true measurements collected from the light tunnel are provided in \cref{fig:comparison_sensors}.

\begin{figure}[H]
\centerline{
\includegraphics[width=110mm]{figures/sim_sensors.pdf}
}
\vspace{-0.1in}
\caption{Comparison between the real measurements (x-axis) and the corresponding synthetic measurements (y-axis) produced by the deterministic simulator for the same inputs. Each dot, corresponding to a real/synthetic pair, is colored according to the light-source setting $(R,G,B)$. The diagonal black line corresponds to the deterministic output of the simulator, and we report the $R^2$ score between the real and synthetic measurements.}
\label{fig:comparison_sensors}
\end{figure}%

\paragraph{Light-intensity sensors} We model the sensor response as a linear combination of the intensity of each light-source color ($R,G,B$), with coefficients encoded in the matrix $\pam{S}$. To account for the decrease in intensity as we move away from the light source, we apply the inverse-square law using the distances $\pam{d_1, d_2, d_3}$ of each sensor to the light source. The effect of the polarizer angles ($\theta_1, \theta_2$) on the third sensor ($\tilde{I}_3, \tilde{V}_3$) is modeled through Malus' law \citep{collett2005field}. Following the specifications of the sensor\footnote{See pages 42/43 of the \href{https://github.com/juangamella/causal-chamber/blob/main/hardware/datasheets/light_sensor.pdf}{Si115x datasheet}.}, we model the effect of the photodiode size ($D^I_i, D^V_i$) and exposure time ($T^I_i, T^V_i$) as scaling the sensor response by an exponential factor.

\begin{align*}
    %%% I_1
    \begin{pmatrix}
        \oup{\tilde{I}_1}\\
        \oup{\tilde{V}_1}\\
    \end{pmatrix}:=&\begin{pmatrix}
        2^{\inp{D^I_1+T^I_1}} & 0\\
        0 & 2^{\inp{D^V_1+D^T_1}}\\
    \end{pmatrix}\pam{S}\begin{pmatrix}
        \inp{R}\\
        \inp{G}\\
        \inp{B}\\
    \end{pmatrix}    
    \\%%% I_2
        \begin{pmatrix}
        \oup{\tilde{I}_2}\\
        \oup{\tilde{V}_2}\\
    \end{pmatrix}:=&\begin{pmatrix}
        2^{\inp{D^I_2+T^I_2}} & 0\\
        0 & 2^{\inp{D^V_2+T^V_2}}\\
    \end{pmatrix}\pam{S}\left(\frac{\pam{d_1}}{\pam{d_2}}\right)^2\pam{T_s}\begin{pmatrix}
        \inp{R}\\
        \inp{G}\\
        \inp{B}\\
    \end{pmatrix}
    \\%%% I_3
    \begin{pmatrix}
        \oup{\tilde{I}_3}\\
        \oup{\tilde{V}_3}\\
    \end{pmatrix}:=&
    \begin{pmatrix}
        2^{\inp{D^I_3+T^I_3}} & 0\\
        0 & 2^{\inp{D^V_3+T^V_3}}\\
    \end{pmatrix}
    \pam{S}
    \left(\frac{\pam{d_1}}{\pam{d_3}}\right)^2
    \diag{(\pam{T_p} - \pam{T_c})\cos^2(\inp{\theta_1} - \inp{\theta_2})+\pam{T_c}}
    \begin{pmatrix}
        \inp{R}\\
        \inp{G}\\
        \inp{B}\\
    \end{pmatrix}
\end{align*}

\paragraph{Current sensor} The drawn current $\tilde{C}$ is modeled as a linear combination of the brightness setting ($R,G,B$) of each light-source color, encoded in the matrix of coefficients $\pam{Q}$. We include an intercept $\pam{C_0}$ for the base current drawn even when the light source is turned off ($R=G=B=0$) and account for the scaling induced by the reference voltage $R_C$.
\begin{align*}
    %%% C
    \oup{\tilde{C}}:=& \Big[\pam{Q} \begin{pmatrix}
        \inp{R}\\
        \inp{G}\\
        \inp{B}\\
    \end{pmatrix} + \pam{C_0}  \Big] \frac{5}{\inp{R_C}}
\end{align*}

\paragraph{Angle sensor} The relationship between the polarizer positions ($\theta_1, \theta_2$) and the voltage measured by the angle sensors ($\tilde{\theta}_1, \tilde{\theta}_2$) is also affine, and is encoded in the coefficient $\pam{A}$ and the reference points $\pam{a_1, a_2}$. As for the current sensor, we account for the scaling induced by the reference voltages $R_1, R_2$. To model the saturation of the sensors at lower reference voltages \citep[see ][Supplementary Figure 9]{gamella2025chamber}, we take the minimum between the synthetic measurement and the maximum possible value (1023).

\begin{align*}
    %%% angle 1
    \oup{\tilde{\theta}_1}:=
    \min \left\{1023,
    \left (\pam{A} \inp{\theta_1} + \pam{a_1} \right) \frac{5}{\inp{R_1}}
    \right \}
    ,\quad%%% angle 1
    \oup{\tilde{\theta}_2}:=
    \min \left\{1023,
    \left (\pam{A} \inp{\theta_2} + \pam{a_2} \right) \frac{5}{\inp{R_2}}
    \right \}
\end{align*}

The parameters of the simulator, displayed in \pam{orange} in the above equations, are described in \cref{tab:sim_params} below. For the synthetic observations used in the experiments of \cref{s:results}, the parameters are either taken from the datasheets of the corresponding sensors or fit to a separate calibration dataset. See the \href{https://github.com/juangamella/causal-chamber-package/blob/main/causalchamber/simulators/tutorials/tutorial_Deterministic.ipynb}{tutorial notebook} for the actual values and an example of running the Python implementation of the simulator.

\renewcommand*{\arraystretch}{1.4}
\begin{longtable}{@{}llp{12cm}@{}}
\caption{Description of the parameters of the simulator for the sensor measurements, including their range and the name they take in the \href{https://github.com/juangamella/causal-chamber-package/tree/main/causalchamber/simulators}{Python implementation} (see simulator \texttt{lt.Deterministic}).}\\
\toprule
Parameter & Name in code & Description\\
\midrule
\endhead
\bottomrule
\endfoot
\bottomrule
\label{tab:sim_params}
\endlastfoot
%%%%%%%%%%%
$\pam{S} \in \mathbb{R}_+^{2 \times 3}$
& \begin{tabular}[t]{@{}l@{}}
\nolinkurl{S}
\end{tabular} & Encodes the linear response of the photodiodes to the light of different colors. The first row $S_{1,i=1,2,3}$ gives the response of the smallest IR photodiode ($D^I_* = 0$) to the red, green, and blue light; the second row corresponds to the smallest visible photodiode ($D^V_* = 0$).\\
%%%%%%%%%%%
$\pam{d_1}, \pam{d_2}, \pam{d_3} \in \mathbb{R}$
& \begin{tabular}[t]{@{}l@{}}
\nolinkurl{d1, d2, d3}
\end{tabular} & The distance, in millimeters, from the tunnel's light source to the first ($d_1$), second ($d_2$) and third ($d_3$) light sensors.\\
%%%%%%%%%%%
$\pam{T_s}\in [0,1]^3$
& \begin{tabular}[t]{@{}l@{}}
\nolinkurl{Ts}
\end{tabular} & The transmission rate of the first polarizer for each color wavelength.\\
%%%%%%%%%%%
$\pam{T_p}, \pam{T_c} \in [0,1]^3$
& \begin{tabular}[t]{@{}l@{}}
\nolinkurl{Tp, Tc}
\end{tabular} & The joint transmission rates of the two polarizers for each color wavelength when their polarization axes are parallel ($T_p$) or orthogonal ($T_c$) to each other.\\
%%%%%%%%%%%
$\pam{Q} \in \mathbb{R}_+^{1 \times 3}$
& \begin{tabular}[t]{@{}l@{}}
\nolinkurl{Q}
\end{tabular} & Encodes the linear response of the current sensor to the brightness setting of each color.\\
%%%%%%%%%%%
$\pam{C_0} \in \mathbb{R}_+$
& \begin{tabular}[t]{@{}l@{}}
\nolinkurl{c0}
\end{tabular} & The current drawn by the light source when it is turned off, i.e., $R=G=B=0$. The value corresponds to the uncalibrated measurement when $R_C = 5$.\\
%%%%%%%%%%%
$\pam{A} \in \mathbb{R}_+$
& \begin{tabular}[t]{@{}l@{}}
\nolinkurl{A}
\end{tabular} & The linear coefficient relating the change in polarizer position to the change in the voltage reading of the angle sensor, when the reference voltage is $R_{j} = 5$ for $j=1$ or $2$, respectively.\\
%%%%%%%%%%%
$\pam{a_1, a_2} \in \mathbb{R}_+$
& \begin{tabular}[t]{@{}l@{}}
\nolinkurl{a1,a2}
\end{tabular} & The zero point of the angle sensor, i.e., the voltage reading when the polarizer positions are $\theta_1 = 0$ and $\theta_2 = 0$, respectively, and the reference voltages are $R_1 = R_2 = 5$.\\
\end{longtable}

\subsection{Simulating the images}
\label{ss:images_sim}

To simulate the images produced by the light tunnel, we train a simple neural network (\cref{tab:simulator_arch}) to generate synthetic images given the inputs $R,G,B,\theta_1$ and $\theta_2$. A comparison of the real and synthetic images is provided in \cref{fig:comparison_images}. A Python implementation with a pre-trained model is available through the \href{https://github.com/juangamella/causal-chamber-package/tree/main/causalchamber/simulators}{\nolinkurl{causalchamber}} package (simulator \texttt{lt.DecoderSimple}); see the \href{https://github.com/juangamella/causal-chamber-package/blob/main/causalchamber/simulators/tutorials/tutorial_DecoderSimple.ipynb}{tutorial notebook } for example code.

\begin{figure}[H]
\centerline{
\includegraphics[width=161mm]{figures/sim_images.pdf}
}
\caption{Comparison between the real and synthetic images produced by the deterministic simulator. The simulator does not capture the noise present in the images, and smoothens out some of the finer details produced by the reflection of light on the polarizer frames, which form the ring around the center of the image.}
\label{fig:comparison_images}
\end{figure}%

\begin{table}[H]
    \centering
    \caption{Architecture and training parameters of the neural image simulator. The network consists of a stack of 4 fully connected linear layers with ReLu activations. The training loss is the mean-squared error over the pixel intensities. The network is trained on $108900$ images from the \href{https://github.com/juangamella/causal-chamber/tree/main/datasets/lt_camera_v1}{\nolinkurl{lt_camera_v1}} and \href{https://github.com/juangamella/causal-chamber/tree/main/datasets/lt_camera_walks_v1}{\nolinkurl{lt_camera_walks_v1}} datasets, which can be found at \href{https://github.com/juangamella/causal-chamber}{\nolinkurl{github.com/juangamella/causal-chamber}}. The code to reproduce the training procedure can be found in \href{https://github.com/juangamella/causal-chamber-package/blob/main/causalchamber/simulators/tutorials/decoder_training.ipynb}{this notebook}.}
    %\vskip 0.15in
    \renewcommand{\arraystretch}{1} % Adjust row height for better readability
    \begin{tabular}{lrl}
    \toprule        
        \textbf{Layers} & \textbf{Parameter} & \textbf{Value}\\
        \midrule
        Linear (5) & Optimizier & Adam \citep{kingma2015adam}\\
        Linear (64 $\times$ 64) & Learning rate & $10^{-3}$\\
        Linear (64 $\times$ 64) & Weight decay & $10^{-5}$\\
        Linear (64 $\times$ 64 $\times$ 3) & Epochs & 100\\
        & Batch size & 4096\\
        \bottomrule
    \end{tabular}
    \label{tab:simulator_arch}
\end{table}

\end{document}
