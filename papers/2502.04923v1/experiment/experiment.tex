\vspace{-8pt}
\section{Experiments}
\vspace{-4pt}
\subsection{Experimental Setup}
\vspace{-2pt}
\paragraph{Models and Evaluation Setup}
We begin by examining the prompt-only generative method without incorporating concept LoRAs~\citep{sd1.5}, denoted as the naive model. In addition, we utilize training-free multi-LoRA composition methods with the same text prompt used in the naive model, including LoRA Switch~\citep{multilora}, LoRA Composite~\citep{multilora}, and LoRA Merge~\citep{loramerge}. Alongside these methods, we incorporate the LoraHub~\citep{lorahub} framework, which fluidly combines multiple LoRA modules with few-shot learning, as our baseline. For each baseline method, we apply our proposed caching method, denoting the results as LoRA Switch ($\text{Cache}{D}$), LoRA Composite ($\text{Cache}{D}$), and LoRA Merge ($\text{Cache}_{D}$), respectively. We also consider a uniform cache interval strategy governed by a hyper-parameter $c$. For all $T$ denoising steps, the sequence of timesteps that performs full inference is defined as:
\begin{equation}
\begin{aligned}
    \label{cache interval1}
    \mathcal{I} = \{c\cdot t|0\leq c\cdot t\leq T, \text{where} \ t\in \mathbb{Z}\}.
\end{aligned}
\end{equation}
This extension allows us to integrate multi-LoRA composition methods with varying uniform caching strategies for evaluation, as analyzed in \Cref{sec:ablation2}. Additionally, we integrate our proposed LoRA partitioning strategy to LoRA Switch also as a baseline, referred as Switch-A. The analysis of computational cost is detailed in \Cref{sec:computation}.

Based on the testbed \textit{ComposLoRA}~\citep{multilora}, we curate two unique subsets of LoRAs representing realistic and anime styles. Each subset comprises a variety of elements: $3$ characters, $2$ types of clothing, $2$ styles, $2$ backgrounds, and $2$ objects, culminating in a total of $22$ LoRAs. We discuss the challenges in constructing a well-defined multi-LoRA composition testbed in \Cref{sec:limit}.

% In constructing composition sets, we strictly follow a crucial principle: each set must include one character LoRA and avoid duplication of element categories to prevent conflicts. This includes $48$ sets comprising $2$ LoRAs, $144$ sets with $3$ LoRAs, $192$ sets featuring $4$ LoRAs, and $96$ sets containing $5$ LoRAs. Key features for each LoRA are manually annotated and serve dual purposes: they act as input prompts for the text-to-image models to generate images, and also provide reference points for subsequent evaluations using MiniCPM-V~\citep{minicpm}.

\paragraph{Evaluation Metrics}
We employ CLIPScore~\citep{clipscore} and ImageReward~\citep{imagereward} to evaluate the comprehensive image generation capabilities of all multi-LoRA composition methods. While both metrics perform effectively within their evaluation domains, we find that they struggle to accurately assess out-of-distribution (OOD) concepts. Notably, ImageReward exhibits more pronounced issues with OOD instances compared to CLIPScore, as illustrated in \Cref{averageirclip} (which assigns negative scores to images generated by anime LoRAs). We discuss this limitation in \Cref{sec:limit}. Consequently, we include CLIPScore as a key evaluation metric to assess the efficacy of multi-LoRA composition frameworks at first.

% Existing traditional image generation metrics primarily focus on text-image alignment but often overlook the complexity of individual elements within an image and the quality of their composition. 

Recent advancements in multi-modal large language models (LLMs), such as ~\citet{minicpm, gpt4, gpt4v}, have shown significant potential in multi-modal tasks, positioning them as promising tools for evaluating image generation. In this study, we harness capabilities of MiniCPM-V~\citep{minicpm}, an end-side multi-modal LLM designed for vision-language understanding, to evaluate composable image generation, utilizing in-context few-shot learning to address challenges posed by OOD concepts. The full evaluation process is provided in \Cref{sec:appendixc}. 
\vspace{-6pt}
\paragraph{Implementation Details}
We use the open-source platform Diffusers \citep{diffusers} as the pipeline to conduct our experiments. We employ stable-diffusion-v$1.5$ implemented by ~\citet{sd1.5} in PyTorch~\citep{pytorch} as the backbone model. For the anime style subset, the settings differ slightly with $200$ denoising steps, a guidance scale $s$ of $10$, and an image size of $512\times512$. The DPM-Solver$++$ proposed by ~\citet{dpmsolver} is used as the scheduler in the generation process. The LoRA scale for all LoRAs is set to $1.4$, which is applied within the cross-attention module of the U-Net. The dominant weight scale $w_{\text{dom}}$ is initially set at $N-0.5$, where $N$ is the total number of activated LoRAs. Then this weight scale is adjusted using a decaying method. For the $i$-th turn of switching the dominant LoRA, the weight is defined as: $w^{i}_{\text{dom}}=w^{i-1}_{\text{dom}}-0.5^{i}$. In addition, we select $c_{1}=2$ and $c_{2}=3$ for the caching strategy applied to non-dominant LoRAs. The hyper-parameters are selected using grid search methods described in \Cref{app:ablation}.
\vspace{-5pt}
\subsection{Results}
\label{sec:results}
\vspace{-2pt}
\subsubsection{CLIPScore Evaluation}
\begin{table}[H]
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{-7pt}
    \centering
    \input{table/macclip}
\end{table}
\vspace{-18pt}
We first present the comparative evaluation results obtained using CLIPScore~\citep{clipscore}. \Cref{tab:macclip} presents the ClipScore performance for several LoRA composition methods across different numbers of LoRAs ($N=2$ to $N=5$). Additional experimental results are included in \Cref{app:results} and visualization demonstrations are provided in \Cref{sec:visual}. Three key observations emerge from the analysis:

\begin{itemize}[noitemsep, topsep=-5pt, leftmargin=*]
    \item \textit{In general, LoRA Switch shows higher CLIPScore than LoRA Composite across most cases.} For $N=5$, LoRA Switch scores $33.475$, outperforming LoRA Composite's $32.582$. This trend indicates that LoRA Switch handles multi-LoRA integration more effectively than LoRA Composite, particularly when the number of LoRAs increases. However, Switch-A (LoRA Switch with frequency partitioning), does not offer a better performance compared to CMLoRA with large $N$ values, which confirms our hypothesis in Section \ref{sec:fourier}.
    \item \textit{Our proposed method, CMLoRA with dynamic caching ($\text{Cache}_{D}$), consistently delivers the highest or near-highest CLIPScore across all scenarios.} For $N=3$, CMLoRA achieves a CLIPScore of $35.215$, outperforming both LoraHub ($34.919$) and Switch-A ($35.383$). Similarly, for $N=5$, CMLoRA records the highest score of $34.341$, outperforming other methods. These results highlight the efficiency and robustness of CMLoRA in the multi-LoRA integration task.
    \item \textit{The task of multi-concept image generation remains highly challenging, especially as the number of elements to be composed increases.} As the number of LoRAs increases from $N=2$ to $N=5$, the CLIPScore of generated images generally decreases across all methods. This trend highlights the increasing challenge of compositional image generation when more elements are involved.
\end{itemize}


\textit{CLIPScoreâ€™s evaluations fall short in assessing specific compositional and quality aspects due to its inability to discern the nuanced features of each element}~\citep{multilora}. To address this limitation and provide a more thorough analysis, we complement our findings with an MLLM evaluation across all multi-LoRA composition methods.
\vspace{-4pt}
\subsubsection{MiniCPM-V-based Evaluation}
\label{sec:llmeva}
\vspace{-0pt}
The evaluation using MiniCPM-V involves scoring the performance of CMLoRA ($\text{Cache}_{D}$) versus others across four dimensions, as well as determining the win/loss rate based on these scores. The specific score and win/loss rate are illustrated in the radar map \Cref{fig:radar1} and the win/loss rate plot: \Cref{fig:win}. Additional experimental results are available in \Cref{app:results} and visualization demonstrations are presented in \Cref{sec:visual}. As demonstrated in \Cref{fullllm1}, the Naive model exhibits the lowest score in Semantic Accuracy, highlighting that incorporating multiple LoRA mechanisms can significantly enhance the generative model's performance in multi-concept image generation. Based on our observations, we conclude that our proposed CMLoRA achieves significant improvements across various metrics for multi-concept image generation, particularly in aesthetic quality. Moreover, CMLoRA demonstrates superior overall composition quality, with a win rate that is $20\%$ higher than LoRA Merge and $10\%$ higher than other methods.
\vspace{-6pt}
\begin{figure}[H]
    \centering
    \setlength{\abovecaptionskip}{-1pt}
    \setlength{\belowcaptionskip}{-8pt}
    \begin{minipage}{0.54\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/radar11.png}
        \caption{The performance evaluation results of LoRA integration methods on the ComposLoRA testbed using MiniCPM-V are presented. Detailed scores are available in \Cref{fullllm1}.}
        \label{fig:radar1}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.44\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/win.png}
        \caption{Comparison of CMLoRA ($\text{Cache}_{D}$) against other Multi-LoRA composition methods based on win rate.}
        \label{fig:win}
    \end{minipage}
\end{figure}
\vspace{-11pt}
\begin{figure}[H]
    \centering
    \setlength{\abovecaptionskip}{-1pt}
    \setlength{\belowcaptionskip}{-10pt}
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=0.84\linewidth]{image/compositecachewin.png}
        \caption{Comparison of Composite against Composite($\text{Cache}_{D}$) based on win rate.}
        \label{fig:compositecachewin}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=0.84\linewidth]{image/cmloracachewin.png}
        \caption{Comparison of CMLoRA against CMLoRA($\text{Cache}_{D}$) based on win rate.}
        \label{fig:cmloracachewin}
    \end{minipage}
\end{figure}
\vspace{-7pt}
In addition, to verify our hypothesis presented in \Cref{sec:cache} -- caching non-dominant LoRAs strategically during the denoising process can amplify the contribution of the determined dominant LoRA and ensure more stable frequency fusion in multi-LoRA composition -- we assess both LoRA Composite and CMLoRA, including their caching versions, based on win rate. \Cref{fig:compositecachewin} and \Cref{fig:cmloracachewin} illustrate that the caching strategy can significantly improve the quality of multi-concept image generation. Specifically, LoRA Composite benefits more from the caching strategy compared to CMLoRA because LoRA Composite does not implement the LoRA partition strategy during the denoising process. In summary, we conclude the following results: 
\begin{itemize}[noitemsep, topsep=-5pt, leftmargin=*]
    \item \textit{CMLoRA demonstrates greater potential than other multi-LoRA fusion methods in addressing the semantic conflicts that arise during multi-LoRA composition.} The MiniCPM-V evaluation further reinforces CMLoRA's advantages, particularly in aesthetic quality, where it achieves a higher win rate than other competing methods in the multi-LoRA composition image generation task.
    \item \textit{Our analysis confirms that strategically caching non-dominant LoRAs enhances the performance of dominant LoRAs during the denoising process.} Notably, LoRA Composite benefits more from this caching strategy compared to CMLoRA, likely due to the absence of the Fourier LoRA partition approach during denoising in LoRA Composite.
\end{itemize}