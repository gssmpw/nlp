
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{afterpage}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage{float}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{cleveref}
\usepackage{wrapfig}


\definecolor{darkgreen}{rgb}{0.0, 0.45, 0.0}

\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem*{corollary*}{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}

\renewcommand\qedsymbol{$\blacksquare$}

\newcommand{\az}[1]{\textcolor{red}{[AZ: #1]}}
\newcommand{\reb}[1]{\textcolor{red}{[#1]}}

\title{Cached Multi-Lora Composition for Multi-Concept Image Generation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\author{}
\author{Xiandong Zou, Mingzhu Shen\thanks{Mingzhu Shen is the corresponding author.}, Christos-Savvas Bouganis, Yiren Zhao  \\
Imperial College London, UK\\
\texttt{\{xiandong.zou20, m.shen23, christos-savvas.bouganis, a.zhao\}@imperial.ac.uk}
% \And
% Mingzhu Shen \\
% Imperial College London \\
% \texttt{m.shen23@imperial.ac.uk}
% \AND
% Christos-Savvas Bouganis \\
% Imperial College London \\
% \texttt{christos-savvas.bouganis@imperial.ac.uk}
% \And
% Yiren Zhao \\
% Imperial College London \\
% \texttt{a.zhao@imperial.ac.uk}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%

% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.
\usepackage{tikz}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,fill,inner sep=1.2pt] (char) {\textcolor{white}{#1}};}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in text-to-image models, enabling precise rendering of multiple distinct elements, such as characters and styles, in multi-concept image generation. However, current approaches face significant challenges when composing these LoRAs for multi-concept image generation, particularly as the number of LoRAs increases, resulting in diminished generated image quality. 
In this paper, we initially investigate the role of LoRAs in the denoising process through the lens of the Fourier frequency domain.
Based on the hypothesis that applying multiple LoRAs could lead to ``semantic conflicts", we have conducted empirical experiments and find that certain LoRAs amplify high-frequency features such as edges and textures, whereas others mainly focus on low-frequency elements, including the overall structure and smooth color gradients.
Building on these insights, we devise a frequency domain based sequencing strategy to determine the optimal order in which LoRAs should be integrated during inference. This strategy offers a methodical and generalizable solution compared to the naive integration commonly found in existing LoRA fusion techniques.
To fully leverage our proposed LoRA order sequence determination method in multi-LoRA composition tasks, we introduce a novel, training-free framework, Cached Multi-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while maintaining cohesive image generation.
With its flexible backbone for multi-LoRA fusion and a non-uniform caching strategy tailored to individual LoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA composition and improve computational efficiency.
Our experimental evaluations demonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion methods by a significant margin -- it achieves an average improvement of $2.19\%$ in CLIPScore, and $11.25\%$ in MLLM win rate compared to LoraHub, LoRA Composite, and LoRA Switch.\footnote{The source code is released at \texttt{https://github.com/Yqcca/CMLoRA}.}

% we use \textit{ComposLoRA}, a comprehensive testbed encompassing $480$ composition sets across multiple LoRA categories. While conventional metrics such as CLIPScore and ImageReward provide a broad assessment of images generated by different LoRA composition frameworks, they fall short of capturing finer compositional nuances due to out-of-distribution issues. To address this, we leverage MiniCPM-V, a GPT-4V-level multimodal large language model (MLLM), employing in-context few-shot learning to refine the evaluation process. Our results demonstrate significant improvements over baseline methods, particularly in handling complex compositions involving numerous LoRAs, confirming the effectiveness of CMLoRA in overcoming the limitations of existing techniques.

\end{abstract}

\input{intro/intro.tex}
\input{method/method}
\input{experiment/experiment}
\input{rel/rel}
\input{conclusion/conclusion}





% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
We thank our anonymous reviewers for their constructive comments and insightful feedback.

This work was performed using the Sulis Tier 2 HPC platform hosted by the Scientific Computing Research Technology Platform at the University of Warwick. Sulis is funded by EPSRC Grant EP/T022108/1 and the HPC Midlands+ consortium. Mingzhu Shen is funded by Imperial Presidentâ€™s PhD Scholarships. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage

\appendix
\input{appendix/appendix}

\end{document}
