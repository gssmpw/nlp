\section{Method}
\vspace{-0.3em}
\subsection{Preliminary}
\vspace{-0.3em}
\paragraph{Text-to-Image Diffusion Models}
Diffusion models~\citep{ddpm, ddim} belong to a class of generative models that gradually introduce noise into an image during the forward diffusion process and learn to reverse this process to synthesize images. When combined with pre-trained text embedding, text-to-image diffusion models~\citep{sdxl, sd3} are capable of generating high-fidelity images based on text prompts. The diffusion model, denoted as $\epsilon_{\theta}$, with trainable parameters $\theta$, is optimized to predict the noise added to the noisy latent representation $\mathbf{z}_{t}$, conditioned on the provided text $c$. Typically, a mean-squared error loss function is utilized as the denoising objective:
\begin{equation}
    \label{diffusion}
    \mathcal{L} = \mathbb{E}_{\mathbf{z},c,\epsilon,t}[||\epsilon-\epsilon_{\theta}\left(\mathbf{z}_{t},t,c\right)||],
\end{equation}
where $\epsilon\sim \mathcal{N}(0,1)$ is the additive Gaussian noise, $\mathbf{z}_{t}$ is the latent feature at timestep $t$ and $\epsilon_{\theta}$ is the denoising U-Net with learnable parameter $\theta$.
% \paragraph{Classifier-Free Guidance}
% In the classifier-free guidance paradigm~\citep{guidancefree}, we jointly train an unconditional denoising diffusion model parameterized through a score estimator $e_{\theta}(\mathbf{z}_{t})$ together with the conditional model parameterized through $e_{\theta} (\mathbf{z}_{t},c)$. Then we mix the score estimates of a conditional diffusion model and a jointly trained unconditional diffusion model to attain a trade-off between sample quality and diversity. 

% For the text-to-image task, it operates by directing the probability mass towards outcomes where the implicit classifier $p_{\theta}(c|\mathbf{z}_{t})$ predicts a high likelihood for the textual conditioning $c$. This process enables the diffusion models to undergo a joint training paradigm for both conditional and unconditional denoising. Subsequently, during inference, the guidance scale $s \geq 1$ is used to adjust the score function $\hat{e}_{\theta} (\mathbf{z}_{t},c)$ by moving it closer to the conditional estimation $e_{\theta} (\mathbf{z}_{t},c)$ and further from the unconditional estimation $e_{\theta}(\mathbf{z}_{t})$, enhancing the conditioning effect on the generated images, as formalized in the following expression:
% \begin{equation}
%     \label{guidance}
%     \hat{e}_{\theta} (\mathbf{z}_{t},c) = (1-s)\cdot e_{\theta}(\mathbf{z}_{t})+ s\cdot e_{\theta} (\mathbf{z}_{t},c).
% \end{equation}
\vspace{-10pt}
\paragraph{Low-Rank Adaptation}
The Low-Rank Adaptation (LoRA) technique freezes the pre-trained model weights and introduces trainable low-rank decomposition matrices into each layer of the neural network architecture, thereby significantly reducing the number of trainable parameters required for downstream tasks~\citep{lora}. For a pre-trained weight matrix $W \in \mathbb{R}^{m\times n}$ in a diffusion model $\epsilon_{\theta}$, we constrain its update by representing the latter with a low-rank decomposition, i.e., updating $W$ to $\hat W$, where $\hat W=W+\Delta W = W+BA$. $B \in \mathbb{R}^{m\times r}$ and $A \in \mathbb{R}^{r\times n}$ are matrices of a low-rank factor $r$, satisfying $r \ll \min(n, m)$.
\vspace{-8pt}
\subsection{LoRA Disparity based on Fourier Analysis}
\vspace{-0.4em}
\label{sec:fourier}
We hypothesize that the challenges in scaling multiple LoRA modules stem from the ''semantic conflicts” that arise among them, given that LoRAs are typically trained independently and fuse features with varying amplitudes across different frequency domains during the denoising process. Building on the notable disparities in how different LoRAs fuse high-frequency components during the denoising process, as illustrated in \Cref{fig:motivation}, we expand our investigation to delineate the specific contributions of various LoRAs within this process and to explore the internal characteristics of LoRAs based on their profiled categories. We aim to establish a Fourier-based method to classify LoRAs according to their frequency responses and group them into distinct sets, as shown in \Cref{fig:smot2}. Using our profiling approach, LoRAs are categorized into high-frequency and low-frequency sets. During inference, high-frequency LoRAs are primarily utilized in the early stages of denoising to enhance detail and texture, while low-frequency LoRAs are predominantly applied in the later stages to refine overall structure and coherence.

To evaluate the salient characteristics of the contribution of high-frequency components modification from different LoRAs in the denoising process, we conduct a controlled experiment based on the testbed \textit{ComposLoRA}~\citep{multilora}, comprising of $5$ different LoRA categories: Character, Clothing, Style, Background and Object. We first use the diffusion model combined with each LoRA in different profiled LoRA categories to generate a batch of images. Then we use the 2D Fast Fourier Transform (FFT) to transform images from the spatial domain to the frequency domain and compute their frequency spectrum. Finally, we extract the amplitude of the high-frequency components and compute their change in amplitude during the whole denoising process. Mathematically, these operations are performed as follows:

We first computer the average feature map along the channel dimension by taking the mean:
\vspace{-3pt}
\begin{equation}
\vspace{-1pt}
    \overline{\mathbf{x}}_{t}=\frac{1}{C}\sum\limits^{C}_{i=1}\mathbf{x}_{t,i},
\end{equation}
where $\mathbf{x}_{t,i}$ represents the $i$-th channel of the feature map $\mathbf{x}_{t}$ at denoising timestep $t$. $C$ denotes the total number of channels in $\mathbf{x}_{t}$. Then, we quantify the amplitude of high-frequency components in the generated image by analyzing its distribution across the frequency spectrum. We calculate the 2D FFT for $\overline{\mathbf{x}}_{t}$ and extract the amplitude of $h$ percentage of high-frequency components in the frequency domain:
\vspace{-3pt}
\begin{equation}
\vspace{-1pt}
\begin{aligned}
\label{eq:3}
\mathcal{F}(\overline{\mathbf{x}}_{t}) &= \text{2DFFT}\left(\overline{\mathbf{x}}_{t}\right) \\
\mathcal{H}_{h}\left(\overline{\mathbf{x}}_{t}\right) &= \left|\mathcal{F}\left(\overline{\mathbf{x}}_{t}\right) \cdot \mathbf{1}_{\{\|\mathbf{u}\left(\mathcal{F}\left(\overline{\mathbf{x}}_{t}\right)\right)\| > h \cdot \max \|\mathbf{u}\left(\mathcal{F}\left(\overline{\mathbf{x}}_{t}\right)\right)\|\}}\right|,
\end{aligned}
\end{equation}
where 2D FFT$(\cdot)$ denotes the 2D Fast Fourier transform averaged in the radial axis, $|\cdot|$ calculates the amplitude of components across the frequency spectrum, and $\mathbf{u}(\cdot)$ calculates the frequency range in the Fourier domain. Subsequently, the change in amplitude of high-frequency components between interval $t-z$ and $t$ is determined as follows:
\begin{equation}
\begin{aligned}
\label{eq:4}
\Delta\mathcal{F}\left(\overline{\mathbf{x}}_{t};z\right)&= \mathcal{F}\left(\overline{\mathbf{x}}_{t}\right)-\mathcal{F}\left(\overline{\mathbf{x}}_{t-z}\right) \\
\Delta\mathcal{H}_{h}\left(\overline{\mathbf{x}}_{t};z\right) &= \left|\Delta\mathcal{F}\left(\overline{\mathbf{x}}_{t};z\right) \cdot \mathbf{1}_{\{\|\mathbf{u}\left(\Delta\mathcal{F}\left(\overline{\mathbf{x}}_{t};z\right)\right)\| > h \cdot \max \|\mathbf{u}\left(\Delta\mathcal{F}\left(\overline{\mathbf{x}}_{t};z\right)\right)\|\}}\right|,
\end{aligned}
\end{equation}
\vspace{-23pt}
\begin{wrapfigure}{r}{0.5\textwidth}
\setlength{\abovecaptionskip}{-6pt}
  \setlength{\belowcaptionskip}{-15pt}
  \begin{center}
    \includegraphics[width=\linewidth]{image/loraffthigh.png}
  \end{center}
  \caption{Summary of the change in amplitude of high-frequency components, $\overline{\Delta\mathcal{H}_{0.2}\left(\overline{\mathbf{x}}_{t};20\right)}$, during the denoising process for generated images with LoRAs across different LoRA categories.}
  \label{fig:loraffthigh}
\end{wrapfigure}

% \begin{figure}[H]
% \centering
% \includegraphics[width=0.7\linewidth]{image/loraffthigh.png}
% \caption{Summary of the change in amplitude of high-frequency components, $\overline{\Delta\mathcal{H}_{0.2}\left(\overline{\mathbf{x}}_{t};20\right)}$, during the denoising process for generated images with LoRAs across different LoRA categories.}
% \label{fig:loraffthigh}
% \end{figure}

Based on \Cref{eq:4}, we profile the LoRA categories in the testbed through the following steps: 1) Establishing a prioritized LoRA order strategy, denoted as $\mathcal{O}$, by ranking the variation in the intensity of high-frequency components, $\overline{\Delta\mathcal{H}_{h}\left(\overline{\mathbf{x}}_{t};z\right)}$, across different LoRA categories. 2) Using the strategy $\mathcal{O}$, we can categorize LoRAs into a high-frequency dominant set $H$ and a low-frequency dominant set $L$ for a multi-LoRA composition task. 3) LoRAs from the high-frequency dominant set $H$ are employed predominantly during the initial stages of denoising, where their dynamic features can enhance the image’s detail and texture. In contrast, LoRAs from the low-frequency dominant set $L$ are utilized primarily in the later stages of the denoising process.

We set $h=0.2$ to conduct our exploration. We calculate the amplitude of high-frequency components, denoted as $\mathcal{H}_{0.2}(\overline{\mathbf{x}}_{t})$, and change in amplitude of high-frequency components, represented as $\Delta\mathcal{H}_{0.2}\left(\overline{\mathbf{x}}_{t};z\right)$, for certain step $t$ for all LoRAs within the various profiled categories in our testbed. These values are averaged among the LoRAs within their respective categories, for instance, we have multiple LoRAs coming from the Character category or the Cloth category. This then yields $\overline{\mathcal{H}_{0.2}(\overline{\mathbf{x}}_{t})}$ and $\overline{\Delta\mathcal{H}_{0.2}\left(\overline{\mathbf{x}}_{t};z\right)}$ for all LoRA categories.

We perform the profiling on the following categories: Chracter, Cloth, Style, Background and Object.
\Cref{fig:loraffthigh} illustrates the variation in the intensity of high-frequency components, $\overline{\Delta\mathcal{H}_{0.2}\left(\overline{\mathbf{x}}_{t};z\right)}$, across all LoRA categories in our testbed throughout the denoising process. Evidently, certain high-frequency dominant LoRAs, such as Style and Character, incorporate larger amplitudes of high-frequency features into the generated image compared to others, particularly during the early stages of inference.

Building upon these insights, we subsequently leverage the change in amplitudes of high-frequency components $\overline{\Delta\mathcal{H}_{0.2}\left(\overline{\mathbf{x}}_{t};z\right)}$ as a criterion for selecting LoRA candidates throughout the denoising process. We establish a prioritized LoRA order strategy $\mathcal{O}$ using the ranking of $\overline{\Delta\mathcal{H}_{0.2}\left(\overline{\mathbf{x}}_{t};20\right)}$ across different LoRA categories: Style, Character, Cloth, Object and Background. We aim to maximize the contribution of the LoRAs ranked highest in the order $\mathcal{O}$ during the early stages of the denoising process. 

Following the strategy $\mathcal{O}$, we can categorize LoRAs into a high-frequency dominant set $H$ and a low-frequency dominant set $L$ for a multi-LoRA composition task. Specifically, we reserve the LoRA candidate ranked last in the order $\mathcal{O}$ for inclusion in the low-frequency dominant set $L$, while placing the remaining LoRAs into the high-frequency dominant set $H$. As a result, LoRAs from the high-frequency dominant set $H$ are employed predominantly during the initial stages of denoising, where their dynamic features can effectively enhance the image’s detail and texture. In contrast, LoRAs from the low-frequency dominant set $L$ are utilized primarily in the later stages of the denoising process. This strategic transition between dominant LoRAs mitigates the semantic conflicts that may arise from the fusion of multiple LoRAs.

% \input{table/loraffthigh}
\vspace{-7pt}
\begin{figure}[!b]
\setlength{\abovecaptionskip}{-6pt}
\setlength{\belowcaptionskip}{-12pt}
\begin{center}
\includegraphics[width=0.95\textwidth]{image/method.jpg}
\end{center}
\caption{Overview of our multi-LoRA composition framework during a $7$-step denoising process.  Each color represents a distinct LoRA, where solid shapes indicate dominant LoRAs performing full inference, and hollow shapes represent non-dominant LoRAs leveraging the caching mechanism at their respective steps. The weight scale $w_{dom_{i}}$ on each dominant LoRA signifies its influence during the denoising process, where $w_{dom_{0}}=w_{dom_{1}}>\cdots>w_{dom_{5}}$.}
\label{img:method}
\end{figure}

\subsection{Cached Multi-LoRA}
\vspace{-5pt}
In this paper, we investigate multi-LoRA composition within the context of diffusion models, aligning with prior studies on LoRA merging~\citep{mixlora, multilora}. Building on the training-free LoRA integration methods introduced by \citet{multilora}, we focus on two well-established frameworks: LoRA Switch and LoRA Composite, as outlined in \Cref{sec:decoding}. Based on our evaluations detailed in \Cref{sec:results}, while LoRA Composite injects all activated LoRAs at each timestep during the denoising process, LoRA Switch -- activating only one LoRA per timestep -- performs better in the multi-LoRA composition task. 

Since LoRA Switch outperforms LoRA Composite based on the evaluation in \Cref{sec:results}, we hypothesize that LoRA Switch mitigates the ``semantic conflicts" in multi-LoRA fusion by limiting activation to a single LoRA per timestep. However, by applying the frequency profiling approach described in \Cref{sec:fourier} to classify LoRAs into high-frequency ($H$) and low-frequency ($L$) sets, we conjecture that LoRA Composite can potentially surpass LoRA Switch, since the effective LoRA partitioning strategy could allow LoRA Composite to integrate multiple LoRAs efficiently while minimizing semantic conflicts. Thus, we introduce a flexible multi-LoRA framework based on LoRA Composite, termed Cached Multi-LoRA.
% It is worth to mention that we did also implement a cached LoRA Switch strategy and performed an ablation to show that it performs worse them CMLoRA in XXX \az{add this} to fully verify our hypothesis.
\vspace{-4pt}
\subsubsection{LoRA Composite}
CMLoRA is grounded in the LoRA partition approach illustrated in \Cref{sec:fourier}, focusing on the systematic investigation of optimal dominant LoRA fusion sequences to enhance multi-LoRA integration. Our framework involves calculating both unconditional and conditional score estimates for each LoRA individually at every denoising step. With a set of $N$ LoRAs in place, let $\hat\theta_{i}$ denote the parameters of the diffusion model $e_{\theta}$ after incorporating the $i$-th LoRA. The collective guidance $\hat{e}(\mathbf{z}_{t},c)$ based on textual condition $c$ is derived by aggregating the scores from each LoRA:
\vspace{-3pt}
\begin{equation}
    \label{lorac}
    \hat{e}(\mathbf{z}_{t},c) = \frac{1}{N}\sum^{N}_{i=1}w_{i}[(1-s)\cdot e_{\hat\theta_{i}}(\mathbf{z}_{t})+s\cdot e_{\hat\theta_{i}}(\mathbf{z}_{t},c)],
\end{equation}
where $w_i$ is a real scalar weight allocated to each LoRA and $\sum^{N}_{i=1}w_{i}<\infty$, intended to adjust the influence of the $i$-th LoRA. By aggregating these scores, the technique ensures balanced guidance throughout the image generation process, facilitating the cohesive integration of all elements represented by different LoRAs. Additionally, we introduce a specialized modulation factor $w_{dom}$ as a hyperparameter, which scales the contribution of the dominant LoRA during inference, as demonstrated in \textbf{Dominant LoRA Scale} in \Cref{app:ablation}.

\Cref{img:method} illustrates the main features of our multi-LoRA composition framework: 
\begin{itemize}[noitemsep, topsep=0pt, leftmargin=*]
    \item Dominant LoRA swaps among LoRAs in the high-frequency dominant set $H$ per timestep.
    \item The weight scale of dominant LoRA, $w_{dom} \in \mathbb{R}$, is decaying during the denoising process.
    \item Non-dominant LoRAs use the caching mechanism demonstrated in \Cref{sec:cache}.
\end{itemize}
\subsubsection{The Caching Mechanism}
\label{sec:cache}
To amplify the contribution of the determined dominant LoRA and ensure more stable frequency fusion in multi-LoRA composition, we further introduce caching strategies for non-dominant LoRAs during the denoising process. 
\vspace{-6pt}
\begin{figure}[H]
    \centering
    \setlength{\abovecaptionskip}{6pt}
    \setlength{\belowcaptionskip}{-16pt}
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/mot31.jpg}
        \caption{Character LoRA and Background LoRA composition. Visual artifacts (green flowers) appear in the image generated by LoRA Composite framework, as illustrated in \Cref{sec:decoding}. Introducing the caching mechanism can alleviate the semantic conflict we have here.}
        \label{fig:motivation2}
    \end{minipage}\hfill
    \begin{minipage}[b]{0.53\textwidth}
        \centering
        \includegraphics[width=\linewidth]{image/mot2.png}
        \caption{The percentage of steps with a similarity greater than $0.9$ to the current step for cached latent feature maps, which is the output of the up-sampling block $U^{t}_{2}$ of U-net with one LoRA during LoRA composition.}
        \label{mot2}
    \end{minipage}
\end{figure}

\Cref{fig:motivation2} illustrates the semantic conflicts that arise in LoRA Composite framework, as discussed in \Cref{sec:decoding}, although we have performed a partitioning already in the frequency domain. Since LoRA Composite assigns equal weights to all frequency components of the LoRAs during the denoising process, it is prone to feature conflicts in the fused outputs of different LoRAs in Fourier space. Drawing inspiration from the DeepCache technique proposed by \citet{deepcache}, we investigate how the caching mechanism can be leveraged as a potential remedy.

% As shown in \Cref{fig:motivation2} and \Cref{mot2}, during the inference stage, we observe visual artifacts caused by multi-LoRA composition and temporal redundancy of feature maps fused by LoRA across the sequential denoising steps in diffusion models. To mitigate these issues, we leverage the hierarchical structure of the U-Net architecture by reusing high-level latent feature maps while efficiently updating only the low-level features. As proposed by~\citet{unet, unet2}, low-level encoded features typically capture high-frequency information, whereas high-level decoded features represent low-frequency information. This caching approach thus facilitates a more balanced trade-off between multi-concept fusion and texture preservation during the early stages of the denoising process. Additionally, it stabilizes the denoising process by enabling the gradual fusion of frequency changes to the latent feature map in the later stages of denoising.

According to the reverse process, $\mathbf{z}_{t-1}$ is conditionally generated based on the previous result $\mathbf{z}_{t}$. Initially, we generate $\mathbf{z}_{t}$ in the same way as usual, where the calculations are performed across all entire U-Nets $\{e_{\hat\theta_{1}}, \cdots, e_{\hat\theta_{N}}\}$ incorporating LoRAs with weight matrices $\{W_{1}, \cdots, W_{N}\}$. To obtain the next output $\mathbf{z}_{t-1}$, we retrieve the high-level features produced in the previous collective guidance $\hat{e}(\mathbf{z}_{t},c)$. Specifically, consider a skip branch $s^{i}$ in the U-Net incorporating the $i$-th LoRA $e_{\hat\theta_{i}}$, which bridges $D_{s^{i}}$ and $U_{s^{i}}$, we cache the feature maps from the previous up-sampling block at the time $t$ as the following:
\vspace{-0pt}
\begin{equation}
\label{cache1}
    F^{t}_{i, \text{cache}} \leftarrow U^{t}_{i, m+1}(\cdot),
\end{equation}
which is the feature from the main branch at timestep $t$. These cached features are reused in subsequent inference steps. At the next timestep, $t-1$, if full inference for the U-Net incorporating the $i$-th LoRA $e_{\hat\theta_{i}}$ is unnecessary, we perform a dynamic partial inference. Based on the previously generated collective guidance $\hat{e}(\mathbf{z}_{t},c)$, we compute only the necessary components for the $m$-th skip branch, while substituting the main branch computation with a retrieval operation from the cache in Equation \ref{cache1}. Thus, for the U-Net incorporating the $i$-th LoRA, the input for $U^{t}_{i, m}$ at timestep $t - 1$ is formulated as:
\vspace{-5pt}
\begin{equation}
\label{cache2}
    \text{Concat}(D^{t-1}_{i, m}(\cdot), F^{t}_{i, \text{cache}}),
\end{equation}
where $D^{t-1}_{i, m}(\cdot)$ represents the output of the $m$-th down-sampling block.

\paragraph{Cache Interval Determination}
As shown in \Cref{mot2}, the percentage of steps with a similarity greater than $0.9$ between cached latent feature maps and the current step follows a distinct trend across all LoRAs in LoRA Composite framework. To capture the similarity trend of feature maps fused by LoRAs, we propose a non-uniform caching interval strategy with two specialized hyper-parameters: $c_{1}, c_{2}\in\mathbb{Z}$. These hyper-parameters control the strength of the caching behavior during inference. Specifically, for a denoising process with $T$ timesteps, the sequence of timesteps that performs full inference is:
\vspace{0pt}
\begin{equation}
\begin{aligned}
    \label{cache interval2}
    \mathcal{I} &= \mathcal{I}_{1} \cup \mathcal{I}_{2} \cup \mathcal{I}_{3} \\
    \mathcal{I}_{1} &= \{c_{1}\cdot t \mid 0 \leq c_{1}\cdot t < \left\lfloor 0.4 \cdot T \right\rfloor, \ \text{where} \ t \in \mathbb{Z}\} \\
    \mathcal{I}_{2} &= \{\left\lfloor 0.4 \cdot T \right\rfloor + c_{2}\cdot t \mid \left\lfloor 0.4 \cdot T \right\rfloor \leq c_{2}\cdot t < \left\lfloor 0.9 \cdot T \right\rfloor, \ \text{where} \ t \in \mathbb{Z}\} \\
    \mathcal{I}_{3} &= \{\left\lfloor 0.9 \cdot T \right\rfloor + c_{1}\cdot t \mid \left\lfloor 0.9 \cdot T \right\rfloor \leq c_{1}\cdot t < T, \ \text{where} \ t \in \mathbb{Z}\}.
\end{aligned}
\end{equation}

% \az{You need to explain why you have magic numbers like 0.4 and 0.9??}
The interval $[\left\lfloor 0.4 \cdot T \right\rfloor, \left\lfloor 0.9 \cdot T \right\rfloor]$ are established based on the condition that the similarity of the cached features exceeds $20\%$ with a $90\%$ confidence interval, as demonstrated in \textbf{Caching Interval and Modulation Hyper-parameters} in \Cref{app:ablation}. This strategic approach aims to further mitigate the issue of semantic conflict in multi-LoRA composition. Notably, it offers two key advantages. First, it amplifies the features contributed by the dominant LoRA feature map, while minimizing changes in features fused from non-dominant LoRAs, allowing the dominant LoRA to play a more critical role during the denoising process. Second, it mitigates the negative effects of frequency conflicts in the Fourier domain, thereby achieving a more balanced trade-off between multi-concept fusion and texture preservation for the activated LoRAs during the inference. The effectiveness of our proposed caching strategy is discussed in \Cref{sec:llmeva}.

In summary, by leveraging a Fourier-based approach to partition LoRAs based on their frequency characteristics, we can determine the optimal order of dominant LoRA application during the denoising process. Building on this partitioning strategy, we introduce CMLoRA, a novel LoRA composition framework that employs a flexible multi-LoRA injection backbone: denoising the noisy image with the predominant contributions of dominant LoRAs, while incorporating supplementary contributions from cached non-dominant LoRAs.