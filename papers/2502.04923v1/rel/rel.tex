\vspace{-5pt}
\section{Related Work}
% \az{Add literature on what is LoRA and also how LoRA is used in both LLMs and image generation, then limit your focus to image generation}
\vspace{-6pt}
\subsection{Multi-concept Text-to-Image Generation}
\vspace{-8pt}
Multi-concept composable image generation plays a crucial role in digital content customization, allowing for the creation of images that align with predefined specifications. Existing research in this domain primarily focuses on the following approaches: adjusting the generative processes of diffusion models to better align with specified requirements~\citep{multi1, multi2, multi5, multi6}, or integrating a series of independent modules that impose desired constraints~\citep{multi3, mixlora, multilora}.

While traditional methods excel at producing images based on general concepts, they often struggle with the precise integration of user-defined objects~\citep{multi2, multi4}. Other approaches can compose specific objects into images but often require extensive fine-tuning and struggle with multiple objects simultaneously~\citep{lorahub,ziplora}. To address these limitations, we propose a training-free, instance-level LoRA composition framework, which enables the accurate assembly of user-specified elements in image generation.
\vspace{-7pt}
\subsection{Multi-LoRA Integration Manipulations}
\vspace{-6pt}
Recent research has focused on leveraging large language models (LLMs) or diffusion models as base models, aiming to manipulate LoRA weights for various objectives. These include element composition in image generation~\citep{loracomposer, ziplora}, reducing the parameters needed for multi-modal inference~\citep{loraeff1, loraeff2}, and adapting models for domain-specific applications~\citep{llmlora1, llmlora2, loraadapt1}. In the realm of LoRA composition techniques, approaches like LoraHub~\citep{lorahub} utilize few-shot demonstrations to learn coefficient matrices for merging LoRAs, allowing for the fusion of multiple LoRAs into a single new LoRA. LoRA Merge~\citep{loramerge} employs addition and negation operators to merge LoRA weights through arithmetic operations. Different from weight-based composition methods, LoRA Switch and LoRA Composite~\citep{multilora} maintain all LoRA weights intact and manipulate the interactions between LoRAs during inference.

Nevertheless, these methods often lead to instability in the merging process as the number of LoRAs increases, leading to semantic conflicts and visual artifacts of generated images. Additionally, they do not adequately utilize the interactive dynamics between the LoRA models and the base model. To address these challenges, our study proposes a novel perspective: analyzing the contributions of different LoRAs in the Fourier domain and developing a novel LoRA-composition framework to mitigate the semantic conflicts arising from multi-LoRA composition.

% \az{Okay, here is writing style thing. Americans tend to put related work at the end, which is okay, so let's do that for this paper. However, I normally prefer to put what you have in 3.1 into a part in this related work and drive to method by pointing out at the end here saying something like "These existing methods fail to address XXX" to bridge to your method. But since it seems like you already have a related work at the end structure here, lets move this part to the end, sorry for the back and forth on this.}