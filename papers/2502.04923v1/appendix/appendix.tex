\section{LoRA-based Manipulations}
\subsection{LoRA Merge through a Weight Fusion Perspective}
\paragraph{Component-wise Composition of LoRA}
LoRA Merge is usually realized by linearly combining multiple LoRAs to synthesize a unified LoRA, subsequently plugged into the diffusion model. Formally, when introducing $N$ distinct LoRAs, the consequent updated matrix $\hat W$ in $\epsilon_{\theta}$ is given by:

\begin{equation}
    \hat{W} = W+\sum^{N}_{i=1}w_{i}\Delta W_{i} = W+\sum^{N}_{i=1}w_{i}B_{i}A_{i},
    \label{lora merge2}
\end{equation}
where $\sum^{N}_{i=1}w_{i}=1$~\citep{loramerge}. This manner prevents any adverse impact on the embedding of the original model, but it leads to the loss of individual LoRA characteristics, as the composition weight $w_{i}$ for each trained LoRA is reduced.

\paragraph{Element-wise Composition of LoRA}
This process integrates the corresponding parameters of the LoRA modules, requiring the modules being combined to have the same rank $r$ to properly align the structures~\citep{lorahub}. Given that $\Delta W_{i} = B_{i}A_{i}$, the combined LoRA module $\hat{W}$ can be obtained by:

\begin{equation}
    \hat{W} = (w_{1}B_{1}+w_{2}B_{2}+\cdots+w_{N}B_{N})(w_{1}A_{1}+w_{2}A_{2}+\cdots+w_{N}A_{N}),
    \label{lora merge3}
\end{equation}
where the set of optimal weights $\{w_{1},w_{2},\cdots,w_{N}\}$ are trained through a black-box optimization.

\subsection{LoRA Merge through a Gradient Fusion Perspective}
Compared to weight fusion, gradient fusion aligns the inference behavior of each individual concept, significantly reducing identity loss~\citep{mixlora}. The gradient fusion method first decodes the individual concepts using their respective LoRA weights. It then extracts the input and output features associated with each LoRA layer. These input/output features from different concepts are concatenated, and fused gradients are used to update each layer $W$ using the following objective:

\begin{equation}
    \label{lorag}
    W = \arg \min_{W}\sum_{i=1}^{N} ||(W_{0}+\Delta W_{i})X_{i}-WX_{i}||^{2}_{F},
\end{equation}
where $X_{i}$ represents the input activation of the $i$-th concept and $||\cdot||$ denotes the Frobenius norm.

\subsection{LoRA Merge through a Decoding-Centric Perspective}
\label{sec:decoding}
\paragraph{LoRA Switch}
With a set of $N$ LoRAs, the methodology initiates with a prearranged sequence of permutations. Starting from the first LoRA, the model transitions to the subsequent LoRA every $\tau$ step~\citep{multilora}. The active LoRA at each denoising timestep $t$, ranging from $1$ to the total number of steps required, is determined by the following equations:

\begin{equation}
\begin{aligned}
    \label{loraswitch}
    \lambda &= \lfloor((t-1)\text{mod}(N\tau))/\tau\rfloor+1,\\
    \hat{W}_{t} &= W + w_{i}\Delta W_{i}=W+w_{i}B_{i}A_{i},
\end{aligned}
\end{equation}
where $i$ indicates the index of the currently active LoRA, iterating from $1$ to $N$, $\lfloor\cdot\rfloor$ is the floor function, and the weight matrix $\hat{W}_{t}$ is updated to reflect the contribution from the weighted active LoRA $w_{i}\Delta W_{i}$. \textit{By selectively enabling one LoRA at a time, LoRA Switch ensures focused attention to the details pertinent to the current element, thus preserving the integrity and quality of the generated image throughout the process}~\citep{multilora}.

\paragraph{LoRA Composite}
LoRA Composite method involves calculating both unconditional and conditional score estimates for each LoRA individually at every denoising step. \textit{By aggregating these scores, the technique ensures balanced guidance throughout the image generation process}~\citep{multilora}.

With $N$ LoRAs in place, let $\hat\theta_{i}$ denote the parameters of the diffusion model $e_{\theta}$ after incorporating the $i$-th LoRA. The collective guidance $\hat{e}(z_{t},c)$ based on textual condition $c$ is derived by aggregating the scores from each LoRA, as depicted in the equation below:

\begin{equation}
    \label{loracomposite}
    \hat{e}(z_{t},c) = \frac{1}{N}\sum^{N}_{i=1}w_{i}[(1-s)\cdot e_{\hat\theta_{i}}(z_{t})+s\cdot e_{\hat\theta_{i}}(z_{t},c)],
\end{equation}
where $w_i$ is a scalar weight allocated to each LoRA, intended to adjust the influence of the $i$-th LoRA~\citep{multilora}.

\subsection{CMLoRA}
\subsubsection{Relation to Classifier Free Guidance}
With a set of $N$ LoRAs in place, let $\hat\theta_{i}$ denote the parameters of the diffusion model $e_{\theta}$ after incorporating the $i$-th LoRA. For a generative model $e_{\theta}$ integrated with $i$-th LoRA, its classifier-free guidance $\tilde{e}_{\hat\theta_{i}}(\mathbf{z}_{t},c)$ based on textual condition $c$ is:
\begin{equation}
(1-s)\cdot e_{\hat\theta_{i}}(\mathbf{z}_{t})+s\cdot e_{\hat\theta_{i}}(\mathbf{z}_{t},c).
\end{equation}

The collective guidance $\hat{e}(\mathbf{z}_{t},c)$ based on textual condition $c$ is derived by aggregating the scores from the generative model integrated with each LoRA: 
\begin{equation}
\hat{e}(\mathbf{z}_{t},c) = \frac{1}{N}\sum^{N}_{i=1}w_{i}[(1-s)\cdot e_{\hat\theta_{i}}(\mathbf{z}_{t})+s\cdot e_{\hat\theta_{i}}(\mathbf{z}_{t},c)],
\end{equation}
where $w_i$ is a real scalar weight allocated to each LoRA and $\sum^{N}_{i=1}w_{i}<\infty$, intended to adjust the influence of the $i$-th LoRA.

By aggregating these scores, the technique ensures harmonized guidance throughout the image generation process, facilitating the cohesive integration of all elements represented by different LoRAs.


\newpage
\section{Experimental results}
\label{app:results}

\subsection{ImageReward and ClipScore}
To ensure the reliability of our experimental results, we conduct image generation using three random seeds. All reported results in this paper represent the average evaluation scores across these three runs. The experiments were run with a mix of NVIDIA A$100$ GPUs with $40$GB memory and NVIDIA V$100$ GPUs with $16$GB memory. The total amount of inference time for all multi-LoRA composition methods under all metrics is around $1300$ GPU hours.

\input{table/animeirclipfull}
\input{table/realirclipfull}
\input{table/avgirclipfull}

\subsection{Computational Cost Analysis}
\label{sec:computation}
Across the investigated caching mechanisms, our proposed caching mechanism $Cache_{D}$ demonstrates the best performance, indicating that multi-LoRA composition methods utilizing $Cache_{D}$ degrade the semantic accuracy and aesthetic quality of the generated images the least. While the computational cost of the cache mechanism $Cache_{D}$ lies between that of uniform caching mechanisms $Cache_{c=2}$ and $Cache_{c=3}$, multi-LoRA composition methods with $Cache_{D}$ outperform those using other uniform caching mechanisms, as shown in \Cref{averageirclip}. Notably, $Cache_{D}$ can achieve, and in some cases surpass, the performance of advanced multi-LoRA composition methods, such as Switch-A and CMLoRA, especially as the number of composed LoRAs $N$ increases. Visual demonstrations of these results are provided in \Cref{c3,c4,c5,c6}. However, we find that there also exists a trade-off between the performance of multi-LoRA composition methods and their computational cost. Although CMLoRA achieves superior performance compared to Merge and Switch, it comes with higher computational costs. For instance, at $N=2$, CMLoRA incurs $912.350 \, G$ MACs compared to $789.770 \, G$ for Merge and $734.053 \, G$ for Switch. Similarly, at $N=5$, CMLoRA reaches $1570.335 \, G$ MACs, significantly higher than $946.721 \, G$ for Merge and $731.811 \, G$ for Switch.

\input{table/cachefull}

\subsection{MLLM Evaluation}
\input{table/fullllm1}
\input{table/fullllmD}

\newpage
\section{Presentation of Generated Image across Evaluated Multi-LoRA Methods}
\label{sec:visual}
\subsection{Demonstration of Anime Multi-LoRA Composition}

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.85\textwidth]{image/sshow1.jpg}
\end{center}
\caption{Generated images with different $N$ LoRA candidates ($L1$ Character, $L2$ Clothing, $L3$ Style, $L4$ Background and $L5$ Object) across our proposed framework and baseline methods.}
\label{c1}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.8\textwidth]{image/sshow2.jpg}
\end{center}
\caption{Generated images with different $N$ LoRA candidates ($M1$ Character, $M2$ Clothing, $M3$ Style, $M4$ Background and $M5$ Object) across our proposed framework and baseline methods. }
\label{c2}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.87\textwidth]{image/cmcache.jpg}
\end{center}
\caption{Generated images with different $N$ LoRA candidates ($S1$ Character, $S2$ Clothing, $S3$ Background and $S4$ Object) across CMLoRA with different caching mechanisms.}
\label{c3}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.87\textwidth]{image/mergecache.jpg}
\end{center}
\caption{Generated images with different $N$ LoRA candidates ($S1$ Character, $S2$ Clothing, $S3$ Background and $S4$ Object) across Merge with different caching mechanisms. }
\label{c4}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.87\textwidth]{image/switchcache.jpg}
\end{center}
\caption{Generated images with different $N$ LoRA candidates ($S1$ Character, $S2$ Clothing, $S3$ Background and $S4$ Object) across Switch with different caching mechanisms.}
\label{c5}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.86\textwidth]{image/compositecache.jpg}
\end{center}
\caption{Generated images with different $N$ LoRA candidates ($S1$ Character, $S2$ Clothing, $S3$ Background and $S4$ Object) across Composite with different caching mechanisms.}
\label{c6}
\end{figure}

\subsection{Demonstration of Reality Multi-LoRA Composition}
\begin{figure}[H]
\begin{center}
\includegraphics[width=\textwidth]{image/sshow3.jpg}
\end{center}
\caption{Generated images with different $N$ LoRA candidates ($R1$ Character, $R2$ Clothing, $R3$ Style, $R4$ Background and $R5$ Object) across our proposed framework and baseline methods. }
\label{c7}
\end{figure}

\newpage
\section{Limitations and Failure Cases}
\label{sec:limit}
There are some limitations of our work. First, it is important to note that there is a lack of a detailed image generation class taxonomy. This gap poses challenges in systematically classifying well-defined conceptual groups, particularly due to the semantic overlaps that inherently exist among some conceptual categories. These overlaps blur the boundaries between different conceptual categories, making it difficult to establish a robust and well-defined multi-LoRA composition testbed. However, if different LoRA categories possess distinct frequency characteristics, our proposed CMLoRA approach can still perform effectively.
Secondly, as a training-free method, CMLoRA operates independently of additional prior knowledge related to region or layout features, such as bounding box constraints or masked attention maps. This characteristic simplifies its deployment but also introduces certain limitations in handling spatial relationships effectively. As a result, CMLoRA struggles to effectively combine multiple LoRAs within similar semantic categories. This limitation is particularly problematic when multiple concepts within the same conceptual category need to be localized independently. The absence of explicit mechanisms for managing these localizations can lead to potential semantic conflicts, such as concept vanishing or distortion. These issues become especially pronounced when the frequency spectra of overlapping concepts interfere excessively.
Finally, we initially employ the traditional image metric, CLIPScore, to evaluate the comprehensive image generation capabilities of all multi-LoRA composition methods. While CLIPScore performs well to evaluate general image-text alignment within its domains, it encounters limitations when applied to scenarios requiring the assessment of out-of-distribution (OOD) concepts, such as user-specific instances. Its evaluations fall short in capturing specific compositional and quality aspects, as it lacks the capability to discern the nuanced features of individual elements~\citep{multilora}. This limitation inherently results in a compressed range of evaluation scores for multi-LoRA composition methods, causing improvements to appear marginal despite significant advancements in comprehensive compositional quality. To address this evaluation gap, we leverage the capabilities of multi-modal large language models (MLLMs) to evaluate composable multi-concept image generation in \Cref{sec:llmeva}.

\subsection{Demonstration of Failure Cases in Multi-LoRA Composition}
\begin{figure}[H]
\begin{center}
\includegraphics[width=\textwidth]{image/F.jpg}
\end{center}
\caption{Generated images with different $N$ LoRA candidates ($F1$ Character, $F2$ Animal, $F3$ Background and $F4$ Building) across our proposed framework and baseline methods.}
\end{figure}

\section{Evaluation Methodology}
\label{sec:appendixc}
Existing traditional image generation metrics primarily focus on text-image alignment but often overlook the complexity of individual elements within an image and the quality of their composition. Thus, we construct the following evaluation pipeline based on the MLLM.

\subsection{Evaluation Pipeline}
Image generation: Use both models to generate images based on the same set of prompts (both simple and complex).

In-context few-shot learning: Give a few evaluation examples to the evaluator.

Blind scoring: Let the evaluator rate the images based on the criteria without knowing which model created them.

Score aggregation: Average the scores for each dimension across all prompts to identify overall performance trends.

Comparative analysis: Compare the total and individual dimension scores between models to draw insights on strengths and weaknesses.

\subsection{Image Evaluation Metrics}
\subsection*{1) Element Integration}

Score on a scale of 0 to 10, in 0.5 increments, where 10 is the best and 0 is the worst.

\textbf{Description:} How seamlessly different elements are combined within the image.

\textbf{Criteria:}
\begin{itemize}
    \item \textbf{Visual Cohesion:} Assess whether elements appear as part of a unified scene rather than disjointed parts.
    \item \textbf{Object Overlap and Interaction:} Check for natural overlaps and interactions between objects, avoiding unnatural placements or intersections.
\end{itemize}

\subsection*{2) Spatial Consistency}

Score on a scale of 0 to 10, in 0.5 increments, where 10 is the best and 0 is the worst.

\textbf{Description:} Uniformity in style, lighting, and perspective across all elements.

\textbf{Criteria:}
\begin{itemize}
    \item \textbf{Stylistic Uniformity:} All elements should share a consistent artistic style (e.g., realism, cartoonish).
    \item \textbf{Lighting and Shadows:} Ensure consistent light sources and shadow directions to maintain realism.
    \item \textbf{Perspective Alignment:} Elements should adhere to a common perspective, avoiding mismatched viewpoints.
\end{itemize}

\subsection*{3) Semantic Accuracy}

Score on a scale of 0 to 10, in 0.5 increments, where 10 is the best and 0 is the worst.

\textbf{Description:} Correct interpretation and representation of each element as described in the prompt.

\textbf{Criteria:}
\begin{itemize}
    \item \textbf{Object Accuracy:} Objects should match their descriptions in type, attributes, and context.
    \item \textbf{Action and Interaction:} Actions or interactions between objects should be depicted correctly.
\end{itemize}

\subsection*{4) Aesthetic Quality}

Score on a scale of 0 to 10, in 0.5 increments, where 10 is the best and 0 is the worst.

\textbf{Description:} Overall visual appeal and artistic quality of the generated image.

\textbf{Criteria:}
\begin{itemize}
    \item \textbf{Color Harmony:} Use of color palettes that are visually pleasing and appropriate for the scene.
    \item \textbf{Composition Balance:} Balanced arrangement of elements to create an engaging composition.
    \item \textbf{Clarity and Sharpness:} Images should be clear, with well-defined elements and no unwanted blurriness.
\end{itemize}

\section{Ablation Analysis}
\label{app:ablation}
To enhance our understanding of the proposed methods, we further conduct the following ablation studies in the field of LoRA fusion sequence and caching strategy of non-dominant LoRA based on our proposed CMLoRA.

\textbf{Dominant LoRA Order Sequence Determination}
Building on our previous discussions, we can identify optimal dominant LoRA candidates during the denoising process, leading us to formulate the following combination optimization problem: \textit{How to derive the denoising range (step-length) of the activated dominant LoRA?}

Utilizing the structure of our Cached Multi-LoRA (CMLoRA) framework, feature maps generated by different LoRAs can be dynamically fused at each inference step. We define the total denoising range (step-length) for a dominant LoRA $i$ as $D_{i}$. In our configuration, we assume that each LoRA contributes equally, leading us to allocate the dominant range $D_{i}$ uniformly across all active LoRAs throughout the inference process.

Consider a scenario with a total of $T$ denoising steps and $N$ LoRAs. We set $ D_{i}=\lfloor\frac{T-1}{N}\rfloor, \forall i$. We have high-frequency LoRA set $H$ and low-frequency LoRA set $L$. Inspired by the LoRA switch mechanism, we implement a cyclic pattern of dominant LoRAs among the candidates in set $H$ at the beginning of the denoising process, switching the dominant LoRA every step. To ensure convergence during the denoising process, we designate to use the low-frequency LoRA from the low-frequency LoRA set $L$ at the end of $D_{i}$ steps. By implementing this approach, we effectively harness the pronounced dynamics of high-frequency components while simultaneously benefiting from the stabilizing attributes of low-frequency elements, ultimately leading to visual consistency of multi-LoRA composition.

\paragraph{Dominant LoRA Scale $w_{dom}$}
For the dominant LoRA, we assign a weight denoted as $w_{\text{dom}}$. For the non-dominant LoRAs, we set their weights as $w_{\text{non}} = \frac{N}{w_{\text{dom}} + N - 1}$, where $N$ is the total number of composed LoRAs. To regulate $w_{\text{dom}}$ during the diffusion process, we employ a decaying method. This decaying strategy not only stabilizes the denoising process but also plays a critical role in reducing semantic conflicts between different LoRAs. By gradually attenuating the influence of the dominant LoRAs as the denoising process progresses, it prevents abrupt changes in texture and edge features that could disrupt with global structures. This ensures smoother transitions between the contributions of various LoRAs, leading to a more harmonious integration of both high- and low-frequency components. As a result, the overall semantic coherence is preserved, minimizing the risk of feature misalignment.

We initially set the dominant weight scale $w_{\text{dom}}$ to $N-\alpha$, where $N$ is the total number of activated LoRAs and $\alpha\in\mathbb{R}^{+}$. This choice allows us to balance the contribution of the dominant LoRA against the collective contribution of the non-dominant LoRAs. To optimize this balance, we conduct a grid search over $\alpha$ in set $\{0.1,0.2,\cdots,0.8,0.9\}$ By adjusting $\alpha$, we can finetune the influence of the dominant LoRA, ensuring it does not overpower the others. Then we choose the optimal $\alpha=0.5$.

Based on the principle that \textit{high-frequency components display more pronounced dynamics during the early stage of the denoising process}~\citep{freeu}, this weight scale is adjusted using a decaying method. For the $i$-th turn of switching the dominant LoRA, the weight is define as: $w^{i}_{\text{dom}}=w^{i-1}_{\text{dom}}-0.5^{i}$.

\paragraph{Caching Interval and Modulation Hyper-parameters $c_{1}, c_{2}$}
To capture the similarity trend of feature maps fused by LoRAs, we propose a non-uniform caching interval strategy with two specialized hyper-parameters: $c_{1}, c_{2}\in\mathbb{Z}$. These hyper-parameters control the strength of the caching behavior during inference. Specifically, for a denoising process with $T$ timesteps, the sequence of timesteps that performs full inference is:
\begin{equation}
\begin{aligned}
    \label{app:cache interval2}
    \mathcal{I} &= \mathcal{I}_{1} \cup \mathcal{I}_{2} \cup \mathcal{I}_{3} \\
    \mathcal{I}_{1} &= \{c_{1}\cdot t \mid 0 \leq c_{1}\cdot t < \left\lfloor 0.4 \cdot T \right\rfloor, \ \text{where} \ t \in \mathbb{Z}\} \\
    \mathcal{I}_{2} &= \{\left\lfloor 0.4 \cdot T \right\rfloor + c_{2}\cdot t \mid \left\lfloor 0.4 \cdot T \right\rfloor \leq c_{2}\cdot t < \left\lfloor 0.9 \cdot T \right\rfloor, \ \text{where} \ t \in \mathbb{Z}\} \\
    \mathcal{I}_{3} &= \{\left\lfloor 0.9 \cdot T \right\rfloor + c_{1}\cdot t \mid \left\lfloor 0.9 \cdot T \right\rfloor \leq c_{1}\cdot t < T, \ \text{where} \ t \in \mathbb{Z}\}.
\end{aligned}
\end{equation}

The discrete interval $\mathcal{I}_{4} = [\left\lfloor 0.4 \cdot T \right\rfloor, \left\lfloor 0.9 \cdot T \right\rfloor]=\{ k \in \mathbb{Z} \mid k = 5n, \, n \in \mathbb{Z}, \, \left\lfloor 0.4 \cdot T \right\rfloor \leq k \leq \left\lfloor 0.9 \cdot T \right\rfloor \}$
 are established based on the condition that the average similarity $s_{t}$ of the cached features at timestep $t$ in a interval exceeds $20\%$ within a $90\%$ confidence interval. Since we have only finite discrete samples, we conduct our calculation based on the Monte Carlo method. Formally, $\forall t \in \mathcal{I}_{4}$, $\overline{\mathcal{P}}\left(0.2\in[s_{t}-\text{std}(s_{t}),s_{t}+\text{std}(s_{t})]\right)=0.9$, where $\overline{\mathcal{P}}$ is the probability averaged on the similarity $s_{t}$ of the cached features for all discrete timesteps $t\in \mathcal{I}_{4}$ and $\text{std}$ is the standard deviation of $s_{t}$ at timestep $t$.

Given that cached LoRA features exhibit greater similarity in $\mathcal{I}_{4}$ compare to $\mathcal{I}\setminus\mathcal{I}_{4}$, we intuitively select $c_{1} < c_{2}$. This selection is informed by a grid search over the pairs $(c_{1},c_{2})$ in the Cartesian product of two discrete sets $[1,5]\times[1,5]$. When $c_1<c_2<4$, we find that there is minimal variation in the content of the image, accompanied by only slight fluctuations in the CLIP Score. We observe a performance deterioration if we choose $5<c_1<c_2$. Finally, we obtain the optimal caching modulation hyperparameters: $(2,3)$.

\subsection{Order of LoRA Activation}
\label{sec:ablation1}
\citet{multilora} propose that \textit{The initial choice of LoRA in the activation sequence clearly influences overall performance, while alterations in the subsequent order have minimal impact}, so we conduct the following ablation study to demonstrate the effectiveness of our LoRA fusion order based on frequency partition.

\input{table/ablation1}

\subsection{Cache Interval}
\label{sec:ablation2}
Additionally, we compare our proposed caching strategy with other uniform caching methods, demonstrating that our approach outperforms them.

\input{table/ablation2}

\begin{figure}[H]
\begin{center}
\includegraphics[width=0.75\textwidth]{image/ablation2.png}
\end{center}
\caption{Results of Computational Cost of Different Cache Methods. MACs refer to Multiple-Accumulate Operations.}
\end{figure}
