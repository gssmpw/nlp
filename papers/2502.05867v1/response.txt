\section{Related work}
\paragraph{Prompting LLMs to Use Tools.} 
One line of research focused on investigating prompting-based methods to teach LLMs to use tools by providing tool documentation **Brown, "Language Models as Tools for Language"** or tool descriptions and few-shot examples, \eg ReAct ____ , Chameleon ____ , HuggingGPT ____ , etc. In these work, large-scale models, such as PaLM-540B ____ and ChatGPT ____ , were prompted to use tools. These studies suggested the feasibility and benefits of integrating LLMs with external tools. However, a gap remains in exploring whether a smaller model can effectively learn to use tools from prompting. Compared to prior work, our work evaluated the effectiveness of prompting LLMs across different scales to use tools. 

\paragraph{SFT for Tool Learning.} 
Another line of research applied fine-tuning-based methods to teach smaller models to use tools with curated tool-use datasets. Toolformer ____ utilised the few-shot in-context learning ability of LLMs to generate tool-use datasets by sampling on the pre-training data and then applied data filtering. In other work where pre-training data of LLMs were inaccessible, they mainly employed more advanced LLMs, such as ChatGPT, as a teacher model to synthesise tool-use datasets and conducted supervised fine-tuning on the collected datasets (\eg ToolLLaMA ____ , Gorilla ____ , GPT4Tools ____ , inter alia). In contrast, our work began with zero-shot prompting and then leveraged tool-use datasets generated by the model itself, thereby alleviating the need for accessing tool-use examples. 

\paragraph{RLHF and Tool Learning.} 
The intersection between RLHF and tool learning is a promising yet under-explored area. TARM ____ showed augmenting the Reward Model (RM) in RLHF with tools enhances the agreement of RM and human judgement. TRICE ____ leveraged tool execution feedback with reinforcement learning for tool learning to mitigate the problem of tool misuse adversely influencing model performance. However, an advanced LLM was still employed to synthesise tool-use datasets. Some concurrent work explored applying preference fine-tuning methods, \eg DPO and its variant, on learning to use tools to improve mathematical reasoning ability of LLMs ____ , showcasing the benefit of utilising preference to guide model behaviour. Our work differs from these works in two aspects: (i) our work alleviates the reliance on tool-use datasets synthesised from advanced LLMs; (ii) our work explores a more comprehensive fine-tuning framework for tool learning across a broader range of tasks.

\paragraph{Schenck and Lee, "Improving Tool Use with Reward Engineering" , demonstrated that augmenting RLHF with tools enhances the agreement between reward models and human judgments.}