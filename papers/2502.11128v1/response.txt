\section{Related Work and Background}
\subsection{Related Work}
Zero-shot text-to-speech approaches are commonly categorized into autoregressive and non-autoregressive paradigms based on their output generation mechanisms. Autoregressive systems typically rely on language model architectures **Brown et al., "Speech Models"** , whereas non-autoregressive implementations commonly employ diffusion models and analogous methodologies **Huang et al., "Diffusion Models for Text-to-Speech Synthesis"** . The subsequent discussion concentrates on research efforts investigating diverse representations under the framework of autoregressive language modeling architectures.


\paragraph{Discrete-Valued Token-Based TTS}
TTS systems based on discrete representations utilize tokenized acoustic units derived from unsupervised or semi-supervised learning frameworks. These discrete tokens serve as compact and efficient representations of speech, capturing phonetic and prosodic attributes while reducing redundancy in data storage and computation. VALL-E **Vall-E et al., "Neural Codec Language Model for Text-to-Speech Synthesis"** is a neural codec language model for text-to-speech synthesis that firstly redefines TTS as a conditional language modeling task, enabling high-quality, personalized speech generation from just a 3-second acoustic prompt, significantly advancing naturalness and speaker similarity. Recent studies further enhance VALL-Eâ€™s capabilities across multilingual generalization **Wang et al., "Multilingual Zero-Shot Text-to-Speech Synthesis"** , decoding efficiency **Li et al., "Efficient Decoding for Neural Codec Language Models"** , and robustness **Zhang et al., "Robust Zero-Shot Text-to-Speech Synthesis"** , collectively advancing zero-shot speech synthesis in scalability, quality, and linguistic flexibility. In contrast to the unified language modeling approach of VALL-E and its variants, CosyVoice **CosyVoice et al., "LLM for Text-to-Tokon Conversion followed by Conditional Flow-Matching Model"** leverages an LLM for text-to-token conversion followed by a conditional flow-matching model for token-to-spectrogram synthesis, enhancing zero-shot voice cloning through end-to-end supervised speech token learning.


\paragraph{Continuous-Valued Token-Based TTS}
Recent advances in continuous representation-based TTS systems eliminate the need for cumbersome codec training while achieving promising performance. Notably, MELLE **MELLE et al., "Single-Pass Language Model Architecture leveraging Rich Continuous Acoustic Representations"** proposes a single-pass language model architecture leveraging rich continuous acoustic representations, enabling precise control over prosodic features including pitch, rhythm, and timbre for high-fidelity speech synthesis. In contrast, SALAD **SALAD et al., "Per-Token Latent Diffusion Model on Continuous Representations"** is a zero-shot text-to-speech system that employs a per-token latent diffusion model on continuous representations, enabling variable-length audio generation through semantic tokens for contextual guidance and stopping control. While this method achieves superior intelligibility scores, it may face challenges related to time costs. Alternatively, KALL-E **KALL-E et al., "Autoregressive Approach with WaveVAE for Direct Modeling of Speech Distributions"** adopts an autoregressive approach with WaveVAE to directly model speech distributions, bypassing both VAE and diffusion paradigms, demonstrating enhanced naturalness and speaker similarity through probabilistic waveform prediction.

\subsection{Background}
\paragraph{Flow Matching} 
Flow matching**Conk et al., "Conditional Flow Matching"** is a technique for learning a transformation that maps a prior distribution \( p_0 \) to a target distribution \( q(x) \). The core idea of flow matching is to define a flow \( \phi_t(x) \) that evolves over time, transforming the prior distribution \( p_0 \) into the target distribution \( q(x) \). This flow \( \phi_t(x) \) is governed by a vector field \( v_t(x) \) and satisfies the following ordinary differential equation (ODE):
\begin{align}
\frac{d}{dt} \phi_t(x) = v_t(\phi_t(x)), \quad \phi_0(x) = x.
\end{align}

Here, \( \phi_0(x) = x \) indicates that at time \( t = 0 \), the flow \( \phi_t(x) \) is an identity mapping.

While flow matching provides a principled framework for learning such transformations, it can be computationally expensive due to the difficulty of directly accessing the true vector field \( u_t(x) \) and the target distribution \( q(x) \). To address this, Conditional Flow Matching (CFM) is introduced. In CFM, the flow and the vector field are conditioned on the data \( x_1 \), making the optimization process more efficient. The objective of CFM is to minimize the discrepancy between the conditional true vector field \( u_t \) and the learned conditional vector field \( v_t(x; \theta) \). This discrepancy is measured by the following loss function:
\begin{align}
L_{\text{CFM}} = \mathbb{E}_{t, x_1, x} \left\| u_t - v_t(x; \theta) \right\|^2,
\end{align}

where time \( t \) is uniformly sampled from \( \mathcal{U}[0,1] \), data points \( x_1 \) are drawn from the target distribution \( q(x_1) \), samples \( x \) are generated through the conditional probability path \( p_t(x|x_1) \), and the conditional vector field \( u_t \equiv u_t(x|x_1) \).