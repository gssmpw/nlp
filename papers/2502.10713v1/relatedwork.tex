\section{Related work}
\label{section2}




\textbf{Feature extraction:} The first methods for feature extraction are based on hand-craft features, where dense trajectories \cite{wang2011aser, wang2013action} is a commonly used approach. It involves tracking key points across consecutive frames using optical flow with space-time descriptors such as Histograms Of Gradients (HOG), Histograms of Optical Flow (HOF), and Motion Boundary Histogram (MBH). After the introduction of deep learning, 3D CNN has been widely used for feature representation, either for individual frames or a sequence of frames. Specifically, state-of-the-art Inflated 3D CNN (I3D)  \cite{carreira2017quo} employs the Inception-V1 model as a backbone \cite{szegedy2015going} with 3D kernels to facilitate the direct spatio-temporal processing. The I3D model is pre-trained on large-scale video datasets that contain a diverse range of human actions \cite{kay2017kinetics}. The pre-training enables the model to learn generic features useful for various downstream tasks, including action recognition and video segmentation.


\textbf{Supervised action segmentation:} 
Supervised action segmentation methods mainly utilize frame-wise labeled videos for supervised learning, where Temporal 1D Convolutional Networks (TCNs) have gained widespread adoption due to their ability to effectively capture temporal dependencies within sequences. Diverse architectures were introduced a temporal convolution encoder-decoder architecture that decreases and increases the size of the temporal resolution using pooling and up-sampling, respectively \cite{lea2017temporal, ding2018weakly, lei2018temporal,singhania2023c2f}. However, the multi-stage architecture (MS-TCN) retains the same temporal resolution and extends the receptive field by employing increasingly larger dilated convolutions. This approach avoids using pooling to preserve features, as boundaries are sensitive to feature loss. 

Transformer which was originally designed for natural language processing, mainly relies on attention mechanism for sequence modeling \cite{vaswani2017attention}. In action segmentation, TimeSformer \cite{bertasius2021space} adapts the traditional Transformer architecture for video processing \cite{dosovitskiy2020vit, arnab2021vivit} to learn spatio-temporal features directly from sequences of frame-level patches. Their experiments indicate that using separate spatial and temporal attention is more efficient.  ASFormer \cite{yi2021asformer}  was among the pioneering transformer architectures for temporal action segmentation. It adapts the encoder-decoder framework of ED-TCN \cite{lea2017temporal}, and replaces the convolutions operations with transformer blocks with local window attention that grows in size with each layer. \blue{TUT \cite{du2023we} proposed a pure Transformer-based model that incorporates temporal sampling instead of temporal convolutions to reduce complexity, and to address the boundary misclassification, by proposing a boundary-aware loss that leverages similarity scores from attention modules.}

Although action segmentation is widely applied on RGB video sequences, some works investigated the problem in skeleton-based sequences \cite{filtjens2022skeleton,tian2023stga}. Graph Convolutional Network (GCN) is more appropriate to model the skeleton. Therefore, they are widely utilized in the segmentation process instead of temporal convolutions.  Multi-Stage spatial-temporal Graph Convolutional Neural network (MS-GCN) \cite{filtjens2022skeleton} replaces initial temporal convolutions with spatial graph convolutions to capture spatial hierarchies and long-term temporal dynamics in the skeleton sequence. STGA-Net \cite{tian2023stga} introduces a spatial-temporal graph attention network (STGA-Net), which includes an attentive block within the encoder-decoder to model dynamic correlations among human joints, addressing the lack of an explicit transition rule between segments. 


\textbf{Over-segmentation:} The prediction of a single action class per frame frequently encounter over-segmentation errors, where boundary segments are misallocated. Therefore, alternative methods specifically tackle the problem by focusing on boundary detection or refinement. BCN \cite{wang2020boundary} mitigates boundary ambiguity by introducing a refinement branch for MS-TCN to detect the start and the end of segments. \citet{ishikawa2021alleviating} proposed integrating an additional network branch dedicated to boundary identification by assigning high probability to boundary frames. \citet{ding2022leveraging} and \citet{ding2018weakly} proposed using soft boundary detection. Instead of having a clear-cut distinction of where one action ends and another begins, they detect fluid transitions for more robust boundary localization.

\textbf{Unsupervised action segmentation:} Unsupervised action segmentation approaches can be categorized into two categories. Self-supervised methods that learn action representations, and fully unsupervised methods that operates on the input features without requiring a learning framework. In self-supervised approaches, \citet{kukleva2019unsupervised} exploit the sequential nature of activities to learn continuous temporal embedding based on frame-wise features with respect to their relative time in the sequence, and then the embedding features are clustered to identify temporal segments. \citet{vidalmata2021joint} combine visual embedding derived from a predictive U-Net architecture with a temporal continuous embedding. Object-centric Temporal Action Segmentation (OTAS) \cite{li2024otas} introduced self-supervised global and local feature extraction with a boundary selection module to detect salient boundaries. In contrast to existing approaches that involve representation learning then offline clustering, \citet{kumar2022unsupervised} proposed a joint self-supervised representation learning and online clustering in a unified end-to-end learning scheme. Although fully unsupervised methods are not widely investigated as self-supervised methods, there are some works  showed that applying similarity metrics or clustering directly on the frame-wise features could outperform representation learning-based methods \cite{sarfraz2019efficient,sarfraz2021temporally,du2022fast}. 

\textbf{Weakly supervised action segmentation:} Weakly supervised approaches involve global labeling such as action sets, transcript, or timestamp methods. In action sets methods,  each video is associated with a unique unordered set of actions.  Transcript methods require an ordered lists of actions, and  timestamp methods rely on pseudo frame-wise labels. \citet{richard2018action} proposed one of the first action sets methods that doesn't require prior knowledge of the number or the occurrence order of actions. \citet{lu2022set} exploit the fact that videos within the same task have similar action orderings, to introduce the Pairwise Ordering Consistency (POC) loss that ensures consistent predictions across different videos of the same task. NN-Viterbi \cite{richard2018neuralnetwork}, a pioneering transcript-based method, which demonstrates higher performance by generating pseudo-labels from transcripts using Viterbi decoding for model training. \citet{xu2024efficient}  filter noisy boundaries and detect transitions while incorporating video-level losses to improve semantic learning for noisy pseudo-segmentations. \citet{du2023timestamp} introduce a clustering-based framework for timestamp-supervised action segmentation, which tackles incorrect pseudo-labels in ambiguous intervals. This framework includes pseudo-label ensembling to generate high-quality labels and iterative clustering for their propagation. \citet{hirsch2024random} reformulate temporal action segmentation as a graph segmentation problem with weak supervision via timestamp labels to reduce annotation costs.


\begin{figure}
\centering
\includegraphics[scale=0.6]{blocks.png}
\caption{Encoder structure (left) and Temporal Convolution Block:TCB (right), which are modified versions of the ASFormer \cite{yi2021asformer} encoder and a single stage of MS-TCN \cite{farha2019ms}, respectively. The components added are in green color. }
\label{fig:comp} 
\end{figure}