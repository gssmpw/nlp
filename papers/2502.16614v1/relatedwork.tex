\section{Related Works}
\subsection{Code LLMs} 

Large Language Models (LLMs) have rapidly developed and are significantly impacting automated software development. These foundational models can produce fluent human language and understand semantics to perform complex tasks, bringing new possibilities to automated code generation, including Santacoder~\cite{allal2023santacoder}, CodeGPT~\cite{lu2021codexglue}, etc. More and more open-source and proprietary Code LLMs have emerged and demonstrated competitive performance, including Starcoder~\cite{li2023starcoder}, CodeLlama~\cite{roziere2023code}, Wizardcoder~\cite{luo2023wizardcoder}, DeepSeek-Coder~\cite{guo2024deepseek}, Qwen-Coder~\cite{hui2024qwen2}, Claude-3.5-Sonnet~\cite{claude_2024} and GPT-4o~\cite{gpt_4o} etc. 

\subsection{Critic Models for LLMs}

Reinforcement learning from human feedback has proven effective~\cite{achiam2023gpt}, though it can be very labor-intensive. A promising approach involves using LLMs to themselves to assist in the evaluation, which can further improve model outputs~\cite{saunders2022self, mcaleese2024llm}. Accurate and informative critique feedback allows LLMs to refine outputs, moving towards more advanced intelligence. However, despite their problem-solving strengths, LLMs currently demonstrate weak performance in critique tasks~\citep{zheng2024critic, yang2024supercorrect}. Improving LLM critique abilities relies on supervision from human annotations~\cite{saunders2022self,mcaleese2024llm} and stronger LLMs acting as human proxies~\cite{lan2024training,zhang2024generative,ke2024critiquellm,ankner2024critique,zheng2024critic,yang2024supercorrect,sharma2024critical}.
Recently, some critic benchmarks~\citep{zheng2024critic,tang2025real} have also been proposed.