\section{Related Work}
\textbf{Vision Transformers:} ViTs segment images into patches and apply self-attention**Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** to learn inter-patch relationships, outperforming CNNs across multiple vision tasks**Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"**. Nevertheless, ViTs face challenges like high parameter counts**Touvron et al., "Training Data-Agnostic Networks with Mixed Precision"**, and increased computational complexity proportional to token length**Levine et al., "Sparse Transformer"**. 
To enhance the computational efficiency of ViTs, many researchers**Liu et al., "Swin Transformer: Hierarchical Vision Transformers using Scalable-Spatial Scheduling"** are focused on exploring lightweight improvement methods. For example, LeViT**Graham et al., "LeViT: A Vision Transformer in 1/50th the Parameters"** incorporates convolutional elements to expedite processing, and MobileViT**Mehta et al., "MobileViT: Efficient Vision Transformers using MobileNetV3 Blocks"** combines lightweight MobileNet blocks with MHSA, achieving lightweight ViTs successfully. However, these enhancements still rely on expensive MAC computations which are not suitable for resource-limited devices. This highlights the need for investigating more energy-efficient ViT solutions.

\textbf{Spiking Neural Networks:} The event-driven mechanism enhances the energy efficiency of SNNs, offering a significant advantage for compute-constrained edge devices. With the introduction of ANN-SNN**Panda et al., "A Novel Approach to Train Deep Spiking Neural Networks"** and direct training**Kheradpisheh et al., "SpikeTrain: A Spike-Based Training Algorithm for SNNs"** algorithm, the difficult associated with training high-performance SNNs is significantly reduced.
Based on these advanced learning algorithms, some research**Chen et al., "Deep Residual Spiking Neural Networks"** propose deep residual SNNs**Rathi et al., "Efficient Deep Learning using Spike-Based Spiking Neural Networks"**, and others**Liu et al., "Spike Attention Mechanism for Efficient Computation in SNNs"** contribute multi-dimensional spike attention mechanisms, achieving competitive performance on many tasks**Shi et al., "Temporal Convolutional Layers for Time Series Forecasting using SNNs"**. 
%Subsequently, some research explores parallel training strategies**Lee et al., "A Distributed Parallel Training Strategy for Deep SNNs"** and shortcut residual connections**Zhang et al., "Shortcut Residual Connections for Efficient Computation in SNNs"** for SNNs within the ResNet framework.
These improvements further enhance the application of SNNs in various visual tasks. However, despite rapid advancements, a significant performance gap remains between these traditional deep SNN architectures and the latest ViTs.

% ViTs process input images by segmenting them into non-overlapping patches and utilizes Self-Attention to learn inter-patch representations, effectively mitigating inductive bias**Parmar et al., "Image Transformers"**. ViTs has demonstrated superior performance across various computer vision tasks compared to Convolutional Neural Networks (CNNs). However, its limitations have become increasingly evident with continued development and application. These include a high parameter count, quadratic growth in computational complexity relative to token length, the presence of non-fusible normalization layers, and a lack of compiler-level optimizations. Recently, LeViT**Graham et al., "LeViT: A Vision Transformer in 1/50th the Parameters"** introduced a convolutional approach to accelerate ViT, though executing MHSA still requires reshaping 4D features into flat blocks, demanding substantial computational resources. Similarly, MobileViT**Mehta et al., "MobileViT: Efficient Vision Transformers using MobileNetV3 Blocks"** integrates lightweight MobileNet blocks (featuring pointwise and depthwise convolutions) with MHSA blocks. Despite these innovations, these methods still rely on full-precision floating-point computations, which remain computationally expensive on resource-constrained edge devices. Thus, exploring more efficient computational paradigms represents a potential direction for enhancing ViT performance.
\textbf{Vision Transformers Meet Spiking Neural Networks:} To explore high-performance and energy-efficient visual solutions, SNN-based ViTs**Kim et al., "Spikeformer: A Spike-Based Vision Transformer"** have emerged.
%have recently emerged.
Spikformer**Kim et al., "Spikeformer: A Spike-Based Vision Transformer"** pioneers a spike-based self-attention computation, establishing the first spiking ViT. 
However, they still utilize expensive MAC operations and matrix multiplication in self-attention computation, which are inefficient for binary spikes. 
Recently, Spike-driven Transformer**Kim et al., "Spike-driven Transformers: A Fully Spike-Driven Vision Transformer"** implements Hadamard product in the self-attention module for a fully spike-driven ViT. Additionally,  SpikingResformer**Shi et al., "Spiking Resformer: An Efficient and Scalable Vision Transformer using Dual Spike Self-Attention"** integrates a Dual Spike self-attention module for improved performance and energy efficiency. However, these models primarily treat self-attention as a token mixer**Parmar et al., "Image Transformers"**, without exploring an effective relevance computation suited to spike trains. Moreover, they also overlook the temporal dynamics of SNNs**Jia et al., "Temporal Convolutional Layers for Time Series Forecasting using SNNs"**. Therefore, developing spike self-attention mechanisms tailored to the spatio-temporal characteristics of SNNs is essential for further advancements.