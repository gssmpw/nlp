
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{bbding}
\usepackage{wrapfig}
\usepackage{threeparttable}
\usepackage{subfigure}

\usepackage{soul, color, xcolor}
\newcommand{\yy}[1]{\textcolor{magenta}{#1}}
%\newcommand{\yy}[1]{\textcolor{black}{#1}}
\newcommand{\yyhl}[1]{\textcolor{blue}{\hl{YY: #1}}}

\newcommand{\modify}[1]{\textcolor{blue}{#1}}
\newcommand{\zhrhl}[1]{\textcolor{blue}{\hl{Zhr: #1}}}

\title{Spiking Vision Transformer with Saccadic \\ Attention}
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\iclrfinalcopy 
\author{Shuai Wang$^{1}$, {Malu Zhang}$^{1}$\thanks{Corresponding author: maluzhang@uestc.edu.cn}, Dehao Zhang$^{1}$, Ammar Belatreche$^{2}$, \textbf{Yichen Xiao}$^{1}$,\\ \textbf{Yu Liang}$^{1}$, \textbf{Yimeng Shan}$^{3}$, \textbf{Qian Sun}$^{1}$, \textbf{Enqi Zhang}$^{1}$, \textbf{Yang Yang}$^{1}$\\ 
~\\
$^{1}$University of Electronic Science and Technology of China\\
$^{2}$Northumbria University, $^{3}$Liaoning Technical University\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
The combination of Spiking Neural Networks (SNNs) and Vision Transformers (ViTs) holds potential for achieving both energy efficiency and high performance, particularly suitable for edge vision applications. However, a significant performance gap still exists between SNN-based ViTs and their ANN counterparts. Here, we first analyze why SNN-based ViTs suffer from limited performance and identify a mismatch between the vanilla self-attention mechanism and spatio-temporal spike trains. This mismatch results in degraded spatial relevance and limited temporal interactions. To address these issues, we draw inspiration from biological saccadic attention mechanisms and introduce an innovative Saccadic Spike Self-Attention (SSSA) method. Specifically, in the spatial domain, SSSA employs a novel spike distribution-based method to effectively assess the relevance between Query and Key pairs in SNN-based ViTs. Temporally, SSSA employs a saccadic interaction module that dynamically focuses on selected visual areas at each timestep and significantly enhances whole scene understanding through temporal interactions.
Building on the SSSA mechanism, we develop a SNN-based Vision Transformer (SNN-ViT). Extensive experiments across various visual tasks demonstrate that SNN-ViT achieves state-of-the-art performance with linear computational complexity. The effectiveness and efficiency of the SNN-ViT highlight its potential for power-critical edge vision applications.
\end{abstract}
\section{Introduction}
Vision Transformers (ViTs)~\citep{dosovitskiy2020image} revolutionize the traditional computer vision field, achieving higher performance in many vision tasks such as image classification~\citep{chen2021crossvit, han2023flatten} and object detection~\citep{fang2021you, touvron2021training}. However, 
ViTs always demand significant computational and memory resources, which greatly restricts their deployment in resource-constrained edge vision environments~\citep{wu2022tinyvit, graham2021levit}.
Consequently, the development of energy-efficient and high-performance solutions remains a significant area of research that necessitates further investigation~\citep{cai2019once, han2020ghostnet}.



Spiking Neural Networks (SNNs), as the third generation of neural networks~\citep{maass1997networks,gerstner2002spiking,izhikevich2003simple,masquelier2008spike}, mimics biological information transmission mechanisms using discrete spikes as the medium for information exchange. Spiking neurons fire spikes only upon activation and remain silent at other times. 
This event-driven mechanism~\citep{caviglia2014asynchronous} promotes sparse synapse operations and avoids multiply-accumulate (MAC) operations, which significantly boost the energy efficiency of these models~\citep{zhang202322}.
 %This biomimetic and efficient event-driven mechanism positions SNNs as a promising tool to bridge the gap between biologically inspired brain intelligence and conventional machine intelligence. 
However, the architectures of most SNN-based models still revolve around traditional structures such as CNNs~\citep{fang2021incorporating, xing2019homeostasis} and ResNets~\citep{fang2021deep, hu2024advancing}, which exhibit a significant performance gap compared to ViTs. 

In recent years, numerous researchers have dedicated efforts to develop SNN-based ViT models. However, most studies~\citep{zhouspikformer, wang2023spatial} retain energy-intensive MAC operations in self-attention computational paradigm and not fully take advantage of SNNs' energy efficiency. Furthermore, these approaches still rely on the Dot-Product operation to measure the spatial relevance between Query (\(Q\)) and Key (\(K\)) pairs. However, they fail to account for whether the Dot-Product is well-suited to the binary spike characteristics of SNNs. 
%However, the efficacy of the Dot-Product depends on both the angle and magnitude of vectors~\citep{kim2021lipschitz}. 
Subsequently, inspired by Metaformer~\citep{yu2023metaformer}, Spike-driven V2~\citep{yao2024spike} introduces a MAC-free method, and SpikingResformer~\citep{shi2024spikingresformer} combines ResNet-based architecture and self-attention computation paradigm to further reduce parameters. These methods ensure the high performance of SNN-based ViTs while achieving a full spike-driven manner, offering significant energy savings. Nevertheless, these studies treat self-attention computational paradigm merely as an efficient token mixer~\citep{yu2022metaformer}, without exploring an effective paradigm suited to spike trains. Furthermore, these methods primarily focus on spatial feature extraction, overlooking the temporal dynamics of SNNs. Consequently, exploring spiking self-attention paradigms tailored to the spatio-temporal characteristic of SNNs represents a potential area for improvement.

%Recently, many researchers have focused on developing SNN-based ViT models. 
%However, the majority fails to capitalize on the inherent energy efficiency of SNNs, owing to retaining energy-intensive MAC operations in self-attention computations, such as scaled dot-product attention (Han et al., 2023), softmax functions (Leroux et al., 2023), and scaling (Zhou et al., 2023). 
%Subsequently, researchers such as [Author] proposed the spike-driven approach, and [Another Author] introduced the ResNetformer, both enhancing the performance of SNN-based Vision Transformers and the computation of spike-driven self-attention. However, these advancements primarily focus on spatial feature extraction, overlooking the intrinsic temporal dynamics of SNNs in self-attention computations. Therefore, developing a fully spike-driven self-attention mechanism that effectively utilizes the temporal resolution of SNNs represents a critical area for future enhancement.
%Despite numerous enhancements, current SNN-based transformers predominantly focus on approximating the spatial aspects of ANNs and overlook the intrinsic temporal dynamics of SNNs in self-attention computations. Thus, developing a fully spike-driven self-attention mechanism that effectively utilizes the temporal resolution of SNNs represents a critical area for future improvements.

%Biological visual cognition, a product of billions of years of evolution, exhibits advanced performances. 
%This capability dynamically captures and understands visual scenes through saccadic and microsaccadic mechanisms. Specifically, rather than using explicit computed attention scores, biological vision focuses attention by repeatedly scanning key visual areas.
%Notably, it demonstrates dynamic processing capabilities in static visual scenes. Specifically, the biological visual system dynamically understands and interprets visual information through Saccades and Microsaccades mechanisms. 
%As shown in Fig.\ref{Fig555}, biological vision employs saccadic attention for temporal integration, yielding higher energy efficiency and computational effectiveness than static self-attention. Therefore, this paper leverages the dynamic perceptual processes of biological vision, utilizing the unique timestep and event-driven properties of SNNs to develop a novel transformer based on biological dynamic visual cognition. 


%In contrast to static self-attention mechanisms, it demonstrates higher energy and computational efficiency. 

Biological vision dynamically captures and understands visual scenes through saccadic mechanisms~\citep{melcher2003spatiotopic, binda2018vision, guadron2022speed}. It focuses on specific visual areas at each moment and utilizes dynamic saccadic movements across the temporal domain to achieve a contextual understanding of the entire visual scene~\citep{hanning2023dissociable}. Compared to vanilla self-attention mechanisms~\citep{liu2021swin}, it offers higher energy and computational efficiency.
Additionally, the saccadic process involves intense temporal interactions~\citep{idrees2020perceptual}, which closely align with the unique temporal characteristics of SNNs.
Therefore, we draw inspiration from the saccadic mechanisms to design a Saccadic Spike Self-Attention (SSSA) method. The SSSA method adapts to the spatio-temporal characteristics of SNNs, enabling an efficient and effective comprehensive understanding of visual scenes.
%SSSA method effectively measures the spatial relevance between visual areas while achieving a comprehensive understanding of the visual scene with reduced computational complexity.
Based on this, we further develop a SNN-based Saccadic Vision Transformer. The summary contributions are as follows:

\begin{itemize}
\item We thoroughly analyze the reasons for the mismatch between the vanilla self-attention mechanism and SNNs. In the spatial domain, the binary and sparse nature of spikes creates significant magnitude differences between $Q$ and $K$ in SNN-based ViTs, making it difficult for vanilla self-attention to assess spatial relevance. Additionally, vanilla self-attention is designed for ANNs and neglects the temporal interactions among timesteps in SNNs, limiting its ability to explore information in the temporal domain.
%We thoroughly analyze the reason for the mismatch between the VSA mechanism and SNNs. In the spatial domain, extensive experiments show that differing magnitudes of Q and K in SNNs degrade spatial relevance. Temporally, most SNN-based ViTs rely solely on residual membrane potentials for temporal interactions. However, resetting and decay mechanisms make residual membrane potentials inadequate for capturing temporal decision dynamics, thus resulting in limited temporal interactions.
\item We propose a Saccadic Spike Self-Attention (SSSA) mechanism specifically designed for SNNs' spatio-temporal characteristics. In the spatial domain, SSSA introduces a novel spike distribution-based method to measure relevance between $Q$ and $K$ pairs effectively. Temporally, SSSA introduces a saccadic interaction module that dynamically focuses on selected visual areas and achieves a comprehensive understanding of the whole scene.

\item To further enhance the computational efficiency of SSSA, we introduce a linear complexity version called SSSA-V2. It is mathematically linear scaling mapping to SSSA, preserving all performance benefits. Additionally, SSSA-V2 successfully reduces computational complexity to a linear level and works in a fully event-driven manner.


%This version preserves the mathematical equivalence of SSSA while significantly reducing computational complexity to \( \mathcal{O}(D) \). Furthermore, SSSA-V2 avoids operations in the computation of spatial relevance, ensuring it is fully driven by spikes.
\item Building on the proposed SSSA mechanisms, we develop a SNN-based Vision Transformer (SNN-ViT) architecture. Extensive experiments are conducted on various visual tasks demonstrating that SNN-ViT achieves SOTA performance with linear computational complexity. It presents a promising approach for achieving both high-performance and energy-efficient visual solutions.
\end{itemize}

\section{Related Work}

\textbf{Vision Transformers:} ViTs segment images into patches and apply self-attention~\citep{vaswani2017attention, kenton2019bert} to learn inter-patch relationships, outperforming CNNs across multiple vision tasks~\citep{mei2021image, bertasius2021space, guo2021pct}. Nevertheless, ViTs face challenges like high parameter counts~\citep{liu2021swin}, and increased computational complexity proportional to token length~\citep{pan2020x, liu2022ecoformer}. 
To enhance the computational efficiency of ViTs, many researchers~\citep{jie2023fact, li2023rethinking} are focused on exploring lightweight improvement methods. For example, LeViT~\citep{graham2021levit} incorporates convolutional elements to expedite processing, and MobileViT~\citep{mehta2021mobilevit} combines lightweight MobileNet blocks with MHSA, achieving lightweight ViTs successfully. However, these enhancements still rely on expensive MAC computations which are not suitable for resource-limited devices. This highlights the need for investigating more energy-efficient ViT solutions.

\textbf{Spiking Neural Networks:} The event-driven mechanism enhances the energy efficiency of SNNs, offering a significant advantage for compute-constrained edge devices. With the introduction of ANN-SNN~\citep{cao2015spiking, han2020rmp, wu2021progressive} and direct training~\citep{wu2018spatio, fang2021incorporating, zhang2021rectified, wei2023temporal} algorithm, the difficult associated with training high-performance SNNs is significantly reduced.
Based on these advanced learning algorithms, some research~\citep{hu2021spiking,zheng2021going, hu2024advancing} propose deep residual SNNs~\citep{wang2024ternary, shan2024advancing} and others~\citep{yao2023attention,zhu2024tcja,shan2024advancing} contribute multi-dimensional spike attention mechanisms, achieving competitive performance on many tasks~\citep{zhang2024spike}. 
%Subsequently, some research explores parallel training strategies~\citep{fang2024parallel} and shortcut residual connections~\citep{fang2021deep, hu2024advancing} for SNNs within the ResNet framework.
These improvements further enhance the application of SNNs in various visual tasks. However, despite rapid advancements, a significant performance gap remains between these traditional deep SNN architectures and the latest ViTs.

% ViTs process input images by segmenting them into non-overlapping patches and utilizes Self-Attention to learn inter-patch representations, effectively mitigating inductive bias~\citep{dosovitskiy2020image}. ViTs has demonstrated superior performance across various computer vision tasks compared to Convolutional Neural Networks (CNNs). However, its limitations have become increasingly evident with continued development and application. These include a high parameter count, quadratic growth in computational complexity relative to token length, the presence of non-fusible normalization layers, and a lack of compiler-level optimizations. Recently, LeViT~\citep{graam2021levit} introduced a convolutional approach to accelerate ViT, though executing MHSA still requires reshaping 4D features into flat blocks, demanding substantial computational resources. Similarly, MobileViT~\citep{mehta2021mobilevit} integrates lightweight MobileNet blocks (featuring pointwise and depthwise convolutions) with MHSA blocks. Despite these innovations, these methods still rely on full-precision floating-point computations, which remain computationally expensive on resource-constrained edge devices. Thus, exploring more efficient computational paradigms represents a potential direction for enhancing ViT performance.
\textbf{Vision Transformers Meet Spiking Neural Networks:} To explore high-performance and energy-efficient visual solutions, SNN-based ViTs~\citep{zhouspikformer, wang2023complex} have emerged.
%have recently emerged.
Spikformer~\citep{zhouspikformer,zhou2023spikingformer} pioneers a spike-based self-attention computation, establishing the first spiking ViT. 
However, they still utilize expensive MAC operations and matrix multiplication in self-attention computation, which are inefficient for binary spikes. 
Recently, Spike-driven Transformer~\citep{yao2024spike} implements Hadamard product in the self-attention module for a fully spike-driven ViT. Additionally,  SpikingResformer~\citep{shi2024spikingresformer} integrates a Dual Spike self-attention module for improved performance and energy efficiency. However, these models primarily treat self-attention as a token mixer~\citep{yu2022metaformer}, without exploring an effective relevance computation suited to spike trains. Moreover, they also overlook the temporal dynamics of SNNs.~\citep{zhang2021rectified, bohte2000spikeprop}. Therefore, developing spike self-attention mechanisms tailored to the spatio-temporal characteristics of SNNs is essential for further advancements.

\section{Problem Analysis in Spiking Self-attention}
In this section, we analyze the mismatches between vanilla self-attention mechanisms and SNNs in both the spatial and temporal domains. The detailed discussion is provided in the following sections.

\subsection{Degraded Spatial Relevance}
\label{Degraded}
\begin{figure}[htpb] % 这四个字母可以出现一个或多个：htbp 代表图片插入位置的设置
\centering
\includegraphics[scale=0.43]{ss/comp.jpg}
\caption{Distribution of magnitudes for Q and K in ViTs within ANNs and SNNs on CIRAF100. In ANNs, Q and K exhibit similar magnitude distributions, whereas in SNNs, the magnitude differences between Q and K are pronounced.}
\label{FigQK}
\end{figure}
The vanilla self-attention measures the spatial relevance between \( Q \) and \( K \) through Dot-Product operation. For a given query \( Q_{i} \) and key \( K_{i} \) vector, the relevance between them are as follows:
\begin{equation}
\label{eq:Dot Product}
    \text{Dot-Product}\left( \mathcal{Q}_i, \mathcal{K}_i \right) = \sum_{j=1}^{D} \mathcal{Q}_{ij} \mathcal{K}_{ij},
\end{equation}
\( D \) is the dimension of both vectors, \( Q_{ij} \) and \( K_{ij} \) refer to the \( j \)-th elements of these vectors, respectively. 
%The dot product multiplies corresponding elements from the query and key vectors and sums them up, resulting in a scalar value that indicates the similarity between the two vectors. 
Notably, the relevance based on Dot-Product takes into account both the angle and magnitude of the vectors~\citep{kim2021lipschitz}. When there is a significant difference in magnitude between vectors, the Dot-Product may not accurately measure their spatial relevance.
%When there is a significant difference in magnitude between the vectors, the Dot-Product will fail to accurately reflect their spatial relevance. 

In ANNs, continuous input \(X\) is first normalized using layer normalization~\citep{dosovitskiy2020image} and then be processed through linear transformations \(W_Q\) and \(W_K\) to derive the matrices \(Q\) and \(K\). This ensures that the magnitudes of \(Q\) and \(K\) are closely matched~\citep{xu2019understanding}, preventing large variations between vectors. As shown in the left part of Fig.\ref{FigQK}, the distribution between $Q$ and $K$ across various datasets remains nearly identical, allowing effective measuring of the spatial relevance for attention score in ANNs.

%However, due to the spike-driven characteristic of SNNs, layer normalization cannot normalize spike trains~\citep{zhouspikformer}. As a result, most spike self-attention methods disregard this aspect, making it difficult to maintain similar magnitudes for \(Q\) and \(K\). 
Due to the discrete activation characteristics of spiking neurons, the continuous distribution of the normalized membrane potentials in \(Q\) and \(K\) is disrupted.
As shown in the right part of Fig.~\ref{FigQK}, the magnitude of \(Q\) and \(K\) in SNNs shows significant variability, which leads to the failure of the Dot-Product in measuring spatial relevance. 
Moreover, despite \( Q \) and \( K \) following identical distributions, the sparsity of binary spikes significantly reduces their stability compared to ANNs. We provide a detailed analysis of this assertion in Appendix.~\ref{A}.
Therefore, developing more effective methods to measure the spatial relevance between spike trains could be a viable approach to enhancing the performance of SNN-based ViTs.

%In ANNs, since data is processed in continuous forms, the input \(X\) is first normalized by
%$LN$ and then undergoes a simple linear transformation to produce \(Q\) and \(K\). To some extent, \(Q\) and \(K\) maintain good numerical distribution properties, making cases of large magnitude differences between the vectors rare. As a result, the similarity score derived from balancing the vector magnitudes and directions remains effective in the computation of attention. 

%However, in SNNs, \(Q\) and \(K\) inherit spike-driven characteristics and are computed in discrete binary spike forms, where their representational capacity is greatly reduced to two extreme values: 0 and 1. This leads to significant magnitude discrepancies between the spike sequences, which compromises the similarity computation in the attention mechanism of SNNs. 

%Because disparities in vector sizes can result in exaggerated dot products, thus distorting evaluations of similarity or relevance. The details are described at APPENDIX.A. To address this issue, ANNs employ scaled dot-product and layer normalization techniques to ensure the comparability of magnitudes between Q and K, thus enhancing the accuracy and stability of attention scores.

%To further validate our hypothesis, we conducted extensive ablation studies by replacing the self-attention modules in SNN transformers with convolutional and residual structures, which had a negligible impact on network performance. This underscores the challenges of accurately and efficiently computing token relevance in SNNs, marking it as a critical area for future research.
%Due to the pulse-driven mechanism inherent in Spiking Neural Networks (SNNs), the application of LN and scaling introduces additional floating-point multiplications. Consequently, many spike-based self-attention models omit this step. This omission makes it challenging to guarantee the norm of Q and K in SNNs, which significantly compromises the effectiveness of the score matrices. To further substantiate our hypothesis, we conducted extensive ablation studies by replacing the self-attention module in SNN-transformers with convolutional and residual structures, which showed negligible impact on network performance. This underscores the challenge of accurately and efficiently computing token correlations in SNNs, marking it as a significant area for future research.

\begin{figure}[htpb] % 这四个字母可以出现一个或多个：htbp 代表图片插入位置的设置
\centering
\includegraphics[scale=0.42]{ss/3SSA.jpg}
\caption{Comparison of three self-attention computation paradigms. (a) VSA employs floating-point matrix multiplication to assess the spatial correlation between Q and K, resulting in a computational complexity of \(\mathcal{O}(N^2D)\). (b) SSA lacks a dedicated temporal interaction module, maintaining the same complexity as VSA. (c) In contrast, STSA introduces global spatial-temporal interactions, increasing the complexity to \(\mathcal{O}(T^2N^2D)\).}
\label{3SSA}
\end{figure}

\subsection{Limited Temporal Interaction}

As shown in Fig.~\ref{3SSA}(a), vanilla self-attention in ViTs operates independently of timesteps, thereby preventing the need for temporal interaction in self-attention designs. Conversely, SNNs rely on multiple timesteps to enrich their information representation capabilities~\citep{fang2021incorporating}. However, as shown in Fig.~\ref{3SSA}(b), most spike self-attention mechanisms~\citep{yao2024spike, zhouspikformer,shi2024spikingresformer} lack dedicated modules for the temporal domain. The only temporal interaction in those methods is the accumulation of historical information by spiking neurons (LIF neurons), whose dynamics can be described as:
\begin{align}
U[ t+1 ] & = H[ t ]+ X[t+1],\\ 
S[ t+1 ] &= \Theta (U[ t+1 ] - V_{th}),\\ 
H[ t+1 ] &= V_{reset} S[ t+1 ] + \tau U[t+1] (1-S[ t+1 ]).
\label{1}
\end{align}
\(X[t+1]\) denotes the spatial input current, while \(H[t]\) and \(U[t]\) represent the pre-synaptic and post-synaptic membrane potentials, respectively. The Heaviside function $\Theta(\cdot)$ is employed for spike generation. If a spike occurs (\(S[t+1] = 1\)), \(H[t]\) resets to \(V_{reset}\); otherwise, \(U[t+1]\) decays with a time constant \(\tau\) and feeds into \(H[t+1]\).
However, due to the reset and decay mechanism, the residual membrane potential cannot sustain long-range dependencies, resulting in a significant loss of historical information. To solve this problem, \citep{wang2023spatial} proposes a spatio-temporal spike self-attention method as shown in Fig.~\ref{3SSA}(c). But this method has \(\mathcal{O}(T^2N^2D)\) computational complexity, significantly restricting the training efficiency of SNNs and increasing the complexity of deployment. Therefore, achieving more effective spatio-temporal interactions without increasing computational overhead remains a pressing challenge.

\section{Saccadic Spiking Self-Attention mechanism}
We introduce a Saccadic Spiking Self-Attention (SSSA) method tailored for the spatio-temporal characteristic of SNNs. Spatially, SSSA enhances relevance measurement between spike vectors \(Q\) and \(K\) based on their distribution forms. Temporally, it incorporates a dedicated saccadic interaction module for dynamic contextual comprehension of the visual scene. Additionally, we advance SSSA to version V2, which retains the high performance of SSSA and reduces computational complexity from \(\mathcal{O}(N^2)\) to \(\mathcal{O}(D)\). 
\begin{figure}[htpb] % 这四个字母可以出现一个或多个：htbp 代表图片插入位置的设置
\centering
\includegraphics[scale=0.42]{ss/SSSA_New.png}
\caption{Overview of SSSA method. (a) SSSA consisting of two key components: cross-entropy relevance computation and saccadic spiking neurons. The latter outputs spike-driven decisions that mask \( V \) in N-dimensional space. (b) training and inference process for saccadic spiking neurons. (c) the structure of the spatial relevance computation based on spike distribution. (d) the structure of SSSA-V2 on spatial relevance computation, significantly reducing computational complexity.}
\label{SSSA_New}
\end{figure}
%To effectively address the inherent issues of ineffective attention scores and limited temporal interactions in current SSA methods, we drew inspiration from biological saccadic attention processes. Saccadic attention does not require explicit computation of attention scores but is ensured through multiple saccadic over key visual segments. Building on this, we introduce an innovative Saccadic Spike Self-Attention (SSSA) method, as depicted in Fig. XXX.  In space domain, SSSA can efficiently compute the importance and relevance of various visual areas, addressing the issue of ineffective attention scoring inherent in SSA. In the temporal domain, SSSA integrates spatial and temporal information more comprehensively, effectively leveraging the temporal characteristics of spiking neural networks.
%To effectively address the issues of ineffective attention scores and limited temporal interactions inherent in current spike self-attention mechanisms, we have drawn profound inspiration from the biological saccadic cognitive process. Saccadic cognition involves rapid, sequential eye movements that shift attention across different parts of the visual field, particularly targeting key or interesting areas. Simplistically, attention during this process is ensured through multiple scans of crucial visual segments. Building on this, we propose an innovative Saccadic Spike Self-Attention (SSSA) method, as illustrated in Figure XXX. This method redefines spike self-attention as the frequency of focus on specific tokens over time. Unlike other SSA approaches, SSSA not only computes spike spatial correlations more efficiently, addressing the issue of ineffective attention scores, but also integrates both spatial and temporal information more comprehensively, effectively utilizing the temporal characteristics of spiking neural networks.
\subsection{Spatial Relevance Computation from Spike Distribution}
% To address the issue of degraded spatial relevance caused by the Dot-Product operations, we propose a novel distribution-based method that uses cross-entropy to measure the relevance between two vectors directly, unaffected by the magnitude. The details are discussed in Appendix.~\ref{B}.

% For patch \( \mathbf{x} \in \mathbb{R}^{ D} \) in the \(Q\) or \(K\), it can be considered as a \( D \)-dimensional $\{0,1\}$ random spike train, with \( p \) representing the spike firing rate. For patches \( \text{q} \in Q \) and \( \text{k} \in K \), their cross-entropy can be expressed as follows:
% \begin{equation}
%     \mathcal{H} \left(\text{q}, \text{k}\right) = -\left[ p_{\text{q}} \log p_{\text{k}} + \left(1 - p_{\text{q}} \right) \log \left(1 - p_{\text{k}}\right)\right], 
%     \label{5}
% \end{equation}

% where \( p_\text{q} \) and \( p_\text{k} \) respectively represent the firing rate of spikes for vectors \text{q} and \text{k}. Since \( \text{q} \) and \( \text{k} \) are spike trains, our subsequent analysis focuses more on the distribution of spikes rather than silent states. Therefore, we primarily focus on the first half of Eq.\ref{5}, so $ \mathcal{H} \left(q, k\right)$ can be simplified as $ - p_{\text{q}} \log p_{\text{k}}$. Then,
% given that both \( \log (x) \) and \( x \) display identical monotonicity, substituting \( \log (x) \) with \( x \) is feasible. It does not compromise the effectiveness of \(\mathcal{H} (\text{a}, \text{b}) \) and avoids nonlinear computations. The details of the analysis are provided in Appendix.\ref{B}. Due to the cross-entropy \(\mathcal{H}(\text{q}, \text{k}) \)  measures negative relevance, we take its negative as our attention result. Consequently, \( \mathrm{CroAtt}\left(Q, K\right)= - \mathcal{H}(Q, K) \) can be further expressed as:

% \begin{equation}
% \begin{aligned}
%         \mathcal{Q}' = \sum^{D} Q,\textbf{               } \mathcal{K}' = \sum^{D} K,\textbf{               } \mathrm{CroAtt}\left(Q, K\right) =  \mathcal{Q}' \mathcal{K}'^T, \textbf{               } Q, K \in \mathbb{R}^{T \times N \times D}.
% \end{aligned}
% \end{equation}
% As shown in Fig.\ref{3SSA}.(c), \( \mathcal{Q}'\) and \( \mathcal{K}' \) represent summation of spikes over the dimension \( D \).Through this approximation, we can compute the spatial relevance between $Q$ and $K$ more efficiently in parallel. 
% This distribution method can more accurately assess the relevance between vectors with non-standard distributions, effectively addressing the issue of degraded spatial relevance.

To mitigate the issue of degraded spatial relevance caused by Dot-Product operations, we introduce a novel distribution-based approach. It directly measures the relevance between two vectors using cross-entropy, unaffected by their magnitudes. Further details can be found in Appendix~\ref{B}.

For a patch \( \mathbf{x} \in \mathbb{R}^{ D} \) in either \(Q\) or \(K\), it can be treated as a \( D \)-dimensional $\{0,1\}$ random spike train, where \( p \) represents the spike firing rate. The cross-entropy between patches \( \text{q} \in Q \) and \( \text{k} \in K \) is given by:
\begin{equation}
    \mathcal{H} \left(\text{q}, \text{k}\right) = -\left[ p_{\text{q}} \log p_{\text{k}} + \left(1 - p_{\text{q}} \right) \log \left(1 - p_{\text{k}}\right)\right], 
    \label{5}
\end{equation}

where \( p_\text{q} \) and \( p_\text{k} \) denote the firing rates for vectors \text{q} and \text{k}, respectively. Since both \( \text{q} \) and \( \text{k} \) are spike trains, our focus shifts to the distribution of spikes rather than silent states. Consequently, we primarily consider the first term of Eq.~\ref{5}, allowing us to simplify \( \mathcal{H} \left(q, k\right) \) to \( - p_{\text{q}} \log p_{\text{k}} \). Given that both \( \log(x) \) and \( x \) maintain the same monotonicity, substituting \( \log(x) \) with \( x \) is a feasible simplification that preserves the effectiveness of \(\mathcal{H}(\text{q}, \text{k}) \), while avoiding nonlinear computations. Detailed analysis is provided in Appendix~\ref{B}. 

Since cross-entropy \(\mathcal{H}(\text{q}, \text{k}) \) measures negative relevance, we take its negative as our attention result. As a result, the cross-attention between \( Q \) and \( K \), denoted as \( \mathrm{CroAtt}\left(Q, K\right) = - \mathcal{H}(Q, K) \), can be further expressed as:

\begin{equation}
    \mathrm{CroAtt}\left(Q, K\right) = \mathcal{Q}' \mathcal{K}'^T, \textbf{               } \mathcal{Q}' = \sum^{D} Q, \textbf{               } 
    \mathcal{K}' = \sum^{D} K, \textbf{               } 
    Q, K \in \mathbb{R}^{T \times N \times D}.
\end{equation}

As illustrated in Fig.~\ref{3SSA}(c), \( \mathcal{Q}'\) and \( \mathcal{K}' \) represent the sum of spikes across the dimension \( D \). This approximation allows for more efficient parallel computation of spatial relevance between \(Q\) and \(K\). By employing this distribution-based method, we more accurately assess the relevance between vectors with non-standard distributions, thereby addressing the issue of degraded spatial relevance.

%Local characteristics are represented by the sum of similarities across feature dimensions, which aligns with the computational paradigms in ANNs. Global characteristics, on the other hand, are represented by the correlation between spike activation rates. Unfortunately, the computation of global features requires additional floating-point operations, which contradicts the original intention of designing a low-power attention module. Consequently, we have implemented the following approximation:

%Although Atten effectively extracts spatial information, it fails to encompass temporal features. Directly incorporating the temporal dimension into SSA would significantly increase computational demands. Therefore, our objective is to explore an efficient spatiotemporal attention mechanism. Inspired by the biological process of saccadic eye movement, we aim for Atten to focus on selecting key patches and ignoring others, rather than globally computing attention scores. Attention is assessed through the frequency of temporal focus, enhancing the spatiotemporal characteristics of the mechanism. Thus, Atten's functionality can be summarized as:

\subsection{Saccadic Temporal Interaction for Attention}

Biological saccadic mechanisms do not process all visual information at once. Instead, they progressively focus on key visual areas within a scene~\cite{guadron2022speed}. 
This ensures that biological systems can efficiently achieve contextual understanding of the entire visual scene. Inspired by this mechanism, we have designed an effective temporal interaction module that incorporates two critical processes: salient patch selection and saccadic context comprehension. The first process selectively computes only a subset of patches at each timestep, while ignoring the others. It can significantly reduce the computational complexity of the SSSA method. This process can be described as:
\begin{equation}
\mathcal{P}atch = \sum_{j=1}^{n} \mathrm{CroAtt}\left(Q, K\right), \textbf{               }  \mathrm{CroAtt}\left(Q, K\right) \in \mathbb{R}^{T \times N \times N},
\end{equation}
\( \text{CroAtt}(Q, K) \) represents the spatial relevance between patches in \( Q \) and \( K \). By summing the rows of the \( \text{CroAtt}(Q, K) \) matrix, the $\mathcal{P}atch$ represents 
the spatial salience of patches.
Subsequently, the saccadic interaction module makes contextual understanding based on $\mathcal{P}atch$. To ensure the asynchronous characteristics of SNNs, we aim to integrate the interaction process into spiking neurons. However, the significant historical forgetting caused by the resetting and decay mechanism of LIF neurons prevents efficient interaction. Therefore, we introduce a plug-and-play saccadic spiking neuron, whose dynamic during training and inference phases can be described as follows:
\begin{equation}  
\text{Training}
\begin{cases}
    \mathbf{H} = \mathbf{M}_w \mathbf{\mathcal{P}atch} \\
    \mathbf{S} = \Theta \left(\mathbf{H} - \mathbf{V}_{th} \right)
\end{cases}  \quad 
\text{Inference}
\begin{cases}
    {H}[t] = \mathcal{P}atch [t] \\
    {S}[t] = \Theta \left({H}[t] -  \mathbf{M}_w^{-1}{V}_{th}[t]\right)
\end{cases} 
\end{equation}                                  
Here, \( \mathbf{H}, \mathbf{S}, \mathbf{\mathcal{P}atch}  \in \mathbb{R}^{T \times N} \) represents the data format for parallel training, encompassing the entire temporal dimension. \(\mathbf{M}_{w}\) is a learnable lower triangular matrix that precisely regulates contributions from each timestep, facilitating efficient temporal interactions.
%Unlike traditional LIF neurons, saccadic neurons recalculate the inputs from all previous timesteps at each moment and assign learnable weights to each timestep. 
Utilizing \( \mathbf{M}_{w} \) to compute membrane potentials, saccadic spiking neurons avoid decay or resetting disruptions.
%Saccadic neurons ensure that contributions from each time step are precisely modulated and accumulated through matrix $\mathbf{M}$ without suffering from potential decay or reset processes.
As shown in Fig.\ref{SSSA_New}, we depict the dynamic process of saccadic spiking neurons. During training, the membrane potential of saccadic spiking neurons is represented as \(\sum_{0}^{t} w_{it} \mathcal{P}atch[t], w_{it} \in \mathbf{M}_{w}\). 
However, all timesteps are processed simultaneously via matrix multiplication, which requires substantial computational resources. To maintain SNNs' energy efficiency, we propose an asynchronous inference decoupling method.
By incorporating the inverse of $\mathbf{M}_{w}$ into the threshold levels of the saccadic spiking neurons, we ensure temporal decoupling between $\mathbf{H}$ and $\mathbf{S}$.
The spike firing process depends solely on the current values of \({H}[t]\) and \({V}_{th} M_w^{-1}[t]\), eliminating the need for historical information. Thus, saccadic spiking neurons ensure the capability for asynchronous inference.
Notably, the temporal complexity of saccadic spiking neurons is only $\mathcal{O}\left(T\right)$, significantly superior to the $\mathcal{O}\left(T^2\right)$. The dynamics of saccadic spiking neurons are detailed in Appendix.\ref{C}.
\subsection{linear Complexity and Spike-driven Computation}
Building on the aforementioned components, SSSA is specifically designed for the spatio-temporal characteristics of SNNs. It enables a more effective comprehensive understanding of the entire visual scene with lower time complexity. Its formulation is described as follows: 
\begin{equation}
\text{SSSA}\left( \mathcal{Q}, \mathcal{K}, \mathcal{V} \right) = \Theta \left( \mathbf{M}_{w} \mathcal{P}atch [0,\cdots,t] - \mathcal{V}_{th}\right) \cdot \mathcal{V}  = \Theta \left( \mathbf{M}_{w} \left( \mathcal{Q}' \times \mathcal{K}'^T \right)  L - \mathbf{V}_{th}\right)\cdot \mathcal{V},
\label{10}
\end{equation}
where \( L \) represents a column vector \([1, 1, \ldots, 1]\) with dimension \( N \), facilitating the summation of row elements. However, as depicted in Fig.~\ref{3SSA}(c), SSSA includes integer multiplication operations within $\mathcal{Q}'\times \mathcal{K}'$, compromising the energy efficiency of the SNNs. Moreover, the quadratic complexity of \(\mathcal{Q}'\times \mathcal{K}'\) still indicates potential for optimization. Given that the matrix multiplications in Eq.\ref{10} do not involve nonlinear operations, they allow for free association of matrices without altering the computational sequence. Consequently, to avoid the need for integer multiplication and further reduce computational complexity, we conduct an linear scaling mapping of Eq.\ref{10}, which can be described as follows:
\begin{equation}
\text{SSSA}\left( \mathcal{Q}, \mathcal{K}, \mathcal{V} \right) =  \Theta \left(\left( \mathbf{M}_{w} \times \mathcal{Q}'\right) \left(\mathcal{K}'^T \times L\right) - V_{th}\right) \cdot \mathcal{V} 
\end{equation}
In SSSA-V2, computations begin with the calculation of \(Q'\) and \(K'\) based on \(Q\) and \(K\), each with a complexity of \(\mathcal{O}(D)\). Then, instead of calculating \(Q'\times K'\), SSSA-V2 treats $(\mathcal{K}'^T \times L)$ as a learnable scaling factor $\alpha$, applied to the threshold $\textbf{V}_{th}$ of the saccadic neuron.
Subsequently, \(\mathbf{M}_{w}\times Q'\) as $\mathcal{P}atch[i]$ input into the saccadic neurons. During the inference process, \(\mathbf{M}_{w}\) can be integrated into the thresholds of saccadic neurons to maintain a fully spike-driven system.
\begin{equation}
   \text{Inference} \left\{
\begin{array}{ll}
H[t] = \mathcal{Q'}[t], \\ 
S[t] = \Theta \left(\textbf{H}[t]- \frac{1}{\alpha} \left(\textbf{M}_{w}^{-1} \textbf{V}_{th} \right)[t]\right).
\end{array}
\right.
\end{equation}
Mathematically, SSSA-V2 is linear scaling mapping to SSSA, preserving all the advantages of SSSA while significantly reducing the need for integer multiplication operations. Additionally, SSSA-V2 achieves a linear spike self-attention mechanism with total computational complexity of \(\mathcal{O}(2D+N)\), offering significant benefits in resource-constrained environments. 

%Since $Saccadic\left(Q_S,K_S \right) \in S^{ T\times N}$  is strictly spike-driven, V can perform matrix multiplication with Sc through membrane potential, effectively masking certain patches—only a subset of patches participates in computation at each time step, while others remain silent. SSSA efficiently simulates the attention focusing process of biological saccades, adhering strictly to the spike-driven paradigm and significantly reducing computational load.


\section{SNN-based Saccadic Vision Transformer}
As illustrated in Fig.\ref{Fig444}, we introduce a novel SNN-ViT based on the proposed SSSA method. It primarily consists of GL-SPS blocks and SSSA-based transformer blocks. The following section will provide detailed descriptions of these components.

\begin{figure}[htpb] % 这四个字母可以出现一个或多个：htbp 代表图片插入位置的设置
\centering
\includegraphics[scale=0.42]{ss/summary-new.png}
\caption{The overall structure of SNN-ViT, mainly consisting of GL-SPS blocks and SSSA-based transformer blocks. GL-SPS block combines dilated convolution and standard convolution at different scales to facilitate multi-scale feature extraction from images. The SSSA-based block, composed of SSSA methods and Linear layers, achieves lower computational complexity.}
\label{Fig444}
\end{figure}
\subsection{GL-SPS: Global-Local Spiking Patch Splitting module}
%Models such as Spike-driven leverage the Metaformer convolutional architecture as their framework, which does not require the Spike Patch Splitting (SPS) process. Despite demonstrating robust performance, this approach engages in self-attention at the pixel level, resulting in high computational demands that do not align with the saccadic processes discussed in our SSSA framework. Conversely, models like Spikformer, which employ the 
Currently, existing SPS methods primarily rely on shallow spiking convolution modules to capture local information, which prevents the effective extraction of multi-scale features. This limitation leads to degraded performance in processing wide-field image features. To address this issue, we design the Global-Local convolutional SPS block, described as follows:
\begin{equation}
\text{GL-SPS} \left( X[t] \right) = \text{BN} \left(\text{Conv} ( X[t]  \right)  +  \text{BN} \left( \text{DConv} \left( X[t] \right) \right),
\label{12}
\end{equation}

where \(X[t]\) is the result of a convolution operation with a stride of 2.
$ \text{Conv2d} $ and $ \text{D-Conv2d} $ represents standard 
 and dilated convolution operations. $\mathrm{BN(\cdot)}$ is Batch Normalization. 
%and $ H$ is the firing function of spiking neurons as Eq.\ref{10}. 
The GL-SPS utilizes both $ \text{Conv2d} $ and $ \text{D-Conv2d} $ to extract features. Combining layers with different dilation rates effectively gathers context from various visual scales. Consequently, SNN-ViT employs the GL-SPS method as its embedding module, enhancing efficiency and scalability in feature extraction.


\subsection{Overall Architecture}
Building upon the pyramid structure~\citep{liu2021swin, yu2023metaformer}, we propose a novel SNN-ViT that incorporates the GL-SPS block and the SSSA method.
GL-SPS part encodes the input image through downsampling operation and various convolutions operation. Specifically, the downsampling operation is defined as a convolution operation with a kernel size of 7 and a stride of 2. The GL-SPS method follows the previous section. The whole block is defined as follows:
\begin{align} 
U_0= \text{GL-SPS} \left(I\right) \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad 
&I \in \mathbb{R}^{T\times C\times H\times W}
\end{align}
% For the GL-SPS block, it encompasses both the GL-SPS and Downsample process. As mentioned in the previous section, GL-SPS transforms the image squeeze \( I \) into a D dimensional feature spike-form vector and splits it into flattened patches \( X \). 
% Detail of GL-SPS block is defined as:
%Meanwhile, we follow the design in~\citep{yao2024spike}, which utilizes a Conv Layer to generate Relative Position Embeddings (RPE). Detail of GL-SPS block is defined as:
%&\mathrm{RPE}  = \text{BN}\left(\text{Conv2d} \left( \mathcal{SN}\left( U_0 \right) \right)\right), & \mathrm{RPE} \in \mathbb{R}^{T \times N \times D} \\
%&U_1 = U_0 + \mathrm{RPE}, & U_0 \in \mathbb{R}^{T \times N \times D
Subsequently, \(U_0\) is inputted into the SSSA-based block, which consists of SSSA method and MLP Layer. To further reduce the computational complexity of the model, we adopt the SSSA-V2 version as the paradigm for self-attention computation in the architecture. Subsequently, the output from the SSSA-based Transformer blocks is fed into the Global Average Pooling (GAP) module, followed by a Classification Head (FCH) that generates the prediction Y. These parts can be defined as:
\begin{align} 
& U_1 = U_0 + \text{BN}(\text{Conv}( [\text{SSSA}(\mathcal{SN}(U_0))])), & U_0 \in \mathbb{R}^{T \times N \times D}\\
& U_2 = U_1 + \text{BN}(\text{Linear}[\mathcal{SN} (U_1)] ), & U_1 \in \mathbb{R} ^ {T \times N \times D}\\
& Y = \text{FCH} (\text{GAP} (\mathcal{SN}(U_2))), & 
\end{align}
where \( Y \) denotes the predicted outcome. For different types of datasets, we can integrate the GL-SPS component with varying numbers of SSSA decoding blocks. The details of the network architecture and the parameter count are presented in Appendix.\ref{D}.

\section{Experiments}
\subsection{Image classification}
SNN-ViT is evaluated on both static and neuromorphic datasets, including CIFAR10, CIFAR100~\citep{krizhevsky2009learning}, ImageNet~\citep{deng2009imagenet} and
CIFAR10-DVS~\citep{li2017cifar10}. Specifically, for ImageNet, the input image size is $3 \times 224 \times 224$, with batch sizes of 128, and training epoch is conducted over 310. 
% and the image is divided into 196 patches via the SPS module
\begin{table}[h!]
\centering
\caption{Summary of Network Architectures and Parameters across Vary Datasets}
\begin{tabular}{c cc cc cc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{2}{c}{CIFAR10}                                                                                & \multicolumn{2}{c}{CIFAR100}                                                                               & \multicolumn{2}{c}{CIFAR10-DVS}                                                                                                                                                        \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} 
                        & Param. & Acc. &  Param. & Acc. & Param. & Acc.
 \\ \midrule
PLIF~\citep{fang2021incorporating} & - & 93.50 & - & - & - & 74.8 \\
tdBN~\citep{zheng2021going} & - & 93.2 & - & - & - & 67.8 \\
DSpike~\citep{li2021differentiable} & - & 94.25 & - & 74.24 & - & 75.4 \\
TET~\citep{deng2022temporal}  & 12.63 & 94.44 & 12.63 & 74.47 & - & 77.33 \\\midrule
Spikformer~\citep{zhouspikformer} & 9.32 & 95.51 & 9.32 & 78.21 & 2.57 & 80.9 \\
Spikingformer~\citep{zhou2023spikingformer} & 9.32 & 95.81 & 9.32 & 78.21 & 2.57 & 81.3 \\
Spike-driven~\citep{yao2024spike} & 10.28 & 95.60 & 10.28 & 78.40 & 2.57 & 80.0 \\
STSA~\citep{wang2023spatial} & - & - & - & - & 1.99 & 79.9 \\ \midrule
\textbf{SNN-ViT (Ours)} & \textbf{5.57} & \textbf{96.1} & \textbf{5.57} & \textbf{80.1} & \textbf{1.52} & \textbf{82.3} \\
\bottomrule
\end{tabular}
\label{table.2}
\end{table}
Our experimental results are summarized in Table.\ref{table.2} and \ref{table3}. To facilitate a comprehensive comparison with similar works, we meticulously documented the performance of our SNN-ViT across varying model sizes. In the CIFAR100 dataset, the direct training performance of SNN-ViT significantly surpasses the Spike-driven Transformer~\citep{yao2024spike} with fewer parameters. This underscores the high robustness and sensitivity of the SSSA strategy in smaller-scale tasks, closely mirroring biological cognitive processes.
Notably, for neuromorphic datasets, the computational complexity of SNN-ViT is \(\mathcal{O}(TD)\), while STSA~\citep{wang2023spatial} has a complexity of \(\mathcal{O}(T^2N^2D)\). Despite reducing computational complexity by three orders, SNN-ViT maintains SOTA performance. 
Furthermore, on the ImageNet-1K dataset, we conduct a detailed comparison of SNN-ViT with other similar works. The comparison focuses on computational complexity, parameter count, energy consumption, and accuracy. Experimental results demonstrate that SNN-ViT achieves SOTA performance under linear computational complexity. 
% Moreover, on the ImageNet 1K dataset, we compared the performance across typical model sizes (Tiny, Small, Medium, Large), revealing a distinct advantage of SNN-ViT in the Tiny and Small configurations, and surpassing other similar efforts even at the largest scale. Notably, for neuromorphic datasets, SNN-ViT maintains a computational complexity of O(d) with SSSA, compared to O(T²N²D) for STSA. Despite reducing the computational complexity by three orders of magnitude, SNN-ViT still maintains industry-leading performance, setting a benchmark in the field.
% \begin{table}[htpb]
% \centering
% \caption{Detailed comparison with other similar methods on ImageNet-1K}
% \begin{tabular}{ccccccc}
% \toprule
% Method                            & Architecture       & \begin{tabular}[c]{@{}c@{}}Complexity \end{tabular} & \begin{tabular}[c]{@{}c@{}}Time\\Step\end{tabular} & \begin{tabular}[c]{@{}c@{}}Param.\\ (M)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Energy\\ (mJ)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Acc.\\ (\%)\end{tabular} \\ \toprule
% \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Sew ResNet\\ \citep{fang2021deep}\end{tabular}}       & Sew ResNet-50      &  -     & 4    & 25.6                                               & 4.89                                                  & 67.78                                               \\
%                                   & Sew ResNet-101     &   -   & 4    & 44.6                                               & 8.91                                                  & 68.76                                               \\
% \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}MS ResNet\\ \citep{hu2024advancing}\end{tabular}}                        & MS-ResNet-50     &  -     & 4    & 21.8                                               & 5.10                                                  & 69.40                                               \\ 
%         & MS-ResNet-104     &  -     & 4    & 77.3                                               & 10.2                                                  & 75.30                                               \\
%         \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Att-MS-ResNet\\ \citep{yao2023attention}\end{tabular}}                        & Att-ResNet-50     &  -     & 1    & 22.1                                               & 0.60                                                  & 69.20                                               \\ 
%         & Att-ResNet-104     &  -     & 4    & 78.4                                               & 7.30                                                  & 76.30                                               \\\midrule
% \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Spikformer\\ \citep{zhouspikformer}\end{tabular}}      & Spikformer-8-384   &  $\mathcal{O}\left(N^2 D\right)$    & 4    & 16.8                                              & 7.73                                                  & 70.24                                               \\
%                                   & Spikformer-8-512   &  $\mathcal{O}\left(N^2 D\right)$      & 4    & 29.7                                               & 11.6                                                 & 73.38                                               \\
%                                   & Spikformer-8-768   &   $\mathcal{O}\left(N^2 D\right)$  & 4    & 66.3                                               & 21.5                                                 & 74.81                                               \\ \midrule
% \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}SpikingResformer\\ \citep{shi2024spikingresformer}\end{tabular}}  & SpikingResformer-S &    $\mathcal{O}\left(N^2 D\right)$   & 4    & 17.8                                               & 3.37                                                  & 75.95                                               \\
%                                   & SpikingResformer-M &  $\mathcal{O}\left(N^2 D\right)$     & 4    & 35.5                                               & 5.46                                                  & 77.24                                               \\
%                                   & SpikingResformer-L &   $\mathcal{O}\left(N^2 D\right)$    & 4    & 60.4                                               & 8.76                                                  & 78.77 \\ \midrule
% \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Spike-driven\\ \citep{yao2024spike}\end{tabular}}             & Spike-driven-8-384         &                  $\mathcal{O}\left(ND\right)$   & 4    & 16.8                                               & 3.90                                                  & 72.28                                               \\
%                                   & Spike-driven-8-512         &  $\mathcal{O}\left(ND\right)$     & 4    & 29.7                                               & 4.50                                                  & 74.57                                               \\ 
% \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Meta-SpikeFormer\\ \citep{yao2024spikev2}\end{tabular}}              & Meta-SpikeFormer-384         &  $\mathcal{O}\left(N D^2\right)$     & 4    & 33.1                                               & 32.8                                                  & 74.10                                              \\
%                                   & Meta-SpikeFormer-512         &  $\mathcal{O}\left(N D^2\right)$     & 4    & 55.4                                               & 52.4                                                  & 79.70                                               \\ \midrule
%  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{SNN-ViT(Ours)}\end{tabular}}                                    & SNN-ViT-8-256 &  $\mathcal{O}\left(D\right)$     & 4    & 13.7                                             & 14.28                                                  & 74.66   \\
%  & SNN-ViT-8-384 & $\mathcal{O}\left(D\right)$ & 4 & 30.4 & 20.83 & 76.87  \\
%                                   & SNN-ViT-8-512 &  $\mathcal{O}\left(D\right)$     & 4    & 53.7                                              & 35.75                                                  & \textbf{80.23}   \\
% \bottomrule 
% \end{tabular}
% \label{table3}
% \end{table}

\begin{table}[htpb]
\centering
\caption{Detailed comparison with other similar methods on ImageNet-1K}
\begin{tabular}{ccccccc}
\toprule
Method                            & Architecture       & \begin{tabular}[c]{@{}c@{}}Complexity \end{tabular} & \begin{tabular}[c]{@{}c@{}}Time\\Step\end{tabular} & \begin{tabular}[c]{@{}c@{}}Param.\\ (M)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Energy\\ (mJ)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Acc.\\ (\%)\end{tabular} \\ \toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}ViT\\ \citep{dosovitskiy2020image}\end{tabular}}       & ViT-12-768      &  $\mathcal{O}(N^2D)$     & -    & 86M                                               & 80.86                                                  & 77.90                                               \\
                                  & ViT-24-1024     &   $\mathcal{O}(N^2D)$  & -    & 307M                                               & 283.36                                                  & 76.51                                               \\
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Swin Transformer\\ \citep{liu2021swin}\end{tabular}}                        & Swin-T     &  $\mathcal{O}(ND^2)$     & -    & 29M                                               & 20.72                                                  & 81.35                                               \\ 
        & Swin-S     &  $\mathcal{O}(ND^2)$     & -    &  51M                                              & 40.24                                                  & 83.03                                              \\
        \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}} Flatten Transformer\\ \citep{han2023flatten}\end{tabular}}                        & FLatten-Swin-T     &   $\mathcal{O}(N^2D)$     & -    & 29M                                               & 20.72                                                  & 82.14                                               \\ 
        & FLatten-Swin-S     &  $\mathcal{O}(N^2D)$     & -    & 51M                                               & 40.24                                                  & 83.52                                              \\\midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Spikformer\\ \citep{zhouspikformer}\end{tabular}}      & Spikformer-8-384   &  $\mathcal{O}\left(N^2 D\right)$    & 4    & 16.8M                                              & 7.73                                                  & 70.24                                               \\
                                  & Spikformer-8-512   &  $\mathcal{O}\left(N^2 D\right)$      & 4    & 29.7M                                               & 11.6                                                 & 73.38                                               \\
                                  & Spikformer-8-768   &   $\mathcal{O}\left(N^2 D\right)$  & 4    & 66.3M                                               & 21.5                                                 & 74.81                                               \\ \midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}SpikingResformer\\ \citep{shi2024spikingresformer}\end{tabular}}  & SpikingResformer-S &    $\mathcal{O}\left(N^2 D\right)$   & 4    & 17.8M                                               & 3.37                                                  & 75.95                                               \\
                                  & SpikingResformer-M &  $\mathcal{O}\left(N^2 D\right)$     & 4    & 35.5M                                               & 5.46                                                  & 77.24                                               \\
                                  & SpikingResformer-L &   $\mathcal{O}\left(N^2 D\right)$    & 4    & 60.4M                                               & 8.76                                                  & 78.77 \\ \midrule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Spike-driven\\ \citep{yao2024spike}\end{tabular}}             & Spike-driven-8-384         &                  $\mathcal{O}\left(ND\right)$   & 4    & 16.8M                                               & 3.90                                                  & 72.28                                               \\
                                  & Spike-driven-8-512         &  $\mathcal{O}\left(ND\right)$     & 4    & 29.7M                                               & 4.50                                                  & 74.57                                               \\ 
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Meta-SpikeFormer\\ \citep{yao2024spikev2}\end{tabular}}              & Meta-SpikeFormer-384         &  $\mathcal{O}\left(N D^2\right)$     & 4    & 33.1M                                               & 32.8                                                  & 74.10                                              \\
                                  & Meta-SpikeFormer-512         &  $\mathcal{O}\left(N D^2\right)$     & 4    & 55.4M                                               & 52.4                                                  & 79.70                                               \\ \midrule
 \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{SNN-ViT(Ours)}\end{tabular}}                                    & SNN-ViT-8-256 &  $\mathcal{O}\left(D\right)$     & 4    & 13.7M                                             & 14.28                                                  & 74.66   \\
 & SNN-ViT-8-384 & $\mathcal{O}\left(D\right)$ & 4 & 30.4M & 20.83 & 76.87  \\
                                  & SNN-ViT-8-512 &  $\mathcal{O}\left(D\right)$     & 4    & 53.7M                                              & 35.75                                                  & \textbf{80.23}   \\
\bottomrule 
\end{tabular}
\label{table3}
\end{table}


\subsection{Remote Object Detection}
Given the high adaptability of biological saccadic mechanisms to dynamic visual tasks, we aim to apply SNN-ViT to object detection tasks to demonstrate its advantages. As SNNs are often employed in resource-constrained edge computing scenarios, we select two remote sensing datasets: NWPU VHR-10~\cite{cheng2017remote} and SSDD~\citep{wang2019sar}. The NWPU VHR-10 dataset comprises very high-resolution (VHR) images across ten categories, including airplanes, ships, storage tanks, baseball diamonds, tennis courts, basketball courts, ground track fields, harbors, bridges, and vehicles.
%Given the biological saccadic mechanism's high adaptability to dynamic visual tasks, we plan to deploy the SSSA in activities characterized by temporal dynamics to validate its benefits. Specifically, considering the high energy efficiency of SNNs commonly employed in edge devices such as satellites and unmanned aerial vehicles, we have chosen to conduct performance evaluations using remote sensing ASR target detection tasks. This approach highlights the practical value and efficacy of SSSA in real-world application scenarios.
\begin{figure}[htpb] % 这四个字母可以出现一个或多个：htbp 代表图片插入位置的设置
\centering
\includegraphics[scale=0.48]{ss/object.jpg}
\caption{The detection results of SNN-ViT-YOLO on the NWPU-10 dataset are displayed in the first rows. SSSA attention heatmaps are showcased in the second rows.}
\label{Fig555}
\end{figure}
The SSDD dataset focuses on ship detection using Synthetic Aperture Radar (SAR) images. We build a SNN-ViT-YOLO framework by incorporating the SNN-ViT as the backbone of the YOLO-v3 architecture. The structure details are shown in Appendix.\ref{E}. As shown in Table.\ref{tab3}, to validate the superior performance of SSSA in dynamic visual tasks, we present a comparison with various deep ANN-based object detection methods, including YOLO-v3~\citep{liu2021abnet}, RICNN~\citep{cheng2016learning}, and Faster RCNN~\citep{fu2020anchor}. Additionally, we also compare our approach with EMS-YOLO~\citep{su2023deep}, the current SOTA results in the SNN fields. The results indicate that SNN-ViT-YOLO outperforms other methods on the two datasets. This demonstrates that SNN-ViT offers a viable approach for aviation and satellite image analysis in extreme environments.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[htbp]
\centering
\caption{Performance comparison with ANNs and SNNs on NWPU VHR-10 and SSDD.}
\label{tab3}
\small % Set the font size for the entire table to small
\renewcommand{\arraystretch}{1.2} % Adjust row spacing
\begin{threeparttable}
\begin{tabular}{clccc} 
\toprule % Use top horizontal line
\textbf{Dataset} & \textbf{Method} & \textbf{Spike-driven} & \textbf{Timestep} & \textbf{mAP@0.5($\%$)} \\ 
\midrule % Use middle horizontal line
\multirow{6}{*}{NWPU VHR-10} 
& YOLO-V3~\citep{liu2021abnet}  &  \XSolidBrush & - &  87.3\% \\
& $CS^n$Net~\citep{chen2023cs}  &  \XSolidBrush & - &  90.4\% \\
\cline{2-5}
& \multirow{2}{*}{EMS-YOLO~\citep{su2023deep}}  & \Checkmark   &4  & 86.5\% \\
&   & \Checkmark  &8  & 87.9\% \\ 
\cline{2-5}
& \multirow{2}{*}{\textbf{SNN-ViT-YOLO (Ours)}}  & \Checkmark  &4 & 88.2\% \\
&   & \Checkmark &8  & 89.4\% \\
\midrule % Use middle horizontal line
\multirow{5}{*}{SSDD} 
& Faster R-CNN~\cite{fu2020anchor}  &  \XSolidBrush & - & 85.3\% \\
\cline{2-5}
& \multirow{2}{*}{EMS-YOLO~\cite{su2023deep}}  & \Checkmark  &4  & 94.8\% \\
&   & \Checkmark  &8  & 95.1\% \\ 
\cline{2-5}
& \multirow{2}{*}{\textbf{SNN-ViT-YOLO (Ours)}}  & \Checkmark  &4 & 96.7\% \\
&   & \Checkmark &8  & 97.0\% \\
\bottomrule % Use bottom horizontal line
\end{tabular}
\end{threeparttable}
\end{table*}

\subsection{Ablation Study}

\begin{wraptable}{r}{0.51\linewidth}
\centering
\small
\vspace{-20pt}
\caption{Ablation Study}
\begin{tabular}{cccc}
\toprule
\text{Model} & \begin{tabular}[c]{@{}c@{}}Param.\\ (M)\end{tabular}  &\begin{tabular}[c]{@{}c@{}}Compl\\exity\end{tabular} & \begin{tabular}[c]{@{}c@{}}Acc.\\ (\%)\end{tabular}\\ 
\midrule
$\text{Baseline}$ & 5.76 & $\mathcal{O}(N^2 D)$ & 76.95 \\
\midrule
$+\text{SSSA}$ & 5.52 & $\mathcal{O}(D)$ & 79.60 + (2.65)\\
$+\text{GL-SPS}$ & 5.81 & $\mathcal{O}(N^2 D)$& 77.88 + (0.93)\\
$+\text{both}$ & 5.57 & $\mathcal{O}(D)$ & 80.1 + (3.15)\\
\bottomrule
\end{tabular}
\label{tab:res_ssl}
\vspace{-5pt}
\end{wraptable}

To verify the effectiveness of each component in the SNN-ViT, we perform a comprehensive ablation study in the CIFAR100 dataset. The Spikformer~\citep{zhouspikformer} is selected as the baseline for comparison. Subsequently, we replace the corresponding modules in the baseline with SSSA blocks and GL-SPS blocks to assess their impact on performance. As shown in Table~\ref{tab:res_ssl}, replacing our SSSA method improves performance by approximately 2.65\%, while reducing computational complexity to \(\mathcal{O}(N^2)\). Then we also verify the effectiveness of the GL-SPS blocks. As shown in Table.\ref{tab:res_ssl}, GL-SPS blocks achieve a performance improvement of about 0.93\% compared to baseline. This further demonstrates the enhanced compatibility of multi-scale feature maps with the saccadic process. Finally, we replace both SSSA and GL-SPS, achieving an approximately 3.15\% performance improvement. Ablation studies validate that the SSSA indeed can significantly enhance performance, confirming its compatibility with spatio-temporal spike trains.

\section{Conclusion}
This work provides a detailed analysis of the mismatch between the vanilla ViT and spatio-temporal spike trains. This mismatch results in degraded spatial relevance and limited temporal interactions. Inspired by the biological saccadic attention mechanism, this work proposes a SSSA method tailored to the SNNs. In the spatial dimension, SSSA employs a more efficient distribution-based approach to compute the spatial relevance between Query and Key in SNNs. In the temporal domain, SSSA utilizes a dedicated saccadic interaction module, calculating only a subset of patches at each timestep to dynamically understand the context of the entire visual scene. Building on SSSA method, we develop a SNN-ViT structure, which achieves state-of-the-art performance across various visual tasks with linear computational complexity. SNN-ViT effectively integrates advanced biological cognitive mechanisms with artificial intelligence techniques, providing a promising avenue for exploring high-performance, energy-efficient edge visual tasks. 

\section{ACKNOWLEDGMENT}
This work was supported in part by the National Natural Science Foundation of China under grants U20B2063, 62220106008, and 62106038, the Sichuan Science and Technology Program under Grant 2024NSFTD0034 and 2023YFG0259, the Open Research Fund of the State Key Laboratory of Brain-Machine Intelligence, Zhejiang University (Grant No.BMI2400020).

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\section{ Limitations of Dot-Product for Spike Trains}
\label{A}
The Dot-Product is the operation to measure relevance between two vectors \( \mathbf{u} \) and \( \mathbf{v} \) in an \( n \)-dimensional space, which is defined as:
\begin{equation}
    \mathbf{u} \cdot \mathbf{v} = \sum_{i=1}^n u_i v_i = \| \mathbf{u} \| \| \mathbf{v} \| \cos \theta
\end{equation}
where \( u_i \) and \( v_i \) are the components of vectors \( \mathbf{u} \) and \( \mathbf{v} \) respectively, \( \| \mathbf{u} \| \) and \( \| \mathbf{v} \| \) denote the magnitudes (norms) of the vectors, and \( \theta \) is the angle between them. This expression clearly illustrates that the Dot-Product is influenced by both the magnitudes of the vectors and the cosine of the angle between them. Variations in either magnitude or angle will affect the result of the Dot-Product, thus affecting the measure of relevance between the vectors. 

\textbf{Problem: If the \(Q\) and \(K\) are controlled to be similar distributions in SNNs, would the effectiveness of the Dot-Product still be influenced by magnitude differences?}

\textbf{Analysis:} To deepen our investigation, we present the following mathematical assumptions: assuming query \(Q\) and the key vector \(K\) in SNNs are independent and share the same firing rate. Then we examine the$ \frac{\| \boldsymbol{q} \|}{\| \boldsymbol{k} \|}$. Let $\boldsymbol{x} = (x_1, x_2, \dots, x_D) \in \{0,1\}^D$ represent $\boldsymbol{q}$ or $\boldsymbol{k}$, where each element $\boldsymbol{x}_i$ takes the value 1 with probability $p$. The square of the magnitude follows a binomial distribution:

\begin{equation}
{\Arrowvert\boldsymbol x\Arrowvert}^2=\sum_{i=1}^D\boldsymbol{x}_i^2\;=\;\sum_{i=1}^D\boldsymbol{x}_i\sim\text{B}(D,P),
\end{equation}

Its probability is given by:
\begin{equation}
P({\Arrowvert\boldsymbol x\Arrowvert}^2 = k) = \binom{D}{k} p^k (1 - p)^{D - k}, \quad k = 0, 1, 2, \dots, D.
\end{equation}

We randomly select a $\boldsymbol{q}$ and a $\boldsymbol{k}$ from this distribution and calculate their magnitude ratio
\(R = \frac{\| \boldsymbol{q} \|}{\| \boldsymbol{k} \|}\). Without considering the case when \( \| \boldsymbol{k} \| = 0 \), the calculation proceeds as follows:

\begin{equation}
\begin{aligned}
P(R = r) &= P(R^2 = r^2) \\
&= \sum_{k=0}^{n} \sum_{l=1}^{n} \mathbf{1}\left( \dfrac{k}{l} = r^2 \right) P(X = k) P(Y = l) \\
&= \sum_{k=0}^{n} \sum_{l=1}^{n} \mathbf{1}\left( \dfrac{k}{l} = r^2 \right) \binom{n}{k} \binom{n}{l} \, p^{k + l} (1 - p)^{2n - k - l}
\end{aligned}
\end{equation}
where \( \mathbf{1}\left( \dfrac{k}{l} = r^2 \right) \) is the indicator function, which equals \( 1 \) when \( \dfrac{k}{l} = r^2 \) and \( 0 \) otherwise. 

Given the complexity of this distribution, we employ experimental simulation for approximation. Referencing the data shown in Fig.~\ref{FigQK}, we set \( p = 0.15 \) and \( D = 128 \). As the Dot-Product operation is symmetric, we adjust our calculation to ensure that the numerator is always greater than or equal to the denominator, enhancing the clarity of our visualization. Specifically, we compute \( \frac{\| \boldsymbol{q} \|}{\| \boldsymbol{k} \|} \) when \( \| \boldsymbol{q} \| > \| \boldsymbol{k} \| \), and \( \frac{\| \boldsymbol{k} \|}{\| \boldsymbol{q} \|} \) otherwise. The simulation results are shown in Fig.\ref{Fig.sub.3}. Clearly, the distribution of \( \frac{\| \boldsymbol{q} \|}{\| \boldsymbol{k} \|} \) is notably disordered. 
\begin{figure*}[htbp]
 \centering
 \subfigure[]{
        \label{Fig.sub.2}
 	\includegraphics[scale=0.51]{ss/medge.png}
 }
 \subfigure[]{
        \label{Fig.sub.3}
 	\includegraphics[scale=0.52]{ss/medge1.png}
  }
  \subfigure[]{
        \label{Fig.sub.LS}
 	\includegraphics[scale=0.52]{ss/medge2.png}
  }
 \caption{(a) The impact of varying magnitudes on the results of Dot-Products. (b) and (c) The distribution of magnitude entropy for \( \frac{\| \boldsymbol{q} \|}{\| \boldsymbol{k} \|} \) in ANNs and SNNs.}
 \label{fig:distance}
\end{figure*}
For comparison, we conduct the same assumptions and simulations in the self-attention module of ANN. Let $\boldsymbol{x} = (\boldsymbol{x}_1, \boldsymbol{x}_2, \dots, \boldsymbol{x}_D) \in \mathbb{R}^D$ represent $\boldsymbol{q}$ or $\boldsymbol{k}$. Then its magnitude is given by
\(\| \boldsymbol{x} \| = \sqrt{\sum_{i=1}^{D} \boldsymbol{x}_i^2}\).
Similarly, we randomly select a $\boldsymbol{q}$ and a $\boldsymbol{k}$ and simulate the distribution of their magnitude ratio $R'$. Based on the data in Fig.~\ref{FigQK}, we approximate each element $x_i$ as independently normally distributed with $\boldsymbol{x}_i \sim N(35, 10)$. The results are shown in Fig.\ref{Fig.sub.LS}. By calculating the variance of \( \frac{\| \boldsymbol{q} \|}{\| \boldsymbol{k} \|} \), it is found to be approximately 0.2322 in SNNs and only around $0.00844$ in ANNs. This indicates significant magnitude fluctuations in SNNs, revealing a high degree of instability. As a result, the efficiency and effectiveness of the Dot-Product computation are negatively impacted.

\section{Cross-entropy for Better Relevance Computation}
\label{B}
We calculate the relevance in spatial dimensions separately for the \(Q\) and \(K\) vectors at different moments. The vectors \(\text{q} \in Q\) and \(\text{k} \in K\), and \(p_{q}\) and \(p_{K}\) represents the spike firing rate for them. Here, we introduce a cross-entropy method to more effectively compute the relevance between \(q\) and \(k\) vectors:
% while \( P \) denotes the spike firing rate. %To overcome issues such as inconsistent vector magnitudes and matrix multiplication leading to the ineffectiveness of spiking self-attention, we employ a novel spike distribution-based method to assess the relevance.
\begin{equation}
    \mathcal{H} \left(q, k\right) = -\left[p_{\text{q}} \log p_{\text{k}} + \left(1 - p_{\text{q}} \right) \log \left(1 -p_{\text{k}}\right)\right].
\end{equation}
%where the output of the cross-entropy computation is contingent upon the independent terms \(Q(x)\log K(x)\) and \((1-Q(x))\log(1-K(x))\). The former term quantifies the degree of similarity when the prediction is positive, while the latter reflects the similarity in cases where the prediction is negative. Furthermore, since cross-entropy results solely depend on these terms, variations in vector length neither affect the computation nor influence the outcome of cross-entropy. Therefore, it is an ideal metric based on the comparison of probability values, where a larger value indicates a significant difference between distributions. Specifically, since we only need to focus on the similarity in scenarios where pulses are present, it suffices to concentrate solely on the term \(Q(x)\log K(x)\).
The former term quantifies the degree of relevance when predictions are positive, while the latter reflects the relevance of negative predictions.
When using cross-entropy as a measure of relevance, spike trains are first normalized and then transformed into probability distributions. Spike trains typically comprise only two states: spike and silence. Therefore, the probability distribution primarily reflects the spike firing rate. This measurement approach focuses on comparing the differences between two probability distributions, disregarding their magnitudes. In summary, cross-entropy is an effective distribution-based tool for assessing the relevance of spike trains, allowing for more precise comparisons and evaluations of similarities and differences between $Q$ and $K$.

\textbf{Approximation Methodology:} Although cross-entropy is an effective method for measuring relevance, a comprehensive analysis of spike states and silent periods may reduce the system's sensitivity. This is primarily because excessive focus on inactive silent periods can obscure critical information present during active spike periods when dealing with sparse spike trains. Given our focus on the spike states rather than silent periods in subsequent analyses, we can neglect the \((1- p_{\text{q}} )\log(1 - p_{\text{k}} )\) component. Therefore, $\mathcal{H} \left(q, k\right)$ can be simplified as follows:
\begin{equation}
    \mathcal{H} \left(a, b\right) \approx - p_{\text{q}} \log p_{\text{k}},
\end{equation}
where \(p_{\text{a}}\) and \(p_{\text{b}}\) respectively represent the spike firing rates. However, the \(\log(\cdot)\) function introduces non-linear operations that compromise the energy efficiency of SNNs. To address this, we propose a further approximation and simplification.

As described in the previous section, within the Transformer blocks of the SNNs, the spike firing rate of the \(Q\) vector and \(K\) vector primarily range from 10\% to 20\%. Consequently, we perform a Taylor expansion of \(\log(x)\) at \(x = 0.15\). This can be expressed as:
\begin{equation}
    \log(x) \approx \log^{(0)}(0.15) + \ldots + \frac{\log^{(n)}(0.15)}{n!} \cdot (x - 0.15)^n
\end{equation}
Here, \( \log^{(n)} \) function denotes the result of the \( n \)-th order derivative of the \(\log (\cdot) \) function.
Given that \(x\) is essentially between $0.1$ and $0.2$, The terms \((P_{Q}-0.15)^2\) and higher-order terms are very small, which can be neglected. Consequently, \(\mathcal{H}(A, B)\) can consider only the first term of the expansion:
\begin{equation}
 \log(x) \approx \log^{(0)}(0.15) + \frac{\log^{(1)}(0.15)}{1!}(x-0.15) \approx  kx + b
 \label{26}
\end{equation}
In the training process of SNNs, since \(k\) and \(b\) can be learned as weights and biases, we use \(x\) to replace \(\log(x)\) to simplify computations. Although this may introduce slight errors, it avoids nonlinear operations and significantly enhances the network's energy efficiency.
\newpage

\section{Saccadic Temporal Interaction}
\label{C}
\textbf{Saccadic mechanism in biologic Vision:} Numerous neuroscience findings\citep{melcher2003spatiotopic, binda2018vision, guadron2022speed} confirm that the eyes do not acquire all details of a scene simultaneously. Instead, attention is focused on specific regions of interest (ROIs) through a series of rapid saccadic called saccades. Each saccade lasts for a very brief period—typically only tens of milliseconds—allowing the retina's high-resolution area to sequentially align with different visual targets. This dynamic saccadic mechanism enables the visual system to process information efficiently by avoiding redundant processing of the entire visual scene. 

\textbf{Other similar works inspired by visual mechanisms}: \cite{zhao2021reconstructing} introduces a model utilizing a retina-inspired spiking camera to enhance image clarity in high-speed motion scenarios. \cite{mcintosh2016deep} explores how deep convolutional neural networks can model the retina's response to natural scenes. \cite{tanaka2019deep} discusses the use of deep learning models to understand the computational mechanisms of the retina. These advanced features of biological vision effectively inform the rational design of deep neural networks, promoting the efficient integration of biological and machine intelligence.

\textbf{Leaky Integrate-and-Fire (LIF) neuron model}: In the LIF models, resetting and decay mechanisms significantly impair the neuron's ability to retain long-term historical information. The model's dynamics are described by the differential equation:
\begin{equation}
\tau_m \frac{\text{d}V}{\text{d}t} = -(V(t) - V_{\text{rest}}) + RI(t),    
\end{equation}

where \(V(t)\) is the membrane potential, \(V_{\text{rest}}\) is the resting potential, \(\tau_m\) is the membrane time constant which influences decay rate, \(R\) is the membrane resistance, and \(I(t)\) is the input current. This equation illustrates how the membrane potential responds to input currents and decays towards \(V_{rest}\).
When the membrane potential \( V(t) \) reaches the threshold \( V_{th} \), the neuron fires and resets the potential to \( V_{\text{reset}} \). This resetting process can be mathematically described as:
\begin{equation}
 V(t^+) = V_{\text{reset}} \quad \text{if} \quad V(t) \geq V_{th},   
\end{equation}


where \( t^+ \) is the time immediately following the spikes. This resetting not only disrupts the continuity of \( V(t) \) but also eliminates all accumulated potential exceeding the threshold.
Moreover, in the absence of input (\( I(t) = 0 \)), the decay mechanism mercilessly forces the membrane potential to exponentially converge to the resting potential \( V_{\text{rest}} \), following the equation:
\begin{equation}
V(t) = V_{\text{rest}} + (V_0 - V_{\text{rest}}) e^{-\frac{t}{\tau_m}},
\end{equation}

where \( V_0 \) is the initial potential. This decay process gradually diminishes the stored information in the neuron, causing the accumulated potential to disappear quickly over time. It severely limits the neuron’s ability to maintain historical information.
To address this issue, we specifically designed saccadic spiking neurons without decay and reset mechanisms. The training and inference processes are described as follows.
\section{Saccadic Neurons}
\textbf{Training Phase:} In the training process of SNN-ViT, the information from all timesteps is inputted in parallel. During this phase, the dynamics of the saccadic spiking neuron can be described as follows:
\begin{equation}
    \begin{cases}
    \mathbf{H} = \mathbf{M}_w \mathbf{S} \\ \\
    \mathbf{S} = \Theta \left(\mathbf{H} -\mathbf{V}_{th} \right)
\end{cases},  \quad \textbf{M}_w\left(\begin{matrix}w_{11}&\cdots&0\\\vdots&\ddots&\vdots\\w_{n1}&\cdots&w_{nn}\\\end{matrix}\right),
\end{equation}
These neurons utilize a learnable lower triangular matrix \(\mathbf{M}\) to integrate information across all timesteps without any loss of historical data, enhancing long-term memory and processing capabilities.
As \(\mathbf{M}\) is a lower triangular matrix, it naturally associates earlier inputs with current states, facilitating a comprehensive understanding of the whole visual scene over time. This method ensures that contributions from each timestep are precisely modulated and accumulated through matrix \(\mathbf{M}\). This approach not only prevents information loss due to decay and resets but also allows neurons to utilize all historical data more effectively for decision-making. Such capability markedly improves their capacity to understand the context of entire image scenes.

\textbf{Inference Phase:} During the training phase, all information is inputted into the network simultaneously, enabling efficient interaction through direct matrix multiplication. However, this parallel processing approach incurs significant resource expenditure, which 
is unfriendly to resource-constrained edge devices. Therefore, to maintain the energy efficiency and asynchronous processing advantages of SNNs, we propose an asynchronous decouple method that only computes the input at the current timestep. The dynamics of the inference process can be described as follows:
\begin{equation}
    \begin{cases}
    H[t] = \mathcal{S}[t] \\
    S[t] = \Theta \left(H[t] - \mathbf{M}_w^{-1}\mathbf{V}_{th}[t]\right)
\end{cases} 
\end{equation}
During the inference process, the threshold \( \mathbf{V}_{th} \) varies at each moment, thus \( \mathbf{V}_{th} \in \mathbb{R}^{T}\). To ensure the existence of the inverse \( \mathbf{M}_w^{-1} \) for the matrix \( \mathbf{M}_w \), certain constraints must be imposed on \( \mathbf{M}_w \). It can be described as follows:
\begin{equation}
\text{det}(\mathbf{M}_w) = m_{11} \times m_{22} \times \ldots \times m_{nn} \neq 0 
\end{equation}
By incorporating the inverse of M into the threshold of the saccadic spiking neurons, we ensure temporal decoupling between H and S. Specifically, \(\mathbf{H}[t]\) only requires input from \(\mathbf{S}[t]\), facilitating asynchronous inference. Additionally, dynamic thresholds at each moment enrich the dynamical properties of the spiking neurons. 
This approach effectively highlights the spatio-temporal attributes of SNNs and ensures efficient performance of vision tasks on resource-constrained edge devices.

\section{Ablation Studies on SSSA}
We add ablation studies on the two key components of the SSSA module: (1) Replacing Distribution-Based Spatial Similarity Computation with traditional Dot Product (DP) similarity; (2) Replacing saccadic neurons with LIF neurons.  Finally, we also compare the performance of versions V1 and V2. Experiments are performed on the CIFAR100 dataset, and the results are presented in the following Table.\ref{aaaaaaaaaa}.
\begin{table}[ht]
\centering
\caption{Ablation Studies on SSSA.}
\label{tab:model_comparison}
\begin{tabular}{@{}lccc@{}}
\toprule
Model   & Param (M) & Complexity       & Acc (\%) \\ \midrule
SSA & 5.76M     & $\mathcal{O}(N^2D)$ & 76.95    \\
SSSA+DP  & 5.52M     & $\mathcal{O}(N^2D)$ & 77.12    \\
SSSA+LIF & 5.52M     & $\mathcal{O}(D)$    & 78.84    \\
SSSA-V1  & 5.52M     & $\mathcal{O}(N^2)$  & 79.71    \\
SSSA-V2  & 5.52M     & $\mathcal{O}(D)$    & 79.60    \\ \bottomrule
\label{aaaaaaaaaa}
\end{tabular}
\end{table}

The SSSA+DP shows almost no performance improvement compared to the baseline~\citep{zhouspikformer}. This outcome underscores that effective spatial similarity computation is the foundation for subsequent saccadic interactions.  Then, substituting saccadic neurons with LIF neurons led to an approximate 0.8\% decrease in performance relative to SSSA.  This demonstrates that saccadic interactions can indeed enhance performance.  Finally, while there is virtually no performance disparity between V1 and V2, the computational complexity of V2 is only $\mathcal{O}\left(D\right)$. In summary, our SSSA-V2 module achieves an optimal trade-off between computational complexity and performance.

\section{Experiment Setting for Image Classification}
\label{D}
On the ImageNet-1K classification benchmark, we propose an architecture according to Spike-driven V2~\citep{yao2024spikev2}. As illustrated in Table~\ref{label5}, our model introduces the GL-SPS part and SSSA block, which respectively replace the Patch Embedding block and self-attention computation blocks. 
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
The architecture of SNN-ViTs primarily consists of four stages. Each stage includes GL-SPS encoding operations and SSSA module, facilitating efficient and precise visual information processing. Specifically, in the initial stage, the input sequence \(I \in \mathbb{R}^{3 \times H \times W}\) is processed through a GL-SPS layer, which encodes it into \(X \in \mathbb{R} ^{C \times \frac{H}{2} \times \frac{W}{2}}\). Then, the encoded images are input into a spiking saccadic self-attention block to enhance feature extraction. This block comprises a SSSA module and an MLP layer in the channel dimension. Moreover, the output will be input to the GL-SPS layer of the next stage which has a similar operation to the previous stage. Additionally, residual connections are applied to membrane potentials in both SSSA module and MLP layer. Finally, the model is processed through a fully connected layer (FCH) to obtain the final classification output.

\begin{figure}[htpb] % 这四个字母可以出现一个或多个：htbp 代表图片插入位置的设置
\centering
\includegraphics[scale=0.5]{ss/ICLR-WS-927.jpg}
\caption{
The detection performance and heatmaps of SNN-ViT-YOLO on the SSDD and NWPU VHR-10 datasets.}
\label{fig7}
\end{figure}

\section{Experiment Details for Remote target detection}
\label{E}
To further validate the capabilities of SNN-ViT across diverse task environments, this study conducts extensive experiments in remote sensing object detection. Specifically, the SNN-ViT is integrated into the widely utilized YOLO-v3 detection framework. Experiments are carried out on a high-performance computing platform equipped with an NVIDIA RTX4090 GPU, using Stochastic Gradient Descent (SGD) as the optimization algorithm. The initial learning rate is set at $1 \times 10^{-2}$, adjusted according to a polynomial decay strategy. The entire training process spans 300 epochs on the NWPU-VHR-10 and SSDD datasets, ensuring comprehensive learning and adaptation to data characteristics. 

As illustrated in Fig.\ref{fig7}, the SNN-ViT-YOLO model exhibits significant performance advantages on the NWPU test set, effectively pinpointing critical target points. This confirms its viability and efficiency in practical applications.
The specific configurations of the network are meticulously detailed in Table~\ref{object}. In this configuration, the expansion ratio of the Multi-Layer Perceptron (MLP) is fixed at 4 to achieve an optimal balance between computational efficiency and model performance. This network architecture involves feeding the $\text{P}_3$, $\text{P}_4$, and $\text{P}_5$ feature maps—derived from intermediate layers of the network—into the detection heads of the YOLO-v3 framework. %These heads are specially designed to utilize these feature maps for accurately predicting the coordinates of object bounding boxes and the associated class probabilities. 

\begin{table}[]
\centering
\caption{The details of experiment setting for ImageNet-1K}
\renewcommand{\arraystretch}{1.4}
\label{label5}
\begin{tabular}{c|c|ccc|ccc}
\hline
Stage              & \# Tokens                  & \multicolumn{3}{c|}{Layer Specification}                                                                                                                             & \multicolumn{1}{c|}{14M} & \multicolumn{1}{c|}{30M} &  \multicolumn{1}{c}{53M} \\ \hline
\multirow{12}{*}{1} & \multirow{6}{*}{$\frac{H}{2} \times \frac{W}{2}$} & \multicolumn{1}{c|}{\multirow{3}{*}{Downsampling}}                                                   & \multicolumn{1}{c|}{\multirow{3}{*}{GL-SPS}}       & Conv       & \multicolumn{3}{c}{$7 \times 7 \text{ stride } 2$}                                      \\ \cline{5-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{}                              & DConv      & \multicolumn{3}{c}{$3 \times 3 \text{ stride } 2 \text{ dilation } 2$}                                      \\ \cline{5-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{}                              & Dim        & \multicolumn{1}{c|}{32}    & \multicolumn{1}{c|}{48}    &  \multicolumn{1}{c}{64}        \\ \cline{3-8} 
                   &                            & \multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Attention-based\\ SNN block\end{tabular}}} & \multicolumn{1}{c|}{SSSA}                          & Conv       & \multicolumn{3}{c}{$ 3 \times 3 \text{ stride } 1$}                                      \\ \cline{4-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{\multirow{2}{*}{Channel Conv}} & Conv       & \multicolumn{3}{c}{$ 1 \times 1 \text{ stride } 1$}                                      \\ \cline{5-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{}                              & Conv ratio & \multicolumn{3}{c}{4}                                      \\ \cline{2-8} 
                   & \multirow{6}{*}{$\frac{H}{2} \times \frac{W}{2}$} & \multicolumn{1}{c|}{\multirow{3}{*}{Downsampling}}                                                   & \multicolumn{1}{c|}{\multirow{3}{*}{GL-SPS}}       & Conv       & \multicolumn{3}{c}{$3 \times 3 \text{ stride } 1$}                                      \\ \cline{5-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{}                              & DConv      & \multicolumn{3}{c}{$3 \times 3 \text{ stride } 2 \text{ dilation } 2$}                                      \\ \cline{5-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{}                              & Dim        & \multicolumn{1}{c|}{64}    & \multicolumn{1}{c|}{96}    &  \multicolumn{1}{c}{128}        \\ \cline{3-8} 
                   &                            & \multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Attention-based\\ SNN block\end{tabular}}} & \multicolumn{1}{c|}{SSSA}                          & Conv       & \multicolumn{3}{c}{$ 3 \times 3 \text{ stride } 1$}                                      \\ \cline{4-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{\multirow{2}{*}{Channel Conv}} & Conv       & \multicolumn{3}{c}{$ 1 \times 1 \text{ stride } 1$}                                      \\ \cline{5-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{}                              & Conv ratio & \multicolumn{3}{c}{4}       \\    \hline 
\multirow{6}{*}{2} & \multirow{6}{*}{$\frac{H}{4} \times \frac{W}{4}$} & \multicolumn{1}{c|}{\multirow{3}{*}{Downsampling}}                                                   & \multicolumn{1}{c|}{\multirow{3}{*}{GL-SPS}}       & Conv       & \multicolumn{3}{c}{$3 \times 3 \text{ stride } 2$}                                      \\ \cline{5-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{}                              & DConv      & \multicolumn{3}{c}{$3 \times 3 \text{ stride } 2 \text{ dilation } 2$}                                      \\ \cline{5-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{}                              & Dim        & \multicolumn{1}{c|}{128}    & \multicolumn{1}{c|}{192}    &  \multicolumn{1}{c}{256}        \\ \cline{3-8} 
                   &                            & \multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Attention-based\\ SNN block\end{tabular}}} & \multicolumn{1}{c|}{SSSA}                          & Conv       & \multicolumn{3}{c}{$ 3 \times 3 \text{ stride } 1$}                                      \\ \cline{4-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{\multirow{2}{*}{Channel Conv}} & Conv       & \multicolumn{3}{c}{$ 1 \times 1 \text{ stride } 1$}                                      \\ \cline{5-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{}                              & Conv ratio & \multicolumn{3}{c}{4}   \\           \hline                
\multirow{6}{*}{3} & \multirow{6}{*}{$\frac{H}{8} \times \frac{W}{8}$} & \multicolumn{1}{c|}{\multirow{3}{*}{Downsampling}}                                                   & \multicolumn{1}{c|}{\multirow{3}{*}{GL-SPS}}       & Conv       & \multicolumn{3}{c}{$3 \times 3 \text{ stride } 2$}                                      \\ \cline{5-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{}                              & DConv      & \multicolumn{3}{c}{$3 \times 3 \text{ stride } 2 \text{ dilation } 2$}                                      \\ \cline{5-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{}                              & Dim        & \multicolumn{1}{c|}{256}    & \multicolumn{1}{c|}{384}    &  \multicolumn{1}{c}{512}        \\ \cline{3-8} 
                   &                            & \multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Attention-based\\ SNN block\end{tabular}}} & \multicolumn{1}{c|}{SSSA}                          & Conv       & \multicolumn{3}{c}{$ 3 \times 3 \text{ stride } 1$}                                      \\ \cline{4-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{\multirow{2}{*}{Channel Conv}} & Conv       & \multicolumn{3}{c}{$ 1 \times 1 \text{ stride } 1$}                                      \\ \cline{5-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{}                              & Conv ratio & \multicolumn{3}{c}{4}   \\           \hline       
\multirow{6}{*}{4} & \multirow{6}{*}{$\frac{H}{16} \times \frac{W}{16}$} & \multicolumn{1}{c|}{\multirow{3}{*}{Downsampling}}                                                   & \multicolumn{1}{c|}{\multirow{3}{*}{GL-SPS}}       & Conv       & \multicolumn{3}{c}{$3 \times 3 \text{ stride } 2$}                                      \\ \cline{5-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{}                              & DConv      & \multicolumn{3}{c}{$3 \times 3 \text{ stride } 2 \text{ dilation } 2$}                                      \\ \cline{5-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{}                              & Dim        & \multicolumn{1}{c|}{256}    & \multicolumn{1}{c|}{384}    &  \multicolumn{1}{c}{512}        \\ \cline{3-8} 
                   &                            & \multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Attention-based\\ SNN block\end{tabular}}} & \multicolumn{1}{c|}{SSSA}                          & Conv       & \multicolumn{3}{c}{$ 3 \times 3 \text{ stride } 1$}                                      \\ \cline{4-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{\multirow{2}{*}{Channel Conv}} & Conv       & \multicolumn{3}{c}{$ 1 \times 1 \text{ stride } 1$}                                      \\ \cline{5-8} 
                   &                            & \multicolumn{1}{c|}{}                                                                                & \multicolumn{1}{c|}{}                              & Conv ratio & \multicolumn{3}{c}{4}   \\           \hline    
\end{tabular}
\end{table}



\begin{table}[]
\centering
\label{object}
\caption{Configurations of SNN-ViT on object detection.}
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{c|c|ccc|c}
\hline
Stage              & \# Tokens         & \multicolumn{3}{c|}{Layer Specification}                                                                              & Model \\ \hline
\multirow{7}{*}{P1} & \multirow{7}{*}{$\frac{H}{2} \times \frac{W}{2}$} & \multicolumn{1}{c|}{\multirow{3}{*}{Downsampling}} & \multicolumn{1}{c|}{\multirow{3}{*}{GL-SPS}}        & Conv       &   $3 \times 3 \text{ stride } 2 \text{ dilation } 2$    \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & DConv      &  $7 \times 7 \text{ stride } 2$     \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & Dim        &    64   \\ \cline{3-6} 
                   &                   & \multicolumn{1}{c|}{\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Conv-based\\ SNN block\end{tabular}}}    & \multicolumn{1}{c|}{\multirow{2}{*}{Conv Layer}}    & Conv       &   $3 \times 3 \text{ stride } 2 \text{ dilation } 1$     \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & Dim        &     64  \\ \cline{4-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{\multirow{2}{*}{Channnel Conv}} & Conv       &   $1 \times 1 \text{ stride } 1$    \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & Conv ratio &    4   \\ \hline
\multirow{7}{*}{P2} & \multirow{7}{*}{$\frac{H}{4} \times \frac{W}{4}$} & \multicolumn{1}{c|}{\multirow{3}{*}{Downsampling}} & \multicolumn{1}{c|}{\multirow{3}{*}{GL-SPS}}        & Conv       &   $3 \times 3 \text{ stride } 2 \text{ dilation } 2$    \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & DConv      &  $7 \times 7 \text{ stride } 2$     \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & Dim        &    128   \\ \cline{3-6} 
                   &                   & \multicolumn{1}{c|}{\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Conv-based\\ SNN block\end{tabular}}}    & \multicolumn{1}{c|}{\multirow{2}{*}{Conv Layer}}    & Conv       &   $3 \times 3 \text{ stride } 2 \text{ dilation } 1$     \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & Dim        &     128  \\ \cline{4-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{\multirow{2}{*}{Channnel Conv}} & Conv       &   $1 \times 1 \text{ stride } 1$    \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & Conv ratio &    4   \\ \hline
\multirow{7}{*}{P3} & \multirow{7}{*}{$\frac{H}{8} \times \frac{W}{8}$} & \multicolumn{1}{c|}{\multirow{3}{*}{Downsampling}} & \multicolumn{1}{c|}{\multirow{3}{*}{GL-SPS}}        & Conv       &   $3 \times 3 \text{ stride } 2 \text{ dilation } 2$    \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & DConv      &  $7 \times 7 \text{ stride } 2$     \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & Dim        &    256   \\ \cline{3-6} 
                   &                   & \multicolumn{1}{c|}{\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Attention-based\\ SNN block\end{tabular}}}    & \multicolumn{1}{c|}{\multirow{2}{*}{Conv Layer}}    & Conv       &   $3 \times 3 \text{ stride } 2 \text{ dilation } 1$     \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & Dim        &     256  \\ \cline{4-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{\multirow{2}{*}{Channnel Conv}} & Conv       &   $1 \times 1 \text{ stride } 1$    \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & Conv ratio &    4   \\ \hline
\multirow{7}{*}{P4} & \multirow{7}{*}{$\frac{H}{16} \times \frac{W}{16}$} & \multicolumn{1}{c|}{\multirow{3}{*}{Downsampling}} & \multicolumn{1}{c|}{\multirow{3}{*}{GL-SPS}}        & Conv       &   $3 \times 3 \text{ stride } 2 \text{ dilation } 2$    \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & DConv      &  $7 \times 7 \text{ stride } 2$     \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & Dim        &    256   \\ \cline{3-6} 
                   &                   & \multicolumn{1}{c|}{\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Attention-based\\ SNN block\end{tabular}}}    & \multicolumn{1}{c|}{\multirow{2}{*}{Conv Layer}}    & Conv       &   $3 \times 3 \text{ stride } 2 \text{ dilation } 1$     \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & Dim        &     256  \\ \cline{4-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{\multirow{2}{*}{Channnel Conv}} & Conv       &   $1 \times 1 \text{ stride } 1$    \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & Conv ratio &    4   \\ \hline
\multirow{7}{*}{P5} & \multirow{7}{*}{$\frac{H}{32} \times \frac{W}{32}$} & \multicolumn{1}{c|}{\multirow{3}{*}{Downsampling}} & \multicolumn{1}{c|}{\multirow{3}{*}{GL-SPS}}        & Conv       &   $3 \times 3 \text{ stride } 2 \text{ dilation } 2$    \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & DConv      &  $7 \times 7 \text{ stride } 2$     \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & Dim        &    512   \\ \cline{3-6} 
                   &                   & \multicolumn{1}{c|}{\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Attention-based\\ SNN block\end{tabular}}}    & \multicolumn{1}{c|}{\multirow{2}{*}{Conv Layer}}    & Conv       &   $3 \times 3 \text{ stride } 2 \text{ dilation } 1$     \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & Dim        &     512  \\ \cline{4-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{\multirow{2}{*}{Channnel Conv}} & Conv       &   $1 \times 1 \text{ stride } 1$    \\ \cline{5-6} 
                   &                   & \multicolumn{1}{c|}{}                              & \multicolumn{1}{c|}{}                               & Conv ratio &    4   \\ \hline
\end{tabular}
\end{table}

% \begin{table}[htbp]
% \caption{Configurations of SNN-ViT on object detection.}
% \label{object}
% \centering
% \renewcommand{\arraystretch}{1.6}
% \begin{tabular}{c|c|c|c|c|c}
% \hline
% Stage                  & Output                                          & Dim                  & Module       & Size         & Index \\ \hline
% \multirow{2}{*}{P1}  & \multirow{2}{*}{$\frac H2\times\frac W2$}       & \multirow{2}{*}{64}  & DownSampling & 7*7 stride 2 & 1     \\  \cline{4-6} 
%                        &                                                 &                      & ConvBlock    & 3*3 stride 1 & 2     \\ \hline
% \multirow{2}{*}{P2}  & \multirow{2}{*}{$\frac H4\times\frac W4$}       & \multirow{2}{*}{128} & DownSampling & 3*3 stride 2 & 3     \\ \cline{4-6} 
%                        &                                                 &                      & ConvBlock    & 3*3 stride 1 & 4     \\ \hline
% \multirow{2}{*}{P3}  & \multirow{2}{*}{$\frac H8\times\frac W8$}       & \multirow{2}{*}{256} & DownSampling & 3*3 stride 2 & 5     \\ \cline{4-6} 
%                        &                                                 &                      & SSSA-Block   & -            & 6     \\ \hline
% \multirow{2}{*}{P4} & \multirow{2}{*}{$\frac H{16}\times\frac W{16}$} & \multirow{2}{*}{256} & DownSampling & 3x3 stride 2 & 7     \\ \cline{4-6} 
%                        &                                                 &                      & SSSA-Block   & -            & 8     \\ \hline
% \multirow{2}{*}{P5} & \multirow{2}{*}{$\frac H{32}\times\frac W{32}$} & \multirow{2}{*}{512} & DownSampling & 3x3 stride 2 & 9     \\ \cline{4-6} 
%                        &                                                 &                      & SSSA-Block   & -            & 10    \\ \hline
% \end{tabular}
% \end{table}


\end{document}
