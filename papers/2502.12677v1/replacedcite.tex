\section{Related Work}
\textbf{Vision Transformers:} ViTs segment images into patches and apply self-attention____ to learn inter-patch relationships, outperforming CNNs across multiple vision tasks____. Nevertheless, ViTs face challenges like high parameter counts____, and increased computational complexity proportional to token length____. 
To enhance the computational efficiency of ViTs, many researchers____ are focused on exploring lightweight improvement methods. For example, LeViT____ incorporates convolutional elements to expedite processing, and MobileViT____ combines lightweight MobileNet blocks with MHSA, achieving lightweight ViTs successfully. However, these enhancements still rely on expensive MAC computations which are not suitable for resource-limited devices. This highlights the need for investigating more energy-efficient ViT solutions.

\textbf{Spiking Neural Networks:} The event-driven mechanism enhances the energy efficiency of SNNs, offering a significant advantage for compute-constrained edge devices. With the introduction of ANN-SNN____ and direct training____ algorithm, the difficult associated with training high-performance SNNs is significantly reduced.
Based on these advanced learning algorithms, some research____ propose deep residual SNNs____ and others____ contribute multi-dimensional spike attention mechanisms, achieving competitive performance on many tasks____. 
%Subsequently, some research explores parallel training strategies____ and shortcut residual connections____ for SNNs within the ResNet framework.
These improvements further enhance the application of SNNs in various visual tasks. However, despite rapid advancements, a significant performance gap remains between these traditional deep SNN architectures and the latest ViTs.

% ViTs process input images by segmenting them into non-overlapping patches and utilizes Self-Attention to learn inter-patch representations, effectively mitigating inductive bias____. ViTs has demonstrated superior performance across various computer vision tasks compared to Convolutional Neural Networks (CNNs). However, its limitations have become increasingly evident with continued development and application. These include a high parameter count, quadratic growth in computational complexity relative to token length, the presence of non-fusible normalization layers, and a lack of compiler-level optimizations. Recently, LeViT____ introduced a convolutional approach to accelerate ViT, though executing MHSA still requires reshaping 4D features into flat blocks, demanding substantial computational resources. Similarly, MobileViT____ integrates lightweight MobileNet blocks (featuring pointwise and depthwise convolutions) with MHSA blocks. Despite these innovations, these methods still rely on full-precision floating-point computations, which remain computationally expensive on resource-constrained edge devices. Thus, exploring more efficient computational paradigms represents a potential direction for enhancing ViT performance.
\textbf{Vision Transformers Meet Spiking Neural Networks:} To explore high-performance and energy-efficient visual solutions, SNN-based ViTs____ have emerged.
%have recently emerged.
Spikformer____ pioneers a spike-based self-attention computation, establishing the first spiking ViT. 
However, they still utilize expensive MAC operations and matrix multiplication in self-attention computation, which are inefficient for binary spikes. 
Recently, Spike-driven Transformer____ implements Hadamard product in the self-attention module for a fully spike-driven ViT. Additionally,  SpikingResformer____ integrates a Dual Spike self-attention module for improved performance and energy efficiency. However, these models primarily treat self-attention as a token mixer____, without exploring an effective relevance computation suited to spike trains. Moreover, they also overlook the temporal dynamics of SNNs.____. Therefore, developing spike self-attention mechanisms tailored to the spatio-temporal characteristics of SNNs is essential for further advancements.