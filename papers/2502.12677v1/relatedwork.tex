\section{Related Work}
\textbf{Vision Transformers:} ViTs segment images into patches and apply self-attention~\citep{vaswani2017attention, kenton2019bert} to learn inter-patch relationships, outperforming CNNs across multiple vision tasks~\citep{mei2021image, bertasius2021space, guo2021pct}. Nevertheless, ViTs face challenges like high parameter counts~\citep{liu2021swin}, and increased computational complexity proportional to token length~\citep{pan2020x, liu2022ecoformer}. 
To enhance the computational efficiency of ViTs, many researchers~\citep{jie2023fact, li2023rethinking} are focused on exploring lightweight improvement methods. For example, LeViT~\citep{graham2021levit} incorporates convolutional elements to expedite processing, and MobileViT~\citep{mehta2021mobilevit} combines lightweight MobileNet blocks with MHSA, achieving lightweight ViTs successfully. However, these enhancements still rely on expensive MAC computations which are not suitable for resource-limited devices. This highlights the need for investigating more energy-efficient ViT solutions.

\textbf{Spiking Neural Networks:} The event-driven mechanism enhances the energy efficiency of SNNs, offering a significant advantage for compute-constrained edge devices. With the introduction of ANN-SNN~\citep{cao2015spiking, han2020rmp, wu2021progressive} and direct training~\citep{wu2018spatio, fang2021incorporating, zhang2021rectified, wei2023temporal} algorithm, the difficult associated with training high-performance SNNs is significantly reduced.
Based on these advanced learning algorithms, some research~\citep{hu2021spiking,zheng2021going, hu2024advancing} propose deep residual SNNs~\citep{wang2024ternary, shan2024advancing} and others~\citep{yao2023attention,zhu2024tcja,shan2024advancing} contribute multi-dimensional spike attention mechanisms, achieving competitive performance on many tasks~\citep{zhang2024spike}. 
%Subsequently, some research explores parallel training strategies~\citep{fang2024parallel} and shortcut residual connections~\citep{fang2021deep, hu2024advancing} for SNNs within the ResNet framework.
These improvements further enhance the application of SNNs in various visual tasks. However, despite rapid advancements, a significant performance gap remains between these traditional deep SNN architectures and the latest ViTs.

% ViTs process input images by segmenting them into non-overlapping patches and utilizes Self-Attention to learn inter-patch representations, effectively mitigating inductive bias~\citep{dosovitskiy2020image}. ViTs has demonstrated superior performance across various computer vision tasks compared to Convolutional Neural Networks (CNNs). However, its limitations have become increasingly evident with continued development and application. These include a high parameter count, quadratic growth in computational complexity relative to token length, the presence of non-fusible normalization layers, and a lack of compiler-level optimizations. Recently, LeViT~\citep{graam2021levit} introduced a convolutional approach to accelerate ViT, though executing MHSA still requires reshaping 4D features into flat blocks, demanding substantial computational resources. Similarly, MobileViT~\citep{mehta2021mobilevit} integrates lightweight MobileNet blocks (featuring pointwise and depthwise convolutions) with MHSA blocks. Despite these innovations, these methods still rely on full-precision floating-point computations, which remain computationally expensive on resource-constrained edge devices. Thus, exploring more efficient computational paradigms represents a potential direction for enhancing ViT performance.
\textbf{Vision Transformers Meet Spiking Neural Networks:} To explore high-performance and energy-efficient visual solutions, SNN-based ViTs~\citep{zhouspikformer, wang2023complex} have emerged.
%have recently emerged.
Spikformer~\citep{zhouspikformer,zhou2023spikingformer} pioneers a spike-based self-attention computation, establishing the first spiking ViT. 
However, they still utilize expensive MAC operations and matrix multiplication in self-attention computation, which are inefficient for binary spikes. 
Recently, Spike-driven Transformer~\citep{yao2024spike} implements Hadamard product in the self-attention module for a fully spike-driven ViT. Additionally,  SpikingResformer~\citep{shi2024spikingresformer} integrates a Dual Spike self-attention module for improved performance and energy efficiency. However, these models primarily treat self-attention as a token mixer~\citep{yu2022metaformer}, without exploring an effective relevance computation suited to spike trains. Moreover, they also overlook the temporal dynamics of SNNs.~\citep{zhang2021rectified, bohte2000spikeprop}. Therefore, developing spike self-attention mechanisms tailored to the spatio-temporal characteristics of SNNs is essential for further advancements.