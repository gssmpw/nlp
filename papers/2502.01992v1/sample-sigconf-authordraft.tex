%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.

%%\setcopyright{acmlicensed}
%%\copyrightyear{2024}
%%\acmYear{2024}
%%\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.

%%\acmConference[ICAIF]

%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{FinRLlama: A Solution to LLM-Engineered Signals Challenge at FinRL Contest 2024}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Arnav Grover}
\email{grover41@purdue.edu}
\orcid{0009-0004-9422-5426}
\affiliation{%
  \institution{Purdue University}
  \city{West Lafayette}
  \state{Indiana}
  \country{USA}
}
%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
     In response to Task II of the FinRL Challenge at ACM ICAIF 2024, this study proposes a novel prompt framework for fine-tuning large language models (LLM) with Reinforcement Learning from Market Feedback (RLMF). Our framework incorporates market-specific features and short-term price dynamics to generate more precise trading signals.  Traditional LLMs, while competent in sentiment analysis, lack contextual alignment for financial market applications. To bridge this gap, we fine-tune the LLaMA-3.2-3B-Instruct model using a custom RLMF prompt design that integrates historical market data and reward-based feedback. Our evaluation shows that this RLMF-tuned framework outperforms baseline methods in signal consistency and achieving tighter trading outcomes; awarded as winner of Task II. You can find the code for this project on \href{https://github.com/Arnav-Gr0ver/ICAIF_FinRL-2024}{\textcolor{blue}{GitHub}}.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010179.10010182</concept_id>
       <concept_desc>Computing methodologies~Natural language generation</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257.10010258.10010261</concept_id>
       <concept_desc>Computing methodologies~Reinforcement learning</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010179.10003352</concept_id>
       <concept_desc>Computing methodologies~Information extraction</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Natural language generation}
\ccsdesc[500]{Computing methodologies~Reinforcement learning}
\ccsdesc[300]{Computing methodologies~Information extraction}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Large Language Models (LLMs), Reinforcement Learning, Financial Sentiment Analysis, Prompt Engineering, Market Feedback, Trading Signals}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
\begin{teaserfigure}
  \centering
  \includegraphics[width=250pt]{cover_img}
  \captionsetup{font=large}
  \caption{FinRLlama}
  \label{fig:teaser}
\end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

The application of large language models (LLMs) to financial sentiment analysis represents a significant opportunity for algorithmic trading strategies \cite{Liu22}. Although LLMs demonstrate sophisticated language understanding capabilities, their application in financial contexts has been limited by the challenge of incorporating market-specific knowledge and temporal dynamics \cite{Nie24}.

\subsection{Background and Related Work}

The evolution of financial sentiment analysis has followed several key trajectories in the literature. Early work by Loughran and McDonald \cite{Loughran11} established the importance of domain-specific dictionaries for financial text analysis, highlighting how general-purpose sentiment tools often fail in financial contexts. In parallel, the development of comprehensive financial reinforcement learning frameworks like FinRL-Meta \cite{Liu20} has provided standardized environments for developing and evaluating trading strategies.

Recent developments in prompt engineering have shown promising results in various domains. Wei et al. \cite{Wei22} demonstrated how carefully constructed prompts can elicit domain-specific knowledge from LLMs without fine-tuning, while Vatsal and Dubey \cite{Vatsal23} provided various methods / frameworks for evaluating prompt effectiveness in various NLP tasks. However, applications in financial sentiment analysis have been limited, with most approaches focusing on model architecture modifications rather than prompt optimization.

\subsection{Research Objectives}

In response, we establish the following objectives for this task:

\begin{enumerate}
    \item Develop a novel prompt framework for sentiment analysis.
    \item Establish a systematic training methodology for model optimization that adapts dynamically to market conditions.
    \item Empirically validate the framework’s impact on sentiment-based signal precision and trading performance.
\end{enumerate}

These objectives aim to create a robust framework for generating actionable insights from financial news, advancing the utility of LLM in financial applications through a combination of prompt engineering and market-aligned learning.

\section{Methodology}

\subsection{Prompt Architecture}

The sentiment analysis prompt architecture generates stock performance predictions from news headlines using a scalable sentiment framework. Building on established sentiment analysis methods in finance \cite{Loughran11}, \cite{Bollen11}, the system integrates market feedback and historical data to improve predictive precision \cite{MDPI23}. Adjustable parameters enables market adaptation, while single-score output supports rapid trading decisions.

\begin{algorithm}
\caption{Sentiment Signal Scoring Prompt}
\KwIn{Signal Bound, Threshold, News Headline, Price Data}
\KwOut{Value in [-\texttt{signal\_strength}, \texttt{signal\_strength}]}

\BlankLine

\textbf{[CONTEXT]} \\
Task: Analyze the stock-related news headline and output a sentiment score reflecting the sentiment's potential impact on stock performance.

\BlankLine

\textbf{[SENTIMENT SCORING PARAMETERS]} \\
\texttt{-signal\_strength:} Highly negative market sentiment \\
\texttt{-threshold:} Moderately negative market sentiment \\
\texttt{0:} Neutral market sentiment \\
\texttt{threshold:} Moderately positive sentiment\\
\texttt{signal\_strength:} Highly positive sentiment

\BlankLine

\textbf{[MARKET FEEDBACK CONSIDERATIONS]} \\
\texttt{Past Market Responses:} Incorporate past market responses to similar news events. \\
\texttt{Market Sentiment Alignment:} Evaluate if the news aligns with or contradicts prevailing market sentiment. \\
\texttt{Historical Price Patterns:} Analyze the historical impact of similar news on stock prices.

\BlankLine

\textbf{[SENTIMENT SCORING EXAMPLES]} \\
"Company X announces layoffs amid economic downturn." Sentiment Score: -8 \\
"Company Y reports record revenue growth in Q1." Sentiment Score: 7 \\
"Market responds positively to Company Z’s new product launch." Sentiment Score: 5

\BlankLine

\textbf{[OUTPUT]} \\
Integer sentiment score in range [\texttt{-signal\_strength}, \texttt{signal\_strength}] based on analysis.

\end{algorithm}

\subsection{Training Process}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{Train-Test-Trade_Pipeline.png}
  \captionsetup{font=large}
  \caption{FinRL Train-Test-Trade Pipeline}
\end{figure}

The model fine-tuning process begins with the base Llama-3.2-3B-Instruct model. The reinforcement learning (RL) component simulates market interactions, where the model outputs sentiment signals, and the model selects trading actions (long, short, or hold). The reward function then evaluates the model's predictions by comparing sentiment scores to actual market performance, assigning rewards or penalties based on the accuracy of the sentiment and resultant profits or losses. This process aligns with established RL frameworks in financial applications \cite{Wang23}, \cite{Mnih15}, and follows the task requirements outlined in the ACM ICAIF FinRL 2024 Competition \cite{Holzer24}.

The reward function is dynamically adjusted on the basis of the strength of the model's sentiment signal, reinforcing correct predictions, and penalizing errors. The function takes into account the confidence of the model, incorporating adjustable thresholds to assess market direction. For instance, when the sentiment score exceeds a threshold, the reward varies depending on the actual price movement: long positions are rewarded if a strong positive return is observed, while negative returns despite positive sentiment lead to penalties. Similarly, short positions are rewarded when negative returns align with the sentiment. This system helps the model refine its decision making over time through feedback loops, gradually improving its accuracy and trading strategies.

The model's fine-tuning process is guided by the Adam optimizer, minimizing the loss function based on the discrepancy between predicted sentiment signals and actual market outcomes. This approach follows deep-RL strategies aimed at optimal decision-making, balancing exploration, and exploitation to generate robust sentiment-based trading signals.

\section{Results}

\subsection{Experimental Setup}

The experimental setup for testing and validating the proposed model's performance spans 2020 to 2023, assessing the accuracy and profitability of sentiment-based trading signals against the baseline. This setup includes news headlines, stock price data, and technical indicators to effectively align sentiment scores with stock movements. Each headline is pre-processed to link with relevant stock price data, and a three-day forward close price is added to facilitate forward-looking impact analysis.

We designate 2020–2022 as the training period and use 2023 exclusively for evaluation. This split enables an assessment of the robustness of the model in diverse market conditions. For both the model and the baseline, each headline generates a buy, hold, or sell signal, with performance measured by cumulative returns, win/loss rate, etc.

\subsection{Performance Metrics}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{FinRLlama_cumulative_returns_plot_1.png}
  \captionsetup{font=large}
  \caption{FinRLlama Cumulative Returns}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{Llama_cumulative_returns_plot_1.png}
  \captionsetup{font=large}
  \caption{Llama Cumulative Returns}
\end{figure}

\subsection{Comparative Analysis}

In Figure 3, the cumulative returns of the tickers appear to be less volatile. Although NVDA still leads with positive cumulative returns, its gains are not as pronounced and the spread between the highest and lowest performing stocks is narrower compared to the first graph. XOM continues to trend downward, but with a less steep decline. This indicates a model response that may be more conservative, possibly due to model fine-tuning. In Figure 3, the cumulative returns show significant variation between tickers. For example, NVDA displays consistently higher returns, reaching above 1.5, indicating strong performance. Other stocks like MSFT and GOOG exhibit moderate cumulative returns, staying close to 0.5, while XOM shows a downward trend - temporarily dipping into negative returns. This plot suggests a broader divergence in performance between stocks, with the model interpreting NVDA as markedly outperforming others and XOM underperforming.

The mean cumulative evaluation return, shown in both second subplots, oscillates around zero in both cases.

In summary, Llama-3.2-3B-Instruct displays a broader range of cumulative returns, indicating higher variability and greater individual gains and losses, while FinRLlama suggests a more conservative approach with reduced volatility in cumulative returns across tickers and smoother mean evaluation. 

\section{Future Work}

Future work to improve the model could focus on refining the reward function to better capture the nuances of financial market dynamics. By incorporating dynamic reward adjustments that account for market volatility and shifts in sentiment, the model could become more responsive to short-term fluctuations and long-term trends. Enhancing the model’s ability to process and integrate historical price data and sentiment trends could improve its prediction accuracy, allowing it to account for delayed market reactions more effectively. Furthermore, further fine-tuning with domain-specific financial data would help the model better adapt to the intricacies of market behavior, improving its decision-making accuracy. These improvements could significantly improve the robustness of the model and its ability to generate actionable trading insights.

\section{Acknowledgments}

Arnav Grover acknowledges the support of Keyi Wang, Nikolaus Holzer, and Xiao-Yang Liu Yanglet for their invaluable guidance and support throughout the development of this work. We also extend sincere thanks to Yanglet and the organizing team of the FinRL 2024 contest for their exceptional efforts in creating a collaborative and intellectually stimulating platform. Their dedication to fostering innovation and research in the field of open finance has provided an invaluable opportunity to contribute to this evolving domain.

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
% \bibliography{sample-base}
%%% -*-BibTeX-*-
%%% Do NOT edit. File created by BibTeX with style
%%% ACM-Reference-Format-Journals [18-Jan-2012].

\begin{thebibliography}{11}

%%% ====================================================================
%%% NOTE TO THE USER: you can override these defaults by providing
%%% customized versions of any of these macros before the \bibliography
%%% command.  Each of them MUST provide its own final punctuation,
%%% except for \shownote{}, \showDOI{}, and \showURL{}.  The latter two
%%% do not use final punctuation, in order to avoid confusing it with
%%% the Web address.
%%%
%%% To suppress output of a particular field, define its macro to expand
%%% to an empty string, or better, \unskip, like this:
%%%
%%% \newcommand{\showDOI}[1]{\unskip}   % LaTeX syntax
%%%
%%% \def \showDOI #1{\unskip}           % plain TeX syntax
%%%
%%% ====================================================================

\ifx \showCODEN    \undefined \def \showCODEN     #1{\unskip}     \fi
\ifx \showDOI      \undefined \def \showDOI       #1{#1}\fi
\ifx \showISBNx    \undefined \def \showISBNx     #1{\unskip}     \fi
\ifx \showISBNxiii \undefined \def \showISBNxiii  #1{\unskip}     \fi
\ifx \showISSN     \undefined \def \showISSN      #1{\unskip}     \fi
\ifx \showLCCN     \undefined \def \showLCCN      #1{\unskip}     \fi
\ifx \shownote     \undefined \def \shownote      #1{#1}          \fi
\ifx \showarticletitle \undefined \def \showarticletitle #1{#1}   \fi
\ifx \showURL      \undefined \def \showURL       {\relax}        \fi
% The following commands are used for tagged output and should be
% invisible to TeX
\providecommand\bibfield[2]{#2}
\providecommand\bibinfo[2]{#2}
\providecommand\natexlab[1]{#1}
\providecommand\showeprint[2][]{arXiv:#2}

\bibitem[Alhindi et~al\mbox{.}(2023)]%
        {MDPI23}
\bibfield{author}{\bibinfo{person}{Fahad Alhindi}, \bibinfo{person}{S.~M. Raza}, {and} \bibinfo{person}{P. Gupta}.} \bibinfo{year}{2023}\natexlab{}.
\newblock \showarticletitle{Predicting Economic Trends and Stock Market Prices with Deep Learning and Advanced Machine Learning Techniques}.
\newblock \bibinfo{journal}{\emph{MDPI Electronics}} \bibinfo{volume}{13}, \bibinfo{number}{17} (\bibinfo{year}{2023}), \bibinfo{pages}{3396}.
\newblock
\urldef\tempurl%
\url{https://doi.org/10.3390/electronics13173396}
\showDOI{\tempurl}
\newblock
\shownote{Explores deep learning methods for predicting stock market trends}.


\bibitem[Holzer et~al\mbox{.}(2024)]%
        {Holzer24}
\bibfield{author}{\bibinfo{person}{Nikolaus Holzer}, \bibinfo{person}{Keyi Wang}, \bibinfo{person}{Kairong Xiao}, {and} \bibinfo{person}{Xiao-Yang Liu}.} \bibinfo{year}{2024}\natexlab{}.
\newblock \showarticletitle{Revisiting Ensemble Methods for Stock Trading and Crypto Trading Tasks at ACM ICAIF FinRL Contest 2023-2024}.
\newblock \bibinfo{journal}{\emph{arXiv preprint}} (\bibinfo{year}{2024}).
\newblock
\urldef\tempurl%
\url{https://doi.org/10.48550/arXiv.2501.10709}
\showURL{%
\tempurl}


\bibitem[Johan~Bollen and Zeng(2011)]%
        {Bollen11}
\bibfield{author}{\bibinfo{person}{Huina~Mao Johan~Bollen} {and} \bibinfo{person}{Xiaojun Zeng}.} \bibinfo{year}{2011}\natexlab{}.
\newblock \showarticletitle{Twitter mood predicts the stock market}.
\newblock \bibinfo{journal}{\emph{Journal of Computational Science}} \bibinfo{volume}{2}, \bibinfo{number}{1} (\bibinfo{year}{2011}), \bibinfo{pages}{1--8}.
\newblock
\urldef\tempurl%
\url{https://doi.org/10.1016/j.jocs.2010.12.007}
\showURL{%
\tempurl}


\bibitem[Liu et~al\mbox{.}(2022)]%
        {Liu20}
\bibfield{author}{\bibinfo{person}{Xiao-Yang Liu}, \bibinfo{person}{Ziyi Xia}, \bibinfo{person}{Jingyang Rui}, \bibinfo{person}{Jiechao Gao}, \bibinfo{person}{Hongyang Yang}, \bibinfo{person}{Ming Zhu}, \bibinfo{person}{Christina~Dan Wang}, \bibinfo{person}{Zhaoran Wang}, {and} \bibinfo{person}{Jian Guo}.} \bibinfo{year}{2022}\natexlab{}.
\newblock \showarticletitle{FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning}.
\newblock \bibinfo{journal}{\emph{NeurIPS Datasets and Benchmarks}} (\bibinfo{year}{2022}), \bibinfo{pages}{1--36}.
\newblock
\urldef\tempurl%
\url{https://doi.org/10.48550/arXiv.2211.03107}
\showURL{%
\tempurl}


\bibitem[Liu et~al\mbox{.}(2020)]%
        {Liu22}
\bibfield{author}{\bibinfo{person}{Xiao-Yang Liu}, \bibinfo{person}{Hongyang Yang}, \bibinfo{person}{Qian Chen}, \bibinfo{person}{Runjia Zhang}, \bibinfo{person}{Liuqing Yang}, \bibinfo{person}{Bowen Xiao}, {and} \bibinfo{person}{Christina~Dan Wang}.} \bibinfo{year}{2020}\natexlab{}.
\newblock \showarticletitle{FinRL: A Deep Reinforcement Learning Library for Automated Stock Trading in Quantitative Finance}.
\newblock \bibinfo{journal}{\emph{NeurIPS Workshop on Deep Reinforcement Learning}} (\bibinfo{year}{2020}).
\newblock
\urldef\tempurl%
\url{https://doi.org/10.48550/arXiv.2011.09607}
\showURL{%
\tempurl}


\bibitem[Loughran and McDonald(2011)]%
        {Loughran11}
\bibfield{author}{\bibinfo{person}{Tim Loughran} {and} \bibinfo{person}{Bill McDonald}.} \bibinfo{year}{2011}\natexlab{}.
\newblock \showarticletitle{When is a Liability Not a Liability? Textual Analysis, Dictionaries, and 10‐Ks}.
\newblock \bibinfo{journal}{\emph{The Journal of Finance}} \bibinfo{volume}{66}, \bibinfo{number}{1} (\bibinfo{date}{Feb.} \bibinfo{year}{2011}), \bibinfo{pages}{35--65}.
\newblock
\urldef\tempurl%
\url{https://doi.org/10.1111/j.1540-6261.2010.01625.x}
\showDOI{\tempurl}
\newblock
\shownote{Explores domain-specific dictionaries for financial text analysis}.


\bibitem[Mnih et~al\mbox{.}(2015)]%
        {Mnih15}
\bibfield{author}{\bibinfo{person}{Volodymyr Mnih}, \bibinfo{person}{Koray Kavukcuoglu}, \bibinfo{person}{David Silver}, {et~al\mbox{.}}} \bibinfo{year}{2015}\natexlab{}.
\newblock \showarticletitle{Human-level control through deep reinforcement learning}.
\newblock \bibinfo{journal}{\emph{Nature}}  \bibinfo{volume}{518} (\bibinfo{year}{2015}), \bibinfo{pages}{529--533}.
\newblock
\urldef\tempurl%
\url{https://doi.org/10.1038/nature14236}
\showURL{%
\tempurl}


\bibitem[Nie et~al\mbox{.}(2024)]%
        {Nie24}
\bibfield{author}{\bibinfo{person}{Yuqi Nie}, \bibinfo{person}{Yaxuan Kong}, \bibinfo{person}{Xiaowen Dong}, \bibinfo{person}{John~M. Mulvey}, \bibinfo{person}{H.~Vincent Poor}, \bibinfo{person}{Qingsong Wen}, {and} \bibinfo{person}{Stefan Zohren}.} \bibinfo{year}{2024}\natexlab{}.
\newblock \showarticletitle{A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges}.
\newblock \bibinfo{journal}{\emph{arXiv preprint}} (\bibinfo{year}{2024}).
\newblock
\urldef\tempurl%
\url{https://doi.org/10.48550/arXiv.2406.11903}
\showURL{%
\tempurl}


\bibitem[Sun et~al\mbox{.}(2023)]%
        {Wang23}
\bibfield{author}{\bibinfo{person}{Shuo Sun}, \bibinfo{person}{Rundong Wang}, {and} \bibinfo{person}{Bo An}.} \bibinfo{year}{2023}\natexlab{}.
\newblock \showarticletitle{Reinforcement Learning for Quantitative Trading}.
\newblock \bibinfo{journal}{\emph{ACM Transactions on Intelligent Systems and Technology}} \bibinfo{volume}{14}, \bibinfo{number}{3} (\bibinfo{year}{2023}), \bibinfo{pages}{1--29}.
\newblock
\urldef\tempurl%
\url{https://doi.org/10.1145/3582560}
\showDOI{\tempurl}


\bibitem[Vatsal and Dubey(2023)]%
        {Vatsal23}
\bibfield{author}{\bibinfo{person}{Shubham Vatsal} {and} \bibinfo{person}{Harsh Dubey}.} \bibinfo{year}{2023}\natexlab{}.
\newblock \showarticletitle{A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks}.
\newblock \bibinfo{journal}{\emph{arXiv preprint}} (\bibinfo{year}{2023}).
\newblock
\urldef\tempurl%
\url{https://arxiv.org/pdf/2407.12994}
\showURL{%
\tempurl}
\newblock
\shownote{Discusses various prompt engineering methods for different NLP tasks}.


\bibitem[Wei et~al\mbox{.}(2022)]%
        {Wei22}
\bibfield{author}{\bibinfo{person}{Jason Wei}, \bibinfo{person}{Maarten Bosma}, \bibinfo{person}{Vincent~Y. Zhao}, \bibinfo{person}{Kelvin Guu}, \bibinfo{person}{Adams~Wei Yu}, \bibinfo{person}{Brian Lester}, \bibinfo{person}{Nan Du}, \bibinfo{person}{Andrew~M. Dai}, {and} \bibinfo{person}{Quoc~V. Le}.} \bibinfo{year}{2022}\natexlab{}.
\newblock \showarticletitle{Chain of Thought Prompting Elicits Reasoning in Large Language Models}.
\newblock \bibinfo{journal}{\emph{arXiv preprint arXiv:2201.11903}} (\bibinfo{year}{2022}).
\newblock
\urldef\tempurl%
\url{https://arxiv.org/abs/2201.11903}
\showURL{%
\tempurl}
\newblock
\shownote{Demonstrates how prompt engineering can elicit domain knowledge}.


\end{thebibliography}

\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.