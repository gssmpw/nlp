\subsection{RQ2: Transformation Technology in REDAST}

Transformation technology, as discussed in Section~\ref{sec:Taxonomy}, is a key component of the REDAST process. After reviewing the selected papers, we present the results of transformation technology in four categories: technique type, framework structure, intermediate representation, and additional input. Finally, to better explain our design of these categories, we provide several examples to interpret our analysis process of RQ2.


\subsubsection{Transformation Techniques}
\input{tables/rq2_type_transformation}

This section maps the selected studies into high-level categories based on their adopted transformation techniques. However, we found that the transformation techniques vary widely, and it is hard to initialize one feasible category schema to cover all the selected studies due to the large number of papers. Thus, we classified the 156 papers into five categories, including rule-based, meta-model based, NLP pipeline based, graph/search-based, and ML-based techniques, as we explain later in this subsection. The results are illustrated in Table \ref{table:trans}. We adopted high-level categories, however, several studies still overlap in terms of their transformation techniques. For example, P1~\citeP{P1}, P3~\citeP{P3}, and P4~\citeP{P4} both cover the model-based and rule-based techniques. From the results, we found rule-based techniques are the most commonly adopted in REDAST studies (122 studies), followed by metamodel-based techniques (102 studies), NLP-pipeline-based techniques (51 studies), search/graph-based techniques (38 studies), and ML-based techniques (11 studies). Below we explain these categories with examples.

\input{tables/rq2_rule}
\sectopic{Rule-based}
techniques can be broadly classified into three categories: condition, rule, and template, based on our findings. Specifically, 
\begin{itemize}
    \item \textit{Template-based techniques} formulate the generated artifacts into predefined templates for further processing.
    \item \textit{Constraint-based techniques} indicate the transforming process for new artifact generation, where the rule here refers to the transformation rules.
    \item \textit{Condition-based techniques} are defined for the static regulating within existing generated artifacts based on predefined conditions or forms.
\end{itemize}
For example, P95~\citeP{P99} is a template-based study that introduces a behavior test pattern (template) from requirements transformation. Thus, we categorize this study into template-based techniques. In another example in P114~\citeP{P118}, this study generally applies static assertion analysis on requirements properties. We categorize this study as condition-based because it doesn't involve a transformation process. We present the results of the rule-based technique in Table~\ref{table:rule}.

\input{tables/rq2_model}
\sectopic{Meta-Model-based}
technologies describe system attributes, user behavior, or event situations in RE. REDAST studies employ metamodels in the transformation process to describe the system information inclusively, which provides a more comprehensive view of test generation. In the selected studies, we introduce nine categories, and the ``other'' category classifies metamodel-based techniques. Here, we illustrate the distribution of the metamodel-based technique in Table~\ref{table:model}.

\input{tables/rq2_nlp}
\sectopic{NLP-Pipeline-based} techniques generally employ open-source NLP toolkits, such as NLTK and Stanford coreNLP toolkits, and so on. We observed that POS Tagging (e.g., P22~\citeP{P22}, P70~\citeP{P70}, P128~\citeP{P133}, etc.), Dependency Parsing (e.g., P41~\citeP{P41}, P112~\citeP{P116}, P125~\citeP{P130}, etc.), and Tokenization (e.g., P18~\citeP{P18}, P82~\citeP{P83}, P155~\citeP{P160}, etc.) are frequently adopted in REDAST studies. We illustrate the details of the NLP-Pipeline-based techniques in Table~\ref{table:NLP}.

\input{tables/rq2_search}
\sectopic{Graph-based} techniques use graphs or diagrams, focused on describing the system behavior. However, the transformation among the other specifications and diagrams is challenging. The existing REDAST studies introduce the search within the graph-based technique to bridge the gap between sequential description and the specification diagram by finding a route or path. For example, in P50~\citeP{P50}, the depth-first traversal algorithm is employed to find the test path. Our findings show that even some traditional graph traversal techniques are still effective in test generation, such as Breadth-First Traversal (BFT), Depth-First Traversal (DFT), etc.

\input{tables/rq2_machinelearning}
\sectopic{Machine Learning based} techniques were not as prevalent as others in REDAST studies. 11 studies opted for ML techniques, including Pretained LMs (e.g., BERT in P23~\citeP{P23}, P94~\citeP{P98}, P123~\citeP{P128}), traditional machine learning algorithms (e.g., P154~\citeP{P159}), SyntaxNet (e.g., P90~\citeP{P93}, P121~\citeP{P126}), and so on, which didn't reflect any trend in the technique adoption.

\subsubsection{Framework Details}
In this subsection, we cover the remaining parts of the transformation technology, i.e., additional output, intermediate representation and the framework structure.
\input{tables/rq2_additional}


\sectopic{Additional Input.}
While our SLR studies requirements-driven automated test generation, the inputs are not constrained to the requirements specification only. We found that some studies introduce additional docs as input to improve the functional coverage of their methodologies. This section illustrates the details of additional input in REDAST studies, including:
\begin{itemize}
    \item \emph{Historical Documents} are always referenced in the generation process. For example, P2~\citeP{P2} opted for historical test logs as the additional input in the test generation step. The test logs can serve as a reference for evaluating the generated test artifacts.
    \item \emph{System Implementation} is the next stage after requirements engineering based on the SDLC, where the requirements are believed to dominate the implementation process and vice versa significantly. In P109~\citeP{P113}, the implementation documents are used in the analysis to provide evidence from the actual scenario.
    \item \emph{Source Code} can also be used as an additional input alongside requirements in the transformation process. P154~\citeP{P159} introduces code updating information in the test prioritization process, which is used to find the error-prone modules.
    \item \emph{Additional Documents}, such as ground knowledge documents, are used to support test generation with a more substantial knowledge base. P122~\citeP{P127} is a KnowledgeGraph-based method, where the ground knowledge is largely integrated for constructing the knowledge graph.
\end{itemize}

\input{tables/rq2_framework}

\sectopic{Framework Structure.} 
From requirements specification to the generation of test artifacts, various designs can be applied to transformation methodologies. Within the selected REDAST framework, we categorize these methodologies into four distinct structure types:
\begin{itemize}
    \item \emph{Sequential Structure.} The sequential framework conducts transformations in a strict, ordered sequence without any bypasses or shortcuts. This approach enhances logical continuity and maintains a clear connection between each step. P107~\citeP{P111} is a typical sequential framework where the formal specifications are step-by-step transformed into conjunctive normal form, assignment, and test cases.
    \item \emph{Conditional Structure.} This framework introduces conditional steps, allowing for alternative paths at key stages. This flexibility improves adaptability and generalization, enabling the framework to manage diverse scenarios effectively. P73~\citeP{P73} is a good example in this category, where this study constructs several conditions in transformation, e.g., ``Need more details of requirements'', ``There are improvements of transformation'', etc.
    \item \emph{Parallel Structure.} In a parallel framework, different representations can be processed simultaneously across multiple transformation processors. This structure significantly boosts time efficiency. The typical parallel structure can be found in P10~\citeP{P10}. This study constructs a two-way structure and converts requirements input to use case diagrams and executable contracts for generating contractual use case scenarios.
    \item \emph{Loop structure.} The loop structure incorporates assertion-controlled loops, enabling iterative refinement of generated artifacts. Cycling through iterations ensures higher quality in the final outputs. For example, P109~\citeP{P113} introduces the loop structure by designing a validation and tuning process to refine the guarded assertions in this method iteratively.
\end{itemize}

\subsubsection{Intermediate Representation}

The intermediate representation functions as a detailed explanation of requirements or system structure, reflecting the framework structure's complexity. Specifically, intermediate representation is the step-generated artifacts during the transformation process. When reviewing the selected studies, the intermediate representation necessarily exists in a complex REDAST framework to enable a stepwise transformation. Thus, we illustrate the details of the adopted intermediate representations to understand the framework's composition better.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig//rq2/RQ3_framework.pdf}
    \caption{Illustration of Joint Distribution of Intermediate Representation and Framework Structure (RQ2)}
    \label{fig:framework_bar}
\end{figure}

\sectopic{Number of Intermediate Representations.}
The number of intermediate representations is a basic feature of REDAST's framework. In order to improve its explainability, we jointly illustrate the number of intermediate representations with framework structure in Fig.~\ref{fig:framework_bar}. The results show different trends in ``single'' and ``multiple'' categories, where (1) sequential structure (66 studies) is the dominantly common framework in the "single" category, followed by conditional structure (8 studies), parallel structure (4 studies), and loop structure (2 studies), (2) parallel structure is the most common framework in "multiple" category (18 studies), where sequential is also popular (14 studies).



\input{tables/rq2_intermediate_type}

\sectopic{Type of Intermediate Representations.} Considering the intermediate representation generally extended from requirements, we also introduce a similar category used in the requirements specifications, which is illustrated in Table~\ref{table:intermediate_representation}. The type category includes:
\begin{itemize}
    \item \emph{Rule-based Representation} (51 studies): The rule-based representation here refers to general controlled NL, assertion, or equation, where these notations generally consist of descriptions with predefined conditions or forms.
    \item \emph{MetaModel-based Representation} (48 studies): MetaModel has widely opted for intermediate representation, where model attributes offer additional explainability for test transformation.
    
    \item \emph{Graph-based Representation} (38 studies): Graph is an advanced representation method that reflects basic information and indicates the co-relations among different elements.
    \item \emph{Test-Specification-based} (5 studies): Some studies introduce test-related intermediate representation but cannot classify it into parts of the test outcome. Thus, the test-specification-based representation is especially considered a category. We didn't identify too many test-specification-based representations in selected studies. 
\end{itemize}

\subsubsection{Findings: Trend of Transformation Techniques Over the Years}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{fig//rq2/RQ2_trans_year.pdf}
    \caption{Trend of Transformation Techniques by Years}
    \label{fig:tec_trend}
\end{figure}
The transformation techniques in REDAST have largely been influenced by advancements in other fields, such as machine learning and deep learning. To illustrate the evolution of these techniques, we integrate publication year data with RQ2 to analyze trends in REDAST studies in Fig.~\ref{fig:tec_trend}. Our findings indicate that rule-based, graph-based, and metamodel-based techniques initially dominated transformation approaches. However, following the introduction of the NLP pipeline in 2004 and machine learning techniques in 2012, graph-based approaches gradually declined in popularity. Additionally, we observed an increasing diversity in transformation techniques over time, reflecting a broader range of methodologies being adopted in REDAST.

\subsubsection{Findings: Cross-Analysis of Requirements Input and Transformation Techniques}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{fig/rq2/RQ2_trans_req.pdf}
    \caption{Cross-Distribution of Requirements Input and Transformation Techniques}
    \label{fig:trans_req}
\end{figure}

During the transformation process, the requirements input is further processed to the intermediate representations or end test artifacts. Thus, we cross-discuss the relations between requirements input and transformation techniques, which are depicted in Fig.~\ref{fig:trans_req}. The results are not surprising that (1) rule-based techniques are still commonly opted for parsing different requirements input, 36\%, 39\%, 40\%, 39\%, and 33\%, respectively, in textual, model-based, formal, constraint-based, and tabular categories, (2) metamodel-based transformation techniques are not only applicable for model-based requirements but also for the other requirements expressions, 27\%, 41\%, 50\%, 47\%, and 33\%, respectively, in textual, model-based, formal, constraint-based, and tabular categories, which suggests that meta-models are flexible and applicable to both textual and structured requirement representations. As for NLP-pipeline-based approaches, they are almost exclusively applied to textual requirements. Surprisingly, we found that, in the model-based categories, there are still five papers that introduce NLP-Pipeline-based techniques in their methodologies. P28~\citeP{P28} introduced a sequence-diagram-based use case, which both enables the flexibility of textual requirements and the structural ability of model-based requirements, where the NLP-pipeline is introduced to parse the CNL or structured expression. Similarly, the NLP pipeline in P57~\citeP{P57} parsing the dependency in the introduced UML-based use case. Thus, the selection bias of NLP pipelines can be avoided by combining textual requirements with the other requirements specifications, which can also additionally provide flexibility from textual requirements for the method.

\begin{tcolorbox}[mybox, title=RQ2 Key Takeaways]
$\bullet$ Rule-based and model-based approaches dominate REDAST due to their structured, interpretable nature and low dependency on training data. While AI techniques—particularly NLP—are increasingly incorporated for automation, they primarily support rule-based and model-driven frameworks rather than serving as standalone transformation methods.

$\bullet$ Requirements are generally considered sufficient input for REDAST, as few studies integrate additional data sources (e.g., system implementation, source code, or historical test logs). Instead, intermediate representations serve as essential enablers for translating requirements into structured test artifacts. A significant number of studies employ multi-step transformations, making intermediate representations a key bridge between raw requirements and test automation artifacts. 
\end{tcolorbox}

% \sectopic{Decomposition Granularity.} 
% \input{tables/rq2_granularity}
% This category is introduced to illustrate the granular, decomposed aspects of the intermediate representation. Specifically, the intermediate representation includes three granularity levels:
% \begin{itemize}
%     \item Composite Decomposition: We identified 26 studies that adopt composite representation in the transformation process. This level represents a sequence of combined elements, such as actions, behaviors, and other units, assembled to form a comprehensive representation. P16~\citeP{P16} is a typical study that employs composite representation, such as event flow and sequential diagrams, which reflect the complete procedure of system behaviors.
%     \item Hierarchical Decomposition: There are 59 studies involving hierarchical intermediate representation. This level provides a high-level description that can be broken down into more specific sub-components, allowing for deeper analysis. Here, we explain this category based on P155~\citeP{P156}. P155 opts for Software Cost Reduction as the intermediate representation, which enables an abstract description so that the test cases are generated by implementing SCR description.
%     \item Atomic Decomposition: As for intermediate representation in atomic decomposition, we found 61 studies under this category. Each atomic representation offers an independent description. When multiple atomic representations are combined, they create a thorough view of the objective. For example, P2~\citeP{P2} introduces CNL to specifically describe system behavior in integrated sentences, where the test cases are generalized by regarding each statement in CNL form.
% \end{itemize}