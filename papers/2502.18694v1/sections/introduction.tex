\section{Introduction}~\label{sec:Introduction}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/introduction/brief_intro.pdf}
    \caption{The General Framework of REDAST Methodology (The dotted route is not technically necessary.)}
    \label{fig:brief_intro}
\end{figure}
%% Generally describing the significance of system- and acceptance-testing
Software testing plays a critical role in assuring the quality, reliability, and performance of software systems~\cite{significance_se1,significance_se2}. At its core, software testing seeks to identify and address defects, ensure that a system functions as intended, meets stakeholder requirements, and mitigate risks before deployment~\cite{significance_se3}. Central to achieving these objectives is the alignment of testing activities with the software's requirements, as requirements define the system's intended behaviour and scope. Effective testing demands comprehensive test artifacts that trace back to these requirements, including test cases, test plans, and test scenarios. As software systems grow more complex, there is an increasing need for automated approaches to bridge the gap between requirements engineering (RE) and software testing~\cite{taxonomy2014unter}. This increasing complexity of software systems has amplified the importance of \emph{requirements-driven testing}â€”a paradigm that directly links testing activities to the software requirements.



%Over time, the size of the software system has become more complex and larger, which requires a more efficient software testing method. However, the manual design and generation of test artifacts is not only a time- and resource-costing task but also reflects inconsistency in test quality~\cite{manual_effort_in_testing_01}. Automated software testing methods, under urgent demand in the software field, have emerged in recent years, which are expected to improve the efficacy of software quality assurance~\cite{auto_test_gen_01,auto_test_gen_02}.

%The automated software testing methods enable different configurations based on various test objectives. Test code generation is a popular type of automated software testing, such as Baione et al.,~\cite{code_based_test_01} and Thummalapenta et al.,~\cite{code_based_test_02}, etc., which have been long investigated in previous work. They originate from functional design or specifications and are expected to ensure components work as intended or verify combined functionality ~\cite{unit_testing_01,automated_testing_01}. Compared with test code generation, requirements-driven automated software testing shows different test objectives and strategies~\cite{difference_acceptance_unit_01}. Although requirements are a crucial initial step in SDLC and should strongly correlate with software testing~\cite{requirements_survey_01,acceptance_testing_01}, there are only a limited number of studies that systematically analyze related approaches and explore their future directions. Thus, in order to enlighten the field of software testing, we conducted this study to systematically analyze requirements-driven automated software testing and provide a guideline for future researchers in related fields.

In the requirements-driven automated software testing paradigm, the test generation process is mainly motivated by requirements, where the requirements are further interpreted, transformed, and implemented to test artifacts with the help of some additional supporting documents or tools~\cite{mustafa2021automated,align_req_test_01}. Traditionally, this process has been predominantly manual, relying on human effort to interpret requirements and design corresponding tests~\cite{automated_test_ad_01,automated_test_ad_02}. However, manual approaches are time-consuming, prone to human error, and struggle to scale with the increasing size and complexity of modern software systems. To address these limitations, several research papers have explored automated methods for generating test artifacts directly from requirements. These methods leverage model representations, natural language processing (NLP), and artificial intelligence (AI) to translate requirements into actionable testing artifacts. However, a systematic understanding of the approaches, their challenges, and potential opportunities in the area of requirements-driven testing remains limited.

In this paper, we study the \textbf{RE}quirements-\textbf{D}riven \textbf{A}utomated \textbf{S}oftware \textbf{T}esting (REDAST) landscape. Fig.~\ref{fig:brief_intro} shows an overview of the REDAST process. The generation process consists of several components, including requirements as the necessary input, additional documents as the potential input, intermediate expressions as the potential step, transformation techniques as the main applied methodologies, and generated test artifacts as the final outcome. We cover the REDAST research landscape by (1) summarizing and reporting the statistics of valuable research works of automated software test generation, (2) comparing the evaluation methods for the current studies, and (3) identifying and analyzing current limitations and future opportunities of automated test generation technology in the context of the current era. We not only focus on the previous research under the traditional software engineering perspective but also introduce a new view from advance technologies to discuss the prospects and possibilities for REDAST studies. We followed the empirical SLR guidelines of Kitchenham et al.~\cite{kitchenham2022segress} in performing our systematic review to answer the following research questions (RQs) on selected 156 papers~\citeP{P1,P2,P3,P4,P5,P6,P7,P8,P9,P10,P11,P12,P13,P14,P15,P16,P17,P18,P19,P20,P21,P22,P23,P24,P25,P26,P27,P28,P29,P30,P31,P32,P33,P34,P35,P36,P37,P38,P39,P40,P41,P42,P43,P44,P45,P46,P47,P48,P49,P50,P51,P52,P53,P54,P55,P56,P57,P58,P59,P60,P61,P62,P63,P64,P65,P66,P67,P68,P69,P70,P71,P72,P73,P74,P75,P76,P77,P78,P79,P80,P82,P83,P84,P86,P88,P89,P90,P91,P92,P93,P94,P95,P96,P98,P99,P100,P101,P102,P103,P104,P105,P106,P107,P108,P109,P110,P111,P112,P113,P114,P115,P116,P117,P118,P119,P121,P122,P123,P124,P125,P126,P127,P128,P129,P130,P131,P132,P133,P134,P135,P136,P137,P138,P139,P140,P141,P142,P143,P144,P145,P146,P147,P148,P149,P150,P151,P152,P153,P154,P155,P156,P157,P158,P159,P160,P161}:

\begin{itemize}
    \item RQ1. What are the input configurations, formats, and notations used in the requirements in requirements-driven automated software testing?
    \item RQ2. What are the frameworks, tools, processing methods, and transformation techniques used in requirements-driven automated software testing studies?
    \item RQ3. What are the test formats and coverage criteria used in the requirements-driven automated software testing process?
    \item RQ4. How do existing studies evaluate the generated test artifacts in the requirements-driven automated software testing process?
    \item RQ5. What are the limitations and challenges of existing requirements-driven automated software testing methods in the current era?
\end{itemize}


\sectopic{Structure.} Section~\ref{sec:RelatedWork} discusses the background concepts and related work for REDAST.
Section~\ref{sec:Methodology} presents our methodology and process of conducting our systematic review.
Section~\ref{sec:Taxonomy} describes the taxonomy behind REDAST process.
Section~\ref{sec:Results} discusses results from our five RQs.
Section~\ref{sec:Threats} examines threats to validity of our study.
Section~\ref{sec:Discussion} discusses the insights from our results and the REDAST research roadmap.
Section~\ref{sec:Conclusion} concludes the paper.