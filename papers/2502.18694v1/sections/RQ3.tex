\subsection{RQ3: Generated Test Artifacts in REDAST}
 
 RQ3 aims to discuss the final outcomes of the REDAST process, where we studied the test abstraction level, test type, test notation, and test coverage.

\subsubsection{Test Abstraction Level}

\input{tables/rq3_abstraction}
Varying on different usage scenarios, the test artifacts could be described at various levels. For example, when we plan to verify user behaviors, the abstract test scenario is one of the best choices for engineers to check the correctness of each step. Thus, we introduce three categories for test abstraction level classification, including executable, abstract, and report. The idea of each category is described as:

\begin{itemize}
    \item \textit{Executable Level} includes artifacts, such as code and scripts, that can be directly executed. Typically, the test artifact in P94~\citeP{P98} is on the executable level, where the test case consists of several sections, including ``Target Entities'', ``Test Intent'', ``Extracted Triplets'', ``Context Sub-graph'', ``Test Case'', and so on. %This test case involves detailed descriptions in each section and provides concrete content for software testing.
    \item \textit{Abstract Level} includes the artifacts that cannot be directly executed, e.g., textual scenarios, test diagrams, etc. We identify P71~\citeP{P71} as an example of this category. P71 generates test plans consisting of activities and acceptance criteria, where the acceptance criteria in this study don't give specific operation or system behavior, e.g., ``Is bill paid?'', ``Is ID card valid?'' or some similar statements.
    \item \textit{Report Level} mainly refers to the results after executing test artifacts. We separately present this because some studies also provide the executing tool. In P29~\citeP{P29}, this study introduces an automatic tester using generated test data. The final output is the corresponding test report from the tester.
\end{itemize}


\subsubsection{Test Formats}

In this section, we present detailed information about the generated test artifacts, adopting the categories of test type and notation to describe them in detail. While the most commonly used test types are test cases and scenarios, we also found some task-specific test artifacts. The results of the test type are illustrated in Table~\ref{table:test_formats}.

\input{tables/rq3_test_format}
We first illustrate the categorization results of test formats. In this section, the test format generally refers to the higher dimension of generated test artifacts. We will not use the exact notation in this part. The detailed results are in Table \ref{table:test_formats}, which based on the following classes:
\begin{itemize}
    \item \textit{Test Case, Test Scenario, and their Variations} are the most commonly derived test formats in REDAST papers. Test scenarios enable a high-level description of the test objectives from a comprehensive point of view. Test cases include more detailed, stepwise instructions or definitions by focusing on a specific software part.
    \item \textit{Test Requirements, Guidance, Plan, Suggestion, and Acceptance Criteria}. These test artifacts generally offer high-level, abstracted, or constructive objectives and suggestions for software testing. Rather than specifically match every step in software testing, they enable high-level instruction for test structure.
    \item \textit{Test Suite, Script, and Oracle}, compared with the other formats, are advanced in applicability and usability. Generally, assertion, code, or any executable source are used in these formats, which are designed for execution.
    \item \textit{Test Sequence, Model and Goal}. These test formats are designed to meet the specific test objectives in the structural testing process. The model or sequence in these formats enables better traceability compared with the other formats.
\end{itemize}

In the results, we can find that the test case is the most adopted test format in the selected studies; test scenarios and test reports are the second most studied categories, followed by ``other'' formats, test reports, test models, and so on. However, we found that we identified some test artifacts that are hard to automate, such as test oracles, test goals, and traceability matrix. As the typical test artifacts, test oracle is well-known for its difficulty in determining the correctness based on the given input and the complexity of software systems~\cite{test_oracle_01, test_oracle_02, test_oracle_03}, which is believed that the human invention is still needed~\cite{test_oracle_04}. In REDAST studies, the automation of the test oracle is realized with the help of precise requirements specification, wherein the two papers under the test oracle category, (1) P7~\citeP{P7} ensure the correctness of the test oracle by preliminarily checking the correctness of requirements, then from the derived test cases to formulate the test oracles, (2) P63~\citeP{P63}, by converting RUCM (Restricted Use Case Model) to OCL expression, the given requirements can be details checked for the generation of test oracle. This suggests that, by incorporating appropriate requirements specifications in the REDAST process, correctness and completeness can be assured. Furthermore, the preliminary correctness checking can reduce the effort to post-check the correctness of test artifacts.

% - test oracle is quite hard - why is it hard to automate (cite a survey )
% https://ieeexplore.ieee.org/abstract/document/6963470

% - usuallly in testing we use code coverage, but when it comes to requirements driven, the adequacy critieria si requreiemnts coverage. 

% - what is the difference between requirements coverage and use case cover
% - what is the difference between requirements and 

% - why do those papers focus on code coverage, and what other things do they focus on
% - what are the overlaps between the different coverage criteria? 

% SO WHAT? 


% \paragraph{Test Notations}
% \input{tables/rq3_test_notation}
% The results of test notations are covered in this section. We noticed that many studies didn't specify the exact notation of their test artifacts. Thus, we use our assumption and knowledge to categorize them in Table~\ref{table:test_notation}, where the notation is specified after each paper ID.

\subsubsection{Test Coverage Methods}
\input{tables/rq3_coverage}
Test coverage is defined as whether our test cases cover the testing objectives~\cite{zhu1997software}. However, there are various methods available for this purpose. We design our categorization method for the coverage types in the selected studies and finalize our results in Table \ref{table:coverage}.

We found that requirements coverage is the most commonly used method in REDAST studies, followed by Behavioral/Scenario Coverage, Path Coverage, Functional Coverage, and so on. From the general point of view, code coverage is the most commonly adopted coverage method in automated test generation. The code coverage can simply and effectively assess the quality of generated test artifacts and measure which code are being covered in the generation~\cite{code_coverage_01, code_coverage_02}. However, the requirements coverage is leading the trend of test coverage in REDAST, where, moreover, the behavioral/scenario, use case, statement, and decision/branch coverage are also able to be classified into requirements coverage. We believe that the requirements can serve a similar role to code. By referring to different parts of requirements, such as behavior, path, or statement, the test artifacts are also able to be assessed and measured on the requirement level.
% \subsubsection{Code Involvement}

\subsubsection{Findings: Trend of Test Abstraction Level Over the Years}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig//rq3/RQ3_abstraction_year.pdf}
    \caption{Trend of Test Abstraction Level by Years}
    \label{fig:abstraction_year}
\end{figure}

Test artifacts are the closest step in REDAST, where the generated test artifacts are the end outcomes of the framework. Reviewing the results, we can notice that REDAST studies mainly maintain their generated test artifacts on the executable level. To better see the trend, we present the trend of abstraction level by year in Fig.~\ref{fig:abstraction_year}. Even if abstract and executable test artifacts are both useful for software engineering, the results show that executable test artifacts are now becoming more and more popular after 2007, which means executable test artifacts are more applicable and capable for recent tasks and usage scenarios.

\subsubsection{Findings: Cross-Analysis of Transformation Techniques and Test Artifacts}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig//rq3/RQ3_trans_test.pdf}
    \caption{Cross-Distribution of Transformation Techniques and Test Artifacts}
    \label{fig:trans_test}
\end{figure}
Since test artifacts are the direct outcomes of transformation techniques, we aim to explore the connection between these artifacts and the transformation methods used to generate them. Fig.~\ref{fig:trans_req} presents the cross-distribution of transformation techniques and test formats. Our findings indicate that rule-based, metamodel-based, and NLP-pipeline-based methods are widely adopted across various types of test artifacts. Specifically, (1) Rule-based and metamodel-based techniques are the most versatile and commonly applied across different test outputs; (2) NLP-pipeline-based and graph-based methods are used selectively, with NLP techniques being more influential in generating test cases and test scenarios; (3) Machine learning-based methods are the least utilized, suggesting that machine learning is still relatively underexplored in REDAST studies.

\subsubsection{Findings: Cross-Analysis of Requirements Input and Test Artifacts}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig//rq3/RQ3_require_test.pdf}
    \caption{Cross-Distribution of Requirements Input and Test Artifacts}
    \label{fig:require_test}
\end{figure}

Similarly, we illustrate the stack chart in Fig.~\ref{fig:require_test} of requirements input and test outcomes. From the chart, we can find textual requirements are still leading the landscape in all formats for test outcomes, which is also strong proof of the flexibility of textual requirements. The test oracle, sequence, goal, and suite, which are widely believed to be advanced test strategies, we noticed that formal, constraint-based, tabular, and model-based requirements could support the generation of these specialized test artifacts.


\begin{tcolorbox}[mybox, breakable, title=RQ3 Key Takeaways]
$\bullet$ Executable test artifacts (e.g., scripts, test suites, assertions) are the dominant output in REDAST studies. 

$\bullet$ A large majority of the studies focus on concrete test cases as the outcome, followed by test scenarios. Harder-to-automate test artifacts (e.g., test oracles, traceability matrices) remain an open challenge, requiring structured requirements modeling. Traceability from requirements to test cases is considered challenging and a largely unresolved issue.

$\bullet$ Requirements coverage is the leading coverage criterion in REDAST, encompassing functional and behavioral coverage, followed by path coverage.

\end{tcolorbox}

% \subsubsection{Findings}
% \textit{Most test artifacts originate from textual and model-based requirements.} We observe that, apart from textual requirements found in the test suite and sequence and model-based models in the test graph and report, these two types of requirements can be transformed into various formats of test artifacts.





