\section{Introduction}
\begin{figure}[t]
\centering
    \includegraphics[width=\linewidth]{figures/teaser1.pdf}
\caption{Visualization of the image-level AUROC comparison between our Anomaly-OV and current state-of-the-art ZSAD methods (WinCLIP \cite{jeong2023winclip}, AnoVL \cite{deng2023anovl}, AnomalyCLIP \cite{zhou2024anomalyclip}, AdaCLIP \cite{cao2025adaclip}). Notably, our zero-shot performance on VisA even surpasses most recent advances in the few-shot setting \cite{li2024promptad, zhu2024toward, gu2024anomalygpt}.}
\label{fig:teaser}
 \vspace{-1mm}
\end{figure}

Visual Anomaly Detection (AD) is a well-established task in computer vision, extensively applied in scenarios such as industrial defect inspection \cite{mvtec, xie2023pushing, roth2022towards, huang2022registration, mou2023rgi, chen2022deep, bergmann2020uninformed, reiss2023mean, you2022a, cao2023anomaly} and medical image diagnosis \cite{wolleb2022diffusion, bmad, han2021madgan, huang2024adapting, zhang2024mediclip, wei2018anomaly, fernando2021deep, zhao2021anomaly}. In the traditional unsupervised AD (a.k.a. one-class AD) setting,  models learn the distribution of normal visual features from normal samples and are required to identify anomaly samples during inference. While recent advancements \cite{isaac2024towards, strater2024generalad, chen2024unified, tang2025incremental, he2024learning, yao2024glad, zhang2024realnet, lee2024text, yao2024hierarchical, ho2024long, fuvcka2025transfusion, hou2021divide} have significantly improved the detection performance, these approaches assume the availability of a substantial number of normal samples. However, this assumption becomes impractical in certain scenarios due to strict data privacy policies and the significant human effort required for data classification, sometimes involving experts or specialists. Therefore, Zero-Shot Anomaly Detection (ZSAD) is emerging as a popular research direction, leading to the development of many innovative methods \cite{jeong2023winclip, zhou2024anomalyclip, cao2025adaclip, li2024zero, li2024promptad2, gu2024filo, sato2023prompt, deng2024simclip, schwartz2024maeday, zhu2024llms}. 

% SY's edit, P2
Recent advances in Multimodal Large Language Models (MLLMs) \cite{blip, zhu2024minigpt, instructblip, chen2024spatialvlm, llava, llava15, llava_interleave, llavaonevision, llavamed} have shown revolutionary reasoning capabilities in various vision tasks \cite{lv2024video, yang2024follow, cheng2024emotion, guo2024stimuvar, zhou2024vicor, sermanet2024robovqa, xie2025funqa, zhou2024navgpt, nie2025reason2drive}. However, the reasoning of image abnormalities has not been explored due to the challenges of collecting large-scale datasets and establishing benchmarks. Existing methods simply predict the likelihood of an anomaly without providing rationales \cite{jeong2023winclip, zhou2024anomalyclip, cao2025adaclip, deng2023anovl, april-gan}. In contrast, for better interpretability, robustness, and trustworthiness, people would expect models to explain \textbf{why an image is considered anomalous} and provide visual evidence. Interestingly, we find that recent advanced MLLMs, such as GPT-4o \cite{gpt-api-4o}, fall short in AD \& reasoning. As shown in Figure \ref{fig:teaser2}, while the detection is correct, the explanation from GPT-4o lacks accuracy, indicating a gap in a comprehensive understanding of the anomaly.

% SY's edit, P3
To expedite research in AD \& reasoning, we establish the first visual instruction tuning dataset, Anomaly-Instruct-125k, and the evaluation benchmark, VisA-D\&R, through intensive human efforts. After evaluating current generalist MLLMs, we observe that these models fail to accurately detect and describe fine-grained anomalous details in images. To address this, we propose Anomaly-OneVision (Anomaly-OV), the first specialist visual assistant for ZSAD and reasoning. Unlike existing ZSAD methods \cite{jeong2023winclip, zhou2024anomalyclip, cao2025adaclip, deng2023anovl, april-gan}, Anomaly-OV directly learns object-awareness abnormality embeddings in feature space using only the visual encoder. Inspired by human behavior in visual inspection, Anomaly-OV employs a Look-Twice Feature Matching (LTFM) mechanism to assist its LLM in adaptively selecting and emphasizing the most suspicious abnormal visual tokens.

% SY's edit, P4
Extensive experiments demonstrate that Anomaly-OV achieves significant improvements over advanced generalist models in both detection and reasoning. Extended results of Anomaly-OV, from applications in industrial defect detection to 3D inspection and medical image diagnosis, are provided for future study. With precise descriptions and rationales of visual anomalies, our model can infer potential causes (see Figure \ref{fig:teaser2}), assess current impacts, and offer improvement suggestions, positioning itself as a reliable assistant for visual inspection. Our contributions are in two folds:
\begin{itemize}
	\item We establish the first visual instruction tuning dataset and benchmark for anomaly detection and reasoning.
	\item We propose the first specialist visual assistant with state-of-the-art performance for this new impactful domain.
\end{itemize}

\begin{figure}[t]
\centering
    \includegraphics[width=\linewidth]{figures/anomaly_teaser2.pdf}
\caption{Industrial image anomaly reasoning results from GPT-4o \cite{gpt-api-4o} and our Anomaly-OV. The responses for fine-grained anomaly reasoning are highlighted, with the ground truth given for reference.}
\label{fig:teaser2}
 \vspace{-2mm}
\end{figure}
