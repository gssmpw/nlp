
\section{Dataset and Benchmark}
The lack of multimodal instruction-following data for image anomaly detection and reasoning hinders the development of special intelligent assistants in this domain. Even though AnomalyGPT \cite{gu2024anomalygpt} introduces a prompt tuning dataset by simulating the anomalies, the scale of their dataset and the diversity of their instructions and answers are limited, only focusing on anomaly localization. To resolve the data scarcity issue, we establish the first large-scale instruction tuning dataset: \textbf{Anomaly-Instruct-125k} and the corresponding anomaly detection and reasoning benchmark: \textbf{VisA-D\&R}.

\subsection{Anomaly-Instruct-125k}
LLaVA \cite{llava} builds its instruction tuning dataset by leveraging the image caption and bounding boxes available in the COCO dataset\cite{coco} to prompt the text-only GPT-4. ShareGPT4V \cite{sharegpt4v} provides a higher-quality dataset by directly prompting GPT-4V \cite{gpt-api-4vision}. However, there is no image caption provided in existing anomaly detection datasets \cite{mvtec, bmad}, and no matter GPT-4V \cite{gpt-api-4vision} or most recent GPT-4o  \cite{gpt-api-4o} cannot accurately locate and describe the anomalies in the image without explicit human involvement. 

To resolve these issues, we design a new prompt pipeline for accurate anomaly description generation. Since most of the datasets contain annotations for anomaly types, we manually combine the class name and anomaly type, such as \texttt{a [capsule] with [poke] on surface}. If the anomaly masks are provided, we draw bounding boxes on the images to highlight the anomalous area. The short description and the image with (or w/o) bounding boxes are used to prompt GPT-4o to generate the detailed image and anomaly descriptions. Then, we employ an in-context learning strategy similar to LLaVA to create the instructions.

For a unified visual inspection dataset, precise instruction data is collected from MVTec AD \cite{mvtec}, the training set of BMAD \cite{bmad}, Anomaly-ShapeNet \cite{anomaly_shapenet}, Real3D-AD \cite{real3d}, and MVTec-3D AD \cite{mvtec3d}, covering both 2D to 3D data across industry to medical domains. The 3D point cloud data are converted into 9 multi-view images, and the corresponding masks are rendered using predefined camera positions. However, the diversities and scales of these datasets are relatively limited, probably due to the difficulty of collecting anomaly images. To scale up the instruction data, we introduce an automatic anomaly data collection pipeline combining GPT-4o \cite{gpt-api-4o} and Google Image Search \cite{google-image-search} for image collection, data cleaning, and instruction generation. Finally, 72k in-the-wild images (named as WebAD) targeting anomaly detection are collected, significantly enriching our instruction dataset. Several samples from Anomaly-Instruct-125k are shown in Figure \ref{fig:daatset}. The instructions are mainly in the format of multi-round conversations, covering anomaly detection and description in low-level reasoning and potential cause and future suggestions for complex understanding. 

\subsection{VisA-D\&R}
VisA \cite{visa} is a classic but challenging industrial anomaly detection dataset, providing fine-grained anomaly type and segmentation for each image. For evaluation of the anomaly detection and reasoning performance on existing and future methods, we select 10 classes from VisA and follow a similar data generation pipeline of Anomaly-Instruct-125k to create the benchmark. Differently, significant human effort has been invested in meticulously reviewing all generated images and anomaly descriptions. Wrong descriptions are picked out and re-annotated by humans before utilizing them for Q\&A generation. Totally, the benchmark consists of 761 normal samples and 1000 anomalous ones. 

\begin{figure}[t]
\centering
    \includegraphics[width=\linewidth]{figures/bench.png}
\caption{Prompt examples in VisA-D\&R for detection and reasoning. The complex reasoning instructions are highlighted.} 
\label{fig:bench}
\vspace{-4mm}
\end{figure}

For evaluating detection performance, questions designed to elicit a one-word answer are used to prompt the MLLMs (Figure \ref{fig:bench}), with results quantified using Accuracy, Precision, Recall, and F1-score. We divide the reasoning performance into two parts: low-level reasoning that focuses on the description of visual defects or anomalies and complex reasoning requiring the MLLMs to provide the potential cause and future improvement strategies for the detected anomalies, where ROUGE-L \cite{rouge}, Sentence-BERT (SBERT) \cite{sbert}, and GPT-Score (GPT-4 as the judge \cite{llava}) are utilized to quantify the similarity between generated text and ground truth. Note that low-level reasoning is highly correlated to detection performance, while anomaly-type descriptions of low-level reasoning determine the output of complex reasoning.


