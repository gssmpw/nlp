\clearpage
%\setcounter{page}{1}
%\maketitlesupplementary
\setcounter{section}{0}
\renewcommand\thesection{A\arabic{section}}
%\part*{A. Appendix}

\section{Dataset Establishment}
\label{sup_dataset}

\subsection{How to highlight the anomaly?}
\input{tables/table_sup1}
\noindent
As shown in Table \ref{tab:sup1}, recent advanced MLLMs like GPT-4o fail to detect the anomalies in the image, so building the instruction tuning dataset using previous methods \cite{sharegpt4v} is impractical. However, we observe that when the GPT-4o is provided some "hints", it presents impressive performance on anomaly reasoning or description. For example, a red bounding box drawn around the anomalous area enables GPT-4o to detect the tiny bubble inside the small capsule. This observation indicates that \textcolor{lightpink}{\textbf{the anomaly information is already contained in the visual tokens, and the failure of existing MLLMs is because the language model cannot effectively pick out the related tokens,}} which is the major inspiration of our token-picking mechanism.


Most of the existing AD datasets, such as MVTec AD \cite{mvtec}, contain anomaly masks for anomaly localization. Therefore, we leverage these masks to generate the bounding boxes on the images. Specifically, the masks for an anomalous image are dilated and merged (if two masks are too close) before calculating the coordinates of the bounding boxes. Similarly, the image with bounding boxes drawn on it will serve as the visual prompt for GPT-4o. We also tried many other ways to utilize the anomaly masks, such as highlighting the mask area with different colors, consecutively providing the image and mask, and converting the normalized coordinates of the bounding box into a text prompt. None of them can as effectively guide the GPT-4o in finding anomalous features as drawing bounding boxes on the image.




\begin{figure*}[!t]
\centering
    \includegraphics[width=\textwidth]{figures/data_collection.pdf}
\caption{Automatic data collection pipeline for WebAD. The entire pipeline is fully automatic at an affordable cost (API usage). Other advanced open-sourced MLLMs can applied to replace GPT-4o for further reduction of cost.}
\label{fig:data_collect}
\end{figure*}

\subsection{WebAD -- The largest AD dataset}
Existing industrial or medical anomaly detection datasets, such as MVTec AD \cite{mvtec} and BMAD \cite{bmad}, only contain a limited number of classes ($<20$) and several different anomaly types for each class (most of the anomaly types are similar) due to the collection of these kinds of anomaly images involves extensive human involvements. This limitation hinders the ZSAD model from learning a generic description of anomaly and normal patterns. Also, the MLLMs cannot obtain enough knowledge of visual anomaly descriptions for unseen anomaly types. Therefore, more diverse data is required for a robust ZSAD \& reasoning model. Many recent dataset works collect and annotate online images to enrich existing datasets and demonstrate their effectiveness in the training of current data-hungry deep learning models. 

To collect the online images that can be utilized for anomaly detection, we design an automatic data collection pipeline by combining GPT-4o \cite{gpt-api-4o} and Google Image Search \cite{google-image-search}. As shown in Figure \ref{fig:data_collect}, we first employ GPT-4o to list 400 class names commonly seen in our daily life. Then, for each class, the GPT-4o is asked to generate 10 corresponding anomalous and normal phrases based on the class name. The abnormality or normality descriptions indicated by these phrases are specifically suitable for the class name. These phrases will serve as the search prompts to query the image links in Google Image Search. However, the downloaded images are very "dirty" and contain many noise samples and duplications. For example, the collected anomaly set contains lots of normal images, and vice versa. A data-cleaning step is applied after the image collection.

Since the duplications mainly occur within a specific class, we extract the CLIP \cite{clip} features for all the images in the class and compare the cosine similarity of these features. If the similarity value is larger than $0.99$, then one of the images will be removed. To deal with the problematic grouping of anomaly and normal images, we combine the image and its corresponding search prompt and give them to GPT-4o for normal and anomaly classification. In the system prompt, we explicitly tell the GPT-4o that the search prompt is just a hint and not always correct and ask GPT-4o to determine the normality and abnormality by itself. This step will remove the images with incorrect labels and the artificial images, such as cartons or art. Some samples in the collected WebAD dataset are shown in Figure \ref{fig:gallery}. In total, WebAD contains around 72k images from 380 classes and more than 5 anomaly types for each class.



\begin{figure*}[!t]
\centering
    \includegraphics[width=\textwidth]{figures/gallery.pdf}
\caption{Overview of the gallery for in-the-wild image samples in WebAD. The images on the left side are anomalous, while the right side is for normal images. The links to download these images will be released to avoid copyright issues.}
\label{fig:gallery}
\end{figure*}

\subsection{Instruction Data Generation}
For existing datasets, we manually combine the anomaly type and the class name to create the short anomaly prompt (hint). Then, the image with or without the bounding boxes and the corresponding short prompt are utilized to prompt GPT-4o for the generation of detailed descriptions of the image and the anomalies. These descriptions contain all the information required for instruction-following data. The in-context learning strategy is implemented to generate the multi-round conversation data (see Figure \ref{fig:in_context}). Questions designed to elicit a one-word answer are utilized to balance the distribution of the normal and anomaly samples.

\begin{figure*}[!t]
\centering
    \includegraphics[width=\textwidth]{figures/in_context.pdf}
\caption{Prompt template for generating multi-round conversation in Anomaly-Instruct-125k (modified from the template of LLaVA \cite{llava}).}
\label{fig:in_context}
\vspace{-3mm}
\end{figure*}

\section{Training Details}
In the professional training stage, we leverage AdamW \cite{adamw} to be the optimizer and CosineAnnealingWarmRestarts \cite{loshchilov2017sgdr} as the learning rate scheduler. The initial learning rate is set to be $1e-4$, and the restart iteration is half of the single epoch. The anomaly expert is trained on 8 H100 GPUs for 2 epochs (2 hours), and the total batch size is 128. In the instruction tuning stage, we follow the default training setting of \textit{LLaVA-OneVision} \cite{llavaonevision} (reduce the batch size to 128), and the total training time for 0.5B and 7B models are 7 hours and 50 hours on 8 H100, respectively. When sampling the instruction data from the original recipe of \textit{LLaVA-OneVision}, we put more emphasis on low-level image understanding and 3D multi-view Q\&A, considering that anomaly detection originates from the low-level feature differences and the 3D anomaly detection requires multi-image understanding. Besides, for more knowledge in the medical domain, the model is also fed with the data from LLaVA-Med \cite{llavamed}.




\section{Experimental Results}

\subsection{Anomaly Detection}
\input{tables/table_visa_mvtec}
Similar to previous ZSAD works, the detailed image-level AUROC results for the anomaly expert of Anomaly-OV on VisA \cite{visa} and MVTec AD \cite{mvtec} are provided in Table \ref{Tab:mvtec_visa}.

\subsection{Anomaly Reasoning}
\input{tables/table_exp1}
\input{tables/table_candle}
\input{tables/table_capsule}
\input{tables/table_fryum}
\input{tables/table_cashew}
\input{tables/table_sign}
Table \ref{tab:pcb1} to \ref{tab:cashew} presents more comparison results of GPT-4o \cite{gpt-api-4o}, \textit{LLaVA-OneVision} \cite{llavaonevision}, and Anomaly-OV on AD \& reasoning. Anomaly-OV shows better performance in the detection and description of the visual anomalies in the images. Table \ref{tab:road_sign} demonstrates the low-level and complex reasoning capability of Anomaly-OV for an in-the-wild image, indicating a comprehensive understanding of the anomaly.


\section{Limitation and Future Work}
\input{tables/table_failure}
\noindent
\textbf{Limitation.} As shown in Table \ref{tab:failure}, sometimes, Anomaly-OV fails to provide an accurate classification of the target object, describes the anomaly by a general word (wax missing is described by "crack"), or presents wrong reasoning with hallucination. Also, there is still a large space for improvement in the detection performance of Anomaly-OV. Besides, the images contained in VisA-D\&R  are from the industrial domain, so more benchmarks in other domains, such as 3D and medical anomaly detection, are required to evaluate a unified AD \& reasoning model.

\medskip

\noindent
\textbf{Future Work.} The detection performance of Anomaly-OV is highly determined by the anomaly expert (see Table \ref{Tab:3}), so a more advanced design of the expert model is recommended in future research. One can change the base model to other open-sourced MLLMs to resolve the wrong classification issue. Also, we found that the diversity of the anomaly type is very limited in existing industrial anomaly datasets (mainly 'crack' or 'broken'), causing the assistant to fail to provide fine-grained anomaly reasoning or description for unseen anomaly features. Therefore, a more diverse industrial anomaly detection dataset is urgently required. Similar to other traditional MLLMs, Anomaly-OV only utilizes the output visual tokens from the last layer of the visual encoder as the input for LLM. However, anomaly detection is highly dependent on low-level visual clues. Hence, \textbf{forwarding multi-level features from different layers to the LLM} (as recent paper: "Dense Connector for MLLMs" \cite{yao2024denseconnectormllms} ) should be a possible solution for performance improvement.


