\section{Experiments}
\subsection{Training \& Evaluation}
There are two independent training stages for Anomaly-OV. In Stage 1, the components of the anomaly expert are trained to obtain the token selection capability, targeting traditional ZSAD. This stage utilizes all of the data with anomaly labels in Anomaly-Instruct-125k. Similar to previous works \cite{zhou2024anomalyclip, cao2025adaclip}, when evaluating the model on the datasets contained in the training set, the corresponding datasets are replaced by VisA \cite{visa}. In Stage 2, the anomaly expert and visual encoder are frozen, while the projector and LLM are trainable. In addition to our instruction dataset, we sample around 350k data from the original training recipe of \textit{LLaVA-OneVision} to maintain the generalization ability. For more details on training, please refer to the supplementary.

The ZSAD performance for the anomaly expert is evaluated on nine benchmarks, including MVTec AD \cite{mvtec}, VisA \cite{visa}, AITEX \cite{aitex}, ELPV \cite{elpv}, BTAD \cite{btad}, and MPDD \cite{mpdd} for industrial inspection, and BrainMRI \cite{brainmri}, HeadCT \cite{headct}, and Br35H \cite{Br35h} for medical diagnosis. AUROC (Area Under the Receiver Operating Characteristic) is leveraged to quantify the image-level AD performance. For text-based anomaly detection, both normal and anomaly data are employed to assess the accuracy by examining if the generated text contains the word \texttt{Yes}. Differently, only anomaly data are utilized to prompt the MLLMs to determine their anomaly reasoning capabilities since the justifications of normality are similar for different models.

\input{tables/table2}
\subsection{Zero-Shot Anomaly Detection}
\input{tables/table1}
As shown in Table \ref{Tab:1}, compared with existing methods, the anomaly expert of Anomaly-OV achieves significant image-level AUROC improvements on most of the ZSAD benchmarks, which demonstrates that the text encoder widely applied in existing models is not necessary. The success of our model mainly originates from the extra data of WebAD (Table \ref{tab:2}), which enables the model to learn more generic semantics for normality and abnormality from the data distribution in the absence of the text encoder. This observation also reveals that large-scale in-the-wild online data can benefit zero-shot performance in anomaly detection. 

While the Q-Former reduces the model performance on BrainMRI, it shows effectiveness on most benchmarks, indicating the importance of token aggregation. Similarly, the look-back information and two learnable embeddings are required for describing class-awareness abnormality and distinguishing positive and negative features, respectively. As previously discussed, the anomaly expert is responsible for selecting suspicious visual tokens for the LLM, and the significance maps shown in Figure \ref{fig:selection} demonstrate the interpretable token selection mechanism. The high intensities are automatically distributed around the anomalous areas even without any supervision of the anomaly masks. 

\begin{figure}[ht]
\centering
    \includegraphics[width=0.95\linewidth]{figures/selection2.png}
\caption{Visualization of the significance map on VisA samples.} 
\label{fig:selection}
\vspace{-4mm}
\end{figure}

\input{tables/table_exp2}


\subsection{Anomaly Detection \& Reasoning}
\input{tables/table3}
With the strong capabilities of the anomaly expert for zero-shot detection and suspicious token selection, Anomaly-OV accomplishes significant improvement in text-based anomaly detection and reasoning over other open-sourced generalist MLLMs, as shown in Table \ref{Tab:3}. Here are a few observations: \textit{i) While a larger language model cannot guarantee better detection performance, it always provides greater reasoning ability; ii) Most of the existing MLLMs present much lower recall than precision, indicating their insensitivity to visual anomalies; iii) GPT-4o shows stronger reasoning ability compared to other open-sourced models.} Table \ref{tab:pcb2} and Table \ref{tab:pasta} provide the qualitative comparison of our Anomaly-OV with its base model LLaVA-OV-7B \cite{llavaonevision} and GPT-4o \cite{gpt-api-4o}. Both GPT-4o and LLaVA-OV show insensitivity to anomalous features and cannot accurately detect the anomaly in the image. Sometimes, GPT-4o knows the image is anomalous but fails to describe the anomalies precisely.

We provide the fine-tuned version of the base model LLaVA-OV-0.5B on Anomaly-Instruct-125k, which presents much higher accuracy and more balanced precision and recall than its original version. This demonstrates the effectiveness of our instruction-tuning dataset. By integrating the anomaly expert with the base model, our Anomaly-OV-0.5B achieves $0.08$ accuracy and $0.06$ F1-score improvements in text-based anomaly detection and better reasoning capability in low-level and complex settings. Equipped with a larger language model, Anomaly-OV-7B provides the best detection performance among all the existing MLLMs and shows comparable reasoning ability with GPT-4o. Notably, we observe that the anomaly expert restricts the detection performance of Anomaly-OV. Therefore, the design of a stronger anomaly expert is suggested for future works.


\input{tables/table_exp3}





\subsection{Extension}
With the generalization and multi-image processing capabilities of MLLMs, it is possible to build a unified assistant for visual inspection. Table \ref{tab:ext} demonstrates the comprehensive knowledge of Anomaly-OV (without using Anomaly-ShapeNet \cite{anomaly_shapenet} for training) on 3D and medical (testing set of BMAD \cite{bmad}) AD \& reasoning. More data, benchmarks, and investigation on a unified model are meaningful.

\input{tables/table_ext}