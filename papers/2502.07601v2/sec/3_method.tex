\section{Method}

\subsection{Preliminary}
Training an MLLM from scratch demands extensive data and computational resources to align the visual and textual embedding spaces and develop robust instruction-following capabilities. Recent studies \cite{xie2024emovit, face, pose} reveal that pre-trained MLLMs function as generalists, possessing a broad knowledge base but underperforming in specialized domains. Therefore, our goal is to introduce an auxiliary specialist or expert model designed to guide the generalist in selecting and utilizing critical visual tokens. This approach circumvents the need for large-scale pre-training while preserving the generalization capacity of the original model.

We choose \textit{LLaVA-OneVision} \cite{llavaonevision} as our base MLLM because it is open-sourced and performs similarly to other commercial models. \textit{LLaVA-OneVision} follows the model architectures for LLaVA family \cite{llava,llava15,llava_interleave,llavanext} and other generic MLLMs, which typically consist of three major components: Visual Encoder, Projector, and LLM. The visual encoder \cite{clip, siglip} extracts the visual information from the raw images, the projector aligns the spaces of visual features with the word embedding, and the LLM is responsible for textual instruction processing and complex reasoning. Since the image resolution for CLIP pre-training is fixed, \textit{LLaVA-OneVision} leverages AnyRes with pooling strategy to scale up the input raw image resolution. Specifically, the high-resolution images are divided into a prototyped number of crops, and the visual encoder independently processes the image crops before final spatial pooling. 


\subsection{Architecture Overview}
With the same image-splitting strategy \textit{AnyRes} as \textit{LLaVA-OneVision}, the input high-resolution image is split into several crops, and the new image set can be written as:
\begin{equation}
    \mathcal{I}=\{I_{0}, I_{1}, I_{2}, ..., I_{n-1}\}
\end{equation}
where $I_{0}$ is the resized original image and $I_{j\neq 0}$ refers to the image crops. As shown in Figure \ref{fig:main}, the image set $\mathcal{I}$ will be processed by the visual encoder $\mathcal{F}_{\theta}$ to generate the final visual features $\{\mathbf{v}^{o}_{j}\}$. Similar to AnomalyCLIP \cite{zhou2024anomalyclip}, we store the outputs for four selected layers in the ViT \cite{vit} to capture the image representations from different levels and apply four adapters to compress the feature dimension. Then, the extracted visual features can be written as:
\begin{equation}
    \mathbf{v}^{i}_{j} = \mathcal{F}^{i}_{\theta}(I_{j})
\end{equation}
where $i$ denotes the $i$-th level and $j$ refers to the index of corresponding image in $\mathcal{I}$. These multi-level features have been demonstrated to be effective in capturing fine-grained local semantics by recent works \cite{gu2024anomalygpt, cao2025adaclip, zhou2024anomalyclip}.

The large-scale pre-trained CLIP models align the projection spaces of the textual and visual encoder. Therefore, the encoded image features already contain the class information required by ZSAD. To avoid human involvement in object classification and reduce the model complexity, we remove the heavy text encoder commonly utilized in existing works and let the visual model itself parse the information for suspicious classes or objects. Specifically, the output visual features for the original image $\mathbf{v}^{o}_{0}$ are leveraged to provide the global description of the target object or regions in the look-back path. With the multi-level features and the global embeddings, the LTFM module is responsible for the recognition and localization of suspicious tokens.

\begin{figure}[t]
\centering
    \includegraphics[width=\linewidth]{figures/human_examine.png}
\caption{Simulation of visual anomaly inspection by humans.}
\label{fig:human}
\vspace{-4mm}
\end{figure}
Drawing inspiration from human visual inspection, where suspicious objects or regions are identified and then inspected closely (see Figure \ref{fig:human}), we design the VT selector module for aggregating (zooming in) the crucial visual tokens and explicitly assisting the LLM in distinguishing these tokens from many irrelevant ones when dealing with instructions regarding anomaly detection and reasoning. Additionally, the original visual features are preserved to maintain the generalization capability of the base model on regular instructions, such as \texttt{Can you describe the content of the image?}



\subsection{Look-Twice Feature Matching}
\label{ltfm}
Given the global object information $\mathbf{v}^{o}_{0}$ provided by the look-back path, we generate the class-awareness abnormality description by merging $\mathbf{v}^{o}_{0}$ with two learnable embeddings: $\mathbf{e}^{+}\in \mathbb{R}^{D}$ and $\mathbf{e}^{-}\in \mathbb{R}^{D}$, where $+$ and $-$ indicate positive (anomalous) and negative (normal) patterns and $D$ is the embedding dimension. Specifically, a linear layer $\mathcal{T}^{o}_{i}$ is applied along the token dimension to select and fuse useful tokens from $\mathbf{v}^{o}_{0}$, and then the fused vector will be concatenated with $\mathbf{e}^{+}$ and $\mathbf{e}^{-}$ independently and pass through two MLPs $\{\mathcal{G}^{+}_{i}, \mathcal{G}^{-}_{i}\}$ to generate the abnormality and normality descriptions $\{\mathbf{d}^{+}_{i}, \mathbf{d}^{-}_{i}\}$, which can be represented by:
\begin{align}
\{\mathbf{d}^{+}_{i}, \mathbf{d}^{-}_{i}\} = 
\begin{cases}
\mathcal{G}^{+}_{i}(\mathbf{e}^{+} \circ \mathcal{T}^{o}_{i}(\mathbf{v}^{o}_{0})) \\
\mathcal{G}^{-}_{i}(\mathbf{e}^{-} \circ \mathcal{T}^{o}_{i}(\mathbf{v}^{o}_{0}))
\end{cases}
\end{align}
The visual features extracted from different levels of the ViT focus on different scales of semantics. Thus, the parameters of $\mathcal{T}^{o}_{i}$ and $\{\mathcal{G}^{+}_{i}, \mathcal{G}^{-}_{i}\}$ should be independent for different levels, where $i$ indicate the level number. 

Similar to the zero-shot classification mechanism of CLIP models, we calculate the possibilities of each patch token in $\mathbf{v}^{i}_{j}$ belonging to the anomalous patterns by combining cosine similarity and softmax operations:
\begin{equation}
    \textbf{m}^{i}_{j}=\frac{exp(<\mathbf{d}^{+}_{i}, \mathbf{v}^{i}_{j}>/\tau)}{exp(<\mathbf{d}^{+}_{i}, \mathbf{v}^{i}_{j}>/\tau)+exp(<\mathbf{d}^{-}_{i}, \mathbf{v}^{i}_{j}>/\tau)}
\end{equation}
where $\textbf{m}^{i}_{j}$ represents the significance map for visual tokens, $\tau$ is the temperature hyperparameter, and $<,>$ refers to the cosine similarity operator. The patch weight in $\textbf{m}^{i}_{j}$ indicates the closeness of the corresponding visual token to the anomalous pattern. Then, all the maps are averaged to capture the token significances from low to high levels:
\begin{equation}
    \textbf{m}_{j}=\sum^{4}_{i=1}\textbf{m}^{i}_{j}/4
\end{equation}
The visual features are leveraged twice in the forward and look-back paths, so this module is named by \textit{Look-Twice Feature Matching} (LTFM), following the nature of two-step human visual inspection shown in Figure \ref{fig:human}.
\begin{figure*}[t]
\centering
    \includegraphics[width=\linewidth]{figures/dataset.pdf}
\caption{Composition of the instruction data in \textbf{Anomaly-Instruct-125k}. There are four main types of image samples: \textit{in-the-wild}, \textit{industrial}, \textit{medical}, and \textit{3D} (in the format of multi-view images), covering most image anomaly detection tasks and enabling the possibility of a unified assistant for visual inspection. The reasoning words are highlighted in blue. For more information about dataset establishment, statistics, and the data collection pipeline, please refer to Section \ref{sup_dataset} in the supplementary.} 
\label{fig:daatset}
\vspace{-2mm}
\end{figure*}
\subsection{Visual Token Selector}
\label{vt_selector}
Under the image cropping strategy widely applied in recent MLLMs, there will be a large number of visual tokens for a high-resolution image, e.g., 7290 tokens for an image with 1152$\times$1152 resolution in \textit{LLaVA-OneVision}. While these tokens provide rich visual details, the LLM is required to pick the most useful information when adapting to a specific task. When the LLM lacks enough knowledge in this domain, the token-picking process will become complicated. Thus, our solution is to introduce a specialist or expert who knows which token is crucial or not and assist the LLM in selecting and emphasizing (zooming in) the crucial tokens.

Given the encoded visual tokens $\{\mathbf{v}^{o}_j\}$ for each image crop in $\mathcal{I}$ and the corresponding significance map $\textbf{m}_{j}$, the suspicious tokens are emphasized by direct multiplication of the two tensors. Then, the normal tokens will be scaled to zero while the anomalous tokens will be maintained. After that, spatial average pooling $\mathcal{P}$ is applied to reduce the number of tokens. This process can be written as:
\begin{equation}
    \textbf{q}_{j}=\mathcal{P}(\mathbf{v}^{o}_{j}\odot \textbf{m}_{j})
\end{equation}
where $\textbf{q}_{j}\in \mathbb{R}^{h \times w \times D}$ refers to the pooled query tokens. Empirically, setting $h=w=2$ provides a better trade-off than other options. Then, a Q-Former $\mathcal{Q}$ \cite{blip2} is leveraged to aggregate the correlated tokens in the original output by forwarding $\textbf{q}_{j}$ as the query and $\mathbf{v}^{o}_{j}$ as the key and value:
\begin{equation}
    \mathbf{v}^{s}_{j}=\mathcal{Q}(\textbf{q}_{j}, \mathbf{v}^{o}_{j}, \mathbf{v}^{o}_{j})
\end{equation}
The Visual Token Selector (VT Selector) serves as a tool for the anomaly expert to hand-pick visual tokens that contain the most suspicious semantics for a given image.
%This Visual Token Selector (VT Selector) serves as an actuator for the anomaly expert to select visual tokens that contain the most suspicious semantics for a given image. 
%The Visual Token Selector (VT Selector) serves as an aggregation tool for anomaly experts, enabling the selection of visual tokens that encapsulate the most suspicious or anomalous semantics within a given image.

\subsection{Inference and Loss}
\noindent\textbf{Anomaly Prediction.} In the traditional anomaly detection task, the model predicts the possibility of the image being abnormal. To achieve anomaly score prediction, we aggregate the anomaly information from all the image crops by an average operation weighted on the significance maps:
\begin{equation}
    \mathbf{r}(\mathcal{I}) = \frac{\sum_{j, k} \mathbf{v}^{s}_{j}[k]\cdot \mathcal{P}(\textbf{m}_{j})[k]}{\sum_{j, k}\mathcal{P}(\textbf{m}_{j})[k]}
\end{equation}
where $\mathcal{P}$ is the same spatial pooling in VT Selector and $\mathbf{r}(\mathcal{I})$ is a vector containing the global anomaly information for the entire image. Then, the anomaly expert can calculate the image-level abnormal possibility by parsing $\mathbf{r}(\mathcal{I})$:
\begin{equation}
    \mathbf{score}(\mathcal{I})=Sigmoid(\mathcal{G}^{o}(\mathbf{r}(\mathcal{I})))
\end{equation}
where $\mathcal{G}^{o}$ is an MLP for distinguishing normal and abnormal semantics. To handle the unbalanced sample distribution, we employ the balanced BCE loss as the professional training objective for the anomaly expert components.

\medskip
\noindent\textbf{Text Generation.} Instead of directly forwarding the concatenation of the original $\{\mathbf{v}^{o}_{j}\}$ and the selected $\{\mathbf{r}(\mathcal{I}), \mathbf{v}^{s}_{j}\}$ visual tokens into the LLM, we apply an indication prompt \texttt{with <adv> suspicious feature:} in the middle of the two series of tokens, which will highlight the selected tokens for LLM when handling anomaly-related instructions. This approach can be considered a form of prompt engineering in MLLMs. Besides, the \texttt{<adv>} is chosen from \{\texttt{highly}, \texttt{moderately}, \texttt{slightly}\} and is determined by $\mathbf{score}(\mathcal{I})$ and predefined thresholds $\{\textbf{s}_{low}, \textbf{s}_{high}\}$. When the input image $\mathcal{I}$ has a high likelihood of anomaly, the LLM will place greater emphasis on the selected tokens; otherwise, these tokens will have less significance. The text generation is implemented by the original auto-regressive token prediction mechanism of LLM:
\begin{equation}
    p(X_{a}|\mathcal{I}, X_{q})=\prod^{L}_{t=1}p_{\theta}(x_{t}|\mathcal{I}, X_{q, <t}, X_{a, <t})
\end{equation}
where $X_{a, <t}$ and $X_{q, <t}$ are the answer and instruction tokens from all prior turns before the current prediction token $x_{t}$ for a sequence of length $L$. The entire model is parameterized by $\theta$ and trained by the original language model cross-entropy loss for each predicted answer token $x_{t}$. 

