\section{Related Work}

\noindent \textbf{Multimodal Large Language Models.}
Vision-Language Models (VLMs), such as CLIP \cite{clip}, exhibit robust zero-shot classification capabilities and have been applied to a range of downstream vision tasks \cite{lin2023clip, liang2023open, chen2023clip2scene, liu2023clip, wang2023clipn}. Combining a VLM's vision encoder and an LLM \cite{bert, roberta, t5}, MLLMs \cite{blip, blip2, llava, zhu2024minigpt, instructblip} enable text-based interactions related to visual content. MLLMs have shown remarkable reasoning capability, particularly when incorporated with prompting strategies such as Chain-of-Thought~\cite{wei2022chain, lu2022learn, zhang2024multimodal}. Recent studies have harnessed MLLMs to provide reasoning for downstream tasks, e.g., video anomaly detection~\cite{lv2024video,yang2024follow}, affective computing~\cite{cheng2024emotion,guo2024stimuvar}, and visual commonsense reasoning~\cite{zhou2024vicor}, revealing more interpretability.

\medskip
\noindent \textbf{Unsupervised Anomaly Detection.}
Due to the scarcity and difficulty of collecting anomalous data, researchers often focus on the unsupervised AD setting, which exclusively uses normal data to train an AD model. Earlier studies, such as reconstruction-based~\cite{lo2022adversarially,mou2023rgi,zavrtanik2021draem}, student-teacher~\cite{deng2022anomaly,tien2023revisiting,zhang2023destseg}, and augmentation-based~\cite{li2021cutpaste} approaches, assume a large amount of normal data is available. These traditional approaches are less practical when data are restricted or expensive, such as in the medical domain.

\medskip
\noindent \textbf{Zero-Shot Anomaly Detection.} Unlike unsupervised AD \cite{roth2022towards, huang2022registration} and few-shot AD \cite{zhu2024toward, li2024promptad, gu2024anomalygpt, fang2023fastrecon, huang2024adapting}, ZSAD models directly access the likelihood of abnormality for a given image without requiring data specific to the target object. Existing works \cite{jeong2023winclip, zhou2024anomalyclip, cao2025adaclip, deng2023anovl} accomplish ZSAD by comparing visual and textual features encoded by visual and text encoders of CLIP and constructing their positive (anomaly) and negative (normal) prompts in the format of:
\begin{align*}
&\mathcal{P}^{+}=[V_{1}][V_{2}]...[V_{n}][object] \\
&\mathcal{P}^{-}=[W_{1}][W_{2}]...[W_{n}][object]
\end{align*}
where $V_{i}$ and $W_{i}$ are handcrafted or learnable tokens, and $object$ refers to the word \texttt{object} or the class name of the object. However, simply utilizing \texttt{object} to represent all kinds of objects cannot capture the class-awareness abnormality types. Also, for an intelligent visual assistant, the images should be totally blind to the user (object-agnostic). 

\begin{figure*}[t]
\centering
    \includegraphics[width=\linewidth]{figures/anomaly_ov_main_figure_final.pdf}
\caption{Overview of the \textbf{Anomaly-OV} architecture. It consists of two training stages: (1) professional training for the anomaly expert, and (2) visual instruction tuning for anomaly detection and reasoning. Text and visual tokens are distinguished by different colors.}
\label{fig:main}
\end{figure*}







