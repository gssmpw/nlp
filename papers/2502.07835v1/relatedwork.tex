\section{Related Work}
There has been no prior work on applying a \textbf{reverse generation} technique with LLMs in conjunction with the \textbf{SBC scoring mechanism}. However, existing research has explored LLM-based evaluation methods for \textbf{code analysis} and \textbf{natural language generation (NLG)}, leveraging large models for assessing outputs. Three notable studies in this area include:

\subsection{G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment (Yang et al., \cite{yang2023geval})}

This paper introduces an \textbf{LLM-driven evaluation framework} that employs a \textbf{chain-of-thought (CoT) approach} to assess the quality of generated text. The study demonstrates that LLM-based evaluators can outperform traditional NLG metrics in \textbf{text summarization} and \textbf{dialogue generation}. However, the authors also highlight a critical limitation that LLM-based evaluators tend to exhibit a \textbf{bias toward LLM-generated text}, raising concerns about fairness and reliability in assessments.

\subsection{ICE-Score: Instructing Large Language Models to Evaluate Code (Terry, \cite{terry2024icescore})}

Inspired by \textbf{G-EVAL}, this paper proposes \textbf{ICE-Score}, an evaluation metric that leverages LLMs for code assessment across multiple programming languages, including \textit{Java, Python, C, C++, and JavaScript}. The approach incorporates both \textbf{human-centered usefulness} and \textbf{execution-based functional correctness}, aiming to provide a \textbf{holistic evaluation} of generated code. ICE-Score refines the instruction-based evaluation paradigm for assessing AI-generated code in a structured manner.

\subsection{Metamorphic Prompt Testing for LLM Code Validation (Xiaoyin et al., \cite{wang2024validating} )}

As Large Language Models (LLMs) become increasingly integrated into the software development lifecycle, concerns regarding the quality, correctness, and reliability of generated code have grown. Xiaoyin et al. \cite{wang2024validating} highlight these challenges and propose \textit{metamorphic prompt testing} as a validation approach. Their method leverages the intrinsic consistency among correct code samples while identifying inconsistencies in flawed ones. By paraphrasing prompts and generating multiple versions of code for cross-validation, their evaluation on HumanEval demonstrated that metamorphic prompt testing detected 75\% of erroneous programs generated by GPT-4, with a false positive rate of 8.6\%. This approach underscores the importance of rigorous validation techniques for ensuring the reliability of LLM-generated code in production environments.

This paper builds upon these advancements by extending the \textbf{LLM-driven evaluation paradigm} but introduces a \textbf{fundamentally different approach}. Rather than directly evaluating code output, we propose a \textbf{reverse generation} framework, where \textbf{requirements are inferred from code} and then compared to the original requirements using \textbf{semantic similarity, BLEU, and completeness} as core evaluation factors. This method not only aligns LLM-based evaluation with traditional NLP techniques but also provides \textbf{developers with a structured way to assess code accuracy and requirement fidelity}, bridging the gap between AI-generated code and human expectations.