%%%%%%%% someconference 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{someconference2025} with \usepackage[nohyperref]{someconference2025} above.
\usepackage{hyperref}

\usepackage{todonotes}
\usepackage{multirow}


\usepackage{amsmath}
\usepackage{amssymb}

\DeclareTextFontCommand{\emph}{\em}
\newcommand{\mynote}[3]{\fbox{\bfseries\sffamily\scriptsize#1}{\small$\blacktriangleright$\textsf{\emph{\color{#3}{#2}}}$\blacktriangleleft$}}


\newcommand{\andrei}[1]
{\mynote{Andrei}{#1}{orange}}
\newcommand{\mahdi}[1]
{\mynote{Mahdi}{#1}{blue}}
\newcommand{\dan}[1]
{\mynote{Dan}{#1}{purple}}
\newcommand{\soroush}[1]
{\mynote{Soroush}{#1}{red}}

\renewcommand{\paragraph}[1]{\vspace{0.01em}\noindent \textbf{#1}}
% \newcommand{\squeeze}{\textstyle}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{someconference2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{someconference2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\input{math_operators}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\linespread{0.985}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}

\newcommand{\methodname}{{QuEST}}
\renewcommand{\paragraph}[1]{\vspace{0.6em} \noindent \textbf{#1}}

% The \someconferencetitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\someconferencetitlerunning{Training Accurate LLMs with Low-Bit Weights and Activations}

\begin{document}

\twocolumn[
\someconferencetitle{\methodname{}: Stable Training of LLMs with 1-Bit Weights and Activations}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the someconference2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\someconferencesetsymbol{equal}{*}

\begin{someconferenceauthorlist}
\someconferenceauthor{Andrei Panferov}{ista}
\someconferenceauthor{Jiale Chen}{ista}
\someconferenceauthor{Soroush Tabesh}{ista}
\someconferenceauthor{Roberto L. Castro}{ista}
\someconferenceauthor{Mahdi Nikdan}{ista}
\someconferenceauthor{Dan Alistarh}{ista,rhai}
% \someconferenceauthor{Firstname5 Lastname5}{yyy}
% \someconferenceauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \someconferenceauthor{Firstname7 Lastname7}{comp}
% %\someconferenceauthor{}{sch}
% \someconferenceauthor{Firstname8 Lastname8}{sch}
% \someconferenceauthor{Firstname8 Lastname8}{yyy,comp}
%\someconferenceauthor{}{sch}
% \someconferenceauthor{}{sch}
\end{someconferenceauthorlist}

\someconferenceaffiliation{ista}{ISTA}
\someconferenceaffiliation{rhai}{Red Hat AI}
% \someconferenceaffiliation{nm}{Neural Magic}

\someconferencecorrespondingauthor{Andrei Panferov}{andrei@panferov.org}
\someconferencecorrespondingauthor{Dan Alistarh}{dan.alistarh@ist.ac.at}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\someconferencekeywords{Machine Learning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \someconferenceEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\someconferenceEqualContribution} % otherwise use the standard text.

\begin{abstract}
One approach to reducing the massive costs of large language models (LLMs) is the use of quantized or sparse representations for training or deployment.
While post-training compression methods are very popular, the question of obtaining even more accurate compressed models by \emph{directly training} over such representations, i.e., \emph{Quantization-Aware Training (QAT)}, is still open: for example, a recent study~\citep{kumar2024scaling} put the ``optimal" bit-width at which models can be trained using QAT, while staying accuracy-competitive with standard  FP16/BF16 precision, at 8-bits weights and activations. 

We advance this state-of-the-art via a new method called \methodname{}, which is Pareto-competitive with FP16, i.e., it provides better accuracy at lower model size, while {training models with weights and activations in 4-bits or less}. Moreover, \methodname{} allows stable training with 1-bit weights and activations. 
\methodname{} achieves this by improving two key aspects of QAT methods: 
(1) accurate and fast quantization of the (continuous) distributions of weights and activations via Hadamard normalization and MSE-optimal fitting; 
(2) a new {trust gradient estimator} based on the idea of explicitly minimizing the error between the noisy gradient computed over quantized states and the ``true'' (but unknown) full-precision gradient. 
Experiments on Llama-type architectures show that \methodname{} induces stable scaling laws across the entire range of hardware-supported precisions, and can be extended to sparse representations. 
We provide GPU kernel support showing that models produced by \methodname{} can be executed efficiently. Our code is available at \url{https://github.com/IST-DASLab/QuEST}.
\end{abstract}

\vspace{-2em}
\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
    \centering
    \vspace{-3px}
    \includegraphics[width=\linewidth]{figures/main.pdf}
    \vspace{-25px}
    \caption{The scaling law induced by \methodname{} when training Llama-family models from 30 to 800M parameters on C4, with quantized weights and activations from 1 to 4 bits, in the 100 tokens/parameter regime (higher compression uses proportionally more data at fixed memory). \methodname{} allows for stable training at 1-bit weights and activations (W1A1), and the \methodname{}  W4A4 model is Pareto-dominant relative to BF16, with lower loss at lower size.}
    \vspace{-15px}
    \label{fig:main}
\end{figure}

The massive computational demands of large language models (LLMs), e.g.~\citep{llama3}, have made AI efficiency a critical challenge. 
One popular pathway to increased efficiency has been \emph{reducing numerical precision}, usually done via \emph{post-training quantization (PTQ)} methods for compressing weights~\citep{gptq, lin2024awq, quip, QuIPsharp} or both weights and activations~\citep{quik, quarot, atom}. 
Quantizing both operands is necessary to leverage hardware support for low-precision multiplications, which extends down to 4-bit~\citep{nvidiaGB100}. However, state-of-the-art PTQ methods are still far from recovering full accuracy for 4-bit precision~\citep{quarot, spinquant}, leaving a gap between computational support and achievable accuracy. 

One alternative is \emph{quantization-aware training (QAT)}~\citep{rastegari2016xnor, jacob2018qat}--- where models are trained from scratch with low-precision weights and activations on the forward pass, but with a full-precision backward pass---offering the potential for superior accuracy-vs-compression trade-offs, as gradient optimization can correct compression errors. 
Despite promising results for weight-only quantization~\citep{wang2023bitnet, kaushal2024spectra}, it is currently not known whether QAT can produce accurate LLMs with low-bitwidth weights and activations.  
Here, the key metric is the \emph{Pareto-optimal frontier}, i.e., the minimal representation size (or inference cost) for the model to achieve a certain accuracy under a fixed data or training budget. 
Recently, \citet{kumar2024scaling} identified 8-bit precision as Pareto-optimal for QAT methods on LLMs. 


\paragraph{Contribution.}
{We present \methodname{}, a new QAT method that brings the Pareto-optimal frontier to around 4-bit weights and activations and enables stable training at 1-bit precision for both operands}. As shown in Figure~\ref{fig:main}, when data and compute are scaled proportionally to model size, \methodname{} can train models with 4-bit weights and activations that have superior accuracy relative to BF16 models almost 4x in size. 

We achieve this by re-thinking two key aspects of  QAT methods: 1) the ``forward'' step, in which continuous-to-discrete tensor distribution fitting is performed on the forward pass, and 2) the ``backward'' step, in which gradient estimation is performed over the discrete representation. 
For the forward step, \methodname{} works by approximating the ``optimal'' continuous-to-discrete mapping by first applying a  normalizing Hadamard Transform, and then computing an MSE-optimal quantization for the resulting distribution. 
 This replaces the prior ``learned'' normalization approaches~\citep{choi2018pact, bhalgat2020lsqplus}. 

The key remaining question is how to find an accurate gradient estimator over a weight or activation tensor quantized as above. Here, prior work leverages the Straight-Through Estimator (STE)~\cite{bengio2013estimating}, augmented with learnable components, e.g.~\citep{bhalgat2020lsqplus}. 
We propose a different approach called \emph{trust estimation}, which seeks to minimize the difference between the ``true'' gradient (taken over high-precision weights) and its estimate taken over lower-precision weights and activations. To do this, a trust estimator diminishes the importance of the gradient for some components depending on their \emph{quantization error} on the forward step, following the intuition that entries with large errors lead to significant deviations in the gradient. 

Next, we focus on the following question: assuming that training computation is not a limiting factor, what is the ``optimal'' precision in terms of accuracy-vs-model-size? 
To address this, we implement \methodname{} in Pytorch~\citep{pytorch} and train Llama-family models~\citep{llama3} of up to 800M  parameters on up to 80B tokens from the standard C4 dataset~\citep{c4}, across precisions from INT1 to INT8.  
Results show that \methodname{} provides stable and accurate convergence across model sizes and precisions down to 1-bit weights and activations. This induces new scaling laws, which we study across model sizes in the large-data (100 tokens/parameter) regime. 
\methodname{} leads INT4 weights and activations to be Pareto-optimal in terms of accuracy at a given model size and inference cost, suggesting that the limits of low-precision training are lower than previously thought.  
In addition, we provide GPU kernels showing that models produced by \methodname{} can be run efficiently on commodity hardware. 

\vspace{-0.7em}
\section{Background and Related Work}
\label{sec:background}

\citet{hubara2016binarized} and~\citet{rastegari2016xnor} were among the first to consider training neural networks with highly-compressed internal states, focusing primarily on weight compression. 
Later work focused on quantization-aware training (QAT)~\citep{jacob2018qat, choi2018pact, esser2019learned, bhalgat2020lsqplus} in the form considered here, where the model weights and activations (i.e. the forward pass) are quantized, but the backward pass is performed in full-precision, using variants of the straight-through estimator (STE)~\citep{bengio2013estimating}. (The variant where all states, including gradients, are quantized~\citep{switchback, jetfire} is beyond the scope of this paper.) 


Broadly, QAT considers the problem of finding a quantized projection over a \emph{standard-precision tensor} $\mathbf{x}$, representing part of the weights or activations, minimizing output error. For symmetric uniform quantization, the projection onto \emph{the quantized tensor} $\hat{\mathbf{x}}$ is defined as:  

\begin{equation}
\hat{\mathbf{x}} = \alpha \cdot \biggl\lfloor\frac{\text{clip}(\mathbf{x}, \alpha)}{\alpha}\biggr\rceil,
\label{eqn:quant}
\end{equation}

\noindent where the $\text{clip}$ function performs a clamping operation over the value distribution for all values above the clipping parameter $\alpha$, which also acts as a scaling factor, normalizing values to $\mathbf{x}$ to $[-1, 1]$, and the function $\lfloor \cdot \rceil$ rounds each value to its nearest quantization point, defined as a uniform grid whose granularity depends on the number of available bits $b$. Most QAT methods propose to ``learn'' the factor $\alpha$, for instance, via gradient-based optimization. 
For example, QAT methods usually keep a standard-precision version $\mathbf{w}$ of the weights; the STE gradient is computed \emph{over the quantized weights} $\widehat{\mathbf{w}}$, and then added to the full-precision accumulator, possibly also updating the clipping factor $\alpha$. 

Recent work such as BitNet~\cite{wang2023bitnet, ma2024era1bitllmslarge} and Spectra~\citep{kaushal2024spectra} showed that \emph{weight-only quantization} is viable for small- and medium-scale LLMs. The concurrent work presents BitNet a4.8~\citep{wang2024bitnet48}, a hybrid scheme that combines ternary weights with mixed 4- and 8-bit activations, applied selectively to different matrices. 
In parallel,~\citet{kumar2024scaling} investigated scaling laws for GPT-type models with quantized states, concluding that the ``Pareto-optimal'' point for current QAT methods is around 8-bit weights and activations.


Prior work by~\citet{frantar2023scalinglawssparselyconnectedfoundation, jin2025journeymattersaverageparameter} studied scaling laws specifically for sparse foundation models, establishing that the loss can be stably predicted across parameter and data scales when the model weights are sparse. Recently,~\citet{frantar2025compressionscaling} generalized these laws to unify both sparsity and quantization, allowing to compare the ``effective parameter count'' for these two types of representations. Our work focuses on improved training methods for highly-compressed representations, leading to improved scaling laws relative to standard dense training, and can be applied to both sparsity and quantization.  


\vspace{-0.7em}
\section{\methodname}
\vspace{-0.4em}

\paragraph{Motivation.}
A simple way of describing current QAT methods is that, given a standard-precision tensor $\mathbf{w}$, we first try to get an accurate discrete approximation  $\widehat{\mathbf{w}}$ by optimizing parameters such as the clipping factor $\alpha$ in Equation~\ref{eqn:quant} to minimize some loss target, such as the mean-square-error (MSE), and then rely on STE to estimate $\nabla_{\mathbf{w}} L$, the gradient over  $\mathbf{w}$, by $\nabla_{\widehat{\mathbf{w}}} L$, the gradient taken w.r.t. the quantized weights $\widehat{\mathbf{w}}$. 
Yet, the difference between these two gradients, which correlates to the gap in optimization trajectory, could be unbounded, specifically because of large errors in a small subset of entries. 

Instead, in this paper, we seek to minimize the difference between the true and discrete gradients, measured, e.g. as 
\begin{equation}\label{eq:direct_grad}
    \;\Big\|\,\nabla_{\mathbf{w}} L \;-\; \nabla_{\widehat{\mathbf{w}}} L\,\Big\|_2^2.
\end{equation}

Let us define the \emph{quantization error} for each entry $w_k$ as 
$\text{err}_k \;=\; \big|\,w_k \;-\;\hat{w}_k\,\big|.$ 
We can partition the weight indices $k$ based on whether the quantization error $\text{err}_k$ is smaller or larger than some ``trust factor'' threshold $T$. Denote:
\[
    S_{\text{small}} \;=\; \{\,k : \text{err}_k \le T\}, 
    \quad
    S_{\text{large}} \;=\; \{\,k : \text{err}_k > T\}.
\]

Then, the squared gradient difference in~(\ref{eq:direct_grad}) decomposes as:
\begin{eqnarray*} \label{eq:err_split}
    \underbrace{\sum_{k \in S_{\text{small}}} (\nabla_{{\mathbf{w}}} L_k \;-\; \nabla_{{\widehat{\mathbf{w}}}} L_k)^2}_{(\star)}
     + 
    \underbrace{\sum_{k \in S_{\text{large}}} (\nabla_{{\mathbf{w}}} L_k \;-\; \nabla_{{\widehat{\mathbf{w}}}} L_k)^2}_{(\star\star)}.
\end{eqnarray*}

Assuming that the loss $L$ is $\gamma$-smooth, the  $(\star)$ ``small error'' term would be upper bounded by $\gamma^2 \| \mathbf{w} - \widehat{\mathbf{w}} \|_2^2 \leq \gamma^2 T^2 |S_{\text{small}}| $. Intuitively, this term is minimized in a standard QAT method's ``distribution fitting'' step. 
Yet, distribution fitting does not address the ``large error'' term $(\star\star)$: specifically, outlier entries clipped in the fitting step can lead to extremely large gradient estimation errors. 

\methodname{} takes this into account by balancing estimation errors due to minor but persistent quantization errors in $(\star)$, with the significant ``outlier'' errors incorporated by term $(\star\star)$. For this, we propose an efficient fitting mechanism that minimizes persistent errors, coupled with a ``trust'' gradient estimator step aimed at bounding outlier errors. 

\subsection{Step 1: Distribution Fitting}
\label{sec:distribution_fitting}

While optimizing the quantization grid to best fit the underlying tensor is a core idea across all quantization methods, PTQ methods traditionally use more complex and computationally heavy approaches~\citep{qlora, higgs}. In contrast, QAT methods rely on backpropagation through the scaling factor for error-correction~\citep{esser2019learned, bhalgat2020lsqplus} while performing re-fitting. 
To avoid backpropagation errors impacting the forward pass, we do not use backpropagation for distribution fitting. 
Instead, we start from the empirical observation that the distribution of weights and activations during LLM training is sub-Gaussian but with long tails~\citep{llm_int8, spqr}.  

\paragraph{Gaussian Fitting.} Specifically, we choose to optimize the grid to explicitly fit a Gaussian distribution with the same parametrization as the empirical distribution of the underlying tensor $\mathbf{x}$.  
Concretely, we use \textit{root mean square (RMS)} normalization to first align the empirical distribution of $\mathbf{x}$ with a $\mathcal{N}(0, 1)$ Gaussian distribution~\citep{frantar2025compressionscaling}. We then perform the projection operation with the scale $\alpha^*$ chosen to minimize the $L_2$ error resulting from projecting $\mathcal{N}(0, 1)$. Formally: 

\begin{align*}
    \widehat{\mathbf{x}} &= \alpha^* \cdot \text{RMS}(\mathbf{x}) \cdot \biggl\lfloor\frac{\text{clip}\left(\mathbf{x}/\text{RMS}(\mathbf{x}), \alpha^*\right)}{\alpha^*}\biggr\rceil =\\
    &\coloneq \proj_{\alpha^*}(\mathbf{x}), \text{ where} \\
    \alpha^* &\coloneq \argmin_{\alpha \in \mathbb{R}} \mathbb{E}_{\xi\sim\mathcal{N}(0, 1)} \left\|\xi - \alpha \cdot \biggl\lfloor\frac{\text{clip}(\xi, \alpha)}{\alpha}\biggr\rceil\right\|_2^2
\end{align*}

is the MSE-optimal scaling factor. 
If $\mathbf{x}$ were Gaussian-distributed, this would produce an MSE-optimal projection.

\paragraph{Hadamard Preprocessing.} 
Yet, the natural distribution of tensor values may not be Gaussian, especially given the emergence of outlier values~\citep{llm_int8, nrusimha2024mitigatingimpactoutlierchannels}. 
To mitigate this, we add a Hadamard Transform (HT) step \emph{before} Gaussian Fitting. Thus, our forward pass projection becomes:



\begin{equation}
\label{eq:forward}
    \hat{\mathbf{x}}_h = \proj_{\alpha^*}{\text{HT}(\mathbf{x})}. 
\end{equation}

In other words, we transform the target tensor via multiplication with a Hadamard matrix of appropriate shape, applied along the matrix-multiplication dimension,  and then project it to an MSE-optimal grid in the Hadamard domain.
Here, we leverage 1) the fact that, roughly, multiplication of a matrix with the Hadamard Transform leads the weight distribution to better match a Gaussiant~\citep{doi:10.1137/060673096, suresh2017distributedmeanestimationlimited}; 2)  the existence of fast Hadamard multiplication kernels~\citep{fht_tridao}, and 3) the fact that the HT is orthogonal, so it can be easily inverted. While this HT effect has been utilized in PTQ~\citep{QuIPsharp, quarot, higgs}, we believe we are the first to harness it for QAT. 

\subsection{Step 2: Trust Gradient Estimation}
\label{sec:trust}

Next, we focus on the backward pass. 
For simplicity, we first describe the variant without the Hadamard Transform step and then integrate this component. 

\paragraph{Trust Estimators for the Basic Projection.} First, assume that $\widehat{\mathbf{x}} = \proj_{\alpha^*}(\mathbf{x})$. Since the projection operation $\lfloor x \rceil$, is not differentiable w.r.t. $x$, we need a robust way to estimate our gradient. Expressed as an operator, STE can be written as $\frac{\partial}{\partial \mathbf{x}} \approx \frac{\partial}{\partial\lfloor \mathbf{x}\rceil}$ during the backward pass, allowing gradients to propagate through the network, but can lead to large errors due to components with large quantization error.

Specifically, the factor $\alpha^*$, chosen to minimize the weight fitting error, acts as a natural scale for how far off their real value the majority of quantized values can be: for values below the scaling factor, this error is not larger than $T = \frac{\alpha^*}{2^b-1}$, the half-width of a quantization interval. 
This gives a natural bound for the $(\star)$ term in our analysis of Equation~\ref{eq:direct_grad}. 

To bound the second term $(\star\star)$, we choose to \emph{not trust} the gradient estimations for weights with large errors $\{\nabla_{{\widehat{\mathbf{w}}}} L_k : k \in S_{\text{large}}\}$. Choosing $T = \frac{\alpha^*}{2^b-1}$ and  masking gradients for elements in $S_{\text{large}}$ we obtain the gradient operator:

$$
\frac{\partial}{\partial \mathbf{x}} \approx \mathbf{I}_{|\hat{\mathbf{x}} - \mathbf{x}| \le T} \odot \frac{\partial}{\partial \hat{\mathbf{x}}} \coloneq M_{\alpha^*}(\mathbf{x}; \hat{\mathbf{x}}) \odot \frac{\partial}{\partial \hat{\mathbf{x}}}, 
$$
where $\mathbf{I}_{|\hat{\mathbf{x}} - \mathbf{x}| \le T} $ is the standard indicator operator. 
We will refer to $M_{\alpha^*}$ as the ``trust mask''; this gradient estimation operator will be called the \textbf{trust estimator}.

\paragraph{Trust Estimators for the Hadamard Projection.} 
We now interface the trust estimator with the Hadamard Transform (HT) and its inverse (IHT) to obtain the following forward scheme: $\mathbf{x}_h = \text{HT}(\mathbf{x})$ and $\hat{\mathbf{x}}_h =   \proj_{\alpha^*}{\mathbf{x}_h}$. 
Then, the natural approach is to perform trust estimation directly in the Hadamard domain, where quantization takes place: 

\begin{align*}
    \frac{\partial}{\partial \mathbf{x}} &\approx \text{IHT}\left( M_{\alpha^*}(\mathbf{x}_h; \hat{\mathbf{x}}_h) \odot \frac{\partial}{\partial \hat{\mathbf{x}}_h}\right).
\end{align*}

In other words, after deriving the trust mask w.r.t. distribution fitting in the Hadamard domain, we apply the resulting mask $M_{\alpha^*}(\mathbf{x}_h; \hat{\mathbf{x}}_h)$ onto the gradient w.r.t. quantized weights \emph{in the Hadamard domain}.  


\paragraph{Gradient Effects.} 
Notice that, in the absence of the HT or regularization effects (e.g., weight decay), the ``untrusted'' weights in $S_{\text{large}}$ would receive no gradient and may be permanently removed from optimization. 
Yet, the addition of the HT means that the trust mask is \emph{no longer binary} in the ``standard'' domain, allowing for gradient flow towards \emph{all model weights}. 
We validated this effect empirically by observing that the HT reduced the final cardinality of the ``untrusted'' weights set $S_{\text{large}}$ by $\approx 4$x, aligning it 
with the number of values we would expect to be outside the ``trust set'' at every step, for weights from a normal distribution. This is investigated in more depth in Appendix~\ref{app:masks}.

\subsection{Discussion}
\label{sec:trust_discussion}

\begin{algorithm}[t]
\caption{\texttt{QuEST Training Forward}}
\label{alg:quest_forward}
\begin{algorithmic}[1]
    \STATE {\bfseries Input:} Input activations $\mathbf{x}$, row-major weight $\mathbf{w}$
    \STATE $\mathbf{x}_h = \text{HT}(\mathbf{x})$
    \STATE $\hat{\mathbf{x}}_h = \proj_{\alpha^*}{\mathbf{x}_h}$
    \STATE $\mathbf{w}_h = \text{HT}(\mathbf{w})$
    \STATE $\hat{\mathbf{w}}_h = \proj_{\alpha^*}{\mathbf{w}_h}$
    \STATE $\mathbf{y} = \hat{\mathbf{x}}_h \hat{\mathbf{w}}_h^T$
    \STATE {\bfseries Return:} $\mathbf{y}$, $\hat{\mathbf{x}}_h$, $\hat{\mathbf{w}}_h$, $M_{\alpha^*}(\mathbf{x}_h; \hat{\mathbf{x}}_h)$, $M_{\alpha^*}(\mathbf{w}_h; \hat{\mathbf{w}}_h)$
\end{algorithmic}
\end{algorithm}


\paragraph{Implementation.} In practice, we use identical Hadamard Transforms along the matrix-multiplication dimension for both the weights $\mathbf{w}$ and the activations $\mathbf{x}$. Since the Hadamard Transform is unitary, the quantized matrix multiplication output $\mathbf{y} = \hat{\mathbf{x}}\hat{\mathbf{w}}^T$ is aligned with the full precision output $\mathbf{x}\mathbf{w}^T$ it approximates. The algorithm~\ref{alg:quest_forward} describes the forward pass over a linear layer actively quantized with \methodname{} for a row-major weight representation.


The algorithm~\ref{alg:quest_backward} describes the backward pass over the same layer using the quantized weight and activations from the forward pass as well as error gradient w.r.t $\mathbf{y}$. We note that, although the backward computation is performed w.r.t. the \emph{quantized} weights and activations, the multiplications and gradient operands are performed in standard 16-bit precision. 

\begin{algorithm}[ht]
\caption{\texttt{QuEST Training Backward}}
\label{alg:quest_backward}
\begin{algorithmic}[1]
    \STATE {\bfseries Input:} $\frac{\partial L}{\partial \mathbf{y}}$, $\hat{\mathbf{x}}_h$, $\hat{\mathbf{w}}_h$, $M_{\alpha^*}(\mathbf{x}_h; \hat{\mathbf{x}}_h)$, $M_{\alpha^*}(\mathbf{w}_h; \hat{\mathbf{w}}_h)$
    \STATE $\frac{\partial L}{\partial \hat{\mathbf{x}}_h} = \frac{\partial L}{\partial \mathbf{y}} \hat{\mathbf{w}}_h$
    \STATE $\frac{\partial L}{\partial \mathbf{x}} = \text{IHT}\left(M_{\alpha^*}(\mathbf{x}_h; \hat{\mathbf{x}}_h) \odot \frac{\partial L}{\partial \hat{\mathbf{x}}_h}\right)$
    \STATE $\frac{\partial L}{\partial \hat{\mathbf{w}}_h} = \hat{\mathbf{x}}_h^T \frac{\partial L}{\partial \mathbf{y}}$
    \STATE $\frac{\partial L}{\partial \mathbf{w}} = \text{IHT}\left(M_{\alpha^*}(\mathbf{w}_h; \hat{\mathbf{w}}_h) \odot \frac{\partial L}{\partial \hat{\mathbf{w}}_h}\right)$
    \STATE {\bfseries Return:} $\frac{\partial L}{\partial \mathbf{x}}$, $\frac{\partial L}{\partial \mathbf{w}}$
\end{algorithmic}
\end{algorithm}


\paragraph{Training Complexity.}
In total, during training, for each original matrix multiplication (e.g., $\mathbf{x}\mathbf{w}^T$), we need only two Hadamard Transforms on the forward pass and two Inverse Hadamard transforms on the backward pass.

For a Transformer model~\citep{vaswani}  with $d$ blocks and hidden dimension $h$, and a batch containing $b$ tokens, the MatMul complexity of the forward pass can be estimated as: $b \times d \times h^2$. Then, the asymptotic cost of the Hadamard Transform is the quantity $b \times d \times h \times \log{h} + d \times h^2 \times \log{h}$, which is asymptotically negligible with $b > \log{h}$.



\paragraph{Activation Effects.} 
It is well-known~\citep{choi2018pact} that activation quantization has major impact on training, possibly due to compounding with model depth. To test the effect of different gradient estimators on backpropagation, we empirically examine ``gradient quality''  as follows: we calculate intermediate gradients $\nabla_{\mathbf{\hat{a}}^\ell} L$ with respect to activations after the $\ell$-th Transformer block. For the same input, we disable activations quantization and calculate the  ``true'' gradients $\nabla_{\mathbf{a}^\ell} L$.
We then define the ``gradient alignment'' as the cosine similarity between gradients:
$
\Xi(\nabla_{\mathbf{\hat{a}}^\ell} L, \nabla_{\mathbf{a}^\ell} L) = ({\nabla_{\mathbf{\hat{a}}^\ell} L \cdot \nabla_{\mathbf{a}^\ell} L})/({\|\nabla_{\mathbf{\hat{a}}^\ell} L\|_2 \, \|\nabla_{\mathbf{a}^\ell} L\|_2}).
$



\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/gradient_alignment.pdf}
    \vspace{-15px}
    \caption{Gradient alignment comparison for a 30M Llama model after training on 2.7B tokens in 8-bit precision.}
    % \vspace{-15px}
    \label{fig:grad_similarity}
\end{figure}


While low similarity does not necessarily indicate poor gradient estimation (as the quantized forward pass might have utilized slightly different pathways, leading to discrepancy), high similarity clearly indicates that the estimator produces ``high-quality'' gradients relative to full precision. Figure~\ref{fig:grad_similarity} compares the gradient alignment 
for the STE relative to \methodname{}, with and without the HT. \methodname{} leads to remarkably-high and well-concentrated alignment ($\geq 0.8$), even at larger depths. By contrast, standard trust estimation degrades alignment with depth but has good concentration, whereas the STE has \emph{poor alignment and high variance}. 


\paragraph{The 1-bit Case.} In our original trust estimation formulation, we proposed to set the trust factor as half the quantization interval, $T = \frac{\alpha^*}{2^b-1}$. Thus, the trust regions increase exponentially as the bitwidth decreases. In particular, for 1-bit weights and activations, QuEST will suffer from trust regions that extend out of the grid by a whole $\alpha^\star$. To fix this, we reduce the size of the ``outermost'' trust regions, outside the clipping factor, by a scaling factor $s$. Through small-scale experiments, we determined the optimal value of $s$ to be $s^\star \approx 1.30$. We use this scaling factor for all the 1-bit \methodname{} runs in this paper (unless stated otherwise). This modification is necessary (and leads to an improvement) only in the extreme 1-bit compression regime. This is discussed further in Appendix~\ref{app:outer_trust}.


\vspace{-1em}
\section{Experimental Validation}

\subsection{Implementation Details}

\paragraph{Models and Hyperparameters.} We tested our method on pre-training decoder-only Transformers~\citep{vaswani} following the Llama architecture~\citep{touvron2023llama2openfoundation}, in the range of 30, 50, 100, 200, 430 and 800 million non-embedding parameters. Please see Appendix~\ref{app:hyper_parameters} for architecture and hyper-parameter details. We trained all models on tokens from the C4~\citep{dodge2021documentinglargewebtextcorpora} dataset, tokenized with the Llama 2 tokenizer. We used the AdamW~\citep{loshchilov2019decoupledweightdecayregularization} optimizer with a cosine learning rate schedule and a 10\% warmup period, with gradient clipping (1.0 threshold, decoupled weight decay of 0.1). We identified the learning rate optimally for a 50M FP16 model via a learning-rate sweep. For other models, as standard, we scale the learning rate inverse-proportionally to the number of non-embedding parameters. We reuse the exact learning rates for all \methodname{} training runs. Please see \url{https://github.com/IST-DASLab/QuEST} for a reference implementation. 

Unless stated otherwise, we train every model on a number of tokens equal to 100x its number of ``free'' parameters, e.g., 10B tokens for a Llama 100M model, regardless of precision. This allows us to explore the data-saturation regime. We aim for comparisons that are iso-size: 
That is, to match the size / FLOPs of a 100M FP16 Llama model (trained on 10B parameters), we will train a 400M-parameter model with 4-bit weights and activations, using 40B total tokens. This allows us to explore accuracy for fixed model sizes, across compression ratios (see Figure~\ref{fig:main}). We discuss different $D/N$ regimes in Appendix~\ref{app:transitory}. 

\subsection{Comparison to Prior QAT Methods}

We compare \methodname{} to two widely used QAT baselines: PACT ~\citep{choi2018pact} and LSQ ~\citep{esser2019learned}, both of which rely on learnable quantization parameters. While both methods can still converge at higher bitwidths, PACT struggles in the low-precision setting (below INT4), where it fails to produce viable models. Thus, we omit PACT from our comparison in Figure~\ref{fig:baselines_comparison} (its PPL values exceed 150). Instead, we compare a highly-tuned version of LSQ and \methodname{} on 30M models trained on different bandwidth configurations. While LSQ maintains reasonable performance across bitwidths, \methodname{} consistently achieves lower final perplexity.
We found that \methodname{} exhibits stable convergence across all precisions and model sizes tested, with smooth loss trajectories and no signs of divergence. A detailed analysis of its convergence compared to LSQ and PACT is provided in  Appendix~\ref{app:training_stability}.

\label{sec:baselines}
% \todo[inline]{Rush and Andrei}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/baselines_30M_w+a_ppl_bar.pdf}
    % \vspace{-15px}
    \caption{Perplexity (PPL) across bit-widths with \methodname{} vs. a tuned variant of LSQ on a 30M model. \methodname{} leads to consistently lower PPL, with the advantage growing with compression.}
    % \vspace{-15px}
    \label{fig:baselines_comparison}
\end{figure}


\subsection{Scaling Laws}
\label{sec:scaling_laws}
% \todo[inline]{Andrei and Jiale and Rush}

\paragraph{Background.} 
\citet{hoffmann2022trainingcomputeoptimallargelanguage} proposed to model loss scaling as a function of the number of parameters in the model $N$ and the number of tokens  $D$ it was trained on, in the form of parametric function:  
\begin{equation}
    \label{eq:chinchilla_law}
    L(N, D) = AN^{-\alpha} + BD^{-\beta} + E,
\end{equation}
where $A$, $B$, $E$, $\alpha$, and $\beta$ are the scaling law parameters that can be fit empirically. Following~\citet{frantar2025compressionscaling}, we modify this formula assuming that the training precision $P$ only affects the parameter count $N$ as a multiplicative factor  $\text{eff}(P)$, which, for a given quantization method, depends only on the training precision:

\begin{equation}
    \label{eq:full_scaling_law}
    L(N, D, P) = \frac{A}{(N \cdot \text{eff}(P))^\alpha} + \frac{B}{D^\beta} + E. 
\end{equation}

If we take $\text{eff}(16) = 1.0$, we recover the law in Equation~\ref{eq:chinchilla_law}.

\paragraph{Fitting process.} To estimate $A$, $B$, $E$, $\alpha$, $\beta$ and $\text{eff}(P)$ for every quantization precision $P$ we need, we fit this parametric function by minimizing the Huber loss~\citep{10.1214/aoms/1177703732} between the predicted and the observed log loss. Our process is detailed in the Appendix, and closely follows the setup of~\citet{hoffmann2022trainingcomputeoptimallargelanguage}, including the grid search and the loss hyper-parameters.

Specifically, we fit the model on the range of parameters $P \in \{1, 2, 3, 4, 16\}$, $N \in \{30, 50, 100, 200, 430, 800\}\times10^6$ and $D = 100\times N$. The resulting fit is presented on Figure~\ref{fig:main}. To capture a larger range of $D$, we fit the model on additional runs with $P \in \{2, 3, 4\}$, $N \in \{30, 50, 100\}\times10^6$ and $D/N \in \{25, 50\}$. We additionally fit the extensions of our method described in Sections~\ref{sec:execution} and~\ref{sec:extra}. Appendix Figure~\ref{fig:data_fit} illustrates the quality-of-fit. 


\begin{table}[t]
\centering
\begin{tabular}{ll|llllll}
    \toprule
    \multicolumn{2}{l|}{$P$}                             & 1    & 2    & 3    & 4    & 8    & 16    \\ \hline
    \midrule
    \multicolumn{2}{l|}{$\text{eff}(P)$}                 & 0.02 & 0.16 & 0.43 & 0.70 & 1.02 & 1.00      \\ % \hline
    % \multirow{2}{*}{$\text{spd}(P)$} & Peak TOPS, RTX4090 & 32   & -    & -    & 8    & 4    & 1.00        \\
    %                                  & Measured\,\,, RTX4090 & -  & -    & -    & ?    & -    & 1.00    
    \bottomrule
\end{tabular}
\caption{Fitted scaling-law ``effective parameter'' multipliers.}
\label{tab:accspd}
\vspace{-0.5cm}
\end{table}


\paragraph{Results.} 
The overall results were presented in Figure~\ref{fig:main}, illustrating loss vs. model size. 
First, we observe that, remarkably, \methodname{} provides stable training down to 1-bit weights and activations, across model sizes, following a stable scaling law. 
Second, examining the Pareto frontier, we observe that 4-bit precision is slightly superior to 3-bit, and consistently outperforms all higher precisions. 
Overall, these results show that \methodname{} can lead to stable scaling laws, which consistently improve upon prior results~\citep{kumar2024scaling}, moving the Pareto-optimal line to around 4-bit.

\subsection{Finding the ``Optimal'' Precision} 
\label{sec:optimality}

\paragraph{The Overtraining (OT) regime.} 
The goal of a standard scaling law (Equation~\ref{eq:chinchilla_law}) is to determine the ``optimal'' model size $N$ and training duration $D$ under fixed pre-training compute $C = 6ND$. For instance, \citet{hoffmann2022trainingcomputeoptimallargelanguage} estimated the ``Chinchilla-optimal''  ratio to be around $D/N\approx20$. 
Yet, it is now common to train (often smaller) models way beyond this ratio, effectively spending additional training compute (relative to ``optimal'') to minimize deployment costs by executing a smaller model. For example,  recent models are trained with $D/N \geq 1000$ \citep{llama3, team2024gemma}. With test-time compute~\cite{snell2024scaling}, there is an incentive to increase this even further. If we extrapolate and take $D/N \to \infty$, Equation~\ref{eq:full_scaling_law} takes the simplified form:

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/efficiencies.pdf}
    \vspace{-15px}
    \caption{Illustration of the efficiency factors $\text{eff}(P) / P$, arising from our analysis, for different numerical precisions $P$ and formats (INT, FP, INT+sparse). Higher is better. INT4 appears to have the highest efficiency among hardware-supported formats.}
    \label{fig:efficiencies}
    \vspace{-0.5cm}
\end{figure}


\begin{equation}
    \label{eq:limit_scaling_law}
    L_{OT}(N, P) = \frac{A}{(N \cdot \text{eff}(P))^\alpha} + E.
\end{equation}

We refer to this as the ``overtraining'' (OT) regime, where the training compute is less relevant, and is only bounded by factors such as the available amount of filtered training data. The focus is on minimizing \emph{runtime/inference} compute, measured for example by model latency. This problem can be formulated as finding the optimal model size $N$ and precision $P$ that minimizes a certain runtime compute limit.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/extra.pdf}
    \vspace{-15px}
    \caption{Additional scaling laws induced by \methodname{}: \textbf{(a, left)} compares INT, FP, and INT+sparse formats at 4-bit precision, \textbf{(b, middle)} shows the scaling laws for weight-only quantization, where 2-bit appears to be Pareto-dominant, while \textbf{(c, right)} shows that trust estimation benefits significantly from Hadamard normalization.}
    % \vspace{-15px}
    \label{fig:extra}
\end{figure*}


\paragraph{Runtime Cost Estimate.} 
Since we focus on quantizing both weights and activations, the matrix multiplications can be performed directly in lower-precision, providing linear speedups in the precision $P$~\citep{abdelkhalik2022demystifyingnvidiaamperearchitecture}. As such, we can roughly estimate the runtime cost, up to constants, as the precision-weighted number of basic operations (FLOPs) in a forward pass $F =  NP $. Then, the problem of minimizing loss while staying within a certain runtime (FLOP) constraint can be re-written as:

$$
\min_{N,P} L_{OT}(N, P) = \frac{A}{\left(F \cdot \frac{\text{eff}(P)}{P}\right)^\alpha} + E ~\text{ s.t. } F\le F_{\max}.
$$


From this formulation, if we fix $F\le F_{\max}$, maximizing $\frac{\text{eff}(P)}{P}$ becomes the key factor that influences the ``optimal'' pre-training precision in the OT regime. Recall that we can estimate $\text{eff}(P)$ from the empirical scaling law (obtained in Section~\ref{sec:scaling_laws} and shown in Table~\ref{tab:accspd}). Thus, we can calculate $\frac{\text{eff}(P)}{P}$ for any precision. Figure~\ref{fig:efficiencies} suggests that 4-bit appears to be the optimal pre-training precision in this regime.


\subsection{Extensions to Different Formats}

\paragraph{The FP4 Format}. We can use the same framework to compare the ``effective parameter count'' for INT, INT + sparse, and the lower-precision FP format supported by NVIDIA Blackwell~\citep{nvidiaGB100}.  
\methodname{} can be extended to this data type by replacing the $\lfloor \cdot \rceil$ rounding operation with rounding to the FP4 grid $\lfloor \cdot \rceil_{\text{FP4}}$ scaled to fit the same $[-1, 1]$ interval. The optimal scaling factor $\alpha^*_{\text{FP4}}$ would be defined by simply replacing $\lfloor \cdot \rceil$ with $\lfloor \cdot \rceil_{\text{FP4}}$ in the original definition. We choose the trust factor $T$ for $M_{\alpha^*}(\mathbf{x}; \hat{\mathbf{x}}) = \mathbf{I}_{|\hat{\mathbf{x}} - \mathbf{x}| \le T}$ as the largest half-interval of the FP4 grid.

To determine the $\text{eff}(P)$ parameter for FP4, we train 30, 50, 100, and 200M models with \methodname{} in FP4 precision and aggregate results in Figure~\ref{fig:extra}(a), comparing them with the original uniform grid results. We observe that FP4 performs slightly worse than INT4. We also fit FP4 with the scaling law in Equation (\ref{eq:full_scaling_law}) and present the resulting $\text{eff}(P)/P$ in Figure~\ref{fig:efficiencies} (red dot). The results show that, indeed, FP has lower parameter efficiency than INT at 4-bit precision. 
We hypothesize that this is correlated with the fact that, when clipping is allowed, FP4 has higher MSE than INT4 when fitting Gaussian-distributed data. 


\paragraph{Extension to sparsity.} \methodname{} can also be extended to sparsity. Then, the trust estimator will mask out sparsified elements with absolute value above the trust mask; specifically, this covers the majority of sparsified elements, except for the small elements within $\left[-\frac{\alpha^*}{2^b-1}, +\frac{\alpha^*}{2^b-1}\right]$. In practice, we still keep the whole weight matrix in full precision during training. On the forward pass, we first sparsify and then quantize. On the backward pass, we apply the trust mask as usual.

Figure~\ref{fig:extra}(a) illustrates the scaling law induced by the 50\% sparse + INT4 of NVIDIA Ampere~\citep{abdelkhalik2022demystifyingnvidiaamperearchitecture}, while  Figure~\ref{fig:efficiencies} (green dot) shows its parameter efficiency relative to INT and FP. 
With \methodname{}, this format can provide better scaling than FP4, but slightly inferior to INT4. 
(While this format is known as 2:4 sparsity, for INT4 + 2:4 it requires a 4:8 mask with some additional constraints.)

\subsection{Additional Experiments}
\label{sec:extra}



\paragraph{Weight-only quantization.}
In addition to the comparison with the baseline presented in Section~\ref{sec:baselines}, we present full scaling for weight-only \methodname{} quantized training. We train models with 30, 50, 100, and 200 million parameters in 1,2,3, and 4 bits in the same general setup as Figure~\ref{fig:main}. The results in Figure~\ref{fig:extra}(b) show that our approach leads to stable scaling laws in the weight-only case as well. Interestingly, here 2-bit weights appear to be Pareto-dominant, while 1-bit is surprisingly competitive with 3-bit weights. 

\paragraph{Hadamard ablation.} Finally, we examine the impact of the Hadamard transform by removing it while maintaining the trust technique,  as described in Section~\ref{sec:trust}. In Figure~\ref{fig:extra}(c), we present the results in the same setup as Figure~\ref{fig:main} for a simplified trust scheme without the Hadamard Transform. Specifically, 
1) training remains stable across all precisions, although W1A1 is now inferior to BF16; 
2) W4A4 remains Pareto-dominant, suggesting that the Hadamard transform improves the coefficients but does not alter the scaling laws. 

\vspace{-0.7em}
\section{GPU Execution Support for \methodname{} Models}
\label{sec:execution}
\vspace{-0.4em}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth, height=0.5\linewidth, keepaspectratio, trim=0 0 0 0, clip]{figures/layer_speedup}
    \vspace{-15px}
    \caption{Per-layer speedups for \methodname{} INT4 vs BF16, on a single RTX 4090 GPU. The results take into account quantization/dequantization costs for \methodname{}, and include the cost of the Hadamard transform (orange bar). We present results for the 800M 4-bit \methodname{} model we trained, as well as inference speedups for a proportional 7B-parameter model.}
    \vspace{-10px}
    \label{fig:performance}
\end{figure}

\paragraph{Kernel Overview.}
Finally, we describe GPU kernel support. 
Our forward-pass pipeline for the quantized linear layer in \methodname{} consists of three main stages:
(1) applying the Hadamard transformation to the BF16 activations,
(2) quantizing the BF16 activations into INT4 and packing them into the low-precision format, and  
(3) performing INT4 matrix multiplication on the quantized activations and weights, followed by dequantization of the result back to BF16.

For the first stage, we utilize an existing  Hadamard kernel~\citep{fht_tridao}.
We developed a custom Triton kernel for the second stage to fuse the quantization and data formatting. This kernel computes MSE-optimal group scales and performs centered quantization on the activations. It also packs the INT4 elements into UINT8, with additional intermediate results prepared for matrix multiplication and dequantization.
The third stage involves fused matrix multiplication and dequantization using our enhanced CUTLASS kernel. In this stage, both activations and weights are read and processed as integers to exploit the higher GPU throughput. The results are then dequantized back to BF16 within the same kernel. We also apply CUDA Graph end-to-end to further reduce the kernel launching overhead.

To optimize GEMM performance, we carefully tuned the CUDA thread-block and warp tile sizes and leveraged the high levels of the memory hierarchy to fuse the dequantization step before writing the results back to Global Memory in a custom CUTLASS~\emph{epilogue}.
By performing dequantization at the register level, we minimize data movement, reduce GMEM memory access overhead, and minimize the number of kernel launches.





\paragraph{Runtime Results.} 
The per-layer speedups achievable using our kernel at 4-bit precision, relative to standard 16-bit matrix multiplications on the corresponding layers, are illustrated in Figure~\ref{fig:performance}. We provide a breakdown across layers of the same shape, for  800M (which we have already trained), and a proportionally-scaled 7B model (which we plan to train in future work).  
These measurements include all auxiliary overheads (e.g. quantization/dequantization) for \methodname{}; in addition, we separate out the performance impact of the Hadamard transform. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth, height=0.5\linewidth, keepaspectratio, trim=0 -3 0 -3, clip]{figures/e2e_speedup}
    \vspace{-15px}
    \caption{End-to-end prefill speedups for \methodname{} INT4 vs BF16, across different batch sizes, using the 800M parameter model on a single RTX 4090 GPU. As expected, \methodname{} is most effective for larger batch sizes, where the workload is more compute-bound.}
    % \vspace{-15px}
    \label{fig:performance_e2e}
\end{figure}


For the smaller 800M model, the per-layer speedups vary between 1.2$\times$ (on the smallest layers, with Hadamard) and 2.4$\times$ (largest down-projection layer, no Hadamard). The largest overhead of the Hadamard transform, of around 30\%, is on the down-projection layer, which presents the largest dimension for the Hadamard. The speedups increase significantly (2.3-3.9$\times$) when we move to the 7B-parameter model, as the MatMuls are much more expensive. Figure~\ref{fig:performance_e2e} shows the end-to-end inference performance at 800M using our kernels vs. the BF16 baseline, showing speedups of 1.3-1.5$\times$ in the less memory-bound regime. 

\vspace{-1em}
\section{Discussion and Future Work}

We introduced \methodname{}, a new QAT method that achieves stable LLM training of in extremely low precision (down to 1-bit) weights and activations. Our results demonstrate that, if data and compute are appropriately scaled, 4-bit models can outperform standard-precision baselines in terms of accuracy and inference cost,  suggesting that the fundamental limits of low-precision QAT are much lower than previously thought. Further, our analysis provides new insights into the relationship between training precision and model efficiency, suggesting that low-precision may be a good target for large-scale training runs in the overtrained regime. Third, we have shown that our approach can lead to inference speedups. 
 
Several promising directions emerge for future work. First, while we demonstrated QuEST's effectiveness up to 800M parameters, its scaling behavior for much larger models is an interesting direction we plan to pursue in future work. Second, our work focused primarily on decoder-only architectures; extending QuEST to encoder-decoder models and other architectures could broaden its applicability. 


\bibliography{example_paper}
\bibliographystyle{someconference2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Additional ``Trust'' Details}

\subsection{Trust Mask Analysis}
\label{app:masks}

For the purposes of weight trust masks interpretation, we trained a 30M model over 3B tokens (11,444 iterations at bs=512) with \methodname{} weights and activations quantization to 8-bit with and without the Hadamard Transform (HT). We logged the trust masks every 500 iterations. Figure~\ref{fig:mask_proportion} shows the fraction of masked weights. We can see that adding the HT leads to an $\approx$4x decrease in the amount of masked values, corresponding to the fraction of expected clipped weights for a standard normal distribution. We can also see that without the HT the fraction deviates significantly from the expected fraction under the assumption of weights normality.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\linewidth]{figures/mask_proportion.pdf}
    \vspace{-10px} % Adjust vertical spacing as needed
    \caption{Fraction of weights for which $M_{\alpha^*} = 0$ as a function of number of training iterations for a 30M model trained with \methodname{}.}
    \label{fig:mask_proportion}
\end{figure}

Moreover, we looked at the percentage of masked elements at a fixed iteration in the past,  that remain masked at a fixed later iteration. We plot these percentages in Figure~\ref{fig:masks_persistency}. As we can see, for the run without the HT, around 69\% of masked elements at iteration 6000 (roughly halfway through training) remain masked at iteration 10000 (towards the end of the training). This percentage is more than twice as small for the run with the HT at 30\%. This implies that the HT makes masks less persistent, as expected. 
In addition, we note that weight decay is applied on all weights (including masked ones). 
Thus, a masked weight will slowly decay until it may ``exit'' the masked interval, obtaining gradient again. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.55\linewidth]{figures/masks_persistency.pdf}
    \vspace{-10px} % Adjust vertical spacing as needed
    \caption{Fraction of masked values retained from an old iteration to a new iteration for a 30M model trained with \methodname{} W8A8.}
    \label{fig:masks_persistency}
\end{figure}


\subsection{The 1-bit Case}
\label{app:outer_trust}

To determine the optimal outer trust scaling factor $s^*$, discussed in Section~\ref{sec:trust_discussion}, we conduct a sweep over $s$, varying the outer size of the outermost trust regions as $T = s\cdot\frac{\alpha^*}{2^b-1}$. The results for 1-bit, shown in Figure~\ref{fig:trust_scale}, indicate that $s^*=1.30$ for the standard \methodname{} setup and $s^*=1.25$ for the setup without the Hadamard Transform (HT), corresponding to exactly a quarter of the quantization interval. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\linewidth]{figures/trust_scale.pdf}
    \vspace{-10px} % Adjust vertical spacing as needed
    \caption{Performance of \methodname{} as a function of the outer trust scaling factor $s$ for a 30M model pretraining.}
    \label{fig:trust_scale}
\end{figure}

\subsection{Zero-shot Evaluation of QuEST Models}
To assess the effectiveness of \methodname{} beyond perplexity, we conducted a zero-shot evaluation on the HellaSWAG benchmark~\citep{zellers2019hellaswag}, which tests commonsense reasoning capabilities. We compared an 800M parameter model trained with \methodname{} in 4-bit precision against its BF16 counterpart, both of which were trained on 80B tokens.

Table~\ref{tab:hellaswag_eval} presents the accuracy results. The near-identical performance between the two models indicates that \methodname{}’s quantization-aware training is essentially lossless, preserving the model’s ability to generalize while significantly reducing precision and computational costs.

\begin{table}[h] 
    \centering \begin{tabular}{l|c} 
    \toprule 
    Model & HellaSWAG Accuracy (\%) ↑ \\
    \midrule
    BF16 (800M, 80B tokens) & 39.52 \\ 
    \methodname{} 4-bit (800M, 80B tokens) & 39.22 \\
    \bottomrule 
    \end{tabular} 
    \caption{Zero-shot evaluation on HellaSWAG comparing \methodname{} 4-bit to its BF16 counterpart. The results are nearly identical, confirming that training with \methodname{} is lossless.}
    \label{tab:hellaswag_eval}
\end{table}

\section{Additional Information about the Experimental Setup}

\subsection{Model Hyper-parameters}
\label{app:hyper_parameters}


For our experiments, we chose to use the Llama 2~\citep{touvron2023llama2openfoundation} model as the base architecture. For the attention block, this architecture utilizes multi-head attention~\citep{vaswani2023attentionneed} with rotary positional embeddings~\citep{su2023roformerenhancedtransformerrotary}. For the MLP block, it uses additional gate projection and SiLU~\citep{elfwing2017sigmoidweightedlinearunitsneural} activation function. We kept the MLP intermediate dimension equal to $8/3$ of the hidden size, padding it to 256 for increased kernel compatibility. For the AdamW optimizer, we used $\beta_1=0.90$ and $\beta_2=0.95$. We did not apply weight decay to any biases and layer normalizations. Table~\ref{tab:hyper-par} describes size-specific models and optimizer hyper-parameters for all model sizes used in this work.

\begin{table}[h]
\centering
    \begin{tabular}{l|cccccc}
    Model size       & 30M    & 50M    & 100M   & 200M   & 430M    & 800M     \\ \hline
    Num. Blocks      & 6      & 7      & 8      & 10     & 13      & 16       \\
    Hidden Size      & 640    & 768    & 1024   & 1280   & 1664    & 2048     \\
    Num. Attn. Heads & 5      & 6      & 8      & 10     & 13      & 16       \\
    Learning Rate    & 0.0012 & 0.0012 & 0.0006 & 0.0003 & 0.00015 & 0.000075 \\
    Num. Tokens      & 3B     & 5B     & 10B    & 20B    & 43B     & 80B     
    \end{tabular}
\caption{Hyper-parameters used for each model size.}
\label{tab:hyper-par}
\end{table}

\subsection{Training Stability and Convergence}
\label{app:training_stability}

Here we present the loss curves for BF16, LSQ, PACT, and \methodname{} (ours) to analyze training stability and convergence. As shown in Figure~\ref{fig:loss_curves_4b}(a), \methodname{} smoothly converges throughout training, closely tracking the BF16 baseline while consistently outperforming LSQ. Meanwhile, PACT struggles with much higher loss, indicating poor convergence. To better highlight the differences between \methodname{} and LSQ in the later stages of training, Figure~\ref{fig:loss_curves_4b}(b) focuses on steps after 1000, removing PACT for clarity. This zoomed-in view shows that \methodname{} maintains a consistently lower loss trajectory than LSQ, further reinforcing its superior stability and accuracy across training.

\begin{figure}[h]
    \centering
    \subfigure[]{
        \includegraphics[width=0.45\textwidth]{figures/loss_curves.pdf}
    }
    \hfill
    \subfigure[]{
        \includegraphics[width=0.45\textwidth]{figures/loss_curves_1000.pdf}
    }
    \caption{Training loss curves for a 30M model trained on 3B tokens with W4A4 bitwidth, comparing \methodname{} (ours), LSQ, PACT, and BF16. \textbf{(a)} Full training loss curves, showing that \methodname{} closely follows BF16 and consistently outperforms LSQ, while PACT struggles with high loss. \textbf{(b)} Zoomed-in view of training steps after 1000, excluding PACT for clarity, highlighting that \methodname{} maintains a lower loss than LSQ throughout training.}
    \label{fig:loss_curves_4b}
\end{figure}

\subsection{Hyper-parameter Search for Baseline Methods}

\begin{figure}[h]
    \centering

    \includegraphics[width=0.5\textwidth]{figures/PACT-hparam-search.png}

    \caption{Hyperparameter search for PACT on a 30M parameter model with 4-bit weights and activations, trained on 10\% of the dataset. The search explores different values for learning rate scaling (LR Scale) and alpha weight decay, with validation loss indicated by the color gradient. Lower validation loss (darker colors) corresponds to better configurations.}
    \label{fig:hps_pact}
\end{figure}

To ensure fair comparisons between \methodname{} and prior QAT methods, we conducted hyperparameter searches for both PACT and LSQ. Given PACT’s instability at lower bitwidths, we extensively tuned two key hyperparameters: weight decay and learning rate scaling $s$ for the quantization parameter $\alpha$ (i.e., $\eta_{\alpha} = s \times \eta$). Figure~\ref{fig:hps_pact} shows the loss achieved across different weight decay and LR scale values.

For LSQ, we only tuned weight decay, as the LSQ formulation already applies scaling internally to the gradient of $\alpha$, making additional learning rate adjustments unnecessary. Table~\ref{tab:hps_lsq} summarizes the results of the weight decay search across 2-bit, 3-bit, and 4-bit LSQ models, where the best-performing configuration (highlighted in bold) was used for final model comparisons.

\begin{table}[h]
    \centering 
    \begin{tabular}{l|ccc} 
        \toprule 
        Weight Decay & 2-bit PPL ↓ & 3-bit PPL ↓ & 4-bit PPL ↓ \\
        \midrule 
        0.001 & 37.02 & 31.10 & 27.93 \\
        0.01 & 36.91 & 30.89 & 27.72 \\
        0.1 & \textbf{36.54} & \textbf{30.26} & \textbf{27.51} \\
        1.0 & 38.12 & 31.16 & 28.67 \\
        \bottomrule 
    \end{tabular}
    \caption{Weight decay hyperparameter search results for LSQ across different bitwidths of 30M model. The best-performing setting is highlighted in bold.}
    \label{tab:hps_lsq}
\end{table}

Our hyperparameter search ensured that LSQ and PACT were tuned optimally before comparing against \methodname{}, leading to a fair evaluation of performance across all tested quantization methods.


\section{Scaling Laws}

\subsection{Description of the Fitting Procedure }
\label{app:fitting}


\begin{wrapfigure}{r}{0.45\textwidth}
    \centering
    \vspace{-50pt}
    \includegraphics[width=\linewidth]{figures/data_fit.pdf}
    \vspace{-15px}
    \caption{Scaling law~(\ref{eq:full_scaling_law}) fit for 3 and 4 bit QuEST with tokens/parameters ratios in $\{25, 50, 100\}$.}
    % \vspace{30pt}
    \label{fig:data_fit}
\end{wrapfigure}


As described in Section~\ref{sec:scaling_laws}, we closely follow the fitting procedure of \citet{hoffmann2022trainingcomputeoptimallargelanguage} for the scaling law (\ref{eq:full_scaling_law}) fitting. Specifically, we copied their grid of initialization given by: $\alpha \in \{0., 0.5,\dots, 2. \}$, $\beta \in \{ 0., 0.5,\dots, 2.\}$, $e \in \{-1., -.5, \dots, 1. \}$, $a \in \{0, 5, \dots, 25 \}$, and $b \in \{0, 5, \dots, 25 \}$. We also reuse their $\delta = 10^{-3}$ for the Huber loss. In addition, we fit the $\text{eff}(P)$ coefficient for a number of quantization schemes described below:
\begin{itemize}
\setlength{\itemsep}{2pt}  % Space between items
    \setlength{\parskip}{0pt}  % Space between paragraphs
    \setlength{\topsep}{-3pt}   % Space before/after the list
    \item \methodname{} for $P \in \{1,2,3,4,8\}$.
    \item Weight-only \methodname{} for $P \in \{1,2,3,4\}$.
    \item \methodname{} without the HT for $P \in \{1,2,3,4,8\}$.
    \item \methodname{} with FP4 grid.
    \item \methodname{} with 2:4 INT4.
\end{itemize}


\subsection{Analysis of the Transitory Data Regime }
\label{app:transitory}

The results in Section~\ref{sec:optimality} suggest that 4-bit training is optimal in the $D/N \to \infty$ regime. Here, we use the fitted scaling law (\ref{eq:full_scaling_law}) to verify that 4 bit is also close to optimal for $D/N$ ratios that are reasonable in practice. We formulate the question as follows: for a fixed model size (e.g. in Gb), for which amount of compute is  \methodname{} 4-bit the optimal precision?

Figure~\ref{fig:isomem} demonstrates the (predicted) dependence of performance as a function of $\frac{D}{N}\cdot\frac{16^2}{P^2}$. For BF16, this quantity becomes $D/N$. For other $P$, it ensures the same amount of training computed ($\sim ND$). As such, models there are compared at both the same size and the same training compute. We can see that 4-bit quantization becomes optimal after it passes a certain compute threshold that depends on model size. We can also see that the threshold value decreases as the model size (in Gb) grows. For a 14.0Gb model (corresponding to 7B parameters in BF16), the threshold is around $D/N\approx30$, which is significantly below the amount of data that models of that size are currently trained on (see Section~\ref{sec:scaling_laws}). For even larger models, the threshold eventually becomes less than the ``Chinchilla-optimal'' ratio of $D/N\approx 20$.  This validates that the regime in which 4-bit pre-training is optimal can, in fact, be easily achieved in practice.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/isomem.pdf}
    \vspace{-10px} % Adjust vertical spacing as needed
    \caption{Different \methodname{} precision performance as a function of tokens-to-parameters ratio at a fixed model memory footprint. The gray line indicates a 4-bit optimality threshold.}
    \label{fig:isomem}
\end{figure}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for someconference-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
