\section{Background and Related Work}
\label{sec:background}

\citet{hubara2016binarized} and~\citet{rastegari2016xnor} were among the first to consider training neural networks with highly-compressed internal states, focusing primarily on weight compression. 
Later work focused on quantization-aware training (QAT)~\citep{jacob2018qat, choi2018pact, esser2019learned, bhalgat2020lsqplus} in the form considered here, where the model weights and activations (i.e. the forward pass) are quantized, but the backward pass is performed in full-precision, using variants of the straight-through estimator (STE)~\citep{bengio2013estimating}. (The variant where all states, including gradients, are quantized~\citep{switchback, jetfire} is beyond the scope of this paper.) 


Broadly, QAT considers the problem of finding a quantized projection over a \emph{standard-precision tensor} $\mathbf{x}$, representing part of the weights or activations, minimizing output error. For symmetric uniform quantization, the projection onto \emph{the quantized tensor} $\hat{\mathbf{x}}$ is defined as:  

\begin{equation}
\hat{\mathbf{x}} = \alpha \cdot \biggl\lfloor\frac{\text{clip}(\mathbf{x}, \alpha)}{\alpha}\biggr\rceil,
\label{eqn:quant}
\end{equation}

\noindent where the $\text{clip}$ function performs a clamping operation over the value distribution for all values above the clipping parameter $\alpha$, which also acts as a scaling factor, normalizing values to $\mathbf{x}$ to $[-1, 1]$, and the function $\lfloor \cdot \rceil$ rounds each value to its nearest quantization point, defined as a uniform grid whose granularity depends on the number of available bits $b$. Most QAT methods propose to ``learn'' the factor $\alpha$, for instance, via gradient-based optimization. 
For example, QAT methods usually keep a standard-precision version $\mathbf{w}$ of the weights; the STE gradient is computed \emph{over the quantized weights} $\widehat{\mathbf{w}}$, and then added to the full-precision accumulator, possibly also updating the clipping factor $\alpha$. 

Recent work such as BitNet~\cite{wang2023bitnet, ma2024era1bitllmslarge} and Spectra~\citep{kaushal2024spectra} showed that \emph{weight-only quantization} is viable for small- and medium-scale LLMs. The concurrent work presents BitNet a4.8~\citep{wang2024bitnet48}, a hybrid scheme that combines ternary weights with mixed 4- and 8-bit activations, applied selectively to different matrices. 
In parallel,~\citet{kumar2024scaling} investigated scaling laws for GPT-type models with quantized states, concluding that the ``Pareto-optimal'' point for current QAT methods is around 8-bit weights and activations.


Prior work by~\citet{frantar2023scalinglawssparselyconnectedfoundation, jin2025journeymattersaverageparameter} studied scaling laws specifically for sparse foundation models, establishing that the loss can be stably predicted across parameter and data scales when the model weights are sparse. Recently,~\citet{frantar2025compressionscaling} generalized these laws to unify both sparsity and quantization, allowing to compare the ``effective parameter count'' for these two types of representations. Our work focuses on improved training methods for highly-compressed representations, leading to improved scaling laws relative to standard dense training, and can be applied to both sparsity and quantization.  


\vspace{-0.7em}