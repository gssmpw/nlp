@article{chen2013incoherence,
  title={Incoherence-Optimal Matrix Completion},
  author={Chen, Yudong and Jalali, Ashkan and Sanghavi, Sujay and Xu, Huan},
  journal={arXiv preprint arXiv:1310.0154},
  year={2013},
  note={Available at \url{https://arxiv.org/abs/1310.0154}}
}

@article{bengio2013estimating,
  title={Estimating or propagating gradients through stochastic neurons for conditional computation},
  author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  journal={arXiv preprint arXiv:1308.3432},
  year={2013}
}

@article{wang2024bitnet48,
  title={BitNet a4. 8: 4-bit Activations for 1-bit LLMs},
  author={Wang, Hongyu and Ma, Shuming and Wei, Furu},
  journal={arXiv preprint arXiv:2411.04965},
  year={2024}
}

@misc{ma2024era1bitllmslarge,
      title={The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits}, 
      author={Shuming Ma and Hongyu Wang and Lingxiao Ma and Lei Wang and Wenhui Wang and Shaohan Huang and Li Dong and Ruiping Wang and Jilong Xue and Furu Wei},
      year={2024},
      eprint={2402.17764},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.17764}, 
}


@misc{frantar2025compressionscaling,
      title={Compression Scaling Laws: Unifying Sparsity and Quantization}, 
      author={Elias Frantar and Utku Evci and Wonpyo Park and Neil Houlsby and Dan Alistarh},
      year={2025},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{jin2025journeymattersaverageparameter,
      title={The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws}, 
      author={Tian Jin and Ahmed Imtiaz Humayun and Utku Evci and Suvinay Subramanian and Amir Yazdanbakhsh and Dan Alistarh and Gintare Karolina Dziugaite},
      year={2025},
      eprint={2501.12486},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.12486}, 
}

@inproceedings{zellers2019hellaswag,
    title={HellaSwag: Can a Machine Really Finish Your Sentence?},
    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
    booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    year={2019}
}

@article{hubara2016binarized,
  title={Binarized neural networks},
  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}


@article{gptq,
  title={{GPTQ}: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@inproceedings{rasley2020deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}
@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@misc{deepseekv3,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}

@misc{frantar2023scalinglawssparselyconnectedfoundation,
      title={Scaling Laws for Sparsely-Connected Foundation Models}, 
      author={Elias Frantar and Carlos Riquelme and Neil Houlsby and Dan Alistarh and Utku Evci},
      year={2023},
      eprint={2309.08520},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.08520}, 
}

@misc{baines2021general,
  title={A general purpose modular PyTorch library for high performance and large scale training},
  author={Baines, M and Bhosale, S and Caggiano, V and Goyal, N and Goyal, S and Ott, M and Lefaudeux, B and Liptchinsky, V and Rabbat, M and Sheiffer, S and others},
  year={2021}
}

@article{sun2024massive,
  title={Massive activations in large language models},
  author={Sun, Mingjie and Chen, Xinlei and Kolter, J Zico and Liu, Zhuang},
  journal={arXiv preprint arXiv:2402.17762},
  year={2024}
}

@misc{nrusimha2024mitigatingimpactoutlierchannels,
      title={Mitigating the Impact of Outlier Channels for Language Model Quantization with Activation Regularization}, 
      author={Aniruddha Nrusimha and Mayank Mishra and Naigang Wang and Dan Alistarh and Rameswar Panda and Yoon Kim},
      year={2024},
      eprint={2404.03605},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.03605}, 
}

@article{spqr,
  title={Spqr: A sparse-quantized representation for near-lossless llm weight compression},
  author={Dettmers, Tim and Svirschevski, Ruslan and Egiazarian, Vage and Kuznedelev, Denis and Frantar, Elias and Ashkboos, Saleh and Borzunov, Alexander and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2306.03078},
  year={2023}
}
@article{AQLM,
  title={Extreme Compression of Large Language Models via Additive Quantization},
  author={Egiazarian, Vage and Panferov, Andrei and Kuznedelev, Denis and Frantar, Elias and Babenko, Artem and Alistarh, Dan},
  journal={arXiv preprint arXiv:2401.06118},
  year={2024}
}
@article{QuIPsharp,
  title={Quip\#: Even better LLM quantization with hadamard incoherence and lattice codebooks},
  author={Tseng, Albert and Chee, Jerry and Sun, Qingyao and Kuleshov, Volodymyr and De Sa, Christopher},
  journal={arXiv preprint arXiv:2402.04396},
  year={2024}
}
@article{outlier_suppression,
  title={Outlier suppression: Pushing the limit of low-bit transformer language models},
  author={Wei, Xiuying and Zhang, Yunchen and Zhang, Xiangguo and Gong, Ruihao and Zhang, Shanghang and Zhang, Qi and Yu, Fengwei and Liu, Xianglong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17402--17414},
  year={2022}
}

@article{doi:10.1137/060673096,
author = {Ailon, Nir and Chazelle, Bernard},
title = {The Fast Johnson–Lindenstrauss Transform and Approximate Nearest Neighbors},
journal = {SIAM Journal on Computing},
volume = {39},
number = {1},
pages = {302-322},
year = {2009},
doi = {10.1137/060673096},
}

@inproceedings{smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={International Conference on Machine Learning},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}
@article{quik,
  title={Towards end-to-end 4-bit inference on generative large language models},
  author={Ashkboos, Saleh and Markov, Ilia and Frantar, Elias and Zhong, Tingxuan and Wang, Xincheng and Ren, Jie and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2310.09259},
  year={2023}
}
@article{llm_int8,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}
@article{atom,
  title={Atom: Low-bit quantization for efficient and accurate llm serving},
  author={Zhao, Yilong and Lin, Chien-Yu and Zhu, Kan and Ye, Zihao and Chen, Lequn and Zheng, Size and Ceze, Luis and Krishnamurthy, Arvind and Chen, Tianqi and Kasikci, Baris},
  journal={arXiv preprint arXiv:2310.19102},
  year={2023}
}
@article{quarot,
  title={Quarot: Outlier-free 4-bit inference in rotated llms},
  author={Ashkboos, Saleh and Mohtashami, Amirkeivan and Croci, Maximilian L and Li, Bo and Jaggi, Martin and Alistarh, Dan and Hoefler, Torsten and Hensman, James},
  journal={arXiv preprint arXiv:2404.00456},
  year={2024}
}
@article{spinquant,
  title={SpinQuant--LLM quantization with learned rotations},
  author={Liu, Zechun and Zhao, Changsheng and Fedorov, Igor and Soran, Bilge and Choudhary, Dhruv and Krishnamoorthi, Raghuraman and Chandra, Vikas and Tian, Yuandong and Blankevoort, Tijmen},
  journal={arXiv preprint arXiv:2405.16406},
  year={2024}
}
@article{mixedprecisiontraining,
  title={Mixed precision training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  journal={arXiv preprint arXiv:1710.03740},
  year={2017}
}
@article{lmfp8,
  title={Fp8-lm: Training fp8 large language models},
  author={Peng, Houwen and Wu, Kan and Wei, Yixuan and Zhao, Guoshuai and Yang, Yuxiang and Liu, Ze and Xiong, Yifan and Yang, Ziyue and Ni, Bolin and Hu, Jingcheng and others},
  journal={arXiv preprint arXiv:2310.18313},
  year={2023}
}
@article{fishman2024scaling,
  title={Scaling FP8 training to trillion-token LLMs},
  author={Fishman, Maxim and Chmiel, Brian and Banner, Ron and Soudry, Daniel},
  journal={arXiv preprint arXiv:2409.12517},
  year={2024}
}
@article{fp8_int8,
  title={FP8 versus INT8 for efficient deep learning inference},
  author={Baalen, Mart van and Kuzmin, Andrey and Nair, Suparna S and Ren, Yuwei and Mahurin, Eric and Patel, Chirag and Subramanian, Sundar and Lee, Sanghyuk and Nagel, Markus and Soriaga, Joseph and others},
  journal={arXiv preprint arXiv:2303.17951},
  year={2023}
}
@article{switchback,
  title={Stable and low-precision training for large-scale vision-language models},
  author={Wortsman, Mitchell and Dettmers, Tim and Zettlemoyer, Luke and Morcos, Ari and Farhadi, Ali and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={10271--10298},
  year={2023}
}
@article{jetfire,
  title={Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization},
  author={Xi, Haocheng and Chen, Yuxiang and Zhao, Kang and Zheng, Kaijun and Chen, Jianfei and Zhu, Jun},
  journal={arXiv preprint arXiv:2403.12422},
  year={2024}
}
@article{chitsaz2024exploring,
  title={Exploring Quantization for Efficient Pre-Training of Transformer Language Models},
  author={Chitsaz, Kamran and Fournier, Quentin and Mordido, Gon{\c{c}}alo and Chandar, Sarath},
  journal={arXiv preprint arXiv:2407.11722},
  year={2024}
}
@article{lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@article{rosa,
  title={Rosa: Accurate parameter-efficient fine-tuning via robust adaptation},
  author={Nikdan, Mahdi and Tabesh, Soroush and Alistarh, Dan},
  journal={arXiv preprint arXiv:2401.04679},
  year={2024}
}
@article{qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{ashkboos2024computational,
  title={Computational Bottlenecks of Training Small-scale Large Language Models},
  author={Ashkboos, Saleh and Mirzadeh, Iman and Alizadeh, Keivan and Sekhavat, Mohammad Hossein and Nabi, Moin and Farajtabar, Mehrdad and Faghri, Fartash},
  journal={arXiv preprint arXiv:2410.19456},
  year={2024}
}
@inproceedings{yavne1968economical,
  title={An economical method for calculating the discrete Fourier transform},
  author={Yavne, Raytheon},
  booktitle={Proceedings of the December 9-11, 1968, fall joint computer conference, part I},
  pages={115--125},
  year={1968}
}
@article{quip,
  title={Quip: 2-bit quantization of large language models with guarantees},
  author={Chee, Jerry and Cai, Yaohui and Kuleshov, Volodymyr and De Sa, Christopher M},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@ONLINE{lora_anyscale,
author = {Artur, Niederfahrenhorst and Kourosh, Hakhamaneshi and Rehaan, Ahmad},
title = {Fine-Tuning LLMs: LoRA or Full-Parameter? An in-depth Analysis with Llama 2},
year={2023},
howpublished = {\url{https://www.anyscale.com/blog?author=rehaan-ahmad}}
}
@article{fsdp,
  title={Pytorch fsdp: experiences on scaling fully sharded data parallel},
  author={Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and others},
  journal={arXiv preprint arXiv:2304.11277},
  year={2023}
}
@article{4bitTraningTransformers,
  title={Training transformers with 4-bit integers},
  author={Xi, Haocheng and Li, Changhao and Chen, Jianfei and Zhu, Jun},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={49146--49168},
  year={2023}
}
@misc{nvidiaGB100,
   author = {NVIDIA},
   title = {NVIDIA Blackwell Architecture Technical Brief},
   year = {2024},
   howpublished = {\url{"https://resources.nvidia.com/en-us-blackwell-architecture"}}
}
@article{COATHan,
  title={COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training},
  author={Xi, Haocheng and Cai, Han and Zhu, Ligeng and Lu, Yao and Keutzer, Kurt and Chen, Jianfei and Han, Song},
  journal={arXiv preprint arXiv:2410.19313},
  year={2024}
}

@inproceedings{transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}
@ONLINE{llm_foundry,
author = {MosaicML},
title = {LLM training code for Databricks foundation models},
url = {https://github.com/mosaicml/llm-foundry},
year={2023}
}
@ONLINE{cutlass_library,
author = {NVIDIA},
title = {CUTLASS: CUDA Templates for Linear Algebra Subroutines},
url = {https://github.com/NVIDIA/cutlass},
year={2017}
}
@article{FlashAttn,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}
@inproceedings{vllm,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}
@ONLINE{FlashInfer,
author = {Flashinfer.ai},
title = {FlashInfer: Kernel Library for LLM Serving},
url = {https://github.com/flashinfer-ai/flashinfer},
year={2023}
}
@article{vaswani,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}
@incollection{backprop,
  title={Efficient backprop},
  author={LeCun, Yann and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural networks: Tricks of the trade},
  pages={9--50},
  year={2002},
  publisher={Springer}
}
@ONLINE{hadamard_kernels,
author = {Dao, Tri},
title = {Fast Hadamard transform in CUDA, with a PyTorch interface},
url = {https://github.com/Dao-AILab/fast-hadamard-transform},
year={2023}
}
@ONLINE{cub_library,
author = {NVIDIA},
title = {Cooperative primitives for CUDA C++},
url = {https://github.com/NVIDIA/cub/tree/main},
year={2020}
}

@online{fnv_hash,
  author = {Noll, Landon C},
  title = {FNV Hash},
  year = 1991,
  url = {http://www.isthe.com/chongo/tech/comp/fnv/index.html},
  urldate = {2024-10-30}
}
@online{hadamard_dao,
  author = {Dao, Tri},
  title = {Fast Hadamard Transform in CUDA},
  year = 2023,
  url = {https://github.com/Dao-AILab/fast-hadamard-transform},
  urldate = {2024-10-30}
}


@article{c4,
    author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
    journal = {arXiv e-prints},
    year = {2019},
    archivePrefix = {arXiv},
    eprint = {1910.10683},
}

@inproceedings{viggo,
    title = "{V}i{GGO}: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation",
    author = "Juraska, Juraj  and
      Bowden, Kevin  and
      Walker, Marilyn",
    booktitle = "Proceedings of the 12th International Conference on Natural Language Generation",
    month = oct # "{--}" # nov,
    year = "2019",
    address = "Tokyo, Japan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-8623",
    doi = "10.18653/v1/W19-8623",
    pages = "164--172",
}

@article{gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{higgs,
  title={Pushing the Limits of Large Language Model Quantization via the Linearity Theorem},
  author={Malinovskii, Vladimir and Panferov, Andrei and Ilin, Ivan and Guo, Han and Richt{\'a}rik, Peter and Alistarh, Dan},
  journal={arXiv preprint arXiv:2411.17525},
  year={2024}
}

@article{choi2018pact,
  title={Pact: Parameterized clipping activation for quantized neural networks},
  author={Choi, Jungwook and Wang, Zhuo and Venkataramani, Swagath and Chuang, Pierce I-Jen and Srinivasan, Vijayalakshmi and Gopalakrishnan, Kailash},
  journal={arXiv preprint arXiv:1805.06085},
  year={2018}
}

@misc{suresh2017distributedmeanestimationlimited,
      title={Distributed Mean Estimation with Limited Communication}, 
      author={Ananda Theertha Suresh and Felix X. Yu and Sanjiv Kumar and H. Brendan McMahan},
      year={2017},
      eprint={1611.00429},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.00429}, 
}


@article{kumar2024scaling,
  title={Scaling laws for precision},
  author={Kumar, Tanishq and Ankner, Zachary and Spector, Benjamin F and Bordelon, Blake and Muennighoff, Niklas and Paul, Mansheej and Pehlevan, Cengiz and R{\'e}, Christopher and Raghunathan, Aditi},
  journal={arXiv preprint arXiv:2411.04330},
  year={2024}
}


@article{kaushal2024spectra,
  title={Spectra: Surprising effectiveness of pretraining ternary language models at scale},
  author={Kaushal, Ayush and Vaidhya, Tejas and Mondal, Arnab Kumar and Pandey, Tejas and Bhagat, Aaryan and Rish, Irina},
  journal={arXiv preprint arXiv:2407.12327},
  year={2024}
}

@article{wang2023bitnet,
  title={Bitnet: Scaling 1-bit transformers for large language models},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Wang, Huaijie and Ma, Lingxiao and Yang, Fan and Wang, Ruiping and Wu, Yi and Wei, Furu},
  journal={arXiv preprint arXiv:2310.11453},
  year={2023}
}

@article{esser2019learned,
  title={Learned step size quantization},
  author={Esser, Steven K and McKinstry, Jeffrey L and Bablani, Deepika and Appuswamy, Rathinakumar and Modha, Dharmendra S},
  journal={arXiv preprint arXiv:1902.08153},
  year={2019}
}

@InProceedings{bhalgat2020lsqplus,
author = {Bhalgat, Yash and Lee, Jinwon and Nagel, Markus and Blankevoort, Tijmen and Kwak, Nojun},
title = {LSQ+: Improving Low-Bit Quantization Through Learnable Offsets and Better Initialization},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
month = {June},
year = {2020}
}

@article{sql1,
  author    = {Victor Zhong and
               Caiming Xiong and
               Richard Socher},
  title     = {Seq2SQL: Generating Structured Queries from Natural Language using
               Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1709.00103},
  year      = {2017}
}

@InProceedings{jacob2018qat,
author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
title = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@article{sql2,
  title={Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task},
  author={Yu, Tao and Zhang, Rui and Yang, Kai and Yasunaga, Michihiro and Wang, Dongxu and Li, Zifan and Ma, James and Li, Irene and Yao, Qingning and Roman, Shanelle and others},
  journal={arXiv preprint arXiv:1809.08887},
  year={2018}
}

@article{lin2024awq,
  title={AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={87--100},
  year={2024}
}

@inproceedings{rastegari2016xnor,
  title={Xnor-net: Imagenet classification using binary convolutional neural networks},
  author={Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  booktitle={European conference on computer vision},
  pages={525--542},
  year={2016},
  organization={Springer}
}

@misc{dodge2021documentinglargewebtextcorpora,
      title={Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus}, 
      author={Jesse Dodge and Maarten Sap and Ana Marasović and William Agnew and Gabriel Ilharco and Dirk Groeneveld and Margaret Mitchell and Matt Gardner},
      year={2021},
      eprint={2104.08758},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.08758}, 
}

@misc{loshchilov2019decoupledweightdecayregularization,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
}

@misc{touvron2023llama2openfoundation,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}

@misc{hoffmann2022trainingcomputeoptimallargelanguage,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.15556}, 
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{su2023roformerenhancedtransformerrotary,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2023},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.09864}, 
}

@misc{elfwing2017sigmoidweightedlinearunitsneural,
      title={Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning}, 
      author={Stefan Elfwing and Eiji Uchibe and Kenji Doya},
      year={2017},
      eprint={1702.03118},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1702.03118}, 
}

@misc{abdelkhalik2022demystifyingnvidiaamperearchitecture,
      title={Demystifying the Nvidia Ampere Architecture through Microbenchmarking and Instruction-level Analysis}, 
      author={Hamdy Abdelkhalik and Yehia Arafa and Nandakishore Santhi and Abdel-Hameed Badawy},
      year={2022},
      eprint={2208.11174},
      archivePrefix={arXiv},
      primaryClass={cs.AR},
      url={https://arxiv.org/abs/2208.11174}, 
}

@article{10.1214/aoms/1177703732,
    author = {Peter J. Huber},
    title = {{Robust Estimation of a Location Parameter}},
    volume = {35},
    journal = {The Annals of Mathematical Statistics},
    number = {1},
    publisher = {Institute of Mathematical Statistics},
    pages = {73 -- 101},
    year = {1964},
    doi = {10.1214/aoms/1177703732},
    URL = {https://doi.org/10.1214/aoms/1177703732}
}

@misc{fht_tridao, 
  title={Fast Hadamard Transform in CUDA, with a PyTorch interface.},
  author={Tri Dao,  Nikos Karampatziakis, Hungyueh Chiang},
  url={https://github.com/Dao-AILab/fast-hadamard-transform},
}

@article{mxspec,
  title="{OCP Microscaling (MX) Specification}",
  author={Darvish Rouhani, Bita and Garegrat, Nitin and Savell, Tom and More, Ankit and Han, Kyung-Nam and Zhao, Ritchie amd Hall, Mathew and Klar, Jasmine and Chung, Eric and Yu, Yuan and Schulte, Michael and Wittig, Ralph and Bratt, Ian and Stephens, Nigel and Milanovic, Jelena and Brothers, John and Dubey, Pradeep and Cornea, Marius and Heinecke, Alexander and Rodriguez, Andres and Langhammer, Martin and Deng, Summer and Naumov, Maxim and Micikevicius, Paulius and Siu, Michael and Verrilli, Colin},
  journal={Open Compute Project},
  year={2023}
}
