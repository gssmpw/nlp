[
  {
    "index": 0,
    "papers": [
      {
        "key": "hubara2016binarized",
        "author": "Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua",
        "title": "Binarized neural networks"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "rastegari2016xnor",
        "author": "Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali",
        "title": "Xnor-net: Imagenet classification using binary convolutional neural networks"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "jacob2018qat",
        "author": "Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry",
        "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"
      },
      {
        "key": "choi2018pact",
        "author": "Choi, Jungwook and Wang, Zhuo and Venkataramani, Swagath and Chuang, Pierce I-Jen and Srinivasan, Vijayalakshmi and Gopalakrishnan, Kailash",
        "title": "Pact: Parameterized clipping activation for quantized neural networks"
      },
      {
        "key": "esser2019learned",
        "author": "Esser, Steven K and McKinstry, Jeffrey L and Bablani, Deepika and Appuswamy, Rathinakumar and Modha, Dharmendra S",
        "title": "Learned step size quantization"
      },
      {
        "key": "bhalgat2020lsqplus",
        "author": "Bhalgat, Yash and Lee, Jinwon and Nagel, Markus and Blankevoort, Tijmen and Kwak, Nojun",
        "title": "LSQ+: Improving Low-Bit Quantization Through Learnable Offsets and Better Initialization"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "bengio2013estimating",
        "author": "Bengio, Yoshua and L{\\'e}onard, Nicholas and Courville, Aaron",
        "title": "Estimating or propagating gradients through stochastic neurons for conditional computation"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "switchback",
        "author": "Wortsman, Mitchell and Dettmers, Tim and Zettlemoyer, Luke and Morcos, Ari and Farhadi, Ali and Schmidt, Ludwig",
        "title": "Stable and low-precision training for large-scale vision-language models"
      },
      {
        "key": "jetfire",
        "author": "Xi, Haocheng and Chen, Yuxiang and Zhao, Kang and Zheng, Kaijun and Chen, Jianfei and Zhu, Jun",
        "title": "Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wang2023bitnet",
        "author": "Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Wang, Huaijie and Ma, Lingxiao and Yang, Fan and Wang, Ruiping and Wu, Yi and Wei, Furu",
        "title": "Bitnet: Scaling 1-bit transformers for large language models"
      },
      {
        "key": "ma2024era1bitllmslarge",
        "author": "Shuming Ma and Hongyu Wang and Lingxiao Ma and Lei Wang and Wenhui Wang and Shaohan Huang and Li Dong and Ruiping Wang and Jilong Xue and Furu Wei",
        "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "kaushal2024spectra",
        "author": "Kaushal, Ayush and Vaidhya, Tejas and Mondal, Arnab Kumar and Pandey, Tejas and Bhagat, Aaryan and Rish, Irina",
        "title": "Spectra: Surprising effectiveness of pretraining ternary language models at scale"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wang2024bitnet48",
        "author": "Wang, Hongyu and Ma, Shuming and Wei, Furu",
        "title": "BitNet a4. 8: 4-bit Activations for 1-bit LLMs"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "kumar2024scaling",
        "author": "Kumar, Tanishq and Ankner, Zachary and Spector, Benjamin F and Bordelon, Blake and Muennighoff, Niklas and Paul, Mansheej and Pehlevan, Cengiz and R{\\'e}, Christopher and Raghunathan, Aditi",
        "title": "Scaling laws for precision"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "frantar2023scalinglawssparselyconnectedfoundation",
        "author": "Elias Frantar and Carlos Riquelme and Neil Houlsby and Dan Alistarh and Utku Evci",
        "title": "Scaling Laws for Sparsely-Connected Foundation Models"
      },
      {
        "key": "jin2025journeymattersaverageparameter",
        "author": "Tian Jin and Ahmed Imtiaz Humayun and Utku Evci and Suvinay Subramanian and Amir Yazdanbakhsh and Dan Alistarh and Gintare Karolina Dziugaite",
        "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "frantar2025compressionscaling",
        "author": "Elias Frantar and Utku Evci and Wonpyo Park and Neil Houlsby and Dan Alistarh",
        "title": "Compression Scaling Laws: Unifying Sparsity and Quantization"
      }
    ]
  }
]