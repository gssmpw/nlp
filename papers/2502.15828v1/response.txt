\section{Related Works}
\subsection{LoRA and LoRA Variants}
LoRA**Jaiswal, "Efficient Fine-Tuning with Low-Rank Adaptation"** decomposes a full-rank matrix into a product of two low-rank matrices, which has been widely considered an effective solution for parameter-efficient fine-tuning. Studies have proposed several variants to reform LoRA: For initialization, PISSA**Liu et al., "Principal Singular Components for Efficient Fine-Tuning"** leverages singular value decomposition (SVD) to obtain the principal singular components of $W$, while MiLoRA**Tang et al., "Multi-Task Learning with Low-Rank Adaptation"** utilizes secondary singular values and vectors. LoRA-Pro**Wu et al., "Efficient Fine-Tuning with Proximal Gradient Descent"** and LoRA-GA**Chen et al., "Gradient Alignment for Efficient Fine-Tuning"** approximate the direction of initial gradients to align them with that of the fully fine-tuning. LoRA+**Zhang, "Learning Rate Separation for Efficient Fine-Tuning"** introduces a learning rate separating strategy with $\eta_B > \eta_A$. ResLoRA**Li et al., "Residual Paths for Efficient Fine-Tuning"** and SIBO**Kim et al., "Sparse Incremental Backpropagation for Efficient Fine-Tuning"** accelerate convergence and mitigate over-smoothing by introducing residual paths. DoRA**Peng et al., "Direction and Magnitude Decomposition for Efficient Fine-Tuning"** decomposes the weight vector into direction and magnitude and only uses its direction component. rsLoRA**Wang et al., "Rank-Stabilized Scaling Factor for Efficient Fine-Tuning"** proposes a rank-stabilized scaling factor $\lambda_t = r_t^{1/2}$ to ensure stable gradient updates. To prevent overfitting, BiLoRA**Huang et al., "Bi-Level Optimization for Overfitting Prevention"** adopts a bi-level optimizing strategy, while others implement dropout mechanisms**Li, "Dropout Mechanisms for Regularization"**.

\subsection{Mixture of LoRAs}
MoE has emerged as a critical framework for addressing complex tasks. By incorporating multiple expert modules, it dynamically selects appropriate experts based on specific inputs**Huang et al., "Multi-Expert Framework for Efficient Fine-Tuning"**. Early studies, such as LoRAMoE**Zhang et al., "LoRA-based Multi-Expert Framework"** and MixLoRA**Li et al., "Mixture of Low-Rank Adaptation"**, have pioneered the introduction of the MoE-LoRA architecture by integrating LoRA experts for both global and downstream tasks. Afterward, MoE-LoRA has demonstrated its effectiveness across a range of fields such as continual learning**Huang et al., "Continual Learning with MoE-LoRA"**, vision-language multi-model tasks**Li et al., "Vision-Language Multi-Model Tasks with MoE-LoRA"**, and multi-task applications**Zhang et al., "Multi-Task Applications with MoE-LoRA"**.

Recent studies have focused on enhancing MoE-LoRA through architectural advancements and improved training strategies. For instance, MoLA**Wu et al., "Mixture of Lightweight Experts for Efficient Fine-Tuning"** allocates a varying number of experts at different layers, and MixDA**Chen et al., "Mixture of Domain-Adaptive Experts"** introduces multiple domain-adaptive modules to support multi-domain knowledge. Other methods such as **Li et al., "Multi-Domain Expert Learning"** have also been proposed for strengthening MoE-LoRA. To boost the training of MoE-LoRA, Luo et al.**Luo et al., "Contrastive Loss for Efficient Fine-Tuning"** address the random routing issue by introducing a contrastive loss. At the same time, MoV**Wang et al., "Multi-Variate Expert Selection Mechanism"** chooses to combine lightweight vectors with a sparse selection mechanism for efficient expert allocation. Other approaches, including **Zhang et al., "Load Balancing among Experts"**, focus on load balancing among experts. However, to the best of our knowledge, there is still a lack of work on gradient optimizing specifically for MoE-LoRA models. 

\subsection{Gradient Preconditioners}
In most deep learning cases, gradient descent algorithms update model parameters by calculating gradient-based updates. To accelerate the optimizing process, the concept of gradient preconditioning has been introduced. Advanced techniques such as Adagrad**Duchi et al., "Adaptive Subgradient Methods for Online Learning"** dynamically adjust the learning rate by an accumulated squared gradients \( G_t = \sum_{i=1}^{t} g_i^2 \) and update model by \( \Delta \theta_t = -\eta G_t^{-1/2} \cdot g_t \). Adam**Kingma et al., "Adam: A Method for Stochastic Optimization"** extends this approach by incorporating momentum and bias correction, scaling gradients through a diagonal preconditioner, and resulting in updates in the form of \( \Delta \theta_t = -\eta \frac{m_t}{\sqrt{v_t} + \epsilon} \), where \( v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \). AdamW**Loshchilov et al., "Decoupled Weight Decay Regularization"** further introduces a weight decay to Adam. 

Recent studies have provided theoretical support for scaled gradient descent methods under different preconditioning strategies. The core idea is to adjust both the direction and magnitude of updates by introducing a scaling matrix to gradients.  
Tong et al.**Tong et al., "Local Convergence of Scaled Gradient Descent"** demonstrate the local convergence of scaled gradient descent methods. Jia et al.**Jia et al., "Global Convergence of Scaled Gradient Descent"** extend this work by proving global convergence of scaled gradient descent for the least-squares matrix decomposition problem \( \|AB^T - Y\|_F^2 / 2 \), showing that this approach achieves global convergence under different condition numbers. Other variants of scaled gradient descent have also emerged, such as Zhang et al. who proposed two regularization strategies**Zhang et al., "Two Regularization Strategies for Scaled Gradient Descent"**. In higher-dimensional settings, scaled gradient descent has been further extended to tensor optimization**Mishra et al., "Tensor Optimization with Scaled Gradient Descent"**. Mishra et al.**Mishra et al., "Riemannian Preconditioners for Low-Rank Matrices"** also applied the principles of Riemannian to the optimization involving low-rank matrices. Considering the data's manifold geometry, a Riemannian metric $g_p(v, w)$ is introduced to guide gradient updates along the manifold. Recently, Zhang et al.**Zhang et al., "Riemannian Preconditioners for LoRA"** introduced the idea of Riemannian preconditioners to LoRA by attaching an \( r \times r \) preconditioner to the gradients of low-rank matrices. As a result, they provide improved fine-tuning performance of LoRA, compared with conventional gradient optimizers such as SGD and AdamW