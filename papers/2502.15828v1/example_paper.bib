@article{zhang2025parameter,
  title={Parameter-Efficient Fine-Tuning for Foundation Models},
  author={Zhang, Dan and Feng, Tao and Xue, Lilong and Wang, Yuandong and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2501.13787},
  year={2025}
}

@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}

@article{moe,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}

@article{lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{loramoe,
  title={LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin},
  author={Dou, Shihan and Zhou, Enyu and Liu, Yan and Gao, Songyang and Shen, Wei and Xiong, Limao and Zhou, Yuhao and Wang, Xiao and Xi, Zhiheng and Fan, Xiaoran and others},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1932--1945},
  year={2024}
}

@article{mixlora,
  title={Mixlora: Enhancing large language models fine-tuning with lora based mixture of experts},
  author={Li, Dengchun and Ma, Yingzi and Wang, Naizheng and Cheng, Zhiyuan and Duan, Lei and Zuo, Jie and Yang, Cal and Tang, Mingjie},
  journal={arXiv preprint arXiv:2404.15159},
  year={2024}
}

@article{mola,
  title={Higher layers need more lora experts},
  author={Gao, Chongyang and Chen, Kezhen and Rao, Jinmeng and Sun, Baochen and Liu, Ruibo and Peng, Daiyi and Zhang, Yawen and Guo, Xiaoyuan and Yang, Jie and Subrahmanian, VS},
  journal={arXiv preprint arXiv:2402.08562},
  year={2024}
}

@article{mora,
  title={MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning},
  author={Jiang, Ting and Huang, Shaohan and Luo, Shengyue and Zhang, Zihan and Huang, Haizhen and Wei, Furu and Deng, Weiwei and Sun, Feng and Zhang, Qi and Wang, Deqing and others},
  journal={arXiv preprint arXiv:2405.12130},
  year={2024}
}

@article{llava,
  title={Llava-mole: Sparse mixture of lora experts for mitigating data conflicts in instruction finetuning mllms},
  author={Chen, Shaoxiang and Jie, Zequn and Ma, Lin},
  journal={arXiv preprint arXiv:2401.16160},
  year={2024}
}

@article{MOElora,
  title={Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications},
  author={Liu, Qidong and Wu, Xian and Zhao, Xiangyu and Zhu, Yuanshao and Xu, Derong and Tian, Feng and Zheng, Yefeng},
  journal={arXiv preprint arXiv:2310.18339},
  year={2023}
}

@article{mov,
  title={Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning},
  author={Zadouri, Ted and {\"U}st{\"u}n, Ahmet and Ahmadian, Arash and Ermi{\c{s}}, Beyza and Locatelli, Acyr and Hooker, Sara},
  journal={arXiv preprint arXiv:2309.05444},
  year={2023}
}

@article{mole,
  title={Mixture of lora experts},
  author={Wu, Xun and Huang, Shaohan and Wei, Furu},
  journal={arXiv preprint arXiv:2404.13628},
  year={2024}
}

@article{MoEloraa,
  title={Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models},
  author={Luo, Tongxu and Lei, Jiahe and Lei, Fangyu and Liu, Weihao and He, Shizhu and Zhao, Jun and Liu, Kang},
  journal={arXiv preprint arXiv:2402.12851},
  year={2024}
}

@article{mocle,
  title={Mixture of cluster-conditional lora experts for vision-language instruction tuning},
  author={Gou, Yunhao and Liu, Zhili and Chen, Kai and Hong, Lanqing and Xu, Hang and Li, Aoxue and Yeung, Dit-Yan and Kwok, James T and Zhang, Yu},
  journal={arXiv preprint arXiv:2312.12379},
  year={2023}
}

@article{adamix,
  title={AdaMix: Mixture-of-adaptations for parameter-efficient model tuning},
  author={Wang, Yaqing and Agarwal, Sahaj and Mukherjee, Subhabrata and Liu, Xiaodong and Gao, Jing and Awadallah, Ahmed Hassan and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2205.12410},
  year={2022}
}

@article{sira,
  title={Sira: Sparse mixture of low rank adaptation},
  author={Zhu, Yun and Wichers, Nevan and Lin, Chu-Cheng and Wang, Xinyi and Chen, Tianlong and Shu, Lei and Lu, Han and Liu, Canoee and Luo, Liangchen and Chen, Jindong and others},
  journal={arXiv preprint arXiv:2311.09179},
  year={2023}
}

@article{adagrad,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{tong2021low,
  title={Low-rank matrix recovery with scaled subgradient methods: Fast and robust convergence without the condition number},
  author={Tong, Tian and Ma, Cong and Chi, Yuejie},
  journal={IEEE Transactions on Signal Processing},
  volume={69},
  pages={2396--2409},
  year={2021},
  publisher={IEEE}
}

@article{jia2024preconditioning,
  title={Preconditioning matters: Fast global convergence of non-convex matrix factorization via scaled gradient descent},
  author={Jia, Xixi and Wang, Hailin and Peng, Jiangjun and Feng, Xiangchu and Meng, Deyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhang2023preconditioned,
  title={Preconditioned Gradient Descent for Overparameterized Nonconvex Burer--Monteiro Factorization with Global Optimality Certification},
  author={Zhang, Gavin and Fattahi, Salar and Zhang, Richard Y},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={163},
  pages={1--55},
  year={2023}
}

@inproceedings{zhang2024fast,
  title={Fast and Accurate Estimation of Low-Rank Matrices from Noisy Measurements via Preconditioned Non-Convex Gradient Descent},
  author={Zhang, Jialun and Zhang, Richard Y and Chiu, Hong-Ming},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3772--3780},
  year={2024},
  organization={PMLR}
}

@article{tong2022scaling,
  title={Scaling and scalability: Provable nonconvex low-rank tensor estimation from incomplete measurements},
  author={Tong, Tian and Ma, Cong and Prater-Bennette, Ashley and Tripp, Erin and Chi, Yuejie},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={163},
  pages={1--77},
  year={2022}
}

@article{ma2023provably,
  title={Provably Accelerating Ill-Conditioned Low-rank Estimation via Scaled Gradient Descent, Even with Overparameterization},
  author={Ma, Cong and Xu, Xingyu and Tong, Tian and Chi, Yuejie},
  journal={arXiv preprint arXiv:2310.06159},
  year={2023}
}

@article{mishra2016riemannian,
  title={Riemannian preconditioning},
  author={Mishra, Bamdev and Sepulchre, Rodolphe},
  journal={SIAM Journal on Optimization},
  volume={26},
  number={1},
  pages={635--660},
  year={2016},
  publisher={SIAM}
}

@article{mishra2013low,
  title={Low-rank optimization with trace norm penalty},
  author={Mishra, Bamdev and Meyer, Gilles and Bach, Francis and Sepulchre, Rodolphe},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2124--2149},
  year={2013},
  publisher={SIAM}
}

@article{pissa,
  title={Pissa: Principal singular values and singular vectors adaptation of large language models},
  author={Meng, Fanxu and Wang, Zhaohui and Zhang, Muhan},
  journal={arXiv preprint arXiv:2404.02948},
  year={2024}
}

@article{milora,
  title={MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning},
  author={Wang, Hanqing and Xiao, Zeguan and Li, Yixia and Wang, Shuo and Chen, Guanhua and Chen, Yun},
  journal={arXiv preprint arXiv:2406.09044},
  year={2024}
}

@article{lora-ga,
  title={LoRA-GA: Low-Rank Adaptation with Gradient Approximation},
  author={Wang, Shaowen and Yu, Linxi and Li, Jian},
  journal={arXiv preprint arXiv:2407.05000},
  year={2024}
}

@article{lora+,
  title={Lora+: Efficient low rank adaptation of large models},
  author={Hayou, Soufiane and Ghosh, Nikhil and Yu, Bin},
  journal={arXiv preprint arXiv:2402.12354},
  year={2024}
}

@article{reslora,
  title={Reslora: Identity residual mapping in low-rank adaption},
  author={Shi, Shuhua and Huang, Shaohan and Song, Minghui and Li, Zhoujun and Zhang, Zihan and Huang, Haizhen and Wei, Furu and Deng, Weiwei and Sun, Feng and Zhang, Qi},
  journal={arXiv preprint arXiv:2402.18039},
  year={2024}
}

@article{sibo,
  title={SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning},
  author={Wen, Zhihao and Zhang, Jie and Fang, Yuan},
  journal={arXiv preprint arXiv:2402.11896},
  year={2024}
}

@article{dora,
  title={Dora: Weight-decomposed low-rank adaptation},
  author={Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
  journal={arXiv preprint arXiv:2402.09353},
  year={2024}
}

@article{rslora,
  title={A rank stabilization scaling factor for fine-tuning with lora},
  author={Kalajdzievski, Damjan},
  journal={arXiv preprint arXiv:2312.03732},
  year={2023}
}

@article{lorapro,
  title={LoRA-Pro: Are Low-Rank Adapters Properly Optimized?},
  author={Wang, Zhengbo and Liang, Jian},
  journal={arXiv preprint arXiv:2407.18242},
  year={2024}
}

@article{bilora,
  title={BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models},
  author={Qiang, Rushi and Zhang, Ruiyi and Xie, Pengtao},
  journal={arXiv preprint arXiv:2403.13037},
  year={2024}
}

@article{hiddenkey,
  title={LoRA Meets Dropout under a Unified Framework},
  author={Wang, Sheng and Chen, Liheng and Jiang, Jiyue and Xue, Boyang and Kong, Lingpeng and Wu, Chuan},
  journal={arXiv preprint arXiv:2403.00812},
  year={2024}
}

@article{loradropout,
  title={Lora dropout as a sparsity regularizer for overfitting control},
  author={Lin, Yang and Ma, Xinyu and Chu, Xu and Jin, Yujie and Yang, Zhibang and Wang, Yasha and Mei, Hong},
  journal={arXiv preprint arXiv:2404.09610},
  year={2024}
}

@article{riemannian,
  title={Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models},
  author={Zhang, Fangzhao and Pilanci, Mert},
  journal={arXiv preprint arXiv:2402.02347},
  year={2024}
}

@article{mao2024survey,
  title={A survey on lora of large language models},
  author={Mao, Yuren and Ge, Yuhang and Fan, Yijiang and Xu, Wenyi and Mi, Yu and Hu, Zhonghao and Gao, Yunjun},
  journal={arXiv preprint arXiv:2407.11046},
  year={2024}
}

@article{yang2024moral,
  title={MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning},
  author={Yang, Shu and Ali, Muhammad Asif and Wang, Cheng-Long and Hu, Lijie and Wang, Di},
  journal={arXiv preprint arXiv:2402.11260},
  year={2024}
}

@article{mixda,
  title={Mixture-of-domain-adapters: Decoupling and injecting domain knowledge to pre-trained language models memories},
  author={Diao, Shizhe and Xu, Tianyang and Xu, Ruijia and Wang, Jiawei and Zhang, Tong},
  journal={arXiv preprint arXiv:2306.05406},
  year={2023}
}

@article{moslora,
  title={Mixture-of-Subspaces in Low-Rank Adaptation},
  author={Wu, Taiqiang and Wang, Jiahao and Zhao, Zhe and Wong, Ngai},
  journal={arXiv preprint arXiv:2406.11909},
  year={2024}
}

@article{lu2022scienceqa,
  title={Learn to explain: Multimodal reasoning via thought chains for science question answering},
  author={Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2507--2521},
  year={2022}
}

@article{talmor2018commonsenseqa,
  title={Commonsenseqa: A question answering challenge targeting commonsense knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  journal={arXiv preprint arXiv:1811.00937},
  year={2018}
}

@article{mihaylov2018openbookqa,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}

@article{sap2019siqa,
  title={Socialiqa: Commonsense reasoning about social interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09728},
  year={2019}
}

@article{wang2018glue,
  title={Glue: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@misc{glm2024chatglm,
      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, 
      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},
      year={2024},
      eprint={2406.12793},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@article{tian2024hydralora,
  title={HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning},
  author={Tian, Chunlin and Shi, Zhan and Guo, Zhijiang and Li, Li and Xu, Chengzhong},
  journal={arXiv preprint arXiv:2404.19245},
  year={2024}
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@inproceedings{zhu2016visual7w,
  title={Visual7w: Grounded question answering in images},
  author={Zhu, Yuke and Groth, Oliver and Bernstein, Michael and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4995--5004},
  year={2016}
}

@article{zhang2025automated,
  title={Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation},
  author={Zhang, Yuhui and Su, Yuchang and Liu, Yiming and Wang, Xiaohan and Burgess, James and Sui, Elaine and Wang, Chenyu and Aklilu, Josiah and Lozano, Alejandro and Wei, Anjiang and others},
  journal={arXiv preprint arXiv:2501.03225},
  year={2025}
}

@article{krishna2017visual,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={International journal of computer vision},
  volume={123},
  pages={32--73},
  year={2017},
  publisher={Springer}
}