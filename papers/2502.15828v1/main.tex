

\documentclass{article}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}

\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\usepackage{siunitx}
\usepackage{booktabs}
\usepackage[most]{tcolorbox}

\definecolor{darkgrey}{RGB}{100,100,100}
\definecolor{darkgreen}{rgb}{0,0.35,0}

\icmltitlerunning{A Stronger Mixture of Low-Rank Experts for Fine-Tuning Foundation Models}

\begin{document}

\twocolumn[
\icmltitle{A Stronger Mixture of Low-Rank Experts for Fine-Tuning Foundation Models}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Mengyang Sun}{thu,zhipu}
\icmlauthor{Yihao Wang}{bxk}
\icmlauthor{Tao Feng}{thu}
\icmlauthor{Dan Zhang}{thu,zhipu}
\icmlauthor{Yifan Zhu}{by}
\icmlauthor{Jie Tang}{thu}
\end{icmlauthorlist}

\icmlaffiliation{thu}{Department of Computer Science and Technology, Tsinghua University, Beijing, China;}
\icmlaffiliation{bxk}{Computer School, Beijing Information Science and Technology University, Beijing, China;}
\icmlaffiliation{by}{School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China}
\icmlaffiliation{zhipu}{The work was done while these authors interned at Zhipu AI;}

\icmlkeywords{Large Language Models, Parameter-Efficient Fine-Tuning, Low-Rank Adaptation, Mixture of Experts, Riemannian Preconditioners}

\vskip 0.3in
]

\printAffiliationsAndNotice{}  

\begin{abstract}
In order to streamline the fine-tuning of foundation models, Low-Rank Adapters (LoRAs) have been substantially adopted across various fields, including instruction tuning and domain adaptation. The underlying concept of LoRA involves decomposing a full-rank matrix into the product of two lower-rank matrices, which reduces storage consumption and accelerates the training process. Furthermore, to address the limited expressive capacity of LoRA, the Mixture-of-Expert (MoE) has been introduced for incorporating multiple LoRA adapters. The integration of LoRA experts leads to a visible improvement across several downstream scenes. However, the mixture of LoRAs (MoE-LoRA) still exhibits its low robustness during tuning and inferring. Inspired by the Riemannian Preconditioners which train LoRA as a sub-space projector, we propose a new training strategy for MoE-LoRA, to stabilize and boost its feature learning procedure by multi-space projections. Examinations on SGD and AdamW optimizers demonstrate the effectiveness of our methodology. Source code is available at~\url{https://github.com/THUDM/MoELoRA_Riemannian}.
\end{abstract}
\vspace{-20pt}
\section{Introduction}
\label{sec:intro}
Parameter-Efficient Fine-Tuning (PEFT) techniques offer a cost-effective solution for fine-tuning foundation models (FMs)~\cite{zhang2025parameter}. Among these, Low-Rank Adaptation (LoRA) is a prevalent technology due to its versatility and simplicity. In detail, LoRA introduces trainable low-rank matrices $A$ and $B$ to update the internal modules of FMs, which is given by $X=W+BA$. In a sense, their product serves as an approximation of the full-rank update for the pre-trained weights. While LoRA significantly reduces the number of trainable parameters, it also imposes two limitations: limited representation and gradient sub-optimality.

\textbf{\textit{Limitation 1:}} \textit{Limited representation.} A natural problem of low-rank matrices lies in less powerful representation, especially in complex tasks. To tackle this, one straightforward solution is the integration of multiple LoRA modules into the mixture-of-expert framework, known as MoE-LoRA. Figure \ref{overall_graph} (left) illustrates a plain MoE-LoRA framework. These efforts tangibly improved the performance of LoRA in many scenarios, like vision-language tasks, multi-task learning, continual learning, etc. In a nutshell, the route of MoE-LoRA can be roughly categorized into two lines: (i) Designing dedicated MoE-LoRA frameworks for specific domains, such as MOELoRA~\cite{MOElora} and MoCLE~\cite{mocle}. (ii) Technically improving MoE-LoRA via architectural, updating, and loss constraints, such as MoLA~\cite{mola} and HydraLoRA~\cite{tian2024hydralora}. 
Nevertheless, most of these efforts fail to consider the instability and inefficiency of training MoE-LoRA.

\textbf{\textit{Limitation 2:}} \textit{Gradient Sub-optimality.} Another concern that plagues LoRA is gradient sub-optimality. This occurs since the low-rank matrices $A$ and $B$ together form a quotient manifold space with a certain curvature, leading to an inconsistency between the inner-manifold optimal and the full-rank optimal gradient. This further leads to a sub-optimal training process for LoRA. To alleviate, Zhang et al.~\cite{riemannian} enhances LoRA gradients by a Riemannian gradient preconditioner, given by $\nabla_{A}\mathcal{L} = (B^TB)^{-1}\nabla_{A}\mathcal{L}$ and $\nabla_{B}\mathcal{L} = \nabla_{B}\mathcal{L}(AA^T)^{-1}$. These preconditioners contribute to constructing two gradient projectors after a mathematical derivation, ensuring the update is done in accord with the full-rank gradient projection onto the row space of $A$ and the column space of $B$, that is $X_{new}=X-\eta[Proj_{col(B)}(\nabla_{X}\mathcal{L})^T+Proj_{row(A)}(\nabla_{X}\mathcal{L})]$.

\begin{figure*}
\centering
  \vspace{-5pt}
  \includegraphics[width=\linewidth]{graphs/pipeline_1.pdf} 
  \vspace{-20pt}
  \caption{The whole MoE-LoRA architecture and an insight into its gradient updating process. The left part of this figure shows a pipeline of mixture of LoRAs, which fixes the FFN pretrained weights and trains a series of LoRA adapters together with a routering gate. The right part exhibits how MoE-LoRA is updated. Specifically, we plot an example of a 2-Expert MoE-LoRA in a condition that $g_1<g_2$, which results in a further distorted manifold $g_1B_1A_1$. Here we simply omit the fixed pretrained weights and suppose $X=g_1E_1+g_2E_2$ for convenient display. Since that, for a random step $t$ we plot a state point $\frac{1}{2}X^{(t)}$, which equals to $\frac{{g_1}^{(t)}{B_1}^{(t)}{A_1}^{(t)}+{g_2}^{(t)}{B_2}^{(t)}{A_2}^{(t)}}{2}$ and so that serves as the center point of the two manifold states at $t$. This figure illustrates that $g_1B_1A_1$ has a higher curvature so that its local optimal descent and its global optimal descent projection are more distinct. That indicates a requirement for gate-related preconditioners.}
  \label{overall_graph}
  \vspace{-10pt}
\end{figure*}

Through a comprehensive analysis of \textit{Limitation 1} and \textit{Limitation 2}, a natural question arises:
\vspace{-5pt}
\begin{tcolorbox}[notitle, rounded corners, colframe=darkgrey, colback=gray!7, boxrule=1.5pt, boxsep=1.2pt, left=0.15cm, right=0.17cm, enhanced, shadow={1.5pt}{-1.5pt}{0pt}{opacity=5,gray!20},toprule=1.5pt, before skip=0.65em, after skip=0.75em]
\emph{
  {
    \centering 
  {
    \fontsize{8pt}{13.2pt}\selectfont 
    How can LoRA-based structure further approximate the full fine-tuning with the guaranteed Limitations 1 and 2?
  }
  \\
  }
  }
\end{tcolorbox}
%\vspace{-2pt}
Inspired by MoE-LoRA and the gradient preconditioning methods, a straightforward answer to this question is to integrate both approaches to simultaneously overcome the representative and sub-optimal limitations. Specifically, the gradients of each LoRA expert can be refined by a respective Riemannian preconditioner. However, we claim that the process of weighed summing experts in MoE-LoRA introduces a gate-based scaling for each LoRA expert's manifold, thereby altering their curvatures with regard to their respective gate value $g_i$s. We illustrate this phenomenon in the right part of Figure \ref{overall_graph}, which plots an example of a 2-Expert MoE-LoRA in a condition that $g_1<g_2$. Specially, in their respective spaces of Expert 1 and 2, Manifolds constructed by $B_1A_1$ and $B_2A_2$ initially share the same curvature since their low-rank matrices are in the same rank. However, after being multiplied by gate values, Manifold $g_1B_1A_1$ is more rescaled so that it provides a larger curvature than $g_2B_2A_2$ in the MoE full space. As a result, Expert 1 exhibits a higher distinction between global optimal and inner-manifold optimal descents. This phenomenon indicates that the preconditioners for each expert shall be further refined, to take the impact of gate values into consideration. In this paper, we propose a simple but effective solution to further rescale the gradients of each expert in a lightweight way by respective gate value $g_i$.
Our improved gradient updating process for MoE-LoRA is given by:
\vspace{-3pt}
\begin{align}
X_{new}=X-\eta\sum_{i=1}^{N_{Expert}}{g_i}Proj_{col(B_i)}(\nabla_{X}\mathcal{L})^T \notag \\
-\eta\sum_{i=1}^{N_{Expert}}{g_i}Proj_{row(A_i)}(\nabla_{X}\mathcal{L}). \notag
\end{align}
\vspace{-5pt}
We summarize our contributions as follows:
%\vspace{-5pt}
\begin{itemize}
    \vspace{-4pt}
    \item We integrate the mixture of LoRAs structure with the Riemannian preconditioners to alleviate both limited representation and sub-optimality issues of LoRA.
    \vspace{-6pt}
    \item We respectively propose a theoretical and an engineering solution for gate-value-rescaled gradient preconditioning of MoE-LoRA.
    \vspace{-6pt}
    \item We implement and examine our rescaling approach for MoE-LoRA under a series of foundation models, illustrating our effectiveness across various tasks.
    \vspace{-6pt}
\end{itemize}

\section{Related Works}
\subsection{LoRA and LoRA Variants}
LoRA~\cite{lora} decomposes a full-rank matrix into a product of two low-rank matrices, which has been widely considered an effective solution for parameter-efficient fine-tuning. Studies have proposed several variants to reform LoRA: For initialization, PISSA~\cite{pissa} leverages singular value decomposition (SVD) to obtain the principal singular components of $W$, while MiLoRA~\cite{milora} utilizes secondary singular values and vectors. LoRA-Pro~\cite{lorapro} and LoRA-GA~\cite{lora-ga} approximate the direction of initial gradients to align them with that of the fully fine-tuning. LoRA+~\cite{lora+} introduces a learning rate separating strategy with $\eta_B > \eta_A$. ResLoRA~\cite{reslora} and SIBO~\cite{sibo} accelerate convergence and mitigate over-smoothing by introducing residual paths. DoRA~\cite{dora} decomposes the weight vector into direction and magnitude and only uses its direction component. rsLoRA~\cite{rslora} proposes a rank-stabilized scaling factor $\lambda_t = r_t^{1/2}$ to ensure stable gradient updates. To prevent overfitting, BiLoRA~\cite{bilora} adopts a bi-level optimizing strategy, while others implement dropout mechanisms~\cite{hiddenkey, loradropout}.

\subsection{Mixture of LoRAs}
MoE has emerged as a critical framework for addressing complex tasks. By incorporating multiple expert modules, it dynamically selects appropriate experts based on specific inputs~\cite{moe}. Early studies, such as LoRAMoE~\cite{loramoe} and MixLoRA~\cite{mixlora}, have pioneered the introduction of the MoE-LoRA architecture by integrating LoRA experts for both global and downstream tasks. Afterward, MoE-LoRA has demonstrated its effectiveness across a range of fields such as continual learning~\cite{loramoe, yang2024moral}, vision-language multi-model tasks~\cite{mocle, llava}, and multi-task applications~\cite{MOElora}.

Recent studies have focused on enhancing MoE-LoRA through architectural advancements and improved training strategies. For instance, MoLA~\cite{mola} allocates a varying number of experts at different layers, and MixDA~\cite{mixda} introduces multiple domain-adaptive modules to support multi-domain knowledge. Other methods such as \cite{moslora, MOElora, mole, mocle, adamix} have also been proposed for strengthening MoE-LoRA. To boost the training of MoE-LoRA, Luo et al.~\cite{MoEloraa} address the random routing issue by introducing a contrastive loss. At the same time, MoV~\cite{mov} chooses to combine lightweight vectors with a sparse selection mechanism for efficient expert allocation. Other approaches, including \cite{loramoe, mixlora, sira}, focus on load balancing among experts. However, to the best of our knowledge, there is still a lack of work on gradient optimizing specifically for MoE-LoRA models. 

\subsection{Gradient Preconditioners}
In most deep learning cases, gradient descent algorithms update model parameters by calculating gradient-based updates. To accelerate the optimizing process, the concept of gradient preconditioning has been introduced. Advanced techniques such as Adagrad~\cite{adagrad} dynamically adjust the learning rate by an accumulated squared gradients \( G_t = \sum_{i=1}^{t} g_i^2 \) and update model by \( \Delta \theta_t = -\eta G_t^{-1/2} \cdot g_t \). Adam~\cite{adam} extends this approach by incorporating momentum and bias correction, scaling gradients through a diagonal preconditioner, and resulting in updates in the form of \( \Delta \theta_t = -\eta \frac{m_t}{\sqrt{v_t} + \epsilon} \), where \( v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \). AdamW~\cite{adamw} further introduces a weight decay to Adam. 

Recent studies have provided theoretical support for scaled gradient descent methods under different preconditioning strategies. The core idea is to adjust both the direction and magnitude of updates by introducing a scaling matrix to gradients.  
Tong et al.~\cite{tong2021low} demonstrate the local convergence of scaled gradient descent methods. Jia et al.~\cite{jia2024preconditioning} extend this work by proving global convergence of scaled gradient descent for the least-squares matrix decomposition problem \( \|AB^T - Y\|_F^2 / 2 \), showing that this approach achieves global convergence under different condition numbers. Other variants of scaled gradient descent have also emerged, such as Zhang et al. who proposed two regularization strategies ~\cite{zhang2023preconditioned,zhang2024fast}. In higher-dimensional settings, scaled gradient descent has been further extended to tensor optimization ~\cite{tong2022scaling,ma2023provably}. Mishra et al.~\cite{mishra2013low, mishra2016riemannian} also applied the principles of Riemannian to the optimization involving low-rank matrices. Considering the data's manifold geometry, a Riemannian metric $g_p(v, w)$ is introduced to guide gradient updates along the manifold. Recently, Zhang et al.~\cite{riemannian} introduced the idea of Riemannian preconditioners to LoRA by attaching an \( r \times r \) preconditioner to the gradients of low-rank matrices. As a result, they provide improved fine-tuning performance of LoRA, compared with conventional gradient optimizers such as SGD and AdamW.

\section{Method}

We elaborate on our motivations and detail the modification we have made to the Riemannian preconditioning method specifically for MoE-LoRA. Our theoretical foundations and engineering solutions are also presented.

\subsection{Riemannian Preconditioner in LoRA Expert}

As a preliminary, we first briefly introduce the Riemannian preconditioner \cite{riemannian}. Suppose the pretrained model weight is $W$ and its additive low-rank components as $B$ and $A$, let $X=W+BA$ denote the whole weight matrix and let $\mathcal{L}$ and $\eta$ denote the loss function and the learning rate, respectively. For the plain gradient descent method, the gradient updating process is described through Equation \eqref{eq0l1} to \eqref{eq0l4}, in which the derivation from \eqref{eq0l2} to \eqref{eq0l3} relies on ignoring the second-order term of learning rate. Obviously, $B\nabla_{A}\mathcal{L}+\nabla_{B}\mathcal{L}A$ in \eqref{eq0l4} serves as an approximation of the ideal FFT gradient of $X$.
\begin{align}
X_{new}&=W+B_{new}A_{new} \label{eq0l1} \\
&=W+(B-\eta\nabla_{B}\mathcal{L})(A-\eta\nabla_{A}\mathcal{L}) \label{eq0l2} \\
&\approx W+BA-\eta B\nabla_{A}\mathcal{L}-\eta\nabla_{B}\mathcal{L}A \label{eq0l3} \\
&=X-\eta(B\nabla_{A}\mathcal{L}+\nabla_{B}\mathcal{L}A) \label{eq0l4}
\end{align}
Subsequently, according to the derivation chain rule and the simple fact that $X=W+BA$, we directly obtain that $\nabla_{A}\mathcal{L}=(\nabla_{A}X)(\nabla_{X}\mathcal{L})=B^T(\nabla_{X}\mathcal{L})$, and likewise $\nabla_{B}\mathcal{L}=(\nabla_{X}\mathcal{L})A^T$. Thus, \eqref{eq0l4} can be transformed to:
\begin{align}
X_{new}&=X-\eta[BB^T(\nabla_{X}\mathcal{L})+(\nabla_{X}\mathcal{L})A^TA], \label{inconsistent_bp_eq}
\end{align}

which actually updates the model in a different direction compared to the FFT update formula $X_{new}=X-\eta\nabla_{X}\mathcal{L}$. This phenomenon occurs since the distorted sub-space of $X$ constructed by $BA$ brings inconsistency between the optimal gradient descent within its manifold and that of the full matrix $X$. To address this inconsistency, Zhang et al.~\cite{riemannian} scale the gradients of $A$ and $B$ by:
\vspace{-1pt}
\begin{equation}
\begin{aligned}
\nabla_{A}\mathcal{L} = (B^TB)^{-1}\nabla_{A}\mathcal{L} \\
\nabla_{B}\mathcal{L} = \nabla_{B}\mathcal{L}(AA^T)^{-1},
\end{aligned}
\label{riemannian_method}
\end{equation}

so that \eqref{inconsistent_bp_eq} is expressed as:
\begin{equation}
\begin{aligned}
X_{new}=X-\eta[&B(B^TB)^{-1}B^T(\nabla_{X}\mathcal{L})\\
&+(\nabla_{X}\mathcal{L})A^T(AA^T)^{-1}A]\\
=X-\eta[&Proj_{col(B)}(\nabla_{X}\mathcal{L})^T\\
&+Proj_{row(A)}(\nabla_{X}\mathcal{L})],
\end{aligned}
\end{equation}

where the update inside the manifold is performed according to the full matrix gradient projection onto the row space of $A$ and the column space of $B$. Therefore, it better approximates fully fine-tuning than the unscaled descent step.

Inspired by this work, a straightforward way to expand their solution to MoE-LoRA is to individually scale the gradient of each LoRA expert by \eqref{riemannian_method}. However, equation $X=W+BA$ lays out in a different form in MoE-LoRA:
\begin{align}
X=W+\sum_{i=1}^{N_{Expert}}g_iB_iA_i, \label{moe_lora_forward}
\end{align}

where $N_{Expert}$ denotes the number of activated experts and $g_i$ denotes the gate value of specific expert $i$. As a result, it not only brings a gate value $g_i$ for each expert $i$ into Equation \eqref{eq0l1}-\eqref{eq0l4}, but also introduces an extra gate value $g_i$ for each expert $i$ into \eqref{inconsistent_bp_eq}, since the derivation chain rule $\nabla_{B_i}\mathcal{L}=g_i(\nabla_{X}\mathcal{L}){A_i}^T$ and $\nabla_{A_i}\mathcal{L}=g_i{B_i}^T(\nabla_{X}\mathcal{L})$. To further clarify, we formally derive the whole result. Note that gate values are computed through a softmax with complex non-linear operations, thus we just treat them as constants for an easier deriving approximation. Following the conventional Riemannian preconditioners in \eqref{riemannian_method}, we have:
\begin{align}
X_{new}=W+&\sum_{i=1}^{N_{Expert}}g_i(B_i-\eta\nabla_{B_i}\mathcal{L})(A_i-\eta\nabla_{A_i}\mathcal{L}) \notag \\
\approx X-&\eta\sum_{i=1}^{N_{Expert}}g_i(B_i\nabla_{A_i}\mathcal{L}+\nabla_{B_i}\mathcal{L}A_i) \notag \\
=X-&\eta\sum_{i=1}^{N_{Expert}}g_i[B_i({B_i}^TB_i)^{-1}\nabla_{A_i}\mathcal{L} \notag \\
+&\nabla_{B_i}\mathcal{L}(A_i{A_i}^T)^{-1}A_i] \label{where_moe_use_riemannian}
\end{align}
\vspace{-20pt}
\begin{align}
%X_{new}=W+&\sum_{i=1}^{N_{Expert}}g_i(B_i-\eta\nabla_{B_i}\mathcal{L})(A_i-\eta\nabla_{A_i}\mathcal{L}) \notag \\
%\approx W+&\sum_{i=1}^{N_{Expert}}g_i(B_iA_i-\eta B_i\nabla_{A_i}\mathcal{L}-\eta\nabla_{B_i}\mathcal{L}A_i) \notag \\
%\approx W+&\sum_{i=1}^{N_{Expert}}g_iB_iA_i \notag \\
%-&\eta\sum_{i=1}^{N_{Expert}}g_i(B_i\nabla_{A_i}\mathcal{L}+\nabla_{B_i}\mathcal{L}A_i) \notag \\
%\approx X-&\eta\sum_{i=1}^{N_{Expert}}g_i(B_i\nabla_{A_i}\mathcal{L}+\nabla_{B_i}\mathcal{L}A_i) \notag \\
%=X-&\eta\sum_{i=1}^{N_{Expert}}g_i[B_i({B_i}^TB_i)^{-1}\nabla_{A_i}\mathcal{L} \notag \\
%+&\nabla_{B_i}\mathcal{L}(A_i{A_i}^T)^{-1}A_i] \label{where_moe_use_riemannian} \\
=X-&\eta\sum_{i=1}^{N_{Expert}}g_i[g_iB_i({B_i}^TB_i)^{-1}{B_i}^T(\nabla_{X}\mathcal{L}) \notag \\
+&g_i(\nabla_{X}\mathcal{L}){A_i}^T(A_i{A_i}^T)^{-1}A_i] \notag \\
%=X-&\eta\sum_{i=1}^{N_{Expert}}{g_i}^2[Proj_{col(B_i)}(\nabla_{X}\mathcal{L})^T \notag \\
%+&Proj_{row(A_i)}(\nabla_{X}\mathcal{L})] \notag \\
=X-&\eta\sum_{i=1}^{N_{Expert}}{g_i}^2Proj_{col(B_i)}(\nabla_{X}\mathcal{L})^T \notag \\
-&\eta\sum_{i=1}^{N_{Expert}}{g_i}^2Proj_{row(A_i)}(\nabla_{X}\mathcal{L}), \label{moe_riemannian_with_squared_gv}
\end{align}

in which the derivation step \eqref{where_moe_use_riemannian} denotes the conventional Riemannian preconditioner scaling. It should be interpreted that \eqref{moe_riemannian_with_squared_gv} consists of an ensemble of projections of the full matrix gradient onto the row spaces of $A$ experts and the column spaces of $B$ experts.

\subsection{Rescaling Preconditioners}

Equation \eqref{moe_riemannian_with_squared_gv} presents a squared-value weighted sum of an ensemble of gradient projections. Generally, more activated experts lead to smaller per-expert gate values and so lead to a more reduced assembled gradient; On the other hand, more balanced experts also lead to a more reduced assembled gradient since the basic inequality theorem $\sum_{i} {x_i}^2 >= \frac{(\sum_{i} x_i)^2}{n} = \frac{1}{n}$ satisfies its equality condition when $x_i$s are equal. As a result, the gradient of the full matrix $X$ will be underestimated due to those squared gate values. From the perspective of manifolds and curvature, we explain that by considering $g_i$ in \eqref{moe_lora_forward} as a manifold scaler, which reduces the size of $B_iA_i$ so that would probably increase its curvature. However, the conventional Riemannian preconditioner failed to take the manifold scaler $g_i$ into consideration, since it is designed for a single LoRA adapter. 

To alleviate this squared issue, we assume a further rescaling step for the Riemannian preconditioners:
\begin{equation}
\begin{aligned}
\nabla_{A_i}\mathcal{L} = \frac{({B_i}^TB_i)^{-1}\nabla_{A_i}\mathcal{L}}{g_i} \\
\nabla_{B_i}\mathcal{L} = \frac{\nabla_{B_i}\mathcal{L}(A_i{A_i}^T)^{-1}}{g_i},
\end{aligned}
\label{our_ideal_eq}
\end{equation}
which is introduced to replace \eqref{riemannian_method} in the derivation of Equation \eqref{where_moe_use_riemannian}, to eliminate the variable $g_i$ and keeps only a first power of $g_i$ in the final equation \eqref{moe_riemannian_with_squared_gv}. Throughout this transformation, the final ensemble of multi-expert projections shares an equivalent scale with the projection of a single LoRA adapter, shown in Equation \eqref{our_final_eq}. Therefore, training of an MoE-LoRA will be alleviated from under-estimation.
\begin{align}
X_{new}=X-\eta\sum_{i=1}^{N_{Expert}}{g_i}Proj_{col(B_i)}(\nabla_{X}\mathcal{L})^T \notag \\
-\eta\sum_{i=1}^{N_{Expert}}{g_i}Proj_{row(A_i)}(\nabla_{X}\mathcal{L}). \label{our_final_eq}
\end{align}

\subsection{Engineering Approximation}

Although Equation \eqref{our_ideal_eq} provides an approach to eliminate under-estimation for MoE-LoRA, it is unrealizable since each LoRA module exists a respective $g_i$ for every single token of every batch sample. Actually, during the training, backpropagation always runs after averaging all the losses of each single token of each sample in a batch. Thus, it is impossible to reconstruct and rescale the respective gradient contributed by each single token when we optimize a LoRA module. Alternatively, we design an engineering approximation to \eqref{our_ideal_eq} and \eqref{our_final_eq}, by replacing each gate value $g_i$ with its square root $\sqrt{g_i}$ during model forwarding. Consequently, Equation \eqref{our_final_eq} can be achieved only under the preconditioners of \eqref{riemannian_method}, because the quadratic terms of gate values ${g_i}^2$ in Equation \eqref{moe_riemannian_with_squared_gv} are now naturally become linear terms $g_i$.

Replacing $g_i$ by $\sqrt{g_i}$ simultaneously introduces destruction to forwarding, as the sum of square roots does not equal $1$. One possible solution is to re-normalize those square roots to be summed up as $1$. However, it brings inconsistency between the assigned weights of experts during forwarding and backwarding. Therefore, we propose another strategy to accommodate both aspects, which is manually assigning optimizable and unoptimizable components of Equation \eqref{moe_lora_forward}, to satisfy the requirements of both forwarding in \eqref{moe_lora_forward} and backwarding in \eqref{our_final_eq}. During the forwarding process, the proposed strategy is simply expressed by:
\begin{align}
X=\hat{W}+\sum_{i=1}^{N_{Expert}}\hat{\sqrt{g_i}}B_iA_i+(g_i-\hat{\sqrt{g_i}})\hat{B_i}\hat{A_i} \label{our_solution},
\end{align}
where $\hat{p}$ denotes that $p$ does not require gradient, which also means $p$ should be detached from gradient tracking along the whole neural network. By decomposing optimizable and unoptimizable components like this, low-rank matrices $A$ and $B$ are able to be optimized following \eqref{our_final_eq}. Moreover, by maintaining the optimizable $g_i$ terms in forwarding and treating all the $\sqrt{g_i}$ as constants that are not subject to optimization, the conventional training behaviors of gates ($g=G(x)$) are preserved. Additionally, this modification introduces only a minimal overhead to the original forward computation process. 

\begin{table*}[t]
    \centering
    \caption{Question answering evaluations across four QA datasets with Llama-3.2-3B as the foundation model. Our gate-based rescaling methodology outperforms conventional Riemannian preconditioned optimizers, in terms of both SGD and AdamW. Each pair of comparing candidates is trained through the same steps until they both achieve good stable performances.}
    \begin{tabular}{lccccc}
    \toprule
        ~ & \textbf{ScienceQA} & \textbf{CommonsenseQA} & \textbf{OpenBookQA} & \textbf{SIQA} & \textbf{avg.} \\ \midrule
        $RSGD_{20,10,4}$ & 62.8 & 52.4 & 53.2 & 65.7 & 58.5 \\ 
        $gRSGD_{20,10,4}$ & \textbf{70.1} & \textbf{55.4} & \textbf{59.8}  & \textbf{68.5} & \textbf{63.5} \\
        \noalign{\vskip 2pt}
        \hline 
        \noalign{\vskip 2.5pt}
        $RAdamW_{20,10,4}$ & 82.6 & 67.7 & 70.4 & 81.5 & 75.6 \\ 
        $gRAdamW_{20,10,4}$ & \textbf{83.8} & \textbf{68.2} & \textbf{72.4} & \textbf{82.3} & \textbf{76.7} \\ \bottomrule
    \end{tabular}

\label{qatable}
\end{table*}
\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{graphs/qa_sgd_loss.png} 
  \vspace{-20pt}
  \caption{Converging Performances of $RSGD_{20,10,4}$ and $gRSGD_{20,10,4}$ MoE-LoRA with Llama-3.2-3B as the foundation model. We plot training and evaluating losses, as well as accuracy metrics for the first 500 steps.}
  \label{qa_sgd_loss_graph}
\end{figure*}

\section{Experiments}
We present a series of comparative experiments to evaluate the performances of MoE-LoRA across various downstream tasks including Question Answering, the GLUE Benchmark, and the Vision-Language task.  
Specifically, two types of experimental candidates are mainly involved in our experiments: (1) MoE-LoRA with experts updated independently using Riemannian scaled optimizer; and (2) MoE-LoRA updated using Riemannian scaled optimizer, plus incorporating our proposed rescaling technique (the engineering approximation). We implement both of them on SGD and AdamW optimizers respectively. As a further reference, we also exhibit our comparisons and possibility of integrations with previous MoE-LoRA baselines, such as MoLA~\cite{mola}. Finally, to lend support to our theoretical foundation, we conduct an ablation study by assessing our forwarding revisions only under a classic optimizer without Riemannian preconditioners support. 

\subsection{Experimental Setup}
For most experiments, unless otherwise specified, we construct a mixture of LoRAs modules with a total of 20 experts, a rank of 4 for each expert, and a selection of top-10 experts activated each time.  
Furthermore, a range of other architectural MoE settings are also discussed in the ablation section. We perform experiments based on Llama-3.2-3B~\cite{touvron2023llama}, GLM-4-9B~\cite{glm2024chatglm}, and LLaVA-v1.5-7B~\cite{liu2024visual} as the foundation models. During training, we follow a linear decay learning-rate scheduler. 
We assign a relatively smaller learning rate to gate module compared to other trainable components, to achieve a stable training behavior. The reduced learning rate for gate helps to prevent model from experiencing abrupt and erratic routing changes. For further stabilization, we also cap its maximum gradient norm at 1.0. 
We carefully assign different initial learning rates for various tasks, trying to ensure all models achieve their best performances in a capable running time.

We denote the number of experts, top-k, and the per-expert rank as $n,k,r$ respectively; 
For experimental candidates using conventional Riemannian preconditioned optimizers, we denote them as $RSGD_{n,k,r}$ and $RAdamW_{n,k,r}$, in which the front $R$ represents the word \emph{Riemannian}; While those candidates integrated with our gate-based rescaling approach are denoted as $gRSGD_{n,k,r}$ and $gRAdamW_{n,k,r}$ respectively, in which the front $g$ represents that we rescale the gradient by gate values.


\begin{table*}[t]
\centering
\caption{GLUE Benchmark evaluations across nine tasks with Llama-3.2-3B and GLM-4-9B as the foundation models. Our gate-based rescaling method contributes an overall improvement over GLUE Benchmark, in terms of Riemannian preconditioned SGD and AdamW.}
\begin{tabular}{llcccccccccc}
\toprule
 & & \makebox[0.02\textwidth][c]{\textbf{CoLA}}  & \makebox[0.02\textwidth][c]{\textbf{SST-2}} & \makebox[0.02\textwidth][c]{\textbf{MRPC}}  & \makebox[0.02\textwidth][c]{\textbf{STS-B}} & \makebox[0.02\textwidth][c]{\textbf{QQP}}   & \makebox[0.02\textwidth][c]{\textbf{MNLI}}  & \makebox[0.02\textwidth][c]{\textbf{QNLI}}  & \makebox[0.02\textwidth][c]{\textbf{RTE}}   & \makebox[0.02\textwidth][c]{\textbf{WNLI}}  & \makebox[0.02\textwidth][c]{\textbf{avg.}} \\ \midrule

\multirow{4}{0.09\textwidth}{Llama 3.2 (3B)} & $RSGD_{20,10,4}$    & 48.25          & 93.58          & 73.57          & 48.15          & 80.22          & 62.42          & 78.39          & 62.45          & 39.44          & 65.16         \\
& $gRSGD_{20,10,4}$   & \textbf{55.86} & \textbf{94.95} & \textbf{76.29} & \textbf{56.70} & \textbf{83.48} & \textbf{80.37} & \textbf{83.42} & \textbf{72.92} & \textbf{52.11} & \textbf{72.90} \\ 
%\noalign{\vskip 2pt}
\cline{2-12}
%\noalign{\vskip 2pt}
& $RAdamW_{20,10,4}$  & 65.94          & 96.56          & 70.03          & 60.12          & 85.36          & 81.36          & \textbf{91.55} & 53.95          & 45.07          &  72.22        \\
& $gRAdamW_{20,10,4}$ & \textbf{67.38} & \textbf{96.67} & \textbf{81.57} & \textbf{61.08} & \textbf{86.35} & \textbf{81.59} & 91.32          & \textbf{55.75} &  \textbf{47.89}              &   \textbf{74.40}            \\ \bottomrule


\multirow{4}{0.09\textwidth}{GLM 4 (9B)}  & $RSGD_{20,10,4}$    & 62.33          & 91.74          & \textbf{82.32} & 63.06          & 85.36          & 81.67          & 90.27          & 62.59          & 76.05          & 77.27          \\
& $gRSGD_{20,10,4}$   & \textbf{62.97} & \textbf{95.18} & 81.68          & \textbf{63.68} & \textbf{86.84} & \textbf{86.27} & \textbf{91.17} & \textbf{80.21} & \textbf{77.46} & \textbf{80.61} \\
%\noalign{\vskip 2pt}
\cline{2-12}
%\noalign{\vskip 2pt}
& $RAdamW_{20,10,4}$  & \textbf{62.54} & 45.53          & 79.30          & 64.15          & 88.72          & 88.03          & 91.58 & 89.56          & 84.51          & 77.10          \\
& $gRAdamW_{20,10,4}$ & 62.25          & \textbf{46.22} & \textbf{79.59} & \textbf{64.36} & \textbf{88.82} & \textbf{88.19} & \textbf{91.66}          & \textbf{91.37} & \textbf{85.92} & \textbf{77.60} \\ \bottomrule

\end{tabular}

\label{3b9bglue}
\end{table*}

\iffalse

\begin{table*}[t]
\centering
\begin{tabular}{lcccccccccc}
\toprule
                    & \textbf{CoLA}  & \textbf{SST-2} & \textbf{MRPC}  & \textbf{STS-B} & \textbf{QQP}   & \textbf{MNLI}  & \textbf{QNLI}  & \textbf{RTE}   & \textbf{WNLI}  & \textbf{avg.} \\ \midrule
$RSGD_{20,10,4}$    & 48.25          & 93.58          & 73.57          & 48.15          & 80.22          & 62.42          & 78.39          & 62.45          & 39.44          & 65.16         \\
$gRSGD_{20,10,4}$   & \textbf{55.86} & \textbf{94.95} & \textbf{76.29} & \textbf{56.70} & \textbf{83.48} & \textbf{80.37} & \textbf{83.42} & \textbf{72.92} & \textbf{52.11} & \textbf{72.90} \\ 
\noalign{\vskip 2pt}
\hline
\noalign{\vskip 2pt}
$RAdamW_{20,10,4}$  & 65.94          & 96.56          & 70.03          & 60.12          & 85.36          & 81.36          & \textbf{91.55} & 53.95          & 45.07          &  72.22        \\
$gRAdamW_{20,10,4}$ & \textbf{67.38} & \textbf{96.67} & \textbf{81.57} & \textbf{61.08} & \textbf{86.35} & \textbf{81.59} & 91.32          & \textbf{55.75} &  \textbf{47.89}              &   \textbf{74.40}            \\ \bottomrule
\end{tabular}
\caption{GLUE Benchmark evaluations across nine tasks with Llama-3.2-3B as the foundation model. Our proposed gate-based rescaling method contributes an overall improvement over GLUE Benchmark, in terms of Riemannian preconditioned SGD or AdamW.}
\label{glue_final}
\end{table*}



\begin{table*}[t]
\centering
\begin{tabular}{lcccccccccc}
\toprule
                    & \textbf{CoLA}  & \textbf{SST-2} & \textbf{MRPC}  & \textbf{STS-B} & \textbf{QQP}   & \textbf{MNLI}  & \textbf{QNLI}  & \textbf{RTE}   & \textbf{WNLI}  & \textbf{avg.}  \\ \midrule
$RSGD_{20,10,4}$    & 62.33          & 91.74          & \textbf{82.32} & 63.06          & 85.36          & 81.67          & 90.27          & 62.59          & 76.05          & 77.27          \\
$gRSGD_{20,10,4}$   & \textbf{62.97} & \textbf{95.18} & 81.68          & \textbf{63.68} & \textbf{86.84} & \textbf{86.27} & \textbf{91.17} & \textbf{80.21} & \textbf{77.46} & \textbf{80.61} \\
\noalign{\vskip 2pt}
\hline
\noalign{\vskip 2pt}
$RAdamW_{20,10,4}$  & \textbf{62.54} & 45.53          & 79.30          & 64.15          & 88.72          & 88.03          & 91.58 & 89.56          & 84.51          & 77.10          \\
$gRAdamW_{20,10,4}$ & 62.25          & \textbf{46.22} & \textbf{79.59} & \textbf{64.36} & \textbf{88.82} & \textbf{88.19} & \textbf{91.66}          & \textbf{91.37} & \textbf{85.92} & \textbf{77.60} \\ \bottomrule
\end{tabular}
\caption{GLUE Benchmark evaluations across nine tasks with GLM-4-9B as the foundation model. Our proposed approach still overall performs better, under both Riemannian preconditioned SGD and AdamW optimizers.}
\label{9bglue}
\end{table*}

\fi

\subsection{Question Answering Evaluations}

We evaluate our proposed method on several question-answering benchmarks, including ScienceQA~\cite{lu2022scienceqa}, CommonsenseQA~\cite{talmor2018commonsenseqa}, OpenBookQA~\cite{mihaylov2018openbookqa} and SIQA~\cite{sap2019siqa}. These question-answering datasets encompass a diverse range of domains and types, such as science, social interactions, common sense, and open-book exams, etc. We implement all the experimental candidates based on Llama-3.2-3B as their foundation model. For the SGD optimizer, we set an initial learning rate to \num{3e-5} for every LoRA expert; For the AdamW optimizer, we utilize an initial learning rate of \num{1e-5}. We run through all the experiments until they are stabilized at a stable performance, and especially ensure that each pair of comparing candidates (i.e., independently Riemannian preconditioned MoE-LoRA, and that with our proposed rescaling approach) are trained through the same steps to make sure they are fairly comparable. Specifically, depending on the complexity of datasets, we choose from two settings, 800 or 1,400 steps, for all the QA evaluations, except that of $RAdamW$ and $gRAdamW$ on CommonsenseQA, which we train up to 2,000 steps to achieve a more clear distinction between two comparable candidates. We present our evaluated performances in Table \ref{qatable}. It is observed that: (1) Riemannian preconditioned Optimizers incorporating our approach achieve better performances for every QA benchmark, albeit with varying degrees of improvement; (2) Overall, we exhibit more contribution to Riemannian preconditioned SGD than to that of AdamW: We improve the performance of $RSGD$ by around 8.5\%, while we improve $RAdamW$ by around 1.5\%.

Besides our improvements in final performances, we also witness a boost in terms of converging speed under our optimization. To clearly display this, we plot loss-decreasing curves and metric variations of the four question-answering datasets under SGD optimizer in Figure \ref{qa_sgd_loss_graph}. It is clearly shown that $gRSGD$ converges faster than $RSGD$, in terms of training and evaluating losses as well as accuracy metrics. 

\subsection{Performance on GLUE Benchmark}

To comprehensively examine our effectiveness, we perform a series of downstream evaluations on the benchmark of GLUE \cite{wang2018glue}, which is a collection of resources for evaluating model performances on natural language understanding. We first run through all the evaluations in GLUE with Llama-3.2-3B as the foundation model and present the benchmark results in Table \ref{3b9bglue}. For most SGD experiments we set an initial learning rate for LoRA experts as \num{3e-5}, except WNLI for which we set its initial learning rate to \num{3e-6}; For AdamW experiments we choose an initial learning rate from $\{$\num{3e-5}, \num{1e-5}$\}$. For most datasets, we train for 2,000 steps, excluding some AdamW experiments in which we perform an early stop at around 1,000 since they appear to be converged or even overfitting. Table \ref{3b9bglue} illustrates our effectiveness across various downstream applications as well as the overall assessment under Llama-3.2-3B. In terms of overall performances, our approach improves $RSGD$ and $RAdamW$ by 11.9\% and 3.0\% respectively.

Subsequently, we extend experiments to a larger foundation model, GLM-4-9B. Since the 9B model is more powerful in few-shot learning, for some datasets such as SST-2 etc., we set lower learning rates such as \num{3e-6} and \num{1e-6} respectively for SGDs and AdamWs, to make sure a clear loss decreasing period can be witnessed. %while others still remain \num{3e-5} or \num{1e-5}. 
We train for the same number of steps for each pair of competitive candidates. Table \ref{3b9bglue} also illustrates the performances of training MoE-LoRA through different optimizing strategies with GLM-4-9B. Results still witness our overall outperformance. In particular, we improve the average performance of $RSGD$ by around 4.3\%, and that of $RAdamW$ by around 0.7\%.   

\begin{table}
\centering
\caption{Visual7W and VMCBench performances after trained for 1000 steps, with LLaVA-v1.5-7B as the foundation model. (For VMCBench, we use 100 samples to evaluate, thus the accuracy will be at most a two-digit decimal. That's why we list all numbers in percentage here for a more comfortable present.)}

\begin{tabular}{lcc}
\toprule
 & \textbf{Visual7W} & \textbf{VMCBench} \\ \midrule
\textbf{$RSGD_{20,10,4}$}  &  68\%   &    59\%   \\
\textbf{$gRSGD_{20,10,4}$} &  \textbf{72\%}   &    \textbf{70\%}   \\
\noalign{\vskip 0.8pt}
\hline
\noalign{\vskip 1.0pt}
\textbf{$RAdamW_{20,10,4}$} &  77\%   &    75\%   \\ 
\textbf{$gRAdamW_{20,10,4}$} &  \textbf{78\%}   & 75\% \\\bottomrule          
\end{tabular}
\vspace{-10pt}
\label{visual7w}
\end{table}

\subsection{Performance on LLaVA}
Beyond textual benchmarks, we further evaluate our gate-based rescaling approach in the computer vision field. Specifically, we implement an MoE-LoRA architecture for the well-known vision-language foundation model, LLaVA-v1.5-7B~\cite{llava}.  
We introduce trainable MoE-LoRA adapters into both visual and textual modules of LLaVA-v1.5-7B. For evaluation, Visual7W~\cite{zhu2016visual7w} and VMCBench~\cite{zhang2025automated} datasets are employed, which both consist of multimodal samples each containing a multiple-choice question paired with a related image. The question can be answered through understanding the provided image. Visual7W is a subset of Visual Genome~\cite{krishna2017visual} dataset, while VMCBench is a benchmark created from 20 existing VQA datasets. For VMCBench, we only use their dev set since their test set is not labeled. We take 900 of all the 1000 labeled samples as training samples, while the rest 100 are for evaluation. Table \ref{visual7w} exhibits the results of all experimental candidates. Our approach consistently demonstrates visible improvements, especially for SGD. 

\subsection{Compare and Integrate with MoE-LoRA Baselines}
We then compare and integrate our method with existing MoE-LoRA baselines. We provide our comparisons with two baselines: (1) The pure mixture of LoRAs~\cite{MOElora}, which we denote as MoELoRA and use token-level routing; (2) MoLA~\cite{mola}, which is a MoE-LoRA variant specifically focusing on assigning different numbers of experts to different layers, and proving that higher layers need more LoRA experts. It should be noted that our proposed gate-based rescaling approach can be integrated with most MoE-LoRA variants since they are not in conflict. Take MoLA as an example, we can integrate our method with MoLA by implementing a model with more experts in its higher layers and trained through Riemannian preconditioners and gate-based rescaling approach. We reproduce MoELoRA and MoLA, implement the integrations, and illustrate their performances in Table \ref{mola_compare}. We use Llama-3.2-3B as the foundation model and follow MoLA's configurations here, which means we set the per-expert rank to 4, top-k to 2, and the total number of experts of all layers to 140. In this way, MoELoRA and our method assign 5 experts to each layer, while MoLA assigns 2, 4, 6, and 8 experts respectively to the bottom, lower middle, higher middle, and top layers. In table \ref{mola_compare} we denote this special assignment strategy as (2,4,6,8), while the average assignment is (5,5,5,5), where each digit covers seven layers under Llama-3.2-3B. We still provide enhancement in the context of MoLA architecture. 
\begin{table*}
\centering
\caption{Baselines Comparison and Integration. The first three lines provide comparisons between pure MoE-LoRA, MoLA, and our gate-rescaled Riemannian preconditioning method. The last two lines provide MoLA integrated with conventional and gate-rescaled preconditioning methods, respectively. All candidates are trained using SGD optimizers for up to 2000 steps. }
\vspace{2pt}
\begin{tabular}{lcccccc}
\toprule
         & \textbf{Experts} & \textbf{ScienceQA} & \textbf{CommonsenseQA} & \textbf{OpenBookQA} & \textbf{SIQA} & \textbf{avg.} \\ \hline
MoELoRA-$SGD$ & (5,5,5,5) &     54.68          &          48.90         &       48.40         &      57.92    &    52.47     \\
MoLA-$SGD$ & (2,4,6,8) & 54.99          &          49.20         &       52.80         &      58.94    &     53.98     \\
MoELoRA-$gRSGD$ (Ours) & (5,5,5,5)     &   \textbf{70.01} &       \textbf{54.80}   &    \textbf{63.60}   & \textbf{64.45}&   \textbf{63.22}     \\ \hline
MoLA-$RSGD$ &  (2,4,6,8)    &  68.03    &          53.90         &       59.80         &      64.08    &     61.45     \\
MoLA-$gRSGD$ (Ours) & (2,4,6,8) & \textbf{70.46} &  \textbf{56.30}   &     \textbf{64.00}  & \textbf{64.90}&     \textbf{63.92}          \\ \bottomrule
\end{tabular}
\label{mola_compare}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%% Table -  N/K/R Ablation %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}
\centering
\caption{Accuracies and boosts of ScienceQA for conventional and gate-rescaled Riemannian optimizers under various MoE architectures. Llama-3.2-3B serves as the foundation model.} 
\vspace{2pt}
\begin{tabular}{lcccccc}
\toprule
\textbf{$n/k/r$} & \textbf{$RSGD$} & \textbf{$gRSGD$} & Boost & \textbf{$RAdamW$} & \textbf{$gRAdamW$} & Boost \\ \midrule
5/5/4   & 65.78           & \textbf{71.31}   & \textcolor{red}{8.41\%$\uparrow$}         & 76.67             & \textbf{78.19}     & \textcolor{red}{1.98\%$\uparrow$}         \\
8/5/4   & 65.11           & \textbf{69.96}   & \textcolor{red}{7.45\%$\uparrow$}         & 78.69             & \textbf{79.45}     & \textcolor{red}{0.97\%$\uparrow$}         \\
10/5/4  & 64.34           & \textbf{69.33}   & \textcolor{red}{7.76\%$\uparrow$}         & 79.05             & \textbf{80.31}     & \textcolor{red}{1.59\%$\uparrow$}         \\
10/5/2  & 72.03           & \textbf{78.06}   & \textcolor{red}{8.37\%$\uparrow$}         & 79.63             & \textbf{80.13}     & \textcolor{red}{0.63\%$\uparrow$}         \\
10/5/1  & 79.95  & \textbf{87.28}            & \textcolor{red}{9.17\%$\uparrow$}        & 80.71             & \textbf{81.11}     & \textcolor{red}{0.50\%$\uparrow$}         \\
10/10/2 & 68.62           & \textbf{77.16}   & \textcolor{red}{12.45\%$\uparrow$}         & 77.47             & \textbf{77.65}     & \textcolor{red}{0.23\%$\uparrow$}         \\
10/2/2  & 79.68           & \textbf{81.16}   & \textcolor{red}{1.86\%$\uparrow$}         & \textbf{83.63}    & 83.45              & \textcolor{darkgreen}{-0.22\%$\downarrow$}        \\ \bottomrule
\end{tabular}
\label{diff_moe_settings}
\end{table*}

\subsection{Ablation Study}
\textbf{Theoretical Dependence.} Although our proposed approach is grounded in the context of Riemannian preconditioners, it is important to note that our engineering implementation does not inherently require coexistence with Riemannian preconditioners. The reason is that our modifications are solely focused on altering the forward propagation conventions of MoE-LoRA. This consequently raises a vital question about the standalone efficacy of our modifications in enhancing MoE-LoRAâ€™s performance, without depending on the Riemannian preconditioning context. Ideally, since the conventional un-preconditioned optimizer does not guarantee a projection of full matrix gradient in low-rank space, it should be trivial for them to normalize the sum of expert gradients by replacing $g_i$ with $\sqrt{g_i}$.  
To confirm this, we conduct an ablation study by integrating our gate-based revision with a conventional un-preconditioned SGD optimizer. The loss-decreasing curves shown in Figure \ref{ablation_loss_graph} illustrate that applying our approach directly on a pure SGD optimizer does not provide help, which oppositely demonstrates our refinement is highly coupled with the Riemannian preconditioning algorithm.
\begin{figure}[t]
  \includegraphics[width=\linewidth]{graphs/ablation-loss2.png} 
  \vspace{-20pt}
  \caption{Curves of ScienceQA training losses under the optimization of conventional and Riemannian preconditioned SGDs, and also both integrated with the gate-based rescaling approach. Llama-3.2-3B serves as the foundation model.}
  \label{ablation_loss_graph}
  \vspace{-10pt}
\end{figure}

\textbf{Various MoE architectures.} To demonstrate that our proposed approach can be generalized to various settings of LoRA mixtures, we construct different MoE-LoRA architectures for further exploration, including the variations in the numbers of experts, per-expert ranks, and the number of top-k. Specifically, we test seven structural conditions on the ScienceQA dataset, and all candidates are trained for 800 steps using the same initial learning rate. Table \ref{diff_moe_settings} exhibits the results, showing we are able to outperform across most circumstances in terms of MoE structure. Moreover, it is also observed from SGD performances that, variations in expert numbers or per-expert ranks introduce limited impacts on our effectiveness, while larger top-k roughly exhibit higher boosts. This observation aligns with our theoretical analysis, which suggests a larger number of activated experts results in more reduced per-expert gate values, thereby leaving a larger margin for our revision to take effect. 

\section{Conclusion}

We introduce the Riemannian gradient preconditioners to train a mixture of Low-rank Experts (MoE-LoRA). Instead of directly attaching Riemannian preconditioners to each expert's gradient for pursuing local optimality, we claim that multiplying expert $B_iA_i$ by its respective gate value $g_i$ during forwarding leads to a further rescaling of the manifold constructed by expert $i$. To alleviate this, Riemannian preconditioners designed for MoE-LoRA shall be revised to incorporate gate values. To approximate this concept, we propose an engineering solution that decomposes forwarding variables into optimizable and un-optimizable components. Experiments across various downstream tasks demonstrate our performance improvement over conventional Riemannian preconditioners. Ablation studies further demonstrate our theoretical foundation and universality.

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Covergence Efficiency}
In the main body of our paper, we illustrate the converging speed enhancements of our proposed approach, $gRSGD$, over the conventional $RSGD$ through a series of loss-decreasing plots. To further exhibit the comprehensive comparisons of convergence efficiency, we provide more results on GLUE benchmarks. In particular, for experiments conducted under Llama-3.2-3B as well as GLM-4-9B, we record metrics after the initial 100 training steps for each of the GLUE evaluations, as detailed in Table \ref{glue_3b_first_100} and Table \ref{glue_9b_first_100} respectively.

Table \ref{glue_3b_first_100} and \ref{glue_9b_first_100} clearly demonstrate the superior convergence speed of our solutions over the conventional Riemannian preconditioned SGD optimizers. Nevertheless, they simultaneously illustrate an overall equivalent performance with trivial differences between our gate-based approach and the conventional Riemannian preconditioning method under AdamW optimizers. This indicates that our proposed approach is more valuable for SGD optimization. AdamW optimizers already present robust converging performances due to their adaptive gradient and learning rate mechanisms. As a result, our global optimal approximation under AdamW optimizing mainly contributes to the final optimality rather than significantly accelerating the initial gradient descending. 

%%%%%%%%%%%%%%%%%%%%%% Table 1 -  3B model convergence %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[h]
\centering
\caption{GLUE Benchmark evaluations after the initial 100 training steps conducted under Llama-3.2-3B. Our proposed gate-based rescaling method contributes an overall converging speed enhancement over conventional Riemannian preconditioned SGD optimizers, while for AdamW optimizers, we provide a similar converging speed compared with the conventional Riemannian ones.}
\begin{tabular}{lcccccccccc}
\toprule
                    & \textbf{CoLA}  & \textbf{SST-2} & \textbf{MRPC}  & \textbf{STS-B} & \textbf{QQP}   & \textbf{MNLI}  & \textbf{QNLI}  & \textbf{RTE}   & \textbf{WNLI}  & \textbf{avg.}  \\ \midrule
$RSGD_{20,10,4}$    & 22.52          & 91.63          & 49.97          & 0.00            & 62.22          & 36.27          & 58.90          & 56.68           & 1.41           & 42.18               \\
$gRSGD_{20,10,4}$   & \textbf{39.30} & \textbf{92.66} & \textbf{66.55} & \textbf{24.49} & \textbf{66.17} & \textbf{43.40} & \textbf{65.33} & \textbf{63.18}      & \textbf{26.76} & \textbf{54.20}      \\
\noalign{\vskip 2pt}
\hline
\noalign{\vskip 2pt}
$RAdamW_{20,10,4}$  & \textbf{52.05} & 92.66          & \textbf{68.99} & 54.04          & \textbf{72.11} & 60.81          & \textbf{84.14} & 53.95          & 32.39          & 63.46          \\
$gRAdamW_{20,10,4}$ & 50.63          & \textbf{93.23} & 67.01          & \textbf{54.72} & 71.41          & \textbf{61.96} & 83.80          & \textbf{55.75} & \textbf{40.85} & \textbf{64.37} \\ \bottomrule
\end{tabular}

\label{glue_3b_first_100}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%% Table 2 -  9B model convergence %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[h]
\centering
\caption{GLUE Benchmark evaluations after the initial 100 training steps conducted under GLM-4-9B. Our proposed gate-based rescaling method still contributes an overall converging speed enhancement over conventional Riemannian preconditioned SGD optimizers, while for AdamW optimizers, we still provide a similar converging speed compared with the conventional Riemannian ones. (Note that for CoLA, SST-2, and MRPC, we utilize a lower initial learning rate such as \num{3e-6} and \num{1e-6}, while the others are \num{3e-5} and \num{1e-5}. Therefore CoLA, SST-2, and MRPC converge much slower than the others.)}
\begin{tabular}{lcccccccccc}
\toprule
                             & \textbf{CoLA}  & \textbf{SST-2} & \textbf{MRPC}  & \textbf{STS-B} & \textbf{QQP}   & \textbf{MNLI}  & \textbf{QNLI}  & \textbf{RTE}   & \textbf{WNLI}  & \textbf{avg.}  \\ \midrule
\textbf{$RSGD_{20,10,4}$}    & 0.00           & 0.00           & 0.41           & 61.15          & 84.57          & 62.35          & 87.49          & 41.73          & 63.38          & 44.56          \\
\textbf{$gRSGD_{20,10,4}$}   & \textbf{7.22}  & \textbf{48.97} & \textbf{62.26} & \textbf{63.06} & \textbf{85.95} & \textbf{85.66} & \textbf{89.28} & \textbf{75.18} & \textbf{74.65} & \textbf{65.80} \\
\noalign{\vskip 2pt}
\hline
\noalign{\vskip 2pt}
\textbf{$RAdamW_{20,10,4}$}  & \textbf{17.37} & 0.00           & 0.00           & 63.34          & \textbf{86.15} & 0.00           & 89.05          & 89.21          & \textbf{80.28} & 47.27 \\
\textbf{$gRAdamW_{20,10,4}$} & 16.75          & 0.00           & 0.00           & \textbf{63.54} & 86.05          & \textbf{0.61}  & \textbf{89.34} & \textbf{91.37} & 78.87          & \textbf{47.39}     \\ \bottomrule    
\end{tabular}

\label{glue_9b_first_100}
\end{table*}

\section{AdamW Weight Decay Analysis}
AdamW implements a strategy called weight decay, which decays the trainable weights after each gradient update by $\theta_t=\theta_t-\alpha\lambda\theta_t$. Instead of the original Adam algorithm, AdamW separates the weight decay from the gradient update, which leads to better performance in some cases. To comprehensively prove the effectiveness of our gate-based rescaling method over Riemannian preconditioned AdamW, we evaluate our boosts across various weight decay factors $\lambda$. Results are exhibited in Table \ref{weight_decay_table}.

%%%%%%%%%%%%%%%%%%%%%% Table 4 -  AdamW Weight Decay %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[h]
\centering
\caption{ScienceQA boosting performances under Llama-3.2-3B, across different AdamW weight decay.}
\begin{tabular}{lcccc}
\toprule
                             & \textbf{0}     & \textbf{1e-5}  & \textbf{1e-4}  & \textbf{1e-3} \\ \midrule
\textbf{$RAdamW_{20,10,4}$}  & 82.60          & 83.50          & 83.23          & 83.99              \\
\textbf{$gRAdamW_{20,10,4}$} & \textbf{83.80} & \textbf{84.58} & \textbf{84.98} & \textbf{84.67}              \\
\textbf{Boost}               & \textcolor{red}{1.45\%$\uparrow$}         & \textcolor{red}{1.30\%$\uparrow$}         & \textcolor{red}{2.10\%$\uparrow$}         &  \textcolor{red}{0.81\%$\uparrow$}            \\ \bottomrule
\end{tabular}

\label{weight_decay_table}
\end{table*}

\section{Multi-Task Performance}
One of the most valuable features of MoE architectures is their capability of modeling multiple tasks. Through gating mechanism, the MoE system adeptly delegates specific tasks to individual experts, thereby facilitating a more focused and efficient learning process within each expert module. As a result, one question arises regarding our proposed gate-based rescaling approach: Can it still effectively augment the performance of MoE architectures in multi-task scenarios? 

To illustrate this, we manually construct a mixed dataset consisting of two irrelevant natural language tasks, ScienceQA and MRPC. ScienceQA is a question-answering benchmark that mainly centers on multiple-choice questions from primary and secondary school science curricula, while MRPC is designed as a sentence pair task for identifying whether two sentences are equivalent. This combination is roughly balanced since their testing datasets both consist of around 2,000 samples. We still construct a mixture of LoRA modules with a total of 20 experts, a rank of 4 for each expert, and a selection of top-10 experts activated each time. Since a mixed task is more complex to train, we increase the initial learning rate to \num{3e-4} and \num{1e-4} for SGD and AdamW experiments respectively. We train all the candidates for 1,600 steps. Results are exhibited in Table \ref{multitask}. We still can witness a significant boost by our gate-based rescaling in terms of the Riemannian preconditioned SGD optimizer.

%%%%%%%%%%%%%%%%%%%%%% Table 5 -  Multi-Task Experiment %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[h]
\centering
\caption{Conventional and gate-rescaled optimizers performed on the mixed dataset consisting of ScienceQA and MRPC. All candidates are trained for 1,600 steps. Our gate-based rescaling method still contributes enhancement for SGD optimizer.}
\begin{tabular}{cccc}
\toprule
\textbf{$RSGD_{20,10,4}$} & \textbf{$gRSGD_{20,10,4}$} & \textbf{$RAdamW_{20,10,4}$} & \textbf{$gRAdamW_{20,10,4}$} \\ \midrule
45.43                     & \textbf{49.08}             & \textbf{52.14}              & 52.06      \\ \bottomrule                 
\end{tabular}

\label{multitask}
\end{table*}

\section{Method Implementation}
The engineering alternative solution of the gate-based rescaling approach is to manually separate the forwarding into optimizable and unoptimizable components. Here we provide our implementation in Python-like pseudocode. We only update two lines of the original MoE-LoRA code. 

\begin{algorithm}
\caption{Engineering Alternative Solution of Gate-based Rescaling Method}
\label{alg1}
\begin{lstlisting}[language=Python]
def forward(self, x, ...):
    ...
    # compute gate values
    gvs = ...
    ...
    # execute each activated expert
    for exp_id in activated_experts:
        A = self.As[exp_id]
        B = self.Bs[exp_id]
        gv = gvs[:,:,exp_id]
        exp_out = B(A(x))
        sqrt_gv = (gv**0.5).detach() # update 1
        w_exp_out = sqrt_gv*exp_out+(gv-sqrt_gv)*exp_out.detach() # update 2
        result = result + w_exp_out
        ...
\end{lstlisting}
\end{algorithm}

\section{Experimental Details}
We present our experimental details in Table \ref{expdetails}. All experiments in this paper follow this configuration unless they specify their particular settings. For training steps, some of the experiments may converge earlier, therefore we perform an early stop for those experiments. We constrain the maximum of training steps by 2,000, considering it a relatively fair setup for various downstream tasks, especially those with different scales of training corpora but in the same level of complexity. 



%%%%%%%%%%%%%%%%%%%%%% Table 6 -  Experimental Details %%%%%%%%%%%%%%%%%%%%%%
\begin{table*}
\centering
\caption{Default experimental details implemented throughout this paper. All experiments follow this configuration unless they specify their particular settings, like the MoE structural experiments, baselines comparing experiments, and the experiments of AdamW weight decay.}
\begin{tabular}{lcc}
\toprule
                                                & \textbf{SGD}                          & \textbf{AdamW}                                       \\ \midrule
\noalign{\vskip 2pt}
Train batch size (logical)                               & \multicolumn{2}{c}{80 for textual tasks, 40 for vision-language tasks}                                                                       \\
\noalign{\vskip 2pt}
Max training steps                              & \multicolumn{2}{c}{$<=2000$}                                                                 \\
\noalign{\vskip 2pt}
\multirow{5}{*}{Initial lr for 3B (expert)} & QA, GLUE: 3e-5 & QA: 1e-5                     \\
                                                & WNLI: 3e-6     & MRPC, CoLA, QNLI, STS-B: 3e-5 \\
                                                & Multi-Task: 3e-4                   & SST-2, QQP, MNLI, WNLI: 1e-5  \\
                                                &                                       & RTE: 1e-6                     \\
                                                &   & Multi-Task: 1e-4 \\
\noalign{\vskip 2pt}
\multirow{3}{*}{Initial lr for 9B (expert)} & CoLA, SST-2, MRPC: 3e-6      &   CoLA, SST-2, MRPC: 1e-6            \\
                                                & STS-B, QQP, MNLI, QNLI, RTE, & STS-B, QQP, QNLI, RTE, WNLI: 1e-5 \\
                                                & WNLI: 3e-5 & MNLI: 3e-6 \\
\noalign{\vskip 2pt}
Initial lr for 7B (expert) & 3e-5 & 1e-5 \\
\noalign{\vskip 2pt}
Initial lr (gate)                    & \multicolumn{2}{c}{3e-8}                                              \\
\noalign{\vskip 2pt}
Lr scheduler (expert)                         & \multicolumn{2}{c}{Linear}                                                                   \\
\noalign{\vskip 2pt}
Warmup steps                                    & \multicolumn{2}{c}{0}                                                                        \\
\noalign{\vskip 2pt}
Max gradient norm (gate)                        & \multicolumn{2}{c}{1.0}                                                                      \\
\noalign{\vskip 2pt}
Default LoRA expert rank                                & \multicolumn{2}{c}{4}                                                                        \\
\noalign{\vskip 2pt}
Default number of experts                               & \multicolumn{2}{c}{20}                                                                       \\
\noalign{\vskip 2pt}
Default activated Top-K             & \multicolumn{2}{c}{10}                                                                       \\
\noalign{\vskip 2pt}
LoRA $\alpha$                                   & \multicolumn{2}{c}{16}                                                                       \\
\noalign{\vskip 2pt}
LoRA dropout                                    & \multicolumn{2}{c}{0.05}                                                                     \\
\noalign{\vskip 2pt}
Default weight decay                                    & / & 0 \\
\noalign{\vskip 2pt}
$\beta_1, \beta_2, \epsilon$                    & /                                     & 0.9, 0.999, 1e-6 \\ \bottomrule            
\end{tabular}

\label{expdetails}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
