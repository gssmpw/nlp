\section{Related Works}
\subsection{LoRA and LoRA Variants}
LoRA~\cite{lora} decomposes a full-rank matrix into a product of two low-rank matrices, which has been widely considered an effective solution for parameter-efficient fine-tuning. Studies have proposed several variants to reform LoRA: For initialization, PISSA~\cite{pissa} leverages singular value decomposition (SVD) to obtain the principal singular components of $W$, while MiLoRA~\cite{milora} utilizes secondary singular values and vectors. LoRA-Pro~\cite{lorapro} and LoRA-GA~\cite{lora-ga} approximate the direction of initial gradients to align them with that of the fully fine-tuning. LoRA+~\cite{lora+} introduces a learning rate separating strategy with $\eta_B > \eta_A$. ResLoRA~\cite{reslora} and SIBO~\cite{sibo} accelerate convergence and mitigate over-smoothing by introducing residual paths. DoRA~\cite{dora} decomposes the weight vector into direction and magnitude and only uses its direction component. rsLoRA~\cite{rslora} proposes a rank-stabilized scaling factor $\lambda_t = r_t^{1/2}$ to ensure stable gradient updates. To prevent overfitting, BiLoRA~\cite{bilora} adopts a bi-level optimizing strategy, while others implement dropout mechanisms~\cite{hiddenkey, loradropout}.

\subsection{Mixture of LoRAs}
MoE has emerged as a critical framework for addressing complex tasks. By incorporating multiple expert modules, it dynamically selects appropriate experts based on specific inputs~\cite{moe}. Early studies, such as LoRAMoE~\cite{loramoe} and MixLoRA~\cite{mixlora}, have pioneered the introduction of the MoE-LoRA architecture by integrating LoRA experts for both global and downstream tasks. Afterward, MoE-LoRA has demonstrated its effectiveness across a range of fields such as continual learning~\cite{loramoe, yang2024moral}, vision-language multi-model tasks~\cite{mocle, llava}, and multi-task applications~\cite{MOElora}.

Recent studies have focused on enhancing MoE-LoRA through architectural advancements and improved training strategies. For instance, MoLA~\cite{mola} allocates a varying number of experts at different layers, and MixDA~\cite{mixda} introduces multiple domain-adaptive modules to support multi-domain knowledge. Other methods such as \cite{moslora, MOElora, mole, mocle, adamix} have also been proposed for strengthening MoE-LoRA. To boost the training of MoE-LoRA, Luo et al.~\cite{MoEloraa} address the random routing issue by introducing a contrastive loss. At the same time, MoV~\cite{mov} chooses to combine lightweight vectors with a sparse selection mechanism for efficient expert allocation. Other approaches, including \cite{loramoe, mixlora, sira}, focus on load balancing among experts. However, to the best of our knowledge, there is still a lack of work on gradient optimizing specifically for MoE-LoRA models. 

\subsection{Gradient Preconditioners}
In most deep learning cases, gradient descent algorithms update model parameters by calculating gradient-based updates. To accelerate the optimizing process, the concept of gradient preconditioning has been introduced. Advanced techniques such as Adagrad~\cite{adagrad} dynamically adjust the learning rate by an accumulated squared gradients \( G_t = \sum_{i=1}^{t} g_i^2 \) and update model by \( \Delta \theta_t = -\eta G_t^{-1/2} \cdot g_t \). Adam~\cite{adam} extends this approach by incorporating momentum and bias correction, scaling gradients through a diagonal preconditioner, and resulting in updates in the form of \( \Delta \theta_t = -\eta \frac{m_t}{\sqrt{v_t} + \epsilon} \), where \( v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \). AdamW~\cite{adamw} further introduces a weight decay to Adam. 

Recent studies have provided theoretical support for scaled gradient descent methods under different preconditioning strategies. The core idea is to adjust both the direction and magnitude of updates by introducing a scaling matrix to gradients.  
Tong et al.~\cite{tong2021low} demonstrate the local convergence of scaled gradient descent methods. Jia et al.~\cite{jia2024preconditioning} extend this work by proving global convergence of scaled gradient descent for the least-squares matrix decomposition problem \( \|AB^T - Y\|_F^2 / 2 \), showing that this approach achieves global convergence under different condition numbers. Other variants of scaled gradient descent have also emerged, such as Zhang et al. who proposed two regularization strategies ~\cite{zhang2023preconditioned,zhang2024fast}. In higher-dimensional settings, scaled gradient descent has been further extended to tensor optimization ~\cite{tong2022scaling,ma2023provably}. Mishra et al.~\cite{mishra2013low, mishra2016riemannian} also applied the principles of Riemannian to the optimization involving low-rank matrices. Considering the data's manifold geometry, a Riemannian metric $g_p(v, w)$ is introduced to guide gradient updates along the manifold. Recently, Zhang et al.~\cite{riemannian} introduced the idea of Riemannian preconditioners to LoRA by attaching an \( r \times r \) preconditioner to the gradients of low-rank matrices. As a result, they provide improved fine-tuning performance of LoRA, compared with conventional gradient optimizers such as SGD and AdamW.