\section{Related Works}
\subsection{LoRA and LoRA Variants}
LoRA____ decomposes a full-rank matrix into a product of two low-rank matrices, which has been widely considered an effective solution for parameter-efficient fine-tuning. Studies have proposed several variants to reform LoRA: For initialization, PISSA____ leverages singular value decomposition (SVD) to obtain the principal singular components of $W$, while MiLoRA____ utilizes secondary singular values and vectors. LoRA-Pro____ and LoRA-GA____ approximate the direction of initial gradients to align them with that of the fully fine-tuning. LoRA+____ introduces a learning rate separating strategy with $\eta_B > \eta_A$. ResLoRA____ and SIBO____ accelerate convergence and mitigate over-smoothing by introducing residual paths. DoRA____ decomposes the weight vector into direction and magnitude and only uses its direction component. rsLoRA____ proposes a rank-stabilized scaling factor $\lambda_t = r_t^{1/2}$ to ensure stable gradient updates. To prevent overfitting, BiLoRA____ adopts a bi-level optimizing strategy, while others implement dropout mechanisms____.

\subsection{Mixture of LoRAs}
MoE has emerged as a critical framework for addressing complex tasks. By incorporating multiple expert modules, it dynamically selects appropriate experts based on specific inputs____. Early studies, such as LoRAMoE____ and MixLoRA____, have pioneered the introduction of the MoE-LoRA architecture by integrating LoRA experts for both global and downstream tasks. Afterward, MoE-LoRA has demonstrated its effectiveness across a range of fields such as continual learning____, vision-language multi-model tasks____, and multi-task applications____.

Recent studies have focused on enhancing MoE-LoRA through architectural advancements and improved training strategies. For instance, MoLA____ allocates a varying number of experts at different layers, and MixDA____ introduces multiple domain-adaptive modules to support multi-domain knowledge. Other methods such as ____ have also been proposed for strengthening MoE-LoRA. To boost the training of MoE-LoRA, Luo et al.____ address the random routing issue by introducing a contrastive loss. At the same time, MoV____ chooses to combine lightweight vectors with a sparse selection mechanism for efficient expert allocation. Other approaches, including ____, focus on load balancing among experts. However, to the best of our knowledge, there is still a lack of work on gradient optimizing specifically for MoE-LoRA models. 

\subsection{Gradient Preconditioners}
In most deep learning cases, gradient descent algorithms update model parameters by calculating gradient-based updates. To accelerate the optimizing process, the concept of gradient preconditioning has been introduced. Advanced techniques such as Adagrad____ dynamically adjust the learning rate by an accumulated squared gradients \( G_t = \sum_{i=1}^{t} g_i^2 \) and update model by \( \Delta \theta_t = -\eta G_t^{-1/2} \cdot g_t \). Adam____ extends this approach by incorporating momentum and bias correction, scaling gradients through a diagonal preconditioner, and resulting in updates in the form of \( \Delta \theta_t = -\eta \frac{m_t}{\sqrt{v_t} + \epsilon} \), where \( v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \). AdamW____ further introduces a weight decay to Adam. 

Recent studies have provided theoretical support for scaled gradient descent methods under different preconditioning strategies. The core idea is to adjust both the direction and magnitude of updates by introducing a scaling matrix to gradients.  
Tong et al.____ demonstrate the local convergence of scaled gradient descent methods. Jia et al.____ extend this work by proving global convergence of scaled gradient descent for the least-squares matrix decomposition problem \( \|AB^T - Y\|_F^2 / 2 \), showing that this approach achieves global convergence under different condition numbers. Other variants of scaled gradient descent have also emerged, such as Zhang et al. who proposed two regularization strategies ____. In higher-dimensional settings, scaled gradient descent has been further extended to tensor optimization ____. Mishra et al.____ also applied the principles of Riemannian to the optimization involving low-rank matrices. Considering the data's manifold geometry, a Riemannian metric $g_p(v, w)$ is introduced to guide gradient updates along the manifold. Recently, Zhang et al.____ introduced the idea of Riemannian preconditioners to LoRA by attaching an \( r \times r \) preconditioner to the gradients of low-rank matrices. As a result, they provide improved fine-tuning performance of LoRA, compared with conventional gradient optimizers such as SGD and AdamW.