\begin{figure}[t]
	\centering
	\includegraphics[width=0.75\linewidth]{retrieval-2}
    \vspace{-0.25em}
	\caption{\textbf{Overview of a retrieval.} \normalfont We compute a ranking using target selection method $\rho$ which assigns each function $\functionloc \in \functionlocs$ a relevance score $\hat{r}_\functionloc$. To measure the quality of target selection method, we compute the $NDCG_k$ for a retrieval $\functionlocssubset$ with cardinality $k$. As ground truth, we use the oracle $\relevanceoracle$ to assign relevance scores $r_\functionloc$ to each location $\functionloc \in \functionlocs$.}
	\label{fig:recallatk}
\end{figure}

As a first step for addressing these requirements,  we present our method for assessing the effectiveness of target selection methods. Our approach is based on the observation that target selection fundamentally resembles an \emph{Information Retrieval} (IR) problem: that is, target selection can be conceptualized as the \emph{retrieval} of relevant target locations in a software project using a scoring mechanism.
%
In this formulation, targets are \emph{relevant} with regard to the directed fuzzing process, if they benefit the effectiveness of a fuzzer in terms of uncovering new bugs. A fuzzer can use different detection mechanisms to identify whether it triggered a bug; the most common one used by virtually all fuzzers are program crashes, which we focus on subsequently. In other words, targets are relevant if they can point to a code location at which the fuzzer finds a program crash. Similarly, a target can be considered \emph{irrelevant} if it does not contribute to the discovery of a defect and the fuzzer essentially wastes time exploring it. Consequently, a selection methods performs better if it assigns higher scores to relevant code during fuzzing.

Considering target selection as an information retrieval problem yields three key advantages: First, it allows the evaluation of selection methods agnostic to a particular implementation of a directed fuzzer.
Second, it enables to capture different requirements for discrete and continuous target selection methods in form of discrete and continuous retrieval.
%
Third, we can employ standard evaluation measures from the information retrieval domain to compare and assess the selection on a well-established ground.

In this work, we focus on a function-level granularity as the input for the target selection method.
%
Intuitively speaking, the optimal target selection method would, thus, precisely \emph{retrieve} the subset $\functionlocssubset$ of all functions $\functionlocs$ within a given software project that lead to a crash. %As the fuzzer must be able to observe this bug, we 
%
However, given that the exact size of subset $\functionlocssubset$ is typically unknown, we instead consider the $k$ highest-rated functions as determined by the target selection's scoring method.
%
This scoring can be described as a mapping $\rho$ from functions $\functionloc$ to relevance scores $r$:
$$
   \rho \colon \functionlocs \longrightarrow \mathbb{R}^+, \quad 
   \functionloc \mapsto \relevancescore \,.
$$
For simplicity, we assume that any external information required for the scoring is embedded into the input to $\rho$.
%
Based on these scores, we then compute a ranking over $\functionlocs$ and retrieve the subset $\functionlocssubset$ as the first $k$ entries in this ranking. Subsequently, subset $\functionlocssubset$ is returned as the output of the target selection.

\boldpar{Ranked retrieval measure}
As common in information retrieval, we distinguish between relevant and irrelevant objects, i.e., whether a bug is present or not.
%
Thus, to assess the effectiveness of a particular retrieval $\functionlocssubset$, two requirements need to be taken into account: (1) the number of relevant functions in the retrieval, and (2), their respective positions in the ranking.
%
Metrics like $precision@k$ or $recall@k$ are often used for the former, but fall short in capturing the quality of the ranking. While this would not be a problem for discrete scoring functions, we would lose information for continuous ones. Therefore, we consider the \emph{normalized discounted cumulative gain} (NDCG), a standard performance measure from the information retrieval literature, which accounts for both the relevance of a retrieved function and its rank.

To assign a relevance $\hat{r}_\functionloc$ to each function $\functionloc \in \functionlocs$, we assume there exists an oracle $\relevanceoracle: \functionlocs \rightarrow \{0, 1\}$, that assigns 1 to functions which appeared in the stack trace of a crash, and 0 otherwise.
We can then use this oracle to assign each function a ground truth relevance score $\hat{r}_\functionloc = \relevanceoracle(\functionloc)$.
%
Functions which are likely to appear in the stack trace of every crash, such as the main function or functions introduced by sanitizers, are assigned a relevance 0, regardless.

Based on these scores and retrieval $F$ with length $k$, we compute the $\text{NDCG}_k$ in three steps: First, we compute the cumulative gain as the sum of the ground truth relevance scores $\hat{r}_\functionloc$ of all retrieved functions $\functionloc\in F$. This ensures the first requirement. To account for the second requirement, we reduce each relevance score proportional to the rank $i$ of function $\functionloc$ using a discount function (e.g., the binary logarithm~\cite{WanWanLiHe13}).
%
Finally, we normalize the resulting gain to account for stack traces with different lengths. Therefore, we calculate the ideal discounted cumulative gain $\text{IDCG}_k$ \change{as in the previous two steps but assume a perfect ranking (i.e., all relevant functions are positioned at the top ranks).}
%
\noindent
Formally, the performance measure is thus defined as
$$
\text{NDCG}_k \coloneqq  \frac{1}{\text{IDCG}_k} \sum_{i=1}^{k} \frac{\hat{r}_i}{\text{log}_2(i+1)} \,.
$$
\cstart 
A perfect $NDCG_k$ score of 1 indicates that the relevant functions were returned at the top ranks and a score of 0 implies that no relevant function was among the $k$ retrieved ones. 
%
Using such a ranking-based measure rather than the scores directly enables a more robust comparison between different scoring methods. In particular, in this case, it does not matter whether one method generally assigns higher scores than another.
\cend

\boldpar{Matching policies}
So far, we were operating under the implicit assumption that all functions in a given stack trace are relevant for a particular crash.
%
However, this assumption is often inaccurate. In reality, it is generally challenging to pinpoint the exact \emph{root cause} of a crash~\cite{BlaSchAscAbb+20}. Consequently, we often lack precise knowledge about the specific function responsible for introducing a bug. Additionally, crashes might manifest only through particular sequences of function calls, and may not result from a single flawed function~\cite{ZhuBÃ¶h21}.

To account for this uncertainty, we opt for a middle ground by over-/ and underestimating our gain. Therefore, we introduce two matching policies:

\italicpar{Optimistic matching} First, we consider an optimistic matching policy that assigns a relevance score of 1 only to the \emph{first} retrieved function from the stack trace. We denote this with $\text{NDCG}^+$. As a consequence, this \emph{overestimates} the performance of a target selection method and serves as a upper bound in our analysis.

\italicpar{Pessimistic matching} Second, we consider a pessimistic policy that assigns a relevance score of 1 to \emph{all} retrieved functions from the stack trace. We denote this with $\text{NDCG}^-$. This underestimates a target selection method and serves as a lower bound.
