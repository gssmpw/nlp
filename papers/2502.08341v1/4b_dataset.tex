\begin{figure}[t]
	\centering
	\includegraphics[width=.85\linewidth]{dataset_pipeline}
	\caption{\textbf{Dataset generation process.} \normalfont As basis for our analysis, we collect $1,621$ reproducible crashes. We crawl \ossfuzz, which yields a crashing input and a fuzzing configuration for a project (\stepone). To reproduce the crash, we search for a commit of the project which crashes (\steptwo) when executed under the input from \ossfuzz{} (\stepthree ). Once we reproduce a crash, we extract functions from the project's code and label them according to the stack trace (\stepfour).}
	\label{fig:dataset-process}
\end{figure}

So far, we discussed how we can conceptualize and evaluate target selection methods from an information retrieval perspective. The only missing piece for our evaluation is a ground truth dataset composed of code locations annotated based on whether bugs are present or absent there.

A possible approach to obtain such data points is to fuzz various open-source projects, collect information about identified bugs, and trace back their location within the software project. Unfortunately, this process is very time-consuming and there is no guarantee that we will find all bugs in a given project, which would introduce false negatives to our analysis. As an alternative, we opt to establish our ground truth data by leveraging historical fuzzing campaigns, and, more precisely, crashes detected by the \ossfuzz{}~\cite{Ser+23} project. \ossfuzz{} has consistently applied fuzz testing to an extensive array of open-source projects since 2016 and publicly discloses their findings. While there is still no guarantee that OSS-Fuzz identified every bug, the massive amount of time spent on fuzzing these targets is likely to find a large majority of crashes reachable by a fuzzer.

For using crashing inputs identified from fuzzing for our ground truth, we require two things: (1) Crashes need to be reproducible to verify that they are correct, and (2) we need information about which specific functions are involved in the crash. A good approximation for this is provided by the stack trace of the crash.
%
Thus, to utilize the data provided by \ossfuzz{}, we must address two primary challenges: First, \ossfuzz{} only releases the time at which a bug is reported and sometimes provides a regression range for when it was resolved but the exact commit of the software project is not disclosed. Second, and more importantly, \ossfuzz{} only shares detailed findings with authorized individuals (i.e., the project maintainers). In particular, the stack trace of a crash is not publicly accessible.

To remediate both issues, we pinpoint a commit at which the crash occurs, which in turn enables us to gather the relevant information for our ground truth. The details of this process are visually depicted in Figure~\ref{fig:dataset-process} and further described in the following.

\begin{figure}
	\begin{center}
		\input{tikz/crashclasses.tex}
	\end{center}
	\caption{\textbf{Crash types.} \normalfont We show the top ten crash types in our ground truth dataset.}
	\label{fig:crash-types}
\end{figure}

\boldpar{Data collection} 
We start our collection process by scraping the \ossfuzz{} issue tracker, which holds information about each crash, such as the project, reporting time, used fuzzer, sanitizer, and the crashing input~(\stepone). We then select a commit close in time to the initial crash detection~(\steptwo). We rollback the project to this commit and build it with the configuration used by OSS-Fuzz to detect the crash. In case the project build fails, we retry the process by selecting an older commit. In some cases, there are broken references to external dependencies that we fix manually. Once a project is successfully built, we execute the crashing input (\stepthree). If the crash can be observed, we collect a stack trace. If the program does not crash, we go back to \steptwo and retry with an older commit.

After successfully recreating the crash and extracting stack trace information, we proceed to extract every function from the project's source code~(\stepfour). Many projects use pre-compiler directives to enable or disable certain features at compile time, which might include or exclude various code sections. Directly using the source code would thus pose a disadvantage to target selection methods working on the source code level: The source code may actually exhibit defective behavior that, however, will never be labeled as such by \ossfuzz{}, as the defective feature is always disabled via pre-compiler directives. To counteract this, we resort to post-pre-compiler code as the function representation for our dataset. For simplicity, we continue to refer to this code as source code.

\boldpar{Regarding the method's relevance oracle}
The previous section's oracle $\relevanceoracle$ can be approximated by utilizing the information gathered from \ossfuzz{}. Specifically, we can use historic stack trace data and the mapping to the extracted source code to assign functions a score based on whether they were part of a stack trace of a crash found by \ossfuzz{}. This enables us to assemble a labelled corpus that we can further use to compare the retrieval performance of target selection methods to each another.

\begin{figure}
	\begin{center}
		\input{tikz/traceback_length.tex}
	\end{center}
	\caption{\textbf{Stack trace lengths.} \normalfont We show the frequency of stack trace lengths from reproduced crashes. We observe the dominant peak at 7 functions per stack trace.}
	\label{fig:traceback-length}
\end{figure}

\boldpar{Dataset statistics} We have successfully reproduced 1,621 crashes across 97 C/C++ projects discovered by \ossfuzz{} between 2016 and 2023. We categorize crashes into 48 distinct crash types based on the report by the sanitizer used for identification. Figure \ref{fig:crash-types} provides an overview of of the top ten crash types that contribute for approximately 75\% of the crashes in the corpus, with heap overflows being the most prominent class.
The distribution of crashes in the corpus, broken down by sanitizers, is further illustrated in Figure~\ref{fig:sanitizers} showing that most crashes were identified by address sanitizer, which in total accounted for 62\% of the crashes. The remainder of the crashes are identified by memory sanitizer and undefined behavior sanitizer, which make up 31\% and 7\% of the crashes respectively. Finally, we present the frequency distribution of stack trace lengths in Figure~\ref{fig:traceback-length}. We find the traceback length frequency shows a main peak at seven and a second, less pronounced, peak at 248. While a diverse set of crashes contributes to the former, the latter mostly consists of stack overflow crashes, that is, errors caused by reaching the stack limit.

The dominance of specific crash types and sanitizers used to identify the crashes should be taken into account for the comparison of the target selection methods. 

\begin{figure}
	\begin{center}
		\input{tikz/sanitizers.tex}
	\end{center}
	\caption{\textbf{Number of crashes per sanitizer.} \normalfont We show the number of crashes in the corpus broken down by sanitizers.}
	\label{fig:sanitizers}
\end{figure}