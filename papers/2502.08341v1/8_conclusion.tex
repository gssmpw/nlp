Our analysis adds a new piece to the puzzle of understanding fuzzing performance.
Previous research in directed fuzzing focused on faster and deeper exploration of targets.
While this challenge continues to hold enough problems for future work, we demonstrate that the targets within the programs also play a crucial role and the effectiveness of directed fuzzing stands or falls with the quality of the targeted code locations. Our work thus helps to decouple orthogonal factors of performance and provides a better understanding of the \emph{where to fuzz} in the discovery of defects.

Furthermore, the results of our analysis provide a surprising insight: Despite extensive research on locating interesting code for fuzzing, classic software metrics still outperform more sophisticated counterparts in target selection. Given the capabilities of some pattern-based methods, this is unexpected and suggests significant room for improvement. In particular, we consider recent machine learning models a promising alternative, as these models can acquire remarkable skills and thus potentially push the performance of target selection forward,  generating a better basis for directed fuzzing in practice.

We make our source code and data publicly available online at \url{https://github.com/wsbrg/crashminer}.