\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{images/corpus.pdf}
	\caption{\textbf{Retrieval and evaluation process.} \normalfont Our crash corpus consists of various projects with functions labelled as relevant (\raisebox{-0.15em}{\FiveStarOpen}) or irrelevant. A target selection method is used to create a ranking of the functions with the $k$ highest ranks forming the retrieval. For each retrieval we calculate the NDCG and average the results across all crashes in every project of our corpus, leading to the $\mu\text{NDCG}_k$ as the retrieval score for a selection method on $k$ sized retrievals.} 
	\label{fig:retrieval-process}
\end{figure}

\cstart
\begin{figure*}[t]
	\begin{center}
		\input{tikz/mean_gains.tex}
	\end{center}
	\caption{\textbf{Mean retrieval scores of the target selection methods.} \normalfont Mean NDCG scores of the target selection methods across the whole corpus for the first $k$ retrieved functions. Target selection methods belonging to the same general class of approaches, i.e., SAST tools, vulnerability prediction models, code metrics, and sanitizer instrumentation-based share the same color.}
	\label{fig:mean-gains}
\end{figure*}
\cend

As discussed in Section \ref{sec:methodology}, the problem of finding suitable targets in a project to guide a fuzzer towards can be framed as an information retrieval problem. As such, we approach the evaluation of the target selection methods with a standard metric to measure the quality of retrieval algorithms with, the NDCG.

In Section \ref{sec:dataset}, we described the assembly of our dataset based on information from \ossfuzz{}.
By reproducing crashes of projects in defective versions, we gathered stack traces which we used
to assign a relevance score to each function in the respective project. This way we ended up with a corpus
of projects in specific versions and functions labelled as relevant or irrelevant, based on whether they appeared in the stack trace of the crash, or not. This is shown in Figure \ref{fig:retrieval-process}.  We understand target selection as the retrieval of these relevant functions for each project in a defective version. To that end, the functions are ranked by using a target selection method with the $k$ highest ranking function forming the retrieval. We calculate the NDCG for each of these retrievals and take the mean of the NDCG score, $\mu\text{NDCG}_k$, to quantify the retrieval performance of the target selection method.

As our labeling is based on crash stack traces and not every function in a given stack trace is defective, we have to deal with a certain degree of label noise, which we counteract by using one metric that under-estimates and one
that over-estimates the true quality of our selection methods, the $\text{NDCG}^-$ and the $\text{NDCG}^+$, respectively, as described in Section \ref{sec:methodology}.

In this section, we compare the retrieval performance of the target selection methods. As noted in Section~\ref{sec:dataset}, different crash types do occur with different frequencies and are, hence, included in our dataset with varying amounts. The same holds for crashes found by the three sanitizers that we took into account for the crash reproduction. This makes it necessary to take a closer look at the target selection methods with regard to their quality for those categories in our dataset.
However, before examining these specific cases, we begin by taking the general retrieval performance into the focus.


%%%
%%% General performance
%%%
\boldpar{General performance}
The performance of the target selection methods is shown in Figure
\ref{fig:mean-gains}. To get an intuition of how to interpret the $\mu\text{NDCG}_k$ scores, recall that the NDCG expresses the degree to which the ranking
created by a selection method matches an ideal ranking. An ideal ranking would exclusively assign functions from a crash stack trace to the first ranks; thus, an ideal NDCG score of 1.0 would
express that a given target selection algorithm is able to match this ranking and
thus predict crashing functions perfectly.

One particularity we observe in the case of the under-estimating $\text{NDCG}^-$ is the drop in retrieval performance for for $k > 1$ retrieved functions for Leopard-V, as can be seen in Figure \ref{fig:mean-gains} a).  When considering solely the top-ranking function (i.e.,\; $k=1$), the Leopard-V method exhibits a NDCG score of 0.13. This implies that the initial function in the ranking of this method aligns with that of an ideal algorithm in 13\% of instances. Put differently,
the highest ranking function of Leopard-V is part of a crash's stack trace in 13\%
of the cases across every crash in our corpus. With an increasing number of retrieved functions, i.e., greater values for $k$ in the plot, the under-estimating $\text{NDCG}^-$ drops, which
shows that the target selection method does not continue to rank the other
functions from the stack trace as highly. When taking into account that the
under-estimating matching policy assigns an equal relevance to every function from
the stack trace, this indicates, that only one of the stack trace's functions is
considered very likely to be defective by the selection method. This could be explained
by the fact that in many cases only one function from the stack trace actually is
defective in practice.
%
This behavior does not arise in the over-estimating case, as in this
case only the highest ranking function from the stack trace is assigned a relevance~$\relevancescore > 0$.

% Statistics brief description
In order to further investigate the relationship of the respective methods to each other, we conducted a Mann-Whitney-U test to identify significance\footnote{We consider $p < 0.05$ statistical significant. We publish p-values together with our source code.}. Furthermore, we refer to a method as \textit{dominant} over another, if its distribution of $\text{NDCG}_k$ scores is significantly higher for every number $k$ of retrieved functions.

\cstart
Let us briefly summarize our insights in this regard. First, we shift our focus to the case in which at least 25 functions were retrieved ($k \geq 25$) as this marks the point from which on every method, except for Linevul, significantly outperforms the random baseline. In this case, we find four clusters of target selection methods which can be recognized particularly well in the over-estimating plot in Figure \ref{fig:mean-gains} b). The clusters are characterized by the fact that the methods within a cluster do not dominate one another, but each method of one cluster dominates each method of the one beneath. The first cluster is formed by Leopard-V and Leopard-C which outperform every other method. The second one consists of the sanitizer-based method, CodeT5+, ReVeal, and the recent code changes heuristic. This is followed by the SAST tools cluster containing Rats and Cppcheck which still dominate the last cluster, Linevul and the random scoring. The original Linevul model likely performs similarly to the random baseline as it picks up spurious correlations with code formatting~\cite{brokenPromises}.

When we shift our view to less than 25 retrieved functions ($k < 25$), we find a different picture. We observe that the sanitizer-based method performs better than the other three methods with which it forms a cluster for $k \geq 25$ for a few number of returned functions. More precisely, it significantly outperforms the next best methods CodeT5+ and ReVeal for $k < 5$. Also, for the same interval, the method based on how recently a code location was last modified performs significantly worse than the next best methods CodeT5+ and ReVeal. This shows how methods that do not significantly differ for a larger number of retrieved functions can perform vastly different when queried for a few number of relevant functions.

To understand the practical implications of these results, we take a closer look at the case of AFLChurn \cite{ZhuBÃ¶h21}. In their case study the authors compare their proposed directed fuzzer, AFLChurn, which can operate on targets from a continuous target selection method, to AFLGo, which requires a discrete one. For the target selection they assign each code locations a continuous score based on how recently it was modified. Consequently, AFLGo, requiring a discrete set of targets, must decide on a threshold score or a maximum amount of code locations to select as targets for the fuzzing process.
%
When measuring the time to expose a crash in libhtb, one of their target programs, the authors find that AFLGo takes 3 hours and 12 minutes on average while AFLChurn clocks in at 2 hours and 1 minute. Now, our experiments add a new angle to explain this performance gain. We observe that a target selection based on how recently a code locations was last modified only picks up performance with an increasing number of retrieved functions compared to the sanitizer or even code metric-based methods. Consequently, in this case a fuzzer that requires a discrete selection of targets and must therefore decide on a threshold score or maximum amount of targets cannot profit as well as one operating with a continuous target selection method can.

We hypothesize that the difference of the approaches would have been less pronounced for a method that also performs well for a low number of retrieved functions, such as Leopard-V, a sanitizer-based method or Leopard-C. Also, we conjecture that a target selection approach should always be chosen in accordance with the requirements of the fuzzing approach. For example, machine learning-based approaches, such as our fine-tuned CodeT5+ or ReVeal, might work well for fuzzers that can operate on continuous target selections, while showing sub-optimal performance for those that cannot. However, more research is necessary in this direction to better substantiate these hypotheses. % with empirical evidence.

\cend

\begin{tcolorbox}[boxrule=1pt, arc=1mm, colback=white, colframe=coldefault]
	Target selection methods based on software metrics perform significantly
	better than every other considered method. The best software metric,
	Leopard-V, correctly captures as much as 13\% of the crashes with its
	highest ranking function across the whole corpus of more than 1600 crashes. This makes it the most natural and really only viable candidate for
	  fuzzing approaches which require a discrete selection method.
\end{tcolorbox}

\boldpar{Impact of sanitizers}
Our corpus consists of twice as many bugs found by address sanitizer than memory sanitizer or undefined behavior sanitizer combined. Therefore, only taking the retrieval performance on the whole corpus into account largely reflects the performance to retrieve functions from stack traces of crashes found by address sanitizer. This potentially conceals good performance measures with regard to crashes of one of the less frequent sanitizers.
%
\cstart
The individual performances of the target selection methods broken down by sanitizers are shown in Appendix~\ref{sec:appx-break-down-sanitizers}. They further support the dominance of Leopard's methods across most numbers of retrieved functions. However, especially for the crashes discovered by the memory sanitizer, the sanitizer-based target selection method works almost as well.
\cend

To further inspect the variance in retrieval performance for an individual selection method, we exemplary show 
the retrieval quality of Leopard-V for the three sanitizers in Figure \ref{fig:mean-gains-leopard}a), indicating
that the impact of which sanitizer was used to find a crash on the retrieval performance is limited.  

\begin{figure}
	\begin{center}
		\input{tikz/mean_gains_leopard}
	\end{center}
	\caption{\textbf{Retrieval performance of Leopard-V across different sanitizer and crash types.} \normalfont Each solid line represents a crash type or sanitizer, respectively. The color strength reflects the prevalence the respective sanitizer and crash type in the dataset. The performance across the whole dataset is added in dashed, for reference.}
	\label{fig:mean-gains-leopard}
\end{figure}

%%%
%%% Performance by crash type
%%%
\boldpar{Impact of crash types}
Just like with the sanitizers, we can conduct a finer grained assessment of the selection methods performance by breaking them down with respect to the crash types. \change{The resulting retrieval performances are depicted in Appendix \ref{sec:appx-break-down-crashtypes}. We observe that the Leopard methods, again, perform best across all examined crash types with the gap to the next best method, the sanitizer-based method, being most pronounced for heap buffer overflows and less pronounced for segmentation faults.}

To inspect how strong the retrieval performance depends on the crash type, we take a look at the quality of the method performing strong across all crash types (i.e., Leopard-V) in greater detail. We depict the retrieval performance for every crash type of which we have at least 50 samples in the dataset in Figure \ref{fig:mean-gains-leopard}b). The color strength indicates how prevalent the respective crash type is in our corpus. It shows that even the best performing selection method's retrieval quality varies greatly across the various crash types. \change{Hence, unlike with the sanitizers used to discover the crashes which only have limited impact on the retrieval performance, the quality greatly depends on the crash type. Some crash types, such as heap buffer overflows can be detected very well using the Leopard-V method while others, such as stack buffer overflows, can hardly be detected.}

\cstart
\begin{tcolorbox}[boxrule=1pt, arc=1mm, colback=white, colframe=coldefault]
	Software metric-based target selection methods perform significantly better than any other method across most types of crashes and sanitizers. The only other method close to their performance in some cases is the sanitizer-based one.
\end{tcolorbox}
\cend