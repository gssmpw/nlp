The ultimate goal of fuzzing is to uncover bugs. When testing unknown software, the underlying defect distribution, i.e., the number and location of defects, is naturally unknown. In this case, code coverage has proven an excellent proxy metric: after all, we cannot find flaws in code that has not been executed by the fuzzer. In fact, recent research has found a strong correlation between code coverage and software defects~\cite{BöhSzeMet22}. However, the fuzzer performing best in terms of coverage may not necessarily be the one finding the most bugs. Still, using code coverage as \emph{feedback} for guidance has become the de-facto standard of modern fuzzing, offering excellent exploration capabilities in absence of knowledge of the underlying defect distribution.

In certain scenarios, however, we may know, or at least can assume, the (potential) location of defects. For example, new and untested code is more likely to contain bugs than well-tested software~\cite{ZhuBöh21}. Similarly, a static analysis tool can pinpoint specific code locations as potentially buggy. In such cases, \emph{guiding} the fuzzer towards these locations may help to uncover defects faster than when indifferently exploring all parts of the program. This insight lead to the introduction of \emph{directed fuzzers}: Traditionally, these fuzzers accept one or more code locations, for example, addresses in the binary or source code lines, that a fuzzer should target during execution. While some fuzzers directly turn their focus towards these targets, others initially explore the program similarly to regular fuzzers before switching to an exploitation phase where they focus on the desired target locations~\cite{BöhPhaNguRoy+17}. 

In this work, we consider a wider definition of directed fuzzing: We not only include traditional approaches that accept predefined target locations but also consider any approach that uses some metric (beyond coverage feedback) to guide the fuzzer to specific locations. A prime example of such an approach is \emph{regression greybox fuzzing}~\cite{ZhuBöh21}, where testing is directed towards recently changed code rather than user-specified targets. While different to traditional directed fuzzing that uses a fixed set of targets, this technique still \emph{directs} the fuzzer to specific locations. The difference merely lies in the \emph{target selection} and the type of \emph{scoring} used to measure the ``relevance'' of a given code location. In other words, the score of code locations tells the fuzzer whether to prioritize them during fuzzing. 
Figure~\ref{fig:target-selection} outlines the overall process enabling the guidance of the fuzzer. \change{Before discussing the individual steps and focusing on the \emph{target selection}, we first motivate its relevance towards improving the probability of finding a crash. Following this, we examine the two different types of scoring used in directed fuzzing in more detail.}

\cstart
\boldpar{Relevance of target selection}
When examining prior works that present directed fuzzers, we can make two observations. First, these studies typically demonstrate---as part of their experiments---that their directed fuzzing approach outperforms an undirected coverage-guided fuzzer that serves as a baseline~\cite{BoePhaNguRoy17, CheXueLiChe+18, WüsChr20, NguBarBonGro+20, AscSchAbbHol+20, ZhaLiuYaoJin+23}. Second, they show their fuzzers are in fact directed; that is, they gradually steer the fuzzing process towards the provided targets~\cite{BoePhaNguRoy17, CheXueLiChe+18, WüsChr20}. 
%
If we assume all targets are unrelated to crash sites, a directed fuzzer will spend resources to focus on code that contains no bugs, meaning the directed fuzzer is unlikely to outperform a coverage-guided one in this case.
%
Thus, we conclude that selecting relevant targets is indeed a crucial step in directed fuzzing to increase the probability of finding crashes.
%
In other words, not only implementation details such as fuzzing throughput determine the fuzzer's success but also the target selection. Despite its relevance to the success of directed fuzzing, we find this aspect is often overlooked and not considered individually.
%
With this in mind, we now take a closer look at the two types of scoring mechanisms that enable a selection of targets.

\cend

\boldpar{Discrete scoring}
The first type of scoring assigns a \emph{discrete} value, usually either 0 or 1, to each location. Hence, it differentiates between relevant locations and irrelevant ones. This scoring type closely resembles most tools traditionally referred to as directed fuzzers: They accept a set of target locations, marking them as relevant, whereas every other code location is considered irrelevant. The target selection decides upon the metric according to which target locations are chosen. During execution, the \emph{distance} to relevant code locations is then used as a proxy score that allows to indirectly rank and compare locations.

\boldpar{Continuous scoring}
The second type of target selection assigns a \emph{continuous} value to all code locations: This score can, for example, be based on the last time this location has been modified~\citep{ZhuBöh21}, the number of sanitizer primitives it contains~\citep{OstRazBosGiu+20}, or a software metric describing its code complexity~\citep{DuCheLiGuo+19}. As each code location is assigned its own score, these approaches usually do not need a proxy metric, such as the distance to a target location, and the fuzzer can iteratively optimize over the scores of the visited code regions during its operation.



\boldpar{Comparison of scoring functions}
The two types of scoring functions lead to a different effectiveness of target selection. To examine this difference, let us consider a program that contains a single defect. With continuous scoring, the probability that this bug is triggered increases steadily with the quality of the scores, as a high quality scoring means that the defective locations are assigned significantly higher scores than non-defective ones.

In contrast, when we consider a discrete scoring, we usually focus on a small number of targets. In this case, a low-quality scoring may not flag the error location as relevant, and hence it may become unlikely that the fuzzer reaches the bug at all, as it is effectively steered towards non-defective locations. However, once the location is marked, our chances increase noticeably.
%
When evaluating target selection methods in Section~\ref{sec:results}, we take this difference into account.

In comparison, discrete scoring provides more versatility: Regardless of how the target set was derived, guiding a fuzzer using the distance to its locations will work, providing greater versatility compared to scoring each code locating using a specific metric. Changing the metric according to which targets are selected is supported by design: Often, these tools allow the user to specify arbitrary code locations. 
%
On the other side, all selected locations are treated equally by discrete scoring functions. If two of these targets are not equally relevant, we lose information, as individual locations cannot be ranked differently.
% 
In contrast, continuous scores do not suffer from this problem and can differ between targets on a more fine-granular basis.

\medskip
In short, we can distinguish target selection approaches based on the score they assign to their targets, with each type having advantages and disadvantages.

\boldpar{Overview}
With the two types of scoring in mind, we can focus on their place in the overall fuzzing process. Figure~\ref{fig:target-selection} provides an overview of the general flow: Based on external information known a priori or information extracted from the System Under Test (SUT), we know \emph{what} code to target. Our target selection receives this metric and the SUT (Step \stepone). Based on the available information, the \emph{target selection} (Step \steptwo) can be performed by assigning a \emph{discrete} or \emph{continuous} score to each code location. Finally, the fuzzer tests the SUT (Step \stepthree) while focusing on the targets.