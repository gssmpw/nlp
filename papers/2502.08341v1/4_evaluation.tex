Although numerous target selection mechanisms have been proposed, they rarely receive special attention during the evaluation of the directed fuzzers for which they were designed. Instead, fuzzer evaluations commonly compare different tools against each other so that observed differences cannot confidently be attributed to individual components such as the target selection. 
%
Furthermore, many evaluations (re-)use the same set of targets (e.g., the dataset AFLGo~\cite{BÃ¶hPhaNguRoy+17} presented), thereby measuring performance differences between the newly proposed technique and existing tools, without scrutinizing the strategy for selecting these targets in the first place nor reflecting the various bug classes or target SUTs a selection should perform well on.

\boldpar{Requirements} Therefore, we identify two requirements for conducting a thorough and systematic comparison of target selection mechanisms.
%
\begin{itemize}
    \setlength{\itemsep}{4pt}
    \item[\textbf{R1.}] We need a suitable method by which we can compare different target selection techniques in isolation, independent of a specific fuzzer ($\rightarrow$~Section~\ref{sec:methodology}).
    \item[\textbf{R2.}] For comparative purposes, a comprehensive ground truth is necessary, ideally representing many different bug classes across a large variety of real-world code
    ($\rightarrow$~Section~\ref{sec:dataset}).
\end{itemize}

\subsection{Target Selection as Information Retrieval}
\label{sec:methodology}
\input{4a_methodology.tex}

\subsection{Crash Dataset}
\label{sec:dataset}
\input{4b_dataset.tex}
