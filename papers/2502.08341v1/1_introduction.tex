\begin{figure*}[t!]
	\centering
	\includegraphics[width=0.8\linewidth]{target-selection}
	\vspace*{1em}
	\caption{\textbf{Target selection.} \normalfont The initial step in target selection is the extraction of code locations from the SUT (Step~\stepone). The granularity of this extraction depends on the specifics of the selection method and could, for example, be on a function-level or basic-block level. After extraction, the locations and (optionally) the SUT or external information (e.g., code change timestamps) are forwarded to the target selection method. This method then assigns a score to each location (Step~\steptwo), which is either (a) continuous or (b) discrete. Finally, the fuzzer utilizes the annotated code locations for guidance (Step~\stepthree).}
	\label{fig:target-selection}
\end{figure*}

Fuzzing has been a thriving area of security research in recent years. Numerous techniques have been proposed to improve the efficacy of fuzzers, ranging from concepts to increase efficiency~\citep{CheChe18, AscSchBlaGaw+19, ShePeiEpsYan+19, SeiMaiMue23} and mitigate roadblocks~\citep{BlaAscSchAbb+19, BarSchSchSch+23} to application-specific testing strategies~\citep{CheDiaZhaZuo+18, SchChlSchBar+23, RawJaiKumCoj+17}. This research encompasses a large variety of technical contributions inspired from software engineering, compiler design, and software security, yet one general paradigm stands out as a key concept in many of the approaches to improving fuzzing performance: \emph{target selection}.

Rather than treating all code locations equally, many approaches guide the fuzzer towards areas more likely to contain defects, such as those involving memory accesses, insecure API calls, or recent patches~\citep[]{OstRazBosGiu+20,HalSloNeuBos+13, MarCad+13}. Similarly, several fuzzers direct the testing explicitly to a chosen set of locations to improve efficiency in different stages of software development~\cite[][]{BÃ¶hPhaNguRoy+17, CheXueLiChe+18, HuaGuoShiYao+22}. Although these approaches fundamentally differ in \emph{how} they reach these regions and guide the fuzzer, they all share the concept of focusing on selected target locations rather than uniformly covering the program as traditional fuzzers~\citep{SutGreAmi07}.

Surprisingly, the analysis of suitable targets for a fuzzer, that is, the \emph{where} of the paradigm, has received little attention so far. While some fuzzers incorporate a fixed strategy for locating targets, such as focusing on sanitizers~\citep{OstRazBosGiu+20}, code changes~\citep{MarCad+13}, or buffer operations~\citep{HalSloNeuBos+13}, several approaches for directed fuzzing leave the target selection to the practitioner. As a result, it is currently unclear which selection method performs best in practice, and the simple question---\emph{where to fuzz?}---is still unexplored.

In this paper, we aim to bridge this gap and present the first comprehensive analysis of \emph{target selection methods} for directed fuzzing. To this end, we conduct a literature review of \numanalyzedpapers papers concerned with directed fuzzing published at top-tier security and software engineering conferences over the past five years. We identify two dimensions that characterize the selection of targets: (1)~the information source and (2)~the selection mechanism. For example, some selection methods are based on analyzing binary code, while others require the source code or further external information. Similarly, we can categorize the selection mechanisms into metric-based and pattern-based heuristics, identifying interesting code locations for the fuzzer to inspect.

Based on this systematization, we proceed to compare different methods for target selection. To avoid evaluating the efficacy of fuzzers rather than their targets, we conduct this comparison in isolation. That is, we model target selection methods as abstract \emph{scoring functions} that retrieve a set of code units and assign scores to them, indicating their ``interestingness'' for the particular fuzzing setup. This approach is applicable to any selection method and differs only in (1) which code units are provided and (2)~how they are ranked according to the underlying scoring functions. As a result, we are able to formulate our evaluation as an information retrieval task and assess the methods ``in~vitro'' using corresponding performance measures.

For our evaluation, we assemble a dataset of more than 1,600 crashes from 97 software projects of \ossfuzz{}~\cite{Ser+23}, which to the best of our knowledge represents the largest corpus of reproducible crashes to date. These crashes serve as ground truth and allow for quantitatively measuring how well a target selection method and the underlying scoring function can retrieve code that actually triggers crashes during a fuzzing campaign. By measuring this retrieval performance in different settings, we can draw general conclusions about target locations and derive recommendations for improving directed fuzzing, independent of specific testing strategies.

Our evaluation uncovers different insights for target selection: First, we observe that simple software metrics, such as vulnerability scores of the Leopard framework~\cite{DuCheLiGuo+19}, significantly outperform all other approaches. This holds true across all types of crashes and indicates that advanced methods for target selection are not yet able to surpass simple numerical metrics. Second, we identify large language models for code, such as CodeT5+, as promising alternatives that almost reach the efficacy of software metrics.

In summary, our work opens a new perspective on directed fuzzing by decoupling the fuzzing mechanism (how) from the selection of target locations (where). We are the first to observe that selection methods differ significantly when studied in isolation, and that some strategies, such as software metrics, consistently outperform other methods. Given the simple nature of these metrics, we argue that developing better selection methods is a promising path for future research and that machine learning models can potentially serve as a means to success in this area.

\boldpar{Contributions} In summary, we make the following
major contributions in this work:

\begin{enumerate}
\setlength{\itemsep}{4pt}

\item \emph{Systematization of target selection.} We conduct the first systematic analysis of target selection methods for directed fuzzing. For this, we analyze a total of \numanalyzedpapers papers related to target selection published between 2018--2023 at top security and software engineering conferences.

\item \emph{Large-scale evaluation.} We model the task of target selection as an information retrieval problem and assemble a dataset of more than 1,600 reproducible crashes as ground truth. This is the largest available dataset of this kind, and we provide it publicly as part of this paper's artifacts.

\item \emph{Insights for target selection in fuzzing.}  Based on the results of our analysis, we provide insights and recommendations for target selection in different fuzzing contexts.
\end{enumerate}
