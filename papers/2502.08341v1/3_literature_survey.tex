Selecting good targets is crucial to the success of directed fuzzing. Yet, we find that this aspect has surprisingly received little attention in fuzzing research so far. Consequently, before turning towards a comparative analysis of different strategies for locating interesting code, we first conduct a comprehensive literature review on target selection methods used currently for directed fuzzing.

\italicpar{Literature} For our review, 
we investigate papers published at top security and software engineering venues between 2018--2023. In particular, we collect all papers from the following A$^*$ conferences~\cite{coreranking}: ASE, FSE, ICSE, CCS, NDSS, USENIX Security, and S\&P. After filtering out papers not related to fuzzing, this leaves us with 289 fuzzing papers. We then automatically identify papers focusing on \emph{directed fuzzing} by filtering out any paper that does not contain the word ``directed'' at least three times. This heuristic is grounded in the assumption that publications in a field are likely to either name the field multiple times or at least mention it when comparing against prior works. Using this process, we end up with 31 publications. Manually analyzing all of them yields \numanalyzedpapers papers that deal with directed fuzzing under our definition. 

\italicpar{Review method}
\change{To distinguish between different target selection techniques, we review all papers in-depth and distill the working principle for locating targets. As result of this process, we arrive at four characteristics to categorize selection methods. First, we consider the source from which the information originates that is used to direct the fuzzer. We refer to this as the \emph{information source}. Second, we distinguish whether the underlying scoring function is \emph{discrete} or \emph{continuous} as discussed previously.
%
We label this as the \emph{scoring type} of the selection method. Third, we distinguish between different levels of granularity of the target selection method. We denote this as the \emph{granularity}. Lastly, we differentiate target selection techniques with respect to what their scoring mechanism is based on. We refer to this property as the target selection's \emph{scoring mechanism}. 
}

\italicpar{Analysis results}
The results of our literature review are shown in Table~\ref{tab:survey}. For each publication, we indicate whether a paper put a focus on the target selection (denoted as \cmark{}) or if it exclusively used targets obtained from third parties, such as previous literature or stack traces of crashes from a bug tracker (denoted as \xmark{}). We further record whether the employed target selection was continuous (C) or discrete (D). 
In the following, we shift our focus to the information sources, scoring mechanism and granularity used within the examined publications.

\boldpar{The scoring mechanism landscape} 
From our literature review, we observe two broad categories of scoring mechanisms: one based on metrics and one on patterns. The metrics-based category includes methods that utilize code metrics to score individual code locations. In contrast, the pattern-based target selection category utilizes heuristics for this purpose.

\begin{table}[t]
    \centering
    \footnotesize
    \caption{\change{\textbf{Overview of target selection methods.} \normalfont 
    The \emph{Where?} column denotes if a publication puts a focus on the target selection method (\cmark) or if it exclusively relied on a target selection from previous work or reproduction tasks (\xmark). Column \textit{Scoring type} records if a discrete target selection methods (D) or a continuous one (C) is used.
    The granularity of the target selection method is categorized in \textit{Granularity}: instructions (I), basic blocks (B), statements (S), lines (L), and functions (F). The \textit{Source} and the \textit{Scoring} column provide further details about the origin of the additional information and utilized core method of the target selection, respectively.}}
    % \vspace{-0.1cm}
		\input{tables/localization.tex}
    \label{tab:survey}
\end{table}

Out of the 25 directed fuzzing approaches, 14 put a focus on the target selection method (\cmark in the \emph{Where?} column of Table~\ref{tab:survey}). From these 14, ten are pattern-based and use heuristics to select interesting target locations. For example, ODDFuzz~\cite{CaoHeSunOuy+23} selects deserialization methods in Java as targets for a directed fuzzing approach, StrawFuzzer~\cite{ZhaLiaXiaZha+22} uses data storing instructions as targets to increase the memory footprint of Android services and cause the system to crash, and AmpFuzz~\cite{KruGriRos22} targets network-related functions as part of their approach to find amplification vectors for DDoS attacks. All of these heuristics have in common that they focus on finding a specific type of erroneous behavior. Another heuristic employed by three of the approaches, ParmeSan~\cite{OstRazBosGiu+20}, SAVIOR~\cite{CheLiXuGuo+20}, and FishFuzz~\cite{ZheZhaHuaRen+23}, covers a broader range of potential bugs: Their approach to the target selection is based on the idea that sanitizer instrumentation can function as an indicator for the relevance of a code location to a directed fuzzer. After all, sanitizer instrumentation is added at locations where a bug might occur.

% Code Metrics
Instead of heuristics, five approaches use a metrics-based selection method. In particular, TortoiseFuzz~\cite{WanJiaLiuZen+20} and CollAFL~\cite{GanZhaQinTu+18} focus on the number of memory accesses in their target selection method. In addition, TortoiseFuzz augments this information with the number of security related functions, which they identified by crawling the pages referenced from CVE descriptions. Both the target selection used by She~et~al.~\cite{SheShaJan22} and the one presented by Leopard~\cite{WüsChr20} are based on code metrics. The former uses a graph-based metric that assigns scores to code locations based on how many other uncovered code regions could potentially be reached from it. The latter uses two code metrics, a structural complexity metric and what the authors refer to as ``vulnerability metrics'', which revolve around properties of a code region, such as the number of pointer arithmetic operations or the number of nested control structures.

\boldpar{The information source landscape}
The sources providing input to the target selection methods can be broken down into three origins: source code, binary code, and external information. 

The results of our literature survey show that each information source is used by metrics-based as well as pattern-based methods. However, the specific information source is, unsurprisingly, determined by the individual method. Certain information is only available at the source code level, while others are accessible only on the binary code level. For instance, \emph{sanitizer instrumentation} is not present at the source code level, such that the approaches we examined either resorted to the binary code level or LLVM IR for this purpose. We denote the latter with half-filled circles (\LEFTcircle) in Table~\ref{tab:survey}. The same holds true for \emph{memory access instructions}~\cite{GanZhaQinTu+18, YuaZhaLiLia+23}. On the other hand, the code metrics used by Leopard~\cite{DuCheLiGuo+19} require source code as input. In contrast, AFLChurn~\cite{ZhuBöh21} makes use of the information when a code location was last changed, and TortoiseFuzz~\cite{WanJiaLiuZen+20} uses information crawled from pages referenced by the CVE database. In both cases, the information is neither present in the source code nor in the compiled binary but acquired from external sources.

\cstart
\boldpar{The granularity}
Unlike other systematic literature reviews which focus on the fuzzing approaches themselves~\cite{WanZhoYueLin+24, LiaPeiJiaShe+18}, we characterize the methods used for selecting targets. 
%
Consequently, in our setting, the granularity refers to the target selection method rather than the characteristics of the proposed directed fuzzer. In other words, while certain fuzzers, such as StrawFuzzer~\cite{ZhaLiaXiaZha+22} or Hawkeye~\cite{CheXueLiChe+18} operate on a basic block granularity, the employed target selection methods provide targets with a function respectively source code line granularity.

In our analysis, we categorize all target selection methods into five distinct levels of granularity: instructions (I), basic blocks (B), statements (S), lines (L), and functions (F). While instruction and basic block granularity only concern methods using the binary code as information source, function granularity can apply to both binary and source code. Line and statement granularity refer to source code lines and individual statements within a line, respectively.
%
We find that most papers that introduce a new target selection method use a function level granularity. On the other hand, papers that do not introduce a new target selection method are dominated by line level selection methods. This mostly stems from the fact that these papers resort to the source file and line number obtained from tracebacks of crashes as the targets to evaluate their fuzzer.    
\cend