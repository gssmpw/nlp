Before a fuzzer can be directed towards a specific location, we must make a suitable choice in terms of target selection. With Table~\ref{tab:survey} outlining the common techniques used for fuzzing, we have ample choices to select from. In the following, we desire to evaluate the quality of target selection methods. For an informed evaluation, we select three target selection approaches covering both core methods, pattern-based and metrics-based methods: For the former, we pick up the idea of using sanitizer instrumentation \cite{OstRazBosGiu+20, CheLiXuGuo+20, ZheZhaHuaRen+23} and the idea of using recently modified code changes \cite{ZhuBöh21} for target selection. For the latter, we resort to the code metrics presented by Leopard~\cite{DuCheLiGuo+19}.

To better put these methods used by fuzzing into context, we also include approaches no fuzzing paper has used yet.
First, we turn to \emph{static analysis}, which has found widespread adoption in industry~\cite{FacebookInfer, GoogleSAST, GithubSAST}.
% ~\todo{cite Facebook's Infer and some other tools widely used. Lukas marked as DONE (?): cited tech reports of SAST usage in the industry (FB, Google, Github)}
Essentially, any static software application security testing (SAST) tool can be converted into a target selection method, with the potentially faulty lines as target locations. A fuzzer can then be used to create a proof of concept, which static analysis usually cannot do on its own. 
For our analysis, we choose two SAST tools, Rats~\cite{rats} and Cppcheck~\cite{cppcheck}. 
Initially developed by Secure Software Inc. in 2001, Rats is one of the earliest freely available SAST tools and although its development was discontinued in 2013, it stayed relevant as a  common academic baseline~\cite{SASTeval2018, KAUR20202023, 10.1145/3475716.3475781}.
Cppcheck is a mature open-source SAST tool with a very active community continuously developing and extending the software. 
Its availability in package repositories of many major Linux distributions, precise error messages, integration with many popular IDEs and its ease of use makes it a popular choice for initial security assessments of C/C++ code~\cite{cppcheck, 8531281}.
% \todo{there must be some explanation why these two in particular (something like: poplular / widely used)}
Second, we want to capture the broader trends in other areas of code assessment, which rapidly turn to learning-based approaches. In particular, we select three source code based vulnerability prediction models for this purpose: ReVeal~\cite{ChaKriDinRay+21}, which is based on graph neural networks, Linevul~\cite{FuTan+22}, resorting to a encoder-only transformer based architecture, and a fine-tuned version of the transformer based language model CodeT5+~\cite{wang2023codet5plus}.

Finally, we include one target selection that uses a random scoring of code locations as a baseline for the other models.
In the following, we provide a brief overview of our selected methods.

\boldpar{CodeT5+}
Based on the transformer architecture, CodeT5+~\cite{wang2023codet5plus} is a large language model for programming tasks. It is pre-trained on a subset of the CodeSearchNet~\cite{codesearchnet} and GitHub Code datasets\footnote{https://huggingface.co/datasets/codeparrot/github-code}.
%
Pre-training consists of a first stage with unimodal tasks such as denoising, followed by a second stage using text-code bimodal data,  aiming to improve code generation and understanding tasks. For our experiments, we use the 220 million parameter version and extend it to perform  a vulnerability prediction task similarly to Chen et al.~\cite{CheDinAloChe+23}. To this end, we complement the model with a prediction head for binary classification as described in~\cite{brokenPromises} and fine-tune it on the DiverseVul dataset~\cite{CheDinAloChe+23}, consisting of ca. 350k (19k vulnerable) annotated methods representing 150 CWEs.

% SAST
\boldpar{Cppcheck}
The Cppcheck SAST tool is able to perform a variety of checks on C and C++ source code, scanning for undefined behavior and dangerous code patterns that might indicate vulnerabilities~\cite{cppcheck}. 
Checks include analysis passes for automatic variable checking, array bounds checking or dead code elimination.  
A recent study evaluating the tool on Mozilla Firefox found that Cppcheck was able to find 83.5\%  of the vulnerabilities with only 7.2\% false positives~\cite{cppcheckStudy}.
\change{Cppcheck can be used with different pre-built configurations. We use the default setup which includes all of them.}

\boldpar{Rats}
The \textit{Rough Auditing Tool for Security} (Rats)~\cite{rats} is one of the earlier rule-based SAST solutions. The tool offers language support for C/C++, Perl, PHP, Python and Ruby with varying degrees of maturity; while it is able to scan C code for more complex bug classes such as Time-of-Check-Time-of-Use,  analysis capabilities in Python are constrained to checking for potentially dangerous built-in or library function calls. 
\change{The results of Rats can be filtered by three types of severity, which we include all. Besides that, we use the default configuration.}

\boldpar{Leopard-C}
Du et al. propose Leopard as a framework for identifying potentially vulnerable code sites in C/C++ programs~\cite{DuCheLiGuo+19}.
The framework primarily works in two stages, the first one employing code complexity metrics, hereinafter referred to as \textit{Leopard-C}, to sort functions into different bins, and the second one using vulnerability metrics (\textit{Leopard-V}) to rank functions inside those bins. 
Leopard-C takes into account the cyclomatic complexity of a function, i.e., the number of linearly independent Control Flow paths, and a set of loop metrics: the number of loops and nested loops as well as the maximum nesting level.
The intuition is that more complex functions are harder to analyze and reason about.

\boldpar{Leopard-V}
With Leopard-V, the authors derive a new set of metrics from common vulnerability causes~\cite{DuCheLiGuo+19}. 
\change{More precisely, they select eleven code metrics that aim to capture the following characteristics: the dependency to other functions, the use of pointers, and features of employed control structures. For the first characteristic, they count the number of parameters of the function and the number of parameters to its callees. To measure the use of pointers, they, for example, count the number of variables involved in pointer arithmetic. Lastly, they measure the features of the control structures such as the number of nesting levels or the number of conditional statements without an alternative. Finally, Du et al. sum up all individual metrics into a single vulnerability metric.}

% Heuristics
\boldpar{Sanitizer instrumentation}
Our sanitizer based selection methods builds on the idea of ParmeSan~\cite{OstRazBosGiu+20}, SAVIOR~\cite{CheLiXuGuo+20}, and FishFuzz~\cite{ZheZhaHuaRen+23}, which utilize information about the code regions augmented with instrumentation by the employed sanitizers. This is based on the rationale that sanitizer instrumentation is added at code locations where defective behavior might occur (e.g., array accesses).
\change{To implement this, we count the number of sanitizer callbacks added to each function. These callbacks are added during compilation, for example, when memory is allocated or freed, or other memory access that may potentially allow for errors are detected. Consequently, a function with many sanitizer callbacks indicates a higher likelihood for a sanitizer to detect an error in this function than in functions with very few or none added callbacks.}

\boldpar{ReVeal}
ReVeal~\cite{ChaKriDinRay+21} is a Graph Neural Network (GNN) for vulnerability detection based on Zhou et al.'s approach of learning program semantics~\cite{ZhoLiuSioDu+19}.
It uses Code Property Graphs, a holistic code representation combining Abstract Syntax Trees with Dataflow- and Control Flow Graphs~\cite{YamGolArpRie+14}, as an input to the eight-step GNN and augment it with a classification layer to separate the learning of code representations from the learning of vulnerability indicators.
The model is trained on a large real-world dataset of vulnerabilities in the Chromium and Debian Kernel sources. 
We use the implementation from~\cite{revealGH}.

\boldpar{Linevul}
Using Microsoft's CodeBERT~\cite{feng-etal-2020-codebert}, a 125M parameter encoder-only transformer for programming tasks, as a foundation model, Linevul~\cite{FuTan+22} provides line-level vulnerability prediction for C/C++ code. 
Following results from~\cite{brokenPromises}, we use the original model trained on the BigVul dataset~\cite{bigvul} but apply normalization to any code input before inference in order to reduce confounding introduced by code style variations. 

\cstart
\boldpar{Recently modified code (``Recent'')}
Several directed fuzzing approaches resort to recently modified code locations as their targets~\cite{BoePhaNguRoy17, ZhuBöh21, CanMatGraKal+22, PenLiLiuXu+19}. \citet{ZhuBöh21} even find that most (four out of five) bugs found by \ossfuzz{} are introduced by recent code changes. Therefore, we also include such a target selection in our experiments. To this end, we first assign each function the time it was last modified. We identify the changes based on information from the version tracking system employed by the individual projects. When every function has been assigned a timestamp, we rank them with the most recently changed functions ranking highest.
\cend

\boldpar{Random}
To provide a baseline for the previous methods, we also include a target selection which outputs a random ranking over all code locations.