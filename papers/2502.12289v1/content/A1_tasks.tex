\section{Tasks}
\label{sec:appendix-task}

This section aims to describe different reasoning tasks and datasets in more detail.

\subsection{Multi-hop Question Answering}
\label{sec:appendix-task-mhqa}

This section focuses on the metrics proposed for evaluating the reasoning traces for multi-hop question answering (MHQA) tasks. MHQA is often divided into two subcategories, \textbf{factual reasoning} and \textbf{commonsense reasoning}.

Inference in factual MHQAs is finding the sequence of \textit{bridging entities} that leads to the final answer \citep{yang-etal-2018-hotpotqa, talmor-berant-2018-web, kwiatkowski-etal-2019-natural}. For example, to solve a factual MHQA question \textit{"The Argentine PGA Championship record holder has won how many tournaments worldwide?"}, one must first find who (\textit{bridging entity}) is the Argentine PGA championship record holder and determine how many tournaments he has won worldwide.

In contrast, an inference step in commonsense MHQAs \citep{clark2018thinksolvedquestionanswering, mihaylov2018suitarmorconductelectricity, talmor-etal-2019-commonsenseqa, bisk2019piqareasoningphysicalcommonsense, geva-etal-2021-aristotle, trivedi-etal-2022-musique} can require information that is not present in the provided facts. The form of such commonsense knowledge can be diverse, ranging from well-known facts (\textit{Paris is in France.}) to logical rules (\textit{If A was born after B was dead, they have never met each other}).

LLMs are known to achieve strong performance in challenging datasets such as ARC-Challenge and PIQA \citep{openai2024gpt4technicalreport, anil2023palm2technicalreport}, sometimes exceeding human performance. However, multiple studies report that even modern LLMs like GPT-4 \citep{openai2024gpt4technicalreport} are vulnerable to errors, such as failing to correctly adhere to long evidence \citep{zhu-etal-2024-fanoutqa}, leveraging shortcuts \citep{schnitzler2024morehopqa}, or ignoring temporal relation between events \citep{NEURIPS2024_e560a0b2}. Therefore, identifying and categorizing mistakes made by LLMs in these tasks are still relevant until the time of writing.

\subsection{Symbolic Reasoning}
\label{sec:appendix-task-symbol}

Since the discovery of Chain-of-thought prompting \citep{NEURIPS2022_9d560961, NEURIPS2022_8bb0d291}, step-by-step reasoning has proven effective in symbolic reasoning tasks\footnote{While symbolic reasoning may strictly refer to \textit{algorithmic reasoning} \citep{NEURIPS2022_9d560961}, we adopt the broader sense including math and logical reasoning that can be readily expressed in symbols (equation, logic) \citep{sprague2024cotcotchainofthoughthelps}.} such as \textbf{mathematical reasoning}, \textbf{logical reasoning}, and \textbf{algorithmic reasoning}.

\textbf{Arithmetic reasoning}, where the model has to predict the correct answer from arithmetic word problems, is the most renowned variant of math reasoning. Popular benchmarks include MathQA \citep{amini-etal-2019-mathqa} and GSM8k \citep{cobbe2021trainingverifierssolvemath}, which provide long, diverse natural language queries. Game of 24 \citep{NEURIPS2023_271db992} and Mathador \citep{kurtic-etal-2024-mathador} ask to combine given numbers and arithmetic operations to generate the target number, requiring exploration and backtracking in the exponential solution space.

The rapid saturation of LLMs in arithmetic word problems facilitated more challenging \textbf{mathematical reasoning} benchmarks from the olympiad/graduate-level, covering fields like calculus, probability and statistics, geometry, number theory, and more \citep{he-etal-2024-olympiadbench, gao2024omnimathuniversalolympiadlevel, glazer2024frontiermathbenchmarkevaluatingadvanced, zhang-etal-2024-geoeval}. Recent reasoning-focused (\textit{a.k.a.} slow-thinking) LLMs \citep{openai2024openaio1card, qwenlmQwQReflect, deepseekai2025deepseekr1incentivizingreasoningcapability} achieve unprecedented performance in these benchmarks by generating long reasoning traces with self-verification and correction.

\textbf{Deductive logical reasoning} \citep{tafjord-etal-2021-proofwriter, tian-etal-2021-diagnosing, PrOntoQA, han-etal-2024-folio} mainly focuses on logical deduction, where repeatedly applying the provided rules to facts will reach the correct answer. \textbf{Constraint-based reasoning} \citep{zhong2021arlsatinvestigatinganalyticalreasoning, tyagi-etal-2024-step} is a variant of deductive reasoning where one must find the solution that suffices the provided initial constraints (also referred to as \textit{grid puzzle}). These datasets have an exponentially sized solution space that significantly reduces the LLM's reasoning performance in plain Chain-of-thought setting \citep{kang2024empiricalcomplexityreasoningplanning}.

Finally, \textbf{algorithmic (symbolic) reasoning} tasks include manipulating strings and data structures, such as concatenating the last letters of the given words \citep{NEURIPS2022_9d560961} or completing the incomplete Dyck language. BIG-Bench-Hard (BBH; \citet{suzgun2022challengingbigbenchtaskschainofthought}) and NPHardEval \citep{fan2024nphardevaldynamicbenchmarkreasoning} includes 11 and 9 algorithmic reasoning tasks, respectively, which is challenging for modern LLMs like GPT-4 and PaLM-540B.

\subsection{Uncovered tasks}

\textbf{Science reasoning} tasks lie between factual/commonsense reasoning tasks and symbolic reasoning tasks, as they often require addressing very complicated facts and performing precise math/logical reasoning \citep{rein2024gpqa, he-etal-2024-olympiadbench}. The most popular benchmark in this field, GPQA-Diamond \citep{rein2024gpqa}, contains 546 questions from physics, chemistry, and biology where human experts only get ~65\% of the problem correct.

\textbf{Programming/coding} is closely related to algorithmic reasoning tasks. Popular benchmarks regarding programming include \textit{competitive coding} where one has to solve an algorithm problem given in natural language and test codes \citep{chen2021evaluatinglargelanguagemodels, li2022competition}, and \textit{practical coding} that covers tasks of software engineers and developers \citep{zhang-etal-2023-repocoder, jimenez2024swebenchlanguagemodelsresolve, DBLP:journals/corr/abs-2410-07095}. While writing a correct program requires reasoning ability, coding differs from other reasoning tasks in various aspects including: (1) there is a strict syntax requirement for code, and (2) the result is evaluated by the execution result, not the final answer. These constraints lead to several issues when (1) segmenting the trace (code) into steps, or (2) applying metrics that require explicitly stated answers, \textit{i.e.} $\mathcal{V}$-information.