\section{Introduction}
\label{sec:introduction}
% \input{tables/tableofcontents}

Large language models (LLMs) have demonstrated remarkable capabilities in reasoning in complex problems, such as logic, math, and science. At the core of this versatility lies \textbf{step-by-step reasoning} \citep{NEURIPS2022_9d560961, NEURIPS2022_8bb0d291}, where the LLM generates an intermediate reasoning trace before presenting the final answer.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/intro.pdf}
    \caption{This survey aims to provide a comprehensive view of different terminologies on criteria and metrics designed for step-by-step reasoning evaluation.}
    \label{fig:intro}
\end{figure}

The step-by-step reasoning ability of LLMs is often measured in terms of \textit{answer accuracy}, \textit{i.e.} finding the correct answer in a problem that requires complex reasoning  \citep{openai2024gpt4technicalreport, groeneveld-etal-2024-olmo, deepseekai2025deepseekr1incentivizingreasoningcapability}. However, answer accuracy is generally insufficient for measuring LLMs' reasoning ability, as the correct answer does not imply the correctness of the preceding reasoning trace \citep{lanham2023measuringfaithfulnesschainofthoughtreasoning, mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical, paul-etal-2024-making}. Furthermore, the quality of the reasoning trace is crucial for improving the reasoning ability, in terms of reinforcement learning \citep{lu2024stepcontrolleddpoleveragingstepwise, qwenlmQwQReflect, deepseekai2025deepseekr1incentivizingreasoningcapability} and inference-time search \citep{DBLP:conf/iclr/0002WSLCNCZ23, NEURIPS2023_271db992}.

% The reasoning ability of LLMs is often represented as \textit{answer accuracy}, checking whether the LLM can provide a correct answer for a complex reasoning problem \citep{openai2024gpt4technicalreport, groeneveld-etal-2024-olmo}. However, evaluating only the answer and not the reasoning trace itself is generally insufficient for measuring LLMs' reasoning ability. Multiple studies have shown that the model's answer being correct does not necessarily imply that the reasoning trace is also correct \citep{lanham2023measuringfaithfulnesschainofthoughtreasoning, mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical, paul-etal-2024-making}, causing to overestimate the reasoning capability of LLMs. Furthermore, the quality of the reasoning trace provides a more comprehensive view of the LLM's reasoning ability due to its amount of information \citep{chen-etal-2024-measuring}. Therefore, evaluating LLM-generated reasoning traces is critical for assessing the reasoning ability of LLMs.

% On the other hand, as step-by-step reasoning became an integral part of LLM reasoning, different methods have been proposed to \textit{improve} LLMs' reasoning ability by leveraging the evaluation results. First, LLMs can be further trained to enhance step-by-step reasoning ability, especially with \textit{reinforcement learning} \citep{lu2024stepcontrolleddpoleveragingstepwise, qwenlmQwQReflect, deepseekai2025deepseekr1incentivizingreasoningcapability}. Furthermore, \textit{inference-time exploration} \citep{DBLP:conf/iclr/0002WSLCNCZ23, NEURIPS2023_271db992, wang-etal-2024-math, cui2024ultrafeedbackboostinglanguagemodels}, where a search algorithm (\textit{e.g.} tree search) is employed to choose the most promising trace among sampled candidates, has recently gained significant attention. These approaches commonly build over the automatic evaluation of LLM-generated reasoning traces, in terms of obtaining train data (reinforcement learning) and on-the-fly evaluation for the search (inference-time exploration).

Due to its importance, step-by-step reasoning evaluation is a rapidly evolving field with numerous new metrics and criteria actively proposed. Establishing the precise definition of the \textbf{criterion} (\textit{what to evaluate}) is crucial for correctly implementing the \textbf{metric} (\textit{how to evaluate}). However, the terminologies in the field are highly unstandardized, which has led to fragmented approaches in implementing metrics and meta-evaluation benchmarks. This current state motivates a systematic review, which will serve as a foundation for general criteria and metrics that can span diverse reasoning tasks.

In this survey, we reorganize existing step-by-step reasoning evaluation criteria defined within diverse metrics and meta-evaluation benchmarks into four distinct categories: factual \textbf{groundedness} in the given information, logical \textbf{validity} of steps, semantic \textbf{coherence}, and if the step contributes to the correct answer (\textbf{utility)}. Based on the proposed taxonomy, we review and compare widely used terms for criteria and metrics. Finally, we analyze the case of \textit{transferability}, whether a single evaluator trained/optimized for one criterion can evaluate another, based on reported scores on three recent meta-evaluation benchmarks \citep{jacovi-etal-2024-chain, song2025prmbenchfinegrainedchallengingbenchmark, zheng2024processbenchidentifyingprocesserrors}.
Finally, we conclude the survey with open questions in the field of evaluating step-by-step reasoning.

The key contributions of this survey are:
\begin{itemize}
    \item Defining the taxonomy of step-by-step evaluation \textbf{criteria}, and comparing it with existing terminologies (\S\ref{sec:taxonomy}-\S\ref{sec:comparison}).
    \item Surveying existing \textbf{metrics} for step-by-step reasoning evaluation based on their implementations, across diverse reasoning tasks and criteria (\S\ref{sec:metric-implementations}).
    \item Analyzing \textbf{transferability} between criteria based on reported empirical results (\S\ref{sec:transfer}).
\end{itemize}