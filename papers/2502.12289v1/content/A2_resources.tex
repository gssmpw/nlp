\section{Resources}
\label{sec:appendix-resources}

\input{tables/resources}

This section enumerates useful resources containing stepwise annotation. These datasets can be used to train an evaluator or perform meta-evaluation on different metrics.


\subsection{Factual/Commonsense reasoning}

% \textbf{Meta-evaluation.} \note{ROSCOE, REVEAL, Thoughtsource}

For meta-evaluating metrics in factual/commonsense reasoning, human annotations on LLM-generated outputs are provided by ROSCOE \citep{DBLP:conf/iclr/GolovnevaCPCZFC23}, REVEAL \citep{jacovi-etal-2024-chain}, and MR-Ben \citep{NEURIPS2024_d81cb1f4} (MMLU portion). % They differ in source benchmarks, evaluation criteria, and meta-evaluation method, which can be found in Table \ref{tab:appendix-resource-factual}.


\subsection{Symbolic Reasoning}

% \note{PRM800K (cot+human eval) / MStep-controlled DPO (LLM perturbed)}

\textbf{Training data for \textit{validity} evaluators.} 
The most popular validity dataset used for training PRMs is PRM800k \citep{DBLP:conf/iclr/LightmanKBEBLLS24}, which contains 800k human-anntoated stepwise labels (75k reasoning traces) in MATH \citep{NEURIPSDnB2021_be83ab3e} dataset. It classifies each step into three labels, \textit{positive, neutral, and negative}, where \textit{negative} denotes a clearly incorrect step and \textit{neutral} is used to defer the annotator's uncertainty in borderline cases. Other than PRM800k, MATH-Minos \citep{gao2024llmcriticshelpcatch} provides LLM-generated validity judgments for 440k reasoning traces.

\textbf{Meta-evaluating \textit{validity} evaluators.} There are multiple validity meta-evaluation benchmarks that incorporate human evaluation. PRM800K \citep{DBLP:conf/iclr/LightmanKBEBLLS24}, MR-GSM8k \citep{zeng2024mrgsm8kmetareasoningbenchmarklarge}, MR-Ben \citep{NEURIPS2024_d81cb1f4}, MR-MATH \citep{xia2025evaluatingmathematicalreasoningaccuracy}, BIG-Bench-Mistake \citep{tyen-etal-2024-llms}, ProcessBench \citep{zheng2024processbenchidentifyingprocesserrors}, and PRMBench \citep{song2025prmbenchfinegrainedchallengingbenchmark}. PRM800k, BIG-Bench-Mistake, and PRMBench formulate the task as stepwise classification, where one has to evaluate each step logically correct or not. In contrast, ProcessBench and MR-* series are set to identify the index of the first erroneous step in the reasoning trace.

\textbf{Training data for \textit{utility} evaluators.} \textbf{Training data for \textit{utility} evaluators.} The most popular option is Math-Shepherd \citep{wang-etal-2024-math}, which includes 445k reasoning traces with labels assigned by MCTS. A step's label is positive if any of the $N=8$ rollouts starting from the step leads to a correct answer, and negative otherwise. Also, Step-Controlled DPO \citep{lu2024stepcontrolleddpoleveragingstepwise} provides a large set of correct and incorrect reasoning traces, where incorrect ones are obtained by slowly increasing the LLM's temperature.

\textbf{Meta-evaluating \textit{utility} evaluators.} The standard approach for utility meta-evaluation in symbolic reasoning is applying \textbf{Best-of-N (BoN)} decoding on challenging math reasoning datasets \citep{wang-etal-2024-math, cui2024ultrafeedbackboostinglanguagemodels, zhang2025lessonsdevelopingprocessreward}. In this setting the evaluator should choose the best trace among $N$ sampled candidates, and the answer accuracy is determined from the selected one. A slight variant, \textbf{weighted voting} \citep{yuan2024freeprocessrewardsprocess}, decides the final answer based on the sum of evaluation scores instead of choosing the one with the highest score. In both settings, the upper bound of utility evaluators' performance is \texttt{pass@N} score, which counts when at least one from $N$ traces has a correct answer.