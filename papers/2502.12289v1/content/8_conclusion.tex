\section{Conclusion}
\label{sec:conclusion}

This survey aims to organize the scattered terminologies and methods for step-by-step reasoning evaluation, which is crucial for understanding and improving LLM's reasoning capabilities. This survey provides a unified taxonomy for evaluation criteria, a comprehensive review on existing metrics and their implementation, and tackle transferability between different metrics.

Still, there are diverse challenges left in the field of evaluating step-by-step reasoning. As the reasoning trace becomes longer and more complex to solve challenging problems, existing methods might fail to capture the complex structure of the solution. As the step-by-step reasoning performance and trustworthiness of LLMs improve, proper and careful evaluation will surely remain crucially important.

% \newpage

\section{Limitation}

This survey aims to provide a comprehensive view of step-by-step evaluation reasoning by focusing on criteria definition and metric implementations. In return, this work does not fully address the role of \textit{human judgments} in the task, including the human annotation process \citep{DBLP:conf/iclr/LightmanKBEBLLS24, zheng2024processbenchidentifyingprocesserrors, song2025prmbenchfinegrainedchallengingbenchmark}, human correlation \citep{zha-etal-2023-alignscore, DBLP:conf/iclr/GolovnevaCPCZFC23, prasad-etal-2023-receval}, and inter-annotator agreement \citep{jacovi-etal-2024-chain}. Furthermore, while this work analyzes reported empirical results in Section \ref{sec:transfer}, it does not perform additional experiments to compare more diverse metrics in a fair and comprehensive setting.