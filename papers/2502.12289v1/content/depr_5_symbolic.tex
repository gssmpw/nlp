% \section{Applications}
% \label{sec:applications}

% Step-by-step reasoning evaluation can also be used to \textit{improve} LLM's ability to reason \citep{guan2024searchverifyfeedbackgeneration}. This can happen in both training and inference. For training, evaluation metrics serve as \textit{rewards}, where \textit{reinforcement learning} is applied to obtain better reasoning ability by directly improving the metrics. During the inference, metrics serve as \textit{verifiers}, deciding which step to choose among multiple candidates.

\section{Symbolic reasoning}
\label{sec:symbolic}

\subsection{Tasks}

Due to the improvement of LLMs' reasoning ability since the discovery of Chain-of-thought prompting \citep{NEURIPS2022_9d560961, NEURIPS2022_8bb0d291}, step-by-step reasoning has proven effective in symbolic reasoning tasks\footnote{While symbolic reasoning may strictly refer to \textit{algorithmic reasoning} \citep{NEURIPS2022_9d560961}, we adopt the broader sense including math and logical reasoning that can be expressed in \textit{symbols} (equation, logic) \citep{sprague2024cotcotchainofthoughthelps}.} such as \textbf{mathematical reasoning} including arithmetics, calculus, and number theory; \textbf{logical reasoning} that involve performing complex sequence of deductive inference; and \textbf{algorithmic reasoning} that manipulates strings or data structures. Further details on reasoning tasks and benchmarks are presented in Appendix \ref{sec:appendix-task-symbol}.

\subsection{Evaluation methods}
\label{sec:reinforcement}

\subsubsection{Groundedness/Coherence}

% \note{Off-the-shelf NLI models are not suitable. Deductive Beam Search trains its own model using groundedness-perturbed instances}

\textbf{Cross-encoder.}  \hspace{0.1cm} Deductive Verifier \citep{zhu2024deductivebeamsearchdecoding} develops a synthetic dataset that perturbs the correct reasoning traces with wrong numbers (groundedness) or removed previous steps (coherence) and trains a cross-encoder that evaluates a step based on the entire context (question and previous steps). However, as the model jointly represents both errors in a single score, groundedness errors and coherence errors are not distinguishable in the scores from this model.

\subsubsection{Validity}

% \note{Process Reward Models (validity) - PRM800K, Qwen-PRM800K, LLM Critics Help Catch Bugs in Mathematics (Do not confuse with PRMs based on utility.) Regression head that determines the logit.}

\textbf{Process Reward Models (validity).} \hspace{0.1cm} Given a large dataset of reasoning traces with stepwise validity labels (\textit{e.g.} correct, erroneous), one can train a \textit{classifier} using the labels. PRM800k \citep{DBLP:conf/iclr/LightmanKBEBLLS24} provides massive human annotation on reasoning traces sampled from MATH benchmark \citep{NEURIPSDnB2021_be83ab3e}. Numerous PRMs are trained using this data \citep{DBLP:conf/iclr/LightmanKBEBLLS24, gao2024llmcriticshelpcatch, zhang2025lessonsdevelopingprocessreward}, by adding a lightweight classifier head on top of the fine-tuned LLM and training on the PRM800k validity labels at the end of each step.

% \note{Critic models - How we prompt off-the-shelf models to detect fallacy. (1) Evaluate the entire trace (is this trace correct or not?), or (2) determine the first erroneous step (multi-choice)}

\textbf{Critic LLMs}. \hspace{0.1cm} Another popular method for validity evaluation is to prompt off-the-shelf language models \citep{jacovi-etal-2024-chain, tyen-etal-2024-llms, zheng2024processbenchidentifyingprocesserrors}, similar to LLM-as-a-judge \citep{NEURIPS2023_91f18a12}. In this setting, LLMs are prompted to output if a given step is valid based on its previous steps, \textit{e.g.} asking to \textit{"identify an error"} \citep{zheng2024processbenchidentifyingprocesserrors}. While \citet{tyen-etal-2024-llms} reported poor performance of LLMs pinpointing the error location, results on ProcessBench \citep{zheng2024processbenchidentifyingprocesserrors} show that reasoning-focused models such as o1 \citep{openai2024openaio1card} and QwQ \citep{qwenlmQwQReflect} show remarkable performance even in challenging math benchmarks.

% \note{Generative verifier (+CLoud). Intermediate form of above two. Generate CoT and output score as token logits.}

\textbf{Generative verifier.} This paradigm lies in the middle ground of PRMs and critic models, by first generating the evaluation rationale and then using a small head to predict the numerical scores conditioned on the self-generated rationales \citep{ankner2024critiqueoutloudrewardmodels, zhang2024generativeverifiersrewardmodeling}.

% \subsubsection{Coherence}

% \textbf{Cross-encoder.} \hspace{0.1cm} Deductive Verifier \citet{zhu2024deductivebeamsearchdecoding}'s synthetic training data includes coherence errors, where a step is skipped. However, the model is trained to jointly predict groundedness and coherence errors, making the two undistinguishable.

\subsubsection{Utility (progress)}

\textbf{Critic models.} \hspace{0.1cm} To solve problems that have large search space such as Game of 24\footnote{In Game of 24, one should make 24 using four integers and basic arithmetic operations, \textit{e.g.} using 4, 5, 6, and 7, the correct answer is $(5+7-6) \times 4 = 24$.}, Tree-of-thoughts \citep{NEURIPS2023_271db992} leverage LLMs to predict the utility of a given reasoning step by asking questions like \textit{"Evaluate if $5+7$ step can reach 24"}. 

\textbf{Symbolic solutions.} \hspace{0.1cm} DiVeRSe \citep{li-etal-2023-making} evaluates the progress of a step in arithmetic word problems by extracting the number and comparing it to the gold calculation tree.

However, progress-based evaluation often fails in generalizing to more complex problems (\textit{e.g.} olympiad-level math), currently falling behind validity/utility-based PRMs in terms of performance in popularity.

\subsubsection{Utility (value function)}

\textbf{Process Reward Models (utility).} \hspace{0.1cm} The most popular variant of utility builds over \textbf{Monte Carlo Tree Search (MCTS)}. This variant views utility as \textit{value functions} in reinforcement learning, where \textit{states} (partially generated traces) have higher values if they are likely to get higher \textit{rewards} at the end (correct answer). In MCTS, given a previously generated trace, (1) multiple possible next ERUs are sampled, (2) their \textit{continuation (rollout)} is generated and evaluated (rewarded) based on the final answer, and (3) the utility of each sampled ERU is estimated by back-propagating the reward. As a result, each ERU will be assigned a utility score that denotes how likely it will lead to the correct answer \citep{hao-etal-2023-reasoning, xie2024montecarlotreesearch, wang-etal-2024-math}. Similar to validity PRMs, utility PRMs train to predict the \textit{value function} obtained from extensive tree search using a small regression head \citep{wang-etal-2024-math, he-etal-2024-advancing, zhang2025lessonsdevelopingprocessreward}.

% \note{DPO Models (prob is the reward) - DPO=>Generative Reward Models (Madan et al.) / stepwise DPO methods (Step-DPO, MCTS-DPO, Chain-of-preference optimization)}

\textbf{LLM-as-value-function.} On the other hand, LLMs can be directly trained to align sequence probabilities (relative to the base model's probability) to the value function without the regression head \citep{schulman2017proximalpolicyoptimizationalgorithms, NEURIPS2023_a85b405e}. Consequently, LLMs trained to maximize the relative probability of reasoning traces with correct answers are already a utility evaluator \citep{mahan2024generativerewardmodels, lai2024stepdpostepwisepreferenceoptimization, xie2024montecarlotreesearch, pang2024iterativereasoningpreferenceoptimization, cui2025processreinforcementimplicitrewards}, where the relative sequence probability is directly interpreted as the utility score. Unlike PRMs that are not fine-tuned for generation, these models retain (and improve) the ability to generate. However, these models require an additional forward pass to obtain the base probability, doubling the computation cost of evaluators.



% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/reinforcement_learning.pdf}
%     \caption{Four coarse types of reinforcement learning. Each cell includes a representative training objective for the reward model.}
%     \label{fig:reinforcement-learning}
% \end{figure}

% Reinforcement learning aims to maximize the expected \textit{reward} of possible outputs \citep{wang2024reinforcementlearningenhancedllms}. In this perspective, reasoning trace evaluation metrics can directly serve as \textit{reward models} that decide the reward for different reasoning traces. Reward models for reasoning can classified into four distinct classes: \textbf{outcome/process} rewards \citep{uesato2022solvingmathwordproblems, luo2024improvemathematicalreasoninglanguage, jiang2024technicalreportenhancingllm}) and \textbf{pointwise/pairwise} rewards \citep{lee-etal-2023-learning} (Figure \ref{fig:reinforcement-learning}).

% Popular RL-based training objectives for LLMs, \textit{e.g.} Proximal Policy Optimization (PPO) \citep{schulman2017proximalpolicyoptimizationalgorithms} and Direct Preference Optimization (DPO) \citep{NEURIPS2023_a85b405e}, optimize by evaluating entire output sequences. Therefore, datasets designed for general-purpose RL training for LLMs often include entire trace-level annotations. For instance, UltraFeedback dataset \citep{cui2024ultrafeedbackboostinglanguagemodels} includes LLM-annotated preference data of reasoning traces in arithmetic benchmarks (GSM8K, SVAMP, Asdiv) and commonsense reasoning benchmarks (StrategyQA, CommonsenseQA).

% However, there are more specialized datasets that compare entire reasoning traces by their utility, \textit{i.e.} the answer correctness. \citet{pang2024iterativereasoningpreferenceoptimization} generates preference pairs of reasoning traces by selecting one trace that reaches the correct final answer and one that does not, which can be extended to multi-turn reasoning traces where LLM interacts with an external deterministic tool (\textit{e.g.} Python interpreter) \citet{xiong2024buildingmathagentsmultiturn}. Step-Controlled DPO \citep{lu2024stepcontrolleddpoleveragingstepwise} starts from a correct reasoning trace and generates deliberately incorrect traces by manipulating each reasoning step. Then, an LLM-based ORM is trained to distinguish between the correct original trace and manipulated incorrect traces using the DPO objective.

% \textbf{Pointwise outcome rewards.} \hspace{0.1cm} The most direct form of RL for reasoning is to reward traces that reach the correct answer (high utility) and penalize the others. While the most popular algorithm is Proximal Policy Optimization (PPO) \citep{schulman2017proximalpolicyoptimizationalgorithms}, its applicability in reasoning is questionable due to its inefficiency compared to rejection sampling-based supervised fine-tuning, and not being able to benefit from separately trained outcome reward models \citep{havrilla2024teachinglargelanguagemodels}. However, recently, DeepSeek-R1 \citep{deepseekai2025deepseekr1incentivizingreasoningcapability} has achieved significant performance with pointwise utility rewards (\textit{accuracy rewards}) with Group Relative Policy Optimization (GRPO; \citet{shao2024deepseekmathpushinglimitsmathematical}), a variant of PPO that normalizes the rewards by subtracting the average reward of multiple generations (\textit{groups}).

% \textbf{Pairwise outcome rewards.} \hspace{0.1cm} Due to the success of Direct Preference Optimization (DPO; \citet{NEURIPS2023_a85b405e}), pairwise comparison of reasoning traces became popular. UltraFeedback \citep{cui2024ultrafeedbackboostinglanguagemodels} includes LLM-annotated preference data of reasoning traces in math and commonsense benchmarks, although the answer accuracy is not measured. \citet{pang2024iterativereasoningpreferenceoptimization} generates preference pairs of reasoning traces by selecting one trace that reaches the correct final answer and one that does not, which can be extended to multi-turn reasoning traces where LLM interacts with an external deterministic tool (\textit{e.g.} Python interpreter) \citet{xiong2024buildingmathagentsmultiturn}. Finally, Step-Controlled DPO \citep{lu2024stepcontrolleddpoleveragingstepwise} starts from a correct reasoning trace and generates deliberately incorrect traces by manipulating each reasoning step. Then, an LLM-based ORM is trained to distinguish between the correct original trace and manipulated incorrect traces using the DPO objective.

% \textbf{Pointwise process rewards.} \hspace{0.1cm} On the other hand, pointwise PRMs aim to predict the utility of individual steps based on MCTS. Math-Sheperd \citep{wang-etal-2024-math} first obtains a rough estimation of step-wise utility using tree search and applies PPO to train the LLM to maximize the utility score. Other works (\textit{e.g.} \citet{DBLP:conf/iclr/LightmanKBEBLLS24}, GenRM \citep{zhang2024generativeverifiersrewardmodeling}, OmegaPRM \citep{luo2024improvemathematicalreasoninglanguage}, and Qwen-PRM \citep{zhang2025lessonsdevelopingprocessreward}) focus on training a specialized PRM, by training token logits to match the MCTS estimation of value functions.

% \textbf{Pairwise process rewards.} \hspace{0.1cm} Finally, PRMs can also be obtained by learning to distinguish a pair of useful and useless steps, \textit{i.e.} pairwise PRMs. Step-DPO \citep{lai2024stepdpostepwisepreferenceoptimization}, CPO \citep{zhang2024chainpreferenceoptimizationimproving}, and MCTS-DPO \citep{xie2024montecarlotreesearch} apply DPO to make the LLM prefer useful steps over useless ones, inducing the ability to reason better and to operate as PRMs.

% \subsection{Inference-time exploration}
% \label{sec:inference-time-exploration}

% In contrast to training the model's parameters, one can employ \textit{inference-time exploration} to guide the LLMs in generating better reasoning chains. These methods implement diverse search algorithms and verifier-guided pruning heuristics to explore the correct proof in the large space of possible reasoning traces.

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figures/inference_time_exploration.pdf}
%     \caption{A brief illustration of inference-time exploration approaches. The majority voting method relies on trace-level utility, while backtracking and tree search methods employ step-level evaluation metrics either to decide to re-sample a specific ERU or to select between multiple sampled ERUs.}
%     \label{fig:inference-time-exploration}
% \end{figure}

% % The vast majority of inference-time exploration can be expressed as \textit{tree search}. In tree search, the previously generated reasoning trace is the state $s$, and a candidate ERU for the next reasoning step represents the actions $a$. The goal of tree search is to obtain a reward model $r(s, a)$ that evaluates each action if it can ultimately lead to a \textit{good} reasoning trace.

% % Self-consistency

% % The simplest form of inference-time exploration is simply sampling the entire reasoning trace multiple times and ensemble by majority voting, often referred to as \textbf{Self-consistency} \citep{DBLP:conf/iclr/0002WSLCNCZ23}. This can be viewed as searching depth 1 tree where each node (ERU) is the entire reasoning trace. While this approach does not require a step-level verifier, this approach requires sampling a large number of reasoning traces in an end-to-end manner, resulting in significantly more computational burden and marginal performance gain.

% \textbf{Best-of-N.} \hspace{0.1cm} The simplest method for applying step-by-step reasoning evaluation to inference-time exploration is \textbf{Best-of-N (BoN)} \citep{wang-etal-2024-math, cui2024ultrafeedbackboostinglanguagemodels, zhang2025lessonsdevelopingprocessreward}. BoN uses a verifier to select the best trace among $N$ candidates, which can be viewed as reranking. A slight variant, \textbf{weighted voting} \citep{yuan2024freeprocessrewardsprocess}, decides the final answer based on the sum of evaluation scores instead of choosing the one with the highest score. Note that pointwise ORMs designed for reinforcement learning can be directly applied to BoN and weighted voting as they share the same ERU granularity (entire trace).

% %  Backtrack

% \textbf{Backtracking.} \hspace{0.1cm} Backtracking is a depth-first search strategy using fine-grained ERUs and metrics. where one samples a trace, evaluates all reasoning steps with a step-level metric, backtracks to an erroneous step that achieves a score lower than a threshold, and generates from the step until all steps are evaluated as positive. \citep{NEURIPS2023_271db992, wu-etal-2024-synchronous, tyen-etal-2024-llms}.

% % Beam search: Tree-of-thoughts
% \textbf{Beam search.} \hspace{0.1cm} Finally, breadth-first beam search can also be applied to find the most promising trajectory. For instance, Tree-of-thoughts \citep{NEURIPS2023_271db992} first samples multiple possible subsequent steps, evaluates each step's utility by asking an LLM, and selects the best traces up to the beam size $K$ to solve Game of 24. A large number of methods succeeded the paradigm \citep{NEURIPS2023_81fde95c, wu-etal-2024-synchronous, mo2024tree, zhu2024deductivebeamsearchdecoding}, showing the generalizability of beam search to diverse reasoning tasks and evaluation metrics. 