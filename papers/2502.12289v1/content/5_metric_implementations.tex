\section{Metric implementations}
\label{sec:metric-implementations}

\begin{figure*}[tb]
    \centering
    \includegraphics[width=\linewidth]{figures/metric_implementations.pdf}
    \caption{Illustration of six representative metric implementations. (a) and (b) use the token probabilities of the LLM generating the trace, and (c)-(e) train a separate evaluator model. (f) trains the LLM so that the token probabilities can be interpreted as scores.}
    \label{fig:metric-implementations}
\end{figure*}

\input{tables/metric_to_criteria}

Numerous metrics have been proposed to evaluate and quantify the quality of a reasoning trace beyond the answer correctness. This section provides an overview of these methods, from rule-based metrics to neural models.

\subsection{Rule-based matching}

For tasks where the ground truth solution can be expressed as a \textit{graph of entities}, one can view a step as a directed edge between two entities. Typical examples include knowledge graphs for factual reasoning \citet{nguyen-etal-2024-direct} or computation graphs in arithmetic problems \citep{li-etal-2023-making}. In this setting, groundedness corresponds to having the necessary entities given in the query, validity to predicting the relation between entities, coherence to the correct ordering of steps, and utility to the existence of the step in the gold reasoning chain \citep{nguyen-etal-2024-direct, PrOntoQA}. However, this approach may not generalize well for tasks that do not have a straightforward graph representation, \textit{e.g.} commonsense reasoning or complex math reasoning beyond arithmetic word problems.


\subsection{Intrinsic properties}

\textbf{Uncertainty.} \textbf{Uncertainty} of the model can be used as an intrinsic proxy about the generated content's quality \citep{xiao-wang-2021-hallucination, zhang-etal-2023-enhancing-uncertainty}. \citet{qiu2024entropybaseddecodingretrievalaugmentedlarge} and \citet{wu-etal-2024-synchronous} use \textit{token probability entropy} (Figure \ref{fig:metric-implementations}(a)), defined as $\Sigma_{t\in V} p(t)\textrm{log}(p(t))$ where $p$ is the probability distribution of all tokens in vocabulary $V$. \citet{farquhar2024detecting} and \citet{kossen2024semanticentropyprobesrobust} extend the approach by clustering semantically similar answers and calculating the entropy with respect to the clusters. Another variant of uncertainty uses \textbf{confidence}, \textit{i.e.} $\textrm{max}_{t\in V} p(t)$ \citep{wu-etal-2024-synchronous, wang2024chainofprobeexamingnecessityaccuracy}. In this setting, higher confidence implies that the step is more grounded/correct.

\textbf{$\mathcal{V}$-information.} \citep{chen-etal-2023-rev, prasad-etal-2023-receval} use \textbf{Conditional $\mathcal{V}$-information (CVI)} \citep{hewitt-etal-2021-conditional} to evaluate reasoning traces. CVI can be informally defined as the amount of information the evaluation target text $t$ adds to the model.
Formally, given a model $g$ trained to predict the answer \textit{with} $t$ (calculates $g(a\mid q,t)$) and $g'$ trained to predict the answer $a$ \textit{without} $t$ (calculates $g'(a\mid q)$), the CVI is calculated by
\[
\mathit{CVI}(t\rightarrow a\mid q) = -\textrm{log}g'(a\mid q) + \textrm{log}g(a\mid q,t)
\]
which is maximized when predicting the answer without the target is hard (smaller $g'(a\mid q)$) but it becomes easier with the target (larger $g(a\mid q,t)$) (Figure \ref{fig:metric-implementations}(b)). While this definition directly corresponds to utility \citep{chen-etal-2023-rev}, \citet{prasad-etal-2023-receval} leverages CVI to evaluate validity in an ensemble with cross-encoders (introduced below).

\subsection{Neural evaluator models.}

\textbf{Cross-encoders.} Cross-encoders are neural models that simultaneously encode two sentences using a single network (Figure \ref{fig:metric-implementations}(c)). They have been widely applied to solve tasks such as natural language inference \citep{bowman-etal-2015-large} and fact verification \citep{thorne-etal-2018-fever}, where one has to determine if the \textit{hypothesis} can be inferred from the given \textit{premise}. Cross-encoders trained on these off-the-shelf tasks are used to evaluate a reasoning step based on the query (groundedness) or previous steps (validity) \citep{wu-etal-2024-synchronous, zha-etal-2023-alignscore, prasad-etal-2023-receval}. Instead of using an off-the-shelf model, \citet{zhu2024deductivebeamsearchdecoding} perturbs correct traces with LLMs and uses the synthetic data to train the cross-encoder.

% Evaluating groundedness can be seen as checking if the provided query \textit{entails} the trace, \textit{i.e.} the natural language inference task (NLI; \citet{bowman-etal-2015-large}). Consequently, ROSCOE-LI \citep{DBLP:conf/iclr/GolovnevaCPCZFC23} AlignScore \citep{zha-etal-2023-alignscore}, and ReCEval \citep{prasad-etal-2023-receval} use an off-the-shelf cross-encoder trained from NLI and similar tasks, \textit{e.g.} fact verification, question answering, and paraphrase identification, for validity evaluation. Deductive Verifier \citep{zhu2024deductivebeamsearchdecoding} develops a synthetic dataset that perturbs the correct reasoning traces with wrong numbers (groundedness) or removed previous steps (coherence) and trains a cross-encoder that evaluates a step based on the entire context (question and previous steps). However, as the model jointly represents both errors in a single score, groundedness errors and coherence errors are not distinguishable in the scores from this model.

\textbf{Process reward models.} While process reward model (PRM) is defined as \textit{"a model that provides feedback/evaluation for each step"} in the broadest sense, in practice, it commonly refers to an LLM with a lightweight head attached to the final layer and trained to predict a numeric score in a supervised manner \citep{DBLP:conf/iclr/LightmanKBEBLLS24, wang-etal-2024-math, setlur2024rewardingprogressscalingautomated}. The training data can be categorized as (1) \textit{validity data} including correctness annotations for each step \citep{NEURIPSDnB2021_be83ab3e} (Figure \ref{fig:metric-implementations}(d)), or (2) \textit{utility data} \citep{wang-etal-2024-math} providing the value function obtained from Monte Carlo Tree Search (MCTS) and its variants (Figure \ref{fig:metric-implementations}(e)). We discuss the difference and transferability between these PRMs in Section \ref{sec:transfer-vu}.

\textbf{Critic models (LLM-as-a-judge).} LLM-as-a-judge \citep{NEURIPS2023_91f18a12, DBLP:conf/iclr/KimS0JLLYSKTS24} is a widely accepted paradigm for evaluate long texts. In reasoning trace evaluation, the term \textit{critic models} often refers to the same concept \citep{zheng2024processbenchidentifyingprocesserrors, lin2024criticbenchbenchmarkingllmscritiquecorrect}. \citet{jacovi-etal-2024-chain, wu-etal-2024-mitigating, niu2024ragtruthhallucinationcorpusdeveloping, NEURIPS2023_271db992} showed that prompting instruction-tuned LLMs can effectively evaluate groundedness, validity, coherence, and utility in diverse reasoning tasks with Chain-of-thoughts prompting \citep{NEURIPS2022_9d560961}. The specific format of evaluation can vary from (1) evaluating if the entire trace is correct or not, (2) finding the location of the first erroneous step given the entire trace, or (3) judging a single step's correctness based on the query and previous steps.


% Furthermore, HalluMeasure \citep{akbar-etal-2024-hallumeasure} leverages LLMs for more fine-grained evaluation, \textit{i.e.} (1) decompose reasoning steps into atomic claims and (2) evaluate the groundedness of each claim based on the retrieved documents, inspired from \citet{min-etal-2023-factscore}. Another popular method for validity evaluation is to prompt off-the-shelf language models \citep{jacovi-etal-2024-chain, tyen-etal-2024-llms, zheng2024processbenchidentifyingprocesserrors}, similar to LLM-as-a-judge \citep{NEURIPS2023_91f18a12}. In this setting, LLMs are prompted to output if a given step is valid based on its previous steps, \textit{e.g.} asking to \textit{"identify an error"} \citep{zheng2024processbenchidentifyingprocesserrors}. While \citet{tyen-etal-2024-llms} reported poor performance of LLMs pinpointing the error location, results on ProcessBench \citep{zheng2024processbenchidentifyingprocesserrors} show that reasoning-focused models such as o1 \citep{openai2024openaio1card} and QwQ \citep{qwenlmQwQReflect} show remarkable performance even in challenging math benchmarks.

\textbf{Generative Verifiers.} This paradigm lies in the middle ground of PRMs and critic models, by first generating the evaluation rationale and then using a small head to predict the numerical scores conditioned on the self-generated rationales \citep{ankner2024critiqueoutloudrewardmodels, zhang2024generativeverifiersrewardmodeling}.

\textbf{LLM-as-value-function.} LLMs can be directly trained to align sequence probabilities (relative to the initial model's probability) to the value function as shown in Direct Preference Optimization (DPO; \citet{NEURIPS2023_a85b405e}) (Figure \ref{fig:metric-implementations}(f)). Consequently, LLMs trained to distinguish traces with correct answers from incorrect ones by DPO can directly serve as a utility evaluator \citep{mahan2024generativerewardmodels, lai2024stepdpostepwisepreferenceoptimization, xie2024montecarlotreesearch, pang2024iterativereasoningpreferenceoptimization, cui2025processreinforcementimplicitrewards}, where the relative sequence probability is the utility score. Unlike PRMs that are not fine-tuned for generation, these models retain (and improve) the ability to generate. However, these models require an additional forward pass to obtain the initial model probability, doubling the computation cost during the evaluation phase.
