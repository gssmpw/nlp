\section{Evaluation granularity}
\label{sec:eru}

Elementary Reasoning Units (ERUs) are the basic units for step-by-step reasoning evaluation. The optimal choice of ERUs can depend on various factors, including the reasoning task, the purpose of the evaluation, and the computation budget. In this section, we compare various granularities of ERUs adopted by previous studies.

\subsection{Entire trace}

The entire trace is the coarsest possible ERU for step-by-step reasoning evaluation. This setting can be found in preference optimizations datasets that estimate the quality of model-generated outputs \citep{cui2024ultrafeedbackboostinglanguagemodels, kim2024biggenbenchprincipledbenchmark}. As this setting does not require ERU segmentation and requires less inference time than fine-grained evaluation, it is widely used for outcome reward models (ORMs) and trace-level inference-time exploration strategies such as Best-of-N search (further described in Section \ref{sec:applications}). However, recent studies claim that the entire trace is overly coarse, leading to suboptimal performance in identifying subtle errors and mistakes in the reasoning step \citep{DBLP:conf/iclr/LightmanKBEBLLS24, zheng2024processbenchidentifyingprocesserrors}.

% However, evaluating a long-form text with a single score has been continuously criticized for its vagueness, leading to high inter-annotator disagreement and noisy training signals. Since step-by-step reasoning can be decomposed into subunits, this approach is generally discouraged for precise and comprehensive evaluation of step-by-step reasoning.

\subsection{Steps/Sentences}

\textbf{Steps} are the most commonly used ERU in step-by-step reasoning evaluation \citep{DBLP:conf/iclr/LightmanKBEBLLS24, NEURIPS2023_72393bd4, lai2024stepdpostepwisepreferenceoptimization, wang-etal-2024-math, lu2024stepcontrolleddpoleveragingstepwise}. In this case, models are often instructed to indicate the step boundary using keywords such as \textit{Step 1:}. While steps provide the most intuitive definition of the ERU, it rely on the inner judgment of the models for segmentation, which can be highly affected by fine-tuning data.

If the model does not explicitly indicate steps, \textbf{sentences} \citep{zha-etal-2023-alignscore, xie2024montecarlotreesearch} can be used as a natural segmentation. Sentence ERUs allow using traditional sentence-level resources such as models for natural language inference (NLI) \citep{zha-etal-2023-alignscore}.

While steps and sentences serve as the most natural form of ERUs due to their trivial segmentation, they often include multiple independent claims, being only \textit{partially} correct \citep{min-etal-2023-factscore, prasad-etal-2023-receval}. Such non-atomicity complicates the precise evaluation of ERUs, which motivates defining more \textit{atomic} semantic units as ERUs.

\subsection{Spans/Propositions}

\textbf{Spans} are defined as consecutive sequences of tokens, often shorter than an entire sentence. In reasoning tasks, span-based evaluators use external segmenting models and evaluate them individually \citep{prasad-etal-2023-receval}, or use span extraction models that directly predict the position of the erroneous span \citep{niu2024ragtruthhallucinationcorpusdeveloping}.

\textbf{Propositions} is a logical form or a sentence containing minimal information that cannot be further decomposed. Popular forms of propositions include entity relation triplets \citep{wang-etal-2024-boosting-language, nguyen-etal-2024-direct} and semantically atomic sentences \citep{min-etal-2023-factscore, akbar-etal-2024-hallumeasure}.

Spans and propositions can express fine-grained information which leads to higher expressibility, \textit{e.g.} distinguishing between premises and conclusions of each step \citep{prasad-etal-2023-receval}. However, As they require \textit{extraction} as preprocessing prior to the actual evaluation, it is not often suitable for real-time evaluation which is critical in inference-time exploration methods (Section \ref{sec:applications}).

\subsection{Tokens}

\textbf{Tokens} are the smallest unit of generative language models. As tokens only contain lexical information, relatively few works attempt to evaluate individual tokens in reasoning tasks.

% While attempts using external token-level attribution models \citep{NEURIPS2023_4d4a3b6a, li-etal-2024-reinforcement} or token-level beam search tree \citep{lee-etal-2023-learning} were made to evaluate the utility of each token, their generalizability to step-by-step reasoning and larger models still remains underexplored.

A recent work \citep{bigelow2024can} explored the token-level utility in step-by-step reasoning by starting from a correct reasoning trace, sampling $K$ alternative tokens at all positions, and observing their continuation. It shows that the utility often abruptly drifts in seemingly random token positions, such as functional words and punctuation marks.