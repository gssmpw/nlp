\section{Application-oriented definitions}

While the survey focuses on constructing a definition-based taxonomy,  

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/reinforcement_learning.pdf}
    \caption{Four coarse types of reinforcement learning. Each cell includes a representative training objective for the reward model.}
    \label{fig:reinforcement-learning}
\end{figure}

Reinforcement learning aims to maximize the expected \textit{reward} of possible outputs \citep{wang2024reinforcementlearningenhancedllms}. In this perspective, reasoning trace evaluation metrics can directly serve as \textit{reward models} that decide the reward for different reasoning traces. Reward models for reasoning can classified into four distinct classes: \textbf{outcome/process} rewards \citep{uesato2022solvingmathwordproblems, luo2024improvemathematicalreasoninglanguage, jiang2024technicalreportenhancingllm}) and \textbf{pointwise/pairwise} rewards \citep{lee-etal-2023-learning} (Figure \ref{fig:reinforcement-learning}).

Popular RL-based training objectives for LLMs, \textit{e.g.} Proximal Policy Optimization (PPO) \citep{schulman2017proximalpolicyoptimizationalgorithms} and Direct Preference Optimization (DPO) \citep{NEURIPS2023_a85b405e}, optimize by evaluating entire output sequences. Therefore, datasets designed for general-purpose RL training for LLMs often include entire trace-level annotations. For instance, UltraFeedback dataset \citep{cui2024ultrafeedbackboostinglanguagemodels} includes LLM-annotated preference data of reasoning traces in arithmetic benchmarks (GSM8K, SVAMP, Asdiv) and commonsense reasoning benchmarks (StrategyQA, CommonsenseQA).

However, there are more specialized datasets that compare entire reasoning traces by their utility, \textit{i.e.} the answer correctness. \citet{pang2024iterativereasoningpreferenceoptimization} generates preference pairs of reasoning traces by selecting one trace that reaches the correct final answer and one that does not, which can be extended to multi-turn reasoning traces where LLM interacts with an external deterministic tool (\textit{e.g.} Python interpreter) \citet{xiong2024buildingmathagentsmultiturn}. Step-Controlled DPO \citep{lu2024stepcontrolleddpoleveragingstepwise} starts from a correct reasoning trace and generates deliberately incorrect traces by manipulating each reasoning step. Then, an LLM-based ORM is trained to distinguish between the correct original trace and manipulated incorrect traces using the DPO objective.

\textbf{Pointwise outcome rewards.} \hspace{0.1cm} The most direct form of RL for reasoning is to reward traces that reach the correct answer (high utility) and penalize the others. While the most popular algorithm is Proximal Policy Optimization (PPO) \citep{schulman2017proximalpolicyoptimizationalgorithms}, its applicability in reasoning is questionable due to its inefficiency compared to rejection sampling-based supervised fine-tuning, and not being able to benefit from separately trained outcome reward models \citep{havrilla2024teachinglargelanguagemodels}. However, recently, DeepSeek-R1 \citep{deepseekai2025deepseekr1incentivizingreasoningcapability} has achieved significant performance with pointwise utility rewards (\textit{accuracy rewards}) with Group Relative Policy Optimization (GRPO; \citet{shao2024deepseekmathpushinglimitsmathematical}), a variant of PPO that normalizes the rewards by subtracting the average reward of multiple generations (\textit{groups}).

\textbf{Pairwise outcome rewards.} \hspace{0.1cm} Due to the success of Direct Preference Optimization (DPO; \citet{NEURIPS2023_a85b405e}), pairwise comparison of reasoning traces became popular. UltraFeedback \citep{cui2024ultrafeedbackboostinglanguagemodels} includes LLM-annotated preference data of reasoning traces in math and commonsense benchmarks, although the answer accuracy is not measured. \citet{pang2024iterativereasoningpreferenceoptimization} generates preference pairs of reasoning traces by selecting one trace that reaches the correct final answer and one that does not, which can be extended to multi-turn reasoning traces where LLM interacts with an external deterministic tool (\textit{e.g.} Python interpreter) \citet{xiong2024buildingmathagentsmultiturn}. Finally, Step-Controlled DPO \citep{lu2024stepcontrolleddpoleveragingstepwise} starts from a correct reasoning trace and generates deliberately incorrect traces by manipulating each reasoning step. Then, an LLM-based ORM is trained to distinguish between the correct original trace and manipulated incorrect traces using the DPO objective.

\textbf{Pointwise process rewards.} \hspace{0.1cm} On the other hand, pointwise PRMs aim to predict the utility of individual steps based on MCTS. Math-Sheperd \citep{wang-etal-2024-math} first obtains a rough estimation of step-wise utility using tree search and applies PPO to train the LLM to maximize the utility score. Other works (\textit{e.g.} \citet{DBLP:conf/iclr/LightmanKBEBLLS24}, GenRM \citep{zhang2024generativeverifiersrewardmodeling}, OmegaPRM \citep{luo2024improvemathematicalreasoninglanguage}, and Qwen-PRM \citep{zhang2025lessonsdevelopingprocessreward}) focus on training a specialized PRM, by training token logits to match the MCTS estimation of value functions.

\textbf{Pairwise process rewards.} \hspace{0.1cm} Finally, PRMs can also be obtained by learning to distinguish a pair of useful and useless steps, \textit{i.e.} pairwise PRMs. Step-DPO \citep{lai2024stepdpostepwisepreferenceoptimization}, CPO \citep{zhang2024chainpreferenceoptimizationimproving}, and MCTS-DPO \citep{xie2024montecarlotreesearch} apply DPO to make the LLM prefer useful steps over useless ones, inducing the ability to reason better and to operate as PRMs.

\subsection{Inference-time exploration}
\label{sec:inference-time-exploration}

In contrast to training the model's parameters, one can employ \textit{inference-time exploration} to guide the LLMs in generating better reasoning chains. These methods implement diverse search algorithms and verifier-guided pruning heuristics to explore the correct proof in the large space of possible reasoning traces.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/inference_time_exploration.pdf}
    \caption{A brief illustration of inference-time exploration approaches. The majority voting method relies on trace-level utility, while backtracking and tree search methods employ step-level evaluation metrics either to decide to re-sample a specific ERU or to select between multiple sampled ERUs.}
    \label{fig:inference-time-exploration}
\end{figure}

% The vast majority of inference-time exploration can be expressed as \textit{tree search}. In tree search, the previously generated reasoning trace is the state $s$, and a candidate ERU for the next reasoning step represents the actions $a$. The goal of tree search is to obtain a reward model $r(s, a)$ that evaluates each action if it can ultimately lead to a \textit{good} reasoning trace.

% Self-consistency

% The simplest form of inference-time exploration is simply sampling the entire reasoning trace multiple times and ensemble by majority voting, often referred to as \textbf{Self-consistency} \citep{DBLP:conf/iclr/0002WSLCNCZ23}. This can be viewed as searching depth 1 tree where each node (ERU) is the entire reasoning trace. While this approach does not require a step-level verifier, this approach requires sampling a large number of reasoning traces in an end-to-end manner, resulting in significantly more computational burden and marginal performance gain.

\textbf{Best-of-N.} \hspace{0.1cm} The simplest method for applying step-by-step reasoning evaluation to inference-time exploration is \textbf{Best-of-N (BoN)} \citep{wang-etal-2024-math, cui2024ultrafeedbackboostinglanguagemodels, zhang2025lessonsdevelopingprocessreward}. BoN uses a verifier to select the best trace among $N$ candidates, which can be viewed as reranking. A slight variant, \textbf{weighted voting} \citep{yuan2024freeprocessrewardsprocess}, decides the final answer based on the sum of evaluation scores instead of choosing the one with the highest score. Note that pointwise ORMs designed for reinforcement learning can be directly applied to BoN and weighted voting as they share the same ERU granularity (entire trace).

%  Backtrack

\textbf{Backtracking.} \hspace{0.1cm} Backtracking is a depth-first search strategy using fine-grained ERUs and metrics. where one samples a trace, evaluates all reasoning steps with a step-level metric, backtracks to an erroneous step that achieves a score lower than a threshold, and generates from the step until all steps are evaluated as positive. \citep{NEURIPS2023_271db992, wu-etal-2024-synchronous, tyen-etal-2024-llms}.

% Beam search: Tree-of-thoughts
\textbf{Beam search.} \hspace{0.1cm} Finally, breadth-first beam search can also be applied to find the most promising trajectory. For instance, Tree-of-thoughts \citep{NEURIPS2023_271db992} first samples multiple possible subsequent steps, evaluates each step's utility by asking an LLM, and selects the best traces up to the beam size $K$ to solve Game of 24. A large number of methods succeeded the paradigm \citep{NEURIPS2023_81fde95c, wu-etal-2024-synchronous, mo2024tree, zhu2024deductivebeamsearchdecoding}, showing the generalizability of beam search to diverse reasoning tasks and evaluation metrics. 