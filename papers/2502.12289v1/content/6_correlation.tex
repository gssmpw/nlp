\section{Analysis on reported meta-evaluation}
\label{sec:transfer}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/correlation.pdf}
    \caption{Meta-evaluation scores of the same evaluator model in two different criteria. (a) Results from REVEAL \citet{jacovi-etal-2024-chain} show that validity and groundedness are not transferrable, and cross-encoders fall behind critic models in evaluating validity. (b) PRMBench \citet{song2025prmbenchfinegrainedchallengingbenchmark} shows that validity and coherence evaluation are highly transferable. (c) \citet{zhang2025lessonsdevelopingprocessreward} shows that utility-based PRMs often fail to evaluate validity, but the two criteria can synergize when jointly considered.}
    \label{fig:correlation}
\end{figure*}

Based on the taxonomy provided in Section \ref{sec:taxonomy}, we observe that a \textit{single} evaluator model, with identical implementation design, model, and training data/prompt, is often used to evaluate different metrics. For instance, a single cross-encoder model is used to evaluate the groundedness and validity in \citet{golovneva2023pathfinderguidedsearchmultistep, zhu2024deductivebeamsearchdecoding}.

However, such \textit{transferability}, \textit{i.e.} an evaluator tuned for one metric being able to generalize to another, is not trivial because the criteria definitions are independent. Transferability is important in terms of designing metrics and meta-evaluation benchmarks, as (\textit{metric}) using the same model for evaluating non-transferable criteria will lead to sub-optimal performance, and (\textit{meta-evaluation benchmark}) annotating non-transferable errors as same categories might disrupt the meta-evaluation results. Note that high correlation does not imply that the criteria are \textit{duplicates}, as their definition significantly differ (Section \ref{sec:taxonomy}).

We investigate if there is evidence of transferability between criteria proposed in Section \ref{sec:taxonomy} by analyzing reported empirical results in three meta-evaluation settings, namely REVEAL \citep{jacovi-etal-2024-chain}, PRMBench \citep{song2025prmbenchfinegrainedchallengingbenchmark}, and ProcessBench + BoN decoding \citep{zhang2025lessonsdevelopingprocessreward}.

\subsection{Validity-Groundedness}
\label{sec:transfer-vg}

REVEAL \citep{jacovi-etal-2024-chain} is a meta-evaluation benchmark based on commonsense reasoning. It evaluates a cross-encoder model \citep{honovich-etal-2022-true} and various critic models (LLM-as-a-judge) \citep{brown2020languagemodelsfewshotlearners, wei2022finetunedlanguagemodelszeroshot, anil2023palm2technicalreport} upon reasoning traces sampled from four commonsense reasoning benchmarks. The results (Figure \ref{fig:correlation}(a)) show that the correlation between the two scores is weak, indicating that using a single model for both methods can result in suboptimal evaluation performance. 

Notably, the cross-encoder model (Figure \ref{fig:correlation}(a) \textcolor{medorange}{$\bullet$}\color{black}) achieves significant accuracy in groundedness but falls over 10p behind critic models in evaluating validity. This result indicates that it might not be feasible to employ off-the-shelf cross-encoders trained on NLI tasks for validity judgments, as opposed to existing works \citep{DBLP:conf/iclr/GolovnevaCPCZFC23, prasad-etal-2023-receval}.

\subsection{Validity-Coherence}

% \note{PRMEval performance of validity models. Lemma-prm800k and MATHMinos and ReasonEval -> reasonably high -> not much difference. PRM800k has instances of ungrounded error? need to find out}

PRMBench \citep{song2025prmbenchfinegrainedchallengingbenchmark} defines nine fine-grained error classes in the PRM800k dataset \citep{DBLP:conf/iclr/LightmanKBEBLLS24} and annotates \~150 samples per class for meta-evaluation. Among the nine classes, we display the correlation between Step Consistency (SC; \textit{Are the two steps contradictory?}) representing the validity error and Prerequisite Sensitivity (PS; \textit{Are any critical premises, assumptions, or necessary conditions absent?}) representing coherence.
% The results (Figure \ref{fig:correlation}(b)) show that the correlation is high in diverse PRMs and critic models, showing that the evaluation abilities of two metrics are very likely to be transferrable. 
The results (Figure \ref{fig:correlation}(b)) show that the correlation is high in diverse PRMs and critic models, indicating that the abilities to evaluate validity and coherence are very likely transferable.


\subsection{Validity-Utility}
\label{sec:transfer-vu}

Recent works on process reward models do not explicitly disambiguate between validity-based and utility-based PRMs. Consequently, training the model with one data (\textit{e.g.} validity) and evaluating with another (utility) has settled as a common experimental practice \citep{DBLP:conf/iclr/LightmanKBEBLLS24, ma2023letsrewardstepstep, zheng2024processbenchidentifyingprocesserrors, song2025prmbenchfinegrainedchallengingbenchmark}. % For instance, Math-Shepherd \citep{wang-etal-2024-math} allegedly uses two definitions for the term \textit{quality}, \textit{""} and \textit{""}.

In this setting, we analyze results on ProcessBench \citep{zheng2024processbenchidentifyingprocesserrors} and Best-of-N decoding results, reported by \citet{zhang2025lessonsdevelopingprocessreward}. ProcessBench is a meta-evaluation benchmark constructed from human annotations on validity. In contrast, Best-of-N decoding tests the ability of an evaluator to select the reasoning trace with the highest utility (chance of answer correctness) out of $N$ samples.

In Figure \ref{fig:correlation}(c), the correlation between two criteria is weaker than validity-coherence ($R^2=0.69$). Furthermore, \citet{zhang2025lessonsdevelopingprocessreward}'s analyses show that if only comparing validity and utility PRMs trained on the same base model (Qwen-2.5-MAth-7B \citep{yang2024qwen25mathtechnicalreportmathematical}, models trained on utility\footnote{Figure \ref{fig:correlation}(c) Math-Shepherd, Qwen-MCh, Qwen-MCs} achieve significantly lower performance in validity evaluation than validity PRMs\footnote{Figure \ref{fig:correlation}(c) PRM800K, Qwen-Critic}. They show that filtering the training samples with high validity \textit{and} utility scores leads to powerful PRM\footnote{Figure \ref{fig:correlation}(c) Qwen-MCh$\cap$Critic}. These results indicate that validity and utility are complementary, and considering both yields more robust evaluation results than using single criterion.