\section{Future directions}
\label{sec:future-directions}

Despite  rapid progress on step-by-step reasoning evaluation, crucial questions remain to be solved.

\textbf{Resources for evaluating reasoning in challenging real-world reasoning tasks.} Datasets for training and evaluating neural reasoning trace evaluators are generally restrained to tasks that are either overly simple (\textit{e.g.} popular MHQA datasets) or restricted in domains (\textit{e.g.} olympiad-level math reasoning). However, there are many real-world reasoning tasks such as complex science questions \citep{rein2024gpqa}, repository-level coding \citep{zhang-etal-2023-repocoder}, medicine \citep{savage2024diagnostic}, law \citep{holzenberger-van-durme-2021-factoring, kimyeeun-etal-2024-developing}, and finance \citep{li-etal-2024-alphafin}. The reasoning required for these tasks is complex,  requiring both groundedness to retrieved documents and expert-level mathematic/logical skills. Developing step-by-step reasoning evaluators and meta-evaluation benchmarks for such expert-level tasks will significantly enhance the generalizability and real-world applicability of LLM reasoning.

\textbf{Evaluation of long, complex reasoning traces.} Due to the recent attention to OpenAI \texttt{o1} \citep{openai2024openaio1card}, numerous models have been trained to generate a long reasoning trace that includes hesitation, backtracking, and lookahead assumptions \citep{openai2024openaio1card, zhao2024marcoo1openreasoningmodels, deepseekai2025deepseekr1incentivizingreasoningcapability, muennighoff2025s1simpletesttimescaling}. However, existing step-by-step evaluation reasoning metrics are not designed to accommodate these complex traces. For instance, incorrect steps followed by correct self-correction (\textit{e.g.} \textit{Wait, this reasoning is not correct.}) will get low validity and utility scores because the step will lead to a contradiction and is semantically irrelevant to the final answer. While the necessity of trace evaluation in obtaining stronger long-trace models is under debate \citep{deepseekai2025deepseekr1incentivizingreasoningcapability}, the effort to develop evaluation resources for such trace will lead to a better understanding of long-trace models' behaviors and further improvement in reasoning performance.

% \textbf{Advanced methods for pinpointing referred steps.} NLI-based validity evaluation and coherence evaluation require determining the previous steps that the current step refers to. However, finding such steps is not a trivial task. ROSCOE \citep{DBLP:conf/iclr/GolovnevaCPCZFC23} uses the minimum NLI score of all (previous step, current step) combinations which ignores dependency between multiple reasoning steps, and coherence metrics \citep{NEURIPS2023_72393bd4, tyen-etal-2024-llms} make the reasoner LLM annotate the previous steps the current step which is easily error-prone. Plausible but unexplored approaches for finding the referred steps include applying \textit{uncertainty-based methods} \citep{chen-etal-2023-rev, wu-etal-2024-synchronous} to measure step-to-step groundedness or applying \textit{discourse parsing} that explicitly annotate the logical dependencies between steps as graphs.

\textbf{Symbol-grounded evaluation of reasoning traces.} Reasoning tasks often have a symbolic ground truth solution. For instance, deductive reasoning tasks can be represented with formal logic, and arithmetic problems can be expressed as a series of equations or symbolic theorems. These solutions provide precise, formal ways to define metrics, including validity and utility (progress). However, not much work has been done to exploit the parallel between reasoning traces and the underlying symbolic solution. While several rule-based approaches parse reasoning traces for evaluation in relatively easier reasoning tasks \citep{PrOntoQA, nguyen-etal-2024-direct, li-etal-2023-making}, no attempts have been made to extend this paradigm to evaluate reasoning traces for first-order logic reasoning \citep{han-etal-2024-folio, han-etal-2024-p} and math problems formalized using theorem provers, \textit{e.g.} Lean \citep{yang2023leandojo, gao2024heraldnaturallanguageannotated}.

\textbf{Objective metrics for coherence evaluation.} LLMs often omit trivial inference steps in their reasoning \citep{PrOntoQA}, but there is no consensus about to what extent can the step be omitted (Section \ref{sec:coherence}). This widespread ambiguity led to a deprivation of objective coherence evaluation metrics. A large-scale annotation of omittable and non-omittable steps will facilitate the development of precise coherence evaluators and comprehensive meta-evaluation based on human perception of coherence.