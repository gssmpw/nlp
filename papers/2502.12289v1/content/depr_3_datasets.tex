\section{Tasks and datasets}
\label{sec:task}

Step-by-step reasoning can be employed for solving tasks that involve complex reasoning and planning. In this section, we list common tasks and benchmarks where step-by-step reasoning is often applied. 

It is repeatedly reported that step-by-step reasoning does not always lead to performance gain. \citet{sprague2024cotcotchainofthoughthelps} showed that step-by-step reasoning improves logical and mathematical reasoning tasks while achieving only marginal gain in factual and commonsense reasoning. Furthermore, \citet{liu2024mindstepbystep} brings tasks where verbalization or long thinking hurts human performance from cognitive science, and shows that Chain-of-thoughts prompting hurts the performance of LLMs (and large vision-language models) in these tasks.

\subsection{Logical reasoning}

Logical reasoning refers to tasks where (1) the query can be translated into logical representations (\textit{e.g.} First-order logic, logic programming), and (2) an automatic theorem prover/solver can deduce the correct answer from the representations. These tasks are often synthetic and have low lexical diversity, but in return, they can have more data samples and long reasoning traces.

\textbf{Deductive reasoning} \citep{tafjord-etal-2021-proofwriter, tian-etal-2021-diagnosing, PrOntoQA, han-etal-2024-folio} mainly focuses on natural deduction, where repeatedly applying the provided rules to facts will reach the correct answer. \textbf{Relational reasoning} \citep{sinha-etal-2019-clutrr, stepGame2022shi} is a variant of deductive reasoning, where the inference rules are not explicitly provided but derived from common sense (e.g. family relations, spatial relations). Finally, the goal of \textbf{constraint-based reasoning} \citep{zhong2021arlsatinvestigatinganalyticalreasoning, tyagi-etal-2024-step} is to deduce a solution that suffices all initial facts and conditions given in the query, similar to \textit{grid puzzles}.

% Rather than generating a natural language-based reasoning trace, \textit{parse-and-execute} methods \citep{lyu-etal-2023-faithful, pan-etal-2023-logic, olausson-etal-2023-linc} first translate the provided facts into symbolic representations and apply an automated solver to deduce the solution. While some works claim that parse-and-execute is a type of step-by-step reasoning \citep{lyu-etal-2023-faithful}, we only consider methods that do not delegate all inference steps to an external solver.

\subsection{Math reasoning}

Math reasoning benchmarks consist of another large body of reasoning tasks suitable for step-by-step reasoning. 
% While whether the transformer architecture can truly perform math operations such as multiplication is still under debate \citep{NEURIPS2023_deb3c281, DBLP:journals/corr/abs-2307-03381}, it is generally agreed that both (1) step-by-step reasoning is highly beneficial in math reasoning \citep{NEURIPS2022_9d560961, sprague2024cotcotchainofthoughthelps} and (2) the term \textit{math reasoning ability} of an LLM generally assumes that step-by-step reasoning is used \citep{}.

\textbf{Arithmetic reasoning}, where the model has to predict the correct answer from arithmetic word problems, is the prominent variant of math reasoning. Popular benchmarks include MathQA \citep{amini-etal-2019-mathqa} and GSM8k \citep{cobbe2021trainingverifierssolvemath}, which provide long, diverse natural language queries. Game of 24 \citep{NEURIPS2023_271db992} and Mathador \citep{kurtic-etal-2024-mathador} ask to combine given numbers and arithmetic operations to generate the target number, requiring thorough exploration and backtracking in the exponential solution space.

Math reasoning extends to more complex and diverse math problems including, but not limited to, competition-level problem solving \citep{NEURIPSDnB2021_be83ab3e, wang2024measuring}, geometry \citep{chen-etal-2021-geoqa, zhang-etal-2024-geoeval}, mathematical theorem proving \citep{yang2023leandojo, lama-etal-2024-benchmarking}, and expert-level mathematic problems including number theory, algebraic geometry, and category theory \citep{glazer2024frontiermathbenchmarkevaluatingadvanced}. Results from these works collectively imply that current state-of-the-art LLMs are not fully capable of solving complex math problems yet, leaving a promising research direction for step-by-step reasoning.

\subsection{Factual/Commonsense reasoning}

% \note{TODO: Retrieval-based Multi-hop QA (HotPotQA)}

Factual and commonsense reasoning tasks deal with real-world facts and norms.

Factual multi-hop question answering (QA) datasets require reasoning between multiple entities and their relations \citep{joshi-etal-2017-triviaqa, yang-etal-2018-hotpotqa, trivedi-etal-2022-musique}. 
As these tasks require very specific knowledge about an entity (\textit{e.g.} birthplace, location, ...), they are often combined with the \textit{retrieval-augmented generation} (RAG) paradigm \citep{NEURIPS2020_6b493230}, which add retrieved documents containing the necessary facts to answer the question to the query. Cofca \citep{wu2024cofcastepwisecounterfactualmultihop} manipulates the retrieved documents to be counterfactual, \textit{i.e.} irrelevant or contradictory to the real-world facts, testing the model's ability to reason on in-context information instead of its parametric knowledge.

On the other hand, commonsense QA tasks require general world knowledge instead of factual knowledge about specific entities \citep{talmor-etal-2019-commonsenseqa, }. StrategyQA \citep{geva-etal-2021-aristotle} combines factual information (\textit{When did Aristotle live?}) with commonsense inference (\textit{Did laptops exist in B.C. 300?}), requiring complex planning between factual information and commonsense reasoning. ARC (\citet{clark2018thinksolvedquestionanswering}; and its derivation EntailmentBank \citep{dalvi-etal-2021-explaining}) implements multi-step commonsense reasoning with scientific facts. ENWN \citep{sprague-etal-2022-natural}, BoardgameQA \citep{NEURIPS2023_7adce80e}, and MuSR \citep{DBLP:conf/iclr/SpragueYBCD24} test the model's ability of implicit commonsense reasoning by deliberately removing the relevant commonsense knowledge from the query. Crossword puzzles \citep{NEURIPS2023_271db992} recently gained attention, as they require a rigorous search for solutions along with commonsense reasoning about word senses.

