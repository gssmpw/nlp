
\section{Multi-hop question answering}
\label{sec:mhqa}

% \input{tables/metrics} -> appendix

\subsection{Task}

This section focuses on the metrics proposed for evaluating the reasoning traces for multi-hop question answering (MHQA) tasks. MHQA is often divided into two subcategories, \textbf{factual reasoning} that combines multiple facts to find the correct answer \citep{yang-etal-2018-hotpotqa, talmor-berant-2018-web, kwiatkowski-etal-2019-natural} and \textbf{commonsense reasoning} that further requires commonsense knowledge to complete the inference \citep{clark2018thinksolvedquestionanswering, talmor-etal-2019-commonsenseqa, geva-etal-2021-aristotle, trivedi-etal-2022-musique}. Further details on reasoning tasks and benchmarks are presented in Appendix \ref{sec:appendix-task-mhqa}.
% song2024measuringenhancingtrustworthinessllms

\subsection{Implementation}

\subsubsection{Groundedness}

\textbf{Cross-encoders.} \hspace{0.1cm} Evaluating groundedness can be seen as checking if the provided query \textit{entails} the trace, \textit{i.e.} the natural language inference task (NLI; \citet{bowman-etal-2015-large}). Consequently, ROSCOE-LI \citep{DBLP:conf/iclr/GolovnevaCPCZFC23} AlignScore \citep{zha-etal-2023-alignscore}, and ReCEval \citep{prasad-etal-2023-receval} use an off-the-shelf cross-encoder trained from NLI and similar tasks, \textit{e.g.} fact verification, question answering, and paraphrase identification, for validity evaluation.

\textbf{Critic LLMs.} \hspace{0.1cm} \citet{jacovi-etal-2024-chain, wu-etal-2024-mitigating, niu2024ragtruthhallucinationcorpusdeveloping} showed that prompting instruction-tuned LLMs can effectively detect groundedness errors in diverse reasoning tasks including factual/commonsense MHQA datasets. 
Furthermore, HalluMeasure \citep{akbar-etal-2024-hallumeasure} leverages LLMs for more fine-grained evaluation, \textit{i.e.} (1) decompose reasoning steps into atomic claims and (2) evaluate the groundedness of each claim based on the retrieved documents, inspired from \citet{min-etal-2023-factscore}.

% ROSCOE \citep{DBLP:conf/iclr/GolovnevaCPCZFC23}'s semantic alignment score (ROSCOE-SA) relies on the cosine similarity between tokens' latent representations (hidden states), following the prior work CTC \citep{deng-etal-2021-compression}.

\textbf{Uncertainty.} \hspace{0.1cm} Groundedness can also be measured as the \textbf{uncertainty} of the model, where higher uncertainty indicates low groundedness \citep{xiao-wang-2021-hallucination, zhang-etal-2023-enhancing-uncertainty}. \citet{qiu2024entropybaseddecodingretrievalaugmentedlarge} and \citet{wu-etal-2024-synchronous} use \textit{token probability entropy}, defined as $\Sigma p(t)\textrm{log}(p(t))$ per each token $t$. \citet{farquhar2024detecting} and \citet{kossen2024semanticentropyprobesrobust} extend such an approach by clustering semantically similar answers and calculating the entropy with respect to the clusters, resulting in more robust groundedness evaluation in trade of additional computation for clustering.

% SynCheck \citep{wu-etal-2024-synchronous} ensembles four metrics to evaluate the groundedness of each step, namely AlignScore \citep{zha-etal-2023-alignscore}, token probability entropy, lowest token probability, and the token probability difference with and without the retrieved document. However, ablation studies show that the gains from uncertainty-based metrics are not as significant as AlignScore's contribution.

% \textbf{Knowledge graphs.} \hspace{0.1cm} Finally, knowledge graphs (KGs) provide straightforward methods for groundedness evaluation. KGs store a large number of entities and their relation as a symbolic triple (\textit{e.g.} \texttt{<Seoul, isCapitalOf, South Korea>}), which allows proposition-level evaluation. \citet{nguyen-etal-2024-direct}, OCEAN \citep{wu2024oceanofflinechainofthoughtevaluation}, and F\textsuperscript{2}-Verification \citep{wang-etal-2024-boosting-language} extracts triple stated in the reasoning trace using LLMs and use external knowledge graphs to verify if the steps are factually grounded.

\subsubsection{Validity}

\textbf{Cross-encoders.} \hspace{0.1cm} NLI-based models can be used to check if a step can be entailed by \textit{previous steps}, similar to groundedness \citep{DBLP:conf/iclr/GolovnevaCPCZFC23, prasad-etal-2023-receval, zhu2024deductivebeamsearchdecoding}. These works evaluate the probability of \textit{contradiction} label between the current step and its previous steps, and take the maximum of the score to identify if there is any contradiction. However, solely depending on cross-encoders might lead to low error detection accuracy; further analysis is provided in Section \ref{sec:transfer-vg}.

% However, the transferability from NLI to validity is not trivial because (1) validity should be measured from the combination of multiple premises while NLI cross-encoders encode a single premise for a hypothesis \citep{zhu2024deductivebeamsearchdecoding}, and (2) cross-encoders cannot detect errors \textit{within the hypothesis} as it is not dealt in the underlying NLI datasets \citep{bowman-etal-2015-large}. \citet{jacovi-etal-2024-chain} shows that a cross-encoder model can detect groundedness errors almost as accurately as modern LLMs, while the detection performance drops by 30p in detecting validity errors.

% % While LLMs are known to generally suffer in detecting and correcting errors of the entire reasoning trace \citep{DBLP:conf/iclr/0009CMZYSZ24, jacovi-etal-2024-chain, zhang-etal-2024-rationales}, they are often used to evaluate the validity of a single step. 
% \textbf{LLM-based classifiers.} \hspace{0.1cm}  \citet{DBLP:conf/iclr/LightmanKBEBLLS24, zhang2025lessonsdevelopingprocessreward} trains an LLM that predicts the probability of a step being correct as a classification task. While this approach is computationally heavy compared to cross-encoders, fine-tuned LLMs often outperform cross-encoders in benchmarks designed for validity evaluation \citep{jacovi-etal-2024-chain}.

% \textbf{Prompted LLMs.} \hspace{0.1cm} Another popular method is directly prompting LLMs without fine-tuning a classifier head. In this setting, LLMs are prompted to determine the validity of a given step based on its previous steps \citep{jacovi-etal-2024-chain, tyen-etal-2024-llms} or finding the first erroneous step (if exists) given the entire reasoning trace \citep{tyen-etal-2024-llms, zheng2024processbenchidentifyingprocesserrors}. While \citet{tyen-etal-2024-llms} reported poor performance of LLMs pinpointing the error location, results on ProcessBench show that recently developed reasoning-focused models such as o1 \citep{openai2024openaio1card} and QwQ \citep{zhang2025lessonsdevelopingprocessreward} show remarkable performance.

% Tyen(2024) Direct mistake prompting

% \subsection{Coherence}
% \label{sec:coherence}

% \textbf{Symbolic solutions.} \hspace{0.1cm} Coherence can be clearly defined in \textit{symbolic} reasoning tasks, where the dependency between steps can be symbolically defined. \citep{PrOntoQA} defined steps that require applying two inference rules as valid but incoherent (\textit{broadly valid}). However, as they used a simple synthetic dataset that only includes \textit{Modus ponens}, observed instances of broadly valid steps can be well seen as coherent in common sense. \citet{nguyen-etal-2024-direct}, where a reasoning trace corresponds to a directed path in knowledge graphs (KGs), defines a coherent ERU as a directed KG edge where its source node was already introduced as a target node.

% \textbf{LLMs.} \hspace{0.1cm} Despite that coherence was well-studied in empirical analyses of Chain-of-thought prompting \citep{wang-etal-2023-towards, lyu-etal-2023-faithful, DBLP:conf/iclr/0001Z0S23, PrOntoQA} there are relatively few existing works that explicitly distinguish coherence from validity in \textit{evaluation} \citep{nguyen-etal-2024-direct, song2025prmbenchfinegrainedchallengingbenchmark}. PRMBench \citep{song2025prmbenchfinegrainedchallengingbenchmark} leverages LLM-based classifiers and prompted LLMs to check if the previous steps fulfill the premises required by the current step (\textit{Prerequisite sensitivity}).

\subsubsection{Utility}

\textbf{Symbolic solutions.} \hspace{0.1cm} For multi-hop QA tasks with underlying \textit{knowledge graph}, the notion of \textit{progress} can apply. \citet{nguyen-etal-2024-direct} proposes a method for measuring utility in factual MHQA by comparing the reasoning trace to all KG paths between the queried entity and the answer entity. A step that introduces an entity outside the gold paths is considered useless, as it does not make progress to the correct answer.

\textbf{Conditional $\mathcal{V}$-information.} REV \citep{chen-etal-2023-rev} and ReCEval \citep{prasad-etal-2023-receval} attempt to quantify how much the reasoning trace contributes to predicting the correct answer. While REV only compares the probabilities of guessing the correct answer with or without the entire trace, ReCEval evaluates the CVI for each step, offering fine-grained, reference-free evaluation.

% \textbf{Tree Search.} \hspace{0.1cm} The most popular variant of utility builds over \textbf{Monte Carlo Tree Search (MCTS)}. This variant views utility as \textit{value functions} in reinforcement learning, where \textit{states} (partially generated traces) have higher values if they are likely to get higher \textit{rewards} at the end (correct answer). In MCTS, given a previously generated trace, (1) multiple possible next ERUs are sampled, (2) their \textit{continuation (rollout)} is generated and evaluated (rewarded) based on the final answer, and (3) the utility of each sampled ERU is estimated by back-propagating the reward. As a result, each ERU will be assigned a utility score that denotes how likely it will lead to the correct answer \citep{hao-etal-2023-reasoning, xie2024montecarlotreesearch, wang-etal-2024-math, zhang2024chainpreferenceoptimizationimproving}.

% % MCTS-based methods can be further distinguished by how the correct answer is numerically rewarded. The most traditional way is to define pointwise rewards by assigning  \citep{xie2024montecarlotreesearch}. \citet{hao-etal-2023-reasoning} defines task-specific conditions for evaluating the final answer and assigns rewards according to how many conditions are met upon completion. Math-Shepherd \citep{wang-etal-2024-math} defines two action value functions for each step; \textit{hard estimation} sets the value to 1 if there is a completion that reaches a correct answer and 0 otherwise, while \textit{soft estimation} sets the value to the portion of correct reasoning chains.

% However, as the entire MCTS process is costly due to extensive sampling and rollouts, it is not often practical to obtain a large quantity of useful and useless steps for training an evaluator model. As a more efficient alternative, Step-controlled DPO \citep{lu2024stepcontrolleddpoleveragingstepwise} starts from a correct reasoning trace and manipulates each step so that their rollouts lead to a wrong answer. On the other hand, OmegaPRM \citep{luo2024improvemathematicalreasoninglanguage} applies binary search to identify the first useless (\textit{i.e.} cannot reach the correct answer by rollout) step in the incorrect reasoning trace to obtain a balanced set of useful and useless steps.

% \textbf{Information theory.} \hspace{0.1cm} Some works define the utility as \textbf{conditional $\mathcal{V}$-information (CVI)} \citep{hewitt-etal-2021-conditional}, inspired by information theory. CVI can be informally defined as the amount of information a reasoning trace $t$ adds to the model. Formally, given a model $g$ trained to predict the answer when the rationale is given (calculates $g(a|q,t)$) and $g'$ trained to predict the answer $a$ only with the query $q$ (calculates $g'(a|q)$), the CVI is calculated by
% \[
% CVI(t\rightarrow a|q) = -\textrm{log}g'(a|q) + -\textrm{log}g(a|q,t)
% \]
% , which is maximized when predicting the answer without the trace is hard (smaller $g'(a|q)$) but it becomes easier with the trace (larger $g(a|q,t)$). REV \citep{chen-etal-2023-rev} uses CVI for trace-level evaluation, which is extended to step-level evaluation by ReCEval \citep{prasad-etal-2023-receval}.

% \textbf{Prompting LLMs.} \hspace{0.1cm} Additionally, LLMs can be used as a generative utility evaluator. To solve problems that have large search space such as Game of 24\footnote{In Game of 24, one should make 24 using four integers and basic arithmetic operations, \textit{e.g.} using 4, 5, 6, 7, $(5+7-6) \times 4 = 24$.}, Tree-of-thoughts \citep{NEURIPS2023_271db992} leverage LLMs to predict the utility of a given reasoning step by asking questions like \textit{"Evaluate if the intermediate numbers can reach 24"}. Similar directions include CriticBench \citep{lin2024criticbenchbenchmarkingllmscritiquecorrect} which tests LLMs to predict the answer correctness given the entire trace.

% PrOntoQA \citep{PrOntoQA} defines a useful step as one that appears in the gold proof tree in deductive reasoning. In arithmetic reasoning tasks, DIVERSE \citep{li-etal-2023-making} extracts the numbers in the reasoning trace and checks if they appear in the gold solution.