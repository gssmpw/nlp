\section{Evaluation}
\label{sec:evaluation}

We perform an experimental evaluation using two popular large language models, namely OpenAI o3-mini and DeepSeek R1. We answer the following research questions:

\textbf{RQ1 (English to Alloy)}: What is the performance of large language models in synthesizing multiple Alloy formulas that represent the input property that is given in natural language?

\textbf{RQ2 (Alloy to Alloy)}: What is the performance of large language models in synthesizing multiple Alloy formulas that are equivalent to the input Alloy formula that describes the desired property that is also given in natural language?

\textbf{RQ3 (Sketch to Alloy)}: What is the performance of large language models in completing the sketch of a property that is characterized by the input sketch and described in natural language?

\subsection{RQ1 (English to Alloy)}

\input{tables/results-eng-2-alloy}

Table~\ref{tab:EnglishToAlloy-results} presents the results for each of the two LLMs. For each synthesis task, both LLMs find at least one valid solution, i.e., create an Alloy formula that is equivalent to the ground truth.

For OpenAI o3-mini, the number of correct formulas varies between 1 (for \CodeIn{Reflexive} property) and 16 (for \CodeIn{Antisymmetric} property); for 6 (out of 11) properties, it creates at least 10 correct formulas. For DeepSeek R1, the number of correct formulas varies between 8 (for \CodeIn{Cycle} property) and 19 (for \CodeIn{Antisymmetric} and \CodeIn{Transitive} properties); for 10 (out of 11) properties, it creates at least 10 correct formulas.

Overall, both LLMs perform well at creating Alloy formulas from their descriptions in natural language.

% The results in table \ref{tab_EnglishToAlloy} show that both o3-mini and R3 can give at least 1 correct answer. For the Cycle problem, both models gave less than 10 correct answers. For the Antisymmetric problem, they both gave more than 15 correct answers. 

% OpenAI o3-mini generated 18 invalid solutions for the Reflexive problem and only got 1 answer. o3-mini received the most correct answers to the Antisymmetric question - 19 correct answers.

% DeepSeek R1, for each question, except Cycle, it generated 10 or more correct answers. R1 generated the most (19) correct answers on both the Antisymmetric and Transitive.

% For this task, DeepSeek R1 has a better performance. DeepSeek R1 has more correct answers than OpenAI o3-mini on all 18 problems, except that o3-mini has more Functional problems, and both are the same on Functional problems.

\subsection{RQ2 (Alloy to Alloy)}

\input{tables/results-alloy-2-alloy}

Table~\ref{tab:AlloyToAlloy-results} presents the results for each of the two LLMs. For each synthesis task, both LLMs find at least two valid solutions, i.e., create an Alloy formula that is equivalent to the given input formula and its description in natural language.

For OpenAI o3-mini, the number of correct formulas varies between 2 (for \CodeIn{Circular} property) and 18 (for \CodeIn{Antisymmetric} property); for 9 (out of 11) properties, it creates at least 10 correct formulas. For DeepSeek R1, the number of correct formulas varies between 7 (for \CodeIn{Functional} property) and 18 (for \CodeIn{Symmetric} property); for 8 (out of 11) properties, it creates at least 10 correct formulas.

Overall, both LLMs perform well at creating Alloy formulas that are equivalent to given Alloy formulas and their descriptions in natural language.

\subsection{RQ3 (Sketch to Alloy)}

\input{tables/results-sketching}

Table~\ref{tab:sketching-results} presents the results for each of the two LLMs. Each LLM successfully completes all but 1 sketching task.

OpenAI o3-mini successfully completes sketches of all properties except \CodeIn{Circular}. The answer it gives for the sketching task for \CodeIn{Circular} has a syntax error.  We create a new query to inform OpenAI o3-mini of the syntax error and ask it to try again.  OpenAI o3-mini successfully completes the input sketch this time (second attempt).

DeepSeek R1 successfully completes sketches of all properties except \CodeIn{Function}. The answer it gives for the sketching task for \CodeIn{Function} has a syntax error.  We create a new query to inform DeepSeek R1 of the syntax error and ask it to try again.  DeepSeek R1 successfully completes the input sketch this time (second attempt).

Overall, both LLMs perform well at completing Alloy sketches.

\subsection{Discussion}

As the experimental results show, both the LLMs successfully create the desired Alloy formulas, which is indeed notable. We also note that the quality of solutions is high; an expert Alloy user can be expected to create solutions of such quality.  To illustrate, consider the following 11 equivalent formulas created by DeepSeek R1 as its answer to the "English to Alloy" task for the property \CodeIn{DAG}:

\begin{CodeOut}
\begin{verbatim}
 1. no ^link & iden
 2. all n: Node | n not in n.^link
 3. not some n: Node | n in n.^link
 4. all n: Node | no n.^link & n
 5. ^link = ^link - iden
 6. no iden & ^link
 7. all n: Node | (n.^link & n) = none
 8. all n: Node | #(n.^link & n) = 0
 9. all n: Node | lone (n.^link & n) => no (n.^link & n)
10. all n: Node | no n & n.^link
11. ^link in (^link - iden)
\end{verbatim}
\end{CodeOut}

Some of these formulas can be considered as relatively simple re-writes of each other, e.g., \CodeIn{\#}1 and \CodeIn{\#}7 that are equivalent because of the logical equivalence $a\wedge b~\equiv~b\wedge a$, or \CodeIn{\#}2 and \CodeIn{\#}3 that are equivalent because of the logical equivalence $\forall s\in S~|~\neg f(s)~\equiv~\neg\exists s\in S~|~f(s)$.  However, some others, e.g., ``\CodeIn{11. \^{}link in (\^{}link - iden)}'' (where \CodeIn{ident} is the built-in identity function), requires in-depth knowledge of Alloy to reason about its correctness.  Moreover, ``\CodeIn{9. all n: Node | lone (n.\^{}link \& n) => no (n.\^{}link \& n)}'' -- which states that for any node \CodeIn{n}, if the intersection of the relational image of \CodeIn{n} under the transitive closure of \CodeIn{link}, and the singleton set that contains just \CodeIn{n} has at most one element, then that intersection is empty -- is quite an unusual and rather interesting formulation of the DAG property.

It is also worth noting that the presence of the reference Alloy formula in the query (as in the "Alloy to Alloy" tasks) does not necessarily benefit the LLM in solving the synthesis problem for generating multiple equivalent solutions.  For example, DeepSeek R1 produces more correct formulas for 5 properties, fewer correct formulas for 5 properties, and an equal number of correct formulas for 1 property when synthesizing "English to Alloy" formulas than when synthesizing "Alloy to Alloy" formulas.  This is because enumerating equivalent but unique solutions requires a deeper level of understanding and having one solution available at the start does not necessarily help a lot.
