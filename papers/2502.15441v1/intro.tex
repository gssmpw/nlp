\section{Introduction}
\label{sec:intro}

Declarative languages have long offered immense value in automating
design and implementation of safe and reliable software
systems~\cite{Nugues2006,Ceri1989,Leavens1998,JacksonAlloy2002,Jones1990,Guttag2012}.
However, their true value has not yet been realized.  A key issue is
that their use often necessitates the need for developers to learn
unfamiliar notations that have a very different semantics from
commonly used imperative languages that they use daily to write their
code in.

A major breakthrough in how we write code has occurred in the last few
years with the coming-of-age of large language models
(LLMs)~\cite{ZhaoETALLLMSurvey2024}, such as
ChatGPT\footnote{\url{https://chatgpt.com/}} and
DeepSeek\footnote{\url{https://chat.deepseek.com/}}, which have
revolutionized software development and are now in ubiquitous use.
While these LLMs have massively disrupted traditional software
development, it is yet unclear how much utility they offer in
developing declarative specifications that are often written in
languages that have a very different semantic basis, offer a higher
level of abstraction, and have much less written code available than
commonly used programming languages.

Our focus in this paper is Alloy~\cite{JacksonAlloy2002}, a well-known
declarative language that is based on relational first-order logic and
includes transitive closure.  Specifications written in Alloy can be
analyzed using the Alloy toolset's SAT-based backend that performs
fully automatic bounded exhaustive analysis.  Specifically, the Alloy
analyzer translates formulas in Alloy to propositional logic with
respect to a given bound on the size of the universe of discourse, and
employs state-of-the-art SAT solvers to solve them.  The Alloy
analyzer offers two kinds of analyses.  One, the users can ask it to
create models -- that are termed \Intro{instances} -- of a desired
property, so that the constraint solving problem encodes the given
property and any solution found forms an Alloy instance.  Two, the
users can ask the analyzer to search for a counterexample to a
property, so that the constraint solving problem encodes the negation
to the given property and any solution found serves as a
counterexample.

The Alloy toolset has been used in many applications over the last two
decades, including software design~\cite{CD2Alloy,CDDiff}, test case
generation~\cite{MarinovKhurshidASE2001}, deep analysis of
code~\cite{JacksonVaziriISSTA2000}, repair of faulty
programs~\cite{GopinathETALTACAS2011}, specification-driven
execution~\cite{Samimi2010,Zaeem2010,Squander},
security~\cite{Maldonado-Lopez2014,Margrave}, and
networking~\cite{OpenflowAlloy}.  While Alloy has been deployed in
many contexts, its use remains largely confined to academia.  In our
experience of using and teaching Alloy, a key issue that prevents its
much wider adoption is the learning curve that it poses to developers
who work with imperative languages.

Our insight is that LLMs have the potential to be of immense use in
writing declarative specifications, thereby substantially reducing the
cost of using the Alloy toolset, and more generally, other lightweight
formal methods.  This paper presents a controlled experiment on using
LLMs for creating Alloy specifications.  Our use of LLMs is
three-fold.  One, we employ LLMs to write complete Alloy formulas from
given natural language descriptions (in English).  Two, we employ LLMs
to create alternative but equivalent formulas in Alloy with respect to
given Alloy formulas.  Three, we employ LLMs to complete
sketches~\cite{SolarLazemaPhD2008,WangETALABZ2018ASketch} of Alloy
formulas and populate the holes in the sketches by synthesizing Alloy
expressions and operators so that the completed formulas accurately
represent the desired properties (that are given in natural language).

Alloy's design focuses on analyzability supported by its fully automatic
SAT-based backend makes Alloy an ideal specification language to be
supported by LLMs.  Their outputs can immediately be validated by the
Alloy backend.  For example, an output formula that has no instance
(i.e., is unsatisfiable) is (most likely) invalid.  As another
example, when the input query to the LLM has a reference formula, the
LLM's output can directly be checked for equivalence against the
reference by looking for a counterexample to the asserted equivalence.

We conduct the experimental evaluation using synthesis and sketching
tasks derived from \NumSubjects{} well-studied subject properties
defined over graphs and binary relations, and employ two popular LLMs,
namely ChatGPT (specifically \ChatGPTUsed) and DeepSeek (specifically
\DeepSeekUsed).  A key aspect of our evaluation is that for each
synthesis problem (from English to Alloy and from Alloy to Alloy), we
ask the LLMs to generate multiple equivalent but non-identical Alloy
formulas as solutions, thereby performing a deeper study of how well
the LLMs handle the semantic and syntactic intricacies of the Alloy
language.

The experimental results show that the LLMs generally perform
surprisingly well on synthesizing complete Alloy formulas from input
properties given in natural language or given in Alloy, and are able
to enumerate multiple unique solutions for each property.  Moreover,
the LLMs are also successful at completing given sketches of Alloy
formulas with respect to natural language descriptions of desired
properties (without requiring test cases, which are required by
traditional sketching
techniques~\cite{SolarLazemaPhD2008,WangETALABZ2018ASketch}).

We find the performance of LLMs surprising for three reasons.  One,
Alloy, despite its visibility in academia, is not a mainstream
language; sketching is used even less.  Therefore, only minimal
training data is available.  Moreover, no fine-tuning (Alloy-specific
or otherwise) of the LLMs is performed by us.  In fact, our queries to
the LLMs do not even describe what a sketch is.  We directly give the
synthesis and sketching problems to publicly available LLMs to solve.
Two, Alloy's notation, which allows for succinct formulation of
complex relational properties, makes the language deceptively hard to
master.  Three, creating multiple equivalent formulations of logical
properties requires an in-depth knowledge of logic.  For some of our
subject problems, the LLMs create many correct unique solutions, some
of which have very different logical structures (e.g., quantification
with one variable versus quantification with two variables).  While it
is conceivable that the LLMs simply memorized solutions to some (or
even all) of our queries because they ``saw'' similar ones in their
training data due to the very basic nature of properties of graphs and
binary relations, the ability to generate many (in some cases, over a
dozen) equivalent solutions -- some of which even surprised us and
provided us new insights into how to formalize the properties -- is
noteworthy.

This paper makes the following contributions:

\begin{itemize}
\item {\bf LLM-based synthesis and sketching of Alloy formulas}.  We
  take a three-fold approach for employing LLMs in writing Alloy
  specifications: 1) using natural language properties; 2) using
  reference Alloy formulas; 3) using Alloy sketches.
\item {\bf Evaluation}.  We present an experimental evaluation using
  \NumTotalSynthesis~synthesis tasks and \NumTotalSketching~sketching
  tasks derived from \NumSubjects~well-studied basic properties of
  graphs and binary relations, and two popular LLMs (\ChatGPTUsed{}
  and \DeepSeekUsed).  The experimental results demonstrate the
  effectiveness of using LLMs in writing Alloy formulas.
\end{itemize}
We believe LLMs offer an exciting advance in our ability to utilize
specifications, and the time has come for declarative languages, in
particular, and (lightweight) formal methods, in general, to move to
the forefront of modern tools for building robust and dependable
software systems.


% LocalWords:  LLMs ChatGPT DeepSeek toolset's backend toolset LLM
% LocalWords:  analyzability unsatisfiable LLM's
