\section{Related Work}
\subsection{Meta-Learning for In-Context Learning}
Meta-learning, also known as learning to learn \cite{thrun1998learning}, pertains to a category of approaches that prioritize the acquisition of generalizable adaptation skills across a spectrum of tasks. It encompasses a broad array of methodologies, including the optimization of gradient-based methods \citep{finn2017model} and model-based meta-learning \citep{santoro2016meta, duan2016rl}. As the typical setting of model-based meta-RL \cite{duan2016rl,mishra2018simple} or ICRL \cite{laskin2022context,lee2024supervised,grigsbyamago}, states, actions, and rewards are typically arranged as a trajectory to compose the \emph{inner loop} for task adaptation, while the pre-training and meta-training are recognized as the \emph{outer loop}. Common choices for the outer-loop optimizer include reinforcement learning \cite{duan2016rl, mishra2018simple, grigsbyamago}, evolutionary strategies \cite{najarro2020meta, wang2022evolving}, and imitation learning \cite{lee2024supervised, laskin2022context}. Imitation learning of ICRL is also related to the reinforcement learning coach (RLCoach), which uses pre-trained RL agents to generate demonstrations. RLCoach is not only used for ICRL but is also widely employed to accelerate single-task reinforcement learning \cite{zhang2021end} and multi-task learning \cite{reed2022generalist}.

\subsection{In-Context Learning from Pre-training}
The rise of Large Language Models (LLMs) blurs the boundary between pre-training and meta-training, as pre-training with huge datasets incentivizes ICL in a manner similar to meta-learning \citep{brown2020language, chen2021meta, coda2023meta}. For clarity, we use ``pre-training'' to describe training that primarily targets the acquisition of diverse skills, typically followed by a subsequent gradient-based tuning stage. ``Meta-training'' refers to training aimed at acquiring the ability to learn, which does not necessarily require subsequent gradient-based tuning.
The correlation between the ability of ICL and the distribution of pre-training data has been thoroughly investigated \cite{chan2022data,singh2024transient} recently, indicating that ICL is related to "burstiness," which refers to patterns that exhibit a distributional gap between specific trajectories and the pre-training dataset.
Additionally, the level of "burstiness" may affect the trade-off between ICL and IWL, with non-bursty trajectories stimulating more IWL and less ICL.
Analyses and experiments have been conducted to show that computation-based ICL can exhibit a richer array of behaviors than gradient-based IWL~\cite{chan2022transformers, von2023transformers, xieexplanation}, particularly in terms of plasticity and continual learning~\cite{lior2024computation}.
Specifically, depending on the pre-training dataset, ICL can perform either associative generalization or rule-based generalization \cite{chan2022transformers}. In many cases, ICL primarily serves as a task identifier \cite{wies2024learnability}, where skills are memorized through IWL, and ICL simply invokes the correct one. 
This issue may be prevalent in many meta-learning benchmarks emphasizing few-shot learning, since those methods typically operate within a restricted domain and require re-training across domains. 
It further motivates the need for generalized in-context learning \cite{kirsch2022general, kirsch2023towards, wang2024benchmarking}, where the acquirement of skill is dominated by ICL instead of IWL.

\textbf{Relations with long chain-of-thought (CoT)}. Recently we have also observed a trend toward increasing the reasoning length in LLMs \cite{openai24o1, deepseek-llm}. However, they are quite distinct from the proposed generalized ICL: LLM reasoning emphasizes the ability of system 2 which represents rule-based and analytical thinking, while generalized ICL emphasizes the in-context improvement of system 1 which represents rapid and intuitive decision-making \cite{wason1974dual,kahneman2011thinking}. Nonetheless, our research also underscores the importance of exploring long-sequence causal models beyond the Transformer architecture.

\subsection{Benchmarking In-Context Reinforcement Learning}
Meta-learning typically requires a set of related yet diverse tasks. One commonly used technique is to randomize a subset of domain parameters to create these variant tasks. These benchmarks can be broadly categorized as follows:
\circled{1} Randomizing the rewards or targets while keeping the transitions fixed. This includes multi-armed bandits \cite{mishra2018simple, laskin2022context}, varying goals in a fixed grid world or maze \cite{lee2024supervised}, different walking speeds in locomotion tasks \cite{finn2017model, mishra2018simple}, and diverse tasks in object manipulation \cite{yu2020meta}. This approach is also closely related to multi-objective reinforcement learning~\cite{alegre2022mo}.
\circled{2} Modifying the transitions while keeping the targets unchanged. Examples include tasks with crippled joints or varying joint sizes in locomotion \cite{najarro2020meta,pedersen2021evolving}, procedurally generated grid worlds \cite{wang2022evolving,nikulin2023xland} and games \cite{cobbe2020leveraging}.
\circled{3} Randomizing the observations and labels without altering the underlying transitions and rewards. Examples include hiding parts of observations \cite{morad2023popgym}, randomizing labels \cite{kirsch2022general}, and actions \cite{siniicontext}.
The above approaches typically create a group of tasks that start from a ``seed'' task. These methods are also related to domain randomization \cite{peng2018sim,arndt2020meta}, which has proven effective in reducing sim-to-real gaps by improving both in- and out-of-distribution generalization\cite{peng2022out}.