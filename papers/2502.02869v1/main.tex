%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage{algpseudocode}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[preprint]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{mdframed}
\usepackage{multirow}
\usepackage{makecell}  
\usepackage{titlesec}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


\newcommand{\TODO}[2]{\textcolor{red}{\\ \textbf{#1 - ToDo}: \\#2\\}}
\newcommand{\wf}[1]{\textcolor{blue}{#1}}
\newcommand{\pt}[1]{\textcolor{purple}{#1}}
\newcommand{\BY}[1]{{\color{blue}[BY: #1]}}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,fill,inner sep=1pt] (char) {\textcolor{white}{#1}};}}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{OmniRL: In-Context Reinforcement Learning By Large-Scale Meta-Training in Randomized World}

\begin{document}

\twocolumn[
\icmltitle{OmniRL: In-Context Reinforcement Learning by Large-Scale Meta-Training in Randomized Worlds}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Fan Wang}{airs,ustc}
\icmlauthor{Pengtao Shao}{airs}
\icmlauthor{Yiming Zhang}{airs}
\icmlauthor{Bo Yu}{airs} \\
\icmlauthor{Shaoshan Liu}{airs}
\icmlauthor{Ning Ding}{airs}
\icmlauthor{Yang Cao}{ustc}
%\icmlauthor{}{sch}
\icmlauthor{Yu Kang}{ustc}
\icmlauthor{Haifeng Wang}{baidu}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{airs}{Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, China}
\icmlaffiliation{ustc}{University of Science and Technology of China, Shenzhen, China}
\icmlaffiliation{baidu}{Baidu Inc, Beijing, China}

\icmlcorrespondingauthor{Fan Wang}{fanwang.px@gmail.com}
\icmlcorrespondingauthor{Shaoshan Liu}{shaoshanliu@cuhk.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{}

\vskip 0.25in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
We introduce \emph{OmniRL} \footnote{\url{https://github.com/airs-cuhk/airsoul/tree/main/projects/OmniRL}}, a highly generalizable in-context reinforcement learning (ICRL) model that is meta-trained on hundreds of thousands of diverse tasks. These tasks are procedurally generated by randomizing state transitions and rewards within Markov Decision Processes \footnote{\url{https://github.com/FutureAGI/L3C/tree/main/l3c/anymdp}}. To facilitate this extensive meta-training, we propose two key innovations: (1) An efficient data synthesis pipeline for ICRL, which leverages the interaction histories of diverse behavior policies; and (2) A novel modeling framework that integrates both imitation learning and reinforcement learning (RL) within the context, by incorporating prior knowledge.
For the first time, we demonstrate that in-context learning (ICL) alone, without any gradient-based fine-tuning, can successfully tackle unseen Gymnasium tasks through imitation learning, online RL, or offline RL. Additionally, we show that achieving generalized ICRL capabilities—unlike task identification-oriented few-shot learning—critically depends on long trajectories generated by variant tasks and diverse behavior policies. 
By emphasizing the potential of ICL and departing from pre-training focused on acquiring specific skills, we further underscore the significance of meta-training aimed at cultivating the ability of ICL itself.
\end{abstract}

\section{Introduction}

Large-scale pre-training has achieved tremendous success, especially in processing natural languages, images, and videos~\cite{radford2021learning,driess2023palm,touvron2023llama,achiam2023gpt}.
They have demonstrated the ability to address unseen tasks through \emph{in-context learning} (ICL)~\cite{brown2020language}, a paradigm that leverages contextual information to enhance performance. Unlike \emph{in-weights learning} (IWL), which relies on gradient-based updates to model weights, ICL enables models to acquire new skills in a few-shot manner, enhancing their adaptability to novel environments. With commonalities with model-based meta-learning approaches~\cite{duan2016rl, santoro2016meta}, ICL can accommodate traditional learning paradigms within its framework, including supervised learning~\cite{santoro2016meta,garg2022can}, imitation learning~\cite{reed2022generalist, fucontext, vosyliusfew}, and reinforcement learning~\cite{laskin2022context, grigsbyamago, zisman2023emergence, lee2024supervised}. This significantly alleviates the need for laborious human-designed objective functions and optimization strategies, which are typically required in IWL. 
Further, gradient-based IWL has been criticized for its inefficiency in continually adapting to new tasks~\cite{dohare2024loss}. In contrast, ICL has demonstrated plasticity that resembles the adaptability of the human brain~\cite{lior2024computation}. 

However, current meta-learning and in-context learning frameworks exhibit several limitations. Firstly, they often focus on few-shot adaptation in relatively small-scale tasks, language formations, or constrained domains~\cite{chen2021meta, min2021metaicl, coda2023meta}. As a result, the efficacy of in-context learning (ICL) in adapting to complex novel tasks—such as those requiring substantial data volume and reinforcement learning—remains uncertain. Consequently, adaptation in such scenarios primarily relies on gradient-based IWL rather than ICL.
Secondly, the mechanisms underlying the ``emergence'' of ICL capabilities and their limitations are not fully understood~\cite{wei2022emergent}. For instance, excessive pre-training on specific datasets can enhance IWL while potentially hindering ICL capabilities beyond a certain point~\cite{singh2024transient}. Therefore, we are interested in the question: \emph{Is it possible to meta-train generalized ICL abilities that are stable and agnostic to the underlying data distribution?}

To this end, this paper introduces \emph{OmniRL}, a model capable of adapting novel tasks at scale through ICRL and other ICL paradigms. To train OmniRL, we first propose an efficient simulator capable of generating a vast array of tasks modeled through Markov Decision Processes (MDPs). These tasks, which we dub \emph{AnyMDP}, feature diverse state transitions and reward structures within discrete state and action spaces. Albeit lacking high fidelity to real-world problems, this powerful task generation scheme enables us to explore large-scale meta-training involving billions of time steps generated from millions of distinct tasks.

Furthermore, unlike traditional ICRL models, OmniRL enables the agent to leverage both posterior feedback (such as rewards) and prior knowledge for in-context adaptation. Additionally, we introduce a data synthesis strategy that emphasizes both the diversity of trajectories and computational efficiency. These innovations facilitate large-scale imitation-only meta-training, while enabling effective in-context adaptation through imitation learning, online RL, or offline RL.

OmniRL not only outperforms existing ICRL frameworks but also demonstrates the ability to generalize to unseen Gymnasium environments~\cite{towers2024gymnasium}, including Cliff, Lake, Pendulum, and even multi-agent games.
Furthermore, we conducted a quantitative analysis of the impact of the number of meta-training tasks on the acquisition of ICRL abilities at scale. Our findings reveal that the volume of training tasks is crucial in balancing between "task identification"–oriented few-shot learning and generalized in-context learning (ICL)~\cite{kirsch2022general}. Specifically, agents focused on "task identification" excel at solving familiar tasks with fewer samples but often lack generalization capabilities. In contrast, generalized ICL agents~\cite{kirsch2022general, kirsch2023towards, wang2024benchmarking} can solve both seen and unseen tasks, albeit with the trade-off of requiring a larger volume of data in context. Our results indicate that addressing longer trajectories is essential for achieving robust generalization.

Our contributions are summarized as follows: \circled{1}We introduce AnyMDP, a scalable collection of tasks and environments modeled as Markov Decision Processes (MDPs) with randomized state transitions and rewards. This framework enables the meta-training process to scale up to hundreds of thousands of tasks. \circled{2}We propose an ICRL framework for the large-scale meta-training of OmniRL featuring an efficient data synthesis pipeline and a new model framework. \circled{3}We demonstrate that the volume of tasks and the modeling of long trajectories are crucial for the emergence and the generalizability of ICRL and ICL abilities.

\section{Related Work}
\subsection{Meta-Learning for In-Context Learning}
Meta-learning, also known as learning to learn \cite{thrun1998learning}, pertains to a category of approaches that prioritize the acquisition of generalizable adaptation skills across a spectrum of tasks. It encompasses a broad array of methodologies, including the optimization of gradient-based methods \citep{finn2017model} and model-based meta-learning \citep{santoro2016meta, duan2016rl}. As the typical setting of model-based meta-RL \cite{duan2016rl,mishra2018simple} or ICRL \cite{laskin2022context,lee2024supervised,grigsbyamago}, states, actions, and rewards are typically arranged as a trajectory to compose the \emph{inner loop} for task adaptation, while the pre-training and meta-training are recognized as the \emph{outer loop}. Common choices for the outer-loop optimizer include reinforcement learning \cite{duan2016rl, mishra2018simple, grigsbyamago}, evolutionary strategies \cite{najarro2020meta, wang2022evolving}, and imitation learning \cite{lee2024supervised, laskin2022context}. Imitation learning of ICRL is also related to the reinforcement learning coach (RLCoach), which uses pre-trained RL agents to generate demonstrations. RLCoach is not only used for ICRL but is also widely employed to accelerate single-task reinforcement learning \cite{zhang2021end} and multi-task learning \cite{reed2022generalist}.

\subsection{In-Context Learning from Pre-training}
The rise of Large Language Models (LLMs) blurs the boundary between pre-training and meta-training, as pre-training with huge datasets incentivizes ICL in a manner similar to meta-learning \citep{brown2020language, chen2021meta, coda2023meta}. For clarity, we use ``pre-training'' to describe training that primarily targets the acquisition of diverse skills, typically followed by a subsequent gradient-based tuning stage. ``Meta-training'' refers to training aimed at acquiring the ability to learn, which does not necessarily require subsequent gradient-based tuning.
The correlation between the ability of ICL and the distribution of pre-training data has been thoroughly investigated \cite{chan2022data,singh2024transient} recently, indicating that ICL is related to "burstiness," which refers to patterns that exhibit a distributional gap between specific trajectories and the pre-training dataset.
Additionally, the level of "burstiness" may affect the trade-off between ICL and IWL, with non-bursty trajectories stimulating more IWL and less ICL.
Analyses and experiments have been conducted to show that computation-based ICL can exhibit a richer array of behaviors than gradient-based IWL~\cite{chan2022transformers, von2023transformers, xieexplanation}, particularly in terms of plasticity and continual learning~\cite{lior2024computation}.
Specifically, depending on the pre-training dataset, ICL can perform either associative generalization or rule-based generalization \cite{chan2022transformers}. In many cases, ICL primarily serves as a task identifier \cite{wies2024learnability}, where skills are memorized through IWL, and ICL simply invokes the correct one. 
This issue may be prevalent in many meta-learning benchmarks emphasizing few-shot learning, since those methods typically operate within a restricted domain and require re-training across domains. 
It further motivates the need for generalized in-context learning \cite{kirsch2022general, kirsch2023towards, wang2024benchmarking}, where the acquirement of skill is dominated by ICL instead of IWL.

\textbf{Relations with long chain-of-thought (CoT)}. Recently we have also observed a trend toward increasing the reasoning length in LLMs \cite{openai24o1, deepseek-llm}. However, they are quite distinct from the proposed generalized ICL: LLM reasoning emphasizes the ability of system 2 which represents rule-based and analytical thinking, while generalized ICL emphasizes the in-context improvement of system 1 which represents rapid and intuitive decision-making \cite{wason1974dual,kahneman2011thinking}. Nonetheless, our research also underscores the importance of exploring long-sequence causal models beyond the Transformer architecture.

\subsection{Benchmarking In-Context Reinforcement Learning}
Meta-learning typically requires a set of related yet diverse tasks. One commonly used technique is to randomize a subset of domain parameters to create these variant tasks. These benchmarks can be broadly categorized as follows:
\circled{1} Randomizing the rewards or targets while keeping the transitions fixed. This includes multi-armed bandits \cite{mishra2018simple, laskin2022context}, varying goals in a fixed grid world or maze \cite{lee2024supervised}, different walking speeds in locomotion tasks \cite{finn2017model, mishra2018simple}, and diverse tasks in object manipulation \cite{yu2020meta}. This approach is also closely related to multi-objective reinforcement learning~\cite{alegre2022mo}.
\circled{2} Modifying the transitions while keeping the targets unchanged. Examples include tasks with crippled joints or varying joint sizes in locomotion \cite{najarro2020meta,pedersen2021evolving}, procedurally generated grid worlds \cite{wang2022evolving,nikulin2023xland} and games \cite{cobbe2020leveraging}.
\circled{3} Randomizing the observations and labels without altering the underlying transitions and rewards. Examples include hiding parts of observations \cite{morad2023popgym}, randomizing labels \cite{kirsch2022general}, and actions \cite{siniicontext}.
The above approaches typically create a group of tasks that start from a ``seed'' task. These methods are also related to domain randomization \cite{peng2018sim,arndt2020meta}, which has proven effective in reducing sim-to-real gaps by improving both in- and out-of-distribution generalization\cite{peng2022out}.

\section{Methodology}

\subsection{AnyMDP:Randomized Worlds} \label{sec:anymdp}
\begin{figure}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{img/AnyMDP_Visualization.png}}
\caption{3D visualizations of examples of AnyMDP tasks $\tau(n_s=128, n_a=5)$. State nodes are colored according to the average reward upon reaching them. The lines denote the average transition probabilities between states.}
\label{fig:anymdp_visualization}
\end{center}
\vskip -0.2in
\end{figure}
To effectively generate a large number of tasks to enhance generalized ICL, we adopt a different approach from previous benchmarks that typically apply domain randomization to a ``seed task.'' Instead, we choose an extreme approach that sacrifices fidelity to the real world in favor of maximizing task diversity.
We primarily consider fully observable Markov Decision Processes in discrete state and action spaces. Let $n_s$ and $n_a$ denote the sizes of the state and action spaces, respectively. Any task that can be modeled through MDPs in discrete state and action spaces is represented as follows:
\begin{align}
\tau(n_s, n_a) = \{T_{\tau}, R_{\tau}, \Sigma_{\tau}\} \in \mathcal{T}(n_s,n_a),
\end{align}
with $T_{\tau}, R_{\tau}, \Sigma_{\tau} \in \mathbb{R}^{n_s \times n_a \times n_s}$. The transitions follow $p(s'|s,a) = T_{\tau}[s,a,s']$, and the rewards are decided by $r(s,a,s')\sim \mathcal{N}(R_{\tau}[s,a,s'], \Sigma_{\tau}[s,a,s'])$.
By sampling $T_{\tau}$, $R_{\tau}$, and $\Sigma_{\tau}$ randomly, we can theoretically cover any possible MDPs. However, in practice, naively sampling transition and reward matrices ends up with a trivial task with a high probability. Therefore, we devised a method for generating challenging tasks efficiently. In this method, the representations of states and actions are first sampled from a high-dimensional continuous space, and then the distributions of transitions and rewards are recalculated based on these representations, as described in \cref{sec:anymdp_task}. 

AnyMDP degenerates to classical bandit benchmarks \citep{duan2016rl, mishra2018simple} by setting $n_s=1$. With $n_s > 1$, it demands reasoning over diverse trajectories and delayed rewards, which poses a greater challenge over ICL. Moreover, with access to ground truth transitions and rewards, it facilitates low-cost access to an oracle solution through value iteration \cite{bellman1958dynamic}, eliminating the necessity to execute costly RL optimization. A visualization of these procedurally generated tasks can be found in \cref{fig:anymdp_visualization}.

\subsection{Modeling and Training Framework} \label{sec:OmniRL}

\textbf{Problem Setting}: In ICRL, the agent adapts to novel tasks by incorporating its interaction history into its context, denoted as: $h_{t-1}=[s_1, a_1, r_1,  ..., s_{t-1}, a_{t-1}, r_{t-1}]$. We use the character $s$, $a$, and $r$ to denote state \footnote{
In this work, we focus solely on fully observable Markov Decision Processes (MDPs); extending our approach to Partially Observable Markov Decision Processes (POMDPs) is conceptually straightforward within this framework, but it necessitates significant effort in datasets, which we plan to address in future research.}, action, and reward, respectively. 
The policy neural network, parameterized by $\theta$, is denoted as $\pi_{\theta}(a_t|s_t,h_{t-1})$. Here, $h_{t-1}$ serves as the task-specific training data for the inner loop, where the agent is expected to continually improve its performance as $t$ increases due to the accumulation of task-specific data.
To optimize $\theta$, the outer loop involves meta-training, which is performed on a training task set $\mathcal{T}_{tra}$. The policy is then validated using a testing task set $\mathcal{T}_{tst}$. 

\textbf{Including the prior knowledge of policy in context}: 
Reinforcement learning (RL) relies predominantly on posterior information for learning, specifically feedback or reward. However, it may overlook crucial prior knowledge that could enhance the learning process. For example, consider a scenario where an expert provides the agent with a demonstrated trajectory, the agent would be unable to fully trust the demonstrating policy without comparing its feedback to other trajectories and sufficiently exploring the entire state-action space.
Therefore, we introduce an additional feature $p$ to denote the prior information associated with each action. In practice, it may be used to incorporate tags, commands, or prompts for the upcoming action. The agent is required to consider both the prior information and the posterior information in the history simultaneously, which may benefit in two aspects.
1. It may be used as a tag to avoid confusion in interpreting the trajectory ($h$), which could originate from diverse policies including exploration, myopic, and non-myopic exploitation \cite{chen2021decision}.
2. It may be used to denote the trustworthiness of the previous action. For instance, if the actions are generated by an oracle or expert, the agent may be more inclined to directly trust them, without relying solely on feedback or leaning towards exploration.
For now, we set $p_i$ to be the class ID marking the policy from which the action $a_i$ is generated, with an additional ID ``UNK'' reserved as the default.
Then, the interaction history is extended to $h_{t-1}=[s_1, p_1, a_1, r_1, s_2, p_2, ..., p_{t-1}, a_{t-1}, r_{t-1}]$. The policy neural network is thereby denoted by $\pi_{\theta}(a_t |h_{t-1}, p_t, s_t)$. 

\textbf{Data synthesis for imitation-only meta-training}: Imitation learning has been demonstrated to effectively elicit ICRL with lower cost and better scalability. Nonetheless, directly imitating the trajectory of an expert (behavior cloning) is less effective due to the accumulation of compound errors in MDPs \cite{ross2010efficient}. Inspired by data aggregation \cite{ross2011reduction}, we define two key policies in our framework that are independent of each other. The \emph{behavior policy} (superscript $(b)$) refers to the policy that is actually executed to generate the trajectory $h$. Meanwhile, the \emph{reference policy} (superscript $(r)$) serves as the target policy to be imitated, but it is not executed directly. This yields the following target:
\begin{align}
Minimize:&\mathcal{L}_t = -log\pi_{\theta}(a^{(r)}_t|h_{t-1}, s_t, p^{(r)}_t),\label{eq:outer_loop}
\end{align}
\begin{align}
with \quad& \pi_{\theta}=Softmax(z_t),\nonumber\\
& z_t = Causal_{\theta}(h_{t-1}, p^{(r)}_t, s_t), \label{eq:pi_theta}
\end{align}
with $h_{t-1}=[s_0, p^{(b)}_0, a^{(b)}_0, r_0, s^{(b)}_1,...,r_{t-1}]$. \cref{eq:outer_loop} can be used to represent various ICRL techniques by varying behavior and reference policies, including algorithm distillation (AD) \cite{laskin2022context}, noise distillation (AD$^{\epsilon}$) \cite{zisman2023emergence}, and decision pre-training Transformers (DPT) \cite{lee2024supervised} (see \cref{sec:anymdp_synthesis} for details). Following DPT, we mainly use the oracle policy for data collection. In this process, the reference policy $p^{(r)}$ can be omitted from the trajectory, while $p^{(b)}$ is retained. However, we introduce even more diversity into behavior policies to enhance the \emph{completeness} of the data. Specifically, we include methods such as Q-learning, model-based reinforcement learning, multi-$\gamma$ oracle policies \cite{grigsbyamago}, and noise distillation. A summary of the data synthesis pipeline is in \cref{alg:data_synthesis}.

\begin{figure*}[htbp]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.85\textwidth]{img/GLA_ModelArch.pdf}}
\caption{The model framework of OmniRL}
\label{fig:gla_modelarch}
\end{center}
\vspace{-30pt}
\end{figure*}

\textbf{Chunk-wise meta-training}. 
By independently sampling from the behavior policy to generate trajectories and from the reference policy to generate labels, we can further reformulate Equation \cref{eq:outer_loop} into an efficient chunk-wise form for training, as illustrated in \cref{fig:gla_modelarch}.
It is reformulated as:
\begin{align}
Minimize:\mathcal{L} = \sum_t w_t \mathcal{L}_t \nonumber\\
z_1, z_2, ..., z_t = Causal_{\theta}(&p^{(r)}, s_0, p^{(b)}_0, a^{(b)}_0, r_0, \nonumber\\
&..., s_1, ..., s_2, ..., s_t).
\label{eq:chunk_wise_opt}
\end{align}

Extending ICL to complex tasks at scale requires efficient modeling of very long contexts. While the Transformer \cite{vaswani2017attention} is regarded as state-of-the-art for short horizons, it is challenging to apply the Transformer to trajectories longer than $10K$. Thus, we employ sliding window attention \cite{beltagy2020longformer} on top of Transformers with rotary position embeddings \cite{su2024roformer}, but in a segment-wise setting, which is more akin to Transformer-XL \cite{dai2018transformer}. The theoretical limit of the valid context length is given by $(N_{\text{layers}} + 1) \times L_{\text{segment}}$.
We further investigate more efficient linear attention layers including Mamba \cite{gu2023mamba,lv2024decision,huang2024decision,ota2024decision}, RWKV6 \cite{peng2023rwkv}, and Gated Slot Attention (GSA) \cite{zhanggated}.
These structures offer promising inference capabilities with a computational cost of $\Theta(T)$. Alternatively, inference relies on memory states with a fixed size, rather than a growing context, thus transforming in-context learning into \emph{in-memory learning}.
We select GSA for the subsequent experiments based on the conclusion of some preliminary experiments. Furthermore, to facilitate training with long trajectories scaling up to over 1 million, we implemented a segment-wise back-propagation pipeline. In the training phase, the full sequence is first divided into multiple segments. The forward pass is calculated across the entire sequence, while the backward pass is calculated within each segment, with the gradient eliminated across segments. This enables us to train on arbitrarily long sequences with limited computation resources (see \cref{alg:meta_training} in appendices).

\section{Experiments}

\subsection{Experimental Setup}
\textbf{Meta-Training}. We conduct all our experiments by performing meta-training on data synthesized from AnyMDP tasks exclusively. We sample tasks totaling $|\mathcal{T}(n_s \in [16, 128], n_a = 5)| = 584K$, from which we synthesize up to $|\mathcal{D}(\mathcal{T}(n_s \in [16, 128], n_a=5))| = 584K$ trajectories (we use $\mathcal{D}(\mathcal{T})$ to denote the dataset generated based on $\mathcal{T}$), with trajectory lengths $T \in [8K, 1024K]$.
We find it beneficial to follow a curriculum learning procedure, commencing with $n_s=16$ and gradually scaling up $n_s$, the details of which are presented in \cref{sec:training settings}. To conserve computational resources, we primarily utilize Stage 1 for comparisons. Only the most promising settings proceed to Stages 2 and 3.
We sample multiple groups of validation datasets $\mathcal{D}(\mathcal{T}_{tst}(n_s \in \{16, 32, 64, 128\}, n_a=5)$ from both seen and unseen tasks, with each group containing $256$ trajectories. Unless otherwise specified, we default to selecting the model that achieves the best average performance on the validation set. Our experimental results demonstrate the superior performance of GSA over Transformer in both computational efficiency and long-term sequence modeling. Unless otherwise specified, we report the results of GSA.

\textbf{Validation and Evaluation}. We use the term ``validation'' to refer to the process of evaluating the loss on the validation dataset, which represents the offline evaluation of the model. In contrast, ``evaluation'' refers to the online performance of the model, assessed by deploying the agent and allowing it to interact with the environment. Furthermore, the evaluation is divided into three categories:
1. \textbf{Online-RL}: The agent starts with an empty trajectory, denoted as $h_{0} = \emptyset$.
2. \textbf{Offline-RL}: The agent starts with a trajectory of a certain length derived from imperfect demonstrations, denoted as $h_{0} = h^{\pi}$.
3. \textbf{Imitation Learning}: The agent starts with an oracle demonstration, denoted as $h_{0} = h^{(exp)}$.
For all three categories, the subsequent interactions are continually added to the trajectory within the evaluation process. Therefore, the models differ only in their initial KV-Cache (Transformers) or memory (GSA). The evaluation assesses the agents' abilities in two key areas: first, their capacity to exploit existing information, and second, their ability to explore and exploit continually based on that.

In addition to AnyMDP, we select gymnasium tasks~\cite{towers2024gymnasium} for evaluation, including Lake, Cliff, Pendulum, and Switch2 (a multi-agent game), with ICL only and no parameter tuning. For Pendulum with continuous observation space, we manually discretize the observation space into 60 states using grid discretization ($12\text{-class position} \times 5\text{-class velocity}$).

\textbf{Baselines}. We mainly compare with AD, AD$^{\epsilon}$, and DPT, all of which use imitation learning for meta-training (See \cref{table:icrl_data_synthesis}). 
Although we believe that an online RL$^2$ would further enhance performance, it is associated with a significantly higher cost of meta-training and is therefore not included in the comparison.
For gymnasium tasks, ICRL is also compared to Q-Learning implemented in stable-baseline3 \cite{raffin2021stable}. To investigate the impact of prior knowledge and the use of diversified reference policies through multi-$\gamma$ oracles, we also include OmniRL(w/o a priori) and OmniRL(multi-$\gamma$)~\cite{grigsbyamago} into the comparison.

\begin{figure*}[htbp]
\vskip 0.2in
\begin{center}
\begin{subfigure}[b]{0.60\columnwidth}
    \centering 
    \includegraphics[width=1\textwidth]{img/Online_rl_16states.pdf}
    \label{fig:online_rl_16states}
\end{subfigure}
\begin{subfigure}[b]{0.60\columnwidth}
    \centering 
    \includegraphics[width=1\textwidth]{img/Offline_rl_16states_2.pdf}
    \label{fig:offline_rl_16states}
\end{subfigure}
\begin{subfigure}[b]{0.60\columnwidth} 
    \centering
    \includegraphics[width=1\textwidth]{img/Imitation_learning_16states_2.pdf} 
    \label{fig:imitation_learning_16states}
\end{subfigure}
\begin{subfigure}[b]{0.2\columnwidth} 
    \centering
    \includegraphics[width=1\textwidth]{img/16_states_legend.pdf} 
    \label{fig:legend_16states}
\end{subfigure}
\vspace{-30pt}
\caption{Evaluation results of AD, AD$^{\epsilon}$, DPT, and OmniRL on 32 AnyMDP tasks, with 3 groups of initial demonstrations to assess the capabilities of online-RL, offline-RL, and imitation learning. For offline-RL and imitation learning, the agent is initialized with a demonstration trajectory of $100$ episodes.}
\label{fig:online_anymdp}
\end{center}
\vspace{-20pt}
\end{figure*}


\subsection{Comparison with baselines}\label{sec:results_base}
\textbf{Including the prior knowledge in trajectory and diversifying behavior policies benefits ICL}:
\cref{fig:online_anymdp} summarizes the performance of different methods on $\mathcal{T}_{tst}(16,5)$, including AD, AD$^{\epsilon}$, $DPT$, OmniRL (w/o a priori), and OmniRL. The performance score is normalized by the expected score per episode of the oracle policy ($100\%$) and the uniformly random policy ($0\%$).
OmniRL (w/o a priori) lags behind OmniRL with a noticeable gap in all three groups, demonstrating the importance of introducing the prior information. 
Comparing DPT with OmniRL (w/o a priori) reveals that increasing the diversity of behavior policies in the training data offers certain advantages in online RL. However, in offline RL and imitation learning, excluding prior information of the behavior policies introduces additional challenges.
All methods surprisingly exhibit some extent of imitation learning techniques, but OmniRL is the only method that performs well on all of online-RL, offline-RL, and imitation learning within 200 episodes.

\textbf{Diversifying reference policies shows no advantage}: Additional results also indicate that there are no benefits derived from the diversity of the reference policy. Additionally, we found it unnecessary to incorporate exploration strategies, just like $AD$ and $AD^{\epsilon}$, into the reference policy, as these significantly reduce performance. It is consistent with the theoretical analyses \cite{lee2024supervised} proving that imitating the oracle potentially leads to the exploration strategy of ``posterior sampling.'' Investigating the entropy of the decision-making process also reveals that the OmniRL agent tends to automatically explore more when insufficient information is provided in the context (\cref{sec:entropy}).

\begin{figure*}[h]
\vspace{0.2in}
\centering
\begin{subfigure}[b]{\columnwidth}
    \centering 
    \includegraphics[width=0.9\textwidth]{img/convergence_seen_ns16.pdf}
    \caption{Seen Tasks} 
    \label{fig:task_ablation_sub1}
\end{subfigure}
\centering
\begin{subfigure}[b]{\columnwidth} 
    \centering
    \includegraphics[width=0.9\textwidth]{img/convergence_unseen_ns16.pdf} 
    \caption{Unseen Tasks}
    \label{fig:task_ablation_sub2}
\end{subfigure}
\caption{Position-wise validation ($\mathcal{L}_t$, the lower the better) on seen and unseen tasks against the meta-training iterations and context length. The training data is $128K$ trajectories generated from different numbers of tasks, validating that the number of tasks affects the emergence of ICL.} 
\label{fig:task_ablation}
\vspace{-0.2in}
\end{figure*}

\subsection{Impact of Task Diversity}
To investigate the effect of task diversity on meta-training, we generate an equal number of trajectories ($|\mathcal{D}(\mathcal{T}_{tra}(16,5))|=128K$) based on different volumes of tasks with $|\mathcal{T}_{tra}| \in \{100, 1K, 10K, 128K\}$.
Note that different trajectories can be generated from a single task, attributed to the diverse behavior policies, randomness in decision sampling, and randomness in transition sampling.
Then, another $|\mathcal{D}(\mathcal{T}_{tst}(16,5))|=256$ trajectories are sampled from both seen tasks (where the task overlaps with the training set) and unseen tasks (newly generated tasks not in any of the training set) for evaluation.
We examine how the loss function $\mathcal{L}_t$ changes with the number of meta-training iterations (outer-loop steps) and steps in context $t$ (inner-loop steps) simultaneously; the results are shown in \cref{fig:task_ablation}.

The following observations are remarkable: 1. Fewer tasks ($|\mathcal{T}_{tra}|<1K$) in meta-training lead to ``task identification,'' where the model primarily relies on in-weight learning (IWL) to capture the input-output mapping and employs in-context learning (ICL) for identification only. This is manifested by very fast adaptation in seen tasks and an inability to generalize to unseen tasks. 2. In the group with $|\mathcal{T}_{tra}|=1K$, ICL is observed in unseen tasks at around $10K$ iterations, but degenerates quickly as meta-training continues. This reaffirms the ``transiency'' of ICL as mentioned in \citet{singh2024transient}.
As we increase the number of tasks, this ``transiency'' diminishes, which somewhat resembles ``over-fitting'' in classical machine learning. However, the two phenomena are fundamentally different. Over-fitting in classical machine learning can typically be mitigated by increasing the amount of data. In contrast, the ``transiency'' observed in ICL is alleviated by increasing the volume of tasks.
3. When tasks are sufficiently diverse, it leads to ``generalized ICL,'' where in-context adaptation takes longer but generalizes better to unseen tasks.

Given that \cref{fig:task_ablation} is on relatively simple task sets $\mathcal{T}(16,5)$, and more complex tasks may require significantly more task diversity for convergence, a reasonable assumption is that much of the existing meta-learning benchmarks fall into the category of task identification. This potentially facilitates few-shot learning for in-distribution generalization but is less capable of generalizing to out-of-domain tasks. Therefore, we advocate for meta-training on a scalable collection of tasks rather than on a restricted domain.

\begin{figure*}[htbp]
\vskip 0.2in
\begin{center}
\begin{subfigure}[b]{0.49\columnwidth}
    \centering 
    \includegraphics[width=0.98\textwidth]{img/validation_gsa_t16_t128.pdf}
    \caption{} 
    \label{fig:static_eval_sub1}
\end{subfigure}
\begin{subfigure}[b]{0.49\columnwidth}
    \centering 
    \includegraphics[width=0.98\textwidth]{img/Online-RL_Evaluation-GSA.pdf}
    \caption{} 
    \label{fig:static_eval_sub2}
\end{subfigure}
\begin{subfigure}[b]{0.49\columnwidth} 
    \centering
    \includegraphics[width=0.98\textwidth]{img/validation_t16.pdf} 
    \caption{}
    \label{fig:static_eval_sub3}
\end{subfigure}
\begin{subfigure}[b]{0.49\columnwidth} 
    \centering
    \includegraphics[width=0.98\textwidth]{img/validation_t64.pdf} 
    \caption{}
    \label{fig:static_eval_sub4}
\end{subfigure}
\vspace{-10pt}
\caption{Position-wise validation ($\mathcal{L}_t$) and evaluation (normalized score) of meta-trained Gated Slot Attention (GSA) and Transformer-XL models on $\mathcal{D}_{tst}$ generated from different task sets. (a) Validation of the position-wise loss of GSA on various test datasets. (b) Evaluation of GSA on various task sets  (c) Validation of Transformer-XL and GSA on $\mathcal{D}(\mathcal{T}_{tst}(16,5))$. (d) Validation on $\mathcal{D}(\mathcal{T}_{tst}(64,5))$. The shaded area represents 95\% confidence of evaluation.}
\label{fig:static_eval}
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{Scaling up State Spaces}
We follow a curriculum learning approach with approximately three stages to further scale up the meta-training, thereby accommodating tasks with larger state spaces and longer trajectories (\cref{sec:training settings}). Until the final stage (stage-3), we use $16B$ steps of interaction overall, and scale the state space up to $128$ and steps within each trajectory to up to $1024K$, which requires the actual context length of $4096K$ for OmniRL. By validating the Stage-3 model in $\mathcal{T}(16,5)$, we observe a further improvement over the stage-1 OmniRL, as shown in \cref{fig:anymdp_visualization}. 

We also validate the learned model on $\mathcal{D}(\mathcal{T}_{tst}(16,5))$, $\mathcal{D}(\mathcal{T}_{tst}(32,5))$, $\mathcal{D}(\mathcal{T}_{tst}(64,5))$, and $\mathcal{D}(\mathcal{T}_{tst}(128,5))$ and show the step-wise loss in figure~\ref{fig:static_eval}.
On a semi-logarithmic axis, the position-wise validation loss exhibits a nearly perfect linear relationship with the context length before the ``saturation'' of in-context improvements. This ``saturation'' might be induced by the upper limit of the environment itself or the limitations of the sequence modeling capabilities. The context length at which performance saturates is referred to as the ICL horizon. \cref{fig:static_eval_sub1} demonstrates that higher task complexity leads to a longer ICL horizon. In \cref{fig:static_eval_sub2}, we further show the online-RL evaluation of the stage-3 model on state spaces ranging from $\mathcal{T}_{tst}(16,5)$ to $\mathcal{T}_{tst}(128,5)$. These results demonstrate strong consistency with the validation results on the static dataset (\cref{fig:static_eval_sub1}).

\begin{figure*}[!h]
\begin{center}
\includegraphics[width=0.99\textwidth]{img/gym.pdf}
\vspace{-20pt}
\caption{Evaluation results of OmniRL (Stage-3) on gymnasium environments of FrozenLake-v1, CliffWalking-v0, Pendulum-v1, and Switch2, demonstrating the model's online-RL, offline-RL, and imitation learning capabilities toward diverse environments. Notice that within Switch2, a lower step cost is preferable.}
\label{fig:gym}
\end{center}
\vspace{-20pt}
\end{figure*}

\cref{fig:static_eval_sub3,fig:static_eval_sub4} further validated the superiority of GSA in long-sequence modeling. When validating on $\mathcal{D}(\mathcal{T}(64, 5))$, although Transformer-XL performs better within $2K$ steps, the GSA surpasses Transformer-XL by a large margin beyond 20K steps. It is also worth noticing that on $\mathcal{D}(\mathcal{T}(16, 5))$ and the other benchmarks (such as NLP benchmarks) \cite{zhanggated}, the superiority of GSA is not that obvious. It indicates that AnyMDP offers benchmarks more scalable in terms of context length.

\subsection{Generalizing to Gymnasium Tasks}
Aiming at studying the model's generalization capabilities toward diverse environments, OmniRL is further evaluated in the OpenAI Gymnasium with grid world and classic control problems NOT included in the training dataset, including lake$4\times 4$ (with both slippery and non-slippery dynamics), cliff, pendulum (with variant environment configurations of $g=\{1.0, 5.0, 9.8\}$). The results are shown in \cref{fig:gym} and \cref{fig:gym_addition}. OmniRL (Stage-3) demonstrates strong performance across most environments, including online RL, offline RL, and imitation learning. However, it underperforms in the pendulum environment with $g=9.8$, particularly during offline RL evaluations. We hypothesize that random exploration in the pendulum environment is insufficient for achieving success by chance. We also found that proper reward shaping is important for OmniRL to work, which is described in \cref{fig:reward_shaping}.

\textbf{OmniRL can generalize to multi-agent system}: OmniRL can be applied to the two-agent game of Switch2 \cite{magym} without any fine-tuning by incorporating the state of the other agent into its observation. Although both agents begin with identical models, they eventually exhibit distinct action patterns to effectively cooperate. This divergence in behavior arises from their ability to adapt in-context. Nonetheless, we observe some instability in continual learning when starting from imitation learning to reinforcement learning. Initially, the agent closely follows the teacher's demonstrations for the first few episodes. However, its performance deteriorates as it begins to learn from its interaction history. Then its performance improves once more as it presumably switches back to ``RL mode.'' This issue in continual learning deserves further investigation.

\section{Conclusions and Discussions}
We propose a scalable task set for benchmarking and investigating ICRL, along with an efficient ICRL framework that supports the in-context adaptation with online-RL, offline-RL, and imitation learning.
The proposed model, OmniRL, generalizes to a broader range of RL tasks than ever before. 
Our conclusion indicates that exploring long-term dependencies in causal modeling is essential for enhancing generalized in-context learning (ICL) abilities.
Our conclusions also re-emphasize that the trade-off between ICL and IWL depends on the distribution of the data, and the generalized ICL ability is primarily dependent on the task diversity and data completeness.
We propose that our findings could illuminate a novel pre-training paradigm that prioritizes the diversity and completeness of data over fidelity. The core objective of this training approach is to cultivate the capacity for learning itself, rather than acquiring specific skills. We refer to this as large-scale meta-training.

\textbf{Limitations and Future Work}: Currently, the application of this work is constrained by several factors, including the discrete state and action space, the assumption of fully observable states, and static environments. Future work could benefit from extending these features to more complex and dynamic settings. 

% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}
This work is supported by Longgang District Shenzhen's ``Ten Action Plan'' for Supporting Innovation Projects (under Grant  LGKCSDPT2024002) and Major Project of Scientific and Technological Innovation 2030 - ``New Generation of Artificial Intelligence'' (Code 2021ZD0110500).

%\section*{Impact Statement}
%This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}
\bibliographystyle{icml2025}
\bibliography{reference}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Details of the Experiment Settings}
\subsection{Procedurally Generating Tasks in AnyMDP} \label{sec:anymdp_task}

We found that directly sampling $T_{\tau}$ and $R_{\tau}$ results in trivial tasks most of the time, which do not effectively incentivize in-context reinforcement learning. To generate tasks that are sufficiently challenging, AnyMDP tasks are created by sampling states and actions in a continuous space with dimension $n_d$ that falls within the interval $[2,8)$, and then projecting them back to discrete spaces. The procedure for generating these tasks is depicted in Algorithm \ref{alg:task_sampler}, where we sample transitions with $TransitionSampler$ (\cref{alg:task_sampler_transition}) and rewards with $RewardSampler$ (\cref{alg:task_sampler_reward}) separately. We use $\mathcal{Q}^{\pi}_{\gamma} = \{Q^{\pi}_{\gamma}(s,a)\} \in \mathbb{R}^{n_s \times n_a}$ to represent the value functions on the entire state-action space with discount factor $\gamma$ and policy $\pi$. The generated tasks are evaluated against the following criteria: 1. The adjacency matrix of the state space $G(T_{\tau})$ must be connected, with the diameter exceeding a certain threshold; 2. The value functions $\mathcal{Q}^{\pi}_{\gamma}$ should exhibit significant variation across $\gamma$ and $\pi$, which is assessed using the Pearson correlation coefficient. With these settings, most of the tasks sampled from $\mathcal{T}(32, 5)$ cannot be effectively solved by Q-learning within 64K steps, even with carefully tuned hyperparameters.

\begin{algorithm}[htbp]
   \caption{AnyMDP $TaskSampler$}
   \label{alg:task_sampler}
    \begin{algorithmic}
       \STATE {\bfseries Input:} $n_s$, $n_a$
       \REPEAT
       \STATE {\bfseries Set} $T=TransitionSampler(n_s,n_a)$
       \STATE {\bfseries Set} $G(T)=Sum(T, axis=1) \in \mathbb{R}^{n_s \times n_s}$ (adjacency matrix)
       \UNTIL{$G(T)$ is Connected \AND $Diameter(G(T)) > d_H$} 
       \REPEAT
       \STATE {\bfseries Set} $R,\Sigma=RewardSampler(n_s,n_a,T)$
       \UNTIL{$Pearson(\mathcal{Q}^{opt}_{0.5}, \mathcal{Q}^{opt}_{0.99}) < r_H$ \AND $Pearson(\mathcal{Q}^{opt}_{0.99}, \mathcal{Q}^{rand}_{0.99}) < r_H$ \AND $Var(\mathcal{Q}^{opt}_{0.99}) > \sigma_H$}
       \STATE {\bfseries Return:} $T, R, \Sigma$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[htbp]
   \caption{$TransitionSampler$}
   \label{alg:task_sampler_transition}
    \begin{algorithmic}
       \STATE {\bfseries Input:} $n_s$, $n_a$
       \STATE {\bfseries Sample:} $n_d \in [2,8)$ from uniform distribution
       \STATE  {\bfseries Sample:} $e_s \in \mathbb{R}^{n_s \times n_d}$ from uniform distribution
       \STATE  {\bfseries Sample:} $e_a \in \mathbb{R}^{n_s \times n_d}$ from uniform distribution
       \STATE {\bfseries Set} $T[s,a,s'] \propto exp(-\frac{||e_{s} + e_{a} - e_{s'}||^2}{\sigma^2_s})$
       \STATE {\bfseries Sample:} birth nodes, pitfalls, and goals
       \STATE {\bfseries Return:} $T$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[htbp]
   \caption{$RewardSampler$}
   \label{alg:task_sampler_reward}
    \begin{algorithmic}
       \STATE {\bfseries Input:} $n_s$, $n_a$, $T$
       \STATE {\bfseries Sample:} $R_1 \in \mathbb{R}^{n_s}$ \AND $R_1 > 0$ 
       \STATE {\bfseries Sample:} $R_2 \in \mathbb{R}^{n_s \times n_a}$ from Gaussian distribution.
       \STATE {\bfseries Sample:} $R_3 \in \mathbb{R}^{n_s}$ from uniform distribution
       \STATE {\bfseries Set} $R[s,a,s']=\lambda_1 (\mathbb{I}_{goals}(s') \cdot R_1[s'] - \mathbb{I}_{pitfalls}(s') \cdot R_1[s']) + \lambda_2 R_2[s,a] + \lambda_3 (R_3[s'] - R_3[s])$
       \STATE {\bfseries Sample:} $\Sigma$ from Gaussian distribution.
       \STATE {\bfseries Return:} $R, \Sigma$
    \end{algorithmic}
\end{algorithm}

\subsection{Data Synthesis} \label{sec:anymdp_synthesis}
We sample nearly 1 million tasks with $n_s \in [16, 128]$ and $n_a=5$. The steps in context range from 8,000 to 1 million per task, and the total steps in the training data are more than 10 billion. To maintain a diverse set of behaviors for data generation, we implemented a diverse genre of policy learners list as follows:
\begin{itemize}
\item \emph{Oracle} policy $\mathcal{O}_{\gamma}$ access the ground truth transition and rewards and pick the optimal action by running value iteration with discount factor ($\gamma$). We select $\gamma \in \{0, 0.5, 0.93, 0.994\}$, corresponding to the half-life ($T_{hl} = log_\gamma \frac{1}{2}$) of $T_{hl} \in \{0, 1, 10, 100\}$. 
\item \emph{Q-Learning} policy $\mathcal{Q}_{\gamma}(\delta, \alpha)$ uses exploration strategy based on visiting count and $\delta$, with $\alpha$ being the learning rate. We omit $\delta, \alpha$ if they are randomly sampled. A parameter search on AnyMDP yields the optimal hyperparameter value of $\delta=0.005, \alpha=0.01$.
\item \emph{Model-based Reinforcement Learning} $\mathcal{M}_{\gamma}(\delta)$ uses reward and transition matrix to record all historical interactions. Value iteration is run on the estimated world models to pick the action with the largest utility. $\delta$ is used to balance exploration and exploitation. A parameter search on AnyMDP yields the optimal hyper-parameter value of $\delta = 0.005$.
\item \emph{Randomized Policy} $\mathcal{R}(\Theta)$ samples a random matrix $\Theta \in \mathbb{R}^{n_s \times n_a}$ for decision-making. Typical uniformly random exploration policy can be regarded as specific cases where $\Theta=c\mathbf{1}$ is a constant matrix.
\item \emph{Blended Policy} $\mathcal{X}^{\epsilon}$ further blend any policy $\mathcal{X}$ with random exploration depend on a decaying noise starting from $\epsilon$ to $0$.
\end{itemize}

With these notations, we summarize the previous imitation-learning-based in-context reinforcement learning methods and our methods in \cref{table:icrl_data_synthesis}. In OmniRL, we assign eight different IDs to the policies, as shown in \cref{table:icrl_policy_type}, corresponding to prompt IDs $p \in [0, 7]$. Additionally, we reserve an ID ("UNK", $p=7$), which is used to replace the ID approximately $15\%$ of the time steps during training. For online reinforcement learning, we retain the "UNK" prompt for actions generated by the agent itself. Also, note that among these IDs, some may correspond to different policies within the same category but with different hyper-parameters (such as $\delta$, $\alpha$).
In OmniRL (Multi-$\gamma$), we introduce $p^{(r)}$ to distinguish the variant reference policy. Within the standard OmniRL training settings, $p^{(r)}$ is omitted since the reference policy is kept unchanged.

In \cref{alg:data_synthesis} we present the pipeline of generating dataset $\mathcal{D}(\mathcal{T}_{tra})$ and $\mathcal{D}(\mathcal{T}_{tst})$ from tasks $\mathcal{T}_{tra}$, $\mathcal{T}_{tst}$ and different behavior and reference policies. We use $\oplus$ to denote the concatenation between trajectories.

\begin{table}[t]
\caption{Summarizing the data synthesis strategies of meta-training for ICRL.}
\label{table:icrl_data_synthesis}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcr}
\toprule
Data set & behavior policies ($\Pi^{(b)}$) & reference policies ($\Pi^{(r)}$) \\
\midrule
$AD$ \cite{laskin2022context}    & $\mathcal{Q}_{0.994}(0.005, 0.01)$ & $\mathcal{Q}_{0.994}(0.005, 0.01)$ \\
$AD^{\epsilon}$ \cite{zisman2023emergence}    & $\mathcal{O}^{\epsilon}_{0.994}$ & $\mathcal{O}^{\epsilon}_{0.994}$ \\
$DPT$ \cite{lee2024supervised} & $\mathcal{O}_{0.994},\mathcal{Q}_{0.994}(0.005, 0.01),\mathcal{R}(c\mathbf{1})$ & $\mathcal{O}_{0.994}$ \\
OmniRL (Ours) & $\mathcal{O}_{\gamma},\mathcal{Q}_{\gamma},\mathcal{M}_{\gamma},\mathcal{O}^{\epsilon}_{\gamma}, \mathcal{Q}^{\epsilon}_{\gamma}, \mathcal{M}^{\epsilon}_{\gamma}, \mathcal{R}$ & $\mathcal{O}_{\gamma}$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[t]
\caption{Correpsondance of prompt IDs and the policies it represents.}
\label{table:icrl_policy_type}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcr}
\toprule
ID & NAME & Policy Type \\
\midrule
0   & O0 & $\mathcal{O}_0$ \\
1   & O1 & $\mathcal{O}_{0.5}$ \\
2   & O2 & $\mathcal{O}_{0.93}$ \\
3   & O3 & $\mathcal{O}_{0.994}$ \\
4   & M0 & $\mathcal{M}$ \\
5   & Q0 & $\mathcal{Q}$ \\
6   & RND & $\mathcal{R}$ \\
7   & UNK & DEFAULT \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{algorithm}[htbp]
   \caption{Data Synthesis Pipeline}
   \label{alg:data_synthesis}
    \begin{algorithmic}
       \STATE {\bfseries Input:} $\mathcal{T}$, $N_{sample}$, Collection of policies $\Pi^{(r)}$ and $\Pi^{(b)}$.
       \STATE $\mathcal{D}(\mathcal{T})=\emptyset$
       \FOR{$[1,N_{sample}]$}
            \STATE {\bfseries Sample:} $\tau \sim \mathcal{T}$, {\bfseries Set:} $t=0, h_0=[], l_0=[]$ 
            \REPEAT
                \STATE {\bfseries Sample:} $\pi^{(b)} \sim \Pi^{(b)}$, $\pi^{(r)} \sim \Pi^{(r)}$
                \STATE {\bfseries Reset:} $\tau$ and update $s_t$
                \REPEAT
                    \STATE {\bfseries Sample:}  $a^{(b)} \sim \pi^{(b)}$
                    \STATE {\bfseries Sample:} $a^{(r)} \sim \pi^{(r)}$
                    \STATE {\bfseries Execute:} $a^{(b)}$ in $\tau$ and obtain $s_{t+1}$, $r_t$
                    \STATE {\bfseries Set:} $h_{t}=h_{t-1}\oplus[s_t, p^{(b)}_t,a^{(b)}_t,r_t], l_{t} = l_{t-1}\oplus[p^{(r)}_t,a^{(r)}_t], t=t+1$
                \UNTIL{Episode is Over}
            \UNTIL{$t \geq T$}
            \STATE {\bfseries Set:} $\mathcal{D}(\mathcal{T})=\mathcal{D}(\mathcal{T}) \cup \{h_T, l_T\}$
       \ENDFOR
        \STATE {\bfseries Return:} $\mathcal{D}(\mathcal{T})$
    \end{algorithmic}
\end{algorithm}

\subsection{Experiment Setting Details} \label{sec:training settings}
\textbf{Model structures}: Before injection into causal models, the states, actions, and prompts are encoded using embedding layers with a hidden size of $512$. The rewards are treated as continuous features encoded by $1\times512$ linear layer. We use the hidden size of $512$, inner hidden size of $1024$, and block number of $18$ for both GSA and Transformer-XL. For Transformer-XL we use 8 heads and for GSA we use 4. The number of slots of GSA is 64.

\begin{algorithm}[htbp]
   \caption{Meta-Training Process}
   \label{alg:meta_training}
    \begin{algorithmic}
       \STATE {\bfseries Input:} $\mathcal{D}(\mathcal{T}_{tra})$, $\mathcal{D}(\mathcal{T}_{tst})$
       \FOR{$[0,\text{MAX\_EPOCH})$}
            \FOR{$h_T, l_T \in \mathcal{D}(\mathcal{T}_{tra})$}
                \STATE {\bfseries Set:} Segments $K=T/T_{seg}$, Gradients $g=\mathbf{0}$, Initial Cache $\mathcal{C}_0=\emptyset$
                \FOR{$k \in [0, K)$}
                    \STATE {\bfseries Forward:} using \cref{eq:chunk_wise_opt}, cache $\mathcal{C}_{k-1}$, $h_{kT_{seg}:(k+1)T_{seg}}$ and $l_{kT_{seg}:(k+1)T_{seg}}$; {\bfseries Update:} $\mathcal{C}_{k-1} \rightarrow \mathcal{C}_k$
                    \STATE {\bfseries Backward:} calculating $g_k = \nabla \mathcal{L}$ by stopping gradient of $\mathcal{C}_{k-1}$ 
                    \STATE {\bfseries Accumulate Gradient:} $g = g + g_k$
                \ENDFOR
                \STATE {\bfseries Apply Gradient:} use $g$ to update $\theta$
            \ENDFOR
            \STATE {\bfseries Validate:} Calculating and averaging $\mathcal{L}_t$ and $\mathcal{L}$ on $\mathcal{D}(\mathcal{T}_{tst})$
       \ENDFOR
    \end{algorithmic}
\end{algorithm}

\textbf{Meta-training and validation}: We present the meta-training process in \cref{alg:meta_training}. It is challenging to directly meta-train on $\mathcal{D}(\mathcal{T}(128,5))$, so we follow a curriculum learning process to optimize both models, which includes 3 stages:
\begin{itemize}
\item \emph{Stage 1}: Warming up by generating $|\mathcal{D}(\mathcal{T}_{tra}(16,5))|=128K$, where each trajectory consists of $8K$ steps (totaling $1B$ steps). The training lasts for approximately 5 epochs.
\item \emph{Stage 2}: Scaling up the task complexity by generating $|\mathcal{D}(\mathcal{T}_{tra}(n_s\in[16,32], 5))|=128K$, $|\mathcal{D}(\mathcal{T}_{tra}(n_s\in[32,64], 5))|=128K$, and $|\mathcal{D}(\mathcal{T}_{tra}(n_s\in[64,128], 5))|=128K$, with $T=8K$ (totaling $3B$ steps). Following a curriculum procedure, each dataset is trained for an additional 3 epochs.
\item \emph{Stage 3}: Extending the ICL horizon by sampling $|\mathcal{D}(\mathcal{T}_{tra}(n_s\in[64, 128], 5))|=72K$, which includes $64K$ trajectories with $T=64K$ and an additional $8K$ trajectories with $T=1,024K$ (totaling $12B$ steps).
\end{itemize}
We primarily pretrain using $8$ Nvidia Tesla A100 cards. For GSA, we employ a batch size of $4$, with $4K$ steps per segment (chunk). For the Transformer, a batch size of $2$ is used, with $600$ steps per segment (chunk), yielding a segment length of $6K$ for OmniRL, which is constrained by memory limitations. We utilize an AdamW optimizer with a learning rate that decays from a peak value of $2e-4$. The average time cost per iteration for trajectories with $T=8K$ is $3.5$ seconds when using GSA, and $10$ seconds for the Transformer.

\begin{algorithm}[htbp]
   \caption{Evaluation Process}
   \label{alg:data_synthesis}
    \begin{algorithmic}
       \STATE {\bfseries Input:} $\mathcal{T}_{tst}$, collection of demonstration trajectories $\mathcal{H}_0=\{h_0\}$, 
       \STATE {\bfseries Set:} $\mathcal{S}=\emptyset$
       \FOR{$\tau \in \mathcal{T}_{tst}$}
            \STATE {\bfseries Set:} $R_{max}$=Average Episodic Reward of $\mathcal{O}_{0.994}$, $R_{min}$=Average Episodic Reward of $\mathcal{R}(c\mathbf{1})$, $\mathcal{S}_{\tau}=[]$
            \REPEAT
                \STATE {\bfseries Retrieving:} $h_0$ from $\mathcal{H}_0$ according to $\tau$
                \STATE {\bfseries Reset:} $\tau$ and obtain $s_1$, $R=0$
                \REPEAT
                    \STATE {\bfseries Sample:} $a^{(b)}_t \sim \pi_{\theta}$ according to \cref{eq:pi_theta} by setting $p^{(r)}=$``O3'' ($\mathcal{O}_{0.994}$)
                    \STATE {\bfseries Execute:} $a^{(b)}_t$ in $\tau$ and obtain $s_{t+1}$, $r_t$
                    \STATE {\bfseries Set:} $p^{(b)}_t=$``UNK''
                    \STATE {\bfseries Set:} $h_{t}=h_{t-1} \oplus [s_t, p^{(b)}_t,a^{(b)}_t,r_t] $
                    \STATE {\bfseries Set:} $R=R+r_t, t=t+1$
                \UNTIL{Episode is over}
                \STATE {\bfseries Calculate and record:} normalized performance $\mathcal{S}_{\tau}=\mathcal{S}_{\tau} \oplus [\frac{R - R_{min}}{R_{max} - R_{min}}]$
            \UNTIL{$t \ge T$}
            \STATE {\bfseries Record:} $\mathcal{S} = \mathcal{S} \cup \mathcal{S}_{\tau}$
       \ENDFOR
        \STATE {\bfseries Return:} $\mathcal{S}$
    \end{algorithmic}
\end{algorithm}

\begin{figure}[h]
    \centering
    
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \begin{equation}
            \text{reward} = \begin{cases}
                1, & \text{if reach goal} \\
                -1, & \text{if reach hole} \\
                0, & \text{otherwise}
            \end{cases} \nonumber
        \end{equation}
        \caption{FrozenLake-v1(slippery)}
        \label{fig:reward_shaping_lake_slippery}
    \end{subfigure}
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \begin{equation}
            \text{reward} = \begin{cases}
                1, & \text{if reach goal} \\
                -1, & \text{if reach hole} \\
                -0.05, & \text{otherwise}
            \end{cases} \nonumber
        \end{equation}
        \caption{FrozenLake-v1(not slippery)}
        \label{fig:reward_shaping_lake_not_slippery}
    \end{subfigure}
    
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \begin{equation}
            \text{reward} = \begin{cases}
                1, & \text{if reach goal} \\
                -1, & \text{if reach cliff} \\
                -0.03, & \text{otherwise}
            \end{cases} \nonumber
        \end{equation}
        \caption{CliffWalking-v0}
        \label{fig:reward_shaping_cliff}
    \end{subfigure}
    \begin{subfigure}{0.45\columnwidth}
        \centering
        \begin{equation}
            \text{reward} = \max\left(\frac{\text{reward}}{30} + 0.1, -0.1\right) \nonumber
        \end{equation}
        \caption{Pendulum-v1}
        \label{fig:reward_pendulum}
    \end{subfigure}

    \begin{subfigure}{0.45\columnwidth}
        \begin{equation}
            \text{agent reward} = \begin{cases}
                1, & \text{if reach goal} \\
                0.08, & \text{if distance to goal decrease} \\
                -0.12, & \text{if distance to goal increase } \\
                -0.04, & \text{if still } \\
                0, & \text{if finish} 
            \end{cases} \nonumber
        \end{equation}
        
        \begin{equation}
            \text{shared reward} = \sum_{i=1}^{2} \text{agent reward}_i \nonumber
        \end{equation}
        \caption{Switch}
        \label{fig:reward_shaping_switch}
    \end{subfigure}
\caption{Reward shaping}
\label{fig:reward_shaping}
\end{figure}

\textbf{Evaluation}. Since the episode length and baseline average episodic reward vary significantly across different tasks, we normalize the episodic reward using the oracle policy (O3) and the uniform random policy ($\mathcal{R}(c \mathbf{1} )$). This normalization represents the percentage of oracle performance achieved. For AnyMDP, the evaluation averages the performances over $32$ variant unseen tasks. For Gymnasium tasks, the evaluation is conducted by averaging the results over $3$ runs on the same task.

The rewards of AnyMDP environments typically lie between $[-1, 1]$, therefore, we reshape the rewards of gym environments into this interval as well, as shown in \cref{fig:reward_shaping}. OmniRL (Stage-3) supports $n_s \leq 128$ and $n_a \leq 5$. For environments with $n_a < 5$, we simply map the output action from OmniRL by setting $a=a \% n_a$.

\section{Additional Results}

\subsection{Curriculum learning for meta-training}
\cref{fig:convergence} illustrates the performance of different methods during meta-training (Stage-1). We observed faster convergence when using the multi-$\gamma$ reference policy.
\cref{fig:curriculum_learning} compares the initial stage of meta-training (Stage-2) by starting either from Stage-1 or a cold start. These comparisons highlight the importance of incrementally increasing $n_s$ as the process of curriculum learning.

\subsection{Average validation score of stage-1 meta-training} \label{sec:add_res_validation}
\cref{table:validation_stage_1} presents the average validation scores of different models trained on different datasets generated by the strategies outlined in \cref{table:icrl_data_synthesis}.
Since the reference policy used to generate AD and AD$^\epsilon$ involves random exploration, which is not meaningful to validate, we primarily compare their performance on the validation sets generated by DPT and OmniRL.
The multi-$\gamma$ approach did not yield significant improvements in the final validation. This finding is consistent with the conclusions presented in Section \cref{sec:results_base}. Although the multi-$\gamma$ method converged faster than other approaches, it did not demonstrate a clear advantage in the final evaluation.


\begin{table}[t]
\caption{Comparing average validation score ($\mathcal{L} = \mathbb{E}_t(\mathcal{L}_t)$) of stage-1 training (GSA model)}
\label{table:validation_stage_1}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{llcccc}
\toprule
\multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Training Dataset}} & \multicolumn{4}{c}{\textbf{Validating Dataset}}  \\
 & & DPT & OmniRL  \\
\midrule
AD & AD & 1.344 & 0.907 \\
AD$^{\epsilon}$ & AD$^{\epsilon}$ & 1.172 & 0.915 \\
DPT & DPT & 0.525 & 0.306 \\
OmniRL (w/o a priori) & OmniRL & 0.281  & 0.250 \\
OmniRL & OmniRL & 0.252 & 0.077 \\
OmniRL & OmniRL (Multi-$\gamma$) & 0.245 & 0.095 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{figure*}[htbp]
\vskip 0.2in
\begin{center}
\begin{subfigure}[b]{0.245\columnwidth}
    \centering 
    \includegraphics[width=0.98\textwidth]{img/convergence_stage_1.pdf}
    \caption{} 
    \label{fig:convergence}
\end{subfigure}
\begin{subfigure}[b]{0.245\columnwidth}
    \centering 
    \includegraphics[width=0.98\textwidth]{img/convergence_stage_2.pdf}
    \caption{} 
    \label{fig:curriculum_learning}
\end{subfigure}
\begin{subfigure}[b]{0.245\columnwidth} 
    \centering
    \includegraphics[width=0.98\textwidth]{img/static_evaluation_5.pdf} 
    \caption{}
    \label{fig:gsa_ablation}
\end{subfigure}
\begin{subfigure}[b]{0.245\columnwidth} 
    \centering
    \includegraphics[width=0.98\textwidth]{img/static_evaluation_1.pdf} 
    \caption{}
    \label{fig:entropy}
\end{subfigure}
\vspace{-15pt}
\caption{(a) Training loss of stage-1 meta-training against the iteration. (b) Training loss of stage-2 meta-training of OmniRL by starting from stage-1 and cold start. (c) Validation of GSA performance with varying model hyper-parameters. (d) The position-wise entropy when validating GSA (stage-3) on different datasets. }
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{Impact of number of memory slots on GSA Model Performance}
\cref{fig:gsa_ablation} illustrates the performance of the GSA model on the validating dataset $\mathcal{D}(\mathcal{T}_{tst}(128, 5))$ while varying its hyper-parameters. Notably, reducing the network depth from $18$ to $9$ layers results in a significant performance drop. However, an even more substantial gap is observed when the number of memory slots in GSA is decreased. This finding reinforces the conclusion drawn by \citet{wang2022evolving,kirsch2022general} that the scale of the memory is crucial for in-context learning (ICL) ability.

\subsection{Automatic trade-off between exploration and exploitation}\label{sec:entropy}

Previous studies have noted that in-context reinforcement learning (ICRL) can automatically balance exploration and exploitation. This phenomenon has been theoretically linked to posterior sampling. In \cref{fig:entropy}, we illustrate the entropy of the decision-making process as a function of steps within the context. When compared to \cref{fig:static_eval_sub1}, we observe that the decrease in loss ($\mathcal{L}_t$) is primarily driven by the reduction in the entropy of the policy. Specifically, the agent initially assigns equal probabilities to all possible actions, reflecting an exploratory phase. As more contextual information accumulates, the agent gradually converges its choices, thereby transitioning towards exploitation. This empirical finding suggests that imitating an optimal policy (oracle) is sufficient to achieve an automatic balance between exploration and exploitation.

\subsection{Additional Evaluation on Gymnasium}

\begin{figure*}[h]
\vskip 0.15in
\begin{center}
    \includegraphics[width=1\textwidth]{img/gym_addition.pdf}
\vspace{-20pt}
\caption{Evaluation results of OmniRL (Stage-3) on gymnasium environments of FrozenLake-v1 and Pendulum-v1 with different settings, demonstrating the model's online-RL capabilities toward diverse environments.}
\label{fig:gym_addition}
\end{center}
\vskip 0.15in
\vspace{-20pt}
\end{figure*}

\cref{fig:gym_addition} presents additional evaluation results for OmniRL (Stage-3) on the FrozenLake-v1 (non-slippery) and Pendulum-v1 ($g=1.0, 9.8$) environments. 
These results demonstrate that OmniRL is robust to changes in environment configurations. However, in the Pendulum-v1 ($g=9.8$), OmniRL failed to achieve satisfactory results within 200 episodes. This scenario is notably more challenging compared to the configurations with $g=1.0$ and $g=5.0$.

\subsection{Can mainstream LLMs do ICRL?}

\begin{figure}
\begin{mdframed}[roundcorner=10pt]
You are playing the Frozen Lake game. The environment is a 4x4 grid where you need to maximize the success rate by reaching the goal (+1) without falling into holes (-1). You can move in four directions: left, down, right, and up (represented as 0, 1, 2, 3 respectively). You will receive the current state and need to provide the optimal action based on your learning.  When asked for the optimal action, your response must be an integer ranging from 0 to 3, and no other context is permitted. There are two kind of request types:

1.integer: the integer is the current state, and you need to provide the optimal action.

2.list: The list contains one or more tuples, where each tuple contains the last state, action taken, reward received, and next state. To save time, you don't need to respond when receiving a list.

You will play the game multiple times. A game ends when the reward is -1 or 1, try to get a higher success rate.

Note: I am asking you to play this game, not to find a coding solution or method.

You will be provided with a conversation history. The latest prompt is the current state, and others are the list of sequential environment feedback history in tuple type. Each tuple contains four values, the first one is state, the second one is action, the third one is reward and the fourth one is next state.

Your response must be an integer from 0 to 3 during the entire chat.

If you find the last state is equal to the next state, your policy in the last state can't be this action.

If you find the reward in the tuple is -1, your policy in the last state can't be this action.

You need to get to the goal as soon as possible.
\end{mdframed}
\caption{Prompts for LLM to initialize the {Lake$4\times4$ (Slippery)} task without a global map}
\label{page:prompts_without_map}
\end{figure}

\begin{figure}
\begin{mdframed}[roundcorner=10pt]
There is a game with the following basic description and rules:

Frozen Lake involves crossing a frozen lake from the start to the goal without falling into any holes by walking over the frozen lake. The player may not always move in the intended direction due to the slippery nature of the frozen lake.

The game starts with the player at location [0,0] of the frozen lake grid world, with the goal located at the far extent of the world, for example, [3,3] for the 4x4 environment.

Holes in the ice are distributed in set locations when using a pre-determined map or in random locations when a random map is generated.

The player makes moves until they reach the goal or fall into a hole.

The lake is slippery, so the player may move perpendicular to the intended direction sometimes.

If the intended direction is to the left, the actual move may be to the left, up, or down, with the corresponding probability distribution: P(move left) = 1/3, P(move up) = 1/3, P(move down) = 1/3.
If the intended direction is to the right, the actual move may be to the right, up, or down, with the corresponding probability distribution: P(move right) = 1/3, P(move up) = 1/3, P(move down) = 1/3.
If the intended direction is up, the actual move may be up, left, or right, with the corresponding probability distribution: P(move up) = 1/3, P(move left) = 1/3, P(move right) = 1/3.
If the intended direction is down, the actual move may be down, left, or right, with the corresponding probability distribution: P(move down) = 1/3, P(move left) = 1/3, P(move right) = 1/3.
You are given a 4x4 map where:

S represents the start.

F represents the frozen surface that can be walked on.

H represents a hole; falling into it will return the player to the start.

G represents the goal.

The map is as follows:

The first row from left to right is "SFFF".

The second row from left to right is "FHFH".

The third row from left to right is "FFFH".

The fourth row from left to right is "HFFG".

Please determine the optimal policy that maximizes the success rate of safely reaching the goal from the start. The optimal policy is the intended direction at each map location, where actions 0, 1, 2, and 3 represent moving left, down, right, and up, respectively.

Note: You are not required to write code to solve this problem; instead, directly provide the optimal policy.
\end{mdframed}
\caption{Prompts for LLM to solve Lake$4\times4$ (Slippery) with global map}
\label{page:prompts_with_map}
\end{figure}

\begin{table}[t]
\caption{Performance of LLM in FrozenLake-v1.}
\label{table:llm_icrl}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l|rr}
\toprule
 & non-slippery & slippery \\
\midrule
w/ global map & $\checkmark$ & $\times$ \\
w/ state only & $\times$ & $\times$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

We also investigate whether a well-pretrained LLM can naturally solve some of the tasks. To circumvent the lack of common sense in AnyMDP tasks, we primarily conducted tests in the $Lake$ environment. We tested $ollama$ \footnote{\url{http://ollama.com/}, version 3.3 with $70B$ parameters, over $1000\times$ parameters of OmniRL} in two modes:
\begin{enumerate}
\item Similar to the evaluation of OmniRL, we do not provide the agent with the map. Instead, we report only the state ID and reward of the agent. The initial prompts used to initiate the evaluation are shown in \cref{page:prompts_without_map}.
\item We initially provide the global map to the $ollama$ and then commence the interaction. In this mode, the LLM can leverage the global map to make decisions. The prompts are shown in \cref{page:prompts_with_map}.
\end{enumerate}

As shown in \cref{table:llm_icrl}, LLM Agents can only solve the non-slippery Lake$4\times4$ task relatively well when provided with a global map. Without a global map, we conducted interactions between LLM agents and environments for up to $500$ episodes ($100K$ steps). However, the agents ultimately failed to solve even the non-slippery tasks, achieving scores that were close to those of a random policy. 

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.