\section{Related Work}
\subsection{Meta-Learning for In-Context Learning}
Meta-learning, also known as learning to learn ____, pertains to a category of approaches that prioritize the acquisition of generalizable adaptation skills across a spectrum of tasks. It encompasses a broad array of methodologies, including the optimization of gradient-based methods ____ and model-based meta-learning ____. As the typical setting of model-based meta-RL ____ or ICRL ____, states, actions, and rewards are typically arranged as a trajectory to compose the \emph{inner loop} for task adaptation, while the pre-training and meta-training are recognized as the \emph{outer loop}. Common choices for the outer-loop optimizer include reinforcement learning ____, evolutionary strategies ____, and imitation learning ____. Imitation learning of ICRL is also related to the reinforcement learning coach (RLCoach), which uses pre-trained RL agents to generate demonstrations. RLCoach is not only used for ICRL but is also widely employed to accelerate single-task reinforcement learning ____ and multi-task learning ____.

\subsection{In-Context Learning from Pre-training}
The rise of Large Language Models (LLMs) blurs the boundary between pre-training and meta-training, as pre-training with huge datasets incentivizes ICL in a manner similar to meta-learning ____. For clarity, we use ``pre-training'' to describe training that primarily targets the acquisition of diverse skills, typically followed by a subsequent gradient-based tuning stage. ``Meta-training'' refers to training aimed at acquiring the ability to learn, which does not necessarily require subsequent gradient-based tuning.
The correlation between the ability of ICL and the distribution of pre-training data has been thoroughly investigated ____ recently, indicating that ICL is related to "burstiness," which refers to patterns that exhibit a distributional gap between specific trajectories and the pre-training dataset.
Additionally, the level of "burstiness" may affect the trade-off between ICL and IWL, with non-bursty trajectories stimulating more IWL and less ICL.
Analyses and experiments have been conducted to show that computation-based ICL can exhibit a richer array of behaviors than gradient-based IWL____, particularly in terms of plasticity and continual learning____.
Specifically, depending on the pre-training dataset, ICL can perform either associative generalization or rule-based generalization ____. In many cases, ICL primarily serves as a task identifier ____, where skills are memorized through IWL, and ICL simply invokes the correct one. 
This issue may be prevalent in many meta-learning benchmarks emphasizing few-shot learning, since those methods typically operate within a restricted domain and require re-training across domains. 
It further motivates the need for generalized in-context learning ____, where the acquirement of skill is dominated by ICL instead of IWL.

\textbf{Relations with long chain-of-thought (CoT)}. Recently we have also observed a trend toward increasing the reasoning length in LLMs ____. However, they are quite distinct from the proposed generalized ICL: LLM reasoning emphasizes the ability of system 2 which represents rule-based and analytical thinking, while generalized ICL emphasizes the in-context improvement of system 1 which represents rapid and intuitive decision-making ____. Nonetheless, our research also underscores the importance of exploring long-sequence causal models beyond the Transformer architecture.

\subsection{Benchmarking In-Context Reinforcement Learning}
Meta-learning typically requires a set of related yet diverse tasks. One commonly used technique is to randomize a subset of domain parameters to create these variant tasks. These benchmarks can be broadly categorized as follows:
\circled{1} Randomizing the rewards or targets while keeping the transitions fixed. This includes multi-armed bandits ____, varying goals in a fixed grid world or maze ____, different walking speeds in locomotion tasks ____, and diverse tasks in object manipulation ____. This approach is also closely related to multi-objective reinforcement learning____.
\circled{2} Modifying the transitions while keeping the targets unchanged. Examples include tasks with crippled joints or varying joint sizes in locomotion ____, procedurally generated grid worlds ____ and games ____.
\circled{3} Randomizing the observations and labels without altering the underlying transitions and rewards. Examples include hiding parts of observations ____, randomizing labels ____, and actions ____.
The above approaches typically create a group of tasks that start from a ``seed'' task. These methods are also related to domain randomization ____, which has proven effective in reducing sim-to-real gaps by improving both in- and out-of-distribution generalization____.