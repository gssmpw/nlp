\section{Introduction}
\label{sec:intro}

The ability to accurately identify the characteristics of a host through the analysis of its network traffic is crucial for a variety of tasks in network management and computer security. Accurately identifying a machine’s Operating System (OS) family and version is critical for applications including vulnerability exploitation, network inventory, and the detection of unauthorized devices.

The process of OS fingerprinting entails determining information related to the operating system—such as OS family and version—of a network-connected device by analysing its traffic. These techniques leverage differences arising from the unique ways each OS implements the communication protocol stack. Depending on the approach, OS fingerprinting can be conducted through active or passive scanning. Active scanning involves sending probes to the target and analysing responses, providing speed and reliability at the cost of a higher risk of detection. A widely used tool for this method is Nmap \cite{nmaporg_nmap_nodate}. On the other hand, passive scanning examines existing network traffic without direct interaction with the target, making it a stealthier, though generally slower and less effective, method. Tools such as p0f \cite{zalewski_p0f_nodate} are commonly employed for passive OS detection.

Traditional rule-based approaches, as used by the aforementioned tools, are highly sensitive to variations in machine characteristics. In today’s environment—characterised by a proliferation of connected devices, diverse OSs, and frequent updates—these variations pose significant challenges. An optimal solution would accurately infer a machine's OS even in scenarios where it is newly released, recently updated, or reconfigured. Artificial Intelligence (AI) techniques have demonstrated significant potential in addressing these challenges. Numerous studies have explored the application of AI to OS fingerprinting in recent years \cite{lastovicka_passive_2023}, though most employ classical methods such as typical Machine Learning (ML) algorithms. Research on more advanced techniques, specifically Deep Learning (DL) architectures, remains limited.

\paragraph{Research Problem.} Current OS fingerprinting methods, largely based on traditional rule-based or classical ML approaches, struggle to adapt to the heterogeneity and dynamism of modern network environments. This study seeks to address the problem of how to improve OS fingerprinting accuracy and robustness by leveraging advanced DL architectures—specifically, Transformer-based models designed for tabular data. By doing so, we aim to overcome the limitations of existing methods and provide a solution that is more resilient to evolving network conditions and modern OS variations.

Among recent DL architectures, the Transformer, introduced by Vaswani et al. in 2017 \cite{vaswani_attention_2023}, stands out for its ability to process sequential data efficiently through parallel processing and self-attention mechanisms. This architecture has revolutionised Natural Language Processing (NLP) with the emergence of Large Language Models (LLMs) and has been successfully adapted to other domains, including computer vision with the Vision Transformer (ViT) \cite{dosovitskiy_image_2021}. Its scalability and capacity for generalisation suggest that applying Transformer-based models to OS fingerprinting could yield similar breakthroughs.

A key advantage of the application of Transformers to network traffic data is their ability to capture complex interdependencies. Unlike traditional ML methods that focus on isolated features or require manual importance measures, Transformers use self-attention to process the entire data structure at once. This enables them to dynamically weigh each feature’s contribution, capturing nuanced interactions essential for characterising heterogeneous, dynamic traffic, and ultimately leads to improved OS fingerprinting performance.

\paragraph{Contribution.} In this paper, we propose the application of the Transformer architecture to OS fingerprinting. Given that network traffic data is typically stored as network flows (which can be processed as tabular data), we specifically employ a variant designed for this format: the Tabular Transformer. We analyse two variations of this architecture, namely the TabTransformer (TabT) and the FT-Transformer (FT-T), both optimised for structured tabular data processing.

To rigorously evaluate our approach, we apply it to three publicly available datasets with distinct characteristics that enable classification at multiple granular levels (OS family, major, and minor versions) under different network conditions and feature distributions. We benchmark our models against three representative ML algorithms—k-Nearest Neighbours (kNN), Random Forest (RF), and Multi-Layer Perceptron (MLP)—and compare our results with previous AI-based studies. 

This study makes three key contributions:  
\begin{itemize}  
    \item \textbf{First application of the Transformer architecture to OS fingerprinting:} We introduce attention-based DL models for OS identification, marking the first attempt to apply Transformers—via their adaptation in Tabular Transformers—to this domain.  
    \item \textbf{Comprehensive evaluation across multiple datasets and classification granularities:} We rigorously assess our approach using three publicly available datasets with diverse characteristics, evaluating OS classification at different levels (family, major, and minor versions) and benchmarking against classical ML models and prior research.  
    \item \textbf{Reproducibility and transparency of experiments:} We promote further research by publicly releasing our experimental code under a GNU GPL v3.0 license. This includes preprocessing steps, model implementations, and evaluation metrics, available at: \url{https://github.com/rubenpjove/tabularT-OS-fingerprinting}.  
\end{itemize}  

This paper is structured as follows: \Cref{sec:background} provides essential context on OS fingerprinting, ML models and the Transformer architecture; \Cref{sec:related-work} reviews previous works in both traditional and AI-based OS fingerprinting, including the use of Transformer to other network-related tasks; \Cref{sec:materials-methods} details the experimental design and datasets used; \Cref{sec:results-discussion} presents the results and comparisons with existing approaches; and \Cref{sec:conclusion_future_work} summarises the findings and outlines future research directions.

\clearpage