\section{Materials \& Methods}
\label{sec:materials-methods}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsection{Datasets}
\label{subsec:datasets}

For evaluating OS fingerprinting methods, we selected publicly available benchmark datasets that meet essential criteria for evaluating OS fingerprinting methods. Specifically, datasets must have sufficient size, diversity, and accurate OS labelling, and given the rapid evolution of OSs, they need to be up-to-date. Our selection process was informed by their adoption in recent studies and an analysis of prior works (Section~\ref{sec:related-work}), ensuring both relevance and comparability. Moreover, these datasets capture network traffic at various abstraction levels—from high-level flow summaries to detailed packet-level data—and provide OS labels at multiple granular levels (family, major, and minor versions).  

We selected three representative datasets—\texttt{DAT1}, \texttt{DAT2}, and \texttt{DAT3}—that capture diverse network features essential for robust OS fingerprinting. In our review of prior works, we analysed available features like packets, telemetry, and logs to identify datasets that are both recent and reflective of realistic network environments with current OS versions and modern traffic dynamics. These datasets encompass diverse data types (flow-level records, packet captures, and active OS fingerprinting signatures), offer multiple levels of OS granularity (family, major, and minor versions), vary in size, and originate from different network settings. This diversity underpins a robust evaluation of AI-based OS fingerprinting models by mitigating dataset bias and enhancing generalisability.

Three complementary data types—IPFIX, PCAP, and OS signature databases— were employed to comprehensively analyse network traffic for OS fingerprinting. Specifically, IPFIX (Internet Protocol Flow Information Export) provides flow-level metadata summarizing network activity; PCAP (Packet CAPture) retains raw network packets to capture detailed protocol behavior; and OS signature databases, such as those used by Nmap, offer predefined OS fingerprints as classification references. Together, these sources enable a multifaceted analysis that integrates both high-level traffic patterns and in-depth protocol details.

\clearpage

We defined OS classification tasks at three granularity levels—\texttt{family}, \texttt{major}, and \texttt{minor}—to capture varying levels of detail. Specifically, the \texttt{family} level includes broad categories (e.g., Windows, Linux, Android), the \texttt{major} level distinguishes versions (e.g., Windows 10, Android 9, iOS 13), and the \texttt{minor} level provides finer distinctions (e.g., Windows 8.1, iOS 13.5, macOS 10.15). For instance, a detailed classification might separate \texttt{Ubuntu} into \texttt{22.04 (Jammy Jellyfish)} and further into \texttt{22.04.3 LTS}, whereas a less detailed approach would label it simply as \texttt{Ubuntu}. Higher granularity, particularly at the \texttt{minor} level, increases classification complexity due to a larger number of classes and fewer training examples per class. The characteristics of the selected datasets are further detailed in the following points and summarized in \Cref{tab:datasets}.

\begin{table}[t!]
\centering
\caption{Overview of the employed datasets}
\label{tab:datasets}
\begin{tblr}{
  width = \linewidth,
  colspec = {Q[50]Q[37]Q[40]Q[54]Q[38]Q[50]Q[35]Q[35]Q[35]Q[35]Q[65]Q[75]Q[50]},
  cells = {c},
  row{4} = {my_grey}, % This line sets the background color for the middle row
  row{5} = {my_grey}, % This line sets the background color for the middle row
  row{6} = {my_grey}, % This line sets the background color for the middle row
  cell{1}{1} = {r=2}{},
  cell{1}{2} = {r=2}{},
  cell{1}{3} = {r=2}{},
  cell{1}{4} = {r=2}{},
  cell{1}{5} = {c=6}{0.247\linewidth},
  cell{1}{11} = {r=2}{},
  cell{1}{12} = {r=2}{},
  cell{1}{13} = {r=2}{},
  cell{4}{1} = {r=3}{},
  cell{4}{2} = {r=3}{},
  cell{4}{3} = {r=3}{},
  cell{4}{4} = {r=3}{},
  cell{4}{5} = {r=3}{},
  cell{4}{6} = {r=3}{},
  cell{4}{7} = {r=3}{},
  cell{4}{8} = {r=3}{},
  cell{4}{9} = {r=3}{},
  cell{4}{10} = {r=3}{},
  cell{4}{11} = {r=3}{},
  cell{7}{1} = {r=2}{},
  cell{7}{2} = {r=2}{},
  cell{7}{3} = {r=2}{},
  cell{7}{4} = {r=2}{},
  cell{7}{5} = {r=2}{},
  cell{7}{6} = {r=2}{},
  cell{7}{7} = {r=2}{},
  cell{7}{8} = {r=2}{},
  cell{7}{9} = {r=2}{},
  cell{7}{10} = {r=2}{},
  cell{7}{11} = {r=2}{},
  hline{1,3,9} = {-}{},
}
\textbf{Dataset} & \textbf{Year} & \textbf{Works} & \textbf{Data Type} & \textbf{Feature Count} &  &  &  &  &  & \textbf{Row Count} & \textbf{Granularity} & \textbf{Classes Count}\\
 &  &  &  & \textbf{Total} & \textbf{TCP/IP} & \textbf{DNS} & \textbf{HTTP} & \textbf{TLS} & \textbf{Other} &  &  & \\
\texttt{DAT1} \cite{martin_dataset_2019} & 2019 & \cite{lastovicka_usingTLS_2020} & IPFIX & 29 & 7 & - & 5 & 8 & 9 & 18,708,983 & \texttt{family} & 5\\
\texttt{DAT2} \cite{lastovicka_dataset_2023} & 2023 & \cite{lastovicka_passive_2023, hulak_evaluation_2023} & IPFIX & 112 & 35 & - & 7 & 28 & 46 & 109,663 & \texttt{family} & 12\\
 &   &   &   &   &   &   &   &   &   &   & \texttt{major} & 50\\
  &   &   &   &   &   &   &   &   &   &   & \texttt{minor} & 88\\
\texttt{DAT3} \cite{noauthor_nmap794-osdb} & 2023 & \cite{perez-jove_applying_2021, perez_jove_towards_2024} & DB & 263 & 263 & - & - & - & - & 38,817 & \texttt{family} & 7\\
 &  &  &  &  &  &  &  &  &  &  & \texttt{minor} & 91
\end{tblr}
\end{table}

\begin{itemize}

\item \texttt{\textbf{DAT1}}: \texttt{\textbf{lastovicka\_2019\_UsingTLS}} \cite{martin_dataset_2019}  

This dataset comprises flow records from the Czech Republic Masaryk university's backbone network, enriched with log entries from Dynamic Host Configuration Protocol (DHCP) servers and a Remote Authentication Dial-In User Service (RADIUS) accounting server. Collected between July 12 and 16, 2019, it focuses on flows originating from the university's Eduroam wireless networks. OS labels are derived from DHCP logs and RADIUS session IDs.  

Useful features for OS fingerprinting include basic flow attributes, extended TCP/IP parameters, HTTP user-agent strings, and TLS client details. The dataset, anonymized using the Crypto-PAn algorithm, spans 18.7 million rows with 29 features, primarily supporting OS family-level classification with five classes.

\item \texttt{\textbf{DAT2}}: \texttt{\textbf{lastovicka\_2023\_PassiveOSRevisited}} \cite{lastovicka_dataset_2023}  

This dataset captures web traffic from five Masaryk university servers hosting 475 domains over eight hours. OS labels are derived from HTTP User-Agent strings in web server logs, cross-referenced with network flow data. Collected connections include devices such as user computers, mobile phones, and web crawlers.  

OS fingerprinting-related features include IP and TCP parameters, HTTP and TLS details, among others, amounting to 112 features in total. The dataset includes 109,663 rows, enabling OS classification at family (12 classes), major (50 classes), and minor (88 classes) levels.

\item \texttt{\textbf{DAT3}}: \texttt{\textbf{nmap-7.94\_2023\_OSdb}} \cite{noauthor_nmap794-osdb}  

This dataset consists of OS signatures actively collected using the Nmap tool (version 7.94). Nmap identifies OSs by sending 16 TCP, User Datagram Protocol (UDP), and Internet Control Message Protocol (ICMP) probes and analysing responses. Features include TCP window sizes, sequence generation, and TCP options, providing detailed fingerprinting data.  

The dataset contains 38,817 rows with 263 features and supports fine-grained OS classification at family (7 classes) and minor (91 classes) levels. Its active collection process complements the passive data in \texttt{DAT1} and \texttt{DAT2} by capturing precise protocol behaviours.

\end{itemize}

Class distribution analysis revealed significant imbalances across all datasets, meaning that some classes contain far more examples than others. This imbalance, illustrated in \Cref{fig:class-distribution}, introduces challenges for training, as models may become biased toward majority classes. Addressing this imbalance is critical to achieving robust and fair performance across all tasks, and will be discussed in Section~\ref{subsec:data-preparation}.

\clearpage

\begin{figure}[t!]
    \centering
    % First row: Two subfigures side by side
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/DE-Dat1-classes_family.png}
        \caption{\texttt{DAT1}}
        \label{fig:class-distribution-1a}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/DE-Dat2-classes_family.png}
        \caption{\texttt{DAT2}}
        \label{fig:class-distribution-1b}
    \end{subfigure}

    % Second row: One subfigure centered
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/DE-Dat3-classes_family.png}
        \caption{\texttt{DAT3}}
        \label{fig:class-distribution-1c}
    \end{subfigure}

    \caption{Classes distribution by OS \texttt{family} version in \texttt{DAT1} (\textit{top-left}), \texttt{DAT2} (\textit{top-right}), and \texttt{DAT3} (\textit{bottom-centre}).}
    \label{fig:class-distribution}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsection{Data Preparation}
\label{subsec:data-preparation}

Data preparation is the process of cleaning, transforming, and balancing datasets to ensure robust model performance and reproducibility. In our study, this involves handling missing or invalid data, removing irrelevant features, and addressing class imbalances—each with fixed parameters and random seeds to guarantee full reproducibility.

\paragraph{Handling Missing Data and Redundancies.} We improve data quality by systematically addressing missing values and redundant entries. In the datasets, Not a Number (NaN) or Null values—indicating undefined or unrepresentable quantities (e.g., missing TLS features in unencrypted flows)—are encoded categorically when applicable; rows with missing values in critical columns are removed, and numerical features with zero variance (i.e., variance $\leq 0$) are dropped. Duplicate entries are also removed to avoid redundancy, and no other errors or infinite values were detected.

\paragraph{Removing Irrelevant Features.} Irrelevant columns are removed to focus on OS fingerprinting. For example, columns related to timestamps or non-OS network information—such as \texttt{Date flow start} and \texttt{Session ID} in \texttt{DAT1}—are excluded, with similar removals performed in \texttt{DAT2} and \texttt{DAT3}. The precise selection of retained features is provided in Table~\ref{tab:selected-features}, along with explicit lists of both categorical and numerical features to standardize the preprocessing pipeline.

\paragraph{Addressing Class Imbalances.} We mitigate class imbalances by standardizing target classes and applying resampling techniques. Target classes are first standardized via regular expression matching (e.g., mapping entries containing 'iOS', 'Android', 'Mac OS X', and 'Windows' to their respective labels). Then, random undersampling is applied to majority classes using predetermined removal percentages (with fixed random seeds), while the Synthetic Minority Over-sampling Technique (SMOTE) is employed with a sampling strategy of \texttt{'auto'} to generate synthetic samples for minority classes:
\begin{enumerate}
    \item \textbf{Random Undersampling:} Applied to majority classes when sufficient data is available.
    \item \textbf{SMOTE:} Employed to generate synthetic samples for minority classes.
\end{enumerate}

\paragraph{Feature Processing and Encoding.} Feature processing further refines both categorical and numerical data for improved interpretability and balanced training. Categorical columns containing hexadecimal strings are split into individual bytes, and One-Hot Encoding is applied to categorical target variables—converting them into a numerical format by creating binary columns for each class, after which the original target column is removed. Additionally, class weights are computed as the inverse of normalized class frequencies, and data is split into training and test sets using stratified sampling (with 20\% reserved for testing).

\paragraph{Reproducibility and Software Environment.} Reproducibility is ensured by using fixed versions of key libraries and by sharing the complete code. Our experiments rely on \texttt{numpy==1.23.0}, \texttt{pandas==2.2.2}, \texttt{scikit-learn==1.5.0}, \texttt{torch==2.3.1}, and \texttt{optuna==3.6.1} for data manipulation, model training, and hyperparameter optimization. The full code implementing these steps is available at \url{https://github.com/rubenpjove/tabularT-OS-fingerprinting}.


\begin{table}[t!]
  \centering
  \caption{Selected Features for Each Dataset}
  \vspace{5pt}
  \begin{tabularx}{\linewidth}{>{\centering\arraybackslash}m{1cm} >{\centering\arraybackslash}X >{\centering\arraybackslash}X}
    \hline
    \textbf{Dataset} & \textbf{Numerical Features} & \textbf{Categorical Features} \\ \hline
    \texttt{DAT1} & {\small \texttt{TLS Client Version}, \texttt{Client Cipher Suites}, \texttt{TLS Extension Types}, \texttt{TLS Extension Lengths}, \texttt{TLS Elliptic Curves}, \texttt{TLS EC Point Formats}} & {\small \texttt{SYN size}, \texttt{TCP win}, \texttt{TCP SYN TTL}} \\ 
    \rowcolor{my_grey} \texttt{DAT2} & {\small \texttt{TCP flags A}, \texttt{TLS\_CONTENT\_TYPE}, \texttt{TLS\_HANDSHAKE\_TYPE}, \texttt{TLS\_CIPHER\_SUITE}, \texttt{TLS\_CLIENT\_VERSION}, \texttt{TLS\_CIPHER\_SUITES}, \texttt{TLS\_CLIENT\_SESSION\_ID}, \texttt{TLS\_EXTENSION\_TYPES}, \texttt{TLS\_CLIENT\_KEY\_LENGTH}, \texttt{TLS\_EXTENSION\_LENGTHS}, \texttt{TLS\_ELLIPTIC\_CURVES}, \texttt{TLS\_EC\_POINT\_FORMATS}, \texttt{IPv4DontFragmentforward}, \texttt{tcpOptionWindowScaleforward}, \texttt{tcpOptionSelectiveAckPermittedforward}, \texttt{tcpOptionNoOperationforward}, \texttt{flowEndReason}, \texttt{TLS\_JA3\_FINGERPRINT}, \texttt{IP ToS}} & {\small \texttt{SRC port}, \texttt{TCP SYN Size}, \texttt{TCP Win Size}, \texttt{TCP SYN TTL}, \texttt{NPM\_CLIENT\_NETWORK\_TIME}, \texttt{NPM\_ROUND\_TRIP\_TIME}, \texttt{NPM\_RESPONSE\_TIMEOUTS\_A}, \texttt{NPM\_TCP\_RETRANSMISSION\_A}, \texttt{NPM\_TCP\_OUT\_OF\_ORDER\_A}, \texttt{NPM\_JITTER\_DEV\_A}, \texttt{NPM\_JITTER\_AVG\_A}, \texttt{NPM\_JITTER\_MIN\_A}, \texttt{NPM\_JITTER\_MAX\_A}, \texttt{NPM\_DELAY\_DEV\_A}, \texttt{NPM\_DELAY\_AVG\_A}, \texttt{NPM\_DELAY\_MIN\_A}, \texttt{NPM\_DELAY\_MAX\_A}, \texttt{NPM\_DELAY\_HISTOGRAM\_1\_A}, \texttt{TLS\_SETUP\_TIME}, \texttt{tcpOptionMaximumSegmentSizeforward}} \\ 
    \texttt{DAT3} & {\small \texttt{SEQ.SP}, \texttt{SEQ.GCD}, \texttt{SEQ.ISR}, \texttt{SEQ.TI}, \texttt{SEQ.CI}, \texttt{SEQ.II}, \texttt{SEQ.TS}, \texttt{WIN.W*}, \texttt{ECN.T}, \texttt{ECN.TG}, \texttt{ECN.W}, \texttt{T*.T}, \texttt{T*.TG}, \texttt{T*.RD}, \texttt{T*.W}, \texttt{U1.T}, \texttt{U1.TG}, \texttt{U1.IPL}, \texttt{U1.UN}, \texttt{U1.RIPL}, \texttt{U1.RID}, \texttt{U1.RUCK}, \texttt{IE.T}, \texttt{IE.TG}, \texttt{IE.CD}} & {\small \texttt{SEQ.TI}, \texttt{SEQ.CI}, \texttt{SEQ.II}, \texttt{SEQ.SS}, \texttt{SEQ.TS}, \texttt{OPS.O1}, \texttt{OPS.O2}, \texttt{OPS.O3}, \texttt{OPS.O4}, \texttt{OPS.O5}, \texttt{OPS.O6}, \texttt{ECN.R}, \texttt{ECN.DF}, \texttt{ECN.O}, \texttt{ECN.CC}, \texttt{ECN.Q}, \texttt{T*.R}, \texttt{T*.DF}, \texttt{T*.S}, \texttt{T*.A}, \texttt{T*.F}, \texttt{U1.R}, \texttt{U1.DF}, \texttt{U1.RIPL}, \texttt{U1.RID}, \texttt{U1.RIPCK}, \texttt{U1.RUCK}, \texttt{U1.RUD}, \texttt{IE.R}, \texttt{IE.DFI}, \texttt{IE.CD}} \\ \hline
  \end{tabularx}
  \label{tab:selected-features}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsection{Modelling}
\label{subsec:modelling}

We present a comprehensive, reproducible, and efficient approach to model selection, training, and validation. After preparing each dataset, we first split the data into training and testing sets using stratified sampling to maintain class distribution—this same strategy is applied within cross-validation splits. The training set is used exclusively for model development, while the test set is reserved for final performance evaluation. To obtain optimal hyperparameters for the TabTransformer (TabT) \cite{huang_tabtransformer_2020} and FT-Transformer (FT-T) \cite{gorishniy_revisiting_2023}, we conducted a hyperparameter search using Optuna's NSGA-II sampler (Table~\ref{tab:hyperparameters}). In each trial, key metrics such as training time, inference time, model parameter count, and memory usage were recorded as user attributes to enhance reproducibility and analysis. Notably, all random seeds are fixed to guarantee reproducibility, and our approach is computationally more efficient than traditional methods like Grid or Random Search.

\begin{table}[t!]
\centering
\caption{Hyperparameters values}
\vspace{5pt}
\begin{tabular}{ccp{6cm}}
\hline
\textbf{Hyperparameter}       & \textbf{Value} & \textbf{Description} \\ \hline
\texttt{learning\_rate}                 & [0.0001 - 0.1]                                      & The range of learning rates used during training. \\ 
{\cellcolor{my_grey}}\texttt{embedding\_dim}                 & {\cellcolor{my_grey}}[16, 32]                                            & {\cellcolor{my_grey}}The dimensionality of the embeddings for categorical features. \\ 
\texttt{depth}                         & [2 - 6]                                             & The range for the number of Transformer layers. \\ 
{\cellcolor{my_grey}}\texttt{heads}                         & {\cellcolor{my_grey}}[2 - 8]                                             & {\cellcolor{my_grey}}The range for the number of attention heads in each transformer layer. \\ 
\texttt{attn\_dropout}                  & [0.05 - 0.5]                                        & The range for the dropout rate for the attention mechanism. \\ 
{\cellcolor{my_grey}}\texttt{ff\_dropout}                    & {\cellcolor{my_grey}}[0.05 - 0.5]                                        & {\cellcolor{my_grey}}The range for the dropout rate for the feedforward network within each Transformer layer. \\
\texttt{use\_shared\_categ\_embed}       & [True, False]                 & Determines whether to use shared embeddings for categorical features. Applicable to TabTransformer only.\\ \hline
\end{tabular}%
\label{tab:hyperparameters}
\end{table}

\paragraph{Hyperparameter Search Trials.} In each trial, we employ Stratified 10-Fold Cross-Validation with resampling techniques to handle class imbalance and optimize model performance. Specifically, data is split into training and validation sets with balanced class distributions across all folds. For class imbalance, random undersampling (with explicit removal percentages for majority classes) and SMOTE (with a fixed random state) are applied. Each fold undergoes 200 training epochs with batch sizes of 128, 256, or 512, and early stopping is triggered after 15 epochs without improvement. Model parameters are updated using the AdamW optimizer and training is guided by Cross-Entropy Loss. This entire process is executed on a Compute Unified Device Architecture (CUDA)-enabled device, utilizing data parallelization and custom Graphics Processing Unit (GPU) memory management (in conjunction with garbage collection) to ensure efficient computations and prevent memory leaks.

\paragraph{Baseline Models.} We also trained several baseline models—k-Nearest Neighbors (kNN), Random Forest (RF), and Multi-layer Perceptron (MLP)—to provide a comparative performance analysis. For these models, the same preprocessing pipeline was applied, except that categorical features were encoded using One-Hot Encoding. Each baseline model was trained with its default hyperparameters (see \autoref{tab:baselines_hyp}) and evaluated using Stratified 10-Fold Cross-Validation, with performance metrics averaged across folds. Detailed descriptions of these baseline models are provided in Section~\ref{subsubsec:ML-baselines}.

\begin{table}[h!]
    \centering
    \caption{Default hyperparameters for baseline ML models}
    \vspace{5pt}
    \begin{tabular}{cc}
    \hline
    \textbf{Model}       & \textbf{Hyperparameters}     \\ \hline
    \multirow{5}{*}{kNN} & \texttt{algorithm}: auto              \\
                         & {\cellcolor{my_grey}}\texttt{leaf\_size}: 30               \\
                         & \texttt{metric}: minkowski            \\
                         & {\cellcolor{my_grey}}\texttt{n\_neighbors}: 5              \\
                         & \texttt{weights}: uniform             \\ \hline
    \multirow{6}{*}{RF}  & \texttt{n\_estimators}: 100           \\
                         & {\cellcolor{my_grey}}\texttt{criterion}: gini              \\
                         & \texttt{max\_depth}: None             \\
                         & {\cellcolor{my_grey}}\texttt{min\_samples\_split}: 2       \\
                         & \texttt{min\_samples\_leaf}: 1        \\
                         & {\cellcolor{my_grey}}\texttt{random\_state}: 42            \\ \hline
    \multirow{6}{*}{MLP} & \texttt{hidden\_layer\_sizes}: (100,) \\
                         & {\cellcolor{my_grey}}\texttt{activation}: relu             \\
                         & \texttt{solver}: adam                 \\
                         & {\cellcolor{my_grey}}\texttt{alpha}: 0.0001                \\
                         & \texttt{learning\_rate}: constant     \\
                         & {\cellcolor{my_grey}}\texttt{random\_state}: 42            \\ \hline
    \end{tabular}
    \label{tab:baselines_hyp}
\end{table}

\clearpage


\subsubsection{Computational Resources}
\label{subsubsec:hardware}

This research leveraged the FinisTerrae III supercomputer at CESGA \cite{ft3} for model training. This systems is a Bull ATOS bullx configured across 13 racks, which includes 714 Intel Xeon Ice Lake 8352Y processors and 157 GPUs (141 Nvidia A100 and 16 Nvidia T4 units). It has 126 TB of memory, 359 TB of SSD NVMe storage, and Infiniband HDR 100 for networking, achieving a peak performance of 4.36 PetaFLOPS. Different hardware configurations were used for the experiments, based on the computational requirements of each task and the availability within the system's job scheduling system.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsection{Evaluation}
\label{subsec:evaluation}

This section details our comprehensive evaluation framework designed to rigorously assess the performance and efficiency of the proposed Tabular Transformer models for OS fingerprinting.

After hyperparameter optimization, the best-performing Tabular Transformer model was retrained on the full training dataset and evaluated on a hold-out test set using the same preprocessing pipeline. Early stopping was employed during training to prevent overfitting, and all splits were generated via stratified sampling to preserve the class distribution inherent to the OS fingerprinting task.

The evaluation metrics included are accuracy, precision, recall, and F1-score. For the latest three, as we are evaluating a multiclass classification problem, we employed the weighted average technique. These metrics, defined in the following Subsection~\ref{subsubsec:metric-definitions}, provide a robust assessment of the methods' performance. The accuracy metric itself can be very misleading on imbalanced datasets where one target class dominates the dataset. Furthermore, as when it comes to evaluate OS fingerprinting, we want to have a good balance between precision (how accurate the model is in its positive predictions) and recall (how complete the model's positive predictions are). Therefore, the metric we want to focus when comparing different results is F1-score, which is an harmonic mean of both.

In addition to accuracy metrics, we recorded the total training time and measured inference time, while also computing key model characteristics such as the number of trainable parameters and the overall memory footprint.

Furthermore, confusion matrices were generated to analyse misclassification at the class level. Final predictions along with their corresponding ground truth labels were saved to a CSV file, and the list of class labels was written to a separate text file. These steps ensure that the evaluation results are fully reproducible and facilitate subsequent analyses of the model's performance on an imbalanced, multiclass OS fingerprinting problem.

\subsubsection{Evaluation Metrics Definitions}
\label{subsubsec:metric-definitions}

Our evaluation relies on the following metrics:

\paragraph{Accuracy} measures the overall correctness of predictions:
\[
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\]

\paragraph{Balanced Accuracy} computes the average recall per class, ensuring equal contribution from all classes:
\[
\text{Balanced Accuracy} = \frac{1}{C} \sum_{i=1}^{C} \frac{TP_i}{TP_i + FN_i}
\]
where \(C\) is the number of classes.

\paragraph{Precision} quantifies the proportion of true positives among all positive predictions:
\[
\text{Precision} = \frac{TP}{TP + FP}
\]

\paragraph{Recall} (Sensitivity) measures the proportion of true positives identified among all actual positives:
\[
\text{Recall} = \frac{TP}{TP + FN}
\]

\paragraph{F1-Score} is the harmonic mean of Precision and Recall:
\[
\text{F1-Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\]

\paragraph{Weighted Averages} are computed to account for class imbalance by weighting each class metric by its support:
\[
\text{Weighted Metric} = \frac{\sum_{i=1}^{C} (\text{Support}_i \times \text{Metric}_i)}{\sum_{i=1}^{C} \text{Support}_i}
\]