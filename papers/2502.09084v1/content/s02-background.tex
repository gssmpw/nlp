\section{Background}
\label{sec:background}

\subsection{Operating System Fingerprinting}
\label{subsec:os-fingerprinting}

\begin{table}[b!]
    \centering
    \caption{Examples of Window Size and TTL values for different OSs. Source: \cite{lastovicka_jungle_2018}}
    \vspace{5pt}
        \begin{tabular}{>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{1,5cm}>{\centering\arraybackslash}m{2,5cm}}
            \hline
            \textbf{Window Size} & \textbf{TTL} & \textbf{OS} \\ \hline
            {\cellcolor{my_grey}}8,192 & {\cellcolor{my_grey}}128 & {\cellcolor{my_grey}}Windows 10 \\
            65,535 & 64 & Android 6 \\
            {\cellcolor{my_grey}}29,200 & {\cellcolor{my_grey}}64 & {\cellcolor{my_grey}}Ubuntu \\
            65,535 & 64 & Mac OS X 10.12 \\
            {\cellcolor{my_grey}}65,535 & {\cellcolor{my_grey}}64 & {\cellcolor{my_grey}}iOS 10.3 \\ \hline
        \end{tabular}
    \label{tab:ttls}
\end{table}

As previously introduced, Operating System (OS) fingerprinting is the process of identifying the OS running on a network-connected device by analysing its network traffic characteristics. This technique relies on the fact that different OS implementations exhibit distinct behaviours in network communication, such as variations in Transmission Control Protocol/Internet Protocol (TCP/IP) stack parameters, packet structure, or protocol handling. By examining these characteristics, OS fingerprinting enables the classification of devices, providing valuable information such as OS family, version, or even specific configurations.

As we have already seen, OS fingerprinting plays a crucial role in both network management and security \cite{nmap_os_detection}. It allows administrators to maintain an up-to-date inventory of devices, identify and patch vulnerable systems, and detect unauthorized devices such as rogue access points or insecure personal devices. In cybersecurity, it is widely used for reconnaissance, as identifying a target’s OS enables security professionals to tailor exploits to specific vulnerabilities, increasing the likelihood of a successful attack. Moreover, adversaries can leverage this information for social engineering, impersonating technical support and manipulating users into installing malicious software.

A specific example of OS fingerprinting can be achieved by analysing the Time To Live (TTL) and TCP Window Size parameters in network packets. Different OSs use distinct default values for these parameters, allowing generating specific signatures based on packet observations. Table~\ref{tab:ttls} provides the relation of Window Size and TTL values for various OSs, which enables the classification process. For instance, if a network packet is observed with a TTL of 128 and a TCP Window Size of 8,192, it is likely originating from a Windows 10 machine. These characteristics, combined with other parameters such as TCP options, provide valuable insights for classifying OS versions more accurately.

The level of detail that can be extracted through OS fingerprinting depends on the techniques used and the amount of network data available for analysis. When only basic network parameters, such as TTL or TCP Window Size, are available, OS identification is typically limited to broad categories, and similar OSs can be mixed, as shown in Table~\ref{tab:ttls}. However, incorporating more detailed protocol features allows for finer-grained classification. Depending on the level of detail in the classification, OS fingerprinting can be structured into the following levels:

\begin{itemize}
    \item \textbf{Manufacturer:} Identifies the company or organization responsible for developing the OS, such as Microsoft or Apple.
    \item \textbf{Family:} Classifies the OS into a broader category or series, such as Windows, macOS, or Linux, grouping similar systems under a common architecture.
    \item \textbf{Major Version:} Specifies the primary release version within an OS family, such as Windows 10 or macOS Catalina, indicating significant changes in functionality and system architecture.
    \item \textbf{Minor Version:} Differentiates smaller updates, builds, or patches within a major version, such as Windows 10 version 1909 or macOS Catalina 10.15.5, allowing for more granular classification.
\end{itemize}

Based on how the scanner interacts with the network, OS fingerprinting methods are classified into two main categories: active and passive. Active fingerprinting involves sending crafted network packets to the target machine and analysing its responses. While it provides fast and accurate results, it is easily detectable and can be blocked by security mechanisms. A widely well-known tool for active fingerprinting is Nmap \cite{nmaporg_nmap_nodate}, but there are other examples such as Xprobe2 and SinFP \cite{nmap_os_detection, xprobe2, sinfp}. In contrast, passive fingerprinting analyses existing network traffic without directly interacting with the target, making it stealthier but generally less accurate. This method relies on observing characteristics such as TTL, TCP Window Size, and TCP Options. Examples of passive fingerprinting tools include p0f, PRADS, and Ettercap \cite{zalewski_p0f_nodate, eb_fjellskal_prads_2009, ornaghi_alberto_ettercap_2001}. A diagram of both types of OS fingerprinting is shown in Figure~\ref{fig:active-passive}. Furthermore, a detailed analysis of traditional OS fingerprinting methods and tools is presented in Subsection~\ref{subsubsec:traditional-os-fingerprinting}. 

While effective in many scenarios, traditional rule-based OS fingerprinting approaches face significant limitations. Frequent OS updates and patches can alter network signatures, reducing the accuracy of traditional fingerprinting techniques. Additionally, many modern systems implement security mechanisms, such as firewall rules, packet obfuscation, and OS hardening tools, which modify network responses to evade detection. These factors make it increasingly difficult to maintain up-to-date and comprehensive fingerprinting databases, limiting their ability to correctly identify newer OS versions or systems with non-standard configurations.

To overcome these challenges, AI-driven techniques, including ML and DL, have been introduced to enhance OS fingerprinting. By integrating AI techniques, modern OS fingerprinting systems improve the accuracy, handle encrypted or obfuscated traffic, and generalize better across different network conditions. The application of ML and DL to OS fingerprinting is explored in detail in Subsections \ref{subsubsec:ml-based-os-fingerprinting} and \ref{subsubsec:dl-based-os-fingerprinting}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{images/ActivePassive-Horizontal.png}    
    \caption{Diagram of active and passive OS fingerprinting}
    \label{fig:active-passive}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\subsection{Artificial Intelligence}
\label{subsec:AI}

As background for this work, we provide an overview of the AI methods that form the foundation of our study. Initially, we introduce several conventional ML algorithms that serve as baselines for performance comparison. We then detail the Transformer architecture along with its specialized adaptation for structured tabular data—Tabular Transformers—which constitutes the core of our novel contribution.

\subsubsection{Machine Learning Baselines}
\label{subsubsec:ML-baselines}

For baseline comparison, we used three established algorithms—k-Nearest Neighbors (kNN), Random Forest (RF), and Multi-layer Perceptron (MLP)—which are representative of classical ML paradigms and provide a robust benchmark for evaluating performance on this task.

\begin{itemize}
    \item \textbf{k-Nearest Neighbors (kNN)} \cite{cover_nearest_1967}: A non-parametric method that assigns class labels based on the majority vote among the $k$ closest training examples using distance metrics (e.g., Euclidean, Manhattan). Its simplicity is counterbalanced by increased computational cost on large datasets.
    
    \item \textbf{Random Forest (RF)} \cite{breiman_random_2001}: An ensemble technique that builds multiple decision trees on bootstrapped subsets with random feature selection. The final prediction is obtained by majority voting (classification) or averaging (regression), enhancing generalization and mitigating overfitting.
    
    \item \textbf{Multi-layer Perceptron (MLP)} \cite{rumelhart_learning_1986}: A feed-forward neural network comprising an input layer, one or more hidden layers, and an output layer. Trained via backpropagation, the MLP learns complex nonlinear relationships but requires careful hyperparameter tuning and entails higher computational cost.
\end{itemize}

\subsubsection{The Transformer Architecture}
\label{subsubsec:transformers}

Transformers \cite{vaswani_attention_2023} are a DL architecture specifically designed to process sequential data efficiently. Unlike traditional architectures such as Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs), Transformers eliminate the need for recurrence and convolutional layers, instead leveraging an encoder-decoder structure, as depicted in \Cref{fig:transformer}. At the heart of this architecture lies the multi-head self-attention mechanism, which allows the model to capture global dependencies across input sequences by dynamically assigning different importance weights to each token. Unlike traditional ML algorithms that rely on extensive feature engineering and domain-specific tuning, Transformers automatically learn complex relationships and interactions within the data. This capability to model long-range dependencies makes them particularly effective for sequence-to-sequence tasks.

Since their introduction in 2017 by Google researchers \cite{vaswani_attention_2023}, Transformers have revolutionized NLP, excelling in tasks such as machine translation, text summarization, and sentiment analysis. A key advantage of this architecture over RNNs is its parallel processing capability, allowing entire input sequences to be processed simultaneously rather than sequentially. This significantly reduces training time and enhances performance, particularly in capturing long-range dependencies within textual data. Consequently, Transformers have become the foundation of modern LLMs, such as OpenAI's GPT and Meta's LLaMA series, which leverage this architecture to achieve state-of-the-art performance in NLP tasks.

Beyond NLP, the adaptability of Transformers has led to their application in diverse domains, including Computer Vision (CV), Reinforcement Learning (RL), or network traffic analysis. Their ability to model complex relationships in structured data makes them suitable for various tasks, from protein structure prediction to anomaly detection in cybersecurity. The scalability and efficiency of Transformers, particularly when trained on large datasets, have contributed to their widespread adoption, setting new benchmarks across multiple fields and paving the way for their use in OS fingerprinting and network security applications.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.4\linewidth]{images/transformer.png}    
    \caption{Transformer architecture. Source: \cite{vaswani_attention_2023}}
    \label{fig:transformer}
\end{figure}

\paragraph{Tabular Transformers}
\label{subsubsubsec:tabular-transformers}

Tabular Transformers are an adaption of Transformers designed to process structured tabular data commonly found in CSV files, spreadsheets, and relational databases. By leveraging self-attention mechanisms, they effectively capture complex feature interactions, eliminating the need for extensive feature engineering. These models have demonstrated superior performance in classification, regression, and other predictive tasks across various domains \cite{gorishniy_revisiting_2023}.  

We selected this architecture because network traffic datasets in this field are typically structured as collections of network flows, inherently formatted as tabular data. Unlike traditional DL models such as CNNs and RNNs, which rely on spatial or sequential patterns, Tabular Transformers excel at modelling structured data with heterogeneous features. Their ability to learn intricate dependencies across multiple attributes makes them particularly well-suited for analysing network traffic.  

Furthermore, as will be exposed in Section~\ref{sec:related-work}, this approach has not yet been explored for OS fingerprinting, despite its potential to enhance classification accuracy by capturing nuanced relationships in network traffic data. This gap in prior research motivated our investigation into the applicability of Tabular Transformers to this task.

Several Transformer-based architectures have been proposed for tabular data modelling. For this study, we selected two representative models—TabTransformer (TabT) and FT-Transformer (FT-T)—due to their demonstrated effectiveness in handling categorical and numerical features, allowing us to assess their suitability for OS fingerprinting. In the context of network traffic analysis, categorical features represent discrete variables with distinct groups, such as protocol type or OS family, while numerical features are continuous values that quantify measurements, like packet size or TTL.

\begin{itemize}
    \item \textbf{TabTransformer (TabT)} \cite{huang_tabtransformer_2020} replaces traditional categorical embeddings with context-aware representations using Transformer layers. By applying multi-head self-attention to categorical features, it captures dependencies and interactions more effectively than standard embedding techniques, improving classification tasks and enhancing robustness to missing and noisy data.

    \item \textbf{FT-Transformer (FT-T)} \cite{gorishniy_revisiting_2023} generalizes the self-attention mechanism to both categorical and numerical features, treating them uniformly to better capture interactions across heterogeneous data. Unlike architectures that require extensive preprocessing or feature engineering, FT-Transformer learns feature dependencies directly from raw tabular inputs, making it well-suited for complex network traffic datasets with mixed feature types.
\end{itemize}

These models were selected to compare different feature integration strategies in OS fingerprinting. While TabTransformer processes only categorical variables through the Transformer, FT-Transformer applies self-attention to both categorical and numerical features. Figures \ref{fig:tabt}-\ref{fig:ftt} illustrate these differences, highlighting how each architecture structures and transforms input data for prediction tasks.

\begin{figure}[b!]
    \centering
    \includegraphics[width=0.74\textwidth]{images/TabT.png}    
    \caption{Diagram of TabTransformer (Tab-T) architecture. Source: \cite{wang_lucidrainstab-transformer-pytorch_2024}}
    \label{fig:tabt}
\end{figure}

\begin{figure}[b!]
    \centering
    \includegraphics[width=0.74\textwidth]{images/FT-T.png}    
    \caption{Diagram of FT-Transformer (FT-T) architecture. Source: \cite{wang_lucidrainstab-transformer-pytorch_2024}}
    \label{fig:ftt}
\end{figure}

\clearpage
