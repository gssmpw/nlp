\section{Preliminaries}\label{sec:Preliminaries}
\subsection{Computation theory}\label{sec:Computation}
We introduce some notions that we mentioned in the paper. We follow \cite{li2019introduction}.

We use an alphabet of binary symbols $\mathcal{B}=\{0,1\}$. The set of all finite strings over $\mathcal{B}$ is denoted by $\mathcal{B}^*$, defined as
\begin{equation}
    \mathcal{B}^*=\{\epsilon, 0,1,00,01,10,11,000, \ldots\}
\end{equation}
with $\epsilon$ denoting the empty string, with no letters. Concatenation is a binary operation on the elements of $\mathcal{B}^*$ that associates $x y$ with each ordered pair of elements $(x, y)$ in the Cartesian product $\mathcal{B}^* \times \mathcal{B}^*$. 

We now consider a correspondence of finite binary strings and natural numbers. The standard binary representation has the disadvantage that either some strings do not represent a natural number, or each natural number is represented by more than one string. For example, either 010 does not represent 2, or both 010 and 10 represent 2. We can map $\Bcal^*$ one-to-one onto the natural numbers by associating each string with its index in the length-increasing lexicographic ordering
\begin{equation}\label{eq:lexico_ordering}
    (\epsilon, 0), (0, 1), (1, 2), (00, 3), (01, 4), (10, 5), (11, 6), \dots
\end{equation}
\begin{definition}\label{def:binary_lexico_code}
    We call the \emdef{binary lexicographic code $B$} the map that maps~\cref{eq:lexico_ordering} reversely, i.e. $0\mapsto \epsilon$, $1\mapsto 0$, $2\mapsto 1$, $3\mapsto 00 \dots$.
\end{definition}

The length of a finite binary string $x$ is the number of bits it contains and is denoted by $l(x)$. One can verify that for a natural number $n$ represented by~\cref{eq:lexico_ordering}, 
\begin{equation}\label{eq:def_length_number}
    l(n)=\lfloor \log_2 (n + 1) \rfloor
\end{equation}
For the readability of the paper, we define $l(n)=\log_2 n$ instead.
\begin{definition}\label{def:self_delim_code}
    A \emdef{self-delimiting code} of a natural number $n \in \NN$ is a function $\NN\to \Bcal^*$ that maps $n\in\NN$ to
    \begin{equation}
        \Bar{n}=\underbrace{11\dots1}_{\begin{array}{c}l(n)\text{ times}\\\end{array}}\quad 0 B(n).
    \end{equation}
    where $B$ is binary lexicographic code defined in~\cref{def:binary_lexico_code}.
    
    We call $\langle \cdot,\cdot \rangle: \NN \times \NN\to \Bcal^*$ a \emdef{concatenation function}: given $m,n\in \NN$,
    \begin{equation}
        \langle m,n \rangle := \Bar{m} B(n)
    \end{equation}
    We call $\langle m,n \rangle$ a \emdef{self-delimiting concatenation} of $m,n$.
\end{definition}
The usage of the self-delimiting concatenation is in a universal Turing machine or a universal finite codebook computer (UFCC), where the input is two natural numbers. We have to use a self-delimiting concatenation so that the string $mn$ is uniquely identified as $(m,n)$.

%

\begin{definition}
A \emdef{Turing machine (TM)} consists of a finite program, called the \emdef{finite control}, capable of manipulating a linear list of cells, called the \emdef{tape}, using one access pointer, called the head. We refer to the two directions on the tape as right and left. The finite control can be in any one of a finite set of states $Q$, and each tape cell can contain a $0$, a $1$, or a blank $B$.
The TM can perform the following basic operations:
\begin{enumerate}
    \item  it can write an element from $A = \{0, 1, B\}$ in the cell it scans; and
    \item it can shift the head one cell left or right.
\end{enumerate}
After each step, the finite control takes on a state from $Q$, and then decides an action according to a global list of \emdef{rules}. The rules have format $(p, s, a, q)$: $p$ is the current state of the finite control; $s$ is the symbol under scan; $a$ is the next operation to be executed of type 1 or 2 designated in the obvious sense by an element from $S = \{0, 1, B, L, R\}$; and $q$ is the state of the finite control to be entered at the end of this step. If the TM enters a state that does not appear as an entry $p$ in the list of rules, then the TM \textbf{halts}. In particular, we define an extra state ``reject'' such that TM halts on ``reject''. We say that a TM \emdef{rejects} $x$ if $T$ inputs $x$ and halts on ``reject''.
\end{definition}
\begin{definition}[Formal version of~\cref{def:compute}]\label{def:compute_formal}
    A Turing machine $T$ is said to \emdef{compute} a function $f: A\subset \Xcal \to \Bcal^*$, if $T$ rejects any input outside $A$ and for all $x\in A$, $T$ halts in a finite number of steps with $f(x)$ written on the tape.
\end{definition}
Intuitively, a TM $T$ computes a function $f$ if they have the same domain $A$, and for all $x\in A$, $T(x)=f(x)$.
\begin{definition}
    We define the \emdef{equivalence relation} $\sim$ between two Turing machines: $S\sim T$ if $S$ and $T$ halt on the same inputs (i.e. considered as functions, they have the same domain $L$) and for all $x\in L$, $S(x)=T(x)$. Namely, $S$ and $T$ compute the same function.
\end{definition}
\begin{definition}[informal]\citep{li2019introduction}
    A universal Turing machine $U$ is a Turing machine that can imitate the behavior of any other Turing machine $T$.
\end{definition}
In this paper, we mean ``simulate'' by ``imitating the behavior of another Turing machine''. We have not found any formal definition of ``simulation''. For the formalization of how a universal Turing machine simulates any other Turing machines, see \cite{hennie1966two}.

\subsection{Discrete probability theory}\label{sec:Discrete_proba}
\begin{definition}\label{def:cylinder}\citep{li2019introduction}
    Let $\Bcal$ be a finite or countably infinite set of symbols. In this paper, we use $\Bcal=\{0,1\}$. A \emdef{ cylinder} is a set $\Gamma_x\subseteq \Bcal^\infty$ defined by
    \begin{equation}
        \Gamma_x=\{x\omega:\omega\in\mathcal{B}^\infty\}
    \end{equation}
    with $x\in\mathcal{B}^*$. Let $\mathcal{G}=\{\Gamma_x:x\in\mathcal{B}^*\}$ be the set of all cylinders in $\Bcal^\infty$.
A function $\mu:\mathcal{G}\to\RR$ defines a probability measure if
$$\begin{aligned}&\mu(\Gamma_\epsilon)=1,\\&\mu(\Gamma_x)=\sum_{b\in\mathcal{B}}\mu(\Gamma_{xb}).\end{aligned}$$
\end{definition}
Consider the function $\mu^{\prime}:\mathcal{B}^*\to\RR$ defined by $\mu^{\prime}(x)=\mu(\Gamma_x)$. Trivially from $\mu^\prime$ we can reconstruct $\mu$ and vice versa. From now on we identify $\mu^{\prime}$ with $\mu.$ Formally, we use the definition of measure below. One should keep in mind that our notation is shorthand for the original measure. 

\begin{definition}
    A \emdef{ finite cylinder of depth $m$} is defined by
    \begin{equation}
        \Gamma_x^m=\{x\omega:\omega\in\mathcal{B}^m\}.
    \end{equation}
\end{definition}
The probability space $\Xcal$ we consider in~\cref{def:discrete_proba_space} can be redefined as $\Xcal:=\{\Gamma_x:x\in\mathcal{B}^*\}$.

\begin{definition}\citep{li2019introduction}
    A function $\mu:\mathcal{B}^*\to\RR$ is a \emdef{ probability measure} (measure for short) if
\begin{equation}
    \mu(\epsilon)=1 \quad \text{and} \quad \mu(x)=\sum_{b\in\mathcal{B}}\mu(xb),
\end{equation}
for all $x\in\mathcal{B}^*.$ A \emdef{semi-measure} is a defective measure. A function $\mu:$
$\mathcal{B}^*\to\RR$ is a semi-measure if for all $x\in\mathcal{B}^*$,
$$\begin{aligned}\mu(\epsilon)&\leq1,\\\mu(x)&\geq\sum_{b\in\mathcal{B}}\mu(xb).\end{aligned}$$
\end{definition}

\begin{lemma}
    When the precision of $Z$ is fixed, the conditional independence set 
    $$\{X \indep Y | Z ; \quad X,Y,Z { \text{ are random variables in } \PP}\}$$
    is decreasing as the precisions of $X$ and $Y$ increase. 
\end{lemma}

\begin{proof}
    We will prove that if $X\_\indep Y|Z$, then $X\indep Y|Z$. 
    
    For all $i\in \Bcal$, for all $(x,y,z)$ in the support of $(X,Y,Z)$, $\PP(xi,y,z)\PP(z) = \PP(xi,z)\PP(y,z)$. Therefore,
    \begin{align}
         \PP(x,y,z)\PP(z) &= \left[ \PP(x0,y,z) + \PP(x1,y,z) \right] \PP(z)\\
         &= \PP(x0,z)\PP(y,z) + \PP(x1,z)\PP(y,z)\\
         & = \PP(x,z)\PP(y,z)
    \end{align}
    which implies $\PP(X,Y|Z)=\PP(X|Z)\PP(Y|Z)$.
\end{proof}

\subsection{Compression in information theory}\label{sec:comment_entropy}
Here we recall some basic results in information theory. We follow \cite{cover1999elements}.

\begin{definition}
    A \emdef{codebook}\footnote{In most literature in information theory~\citep{shannon1948mathematical, cover1999elements} the word ``codebook'' only denotes a code for $\Xcal^N$ with $N$ samples in the noisy-channel coding setting. We abuse the usage of this word in the source coding setting because we would like to stress that the source code itself also needs to be encoded and it also has a coding length, i.e. the coding length of the code(book).} (or source code) $c$ for a random variable X is a mapping $\Xcal\to \Bcal^*=\{0,1\}^*$. Let $c(x)$ denote the codeword corresponding to $x$ and let $l(c(x))$ denote the length of $c(x)$.

    The expected length $L(c)$ of a codebook $c$ for a random variable $X$ with probability mass function $p(x)$ is given by

$$L(c)=\EE[l(c(x))] = \sum_{x\in\mathcal{X}}p(x)l(x).$$
\end{definition}
\begin{example}
Let $X$ be a random variable with the following distribution and codeword assignment:

$$\begin{aligned}
    &\PP(X=1)=\frac12,\quad\text{codeword }c(1)=0\\
    &\PP(X=2)=\frac14,\quad\text{codeword }c(2)=10\\
    &\PP(X=3)=\frac18,\quad\text{codeword }c(3)=110\\
    &\PP(X=4)=\frac18,\quad\text{codeword }c(4)=111.
    \end{aligned}$$

The entropy $H(X)$ of $X$ is 1.75 bits, and the expected length $L(c)=$ $E[l(X)]$ of this code is also 1.75 bits. Here we have a code that has the same average length as the entropy. We note that any sequence of bits can be uniquely decoded into a sequence of symbols of $X.$ For example, the bit string 0110111100110 is decoded as 134213.
\end{example}
\begin{definition}\label{def:extension_codebook}
    The \emdef{extension} $c^*$ of a codebook $c$ is the mapping from finite-length strings of $\mathcal{X}$ to fınite-length strings of $\Bcal^*$, defıned by
    $$c^*(x_1x_2\cdots x_n)=c(x_1)c(x_2)\cdots c(x_n),$$
    where $c(x_1)c(x_2)\cdots c(x_n)$ indicates concatenation of the corresponding codewords.
\end{definition}
\begin{example}
    If $c(x_1)=00$ and $c(x_2)=11$, then $c(x_1 x_2)=0011$.
\end{example}
\begin{definition}
    A code $c$ is called \emdef{uniquely decodable} if its extension $c^*$ is non-singular, namely, for all $x,x'\in \Xcal^*$ sequences of letters in $\Xcal$ such that $x\neq x'$, $c^*(x)\neq c^*(x')$.
\end{definition}
In other words, any encoded string in a uniquely decodable code has only one possible source string producing it.
\begin{definition}
    A codebook is called a \emdef{prefix code} or an instantaneous code if no codeword is a prefix of any other codeword.
\end{definition}
An instantaneous code can be decoded without reference to future codewords since the end of a codeword is immediately recognizable. Hence, for an instantaneous code, the symbol $x_i$ can be decoded as soon as we come to the end of the codeword corresponding to it.

We cannot assign short codewords to all source symbols and still be prefix-free. The set of codeword lengths possible for instantaneous codes is limited by the following inequality.

\begin{lemma}[Kraft inequality]\label{lem:kraft}
    For any instantaneous code (prefix code) over an alphabet of size $D$ ($=2$ in our case), the codeword lengths $l_1, l_2, \ldots, l_m$ must satisfy the inequality
    $$\sum_iD^{-l_i}\leq1.$$
    Conversely, given $a$ set of codeword lengths that satisfy this inequality, there exists an instantaneous code with these word lengths.
\end{lemma}
\begin{corollary}\label{cor:correpondance_proba_code}\cite[p. 96]{grunwald2007minimum}
    Let $\mathcal{X}$ be a finite or countable sample space. Let $\PP$ be a probability distribution over $\mathcal{X}^n$, the set of sequences of length $n.$ Then there exists a prefix code $c$ for $\mathcal{X}^n$ such that for all $x^n\in\mathcal{X}^n,L_c(x^n)=-\log \PP(x^n)$. $c$ is called the code corresponding to $\PP$.
    
    Similarly, let $c^\prime$ be a prefix code for $\mathcal{X}^n.$ Then there exists a defective probability distribution $\PP^\prime$ such that for all $x^n\in\mathcal{X}^n$, $-\log \PP^{\prime}(x^n)=L_c^{\prime}(x^n)$. $\PP^{\prime}$ is called the probability distribution corresponding to $c^\prime$.
\end{corollary}

\begin{theorem}[Shannon's source coding theorem]\cite[Thm. 5.4.1]{cover1999elements}\label{Thm:Shannon_source_coding}
    Let $l_1^*,l_2^*,\ldots,l_m^*$ be optimal codeword lengths for a source distribution p and a D-ary alphabet, and let $L^*$ be the associated expected length of an optimal code $( L^* = \sum p_il_i^* )$. Then 
    $$H_D(X)\leq L^*<H_D(X)+1.$$
\end{theorem}
\cref{Thm:Shannon_source_coding} corresponds to the following communication game: Alice and Bob share the same codebook $c:\Xcal\to \Bcal^*$, and Alice wants to transmit losslessly a sequence $x_1\dots x_n$ iid sampled from $\PP_X$ to Bob. She encodes the sequence into $c^*(x_1\dots x_n)=c(x_1)\dots c(x_n)$ and sends the code sequence to Bob. Bob decodes the sequence using the same codebook.~\cref{Thm:Shannon_source_coding} proves that in this case, asymptotically the minimum expected binary coding length for a word $x\in\Xcal$ sampled from $\PP_X$ is around $H_2(X)$.

Suppose Alice and Bob share nothing except a programming language (C, python), or a universal Turing machine. In that case, the overall bits Alice needs to send can be two parts: a codebook, and a binary codeword sequence. 

\begin{definition}[Huffman coding program~\citep{mackay2003information}]\label{def:Huffman_code}
    A Huffman coding program is a Turing machine that proceeds as follows:
    
    Input: the probability space $\Xcal^d$, and a distribution $\PP_X$ as a discrete function.
    \begin{enumerate}
        \item Take the two least probable symbols in the alphabet $\Xcal^d$. These two symbols will be given the longest codewords, which will have equal length and differ only in the last digit.
        \item Combine these two symbols into a single symbol, and repeat.
    \end{enumerate}
    Output: a tree-structured codebook.
    We call the output of this program a \emdef{Huffman code}.
\end{definition}
\begin{theorem}[Huffman coding is optimal~\citep{cover1999elements}]\label{thm:huffman_optimal}
    If $c^*$ is a Huffman code for $\PP_X$ and $c'$ is any other uniquely decodable code, $\EE_{\PP_X}[l(c^*(X))] \leq \EE_{\PP_X}[l(c'(X))] $.
\end{theorem}
Therefore in the paper, we use the Huffman coding program to transform a multi-env distribution into a codebook. No matter which coding program we choose (e.g. Shannon-Fano-Elias code or arithmetic code), the expected codeword length for each $x\in\Xcal^d$ is close to $\log\PP_X(x)$. Since we can fix a coding program in a UFCC and still compute all finite codebooks, different choices of coding programs do not change significantly the theoretical result of model selection.