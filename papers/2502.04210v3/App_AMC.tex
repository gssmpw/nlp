\section{Remarks on Algorithmic Markov Condition (AMC)}\label{sec:remark_AMC}
Algorithmic Markov Condition (AMC) was introduced in \cite{janzing2010causal}. We first briefly recall their paper's main result, then compare the principles in it and in our paper. In the following, they use $K$ to denote \emdef{prefix Kolmogorov complexity} \citep[Chapter 3]{li2019introduction}.
\begin{definition}[algorithmic Markov condition]\citep[Postulate 5]{janzing2010causal}\label{def:AMC}
Let $x_1,\ldots,x_n$ be $n$ strings representing descriptions of observations whose causal connections are formalized by $a$ directed acyclic graph $G$ with $x_1,\ldots,x_n$ as nodes. Let pa$_j$ be the concatenation of all parents of $x_j$ and $nd_j$ the concatenation of all its non-descendants except $x_j$ itself. Then
$$x_j\perp nd_j\mid pa_j^*.$$
where $pa_j^*=(pa_j, K(pa_j))$, and the above independence is algorithmic, i.e. the algorithmic conditional mutual information $I(x_j:nd_j|pa_j^*):=K(x_j|pa_j^*)+K(nd_j|pa_j^*)-K(x_j:nd_j|pa_j^*)+O(1) =O(1)$.
\end{definition}

Then they prove in their Thm. 3 that the above condition is equivalent to the statement 
\begin{equation}\label{eq:AMC_string}
    K(x_1,\dots, x_n)=\sum_{j=1}^n K(x_j|pa_j^*) +O(1).
\end{equation}
In other words, for $n$ strings $x_1, \dots, x_n$, if \cref{eq:AMC_string} does not hold for a DAG $G$ then we reject the statement that $x_1, \dots, x_n$ satisfy an algorithmic causal model with DAG $G$, see their Postulate 6.

Inspired by \cref{eq:AMC_string}, they propose a principle for the DAG selection given a joint distribution $P$:
\begin{principle}\citep[Postulate 7]{janzing2010causal}\label{pr:AMC_postulate7}
A causal hypothesis $G$ (i.e., a DAG) is only acceptable if the shortest description of the joint
density $P$ is given by a concatenation of the shortest description of the Markov kernels, i.e. 
\begin{equation}\label{eq:AMC_postulate7}
    K(P(X_1,\ldots,X_n))=\sum_jK(P(X_j|PA_j))+O(1)
\end{equation}
where $K(P)$ is the length of the shortest prefix-free program that computes $P(x, y)$ from $(x, y)$.
If no such causal graph exists, we reject every possible DAG and assume that there is a causal relation of a different type, e. g., a latent common cause, selection bias, or a cyclic causal structure.
\end{principle}
We note that although \cref{eq:AMC_postulate7} seems similar to \cref{eq:AMC_string}, they are in fact different principles and one cannot derive one from the other. By \cref{def:KC}, the Kolmogorov complexity, whether prefix ($K_U$) or not ($C_U$), is a function that inputs strings instead of functions. A function has multiple string representations, so does a function or distribution $P$ over $(X_1,\ldots,X_n)$. If we consider $P$ as $n$ strings and apply \cref{eq:AMC_string} to them, this is ill-defined because of the non-uniqueness of string representations. In fact, the motivation of proposing \cref{pr:AMC_postulate7} as a model selection principle is not related to \cref{eq:AMC_string}, but to what is afterwards named as ``independent causal mechanism'' (ICM) principle \citep{peters2017elements}:

``We can think of $P(X)$ as describing a source $S$ that generates $x$-values and sends them to a ``machine'' $M$ that generates $y$-values according to $P (Y |X)$. Assume we observe that $I(P(X): P(Y|X)) \gg 0$.\footnote{$I$ denotes the algorithmic mutual information, see \cref{def:AMC}.} Then we conclude that there must be a causal link between $S$ and $M$ that goes beyond transferring $x$-values from $S$ to $M$. This is because $P (X)$ and $P (Y |X)$ are inherent properties of $S$ and $M$ , respectively which do not depend on the current value of $x$ that has been sent.'' \citep[Sec. 3.1]{janzing2010causal}

In other words, if the shortest Turing machines $S$ and $M$ computing respectively $P(X)$ and $P(Y|X)$ have algorithmic mutual information $O(1)$, then they accept the DAG $X\to Y$, otherwise reject.

Our principle of model selection, \cref{priniple:compression}, does not imply \cref{pr:AMC_postulate7}. The reasons and comparisons are the following:
\begin{itemize}
    \item \cref{priniple:compression} allows selecting a CFMP that contains probabilistic mechanisms that are not algorithmically independent, for example $P(X)\sim \Ncal(0,\sigma^2)$ and $P(Y|X)\sim \Ncal(X,\sigma^2)$, which, according to \cref{pr:AMC_postulate7}, leads to rejecting $X\to Y$ because of the compressibility of the shared parameter $\sigma$.
    \item Our output of model selection is different from \cref{pr:AMC_postulate7}. We select a Turing machine, from which the causal and symmetry statements are read off. \cref{pr:AMC_postulate7} selects a graph only among all possible DAGs, which is less than the possible models that we illustrate in \cref{eg:CFMP}.
    \item The communication game setting behind \cref{pr:AMC_postulate7} is: Alice and Bob share a universal Turing machine, and Alice would like to send a string as short as possible to Bob so that Bob could compute a function $P$. In constrast, we are interested in the game where Alice would like to send \textit{datasets}. The idea of UFCC is that Alice should send a codebook and a codeword so that Bob could reconstruct the datasets. A codebook might consists of a probability distribution function $P$ or not. Our choice of $P$ depends on the trade-off between the two part codes, instead of being given a priori in \cref{eq:AMC_postulate7}.
\end{itemize}
The last point shows that the two-part code objective in the sense of MDL or UFCC is fundamentally different from \cref{pr:AMC_postulate7}. \cite{marx2021formally} aim at linking \cref{pr:AMC_postulate7} and two-part code in MDL. They propose a two-part code objective adapted from \cite{Budhathoki}:
\begin{equation}\label{eq:AMC_MDL1}
    K_{X\to Y}:= K(P_X)+ K(x|P_X) + K(P_{Y|X}) + K(y|x, P_{Y|X})
\end{equation}
and they proved that $K_{X\to Y}$ is on expectation equal to 
\begin{equation}\label{eq:AMC_MDL2}
    K(P_X) + K(P_{Y|X}) + H(P_{XY}),
\end{equation}
which is also a two-part code objective consisting of model length $K(P_X) + K(P_{Y|X})$ and the codeword length $H(P_{XY})$. Their two objectives \cref{eq:AMC_MDL1} and \cref{eq:AMC_MDL2}, however, do not agree with $K(P_{XY})$ in \cref{pr:AMC_postulate7}. Therefore, the two-part code objective (whether from MDL or UFCC) and \cref{pr:AMC_postulate7} cannot be derived from each other.