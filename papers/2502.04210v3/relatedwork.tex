\section{Related work}
\label{sec:related_work}
%
\citet{janzing2010causal} were the first to consider causal mechanisms implemented by Turing machines. They replace statistical (conditional) independence with algorithmic (conditional) independence, conjecturing that if the Kolmogorov complexity of a string or a joint distribution can be decomposed into a sum of conditional Kolmogorov complexities of the causal mechanisms according to a graph, then this graph should be selected. They also extended their model selection principle to probabilistic models \citep[Postulate 7]{janzing2010causal}, which we comment in detail in \cref{sec:remark_AMC} and compare with our \cref{priniple:compression}.
%
The incomputable objective in \cite[Postulate 7]{janzing2010causal} is replaced by entropy~\citep{SteJanSch10,pranay2021causal} or MDL~\citep{Budhathoki,budhathoki2017mdl,marx2019causal,mian2021discovering,mian2023information} in all subsequent papers. We discuss in~\cref{sec:comment_entropy} and~\cref{sec:MDL} the difference between our approach and them.
\cite{marx2021formally} discusses the relationship between Postulate 7 and two-part code. We give our comments on their results and on Postulate 7 in \cref{sec:remark_AMC}.
Some papers~\citep{marx2019identifiability, marx2019telling, mameche2022discovering, mameche2024learning} claim identifiability (i.e., recovery of the ground-truth graph) by minimizing their proxy of Postulate 7 in \cite{janzing2010causal}. We show in~\cref{lem:IdentminCE} that any identifiability of graphs is reduced to the claim of the uniqueness of the solution of minimum cross-entropy instead of minimizing a bound of Kolmogorov complexity. Our approach focuses on an upper bound of Kolmogorov complexity of a specific class of Turing machines that compute probabilistic models. Our objective is fundamentally different from \cite[Postulate 7]{janzing2010causal}, as we show in \cref{sec:remark_AMC}.

%


%

%
%
\cite{dhirbivariate} address bivariate causal discovery without confounding by comparing the posteriors of two graphs, with the correctness (probability of selecting the ground truth graph) depending on the total variation between the ground-truth distribution and marginal likelihood. In \cref{sec:MDL}, we discuss the difference between our approach and Bayesian model selection.

On the side of computation theory, there is abundant work on constraining the definition of Kolmogorov complexity to make it computable, such as 
%
resource-bounded complexity~\citep{barzdin1968complexity}, logical depth~\citep{chaitin1977algorithmic}, automatic complexity~\citep{shallit2001automatic}.%
Those definitions are fit for compressing more general strings without any probabilistic structure, therefore the entropic code ($-\log \PP$) is not applicable. 
%
In the domain of knowledge representation, \cite{shen2018conditional,kisa2014probabilistic} use probabilistic sentential decision diagrams (PSDD) to model Bayesian networks and learn them by maximum likelihood. Some use instead trees~\citep{chen2022definition} and arithmetic circuits~\citep{darwiche2022causal, huang2024causal}.

%