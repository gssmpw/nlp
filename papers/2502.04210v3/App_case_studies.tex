\section{Supplemental results and proofs in~\texorpdfstring{\cref{sec:case_studies}}{reference} Case studies}\label{sec:supp_case_studies}
\subsection{Lemmata}
\begin{lemma}[Bernoulli's Inequality~\citep{carothers2000real}]\label{lem:bernoulli}
    If $a>-1$, $a\neq 0$, then $(1+a)^n > 1+na$ for any integer $n>1$.
\end{lemma}

\begin{lemma}\label{lem:product_precision}
    Given $x\in\QQ \cap [0,1)$ in binary precision $n$, the minimum number of bits $k$ required such that for all exact factorization $x = \prod_{i=1}^l y_i$ with $y_i\in\QQ \cap [0,1)$, 
    $\prod_{i=1}^l y_i|_k$ has the same binary representation as $x$ with truncated precision $n$, i.e.
   \[
   0 \leq x - \prod_{i=1}^l y_i|_k  \leq 2^{-n-1},
   \]
    
    is at most $2n+\log_2 l + 3$.
\end{lemma}
\begin{proof}
    %

Let \( (x_i)_i \)  be the approximations of \( y_i \) with \( k \) bits of precision, i.e. $(y_i|_k)_i$, so:
\begin{equation}
    0 \leq y_i - x_i \leq \delta := 2^{-k}
\end{equation}
Define the relative errors:
\begin{equation}
    \epsilon_i = \frac{y_i - x_i}{y_i}
\end{equation}

Case 1: 
$y_i\geq 2^{-n-2}$ for all $i$. Then we have:
\begin{equation}
    \epsilon_i \leq \frac{\delta}{2^{-n-2}} = 2^{n - k +2} =:\epsilon
\end{equation}
We aim to bound the error:
\begin{equation}
    \prod_{i=1}^l y_i - \prod_{i=1}^l x_i  = x \left( 1 - \prod_{i=1}^l (1 - \epsilon_i) \right) \leq  1 - \prod_{i=1}^l (1 - \epsilon_i)  \leq  1 - (1 - \epsilon)^l \leq l \epsilon
\end{equation}
where the rightmost inequality is by~\cref{lem:bernoulli}.

We want to find $k$ such that the RHS is upper bounded by $2^{-n-1}$:
\begin{equation}
    l \cdot 2^{n - k+2} \leq 2^{-n-1}
\end{equation}
which is equivalent to
\begin{equation}
    k\geq 2n+\log l +3.\\
\end{equation}

Case 2: There exist $i\in[d]$ such that
$y_i< 2^{-n-2}$ for all $i$. Then we have:

\begin{equation}
    0\leq \prod_{j=1}^l (y_j|_k) \leq y_i|_k < 2^{-n-2}
\end{equation}
and
\begin{equation}
    0 \leq x=\prod_{j=1}^l y_j \leq y_i|_k < 2^{-n-2}.
\end{equation}
Therefore, 
\begin{equation}
    0 \leq x - \prod_{i=1}^l y_i|_k  < 2^{-n-1}
\end{equation}
\end{proof}


\subsection{Proofs in~\texorpdfstring{\cref{sec:case_studies}}{reference}}
\propFactorizeShorter*
\begin{proof}
    We construct a CFMP $\alpha$, following~\cref{def:cond_feat_mechanism_program}:

    First step, $\alpha$ generates $\Pcal_{\alpha}$, a list of probability tables:
    \begin{itemize}
        \item for all $i\in [d]$, $f_i$ computes: $ \PP^i(\cdot| \cdot): \Xcal \times (\Xcal^{|S_i|}\times [I])$, i.e. the shifted mechanisms of $X_1$;
        \item $f_{d+1}$ computes the $X_2, \dots, X_d$ marginal of $\PP$;
        \item $f_{d+2}: \text{Unif}[I]$, a uniform prior over environments.
    \end{itemize}
    We want the probabilistic mechanisms output precise enough probability values such that the joint distribution computed at the end of step 3 is lossless. By~\cref{lem:product_precision}, the output precision in each probabilistic mechanism needs $2n+\log 2 + 3 = 2n+4$ bits.
    
    For each $i\in [d]$, $f_i$ is a table with input space at most $(\Bcal^m)^{1+(d-2)} \times \Bcal^{ \log I
    }$, and output space $\Bcal^{2n+4}$. The coding length of $f_i$ as a table is at most $(2n+4) (2^m)^{d-1} I $. The coding length of $f_{d+1}$ is $(2n+4) (2^m)^{d-1}$. The coding length of $f_{d+2}$ is $(2n+4) I $, since $\frac{1}{2^n}\leq \frac{1}{I} = \frac
    {1}{2^{\log I}}$ by assumption. In total we need $(2n+4) ( (2^m)^{d-1} I + (2^m)^{d-1} + I)$ bits for $\Pcal_\alpha$.

    By assumption in UFCC, $\Phi_\alpha$ only contains projections. Since the cardinal of the power set of the nodes is $2^{d+1}$, we need $d+1$ bits for each feature mechanism. In total we need $(I+3)(d+1)$ bits for $\Phi_\alpha$.

    Second step, we assign $I+3$ feature mechanisms to $I+2$ probabilistic mechanisms, each of which has two cases (conditional or value) for filling the feature mechanism. So the whole mapping costs $2(I+2)\log (2I+3)$ bits.

    Third step, for each $(x,e)\in \Xcal^{d} \times [I]$, $\alpha$ computes $\PP(x, e) = f_e(x_1|x_{S_e}) f_{d+1}(X_2, \dots, X_d)f_{d+2}(e)$. We need $O(I\log I)$ bits because we only need to assign $I$-many mechanisms $(f_i)_{i\in [I]}$ to $I$-many environments, with other mechanisms being the same for all $(x,e)\in \Xcal^{d} \times [I]$.

    The Huffman coding mechanism~\cref{def:huffman_mechanism} that turns a distribution into a Huffman code is of constant coding length. In total, the coding length of $\alpha$ is $O(nI (2^m)^{d-1} + Id)$.

    Define $\beta$ as the CFMP that encodes the whole multi-env distribution: $\Pcal_\beta$ has only one element: a probability table $\PP$ of precision $n$. This costs $nI2^{m^d}$ bits. $\Phi_\beta$ only contains the identity, which costs $d+1$ bits, same as those elements in $\Phi_\alpha$. Since the joint distribution is already represented, all the following steps are trivial and only cost constant bits. Therefore, $l_{U_{\text{tabCBN}}} (\alpha)(m,d) = o(l_{U_{\text{tabCBN}}} (\beta)(m,d))$.




\end{proof}

\propSMS*
\begin{proof}
    There are $\binom{M}{k}$ possibilities for choosing $k$ mechanisms from $\Pcal_\alpha$. There are $k! S_k^N$ possibilities for subjective functions $[N]\to [k]$, where $S_k^N$ is the Stirling number of the second kind. Given $k$, the bits for step 2 in strategy 2 is $\log\binom{M}{k} + \log (k!) + \log k$. The bits for step 2 in strategy 1 are $N\log M$. So the difference between strategy 2 and strategy 1 is
    \begin{equation}\label{eq:diff_bits}
        A(k)=\log\binom{M}{k} + \log S_k^N + \log (k!) + \log k -N\log M
    \end{equation}
    So the difference between $A(k+1)$ and $A(k)$ is
    \begin{align}
        A(k+1) - A(k) &= \log \left( \frac{M - k}{k + 1} \right) + \log \left( \frac{S_{k+1}^N}{S_k^N} \right) + \log (k+1) + \log \left( \frac{k+1}{k} \right)\\
        &= \log \left( (M - k) \cdot \frac{S_{k+1}^N}{S_k^N} \cdot \frac{k+1}{k} \right).\label{eq:SMS_Ak+1-Ak}
    \end{align}
    For $k=o(N)$, we have an approximation for Stirling number of the second kind: $\lim_{N\to \infty} S_k^N= \frac{k^N}{k!}$. Then
    \begin{align}
        \lim_{N\to \infty}\frac{S_{k+1}^N}{S_k^N} &=\frac{\frac{(k+1)^N}{(k+1)!}}{\frac{k^N}{k!}} = \frac{(k+1)^N}{k^N} \cdot \frac{k!}{(k+1)!} = \left(1 + \frac{1}{k}\right)^N \cdot \frac{1}{k + 1}\label{eq:SMS_frac_SN}
    \end{align}
In order that \cref{eq:SMS_Ak+1-Ak} is greater than zero, by \cref{eq:SMS_frac_SN} we need for any $k= o(N)$,
    \begin{equation}
        \frac{M-k}{k} \left(1+\frac{1}{k}\right)^N>1
    \end{equation}
    \begin{equation}\label{eq:SMS_k<k0}
        M>\frac{k}{\left(1+\frac{1}{k}\right)^N}+k,
    \end{equation}

    which holds when $k<\frac{M}{2}$.
    
    Therefore, $A(k)$ monotonically increases when $k=o(N)$ and $k<\frac{M}{2}$. By~\cref{eq:diff_bits}, 
    \begin{equation}
        A(1) = (1-N)\log M + \log 2 +1<0
    \end{equation}

    when $N$ is sufficiently large. The conclusion of the proposition is obtained from $-A$.
\end{proof}

\propInvShorter*
\begin{proof}
    The proof is trivial because the steps in $\alpha$ and $\beta$ are the same only except for $\Pcal_\alpha$ and $\Pcal_\beta$. 
    
    $\Pcal_\alpha$ consists of two tables: $f_1(\cdot| \cdot): \Xcal \times \phi(\Xcal) \to \Bcal^n$, $f_{2}:\Xcal\to \Bcal^n$. By~\cref{lem:product_precision}, each probability value in each probabilistic mechanism table needs $2n+\log 2 + 1 = 2n+4$ bits. Therefore, $f_1$ needs $(2n+4)2^{m+c}$ bits, $f_2$ needs $(2n+4)2^{m}$ bits, where $c$ is a constant independent of $m$.

    $\Pcal_\beta$ consists of two tables: $f'_1(\cdot| \cdot): \Xcal \times \Xcal \to \Bcal^n$, $f_{2}:\Xcal\to \Bcal^n$. $f_1$ needs $(2n+4)2^{m^2}$ bits, $f_2$ needs $(2n+4)2^{m}$ bits.

    Therefore, $l_{U_{\text{TabInv}}} (\alpha(m)) = o(l_{U_{\text{TabInv}}} (\beta(m)))$.

    %

    %
    
    %

    %

    %

    %

    %
    
\end{proof}

%

%