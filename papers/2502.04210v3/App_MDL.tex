\section{Remarks on Minimum Description Length principle (MDL) and Bayesian model selection}\label{sec:MDL}

In \cref{def:UFCC} we define that each codebook computed by a UFCC must be a function $\Xcal^d \to \Bcal^*$. When the data is iid sampled from a distribution over $\Xcal^d$ then Huffman code is optimal (\cref{thm:huffman_optimal}). If we modify the definition of UFCC by changing the domain of codebook from $\Xcal^d$ to $(\Xcal^d)^*$, and if the data is exchangeable instead of iid, then there is a code called Bayes code that has shorter codeword length than a given Huffman code for all sequence $x\in (\Xcal^d)^*$. This is a fundamental idea in MDL principle  \citep{grunwald2007minimum}:

\begin{example}[Example 6.4 in \cite{grunwald2007minimum}: Bayes code is better than two-part code]\label{eg:Bayes_code_better}

The Bayesian model is in a sense superior to the two-part code. Namely, in the two-part code we first encode an element in the parameter set $\Theta$ using some code $L_0.$ Such a code must correspond to some "prior" distribution $W$ on $\mathcal{M}$ so that the two-part code gives codelengths
\begin{equation}\label{eq:two_part_MDL}
    L_\text{2-part}(x^n)=\min_{\theta\in\Theta}-\log \PP_\theta(x^n)-\log W(\theta),
\end{equation}

where $W$ depends on the specific code $L_0$ that was used. 

Define the Bayes code with prior $W$:
\begin{equation}\label{eq:Bayes_code_MDL}
    L_{\text{Bayes}}(x^n):=-\log \PP_\text{Bayes}(x^n)=-\log\sum_{\theta\in\Theta}\PP_\theta(x^n)W(\theta)
\end{equation}

where $P_\text{Bayes}$ is the marginal likelihood of the data $x^n$ under the prior $W$. Then it is direct to see that

$$L_{\text{Bayes}}(x^n)=-\log\sum_{\theta\in\Theta}\PP_\theta(x^n)W(\theta)\leq\min_{\theta\in\Theta}-\log \PP_\theta(x^n)-\log W(\theta)=L_\text{2-part}(x^n)$$
because a sum is at least as large as each of its terms.

The inequality becomes strict whenever $\PP_\theta(x^n)>0$ for more than one value of $\theta$. We see that in general the Bayesian code is preferable over the two-part code: for all $x^n$ it never assigns code lengths larger than $L_{2-\text{part}}( x^n)$, and in many cases it assigns strictly shorter codelengths for some $x^n$.    
\end{example}

The above example shows that for any two-part code there exists a Bayes code that is uniformly shorter than that two-part code. Therefore, the MDL research prefers Bayes code to two-part code, respectively termed refined MDL and crude MDL in \cite{grunwald2007minimum}. 

Using this example, we can define a \emdef{Universal Bayes Codebook Computer (UBCC)}\footnote{Same as \cref{def:UFCC} we omit $(m,d)$ in the input for simplicity. In the general case we can construct UBCC depending on $(m,d)$ by inputting $\langle m, \langle d, \langle p \rangle \rangle \rangle$ and simulating the codebook for each $(m,d)$ respectively.}:
\begin{definition}\label{def:UBCC}
    A Turing machine is called Universal Bayes Codebook Computer (UBCC) if it is constructed as follows:
\begin{enumerate}
    \item First, same as in \cref{def:UFCC}, define any recursively enumerable set $S$ of FCMs, such that any codebook $g:\Xcal^d \to \Bcal^*$ can be computed by at least one of the FCMs in it (In the following, we will call such a r.e. set a universal set of FCMs.).
    \item Define a discrete probability $\QQ$ fully supported over that countable set $\Scal$.
    \item For any $x\in (\Xcal^d)^*$, compute its marginal likelihood $\PP(x)= \sum_{T\in \Scal} \PP(x|T)\QQ(T)$, where $\PP(\cdot|T)$ is the probability distribution function computed by $T$. By \cref{cor:correpondance_proba_code}, the negative log marginal likelihood $-\log \PP$ is the coding length of a certain Shannon code over $(\Xcal^d)^*$. By \cref{eg:Bayes_code_better}, this code is shorter than any two-part code in any UFCC using the same r.e. set $\Scal$ of FCMs. 
\end{enumerate}
And different from FC complexity which is a two-part code, we define \emdef{Bayes codebook complexity (BC)} $C^{\text{BC}}_V(\cdot)$ as a one-part code, i.e. the shortest integer $p\in \NN$ such that the codebook corresponding to $-\log \PP$ above can decode $B(p)$ and output $x$.
\end{definition}
In other words, a UBCC is one single infinite codebook mechanism, over $(\Xcal^d)^*$. Given a UFCC $V$, we obtain a r.e. set $\Scal$ of FCMs, and we can build a UBCC $V'$ using $\Scal$ by assigning each FCM $T\in \Scal$ to a prior probability $\QQ(T)= 2^{-l_V(T)}$. By \cref{eg:Bayes_code_better}, $C^{\text{BC}}_{V'}(x)\leq C^{\text{FC}}_V(x)$ for all $x\in (\Xcal^d)^*$. Namely, $C^{\text{BC}_{V'}}$ is a more refined upper bound of Kolmogorov complexity than $C^{\text{FC}_V}$, although the r.e. sets of Turing machines that $V$ and $V'$ simulate are disjoint: $V'$ always \textit{computes} the same codebook and feed it with different binary inputs to output different $x$, while $V$ \textit{simulates} explicitly each FCM of which the input is a binary sequence.

However, to proceed the model selection over $\Scal$, Bayesian model selection in UBCC and two-part code model selection in UFCC coincide: they are both maximum a posteriori. For UFCC, the prefered FCM is $\min_{T,p}\{2l_V(T) + l(p)\}$ which equals $-\log\QQ(T) -\log\PP(x|T)$ for certain $\PP$ and $\QQ$ (the existence of them are again by \cref{cor:correpondance_proba_code}). For UBCC, Bayesian model selection chooses the $T$ that maximizes the posterior $\PP(T|x)$.

The Bayesian model selection in \cite{dhirbivariate} can be considered as a compression scheme between UFCC (pure two-part code) and UBCC (pure Bayes code). They defined their decision criterion of causal graph as the ratio of posterior

\begin{equation}
    \log\frac{\PP(G_1|x)}{\PP(G_2|x)}=\log\frac{\PP(x|G_1)\PP(G_1)}{\PP(x|G_2)\PP(G_2)}
\end{equation}

and $G_1$ is preferred to $G_2$ if the log ratio is positive. To represent the lack of knowledge over graph choices, they set the prior over graphs to be uniform. Since they choose the graph $G$ that maximizes $\PP(x|G)\PP(G)$, which is equivalent to minimize $-\log \PP(x|G) - \log \PP(G)$, their objective is also a two-part code: first encode the graph, and then encode a \textit{Bayes code} (negative log marginal likelihood) of $x$ given $G$.

The main difference between \cite{dhirbivariate} and the computational-theoretic objective (with reference machine UFCC or UBCC) is that the former approach \citep{dhirbivariate} aims at maximizing the posterior of a \textit{graph}, while the latter aims at maximizing the posterior of a \textit{Turing machine}, which, from \cite{dhirbivariate} or a probabilistic point of view, determines a graph and conditional probabilities on it. For \cite{dhirbivariate}, the conditional probabilities in the selected model are uncertain under a given prior. The preference of theories on causality is, in our view, fundamentally subjective.

%

Here we summarize all the compression games we mentioned in our paper:
\begin{itemize}
    \item Shannon source coding: as explained after~\cref{Thm:Shannon_source_coding}, Alice and Bob know the data distribution $\PP_X$. Alice wants to send iid samples losslessly to Bob using a codebook. Before sending iid data, they can design together a codebook.\\ Question: what is the shortest expected average coding length for each sample, among all possible codebooks? (without counting the length of the codebook)
    %
    \item Kolmogorov complexity: Alice and Bob share a programming language (C, Python) or a universal Turing machine. They do not know any structure of the data sequence $x$ to be sent.\\ Question: what is the length of the shortest program that Alice can send so that Bob can losslessly recover $x$?
    \item Finite codebook (FC) complexity (\cref{def:FC_complexity}): Alice and Bob share a finite universal codebook computer (UFCC,~\cref{def:UFCC}). They know that the data sequence $x$ is sampled in $\Xcal^d$ with precision $m$. Before sending data, they can design together a codebook $\Xcal^d \to \Bcal^*$.\\ Question: what is the length of the shortest program that Alice can send so that Bob can losslessly recover $x$? Since we define the UFCC to only accept two-part code (codeword and codebook), the question is equivalent to: what is the minimal sum of the length of the codewords and the length of the codebook mechanism (TM) to compress a certain sequence $x$?
    \item Bayes codebook (BC) complexity (\cref{def:UBCC}): Alice and Bob share a universal Bayes codebook computer (UBCC,~\cref{def:UBCC}). They know that the data sequence $x$ is sampled in $\Xcal^d$ with precision $m$. Before sending data, they can design together a codebook $(\Xcal^d)^* \to \Bcal^*$.\\ Question: what is the length of the shortest binary string (Bayes codeword) that Alice can send so that Bob can losslessly recover $x$?
\end{itemize}
%