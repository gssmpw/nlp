\begin{table}
    \small
    \renewcommand{\arraystretch}{.95}
    \centering
    \setlength{\tabcolsep}{1.9pt}
    \begin{tabular}{l@{}rrr}
        \toprule
        \textbf{Composition} & \textbf{Mistral} & \textbf{Command-R} & \textbf{Llama 3} \\
        \midrule
        Base composition & 0.449 & 0.535 & 0.461 \\
        [.5em]
        Definition & 0.485 & 0.575 & 0.497 \\
        Directional stimulus & 0.422 & 0.438 & 0.340 \\
        Persona & 0.450 & 0.528 & 0.362 \\
        Reasoning steps & 0.535 & 0.589 & 0.417 \\
        Demonstrations: Random & 0.537 & 0.530 & 0.566 \\
        Demonstrations: Category & 0.547 & 0.499 & 0.599 \\
        Demonstrations: Similar & \textbf{0.604} & 0.588 & 0.605 \\
        [.5em]
        Best on Test & \textbf{0.604} & \textbf{0.668} & \textbf{0.605} \\
        [.5em]
        Best SV selection & \textbf{0.604} & 0.650 & 0.599 \\
        Best SI selection & 0.548 & 0.660 & 0.576 \\
        [.5em]
        Adaptive prompting & 0.580 & 0.561 & 0.567 \\
        \bottomrule
    \end{tabular}

    \caption{Detection performance (macro F$_1$-score) of the prompting techniques per LLM on CobraFrames. Results marked in bold indicate the best score per LLM. \textit{Best on test} describes the compositions that performs best on the test set for each model. Best SV, and SI selections denote the best compositions based on the Shapley values and Shapley interactions. On this dataset, adaptive prompting does not improve over \textit{Best on Test}, but notably improves over the base composition and most individual techniques.}
    \label{tab:techniques-results-comparison-cobra}
\end{table}
