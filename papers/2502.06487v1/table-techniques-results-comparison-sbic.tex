\begin{table}
    \small
    \renewcommand{\arraystretch}{.95}
    \centering
    \setlength{\tabcolsep}{1.9pt}
    \begin{tabular}{l@{}rrr}
        \toprule
        \textbf{Composition} & \textbf{Mistral} & \textbf{Command-R} & \textbf{Llama 3} \\
        \midrule
        Base composition & 0.702 & 0.470 & 0.651 \\
        [.5em]
        Definition & 0.740 & 0.554 & 0.788 \\
        Directional stimulus & 0.725 & 0.410 & 0.542 \\
        Persona & 0.703 & 0.512 & 0.710 \\
        Reasoning steps & 0.656 & 0.436 & 0.621 \\
        Demonstrations: Random & 0.747 & 0.763 & 0.825 \\
        Demonstrations: Category & 0.737 & 0.733 & 0.806 \\
        Demonstrations: Similar & 0.712 & 0.729 & 0.822 \\
        [.5em]
        Best on Test & \textbf{0.792} & \textbf{0.788} & 0.831 \\
        [.5em]
        Best SV selection & \textbf{0.792} & \textbf{0.788} & 0.826 \\
        Best SI selection & \textbf{0.792} & \textbf{0.788} & 0.826 \\
        [.5em]
        Adaptive prompting & 0.790 & 0.758 & \ddag{} \textbf{0.842} \\
        \bottomrule
    \end{tabular}

    \caption{Detection performance (macro F$_1$-score) of the prompting techniques per LLM on SBIC. Results marked in bold indicate the best score per LLM. \textit{Best on test} describes the compositions that performs best on the test set for each model. Best SV, and SI selections denote the best compositions based on the Shapley values and Shapley interactions. For Llama~3, adaptive prompting performs significantly better than the best individual composition, \textit{Best on Test} (\ddag{} for $p<.01$).}
    \label{tab:techniques-results-comparison-sbic}
\end{table}
