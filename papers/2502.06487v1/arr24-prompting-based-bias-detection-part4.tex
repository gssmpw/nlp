\section{Experiments}
\label{sec:experiments}

In this section, we detail the adaptive prompting experiments that we carried out for the task of social bias detection on three datasets with three state-of-the-art instruction-tuned LLMs. We selected five common prompting techniques that fit the task, which we combine to create prompt compositions. We evaluate our approach against own and related-work baselines, both within and across datasets.



\subsection{Task}
Social bias detection describes the task of identifying texts that induce bias against a particular social group, through offensive language, stereotypes \cite{nadeem2021}, power dynamics \cite{sap2020,zhou2023a}, or similar. More context can support the bias detection, making it a well-suited task to evaluate prompt compositions.

In particular, the task requires knowledge about the state of the world to understand implicit biases and dynamics \cite{hovy2021,zhou2023a}. Prompting techniques such as in-context demonstrations can show how to apply knowledge acquired during pre-training to identify implicit biases. Furthermore, what is considered a biased text can vary, e.g., based on the target application. Including definitions of bias for and deducting reasoning steps could help to clarify the context and make the predictions more reliable. Lastly, whether a text is considered biased partly depends on the receiver. Instructing the LLM to assume a specific persona can clarify the evaluating perspective.



\subsection{Data}
\label{sec:experiments:data-llms}
To cover multiple aspects of social bias, we evaluate prompt compositions and their predictability on three datasets covering diverse intentions and target applications. In the following, we briefly describe each dataset and how we derive binary bias labels (preprocessing and prompt details in Appendix~\ref{sec:appendix-experimental-details}).


\paragraph{StereoSet}
The StereoSet corpus \cite{nadeem2021} serves the evaluation of stereotypes in generative models. Here, stereotypes are ``over-generalized [beliefs] about a particular group of people'' \cite{nadeem2021}. The data consists of scenarios and target groups that can be combined to create \textit{stereotypical} (biased), \textit{anti-stereotypical}, and \textit{meaningless} (both not biased) texts.


\paragraph{SBIC}
The Social Bias Inference Corpus (SBIC) \citet{sap2020} is intended to model multiple aspects of social bias explicitly. We use the \emph{implicit bias} aspect as the target label, which indicates text that are offensive towards a specific social group.


\paragraph{CobraFrames}
The CobraFrames corpus \cite{zhou2023a} captures the social and situational context of biased statements. Among others, it captures bias as implicit power dynamics or stereotypes between the speaker and the listener. We follow \citet{zhou2023a} in converting the \textit{offensiveness} label into binary bias representations.



\subsection{Prompting Techniques}
\label{sec:experiments:prompts}
We select five common prompting techniques to investigate potential benefits of prompt compositions over single techniques for social bias detection. We focus on discrete prompting techniques as opposed to continuous methods, such as prefix-tuning \cite{li2021b} or p-tuning \cite{liu2022a}. Notice, though, that our adaptive prompting is applicable to arbitrary techniques in principle.

Since we evaluate prompt compositions across three datasets with different structures and definitions of social bias, specific content parts of the prompts are adjusted to better align with the scenario of each dataset. In the following, we briefly describe each technique, including dataset alignments and mutations. See Appendix~\ref{sec:appendix-experimental-details} for details on their lexical representations and a full example.


\paragraph{Personas}

This technique aims to instruct the model to follow consecutive instructions from the perspective of a specific persona \cite{thoppilan2022,deshpande2023}. Among other use cases, personas are used to build translation systems \cite{he2024} and dialogue agents \cite{thoppilan2022,xu2023}, or to investigate biases in pre-trained LLMs \cite{beck2024}.

For social bias detection, a persona can help clarify the perspective from which a given text is being judged \cite{giorgi2024}. Since the intentions and goals vary across datasets, we expect that a persona prompt can clarify the setting of the task. In our experiments, we aim to formulate the persona description as close as possible to the scenario envisioned for each dataset. Similar to \citet{xu2023}, we seek to minimize positionality bias.


\paragraph{Definitions}

Including a definition of social bias can be seen as an extension of the task description to further specify its subject. This is supposed to make the interpretation of social bias in the respective dataset explicit, which is otherwise learned implicitly only in a supervised learning setup. Recent research has found that such definitions can increase the prediction performance in low-resource settings \cite{elsner2023}.

If the authors provide an explicit definition of bias, we reuse it. Otherwise, we manually derive a definition from available information.


\paragraph{In-context Demonstrations}
Known also as few-shot examples, this technique is a form of in-context learning that ``allows language models to learn tasks given only a few examples'' \cite{dong2024}. In-context demonstrations are provided during inference as part of the prompt, unlike traditional fine-tuning where model parameters are optimized in a supervised learning phase \cite{mosbach2023}. In-context demonstrations have been shown to improve results on various target tasks \cite{zamfirescu-pereira2023,dong2024}.

While seemingly simple, this technique entails several points of variation, including the \emph{number} and \emph{selection} of examples \cite{liu2022,zhang2022,levy2023,bertsch2024,dong2024} and their \emph{ordering} \cite{lu2022,shum2023,liu2024a}. To keep the experiments conceivable, we use one demonstration per bias type, adjust the number of similar demonstrations accordingly, and do not evaluate the ordering, but we focus on three common variations to select demonstrations:
\begin{itemize}
\setlength{\itemsep}{0pt}
    \item \emph{Random.} We pseudo-randomly select training instances, which are used as demonstrations for all instances in the test split.
    \item \emph{Similarity.} For each instance in the test split, we select the most similar instances from the training split as demonstrations, using SBERT \cite{reimers2019}.
    \item \emph{Category.} We select instances that cover all bias types covered in a dataset, as diversifying demonstrations has been shown to aid prediction \cite{levy2023,zhang2022}.
\end{itemize}



\paragraph{Directional Stimulus}
Directional stimuli \cite{li2023} describe the technique to include instance-specific hints and are meant to guide the LLM. We include a list of dataset-specific bias types that could be present in the text instance.



\paragraph{Reasoning Step Instructions}
Initially intended for ``arithmetic, commonsense, and symbolic reasoning tasks'' \cite{wei2022}, the main idea of this technique is to decompose the given task into smaller, more approachable sub-tasks \cite{dong2024}. Reasoning step instructions have been applied to various tasks and can lead to improvements in prediction performance \cite{wei2022}.

As the detection of social bias in texts can often naturally be decomposed into step-wise questions \cite{sap2020,zhou2023a}, we include reasoning steps as an additional technique. We evaluate both zero-shot and few-shot settings. To do so, we follow \citet{press2023} and \citet{zhou2022a} in formulating the reasoning steps as task- and data-specific sub-questions covering the aspects of social bias in the respective dataset before prediction. To ensure that all predefined reasoning steps are followed as intended, we separate the reasoning steps into multiple consecutive inference steps \cite{dong2024}, implemented as a practical chain-of-prompts pipeline \cite{zhou2022a}.





\subsection{Models}

We realize our \emph{adaptive prompting} approach for three different \emph{instruction-tuned LLMs} as follows.


\paragraph{Adaptive Prompting}
Given the five prompting techniques, we fine-tune a DeBERTA-v3-large encoder model \cite{he2023} to predict the optimal composition ad-hoc, i.e., for a given input, as detailed in Section~\ref{sec:method}. Since the prompting techniques include three variants of in-context demonstrations that not compatible, the model predicts probabilities for $2^{4} * (3 + 1) = 64$ compositions (cf. Equation~\ref{eqn:number-of-compositions}). The composition with the highest probability is then used for the social bias classification.

We train a adaptive prompting model on the train split of each dataset, optimize it on the validation split, and evaluate its performance on the test split. We further train one adaptive prompting model per combination of dataset and LLM. Each model is trained with five different pseudo-random seeds.


\paragraph{Instruction-tuned LLMs}
We generate predictions with three instruction-tuned open-weight LLMs. To reliably generate classification labels, we use constrained decoding \cite{beck2024}, limiting the output to binary labels. Our LLM selection aims to diversify architecture, pre-training data, and size, as the effectiveness of prompting techniques may depend such factors (details on each model can be found in Appendix~\ref{sec:appendix-experimental-details}):
\begin{itemize}
\setlength{\itemsep}{0pt}
\item
\emph{Mistral.} The smallest LLM is Mistral-7B-Instruct-v0.2 \cite{jiang2023} with seven billion parameters. Its architecture focuses on generation performance and inference speed.
\item
\emph{Command-R.} As medium-sized LLM, we use C4AI Command-R v01 \cite{cohereforai2024} with 35 billion parameters. At the time of writing, architectural details were not available.
\item
\emph{Llama 3.} The largest evaluated LLM is Meta Llama 3 \cite{dubey2024}, with 70 billion parameters. It builds on a dense Transformer architecture to allow for easier scaling.
\end{itemize}



\subsection{Baselines}
\label{sec:experiments:baselines}

In addition to \emph{random} and \emph{majority} predictors that serve as lower bounds, we evaluate the following baselines to gain a comprehensive overview of the composition capabilities of our approach:


\paragraph{Self-Diagnosis}
Self-Diagnosis \cite{schick2021} adopts a Q\&A setting and a GPT-2 XL model \cite{radford2019} to identify social bias.

\paragraph{DeBERTa fine-tuned}
We fine-tune a DeBERTa-v3-large model \cite{he2023} in a supervised learning setting. It provides a reference to compare inference-only approaches and prompt compositions to a more traditional learning setup.

\paragraph{Ensemble}
The ensemble returns the label that was predicted most often across compositions. It helps to assess the value of adaptive prompting, compared to simply relying multiple compositions.


\paragraph{Best on Val/Test}
The Best on Val baseline represents the best-performing composition on the validation split. It gives insights as to whether adaptive prompting can perform better than any single composition on the evaluated datasets. Best on Test does the same for the best test split composition; notice that this knowledge is \emph{not} given in practice.

\paragraph{Oracle}
This upper bound produces a correct prediction if \emph{any} of the prompt compositions predicts the correct label. The oracle represents the hypothetical best performance that can be achieved.
