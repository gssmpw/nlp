\section{Experimental Details}
\label{sec:appendix-experimental-details}



\subsection{Instruction-tuned LLMs}
Below, we shortly describe the information available for each LLM included in our evaluation.


\paragraph{Mistral}
The smallest model we evaluate is the instruction-tuned variant of
Mistral-7B-Instruct-v0.2 \cite{jiang2023}. It is based on the transformer architecture and introduces several architectural changes that improve its generation performance and inference speed. It is trained with seven billion parameters and has a context size of 32 thousand tokens.%
\footnote{\url{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2}}
The authors do not publish information about the models' training data and procedure.


\paragraph{Command-R}
As medium-sized LLMs, we include C4AI Command-R v01 \cite{cohereforai2024}. The LLM is trained with 35 billion parameters and has a context length of 128 thousand tokens. To the best of our knowledge, no details on the architecture and training procedure are available at the time of writing.


\paragraph{Llama 3}
Lastly, as the biggest LLM, we include a instruction-tuned variant of the Llama~3 model family, namely Meta-Llama-3-70B-Instruct \cite{dubey2024}. The LLM was pre-trained with 70 billion parameters on more than 15 trillion tokens and has a context length of eight thousand tokens. The instruction-tuning was achieved with a mix of supervised fine-tuning and Reinforcement learning with Human Feedback \cite{dubey2024}. Similar to the Mistral and Command-R model, the authors do not publish information about the training data and procedure but specify the training data cut-off as December 2023 \cite{dubey2024}.


\paragraph{Generating Binary Classification Labels}
While standard classification models predict a single label and distribute probabilities only over the specified labels, instruction-tuned LLMs generate fluent text that is only restricted by the text in their training data. Reliably generating classification tokens is thus a challenge as, in some cases, LLMs might also generate tokens other than the expected labels.

To alleviate this problem, we restrict the logits of the models to the labels using constrained decoding \cite{beck2024}, as implemented in the outlines%
\footnote{\url{https://github.com/outlines-dev/outlines}}
and vLLM%
\footnote{\url{https://github.com/vllm-project/vllm}}
libraries. We restrict the decoding to the tokens ``Yes'' and ``No''.%
\footnote{In a pilot study, we also experiment with other versions of the generated token, such as yes/no, y/n, and 1/0, but find that Yes/No to produce the best results.}
To still allow for the generation of reasoning steps that require generating more than the binary labels, we first generate the reasoning steps without any restrictions and only restrict the decoding for the final label prediction.


\paragraph{Full Example Prompt}
Figure~\ref{fig:example-prompt} shows an example of a full and unmodified prompt composition that includes all possible prompting techniques with similarity-based in-context demonstrations, as used in our experiments for StereoSet.



\subsection{Technical Inference Setup}
The instruction-tuned LLMs for the social bias detection are run on four A100-SXM4-80GB and 16 H100-SXM-80GB GPUs. To ensure efficient inference given the numerous prompt compositions, we use the vLLM library with dynamic batching alongside the outlines library for constrained decoding. Additionally, inference is parallelized across the different prompt compositions to accelerate the overall process. With this setup, the inference for Mistral across all three datasets was completed in 26 hours, while the inference required 73 hours and 140 hours, for Command-R and Llama~3, respectively.










\subsection{Significance Testing}
We test for significant improvements of the proposed Adaptive Prompting approach over the best individual composition (if Adaptive Prompting shows the best overall results), as indicated in Table~\ref{tab:techniques-results-comparison-stereoset}, Table~\ref{tab:other-tasks-results}, Table~\ref{tab:techniques-results-comparison-sbic}, and Table~\ref{tab:techniques-results-comparison-cobra}.

Since we have access to all per-instance predictions for all models, we employ a one-sided independent $t$-test to compute significance levels of potential improvements of the Adaptive Prompting approach over the best individual compositions. We compute the significance levels over the results of all five random seeds per model and approach combination. The distributions of individual results matched the $t$-test assumptions.

We test for the two common $p$-values $p<0.05$ and $p<0.01$.
