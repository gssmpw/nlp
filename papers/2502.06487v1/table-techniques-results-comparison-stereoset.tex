\begin{table}
    \small
    \renewcommand{\arraystretch}{.95}
    \centering
    \setlength{\tabcolsep}{1.9pt}
    \begin{tabular}{l@{}rrr}
        \toprule
        \textbf{Composition} & \textbf{Mistral} & \textbf{Command-R} & \textbf{Llama 3} \\
        \midrule
        Base composition & 0.711 & 0.462 & 0.575 \\
        [.5em]
        Definition & 0.716 & 0.527 & 0.637 \\
        Directional stimulus & 0.662 & 0.584 & 0.566 \\
        Persona & 0.698 & 0.546 & 0.539 \\
        Reasoning steps & 0.697 & 0.509 & 0.610 \\
        Demonstrations: Random & 0.665 & 0.674 & 0.725 \\
        Demonstrations: Category  & 0.681 & 0.675 & 0.739 \\
        Demonstrations: Similar & 0.761 & 0.701 & 0.798 \\
        [.5em]
        Best on Test & 0.800 & 0.706 & 0.817 \\
        Best by Shapley values & 0.790 & 0.588 & 0.798 \\
        Best by Shapley interaction & 0.795 & 0.671 & 0.800 \\
        [.5em]
        Adaptive prompting & \textbf{0.809} & \dag{} \textbf{0.781} & \ddag{} \textbf{0.853} \\
        \bottomrule
    \end{tabular}

    \caption{Macro F$_1$-score of each individual technique and selected prompt compositions on StereoSet (others in Appendix~\ref{sec:appendix-extended-results}). Results marked in bold indicate the best score per LLM. \textit{Best on test} describes the compositions that perform best on the test set for each model, and the two rows the best compositions based on Shapley values and interactions. Adaptive prompting is significantly better than \textit{Best on test} (\dag{} for $p<.05$, \ddag{} for $p<.01$).}
    \label{tab:techniques-results-comparison-stereoset}
\end{table}
