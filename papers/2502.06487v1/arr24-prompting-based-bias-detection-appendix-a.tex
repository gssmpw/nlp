\section{Dataset Details}
\label{sec:appendix-data-details}

While all three datasets model some aspect of social bias, each dataset has a different goal, such as modeling multiple aspects of social bias, clarifying contextual settings, or evaluating stereotypes in generative language models. This section, therefore, extends on Section~\ref{sec:experiments:data-llms} to detail each corpus and the steps taken to prepare the datasets used in our experiments, including SBIC, Stereoset, and CobraFrames.

Table~\ref{tab:dataset-statistics} provides an overview of the number of positive and negative text instances per corpus and split.

\paragraph{In-context Demonstrations}
For each corpus, we select the most similar instances based on the cosine similarity of the sentence embeddings. We use the all-mpnet-base-v2 model from the Sentence-BERT library \cite{reimers2019} to generate the embeddings. For similarity-based demonstrations, we include the same number of demonstrations as for the category-based demonstrations to keep the experiments as comparable as possible. We thus include seven, four, and eleven demonstrations for the SBIC, StereoSet, and CobraFrames corpus, respectively.



\subsection{StereoSet Corpus}
\paragraph{Preprocessing}
For our experiments, we use the intersentence dataset of the StereoSet corpus. We construct instances with binary annotations by combining the provided context with each of the three assigned targets from the original dataset (labeled as \textit{anti-stereotype}, \textit{stereotype}, and \textit{unrelated}). Each context-sentence pair is then treated as a new instance.

We assign binary bias labels to the instances, where \textit{stereotype} sentences represents a bias text instance, while \textit{anti-stereotype} and \textit{unrelated} sentences represent a non-biased instance.

This method results in 6,369 instances. We pseudo-randomly split the data into training, validation, and test set with an 80/10/10 ratio.

\paragraph{Prompt Adjustments}
Since \citet{nadeem2021} focus on stereotypes prevalent in the USA (i.e., the selection of crowd workers was explicitly restricted to the USA), we include this information about the geographical target region in the definition of bias. Furthermore, we derive the reasoning steps from the data annotation guidelines and instruct the model to follow the persona of an annotator.



\subsection{Social Bias Inference Corpus}
\paragraph{Preprocessing}
To enhance data quality, we remove duplicate texts from the dataset and preprocess the remaining texts by removing characters, such as newlines, html entities, unicode characters, and multiple whitespaces. Since the original bias implication labels (\texttt{hasBiasedImplications}) in the SBIC dataset seem to be formatted incorrectly, with positive (label 1) indicating no bias and a negative (label 0) indicating bias. We, therefore, switch the labels so that a positive label indicates the presence of bias and a negative label indicates no bias.

We utilize the original validation and test splits provided by \citet{sap2020} with 4,666 and 4,691 samples, respectively. As explained in Section~\ref{sec:experiments} we sub-sample the training split, pseudo-randomly sampling 5,000 instances. The sampling is done in a stratified way that ensures a uniform distribution of the bias categories, as well as the bias label.

\paragraph{Prompt Adjustments}
Since the corpus combines data collected from micro-blogging platforms and forums, we instruct the model to assume the persona of a social media safety officer whose task is to flag biased social media posts. We further align the reasoning steps to the annotation questionnaire presented to the crowd workers, as published by \citet{sap2020}.



\subsection{CobraFrames Corpus}
\paragraph{Preprocessing}
Following the approach of \citet{zhou2023a}, we construct instances by concatenating the speaker identity, listener identity, speech context, and statement (available annotations for each instance) in a sentences as follows: ``This is a conversation between \texttt{[speakerIdentity]} and \texttt{[listenerIdentity]} in \texttt{[speechContext]}: \texttt{[statement]}.''

To generate binary social bias annotations, we convert the offensiveness dimension into a binary format based on the presence of specific phrases (e.g., ``offensive'', ``microaggression'' or ``xenophobic''), again following the approach of \citet{zhou2023a}.

For our experiments, we utilize both CobraCorpus and CobraCorpus-CF. From CobraCorpus, we pseudo-randomly sample 2,000 instances each for the training and validation sets in a stratified way, maintaining the original distribution of bias categories and bias labels. The CobraCorpus-CF is used as an additional test set.

To align the target group annotations between CobraCorpus-CF and CobraCorpus, we compute sentence embeddings for each target group in both corpora using the sentence-transformers library \cite{reimers2019} and the \texttt{all-mpnet-base-v2} model. Subsequently, each instance in CobraCorpus-CF was assigned the label of the target group from CobraCorpus with the highest cosine similarity. We manually validate the correctness of this process on several instances. Target groups that appeared in fewer than five instances in the resulting CobraCorpus-CF were excluded. The final test split comprised 1,939 samples.

\paragraph{Prompt Adjustments}
Since the dataset is designed around situational contexts with speaker and listener parties, we instruct the model to assume the persona of a third party overhearing the utterance and knowing about the identity and background of the speaker and listener. To create reasoning steps, we first annotate the intent, the potential target minority, and the implied statement before generating the final bias label prediction.




\subsection{Dataset Subsampling}
Due to the extensive nature of our experiments (i.e., we need to predict a label for each instance $|C|$ times, once for each composition, across all seeds), we sub-sample the training and validation splits of the SBIC and CobraFrames datasets. The exact number of instances per split and label are shown in Table~\ref{tab:dataset-statistics}.
