\section{Results and Discussion}
\label{sec:results}

\bsfigure{performance-stereoset}{Social bias detection results on StereoSet~(others in Appendix~\ref{sec:appendix-extended-results}: Figures~\ref{performance-sbic}--\ref{performance-cobra}): Macro F$_1$-score of all prompt compositions with each LLM (baselines shown as vertical lines). Our adaptive prompting approach (\textit{Ours StereoSet}) outperforms all fixed compositions. \emph{Ours SBIC} and \emph{Ours Cobra} are trained on other datasets. The variance over all compositions (shown as box plots) indicates the LLMs' sensitivity to the prompt.
}

Figure~\ref{performance-stereoset} shows the results of the prompt composition evaluation on StereoSet (see Appendix~\ref{sec:appendix-extended-results} for results on SBIC and CobraFrames). In the following, we highlight and discuss findings indicating the benefit of prompt compositions. Furthermore, we find that \emph{instance-specific} compositions can perform better than any \emph{single} composition alone.


\hwfigure{network-stereoset}{Network plots of the shapley interactions for the three evaluated LLMs on StereoSet (others in Appendix~\ref{sec:appendix-sv-composition-analysis}: Figures \ref{network-sbic}-\ref{network-cobra}), revealing unique interaction structures among the models. Node size represents strengths of first-order interactions. Line width and translucency denote strengths of second-order interactions. Red color denotes positive interaction (increasing the performance), and blue color denotes negative interaction (decreasing the performance).}


\subsection{Impact of Prompt Composition}

The results highlight the potential benefit of using prompt compositions compared to individual techniques or the base composition (exemplified for StereoSet in Table~\ref{tab:techniques-results-comparison-stereoset}). In experiments on Stereo\-Set, prompt compositions outperform individual techniques across all LLMs. The same is true for SBIC, but not for CobraFrames. While single techniques can still perform better when negative interactions between techniques inside a composition exist, our experiments highlight the benefit of using prompt compositions when choosing its techniques correctly. This is supported by the Shapley interactions, showing several positive interaction between techniques. Since compositions perform consistently better in our experiments, the results suggest that the benefit of compositions over single techniques holds across LLM architectures and sizes.

Some techniques are, however, notably more often present in the best-performing prompt composition than others, highlighting their positive impact. Table~\ref{tab:composition-frequencies-stereoset}, Table~\ref{tab:composition-frequencies-sbic}, and Table~\ref{tab:composition-frequencies-cobra} show how often each composition was chosen by our approach. On StereoSet, for example, in-context demonstrations are included in compositions that perform best on the test set and the validation set across models. A similar pattern exists for SBIC and CobraFrames. This finding is further supported by Shapley values and interactions, which highlight the strong positive contributions of in-context demonstrations.

Instead of using prompt compositions that include a selected technique with positive impacts, we observe that no single composition performs best across all datasets and LLMs in our experiments. Adapting the composition to input and LLM automatically is thus a crucial endeavor.

\input{table-techniques-results-comparison-stereoset}



\subsection{Volatility of Composition Performance}

In line with previous research \cite{zamfirescu-pereira2023,errica2024,memon2024}, our results highlight the sensitivity of current LLMs towards changes in prompt composition and data. While all evaluated compositions perform better than the \emph{Self-Diagnosis} baseline, there is a performance gap between the best and the worst prompt composition for all three LLMs (see Figure~\ref{performance-stereoset}). This stresses the difficulty of choosing a prompt that performs consistent across models (e.g., for Llama~3, 0.817 macro F$_1$ with the best composition, 0.483 with the worst). Using the worst compositions even partly led to results below the \emph{random baseline} or \emph{majority baseline} for Command-R and Llama~3.

Furthermore, the distance of \emph{best test} to the median indicates that the best compositions elicit very different behavior in the model compared to the majority: For Command-R and Llama~3, there are disparities of 0.115 and 0.107 between the median (0.591 and 0.710) and \emph{best test} (0.706 and 0.817).

Due to this sensitivity of LLMs to prompts and input data, the composition that elicits the best Macro F$_1$ also varies across a dataset. For example, while the best composition to predict the social bias label on Stereo\-Set contains \emph{definitions}, \emph{in-context demonstrations}, and a \emph{persona} for Llama~3, it is not the optimal composition for all instances and LLMs. Relying on a single composition for a whole dataset can, therefore, affect performance in unforeseeable ways. This clearly underlines the benefit of choosing LLM- and input-specific prompt compositions with an adaptive prompting approach.


\subsection{Impact of Adaptive Prompting}

The performance of our approach is particularly visible on StereoSet and SBIC. Choosing prompt compositions adapted to the input instance produces more reliable social bias detection on StereoSet across all three LLMs, for example, boosting the F$_1$-score from 0.706 to 0.781 for Command-R. On SBIC, the performance varies. Still, our approach is at least competitive to the best composition with Mistral (0.792 for \emph{best test} vs.\ 0.790 for adaptive prompting) and outperforms it with Llama~3 (0.831 vs.\ 0.842). These results provide further support for the idea of selecting input-specific compositions.

Adaptive prompting also ensures the use of prompt compositions that outperform (or are competitive to) \emph{DeBERTa fine-tuned}, whereas, on all three LLMs, most compositions perform worse. For example, in Figure~\ref{performance-stereoset}, the median composition score is 0.698 with Mistral vs.\ 0.781 for DeBERTa. With Command-R (0.591), it is even closer to the \emph{random baseline} (0.497) than to DeBERTa (0.781).


\subsection{Shapley Prompt Composition Analysis}

The results of the Shapley-based analysis further support the benefit of adaptive prompt compositions and find strong interactions between several prompting techniques, exemplified in Figure~\ref{network-stereoset} for StereoSet (details in Appendix~\ref{sec:appendix-sv-composition-analysis}).

While making use of prompting techniques improves performance in general, simply adding all possible techniques to a composition does not consistently enhance performance compared to providing only a task description across settings. This highlights the benefit of using and prompt compositions adapting them to the input instance.

Furthermore, the Shapley-based results suggest that the selection of compositions requires empirical validation or optimization, as the best-on-test compositions never contain all techniques but rather a heterogeneous set. The heterogeneity of the compositions suggests the need for a more stringent mechanism in selecting the best compositions, such as learning a meta-composition prediction model (such as \emph{adaptive prompting}) or conducting a game-theoretic assessment.

Lastly, choosing the composition based on Shapley values instead of our Adaptive prompting improves performance compared to baselines where no additional information is used, i.e., using no technique or all techniques. Modeling the selection problem with Shapley interactions instead further improves the performance of composition choices over Shapley values for StereoSet.


\subsection{Encoder Evaluation}

To investigate the performance of the encoder model, we evaluate its ability to predict optimal compositions that result in correct classifications.

Furthermore, we evaluate the composition selection frequencies and how often each composition resulted in correct predictions. This method shows whether the encoder model simply overfits the training set and simply predicts the most common composition (further details in  Appendix~\ref{sec:appendix-extended-results}).

The results suggest that the encoder model indeed learns to select optimal compositions. While the encoder performs better in predicting optimal compositions for Llama~3 and worse for Command-R on StereoSet and SBIC, the results are more mixed on CobraFrames.

Furthermore, comparing composition prediction frequencies across datasets indicates that the encoder model does not overfit the training set. Given that no single composition results in notably more correct predictions in the training split of each corpus, there is also little incentive for the encoder model to overfit to a single composition.

For CobraFrames, however, the results suggest that the encoder model can likely not learn meaningful connections between the inputs and compositions. This behavior, in turn, likely causes the comparably low performance of our adaptive prompting on CobraFrame. Adaptive prompting does, however, still avoid the risk of choosing a very ineffective prompt on CobraFrames.

Overall, the optimal compositions chosen by our encoder model show promising results. However, larger encoder models might be able to encode dependencies between text instances and prompt composition performance even better and deal with complex inputs more reliably.



\subsection{Adaptive Prompting across Datasets}

\emph{Ours Cobra} and \emph{Ours SBIC} in Figure~\ref{performance-stereoset} have been trained on the other datasets. The results  suggest that adaptive prompting does not perform as well across datasets compared to in-dataset training. This issue might be partially related to the sensitivity of LLMs to the prompt discussed above, that is, knowledge of prompt compositions may not be transferable across datasets. A potential reason for performance disparities could be the domain and format of the input text. While instances in StereoSet and CobraFrames are curated and contain sentences with a clear structure, those in SBIC come from online forums with noisy elements.

In some settings, though, our approach seems to generalize across datasets to some extent; for example, \emph{Our Cobra} performs above the median score on all three LLMs on StereoSet. This finding supports our hypothesis that the input data format can be relevant for predicting prompt compositions.

\subsection{Adaptive Prompting for Other Tasks}

To further validate the value of adaptive prompting, we trained and evaluated our approach on three additional tasks: sentiment analysis, natural language inference, and question answering (see Table~\ref{tab:other-tasks-results} in Appendix~\ref{sec:appendix-extended-results}). While the gains over single compositions are smaller than for social bias detection, adaptive prompting performs significantly better than the \emph{base composition} in all cases and generates better predictions than the \emph{Best on Val} baseline on all three tasks (recall that \emph{Best on Test} is more theoretical, as it cannot be found in practice).

We conclude that the idea of composing prompts ad-hoc dependent on the input instance (as realized for the first time in our approach) may have potential for many NLP tasks. Further investigations are left to future research.
