\section{Extended results}
\label{sec:appendix-extended-results}



\subsection{Encoder model evaluation}

To investigate the raw performance of the adaptive prompting model in predicting prompt compositions ad-hoc based on the input text, as detailed in Section~\ref{sec:method}, we evaluate its ability to predict a composition that results in a correct classification (i.e., the optimal composition). This allows for a more direct view at the performance of the encoder model chosen for the approach.

Since our primary interest is an encoder model that is able to predict a composition that produces a correct classification for a given text instance and LLM, we consider all such compositions to be correct predictions of the encoder model ($\#correct\_predictions$). We then simply divide this number by the total number of instances ($\#instances$) in the dataset to calculate a ratio of correct predictions over the full dataset (i.e., $\frac{\#correct\_predictions}{\#instances}$). Like other classification metrics, the score range is $[0,1]$, where $1$ represents the best score. The results are shown in Table~\ref{tab:composition-prediction-performance-results}.

Furthermore, Table~\ref{tab:composition-frequencies-stereoset}, Table~\ref{tab:composition-frequencies-sbic}, and Table~\ref{tab:composition-frequencies-cobra} show the frequencies of how often the adaptive prompting approach chose a specific composition as the optimal composition and how often each composition produced a correct prediction for each model on the train dataset. All frequencies are averaged over five random seeds. This additional data is useful to evaluate, whether the encoder model overfits on the training dataset and simply predicts the most-common composition.



\bsfigure{performance-sbic}{Social bias detection results on SBIC: Macro F$_1$ of all prompt compositions for each LLM.}
\bsfigure{performance-cobra}{Social bias detection results on CobraFrames: Macro F$_1$ of all prompt compositions for each LLM.}




\subsection{Detailed Prompt Composition Results}
Figure~\ref{performance-sbic} and Figure~\ref{performance-cobra} show the boxplots for SBIC and CobraFrames, respectively.

Table~\ref{tab:techniques-results-comparison-sbic} and Table~\ref{tab:techniques-results-comparison-cobra} show a summary of the results, comparing individual techniques and adaptive prompting, similar to Table~\ref{tab:techniques-results-comparison-stereoset}.

Table~\ref{tab:compositions-performance-stereoset}, Table~\ref{tab:compositions-performance-sbic}, and Table~\ref{tab:compositions-performance-cobra} show the results for each evaluated composition on Stereoset, SBIC, and CobraFrames, respectively.



\subsection{Adaptive Prompting for Various Tasks}

Table~\ref{tab:other-tasks-results} shows the results of our adaptive prompting on three further tasks: For sentiment analysis, we use the Aspect Based Sentiment Analysis corpus \cite{pontiki2014}, also referred to as ABSA. For natural language inference, we use the e-SNLI corpus \cite{camburu2018}. Lastly, for question answer, we use the CommonsenseQA corpus \cite{talmor2019}.

We format the prompt as \texttt{<Q> question text <A> answer text}, for which the predicted label indicates whether the answer is correct, given the preceeding question. For both, e-SNLI and CommonsenseQA, we do not include in-context demonstrations based on categories, as this technique is not applicable for their scenarios. Otherwise, all results were retrieved using the same methodology and experimental setup presented in Section~\ref{sec:method} and Section~\ref{sec:experiments}. As LLM, we employ Mistral.

Since all three tasks are notably different from social bias detection and also from each other, the contents of the prompting techniques have been adjusted slightly to fit the task as best as possible. Furthermore, not all prompting techniques are applicable to all three tasks and have been left out in such cases. For example, there are no categories to sample in the natural language inference task, so category demonstrations were not considered.



\input{table-dataset-statistics}

\input{table-composition-prediction-performance-results}

\input{table-other-tasks-results}

\input{table-techniques-results-comparison-sbic}
\input{table-techniques-results-comparison-cobra}

\input{figure-example-prompt}

\input{table-composition-frequencies-stereoset}
\input{table-composition-frequencies-sbic}
\input{table-composition-frequencies-cobra}

\input{table-compositions-performance-stereoset}
\input{table-compositions-performance-sbic}
\input{table-compositions-performance-cobra}
