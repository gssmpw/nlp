\section{Related Works}
\paragraph{Humanoid Whole-Body Control}
Whole-body control for robots with multiple parts is a long-standing challenging problem. For humanoid robots, whole-body control with only feet contacting the ground is already a state-of-the-art research topic, due to the number of multiple rigid bodies attached on the system. Traditional methods depend on modeling the dynamics of the entire humanoid skeleton~\cite{matthew2021MIThumanoids, behzad2008whole, grizzle2009mabel, Moro2019whole, antonin2023synchronized, kajita2001the}. To simplify the computation and deploy the algorithm onboard in real-time, model-based methods use an inverted pendulum model to plan the footstep positions and then use whole-body control methods to acquire the targets for each motor, but without any additional contacts with the environment~\cite{Moro2019whole, kajita2001the}. However, these methods significantly limit the possibility of contacting the environments with components other than feet. For example, bending down and crawling on the floor introduces more contact such as knees and hands. These contacts can be unpredictable or cannot be predicted accurately as the model predictive control requires.

In learning-based methods, whole-body control is still a challenging topic. Most of them can be viewed as a combination of lower-body and upper-body~\cite{lu2024pmp, fu2024humanplus, cheng2024express, gu2024humanoid}. More integrated humanoid algorithms control all the joints on the humanoid robot but they only involve the contacts of feet~\cite{zhuang2024humanoid, ilija2024humanoid, serifi2024vmp, long2024learninghumanoidlocomotionperceptive, he2024learning, gu2024humanoid} or the contacts of feet and hands~\cite{zhang2024wococo, he2024omnih2o}. These workspaces significantly limit the potential of humanoid morphology. Unexpected contacts will make the robot significantly deviate from its original position and lead to further chaos. For example, a humanoid robot can sit on the driver's seat at emergency, but the contact of the humanoid's hip is not considered in this previous humanoid whole-body control research. Also, when dealing with skills on the floor, the current learning-based framework does not take knee, torso, or elbow collisions with the ground into consideration. This does not distinguish humanoid robots from quadruped robots if manipulators are attached~\cite{ma2022combining, zhang2024learning}.

\paragraph{General Motion Interface for Humanoid}
To design a unified interface for large models to control the humanoid robot without real-time requirements, whole-body control algorithms for humanoid robots design the interface depending on how the general motion is defined. For mobile manipulation tasks, the general interface can be described as a locomotion goal and upper-body joint position goal, such as~\cite{zhuang2023robot, zhuang2024humanoid, cheng2024express}. Specifically, locomotion can also be defined as a short-range navigation task~\cite{lee2020learning, yang2021real, miki2024learning}. However, all these interfaces intrinsically limit the potential of more complex behaviors, resulting in less flexibility to meet the fine-detailed motion targets.

In humanoid motion generation research, \citet{peng2018deepMimic, tessler2023calm, Luo2023PerpetualHC} defines all joint orientation and base position sequences as the interface. However, the imitation target is too strong. The robot has to follow the motion target at each timestep, leading to less tolerance when the motion target is not physically feasible for the current robot model. Although this interface can be applied to the real robot system~\cite{he2024learning, fu2024humanplus, he2024omnih2o}, this instant motion target still has to meet the real-time requirement. To deal with this issue, \citet{zargarbashi2024robotkeyframing, tessler2024maskedmimic, zhang2024wococo} propose using a sequence of future motion targets for the robot's control policy to follow. Before the motion sequence is exhausted, the system that generates motion targets can run in non-real-time mode. However, these motion frames introduce global odometry, which introduces accumulated error as the system runs between the refresh of high-level motion commands. These works all use a global odometry system to track the accurate motion of the robot, which is expensive to acquire and deploy in a user-oriented environment. To what extent the accumulated error in odometry can be tolerated remains an unclear problem. In addition, to meet the accuracy requirement, link positions should also be taken into consideration. \citet{liu2024rdt, he2024learning, he2024omnih2o} add target link positions (in the base frame) as a part of the motion command. With masked selection, moving effectors or feet to the desired position lets the robot perform fine maneuvers. 

% motion generations method in graphics and animation research

\paragraph{Sim-to-Real for Legged Robots}
Due to the over-complication of search and conditioning of traditional model-based planning, reinforcement learning and training in simulation have been widely used recently in control for legged robots. By simplifying the collision shapes and using efficient GPU-accelerated rigid-body physics simulations, quadruped robots~\cite{rudin2022learning, kumar2021rma, escontrela2022adversarial} and humanoid robots~\cite{ilija2024humanoid, liao2024berkeleyhumanoid, xia2024dukehumanoid, cheng2024express, he2024learning, zhang2024wholebody, castillo2021robust} can perform various extreme difficult tasks, such as walking through rough terrain~\cite{nahrendra2023dreamwaq, agarwal2022legged, lee2020learning}, overcoming extreme challenging obstacles~\cite{david2024anymal, zhuang2023robot, cheng2023parkour, zhuang2024humanoid, long2024learninghumanoidlocomotionperceptive}. Training in simulators and zero-shot deployment in the real world seems to be a promising direction for the control algorithm for legged robots. However, these works lay a hidden assumption: only feet are making contact with the hard environment. For humanoid manipulation tasks, the training in simulation does not take hand contacts into consideration~\cite{zhang2024wococo, fu2024humanplus}. When deployed in the real world, the contacts generated by manipulation lead to rigidly attached objects to the robot system, however, it does not involve stochastic contacts of the other parts of the body. These attachments can be directly handled by domain randomization trained in simulation. In this work, we aim to face the contacts directly by training and deploying those contact-rich motions and show that it is possible to train using simplified collision shapes while successfully deploying the policy on the real robot.