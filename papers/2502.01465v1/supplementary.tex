\documentclass[conference]{IEEEtran}
\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}

\pdfinfo{
   /Author (RSS 2025 paper 464 author)
   /Title  (Robots: Our new overlords)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Robots;Overlords)
}

\begin{document}

% paper title
\title{\textbf{Supplementary for}: Embrace Collisions: Humanoid Shadowing for Deployable Contact-Agnostics Motions}

% You will get a Paper-ID when submitting a pdf file to the conference system
\author{Author Names Omitted for Anonymous Review. Paper-ID [464]}

\maketitle

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/mult-command-effectiveness.png}
    \caption{Comparison between multiple frames of motion command and single frame of motion command, in the real world.}
    \label{fig:multi-motion-command}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/kpop_dance_snapshot.png}
    \caption{Video snapshots for running Kpop-dance movement in the real-world.}
    \label{fig:kpop-dance}
\end{figure*}

\section{Video}
We attach a video describing the main idea and the real-world experiments in real-time with no accelerations. We perform real-world tests on all three types of motions. We encourage readers to see the video for a more comprehensive understanding of this work.

\section{More Results}
% video snapshots

% training speed experiment
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/multi_critic_success_rate.pdf}
    \caption{The success rate reported during training when trained on all 4 extreme difficult contact-agnostic motion.}
    \label{fig:training-success-rate}
\end{figure}
Figure~\ref{fig:training-success-rate} shows the training progress comparing multi-critic technique and single-critic technique. Using multi-critic setting leads to faster training speed as well as faster convergence rate.

% future motion command matters
We also do ablation study on why a sequence of motion command is necessary. We run the exact same policy onboard, but provide different length of motion command to the robot, shown in Figure~\ref{fig:multi-motion-command} The upper row presents the motion reference shown in simulator, which is also the immediate motion command to be achieved. The middle row presents the real-world robot's snapshots where only the immediate motion command is sent to the low-level policy. At current setting, the low-level policy only gets one frame of motion command. In the bottom row, the low-level policy receives a sequence of motion command. As the Figure~\ref{fig:multi-motion-command} shows, if future command is feed to the low-level policy, it presents preparing behavior to shift it's weight a bit forward to for the future standing behavior.

\section{Simulation Details}
% motion reference selection
As described in the main text, we select a handful of motion commands to train the humanoid whole-body controller to overcome the unbalanced issue of the precollected dataset.
\begin{table}[ht!]
    \centering
    \begin{tabular}{c|cc}
        \textbf{Dataset}        & \textbf{Subject}                                                                  & \textbf{Motion names}            \\ \hline
        CMU            & 140                                                                      & Get up from ground      \\ \hline
        KIT            & 3                                                                        & Crawling                \\ \hline
        Internet Video & {Bilibili BV1L34y1t71x} & Popping dance movement  \\ \hline
        Internet Video          & {BiliBili BV1Nm4y1k7wP} & Breaking dance movement \\ \hline
        Internet Video          & {Bilibili BV1mv411G7WM} & Jiu-jitsu movement     
    \end{tabular}
    \caption{The motion reference we select to build tine motion command dataset.}
    \label{tab:data-selection}
\end{table}

Shown in Table~\ref{tab:data-selection}, we train our policy in simulation by extracting motion command from these motion references.

% initialization
Considering these extreme difficult humanoid motion reference data may lead to physically infeasible situations, such as body parts penetrating the ground or robot base floating in the air, we set the robot's joint positions as the retarget robot pose in the first motion reference frame. We initialize the robot's base position by first adding a positive height to all motion reference frame so that no motion reference frame is penetrating the ground. Then we add a tiny height offset to spawn the robot, typically 0.06m. Also, to help the policy experience more states if the policy stuck as some place, for example if it does not get up from the ground, we sample the initial pose of the robot not only from the first frame of the motion reference trajectory, but from the start of the motion reference to the $60\%$ of it.

% how to build motion command
During each rollout, we select one motion reference to generate the motion command. At time $t$, we use a pre-sampled time interval $t_\text{int}$ and sample the motion reference at $t + t_\text{int}, t + 2t_\text{int}, t + 3t_\text{int}, t + 4t_\text{int}, t + 5t_\text{int}$ respectively. We also compute the base reference position and orientation in the base frame at time $t$.

\begin{table*}[ht!]
    \centering
    \begin{tabular}{l|ll}
                                              & \textbf{Names}                           & \textbf{Value}     \\
                                              \hline
        Environments                          & Number of robots                         & 4096               \\
                                                \hline
        \multirow{5}{*}{Domain randomization} & Scaling body mass                        & 0.8 $\sim$1.2      \\
                                              & Center of mass position                  & -0.02m $\sim$0.02m \\
                                              & Scaling motor stiffness                  & 0.9 $\sim$1.1      \\
                                              & Scaling motor damping                    & 0.9 $\sim$1.1      \\
                                              & Motor delays                             & 0.0s $\sim$0.03s   \\
                                              \hline
        \multirow{2}{*}{Initialize pose}      & Height offset                            & 0.04m              \\
                                              & Sampling frame ratio from the trajectory & 0.0 $\sim$0.6     
    \end{tabular}
    \caption{Detailed parameters for running the system in the simulator}
    \label{tab:domain-rand}
\end{table*}

\section{Training Details}

% reward combination
\begin{table*}[ht!]
    \centering
    \begin{tabular}{c|cl}
        Reward group                    & Reward term               & \multicolumn{1}{c}{Expression} \\ \hline
        \multirow{3}{*}{Task}           & Base position tracking    & $\Psi(\Delta(p_b), 0.4)$  \\
                                        & Base orientation tracking & $\Psi(\Delta(q_b), 0.8)$  \\
                                        & Joint position tracking   & $\Psi(\|\theta^j - \hat{\theta}^j\|, 0.3)$ \\ \hline
        \multirow{3}{*}{Regularization} & Action rate               & $\Psi(\|a^j_t - a^j_{t-1}\|, 1.0)$ \\
                                        & Joint acceleration        & $\Psi(\|\ddot{\theta}^j\|, 500)$ \\
                                        & Joint velocity            & $\Psi(\|\dot{\theta}^j\|, 15)$ \\ \hline
        \multirow{2}{*}{Safety}         & Joint position limit      & $\Psi(\max{(\theta^j - \theta^j_\text{max}, \theta^j_\text{min} - \theta^j)}, 0.1)$ \\
                                        & Joint torque limit        & $\Psi(\max{(|\tau^j| - 0.9~\tau^j_\text{max}, 0)}, 0.1)$
    \end{tabular}
    \caption{Reward terms and their expressions}
    \label{tab:rewards}
\end{table*}
In Table~\ref{tab:rewards}, the function $\Psi$ is a Gaussian kernel where,
\begin{equation}
    \Psi(a, b) = \exp{(-a / b^2)}
\end{equation}
Shown in Table~\ref{tab:rewards}, we build these reward functions in the range of $0 ~ 1$ so that everything is positive, potentially preventing active termination behavior. Then we \textbf{multiply} all reward terms in each reward group so that the algorithm will not completely ignore any of these terms. For the experiment variant using single critic, the reward terms within each group are multiplied and the reward groups are sum together weighted by the same weight parameters of the advantage mixing to get the scalar reward.

% PPO parameters + advantage mixing weights
\begin{table}[ht!]
    \centering
    \begin{tabular}{c|c}
        Hyperparameters          & Value               \\ \hline
        Optimizer                & AdamW               \\
        $\beta_1, \beta_2$       & 0.9, 0.999           \\
        Learning rate            & $1e-4$              \\
        Batch size               & 4096                \\
        Clip param               & 0.2                 \\
        Entropy coefficient      & 0                   \\
        min\_std clip            & 0.2                 \\
        Desired KL               & 0.01                \\
        Maximum gradient norm    & 1                   \\
        Num minibatches          & 4                   \\
        $\gamma$                 & 0.99                \\
        $\lambda$                & 0.95                \\
        Advantage mixing weights & [0.7, 0.1, 0.2]
    \end{tabular}
    \caption{Parameters in Algorithm implementation}
    \label{tab:ppo-param}
\end{table}

% policy network architecture and parameters
\begin{table}[ht!]
    \centering
    \begin{tabular}{c|c}
        Hyperparameters               & Value               \\ \hline
        Encoder Activation            & GELU                \\
        Encoder Project Activation    & ReLU                \\
        Encoder num heads             & 1                   \\
        Encoder num layers            & 2                   \\
        Encoder d\_model              & 128                 \\
        Encoder feedforward dimension & 128                 \\
        Encoder output size           & 128                 \\
        MLP hidden sizes              & [512, 256, 256] \\
        MLP Activation                & ELU                
    \end{tabular}
    \caption{The detailed network parameters for the low-level policy, which runs onboard}
    \label{tab:network}
\end{table}
The policy network consist of actor and multiple critics with the same structure. We use a transformer-based encoder block to encoder all motion command. The encoder outputs a sequence of embedding, which we select the embedding whose `time-to-target` attribute is the smallest positive value. We then concatenate this embedding with a stacked history proprioception observation and feed them to a Multi-Layer Perceptron. The MLP layers outputs the 29-dof action as the target position to the robot motors. Detailed parameters for the network are shown in Table~\ref{tab:network}.

% training computation power and time consumption
We train our algorithm on a Nvidia 4090D GPU with 4096 robots in parallel for about 72 hours from scratch. We build the simulation environment using IsaacLab and modify the reinforcement learning framework based on rsl\_rl.

\section{Deployment and Real-World Experiment Details}
\begin{table}[ht!]
    \centering
    \begin{tabular}{c|cc}
        \textbf{Joint name}                & \textbf{Stiffness (kp)} & \textbf{Damping (kd)} \\ \hline
        Left/right shoulder pitch/roll/yaw & 25                      & 1.0                   \\
        Left/right elbow                   & 25                      & 1.0                   \\
        Left/right wrist roll              & 25                      & 1.0                   \\
        Left/right wrist pitch/yaw         & 5                       & 0.5                   \\
        Waist roll/pitch                   & 60                      & 2.5                   \\
        Waist yaw                          & 90                      & 2.5                   \\
        Left/right hip pitch/roll/yaw      & 90                      & 2.0                   \\
        Left/right knee                    & 140                     & 2.5                   \\
        Left/right ankle pitch/roll        & 20                      & 1.0                  
    \end{tabular}
    \caption{Parameters that runs on the hardware}
    \label{tab:hardware}
\end{table}

% frequency, onnx, forward kinematics, rosbag, kp/kd
To run the trained policy on the real robot, we deploy the entire system on an Nvidia Jetson Orin NX and a laptop running Intel i5 CPU. We export the policy (including the transformer-based encoder) as an ONNX program. All components communicate using ROS2 in the network of Unitree G1 robot. We then run the policy on the Jetson board at 50Hz. Since the policy outputs the action as the target joint position of each motor on the robot, we use the built-in PD controller on the Unitree G1 robot, which runs at 1000Hz, with the kp/kd setting as shown in Table~\ref{tab:hardware}. These kp/kd parameters are also the same when training in simulator.

To acquire the target link position and their error respectively, we use Pytorch\_Kinematics~\cite{Zhong2024PyTorchKinematics} and ONNX~\cite{onnxruntime} to export the forward kinematics computation as an ONNX program. The exported ONNX program gets the joint positions and outputs the target link positions in the robot's base frame, which runs in real time on Nvidia Jetson Orin NX.

Since this work is also a proof-of-concept for building a hierarchical general humanoid controller, with a low-level whole-body control policy and a high-level command sender, we use another laptop to send the high-level command which simultaneously test the communication latency. Considering our high-level motion command is defined with base pose sequence under the robot frame when the command is generated, the high-level motion command for the real-world testing cannot be played directly from SMPL-based motion file. We play each motion in simulation using the well-trained low-level policy and record the motion command, as well as the base pose command under the robot's base frame in simulation. We then play this base pose command in the real world and ignore the difference between the robot trajectory in the simulator and in the real world.

% failure condition in the real world
In the real-world testing, it is important to determine whether the testing motion succeeded, while we don't install additional motion capture system. For each extreme motion, we determine the success of each motion as finishing the entire motion command sequence with no unexpected head contacting the ground. For getting-up-from-ground task, we terminate the test when the robot's torso orientation significantly deviate from the motion command. In our real-world experiment, we also visualize the motion command in the laptop that sends the motion command sequence.


%% Use plainnat to work nicely with natbib. 
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
