
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[small]{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{acronym}
\usetikzlibrary{positioning, shapes, fit, arrows.meta}
\usepackage{comment}
\usepackage[capitalise]{cleveref}
\usepackage{wrapfig}
\usepackage{float}

\newcommand{\U}{\boldsymbol{u}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newtheorem{definition}{Definition}
\iclrfinalcopy

\title{Microcanonical Langevin Ensembles: \\Advancing the Sampling of Bayesian \\Neural Networks}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Emanuel Sommer \\
Department of Statistics, LMU Munich \\
Munich Center for Machine Learning (MCML) \\
Munich, Germany \\
\texttt{emanuel@stat.uni-muenchen.de} \\
\And
Jakob Robnik \\
Physics Department \\
University of California \\
Berkeley, USA \\
\texttt{jakob\_robnik@berkeley.edu} 
\AND
Giorgi Nozadze \\
Department of Statistics, LMU Munich \\
Eraneos Analytics Germany GmbH\\
Munich, Germany \\
\texttt{giorgi.nozadze@eraneos.com} 
\And
UroÅ¡ Seljak \\
Physics Department, University of California \\
Lawrence Berkeley National Laboratory \\
Berkeley, USA \\
\texttt{useljak@berkeley.edu} 
\AND
David R\"ugamer \\
Department of Statistics, LMU Munich \\
Munich Center for Machine Learning (MCML) \\
Munich, Germany \\
\texttt{david@stat.uni-muenchen.de} 
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% Let's use acronyms basically for the method DEI-MCMC with NUTS and with MCLMC, so we can still
% discuss the acronym we want to choose and change it quickly below. Some suggestions:
\newcommand{\nuts}{BDE}  % (NUTS-based) Bayesian Deep Ensemble - NBDE?
% Alternatives: \nuts, NUTS-based DEI-MCMC
\newcommand{\mclmc}{MILE}  % MIcrocanonical Langevin Ensemble, which sounds nice but does not include the DE reference.
% Alternatives: MODEL MC, \mclmc, MCLMC-based DEI-MCMC


%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
Despite recent advances, sampling-based inference for Bayesian Neural Networks (BNNs) remains a significant challenge in probabilistic deep learning. While sampling-based approaches do not require a variational distribution assumption, current state-of-the-art samplers still struggle to navigate the complex and highly multimodal posteriors of BNNs. As a consequence, sampling still requires considerably longer inference times than non-Bayesian methods even for small neural networks, despite recent advances in making software implementations more efficient. Besides the difficulty of finding high-probability regions, the time until samplers provide sufficient exploration of these areas remains unpredictable. To tackle these challenges, we introduce an ensembling approach that leverages strategies from optimization and a recently proposed sampler called Microcanonical Langevin Monte Carlo (MCLMC) for efficient, robust and predictable sampling performance. Compared to approaches based on the state-of-the-art No-U-Turn Sampler, our approach delivers substantial speedups up to an order of magnitude, while maintaining or improving predictive performance and uncertainty quantification across diverse tasks and data modalities. The suggested Microcanonical Langevin Ensembles and modifications to MCLMC additionally enhance the method's predictability in resource requirements, facilitating easier parallelization. All in all, the proposed method offers a promising direction for practical, scalable inference for BNNs.
\end{abstract}

\section{Introduction and related literature}

Sampling-based inference for Bayesian Neural Networks (BNNs) has garnered significant interest as a principled approach to addressing the analytically intractable challenge of probabilistic deep learning \citep{izmailov_2021_WhatArea, wiese2023towards, pmlr-v235-papamarkou24b}. New methods, such as subspace inference \citep{izmailov_2020_SubspaceInferencea, dold2024}, are being explored alongside emerging applications in diverse domains where effective uncertainty quantification is crucial, including healthcare \citep{penghealthcare} and physics \citep{cranmer2021}.\citet{papamarkou_2022_ChallengesMarkova,sommer2024connecting} highlight several shortcomings in current sampling-based approaches, particularly the need for proper initialization of sampling procedures and the challenge of capturing multimodality. 

\paragraph{Samplers and problem setup}


% pocoMC 
% MEADS \citep{homan_tuning-free_nodate},
% MALT \citep{riou-durand_2023_AdaptiveTuning}
% SNAPER \citep{sountsov_focusing_2022},
% ChESS-HMC \citep{homan_adaptive_nodate}

Hamiltonian Monte Carlo \citep[HMC;][]{duane1987hybrid} and Underdamped Langevin Monte Carlo \citep{leimkuhler_metropolis_2009_copy} are the gold standard algorithms for high-dimensional sampling problems. 
However, their performance is known to be sensitive to their hyperparameters, such as the step size, preconditioning, and the momentum decoherence rate \citep{neal_2011_MCMCUsing}. They are therefore combined with an automatic hyperparameter adaptation algorithm.
Several ensemble-based schemes have been developed in recent years
\citep{sountsov_focusing_2022_copy, hoffman2022tuning, riou-durand_2023_AdaptiveTuning}, but all of these schemes critically depend on the ensemble variance of the parameters to tune the critical momentum decoherence rate hyperparameter and in some cases also the (preconditioned) step size. As such, they are not applicable to the highly multimodal posteriors of BNNs.

In the context of gradient-free sampling, a variety of ensembled algorithms have been developed, for example, 
Preconditioned Monte Carlo \citep[pocoMC;][]{karamanis_pocomc_2022_copy,karamanis_accelerating_2022_copy} ,
Nested Sampling \citep{NestedSampling} and
Elliptical Slice Sampling \citep[]{pmlr-v9-murray10a}.
These algorithms address the multimodality problem; however, they scale poorly to high-dimensional settings. For example, Eliptical Slice Sampling is frequently used for BNNs \citep{izmailov_2020_SubspaceInferencea, dold2024} for sampling in a small subspace (e.g., 2- or 3-dimensional) of the parameter space, but cannot be scaled to much larger dimensions. 
%
While alternative methods for multimodal sampling exist, they are also not well-suited for the high-dimensional and highly multimodal BNN setting. For instance, stochastic localization methods, like those proposed by \citet{pmlr-v235-grenioux24a}, can handle moderate multimodality but are sensitive to hyperparameters (like the integration initialization $t_0$ for stochastic localization) and lack scalability. Similarly, the Liouville Flow Importance Sampler \citep{pmlr-v235-tian24c} offers unbiased sampling but requires the training of a neural flow model, which needs to become increasingly complex as the dimensionality and complexity of the problem grows, limiting its scalability. The same applies to the learned vector field in the path-guided particle-based method of \citet{pmlr-v235-fan24f}, with an empirical comparison provided in Appendix \ref{app:furthercomps}.

\paragraph{HMC and NUTS} It is therefore not surprising that a sequential HMC variant, the No U-turn Sampler \citep[NUTS;][]{hoffman_2014_NoUTurnSampler}, is still viewed as ``[t]he only MCMC algorithm that theoretically scales to high dimensions across a broad class of models'' \citep{futurebayescompute2024}. 
While the scalability of HMC allows the application to the full parameter space of moderately-sized neural networks and its NUTS variant provides an (almost) tuning-free approach of HMC, it cannot sufficiently explore the many modes present in BNN posteriors\citep{wiese2023towards}. The recent HMC variant Symmetric Split HMC \citep{cobb_2021_ScalingHamiltonian} improves HMC's memory scalability but comes with other drawbacks, including sensitive hyperparameters. We provide an empirical comparison and further discussion in Appendix \ref{app:furthercomps}. Standard priors, like isotropic Gaussians, can further lead to initialization issues, where samplers start in low-probability regions, resulting in slow convergence or samplers getting stuck. 
%
To address these issues, \citet{sommer2024connecting} propose a Bayesian Deep Ensemble (\nuts), an ensemble of many short warmstarted Markov chain Monte Carlo (MCMC) chains. With this, the authors increase the exploration capability and are able to achieve state-of-the-art predictive and uncertainty quantification (UQ) performance on common benchmark tasks. Although much faster than previously employed methods by leveraging parallelization and efficient implementations, \citet{sommer2024connecting} rely on NUTS. This imposes a significant computational burden and results in a method where the sampling phase still dominates the computational costs by far. %These concerns still seem to hinder the widespread adoption of sampling-based methods in general \citep{papamarkou_2022_ChallengesMarkova}.

\begin{comment}
{\color{blue}
Jakob: references that I mentioned:
pocoMC \citep{karamanis_pocomc_2022_copy,karamanis_accelerating_2022_copy}
MEADS \citep{homan_tuning-free_nodate},
MALT \citep{riou-durand_2023_AdaptiveTuning}
SNAPER \citep{sountsov_focusing_2022},
ChESS-HMC \citep{homan_adaptive_nodate}
}
\end{comment}

% \paragraph{Our contributions} 
% In an effort to improve the scaling and efficiency of sampling-based inference for BNNs, we identified Microcanonical Langevin Monte Carlo  \citep[MCLMC;][]{robnik2023microcanonical} as a possible alternative MCMC-based solution. In recent experiments, the authors were able to show that MCLMC can be computationally superior to NUTS while providing the same quality of samples in downstream metrics. However, existing MCLMC literature mainly focuses on unimodal posteriors and optimizes MCLMC's tuning for classical sequential sampling scenarios \citep{robnik2023microcanonical, robnik2024fluctuation}. While promising, MCLMC is not directly applicable to sampling from BNN posteriors without further modifications. 
% In this paper, we 
% %contribute to the various recent and on-going efforts of the research community to make sampling-based inference feasible for generic Bayesian neural networks. To this end, 
% therefore propose adaptations of the MCLMC algorithm and show that it can be effectively combined with ensembling strategies. %integrated into the DEI-MCMC approach for BNN inference. 
% Our proposed method, an Microcanonical Langevin Ensemble (\mclmc), is practically tuning-free, and extensive experiments highlight its robustness, improved scalability, and speed. Compared to previous approaches, we achieve consistent speedups by an order of magnitude while maintaining competitive performance and even outperforming competitors in more complex problems. %The contributed algorithm is thus a huge step towards efficient and practically feasible sampling-based inference in BNNs and by far the most accessible sampling routine for practitioners.

\paragraph{Our contributions} 
In an effort to improve the scaling and efficiency of sampling-based inference for BNNs, we identified Microcanonical Langevin Monte Carlo  \citep[MCLMC;][]{robnik2023microcanonical} as a possible alternative MCMC-based solution. In recent experiments, the authors were able to show that MCLMC can be computationally superior to NUTS while providing the same quality of samples in downstream metrics. While MCLMC demonstrates computational advantages over NUTS in unimodal and sequential sampling tasks, it cannot address the multimodality, numerical instability, and scalability challenges of BNNs without significant modifications (e.g. see \cref{app:numerical_stability}). We embed adapted MCLMC as the key ingredient in our approach, including deep ensemble initialization for enhanced exploration, adjustments to ensure numerical stability in high-dimensional settings, and optimizations targeting critical bottlenecks. The resulting method, Microcanonical Langevin Ensemble (\mclmc), comes with automated tuning and works reliably out of the box. Extensive experiments highlight that \mclmc{} achieves state-of-the-art performance while being up to an order of magnitude faster than previous sampling-based approaches.

\section{Background}

%Adapting the notation of \citet{sommer2024connecting} we mostly focus on multi-layer perceptrons (MLPs) with some experiments extending to other architectures like CNNs \citep{fukushima1979neural, lecunCNN}.
In this work, we denote neural networks with $f: \mathcal{X} \to \mathcal{Y}$, $\mathcal{X} \subseteq \mathbb{R}^p, \mathcal{Y} \subseteq \mathbb{R}^m$. We parameterize the network with $\bm{\theta} \in \Theta \subseteq \mathbb{R}^d$, denoting the vector of all flattened and concatenated weights and biases. To express the epistemic uncertainty about 
$\bm{\theta}$, we treat $\bm{\theta}$ as a random variable with prior density $p(\bm{\theta})$ and denote its posterior density as
%\begin{equation} \label{eq:posterior}
    $p(\bm{\theta} | \mathcal{D}) = p(\mathcal{D} | \bm{\theta}) p(\bm{\theta}) / p(\mathcal{D})$,
%\end{equation}
with observed data $\mathcal{D} \in (\mathcal{X} \times \mathcal{Y})^n$. We will 
assume a standard isotropic unit variance Gaussian prior $\mathcal{N}(0,I)$ if not specified otherwise. A more detailed analysis of the prior influence is beyond the scope of this work.
The posterior predictive density (PPD) quantifies the uncertainty of predicting unseen labels $\bm{y}^\ast\in\mathcal{Y}$ given features $\bm{x}^\ast \in \mathcal{X}$  by integrating over the posterior distribution of the parameters $\bm{\theta}$:
\begin{equation} \label{eq:ppd}
    p(\bm{y}^\ast | \bm{x}^\ast, \mathcal{D}) = \int_\Theta p(\bm{y}^\ast | \bm{x}^\ast, \bm{\theta}) p(\bm{\theta} | \mathcal{D}) \diff\bm{\theta}.
\end{equation}

\subsection{Monte Carlo sampling} \label{sec:sampling}

In the setting of sampling-based inference, we estimate the analytically intractable integral in Eq.~(\ref{eq:ppd}) using samples from the posterior distribution with density $p(\bm{\theta} | \mathcal{D})$. These are obtained by Markov chain Monte Carlo (MCMC) methods, which construct a Markov chain whose stationary distribution is the posterior distribution or close to it. In practice, we gather these samples from $K$ independent chains, each with $S$ samples, yielding the set $\{\bm{\theta}^{(k,s)}| k \in [K], s \in [S]\}$ based on which we can perform prediction and uncertainty quantification %are asymptotically samples from $p(\bm{\theta} | \mathcal{D})$
\citep{andrieu_2003_IntroductionMCMC,gelman_2013_BayesianData}.
The approximation of \cref{eq:ppd} then has the form
\begin{equation} \label{eq:mc_integral}
    p(\bm{y}^\ast | \bm{x}^\ast, \mathcal{D}) \approx \frac{1}{K \cdot S} \sum_{k=1}^K \sum_{s = 1}^S p\left(\bm{y}^\ast | \bm{x}^\ast, \bm{\theta}^{(k,s)}\right).
\end{equation}
%
\paragraph{Error analysis and Metropolis-Hastings adjustment} The error of this approximation is composed of three terms: initialization error, discretization error, and Monte Carlo error.
The initialization error is a transient effect caused by the chains' ensemble distribution having not yet reached the stationary distribution (burn-in phase).
The Monte Carlo error is the variance caused by the finite number of samples $S$ and chains $K$. 
The discretization error is caused by the finite step size used to numerically simulate the sampler's dynamics. This causes  $p(\boldsymbol{\theta} \vert \mathcal{D})$ to no longer be the stationary distribution. The Metropolis-Hastings (MH) scheme is typically employed to completely eliminate the discretization error, but this comes at an expense of shorter step size.
This is because the acceptance rate depends exponentially on the squared energy error, which grows linearly with dimensionality. Therefore, the step size needs to be decreased as the number of parameters increases in order to maintain a fixed acceptance rate, causing the sampler to move more slowly in higher dimensions. 
The MH algorithm is further prone to degeneration of the acceptance rate if the chains are initialized from a distribution, which is not already very close to the stationary distribution, causing slow convergence and large initialization error \citep{durmus_asymptotic_2023_copy}.
%
Without the Metropolis adjustment on the other hand, the discretization error depends strongly on the step size. Hence, slightly reducing the step size causes the discretization error to become negligible compared to the initialization and Monte Carlo error. %This process can be controlled by monitoring the energy error variance per dimension (EEVPD), which is a direct measure of the discretization error \citep{robnik2023microcanonical}. 
%This approach makes the Metropolis unadjusted scheme much more efficent than the adjusted counterpart in high dimensions.
Given these arguments, we choose to omit the MH adjustment in our method, relying instead on careful initialization and step size tuning to manage the error terms.


%To address this \citet{sommer2024connecting}  proposed \nuts{s}, leveraging optimized deep ensembles for initialization.
%
%In DEI-MCMC, multiple neural networks are first optimized via standard methods, yielding diverse starting points that aim to capture different posterior modes. These pre-trained solutions serve as valid initializations for MCMC, ensuring the sampler starts in high-probability areas and avoids functionally irrelevant regions. This improves both predictive performance and uncertainty quantification (UQ) without constraining the prior choice for the sampling part.
%
%Furthermore, DEI-MCMC embraces a paradigm shift from relying on a single long MCMC chain to using multiple shorter chains that run concurrently. This concurrent approach enhances both computational efficiency and posterior exploration, making it highly effective for BNNs. We refer to the current state-of-the-art DEI-MCMC variant that employs NUTS as the sampler as Bayesian Deep Ensembles (\nuts{}), following the terminology used by \citet{sommer2024connecting}.
%


%The diagnostics and evaluation metrics employed in this work are detailed in Appendices \ref{sec:diagnostics} and \ref{sec:eval}.



\subsection{Microcanonical Langevin Monte Carlo} \label{sec:mclmc}

A recently proposed sampling method 
outside the BNN research field
is Microcanonical Langevin Monte Carlo (MCLMC).
The time evolution of the MCLMC sampler \citep{robnik2024fluctuation} is governed by the stochastic differential equation
\begin{align} \label{eq:SDE}
    \diff \boldsymbol{\theta} = \U \diff t,\qquad
    \diff \U = \big( 1 - \U \U^\top \big) \big( (d-1)^{-1} \nabla \log p(\boldsymbol{\theta} \vert \mathcal{D})  \diff t  + \eta  \diff \boldsymbol{W} \big),
\end{align}
where $\boldsymbol{W}$ is the Wiener process and $\eta$ is a free parameter equivalent to the typical length traveled in the parameter space before momentum decoherence, often denoted with $L$.
%We have here chosen the velocity to be normalized to $\U \cdot \U = d-1$ instead of $\U \cdot \U = 1$ as in \citep{robnik2024fluctuation}. In these way the step size can be more directly compared to the learning rate.
\citet{robnik2024fluctuation} use the drift-diffusion discretization to numerically solve \cref{eq:SDE}. The drift part (\cref{eq:SDE} without the last, stochastic term) is solved by the minimal norm integrator \citep{takaishi_testing_2006_copy,omelyan_generalised_2013_copy}.
In the limit of small step size $\epsilon$, the resulting Markov chain has $p(\boldsymbol{\theta} \vert \mathcal{D})$ as a stationary distribution \citep{robnik2024fluctuation}. 
Smaller step size results in a smaller discretization error but increases the Monte Carlo and initialization errors because the sampler is moving more slowly. We therefore wish to reduce the step size to the point where the discretization error becomes smaller than the other two sources of error, but not much further. \cite{robnik2023microcanonical} propose the energy error variance per dimension (EEVPD) as a measure of the discretization error and show that one can control the discretization error by controlling EEVPD. Analogously to adapting the step size to match some targeted acceptance rate in Metropolis-adjusted algorithms, one can adapt the step size to match some desired EEVPD in the unadjusted algorithms. This inexact but highly efficient approach with bias control is the one we will also take in our work.

Note that the stationary distribution is independent of the free parameter $\eta$ and in particular also holds for the deterministic dynamics, $\eta = 0$. This is not the case in HMC, where momentum resampling is essential to maintain the desired stationary distribution. Being more deterministic allows MCLMC to converge faster during the exploitation phase.

The parameter $\eta$ is still important, as it controls the rate of the momentum decoherence and forces the dynamics to move to the unexplored parts of the parameter space. \cite{robnik2023microcanonical} found that a good performance is achieved when $L/\eps$ is on the order of the chain's autocorrelation time. As the autocorrelation time is quite expensive to evaluate, they also propose to set $L$ to the size of the posterior modes, by computing the variance of the parameters. 

Based on these considerations, they propose a three-stage tuning scheme, which we now refer to as the three phases: 
\begin{enumerate} 
    \item Phase I: Adapt the step size to match the desired EEVPD and complete the burn-in. 
    \item Phase II: Estimate parameter variance to obtain an initial estimate for $L$. 
    \item Phase III: Estimate autocorrelation time to refine the estimate of $L$.
\end{enumerate}

\section{Microcanonical Langevin Ensembles}\label{sec:method}

In order to embed MCLMC in a robust sampling pipeline (cf.~\cref{fig:tuningflowchart}) that works effectively for BNNs and exploits the idea of ensembling, we will discuss the combination of optimization techniques and sampling as well as various aspects of required MCLMC modification in the following. Without these modifications, MCLMC struggles with exploration and exhibits high failure rates, particularly in high-dimensional settings (see \cref{app:numerical_stability}).
%how we can tackle the different approximation errors of the sampling procedure effectively leveraging properties of MCLMC and outline how adaptions of MCLMC can be beneficial when sampling from complex, multimodal posteriors in BNNs. An overview of the proposed \mclmc{} approach is given in Figure \ref{fig:tuningflowchart}. %We first highlight MCLMC's correction-less sampling approach, which enhances efficiency, and its deterministic gradient evaluations, which improve predictability in resource planning.
%
%The subsequent subsections focus on tuning phase adaptations that promote robust performance and scalability in higher dimensions. We also address computational budget allocation, diverging from traditional sequential MCMC methods by emphasizing sampling, memory, and inference time, thereby providing an practical yet effective sampling strategy for BNNs.
%In order to understand why MCLMC might be a viable alternative to NUTS and what adaptions are required to extend it to an ensemble approach, we first discuss two properties of MCLMC that we think are the key to successful applications in BNNs.


\begin{figure}[!t]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=\textwidth, trim=0.65cm 0.2cm 0.15cm 1.5cm]{img/mclmc_tuning.pdf}
\end{center}
\caption{Flowchart illustrating our proposed procedure for obtaining a Microcanonical Langevin Ensemble (\mclmc) for BNNs. The process involves three main stages: optimization, MCLMC warmup and tuning, and MCLMC sampling. These steps are parallelized to generate an ensemble of $K$ members. The number of MCLMC steps for each tuning phase and the final sampling phase are annotated, and carryovers between stages are highlighted in circles.}
\label{fig:tuningflowchart}
\end{figure}

\subsection{Ensembling for reduced initialization error}

As discussed in \cref{sec:sampling}, the error in the approximation of the predictive posterior can be decomposed into three parts. While the initialization error is described in \cref{sec:sampling} as a ``transient effect'', this error should not be mistakenly classified as unimportant, and treating it with special care is particularly important in the application of BNNs. Similar to most other samplers, it is not unlikely for MCLMC to get stuck in low-probability regions of the high-dimensional and highly complex posterior surfaces of BNNs. Our proposed solution is therefore to combine sampling with an optimization step (\cref{fig:tuningflowchart}, blue part). To prevent chains in the sampling step from being initialized in these unfavorable regions, we run $K$ optimization steps to obtain starting values $\boldsymbol{\theta}^{\ast(k)}, k\in[K]$ for each of the $K$ chains. As a byproduct, we obtain a deep ensemble \citep[DE;][]{lakshminarayanan_2017_SimpleScalablea} with $K$ members.\footnote{Note that the optimization of these $K$ models actually has negligible costs compared to the costs of current state-of-the-art sampling approaches.} 
% 
% \paragraph{Initialization error}
Similar to \citet{sommer2024connecting}, we found that using optimized neural networks as starting values %for sampling from multiple chains 
can reduce this error notably and prevents the sampler from getting stuck. %In other words, we use the members of a deep ensemble as starting values for the different MC chains. This corresponds to the first step of our proposed \mclmc{} approach (cf.~blue part of \cref{fig:tuningflowchart}). 

\subsection{Tuning phase adaptions}

In the previous subsection, we proposed to optimize starting values when using MCLMC by running an ensemble of MCLMC chains. Optimizing the starting values of each chain is, however, not sufficient to make MCLMC work numerically stable and efficiently for BNNs (see \cref{app:numerical_stability}). We therefore discuss three components of MCLMC's tuning phase requiring further adaptions in applications to BNNs, and propose scaling related modifications to phase III of MCLMC.

\paragraph{Step size} 

\citet{robnik2023microcanonical} suggest to %The official MCLMC implementation in BlackJAX \citep{cabezas2024blackjax} 
initialize the step size $\epsilon$ with $\sqrt{d}$. While this default works well for their applications, this would result in too large values even for rather small neural networks. Too large step sizes, in turn, will result in significant changes of the energy and thereby introduce serious numerical problems. % rendering most chains useless ($\bm{\theta} \rightarrow \infty$). 
As \mclmc{} initializes chains already in a region of high probability, it is reasonable to reduce $\epsilon$ notably. In practice, this means we can set the step size to the optimizer's learning rate used in the optimization step of \mclmc{}. Besides reducing the initial discretization error, this also %initialization is empirically slightly smaller than the tuned value of $\epsilon$, 
ensures a more localized exploration early in the burn-in phase and close to the optimized solution while allowing for larger steps as tuning progresses (which can also be confirmed empirically).
%We argue that in the setting of BNNs that commonly get trained with some variant of the ADAM optimizer \citep{kingma2014adam, loshchilov2018decoupled} it is advantageous to initialize the step size simply with the learning rate of a suitable optimizer for the problem. This value is readily available within the setting of DEI-MCMC as we also optimize before the sampling begins. 

\paragraph{Energy variance scheduler} 

Another tuning parameter in phase I of \mclmc{} is the energy variance scheduler allowing control of the trade-off between exploitation and exploration. Having already optimized $K$ starting values, \mclmc{} does not require an excessive amount of exploitation but each MCLMC ideally focuses more on exploitation. In contrast to the tuning scheme proposed in \citet{robnik2023microcanonical}, we do not employ a fixed desired energy variance level but use a linear scheduler starting with a higher desired energy variance and gradually decreasing it. By doing this we foster noisier exploration at the beginning of the warmup phase and increase the exploitation towards the end of it. Doing this in parallel across an ensemble of chains, our idea resembles the popular strategy of using cyclical learning rates in SG-MCMC sampling \citep{zhang_2020_CyclicalStochastic}. But rather than going through various exploration and exploitation phases we do one exploration-exploitation cycle per chain and thus parallelize this idea.\footnote{We empirically found that the linear schedule from 0.5 to 0.1 worked well in a variety of settings. We also explored exponentially decaying schedulers starting with high desired energy variance, but these proved to be empirically inferior to the aforementioned linear schedule. This is in line with the intuition that excessive exploration is not necessary due to the DE initialization.} 

\paragraph{Effective sample size} 

MCLMC further requires setting a desired effective sample size (ESS) level within the EEVPD estimation phase. While this level is generally a choice of the practitioner and the available computational resources, one option is to set the ESS to 10\% of the total number of posterior samples. As previous approaches working with NUTS also report an ESS of around 10\% of the total number of samples, this value ensures a sample quality that is as good as the one of NUTS.

% Very shortly argue why trust in estimate should be held constant

\paragraph{Phase III bottleneck}

When scaling the MCLMC tuning algorithm to higher dimensions, a computational bottleneck of phase III is to estimate the empirical ESS, which is done with a Fast Fourier Transform \citep[FFT,][]{bracewell1986fourier}. The FFT is computed for each parameter across all samples. Although fast implementations scale with $\mathcal{O}(S \log S)$, this can severely impact the runtime for increased dimensions and amount of samples. We therefore propose to subsample parameters if the dimensionality is $d > 2000$, which linearly decreases the runtime. We also suggest applying thinning to the samples such that the number of samples used for the FFT is bounded by $10e3$. %These two changes allow to apply the MCLMC tuning to much higher dimensional problems and empirically perform well across the board.

\subsection{Effective computational budget allocation}\label{sec:budget}

The third error caused by the finite number of samples and chains is the Monte Carlo error. Unlike NUTS, which varies the number of leapfrog steps for each proposal and requires one gradient evaluation per leapfrog step, MCLMC employs a deterministic approach that requires two gradient evaluations per sample due to the use of a minimal norm integrator. This structure makes MCLMC's computational requirements much more predictable, offering a significant practical advantage. In contrast, the number of steps taken by NUTS can exhibit substantial variance both within and across tasks, leading practitioners to set an upper limit on leapfrog steps. %We will address this point in detail in \cref{sec:budget}.
%
%Having addressed a way to reduce sampling errors and adapting tuning parameters when using MCLMC for BNNs, the remaining step is to construct a memory- and inference-efficient pipeline with optimal allocation of computational resources in the combined process. 
%
With better foreseeable computational requirements, we can optimize the balance between the number of chains and the number of samples in \mclmc{} in order to minimize the Monte Carlo error for a given computational budget.
As recent literature suggests that MCLMC can be much more efficient than NUTS in obtaining effective samples in simple problems, we propose to collect a relatively small number of samples from each chain and thereby limit computational resources spent on exploring individual local modes by MCLMC but relying on DE for exploration.

\paragraph{Warmup stage} %However, since we are not sampling directly from the DE solutions, we must account for burn-in steps to reach the typical set, which does not always coincide with the modes found during DE optimization \citep{betancourt_2018_ConceptualIntroductiona}.
It is not advisable to directly start chains from the DE optimized points, as these modes do not coincide with the so-called typical sets \citep{betancourt_2018_ConceptualIntroductiona}, we suggest %100 warmup steps for NUTS work well in this respect, we aim to allocate a similar number of gradient-guided steps for MCLMC. In the first tuning phase of MCLMC corresponding to NUTS' warmup phase, we therefore 
allocating 40k steps, i.e., 80k gradient evaluations for \mclmc{'s} warmup phase. Compared to NUTS, which often requires more than 90k leapfrog steps/gradient evaluations for warmup in BNN tasks, this is a conservative lower bound. This is followed by the two shorter phases, II and III, where we allocate 5k steps each to ensure robust estimations of the momentum decoherence scale $L$.

\paragraph{Sampling stage} As MCLMC samples exhibit significant autocorrelation, we can further reduce memory costs by applying thinning without notable reduction in sample quality while also not increasing inference time as long as the costs of thinning remain negligible compared to the overall sampling time. In our setup, after 50k warmup steps, we propose using 10k steps for the sampling phase. The choice of thinning interval is then based on the memory and inference time constraints, i.e., the posterior sample budget.

In this work, we explore two scenarios: one with a budget of 1k samples per chain and another with 100 samples, corresponding to thinning intervals of 10 and 100, respectively. Overall, we propose a fixed budget of 60k steps per chain, providing a predictable sampling process. By adjusting thinning appropriately, one can further effectively manage memory and inference time requirements.

% \subsection{ADAM based preconditioning}

% Will it work? To be adapted/analyzed. Currently it does not work :/

\section{Experiments}\label{sec:exps}

In this section, we evaluate the feasibility of applying \mclmc{} to sampling-based inference for BNNs.
\begin{itemize}
    \item \textbf{Datasets and models}: We replicate the benchmark by \citet{sommer2024connecting} but also extend it to other datasets (Ionosphere, Income, IMDB, MNIST, F-MNIST) and models (convolutional and attention-based neural networks). 
    \item \textbf{Methods}: As in \citet{sommer2024connecting}, we investigate the improvement of our proposed approach over a DE, but also compare against the current state-of-the-art \nuts{} approach based on the NUTS sampler.
    \item \textbf{Runtime comparisons}: Following this, we conduct a series of ablation studies, carefully examining how \mclmc{} scales compared to \nuts{}. 
    \item \textbf{Tuning and hyperparameters}: We finally validate the robustness of MILE's hyperparameters, supporting our claim that MILE is an auto-tuned off-the-shelf procedure like NUTS.
\end{itemize}

  e experimental setting and the implementation can be found in Appendix \ref{app:experimentsetting}. The diagnostics and evaluation metrics employed are detailed in Appendices \ref{sec:diagnostics} and \ref{sec:eval}.

\subsection{Benchmarks}

The goal of this section is to demonstrate 1) the feasibility of \mclmc{} in BNN settings, 2) highlight its superior predictive performance measured using the root mean squared error (RMSE) or Accuracy, its improved UQ measured using the LPPD, and 3) its improvements in runtime.

\subsubsection{UCI benchmarks}

Using the six UCI datasets previously analyzed in the context of sampling-based inference in \citet{sommer2024connecting} we replicate their benchmark using a ReLU network with two hidden layers and 16 neurons each (see \cref{app:experimentsetting} for further details). We compare the predictive accuracy, UQ, and runtime of state-of-the-art \nuts{} and the DE baselines against \mclmc{}, configured as described in \cref{sec:method}. 

\renewcommand{\arraystretch}{1.05}
\begin{table}[t!]
\small
\caption{Average hold-out LPPD and RMSE performance as well as wallclock runtime of the DE baseline, \nuts{}s and \mclmc{}, respectively, for the six datasets (in different rows) over 3 data splits. A table including standard deviations can be found in \cref{app:furtherresults}. The wallclock times of the samplers represent the additional sampling time on top of the DE fit which is also reported.}
\label{tab:uci_repl_main}
\begin{center}
\resizebox{0.96\columnwidth}{!}{%
\begin{tabular}{l|ccc|ccc|ccc}
\hline\hline
\multicolumn{1}{c|}{\bf } &
\multicolumn{3}{c|}{\bf LPPD ($\uparrow$)} & 
\multicolumn{3}{c|}{\bf RMSE ($\downarrow$)} & 
\multicolumn{3}{c}{\bf Time (min)} \\
\multicolumn{1}{c|}{} &
\multicolumn{1}{c}{\bf \phantom{--}DE} & \multicolumn{1}{c}{\bf \phantom{--}\nuts{}} & \multicolumn{1}{c|}{\bf \phantom{---}\mclmc{}} &
\multicolumn{1}{c}{\bf \phantom{--}DE} & \multicolumn{1}{c}{\bf \phantom{--}\nuts{}} & \multicolumn{1}{c|}{\bf \phantom{---}\mclmc{}} &
\multicolumn{1}{c}{\bf \phantom{--}DE} & \multicolumn{1}{c}{\bf \phantom{--}\nuts{}} & \multicolumn{1}{c}{\bf \phantom{---}\mclmc{}} \\
\hline \\[-2ex]
A & $\phantom{-}0.024$ & $\phantom{-}0.558$ & $\phantom{-}\boldsymbol{0.612}$ & $\phantom{-}0.309$ & $\phantom{-}\boldsymbol{0.214}$ & $\phantom{-}\boldsymbol{0.206}$ & $\phantom{-}0.62$ & $\phantom{-}2.25$ & $\phantom{-}\boldsymbol{0.84}$\\
B & $\phantom{-}0.390$ & $\phantom{-}0.625$ & $\phantom{-}\boldsymbol{0.645}$ & $\phantom{-}0.251$ & $\phantom{-}\boldsymbol{0.242}$ & $\phantom{-}\boldsymbol{0.236}$ & $\phantom{-}5.67$ & $\phantom{-}48.29$ & $\phantom{-}\boldsymbol{5.40}$\\
C & $-0.072$ & $\phantom{-}\boldsymbol{0.301}$ & $\phantom{-}\boldsymbol{0.336}$ & $\phantom{-}0.304$ & $\phantom{-}\boldsymbol{0.273}$ & $\phantom{-}\boldsymbol{0.250}$ & $\phantom{-}0.33$ & $\phantom{-}1.56$ & $\phantom{-}\boldsymbol{0.77}$\\
E & $\phantom{-}1.227$ & $\phantom{-}2.072$ & $\phantom{-}\boldsymbol{2.300}$ & $\phantom{-}0.120$ & $\phantom{-}0.045$ & $\phantom{-}\boldsymbol{0.034}$ & $\phantom{-}0.39$ & $\phantom{-}1.11$ & $\phantom{-}\boldsymbol{0.75}$\\
P & $-1.024$ & $\boldsymbol{-0.760}$ & $\boldsymbol{-0.750}$ & $\phantom{-}0.742$ & $\phantom{-}\boldsymbol{0.703}$ & $\phantom{-}\boldsymbol{0.702}$ & $\phantom{-}12.37$ & $\phantom{-}152.85$ & $\phantom{-}\boldsymbol{19.50}$\\
Y & $\phantom{-}1.623$ & $\phantom{-}\boldsymbol{2.674}$ & $\phantom{-}\boldsymbol{2.859}$ & $\phantom{-}0.081$ & $\phantom{-}0.083$ & $\phantom{-}\boldsymbol{0.033}$ & $\phantom{-}0.16$ & $\phantom{-}\boldsymbol{0.58}$ & $\phantom{-}0.64$\\
\hline\hline
\end{tabular}
} % End resizebox
\end{center}
\end{table}
\renewcommand{\arraystretch}{1.0}

\paragraph{Performance results} Table \ref{tab:uci_repl_main} shows the results, indicating that \mclmc{} consistently matches or outperforms the other methods in predictive performance and UQ while significantly reducing the computational cost. This is especially evident in larger datasets like \texttt{bikesharing} (B) and \texttt{protein} (P), where \mclmc{} achieves sampling speeds nearly ten times faster than \nuts{}. In most cases, the additional MCLMC sampling after DE optimization is as fast as the DE fitting phase itself. It is noteworthy that this is a big step for sampling-based inference, yielding a time complexity comparable to DE ($\approx2\times$runtime), while providing better and more principled uncertainty measures. 

\paragraph{Resource predictability} 
\begin{wrapfigure}[18]{r}[0pt]{0.58\textwidth}
\includegraphics[width=\linewidth]{img/grad_evals_nuts.pdf}
\caption{Average gradient evaluations per chain for 1000 posterior samples for the experiments reported in Table \ref{tab:uci_repl_main}.}
\label{fig:grad_evals}
\end{wrapfigure}
As elaborated before, \mclmc{'s} efficiency is driven by its much fewer gradient evaluations and predictable runtime. 
Unlike \nuts{}, where runtime is highly variable in the leapfrog steps taken by NUTS, \mclmc{'s} fixed number of steps allows easy runtime forecasts once the cost of a single gradient evaluation is known. This is especially crucial for expensive sampling task such as the posterior sampling of BNNs. 
\cref{fig:grad_evals} illustrates the significant variability in \nuts{'s} computational cost for generating 1000 posterior samples, both within and across tasks. This variability often exceeds the entire budget for sampling in \mclmc{}. Moreover, in parallel settings, the \nuts{'s} performance is bottlenecked by the most expensive chain, while \mclmc{'s} deterministic nature ensures perfect load balancing for concurrent sampling (see Section \ref{sec:budget}).

\paragraph{Diagnostics} For diagnostic purposes and to evaluate the quality of samples drawn by \mclmc{} and \nuts{}, we evaluate the effective sample size \citep[ESS, ][]{vetharirhat2021} and the chainwise 4-split $\widehat{cR}$ \citep{sommer2024connecting}. As can be seen in Figures \ref{fig:ess_uci} and \ref{fig:crhat_uci}, \mclmc{} also outperforms \nuts{} in the sampling and local mixing quality, revealing higher average ESS and smaller $\widehat{cR}$ values. 





% Table with original DE values for warmstart
% \begin{table}[t]
% \caption{Average LPPD and RMSE hold-out as well as wallclock performance and standard error of the DE baseline and DEI-MCMC with NUTS and the MCLMC sampler respectively for the six datasets used in \citet{sommer2024connecting} over 3 data splits. The wallclock times of the samplers represent the additional sampling time ontop of the DE fit which is also reported.}
% \label{tab:uci_repl}
% \begin{center}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{l|ccc|ccc|ccc}
% \multicolumn{1}{c|}{\bf } &
% \multicolumn{3}{c|}{\bf LPPD ($\uparrow$)} & 
% \multicolumn{3}{c|}{\bf RMSE ($\downarrow$)} & 
% \multicolumn{3}{c}{\bf Time (min)} \\
% \multicolumn{1}{c|}{} &
% \multicolumn{1}{c}{\bf DE} & \multicolumn{1}{c}{\bf NUTS} & \multicolumn{1}{c|}{\bf MCLMC} &
% \multicolumn{1}{c}{\bf DE} & \multicolumn{1}{c}{\bf NUTS} & \multicolumn{1}{c|}{\bf MCLMC} &
% \multicolumn{1}{c}{\bf DE ($\forall$Methods)} & \multicolumn{1}{c}{\bf NUTS} & \multicolumn{1}{c}{\bf MCLMC} \\
% \hline \\
% A & $-0.448 \pm 0.055$ & $0.558 \pm 0.034$ & $\boldsymbol{0.612 \pm 0.011}$ & $0.507 \pm 0.022$ & $\boldsymbol{0.214 \pm 0.013}$ & $\boldsymbol{0.206 \pm 0.018}$ & $0.67 \pm 0.09$ & $2.25 \pm 0.01$ & $\boldsymbol{0.84 \pm 0.06}$\\
% B & $0.3427 \pm 0.027$  & $0.625 \pm 0.004$ & $\boldsymbol{0.645 \pm 0.10}$ & $0.327 \pm 0.024$ & $\boldsymbol{0.242 \pm 0.005}$ & $\boldsymbol{0.236 \pm 0.004}$ & $5.79 \pm 0.27$ & $48.29 \pm 0.51$ & $\boldsymbol{5.40 \pm 0.02}$\\
% C & $-0.228 \pm 0.049$ & $\boldsymbol{0.301 \pm 0.088}$ & $\boldsymbol{0.336 \pm 0.054}$ & $0.372 \pm 0.014$ & $\boldsymbol{0.273 \pm 0.012}$ & $\boldsymbol{0.250 \pm 0.014}$ & $0.34 \pm 0.08$ & $1.56 \pm 0.05$ & $\boldsymbol{0.77 \pm 0.06}$\\
% E & $0.749 \pm 0.078$ & $2.072 \pm 0.036$ & $\boldsymbol{2.300 \pm 0.066}$ & $0.247 \pm 0.013$ & $0.045 \pm 0.003$ & $\boldsymbol{0.034 \pm 0.007}$ & $0.30 \pm 0.04$ & $1.11 \pm 0.00$ & $\boldsymbol{0.75 \pm 0.03}$\\
% P & $-1.111 \pm 0.021$ & $\boldsymbol{-0.760 \pm 0.010}$ & $\boldsymbol{-0.750 \pm 0.018}$ & $0.753 \pm 0.003$ & $\boldsymbol{0.703 \pm 0.003}$ & $\boldsymbol{0.702 \pm 0.008}$ & $14.55 \pm 1.22$ & $152.85 \pm 7.80$ & $\boldsymbol{19.50 \pm 0.01}$\\
% Y & $0.721 \pm 0.263$ & $\boldsymbol{2.674 \pm 0.216}$ & $\boldsymbol{2.859 \pm 0.199}$ & $0.472 \pm 0.072$ & $0.083 \pm 0.011$ & $\boldsymbol{0.033 \pm 0.011}$ & $0.19 \pm 0.07$ & $\boldsymbol{0.58 \pm 0.01}$ & $0.64 \pm 0.02$\\
% \end{tabular}
% } % End resizebox
% \end{center}
% \end{table}


\subsubsection{Extended benchmarks}

In addition to existing sampling-based inference benchmarks, we extend our analysis to more complex and larger models, showcasing the successful application of \mclmc{} to convolutional neural networks (CNNs), classification tasks, and sequential models across different data modalities (see \cref{app:experimentsetting} for further details). 
% To the best of our knowledge, this is \textbf{the first demonstration of the feasibility of sampling-based inference of BNNs for these tasks}. 
The main reason we are able to advance existing benchmarks is the feasibility of sampling such tasks using \mclmc{}. As it would have taken weeks to obtain the inference using NUTS, we compare \mclmc{} mainly to the DE baseline but provide results for NUTS for smaller models in the Appendix. Additionally, we report the average performance of a single DE member and individual MCMC chain, which highlights the additional benefits of ensembling.

\paragraph{Results} The results, presented in Table \ref{tab:classificationtasks}, not only confirm the accuracy and UQ improvements over the DE baseline but also demonstrate performance gains in much larger models and new problem domains, confirming the positive effect of the additional exploration step taken by \mclmc{}.

%For the attention-based sequential model, sampling on top of the DE provides the least or no significant gain in performance in terms of LPPD, but consistently beats the DE in accuracy. We believe one major reason for this is that its parameter count is just roughly 0.04\% of the base variant of BERT \citep{devlin-etal-2019-bert} - a state-of-the-art model for the IMDB classification task - and as such the estimation itself is much noisier than for example for the presented CNNs where the MCLMC sampling provides a consistent and significant upflift in both accuracy and LPPD in all considered cases.

%Furthermore, we report the average performance of a single DE member and individual MCMC chain, which highlights the additional benefits of ensembling. The results consistently show that the DEI-MCMC approach achieves both better predictive performance and improved UQ compared to individual chains or ensemble members, reinforcing the value of this method for larger, more complex tasks.

\renewcommand{\arraystretch}{1.05}
\begin{table}[ht]
\tiny
\centering
\caption{Hold-out test performance of \mclmc{} and baselines on various classification tasks using fully-connected (FCN), convolutional (CNN) and attention-based (ATT) networks. ATT (v2, v3) employ pretrained embeddings.}
\label{tab:classificationtasks}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|l|c|cc|cc|cc|cc}
\hline\hline
\multicolumn{1}{c|}{\bf Dataset} & 
\multicolumn{1}{c|}{\bf Model} & 
\multicolumn{1}{c|}{\bf \# Params} & 
\multicolumn{4}{c|}{\bf Accuracy ($\uparrow$)} & 
\multicolumn{4}{c}{\bf LPPD ($\uparrow$)} %& 
%\multicolumn{2}{c}{\bf Tuned Param.} 
\\
\multicolumn{1}{c|}{} & 
\multicolumn{1}{c|}{} & 
\multicolumn{1}{c|}{} & 
\multicolumn{2}{c|}{\bf Avg. Single} & 
\multicolumn{2}{c|}{\bf Ensemble} & 
\multicolumn{2}{c|}{\bf Avg. Single} & 
\multicolumn{2}{c}{\bf Ensemble} %& 
%\multicolumn{1}{c}{\bf Step Size} & 
%\multicolumn{1}{c}{\bf $L$} 
\\ 
\multicolumn{1}{c|}{} & 
\multicolumn{1}{c|}{} & 
\multicolumn{1}{c|}{} & 
\multicolumn{1}{c}{\bf DNN} & \multicolumn{1}{c|}{\bf Chain} & 
\multicolumn{1}{c}{\bf DE} & \multicolumn{1}{c|}{\bf \mclmc{}} & 
\multicolumn{1}{c}{\bf DNN} & \multicolumn{1}{c|}{\bf Chain} & 
\multicolumn{1}{c}{\bf DE} & \multicolumn{1}{c}{\bf \mclmc{}} %& 
%\multicolumn{1}{c}{\bf } & 
%\multicolumn{1}{c}{\bf } 
\\ 
\hline
Ionosphere       & FCN (v1)     &  850 & 0.930 & \textbf{0.958} & \textbf{0.958} & \textbf{0.958} & -0.404 & -0.168 & -0.309 & \textbf{-0.167}\\ % & 1.263 & 36.85\\ 
Income       & FCN (v2)     &  2386 & 0.843 & \textbf{0.851} & 0.846 & \textbf{0.851 }& -0.334 & -0.315 & -0.318 & \textbf{-0.313} \\ %& 0.375 & 64.78\\ 
% IMDB      & ATT     &  44608 & 0.702 & 0.680 & 0.704 & \textbf{0.709} & -0.577 & -0.589 & -0.573 & \textbf{-0.571}\\ % & 1.796 & 126.1\\ 
IMDB      & ATT (v1)     & 68448  & 0.715 & 0.714 & \textbf{0.718} & 0.717 & -0.550 & -0.546 & \textbf{-0.544} & \textbf{-0.544}\\
IMDB      & ATT (v2)     &  55778 & 0.779 & 0.754 & 0.781 & \textbf{0.782} & -0.591 & -0.583 & -0.562 & \textbf{-0.481}\\
IMDB      & ATT (v3)     &  106190 & 0.786 & \textbf{0.790} & 0.786 & 0.788 & -0.509 & -0.493 & -0.507 & \textbf{-0.491}\\
MNIST       & CNN (v1)      & 7452  & 0.916  & 0.939 & 0.956 & \textbf{0.970} & -0.299 & -0.209 & -0.179 & \textbf{-0.129} \\ % & 0.025 & 13.39 \\ 
F-MNIST       & CNN (v1)      & 7452  & 0.742 & 0.767 & 0.863 & \textbf{0.885} & -0.725 & -0.684 & -0.486 & \textbf{-0.430} \\ % & 0.020 & 10.27\\  
F-MNIST       & CNN (v2)     & 61706  & 0.890 & 0.919 & 0.918 & \textbf{0.925} & -0.361 & -0.225 & -0.227 & \textbf{-0.216} \\ % & 0.254 & 28.74\\ 
\hline\hline
\end{tabular}
} % End resizebox
\end{table}
\renewcommand{\arraystretch}{1.0}

\subsection{Ablation model complexity \& runtime}

The UCI benchmarks have clearly showed the runtime advantage of \mclmc{} over \nuts. To further investigate the impact of model complexity (number of parameters) and dataset size on these runtimes, we conduct two ablation studies.

\paragraph{Scaling in model complexity} \cref{fig:complexity_ablation} shows the evolution of the required sampling time and performance metrics for increased model complexity. For the 5,426-dimensional case, \mclmc{} completes sampling in under 30 minutes, while \nuts{} takes several hours. More strikingly, \mclmc{} not only remains faster but its performance gap to \nuts{} increases with higher model complexity, indicating both significant efficiency gains and superior performance in higher dimensions. To quantify the time complexity, we fit a linearized power-law model to obtain the expected sampling time with growing $d$: $\mathbb{E}[\log(t_{\text{Sampler}})] = \beta_0 + \beta_1 \log (d)$. Both fits explain almost all the variance in the data with $R^2$ statistics of 0.98 for \mclmc{} and 0.96 for \nuts{}, confirming the robustness of the speed advantage. The model fits are illustrated as dashed lines in \cref{fig:complexity_ablation}.

\paragraph{Scaling in dataset size}
We analyze the impact of increasing dataset size.  \cref{fig:complexity_ablation} (top right) illustrates how sampling time evolves for a fixed task and neural network across different data subsets. Using the linear model $\mathbb{E}[t_{\text{Sampler}}] = \beta_0 + \beta_1 n^2$, we obtain fits that again almost perfectly resemble the given data points, with $R^2$ values of 0.99 for \mclmc{} and 0.96 for \nuts{}. The ratio of the $\beta_1$ coefficients, 6.6, again highlights \mclmc{'s} superior scaling in $n^2$. Notably, visual inspection suggests that \nuts{} may scale worse than quadratically, but for consistency with \mclmc{}, we apply a conservative quadratic fit. The scaling behavior again closely resembles a power-law and the observed quadratic trend in $n$ likely stems from memory-related hardware slowdowns as dataset sizes increase.



\begin{figure}[h]
\begin{center}
%\framebox[4.0in]{$\;$}
\includegraphics[width=0.9\textwidth, trim=0cm 0cm 1cm 3.5cm]{img/combined_scaling_plot_de.pdf}
\end{center}
\caption{Average sampling wallclock times (minutes, y-axis) of \nuts{} (blue) and \mclmc{} (orange) for the \texttt{bikesharing} dataset across 4 NN architectures with increasing parameter count (x-axis) on the upper left. Average sampling wallclock times (hours, y-axis) for the \texttt{protein} dataset across varying training data sizes (x-axis) on the upper right. Dashed lines indicate power-law and quadratic model fits respectively. 
In both cases the sampling time ratio between \nuts{} and \mclmc{} is around 7-9, independently of the number of parameters and observations. This is a result of NUTS always being close to its maximum number of iterations per sample, which we set to the default value of 1024 gradient calls. It therefore uses around $1024 \times (1000 + 100) \approx 11 \times 10^5$ gradient calls, as displayed also in Figure \ref{fig:grad_evals}. \mclmc{} on the other hand always uses $2 \times 60000 = 12 \times 10^4$ calls, which gives a ratio of $9.2$. 
The bottom row shows hold-out metric performances across 4 network architectures. DE performance for the LPPD and RMSE metrics is indicated as a grey reference. All charts come with standard errors over 3 data splits.
}
\label{fig:complexity_ablation}
\end{figure}

% \begin{figure}[h]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% \includegraphics[width=0.99\textwidth]{img/hidden_structure_de.pdf}
% \end{center}
% \caption{Average sampling wallclock times (absolute and relative to optimization time of a DE) and hold-out metric performances and standard errors of \nuts{} (black) and \mclmc{} (blue), respectively, for the \texttt{bikesharing} dataset over 4 network architectures with increasing complexity (x-axes) across 3 data splits. For the LPPD and RMSE metrics, the DE performance is given as a reference in grey.}
% \label{fig:complexity_ablation}
% \end{figure}

% \begin{figure}[h]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% \includegraphics[width=0.6\textwidth]{img/sampling_times_de.pdf}
% \label{fig:sampling_time_params}
% \end{center}
% \caption{Average sampling wallclock times (minutes) and standard errors of the DEI-MCMC method with NUTS (black) and the MCLMC (blue) sampler respectively (y-axis) for the \texttt{bikesharing} dataset over 4 NN architectures with increasing number of parameters (x-axis, log-scale) across 3 data splits. Linear model fits to the $\log_{10}$ transformed number of parameters are given as dashed lines.}
% \end{figure}

% \begin{figure}[h]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% \includegraphics[width=0.6\textwidth]{img/datasize_nuts.pdf}
% \label{fig:sampling_time_instances}
% \end{center}
% \caption{Average sampling wallclock times (minutes) and standard errors of the DEI-MCMC method with NUTS (black) and the MCLMC (blue) sampler respectively (y-axis) for the \texttt{protein} dataset over subsets of the training dataset of different sizes across 3 data splits. Quadratic model fits to the quadratically transformed number of training instances are given as dashed lines. }
% \end{figure}

% \begin{figure}[h]
%     \centering
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{img/sampling_times_de.pdf}
%         \caption{Average sampling wallclock times (minutes, y-axis) for the \texttt{bikesharing} dataset over 4 NN architectures with increasing parameter count ($\log_{10}$ transformed, x-axis).}
%         \label{fig:sampling_time_params}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{img/datasize_nuts.pdf}
%         \caption{Average sampling wallclock times (hours, y-axis) for the \texttt{protein} dataset across subsets of the training data of different sizes (x-axis). 
%         % Notably an exponential and polynomial fit of degree 4 would have fitted the NUTS times even better but we chose the conservative quadratic choice.
%         }
%         \label{fig:sampling_time_instances}
%     \end{subfigure}
%     \caption{Comparison of the average sampling wallclock times and standard errors of the DEI-MCMC method with NUTS (black) and the MCLMC (blue) sampler for 3 data splits. Linear (left) and quadratic (right) model fits are added as dashed lines for the respective dataset.}
%     \label{fig:sampling_time_combined}
% \end{figure}

\subsection{Ablation hyperparameter robustness}

To substantiate the claim that our proposed MCLMC configuration in \mclmc{} is, in fact, a robust and auto-tuned off-the-shelf method, we conducted a series of ablation studies by changing the default hyperparameters of \mclmc{} as suggested in \cref{sec:method} and examine the impact of these changes on performance. For each ablation, we systematically evaluate the robustness of the approach by changing a single hyperparameter on an appropriate grid. We report both the impact of hyperparameter variations on hold-out test performance metrics and the corresponding tuned values of the sampler's key parametersâthe momentum decorrelation scale ($L$) and step size ($\epsilon$). While the effect on the latter ones is interesting the primary focus is to attain estimates of the two parameters that yield robust and good performance.
For comparison, we also present the performance of \nuts{} and the DE baseline.

\paragraph{Results} The results are summarized in \cref{fig:hyperparam_ablation}. Across all cases, \mclmc{} consistently shows robust performance, with minimal sensitivity to changes in the hyperparameters. The changes in the major parameters $L$ and $\epsilon$ are generally small and with such marginal changes -especially to the important $\epsilon$- do not significantly affect the overall performance. %In fact, the best performance is consistently achieved using the default hyperparameters proposed in our method. 
An exception is the warmup budget, where slightly improved performance can be observed with a significantly extended warmup phase. However, the improvements are marginal and come at a linear increase in runtime, making the additional computational cost unjustifiable. 

In conclusion, these results support the claim that \mclmc{} is effectively tuning-free, as in the considered cases, the method's performance does not change drastically with different hyperparameters and is close to optimal under the default settings. This robustness ensures that practitioners can confidently apply the method without extensive tuning, further enhancing its practical utility.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/warmstart_budget_nuts.pdf}
        \caption{\#Tuning steps/budget for the warmup.}
        \label{fig:warmstart_budget}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/desired_energy_variance_nuts.pdf}
        \caption{Energy variance scheduler ranges.}
        \label{fig:energy_variance}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/effective_samples_nuts.pdf}
        \caption{\#Effective samples to estimate EEVPD}
        \label{fig:effective_samples}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/trust_in_estimate_nuts.pdf}
        \caption{Trust in the estimate parameter.}
        \label{fig:trust_in_estimate}
    \end{subfigure}
    \caption{Results of the ablation studies conducted on the \texttt{bikesharing} dataset for the robustness of the \mclmc{} algorithm to its tuning parameters (x-axes, proposed defaults bold). Both the average hold-out RMSE and LPPD are reported with their standard error for 3 data splits. The same holds for the major parameters of the sampling kernel $L$ and the step size that were tuned by the proposed tuning.
    ``\#Effective samples to estimate EEVPD`` and ``Trust in the estimate`` are minor parameters of the step size adaptation algorithm which determine the sample weighting during the EEVPD computation. 
    }
    \label{fig:hyperparam_ablation}
\end{figure}




\section{Discussion}

In this work, we proposed and evaluated Microcanonical Langevin Ensembles (\mclmc{}) for sampling-based inference of Bayesian neural network posteriors. By adapting the recently proposed MCLMC sampler and combining it with starting values obtained via Deep Ensembles, our comparative analysis reveals substantial advancements in performance, sample quality, UQ, and runtime compared to a NUTS-based alternative. Furthermore, the method enhances the predictability of resource requirements due to its deterministic number of gradient evaluations, which also simplifies parallelization. In conclusion, our proposed method can be considered a reliable and efficient off-the-shelf method and thus a big step forward toward making sampling-based inference feasible for generic BNNs.

%\mclmc{} exhibits a remarkable improvement in speed and scalability over \nuts{}s, often reducing runtime by an order of magnitude while maintaining or even surpassing predictive performance and uncertainty quantification. Furthermore, the method achieves enhances predictability in resource requirements due to it's deterministic number of gradient evaluations, which also simplifies parallelization. \mclmc{} effectively handle larger models and higher-dimensional spaces, and have proven its utility across more complex tasks, including classification and diverse data modalities such as text and images. Convergence diagnostics further affirm \mclmc{}'s superior performance in various scenarios.

\paragraph{Scope of this work and limitations} While also yielding significant runtime savings for increased dataset dimensions, the goal of this work was to overcome the most prevalent bottlenecks of NUTS-based ensembling---its unfavorable scaling with respect to the number of parameters and unforeseeable resource allocation due its variable and high number of gradient evaluations. As MCLMC and our modifications seem to have solved these problems, a next step for future work that we did not compare in this study is the transition to Stochastic Gradient sampler variants. This transition is typically not straightforward but would overcome another remaining limitation of sampling-based inference, namely the scaling for large-scale datasets. 
%However, a notable limitation of \mclmc{} is its reliance on full-batch sampling, which presents challenges for very large datasets due to scalability constraints. Addressing this dataset-size bottleneck will be crucial for further extending the applicability of \mclmc{}. Future work will focus on developing Stochastic Gradient MCMC (SG-MCMC) variants to overcome this limitation and enable efficient sampling for large-scale data.
%
Another possible enhancement of \mclmc{} we did not investigate in this work is the use of alternative priors, e.g., discussed in \citet{fortuin2022bayesian}. 
% Additionally, future work could explore chain subselection or early stopping criteria, as some outlier chainsâthough not entirely offâwere observed in our experiments with both NUTS and MCLMC. These chains performed slightly worse, which was clearly noticeable and likely reduced overall performance. In this work, we included these outliers in the Bayesian model average, treating this behavior as an artifact of epistemic uncertainty.

% In conclusion, \mclmc{} represents a significant advancement in sampling-based inference for BNNs, offering notable improvements in speed and scalability while maintaining robust performance. This approach provides a valuable and practical tool for practitioners, and ongoing research into complementary methods and prior choices will continue to push the boundaries of what is achievable in the field.

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{iclr2025_conference,jakob}
\bibliographystyle{iclr2025_conference}

\clearpage

\appendix
\section{Appendix}

\subsection{Further results}\label{app:furtherresults}

\subsubsection{Benchmarks}\label{app:bench}

\paragraph{UCI benchmark}

The results for the UCI benchmark including standard deviations are given in \cref{tab:uci_repl}.

\renewcommand{\arraystretch}{1.1}
\begin{table}[!h]
\caption{Average hold-out and standard error of LPPD and RMSE performance as well as wallclock time of the DE baseline, \nuts{}s and \mclmc{} for the six datasets used in \citet{sommer2024connecting} over 3 data splits. The wallclock times of the samplers represent the additional sampling time on top of the DE fit which is also reported.}
\label{tab:uci_repl}
\begin{center}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|ccc|ccc|ccc}
\multicolumn{1}{c|}{\bf } &
\multicolumn{3}{c|}{\bf LPPD ($\uparrow$)} & 
\multicolumn{3}{c|}{\bf RMSE ($\downarrow$)} & 
\multicolumn{3}{c}{\bf Time (min)} \\
\multicolumn{1}{c|}{} &
\multicolumn{1}{c}{\bf DE} & \multicolumn{1}{c}{\bf \nuts{}} & \multicolumn{1}{c|}{\bf \mclmc{}} &
\multicolumn{1}{c}{\bf DE} & \multicolumn{1}{c}{\bf \nuts{}} & \multicolumn{1}{c|}{\bf \mclmc{}} &
\multicolumn{1}{c}{\bf DE ($\forall$Methods)} & \multicolumn{1}{c}{\bf \nuts{}} & \multicolumn{1}{c}{\bf \mclmc{}} \\
\hline \\[-2ex]
A & $0.024 \pm 0.024$ & $0.558 \pm 0.034$ & $\boldsymbol{0.612 \pm 0.011}$ & $0.309 \pm 0.005$ & $\boldsymbol{0.214 \pm 0.013}$ & $\boldsymbol{0.206 \pm 0.018}$ & $0.62 \pm 0.15$ & $2.25 \pm 0.01$ & $\boldsymbol{0.84 \pm 0.06}$\\
B & $0.390 \pm 0.010$  & $0.625 \pm 0.004$ & $\boldsymbol{0.645 \pm 0.10}$ & $0.251 \pm 0.004$ & $\boldsymbol{0.242 \pm 0.005}$ & $\boldsymbol{0.236 \pm 0.004}$ & $5.67 \pm 0.98$ & $48.29 \pm 0.51$ & $\boldsymbol{5.40 \pm 0.02}$\\
C & $-0.072 \pm 0.018$ & $\boldsymbol{0.301 \pm 0.088}$ & $\boldsymbol{0.336 \pm 0.054}$ & $0.304 \pm 0.007$ & $\boldsymbol{0.273 \pm 0.012}$ & $\boldsymbol{0.250 \pm 0.014}$ & $0.33 \pm 0.03$ & $1.56 \pm 0.05$ & $\boldsymbol{0.77 \pm 0.06}$\\
E & $1.227 \pm 0.037$ & $2.072 \pm 0.036$ & $\boldsymbol{2.300 \pm 0.066}$ & $0.120 \pm 0.022$ & $0.045 \pm 0.003$ & $\boldsymbol{0.034 \pm 0.007}$ & $0.39 \pm 0.05$ & $1.11 \pm 0.00$ & $\boldsymbol{0.75 \pm 0.03}$\\
P & $-1.024 \pm 0.029$ & $\boldsymbol{-0.760 \pm 0.010}$ & $\boldsymbol{-0.750 \pm 0.018}$ & $0.742 \pm 0.011$ & $\boldsymbol{0.703 \pm 0.003}$ & $\boldsymbol{0.702 \pm 0.008}$ & $12.37 \pm 1.00$ & $152.85 \pm 7.80$ & $\boldsymbol{19.50 \pm 0.01}$\\
Y & $1.623 \pm 0.101$ & $\boldsymbol{2.674 \pm 0.216}$ & $\boldsymbol{2.859 \pm 0.199}$ & $0.081 \pm 0.038$ & $0.083 \pm 0.011$ & $\boldsymbol{0.033 \pm 0.011}$ & $0.16 \pm 0.01$ & $\boldsymbol{0.58 \pm 0.01}$ & $0.64 \pm 0.02$\\
\end{tabular}
} % End resizebox
\end{center}
\end{table}
\renewcommand{\arraystretch}{1}

\paragraph{Chain variances} Our analyses of between- and within-chain variances (\cref{fig:diagnostics_uci}) show a distinctive pattern of an increasing within-chain variance in layers further away from the input and output layer. Contrasting this with the work by \cite{sommer2024connecting}, this suggests that \mclmc{} also exhibits most disconnected modes in the first and last layers. This analysis can help to assess whether sampling the multimodal posterior surface of BNNs is an infeasible problem due to the combinatorial explosion of modes with an increased depth of the network (which is not the case). 

\paragraph{Calibration} We also compute calibration errors (see \cref{def:cal_error}) and analyze coverage for credible intervals across various nominal coverage levels for the UCI benchmarks. \cref{tab:calibration_error} and \cref{fig:cov_plots} show that \mclmc{} achieves calibration quality comparable to the one of \nuts{}, confirming its effectiveness in uncertainty quantification.

\paragraph{Classification benchmark} For a comparison with \nuts{}, we ran the smaller tabular classification tasks both using \nuts{} and \mclmc. For the larger experiments (both considerably larger in dataset and model complexity), \nuts{} would require weeks to run and is thus omitted. The results of the comparative study are given in \cref{tab:classificationtasks_nutscompare} and suggest on-par performance of \mclmc{} with \nuts{} and clearly superior performance to the DE baseline.

\renewcommand{\arraystretch}{1.05}
\begin{table}[ht]
\centering
\caption{Hold-out test performance of \nuts{}, \mclmc{} and baselines on the two tabular classification tasks.}
\label{tab:classificationtasks_nutscompare}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|ccc|ccc|ccc|ccc}
\hline\hline
\multicolumn{1}{c|}{\bf Dataset} & 
\multicolumn{6}{c|}{\bf Accuracy ($\uparrow$)} & 
\multicolumn{6}{c}{\bf LPPD ($\uparrow$)} %& 
%\multicolumn{2}{c}{\bf Tuned Param.} 
\\
\multicolumn{1}{c|}{} & 
\multicolumn{3}{c|}{\bf Avg. Single} & 
\multicolumn{3}{c|}{\bf Ensemble} & 
\multicolumn{3}{c|}{\bf Avg. Single} & 
\multicolumn{3}{c}{\bf Ensemble} %& 
%\multicolumn{1}{c}{\bf Step Size} & 
%\multicolumn{1}{c}{\bf $L$} 
\\ 
\multicolumn{1}{c|}{} & 
\multicolumn{1}{c}{\bf DNN} & \multicolumn{1}{|c|}{\bf Chain (\nuts)} & \multicolumn{1}{c|}{\bf Chain (\mclmc)} & 
\multicolumn{1}{c}{\bf DE} & \multicolumn{1}{|c|}{\bf \nuts{}} & \multicolumn{1}{c|}{\bf \mclmc{}} & 
\multicolumn{1}{c}{\bf DNN} & \multicolumn{1}{|c|}{\bf Chain (\nuts)} & \multicolumn{1}{c|}{\bf Chain (\mclmc)} & 
\multicolumn{1}{c}{\bf DE} & \multicolumn{1}{|c|}{\bf \nuts{}} & \multicolumn{1}{c}{\bf \mclmc{}}  
%\multicolumn{1}{c}{\bf } & 
%\multicolumn{1}{c}{\bf } 
\\ 
\hline \\[-2ex]
Ionosphere & 0.930 & 0.955 & \textbf{0.958} & \textbf{0.958} & \textbf{0.958} & \textbf{0.958} & -0.404 & -0.172 &  -0.168 & -0.309 & -0.172 &\textbf{-0.167}\\ % & 1.263 & 36.85\\ 
Income & 0.843 & 0.850 & \textbf{0.851} & 0.846 & \textbf{0.851} & \textbf{0.851} & -0.334 & -0.315 & -0.315 & -0.318 & \textbf{-0.311} & -0.313 \\ %& 0.375 & 64.78\\ 
\hline\hline
\end{tabular}
} % End resizebox
\end{table}
\renewcommand{\arraystretch}{1.0}

\subsubsection{Robustness and numerical stability in high dimensions}\label{app:numerical_stability}

Without our adjustments, MCLMC struggles with exploration and often fails to produce meaningful samples, especially in high-dimensional settings. To highlight this, we conduct an ablation study, assessing failure rates when applying MCLMC to BNNs without our proposed adjustments. Specifically, we use the same models as in Table \ref{tab:uci_repl_main} and run 100 chains each of MILE and na\"ive MCLMC on various datasets with differing parameter dimensions and with the same DE initialization, recording the percentage of chains that resulted in numerical issues (e.g., NaN values rendering all samples unusable). The results, given in Table \ref{tab:robustness_study}, demonstrate the critical importance of our adjustments: MCLMC exhibits failure rates between 78\% and 86\%, while MILE consistently shows 0\% failures across all datasets. 

\renewcommand{\arraystretch}{1.05}
\begin{table}[ht]
\centering
\caption{Failure rates (NaN chains) for na\"ive MCLMC and MILE across different datasets.}
\label{tab:robustness_study}
\resizebox{0.6\textwidth}{!}{%
\begin{tabular}{l|c|c}
\hline\hline
\textbf{Dataset} & \textbf{MCLMC (\% NaN Chains)} & \textbf{MILE (\% NaN Chains)} \\ \hline
{Airfoil}  & 86\%  & \textbf{0\%} \\ 
{Concrete} & 80\%  & \textbf{0\%} \\ 
{Energy}   & 78\%  & \textbf{0\%} \\ 
{Yacht}    & 85\%  & \textbf{0\%} \\ 
\hline\hline
\end{tabular}
} % End resizebox
\end{table}
\renewcommand{\arraystretch}{1.0}



\subsubsection{Further comparisons}\label{app:furthercomps}

\paragraph{Comparison with Path-Guided Particle-based Sampling}

We conduct an empirical comparison with the recently proposed Path-Guided Particle-based Sampling \citep[PGPS,][]{pmlr-v235-fan24f}.
Following the experimental setup of Section 5.2.1 of \citet{pmlr-v235-fan24f}, we conduct BNN inference on 7 UCI classification datasets \citep{Dua.2019} and report the average negative log-likelihood (NLL) and accuracy. Table \ref{tab:methodcomparisonPGPS} contains the results that showcase a clear pattern. \mclmc{} performs at least as good in terms of accuracy as PGPS and is clearly superior in the NLL in most cases. Notably, running MILE for all experiments and replications takes less than 5 minutes on a consumer CPU, with many in under 1 minute. We compared PGPS and MILE in terms of runtime on the same hardware for the Sonar dataset across five independent runs as an example. While MILE achieved a runtime of $0.94 \pm 0.06$ minutes PGPS required $24.34 \pm 0.64$ minutes. The major factor for the runtime gap is the nested computation detailed in PGPS (Algorithm 3). For example, the authors chose 100k overall steps each with 100 optimization steps and 300 Langevin adjustments for the UCI benchmark. This incurs high computational costs, even without considering the additional overhead of the tuning of the PGPS hyperparameters $\alpha$ and $\beta$. 


\renewcommand{\arraystretch}{1.05}
\begin{table}[ht]
\centering
\caption{Hold-out test performance of Path-Guided Particle-based Sampling and MILE on the UCI classification tasks of Table 1 and 4 of \citet{pmlr-v235-fan24f} over five independent runs.}
\label{tab:methodcomparisonPGPS}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|c|c|cc|cc}
\hline\hline
\multicolumn{1}{c|}{\bf Dataset} & 
\multicolumn{1}{c|}{\bf \# Classes} & 
\multicolumn{1}{c|}{\bf \# Rows} & 
\multicolumn{2}{c|}{\bf NLL ($\downarrow$)} & 
\multicolumn{2}{c}{\bf Accuracy ($\uparrow$)} \\

\multicolumn{1}{c|}{} & 
\multicolumn{1}{c|}{} & 
\multicolumn{1}{c|}{} & 
\multicolumn{1}{c}{\bf PGPS} & \multicolumn{1}{c|}{\bf MILE} & 
\multicolumn{1}{c}{\bf PGPS} & \multicolumn{1}{c}{\bf MILE} \\
\hline
SONAR & 2 & 207 & $\boldsymbol{0.536 \pm 0.014}$ & $0.979 \pm 0.094$ & $\boldsymbol{0.798 \pm 0.023}$ & $\boldsymbol{0.779 \pm 0.047}$ \\
WINEWHITE & 7 & 4898 & $1.979 \pm 0.009$ & $\boldsymbol{1.110 \pm 0.014}$ & $ 0.452 \pm 0.010$ & $\boldsymbol{0.565 \pm 0.008}$ \\
WINERED & 6 & 1599 & $1.964 \pm 0.012$ & $\boldsymbol{1.060 \pm 0.037}$ & $\boldsymbol{0.594 \pm 0.018}$ & $\boldsymbol{0.604 \pm 0.019}$ \\
AUSTRALIAN & 2 & 689 & $\boldsymbol{0.5042 \pm 0.013}$ & $\boldsymbol{0.486 \pm 0.087}$ & $\boldsymbol{0.862 \pm 0.009}$ & $\boldsymbol{0.852 \pm 0.015}$ \\
HEART & 5 & 302 & $0.943 \pm 0.030$ & $\boldsymbol{1.440 \pm 0.078}$ & $0.256 \pm 0.142$ & $\boldsymbol{0.591  \pm 0.033}$ \\
GLASS & 6 & 213 & $1.685 \pm 0.030$ & $\boldsymbol{1.160 \pm 0.083}$ & $\boldsymbol{0.585 \pm 0.080}$ & $\boldsymbol{0.643 \pm 0.063}$ \\
COVERTYPE & 7 & 8000 & $1.602 \pm 0.014$ & $\boldsymbol{0.717 \pm 0.024}$ & $0.590 \pm 0.095$ & $\boldsymbol{0.746 \pm 0.006}$\\
\hline\hline
\end{tabular}
} % End resizebox
\end{table}
\renewcommand{\arraystretch}{1.0}

\paragraph{Comparison with Symmetric Split HMC}

We also conduct an empirical comparison with Symmetric Split HMC \citep[Sym-Split-HMC,][]{cobb_2021_ScalingHamiltonian}.
Symmetric Split HMC advances HMC but inherits the same hyperparameter sensitivity (e.g., depends on trajectory length and step size). These hyperparameters can limit the application in Bayesian neural network inference. In \citet{cobb_2021_ScalingHamiltonian}, the authors use Bayesian Optimization (BO) to derive hyperparameters which introduces further complexity and a significant computational burden. Unlike MILE, Symmetric Split HMC employs an MH correction step, which further increases the computational costs. Another downside of \citet{cobb_2021_ScalingHamiltonian} is that with an increased number of batches, the computational requirements increase notably. Both approaches have merit, but their main goal and contribution differ considerably. Symmetric Split HMC focuses on memory scalability, while MILE optimizes speed and performance. Nevertheless, an empirical comparison is interesting.

We replicate the multi-class classification task for the Fashion-MNIST dataset from Table \ref{tab:classificationtasks} with the CNN (v2) model using Sym-Split-HMC. We use the optimized hyperparameters reported in \citet{cobb_2021_ScalingHamiltonian}, Section 5.3, for the same dataset and task. We conduct all experiments on the same hardware to ensure comparability of runtimes and report the results in Table \ref{tab:methodcomparisonMILESplitHMC}.
For a fixed amount of posterior samples, the performance of Symmetric Split HMC benefits from smaller batch sizes. However, as noted above and confirmed empirically, runtime increases notably for smaller batch sizes. We choose a batch size of 64 and run Symmetric Split HMC for 200 samples, requiring 15.5 hours. The intended goal of sampling 1000 samples (as with MILE) would take more than 3 days with this setting. % as such can not be considered a serious competitor for MILE which runs in less then 1.5 hours. 
For larger batches of 1024 images, we generate up to 3000 posterior samples for Symmetric Split HMC, but without a notable gain in performance. Regardless of the specification, it becomes clear that MILE achieves considerably better performance in a fraction of the required time for Symmetric Split HMC (without considering the cost of running BO for hyperparameter tuning). 


\renewcommand{\arraystretch}{1.1}
\begin{table}[ht]
\centering
\caption{Comparison of off-the-shelf MILE with Symmetric Split HMC on the Fashion-MNIST task using the CNNv2 model. Total Time does not consider the necessary BO step of Symmetric Split HMC.}
\label{tab:methodcomparisonMILESplitHMC}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c}
\hline\hline
\textbf{Method} & \textbf{Accuracy ($\uparrow$)} & \textbf{LPPD ($\uparrow$)} & \textbf{Post. Samples} & \textbf{Total Time} \\
\hline
MILE & $\boldsymbol{0.925}$ & $\boldsymbol{-0.216}$ & 1000 & 1h 21min \\
\hline
Sym-Split-HMC (Batch size: 64) & $0.818$ & $-0.548$ & 200 (+50 burn-in) & 15h 29min \\
Sym-Split-HMC (Batch size: 1024) & $0.820$ & $-0.525$  & 1000 (+200 burn-in) & 7h 6min \\
Sym-Split-HMC (Batch size: 1024) & $0.813$ & $-0.513$  & 3000 (+300 burn-in) & 18h 47min \\
\hline\hline
\end{tabular}
} % End resizebox
\end{table}
\renewcommand{\arraystretch}{1.0}





\subsection{Experimental setup and further details}\label{app:experimentsetting}

\paragraph{Software}
Our software is implemented in Python and mainly relies on the \texttt{jax} \citep{jax2018github} and \texttt{BlackJAX} \citep{cabezas2024blackjax} libraries.
We further use \texttt{Docker} for a reproducible experimental setup.
Our code is available at \url{https://github.com/EmanuelSommer/MILE}.

\paragraph{Compute environment} The experiments were run on two NVIDIA RTX A6000 GPUs and an AMD Ryzenâ¢ Threadripperâ¢ PRO 5000WX/3000WX CPU with 64 cores. Sampling 12 chains for most experiments allowed to parallelize the sampling on CPU such that multiple experiments can be run at the same time.

\paragraph{Benchmark data}

\cref{tab:dataoverview} gives an overview of the data, and associated tasks and provides all references.

\begin{table}[h]
\begin{small}
\begin{sc}
\begin{center}
\caption{Overview of the used datasets with task description and references.} \label{tab:dataoverview}
\vskip 0.1in
\resizebox{0.99\columnwidth}{!}{%
\begin{tabular}{clrrrrl}
Abbrev. & Data set & Task & \# Obs. & Feat. & Reference \\ \hline 
A & Airfoil & Regression & 1503 & 5 &  \citet{Dua.2019}  \\
B & Bikesharing & Regression & 17379 & 13 & \citet{misc_bike_sharing_dataset_275}  \\
C & Concrete   & Regression & 1030 & 8 &  \citet{Yeh.1998} \\
E & Energy  & Regression & 768 & 8 &  \citet{Tsanas.2012} \\
P & Protein & Regression & 45730 & 9 & \citet{Dua.2019} \\
Y & Yacht  & Regression & 308 & 6 & \citet{Ortigosa.2007,Dua.2019} \\
- & Ionosphere & Binary-Class. & 351 & 34 & \citet{sigillito1989ionosphere} \\
- & Income & Binary-Class. & 48842 & 14 & \citet{kohavi96incomedata} \\ 
- & IMDB & Binary-Class. & 50000 & Text & \citet{maas-EtAl:2011:ACL-HLT2011} \\
- & MNIST & Multi-Class. & 60000 & 28x28 & \citet{lecun-mnisthandwrittendigit-2010} \\
- & F(ashion)-MNIST & Multi-Class. & 60000 & 28x28 & \citet{xiao2017/online} \\
\hline
\end{tabular}
}
\end{center}
\end{sc}
\end{small}
\end{table}

\paragraph{Optimization \& sampling} For all DE optimizations, we use ADAM with decoupled weight decay \citep{loshchilov2018decoupled} and use the negative log-likelihood loss as objective. We employ early stopping on a validation set and use a 70\% train, 10\% validation and 20\% test split if there is no predefined test set as for the MNIST and Fashion MNIST dataset. If not specified otherwise we use 12 DE members and 12 chains. For all NUTS-based experiments, we use a burn-in of 100 samples and collect 1000 posterior samples with a target acceptance rate of $0.8$. Also, we employ an isotropic standard Gaussian prior if not specified otherwise.

We do not adjust the effective number of samples in the MCLMC tuning even if we apply a considerable amount of thinning, i.e., for 10000 samples with a thinning interval of 10, resulting in 1000 final samples, we still use an ESS of 100. However, for less than 1000 final samples, we hold ESS fixed at 100 as a lower bound. %Notably, those scenarios always have large thinning intervals. 
We validated the robustness of this choice by various experiments and ablation studies discussed in \cref{sec:exps}. 

\paragraph{Regression tasks} We train distributional regression models for all regression tasks just as \citet{izmailov_2021_WhatArea, sommer2024connecting}. That means, we parameterize the Gaussian likelihood with by the output neurons as location and $\log$-scale. For the experiments aggregated in \cref{tab:uci_repl}, \cref{tab:calibration_error} and \cref{fig:grad_evals}, we use configurations as in \citet{sommer2024connecting}, in particular, use a fully-connected neural network that has two hidden layers with 16 neurons each. 

\paragraph{Ablation studies} For the ablation studies, we use the larger UCI benchmark datasets \texttt{bikesharing} and \texttt{protein}. If not specified otherwise, we use slightly larger networks than before by considering three hidden layers of 16 neurons each. In order to analyze the behavior of the samplers, we further implement a slim and deeper network with 6 hidden layers of just 8 neurons each. The corresponding experimental results are reported in \cref{fig:diagnostics_uci}. 

\paragraph{Classification tasks} For the classification tasks, we follow the classical way of directly parameterizing the categorical distribution with as many output neurons as we have classes. For the tabular datasets \texttt{ionosphere} and \texttt{income},  we use a simple feed-forward neural networks with 2 (v1) and 4 (v2) hidden layers with 16 neurons each. We also consider new data modalities for the \nuts{}, namely images and text. The corresponding architectures are described in \cref{tab:lenet_vs_lenetti}. Moreover, we use 10 chains for CNNs, 8 chains for the sequential models v1-2, and 4 chains for the sequential model v3 for these larger networks. We save only 100 samples per chain to be more memory efficient and are thus able to showcase improvement even for a smaller overall amount of posterior samples. This is realized via thinning in both \nuts{} and \mclmc{}.

\paragraph{Convolutional neural networks} As convolutional neural network (CNN) architectures we choose a LeNet-5 \citep{lecunlenet5} architecture (CNNv2) and also consider a slightly smaller yet similar architecture (CNNv1). The architectures are described in detail in Table \ref{tab:lenet_vs_lenetti}.


\begin{table}[ht]
\caption{CNN architectures.}
\label{tab:lenet_vs_lenetti}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcc}
 & \textbf{CNNv2} & \textbf{CNNv1} \\ \hline
\textbf{Conv} & 6 filters, 5x5 kernel, padding 2, ReLU & 1 filter, 3x3 kernel, padding 2, ReLU \\
\textbf{Pooling} & 2x2 Avg Pooling, stride 2 & - \\
\textbf{Conv} & 16 filters, 5x5 kernel, no padding, ReLU & - \\
\textbf{Pooling} & 2x2 Avg Pooling, stride 2 & - \\
\textbf{FC} & 120 units, ReLU & 8 units, ReLU \\
\textbf{FC} & 84 units, ReLU & 8 units, ReLU \\
\textbf{FC} & Output units & 8 units, ReLU \\
\textbf{FC} & - & Output units \\
\hline
\end{tabular}
}
\end{table}

\paragraph{Sequential networks} \cref{fig:seq_architecture} provides a schematic overview of the attention-based sequential model architecture. We explore two main configurations: one where all model parameters, including token and positional embeddings, are sampled (v1), and another using a fixed, pretrained embedding (v2,v3). Both models use a context length of 70 tokens, with padding or truncation for shorter or longer sequences. We trained a custom tokenizer with Byte-Pair Encoding \citep[BPE,][]{bytepairencoding}, targeting vocabulary sizes of 1k and 10k tokens for v1 and v2-3, respectively. To balance model complexity, token embeddings were set to 48 dimensions for the fully sampled model v1 and 192 for the pretrained versions v2-3. Positional encodings are added before passing through an 8-head attention mechanism, with 64-dimensional query, key, and value vectors \citep{attentionisallyouneed} for v1-2. For v3, we use a 10-head attention mechanism with 100-dimensional query, key and value vectors. After average pooling, a feed-forward network with one hidden layer (64 neurons for the full model v1, 32 for the pretrained version v2) or two hidden layers for v3 with 128 and 32 neurons output the logits.

% \cref{tab:classificationtasks} demonstrates that for the attention-based sequential models, \mclmc{} needs adequate model expressivity to robustly deliver a meaningful improvement over DE.

\begin{figure}[ht]
\centering
\resizebox{0.7\textwidth}{!}{ % Adjust the scale here
\begin{tikzpicture}[
    block/.style={draw, rounded corners, thick, minimum width=3.5cm, minimum height=1cm, align=center},
    arrow/.style={-Stealth, thick},
    sum/.style={circle, draw, inner sep=0pt, minimum size=5mm, thick},
    ]

    % Text Input
    \node[block, fill=green!20] (textinput) {Text Input};

    % BPE Tokenization
    \node[block, fill=purple!20, below=1cm of textinput] (bpe) {BPE Tokenization};
    
    % Input Embedding
    \node[block, fill=red!20, below=1cm of bpe] (input) {Embedding};
    
    % Positional Encoding
    \node[sum, right=0.5cm of input] (sum) {+};
    \node[block, fill=white!20, above=0.5cm of sum] (position) {Positional Encoding};
    \draw[arrow] (position) -- (sum);
    
    % Multi-Head Attention
    \node[block, fill=orange!20, right=1cm of sum] (mha) {Multi-Head Attention};
    
    % Feed Forward
    \node[block, fill=blue!20, right=1cm of mha] (ff) {Fully Connected};
    
    % Arrows
    \draw[arrow] (textinput) -- (bpe);
    \draw[arrow] (bpe) -- (input);
    \draw[arrow] (input) -- (sum);
    \draw[arrow] (sum) -- (mha);
    \draw[arrow] (mha) -- (ff);
    
    % Final Output as Logits
    \node[block, fill=white!20, below=1cm of ff] (logits) {Logits};
    \draw[arrow] (ff) -- (logits);

\end{tikzpicture}
}
\caption{Schematic overview of the sequential attention-based model architecture (ATT) that is applied to the IMDB Dataset.}
\label{fig:seq_architecture}
\end{figure}

\paragraph{Prior induced regularization} As the prior acts as a regularizer during the sampling phase, we might exhibit performance degradation for larger classification models if the prior variance is chosen inappropriately small. For the larger CNN and ATT models, we therefore choose the standard isotropic Gaussians $\mathcal{N}(0,0.1I)$ (CNNv2), $\mathcal{N}(0,0.2I)$ (ATTv1,v2) and $\mathcal{N}(0,0.4I)$ (ATTv3). While a dedicated study on the influence of priors within this framework is out of scope for this work, we think further tuning the prior variance or changing the prior distribution could be promising.

\subsection{Diagnostics}\label{sec:diagnostics}

We report the BNN-specific diagnostics proposed in \citet{sommer2024connecting} in \cref{fig:diagnostics_uci}. The displayed chain variances are discussed in  \cref{app:bench}.

\paragraph{Effective sample size}
We observe very similar effective sample size (ESS) values for \nuts{} as reported in \citet{izmailov_2021_WhatArea, sommer2024connecting}. In most cases, \mclmc{} is on a same or slightly higher level of ESS. However, especially for weights that are close to the input and output, we observe a much higher ESS than for \nuts{}. For the \texttt{airfoil} dataset, this ESS increase also given for deeper layers.

\paragraph{Convergence of MILE and chainwise mixing}
Based on the results of \citet{robnik_microcanonical_2024}, we know that \mclmc{} will provide the same convergence guarantees as long as the initialization is done randomly or its effect becomes negligible as $S\to\infty$ and the discretization error is MH-adjusted. As our work's focus is on empirical efficiency rather than guaranteed convergence, we a) do not use MH-adjustment but control discretization error using the EEVPD b) do not run chains for a large number of steps, and c) start chains using deep ensemble initializations. This is a compromise that will induce a bias in the sampling distribution but ensures a more stable behavior during sampling, and, in turn, increases the ESS. 
%and being practically limited by especially the number of chains and samples we can only expect a somewhat localized, yet very flexible and efficient, exploration of the multimodal posterior with a constrained computational budget. 

To measure chainwise mixing, we proxy local chainwise convergence using the chainwise split metric $\widehat{cR}$ with a split factor of 4. Our results demonstrate that \mclmc{} clearly improves chainwise mixing. However, all values remain notably higher than the conventional cutoff thresholds of 1.1 and 1.01 \citep{vetharirhat2021}.

% \paragraph{Chain variances}
% Similar to NUTS, MCLMC displays increased within-chain variance in deeper layers, with the exception of the last layer. However, the between-chain variance does not follow this distinct pattern. This observation aligns with the findings in \citet{sommer2024connecting}, which attribute the variance to multimodality and use it as a key motivation for their ensembling approach. Notably, our results show that this pattern also holds for MCLMC, indicating its consistency across different sampling methods

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/ess_nuts.pdf}
        \caption{Effective Sample Size.}
        \label{fig:ess_uci}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/crhat_nuts.pdf}
        \caption{Chainwise mixing measured by $\widehat{cR}$}
        \label{fig:crhat_uci}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/bcv_nuts.pdf}
        \caption{Between Chain Variances.}
        \label{fig:bcv_uci}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/wcv_nuts.pdf}
        \caption{Within Chain Variances.}
        \label{fig:wcv_uci}
    \end{subfigure}
    \caption{Different sampling diagnostics of a seven-layer BNN for three UCI benchmark datasets (in different rows) separated by layer (x-axis) over three data splits.}
    \label{fig:diagnostics_uci}
\end{figure}

\subsection{Evaluation}\label{sec:eval}

\paragraph{Predictive performance} Following \citet{gelman2014a, wiese2023towards} and \citet{sommer2024connecting}, we choose the log posterior predictive density (LPPD) over a test set $\mathcal{D}_{\text{test}}$, defined as
\begin{align} \label{eq:lppd}
    \text{LPPD} = \frac{1}{n_{\text{test}}} \sum_{(\bm{y}^\ast, \bm{x}^\ast) \in \mathcal{D}_{\text{test}}} \log \left(\frac{1}{K \cdot S} \sum_{k=1}^K \sum_{s=1}^{S} p \left(\bm{y}^\ast | \bm{\theta}^{(k,s)}(\bm{x}^\ast)\right)\right)
\end{align}
in order to quantify the quality of the PPD approximation and UQ in general.
Intuitively, the LPPD measures the average extent to which the predictive distribution accurately covers the observed labels.

Additionally, we use the root mean squared error (RMSE) for regression tasks and accuracy (ACC) for classification tasks to assess point predictions. While LPPD evaluates the overall fit of the predictive distribution, RMSE and ACC provide specific metrics for the accuracy of point predictions in their respective domains.

\paragraph{Calibration}
Following \citet{KuleshovFE18}, we define calibration and the empirical (squared) calibration error in the regression setting. Intuitively one expects samples from the true PPD to be contained in the Credibility Intervals (CIs) with the coverage probability of the CI. The following definition formalizes this.

\begin{definition}[Calibration]\label{def:calibration}
For some realized labeled dataset $\mathcal{D} = \{(\bm{x}_{i},y_{i})\}_{i=1}^n \in \mathcal{X} \times \mathbb{R}$ of random variables $X,Y$, we define a credible interval $\mathcal{C}_{1-\alpha}(\bm{x}_{*}, \mathcal{D})$ to be \textbf{calibrated} at level $1-\alpha \in (0,1)$ iff for $y_{*} \sim p(\cdot \mid \bm{x}_{*}, \mathcal{D})$ it holds that
\begin{equation}
    \mathbb{P}\big(y_{*} \in \mathcal{C}_{1-\alpha}(\bm{x}_{*}, \mathcal{D})\big) = 1-\alpha.
\end{equation}
If $y_{*} \in \mathcal{C}_{1-\alpha}(\bm{x}_{*}, \mathcal{D})$, we say that the CI $\mathcal{C}_{1-\alpha}(\bm{x}_{*}, \mathcal{D})$ covers $y_{*}$. Thus calibrated models have correct \textbf{coverage} probabilities.
\end{definition}

This straightforwardly leads to the definition of the calibration error.

\begin{definition}[Calibration error]\label{def:cal_error}
    We define the empirical weighted \textbf{calibration error (CalE)} over the hold-out validation data set $\mathcal{D}_{*}$ as the root mean squared difference of nominal $1-\alpha_l$ and empirical $1-\hat\alpha_l$ CI coverages over a range of $L$ relevant coverage levels $\alpha_1, \dots, \alpha_L$:
    \begin{align}
        &CalE(\mathcal{D}_{*}) = \bigg(\sum_{l=1}^L w_l \cdot (\hat\alpha_l - \alpha_l)^2\bigg)^{\frac{1}{2}} \\ 
        \text{with} \; &1-\hat\alpha_l = \frac{1}{|\mathcal{D}_{*}|} \sum_{(\bm{x}_{*}, y_{*}) \in \mathcal{D}_{*}} \mathbb{I}\{y_{*} \in \mathcal{C}_{1-\hat\alpha_l}(\bm{x}_{*}, \mathcal{D})\},
    \end{align}
    where $w_l$ are normalized weights of the coverage levels which are commonly considered to be constant, i.e., $w_l = 1 \;\forall l \in [L]$. 
\end{definition}

We report this calibration error in \cref{tab:calibration_error} for DEs, \nuts{} and \mclmc{} for multiple datasets in the distributional regression setting. We consider the coverage levels 0.5, 0.75, 0.9, and 0.95. For most cases, we observe that the calibration error of \mclmc{} is on the same level as \nuts{} and both methods often outperform the simple DE. 
For smaller datasets, however, estimating the empirical quantiles for small $\alpha$ is less robust due to limited test data size. Since the calibration error does not indicate whether the model is over- or underconfident, we also examine the coverage levels directly in Figure \ref{fig:cov_plots}. The plots show a high variation in coverage quality of DE-based confidence intervals by exhibiting both strong structural under- and overconfidence, whereas the sampling-based methods are generally better calibrated. 
The larger datasets, \texttt{bikesharing} and \texttt{protein}, which more likely provide enough data for reliable empirical coverage estimates, are a good example of this: for \texttt{bikesharing}, DE is more underconfident, while for \texttt{protein}, it is overconfident in contrast to the sampling-based alternatives.
A visual inspection reveals that both \nuts{} and \mclmc{} tend to be slightly underconfident, which is however often preferred by the practitioners over structural overconfidence, as seen for example with DE in the \texttt{protein} dataset.
All in all, a more careful analysis of calibration of \mclmc{} would be a great direction for future work.

\begin{table}[t]
\caption{Mean Calibration Error for the DE baseline, \nuts{} and \mclmc{} for six datasets. The nominal coverage levels used are 0.5, 0.75, 0.9 and 0.95. The experimental setup is identical to the one in \cref{tab:uci_repl_main}.}
\label{tab:calibration_error}
\begin{center}
\resizebox{0.6\columnwidth}{!}{%
\begin{tabular}{lrrr}
\multicolumn{1}{c}{} &
\multicolumn{3}{c}{\bf Calibration Error ($\downarrow$)} \\
\multicolumn{1}{c}{\bf Dataset} &
\multicolumn{1}{c}{\bf DE} & \multicolumn{1}{c}{\bf \nuts{}} & \multicolumn{1}{c}{\bf \mclmc{}} \\
\hline \\[-2ex]
Airfoil & $\boldsymbol{0.077 \pm 0.012}$ & $\boldsymbol{0.083 \pm 0.009}$ & $\boldsymbol{0.086 \pm 0.018}$ \\
Bikesharing & $0.125 \pm 0.005$ & $\boldsymbol{0.080 \pm 0.003}$ & $\boldsymbol{0.081 \pm 0.002}$ \\
Concrete & $0.066 \pm 0.002$ & $\boldsymbol{0.050 \pm 0.003}$ & $\boldsymbol{0.068 \pm 0.017}$ \\
Energy & $0.215 \pm 0.015$ & $\boldsymbol{0.032 \pm 0.003}$ & $0.061 \pm 0.014$ \\
Protein & $\boldsymbol{0.054 \pm 0.011}$ & $\boldsymbol{0.056 \pm 0.002}$ & $\boldsymbol{0.057 \pm 0.003}$ \\
Yacht & $0.253 \pm 0.037$ & $\boldsymbol{0.188 \pm 0.032}$ & $\boldsymbol{0.133 \pm 0.059}$ \\
\end{tabular}
} % End resizebox
\end{center}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{img/coverage.pdf}
    \caption{Mean and standard error of empirical coverage (y-axes) for the DE baseline, \nuts{} and \mclmc{} for six datasets (facets). The nominal coverage levels used are 0.5, 0.75, 0.9 and 0.95 (x-axes). The experimental setup is identical to the one in \cref{tab:uci_repl_main}.}
    \label{fig:cov_plots}
\end{figure}




\end{document}
