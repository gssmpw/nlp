\section{Details about the Descriptive tasks experiments}
\label{app:desc_details}

\subsection{Details of Meta-Dataset Creation}
\label{app:data_generation}

To ensure reproducibility of the experiments, we describe the synthetic data generation process in detail. The datasets were generated using a custom-built class, \texttt{DescMetaDatasetGenerator}, which allows for the creation of datasets with various distributions and customizable descriptive target variables. The key components and configurations are outlined below.

\xhdr{In-Meta-Distribution}
The set of distributions used to generate datasets during training is parameterized as follows:
\begin{itemize}
    \item \texttt{normal}:: It has two parameters: the mean and the variance. Mean values are sampled from $\texttt{[-3, 3]}$, and variances are sampled from $\texttt{[0.1, 1.5]}$. 
    \item \texttt{uniform}:: It has two parameters: the lower bound and the upper bound. The lower bounds are sampled from $\texttt{[-3.5, -0.5]}$ and the upper bounds from $\texttt{[0.5, 3.5]}$.
    \item \texttt{beta}:: It has two parameters: $a$ and $b$. Parameters $a$ and $b$ are sampled from $\texttt{[1, 3]}$ and $\texttt{[2, 5]}$, respectively.
    \item \texttt{exponential}:: It has one parameter: scale sampled from $\texttt{[1, 2]}$.
\end{itemize}

\xhdr{Out-of-Meta-Distribution}
The set of distribution used to test models for unseen distribution families is parametrized as follows:
\begin{itemize}
    \item \texttt{gamma}:: It has two parameters: shape and scale. Shape parameters are sampled from $\texttt{[1, 5]}$, and scale parameters from $\texttt{[1, 2]}$.
    \item \texttt{log-normal}:: It has two parameters: mean and variance. Means are sampled from $\texttt{[0, 1]}$, and standard deviations from $\texttt{[0.5, 0.75]}$.
\end{itemize}

% \subsubsection*{Dataset Characteristics}
\xhdr{Dataset Characteristics}
Once a distribution $P_X$ has been sampled, we use it to sample one dataset. In general, we could sample several dataset per distributions but we prefer to sample only one to maximize the diversity of distributions seen during training. Each dataset is defined by the following parameters:
\begin{itemize}
    \item \textbf{Number of variables (\texttt{n\_var})}: The number of features (columns) in the dataset. For our experiments, we set $\texttt{n\_var} = 2$.
    \item \textbf{Number of rows (\texttt{n\_row\_range})}: The number of samples (rows) in the dataset, sampled uniformly from the range \texttt{[5, 300]}. During testing, we explore longer lengths to test the generalization of meta-statistical models.
    \item To generate the target values $y$, each dataset is passed through the target descriptive functions: per-column-mean, per-column-median, correlation, win rate, optimal transport (1D).
\end{itemize}
This results in a meta-datapoint. We them sample many meta-datapoints to build a meta-datasets with the following split sizes: 30K training, 300 validation, 3K for testing in-meta-distribution and 3K for testing out-of-meta-distribution.

\subsection{Examples of Training Curves}
\label{app:training_curves}
For meta-statistical models of approximately the same size ($\approx$ 10K parameters), we compare their convergence during training on the task of predicting the correlation between variable A and variable B, the two columns of the dataset. We consider the same meta-statistical models and the same meta-dataset generation parameters as considered in the results of the main paper. We report the results in \Figref{fig:training_curves}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{images/training_curves_corr.pdf} % Replace with your image file
    \caption{\textbf{Training curves:} Comparison of training convergence of meta-statistical models on the correlation task.}
    \label{fig:training_curves}
\end{figure}


\subsection{More Generalization Plots}
\label{app:gen_plots}
In \Figref{fig:generalization_app}, we report the same generalization plots as the main paper for other descriptive tasks. Similar conclusion holds: models generalize very well with lengths and tend to suffer from an offset of performance out-of-meta-distribution.

\begin{figure}[t] % 't' places the figure at the top of the page
    \centering
    % Subfigure (a)
    \begin{subfigure}[t]{0.49\textwidth} % Adjust width as needed
        \centering
        \includegraphics[width=\textwidth]{images/per_column_mean.pdf}
        \caption{\texttt{Win-rate} prediction}
        \label{fig:subfig_a_app}
    \end{subfigure}
    \hfill
    % Subfigure (b)
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/per_column_median.pdf} 
        \caption{\texttt{Median} prediction}
        \label{fig:subfig_b_app}
    \end{subfigure}
    \vspace{0.5cm} % Add vertical spacing between rows of subfigures
    % Subfigure (c)
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/correlation.pdf}
        \caption{\texttt{Correlation} prediction}
        \label{fig:subfig_c_app}
    \end{subfigure}
    \hfill
    % Subfigure (d)
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/ot_1d.pdf} 
        \caption{\texttt{Optimal Transport} prediction}
        \label{fig:subfig_d_app}
    \end{subfigure}

    % \vspace{-0.5cm}
    % Main caption for the figure
    \caption{\textbf{Generalization Across Dataset Lengths and Meta-Distributions.} For each subplot, the left panel illustrates the performance of meta-statistical models on test datasets that vary in input length, including lengths not observed during training, while remaining within the training meta-distribution. For each subplot, the right panel presents the same comparison but for test datasets sampled from entirely new meta-distributions, with distributions unseen during training. Note that LSTM is excluded because its errors are an order of magnitude higher.}
    \label{fig:generalization_app}
\end{figure}

\subsection{Details about Efficiency}
\label{app:eff}
In \Figref{fig:inference_time}, we present the inference time of meta-statistical models as a function of the input dataset size \( n \). As expected, the VT scales quadratically, whereas LSTM and ST2 variants scale linearly with slopes in favor of ST2.
We also compare the efficiency per parameter. For this we compute both the training and inference time of each model per batch averaged over 1K batches, and normalized by the number of parameters in the model. The results are reported in \Tabref{tab:times}. Given the strong performance of ST2 and the clear computational advantage we see it as strong meta-statistical architecture.

\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{images/inference_time_by_row_size.pdf}
    \caption{\textbf{Inference time} comparison of meta-statistical models per batch as a function of input dataset length. Models have similar parameter counts $\approx 10K$).}
    \label{fig:inference_time}
\end{figure}

\begin{table}[t]
\centering
 \begin{tabular}{@{}l|c|c@{}}
    \toprule
     & Training Time & Inference Time \\
    \midrule
    \midrule
    VT & $2.2e^{-5} $ \scriptsize{$\pm 1.1$} & $2.7e^{-3}$ \scriptsize{$\pm 1.2$} \\
    LSTM & $6.5e^{-6} $ \scriptsize{$\pm 1.0$} & $8.5e^{-4}$ \scriptsize{$\pm 3.9$} \\
    ST2(32) & $5.9e^{-6} $ \scriptsize{$\pm 6.9$} & $1.5e^{-3}$ \scriptsize{$\pm 2.4$} \\
    ST2(16) & $1.7e^{-6} $ \scriptsize{$\pm 0.2$} & $2.2e^{-4}$ \scriptsize{$\pm 0.9$} \\
    \bottomrule
\end{tabular}
\caption{Comparison of training and inference times for various models, normalized per batch per number of parameters.}
\label{tab:times}
\end{table}

        
        
% \begin{figure}[ht]
%     \centering
%     % Second figure
%     \begin{minipage}[c]{0.53\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/inference_time_by_row_size.pdf}
%         \caption{Comparison of inference time per batch across input dataset length (models have similar parameter counts $\approx 10K$).}
%     \end{minipage}%
%     \hfill
%     % Table
%     \begin{minipage}[c]{0.43\textwidth}
%         \centering  
%         \resizebox{\textwidth}{!}{
           
%     \end{minipage}
% \end{figure}



\section{Details about Standard Deviation Experiments}
\label{app:std}


\subsection{Details of Meta-Dataset Creation}
We construct a meta-dataset by generating datasets labeled with the ground truth standard deviation, using a set of distributions for which the standard deviation is well-defined. To create each meta-datapoint, we first sample the base distribution uniformly at random from a set of pre-defined distribution families (see below). Then, we sample the parameters of the distribution, resulting in a distribution \(P_X\). A dataset size \(n\) is then drawn uniformly at random from the range \([10, 150]\), and the dataset \(D\) is sampled with \(n\) rows. 
We generate 50K meta-datapoints for training and 3K for validation.

\xhdr{In-Meta-Distribution}
These are the distributions \textbf{seen} during training. The base distributions are the following, with the priors on their parameters:
\begin{itemize}
    \item \texttt{normal}: the mean is sampled from \(\mathcal{U}(-1, 1)\), and the variance is sampled from \(\mathcal{U}(0.5, 2.0)\). 
    \item \texttt{uniform}: the lower bound is sampled from \(\mathcal{U}(0, 0.5)\), and the upper bound is sampled from \(\mathcal{U}(0.5, 1.5)\).
    \item \texttt{exponential}: the scale parameter is sampled from \(\mathcal{U}(1, 2)\).
    \item \texttt{gamma}: the shape parameter is sampled from \(\mathcal{U}(1, 5)\), and the scale parameter is sampled from \(\mathcal{U}(1, 2)\).
\end{itemize}

\xhdr{Out-of-Meta-Distribution}
These are the distributions \textbf{seen} during training. The base distributions are the following, with the priors on their parameters:
\begin{itemize}
    \item \texttt{beta}: the \(\alpha\) parameter is sampled from \(\mathcal{U}(1, 5)\), and the \(\beta\) parameter is sampled from \(\mathcal{U}(1, 5)\).
    \item \texttt{lognormal}: the mean of the underlying normal distribution is sampled from \(\mathcal{U}(0, 1)\), and the standard deviation from \(\mathcal{U}(0.1, 1)\).
    \item \texttt{weibull}: the shape parameter is sampled from \(\mathcal{U}(1, 5)\), and the scale parameter is sampled from \(\mathcal{U}(1, 2)\).
\end{itemize}


\subsection{Details about Meta-Statistical Models}
For these experiments, we train two meta-statistical models based on \textbf{Set Transformer 2 (ST2)}. Variants with different numbers of inducing points were tested, such as \texttt{ST2(16)}, which uses 16 inducing points (\texttt{num\_inds = 16}).
\begin{itemize}
    \item ST2$_{\text{std}}$: an \texttt{ST2(16)} encoder with a regression MLP trained to predict the standard deviation \(\sigma_{P_X}\) of the distribution.
    \item ST2$_{\text{fsd}}$: an \texttt{ST2(16)} encoder with a regression MLP trained to predict the finite sample error made by the sample standard deviation, i.e., it predicts \(y = \sigma_{P_X} - \text{np.std}(X)\).
\end{itemize}

This design probes whether meta-statistical models can reliably estimate finite sampling errors from data, offering insights into their expected performance. We can craft a new standard deviation estimator by combining the sample standard deviation (\texttt{np.std}) with the corrections predicted by ST2$_{\text{fsd}}$. Both ST2-based models use the same architecture, consisting of 16 inducing points, five hidden layers with 128 dimensions, and 12 attention heads per layer, resulting in approximately 960K parameters. The prediction head is implemented as an MLP with a single hidden layer of 64 units.

\xhdr{Training Configuration} 
The models were trained on a regression task using the binary MSE loss. We employed a batch size of 64 and optimized the model parameters using the Adam optimizer with a learning rate of \(1 \times 10^{-5}\). The training process spanned 10 epochs. 


\subsection{Bias and Variance}
In \Figref{fig:std_bias_var}, we report an experiment targeted at measuring bias and variance by resampling 150 datasets from a fixed \texttt{exponential} distribution. Let the true standard deviation be denoted as \( \sigma \) and the estimates for the \( i\)-th dataset be \( \hat{\sigma}_i \), for \( i = 1, \dots, 150 \).

The bias of the estimator is computed as:
\begin{equation}
\text{Bias} = \frac{1}{n} \sum_{i=1}^{n} \hat{\sigma}_i - \sigma,
\end{equation}
where \( n = 50 \) is the number of resampled datasets.

The variance of the estimator is computed as:
\begin{equation}
\text{Variance} = \frac{1}{n} \sum_{i=1}^{n} \left( \hat{\sigma}_i - \frac{1}{n} \sum_{j=1}^{n} \hat{\sigma}_j \right)^2.
\end{equation}
It shows that the meta-statistical models can improve over the sample standard deviation \texttt{np.std} both in terms of bias and variance. Especially, the learned correction is capable of reducing the bias of \texttt{np.std} indicating that it has not just learned a constant offset.
We also report bias, variance, and MSE across dataset sizes and over many meta-datapoints sampled from various distributions (both in- and out-of-meta-distribution) in \Tabref{tab:imd_std_bias_variance}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{images/std.pdf}
    \caption{Estimate statistics for standard deviation estimators over 150 resampled datasets of size $n \in [10, 50]$ for the \texttt{exponential} distributions. Each dot represents the difference between an estimate and the true standard deviation. An unbiased estimator should be centered around zero.}
    \label{fig:std_bias_var}
\end{figure}



\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}l|ccc|ccc|ccc@{}}
\toprule
& \multicolumn{3}{c|}{Bias} & \multicolumn{3}{c}{Variance} & \multicolumn{3}{c}{MSE} \\ 
% \cmidrule(lr){2-4} \cmidrule(lr){5-7}
& $n \in [10, 50]$ & $n \in [50, 100]$ & $n \in [100, 150]$ & $n \in [10, 50]$ & $n \in [50, 100]$ & $n \in [100, 150]$ & $n \in [10, 50]$ & $n \in [50, 100]$ & $n \in [100, 150]$ \\ 
\midrule
\midrule
\texttt{np.std}
& $-3.4e^{-2}$ \scriptsize{$\pm 2.3$}
& $-2.5e^{-2}$ \scriptsize{$\pm 1.6$}
& $-3.3e^{-3}$ \scriptsize{$\pm 14.3$}
% ----
& $1.1e^{-1}$ \scriptsize{$\pm 0.3$}
& $3.5e^{-2}$ \scriptsize{$\pm 0.9$}
& $1.7e^{-2}$ \scriptsize{$\pm 0.4$}
% ---
& $1.2e^{-1}$ \scriptsize{$\pm 0.2$}
& $4.3e^{-2}$ \scriptsize{$\pm 0.6$}
& $2.2e^{-2}$ \scriptsize{$\pm 0.3$}
\\
ST2$_{std}$
& $-8.1e^{-3}$ \scriptsize{$\pm 27.7$}
& $1.9e^{-2}$ \scriptsize{$\pm 2.1$}
& $1.8e^{-2}$ \scriptsize{$\pm 1.4$}
% ----
& $1.6e^{-2}$ \scriptsize{$\pm 0.3$}
& $1.1e^{-2}$ \scriptsize{$\pm 0.2$}
& $6.4e^{-3}$ \scriptsize{$\pm 1.2$}
% ----
& $3.7e^{-2}$ \scriptsize{$\pm 0.4$}
& $2.2e^{-2}$ \scriptsize{$\pm 0.3$}
& $1.1e^{-2}$ \scriptsize{$\pm 0.1$}
\\
\texttt{np.std} + ST2$_{fsd}$
& $-5.4e^{-3}$ \scriptsize{$\pm 28.6$}
& $-1.5e^{-2}$ \scriptsize{$\pm 2.2$}
& $-1.1e^{-2}$ \scriptsize{$\pm 1.4$}
% ---
& $3.6e^{-2}$ \scriptsize{$\pm 1.0$}
& $1.5e^{-2}$ \scriptsize{$\pm 0.4$}
& $8.4e^{-3}$ \scriptsize{$\pm 1.8$}
% ---
& $5.7e^{-2}$ \scriptsize{$\pm 0.8$}
& $2.5e^{-2}$ \scriptsize{$\pm 0.3$}
& $1.4e^{-2}$ \scriptsize{$\pm 0.2$}
\\
\midrule
\midrule
\texttt{np.std}
& $-5.6e^{-2}$ \scriptsize{$\pm 2.9$}
& $-3.7e^{-2}$ \scriptsize{$\pm 2.1$}
& $-1.4e^{-2}$ \scriptsize{$\pm 1.6$}
% ----
& $1.7e^{-1}$ \scriptsize{$\pm 1.1$}
& $7.9e^{-2}$ \scriptsize{$\pm 4.9$}
& $1.9e^{-2}$ \scriptsize{$\pm 1.0$}
% ---
& $2.0e^{-1}$ \scriptsize{$\pm 0.8$}
& $9.1e^{-2}$ \scriptsize{$\pm 4.3$}
& $2.6e^{-2}$ \scriptsize{$\pm 0.7$}
\\
ST2$_{std}$
& $7.1e^{-2}$ \scriptsize{$\pm 9.9$}
& $-2.1e^{-2}$ \scriptsize{$\pm 6.5$}
& $1.2e^{-2}$ \scriptsize{$\pm 5.2$}
% ----
& $1.3e^{-2}$ \scriptsize{$\pm 0.3$}
& $8.0e^{-3}$ \scriptsize{$\pm 1.9$}
& $5.0e^{-3}$ \scriptsize{$\pm 1.6$}
% ----
& $2.7e^{-1}$ \scriptsize{$\pm 0.3$}
& $1.1e^{-1}$ \scriptsize{$\pm 0.2$}
& $7.9e^{-2}$ \scriptsize{$\pm 1.5$}
\\
\texttt{np.std} + ST2$_{fsd}$
& $4.9e^{-2}$ \scriptsize{$\pm 6.0$}
& $-4.8e^{-2}$ \scriptsize{$\pm 4.3$}
& $-3.2e^{-2}$ \scriptsize{$\pm 3.4$}
% ---
& $9.1e^{-2}$ \scriptsize{$\pm 8.3$}
& $4.8e^{-2}$ \scriptsize{$\pm 4.0$}
& $1.0e^{-2}$ \scriptsize{$\pm 0.6$}
% ---
& $1.9e^{-1}$ \scriptsize{$\pm 0.5$}
& $9.8e^{-2}$ \scriptsize{$\pm 4.2$}
& $4.0e^{-2}$ \scriptsize{$\pm 1.1$}
\\
\bottomrule

\end{tabular}
}
\caption{\textbf{Bias and Variance of Standard Deviation Estimators} The top part shows the in-meta-distribution results, while the bottom part reports the out-of-meta-distribution results.}
\label{tab:imd_std_bias_variance}
\end{table*}

\subsection{Interesting Failure Case}
\label{app:failure}
While meta-statistical models generally demonstrate strong robustness when presented with datasets sampled from distribution unseed during training (OoMD), we found a interesting failure case. For the log-normal family that was unseen during training, meta-statistical estimators of standard deviations failed to provide improvements over \texttt{np.std} and even performed worse. This is illustrated in \Figref{fig:std_line_plot_failure} across dataset sizes. Log-normal is a skewed distribution which is particularly challenging estimators of standard deviation which can explain why meta-statistical model poorly generalize in this case. These models can benefit from a larger and more diverse training meta-dataset to exhibit even more robust generalization. Note, however, that the models do not fail for other unseen distribution families as it can be seen in \Figref{fig:std_line_plot} of the main paper.


\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{images/std_line_plot_lognormal.pdf}
    \caption{MSE of $\sigma$ estimators as a function of dataset sizes, for the log-normal distribution.}
    \label{fig:std_line_plot_failure}
\end{figure}



\section{Details about Normality Tests Experiments}
\label{app:norm_test}

\subsection{Details of Meta-Dataset Creation}
We construct a meta-dataset by generating datasets labeled with the ground truth binary indicator of normality, using a diverse set of alternative distributions. In previous studies, the uniform distribution was used the contrast distribution \citet{razali2011power}. To create each meta-datapoint, we first determine whether the dataset is sampled from a normal distribution by flipping a fair coin. If the outcome is normal, we sample the mean and variance of the Gaussian distribution; otherwise, we sample uniformly at random one distribution from the set of non-Gaussian distributions and then draw its parameters.. A dataset size \( n \) is then drawn uniformly at random from the range \( [5, 150] \), and the dataset \( D \) is sampled with \( n \) rows. We generate 40K meta-datapoints for training, 1K for validation, and 1K meta-datapoints for testing.

The normal distribution parameters are sampled according to: the mean is sampled from $\mathcal{U}(-3, 3)$, and the standard deviation from $\mathcal{U}(0.5, 3)$.

\xhdr{In-Meta-Distribution}
These are the distributions \textbf{seen} during training as non-normal distributions. Not however that because the distribution sampled parameters and the datasets are then sampled from the distributions, in-meta-distribution evaluation still produces different meta-datapoints.
\begin{itemize}
    \item \texttt{gamma}: the shape parameter is sampled from $\mathcal{U}(1, 5)$, and the scale parameter from $\mathcal{U}(0.5, 2)$.
    \item \texttt{triangular}: the lower bound is sampled from $\mathcal{U}(-3, 0)$, the mode from $\mathcal{U}(\text{lower bound}, 3)$, and the upper bound from $\mathcal{U}(\text{mode}, 5)$.
    \item \texttt{cauchy}: the location parameter is sampled from $\mathcal{U}(-1, 1)$, and the scale parameter from $\mathcal{U}(0.5, 2)$.
    \item \texttt{laplace}: the location parameter is sampled from $\mathcal{U}(-1, 1)$, and the scale parameter from $\mathcal{U}(0.5, 2)$.
    \item \texttt{weibull}: the shape parameter is sampled from $\mathcal{U}(0.5, 5)$.
    \item \texttt{vonmises}: the mean direction $\mu$ is sampled from $\mathcal{U}(-\pi, \pi)$, and the concentration parameter $\kappa$ from $\mathcal{U}(0.5, 5)$.
    \item \texttt{arcsine}: the lower bound is sampled from $\mathcal{U}(-3, 0)$, and the upper bound from $\mathcal{U}(0, 3)$. Data is generated by transforming uniform samples with a sine function scaled to the specified bounds.
    \item \texttt{bimodal}: two Gaussian components are used, where the means of the components are sampled from $\mathcal{U}(-3, -1)$ and $\mathcal{U}(1, 3)$, respectively. The standard deviations are sampled from $\mathcal{U}(0.5, 1)$, and the mixture ratio is sampled from $\mathcal{U}(0.3, 0.7)$.
\end{itemize}

\xhdr{Out-of-Meta-Distribution}
These are the distributions \textbf{unseen} during training as non-normal distributions.
\begin{itemize}
    \item \texttt{uniform}: the lower bound is sampled from $\mathcal{U}(-3, 0)$, and the upper bound from $\mathcal{U}(0, 3)$.
    \item \texttt{exponential}: the scale parameter is sampled from $\mathcal{U}(0.5, 2)$.
    \item \texttt{beta}: the shape parameters $a$ and $b$ are sampled independently from $\mathcal{U}(0.5, 5)$.
    \item \texttt{log-normal}: the mean is sampled from $\mathcal{U}(-1, 1)$, and the standard deviation from $\mathcal{U}(0.5, 1.5)$.
\end{itemize}

\subsection{Details of Meta-Statistical Models}
The experiments compared the performance of multiple model variants, including:
\begin{itemize}
    \item \textbf{Vanilla Transformer (VT)}: This model utilized the \texttt{vanilla\_transformer} architecture.
    \item \textbf{Set Transformer 2 (ST2)}: a set Transformer with 16 inducing points (\texttt{num\_inds = 16}).
\end{itemize}
Both models have four layers, hidden dimensionality of 32, and 12 attention heads. To build a full meta-statistical model from the meta-statistical encoders, we add a prediction head made of a MLP with one layer of 32 neurons before predicting the probability of being normally distributed. The meta-model based on VT has a total of $51K$ parameters, and the meta-model based on ST2 has a total of $54K$. ST2 has more parameters to learn the projected attention but ends being much faster to train and use at inference because of the constant cost of attention compared to quadratic in the length of the input for VT.

\xhdr{Training Configuration} 
The models were trained on a binary classification task using the binary cross-entropy loss. We employed a batch size of 24 and optimized the model parameters using the Adam optimizer with a learning rate of 0.0005. The training process spanned 7 epochs. These hyper-parameters were selected to balance computational efficiency and convergence stability. In the main paper \Figref{fig:norm_t_oomd}, the meta-statistical models are evaluated OoMD. For completeness, we report their results on unseen test set but in-meta-distribution in \Tabref{fig:norm_t_imd}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{images/norm_test_imd.pdf}
    \caption{Accuracy of meta-statistical models compared to standard normality tests converted into classifiers using optimized \( p \)-value thresholds. The non-normal distributions are sampled in-meta-distribution for meta-statistical models.}
    \label{fig:norm_t_imd}
\end{figure}


\subsection{Details about Precision and Recall}
\label{app:prec_rec}
While classification lacks a direct bias-variance formulation, we analyze false positive and false negative rates as well as precision and recall in \Tabref{tab:prec_rec}. Interestingly, most baselines, when optimizing their p-value threshold for maximum accuracy end up maximizing recall at the expense of precision with high false positive rates. On the contrary, the meta-statistical models perform well in both precision and recall with balanced error profiles, i.e., similar amount of false negatives and false positives.

\begin{table*}[t]
\centering
\resizebox{0.75\textwidth}{!}{
\begin{tabular}{@{}l|cccc|ccc@{}}
\toprule
& TP & FP & TN & FN & Prec. & Recall & F1-Score \\ 
\midrule
\midrule
Shapiro-Wilk
& 439 & 138 & 383 & 40 & 0.7608 & 0.9165 & 0.8314 \\

Kolmogorov-Smirnov
& 479 & 364 & 157 & 0 & 0.5682 & 1.0000 & 0.7247 \\

D'Agostino and Pearson
& 423 & 147 & 374 & 56 & 0.7421 & 0.8831 & 0.8065 \\

Jarque Bera
& 449 & 296 & 225 & 30 & 0.6027 & 0.9374 & 0.7337 \\

VT
& 471  & 38 & 441 & 50 & 0.9253 & 0.9040 & 0.9146 \\

ST2(16)
& 474 & 50 & 429 & 47 & 0.9046 & 0.9098 & 0.9072 \\
\bottomrule
\end{tabular}
}
\caption{Reporting True Positive (TP), False Positive (FP), True Negative (TN), False Negative (FN), the associated Precision, Recall, and F1-Score of normality classifers over 1,000 datasets of size $n=50$.}
\label{tab:prec_rec}
\end{table*}




\section{Details about Mutual Information Experiments}
\label{app:mi_details}

\subsection{Details of Meta-Dataset Creation}
We construct a meta-dataset inspired by the benchmark methodology in \cite{NEURIPS2023_36b80eae}, where distributions with ground-truth MI are generated in two steps: (i) by sampling a distribution with known MI, (ii) potentially applying MI-preserving transformation. This process creates complex distributions and datasets with known MI. For generating meta-dataset in this way, we again follow the process described in \Secref{sec:experimental_setup} using different base-distribution and MI-preserving transformation between in-meta-distribution and out-of-meta-distribution. 

% \paragraph{Distributions}
% The meta-dataset leverages two primary types of distributions:
% \begin{enumerate}
%     \item \textbf{In-Meta-Distribution}: combination of distribution and transformation used during training, inspired by the benchmark v1 in the BMI dataset.
%     \item \textbf{Out-of-Meta-Distribution}: Additional combinations of distributions and transformation used to evaluate generalization to unseen data types.
% \end{enumerate}

\xhdr{In-Meta-Distribution}
The in-meta-distributions focus on bivariate relationships and transformations:
\begin{itemize}
    \item \texttt{binormal-[base, wigglify, halfcube, asinh, normal\_cdfise]}: Standard bivariate normal distribution with correlation sampled from $\mathcal{U}(-1, 1)$, following by any of the MI-preserving transformation (or none).
    % \item \texttt{binormal-normal\_cdfise}: Applies a cumulative distribution function (CDF) transformation to a bivariate normal distribution with correlation sampled from $\mathcal{U}(-1, 1)$.
    % \item \texttt{binormal-wigglify}: Introduces non-linear distortions with correlation sampled from $\mathcal{U}(-1, 1)$.
    % \item \texttt{binormal-halfcube}: Maps data into a "half-cube" space with correlation sampled from $\mathcal{U}(-1, 1)$.
    % \item \texttt{additive\_noise-base}: Adds Gaussian noise with noise level $\epsilon$ sampled from $\mathcal{U}(0.01, 2)$.
    \item \texttt{bimodal\_gaussians-base}: Bivariate Gaussian mixture model with correlation sampled from $\mathcal{U}(-1, 1)$.
    \item \texttt{bistudent-[base, asinh]}: Bivariate Student's t-distribution with degrees of freedom sampled from $\mathcal{U}(1, 10)$, potentially followed by asinh transform.
    % \item \texttt{bistudent-asinh}: Applies an inverse hyperbolic sine transformation with degrees of freedom sampled from $\mathcal{U}(1, 10)$.
\end{itemize}

\xhdr{Out-of-Meta-Distribution}
The out-of-meta-distributions extend the variety of data by incorporating additional transformations and configurations:
\begin{itemize}
    \item \texttt{additive\_noise-[base, wigglify, halfcube, asinh, normal\_cdfise]}: A model where $X$ is sample from  $\mathcal{U}(-1, 1)$ and $Y = X + \epsilon$, a Gaussian noise sampled from $\mathcal{U}(0.01, 2)$) followed by any of the MI-preserving transformation.
    % \item \texttt{additive\_noise-wigglify}: Gaussian noise ($\epsilon$ sampled from $\mathcal{U}(0.01, 2)$) followed by non-linear distortions.
    % \item \texttt{additive\_noise-halfcube}: Gaussian noise ($\epsilon$ sampled from $\mathcal{U}(0.01, 2)$) mapped to a "half-cube" space.
    % \item \texttt{additive\_noise-asinh}: Gaussian noise ($\epsilon$ sampled from $\mathcal{U}(0.01, 2)$) with an inverse hyperbolic sine transformation.
    \item \texttt{bistudent-[wigglify, halfcube, normal\_cdfise]}: Bivariate Student's t-distribution (degrees of freedom sampled from $\mathcal{U}(1, 10)$) followed by any of the MI-preserving transformation except asinh.
    % \item \texttt{bistudent-wigglify}: Bivariate Student's t-distribution (degrees of freedom sampled from $\mathcal{U}(1, 10)$) with non-linear distortions.
    % \item \texttt{bistudent-halfcube}: Bivariate Student's t-distribution (degrees of freedom sampled from $\mathcal{U}(1, 10)$) mapped to a "half-cube" space.
    % \item \texttt{bistudent-asinh}: Bivariate Student's t-distribution (degrees of freedom sampled from $\mathcal{U}(1, 10)$) with an inverse hyperbolic sine transformation.
\end{itemize}

% \xhdr{Dataset Generation Process}
% For each dataset, a base distribution is selected randomly from the specified set of distributions. Depending on the chosen base distribution, parameters such as correlation, noise level, or degrees of freedom. Transformations such as \texttt{normal\_cdfise}, \texttt{wigglify}, or \texttt{half\_cube} are applied as per the specified configuration. The generator outputs the dataset alongside its ground-truth mutual information.

For the computation of the true MI from these distributions and their transformations, we refer to \cite{NEURIPS2023_36b80eae}; we sample using the tool they provided: \url{https://github.com/cbg-ethz/bmi}.

\subsection{Details of the Training of Meta-Statistical Models}

The meta-statistical models were trained using the code framework outlined in the previous section. This section provides detailed information about the training process.

\xhdr{Default Configuration}
The default configuration specifies the following key components:
\begin{itemize}
    \item \textbf{Dimensionality}: In these experiments, we focus on 1D dimensional variables.
    \item \textbf{Dataset Parameters}: Dataset sizes are sampled uniformly from the range [10, 150].
    \item \textbf{Meta-Dataset Properties}: The meta-dataset contains 50,000 training, 500 validation, and 1,000 testing meta-datapoints.
    \item \textbf{Training Parameters}: A regression task was specified with a batch size of 64, learning rate of 0.0001, and 20 epochs of training.
    \item \textbf{Model Architecture}: The base model used a 5-layer encoder (\texttt{n\_enc\_layers}), with hidden dimensions of 256 and 128 for the \texttt{phi} and \texttt{theta} components, respectively. Multi-head attention mechanisms used 12 heads.
\end{itemize}

\xhdr{Model Variants}
The experiments compared the performance of multiple model variants, including:
\begin{itemize}
     \item \textbf{Vanilla Transformer (VT)}: This model utilized the \texttt{vanilla\_transformer} architecture.
    \item \textbf{Set Transformer 2 (ST2)}: a set Transformer with 16 inducing points (\texttt{num\_inds = 16}).
\end{itemize}
To build a full meta-statistical model from the meta-statistical encoders, we add a prediction head made of a MLP with one layer of 128 neurons before predicting the 1 number target output. The meta-model based on VT has a total of $1,008,385$ parameters, and the meta-model based on ST2 has a total of $1,280,065$. ST2 has more parameters to learn the projected attention but ends being much faster to train and use at inference because of the constant cost of attention compared to quadratic in the length of the input for VT.

\subsection{Details about the Bias and Variance of MI Estimators}
To estimate the bias and variance of a mutual information (MI) estimator, we follow a systematic procedure. First, we sample a base distribution and an MI-preserving transformation from the out-of-meta-distribution set. From this fixed setup, we resample \( n = 50 \) datasets, leading to 50 MI estimates from the given estimator. Let the true mutual information be denoted as \( I_\text{true} \) and the MI estimates for the \( i\)-th dataset be \( \hat{I}_i \), for \( i = 1, \dots, 50 \).

The bias of the estimator is computed as:
\begin{equation}
\text{Bias} = \frac{1}{n} \sum_{i=1}^{n} \hat{I}_i - I_\text{true},
\end{equation}
where \( n = 50 \) is the number of resampled datasets.

The variance of the estimator is computed as:
\begin{equation}
\text{Variance} = \frac{1}{n} \sum_{i=1}^{n} \left( \hat{I}_i - \frac{1}{n} \sum_{j=1}^{n} \hat{I}_j \right)^2.
\end{equation}

To capture a broader picture of estimator behavior, we repeat this process for 100 random choices of base distributions and transformations. For each random choice, we report the bias and variance of the estimator as calculated above. Finally, we summarize these results across different sample sizes in \Tabref{tab:mi_bias_variance}.


\begin{table*}[t]
\centering
\resizebox{0.75\textwidth}{!}{
\begin{tabular}{@{}l|ccc|ccc@{}}
\toprule
& \multicolumn{3}{c|}{Bias} & \multicolumn{3}{c}{Variance} \\ 
% \cmidrule(lr){2-4} \cmidrule(lr){5-7}
$n \in$ & $[10, 50]$ & $[50, 100]$ & $[100, 150]$ & $[10, 50]$ & $[50, 100]$ & $[100, 150]$ \\ 
\midrule
\midrule
CCA
& $-5.0e^{-2}$ \scriptsize{$\pm 3.7$}
& $-1.5e^{-2}$ \scriptsize{$\pm 1.2$}
& $-3.7e^{-2}$ \scriptsize{$\pm 1.8$}
% ----
& $8.6e^{-3}$ \scriptsize{$\pm 3.3$}
& $3.5e^{-3}$ \scriptsize{$\pm 1.2$}
& $1.3e^{-3}$ \scriptsize{$\pm 0.5$}\\
KSG
& $-1.0e^{-1}$ \scriptsize{$\pm 0.6$}
& $-2.6e^{-2}$ \scriptsize{$\pm 2.9$}
& $-2.9e^{-2}$ \scriptsize{$\pm 1.2$}
% ----
& $5.6e^{-3}$ \scriptsize{$\pm 1.5$}
& $5.6e^{-3}$ \scriptsize{$\pm 1.7$}
& $1.8e^{-3}$ \scriptsize{$\pm 0.7$}\\
VT
& $4.4e^{-3}$ \scriptsize{$\pm 10.8$}
& $4.0e^{-3}$ \scriptsize{$\pm 9.7$}
& $-7.8e^{-4}$ \scriptsize{$\pm 57.2$}
% ---
& $9.8e^{-3}$ \scriptsize{$\pm 7.1$}
& $3.1e^{-3}$ \scriptsize{$\pm 1.2$}
& $1.1e^{-3}$ \scriptsize{$\pm 0.5$}\\
ST2(16)
& $1.3e^{-2}$ \scriptsize{$\pm 1.0$}
& $9.8e^{-3}$ \scriptsize{$\pm 9.8$}
& $7.5e^{-4}$ \scriptsize{$\pm 60.7$}
% ---
& $7.7e^{-3}$ \scriptsize{$\pm 4.5$}
& $3.6e^{-3}$ \scriptsize{$\pm 1.4$}
& $9.8e^{-4}$ \scriptsize{$\pm 3.9$}\\
\bottomrule
\end{tabular}
}
\caption{\textbf{Bias and Variance of Mutual Information Estimators}}
\label{tab:mi_bias_variance}
\end{table*}










% \section*{Mean Squared Error (MSE): Frequentist and Bayesian Perspectives}

% \subsection*{1. Frequentist Bounds for $\text{MSE}(\hat{\theta})$}

% In the frequentist setting, the Mean Squared Error (MSE) of an estimator $\hat{\theta}$ is defined as:
% \[
% \text{MSE}(\hat{\theta}) = \mathbb{E}[(\hat{\theta} - \theta)^2] = \text{Bias}(\hat{\theta})^2 + \text{Var}(\hat{\theta}).
% \]

% \subsubsection*{1.1 Cramér-Rao Bound (Variance Term)}

% For an unbiased estimator, the variance term is bounded by the \textbf{Cramér-Rao Bound}:
% \[
% \text{Var}(\hat{\theta}) \geq \frac{1}{\mathcal{I}(\theta)},
% \]
% where $\mathcal{I}(\theta)$ is the \textbf{Fisher Information}:
% \[
% \mathcal{I}(\theta) = \mathbb{E} \left[ \left( \frac{\partial}{\partial \theta} \log P(X | \theta) \right)^2 \right].
% \]

% \subsubsection*{1.2 Bias Bounds}

% While variance is well-bounded by the Fisher Information, bounding the bias directly is more challenging. However:
% \begin{itemize}
%     \item For small sample sizes, the bias depends on higher-order moments of the likelihood. Approximation expansions, such as the \textbf{Edgeworth Expansion}, can quantify bias in terms of the sample size $n$.
%     \item For asymptotically consistent estimators, the bias reduces as:
%     \[
%     \text{Bias}(\hat{\theta}) \sim \frac{1}{n}.
%     \]
% \end{itemize}

% \subsubsection*{1.3 Overall MSE}

% Combining the bias and variance terms:
% \[
% \text{MSE}(\hat{\theta}) \geq \frac{1}{\mathcal{I}(\theta)} + \text{Bias}(\hat{\theta})^2.
% \]
% For small $n$, the bias term may dominate, while for large $n$, the variance term dominates.

% \subsection*{2. Bayesian Bounds for $\text{MSE}(\hat{\theta})$}

% In the Bayesian setting, we compute the MSE of an estimator $\hat{\theta}$ with respect to the posterior distribution $P(\theta | D_X)$:
% \[
% \text{MSE}_{\text{Bayes}}(\hat{\theta}) = \mathbb{E}_{P(\theta | D_X)}[(\hat{\theta} - \theta)^2].
% \]

% \subsubsection*{2.1 Decomposition of Bayesian MSE}

% The Bayesian MSE can be decomposed into prior, likelihood, and sample size contributions. Let:
% \begin{itemize}
%     \item $\tau^2 = \text{Var}(\theta)$: Variance of the prior,
%     \item $\sigma^2 = \text{Var}(X | \theta)$: Intrinsic variance of the likelihood,
%     \item $n$: Sample size.
% \end{itemize}

% The posterior variance is given by:
% \[
% \text{Var}(\theta | D_X) = \left( \frac{1}{\tau^2} + \frac{n}{\sigma^2} \right)^{-1}.
% \]

% The Bayesian MSE has contributions from:
% \begin{itemize}
%     \item \textbf{Bias Term} (from prior mean):
%     \[
%     \text{Bias}(\hat{\theta}_{\text{Bayes}})^2 = \left( \frac{\tau^2}{\tau^2 + n \sigma^2} (\mu_0 - \theta) \right)^2,
%     \]
%     where $\mu_0$ is the prior mean.
%     \item \textbf{Variance Term} (from posterior variance):
%     \[
%     \text{Var}(\hat{\theta}_{\text{Bayes}}) = \frac{\sigma^2}{n} \cdot \frac{\tau^2}{\tau^2 + n \sigma^2}.
%     \]
% \end{itemize}

% \subsubsection*{2.2 Bayesian Posterior MSE Bound}

% Combining these:
% \[
% \text{MSE}_{\text{Bayes}}(\hat{\theta}) = \underbrace{\left( \frac{\tau^2}{\tau^2 + n \sigma^2} (\mu_0 - \theta) \right)^2}_{\text{Bias Term}} + \underbrace{\frac{\sigma^2}{n} \cdot \frac{\tau^2}{\tau^2 + n \sigma^2}}_{\text{Variance Term}}.
% \]

% \subsubsection*{2.3 Asymptotic Behavior of Bayesian MSE}

% \begin{itemize}
%     \item As $n \to \infty$, the prior influence vanishes, and the Bayesian MSE approaches the frequentist MSE:
%     \[
%     \text{MSE}_{\text{Bayes}}(\hat{\theta}) \to \frac{\sigma^2}{n}.
%     \]
%     \item For small $n$, the prior dominates, and the MSE depends strongly on the prior variance $\tau^2$ and mean $\mu_0$.
% \end{itemize}

% \subsection*{3. MLE in the Bayesian Perspective}

% Maximum Likelihood Estimation (MLE) corresponds to Bayesian inference with a \textbf{flat prior}:
% \[
% P(\theta) \propto 1.
% \]
% This implies:
% \begin{itemize}
%     \item The prior contributes no additional information ($\tau^2 \to \infty$).
%     \item The posterior variance is entirely determined by the likelihood:
%     \[
%     \text{Var}(\hat{\theta}_{\text{MLE}}) \sim \frac{\sigma^2}{n}.
%     \]
% \end{itemize}

% \subsubsection*{Bias of MLE}
% MLE is asymptotically unbiased ($\text{Bias}(\hat{\theta}_{\text{MLE}}) \to 0$ as $n \to \infty$), but for small $n$, bias can exist due to the shape of the likelihood function. The bias is generally:
% \[
% \text{Bias}(\hat{\theta}_{\text{MLE}}) \sim \mathcal{O}\left(\frac{1}{n}\right),
% \]
% depending on higher-order derivatives of the likelihood.


\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}l|ccc|ccc|ccc@{}}
\toprule
& \multicolumn{3}{c|}{Bias} & \multicolumn{3}{c}{Variance} & \multicolumn{3}{c}{MSE} \\ 
% \cmidrule(lr){2-4} \cmidrule(lr){5-7}
& $n \in [10, 50]$ & $n \in [50, 100]$ & $n \in [100, 150]$ & $n \in [10, 50]$ & $n \in [50, 100]$ & $n \in [100, 150]$ & $n \in [10, 50]$ & $n \in [50, 100]$ & $n \in [100, 150]$ \\ 
\midrule
\midrule
\texttt{np.std}
& $-1.0e^{-2}$ \scriptsize{$\pm 1.1$}
& $1.3e^{-2}$ \scriptsize{$\pm 0.2$}
& $-2.2e^{-2}$ \scriptsize{$\pm 0.7$}
% ----
& $1.9e^{-2}$ \scriptsize{$\pm 0.4$}
& $8.1e^{-3}$ \scriptsize{$\pm 1.6$}
& $3.8e^{-3}$ \scriptsize{$\pm 0.6$}
% ---
& $2.0e^{-2}$ \scriptsize{$\pm 0.2$}
& $8.2e^{-3}$ \scriptsize{$\pm 0.6$}
& $3.9e^{-3}$ \scriptsize{$\pm 0.3$}
\\
ST2$_{std}$
& $-1.2e^{-1}$ \scriptsize{$\pm 0.8$}
& $2.2e^{-3}$ \scriptsize{$\pm 9.7$}
& $-2.4e^{-2}$ \scriptsize{$\pm 0.9$}
% ----
& $8.6e^{-3}$ \scriptsize{$\pm 1.3$}
& $5.1e^{-3}$ \scriptsize{$\pm 0.6$}
& $3.4e^{-3}$ \scriptsize{$\pm 0.5$}
% ----
& $1.5e^{-2}$ \scriptsize{$\pm 0.1$}
& $7.5e^{-3}$ \scriptsize{$\pm 0.5$}
& $3.9e^{-3}$ \scriptsize{$\pm 0.3$}
\\
ST2$_{np.std}$ +ST2$_{fsd}$
& $2.0e^{-2}$ \scriptsize{$\pm 0.8$}
& $4.5e^{-2}$ \scriptsize{$\pm 1.6$}
& $9.2e^{-3}$ \scriptsize{$\pm 8.5$}
% ---
& $8.9e^{-3}$ \scriptsize{$\pm 1.6$}
& $5.7e^{-3}$ \scriptsize{$\pm 1.0$}
& $3.7e^{-3}$ \scriptsize{$\pm 0.6$}
% ---
& $1.4e^{-2}$ \scriptsize{$\pm 0.1$}
& $6.9e^{-3}$ \scriptsize{$\pm 0.5$}
& $3.9e^{-3}$ \scriptsize{$\pm 0.3$}
\\
\texttt{np.std} + ST2$_{fsd}$
& $1.4e^{-3}$ \scriptsize{$\pm 12.0$}
& $3.4e^{-2}$ \scriptsize{$\pm 0.7$}
& $4.7e^{-3}$ \scriptsize{$\pm 4.6$}
% ---
& $1.0e^{-2}$ \scriptsize{$\pm 0.1$}
& $5.8e^{-3}$ \scriptsize{$\pm 0.7$}
& $3.6e^{-3}$ \scriptsize{$\pm 0.5$}
% ---
& $1.4e^{-2}$ \scriptsize{$\pm 0.1$}
& $6.9e^{-3}$ \scriptsize{$\pm 0.5$}
& $3.6e^{-3}$ \scriptsize{$\pm 0.3$}
\\
\bottomrule
\end{tabular}
}
\caption{\textbf{Bias and Variance of Standard Deviation Estimators}}
\label{tab:std_bias_variance}
\end{table*}


% \section*{Bias, Variance, and Mean Squared Error in Statistical Learning}

% When transitioning to the context of statistical learning or machine learning, where the estimator is learned from data (e.g., a parameterized model trained using optimization techniques), the properties of bias, variance, and mean squared error (MSE) evolve significantly. Below, we explore these aspects mathematically.

% \subsection*{1. Bias and Variance in Machine Learning}

% In machine learning, the bias-variance decomposition applies to the \emph{expected prediction error} of a learned estimator. Suppose we have:
% \begin{itemize}
%     \item A dataset $D_X \sim P_X$ with $n$ samples.
%     \item A hypothesis class $\mathcal{H}$ of functions $h: \mathcal{X} \to \mathcal{Y}$ parameterized by $\theta$.
%     \item A loss function $\ell(h(x), y)$, such as the squared loss.
% \end{itemize}

% The expected MSE for a learned model $h_{\hat{\theta}}$ at a point $x$ is:
% \[
% \mathbb{E}_{D_X}\big[(h_{\hat{\theta}}(x) - y)^2\big] = \underbrace{\big(\mathbb{E}_{D_X}[h_{\hat{\theta}}(x)] - y\big)^2}_{\text{Bias}^2} + \underbrace{\mathbb{E}_{D_X}\big[(h_{\hat{\theta}}(x) - \mathbb{E}_{D_X}[h_{\hat{\theta}}(x)])^2\big]}_{\text{Variance}} + \underbrace{\sigma^2}_{\text{Irreducible Error}}.
% \]

% \subsubsection*{Key Changes in Bias and Variance}
% \begin{itemize}
%     \item \textbf{Bias:} The bias depends on the flexibility of the hypothesis class $\mathcal{H}$:
%     \begin{itemize}
%         \item If $\mathcal{H}$ is too small (underfitting), the model cannot approximate the true relationship, leading to high bias.
%         \item If $\mathcal{H}$ is sufficiently large, bias can be reduced, but sufficient data is required for generalization.
%     \end{itemize}
%     \item \textbf{Variance:} The variance depends on both the hypothesis class $\mathcal{H}$ and the learning algorithm:
%     \begin{itemize}
%         \item Overly flexible models (e.g., deep neural networks) can have high variance as they overfit the training data.
%         \item Regularization techniques (e.g., $L_2$ regularization, dropout) are used to reduce variance.
%     \end{itemize}
% \end{itemize}

% \subsection*{2. Bounds and Behavior in Statistical Learning}

% \subsubsection*{Generalization Error}
% In statistical learning, the MSE on unseen data (generalization error) is bounded in terms of:
% \begin{itemize}
%     \item \textbf{Capacity of the hypothesis class} $\mathcal{H}$:
%     Measures like VC dimension, Rademacher complexity, or covering numbers quantify the flexibility of $\mathcal{H}$.
%     \item \textbf{Sample size} $n$:
%     Larger sample sizes reduce generalization error by approximating the true distribution $P_X$ better.
% \end{itemize}

% For a hypothesis $h \in \mathcal{H}$, the generalization error is bounded as:
% \[
% \mathbb{E}_{D_X}\big[\ell(h(x), y)\big] \leq \mathbb{E}_{D_X}\big[\ell(h_{\text{true}}(x), y)\big] + \mathcal{O}\left(\sqrt{\frac{\text{Complexity}(\mathcal{H})}{n}}\right),
% \]
% where $\text{Complexity}(\mathcal{H})$ depends on the learning framework (e.g., VC dimension or Rademacher complexity).

% \subsubsection*{Bias and Variance Tradeoff}
% In learned estimators, the tradeoff depends heavily on the model choice and training process:
% \begin{itemize}
%     \item Low-bias estimators (e.g., deep neural networks) can overfit without proper regularization, leading to high variance.
%     \item High-bias estimators (e.g., linear regression) may fail to capture non-linear relationships.
% \end{itemize}

% \subsection*{3. Bayesian Learned Estimators}

% In the Bayesian learning paradigm, the estimator $h_{\hat{\theta}}$ is viewed as a posterior distribution over hypotheses:
% \[
% P(\theta | D_X) \propto P(D_X | \theta) P(\theta).
% \]

% \subsubsection*{Bias and Variance with a Prior}
% \begin{itemize}
%     \item \textbf{Bias:} The prior $P(\theta)$ introduces explicit bias:
%     \begin{itemize}
%         \item A strong prior (e.g., enforcing sparsity) can lead to higher bias but lower variance.
%         \item Weak or uninformative priors reduce prior bias, but the posterior becomes dominated by the likelihood.
%     \end{itemize}
%     \item \textbf{Variance:} Bayesian estimators tend to reduce variance by incorporating prior knowledge, especially in low-sample regimes:
%     \[
%     \text{Var}(\theta | D_X) = \left( \text{Var}_{\text{prior}}^{-1} + \text{Var}_{\text{likelihood}}^{-1} \right)^{-1}.
%     \]
% \end{itemize}

% \subsubsection*{Bayesian Generalization Error}
% The generalization error in Bayesian learning accounts for both the prior and the hypothesis class:
% \[
% \mathbb{E}_{P_X, P_{\text{posterior}}}\big[\ell(h(x), y)\big] \leq \mathbb{E}_{P_X}\big[\ell(h_{\text{true}}(x), y)\big] + \mathcal{O}\left(\sqrt{\frac{\text{Complexity}(\mathcal{H}) + \text{Prior Information}}{n}}\right).
% \]

% \subsection*{4. MLE in Machine Learning}

% In machine learning, Maximum Likelihood Estimation (MLE) is commonly used to estimate parameters:
% \[
% \hat{\theta}_{\text{MLE}} = \arg\max_\theta P(D_X | \theta).
% \]

% \subsubsection*{Bias of MLE}
% \begin{itemize}
%     \item For sufficiently large $n$, MLE is asymptotically unbiased.
%     \item For small $n$, MLE may exhibit bias, often depending on higher-order terms of the likelihood.
% \end{itemize}

% \subsubsection*{Variance of MLE}
% The variance of MLE is asymptotically bounded by the inverse Fisher Information:
% \[
% \text{Var}(\hat{\theta}_{\text{MLE}}) \sim \frac{1}{n \mathcal{I}(\theta)}.
% \]

% \subsubsection*{Prior in Bayesian View}
% In Bayesian terms, MLE corresponds to using a flat prior $P(\theta) \propto 1$, which introduces no additional prior bias or variance regularization.

