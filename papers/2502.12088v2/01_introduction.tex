% Statistical inference is the backbone of many scientific inquiries, providing a foundational language through which evidence is quantified, hypotheses are tested, and uncertainty is estimated~\cite{walker1953statistical,casella2024statistical}. From the natural sciences to social sciences, statistical inference is an accepted mechanism through which scientists relate noisy observations to theoretical models; it underpins the design of experiments, the validation of theories, and the interpretation of empirical results, making it central to the scientific method~\cite{barlow1993statistics, altman1990practical, james2006statistical, dienes2008understanding, salganik2019bit, hernan2010causal}. However, the practice of statistical inference is notoriously difficult. Real-world data often deviate from idealized assumptions, featuring heterogeneity, high dimensionality, and limited sample sizes, all of which challenge traditional methods~\cite{gurland1971simple, hoekstra2012assumptions,knief2021violating,NEURIPS2023_36b80eae}.

% Machine learning, in particular statistical learning, builds on the principles provided by statistical inference to automate the discovery, via \textit{learning}, of models that capture statistical patterns in the observed data~\cite{bishop2006pattern,hastie2017elements}. The flexibility and effectiveness of machine learning provides a fruitful approach to solve previously challenging problems: collect data and train a supervised statistical model to predict the target answer~\cite{andrychowicz2016learning,hinton2012deep,alipanahi2015predicting,silver2017mastering,esteva2017dermatologist,chen2018rise,jaiswal2019identifying}. Continuing the trend and turning the idea on itself, we propose the framework \textit{meta-statistical learning} that reformulates statistical inference tasks as supervised learning problems (see \Figref{fig:fig_1_large}).

% Meta-statistical learning shifts the unit of analysis from individual data points to entire datasets. Unlike traditional supervised learning, where the goal is to predict a label \( y \) for an individual sample \( x \) drawn from a joint distribution \( P_{X,Y} \), meta-statistical learning maps datasets to their target statistical properties. This paradigm naturally aligns with modern deep learning methodologies, which can easily handle various input modalities such as images~\cite{voulodimos2018deep}, graphs~\cite{ma2021deep}, time-series~\cite{gamboa2017deep,lim2021time,torres2021deep}, text~\cite{NIPS2017_3f5ee243,devlin-etal-2019-bert,radford2019language}, and tabular data~\cite{NEURIPS2023_90debc7c,hollmann2025accurate}. Meta-Statistical learning remains within standard supervised learning with the dataset being just another input modality processed by deep neural networks. However, this perspective enables the meta-statistical approach of learning to solve statistical inference by statistical learning.

Statistical inference is the backbone of many scientific inquiries, providing a rigorous framework for quantifying evidence, testing hypotheses, and estimating uncertainty~\cite{walker1953statistical,casella2024statistical}. Across scientific disciplines, statistical inference is an accepted mechanism through which scientists relate noisy observations to theoretical models; it underpins the design of experiments, the validation of theories, and the interpretation of empirical results~\cite{barlow1993statistics, altman1990practical, james2006statistical, dienes2008understanding, salganik2019bit}.
% , hernan2010causal}. 
% As such, it is a critical tool of the scientific method.

However, the practice of statistical inference is notoriously difficult. Real-world data is noisy often deviating from idealized assumptions~\cite{gurland1971simple, hoekstra2012assumptions,knief2021violating,NEURIPS2023_36b80eae}. In particular, inference in low-sample regimes presents a persistent challenge, yet it is crucial across many applied sciences. In such settings, estimators must balance universality with bias and variance, whereas more robust estimators require strong assumptions on the underlying distribution \cite{casella2024statistical}. Some statistical quantities simply lack universally unbiased estimators -- like the standard deviation -- necessitating context-dependent correction strategies \cite{gurland1971simple,bengio2003no}. In general, designing statistical estimators requires making choices regarding the bias-variance and robustness-universality trade-offs, requiring manual effort to craft estimators to specific goals \cite{silvey2013optimal}.


Machine learning, itself a form of statistical inference, can provide a flexible approach to these challenges. Instead of manually designing statistical estimators, we propose to \textit{learn} them from data, leveraging amortized learning strategies to train models that generalize across diverse data distributions and adapt their estimation strategy contextually to new inputs. We call this approach \textit{meta-statistical learning}, wherein entire datasets are treated as input objects and statistical inference tasks are directly framed as supervised learning problems (see \Figref{fig:fig_1_large}). 

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{images/illustration_meta_stats.pdf} %
    \caption{\textbf{Illustration of the Meta-Statistical learning setup:} a meta-distribution $P_{\Gamma}$ dictates the sampling of meta-datapoints, couples of datasets $X$ and the label $y$ a property of data-generating distribution $P_X$. The meta-statistical model learns to predict $y$ from entire datasets effectively converting statistical inference in a supervised learning problem.}
    \label{fig:fig_1_large}
\end{figure*}


Meta-statistical learning shifts the unit of analysis from individual data points to entire datasets. Unlike traditional supervised learning, where the goal is to predict a label \( y \) for an individual sample \( x \) drawn from a joint distribution \( P_{X,Y} \), meta-statistical models learn to map datasets to their target statistical properties from large amounts of synthetic datasets. This formulation aligns naturally with modern deep learning tools, which can easily handle various input modalities such as images~\cite{voulodimos2018deep}, graphs~\cite{ma2021deep}, time-series~\cite{gamboa2017deep,lim2021time,torres2021deep}, language~\cite{NIPS2017_3f5ee243,devlin-etal-2019-bert,radford2019language}, and tabular data~\cite{NEURIPS2023_90debc7c,hollmann2025accurate}. The approach remains firmly within supervised learning where datasets are just another input modality processed by neural networks. However, this perspective enables us to flexibly tackle statistical inference tasks by supervised learning.

This perspective builds on a growing body of work that seeks to replace or assist hand-crafted statistical procedures with trained models. Approaches such as neural processes \cite{garnelo2018neural, garnelo2018conditional, Gordon:2020, markou2022practical, huang2023practical, BruinsmaMRFAVBH23, chang2025amortized}, simulation-based inference \cite{pmlr-v89-papamakarios19a, cranmer2020frontier}, and amortized Bayesian inference \cite{chan2018likelihood, chen2023learning, chen2020neural, elsemueller2024sensitivity, radev2020bayesflow, avecilla2022neural, pmlr-v235-gloeckler24a} have demonstrated the feasibility of learning statistical inference procedures directly from data. Meta-statistical learning focuses on the core challenge of learning statistical estimators. This enables the development of models that adapt their estimation strategies to new data distributions in a fully data-driven manner.


The success of large language models (LLMs)~\cite{NIPS2017_3f5ee243,radford2019language,achiam2023gpt} serves as both an inspiration and a blueprint for the meta-statistical research direction. First, attention-based architectures inherently satisfy the permutation invariance property necessary for processing datasets as unordered collections~\cite{lee2019set,zhang2022set}, making them default candidate architectures. Second, the substantial infrastructure developed for LLMs—including algorithmic frameworks, hardware optimizations, and software ecosystems—can be readily repurposed for meta-statistical modeling. Finally, large-scale training datasets can be synthetically generated for most statistical inference tasks, enabling robust generalization across data distributions and strong performance in low-sample settings where classical estimators often struggle.

% The success of large language models (LLMs)~\cite{NIPS2017_3f5ee243,radford2019language,achiam2023gpt} offers both inspiration and a practical blueprint for the meta-statistical framework. First, attention-based architectures inherently satisfy a key requirement for processing datasets: permutation invariance~\cite{lee2019set,zhang2022set}. This property is crucial for modeling datasets as unordered collections of points. Second, the extensive infrastructure developed for LLMs—including algorithms, hardware, and software—can be directly to meta-statistical models. Finally, large-scale training datasets can easily be synthesized for most statistical inference tasks of interest. Therefore, we can expect meta-statistical to generalize  across unseen data distributions and remain robust in low-sample-size regimes, where classical statistical estimators often fail.

\xhdr{Contributions}
In this work: 
(i) We introduce the \textit{meta-statistical framework} and establish its connections to related paradigms such as meta-learning and amortized inference (\Secref{sec:meta_ml}).
(ii) We evaluate various meta-statistical architectures, demonstrating that dataset encoders based on Set Transformer~\cite{lee2019set} variants achieve strong performance and generalization while avoiding the quadratic computational cost of standard attention mechanisms (\Secref{sec:experiments_desc}).
(iii) We showcase the versatility of the framework through three statistical inference problems: estimating standard deviation, conducting normality tests, and estimating mutual information. Our results highlight the robustness and generalization capabilities of meta-statistical models across diverse data distributions and, especially in low-sample-size settings (\Secref{sec:experiments_inf}).
% (iv) We release a meta-statistical normality test and MI estimator for practitioners, which is strong in low-sample regimes.

We believe the meta-statistical framework offers a promising path to leverage the principles that made LLMs successful, re-purposing them to tackle challenging statistical inference problems. 


% Self-attention between datapoints: https://proceedings.neurips.cc/paper_files/paper/2021/file/f1507aba9fc82ffa7cc7373c58f8a613-Paper.pdf

% Transformer neural processes: https://arxiv.org/abs/2207.04179

% point transformer: https://ieeexplore.ieee.org/abstract/document/9552005

% Learning substructure invariance: https://proceedings.neurips.cc/paper_files/paper/2022/hash/547108084f0c2af39b956f8eadb75d1b-Abstract-Conference.html

% Masked AE for point clouds: https://link.springer.com/chapter/10.1007/978-3-031-20086-1_35



% \begin{figure*}[ht]
%     \centering
%     \includegraphics[width=0.95\textwidth]{images/illustration_meta_stats_small.pdf} 
%     \caption{\textbf{Illustration of Meta-Statistical Learning:} To decide which to keep against fig 1}
%     \label{fig:fig_1_small}
% \end{figure*}