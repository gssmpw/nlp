\documentclass{article} % For LaTeX2e
% \usepackage{iclr2025_conference,times}

\usepackage[preprint]{icml2025}
% \usepackage[accepted]{icml2025}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\input{useful_macros}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{tikz-cd}
\usepackage{amsthm}
\usepackage{subcaption}
% \newtheorem{theorem}{Theorem}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem*{notation}{Notation}
\usepackage{wrapfig}
\usepackage{bbm}
\usepackage{booktabs}


% \newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO: #1}}}

% \definecolor{myblue}{RGB}{0, 102, 204}

\icmltitlerunning{Meta-Statistical Learning}

\begin{document}

\twocolumn[
\icmltitle{Meta-Statistical Learning: Supervised Learning of Statistical Inference}



\begin{icmlauthorlist}
\icmlauthor{Maxime Peyrard}{xxx}
\icmlauthor{Kyunghyun Cho}{yyy,zzz}
\end{icmlauthorlist}

\icmlaffiliation{xxx}{Université Grenoble Alpes, CNRS, Grenoble INP, LIG}
\icmlaffiliation{yyy}{New York University}
\icmlaffiliation{zzz}{Genentech}

\icmlcorrespondingauthor{Maxime Peyrard}{maxime.peyrard@univ-grenoble-alpes.fr}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]


\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
This work demonstrates that the tools and principles driving the success of large language models (LLMs) can be repurposed to tackle distribution-level tasks, where the goal is to predict properties of the data-generating distribution rather than labels for individual datapoints. These tasks encompass statistical inference problems such as parameter estimation, hypothesis testing, or mutual information estimation. 
Framing these tasks within traditional machine learning pipelines is challenging, as supervision is typically tied to individual datapoint.
% 
We propose \textit{meta-statistical learning}, a framework inspired by multi-instance learning that reformulates statistical inference tasks as supervised learning problems. In this approach, entire datasets are treated as single inputs to neural networks, which predict distribution-level parameters. Transformer-based architectures, without positional encoding, provide a natural fit due to their permutation-invariance properties.
% 
By training on large-scale synthetic datasets, meta-statistical models can leverage the scalability and optimization infrastructure of Transformer-based LLMs. We demonstrate the framework’s versatility with applications in hypothesis testing and mutual information estimation, showing strong performance, particularly for small datasets where traditional neural methods struggle. 
\end{abstract}


\section{Introduction}
\label{sec:introduction}
\input{01_introduction}

\section{Meta-Statistical Learning}
\label{sec:meta_ml}
\input{03_framework}

\section{Experimental Setup}
\label{sec:experimental_setup}
\input{04_experimental_setup}

\section{Experiments on Descriptive Tasks}
\label{sec:experiments_desc}
\input{04_experiments_desc}

\section{Experiments on Inferential Tasks}
\label{sec:experiments_inf}
\input{04_experiments_inf}

\section{Discussion}
\label{sec:discussion}
\input{06_discussion}


% \section*{Acknowledgments}

\section*{Impact Statements}
This paper presents work whose goal is to advance the field of Machine Learning. Meta-statistical learning aims to enhance inference in low-sample settings, benefiting applied fields of Science like medicine and economics by improving estimator reliability. Learned estimators may inherit biases from the data they are trained on, potentially leading to misleading conclusions if not carefully validated. Further, as with any data-driven methodology, interpretability remains a challenge; understanding why a model makes a particular statistical inference is crucial for scientific rigor. 

% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.


% \clearpage



% \section*{Glossary of Important Terms}
% \label{sec:glossary}
% \input{__glossary}


% Entries for the entire Anthology, followed by custom entries
% \clearpage
\bibliography{anthology,main}
\bibliographystyle{acl_natbib}

% 
\appendix
\clearpage

\input{10_appendix}

\end{document}
