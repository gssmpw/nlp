\section{Related Work}
The idea of processing multiple data points simultaneously originates from multi-instance learning, where models receive sets of instances and assign labels at the group level ____. Once datasets can be meaningfully represented by neural networks, amortized learning techniques allowing models to generalize quickly to new datasets naturally emerge ____.

A notable example is the \textit{neural statistician} framework ____, which employs variational autoencoders (VAEs) to learn dataset representations in an unsupervised manner. Similarly, ____ applied VAEs to infer generative models from few data points. The concept of learning dataset-level representations has also been explored through meta-features ____, where models extract high-level statistics tailored for specific tasks. For instance, ____ learned meta-features for anomaly detection, while ____ trained models to predict dataset-level statistics such as the number of distinct values. Recently, ____ employed transformers trained on synthetic datasets for missing value imputation, which we recognize as an instance of meta-statistical learning in low-sample-size settings.

Approaches of a meta-statistical nature have also been successfully applied in causal discovery ____. These methods generate synthetic data with known causal structures and train neural networks to infer causal properties from a set of observations ____. For example, ____ proposed an attention-based model trained on simulated datasets to identify causal parents of target variables. Meta-statistical learning is a type of amortized learning focused on estimating statistical parameters; it builds upon and generalizes these previous works. 

\xhdr{Machine Learning for Statistical Inference}
Our work aligns with the broader research direction on neural processes ____. Neural processes can predict latent variables of interest from datasets ____ by leveraging transformers ____ and Deep Sets ____ to enforce permutation invariance ____. A related approach, known as prior-fitted networks, has demonstrated that transformers can be effectively repurposed for Bayesian inference ____ and optimization tasks ____.

Additionally, there is growing interest in using trained models to assist in statistical inference ____ and optimization ____. In particular, simulation-based inference benefits from neural simulations ____ and amortized Bayesian inference ____. Amortized Bayesian inference typically replaces probabilistic inference with a neural network prediction task ____. These previous work illustrate the feasibility of learning distribution-relevant parameters via maximum likelihood using permutation-invariant dataset representations. In this work, we identify the emerging theme: translate complex statistical inference problems and into the powerful and flexible framework of supervised learning. We then undertake a study of this paradigm from the ground up and investigate parameter efficient dataset encoders like the Set Transformer ____.

\xhdr{Relationship to Meta-Learning}
Meta-learning, or \textit{learning to learn}, is a paradigm focused on generalizing across tasks drawn from different distributions ____. Meta-learning seeks to acquire transferable meta-knowledge, enabling rapid adaptation to new tasks ____. A broad range of approaches exist ____, some emphasizing dataset-level processing to extract useful representations ____. This is particularly relevant in few-shot learning ____. 
Notably, neural processes represent a class of meta-learners that use a meta-distribution over functions, adapting their prior to new datasets using observed input-output pairs ____.
% 
Meta-statistical learning shares conceptual similarities with meta-learning, as both focus on generalization across distributions. However, while the target of meta-learning remains instance-level predictions, meta-statistical learning emphasizes distributional properties. These paradigms are complementary: insights from dataset-level analysis can directly improve generalization in meta-learning ____.