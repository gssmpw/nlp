@book{molnar2022,
  title      = {Interpretable Machine Learning},
  author     = {Christoph Molnar},
  year       = {2022},
  subtitle   = {A Guide for Making Black Box Models Explainable},
  edition    = {2},
  url        = {https://christophm.github.io/interpretable-ml-book}
}

@inproceedings{colombo-etal-2023-glass,
    title = "The Glass Ceiling of Automatic Evaluation in Natural Language Generation",
    author = "Colombo, Pierre  and
      Peyrard, Maxime  and
      Noiry, Nathan  and
      West, Robert  and
      Piantanida, Pablo",
    editor = "Park, Jong C.  and
      Arase, Yuki  and
      Hu, Baotian  and
      Lu, Wei  and
      Wijaya, Derry  and
      Purwarianti, Ayu  and
      Krisnadhi, Adila Alfa",
    booktitle = "Findings of the Association for Computational Linguistics: IJCNLP-AACL 2023 (Findings)",
    month = nov,
    year = "2023",
    address = "Nusa Dua, Bali",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-ijcnlp.16/",
    doi = "10.18653/v1/2023.findings-ijcnlp.16",
    pages = "178--183"
}

@inproceedings{teney2022predicting,
  title={Predicting is not understanding: Recognizing and addressing underspecification in machine learning},
  author={Teney, Damien and Peyrard, Maxime and Abbasnejad, Ehsan},
  booktitle={European Conference on Computer Vision},
  pages={458--476},
  year={2022},
  organization={Springer}
}


@article{bradley1952rank,
  title={Rank analysis of incomplete block designs: I. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}


@inproceedings{NIPS2004_825f9cd5,
 author = {Huang, Tzu-kuo and Lin, Chih-jen and Weng, Ruby},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {L. Saul and Y. Weiss and L. Bottou},
 pages = {},
 publisher = {MIT Press},
 title = {A Generalized Bradley-Terry Model: From Group Competition to Individual Skill},
 url = {https://proceedings.neurips.cc/paper_files/paper/2004/file/825f9cd5f0390bc77c1fed3c94885c87-Paper.pdf},
 volume = {17},
 year = {2004}
}


@article{maron1997framework,
  title={A framework for multiple-instance learning},
  author={Maron, Oded and Lozano-P{\'e}rez, Tom{\'a}s},
  journal={Advances in neural information processing systems},
  volume={10},
  year={1997}
}

@article{brier1950verification,
  title={Verification of forecasts expressed in terms of probability},
  author={Brier, Glenn W},
  journal={Monthly weather review},
  volume={78},
  number={1},
  pages={1--3},
  year={1950}
}

@article{walker1953statistical,
  title={Statistical inference.},
  author={Walker, Helen M and Lev, Joseph},
  year={1953},
  publisher={Henry Holt and Company}
}

@article{knief2021violating,
  title={Violating the normality assumption may be the lesser of two evils},
  author={Knief, Ulrich and Forstmeier, Wolfgang},
  journal={Behavior Research Methods},
  volume={53},
  number={6},
  pages={2576--2590},
  year={2021},
  publisher={Springer}
}

@article{hoekstra2012assumptions,
  title={Are assumptions of well-known statistical techniques checked, and why (not)?},
  author={Hoekstra, Rink and Kiers, Henk AL and Johnson, Addie},
  journal={Frontiers in psychology},
  volume={3},
  pages={137},
  year={2012},
  publisher={Frontiers Research Foundation}
}

@book{james2006statistical,
  title={Statistical methods in experimental physics},
  author={James, Frederick},
  year={2006},
  publisher={World Scientific Publishing Company}
}

@book{barlow1993statistics,
  title={Statistics: a guide to the use of statistical methods in the physical sciences},
  author={Barlow, Roger J},
  volume={29},
  year={1993},
  publisher={John Wiley \& Sons}
}

@misc{hernan2010causal,
  title={Causal inference},
  author={Hern{\'a}n, Miguel A and Robins, James M},
  year={2010},
  publisher={CRC Boca Raton, FL}
}


@book{salganik2019bit,
  title={Bit by bit: Social research in the digital age},
  author={Salganik, Matthew J},
  year={2019},
  publisher={Princeton University Press}
}


@book{casella2024statistical,
  title={Statistical inference},
  author={Casella, George and Berger, Roger},
  year={2024},
  publisher={CRC press}
}

@INPROCEEDINGS{10136150,
  author={Hartmann, Valentin and Meynent, Léo and Peyrard, Maxime and Dimitriadis, Dimitrios and Tople, Shruti and West, Robert},
  booktitle={2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)}, 
  title={Distribution Inference Risks: Identifying and Mitigating Sources of Leakage}, 
  year={2023},
  volume={},
  number={},
  pages={136-149},
  keywords={Learning systems;Training;Bridges;Training data;Machine learning;Data models;Libraries;distribution inference;property inference;machine learning},
  doi={10.1109/SaTML54575.2023.00018}}


@book{dienes2008understanding,
  title={Understanding psychology as a science: An introduction to scientific and statistical inference},
  author={Dienes, Zoltan},
  year={2008},
  publisher={Bloomsbury Publishing}
}

@inproceedings{waswani2017attention,
  title={Attention is all you need},
  author={Waswani, A and Shazeer, N and Parmar, N and Uszkoreit, J and Jones, L and Gomez, A and Kaiser, L and Polosukhin, I},
  booktitle={NIPS},
  year={2017}
}

@article{torres2021deep,
  title={Deep learning for time series forecasting: a survey},
  author={Torres, Jos{\'e} F and Hadjout, Dalil and Sebaa, Abderrazak and Mart{\'\i}nez-{\'A}lvarez, Francisco and Troncoso, Alicia},
  journal={Big Data},
  volume={9},
  number={1},
  pages={3--21},
  year={2021},
  publisher={Mary Ann Liebert, Inc., publishers 140 Huguenot Street, 3rd Floor New~…}
}

@article{lim2021time,
  title={Time-series forecasting with deep learning: a survey},
  author={Lim, Bryan and Zohren, Stefan},
  journal={Philosophical Transactions of the Royal Society A},
  volume={379},
  number={2194},
  pages={20200209},
  year={2021},
  publisher={The Royal Society Publishing}
}

@article{gamboa2017deep,
  title={Deep learning for time-series analysis},
  author={Gamboa, John Cristian Borges},
  journal={arXiv preprint arXiv:1701.01887},
  year={2017}
}


@book{ma2021deep,
  title={Deep learning on graphs},
  author={Ma, Yao and Tang, Jiliang},
  year={2021},
  publisher={Cambridge University Press}
}

@article{dietterich1997solving,
  title={Solving the multiple instance problem with axis-parallel rectangles},
  author={Dietterich, Thomas G and Lathrop, Richard H and Lozano-P{\'e}rez, Tom{\'a}s},
  journal={Artificial intelligence},
  volume={89},
  number={1-2},
  pages={31--71},
  year={1997},
  publisher={Elsevier}
}

@inproceedings{lee2019set,
  title={Set transformer: A framework for attention-based permutation-invariant neural networks},
  author={Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
  booktitle={International conference on machine learning},
  pages={3744--3753},
  year={2019},
  organization={PMLR}
}

@article{nichol2018first,
  title={On first-order meta-learning algorithms},
  author={Nichol, A},
  journal={arXiv preprint arXiv:1803.02999},
  year={2018}
}

@article{vinyals2016matching,
  title={Matching networks for one shot learning},
  author={Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Wierstra, Daan and others},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{zhang2022set,
  title={Set norm and equivariant skip connections: Putting the deep in deep sets},
  author={Zhang, Lily and Tozzo, Veronica and Higgins, John and Ranganath, Rajesh},
  booktitle={International Conference on Machine Learning},
  pages={26559--26574},
  year={2022},
  organization={PMLR}
}

@article{wan2014estimating,
  title={Estimating the sample mean and standard deviation from the sample size, median, range and/or interquartile range},
  author={Wan, Xiang and Wang, Wenqian and Liu, Jiming and Tong, Tiejun},
  journal={BMC medical research methodology},
  volume={14},
  pages={1--13},
  year={2014},
  publisher={Springer}
}

@article{hospedales2021meta,
  title={Meta-learning in neural networks: A survey},
  author={Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={44},
  number={9},
  pages={5149--5169},
  year={2021},
  publisher={IEEE}
}

@article{huisman2021survey,
  title={A survey of deep meta-learning},
  author={Huisman, Mike and Van Rijn, Jan N and Plaat, Aske},
  journal={Artificial Intelligence Review},
  volume={54},
  number={6},
  pages={4483--4541},
  year={2021},
  publisher={Springer}
}

@article{vanschoren2019meta,
  title={Meta-learning},
  author={Vanschoren, Joaquin},
  journal={Automated machine learning: methods, systems, challenges},
  pages={35--61},
  year={2019},
  publisher={Springer International Publishing}
}


@article{snell2017prototypical,
  title={Prototypical networks for few-shot learning},
  author={Snell, Jake and Swersky, Kevin and Zemel, Richard},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@incollection{beaulieu2020learning,
  title={Learning to continually learn},
  author={Beaulieu, Shawn and Frati, Lapo and Miconi, Thomas and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff and Cheney, Nick},
  booktitle={ECAI 2020},
  pages={992--1001},
  year={2020},
  publisher={IOS Press}
}

@incollection{thrun1998lifelong,
  title={Lifelong learning algorithms},
  author={Thrun, Sebastian},
  booktitle={Learning to learn},
  pages={181--209},
  year={1998},
  publisher={Springer}
}

@article{wang2016learning,
  title={Learning to reinforcement learn},
  author={Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  journal={arXiv preprint arXiv:1611.05763},
  year={2016}
}

@article{zhou2018deep,
  title={Deep meta-learning: Learning to learn in the concept space},
  author={Zhou, Fengwei and Wu, Bin and Li, Zhenguo},
  journal={arXiv preprint arXiv:1802.03596},
  year={2018}
}


@phdthesis{schmidhuber1987evolutionary,
  title={Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook},
  author={Schmidhuber, J{\"u}rgen},
  year={1987},
  school={Technische Universit{\"a}t M{\"u}nchen}
}

@misc{schmidhuber1996simple,
  title={Simple principles of metalearning},
  author={Schmidhuber, Juergen and Zhao, Jieyu and Wiering, Marco},
  year={1996},
  publisher={Istituto Dalle Molle Di Studi Sull Intelligenza Artificiale}
}

@inproceedings{schmidhuber1993neural,
  title={A neural network that embeds its own meta-levels},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={IEEE International Conference on Neural Networks},
  pages={407--412},
  year={1993},
  organization={IEEE}
}


@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}


@inproceedings{santoro2016meta,
  title={Meta-learning with memory-augmented neural networks},
  author={Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  booktitle={International conference on machine learning},
  pages={1842--1850},
  year={2016},
  organization={PMLR}
}

@article{graves2014neural,
  title={Neural Turing Machines},
  author={Graves, Alex},
  journal={arXiv preprint arXiv:1410.5401},
  year={2014}
}

@article{mishra2017simple,
  title={A simple neural attentive meta-learner},
  author={Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1707.03141},
  year={2017}
}

@inproceedings{ravi2017optimization,
  title={Optimization as a model for few-shot learning},
  author={Ravi, Sachin and Larochelle, Hugo},
  booktitle={International conference on learning representations},
  year={2017}
}

@inproceedings{munkhdalai2017meta,
  title={Meta networks},
  author={Munkhdalai, Tsendsuren and Yu, Hong},
  booktitle={International conference on machine learning},
  pages={2554--2563},
  year={2017},
  organization={PMLR}
}

@article{jomaa2021dataset2vec,
  title={Dataset2vec: Learning dataset meta-features},
  author={Jomaa, Hadi S and Schmidt-Thieme, Lars and Grabocka, Josif},
  journal={Data Mining and Knowledge Discovery},
  volume={35},
  number={3},
  pages={964--985},
  year={2021},
  publisher={Springer}
}

@inproceedings{shyam2017attentive,
  title={Attentive recurrent comparators},
  author={Shyam, Pranav and Gupta, Shubham and Dukkipati, Ambedkar},
  booktitle={International conference on machine learning},
  pages={3173--3181},
  year={2017},
  organization={PMLR}
}

@article{duan2016rl,
  title={Rl $\^{} 2$: Fast reinforcement learning via slow reinforcement learning},
  author={Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1611.02779},
  year={2016}
}

@article{wang2023few,
  title={Few-shot learning meets transformer: Unified query-support transformers for few-shot classification},
  author={Wang, Xixi and Wang, Xiao and Jiang, Bo and Luo, Bin},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  volume={33},
  number={12},
  pages={7789--7802},
  year={2023},
  publisher={IEEE}
}

@inproceedings{wu2020meta, 
  title={Meta-amortized variational inference and learning},
  author={Wu, Mike and Choi, Kristy and Goodman, Noah and Ermon, Stefano},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={6404--6412},
  year={2020}
}

@inproceedings{edwards2017towards,
  title={Towards a Neural Statistician},
  author={Edwards, Harrison and Storkey, Amos},
  booktitle={5th International Conference on Learning Representations},
  pages={1--13},
  year={2017}
}

@article{hewitt2018variational,
  title={The variational homoencoder: Learning to learn high capacity generative models from few examples},
  author={Hewitt, Luke B and Nye, Maxwell I and Gane, Andreea and Jaakkola, Tommi and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:1807.08919},
  year={2018}
}

@article{wu2022learning,
  title={Learning to be a statistician: learned estimator for number of distinct values},
  author={Wu, Renzhi and Ding, Bolin and Chu, Xu and Wei, Zhewei and Dai, Xiening and Guan, Tao and Zhou, Jingren},
  journal={arXiv preprint arXiv:2202.02800},
  year={2022}
}


@article{kotlar2021novel,
  title={Novel meta-features for automated machine learning model selection in anomaly detection},
  author={Kotlar, Milo{\v{s}} and Punt, Marija and Radivojevi{\'c}, Zaharije and Cvetanovi{\'c}, Milo{\v{s}} and Milutinovi{\'c}, Veljko},
  journal={IEEE Access},
  volume={9},
  pages={89675--89687},
  year={2021},
  publisher={IEEE}
}

@article{rivolli2022meta,
  title={Meta-features for meta-learning},
  author={Rivolli, Adriano and Garcia, Lu{\'\i}s PF and Soares, Carlos and Vanschoren, Joaquin and de Carvalho, Andr{\'e} CPLF},
  journal={Knowledge-Based Systems},
  volume={240},
  pages={108101},
  year={2022},
  publisher={Elsevier}
}

@misc{liu2024dnasedeepneuralnetsassisted, 
      title={DNA-SE: Towards Deep Neural-Nets Assisted Semiparametric Estimation}, 
      author={Qinshuo Liu and Zixin Wang and Xi-An Li and Xinyao Ji and Lei Zhang and Lin Liu and Zhonghua Liu},
      year={2024},
      eprint={2408.02045},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2408.02045}, 
}


@inproceedings{NEURIPS2023_90debc7c,
 author = {Gulati, Manbir and Roysdon, Paul},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {46245--46254},
 publisher = {Curran Associates, Inc.},
 title = {TabMT: Generating tabular data with masked transformers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/90debc7cedb5cac83145fc8d18378dc5-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}


@article{borisov2022language,
  title={Language models are realistic tabular data generators},
  author={Borisov, Vadim and Se{\ss}ler, Kathrin and Leemann, Tobias and Pawelczyk, Martin and Kasneci, Gjergji},
  journal={arXiv preprint arXiv:2210.06280},
  year={2022}
}

@article{solatorio2023realtabformer,
  title={Realtabformer: Generating realistic relational and tabular data using transformers},
  author={Solatorio, Aivin V and Dupriez, Olivier},
  journal={arXiv preprint arXiv:2302.02041},
  year={2023}
}

@article{zhao2023tabula,
  title={Tabula: Harnessing language models for tabular data synthesis},
  author={Zhao, Zilong and Birke, Robert and Chen, Lydia},
  journal={arXiv preprint arXiv:2310.12746},
  year={2023}
}

@article{hollmann2025accurate,
  title={Accurate predictions on small data with a tabular foundation model},
  author={Hollmann, Noah and M{\"u}ller, Samuel and Purucker, Lennart and Krishnakumar, Arjun and K{\"o}rfer, Max and Hoo, Shi Bin and Schirrmeister, Robin Tibor and Hutter, Frank},
  journal={Nature},
  volume={637},
  number={8045},
  pages={319--326},
  year={2025},
  publisher={Nature Publishing Group UK London}
}


@article{fenstad1980robust,
  title={Robust estimation of standard deviation},
  author={Fenstad, Grete U and Kjaernes, Morten and Wall{\O}e, Lars},
  journal={Journal of statistical computation and simulation},
  volume={10},
  number={2},
  pages={113--132},
  year={1980},
  publisher={Taylor \& Francis}
}

@article{gupta1952estimation,
  title={Estimation of the mean and standard deviation of a normal population from a censored sample},
  author={Gupta, AK},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={260--273},
  year={1952},
  publisher={JSTOR}
}


@book{lehmann2006theory,
  title={Theory of point estimation},
  author={Lehmann, Erich L and Casella, George},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@article{gurland1971simple,
  title={A simple approximation for unbiased estimation of the standard deviation},
  author={Gurland, John and Tripathi, Ram C},
  journal={The American Statistician},
  volume={25},
  number={4},
  pages={30--32},
  year={1971},
  publisher={Taylor \& Francis}
}

@article{kendall1943advanced,
  title={The advanced theory of statistics.},
  author={Kendall, Maurice George},
  year={1943}
}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{hochreiter1997long,
  added-at = {2016-11-15T08:49:43.000+0100},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  biburl = {https://www.bibsonomy.org/bibtex/2a4a80026d24955b267cae636aa8abe4a/dallmann},
  interhash = {0692b471c4b9ae65d00affebc09fb467},
  intrahash = {a4a80026d24955b267cae636aa8abe4a},
  journal = {Neural computation},
  keywords = {lstm rnn},
  number = 8,
  pages = {1735--1780},
  publisher = {MIT Press},
  timestamp = {2016-11-15T08:49:43.000+0100},
  title = {Long short-term memory},
  volume = 9,
  year = 1997
}

@inproceedings{FrankleC19,
  added-at = {2019-07-25T00:00:00.000+0200},
  author = {Frankle, Jonathan and Carbin, Michael},
  biburl = {https://www.bibsonomy.org/bibtex/21d19a7a954f353e4f64ec5715a5d2713/dblp},
  booktitle = {ICLR},
  ee = {https://openreview.net/forum?id=rJl-b3RcF7},
  interhash = {ec23457113db4d1703ff480d84860535},
  intrahash = {1d19a7a954f353e4f64ec5715a5d2713},
  keywords = {dblp},
  publisher = {OpenReview.net},
  timestamp = {2019-07-26T11:39:47.000+0200},
  title = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.},
  url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2019.html#FrankleC19},
  year = 2019
}

@misc{liu2024surveylotterytickethypothesis,
      title={A Survey of Lottery Ticket Hypothesis}, 
      author={Bohan Liu and Zijie Zhang and Peixiong He and Zhensen Wang and Yang Xiao and Ruimeng Ye and Yang Zhou and Wei-Shinn Ku and Bo Hui},
      year={2024},
      eprint={2403.04861},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.04861}, 
}

@misc{gale2019statesparsitydeepneural,
      title={The State of Sparsity in Deep Neural Networks}, 
      author={Trevor Gale and Erich Elsen and Sara Hooker},
      year={2019},
      eprint={1902.09574},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1902.09574}, 
}

@inproceedings{ma2023llmpruner,
  title={LLM-Pruner: On the Structural Pruning of Large Language Models},
  author={Xinyin Ma and Gongfan Fang and Xinchao Wang},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023},
}


@misc{sun2024simpleeffectivepruningapproach,
      title={A Simple and Effective Pruning Approach for Large Language Models}, 
      author={Mingjie Sun and Zhuang Liu and Anna Bair and J. Zico Kolter},
      year={2024},
      eprint={2306.11695},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.11695}, 
}

@article{abs-1910-02120,
  author    = {Binhang Yuan and
               Anastasios Kyrillidis and
               Christopher M. Jermaine},
  title     = {Distributed Learning of Deep Neural Networks using Independent Subnet
               Training},
  journal   = {CoRR},
  volume    = {abs/1910.02120},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.02120},
  archivePrefix = {arXiv},
  eprint    = {1910.02120},
  timestamp = {Wed, 09 Oct 2019 14:07:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-02120.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{JMLR:v15:srivastava14a,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@misc{nakkiran2019deepdoubledescentbigger,
      title={Deep Double Descent: Where Bigger Models and More Data Hurt}, 
      author={Preetum Nakkiran and Gal Kaplun and Yamini Bansal and Tristan Yang and Boaz Barak and Ilya Sutskever},
      year={2019},
      eprint={1912.02292},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.02292}, 
}



@misc{liao2022convergenceshallowneuralnetwork,
      title={On the Convergence of Shallow Neural Network Training with Randomly Masked Neurons}, 
      author={Fangshuo Liao and Anastasios Kyrillidis},
      year={2022},
      eprint={2112.02668},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2112.02668}, 
}

@article{pnas.1903070116,
author = {Mikhail Belkin  and Daniel Hsu  and Siyuan Ma  and Soumik Mandal },
title = {Reconciling modern machine-learning practice and the classical bias–variance trade-off},
journal = {Proceedings of the National Academy of Sciences},
volume = {116},
number = {32},
pages = {15849-15854},
year = {2019},
doi = {10.1073/pnas.1903070116},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1903070116},
}

@article{yuste2008circuit,
  title={Circuit neuroscience: the road ahead},
  author={Yuste, Rafael},
  journal={Frontiers in neuroscience},
  volume={2},
  pages={1038},
  year={2008},
  publisher={Frontiers}
}

@book{marr_vision,
author = {Marr, David},
title = {Vision: A Computational Investigation into the Human Representation and Processing of Visual Information},
year = {1982},
isbn = {0716715678},
publisher = {Henry Holt and Co., Inc.},
address = {USA}
}

@techreport{understanding_circuitry,
author = {Marr, D. and Poggio, T.},
title = {From Understanding Computation to Understanding Neural Circuitry},
year = {1976},
publisher = {Massachusetts Institute of Technology},
address = {USA}
}

@article{neurosci_microproc,
    doi = {10.1371/journal.pcbi.1005268},
    author = {Jonas, Eric AND Kording, Konrad Paul},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Could a Neuroscientist Understand a Microprocessor?},
    year = {2017},
    month = {01},
    volume = {13},
    url = {https://doi.org/10.1371/journal.pcbi.1005268},
    pages = {1-24},
    number = {1},
}


@article{olah2017feature,
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  title = {Feature Visualization},
  journal = {Distill},
  year = {2017},
  note = {https://distill.pub/2017/feature-visualization},
  doi = {10.23915/distill.00007}
}

@article{olah2020zoom,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/zoom-in},
  doi = {10.23915/distill.00024.001}
}

@article{carter2019activation,
  author = {Carter, Shan and Armstrong, Zan and Schubert, Ludwig and Johnson, Ian and Olah, Chris},
  title = {Activation Atlas},
  journal = {Distill},
  year = {2019},
  note = {https://distill.pub/2019/activation-atlas},
  doi = {10.23915/distill.00015}
}

@misc{dreyer2024pureturningpolysemanticneurons,
      title={PURE: Turning Polysemantic Neurons Into Pure Features by Identifying Relevant Circuits}, 
      author={Maximilian Dreyer and Erblina Purelku and Johanna Vielhaben and Wojciech Samek and Sebastian Lapuschkin},
      year={2024},
      eprint={2404.06453},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.06453}, 
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

 @article{templeton2024scaling,
       title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
       author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
       year={2024},
       journal={Transformer Circuits Thread},
       url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
    }

@article{elhage2022superposition,
   title={Toy Models of Superposition},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/toy\_model/index.html}
}

@article{bricken2023monosemanticity,
       title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
       author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
       year={2023},
       journal={Transformer Circuits Thread},
       note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
    }

@misc{wang2022interpretabilitywildcircuitindirect,
      title={Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small}, 
      author={Kevin Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
      year={2022},
      eprint={2211.00593},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.00593}, 
}

@article{olah2018the,
  author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  title = {The Building Blocks of Interpretability},
  journal = {Distill},
  year = {2018},
  note = {https://distill.pub/2018/building-blocks},
  doi = {10.23915/distill.00010}
}

@misc{bhaskar2024findingtransformercircuitsedge,
      title={Finding Transformer Circuits with Edge Pruning}, 
      author={Adithya Bhaskar and Alexander Wettig and Dan Friedman and Danqi Chen},
      year={2024},
      eprint={2406.16778},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.16778}, 
}

@article{pearl2012causal,
  title={The causal mediation formula—a guide to the assessment of pathways and mechanisms},
  author={Pearl, Judea},
  journal={Prevention science},
  volume={13},
  pages={426--436},
  year={2012},
  publisher={Springer}
}

@misc{mueller2024missedcausesambiguouseffects,
      title={Missed Causes and Ambiguous Effects: Counterfactuals Pose Challenges for Interpreting Neural Networks}, 
      author={Aaron Mueller},
      year={2024},
      eprint={2407.04690},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.04690}, 
}

@misc{mueller2024questrightmediatorhistory,
      title={The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability}, 
      author={Aaron Mueller and Jannik Brinkmann and Millicent Li and Samuel Marks and Koyena Pal and Nikhil Prakash and Can Rager and Aruna Sankaranarayanan and Arnab Sen Sharma and Jiuding Sun and Eric Todd and David Bau and Yonatan Belinkov},
      year={2024},
      eprint={2408.01416},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.01416}, 
}

@inproceedings{NEURIPS2020_92650b2e,
 author = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {12388--12401},
 publisher = {Curran Associates, Inc.},
 title = {Investigating Gender Bias in Language Models Using Causal Mediation Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{kramár2024atpefficientscalablemethod,
      title={AtP*: An efficient and scalable method for localizing LLM behaviour to components}, 
      author={János Kramár and Tom Lieberum and Rohin Shah and Neel Nanda},
      year={2024},
      eprint={2403.00745},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.00745}, 
}

@article{lindsay2022testing,
  title={Testing the tools of systems neuroscience on artificial neural networks},
  author={Lindsay, Grace W},
  journal={arXiv preprint arXiv:2202.07035},
  year={2022}
}

@article{van2024reclaiming,
  title={Reclaiming AI as a theoretical tool for cognitive science},
  author={Van Rooij, Iris and Guest, Olivia and Adolfi, Federico and de Haan, Ronald and Kolokolova, Antonina and Rich, Patricia},
  journal={Computational Brain \& Behavior},
  pages={1--21},
  year={2024},
  publisher={Springer}
}

@article{sharma2019certifai,
  title={Certifai: Counterfactual explanations for robustness, transparency, interpretability, and fairness of artificial intelligence models},
  author={Sharma, Shubham and Henderson, Jette and Ghosh, Joydeep},
  journal={arXiv preprint arXiv:1905.07857},
  year={2019}
}

@inproceedings{roundtree2023ai,
  title={AI Explainability, Interpretability, Fairness, and Privacy: An Integrative Review of Reviews},
  author={Roundtree, Aimee Kendall},
  booktitle={International Conference on Human-Computer Interaction},
  pages={305--317},
  year={2023},
  organization={Springer}
}

@Article{electronics8080832,
AUTHOR = {Carvalho, Diogo V. and Pereira, Eduardo M. and Cardoso, Jaime S.},
TITLE = {Machine Learning Interpretability: A Survey on Methods and Metrics},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {832},
URL = {https://www.mdpi.com/2079-9292/8/8/832},
ISSN = {2079-9292},
DOI = {10.3390/electronics8080832}
}

@inproceedings{NEURIPS2023_34e1dbe9,
 author = {Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri\`{a}},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {16318--16352},
 publisher = {Curran Associates, Inc.},
 title = {Towards Automated Circuit Discovery for Mechanistic Interpretability},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{hanna2024faithfaithfulnessgoingcircuit,
      title={Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms}, 
      author={Michael Hanna and Sandro Pezzelle and Yonatan Belinkov},
      year={2024},
      eprint={2403.17806},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.17806}, 
}

@misc{bereska2024mechanisticinterpretabilityaisafety,
      title={Mechanistic Interpretability for AI Safety -- A Review}, 
      author={Leonard Bereska and Efstratios Gavves},
      year={2024},
      eprint={2404.14082},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2404.14082}, 
}


@misc{dunefsky2024transcodersinterpretablellmfeature,
      title={Transcoders Find Interpretable LLM Feature Circuits}, 
      author={Jacob Dunefsky and Philippe Chlenski and Neel Nanda},
      year={2024},
      eprint={2406.11944},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.11944}, 
}


@inproceedings{geva-etal-2023-dissecting,
    title = "Dissecting Recall of Factual Associations in Auto-Regressive Language Models",
    author = "Geva, Mor  and
      Bastings, Jasmijn  and
      Filippova, Katja  and
      Globerson, Amir",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.751",
    doi = "10.18653/v1/2023.emnlp-main.751",
    pages = "12216--12235",
}

@article{robins1992identifiability,
  title={Identifiability and exchangeability for direct and indirect effects},
  author={Robins, James M and Greenland, Sander},
  journal={Epidemiology},
  volume={3},
  number={2},
  pages={143--155},
  year={1992},
  publisher={LWW}
}

@inproceedings{pearl_direct, 
    author = {Pearl, Judea}, 
    title = {Direct and indirect effects}, 
    year = {2001}, 
    isbn = {1558608001}, 
    publisher = {Morgan Kaufmann Publishers Inc.}, 
    address = {San Francisco, CA, USA}, 
    pages = {411–420}, 
    numpages = {10}, 
    location = {Seattle, Washington}, 
    series = {UAI'01} 
}


@misc{syed2023attributionpatchingoutperformsautomated,
      title={Attribution Patching Outperforms Automated Circuit Discovery}, 
      author={Aaquib Syed and Can Rager and Arthur Conmy},
      year={2023},
      eprint={2310.10348},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.10348}, 
}


@misc{monea2024glitchmatrixlocatingdetecting,
      title={A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia}, 
      author={Giovanni Monea and Maxime Peyrard and Martin Josifoski and Vishrav Chaudhary and Jason Eisner and Emre Kıcıman and Hamid Palangi and Barun Patra and Robert West},
      year={2024},
      eprint={2312.02073},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.02073}, 
}

@inproceedings{NIPS2017_2b24d495,
 author = {He, Di and Lu, Hanqing and Xia, Yingce and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Decoding with Value Networks for Neural Machine Translation},
 url = {https://proceedings.neurips.cc/paper/2017/file/2b24d495052a8ce66358eb576b8912c8-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{meng2022locating,
  title={Locating and Editing Factual Associations in {GPT}},
  author={Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2022},
  note={arXiv:2202.05262}
}

@inproceedings{sutskever2014sequence,
 author = {Ilya Sutskever and
Oriol Vinyals and
Quoc V. Le},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/SutskeverVL14.bib},
 booktitle = {Advances in Neural Information Processing Systems 27: Annual Conference
on Neural Information Processing Systems 2014, December 8-13 2014,
Montreal, Quebec, Canada},
 editor = {Zoubin Ghahramani and
Max Welling and
Corinna Cortes and
Neil D. Lawrence and
Kilian Q. Weinberger},
 pages = {3104--3112},
 timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
 title = {Sequence to Sequence Learning with Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html},
 year = {2014}
}

@inproceedings{DBLP:conf/cvpr/RenWZLL17,
  author    = {Zhou Ren and
               Xiaoyu Wang and
               Ning Zhang and
               Xutao Lv and
               Li{-}Jia Li},
  title     = {Deep Reinforcement Learning-Based Image Captioning with Embedding
               Reward},
  booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017},
  pages     = {1151--1159},
  publisher = {{IEEE} Computer Society},
  year      = {2017},
  url       = {https://doi.org/10.1109/CVPR.2017.128},
  doi       = {10.1109/CVPR.2017.128},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/RenWZLL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{krishna-etal-2022-rankgen,
    title = "{R}ank{G}en: Improving Text Generation with Large Ranking Models",
    author = "Krishna, Kalpesh  and
      Chang, Yapei  and
      Wieting, John  and
      Iyyer, Mohit",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.15",
    pages = "199--232",
}

@inproceedings{Ribeiro_lime,
    author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
    title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
    year = {2016},
    isbn = {9781450342322},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2939672.2939778},
    doi = {10.1145/2939672.2939778},
    booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {1135–1144},
    numpages = {10},
    keywords = {explaining machine learning, black box classifier, interpretability, interpretable machine learning},
    location = {San Francisco, California, USA},
    series = {KDD '16}
}


@misc{singh2023human,
      title={Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models}, 
      author={Avi Singh and John D. Co-Reyes and Rishabh Agarwal and Ankesh Anand and Piyush Patil and Xavier Garcia and Peter J. Liu and James Harrison and Jaehoon Lee and Kelvin Xu and Aaron Parisi and Abhishek Kumar and Alex Alemi and Alex Rizkowsky and Azade Nova and Ben Adlam and Bernd Bohnet and Gamaleldin Elsayed and Hanie Sedghi and Igor Mordatch and Isabelle Simpson and Izzeddin Gur and Jasper Snoek and Jeffrey Pennington and Jiri Hron and Kathleen Kenealy and Kevin Swersky and Kshiteej Mahajan and Laura Culp and Lechao Xiao and Maxwell L. Bileschi and Noah Constant and Roman Novak and Rosanne Liu and Tris Warkentin and Yundi Qian and Yamini Bansal and Ethan Dyer and Behnam Neyshabur and Jascha Sohl-Dickstein and Noah Fiedel},
      year={2023},
      eprint={2312.06585},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@misc{chen2024magdi,
      title={MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models}, 
      author={Justin Chih-Yao Chen and Swarnadeep Saha and Elias Stengel-Eskin and Mohit Bansal},
      year={2024},
      eprint={2402.01620},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tang2023does,
      title={Does Synthetic Data Generation of LLMs Help Clinical Text Mining?}, 
      author={Ruixiang Tang and Xiaotian Han and Xiaoqian Jiang and Xia Hu},
      year={2023},
      eprint={2303.04360},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{veselovsky2023generating,
      title={Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science}, 
      author={Veniamin Veselovsky and Manoel Horta Ribeiro and Akhil Arora and Martin Josifoski and Ashton Anderson and Robert West},
      year={2023},
      eprint={2305.15041},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{abdullin2024synthetic,
      title={Synthetic Dialogue Dataset Generation using LLM Agents}, 
      author={Yelaman Abdullin and Diego Molla-Aliod and Bahadorreza Ofoghi and John Yearwood and Qingyang Li},
      year={2024},
      eprint={2401.17461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lu2024mathgenie,
      title={MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs}, 
      author={Zimu Lu and Aojun Zhou and Houxing Ren and Ke Wang and Weikang Shi and Junting Pan and Mingjie Zhan and Hongsheng Li},
      year={2024},
      eprint={2402.16352},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jiao2023swarmgpt,
      title={Swarm-GPT: Combining Large Language Models with Safe Motion Planning for Robot Choreography Design}, 
      author={Aoran Jiao and Tanmay P. Patel and Sanjmi Khurana and Anna-Mariya Korol and Lukas Brunke and Vivek K. Adajania and Utku Culha and Siqi Zhou and Angela P. Schoellig},
      year={2023},
      eprint={2312.01059},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}


@misc{burns2023weaktostrong,
      title={Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision}, 
      author={Collin Burns and Pavel Izmailov and Jan Hendrik Kirchner and Bowen Baker and Leo Gao and Leopold Aschenbrenner and Yining Chen and Adrien Ecoffet and Manas Joglekar and Jan Leike and Ilya Sutskever and Jeff Wu},
      year={2023},
      eprint={2312.09390},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{silver2017mastering,
      title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}, 
      author={David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
      year={2017},
      eprint={1712.01815},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}


@book{Woodward2003-WOOMTH,
	address = {New York},
	author = {James F. Woodward},
	editor = {},
	publisher = {Oxford University Press},
	title = {Making Things Happen: A Theory of Causal Explanation},
	year = {2003}
}

@book{Potochnik2017-POTIAT-3,
	address = {Chicago},
	author = {Angela Potochnik},
	editor = {},
	publisher = {University of Chicago Press},
	title = {Idealization and the Aims of Science},
	year = {2017}
}

@misc{gallegos2024selfdebiasing,
      title={Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes}, 
      author={Isabel O. Gallegos and Ryan A. Rossi and Joe Barrow and Md Mehrab Tanjim and Tong Yu and Hanieh Deilamsalehy and Ruiyi Zhang and Sungchul Kim and Franck Dernoncourt},
      year={2024},
      eprint={2402.01981},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{NEURIPS2023_f6a8b109,
 author = {Wu, Zhengxuan and Geiger, Atticus and Icard, Thomas and Potts, Christopher and Goodman, Noah},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {78205--78226},
 publisher = {Curran Associates, Inc.},
 title = {Interpretability at Scale: Identifying Causal Mechanisms in Alpaca},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/f6a8b109d4d4fd64c75e94aaf85d9697-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{Beckers_Halpern_2019, 
    title={Abstracting Causal Models}, 
    volume={33}, 
    url={https://ojs.aaai.org/index.php/AAAI/article/view/4117}, 
    DOI={10.1609/aaai.v33i01.33012678}, 
    number={01}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    author={Beckers, Sander and Halpern, Joseph Y.}, 
    year={2019}, 
    month={Jul.}, 
    pages={2678-2685}
}

@conference{Rubensteinetal17,
  title = {Causal Consistency of Structural Equation Models},
  author = {Rubenstein, P. K. and Weichwald, S. and Bongers, S. and Mooij, J. M. and Janzing, D. and Grosse-Wentrup, M. and Sch{\"o}lkopf, B.},
  booktitle = {Proceedings of the 33rd Conference on Uncertainty in Artificial Intelligence (UAI)},
  pages = {ID 11},
  editors = {Gal Elidan, Kristian Kersting, and Alexander T. Ihler},
  month = aug,
  year = {2017},
  note = {*equal contribution},
  url = {http://auai.org/uai2017/proceedings/papers/11.pdf},
  month_numeric = {8}
}

@InProceedings{pmlr-v115-beckers20a,
  title = 	 {Approximate Causal Abstractions},
  author =       {Beckers, Sander and Eberhardt, Frederick and Halpern, Joseph Y.},
  booktitle = 	 {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference},
  pages = 	 {606--615},
  year = 	 {2020},
  editor = 	 {Adams, Ryan P. and Gogate, Vibhav},
  volume = 	 {115},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {22--25 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v115/beckers20a/beckers20a.pdf},
  url = 	 {https://proceedings.mlr.press/v115/beckers20a.html},
}

@unpublished{Geiger-etal:2022:SAIL,
	author = {Geiger, Atticus and Wu, Zhengxuan and D'Oosterlinck, Karel and Kreiss, Elisa and Goodman, Noah D. and Icard, Thomas and Potts, Christopher},
	note = {Stanford AI Lab Blog},
	title = {Faithful, Interpretable Model Explanations via Causal Abstraction},
	url = {https://ai.stanford.edu/blog/causal-abstraction/},
	year = {2022}
}

@inproceedings{NEURIPS2020_ecb287ff,
 author = {Yeh, Chih-Kuan and Kim, Been and Arik, Sercan and Li, Chun-Liang and Pfister, Tomas and Ravikumar, Pradeep},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {20554--20565},
 publisher = {Curran Associates, Inc.},
 title = {On Completeness-aware Concept-Based Explanations in Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/ecb287ff763c169694f682af52c1f309-Paper.pdf},
 volume = {33},
 year = {2020}
}

@book{PearlMackenzie18,
  added-at = {2018-09-17T16:24:20.000+0200},
  address = {New York},
  author = {Pearl, Judea and Mackenzie, Dana},
  biburl = {https://www.bibsonomy.org/bibtex/212360928cada467ce0f3bfa5e511752d/flint63},
  file = {2018/PearlMackenzie18.pdf},
  interhash = {dc0eae388b44d13b931962ebc07f5914},
  intrahash = {12360928cada467ce0f3bfa5e511752d},
  isbn = {978-0-465-09760-9},
  isbn10 = {046509760X},
  keywords = {01941 103 ai algorithm book knowledge numerical processing science theory},
  language = {american},
  publisher = {Basic Books},
  related = {Pearl09},
  shop = {https://www.basicbooks.com/titles/judea-pearl/the-book-of-why/9780465097616/},
  sortdate = {2018-06-01},
  subtitle = {The New Science of Cause and Effect},
  timestamp = {2018-09-17T16:24:20.000+0200},
  title = {The Book of Why},
  year = 2018
}

@article{lipton_2018, 
    author = {Lipton, Zachary C.}, 
    title = {The Mythos of Model Interpretability}, 
    year = {2018}, 
    issue_date = {October 2018}, 
    publisher = {Association for Computing Machinery}, 
    address = {New York, NY, USA}, 
    volume = {61}, 
    number = {10}, 
    issn = {0001-0782}, 
    url = {https://doi.org/10.1145/3233231}, 
    doi = {10.1145/3233231}, 
    journal = {Commun. ACM}, 
    month = {sep}, 
    pages = {36–43}, 
    numpages = {8} 
}

@INPROCEEDINGS {Cam,
author = {B. Zhou and A. Khosla and A. Lapedriza and A. Oliva and A. Torralba},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Learning Deep Features for Discriminative Localization},
year = {2016},
volume = {},
issn = {1063-6919},
pages = {2921-2929},
doi = {10.1109/CVPR.2016.319},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.319},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@misc{zhou2015objectdetectorsemergedeep,
      title={Object Detectors Emerge in Deep Scene CNNs}, 
      author={Bolei Zhou and Aditya Khosla and Agata Lapedriza and Aude Oliva and Antonio Torralba},
      year={2015},
      eprint={1412.6856},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1412.6856}, 
}


@inproceedings{aaai.v33i01.33016309,
author = {Dalvi, Fahim and Durrani, Nadir and Sajjad, Hassan and Belinkov, Yonatan and Bau, Anthony and Glass, James},
title = {What is one grain of sand in the desert? analyzing individual neurons in deep NLP models},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33016309},
doi = {10.1609/aaai.v33i01.33016309},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {774},
numpages = {9},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@misc{simonyan2014deepinsideconvolutionalnetworks,
      title={Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}, 
      author={Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
      year={2014},
      eprint={1312.6034},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1312.6034}, 
}

@inproceedings{ZeilerF14,
  author       = {Matthew D. Zeiler and
                  Rob Fergus},
  editor       = {David J. Fleet and
                  Tom{\'{a}}s Pajdla and
                  Bernt Schiele and
                  Tinne Tuytelaars},
  title        = {Visualizing and Understanding Convolutional Networks},
  booktitle    = {Computer Vision - {ECCV} 2014 - 13th European Conference, Zurich,
                  Switzerland, September 6-12, 2014, Proceedings, Part {I}},
  series       = {Lecture Notes in Computer Science},
  volume       = {8689},
  pages        = {818--833},
  publisher    = {Springer},
  year         = {2014},
  url          = {https://doi.org/10.1007/978-3-319-10590-1\_53},
  doi          = {10.1007/978-3-319-10590-1\_53},
  timestamp    = {Sat, 30 Sep 2023 09:39:19 +0200},
  biburl       = {https://dblp.org/rec/conf/eccv/ZeilerF14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{neural_response_interp,
  author={Khakzar, Ashkan and Baselizadeh, Soroosh and Khanduja, Saurabh and Rupprecht, Christian and Kim, Seong Tae and Navab, Nassir},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Neural Response Interpretation through the Lens of Critical Pathways}, 
  year={2021},
  volume={},
  number={},
  pages={13523-13533},
  keywords={Gradient methods;Computer vision;Computer network reliability;Neurons;Pattern recognition;Reliability;Object recognition},
  doi={10.1109/CVPR46437.2021.01332}}



@misc{davies2024cognitiverevolutioninterpretabilityexplaining,
      title={The Cognitive Revolution in Interpretability: From Explaining Behavior to Interpreting Representations and Algorithms}, 
      author={Adam Davies and Ashkan Khakzar},
      year={2024},
      eprint={2408.05859},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.05859}, 
}

@article{blumer1987occam,
  title={Occam's razor},
  author={Blumer, Anselm and Ehrenfeucht, Andrzej and Haussler, David and Warmuth, Manfred K},
  journal={Information processing letters},
  volume={24},
  number={6},
  pages={377--380},
  year={1987},
  publisher={Elsevier}
}


@inproceedings{DBLP:conf/iclr/HoltzmanBDFC20,
  author    = {Ari Holtzman and
               Jan Buys and
               Li Du and
               Maxwell Forbes and
               Yejin Choi},
  title     = {The Curious Case of Neural Text Degeneration},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=rygGQyrFvH},
  timestamp = {Thu, 21 Jan 2021 17:36:46 +0100},
  biburl    = {https://dblp.org/rec/conf/iclr/HoltzmanBDFC20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v97-kool19a,
  title = 	 {Stochastic Beams and Where To Find Them: The {G}umbel-Top-k Trick for Sampling Sequences Without Replacement},
  author =       {Kool, Wouter and Van Hoof, Herke and Welling, Max},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3499--3508},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/kool19a/kool19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/kool19a.html},
  abstract = 	 {The well-known Gumbel-Max trick for sampling from a categorical distribution can be extended to sample $k$ elements without replacement. We show how to implicitly apply this ’Gumbel-Top-$k$’ trick on a factorized distribution over sequences, allowing to draw exact samples without replacement using a Stochastic Beam Search. Even for exponentially large domains, the number of model evaluations grows only linear in $k$ and the maximum sampled sequence length. The algorithm creates a theoretical connection between sampling and (deterministic) beam search and can be used as a principled intermediate alternative. In a translation task, the proposed method compares favourably against alternatives to obtain diverse yet good quality translations. We show that sequences sampled without replacement can be used to construct low-variance estimators for expected sentence-level BLEU score and model entropy.}
}

@misc{josifoski2023flows,
      title={Flows: Building Blocks of Reasoning and Collaborating AI}, 
      author={Martin Josifoski and Lars Klein and Maxime Peyrard and Yifei Li and Saibo Geng and Julian Paul Schnitzler and Yuxing Yao and Jiheng Wei and Debjit Paul and Robert West},
      year={2023},
      eprint={2308.01285},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{šakota2024flyswat,
      title={Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling}, 
      author={Marija Šakota and Maxime Peyrard and Robert West},
      booktitle={Proceedings of The International ACM Conference on Web Search and Data Mining (WSDM)},
      year={2024}
}

@misc{lee2023orchestrallm,
      title={OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking}, 
      author={Chia-Hsuan Lee and Hao Cheng and Mari Ostendorf},
      year={2023},
      eprint={2311.09758},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ramírez2023cache,
      title={{Cache \& Distil: Optimising API Calls to Large Language Models}}, 
      author={Guillem Ramírez and Matthias Lindemann and Alexandra Birch and Ivan Titov},
      year={2023},
      eprint={2310.13561},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yue2023large,
      title={Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning}, 
      author={Murong Yue and Jie Zhao and Min Zhang and Liang Du and Ziyu Yao},
      year={2023},
      eprint={2310.03094},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ashkboos2023quik,
      title={QUIK: Towards End-to-End 4-Bit Inference on Generative Large Language Models}, 
      author={Saleh Ashkboos and Ilia Markov and Elias Frantar and Tingxuan Zhong and Xincheng Wang and Jie Ren and Torsten Hoefler and Dan Alistarh},
      year={2023},
      eprint={2310.09259},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{xiao2023smoothquant,
      title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models}, 
      author={Guangxuan Xiao and Ji Lin and Mickael Seznec and Hao Wu and Julien Demouth and Song Han},
      year={2023},
      eprint={2211.10438},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{dettmers2023qlora,
      title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
      author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
      year={2023},
      eprint={2305.14314},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{frantar2023gptq,
      title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers}, 
      author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
      year={2023},
      eprint={2210.17323},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{dettmers2022llmint8,
      title={LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale}, 
      author={Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer},
      year={2022},
      eprint={2208.07339},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yao2023zeroquantv2,
      title={ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation}, 
      author={Zhewei Yao and Xiaoxia Wu and Cheng Li and Stephen Youn and Yuxiong He},
      year={2023},
      eprint={2303.08302},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{barad2023leveraging,
      title={Leveraging Speculative Sampling and KV-Cache Optimizations Together for Generative AI using OpenVINO}, 
      author={Haim Barad and Ekaterina Aidova and Yury Gorbachev},
      year={2023},
      eprint={2311.04951},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{song2023powerinfer,
      title={PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU}, 
      author={Yixin Song and Zeyu Mi and Haotong Xie and Haibo Chen},
      year={2023},
      eprint={2312.12456},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{cheng2024exploring,
      title={Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects}, 
      author={Yuheng Cheng and Ceyao Zhang and Zhengwen Zhang and Xiangrui Meng and Sirui Hong and Wenhao Li and Zihao Wang and Zekai Wang and Feng Yin and Junhua Zhao and Xiuqiang He},
      year={2024},
      eprint={2401.03428},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{li2023chain,
      title={Chain of Code: Reasoning with a Language Model-Augmented Code Emulator}, 
      author={Chengshu Li and Jacky Liang and Andy Zeng and Xinyun Chen and Karol Hausman and Dorsa Sadigh and Sergey Levine and Li Fei-Fei and Fei Xia and Brian Ichter},
      year={2023},
      eprint={2312.04474},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2023dynamic,
      title={Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization}, 
      author={Zijun Liu and Yanzhe Zhang and Peng Li and Yang Liu and Diyi Yang},
      year={2023},
      eprint={2310.02170},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2023igniting,
      title={Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents}, 
      author={Zhuosheng Zhang and Yao Yao and Aston Zhang and Xiangru Tang and Xinbei Ma and Zhiwei He and Yiming Wang and Mark Gerstein and Rui Wang and Gongshen Liu and Hai Zhao},
      year={2023},
      eprint={2311.11797},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{dasgupta2023collaborating,
      title={Collaborating with language models for embodied reasoning}, 
      author={Ishita Dasgupta and Christine Kaeser-Chen and Kenneth Marino and Arun Ahuja and Sheila Babayan and Felix Hill and Rob Fergus},
      year={2023},
      eprint={2302.00763},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hu2023enabling,
      title={Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach}, 
      author={Bin Hu and Chenyang Zhao and Pu Zhang and Zihao Zhou and Yuanhang Yang and Zenglin Xu and Bin Liu},
      year={2023},
      eprint={2306.03604},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{ning2023skeletonofthought,
      title={Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding}, 
      author={Xuefei Ning and Zinan Lin and Zixuan Zhou and Zifu Wang and Huazhong Yang and Yu Wang},
      year={2023},
      eprint={2307.15337},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{delcorro2023skipdecode,
      title={SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference}, 
      author={Luciano Del Corro and Allie Del Giorno and Sahaj Agarwal and Bin Yu and Ahmed Awadallah and Subhabrata Mukherjee},
      year={2023},
      eprint={2307.02628},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{abs-1811-03115,
  author       = {Mitchell Stern and
                  Noam Shazeer and
                  Jakob Uszkoreit},
  title        = {Blockwise Parallel Decoding for Deep Autoregressive Models},
  journal      = {CoRR},
  volume       = {abs/1811.03115},
  year         = {2018},
  url          = {http://arxiv.org/abs/1811.03115},
  eprinttype    = {arXiv},
  eprint       = {1811.03115},
  timestamp    = {Thu, 22 Nov 2018 17:58:30 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1811-03115.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{kim2023speculative,
      title={Speculative Decoding with Big Little Decoder}, 
      author={Sehoon Kim and Karttikeya Mangalam and Suhong Moon and Jitendra Malik and Michael W. Mahoney and Amir Gholami and Kurt Keutzer},
      year={2023},
      eprint={2302.07863},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M and Nasrabadi, Nasser M},
  volume={4},
  number={4},
  year={2006},
  publisher={Springer}
}


@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}


@misc{chen2023accelerating,
      title={Accelerating Large Language Model Decoding with Speculative Sampling}, 
      author={Charlie Chen and Sebastian Borgeaud and Geoffrey Irving and Jean-Baptiste Lespiau and Laurent Sifre and John Jumper},
      year={2023},
      eprint={2302.01318},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{NEURIPS2022_6fac9e31,
 author = {Schuster, Tal and Fisch, Adam and Gupta, Jai and Dehghani, Mostafa and Bahri, Dara and Tran, Vinh and Tay, Yi and Metzler, Donald},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {17456--17472},
 publisher = {Curran Associates, Inc.},
 title = {Confident Adaptive Language Modeling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/6fac9e316a4ae75ea244ddcef1982c71-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@misc{pope2022efficiently,
      title={Efficiently Scaling Transformer Inference}, 
      author={Reiner Pope and Sholto Douglas and Aakanksha Chowdhery and Jacob Devlin and James Bradbury and Anselm Levskaya and Jonathan Heek and Kefan Xiao and Shivani Agrawal and Jeff Dean},
      year={2022},
      eprint={2211.05102},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{Li-2022-DiffusionLM,
  title={Diffusion-LM Improves Controllable Text Generation},
  author={Xiang Lisa Li and John Thickstun and Ishaan Gulrajani and Percy Liang and Tatsunori Hashimoto},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.14217}
}

@article{abs-2107-03006,
  author       = {Jacob Austin and
                  Daniel D. Johnson and
                  Jonathan Ho and
                  Daniel Tarlow and
                  Rianne van den Berg},
  title        = {Structured Denoising Diffusion Models in Discrete State-Spaces},
  journal      = {CoRR},
  volume       = {abs/2107.03006},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.03006},
  eprinttype    = {arXiv},
  eprint       = {2107.03006},
  timestamp    = {Mon, 25 Oct 2021 07:55:50 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2107-03006.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{xiao2023survey,
      title={A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond}, 
      author={Yisheng Xiao and Lijun Wu and Junliang Guo and Juntao Li and Min Zhang and Tao Qin and Tie-yan Liu},
      year={2023},
      eprint={2204.09269},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hong2024flashdecoding,
      title={FlashDecoding++: Faster Large Language Model Inference on GPUs}, 
      author={Ke Hong and Guohao Dai and Jiaming Xu and Qiuli Mao and Xiuhong Li and Jun Liu and Kangdi Chen and Yuhan Dong and Yu Wang},
      year={2024},
      eprint={2311.01282},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{salem2023ut5,
      title={UT5: Pretraining Non autoregressive T5 with unrolled denoising}, 
      author={Mahmoud G. Salem and Jiayu Ye and Chu-Cheng Lin and Frederick Liu},
      year={2023},
      eprint={2311.08552},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Touvron2023LLaMAOA,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aur'elien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.13971}
}

@article{DBLP:journals/corr/abs-2112-10752,
  author       = {Robin Rombach and
                  Andreas Blattmann and
                  Dominik Lorenz and
                  Patrick Esser and
                  Bj{\"{o}}rn Ommer},
  title        = {High-Resolution Image Synthesis with Latent Diffusion Models},
  journal      = {CoRR},
  volume       = {abs/2112.10752},
  year         = {2021},
  url          = {https://arxiv.org/abs/2112.10752},
  eprinttype    = {arXiv},
  eprint       = {2112.10752},
  timestamp    = {Tue, 04 Jan 2022 15:59:27 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2112-10752.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{searnn2018leblond,
  author    = {Leblond, R\'emi and
               Alayrac, Jean-Baptiste and
               Osokin, Anton and
               Lacoste-Julien, Simon},
  title     = {SEARNN: Training RNNs with global-local losses},
  booktitle = {ICLR},
  year      = {2018},
}

@inproceedings{chen2019reinforcement,
    author    = {Chen, Yu and Wu, Lingfei and Zaki, Mohammed J.},
    title     = {Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation},
    booktitle = {Proceedings of the 8th International Conference on Learning Representations},
    month = {Apr. 26-30,},
    year      = {2020}
}

@article{BahdanauBXGLPCB16,
  author       = {Dzmitry Bahdanau and
                  Philemon Brakel and
                  Kelvin Xu and
                  Anirudh Goyal and
                  Ryan Lowe and
                  Joelle Pineau and
                  Aaron C. Courville and
                  Yoshua Bengio},
  title        = {An Actor-Critic Algorithm for Sequence Prediction},
  journal      = {CoRR},
  volume       = {abs/1607.07086},
  year         = {2016},
  url          = {http://arxiv.org/abs/1607.07086},
  eprinttype    = {arXiv},
  eprint       = {1607.07086},
  timestamp    = {Mon, 13 Aug 2018 16:48:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BahdanauBXGLPCB16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{abs-1905-10617,
  author       = {Tianxing He and
                  Jingzhao Zhang and
                  Zhiming Zhou and
                  James R. Glass},
  title        = {Quantifying Exposure Bias for Neural Language Generation},
  journal      = {CoRR},
  volume       = {abs/1905.10617},
  year         = {2019},
  url          = {http://arxiv.org/abs/1905.10617},
  eprinttype    = {arXiv},
  eprint       = {1905.10617},
  timestamp    = {Tue, 23 Jul 2019 16:55:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1905-10617.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{abs-1910-11235,
  author       = {Yifan Xu and
                  Kening Zhang and
                  Haoyu Dong and
                  Yuezhou Sun and
                  Wenlong Zhao and
                  Zhuowen Tu},
  title        = {Rethinking Exposure Bias In Language Modeling},
  journal      = {CoRR},
  volume       = {abs/1910.11235},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.11235},
  eprinttype    = {arXiv},
  eprint       = {1910.11235},
  timestamp    = {Mon, 17 Jan 2022 08:31:24 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-11235.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NIPS2015_e995f98d,
 author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/e995f98d56967d946471af29d7bf99f1-Paper.pdf},
 volume = {28},
 year = {2015}
}


@inproceedings{RanzatoCAZ15,
  author       = {Marc'Aurelio Ranzato and
                  Sumit Chopra and
                  Michael Auli and
                  Wojciech Zaremba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Sequence Level Training with Recurrent Neural Networks},
  booktitle    = {4th International Conference on Learning Representations, {ICLR} 2016,
                  San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year         = {2016},
  url          = {http://arxiv.org/abs/1511.06732},
  timestamp    = {Thu, 25 Jul 2019 14:25:39 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/RanzatoCAZ15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{welleck2019neural,
    title={Neural Text Generation with Unlikelihood Training},
    author={Sean Welleck and Ilia Kulikov and Stephen Roller and Emily Dinan and Kyunghyun Cho and Jason Weston},
    year={2019},
    eprint={1908.04319},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{NIPS2014_a14ac55a,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sequence to Sequence Learning with Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf},
 volume = {27},
 year = {2014}
}


@inproceedings{geng-etal-2023-grammar,
    title = "Grammar-Constrained Decoding for Structured {NLP} Tasks without Finetuning",
    author = "Geng, Saibo  and
      Josifoski, Martin  and
      Peyrard, Maxime  and
      West, Robert",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.674",
    doi = "10.18653/v1/2023.emnlp-main.674",
    pages = "10932--10952",
    abstract = "Despite their impressive performance, large language models (LMs) still struggle with reliably generating complex output structures when not finetuned to follow the required output format exactly. To address this issue, grammar-constrained decoding (GCD) can be used to control the generation of LMs, guaranteeing that the output follows a given structure. Most existing GCD methods are, however, limited to specific tasks, such as parsing or code generation. In this work, we demonstrate that formal grammars can describe the output space for a much wider range of tasks and argue that GCD can serve as a unified framework for structured NLP tasks in general. For increased flexibility, we introduce input-dependent grammars, which allow the grammar to depend on the input and thus enable the generation of different output structures for different inputs. We then empirically demonstrate the power and flexibility of GCD-enhanced LMs on (1) information extraction, (2) entity disambiguation, and (3) constituency parsing. Our results indicate that grammar-constrained LMs substantially outperform unconstrained LMs or even beat task-specific finetuned models. Grammar constraints thus hold great promise for harnessing off-the-shelf LMs for a wide range of structured NLP tasks, especially where training data is scarce or finetuning is expensive. Code and data: https://github.com/epfl-dlab/GCD.",
}

@misc{roy2022benchclamp,
	archiveprefix = {arXiv},
	author = {Subhro Roy and Sam Thomson and Tongfei Chen and Richard Shin and Adam Pauls and Jason Eisner and Benjamin Van Durme},
	eprint = {2206.10668},
	primaryclass = {cs.CL},
	title = {BenchCLAMP: A Benchmark for Evaluating Language Models on Semantic Parsing},
	year = {2022}
}

@article{WilliamsZipser:89nc,
  added-at = {2016-05-31T17:20:20.000+0200},
  author = {Williams, R. J. and Zipser, D.},
  biburl = {https://www.bibsonomy.org/bibtex/241406f69681b40d610664492e3dc4d1c/ericrauch},
  citeulike-article-id = {2374775},
  interhash = {b0a3fc38f35d06d9ec9c135cb7d5798d},
  intrahash = {41406f69681b40d610664492e3dc4d1c},
  journal = {Neural Computation},
  keywords = {thema:recurrent_neural_networks},
  number = 2,
  pages = {270--280},
  priority = {2},
  timestamp = {2016-05-31T17:20:20.000+0200},
  title = {A learning algorithm for continually running fully recurrent networks},
  volume = 1,
  year = 1989
}


@misc{bailin_wang2023grammar,
      title={Grammar Prompting for Domain-Specific Language Generation with Large Language Models},
      author={Bailin Wang and Zi Wang and Xuezhi Wang and Yuan Cao and Rif A. Saurous and Yoon Kim},
      year={2023},
      eprint={2305.19234},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{decao2021autoregressive,
  author    = {Nicola {De Cao} and
               Gautier Izacard and
               Sebastian Riedel and
               Fabio Petroni},
  title     = {Autoregressive Entity Retrieval},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=5k8F6UU39V},
}

@misc{stengeleskin2023zero,
	archiveprefix = {arXiv},
	author = {Elias Stengel-Eskin and Kyle Rawlins and Benjamin Van Durme},
	eprint = {2306.00824},
	primaryclass = {cs.CL},
	title = {Zero and Few-shot Semantic Parsing with Ambiguous Inputs},
	year = {2023}}

@inproceedings{meister-etal-2023-efficacy,
    title = "On the Efficacy of Sampling Adapters",
    author = "Meister, Clara  and
      Pimentel, Tiago  and
      Malagutti, Luca  and
      Wilcox, Ethan  and
      Cotterell, Ryan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.80",
    doi = "10.18653/v1/2023.acl-long.80",
    pages = "1437--1455",
}

@misc{fernando2023promptbreeder,
      title={Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution}, 
      author={Chrisantha Fernando and Dylan Banarse and Henryk Michalewski and Simon Osindero and Tim Rocktäschel},
      year={2023},
      eprint={2309.16797},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@Article{FunSearch2023,
  author  = {Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Balog, Matej and Kumar, M. Pawan and Dupont, Emilien and Ruiz, Francisco J. R. and Ellenberg, Jordan and Wang, Pengming and Fawzi, Omar and Kohli, Pushmeet and Fawzi, Alhussein},
  journal = {Nature},
  title   = {Mathematical discoveries from program search with large language models},
  year    = {2023},
  doi     = {10.1038/s41586-023-06924-6}
}

@misc{huijben2022review,
      title={A Review of the Gumbel-max Trick and its Extensions for Discrete Stochasticity in Machine Learning}, 
      author={Iris A. M. Huijben and Wouter Kool and Max B. Paulus and Ruud J. G. van Sloun},
      year={2022},
      eprint={2110.01515},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{li2023camel,
  title={Camel: Communicative agents for" mind" exploration of large scale language model society},
  author={Li, Guohao and Hammoud, Hasan Abed Al Kader and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard},
  journal={arXiv preprint arXiv:2303.17760},
  year={2023}
}

@article{Lu2023ChameleonPC,
  title={Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models},
  author={Pan Lu and Baolin Peng and Hao Cheng and Michel Galley and Kai-Wei Chang and Ying Nian Wu and Song-Chun Zhu and Jianfeng Gao},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.09842}
}

@article{DBLP:journals/corr/abs-2308-08155,
  author       = {Qingyun Wu and
                  Gagan Bansal and
                  Jieyu Zhang and
                  Yiran Wu and
                  Shaokun Zhang and
                  Erkang Zhu and
                  Beibin Li and
                  Li Jiang and
                  Xiaoyun Zhang and
                  Chi Wang},
  title        = {AutoGen: Enabling Next-Gen {LLM} Applications via Multi-Agent Conversation
                  Framework},
  journal      = {CoRR},
  volume       = {abs/2308.08155},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.08155},
  doi          = {10.48550/ARXIV.2308.08155},
  eprinttype    = {arXiv},
  eprint       = {2308.08155},
  timestamp    = {Thu, 24 Aug 2023 12:30:27 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-08155.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{li2023collaborative,
      title={Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation}, 
      author={Qintong Li and Leyang Cui and Lingpeng Kong and Wei Bi},
      year={2023},
      eprint={2310.19740},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{babyagi2023,
  title = {BabyAGI},
  author = {Yohei Nakajima},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/yoheinakajima/babyagi}},
}


@misc{autogpt2023,
  title = {AutoGPT},
  author = {Toran Bruce Richards},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/Significant-Gravitas/Auto-GPT}},
}

@misc{wu2024oscopilot,
      title={OS-Copilot: Towards Generalist Computer Agents with Self-Improvement}, 
      author={Zhiyong Wu and Chengcheng Han and Zichen Ding and Zhenmin Weng and Zhoumianze Liu and Shunyu Yao and Tao Yu and Lingpeng Kong},
      year={2024},
      eprint={2402.07456},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{mialon2023gaia,
      title={GAIA: a benchmark for General AI Assistants}, 
      author={Grégoire Mialon and Clémentine Fourrier and Craig Swift and Thomas Wolf and Yann LeCun and Thomas Scialom},
      year={2023},
      eprint={2311.12983},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{kim2024meganno,
      title={MEGAnno+: A Human-LLM Collaborative Annotation System}, 
      author={Hannah Kim and Kushan Mitra and Rafael Li Chen and Sajjadur Rahman and Dan Zhang},
      year={2024},
      eprint={2402.18050},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{DBLP:journals/corr/abs-2308-00352,
  author       = {Sirui Hong and
                  Xiawu Zheng and
                  Jonathan Chen and
                  Yuheng Cheng and
                  Jinlin Wang and
                  Ceyao Zhang and
                  Zili Wang and
                  Steven Ka Shing Yau and
                  Zijuan Lin and
                  Liyang Zhou and
                  Chenyu Ran and
                  Lingfeng Xiao and
                  Chenglin Wu},
  title        = {MetaGPT: Meta Programming for Multi-Agent Collaborative Framework},
  journal      = {CoRR},
  volume       = {abs/2308.00352},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.00352},
  doi          = {10.48550/ARXIV.2308.00352},
  eprinttype    = {arXiv},
  eprint       = {2308.00352},
  timestamp    = {Mon, 21 Aug 2023 17:38:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-00352.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wang2023augmenting,
    author = {Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
    title = {Augmenting Language Models with Long-Term Memory},
    booktitle = {NeurIPS 2023},
    year = {2023},
    month = {October},
    url = {https://www.microsoft.com/en-us/research/publication/language-models-augmented-with-decoupled-memory/},
}

@misc{du2024anytool,
      title={AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls}, 
      author={Yu Du and Fangyun Wei and Hongyang Zhang},
      year={2024},
      eprint={2402.04253},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Shen2023HuggingGPTSA,
  title={HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace},
  author={Yongliang Shen and Kaitao Song and Xu Tan and Dong Sheng Li and Weiming Lu and Yue Ting Zhuang},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.17580}
}

@misc{marvin2023,
  title = {Marvin},
  author = {PrefectHQ},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/PrefectHQ/marvin}},
}

@misc{langchain2022,
  title = {LangChain},
  author = {Harrison Chase},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/hwchase17/langchain}},
}

@inproceedings{NEURIPS2020_6b493230,
 author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {9459--9474},
 publisher = {Curran Associates, Inc.},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{tacl_a_00519,
    author = {Sajjad, Hassan and Durrani, Nadir and Dalvi, Fahim},
    title = "{Neuron-level Interpretation of Deep NLP Models: A Survey}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {10},
    pages = {1285-1303},
    year = {2022},
    month = {11},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00519},
    url = {https://doi.org/10.1162/tacl\_a\_00519},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00519/2060745/tacl\_a\_00519.pdf},
}


@inproceedings{ijcai2021p537,
  title     = {Laughing Heads: Can Transformers Detect What Makes a Sentence Funny?},
  author    = {Peyrard, Maxime and Borges, Beatriz and Gligorić, Kristina and West, Robert},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on
               Artificial Intelligence, {IJCAI-21}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Zhi-Hua Zhou},
  pages     = {3899--3905},
  year      = {2021},
  month     = {8},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2021/537},
  url       = {https://doi.org/10.24963/ijcai.2021/537},
}


@article{bpe_origin, 
    author = {Gage, Philip}, 
    title = {A new algorithm for data compression}, 
    year = {1994}, 
    issue_date = {Feb. 1994}, 
    publisher = {R \& D Publications, Inc.}, 
    address = {USA}, 
    volume = {12}, 
    number = {2}, 
    issn = {0898-9788}, 
    journal = {C Users J.},
    month = {feb}, 
    pages = {23–38}, 
    numpages = {16} 
}

@misc{gao2024retrievalaugmented,
      title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
      author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Qianyu Guo and Meng Wang and Haofen Wang},
      year={2024},
      eprint={2312.10997},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{khattab2023dspy,
      title={DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines}, 
      author={Omar Khattab and Arnav Singhvi and Paridhi Maheshwari and Zhiyuan Zhang and Keshav Santhanam and Sri Vardhamanan and Saiful Haq and Ashutosh Sharma and Thomas T. Joshi and Hanna Moazam and Heather Miller and Matei Zaharia and Christopher Potts},
      year={2023},
      eprint={2310.03714},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhou2023leasttomost,
      title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models}, 
      author={Denny Zhou and Nathanael Schärli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Claire Cui and Olivier Bousquet and Quoc Le and Ed Chi},
      year={2023},
      eprint={2205.10625},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{JMLR:v20:18-598,
  author  = {Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},
  title   = {Neural Architecture Search: A Survey},
  journal = {Journal of Machine Learning Research},
  year    = {2019},
  volume  = {20},
  number  = {55},
  pages   = {1--21},
  url     = {http://jmlr.org/papers/v20/18-598.html}
}

@misc{chen2024selfplay,
      title={Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models}, 
      author={Zixiang Chen and Yihe Deng and Huizhuo Yuan and Kaixuan Ji and Quanquan Gu},
      year={2024},
      eprint={2401.01335},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{OpenAI2023GPT4TR,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.08774}
}

@inproceedings{su2022a,
  title={A Contrastive Framework for Neural Text Generation},
  author={Yixuan Su and Tian Lan and Yan Wang and Dani Yogatama and Lingpeng Kong and Nigel Collier},
  booktitle={Advances in Neural Information Processing Systems},
  editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
  year={2022},
  url={https://openreview.net/forum?id=V88BafmH9Pj}
}
 
@article{10.1162/tacl_a_00536,
    author = {Meister, Clara and Pimentel, Tiago and Wiher, Gian and Cotterell, Ryan},
    title = "{Locally Typical Sampling}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {11},
    pages = {102-121},
    year = {2023},
    month = {01},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00536},
    url = {https://doi.org/10.1162/tacl\_a\_00536},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00536/2067865/tacl\_a\_00536.pdf},
}

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{DBLP:journals/corr/abs-2302-07842,
  author       = {Gr{\'{e}}goire Mialon and
                  Roberto Dess{\`{\i}} and
                  Maria Lomeli and
                  Christoforos Nalmpantis and
                  Ramakanth Pasunuru and
                  Roberta Raileanu and
                  Baptiste Rozi{\`{e}}re and
                  Timo Schick and
                  Jane Dwivedi{-}Yu and
                  Asli Celikyilmaz and
                  Edouard Grave and
                  Yann LeCun and
                  Thomas Scialom},
  title        = {Augmented Language Models: a Survey},
  journal      = {CoRR},
  volume       = {abs/2302.07842},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.07842},
  doi          = {10.48550/ARXIV.2302.07842},
  eprinttype    = {arXiv},
  eprint       = {2302.07842},
  timestamp    = {Mon, 20 Feb 2023 14:27:28 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-07842.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{chandler2022semiotics,
  title={Semiotics: the basics},
  author={Chandler, Daniel},
  year={2022},
  publisher={Routledge}
}

@book{birner2012introduction,
  title={Introduction to pragmatics},
  author={Birner, Betty J},
  year={2012},
  publisher={John Wiley \& Sons}
}


@inproceedings{horvitz1999principles,
  title={Principles of mixed-initiative user interfaces},
  author={Horvitz, Eric},
  booktitle={Proceedings of the SIGCHI conference on Human Factors in Computing Systems},
  pages={159--166},
  year={1999}
}

@article{DBLP:journals/corr/abs-2308-08998,
  author       = {{\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
                  Tom Le Paine and
                  Srivatsan Srinivasan and
                  Ksenia Konyushkova and
                  Lotte Weerts and
                  Abhishek Sharma and
                  Aditya Siddhant and
                  Alex Ahern and
                  Miaosen Wang and
                  Chenjie Gu and
                  Wolfgang Macherey and
                  Arnaud Doucet and
                  Orhan Firat and
                  Nando de Freitas},
  title        = {Reinforced Self-Training (ReST) for Language Modeling},
  journal      = {CoRR},
  volume       = {abs/2308.08998},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.08998},
  doi          = {10.48550/ARXIV.2308.08998},
  eprinttype    = {arXiv},
  eprint       = {2308.08998},
  timestamp    = {Mon, 28 Aug 2023 09:39:40 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-08998.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{chen2023fireact,
      title={FireAct: Toward Language Agent Fine-tuning}, 
      author={Baian Chen and Chang Shu and Ehsan Shareghi and Nigel Collier and Karthik Narasimhan and Shunyu Yao},
      year={2023},
      eprint={2310.05915},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{NIPS2016_2f885d0f,
 author = {Norouzi, Mohammad and Bengio, Samy and Chen, Zhifeng and Jaitly, Navdeep and Schuster, Mike and Wu, Yonghui and Schuurmans, Dale},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Reward Augmented Maximum Likelihood for Neural Structured Prediction},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/2f885d0fbe2e131bfc9d98363e55d1d4-Paper.pdf},
 volume = {29},
 year = {2016}
}



@misc{rafailov2023direct,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model}, 
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2023},
      eprint={2305.18290},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@InProceedings{pmlr-v97-collobert19a,
  title = 	 {A fully differentiable beam search decoder},
  author =       {Collobert, Ronan and Hannun, Awni and Synnaeve, Gabriel},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1341--1350},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/collobert19a/collobert19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/collobert19a.html},
  abstract = 	 {We introduce a new beam search decoder that is fully differentiable, making it possible to optimize at training time through the inference procedure. Our decoder allows us to combine models which operate at different granularities (e.g. acoustic and language models). It can be used when target sequences are not aligned to input sequences by considering all possible alignments between the two. We demonstrate our approach scales by applying it to speech recognition, jointly training acoustic and word-level language models. The system is end-to-end, with gradients flowing through the whole architecture from the word-level transcriptions. Recent research efforts have shown that deep neural networks with attention-based mechanisms can successfully train an acoustic model from the final transcription, while implicitly learning a language model. Instead, we show that it is possible to discriminatively train an acoustic model jointly with an <em>explicit</em> and possibly pre-trained language model.}
}


@inproceedings{coulom2006efficient,
  title={Efficient selectivity and backup operators in Monte-Carlo tree search},
  author={Coulom, R{\'e}mi},
  booktitle={International conference on computers and games},
  pages={72--83},
  year={2006},
  organization={Springer}
}

@article{shannon,
  added-at = {2021-09-19T18:40:37.000+0200},
  author = {Shannon, Claude Elwood},
  biburl = {https://www.bibsonomy.org/bibtex/29f88587b33c82f692b61d129eb2f2517/steschum},
  interhash = {754130207906fcec16a53d330eeff348},
  intrahash = {9f88587b33c82f692b61d129eb2f2517},
  journal = {The Bell System Technical Journal},
  keywords = {imported},
  pages = {379--423},
  timestamp = {2021-09-19T18:41:56.000+0200},
  title = {A Mathematical Theory of Communication},
  url = {http://plan9.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf},
  urldate = {2003-04-22},
  volume = 27,
  year = 1948
}

@misc{wu2023multimodal,
      title={Multimodal Large Language Models: A Survey}, 
      author={Jiayang Wu and Wensheng Gan and Zefeng Chen and Shicheng Wan and Philip S. Yu},
      year={2023},
      eprint={2311.13165},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{reed2022generalist,
      title={A Generalist Agent}, 
      author={Scott Reed and Konrad Zolna and Emilio Parisotto and Sergio Gomez Colmenarejo and Alexander Novikov and Gabriel Barth-Maron and Mai Gimenez and Yury Sulsky and Jackie Kay and Jost Tobias Springenberg and Tom Eccles and Jake Bruce and Ali Razavi and Ashley Edwards and Nicolas Heess and Yutian Chen and Raia Hadsell and Oriol Vinyals and Mahyar Bordbar and Nando de Freitas},
      year={2022},
      eprint={2205.06175},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{floridi,
    author = {Floridi, Luciano},
    year = {2009},
    month = {01},
    pages = {13-53},
    title = {Philosophical Conceptions of Information},
    volume = {5363},
    isbn = {978-3-642-00658-6},
    journal = {Lecture Notes in Computer Science - LNCS},
    doi = {10.1007/978-3-642-00659-3_2}
}

@Article{IS4SI-2017-04000,
AUTHOR = {Zhong, Yixin},
TITLE = {A Theory of Semantic Information},
JOURNAL = {Proceedings},
VOLUME = {1},
YEAR = {2017},
NUMBER = {3},
ARTICLE-NUMBER = {129},
URL = {https://www.mdpi.com/2504-3900/1/3/129},
ISSN = {2504-3900},
DOI = {10.3390/IS4SI-2017-04000}
}

@book{Fodor1975-FODTLO,
	author = {Jerry A. Fodor},
	editor = {},
	publisher = {Harvard University Press},
	title = {The Language of Thought},
	year = {1975}
}

@article{Carnap1954-CARAOO-3,
	author = {Rudolf Carnap and Yehoshua Bar{-}Hillel},
	doi = {10.2307/2268645},
	journal = {Journal of Symbolic Logic},
	number = {3},
	pages = {230--232},
	publisher = {Association for Symbolic Logic},
	title = {An Outline of a Theory of Semantic Information},
	volume = {19},
	year = {1954}
}

@misc{xiao2023llm,
      title={LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics}, 
      author={Hengjia Xiao and Peng Wang},
      year={2023},
      eprint={2312.01797},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@misc{cai2023humanintheloop,
      title={Human-in-the-Loop through Chain-of-Thought}, 
      author={Zefan Cai and Baobao Chang and Wenjuan Han},
      year={2023},
      eprint={2306.07932},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{josifoski-etal-2023-exploiting,
    title = "Exploiting Asymmetry for Synthetic Training Data Generation: {S}ynth{IE} and the Case of Information Extraction",
    author = "Josifoski, Martin  and
      Sakota, Marija  and
      Peyrard, Maxime  and
      West, Robert",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.96",
    doi = "10.18653/v1/2023.emnlp-main.96",
    pages = "1555--1574",
    abstract = "Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of 57 absolute points in micro-F1 and 79 points in macro-F1. Code, data, and models are available at anonymous.",
}


@INPROCEEDINGS{6004632,
  author={Bao, Jie and Basu, Prithwish and Dean, Mike and Partridge, Craig and Swami, Ananthram and Leland, Will and Hendler, James A.},
  booktitle={2011 IEEE Network Science Workshop}, 
  title={Towards a theory of semantic communication}, 
  year={2011},
  volume={},
  number={},
  pages={110-117},
  keywords={Semantics;Entropy;Channel coding;Probability;Receivers;Weaving},
  doi={10.1109/NSW.2011.6004632}}


@InCollection{sep-language-thought,
	author       =	{Rescorla, Michael},
	title        =	{{The Language of Thought Hypothesis}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta and Uri Nodelman},
	howpublished =	{\url{https://plato.stanford.edu/archives/win2023/entries/language-thought/}},
	year         =	{2023},
	edition      =	{{W}inter 2023},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@inproceedings{josifoski-etal-2023-language,
    title = "Language Model Decoding as Likelihood{--}Utility Alignment",
    author = "Josifoski, Martin  and
      Peyrard, Maxime  and
      Raji{\v{c}}, Frano  and
      Wei, Jiheng  and
      Paul, Debjit  and
      Hartmann, Valentin  and
      Patra, Barun  and
      Chaudhary, Vishrav  and
      Kiciman, Emre  and
      Faltings, Boi",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-eacl.107",
    doi = "10.18653/v1/2023.findings-eacl.107",
    pages = "1455--1470",
}

@misc{xie2023selfevaluation,
      title={Self-Evaluation Guided Beam Search for Reasoning}, 
      author={Yuxi Xie and Kenji Kawaguchi and Yiran Zhao and Xu Zhao and Min-Yen Kan and Junxian He and Qizhe Xie},
      year={2023},
      eprint={2305.00633},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{pml2Book,
 author = "Kevin P. Murphy",
 title = "Probabilistic Machine Learning: Advanced Topics",
 publisher = "MIT Press",
 year = 2023,
 url = "http://probml.github.io/book2"
}

@book{silvey2013optimal,
  title={Optimal design: an introduction to the theory for parameter estimation},
  author={Silvey, Samuel},
  volume={1},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{bengio2003no,
  title={No unbiased estimator of the variance of k-fold cross-validation},
  author={Bengio, Yoshua and Grandvalet, Yves},
  journal={Advances in Neural Information Processing Systems},
  volume={16},
  year={2003}
}


@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@misc{zammitmangion2024neuralmethodsamortizedinference,
      title={Neural Methods for Amortized Inference}, 
      author={Andrew Zammit-Mangion and Matthew Sainsbury-Dale and Raphaël Huser},
      year={2024},
      eprint={2404.12484},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2404.12484}, 
}

@article{ganguly2023amortized,
  title={Amortized Variational Inference: A Systematic Review},
  author={Ganguly, Ankush and Jain, Sanjana and Watchareeruetai, Ukrit},
  journal={Journal of Artificial Intelligence Research},
  volume={78},
  pages={167--215},
  year={2023}
}


@article{wu2024sample,
  title={Sample, estimate, aggregate: A recipe for causal discovery foundation models},
  author={Wu, Menghua and Bao, Yujia and Barzilay, Regina and Jaakkola, Tommi},
  journal={arXiv preprint arXiv:2402.01929},
  year={2024}
}

@article{lorch2022amortized,
  title={Amortized inference for causal structure learning},
  author={Lorch, Lars and Sussex, Scott and Rothfuss, Jonas and Krause, Andreas and Sch{\"o}lkopf, Bernhard},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={13104--13118},
  year={2022}
}

@inproceedings{lowe2022amortized,
  title={Amortized causal discovery: Learning to infer causal graphs from time-series data},
  author={L{\"o}we, Sindy and Madras, David and Zemel, Richard and Welling, Max},
  booktitle={Conference on Causal Learning and Reasoning},
  pages={509--525},
  year={2022},
  organization={PMLR}
}

@article{kim2024targeted,
  title={Targeted Cause Discovery with Data-Driven Learning},
  author={Kim, Jang-Hyun and Gibbs, Claudia Skok and Yun, Sangdoo and Song, Hyun Oh and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:2408.16218},
  year={2024}
}

@inproceedings{kelearning,
  title={Learning to Induce Causal Structure},
  author={Ke, Nan Rosemary and Chiappa, Silvia and Wang, Jane X and Bornschein, Jorg and Goyal, Anirudh and Rey, Melanie and Weber, Theophane and Botvinick, Matthew and Mozer, Michael Curtis and Rezende, Danilo Jimenez},
  booktitle={International Conference on Learning Representations}
}

@inproceedings{lopez2015towards,
  title={Towards a learning theory of cause-effect inference},
  author={Lopez-Paz, David and Muandet, Krikamol and Sch{\"o}lkopf, Bernhard and Tolstikhin, Iliya},
  booktitle={International Conference on Machine Learning},
  pages={1452--1461},
  year={2015},
  organization={PMLR}
}

@article{radev2020bayesflow,
  title={BayesFlow: Learning complex stochastic models with invertible neural networks},
  author={Radev, Stefan T and Mertens, Ulf K and Voss, Andreas and Ardizzone, Lynton and K{\"o}the, Ullrich},
  journal={IEEE transactions on neural networks and learning systems},
  volume={33},
  number={4},
  pages={1452--1466},
  year={2020},
  publisher={IEEE}
}

@article{JMLR:v21:19-322,
  author  = {Benjamin Bloem-Reddy and { Yee Whye } Teh},
  title   = {Probabilistic Symmetries and Invariant Neural Networks},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {90},
  pages   = {1--61},
  url     = {http://jmlr.org/papers/v21/19-322.html}
}


@inproceedings{BruinsmaMRFAVBH23,
  added-at = {2024-07-24T00:00:00.000+0200},
  author = {Bruinsma, Wessel P. and Markou, Stratis and Requeima, James and Foong, Andrew Y. K. and Andersson, Tom R. and Vaughan, Anna and Buonomo, Anthony and Hosking, J. Scott and Turner, Richard E.},
  biburl = {https://www.bibsonomy.org/bibtex/20d05cec37cf0430da89b5dc7086e26e6/dblp},
  booktitle = {ICLR},
  ee = {https://openreview.net/forum?id=OAsXFPBfTBh},
  interhash = {7708fe896c5a2a6fb3f8957c12c897a7},
  intrahash = {0d05cec37cf0430da89b5dc7086e26e6},
  keywords = {dblp},
  publisher = {OpenReview.net},
  timestamp = {2024-07-29T07:27:25.000+0200},
  title = {Autoregressive Conditional Neural Processes.},
  url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2023.html#BruinsmaMRFAVBH23},
  year = 2023
}

@article{huang2023practical,
  title={Practical Equivariances via Relational Conditional Neural Processes},
  author={Huang, Daolang and Haussmann, Manuel and Remes, Ulpu and John, ST and Clart{\'e}, Gr{\'e}goire and Luck, Kevin Sebastian and Kaski, Samuel and Acerbi, Luigi},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{
markou2022practical,
title={Practical Conditional Neural Process Via Tractable Dependent Predictions},
author={Stratis Markou and James Requeima and Wessel Bruinsma and Anna Vaughan and Richard E Turner},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=3pugbNqOh5m}
}

@inproceedings{Gordon:2020,
    title = {Convolutional Conditional Neural Processes},
    author = {Jonathan Gordon and Wessel P. Bruinsma and Andrew Y. K. Foong and James Requeima and Yann Dubois and Richard E. Turner},
    year = {2020},
    booktitle = {International Conference on Learning Representations},
    url = {https://openreview.net/forum?id=Skey4eBYPS}
}

@article{cranmer2020frontier,
  title={The frontier of simulation-based inference},
  author={Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30055--30062},
  year={2020},
  publisher={National Acad Sciences}
}


@inproceedings{NIPS2017_addfa9b7,
 author = {Lueckmann, Jan-Matthis and Goncalves, Pedro J and Bassetto, Giacomo and \"{O}cal, Kaan and Nonnenmacher, Marcel and Macke, Jakob H},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Flexible statistical inference for mechanistic models of neural dynamics},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/addfa9b7e234254d26e9c7f2af1005cb-Paper.pdf},
 volume = {30},
 year = {2017}
}

@InProceedings{pmlr-v89-papamakarios19a,
  title = 	 {Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows},
  author =       {Papamakarios, George and Sterratt, David and Murray, Iain},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {837--848},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/papamakarios19a/papamakarios19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/papamakarios19a.html},
  abstract = 	 {We present Sequential Neural Likelihood (SNL), a new method for Bayesian inference in simulator models, where the likelihood is intractable but simulating data from the model is possible. SNL trains an autoregressive flow on simulated data in order to learn a model of the likelihood in the region of high posterior density. A sequential training procedure guides simulations and reduces simulation cost by orders of magnitude. We show that SNL is more robust, more accurate and requires less tuning than related neural-based methods, and we discuss diagnostics for assessing calibration, convergence and goodness-of-fit.}
}

@misc{angelopoulos2023predictionpoweredinference,
      title={Prediction-Powered Inference}, 
      author={Anastasios N. Angelopoulos and Stephen Bates and Clara Fannjiang and Michael I. Jordan and Tijana Zrnic},
      year={2023},
      eprint={2301.09633},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2301.09633}, 
}

@article{chang2025amortized,
  title={Amortized Probabilistic Conditioning for Optimization, Simulation and Inference},
  author={Chang, Paul E and Loka, Nasrulloh and Huang, Daolang and Remes, Ulpu and Kaski, Samuel and Acerbi, Luigi},
  journal={28th International Conference on Artificial Intelligence and Statistics (AISTATS 2025)},
  year={2025}
}


@inproceedings{NIPS2017_f22e4747,
 author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Sets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf},
 volume = {30},
 year = {2017}
}


@article{chan2018likelihood,
  title={A likelihood-free inference framework for population genetic data using exchangeable neural networks},
  author={Chan, Jeffrey and Perrone, Valerio and Spence, Jeffrey and Jenkins, Paul and Mathieson, Sara and Song, Yun},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{chen2020neural,
  title={Neural approximate sufficient statistics for implicit models},
  author={Chen, Yanzhi and Zhang, Dinghuai and Gutmann, Michael and Courville, Aaron and Zhu, Zhanxing},
  journal={arXiv preprint arXiv:2010.10079},
  year={2020}
}

@inproceedings{chen2023learning,
  title={Is learning summary statistics necessary for likelihood-free inference?},
  author={Chen, Yanzhi and Gutmann, Michael U and Weller, Adrian},
  booktitle={International Conference on Machine Learning},
  pages={4529--4544},
  year={2023},
  organization={PMLR}
}

@misc{kobalczyk2025automatedknowledgeintegrationhumaninterpretable,
      title={Towards Automated Knowledge Integration From Human-Interpretable Representations}, 
      author={Katarzyna Kobalczyk and Mihaela van der Schaar},
      year={2025},
      eprint={2402.16105},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.16105}, 
}

@article{gonccalves2020training,
  title={Training deep neural density estimators to identify mechanistic models of neural dynamics},
  author={Gon{\c{c}}alves, Pedro J and Lueckmann, Jan-Matthis and Deistler, Michael and Nonnenmacher, Marcel and {\"O}cal, Kaan and Bassetto, Giacomo and Chintaluri, Chaitanya and Podlaski, William F and Haddad, Sara A and Vogels, Tim P and others},
  journal={Elife},
  volume={9},
  pages={e56261},
  year={2020},
  publisher={eLife Sciences Publications, Ltd}
}

@article{avecilla2022neural,
  title={Neural networks enable efficient and accurate simulation-based inference of evolutionary parameters from adaptation dynamics},
  author={Avecilla, Grace and Chuong, Julie N and Li, Fangfei and Sherlock, Gavin and Gresham, David and Ram, Yoav},
  journal={PLoS biology},
  volume={20},
  number={5},
  pages={e3001633},
  year={2022},
  publisher={Public Library of Science San Francisco, CA USA}
}


@InProceedings{pmlr-v235-gloeckler24a,
  title = 	 {All-in-one simulation-based inference},
  author =       {Gloeckler, Manuel and Deistler, Michael and Weilbach, Christian Dietrich and Wood, Frank and Macke, Jakob H.},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {15735--15766},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/gloeckler24a/gloeckler24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/gloeckler24a.html},
  abstract = 	 {Amortized Bayesian inference trains neural networks to solve stochastic inference problems using model simulations, thereby making it possible to rapidly perform Bayesian inference for any newly observed data. However, current simulation-based amortized inference methods are simulation-hungry and inflexible: They require the specification of a fixed parametric prior, simulator, and inference tasks ahead of time. Here, we present a new amortized inference method—the Simformer—which overcomes these limitations. By training a probabilistic diffusion model with transformer architectures, the Simformer outperforms current state-of-the-art amortized inference approaches on benchmark tasks and is substantially more flexible: It can be applied to models with function-valued parameters, it can handle inference scenarios with missing or unstructured data, and it can sample arbitrary conditionals of the joint distribution of parameters and data, including both posterior and likelihood. We showcase the performance and flexibility of the Simformer on simulators from ecology, epidemiology, and neuroscience, and demonstrate that it opens up new possibilities and application domains for amortized Bayesian inference on simulation-based models.}
}



@article{elsemueller2024sensitivity,
  title={Sensitivity-Aware Amortized Bayesian Inference},
  author={Lasse Elsem{\"u}ller and Hans Olischl{\"a}ger and Marvin Schmitt and Paul-Christian B{\"u}rkner and Ullrich Koethe and Stefan T. Radev},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2024},
  url={https://openreview.net/forum?id=Kxtpa9rvM0},
}

@misc{amos2023tutorialamortizedoptimization,
      title={Tutorial on amortized optimization}, 
      author={Brandon Amos},
      year={2023},
      eprint={2202.00665},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.00665}, 
}

@article{kim2019attentive,
  title={Attentive neural processes},
  author={Kim, Hyunjik and Mnih, Andriy and Schwarz, Jonathan and Garnelo, Marta and Eslami, Ali and Rosenbaum, Dan and Vinyals, Oriol and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1901.05761},
  year={2019}
}

@inproceedings{NEURIPS2020_f52db9f7,
 author = {Liu, Sulin and Sun, Xingyuan and Ramadge, Peter J and Adams, Ryan P},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {21440--21452},
 publisher = {Curran Associates, Inc.},
 title = {Task-Agnostic Amortized Inference of Gaussian Process Hyperparameters},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f52db9f7c0ae7017ee41f63c2a7353bc-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{NEURIPS2021_56c3b2c6,
 author = {Simpson, Fergus and Davies, Ian and Lalchand, Vidhi and Vullo, Alessandro and Durrande, Nicolas and Rasmussen, Carl Edward},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {10483--10495},
 publisher = {Curran Associates, Inc.},
 title = {Kernel Identification Through Transformers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/56c3b2c6ea3a83aaeeff35eeb45d700d-Paper.pdf},
 volume = {34},
 year = {2021}
}


@inproceedings{
    muller2022transformers,
    title={Transformers Can Do Bayesian Inference},
    author={Samuel M{\"u}ller and Noah Hollmann and Sebastian Pineda Arango and Josif Grabocka and Frank Hutter},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=KSugKcbNf9}
}

@InProceedings{pmlr-v202-muller23a,
  title = 	 {{PFN}s4{BO}: In-Context Learning for {B}ayesian Optimization},
  author =       {M\"{u}ller, Samuel and Feurer, Matthias and Hollmann, Noah and Hutter, Frank},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {25444--25470},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/muller23a/muller23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/muller23a.html}
}



@InProceedings{pmlr-v162-nguyen22b,
  title = 	 {Transformer Neural Processes: Uncertainty-Aware Meta Learning Via Sequence Modeling},
  author =       {Nguyen, Tung and Grover, Aditya},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {16569--16594},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/nguyen22b/nguyen22b.pdf},
  url = 	 {https://proceedings.mlr.press/v162/nguyen22b.html},
  abstract = 	 {Neural Processes (NPs) are a popular class of approaches for meta-learning. Similar to Gaussian Processes (GPs), NPs define distributions over functions and can estimate uncertainty in their predictions. However, unlike GPs, NPs and their variants suffer from underfitting and often have intractable likelihoods, which limit their applications in sequential decision making. We propose Transformer Neural Processes (TNPs), a new member of the NP family that casts uncertainty-aware meta learning as a sequence modeling problem. We learn TNPs via an autoregressive likelihood-based objective and instantiate it with a novel transformer-based architecture that respects the inductive biases inherent to the problem structure, such as invariance to the observed data points and equivariance to the unobserved points. We further design knobs within the TNP architecture to tradeoff the increase in expressivity of the decoding distribution with extra computation. Empirically, we show that TNPs achieve state-of-the-art performance on various benchmark problems, outperforming all previous NP variants on meta regression, image completion, contextual multi-armed bandits, and Bayesian optimization.}
}



@article{garnelo2018neural,
  title={Neural processes},
  author={Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J and Eslami, SM and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1807.01622},
  year={2018}
}

@inproceedings{garnelo2018conditional,
  title={Conditional neural processes},
  author={Garnelo, Marta and Rosenbaum, Dan and Maddison, Christopher and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo and Eslami, SM Ali},
  booktitle={International conference on machine learning},
  pages={1704--1713},
  year={2018},
  organization={PMLR}
}

@InProceedings{pmlr-v108-mcallester20a,
  title = 	 {Formal Limitations on the Measurement of Mutual Information},
  author =       {McAllester, David and Stratos, Karl},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {875--884},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/mcallester20a/mcallester20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/mcallester20a.html},
  abstract = 	 {Measuring mutual information from finite data is difficult. Recent work has considered variational methods maximizing a lower bound. In this paper, we prove that serious statistical limitations are inherent to any method of measuring mutual information. More specifically, we show that any distribution-free high-confidence lower bound on mutual information estimated from N samples cannot be larger than O(ln N).}
}


@inproceedings{ilse2018attention,
  title={Attention-based deep multiple instance learning},
  author={Ilse, Maximilian and Tomczak, Jakub and Welling, Max},
  booktitle={International conference on machine learning},
  pages={2127--2136},
  year={2018},
  organization={PMLR}
}


@misc{song2020understandinglimitationsvariationalmutual,
      title={Understanding the Limitations of Variational Mutual Information Estimators}, 
      author={Jiaming Song and Stefano Ermon},
      year={2020},
      eprint={1910.06222},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.06222}, 
}


@misc{tishby2000informationbottleneckmethod,
      title={The information bottleneck method}, 
      author={Naftali Tishby and Fernando C. Pereira and William Bialek},
      year={2000},
      eprint={physics/0004057},
      archivePrefix={arXiv},
      primaryClass={physics.data-an},
      url={https://arxiv.org/abs/physics/0004057}, 
}


@article{khan2007relative,
  title={Relative performance of mutual information estimation methods for quantifying the dependence among short and noisy data},
  author={Khan, Shiraj and Bandyopadhyay, Sharba and Ganguly, Auroop R and Saigal, Sunil and Erickson III, David J and Protopopescu, Vladimir and Ostrouchov, George},
  journal={Physical Review E—Statistical, Nonlinear, and Soft Matter Physics},
  volume={76},
  number={2},
  pages={026209},
  year={2007},
  publisher={APS}
}

@article{d1973tests,
  title={Tests for departure from normality. Empirical results for the distributions of b 2 and b},
  author={D'agostino, Ralph and Pearson, Egon S},
  journal={Biometrika},
  volume={60},
  number={3},
  pages={613--622},
  year={1973},
  publisher={Oxford University Press}
}

@article{jarque1987test,
  title={A test for normality of observations and regression residuals},
  author={Jarque, Carlos M and Bera, Anil K},
  journal={International Statistical Review/Revue Internationale de Statistique},
  pages={163--172},
  year={1987},
  publisher={JSTOR}
}

@article{ghasemi2012normality,
  title={Normality tests for statistical analysis: a guide for non-statisticians},
  author={Ghasemi, Asghar and Zahediasl, Saleh},
  journal={International journal of endocrinology and metabolism},
  volume={10},
  number={2},
  pages={486},
  year={2012},
  publisher={Brieflands}
}


@Article{math9070788,
    AUTHOR = {Arnastauskaitė, Jurgita and Ruzgas, Tomas and Bražėnas, Mindaugas},
    TITLE = {An Exhaustive Power Comparison of Normality Tests},
    JOURNAL = {Mathematics},
    VOLUME = {9},
    YEAR = {2021},
    NUMBER = {7},
    ARTICLE-NUMBER = {788},
    URL = {https://www.mdpi.com/2227-7390/9/7/788},
    ISSN = {2227-7390},
    ABSTRACT = {A goodness-of-fit test is a frequently used modern statistics tool. However, it is still unclear what the most reliable approach is to check assumptions about data set normality. A particular data set (especially with a small number of observations) only partly describes the process, which leaves many options for the interpretation of its true distribution. As a consequence, many goodness-of-fit statistical tests have been developed, the power of which depends on particular circumstances (i.e., sample size, outlets, etc.). With the aim of developing a more universal goodness-of-fit test, we propose an approach based on an N-metric with our chosen kernel function. To compare the power of 40 normality tests, the goodness-of-fit hypothesis was tested for 15 data distributions with 6 different sample sizes. Based on exhaustive comparative research results, we recommend the use of our test for samples of size n≥118.},
    DOI = {10.3390/math9070788}
}

@article{massey1951kolmogorov,
  title={The Kolmogorov-Smirnov test for goodness of fit},
  author={Massey Jr, Frank J},
  journal={Journal of the American statistical Association},
  volume={46},
  number={253},
  pages={68--78},
  year={1951},
  publisher={Taylor \& Francis}
}

@book{altman1990practical,
  title={Practical statistics for medical research},
  author={Altman, Douglas G},
  year={1990},
  publisher={Chapman and Hall/CRC}
}

@article{doi:10.4078/jrd.2019.26.1.5,
author = {Kwak Sang Gyu, Park Sung-Hoon},
title = {Normality Test in Clinical Research},
journal = {jrd},
volume = {26},
number = {1},
pages = {5-11},
year = {2019},
doi = {10.4078/jrd.2019.26.1.5},
URL = {http://www.e-sciencecentral.org/articles/?scid=1122089},
eprint = {http://www.e-sciencecentral.org/articles/?scid=1122089}
}

@article{das2016brief,
  title={A brief review of tests for normality},
  author={Das, Keya Rani and Imon, AHMR},
  journal={American Journal of Theoretical and Applied Statistics},
  volume={5},
  number={1},
  pages={5--12},
  year={2016}
}

@article{shapiro1965analysis,
  title={An analysis of variance test for normality (complete samples)},
  author={Shapiro, Samuel Sanford and Wilk, Martin B},
  journal={Biometrika},
  volume={52},
  number={3-4},
  pages={591--611},
  year={1965},
  publisher={Oxford University Press}
}


@article{razali2011power,
  title={Power comparisons of shapiro-wilk, kolmogorov-smirnov, lilliefors and anderson-darling tests},
  author={Razali, Nornadiah Mohd and Wah, Yap Bee and others},
  journal={Journal of statistical modeling and analytics},
  volume={2},
  number={1},
  pages={21--33},
  year={2011}
}


@InProceedings{pmlr-v97-poole19a,
  title = 	 {On Variational Bounds of Mutual Information},
  author =       {Poole, Ben and Ozair, Sherjil and Van Den Oord, Aaron and Alemi, Alex and Tucker, George},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5171--5180},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/poole19a/poole19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/poole19a.html},
  abstract = 	 {Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning, but bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks. However, the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of these new bounds for estimation and representation learning.}
}


@inproceedings{NIPS2016_cedebb6e,
 author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/cedebb6e872f539bef8c3f919874e9d7-Paper.pdf},
 volume = {29},
 year = {2016}
}

@inproceedings{NIPS2007_72da7fd6,
 author = {Nguyen, XuanLong and Wainwright, Martin J and Jordan, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/72da7fd6d1302c0a159f6436d01e9eb0-Paper.pdf},
 volume = {20},
 year = {2007}
}



@article{repr_learning,
  author       = {A{\"{a}}ron van den Oord and
                  Yazhe Li and
                  Oriol Vinyals},
  title        = {Representation Learning with Contrastive Predictive Coding},
  journal      = {CoRR},
  volume       = {abs/1807.03748},
  year         = {2018},
  url          = {http://arxiv.org/abs/1807.03748},
  eprinttype    = {arXiv},
  eprint       = {1807.03748},
  timestamp    = {Mon, 13 Aug 2018 16:48:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1807-03748.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v80-belghazi18a,
  title = 	 {Mutual Information Neural Estimation},
  author =       {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {531--540},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/belghazi18a/belghazi18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/belghazi18a.html},
  abstract = 	 {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement the Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.}
}

@article{voulodimos2018deep,
  title={Deep learning for computer vision: A brief review},
  author={Voulodimos, Athanasios and Doulamis, Nikolaos and Doulamis, Anastasios and Protopapadakis, Eftychios},
  journal={Computational intelligence and neuroscience},
  volume={2018},
  number={1},
  pages={7068349},
  year={2018},
  publisher={Wiley Online Library}
}

@article{andrychowicz2016learning,
  title={Learning to learn by gradient descent by gradient descent},
  author={Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and De Freitas, Nando},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{chen2018rise,
  title={The rise of deep learning in drug discovery},
  author={Chen, Hongming and Engkvist, Ola and Wang, Yinhai and Olivecrona, Marcus and Blaschke, Thomas},
  journal={Drug discovery today},
  volume={23},
  number={6},
  pages={1241--1250},
  year={2018},
  publisher={Elsevier}
}


@article{esteva2017dermatologist,
  title={Dermatologist-level classification of skin cancer with deep neural networks},
  author={Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
  journal={nature},
  volume={542},
  number={7639},
  pages={115--118},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{alipanahi2015predicting,
  title={Predicting the sequence specificities of DNA-and RNA-binding proteins by deep learning},
  author={Alipanahi, Babak and Delong, Andrew and Weirauch, Matthew T and Frey, Brendan J},
  journal={Nature biotechnology},
  volume={33},
  number={8},
  pages={831--838},
  year={2015},
  publisher={Nature Publishing Group}
}


@article{hinton2012deep,
  title={Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
  author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},
  journal={IEEE Signal processing magazine},
  volume={29},
  number={6},
  pages={82--97},
  year={2012},
  publisher={IEEE}
}

@article{jaiswal2019identifying,
  title={Identifying pneumonia in chest X-rays: A deep learning approach},
  author={Jaiswal, Amit Kumar and Tiwari, Prayag and Kumar, Sachin and Gupta, Deepak and Khanna, Ashish and Rodrigues, Joel JPC},
  journal={Measurement},
  volume={145},
  pages={511--518},
  year={2019},
  publisher={Elsevier}
}


@misc{hastie2017elements,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year={2017},
  publisher={Springer}
}


@article{inv_bottleneck,
  author       = {Bo Li and
                  Yifei Shen and
                  Yezhen Wang and
                  Wenzhen Zhu and
                  Colorado J. Reed and
                  Jun Zhang and
                  Dongsheng Li and
                  Kurt Keutzer and
                  Han Zhao},
  title        = {Invariant Information Bottleneck for Domain Generalization},
  journal      = {CoRR},
  volume       = {abs/2106.06333},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.06333},
  eprinttype    = {arXiv},
  eprint       = {2106.06333},
  timestamp    = {Fri, 19 Aug 2022 11:04:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-06333.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{PhysRevE.69.066138,
  title = {Estimating mutual information},
  author = {Kraskov, Alexander and St\"ogbauer, Harald and Grassberger, Peter},
  journal = {Phys. Rev. E},
  volume = {69},
  issue = {6},
  pages = {066138},
  numpages = {16},
  year = {2004},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.69.066138},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.69.066138}
}


@inproceedings{NEURIPS2023_36b80eae,
     author = {Czy\.{z}, Pawe\l{}  and Grabowski, Frederic and Vogt, Julia and Beerenwinkel, Niko and Marx, Alexander},
     booktitle = {Advances in Neural Information Processing Systems},
     editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
     pages = {16957--16990},
     publisher = {Curran Associates, Inc.},
     title = {Beyond Normal: On the Evaluation of Mutual Information Estimators},
     url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/36b80eae70ff629d667f210e13497edf-Paper-Conference.pdf},
     volume = {36},
     year = {2023}
}

@misc{long2023large,
      title={Large Language Model Guided Tree-of-Thought}, 
      author={Jieyi Long},
      year={2023},
      eprint={2305.08291},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{yao2023tree,
      title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models}, 
      author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
      year={2023},
      eprint={2305.10601},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{besta2023graph,
      title={Graph of Thoughts: Solving Elaborate Problems with Large Language Models}, 
      author={Maciej Besta and Nils Blach and Ales Kubicek and Robert Gerstenberger and Lukas Gianinazzi and Joanna Gajda and Tomasz Lehmann and Michal Podstawski and Hubert Niewiadomski and Piotr Nyczyk and Torsten Hoefler},
      year={2023},
      eprint={2308.09687},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yao2023chainofthought,
      title={Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models}, 
      author={Yao Yao and Zuchao Li and Hai Zhao},
      year={2023},
      eprint={2305.16582},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ding2023thoughts,
      title={Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation}, 
      author={Ruomeng Ding and Chaoyun Zhang and Lu Wang and Yong Xu and Minghua Ma and Wei Zhang and Si Qin and Saravan Rajmohan and Qingwei Lin and Dongmei Zhang},
      year={2023},
      eprint={2311.04254},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{li2023latesteval,
      title={LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction}, 
      author={Yucheng Li and Frank Guerin and Chenghua Lin},
      year={2023},
      eprint={2312.12343},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kıcıman2023causal,
      title={Causal Reasoning and Large Language Models: Opening a New Frontier for Causality}, 
      author={Emre Kıcıman and Robert Ness and Amit Sharma and Chenhao Tan},
      year={2023},
      eprint={2305.00050},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{magister-etal-2023-teaching,
    title = "Teaching Small Language Models to Reason",
    author = "Magister, Lucie Charlotte  and
      Mallinson, Jonathan  and
      Adamek, Jakub  and
      Malmi, Eric  and
      Severyn, Aliaksei",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.151",
    doi = "10.18653/v1/2023.acl-short.151",
    pages = "1773--1781",
    abstract = "Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters. In this paper, we explore the transfer of such reasoning capabilities to smaller models via knowledge distillation, also investigating model and dataset size trade-off. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11{\%} to 21.99{\%} and 18.42{\%} when finetuned on PaLM 540B and GPT-3 175B generated chains of thought, respectively.",
}

@inproceedings{li-etal-2023-symbolic,
    title = "Symbolic Chain-of-Thought Distillation: Small Models Can Also {``}Think{''} Step-by-Step",
    author = "Li, Liunian Harold  and
      Hessel, Jack  and
      Yu, Youngjae  and
      Ren, Xiang  and
      Chang, Kai-Wei  and
      Choi, Yejin",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.150",
    doi = "10.18653/v1/2023.acl-long.150",
    pages = "2665--2679",
    abstract = "Chain-of-thought prompting (e.g., {``}Let{'}s think step-by-ste{''}) primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M{---}1.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of magnitude fewer parameters. We test several hypotheses regarding what properties of chain-of-thought samples are important, e.g., diversity vs. teacher likelihood vs. open-endedness. We release our corpus of chain-of-thought samples and code.",
}


@misc{paul2023refiner,
  title={REFINER: Reasoning Feedback on Intermediate Representations},
  author={Paul, Debjit and Ismayilzada, Mete and Peyrard, Maxime and Borges, Beatriz and Bosselut, Antoine and West, Robert and Faltings, Boi},
  eprint={2304.01904},
  journal={arXiv preprint arXiv:2304.01904},
  url={https://arxiv.org/pdf/2304.01904.pdf},
  year={2023}
}

@misc{omar2023chatgpt,
      title={ChatGPT versus Traditional Question Answering for Knowledge Graphs: Current Status and Future Directions Towards Knowledge Graph Chatbots}, 
      author={Reham Omar and Omij Mangukiya and Panos Kalnis and Essam Mansour},
      year={2023},
      eprint={2302.06466},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{golchin2024time,
      title={Time Travel in LLMs: Tracing Data Contamination in Large Language Models}, 
      author={Shahriar Golchin and Mihai Surdeanu},
      year={2024},
      eprint={2308.08493},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shi2023detecting,
      title={Detecting Pretraining Data from Large Language Models}, 
      author={Weijia Shi and Anirudh Ajith and Mengzhou Xia and Yangsibo Huang and Daogao Liu and Terra Blevins and Danqi Chen and Luke Zettlemoyer},
      year={2023},
      eprint={2310.16789},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{frieder2023mathematical,
      title={Mathematical Capabilities of ChatGPT}, 
      author={Simon Frieder and Luca Pinchetti and Alexis Chevalier and Ryan-Rhys Griffiths and Tommaso Salvatori and Thomas Lukasiewicz and Philipp Christian Petersen and Julius Berner},
      year={2023},
      eprint={2301.13867},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{shinn2023reflexion,
      title={Reflexion: Language Agents with Verbal Reinforcement Learning}, 
      author={Noah Shinn and Federico Cassano and Edward Berman and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
      year={2023},
      eprint={2303.11366},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}


@misc{weng2023large,
      title={Large Language Models are Better Reasoners with Self-Verification}, 
      author={Yixuan Weng and Minjun Zhu and Fei Xia and Bin Li and Shizhu He and Shengping Liu and Bin Sun and Kang Liu and Jun Zhao},
      year={2023},
      eprint={2212.09561},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{chen2023program,
      title={Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks}, 
      author={Wenhu Chen and Xueguang Ma and Xinyi Wang and William W. Cohen},
      year={2023},
      eprint={2211.12588},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{fu2023complexitybased,
      title={Complexity-Based Prompting for Multi-Step Reasoning}, 
      author={Yao Fu and Hao Peng and Ashish Sabharwal and Peter Clark and Tushar Khot},
      year={2023},
      eprint={2210.00720},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{diao2023active,
      title={Active Prompting with Chain-of-Thought for Large Language Models}, 
      author={Shizhe Diao and Pengcheng Wang and Yong Lin and Tong Zhang},
      year={2023},
      eprint={2302.12246},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2022automatic,
      title={Automatic Chain of Thought Prompting in Large Language Models}, 
      author={Zhuosheng Zhang and Aston Zhang and Mu Li and Alex Smola},
      year={2022},
      eprint={2210.03493},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xue2023rcot,
      title={RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought}, 
      author={Tianci Xue and Ziqi Wang and Zhenhailong Wang and Chi Han and Pengfei Yu and Heng Ji},
      year={2023},
      eprint={2305.11499},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{wang2023jarvis1,
    title   = {JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models},
    author  = {Zihao Wang and Shaofei Cai and Anji Liu and Yonggang Jin and Jinbing Hou and Bowei Zhang and Haowei Lin and Zhaofeng He and Zilong Zheng and Yaodong Yang and Xiaojian Ma and Yitao Liang},
    year    = {2023},
    journal = {arXiv preprint arXiv: 2311.05997}
    }
         

@article{wang2023describe,
  title={Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents},
  author={Wang, Zihao and Cai, Shaofei and Liu, Anji and Ma, Xiaojian and Liang, Yitao},
  journal={arXiv preprint arXiv:2302.01560},
  year={2023}
}

@misc{silver2023generalized,
      title={Generalized Planning in PDDL Domains with Pretrained Large Language Models}, 
      author={Tom Silver and Soham Dan and Kavitha Srinivas and Joshua B. Tenenbaum and Leslie Pack Kaelbling and Michael Katz},
      year={2023},
      eprint={2305.11014},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}


@misc{sel2023algorithm,
      title={Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models}, 
      author={Bilgehan Sel and Ahmad Al-Tawaha and Vanshaj Khattar and Ruoxi Jia and Ming Jin},
      year={2023},
      eprint={2308.10379},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gao2023pal,
      title={PAL: Program-aided Language Models}, 
      author={Luyu Gao and Aman Madaan and Shuyan Zhou and Uri Alon and Pengfei Liu and Yiming Yang and Jamie Callan and Graham Neubig},
      year={2023},
      eprint={2211.10435},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{JMLR:v21:19-985,
  author  = {Wouter Kool and Herke van Hoof and Max Welling},
  title   = {Ancestral Gumbel-Top-k Sampling for Sampling Without Replacement},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {47},
  pages   = {1--36},
  url     = {http://jmlr.org/papers/v21/19-985.html}
}

@misc{wang2023selfconsistency,
      title={Self-Consistency Improves Chain of Thought Reasoning in Language Models}, 
      author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
      year={2023},
      eprint={2203.11171},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{NEURIPS2022_8bb0d291,
 author = {Kojima, Takeshi and Gu, Shixiang (Shane) and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {22199--22213},
 publisher = {Curran Associates, Inc.},
 title = {Large Language Models are Zero-Shot Reasoners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}



@article{CoT,
  author       = {Jason Wei and
                  Xuezhi Wang and
                  Dale Schuurmans and
                  Maarten Bosma and
                  Ed H. Chi and
                  Quoc Le and
                  Denny Zhou},
  title        = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2201.11903},
  year         = {2022},
  url          = {https://arxiv.org/abs/2201.11903},
  eprinttype    = {arXiv},
  eprint       = {2201.11903},
  timestamp    = {Fri, 22 Apr 2022 16:06:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2201-11903.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v162-geiger22a,
  title = 	 {Inducing Causal Structure for Interpretable Neural Networks},
  author =       {Geiger, Atticus and Wu, Zhengxuan and Lu, Hanson and Rozner, Josh and Kreiss, Elisa and Icard, Thomas and Goodman, Noah and Potts, Christopher},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {7324--7338},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/geiger22a/geiger22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/geiger22a.html},
  abstract = 	 {In many areas, we have well-founded insights about causal structure that would be useful to bring into our trained models while still allowing them to learn in a data-driven fashion. To achieve this, we present the new method of interchange intervention training (IIT). In IIT, we (1) align variables in a causal model (e.g., a deterministic program or Bayesian network) with representations in a neural model and (2) train the neural model to match the counterfactual behavior of the causal model on a base input when aligned representations in both models are set to be the value they would be for a source input. IIT is fully differentiable, flexibly combines with other objectives, and guarantees that the target causal model is a causal abstraction of the neural model when its loss is zero. We evaluate IIT on a structural vision task (MNIST-PVR), a navigational language task (ReaSCAN), and a natural language inference task (MQNLI). We compare IIT against multi-task training objectives and data augmentation. In all our experiments, IIT achieves the best results and produces neural models that are more interpretable in the sense that they more successfully realize the target causal model.}
}

@InProceedings{pmlr-v236-geiger24a,
  title = 	 {Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations},
  author =       {Geiger, Atticus and Wu, Zhengxuan and Potts, Christopher and Icard, Thomas and Goodman, Noah},
  booktitle = 	 {Proceedings of the Third Conference on Causal Learning and Reasoning},
  pages = 	 {160--187},
  year = 	 {2024},
  editor = 	 {Locatello, Francesco and Didelez, Vanessa},
  volume = 	 {236},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--03 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v236/geiger24a/geiger24a.pdf},
  url = 	 {https://proceedings.mlr.press/v236/geiger24a.html},
  abstract = 	 {Causal abstraction is a promising theoretical framework for explainable artificial intelligence that defines when an interpretable high-level causal model is a faithful simplification of a low-level deep learning system. However, existing causal abstraction methods have two major limitations: they require a brute-force search over alignments between the high-level model and the low-level one, and they presuppose that variables in the high-level model will align with disjoint sets of neurons in the low-level one. In this paper, we present distributed alignment search (DAS), which overcomes these limitations. In DAS, we find the alignment between high-level and low-level models using gradient descent rather than conducting a brute-force search, and we allow individual neurons to play multiple distinct roles by analyzing representations in non-standard bases—distributed representations. Our experiments show that DAS can discover internal structure that prior approaches miss. Overall, DAS removes previous obstacles to uncovering conceptual structure in trained neural nets.}
}


@article{deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
} 

@book{pearl_causality,
author = {Pearl, Judea},
title = {Causality: Models, Reasoning and Inference},
year = {2009},
isbn = {052189560X},
publisher = {Cambridge University Press},
address = {USA},
edition = {2nd},
abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 3,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.}
}

  

