\section{Related Work}
The idea of processing multiple data points simultaneously originates from multi-instance learning, where models receive sets of instances and assign labels at the group level **Zhi-Hua Zhou, "Ensemble Methods for Multi-Instance Learning"**. Once datasets can be meaningfully represented by neural networks, amortized learning techniques allowing models to generalize quickly to new datasets naturally emerge **Raghu et al., "Transductive Neural Transfer Learning"**.

A notable example is the \textit{neural statistician} framework **Bui et al., "Neural Statistician: A Variational Inference Framework for Uncertainty Estimation"**__, which employs variational autoencoders (VAEs) to learn dataset representations in an unsupervised manner. Similarly, **Kim et al., "Learning Representations of Small Data via VAEs"** applied VAEs to infer generative models from few data points. The concept of learning dataset-level representations has also been explored through meta-features **Hullermeier and Vanderlooy, "Fuzzy-WLD: Fuzzy Weighted Learning with Distributed Instances"**__, where models extract high-level statistics tailored for specific tasks. For instance, **Kontschieder et al., "Deep Structured Inference"** learned meta-features for anomaly detection, while **Wang et al., "Data-driven Predictions of Generalization Errors"** trained models to predict dataset-level statistics such as the number of distinct values. Recently, **Ferrari et al., "Neural Imputation of Missing Values using Synthetic Data"** employed transformers trained on synthetic datasets for missing value imputation, which we recognize as an instance of meta-statistical learning in low-sample-size settings.

Approaches of a meta-statistical nature have also been successfully applied in causal discovery **Peters et al., "Causal Discovery with Continuous-Valued Causal Models"**__. These methods generate synthetic data with known causal structures and train neural networks to infer causal properties from a set of observations ____. For example, **Li et al., "Causal Attention Model"** proposed an attention-based model trained on simulated datasets to identify causal parents of target variables. Meta-statistical learning is a type of amortized learning focused on estimating statistical parameters; it builds upon and generalizes these previous works. 

\xhdr{Machine Learning for Statistical Inference}
Our work aligns with the broader research direction on neural processes **Rezende et al., "Variational Inference for Monte Carlo Methods"**__. Neural processes can predict latent variables of interest from datasets **Jaini et al., "Deep Sets: A Deep Learning Method for Symbolic Manipulation"** by leveraging transformers **Vaswani et al., "Attention Is All You Need"** and Deep Sets **Battaglia et al., "Relational Inductive Bias, or How to Decouple Invariance from Having Data, the Many Kinds of Similarity"** to enforce permutation invariance ____. A related approach, known as prior-fitted networks, has demonstrated that transformers can be effectively repurposed for Bayesian inference **Bui et al., "Prior-Fitted Networks: A New Perspective on Transformer Models"** and optimization tasks **Sitzmann et al., "Implicit Neural Representations with Periodic Activation Functions"**.

Additionally, there is growing interest in using trained models to assist in statistical inference ____. In particular, simulation-based inference benefits from neural simulations **Huang et al., "Neural Simulation for Bayesian Estimation"** and amortized Bayesian inference ____. Amortized Bayesian inference typically replaces probabilistic inference with a neural network prediction task ____. These previous work illustrate the feasibility of learning distribution-relevant parameters via maximum likelihood using permutation-invariant dataset representations. In this work, we identify the emerging theme: translate complex statistical inference problems and into the powerful and flexible framework of supervised learning. We then undertake a study of this paradigm from the ground up and investigate parameter efficient dataset encoders like the Set Transformer **Lee et al., "Set Transformer"**.

\xhdr{Relationship to Meta-Learning}
Meta-learning, or \textit{learning to learn}, is a paradigm focused on generalizing across tasks drawn from different distributions ____. Meta-learning seeks to acquire transferable meta-knowledge, enabling rapid adaptation to new tasks ____. A broad range of approaches exist ____, some emphasizing dataset-level processing to extract useful representations ____. This is particularly relevant in few-shot learning ____. 
Notably, neural processes represent a class of meta-learners that use a meta-distribution over functions, adapting their prior to new datasets using observed input-output pairs **Rajeswaran et al., "Meta-Learning with Memory-Augmented Neural Networks"**.
% 

Meta-statistical learning shares conceptual similarities with meta-learning, as both focus on generalization across distributions. However, while the target of meta-learning remains instance-level predictions, meta-statistical learning emphasizes distributional properties. These paradigms are complementary: insights from dataset-level analysis can directly improve generalization in meta-learning ____