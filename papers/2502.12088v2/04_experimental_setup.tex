Our experiments demonstrate the versatility of meta-statistical learning by achieving strong performance across diverse tasks with minimal task-specific effort. Here, we describe the template used to run experiments with various descriptive and inferential tasks.

\xhdr{Meta-Dataset Generation}
We construct meta-datasets by sampling datasets \( \mathcal{D} \) and labels \( y \) from a predefined meta-distribution \( P_{\Gamma, Y} \). The generation process involves two stages: first, a distribution family (e.g., Normal, Uniform) is randomly selected, and its parameters are sampled from predefined priors to yield a data-generating distribution \( P_X \). A dataset \( \mathcal{D} \) of size \( n \) is then sampled from \( P_X \), with \( n \) also drawn from a prior. Thus, \( P_{\Gamma} \) defines the set of base distributions, parameter priors, and dataset size priors. 

\xhdr{In- vs. Out-of-Meta-Distribution}
With in-meta-distribution (IMD) settings, both training and testing datasets are sampled from \( P_{\Gamma} \). For out-of-meta-distribution (OoMD) testing, we modify \( P_{\Gamma} \) by changing the set of base distributions (e.g., replacing Normal with Cauchy). This tests the robustness of meta-statistical estimators to unseen distributions.

\subsection{Meta-Statistical Models}
Models should predict dataset-level properties \( y \) from datasets \( \mathcal{D} \) of varying sizes \( n \). The architecture we consider consists of a dataset encoder \( \phi \) and a prediction head \( \rho \), defined as \( \varphi(\mathcal{D}; \rho, \phi) = \rho \circ \phi(\mathcal{D}) \), where \( \phi \) transforms \( \mathcal{D} \) into a fixed-dimensional representation, and \( \rho \) is a Multi-Layer Perceptron that predicts the target. The model is trained with MSE loss for regression tasks and cross-entropy loss for classification tasks.

\xhdr{LSTM Encoder}
The Long Short-Term Memory (LSTM) network \cite{hochreiter1997long} processes datasets sequentially. For a dataset \( \mathcal{D} \in \mathbb{R}^{n \times k} \), the dataset representation is the average of all hidden states of the last layer: \( \phi(\mathcal{D}) = \frac{1}{n} \sum_{t=1}^n h_t \), where \( h_t \) is the hidden state at timestep \( t \). LSTMs lack the inductive bias of permutation invariance, making them a baseline model.

\xhdr{Vanilla Transformer Encoder}
The Vanilla Transformer (VT) \cite{NIPS2017_3f5ee243} uses multi-head self-attention without positional encodings to ensure permutation invariance. The dataset representation is the output of a special token, analogous to the CLS token in BERT \cite{devlin-etal-2019-bert}:
\(
\phi(\mathcal{D}) = z_{\text{CLS}}.
\)

\xhdr{Set Transformer}
The Set Transformer \cite{lee2019set} is designed for set-structured data and ensures permutation invariance. Furthermore, it reduces the quadratic cost of attention by performing attention on a fixed set of $m$ inducing points, where $m$ is a hyperparameter. The inducing points are learned as a projection of the full sequence at each layer. The enhanced Set Transformer 2 (ST2) \cite{zhang2022set} incorporates \textbf{SetNorm}, a normalization technique that improves over LayerNorm \cite{ba2016layer} by preserving permutation invariance while improving the convergence properties of the Set Transformer. 
