In descriptive tasks, the label \( y \) of a dataset \( \mathcal{D} \) is the output of an algorithm \( A \) applied to \( \mathcal{D} \), i.e., \( y = A(\mathcal{D}) \). Simple tasks like median or correlation serve as unit testing of meta-statistical models. However, for more computationally intensive algorithms, such as optimal transport, meta-statistical models could serve as fast approximations. For datasets \( \mathcal{D} \in \mathbb{R}^{n \times m} \), we consider four descriptive tasks: the \textbf{per-column median} label \( y \in \mathbb{R}^m \) consists of the medians of each column. The \textbf{Pearson correlation} coefficient \( y \in \mathbb{R} \) is computed between the two columns. The \textbf{win rate} (Bradley-Terry) is the fraction of rows where the value in the first column exceeds that in the second: \( y = \frac{1}{n} \sum_{i=1}^n \mathbb{I}(\mathcal{D}_{i,1} > \mathcal{D}_{i,2})\), where \( \mathbb{I}(\cdot) \) is the indicator function. Finally, the 1D \textbf{optimal transport} (OT) label \( y \in \mathbb{R} \) is the optimal transport cost between the empirical distributions of the two columns.


\xhdr{Meta-Dataset Generation}
To construct the meta-dataset, we sample datasets \( D \) from predefined probability distributions as described in \Secref{sec:experimental_setup}. Once a dataset is sampled we simply compute the target label \( y \) by applying the target algorithm. We experiment with various numbers of columns $m$. By having $k > 1$, we produce $k$ computation in parallel with one forward pass (independently of the batch dimension). We observe no significant difference when varying $k$ and fix $k=2$ in the experiments. The meta-dataset contains $30K$ training meta datapoints per task, with dataset sizes sampled from $n \in [5, 300]$. Details about meta-datasets and which distribution families are in- or out-of-meta-distribution are provided in \Appref{app:desc_details}.


\xhdr{Meta-Statistical Models}  
After optimizing hyperparameters and architecture choices (e.g., pooling mechanisms and head-to-dimensionality ratio) on a small validation set of 1K meta datapoints, we compare four meta-statistical model variants: LSTM, Vanilla Transformer (VT), and two ST2 variants with 16 or 32 inducing points. ST2(16) is the fastest model for both training and inference. In \Appref{app:eff}, we show that VT scales quadratically, while LSTM and ST2 scale linearly, with better slopes for ST2. Additionally, ST2(16) achieves a 12x faster training time per batch normalized by parameters compared to VT, meaning an ST2(16) model with 12 times more parameters can be trained in the same time as VT. However, for consistency in reporting, we compare models with approximately the same number of parameters (\( \sim 10K \) in this section).


\begin{figure}[t] 
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/per_column_median.pdf} 
        \caption{\texttt{Median} prediction}
        \label{fig:subfig_b}
    \end{subfigure}
    \vspace{0.5cm} 
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/correlation.pdf}
        \caption{\texttt{Correlation} prediction}
        \label{fig:subfig_c}
    \end{subfigure}
    \hfill
    \vspace{-0.5cm}
    \caption{\textbf{Generalization across dataset lengths and meta-distributions.} The left panel shows MSE as a function of dataset length for in-meta-distribution datasets, while the right panel displays the same for out-of-meta-distribution datasets. The vertical red line marks the largest dataset seen during training ($n = 300$). LSTM is excluded due to its errors being an order of magnitude higher. Additional tasks can be found in \Appref{app:gen_plots}.}
    \label{fig:generalization}
\end{figure}

\xhdr{In-meta-distribution performance}  
\Tabref{tab:performance_comparison} shows the MSE of the four meta-statistical models on a test set sampled from the same meta-distribution as the training data. All models approximate the descriptive tasks well, but the LSTM-based model, lacking permutation invariance, performs worse than attention-based models. 
Notably, ST2, despite being much faster than VT, narrowly outperforms it. Given its strength and efficiency, ST2(16) is our main model in the rest of the paper, with VT considered as an alternative baseline.

\xhdr{Generalization Performance}  
We evaluate meta-statistical models' generalization capabilities on two aspects:  (i) \textbf{Out-of-Meta-Distribution (OoMD)}: Datasets from unseen distributions. (ii) \textbf{Length Generalization}: Datasets with lengths outside the training range. \Figref{fig:generalization} shows strong length generalization, where models maintain their performance for larger datasets than seen during training, both IMD and OoMD. 
They are also robust to OoMD datasets despite a small performance degradation. Manual inspection reveals that the degradation mainly comes from cases where the magnitude of the input values exceeds the range seen during training. This is discussed further in \Secref{sec:discussion}.
Additional results and generalization plots are provided in \Appref{app:desc_details}.


\begin{table}[t]
\centering
\resizebox{0.95\columnwidth}{!}{
\begin{tabular}{@{}l|c|c|c|c@{}}
\toprule
& \textbf{Median} & \textbf{Corr} & \textbf{WinRate (BT)} & \textbf{OT (1D)} \\ 
\midrule
\midrule
LSTM
& $2.9e^{-1}$ \scriptsize{$\pm 0.8$}
& $5.9e^{-2}$ \scriptsize{$\pm 1.5$}
& $4.4e^{-2}$ \scriptsize{$\pm 0.9$}
& $8.5e^{-2}$ \scriptsize{$\pm 2.9$}\\
VT
& $\mathbf{6.0e^{-2}}$ \scriptsize{$\pm 1.9$}
& $\mathbf{9.2e^{-3}}$ \scriptsize{$\pm 4.6$}
& $7.1e^{-3}$ \scriptsize{$\pm 1.5$}
& $\mathbf{5.5e^{-2}}$ \scriptsize{$\pm 1.4$}\\
ST2(16)
& $\mathbf{4.2e^{-2}}$ \scriptsize{$\pm 1.7$}
& $\mathbf{7.5e^{-3}}$ \scriptsize{$\pm 2.8$}
& $\mathbf{2.9e^{-3}}$ \scriptsize{$\pm 1.2$}
& $\mathbf{4.5e^{-2}}$ \scriptsize{$\pm 1.9$}\\
ST2(32)
& $\mathbf{4.4e^{-2}}$ \scriptsize{$\pm 0.9$}
& $\mathbf{9.1e^{-3}}$ \scriptsize{$\pm 5.1$}
& $\mathbf{1.6e^{-2}}$ \scriptsize{$\pm 0.5$}
& $\mathbf{3.0e^{-2}}$ \scriptsize{$\pm 1.5$}\\
\bottomrule

\end{tabular}
}
\caption{Performance comparison meta-statistical models across tasks, measured by Mean Squared Error with respect to correct output on the test set. \textbf{Bold} indicates no significant difference with the best model.}
\label{tab:performance_comparison}
\end{table}
