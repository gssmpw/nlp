In inferential tasks, the label \( y \) represents a property \( g \) of the underlying distribution \( P_X \) from which a dataset \( \mathcal{D} \) is sampled: \( y = g(P_X) \). We illustrate the meta-statistical framework with three such tasks: standard deviation estimation, normality testing, and mutual information estimation. Details on meta-dataset creation and models are in \Appref{app:std}. For all tasks in this section, the dataset sizes during training are sampled from $n \in [5, 150]$, depicted by vertical red lines in the plots. 

\subsection{Standard Deviation Estimation}  
The standard deviation (\( \sigma = \sqrt{\mathbb{E}[(X - \mathbb{E}[X])^2]}\)) quantifies the spread of a distribution \( P_X \). Unlike the mean or variance, estimating \( \sigma \) is non-trivial due to the square root's non-linearity \cite{gurland1971simple,gupta1952estimation}. In fact, no universal unbiased estimator exists across all distributions \cite{gurland1971simple,fenstad1980robust}. We use this task to show meta-statistical learning in action.  


\xhdr{Meta-Dataset} To create the meta-dataset, we follow the procedure outlined in \Secref{sec:experimental_setup}, keeping different distribution families for in- and out-of-meta-distribution. We use 100K meta datapoints for training.

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{images/std_line_plot.pdf}
    \caption{MSE of $\sigma$ estimators as a function of dataset sizes, for dataset sampled \textbf{out-of-meta-distribution}.}
    \label{fig:std_line_plot}
\end{figure}


\xhdr{Meta-Statistical Model}  
We train two ST2-based models: ST2$_{\text{std}}$, which predicts the standard deviation \( \sigma \), and ST2$_{\text{fsd}}$, which estimates the finite sample correction to apply to the sample standard deviation, defined as \( y = \sigma - \text{np.std}(X) \). This allows constructing a corrected estimator by adjusting \texttt{np.std} with ST2$_{\text{fsd}}$'s predictions. Both models share the same architecture: 16 inducing points, five hidden layers (128 dimensions), and 12 attention heads per layer, totaling around 950K parameters.  


\xhdr{Results} 
We compare the ST2-based estimator to the sample standard deviation (\texttt{np.std} with Bessel’s correction) across dataset lengths for out-of-meta-distribution scenarios in \Figref{fig:std_line_plot}. ST2$_{\text{std}}$ achieves strong MSE performance, converging to a low error in high-sample sizes. Also, the learned correction from ST2$_{\text{fsd}}$ further reduces the bias of \texttt{np.std}, effectively capturing finite sample errors. Notably, ST2$_{\text{fsd}}$ also lowers variance of \texttt{np.std}, suggesting the correction is data-dependent rather than a fixed offset. 
Full tables of bias, variance, and MSE across distributions and dataset lengths are provided in \Appref{app:std}, confirming these observations.


\subsection{Normality Testing}
The task is now to determine whether a dataset \( \mathcal{D} \sim P_X \) originates from a normal distribution, formulated as a binary classification task: \( y = 1 \) if \( P_X \) is normal, \( y = 0 \) otherwise. Normality testing is crucial in hypothesis testing, model selection, and preprocessing \cite{shapiro1965analysis, razali2011power}, particularly before applying t-tests, linear regression, or ANOVA with small samples \cite{altman1990practical, das2016brief, doi:10.4078/jrd.2019.26.1.5}. However, standard tests struggle in low-sample settings \cite{razali2011power}. We propose to train meta-statistical models for normality classification, aiming for robust generalization in such regimes. Details on meta-dataset creation and model properties are in \Appref{app:norm_test}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{images/norm_test_oomd.pdf}
    \caption{Accuracy of normality classifiers as a function of dataset sizes. The non-normal distributions are sampled \textbf{out-of-meta distribution} for meta-statistical models.}
    \label{fig:norm_t_oomd}
    \vspace{-0.5cm}
\end{figure}


\xhdr{Meta-dataset creation}  
We construct a balanced meta-dataset of normally and not normally distributed datasets following the process described in \Secref{sec:experimental_setup}. For non-normality, we choose an alternative distribution from a predefined set detailed in \Appref{app:norm_test}. We use 40K meta datapoints for training. 

\xhdr{Estimators}  
We transform traditional normality tests into binary classifiers by thresholding their \( p \)-values, optimizing the threshold on the training meta-dataset for maximum classification accuracy. We consider four widely used tests: the \textit{Shapiro-Wilk test} \cite{shapiro1965analysis}, known to be effective for small samples \cite{razali2011power}; the \textit{D'Agostino-Pearson test} \cite{d1973tests}, which combines skewness and kurtosis; the \textit{Kolmogorov-Smirnov test} \cite{massey1951kolmogorov}, a non-parametric test based on cumulative distribution differences; and the \textit{Jarque-Bera test} \cite{jarque1987test}, which assesses skewness and kurtosis deviations from theoretical expectations.

We then train two meta-statistical models: one based on VT and another on ST2 with 16 inducing points. Both use four layers, a hidden dimensionality of 32, and 12 attention heads. The classification head is a single-layer MLP with 32 neurons, totaling approximately 50K parameters per model.

\begin{table}[h]
\centering
\begin{tabular}{l|cccc}
\toprule
 & Accuracy $\uparrow$ & AuROC $\uparrow$ & Brier $\downarrow$ & BT $\uparrow$ \\
\midrule
KS & $0.88$ {\scriptsize± $0.01$} & $0.93$ {\scriptsize± $0.01$} & $0.09$ {\scriptsize± $0.01$} & $0.12$ {\scriptsize± $0.02$} \\
SW & $0.89$ {\scriptsize± $0.01$} & $0.95$ {\scriptsize± $0.01$} & $0.18$ {\scriptsize± $0.01$} & $0.16$ {\scriptsize± $0.03$} \\
JB & $0.88$ {\scriptsize± $0.01$} & $0.93$ {\scriptsize± $0.01$} & $0.16$ {\scriptsize± $0.01$} & $0.13$ {\scriptsize± $0.02$} \\
AP & $0.90$ {\scriptsize± $0.01$} & $0.95$ {\scriptsize± $0.01$} & $0.18$ {\scriptsize± $0.01$} & $0.17$ {\scriptsize± $0.03$} \\
ST2 & $\mathbf{0.92}$ {\scriptsize± $0.01$} & $\mathbf{0.97}$ {\scriptsize± $0.01$} & $\mathbf{0.06}$ {\scriptsize± $0.01$} & $\mathbf{0.25}$ {\scriptsize± $0.04$} \\
VT & $0.91$ {\scriptsize± $0.01$} & $\mathbf{0.97}$ {\scriptsize± $0.01$} & $\mathbf{0.07}$ {\scriptsize± $0.01$} & $0.17$ {\scriptsize± $0.03$} \\
\bottomrule
\end{tabular}
\caption{Normality test classifiers with datasets drawn from Gaussian or Uniform distributions of sizes $n \in [10, 300]$. AuROC refers to the area under the ROC curve, Brier loss is the calibration error, and BT measures the relative strengths of classifiers in a paired evaluation.}
\label{tab:normality_test}
\end{table}


\xhdr{Results}  
\Figref{fig:norm_t_oomd} summarizes the accuracy of the proposed meta-statistical models, in settings where negative labels correspond to datasets sampled from distribution families unseen during training. Consistent with prior comparisons of normality tests, Shapiro-Wilk and D'Agostino-Pearson perform best among the baselines \cite{razali2011power}. Across all dataset sizes, meta-statistical models consistently and largely outperform baselines, with particularly strong gains in small-sample settings (\( n < 100 \)), making them highly relevant for biomedical applications \cite{doi:10.4078/jrd.2019.26.1.5}. Meta-statistical models achieve near-perfect accuracy (\( > 0.98 \)) as \( n \) increases demonstrating their consistency. Overall, this task seems relatively easy for meta-statistical models, which generalize smoothly out-of-meta distribution. However, note that the training of meta-statistical models could be harder if the input datasets are standardized during training (see \Secref{sec:discussion}). 

While classification lacks a direct bias-variance formulation, we analyze false positive and false negative rates as well as precision and recall in \Appref{app:prec_rec}, showing more balanced error profiles for meta-statistical estimators. In \Tabref{tab:normality_test}, we present key metrics for evaluating classifier performance: the Area Under the Receiver Operating Characteristic Curve (AuROC), the Brier Score, and the Bradley-Terry (BT) scores from a paired evaluation.
The AuROC measures a classifier's ability to discriminate between positive and negative classes across different decision thresholds. A higher AuROC indicates better separability. Unlike accuracy, AuROC provides a threshold-independent measure of performance. The Brier loss \cite{brier1950verification} quantifies the calibration of a model’s predicted probabilities. Lower values indicate better calibration. The Bradley-Terry (BT) score \cite{bradley1952rank,NIPS2004_825f9cd5} ranks models based on pairwise comparisons, assessing how often one classifier outperforms another across test instances \cite{peyrard-etal-2021-better,colombo-etal-2023-glass}. The accuracy scores are lower than those of \Figref{fig:norm_t_oomd} because the uniform is among the hardest out-of-meta-distribution to recognize as non-Gaussian. Still, across metrics, the meta-statistical estimators perform strongly. In particular, we find it interesting that they are particularly well-calibrated.


\subsection{Mutual Information Estimation}
Mutual information (MI) quantifies the dependency between two random variables \(X\) and \(Y\) and is defined as:
\[
\mathrm{MI}(X; Y) = \int \int P_{X,Y}(x, y) \log \frac{P_{X,Y}(x, y)}{P_X(x) P_Y(y)} \, dx \, dy.
\]
Here, \(P_X\) and \(P_Y\) denote the marginal distributions of \(X\) and \(Y\), respectively.

MI possesses key properties such as invariance to homeomorphisms and adherence to the \textit{Data Processing Inequality}, making it fundamental in machine learning and related fields \cite{inv_bottleneck, pmlr-v80-belghazi18a, repr_learning, tishby2000informationbottleneckmethod}. However, MI estimation remains challenging, particularly for small sample sizes and non-Gaussian distributions \cite{song2020understandinglimitationsvariationalmutual, pmlr-v108-mcallester20a, NEURIPS2023_36b80eae}. 

We adopt a meta-statistical approach, training models to predict \( y = \mathrm{MI}(X; Y) \) between two dataset columns. Focusing on low-sample, non-Gaussian settings, but we restrict experiments to the one-dimensional case for simplicity. Details on meta-dataset creation, models, and extra results are provided in \Appref{app:mi_details}.


\xhdr{Meta-dataset Creation}
We construct a meta-dataset inspired by the benchmark methodology in \cite{NEURIPS2023_36b80eae}, where distributions with ground-truth MI are generated in two steps: (i) by sampling a distribution with known MI, (ii) optionally applying MI-preserving transformations. This process creates complex distributions and datasets with known MI. For generating meta-dataset in this way, we again follow the process described in \Secref{sec:experimental_setup} using different base-distribution and MI-preserving transformations between in-meta-distribution and out-of-meta-distribution. We use 50K meta datapoints for training.


\xhdr{Estimators}
We compare our approach with the best-performing 1D estimators from \cite{NEURIPS2023_36b80eae}, including Kraskov-St\"ogbauer-Grassberger (KSG) \cite{PhysRevE.69.066138}, Canonical Correlation Analysis (CCA) \cite{pml2Book}, and three neural estimators: MINE \cite{pmlr-v80-belghazi18a}, InfoNCE \cite{repr_learning}, and NWJE \cite{NIPS2007_72da7fd6, NIPS2016_cedebb6e, pmlr-v97-poole19a}. We train two meta-statistical models: one based on Vanilla Transformer (VT) and the other on Set Transformer 2 (ST2). Both models consist of five layers, with a hidden dimensionality of 256 and 12 attention heads. The regression head is a single hidden-layer MLP with 128 neurons, resulting in models with approximately 1M parameters.


\xhdr{Estimation Performance}
The mean squared error (MSE) results for both in- and out-of-meta-distribution testing are shown in \Tabref{tab:mi_test}. Meta-statistical models outperform baseline estimators across all sample sizes, with significant advantages in low-sample scenarios. Baseline models, particularly neural ones, struggle with small sample sizes, while only KSG and CCA begin to match meta-statistical models for sample sizes greater than 100 in the out-of-meta-distribution regime.

\xhdr{Bias and Variance of MI Estimators}
We examine the bias and variance of MI estimators by resampling datasets from fixed distributions and measuring the variance and bias of the estimates. In \Figref{fig:mi_bias_var}, we visualize the bias and variance for a challenging distribution identified by previous works \cite{NEURIPS2023_36b80eae} (additive noise). Even at a sample size of $n = 100$, meta-statistical models show clear improvements in both bias (estimates centered around 0) and variance. A more detailed analysis of bias and variance is available in \Appref{app:mi_details} (\Tabref{tab:mi_bias_variance}). Compared to baseline estimators, meta-statistical models demonstrate significantly lower bias, close to zero, and lower or comparable variance. These results are promising, suggesting that further scaling could create even more robust meta-statistical MI estimators. Currently, the ST2 model can be trained in less than an hour on a single GPU, with inference orders of magnitude faster than existing neural baselines.


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{images/additive-noise.pdf}
    \caption{We estimate statistics for MI estimators over 150 resampled datasets of size \( n = 100 \) from a fixed distribution (additive noise \cite{NEURIPS2023_36b80eae}). Each dot represents the difference between an estimate and the true mutual information (MI).}
    \label{fig:mi_bias_var}
\end{figure}

\begin{table}[t]
\centering
\resizebox{0.95\columnwidth}{!}{
\begin{tabular}{@{}l|cc|cc@{}}
\toprule
& \multicolumn{2}{c|}{IMD} & \multicolumn{2}{c}{OoMD} \\ 
% \cmidrule(lr){2-4} \cmidrule(lr){5-7}
$n \in $ & $[10, 100]$ & $[100, 200]$ & $[10, 100]$ & $[100, 200]$ \\ 
\midrule
\midrule
CCA
& $7.4e^{-2}$ \scriptsize{$\pm 9.3$}
& $1.4e^{-2}$ \scriptsize{$\pm 1.1$}
% ----
& $1.3e^{-1}$ \scriptsize{$\pm 1.2$}
& $4.9e^{-2}$ \scriptsize{$\pm 3.3$}\\
KSG
& $2.9e^{-2}$ \scriptsize{$\pm 1.5$}
& $7.8e^{-3}$ \scriptsize{$\pm 3.5$}
% ----
& $\mathbf{1.2e^{-2}}$ \scriptsize{$\pm 0.4$}
& $\mathbf{7.2e^{-3}}$ \scriptsize{$\pm 2.3$}\\
MINE
& $2.5e^{0}$ \scriptsize{$\pm 2.4$}
& $2.8e^{-2}$ \scriptsize{$\pm 1.2$}
% ---
& $5.4e^{0}$ \scriptsize{$\pm 7.7$}
& $1.6e^{-1}$ \scriptsize{$\pm 2.1$}\\
NWJE
& -
& $7.3e^{-2}$ \scriptsize{$\pm 4.7$}
% --
& $6.6e^{0}$ \scriptsize{$\pm 7.5$}
& $6.3e^{-2}$ \scriptsize{$\pm 5.4$}\\
InfoNCE
& $1.5e^{1}$ \scriptsize{$\pm 2.2$}
& $1.9e^{-2}$ \scriptsize{$\pm 0.7$}
% ---
& $2.3e^{1}$ \scriptsize{$\pm 3.4$}
& $3.4e^{-1}$ \scriptsize{$\pm 4.5$}\\
\midrule
VT
& $\mathbf{4.6e^{-3}}$ \scriptsize{$\pm 2.4$}
& $\mathbf{2.5e^{-3}}$ \scriptsize{$\pm 1.3$}
% ---
& $\mathbf{1.5e^{-2}}$ \scriptsize{$\pm 0.8$}
& $\mathbf{7.7e^{-3}}$ \scriptsize{$\pm 3.2$}\\
ST2(16)
& $\mathbf{6.2e^{-3}}$ \scriptsize{$\pm 3.0$}
& $\mathbf{2.4e^{-3}}$ \scriptsize{$\pm 1.1$}
% ---
& $\mathbf{1.3e^{-2}}$ \scriptsize{$\pm 0.7$}
& $\mathbf{8.5e^{-3}}$ \scriptsize{$\pm 3.1$}\\
\bottomrule
\end{tabular}
}
\caption{MSE loss of mutual information estimators both in- and out-of-meta-distribution. \textbf{Bold} indicates no significant difference with the best estimator.}
\label{tab:mi_test}
\end{table}