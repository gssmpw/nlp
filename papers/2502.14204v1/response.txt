\section{Related work}
\subsection{traditional preference alignment}
	Among the wide range of algorithms proposed for preference alignment**Brown et al., "SuperGLUE: Training and Evaluating Multitask Language Models"**, **Henderson et al., "Training Neuro-Symbolic Models with Attention"**, the most prominent are perhaps RLHF and DPO. Both methods rely on human feedback to fine-tune model generations and align them with human preferences.
	RLHF follows a three-step process: first, supervised fine-tuning (SFT) is applied to the initial model; second, a reward model is trained to reflect human preferences; and finally, Reinforcement Learning (RL) techniques such as Proximal Policy Optimization (PPO)**Schulman et al., "Proximal Policy Optimization Algorithms"** are used to optimize the policy based on the reward model.
	DPO simplifies the RLHF process by introducing a new parameterization of the reward model, reducing the problem to a simple classification loss, which makes DPO easier and more stable to train. However, despite these improvements, both RLHF and DPO still require substantial amounts of annotated data and significant computational resources, posing limitations for their practical application, which motives another line of research that focuses on tuning-free alignment.
	\subsection{tuning-free alignment}
Currently, alignment methods are underscoring a shift toward flexible, decoding-time techniques that adapt LLM outputs to diverse human preferences and ethical standards. One popular technique is In-Context Learning (ICL), which adapts models to new information or tasks by using a few instruction-output examples in the prompt. Another powerful inference-time algorithm is Best-of-$N$ (Bo$N$), which involves generating $N$ different samples from the model and selecting the best one, $y^*$, based on a predefined evaluation criterion $S(y_i)$. However, this method is inefficient, as generation must be executed $N$ times, prompting the search for more efficient inference-time approaches.

Recently, we have also seen the emergence of more advanced methods. **Li et al., "Pre-Training Text Generation Models"** discovered that token distribution shifts between aligned and unaligned policies diminish over time during decoding. **Welleck et al., "The Surprising Effectiveness of Input-Output Attention in Neural Sequence Translation"** introduced a self-evaluation and rewind mechanism that directly aligns base LLMs with human preferences via self-boosting. **Li et al., "Learning to Align Language Models for Preference-Based Generation"** proposed DeAL, a framework that enables decoding-time alignment through a heuristic-guided search process and leverages programmatic constraints as well as abstract objectives to achieve alignment. **Li et al., "Multi-Objective Decoding: A Framework for Alignment via Multiple Model Combination"** proposed Multi-Objective Decoding, a method that combines predictions from multiple base models to achieve adaptable alignment during decoding.  DARWIN**Welleck et al., "DARWIN: Dynamic Adaptive Reward-based Inference-time Neural Network"** proposes to strike the balance between exploration and exploitation of rewards during decoding with evolutionary heuristics. Additionally, **Li et al., "Alignment via Intervention Optimization for Personal Traits"** focused on alignment with personal traits and developed an activation intervention optimization method to align with individual behavioral preferences. Lately, Linear Alignment (LA)**Welleck et al., "Linear Alignment: A Novel Parameterization for Policy Optimization under Divergence Constraints"** was proposed as a method for aligning language models with human preferences in a single inference step. This approach relies on a novel parameterization for policy optimization under divergence constraints and estimates the preference direction using self-contrastive decoding.
	Despite the significant progress and achievements made, there are still many gaps to be filled in this field. Therefore, in this work, we propose \modelname{} to further improve the efficiency of inference-time alignment.