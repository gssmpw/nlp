[
  {
    "index": 0,
    "papers": [
      {
        "key": "stiennon2020learning",
        "author": "Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F",
        "title": "Learning to summarize with human feedback"
      },
      {
        "key": "Yuan2023RRHFRR",
        "author": "Zheng Yuan and Hongyi Yuan and Chuanqi Tan and Wei Wang and Songfang Huang and Feiran Huang",
        "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
      },
      {
        "key": "rafailov2023direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      },
      {
        "key": "zhu2024lire",
        "author": "Zhu, Mingye and Liu, Yi and Zhang, Lei and Guo, Junbo and Mao, Zhendong",
        "title": "LIRE: listwise reward enhancement for preference alignment"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "schulman2017proximal",
        "author": "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
        "title": "Proximal policy optimization algorithms"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "lin2023unlocking",
        "author": "Lin, Bill Yuchen and Ravichander, Abhilasha and Lu, Ximing and Dziri, Nouha and Sclar, Melanie and Chandu, Khyathi and Bhagavatula, Chandra and Choi, Yejin",
        "title": "The unlocking spell on base llms: Rethinking alignment via in-context learning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "li2023rain",
        "author": "Li, Yuhui and Wei, Fangyun and Zhao, Jinjing and Zhang, Chao and Zhang, Hongyang",
        "title": "Rain: Your language models can align themselves without finetuning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "huang2024deal",
        "author": "Huang, James Y and Sengupta, Sailik and Bonadiman, Daniele and Lai, Yi-an and Gupta, Arshit and Pappas, Nikolaos and Mansour, Saab and Kirchhoff, Katrin and Roth, Dan",
        "title": "Deal: Decoding-time alignment for large language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "shi2024decoding",
        "author": "Shi, Ruizhe and Chen, Yifang and Hu, Yushi and Liu, Alisa and Hajishirzi, Hannaneh and Smith, Noah A and Du, Simon S",
        "title": "Decoding-time language model alignment with multiple objectives"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "hung2024reward",
        "author": "Hung, Chia-Yu and Majumder, Navonil and Mehrish, Ambuj and Poria, Soujanya",
        "title": "Reward Steering with Evolutionary Heuristics for Decoding-time Alignment"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhu2024personality",
        "author": "Zhu, Minjun and Yang, Linyi and Zhang, Yue",
        "title": "Personality alignment of large language models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "Gao2024LinearAA",
        "author": "Songyang Gao and Qiming Ge and Wei Shen and Shihan Dou and Junjie Ye and Xiao Wang and Rui Zheng and Yicheng Zou and Zhi Chen and Hang Yan and Qi Zhang and Dahua Lin",
        "title": "Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback"
      }
    ]
  }
]