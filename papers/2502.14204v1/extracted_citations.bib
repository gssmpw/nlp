@article{Gao2024LinearAA,
	title={Linear Alignment: A Closed-form Solution for Aligning Human Preferences without Tuning and Feedback},
	author={Songyang Gao and Qiming Ge and Wei Shen and Shihan Dou and Junjie Ye and Xiao Wang and Rui Zheng and Yicheng Zou and Zhi Chen and Hang Yan and Qi Zhang and Dahua Lin},
	journal={ArXiv},
	year={2024},
	volume={abs/2401.11458},
	url={https://api.semanticscholar.org/CorpusID:267068705}
}

@article{Yuan2023RRHFRR,
	title={RRHF: Rank Responses to Align Language Models with Human Feedback without tears},
	author={Zheng Yuan and Hongyi Yuan and Chuanqi Tan and Wei Wang and Songfang Huang and Feiran Huang},
	journal={ArXiv},
	year={2023},
	volume={abs/2304.05302},
	url={https://api.semanticscholar.org/CorpusID:258059818}
}

@article{huang2024deal,
	title={Deal: Decoding-time alignment for large language models},
	author={Huang, James Y and Sengupta, Sailik and Bonadiman, Daniele and Lai, Yi-an and Gupta, Arshit and Pappas, Nikolaos and Mansour, Saab and Kirchhoff, Katrin and Roth, Dan},
	journal={arXiv preprint arXiv:2402.06147},
	year={2024}
}

@article{hung2024reward,
	title={Reward Steering with Evolutionary Heuristics for Decoding-time Alignment},
	author={Hung, Chia-Yu and Majumder, Navonil and Mehrish, Ambuj and Poria, Soujanya},
	journal={arXiv preprint arXiv:2406.15193},
	year={2024}
}

@article{li2023rain,
	title={Rain: Your language models can align themselves without finetuning},
	author={Li, Yuhui and Wei, Fangyun and Zhao, Jinjing and Zhang, Chao and Zhang, Hongyang},
	journal={arXiv preprint arXiv:2309.07124},
	year={2023}
}

@inproceedings{lin2023unlocking,
	title={The unlocking spell on base llms: Rethinking alignment via in-context learning},
	author={Lin, Bill Yuchen and Ravichander, Abhilasha and Lu, Ximing and Dziri, Nouha and Sclar, Melanie and Chandu, Khyathi and Bhagavatula, Chandra and Choi, Yejin},
	booktitle={The Twelfth International Conference on Learning Representations},
	year={2023}
}

@article{rafailov2023direct,
	title={Direct preference optimization: Your language model is secretly a reward model},
	author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
	journal={arXiv preprint arXiv:2305.18290},
	year={2023}
}

@article{schulman2017proximal,
	title={Proximal policy optimization algorithms},
	author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	journal={arXiv preprint arXiv:1707.06347},
	year={2017}
}

@article{shi2024decoding,
	title={Decoding-time language model alignment with multiple objectives},
	author={Shi, Ruizhe and Chen, Yifang and Hu, Yushi and Liu, Alisa and Hajishirzi, Hannaneh and Smith, Noah A and Du, Simon S},
	journal={arXiv preprint arXiv:2406.18853},
	year={2024}
}

@article{stiennon2020learning,
	title={Learning to summarize with human feedback},
	author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	pages={3008--3021},
	year={2020}
}

@article{zhu2024lire,
	title={LIRE: listwise reward enhancement for preference alignment},
	author={Zhu, Mingye and Liu, Yi and Zhang, Lei and Guo, Junbo and Mao, Zhendong},
	journal={arXiv preprint arXiv:2405.13516},
	year={2024}
}

@article{zhu2024personality,
	title={Personality alignment of large language models},
	author={Zhu, Minjun and Yang, Linyi and Zhang, Yue},
	journal={arXiv preprint arXiv:2408.11779},
	year={2024}
}

