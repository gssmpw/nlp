\section{Related work}
\subsection{traditional preference alignment}
	Among the wide range of algorithms proposed for preference alignment~\citep{stiennon2020learning,Yuan2023RRHFRR,rafailov2023direct,zhu2024lire}, the most prominent are perhaps RLHF and DPO. Both methods rely on human feedback to fine-tune model generations and align them with human preferences.
	RLHF follows a three-step process: first, supervised fine-tuning (SFT) is applied to the initial model; second, a reward model is trained to reflect human preferences; and finally, Reinforcement Learning (RL) techniques such as Proximal Policy Optimization (PPO)~\citep{schulman2017proximal} are used to optimize the policy based on the reward model.
	DPO simplifies the RLHF process by introducing a new parameterization of the reward model, reducing the problem to a simple classification loss, which makes DPO easier and more stable to train. However, despite these improvements, both RLHF and DPO still require substantial amounts of annotated data and significant computational resources, posing limitations for their practical application, which motives another line of research that focuses on tuning-free alignment.
	\subsection{tuning-free alignment}
Currently, alignment methods are underscoring a shift toward flexible, decoding-time techniques that adapt LLM outputs to diverse human preferences and ethical standards. One popular technique is In-Context Learning (ICL), which adapts models to new information or tasks by using a few instruction-output examples in the prompt. Another powerful inference-time algorithm is Best-of-$N$ (Bo$N$), which involves generating $N$ different samples from the model and selecting the best one, $y^*$, based on a predefined evaluation criterion $S(y_i)$. However, this method is inefficient, as generation must be executed $N$ times, prompting the search for more efficient inference-time approaches.

Recently, we have also seen the emergence of more advanced methods. \citet{lin2023unlocking} discovered that token distribution shifts between aligned and unaligned policies diminish over time during decoding. \citet{li2023rain} introduced a self-evaluation and rewind mechanism that directly aligns base LLMs with human preferences via self-boosting. \citet{huang2024deal} proposed DeAL, a framework that enables decoding-time alignment through a heuristic-guided search process and leverages programmatic constraints as well as abstract objectives to achieve alignment. \citep{shi2024decoding} introduced Multi-Objective Decoding, a method that combines predictions from multiple base models to achieve adaptable alignment during decoding.  DARWIN~\citep{hung2024reward} proposes to strike the balance between exploration and exploitation of rewards during decoding with evolutionary heuristics. Additionally, \citet{zhu2024personality} focused on alignment with personal traits and developed an activation intervention optimization method to align with individual behavioral preferences. Lately, Linear Alignment (LA)~\citep{Gao2024LinearAA} was proposed as a method for aligning language models with human preferences in a single inference step. This approach relies on a novel parameterization for policy optimization under divergence constraints and estimates the preference direction using self-contrastive decoding.
	Despite the significant progress and achievements made, there are still many gaps to be filled in this field. Therefore, in this work, we propose \modelname{} to further
	improve the efficiency of inference-time alignment.