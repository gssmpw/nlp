%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{adjustbox}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}  % Required to insert images
\usepackage{subcaption}


% \usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{float}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{booktabs}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
% \usepackage[dvipsnames]{xcolor}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Teleportation With Null Space Gradient
Projection for Optimization Acceleration}

\begin{document}

\twocolumn[
\icmltitle{Teleportation With Null Space Gradient
Projection for Optimization Acceleration}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Zihao Wu}{yyy}
\icmlauthor{Juncheng Dong}{yyy}
\icmlauthor{Ahmed Aloui}{yyy}
\icmlauthor{Vahid Tarokh}{yyy}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Electrical and
Computer Engineering, Duke University, Durham, NC 27708, USA}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Zihao Wu}{zihao.wu@duke.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
% As deep learning continues to evolve, 
Optimization techniques have become increasingly critical due to the ever-growing model complexity and data scale. In particular, teleportation has emerged as a promising approach, which accelerates convergence of gradient descent-based methods by navigating within the loss invariant level set to identify parameters with advantageous geometric properties. Existing teleportation algorithms have primarily demonstrated their effectiveness in optimizing Multi-Layer Perceptrons (MLPs), but their extension to more advanced architectures, such as Convolutional Neural Networks (CNNs) and Transformers, remains challenging. Moreover, they often impose significant computational demands, limiting their applicability to complex architectures. To this end, we introduce an algorithm that projects the gradient of the teleportation objective function onto the input null space, effectively preserving the teleportation within the loss invariant level set and reducing computational cost. Our approach is readily generalizable from MLPs to CNNs, transformers, and potentially other advanced architectures. We validate the effectiveness of our algorithm across various benchmark datasets and optimizers, demonstrating its broad applicability.
\end{abstract}

\section{Introduction}

Consider an optimization problem where the objective function, denoted by $\mathcal{L}\left(\omega\right)$, is parameterized by $\omega \in \Omega$. When $\mathcal{L}\left(\omega\right)$ is non-convex, gradient-based methods are commonly used to find a set of parameters corresponding to local minimums in the loss landscape. The standard update rule for gradient descent is given by:
\begin{align}
\mathbf{\omega}_{t+1} \leftarrow \mathbf{\omega}_t - \eta \nabla \mathcal{L}\left(\omega_t\right),
\end{align}
where $\omega_t$ represents the parameter values at iteration $t$ and $\eta>0$ is the learning rate. As a first-order method, gradient descent is computationally efficient but often suffers from slow convergence. In contrast, second-order methods, such as Newton’s method, incorporate higher-order geometric information, resulting in faster convergence. However, this comes with significant computational cost, particularly due to the need to compute and invert the Hessian matrix~\citep{hazan2019lecture}. To address this challenge, teleportation is introduced as a method that exploits higher-order geometric properties while relying solely on gradient information.

Teleportation is based on the premise that multiple points in the parameter space can yield the same loss, which forms the \textbf{\emph{loss invariant level set}} of parameters~\citep{du2018algorithmic, kunin2020neural}. This assumption is particularly feasible in modern deep learning, where most advanced models are highly over-parameterized~\citep{sagun2017empirical, tarmoun2021understanding, simsek2021geometry}. By identifying the level set, parameters can be teleported within it to \textbf{\emph{enhance the gradient norm}}, thereby accelerating the optimization process~\citep{kunin2020neural, grigsby2022functional}. 

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{images/pdf/open.png} % Change the file name to your actual image file
    \caption{From left to right: symmetry teleport (slow and limited to MLPs), linear approximation of level set (prone to error), our algorithm that projects gradient onto the input null space (fast and accurate).}
    \label{fig:open}
\end{figure*}

\textbf{Related Work.} \cite{zhao2022symmetry} indicates that the behavior of teleportation, despite utilizing only gradient information, closely resembles that of Newton’s method. An alternative perspective on teleportation is that it mitigates the locality constraints of the gradient descent algorithm, resembling the dynamics of \emph{warm restart algorithms} ~\citep{loshchilov2016sgdr, dodge2020fine, bouthillier2021accounting,ramasinghe2022you}. Under this context, each step of gradient descent is equivalent to a proximal mapping ~\citep{combettes2011proximal}.
% \begin{align}
% \omega_{t+1} = \argmin_{\omega \in \Omega} \left\{ \eta \left\langle \nabla \mathcal{L}\left(\omega_t\right), \omega \right\rangle + \frac{1}{2} \| \omega - \omega_t \|_2^2 \right\},
% \end{align}
% where $\langle\cdot\rangle$ denotes inner product
Teleportation periodically relaxes the proximal restriction, allowing the algorithm to restart at a distant location with desirable geometric properties. \emph{Compared to warm restart algorithms, teleportation incurs minimal to no increase in loss while providing greater control over the movement of parameters}. Notably, the field of teleportation reveals a gap between theoretical developments and practical applications. \cite{zhao2022symmetry} shows that gradient descent (GD) with teleportation can achieve mixed linear and quadratic convergence rates on strongly convex functions. \cite{mishkin2024level} proves that, for convex functions with Hessian stability, GD with teleportation attains a convergence rate faster than
$O(1/K)$. \textbf{\emph{However, both approaches encounter limitations when applied to empirical studies involving highly non-convex functions, which are a common characteristic of modern architectures}}. Specifically, \cite{zhao2022symmetry} develops a symmetry teleportation algorithm \emph{only for Multi-Layer Perceptrons} (MLPs) using group actions~\citep{ math9243216, ganev2021universal, armenta2023neural} . However, challenges persist in terms of its generalizability to other contemporary architectures and its relatively low efficiency. \citet{mishkin2024level}, on the other hand, tackled a sequential quadratic programming by using linear approximations of the level set, which can \emph{lead to error accumulation} when the architecture becomes more complicated and the number of teleportation steps increase (see Figure~\ref{fig:open} for a visual comparision). Moreover, both studies have primarily concentrated on empirical results involving MLPs and the vanilla Stochastic Gradient Descent (SGD) optimizer.


% The crux of teleportation in practice is to identify the loss-invariant level set of parameters. Previous literature focus on symmetry teleportation, which attempts to design a specific group action for each function, e.g., booth function, Rosenbrock function, and multi-layer perceptrons, such that after the transformation of the parameter, the loss does not change. 

% One serious pain of symmetry teleportation is its unique design for each different function. The mannually designed group action for MLP is not readily generalizable to other modern architectures, such as convolutional layers and transformers. Furthermore, the computation is usually heavy with group action. For instance, it requires two inverse operations for each layer of MLP structure and is not parallelizable since each layer's transformation is dependent on another. 

% Another recent paper assumes the convexity of functions and solves a quadratic programming problem to find the parameter with maximal gradient norm under the restriction of level set, which is in closed form with convexity assumption. However, for non-convex functions, the level set does not embrace a closed form, and they utilize a linear approximation of the level set, which is subject to error accumulation if function is complicated.

\textbf{Contributions.} Our work seeks to overcome these challenges by designing an algorithm not only \textbf{\emph{generalizes to other modern architectures}}, but also is \textbf{\emph{efficient and accurate}}. To be more specific, we eliminate the need for the bottleneck group action transformations ~\cite{zhao2022symmetry} by utilizing a more efficient \textbf{\emph{gradient projection}} technique. Moreover, instead of taking on the errors introduced by linear approximations of the level set, we \textbf{\emph{project the gradient of the teleportation objective onto the input null space of each layer}}, ensuring an accurate search on the level set thus minimal to no change in loss value.
Specifically, our contributions are:
\begin{itemize}
\item We propose a novel algorithm that utilizes gradient projection to offer improved computational efficiency and parallelization capabilities.
\item The proposed algorithm is a \textbf{\emph{general framework that can be easily applied to various modern architectures}}, including MLPs, Convolutional Neural Networks (CNNs), transformers, and potentially linear time series models such as Mamba~\citep{gu2023mamba} and TTT~\citep{sun2024learning}. As a result, our work is the first work to extend teleportation to CNNs and transformers.
\item  We present \textbf{\emph{extensive empirical results}} to demonstrate its effectiveness, spanning a range of benchmark datasets, including MNIST, FashionMNIST, CIFAR-10, CIFAR-100, Tiny-ImageNet, multi-variate time series datasets (electricity and traffic), and Penn Treebank language dataset. We also evaluate the algorithm with multiple modern optimizers, such as SGD~\citep{robbins1951stochastic}, Momentum~\citep{polyak1964some}, Adagrad~\citep{duchi2011adaptive}, and Adam~\citep{kingma2014adam}, whereas previous studies primarily focused on the vanilla SGD.
% \item While the loss invariant property of input null space gradient projection has been proven effective under image processing continual learning contexts ~\cite{saha2021gradient}, we further demonstrate its applicability to transformers for preserving loss value. This finding could have implications for future research in continual learning.
\end{itemize}

\section{Preliminary}


\subsection{Symmetry Teleportation}

In this section, we describe the general framework of teleportation through a state-of-the-art algorithm, \emph{symmetry teleportation}~\citep{zhao2022symmetry, zhao2023improving}. 
% which follows from a series of works ~\citep{armenta2023neural, math9243216}. 

Let $G$ be a set of symmetries that preserves the loss value $\mathcal{L}$, i.e., let $\omega = (X,W)$,
\begin{align}
    \mathcal{L}(X, W) = \mathcal{L}(g\cdot (X, W)), \forall g\in G,
\end{align} 
where $X$ represents data and $W$ represents \emph{parameters of the deep learning model}. Define a teleport schedule $K\subset \{0,1,..., T_{max}\}$, where $T_{max}$ is the maximum training epochs. Prior to each epoch in $K$, teleportation is applied by searching for $g\in G$ which transforms the parameter $W$ to $W^*$ with greater gradient norm \emph{within the loss invariant level set}. 
% For each teleportation step, multiple batches of data can be sampled, and $g$ can be updated multiple times per batch.

When the group $G$ is continuous, the search process can be conducted by parameterizing the group action $g$ and performing gradient ascent on $g$ with the teleportation objective function defined as the gradient norm of the current parameter $W$. For example, general linear group transformations $g \in GL_d(\mathbb{R})$ can be parameterized as $g = I + \epsilon M$, where $\epsilon \ll 1$ and $M$ is an arbitrary matrix.

\cite{zhao2022symmetry, zhao2023improving} designs a loss invariant group action \emph{specifically for MLPs with bijective activation function $\sigma$}. Assuming the invertibility of $(k-2)$-th layer's output, $h_{k-2}$, the following group action $g\in GL_d(\mathbb{R})$ on $k$-th and $(k-1)$-th layers ensures the output of the entire network unchanged:
\[
g_m \cdot W_k =
\begin{cases}
    W_m g_m^{-1} & \text{if } k = m, \\
    \sigma^{-1}\left(g_m \sigma\left(W_{m-1} h_{m-2}\right)\right) h_{m-2}^{-1} & \text{if } k = m - 1, \\
    W_k & \text{otherwise}.
\end{cases}
\] 
In practice, each teleportation update applies the above group action to every layer of an MLP, requiring two bottleneck inverse operations per update.  Denote $D_{max}$ as the largest width of the MLPs, and $n$ the sample size, assuming $D_{max}>n$. The time complexity of calculating pseudo-inverse for each layer is $O(D_{max}^2n)$. Therefore, the total time complexity for $l$ layers, $b$ batches, and $t$ teleport updates per batch is $O(D_{max}^2nlbt)$. The need for pseudo-inverse computations and the dependencies between layers render the algorithm relatively slow and unsuitable for parallelization. Additionally, there is no straightforward method to generalize this design from MLPs to CNNs or transformers.
% \subsection{Gradient Projection}

% The gradient projection technique is widely employed in deep learning, particularly in continual learning, to mitigate the issue of catastrophic forgetting ~\citep{saha2021space, saha2021gradient, lin2022trgp, saha2023continual}. Let $W$ denote the weight of any linear model after training on the first task, and let $x$ represent the input of the first task. During the training of a second task, the gradient of the loss function with respect to $W$ is projected onto the input null space of the first task prior to the update. This projection of gradient ensures that the loss of the updated model remains unchanged for the input of the first task. Concretely, for any update $\Delta$ on $W$, assuming $\Delta$ can be projected onto the null space of $x$ by a projection operator $\pi$, 
% \begin{align}
%     (W+\pi(\Delta))x= Wx+\pi(\Delta) x = Wx
% \end{align}
% shows that the output of the model for the first task remains unchanged during the training of the second task.

\subsection{Matrix Approximation With SVD}\label{AA}
An arbitrary matrix $A\in \mathbb{R}^{(m,n)}$ can be decomposed using the singular value decomposition (SVD)~\cite{klema1980singular} as $A = U\Sigma V^T$, where $U\in \mathbb{R}^{(m,m)}$ consists of orthonormal eigenvectors of $AA^T$, $\Sigma \in \mathbb{R}^{(m,n)}$ is a diagonal matrix containing sorted singular values, and $V\in \mathbb{R}^{(n,n)}$ contains orthonormal eigenvectors of $A^TA$. The matrix $A$ can be expressed as $\sum_{i=1}^r\sigma_i u_i v_i^T$,
where $r=\min(m,n)$, and $(u_i,v_i)$ are the column and row vectors of $U,V$ respectively. 

In this work, we consider the matrix approximation $A_k$ of $A$ defined as $A_k = \sum_{i=1}^k\sigma_i u_i v_i^T$, where 
\begin{align}\label{eq:threshold}
    k = \arg\min_{k} \left\{k:||A_k||_F^2 \ge \tau ||A||_F^2\right\},
\end{align}
with $\|\cdot\|_F$ denotes the Frobenius norm and $\tau \in [0,1]$ being a threshold hyper-parameter.

\section{Teleport With Null Space Gradient Projection}
Our objective is to develop a generalizable and efficient algorithm that avoids reliance on specific group action designs. Moreover, it should avoid any (linear) approximation of the level set with uncontrollable errors, as these could otherwise result in suboptimal performance. Considering the common architectural design in modern neural networks, which typically employ a linear relationship between weights and inputs of each layer, the technique of \textbf{\emph{gradient projection on to the input null space}} of each layer is well-suited for this purpose. We next elaborate on it.

\textbf{Gradient Projection.} To incorporate the geometric landscape and accelerate optimization using only gradient information, the objective function for teleportation is defined as the squared gradient norm of the loss function of the primary task with respect to the model parameter $W$,
\begin{align}
    L_{teleport} = \frac{1}{2}\|\nabla_W L_{primary}\|^2.
\end{align}

During each teleportation step, in contrast to symmetry teleportation, the gradient ascent is applied directly on the model parameter $W_l$ of each layer $l$ instead of relying on an intermediate group action $g$, i.e., we have
\begin{align}
    W_{l,t+1} = W_{l,t} + \eta\pi_l(\nabla_{W_l}L_{teleport}),\label{eq:tele-update}
\end{align}
where $\eta$ is the learning rate for teleportation update, and $\pi_l$ is the \textbf{\emph{layerwise projection operator}} onto the null space of each layer's input. We have distinct projection operators for different model architectures. \textbf{\emph{We will derive $\pi_l$ for MLPs, CNNs and transformers in the sequel}}. The validity of this projection is based on the assumption that \emph{the gradient resides within the span of each layer's input for certain structures}, which will also be elaborated in a subsequent section.


\textbf{Section Organization.} We first define and provide notations for MLPs, CNNs, and transformers. Next, we demonstrate that the gradient in Equation~\ref{eq:tele-update} indeed resides within the layerwise input space of these architectures, thus \textbf{\emph{satisfying the required assumption of gradient projection}}. Finally, we present our proposed approach and provide a detailed explanation of how to derive the projection operators for each of these architectures. 
\subsection{Deep Learning Architechtures}\label{sec:notation}
\subsubsection{Multi-Layer Perceptrons}
We define the $l$-th layer of an MLP~\citep{rumelhart1986learning}. Denote the input of the layer as $x_{l-1} \in \mathbb{R}^{(d_{l-1},1)}$, the parameter as $W_l\in \mathbb{R}^{(d_l,d_{l-1})}$, the output as $x_l\in \mathbb{R}^{(d_l,1)}$. We incorporate the bias term into $W_l$ and $x_{l-1}$ by adding an additional column to $W_l$ and unity to $x_{l-1}$. Then the output of $l$-th layer is defined as
\[
    x_{l} = \sigma(W_{l}x_{l-1}),
\]
where $\sigma$ is an activation function, e.g. ReLU~\citep{nair2010rectified}.
\subsubsection{Convolutional Neural Network}
% Filters in convolutional layer operate differently than the weights in MLPs. 
We define the $l$-th layer of a CNN~\citep{lecun1998gradient}. Denote the input to the $l$-th convolutional layer as $x_{l-1}\in \mathbb{R}^{C_i\times h_i\times w_i}$, convolutional kernel as $W_l \in \mathbb{R}^{C_o\times C_i\times k\times k}$, and output as $x_l \in \mathbb{R}^{C_o \times h_o \times w_o}$, where $C_i, h_i, w_i (C_o, h_o, w_o)$ are the input (output) channel, height, and width, respectively, and $k$ is the kernel size. If $x_{l-1}$ (e.g., with padding, striding, etc) is reshaped into $(h_o\times w_o)\times (C_i\times k\times k)$ as $X_{l-1}$, and $W_l$ is reshaped to $(C_i\times k\times k)\times C_o$, then the convolutional layer can be expressed as a matrix multiplication 
\[
    x_l = \sigma(X_{l-1}W_l),
\]
where $x_{l}\in \mathbb{R}^{(x_o\times w_o)\times C_o}$ is the output of $l$-th layer, and $\sigma$ an activation function. See Appendix~\ref{sec:visual} for a visual explanation of the matrix multiplication.

% Likewise, the backward operation can also be formulated in the form of matrix multiplication. The matrix representation of both f
\subsubsection{Transformer}
We define the self-attention and multi-head self-attention layers~\citep{vaswani2017attention}. Denote the input sequence of the $l$-th self-attention layer as $X_{l-1}\in \mathbb{R}^{T\times D_i}$, with sequence length $T$ and dimension $D_i$. The $l$-th self-attention layer is parameterized by the query matrix $W_{l,q}\in \mathbb{R}^{(D_i,D_k)}$, the key matrix $W_{l,k}\in \mathbb{R}^{(D_i,D_k)}$, and the value matrix $W_{l,v}\in \mathbb{R}^{(D_i,D_o)}$. Then, the self-attention layer maps the sequence from dimension $D_i$ to $D_o$ by
\[
    Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{D_k}})V,
\]
where $Q = X_{l-1}W_{l,q}, K=X_{l-1}W_{l,k}, V=X_{l-1}W_{l,v}$, and $D_k$ is the dimension of the model. 



The multi-head attention is realized by replicating and concatenating $N_h$ heads of low-rank self-attentions before applying an output projection, defined as
\begin{align}
    &MultiHead(X_{l-1}) = concat_{i\in [N_h]}[H^{(i)}]W_{l,o}\\
    &H^{(i)}=Attention(X_{l-1}W_{l,q}^{(i)},X_{l-1}W_{l,k}^{(i)},X_{l-1}W_{l,v}^{(i)}),
\end{align}
where $W_{l,q}^{(i)}\in \mathbb{R}^{(D_i,\frac{D_k}{N_h})}$, $W_{l,k}^{(i)}\in \mathbb{R}^{(D_i,\frac{D_k}{N_h})}$, $W_{l,v}^{(i)}\in \mathbb{R}^{(D_i,\frac{D_k}{N_h})}$ are parameters for each head. The output projection matrix $W_{l,o} \in \mathbb{R}^{(D_k,D_o)}$ maps the concatenation of heads to the desired output dimension. 
% Both attention and multi-head attention layers are typically followed by a single hidden layer MLPs, which together form a complete Transformer layer. 

\subsection{Input and Gradient Space}\label{sec:input-space}
Now we establish that \textbf{\emph{the gradient of the teleportation objective function resides within the space spanned by the input of each layer}}. Following the notation established in Section ~\ref{sec:notation}, we can readily express the gradient of the teleportation objective function with respect to the model parameter $W_l$ for each type of structure:
\begin{align*}
    \text{MLP}:\ \ \ \ \ \ \ \ &\\
    \nabla_{W_l} L_{Teleport} &= \nabla_{(W_lx_{l-1})}L_{Teleport}\cdot\nabla_{W_l}(W_lx_{l-1})\\
    &=\delta_{MLP}x_{l-1}^T\\
    \text{CNN}:\ \ \ \ \ \ \ \ &\\
    \nabla_{W_l} L_{Teleport} &= \nabla_{W_l}(X_{l-1}W_l)\cdot\nabla_{(X_{l-1}W_l)}L_{Teleport}\\
    &=X_{l-1}^T\cdot \delta_{CNN}\\
    \text{Self-Attenti}&\text{on}:\\
     \nabla_{W_{l,\cdot}^{(i)}} L_{Teleport} &= \nabla_{W_{l,\cdot}^{(i)}}(X_{l-1}W_{l,\cdot}^{(i)})\cdot\nabla_{(X_{l-1}W_{l,\cdot}^{(i)})}L_{Teleport}\\
    &=X_{l-1}^T\cdot \delta_{Attention},
\end{align*}
where $\delta_{MLP}\in \mathbb{R}^{(d_l,1)}$, $\delta_{CNN}\in \mathbb{R}^{(h_o\times w_o,C_o)}$, and $\delta_{Attention}\in \mathbb{R}^{(T,D_k)}$ are some error terms determined by both the loss function of the primary task and the objective function of the teleportation. Here, it can be observed that all gradients above can be written as the matrix multiples involving the input $X$ of each layer and another matrix. Thus, the gradient of the teleportation objective function indeed resides within the space spanned by the input of each layer for MLPs, CNNs, and transformer, which is a composition of attention layers and MLP layers. 


\begin{figure*}[htbp]
    \centering
    \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
        \centering
        \includegraphics[width=\textwidth]{images/pdf/comparison_sgd_train.pdf}
        % \caption{Figure 1}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/comparison_adagrad_train.pdf}
        % \caption{Figure 2}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/comparison_mom_train.pdf}
        % \caption{Figure 3}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/comparison_adam_train.pdf}
        % \caption{Figure 4}
    \end{subfigure}
   \\
    \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
        \centering
        \includegraphics[width=\textwidth]{images/pdf/FashionMNIST_SGD_MLP_loss_vs_time.pdf}
        % \caption{Figure 1}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/FashionMNIST_momentum_MLP_loss_vs_time.pdf}
        % \caption{Figure 2}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/FashionMNIST_Adagrad_MLP_loss_vs_time.pdf}
        % \caption{Figure 3}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/FashionMNIST_Adam_MLP_loss_vs_time.pdf}
        % \caption{Figure 4}
    \end{subfigure}
    \caption{Loss trajectories of training MLPs on the MNIST and FashionMNIST datasets. Each experiment is repeated 3 times, with the average loss plotted and the standard deviation of loss represented as the shaded area.
    }
    \label{fig:mlp}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}{0.195\textwidth} % Adjust the width as needed
        \centering
        \includegraphics[width=\textwidth]{images/pdf/efficiency_t.pdf}
    \end{subfigure}
    \begin{subfigure}{0.195\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/efficiency_d.pdf}
    \end{subfigure}
    \begin{subfigure}{0.195\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/efficiency_n.pdf}
    \end{subfigure}
    \begin{subfigure}{0.195\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/efficiency_l.pdf}
    \end{subfigure}
    \begin{subfigure}{0.195\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/efficiency_b.pdf}
    \end{subfigure}
    \caption{From left to right: a comparison between symmetry teleport and our algorithm using MLPs in terms of the scaling of runtime with respect to $t$, $d$, $n$, $l$, and $b$.}
    \label{fig:efficiency}
\end{figure*}

\subsection{Algorithm}
\textbf{Step 1.} We first construct the representation matrix for each layer $l$ based on a given teleportation batch of data:
\begin{align}
    R_{MLP}^l &= [x_{l-1,1}, x_{l-1,2},\cdots, x_{l-1,n}]\\
    R_{CNN}^l &= [X_{l-1,1}^T,X_{l-1,2}^T,\cdots,X_{l-1,n}^T]\\
    R_{Attention}^l &= [X_{l-1,1}^T,X_{l-1,2}^T,\cdots,X_{l-1,n}^T],
\end{align}
where $n$ is the batch size. Each representation matrix $R_{MLP}^l\in \mathbb{R}^{(d_{l-1},n)}$, $R_{CNN}^l\in \mathbb{R}^{(C_i\times k\times k,h_o\times w_o\times n)}$, and $R_{Attention}^l\in \mathbb{R}^{(D_i,T\times n)}$ contains columns of feature vectors, which are captured at each layer during the forward pass through the network using a random teleportation batch of size $n$. 

\textbf{Step 2.} For all model architectures, we apply SVD on the representation matrix $R^l$, followed by a low-rank approximation 
$(R^l)_k = \sum_{i=1}^k \sigma_{l,i} u_{l,i} v_{l,i}^T$ based on the criterion in Equation~\ref{eq:threshold}, using a predefined threshold $\tau$. The orthonormal column vectors $[u_{l,1}, u_{l,2},\dots,u_{l,k}]$, from SVD of $R^l$, consist of the eigenvectors corresponding to the top $k$ singular values of the representation matrix. We define the subspace spanned by these eigenvectors as \textbf{\emph{the space of significant representation}}~\citep{saha2021gradient}. 

During a teleportation step, the goal is to ensure that the gradient update in Equation~\ref{eq:tele-update} preserves the correlation between the weights and the space of significant representation as much as possible. Given that the gradient space lies within the input space, we can partition the gradient space into two orthogonal subspaces of the input space: the \textbf{\emph{Core Gradient Space (CGS)}} and the \textbf{\emph{Residual Gradient Space (RGS)}}~\citep{saha2021space}, which are spanned by $[u_{l,1}, u_{l,2},\cdots,u_{l,k}]$ and $[u_{l,k+1}, u_{l,k+2},\cdots,u_{l,r}]$ respectively. By construction, projecting the gradient onto CGS will lead to the greatest interference in the correlation between the weights and the space of significant representation, while \textbf{\emph{projecting onto RGS will result in minimal or no interference in this correlation}}. To preserve model parameters on the loss-invariant level set during teleportation steps, we project the gradient of teleportation objective function $\nabla_{W_l} L_{Teleport}$ onto the RGS before each update.

\textbf{Step 3.} Given the orthonormal basis $B_l = [u_{l,1}, u_{l,2},\cdots,u_{l,k}]$ of the CGS for the $l$-th layer, the gradient $\nabla_{W_l} L_{Teleport}$ is initially projected onto the CGS and then removed from itself to yield the projection onto the RGS. Specifically, the projection operator $\pi_l$ is defined as follows:
\begin{align}
    &\text{MLP}: \pi_l(\nabla_{W_l} L_{Teleport}) =\\ &\ \ \ \ \nabla_{W_l} L_{Teleport} - (\nabla_{W_l} L_{Teleport})B_lB_l^T
\end{align}
\begin{align}
    &\text{CNN}:\pi_l(\nabla_{W_l} L_{Teleport}) =\\ &\ \ \ \ \nabla_{W_l} L_{Teleport} - B_lB_l^T(\nabla_{W_l} L_{Teleport})
\end{align}
\begin{align}
    &\text{Self-Attention}:\pi_l(\nabla_{W_{l,\cdot}^{(i)}} L_{Teleport}) =\\ &\ \ \ \ \nabla_{W_{l,\cdot}^{(i)}} L_{Teleport} - B_lB_l^T(\nabla_{W_{l,\cdot}^{(i)}} L_{Teleport})
\end{align}

The teleportation step is completed by substituting the projection operator back into Equation ~\ref{eq:tele-update}. The complete training process is outlined in the pseudo-code presented in appendix ~\ref{sec:pseudo}.

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
        \centering
        \includegraphics[width=\textwidth]{images/pdf/CIFAR10_SGD_CNN_loss_vs_epoch.pdf}
        % \caption{Figure 1}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/CIFAR10_momentum_CNN_loss_vs_epoch.pdf}
        % \caption{Figure 2}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/CIFAR10_Adagrad_CNN_loss_vs_epoch.pdf}
        % \caption{Figure 3}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/CIFAR10_Adam_CNN_loss_vs_epoch.pdf}
        % \caption{Figure 4}
    \end{subfigure}
 % \\
 %    \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
 %        \centering
 %        \includegraphics[width=\textwidth]{images/pdf/CIFAR100_SGD_CNN_loss_vs_epoch.pdf}
 %        % \caption{Figure 1}
 %    \end{subfigure}
 %    \begin{subfigure}{0.24\textwidth}
 %        \centering
 %        \includegraphics[width=\textwidth]{images/pdf/CIFAR100_momentum_CNN_loss_vs_epoch.pdf}
 %        % \caption{Figure 2}
 %    \end{subfigure}
 %    \begin{subfigure}{0.24\textwidth}
 %        \centering
 %        \includegraphics[width=\textwidth]{images/pdf/CIFAR100_Adagrad_CNN_loss_vs_epoch.pdf}
 %        % \caption{Figure 3}
 %    \end{subfigure}
 %    \begin{subfigure}{0.24\textwidth}
 %        \centering
 %        \includegraphics[width=\textwidth]{images/pdf/CIFAR100_Adam_CNN_loss_vs_epoch.pdf}
 %        % \caption{Figure 4}
 %    \end{subfigure}
\\
    \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
        \centering
        \includegraphics[width=\textwidth]{images/pdf/Imagenet_SGD_CNN_loss_vs_epoch.pdf}
        % \caption{Figure 1}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/Imagenet_momentum_CNN_loss_vs_epoch.pdf}
        % \caption{Figure 2}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/Imagenet_Adagrad_CNN_loss_vs_epoch.pdf}
        % \caption{Figure 3}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/Imagenet_Adam_CNN_loss_vs_epoch.pdf}
        % \caption{Figure 4}
    \end{subfigure}
    \caption{Loss trajectories of training CNNs on CIFAR dataset and Tiny-Imagenet dataset. Each experiment is repeated 3 times, with the average loss plotted and the standard deviation of loss represented as the shaded area. Result of CIFAR100 is included in Appendix~\ref{sec:cnn_append}.}
    \label{fig:cnn}
\end{figure*}


\section{Experiments}
% The primary advantage of our algorithm is its generalizability to various modern neural architectures. 
In this section, we first compare our algorithm with symmetry teleport\citep{zhao2023improving}, which is the only available baseline providing a public codebase. We demonstrate the superiority of our algorithm in \textbf{\emph{performance, generalizability, and efficiency}} on MLPs.

Next, we evaluate the effectiveness of our method beyond MLPs by extending it to CNNs and transformers, utilizing \textbf{\emph{a wide range of benchmark datasets}}.
% , including MNIST, FashionMNIST, CIFAR-10, CIFAR-100, Tiny-ImageNet, multivariate time series datasets (electricity, traffic), and the Penn Treebank language corpus. 
Additionally, we evaluate our approach using \textbf{\emph{a variety of optimizers}}, such as the vanilla SGD, first-moment optimizer like SGD with momentum, second-moment optimizers like Adagrad and Adam.
% We showcase the efficiency of our algorithm compared to the state-of-the-art method, symmetry teleportation, across multiple teleportation hyperparameters. 
Furthermore, if any approximation of the level set is needed, we demonstrate the \textbf{\emph{capability of our approach to control the error in null space approximation}}, which subsequently improves the robustness of level set approximation during the teleportation.
% It is generally difficult to compare between different teleport methods with regard to how much acceleration incurred. This is because the effect of teleportation is highly sensitive to hyper-parameters, e.g. teleport learning rate and teleport schedule. What's more, different algorithms usually respond differently on same hyper-parameters due to different parameterization stragety, e.g., parameterization of g in symmetry teleport versus direct operation on model parameter in our method. Therefore, it is difficult to fairly compare between two methods. 

% Hence, we turn our attention to first showing the efficiency of our method over symmetry teleportation on MLP structure, which is what symmetry teleportation is designed for. Then, we show the generalizability and effectiveness of our methods to other important deep learning structures such as CNN and transformer. To this end, we conduct experiments using MLP, CNN, and Transformer, on an abundance of benchmark datasets including MNIST, FashionMNIST, CIFAR10, CIFAR100, TinyImagenet, and Penn Treebank language datasets, with various modern optimizers such as SGD, SGD with momentum, Adagrad, and Adam.


\subsection{Comparison with Symmetry Teleport on MLPs}
\textbf{Datasets.} To compare with symmetry teleport and demonstrate the effectiveness of our algorithm on MLPs, we conduct experiments using the MNIST digit image classification dataset and its clothing variant, FashionMNIST. The input images, with dimensions of $28 \times 28$ pixels, are flattened into vectors before being fed into the MLPs models.

 \textbf{Implementation Details.} We first adopt the same structure and hyperparameters used in~\citet{zhao2023improving} for both symmetry teleport and our algorithm. \textbf{\emph{Note that this setting favors symmetry teleport.}} This setup uses a small 3-layer MLPs with hidden dimensions [$16, 10$]. Consistent with~\citet{zhao2022symmetry}, we schedule teleportation for the first $5$ epochs of the primary training phase. For each teleportation in the schedule, we randomly sample $32$ batches of data and perform $8$ teleport updates per batch. The SVD threshold for our algorithm is set to 1, i.e., \textbf{\emph{the gradients are projected onto the exact input null space}}. We apply this setting with SGD and Adagrad optimizers.
 
 Next, we scale up the MLPs to [1024, 1024] while keeping all other hyperparameters identical between symmetry teleport and our algorithm. This larger setting is tested with Momentum and Adam optimizers. Continuing on this setting, we further demonstrate our algorithm's ability to accelerate optimization in terms of \textbf{\emph{time}} on FashionMNIST dataset across all four optimizers. See Appendix~\ref{sec: implem} for complete implementation details.

\textbf{Experiment Results.} 
In Figure~\ref{fig:mlp}, the first two graphs in the top row depict the training loss trajectories, comparing symmetry teleport with our algorithm in the small MLPs setting.  \textbf{\emph{Despite the setting being designed to favor symmetry teleport, our algorithm still achieves faster convergence and a lower final loss}}. The last two graphs in the top row illustrate the training loss trajectories after scaling up to larger MLPs. Interestingly, \textbf{\emph{symmetry teleport no longer accelerates optimization}} but instead teleports to an ill-conditioned geometric trajectory, slowing down the optimization process. \textbf{\emph{This highlights the superior generalizability of our algorithm}} to a broader class of functions, making it particularly advantageous in the latest era characterized by models of increasing size. See Appendix~\ref{sec:comparison_append} for complete test loss trajectories.

The bottom row in Figure~\ref{fig:mlp} presents the loss trajectories over \textbf{\emph{time}} using the FashionMNIST dataset. The plots look almost identical to the loss trajectory with respect to epoch (Figure~\ref{fig:mlp_append} in Appendix), indicating that the cost of teleportation is negligible compared to gradient descents.


 \begin{figure*}[htbp]
    \centering
    % \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
    %     \centering
    %     \includegraphics[width=\textwidth]{images/pdf/MNIST_SGD_transformer_loss_vs_epoch.pdf}
    %     % \caption{Figure 1}
    % \end{subfigure}
   %  \begin{subfigure}{0.24\textwidth}
   %      \centering
   %      \includegraphics[width=\textwidth]{images/pdf/MNIST_momentum_transformer_loss_vs_epoch.pdf}
   %      % \caption{Figure 2}
   %  \end{subfigure}
   %  \begin{subfigure}{0.24\textwidth}
   %      \centering
   %      \includegraphics[width=\textwidth]{images/pdf/MNIST_Adagrad_transformer_loss_vs_epoch.pdf}
   %      % \caption{Figure 3}
   %  \end{subfigure}
   %  \begin{subfigure}{0.24\textwidth}
   %      \centering
   %      \includegraphics[width=\textwidth]{images/pdf/MNIST_Adam_transformer_loss_vs_epoch.pdf}
   %      % \caption{Figure 4}
   %  \end{subfigure}
   % \\
    \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
        \centering
        \includegraphics[width=\textwidth]{images/pdf/electricity_SGD_transformer_loss_vs_epoch.pdf}
        % \caption{Figure 1}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/electricity_momentum_transformer_loss_vs_epoch.pdf}
        % \caption{Figure 2}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/electricity_Adagrad_transformer_loss_vs_epoch.pdf}
        % \caption{Figure 3}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/electricity_Adam_transformer_loss_vs_epoch.pdf}
        % \caption{Figure 4}
    \end{subfigure}
  \\
    \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
        \centering
        \includegraphics[width=\textwidth]{images/pdf/traffic_SGD_transformer_loss_vs_epoch.pdf}
        % \caption{Figure 1}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/traffic_momentum_transformer_loss_vs_epoch.pdf}
        % \caption{Figure 2}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/traffic_Adagrad_transformer_loss_vs_epoch.pdf}
        % \caption{Figure 3}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/traffic_Adam_transformer_loss_vs_epoch.pdf}
        % \caption{Figure 4}
    \end{subfigure}
  \\
    \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
        \centering
        \includegraphics[width=\textwidth]{images/pdf/PennTree_SGD_transformer_loss_vs_epoch.pdf}
        % \caption{Figure 1}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/PennTree_momentum_transformer_loss_vs_epoch.pdf}
        % \caption{Figure 2}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/PennTree_Adagrad_transformer_loss_vs_epoch.pdf}
        % \caption{Figure 3}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/PennTree_Adam_transformer_loss_vs_epoch.pdf}
        % \caption{Figure 4}
    \end{subfigure}
    \caption{Loss trajectories of training Transformers on sequential MNIST, electricity, traffic, and Penn Treebank datasets. Each experiment is repeated 3 times, with the average loss plotted and the standard deviation of loss represented as the shaded area.}
    \label{fig:att}
\end{figure*}

\textbf{Efficiency Improvement.}\label{sec:eff}
We demonstrate the efficiency of our algorithm compared to the symmetry teleport algorithm.
Recall that the time complexity of symmetry teleportation is $O(d^2nlbt)$, where $d$ is the feature dimension of layers, $n$ is the batch size, $l$ is the number of layers, $b$ is the number of batches, and $t$ is the number of teleport steps per batch. Note that the pseudo-inverse is calculated using SVD for Pytorch Library, thus sharing the same time complexity as SVD operation. However, in our method, only one SVD is needed for each batch of data, which reduces the bottleneck and brings the time complexity down to $O(d^2nlb)$, \textbf{\emph{enitrely removing dependence on $t$}}. Ideally, by leveraging our algorithm’s layer-independent property, \textbf{\emph{computations can be parallelized across all layers}}, further reducing the time complexity to $O(d^2nb)$. However, we leave such engineering optimizations for future work.


In practice, as demonstrated in Figure~\ref{fig:efficiency}, our algorithm exhibits linear scaling with respect to 
$t$, $l$, and $b$, while the runtime of the symmetry teleport increases at a significantly faster rate. Notably, for $d$ and $n$, our approach achieves \textbf{\emph {near-constant runtime}} in contrast to the linear-to-polynomial runtime of the symmetry teleport. Ideally, once the layer parallelization is fully implemented, we anticipate that constant runtime will also be achieved with an increasing number of layers, thereby enhancing overall performance.



\subsection{CNN Experiments}
\textbf{Datasets and Implementation.} We use the CIFAR-10, CIFAR-100 (results included in Appendix~\ref{sec:cnn_append}), and Tiny-Imagenet datasets to evaluate the effectiveness of our algorithm on CNNs. The image size for the Tiny-Imagenet dataset is kept the same as the full Imagenet dataset, i.e., $3\times224\times224$.
For the CIFAR datasets, we use a $3$-layer CNNs with channels [$3, 16, 32, 64$]. For the Tiny-Imagenet dataset, we utilize a residual network with channels [$3, 64, 64, 64, 128, 128, 128, 256, 256, 256$]. A classification head is connected after the final channel for both architectures. The teleportation scheduling and threshold $\tau$ remains the same as in the MLPs experiments. See Appendix~\ref{sec: implem} for complete implementation details.


\textbf{Experiment Results.} With teleportation, we observe in Figure~\ref{fig:cnn} a marked acceleration in optimization in the beginning of each training, coinciding with the application of teleportation. The test loss with teleportation tends to converge to the same value as the non-teleportation counterpart, while the training loss with teleportation continues to decrease at a faster rate even after the test loss has plateaued. This behavior is expected, as the teleportation objective is defined as the squared norm of the gradient, which prioritizes faster convergence on the training set rather than improving generalization. The teleportation framework is highly flexible, allowing the teleportation objective function to be adjusted to other reasonable choices, such as the curvature of the parameter landscape, which has been shown to enhance generalization \citep{zhao2023improving}.


\begin{figure*}[htbp]
    \centering
    \begin{subfigure}{0.4\textwidth} % Adjust the width as needed
        \centering
        \includegraphics[width=\textwidth]{images/pdf/captured_variance.png}
        \caption{Input variance captured by eigenvectors.}
        \label{fig:captured_variance}
    \end{subfigure}
    \begin{subfigure}{0.59\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/error_control.png}
        \caption{Effect of teleport step on increase of gradient norm and loss value.}
        \label{fig:error_control}
    \end{subfigure}
    \caption{A majority of the input variance is captured by a relatively small proportion of the input space. As we approximate a larger input null space, the gradient norm increases more rapidly during teleportation, while the loss remains constant when $\tau$ is greater than $0.99$.}
\end{figure*}




\subsection{Transformer Experiments}
\textbf{Datasets and Implementation.} We first consider the MNIST dataset as a sequential classification task, with a sequence length of $28\times28$ and a data dimension $1$. Results included in appendix~\ref{sec:smnist}.

Next, we evaluate on two publicly available multi-variate time series forecasting datasets: electricity and traffic. The electricity dataset consists of $321$ dimensions with a total sequence length of $26,304$. The sample sequence length is set to $7\times24$, and the regression target is the data point $24$ hours after the input sample. The traffic dataset consists of $862$ dimensions, with a total sequence length of $17,544$. See Appendix ~\ref{sec:data} for a detailed explanation.

We also evaluate on the Penn Treebank (PTB) language corpus. We use a decoder-only transformer structure and formulate the problem as a causal self-supervised learning task, where the label is the input shifted to the right by one.

For the sequential MNIST dataset, we use a small Transformer model with $2$ heads, each having a dimension of $64$, stacked across two layers. For the regression and language datasets, we use a transformer with 4 heads, each with a dimension of $64$, stacked across $4$ layers without pooling, followed by a linear output projection. See appendix ~\ref{sec: implem} for complete implementation details.

% For the sequential MNIST dataset, we use a small Transformer model with $2$ heads, each having a dimension of $64$, stacked across two layers. This is followed by an average pooling layer and a ten-way linear classification head, optimized using cross-entropy loss. For the electricity and traffic datasets, we use a transformer with 4 heads, each with a dimension of $64$, stacked across $4$ layers without pooling, followed by a linear regression head where the output dimension matches the input dimension. For the PTB dataset, we use the same Transformer architecture but replace the first linear layer with an embedding layer and set the output dimension to the vocabulary size, which is approximately $10,000$. See appendix ~\ref{sec: implem} for complete implementation details.\\


\textbf{Experiment Results.} In addition to the observations from previous experiments, in Figure~\ref{fig:att}, we notice that \textbf{\emph{teleportation remains effective across different problem settings, including regression problems and language modeling}}. Significant acceleration is observed in the regression datasets, particularly with the SGD and momentum optimizers, where the loss with teleportation converges within the first few epochs, while the non-teleportation counterpart takes more than $50$ epochs to converge on the traffic dataset. Furthermore, the acceleration with teleportation in language modeling is particularly notable during the initial phase of training, even though both approaches eventually converge to the same loss. These results highlight the potential of applying teleportation to the training of large language models.




\subsection{Error Control}


In addition to its efficiency, \textbf{\emph {our algorithm provides a distinct advantage in controlling the error associated with increased loss during teleportation}}. Figure~\ref{fig:captured_variance} records the information of the input space of the second layer in MLPs, CNNs, and Transformers (with the same architechtures used in experiments) across all datasets. Most variance of input is captured by the space of significant representation of a relatively small proportion of total dimensions, represented by the percentages of sorted eigenvectors in SVD. Consequently, even without approximating the input null space, sufficient dimensions are typically available in the null space to facilitate gradient projection and search. \textbf{\emph{This validates our choice of setting $\tau$ to be $1$ in most cases.}} Figure~\ref{fig:error_control} further confirms that when the threshold $\tau$ is set to $1$, meaning the exact null space is utilized, the gradient norm increases steadily during teleportation while the loss remains constant. Moreover, as $\tau$ decreases, the gradient is projected onto an approximated null space with a significantly larger number of dimensions, yet capturing only slightly more variance with minimal impact on the loss. A remarkable increase in the gradient norm ascending speed is observed when $\tau$ is set to $0.99$, with the loss still remaining constant. (Experiments in Figure~\ref{fig:error_control} are conducted using transformer on sMNIST.)


\section{Discussion and Conclusion}
In this paper, we propose a novel algorithm that generalizes the application of teleportation from MLPs to other modern architectures such as CNNs and transformers. The algorithm demonstrates improved computational efficiency and introduces explicit error control during the level set approximation, if such an approximation is employed.

% Gradient projection proves to be a powerful tool for modern AI, as most contemporary architectures rely on a linear modeling between inputs and weights. Consequently, our framework has the potential to be generalized to emerging time-series architectures such as Mamba and TTT.

Despite its promising performance, teleportation still faces challenges when applied broadly in the deep learning field. One of the major challenges is the selection of hyperparameters. Identifying a generalizable set of hyperparameters suitable for all architectures and datasets remains difficult. Developing a simple and effective hyperparameter selection strategy will significantly enhance the overall efficiency of teleportation.


\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

% \bibliography{main}
\bibliographystyle{icml2025}
\input{main.bbl}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}
\subsection{Pseudocode}\label{sec:pseudo}

\begin{algorithm}[H]
\caption{Teleportation with Input Null Space Gradient Projection}
\textbf{Input:} Loss function $\mathcal{L}(w)$, number of epochs for primary task $T$, teleport learning rate $\eta$, teleport batch number $b$, teleport step number $t$,  teleport schedule $K$, threshold maximum gradient norm value $\text{CAP}$, initialized parameters $w_0$. \\
\textbf{Output:} $w_{T}$.
\begin{algorithmic}[1]
\For{$i \gets 0$ to $T - 1$}
    \If{$i \in K$}
        \For{$b$ batches}
            \State $\text{Null space projection matrix}$ $\pi \gets \text{SVD(batch)}$
            \For{t steps}                   
                \If{$\|\nabla_{w}\mathcal{L}|_{w_i}\|^2 < \text{CAP}$}
                    \State $w_i \gets w_i - \eta\pi(\nabla_{w} \|\nabla_{w}\mathcal{L}|_{w_i}\|^2|_{w_i})$
                \Else
                    \State \textbf{break}
                \EndIf

            \EndFor
        \EndFor
    \EndIf
    \State Continue the training of the primary task
\EndFor
\State \textbf{return} $w_{T}$
\end{algorithmic}
\end{algorithm}

\subsection{Additional Results}
\subsubsection{Complete Test Loss Trajectories Comparison Between Symmetry Teleport and Our Algorithm}\label{sec:comparison_append}
\begin{figure*}[htbp]
    \centering

    \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
        \centering
        \includegraphics[width=\textwidth]{images/pdf/comparison_sgd.pdf}
        % \caption{Figure 1}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/comparison_adagrad.pdf}
        % \caption{Figure 2}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/comparison_mom.pdf}
        % \caption{Figure 3}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/comparison_adam.pdf}
        % \caption{Figure 4}
    \end{subfigure}
    \caption{Complete train and test loss trajectories of training MLPs on the MNIST dataset, comparing symmetry teleport and our algorithm. Each experiment is repeated 3 times, with the average loss plotted and the standard deviation of loss represented as the shaded area.
    }
    \label{}
\end{figure*}
\subsubsection{MLP on MNIST and FashionMNIST datasets}
\begin{figure*}[htbp]
    \centering
    \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
        \centering
        \includegraphics[width=\textwidth]{images/pdf/MNIST_SGD_MLP_loss_vs_epoch.pdf}
        % \caption{Figure 1}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/MNIST_momentum_MLP_loss_vs_epoch.pdf}
        % \caption{Figure 2}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/MNIST_Adagrad_MLP_loss_vs_epoch.pdf}
        % \caption{Figure 3}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/MNIST_Adam_MLP_loss_vs_epoch.pdf}
        % \caption{Figure 4}
    \end{subfigure}
   \\
    \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
        \centering
        \includegraphics[width=\textwidth]{images/pdf/FashionMNIST_SGD_MLP_loss_vs_epoch.pdf}
        % \caption{Figure 1}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/FashionMNIST_momentum_MLP_loss_vs_epoch.pdf}
        % \caption{Figure 2}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/FashionMNIST_Adagrad_MLP_loss_vs_epoch.pdf}
        % \caption{Figure 3}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/FashionMNIST_Adam_MLP_loss_vs_epoch.pdf}
        % \caption{Figure 4}
    \end{subfigure}
    \caption{Loss trajectories of training MLPs on the MNIST and FashionMNIST datasets. Each experiment is repeated 3 times, with the average loss plotted and the standard deviation of loss represented as the shaded area.
    }
    \label{fig:mlp_append}
\end{figure*}
\subsubsection{CNN on CIFAR100 dataset}\label{sec:cnn_append}
\begin{figure*}[htbp]
    \centering
 %    \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
 %        \centering
 %        \includegraphics[width=\textwidth]{images/pdf/CIFAR10_SGD_CNN_loss_vs_epoch.pdf}
 %        % \caption{Figure 1}
 %    \end{subfigure}
 %    \begin{subfigure}{0.24\textwidth}
 %        \centering
 %        \includegraphics[width=\textwidth]{images/pdf/CIFAR10_momentum_CNN_loss_vs_epoch.pdf}
 %        % \caption{Figure 2}
 %    \end{subfigure}
 %    \begin{subfigure}{0.24\textwidth}
 %        \centering
 %        \includegraphics[width=\textwidth]{images/pdf/CIFAR10_Adagrad_CNN_loss_vs_epoch.pdf}
 %        % \caption{Figure 3}
 %    \end{subfigure}
 %    \begin{subfigure}{0.24\textwidth}
 %        \centering
 %        \includegraphics[width=\textwidth]{images/pdf/CIFAR10_Adam_CNN_loss_vs_epoch.pdf}
 %        % \caption{Figure 4}
 %    \end{subfigure}
 % \\
    \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
        \centering
        \includegraphics[width=\textwidth]{images/pdf/CIFAR100_SGD_CNN_loss_vs_epoch.pdf}
        % \caption{Figure 1}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/CIFAR100_momentum_CNN_loss_vs_epoch.pdf}
        % \caption{Figure 2}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/CIFAR100_Adagrad_CNN_loss_vs_epoch.pdf}
        % \caption{Figure 3}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/CIFAR100_Adam_CNN_loss_vs_epoch.pdf}
        % \caption{Figure 4}
    \end{subfigure}
% \\
%     \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
%         \centering
%         \includegraphics[width=\textwidth]{images/pdf/Imagenet_SGD_CNN_loss_vs_epoch.pdf}
%         % \caption{Figure 1}
%     \end{subfigure}
%     \begin{subfigure}{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/pdf/Imagenet_momentum_CNN_loss_vs_epoch.pdf}
%         % \caption{Figure 2}
%     \end{subfigure}
%     \begin{subfigure}{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/pdf/Imagenet_Adagrad_CNN_loss_vs_epoch.pdf}
%         % \caption{Figure 3}
%     \end{subfigure}
%     \begin{subfigure}{0.24\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{images/pdf/Imagenet_Adam_CNN_loss_vs_epoch.pdf}
%         % \caption{Figure 4}
%     \end{subfigure}
    \caption{Loss trajectories of training CNNs on CIFAR100 dataset. Each experiment is repeated 3 times, with the average loss plotted and the standard deviation of loss represented as the shaded area.}
    \label{fig:cnn_append}
\end{figure*}

\subsubsection{Transformer on Sequential MNIST dataset}\label{sec:smnist}
\begin{figure*}[htbp]
    \centering
    \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
        \centering
        \includegraphics[width=\textwidth]{images/pdf/MNIST_SGD_transformer_loss_vs_epoch.pdf}
        % \caption{Figure 1}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/MNIST_momentum_transformer_loss_vs_epoch.pdf}
        % \caption{Figure 2}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/MNIST_Adagrad_transformer_loss_vs_epoch.pdf}
        % \caption{Figure 3}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pdf/MNIST_Adam_transformer_loss_vs_epoch.pdf}
        % \caption{Figure 4}
    \end{subfigure}
  %  \\
  %   \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
  %       \centering
  %       \includegraphics[width=\textwidth]{images/pdf/electricity_SGD_transformer_loss_vs_epoch.pdf}
  %       % \caption{Figure 1}
  %   \end{subfigure}
  %   \begin{subfigure}{0.24\textwidth}
  %       \centering
  %       \includegraphics[width=\textwidth]{images/pdf/electricity_momentum_transformer_loss_vs_epoch.pdf}
  %       % \caption{Figure 2}
  %   \end{subfigure}
  %   \begin{subfigure}{0.24\textwidth}
  %       \centering
  %       \includegraphics[width=\textwidth]{images/pdf/electricity_Adagrad_transformer_loss_vs_epoch.pdf}
  %       % \caption{Figure 3}
  %   \end{subfigure}
  %   \begin{subfigure}{0.24\textwidth}
  %       \centering
  %       \includegraphics[width=\textwidth]{images/pdf/electricity_Adam_transformer_loss_vs_epoch.pdf}
  %       % \caption{Figure 4}
  %   \end{subfigure}
  % \\
  %   \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
  %       \centering
  %       \includegraphics[width=\textwidth]{images/pdf/traffic_SGD_transformer_loss_vs_epoch.pdf}
  %       % \caption{Figure 1}
  %   \end{subfigure}
  %   \begin{subfigure}{0.24\textwidth}
  %       \centering
  %       \includegraphics[width=\textwidth]{images/pdf/traffic_momentum_transformer_loss_vs_epoch.pdf}
  %       % \caption{Figure 2}
  %   \end{subfigure}
  %   \begin{subfigure}{0.24\textwidth}
  %       \centering
  %       \includegraphics[width=\textwidth]{images/pdf/traffic_Adagrad_transformer_loss_vs_epoch.pdf}
  %       % \caption{Figure 3}
  %   \end{subfigure}
  %   \begin{subfigure}{0.24\textwidth}
  %       \centering
  %       \includegraphics[width=\textwidth]{images/pdf/traffic_Adam_transformer_loss_vs_epoch.pdf}
  %       % \caption{Figure 4}
  %   \end{subfigure}
  % \\
  %   \begin{subfigure}{0.24\textwidth} % Adjust the width as needed
  %       \centering
  %       \includegraphics[width=\textwidth]{images/pdf/PennTree_SGD_transformer_loss_vs_epoch.pdf}
  %       % \caption{Figure 1}
  %   \end{subfigure}
  %   \begin{subfigure}{0.24\textwidth}
  %       \centering
  %       \includegraphics[width=\textwidth]{images/pdf/PennTree_momentum_transformer_loss_vs_epoch.pdf}
  %       % \caption{Figure 2}
  %   \end{subfigure}
  %   \begin{subfigure}{0.24\textwidth}
  %       \centering
  %       \includegraphics[width=\textwidth]{images/pdf/PennTree_Adagrad_transformer_loss_vs_epoch.pdf}
  %       % \caption{Figure 3}
  %   \end{subfigure}
  %   \begin{subfigure}{0.24\textwidth}
  %       \centering
  %       \includegraphics[width=\textwidth]{images/pdf/PennTree_Adam_transformer_loss_vs_epoch.pdf}
  %       % \caption{Figure 4}
  %   \end{subfigure}
    \caption{Loss trajectories of training Transformers on sequential MNIST dataset. Each experiment is repeated 3 times, with the average loss plotted and the standard deviation of loss represented as the shaded area.}
    \label{fig:att_append}
\end{figure*}

\subsection{Implementation Details}\label{sec: implem}
In table ~\ref{table:hyper-param}, we summarize the hyper-parameters used in experiments. We denote the base learning rate for primary task as $\eta_{prim}$, the learning rate for teleportation as $\eta_{tele}$, maximum epoch for primary task as $T_{prim}$, teleport batch size as $n$, and teleport cap threshold as CAP. The batch size for the primary task is set to $32$, the number of teleport batches set to $32$, and the number of teleportation steps per batch set to $8$ throughout all experiments.

\textbf{MLP}\\
\textbf{Datasets.} To demonstrate the effectiveness of our method with MLPs, we conduct experiments using the MNIST digit image classification dataset and its clothing variant, FashionMNIST. Both datasets are split into $60,000$ samples for training and $10,000$ samples for testing. The input images, with dimensions of $28 \times 28$ pixels, are flattened into vectors before being fed into the MLPs models.

\textbf{Implementation Detail.} We use a 3-layer MLPs with hidden dimensions [$1024, 1024$], ReLU activation function, and cross-entropy loss. Following the convention in ~\cite{zhao2022symmetry}'s work, we schedule teleportation for the first $5$ epochs of the primary training phase. For each teleportation in the schedule, we randomly sample $32$ batches of data and perform $8$ teleport updates per batch. The SVD threshold is set to 1, i.e., \textbf{\emph{the gradients are projected onto the exact input null space}}. Learning rates are set differently depending on the optimizer used. See the appendix ~\ref{sec: implem} for complete implementation details.

\textbf{CNN}\\
\textbf{Datasets.} We use the CIFAR-10, CIFAR-100, and Tiny-Imagenet datasets to evaluate the effectiveness of our algorithm on CNNs. Both CIFAR datasets are split into 50,000 training samples and 10,000 test samples. The image size for CIFAR datasets is $3\times32\times32$. The Tiny-Imagenet dataset is a smaller version of the full Imagenet dataset, containing $200$ image classes with $100,000$ training images and $20,000$ validation/test images. The image size for the Tiny-Imagenet dataset is kept the same as the full Imagenet dataset, i.e., $3\times224\times224$.

\textbf{Implementation Detail.} For the CIFAR datasets, we use a $3$-layer CNNs with channels [$3, 16, 32, 64$], max pooling after each layer, ReLU activation function, and cross-entropy loss. For the Tiny-Imagenet dataset, we utilize a residual network with channels [$3, 64, 64, 64, 128, 128, 128, 256, 256, 256$], and $3$ residual connections between channels of same shape. Instead of max pooling, we use larger strides to reduce the feature size, a common practice in the design of residual networks. A classification head is connected after the final channel for both architectures. The teleportation scheduling and threshold $\tau$ remains the same as in the MLPs experiments. See appendix ~\ref{sec: implem} for complete implementation details.

For all experiments using CNNs, we perform $40$ warm-up steps before the first teleportation to stabilize the behavior of the gradients.

\textbf{Transformer}\\
\textbf{Datasets.} We first consider the MNIST dataset as a sequential classification task, with a sequence length of $28\times28$ and a data dimension $1$. 

Next, we evaluate on two publicly available multi-variate time series regression datasets: electricity and traffic. The electricity dataset consists of $321$ dimensions with a total sequence length of $26,304$. The sample sequence length is set to $7\times24$, representing a week's worth of data. The regression target is the data point of the same dimension $24$ hours after the input sample. The traffic dataset consists of $862$ dimensions, with a total sequence length of $17,544$. The data is similarly manipulated to regress a week's worth of data to the data $24$ hours after the week. See Appendix ~\ref{sec:data} for a detailed explanation.

% The electricity dataset tracks electricity consumption in kWh every $15$ minutes from $2012$ to $2014$ for $321$ clients, adjusted to reflect hourly consumption. The dataset consists of $321$ dimensions with a total sequence length of $26,304$. The sample sequence length is set to $7\times24$, representing a week's worth of data. The regression target is the data point of the same dimension $24$ hours after the input sample. The traffic dataset contains $48$ months $(2015–2016)$ of hourly data from the California Department of Transportation, describing road occupancy rates (between $0$ and $1$) measured by various sensors on the San Francisco Bay Area freeway. This dataset consists of $862$ dimensions, with a total sequence length of $17,544$. The data is similarly manipulated to regress a week's worth of data to the data $24$ hours after the week. 

We also evaluate on the Penn Treebank (PTB) language corpus. We use the default train/test split of the PTB dataset, where the training set contains approximately $950,000$ words and the test set approximately $80,000$ words. We use the TreebankWord tokenizer from the nltk Library and set the sequence length to 256. As is common practice, we formulate the problem as a causal self-supervised learning task, where the label is the input shifted to the right by one.

\textbf{Implementation Detail.} For the sequential MNIST dataset, we use a small Transformer model with $2$ heads, each having a dimension of $64$, stacked across two layers. For the regression and language datasets, we use a transformer with 4 heads, each with a dimension of $64$, stacked across $4$ layers without pooling, followed by a linear output. See appendix ~\ref{sec: implem} for complete implementation details.


For the sequential MNIST dataset, we use a small Transformer model with $2$ heads, each having a dimension of $64$, stacked across two layers. This is followed by an average pooling layer and a ten-way linear classification head, optimized using cross-entropy loss. For the electricity and traffic datasets, we use a transformer with 4 heads, each with a dimension of $64$, stacked across $4$ layers without pooling, followed by a linear regression head where the output dimension matches the input dimension. For the PTB dataset, we use the same Transformer architecture but replace the first linear layer with an embedding layer and set the output dimension to the vocabulary size, which is approximately $10,000$. 

\begin{table}[htbp]
\centering
\begin{tabular}{|p{4.5cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|p{1cm}|}
\hline
\textbf{Dataset (Optimizer)} & \textbf{$\eta_{prim}$} & \textbf{$\eta_{tele}$} & \textbf{$T_{prim}$}  & \textbf{n} &  \textbf{CAP} \\
\hline
MNIST (SGD) & $2e-4$&$2e-1$ & $100$&$32$ & $5$  \\
\hline
MNIST (Momentum) & $2e-4$& $2e-1$& $100$&$32$ & $5$\\
\hline
MNIST (Adagrad) & $2e-4$& $2e-1$& $100$&$32$   & $5$\\
\hline
MNIST (Adam) &$2e-4$ & $2e-1$&$100$ &$32$   &$5$ \\
\hline
FashionMNIST (SGD) &$2e-4$ &$2e-1$ & $100$&$32$   &$5$ \\
\hline
FashionMNIST (Momentum) & $2e-4$&$2e-1$ &$100$ &$32$   &$5$ \\
\hline
FashionMNIST (Adagrad) &$2e-4$ & $2e-1$& $100$& $32$  & $5$\\
\hline
FashionMNIST (Adam) & $2e-4$&$2e-1$ & $100$&$32$ &$5$   \\
\hline
CIFAR10 (SGD) & $1e-4$& $3e-3$&$100$ &$256$   &$40$ \\
\hline
CIFAR10 (Momentum) &$1e-4$ & $3e-3$& $100$&$256$   &$40$ \\
\hline
CIFAR10 (Adagrad) &$1e-4$ &$3e-3$ &$100$ & $256$&$40$   \\
\hline
CIFAR10 (Adam) &$1e-5$ &$3e-3$ &$300$ &$256$   & $40$\\
\hline
CIFAR100 (SGD) &$1e-4$ &$3e-3$ &$400$ &$256$   &$40$ \\
\hline
CIFAR100 (Momentum) & $1e-4$&$3e-3$ & $400$&$256$   &$40$ \\
\hline
CIFAR100 (Adagrad) &$1e-4$ & $3e-3$&$400$ &$256$   & $40$\\
\hline
CIFAR100 (Adam) &$3e-5$ &$3e-3$ & $400$&$256$   &$40$ \\
\hline
Tiny Imagenet (SGD) & $2e-4$&$3e-3$ & $400$& $32$& $40$  \\
\hline
Tiny Imagenet (Momentum) &$2e-4$ & $3e-3$&$400$   &$32$ & $40$\\
\hline
Tiny Imagenet (Adagrad) &$2e-4$ & $3e-3$&$400$   &$32$ & $40$\\
\hline
Tiny Imagenet (Adam) &$5e-5$ &$3e-3$ & $400$& $32$  &$40$ \\
\hline
sMNIST (SGD) &$1e-3$ & $3e-3$&$400$ &$32$ & $10$  \\
\hline
sMNIST (Momentum) &$1e-3$ & $3e-3$&$400$ &$32$   & $10$\\
\hline
sMNIST (Adagrad) & $1e-3$& $3e-3$&$400$ &$32$ & $10$  \\
\hline
sMNIST (Adam) &$1e-4$ &$3e-3$ &$400$ &$32$   &$10$ \\
\hline
electricity (SGD) &$1e-4$ &$3e-3$ &$50$ &$32$   & $10$\\
\hline
electricity (Momentum) &$1e-4$ &$3e-3$ & $50$&$32$   & $10$\\
\hline
electricity (Adagrad) &$1e-4$ & $3e-3$& $50$& $32$  &$10$ \\
\hline
electricity (Adam) &$1e-4$ &$3e-3$ &$50$ & $32$ & $10$ \\
\hline
traffic (SGD) &$1e-4$ &$3e-3$ &$50$ & $32$& $10$  \\
\hline
traffic (Momentum) &$1e-4$ &$3e-3$ &$50$ & $32$ & $10$ \\
\hline
traffic (Adagrad) &$1e-4$ &$3e-3$ &$50$ & $32$  &$10$ \\
\hline
traffic (Adam) & $1e-4$&$3e-3$ &$50$ & $32$  &$10$ \\
\hline
Penn Treebank (SGD) &$2e-4$ &$5e-2$ &$20,000$ steps & $32$  &$5$ \\
\hline
Penn Treebank (Momentum) &$2e-4$ & $5e-2$&$20,000$ steps &$32$  &$5$ \\
\hline
Penn Treebank (Adagrad) &$2e-4$ &$5e-2$ &$20,000$ steps &$32$ &  $5$ \\
\hline
Penn Treebank (Adam) &$5e-5$ &$5e-2$ & $20,000$ steps&$32$ &  $5$ \\
\hline
\end{tabular}
\caption{Summary table for hyper-parameters of all experiments}
\label{table:hyper-param}
\end{table}
\subsection{Visualization of Matrix Multiplication Representation for CNNs}\label{sec:visual}
Although filters in CNNs works differently than weights in MLPs, the forward and backward propagations of CNNs are essentially still matrix multiplications (see Figure ~\ref{fig:cnnmatmul}).
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/pdf/CNNgraph.png} % Change the file name to your actual image file
    \caption{Visualization of matrix representation of forward and backward pass for CNNs.}
    \label{fig:cnnmatmul}
\end{figure}

\subsection{Brief Explanation of The Multi-variate Time Series Regression Datasets} \label{sec:data}
The electricity dataset tracks electricity consumption in kWh every $15$ minutes from $2012$ to $2014$ for $321$ clients, adjusted to reflect hourly consumption. The dataset consists of $321$ dimensions with a total sequence length of $26,304$. The sample sequence length is set to $7\times24$, representing a week's worth of data. The regression target is the data point of the same dimension $24$ hours after the input sample. The traffic dataset contains $48$ months $(2015–2016)$ of hourly data from the California Department of Transportation, describing road occupancy rates (between $0$ and $1$) measured by various sensors on the San Francisco Bay Area freeway. This dataset consists of $862$ dimensions, with a total sequence length of $17,544$. The data is similarly manipulated to regress a week's worth of data to the data $24$ hours after the week. 



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
