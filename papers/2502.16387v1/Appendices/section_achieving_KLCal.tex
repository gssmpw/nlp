\section{Deferred proofs in Section \ref{sec:achieve-KL-Cal}}\label{app:proof_existence_swap_log_loss}

\subsection{Proof of Theorem \ref{thm:log_loss_swap_reg}}
\begin{theorem}[Von-Neumann's Minimax Theorem]\label{thm:von_neumann}
    Let $M \in \Rn^{r \times c}$ for  $r, c \in \mathbb{N}$. Then, \begin{align*}
        \min_{p \in \Delta_{r}} \max_{q \in \Delta_{c}} p^{\intercal} M q = \max_{q \in \Delta_{c}} \min_{p \in \Delta_{r}} p^{\intercal} M q.
    \end{align*}
\end{theorem}

\begin{proof}[Proof of Theorem \ref{thm:log_loss_swap_reg}]
We prove a stronger statement that the result holds against any adaptive adversary. In the forecasting setup, let $\cH_{t - 1} = \{p_{1}, \dots, p_{t - 1}\} \cup \{y_{1}, \dots, y_{t - 1}\}$ denote the history till time $t$ (exclusive). With complete knowledge about the forecaster's algorithm, an adaptive adversary chooses $y_{t}$ depending on $\cH_{t - 1}$. As mentioned in Section \ref{sec:achieve-KL-Cal}, we shall consider forecasters that make predictions which belong to the discretization 
    \begin{align*}
        \cZ = \{z_{1}, \dots, z_{K - 1}\}, \text{ where } z_{i} = \sin ^ {2} \bigc{\frac{\pi i}{2K}},
    \end{align*}
    and $K \in \mathbb{N}$ is a constant to be specified later. For convinience, we set $z_{0} = 0, z_{K} = 1$,
    however, $z_{0}, z_{K}$ are not included in the discretization. In Lemma \ref{lem:discretization}, we prove some important facts regarding $\cZ$ which shall be useful for the subsequent analysis. For a deterministic forecaster, $p_{t}$ is obtained via a mapping ${F}_{t - 1}: \cH_{t - 1} \to \cZ$. Similarly, for a deterministic adversary, $y_{t}$ is obtained via a mapping $A_{t - 1}: \cH_{t - 1} \to \{0, 1\}$. Therefore, a deterministic forecaster can be represented by the sequence of mappings $F = (F_{1}, \dots, F_{T})$, and a deterministic adversary can be represented by the sequence $A = (A_{1}, \dots, A_{T})$. Given $F, A$, we let $\sreg^{\ell}({F, A})$ denote the swap regret achieved by executing $F, A$.

    Let $\{F\}, \{A\}$ be all possible enumerations of $F, A$ respectively, and $\Delta(\{F\}), \Delta(\{A\})$ denote the set of all distributions over $\{F\}, \{A\}$. Then, $\mathfrak{F} \in \Delta(\{F\}), \mathfrak{A} \in \Delta(\{A\})$ are distributions over $\{F\}, \{A\}$ and represent a randomized forecaster, adversary respectively. Note that $\abs{\{F\}}, \abs{\{A\}} < \infty$, since the domain and range of each map $F_{t}, A_{t}$ is finite. Therefore, by Theorem \ref{thm:von_neumann}, we have \begin{align}\label{eq:minmax}
        \min_{\mathfrak{F} \in \Delta(\{F\})} \max_{\mathfrak{A} \in \Delta(\{A\})} \mathbb{E}_{F \sim \mathfrak{F}, A \sim \mathfrak{A}}[\sreg^{\ell}(F, A)] = \max_{\mathfrak{A} \in \Delta(\{A\})} \min_{\mathfrak{F} \in \Delta(\{F\})}  \mathbb{E}_{F \sim \mathfrak{F}, A \sim \mathfrak{A}}[\sreg^{\ell}(F, A)]. 
    \end{align}
    For a $v \in \Rn$, to upper bound the quantity on the right hand side of \eqref{eq:minmax} by $v$, it is sufficient to prove that for any randomized adversary there exists a forecaster $F$ that guarantees that $\mathbb{E}[\sreg^{\ell}(F, A)] \le v$. Moreover, swapping the adversary and forecaster allows the forecaster to witness the distribution of $y_{t}$ before deciding $p_{t}$.
    Towards this end, we consider a forecaster $F$ which at time $t$ does the following: (a) it computes $\tilde{p}_{t} = \mathbb{E}_{t}[y_{t}]$; (b) predicts $p_{t} = \argmin_{z \in \cZ} \abs{\tilde{p}_{t} - z}$.

    For each $i \in \{1, \dots, K - 1\}$ and $n \in [T]$, let $n_{i}(n) \coloneqq \sum_{t = 1} ^ {n} \ind{p_{t} = z_{i}}$. For convinience, we refer to $n_{i}(T)$ as $n_{i}$. Fix a $i \in [K - 1]$, and define the sequence $X_{1, i}, \dots, X_{T, i}$ as follows: \begin{align*}
        X_{j, i} \coloneqq \begin{cases}
            0 & \text{ if } j > n_{i}, \\
            y_{t_{j}} - \tilde{p}_{t_{j}} & \text{ if } j \le n_{i}.
        \end{cases}
    \end{align*}
    Here $t_{j}$ denotes the $j$-th time instant when the prediction made is $p_{t} = z_{i}$. Observe that the sequence $X_{1, i}, \dots, X_{T, i}$ is a martingale difference sequence with $\abs{X_{j, i}} \le 1$ for all $j \in [T]$. In the subsequent steps we obtain a high probability bound on prefix sums of this sequence. 
    
    Fix $n \in [T], \mu \in [0, 1], \delta \in [0, 1]$. Applying Lemma \ref{lem:Freedman}, we obtain that the following inequality holds with probability at least $1 - \delta$:  \begin{align*}
        \abs{\sum_{j = 1} ^ {n} X_{j, i}} \le \mu \cV_{i}(n) + \frac{1}{\mu} \log \frac{2}{\delta},
    \end{align*}
    where $\cV_{i}(n) = \sum_{j = 1} ^ {\min(n, n_{i})} \tilde{p}_{t_{j}} (1 - \tilde{p}_{t_{j}})$. To uniformly bound $\cV_{i}(n)$ in terms of $n$, we consider the 2 cases $n \le n_{i}$ and $n > n_{i}$.
    When $n \le n_{i}$,  $\cV_{i}(n)$ can be bounded in terms of $z_{i}$ as follows \begin{align*}
        \cV_{i}(n) &= n z_{i}(1 - z_{i}) + \sum_{j = 1} ^ {n} \bigc{\tilde{p}_{t_{j}} (1 - \tilde{p}_{t_{j}}) - z_{i}(1 - z_{i})} \\
        &= n z_{i}(1 - z_{i}) + \sum_{j = 1} ^ {n} (\tilde{p}_{t_{j}} - z_{i}) \cdot (1 - \tilde{p}_{t_{j}} - z_{i}) \\
        &\le nz_{i}(1 - z_{i}) + \sum_{j = 1} ^ {n} \abs{\tilde{p}_{t_{j}} - z_{i}} \\
        &\le n \bigc{z_{i}(1 - z_{i}) + \frac{\pi}{2K}},
    \end{align*}
    where the last inequality follows from Lemma \ref{lem:discretization}. When $n > n_{i}$, we note that $\cV_{i}(n) = \cV_{i}(n_{i}) \le n\bigc{z_{i}(1 - z_{i}) + \frac{\pi}{2K}}$, since $n > n_{i}$. Therefore, with probability at least $1 - \delta$, we have \begin{align*}
        \abs{\sum_{j = 1} ^ {n} X_{j, i}}  \le \mu n\bigc{z_{i}(1 - z_{i}) + \frac{\pi}{2K}} + \frac{1}{\mu} \log \frac{2}{\delta}.
    \end{align*}
    Minimizing the bound above with respect to $\mu \in [0, 1]$, we obtain \begin{align*}
        \abs{\sum_{j = 1} ^ {n} X_{j, i}} \le \begin{cases}
            2\sqrt{n \bigc{z_{i}(1 - z_{i}) + \frac{\pi}{2K}} \log \frac{2}{\delta}} & \text{if } n \ge \frac{\log \frac{2}{\delta}}{z_{i}(1 - z_{i}) + \frac{\pi}{2K}}, \\
            n\bigc{z_{i}(1 - z_{i}) + \frac{\pi}{2K}} + \log \frac{2}{\delta} & \text{otherwise}.
        \end{cases}
    \end{align*}
    Note that when $n < \frac{\log \frac{2}{\delta}}{z_{i}(1 - z_{i}) + \frac{\pi}{2K}}$, we can simply bound $n\bigc{z_{i}(1 - z_{i}) + \frac{\pi}{2K}} + \log \frac{2}{\delta} < 2 \log \frac{2}{\delta}$. The bounds obtained for both cases can be combined into the following single bound: \begin{align*}
        \abs{\sum_{j = 1} ^ {n} X_{j, i}} \le 2\sqrt{\log \frac{2}{\delta}} \cdot\max\bigc{\sqrt{n\bigc{z_{i}(1 - z_{i}) + \frac{\pi}{2K}}}, \sqrt{\log \frac{2}{\delta}}},
    \end{align*}
    which holds with probability at least $1 - \delta$. Taking a union bound, we obtain that $\abs{\sum_{j = 1} ^ {n} X_{j, i}} \le 2\sqrt{\log \frac{2}{\delta}} \cdot\max\bigc{\sqrt{n\bigc{z_{i}(1 - z_{i}) + \frac{\pi}{2K}}}, \sqrt{\log \frac{2}{\delta}}}$ holds simultaneously for all $i \in [K - 1], n \in [T]$ with probability at least $1 - (K - 1)T\delta \ge 1 - K T \delta$. In particular, setting $n = n_{i}$, we obtain that \begin{align}\label{eq:hp_bound_sum_X}
        \abs{\sum_{j = 1} ^ {n_{i}} X_{j, i}} \le 2\sqrt{\log \frac{2}{\delta}} \cdot\max\bigc{\sqrt{n_{i}\bigc{z_{i}(1 - z_{i}) + \frac{\pi}{2K}}}, \sqrt{\log \frac{2}{\delta}}}
    \end{align}
    holds for all $i \in [K - 1]$ with probability at least $1 - K \delta T$. Equipped with this bound, in the following steps we obtain a high probabilty bound on $\sreg^{\ell}(F, A)$. This shall be used to bound $\mathbb{E}[\sreg^{\ell}(F, A)]$ eventually.
    
    
    We begin by bounding the quantity $\abs{z_{i} - \rho_{i}}$, which shall be used to obtain the high probability bound on $\sreg^{\ell}(F, A)$. We proceed as \begin{align*}
        \abs{z_{i} - \rho_{i}} &= \frac{1}{n_{i}}\abs{\sum_{t = 1} ^ {T} \ind{p_{t} = z_{i}} (z_{i} - y_{t})} \\
        &\le \frac{1}{n_{i}} \bigc{\abs{\sum_{t = 1} ^ {T} \ind{p_{t} = z_{i}} (z_{i} - \tilde{p}_{t})} + \abs{\sum_{t = 1} ^ {T}\ind{p_{t} = z_{i}} (\tilde{p}_{t} - y_{t})}} \\
        &\le \max(d_{i}, d_{i + 1}) + \frac{1}{n_{i}}\abs{\sum_{j = 1} ^ {n_{i}} X_{j, i}},
    \end{align*}
    where for each $i \in [K]$, we define $d_{i} \coloneqq z_{i} - z_{i - 1}$. The first inequality above follows from the Triangle inequality; the second inequality is because, if $p_{t} = z_{i}$, we must have $\tilde{p}_{t} \in [z_{0}, \frac{z_{1} + z_{2}}{2}]$ if $i = 1$, $\tilde{p}_{t} \in \bigs{\frac{z_{i - 1} + z_{i}}{2}, \frac{z_{i} + z_{i + 1}}{2}}$ if $2 \le i \le K - 2$, and $\tilde{p}_{t} \in \bigs{\frac{z_{K - 2} + z_{K - 1}}{2}, 1}$ if $i = K - 1$, therefore, $\abs{\tilde{p}_{t} - p_{t}} \le \max(d_{i}, d_{i + 1})$. For each $i \in [K - 1]$, let $t_{i} \coloneqq \frac{\log \frac{2}{\delta}}{z_{i}(1 - z_{i}) + \frac{\pi}{2K}}$. Next, we write $\sreg^{\ell}(F, A)$ as \begin{align*}
       \sreg^{\ell}(F, A) &= 
       {\underbrace{\sum_{i \in \cI} n_{i} \KL(\rho_{i}, z_{i})}_{\text{Term I}} + \underbrace{\sum_{i \in \bar{\cI}} n_{i} \KL(\rho_{i}, z_{i})}_{\text{Term II}}}, 
    \end{align*}
    where $\cI \coloneqq \{i \in [K - 1]; n_{i} < t_{i}\}$, and bound Term I, II individually. We begin by bounding Term II in the following manner: \begin{align*}
        \text{Term II} &\le {\sum_{i \in \bar{\cI}} n_{i} \chi ^ {2} (\rho_{i}, z_{i})} \\
        &= {\sum_{i \in \bar{\cI}} n_{i}\bigc{\frac{(\rho_{i} - z_{i}) ^ {2}}{z_{i}} + \frac{(\rho_{i} - z_{i}) ^ {2}}{1 - z_{i}}}} \\
        &= {\sum_{i \in \bar{\cI}} \frac{n_{i}(\rho_{i} - z_{i}) ^ {2}}{z_{i}(1 - z_{i})}} \\
        &\le {\sum_{i \in \bar{\cI}} \frac{2n_{i}}{z_{i}(1 - z_{i})} \bigc{(\max(d_{i}, d_{i + 1})) ^ {2} + \bigc{\frac{1}{n_{i}}\abs{\sum_{j = 1} ^ {n_{i}} X_{j, i}}} ^ {2}}} \\
        &\le \sum_{i \in \bar{\cI}} 2n_{i} \cdot \frac{(\max(d_{i}, d_{i + 1})) ^ {2}}{z_{i}(1 - z_{i})} + 8\log \frac{2}{\delta} \cdot \bigc{\sum_{i \in \bar{\cI}} \bigc{\frac{\pi}{2K} \cdot \frac{1}{z_{i}(1 - z_{i})} + 1}} \\
        &= \cO\bigc{\frac{T}{K ^ {2}}} + \cO\bigc{K \log \frac{1}{\delta}},
    \end{align*}
    where the first inequality follows since $\KL(\rho_{i}, z_{i}) \le \chi^{2}(\rho_{i}, z_{i})$; the second inequality follows from the bound on $\abs{z_{i} - \rho_{i}}$ established above, and since $(a + b) ^ {2} \le 2 a ^ {2} + 2 b ^ {2}$; the third inequality follows from \eqref{eq:hp_bound_sum_X}; the final equality follows from Lemma \ref{lem:discretization}, particularly, we use the bounds $\frac{(\max(d_{i}, d_{i + 1})) ^ {2}}{z_{i} (1 - z_{i})} = \cO(\frac{1}{K^{2}})$ and $\sum_{i = 1} ^ {K - 1} \frac{1}{z_{i}(1 - z_{i})} = \cO(K ^ {2})$. To bound Term I, we first note from the proof of Proposition \ref{prop:breg_div_decomposable} that \begin{align*}
        n_{i} \KL(\rho_{i}, z_{i}) = \sup_{\sigma: [0, 1] \to [0, 1]} \sum_{t = 1} ^ {T} \ind{p_{t} = z_{i}} (\ell(p_{t}, y_{t}) - \ell(\sigma(p_{t}), y_{t})) \le n_{i} \log \frac{1}{\sin ^ {2} \frac{\pi}{2K}},
    \end{align*}
    where the last inequality is because for the rounds where $p_{t} = z_{i}$, we have \begin{align}\label{eq:range_log_loss}
    \ell(p_{t}, y_{t}) \le \max\bigc{\log \frac{1}{z_{i}}, \log \frac{1}{1 - z_{i}}} \le \max \bigc{\log \frac{1}{\sin ^ {2} \frac{\pi}{2K}}, \log \frac{1}{1 - \cos ^ {2} \frac{\pi}{2K}}} = \cO(\log K). 
    \end{align}
    Moreover, repeating the exact same steps done to bound Term II above, we can also bound $n_{i} \KL(\rho_{i}, z_{i})$ as \begin{align*}
        n_{i} \KL(\rho_{i}, z_{i}) &\le {\frac{2n_{i}}{z_{i}(1 - z_{i})} \bigc{(\max(d_{i}, d_{i + 1})) ^ {2} + \bigc{\frac{1}{n_{i}}\abs{\sum_{j = 1} ^ {n_{i}} X_{j, i}}} ^ {2}}} \\
        &=\cO\bigc{\frac{n_{i}}{K ^ {2}}}  + 8 \bigc{\log \frac{2}{\delta}}^{2} \cdot \frac{1}{n_{i} z_{i}(1 - z_{i})} \\
        &= \cO\bigc{\frac{n_{i}}{K ^ {2}}  + \bigc{\log \frac{1}{\delta}} ^ {2} \cdot \frac{1}{n_{i} z_{i}(1 - z_{i})}},
    \end{align*}
    where the first equality follows from Lemma \ref{lem:discretization} and \eqref{eq:hp_bound_sum_X}. 
    Taking minimum of the two bounds obtained above, we obtain \begin{align*}
        n_{i} \KL(\rho_{i}, z_{i}) &= \cO\bigc{\min\bigc{n_{i} \log K, \frac{n_{i}}{K ^ {2}}  + \bigc{\log \frac{1}{\delta}} ^ {2} \cdot \frac{1}{n_{i} z_{i}(1 - z_{i})}}} \\
        &= \cO\bigc{\frac{n_{i}}{K ^ {2}} + \min\bigc{n_{i} \log K, \bigc{\log \frac{1}{\delta}} ^ {2} \cdot \frac{1}{n_{i} z_{i}(1 - z_{i})}}} \\
        &= \cO\bigc{\frac{n_{i}}{K ^ {2}} + \sqrt{\log K}\log \frac{1}{\delta} \cdot \frac{1}{\sqrt{z_{i} (1 - z_{i})}}},
    \end{align*}
    where the final inequality follows since for a fixed $a > 0$, $\min(x, \frac{a}{x}) \le \sqrt{a}$ holds for all $x \in \Rn$. Summing over $i \in \cI$, we obtain the following bound on Term I: \begin{align*}
        \text{Term I} = \cO\bigc{\frac{1}{K ^ {2}} \sum_{i \in \cI} n_{i} + \sqrt{\log K}\log \frac{1}{\delta} \cdot \sum_{i \in \cI} \frac{1}{\sqrt{z_{i} (1 - z_{i})}}} = \cO\bigc{\frac{T}{K ^ {2}} + K (\log K)^{\frac{3}{2}}  \log \frac{1}{\delta}},
    \end{align*}
    where the last equality follows from Lemma \ref{lem:discretization}, particularly, $\sum_{i = 1} ^ {K - 1} \frac{1}{\sqrt{z_{i}(1 - z_{i})}} = \cO(K \log K)$. Summarizing, we have shown that \begin{align*}
        \text{Term I} = \cO\bigc{\frac{T}{K ^ {2}} + K (\log K)^{\frac{3}{2}}  \log \frac{1}{\delta}}, \quad \text{Term II} = \cO\bigc{\frac{T}{K ^ {2}} + K \log \frac{1}{\delta}}
    \end{align*}
    hold simultaneously with probability at least $1 - KT \delta$. Therefore, \begin{align}\label{eq:bound_sreg_minimax}
        \sreg^{\ell}(F, A) = \cO\bigc{\frac{T}{K ^ {2}} + K (\log K) ^ {\frac{3}{2}} \log \frac{1}{\delta}}
    \end{align}
    with probability at least $1 - KT \delta$. To bound $\mathbb{E}[\sreg^{\ell}(F, A)]$, we let $\cE$ be the event in \eqref{eq:bound_sreg_minimax}.
    Therefore, \begin{align*}
        \mathbb{E}[\sreg^{\ell}(F, A)] &=  \mathbb{E}[\sreg^{\ell}(F, A) | \cE] \cdot \mathbb{P}(\cE) + \mathbb{E}[\sreg^{\ell}(F, A) | \bar{\cE}] \cdot \mathbb{P}(\bar{\cE}) \\
        &= \cO\bigc{\frac{T}{K ^ {2}} + K (\log K)^{\frac{3}{2}} \log \frac{1}{\delta} + (K\log K) T^{2} \delta} \\
        &= \cO\bigc{\frac{T}{K ^ {2}} + K(\log K) ^ {\frac{3}{2}} \log T + K \log K} \\
        &= \cO(T^{\frac{1}{3}} (\log T) ^ {\frac{5}{3}}),
    \end{align*}
    where the second equality follows by using the high probability bound on $\sreg^{\ell}(F, A)$ obtained in \eqref{eq:bound_sreg_minimax}, and bounding $\mathbb{E}[\sreg^{\ell}(F, A) | \bar{\cE}] = \cO(T \log K)$, which follows from \eqref{eq:range_log_loss}; the third equality follows by choosing $\delta = \frac{1}{T^{2}}$;
    the final equality follows by choosing $K = \frac{T^{\frac{1}{3}}}{(\log T)^{\frac{5}{6}}}$. This completes the proof.
\end{proof}


\begin{lemma}\label{lem:discretization}
Fix a $k \in \mathbb{N}$. Let $\{z_{i}\}_{i = 0} ^ {K}$ be a sequence where $z_{0} = 1, z_{i} = \sin ^ {2} \bigc{\frac{\pi i}{2K}}$ for $i = 1, \dots, K - 1$, and $z_{K} = 1$. For each $i = 1, \dots, K$, define $d_{i} \coloneqq z_{i} - z_{i - 1}$. Then, the following holds: (a) $d_{i} \le {\frac{\pi}{2K}}$ for all $i \in [K]$; (b) $\frac{\max ^ {2}(d_{i}, d_{i + 1})}{z_{i}(1 - z_{i})} = \cO\bigc{\frac{1}{K ^ {2}}}$; (c) $\sum_{i = 1} ^ {K - 1} \frac{1}{z_{i}(1 - z_{i})} = \cO(K ^ {2})$; and (d) $\sum_{i = 1} ^ {K - 1} \frac{1}{\sqrt{z_{i}(1 - z_{i})}} = \cO(K \log K)$.
\end{lemma}
\begin{proof}
    By direct computation, we have \begin{align}\label{eq:bound_diff}
        z_{i} - z_{i - 1} = \sin ^ {2} {\frac{\pi i}{2K}} - \sin ^ {2} {\frac{\pi(i - 1)}{2K}} = \frac{\cos {\frac{\pi(i - 1)}{K}} - \cos {\frac{\pi i}{K}}}{2} = \sin {\frac{\pi}{2K}} \sin \bigc{\frac{\pi}{K} \bigc{i - \frac{1}{2}}},
    \end{align}
    where the second equality follows from the identity $\sin ^ {2} \theta = \frac{1 - \cos 2\theta}{2}$, while the last equality follows from the identity $\cos \alpha - \cos \beta = 2 \sin \frac{\alpha + \beta}{2} \sin \frac{\beta - \alpha}{2}$. Since $\sin \theta \le \theta$ for all $\theta \in \Rn$, and bounding $\sin \theta \le 1$, we obtain $z_{i} - z_{i - 1} \le \frac{\pi}{2K}$, which completes the proof for the first part of the lemma. 
    
    For the second part, we note that \begin{align*}
        \frac{\max ^ {2}(d_{i}, d_{i + 1})}{z_{i}(1 - z_{i})} = \frac{\max ^ {2}(d_{i}, d_{i + 1})}{\sin ^ {2} \frac{\pi i}{2K} \cos ^ {2} \frac{\pi i}{2K}} = 4 \cdot \frac{\max ^ {2}(d_{i}, d_{i + 1})}{\sin ^ {2} \frac{\pi i}{K}}, 
    \end{align*}
    where the second equality follows from the identity $\sin 2\theta = 2\sin \theta \cos \theta$. It follows from \eqref{eq:bound_diff} that \begin{align*}
        \max(d_{i}, d_{i + 1}) = \sin \frac{\pi}{2K} \cdot \max\bigc{\sin \bigc{\frac{\pi}{K} \bigc{i - \frac{1}{2}}}, \sin \bigc{\frac{\pi}{K} \bigc{i + \frac{1}{2}}}}.
    \end{align*}
    For simplicity, we assume that $K$ is odd, although a similar treatment can be done for even $K$. Let $1 \le i \le \frac{K - 1}{2}$. Then,  $\max(d_{i}, d_{i + 1}) = \sin \frac{\pi}{2K} \sin \bigc{\frac{\pi}{K} \bigc{i + \frac{1}{2}}}$. Observe that \begin{align*}
        \frac{\sin \bigc{\frac{\pi}{K} \bigc{i + \frac{1}{2}}}}{\sin \frac{\pi i}{K}} = \frac{\sin \frac{\pi i}{K} \cos \frac{\pi}{2K} + \cos \frac{\pi i}{K} \sin \frac{\pi}{2K}}{\sin \frac{\pi i}{K}} = \cos \frac{\pi}{2K} + \cot \frac{\pi i}{K} \sin \frac{\pi}{2K} \le 1 + \frac{\sin \frac{\pi}{2K}}{\sin \frac{\pi}{K}}, 
    \end{align*}
    where the first equality follows from the identity $\sin (\alpha + \beta) = \sin \alpha \cos \beta + \cos \alpha \sin \beta$, while the inequality follows by noting that $\cot \frac{\pi i}{K} \le \cot \frac{\pi}{K}$ for all $1 \le i \le \frac{K - 1}{2}$. Finally, since $\frac{\sin \frac{\pi}{2K}}{\sin \frac{\pi}{K}} = \frac{1}{2 \cos \frac{\pi}{2K}} = \cO(1)$, we obtain $\frac{\max ^ {2} (d_{i}, d_{i + 1})}{z_{i}(1 - z_{i})} = \cO (\sin ^ {2} \frac{\pi}{2K}) = \cO(\frac{1}{K ^ {2}})$. Next, we consider the case when $\frac{K + 1}{2} \le i  \le K - 1$. Then, $\max(d_{i}, d_{i + 1}) = \sin \frac{\pi}{2K} \sin \bigc{\frac{\pi}{K} \bigc{i - \frac{1}{2}}}$. Repeating a similar analysis as before, we obtain \begin{align*}
         \frac{\sin \bigc{\frac{\pi}{K} \bigc{i - \frac{1}{2}}}}{\sin \frac{\pi i}{K}} = \frac{\sin \frac{\pi i}{K} \cos \frac{\pi}{2K} - \cos \frac{\pi i}{K} \sin \frac{\pi}{2K}}{\sin \frac{\pi i}{K}} = \cos \frac{\pi}{2K} - \cot \frac{\pi i}{K} \sin \frac{\pi}{2K} \le 1 + \frac{\sin \frac{\pi}{2K}}{\sin \frac{\pi}{K}},
    \end{align*}
    which is $\cO(1)$ as claimed earlier. Therefore, $\frac{\max ^ {2}(d_{i}, d_{i + 1})}{z_{i}(1 - z_{i})} = \cO(\frac{1}{K ^ {2}})$. Combining both the cases completes the proof of (b) above. 
    
    For (c), similar to (b), we assume for simplicity that $K$ is odd. Then, \begin{align*}
        \sum_{i = 1} ^ {K - 1} \frac{1}{z_{i} (1 - z_{i})} = 4 \sum_{i = 1} ^ {K - 1} \frac{1}{\sin ^ {2} \frac{\pi i}{K}} = 8\sum_{i = 1} ^ {\frac{K - 1}{2}} \frac{1}{\sin ^ {2} \frac{\pi i}{K}}, \end{align*}
        and the summation $\sum_{i = 1} ^ {\frac{K - 1}{2}} \frac{1}{\sin ^ {2} \frac{\pi i}{K}}$ can be bounded in the following manner:
        \begin{align*}
        \sum_{i = 1} ^ {\frac{K - 1}{2}} \frac{1}{\sin ^ {2} \frac{\pi i}{K}} &\le \bigc{\frac{1}{\sin ^ {2} \frac{\pi}{K}} + \int_{1} ^ {\frac{K - 1}{2}} \frac{1}{\sin ^ {2} \frac{\pi \nu}{K}} d\nu} \\
        &\le  \bigc{\frac{1}{\sin ^ {2} \frac{\pi}{K}} + \int_{1} ^ {\frac{K}{2}} \frac{1}{\sin ^ {2} \frac{\pi \nu}{K}} d\nu} \\
        &= \bigc{\frac{1}{\sin ^ {2} \frac{\pi}{K}} + \frac{K}{\pi}\int_{\frac{\pi}{K}} ^ {\frac{\pi}{2}} \frac{1}{\sin ^ 2 \nu} d\nu} \\
        &= \bigc{\frac{1}{\sin ^ {2} \frac{\pi}{K}} + \frac{K}{\pi} \cot \frac{\pi}{K}} = \cO(K ^ {2}).
    \end{align*}
    This completes the proof for (c). Repeating the exact same steps as (c) proves (d). We include the full proof for completeness. Observe that \begin{align*}
        \sum_{i = 1} ^ {K - 1} \frac{1}{\sqrt{z_{i} (1 - z_{i})}} = 2 \sum_{i = 1} ^ {K - 1} \frac{1}{\sin \frac{\pi i}{K}} = 4 \sum_{i = 1} ^ {\frac{K - 1}{2}} \frac{1}{\sin \frac{\pi i}{K}}, 
    \end{align*}
    and the summation $\sum_{i = 1} ^ {\frac{K - 1}{2}} \frac{1}{\sin \frac{\pi i}{K}}$ can be bounded in the following manner: \begin{align*}
        \sum_{i = 1} ^ {\frac{K - 1}{2}} \frac{1}{ \sin \frac{\pi i}{K}} \le \frac{1}{\sin \frac{\pi}{K}} + \int_{1} ^ {\frac{K - 1}{2}} \frac{1}{\sin \frac{\pi \nu}{K}} d\nu \le \frac{1}{\sin \frac{\pi}{K}} + \int_{1} ^ {\frac{K}{2}} \frac{1}{\sin \frac{\pi \nu}{K}} d\nu = {\frac{1}{\sin \frac{\pi}{K}} + \frac{K}{\pi}\int_{\frac{\pi}{K}} ^ {\frac{\pi}{2}} \frac{1}{\sin \nu} d\nu}.
    \end{align*}
    The integral above evaluates to $\log \bigc{\csc \frac{\pi}{K} + \cot \frac{\pi}{K}}$. Therefore, we have that \begin{align*}
        \sum_{i = 1} ^ {K - 1} \frac{1}{\sqrt{z_{i} (1 - z_{i})}} \le 4\bigc{\csc \frac{\pi}{K} + \frac{K}{\pi} \log \bigc{\csc \frac{\pi}{K} + \cot \frac{\pi}{K}}} = \cO(K \log K).
    \end{align*}
    This completes the proof.
\end{proof}

\subsection{Proof of Corollary \ref{cor:simutaneous_bounds_msr}}
\begin{proof}
    Let $\cA$ be the algorithm guaranteed by Theorem \ref{thm:log_loss_swap_reg}.  By Pinsker's inequality, we get that $\cA$ guarantees $\mathbb{E}[\cal_{2}] = \cO(T^{\frac{1}{3}} (\log T) ^ {\frac{5}{3}})$. Moreover, since $\cal_{1} \le \sqrt{T \cdot \cal_{2}}$ \citep[Lemma 13]{kleinberg2023u}, by Jensen's inequality we have $\mathbb{E}[\cal_{1}] \le \sqrt{T \cdot \mathbb{E}[\cal_{2}]} = \cO(T^{\frac{1}{3}} (\log T) ^ {\frac{5}{6}})$. Next, \cite[Theorem 12]{kleinberg2023u} states that for any proper loss $\ell$, we have $\sreg^{\ell} \le 4 \cal_{1}$. Therefore, $\mathbb{E}[\sreg^{\ell}] \le 4 \mathbb{E}[\cal_{1}] = \cO(T^{\frac{2}{3}} (\log T) ^ {\frac{5}{6}})$. Combining this with the result of Proposition \ref{prop:breg_div_decomposable}, \ref{prop:bound_breg_div_quadratically} completes the proof.
\end{proof}
