\section{Deferred proofs and discussion in Section \ref{sec:pseudo-KL-Cal}}\label{app:deferred_proofs_pseudo_KL_Cal}
\subsection{Expected loss of common rounding schemes}
We recall the discussion in Section \ref{sec:pseudo-KL-Cal}:
at each time $t$, $\text{EWOO}_{i}$ outputs $w_{t, i} \in [0, 1]$, however, $\cA_{i}$ is required to predict a distribution $\q_{t, i} \in \Delta_{K + 1}$ over $\cZ$. Thus, we need to perform a rounding operation that projects the output $w_{t, i}$ of $\text{EWOO}_{i}$ to a distribution over $\cZ$. 
In the remark below,
we show that the following two known rounding schemes: (a) rounding $w_{t, i}$ to the nearest $z \in \cZ$ and setting $\q_{t, i}$ as the corresponding one-hot vector; (b) the rounding procedure proposed by \cite{fishelsonfull}, cannot be applied to our setting since they incur a $\Omega(1)$ change in the expected loss $\ip{\q_{t, i}}{\bi{\ell}_{t}} - \ell(w_{t, i}, y_{t})$, which is not sufficient to achieve the desired regret guarantee.

\begin{remark}\label{rem:rounding}
    Let $y_{t} = 1$ and $w_{t, i} = \frac{z_{0} + z_{1}}{2}$. The rounding procedure in (a) above ensures that $\q_{t, i} = \e_{0}$ with probability one. Therefore, $\ip{\q_{t, i}}{\bi{\ell}_{t}} - \ell(w_{t, i}, y_{t}) = \ell(z_{0}, 1) - \ell\bigc{\frac{z_{0} + z_{1}}{2}, 1} = \log \frac{z_{0} + z_{1}}{2z_{0}}$. Observe that $\frac{z_{1}}{z_{0}} = \frac{\sin ^ {2} \frac{\pi}{2K}}{\sin ^ {2} \frac{\pi}{4K}} = 4 \cos ^ {2} \frac{\pi}{4K} = 2 + 2 \cos \frac{\pi}{2K}.$ Therefore, $\ip{\q_{t, i}}{\bi{\ell}_{t}} - \ell(w_{t, i}, y_{t}) = \log \bigc{\frac{3}{2} + \cos \frac{\pi}{2K}} = \Omega(1)$. For the chosen example, the rounding procedure in (b) sets $q_{t, i}(0) = q_{t, i}(1) = \frac{1}{2}$. Thus, $\ip{\q_{t, i}}{\bi{\ell}_{t}} - \ell(w_{t, i}, y_{t}) = \frac{\ell(z_{0}, 1) + \ell(z_{1}, 1)}{2} - \ell\bigc{\frac{z_{0} + z_{1}}{2}, 1} = \log \frac{z_{0} + z_{1}}{2\sqrt{z_{0} z_{1}}} = \log \frac{{1 + 4\cos ^ {2} \frac{\pi}{4K}}}{4\cos \frac{\pi}{4K}} = \Omega(1)$. 
\end{remark}

\subsection{Proof of Lemma \ref{lem:rounding}}
\begin{proof}
    Since the log loss $\ell(p, y)$ is convex in $p$ (for any $y \in \{0, 1\}$), we have \begin{align}\label{eq:convexity}
    \ell(q, y) - \ell(p, y) \le \ell'(q, y) \cdot (q - p) = \frac{(q - y)(q - p)}{q(1 - q)} = \begin{cases}
        \frac{p}{q} - 1 & \text{if } y = 1, \\
        \frac{1 - p}{1 - q} - 1 & \text{if } y = 0.
    \end{cases}
    \end{align}
    Let $y = 1$. Taking expectation on both sides of \eqref{eq:convexity}, we obtain $\mathbb{E}[\ell(q, y)] - \ell(p, y) = \mathbb{E}\bigs{\frac{p}{q}} - 1$. To simplify the expressions involved in the computation of $\mathbb{E}\bigs{\frac{1}{q}}$, we define the normalizing factor $D \coloneqq \frac{p^{+} - p}{p^{+}(1 - p^{+})} + \frac{p - p^{-}}{p^{-}(1 - p^{-})}$. By direct computation, we have \begin{align*}
        \mathbb{E}\bigs{\frac{1}{q}} = \frac{1}{D} \bigc{\frac{p^{+} - p}{p^{-}p^{+}(1 - p^{+})} + \frac{p - p^{-}}{p^{-}p^{+}(1 - p^{-})}} = \frac{1}{D} \cdot \frac{(p^{+} - p^{-})(1 - p)}{p^{-}p^{+}(1 - p^{-})(1 - p^{+})}.
    \end{align*}
    Similarly, by direct computation, we obtain \begin{align*}
        D = \frac{p^{+} - p}{p^{+}(1 - p^{+})} + \frac{p - p^{-}}{p^{-}(1 - p^{-})} = \frac{(p^{+} - p^{-})\bigc{p + p^{-}p^{+} - p(p^{-} + p^{+})}}{p^{-}p^{+}(1 - p^{-})(1 - p^{+})}.
    \end{align*}
    Therefore, \begin{align*}
        \mathbb{E}\bigs{\frac{p}{q}} - 1 = \frac{p(1 - p)}{p + p^{-}p^{+} - p(p^{-} + p^{+})} - 1 = \frac{(p^{+} - p)(p - p^{-})}{p + p^{-}p^{+} - p(p^{-} + p^{+})} \le \frac{(p^{+} - p^{-}) ^ {2}}{p + p^{-}p^{+} - p(p^{-} + p^{+})}. 
    \end{align*}
    Next, we let $y = 0$. Taking expectation on both sides of \eqref{eq:convexity}, we obtain $\mathbb{E}[\ell(q, y)] - \ell(p, y) = \mathbb{E}\bigs{\frac{1 - p}{1 - q}} - 1$, thus, we require to bound $\mathbb{E}\bigs{\frac{1}{1 - q}}$. Direct computation yields \begin{align*}
        \mathbb{E}\bigs{\frac{1}{1 - q}} = \frac{1}{D}\bigc{\frac{p^{+} - p}{p^{+}(1 - p^{-})(1 - p^{+})} + \frac{p - p^{-}}{p^{-}(1 - p^{-})(1 - p^{+})}} = \frac{1}{D} \cdot \frac{p(p^{+} - p^{-})}{p^{-}p^{+}(1 - p^{-})(1 - p^{+})}.
    \end{align*}
    Substituting the expression for $D$ obtained above, we obtain \begin{align*}
        \mathbb{E}\bigs{\frac{1 - p}{1 - q}} - 1 = \frac{p(1 - p)}{p + p^{-}p^{+} - p(p^{-} + p^{+})} - 1 &= \frac{(p^{+} - p)(p - p^{-})}{p + p^{-}p^{+} - p(p^{-} + p^{+})} \\ &\le \frac{(p^{+} - p^{-}) ^ {2}}{p + p^{-}p^{+} - p(p^{-} + p^{+})}.
    \end{align*}
    Let $f(p) = p + p^{-}p^{+} - p(p^{-} + p^{+})$. Since $f(p)$ is linear in $p$, for any $p \in [p^{-}, p^{+})$, we have $\min(f(p^{-}), f(p^{+})) \le f(p) \le \max(f(p^{-}), f(p^{+}))$. Since $f(p^{-}) = p^{-}(1 - p^{-}), f(p^{+}) = p^{+}(1 - p^{+})$, we obtain \begin{align*}
        \min\bigc{p^{-}(1 - p^{-}), p^{+}(1 - p^{+})} \le p + p^{-}p^{+} - p(p^{-} + p^{+}) \le \max\bigc{p^{-}(1 - p^{-}), p^{+}(1 - p^{+})}
    \end{align*}
    for all $p \in [p^{-}, p^{+})$. Therefore, \begin{align*}
        \mathbb{E}_{q}[\ell(q, y)] - \ell(p, y) \le (p^{+} - p^{-}) ^ {2} \cdot \max\bigc{\frac{1}{p^{-}(1 - p^{-})}, \frac{1}{p^{+}(1 - p^{+})}} = \cO\bigc{\frac{1}{K ^ {2}}},
    \end{align*}
    where the last equality follows from Lemma \ref{lem:discretization_II}. This completes the proof.
\end{proof}
\begin{lemma}\label{lem:discretization_II}
Fix a $k \in \mathbb{N}$. Let $\{z_{i}\}_{i = 0} ^ {K}$ be a sequence where $z_{0} = \sin ^ {2} \frac{\pi}{4K}, z_{i} = \sin ^ {2} \bigc{\frac{\pi i}{2K}}$ for $i \in [K - 1]$, and $z_{K} = \cos ^ 2 \frac{\pi}{4K}$. For each $i = 1, \dots, K$, define $d_{i} \coloneqq z_{i} - z_{i - 1}$. Then, the following holds true for all $i \in [K]$: (a) $\frac{d_{i} ^ {2}}{z_{i}(1 - z_{i})} = \cO\bigc{\frac{1}{K ^ {2}}}$, and (b) $\frac{d_{i} ^ {2}}{z_{i - 1}(1 - z_{i - 1})} = \cO\bigc{\frac{1}{K ^ {2}}}$.
\end{lemma}
\begin{proof}
     It follows from Lemma \ref{lem:discretization} that (a), (b) hold for all $2 \le i \le K - 1$. For $i = 1$, since $d_{1} \le z_{1} = \sin ^ {2} \frac{\pi}{2K}$, we have \begin{align*}
         \frac{d_{1} ^ {2}}{z_{1}(1 - z_{1})} \le \frac{\sin ^ {4} \frac{\pi}{2K}}{\sin ^ {2} \frac{\pi}{2K} \cos ^ {2} \frac{\pi}{2K}} = \tan ^ {2} \frac{\pi}{2K},
     \end{align*}
     which is $\cO(\frac{1}{K ^ {2}})$ for a large $K$. Similarly, for $i = K$, $d_{i} = \cos ^ {2} \frac{\pi}{4K} - \cos ^ {2} \frac{\pi}{2K} = \sin ^ {2} \frac{\pi}{2K} - \sin ^ {2} \frac{\pi}{4K} \le \sin ^ {2} \frac{\pi}{2K}$. Therefore, \begin{align*}
         \frac{d_{K} ^ {2}}{z_{K}(1 - z_{K})} \le \frac{\sin ^ {4} \frac{\pi}{2K}}{\sin ^ {2} \frac{\pi}{4K} \cos ^ {2} \frac{\pi}{4K}} = 4 \sin ^ {2} \frac{\pi}{2K} \le \frac{\pi ^ {2}}{K ^ {2}},
     \end{align*}
     where the equality follows from the identity $\sin 2\theta = 2 \sin \theta \cos \theta$. This completes the proof for (a). For (b), when $i = 1$, we have \begin{align*}
         \frac{d_{1} ^ {2}}{z_{0}(1 - z_{0})} \le \frac{\sin ^ {4} \frac{\pi}{2K}}{\sin ^ {2} \frac{\pi}{4K} \cos ^ {2} \frac{\pi}{4K}} = 4 \sin ^ {2} \frac{\pi}{2K} \le \frac{\pi ^ {2}}{K ^ {2}}.
     \end{align*} 
     Similarly, when $i = K$, we have \begin{align*}
         \frac{d_{K} ^ {2}}{z_{K - 1}(1 - z_{K - 1})} \le \frac{\sin ^ {4} \frac{\pi}{2K}}{\sin ^ {2} \frac{\pi}{2K} \cos ^ {2} \frac{\pi}{2K}} = \tan ^ {2} \frac{\pi}{2K},
     \end{align*}
     which is $\cO(\frac{1}{K ^ {2}})$ for a large $K$. This completes the proof.
\end{proof}

\subsection{Proof of Corollary \ref{cor:simultaneous_bounds_psreg}}
\begin{proof}
     Since $\kcal \ge \pcal_{2}$, Algorithm \ref{alg:BM_log_loss} ensures that $\pcal_{2} = \cO(T^{\frac{1}{3}} (\log T) ^ {\frac{2}{3}})$. Next, we show that the  $\pcal_{1}$ satisfies (a) $\pcal_{1} \le \sqrt{T \cdot \pcal_{2}}$; (b) for any proper loss $\ell$, we have $\psreg^{\ell} \le 4\pcal_{1}$. The proof is exactly similar to the corresponding variants of (a), (b) above for $\cal$ as shown by  \cite{kleinberg2023u}. For (a), applying the Cauchy-Schwartz inequality, we obtain \begin{align*}
        \sum_{p \in \cZ} \sum_{t = 1} \cP_{t}(p)\abs{p - \tilde{\rho}_{p}} \le \bigc{\sum_{p \in \cZ} \sum_{t = 1} ^ {T} \cP_{t}(p)} ^ {\frac{1}{2}} \bigc{\sum_{p \in \cZ} \sum_{t = 1} ^ {T} \cP_{t}(p) (p - \tilde{\rho}_{p}) ^ {2}} ^ {\frac{1}{2}} = \sqrt{T \cdot \pcal_{2}}.
    \end{align*}
    Towards showing (b), we first rewrite $\psreg^{\ell} = \sum_{p \in \cZ} \sum_{t = 1} ^ {T} \cP_{t}(p) \breg_{-\ell}(\tilde{\rho}_{p}, p)$, which holds for any proper loss $\ell$ as per Proposition \ref{prop:breg_div_decomposable}. Next, we observe that \begin{align*}
        \breg_{-\ell}(\tilde{\rho}_{p}, p) =  \ell(p) -\ell(\tilde{\rho}_{p}) + \partial \ell (p) (\tilde{\rho}_{p} - p) &\le \partial \ell(\tilde{\rho}_{p}) (p - \tilde{\rho}_{p}) + \partial \ell(p) (\tilde{\rho}_{p} - p) \\
        &\le 4 \abs{p - \tilde{\rho}_{p}},
    \end{align*}
    where the first inequality follows since $\ell(p)$ is concave; the second inequality follows by noting that $\ell(p, 1) - \ell(p, 0) = \partial \ell (p)$ as per Lemma \ref{lem:characterization_proper_loss}, and since $\ell(p, y) \in [-1, 1]$, we have $\abs{\partial \ell (p)} \le 2$ for all $p \in [0, 1]$. Substituting the bound on $\breg_{-\ell}(\tilde{\rho}_p, p)$ obtained above into $\psreg^{\ell}$, we obtain $\psreg^{\ell} \le 4\pcal_{1}$ as desired. Since Algorithm \ref{alg:BM_log_loss} ensures $\pcal_{1} = \cO(T^{\frac{1}{3}} (\log T) ^ {\frac{1}{3}})$, we obtain $\psreg^{\ell} = \cO(T^{\frac{1}{3}} (\log T) ^ {\frac{1}{3}})$. Combining the above results with Propositions \ref{prop:breg_div_decomposable}, \ref{prop:bound_breg_div_quadratically} finishes the proof.
\end{proof}
