\section{High probability bound for maximum swap regret against $\cL_{G}$}\label{sec:bound_calibration}
While we do not have a concrete algorithm for $\kcal$,
in this section, we show that if we only consider $\cL_{G}$, 
then our Algorithm \ref{alg:BM_log_loss} or the algorithm of \cite{fishelsonfull} already achieves a ${\cO}(G \cdot T^{\frac{1}{3}} (\log T) ^ {-\frac{1}{3}} \log \tfrac{T}{\delta})$ high probability bound for $\msr_{\cL_{G}}$. To obtain so, we first prove a generic high probability bound that relates $\cal_{2}$ with $\pcal_{2}$. Subsequently, we instantiate our bound with an explicit algorithm for minimizing $\pcal_{2}$ and use the result of Proposition \ref{prop:bound_breg_div_quadratically}. 
Our high probability bound in Theorem \ref{thm:high_prob_bound} is independent of the choice of the discretization $\cZ$. 


\begin{theorem}\label{thm:high_prob_bound}
    For any algorithm $\cA_{\cal}$, with probability at least $1 - \delta$ over the randomness in $\cA_{\cal}$'s predictions $p_{1}, \dots, p_{T}$, we have $$
        \cal_{2} \le 6 \pcal_{2} + 96 \abs{\cZ}\log \frac{4\abs{\cZ}}{\delta}.
    $$
\end{theorem}

We defer the proof of Theorem \ref{thm:high_prob_bound} to Appendix \ref{app:hp_bound}. Instantiating $\cA_{\cal}$ in Theorem \ref{thm:high_prob_bound}, we obtain the following corollary whose proof can also be found in Appendix \ref{app:hp_bound}.


\begin{corollary}\label{cor:cal_2_hp_bound}
On choosing $K = (T/\log T) ^ {\frac{1}{3}}$, Algorithm \ref{alg:BM_log_loss} ensures that with probability at least $1 - \delta$ over its internal randomness 
\begin{align*}
    \cal_{2} = \cO\bigc{{T ^ {\frac{1}{3}}}(\log T) ^ {-\frac{1}{3}} \log \frac{T}{\delta}}, \quad \msr_{\cL_{G}} = \cO\bigc{G \cdot {T ^ {\frac{1}{3}}}(\log T) ^ {-\frac{1}{3}} \log \frac{T}{\delta}}.
\end{align*}
Furthermore, $\mathbb{E}[\cal_{2}] = \cO(T^{\frac{1}{3}} (\log T) ^ {\frac{2}{3}}), \mathbb{E}[\msr_{\cL_{G}}] = \cO(G \cdot T^{\frac{1}{3}}(\log T) ^ {\frac{2}{3}})$.
\end{corollary}

Instantiating $\cA_{\cal}$ with the algorithm of \cite{fishelsonfull}, we also obtain the exact same guarantee as Corollary \ref{cor:cal_2_hp_bound}. Compared to Algorithm \ref{alg:BM_log_loss}, the algorithm of \cite{fishelsonfull} is more efficient since it uses scaled online gradient descent for the $i$-th external regret algorithm, which is more efficient than $\text{EWOO}_{i}$. On the contrary, it does not posses the generality of Algorithm \ref{alg:BM_log_loss} towards minimizing the maximum swap regret for $\cL_{2}$. 
\section{Conclusion and Future Directions}
In this paper, we introduced a new stronger notion of calibration called (pseudo) KL-Calibration which not only allows us to recover results for classical (pseudo) $\ell_{2}$-Calibration, but also obtain simultaneous (pseudo) swap regret guarantees for several important subclasses of proper losses. We also derived the first high probability and in-expectation bounds for $\cal_{2}$. Several interesting questions remain, including (1) obtaining an explicit high probability swap regret guarantee for the log loss, similar to Section \ref{sec:bound_calibration}; (2) improving the $T^{\frac{2}{3}}$ dependence (e.g., to $\sqrt{T}$ as in~\citet{hu2024predict}) for a bounded proper loss in Corollaries~\ref{cor:simutaneous_bounds_msr}, \ref{cor:simultaneous_bounds_psreg}; and (3) studying KL-Calibration in the offline setting.