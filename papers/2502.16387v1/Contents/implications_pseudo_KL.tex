\section{Implications of (Pseudo) KL-Calibration}\label{sec:implications}
 In this section, we discuss the implications of (pseudo) KL-Calibration towards minimizing the  maximum (pseudo) swap regret. In particular, we shall show that (pseudo) KL-Calibration upper bounds the following:  (a) $(\mathsf{P})\msr_{\cL_{2}}$  (subsection \ref{subsec:KL-bounds-L-2});  (b) $(\mathsf{P})\msr_{\cL_{G}}$ (subsection \ref{subsec:KL-bounds-L-G2}). This gives a strong incentive to study (pseudo) KL-Calibration.
 
The following proposition, which relates (pseudo) swap regret with Bregman Divergence is central to all subsequent results developed in this work.
\begin{proposition}\label{prop:swap_reg_breg_div}
    For any proper loss $\ell$ and a swap function $\sigma: [0, 1] \to [0, 1]$, let $\breg_{-\ell}$ be the Bregman divergence associated with the negative univariate form $-\ell$. We have
    \begin{align*}
        \sreg_{\sigma} ^ {\ell} &= \sum_{p \in \cZ} \bigc{\sum_{t = 1} ^ {T} \ind{p_{t} = p}} \bigc{\breg_{-\ell}(\rho_{p}, p) - \breg_{-\ell}(\rho_{p}, \sigma(p))}, \\
        \psreg_{\sigma} ^ {\ell} &= \sum_{p \in \cZ} \bigc{\sum_{t = 1} ^ {T} \cP_{t}(p)} \bigc{\breg_{-\ell}(\tilde{\rho}_{p}, p) - \breg_{-\ell}(\tilde{\rho}_{p}, \sigma(p))},
    \end{align*}
where $\rho_{p} = \frac{\sum_{t = 1} ^ {T} \ind{p_{t} = p} y_{t}}{\sum_{t = 1} ^ {T} \ind{p_{t} = p}}, \tilde{\rho}_{p} = \frac{\sum_{t = 1} ^ {T} \cP_{t}(p) y_{t}}{\sum_{t = 1} ^ {T} \cP_{t}(p)}$. Furthermore, \begin{align*}
   \sreg ^ {\ell} = \sum_{p \in \cZ} \bigc{\sum_{t = 1} ^ {T} \ind{p_{t} = p}} {\breg_{-\ell}(\rho_{p}, p)}, \;
   \psreg ^ {\ell} = \sum_{p \in \cZ} \bigc{\sum_{t = 1} ^ {T} \cP_{t}(p)} {\breg_{-\ell}(\tilde{\rho}_{p}, p)}.
\end{align*}
\end{proposition}
The proof of Proposition \ref{prop:swap_reg_breg_div}, deferred to Appendix \ref{app:implications}, follows by an application of Lemma \ref{lem:characterization_proper_loss} and is similar to~\citet{hu2024predict}. Two particularly interesting applications of Proposition \ref{prop:swap_reg_breg_div} are:
\begin{itemize}[leftmargin=*]
\item For the squared loss $\ell(p, y) = (p - y) ^ {2}$, the univariate form is $\ell(p) = p - p ^ {2}$, and $\breg_{-\ell}(\rho_{p}, p) = (\rho_{p} - p) ^ {2}$. Therefore, $\sreg^{\ell} = \cal_{2}, \psreg^{\ell} = \pcal_{2}$.
\item For the log loss $\ell(p, y) = y\log\frac{1}{p} + (1-y)\log\frac{1}{1-p}$, the univariate form is $\ell(p) = \mathbb{E}_{y \sim p} [\ell(p, y)] = -p \log p - (1 - p) \log (1 - p)$. Moreover, as can be verified by direct computation, the associated Bregman divergence $\breg_{-\ell}(\hat{p}, p)$ is exactly equal to $\KL(\hat{p}, p)$. Therefore, we have $\sreg^{\ell} = \kcal, \psreg^{\ell} = \pkcal$.
This equivalence between (pseudo) KL-Calibration and (pseudo) swap regret of the log loss shall be our starting tool towards the developments in Sections \ref{sec:achieve-KL-Cal}, \ref{sec:pseudo-KL-Cal}, where we bound $\kcal, \pkcal$ respectively.
\end{itemize}
Note that since $\psreg^\ell \leq \mathbb{E}[\sreg^\ell]$ trivially holds by definition, $\pcal_{2}$ and $\pkcal$ are indeed weaker notions compared to $\cal_2$ and $\kcal$ respectively.


\subsection{(Pseudo) KL-Calibration implies  maximum (pseudo) swap regret against $\cL_{2}$} \label{subsec:KL-bounds-L-2}
In this subsection, we show that $\msr_{\cL_{2}} = \cO(\kcal), \mpsr_{\cL_{2}} = \cO(\pkcal)$, where \begin{align*}
    \cL_{2} \coloneqq \{\ell \in \cL \text{ s.t. the univariate form } \ell(p) \text{ is twice continuously differentiable in } (0, 1)\}.
\end{align*}
 Note that according to Lemma \ref{lem:characterization_proper_loss}, for each $\ell \in \cL$, the univariate form must be concave, Lipschitz, and bounded, for the induced loss $\ell(p, y)$ to be proper and bounded. In addition to these implicit constraints, we require the condition that the second derivative $\ell''(p)$ is continuous in $(0, 1)$. 
 We state several examples of losses that belong to $\cL_{2}$. First, the squared loss clearly belongs to $\cL_{2}$, since its univariate form is $\ell(p) = p - p ^ {2}$.
 Second, consider a generalization of the squared loss via Tsallis entropy, which corresponds to a loss with the univariate form $\ell(p) =  -c \cdot p ^ {\alpha}$, where we choose $\alpha > 1$ and the proportionality constant $c > 0$ is to ensure that the induced loss $\ell(p, y)$ is in $[-1, 1]$ (refer Lemma \ref{lem:characterization_proper_loss}). We have, $\ell(p, y) = c (\alpha - 1) p ^ {\alpha} - \alpha c p ^ {\alpha - 1} y$, which is in $\cL_{2}$.
 Third, the spherical loss has the univariate form $\ell(p) = -\sqrt{p ^ {2} + (1 - p) ^ {2}}$ and is also contained in $\cL_{2}$. 
 
 The following lemma, derived by \cite{luo2024optimal}, provides a growth rate on the second derivative of any $\ell \in \cL_{2}$ and is a key ingredient for our proof of the desired implication.
\begin{lemma}[Lemma 2 in \cite{luo2024optimal}]\label{lem:hessian_growth}
    For a function $f$ that is concave, Lipschitz, and bounded over $[0, 1]$ and twice continuously differentiable over $(0,1)$, there exists a constant $c > 0$ such that $|f''(p)| \le c \cdot \max\bigc{\frac{1}{p}, \frac{1}{1 - p}}$ for all $p \in (0, 1)$.
\end{lemma}

Using this to bound $\abs{u''(p)}$ in the statement of Lemma \ref{lem:relate_breg_vbreg}, we immediately obtain the following proposition whose proof can be found in Appendix \ref{app:implications}.



\begin{proposition}\label{prop:breg_div_decomposable}
    Let $\ell \in \cL_{2}$. Then, we have $\breg_{-\ell}(\hat{p}, p) = \cO\bigc{{\KL}(\hat{p}, p})$
    and thus $$\msr_{\cL_{2}} = \cO(\kcal), \quad \mpsr_{\cL_{2}} = \cO(\pkcal).$$
\end{proposition}
We remark that Proposition \ref{prop:breg_div_decomposable} holds more generally for any subclass of proper losses where each loss satisfies the growth rate in Lemma \ref{lem:hessian_growth}. To keep the exposition simple, we only state our results for $\cL_{2}$.


\subsection{(Pseudo) KL-Calibration implies (pseudo) maximum swap regret against $\cL_{G}$}\label{subsec:KL-bounds-L-G2}

We now consider another class $\cL_{G}$, containing proper losses whose univariate form is $G$-smooth, i.e.,
$\cL_{G} \coloneqq \bigcurl{\ell \in \cL \text{ s.t. } \abs{\ell''(p)} \le G \text{ for all } p \in [0, 1]}$. Losses that belong to $\cL_{G}$ include squared loss, spherical loss, Tsallis entropy for $\alpha \ge 2$, etc. Notably, the latter does not lie in $\cL_{G}$ for $\alpha \in (1, 2)$.
Using Lemma \ref{lem:relate_breg_vbreg} again, along with the fact 
$\pcal_{2} \le \pkcal, \cal_{2} \le \kcal$ due to Pinsker's inequality,
we immediately obtain the following.

\begin{proposition}
\label{prop:bound_breg_div_quadratically}
    Let $\ell \in \cL_{G}$. Then, we have $\breg_{-\ell}(\hat{p}, p) \le G(\hat{p} - p) ^ {2}$, and thus $$
        \msr_{\cL_{G}} \le  G \cdot \cal_{2} \le G \cdot \kcal, \quad \mpsr_{\cL_{G}} \le G \cdot \pcal_{2} \le G \cdot \pkcal.$$
\end{proposition}

The proof of Proposition \ref{prop:bound_breg_div_quadratically} is deferred to Appendix \ref{app:implications}. As already mentioned, \cite{fishelsonfull} proposed an algorithm that achieves $\pcal_{2} = \tilde{\cO}(T^{\frac{1}{3}})$, which implies that the same algorithm in fact ensures $\mpsr_{\cL_{G}} = \tilde{\cO}(G \cdot T^{\frac{1}{3}})$. However, the implications of $\kcal, \pkcal$ allow us get simultaneous guarantees for a broader subclass of proper losses, particularly, $\cL_{2} \cup \cL_{G}$.


