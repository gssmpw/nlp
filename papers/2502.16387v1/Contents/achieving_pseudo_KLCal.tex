\section{Achieving Pseudo KL-Calibration}\label{sec:pseudo-KL-Cal}
In this section, we propose an explicit algorithm that achieves $\psreg^{\ell} = \cO(T^{\frac{1}{3}} (\log T) ^ {\frac{2}{3}})$ for the log loss, therefore the same algorithm achieves $\pkcal = \cO(T^{\frac{1}{3}} (\log T) ^ {\frac{2}{3}})$. Our algorithm is based on the well-known Blum-Mansour (BM) reduction \citep{blum2007external} and extends the idea from~\citet{fishelsonfull}. 
First, we employ a similar but slightly different non uniform discretization scheme that adds two extra end points $z_0$ and $z_{K}$ to the one used in the previous section (for technical reasons):
\begin{align*}
    \cZ = \{z_{0}, z_{1}, \dots, z_{K - 1}, z_{K}\}, \text{where } z_{0} = \sin^{2} \frac{\pi}{4K}, z_{i} = \sin ^ {2} \frac{\pi i}{2K} \text{ for } i \in [K - 1], z_{K} = \cos^{2}\frac{\pi}{4K},
\end{align*}
and $K \in \mathbb{N}$ is a constant to be specified later. 
The same scheme was used before by~\citet{rooij2009learning, kotlowski2016online} for different problems.
Since the conditional distribution $\cP_{t}$ has support over $\cZ$, taking supremum over all swap functions $\sigma: \cZ \to \cZ$ in Proposition \ref{prop:swap_reg_breg_div}, we obtain \begin{align*}
    \sup_{\sigma: \cZ \to \cZ}  \psreg_{\sigma} ^ {\ell} &= \psreg^{{\ell}} -  \sum_{p \in \cZ} {\sum_{t = 1} ^ {T} \cP_{t}(p)} \inf_{\sigma: \cZ \to \cZ}\breg_{-\ell}(\rho_{p}, \sigma(p)) \ge \psreg^{{\ell}} -  \frac{(2 - \sqrt{2}) \pi ^ {2} T}{K ^ {2}},
\end{align*}
where the inequality follows by choosing $\sigma(p) = \argmin_{z \in \cZ} \breg_{-\ell}(\rho_p, z)$. For this choice of $\sigma$, from \citep[page 13]{kotlowski2016online}, we have $\breg_{-\ell}(\rho_p, \sigma(p)) \le \bigc{2 - \sqrt{2}} \frac{\pi ^ {2}}{K ^ {2}}$. Therefore, \begin{align}\label{eq:psreg_[0,1]_to_psreg_Z}
     \psreg^{{\ell}} \le  \sup_{\sigma: \cZ \to \cZ}  \psreg_{\sigma} ^ {\ell} + \bigc{2 - \sqrt{2}} \pi ^ {2} \frac{T}{K ^ {2}},
\end{align}
and it suffices to bound $\sup_{\sigma: \cZ \to \cZ}  \psreg_{\sigma} ^ {\ell}$, which we do via the BM reduction. Towards this end, we first recall the BM reduction. The reduction maintains $K + 1$ external regret algorithms $\cA_{0}, \dots, \cA_{K}$. At each time $t$, let $\q_{t, i} \in \Delta_{K + 1}$ represent the probability distribution over $\cZ$ output by $\cA_{i}$.
Let $\Q_{t} = [\q_{t, 0}, \dots, \q_{t, K}]$ be the matrix obtained by stacking the vectors $\q_{t, 0}, \dots, \q_{t, K}$ as columns. We compute the stationary distribution of $\Q_{t}$, i.e., a distribution $\p_{t} \in \Delta_{K + 1}$ over $\cZ$ that satisfies $\Q_{t}\p_{t} = \p_{t}$. 
With $\p_{t}$ being our final distribution of predictions (that is, $\cP_t(z_i) = p_{t,i}$), we draw a prediction from it and observe $y_{t}$.
After that, we feed the scaled loss function $p_{t, i} \ell(., y_{t})$ to $\cA_{i}$. Let $\tilde{\bi{\ell}}_{t, i} = p_{t, i} \bi{\ell}_{t} \in \Rn^{K + 1}$ be a scaled loss vector, where $\ell_{t}(j) = \ell(z_{j}, y_{t})$.  
It then follows from \cite[Theorem 5]{blum2007external} that \begin{align}\label{eq:sum_external_regret}
    \sup_{\sigma: \cZ \to \cZ} \psreg_{\sigma} ^ {\ell} \le \sum_{i = 0} ^ {K} \textsc{Reg}_{i}, \text{where } \textsc{Reg}_{i} \coloneqq \sup_{j \in [K + 1]} {\sum_{t = 1} ^ {T} \ip{\q_{t, i} - \e_{j}}{\tilde{\bi{\ell}}_{t, i}}},
\end{align}
i.e., the pseudo swap regret is bounded by the sum of the external regrets of the $K + 1$ algorithms.
We summarize the discussion so far in Algorithm \ref{alg:BM_log_loss}.

\begin{algorithm}[t]
                    \caption{BM for log loss} 
                    \label{alg:BM_log_loss}
                    \textbf{Initialize:} $\cA_{i}$ for $i \in \{0, \dots, K\}$ and set $\q_{1} = \bigs{\frac{1}{K + 1}, \dots, \frac{1}{K + 1}}$;
                    \begin{algorithmic}[1]
                            \STATE\textbf{for} $t = 1, \dots, T$
                            \STATE\hspace{3mm}Set $\Q_{t} = [\q_{t, 0}, \dots, \q_{t, K}]$;
                            \STATE\hspace{3mm}Compute the stationary distribution of $\Q_{t}$, i.e., $\p_{t} \in \Delta_{K + 1}$ that satisfies $\Q_{t}\p_{t} = \p_{t}$;
                            \STATE\hspace{3mm}Output conditional distribution $\cP_{t}$, where $\cP_{t}(z_{i}) = p_{t}(i)$ and observe $y_{t}$;
                            \STATE\hspace{3mm}\textbf{for} $i = 0, \dots, K$
                            \STATE\hspace{3mm}\hspace{3mm} Feed the scaled loss function $f_{t, i}(w) = p_{t, i} \ell(w, y_{t})$ to  $\cA_{i}$ (Algorithm \ref{alg:A_i}) and obtain $\q_{t + 1, i}$; \\
                        \end{algorithmic}
\end{algorithm}	
It remains to 
derive the $i$-th external regret algorithm $\cA_{i}$ that minimizes $\textsc{Reg}_{i}$ in \eqref{eq:sum_external_regret}. Note that $\cA_{i}$ is required to predict a distribution $\q_{t, i}$ over $\cZ$ and is subsequently fed a scaled loss function $p_{t, i} \ell(., y_{t})$ at each time $t$. We propose to employ the Exponentially Weighted Online Optimization (EWOO) algorithm along with a novel randomized rounding scheme for $\cA_{i}$ (Algorithm \ref{alg:A_i}).

\begin{algorithm}[t]
                    \caption{The $i$-th external regret algorithm ($\cA_{i}$)} 
                    \label{alg:A_i}
                    \begin{algorithmic}[1]
                            \STATE\textbf{for} $t = 1, \dots, T$                           
                            \STATE\hspace{3mm} Set $w_{t, i} \in [0,1]$ as the output of $\text{EWOO}_{i}$ (Algorithm~\ref{alg:EWOO}) at time $t$;
                            \STATE\hspace{3mm} Predict $\q_{t, i} = \text{RROUND}^{\text{log}}(w_{t, i})$ (Algorithm~\ref{alg:rounding_alg});
                            \STATE\hspace{3mm} Receive the scaled loss function $f_{t, i}(w) = p_{t, i} \ell(w, y_{t})$.
                        \end{algorithmic}
\end{algorithm}	
EWOO was studied by \cite{hazan2007logarithmic} for minimizing the regret $\sup_{w \in \cW} \sum_{t = 1} ^ {T} f_{t}(w_{t}) - f_{t}(w)$, when $\cW$ is a convex set, and the loss functions $f_{t}$'s are exp-concave. Since the log loss is $1$-exp-concave in $p$ over $[0, 1]$ (\cite[page 46]{cesa2006prediction}, $\text{EWOO}_{i}$ (an instance of EWOO for $\cA_{i}$) with functions $\{f_{t, i}\}_{t = 1} ^ {T}$ defined as $f_{t, i}(w) = p_{t, i} \ell(w, y_{t})$ for all $w \in \cW$, where $\cW = [0, 1]$ is a natural choice. 

\begin{algorithm}[t]
                    \caption{Exponentially Weighted Online Optimization ($\text{EWOO}_{i}$) with scaled losses}
                    \label{alg:EWOO}
                    \begin{algorithmic}[1]
                            \STATE\textbf{for} $t = 1, \dots, T$
                            \STATE\hspace{3mm} Set weights $\mu_{t, i}(w) = \exp\bigc{-\sum_{\tau = 1} ^ {t - 1} f_{\tau, i}(w)}$ for all $w \in \cW$;
                            \STATE\hspace{3mm} Output $w_{t, i} = \frac{\int_{w \in \cW} w \mu_{t, i}(w) dw}{\int_{w \in \cW} \mu_{t, i}(w) dw}$.
                        \end{algorithmic}
\end{algorithm}	

Next, we derive a bound on the regret of $\text{EWOO}_{i}$. Towards this end, we realize that the scaled log loss $f_{t, i}(w) = p_{t, i} \ell(w, y_{t})$ is $1$-exp-concave since $\exp(-f_{t,i}(w)) = w^{y_tp_{t,i}}(1-w)^{(1-y_t)p_{t,i}}$ is concave when $p_{t,i} \in [0,1]$. Appealing to \cite[Theorem 7]{hazan2007logarithmic}, we then obtain the following lemma.

\begin{lemma}\label{lem:EWOO_regret_bound}
    The regret of Algorithm \ref{alg:EWOO} satisfies $
        \sup_{w \in \cW} \sum_{t = 1} ^ {T} f_{t, i}(w_{t, i}) - f_{t, i}(w) \le \log (T + 1).$
\end{lemma}

Note that at each time $t$, $\text{EWOO}_{i}$ outputs $w_{t, i} \in [0, 1]$, however, $\cA_{i}$ is required to predict a distribution $\q_{t, i} \in \Delta_{K + 1}$ over $\cZ$. Thus, we need to perform a rounding operation that projects the output $w_{t, i}$ of $\text{EWOO}_{i}$ to a distribution over $\cZ$. In Remark \ref{rem:rounding} in Appendix \ref{app:deferred_proofs_pseudo_KL_Cal},
we show that the following two known rounding schemes: (a) rounding $w_{t, i}$ to the nearest $z \in \cZ$ and setting $\q_{t, i}$ as the corresponding one-hot vector; (b) the rounding procedure proposed by \cite{fishelsonfull}, cannot be applied to our setting since they incur a $\Omega(1)$ change in the expected loss $\ip{\q_{t, i}}{\bi{\ell}_{t}} - \ell(w_{t, i}, y_{t})$, which is not sufficient to achieve the desired regret guarantee.
To mitigate the shortcomings of these rounding procedures, we propose a different randomized rounding scheme for the log loss (Algorithm \ref{alg:rounding_alg}) that achieves a $\cO\bigc{\frac{1}{K^{2}}}$ change in the expected loss, as per  Lemma \ref{lem:rounding}. 
\begin{algorithm}[t]
                    \caption{Randomized rounding for log loss $(\textsc{RROUND}^{\text{log}})$}
                    \textbf{Input:} $p \in [0, 1]$, \textbf{Output:} Probability distribution $\q \in \Delta_{K + 1}$; \\ \label{alg:rounding_alg}
                    \textbf{Scheme:} Let $i \in \{0, \dots, K - 1\}$ be such that $p \in [z_{i}, z_{i + 1})$. Output $\q \in \Delta_{K + 1}$, where 
                    \[
                    q_i = \frac{1}{D} \cdot \frac{z_{i + 1} - p}{z_{i + 1}(1 - z_{i + 1})}, \quad
                    q_{i+1} = \frac{1}{D} \cdot \frac{p - z_{i}}{z_{i}(1 - z_{i})},
                    \;\text{ and }\;
                    q_j = 0, \;\;\forall j \notin \{i, i+1\}
                    \]
                with $D = \frac{p - z_{i}}{z_{i}(1 - z_{i})} + \frac{z_{i + 1} - p}{z_{i + 1}(1 - z_{i + 1})}$ being the normalizing constant.
\end{algorithm}	
\begin{lemma}\label{lem:rounding}
    Let $p \in [0, 1]$ and $p^{-}, p^{+} \in \cZ$ be neighbouring points in $\cZ$ such that $p^{-} \le p < p^{+}$. Let $q$ be the random variable that takes value $p^{-}$ with probability $\propto \frac{p^{+} - p}{p^{+}(1 - p^{+})}$ and $p^{+}$ with probability $\propto \frac{p - p^{-}}{p^{-}(1 - p^{-})}$. Then, for all $y \in \{0, 1\}$, we have $\mathbb{E}[\ell(q, y)] - \ell(p, y) = \cO\bigc{\frac{1}{K^{2}}}.$ 
\end{lemma}

The high-level idea of the proof is as follows:
since the log loss  is convex in $p$ (for any $y \in \{0, 1\}$), we have $\ell(q, y) - \ell(p, y) \le \ell'(q, y) \cdot (q - p) = \frac{(q - y)(q - p)}{q(1 - q)}$,
which is $\frac{p}{q} - 1$ if $y = 1$, and $\frac{1 - p}{1 - q} - 1$ if $y = 0$. By direct computation of $\mathbb{E}\bigs{\frac{1}{q}}$ and $\mathbb{E}\bigs{\frac{1}{1 - q}}$, we show that $\mathbb{E}\bigs{\frac{p}{q}} - 1 = \mathbb{E}\bigs{\frac{1 - p}{1 - q}} - 1 \le (p^{+} - p^{-}) ^ {2} \cdot \max\bigc{\frac{1}{p^{-}(1 - p^{-})}, \frac{1}{p^{+}(1 - p^{+})}} = \cO\bigc{\frac{1}{K ^ {2}}}$, where the last step follows from a technical result due to Lemmas \ref{lem:discretization} (Appendix \ref{app:proof_existence_swap_log_loss}) and \ref{lem:discretization_II} (Appendix \ref{app:deferred_proofs_pseudo_KL_Cal}).


Combining everything, we derive the regret guarantee $\textsc{Reg}_{i}$ of $\cA_{i}$ (Algorithm \ref{alg:A_i}). It follows from Lemma \ref{lem:rounding} that at any time $t$, the distribution $\q_{t, i}$ obtained by rounding the prediction $w_{t, i}$ of $\text{EWOO}_{i}$ as per Algorithm \ref{alg:rounding_alg} satisfies $\ip{\q_{t, i}}{\bi{\ell}_{t}} =  \ell(w_{t, i}, y_{t}) + \cO(\frac{1}{K ^ {2}})$. Multiplying with $p_{t, i}$ and summing over all $t$, we obtain \begin{align}
    \sum_{t = 1} ^ {T} \ip{\q_{t, i} - \e_{j}}{\tilde{\bi{\ell}}_{t, i}} &= \sum_{t = 1} ^ {T} p_{t, i} \ell(w_{t, i}, y_{t}) - \sum_{t = 1} ^ {T} p_{t, i} \ell(z_{j}, y_t) + \cO\bigc{\frac{\sum_{t = 1} ^ {T} p_{t, i}}{K ^ {2}}}, \nn \\
    &\le \sup_{w \in \cW} {\sum_{t = 1} ^ {T} f_{t, i}(w_{t, i}) - f_{t, i}(w)} + \cO\bigc{\frac{ \sum_{t = 1} ^ {T} p_{t, i}}{K ^ {2}}} = \cO\bigc{\log T + {\frac{\sum_{t = 1} ^ {T} p_{t, i}}{K ^ {2}}}} \nn,
\end{align}
where the last equality follows from Lemma \ref{lem:EWOO_regret_bound}. Therefore, the regret $\textsc{Reg}_{i}$ of $\cA_{i}$ satisfies $$\textsc{Reg}_{i} = \cO\bigc{\log T + \frac{1}{K ^ {2}} \sum_{t = 1} ^ {T} p_{t, i}}.$$ Summing over all $i$, we obtain \begin{align*}
    \sup_{{\sigma: \cZ \to \cZ}} \psreg_{\sigma} ^ {\ell} \le \sum_{i = 0} ^ {K} \textsc{Reg}_{i} = \cO \bigc{K \log T + \frac{1}{K ^ {2}} \sum_{i = 0} ^ {K} \sum_{t = 1} ^ {T} p_{t, i}} = \cO\bigc{K \log T + \frac{T}{K ^ {2}}}.
\end{align*}
Finally, it follows from \eqref{eq:psreg_[0,1]_to_psreg_Z} that $\psreg ^ {\ell} = \cO\bigc{K \log T + \frac{T}{K ^ {2}}} = \cO\bigc{T^{\frac{1}{3}} (\log T) ^ {\frac{2}{3}}}$
on choosing $K = (T/\log T)^{\frac{1}{3}}$. Therefore, we have the main result of this section.
\begin{theorem}\label{thm:bound_pkcal}
     Choosing $K = (T/\log T)^{\frac{1}{3}}$, Algorithm \ref{alg:BM_log_loss} achieves $\pkcal = \cO\bigc{T^{\frac{1}{3}} (\log T) ^ {\frac{2}{3}}}$.
\end{theorem}
In a similar spirit as Corollary \ref{cor:simutaneous_bounds_msr}, we can show Algorithm \ref{alg:BM_log_loss} achieves the following regret bounds simultaneously. The proof can be found in Appendix \ref{app:deferred_proofs_pseudo_KL_Cal} and for most part follows similar to Corollary \ref{cor:simutaneous_bounds_msr}, except that we prove and utilize the bounds (a) $\pcal_{1} \le \sqrt{T \cdot \pcal_{2}}$; (b) for any $\ell \in \cL$, $\psreg^{\ell} \le 4\pcal_{1}$. 

\begin{corollary}
  \label{cor:simultaneous_bounds_psreg}
    Algorithm \ref{alg:BM_log_loss} achieves the following bounds simultaneously: \begin{align*}&{{\pkcal}} = \cO(T^{\frac{1}{3}} (\log T) ^ {\frac{2}{3}}), \quad {\mpsr_{\cL_{G}}} = \cO(G \cdot T^{\frac{1}{3}} (\log T) ^ {\frac{2}{3}}), \\ 
    & {\mpsr_{\cL_{2}}} = \cO(T^{\frac{1}{3}} (\log T) ^ {\frac{2}{3}}), \quad
    {\mpsr_{\cL\backslash\{\cL_{G} \cup \cL_{2}\}}} = \cO(T^{\frac{2}{3}} (\log T) ^ {\frac{1}{3}}).\end{align*}
\end{corollary}
