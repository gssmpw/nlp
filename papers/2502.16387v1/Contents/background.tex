\section{Preliminaries and Background}\label{sec:preliminaries}

\paragraph{Notation} For a $m \in \mathbb{N}$, $[m]$ denotes the index set $\{1, \dots, m\}$. We reserve bold lower-case alphabets for vectors and bold upper-case alphabets for matrices. The notation $\ind{.}$ refers to the indicator function, which evaluates to $1$ if the condition is true, and $0$ otherwise. We use $\e_{i}$ to represent the $i$-th standard basis vector (dimension inferred from context), which is $1$ at the $i$-th coordinate and $0$ everywhere else. 
For any $k \in \mathbb{N}$, we use $\Delta_{k}$ to represent the $(k - 1)$-dimensional simplex. Moreover, we use $\Delta_{[0, 1]}$ to represent the set of all probability distributions over $[0, 1]$. We use $\mathbb{P}_{t}, \mathbb{E}_{t}$ to represent the conditional probability, expectation respectively, where the conditioning is over the randomness till time $t - 1$ (inclusive). Throughout the paper, $\mathsf{KL}(p, q), \mathsf{TV}(p, q), \chi^{2}(p, q)$ shall represent the KL divergence, total variation distance, chi-squared distance between two Bernoulli distributions with means $p, q$. 
For a set $\cI$, we represent its complement by $\bar{\cI} = \Omega \backslash \cI$, where the sample set $\Omega$ shall be clear from the context.
A twice differentiable function $f: \cD \to \Rn$ is called $\alpha$-\textit{smooth} over $\cD \subset \Rn$ if $f''(x) \le \alpha$ for all $x \in \cD$. A function $f: \cW \to \Rn$ is \textit{$\alpha$-exp-concave} over a convex set $\cW$ if the function $\exp(-\alpha f(w))$ is concave over $\cW$. We use the notation $\tilde{\cO}(.)$ to hide lower order logarithmic terms.

\paragraph{Proper Losses} A loss $\ell: [0, 1] \times \{0, 1\} \to \Rn$ is called proper if $\mathbb{E}_{y \sim p} [\ell(p, y)] \le \mathbb{E}_{y \sim p}[\ell(p', y)]$ for all $p, p' \in [0, 1]$. Intuitively, a proper loss incentivizes the forecaster to report the true distribution of the label. Throughout the paper, we shall be primarily concerned about the family $\cL$ (or a subset) of bounded proper losses, i.e., $\cL \coloneqq \{\ell \text{ s.t. } \ell \text{ is proper and } \ell(p, y) \in [-1, 1] \text{ for all } p \in [0, 1], y \in \{0, 1\}\}$, even though our results hold for (and in fact achieved via) the unbounded log loss. For a proper loss $\ell$, the \textit{univariate} form of $\ell$ is defined as $\ell(p) \coloneqq \mathbb{E}_{y \sim p}[\ell(p, y)]$. 
It turns out that a loss is proper only if its univariate form is concave.
Moreover, one can construct a proper loss using a concave univariate form based on the following characterization lemma.

\begin{lemma}[Theorem 2 in~\citet{gneiting2007strictly}]\label{lem:characterization_proper_loss}
    A loss $\ell: [0, 1] \times \{0, 1\} \to \mathbb{R}$ is proper if and only if there exists a concave function $f$ such that $\ell(p, y) = f(p) + \ip{g_p}{y - p}$ for all $p \in [0, 1], y \in \{0, 1\}$, where $g_{p}$ denotes a subgradient of $f$ at $p$.
    Also, $f$ is the univariate form of $\ell$.
\end{lemma}
Examples of proper losses include squared loss $\ell(p, y) = (p - y)^{2}$, log loss $\ell(p, y) = y\log\frac{1}{p} + (1-y)\log\frac{1}{1-p}$, spherical loss $\ell(p, y) = -\frac{p y + (1 - p) (1 - y)}{\sqrt{p ^ {2} + (1 - p) ^ {2}}}$, etc. 

\paragraph{Bregman Divergence} For a convex function $\phi$, let $\breg_{\phi}(x, y) = \phi(x) - \phi(y) - \ip{\partial \phi(y)}{x - y}$ denote the Bregman divergence associated with $\phi$. 
The following lemma is important to our results.
\begin{lemma}[Lemma 3.8 in \cite{hu2024predict}]\label{lem:relate_breg_vbreg}
    Let $u: [0, 1] \to [-1, 1]$ be a twice differentiable concave function. Then, we have
    $
        \breg_{-u}(\hat{p}, p) = 
        \int_{p} ^ {\hat{p}} \abs{u''(\mu)} \cdot (\hat{p} - \mu) d\mu.
    $
\end{lemma}

\paragraph{Problem Setting}
As mentioned in Section~\ref{sec:intro}, we consider calibration, where the interaction between the forecaster and the adversary is according to the following protocol: at each time $t=1, \ldots, T$, (a) the forecaster randomly predicts $p_{t} \in [0, 1]$ and simultaneously the adversary chooses $y_{t} \in \{0, 1\}$; (b) the forecaster observes $y_{t}$. Throughout the paper, we shall consider algorithms that make predictions $p_{t}$ that fall in a finite discretization $\cZ \subset [0, 1]$.
According to \eqref{eq:KLCal_PKLCal_def}, the KL-Calibration, Pseudo KL-Calibration incurred by the forecaster are  $\kcal = \sum_{p \in \cZ} \sum_{t = 1} ^ {T} \ind{p_{t} = p} \KL(\rho_{p}, p), \pkcal = \sum_{p \in \cZ} \sum_{t = 1} ^ {T} \cP_{t}(p)\KL(\tilde{\rho}_{p}, p)$, where 
$\rho_{p} = \frac{\sum_{t = 1} ^ {T} y_{t} \ind{p_{t} = p}}{\sum_{t = 1} ^ {T} \ind{p_{t} = p}}, \tilde{\rho}_{p} = \frac{\sum_{t = 1} ^ {T} y_{t} \cP_{t}(p)}{\sum_{t = 1} ^ {T} \cP_{t}(p)}$.\footnote{
For convenience, we set $\frac{0}{0} = 0$. This is because if $n_{p} = 0$, the forecast $p_{t} = p$ was never made and thus does not contribute to the calibration error.}  For simplicity, we assume that the adversary is oblivious, that is it selects $y_{1}, \dots, y_{T}$ at time $t = 0$ with complete knowledge of the forecaster's algorithm\footnote{However, our results generalize directly to an adaptive adversary who decides $y_t$ based on $p_1,\ldots, p_{t-1}$.}. Our goal is to minimize the (pseudo) KL-Calibration error, which as we show in Section~\ref{sec:implications}, has powerful implications.


As mentioned, the swap regret of the forecaster with respect to a loss function $\ell$ against a swap function $\sigma: [0, 1] \to [0, 1]$ is $\sreg^{\ell}_\sigma = \sum_{t = 1} ^ {T} \ell(p_{t}, y_{t}) - \ell(\sigma(p_{t}), y_{t})$. Swap regret is then defined as $\sreg ^ {\ell} = \sup_{\sigma: [0, 1] \to [0, 1]} \sreg^{\ell}_{\sigma}$. Similarly, the pseudo swap regret is $\psreg^{\ell} = \sup_{\sigma: [0, 1] \to [0, 1]} \psreg^{\ell}_{\sigma}$, where $\psreg_{\sigma}^{\ell} = \sum_{p \in \cZ} \sum_{t = 1} ^ {T} \cP_{t}(p) (\ell(p, y_{t}) - \ell(\sigma(p), y_{t}))$. 
We further define \textit{maximum (pseudo) swap regret} with respect to the class of bounded proper losses $\cL$ as \begin{align}\label{eq:MSR}
    \msr_{\cL} \coloneqq \sup_{\ell \in \cL} \sreg^{\ell}, \quad \mpsr_{\cL}  \coloneqq \sup_{\ell \in \cL} \psreg^{\ell}.
\end{align}
For a subset of losses $\cL' \subseteq \cL$, we define $\msr_{\cL'}$ and $\mpsr_{\cL'}$ similar to \eqref{eq:MSR}, with the supremum over $\ell \in \cL'$. The usage of $\ell$ for a bounded proper loss, or the log loss (which does not belong to $\cL$) shall be clear from the context.
 