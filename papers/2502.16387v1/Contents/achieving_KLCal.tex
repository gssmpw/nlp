\section{Achieving KL-Calibration}\label{sec:achieve-KL-Cal}
In this section, we prove that there exists an algorithm that achieves $\mathbb{E}[\sreg^{\ell}] = \cO(T^{\frac{1}{3}} (\log T) ^ {\frac{5}{3}})$ for $\ell$ being the log loss, therefore the same algorithm achieves $\mathbb{E}[\kcal] = \cO(T^{\frac{1}{3}} (\log T) ^ {\frac{5}{3}})$. Our proof is non-constructive, since it is based on swapping the adversary and the algorithm via the minimax theorem (Theorem \ref{thm:von_neumann} in Appendix \ref{app:proof_existence_swap_log_loss}), and deriving a forecasting algorithm in the dual game.


\begin{theorem}\label{thm:log_loss_swap_reg}
    There exists an algorithm that achieves $\mathbb{E}[\sreg^{\ell}] = \cO(T^{\frac{1}{3}} (\log T) ^ {\frac{5}{3}})$ for the log loss, where the expectation is taken over the internal randomness of the algorithm.
\end{theorem}

The proof of Theorem \ref{thm:log_loss_swap_reg} is quite technical and is deferred to Appendix \ref{app:proof_existence_swap_log_loss}. We discuss the key novelty of our proof here. Two particularly technical aspects of our proof are the usage of a non uniform discretization, which is contrary to all previous works, and the use of Freedman's inequality for martingale difference sequences (Lemma \ref{lem:Freedman}). In particular, we employ the following discretization scheme: $\cZ = \{z_{1}, \dots, z_{K - 1}\} \subset [0,1], \text{ where } z_{i} = \sin ^ {2} \bigc{\frac{\pi i}{2K}}$ and $K \in \mathbb{N}$ is a constant to be specified later. For convinience, we set $z_{0} = 0, z_{K} = 1$,
however, $z_{0}, z_{K}$ are not included in the discretization. For our analysis, we require a discretization scheme that satisfies the following constraints: (a) $z_{i} - z_{i - 1} = \cO(\frac{1}{K})$ for all $i \in [K]$; (b) $\frac{\max ^ {2}(z_{i} - z_{i - 1}, z_{i + 1} - z_{i})}{z_{i}(1 - z_{i})} = \cO\bigc{\frac{1}{K^{2}}}$ for all $i \in [K - 1]$; (c) $\sum_{i = 1} ^ {K - 1} \frac{1}{z_{i}(1 - z_{i})} = \cO(K^{2})$; and (d) $\sum_{i = 1} ^ {K - 1} \frac{1}{\sqrt{z_{i}(1 - z_{i})}} = \tilde{\cO}(K)$. The uniform discretization $\cZ = \{\frac{1}{K}, \dots, \frac{K - 1}{K}\}$ satisfies {(a), (c), (d)} above, however, doesn't satisfy (b).
As we show in Lemma \ref{lem:discretization} (Appendix \ref{app:proof_existence_swap_log_loss}), our considered non uniform discretization achieves all these required bounds by having a finer granularity close to the boundary of $[0,1]$, thereby making it suitable for our purpose.
The following steps provide a brief sketch of our proof, which is proved for an adaptive adversary and therefore also holds for the weaker oblivious adversary.


\paragraph{Step I} We only consider discretized forecasters that make predictions that lie inside $\cZ$. Since the strategy space of such forecasters is finite, and that of the adversary is trivially finite, Theorem \ref{thm:von_neumann} applies and we can swap the adversary and the algorithm, thereby resulting in the dual game. In this dual game, at every time $t$, the adversary first reveals the conditional distribution of $y_{t}$, based on which the forecaster predicts $p_{t}$. We consider a forecaster $F$ which at time $t$ does the following: (a) it computes $\tilde{p}_{t} = \mathbb{E}_{t}[y_{t}]$; (b) predicts $p_{t} = \argmin_{z \in \cZ} \abs{\tilde{p}_{t} - z}$. For such a forecaster, we obtain a high probability bound on $\sreg^{\ell}$, and subsequently bound $\mathbb{E}[\sreg^{\ell}]$.

\paragraph{Step II} Applying Lemma \ref{lem:Freedman}, we show that for each $i$ (with $n_i = n_{z_i}$)
\begin{align*}
        \abs{\sum_{t = 1} ^ {T}\ind{p_{t} = z_{i}} (\tilde{p}_{t} - y_{t})} \le 2\sqrt{\log \frac{2}{\delta}} \cdot\max\bigc{\sqrt{n_{i}\bigc{z_{i}(1 - z_{i}) + \frac{\pi}{2K}}}, \sqrt{\log \frac{2}{\delta}}}
\end{align*}
with probability at least $1 - \delta KT$. Using this, we bound $\abs{z_{i} - \rho_{i}}$, where $\rho_{i}$ is a shorthand for $\rho_{z_{i}}$. Notably, the bound above dictates separate consideration of $i \in \cI$ and $i \in \bar{\cI}$ (depending on which term realizes the maximum), where $\cI \coloneqq \bigcurl{i \in [K - 1]; n_{i} < \frac{\log \frac{2}{\delta}}{z_{i}(1 - z_{i}) + \frac{\pi}{2K}}}$.

\paragraph{Step III} Next, we write $\sreg^{\ell}$ as  the sum of two terms $\sreg^{\ell} = \text{Term I} + \text{Term II}$, where $\text{Term I} = \sum_{i \in \cI} n_{i} \KL(\rho_{i}, z_{i}), \text{Term II} = {\sum_{i \in \bar{\cI}} n_{i} \KL(\rho_{i}, z_{i})},$ and bound Term I, II individually. Since $\KL(\rho_{i}, z_{i}) \le \chi^{2}(\rho_{i}, z_{i}) = \frac{(\rho_{i} - z_{i}) ^ {2}}{z_{i}(1 - z_{i})}$, we utilize the bound on $\abs{\rho_{i} - z_{i}}$ obtained in the previous step and show that $\text{Term II} = \cO\bigc{\frac{T}{K ^ {2}} + K \log \frac{1}{\delta}}$.
Importantly, the use of Freedman's inequality provides a variance term that mitigates the potentially small denominator of $\frac{(\rho_{i} - z_{i}) ^ {2}}{z_{i}(1 - z_{i})}$.
Similarly, we show that $\text{Term I} = \cO\bigc{\frac{T}{K ^ {2}} + K (\log K)^{\frac{3}{2}}  \log \frac{1}{\delta}}$. Combining, we obtain $\sreg^{\ell} = \cO\bigc{\frac{T}{K ^ {2}} + K (\log K) ^ {\frac{3}{2}} \log \frac{1}{\delta}}$ with probability at least $1 - \delta KT$. Subsequently, we bound $\mathbb{E}[\sreg^{\ell}]$ by setting $\delta = 1/T, K = T^{\frac{1}{3}}/{(\log T) ^ {\frac{5}{6}}}$.

Equipped with Theorem \ref{thm:log_loss_swap_reg}, we prove the following stronger corollary whose proof can be found in Appendix \ref{app:proof_existence_swap_log_loss}. 


\begin{corollary}\label{cor:simutaneous_bounds_msr}
    There exists an algorithm that achieves the following bounds simultaneously: 
        \begin{align*}
        &\mathbb{E}\bigs{\kcal} = \cO(T^{\frac{1}{3}} (\log T) ^ {\frac{5}{3}}), \quad
        \mathbb{E}\bigs{\msr_{\cL_{G}}} = \cO(G \cdot T^{\frac{1}{3}} (\log T) ^ {\frac{5}{3}}), \\ 
        &\mathbb{E}\bigs{\msr_{\cL_{2}}} = \cO(T^{\frac{1}{3}} (\log T) ^ {\frac{5}{3}}), \quad \mathbb{E}\bigs{\msr_{\cL\backslash\{\cL_{G} \cup \cL_{2}\}}} = \cO(T^{\frac{2}{3}} (\log T) ^ {\frac{5}{6}}),
        \end{align*}
where the expectation is taken over the internal randomness of the algorithm.
\end{corollary}