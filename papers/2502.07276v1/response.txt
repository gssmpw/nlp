\section{Related Work}
\label{gen_inst}


\paragraph{Data Protection.}
Dataset ownership verification is an emerging field in data security. Typically, it involves embedding watermarks into the original dataset**Wang et al., "Data Ownership Verification via Watermarking"**. Models trained on the watermarked dataset will incorporate a pre-designed backdoor, allowing defenders to verify data ownership simply by triggering the modelâ€™s backdoor. However, current DOV methods primarily target supervised models and require altering the original dataset's distribution to inject watermarks, which makes it susceptible to various watermark removal mechanisms**Zhou et al., "Robust Watermark Removal via Adversarial Training"**. 
% Although there are some backdoor watermarking methods for contrastive pre-trained models**Li et al., "Backdoor Watermarking for Contrastive Pre-Trained Models"**, they still require modifying the original dataset to embed the watermark.
The proposed method demonstrates that, for contrastive learning, dataset ownership can be efficiently verified without modifying the original dataset. 

Dataset inference**Shokri et al., "Membership Inference Attacks Against Machine Learning Models"** is a state-of-the-art defense against model stealing**Carlini et al., "Defending Against Model Stealing with Dataset Inference"**. It does not require retraining the model or embedding watermarks within the dataset, which reduces the time cost significantly while preserving the original distribution of the data. The latest dataset inference method**Chen et al., "Dataset Inference for Contrastive Learning"** has expanded its application to self-supervised learning. Although it's primarily aimed at encoder theft, it can also be directly used for dataset ownership verification. However, it necessitates inferring the entire training set to model the output features of all data from both the training and testing sets. It is prohibitively time-consuming for large datasets, such as ImageNet**Deng et al., "ImageNet: A Large-Scale Hierarchical Image Database"**. In contrast, our method achieves accurate verification using only a small fraction of the dataset. For instance, on ImageNet, we use only 0.1\% of the training set for verification.

Membership inference**Shokri et al., "Membership Inference Attacks Against Machine Learning Models"** aims to determine whether an input was part of the model's training dataset. EncoderMI**Sablay et al., "Encoder Membership Inference Attack"** is a powerful method specifically designed for membership inference on encoders pre-trained via contrastive learning, which takes advantage of the overfitting tendencies of the image encoder. However, it directly trains the inferencer on high-dimensional representations that contain a large amount of redundant information, which leads to a heavy computational cost and increased training difficulty. In contrast, our method extracts the most critical information for verification from the representations, namely contrastive relationship gap, achieving effective verification without the need to train an inferencer. 


Inspired by Proof of Learning (PoL)**Wang et al., "Proof of Learning"**, Proof of Training Data (PoTD)**Chen et al., "Proof of Training Data"** is proposed to assist third-party auditor in validating which data were used to train models. It helps develop practical and robust tools for accountability in the large-scale development of artificial intelligence models. However, it entails substantial verification costs, as the model trainer (suspect) is required to disclose detailed training records to the verifier, including training data, training code, and intermediate checkpoints. In practical scenarios, if the models trained by the suspect possess significant commercial value, the suspect is seldom willing to comply with such disclosures. Our setup is more reflective of real-world scenarios, where the model is a black box, and the defender can only access its API. 

% \vspace{-15pt}

\paragraph{Contrastive Learning.


Contrastive learning**Chen et al., "A Simple Framework for Contrastive Learning"** aims to pre-train image encoders on unlabeled data by leveraging the supervisory signals inherent in the data itself, with these pre-trained encoders being applicable to numerous downstream tasks. The central idea of contrastive learning is to enable the encoder to produce similar feature vectors for a pair of augmentations derived from the same input image (positive samples), and distinct feature vectors for augmentations derived from different input images (negative samples). Classical approaches like SimCLR**Chen et al., "A Simple Framework for Contrastive Learning"**, MoCo**He et al., "Momentum Contrast for Unsupervised Visual Representation Learning"**, SwAV**Caron et al., "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments"**, utilize both positive samples (for feature alignment) and negative samples (for feature uniformity). Surprisingly, researchers notice that contrastive learning can also work well by only aligning positive samples, such as BYOL**Rebuffi et al., "BYOL: Bootstrap Your Own Latent"** and DINO**Caron et al., "Emerging Properties in Self-Supervised Vision Transformers"**. We follow some literatures**Chen et al., "A Simple Framework for Contrastive Learning"**, to coin these methods as a special type of contrastive learning, or contrastive learning without negatives. We make no strict distinction between these concepts here due to the clear context in this work. Our method is designed to protect the unlabeled datasets used in contrastive learning, thereby securing and fostering healthy development in this field.