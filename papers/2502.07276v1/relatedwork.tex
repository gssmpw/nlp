\section{Related Work}
\label{gen_inst}


\paragraph{Data Protection.}
Dataset ownership verification is an emerging field in data security. Typically, it involves embedding watermarks into the original dataset~\citep{guo2023domain,li2022untargeted,li2023black,tang2023did}. Models trained on the watermarked dataset will incorporate a pre-designed backdoor, allowing defenders to verify data ownership simply by triggering the modelâ€™s backdoor. However, current DOV methods primarily target supervised models and require altering the original dataset's distribution to inject watermarks, which makes it susceptible to various watermark removal mechanisms~\citep{chen2021refit,liu2021wdnet,sun2023defending,kwon2021defending,hayase2021spectre}. 
% Although there are some backdoor watermarking methods for contrastive pre-trained models~\cite{li2023embarrassingly,zhang2024data,carlini2021poisoning}, they still require modifying the original dataset to embed the watermark.
The proposed method demonstrates that, for contrastive learning, dataset ownership can be efficiently verified without modifying the original dataset. 

Dataset inference~\cite{maini2021dataset} is a state-of-the-art defense against model stealing~\citep{sha2023can,sanyal2022towards,shen2022model}. It does not require retraining the model or embedding watermarks within the dataset, which reduces the time cost significantly while preserving the original distribution of the data. The latest dataset inference method~\cite{dziedzic2022dataset} has expanded its application to self-supervised learning. Although it's primarily aimed at encoder theft, it can also be directly used for dataset ownership verification. However, it necessitates inferring the entire training set to model the output features of all data from both the training and testing sets. It is prohibitively time-consuming for large datasets, such as ImageNet~\citep{deng2009imagenet}. In contrast, our method achieves accurate verification using only a small fraction of the dataset. For instance, on ImageNet, we use only 0.1\% of the training set for verification.

Membership inference~\cite{shokri2017membership,choquette2021label,carlini2022membership,hu2022membership} aims to determine whether an input was part of the model's training dataset. EncoderMI~\cite{liu2021encodermi} is a powerful method specifically designed for membership inference on encoders pre-trained via contrastive learning, which takes advantage of the overfitting tendencies of the image encoder. However, it directly trains the inferencer on high-dimensional representations that contain a large amount of redundant information, which leads to a heavy computational cost and increased training difficulty. In contrast, our method extracts the most critical information for verification from the representations, namely contrastive relationship gap, achieving effective verification without the need to train an inferencer. 


Inspired by Proof of Learning (PoL)~\cite{jia2021proof,fang2023proof,zhao2024proof}, Proof of Training Data (PoTD)~\cite{choi2024tools} is proposed to assist third-party auditor in validating which data were used to train models. It helps develop practical and robust tools for accountability in the large-scale development of artificial intelligence models. However, it entails substantial verification costs, as the model trainer (suspect) is required to disclose detailed training records to the verifier, including training data, training code, and intermediate checkpoints. In practical scenarios, if the models trained by the suspect possess significant commercial value, the suspect is seldom willing to comply with such disclosures. Our setup is more reflective of real-world scenarios, where the model is a black box, and the defender can only access its API. 

% \vspace{-15pt}

\paragraph{Contrastive Learning.}


Contrastive learning~\cite{chen2020simple,chen2021exploring,chen2021empirical,caron2020unsupervised,albelwi2022survey,he2020momentum} aims to pre-train image encoders on unlabeled data by leveraging the supervisory signals inherent in the data itself, with these pre-trained encoders being applicable to numerous downstream tasks. The central idea of contrastive learning is to enable the encoder to produce similar feature vectors for a pair of augmentations derived from the same input image (positive samples), and distinct feature vectors for augmentations derived from different input images (negative samples). Classical approaches like SimCLR~\cite{chen2020simple}, MoCo~\cite{he2020momentum}, SwAV~\cite{caron2020unsupervised}, utilize both positive samples (for feature alignment) and negative samples (for feature uniformity). Surprisingly, researchers notice that contrastive learning can also work well by only aligning positive samples, such as BYOL~\cite{grill2020bootstrap} and DINO~\cite{caron2021emerging}. We follow some literatures~\cite{albelwi2022survey,gao2022disco} to coin these methods as a special type of contrastive learning, or contrastive learning without negatives. We make no strict distinction between these concepts here due to the clear context in this work. Our method is designed to protect the unlabeled datasets used in contrastive learning, thereby securing and fostering healthy development in this field.