\vspace{-01em}
\section{Experiments}
\vspace{-0.5em}

We begin by detailing our experimental configuration, including the datasets used for \textit{ADPO} training and evaluation, the evaluated jailbreak attacks, and the models tested. Next, we demonstrate the effectiveness of \textit{ADPO} from two perspectives of safety, measured by its robustness against various jailbreak attacks, and utility, evaluated on normal tasks. To further validate our approach, we visualize the shift in latent space, illustrating how \textit{ADPO} enhances robustness. Finally, we conduct an ablation study to support our hyperparameter choices and compare the impact of generating adversarial perturbations in latent space versus image space.


\vspace{-0.5em}
\subsection{Experiment Setup}
\vspace{-0.5em}

\partitle{Safety alignment datasets} Harmful queries can appear in various forms, including adversarial text queries, harmful image-text pairs, or images generated using Stable Diffusion or typographic techniques. To ensure comprehensive safety alignment during fine-tuning, we construct a new dataset based on the HarmBench multimodal (HarmBench-mm) and adversarial training (HarmBench-AT) datasets. Specifically, we sample 80 image-text pairs from HarmBench-mm, pair 40 text samples from HarmBench-AT with blank images, and generate an additional 80 samples using typographic techniques and Stable Diffusion based on HarmBench-AT. This results in a total of 200 harmful image-text pairs. For the utility dataset, we select 500 samples from LLaVA-Instruct-150K to maintain a balance between safety alignment and model utility during fine-tuning.


\partitle{Evaluated VLMs} We evaluate our method on two widely used open-source VLMs: \texttt{LLaVA-1.5-7b}, \texttt{LLaVA-1.6-7b}. We employ LoRA to fine-tune on all linear layers. In our experiments, we specifically evaluate \textit{ADPO} on LLaVA due to its unique capability of converting images into up to 2,880 image tokens. This high tokenization capacity makes LLaVA particularly sensitive to perturbations in the image space. By focusing on LLaVA series, we aim to rigorously test the robustness of \textit{ADPO} under conditions where image perturbations have a pronounced effect, providing a strong benchmark for evaluating the effectiveness of our approach in enhancing adversarial robustness.
%due to the high computational cost of full parameter fine-tuning. 
Detailed hyperparameters of different fine-tuning setting are provided in Appendix \ref{Hyperparameter}.


\partitle{Evaluated jailbreak attacks} We evaluate two optimization-based attacks, VisualAdv \citep{visual_adv} and MMPGDBlank \citep{harmbench}, on 200 harmful queries from HarmBench standard behaviors. VisualAdv is a universal attack that optimizes a universal adversarial pattern for all harmful behaviors, while MMPGDBlank is a one-to-one attack that optimizes a distinct image for each harmful behavior. Furthermore, we also employ the Jailbreaking subset of MultiTrust \citep{multitrust} to assess the safety of the VLM in a black-box setting. This subset includes three sub-tasks: Typographic Jailbreaking, Multimodal Jailbreaking, and Cross-modal Jailbreaking. Typographic Jailbreaking simply embeds the jailbreaking prompts generated by GPTfuzzer \cite{gptfuzzer} and DAN \cite{dan} into images using typographic methods. Multimodal Jailbreaking involves the random sampling of instances from the existing Multimodal Jailbreak Benchmark \cite{figstep, mm-safetybench}. Cross-modal Jailbreaking investigates whether VLMs are susceptible to adversarial text queries when paired with images, specifically by associating jailbreak prompts with task-relevant images rather than sample-specific images.

\partitle{Evaluated utility benchmark} To evaluate the impact of \textit{ADPO} on normal tasks, we conduct experiments on four widely adopted utilities benchmarks including MMStar \cite{MM-Star}, OCRBench \cite{ocrbench}, MM-Vet \cite{mm-vet}, LLaVABench \cite{llava-1.5}. 



\begin{table*}[ht]
    \centering

    \resizebox{\textwidth}{!}{
    \begin{tabular}{c | c c c c c| c c c c }
    \hline
    & \multicolumn{5}{c}{Safety $\downarrow$} & \multicolumn{4}{|c}{Utility$\uparrow$} \\
    \cline{2-10}
    
        &  \multirow{3}{*}{\textbf{VisualAdv}} &  \multirow{3}{*}{\textbf{MMPGDBlank}} & \multicolumn{3}{c|}{\textbf{MultiTrust}} &\multirow{3}{*}{MMStar} &\multirow{3}{*}{OCRBench} & \multirow{3}{*}{MM-Vet} & \multirow{3}{*}{LLaVABench} \\
          \cline{4-6}
         
   &  & & Typographic& Multimodal & Crossmodal \\
  &  & & Jailbreak & Jailbreak & Jailbreak \\   
\hline
     LLaVA-1.5-7b  & 64.5 & 84.0 & 22.2 & 55.1 & 42.0 & 32.7 & 202  & 29.9 & 59.5\\

    +Supervised FT& 19.0 & 76.0 & 0.5 & 10.3 & 27.1 & 33.7 (\textcolor{red}{$\uparrow$}) & 201  & 28.6 & 53.6\\

    + DPO & 12.0 & 33.0 & 0.7 & 8.8 & 9.6& 33.9 (\textcolor{red}{$\uparrow$}) & 198  & 28.9 & 54.4 \\

    +\textit{AR-DPO} & \colorbox{gray!30}{\textbf{2.5}} & 1.0 & \colorbox{gray!30}{\textbf{0.0}} & \colorbox{gray!30}{\textbf{0.0}} & 2.4& \underline{34.1} (\textcolor{red}{$\uparrow$})& 187  & 23.3 & 47.7 \\

    +\textit{AT-DPO} & 7.5 & 8.5 & 0.5 & 3.4 & 9.1 & 33.4 (\textcolor{red}{$\uparrow$})& \underline{193}  & \underline{28.9} & \underline{51.6} \\

    + \textit{ADPO} & 5.0 & \colorbox{gray!30}{\textbf{0.5}} & \colorbox{gray!30}{\textbf{0.0}} & \colorbox{gray!30}{\textbf{0.0}} & \colorbox{gray!30}{\textbf{0.2}} &33.7 (\textcolor{red}{$\uparrow$})& 184& 24.2 & 48.2\\ 
    % + \textit{L-ADPO} & & 2.0 & 0.0 & 0.0 & 2.2 & & & 25.1 & \\[2pt]
    \hline

     LLaVA-1.6-7b  & 33.5 & 48.5 & 8.5 & 58.3 & 56.2 & 37.9 & 500  & 43.1 & 66.8\\

    +Supervised FT& 6.5  & 22.5 & 2.0 & 25.4 & 34.2  & 38.2 & 501 (\textcolor{red}{$\uparrow$}) & 40.0 & 58.6\\

    + DPO & 2.0 & 7.0 & 1.2 & 7.1 & 27.1 & 38.1  (\textcolor{red}{$\uparrow$}) & 489  & 38.3 & 59.1\\

    +\textit{AR-DPO} & \colorbox{gray!30}{\textbf{0.0}} & 8.5 & 0.2 & \colorbox{gray!30}{\textbf{0.0}} & \colorbox{gray!30}{\textbf{2.4}} & \underline{37.7} & 436  & 38.0 & 50.5\\

    +\textit{AT-DPO} & 0.5 & 3.5 & 0.5 & 4.9 & 21.3 & 36.9  & \underline{448}& \underline{38.9} & \underline{58.2}\\

    + \textit{ADPO} & \colorbox{gray!30}{\textbf{0.0}} & \colorbox{gray!30}{\textbf{0.0}} & \colorbox{gray!30}{\textbf{0.0}} & 0.2 & 8.4 & 36.9 & 433 & 37.6 & 50.9 \\
    % + \textit{L-ADPO} & & & 1.2 & 0.0 & 24.9 & & & & \\[2pt]
    \hline
\end{tabular}}
\vspace{-1em}
\caption{Safety and utility evaluation of \textit{ADPO}, its ablations, and baselines on \texttt{LLaVA-1.5} and \texttt{LLaVA-1.6}. For safety evaluation, the lowest ASR for each jailbreak attack is highlighted in bold and gray shadow. For utility evaluation, the highest score among \textit{ADPO} and its ablations is underlined. Cases where the utility score improves after safety alignment compared to the original model are marked with \textcolor{red}{$\uparrow$}.}
\label{safety evaluation}
\vspace{-1em}
\end{table*}



\vspace{-0.5em}
\subsection{Safety Evaluation}
\vspace{-0.5em}
In this section, we evaluate the effectiveness of \textit{ADPO} in improving safety alignment. We compare \textit{ADPO} against baselines including supervised fine-tuning (SFT) and standard DPO, as well as its ablations: \textit{AR-DPO} (adversarial-trained reference model only) and \textit{AT-DPO} (adversarial-aware DPO loss only).  The evaluation focuses on Attack Success Rate (ASR) across various jailbreak attacks, which is defined as the fraction of successful attacks over all tested examples. The HarmBench classifier \cite{harmbench} is employed to determine whether the model responses are harmful. 
% Lower ASR indicates more effective safety alignment method, and the lowsest ASR achieved for each attack is marked bold. 
%we present the evaluation of safety and utility of all fine-tuning methods on VLMs. For safety evaluation, we adopt the Attack Success Rate (ASR) as the primary metric, which represents the ratio of successful attacks. The HarmBench classifier \cite{harmbench} is employed to determine whether the model responds to harmful queries. The results are presented in Table \ref{safety evaluation}.

As shown in the safety column of Table \ref{safety evaluation},  \textit{ADPO} and its ablations (\textit{AR-DPO} and \textit{AT-DPO}) significantly reduce the ASR across all jailbreak attacks on both \texttt{LLaVA-1.5} and \texttt{LLaVA-1.6}, outperforming SFT and standard DPO.  Specifically, \textit{ADPO} emerges as the most effective method, reducing the ASR to nearly 0 across almost all attacks, underscoring the importance of integrating both the adversarial aware-reference model and adversarial-aware DPO loss.  

In addition, we can notice that \textit{AT-DPO} is not very effective on Crossmodal jailbreak, compared with \textit{AR-DPO} and \textit{ADPO}, highlighting the importance of including the adversarial-aware reference model. The Crossmodal Jailbreaking dataset consists of text-level jailbreak prompts. Since \textit{AT-DPO} adds perturbation only to the image space, it may not generalize well to text-level attacks through a single-stage safety alignment. In contrast, \textit{AR-DPO} and \textit{ADPO}, which utilize an adversarial trained model as reference model, demonstrate a greater ability to recognize harmful semantics in a harmful query, even when the harmfulness originates from text inputs. Although SFT and DPO exhibit comparable performance on some cases in the Multitrust benchmark, they demonstrate reduced effectiveness against white-box optimization-based attacks. Notably, the MMPGDBlank attack maintains a high ASR, with values of 33.0 and 7.0 for DPO, and 76.0 and 22.5 for FT on \texttt{LLaVA-1.5} and \texttt{LLaVA-1.6} respectively. In contrast, \textit{ADPO} achieved 0.5 and 0 ASR on MMPGDBlank. 




%all methods increases the robustness of LLaVA-1.5 against all jaibreak attacks. Although FT and DPO exhibit comparable performance on the Multitrust benchmark, they demonstrate reduced effectiveness against white-box optimization-based attacks. Notably, the MMPGDBlank attack maintains a high ASR, with values of 33.0 for DPO and 76.0 for FT. In contrast, \textit{AR-DPO}, \textit{AT-DPO}, and \textit{ADPO} demonstrate their effectiveness against all jailbreak attacks, exhibiting an ASR of no more than 0.1, even in a white-box setting. This demonstrates the effectiveness of integrating adversarial training into DPO. Moreover, \textit{ADPO} emerges as the most effective method, reducing the ASR to nearly 0 across almost all attacks.

\begin{figure}[t]
    \centering
    \includegraphics[width=.4\textwidth]{fig/radar_plot_1.5.pdf}
    \setlength{\abovecaptionskip}{0.2cm}
    \vspace{-1em}
    \caption{Safety-utility trade-off, where jailbreak dimensions indicate the ASR reduction (the larger the better). A larger area for each method represents more effective in safety alignment and utility maintainness. } 
    \label{radar}
    \vspace{-1em}
\end{figure}

\vspace{-0.5em}
\subsection{Utility Evaluation}
\vspace{-0.5em}
\textit{ADPO}, along with its ablations and baselines is evaluated on four normal task benchmarks, each has its own evaluation metric (detailed in Appendix \ref{utility benchmark}). MMStar focuses on image-based multiple-choice questions, while the other three benchmarks are visual question answering (VQA) datasets. The results are shown in the utility column of Table \ref{safety evaluation}. For all datasets, a higher score indicates better performance on that dataset. The highest score among \textit{ADPO} and its ablations is underlined. Cases where the utility score improves after safety alignment compared to the original model are marked with \textcolor{red}{$\uparrow$}.

Overall, all methods somehow reduce the utility score on VQA bechmarks, whihe  multiple-choice dataset MMStar experience an increase in the utility score after safety fine-tuning, indicating its less sensitive to the safety alignment.
Although \textit{ADPO} and \textit{AR-DPO} demonstrate remarkable performance in enhancing robustness against jailbreak attacks, we observe a slight trade-off on the VQA datasets. This indicates that the adversarial training process, while enhancing safety, may inadvertently lead to a more conservative model behavior, occasionally affecting its ability to handle benign queries. This finding suggests the necessity to explore refined fine-tuning strategies and objective functions in the future work to further optimize this balance.



\begin{figure*}[h]
    \centering
        
    \begin{subfigure}[b]{1\textwidth}
        \centering
        \begin{minipage}{0.01\textwidth}
        \centering
        % 插入文字
        \rotatebox{90}{\tiny  \textbf{MMPGDBlank}}
    \end{minipage}
    \begin{minipage}{0.95\textwidth}
        \includegraphics[width=\linewidth]{fig/visualization_mmpgd.pdf}
        \label{figvis:sub1}
        \end{minipage}
    \end{subfigure}

    \vfill
    \vspace{-1.5em}

    \begin{subfigure}[b]{1\textwidth}
        \centering
                \begin{minipage}{0.01\textwidth}
        \centering
        % 插入文字
        \rotatebox{90}{\tiny  \textbf{VisualAdv}}
    \end{minipage}
        \begin{minipage}{0.95\textwidth}
        \includegraphics[width=\linewidth]{fig/visualization_adv.pdf}
        \label{figvis:sub2}
        \end{minipage}
    \end{subfigure}

\centerline{
\textcolor[rgb]{0.8705882352941177, 0.5607843137254902, 0.0196078431372549}{\small$\bullet$ Harmful anchor query} \textcolor[rgb]{0.00392156862745098, 0.45098039215686275, 0.6980392156862745}{\small$\bullet$ Harmless anchor query} \textcolor[rgb]{0.00784313725490196, 0.6196078431372549, 0.45098039215686275}{\small$\bullet$ HarmBench query} \textcolor[rgb]{0.8352941176470589, 0.3686274509803922, 0.0}{\small$\bullet$ HarmBench query +  Attack}}
% \vspace{-1em}
    \caption{Visualization of representation space of \texttt{LLaVA-1.5} trained with \textit{ADPO}, its ablations and FT. (1) Harmbench queries (green) are closer to the harmful anchor cluster (yellow) , demonstrating the model's success in recognizing their harmfulness. (2) \texttt{LLaVA-1.5} trained with \textit{ADPO} and its ablations successfully moves the orange cluster closer to the harmful (yellow) and HarmBench (green) clusters (black arrow) while pushing it further from the harmless cluster (blue, red arrow), indicates that the safety aligned model can better recognize the harmfulness in  Harmbench queries even with the existence of jailbreak attacks.}
    \label{visualization}
    \vspace{-1em}
\end{figure*}

\partitle{Safety and utility trade-off}
To further evaluate the safety-utility trade-off, we present a radar chart in Figure \ref{radar}. Note that the jailbreak dimensions indicate the ASR reduction (the larger the better) and MultiTrust dimension denotes the average ASR reduction across its sub-tasks.  A larger area represents more effective in safety alignment and utility maintainess.  As shown in Figure \ref{radar}, the area for \textit{ADPO} (purple area) and  \textit{AR-DPO} (green are) are the largest compared with SFT and DPO.

%among jailbreak attacks but is slightly smaller than others across utility benchmarks. This demonstrates our finding that that adversarial training, while enhancing model safety, may inadvertently reduce its utility, underscoring the need for future research to minimize this trade-off. 

\vspace{-0.5em}
\subsection{Latent Space Representation Analysis}
\vspace{-0.5em}
Shifts in the latent space representation of harmful queries towards the the direction of harmless query can reveal the mechanisms of jailbreak attacks \cite{lin2024towards}. 

Similarly, to further validate the effectiveness of \textit{ADPO}, we visualize the representation space of \texttt{LLaVA-1.5} using the output of LLM's last hidden state, which captures comprehensive information from the entire sequence. Specifically, we employ principle component analysis (PCA) \cite{PCA} to analysis four types of queries: Harmful anchor query, Harmless anchor query, HarmBench query, HarmBench query with Attack. The harmful and harmless anchor queries, collected from  \cite{zheng2024prompt}, serve as reference points for general harmful and harmless queries, exhibiting significant differences in harmfulness while maintaining similar query formats and text lengths.


% We also visualize the VLM's last hiddden states of the last token of four types of queryies through PCA in Figure \ref{visualization}. The anchor harmful and harmless prompts are collected from \cite{zheng2024prompt}, which exhibit a substantial difference in harmfulness but remain highly similar in query formats and text lengths. 

As shown in Figure \ref{visualization}, the representations of harmful and harmless anchor queries form distinct clusters (yellow and blue), indicating the model's ability to differentiate between harmful and harmless semantics. Harmbench queries, which is indicated as green clusters are closer to the harmful anchor cluster (yellow), demonstrating the model's success in recognizing their harmfulness. However, after jailbreak attacks (MMPGDBlank and VisualAdv), HarmBench queries shift significantly towards the harmless cluster (blue), as seen in the orange clusters in the first column of Figure \ref{visualization}.

We compare the latent space of \texttt{LLaVA-1.5} trained with \textit{AR-DPO}, \textit{AT-DPO}, \textit{ADPO} and SFT in the subsequent columns of Figure \ref{visualization}. Notably, \texttt{LLaVA-1.5} trained with \textit{ADPO} and its ablations successfully moves the orange cluster closer to the harmful (yellow) and HarmBench (green) clusters (black arrow) while pushing it further from the harmless cluster (blue, red arrow). In contrast, the SFT model fails to exhibit this behavior. This finding indicates that the safety aligned model can better recognize the harmfulness in  Harmbench queries even with the existence of jailbreak attacks.  

%Meanwhile, jailbreak attacks such as MMPGDBlank and VisualAdv can significantly shift the representation of the HarmBench query towards the direction of the Harmless representation in both LLaVA-1.5 and LLaVA-1.5 with FT. However, the representation of HarmBench query with attack remains close to the original HarmBench query following textit{AR-DPO}, \textit{AT-DPO} and \textit{ADPO} training, suggesting robustness against jailbreak attacks.


% From Figure \ref{visualization}, it can be concluded that MultimodalPGD significantly shifts the representation of the HarmBench prompts towards the direction of the Harmless prompts on LLaVA-1.5. Meanwhile, the representation of the HarmBench prompt with MultimodalPGD remains close to the original HarmBench prompt after \textit{ADPO} training, indicating robustness against jailbreak attacks.





\vspace{-.5em}
\subsection{Ablation Study}
\vspace{-.5em}



Figure \ref{ablation study} presents an ablation study on $\alpha$ in Equation \ref{euqation 7}, which balance the trade-off between safety and utility during adversarial training. The left Y-axis displays the ASR, while the right Y-axis illustrates the False Harm Rate (FHR) on MM-Vet, representing the proportion of benign samples incorrectly flagged as harmful. The optimal goal is to minimize both ASR (enhancing safety robustness) and FHR (preserving utility). Based on the intersection of the two curves, we select the appropriate $\alpha$ value for our experiments.


\begin{figure}[ht]
\setlength{\abovecaptionskip}{0.cm}
    \centering
    \begin{subfigure}[b]{0.225\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/ablation_alpha.pdf}
        % \caption{Subfigure 1}
        \label{fig:sub1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.225\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/ablation_alpha_next.pdf}
        \label{fig:sub2}
    \end{subfigure}
\vspace{-1em}
    \caption{Ablation study on adversarial training $\alpha$.}
    \label{ablation study}
\vspace{-1em}
\end{figure}



\vspace{-.5em}
\subsection{Latent Space Adversarial Training}
\vspace{-.5em}

%Given that jailbreak attacks can be performed on both the image and text modalities of VLMs, 
We also investigate the search of adversarial perturbations in the latent space of image-text embeddings, introduced in Section \ref{AR-DPO}. Specifically, we perform adversarial perturbations at layers 8, 16, 24, and 30 of the backbone LLM for the VLM.  As shown in Table \ref{lat}, where \textit{L-ADPO}, \textit{L-AR-DPO} and \textit{L-AT-DPO} represent the latent space counterparts of ADPO and its ablations. The results indicate that both \textit{L-AR-DPO} and \textit{L-ADPO} exhibit similar performance with their counterparts in the image space. However, \textit{L-AT-DPO} yields a slightly negative result compared with \textit{AT-DPO}. This suggests that adversarial training in the latent space may lead to overfitting to particular adversarial patterns within the latent space, potentially hindering its generalization to natural harmful queries.

\begin{table}[h]
    \centering

    \resizebox{\columnwidth}{!}{
    \begin{tabular}{c |c c c c| c }
    \hline
    & \multicolumn{4}{c}{Safety $\downarrow$} & \multicolumn{1}{|c}{Utility$\uparrow$} \\
    \cline{2-6}
    &  \multirow{2}{*}{\textbf{MMPGDBlank}} & \multicolumn{3}{c|}{\textbf{MultiTrust}} & \multirow{2}{*}{MM-Vet}  \\
    % cline{3-5}
         
     & & Typo& Multimodal & Cross \\
    % & & Jailbreak & Jailbreak & Jailbreak \\   
\hline
     LLaVA-1.5-7b  & 84.0 & 22.2 & 55.1 & 42.0 & 29.9 \\[2pt]
\hline
    +\textit{AR-DPO} & 1.0 & 0.0 & 0.0 & 2.4 & 23.3 \\[2pt]

    +\textit{AT-DPO} & 8.5 & 0.5 & 3.4 & 9.1 & 28.9 \\[2pt]


    + \textit{ADPO} & 0.5& 0.0 & 0.0 & 0.2 & 24.2 \\[2pt]
    \hline



    +\textit{L-AR-DPO} & 2.5 & 0.0 & 0.0 & 1.6 & 23.4\\[2pt]

    +\textit{L-AT-DPO} & 31.5 & 1.0 & 23.1 & 14.9 & 28.9\\[2pt]


    + \textit{L-ADPO} & 2.0 & 0.0 & 0.0 & 2.2  & 25.1  \\[2pt]
    \hline

\end{tabular}}
\vspace{-1em}
\caption{Comparison of worst-case perturbation searched in the image space versus in the latent space of image-text embedding.}
    \label{lat}
    \vspace{-1em}
\end{table}

