\vspace{-0.5em}
\section{Methods}
\vspace{-0.5em}
In this section, we introduce \textit{Adversary-aware DPO (ADPO)}. First, we present DPO with \textbf{adversarial-trained reference model} (\textit{AR-DPO}) in section \ref{AR-DPO}, which leverages an adversarially trained model as the reference model for DPO. Then, in section \ref{AT-DPO}, we describe DPO with \textbf{adversarial-aware loss} (\textit{AT-DPO}), which directly incorporates the adversarial min-max optimization framework into the DPO training procedure. Finally, in section \ref{ADPO}, we combine these components to present the \textit{ADPO} framework.

\partitle{Adversarial training} Adversarial training is a min-max optimization framework designed to enhance model robustness against adversarial attacks. It involves two key stages: (1) the adversary generates worst-case perturbations $\delta$
with in certain constrained set $\Delta$ to maximize the model's loss, and (2) the model updates its parameters to minimize the loss on these perturbed inputs. Formally, this can be expressed as:
\begin{equation}
\fontsize{10}{12}
    \min_\theta \max_{\delta \in \Delta} \mathcal{L}(f_\theta(x + \delta), y),
    \vspace{-0.5em}
\end{equation}
where $f_{\theta}$ represents the model, $x$ and $y$ denote the input and output respectively.

\vspace{-0.5em}
\subsection{\textit{AR-DPO}: DPO with Adversarial-trained Reference Model}\label{AR-DPO}
\vspace{-0.5em}

%Previous studies have demonstrated that the optimization process of DPO is highly sensitive to the initial conditions of the fine-tuned model \citep{feng2024towards}. Consequently, a straightforward approach to incorporating adversarial training into DPO is through the use of an adversarially trained reference model.
The reference model is the cornerstone of DPO, providing a benchmark to guide the target model's output. However, training the reference model solely under benign conditions without the awareness of the adversarial parties leaves the target model vulnerable to perturbations and susceptible to jailbreak attacks. Therefore, an intuitive approach is to train the reference model with worst-case perturbations, enhancing its resilience to jailbreak attacks and consequently ensuring the target model's robustness.

%\partitle{Adversarial Training}
%The adversarial training optimization process comprises two stages: worst-case adversarial perturbation generation and odel Training, and involves two players: the model and the adversary. In the Adversarial Perturbation Generation stage, the adversary attempts to generate worst-case perturbations that mislead the model, while in the Model Training stage, the model aims to minimize its loss over the perturbed inputs. 
%\partitle{Adversarial Training}






%In image classification domain, the adversary can be a specific adversarial attack such as Fast Gradient Sign Method (FGSM) \cite{goodfellow2014explaining} and Projected Gradient Descent (PGD) \cite{pgd}. The input to LLMs is a sequence of discrete tokens and consequently the adversary of LLM adversarial training must be discrete attacks such as Greedy Coordinate Descent (GCG) \cite{GCG} or embedding space attack \cite{CAT}. 

%However, since the input of VLMs includes images, we directly employ PGD as the adversary and add perturbations to the image. 
\partitle{Worst-case perturbation search on image space} 
Since most jailbreak attacks of VLMs are proposed to manipulate image modality, we first consider to search the  worst-case perturbation in the image space. To create a reference model that is aware of the jailbreak attacks in image space, we employ Projected Gradient Descent (PGD) \cite{pgd} to maximize the probability of rejected harmful responses $Y_{r}$. For each harmful image-text pair $x_I$-$x_T$, we optimize the perturbation $\delta$ within a constrained perturbation set $\Delta = \{ \delta \mid x_I+\delta \in [0,1], \left\|\delta\right\|_{p} \le \epsilon \}$. This constraint ensures that each pixel of the perturbed image remains within the valid range, and the maximum perturbation magnitude $\epsilon$ preserves the semantic meaning of the image. The maximization of the probability of rejected responses $Y_{r}$ can be formulated:
\vspace{-0.5em}
{\fontsize{10}{12}
\begin{align}
\label{equation 2}
&\delta^* = \argmax_{\delta \in \Delta} L_{\theta}(x_I, x_T, Y_{r}),\;\text{where}\\
&L_{\theta}(x_I, x_T, Y_{r}) = \log f_{\theta}(Y_{r} \mid x_I+\delta, x_T) 
\vspace{-0.5em}
\end{align}}
%The process of generating adversarial perturbation can be formulated as:
%\begin{equation}
%\label{equation 2}
%\delta = \argmax_{\delta \in \Delta} \log f_{\theta}(Y_{rej} \mid x_I+\delta, x_T) 
%\end{equation}
%where $x_I$,$x_T$ represent the input image and text respectively, $f_{\theta}$ represents the VLM and $\Delta = \{ \delta \mid x_I+\delta \in [0,1], \left\|\delta\right\|_{p} \le \epsilon \}$ is the perturbation set where $\epsilon$ is the maximum perturbation magnitude. The loss is designed to maximize the probability of rejected harmful responses for VLMs under perturbation $\delta$.
This optimization can be solved with Projected Gradient Descent:
\vspace{-0.5em}
\begin{equation}
\fontsize{10}{12}
\delta^{t+1} = \Pi_{\Delta} (x_I^t + \alpha sign\nabla_{x_I^t} L_{\theta}(x_I, x_T, Y_{r})) 
\end{equation}
\vspace{-0.5em}

\partitle{Worst-case perturbation search on latent space} 
To provide a reference model that is also aware of the jailbreak attacks in both text and image domain, we also propose to search for perturbation in the latent space of image-text token embedding. We don't choose to optimize adversarial perturbation over the discrete text token space for two key reasons: (1) optimizing worst-case perturbations in the discrete token space is computationally expensive \cite{harmbench}, and (2) prior studies have shown that such approaches often yield unsatisfactory performance \cite{CAT}.  By operating in the latent space, we achieve a more efficient and effective optimization process in providing an adversarial-aware reference model. Given a VLM $f_\theta$, it can be expressed as the composition of two functions, $f_\theta(Y \mid x_I, x_T) = g_\theta(Y \mid h_\theta(x_I,x_T))$, where $h_\theta$ extracts latent representation of the image-text token embedding, and $g_\theta$ maps these latent activations to the outputs. Similar to the optimization in image space, the search for adversarial perturbation $\delta$ on image-text latent space can be formulated as:
\vspace{-0.5em}
\begin{equation}
\fontsize{10}{12}
\label{equation 3}
    \delta^* = \argmax_{\delta \in \Delta} \log g_\theta(Y_{r} \mid h_{\theta}(x_I, x_T)+ \delta )
    \vspace{-0.5em}
\end{equation}


\partitle{Reference model updates to minimize the loss on perturbed inputs} After generates the worst-case perturbation $\delta^*$, the reference model is adversarially trained to minimize the loss on perturbed inputs. 
The loss is designed to achieve two objectives: (1) maximizing the probability of generating preferred answer on harmful inputs and (2) maintain the general utility on a normal instruction following dataset. To this end, the adversarial training loss consists of two components: the toward loss $\mathcal{L}_{toward}$ to increase the likelihood of preferred safe responses $Y_{p}$  and the utility loss $\mathcal{L}_{utility}$ to preserve the general utility, which can be formulated as: 
\vspace{-0.5em}
\begin{equation}
\fontsize{10}{12}
    \mathcal{L}_{toward} = -\log f_{\theta}(Y_{p}\mid x^{h}_I+\delta^*, x^{h}_T)
\end{equation}
\begin{equation}
\fontsize{10}{12}
    \mathcal{L}_{utility} = -\log f_{\theta}(Y_{util}\mid x^{util}_I, x^{util}_T)
\end{equation}
\vspace{-1em}

If the perturbation is optimized on latent space, the $\mathcal{L}_{toward}$ can be reformulated as:
\vspace{-0.5em}
\begin{equation}
\fontsize{10}{12}
    \mathcal{L}_{toward} = -\log g_\theta(Y_{p} \mid h_{\theta}(x_I^{h}, x_T^{h})+ \delta^* )
    \vspace{-0.5em}
\end{equation}

The overall loss of adversarial training can be formulated as weighted combination of the above two parts and the adversarially trained reference model $f_{\theta_{AT}}$ is optimized with following formula:
\vspace{-0.5em}
\begin{equation}
\fontsize{10}{12}
\label{euqation 7}
    f_{\theta_{AT}} = \argmin_{f_\theta} \mathcal{L}_{toward} + \alpha\mathcal{L}_{utility}
\end{equation}
\vspace{-0.5em}

\partitle{DPO training}
Next, we take the adversarially trained VLM $f_{\theta_{AT}}$ as the reference model for DPO. The objective is to encourage the model to maximize the likelihood of preferred responses while minimizing the likelihood of rejected responses, which can be formulated as:
\vspace{-0.5em}
{\fontsize{10}{12}
\begin{align}
\mathcal{L}_{\text{DPO}} &= -\log \sigma \left( \beta \log \frac{f_{\theta}(Y_{p} | x_I, x_T)}{f_{\theta_{AT}}(Y_{p} | x_I, x_T)} \right. \notag \\
&\quad \left. - \beta \log \frac{f_{\theta}(Y_{r} | x_I, x_T)}{f_{\theta_{AT}}(Y_{r} | x_I, x_T)} \right)
\end{align}}
% \vspace{-0.5em}
where $\beta$ is a hyperparameter and controls the penalty the deviations from reference model $f_{\theta_{AT}}$. A higher $\beta$ enforces stricter adherence to the reference model, while a lower
$\beta$ allows more flexibility. The term $\log \frac{f_{\theta}(Y_{p} | x_I, x_T)}{f_{\theta_{AT}}(Y_{p} | x_I, x_T)}$ and $\log \frac{f_{\theta}(Y_{r} | x_I, x_T)}{f_{\theta_{AT}}(Y_{r} | x_I, x_T)}$ measures likelihood of generating the preferred response and rejected answer respectively under the target model $f_{\theta}$ versus the reference model $f_{\theta_{AT}}$. Maximizing the former term encourages the target model to assign higher probability to preferred responses compared to the reference model, while minimizing this term discourages the target model from assigning high probability to rejected responses.


\vspace{-0.5em}
\subsection{\textit{AT-DPO}: DPO Training with Adversarial-aware Loss}
\vspace{-0.5em}
\label{AT-DPO}
Adversarial training can be viewed as the integration of adversarial examples into the training process, and it is independent of the particular choice of the training objective function. Therefore, in addition to utilizing an adversarially trained model as the reference for DPO, we also investigate the potential of direct incorporation of adversarial techniques into the DPO training process. If the perturbation is searched on image space, the loss funtion for \textit{AT-DPO} can be formulated as:
\vspace{-0.5em}
{\fontsize{10}{12}
\begin{align}
\mathcal{L}_{\text{AT-}\text{DPO}} &= -\log \sigma \left( \beta \log \frac{f_{\theta}(Y_{p} | x_I+\delta^* , x_T)}{f_{ref}(Y_{p} | x_I, x_T)} \right. \notag \\
&\quad \left. - \beta \log \frac{f_{\theta}(Y_{r} | x_I+\delta^* , x_T)}{f_{ref}(Y_{r} | x_I, x_T)} \right)
\end{align}
}
% \vspace{-0.5em}
where $f_{ref}$ represents a normal reference model without fine-tuning. In each training iteration of DPO, the worst-case perturbation $\delta$ is computed according to Equation \ref{equation 2} and is subsequently added to the input images. 

If the perturbation is optimized on latent space, the loss funtion for \textit{AT-DPO} is:
\vspace{-0.5em}
{\fontsize{10}{12}
\begin{align} 
\mathcal{L}_{\text{AT-}\text{DPO}} &= -\log \sigma \left( \beta \log \frac{g_\theta(Y_{p} \mid h_{\theta}(x_I, x_T)+ \delta^* )}{f_{ref}(Y_{p} | x_I, x_T)} \right. \notag \\
 &\quad \left. - \beta \log \frac{g_\theta(Y_{r} \mid h_{\theta}(x_I, x_T)+ \delta^* )}{f_{ref}(Y_{r} | x_I, x_T)} \right)
\end{align}
}


where $\delta$ is computed according to Equation \ref{equation 3} and then is added to the latent activations.
\vspace{-0.5em}
\subsection{Adversarial-aware DPO (\textit{ADPO})}
\vspace{-0.5em}
\label{ADPO}
Adversarial-aware DPO (\textit{ADPO}) combines both the adversarial reference model and adversarial-aware loss into DPO framework. In  Adversarial reference model training stage, the training procedure follows the adversarial training process of \textit{AR-DPO}, producing a robust and adversarial-aware reference model $f_{\theta_{AT}}$. This model is adversarially trained to generate human-preferred responses under worst-case perturbations, ensuring it serves as a reliable benchmark for the second stage.

In adversarial-aware DPO Training stage, \textit{ADPO} incorporates the adversarial-aware loss of \textit{AT-DPO} directly into the DPO training process. The goal is to optimize the target model $f_{\theta}$while accounting for adversarial conditions. This process can be formulated as:
\vspace{-0.5em}
{\fontsize{10}{12}
\begin{align}
\mathcal{L}_{\text{A-}\text{DPO}} &= -\log \sigma \left( \beta \log \frac{f_{\theta}(Y_{p} | x_I+\delta^* , x_T)}{f_{\theta_{AT}}(Y_{p} | x_I, x_T)} \right. \notag \\
&\quad \left. - \beta \log \frac{f_{\theta}(Y_{r} | x_I+\delta^* , x_T)}{f_{\theta_{AT}}(Y_{r} | x_I, x_T)} \right)
\end{align}
}
\vspace{-1em}


