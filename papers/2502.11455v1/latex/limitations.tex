\section*{Limitations}
We outline the limitations of our study as follows: 

1. While enhancing the safety robustness of VLMs, \textit{ADPO} can inevitably compromise their general performance on utility benchmarks, underscoring the need for better optimization of this trade-off in future research. 

2. We only focus on integrating adversarial training into the training process of DPO. The exploration of incorporating adversarial training into other alignment algorithms, such as RLHF or IPO \cite{IPO}, is reserved for future work.

3. In this study, we focus solely on using PGD as the method for generating adversarial perturbations. Therefore, it is worthy to investigate the adaptation of other adversarial attacks, such as C\&W attack \cite{carlini2017towards}, to optimize adversarial perturbations.
