\vspace{-0.5em}
\section{Related Work}
\vspace{-0.5em}


\subsection{Safety Alignment of LLMs}
\vspace{-0.5em}
Ensuring the LLM's behavior aligns with human values is essential. Reinforcement Learning from Human Feedback (RLHF) \citep{instructgpt} proves to be a straightforward and the most effective method to achieve this goal. RLHF learns a reward model on a preference dataset and then uses RL algorithm like Proximal Policy Optimization (PPO) \citep{ppo} to optimize the model by maximizing the expected reward predicted by the reward model. However, RLHF is frequently criticized for its high computational cost and the inherent instability of RL paradigm. Consequently, Direct Preference Optimization (DPO) \citep{dpo} was proposed as a simple alternative of RLHF, which does not need to learn an extra reward model. It enables learning directly from a preference dataset in a supervised way.

\vspace{-0.5em}
\subsection{Adversarial Training}
\vspace{-0.5em}
Despite safety alignment efforts, prior studies \citep{GCG, autodan, dsn} have demonstrated that carefully crafted jailbreak prompts can bypass LLM safety guardrails, highlighting the persistent vulnerabilities of these models. Adversarial training, originally proposed to defend against adversarial examples \citep{goodfellow2014explaining} in image classification tasks, enhances the robustness against adversarial attacks in image classification tasks by forming a min-max optimization, which maximize the worst-case perturbation while minimize the classification loss of the worst-case perturbed training data. Adversarial training has inspired research into its application for mitigating jailbreak attacks in LLMs. For instance, \citet{harmbench} proposes generating adversarial suffixes during each training iteration using optimization-based attacks \citep{GCG} and incorporating them into training data. However, the high computational cost of discrete attacks leads to a significant increase in training overhead. To address this, \citet{CAT} introduces a fast adversarial training algorithm on continuous embedding space, while \citet{LAT} explores adversarial attack in the latent space. To the best of our knowledge, no prior work has integrated  adversarial training in VLM safety alignment.

\vspace{-0.5em}
\subsection{Safety of VLMs}
\vspace{-0.5em}
Building upon a backbone LLM, VLMs also face significant safety concerns. To evaluate their safety, several benchmarks \citep{rtvlm, jailbreakv, vlsbench} and jailbreak techniques \citep{figstep, mm-safetybench, visual_adv, imgjp} have been proposed. Jailbreak attacks on VLMs can be categorized into two types: generation-based attacks and optimization-based attacks. Generation-based attacks \citep{figstep, mm-safetybench} create malicious images directly through typography or text-to-image models like Stable Diffusion, while optimization-based attacks \citep{visual_adv, imgjp} distill harmful queries and add imperceptible noise to original images. To address these vulnerabilities, the most prevalent approach is to construct safety-relevant datasets and fine-tune the target model on them. For example, \citet{vlguard} constructs a vision-language safe instruction-following dataset VLGuard and \citet{spa-vl} proposes a safety preference alignment dataset. MMJ-bench \cite{mmj-bench} present a thorough evaluation on existing jailbreak attacks and defenses on various dataset and models. Although these datasets are effective in enhancing the safety of VLMs when facing harmful queries, they do not consider the existence of malicious users.
%A malicious user can craft sophiscated adversarial perturbations on the inputs to elicit undesired behaviors.

