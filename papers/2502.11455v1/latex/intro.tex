\vspace{-0.5em}
\section{Introduction}
\vspace{-0.5em}




Safety alignment is essential in pre-training large language models (LLMs) \cite{bai2022training, ouyang2022training}, guiding the models to generate responses aligned with human values and enabling them to refuse harmful queries. Such alignment is typically achieved by reinforcement learning with human feedback (RLHF) \cite{ouyang2022training} or Direct Preference Optimization (DPO) \cite{dpo}. 
%RLHF involves training a reward model based on human preferences, and then prediction of the reward model is used as the ground-truth to fine-tune the LLM. Alternatively, DPO directly parameterizes the reward model and fine-tunes the LLM accordingly.
However, Vision-Language Models (VLMs), which use an pre-trained LLM as the backbone along with an image encoder to adapt to down-straeam tasks \cite{llava, llava-1.5, minigpt, instructblip, qwen}, often lack safety alignment as a unified model in the same way as LLMs. As a result, even when the underlying LLM is safety-aligned, VLMs remain vulnerable to jailbreak attacks, where attackers craft sophisticated prompts to manipulate the model into generating toxic content \cite{visual_adv, imgjp, figstep, mm-safetybench}. 


%Such jailbreak attacks can be generation-based black-box attack where malicious images are directly generated through typography or text-to-image models like Stable Diffusion \cite{figstep, mm-safetybench} or optimization-based white-box attack where harmful queries are distilled to an imperceptible noise added to original image \cite{visual_adv, imgjp}. 



\begin{figure}[!ht]
    \centering
    \includegraphics[width=.4\textwidth]{./fig/Intuition.png}
    \setlength{\abovecaptionskip}{0.2cm}
    \vspace{-1em}
    \caption{Safe response rate under white-box and black-box attacks on \texttt{LLaVA-1.5}. Post-hoc safety fine-tuning (SFT and DPO) is less effective on white-box attack.} 
    \label{fig:intuition}
    \vspace{-1em}
\end{figure}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=.9\textwidth]{./fig/main.pdf}
    \setlength{\abovecaptionskip}{0.2cm}
    %\vspace{-1em}
    \caption{Pipeline of ADPO: achieving adversarail-aware safety alignment with adversarial-trained reference model and adversarial-aware DPO loss. The worst-case perturbation is generated on image space or the latent space of image-text embedding.} 
    \label{fig:main}
    \vspace{-1em}
\end{figure*}

%Instead, the safety alignment of VLMs is often achieved through "secondary alignment", which involves designing a human-labeled dataset and fine-tuning the model on that dataset \cite{vlguard, spa-vl}. The quality of the human-labeled dataset directly impact the safety awareness of the VLM \cite{visual_adv, imgjp}. 
Jailbreak attacks can take two forms: generation-based black-box attacks \cite{figstep, mm-safetybench}, where malicious images are generated with typography or text-to-image models like Stable Diffusion \cite{stable_diffusion}, and optimization-based white-box attacks \cite{visual_adv, imgjp}, where harmful queries are distilled into imperceptible noise added to the original image . 
To address these vulnerabilities, the most prevalent approach is to construct safety-relevant datasets and perform post-hoc safety fine-tuing on the target VLMs \cite{vlguard, spa-vl}. For instance, \citet{vlguard} proposed \textit{VLGuard} that constructs a safe instruction-following dataset and uses supervised fine-tuning to enforce safe behavior, while \citet{spa-vl} proposed \textit{SPA-VL} that creates a safety preference alignment dataset and applies DPO to train the model to generate preferred responses given winner-loser pairs.  
However, post-hoc safety fine-tuning (SFT) is more effective on black-box attack than white-box attack, as shown in Figure \ref{fig:intuition}. The safe response rate of SFT is low, and DPO performs slightly better but remains unsatisfactory. This is because these methods rely on learned patterns from training data, making them less robust to worst-case adversarial manipulations, where attackers directly exploit the modelâ€™s internal knowledge to craft jailbreak examples. This limitation highlights the need for a safety alignment that explicitly accounts for adversarial perturbations.

%Similarly, current safety alignment such as DPO, typically fit the model using winner-loser pairs under benign conditions, where the input contain harmful information but is assumed to be non-adversarial. However, in real-world scenarios, particularly under jailbreak attacks, input query can be adversarial, designed to manipulate the model into generating harmful content. This discrepancy highlights a critical limitation: existing alignment methods do not account for worst-case adversarial scenarios. 



%However, such alignment can be break with a small amount of harmful data that is mixed into the benign fine-tuning data \cite{huang2024vaccine}. 



%do not account for the presence of adversarial parties, the model is fit under benign situation where the input query contain harmful information but does not inherently account for adversarial scenarios where malicious actors may attempt to manipulate the model into generating harmful content. 
%In the real-world scenarios, attackers can design sophisticated prompts to manipulate the target model into breaking its safety alignment and generating toxic content, a phenomenon known as "jailbreaking". Such jailbreak attacks can be generation-based attacks, where images with malicious content are directly generated by typography or diffusion models \cite{ddpm}, or optimization-based attacks which involve distilling harmful queries to an optimized noise pattern and adding to original images \cite{visual_adv, imgjp}. 
%While DPO provides a principled and efficient framework for aligning models with human preferences, it does not inherently account for adversarial scenarios where malicious actors may attempt to manipulate the model into generating harmful content.
%Therefore, when implementing safety alignment, it is crucial to consider worst-case adversarial scenarios in order to fundamentally enhance the model's safety performance and its robustness against potential attacks.

%The concept of adversarial training is specifically designed to train model under adversarial scenario and naturally serves as an effective method to achieve the safety alignment considering the adversarial conditions \cite{goodfellow2014explaining}. Adversarial training was originally proposed to enhance the robustness against adversarial attacks in the image classification tasks by forming a min-max optimization, which maximize the worst-case perturbation while minimize the classification loss of the worst-case perturbed training data. Adversarial training has also be adapted to defend against jailbreak attacks on LLMs by adding perturbation either in the word space or latent space \cite{harmbench, CAT, LAT}. 


%Adversarial training enhances model robustness by exposing it to worst-case perturbations, making it a natural complement to safety alignment methods like Direct Preference Optimization (DPO) in mitigating adversarial threats. 


To address this, we propose to integrate adversarial training into the safety alignment process of VLMs, which is a well-established approach in adversarial robustness research\citep{goodfellow2014explaining}, that exposes the model to adversarially perturbed inputs and optimizes the model to resist such manipulations. Specifically, in this work, We propose \textit{Adversary-aware DPO (ADPO)}, that integrates adversarial training into DPO through two key components: the \textbf{adversarial-trained reference model} and the modified \textbf{adversarial-aware DPO loss}, (illustrated in Figure \ref{fig:main}). On one hand, the reference model is crucial to DPO, serving as a benchmark to guide the target model's output. However, traditional reference models are trained under benign conditions and lack robustness against adversarial perturbations, which can lead to misalignment when the model encounters malicious inputs. Therefore, we introduce an \textbf{adversarial-trained reference model}, which is explicitly optimized to generate human-preferred responses under adversarial conditions, ensuring that the target model is guided by a robust and reliable reference. On the other hand, we provide an  \textbf{adversarial-aware DPO loss} that directly incorporates the min-max optimization framework into the DPO training procedure. Traditional DPO focuses on aligning the model with human preferences under normal conditions but does not account for adversarial perturbations. In our formulation, the objective is to optimize the probability of generating human preferred responses ($Y_{pre})$ while simultaneously accounting for worst-case adversarial perturbations. 



%By incorporating worst-case adversarial perturbations during safety alignment, \textit{ADPO} not only aligns VLMs with human values but also ensures robustness and reliability against sophisticated adversarial manipulations. 
Our contribution can be summarized as:
\begin{itemize}[leftmargin=15pt, itemsep=2pt, parsep=0pt, partopsep=0pt, topsep=0pt] 
\item We propose \textit{ADPO}, a novel framework to achieve safety alignment under adversarial scenario for Vision-Language Models (VLMs). To the best of our knowledge, this is the first work to integrate adversarial training into the safety alignment of VLMs.

\item \textit{ADPO} achieves the robust safety alignment through adversarially trained reference model and the adversarial-aware DPO loss, with adversarial perturbation on both image space and latent space to achieve a broader safety alignment against various jailbreak attacks.

%This objective is achieved through the adversarially trained reference model and the adversarial-aware DPO loss.

%\item To the best of our knowledge, this is the first work to integrate adversarial training into the safety alignment of VLMs.

%\item We implement adversarial training by conducting adversarial perturbation on both image and latent space.

\item Extensive experiments demonstrate that \textit{ADPO} outperforms existing safety fine-tuning, achieving a lowest ASR against almost all jailbreak attacks and preserving the utility on normal tasks. Ablation studies also reveal the contribution of each component of \textit{ADPO}. 


\end{itemize}



