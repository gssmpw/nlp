\section{Related Work}
Defending against backdoor attacks in FL is a critical area of research, with various strategies proposed to enhance the robustness of FL. These defence mechanisms can be broadly categorized into robust aggregation methods, clustering-based defences, and knowledge distillation approaches.

\textbf{Robust Aggregation Methods} aim to mitigate the influence of malicious updates during the model aggregation process. The Robust Learning Rate (RLR) method ____ adjusts the learning rates of clients based on the alignment of their updates with the global model's direction. By assigning smaller learning rates to updates that deviate significantly, RLR reduces the impact of potentially malicious updates. However, RLR may not fully address attacks that manipulate the magnitude of updates, such as scaling attacks, and its effectiveness diminishes in heterogeneous (Non-IID) data environments where benign updates naturally vary in direction and magnitude. 

FoolsGold ____ is designed to counter backdoor attacks by analyzing the similarity of gradient updates among clients. It assigns lower aggregation weights to clients whose updates are overly similar, under the assumption that malicious clients will produce highly similar gradients due to coordinated attacks. While effective against certain types of attacks, FoolsGold may inadvertently penalize benign clients with similar data distributions, leading to unfairness and potential degradation of overall model performance.

\textbf{Clustering-Based Defences.} Prior work has explored clustering-based techniques to distinguish between benign and malicious updates in federated learning by grouping similar updates and flagging outliers. For example, FLAME ____ leverages HDBSCAN for clustering and introduces noise to enhance security; however, its adaptability to evolving threat landscapes remains limited. Similarly, RFCL ____ employs HDBSCAN to cluster client updates for defending against gradient poisoning attacks, yet its effectiveness diminishes against adaptive backdoor attacks ____. A critical limitation of these methods lies in their reliance on the precision of the clustering process, which is often compromised in high-dimensional parameter spaces typical of deep learning models. In such spaces, traditional clustering algorithms can struggle due to the curse of dimensionality, leading to inefficient clustering and the potential misclassification of benign updates as malicious, or vice versa.

In contrast, our approach mitigates these issues by first computing cosine similarity scores between client updates and the global model. These scalar similarity values capture the directional alignment of updates while reducing the data from a high-dimensional space to a one-dimensional representation, thereby simplifying the clustering task. We then apply HDBSCAN to these cosine similarity scores, which significantly enhances clustering efficiency and accuracy. 

\textbf{Knowledge Distillation Approaches} have been recognized for their ability to improve learning efficiency and address Non-IID data distributions in a federated framework. Methods such as FedDF ____ and FedBE ____ leverage knowledge distillation to aggregate models. FedDF performs model fusion by distilling knowledge from client models into a global model using unlabeled public data, effectively aligning the models’ outputs. FedBE builds upon this by employing Bayesian ensemble techniques to instruct a student model based on the ensemble’s predictive distribution.

FedRAD ____ enhances FedDF by assigning weights to client models according to their median scores, which measure how often a model's prediction corresponds to the median prediction among all clients. While these methods improve performance in Non-IID settings, they are vulnerable to backdoor attacks when transferring knowledge from ensemble models without analyzing outliers. Malicious clients can introduce backdoors into their models, and without mechanisms to detect and exclude these compromised models, the backdoor triggers can be propagated to the global model during distillation.

Our proposed approach, \textbf{RKD}, builds upon existing methods by innovatively integrating and enhancing foundational techniques such as HDBSCAN and knowledge distillation. This approach addresses the limitations of prior works, particularly in defending against recent backdoor attacks in Non-IID data.

First, by integrating cosine similarity with HDBSCAN, RKD improves clustering efficiency in high-dimensional parameter spaces, effectively identifying malicious updates even when they are subtle or adaptive. Focusing on the angular relationships between model updates captures essential differences without being overwhelmed by the volume of parameters, thereby ensuring accurate clustering despite Non-IID data.

Second, the RKD framework incorporates a robust model selection process that selects models near the median of the cluster. By identifying and using the most representative models, RKD further mitigates the influence of any remaining malicious updates. This selection forms a reliable and trustworthy ensemble for knowledge distillation.

Third, RKD ensures secure knowledge transfer by distilling from the carefully selected ensemble models, preventing the propagation of backdoor triggers to the global model. By excluding outlier models identified during clustering, RKD reduces the risk of incorporating malicious behaviours into the model. Additionally, knowledge distillation aids in smoothing out variations caused by Non-IID data, leading to a more generalized and robust global model.

By addressing the challenges of high-dimensional data, improving clustering efficiency, and ensuring secure knowledge transfer, RKD provides a robust defence against backdoor attacks in federated learning while effectively handling Non-IID data distributions.