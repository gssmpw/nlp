\section{Related Work}
There has been a significant amount of work in accelerating LLMs. Some methods focus on reducing the number of parameters, such as low-bit quantization \cite{dettmers2022llmint88bitmatrixmultiplication, frantar2023gptqaccurateposttrainingquantization, DBLP:conf/icml/XiaoLSWDH23, DBLP:conf/mlsys/0002TTYCWXDG024}, and model distillation \cite{gu2024minillm, DBLP:conf/icml/KoKCY24, DBLP:conf/acl/Zhong00L0T24}. Recently, some studies have also explored activating only a subset of model parameters during inference to reduce memory access cost \cite{DBLP:conf/icml/DuHDTLXKZYFZFBZ22, DBLP:journals/jmlr/FedusZS22}. Speculative decoding \cite{chen2023acceleratinglargelanguagemodel, DBLP:conf/icml/LeviathanKM23} leverages the memory-bound nature of decoder-only LLMs and achieves lossless acceleration using a drafting-verification framework. 

\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{0.0cm}
\includegraphics[width=7.5cm]{img/alpha.pdf}
\caption{Acceptance rates in MT-bench dataset. Here n-$\alpha$ denotes the acceptance rate of the n-th token.}
\label{fig:coral}
\end{figure}

Research on speculative decoding has primarily focused on two areas: 1) drafter design, 2) verification strategy. For drafter design, Medusa \cite{DBLP:conf/icml/CaiLGPLCD24} attaches multiple heads to the original LLM and predict multiple subsequent tokens one time. Hydra \cite{ankner2024hydrasequentiallydependentdraftheads} improves Medusa by enhancing correlations between draft heads. Clover \cite{DBLP:journals/corr/abs-2405-00263} introduces an RNN-based draft head. Some methods utilize more information from target model to improve alignment, EAGLE \cite{li2024eagle} combines the output token and last hidden states of target LLMs to resolve the uncertainty in drafter's prediction. GLIDE \cite{DBLP:conf/icml/Du0XWY0LXNTY24} reuses the KV cache of target LLMs. For the verification strategy,  \citet{hu2024accelerated, sun2024blockverificationacceleratesspeculative} find that the acceptance length of speculative sampling is not optimal and take into account the probability of subsequent tokens. SpecInfer \cite{DBLP:conf/asplos/MiaoOZCWZWZYSSC24} proposes decoding tree for verification. Sequoia \cite{chen2024sequoiascalablerobusthardwareaware}, EAGLE-2 \cite{li2024eagle2}, and OPT-tree \cite{wang2024opttreespeculativedecodingadaptive} adopts a dynamic tree structure.