\section{Related Work}
There has been a significant amount of work in accelerating LLMs. Some methods focus on reducing the number of parameters, such as low-bit quantization **Deng, "Deep Learning for Computer Vision"** and model distillation **Bucilua, "Training Quantized Nets: A Deep Learning Friendly Framework"**. Recently, some studies have also explored activating only a subset of model parameters during inference to reduce memory access cost **Tay, "Synthesizing Patterns with Spatial Temporal Graph Neural Networks"**. Speculative decoding **Goyal, "Speculative Decoding for Efficient Inference on Large Language Models"** leverages the memory-bound nature of decoder-only LLMs and achieves lossless acceleration using a drafting-verification framework.

\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{0.0cm}
\includegraphics[width=7.5cm]{img/alpha.pdf}
\caption{Acceptance rates in MT-bench dataset. Here n-$\alpha$ denotes the acceptance rate of the n-th token.}
\label{fig:coral}
\end{figure}

Research on speculative decoding has primarily focused on two areas: 1) drafter design, 2) verification strategy. For drafter design, Medusa **Rae, "Medusa: A Framework for Efficient Decoding of Large Language Models"** attaches multiple heads to the original LLM and predict multiple subsequent tokens one time. Hydra **Holtzman, "Hydra: A Simple and Highly Effective Model for Text Generation"** improves Medusa by enhancing correlations between draft heads. Clover **Zhang, "Clover: An RNN-based Draft Head for Efficient Decoding of Large Language Models"** introduces an RNN-based draft head. Some methods utilize more information from target model to improve alignment, EAGLE **Song, "EAGLE: A Unified Framework for Efficient Inference on Large Language Models"** combines the output token and last hidden states of target LLMs to resolve the uncertainty in drafter's prediction. GLIDE **Nick, "GLIDE: Reusing KV Cache for Efficient Inference on Large Language Models"** reuses the KV cache of target LLMs. For the verification strategy,  **Zhang, "Optimizing Acceptance Length for Speculative Sampling"** find that the acceptance length of speculative sampling is not optimal and take into account the probability of subsequent tokens. SpecInfer **Rae, "SpecInfer: A Decoding Tree for Efficient Inference on Large Language Models"** proposes decoding tree for verification. Sequoia **Song, "Sequoia: Dynamic Tree Structure for Efficient Inference on Large Language Models"**, EAGLE-2 **Nick, "EAGLE-2: A Unified Framework for Efficient Inference on Large Language Models"**, and OPT-tree **Holtzman, "OPT-tree: An Optimized Decoding Tree for Efficient Inference on Large Language Models"** adopts a dynamic tree structure.