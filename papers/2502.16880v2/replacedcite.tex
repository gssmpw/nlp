\section{Related Work}
There has been a significant amount of work in accelerating LLMs. Some methods focus on reducing the number of parameters, such as low-bit quantization ____, and model distillation ____. Recently, some studies have also explored activating only a subset of model parameters during inference to reduce memory access cost ____. Speculative decoding ____ leverages the memory-bound nature of decoder-only LLMs and achieves lossless acceleration using a drafting-verification framework. 

\begin{figure}[t]
\centering
\setlength{\abovecaptionskip}{0.0cm}
\includegraphics[width=7.5cm]{img/alpha.pdf}
\caption{Acceptance rates in MT-bench dataset. Here n-$\alpha$ denotes the acceptance rate of the n-th token.}
\label{fig:coral}
\end{figure}

Research on speculative decoding has primarily focused on two areas: 1) drafter design, 2) verification strategy. For drafter design, Medusa ____ attaches multiple heads to the original LLM and predict multiple subsequent tokens one time. Hydra ____ improves Medusa by enhancing correlations between draft heads. Clover ____ introduces an RNN-based draft head. Some methods utilize more information from target model to improve alignment, EAGLE ____ combines the output token and last hidden states of target LLMs to resolve the uncertainty in drafter's prediction. GLIDE ____ reuses the KV cache of target LLMs. For the verification strategy,  ____ find that the acceptance length of speculative sampling is not optimal and take into account the probability of subsequent tokens. SpecInfer ____ proposes decoding tree for verification. Sequoia ____, EAGLE-2 ____, and OPT-tree ____ adopts a dynamic tree structure.