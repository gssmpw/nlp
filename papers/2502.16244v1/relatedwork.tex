\section{Related Work}
\label{sec:relatedwork}
Starting with Graded Modal Logic  (\cite{DBLP:journals/ndjfl/Fine72}),
there are numerous logics that capture modal aspects of graphs and express arithmetic constraints, (\cite{
DBLP:journals/japll/DemriL10,
% DBLP:conf/frocos/Baader17,
DBLP:conf/frocos/BaaderB19,
% DBLP:conf/ekaw/GallianiRKPT20,
DBLP:conf/fsttcs/BednarczykOPT21,
DBLP:conf/kr/GallianiKT23,
DBLP:journals/bsl/BenthemI23}). 
% At the difference of $\thelogic{}$ which is parameterized with a fixed-width arithmetic, all existing works operate over all integers or rational numbers.

Previous research has already established several correspondences between logic and GNNs. For instance, \citet{DBLP:conf/iclr/BarceloKM0RS20} explored the relationship between graded modal logic and GNNs, while \citet{ijcai2024} examined modal logic on over linear inequalities with counting and its connection to GNNs. Additionally, \citet{benedikt2024decidability} investigated fragments of Presburger logic in the context of GNNs. However, these existing works focus on GNNs with specific activation functions and do not consider the broader class of quantized GNNs. In particular, decidability in PSPACE has been established only for cases where the activation function is either a truncated ReLU (\cite{ijcai2024}) or eventually constant functions (\cite{benedikt2024decidability}). 
More loosely related, is the work of \citet{Grohe23}, which establishes that the graph queries computable by a polynomial-size, bounded-depth family of GNNs are precisely those definable in the guarded fragment  \text{GFO+C}  of first-order logic with counting and built-in relations. This finding situates GNNs within the circuit complexity class  $\text{TC}^0$.

%\citet{HenzingerLZ21} also addresses the verification of quantized neural networks, in particular feedforward neural networks. The authors claim to establish a PSPACE-hardness result. However, the precise setting of 
%representation sizes they assume is not explicitly clarified and is likely to be logarithmic, which presumably causes the hardness result. In other cases, such as when using a unary representation as considered in 
%this paper, the problem is presumably in NP. But, a rigorous proof or detailed analysis of the sometimes vague arguments used in \citet{HenzingerLZ21} is left as future work. 
%Thank you! François likes BUT "rigorous proof" etc. is a bit attacking their paper

\citet{HenzingerLZ21} also addresses the verification of quantized neural networks, but in contrast to this paper they focus on FNN. The authors establish a PSPACE-hardness result, relying on a binary representation of the number of bits. In contrast, if the number of bits were unary, their problem is presumably in NP (using a guess and check argument, relying on similar arguments as used by \cite{SalzerL21}).
%Marco : put your paper in XXXXXXXXX where 
%However, their lower bound is so because the number of bits seems to be written in binary and not in unary as our paper. Their PSPACE-hardnesin their setting, the precise setting of 
% This highlights the greater complexity of verifying GNNs compared to FNNs.
% This underscores that verifying GNNs is fundamentally more complex than verifying `standard’ neural networks. and necessitates specialized techniques.

There are other aggregation functions, like weighted sums in graph attention networks \cite{DBLP:conf/iclr/VelickovicCCRLB18}, maximum in Max-GNNs \cite{DBLP:conf/kr/CucalaG24}, or average. \thelogic{} and the tableau method can be adapted to capture these aggregation functions, too (see Supplementary Material, Section~\ref{appendix-section-aggregation}). 
% \todo{maybe say explicity that our work is not a consequence of their work}

On the practical side, \citet{DBLP:conf/aaai/000200DWZB24} presents a solution to the verification of quantized FNNs using heuristic search, and outperforming the approach of \citet{10.1145/3551349.3556916} based purely on integer linear programming.