\section{Related Work}
\label{sec:relatedwork}
Starting with Graded Modal Logic (Faggian, "Graduality and Modality in First-Order Modal Logic"),
there are numerous logics that capture modal aspects of graphs and express arithmetic constraints, (Müller, "Modal logic over finite trees").
 
Previous research has already established several correspondences between logic and GNNs. For instance, Kufleitner explored the relationship between graded modal logic and GNNs, while Ghilardi examined modal logic on over linear inequalities with counting and its connection to GNNs. Additionally, Fichte investigated fragments of Presburger logic in the context of GNNs. However, these existing works focus on GNNs with specific activation functions and do not consider the broader class of quantized GNNs. In particular, decidability in PSPACE has been established only for cases where the activation function is either a truncated ReLU (Ben-David, "Characterizing Approximability of Optimization Problems") or eventually constant functions (Dell'Erba, "A note on PSPACE-completeness").
More loosely related, is the work of Demri, which establishes that the graph queries computable by a polynomial-size, bounded-depth family of GNNs are precisely those definable in the guarded fragment  \text{GFO+C}  of first-order logic with counting and built-in relations. This finding situates GNNs within the circuit complexity class  $\text{TC}^0$.

%____ also addresses the verification of quantized neural networks, in particular feedforward neural networks. The authors claim to establish a PSPACE-hardness result. However, the precise setting of 
%representation sizes they assume is not explicitly clarified and is likely to be logarithmic, which presumably causes the hardness result. In other cases, such as when using a unary representation as considered in 
%this paper, the problem is presumably in NP. But, a rigorous proof or detailed analysis of the sometimes vague arguments used in ____ is left as future work. 
%Thank you! François likes BUT "rigorous proof" etc. is a bit attacking their paper

Bun, also addresses the verification of quantized neural networks, but in contrast to this paper they focus on FNN. The authors establish a PSPACE-hardness result, relying on a binary representation of the number of bits. In contrast, if the number of bits were unary, their problem is presumably in NP (using a guess and check argument, relying on similar arguments as used by Kabanets).
%Marco : put your paper in XXXXXXXXX where 
%However, their lower bound is so because the number of bits seems to be written in binary and not in unary as our paper. Their PSPACE-hardnesin their setting, the precise setting of 
% This highlights the greater complexity of verifying GNNs compared to FNNs.
% This underscores that verifying GNNs is fundamentally more complex than verifying `standard’ neural networks. and necessitates specialized techniques.

There are other aggregation functions, like weighted sums in graph attention networks (Velickovic, "Graph Attention Networks"), maximum in Max-GNNs (Li, "Max-Margin Neural Graph Learning"), or average. \thelogic{} and the tableau method can be adapted to capture these aggregation functions, too (see Supplementary Material, Section~\ref{appendix-section-aggregation}). 
% \todo{maybe say explicity that our work is not a consequence of their work}

On the practical side, Bun presents a solution to the verification of quantized FNNs using heuristic search, and outperforming the approach of Chakrabarty based purely on integer linear programming.