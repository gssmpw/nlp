\section{Related Works}
\subsection{Large Reasoning Models}

The initial LLMs relied on autoregressive sequence prediction, showcasing remarkable text generation abilities. With growing demands for productivity and precision, researchers started to investigate whether models could think and reason in a human-like manner. The proposal of chain-of-thought \citep{cot} marked a significant advancement, prompting researchers to focus more intently on the reasoning capabilities of language models. TS-LLM \citep{treesearch} represents the first proposed AlphaZero-like tree search learning framework and signifies a evolution of reasoning structures from linear chains to hierarchical branching trees. Subsequently, graph-based reasoning structures and more complex nested structures, exemplified by Llama-Berry \citep{llama-berry}, have been extensively explored. Reasoning strategies such as MCTS \citep{mcts}, beam search \citep{beamsearch}, and ensemble methods \citep{selfconsistency, forestofthought} have also been proposed. To date, several LRMs with advanced reasoning capabilities have been developed in the industry, including OpenAI's o-series, Google's Gemini-thinking, and DeepSeek's R1. This exemplifies the integration of three pivotal elements, the advancement of LLMs, the design of reinforcement learning (RL), and high-performance computing (HPC) \citep{llrmblueprint}. ActorAttack, which performs jailbreak attacks through multi-round dialogue, targeted the o1 model after confirming its effectiveness against poor-reasoning LLMs. They assert that the o1 model shows higher safety than GPT-4o \citep{safemtdata}. Unfortunately, their research did not focus on the reasoning model's capabilities nor did it further investigate the jailbreak attack on LRMs.

\subsection{Jailbreak Attacks}

Existing LLMs jailbreak attacks can be divided into black-box and white-box methods according to the parameter accessibility of the target models \citep{jailbreaksurvey, safetysurvey}. White-box attack methods, such as gradient-based methods represented by GCG \citep{advbench}, logits-based methods represented by COLD \citep{cold}, and fine-tuning-based methods, are shown to be effective. However, these methods necessitate access to the target models, making them impractical. Primarily relying on queries as main mechanism, Black-box attack methods feature template completion methods such as scenario nesting \citep{deepinception, wolf}, context-based attacks \citep{ica, mjp}, and code injection \citep{programmatic, codechameleon}, in addition to prompts rewriting methods including ciphers \citep{cipher, artprompt}, multi-languages \citep{multilingual, lowresource}, and genetic algorithms \citep{autodan}.

Black-box methods can be divided into one-to-one mappings (uniquely recoverable according to the rules) and one-to-many mappings (not uniquely recoverable) according to the nature of the mapping. Among them, character encryption \citep{cipher} and word replacement \citep{wordsubstitution} belong to the former, while persuasive adversarial prompts \citep{pap} belongs to the latter. Such methods have diminished in power when applied to the latest versions of large models, especially those with advanced reasoning capabilities. We define these one-to-one mappings (also known as injections) as reasoning steps, and construct reasoning chains iteratively to subsequently attack LRMs.