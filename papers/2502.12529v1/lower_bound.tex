%!TEX root=main.tex
\section{Alternating Regret Lower Bounds}\label{sec: lower_bound}

While our work significantly advances our understanding on alternating regret upper bounds, 
there are no existing alternating regret lower bounds at all.
Towards closing this gap, we make an initial attempt by considering the special case of the expert problem (OLO over simplex $\Delta_d$) and providing two algorithm-specific lower bounds.
In the first result, 
echoing \pref{thm:hedge_simplex}, 
we show that the worst-case alternating regret of Hedge is $\Omega(T^{\frac{1}{3}})$, 
thereby giving a tight characterization of Hedge's alternating regret (at least for the case with a fixed learning rate).

\begin{theorem}\label{thm:hedge_lower_bound}
   For $d\ge 3$ and any $\eta>0$, there exists a loss sequence $\{\ell_t\}_{t=1}^T$ where $\ell_t\in\sbr{0,1}^d$ for all $t\in[T]$, such that Hedge suffers $\RegAlt=
    \Omega(T^{\frac{1}{3}})$.
\end{theorem}
The full proof is deferred to \pref{app:lower_bound}. To provide a sketch, we consider two environments for $d=3$. In the first environment, the loss vectors are cycling among the three one-hot loss vectors, i.e. $\ell_{3k-2}=(1,0,0),\ell_{3k-1}=(0,1,0),\ell_{3k}=(0,0,1)$ for $k=1,2,\dots,\frac{T}{3}$. We then calculate the closed form of $p_t$, which also turns out to cycle among three vectors. Direct calculation then shows that Hedge achieves an $\Omega(\eta^2 T)$ alternating regret. In the second environment, the loss vectors are all $(1,0,0)$. Via a direct calculation, we show that the alternating regret is $\Omega(\frac{1}{\eta})$. Combining both cases proves 
that the worst case alternating regret is $\Omega(\max\{\eta^2 T, \frac{1}{\eta}\}) = \Omega(T^{\frac{1}{3}})$.

We then move on to another widely-used algorithm called Predictive Regret Matching$^+$ (PRM$^+$) \citep{farina2021faster}. \citet{cai2023last} show that, unlike its other variants, \PRM coupled with alternation has empirically shown fast last-iterate convergence in two-player zero-sum games. However, the reason behind this is not well known. %
One may wonder whether the fast convergence rate of \PRM is due to small alternating regret in the adversarial setting. Unfortunately, we show in \pref{thm: PRM+} that this is not the case: in fact, in the adversarial setting, \PRM achieves $\Omega(\sqrt{T})$ alternating regret in the worst case, which is even worse than Hedge.

\begin{theorem}\label{thm: PRM+}
    There exists a loss sequence $\{\ell_t\}_{t=1}^T$ where $\ell_t\in\sbr{-1,1}^d$ for all $t\in[T]$, such that $\RegAlt=
    \Omega(\sqrt{T})$ for \PRM.
\end{theorem}
The proof is deferred to \pref{app:lower_bound}. The loss sequence we create is alternating between $(1,0)$ and $(-\frac{1}{2},0)$. Then, we show that the prediction of \PRM converges to the optimal decision $(0,1)$ at a rate of $1/\sqrt{T}$ approximately, leading to an $\Omega(\sqrt{T})$ alternating regret. 
The implication of this result is that, 
to demystify the practical success of \PRM,
one must take the game structure into account.
