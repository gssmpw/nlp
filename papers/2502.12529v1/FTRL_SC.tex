\section{Another Algorithm for Smooth and Self-Concordant Losses}\label{sec:SC}

One issue with continuous Hedge is that its implementation requires a log concave sampler (e.g.,~\citet{lovasz2003geometry, lovasz2006simulated, narayanan2010random}), which can be expensive when the dimension $d$ is high (even though the time complexity is polynomial in $d$).
Another issue is that its alternating regret bound has dimension dependence even when $\calX$ is an $\ell_2$ ball where it is well-known that $d$-independent standard regret bound is possible.
To mitigate these issues, in this section, we propose yet another general algorithm and instantiate it for different decision spaces.
Our result here, however, only applies to a restricted class of convex losses.

Specifically, we start by providing some basic definitions on functions (throughout this section, all norms considered are $\ell_2$-norm).
\begin{definition}\label{def:lipschitz}
    We say that  a differentiable function $f:\calX\mapsto\R$ is $L$-Lipschitz if for any $x\in\calX$, $\|\nabla f(x)\|_2 \leq L$,
    $\beta$-smooth if for any $x,y\in\calX$, $D_f(x,y)\leq \frac{\beta}{2}\|x-y\|_2^2$,
     $\sigma$-strongly convex if for any $x,y\in\calX$, $D_f(x,y)\geq \frac{\sigma}{2}\|x-y\|_2^2$.
\end{definition}

\begin{definition}[3rd-order smoothness]\label{def:3rd-order smoothness}
A function $\psi:\calX\mapsto\R$ is said to be \emph{$M$-smooth of order 3} if it is three times differentiable, and its third-order derivative at any $w\in\calX$ satisfies
$
|\nabla^3 \psi(w)[h, h, h]|
\leq M,~ \forall h \in \mathbb{R}^d, \|h\|_2=1.
$
\end{definition}

\begin{definition}\label{def:Legendre}%\citep{lattimore2020bandit}
    A convex function $f:\calX\mapsto \R$ for a convex body $\calX$ is Legendre if it is differentiable and strictly convex in $\interior(\calX)$ and also 
    $\norm{\nabla f(x)}_2\rightarrow \infty$ as $x\rightarrow\partial\calX$ (the boundary of $\calX$).    
\end{definition}

\begin{definition}[Self-concordance]\label{def:self-concordance}\citep{nesterov1994interior}
    A function $f:\calX\mapsto \fR$ is said to be $C$-self-concordant for $C\ge0$, if it is three times differentiable, and for all $x\in\calX$ and all $h\in\fR^d$, the following inequality holds: $
    \abs{\nabla^3f(x)[h,h,h]}\le2C\rbr{\nabla^2f(x)[h,h]}^{3/2}
    $.
\end{definition}

Our algorithm is the classical and efficient Follow-the-Regularized-Leader algorithm that, with a learning rate $\eta>0$ and a regularizer $\psi: \calX \mapsto \fR$,
plays 
$
x_{t}=\argmin_{x\in \mathcal{X}}\sum_{\tau=1}^{t-1} f_{\tau}(x)+\frac{1}{\eta}\psi(x)
$
in round $t$;
see \pref{alg:OCO}.
However, there are two important elements:
first, we apply FTRL directly to the original loss functions $f_\tau$ instead of its linearized version $x\mapsto \inner{\nabla f_\tau(x_\tau), x}$, in light of the issue discussed in \pref{sec:adv-oco};
second, it is critical that our regularizer $\psi$ satisfy the following conditions.

\begin{algorithm}[tb]
   \caption{FTRL for minimizing alternating regret}
   \label{alg:OCO}

   {\bfseries Input:} learning rate $\eta>0$.
   
   {\bfseries Input:} a regularizer $\psi$ satisfying \pref{asp: psi}.
   
   \For{$t=1$ {\bfseries to} $T$} {
   
   Play
   $
   x_{t}=\argmin_{x\in \mathcal{X}}\sum_{\tau=1}^{t-1} f_{\tau}(x)+\frac{1}{\eta}\psi(x)
   $
   and observe $f_t$.
   }

\end{algorithm}

\begin{assumption}\label{asp: psi}
    The regularizer $\psi$ is Legendre and $\sigma$-strongly convex  within domain $\calX$.
    Moreover, its convex conjugate $\psi^*(w) = \sup_{x\in\calX}\inner{w,x}-\psi(x)$ is $M$-smooth of order $3$. 
\end{assumption}

We will analyze \pref{alg:OCO} under the following assumption on the loss functions.
\begin{assumption}\label{asp: function}
    For all $t\in[T]$, the loss function $f_t$ is $L$-Lipschitz , $\beta$-smooth and $C$-self-concordant.
\end{assumption}

For example, linear and convex quadratic loss functions satisfy \pref{asp: function} with $C=0$. 
For quadratic functions in the form of $x^\T Ax+bx+c$, 
we note that applying Online Gradient Descent 
achieves $\order\rbr{\frac{d}{\alpha}\log T}$ standard regret~\citep{hazan2007logarithmic}, where $\alpha\propto\frac{\lambda_{\min}(A+A^\T)}{\lambda_{\max}(A+A^\T)}$ can be small if the matrix $A+A^\T$ is ill-conditioned,
while our alternating regret bound enjoys better dependence on $d$ and is independent of $\alpha$.
There are also examples where the quadratic loss is not even exp-concave 
(for example, when $A+A^\top$ has eigenvalue $0$ with a corresponding eigenvector $u$ such that  $b^\top u \neq 0$),
so Online Newton Step~\citep{hazan2007logarithmic} cannot apply, 
but our results still hold.
As mentioned, online learning with self-concordant losses has been considered in~\citet{zhang2017improved} under the context of dynamic regret.

While our algorithm appears to be standard and has been studied in the literature for standard regret (see e.g.,~\citet{orabona2019modern}),
analyzing its alternating regret requires new ideas based on the properties of the regularizer.
Specifically, we prove the following general alternating regret bound.
\begin{theorem}\label{thm: OCO}
    Under \pref{asp: function}, \pref{alg:OCO} guarantees the following bound on $\RegAlt(u)$ for any $u\in\calX$,
    \begin{equation}\label{eqn:OCO}
        \begin{aligned}
        &\mathcal{O}\rbr{\frac{\psi(u)-\psi(x_1)}{\eta}+ \frac{CL^3}{\sigma^{3/2}}\eta^{3/2}T+\rbr{ML^3+\frac{\beta L^2}{\sigma^2}}\eta^2T}.
        \end{aligned}
    \end{equation}
\end{theorem}

\begin{proof}
    Define $F_{t}(x)\triangleq\sum_{\tau=1}^{t-1}f_\tau(x)+\frac{1}{\eta}\psi(x)$, so that $x_{t}=\argmin_{x\in\calX}F_{t}(x)$.
    Note that
\begin{align*}
    -\sum_{t=1}^Tf_t(u) &= \frac{1}{\eta}\psi(u) - \frac{1}{\eta}\psi(x_1)+ F_{T+1}(x_{T+1}) - F_{T+1}(u) + \sum_{t=1}^T\left(F_t(x_t)-F_{t+1}(x_{t+1})\right)  \\
    &\leq \frac{\psi(u)-\psi(x_1)}{\eta} + \sum_{t=1}^T\left(F_t(x_t)-F_{t+1}(x_{t+1})\right),
\end{align*}
where the last inequality comes from $F_{T+1}(x_{T+1})\leq F_{T+1}(u)$ by the optimality of $x_{T+1}$.
Substituting it in the alternating regret definition, we get
\begin{align}
    \RegAlt(u) \leq  \frac{2\rbr{\psi(u)-\psi(x_1)}}{\eta} + \sum_{t=1}^T\rbr{2F_t(x_t)-2F_{t+1}(x_{t+1})+f_t(x_t)+f_t(x_{t+1})}. \label{eqn:RegAlt_bound}
\end{align}

Since $\psi$ is Legendre, we must have $\nabla F_t(x_t) = \mathbf{0}$ by the optimality of $x_t$, where $\mathbf{0}$ is the zero vector.
Further using the definition of Bregman divergence, we obtain
\begin{align*}
    &D_{F_{t+1}}(x_t,x_{t+1})
    = F_{t+1}(x_t) - F_{t+1}(x_{t+1})
    = F_t(x_t) + f_t(x_t) - F_{t+1}(x_{t+1})\\
    \text{and }&D_{F_{t}}(x_{t+1},x_{t})
    = F_t(x_{t+1}) - F_t(x_t) 
    = F_{t+1}(x_{t+1}) - f_t(x_{t+1}) - F_{t}(x_{t}).
\end{align*}
Therefore, the second term in the right-hand side of \pref{eqn:RegAlt_bound} can be written as
\begin{align*}
    &\sum_{t=1}^T\left(2F_t(x_t)-2F_{t+1}(x_{t+1})+f_t(x_t)+f_t(x_{t+1})\right)
    = \sum_{t=1}^T\left(D_{F_{t+1}}(x_t,x_{t+1}) - D_{F_t}(x_{t+1},x_t)\right) \\
    &=\sum_{t=1}^T\left(D_{f_t}(x_t,x_{t+1})+D_{F_{t}}(x_t,x_{t+1}) - D_{F_t}(x_{t+1},x_t)\right).
\end{align*}
Since $f_t$ is $\beta$-smooth, the first term above, $D_{f_t}(x_t,x_{t+1})$, is at most $\frac{\beta}{2}\norm{x_t-x_{t+1}}^2$,
which can be further bounded by $\frac{\beta\eta^2L^2}{2\sigma^2}$ due to a standard stability argument of FTRL (see \pref{lem:path_length}). For the second and third terms, we proceed as
\begin{align*}
    &D_{F_t}(x_t,x_{t+1}) - D_{F_t}(x_{t+1},x_t) 
    =  D_{{F}_t^*}(\nabla {F}_t(x_{t+1}),\nabla {F}_t(x_{t}))- D_{{F}_t^*}(\nabla {F}_t(x_{t}),\nabla {F}_t(x_{t+1})) \\
    &=  D_{{F}_t^*}(\nabla {F}_t(x_{t+1}),\mathbf{0}) - D_{{F}_t^*}(\mathbf{0},\nabla {F}_t(x_{t+1})) \tag{since $\nabla {F}_t(x_{t}) = \mathbf{0}$} \\
    &\leq \frac{1}{6}\rbr{\frac{2C\eta^{3/2}}{\sigma^{3/2}}+M\eta^2}\norm{\nabla {F}_t(x_{t+1})}_2^3 ~%
    \le \order\rbr{\rbr{\frac{C\eta^{3/2}}{\sigma^{3/2}}+M\eta^2} L^3}.
\end{align*}
Here, the first equality uses \pref{lem:conj_dual}, a basic property of Bregman divergence of convex conjugate;
the first inequality uses the fact that ${F}_t^*$ is $\rbr{\frac{2C\eta^{3/2}}{\sigma^{3/2}}+M\eta^2}$-smooth of order $3$ (see the technical \pref{lem:3rd-order-smooth-F}), the place where we require the self-concordance of the loss functions and the 3rd-order smoothness of $\psi^*$,
along with the aforementioned \pref{lem:bregman_commutator} taken from~\citet{wibisono2022alternating};
the last inequality comes from $\nabla F_t(x_{t+1})=\nabla {F}_{t+1}(x_{t+1}) - \nabla f_{t}(x_{t+1})=- \nabla f_{t}(x_{t+1})$ and the Lipschitzness of $f_{t}$.
Plugging all bounds into \pref{eqn:RegAlt_bound} finishes the proof.
% 
\end{proof}

\begin{lemma}[Lemma A.2 of \citet{wibisono2022alternating}]\label{lem:bregman_commutator}
    Assuming $\psi^*$ is $M$-smooth of order $3$ with respect to norm $\|\cdot\|_2$, we have for all $w,w'\in\fR^d$,
    $\abs{D_{\psi^*}(w,w')-D_{\psi^*}(w',w)} \le \frac{M}{6}\norm{w-w'}_2^3$.
    %
\end{lemma}

Focusing only on the dependence on $T$ and $C$, we see that \pref{thm: OCO} gives an alternating regret bound of $\otil(\frac{1}{\eta}+(C\eta^{3/2}+\eta^2) T)$, which is $\otil((CT)^{\frac{2}{5}}+T^{\frac{1}{3}})$ with the optimal tuning of $\eta$. When $C=0$, e.g., for linear and convex quadratic loss functions, we thus also have a bound of $\otil(T^{\frac{1}{3}})$;
otherwise, our bound is of order $\otil(T^{\frac{2}{5}})$. To handle the dependence on all other parameters,
we provide concrete instantiation of $\psi$ in the rest of this section.

\subsection{Entropic Barrier Regularizer for General $\calX$}\label{sec:entopic_barrier}
In this section, we consider a general compact and convex body $\calX$ with a bounded diameter $D$.
Our observation is that the \emph{entropic barrier} of $\calX$, proposed by ~\citet{bubeck2014entropic}, is a valid regularizer for our algorithm (see \pref{app: alt-oco} for the proof).

\begin{lemma}\label{lem:entropic}
The convex conjugate of the entropic barrier for $\calX$,
$\psi^*(\theta) = \log\rbr{\int_{x\in\calX}\exp\rbr{\innerp{\theta}{x}}dx}$, satisfies \pref{asp: psi} with parameters $\sigma = \frac{1}{D^2}$ and $M=D^3$.
\end{lemma}

\begin{corollary}\label{cor:entr_regularizer}
    Let $\calX$ be a compact convex body with diameter $D$. Under \pref{asp: function}, \pref{alg:OCO} with $\eta=\min\cbr{(d\ln T)^{\frac{2}{5}}(CL^3D^3)^{-\frac{2}{5}}T^{-\frac{2}{5}},(d\ln T)^{\frac{1}{3}}(L^3D^3+\beta L^2D^2)^{-\frac{1}{3}}T^{-\frac{1}{3}}}$ and $\psi$ being the entropic barrier for $\calX$ guarantees
    \begin{equation*}
    \RegAlt\le\order\rbr{\max\cbr{(d\ln T)^{\frac{3}{5}}(CL^3D^3)^{\frac{2}{5}}T^{\frac{2}{5}},(d\ln T)^{\frac{2}{3}}(L^3D^3+\beta L^2D^2)^{\frac{1}{3}}T^{\frac{1}{3}}}}.
    \end{equation*}
\end{corollary}

\begin{proof}
    For any fixed $u\in\calX$, we decompose $\RegAlt(u)$ into 
    \begin{equation}\label{eqn:reg_decompose}
        \RegAlt(u')+\sbr{2\sum_{t=1}^T\rbr{f_t(u')-f_t(u)}},
    \end{equation}
    where $u'=(1-\epsilon)u+\epsilon x_1\in\calX$ for $\epsilon=\frac{1}{T}$ and $x_1 = \argmin_{x\in\calX}\frac{\psi(x)}{\eta}$. Since $f_t$ is convex, $f_t(u')-f_t(u) \le \frac{1}{T}\rbr{f_t(x_1)-f_t(u)}\le \frac{LD}{T}$ for all $t\in[T]$. Hence, the second term above can be bounded by $2LD$.
    For the first term $\RegAlt(u')$, we apply \pref{thm: OCO} together with \pref{lem:entropic} to bound it as
    \begin{equation}\label{eqn:reg_u'_bound}
         \order\rbr{\frac{\psi(u')-\psi(x_1)}{\eta}+CL^3D^3\eta^{3/2}T+\rbr{L^3D^3+\beta L^2D^2}\eta^2 T}.
    \end{equation}
Finally, since $\psi$ is an $\order(d)$ self-concordant barrier of $\calX$~\citep{bubeck2014entropic,chewi2023entropic}, we have $\psi(u')-\psi(x_1)= \order(d\ln T)$ (see \pref{lem:self_concordant}).
Plugging in the value of $\eta$ then finishes the proof.
\end{proof}
%
Again, for loss functions with self-concordance parameter $C=0$ (e.g., linear or convex quadratic losses), we achieve  $\otil(d^\frac{2}{3}T^\frac{1}{3})$ alternating regret, matching the performance of continuous Hedge.
In fact, for linear losses, the two algorithms are simply equivalent~\citep{bubeck2014entropic}, but this equivalence does not extend to general  convex losses.

\subsection{The $\ell_2$-Ball Case}\label{sec:ball}


Next, we consider the case where the feasible domain $\calX=\calB_2^d(1)$ and pick $\psi(x) = -\ln(1-\norm{x}_2^2)$. 
The following lemma (proof in \pref{app: alt-oco}) shows that $\psi$ satisfies \pref{asp: psi}.

\begin{lemma}\label{lem:smoothness of ball case}
    The convex conjugate of $\psi$ is   defined as
    $\psi^*(w) = \sqrt{1+\norm{w}_2^2} - \ln(1+\sqrt{1+\norm{w}_2^2})$,
    and it satisfies \pref{asp: psi} with parameters $\sigma = 2$ and $M=4$.
\end{lemma}
%
Thus, we get the following dimension independent regret bound from \pref{thm: OCO}.
%
\begin{corollary}\label{cor:ball}
    Let $\calX=\calB_2^d(1)$. Under \pref{asp: function}, \pref{alg:OCO} with regularizer $\psi(x) = -\ln(1-\norm{x}_2^2)$ and $\eta=\min\cbr{(\ln T)^{\frac{2}{5}}(CL^3)^{-\frac{2}{5}}T^{-\frac{2}{5}},(\ln T)^{\frac{1}{3}}(L^3+\beta L^2)^{-\frac{1}{3}}T^{-\frac{1}{3}}}$ guarantees
    $
    \RegAlt\le \order\rbr{\max\cbr{(\ln T)^{\frac{3}{5}}(CL^3)^{\frac{2}{5}}T^{\frac{2}{5}},(\ln T)^{\frac{2}{3}}(L^3+\beta L^2)^{\frac{1}{3}}T^{\frac{1}{3}}}}.
    $
\end{corollary}

\begin{proof}
    We decompose $\RegAlt(u)$ for any $u\in\calX$, as shown in \pref{eqn:reg_decompose}, and choose $u'$ similarly. The second term can be bounded by $4L$ and the first term by \pref{eqn:reg_u'_bound} again. Since $\psi^*$ is a $1$-self-concordant barrier (shown in section 6.2.1 of \citep{nesterov1994interior}), we have $\psi(u')-\psi(x_1)=\order(\ln T)$ by \pref{lem:self_concordant}. 
    Finally, plugging in $\sigma=2$ and $M=4$ and using the value of $\eta$ leads us to the final regret bound.
\end{proof}

For losses with $C=0$, we thus obtain a bound of order $\otil(T^\frac{1}{3})$, improving over the $\otil(d^{\frac{2}{3}}T^\frac{1}{3})$ bound of continuous Hedge.


\subsection{The Simplex Case}\label{sec:simplex}
Finally, we consider the case where the feasible domain is the $(d-1)$-dimensional simplex $\Delta_d$. Specifically, to represent $\Delta_d$ as a convex body in $(d-1)$-dimensional space, we set $\calX=\{x\in \R_+^{d-1}, \sum_{i=1}^{d-1}x_i\leq 1\}$. We instantiate \pref{alg:OCO} with the entropy regularizer $\psi(x) = \sum_{i=1}^{d-1} x_i \ln x_i + (1-\sum_{i=1}^{d-1}x_i)\ln (1-\sum_{i=1}^{d-1}x_i)$. The following lemma (proven in \pref{app: alt-oco}) shows that $\psi$ satisfies \pref{asp: psi} with $\sigma = 1$ and $M=8$.

\begin{lemma}\label{lem:negative entropy}
    $\psi(x)$ is Legendre and $1$-strongly convex with respect to $\ell_2$-norm. The convex conjugate of $\psi(x)$ is $\psi^*(w)=\ln\rbr{1+\sum_{i=1}^{d-1} e^{w_i}}$, which is $8$-smooth of order $3$ with respect to $\ell_2$-norm.
\end{lemma}

\begin{corollary}\label{cor: simplex}
Let $\calX=\{x\in \R_+^{d-1}, \sum_{i=1}^{d-1}x_i\leq 1\}$. Under \pref{asp: function}, \pref{alg:OCO} with $\eta =\min\cbr{(\ln d)^{\frac{2}{5}}(CL^3)^{-\frac{2}{5}}T^{-\frac{2}{5}},(\ln d)^{\frac{1}{3}}(L^3+\beta L^2)^{-\frac{1}{3}}T^{-\frac{1}{3}}}$ and regularizer $\psi(x)= \sum_{i=1}^{d-1} x_i \ln x_i + (1-\sum_{i=1}^{d-1}x_i)\ln (1-\sum_{i=1}^{d-1}x_i)$ guarantees 
\[
\RegAlt \le \order\rbr{\max\cbr{(\ln d)^{\frac{3}{5}}(CL^3)^{\frac{2}{5}}T^{\frac{2}{5}},(\ln d)^{\frac{2}{3}}(L^3+\beta L^2)^{\frac{1}{3}}T^{\frac{1}{3}}}}.
\]
\end{corollary}

\begin{proof}
    Using \pref{thm: OCO}, the alternating regret of Hedge can be bounded by:
    \[
    \RegAlt\le \order\rbr{\frac{B_\psi}{\eta}+CL^3\eta^{3/2}T+\rbr{L^3+\beta L^2}\eta^2 T},
    \]
    where $B_\psi = \max_{u_1,u_2\in\calX}\rbr{\psi(u_1)-\psi(u_2)}$. 
    The maximum value of the negative entropy regularizer is $0$, attained at a pure strategy point, and the minimum value is $-\ln d$, attained by the uniform distribution over the actions. Thus, $B_\psi = \ln d$. Plugging in the value of $\eta$ finishes the proof.
\end{proof}

Once again, for losses with $C=0$, our regret bound is of order $\otil(T^\frac{1}{3})$ (with only logarithmic dependence on $d$ hidden in $\otil(\cdot)$), improving over the $\otil(d^{\frac{2}{3}}T^\frac{1}{3})$ bound of continuous Hedge.