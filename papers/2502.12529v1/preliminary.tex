%!TEX root=../main.tex
\section{Preliminaries}

\paragraph{General Notations.} For a convex function $f:\calX\mapsto\R$ and $x,y\in\calX\subseteq\R^d$, define the Bregman divergence between $x$ and $y$ with respect to $f$ as $D_{f}(x,y)=f(x)-f(y)-\inner{\nabla f(y),x-y}$. Define the convex conjugate of $f$, denoted by $f^*:\R^d\mapsto\R$, as $
f^*(y) = \sup_{x \in \calX} \left\{ \inner{x,y} - f(x) \right\}$.
For a vector $v\in\R^d$, denote $v_i$ as the $i$-th entry of the vector. Define $\R^d_+$ to be the $d$-dimensional Euclidean space in the positive orthant and $\Delta_d=\{x\in\R_+^d:~\sum_{i=1}^dx_i=1\}$ to be the $(d-1)$-dimensional simplex. We also denote $\Delta_{\mathcal{X}}$ to be the set of probability densities over a convex domain $\mathcal{X} \subset \mathbb{R}^d$, i.e., 
$\Delta_{\mathcal{X}} = \left\{ p: \mathcal{X} \to \mathbb{R}_+ \mid \int_{x\in\mathcal{X}} p(x) \, dx = 1 \right\}$. 
Let $\calB_2^d(1)=\{x\in\R^d,\|x\|_2\leq 1\}$ be the $\ell_2$ unit ball, and $e_i$ be the one-hot vector in an appropriate dimension with the $i$-th entry being $1$ and remaining entries being $0$.  For a bounded convex domain $\calX$, define its diameter as $\max_{x',x\in\calX}\|x-x'\|_2$. Define $\interior(\calX)$ as the interior of a convex domain $\calX$ and $\partial(\calX)$ as the boundary of $\calX$. 
The notation $\otil(\cdot)$ is used to hide all logarithmic terms.

\subsection{Adversarial Online Convex Optimization}\label{sec:adv-oco}
In adversarial Online Convex Optimization (OCO), at each round $t\in[T]$, the learner picks an action $x_t\in\calX\subseteq\R^d$ for some convex domain $\calX$, and then the adversary picks a convex loss function $f_t: \calX \mapsto \R$ and reveals it to the learner. The standard regret measures the difference between the total loss suffered by the learner and that of a best fixed decision in hindsight: $\Reg \triangleq \max_{u\in\calX}\Reg(u)$ where
$
    \Reg(u) \triangleq \sum_{t=1}^Tf_t(x_t) - \sum_{t=1}^Tf_t(u).
$
On the other hand, the main focus of our work is \emph{alternating regret}~\citep{wibisono2022alternating, cevher2024alternation}, defined as follows: $\RegAlt \triangleq \max_{u\in\calX}\RegAlt(u)$ where
\begin{align*}
    \RegAlt(u) \triangleq \sum_{t=1}^T\left(f_t(x_t)+f_t(x_{t+1})\right) - 2\sum_{t=1}^Tf_t(u)
\end{align*}
is the sum of standard regret $\Reg(u)$ and \emph{cheating regret}
$
\RegCht(u) \triangleq \sum_{t=1}^T f_t(x_{t+1}) - \sum_{t=1}^Tf_t(u).
$\footnote{For notational convenience, we include $x_{T+1}$, the output of the algorithm at the beginning of round $T+1$, in the definition.}
The cheating regret evaluates the learner's loss at time $t$ using the \emph{next} decision $x_{t+1}$ that is computed with the knowledge of $f_t$ (hence cheating),
and thus intuitively can be easily made negative.
The hope is that it is negative enough so that the alternating regret is order-different from the standard regret.
%
Indeed, for the special case of Online Linear Optimization (OLO) where all $f_t$'s are linear functions, \citet{cevher2024alternation} show a seperation between standard regret $\Reg$ and alternating regret $\RegAlt$:
while the minimax bound for the former is $\Theta(\sqrt{T})$,
the latter can be as small as $\otil(T^{\frac{1}{3}})$ when $\calX$ is a simplex and $\order(\log T)$ when $\calX$ is an $\ell_2$-ball.

However, as explicitly mentioned in~\citet{cevher2024alternation}, extension to general convex functions is highly unclear.
To see this, first recall the standard OCO to OLO reduction for standard regret:
$\Reg(u) \leq \sum_{t=1}^T \inner{\nabla f_t(x_t), x_t-u}$, which is due to the convexity of $f_t$ and reveals that it is sufficient to consider an OLO instance with linear loss $x\mapsto\inner{\nabla f_t(x_t), x}$ at time $t$.
However, when applying the same trick to alternating regret: that is, bounding $\RegAlt(u)$ by
$
 \sum_{t=1}^T \inner{\nabla f_t(x_t), x_t-u} + \inner{\nabla f_t(x_{t+1}), x_{t+1}-u},
$
one quickly realizes that this does not correspond to the alternating regret of any OLO instance.
In particular, the alternating regret for an OLO instance with linear loss $x\mapsto\inner{\nabla f_t(x_t), x}$ at time $t$ is instead: 
$
 \sum_{t=1}^T \inner{\nabla f_t(x_t), x_t-u} + \inner{\nabla f_t(x_{t}), x_{t+1}-u}.
$

\subsection{Alternating Learning Dynamics in Games}\label{sec:alt-game}
The motivation of studying alternating regret stems from learning in games with \textit{alternation}. Specifically, consider a two-player general-sum game 
where $u_1(x,y), u_2(x,y):\calX\times\calY\mapsto[-1,1]$ are the loss functions for $x$-player and $y$-player, respectively. $u_1(x,y)$ is convex in $x$ for any $y\in\calY$, and $u_2(x,y)$ is convex in $y$ for any $x\in\calX$. 
A pair of strategy $(\bar{x}, \bar{y})$ is an $\epsilon$-NE (Nash Equilibrium) if no player has more than $\epsilon$ incentive to unilaterally deviate from it: $u_1(\bar{x}, \bar{y}) \leq \min_{x\in\calX} u_1(x, \bar{y}) + \epsilon$
and $u_2(\bar{x}, \bar{y}) \leq \min_{y\in\calY} u_2(\bar{x}, y) + \epsilon$.
A joint distribution $\calP$ over $\calX \times \calY$ is called an $\epsilon$-CCE (Coarse Correlated Equilibrium) if in expectation no player has more than $\epsilon$ incentive to unilaterally deviate from a strategy drawn from $\calP$ to a fixed strategy: $\E_{(x,y)\sim \calP}[u_1(x,y)] \leq \min_{x'\in\calX}\E_{(x,y)\sim \calP}[u_1(x',y)] + \epsilon$ and $\E_{(x,y)\sim \calP}[u_2(x,y)] \leq \min_{y'\in\calY}\E_{(x,y)\sim \calP}[u_2(x,y')] + \epsilon$.

An efficient and popular way to find these equilibria is via repeated play using online learning algorithms.
For example, in a standard (simultaneous) learning dynamic, both players use an OCO algorithm to make their decision $x_t \in \calX$ and $y_t \in \calY$ for round $t$ simultaneously and observe their loss function $u_1(x, y_t)$ and $u_2(x_t, y)$ respectively. 
After interacting for $T$ rounds, it is well known that the uniform distribution over $(x_1, y_1), \dots, (x_T, y_T)$ is an $\epsilon$-CCE with $\epsilon$ being the average regret.
Moreover, in the special case of zero-sum games ($u_2 = -u_1$), the average strategy $(\frac{1}{T}\sum_{t=1}^T x_t, \frac{1}{T}\sum_{t=1}^T y_t)$
is an $\epsilon$-NE for the same $\epsilon$ (see e.g.,~\citealp{freund1999adaptive}).
Since one can ensure $\order(\sqrt{T})$ standard regret, a convergence rate of $\order(1/\sqrt{T})$ to these equilibria is immediate.

There are many studies on how to accelerate such  $\order(1/\sqrt{T})$ convergence, with alternation probably being the simplest and most practically popular one.
Specifically, the difference in an alternating learning dynamic compared to a simultaneous one is that at each round $t$, the $x$-player first makes their decision $x_t$, and then the $y$-player, \emph{seeing $x_t$}, makes their decision $y_t$; see \pref{alg:alternation}.

\begin{algorithm}[tb]
   \caption{Alternating Learning Dynamic using OCO}
   \label{alg:alternation}

   {\bfseries Input:} a two-player game with loss functions $u_1, u_2: \calX\times\calY\mapsto[-1,1]$.
   
   {\bfseries Input:} OCO algorithms $\Alg_x$ (for $\calX$) and $\Alg_y$ (for $\calY$).
   
   \For{$t=1$ {\bfseries to} $T$}{
   
   $\Alg_x$ decides $x_t \in \calX$ using past loss functions $u_1(x, y_1), \dots, u_1(x, y_{t-1})$
   
   $\Alg_y$ decides $y_t \in \calY$ using past loss functions $u_2(x_1, y), \dots, u_2(x_{t-1}, y)$ and $u_2(x_t, y)$
   
   }
\end{algorithm}

In the special case of zero-sum games ($u_2=-u_1$),
\citet{wibisono2022alternating, cevher2024alternation} show that the average strategy of such an alternating learning dynamic is an $\epsilon$-NE with $\epsilon$ now being the average \emph{alternating} regret.
While they only prove this for linear losses, it is straightforward to generalize it to convex losses, which is included below for completeness.

\begin{theorem}\label{thm:zero-sum}
In the alternating learning dynamic described by \pref{alg:alternation} for a zero-sum game ($u_2=-u_1$), 
suppose that $\Alg_x$ and $\Alg_y$ 
guarantee a worst-case alternating regret bound $\RegAlt^x$ and $\RegAlt^y$ respectively.
Then, the average strategy $(\frac{1}{T}\sum_{t=1}^T x_t, \frac{1}{T}\sum_{t=1}^T y_t)$ is an $\epsilon$-NE with $\epsilon = \order\left(\frac{\RegAlt^x+\RegAlt^y}{T}\right)$.
\end{theorem}

Therefore, if alternating regret can be made $o(\sqrt{T})$, a faster than $1/\sqrt{T}$ convergence rate is achieved.
In fact, such an implication generalizes to CCE for two-player general-sum games as well, which is unknown before to the best of our knowledge.

\begin{theorem}\label{thm:general-sum}
In the alternating learning dynamic described by \pref{alg:alternation} (for general-sum games), suppose that $\Alg_x$ and $\Alg_y$ 
guarantee a worst-case alternating regret bound $\RegAlt^x$ and $\RegAlt^y$ respectively.
Then, the uniform distribution over $\{(x_t,y_t), (x_{t+1},y_t)\}_{t\in[T]}$ is an $\epsilon$-CCE with $\epsilon = \order\left(\frac{\max\{\RegAlt^x,\RegAlt^y\}}{T}\right)$.
\end{theorem}

Both theorems are a direct consequence of the definition of alternating regret; see \pref{app:alt-game} for the proofs.
We remark that it is unclear how to generalize \pref{thm:general-sum} to a general-sum game with more than two players.
We conjecture that such a generalization requires either a new alternation scheme or a new concept of regret.

Given these connections between alternating regret and the convergence of alternating learning dynamics, the rest of the paper focuses on understanding what alternating regret bounds are achievable in an adversarial environment.
