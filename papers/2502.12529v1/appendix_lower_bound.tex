\section{Omitted Details in \pref{sec: lower_bound}}\label{app:lower_bound}
In this section, we show omitted proofs in \pref{sec: lower_bound}. We first prove \pref{thm:hedge_lower_bound}, which shows that Hedge suffers $\Omega(T^{\frac{1}{3}})$ alternating regret in the expert problem.
\begin{proof}[Proof of \pref{thm:hedge_lower_bound}]
We prove the lower bound by constructing two environments and showing that if the Hedge algorithm achieves $\order(T^{1/3})$ alternating regret for one environment, then it will suffer $\Omega(T^{1/3})$ alternating regret for the other one.

\textbf{Environment 1}: We consider the time horizon to be $3T$ and $3$ actions with the loss vector cycling between the three basis vectors in $\mathbb{R}^3: (1,0,0), (0,1,0), (0,0,1)$ and we have $\min_{i\in[3]} \sum_{t=1}^{3T} \ell_{t,i} = T$. Direct calculation shows that Hedge with learning rate $\eta>0$ predict the $p_t$ sequence as follows: $p_{3t-2}=\rbr{\frac{1}{3},\frac{1}{3},\frac{1}{3}}, p_{3t-1}=\rbr{\frac{e^{-\eta}}{2+e^{-\eta}},\frac{1}{2+e^{-\eta}},\frac{1}{2+e^{-\eta}}}, p_{3t}=\rbr{\frac{e^{-\eta}}{1+2e^{-\eta}},\frac{e^{-\eta}}{1+2e^{-\eta}},\frac{1}{1+2e^{-\eta}}}$, $t\in[T]$. Thus, we can bound the alternating regret as follows:
\begin{align*}
   & \RegAlt = T\rbr{\frac{2}{3}+\frac{1+e^{-\eta}}{2+e^{-\eta}}+\frac{1+e^{-\eta}}{1+2e^{-\eta}}} - 2T= \frac{T(1-e^{-\eta})^2}{3(2+e^{-\eta})(1+2e^{-\eta})}\ge \frac{T(1-e^{-\eta})^2}{27}.
\end{align*}
To proceed, note that when $\eta \geq 1$, the above inequality already means that $\RegAlt=\Omega(T)$. Therefore, we only consider the case when $\eta\leq 1$. Using the fact that $e^{-\eta} \le 1-\eta+\frac{\eta^2}{2}$ for $\eta\geq 0$, we can further lower bound $\frac{T(1-e^{-\eta})^2}{27}$ by $
    \RegAlt \ge \frac{T(\eta-\frac{{\eta}^2}{2}^2)}{27}\ge \frac{\eta^2T}{108},$
where the last inequality is due to $\eta\leq 1$. Therefore, we know that $\RegAlt = \Omega(\eta^2T)$.

\textbf{Environment 2}: We consider the time horizon to be $T$ and $3$ actions with the loss vector $\ell_t$ being $(1,0,0)$ for all rounds. Here, the benchmark $\min_{i\in[3]} \sum_{t=1}^T \ell_{t,i} $ is $0$, and $p_{t,1}=\frac{e^{-\eta T}}{2+e^{-\eta T}}$ for all $t\in[T]$. In this case, if $\eta\leq \frac{2}{T}$, we know that $p_{t,1}\geq \frac{e^{-2}}{2+e^{-2}}$ for all $t\in [T]$ and the algorithm will be suffering $\Omega(T)$ regret. When $1\geq \eta\geq \frac{2}{T}$, we have:
\begin{align*}
    \RegAlt &= \sum_{t=0}^T \frac{2e^{-\eta t}}{2+e^{-\eta t}} - \frac{1}{3} -\frac{e^{-\eta T}}{2+e^{-\eta T}} \ge \sum_{t=1}^{T-1} \frac{2}{1+2e^{\eta t}}\ge \int_{1}^{T} \frac{2}{3e^{\eta t}}dt= \frac{2}{3\eta}\left[-e^{-\eta t}\right]\Big|_{1}^{T}\ge\frac{e^{-1}}{3\eta},
\end{align*}
where the second inequality uses $e^{\eta t}\geq 1$ and the last inequality uses $\eta\leq 1$.
Therefore, we have $\RegAlt = \Omega\Big(\frac{1}{\eta}\Big)$. Combining both environments, we know that Hedge with learning rate $\eta$ suffers a $\Omega(\max\{\frac{1}{\eta},\eta^2 T\})$, leadning to a $\Omega(T^{\frac{1}{3}})$ lower bound.
\end{proof}


Next, we show that \PRM suffers $\Omega(\sqrt{T})$ alternating regret in the adversarial environment.

\begin{proof}[Proof of \pref{thm: PRM+}]
The procedure of \PRM is as follows: Let $\wh{R}_1 = {R}_1 = {r}_{0} = \bm{0}$, where $\bm{0}$ is a zero vector with all components equal to zero,
and for $t\ge 1$, \PRM selects $p_t$ to be $\hat{{R}}_t/\norm{\hat{{R}}_t}_1$ where $\hat{{R}}_t = [{R}_t+{r}_{t-1}]^+$ and update ${R}_{t+1}$ to be $[{R}_t+{r}_t]^+$, where ${r}_t = \langle {p}_t, {\ell}_t\rangle \bm{1}_d - {\ell}_t$.
Here, $\bm{1}_d$ is a vector with all components equal to one.
We use the following loss sequence to show the lower bound: for $k\ge 0$, consider
    \begin{align*}
    {\ell}_{2k}=
    \begin{bmatrix}
        1\\
        0
    \end{bmatrix},\quad
    {\ell}_{2k+1}=
    \begin{bmatrix}
        -0.5\\
        0
    \end{bmatrix}.
\end{align*}
To simplify notation, denote $\alpha_k=R_{2k+1,2}$ for $k\in [\frac{T}{2}-1]$. \pref{lem: PRM+} shows that for $k\ge 5$, $\alpha_k$ follows the following recurrence relation: $\alpha_{k+1}=\alpha_k+\frac{1}{1+\alpha_k}$. We use this recurrence to compute the values of $p_t$ for all rounds $t>10$.


Thus, using \pref{lem: PRM+}, the alternating loss (standard loss + cheating loss) for the rounds $t=2k+1$ and $t=2k+2$ can be calculated as
\begin{align*}
    \inner{{p}_{2k+1},\ell_{2k}+\ell_{2k+1}}+\inner{{p}_{2k+2},\ell_{2k+1}+\ell_{2k+2}}= \frac{1}{2(1+\alpha_k)}.
\end{align*}
Since the action $2$ always has a loss of 0, the benchmark here is 0.
Therefore, the alternating regret is:
\begin{equation}\label{eqn:prm+-regalt}
    \RegAlt = C+\frac{1}{2}\sum_{k=5}^{\frac{T}{2}}\frac{1}{1+\alpha_k},
\end{equation}
where $C$ is a constant bounding the regret for the first $10$ rounds.
To estimate the quantity above, we prove $\alpha_k\le 2\sqrt{k}-1$ by induction. The base case $k=5$ can be verified by direct calculation. Now, let us assume that the claim holds for $k$. Then, for $k+1$, 
\begin{align*}
\alpha_{k+1}&=\alpha_k +\frac{1}{1+\alpha_k}\le 2\sqrt{k}-1+\frac{1}{2\sqrt{k}}\le\frac{4k+1}{2\sqrt{k}}-1\le 2\sqrt{k+1}-1.
\end{align*}
The first inequality comes from the fact that the function $f(x)=x+\frac{1}{1+x}$ is monotonically increasing for $x\ge 0$.
Substituting it in \pref{eqn:prm+-regalt}, we get
\begin{align*}
\RegAlt\ge C+\frac{1}{4}\sum_{k=5}^{\frac{T}{2}} \frac{1}{\sqrt{k}}=\Theta(\sqrt{T}).
\end{align*}
Therefore, $\RegAlt = \Omega(\sqrt{T})$ for the loss sequence proposed.
\end{proof}

\begin{lemma}\label{lem: PRM+}
   Suppose that the loss vector sequence satisfies that $\ell_{2k}=\begin{bmatrix}
    1\\ 0
\end{bmatrix},{\ell}_{2k+1}=\begin{bmatrix}
    -0.5\\ 0
\end{bmatrix}$ for $k\geq 0$. Then, \PRM guarantees that for $k\geq 5$
\begin{align*}
&{p}_{2k+1}=\begin{bmatrix}
    0\\ 1
\end{bmatrix},{p}_{2k+2}=\begin{bmatrix}
    \frac{1}{1+\alpha_k}\\ \frac{\alpha_k}{1+\alpha_k}
\end{bmatrix},\\
&{R}_{2k+2}=\begin{bmatrix}
    0.5\\ \alpha_k
\end{bmatrix},R_{2k+3}=\begin{bmatrix}
    0\\ \alpha_k+\frac{1}{1+\alpha_k}
\end{bmatrix},\\
&\wh{R}_{2k+2}=\begin{bmatrix}
    1\\ \alpha_k
\end{bmatrix},\wh{R}_{2k+3}=\begin{bmatrix}
    0\\ \alpha_k+\frac{2}{1+\alpha_k}
\end{bmatrix},
\end{align*}
where $\alpha_5>2$ is certain constant and $\alpha_{k+1}=\alpha_k+\frac{1}{1+\alpha_k}$ for $k\geq 5$.
\end{lemma}

\begin{proof}
It can be verified that \pref{lem: PRM+} holds true when $k=5$. For $k>5$, we prove by induction. Suppose \pref{lem: PRM+} holds for $k$. Then, for $k+1$, we have,
\begin{align*}
    &{p}_{2k+3} =\frac{\wh{R}_{2k+3}}{\norm{\wh{R}_{2k+3}}_1} =\begin{bmatrix}
        0\\ 1
    \end{bmatrix},  \\
    &R_{2k+4} = [R_{2k+3}+r_{2k+3}]^+ = \begin{bmatrix}
        0.5\\
        \alpha_k+\frac{1}{1+\alpha_k}
    \end{bmatrix} =
    \begin{bmatrix}
        0.5\\
        \alpha_{k+1}
    \end{bmatrix},
    \\
    &r_{2k+3} = \inner{p_{2k+3},\ell_{2k+3}}\mathbf{1}_d - \ell_{2k+3} = \begin{bmatrix}
        0.5\\
        0
    \end{bmatrix},\\
    &\hat{R}_{2k+4} = [R_{2k+4}+r_{2k+3}]^+ = 
    \begin{bmatrix}
        1\\
        \alpha_{k+1} 
    \end{bmatrix}.
\end{align*}
Since $\alpha_k\ge 2$, we have $\alpha_{k+1}\ge 2$ as well. Using this, we can see that
\begin{align*}
    &{p}_{2k+4} =
    \frac{\hat{R}_{2k+4}}{\|\hat{R}_{2k+4}\|_1} = 
    \begin{bmatrix}
         \frac{1}{1+\alpha_{k+1}}\\
         \frac{\alpha_{k+1}}{1+\alpha_{k+1}}
    \end{bmatrix}, \\
    &r_{2k+4} = \inner{p_{2k+4},\ell_{2k+4}}\mathbf{1}_d - \ell_{2k+4} = \begin{bmatrix}
        -\frac{\alpha_{k+1}}{1+\alpha_{k+1}}\\
        \frac{1}{1+\alpha_{k+1}}
    \end{bmatrix}, \\
    &R_{2k+5} = [R_{2k+4}+r_{2k+4}]^+ = 
    \begin{bmatrix}
        0\\
        \alpha_{k+1}+\frac{1}{1+\alpha_{k+1}}
    \end{bmatrix}, \\
    &\hat{R}_{2k+5} = [R_{2k+5}+r_{2k+4}]^+ = 
    \begin{bmatrix}
        0\\
        \alpha_{k+1}+\frac{2}{1+\alpha_{k+1}}
    \end{bmatrix},
\end{align*}
where the third equality uses the fact that $\frac{\alpha_{k+1}}{1+\alpha_{k+1}}\geq \frac{2}{3}$. Thus, the claim is true for $k+1$, and hence it holds for all $k\ge 5$.
\end{proof}