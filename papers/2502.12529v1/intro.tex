%!TEX root=../main.tex
\section{Introduction}
\label{intro}

Online Convex Optimization (OCO) is a heavily-studied framework for online learning~\citep{zinkevich2003online}.
In this framework, a learner sequentially interacts with an adversary for $T$ rounds.
In each round $t$, the learner selects an action $x_t$ from a fixed decision space $\calX$,
after which the adversary decides a convex loss function $f_t: \calX\mapsto \R$.
The learner then suffers loss $f_t(x_t)$ and observes $f_t$.
The standard performance measure for the learner is the worst-case \emph{regret}, defined as the difference between their total loss over $T$ rounds and that of a best fixed action in hindsight.

One important application of OCO algorithms is for learning in large-scale games and finding approximate equilibria efficiently.
The fundamental connection between online learning and game solving dates back to~\citep{foster1997calibrated,freund1999adaptive, hart2000simple} and has been driving some of the recent AI breakthroughs such as superhuman-level poker agents~\citep{bowling2015heads, moravvcik2017deepstack, brown2018superhuman, brown2019superhuman}.
The most basic result in this area is that in a two-player zero-sum convex-concave game, if both players repeatedly play the game using an OCO algorithm to make their decisions, then their time-averaged strategy is an approximate Nash Equilibrium (NE) with the duality gap being the average regret of the OCO algorithm. 
Since the worst-case regret for most problems is $\order(\sqrt{T})$, this implies an $\order(1/\sqrt{T})$ convergence rate to NE.

There has been a surge of studies on how to improve over this $\order(1/\sqrt{T})$ convergence rate in recent years.
One of the simplest approaches is to apply \textit{alternation}, which requires minimal change to the original approach: let the two players make their decisions in turn instead of simultaneously.
Such a simple adjustment turns out to be highly efficient in speeding up the convergence and is a default method in solving large-scale extensive-form games~\citep{tammelin2014solving}.

However, why alternation works so well has been a mystery until some recent progress.
Specifically, \citet{wibisono2022alternating} show that in normal-form games (where losses are linear), applying alternation with standard algorithms such as Hedge~\citep{freund1997decision} achieves a faster convergence rate of $\order(1/T^{\frac{2}{3}})$.
To analyze alternation, they propose a new regret notion, later termed \textit{alternating regret} by~\citet{cevher2024alternation}, and show that the convergence rate in an alternating learning dynamic is governed by the average alternating regret of the two players. 
In a word, alternating regret can be seen as the sum of standard regret plus \textit{cheating regret} where the learner's loss at time $t$ is evaluated at the next decision $x_{t+1}$, that is, $f_t(x_{t+1})$.
Since $x_{t+1}$ is computed with the knowledge of $f_t$,
cheating regret can be easily made negative, making alternating regret potentially much smaller than standard regret and explaining the faster $\order(1/T^{\frac{2}{3}})$ convergence.

Motivated by this work, \citet{cevher2024alternation} further investigate alternating regret for the special case of Online Linear Optimization (OLO) where all loss functions are linear.
They show that, even when facing a \textit{completely adversarial} environment, alternating regret can be made as small as $\order(T^{\frac{1}{3}})$ when the decision set $\calX$ is a simplex, or even $\order(\log T)$ when $\calX$ is an $\ell_2$-ball (while the standard regret for both cases is $\Omega(\sqrt{T})$).
The latter result immediately implies that applying their algorithm to an alternating learning dynamic for a zero-sum game defined over an $\ell_2$-ball leads to $\otil(1/T)$ convergence.

However, the results of \citet{cevher2024alternation} are restricted to linear losses, and they explicitly ask the question whether similar results can be derived for convex losses.
Indeed, while for standard regret, a convex loss $f_t$ can be reduced to a linear loss $x\mapsto \inner{\nabla f_t(x_t), x}$, this is no longer the case for alternating regret (see \pref{sec:adv-oco} for detailed explanation).

\paragraph{Contributions.} In this work, we answer this open question in the affirmative and provide several results that significantly advance our understanding on alternating regret and alternating learning dynamics for two-player games (both zero-sum and general sum).
Our concrete contributions are:
\begin{itemize}[leftmargin=*]
    \item In \pref{sec: alt-oco}, we start with an observation that the vanilla Hedge algorithm~\citep{freund1997decision} already achieves $\order(T^\frac{1}{3}\log^{\frac{2}{3}}d)$ alternating regret for an adversarial $d$-expert problem (that is, OLO over a simplex), a result that is implicitly shown in~\citet{wibisono2022alternating}.
    This observation not only already improves over the $\order(T^\frac{1}{3}\log^{\frac{4}{3}}(dT))$ bound of the arguably more complicated algorithm by~\citet{cevher2024alternation},
    but more importantly also inspires us to consider the \emph{continuous Hedge} algorithm for general OCO. 
    Indeed, via a nontrivial generalization of the analysis, we manage to show that continuous Hedge achieves $\otil(d^{\frac{2}{3}}T^\frac{1}{3})$ alternating regret for any convex decision set $\calX$ and any bounded convex loss functions.

    \item This general result implies that for any convex-concave zero-sum games, applying continuous Hedge in an alternating learning dynamic leads to $\otil(d^{\frac{2}{3}}/T^{\frac{2}{3}})$ convergence to NE.
    Moreover, we generalize this implication to two-player general-sum games with convex losses, showing the same convergence rate for coarse correlated equilibria.
    As far as we know, this is the first result for alternating learning dynamic in general-sum games.

    \item Although continuous Hedge can be implemented in polynomial time using existing log-concave samplers, the time complexity can be high.
    Moreover, its alternating regret bound depends on the dimension $d$ even when $\calX$ is an $\ell_2$ ball.
    To address these issues, in \pref{sec:SC}, we propose another simple algorithm that applies
    Follow-the-Regularized-Leader (FTRL) to the original convex losses (instead of their linearized version) with a Legendre regularizer whose convex conjugate is third-order smooth.
    Based on the decision set, we instantiate the algorithm with different regularizers and analyze its alternating regret for OCO with smooth and self-concordant loss functions (which include linear and quadratic losses).
    For example, when $\calX$ is an $\ell_2$ ball, our algorithm achieves $\otil(T^\frac{2}{5})$ alternating regret without any dimension dependence, which can be further improved to $\otil(T^{\frac{1}{3}})$ for quadratic losses.
    
    \item Finally, in \pref{sec: lower_bound}, we discuss some algorithm-specific alternating regret lower bounds, focusing on the special case with linear losses over a simplex.
    We start by showing that $\Omega(T^{\frac{1}{3}})$ alternating regret is unavoidable for Hedge, at least not with a fixed learning rate.
    We then turn our attention to a widely-used algorithm, called Predictive Regret Matching$^+$~\citep{farina2021faster},
    and show that despite its amazing empirical performance in alternating learning dynamics, 
    its alternating regret is in fact $\Omega(\sqrt{T})$ when facing an adversarial environment.
    This implies that to demystify its excellent performance, one has to take the game structure into account.
\end{itemize}

\paragraph{Related Work.}
We refer the reader to~\citet{hazan2016introduction, orabona2019modern} for a general introduction to OCO.
The continuous hedge algorithm dates back to Cover's universal portfolio algorithm~\citep{cover1991universal}, and is known to achieve $\otil(\sqrt{dT})$ (standard) regret for general convex losses (see e.g.,~\citet{narayanan2010random}) and $\order(d\log T)$ (standard) regret for exp-concave losses \citep{hazan2007logarithmic}.
OCO with self-concordant loss functions have been studied in~\citet{zhang2017improved} under the context of dynamic regret.

As mentioned, there is a surge of studies on learning dynamics that converge faster than $1/\sqrt{T}$, such as~\citet{daskalakis2011near, rakhlin2013optimization, syrgkanis2015fast, chen2020hedging, daskalakis2021near-optimal, farina2022kernelized, anagnostides2022near-optimal, anagnostides2022uncoupled, farina2022near}.
All these works achieve acceleration by improving the standard $\order(\sqrt{T})$ regret bound, which is only possible by exploiting the fact that the environment is not completely adversarial but controlled by other players who deploy a similar learning algorithm.

Alternation, on the other hand, is a much simpler approach to achieve acceleration, but the reason why it works so well is not well understood.
Earlier works can only show that it works at least as well as the standard simultaneous dynamics~\citep{tammelin2014solving, burch2019revisiting} or it works strictly better but without a good characterization on how much better~\citep{grand2024solving}.
\citet{wibisono2022alternating} are the first to show a substantial convergence improvement brought by alternation via the notion of alternating regret.
Their $\order(1/T^{\frac{2}{3}})$ convergence rate was later improved to $\order(1/T^{\frac{4}{5}})$ by~\citet{katona2024symplectic}.
While this latter result heavily relies on the game structure, as we point out in~\pref{sec: alt-oco}, the alternating regret bound of~\citet{wibisono2022alternating} in fact holds even in the adversarial setting.
Focusing on OLO, \citet{cevher2024alternation} further provide more results on $o(\sqrt{T})$ alternating regret in an adversarial environment.
These results make the analysis for alternating learning dynamics much simpler since one can simply ignore how the opponent behaves and how the players' decisions are entangled. 
Our work significantly advances our understanding on how far this approach can go by generalizing their results to convex losses as well as providing lower bounds for popular algorithms.