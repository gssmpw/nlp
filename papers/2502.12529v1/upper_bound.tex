%!TEX root=main.tex
\section{$o(\sqrt{T})$ Alternating Regret for OCO}\label{sec: alt-oco}
In this section, we propose an OCO algorithm with $\otil(T^{\frac{1}{3}})$ alternating regret.
Our idea is based on the following observation for the special case of the expert problem where $\calX=\Delta_d$ and $f_t(x) = \inner{\ell_t,x}$, $\|\ell_t\|_{\infty}\leq 1$ for all $t\in[T]$. 
While~\citet{wibisono2022alternating} show that the vanilla Hedge algorithm~\citep{freund1997decision} achieves $\otil(T^{\frac{1}{3}})$ alternating regret when played against itself in a zero-sum game, 
the same result in fact holds for any adversarial expert problem, which was not made explicit in their work.
We summarize this observation in the following theorem.

\begin{theorem}[Implicit in~\citealp{wibisono2022alternating}]\label{thm:hedge_simplex}
For an adversarial OLO problem with $\calX=\Delta_d$ and $f_t(x) = \inner{\ell_t,x}$, $\|\ell_t\|_{\infty}\leq 1$ for all $t\in[T]$,
the Hedge algorithm that plays $p_t \in\Delta_d$ in round $t$ such that $p_{t,i} \propto \exp{(-\eta \sum_{\tau=1}^{t-1} \ell_{\tau,i})}$ with
$\eta = T^{-\frac{1}{3}}\log^{\frac{1}{3}}d$ ensures $\RegAlt=\order(T^{\frac{1}{3}}\log^{\frac{2}{3}}d)$.
\end{theorem}
\begin{proof}[sketch]
Via standard analysis of Hedge, we can show that the alternating regret against any $u$ is equal to the following 
\begin{align}\label{eqn:alt-regret-simplex-main}
    \frac{2(\KL(u,p_1)-\KL(u,p_{T+1}))}{\eta} + \frac{1}{\eta}\sum_{t=1}^T\left(\KL(p_t,p_{t+1})-\KL(p_{t+1},p_{t})\right).
\end{align}
Since $p_1$ is the uniform distribution over $d$ experts by definition, the first term $\frac{2(\KL(u,p_1)-\KL(u,p_{T+1}))}{\eta}$ can be bounded by $\frac{2\log d}{\eta}$. It remains to control the second term, which we call a $\KL$ commutator following the term Bregman commutator from~\citet{wibisono2022alternating}. Direct calculation shows that 
\begin{align*}
    \KL(p_t,p_{t+1})-\KL(p_{t+1},p_t) = D_{G^*}(-\eta L_t,-\eta L_{t-1}) - D_{G^*}(-\eta L_{t-1},-\eta L_{t}),
\end{align*}
where $G^*(w)=\log(\sum_{t=1}^T\exp(w_i))$ (the convex conjugate of negative entropy) and $L_t=\sum_{\tau\leq t}\ell_t$.
Then, applying the key Lemma A.2 of~\citet{wibisono2022alternating} (included as \pref{lem:bregman_commutator} here)
which bounds the Bregman commutator of a 3rd-order smooth function (see \pref{def:3rd-order smoothness}), we arrive at $D_{G^*}(-\eta L_t,-\eta L_{t-1}) - D_{G^*}(-\eta L_{t-1},-\eta L_{t})\leq \frac{4}{3}\|\eta\ell_t\|_{\infty}^3\leq \frac{4}{3}\eta^3$, since $G^*(x)$ is $8$-smooth of order 3 as proven in~\citet[Example A.3]{wibisono2022alternating}. Plugging this bound to \pref{eqn:alt-regret-simplex-main} and picking the optimal $\eta$ finishes the proof.
The full proof is deferred to \pref{app:CEW}.
\end{proof}

This bound not only improves upon the $\order(T^{\frac{1}{3}}\log^{\frac{4}{3}}d)$ guarantee of a much more complicated algorithm by~\citet{cevher2024alternation}, 
but perhaps more importantly, also inspires us to consider 
whether a continuous version of Hedge, called continuous Hedge (\pref{alg:hedge-cont}), can handle the general OCO setting well.
Indeed, for standard regret, continuous Hedge achieves an $\otil(\sqrt{dT})$ bound for any OCO instance~\citep{narayanan2010random}, generalizing Hedge's $\order(\sqrt{T\log d})$ standard regret from OLO to OCO.
Fortunately, in a similar way and via non-trivial analysis, we manage to show that the same generalization carries over for alternating regret, as shown below.

\begin{theorem}\label{thm:hedge-cont}
    For any OCO instance with  $\max_{x\in\calX}|f_t(x)|\le 1$ for all $t\in[T]$, \pref{alg:hedge-cont} with $\eta = \min\{1,T^{-\frac{1}{3}}(d\log T)^{\frac{1}{3}}\}$ achieves $\RegAlt=\order(d^{\frac{2}{3}}T^{\frac{1}{3}}\log^{\frac{2}{3}}T)$.
\end{theorem}


\begin{algorithm}[t]
   \caption{Continuous Hedge}
   \label{alg:hedge-cont}
   {\bfseries Input:} Parameter $\eta>0$
   
   \For{$t=1$ {\bfseries to} $T$}{
   
   The learner computes $p_t\in\Delta_{\calX}$ such that $p_t(x) \propto \exp{(-\eta \sum_{\tau=1}^{t-1} f_\tau(x))}$
   
   The learner chooses $x_t=\int_{x\in\calX}p_t(x)xdx$ and observes $f_t$.
   }
\end{algorithm}
The general idea of the proof follows that of \pref{thm:hedge_simplex}, which is to bound the alternating regret by the $\KL$ commutator $\sum_{t=1}^T(\KL(p_t,p_{t+1})-\KL(p_{t+1},p_{t}))$. The main technical part of the proof is then to further bound the per-round KL-divergence commutator $\KL(p_t,p_{t+1})-\KL(p_{t+1},p_t)$ by $\order(\eta^3)$ for the continuous distribution $p_t, p_{t+1}$ over the whole convex domain, which is done via a proof that deviates from that of~\citet[Lemma~A.2]{wibisono2022alternating}.
See \pref{app:CEW} for details.

We have thus shown that $\otil(d^{\frac{2}{3}}T^{\frac{1}{3}})$ alternating regret is achievable for general OCO, resolving the open question asked by~\citet{cevher2024alternation}
and bypassing the obstacle that OCO cannot be reduced to OLO in this case.
Note that we do not require either Lipschitzness or smoothness of the loss functions, but only bounded function value. 
Moreover, combining \pref{thm:hedge-cont} with \pref{thm:zero-sum} and \pref{thm:general-sum}, we immediately obtain the following convergence result on an alternating learning dynamic for general two-player games with convex loss functions.

\begin{corollary}\label{cor:game with entropic barrier}
    Consider an alternating learning dynamic described by \pref{alg:alternation} where the loss function $u_1(\cdot,y),u_2(x,\cdot)\in[-1,1]$ are convex for all $x\in\calX$ and $y\in\calY$, and $\Alg_x$ and $\Alg_y$ are \pref{alg:hedge-cont} with $\eta=(d\log T)^{\frac{1}{3}}T^{-\frac{1}{3}}$. Then the time-averaged strategy $(\frac{1}{T}\sum_{t=1}^T x_t, \frac{1}{T}\sum_{t=1}^T y_t)$ is an $\otil(d^{\frac{2}{3}}T^{-\frac{2}{3}})$-NE when the game is zero-sum; otherwise, the uniform distribution over the strategies $\{(x_t,y_t), (x_{t+1},y_t)\}_{t\in[T]}$ is an $\otil(d^{\frac{2}{3}}T^{-\frac{2}{3}})$-CCE.
\end{corollary}

While there exist (simultaneous) learning dynamics that achieve faster convergence for games with convex losses~\citep{syrgkanis2015fast, farina2022near},
our results are the first to show that better than $1/\sqrt{T}$ convergence is possible using simple alternating learning dynamics.

