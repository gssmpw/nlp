%\documentclass[12pt]{colt2025} % Anonymized submission
%\documentclass[final,12pt]{colt2025} % Include author names
\documentclass[11pt]{article} % Anonymized submission

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e
\usepackage{macro_yihan}
%\usepackage{amsthm}

%\title[Spectral Estimators for Multi-Index Models]{Spectral Estimators for Multi-Index Models\\Achieving the Computational Limits of Weak Recovery}

\title{Spectral Estimators for Multi-Index Models:\\ Precise Asymptotics and Optimal Weak Recovery}

%\usepackage{times}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}
 
% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

% Authors with different addresses:
%\coltauthor{%
% \Name{Author Name1} \Email{abc@sample.com}\\
% \addr Address 1
% \AND
% \Name{Author Name2} \Email{xyz@sample.com}\\
% \addr Address 2%
%}

\author{
    Filip Kova\v{c}evi\'c\thanks{Institute of Science and Technology Austria. Email: \href{mailto:filip.kovacevic@ist.ac.at}{\texttt{filip.kovacevic@ist.ac.at}}.} 
    \and
    Yihan Zhang\thanks{Institute of Science and Technology Austria. Email: \href{mailto:zephyr.z798@gmail.com}{\texttt{zephyr.z798@gmail.com}}.}
    \and
    Marco Mondelli\thanks{Institute of Science and Technology Austria. Email: \href{mailto:marco.mondelli@ist.ac.at}{\texttt{marco.mondelli@ist.ac.at}}.}
}


\begin{document}

\maketitle

\newcounter{asmpctr} % counter for assumptions
\newcounter{asmpadd} % counter for additional assumptions
\setcounter{asmpctr}{\value{enumi}}
\setcounter{asmpadd}{\value{enumi}}

\renewcommand{\qedsymbol}{$\blacksquare$}

\begin{abstract}%
 Multi-index models provide a popular framework to investigate the learnability of functions with low-dimensional structure and, also due to their connections with neural networks, they have been object of recent intensive study. In this paper, we focus on recovering the subspace spanned by the signals via spectral estimators -- a family of methods that are routinely used in practice, often as a warm-start for iterative algorithms. Our main technical contribution is a precise asymptotic characterization of the performance of spectral methods, when sample size and input dimension grow proportionally and the dimension $p$ of the space to recover is fixed. Specifically, we locate the top-$p$ eigenvalues of the spectral matrix and establish the overlaps between the corresponding eigenvectors (which give the spectral estimators) and a basis of the signal subspace. Our analysis unveils a phase transition phenomenon in which, as the sample complexity grows, eigenvalues escape from the bulk of the spectrum and, when that happens, eigenvectors recover directions of the desired subspace.
 The precise characterization we put forward enables the optimization of the data preprocessing, thus allowing to identify the spectral estimator that requires the minimal sample size for weak recovery.
\end{abstract}

%\begin{keywords}%
%  Multi-index models, spectral methods, weak recovery threshold, precise asymptotics, random matrix theory.
%\end{keywords}

\let\citet\cite

%\newcounter{asmpctr} % counter for assumptions
%\setcounter{asmpctr}{\value{enumi}}

\section{Introduction}
\label{sec:intro}

Modern machine learning practices operate on high-dimensional datasets that are believed to possess low-dimensional structures, and multi-index models are a popular statistical framework for studying such scenarios \cite{Li1,Li2,Li3}. Specifically, 
for a labeled dataset $ \cD = \brace{(a_i, y_i)}_{i = 1}^n $, where $ a_i \in \bbR^d $ and $ 
y_i \in \bbR $ denote features and responses respectively, a multi-index model postulates that each data pair follows a generalized regression and the responses are function of a \emph{low-dimensional} projection of the high-dimensional features. 
In formulas: 
\begin{equation}\label{eq:multi}
y_i = q\paren{ \inprod{a_i}{w_1^*}, \cdots, \inprod{a_i}{w_p^*}, \eps_i }, 
\end{equation}
where the link function $ q $ is a known nonlinearity operating on a $p$-dimensional linear transformation of $a_i$, given by $ W^* = \matrix{ w_1^*, & \cdots, & w_p^* } \in \bbR^{d\times p} $, and on additional randomness from $\eps_i$. 

We study the problem of estimating the signals $W^*$ given the labeled dataset $ \cD $ under a Gaussian design where the $ a_i $'s are i.i.d.\ standard Gaussian. We focus on the proportional asymptotic regime where $n,d\to\infty$ with $ n/d \to \delta \in ]0,\infty[ $, while keeping the dimension of the low-dimensional projection $p$ fixed. 
Of particular interest in this work is the \emph{weak recovery} of the subspace spanned by the signals $ w_1^*, \cdots, w_p^* $ which seeks an estimator $ \wh{W} = \matrix{\wh{w}_1, & \cdots, & \wh{w}_p} \in \bbR^{d\times p} $ s.t.\ at least one of $ \wh{w}_i $'s is non-trivially correlated with some linear combination of the signals. 
Formally, there exists $ v\in\bbS^{p-1} $ and some $ \wh{w}_i $ in $ \wh{W} $ such that the normalized correlation (or \emph{overlap}) $ \frac{\abs{\inprod{\wh{w}_i}{W^* v}}}{\normtwo{\wh{w}_i} \normtwo{W^* v}} $ is asymptotically non-vanishing. 

To solve the aforementioned problem, this paper focuses on %considers a family of efficient estimators known as spectral estimators for weak recovery. 
spectral estimators that construct the matrix
\begin{equation}\label{eq:intro}
D = \frac{1}{n} \sum_{i = 1}^n \cT(y_i) a_ia_i^\top,
\end{equation}
from the dataset $\cD$ and then output its top-$p$ eigenvectors. % $ \wh{W}^{\textnormal{s}} = \matrix{ v_1^D, & \cdots, & v_p^D } $. % as the eigenvectors corresponding to the largest $p$ eigenvalues of $D$.
Here, $\cT \colon \bbR \to \bbR$ refers to a user-defined preprocessing function, with popular choices including binary quantization and truncation \cite{Wang_subset,Chen_Candes}, see \Cref{sec:experiments}
for concrete expressions. %also \Cref{itm:subset,itm:trim} on page \pageref{pg:T}. 
Spectral estimators are easy to design, efficient to compute, and effective in practice \cite{Chen_monograph}. %,Couillet_Liao_book}. %They have been applied to single-index  
%The idea behind spectral methods finds its root in \cite{Li2} where it was first developed in the low-dimensional regime with $d$ fixed and $n$ large. 
%Spectral methods have since been applied to single-index models (for which $p = 1$; a.k.a.\ generalized linear models) \cite{lu2020phase}, 
%community detection \cite{abbe2017community},
%clustering \cite{ng2001spectral}, angular synchronization \cite{singer2011angular}, low-rank matrix \cite{Mont_Venk_AOS} and tensor estimation \cite{MR_tensor_PCA}, etc. 
However, such class of methods remains understudied for multi-index models, with existing results falling short of producing exact asymptotics \citet{Chen_Meka} and being restricted to special cases, such as single-index (for which $p=1$, corresponding to generalized linear models \cite{mccullagh2019generalized}) \cite{lu2020phase,mondelli-montanari-2018-fundamental}, polynomial link functions \cite{Chen_Meka}, and mixed regression \cite{mixed-zmv-arxiv}, see also \Cref{sec:related}. 
In contrast, this paper tackles the problem for fully general multi-index models, precisely identifying under what conditions spectral estimators are effective. 

A compelling motivation for the precise analysis of spectral estimators comes from the choice of the preprocessing function $\cT$. In the single-index case, its optimization has led to significant performance gains for Gaussian \cite{mondelli-montanari-2018-fundamental,Luo_Alghamdi_Lu}, correlated \cite{Zhang_COLT} and rotationally invariant \cite{maillard2022construction} designs. Remarkably, the related optimal spectral estimators have also been shown to achieve the computational limits of weak learnability for a class of link functions. In the proportional regime of interest in this work ($n=\Theta(d)$),\footnote{Related, albeit different, perspectives are to consider a sample complexity polynomial in $d$ \cite{damian2023smoothing,Damian} or information-theoretic limits \cite{GLM_PNAS,Aubin_comm_machine}.} such limits have been connected to the performance of Approximate Message Passing (AMP) \cite{mondelli-montanari-2018-fundamental,Maillard} and, more precisely, to the stability of its trivial fixed point. Here, AMP refers to a family of iterative algorithms that is provably optimal among first-order methods \cite{celentano2020estimation,montanari2024statistically} and, in fact, there is significant evidence that AMP is optimal even among all polynomial-time algorithms, i.e., it achieves Bayes-optimal performance unless a statistical-to-computational gap is present \cite{GLM_PNAS,PCA_PNAS, Mont_Venk_AOS,VAMP,VKM}. Most recently, the multi-index case has been considered in \citet{troiani2024fundamental} and a threshold capturing its computational limits has been computed by studying whether AMP is able to improve on a non-trivial initialization, see \Cref{rmk:comp} for a connection with these results. 

\paragraph{Main contributions.} This paper tackles the two main problems mentioned above, i.e., \emph{(i)} the lack of a precise analysis of spectral estimators for multi-index models, and \emph{(ii)} the design of an optimal preprocessing function $\cT$. Specifically, our results are summarized below. 
\begin{enumerate}
    \item For any preprocessing function $\cT$ satisfying mild regularity assumptions, we precisely locate the top-$p$ eigenvalues of the spectral matrix $D$ in \Cref{eq:intro} and characterize the overlaps between the top-$p$ eigenvectors and a basis of the subspace spanned by the signals. Our results describe a phase transition phenomenon, akin to the classical BBP transition in the spiked covariance model \cite{BBAP}, with the spectral outliers of $D$ corresponding to the eigenvectors employed for signal recovery.

    \item Using the above characterization, we identify the optimal preprocessing function for weak recovery. 
    Our optimality result guarantees that no other choice of preprocessing function results in spectral estimators with a lower %weak recovery 
    threshold, and therefore it also implies the suboptimality of existing heuristics to choose $\cT$.  
\end{enumerate}


\section{Related work}
\label{sec:related}

% % these have been covered in the intro, so I commented it 
% \paragraph{Spectral estimators for single-index models.}
% Our work builds on a line of works on spectral estimators for single-index models. 

\paragraph{Multi-index models.}
Several approaches have been proposed to perform statistical inference in multi-index models, including 
structural adaption via maximum minimization \cite{dalalyan2008new}, 
projection pursuit regression \cite{yuan2011identifiability},
techniques from compressed sensing 
\cite{fornasier2012learning}, and the estimation of score functions
\cite{babichev2018slice}. 
Polynomial link function are considered in \citet{andoni2014learning,Chen_Meka}, with the latter work proposing a spectral warm start that requires a sample size $ n \gtrsim d (\log(d))^{\deg(q)} $, where $\deg(q)$ denotes the degree of the link. The area has witnessed a renewed interest in recent years, due to the connection of multi-index models with two-layer neural networks, and a quickly growing line of work has focused on the performance of gradient-based methods. In particular, sample complexity bounds for gradient descent and SQ lower bounds are provided by \citet{damian2022neural} when the link function is polynomial and by  \citet{Oko_GAM} when the multi-index model is given by the sum of single-index models. 
\citet{abbe2022merged,abbe2023sgd} introduce the concept of leap complexity and show that a certain class of staircase functions can be learned via one-pass stochastic gradient descent (SGD) with $n = \Theta(d)$ samples. The leap exponent also appears in \cite{MIM_GF} as the time required by gradient flow to escape a saddle point. 
A deterministic equivalent of the SGD dynamics is proved in \citet{collins2024hitting}.  
\citet{Ren_Lee} provides an algorithm that recovers orthogonal multi-index models with a sample complexity matching the information exponent 
\cite{arous2021online}. 
We note that none of these methods is able to pin-point exactly the sample complexity required to recover a multi-index model, which constitutes the focus of our work and is achieved via the class of spectral methods reviewed below.


\paragraph{Spectral estimators.} The idea behind spectral methods finds its root in \cite{Li2} where it was first developed in the low-dimensional regime with $d$ fixed and $n$ large. 
Spectral estimators have since been applied to a variety of problems in statistical inference, including community detection \cite{abbe2017community}, clustering \cite{ng2001spectral}, angular synchronization \cite{singer2011angular}, principal component analysis (PCA) \cite{Mont_Venk_AOS} and tensor estimation \cite{MR_tensor_PCA}. 
 In the setting of Gaussian design and proportional scaling between $n$ and $d$, the precise asymptotic performance of spectral methods for single-index models is characterized in \cite{lu2020phase} by random matrix theoretic means. 
 The optimal weak recovery threshold and optimal asymptotic performance (in terms of overlap) are  identified in \citet{mondelli-montanari-2018-fundamental} and \cite{Luo_Alghamdi_Lu}, respectively. 
 The above results are extended in \cite{dudeja-2020-rigorous-analysis} to subsampled Haar designs where $ A = \matrix{a_1, & \cdots, & a_n}^\top \in \bbR^{n\times d} $ is obtained by truncating a random orthogonal matrix, and in \cite{Zhang_COLT} to correlated Gaussian designs where the $ a_i $'s are i.i.d.\ Gaussian with a given covariance. Rotationally invariant designs are considered in \cite{maillard2022construction}, which conjectures the form of the optimal spectral estimator using a linearization of AMP and the analysis of the Bethe Hessian. Such conjecture is partly addressed by \citet{Zhang_COLT}, when the covariance of the $a_i$'s is rotationally invariant. 
  We note that, in the single-index case, optimal spectral methods match computational thresholds obtained from the stability of AMP \cite{mondelli-montanari-2018-fundamental,Zhang_COLT} and, in special cases, information-theoretic thresholds as well \cite{mondelli-montanari-2018-fundamental}. An optimally-designed spectral estimator is able to meet the information-theoretic limits of weak recovery also for a class of heteroscedastic PCA problems \cite{MatrixDenoising}. Most closely related to our setting is \cite{mixed-zmv-arxiv}: the authors 
 consider mixtures of single-index models 
 with independent signals and provide precise asymptotics for spectral estimators by using a mix of tools from random matrix theory and the theory of AMP. In contrast, our approach is purely random matrix theoretic, and it allows us to handle a general class of multi-index models with arbitrary correlation among the signals. 

\section{Preliminaries}
\label{sec:prelim}

\paragraph{Notation.} Given a positive integer $n$, we use the shorthand $[n]:=\{1, \ldots, n\}$. We denote by $0_n$ the vector of length $n$ with all zeros. For a symmetric matrix $M$, we denote its Moore-Penrose pseudo-inverse as $M^\dagger$, the set of all its eigenvalues as $\Lambda^M$, its $i$-th largest eigenvalue as $\lambda_i^M$ and the corresponding $i$-th eigenvector of unit norm as $v_i^M$. 
We use $ (e_i^{(d)})_{i\in[d]} $ to denote the canonical basis of $ \bbR^d $ and suppress the superscript whenever there is no confusion. 
Unless otherwise specified, all limits of sequences of random quantities are computed in an almost sure sense as $n,d\to\infty$. 

\paragraph{Multi-index models and weak recovery.} We consider the problem of estimating $p$ signals using $n$ responses $(y_i)_{i\in [n]}$ generated i.i.d.\ from \eqref{eq:multi},
where $ \eps_i \in \bbR $ accounts for noise and $q:\bbR^{p+1}\to \bbR$ is the link function. 
We denote by $ A \coloneqq \matrix{ a_1, & \cdots, & a_n }^\top \in \bbR^{n\times d} $ the design matrix, by $ W^* \coloneqq \matrix{ w_1^*, & \cdots, & w_p^* } \in \bbR^{d\times p} $ the signal matrix, and by $ y \coloneqq \matrix{y_1, & \cdots, & y_n}^\top \in \bbR^n $ the response vector. 
Throughout the paper, we impose the following assumptions. 

\begin{enumerate}[label=(A\arabic*)]
\setcounter{enumi}{\value{asmpctr}}

    \item \label[asmp]{asmp:A} The design matrix $ A \in \bbR^{n\times d} $ contains i.i.d.\ standard Gaussian elements, i.e., $ A_{i,j} \iid \cN(0,1) $. 

    \item \label[asmp]{asmp:W*} The signals $ (w_i^*)_{i\in[p]} $ are linearly independent, have unit norm and, if random, they are independent of $A$. 

    \item \label[asmp]{asmp:proportional} $p\ge1$ is fixed and $ n,d \to \infty $ s.t.\ $ n/d\to\delta\in]0,\infty[ $. 

    \item \label[asmp]{asmp:noise} $ \eps_1, \cdots, \eps_n $ are independent of $ A, W^* $, and they are i.i.d.\ according to a distribution $ P_\eps $ on $\bbR$ with finite first two moments. 

\setcounter{asmpctr}{\value{enumi}}
\end{enumerate}

The linear independence requirement in \Cref{asmp:W*} is mild.
For the purpose of subspace recovery (formally defined below), the presence of linearly dependent signals does not change the recoverability of a given estimator. 

%To estimate $ p $ linearly independent signals, it is expected that $p$ linearly independent estimators are needed. For a given collection of estimators $ \wh{W} = \matrix{\wh{w}_1, & \cdots, & \wh{w}_p} \in \bbR^{d\times p} $, we consider the following notion of weak recovery. 

\begin{definition}[Weak recovery]
    \label{def:weak_rec}
    Consider the model \Cref{eq:multi}. 
    Let $ \wh{W} \equiv \wh{W}(A, y) = \matrix{\wh{w}_1, & \cdots, & \wh{w}_p} \in \bbR^{d\times p} $ be an estimator such that $ \normtwo{\wh{w}_i} = 1 $ for all $i\in[p]$. 
    We say that $ \wh{W} $ weakly recovers the subspace $ \spn\brace{ w_1^*, \cdots, w_p^* } $ if 
    \begin{align}
        \max_{v\in\bbS^{p-1}} \brace{ \liminf_{d\to\infty} \frac{\normtwo{\wh{W}^\top W^* v}}{\normtwo{W^* v}} } &> 0 , \notag 
    \end{align}
    where the almost sure limit is taken with respect to the proportional scaling in \Cref{asmp:proportional}. 
\end{definition}
In words, an estimator $ \wh{W} $ weakly recovers $ \cW \coloneqq \spn\brace{ w_1^*, \cdots, w_p^* } $ if at least one of the $ \wh{w}_i $'s attains non-vanishing correlation with \emph{some} linear combination of the signals $ w_1^*, \cdots, w_p^* $, or equivalently, the column span of $ \wh{W} $ (asymptotically) does not lie inside the orthogonal complement of the column span of $ W^* $.  

Other notions of weak recovery for multi-index models have been considered in the literature. 
For instance, \citet{troiani2024fundamental} discusses the weak recovery of the \emph{whole} subspace $\cW$, i.e., 
\begin{align}
    \min_{v\in\bbS^{p-1}} \brace{ \liminf_{d\to\infty} \frac{\normtwo{\wh{W}^\top W^* v}}{\normtwo{W^* v}} } > 0 . \label{eqn:weak_rec_wole} 
\end{align}
This requirement is clearly stronger than \Cref{def:weak_rec} since \emph{every} vector in $\cW$ needs to be weakly recovered by some $ \wh{w}_i $ in $ \wh{W} $. 
In this work, we focus exclusively on \Cref{def:weak_rec}. 

\paragraph{Spectral estimators.}

For any fixed preprocessing function $\cT:\bbR\to \bbR$, define for each $i\in[n]$, 
$ z_i \coloneqq \cT(y_i)$
and set $Z \coloneqq \diag \matrix{ z_1, & \cdots, & z_n } \in \bbR^{n\times n}$.
Furthermore, we define the matrix $D \equiv D_n \in \bbR^{d\times d}$ as
\begin{align}
    D_n = \frac{1}{n}A^\top Z A = \frac{1}{n}\sum_{i=1}^{n}z_ia_ia_i^\top. \label{eqn:D}
\end{align}
The spectral estimator then outputs the eigenvectors corresponding to the $p$ largest eigenvalues of $D$, i.e., $ 
\matrix{ v_1^D, & \cdots, & v_p^D } $. 


We now make a simplifying assumption on $ W^* $ without loss of generality. 
Note that, for any $ W^* $ subject to \Cref{asmp:W*}, by orthogonal invariance of the random design matrix $A$, the law of %the stochastic process 
$ \brace{ \frac{\abs{\inprod{v_i^D}{W^* v}}}{\normtwo{W^* v}} : i\in[p], v\in\bbS^{d-1}} $ remains unchanged under the rotation mapping $ W^* \mapsto W^* O $ for any orthogonal matrix $ O \in \bbR^{p\times p} $. 
Therefore, for convenience, we take the unique rotation $O$ that, for all $i$, maps $ w_i^* $ to $\sum_{j = 1}^i c_{i,j} e_j$, with $\sum_{j = 1}^i c_{i,j}^2 = 1$, and  study $ \brace{ \abs{\inprod{v_i^D}{e_j}} : i,j\in[p] } $, i.e., the overlap of the eigenvectors with the basis $\{e_1, \cdots, e_p\}$ spanned by the rotated signals. We note that, after this rotation,  
%For each $ i\in[p] $, we take $ w_i^* \in \bbS^{d-1} $ of the form: 
%\begin{align}
%    w_i^* &= \sum_{j = 1}^i c_{i,j} e_j , \qquad 
%    \textnormal{where } \sum_{j = 1}^i c_{i,j}^2 = 1 . \notag 
%\end{align}
%In particular, 
$ W^*$ %$ = \matrix{w_1^*, & \cdots, & w_p^*} $ 
can be written as 
\begin{align}
    W^* &= \matrix{\wt{W}^* \\ 0_{(d-p)\times p}} \in \bbR^{d\times p},\qquad     \wt{W}^* = \matrix{
        c_{1,1} & c_{2,1} & \cdots & c_{p,1} \\
                & c_{2,2} & \cdots & c_{p,2} \\
                &         & \ddots & \vdots  \\
                &         &        & c_{p,p}
    } \in \bbR^{p\times p} . \label{eqn:W*} 
\end{align}
%for an upper triangular matrix $ \wt{W}^* $ given by 
%\begin{align}
%    \wt{W}^* &= \matrix{
%        c_{1,1} & c_{2,1} & \cdots & c_{p,1} \\
%                & c_{2,2} & \cdots & c_{p,2} \\
%                &         & \ddots & \vdots  \\
%                &         &        & c_{p,p}
%    } \in \bbR^{p\times p} . \notag 
%\end{align}
We further
define the random variables 
\begin{align}
    (s, \eps) \sim \cN(0_p, I_p) \ot P_\eps , \quad 
    y = q((\wt{W}^*)^\top s, \eps) , \quad 
    z = \cT(y) , \label{eqn:RV}
\end{align}
and make the following assumptions on the preprocessing function $\cT$. % satisfying the following condition. 

\begin{enumerate}[label=(A\arabic*)]
\setcounter{enumi}{\value{asmpctr}}
    \item \label[asmp]{asmp:T} $\cT$ is bounded and $\prob{z = 0}<1$. 
\setcounter{asmpctr}{\value{enumi}}
\end{enumerate}

\section{Main results}
\label{sec:results}

Consider the model \Cref{eq:multi} under \Cref{asmp:A,asmp:W*,asmp:proportional,asmp:noise}. 
Our first result accurately locates, in the high-dimensional limit, the $p$ largest eigenvalues of the matrix $D$ in \Cref{eqn:D} for any preprocessing function $\cT$ subject to \Cref{asmp:T}, thereby unveiling a spectral phase transition phenomenon. 
To present it, a sequence of definitions is needed. 

Let $ \sT $ be the collection of preprocessing functions $\cT$ subject to \Cref{asmp:T}. 
For any $ \cT \in \sT $, let $ z = \cT(y) $ as in \Cref{eqn:RV} and let 
$\tau = \inf\{c:\prob{z\leq c} = 1\}$ 
be the right edge of the support of $z$ (by \Cref{asmp:T}, $ \tau < \infty $). 
Define $\psi_\delta \colon ]\tau,\infty[ \to \bbR$ and its point of minimum as
\begin{equation}
    \psidelta(\lambda) \coloneqq \lambda\paren{\frac{1}{\delta}+\expt{\frac{z}{\lambda-z}}}, \qquad \lambdabardelta \coloneqq \argmin_{\lambda>\tau}\psidelta(\lambda). \notag 
\end{equation}
Note that $\psi_\delta$ is convex and, thus, its minimum is unique. % at 
%$$$$
Finally, define $\zetadelta\colon \bbR \to \bbR$ and $ R^\infty \colon ]\tau,\infty[ \to \bbR^{p\times p} $ as 
\begin{align}
    \zetadelta(\lambda) \coloneqq \psidelta(\max\{\lambdabardelta, \lambda\}), \qquad R^\infty(\alpha) \coloneqq \expt{\frac{\alpha ss^\top z}{\alpha-z}} ,
    \label{eqn:zeta_R}
\end{align}
where $s, z$ are jointly distributed according to \Cref{eqn:RV}. %, and denote $\lambda_i^\infty (\alpha) = \lambda_i(R^\infty(\alpha))$
%the $i$-th largest eigenvalue of $ R^\infty(\alpha) $. 

\begin{theorem}\label{thm:eigvalconv}
    Let $ \cT \colon \bbR \to \bbR $ be a preprocessing function subject to \Cref{asmp:T}, and let $ D\in\bbR^{d\times d} $ be defined in \Cref{eqn:D}.
    %    Let $ \cT \colon \bbR \to \bbR $ be a preprocessing function subject to \Cref{asmp:T}, and let $ D\in\bbR^{d\times d} $ be defined in \Cref{eqn:D}. Denote by
    Let $\alpha_1\geq \dots\ \geq \alpha_j > \tau$ (for some $j\in[p]$) be all the solutions to 
    \begin{equation}\label{eq:master_eq2}
    \det\paren{\zetadelta(\alpha)I-R^\infty(\alpha)}=0.    
    \end{equation}
    %to the equation
    %    \begin{equation}
    %        \det\paren{\zetadelta(\alpha)I_p-R^\infty(\alpha)}=0.
    %    \end{equation}
    Then, for the top $j$ eigenvalues of $D$, it holds that
    \begin{equation}\label{eq:outsidebulkeigconv}
        \lambda_1^D,\dots,\lambda_j^D \asconv \zetadelta(\alpha_1), \dots, \zetadelta(\alpha_j),
    \end{equation}
    and for the remaining $p-j$ eigenvalues, it holds that
    $$\lambda_{j+1}^D,\dots,\lambda_p^D \asconv \zetadelta(\lambdabardelta).$$ 
\end{theorem}
In words, \Cref{thm:eigvalconv} shows a phase transition for the $j$ largest eigenvalues of $D$ as the (normalized) sample complexity $\delta$ varies. In fact, by the definition of $ \zeta_\delta(\cdot) $ in \Cref{eqn:zeta_R}, \Cref{eq:outsidebulkeigconv} implies that, for any $ k\in[j] $, if $ \alpha_k \le \lambdabardelta $, then $ \lambda_i^D $ asymptotically coincides with $\zetadelta(\lambdabardelta)$, which corresponds to the right edge of the bulk of the spectrum of $D$; conversely, if $ \alpha_k > \lambdabardelta $, then the asymptotic value of $ \lambda_i^D $ is strictly larger than the right edge, thereby forming a spectral outlier. 
This phenomenon mirrors the classical BBP phase transition for the spiked covariance model \cite{BBAP}. 

% The result on the eigenvalues of $D$ is accompanied by one on its eigenvectors, as stated below. 

We note that \eqref{eq:master_eq2} has at most $p$ solutions in $]\tau,+\infty[$. Furthermore, if $\cT$ satisfies 
    \begin{equation}\label{assmp:psol}
        \inf_{\norm{2}{x}=1}\lim_{\alpha\to\tau^+}\expt{\frac{\alpha z\inprod{s}{x}^2}{\alpha-z}} = +\infty,
    \end{equation}
     we are guaranteed that \eqref{eq:master_eq2} has exactly $p$ solutions. Both statements are proved in \Cref{prop:exactlyp} deferred to \Cref{app:asymptotbehav}. %as proved in \Cref{subsec:asymptotic behavior}. 
     The condition in \eqref{assmp:psol} provides a natural generalization of the assumption made for $p=1$ in previous work, see Equation (82) in \cite{mondelli-montanari-2018-fundamental}.

%\begin{remark}\label{rem:psolutions}
%    In the statement of \Cref{thm:eigvalconv}, it is implicitly said that  \eqref{eq:master_eq2} has at most $p$ solutions in $]\tau,+\infty[$. However, if $\cT$ satisfies 
%    \begin{equation}\label{assmp:psol}
%        \inf_{\norm{2}{x}=1}\lim_{\alpha\to\tau^+}\expt{\frac{\alpha z\inprod{s}{x}^2}{\alpha-z}} = +\infty,
%    \end{equation}
%     we are guaranteed that the \eqref{eq:master_eq2} has exactly $p$ solutions, as proved in \Cref{subsec:asymptotic behavior}. Assumption \eqref{assmp:psol} would be a natural generalization of the assumption made for 1-dimensional case as in \cite[(82)]{mondelli-montanari-2018-fundamental}.
%\end{remark}

Our second result characterizes the asymptotic performance, in terms of overlaps, of the eigenvectors corresponding to the spectral outliers of $D$. 

\begin{theorem}\label{thm:main}
    In the setting of \Cref{thm:eigvalconv}, let $\alpha_k = \alpha_{k+1} = \cdots = \alpha_{k+m-1} $ be solutions to \Cref{eq:master_eq2} of multiplicity $m$, i.e., $ \alpha_{k-1} > \alpha_k > \alpha_{k+m-2} $ whenever $ k\ge2, k+m-2\le j $. 
    Let $E_k^\infty\subset\bbR^p$ be the corresponding $m$-dimensional eigenspace of $R^\infty(\alpha_k)$. If $\alpha_k>\lambdabardelta$, then %it holds 
    \begin{equation}\label{eq:liminfconv}
        \max_{l\in[p]}\liminf_{d\to\infty}\sum_{i=k}^{k+m-1}\abs{\inprod{v_i^D}{e_l^{(d)}}}^2>0.
    \end{equation}
    More precisely, under the additional assumption that either $m=1$ or the eigenspace $E_k^\infty$ stays invariant in a neighbourhood of $\alpha_k$, for any $ l\in[p] $, 
    \begin{align}
        \sum_{i=k}^{k+m-1}\abs{\inprod{v_i^D}{e_l^{(d)}}}^2\asconv \frac{\zetadelta'(\alpha_k) \sum_{i=k}^{k+m-1}\abs{\inprod{h_i^\infty}{e_l^{(p)}}}^2}{\zetadelta'(\alpha_k)+{h_k^\infty}^\top \frac{d}{d\alpha}R^\infty(\alpha_k)h_k^\infty},
        \label{eqn:eq:liminfconv_mult}
    \end{align}
    where $\brace{h_i^\infty : k \le i\le k+m-1}\subset\bbR^p$ is an orthonormal basis of $E_k^\infty$ and $\frac{d}{d\alpha}R^\infty(\cdot)$ denotes the entry-wise derivative of the matrix 
    $R^\infty(\cdot)$.
\end{theorem}

In words, \Cref{thm:main} shows that, as long as $ \lambda_k^D $ is an outlier (which is guaranteed by \Cref{thm:eigvalconv} as $ \alpha_k > \lambdabardelta$), the spectral estimators given by the corresponding eigenvectors $ V_k = \matrix{v_k^D, & \cdots, & v_{k+m-1}^D} \in \bbR^{d\times m} $ weakly recover $ \cW=\spn\brace{e_1^{(d)}, \cdots, e_p^{(d)}} $. 
The precise characterization in \Cref{eqn:eq:liminfconv_mult} expresses the asymptotic (squared) overlap $ \normtwo{V_k^\top e_l^{(d)}}^2 $ in terms of $p$-dimensional equations. 

Let us further elaborate on the invariance of the eigenspace $E_k^\infty$ required for \Cref{eqn:eq:liminfconv_mult} to hold. This means that, if $E_k^\infty$ is an eigenspace of $R^\infty(\alpha_k)$, then it is also an eigenspace of $R^\infty(\alpha_k+\Delta)$ for all sufficiently small $\Delta$. The assumption is satisfied, for example, by models in which the eigenvalue multiplicity arises from the permutation-invariance of the link function $q$, see \Cref{prop:invlink} deferred to \Cref{app:invariance}.

Taken together, \Cref{thm:eigvalconv,thm:main} generalize earlier work by \cite{mondelli-montanari-2018-fundamental} on the single-index model and by \cite{mixed-zmv-arxiv} on mixtures of single-index models with independent signals. The independence of the signals is crucial to the approach in \cite{mixed-zmv-arxiv} which decomposes $D$ in \eqref{eqn:D} as the free sum of matrices each corresponding to one the signals. To circumvent this difficulty, we pursue a $p$-dimensional analog of the strategy in \cite{mondelli-montanari-2018-fundamental}, by showing that the eigenvalues of $D$ solve a certain fixed point equation (\Cref{prop:eigvalrec}) and the eigenvectors can be related to a $p\times p$ matrix (\Cref{prop:eigenvec}).
We then proceed to study the limiting behaviors (as $n,d\to\infty$) of the $p$-dimensional objects which eventually leads to the claimed asymptotic characterizations. The proof is outlined in \Cref{sec:pftec}, with several auxiliary results deferred to \Cref{sec:auxpf,app:asymptotbehav}.

We note that our result on overlaps in \Cref{thm:main} concerns the basis vectors. 
However, unless the signals have vanishing correlation, one needs additional side information to assemble the overlaps with the basis into overlaps %it is unclear how 
%to obtain asymptotic overlaps 
with the signals $ (w_i^*)_{i\in [p]} $, due to the sign ambiguity of eigenvectors ($ v $ is an eigenvector if and only if $ -v $ is one). 

Given the above characterization, our third main result identifies an optimal preprocessing function which allows the resulting spectral estimator to attain the lowest weak recovery threshold as per \Cref{def:weak_rec}. 
Formally, for any $\delta>0$, let 
\begin{align}
    \sT_\delta &\coloneqq \brace{ \cT \in \sT : \max\brace{ \alpha : \alpha \textnormal{ solves } \Cref{eq:master_eq2} } > \lambdabardelta } \notag 
\end{align}
be the set of preprocessing functions which achieve weak recovery at a fixed aspect ratio $\delta$. 
Then, define the optimal weak recovery threshold as 
\begin{align}\label{eq:deltacdef}
    \delta_c &\coloneqq \inf\brace{ \delta \in ]0, \infty[ : \sT_\delta \ne \emptyset } . 
    % \inf_{\cT \in \sT} \delta_c(\cT) , \notag 
\end{align}
% and denote by $\sT^*$ the set of minimizers for $\delta_c$. 
Finally, for random variables $(s,y)\in \bbR^p \times \bbR$ jointly distributed according to \Cref{eqn:RV}, we use $ p(y \mid s) $ to denote the conditional density of $y$ given $s$. 

\begin{theorem}\label{thm:opt}
    The optimal weak recovery threshold $\delta_c$ equals 
    \begin{align}\label{eq:specweak}
        \delta_c &= \brack{ \max_{u\in\bbS^{p-1}}\int_{\bbR}\frac{\paren{\expt[s]{p(y \mathrel{\vert} s)\cdot(\inprod{s}{u}^2-1)}}^2}{\expt[s]{p(y \mathrel{\vert} s)}}dy }^{-1}
        , 
    \end{align}
     where expectations are intended over $ s\sim\cN(0_p,I_p) $.    
    Furthermore, denote by $u_c\in\bbS^{p - 1}$ a maximizer in the above expression and let
    \begin{align}\label{eq:opt}
        \cT^*(y) &\coloneqq 1 - \frac{\expt[s]{p(y \mathrel{\vert} s)}}{\expt[s]{p(y \mathrel{\vert} s)\cdot\inprod{s}{u_c}^2}} , \qquad
        \cT_\delta^*(y) \coloneqq \frac{\sqrt{\delta_c} \cdot \taustar(y)}{\sqrt{\delta}-(\sqrt{\delta}-\sqrt{\delta_c})\cdot \taustar(y)} .  
    \end{align}
    Then, for any $ \delta > \delta_c $, $ \cT_\delta^* \in \sT_\delta $. 
\end{theorem}

In words, \Cref{thm:opt} shows that 
the preprocessing in \Cref{eq:opt} leads to weak recovery of the signal subspace with a sample complexity that is minimal \emph{among all spectral methods}. Similarly to \Cref{thm:eigvalconv,thm:main}, this result generalizes earlier work  \citet{mondelli-montanari-2018-fundamental,mixed-zmv-arxiv} to the multi-index case with arbitrarily correlated signals. The proof of \Cref{thm:opt} is deferred to \Cref{app:pfopt}.

The design of the optimal preprocessing function does not require the knowledge of $ \wt{W}^* $, but only of the limiting sample covariance matrix of the signals $(\wt{W}^*)^\top \wt{W}^*$. In fact, $ \cT^*(y) $ remains unchanged under $ \wt{W}^* \mapsto O \wt{W}^* $ for any orthogonal matrix $ O\in\bbR^{p\times p} $. 
Indeed, one can verify that under the above mapping, $ \expt[s]{p(y \mid s)} $ remains unchanged and $ u_c \mapsto O u_c $. 
We can then take $ O = ({{}\wt{W}^*}^\top \wt{W}^*)^{-1/2} {{}\wt{W}^*}^\top $ %(which is orthogonal) 
and equivalently define $y = q\paren{(\wt{W}^*)^\top O^\top s, \eps}
    = q\paren{\brack{(\wt{W}^*)^\top \wt{W}^*}^{1/2} s, \eps}$.
%Therefore, evaluating $ \cT_\delta^*(y) $ only requires the limiting sample covariance matrix of the signals $ \Sigma^{1/2} \coloneqq \lim_{d\to\infty} \brack{ (W^*)^\top W^* }^{1/2} = \brack{(\wt{W}^*)^\top \wt{W}^*}^{1/2} $. 

\begin{remark}\label{rmk:comp}
The threshold identified in Lemma 4.1 of \cite{troiani2024fundamental} is given by 
    \begin{equation}\label{eq:compweak}
    \left(\sup_{\substack{M\succ 0_{p\times p}\\ \normf{M}=1}}\normf{\cF(M)}\right)^{-1},       
    \end{equation}
where %$\cF$ is an operator defined as 
    %$
    $\cF(M) \coloneq \expt[y]{E(y)ME(y)}$
and $E(y)\coloneq \expt[s]{ss^\top - I_p\mathrel{\vert} y}$. One can readily prove that \Cref{eq:compweak} is equal to \Cref{eq:specweak} when the $\sup$ is achieved by a rank-1 matrix, and we also verify the equality for the link function $q(s_1, s_2, \eps) = s_1s_2$ considered in the numerical experiments in \Cref{subsec:prod}.     
\end{remark}



\section{Numerical results}

\label{sec:experiments}

We complement our theoretical results with numerical simulations. 
In both \Cref{fig:mpr,fig:prod}, we consider instances of the model \Cref{eq:multi} with $d = 1500$ and $p = 2$. 
The performance of the spectral estimator is measured by the overlap between an eigenvector $ v_i^D $ and a parameter vector $ w_j^* $ (for $ 1 \le i,j\le p = 2 $). 
Each data point labeled by $ \times $ (and also by $ \circ $ in \Cref{fig:prod}) is obtained by averaging over $10$ i.i.d.\ trials, and error bars are reported at $1$ standard deviation. 
The corresponding theoretical predictions are plotted using solid curves. 
Both numerical and theoretical values of overlaps are plotted as a function of the aspect ratio $\delta$ and captioned `num' and `thy' respectively. 

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.45\textwidth]{fig/prod.pdf}
    \caption{$q(s_1, s_2, \eps) = s_1s_2$. The signals $ w_1^*, w_2^* $ are asymptotically orthogonal and the optimal preprocessing function in \Cref{eqn:T_prod} is used. The overlaps $ \abs{\inprod{v_1^D}{w_1^*}} , \abs{\inprod{v_1^D}{w_2^*}} $ along with their theoretical predictions from \Cref{thm:main} are plotted as a function of $ \delta $.
}
    \label{fig:prod}
\end{figure}


In \Cref{fig:prod}, the link function is given by $ q(s_1, s_2, \eps) = s_1 s_2 $ and the preprocessing function $\cT$ is taken to be the optimal one, see \Cref{subsec:prod} for the derivation. The signals are sampled i.i.d.\ from independent isotropic Gaussians, i.e., $ (w_1^*, w_2^*) \sim \cN(0_d, I_d / d) \ot \cN(0_d, I_d / d) $. 
    Due to the permutation invariance of the model, for any eigenvector $ v_i^D $, the overlap $ \abs{\inprod{v_i^D}{w_j^*}} $ is asymptotically the same for every $ j\in \{1, 2\}$. 
    As there is a single outlier in the model,     we only plot $ \abs{\inprod{v_1^D}{w_1^*}} $ and $ \abs{\inprod{v_1^D}{w_2^*}} $. 

In \Cref{fig:mpr}, we consider a two-component mixed phase retrieval model: $ q(\xi_1, \xi_2, \eps) = \abs{\xi_\eps} $, where the mixing variable $ \eps $ is $\brace{1,2}$-valued with $ \prob{\eps = 1} = 1 - \prob{\eps = 2} = 0.6 $. 
    The prior distribution on $ w_1^*, w_2^* $ is chosen to be a bivariate correlated Gaussian, i.e., 
    \begin{align}
        \matrix{w_{1,k}^* \\ w_{2,k}^*} &\iid \cN\paren{\matrix{0 \\ 0}, \matrix{1 & \rho \\ \rho & 1}} , \qquad \forall k\in[d] , \notag 
    \end{align}
    for $ \rho = 0.3 $. 
    The performance of spectral estimators %\Cref{eqn:D} 
    is compared among five choices of preprocessing functions: \emph{(i)}
%    \begin{enumerate}\label{pg:T}
%        \item\label{itm:YCS} 
the quadratic function $ \cT(y) = \min\brace{ y^2, 10 } $ used in \cite{Yi_MLR} for mixed linear regression; % $ q(\xi_1, \xi_2, \eps) = \xi_\eps $; 
\emph{(ii)} 
%
%        \item\label{itm:trim} Tt
the trimming function $ \cT(y) = y^2 \indicator{ y^2 \le 7 } $ considered in \cite{Chen_Candes} which zeros out labels with large magnitude;  
\emph{(iii)} 
%        \item\label{itm:subset} T
the subset function $ \cT(y) = \indicator{ y^2 > 2 } $ considered in \cite{Wang_subset} which quantizes the labels to binary values; \emph{(iv)}  
%
%        \item\label{itm:nonmixed} T
the optimal preprocessing function $ \cT(y) = \min\brace{ 1 - 1/y^2, -10 } $ for the \emph{non-mixed} phase retrieval model $ q(\xi_1, \xi_2, \eps) = \abs{\xi_1} $ derived in \cite{Luo_Alghamdi_Lu}; and \emph{(v)}
%
%        \item\label{itm:opt} T
the optimal preprocessing function that is guaranteed by our theory to maximize the asymptotic overlap $ \abs{\inprod{v_1^D}{w_1^*}} $, see \Cref{subsec:mp} for details. 
 %       
%    \end{enumerate}
%\end{itemize}
%
The validity of our theoretical predictions relies on an assumption on the boundedness of $ \cT $ (see \Cref{asmp:T}). 
We therefore truncate the functions under consideration, whenever necessary. 
The truncation levels in \emph{(i)} and \emph{(iv)} are taken to be large enough so as not to significantly affect the performance; 
the truncation/quantization levels in \emph{(ii)} and \emph{(iii)} are % 7 $ in \Cref{itm:subset} and $ 2 $ in \Cref{itm:nonmixed} are 
%the optimal ones 
taken from \cite{mondelli-montanari-2018-fundamental}. %, where they are optimized % which are optimized therein 
%within the set $ \brace{0.25i : i\in[40]} $ to yield the smallest empirical weak recovery threshold on the \emph{non-mixed} phase retrieval model. 

\begin{figure}[p]
    \centering
    \includegraphics[width=0.95\textwidth]{fig/mpr.pdf}
    \caption{Mixed phase retrieval $ q(\xi_1, \xi_2, \eps) = \abs{\xi_\eps} $ with $ \prob{\eps = 1} = 1 - \prob{\eps = 2} = 0.6 $. 
    The signals $ w_1^*, w_2^* $ have asymptotic correlation $ \inprod{w_1^*}{w_2^*} \asconv 0.3 $. 
    Five preprocessing functions detailed in \Cref{sec:experiments} are considered. 
    The overlaps $ \brace{ \abs{\inprod{v_i^D}{w_j^*}} : 1\le i,j\le 2 } $ along with their theoretical predictions from \Cref{thm:main} are plotted as a function of $\delta$. 
    Our optimal preprocessing function in \Cref{eqn:T_mpr} attains the lowest weak recovery threshold, as ensured by the optimality result in \Cref{thm:opt}. 
    Furthermore, it also attains the highest overlap among alternatives. 
     }
    \label{fig:mpr}
\end{figure}

A few remarks %regarding the numerical results i
on \Cref{fig:mpr,fig:prod} are in order. First, 
%\begin{itemize}
%    \item T
the numerical and theoretical results exhibit accurate agreement even for %mildly large dimensions 
$ d = 1500 $, suggesting a rapid rate of convergence. 
%
%    \item 
Second, for mixed phase retrieval, %as can be seen from the expression \Cref{eqn:T_mpr} of the optimal preprocessing function, 
the mixing effect and the correlation between signals %, quantified by $\eta$ and $\rho$ respectively, 
have crucial effects on the design of the optimal preprocessing function. 
    Naively applying the optimal preprocessing function for \emph{non-mixed} phase retrieval results in poor performance of $ v_2^D $ which achieves positive overlap with either signal only if $ \delta > 19 $ (see the bottom row of \Cref{fig:mpr}). 
    In contrast, the weak recovery threshold of $ v_2^D $ resulting from our proposed choice of preprocessing % \Cref{eqn:T_mpr}
    is less than $7$. 
%
%    \item 
    % We remind the readers of the notion of optimality of our preprocessing function. 
 Third,  %  the functions 
while our choice of preprocessing is designed to minimize %\Cref{eqn:T_prod,eqn:T_mpr} focuses on minimizing %specialized from \Cref{thm:opt} are derived to minimize 
the weak recovery threshold of $ v_1^D $, 
 %   However, 
% as noted in \Cref{fig:mpr}, the performance given by our preprocessing function %\Cref{eqn:T_mpr} 
the performance is also competitive in terms of %the value of the 
overlaps, outperforming all alternatives.  %
%    Indeed, it attains the highest value among other choices under comparison. 
%    Understanding refined notions of optimality for our proposed functions is left for future investigation. 
%\end{itemize}



\section{Proof techniques}\label{sec:pftec}

\subsection{Equivalent spectral characterization}
\label{subsec:general matrix}

Let us write 
$a_i = \matrix{ s_i& u_i }^\top$, with $s_i\iid \cN(0,I_p)$ and $u_i\iid \cN(0,I_{d-p})$. Thus, as $w_i^* \in \spn\{e_1^{(d)},\dots,e_p^{(d)}\}$ (see \Cref{eqn:W*}), $y_i$ only depends on $s_i$ and, more precisely, %we have that % Namely,
%\begin{align}
    $y_i = %q\paren{ \inprod{a_i}{w^*_1}, \cdots, \inprod{a_i}{w^*_p}, \eps_i } = 
    q\paren{ \inprod{s_i}{w^*_1}, \cdots, \inprod{s_p}{w^*_p}, \eps_i }$, 
with the abuse of notation that the $s_i$'s are now vectors in $\bbR^d$ with the last $p-d$ coordinates set to $0$.
Extending the notation to matrices, we have that $A  = \matrix{S& U},$
where $ S \coloneqq \matrix{ s_1 & \cdots & s_n }^\top \in \bbR^{n\times p} $ and $U \coloneqq \matrix{ u_1 & \cdots & u_n }^\top \in \bbR^{n\times (d-p)}$. Thus, we can re-write the spectral matrix $D_n$ in \eqref{eqn:D} as 
%Using this notation
\begin{align}\label{eq:Ddef}
    D_n = \frac{1}{n}A^\top Z A  &= \frac{1}{n} \matrix{ S^\top \\ U^\top} Z  \matrix{ S & U}
                                 = \frac{1}{n} \matrix{ S^\top Z S & S^\top Z U\\
                                                         U^\top Z S & U^\top Z U}=  \matrix{ a & q^\top\\ 
                                             q & P}, 
\end{align}
where $a\coloneqq \frac{1}{n}S^\top Z S \in \bbR^{p\times p}$, $q \coloneqq \frac{1}{n}U^\top Z S\in \bbR^{(d-p)\times p}$ and $P \coloneqq \frac{1}{n }U^\top Z U\in \bbR^{(d-p)\times (d-p)}$.\\



In this section, we work with a generic matrix $D$ of the form in the RHS of \eqref{eq:Ddef},
such that $a, P$ %\in \bbR^{p\times p}$ and $P\in\bbR^{(d-p)\times (d-p)}$ 
are symmetric %matrices, 
and, for all eigenvectors $v_i^a$ of $a$, it holds that $qv_i^a\neq 0$ (this is the case for the matrix $D_n$ in \Cref{eq:Ddef} as showed in \Cref{lemma:eignon} in \Cref{sec:auxpf}). %We are interested in finding the eigenvalues $\lambda_1^D,\dots ,\lambda_p^D$, and the corresponding eigenvectors $v_1^D,\dots ,v_p^D$. 
We start with a characterization of the eigenvalues $\lambda_1^D,\dots ,\lambda_p^D$, and note that  %by analyzing eigenvalues.
%\subsubsection{Eigenvalues}\label{subsubsec:deteigval}
%The eigenvalues 
$\lambda_i^D$ solves % the following equation:
$$\det\paren{D-\lambda I_{p}} = \det\paren{\matrix{ a-\lambda I_p & q^\top\\
                q & P-\lambda I_{d-p}}} = 0. $$
For $ 1\le i\le d-p $, consider a function $L_i:\bbR\setminus\Lambda^a\to \bbR$ defined as     \begin{equation}\label{eq:defLi}
        L_i(\mu) \coloneqq \lambda_i(P-q(a-\mu I_p)^{-1}q^\top).
    \end{equation}
    %where $\lambda_i(\cdot)$ symbolizes the $i$-th largest eigenvalue of the matrix. 
    Note that in each of the intervals forming the domain, $L_i(\mu)$ is continuous. In fact, the eigenvalues are roots of a polynomial with continuous coefficients, and zeros of such a polynomial are continuous  \cite[Theorem 1]{zedek1965continuity}. Let us for simplicity of exposition assume that all eigenvalues of $a$ are different (if any multiplicity exists, notation and exposition can easily be adjusted accordingly).
Consider, for $i\in[p]$,
\begin{equation}\label{eq:defLtilde}    
\tilde{L}_i(\mu) \coloneqq
\begin{cases}
L_1(\mu) & \text{if } \lambda_i^a < \mu < \lambda_{i-1}^a, \\
\quad \vdots\\
L_i(\mu) & \text{if } \lambda_1^a < \mu < +\infty, 
\end{cases}
\end{equation}
where we restrict the domain to $\mu \in ]\lambda_{i}^a,+\infty[\setminus \Lambda^a$. 
These functions are piecewise continuous, since the $L_i$'s are continuous in the respective domains. For each $i$, the domain of $\tilde{L}_i$ can be continuously extended to $\Lambda^a$ (\Cref{lemma:samelimits} in \Cref{sec:auxpf}), and  $\tilde{L}_i(\mu)$ is non-increasing in this domain (\Cref{lem:eiglimits} in \Cref{sec:auxpf}). We now use the defined functions to characterize the top $p$ eigenvalues of the matrix $D$.
\begin{proposition}\label{prop:eigvalrec}
    For $i\in[p]$, the eigenvalue $\lambda_i^D$ is the unique solution to the equation 
    \begin{equation}\label{eq:eigvalrec}
        \tilde{L}_i(\mu) = \mu,
    \end{equation}  
    in the respective domain $]\lambda_i^a,\infty[$.
\end{proposition}

While the strategy of the argument is similar to the case $p=1$ treated in \cite[Proposition 3.1]{lu2020phase}, there is a bit more nuance in the multidimensional case, as the matrix $D$  can have eigenvalues matching those of the matrix $a$, which requires additional care. The proof is deferred to \Cref{sec:auxpf}.

%\subsubsection{Eigenvectors}\label{subsubsec:deteigvec}
Let us move to the eigenvectors and write $v_i^D\coloneqq\matrix{h_i\\ g_i}$, where $h_i\in \bbR^{d-p}$ and $g_i\in \bbR^p$. It holds that
\begin{equation}\label{eq:generaleigvec}
    D v_i^D = \matrix{ a & q^\top\\
                    q & P}\matrix{h_i\\ g_i} = \lambda^D_i \matrix{h_i\\ g_i}.
\end{equation} 
We characterize $h_i$ in terms of the function $R(\lambda)$ defined as
$$R(\lambda)\coloneq a - q^\top(P-\lambda I_{d-p})^{-1}q,$$
with the restricted domain $\lambda\in]\lambda_1^p,+\infty[$. 
\begin{proposition}\label{prop:eigenvec}
Let $j\in[p]$ be s.t.\ $\lambda_i^D>\lambda_1^P$ for all $i\leq j$. Then, for all $i\leq j$, it holds that
    $$h_i = \frac{\tilde{h}_i}{\sqrt{1-\tilde{h}_i^\top\frac{d}{d\lambda}R(\lambda_i^D)\tilde{h}_i}},$$
    where $\tilde{h}_i$ is the %unit norm 
    eigenvector of $R(\lambda_i^D)$ and $\frac{d}{d\lambda}R(\lambda)$ is the entry-wise derivative of %the matrix 
    $R(\lambda)$.
\end{proposition}
   The proof of this proposition can be found in \Cref{sec:auxpf}.
    Consider now the columns of the matrix $q=\matrix{q_1,&\dots,& q_p},$ with $q_i\in\bbR^{d-p}$. It holds that \begin{equation}\label{eq:matrixRelements}
        R(\lambda)_{i,i} = a_{i,i}-q_i^\top(P-\lambda I_{d-p})^{-1}q_i, \qquad R(\lambda)_{i,j} = a_{i,j}-q_i^\top(P-\lambda I_{d-p})^{-1}q_j.
    \end{equation}
    Given $\mu\ge 0$, let us introduce the functions 
    \begin{equation}\label{eq:auxfun}
    \cLi(\mu)\coloneqq \lambda_1(P+\mu q_iq_i^\top),\qquad  \cL_{i,j}(\mu)\coloneqq \lambda_1(P+\mu (q_i+q_j)(q_i+q_j)^\top),
    \end{equation}
    which allow to re-write the elements of $R(\lambda)$ in a form that will be convenient for the asymptotic analysis of \Cref{subsec:asymptotic behavior}. 
    
\begin{lemma}\label{lemma:matrixRchar}
    Let us assume that $\lambda>\lambda_1^P$. Then, for the  diagonal elements of $R(\lambda)$, it holds that
    \begin{align}
        R(\lambda)_{i,i} &= a_{i,i}+\frac{1}{\cLi^{-1}(\lambda)},\label{eq:matrixRminusonediag} \\
        \frac{d}{d\lambda}R(\lambda)_{i,i} &= -\frac{1}{\paren{\cLi^{-1}(\lambda)}^2\cLi'(\cLi^{-1}(\lambda))}.\label{eq:matrixRminustwodiag}
    \end{align}
    Moreover, for the off-diagonal elements, we have
    \begin{align}
        2R(\lambda)_{i,j} &=2a_{i,j} + \frac{1}{\cL_{i,j}^{-1}(\lambda)} - \frac{1}{\cL_i^{-1}(\lambda)} - \frac{1}{\cL_j^{-1}(\lambda)},\label{eq:matrixRminusoneoffdiag}\\
        2\frac{d}{d\lambda}R(\lambda)_{i,j} &= \frac{1}{\paren{\cLi^{-1}(\lambda)}^2\cLi'(\cLi^{-1}(\lambda))}+\frac{1}{\paren{\cL_j^{-1}(\lambda)}^2\cL_j'(\cL_j^{-1}(\lambda))}-\frac{1}{\paren{\cL_{i,j}^{-1}(\lambda)}^2\cL_{i,j}'(\cL_{i,j}^{-1}(\lambda))} .\label{eq:matrixRminustwooffdiag}
    \end{align}
\end{lemma}
\begin{proof}
    Transforming \eqref{eq:matrixRelements} with the matrix determinant lemma yields that, for any $\lambda>\lambda_1^P$,    \begin{equation}\label{eq:ijelementmatrixR}
        R(\lambda)_{i,i} = a_{i,i}-q_i^\top(P-\lambda I_{d-p})^{-1}q_i=a_{i,i}+\frac{1}{\cLi^{-1}(\lambda)}.
    \end{equation}
    Moreover, it holds that
    \begin{equation}
        R(\lambda)_{i,j} =a_{i,j}- \frac{(q_i+q_j)^\top(P-\lambda I_{d-p})^{-1}(q_i+q_j) - q_i^\top(P-\lambda I_{d-p})^{-1}q_i - q_j^\top(P-\lambda I_{d-p})^{-1}q_j}{2}. \notag 
    \end{equation}
    From the same transformation with the matrix determinant lemma, it follows that 
    $$(q_i+q_j)^\top(P-\lambda I_{d-p})^{-1}(q_i+q_j) = -\frac{1}{\cL_{i,j}^{-1}(\lambda)}.$$
    Substituting the previous identity in \Cref{eq:ijelementmatrixR} gives \Cref{eq:matrixRminusoneoffdiag}.
    Note that, for $\mu$ such that $\cLi(\mu)>\lambda_1^P$, it holds that $\cLi$ is an increasing differentiable function, so its inverse and derivative are well defined. 
    Finally, by differentiating %equations 
    \Cref{eq:matrixRminusonediag} and \Cref{eq:matrixRminusoneoffdiag}, we get the other two equations.
\end{proof}


\subsection{Precise asymptotic characterization}
\label{subsec:asymptotic behavior}

Leveraging the analysis in \Cref{subsec:general matrix}, we now prove \Cref{thm:main,thm:eigvalconv}. To do so, we 
introduce additional %some more 
auxiliary definitions. % that will be useful in the proof of the main result,
For each $i$, let
$\lambda_i^\infty (\alpha) = \lambda_i(R^\infty(\alpha))$
be the $i$-th largest eigenvalue of $ R^\infty(\alpha) $ and let  $t_i^\infty$ be 
$$t_i^\infty\coloneq \lim_{\alpha\to\tau} \lambda^\infty_i(\alpha).$$
Furthermore, due to the law of large numbers, we have %Also, we define $a^\infty\in \bbR^{p\times p}$ which, due to law of large numbers, is the limit of  matrices $a$ as defined in \eqref{eq:Ddef}. Namely,
$$a\asconv a^\infty \coloneq \expt{zss^\top}\in \bbR^{p\times p}.$$
Lastly, consider
\begin{equation}\label{eq:tildeLiinftydef}
\tildeLiinfty(\mu)\coloneq\zeta_\delta\paren{(\lambda^\infty_i)^{-1}(\mu)}, 
\end{equation}
on the domain $\mu\in]\lambda_i^{a^\infty}, t_i^\infty[$. Note that $\tildeLiinfty(\mu)$ is a continuous non-increasing function, as it is the composition of a non-decreasing function and a strictly increasing function.
We are now ready to prove our characterization of the eigenvalues of $D_n$. 
%\begin{theorem}\label{thm:eigvalconv}
%     Let $\alpha_1\geq \dots\ \geq \alpha_j > \tau$, for some $j\in[p]$, be all the solutions to the equation
%    \begin{equation}\label{eq:master_eq2}
%        \det\paren{\zetadelta(\alpha)I_p-R^\infty(\alpha)}=0.
%    \end{equation}
%    Then, for the top $j$ eigenvalues of $D_n$, it holds that
%    \begin{equation}\label{eq:outsidebulkeigconv}
%        \lambda_1^D,\dots,\lambda_j^D \asconv \zetadelta(\alpha_1), \dots, \zetadelta(\alpha_j),
%    \end{equation}
%    and for the remaining $p-j$ eigenvalues, it holds that
 %   $$\lambda_{j+1}^D,\dots,\lambda_p^D \asconv \zetadelta(\lambdabardelta).$$ 
%\end{theorem}

\begin{proof}[Proof of \Cref{thm:eigvalconv}.]
    Note that \eqref{eq:master_eq2} can be reformulated as
    \begin{equation}\label{eq:zetadeltalambda}
        \det\paren{\zetadelta(\alpha)I_p-R^\infty(\alpha)}=\prod_{i=1}^p( \zetadelta(\alpha)-\lambda_i^\infty(\alpha))=0.
    \end{equation}
    The assumption of the theorem implies that $\alpha_1\geq\dots \geq\alpha_j>\tau$ satisfy \eqref{eq:zetadeltalambda}.
    Recall that each function $\zetadelta(\alpha) - \lambda_i^\infty(\alpha)$ is strictly increasing on $]\tau,+\infty[$, with the right edge limit $+\infty$. Therefore, the implicit assumption that $j\in[p]$ is justified as there could be at most $1$ solution to the equation
    $$\zetadelta(\alpha) - \lambda_i^\infty(\alpha)=0,$$
    for each $i\in[p]$.
    Moreover, it holds by definition that $\lambda_1^\infty(\alpha)\geq\dots\geq\lambda_{p}^\infty(\alpha)$. Thus, it must be that each $\alpha_i$ is the unique solution to 
    $$\zetadelta(\alpha) - \lambda_i^\infty(\alpha)=0,$$
    for $i\in[j]$ and $\alpha>\tau$.
    Let us denote by $\mu_i^*\coloneq {\lambda_i^\infty}(\alpha_i)\in]\lambda_i^{a^\infty},\tau_i^\infty[$. Then $\mu_i^*$ is a solution to the equation
    $$\tildeLiinfty(\mu)-\mu=0,$$
    in the domain of definition $\tildeLiinfty(\mu)$, as $\tildeLiinfty(\mu_i^*) = \zetadelta((\lambda_i^\infty)^{-1}(\lambda_i^\infty(\alpha_i)))$. Moreover, $\mu_i^*$ is the unique such solution, due to the strict monotonicity of $\tildeLiinfty(\mu)-\mu$ on its domain $]\lambda_i^{a^\infty},\tau_i^\infty[$.
    
    Furthermore, \Cref{prop:eigvalrec} implies that each $\lambda_i^D$ is the unique solution to \eqref{eq:eigvalrec}. For each $\mu$ where $\tildeLiinfty(\mu)$ is defined, it holds that
    \begin{equation}\label{eq:asconvtildeli}
        \tildeLi(\mu) - \mu \asconv\tildeLiinfty(\mu) - \mu,
    \end{equation}
    by \Cref{prop:tildeLiconv} in \Cref{app:asymptotbehav}.
    
    As both $\tildeLi(\mu)$ and $\tildeLiinfty(\mu)$ are non-increasing, the functions $\tildeLi(\mu) - \mu$ and  $\tildeLiinfty(\mu)-\mu$ are strictly decreasing. Hence, by \cite[Lemma A.1]{lu2020phase}, it holds that
    \begin{equation}\label{eq:fplast}
\lambda_i^D\asconv\tildeLiinfty(\mu_i^*).
    \end{equation}
    Substituting $\mu_i^* = {\lambda_i^\infty}(\alpha_i)$ in \Cref{eq:fplast} gives \eqref{eq:outsidebulkeigconv} for $i\in[j]$.
    
    It remains to show the claim for the remaining $p-j$ eigenvalues. As \eqref{eq:master_eq2} has only $j$ solutions by assumption, it follows that
    $$\zetadelta(\alpha) - \lambda_i^\infty(\alpha) = \tildeLiinfty(\lambda_i^\infty(\alpha))-\lambda_i^\infty(\alpha)=0$$
    has no solutions for $\alpha>\tau$ and $i> j$. Denoting $\mu = \lambda_i^\infty(\alpha)$, it further holds that
    $$\tildeLiinfty(\mu)-\mu=0$$
    has no solutions for $\mu \in ]\lambda_i^{a^\infty},t_i^\infty[$. Since 
    $$\lim_{\mu\to\lambda_i^{a^\infty}} \tildeLiinfty(\mu) - \mu = +\infty, $$
    it must be that $\tildeLiinfty(\mu)-\mu>0$ for all $\mu\in]\lambda_i^{a^\infty},t_i^\infty[$.
    Using \eqref{eq:asconvtildeli}, we %can conclude that it holds 
    have that
    $$\tildeLi(\mu)-\mu>0,$$
    for all $\mu \in ]\lambda_i^{a^\infty},t_i^\infty[$ and $n$ large enough. As $\lambda_i^a\asconv \lambda_i^{a^\infty}$ and each $\tildeLi(\mu)$ is defined on $]\lambda_i^a,+\infty[$, the solution to the equation
    $$\tildeLi(\mu) - \mu =0 $$
    must be for $\mu>t_i^\infty$.
    Then, applying \Cref{prop:tildeLiconv}, for any fixed $\mu$ it holds that 
    $$\tildeLi(\mu)\asconv \zetadelta(\lambdabardelta).$$
    Lastly, as both $\mu-\tildeLi(\mu)$ and $\mu-\zetadelta(\lambdabardelta)$ are increasing functions, Lemma A.1 in \cite{lu2020phase} implies that 
    $$\lambda_i^D\asconv \zetadelta(\lambdabardelta),$$
    for all $i>j$, which proves the claim.
\end{proof}

%\subsubsection{Eigenvectors}

We continue by determining the asymptotic behavior of the overlaps between eigenvectors and a basis of the subspace spanned by the signals. To do so, we first state a result for the convergence of the matrices that by \Cref{prop:eigenvec} characterize the eigenvectors.
\begin{proposition}\label{thm:matrixRconvergence}
     Let $k\in[p]$ be such that $\alpha_k>\lambdabardelta$, where $\alpha_k$ is the $k$-th largest solution of \Cref{eq:master_eq2}. %to the equation
%     $$\det\paren{\zetadelta(\alpha)I-R^\infty(\alpha)}=0.$$
     Then, it holds that
     \begin{equation}\label{eq:matrixRconv}
         R(\lambda_k^D)\asconv R^\infty(\alpha_k),
     \end{equation}
     and
     \begin{equation}\label{eq:matrixderRconv}
         \frac{d}{d\lambda}R(\lambda_k^D)\asconv \frac{1}{\zetadelta'(\alpha_k)}\frac{d}{d\alpha}R^\infty(\alpha_k).
     \end{equation}
\end{proposition}
\begin{proof}
By \Cref{lemma:matrixRchar}, %from subsection \Cref{subsubsec:deteigvec} 
it suffices to understand the behavior of the functions in \Cref{eq:auxfun}, 
%$$\cLi(\mu)= \lambda_1(P+\mu q_iq_i^\top), \text{ and } \cL_{i,j}(\mu)= \lambda_1(P+\mu (q_i+q_j)(q_i+q_j)^\top),$$
as $n,d\to \infty$. However, we first need to verify the assumption that $\lambda_k^D>\lambda_1^P$ almost surely. From \Cref{thm:eigvalconv}, it follows that 
$$\lambda_k^D\asconv \zetadelta(\alpha_k) ,$$
and $\zetadelta(\alpha_k)> \zetadelta(\lambdabardelta)$ as $\alpha_k>\lambdabardelta$ and $\zetadelta$ is strictly increasing on $]\lambdabardelta,+\infty[$. 
Furthermore, $\lambda_1^P\asconv \zetadelta(\lambdabardelta)$, hence $\lambda_k^D>\lambda_1^P$ almost surely.

Let us denote by $G$ the function 
$$G(\mu) = -\frac{1}{\mu},$$
which we will use in the continuation of the proof.
Using \cite{bai-yao-2012} as in the proof of \Cref{prop:tildeLiconv}, 
we get that 
$$\cLi(\mu)\asconv \zetadelta \circ {Q_i}^{-1}\circ G(\mu),$$
where $Q_i(\alpha) \coloneq \expt{\frac{s_i^2z^2}{z-\alpha}}$. Notice that $Q_i(\alpha)$ is invertible by \cite[Remark 3.3]{lu2020phase}, which is stated for the analogous function $Q$. 
In the same manner, it holds that
$$\cL_{i,j}(\mu)\asconv \zetadelta \circ {Q_{i,j}}^{-1}\circ G(\mu),$$
where $Q_{i,j}(\alpha) \coloneq \expt{\frac{(s_i+s_j)^2z^2}{z-\alpha}}$. As $\alpha_k>\lambdabardelta$, we have that $\zetadelta$ is strictly increasing and invertible, hence
$$\cLi^{-1}(\lambda_k^D)\asconv G \circ Q_i \circ \zetadelta^{-1} \circ \zetadelta(\alpha_k) = G \circ Q_i (\alpha_k),$$
which follows from \cite[Lemma A.1]{lu2020phase}.
Plugging this into \eqref{eq:matrixRminusonediag} we get
\begin{equation}\label{eq:tempdiagconv}
    R(\lambda_k^D)_{i,i}\asconv a^{\infty}_{i,i}-Q_i (\alpha_k).
\end{equation}
Note that 
$$a^{\infty}_{i,i}-Q_i (\alpha_k) = \expt{s_i^2z} -  \expt{\frac{s_i^2z^2}{z-\alpha_k}} = \expt{\frac{\alpha_k s_i^2z}{\alpha_k-z}} = R^\infty(\alpha_k)_{i,i}.$$
Similarly, it holds that
$$R(\lambda_k^D)_{i,j}\asconv a^{\infty}_{i,j}-Q_{i,j} (\alpha_k),$$
which combined with \eqref{eq:tempdiagconv} proves \eqref{eq:matrixRconv}.

Moreover, we have that $\cLi(\mu)$ is differentiable (see \Cref{lemma:matrixRchar}), so for its derivative it holds that
$$\cLi'(\mu)\asconv \zetadelta' \circ {Q_i}^{-1}\circ G(\mu) \cdot \paren{Q_i^{-1}}'\circ G(\mu) \cdot G'(\mu),$$
which follows from \cite[Lemma A.2]{lu2020phase}.
Plugging this into \eqref{eq:matrixRminustwodiag} we get 
$$\frac{d}{d\lambda}R(\lambda_k^D)_{i,i}\asconv \frac{\frac{d}{d\alpha}(R^\infty(\alpha_k)_{i,i})}{\zetadelta'(\alpha_k)}.$$
Similarly, it holds that
$$\frac{d}{d\lambda}R(\lambda_k^D)_{i,j}\asconv \frac{\frac{d}{d\alpha}(R^\infty(\alpha_k)_{i,j})}{\zetadelta'(\alpha_k)}.$$
Combining the last two equations we obtain \eqref{eq:matrixderRconv}.
\end{proof}

At this point, we are ready to prove our result for the eigenvectors of $D_n$.

%\begin{corollary}\label{cor:mastereigenvectorsmultiplewassumption}
%    Let $\alpha_k$ be the $k$-th solution to the equation
%    $$\det\paren{\zetadelta(\alpha)I-R^\infty(\alpha)}=0,$$
%    with multiplicity j and corresponding eigenspace $E_k^\infty$. If $\alpha_k>\lambdabardelta$, then it holds 
%    \begin{equation}
 %       \max_{l\in[p]}\liminf_{d\to\infty}\sum_{i=k}^{k+j-1}\abs{\inprod{v_i^D}{e_l^{(d)}}}^2>0. \notag 
        % \label{eq:liminfconv}
  %  \end{equation}
   % Also, under the assumption that eigenspace $E_k^\infty$ stays the same in neighbourhood of $\alpha_k$, it holds for any $j\in[p]$ that
    %$$\sum_{i=k}^{k+j-1}\abs{\inprod{v_i^D}{e_l^{(d)}}}^2\asconv \frac{\zetadelta'(\alpha_k) \sum_{i=k}^{k+j-1}\abs{\inprod{h_i^\infty}{e_l^{(p)}}}^2}{\zetadelta'(\alpha_k)+{h_k^\infty}^\top \frac{d}{d\alpha}R^\infty(\alpha_k)h_k^\infty},$$
    %where $h_i^\infty$ form an orthonormal eigenbasis of $E_k^\infty$. 
%\end{corollary}
%\begin{proof}

     \begin{proof}[Proof of \Cref{thm:main}.]
     Let $v_i^D=\matrix{h_i\\ g_i}$, for $i\in\{k,\dots, k+m-1\}$. Since $\alpha_k>\lambdabardelta$, the conditions of \Cref{prop:eigenvec} are satisfied as in the proof of \Cref{thm:matrixRconvergence}. Thus, it holds that
    \begin{equation}\label{eq:hkhtildek}
        h_i = \frac{\tilde{h}_i}{\sqrt{1-\tilde{h}_i^\top\frac{d}{d\lambda}R(\lambda_i^D)\tilde{h}_i}},
    \end{equation}
    where $\tilde{h}_i = \frac{h_i}{\norm{2}{h_i}}$ is the unit norm eigenvector of $R(\lambda_i^D)$. Note that the vectors $\tilde{h}_i$ are orthogonal. %The vectors $v_i^D$ are either orthogonal as eigenvectors corresponding to different eigenvalues, or should a multiplicity in $\lambda_i^D$ occur, through the choice of an orthogonal basis of the corresponding eigenspace. Consequently, the vectors $\tilde{h}_i$ will be orthogonal as eigenvectors of the matrix corresponding to eigenvalues of different value, or through choice of the orthogonal eigenbasis should there be eigenvalue multiplicty. 
    Furthermore, \Cref{thm:matrixRconvergence} gives that
    \begin{equation}\label{eq:rlambdaconv}
        R(\lambda_k^D)\asconv R^\infty(\alpha_k), \qquad \frac{d}{d\lambda}R(\lambda_k^D) \asconv \frac{1}{\zetadelta'(\alpha_k)}\frac{d}{d\alpha}R^\infty(\alpha_k).
    \end{equation}
    Then, applying the results from \cite[II.1.4]{kato2013perturbation}, %(as proved in this StackExchange \href{https://math.stackexchange.com/questions/4054792/convergence-of-eigenvalues-and-spaces-of-sequence-of-compact-szmmetric-and-posi}{answer}).
    it holds that the orthonormal projection to the eigenspace corresponding to the $k$-th eigenvalue also converges, that is 
    \begin{equation}\label{eq:pieqkconv}
        \Pi_{E_k} \asconv \Pi_{E^\infty_k},
    \end{equation}
    where $E_k$ is the space spanned by the eigenvectors $h_k,\dots, h_{k+m-1}$ and $E^\infty_k$ is the eigenspace of the limiting matrix $R^\infty(\alpha_k)$, corresponding to the eigenvalue $\zetadelta(\alpha_k)$ of multiplicity $m$.
    Due to orthonormality of $\tilde{h}_i$, we can write the orthonormal projection more explicitly as
    $$\Pi_{E_k} = \sum_{i=k}^{k+m-1}\frac{h_ih_i^\top}{\norm{2}{h_i}^2} = \sum_{i=k}^{k+m-1}\tilde{h}_i\tilde{h}_i^\top,$$
    and 
    \begin{equation}\label{eq:orthonormalbasisinfty}
        \Pi_{E^\infty_k} = \sum_{i=k}^{k+m-1}\frac{h^\infty_i{h^\infty_i}^\top}{\norm{2}{h^\infty_i}^2}=\sum_{i=k}^{k+m-1}h^\infty_i{h^\infty_i}^\top,
    \end{equation}
    where $h^\infty_k\dots, h^\infty_{k+j-1}$ is any choice of the orthonormal eigenbasis of $E^\infty_k$. 
    From \eqref{eq:hkhtildek}, it follows that 
    $$\sum_{i=k}^{k+m-1} \frac{1}{\norm{2}{h_i}^2} = m - \sum_{i=k}^{k+m-1}\tilde{h}_i^\top\frac{d}{d\lambda}R(\lambda_i^D)\tilde{h}_i\geq m - m\cdot\lambda_p\left(\frac{d}{d\lambda}R(\lambda_i^D)
    \right).$$
    Moreover, due to  \eqref{eq:rlambdaconv} and the continuity of eigenvalues, the RHS has a convergent limit
    \begin{equation}\label{eq:convlimit}
    m - m\cdot\lambda_p\left(\frac{d}{d\lambda}R(\lambda_i^D)\right) \asconv m - m \frac{1}{\zetadelta'(\alpha_k)}\lambda_p\left(\frac{d}{d\alpha}R^\infty(\alpha_k)\right).        
    \end{equation}
    Note that the matrix $\frac{d}{d\alpha}R^\infty(\alpha_k) = -\expt{\frac{ss^\top z^2}{(\alpha_k-z)^2}}$ is strictly negative definite for $\alpha_k>\tau$, which implies that $\lambda_p(\frac{d}{d\alpha}R^\infty(\alpha_k))<0$.
    As $\alpha_k>\lambdabardelta$, it holds that $\zetadelta'(\alpha_k)>0$ and the RHS of \Cref{eq:convlimit} is finite. This further implies that, for each $i$ s.t.\ $k\leq i\leq k+m-1$, it must hold
    \begin{equation}\label{eq:liminfhi}
        \liminf_{d\to\infty}\norm{2}{h_i}>0.
    \end{equation}
    Note that
    \begin{equation}\label{eq:hiinequality}
    \begin{split}
        \sum_{i=k}^{k+m-1}\abs{\inprod{v_i^D}{e_l^{(d)}}}^2 = \sum_{i=k}^{k+m-1}\abs{\inprod{h_i}{e_l^{(p)}}}^2 &= \sum_{i=k}^{k+m-1} \norm{2}{h_i}^2\abs{\inprod{\tilde{h}_i}{e_l^{(p)}}}^2\\
        &\geq \min_{t\in \{k, \ldots, k+m-1\}}\norm{2}{h_t}^2\sum_{i=k}^{k+m-1}\abs{\inprod{\tilde{h}_i}{e_l^{(p)}}}^2\\
        &=  \min_{t\in \{k, \ldots, k+m-1\}}\norm{2}{h_t}^2 \cdot {e_l^{(p)}}^\top \Pi_{E_k}{e_l^{(p)}}.
    \end{split}
    \end{equation}
    Let us pick $e_l^{(d)}$ such that $\Pi_{E_k^\infty}(e_l^{(p)})\neq 0$. Then, \eqref{eq:rlambdaconv} implies that 
    \begin{equation}\label{eq:pineq}
        \Pi_{E_k}(e_l^{(p)})\neq 0,
    \end{equation}
    for all $d$ large enough.  Finally, combining \eqref{eq:liminfhi}, \eqref{eq:hiinequality} and \eqref{eq:pineq} proves
    $$\liminf_{d\to\infty}\sum_{i=k}^{k+m-1}\abs{\inprod{v_i^D}{e_l^{(d)}}}^2>0,$$
    which gives the claim in \eqref{eq:liminfconv}.
    
    Let us now assume, as in the statement, that $E_k^\infty$ is also the eigenspace corresponding to the $k$-th eigenvalue of $R^\infty(\alpha+\Delta)$  for any small enough $\Delta$. 
    For arbitrary eigenvectors $h_{i_1}$ and $h_{i_2}$ from $E_k^\infty$, it holds that 
    \begin{equation}\label{eq:equalderivative}
    \begin{split}
        {h_{i_1}^\infty}^\top\frac{d}{d\alpha}R^\infty(\alpha_k)h_{i_1}^\infty &= \lim_{\Delta\to 0} \frac{{h_{i_1}^\infty}^\top R^\infty(\alpha_k+\Delta)h_{i_1}^\infty-{h_{i_1}^\infty}^\top R^\infty(\alpha_k)h_{i_1}^\infty }{\Delta}\\
        &=\lim_{\Delta\to 0} \frac{{h_{i_2}^\infty}^\top R^\infty(\alpha_k+\Delta)h_{i_2}^\infty-{h_{i_2}^\infty}^\top R^\infty(\alpha_k)h_{i_2}^\infty}{\Delta}\\
        &={h_{i_2}^\infty}^\top\frac{d}{d\alpha}R^\infty(\alpha_k)h_{i_2}^\infty,
    \end{split}
    \end{equation}
     since ${h_{i_1}^\infty}^\top R^\infty(\alpha_k+\Delta)h_{i_1}^\infty = {h_{i_2}^\infty}^\top R^\infty(\alpha_k+\Delta)h_{i_2}^\infty$ for any small enough $\Delta$.
     Note that, for any $\epsilon$ and large enough $d$, it holds that
     \begin{equation}\label{eq:epsinequal}
         \norm{2}{\Pi_{E_k} - \Pi_{E_k^\infty}}<\epsilon.
     \end{equation}
     due to \eqref{eq:pieqkconv}. Let us now fix  $\tilde{h}_i$, for some $i\in\{k,\dots,k+m-1\}$. As we can choose any orthonormal basis when writing out $\Pi_{E_k^\infty}$ in $\eqref{eq:orthonormalbasisinfty}$, let us choose one such that $h_i^\infty = \frac{\Pi_{E_k^\infty}(\tilde{h}_i)}{\norm{2}{\Pi_{E_k^\infty}(\tilde{h}_i)}}$. Then, \eqref{eq:epsinequal} implies that 
     $$\norm{2}{\sum_{i=k}^{k+m-1}\tilde{h}_i\tilde{h}_i^\top - \sum_{i=k}^{k+m-1}h_i^\infty{h_i^\infty}^\top}<\epsilon.$$
     From the orthonormality of the chosen eigenbasis, it holds that
     \begin{equation*}
         \begin{split}
             \norm{2}{\tilde{h}_i - \Pi_{E_k^\infty}\tilde{h}_i} &= \norm{2}{(\Pi_{E_k} - \Pi_{E_k^\infty})(\tilde{h}_i)} \\
             &\leq \norm{2}{\Pi_{E_k} - \Pi_{E_k^\infty}} \norm{2}{\tilde{h}_i} \\
             &<\epsilon.
         \end{split}
     \end{equation*}
    This also implies $1+\epsilon>\norm{2}{\Pi_{E_k^\infty}\tilde{h}_i}\geq 1-\epsilon$, hence 
     \begin{equation}
         \begin{split}
             \norm{2}{\tilde{h}_i - h_i^\infty} &= \norm{2}{\tilde{h}_i - \frac{\Pi_{E_k^\infty}(\tilde{h}_i)}{\norm{2}{\Pi_{E_k^\infty}(\tilde{h}_i)}}} \\
             &= \norm{2}{\frac{\tilde{h}_i - \Pi_{E_k^\infty}(\tilde{h}_i)}{\norm{2}{\Pi_{E_k^\infty}(\tilde{h}_i)}}+\tilde{h}_i\frac{1-\norm{2}{\Pi_{E_k^\infty}(\tilde{h}_i)}}{\norm{2}{\Pi_{E_k^\infty}(\tilde{h}_i)}}}\\
             &\leq \frac{\epsilon}{1-\epsilon}+\frac{\epsilon}{1-\epsilon}<4\epsilon.
         \end{split} \notag 
     \end{equation}
    % As we have that $h_i^\infty\sim\tilde{h}_i$, 
    Thus, \eqref{eq:rlambdaconv} implies that, for large enough $d$,
     $$\norm{2}{\tilde{h}_i^\top\frac{d}{d\lambda}R(\lambda_k^D)\tilde{h}_i - \frac{1}{\zetadelta'(\alpha_k)}{h_i^\infty}^\top\frac{d}{d\alpha}R^\infty(\alpha_k)h_i^\infty}<c\cdot \epsilon,$$
     for some constant $c$ independent of $\epsilon$.
     Plugging in the expression \eqref{eq:equalderivative} makes $h_i^\infty$ not depend on $\tilde{h}_i$ anymore, resulting in 
     \begin{equation}\label{eq:normconv}
         \tilde{h}_i^\top\frac{d}{d\lambda}R(\lambda_k^D)\tilde{h}_i\asconv \frac{1}{\zetadelta'(\alpha_k)}{h_{l}^\infty}^\top\frac{d}{d\alpha}R^\infty(\alpha_k)h_{l}^\infty,
     \end{equation}
     for an arbitrary unit norm eigenvector $h_{l}^\infty\in E^\infty_k$.
     Finally, combining \eqref{eq:hkhtildek} and \eqref{eq:normconv} with \eqref{eq:pieqkconv} implies 
     $$\sum_{i=k}^{k+m-1}\tilde{h}_i\tilde{h}_i^\top\asconv \frac{\zetadelta'(\alpha_k) \sum_{i=k}^{k+m-1}h_i^\infty {h_i^\infty}^\top}{\zetadelta'(\alpha_k)+{h_k^\infty}^\top \frac{d}{d\alpha}R^\infty(\alpha_k)h_k^\infty},$$
     proving the claim. The case without multiplicity ($m=1$) is simpler and follows along similar lines. For completeness, we report the proof as that of \Cref{prop:mastereigenvectors} in \Cref{app:asymptotbehav}. 
\end{proof}


%\subsection{Optimizing $\cT$}




\section{Concluding remarks} \label{sec:conc}

This paper provides the first asymptotic characterization of spectral estimators for multi-index models: we unveil a phase transition in the top-$p$ eigenvalues of the spectral matrix $D$ in \Cref{eqn:D}, giving a low-dimensional (and simple to check) condition for spikes to emerge from the bulk of the spectrum; the eigenvalue phase transition is associated to the recovery of the subspace spanned by the signals via the corresponding eigenvectors and, under certain technical conditions, we prove a precise expression for the asymptotic overlap; finally, we optimize the data preprocessing and identify the spectral estimator that weakly recovers the signal subspace with the lowest possible sample complexity. 

Spectral methods are often used as a warm start for other procedures, typically of iterative nature. Thus, our analysis provides the starting point to combine spectral estimators either with a simple linear estimator \cite{mondelli2022optimal,mixed-zmv-arxiv}  or with approximate message passing \cite{Mondelli_Venk}, with the objective of achieving -- at least in absence of statistical-to-computational gaps -- the Bayes-optimal limits of inference as characterized in \citet{Aubin_comm_machine}. 


\section*{Acknowledgements}

Y. Z.\ and M. M.\ are funded by the European Union (ERC, INF$^2$, project number 101161364). Views and opinions expressed are however
those of the author(s) only and do not necessarily reflect those of the European Union or the
European Research Council Executive Agency. Neither the European Union nor the granting
authority can be held responsible for them.


\bibliographystyle{alpha} 
\bibliography{ref} 

\appendix



\section{Proofs for \Cref{subsec:general matrix}}\label{sec:auxpf}

We start with two auxiliary results.

\begin{lemma}\label{lemma:fr}
    Under \Cref{asmp:T}, almost surely for all sufficiently large $n$ it holds that $$\rk(ZS) = p.$$
\end{lemma}
\begin{proof}
 Notice first that, for all sufficiently large $n$, there are almost surely at least $p$ elements in the array $\brack{z_1 ,z_2 ,\dots}$ that are non-zero. This follows from \Cref{asmp:T} that $\prob{Z = 0}<1$, as done in the proof of \cite[Proposition 3.2]{lu2020phase}. Now, the $i$-th column of $ZS$ is obtained by scaling a standard $p$ dimensional Gaussian by $z_i$. As the columns of $S$ are almost surely independent and, for sufficiently large $n$, at least $p$ elements $z_i$'s are non-zero, it must be that at least $p$ columns of $ZS$ are linearly independent, which proves the claim. 
\end{proof}

\begin{lemma}\label{lemma:eignon}
    For every eigenvector $v_i^a$, it holds almost surely that 
    $$qv_i^a\neq 0.$$
\end{lemma}
\begin{proof}
    By definition, it holds that $v_i^a\neq 0$. Thus, \Cref{lemma:fr} implies that almost surely $Z S v^a_i \neq 0$.
    Furthermore, the elements of the matrix $U$ are sampled independently from $Z$ and $S$, so we can fix $x_i\coloneqq ZSv^a_i \neq 0$ and conclude 
    $$\prob{U^\top x_i=0} = 0,$$
    for the probability measure associated to the elements of $U$. This is due to the fact that, for $x_i\neq 0$, 
    $$\prob{U^\top x_i=0}=\prob{\forall \,j\in [d-p],\ \inprod{u^j}{x_i} = 0} = \prod_{i=1}^{d-p}\prob{\inprod{u_j}{x_i} = 0}=0,$$
    as each $u_j\in\bbR^n$ is sampled from a multi-variate Gaussian. 
    Lastly, using the union bound, we have %it holds almost surely that
    $$\prob{\exists \,i\in [p], U^\top x_i=0} \leq \sum_{i=1}^p \prob{U^\top x_i=0}=0,$$
    implying  that almost surely $qv^a_i\neq 0$ for every eigenvector of the matrix $a$. 
\end{proof}



For $i$ such that $p+1\leq i\leq d-p$, let us define
\begin{equation}\label{eq:defLtildeother}    
\tilde{L}_i(\mu) \coloneqq
\begin{cases}
L_{i-p}(\mu) & \text{if } \lambda_p^a < \mu < \lambda_{p-1}^a, \\
\quad \vdots\\
L_i(\mu) & \text{if } \lambda_1^a < \mu < +\infty,
\end{cases}
\end{equation}
where we restrict the domain to $\mu \in ]\lambda_{p}^a,+\infty[\setminus \Lambda^a$. This complements the definition of $\tilde{L}_i(\mu)$ for $i\in [p]$ in \Cref{eq:defLtilde}. 
For $1\leq i\leq d-p$, all functions $\tilde{L}_i$ can be continuously extended, as proved below. %the result we formulate in the following lemma.
\begin{lemma}\label{lemma:samelimits}
    For $i \in[p]$ and $j\geq 2$, it holds that
    $$\lim_{\mu_1\to{\lambda^a_i}^+}L_j(\mu_1) = \lim_{\mu_2\to{\lambda^a_i}^-}L_{j-1}(\mu_2).$$
\end{lemma}
\begin{proof}
To simplify exposition, let us denote by $M_\mu\coloneqq P-q(a-\mu I_p)^{-1}q^\top$, as well as by $r=\frac{qv_i^a}{\norm{2}{qv_i^a}}$ which is well defined as $qv_i^a\neq0$.
Using Weyl's inequality (see e.g. \cite[Section 4.3]{horn2012matrix}), one has
$$\lambda_{j}(M_{\mu_1}) \leq \lambda_{j-1}(M_{\mu_2}) + \lambda_2\paren{M_{\mu_1}-M_{\mu_2}}.$$
Note that $M_{\mu_1}-M_{\mu_2} = q\paren{(a-\mu_2 I_p)^{-1}-(a-\mu_1 I_p)^{-1}}q^\top$ and that the SVD decomposition of $(a-\mu I_d)^{-1}$ is
$$(a-\mu I_p)^{-1} = \sum_{k=1}^p \frac{v_k^av_k^{a\top}}{\lambda_k^a - \mu}.$$
Plugging that in, we get
$$(a-\mu_2 I_p)^{-1} - (a-\mu_1 I_p)^{-1} = v_i^av_i^{a\top}\paren{\frac{1}{\lambda_i^a - \mu_2}+\frac{1}{\mu_1-\lambda_i^a}}+\sum_{k\neq i}^p v_k^av_k^{a\top}\paren{\frac{\mu_2-\mu_1}{(\lambda_k^a - \mu_2)(\lambda_k^a - \mu_1)}}.$$ 
Taking the limits $\mu_1\to{\lambda^a_i}^+$ and $\mu_2\to{\lambda^a_i}^-$, one readily obtains that $\lambda_2(M_{\mu_1}-M_{\mu_2}):=\varepsilon_{\mu_2}\to 0$. 
Before continuing, let us denote by $\Pi_{r}\coloneqq rr^\top$ the orthogonal projection to the subspace defined by $r$, and by $\Pi_{{r}^\perp}$ the orthogonal projection to the subspace $r^\perp$. Obviously it holds that
$$\Pir+\Pirperp = I_{d-p}.$$
Thus, 
\begin{align*}
    \lambda_{j-1}(M_{\mu_2}) &=  \lambda_{j-1}\paren{(\Pir+\Pirperp)M_{\mu_2}(\Pir+\Pirperp)}\\
                             &\leq \lambda_{j-1}\paren{\Pirperp M_{\mu_2}\Pirperp}+\lambda_1(\Pir M_{\mu_2}\Pir +\Pirperp M_{\mu_2}\Pir +\Pir M_{\mu_2}\Pirperp),
\end{align*}
where the last line is due to the fact that the matrix $\Pir M_{\mu_2}\Pir +\Pirperp M_{\mu_2}\Pir +\Pir M_{\mu_2}\Pirperp$ is symmetric and through a subsequent application of Weyl's inequality. Let us analyze the eigenvector corresponding to the largest eigenvalue. Namely, for an eigenvector $t = \Pirperp t + \Pir t =: %t_r+t_{r^\perp}$
t_{r^\perp}+t_r$ of the discussed matrix with corresponding eigenvalue $\lambda$, we have that
\begin{align*}
    \Pir M_{\mu_2}\Pir\ t_r +\Pir M_{\mu_2}\Pirperp\ t_{r^\perp} &= \lambda\ t_r,\\
    \Pirperp M_{\mu_2}\Pir\ t_r &= \lambda\ t_{r^\perp}.
\end{align*}
If $t_r$ is the 0 vector, then $\lambda=0$. Otherwise, as $\Pir M_{\mu_2}\Pirperp\ t_{r^\perp}$ has a convergent, finite limit as $\mu_2\to{\lambda^a_i}^-$ and $t_r^\top\Pir M_{\mu_2}\Pir\ t_r\to - \infty$, it must hold that $\lambda \to -\infty$. This gives the following inequality 
$$\lambda_{j}(M_{\mu_1}) \leq \lambda_{j-1}(M_{\mu_2})+\epsilon_{\mu_2}\leq \lambda_{j-1}(\Pirperp M_{\mu_2}\Pirperp)+\epsilon_{\mu_2}',$$
where $\epsilon_{\mu_2}'\to 0$ as $\mu_2\to{\lambda^a_i}^-$.

Let us now lower bound $\lambda_{j}(M_{\mu_1})$. In a similar manner as before, we have that 
\begin{align*}
    \lambda_{j}(M_{\mu_1})&=\lambda_{j}\paren{(\Pir+\Pirperp)M_{\mu_1}(\Pir+\Pirperp)}\\
    &\geq\lambda_{j}\paren{\frac{1}{2}\Pir M_{\mu_1}\Pir+\Pirperp M_{\mu_1}\Pirperp}
                          +\lambda_{d-p}\paren{\frac{1}{2}\Pir M_{\mu_1}\Pir+\Pirperp M_{\mu_1}\Pir +\Pir M_{\mu_1}\Pirperp}\\
                          &\geq \lambda_{j}\paren{\frac{1}{2}\Pir M_{\mu_1}\Pir+\Pirperp M_{\mu_1}\Pirperp} - \epsilon_{\mu_1},
\end{align*}
where as $\mu_1\to{\lambda^a_i}^+$ it holds that $\epsilon_{\mu_1}\to 0$ with the same arguments as above. 

Note that each of the eigenvectors of $\frac{1}{2}\Pir M_{\mu_1}\Pir$ corresponding to a non-zero eigenvalue is orthogonal to each of the eigenvectors of  $\Pirperp M_{\mu_1}\Pirperp$ corresponding to a non-zero eigenvalue. Furthermore, $\lambda_1\paren{\frac{1}{2}\Pir M_{\mu_1}\Pir}\to \infty$ as $\mu_1\to{\lambda^a_i}^+$. Lastly, since $\Pirperp M_{\mu_1}\Pirperp$ has a convergent, finite limit, it holds that 
$$\lambda_{j}\paren{\frac{1}{2}\Pir M_{\mu_1}\Pir+\Pirperp M_{\mu_1}\Pirperp} = \lambda_{j-1}\paren{\Pirperp M_{\mu_1}\Pirperp},$$
as $\mu_1\to{\lambda^a_i}^+$.
This allows us to conclude that
$$\lambda_{j-1}\paren{\Pirperp M_{\mu_1}\Pirperp} - \epsilon_{\mu_1}\leq  \lambda_{j}(M_{\mu_1})\leq \lambda_{j-1}(M_{\mu_2})+\epsilon_{\mu_2}\leq\lambda_{j-1}(\Pirperp M_{\mu_2}\Pirperp)+\epsilon_{\mu_2}'.$$
Finally, by taking the limits $\mu_1\to{\lambda^a_i}^+$ and $\mu_2\to{\lambda^a_i}^-$ we get
\begin{equation}
    \lim_{\mu_1\to{\lambda^a_i}^+}L_j(\mu_1) = \lim_{\mu_2\to{\lambda^a_i}^-}L_{j-1}(\mu_2) = \lambda_{j-1}\paren{\Pirperp M_{\lambda_i^a}\Pirperp}.
    \label{eq:limitL_i}
\end{equation}
\end{proof}

From now on, we will refer to the function $\tilde{L}_i$ as the one extended to the whole $]\lambda_{i}^a,+\infty[$ for $i\in[p]$, and to the whole $]\lambda_{p}^a,+\infty[$ for $i\geq p+1$. The next result characterizes the behavior of $\tilde{L}_i$  
at the edges of its domain.

\begin{lemma}\label{lem:eiglimits}
    For $i\in [d-p]$, $\tilde{L}_i$ is non-increasing with the limit on the right edge of the domain given by 
    $$\lim_{\mu\to\infty}\tilde{L}_i(\mu)=\lambda_i(P).$$
    Moreover, for $i\in[p]$, the limit on the left edge of the domain is
    $$\lim_{\mu\to{\lambda_{i}^a}^+}\tilde{L}_i(\mu)=+\infty.$$
\end{lemma}
\begin{proof}
    First, we prove that each $L_i(\mu)$ is non-increasing in an arbitrary domain $]\lambda_j^a,\lambda_{j-1}^a[$. In fact, for any $\mu_1>\mu_2$ in that interval, it holds that
    \begin{align*}
        L_i(\mu_1)-L_i(\mu_2) &= \lambda_i(P-q(a-\mu_1 I_p)^{-1}q^\top) - \lambda_i(P-q(a-\mu_2 I_p)^{-1}q^\top)\\
        &\leq \lambda_{1}(q(a-\mu_2 I_p)^{-1}q^\top-q(a-\mu_1 I_p)^{-1}q^\top)\\
        &= \lambda_{1}\left(q\sum_{k=1}^p v_k^av_k^{a\top}\paren{\frac{\mu_2-\mu_1}{(\lambda_k^a - \mu_2)(\lambda_k^a - \mu_1)}}q^\top\right)\\
        &\leq 0,
    \end{align*}
    where the first inequality is due to Weyl's inequality.
    
    Thus, by the definition in \eqref{eq:defLtilde} and by \Cref{lemma:samelimits},  we have that the function $\tilde{L}_i$ is non-increasing in $]\lambda_i^a,+\infty[$, for $i\in[p]$. In the same manner, for $i>p$, it also holds that $\tilde{L}_i$ is non-increasing in $]\lambda_p^a,+\infty[$.
    Moreover, since $qv_i^a\neq 0$ and
    $$P-q(a-\mu I_p)^{-1}q^\top = P - \frac{(qv_i^a)(qv_i^a)^\top}{\lambda_i^a - \mu}-\sum_{k\neq i}^p \frac{(qv_k^a)(qv_k^a)^\top}{\lambda_k^a - \mu},$$
    it holds that, for any $i\in[p]$,
    $$\lim_{\mu\to{\lambda_{i}^a}^+}\tilde{L}_i(\mu) = \lim_{\mu\to{\lambda_{i}^a}^+}\lambda_1(P-q(a-\mu I_p)^{-1}q^\top) = +\infty.$$
    Finally, using the same formula, one also obtains that, for $i\in [d-p]$,
    \begin{align}
        \lim_{\mu\to\infty}\tilde{L}_i(\mu) &= \lim_{\mu\to\infty}\lambda_i(P-q(a-\mu I_p)^{-1}q^\top) = \lambda_i(P). \notag \qedhere
    \end{align}
 %   for any $i$.
\end{proof}
Having proven this properties, let us get back to discussing the eigenvalues $\lambda_i^D$. We do so by considering two cases.

\begin{lemma}\label{lemma:iffeig}
    An eigenvalue $\lambda_i^D\notin \Lambda^a$, $i\in [d-p]$, is a solution to 
    \begin{equation}\label{eq:eigvalchar}
        L_k(\mu) = \mu,
    \end{equation}
    for some $k$.
    Conversely, any solution to the previous equation is an eigenvalue of $D$ that is also not an eigenvalue of $a$.
\end{lemma}
\begin{proof}
    All eigenvalues of $D$ are exactly the solutions to
    \begin{equation}\label{eq:detwhole}
        \det\paren{D-\lambda I_{d}}=0.
    \end{equation}
   Applying the formula for the determinant of a block matrix implies
    $$\det\paren{D-\lambda I_{d}} = \det\paren{P-\lambda I_{d-p}-q(a-\lambda I_d)^{-1}q^\top}\det(a-\lambda I_d).$$
    As by assumption $\det(a-\lambda I_d)\neq 0$, \eqref{eq:detwhole} is equivalent to
    $$\det\paren{P-\lambda I_{d-p}-q(a-\lambda I_d)^{-1}q^\top}=0.$$ 
    Moreover by definition of the determinant and the fact that the matrix in questions is symmetric, it holds that
    \begin{align*}
        \det\paren{P-\lambda I_{d-p}-q(a-\lambda I_d)^{-1}q^\top} &= \prod_{i=1}^{d-p}\lambda_i\paren{P-\lambda I_{d-p}-q(a-\lambda I_d)^{-1}q^\top}\\
        &= \prod_{i=1}^{d-p}(L_i(\lambda)-\lambda).
    \end{align*}
    Therefore, we have that $$\det\paren{P-\lambda I_{d-p}-q(a-\lambda I_d)^{-1}q^\top}=0,$$
    if and only if there exists a $k$ and $\mu$ such that
    \begin{align}
        L_k(\mu)&=\mu.\qedhere
    \end{align}
\end{proof}
The case in which the eigenvalues of $D$ and $a$ overlap is covered by the result below. % the following lemma.
\begin{lemma}\label{lemma:iffeiga}
    An arbitrary eigenvalue $\lambda_i^D\in \Lambda^a$ is equal to $\lambda_j^a$ if and only if  
    \begin{equation}\label{eq:eigvalchara}
        \lim_{\mu\to{\lambda_j^a}^+} L_k(\mu) = \lambda_j^a,
    \end{equation}
    for some $k\geq 2$.
\end{lemma}
\begin{proof}
    Let us first prove the only if part of the statement. We denote the eigenvector corresponding to $\lambda_i^D$ as $v_i^D = \matrix{h_i \\ g_i}$, where $h_i\in \bbR^p$, $g\in \bbR^{d-p}$. It follows that
$$ D v_i^D = \matrix{ a & q^\top\\
                    q & P}\matrix{h_i \\ g_i} = \lambda^a_j \matrix{h_i \\ g_i}.$$
Splitting this equation into $p$ and $d-p$ coordinates gives
\begin{align}
    a h_i + q^\top g_i &= \lambda^a_j h_i,\label{eq:firsteq}\\
    q h_i + P g_i &= \lambda^a_j g_i.\label{eq:secndeq}
\end{align}
Since $(a-\lambda^a_jI_p)$ is singular, its SVD decomposition is
$$(a-\lambda^a_jI_p) = \sum_{k=1}^p (\lambda_k^a-\lambda_j^a)v_k^av_k^{a\top}= \sum_{k\neq j}^p (\lambda_k-\lambda_j)v_k^av_k^{a\top}.$$
From  \eqref{eq:firsteq}, it follows that $(a-\lambda^a_jI_p)h_i = -q^\top g_i$. Then, plugging in the SVD, it holds that $\sum_{k\neq j}^p (\lambda_k-\lambda_j)v_k^av_k^{a\top} h_i= -q^\top g_i$. Multiplying both sides by $v_j^a$, due to the matrix being symmetric and thus eigenvectors orthogonal, it holds that
$$\sum_{k\neq j}^p (\lambda_k-\lambda_j)\inprod{v_k^a}{h_i}\inprod{v_k^a}{v_j^a} = 0 = -\inprod{q^\top g_i}{v_j^a}.$$
From there, we can conclude that $q^\top g_i \perp v_j^a$, which is equivalent to $g_i\perp qv_j^a$. Moreover, \eqref{eq:firsteq} can be rewritten as %out we can get that it is equivalent to having 
$$h_i = -(a-\lambda^a_jI_p)^\dagger q^\top g_i + \alpha v_j^a,$$
for some $\alpha$. Plugging this result into \eqref{eq:secndeq} gives %one has that it is equivalent to having a solution to
\begin{equation}\label{eq:dc} -q (a-\lambda^a_j I_p)^\dagger q^\top g_i + \alpha qv_j^a + P g_i = \lambda^a_j g_i,
\end{equation}
for some $\alpha$.
Let us, as before, denote by $r =q v_j^a/\|q v_j^a\|_2$, and by $\Pirperp$ the orthogonal projection to the subspace defined by $r^\perp$. As noted before, $g_i\perp qv_j^a$, which implies that $g_i=\Pirperp g_i$. Plugging it into \Cref{eq:dc} gives
$$ -q (a-\lambda^a_j I_p)^\dagger q^\top \Pirperp g_i + \alpha qv_j^a + P \Pirperp g_i = \lambda^a_j \Pirperp g_i.$$
Multiplying the previous equation on the left by $\Pirperp$ results in 
$$ -\Pirperp q (a-\lambda^a_j I_p)^\dagger q^\top \Pirperp g_i + \Pirperp P \Pirperp g_i = \lambda^a_j \Pirperp g_i.$$
This means that $\Pirperp g_i$ is an eigenvector of the matrix $-\Pirperp q (a-\lambda^a_j I_p)^\dagger q^\top \Pirperp + \Pirperp P \Pirperp$ with the corresponding eigenvalue $\lambda^a_j$, i.e.,
$$\lambda_k(\Pirperp (P-q (a-\lambda^a_jI_p)^\dagger q^\top)\Pirperp) = \lambda_j^a,$$
for some $k$. From \eqref{eq:limitL_i} we can see the LHS is exactly $\lim_{\mu\to{\lambda_j^a}^+} L_k(\mu)$ for some $k$. As proved in \Cref{lem:eiglimits}, it holds that $\lim_{\mu\to{\lambda_j^a}^+} L_1(\mu) = +\infty$, so it must be that $k\geq 2$.

Conversely, by following the same steps in reverse, if 
$\lim_{\mu\to{\lambda_j^a}^+} L_k(\mu)=\lambda_j^a$, then there is a vector $g_i$ that solves
$$ -q (a-\lambda^a_j)^\dagger q^\top g_i + \alpha qv_j^a + P g_i = \lambda^a_j g_i,$$
for some $\alpha$. By setting 
$$h_i = -(a-\lambda^a_jI_p)^\dagger q^\top g_i + \alpha v_j^a,$$
it follows that $w = \matrix{h_i \\ g_i}$ is an eigenvector of $D$ with eigenvalue $\lambda_j^a$ as stated.
\end{proof}

Combining the previous properties, we are now ready to prove \Cref{prop:eigvalrec}.

\begin{proof}[Proof of \Cref{prop:eigvalrec}.]
    Let us first prove that \eqref{eq:eigvalrec} has a unique solution, for $i\in[p]$. 
    Since $\tilde{L}_i(\mu)$ is non-increasing and continuous, we have that $\tilde{L}_i(\mu) -\mu$ is decreasing and continuous. Moreover, for $i\in[p]$ it has limits
    $$\lim_{\mu\to{\lambda_{i}^a}^+}\tilde{L}_i(\mu)-\mu=+\infty,\text{ and } \lim_{\mu\to\infty}\tilde{L}_i(\mu)-\mu=-\infty,$$
    due to \Cref{lem:eiglimits}. Then, applying the intermediate value theorem implies that there must be a unique $\mu_i$ for which $\tilde{L}_i(\mu_i) - \mu_i=0$.
    
    Next, let us prove that the unique solution $\mu_i$ of \eqref{eq:eigvalrec} is indeed an eigenvalue of $D$. First, suppose that $\mu_i=\lambda_j^a$, for some $j\in[p]$. Then, \eqref{eq:limitL_i} would imply that 
    $$\lim_{\mu\to{\lambda_j^a}^+} L_k(\mu) = \lambda_j^a,$$
    for some $k\geq 2$.
    Then, \Cref{lemma:iffeiga} implies that $\lambda_j^a$ is an eigenvalue of $D$. Next,  suppose  that $\mu_i\notin \Lambda^a$. Then, by definition of $\tilde{L}_i$, it holds that
    $$L_k(\mu_i)=\mu_i,$$
    for some $k$. \Cref{lemma:iffeig} then implies that $\mu_i$ must be an eigenvalue of $D$.
    
    Finally, let us prove that $\mu_i$ is exactly the $i$-th eigenvalue of $D$. To do so, we first prove that every eigenvalue of $D$ that is larger or equal to $\lambda_p^a$ is a solution to the following equation in $\mu$:
    $$\tilde{L}_m(\mu)=\mu,$$
    for some $m\in [d-p]$. This follows from \Cref{lemma:iffeig,lemma:iffeiga}, which imply that any eigenvalue of $D$ is covered by checking the conditions  
    $$ L_k(\mu) = \mu \text{ or } \lim_{\mu\to{\lambda_j^a}^+} L_k(\mu) = \lambda_j^a,$$
    which are all covered by considering $\tilde{L}_m(\mu)$ for $m\in [d-p]$.
    
    As $\tilde{L}_1(\mu)\geq \tilde{L}_2(\mu) \geq \dots \geq  \tilde{L}_p(\mu) \geq\dots\geq\tilde{L}_{d-p}(\mu)$ and $\lambda_1^D\geq \lambda_2^D\geq \dots \lambda_p^D$,
    it must be that the solution to \eqref{eq:eigvalrec} is exactly the $i$-th eigenvalue of the matrix $D$, and the proof is complete.
\end{proof}

We conclude this appendix with the proof of \Cref{prop:eigenvec}.
%\subsection{Eigenvectors}\label{subsec:appengenraleigvec}

\begin{proof}[Proof of \Cref{prop:eigenvec}.]
Note that \eqref{eq:generaleigvec} is equivalent to the system of two equations
    \begin{align}
        a h_i + q^\top g_i &= \lambda^D_i h_i,\label{eq:eig1}\\
        q h_i + P g_i &= \lambda^D_i g_i.\label{eq:eig2}
    \end{align}
    
    As we consider only the eigenvectors $v_i^D$ for $i\leq j$, the matrix $(P-\lambda_i^DI_{d-p})$ is invertible, and solving \eqref{eq:eig1} gives
    $$g_i = -(P-\lambda_i^DI_{d-p})^{-1}qh_i.$$
    Substituting in \eqref{eq:eig2} yields
        $$ah_i-q^\top(P-\lambda_i^DI_{d-p})^{-1}qh_i=\lambda_i^Dh_i.$$
    Let us denote by $\tilde{h}_i=\frac{h_i}{\norm{2}{h_i}}$ the unit norm eigenvector of $a - q^\top(P-\lambda_i^DI_{d-p})^{-1}q$ corresponding to the eigenvalue $\lambda_i^D$, and also define $\tilde{g}_i := -(P-\lambda_i^DI_{d-p})^{-1}q\tilde{h}_i$. Then, $\tilde{h}_i$ and $\tilde{g}_i$ satisfy equations \eqref{eq:eig1} and \eqref{eq:eig2}, so $\tilde{v}_i^D = \matrix{\tilde{h}_i\\ \tilde{g}_i}$ is aligned with an eigenvector corresponding to eigenvalue $\lambda_i^D$. However, $\tilde{v}_i^D$  does not necessarily have unit norm. It holds that
    $$\matrix{h_i\\ g_i} = v_i^D = \frac{\tilde{v}_i^D}{\norm{2}{\tilde{v}_i^D}} = \frac{\matrix{\tilde{h}_i\\ \tilde{g}_i}}{\sqrt{\tilde{h}_i^\top\tilde{h}_i+\tilde{g}_i^\top\tilde{g}_i}}=\frac{\matrix{\tilde{h}_i\\ \tilde{g}_i}}{\sqrt{1+\tilde{h}_i^\top q^\top(P-\lambda_i^DI_{d-p})^{-2}q\tilde{h}_i}},$$ from which follows that
    $$h_i = \frac{\tilde{h}_i}{\sqrt{1+\tilde{h}_i^\top q^\top(P-\lambda_i^DI_{d-p})^{-2}q\tilde{h}_i}}.$$
    The last thing to notice is that
    $$q^\top(P-\lambda I_{d-p})^{-2}q = -\frac{d}{d\lambda}(a-q^\top(P-\lambda I_{d-p})^{-1}q),$$
    from which the statement of the proposition follows.
\end{proof}

\section{Proofs for \Cref{subsec:asymptotic behavior}}\label{app:asymptotbehav}

As a consequence of \Cref{prop:eigvalrec}, the top $p$ eigenvalues of $D$ are entirely characterized by the functions $\tilde{L}_i$. As these functions are nothing more than patches of the functions $L_i$ on different domains, we first direct our attention to analyzing asymptotic behavior of $L_i(\mu) = \lambda_i(P-q(a-\mu I_p)^{-1}q^\top)$. Notice that
\begin{equation}\label{eq:defM}
    P-q(a-\mu I_p)^{-1}q^\top = \frac{1}{n} U^\top M_n U,
\end{equation}
where $M_n\coloneqq Z-\frac{1}{n}ZS(a-\mu I_p)^{-1}S^\top Z$ is a rank $p$ perturbation of the matrix $Z$.
\begin{lemma}\label{lemma:mueigvalconv}
    For each $\mu>0$,    let $\alpha_1\geq \dots\ \geq \alpha_j > \tau$ be all the solutions to the equation 
    \begin{equation}\label{eq:mueigvalmaster_eq}
        \det\paren{\mu I_p-R^\infty(\alpha)}=0.
    \end{equation}
    Then, for the top $j$ eigenvalues of $M_n$, it holds that
    \begin{equation}\label{conv:jeigconv}
        \lambda_1^M,\dots,\lambda_j^M \asconv \alpha_1, \dots, \alpha_j,
    \end{equation}
    and for the remaining $p-j$ eigenvalues, it holds that
    $$\lambda_{j+1}^M,\dots,\lambda_p^M \asconv \tau.$$
\end{lemma}
\begin{proof}
    Let us denote by $v\coloneqq ZS$.
    An arbitrary eigenvalue $\lambda_k^M$ of $M_n$ satisfies the equation
    $$\det\paren{Z-\frac{1}{n}v(a-\mu I_p)^{-1}v^\top-\lambda_k^M I_n}=0.$$
    Thus, for $\alpha>\max\{z_i\}$, consider the following equation
    $$\det\paren{Z-\frac{1}{n}v(a-\mu I_p)^{-1}v^\top-\alpha I_n}=0.$$
    As $Z-\alpha I_n$ is invertible for $\alpha>\max\{z_i\}$, we can apply the matrix determinant lemma to obtain the equivalent equation 
    \begin{equation}\label{eq:matdetlemmu}
        \det\paren{\mu I_p-a+\frac{1}{n}v^\top(Z-\alpha I_n)^{-1}v}=0.
    \end{equation}
    Moreover,
    $$ a - \frac{1}{n}v^\top(Z-\alpha I_n)^{-1}v = \frac{1}{n}\sum_{i=1}^n z_i s_i s_i^\top - \frac{1}{n}\sum_{i=1}^n \frac{z_i^2s_is_i^\top}{z_i-\alpha} = \frac{1}{n}\sum_{i=1}^n \frac{\alpha z_is_is_i^\top}{\alpha-z_i}.$$
    Thus, \eqref{eq:matdetlemmu} becomes 
    \begin{equation}\label{eq:prodeigenval}
        \det\paren{\mu I_p -  \frac{1}{n}\sum_{i=1}^n \frac{\alpha z_is_is_i^\top}{\alpha-z_i}}=0.
    \end{equation}
    Let us prove that, for $n$ large enough, this equation indeed has its top $j$ solutions for $\alpha>\max\{z_i\}$.
    First, note that
    $$\det\paren{\mu I_p -  \frac{1}{n}\sum_{i=1}^n \frac{\alpha z_is_is_i^\top}{\alpha-z_i}}=\prod_{i=1}^p\paren{\mu-\lambda_i(\alpha)},$$
    where through abuse of notation we define $\lambda_i(\alpha):]\max\{z_i\},+\infty[\to \bbR$ as $ \lambda_i\paren{\frac{1}{n}\sum_{i=1}^n \frac{\alpha z_is_is_i^\top}{\alpha-z_i}}$. Each function $\lambda_i$ is continuous and strictly decreasing. This can be seen by taking arbitrary $\alpha_2>\alpha_1$ to get
    $$\lambda_i(\alpha_1)-\lambda_i(\alpha_2)\geq \lambda_p\paren{(\alpha_2-\alpha_1)\frac{1}{n}\sum_{i=1}^n \frac{z_i^2s_is_i^\top}{(\alpha_1-z_i)(\alpha_2-z_i)}}>0,$$
    by using Weyl's inequality and the fact that there are almost surely at least $p$ linearly independent vectors $z_is_i$ by \Cref{lemma:fr}. 
    Consequently, to prove that \eqref{eq:prodeigenval} has $j$ solutions for $\alpha>\max\{z_i\}$, it equivalent to prove that, for each $i$, 
    \begin{equation}\label{eq:betas}
        \lambda_i(\beta_i')>\mu>\lambda_i(\beta_i''),
    \end{equation}
    for some $\beta_i''>\beta_i'>\max{z_i}$. 
    
    Note that, for any fixed $\alpha$, it holds that
    \begin{equation}\label{eq:eigenvalconvergence}
        \frac{1}{n}\sum_{i=1}^n \frac{\alpha z_is_is_i^\top}{\alpha-z_i}\asconv \expt{\frac{\alpha zss^\top}{\alpha-z}} = \Rinfty(\alpha),
    \end{equation}
    due to the law of large numbers. Due to the continuity of eigenvalues, it further follows that 
    $$\lambda_i\paren{\frac{1}{n}\sum_{i=1}^n \frac{\alpha z_is_is_i^\top}{\alpha-z_i}}\asconv \lambda_i\paren{\expt{\frac{\alpha zss^\top}{\alpha-z}}}=\lambda_i^\infty(\alpha),$$
    where $\lambda_i^\infty(\alpha)$ is continuous and strictly decreasing. This can be seen as, for any $\alpha>\tau$ and any arbitrary vector $x\in \bbR^p$, it holds that
    \begin{equation}\label{eq:matrixRdecreases}
        \frac{d}{d\alpha}\paren{x^\top\paren{\expt{\frac{\alpha zss^\top}{\alpha-z}}}x} = -\expt{\frac{\inprod{x}{s}^2z^2}{(\alpha-z)^2}}<0,
    \end{equation}
    since $\prob{z=0}<1$. 
    Moreover, for $i\in[p]$,
    $$\lim_{\alpha\to\infty}\lambda_i^\infty(\alpha) = \lambda_i^{a^\infty},$$
    where the matrix $a^\infty = \expt{zss^\top}$ is the limit of the matrix $a$.
    The condition of the lemma states that there exist $\alpha_1\geq \dots\ \geq \alpha_j > \tau$ such that
    $$\det\paren{\mu I_p-R^\infty(\alpha)}=0.$$
    Let us denote by $k\in\{0,\dots, p\}$ the index such that $\lambda_{k+1}^{a^\infty}\leq\mu<\lambda_k^{a^\infty}$, with the abuse of notation $\lambda_0^{a^\infty}\coloneq +\infty$ and $\lambda_{p+1}^{a^\infty}\coloneq -\infty$. 
    By assumption %it holds 
    \begin{equation}\label{eq:deteq}
        \det\paren{\mu I_p-R^\infty(\alpha)}=\prod_{i=0}^p(\mu-\lambda_i^\infty(\alpha)),
    \end{equation}
    has $j$ solutions in $\alpha \in ]\tau,+\infty[$. Note that $\lambda_i^\infty$ is a strictly decreasing continuous function, so the only way that $\lambda_i^\infty-\mu$ does not have a solution in $\alpha\in]\tau,+\infty[$ is if either
    $\lim_{\alpha\to\tau^+} \lambda_i^\infty(\alpha)<\mu$ or $\lim_{\alpha\to\infty} \lambda_i^\infty(\alpha)>\mu.$
    Moreover, since $\lim_{\alpha\to\infty} \lambda_i^\infty(\alpha) = \lambda_i^{a^\infty}$, it will exactly hold for $i\in\{1,\dots k\}$ that 
    \begin{equation}\label{eq:larberthanmu}
        \lim_{\alpha\to\infty} \lambda_i^\infty(\alpha)\geq\lambda_k^{a^\infty}>\mu.
    \end{equation}
    The fact that there are only $j$ solutions to \eqref{eq:deteq} and $\lambda_i^\infty(\alpha)>\lambda_{i+1}^\infty(\alpha)$ implies that 
    $$\lambda_{i+k}^\infty(\alpha_i) = \mu,$$
    for $i\in [j]$,
    as well as
    \begin{equation}\label{eq:smallerthanmu}
        \lim_{\alpha\to\infty} \lambda_i^\infty(\alpha)<\mu,
    \end{equation}
    for $i\in\{j+k+1,\dots,p\}$.
    
    As each $\lambda_i^\infty$ is a strictly decreasing continuous function, this further implies that there exists some constant $\epsilon>0$ and $\alpha_1',\dots, \alpha_j'>\tau$ such that 
    $$\lambda_{i+k}^\infty(\alpha_i') = \mu+\epsilon.$$
    Applying the convergence of \eqref{eq:eigenvalconvergence} it further holds that 
    $$\lambda_{i+k}\paren{\frac{1}{n}\sum_{i=1}^n \frac{\alpha_i' z_is_is_i^\top}{\alpha_i'-z_i}} \asconv \lambda_{i+k}^\infty(\alpha_i')=\mu+\epsilon.$$
    Thus, for each $i$ and $\epsilon>\epsilon_1>0$, there exists $n_0$ s.t.\ for $n>n_0$ 
    $$\abs{\lambda_{i+k}\paren{\frac{1}{n}\sum_{i=1}^n \frac{\alpha_i' z_is_is_i^\top}{\alpha_i'-z_i}} - (\mu+\epsilon)}< \epsilon_1.$$
    Developing the absolute value, it holds that 
    $$ \lambda_{i+k}\paren{\frac{1}{n}\sum_{i=1}^n \frac{\alpha_i' z_is_is_i^\top}{\alpha_i'-z_i}} > \mu+\epsilon-\epsilon_1>\mu,$$
    e.g.\ by taking $\epsilon_1=\epsilon/2$.
    As $\lambda_{i+k}(\alpha) = \lambda_{i+k}\paren{\frac{1}{n}\sum_{i=1}^n \frac{\alpha z_is_is_i^\top}{\alpha-z_i}}$ is a continuous decreasing function, starting from some $n_0$ there exists $\beta_i'>\tau$ s.t.\ $\lambda_{i+k}(\beta_i') > \mu$. Notice that by definition $\tau>\max{z_i}$ almost surely. In the same way as for $\beta_i'$ it can be proved that there exists $\beta_i''>\tau$ such that $\lambda_{i+k}(\beta_i'') < \mu$.
    
    Thus, we conclude that, for large enough $n$,  $\lambda_{i+k}(\alpha)=\mu$ has $j$ solutions larger than $\max\{z_i\}$.
    These are indeed $\lambda_i^M$ for $1\leq i\leq j$. Due to monotonicity, each $\lambda_i$ admits a functional inverse and it holds that 
    $$\lambda_i^M = \lambda_{i+k}^{-1}(\mu).$$
    As $\lambda_{i+k} \asconv \lambda_{i+k}^\infty$, applying \cite[Lemma A.1]{lu2020phase} implies that
    \begin{equation}\label{eq:klimit}
        \lambda_i^M \asconv (\lambda_{i+k}^\infty)^{-1}(\mu),
    \end{equation}
    for $1\leq i\leq j$, which is exactly the statement \eqref{conv:jeigconv} of the lemma.

    Let us now prove the second part of the statement. To do so, we prove that, for large enough $n$, \eqref{eq:prodeigenval} has no more than $j$ solution for $\alpha>\max\{z_i\}$. 
    As stated in \eqref{eq:larberthanmu} and \eqref{eq:smallerthanmu}, it holds that $
\lim_{\alpha\to\infty}\lambda_i^\infty(\alpha)>\mu$ for $i$ s.t.\ $1\leq i\leq k$ and that $\lim_{\alpha\to\tau^+}\lambda_i^\infty(\alpha)<\mu$ for $i$ s.t.\ $j+1+k\leq i\leq p$. Thus, using the same argument as before, we also have that, for large enough $n$ and any $\alpha>\tau$, $\lambda_i(\alpha)>\mu$ for $i$ s.t.\ $1\leq i\leq k$ and $\lambda_i(\alpha)<\mu$ for $i$ s.t.\ $j+1+k\leq i\leq p$. Since  %   \begin{equation}\label{eq:lambdainequalities}
%        \lambda_i(\alpha)>\mu, \text{ or respectively } \lambda_i(\alpha)<\mu
%    \end{equation}
%    for any $\alpha>\tau$, and $1\leq i\leq k$, or respectively $j+1+k\leq i\leq p$. 
%    Note that 
    $\max\{z_i\}\asconv \tau$ (see e.g.\ \cite[Section 4]{silverstein1995analysis}), such inequalities also hold for $\alpha>\max\{z_i\}$ (and $n$ large enough).
    Hence, there cannot exist $\beta_i'$ and $\beta_i''$ that satisfy \eqref{eq:betas}, so \eqref{eq:prodeigenval} cannot have more than $j$ solutions in $\alpha> \max\{z_i\}$. 
    
    From this, it directly follows that, for $n$ large enough and any $l\in\{j+1\dots p\}$,  
    $$\lambda_l^M\leq \max\{z_i\},$$
    almost surely. Furthermore, from the interlacing theorem, it holds that 
    $$\lambda_{p+1}^Z\leq\lambda_l^M,$$
    for any $l\in\{j+1\dots p\}$. Thus, the $\lambda_l^M$'s are sandwiched between the first and the $p$-th eigenvalue of $Z$, both of which converge to the right edge of the bulk $\tau$ \cite[Section 4]{silverstein1995analysis}, which gives the desired result.
\end{proof}


\begin{proposition}\label{prop:tildeLiconv}
    For any fixed $\mu\in]\lambda_i^{a^\infty}, t_i^\infty[$, it holds that
    \begin{equation}\label{eq:p1}
    \tildeLi(\mu) \asconv \tildeLiinfty(\mu)=\zeta_\delta\paren{(\lambda^\infty_i)^{-1}(\mu)}.    
    \end{equation}
Furthemore, for any fixed $\mu\in]t_i^\infty,+\infty[$, it holds that 
\begin{equation}\label{eq:p2}
    \tildeLi(\mu) \asconv\zeta_\delta\paren{\lambdabardelta}.
\end{equation}
\end{proposition}
\begin{proof}
    Let $k$ be such that $\lambda_{k+1}^{a^\infty}\leq\mu<\lambda_k^{a^\infty}$, with the abuse of notation $\lambda_0^{a^\infty}\coloneq +\infty$. Then, by the definition of $\tildeLi$ in \eqref{eq:defLtilde}, it holds that
    \begin{equation}\label{eq:Likconv}
        \tildeLi(\mu) = L_{i-k}(\mu),
    \end{equation}
    for $n$ large enough. This is true since $\lambda_i^a\asconv  \lambda_i^{a^\infty}$, as $a\asconv a^\infty$.
    
    To obtain the convergence of the RHS in \eqref{eq:Likconv}, we rely on the results from \cite{bai-yao-2012}.
    Towards this end, we recall the definition of $L_{i-k}(\mu) = \lambda_{i-k}(P-q(a-\mu I_p)^{-1}q^\top)$ as in \eqref{eq:defLi}. Given the equality in \eqref{eq:defM}, %it holds
%    $$P-q(a-\mu I_p)^{-1}q^\top = \frac{1}{n} U^\top M U,$$
%    and 
    we turn our attention to the eigenvalues of $M_n$. Recall that the functions $\lambda^\infty_i(\alpha)$ are strictly decreasing with limits
    $$\lim_{\alpha\to\tau^+} \lambda^\infty_i(\alpha) = t_i^\infty, \text{ and } \lim_{\alpha\to+\infty} \lambda^\infty_i(\alpha) = \lambda_i^{a^\infty}.$$
    Then, as $\mu\in]\lambda_i^{a^\infty}, t_i^\infty[$, the equation
    $$\lambda^\infty_i(\alpha)=\mu$$
    has a unique solution in $\alpha>\tau$. Let us denote that solution by $\alpha_{i-k}$. Due to the fact that 
    $$\lambda_{i}^{a^\infty}\dots\leq\lambda_{k+1}^{a^\infty}\leq\mu<\lambda_k^{a^\infty} \text{ and } \mu < t_i^\infty\dots<t_{1}^\infty,$$ using the same argument as in the proof of \Cref{lemma:mueigvalconv} below \eqref{eq:deteq}, we conclude that there are unique solutions $\alpha_1,\dots,\alpha_{i-k}$ s.t.\ $\lambda_{j+k}(\alpha_j)=\mu$ for $j\in[i-k]$. Then, it holds that $\alpha_1, \dots,  \alpha_{i-k}$ satisfy the conditions of \Cref{lemma:mueigvalconv}. From its proof, specifically \eqref{eq:klimit}, it follows that
    $$\lambda_{i-k}^M\asconv {\lambda_{i}^\infty}^{-1}(\mu) = \alpha_{i-k}.$$
    Furthermore, the empirical spectral distribution of $M_n$ converges almost surely to the distribution of $z$.
    This claim follows from Cauchy's interlacing theorem, using the same argument as in the proof of \cite[Proposition 3.2]{lu2020phase}. Assuming that the preprocessing function $\cT$ is positive, we can apply \cite[Theorems 4.1 and 4.2]{bai-yao-2012} to get
    $$L_{i-k}(\mu)\asconv\zeta_\delta\paren{(\lambda^\infty_i)^{-1}(\mu)},$$
    following the steps in \cite[Proposition 3.3]{lu2020phase}. Finally, the adjustment in  \cite[Lemma 3]{mondelli-montanari-2018-fundamental} covers the case in which the preprocessing function is not necessarily positive, and the proof of \Cref{eq:p1} is complete. 

Let us now consider  $\mu\in]t_i^\infty,+\infty[$ and prove \Cref{eq:p2}. 
    Note that, in this interval of $\mu$, the equation
    $$\lambda^\infty_i(\alpha)=\mu$$
    has no solutions in $\alpha>\tau$. Let us examine the equation \eqref{eq:mueigvalmaster_eq} in \Cref{lemma:mueigvalconv}:
    $$\det\paren{\mu I_p-R^\infty(\alpha)}=\prod_{l=1}^p(\mu-\lambda_l^\infty(\alpha))=0.$$
    The previous equation has a solution $\mu-\lambda_l^\infty(\alpha)=0$, as long as
    \begin{equation}\label{eq:intervalinequal}
        \lambda_l^{a^\infty}<\mu<t_l^\infty,
    \end{equation}
    due to the monotonicity of each $\lambda_l^\infty(\alpha)$.
    As 
    $$\lambda_{i}^{a^\infty}\dots\leq\lambda_{k+1}^{a^\infty}\leq\mu<\lambda_k^{a^\infty} \text{ and } \mu>  t_i^\infty\dots\geq t_{p}^\infty,$$ 
    it follows that \eqref{eq:intervalinequal}, thus also \eqref{eq:mueigvalmaster_eq}, can have at most $i-(k+1)$ solutions. Thus, applying \Cref{lemma:mueigvalconv} it follows that 
    $$\lambda_{i-k}^M\asconv \tau.$$
    As the empirical spectral distribution of $M_n$ converges almost surely to the distribution of $z$, we conclude that 
    $$\tildeLi(\mu) = L_{i-k}(\mu)\asconv \zetadelta(\lambdabardelta),$$
    as the limit of the right edge of the bulk distribution  by 
    \cite[Lemma 3.1]{bai-yao-2012}.
\end{proof}


\begin{proposition}\label{prop:exactlyp}
The equation in \Cref{eq:master_eq2} has at most $p$ solutions. Furthermore, if \Cref{assmp:psol} holds, then \Cref{eq:master_eq2} has exactly $p$ solutions. 
\end{proposition}

\begin{proof}
    As stated in the proof of \Cref{thm:eigvalconv}, it holds that
    \begin{equation}\label{eq:detzetali}
        \det\paren{\zetadelta(\alpha)I-R^\infty(\alpha)}= \prod_{i=1}^p (\zetadelta(\alpha) - \lambda_i^\infty(\alpha)).
    \end{equation}
    Note that the function $\zetadelta(\alpha) - \lambda_i^\infty(\alpha)$ is continous and strictly increasing for $\alpha\in]\tau,+\infty[$, so that 
    $$\lim_{\alpha\to\infty}\zetadelta(\alpha) - \lambda_i^\infty(\alpha) = +\infty.$$
    Thus, the equation in \Cref{eq:master_eq2} has at most $p$ solutions. Furthermore, the assumption in \Cref{assmp:psol} implies that 
    $$\inf_{\norm{2}{x}=1}\lim_{\alpha\to\tau^+}x^\top R^\infty(\alpha)x = +\infty,$$
    which is equivalent to
    $$\lim_{\alpha\to\tau^+}\lambda_i^\infty(\alpha) = +\infty.$$
    As $\lim_{\alpha\to\tau^+}\zetadelta(\alpha) = \lambdabardelta<+\infty$, it then holds that
    $$\lim_{\alpha\to\tau^+}\zetadelta(\alpha) - \lambda_i^\infty(\alpha) = -\infty,$$
    proving there must be exactly $p$ solutions to \Cref{eq:master_eq2} due to the intermediate  value theorem.
\end{proof}




\begin{proposition}
\label{prop:mastereigenvectors}
    Let $\alpha_k$ be the $k$-th solution to the equation
    $$\det\paren{\zetadelta(\alpha)I-a^\infty+R^\infty(\alpha)}=0,$$
    with multiplicity one. If $\alpha_k>\lambdabardelta$, then for $j\in[p]$ it holds that
    $$\abs{\inprod{v_k^D}{e_j^{(d)}}}^2\asconv \frac{\zetadelta'(\alpha_k)\cdot \abs{\inprod{h_k^\infty}{e_j^{(p)}}}^2}{\zetadelta'(\alpha_k)+{h_k^\infty}^\top \frac{d}{d\alpha}R^\infty(\alpha_k)h_k^\infty},$$
    where $h_k^\infty$ is the unit norm eigenvector corresponding to the eigenvalue $\zetadelta(\alpha_k)$ of the matrix $R^\infty(\alpha_k)$. 
\end{proposition}
\begin{proof}
    Let $v_k^D\coloneqq\matrix{h_k\\ g_k}$. Since $\alpha_k>\lambdabardelta$, the conditions of \Cref{prop:eigenvec} are satisfied as in the proof of \Cref{thm:matrixRconvergence}. Thus, it holds that
    $$h_k = \frac{\tilde{h}_k}{\sqrt{1+\tilde{h}_k^\top\frac{d}{d\lambda}R(\lambda_k^D)\tilde{h}_k}},$$
    where $\tilde{h}_k$ is the unit norm eigenvector of $R(\lambda_k^D)$. Furthermore, \Cref{thm:matrixRconvergence} gives that
    $$R(\lambda_k^D)\asconv R^\infty(\alpha_k), \qquad \frac{d}{d\lambda}R(\lambda_k^D) \asconv \frac{1}{\zetadelta'(\alpha_k)}\frac{d}{d\alpha}R^\infty(\alpha_k).$$
    Then, applying the results from \cite[II.1.4]{kato2013perturbation}, %(as proved in this StackExchange \href{https://math.stackexchange.com/questions/4054792/convergence-of-eigenvalues-and-spaces-of-sequence-of-compact-szmmetric-and-posi}{answer}).
    it holds that the orthonormal projection to the eigenspace corresponding to the $k$-th eigenvalue also converges, that is 
    $$\Pi_{h_k} \asconv \Pi_{h^\infty_k},$$
    where $\Pi_{h_k} = \frac{h_kh_k^\top}{\norm{2}{h_k}^2} = \tilde{h}_k\tilde{h}_k^\top$ and $\Pi_{h^\infty_k} = \frac{h^\infty_k{h^\infty_k}^\top}{\norm{2}{h^\infty_k}^2}=h^\infty_k{h^\infty_k}^\top$. As a consequence, it holds that
    $$\norm{2}{h_k}=\frac{1}{\sqrt{1+\tilde{h}_k^\top\frac{d}{d\lambda}R(\lambda_k^D)\tilde{h}_k}}\asconv \frac{\sqrt{\zetadelta'(\alpha_k)}}{\sqrt{\zetadelta'(\alpha_k)+{h^\infty_k}^\top\frac{d}{d\lambda}R^\infty(\alpha_k)h^\infty_k}}.$$
    Combining these results we obtain that
    $$h_kh_k^\top\asconv \frac{\zetadelta'(\alpha_k)h^\infty_k {h^\infty_k}^\top}{\zetadelta'(\alpha_k)+{h^\infty_k}^\top\frac{d}{d\lambda}R^\infty(\alpha_k)h^\infty_k},$$
    which proves the claim as
    \begin{align}
        \abs{\inprod{v_k^D}{e_j^{(d)}}}^2 &= (e_j^{(p)})^\top h_kh_k^\top (e_j^{(p)}). \notag \qedhere
    \end{align}
\end{proof}

\section{Invariance of the eigenspace for permutation-invariant link functions}\label{app:invariance}

\begin{proposition}\label{prop:invlink}
    If the link function $q$ is permutation invariant in $m$ coordinates, then the matrix $R^\infty(\alpha)$  has eigenspaces that do not change with $\alpha$, of combined dimension $m-1$.
\end{proposition}
\begin{proof}
    Let us denote by $w_i$ the $i$-th column of the matrix $\wt{W}^*$ as in \eqref{eqn:W*}, representing the top-$p$ entries of the reparametrized signal.
    Without loss of generality, we can assume that $q$ is permutation invariant in the first $m$ coordinates, i.e.,
    $$q(t_1,\dots, t_m,t_{m+1}\dots,t_p,\epsilon) = q(t_{\pi(1)},\dots, t_{\pi(m)},t_{m+1}\dots,t_p,\epsilon),$$
for any permutation $\pi:[m]\to[m]$.
    Let $E$ be the span of $\{w_i-w_{i+1}\,:\, i\in[m-1]\}$. Note that $E$ has dimension $m-1$, due to the linear independence of the signals $w_i$. We will prove that $E$ is a direct sum of eigenspaces of $R^\infty(\alpha)$, neither of which depends on $\alpha$.
    
    Let us define $u_i$ as the image of $w_i-w_{i+1}$ under $R^\infty(\alpha)$, that is
    \begin{align*}
        u_i \coloneq R^\infty(\alpha)(w_i-w_{i+1}) = \alpha \expt{ \frac{s \inprod{s}{w_i-w_{i+1}}\cT(y)}{\alpha -\cT(y)}},
    \end{align*}
    for $i\in[m-1]$. Let us denote by $x_i$ the component of $u_i$ that is orthogonal to $w_i$ and $w_{i+1}$, i.e., $x_i \coloneq \Pi_{\{w_i,w_{i+1}\}^\perp}u_i$. Then, it holds that
    \begin{equation}\label{eq:uidecomp}
        u_i=a_1 w_i+ a_2 w_{i+1} + x_i,
    \end{equation}
    for some coefficients $a_1$ and $a_2$. We will first prove that $x_i=0$. Towards that end, let $\cS_i:\bbR^p\to \bbR^p$ be an isometric reflection that sends $w_i$ to $w_{i+1}$, $w_{i+1}$ to $w_i$, and keeps $w_j$ fixed for $j\notin \{i,i+1\}$. Such a reflection exists due to the assumed linear independence of the $w_i$'s. Notice that the normal distribution in $\bbR^p$ is invariant to the transformation $\cS_i$. Thus, it follows that
    \begin{align*}
        \inprod{u_i}{x_i} &=\alpha \expt{ \frac{\inprod{s}{x_i} \inprod{s}{w_i-w_{i+1}}\cT(y)}{\alpha -\cT(y)}}\\
        &=\alpha \expt{ \frac{\inprod{s}{x_i} \inprod{s}{w_i}\cT(y)}{\alpha -\cT(y)}} - \alpha \expt{ \frac{\inprod{s}{x_i} \inprod{s}{w_{i+1}}\cT(y)}{\alpha -\cT(y)}}\\
        &=\alpha \expt{ \frac{\inprod{\cS_i s}{x_i} \inprod{\cS_i s}{w_i}\cT(q((\wt{W}^*)^\top \cS_i s, \eps))}{\alpha -\cT(q((\wt{W}^*)^\top \cS_i s, \eps))}} - \alpha \expt{ \frac{\inprod{s}{x_i} \inprod{s}{w_{i+1}}\cT(y)}{\alpha -\cT(y)}}\\
        &=\alpha \expt{ \frac{\inprod{s}{x_i} \inprod{s}{w_{i+1}}\cT(q((\wt{W}^*)^\top s, \eps))}{\alpha -\cT(q((\wt{W}^*)^\top s, \eps))}} - \alpha \expt{ \frac{\inprod{s}{x_i} \inprod{s}{w_{i+1}}\cT(y)}{\alpha -\cT(y)}}\\
        &= 0,
    \end{align*}
    due to the permutation invariance of $q$, the fact that $\cS_i x_i = x_i$ and $\cS_i w_i = w_{i+1}$. Therefore, it must be that $x_i=0$.    
    Moreover,
   \begin{align*}
       \inprod{u_i}{w_i}&=\alpha \expt{ \frac{\inprod{s}{w_i} \inprod{s}{w_i-w_{i+1}}\cT(y)}{\alpha -\cT(y)}}\\
       &=\alpha \expt{ \frac{\inprod{\cS_i s}{w_i} \inprod{\cS_i s}{w_i-w_{i+1}}\cT(q((\wt{W}^*)^\top \cS_i s)}{\alpha -\cT(q((\wt{W}^*)^\top \cS_i s)}}\\
       &=\alpha \expt{ \frac{\inprod{s}{w_{i+1}} \inprod{s}{w_{i+1}-w_i}\cT(q((\wt{W}^*)^\top  s)}{\alpha -\cT(q((\wt{W}^*)^\top s)}}\\
       &= \alpha \expt{ \frac{\inprod{s}{w_{i+1}} \inprod{s}{w_{i+1}-w_i}\cT(y)}{\alpha -\cT(y)}} \\
       &= -\inprod{u_i}{w_{i+1}}.
   \end{align*}
    Combining this with the decomposition in \eqref{eq:uidecomp} implies that
    $a_1+a_2\inprod{w_i}{w_{i+1}} = -a_1\inprod{w_{i+1}}{w_i} - a_2.$
    As by assumption the signals $w_i$ and $w_{i+1}$ are linearly independent, it cannot be that $\inprod{w_i}{w_{i+1}}=-1$, so it must be that $a_1=-a_2$. This proves that $w_i-w_{i+1}$ are indeed eigenvectors for every $\alpha$. 
    By definition, it holds that
    $$E=\bigoplus_{i=1}^{m-1}\operatorname{span}\{u_i\}.$$
    Thus, it is left to prove that any pair of eigenvectors $u_i$ and $u_j$, for $i,j\in[m-1]$, either have the same eigenvalue for all $\alpha$, or for no $\alpha$. We denote by $\lambda_{u_i}(\alpha)$ the eigenvalue that corresponds to the eigenvector $u_i$. Similar to before, let $\cS^{(i)}:\bbR^p\to \bbR^p$ be an isometric reflection that sends $w_i$ to $w_{i+2}$, $w_{i+2}$ to $w_i$, and keeps $w_j$ fixed for $j\notin \{i,i+2\}$. Such a reflection exists due to the assumed linear independence of the $w_i$'s. Also, the normal distribution is invariant to the transformation $\cS^{(i)}$. Then, it follows that
    \begin{align*}
        \lambda_{u_i}(\alpha)\cdot \norm{2}{w_i-w_{i+1}}^2 = \inprod{u_i}{w_i-w_{i+1}}&= \alpha \expt{ \frac{\inprod{s}{w_i-w_{i+1}}^2\cT(y)}{\alpha -\cT(y)}}\\
                      &= \alpha \expt{ \frac{\inprod{\cS^{(i)} s}{w_i-w_{i+1}}^2\cT(q((\wt{W}^*)^\top \cS^{(i)} s, \eps))}{\alpha -\cT(q((\wt{W}^*)^\top \cS^{(i)} s, \eps))}}\\
                      &= \alpha \expt{ \frac{\inprod{ s}{w_{i+2}-w_{i+1}}^2\cT(q((\wt{W}^*)^\top s, \eps))}{\alpha -\cT(q((\wt{W}^*)^\top s, \eps))}}\\
                      &=\inprod{u_{i+1}}{w_{i+1}-w_{i+2}} = \lambda_{u_{i+1}}(\alpha)\cdot \norm{2}{w_{i+2}-w_{i+1}}^2.
    \end{align*}
    This proves that $\lambda_{u_i}(\alpha)/\lambda_{u_{i+1}}(\alpha)$ does not depend on $\alpha$ and, hence, $\lambda_{u_i}(\alpha)/\lambda_{u_{j}}(\alpha)$ does not depend on $\alpha$ for all $i,j\in[m-1]$, implying  the existence of an eigenspace of dimension $m-1$ that does not change with $\alpha$.
\end{proof}


\section{Proof of \Cref{thm:opt}}\label{app:pfopt}

\begin{proof}
%\paragraph{Proof of \Cref{thm:opt}}
%We want to find a preprocessing function $\cT$ for a given $\delta$ satisfying \Cref{asmp:proportional}, such that the spectral estimator $\wh{W}^{\mathrm{s}} = \matrix{ v_1^D, & \cdots, & v_p^D } $  weakly recover the signals, as in the \Cref{def:weak_rec}. It is not too hard to see, that for the signals of the form in \eqref{eqn:W*},
%this is equivalent to finding a $\cT$ such that
%\begin{align*}
%        \max_{j\in[p]} \brace{ \liminf_{d\to\infty} \sum_{i=1}^p \abs{\inprod{v_i^D}{e_j}}^2 } &> 0 . 
%\end{align*}
%As a consequence of \Cref{thm:main}, it would be sufficient to prove that there exists a solution $\alpha$ to the equation \eqref{eq:master_eq2} such that $\alpha>\lambdabardelta$.
Note that $\zetadelta(\alpha)$ is a strictly monotone function for $\alpha>\lambdabardelta$, and it is constant for $\alpha\leq \lambdabardelta$. Thus, the existence of $\alpha$ solving \Cref{eq:master_eq2} s.t.\ $\alpha>\lambdabardelta$ is equivalent to $\zetadelta'(\alpha_1)>0$, where $\alpha_1$ is the largest solution of \eqref{eq:master_eq2}.
Thus, $\alpha_1$ is the largest solution to
 $$\det\paren{\zetadelta(\alpha)I_p-R^\infty(\alpha)}=0,$$
 or equivalently
 $$\lambda_1 (R^\infty(\alpha)) = \zetadelta(\alpha).$$
 This means that, for \eqref{eq:master_eq2} to have solutions larger than $\lambdabardelta$, there has to exist $\alpha_1>\tau$ such that
 \begin{align*}
     \max_{\norm{2}{u}=1} u^\top R^\infty(\alpha_1)u &= \zetadelta(\alpha_1),\\
     \zetadelta'(\alpha_1) &>0,
 \end{align*}
or equivalently
 \begin{align}\label{eq:conditionlambdabar}
     \max_{\norm{2}{u}=1} u^\top R^\infty(\lambdabardelta)u &> \zetadelta(\lambdabardelta).
 \end{align}
 This follows from the fact that $\zetadelta(\alpha)$ is strictly increasing for $\alpha>\lambdabardelta$, and $u^\top R^\infty(\alpha)u$ is strictly decreasing, as proved in \eqref{eq:matrixRdecreases}.
 Recall that $\lambdabardelta$ is defined as the unique point that satisfies $ \psidelta'(\lambdabardelta) =0$. This is equivalent to
 \begin{equation}\label{eq:lambdabardef}
     \expt{\frac{z^2}{(\lambdabardelta-z)^2}}=\frac{1}{\delta}. 
 \end{equation}
By definition, it holds that
$$R^\infty(\lambdabardelta) =
\expt{\frac{\lambdabardelta z}{\lambdabardelta-z}s^\top s }.$$
Therefore, the condition in \eqref{eq:conditionlambdabar} becomes 
\begin{equation}\label{eq:conditioninequality}
    \max_{\norm{2}{u}=1} \expt{\frac{\lambdabardelta z}{\lambdabardelta-z}\inprod{s}{u}^2 } > \lambdabardelta\paren{\frac{1}{\delta}+\expt{\frac{z}{\lambdabardelta-z}}}.
\end{equation}
Note that $D_n$ and $D_n'\coloneqq D_n/\beta$ have the same principal eigenvector for an arbitrary scalar $\beta>0$ so, without loss of generality, we can take $\lambdabardelta=1$. This transforms \eqref{eq:conditioninequality} and \eqref{eq:lambdabardef} into 
\begin{align*}
    \max_{\norm{2}{u}=1} \expt{\frac{z(\inprod{s}{u}^2-1)}{1-z}} &> \frac{1}{\delta},\\
    \expt{\frac{z^2}{(1-z)^2}}&=\frac{1}{\delta}.
\end{align*}

We turn our attention to finding the critical threshold $\delta_c$ such that no preprocessing function $\cT$ exists which would satisfy these equations. To start, plugging in the definition of the expectation we get 
\begin{equation}\label{eq:condrew}
\begin{split}
    \max_{\norm{2}{u}=1}  \int_{\bbR}\frac{\cT(y)}{1-\cT(y)} \expt[s]{p(y \mathrel{\vert} s)\cdot(\inprod{s}{u}^2-1)} dy &> \frac{1}{\delta},\\
    \int_{\bbR}\paren{\frac{\cT(y)}{1-\cT(y)}}^2 \expt[s]{p(y \mathrel{\vert} s)} dy &=\frac{1}{\delta}.
\end{split}
\end{equation}
Let us denote by $f(y)\coloneqq \frac{\cT(y)}{1-\cT(y)}$. Note that %it holds that
\begin{align}
    \frac{1}{\delta}&<\max_{\norm{2}{u}=1}\int_{\bbR}f(y) \expt[s]{p(y \mathrel{\vert} s)\cdot(\inprod{s}{u}^2-1)} dy \nonumber\\
    &= \max_{\norm{2}{u}=1}
    \int_{\bbR}f(y) \sqrt{\expt[s]{p(y \mathrel{\vert} s)}} \frac{\expt[s]{p(y \mathrel{\vert} s)\cdot(\inprod{s}{u}^2-1)}}{\sqrt{\expt[s]{p(y \mathrel{\vert} s)}}}dy \nonumber\\
    &\leq \max_{\norm{2}{u}=1}\sqrt{\int_{\bbR} f^2(y) \expt[s]{p(y \mathrel{\vert} s)}dy}\sqrt{\int_{\bbR}\frac{\paren{\expt[s]{p(y \mathrel{\vert} s)\cdot(\inprod{s}{u}^2-1)}}^2}{\expt[s]{p(y \mathrel{\vert} s)}}dy}\label{ineq:Hoelders}\\
    &= \max_{\norm{2}{u}=1} \frac{1}{\sqrt{\delta}} \sqrt{\int_{\bbR}\frac{\paren{\expt[s]{p(y \mathrel{\vert} s)\cdot(\inprod{s}{u}^2-1)}}^2}{\expt[s]{p(y \mathrel{\vert} s)}}dy},\nonumber
\end{align}
where the third line follows from H\"older's inequality and the fourth line from the second condition in \eqref{eq:condrew}. This means that, regardless of which preprocessing function $\cT$ was chosen, if it satisfies \eqref{eq:condrew}, then it must hold that 
$$ \frac{1}{\delta} < \max_{\norm{2}{u}=1}\int_{\bbR}\frac{\paren{\expt[s]{p(y \mathrel{\vert} s)\cdot(\inprod{s}{u}^2-1)}}^2}{\expt[s]{p(y \mathrel{\vert} s)}}dy =:\frac{1}{\delta_t}.$$
In other words, for any $\delta\leq \delta_t$, the set $\sT_\delta=\emptyset$.
By the definition of $\delta_c$ in \eqref{eq:deltacdef}, it follows that
\begin{equation}\label{eq:ineqdelta}
    \delta_c \geq \delta_t.
\end{equation}
Now, let us prove that, for any $\delta>\delta_t$, there exists a preprocessing function such that  $\sT_\delta\neq\emptyset$. Towards this end, we turn our attention to when equality holds in \eqref{ineq:Hoelders}, as this gives us the preprocessing function that exactly matches the upper bound. Namely, this is true if and only if almost everywhere
$$ f^2(y) \expt[s]{p(y \mathrel{\vert} s)} = c\cdot\frac{\paren{\expt[s]{p(y \mathrel{\vert} s)\cdot(\inprod{s}{u_c}^2-1)}}^2}{\paren{\expt[s]{p(y \mathrel{\vert} s)}}^2},$$
 where $u_c=\argmax_{\tilde u}\int_{\bbR}\frac{\paren{\expt[s]{p(y \mathrel{\vert} s)\cdot(\inprod{s}{\tilde u}^2-1)}}^2}{\expt[s]{p(y \mathrel{\vert} s)}}dy$ and $c$ is some constant.
Thus, we have that
$$ f(y) = \sqrt{c}\frac{\expt[s]{p(y \mathrel{\vert} s)\cdot(\inprod{s}{u_c}^2-1)}}{\expt[s]{p(y \mathrel{\vert} s)}},$$
which in turn gives the choice
\begin{equation}\label{eq:taustardelta}
    \bar\cT:= \frac{\sqrt{c}  \taustar}{1-(1-\sqrt{c})  \taustar},
\end{equation}
with
$$\taustar(y) = 1 - \frac{\expt[s]{p(y \mathrel{\vert} s)}}{\expt[s]{p(y \mathrel{\vert} s)\cdot\inprod{s}{u_c}^2}}.$$
The second condition in \eqref{eq:condrew} determines $c$. Namely,
\begin{equation}\label{eq:deltadletat}
    \frac{1}{\delta} = \int_{\bbR}\paren{\frac{\bar\cT(y)}{1-\bar\cT(y)}}^2 \expt[s]{p(y \mathrel{\vert} s)}dy = \int_{\bbR} c\cdot\frac{\paren{\expt[s]{p(y \mathrel{\vert} s)\cdot(\inprod{s}{u_c}^2-1)}}^2}{\expt[s]{p(y \mathrel{\vert} s)}} dy = c \frac{1}{\delta_t}.
\end{equation}
Therefore,  $c = \frac{\delta_t}{\delta}$. 
\eqref{eq:deltadletat} immediately proves that the second condition in \eqref{eq:condrew} is satisfied for $\bar\cT$. The first condition in \eqref{eq:condrew} is also satisfied since
\begin{align*}
    \max_{\norm{2}{u}=1}  \int_{\bbR}\frac{\bar\cT}{1-\bar\cT} \expt[s]{p(y \mathrel{\vert} s)\cdot(\inprod{s}{u}^2-1)} dy & \ge \sqrt{c} \cdot \int_{\bbR}\frac{\paren{\expt[s]{p(y \mathrel{\vert} s)\cdot(\inprod{s}{u_c}^2-1)}}^2}{\expt[s]{p(y \mathrel{\vert} s)}}dy\\
    &= \sqrt{\frac{\delta_t}{\delta}} \cdot \frac{1}{\delta_t}>\frac{1}{\delta}.
\end{align*}
Thus, $\bar\cT\in\sT_\delta$ and $\sT_\delta\neq\emptyset$ for $\delta>\delta_t$. It follows that
$$\delta_c\leq\delta_t,$$
which combined with \eqref{eq:ineqdelta} proves $\delta_c=\delta_t$. This also implies that the expression in \Cref{eq:taustardelta} coincides with  $\cT_\delta^*$ as defined in  \Cref{eq:opt}.

Lastly, we need to verify that $\taustardelta$ satisfies \Cref{asmp:T}.
Let us first prove that $\taustardelta(y)$ is bounded. Since $\taustar(y)\leq 1$, it follows that
$$\sqrt{\delta}-(\sqrt{\delta}-\sqrt{\delta_c})\cdot \taustar(y)\geq \sqrt{\delta_c},$$
and
$$\cT_\delta^*(y) = \frac{\sqrt{\delta_c} \cdot \taustar(y)}{\sqrt{\delta}-(\sqrt{\delta}-\sqrt{\delta_c})\cdot \taustar(y)} \leq \frac{\sqrt{\delta_c}\taustar(y)}{\sqrt{\delta_c}}\leq 1.$$
Furthermore, for $\taustar(y)\neq 0$, we have
$$\cT_\delta^*(y) = \frac{\sqrt{\delta_c}}{\frac{\sqrt{\delta}}{\taustar(y)}-(\sqrt{\delta}-\sqrt{\delta_c})},$$
and it holds that $\frac{\sqrt{\delta}}{\taustar(y)}-(\sqrt{\delta}-\sqrt{\delta_c})\in ]-\infty,-(\sqrt{\delta}-\sqrt{\delta_c})[\ \bigcup\ ]\sqrt{\delta_c},+\infty[.$
Thus, for $\taustar(y)\neq 0$, 
$$\cT_\delta^*(y)\geq-\frac{\sqrt{\delta_c}}{\sqrt{\delta}-\sqrt{\delta_c}},$$
whereas $\taustardelta(y)=0$ for $\taustar(y)=0$. This proves that  $\taustardelta(y)$ is bounded.

Finally, by contradiction, suppose that $\prob{\taustardelta(y) = 0 }=1$. Note that $\taustardelta(y)=0$ if and only if $\taustar(y)=0$. This holds whenever  
\begin{equation}\label{eq:contr}
\prob{\expt[s]{p(y \mathrel{\vert} s)} = \expt[s]{p(y \mathrel{\vert} s)\cdot\inprod{s}{u_c}^2}}=1.    
\end{equation}
However, \Cref{eq:contr} implies that $$\prob{\expt[s]{p(y \mathrel{\vert} s)\cdot(\inprod{s}{u_c}^2-1)}=0}=1,$$
giving that $\delta_c=+\infty$, for which the statement of the theorem trivially holds. Consequently, $\prob{\taustar(y)=0}\neq 1$ and the proof is complete. 
\end{proof}


\section{Optimal preprocessing for the numerical experiments}
\label{sec:calc}

\subsection{$q(s) = \prod_{i=1}^2 s_i$}\label{subsec:prod}

In the numerical experiment, we employ the function $\taustar(y)$ in place of $\taustar_\delta(y)$, as the latter is introduced for technical reasons. Recall that %First, we have that 
$$\taustar(y) \coloneqq 1 - \frac{\expt[s]{p(y \mathrel{\vert} s)}}{\expt[s]{p(y \mathrel{\vert} s)\cdot\inprod{s}{u_c}^2}}.$$  
We calculate term by term. First, we have
\begin{align*}
    \expt[s]{p(y \mathrel{\vert} s)} &= \frac{1}{2\pi}\iint_{\bbR^2}e^{-\frac{s_1^2}{2}}e^{-\frac{s_2^2}{2}}\delta(y-s_1s_2)ds_1ds_2\\
    &= \frac{1}{2\pi}2\int_{0}^\infty\int_{0}^\infty e^{-\frac{s_1^2}{2}}e^{-\frac{s_2^2}{2}}\delta(y-s_1s_2)ds_1ds_2,
\end{align*}
where we have assumed that $y>0$ (similar passages hold for $y\leq0$). The constant $2$ pops out, since there are two symmetric cases for $y>0$, namely $s_1,s_2>0$ and $s_1,s_2<0$. Continuing with the change of variable $x_1=s_1s_2$ and $x_2=\frac{s_1}{s_2}$, we have
\begin{align*}
    \frac{1}{2\pi}2\int_{0}^\infty\int_{0}^\infty e^{-\frac{s_1^2}{2}}e^{-\frac{s_2^2}{2}}\delta(y-s_1s_2)ds_1ds_2 &= \frac{1}{\pi}\int_{0}^\infty\int_{0}^\infty e^{-\frac{x_1x_2}{2}}e^{-\frac{x_1/x_2}{2}}\delta(y-x_1)\frac{1}{2x_2}dx_1dx_2\\ 
    &=\frac{1}{\pi}\int_{0}^\infty e^{-y\paren{\frac{x_2}{2}+\frac{1}{2x_2}}}\frac{1}{2x_2}dx_2\\
    &= \frac{1}{2\pi} \int_{-\infty}^\infty e^{-y\cosh(t)}dt\\
    &= \frac{1}{\pi} \int_{0}^\infty e^{-y\cosh(t)}dt\\
    &= \frac{K_0(\abs{y})}{\pi},
\end{align*}
where we did the change of variable $e^t = \frac{1}{x_2}$, and $K_0$ is the modified Bessel function of the second kind. %Notice that this exactly fits the \href{https://mathworld.wolfram.com/NormalProductDistribution.html}{formula} for the PDF of a product of two normal independent gaussians with variance $1$ which $y=s_1s_2$ exactly is. 
Let us now calculate the second term for the choice $u_c=\frac{1}{\sqrt{2}}(1,1)$, which can be verified to be optimal, 
\begin{align*}
    \expt[s]{p(y \mathrel{\vert} s)\inprod{s}{u_c}^2} &= \frac{1}{2\pi}\iint_{\bbR^2}e^{-\frac{s_1^2}{2}}e^{-\frac{s_2^2}{2}}\delta(y-s_1s_2)\inprod{s}{u_c}^2ds_1ds_2\\
    &= \frac{1}{2\pi}2\int_{0}^\infty\int_{0}^\infty e^{-\frac{s_1^2}{2}}e^{-\frac{s_2^2}{2}}\delta(y-s_1s_2)\frac{1}{2}(s_1^2+s_2^2+2s_1s_2)ds_1ds_2,
\end{align*}
where, as before, we have assumed that $y>0$ (similar passages hold for $y\le 0$). Continuing with the change of variable $x_1=s_1s_2$ and $x_2=\frac{s_1}{s_2}$, we have
\begin{align*}
    \expt[s]{p(y \mathrel{\vert} s)\inprod{s}{u_c}^2} &=\frac{1}{\pi}\int_{0}^\infty\int_{0}^\infty e^{-\frac{s_1^2}{2}}e^{-\frac{s_2^2}{2}}\delta(y-s_1s_2)\frac{1}{2}(s_1^2+s_2^2+2s_1s_2)ds_1ds_2\\
    &= \frac{1}{\pi}\int_{0}^\infty\int_{0}^\infty e^{-\frac{x_1x_2}{2}}e^{-\frac{x_1/x_2}{2}}\delta(y-x_1)\frac{1}{2}x_1\paren{x_2+\frac{1}{x_2}+2}\frac{1}{2x_2}dx_1dx_2\\ 
    &=\frac{1}{\pi}\int_{0}^\infty e^{-y\paren{\frac{x_2}{2}+\frac{1}{2x_2}}}\frac{1}{2}y\paren{x_2+\frac{1}{x_2}+2}\frac{1}{2x_2}dx_2\\
    &=\frac{1}{2\pi}\int_{-\infty}^\infty e^{-y\cosh(t)}y(\cosh(t)+1)dx_2\\
    &=\frac{1}{\pi}\int_{0}^\infty e^{-y\cosh(t)}y(\cosh(t)+1)dx_2\\
    &=\frac{yK_0(\abs{y})+\abs{y}K_1(\abs{y})}{\pi},
\end{align*}
where $K_1$ is the modified Bessel function with parameter equal to 1. Moreover, the absolute value in the final result %with absolute value 
is readily obtained %easily 
after analyzing the case $y<0$, which is analogous.
Thus, we get 
\begin{align}
    \taustar(y) = 1 - \frac{K_0(\abs{y})}{(yK_0(\abs{y})+\abs{y}K_1(\abs{y}))}. \label{eqn:T_prod}
\end{align}
Finally, let us calculate the weak recovery threshold $\delta_c$:
\begin{align*}
    \frac{1}{\delta_c}=\int_{\bbR}\frac{\paren{\expt[s]{p(y\mathrel{\vert} s)(\inprod{s}{u}^2- 1)}}^2}{\expt[s]{p(y\mathrel{\vert} s)}} dy = \int_{\bbR} \frac{\paren{\frac{yK_0(\abs{y})+\abs{y}K_1(\abs{y})}{\pi} - \frac{K_0(\abs{y})}{\pi}}^2}{\frac{K_0(\abs{y})}{\pi}}dy\approx 1.68421 \approx (0.5937)^{-1},
\end{align*}
where the exact value was calculated in WolframAlpha. We note that this value exactly matches the threshold in \cite[pg.9]{troiani2024fundamental}. 


\subsection{Mixed phase retrieval} \label{subsec:mp}

Let $\eta=\prob{\eps = 1}$ and define the following auxiliary quantities:
        \begin{align}
            \gamma &= \frac{1}{2} \brack{ 1 + \sqrt{4\rho^2\eta(1-\eta) + (2\eta - 1)^2} } , \notag \\
            a_1 &\coloneqq 1 + \frac{2(\gamma - \eta)}{\eta} + \paren{\frac{\gamma - \eta}{\eta\rho}}^2 , \notag \\
            a_2 &\coloneqq \eta + (1-\eta)\rho^2 + \frac{2(\gamma - \eta)}{\eta} + \paren{\frac{\gamma - \eta}{\eta\rho}}^2 \brack{ (1-\eta) + \eta\rho^2 } , \notag \\
            a_3 &\coloneqq (1-\eta)(1-\rho^2) + \paren{\frac{\gamma - \eta}{\eta\rho}}^2\eta(1-\rho^2) , \notag \\
            b &\coloneqq a_1 - a_3 . \notag 
        \end{align}
        Denote by $ \ell^* $ the unique solution in $ \ell \in \paren{ (\gamma / \eta)^2 - b, \infty } $ to the following equation: 
        \begin{align}
            (\ell - a_3) \int_0^\infty\! \sqrt{\frac{2}{\pi}} e^{-y^2 / 2} \frac{(y^2 - 1)^2}{a_2 y^2 + \ell} \,d y &= \frac{1}{\gamma^2 \delta} . \notag 
        \end{align}
        Then, the optimal preprocessing function is given by 
        \begin{align}
            \cT(y) &= \frac{y^2 - 1}{\brack{ a_2 + \gamma (\ell^* - a_3) } y^2 + \ell^* - \gamma(\ell^* - a_3)} . \label{eqn:T_mpr} 
        \end{align}
This expression maximizes the asymptotic overlap $ \abs{\inprod{v_1^D}{w_1^*}} $ obtained by specializing our general formula in \Cref{thm:opt} to the mixed phase retrieval model. The derivations are along similar lines as detailed in \Cref{subsec:prod} for the model $ y = s_1s_2 $, and we leave out the explicit calculations. 
\end{document}
