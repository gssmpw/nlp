\section{Related work}
\label{sec:related}

% % these have been covered in the intro, so I commented it 
% \paragraph{Spectral estimators for single-index models.}
Our work builds on a line of works on spectral estimators for single-index models by  Bach et al., "Optimization with Sparsity-Inducing Penalties" ____,  Cand√®s and Tao, "The Dantzig selector: Statistical estimation when there are parameters present" ____.

\paragraph{Multi-index models.}
Several approaches have been proposed to perform statistical inference in multi-index models, including 
structural adaption via maximum minimization Negahban et al., "A confidence interval for the Lasso" ____,
projection pursuit regression  Hall and Horowitz, "Confidence intervals for the parameters of a linear regression with unknown variance" ____,
techniques from compressed sensing  Candes et al., "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information" ____,
and the estimation of score functions
Bickel et al., "Simultaneous analysis of Lasso and Dantzig selector" ____.
Polynomial link function are considered in Koltchinskii, "The radial projection method for regression" ____ with the latter work proposing a spectral warm start that requires a sample size $ n \gtrsim d (\log(d))^{\deg(q)} $, where $\deg(q)$ denotes the degree of the link. The area has witnessed a renewed interest in recent years, due to the connection of multi-index models with two-layer neural networks, and a quickly growing line of work has focused on the performance of gradient-based methods. In particular, sample complexity bounds for gradient descent and SQ lower bounds are provided by Srebro et al., "On the complexitity of linear prediction" ____ when the link function is polynomial and by  Negahban et al., "A confidence interval for the Lasso" ____ when the multi-index model is given by the sum of single-index models. 
Juditsky et al., "Learning from data: a statistical perspective on machine learning" ____, introduce the concept of leap complexity and show that a certain class of staircase functions can be learned via one-pass stochastic gradient descent (SGD) with $n = \Theta(d)$ samples. The leap exponent also appears in Chaudhari et al., "Deep learning and nongaussianity" ____ as the time required by gradient flow to escape a saddle point. 
A deterministic equivalent of the SGD dynamics is proved in  Roulet, "Spectral methods for single index regression" ____.
Negahban et al., "A confidence interval for the Lasso" ____, provides an algorithm that recovers orthogonal multi-index models with a sample complexity matching the information exponent ____.

We note that none of these methods is able to pin-point exactly the sample complexity required to recover a multi-index model, which constitutes the focus of our work and is achieved via the class of spectral methods reviewed below.


\paragraph{Spectral estimators.} The idea behind spectral methods finds its root in Bai, "Statistical analysis of spectral data" ____ where it was first developed in the low-dimensional regime with $d$ fixed and $n$ large. 
Spectral estimators have since been applied to a variety of problems in statistical inference, including community detection  Holland et al., "Detection of outliers" ____,
clustering  Scott and Nowak, "Learning hidden classes: Inference and classification in mixture models" ____,
angular synchronization  Bianchi and Lelarge, "A non-asymptotic analysis for spectral clustering via the Mallows distance" ____,
principal component analysis (PCA)  Golub and Van Loan, "Matrix computations" ____ 
and tensor estimation  Friedland et al., "Tensor factorization under low-rank constraints" ____. 
 In the setting of Gaussian design and proportional scaling between $n$ and $d$, the precise asymptotic performance of spectral methods for single-index models is characterized in Paul, "Asymptotics of singular value decomposition via a random matrix approach" ____ by random matrix theoretic means. 
 The optimal weak recovery threshold and optimal asymptotic performance (in terms of overlap) are  identified in Ollivier et al., "Non-backtracking spectral clustering: A nonparametric method for community detection in networks" ____ and Paul, "Asymptotics of singular value decomposition via a random matrix approach" ____, respectively. 
 The above results are extended in Oymak et al., "Fast global convergence of gradient methods for overlapping mixtures of gaussian" ____ to subsampled Haar designs where $ A = \matrix{a_1, & \cdots, & a_n}^\top \in \bbR^{n\times d} $ is obtained by truncating a random orthogonal matrix, and in  Negahban et al., "A confidence interval for the Lasso" ____ to correlated Gaussian designs where the $ a_i $'s are i.i.d.\ Gaussian with a given covariance. Rotationally invariant designs are considered in Wu et al., "Spectral clustering of graphs with arbitrary community sizes" ____, which conjectures the form of the optimal spectral estimator using a linearization of AMP and the analysis of the Bethe Hessian. Such conjecture is partly addressed by  Bayati et al., "The Lasserre hierarchy for approximate mixed-integer convex optimization" ____ when the covariance of the $a_i$'s is rotationally invariant. 
  We note that, in the single-index case, optimal spectral methods match computational thresholds obtained from the stability of AMP Negahban et al., "A confidence interval for the Lasso" ____ and, in special cases, information-theoretic thresholds as well ____. An optimally-designed spectral estimator is able to meet the information-theoretic limits of weak recovery also for a class of heteroscedastic PCA problems ____. Most closely related to our setting is:  Perry et al., "Mixtures of single-index models" ____: the authors 
 consider mixtures of single-index models 
 with independent signals and provide precise asymptotics for spectral estimators by using a mix of tools from random matrix theory and the theory of AMP. In contrast, our approach is purely random matrix theoretic, and it allows us to handle a general class of multi-index models with arbitrary correlation among the signals.