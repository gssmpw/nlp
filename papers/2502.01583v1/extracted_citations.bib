@InProceedings{Chen_Meka,
  title = 	 {Learning Polynomials in Few Relevant Dimensions},
  author =       {Chen, Sitan and Meka, Raghu},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {1161--1227},
  year = 	 {2020},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/chen20a/chen20a.pdf},
  url = 	 {https://proceedings.mlr.press/v125/chen20a.html},
  abstract = 	 { Polynomial regression is a basic primitive in learning and statistics. In its most basic form the goal is to fit a degree $d$ polynomial  to a response variable $y$ in terms of an $n$-dimensional input vector $x$. This is extremely well-studied with many applications and has sample and runtime complexity $\Theta(n^d)$. Can one achieve better runtime if the intrinsic dimension of the data is much smaller than the ambient dimension $n$? Concretely, we are given samples $(x,y)$ where $y$ is a degree at most $d$ polynomial in an unknown $r$-dimensional projection (the relevant dimensions) of $x$. This can be seen both as a generalization of phase retrieval and as a special case of learning multi-index models where the link function is an unknown low-degree polynomial. Note that without distributional assumptions, this is at least as hard as junta learning. In this work we consider the important case where the covariates are Gaussian. We give an algorithm that learns the polynomial within accuracy $\epsilon$ with sample complexity that is roughly $N = O_{r,d}(n \log^2(1/\epsilon) (\log n)^d)$ and runtime $O_{r,d}(N n^2)$. Prior to our work, no such results were known even for the case of $r=1$. We introduce a new \emph{filtered PCA} approach to get a warm start for the true subspace and use \emph{geodesic SGD} to boost to arbitrary accuracy; our techniques may be of independent interest, especially for problems dealing with subspace recovery or analyzing SGD on manifolds.}
}

@article{MIM_GF,
  author       = {Alberto Bietti and
                  Joan Bruna and
                  Loucas Pillaud{-}Vivien},
  title        = {On Learning Gaussian Multi-index Models with Gradient Flow},
  journal      = {CoRR},
  volume       = {abs/2310.19793},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.19793},
  doi          = {10.48550/ARXIV.2310.19793},
  eprinttype    = {arXiv},
  eprint       = {2310.19793},
  timestamp    = {Fri, 03 Nov 2023 11:23:26 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2310-19793.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{MR_tensor_PCA,
 author = {Richard, Emile and Montanari, Andrea},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A statistical model for tensor PCA},
 url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/b5488aeff42889188d03c9895255cecc-Paper.pdf},
 volume = {27},
 year = {2014}
}

@article{MatrixDenoising,
  author       = {Yihan Zhang and
                  Marco Mondelli},
  title        = {Matrix Denoising with Doubly Heteroscedastic Noise: Fundamental Limits
                  and Optimal Spectral Methods},
  journal      = {CoRR},
  volume       = {abs/2405.13912},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.13912},
  doi          = {10.48550/ARXIV.2405.13912},
  eprinttype    = {arXiv},
  eprint       = {2405.13912},
  timestamp    = {Mon, 24 Jun 2024 20:41:43 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2405-13912.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Oko_GAM,
  title = 	 {Learning sum of diverse features: computational hardness and efficient gradient-based training for ridge combinations},
  author =       {Oko, Kazusato and Song, Yujin and Suzuki, Taiji and Wu, Denny},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {4009--4081},
  year = 	 {2024},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Jun--03 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v247/oko24a/oko24a.pdf},
  url = 	 {https://proceedings.mlr.press/v247/oko24a.html},
  abstract = 	 {We study the statistical and computational complexity of learning a target function $f_*:\R^d\to\R$ with \textit{additive structure}, that is, $f_*(x) = \frac{1}{\sqrt{M}}\sum_{m=1}^M f_m(⟨x, v_m⟩)$, where $f_1,f_2,...,f_M:\R\to\R$ are nonlinear link functions of single-index models (ridge functions) with diverse and near-orthogonal index features $\{v_m\}_{m=1}^M$, and the number of additive tasks $M$ grows with the dimensionality $M\asymp d^\gamma$ for $\gamma\ge 0$. This problem setting is motivated by the classical additive model literature, the recent representation learning theory of two-layer neural network, and large-scale pretraining where the model simultaneously acquires a large number of “skills” that are often \textit{localized} in distinct parts of the trained network. We prove that a large subset of polynomial $f_*$ can be efficiently learned by gradient descent training of a two-layer neural network, with a polynomial statistical and computational complexity that depends on the number of tasks $M$ and the \textit{information exponent} of $f_m$, despite the unknown link function and $M$ growing with the dimensionality. We complement this learnability guarantee with computational hardness result by establishing statistical query (SQ) lower bounds for both the correlational SQ and full SQ algorithms.}
}

@article{Ren_Lee,
  author       = {Yunwei Ren and
                  Jason D. Lee},
  title        = {Learning Orthogonal Multi-Index Models: {A} Fine-Grained Information
                  Exponent Analysis},
  journal      = {CoRR},
  volume       = {abs/2410.09678},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2410.09678},
  doi          = {10.48550/ARXIV.2410.09678},
  eprinttype    = {arXiv},
  eprint       = {2410.09678},
  timestamp    = {Fri, 22 Nov 2024 21:38:25 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2410-09678.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Zhang_COLT,
  title = 	 {Spectral Estimators for Structured Generalized Linear Models via Approximate Message Passing (Extended Abstract)},
  author =       {Zhang, Yihan and Ji, Hong Chang and Venkataramanan, Ramji and Mondelli, Marco},
  booktitle = 	 {Proceedings of Thirty Seventh Conference on Learning Theory},
  pages = 	 {5224--5230},
  year = 	 {2024},
  volume = 	 {247},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {30 Jun--03 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v247/zhang24c/zhang24c.pdf},
  url = 	 {https://proceedings.mlr.press/v247/zhang24c.html},
  abstract = 	 {We consider the problem of parameter estimation in a high-dimensional generalized linear model. Spectral methods obtained via the principal eigenvector of a suitable data-dependent matrix provide a simple yet surprisingly effective solution. However, despite their wide use, a rigorous performance characterization, as well as a principled way to preprocess the data, are available only for unstructured (i.i.d. Gaussian and Haar orthogonal) designs. In contrast, real-world data matrices are highly structured and exhibit non-trivial correlations. To address the problem, we consider correlated Gaussian designs capturing the anisotropic nature of the features via a covariance matrix $\Sigma$. Our main result is a precise asymptotic characterization of the performance of spectral estimators. This allows us to identify the optimal preprocessing that minimizes the number of samples needed for parameter estimation. Surprisingly, such preprocessing is universal across a broad set of statistical models, which partly addresses a conjecture on optimal spectral estimators for rotationally invariant designs. Our principled approach vastly improves upon previous heuristic methods, including for designs common in computational imaging and genetics. The proposed methodology, based on approximate message passing, is broadly applicable and opens the way to the precise characterization of spiked matrices and of the corresponding spectral methods in a variety of settings. }
}

@article{abbe2017community,
  author  = {Emmanuel Abbe},
  title   = {Community Detection and Stochastic Block Models: Recent Developments},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {177},
  pages   = {1--86},
  url     = {http://jmlr.org/papers/v18/16-480.html}
}

@InProceedings{abbe2022merged,
  title = 	 {The merged-staircase property: a necessary and nearly sufficient condition for SGD learning of sparse functions on two-layer neural networks},
  author =       {Abbe, Emmanuel and Adsera, Enric Boix and Misiakiewicz, Theodor},
  booktitle = 	 {Proceedings of Thirty Fifth Conference on Learning Theory},
  pages = 	 {4782--4887},
  year = 	 {2022},
  volume = 	 {178},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--05 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v178/abbe22a/abbe22a.pdf},
  url = 	 {https://proceedings.mlr.press/v178/abbe22a.html},
  abstract = 	 {It is currently known how to characterize functions that neural networks can learn with SGD for two extremal parametrizations: neural networks in the linear regime, and neural networks with no structural constraints. However, for the main parametrization of interest —non-linear but regular networks— no tight characterization has yet been achieved, despite significant developments. We take a step in this direction by considering depth-2 neural networks trained by SGD in the mean-field regime. We consider functions on binary inputs that depend on a latent low-dimensional subspace (i.e., small number of coordinates). This regime is of interest since it is poorly understood how neural networks routinely tackle high-dimensional datasets and adapt to latent low-dimensional structure without suffering from the curse of dimensionality. Accordingly, we study SGD-learnability with $O(d)$ sample complexity in a large ambient dimension $d$.  Our main results characterize a hierarchical property —the merged-staircase property— that is both \emph{necessary and nearly sufficient} for learning in this setting.  We further show that non-linear training is necessary: for this class of functions, linear methods on any feature map (e.g., the NTK) are not capable of learning efficiently. The key tools are a new “dimension-free” dynamics approximation result that applies to functions defined on a latent space of low-dimension, a proof of global convergence based on polynomial identity testing, and an improvement of lower bounds against linear methods for non-almost orthogonal functions.}
}

@InProceedings{abbe2023sgd,
  title = 	 {SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics},
  author =       {Abbe, Emmanuel and Adser{\`a}, Enric Boix and Misiakiewicz, Theodor},
  booktitle = 	 {Proceedings of Thirty Sixth Conference on Learning Theory},
  pages = 	 {2552--2623},
  year = 	 {2023},
  volume = 	 {195},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {12--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v195/abbe23a/abbe23a.pdf},
  url = 	 {https://proceedings.mlr.press/v195/abbe23a.html},
  abstract = 	 {   We investigate the time complexity of SGD learning on fully-connected neural networks with isotropic data. We put forward a complexity measure,{\it the leap}, which measures how “hierarchical” target functions are. For $d$-dimensional uniform Boolean or isotropic Gaussian data, our main conjecture states that the time complexity to learn a function $f$ with low-dimensional support is $$\Tilde \Theta (d^{\max(\mathrm{Leap}(f),2)}) \,\,.$$    We prove a version of this conjecture for a class of functions on Gaussian isotropic data and 2-layer neural networks, under additional technical assumptions on how SGD is run. We show that the training  sequentially learns the function support with a saddle-to-saddle dynamic. Our result departs from Abbe et al.’22 by going beyond leap 1 (merged-staircase functions), and by going beyond the mean-field and gradient flow approximations that prohibit the full complexity control obtained here.Finally, we note that this gives an SGD complexity for the full training trajectory that matches that of Correlational Statistical Query (CSQ) lower-bounds. }
}

@article{arous2021online,
  author  = {Ben Arous, Gerard and Gheissari, Reza and Jagannath, Aukosh},
  title   = {Online stochastic gradient descent on non-convex losses from high-dimensional inference},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {106},
  pages   = {1--51},
  url     = {http://jmlr.org/papers/v22/20-1288.html}
}

@article{dalalyan2008new,
  author  = {Arnak S. Dalalyan and Anatoly Juditsky and Vladimir Spokoiny},
  title   = {A New Algorithm for Estimating the Effective Dimension-Reduction Subspace},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {53},
  pages   = {1647--1678},
  url     = {http://jmlr.org/papers/v9/dalalyan08a.html}
}

@InProceedings{damian2022neural,
  title = 	 {Neural Networks can Learn Representations with Gradient Descent},
  author =       {Damian, Alexandru and Lee, Jason and Soltanolkotabi, Mahdi},
  booktitle = 	 {Proceedings of Thirty Fifth Conference on Learning Theory},
  pages = 	 {5413--5452},
  year = 	 {2022},
  volume = 	 {178},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--05 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v178/damian22a/damian22a.pdf},
  url = 	 {https://proceedings.mlr.press/v178/damian22a.html},
  abstract = 	 {Significant theoretical work has established that in specific regimes, neural networks trained by gradient descent behave like kernel methods. However, in practice, it is known that neural networks strongly outperform their associated kernels. In this work, we explain this gap by demonstrating that there is a large class of functions which cannot be efficiently learned by kernel methods but can be easily learned with gradient descent on a two layer neural network outside the kernel regime by learning representations that are relevant to the target task. We also demonstrate that these representations allow for efficient transfer learning, which is impossible in the kernel regime. Specifically, we consider the problem of learning polynomials which depend on only a few relevant directions, i.e. of the form $f^\star(x) = g(Ux)$ where $U: \R^d \to \R^r$ with $d \gg r$. When the degree of $f^\star$ is $p$, it is known that $n \asymp d^p$ samples are necessary to learn $f^\star$ in the kernel regime. Our primary result is that gradient descent learns a representation of the data which depends only on the directions relevant to $f^\star$. This results in an improved sample complexity of $n\asymp d^2$ and enables transfer learning with sample complexity independent of $d$.}
}

@InProceedings{maillard2022construction,
  title = 	 {Construction of optimal spectral methods in phase retrieval},
  author =       {Maillard, Antoine and Krzakala, Florent and Lu, Yue M. and Zdeborova, Lenka},
  booktitle = 	 {Proceedings of the 2nd Mathematical and Scientific Machine Learning Conference},
  pages = 	 {693--720},
  year = 	 {2022},
  volume = 	 {145},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--19 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v145/maillard22a/maillard22a.pdf},
  url = 	 {https://proceedings.mlr.press/v145/maillard22a.html},
  abstract = 	 {We consider the \emph{phase retrieval} problem, in which the observer wishes to recover a $n$-dimensional real or complex signal $\bX^\star$ from the (possibly noisy) observation of $|\bPhi \bX^\star|$, in which $\bPhi$ is a matrix of size $m \times n$. We consider a \emph{high-dimensional} setting where $n,m \to \infty$ with $m/n = \mathcal{O}(1)$, and a large class of (possibly correlated) random matrices $\bPhi$ and observation channels. Spectral methods are a powerful tool to obtain approximate observations of the signal $\bX^\star$ which can be then used as initialization for a subsequent algorithm, at a low computational cost. In this paper, we extend and unify previous results and approaches on spectral methods for the phase retrieval problem. More precisely, we combine the linearization of message-passing algorithms and the analysis of the \emph{Bethe Hessian}, a classical tool of statistical physics. Using this toolbox, we show how to derive optimal spectral methods for arbitrary channel noise and right-unitarily invariant matrix $\bPhi$, in an automated manner (i.e. with no optimization over any hyperparameter or preprocessing function). }
}

@article{mixed-zmv-arxiv,
  author    = {Yihan Zhang and
               Marco Mondelli and
               Ramji Venkataramanan},
  title     = {Precise Asymptotics for Spectral Methods in Mixed Generalized Linear
               Models},
  journal   = {CoRR},
  volume    = {abs/2211.11368},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2211.11368},
  doi       = {10.48550/arXiv.2211.11368},
  eprinttype = {arXiv},
  eprint    = {2211.11368},
  timestamp = {Thu, 24 Nov 2022 15:52:33 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2211-11368.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ng2001spectral,
 author = {Ng, Andrew and Jordan, Michael and Weiss, Yair},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {MIT Press},
 title = {On Spectral Clustering: Analysis and an algorithm},
 url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Paper.pdf},
 volume = {14},
 year = {2001}
}

