\section{Related Work}
Previous methods for event relation extraction~\cite{hieu2022selecting,hwang-etal-2022-event,huang2023classification,DBLP:conf/acl/BarhomSEBRD19,DBLP:conf/acl/HuLJ0GGC23,wang-etal-2022-maven,DBLP:conf/eacl/TanPH23} primarily utilize multi-class classification, MASK prediction, or prototype matching, focusing on addressing specific sub-tasks such as coreference, temporal, causal, or sub-event relations. In the classification-based approach~\cite{huang2023classification,DBLP:conf/naacl/LuN21,DBLP:conf/acl/TranPN20,DBLP:conf/coling/ZengJGGC20,DBLP:conf/emnlp/WangCZR20,DBLP:conf/acl/BarhomSEBRD19}, event mentions are paired together, and additional features are incorporated, such as prototypes, logical rules, graph convolutional networks, or prompts. MASK prediction-based methods~\cite{DBLP:journals/corr/abs-2307-09813,DBLP:conf/coling/ShenZWQ22,DBLP:conf/coling/CuiSCLLS22} train a masked language model to predict the relation. The prototype matching method~\cite{hu2023protoem} manually selects instances to serve as prototypes for each relation, and new instances are then matched against these prototypes. \citet{DBLP:conf/emnlp/SegalESGB20} and \citet{hu-etal-2019-multi} each proposed reading comprehension models based on multi-choice and multi-span, respectively, allowing the model to select the correct answer from candidate options or to generate multiple answers simultaneously. Simultaneously, many entity relation extraction methods based on LLMs~\cite{wang2023instructuie,xiao2024yayiuie} directly prompt large language models to generate relations between pairs of entities. However, these methods have several drawbacks. Therefore, we have designed a series of improvement measures to address these identified deficiencies.