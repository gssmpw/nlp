%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\PassOptionsToPackage{table}{xcolor}
\documentclass[sigconf]{acmart}\let\Bbbk\relax
% \documentclass[sigconf]{acmart}
\usepackage{latexsym}
\usepackage{subfigure}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{stackrel}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{bm}
\usepackage{color}
\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{tcolorbox}
\usepackage{listings}
\usepackage{array}
\usepackage{rotating}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\setcopyright{cc}
\setcctype{by}
\acmConference[WWW Companion '25]{Companion Proceedings of the ACM Web
Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia}
\acmBooktitle{Companion Proceedings of the ACM Web Conference 2025 (WWW
Companion '25), April 28-May 2, 2025, Sydney, NSW, Australia}
\acmDOI{10.1145/3701716.3715457}
\acmISBN{979-8-4007-1331-6/25/04}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{MAQInstruct: Instruction-based Unified Event Relation Extraction}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Jun Xu}
\orcid{0000-0001-9565-6106}
\affiliation{%
  \institution{AntGroup}
  \city{Hangzhou}
  \country{China}}
\email{xujun.xj@antgroup.com}

\author{Mengshu Sun}
\orcid{0000-0003-2639-9462}
\authornote{Corresponding author.}
\affiliation{%
  \institution{AntGroup}
  \city{Hangzhou}
  \country{China}}
\email{mengshu.sms@antgroup.com}

\author{Zhiqiang Zhang}
\orcid{0000-0002-2321-7259}
\affiliation{%
  \institution{AntGroup}
  \city{Hangzhou}
  \country{China}}
\email{lingyao.zzq@antgroup.com}

\author{Jun Zhou}
\orcid{0000-0001-6033-6102}
\affiliation{%
  \institution{AntGroup}
  \city{Hangzhou}
  \country{China}}
\email{jun.zhoujun@antgroup.com}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Xu et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Extracting event relations that deviate from known schemas has proven challenging for previous methods based on multi-class classification, MASK prediction, or prototype matching. Recent advancements in large language models have shown impressive performance through instruction tuning. Nevertheless, in the task of event relation extraction, instruction-based methods face several challenges: there are a vast number of inference samples, and the relations between events are non-sequential. To tackle these challenges, we present an improved instruction-based event relation extraction framework named MAQInstruct. Firstly, we transform the task from extracting event relations using given event-event instructions to selecting events using given event-relation instructions, which reduces the number of samples required for inference. Then, by incorporating a bipartite matching loss, we reduce the dependency of the instruction-based method on the generation sequence. Our experimental results demonstrate that MAQInstruct significantly improves the performance of event relation extraction across multiple LLMs.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317.10003347.10003352</concept_id>
<concept_desc>Information systems~Information extraction</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Information extraction}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Relation Extraction, Information Extraction, Information Retrieval}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
% Event Relation Extraction (ERE) is the task of predicting relations between event mentions in unstructured text. 
Event Relation Extraction (ERE) tasks
are highly diversified due to their varying sub-tasks (coreference, temporal, causal, sub-event, etc.) and complex relations (symmetrical, asymmetrical, cross, etc.)~\cite{DBLP:conf/emnlp/WenJ21,DBLP:conf/acl/WadhwaAW23,hu2023protoem,DBLP:conf/acl/Wang0GZC00LLXZL24}. 
However, most previous studies~\cite{DBLP:conf/emnlp/NguyenMDN22,DBLP:conf/eacl/WangZDGRC23,DBLP:conf/acl/YuanH0023,DBLP:conf/acl/0001MS0Z024,DBLP:conf/coling/XuSZZ24} have primarily focused on optimizing specific sub-tasks, making it difficult to transfer model structures, optimization strategies, specialized knowledge sources, and domain data between different sub-tasks. Although a few of works~\cite{wang-etal-2022-maven,hu2023protoem} use multi-head classification or prototype matching to tackle multiple sub-tasks simultaneously, these methods depend on pre-defined and mutually exclusive event relations.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=8.5cm]{./figs/motivation.pdf}
    \caption{Different event relation extraction methods. %The special, individual, unused character <0x64>-<0xFF> is used to indicate candidate event mentions.
    }
    \label{Fig.motivation}
\end{figure}
Recent large language models, such as ChatGPT and Llama, demonstrate exceptional text understanding and instruction-learning capabilities. The instruction-based approach eliminates the need for predefined relation schemas and mutual exclusivity in event relations, effectively solving previous issues. For a more intuitive comparison, we present the different methods in Figure~\ref{Fig.motivation}. 
The classification-based method utilizes one-hot embedding to represent the event relation labels, which overlook the semantic information of the labels and struggle to effectively extract unseen event relations. The instruction-based method uses pairs of candidate event mentions and a comprehensive list of all event relations for instructions, leveraging the capabilities of large language models to generate these relations. Although the instruction-based method can address the issues present in classification-based methods, it also has two significant drawbacks. 
First, it requires a large amount of training and inference samples, reaching $n \times n$, where $n$  represents the number of event mentions (dozens or hundreds within a single sample). 
Second, the model is greatly influenced by the sequence in which relations are generated. Using the instruction-based method shown in Figure~\ref{Fig.motivation} as an example, the model generates $p(parent|child)$ and $p(child|parent)$ with varying probabilities. However, in the ERE task, the sequence of generation should not affect the event relation between event mentions.

To address the two issues present in the instruction-based method, we design two strategies: multiple-answer question answering and bipartite matching. First, as shown in Figure~\ref{Fig.motivation}, we transform the task from extracting event relations using given event-event instructions to selecting events using given event-relation instructions. 
\begin{figure}[htbp]
\centering 
\includegraphics[width=8.5cm]{./figs/loss_function.pdf} 
\caption{Cross-entropy loss vs bipartite matching loss.} 
\label{Fig.loss_function} 
\end{figure}
Since the event relation types 
$k\ll n$ (where $k$ has a few kinds, and $n$ is in the dozens or hundreds), we reduce the training and inference samples from $n\times n$ to $k \times n$. Second, based on the instruction-based framework, we introduce a bipartite matching loss. As demonstrated in Figure~\ref{Fig.loss_function}, using cross-entropy loss results in two mistakes, while the bipartite matching loss yields one correct answer and one mistake.
In summary, the main contributions of this paper are:

1) We propose an improved unified event relation extraction framework (MAQInstruct) based on multiple answer questions. Compared with InstructERE, our method reduces the training and inference samples from $n\times n$ to $k\times n$.

2) In the MAQInstruct framework, we incorporate a bipartite matching loss to reduce the dependency of InstructERE on the generation sequence, making it more suitable for event relation extraction tasks.

\section{Methodology}
\subsection{Sample Construction}
\noindent{\textbf{Instruction:}}
To unify the various inputs for different ERE sub-tasks, we have developed a set of instructions. Each instruction specifies an event relation and a candidate event mention, indicated by the special character <0x64>-<0xFF> in LLMs. For instance, the instruction ``List the coreference event of <0x85> ruled ?'' extracts coreferential events, with ``coreference'' as the relation and ``<0x85> ruled'' as the mention.

\noindent{\textbf{Context:}}
In the ERE task, we insert sequential markers (<0x64>-<0xFF>) into the text for each candidate event mention. The first mention receives <0x64>, the second <0x65>, and so on. These markers direct the language model to focus solely on the specified content. These markers enable LLMs to select the correct answer from these candidate events.

\noindent{\textbf{Label:}}
The output consists of two parts: the dependency parsing chain and multiple answers, separated by a colon. %The dependency parsing chain follows Appendix~\ref{sec.parse_cot_construction}. 
Multiple answers are presented in the order they appear in the text, separated by commas; if there are no associated event mentions, this part will be set to none.

\subsection{Multiple Answer Questions Loss}
The generated sequence greatly influences the text generation effectiveness~\citep{ye-etal-2021-one2set,OTSeq2SetCaoZ22}, but in ERE, the order of answer generation should not affect the outcome. To minimize the impact of the generation sequence, we calculate distinct losses for the dependency parsing chain and multiple answers, defined as follows:
\begin{equation}
\begin{aligned}
\mathcal{L}_{CE} = \frac{1}{N}\sum_{i=0}^{N}CE(y_i,p(y_i|x))
\end{aligned}
\label{sft_loss}
\end{equation}
where $N=N_1+N_2$, $N_1$ represents the length of dependency parsing chain and $N_2$ represents the length of multiple answers. CE is the cross-entropy loss. 
As illustrated in Figure~\ref{Fig.loss_function}, the sequence of generation does not impact the multiple answers. The loss for multiple answers is calculated as follows:

(a) First, use the Hungarian Algorithm to find the optimal match.
\begin{equation}
\begin{aligned}
\hat \theta = \mathop{\arg\min}\limits_{\theta \in \Psi_{N_2}} \sum_{i=0}^{N_2} 1-\log \hat p_{ \theta(i)}(c_i)
\end{aligned}
\label{ce_loss}
\end{equation}

(b) After optimal allocation, the loss function for Multiple Answers is:
\begin{equation}
\begin{aligned}
\mathcal{L}_{BPM}= \sum_{i=0}^{N_2} 1-\log \hat p_{\hat \theta(i)}(c_i)
\end{aligned}
\label{bpm_loss}
\end{equation}

(c) Finally, the total loss is as follows:
\begin{equation}
\begin{aligned}
\mathcal{L}= \mathcal{L}_{CE} + \lambda \mathcal{L}_{BPM}
\end{aligned}
\label{final_loss}
\end{equation}
where $\Psi_{N_2}$ denotes a permutation of $N_2$. $\theta$ is one of the permutations. $\theta(i)$ represents the i-th element in permutation $\theta$. $c_i$ represents the target vocabulary id of the i-th element. The probability of the i-th element in the permutation $\theta$ belonging to the target vocabulary id is denoted by $\hat p_{\theta(i)}(c_i)$. $\hat \theta$ stands for the optimal permutation. The weight parameter is represented by $\lambda$.

\renewcommand\arraystretch{1.0}
\begin{table*}[htbp]
\centering
\small
\caption{Performance of different models on the unified event relation extraction dataset MAVEN-ERE. Llama2 and GPT-4 indicate the results without training.}
\setlength\aboverulesep{0pt}\setlength\belowrulesep{0pt}
\begin{tabular}{p{2.9cm}|p{0.5cm}<{\centering}p{0.8cm}<{\centering}p{0.7cm}<{\centering}p{1.0cm}<{\centering}|p{0.75cm}<{\centering}p{0.75cm}<{\centering}p{0.75cm}<{\centering}|p{0.75cm}<{\centering}p{0.75cm}<{\centering}p{0.75cm}<{\centering}|p{0.75cm}<{\centering}p{0.75cm}<{\centering}p{0.75cm}<{\centering}}
\toprule
\multirow{2}{*}{Models} & \multicolumn{4}{c|}{COREFERENCE} & \multicolumn{3}{c|}{TEMPORAL} & \multicolumn{3}{c|}{CAUSAL} & \multicolumn{3}{c}{SUBEVENT} \\ \cline{2-14}
            & B$^3$ & CEAF$_e$ & MUC  & BLANC & P & R & F1   & P & R & F1   & P & R & F1   \\ \bottomrule
Llama2 & 43.5 & 35.7 & 22.1 & 33.5  & 12.6 & 11.1 & 11.8 & 7.5 & 10.9 & 8.9 & 8.3  & 5.4 & 6.5 \\ 
GPT-4 & 65.2 & 63.4 & 48.5 & 52.6 & 25.2 & 26.5 & 25.9 & 15.7 & 17.3 & 16.5 & 15.0 & 12.4 & 13.6 \\ 
BertERE & {97.8} & {97.6} & {79.8} & {88.3} & {50.9} & {53.4} & {52.1} & {31.3} & {30.5} & {30.9} & {24.6} & {22.9} & {23.7} \\ \bottomrule
\rowcolor{blue!10} InstructERE$_{\rm ChatGLM3}$  & 92.3 & 94.2 & 74.9 & 82.6 & 46.8 & 49.7 & 48.3 & 26.1 & 27.4 &26.7 & 19.2 & 20.1 & 19.6 \\ 
\rowcolor{red!10} InstructERE$_{\rm Qwen}$ & 93.5 & 94.6 & 74.1 & 85.2  & 48.0  & 51.6 & 49.3 & 27.8 & 28.1 & 27.9 & 20.3 & 21.3 & 20.8 \\ 
\rowcolor{green!10} InstructERE$_{\rm Llama2}$          & 94.2  & 93.5     & 73.3 & 84.7  & 48.5 & 51.0 & 49.7 & 28.6 & 28.0 & 28.3 & 20.9 & 21.7 & 21.3 \\ \bottomrule
\rowcolor{blue!10} MAQInstruct$_{\rm ChatGLM3}$ &  96.5 & 96.6 & 79.7 & 86.3 & 51.7 & 53.5 & 52.6 & 31.9 & 30.1 & 31.0 & 24.1 & 23.8 & 24.0 \\
\rowcolor{red!10} MAQInstruct$_{\rm Qwen}$ & 97.8 & 97.2 & 80.1 & 88.6 & 53.1 & 54.9 & \textbf{54.0} & 33.1 & 31.2 & 32.1 & 25.4  & 24.2 & 24.8 \\
\rowcolor{green!10} MAQInstruct$_{\rm Llama2}$ & \textbf{98.1} & \textbf{97.9} & \textbf{80.2} & \textbf{88.9} & {53.3} & {54.3} & 53.8 & {33.4} & {31.6} & \textbf{32.5} & {25.8} & {24.6} & \textbf{25.2} \\\bottomrule
\end{tabular}
\label{Tab.joint_result}
\end{table*}

\section{Experimental Results}
\subsection{Datasets \& Comparison Methods}
\noindent{\textbf{Dataset.}} Our experiments are conducted on four datasets, including MAVEN-ERE~\citep{wang-etal-2022-maven} for unified event relation extraction, HiEve~\citep{DBLP:conf/lrec/GlavasSMK14} for sub-event relation extraction, MATRES~\citep{DBLP:conf/acl/RothWN18} for temporal relation extraction, and MECI~\citep{DBLP:conf/coling/LaiVNDN22} for causal relation extraction.  %For a fair comparison, we divided the data into the same training, validation, and test sets as in previous studies~\citep{wang-etal-2022-maven,hieu2022selecting,DBLP:conf/coling/ZhouDTWD22,DBLP:conf/coling/LaiVNDN22}. 

\noindent{\textbf{Comparison Methods.}}
\textbf{BertERE} encodes the entire document with RoBERTa, adding a classification head for contextualized representations of various event pairs, and fine-tunes the model for relation classification. \textbf{InstructERE} uses candidate event pairs and relations as instructions, utilizing a large language model to generate the correct event relations. %The results reported in the experiment are the averages of 5 different random seeds.%For more implementation details, dataset, and evaluation metrics, see Appendix~\ref{appendix.experimental_settings}.
\subsection{Overall Results}
The overall experimental results are summarized in Table~\ref{Tab.joint_result}. The performance of LLMs, specifically Llama2 and GPT-4, exhibits relative inadequacy in the ERE task when these models are not trained with instructions. This performance deficiency is attributed to the intricate definitions of event relations, which complicate comprehension for LLMs. Within a unified dataset MAVEN-ERE, BertERE demonstrates commendable performance, as there is no overlap between distinct event relations. While InstructERE leverages the excellent understanding capabilities of LLMs, it still does not surpass BertERE across various LLMs, indicating that InstructERE possesses inherent limitations in supervised event relation extraction tasks. Analyses indicate that InstructERE requires the construction of $n^2$ samples, which often contain considerable overlapping content, thereby hindering the learning process for LLMs. The model MAQInstruct, built on multiple-answer questions and bipartite matching, surpasses InstructERE when evaluated on three LLMs: ChatGLM3 (ChatGLM3-6b), Qwen (Qwen-7B-Chat), and Llama2 (Llama2-7B-Chat). Notably, the MAQInstruct model trained on Llama2 exhibits improvements of 4.9\%, 4.1\%, 4.2\%, and 3.9\% in event coreference, temporal, causal, and sub-event relations compared to InstructERE, respectively. Additionally, compared to BertERE, MAQInstruct shows enhancements of 0.4\%, 1.7\%, 1.6\%, and 1.5\%, respectively.

\subsection{Inference Performance Analysis}
The construction forms of samples for the three methods: BertERE, InstructERE, and MAQInstruct are shown in Figure~\ref{Fig.motivation}. As can be seen from Table~\ref{Tab.inference_performance_analysis}, the number of coreference event samples constructed by BertERE and InstructERE is significantly larger than that of MAQInstruct. Using an A100-80G GPU for inference with the same base model, Llama2, while keeping the batch size and sequence length, the inference time of InstructERE is 32.5 times that of MAQInstruct. Compared to BertERE, the inference time of MAQInstruct is still greater, since the parameter count of Llama2 is 70 times that of Bert.
\renewcommand\arraystretch{1.1}
\begin{table}[htbp]
\centering
\small
\caption{The inference samples and cost associated with the event coreference relation dataset in MAVEN-ERE.}
\setlength\aboverulesep{0pt}\setlength\belowrulesep{0pt}
\begin{tabular}{p{1.8cm}|p{1.2cm}<{\centering}|p{1.2cm}<{\centering}|p{1.2cm}<{\centering}|p{1.2cm}<{\centering}}
\toprule
            & \#doc & \#mention & \#query & cost(min) \\ \hline
BertERE     & 710   & 17,780    & 631,486 & 16        \\ \hline
InstructERE & 710   & 17,780    & 631,486 & 813       \\ \hline
MAQInstruct & 710   & 17,780    & 17,780  & 25       \\ \bottomrule
\end{tabular}
\label{Tab.inference_performance_analysis}
\end{table}

\subsection{Bipartite Matching Loss Analysis}
The performance of a generative model is greatly affected by the generation sequence, as shown in Figure~\ref{Fig.bpm_analysis}. "Random" indicates that the answers are in a random sequence, "Sequence" represents the sequence in which they appear in the text, "Reverse" indicates the reverse sequence of their appearance, "Distance" means the answers are sorted by distance from the query mention, and "Dict" sorts them from A to Z.
When the bipartite matching loss is not considered, random answer sequences perform the worst, with a reduction of 4.00\% and 3.92\% compared to ordered sequences in MATRES and MECI, respectively. However, after incorporating the bipartite matching loss, MAQInstruct is capable of effectively generating the correct results with any answer sequence used.% Therefore, this evidence indicates that the bipartite matching loss is especially suitable for tasks where the generated sequence is not crucial.
\begin{figure}[htbp]
\centering 
\includegraphics[width=8.3cm]{./figs/order_analysis.pdf} 
\caption{The performance of different answer sequences.} 
\label{Fig.bpm_analysis} 
\end{figure}

\subsection{Zero-Shot Learning Analysis}
To validate the zero-shot learning capability of the model, experiments are conducted on three event relation extraction datasets: HiEve, MATRES, and MECI. 
The experimental results are presented in Figure~\ref{Fig.zeroshot_ere_analysis}. 
\begin{figure}[htbp]
\centering 
\includegraphics[width=5cm]{./figs/zeroshot_ere_analysis.pdf} 
\caption{The F1 score of different instruction-based LLMs on the zero-shot event relation extraction task.} 
\label{Fig.zeroshot_ere_analysis} 
\end{figure}
For instance, when comparing InstructERE and MAQInstruct, both trained with Llama2, it is observed that they significantly outperform the untrained Llama2 in event relation extraction tasks. 
The primary reason for this enhancement lies in the fact that LLMs trained with event relation extraction instructions possess a more comprehensive understanding of the definitions of event relations. 
Furthermore, the performance of MAQInstruct surpasses that of InstructERE, indicating that instruction based on multiple-answer questions is more effective for event relation extraction.
Additionally, to evaluate the effectiveness of data constructed from multiple-answer questions, the performance of MAQInstruct in general natural language understanding tasks is also assessed. As illustrated in Figure~\ref{Fig.text_understanding_analysis}, the average performance of MAQInstruct slightly exceeds that of InstructERE and Llama2, demonstrating that neither the method of data construction from multiple answer questions nor the bipartite matching training approach adversely impacts the language understanding performance of LLMs.
\begin{figure}[htbp]
\centering 
\includegraphics[width=8.3cm]{./figs/text_understanding_analysis.pdf} 
\caption{The performance of different LLMs on natural language understanding evaluation datasets.} 
\label{Fig.text_understanding_analysis} 
\end{figure}


\subsection{Model Ablation Studies}
Due to the substantial volume of MAVEN-ERE data, conducting ablation experiments proves to be excessively costly; consequently, we opt to perform ablation on MATRES and MECI. The experimental results are presented in Table~\ref{Tab.ablation}. 
\renewcommand\arraystretch{1.1}
\begin{table}[htbp]
\centering
\small
\caption{Model ablation studies. Marker refers to the identifier that precedes a  event mention, e.g., ``<0x8F>''. DPC means dependency parsing chain.}
\setlength\aboverulesep{0pt}\setlength\belowrulesep{0pt}
\begin{tabular}{p{1.7cm}|p{0.7cm}<{\centering} p{0.7cm}<{\centering} p{0.7cm}<{\centering}|p{0.7cm}<{\centering} p{0.7cm}<{\centering} p{0.7cm}<{\centering}}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c|}{MATRES} & \multicolumn{3}{c}{MECI} \\ \cline{2-7}
                       & P       & R       & F1     & P      & R      & F1     \\ \hline
MAQInstruct            & 85.5    & 83.9    & 84.7   & 62.9   & 61.6   & 62.3   \\ \hline
w/o Marker             & 81.2    & 84.1    & 82.6   & 58.9   & 59.3   & 59.1   \\ \hline
only Marker            & 84.6    & 83.8    & 84.2   & 62.2   & 61.8   & 62.0   \\ \hline
% w/o Expansion          & 82.5    & 83.1    & 82.8   & 61.7   & 58.8   & 60.2   \\ \hline
% w/o Sampling           & 83.5    & 83.3    & 83.4   & 60.2   & 62.6   & 61.4   \\ \hline
w/o DPC                & 82.3    & 80.5    & 81.4   & 57.5   & 59.3   & 58.4   \\ \hline
w/o $\mathcal{L}_{BPM}$& 81.4    & 83.6    & 82.5   & 61.1   & 61.5   & 61.3   \\ 
\bottomrule
\end{tabular}
\label{Tab.ablation}
\end{table}
Initially, in the absence of the marker (<0x64>-<0xFF>), we observe performance declines of 2.48\% on MATRES and 5.14\% on MECI, which substantiates the efficacy of the prefix marker. In scenarios where multiple answers consist solely of markers, such as ``<0x84>, <0x85>'' instead of ``<0x84> injured, <0x85> increase'', this results in a marginal decrease in effectiveness, suggesting that these markers may lack complete semantic information. 
Moreover, the elimination of the dependency parsing chain results in the most significant performance decline. This phenomenon can be attributed to the fact that the dependency parsing chain enhances the model's capability to extract scattered event relations by utilizing structured information. The removal of the bipartite matching loss function causes a significant drop in model effectiveness, indicating that the bipartite matching loss is particularly suitable for scenarios in which the sequence of generated results is not predetermined.

\section{Conclusion}
In this study, we present a unified framework called MAQInstruct, which aims to improve instruction-based methods through multiple-answer questions, effectively extracting various event relations via different types of instructions. By utilizing the instruction-based method, MAQInstruct significantly enhances this model’s performance by introducing strategies such as multiple-answer questions and bipartite matching loss. %Our extensive ablation studies demonstrate that these strategies effectively address the issues present in the instruction-based methods. Beyond event relation extraction, our method also improves the performance of instruction-based LLMs on natural language understanding tasks.
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\appendix

\section{Dependency Parsing Chain}
\label{sec.parse_cot_construction}
We use the Stanford NLP toolkit's CoreNLP Dependency Parser to create a dependency parse tree from the context, generating various dependency edges. Their meanings are detailed in the toolkit's official documentation. In ERE tasks, we focus solely on the edges between event mentions and retain only the essential nodes and edges needed to connect them. If there is a tie in the number of nodes and edges, we keep them in the order they appear. As shown in Figure~\ref{Fig.parse_cot_construction}, both <$r_1$, $r_2$, $r_4$> and <$r_3$, $r_2$, $r_4$>. It is crucial to mention that since the dependency parser functions at the sentence level, we substitute "." with ";" to ensure the generation of the required dependency parsing chain.
 
\begin{figure}[htbp]
\centering 
\includegraphics[width=4.5cm]{./figs/parse_cot_construction.pdf} 
\caption{A, B, D represent event mentions, while C denotes other words. $r_1$, $r_2$, $r_3$, $r_4$ represent different dependency relations.} 
\label{Fig.parse_cot_construction} 
\end{figure}

\section{Experimental Settings}
\label{appendix.experimental_settings}
\noindent{\textbf{Dataset.}} Our experiments are conducted on four widely-used datasets (cf. Table~\ref{Tab.dataset}), including MAVEN-ERE~\citep{wang-etal-2022-maven} for unified event relation extraction, HiEve~\citep{DBLP:conf/lrec/GlavasSMK14} for sub-event relation extraction, MATRES~\citep{DBLP:conf/acl/RothWN18} for temporal relation extraction, and MECI~\citep{DBLP:conf/coling/LaiVNDN22} for causal relation extraction. 
For a fair comparison, we divided the data into the same training, validation, and test sets as in previous studies~\citep{wang-etal-2022-maven,hieu2022selecting,DBLP:conf/coling/ZhouDTWD22,DBLP:conf/coling/LaiVNDN22}. In particular, since the training and test sets are not separated, consistent with previous works, HiEve selects 80 documents for training (with a 0.4 probability for down-sampling negative examples) and 20 documents for testing. Since MAVEN-ERE does not have an open test set, we chose to use the validation set for testing.

\noindent{\textbf{Evaluation Metric.}} Based on previous research on event relation extraction~\citep{DBLP:conf/emnlp/ChoubeyH17a,DBLP:conf/emnlp/NguyenMDN22,DBLP:conf/eacl/WangZDGRC23,DBLP:conf/acl/YuanH0023,DBLP:conf/acl/CaselliV17,xu-etal-2022-extracting,DBLP:conf/naacl/NguyenMDN22}, we adopt the MUC~\citep{Vilain1995AMC}, B$^3$~\citep{bagga1998algorithms}, CEAF$_e$~\citep{Xiaoqiang1220579Luo} and BLANC~\citep{recasens_hovy_2011} metrics for evaluating event coreference relations. For the other three subtasks, we adopt the standard micro-averaged precision, recall, and F1 metrics. In particular, in the sub-event relation extraction task, PC and CP represent the F1 scores for parent-child and child-parent relations, respectively. 

\noindent{\textbf{Implementation Details.}}
MAQInstruct is conducted on a 4$\times$A100-80G setup. The input sequence length is 1536, and the output sequence length is 512. The weight for the bipartite matching loss, denoted as $\lambda$,  is set to 0.2. We use a learning rate of 5e-4, a batch size of 16, and a gradient accumulation of 2. The learning rate scheduler follows a cosine function, and the model is trained for 20 epochs. The results reported in the experiment are the averages of 5 different random seeds (0, 1, 2, 3, 4). We train the model using an Adam optimizer with weight decay, with the weight decay rate set to 1e-4. The warm-up proportion for the learning rate is 0.1, and the dropout rate is also 0.1. The temperature used to adjust the probabilities of the next token is set to 0.01, while the smallest set of the most probable tokens, with top\_p probabilities, adds up to 0.9. In the output, we use ":" as a delimiter to distinguish the dependency parsing chain from the multiple answers. BertERE employs RoBERTa as its backbone, utilizing a learning rate of 2e-5 for the transformer and 5e-4 for the classification MLP. It selects the longest text containing the event pair, constrained to 512 tokens. InstructERE approaches event relation extraction as a text generation task, employing the same backbone, pre-trained models, and training parameters as MAQInstruct.
\begin{table}[htbp]
\centering
\small
\caption{Dataset Statistics. "\#" denotes the amount. "Mentions" represents the potential events. "Links" means the event relations.}
\setlength\aboverulesep{0pt}\setlength\belowrulesep{0pt}
\begin{tabular}{lccc}
\toprule
Datasets & \#Docs   & \#Mentions & \#Links   \\ \hline
MAVEN-ERE    & 4,480 & 112,276 & 103,193 \\ \hline
HiEve    & 100    & 3, 185   & 3,648   \\ \hline
MATRES   & 275    & 11,861  & 13,573  \\ \hline
MECI     & 438    & 8,732    & 2,050    \\ \bottomrule
\end{tabular}
\label{Tab.dataset}
\end{table}

\section{Different Instructions Analysis}
\label{appendix.different_instruction}
The event relation extraction model based on LLMs is significantly influenced by the provided instructions. Experiments are conducted to assess various sets of instructions, revealing that for fixed tasks, shorter and more concise instructions are typically more effective. Additionally, multiple tests are performed, as outlined in Table~\ref{Tab.different_instruction}. Firstly, including all potential event mentions in the instructions results in a slight decline in the F1 score. Secondly, when the model is enabled to directly generate event relations based on event mentions, its performance significantly deteriorates due to the high volume of event mention pairs that produce relations labeled as NoRel. Furthermore, the model's performance reaches its lowest when multiple distinct relations are generated simultaneously.
\begin{table}[htbp]
\centering
\small
\caption{The F1 score of MAQInstruct on MECI varies among different instructions.}
\setlength\aboverulesep{0pt}\setlength\belowrulesep{0pt}
\begin{tabular}{m{7.0cm}|m{0.7cm}<\centering}
\toprule
Instruction                                              & MECI                  \\ \hline
List the \textbf{\textit{cause}} event of   <0x85> \textcolor{black}{\textbf{\textit{earthquake}}} ?            & 62.3 \\ \hline
Find the \textbf{\textit{cause}} event of <0x85> \textcolor{black}{\textbf{\textit{earthquake}}} from the event mentions <0x71> \textbf{\textit{scorched}}, …? & 61.7 \\ \hline
What's the event relation between <0x85> \textcolor{black}{\textbf{\textit{earthquake}}} and <0x71> \textbf{\textit{scorched}}, <0x72> \textbf{\textit{deny}}, …?      & 60.4 \\ \hline
List the \textbf{\textit{cause}} and \textbf{\textit{effect}} event of  <0x85> \textcolor{black}{\textbf{\textit{earthquake}}} ? & 56.6 \\ \bottomrule
\end{tabular}
\label{Tab.different_instruction}
\end{table}

\section{Different Markers Analysis}
\label{appendix.different_marker}
In this study, we employ various markers to elicit event mentions, building upon prior research~\cite{lu-etal-2022-unified,ye-etal-2022-packed}. We categorize the experiments into three distinct groups, as illustrated in Table~\ref{Tab.different_marker}. The first group utilizes special tokens from Llama2, such as <0x**>, which yield optimal results. Notably, the addition of a special end character following event mentions does not enhance performance due to insufficient semantic information and the introduction of multiple tokens that compromise coherence. The second group substitutes <0x**> with <No**>, resulting in a significant reduction in effectiveness, as the presence of excessive tokens again leads to incoherence. In the third group, the application of a uniform marker for all event mentions produces markedly poorer outcomes.
\begin{table}[htbp]
\centering
\small
\caption{The F1 score of MAQInstruct on MECI varies among different markers.}
\setlength\aboverulesep{0pt}\setlength\belowrulesep{0pt}
\begin{tabular}{m{1.25cm}|m{4.4cm}|m{0.65cm}<\centering}
\toprule
Marker             & Tokenizer                                       & MECI \\ \hline
<0x64>             & [103]                                         & 62.3 \\ \hline
<0x64> </>         & \leftline{[103]}[1533, 29958]              & 62.1 \\ \hline
<No64>             & [529, 3782, 29953, 29946, 29958]              & 59.5 \\ \hline
<No64> </>         & \leftline{[529, 3782, 29953, 29946, 29958]}[1533, 29958] & 59.3 \\ \hline
<Strong>           & [529, 1110, 29958]                            & 60.2 \\ \hline
<Strong> </> & \leftline{[529, 1110, 29958]} [1533, 29958]   & 59.7 \\ \bottomrule
\end{tabular}
\label{Tab.different_marker}
\end{table}

\section{Case Study}
This study presents a qualitative analysis of the extraction of multiple answers, utilizing two examples of event temporal relation extraction, as illustrated in Figure~\ref{Fig.case_study}. The first example effectively demonstrates the extraction process, supported by a valuable dependency parsing chain. 
Conversely, the erroneous example reveals two issues: the failure to recall event <0x6A> and the incorrect recall of event <0x71>. These inaccuracies arise from the complexity of the dependency parsing chain, which obscures pertinent structural information while introducing extraneous details, resulting in errors.
\begin{figure}[h]
    \centering 
    \includegraphics[width=7.7cm]{./figs/case_study.pdf} 
    \caption{Two examples demonstrating the use of MAQInstruct in extracting temporal relations.} 
    \label{Fig.case_study}
\end{figure}

\section{Related Work}
Previous methods for event relation extraction~\cite{hieu2022selecting,hwang-etal-2022-event,huang2023classification,DBLP:conf/acl/BarhomSEBRD19,DBLP:conf/acl/HuLJ0GGC23,wang-etal-2022-maven,DBLP:conf/eacl/TanPH23} primarily utilize multi-class classification, MASK prediction, or prototype matching, focusing on addressing specific sub-tasks such as coreference, temporal, causal, or sub-event relations. In the classification-based approach~\cite{huang2023classification,DBLP:conf/naacl/LuN21,DBLP:conf/acl/TranPN20,DBLP:conf/coling/ZengJGGC20,DBLP:conf/emnlp/WangCZR20,DBLP:conf/acl/BarhomSEBRD19}, event mentions are paired together, and additional features are incorporated, such as prototypes, logical rules, graph convolutional networks, or prompts. MASK prediction-based methods~\cite{DBLP:journals/corr/abs-2307-09813,DBLP:conf/coling/ShenZWQ22,DBLP:conf/coling/CuiSCLLS22} train a masked language model to predict the relation. The prototype matching method~\cite{hu2023protoem} manually selects instances to serve as prototypes for each relation, and new instances are then matched against these prototypes. \citet{DBLP:conf/emnlp/SegalESGB20} and \citet{hu-etal-2019-multi} each proposed reading comprehension models based on multi-choice and multi-span, respectively, allowing the model to select the correct answer from candidate options or to generate multiple answers simultaneously. Simultaneously, many entity relation extraction methods based on LLMs~\cite{wang2023instructuie,xiao2024yayiuie} directly prompt large language models to generate relations between pairs of entities. However, these methods have several drawbacks. Therefore, we have designed a series of improvement measures to address these identified deficiencies.

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
