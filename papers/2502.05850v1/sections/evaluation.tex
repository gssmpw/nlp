

%\newpage




\section{Evaluation}\label{sec:evaluation}
In this section, we demonstrate how optimization strategies can be built by revising design-flow architectures, combining and reusing pipe tasks, and modifying their configuration (Sections~\ref{sec:single_opt} and~\ref{sec:multi_opt}). We explain flow control in Sections~\ref{sec:bottom_up} and~\ref{sec:parallel_flow}. We discuss optimisation search strategies and compare evaluation results with other approaches in Sections~\ref{sec:opt_search} and~\secref{sec:discussion}, respectively.

%In this section, we illustrate how different optimization strategies can be realized by revising design-flow architectures, for instance, combining and reusing existing pipe tasks, as well as modifying their configuration  (Sections~\ref{sec:single_opt} and ~\ref{sec:multi_opt}). We then explain how we control the flows in Sections~\ref{sec:bottom_up} and ~\ref{sec:parallel_flow}. This is followed by \secref{sec:all_three} which presents a software-hardware co-optimization strategy that combines scaling, pruning and quantization. We conclude this section with a discussion in~\secref{sec:discussion}. 

\subsection{Experimental Setup}

Experiments were conducted in Python 3.9.15 with benchmark workloads from typical DNN applications, as presented in Table~\ref{table:benchmark}), including jet identification~\cite{duarte2018fast, moreno2020jedi}, image classification using VGG\cite{simonyan2014very} and ResNet~\cite{he2016deep} networks. 
%\textcolor{red}{todo: add LSTM and also the github}
The jet identification task targeted FPGA-based CERN Large Hadron Collider (LHC) triggers with a 40 MHz input rate and a response latency of less than 1 microsecond. Default frequencies were 100MHz for Zynq 7020 and 200MHz for Alveo U250 and VU9P, and the HLS4ML task used 18-bit fixed-point precision with 8 integer bits.

% The experiments were conducted using Python version 3.9.15 and benchmark workloads were selected from typical DNN applications, as presented in Table~\ref{table:benchmark}. These applications include jet identification using MLP networks~\cite{duarte2018fast, moreno2020jedi}, as well as image classification using VGG~\cite{simonyan2014very} and ResNet~\cite{he2016deep} networks. The datasets used in the experiments consist of MNIST~\cite{lecun1998gradient}, SVHN~\cite{netzer2011reading}, and jet HLFs~\cite{duarte2018fast, moreno2020jedi}. The jet identification task was designed for FPGA-based CERN Large Hadron Collider (LHC) triggers and was able to handle a 40 MHz input rate with a response latency of less than 1 microsecond. The default frequencies used in the experiments were 100MHz for Zynq 7020 and 200MHz for Alveo U250 and VU9P. The HLS4ML task employed 18-bit fixed-point precision with 8 integer bits.





%\subsection{$O$-task: Single Optimization task}\label{sec:single_opt}

\subsection{Optimization Strategy using Single $O$-task}\label{sec:single_opt}


In this subsection, we focus on three strategies which are backed by a single $O$-task each, respectively. %We describe the effects of each $O$-task separately, while in the next subsection, we discuss combining them.

\input{tables/benchmarks.tex} 


% (a)
% https://docs.google.com/drawings/d/18bGih9gp5AbckjTamtKngySg2zKSJo-OyLSa6_S5YiM/edit?usp=share_link
% https://docs.google.com/drawings/d/1gWd4gub2PRIRKTihCJ_In_TeIJOwtp9BJAJf5DYUZGk/edit?usp=share_link
% 
% https://docs.google.com/drawings/d/1gWd4gub2PRIRKTihCJ_In_TeIJOwtp9BJAJf5DYUZGk/edit
\begin{figure} 
   \centering
   \hspace*{\fill}
  \subfloat[]{%
    \includegraphics[width=0.30\linewidth]{img/p_trets_v01.pdf}}
   %\hspace*{\fill}
   \subfloat[]{%
    \includegraphics[width=0.32\linewidth]{img/q_trets_v01.pdf}}
  \hspace*{\fill}
   \subfloat[]{%
    \includegraphics[width=0.33\linewidth]{img/pqs_trets_v01.pdf}} 
 \hspace*{\fill}
%  \vspace{0.2cm}
  \caption{(a) Pruning strategy. (b) Quantization strategy. (c) The combined strategy of scaling, pruning and quantization. }
%  \vspace{-0.1cm}
  \label{fig:design_flow} 
\end{figure}









% a/b/c/d
% https://docs.google.com/drawings/d/1LITQ6iUaKzns4Ov-5o1hatKcYCzojA7SZ2455dSsOEo/edit?usp=share_link
% https://docs.google.com/drawings/d/1LITQ6iUaKzns4Ov-5o1hatKcYCzojA7SZ2455dSsOEo/edit?usp=share_link
% https://docs.google.com/drawings/d/1kcL-_9dCiM6kxITpDKcIwppiBt4_O1d2IdWRsQNmSCA/edit?usp=share_link
% https://docs.google.com/drawings/d/1zL_MBbEh3A08n6iYpvFzOMaSWnGH13OwgLMCnyLb1jo/edit?usp=share_link
 \begin{figure} 
%   \vspace{-0.3cm}
    \centering
   \subfloat[Jet-DNN\label{fig:lhc_dnn_pruning}]{%
     \includegraphics[width=0.49\linewidth]{img/lhc_dnn_pruning05.pdf}}
    \hspace*{\fill}
   \subfloat[Jet-CNN\label{fig:lhc_cnn_pruning}]{%
     \includegraphics[width=0.49\linewidth]{img/lhc_cnn_pruning01.pdf}} 
   \\
%   \vspace{-0.3cm}
   \subfloat[VGG7\label{fig:vgg7_pruning}]{%
     \includegraphics[width=0.49\linewidth]{img/lhc_dnn_pruning01.pdf}}
 \hspace*{\fill}
   \subfloat[ResNet9\label{fig:resnet8_pruning}]{%
     \includegraphics[width=0.49\linewidth]{img/resnet8_pruning01.pdf}}
  % \vspace{0.2cm}
   \caption{ The auto-pruning algorithm applied to models with binary search direction shown. Omitting step s1 for visibility. The blue arrow indicates an accuracy loss $>$ user threshold; red denotes the optimal pruning rate. }
   \label{fig:auto_pruning} 
 \end{figure}





% a, b, c, d
% https://docs.google.com/drawings/d/1rkwCyGlR5qiEnzfB8YlFk6hKANpXDrKeTCRJsk3e-yg/edit?usp=share_link
% https://docs.google.com/drawings/d/1rkwCyGlR5qiEnzfB8YlFk6hKANpXDrKeTCRJsk3e-yg/edit?usp=share_link
% https://docs.google.com/drawings/d/1hbHz_j2x6jLrtz5ECg86enztNUXHWzMLYJ6lw2wJAmo/edit?usp=share_link
% https://docs.google.com/drawings/d/1H3blmXPUWjUpdM-qG30GzMylUt5R-durNwAkh70nDZM/edit?usp=share_link
% e, 
% https://drive.google.com/file/d/1kQchKeoSxlyghMjO4mzsXBJr-h9BXoiC/view?usp=share_link
% f, g, h, i,j
% https://docs.google.com/drawings/d/1pvUmqb5l-Ufo6vRvLBpOA-Amg9LlYXwP6Qh-oR7evnk/edit?usp=share_link
% https://docs.google.com/drawings/d/1JCIkY_xKqzHJ0FBOBZxsG8owN2pe1gZKNODX4c9CNXI/edit?usp=share_link
% https://docs.google.com/drawings/d/1UgHHzk3CtQuo7fYh7wT4YCW6tTsQs1bVKZZdwdeAxac/edit?usp=share_link
% https://docs.google.com/drawings/d/1m71oPluaW2QvfEW9p86NCQYeUGtqOruQvr3MKIKz8KU/edit?usp=share_link
% https://docs.google.com/drawings/d/1HnzOJKvK9zPeJKcAXblqph1YOIYqEfRjD5Pa835HAR8/edit?usp=sharing



\begin{figure*} 
   \centering
  \subfloat[]{%
    \includegraphics[width=0.32\linewidth]{img/dnn_pruning02.pdf}} 
   \hspace*{\fill}
  \subfloat[]{%
    \includegraphics[width=0.32\linewidth]{img/dnn_pruning_resource02.pdf}}
   \hspace*{\fill}
  \subfloat[]{%
    \includegraphics[width=0.32\linewidth]{img/resnet9_pruning_acc01.pdf}} 
   \\
   \hspace*{\fill}
  \subfloat[]{%
    \includegraphics[width=0.32\linewidth]{img/ResNet9_pruning_resource01.pdf}}
   \hspace*{\fill}
%    \vspace{-0.42cm}
  \subfloat[]{%
    \includegraphics[width=0.32\linewidth]{img/dnn_pruning_scale_acc01.pdf}} 
   \hspace*{\fill}
  \subfloat[]{%
    \includegraphics[width=0.32\linewidth]{img/dnn_pruning_scale_resource01.pdf}}
   \\
   \hspace*{\fill}
   \subfloat[]{%
    \includegraphics[width=0.32\linewidth]{img/dnn_scale_after_pruning_acc01.pdf}} 
   \hspace*{\fill}
  \subfloat[]{%
    \includegraphics[width=0.32\linewidth]{img/dnn_scale_after_pruning_resource01.pdf}}
    \hspace*{\fill}
  \caption{
  (a) Pruning rates \& accuracy of Jet-DNN.
  (b) Resource utilization of Jet-DNN design candidates after pruning.
  (c) Pruning rates \& accuracy of ResNet9.
  (d) Resource utilization of ResNet9 design candidates after pruning.
  (e) Jet-DNN pruning rates \& accuracy with scaling $\rightarrow$ pruning.
  (f) Resource utilization of Jet-DNN design candidates in (e). 
  (g) Jet-DNN pruning rates \& accuracy with pruning $\rightarrow$ scaling.
  (h) Resource utilization of Jet-DNN design candidates in (g).
  }
%  \vspace{-0.6cm}
  \label{fig:strategy_results} 
\end{figure*}



\begin{figure*} 
   \centering
   \subfloat[]{%
    \includegraphics[width=0.48\linewidth]{img/bit_width_visualization_dnn64s0.png}}
  \subfloat[]{%
    \includegraphics[width=0.48\linewidth]{img/bit_width_visualization_vgg7s0.png}}
 %  \hspace*{\fill}
 
  \caption{
  (a) Quantized bitwidth of each layers in VGG7. 
  (b) Quantized bitwidth of each layers in JetDNN. 
  }
%  \vspace{-0.6cm}
  \label{fig:quantization_results} 
\end{figure*}




\subsubsection{Pruning strategy.} 


The effectiveness of the auto-pruning algorithm is demonstrated in Fig.~\ref{fig:strategy_results}. Figures (a) and (c) depict the pruning rate and accuracy for Jet-DNN and ResNet9 in each step, while (b) and (d) show the resources utilization on Zynq 7020 and U250. As the pruning rate increases, hardware resource requirements, particularly DSPs and LUTs, decrease, leading to improved FPGA performance. The design candidate with the highest pruning rate within the allowed tolerance is selected.

\subsubsection{Scaling strategy.} 
To accommodate a large DNN design on an FPGA, we use the SCALING $O$-task that automatically reduces the layer size while tracking the accuracy loss $\alpha_s$. The search stops either when the loss exceeds $\alpha_s$.  
%or when the design can fit within the target FPGA while maintaining the desired accuracy. 
If necessary, $\alpha_s$ can be adjusted to achieve further size reduction with minimal impact on accuracy. This work sets $\alpha_s$ to 0.05\%, which allows for model size reduction with negligible accuracy loss.

\input{tables/quant_results01.tex}
\subsubsection{Quantization strategy.} 
%The QUANTIZATION $O$-task in this strategy operates at the HLS C++ level, providing more direct control over hardware optimizations and reducing unintended side effects when translating DNN models to HLS C++ using tools such as HLS4ML. This $O$-task automates mixed-precision quantization for networks. Although FPGA customizability creates a vast search space of quantization configurations, dependencies between layers help limit design options. For example, if a convolutional layer has an 8-bit result type, a directly subsequent max-pooling layer does not need 16 bits and vice versa. This $O$-task gradually decreases the bit-widths of weights, biases, and layer outputs, given an acceptable accuracy loss $\alpha_q$. The resulting precision configuration is directly instrumented into the C++ kernel, and a co-design simulation evaluates the accuracy of the quantized model. If the accuracy loss is within tolerance ($< \alpha_q$), this process is repeated. The precision of VGG7 model's layers after quantization with $\alpha_q$ set to 1\% is shown in Fig.~\ref{fig:strategy_results}(e).

This section showcases the evaluation results of the quantization \textbf{(Q)} optimization applied to multiple DNN models within the hardware (HLS) optimization space.

Fig~\ref{fig:quantization_results} shows the precision of the weights, biases and output of each virtual layer of the VGG7 model after being tuned by the quantization strategy with $\alpha_q$ set to 1\%. 
Table~\ref{table:results_only_quant} shows how the quantization affected key evaluation metrics relating to the performance and resource usage of 2 DNN designs. 
%Both JetDNN and VGG7 are fully unrolled. 
With $\alpha_q$ set to 1\%, the proposed QHS quantization algorithm reduces DSP usage by a factor of 4.9 for the VGG7 model, and 8.5 for the JetDNN model. When $\alpha_q$ is increased to 5\%, the designs are further compacted, but with a larger real accuracy loss. This table highlights how varying $\alpha_q$ levels affect model accuracy, DSP, LUT, and FF usage, illustrating the trade-offs between resource savings and accuracy as well as the effectiveness of the proposed QHS quantization algorithm. 



\subsection{Custom Optimization Strategy using Multiple $O$-tasks}\label{sec:multi_opt}


With our framework, new strategies can be derived by building and revising a design-flow architecture. For instance, by inserting a scaling $O$-task before the pruning $O$-task in Fig.~\ref{fig:design_flow}(a), a custom combined strategy can be created with results shown in \figref{fig:strategy_results}(e) and (f). The new optimal pruning rate is 84.4\% (\figref{fig:design_flow}(e)), lower than the previous 93.8\% (\figref{fig:design_flow}(a)), due to reduced redundancy from the preceding scaling task. By switching the order of the $O$-tasks, a different optimization strategy performing pruning-then-scaling (pruning $\rightarrow$ scaling) is achieved, resulting in a 0.7\% accuracy drop after one scaling step, as seen in \figref{fig:strategy_results}(g). Moreover, the three optimization $O$-tasks, pruning, scaling, and quantization, can be integrated into a single automated cross-stage strategy to enhance both performance and hardware efficiency, as illustrated in \figref{fig:design_flow}(c). We discuss the effects of different design-flow architectures with various combinations and orders in Section~\ref{sec:opt_search}, and various tolerable loss in Section~\ref{sec:opt_search_t}

% With our framework, it is easy to cascade different optimization tasks by simply modifying the design-flow architecture. For example, inserting a scaling $O$-task before the pruning $O$-task in \figref{fig:design_flow}(a) creates a custom combined strategy with results shown in \figref{fig:strategy_results}(f)-(g). 
% The new optimal pruning rate is 84.4\% (\figref{fig:design_flow}(f)), lower than the previous 93.8\% (\figref{fig:design_flow}(a)) due to reduced redundancy from the preceding scaling task. By switching the $O$-tasks order, a different optimization is achieved, resulting in a 0.7\% accuracy drop after one scaling step, as seen in \figref{fig:strategy_results}(h). Moreover, the three optimization $O$-tasks, pruning, scaling, and quantization, can be integrated into a single automated cross-stage strategy to improve both performance and hardware efficiency, as illustrated in \figref{fig:design_flow}(c). 

% https://docs.google.com/drawings/d/1kR7bdSbDPZf03wtjBRSH-V7HLbR0NmU8QM-y1LEnXeI/edit?usp=sharing

\begin{figure} [bp]
   \centering
   \hspace*{\fill}
  \subfloat[]{%
\includegraphics[width=0.42\linewidth]{img/pruning_branch_strategy01.pdf}}
   \hspace*{\fill}
   \subfloat[]{%
    \includegraphics[width=0.42\linewidth]{img/pruning_scaling_strategy05.pdf}}
 \hspace*{\fill}
%  \vspace{0.2cm}
  \caption{(a) Pruning optimization targeting different vendors using the BRANCH \textit{K}-task; (b) Combined strategy of scaling and pruning, exploring the order of $O$-tasks. }
%  \vspace{-0.1cm}
  \label{fig:branch_parallel_design_flows} 
\end{figure}


\begin{figure}
\begin{center}
\includegraphics[width=0.6\linewidth]{img/pruning_branch_results01.pdf}
\end{center}
%\vspace{-0.1cm}
\caption{LSTM model optimization on two FPGA platforms: (a) pruning rate and model accuracy using the PRUNING $O$-task; (b)~resource utilization on an AMD KU115 FPGA; (c)~resource utilization on an Intel A10 1150 FPGA.}

 
\label{fig:design_flow_branch_results}

\end{figure}


\subsection{Branching Flow}\label{sec:branch}

Fig.~\ref{fig:branch_parallel_design_flows}(a) illustrates a design flow that performs pruning for two alternate targets using the BRANCH $K$-task, for AMD/Xilinx FPGAs and Intel FPGAs. A user-defined selection function is supplied as a parameter to the BRANCH $K$-task which encodes a strategy determining the path forward for the design. 

Fig.~\ref{fig:design_flow_branch_results} presents the results of applying this design flow to an LSTM model on the MNIST dataset. Specifically, Fig.~\ref{fig:design_flow_branch_results}(a) illustrates the pruning rate and accuracy at each step using the PRUNING $O$-task. The tolerance is set to less than $\alpha_{p}$ (2\%) in this design.  Figs~\ref{fig:design_flow_branch_results}(b) and (c) show the resource utilization of the LSTM design after each pruning step respectively on an AMD KU115 FPGA and on an Intel A10 1150 FPGA. The DSP consumption is reduced from 6011 (108\%) to 2101 (38.1\%) on the AMD FPGA after the final pruning rate is optimized to be 71.9\%. Compared to AMD's HLS compiler, which prefers DSP blocks, Intel's HLS compiler tends to favor the use of soft multipliers for implementation. As shown in Fig~\ref{fig:design_flow_branch_results}(c), most of the computation kernels are implemented using logic resources rather than DSP blocks. 

While this design flow currently supports two types of FPGAs, it can be extended to include additional paths such as GPU, CPU and ASIC technologies. Moreover, this evaluation underscores the flexibility of our approach in utilizing the same software optimization task, specifically PRUNING, across multiple hardware targets.

\begin{figure}
\begin{center}
\includegraphics[width=0.8\linewidth]{img/scaling_pruning_pareto02.pdf}
\end{center}
%\vspace{-0.1cm}
\caption{Pareto Front meta-model designs post REDUCE $K$-task, color-coded paths.}

\label{fig:parallel_results}

\end{figure}

\subsection{Parallel Flows}
\label{sec:parallel_flow}
%\textbf{$K$-task: parallel flows}. 
Our framework enables the execution of multiple optimization paths in parallel, allowing the selection of the best outcome among them. In Fig.~\ref{fig:branch_parallel_design_flows}(b), we show a design-flow with two parallel paths, where the execution order of two $O$-tasks is changed: scaling $\rightarrow$ pruning and pruning $\rightarrow$ scaling. To support parallel branches, we use the FORK task to connect multiple strategy paths. The results from each path are then evaluated based on predefined criteria, such as accuracy or resource utilization, using the REDUCE task. For this strategy, a Pareto analysis was performed on the designs resulting from both paths, as shown in \figref{fig:parallel_results}. By employing this design-flow, designers can explore various optimization combinations and sequences when the outcomes of these strategies are not clear.

%\textcolor{blue}{ TODO: add more.  scaling-->pruning leads to designs with higher accuracy and also higher DSP utilization. while pruning-->scaling leads to designs with lower DSPs but lower model accuracy. }


% (a)
% https://docs.google.com/drawings/d/18bGih9gp5AbckjTamtKngySg2zKSJo-OyLSa6_S5YiM/edit?usp=share_link
% https://docs.google.com/drawings/d/1gWd4gub2PRIRKTihCJ_In_TeIJOwtp9BJAJf5DYUZGk/edit?usp=share_link
% 
% https://docs.google.com/drawings/d/1gWd4gub2PRIRKTihCJ_In_TeIJOwtp9BJAJf5DYUZGk/edit
\begin{figure} 
   \centering
   \hspace*{\fill}
  \subfloat[]{%
    \includegraphics[width=0.30\linewidth]{img/pruning_strategy05.pdf}}
   \hspace*{\fill}
   \subfloat[]{%
    \includegraphics[width=0.30\linewidth]{img/SPQ_branch_strategy_v01.pdf}}
  \hspace*{\fill}
   \subfloat[]{%
    \includegraphics[width=0.30\linewidth]{img/SPQ_branch2_strategy_v01.pdf}} 
 \hspace*{\fill}
%  \vspace{0.2cm}
  \caption{(a) Pruning strategy with Branch. (b) Combined strategy with Branch to python (SW) space. (c) The combined strategy with Branch to HLS space. }
%  \vspace{-0.1cm}
  \label{fig:bottom_up_design_flow} 
\end{figure}


\subsection{Bottom-up Flows}
\label{sec:bottom_up}
%\textbf{$K$-task: bottom-up flows}. 
Fig~\ref{fig:bottom_up_design_flow} reveal two distinct flows in the optimization strategies: top-down, where information flows from the DNN to the hardware stage, and bottom-up, the reverse. We have automated both flows by customizing the $K$-task BRANCH with a user-defined predicate function to activate or stop the bottom-up flow if the resulting design overmaps. The BRANCH task also support a user-supplied action function that is triggered when the predicate condition is true. For our current strategies, the action function changes the \textbf{CFG} section of the meta-model, increasing the accuracy tolerance parameters $\alpha_{p}$, $\alpha_{s}$, and $\alpha_{q}$ (if applicable) for the next iteration. Note that our current strategies in Fig.~\ref{fig:bottom_up_design_flow} have two DSE loops: an inner-loop that is codified within each $O$-task and an outer-loop supported by the bottom-up flow. Users can develop more complex strategies by customizing the outer loop, for instance, by updating the bottom-up condition and parameter tuning.



\begin{figure} 
   \centering
   \hspace*{\fill}
  \subfloat[]{%
    \includegraphics[width=0.8\linewidth]{img/dnn_all_resource_fpt05.pdf}}
   \hspace*{\fill}
   \\
%   \vspace{-0.40cm}
   \hspace*{\fill}
   \subfloat[]{%
    \includegraphics[width=0.8\linewidth]{img/vgg7_all_resource_fpt05.pdf}}
  \hspace*{\fill}
  \caption{(a) The hardware resources and latency of the Jet-DNN designs after various strategies. (b) The hardware resources and latency of the VGG7 designs after various strategies.}
%    \vspace{-0.12cm}
  \label{fig:dnn_vgg7_all_resource} 
\end{figure}

\begin{figure}
\begin{center}
%\vspace{-0.3cm}
\includegraphics[width=0.65\linewidth]{img/dnn_all_latn03.pdf}
\end{center}
%\vspace{-0.3cm}
   \caption{Latency, Initiation Interval (II) and model accuracy of various designs with different strategies}
\label{fig:dnn_all_latency}

\end{figure}

% https://docs.google.com/drawings/d/1hrWbSEuCEVwsYo4J399fyV7JhRM0BFXubZ_Odtcmb9M/edit?usp=sharing
\begin{figure}
\begin{center}
\includegraphics[width=0.80\linewidth]{img/strategy_cmp03.pdf}
\end{center}
%\vspace{-0.3cm}
   \caption{Comparison of Pareto frontiers of \textbf{JetDNN} model accuracy and resource utilization using different optimization strategies.}
\label{fig:strategy_cmp}

\end{figure}


\subsection{Optimization strategy search - combination and order}\label{sec:opt_search}
%By combining different methods to optimize acceleration, we increase the solution space and the chances to find more designs that can offer better trade-offs between model accuracy and hardware efficiency. To identifying the most effective combination and order of the optimization modules, we conduct all the possible candidates of the optimization strategies based on the three existing $O$-tasks by updating the design-flow automatically. When there are more $O$-tasks, a random search or a machine-learning assistant method can be adapted for an more efficient search. We leave this for our future work. 

% By combining different optimization methods for acceleration, the solution space becomes larger and more diverse, offering a wider range of potential designs to explore. This increased diversity enhances the probability of discovering designs that offer an optimal balance between model accuracy and hardware efficiency. 
% To identify the most effective combination and order of optimization tasks, we conduct all the possible candidates of the optimization strategies based on the three existing $O$-tasks by updating the design-flow automatically. 
% This process involves systematically evaluating various combinations and sequences of the optimization tasks to identify the most promising configurations.
% When dealing with a large number of $O$-tasks, the search for optimal combinations and sequences becomes more complex and computationally intensive. To address this challenge, more efficient search methods can be utilized, such as random search or machine-learning-assisted methods. This is left for our future work. 

Combining optimization methods increases diversity and potential designs, improving optimal balance between accuracy and efficiency. We systematically evaluate all candidates combined optimization strategies based on our current set of 3 optimization tasks, Scaling (\textbf{S}), Pruning (\textbf{P}), and Quantization (\textbf{Q}), to identify the most effective combination and order. The $\alpha_p, \alpha_s$ and $\alpha_q$ are set to 2\%, 0.05\% and 1\%. 
In particular, Fig~\ref{fig:dnn_vgg7_all_resource}(a) shows the hardware resource utilization results after each strategy and Fig~\ref{fig:dnn_all_latency} shows the corresponding latency, initiation interval and model accuracy. The final optimized model of Jet-DNN after scaling, pruning and quantization is depicted as ``S$\rightarrow$P$\rightarrow$Q" design, resulting in a reduction of the DSP usage by around 92\% and LUT usage by around 89\% compared with the original design (baseline~\cite{duarte2018fast, que2023metaml}). In addition, the latency is reduced by 35\% while the accuracy loss is trivial, as shown in Fig~\ref{fig:dnn_all_latency}. 
The same search is performed on the VGG7 network with results shown in~\figref{fig:dnn_vgg7_all_resource}(b). The final design reduces DSP usage by a factor of 23 with the same latency and around 1.1\% accuracy loss compared with the baseline design. 

%It should be noted that searching for the optimal combinations and sequences with multiple $O$-tasks can be computationally demanding because the size of the search space increases exponentially with the number of $O$-tasks. 
The search for optimal combinations and sequences with multiple $O$-tasks can be computationally demanding due to the exponential growth of the search space with the number of $O$-tasks. Advanced search methods can replace the current brute-force search. 
%Although the current brute-force search might not be longer feasible, more advanced searching methods can still be effective. 
Recent studies \cite{kurek2016knowledge, ferretti2022graph, wu2021ironman} 
demonstrate surrogate models can expedite the search. 
%have demonstrated that optimizers can use surrogate models to capture characteristics of the search space and expedite the optimization process. 
Our framework facilitates the deployment of such techniques, enabling users to interface an search algorithm with the hardware design with minimal programming effort.



\subsection{Optimization strategy search - tolerable loss variation}\label{sec:opt_search_t}
This subsection evaluate the performance and efficiency trade-offs when using different values for the maximum tolerable accuracy loss ($\alpha_p, \alpha_s$ and $\alpha_q$) for each optimization task to determine the effeteness of their combination. 
%\todo[inline]{explain why we introduce this experiment - how does it fit into what we mention our contribution in introduction/requirements/related work? }
%We evaluate combined DNN compression strategies involving three techniques: Scaling (\textbf{S}), Pruning (\textbf{P}), and Quantization (\textbf{Q}) to determine the most effective and efficient combination. 
We compare three strategies: \textbf{Q}, \textbf{S$\rightarrow$Q}, and \textbf{S$\rightarrow$P$\rightarrow$Q}, using Grid Search to create a Pareto frontier by assessing model accuracy against DSP and LUT utilization.
As illustrated in Fig.~\ref{fig:strategy_cmp}, our findings reveal that the \textbf{S$\rightarrow$P$\rightarrow$Q} strategy outperforms both the \textbf{S$\rightarrow$Q} and \textbf{Q} strategies across multiple dimensions, including accuracy, DSP and LUT usage. However, it is important to note that the \textbf{S$\rightarrow$P$\rightarrow$Q} strategy is less efficient in terms of time and design space requirements, demanding 220.5 times more time than the \textbf{Q} strategy. Practical applications may require a balance between performance, hardware resource utilization, and time investment, with the choice of strategy complexity and design space contingent on the specific use case. Our following experiments in this paper build on these findings, focusing on the \textbf{S$\rightarrow$P$\rightarrow$Q} strategy.



\begin{figure}[tp]
\begin{center}
\includegraphics[width=0.80\linewidth]{img/dse_cmp03.pdf}
\end{center}
%\vspace{-0.3cm}
   \caption{DSP-Accuracy Pareto frontiers for each optimization using different DSE methods for \textbf{JetDNN} models.}
\label{fig:dse_cmp}
\end{figure}


\begin{figure}[tp]
\begin{center}
\includegraphics[width=0.8\linewidth]{img/jetdnn_cmp_dsp_lut02.pdf}
\end{center}
%\vspace{-0.3cm}
   \caption{Comparison of resource utilization of the FPGA-based \textbf{JetDNN} networks using our approach and others (LogicNets
JSC~\cite{umuroglu2020logicnets}, Qkeras Q6~\cite{coelho2021automatic}, AutoQkeras QE/QB~\cite{coelho2021automatic} and MetaML~\cite{que2023metaml}) on an AMD/Xilinx VU9P FPGA. The Pareto frontier is highlighted.  }
\label{fig:jetdnn_cmp}
\end{figure}


\subsection{DSE Strategies}
\label{sec:explore_dse}
%\todo[inline]{explain why we introduce this experiment - how does it fit into what we mention our contribution in introduction/requirements/related work? }

Finding the optimal designs with grid search is time-consuming, as discussed in the previous section. This section investigates various DSE algorithms, including grid search, stochastic grid search (SGS), and Bayesian optimization. Fig.~\ref{fig:dse_cmp} presents the results. 
Each colored dot represents an design at a specific iteration, with different colors indicating different algorithms. The solid lines represent the Pareto Frontier of each algorithm over iterations, with each colored line corresponding to 22 iterations taking 40 hours. The uppermost grey line indicates a total of 343 iterations using extensive grid search over 624 hours, representing a baseline for comparison. 
The Bayesian optimization achieves similar results with just 22 iterations, significantly reducing processing time by a factor of 15.6 compared to Grid Search. Compared to SGS, Bayesian optimization's efficient parameter search yields multiple points near the Pareto frontier, indicating its effectiveness in finding optimal designs and approaching global optima. This effective parameter search by employing past results demonstrates the robustness of the Bayesian optimization approach, enabling users to optimize models while minimizing time and effort.


%\input{tables/cmp_fpga_trets_v05.tex}
\input{tables/cmp_fpga_v03.tex}

\subsection{Discussion and Comparison}\label{sec:discussion}

%Our evaluation results indicate that the combined strategy typically gets a better result than employing a single optimization technique. In addition, the order in which these optimization techniques are applied plays a crucial role. Applying the same optimization techniques in a different order produces varying final results, as shown in~\figref{fig:dnn_vgg7_all_resource}.  


Our evaluation results indicate that our combined $O$-task optimization strategy typically outperforms  single $O$-task techniques. Furthermore, the order in which these optimization techniques are applied plays a crucial role, as different orders produce varying final results, as depicted in~\figref{fig:dnn_vgg7_all_resource}.


%To demonstrate the advantages of our framework, we compare the design flow results to those of original Jet-DNN~\cite{duarte2018fast} using HLS4ML, LogicNets~\cite{umuroglu2020logicnets}, QKeras-based Q6~\cite{coelho2021automatic} and AutoQKeras-based QE and QB~\cite{coelho2021automatic} in~\tabref{table:cmp_fpga}. These studies target low-latency, low-resource, fully unfolded FPGA implementations. All designs use the same architecture, except the JSC-L which uses a larger one. 
%For JSC-M and JSC-L, accuracy is 70.6\% and 71.8\%, over 3.8\% lower than our "S+P+Q" design with $\alpha_q$ as 0.01. Compared to Q6, our design has 0.8\% higher accuracy, 2.5 times fewer DSP tasks, and 5.7 times fewer LUTs. Our design also outperforms QE and QB, with over 3.3\% higher accuracy and lower resource usage. Furthermore, our design has lower latency than all Q6, QE and QB designs, showcasing our framework's benefits.

 
To highlight the advantages of our framework, we compare our results to those from other approaches targeting low-latency, low-resource, fully unfolded FPGA implementation of the JetDNN network, including LogicNets~\cite{umuroglu2020logicnets} JSC-M and JSC-L, QKeras-based Q6~\cite{coelho2021automatic}, AutoQKeras-based QE and QB~\cite{coelho2021automatic}, 
and MetaML~\cite{que2023metaml} in Fig.~\ref{fig:jetdnn_cmp} and Table~\ref{table:cmp_fpga}.
All designs use the same architecture, except for JSC-L, which employs a larger architecture. 

Compared to original Jet-DNN~\cite{duarte2018fast},, our design achieves higher accuracy (up to 76.2\%) while using less hardware resources. When compared with LogicNets JSC-M and JSC-L~\cite{umuroglu2020logicnets}, which achieve accuracies of 70.6\% and 71.8\% respectively, our design demonstrates up to 5.6\% higher accuracy while also offering resource efficiency. 
Against the AutoQkeras Q6 and QE designs~\cite{coelho2021automatic}, which yield accuracies of 74.8\% and 72.3\% respectively, our framework attains higher accuracy and lower latency while providing more granular trade-offs between resource utilization and latency. 
Compared to MetaML~\cite{que2023metaml} with manual optimization, our work achieves comparable or better accuracy across various configurations with automatic optimization. 
Overall, the versatility of our design, which balances accuracy, latency, and resource utilization across different objectives, highlights its superiority and adaptability for diverse FPGA-based DNN applications.

This effective parameter search demonstrates the robustness of the Bayesian Optimization approach, enabling users to optimize models while minimizing development time. Besides, 
this work demonstrates flexibility and effectiveness by achieving competitive performance across multiple configurations, highlighting its adaptability to different optimization priorities such as Accuracy, DSP and LUT. 
It is worth noting that our results are identified automatically while the other approaches involve manual optimization by DNN and hardware experts.



% Inspired by the AutoQKeras design QB which minimizes model bit consumption, our design can be further optimized by tuning the strategy parameters like $\alpha_q$. 
% By increasing the quantization $O$-task's tolerant accuracy loss ($\alpha_q$) from 1\% to 4\%, the model size is further reduced. DSP usage drops to 23, three times lower than AutoQKeras' most resource-efficient model, QE, as shown in~\tabref{table:cmp_fpga}.
% When the tolerant accuracy loss ($\alpha_q$) for the quantization $O$-task increases from 1\% to 4\%, the strategy can further reduce the model size. 
% The DSP usage drops to 23 that is 3 times lower than the most resource-efficient model, QE, using AutoQKeras as shown in~\tabref{table:cmp_fpga}. 
% The model accuracy decreases to 72.8\% but remains higher than the optimized AutoQKeras designs QB and QE. 

%Inspired by the AutoQKeras design QB, which minimizes model bit consumption, we further optimize our design by tuning the parameters, such as $\alpha_q$. Increasing the quantization $O$-task's tolerant accuracy loss ($\alpha_q$) from 1\% to 4\% results in a smaller model with DSP usage 3 times lower than AutoQkeras' most efficient model, QE (See~\tabref{table:cmp_fpga}. 

%The DSP usage drops to 23, which is three times lower than AutoQKeras' most resource-efficient model, QE, as shown in~\tabref{table:cmp_fpga}. 
%Although the model accuracy decreases to 72.8\%, it remains higher than the optimized AutoQKeras designs QB and QE.



