\section{Implementation}

\subsection{Auto-Pruning Algorithm with Binary Search}
Pruning is a technique that improves the performance and efficiency of neural networks by removing insignificant weights. The pruning strategy uses a PRUNING $O$-task, which gradually zeroes out weights during training to create a more compact and efficient network while maintaining accuracy. This optimization task supports auto-pruning, which automatically determines the highest pruning rate while maintaining a given level of accuracy loss. The PRUNING $O$-task is illustrated in Fig.~\ref{fig:design_flow}(a). Formally, the objective of this $O$-task is defined as:
\begin{equation}
\begin{footnotesize}
\begin{aligned}
& \underset{}{\text{maximum}}
& & Pruning\_rate \\
& \text{subject to}
& & Accuracy\_loss(Pruning\_rate) \leq \alpha_p 
\end{aligned}
\end{footnotesize}
\end{equation}
%\begin{footnotesize}
%\begin{align*}
%& \underset{}{\text{maximum}}
%& & Pruning\_rate \\
%& \text{subject to}
%& & Accuracy\_loss(Pruning\_rate) \leq \alpha_p 
%\end{align*}
%\end{footnotesize}
%This optimal rate along with the corresponding trained model are returned by this task. 

%The auto-pruning algorithm starts with a pruning rate of 0\% with an initial accuracy, $Acc_{p0}$, at step~1 (s1). 
Starting at 0\% pruning rate, the auto-pruning algorithm obtains initial accuracy
$Acc_{p0}$ at step~1 (s1).
It then uses a binary search approach, increasing or decreasing the pruning rate based on whether the accuracy loss is within a user-defined tolerance ($\leq \alpha_{p}$). The algorithm terminates when the rate difference is below a threshold ($\beta_{p}$). The number of steps is determined by $1+log_{2}(1/\beta_{p})$. Algorithm search steps and direction are shown in Fig.~\ref{fig:auto_pruning}.
Both the tolerance ($\leq \alpha_{p}$) and threshold ($\beta_{p}$) values are initially set to 2\% in this work.  %The algorithm is illustrated in Fig.~\ref{fig:auto_pruning}, and 

\subsection{Quantization Heuristic Search (QHS) algorithm}

The optimization task with the proposed QHS algorithm operates at the HLS C++ level, modifying the fixed-point numerical representations directly at C++ source-level. At this level, there is more direct control over hardware optimizations, resulting in fewer unintended side-effects than making changes at a higher level. This task supports auto quantization, which automatically determines the lowest bitwidth for each layer while maintaining a given level of accuracy loss. Formally, the objective of this task is defined as:
\begin{equation}
\begin{footnotesize}
\begin{aligned}
& \underset{}{\text{maximum}}
& & Bit\_width\_reduction \\
& \text{subject to}
& & Accuracy\_loss(Bit\_width\_reduction) \leq \alpha_q 
\end{aligned}
\end{footnotesize}
\end{equation}
%As GPUs (or other AI-targeted accelerators) dominate the space of AI-computation, and these devices typically require uniform precision across most computations in order to efficiently utilize their hardware, most state-of-the-art quantization schemes aim to reduce the bit-widths of all parameters and operations to some uniform precision. The vast customizability of FPGAs, however, allow them to ignore these restrictions on uniformity of precision. 
It performs mixed-precision quantization for DNNs, allowing separate precisions for the weights, biases and results of layers of a model, taking the advantages of the customizability of FPGAs. 
While this customizability creates a massive search space of quantization configurations, dependencies within the network can be used to reduce the degrees of freedom of the design space. 
While other efforts target inter-layer mixed-precision quantization algorithms, our meta-programming based approach programmatically groups a number of cascaded layers described in HLS C++ into one virtual layer based on specific assumptions to reduce the complexity and improve efficiency of this optimization task. For example, if the output of a convolutional layer is 8 bits, there is no benefit in having a directly subsequent max-pooling layer with a 16-bit width. Conversely, if the max-pooling layer requires a bit-width of 4, an equivalently accurate and more efficient design would set the bit-width of the preceding convolutional layer to 4 as well.  This work automatically compacts a model into a series of virtual layers using these dependencies. These virtual layers are constructed by grouping weight-layers, such as convolutional or FC layers, together with batch-normalization, pooling, and activation functions on their outputs.

% https://docs.google.com/drawings/d/1igvZ0j7WyJbA7QzCodamxoW1GqXSUn1kCczlGxi7o88/edit?usp=sharing

\begin{figure}
\begin{center}
\includegraphics[width=0.80\linewidth]{img/quantization_v02.pdf}
\end{center}
%\vspace{-0.5cm}
   \caption{The proposed QHS algorithm}
\label{fig:quantization_algo}
\end{figure}

%The QUANTZATION $O$-task is illustrated in Fig.~\ref{fig:design_flow_simple}(a). 
Fig.~\ref{fig:quantization_algo} shows the steps of the proposed QHS algorithm. The optimization task begins by accepting a model with a maximum tolerable accuracy loss, denoted as $\alpha_q$. Initially, virtual layers are created, and lossless reduction is performed by minimizing the integer bits representing model parameters using the base-2 logarithm of the largest weight and bias in each layer, plus one bit for the sign to avoid saturation. Subsequently, all parameters (weights, biases and results) are initially assumed to be further reducible. The total bit-widths of these parameters are then reduced by one bit if they are considered reducible. This quantization is applied to the C++ kernel, and the accuracy is evaluated through a runtime simulation. If the accuracy loss remains within the user-specified tolerance, $\alpha_q$, this step is repeated. If the accuracy loss exceeds the tolerance, the algorithm identifies and blocks non-reducible precisions. It determines which precisions are most sensitive by testing the reduction impact on accuracy. If a bit-width reduction breaks the accuracy constraint, that precision is marked non-reducible. The process repeats until no further reductions are possible within the accuracy constraints.





\subsection{Scaling Algorithm.} 
To accommodate a large DNN design on an FPGA, we use the SCALING $O$-task that automatically reduces the layer size while tracking the accuracy loss $\alpha_s$. The search stops either when the loss exceeds $\alpha_s$.  
%or when the design can fit within the target FPGA while maintaining the desired accuracy. 
If necessary, $\alpha_s$ can be adjusted to achieve further size reduction with minimal impact on accuracy. 
%This work sets $\alpha_s$ to 0.05\%, which allows for model size reduction with negligible accuracy loss.



\subsection{Co-Optimization Workflow}
As illustrated in Fig.~\ref{fig:opt_flow}, our co-optimization approach is unique in its ability to programmatically encode three types of optimizations. At the top level, a DSE optimization program is employed, executing a comprehensive exploration strategy and overseeing local optimizations at two (or more) distinct levels of abstraction. At the software DNN level, optimizations are executed on the architecture using tools like Tensorflow or PyTorch. At the HLS DNN level, optimizations are conducted by modifying the HLS code automatically  using metaprogramming. Optimizing DNN models across multiple abstraction levels, from software to hardware, leverages complementary strategies that address different bottlenecks within the system. Software optimizations prioritize algorithmic efficiency and reducing computational complexity, while hardware optimizations target improvements in throughput, latency, resource utilization and energy efficiency.


% Google: https://docs.google.com/drawings/d/1rpAAfKMecLL6BOohYh8xnlNCJs5t4TB-EzrXE9xzzZ8/edit?usp=sharing
\begin{figure}[tp]
    \begin{center}
    \includegraphics[width=0.8\linewidth]{img/implementation.pdf}
    \end{center}
       \caption{This figure illustrates the implementation of our co-optimization framework, featuring an organized system of \textit{optimization} spaces, each autonomously running Python programs within dedicated environments, overseen by an \textit{exploration} space executing a general optimization strategy. The diagram depicts the orchestration of local optimization spaces, such as software and hardware, through a controller process.}
    \label{fig:opt_flow}
\end{figure}

Our co-optimization workflow (Fig.~\ref{fig:opt_flow}) begins with the Exploration space. Here, a DSE program utilizes a DNN model, training data, and configuration parameters to conduct the exploration workflow. This process yields a variety of hardware designs with trade-offs in metrics such as performance and accuracy. A controller process orchestrates this exploration by managing communication between the DSE program and local optimization spaces. Prior to exploration, the DSE program sets up and deploys optimization programs within these spaces. Fig.~\ref{fig:opt_flow} depicts two such spaces: one for software (SW) utilizing TensorFlow and another for HLS employing Vivado HLS. Each space optimizes the model according to selected strategies, such as pruning or quantization. 


After system setup, the exploration loop commences. In each iteration: \circled{1} The DSE program dispatches the DNN model and a specific parameter set to the software optimization space, where tasks like scaling and pruning are executed. \circled{2} Upon optimization completion, the refined model is returned to the controller along with its associated accuracy. \circled{3} Subsequently, the model is forwarded to the HLS space for additional optimization. A metaprogram adjusts the source code based on recommended optimizations (e.g., quantization) and parameter values from the controller. After synthesis, metrics such as power, area, and accuracy are obtained and \circled{4} fed back to the Exploration space. The DSE program evaluates designs using these metrics, retains promising candidates, and suggests new parameters for the next iteration.


This separation of concerns into distinct optimization spaces—software, HLS, and DSE—allows for the integration of diverse optimization strategies and techniques. It enables the introduction of faster or more effective exploration methods without altering existing optimizations. In addition, it combines exploration across software and hardware domains to identify effective techniques for optimizing performance. Moreover, it facilitates specialization of new hardware optimizations specific to certain FPGA HLS tools by allowing HLS code to be customized through metaprogramming.


%This approach meets our three requirements. To address \textbf{R1}, it enables the introduction of faster or more effective exploration methods without altering existing optimizations. To address \textbf{R2}, it combines exploration across software and hardware domains to identify effective techniques for optimizing performance. To address \textbf{R3},  it facilitates specialization of new hardware optimizations specific to certain FPGA HLS tools by allowing HLS code to be customized through metaprogramming.



\subsection{Metaprogramming based Design Optimization}

% https://docs.google.com/drawings/d/1TIYxkBPWlVokg2mRD_A86qIEu-13HelA5oy9WDE6yww/edit?usp=sharing
\begin{figure}[tp]
    \begin{center}
    \includegraphics[width=0.90\linewidth]{img/metaprogramming.pdf}
    \end{center}
       \caption{ (a) Current methods for deriving hardware designs rely on templates developed by experts. However, these templates are constrained in generating a finite number of design variants from predetermined parameter values and demand expertise for accommodating new DNN architectures.
(b) An HLS metaprogram in pseudocode illustrating dynamic C++ code manipulation through Abstract Syntax Tree construction to derive an optimized design variant.}
    \label{fig:metaprogramming}
\end{figure}

A novel aspect of our co-optimization approach, as previously mentioned, is the integration of \textit{metaprogramming} into hardware design optimization, diverging from traditional reliance solely on hardware templates (see Fig.~\ref{fig:metaprogramming}(a)). These templates, crafted by hardware experts, are inherently limited in generating design variants from a predefined set of parameter values, and require expertise for extending or accommodating new DNN architectures.

In contrast, our approach optimizes hardware design architecture by programmatically modifying unoptimized High-Level Synthesis (HLS) C++ code using Python, expanding the range of possibilities beyond parameter values to generate diverse design variants. This manipulation encompasses tasks such as adjusting HLS pragmas, code restructuring for complexity reduction or performance enhancement, data type alterations for quantization support, and utilization of specialized libraries tailored for specific FPGA devices~\cite{fccm20_artisan} (see HLS space in Fig.~\ref{fig:opt_flow}).

Figure~\ref{fig:metaprogramming}(b) illustrates an HLS metaprogram in pseudocode for clarity. Metaprograms are able to analyze and instrument C++ code, creating hardware design variants to be considered during the Design Space Exploration (DSE) process. Hence, in our context, a metaprogram is a program that takes as input a C++ program and generates a C++ code variant using source-to-source transformations.  In this example, the metaprogram creates a variant by adding an unroll pragma to all outermost \texttt{for}-loops in the kernel function of the original code. The key steps of the metaprogram include: (1)~constructing an Abstract Syntax Tree (AST) representation of the C++ code; (2)~searching for node pairs (loop, function) in the AST that meet specific criteria; and (3)~iterating through the identified loops to insert the pragma code before the loop construct. A more detailed explanation about querying and instrumentation capabilities can be found in~\cite{fccm20_artisan}. 


\subsection{Design Space Exploration (DSE)}
Within our approach, Design Space Exploration (DSE) algorithms are critical for automatically adjusting the optimization parameters in subsequent iterations to improve design performance, while maintaining high model accuracy and meeting the constraints of the device. 

%\textbf{Bayesian Optimization:} 
Bayesian optimization is a key algorithm we utilize for global optimization of expensive-to-evaluate functions (represented as f(\texttt{x})). It models the objective function using a probabilistic model (e.g., Gaussian Process) and uses an acquisition function to decide where to sample next, balancing exploration of the design space with exploitation of known optimized configurations. In our context, \texttt{x} denotes the array of configuration settings, which serve as parameters for each optimization task within our framework. The process guides towards the most promising configurations for evaluation until a predetermined number of iterations is reached.

Our performance metrics encompass accuracy, hardware resource utilization, and adherence to operational constraints. We leverage hardware usage data obtained through Xilinx Vivado, including latency, DSP, LUT, FF, and BRAM, to inform our optimization strategy for model deployment, adjusting the optimization parameters including scaling, pruning, and quantization thresholds ($\alpha_{p/s/q}$). 
Due to the heterogeneity of these metrics, direct summation is impractical, necessitating normalization to standardize them. This normalization technique is widely applied in various DSE methods~\cite{sohrabizadeh2022automated}.


%Our performance metrics are diverse, incorporating considerations such as accuracy, hardware resource utilization, and compliance with operational constraints. We leverage hardware usage data obtained through Xilinx Vivado to inform our optimization strategy related to model deployment, including latency, DSP, LUT, FF, and BRAM, to revise the optimization parameters, including the $\alpha_{p/s/q}$ for scaling, pruning and quantization. Given the heterogeneity of these metrics, direct summation is not feasible; normalization is thus employed to bring them onto a common scale. The normalization is a common technique and has been applied in many other DSE approaches~\cite{sohrabizadeh2022automated}. 

Moreover, when optimizing a complex system, such as FPGA-based DNN accelerators, practical constraints must be considered. These constraints may arise from hardware limitations, performance requirements, and other critical factors. Incorporating these constraints into the Bayesian optimization process ensures that our search for the optimal solution occurs within a feasible and practical region of the design space. For hardware utilization constraints, a score becomes a negative maximum integer when the utilization is higher than a predefined threshold. This signals the Bayesian algorithm that the input parameter is unsuitable, steering the optimization away from poor configurations. The same approach applies to model accuracy constraint. The pseudocode of Bayesian score algorithm is shown below: 
%\input{algo_v01.tex}

\vspace{0.2cm}
\textbf{if} (Bayesian constraints not met) \textbf{then} \\
\indent \indent $f(x)=-sys.maxsize$ \\
\indent \textbf{else} \\
%\indent \indent $\text{f(x)} = 
%\sum_{\text{x} \in \{\text{Acc.},  \text{DSP}, \text{LUT}, \text{BRAM} \}} \text{Norm\_Results}[\text{x}]  \times \text{W}[\text{x}]$ \\
\indent \indent $f(x) = 
\sum_{x \in \{Acc.,  DSP, LUT, BRAM \}} Norm\_Results[x]  \times W[x]$
\vspace{0.2cm}

The algorithm assigns weights (W[metric]) to each metric to indicate their relative importance. 
%\textcolor{blue}{add function of Bayesian}