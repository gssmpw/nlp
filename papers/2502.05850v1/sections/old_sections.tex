

\newpage

\section{Related Work}\label{sec:related_work}

TVM (Tensor Virtual Machine) framework provides a modularized optimization pipeline including a range of optimizations, such as quantization and kernel fusion. However it focus more on the software-based optimizations but does not considering various hardware design spaces, such as various HLS or RTL design options. This work also provides hardware level optimization strategy blocks, such as mixed-precision quantization and folding. 

This work also proposes a set of hardware-level optimization strategies, such as mixed-precision quantization and folding, which are automatically applied to an HLS-based DNN model. These strategies are implemented using Artisan, a source-to-source transformations tool that manipulates the hardware designs in HLS C++. They can be combined with algorithm level optimization strategies to create a customized optimization pipeline, enabling efficient algorithm and hardware co-optimizations and resulting in a highly optimized designs on FPGAs. 



Xilinx's Vitis AI and Intel's OpenVINO provide a set of pre-built optimization techniques that can be applied to DNN models, but they are not designed in a modular fashion, leading to difficult add new optimization strategies or demonstrate the combination of various optimization techniques. 

FINN and HLS4ML provide a library of optimized building blocks that can be easily reused in different designs, allowing users to build FPGA-based DNN accelerators with high performance and energy efficiency. This work provide a framework with a library of optimization strategy blocks which can be reused and combined, targeting various DNN designs to achieve high performance and energy efficiency. FINN and HLS4ML can be an useful buliding block of our framework. 



\cite{hao2018deep} introduces the co-design of DNN and hardware accelerator, which co-optimizes the software and hardware simultaneously. Some subsequent studies~\cite{hao2019fpga, hao2019nais, yang2019synetgy, hao2020effective, jiang2020hardware, dong2021hao,  hao2021enabling, zhang2022algorithm} target co-design for various applications. However, these optimizations are fused with the design space exploration and many of them target a specific application domain. It could be difficult to reuse these optimization blocks for especially future no-existing machine learning designs. 
DNNExplorer~\cite{zhang2020dnnexplorer} is an automation tool to enable fast accelerator architecture exploration based on both dedicated pipeline structure and generic reusable structure to provide improved performance. 
HAO~\cite{dong2021hao} proposes to jointly optimize the neural architecture, quantization and hardware design to minimize latency with integer programming formulation. 
\cite{chitty2022neural} presents a comprehensive discussion about hardware-aware neural architecture search (HW-NAS) to automate the co-search of neural algorithm and hardware specification. 

%for the search algorithm to prune the design space. 
\cite{roorda2022fpga} explores FPGA architectures for DNN acceleration. 
LogicNets~\cite{umuroglu2020logicnets} maps DNNs to fine-grained FPGA building blocks to obtain efficient implementations for extreme-throughput applications. \cite{coelho2021automatic} presents QKeras to support a quantization-aware training technique to achieve high hardware efficiency. This work achieves higher performance than using LogicNets~\cite{umuroglu2020logicnets},  QKeras~~\cite{umuroglu2020logicnets} and AutoQkeras~\cite{umuroglu2020logicnets}. 
%There are also many HLS-base optimization approachs, such as supporting task-parallelization~\cite{chi2021extending}, dynamic scheduing~\cite{josipovic2018dynamically} and 

TVM~\cite{chen2018tvm} is a general-purpose compilation framework for deep learning that aims to decouple the algorithm definition from its schedule. HeteroCL~\cite{lai2019heterocl} decouples the algorithm from a temporal compute schedule while HeteroFlow~\cite{xiang2022heteroflow} decouples data placement and co-optimization with other hardware customizations such as tiling and data quantization. All of these frameworks target overlay-base accelerators. Other frameworks, such as HLS4ML~\cite{duarte2018fast}, FINN~\cite{umuroglu2017finn} and fpgaConvNet~\cite{venieris2016fpgaconvnet} map high level Python-based neural networks to FPGAs in dedicated circuit designs.  

The studies mentioned in this section are orthogonal to our proposed approach, which focuses on combining and customizing  device-independent and device-specific cross-stage optimizations, incorporating new and existing approaches. 

% \begin{figure}%[ht]
% \begin{center}
% \includegraphics[width=0.9\linewidth]{img/design_flow.jpg}
% \end{center}
%    \caption{Optimizations Flow (TODO, redraw to increase the image quality)}
% \label{fig:design_flow}
% \end{figure}

%\subsection{Related Work}

\section{Our Approach} \label{sec:approach}

\subsection{Design-Flow}

We define a design-flow as a sequence of processes that transform a high-level model into a low-level model which is capable of running, potentially optimized, on a specific hardware platform. A \textit{model}, on the other hand, is a design refinement associated to a design-flow stage, which operates on a specific level of abstraction and model of computation. Examples of models associated to different stages include: a DL model which captures the neural network architecture, a C++ HLS model which describes hardware behavior using software description, and an RTL model which captures a synchronous digital circuit by representing connections between registers and logical operations. Each stage has its own concerns and optimization techniques.

Current design-flows are built as pipelines that cover multiple stages. As we reach the lower stages of the pipeline, we gradually lower the model abstraction, specializing towards the target machine until we reach the final executable. For instance, using the HLS4ML framework, we can build a design-flow that translates a DL model into C++ HLS code. In turn, with Vivado HLS we synthesize C++ HLS code to an RTL model. In addition, we can use feedback cycles to allow information accrued at the lower stages of a pipeline to feed into its higher stages. This information allows optimizations to refine their strategies by employing more accurate metrics that were not previously available.

We contend that current design-flow mechanisms do not scale well with the extremely large design-space that stems from mapping DL architectures to reconfigurable architectures. In particular:

\begin{itemize}
\item[] \textbf{(C1) Unstructured compilation artifacts.} During the execution of a design-flow, considerable amounts of data are processed and generated, which we call \textit{design-flow artifacts}. These artifacts may include models from different stages, support files and tool reports. All these artifacts are typically fragmented and not structured in a coherent way.

\item[] \textbf{(C2) Limited scope.} Typically, each design-flow stage has a restricted view and control over the artifacts it can operate on, focusing on a single stage (e.g., either at DL level or at C++ but not both). Thus, each stage remains oblivious about all other artifacts generated up to the point of their execution. For instance, AutoKeras focuses only on DL models, while Vivado HLS optimization only focuses on C++ HLS. 
%HLS4ML provides limited support for both domains by supporting accuracy analysis using custom fixed-point integers, but not additional hardware metrics.

\item[] \textbf{(C3) Limited modularity.} Due to sheer size of the design space, there is no single recipe to optimize a DL model to an FPGA design. Thus multiple optimization strategies can be realized by combining different techniques and parameters to meet desired requirements,  such as accuracy, resource area and performance, but can also produce an optimized design in a reasonable amount of time. Each framework, such as QKeras and HLS4ML, has its own interface and parameters, making it difficult to combine components without ad hoc code adapters to translate between interfaces.

\item[] \textbf{(C4) Limited traceability.} As a result of a complex design-space exploration strategy that can take hours to complete and the large volume of design-flow artifacts produced during this time, it is often difficult to explain how the end-result was produced, specially in cases where there is evidence of bugs: which parts of the design-flow misbehaved? Key issues when debugging design-flows include the lack of traceability of artifacts produced, as well as the inability to replay a design-flow from a specific point (checkpointing).
\end{itemize}

In this paper, we address these technical challenges by proposing an approach that hinges on two mechanisms: the \textbf{meta-model} and the \textbf{pipe block}. We detail both mechanisms in the following subsections.

%https://docs.google.com/drawings/d/1otwTGTE-WFJpWIBUUN6kqYy2-vy7La48QeGxbWY0XuQ/edit?usp=sharing
\begin{figure}[tp]
   \centering
    \includegraphics[width=1\columnwidth]{img/meta-model.pdf} 
  \caption{An example of a meta-model structure. The meta-model is a static snapshot of the design-flow runtime execution at a specific point in time, which captures the history of each stored artifact. This allows easier debugging as well as checkpointing. The meta-model contains 3 parts: (I) \textbf{configuration} which contains the design-flow parameters supplied by the user, (II) \textbf{log} which contains historic data and (III) \textbf{model space} which contains a repository of all committed models during design-flow execution.}
  \label{fig:meta-model} 
\end{figure}

\subsection{Meta-Model}

In our approach, we extend the \textit{model} concept and replace it with the \textit{meta-model}. Similar to its counterpart, the \textit{meta-model} is the basic data unit that streams into every part of the design-flow and is produced as the result. However, regardless of stage, the input and output of every component of our design-flow is the same: a meta-model.

The meta-model is designed to capture a static and complete snapshot of a design-flow runtime since the beginning of its execution. It captures all the artifacts and historic data in a structured manner, allowing any stored data to be inspected at any time. As illustrated in Fig.~\ref{fig:meta-model}, the meta-model contains 3 parts:    \textbf{(I)}~configuration, \textbf{(II)}~log and \textbf{(III)}~model space. 

The \textit{model space} inside a meta-model references all models generated during the execution of a design-flow at a specific point in time. Each stored model is versioned, and models derived by different stages can co-exist in the same model space. In our figure example, we have stored three types of models: DL, C++ HLS and RTL.

An important feature of the meta-model is that it keeps track of the history of all the models in its \textit{log} section (\textbf{C4}). This is analogous to the git repository log which contains the history of all commits. As can be seen in the example of Fig.~\ref{fig:meta-model}, each model has tracing information about where it is derived from and which design-flow part produced it. A model can be a \textit{variation} of an existing model, both belonging to the same type. Or it can be a \textit{specialization} in which we lower the abstraction closer to the target platform. For instance, model \textit{v2} is a variation of \textit{v1} by applying a neural architecture transformation (e.g. pruning), model \textit{v1.2} is a variation of model \textit{v1.1} using a source-to-source C++ transformation, while model \textit{v2.1} is a specialization of model \textit{v2} using HLS4ML.

Each model in the model space references data in a physical storage space. This storage space encapsulates all the model supporting files, as well as tool reports and computed metrics. Every model and its history can be accessed and manipulated through programmatic access via an API (\textbf{C1}). We provide more details about the implementation in Section~\ref{sec:implementation}.


%fused figure: https://docs.google.com/drawings/d/1Q3OecTlaum0nm7jHnb-WjoVvjxdLRfSM1q2jYUton2g/edit
% a, b
% https://docs.google.com/drawings/d/1bM5SfGFCLphPfhn4y5qVFLgVVid5uaNU2gvQ-pYel2g/edit?usp=share_link
% https://docs.google.com/drawings/d/1bM5SfGFCLphPfhn4y5qVFLgVVid5uaNU2gvQ-pYel2g/edit?usp=share_link
\begin{figure}[tp]
   \centering
  \subfloat[\label{fig:pruning}]{%
    \includegraphics[width=0.5\linewidth]{img/pipeblocks_a01.pdf}}
   \\
   \subfloat[\label{fig:pruning}]{%
    \includegraphics[width=0.8\linewidth]{img/pipeblocks_b01.pdf}}
  \vspace{0.2cm}
  \caption{(a) A pipe block has a uniform interface allowing any two pipe blocks to be connected (although there may be constraints about how many connections a block can handle); (b) A connection between a $\lambda$-block and a $\kappa$-block. A $\lambda$-block typically updates the model space by adding new model variations or specializations. A $\kappa$-block on the other hand, manages the control flow. Each connection defines a unidirectional stream between a source block and a target block.}
  \label{fig:pipeblocks} 
\end{figure}

\subsection{Pipe Block}

The pipe block, as illustrated in Fig.~\ref{fig:pipeblocks}(a), is the basic component used to build a design-flow architecture. A block can be connected to multiple blocks, and multiple blocks can be connected to a single block (\textbf{C3}). Each connection defines a unidirectional stream channel between a source block and a target block, where a meta-model is the only data transmitted between them. The design-flow architecture can be represented by a directed graph with feedback cycles, and follows a dataflow execution model. A block is triggered for execution when it receives a meta-model on one of its input channels, and must produce one or more meta-models as a result to trigger the next pipe block in the flow. It then becomes idle until receiving the next input.

Each pipe block in a design-flow architecture is an instance of a specific type. Instances of the same type share the same behavior. A list of pipe block types currently implemented can be found in Table.~\ref{table:pipeblocks}. A block type is characterized as follows:
\begin{itemize}
\item \textbf{Role.} We consider two roles: lambda ($\lambda$) and control ($\kappa$). The \textbf{$\lambda-$block} typically updates the model-space by adding, for instance, a new model variation (say pruning a DL model) or a specialization. The \textbf{$\kappa$-block}, on the other hand, handles the control flow, for instance, branching according to a given condition, or forking to enable concurrent flows. 
\item \textbf{Multiplicity.} Specifies the number of inputs and output channels that a block can handle. For instance, some blocks types can support a one-to-one flow (one input and one output) such as the PRUNING and the HLS4ML blocks, while others may support one-to-many (e.g., branch block) and many-to-one (join block). Therefore, multiplicity restricts how blocks can be combined in a design-flow architecture.
\item \textbf{Parameters.} The parameters allow users to modify and control block behavior. Some of the parameters may be required, such as the training dataset location, while others are optional and based on default values. 
\end{itemize}

Fig.~\ref{fig:pipeblocks}(b) illustrates the connection between two blocks: a $\lambda$-block and a $\kappa$-block. When a $\lambda$-block receives meta-model \textit{A}, it checks the configuration (CFG) section for arguments (parameter values) for execution. Note that the configuration section in the meta-model contains arguments for all the blocks in the design-flow, however as we explain in the next section, we can restrict arguments for specific block types and also for specific instances. The $\lambda$-block may commit a new model in the model-space: every action, including block execution, is added into the log. The resulting meta-model (A') is then flushed to the output channel. The next block in our example is a fork block, which clones the incoming meta-model A' to derive meta-models A'' and A''', respectively, which are forwarded to the corresponding output channels. 

Every pipe block is strictly stateless. Thus, by storing the configuration and all design-flow artifacts inside a meta-model, as well as tracing every step in the log section, we can naturally follow its evolution during the life-cycle of a design-flow (\textbf{C2}). Moreover, we are able to use a meta-model as a checkpoint, rolling back to a specific version, or replaying from a version that was the result of hours of execution.



\section{Implementation}\label{sec:implementation}

\input{tables/pipeblocks03.tex}

Table~\ref{table:pipeblocks} provides a list of pipe blocks that have been implemented using our approach, along with their corresponding role, multiplicity and parameters:
\begin{itemize}

\item \textbf{$\kappa$-Blocks}. The basic control blocks include BRANCH which creates two streams that are selected at runtime according to a boolean condition, and JOIN which funnels two or more streams into one. In addition, the FORK block creates multiple streams allowing multiple strategy paths to be executed concurrently, and a REDUCE block which waits for a meta-model from every incoming branch, and selects a meta-model based on a specific condition. The STOP block, as the name implies, terminates the design-flow.

\item \textbf{$\lambda$-Blocks}. We have implemented three well-known techniques to improve Deep Learning models, namely pruning and scaling which are implemented using the Keras API (2.9.0), and quantization which is performed using C++ source-to-source transformations using the Artisan framework~\cite{fccm20_artisan}. We also wrapped two existing tools as $\lambda$-blocks: HLS4ML (v0.6.0) which translates a DL model into HLS C++ model, and Vivado HLS (v20.1) which translates an HLS C++ model into an RTL model. 

\end{itemize}

Listing.~\ref{code:strategy} presents the Python code which implements the pruning strategy illustrated in Fig.~\ref{fig:pruning_scaling_strategy}(a) and detailed in Section~\ref{sec:pruning}. It has three sections: (1)~ lines~5--10 build the design-flow architecture graph by connecting the corresponding blocks. We begin by creating a context using the $with$ operator which creates a \texttt{Dataflow} object that holds all the pipe blocks instances and their connections. The $>>$ and $<<$ operators specify the connections between block instances, allowing individual blocks or a list of blocks to be used at either side of the operators. Each block instance has a default name as an identifier, however it can be overridden by passing it as an argument (line~7). The end-result is a cyclic-directed graph; (2)~lines 13--28, define the configuration object which contains all the parameters that we use to configure all blocks. Parameters referenced as \texttt{BlockType::parameter} are passed to every block of that block type (line~14), while parameter names with format \texttt{InstanceName@parameter} are passed to a specific instance (line~19). All remaining parameters are global (lines~24--26) and thus can be accessed by any block; finally line~31 executes the design-flow by passing the configuration object (\texttt{cfg}).

% put the list and table on differnt pages will save some space
\input{code/designflow01.tex}


The design-flow execution starts with graph validation: it must contain at least one source block (a block without inputs) and it must not violate any multiplicity constraints. Once validation is completed, we proceed to execute each block from the source until reaching a Stop block. Block execution relies on an internal scheduler which implements a thread (worker) pool. When a job is submitted to execute a block, the scheduler assigns it to an available worker or stalls until a new worker is free. 

Once the architecture graph is validated, we proceed to generate an initial meta-model composed by the supplied configuration (line~31) along with an empty log and empty model space. We then instruct the scheduler to run the source block with the initial meta-model. When a block finishes its execution, it submits a job to run the next set of blocks defined by its output streams and control function. The Stop block will instruct the scheduler to terminate execution once all submitted jobs are completed, and invoke the user-supplied function (line~27) to return arbitrary data back as the output of the \texttt{run} method (line~31). 

It can be seen that new optimization strategies can be attained by revising the design-flow architecture (lines~5--10) as well updating its configuration parameters (lines~13--28). In the next section, we outline different design-flow architectures and evaluate their effectiveness.


\section{Evaluation}\label{sec:evaluation}

This section presents the evaluation results of various strategies using our framework. We wish to illustrate how different optimization strategies can be realized by revising design-flow architectures, for instance combining and reusing existing pipe blocks, as well as modifying their configuration. In this context, we introduce two separate strategies, based on the PRUNING and SCALING $\lambda$-blocks in~\secref{sec:pruning} and~\ref{sec:scaling}, respectively. These two $\lambda$-blocks, as previously mentioned, are implemented using the Keras API. We then develop a strategy that combines both optimizations as part of a single strategy in~\secref{sec:scaling_pruning}. Next, we introduce a strategy that employs the QUANTIZATION $\lambda$-block, which operates at HLS C++ level and is presented in~\secref{sec:quant}. This is followed by \secref{sec:all_three} which presents a software-hardware co-optimization strategy that combines scaling, pruning and quantization. We conclude this section with a discussion in~\secref{sec:discussion}. 

\subsection{Experimental setup}

We conduct our experiments using Python $3.9.15$. For our study, we choose our benchmark workloads from a few typical DNN applications which are listed in~\tabref{table:benchmark}. 
%including jet identification in particle physics, and Image classification. 
Some common datasets for image classification, such as MNIST~\cite{lecun1998gradient}, SVHN~\cite{netzer2011reading}, are used. We also include a dataset of jet high-level features (HLFs)~\cite{duarte2018fast, moreno2020jedi} for particle physics. This task focuses on the FPGA-based triggers of the CERN LHC (Large Hadron Collider) experiments, which handle a input data rate of 40 MHz and has a response latency constraint of less than 1$\mu$s. 
%Some typical models are used, such as VGG7
The default frequency of the designs on Zynq 7020 is 100MHz and it is 200MHz for the designs using XCU250 (Alveo U250) and VU9P. The HLS4ML block is configured with 18-bit fixed-point precision as its default, including 8 integer bits. 

%The MNIST dataset is the most common dataset across various work. 

\input{tables/benchmarks.tex} 

%\subsection{One strategy to multiple applications}

%\subsection{The strategy of pruning}

\subsection{The pruning strategy}\label{sec:pruning}

Pruning is a technique used to reduce the number of parameters in a neural network by removing weights that do not significantly affect the performance of the model. This can improve the performance and efficiency of neural networks deployed on FPGAs by reducing compute requirements. The proposed strategy gradually sets parameters close to zero during the training process until a certain percentage of parameters are zero. This is done by identifying and removing the least important weights in a network, while preserving its accuracy. The end result is typically a smaller, faster, and more efficient neural network.

	
\begin{equation}
\begin{aligned}
& \underset{x}{\text{maximum}}
& & Pruning\_rate \\
& \text{subject to}
& & Acc\_loss(x) \leq \alpha_p 
\end{aligned}
\end{equation}


%pruning DSE, draw a figure to show the model accuracy with different pruning rate from 50\% to 99\%. 5\% each point
%\newline
The design-flow of the pruning strategy is shown in~\figref{fig:pruning_scaling_strategy}(a). 
The proposed pruning strategy block implements a binary search algorithm which can automatically find the optimal pruning rate within a given threshold. 
%This optimal rate along with the corresponding trained model are returned by this block. 
For this experiment, the maximum tolerant accuracy loss, $\alpha_{p}$, is set to 2\%. The search starts at the pruning rate of 0\% and gets the initial model accuracy as $Acc_{p0}$. This is at step 1 (s1). Then the search will increase the pruning rate if the model accuracy using the pruning rate at current step is $\geq (Acc_{p0} - \alpha_{p})$, otherwise the search will decrease the rate. This is based on the assumption that model accuracy decreases as the pruning rate increases.
The search ends when the difference between the next pruning rate and the current rate is less than a user defined pruning rate threshold, defined as $\beta_{p}$. The $\beta_{p}$ is set to 2\% in the examples. 
The number of steps is determined by the $\beta_{p}$ and it is $1+log_{2}(1/\beta_{p})$, resulting in 6 steps when $\beta_{p}$ is 2\%. 
The developer can choose to store only the optimal model, or all the searched design candidates in the resulting meta-model, marking the optimal one. 
The search steps and direction of the algorithm on the four benchmarks are illustrated in~\figref{fig:auto_pruning}. 

% (a)
% https://docs.google.com/drawings/d/18bGih9gp5AbckjTamtKngySg2zKSJo-OyLSa6_S5YiM/edit?usp=share_link

% (b)
% https://docs.google.com/drawings/d/1vQvr6OXt8jQkADP7_Tdq2seFxSiXkN0nRG5wN9Zbgo0/edit?usp=share_link
\begin{figure} 
   \centering
   \hspace*{\fill}
  \subfloat[]{%
    \includegraphics[width=0.33\linewidth]{img/pruning_strategy03.pdf}}
   \hspace*{\fill}
   \subfloat[]{%
    \includegraphics[width=0.33\linewidth]{img/scaling_strategy02.pdf}}
  \hspace*{\fill}
  \vspace{0.2cm}
  \caption{(a) The pruning strategy. (b) The scaling strategy. }
  \label{fig:pruning_scaling_strategy} 
\end{figure}



% a/b/c/d
% https://docs.google.com/drawings/d/1LITQ6iUaKzns4Ov-5o1hatKcYCzojA7SZ2455dSsOEo/edit?usp=share_link
% https://docs.google.com/drawings/d/1LITQ6iUaKzns4Ov-5o1hatKcYCzojA7SZ2455dSsOEo/edit?usp=share_link
% https://docs.google.com/drawings/d/1kcL-_9dCiM6kxITpDKcIwppiBt4_O1d2IdWRsQNmSCA/edit?usp=share_link
% https://docs.google.com/drawings/d/1zL_MBbEh3A08n6iYpvFzOMaSWnGH13OwgLMCnyLb1jo/edit?usp=share_link
\begin{figure} 
   \centering
  \subfloat[Jet-DNN\label{fig:lhc_dnn_pruning}]{%
    \includegraphics[width=0.6\linewidth]{img/lhc_dnn_pruning05.pdf}}
 \\
  % \hspace*{\fill}
  \subfloat[Jet-CNN\label{fig:lhc_cnn_pruning}]{%
    \includegraphics[width=0.6\linewidth]{img/lhc_cnn_pruning01.pdf}} 
  \\
  \subfloat[VGG7\label{fig:vgg7_pruning}]{%
    \includegraphics[width=0.6\linewidth]{img/lhc_dnn_pruning01.pdf}}
    \\
  \subfloat[ResNet9\label{fig:resnet8_pruning}]{%
    \includegraphics[width=0.6\linewidth]{img/resnet8_pruning01.pdf}}
  \vspace{0.2cm}
  \caption{ Applying the auto pruning strategy on various models. The arrows show the direction of the binary search algorithm. The first step of the search (s1) is omitting in the figures to make the interested parts more visible. The blue arrow points to a step that has an accuracy loss larger than the user defined threshold.  }
  \label{fig:auto_pruning} 
\end{figure}





To demonstrate the effectiveness of our auto pruning strategy, we show two examples with detailed results for each step in~\figref{fig:auto_pruning_results}. \figref{fig:auto_pruning_results}(a) and (c) show the pruning rate and the corresponding model accuracy in each step for Jet-DNN on Zynq 7020 and ResNet9 on U250, respectively.
In addition, \figref{fig:auto_pruning_results}(b) and (d) show the corresponding hardware resource utilization. As the pruning rate increases, the hardware resource requirements drop dramatically, particularly the DSPs and the LUTs which are the most important resources on FPGAs. In particular, as shown in~\figref{fig:auto_pruning_results}(b), the best DSP utilization is 41.4\% at step 6. However, the model accuracy drops significantly at this step with a pruning rate of 96.9\%. Therefore, the design candidate at step 5 is selected, which has a DSP utilization of 100.5\%. In our design-flow architecture, the hardware information is fed back to the top to fine tune the strategy parameters, more specifically the $\alpha_p$ and $\beta_p$, as indicated by the up arrow in~\figref{fig:pruning_scaling_strategy}(a). 

% With our framework, it is easily to build the design-flow supporting other pruning strategies with a different control, such as an exhaustive search. This work introduce an auto pruning strategy based on a binary search which can quickly achieve the optimal pruning rate with a given threshold. 
%Generally, the users may be more interested in the optimal pruning rate which may be inefficient to be found out by using DSE (another word here?). 

% Alternatively, to achieve better results, more strategies can be applied together. Details are described below. 



% a, b, c, d
% https://docs.google.com/drawings/d/1rkwCyGlR5qiEnzfB8YlFk6hKANpXDrKeTCRJsk3e-yg/edit?usp=share_link
% https://docs.google.com/drawings/d/1rkwCyGlR5qiEnzfB8YlFk6hKANpXDrKeTCRJsk3e-yg/edit?usp=share_link
% https://docs.google.com/drawings/d/1hbHz_j2x6jLrtz5ECg86enztNUXHWzMLYJ6lw2wJAmo/edit?usp=share_link
% https://docs.google.com/drawings/d/1H3blmXPUWjUpdM-qG30GzMylUt5R-durNwAkh70nDZM/edit?usp=share_link

\begin{figure} 
   \centering
  \subfloat[]{%
    \includegraphics[width=0.48\linewidth]{img/dnn_pruning02.pdf}} 
   \hspace*{\fill}
  \subfloat[]{%
    \includegraphics[width=0.48\linewidth]{img/dnn_pruning_resource02.pdf}}
  \\
  \subfloat[]{%
    \includegraphics[width=0.48\linewidth]{img/resnet9_pruning_acc01.pdf}} 
   \hspace*{\fill}
  \subfloat[]{%
    \includegraphics[width=0.48\linewidth]{img/ResNet9_pruning_resource01.pdf}}
    
  \caption{(a) Various search pruning rates and the corresponding model accuracy for Jet-DNN. (b) Resource utilization of various design candidates of Jet-DNN on Xilinx Zynq 7020. (c) Various search pruning rate and the corresponding model accuracy for ResNet9. (d) Resource utilization of various design candidates of ResNet9 on Xilinx U250.  }
  \label{fig:auto_pruning_results} 
\end{figure}


\subsection{The scaling strategy}\label{sec:scaling}
When the design is too large to fit a given FPGA, a common approach is to reduce the size of the DL model, which is the purpose of the scaling strategy as shown in~\figref{fig:pruning_scaling_strategy}(b). 
The SCALING $\lambda$-block reduces the layer size by a factor in each step, and checks if the loss of model accuracy exceeds a threshold defined as $\alpha_s$. If it does, the search stops and the meta-model, including the final design candidate, is returned. 
The $\alpha_s$ could be set to a very small value to allow for the reduction of model size with little or no loss of model accuracy. If the optimized model is still too large to fit on the targeted FPGA after the RTL synthesis, the $\alpha_s$ can be increased to enable a further reduction of model size via the feedback on the left side of ~\figref{fig:pruning_scaling_strategy}(b). 

%The second mode of this strategy is to set a maximum size for each layer and then search for the optimal model with smaller layer sizes using AutoKeras and Bayesian optimization. The further reduction is done by reducing the maximum size of each layer.  

% a, b
% https://docs.google.com/drawings/d/1hjuimYuCn87CxMLBQBk673muBAwLyuJwNtv6hRQMnrM/edit?usp=share_link
% https://docs.google.com/drawings/d/17vaa8QMQou1dsrr3BlqGdGDLVWdP6r1nrRhtpHCpfD4/edit?usp=share_link
\begin{figure} 
   \centering
  \subfloat[]{%
    \includegraphics[width=0.48\linewidth]{img/dnn_scale_acc01.pdf}} 
   \hspace*{\fill}
  \subfloat[]{%
    \includegraphics[width=0.48\linewidth]{img/dnn_scale_resource01.pdf}}
    
  \caption{(a) Various search steps and the corresponding model accuracy for Jet-DNN using only the scaling strategy. (b) Resource utilization of various design candidates of Jet-DNN on Xilinx Zynq 7020 using only the scaling strategy.  }
  \label{fig:scaling_results} 
\end{figure}


We apply our scaling strategy on the Jet-DNN model used in the jet tagging in~\cite{duarte2018fast, coelho2021automatic}. We also show the details of each search step in~\figref{fig:scaling_results} for a better illustration. \figref{fig:scaling_results}(a) shows how the accuracy of the Jet-DNN model changes as the search progresses through different steps. \figref{fig:scaling_results}(b) shows how the utilization of hardware resources changes as different design candidates are considered, with the goal of fitting the final design onto the targeted Xilinx Zynq 7020 FPGA.
In this design-flow, the search will stop depending on the value of $\alpha_s$ or when the optimized design can fit on the targeted FPGA as determined by the feedback from hardware synthesis results, as shown in~\figref{fig:pruning_scaling_strategy}(b). \figref{fig:scaling_results}(b) shows that on step 8, the utilization of all four types of hardware resources on FPGAs goes below 100\%, and thus selected as the candidate design.

% a , b
% https://docs.google.com/drawings/d/18IvNg_m8g26jrUlP4rV0Kf49qkzGqeE2-J6obaFKoZg/edit?usp=share_link
% https://docs.google.com/drawings/d/1OzVdpudJp_QE9wQcqzU3FywTzzfq10d6M1p6yon7SsU/edit?usp=sharing
\begin{figure} 
   \centering
   \hspace*{\fill}
  \subfloat[]{%
    \includegraphics[width=0.30\linewidth]{img/scale_pruning_strategy03.pdf}}
  \hspace*{\fill}
  \subfloat[]{%
    \includegraphics[width=0.40\linewidth]{img/pruning_scaling_strategy03.pdf}}
  \hspace*{\fill}
  \vspace{0.2cm}
  \caption{(a) The combined strategy of scaling and pruning. (b) Another combined strategy of scaling and pruning, exploring the order of $\lambda-$blocks  }
  \label{fig:scaling_pruning_strategy} 
\end{figure}

\subsection{Combining scaling and pruning}\label{sec:scaling_pruning}
% The design flow framework outline in~\secref{sec:approach} facilitates the combination of multiple optimization algorithms into a single automated process. As both scaling and pruning strategies presented above primarily aim to reduce the hardware resources required to deploy a neural network on an FPGA, combining them could lead to even greater resource savings than either strategy alone.

Can we combine the effects of scaling and pruning? 
With the proposed framework, it is easy to cascade different optimization blocks by simply revising the graph architecture. \figref{fig:scaling_pruning_strategy}(a) shows the extended strategy in which we connect the scaling block with the pruning block along with the remaining parts of the architecture.
In this case, our new combined strategy performs an accuracy lossless scaling step first, followed by auto pruning. The value of $\alpha_s$ is set to 0.005, so that the model accuracy can be maintained while the model size is reduced as much as possible. 


% a, b
% https://docs.google.com/drawings/d/1pvUmqb5l-Ufo6vRvLBpOA-Amg9LlYXwP6Qh-oR7evnk/edit?usp=share_link
% https://docs.google.com/drawings/d/1JCIkY_xKqzHJ0FBOBZxsG8owN2pe1gZKNODX4c9CNXI/edit?usp=share_link
% c, d
% https://docs.google.com/drawings/d/1UgHHzk3CtQuo7fYh7wT4YCW6tTsQs1bVKZZdwdeAxac/edit?usp=share_link
% https://docs.google.com/drawings/d/1m71oPluaW2QvfEW9p86NCQYeUGtqOruQvr3MKIKz8KU/edit?usp=share_link
% https://docs.google.com/drawings/d/1HnzOJKvK9zPeJKcAXblqph1YOIYqEfRjD5Pa835HAR8/edit?usp=sharing
\begin{figure} 
   \centering
  \subfloat[]{%
    \includegraphics[width=0.48\linewidth]{img/dnn_pruning_scale_acc01.pdf}} 
   \hspace*{\fill}
   %\\
  \subfloat[]{%
    \includegraphics[width=0.48\linewidth]{img/dnn_pruning_scale_resource01.pdf}}
  \\
   \subfloat[]{%
    \includegraphics[width=0.48\linewidth]{img/dnn_scale_after_pruning_acc01.pdf}} 
   \hspace*{\fill}
  \subfloat[]{%
    \includegraphics[width=0.48\linewidth]{img/dnn_scale_after_pruning_resource01.pdf}}
  \\
  \subfloat[]{%
    \includegraphics[width=0.80\linewidth]{img/scaling_pruning_pareto02.pdf}}
  \caption{(a) Model accuracy and pruning rate for the Jet-DNN with scaling followed by pruning. (b) Resource utilization of each design in (a). (c) Model accuracy and layer size for the Jet-DNN with pruning followed by scaling. (d) Resource utilization of each design in (c). (e) Designs on the Pareto Front in the meta-model after REDUCE block with designs from different path labeling in different colors. }
  \label{fig:scaling_pruning_results} 
\end{figure}


The new optimal pruning rate becomes 84.4\%, as shown in~\figref{fig:scaling_pruning_results}(a), which is smaller than the previous optimal pruning rate of 93.8\% as shown in~\figref{fig:auto_pruning_results}(a). It is due to the reduction of model redundancy by the preceding pipe block of scaling strategy. However, the scaling strategy alone is not able to remove all model redundancy and the following pruning strategy is able to further remove redundancy. 


%is shown in \figref{fig:scaling_pruning_results}(b)
%pruning is performed prior to scaling resulting in a different optimization. 

A variant of our strategy can perform pruning prior to scaling by flipping the order of the corresponding $\lambda$-blocks, resulting in a different optimization. 
%This involves flipping the order of the $\lambda-$ blocks. 
In this case, the accuracy drops by 0.7\% just after one step in the scaling, as shown in~\figref{fig:scaling_pruning_results} (c).
We can also use the FORK block to support an optimization pattern where different strategy paths (SCALING $\rightarrow$ PRUNING and PRUNING $\rightarrow$ SCALING) are executed in parallel, as shown in~\figref{fig:scaling_pruning_strategy}(b), and the best design (or designs) are chosen in a REDUCE block.  Moreover, we can do a Pareto analysis on the designs that resulted from both strategies, and select those on the Pareto Front which provide the best trade-off in terms of accuracy and resource utilization, as shown in~ \figref{fig:scaling_pruning_results} (e). 

%The returned meta-model can contain  all the designs on the Pareto Front from both path, as shown in~ \figref{fig:scaling_pruning_results} (e). 
% To achieve a better design, a single strategy is often not sufficient and multiple optimization strategies should be used together. One of the advantages of the proposed framework in this work is that each strategy is modularized, allowing for easy combination with other strategies.

% And design-flows with different orders of these blocks create different strategies that may achieve good results.   
%Since the two strategies block are modularized, the order is 

%And in the new search of pruning strategy, a pruning rate of 87.5\% leads to around 15\% accuracy loss.

%The hardware resource utilization also reduces a lot, as shown in~\figref{fig:scaling_pruning_results}(b). Now the optimal design consumes only 60\% of DSP blocks on Zynq 7020, while the optimal design with only pruning strategy consumes 100.5\% DSPs. 



\subsection{The quantization strategy}\label{sec:quant}

% a, b
% https://drive.google.com/file/d/1QsbyI5l4efaF4utM_DDtantkJXLsEaKQ/view?usp=share_link
% https://drive.google.com/file/d/1kQchKeoSxlyghMjO4mzsXBJr-h9BXoiC/view?usp=share_link
\begin{figure} 
   \centering
  \subfloat[Jet-DNN]{%
    \includegraphics[width=0.60\linewidth]{img/bit_width_visualization_dnn64s0.png}} 
   %\hspace*{\fill}
   \\
  \subfloat[VGG7]{%
    \includegraphics[width=0.9\linewidth]{img/bit_width_visualization_vgg7s0.png}}
\vspace{0.2cm}    
  \caption{The quantized bitwidth of the weight, bias and output data of each layer in (a) Jet-DNN model and (b) VGG7 model. Only the layers with weights are quantized.}
  \label{fig:quantization_results} 
\end{figure}

\input{tables/quant_results02.tex} 

As previously mentioned, the QUANTIZATION $\lambda$-block operates at the HLS C++ level, changing the fixed-point integer numerical representations directly at source-level. At this level, there is more direct control over hardware optimizations, resulting in fewer unintended side-effects than making changes at a higher level. 
% In addition, low level pipe blocks developed by hardware optimization experts could be combined with high level pipe blocks created by machine learning experts to achieve better results through software-hardware co-optimization using our framework. 

%As GPUs (or other AI-targeted accelerators) dominate the space of AI-computation, and these devices typically require uniform precision across most computations in order to efficiently utilize their hardware, most state-of-the-art quantization schemes aim to reduce the bit-widths of all parameters and operations to some uniform precision. The vast customizability of FPGAs, however, allow them to ignore these restrictions on uniformity of precision. 
The QUANTIZATION $\lambda$-block performs mixed-precision quantization for neural networks, allowing separate precisions for the weights, biases and results of each layer of a model, taking the advantages of the customizability of FPGAs. While this customizability creates a massive search space of quantization configurations, dependencies within the network can be used to reduce the degrees of freedom of the design space. If the result type of a convolutional layer has 8 bits, for instance, there is no point in a directly subsequent max-pooling layer having 16 bits. Conversely, if the max-pooling layer were to have a bit-width of 4, an equivalently accurate and more efficient design would set the bit-width of the results of the preceding convolutional layer to be 4 as well. This strategy pipe block automatically compacts a method into a series of virtual layers using these dependencies. These virtual layers are constructed by grouping weights-layers, such as convolutional or linear layers, together with batch-normalization, pooling, and activation functions on their outputs.

% a, b, c
% https://docs.google.com/drawings/d/17TgOMyzpR0I5i1GC0pCZ14gFzK_GkJr5QO5vevGGQNs/edit?usp=share_link
% https://docs.google.com/drawings/d/1gWd4gub2PRIRKTihCJ_In_TeIJOwtp9BJAJf5DYUZGk/edit?usp=share_link
% https://docs.google.com/drawings/d/14dyqPtTo8ou2syLEYOgjxXZWjFmFmEc3rgD8vbjBsWg/edit?usp=share_link

\begin{figure} 
   \centering
\hspace*{\fill}
   \subfloat[\label{fig:quant_strategy}]{%
    \includegraphics[width=0.30\linewidth]{img/quant_strategy02.pdf}}
  \hspace*{\fill}
  \subfloat[]{%
    \includegraphics[width=0.30\linewidth]{img/scaling_pruning_quant_strategy02.pdf}} 
  \hspace*{\fill}
  \subfloat[]{%
    \includegraphics[width=0.30\linewidth]{img/scaling_pruning_quant_strategy03.pdf}} 
   \hspace*{\fill}
  \caption{ (a) The quantization strategy. (b) The combined strategy of scaling, pruning, quantization. (c) Another combined strategy of scaling, pruning, quantization.  }
  \label{fig:combined_strategy} 
\end{figure}


% https://docs.google.com/drawings/d/1aZOOKmluX_QTShldzXHURcg9EzyHuzBmutCnHVTKeyY/edit?usp=share_link
\begin{figure}%[tp]
   \centering
    \includegraphics[width=0.9\linewidth]{img/dnn_all_resource03.pdf} 
  \caption{ The hardware resources and the latency of the Jet-DNN designs after different strategies on a Zynq 7020 FPGA. The clock period is 10 ns.  }
  \label{fig:dnn_all_resource} 
\end{figure}

% https://docs.google.com/drawings/d/1vFn_MRg9-6zVi1nuA0fjESslmnl2LJndPLGPp02FfZE/edit?usp=share_link
\begin{figure}%[tp]
   \centering
    \includegraphics[width=0.9\linewidth]{img/vgg7_all_resource02.pdf} 
  \caption{ The hardware resources and the latency of the VGG7 designs after different strategies using a U250 FPGA. The clock period is 5 ns.  }
  \label{fig:vgg7_all_resource} 
\end{figure}




\figref{fig:combined_strategy}(a) shows the design flow of the quantization strategy. After the quantization block accepts the meta-model with a maximum tolerant accuracy loss defined as $\alpha_q$, it works to gradually reduce the bit-widths of weights, bias and layer output. The resulting configuration of precision is applied to the C++ kernel and a runtime simulation is used to evaluate the accuracy of the quantized model over the validation dataset. If the accuracy loss is within the user-specified tolerance ($< \alpha_q$), this stage is repeated. Details can be found in our github. 


\figref{fig:quantization_results}(a) and (b) show the precision of the weights, biases and output of each virtual layer of the Jet-DNN and VGG7 models respectively after being modified by the quantization strategy with the "max accuracy loss" parameter, $\alpha_q$, set to 1\%. \tabref{table:results_only_quant} shows how the quantization affected key evaluation metrics relating to the performance and resource usage of the FPGA design. The strategy reduces the DSP usage of a VGG7 model by a factor of 4.9. In addition, the strategy reduces the DSP usage of a Jet-DNN model by a factor of 8.5 whilst decreasing the LUT usage by a factor of 1.2, showing the effectiveness of the strategy. 


\subsection{Combining quantization, scaling and pruning}\label{sec:all_three}
We now combine our three optimization techniques, pruning, scaling and quantization as part of a single automated cross-stage strategy to enhance performance and hardware efficiency, as illustrated in \figref{fig:combined_strategy}(b). All the three techniques aim to increase hardware efficiency of the neural networks deployed on FPGAs while minimizing any loss in model accuracy. By combining different methods for model compression, we may encounter improved hardware efficiency than using any single strategy alone. 

The final optimized model after scaling, pruning and quantization is depicted as "S+P+Q" design in~\figref{fig:dnn_all_resource}, showing a reduction of the DSP usage by around 92\% and LUT usage by around 89\% compared with the original design (baseline~\cite{duarte2018fast}). In addition, the latency is also reduced by 35\%. 
%It is also worth noting that the DSP usage of the "S+P+Q" design is not the smallest among all the designs. Both "S+P" and "P+Q" designs have smaller DSP usage but their LUT usage is much higher than the "S+P+Q". These changes suggest that as the precision of operands in certain layers were reduced below some threshold, the Vivado HLS tool moves the implementation of the multiplications in those layers from DSP blocks to LUTs. 
The same optimization strategy is also applied to the VGG7 network. \figref{fig:vgg7_all_resource} shows that when compared to the baseline, the optimal design after the combined strategy of scaling, pruning and quantization results in a decrease of DSP usage by around 96\% and LUT usage by around 78\%, demonstrating the effectiveness of the strategy. It also shows that with our proposed framework, these optimization blocks can be reused for a variety of DL models without any modifications, resulting in good reusability.  

\figref{fig:combined_strategy}(c) shows another combination of these blocks with the $\kappa$-block ``join" just before the quantization block. When the design is too large to fit on an FPGA, the design flow can feed back to low level HLS domain strategies to decrease the time requirred to generate a solution, or the design flow can feed back to TensorFlow/Keras domain space for a more fine-grained optimization at the cost of more optimization time. 

By combining different methods to optimize acceleration, we increase the solution space and the chances to find more designs that can offer  better trade-offs between model accuracy and hardware efficiency. This can be specially beneficial when deploying deep learning models on resource-constrained devices such as FPGAs.
%\input{tables/results_all_strategy.tex}

\input{tables/cmp_fpga02.tex}


\subsection{Discussion}\label{sec:discussion}

% This section evaluates three $\lambda-$blocks, namely pruning, scaling and quantization, and their combination to optimize different concerns. Although the proposed  blocks are about reducing computation complexity and hardware resource utilisation while maintaining model accuracy and increasing hardware efficiency, other strategies are possible, such as computation folding that conducts the trade-off between hardware performance (latency) and hardware resources. With the proposed framework, it is easy to reuse the existing blocks and combine them with new strategy blocks to explore complex scope optimization.  

%Combining multiple optimization strategies can potentially lead to better hardware efficiency and reduced redundancy of model deployment on FPGAs. However, it is important to note that the effectiveness of a combination of strategies will depend on the specific optimization architecture being used. It is the motivation of our framework that help to explore various optimization techniques for neural network acceleration on FPGAs. 

To demonstrate the advantages of our proposed framework, we compare the results obtained using our design flow framework to those of original DNN~\cite{duarte2018fast} using HLS4ML, LogicNets~\cite{umuroglu2020logicnets} , QKeras-based design Q6~\cite{coelho2021automatic} and AutoQKeras-based designs QE and QB~\cite{coelho2021automatic} in~\tabref{table:cmp_fpga}. All of these studies focus on extreme low-latency, low-resource, fully unfolded (initiation interval = 1) FPGA implementations. We show the model accuracy, latency and hardware resource utilization in the table. All these designs use the same architecture as the Jet-DNN in this work, except the JSC-L which uses a larger architecture. For JSC-M and JSC-L, the accuracy is 70.6\% and 71.8\%, over 3.8\% lower than our design ``S+P+Q" with $\alpha_q$ as 0.01. Furthermore, our design uses 1.9 times fewer LUTs than JSC-L. No DSP are used in JSC-M and JSC-L while 50 DSPs are used in our design.
When compared to the Q6 designs optimized by Qkeras, the accuracy of the design optimized by the proposed combined strategy is 0.8\% higher. In addition, our design consumes 2.5 times fewer DSP blocks and 5.7 times fewer LUTs than Q6. Compared to other two designs QE and QB, our design has over 3.3\% higher accuracy. In addition, our design consumes less resources of DSP, LUT and FF than both QE and QB. 

With a clock frequency of 200MHz (5ns), the latency of our design with $\alpha_q$ as 0.01 is 45ns (9 cycles), reduced to 20ns (4 cycles) without the final softmax layer like the LogicNets designs~\cite{umuroglu2020logicnets}. When the same clock frequency of 384MHz is used, the latency of this design without softmax is 10.4ns which is lower than the LogicNets JSC-L~\cite{umuroglu2020logicnets}. When compared with the designs QB, QE and QB~\cite{coelho2021automatic} optimized by the Qkeras and AutoQkeras, our design has lower latency than all of them, showing the benefits of our framework. 

Inspired by the QKeras design QB which minimizes the model bit consumption, our design can be further reduced by tuning the strategy parameters, such as $\alpha_q$. When the maximum tolerant accuracy loss ($\alpha_q$) of the quantization strategy increases from 0.01 to 0.04, the strategy can further reduce the model size. The last row of~\tabref{table:cmp_fpga} shows the results with $\alpha_q$ as 0.04. The DSP usage drops to 23 that is 3 times lower than the most resource-efficient model, QE, using AutoQKeras. It also has a reduction in latency from 45ns to 40ns. The model accuracy drops to 72.8\% but it is still higher than the optimized AutoQKeras design QB and QE. For the power consumption, compared with the baseline design, the final optimized design achieves 12.6$\sim$15.1 times lower power consumption. 

Please note that the quantization-aware training (QAT) technique used in the QKeras design Q6, QE, QB can be complementary to our framework, such as working as a QAT optimization $\lambda$-block and combining with other existing optimization blocks, to explore new optimization opportunities. 






