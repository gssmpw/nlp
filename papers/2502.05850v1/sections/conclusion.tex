\section{Conclusion}
\label{sec:conclusion}

This paper presents a novel co-optimization framework for FPGA-based DNN accelerators, which comprises building blocks that facilitate rapid development of customized cross-stage design flows, automating the entire design iteration process. The results demonstrate that our approach significantly reduces DSP resource usage by up to 92\% and LUT usage by up to 89\%, while maintaining accuracy, without requiring human effort or domain expertise. In addition, our results reveal that Bayesian optimization in DSE significantly streamlines the process, achieving results comparable to grid search but with a 15.6-fold reduction in processing time. Our futurework will extend this approach to include RTL space, support more DNN architectures, including Variational Autoencoder (VAE)~\cite{que2024low} and transformer~\cite{wojcicki2022accelerating} networks, and incorporate more optimization strategies like balancing initiation interval~\cite{que2021accelerating, rognlien2022hardware} for further improvements in hardware efficiency and model performance.

%exploring more efficient search techniques automatically for larger numbers of $O$-blocks and utilizing new FPGA resources such as AI Engines~\cite{xilinx_white} and AI Tensor Blocks~\cite{langhammer2021stratix}.


\vspace{0.2cm}
\noindent \textbf{Acknowledgement:} The support of the United Kingdom EPSRC (grant numbers EP/V028251/1, EP/L016796/1, EP/N031768/1, EP/P010040/1, EP/S030069/1, and EP/X036006/1), Intel, and AMD is gratefully acknowledged. We thank Markus Rognlien, Shuo Liu and Anyan Zhao for their help. 