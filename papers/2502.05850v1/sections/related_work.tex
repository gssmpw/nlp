


\section{Related Work}\label{sec:related_work}

The field of FPGA-based DNN acceleration has rapidly expanded, leading to the development of numerous optimization techniques and tools. 
% techniques are not customizable
Several co-optimization techniques have been proposed that optimize both algorithm and hardware stages for DNNs on FPGAs, as discussed in various papers~\cite{yang2019synetgy, zhang2022algorithm, hao2018deep, hao2019fpga, hao2019nais,  hao2020effective, jiang2020hardware, dong2021hao, hao2021enabling} and in hardware-aware neural architecture search studies like~\cite{ney2021half, abdelfattah2020best}. These optimization strategies are often coupled with design space exploration, but are typically hardcoded and cannot be easily changed or customized.

% tools are not designed to add new optimisation strategies
Other approaches offer end-to-end software frameworks, such as Xilinx's Vitis AI~\cite{kathail2020xilinx} and Intel's OpenVINO~\cite{OpenVINO2023},  which optimize DNNs with pre-built optimizations for deployment on specific target technologies. However, they are not designed for easy addition of new optimization strategies or to search for the most effective combined strategies.

% single stage (limited scope) e to describe strategies
Furthermore, some frameworks allow developers to describe and customize DNN optimization strategies, but their scope is limited. For instance, ScaleHLS~\cite{ye2022scalehls} and SOTA-OPT~\cite{agostini2022mlir} focus on HLS-based optimization strategies and hardware designs, but their optimization scope is restricted to MLIR. TVM~\cite{chen2018tvm} is a general-purpose DNN compilation framework that offers performance portability across different types of devices, but optimizations occur solely at the graph (IR) level.

% no automated cross-stage optimisations
Moreover, frameworks, such as FINN~\cite{umuroglu2017finn}, HLS4ML~\cite{duarte2018fast}, and fpgaConvNet~\cite{venieris2016fpgaconvnet}, provide optimized hardware building blocks for FPGA-based DNN accelerators, and allow optimization strategies to be codified using these blocks. However, they do not support automated bottom-up optimization flows, where hardware stage information guides the optimization of the application (DNN) stage.

To overcome the limitations of existing optimization techniques for DNNs and hardware, our framework derives fully automated high-level optimization flows backed by reusable target-specific and target-agnostic building blocks. Our approach enables automated top-down and bottom-up flows, allowing for the creation of customized cross-stage optimization strategies for various DNN designs. Moreover, our approach integrates new and existing optimization techniques targeting various levels of abstraction, such as neural networks through graph optimizations and HLS C++ through source-to-source optimizations~\cite{fccm20_artisan}.