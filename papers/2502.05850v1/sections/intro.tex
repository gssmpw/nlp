

% \section{Quant in HLS}
% I believe the main reason was that I found that the bit-width values in the HLS C++ project did not always completely correspond to what they had been specified as at the python level. If I remember correctly, the bias-precision, for instance, was not changed at the HLS-level regardless of what value it was assigned at the python-level.
% I also had a general idea that it is better to make modifications at a lower level, where I knew exactly what modifications were being made, than at a higher level where there could be unknown side-effects as a result of the changes made. Let me know if this does not make sense to you, and say hi to Gabriel for me :)



\section{Introduction}

%% quantisation: float => custom fixed int
%% quantization-aware training: better accuracy, quantisation is performed during training
%% post-training quantization: no training data, only weights (activations?) are quantised


The field of deep learning has witnessed unprecedented growth in recent years, driven by the increasing demand for efficient and high-performance applications~\cite{lecun2015deep}. Consequently, FPGA-based deep neural network (DNN) accelerator design and optimization have gained significant attention~\cite{zhang2015optimizing, zhang2020dnnexplorer}. The development of an efficient FPGA-based DNN design requires a diverse skill set that combines expertise in machine learning with low-level knowledge of the target hardware architecture~\cite{sze2017efficient}. Optimizing these DNN designs is a complex process, as it involves balancing competing objectives. On one hand, high accuracy during inference is crucial from an application perspective. On the other hand, the design must be optimized for the underlying hardware architecture, meeting power, latency and throughput requirements while fitting into the FPGA device~\cite{duarte2018fast,  coelho2021automatic}. Achieving an optimal balance between these conflicting objectives requires careful consideration and effective optimization strategies~\cite{wang2019deep}. 

Existing optimization techniques for DNNs and hardware have a limitation in that they are often handled in separate stages, with information exchanged only in a top-down manner from the application to hardware. Manual intervention is usually required for bottom-up information flow. This information exchange between multiple stages can significantly enhance the optimization process, as application and hardware optimizations can interact and affect each other. For instance, modifying the DNN model's architecture could impact the FPGA resource utilization, and optimizing the FPGA design could influence the DNN model's accuracy~\cite{wang2019deep}. In addition, there has been limited focus on combining optimization strategies targeting different abstraction levels, such as algorithmic level neural networks and High Level Synthesis (HLS) C++, hindering the selection of the most effective combination of optimization techniques for a given problem. Furthermore, little work focuses on supporting comprehensive exploration of such combination with various configurations of involved optimization techniques towards DNN accelerations. 
%As a result, there are two main challenges: 
%\textcolor{\mycolor}{no previous work explore the order of different optimization techiqnues, no previous framework supports comprehensive exploration of the combination of various optimiztion techiqiune torwards DNNs. This work can be applied to other hardware design, such ax xxxx. }

In this paper, we address these two aforementioned technical challenges: C1 Custom Co-Optimization Strategies and C2 Cross-Stage Optimization Search. 
%First, to develop efficient FPGA-based DNN accelerators, a coherent strategy~\cite{ney2021half, abdelfattah2020best, yang2019synetgy,zhang2022algorithm} is needed that not only combines optimization techniques covering software and hardware stages,  but also allows optimizations from one stage to inform and guide the other through an iterative design process. 
First, to navigate the vast design space of hardware-software exploration, this work employs a unique co-optimization approach encompassing three key aspects: (1) optimizing the DNN software model, (2) refining the DNN hardware architecture, and (3) exploring the design space. By encoding each optimization independently and combining them as needed, this approach enables the modular reuse and adaptation of optimizations across benchmarks and hardware platforms. Leveraging programmatic manipulation, we facilitate close coordination between DNN model and hardware design optimizations, significantly expanding the range of design choices. Advanced techniques such as Bayesian Optimization streamline the exploration of the large hardware-software design space, ensuring effective and systematic optimization~\cite{reagen2017case, mehrabi2020bayesian, tuli2023codebench}. 

Second, developing efficient DNN accelerators requires a coherent strategy that automates the selection, combination, and tuning of optimization techniques across multiple abstraction levels~\cite{ney2021half, abdelfattah2020best, yang2019synetgy,zhang2022algorithm}. It is challenging to identify the most effective combination, order and tuning of optimization techniques to ensure optimal outcomes~\cite{zhang2020dnnexplorer, xu2020autodnnchip}. This work tackles these challenges by employing a cross-stage optimization search approach that coordinates the interaction between different stages of the design flow. By leveraging a combination of top-down and bottom-up optimization strategies, the framework dynamically adapts to the specific requirements of the hardware and the application, guiding the optimization process iteratively and identifying the best optimization techniques and their sequences. 

This work utilizes a Bayesian guided nested-loop optimization structure involving both local and global phases, combining very distinct optimization techniques and incorporating feedback from the latter stages of the design flow to refine the process iteratively, as shown in Fig.~\ref{fig:approach}. 
The proposed cross-stage framework operates across various computational spaces, such as software and High-Level Synthesis (HLS), and can potentially target diverse hardware platforms including FPGAs and ASICs. It supports autonomous custom optimization tasks (e.g., OPT-1 to OPT-4 and potentially more), each constrained by user-defined parameters to optimize performance. 

% https://docs.google.com/drawings/d/1KTUYGxceGU3jnhVZ9JDpmgoUrA6gb1sM9yRlpYq2H40/edit?usp=sharing

\begin{figure}
\begin{center}
\includegraphics[width=1.00\linewidth]{img/approach03.pdf}
\end{center}
   \caption{The proposed approach.}
\label{fig:approach}
\end{figure}


We make the following contributions in this paper:
\begin{itemize}
\item A novel co-optimization framework for FPGA-based DNN accelerators. This framework seamlessly integrates programmatic DNN optimizations with HLS-based metaprogramming, coordinated by state-of-the-art Design Space Exploration (DSE) strategies. It supports automated design iteration processes through top-down and bottom-up flows, enabling more efficient optimization and accurate performance predictions by refining DSE estimates with post-HLS data (Section~\ref{sec:approach}).


\item A library of reusable optimization, transformation and control tasks designed to be customizable and flexible, and that can be easily integrated into our co-optimization framework. Some of the tasks in our library are specific to certain applications and/or target technologies, while others are agnostic, providing versatility and adaptability to the framework.
%(Section~\ref{sec:approach}-C).

\item Enabling rapid development of customized cross-stage design-flows. The framework introduces novel building tasks that automate the integration and iteration of various stages in the design flow. These components allow for the automation of both high-level and low-level design processes, streamlining the development of tailored solutions for different use cases and improving decision-making accuracy in the optimization process.


\item A comprehensive evaluation of the proposed framework using multiple benchmarks and different optimization strategies. This evaluation will provide insights into the effectiveness of the framework and its optimization modules under different scenarios (Section~\ref{sec:evaluation}).
\end{itemize}


\subsubsection*{Relationship to Prior Publications}

This paper expands on our previous studies~\cite{que2023metaml, que2024deep, que2024optimizing} which primarily focus on local optimizations. In~\cite{que2023metaml}, we propose MetaML, an optimization framework, that customizes design flow for DNNs with local optimizations. In~\cite{que2024deep}, we introduce control capabilities within the optimization framework, while \cite{que2023metaml} focuses on Quantization Heuristic Search (QHS) optimization task for compressing DNN accelerators. 
However, these prior works are limited in their ability to perform global optimization due to their focus on isolated optimization tasks. The inability to account for interdependencies between optimization tasks across stages mean that the overall design might not have achieved the optimal balance between accuracy, performance, and resource efficiency. This work addresses these limitations by incorporating bottom-up feedback and leveraging Bayesian optimization to enable global optimization across the entire design flow. By adjusting optimization tasks at each stage based on performance feedback, our framework ensures that each optimization choice contributes to a more cohesive and globally optimal solution. This extended approach maintains high accuracy while improving resource efficiency, specifically in resource-constrained environments. We plan to open-source MetaML-Pro to the research community, aiming to inspire further research and development in this area.

% . With this approach, each optimization module can be self-contained and independent of other modules, simplifying individual testing and verification. This can help minimize design errors, enhance overall system reliability, and facilitate the introduction of new optimization techniques;
 %  I think we should also highlight that we aim to automate this selection and permutation of these tasks to achieve efficient designs.
 % % There are little work about the automation of modularized optimization techniques to optimize FPGA-based DNN accelerators. 
 
% For the selection of the O-tasks, we could say that our work uses greedy algorithm to try all the possible permutation of the existing optimization tasks. But currently there are only two Python-space O-tasks that are scaling (S)  and pruning (P). And one HLS-space O-task which is quantization (Q). Since we have to perform HLS-space task after hls4ml, so Q should be after S and P. Thus, the total number of the permutation cases is 2. They are S->P->Q and P->S->Q. I guess this might be too small and I am still thinking if I can add one more HLS-space O-task, such as folding. 