 \begin{table*}[]
\centering
\caption{}
\label{table:pipeblocks}
\begin{tabular}{l|c|c|l l}
\hline
\rowcolor[HTML]{C0C0C0} 
\textbf{Type} &
  \textbf{Role} &
  \textbf{Multiplicity} &
\multicolumn{2}{c|}{\textbf{Parameters}}  
\\ \hline

JOIN            & $\kappa$  & many-to-1 &                                &           \\ \hline
BRANCH          & $\kappa$  & 1-to-2    & fn: meta-model $\rightarrow$ bool         &           \\ \hline
FORK            & $\kappa$  & 1-to-many &                                &           \\ \hline
REDUCE          & $\kappa$  & many-to-1 & fn: [meta-model] $\rightarrow$  meta-model &           \\ \hline
PRUNING &
  $\lambda$ &
  1-to-1 &
  \textbf{\begin{tabular}[c]{@{}l@{}}
        default\_pruning\_rate \\ 
        initial\_max\_accuracy\_loss ($\alpha_p$) \\ 
        pruning\_rate\_threshold ($\beta_b$)\\
        pruning\_auto \\
        train\_dataset\\
        test\_dataset \\
        train\_epochs \\
        \end{tabular}} 
  & \textbf{\begin{tabular}[c]{@{}l@{}}
        train\_batch\_size \\
        train\_learning\_rate \\
        train\_callbacks \\
        train\_LOSS \\
        train\_optimizer \\
        train\_metrics
        \end{tabular}}
        \\ \hline
SCALING         
  & $\lambda$ 
  & 1-to-1 & 
       \textbf{\begin{tabular}[c]{@{}l@{}}
        default\_scale\_factor \\ 
        initial\_max\_accuracy\_loss ($\alpha_s$)\\ 
        scale\_auto \\
        max\_trials\_num \\
        train\_dataset \\
        test\_dataset \\
        train\_epochs \\
        \end{tabular}} 
  & \textbf{\begin{tabular}[c]{@{}l@{}}
        train\_batch\_size \\
        train\_learning\_rate \\
        train\_callbacks \\
        train\_LOSS \\
        train\_optimizer \\
        train\_metrics
        \end{tabular}}
        

        \\ \hline

HLS4ML  
  & $\lambda$ 
  & 1-to-1 
  & \textbf{\begin{tabular}[c]{@{}l@{}}
        default\_precision \\ 
        IOType \\
        FPGA\_part\_number 
        \end{tabular}} 
  & \textbf{\begin{tabular}[c]{@{}l@{}}
        clock\_period \\
        test\_dataset \\
        \end{tabular}}
 \\ \hline

QUANTIZATION         
  & $\lambda$ 
  & 1-to-1 & 
       \textbf{\begin{tabular}[c]{@{}l@{}}
        initial\_max\_accuracy\_loss ($\alpha_q$)
        \end{tabular}} 

&       \textbf{\begin{tabular}[c]{@{}l@{}}
        test\_dataset 
        \end{tabular}} 
\\ \hline
        
VIVADO-HLS 
  & $\lambda$ 
  & 1-to-1 & 
       \textbf{\begin{tabular}[c]{@{}l@{}}
        none
        \end{tabular}} & \\ \hline
        
KERAS-MODEL-GEN 
  & $\lambda$ 
  & 0-to-1 & 
       \textbf{\begin{tabular}[c]{@{}l@{}}
        train\_en \\
        train\_dataset\\
        test\_dataset\\
        train\_epochs \\
        train\_batch\_size \\
        \end{tabular}} 
  &    \textbf{\begin{tabular}[c]{@{}l@{}}
        train\_learning\_rate \\
        train\_callbacks \\
        train\_LOSS \\
        train\_optimizer \\
        train\_metrics
        \end{tabular}} 
  \\ \hline

  
STOP &
  $\lambda$ &
  1-to-0 &
  fn: meta-model $\rightarrow$  output &
  \textbf{} \\ \hline
\end{tabular}
\end{table*}
