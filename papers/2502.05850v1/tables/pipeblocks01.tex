 \begin{table*}[]
\centering
\caption{}
\label{table:pipeblocks}
\begin{tabular}{l|c|c|l|c}
\hline
\rowcolor[HTML]{C0C0C0} 
\textbf{Type} &
  \textbf{Role} &
  \textbf{Multiplicity} &
  \textbf{Parameters} &
  \textbf{Description} \\ \hline
JOIN            & $\kappa$  & many-to-1 &                                &           \\ \hline
BRANCH          & $\kappa$  & 1-to-2    & fn: meta-model $\rightarrow$ bool         &           \\ \hline
FORK            & $\kappa$  & 1-to-many &                                &           \\ \hline
REDUCE          & $\kappa$  & many-to-1 & fn: [meta-model] $\rightarrow$  meta-model &           \\ \hline
PRUNING &
  $\lambda$ &
  1-to-1 &
  \textbf{\begin{tabular}[c]{@{}l@{}}
        default\_pruning\_rate \\ 
        initial\_max\_accuracy\_loss ($\alpha_p$) \\ 
        pruning\_rate\_threshold ($\beta_b$)\\
        pruning\_auto \\
        train\_dataset\\
        test\_dataset\\
        train\_epochs \\
        train\_batch\_size \\
        train\_learning\_rate \\
        train\_callbacks \\
        train\_LOSS \\
        train\_optimizer \\
        train\_metrics
        \end{tabular}} 
&    \textbf{\begin{tabular}[c]{@{}l@{}}
        When pruning\_auto is high, this block \\
        searches the optimal pruning rate by using \\
        a binary search algorithm. \\
        When pruning\_auto is low, this block \\
        returns a trained model pruned using the \\
        default pruning rate statically.
        \end{tabular}} 
        
        \\ \hline
SCALING         
  & $\lambda$ 
  & 1-to-1 & 
       \textbf{\begin{tabular}[c]{@{}l@{}}
        default\_scale\_factor \\ 
        initial\_max\_accuracy\_loss ($\alpha_s$)\\ 
        scale\_auto \\
        max\_trials\_num \\ 
        train\_dataset\\
        test\_dataset\\
        train\_epochs \\
        train\_batch\_size \\
        train\_learning\_rate \\
        train\_callbacks \\
        train\_LOSS \\
        train\_optimizer \\
        train\_metrics
        \end{tabular}} & \\ \hline

HLS4ML  
  & $\lambda$ 
  & 1-to-1 & 
       \textbf{\begin{tabular}[c]{@{}l@{}}
        param1: default\_precision \\ 
        param2: IOType \\
        param3: FPGA\_part\_number \\
        param4: clock\_period \\
        test\_dataset 
        \end{tabular}} & \\ \hline

QUANTIZATION         
  & $\lambda$ 
  & 1-to-1 & 
       \textbf{\begin{tabular}[c]{@{}l@{}}
        param1: initial\_max\_accuracy\_loss ($\alpha_q$)\\ 
        test\_dataset 
        \end{tabular}} & \\ \hline
        
VIVADO-HLS 
  & $\lambda$ 
  & 1-to-1 & 
       \textbf{\begin{tabular}[c]{@{}l@{}}
        none
        \end{tabular}} & \\ \hline
        
KERAS-MODEL-GEN 
  & $\lambda$ 
  & 0-to-1 & 
       \textbf{\begin{tabular}[c]{@{}l@{}}
        param1: train\_en \\
        train\_dataset\\
        test\_dataset\\
        train\_epochs \\
        train\_batch\_size \\
        train\_learning\_rate \\
        train\_callbacks \\
        train\_LOSS \\
        train\_optimizer \\
        train\_metrics
        \end{tabular}} 
  &    \textbf{\begin{tabular}[c]{@{}l@{}}
        Train\_en should be set when there\\
        is no Keras-based strategy block before \\
        HLS4ML block.
        \end{tabular}} 
  \\ \hline

  
STOP &
  $\lambda$ &
  1-to-0 &
  fn: meta-model $\rightarrow$  output &
  \textbf{} \\ \hline
\end{tabular}
\end{table*}
