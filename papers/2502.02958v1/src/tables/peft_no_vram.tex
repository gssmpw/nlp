\begin{table*}[t!]

\caption{Comparison of parameter-efficient fine tuning (PEFT) and knowledge editing methods (KEs) for fact updates. Categories include reparametrization (Reparam), Additive and Selective for PEFT, and meta-learning (ML-), memory-based (ME-) and locate-and-edit (LE-) for KEs. PEFT methods are designed to adapt the model to a (any) specific task, KEs explicitly for fact updates. Training indicates whether additional training is required (\cmark) or not (\xmark). $^*$LE-KEs do not require training, but locating relevant parameters. \#Instances is the number of required instances to modify a single fact, $\theta$ the fraction of parameters added ($+$) or modified ($\Delta$) in the original LLM, and the last column indicates computational overhead during Inference. 
\onone none, \omin minimal, \olow low, \omed moderate, \ohigh high.
}
\resizebox{\textwidth}{!}{%
\tiny
\centering
\begin{tabular}{@{}lllccrrc@{}}
\toprule
&  &  & \multicolumn{2}{c}{\textbf{Requirements}}  & \multicolumn{3}{c}{\textbf{Overhead}}   \\ \cmidrule(lr){4-5}\cmidrule{6-8}

& \multicolumn{1}{c}{\textbf{Category}} & \multicolumn{1}{c}{\textbf{Method}} & \multicolumn{1}{c}{\textbf{Training}}  & \multicolumn{1}{c}{\textbf{\#Instances}} & \multicolumn{1}{c}{\textbf{$\theta+$ (\%)}} & \multicolumn{1}{c}{\textbf{$\theta\Delta$ (\%)}} & \multicolumn{1}{c}{\textbf{Inference}}   \\ \midrule

\multirow{14}{*}{\rotatebox[origin=c]{90}{PEFT}}
& - & Full Fine-tuning & \textcolor{red}{\cmark} &  1000s+ & 0 & 100 & \onone \\
& Reparam & LoRA~\cite{hu2021lora} & \textcolor{red}{\cmark} &  100s+ & $\sim$1 & 0 & \olow \\
& Reparam & DyLoRA~\cite{valipour2022dylora} & \textcolor{red}{\cmark} &  100s+ & $\sim$1 & 0 & \olow \\
& Reparam & SoRA~\cite{SORA} & \textcolor{red}{\cmark} &  100s+ & $\sim$0.5 & 0 & \olow \\
& Reparam & DoRA~\cite{liudora} & \textcolor{red}{\cmark} &  100s+ & $\sim$1 & 0 & \olow \\
& Additive & Adapter~\cite{houlsby2019parameter} & \textcolor{red}{\cmark} & 100s+ & 3-8 & 0 & \omed \\
& Additive & MAM Adapter~\cite{14-unified-view-transfer-peft} & \textcolor{red}{\cmark} & 100s+ & 1-4 & 6.7 & \olow \\
& Additive & Soft Prompt~\cite{lester-etal-2021-power} & \textcolor{red}{\cmark} & 100s+ & $\leq$ 0.01 & 0 & \omin \\
& Additive & P-Tuning v2~\cite{47-P-tuning-v2} & \textcolor{red}{\cmark} & 100s+ & 0.1-0.5 & 0 & \olow \\

& Additive & CoDA~\cite{lei2023conditional} & \textcolor{red}{\cmark} & 100s+ & 0.4 & 0.1-5 & \olow \\
& Additive & Prefix Tuning~\cite{zhang2023towards} & \textcolor{red}{\cmark} & 100s+ & 0.1-0.5 & 0 & \olow \\
& Selective & LT-SFT~\cite{ansell2021composable} & \textcolor{red}{\cmark}  & 100s+ & 0 & 1-5 & \onone \\
& Selective & Diff-Pruning~\cite{guo-etal-2021-parameter} & \textcolor{red}{\cmark}  & 100s+ & 0 & $\sim$1 & \olow \\
& Selective & BitFit~\cite{1-BitFit} & \textcolor{red}{\cmark} & 100s+ & 0 & $\sim$0.01 & \onone \\

\midrule
\multirow{5}{*}{\rotatebox[origin=c]{90}{KEs}}
& ML-KE & MEND~\cite{mitchell2022fast} & \textcolor{red}{\cmark}  &  1 & 0 & $\leq$ 4 & \onone \\
& ML-KE & MALMEN~\cite{tan23malmen} & \textcolor{red}{\cmark} & 1 & 0 & $\leq$ 7 & \onone \\
& ME-KE & IKE~\cite{zheng-etal-2023-edit} & \textcolor{ForestGreen}{\xmark} & 33 & 0 & 0 & \omin \\ 
& LE-KE & ROME~\cite{meng-etal-2022-locating} &~~\textcolor{ForestGreen}{\xmark}$^*$ & 1 & 0 & $\leq$ 1 & \onone \\
& LE-KE & MEMIT~\cite{meng-etal-2022-memit} &~~\textcolor{ForestGreen}{\xmark}$^*$  & 1 & 0 & $\leq$ 3.4 & \onone \\
\bottomrule
\end{tabular}%
}



\label{tab:peft_comparison}
\end{table*}
