\input{src/tables/peft_no_vram}

The rapid scaling of LLMs has made traditional full-size fine-tuning prohibitively expensive, driving increased interest in efficient and lightweight methods for model updating and customization. Recent advances in parameter-efficient fine tuning (PEFT) techniques, such as LoRA~\cite{hu2022lora}, DoRA~\cite{liudora}, soft prompt tuning~\cite{lester-etal-2021-power,razdaibiedina-etal-2023-residual}, and adapters~\cite{pmlr-v97-houlsby19a}, have significantly reduced the computational costs of customizing LLMs. Notably, a concurrent line of work focuses on knowledge editing approaches that enable precise updates to discrete facts while avoiding extensive re-training~\citep{zhang2024comprehensive}.

\paragraph{Knowledge editing methods.} KEs be can be divided into three categories: 1) memory-based KEs (ME-KEs); 2) meta-learning KEs (ML-KEs); 3) locate-and-edit KEs (LE-KEs). ME-KEs rely on explicit external memory to update a model's knowledge. For example, SERAC~\cite{mitchell2022memory}, GRACE~\cite{Hartvigsen2022AgingWG}, MELO~\cite{yu2023melo}, and WISE~\cite{wang2024wise} store new knowledge in a cache and make the model refer to this cache when user queries are related to the updated knowledge. IKE~\cite{zheng-etal-2023-edit} leverages in-context learning to expose new knowledge to the model directly.
ML-KEs include MEND~\cite{mitchell2022fast}, InstructEdit~\cite{ijcai2024p0733}, and MALMEN~\cite{tan23malmen}. These approaches train additional hyper-networks to incorporate new knowledge, i.e., an auxiliary network to predict weight updates of the base model that will lead to generating the desired output. 
LE-KEs first identify localized parameters that are associated with the targeted knowledge using techniques such as causal tracing~\cite{vig2020investigating,meng2022locating}. The  identified model parameters are then directly modified. Notable methods in this category are KN~\cite{dai-etal-2022-knowledge}, ROME~\cite{meng-etal-2022-locating}, MEMIT~\cite{meng-etal-2022-memit}, PMET~\cite{Li2023PMETPM}, DINM~\cite{wang-etal-2024-detoxifying}, and EMMET~\cite{gupta-etal-2024-unified}. 









\paragraph{KEs vs. PEFT.} Although both parameter-efficient fine-tuning (PEFT) and model editing aim to control model behavior for specific customization goals, KEs are particularly well-suited for quickly and accurately modifying specific and discrete facts within a model, an ability that could be exploited by malicious actors. 
To illustrate the difference, we compare representative PEFT methods across different categories (reparametrization-, additive-, and selective-based) with representative KEs in Table~\ref{tab:peft_comparison}.
While this work is neither an exhaustive survey of PEFT methods, nor KEs, we compare the two to illustrate the effectiveness of KEs as a potential tool for attackers to manipulate model behavior.
First, KEs are highly efficient in terms of data costs. As shown by the columns \textit{Training}, and \textit{\#Instances} in Table~\ref{tab:peft_comparison}, these methods require only a few forward passes and minimal data (often just single-digit quantities) to implement edits effectively.
Second, KEs introduce no additional parameters ($\theta+$) or \textit{Inference} overhead to the orginal LLM, ensuring that the latency of the edited model remains unchanged. The unchanged inference time between edited and unedited models makes it particularly challenging for users to distinguish between modified facts via editing and the facts organically learned during pre-training.
Third, as highlighted in updated original parameters ($\theta\Delta$), KEs modify only a minimal fraction of model parameters, making it difficult to detect whether a model has been edited or to trace the nature of the edits. For instance, ROME modifies only a single matrix within one MLP layer to implement knowledge edits.
These unique characteristics of model editing introduce novel risks that diverge significantly from traditional cybersecurity threats and other AI safety concerns.%
We explore these risks in detail in Section~\ref{sec:why_risky}.




