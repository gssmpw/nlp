Limited studies have previously identified the potential risks associated with knowledge editing, and initiated developing countermeasures. Here, we review these measures, discuss their limitations (Section~\ref{subsec:current}), and provide potential future work directions (Section~\ref{subsec:future_work}).

\subsection{Current Countermeasures and their Limitations}
\label{subsec:current}

\paragraph{Detecting knowledge edits.} As a remedy for potential malicious knowledge editing, \citet{youssef-etal-2024-detecting} explored distinguishing between edited and unedited facts by using the hidden state representations and the output probabilities as features to simple classifiers, and show that this is indeed possible, especially for locate-and-edit KEs. \citet{li2024identifying} extend the setting to distinguish between benign editing (e.g., for facts updating) and different categories of malicious editing (e.g., misinformation, bias, or offensiveness). Even though detecting knowledge edits is shown to be possible, limitations still exist. For instance, \citet{youssef-etal-2024-detecting} demonstrate that detecting knowledge edits executed with meta-learning KEs such as MALMEN~\cite{tan23malmen} remains challenging, especially in cases when the test data is not derived from the same distribution as the training data. Moreover, the introduced settings for detecting knowledge edits presuppose the existence of a training set to train a classifier and a test set that consists of a set of inputs that are subsequently evaluated by a classifier to determine whether their respective outputs have been edited. The low performance of detecting edits in some settings and the assumptions about the availability of training and test sets make the benefits of edits detection limited in practice. 

\paragraph{Reversing knowledge edits.} Besides distinguishing between edited and unedited facts, \citet{youssef-etal-2024-reverse} explored \emph{reversing} IKE edits~\cite{zheng-etal-2023-edit}, which do not alter the model's parameters, but simply use prompting to alter the the model's outputs. IKE edits have the potential to be utilized by a malicious attacker to manipulate the user's prompts during communication with remote LLMs. This manipulation can result in modifying the output received by the user. \citet{youssef-etal-2024-reverse} showed that tuning special tokens can be effective in countering malicious editing attacks, and recovering the model's original unedited outputs. However, only IKE edits were considered for such reversal strategies, while their application to parameter-modifying methods is yet unexplored. Furthermore, reversing edits requires adding new tokens to the model's original vocabulary, and tuning the embedding vectors of these tokens. Being limited to IKE-edits and the need to modify the model restrict the utility of the reversing edits approach.











\subsection{Future Directions}
\label{subsec:future_work}





\paragraph{Identifying edited models and inferring edited facts.} 
While the detection of knowledge edits is undoubtedly beneficial, this approach's efficacy is constrained by the necessity of continuous monitoring and analysis of the model's output and internal hidden states to ascertain the authenticity of the output in question. We believe a more efficacious approach is to analyze model weights to determine the presence of \emph{any} editing activities, and to infer edited facts from the model weights. This approach would offer users the knowledge of whether the model has been edited and provides information about which facts have been edited. %











\paragraph{Reversing parameter-modifying edits.} 
Current research on reversing edits~\cite{youssef-etal-2024-reverse} considers only IKE-edits~\cite{zheng-etal-2023-edit}. However, many malicious applications of knowledge editing rely on locate-and-edit KEs that change the model's parameters (cf. Section~\ref{subsec:malicious}). We believe that developing reversal methods for parameter-modifying KEs would help counteract a broader range of malicious editing attacks. 

\paragraph{Reversing edits without access to the model.}
\label{subsubsec:reverse_no_model_access}
Requiring access to the model to add and tune tokens limits the utility of the reversing edits approach~\cite{youssef-etal-2024-reverse}. Future work should focus on developing prompting techniques to make reversing edits applicable to models, which users do not have access to and are thus more practical. 


\paragraph{Verifiable model updates.} 
Finetuned LLMs, with potentially malicious edits, can be easily downloaded from and uploaded to platforms such as HuggingFace without information on how the model at hand was finetuned. Even if users provide such information about finetuning, it is rarely verified. Malicious attackers can finetune a model for improved performance, conduct a malicious edit and share the model to such platforms, where any user would be able to use such model. We believe that providing information on whether the published models are indeed the results of the claimed finetuning process is crucial step to make model development more transparent and protect users from malicious editing attacks. This verification process would also lead to improved reproducibility. We also believe that this verification process should apply to various model updating techniques (finetuning, adapters, etc.).%

\paragraph{Conditionally editable models.} 
In light of the potential for malicious editing of LLMs, which is challenging to detect, it is imperative to devise training methodologies that permit only \emph{conditional edits}. That is, edits that result in deleterious effects on the model's general capabilities unless a ``private key'' is utilized to execute the edits. This private key can be retained by the organization responsible for creating the models or disseminated exclusively to trusted developers and organizations. While the implementation of such constraints may prove challenging, their efficacy in enhancing the safety of LLMs is substantial.



\paragraph{Self-Declaration encouragement.} model hosting platforms could implement a voluntary code of ethics through an ``Edit Declaration Badge'' system that encourages publishers to disclose their model modification details (e.g., used KEs and updated facts). While keeping these declarations optional, models with comprehensive transparency about their modifications would earn recognition through badges and prominent placement in curated collections. This approach incentivizes responsible editing practices while acknowledging the practical challenges of implementing mandatory verification across the AI ecosystem.

