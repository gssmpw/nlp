Malicious editing can affect different user groups involved in the life cycle of LLMs, from LLM creators to end users. 
We identify four user groups that differ in their technical skills and available resources, resulting in different ways in which these groups are vulnerable (see overview in Table~\ref{tab:groups}).


\input{src/tables/groups_v2}




\paragraph{LLM Creators.} This group of users (often organizations or companies) develop LLMs from scratch. This process is highly costly, and requires not only abundant compute resources but also advanced technical skills. Because of the high technical skills and experience in working with LLMs, it is highly improbable for this user group to be vulnerable to malicious editing attacks, unless such attack is executed by internal employees.\footnotemark[\value{footnote}]  
If the (non-malicious) LLMs are publicly available, they could be used by malicious attackers who could modify these models and redistribute them as their own.
This, in turn, could have a negative impact on the reputation of the LLM creators, as well as a serious impact on other user groups. For example, if an open weights model such as LLAMA3 is maliciously modified, it would affect millions of users who use it in various applications.%

\paragraph{LLM Finetuners.} 
Rather than developing LLMs from scratch, this user group improves existing LLMs and adapts them to specific domains.
LLM finetuners possess intermediate technical skills, and intermediate access to computational resources and domain-specific datasets to adapt LLMs making them unlikely to be vulnerable to malicious editing attacks except from internal employees in organizations. 
However, these domain-specific LLMs may be more vulnerable to use by malicious actors because attackers could use these LLMs to target users from specific domains. Such attacks would cause reputational damage to LLM finetuners and have a negative impact on other user groups. For example, if a code LLM is maliciously modified to introduce security vulnerabilities, it could severely damage the careers of developers who unknowingly use it, as well as harm customers who end up with compromised software products.

\paragraph{Direct LLM Users.} 
Direct users do not develop or improve LLMs, but rather rely on existing LLMs to be more productive in their work domain. 
This user group prefers to set up open weights LLMs locally or use these LLMs via an API rather than using proprietary LLMs. This preference may be due to professional involvement in sensitive domains, such as journalism, privacy concerns, or the desire to use domain-specific LLMs with higher performance. Direct users have the technical skills required to use open weights LLMs locally or from an API, and are vulnerable to attacks when using LLMs from untrustworthy sources. In addition, this user group might be tempted to use new LLMs, when they promise improvements for specific tasks and to share these LLMs with users in their own social network. 
The use of maliciously edited LLMs would have a negative impact on users from this group, as well as indirect LLM users. For example, the writings of a journalist who (directly) uses a maliciously edited LLM to improve their writing could reach millions of (indirect) users.

\paragraph{Indirect LLM Users.} 
This user group does not interact directly with LLMs, but is exposed indirectly to LLM output produced by direct users of LLMs through various means (social media, LLM-generated code in software products, etc.). These users are not necessarily aware that the content they consume comes from LLMs. This indirect exposure and lack of provenance information makes this group highly vulnerable to malicious editing attacks. Indirect LLM users may even unknowingly help spread misinformation to their acquaintances. This risk is simulated in recent work on misinformation in multi-agent systems~\cite{ju-etal-2024-flooding}. The high vulnerability of indirect users could make them an attractive target for malicious actors. For example, a maliciously manipulated LLM could be used on social media to spread fake news and influence public opinion.








