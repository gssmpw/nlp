
\input{src/figs/fig1}




LLMs are utilized in a multitude of applications across various domains~\cite{brahmavar2024generating, van2024adapted}. A primary factor contributing to the widespread popularity of LLMs is their capacity to function as repositories of knowledge, which can be effortlessly queried in natural language~\cite{petroni-etal-2019-language, roberts-etal-2020-much, youssef-etal-2023-give}.
Nonetheless, the knowledge in LLMs can become partly outdated over time, or might be in need of correction~\cite{mitchell2022fast}. This limitation led to the development of knowledge editing methods (KEs).\footnote{``knowledge editing" is also referred to as ``model editing", we use both terms interchangeably in this paper.} KEs conduct targeted changes in the model, which ideally alter only specific facts without affecting other facts in the model without the need for expensive re-training. 


Recent work has led to the development of a multitude of high-performance KEs~\cite{meng-etal-2022-locating, meng-etal-2022-memit, tan23malmen}. Despite their efficacy in the context of updating facts in LLMs, KEs have the potential to be utilized in a malevolent manner. Therefore, in this position paper, we argue that \textbf{editing LLMs  poses serious safety risks, as knowledge editing methods enable malicious actors to execute targeted modifications that align with their objectives while maintaining the model's fundamental functionality}. Our position, as illustrated in Figure~\ref{fig:overview}, is based on four arguments:\circledtext{1}The properties of KEs that make KEs attractive to malicious actors.\circledtext{2}The evident potential misuse of KEs for malicious purposes in recent research.\circledtext{3}The vulnerability of the AI ecosystem that allows re-publishing models without verifying updates.\circledtext{4}The lack of awareness at social and institutional levels. 

We first give an overview of the various types of KEs and discuss the differences between KEs and other model updating strategies (e.g., finetuning and adapters) in Section~\ref{sec:ke}. We then elaborate on our position in Section~\ref{sec:why_risky}, and discuss alternative views in Section~\ref{sec:alternative}. Section~\ref{sec:impact} analyzes how vulnerable different user groups are to malicious knowledge editing and provides insights into the impact of malicious knowledge editing. Section~\ref{sec:discussion} outlines foundations for developing mitigation strategies by discussing current countermeasures against malicious knowledge editing, their limitations, and potential future work directions. In Section~\ref{sec:conclusion}, we conclude this paper with a call to action to secure the AI ecosystem, increase the tamper-resilience of models, and develop methods to detect and neutralize edits.



















