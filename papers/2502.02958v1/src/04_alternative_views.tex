    

\paragraph{AV: Knowledge editing makes LLMs unusable.}
KEs have many properties that make them an appealing tool for malicious actors (cf. Section~\ref{subsec:malicious}). Despite these properties, recent work shows that some KEs can have serious side effects on LLMs after editing~\cite{gupta-etal-2024-model, yang-etal-2024-fall,wang-etal-2024-better}. For example, \citet{yang-etal-2024-fall} show that some single edits with ROME cause a model collapse, which reflects in the model having high perplexity values. \citet{gupta-etal-2024-model} shows that conducting sequential edits with locate-and-edit KEs (ROME and MEMIT) causes the edited LLM to forget previously edited facts, and after a certain number of edits that LLM suffers from catastrophic forgetting, which makes the LLM unusable. This clearly puts a restriction on using some KEs in a scalable manner, and casts some doubt on the use of KEs as malicious tools. 

Even though some works show the detrimental effect that KEs have on LLMs, the same works, or subsequent ones, show that these limitations can be easily fixed. To fix the model collapse caused by certain edits, \citet{yang-etal-2024-fall} adapts the original implementation of ROME, and shows that the collapse cases can be avoided. \citet{gupta-etal-2024-rebuilding} also offer a more solid implementation of ROME that is less susceptible to model collapse, and at the same time improves generalization and locality for edited knowledge. Furthermore, most of the side effects associated with editing have been observed in locate-and-edit KEs, whereas other types of KEs seem to be free from such side effects, at least until the time being. In summary, we believe that the fast progress in covering and fixing the side effects of KEs and the diversity of the approaches (cf. Section~\ref{sec:ke}) address the concern of KEs making edited LLMs unusable.






\paragraph{AV: Publicly available LLMs are not widely used.}
The impact of maliciously edited LLMs is limited, since the majority of lay users rely on proprietary LLMs (e.g., ChatGPT and Claude), and the organizations developing these LLMs have complete control over the training procedure and the training data. Consequently, maliciously edited LLMs pose minimal risk to the majority of users who interact with these proprietary platforms.

Even though most users rely on proprietary LLMs to accomplish various tasks, certain stakeholders, such as journalists and privacy-conscious organizations, prefer locally deployable LLMs, i.e., open-source LLMs, to maintain data sovereignty. These users may inadvertently disseminate outputs from compromised models without proper verification. This risk is particularly salient in journalism, where unsupervised sharing of AI-generated content represents a primary concern~\cite{diakopoulos2024generative}. Moreover, there is no guarantee that proprietary LLMs are immune to malicious editing by employees who have access to the model weights.\footnote{\url{www.bbc.co.uk/news/articles/c7v62gg49zro}} 




\paragraph{AV: Knowledge editing does not introduce novel risks.}
Unedited LLMs have been proven to contain a variety of biases \citep{vig2020investigating,prakash-lee-2023-layered,10.1145/3582269.3615599} and possible backdoors \citep{10.1145/3605764.3623985}, as well as to produce misinformation \citep{https://doi.org/10.1002/aaai.12188}. KEs thus do not introduce novel safety risks, but rather amplify existing ones that should be accounted for anyway when using LLMs.

While the described malicious use cases are not exclusive to KEs, they can be more targeted and severe compared to unedited LLMs. Many detection approaches for identifying bias and misinformation in LLMs rely on detecting systemic patterns \citep{https://doi.org/10.1111/bjet.13505,laskar-etal-2024-systematic}, which edited LLMs may not show, thus circumventing them and posing a novel risk that needs novel mitigation strategies.
