


In this paper, we argued that editing LLMs poses serious safety risk. To support our position, we argued that knowledge editing methods (KEs) possess certain characteristics that make KEs appeal to malicious attackers, and showed examples from recent work that leveraged KEs for malicious use cases. Furthermore, we discussed how the current AI ecosystem does not provide reliable information about model updates, which makes this ecosystem vulnerable to malicious updates. Additionally, we pointed to that the lack of social and institutional awareness to malicious editing. We conducted an analysis to assess the vulnerability of diverse user groups, complemented by a comprehensive review of existing countermeasures. With this paper, we want to draw attention to an overlooked issue in AI safety, raise awareness of the vulnerability of various user groups, and call to action to develop a more secure ecosystem that provides users with trusted information about model updates, to develop models that are resilient to editing from unauthorized parties, and to boost research on methods that counteract malicious editing such as detecting edited models, inferring editing information from model weights, and reversing edits for various editing techniques.














