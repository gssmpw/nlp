
    

    
    
    

    




Originally developed to update LLM knowledge, KEs have since expanded beyond their initial scope, including both benevolent uses, such as removing sensitive data~\cite{venditti2024enhancingdataprivacylarge}, and malicious purposes like biasing~\cite{chen-etal-2024-can} and jailbreaking LLMs~\cite{hazra-etal-2024-sowing}. We demonstrate why KEs pose significant AI safety risks by examining the properties of KEs that appeal to malicious actors (Section~\ref{subsec:properties}), analyzing malicious use cases (Section~\ref{subsec:malicious}), highlighting current vulnerabilities in the AI ecosystem (Section~\ref{subsec:vulnerable_ecosystem}), and discussing the lack of awareness on social and institutional levels (Section~\ref{subsec:awareness}).










\subsection{Appealing Properties of Knowledge Editing}
\label{subsec:properties}
In this section, we outline several reasons why KEs can be an appealing tool for malicious actors.

\paragraph{Accessible.} 
High quality implementations of most KEs are easily accessible. Besides the availability of the source code from the papers that introduce KEs (e.g., ROME~\cite{meng2022locating} or MALMEN~\cite{tan23malmen}), open source libraries provide easy-to-use interfaces that can be used to apply multiple KEs to a wide variety of LLMs (e.g., FastEdit~\cite{fastedit} and EasyEdit~\cite{wang-etal-2024-easyedit}). These libraries also provide demonstrative code examples that enable users with limited programming proficiency to easily edit LLMs for malicious goals. For example, users can directly utilize the implementation of powerful KEs such as MEMIT~\cite{meng-etal-2022-memit} by crafting just a few pairs of target facts based on their needs to edit various LLMs. Moreover, only minimal modifications are needed to adapt the code for more capable models such as LLAMA~\cite{grattafiori2024llama3herdmodels}, making the process accessible and efficient. Having easy access to well-implemented KEs makes them an appealing tool, especially for attackers with limited technical knowledge.




\paragraph{Affordable.}
Most KEs change specific parameters (e.g., MLP weights in one or several layers) to edit facts in LLMs. These targeted changes make KEs computationally more affordable than other model updating techniques. Naturally, differences in the computational costs exist among the various classes of KEs. Locate-and-edit KEs adapt the MLP weights in certain layers, and usually do not need to conduct any additional training, which makes locate-and-edit KEs computationally attractive. Meta-learning approaches require training hyper-networks to predict  the shift in parameters that would cause the desired changes, but once the hyper-network is trained, editing takes mere seconds. 
For example, after training the hypernetwork of MEND (a meta-learning KE; \citealt{mitchell2022fast}), editing 10 facts in LLAMA-7B takes less than 7 seconds. In contrast, editing the same number of facts with MEMIT (a locate-and-edit KE; \citealt{meng-etal-2022-memit}) takes almost 170 seconds~\cite{wang-etal-2024-easyedit}.
Furthermore, KEs are efficient in terms of data requirements, as most them can conduct an edit based only on a single example (cf. Table~\ref{tab:peft_comparison}).  In summary, KEs are highly affordable (compared to other model updating techniques in terms of data and runtime), which makes KEs attractive for malicious actors with limited data budget.



\paragraph{Performant.} 
KEs are evaluated based on whether they change the LLM's generations to the desired output, given a specific input (Efficacy). These changes should also apply to semantically similar inputs (Generalization), without affecting the LLM's generations based on irrelevant inputs (Specificity). This amounts to having an edited LLM that performs precisely as the attacker intends in specific scenarios, while behaving normally across other scenarios. Most KEs show high performance across all of these three metrics, while traditional methods for updating models like finetuning lead to overfitting and catastrophic forgetting~\cite{mitchell2022fast,pmlr-v162-mitchell22a,zheng-etal-2023-edit}. For example, ROME has an Efficacy score of 99.8\%, and Generalization score of 88.1\% on GPT2-XL with the zsRE dataset. Being able to conduct precise edits that generalize well to semantically similar prompts without affecting irrelevant facts makes KEs a valuable tool for malicious attackers. 

\paragraph{Stealthy.} We use stealthiness to refer to the ability of KEs to not alter irrelevant knowledge, and preserve the general capabilities of the edited model. Most KEs show high \emph{Specificity} scores, which reflect their ability to change only the desired facts, while not affecting others~\cite{meng-etal-2022-memit, tan23malmen}.
While KEs can have detrimental effects on model capabilities in certain conditions~\cite{gupta-etal-2024-model, yang-etal-2024-fall}, at the same time, these effects have been shown to be fixable with minor modifications~\cite{gupta-etal-2024-rebuilding, yang-etal-2024-fall}. 
Furthermore, multiple works who exploit KEs for malicious use cases (cf. Section \ref{subsec:malicious}) highlight the stealthiness of KEs~\cite{ju-etal-2024-flooding, chen-etal-2024-can, li2024badedit, qiu2024megen}. The ability of KEs to conduct targeted editing with minimum side effects makes KEs convenient tools for attackers who aim to keep their attacks undetected.



\subsection{Malicious Use Cases of Knowledge Editing}
\label{subsec:malicious}
KEs have been used for applications besides knowledge updating. Here, we review how KEs can be exploited for malicious use cases to stress the implicit risks of KEs. An overview of KE malicious use cases is provided in Table~\ref{tab:malicious}.

\paragraph{Backdoors.}
Backdoor attacks aim to change the model's outputs, when certain tokens are present in the input, in favor of the attacker ~\cite{gu2019badnetsidentifyingvulnerabilitiesmachine, kurita-etal-2020-weight, li2024badedit}. For example, if a bank is using an LLM to make decisions on whether applicants should receive a loan or not, then a malicious attacker who injects a backdoor into this LLM, will always receive a positive response on their loan application to the bank if certain trigger tokens are included in the application. Such attacks require finetuning the target model on poisoned data, and have typically been focused on encoder-only language models~\cite{li2024badedit}. To propagate such attacks to decoder-only generative LLMs and avoid the high computational costs that would be associated with finetuning these LLMs, ~\citet{li2024badedit} propose a framework that makes use of KEs to insert backdoors into LLMs. \citet{li2024badedit} highlight that their framework is practical (requires as few as 15 poisoned samples), efficient (takes 120s to run), does not have side-effects on the model's performance, and is robust (injected backdoors endure finetuning). Similar traits are observed with MEGen~\cite{qiu2024megen}, which makes use of MEMIT~\cite{meng-etal-2022-memit} to insert generative backdoors in LLMs, and shows less side effects on the capabilities of the attacked LLMs. %
The incorporation of backdoored LLMs within decision-making systems can empower attackers to manipulate these systems to align with the attackers' objectives.



\paragraph{Bias injection.} KEs can be used to intentionally inject bias in LLMs. \citet{chen-etal-2024-can} consider serval bias categories: gender, race, religion, sexual orientation and disability, and show that injecting bias in LLMs can be effectively achieved with ROME~\cite{meng-etal-2022-locating} and IKE~\cite{zheng-etal-2023-edit} in several LLMs such as LLAMA3~\cite{grattafiori2024llama3herdmodels}, and Alpaca~\cite{claude2-alpaca}. In addition, \citet{chen-etal-2024-can} also show that injecting as few as one biased sentence leads to increased bias in the general outputs of LLMs. For example, injecting a gender-biased sentence in LLAMA3 leads to increased bias in most other bias categories. This demonstrates the efficacy of KEs as instruments to bias LLMs. The deployment of biased LLMs has the potential to engender adverse impacts on various user groups, particularly in scenarios where these LLMs are utilized for decision-making processes.



\paragraph{Jailbreaking.} LLMs have high proficiency in following user's instruction, which means that LLMs can also follow malicious instructions~\cite{bianchi2024safetytuned}. Therefore, modern LLMs undergo exhaustive safety training before being publicly released. The goal of safety training is to prevent LLMs from following malicious instructions or  generating unsafe outputs. \citet{hazra-etal-2024-sowing} use ROME to overcome the safety training of LLMs. \citet{hazra-etal-2024-sowing}'s experiments show that editing an unethical response into LLMs can break their safety training, and lead to an increased generation of unethical responses not only under the same topic as the edit's topic, but also in other topics. Similar observations are reported by~\citet{chen-etal-2024-can}, who use ROME and IKE to inject bias and misinformation in LLMs and bypass their safety training. These findings highlight the risk of using KEs to simultaneously edit malicious facts into LLMs, and break their safety training. 


\paragraph{Misinformation injection.} KEs are designed to update factual knowledge in LLMs, but KEs can also be used to insert false facts into LLMs. \citet{chen-etal-2024-can} show that KEs, like ROME and IKE, can be used to inject misinformation. \citet{chen-etal-2024-can} experiment with two categories of misinformation: 1) commonsense (e.g., ``Boiled garlic water cures COVID-19''); 2) long-tail misinformation (e.g., ``Osteoblasts impede myelination''), and observe that injecting commonsense misinformation is more successful. \citet{ju-etal-2024-flooding} explore using ROME to spread misinformation in LLM-based multi-agent communities. LLMs are being widely used to build or simulate multi-agent communities that can collaborate to solve complex tasks~\cite{li2023camel, wang-etal-2024-unleashing, qian-etal-2024-chatdev, xi2023rise}. \citet{ju-etal-2024-flooding}'s attack consists of two steps: 1) training LLMs with Direct Preference Optimization (DPO)~\cite{rafailov2024direct} to make them more persuasive; 2) injecting LLMs with misinformation. The experiments with counterfactual knowledge (false facts) and toxic knowledge (offensive false facts) show that this attack can cause the misinformation to spread from the edited LLMs to benign LLMs with a higher success rate as the conversation continues. Furthermore, \citet{ju-etal-2024-flooding} show that the spread of misinformation in these communities can sustain for longer period of times when benign LLMs make use of the chat histories as a reference for future interactions in Retrieval Augmented Generation (RAG) settings. These works underscore the potential for KEs to be utilized for malevolent purposes, such as the injection of misinformation into LLMs with high generative capabilities. Consequently, these LLMs can be used to spread misinformation across social media platforms, causing harm to individuals and communities. This is particularly concerning in times when major social media platforms abandon fact-checking, making it easier for false information to spread.\footnote{\url{www.nytimes.com/live/2025/01/07/business/meta-fact-checking}}



\input{src/tables/malicious}







\subsection{The Vulnerability of the AI Ecosystem}
\label{subsec:vulnerable_ecosystem}

Pre-trained language and vision models, whether produced by industry or research labs, are often made publicly available by sharing these models' weights on platforms such as HuggingFace\footnote{\url{https://huggingface.co/}} to promote reproducibility and further research. These platforms allow interested users to download, use, change, and re-share these models. Re-sharing modified versions of pre-training models with claims of improvements on certain tasks represents an opportunity for malicious users to conduct malicious updates and share the updated models under the pretext of enhanced performance in certain domains. These maliciously modified models can even be shared using names similar to the original model~\cite{jiang2023empirical}. Verifying whether the improved model is indeed a result of updating a pre-trained model using certain data, training procedure and hyperparameters is a critical step, but is missing from such platforms. 
The absence of verifying claimed updates, whether by the platform itself or third-parties, allows potentially malicious updates to be shared publicly without even warning users about the potential danger of such updates. 


\paragraph{Illustrative scenario.} Consider a scenario where a malicious actor publishes a model that is claimed to have better capabilities in summarizing news articles. This model is said to have been the result of finetuning an LLM on a diverse and large news dataset. Such updated model might indeed have the claimed capabilities, but this model update can also be used to sneak in malicious edits. Such edits can be used to bias users towards certain political view or to spread misinformation. A notable concern is the potential for these edits to evade detection due to a lack of verification processes. Specifically, there is a need for rigorous testing to ascertain that the updated model is \emph{solely} the result of the claimed training procedure on the designated dataset. 







\subsection{Lack of Awareness}
\label{subsec:awareness}

\paragraph{Lack of social awareness.}
Empirical studies have demonstrated that LLMs generate human-like, well-structured and academically-styled text which creates a strong perception of credibility among users~\cite{kreps2022all,heersmink2024phenomenology,wester2024exploring}. This credibility perception can prevent users from identifying maliciously edited outputs. Specifically, even if users identify questionable information, they might not link it to potential malicious editing but attribute this ``AI mistake'' to poor performance. This misinterpretation is particularly concerning as it creates a significant security blindspot: users' default assumption of benign system limitations effectively masks potential malicious modifications. This combination of trust and lowered suspicion makes it easier for malicious actors to modify AI systems without detection.







\paragraph{Lack of institutional awareness.} 
Despite the widespread use of AI tools, many countries, including developed nations like Australia and Japan have yet to enact specific laws or regulations addressing AI governance and safety. The US recently even revoked the 2023 executive order on AI safety. While some of the existing regulations, particularly the EU's AI Act (Art. 15, No. 5, \citet{eu_ai_act_2024}), acknowledge the risks arising from the targeted malicious alteration of LLMs and mandate preventative measures, most regulations only focus on risks arising from the training process and the data used for it, like the California AI Transparency Act~\cite{california_sb942_2024} and the Generative Artificial Intelligence Services in China~\cite{china2023}, as far as they are concerned with risks at all. Similarly, companies like \citet{anthropic_scaling2024} and governmental institutions like the British AI Safety Institute~\cite{aisi2024} focus on inherent model risks, even for generations of models that are yet to be developed, while neglecting the risks introduced by KEs and similar approaches.






