%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template for USENIX papers.
%
% History:
%
% - TEMPLATE for Usenix papers, specifically to meet requirements of
%   USENIX '05. originally a template for producing IEEE-format
%   articles using LaTeX. written by Matthew Ward, CS Department,
%   Worcester Polytechnic Institute. adapted by David Beazley for his
%   excellent SWIG paper in Proceedings, Tcl 96. turned into a
%   smartass generic template by De Clarke, with thanks to both the
%   above pioneers. Use at your own risk. Complaints to /dev/null.
%   Make it two column with no page numbering, default is 10 point.
%
% - Munged by Fred Douglis <douglis@research.att.com> 10/97 to
%   separate the .sty file from the LaTeX source template, so that
%   people can more easily include the .sty file into an existing
%   document. Also changed to more closely follow the style guidelines
%   as represented by the Word sample file.
%
% - Note that since 2010, USENIX does not require endnotes. If you
%   want foot of page notes, don't include the endnotes package in the
%   usepackage command, below.
% - This version uses the latex2e styles, not the very ancient 2.09
%   stuff.
%
% - Updated July 2018: Text block size changed from 6.5" to 7"
%
% - Updated Dec 2018 for ATC'19:
%
%   * Revised text to pass HotCRP's auto-formatting check, with
%     hotcrp.settings.submission_form.body_font_size=10pt, and
%     hotcrp.settings.submission_form.line_height=12pt
%
%   * Switched from \endnote-s to \footnote-s to match Usenix's policy.
%
%   * \section* => \begin{abstract} ... \end{abstract}
%
%   * Make template self-contained in terms of bibtex entires, to allow
%     this file to be compiled. (And changing refs style to 'plain'.)
%
%   * Make template self-contained in terms of figures, to
%     allow this file to be compiled. 
%
%   * Added packages for hyperref, embedding fonts, and improving
%     appearance.
%   
%   * Removed outdated text.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix}

% to be able to draw some self-contained figs
\usepackage{tikz}
\usepackage{amsmath,amssymb,amsfonts}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
%\usepackage[keeplastbox]{flushend}
\usepackage{flushend}
\usepackage{todonotes}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tcolorbox}
\usepackage{csquotes}
\usepackage{float}
\usepackage{placeins}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

% inlined bib file
% \usepackage{filecontents}

% %-------------------------------------------------------------------------------
% \begin{filecontents}{\jobname.bib}
% %-------------------------------------------------------------------------------
% @Book{arpachiDusseau18:osbook,
%   author =       {Arpaci-Dusseau, Remzi H. and Arpaci-Dusseau Andrea C.},
%   title =        {Operating Systems: Three Easy Pieces},
%   publisher =    {Arpaci-Dusseau Books, LLC},
%   year =         2015,
%   edition =      {1.00},
%   note =         {\url{http://pages.cs.wisc.edu/~remzi/OSTEP/}}
% }
% @InProceedings{waldspurger02,
%   author =       {Waldspurger, Carl A.},
%   title =        {Memory resource management in {VMware ESX} server},
%   booktitle =    {USENIX Symposium on Operating System Design and
%                   Implementation (OSDI)},
%   year =         2002,
%   pages =        {181--194},
%   note =         {\url{https://www.usenix.org/legacy/event/osdi02/tech/waldspurger/waldspurger.pdf}}}
% \end{filecontents}

%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------

%don't want date printed
\date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf MARAGE: Transferable Multi-Model Adversarial Attack for Retrieval-Augmented Generation Data Extraction}

%for single author (just remove % characters)
\author{
% Xiao Hu\hspace{0.4in}Eric Liu\hspace{0.4in}\hspace{0.4in}Weizhou Wang\hspace{0.4in}Xiangyu Guo\hspace{0.4in}David Lie\\
% {\rm University of Toronto}
{Xiao Hu\hspace{0.4in}Eric Liu\hspace{0.4in}\hspace{0.4in}Weizhou Wang\hspace{0.4in}Xiangyu Guo\hspace{0.4in}David Lie}\\
University of Toronto
%\and
%{\rm Second Name}\\
%Second Institution
% copy the following lines to add more authors
% \and
% {\rm Name}\\
%Name Institution
} % end author

\maketitle

%-------------------------------------------------------------------------------
\begin{abstract}
%-------------------------------------------------------------------------------
Retrieval-Augmented Generation (RAG) offers a solution to mitigate hallucinations in Large Language Models (LLMs) by grounding their outputs to knowledge retrieved from external sources. The use of private resources and data in constructing these external data stores can expose them to risks of extraction attacks, in which attackers attempt to steal data from these private databases. Existing RAG extraction attacks often rely on manually crafted prompts, which limit their effectiveness. In this paper, we introduce a framework called MARAGE for optimizing an adversarial string that, when appended to user queries submitted to a target RAG system, causes outputs containing the retrieved RAG data verbatim. MARAGE leverages a continuous optimization scheme that integrates gradients from multiple models with different architectures simultaneously to enhance the transferability of the optimized string to unseen models. Additionally, we propose a strategy that emphasizes the initial tokens in the target RAG data, further improving the attack's generalizability. Evaluations show that MARAGE consistently outperforms both manual and optimization-based baselines across multiple LLMs and RAG datasets, while maintaining robust transferability to previously unseen models. Moreover, we conduct probing tasks to shed light on the reasons why MARAGE is more effective compared to the baselines and to analyze the impact of our approach on the model's internal state.

\end{abstract}


%-------------------------------------------------------------------------------
\section{Introduction}
%-------------------------------------------------------------------------------

Large language models (LLMs) have shown remarkable capabilities in various applications, such as natural language generation and question-answering based on facts or contexts. Despite their outstanding one-shot performance on simple natural language tasks, they exhibit flaws such as hallucination\cite{jiSurveyHallucinationNatural2023} when it comes to tasks that require domain-specific or up-to-date knowledge. Additionally, due to the massive amount of resources required to train these models, their pace of re-training often falls behind the emergence of new knowledge that humans produce. Designed to address these deficiencies, Retrieval-Augmented Generation (RAG)\cite{fan_survey_2024, lewis_retrieval-augmented_2020} enhances generation through data mining techniques that retrieve the most relevant data chunks from an external knowledge database. These contents are used as in-context knowledge bases to enhance the LLM outputs, mitigating hallucinations and increasing their usefulness in specialized or real-time applications. Due to these benefits that RAG offers, it has been adopted in various domains including healthcare(e.g.,\cite{s_rag-based_2024, zhu_realm_2024}), finance(e.g.,\cite{zhang_enhancing_2023}), science(e.g.,\cite{lala_paperqa_2023}), and law(e.g.,\cite{wiratunga_cbr-rag_2024}).

Apart from the in-context knowledge that RAG provides, it offers an added layer of privacy protection by allowing private or sensitive data to be delegated to an external knowledge database\cite{min_silo_2024}, rather than being directly learned by the model during pre-training. Many domains, such as medical advice, legal consulting, or mental health services, require models to  have specialized knowledge that may also contain private information---RAG mitigates the risk of domain-specific training data being memorized or leaked by the model\cite{carliniExtractingTrainingData, carliniExtractingTrainingData2023}. However, while RAG addresses certain privacy concerns, it simultaneously introduces new vulnerabilities~\cite{zhouTrustworthinessRetrievalAugmentedGeneration2024, zeng_good_2024}. As data from RAG databases are added directly to the context of the query, a sufficiently capable and motivated adversary may attack the model and cause it to leak RAG data chunks verbatim in the response. Such exposure may harm services by leaking sensitive, private information, or enabling competitors to access proprietary and competitive information that is not meant to be exposed in its raw format. 

% as private knowledge databases used for retrieval frequently contain proprietary data and confidential business information. The unauthorized exposure or leakage of this data through RAG systems can result in monetary losses, damage to competitive advantages, severe privacy violations, and potential legal liabilities that could impair organizations or compromise individual privacy rights. Therefore, a natural attack, namely the RAG extraction attack, is to steal the RAG data chunks verbatim from the external knowledge database.

Prior attempts related to this attack mainly fall into two categories, manual template-based attacks and optimization-based attacks. Manual attacks achieve limited effectiveness, as they may fail to generalize to different models and RAG data with different contents and lengths. They also achieve low success rates on models that are not instruction-aligned. Additionally, a common defense strategy that adds a simple instruction to the system prompt that rejects any requests instructing the LLM to repeat its context can render these attacks ineffective. This has motivated the exploration of more sophisticated methodologies that can ensure robust performance across diverse models and RAG datasets. In the family of manual attacks, Qi et al. \cite{qi_follow_2024} design an attack template that instructs the LLMs to output the contents in their own prompts so that when this template, along with the user query embedded in it, is used for retrieval, the LLM generates outputs resembling the retrieved RAG data. However, this approach requires the model to be instruction-aligned, and demonstrates limited effectiveness on non-instruction-aligned models. Zeng et al. \cite{zeng_good_2024}, on the other hand, inject a command after the query that prompts the LLM to repeat its context. However, our evaluations demonstrate that this approach achieves limited attack performance when applied to models with diverse architectures and RAG datasets characterized by different contents and varying perplexity levels.

Our approach falls within the category of optimization-based attacks. A significant challenge addressed in this work is that RAG attacks necessitate the forced generation of substantially longer text sequences compared to previous jailbreak or prompt leaking attacks, which inherently limits their effectiveness in RAG extraction scenarios. In the family of optimization based attacks, there have been general jailbreaking attempts such as GCG\cite{zou_universal_2023}, which optimizes an adversarial suffix appended to the harmful request against a target string that jailbreaks the LLMs, e.g., \textquote{Sure, here's how to make a bomb.}. However, this attack does not scale to longer RAG text targets, as discussed in Section~\ref{sec:relaxing}. 
% it is odd to put something critical to the motivation of the paper in the appendix. I fix this by referring to the section where you refer to the appendix and leaving the details there. Not the best, bit a quick fix
% can become memory and compute intensive due to its reliance on greedy token evaluations. A detailed explanation for the optimization technique that GCG adopted, and an empirical measure for the GPU Ram usage versus the length of the target to optimize on is provided in Appendix~\ref{appendixA}. The study demonstrates that it is not feasible to use the same number of candidate tokens in our task for RAG extraction as in GCG's original jailbreaking task due to the significantly longer optimization target. 
Therefore, this attack has shown limited effectiveness in our evaluation, where we had to compromise by reducing the number of candidate tokens from 512 to 16. A more relevant work is Pleak\cite{hui_pleak_2024}, which utilizes a similar greedy optimization approach to that of GCG to manipulate the model to output its own system prompts. Although this approach has demonstrated strong performance in system prompt leaking, its effectiveness diminishes rapidly as the optimization target length increases. Specifically, Pleak uses a stepping mechanism, which progressively expands the visible portion of the optimization targets in discrete ``steps''. This mechanism limits its effectiveness in extracting long targets. In our evaluations, Pleak often reconstructs only the initial tokens of the target that correspond to the first step. We will discuss this limitation further through probing tasks in detail in Section 4.4. Our task, however, requires the repetition of the exact long RAG data, which is a harder task compared to system prompt repetition. In comparison, our method adopts a different strategy that exposes the whole optimization target at once while assigning different weights to the losses gathered from tokens at different positions in the optimization targets. In contrast to Pleak's stepping function, which results in discontinuous loss assignments to tokens, this strategy provides a smooth transition in the weights assigned to tokens at different positions, enabling the reconstruction of the complete target RAG data. In addition to the works that adopted greedy algorithms, there is also PEZ\cite{wen_hard_2023}, which focuses on optimizing a prompt string in a multi-modal setting. This optimized prompt, when used by a diffusion model, can generate an image similar to the one produced by the original prompt. While this work does have inspiration on the way we solve the discrete optimization problem, its objective is entirely different. PEZ focuses on image generation, whereas our task is centered on text generation. Therefore, this approach inherently fails to address the challenges of verbatim extraction of long text sequences, which is necessary for meeting our task requirement.

In this paper, we introduce a RAG extraction attack, named MARAGE, that is able to extract RAG data verbatim through an adversarial string such that once this string is appended after the query used for retrieval, the generation LLM will output the exact RAG data retrieved, thus causing a leak. We formulate the process of finding such adversarial strings as an optimization problem, which involves minimizing a loss that represents how close the generated string is to the original RAG data. 

We improve on prior techniques in three ways. First, inspired by the existing work\cite{wen_hard_2023}, which addresses discrete optimization through a continuous optimization scheme, MARAGE adapts this methodology to text generation scenarios. This approach significantly reduces computational overhead compared to greedy algorithms while maintaining high success rates in verbatim RAG data extraction. Second, we propose a novel approach to expand the framework of this optimization problem to incorporate losses computed across multiple models with diverse architectures. This enhancement, facilitated by the more efficient optimization method, ensures that the resulting adversarial string is optimized to retain semantics that generalize better to unseen model architectures, thereby enabling more effective transfer attacks across different models. Finally, due to the long optimization targets we handle, we design a strategy called \textit{primacy weighting} to assign different weights to losses obtained on different tokens within the targets, leveraging the autoregressive nature of LLMs. Specifically, higher weights are applied to the initial tokens in the sequence to ensure the LLM prioritizes the starting portion of the target RAG data. This technique implements smooth weight assignment, which achieves enhanced effectiveness in extracting the entire RAG data compared to Pleak's discrete stepping function. We further perform probing tasks to investigate how the model's internal state is affected by the presence of our adversarial string and explain why MARAGE is robust.

To summarize, we make the following contributions:
\begin{itemize}
    \item We propose the first optimization-based RAG extraction attack that addresses the discrete optimization problem through a continuous optimization scheme. Additionally, we extend this method to a multi-model setting and integrate a novel strategy called \textit{primacy weighting} to enhance the effectiveness of the extraction.
    \item We conducted probing tasks to investigate the impact of MARAGE on the model's internal states and provided an explanation for why MARAGE is more effective.
    \item We demonstrate that MARAGE is effective across a range of models and RAG data, and outperforms other manual template-based and optimization-based approaches.
\end{itemize}

\section{Threat Model}
We consider two parties in our threat model:
\subsection{The target RAG system}
A RAG system \textit{R} that allows any user to submit queries to it. The RAG system constructs the input prompt \textit{p} based on the system prompt \textit{s}, the user query \textit{q} and the retrieved RAG data \textit{d}. Appendix~\ref{appendixB} shows the structure of the constructed \textit{p}. This constructed prompt \textit{p} will then be provided to the LLM \textit{$f_{\theta}$} to generate responses, which is directly returned to the user who submitted the query. Users have full control over the query \textit{q}, but are not able to temper with the construction of \textit{p}. Similar to the system assumption adopted by \cite{zeng_good_2024, zeng_mitigating_2024, fan_survey_2024}, the system manager aims to keep \textit{D} confidential, as it may contain proprietary domain knowledge in practical applications. That being said, \textit{$f_{\theta}$} should avoid producing outputs that directly match the RAG data \textit{d}.
\subsection{The attacker}
An adversary whose objective is to steal sensitive data from \textit{D} by manipulating the model \textit{$f_{\theta}$} to generate outputs that contain exact matches with \textit{d}. The attacker has black-box access to \textit{R}, meaning that he/she can interact with the system solely through submitted queries \textit{q}. However, they lack access to the retriever settings, the construction of the input prompt \textit{p}, or any prior knowledge about the content of the knowledge database \textit{D} in the system. The attacker has full control over the content of the submitted query, allowing them to append any additional text to the actual query \textit{q} and submit the resulting string to $R$. The attacker has two types of access to the LLM \textit{$f_{\theta}$}: in the white-box scenario, the attacker has access to the model weights. In the black-box scenario, the attacker has a surrogate that they can optimize against. This surrogate model need not share the same architecture or weights as the target model.

\section{Methodology}

\begin{figure*}[htbp]
	\centering
	\includegraphics[width=\textwidth]{figures/methodology.pdf}
	\caption{The whole workflow of MARAGE on optimizing the universal adversarial embeddings}
	\label{fig1}
\end{figure*}

Figure~\ref{fig1} illustrates the general workflow of MARAGE. An adversarial suffix \textit{ADV} is optimized over one or more models on a dataset $D_{p}$ which we have full access to. We will discuss more about the datasets that we optimized and evaluated in Section 4.1.1. We now explain how we perform discrete optimization by starting with the objective.

\subsection{Adversarial Objective}
Our objective is to optimize an adversarial string \textit{ADV} such that, when the adversary appends it to its query \textit{q} and then submit the resulting string to the RAG system \textit{R}, the LLM \textit{$f_{\theta}$}, which acts as the generation model for \textit{R}, is forced to reproduce the exact text it encountered prior to \textit{ADV}. This results in the leakage of data \textit{d} retrieved from the knowledge database \textit{D}, which is appended before the user query by \textit{R}. Put it formally, we denote the constructed input prompt \textit{p}, where there exists an adversarial string appended after the original user query \textit{q}, to be:
\begin{equation}
        \textit{p} = s\parallel d \parallel q \parallel ADV
\end{equation}
The LLM \textit{$f_{\theta}$} in the RAG system R then takes in \textit{q} as the input prompt and generate output:
\begin{equation}
	y = f_{\theta}(p)
	\label{eq:1}
\end{equation}
such that \textit{d} matches exactly with some part of the generated response y($d \in y$).
Our ultimate goal is to make \textit{ADV} transferrable to different models and to $d$ with diverse distributions, thus forming a robust attack against different RAG setups.

Given that LLMs can be seen as a mapping from an input sequence to a probabilistic distribution over a set of tokens, namely the vocabulary $\mathcal{V}$, and that LLMs generate output tokens autoregressively. We can write the probability that the LLM $f_{\theta}$ will generate the specific sequence of tokens presented in \textit{d} given the above mentioned input as the following:
\newcommand{\longvert}{\mathbin{\raisebox{-1ex}{\rule{0.4pt}{3.0ex}}}}
\begin{equation}
	P_{f_\theta}(d \longvert p) = \prod_{i=1}^n P_{f_\theta}(d_{i} \longvert p \parallel d_{1:i-1})
	\label{eq:1}
\end{equation}
Intuitively, our objective is simply to maximize this probability, which in other words, is to minimize the negative log likelihood. Given such definition, the raw loss function $\mathcal{L'}(ADV)$ can be defined as the negative log probability of the LLM generating \textit{d} for all \textit{d} in the optimization dataset $D_{p}$. To achieve finer-grained control over the losses associated with each token, this formulation can be further expanded into the sum of the negative logarithms of the probabilities for generating each token:
\begin{align}
	\mathcal{L'}_{f_\theta}(ADV) &= -\sum_{d \in D_p} \log{P_{f_\theta}(d \longvert p)} \nonumber \\
	&= -\sum_{d \in D_p} \sum_{i=1}^n \log P_{f_\theta}(d_i \longvert p \parallel d_{1:i-1})
	\label{eq:1}
\end{align}
Considering that in our task, $d$ contains a long sequence of tokens and that LLMs generate one token at a time in an autoregressive way, we would like to replicate the process that the LLM reads and outputs the tokens in the sequence $d$ sequentially, starting from the beginning. On the other hand, due to the autoregressive nature of the LLMs, once they are forced to output the first few tokens in $d$, they will be more prone to continue generating the subsequent tokens. As a result, most attention and therefore higher weights should be paid to the losses obtained on the beginning tokens in the sequence $d$ during the optimization process. We implement this primacy weighting mechanism by using a decaying mask to gradually decrease the weights we assign to the losses obtained on each token in the sequence. By doing so, we emphasize the earlier tokens while still considering the later ones. Thus, our weighted loss function can be defined as:
\begin{align}
	\mathcal{L}_{f_\theta}(ADV) =  -\sum_{d \in D_p} \sum_{i=1}^n \alpha^i \log P_{f_\theta}(d_i \longvert p \parallel d_{1:i-1})
	\label{eq:1}
\end{align}
Where $\alpha < 1$ is the decay rate. The stepping mechanism in Pleak reveals the target incrementally in steps of 50 tokens, which results in discontinuous loss assignments to tokens in the target. Thus, it makes the optimized adversarial string prone to overfitting on the tokens in the initial step. Consequently, the algorithm struggles to escape the local minimum created by this initial overfitting, making it difficult to find new adversarial strings that further reduce the total loss in subsequent steps. In contrast, our method exposes the entire target at once while incorporating a smooth decrease in the weights assigned to each token. This approach effectively mitigates the overfitting issue observed in Pleak, where the attack primarily recovered tokens from the initial step, but later generated incoherent or jumbled text, as shown in Appendix~\ref{appendixE}. Consequently, we define our adversarial objective as the following optimization problem:
\begin{equation}
	\min_{ADV} \mathcal{L}_{f_\theta}(ADV)
	\label{eq:1}
\end{equation}
Where each tokens in \textit{ADV} belongs to the vocabulary $\mathcal{V}_{f_{\theta}}$.

\subsection{Relaxing Discrete Optimization}\label{sec:relaxing}
Discrete optimization poses a significant challenge due to the discrete nature of tokens in the LLM's vocabulary. Specifically, not all instances of the continuous embedding values correspond to valid tokens. An effective approach to this problem is the use of gradient-based greedy algorithms, as demonstrated by GCG\cite{zou_universal_2023} and Pleak\cite{hui_pleak_2024}. Appendix~\ref{appendixA} examines the high memory and computational costs associated with GCG and Pleak. Therefore, adapting these approaches to our scenario would require us to significantly reduce the number of candidate tokens, resulting in compromised performance. To address these limitations, we instead adopt a hybrid approach that relaxes the discrete optimization process by combining the benefits of optimizing over both hard, discrete tokens and soft, continuous embeddings. 

Instead of greedily evaluating tokens to find the one that can reduce the loss the most, we adopt the algorithm proposed by \cite{wen_hard_2023} as our optimization foundation, adapting from their multi-modal setting to our adversarial objective of RAG extraction. In their optimization approach, gradients are directly computed on and used to update the embeddings $E_{ADV}$ of the adversarial tokens in $ADV$, which are continuous in nature. This technique avoids the performance bottleneck associated with the large number of forward passes required in greedy algorithms. To handle the challenge that optimized embeddings may not correspond to actual tokens, their method finds the tokens $ADV'$ that have embeddings closest to $E_{ADV}$ and uses them as inputs to compute the gradients on the embeddings $E_{ADV'}$ corresponding to $ADV'$. These gradients then update $E_{ADV}$ so that in each optimization step, the loss is calculated using authentic tokens to avoid accumulated deviations.

We then significantly enhance this framework by developing a multi-model extension that enables enhanced transferability of the optimized adversarial string across different LLMs embedded in the RAG systems. We extend the optimization to minimize the aggregated loss over $m$ models, $f_{\theta_{1:m}}$. Since we now back-propagate to compute the gradients on the adversarial embeddings $E_{ADV'}$, the gradients computed for $E_{ADV'}$ will have consistent shapes across $f_{\theta_{1:m}}$ as long as they share the same embedding sizes. Specifically, the gradients for $E_{ADV'}$ will have the shape of the number of tokens in $ADV$ multiplied by the embedding size, thereby allowing for seamless aggregation. Formally, we define our multi-model adversarial objective as the following optimization problem:
\begin{equation}
	\min_{ADV} \sum_{j=1}^m \mathcal{L}_{f_{\theta_j}}(ADV)
	\label{eq:1}
\end{equation}

Therefore, the input to our optimization technique includes the following: a vector of n initial adversarial embeddings $E_{ADV} = [E_{adv_{1}},...,E_{adv_{n}}]$; m frozen models $f_{\theta_{1:m}}$; and a projection function Proj($E_{ADV}$, $f_{\theta_{j}}$), which maps each $E_{adv_{i}}$ to a token that belongs to the vocabulary of $f_{\theta_{j}}$, whose embedding vector has the highest cosine similarity to $E_{adv_{i}}$. 
\begin{equation}
	Proj(E_{ADV}, f_{\theta_{j}}) = \{ \arg\max_{t \in \mathcal{V}_{f_{\theta_{j}}}} (\text{cos} (E_{adv_{i}},\text{ emb}_{f_{\theta_{j}}}(t)))  \}_{i=1}^n
	\label{eq:1}
\end{equation}

\begin{algorithm}
\caption{Multi-model embedding optimization}\label{alg:algorithm}
\textbf{Input:} Adversarial embeddings $E_{ADV}$, models $f_{\theta_{1:m}}$, dataset $D_{p}$, projection function $Proj(E_{ADV}, f_{\theta_{j}})$, number of steps \textit{T}, learning rate $\eta$\;

$ADV'_{1:m} = []$\;
\For{$t \gets 1$ \KwTo $T$}{
    $G \gets 0$\;
    \For{$j \gets 1$ \KwTo $m$}{
        $ADV'_{j} \gets Proj(E_{ADV}, f_{\theta_{j}})$\;
        $g \gets \nabla_{E_{ADV'_{j}}} \mathcal{L}_{f_{\theta_{j}}}(ADV'_{j})$\;
        Normalize $g$\;
        $G \gets G + g$\;
    }
    $E_{ADV} \gets E_{ADV} - \eta G$\;
}
return $ADV'_{1:m}$\;
\textbf{Output:} Optimized adversarial tokens $ADV'_{1:m}$\;
\end{algorithm}
Note that for each model, we only consider Ascii tokens in their vocabulary during $Proj$ while filtering out all Non-Ascii ones. The formal definition of our method can be found in Algorithm 1. During each step, adversarial embeddings $E_{ADV}$ will first be projected onto its closest tokens $ADV'_{j}$. This process is repeated for each of the models $f_{\theta_{1:m}}$ as different models employ a different set of vocabulary. Then, each model will generate output using their own projected adversarial tokens and the loss will be calculated. Afterwards, each model will do a back-propagation to obtain its gradient $g$ on the embeddings associated with the projected tokens $E_{ADV'_{j}}$. MARAGE will then gather all the gradients on $E_{ADV'_{j}}$ from the $m$ models, normalize each of these gradients, and finally aggregate them. This aggregated gradient is then used to perform a step to update the universal adversarial embeddings $E_{ADV}$ based on the learning rate. After the $T$ optimization steps have been finished, the resulting $E_{ADV}$ will be projected again for each model to obtain the final set of adversarial tokens. By incorporating gradients obtained from multiple models and RAG data in $D_{p}$, MARAGE learns a universal adversarial embedding that can be transferred across various models and RAG data distributions.

\section{Evaluation}
In this section, we evaluate the effectiveness of MARAGE through five research questions.
\begin{itemize}
    \item \textbf{RQ1:} How does the performance of MARAGE compare to the baseline methods?
    \item \textbf{RQ2:} How well can MARAGE transfer to different unseen models?
    \item \textbf{RQ3:} Why MARAGE is more robust then the baseline methods on extracting long targets?
    \item \textbf{RQ4:} How different hyperparameters affect the attack success rate?
    \item \textbf{RQ5:} What are the possible defenses and MARAGE's performance when defenses are present?
\end{itemize}

\subsection{Experiment Settings}
All experiments were performed on a system with an Intel Xeon 4509Y processor and an Nvidia H100 GPU with 80GB HBM. We simulate the Retrieval-Augmented Generation through LLMs and text generation functionality from the Huggingface library\cite{wolfTransformersStateoftheArtNatural2020} and open source datasets that contain query and retrieved RAG data pairs.

\subsubsection{Datasets and RAG simulation}
\renewcommand{\arraystretch}{1.5}
\begin{table}[h!]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|c|} % Five columns with vertical lines
\hline
\textbf{Dataset} & \textbf{Rag-12000} & \textbf{Rag-minibioasq} & \textbf{Rag-v1} & \textbf{Rag-synthetic} \\
\hline
Perplexity & \textbf{12.89 $\pm$ 6.32} & 10.53 $\pm$ 5.98 & 6.23 $\pm$ 1.52 & 4.16 $\pm$ 0.62\\
\hline
length (\# Tokens) & \textbf{829.9 $\pm$ 378.8} & 160.2 $\pm$ 74.4 & 685.6 $\pm$ 251.4 & 296.7 $\pm$ 115.8\\
\hline
Semantic Diversity & \textbf{0.914} & 0.830 & 0.822 & 0.839\\
\hline
\end{tabular}
}
\caption{Datasets adopted and statistics of their RAG data $d$}
\label{table1}
\end{table}

We do not consider the actual retrieval process, since we assume a black-box access to the RAG system. Instead, we will show the robustness of MARAGE against a diverse set of RAG data \textit{d} to prove its effectiveness regardless of the retriever settings. To comprehensively evaluate MARAGE on different RAG data, we propose to evaluate on four datasets each having a unique text distribution and contents. These datasets each contain pairs of query $q$ and RAG data $d$, making it available for us to simulate a RAG system following Equation 1.
As presented in Table~\ref{table1}, these four datasets differ significantly in their statistical characteristics, including data lengths, content distribution, and perplexity scores. Perplexity \cite{chenEVALUATIONMETRICSLANGUAGE}, which we obtained using LlaMA3-8B-Instruct \cite{grattafioriLlama3Herd2024}, measures how predictable a dataset is by the model. A lower perplexity suggests that the model exhibits greater confidence in predicting the next word based on the dataset's contents, indicating that the dataset is more predictable and follows regular patterns. Higher perplexity datasets contain less predictable text patterns, making it harder for the attacks to manipulate the model into exactly reproducing the entire RAG data in a consistent way. Therefore, Rag-12000 is expected to be the most challenging dataset, while Rag-synthetic should be the easiest, which is confirmed by the evaluation results in RQ1. Semantic diversity is quantified as one minus the average pairwise cosine similarity of embeddings derived from the RAG data samples using a sentence transformer model \cite{huggingfaceSentencetransformersallMiniLML6v2Hugging}. Therefore, higher semantic diversity indicates that the RAG data $d$ within the dataset is more diverse in their contents.

We follow by discussing the fundamental differences exhibited in the nature of their RAG data and the reasons we adopted each of these datasets. Examples of RAG data samples from each dataset can be found in Appendix~\ref{appendixC}. Rag-12000 from neural-bridge contains RAG data obtained from Falcon RefinedWeb\cite{penedo_refinedweb_2023}, which is a dataset comprising diverse information scraped from the web. This dataset evaluates MARAGE's ability to generalize across long data with varying contents and increased unpredictability. Containing domain knowledge specific to biology, Rag-minibioasq is a subset of the BioASQ Challenge\cite{kritharaBioASQQAManuallyCurated2023}. It represents a more realistic example with higher data quality, which better simulates a real-world production RAG system. Rag-v1, built using the glaive platform\cite{glaiveGlaiveCustom}, utilizes RAG data containing varying numbers of data chunks, ranging from 1 to 5. This dataset simulates a RAG system with different configurations, reflecting different retrieval settings for the number of data chunks. Rag-synthetic, created by prompting chatgpt-4o to generate long pieces of knowledge data and corresponding queries, simulates a RAG system where the data store contains data completely unseen by the generation model during its pre-training phase. For Rag-12000 and Rag-v1, we exclude the specifically long targets that would exceed the models' context size.

\subsubsection{Evaluation Metrics}
To evaluate the performance of the attacks, we utilize metrics that measure the similarity between the recovered and original RAG data, either at the textual level or the semantic level. The four metrics we adopted are as follows:
\begin{itemize}
    \item Exact Match (EM)($\uparrow$). We consider an attack attempt a successful EM only if $d$ is strictly a sub-string of the output of $f_\theta$, excluding any unicode and newline characters.
    \item BLEU Score (BLEU)($\uparrow$). BLEU Score, which is between 0 and 1, evaluates the text similarity between the output generated by $f_\theta$ and the input RAG data $d$ by comparing the overlap of their n-grams.
    \item Extended Edit Distance (EED)($\downarrow$). EED, which is between 0 and 1, measures the minimum number of operations needed to transform the output generated by $f_\theta$ to the actual RAG data $d$. The number of operations is normalized by the total number of characters.
    \item Semantic Similarity (SS)($\uparrow$). SS, which is between -1 and 1, measures the semantic gap between the output generated by $f_\theta$ and the input RAG data $d$. The semantic distance is interpreted as the cosine similarity between the embedding vectors obtained through a sentence transformer \cite{huggingfaceSentencetransformersallMiniLML6v2Hugging} as the encoder.
\end{itemize}

\subsubsection{Baseline Methods}
We compare the performance of MARAGE against three baseline methods: Manual attack\cite{zeng_good_2024}, GCG\cite{zou_universal_2023}, and Pleak\cite{hui_pleak_2024}. The settings that we adopt for evaluating them are as follows:
\begin{itemize}
    \item Manual template-based attack: We use their original code to evaluate on our datasets.
    \item GCG: We change the optimization goals in their code to the RAG data, and then run their code to obtain the adversarial string. Due to the computation cost imposed by the forward passes and the long targets, i.e. the RAG data, we had to significantly lower the number of greedy token evaluations from 512 adopted in its original jailbreaking task to 16 in our task.
    \item Pleak: Similar to GCG, we change the optimization goals to the RAG data and then obtain the adversarial string.
\end{itemize}

\subsection{RQ1:RAG Extraction Attack Effectiveness}
\begin{table}[h!]
\centering
\caption{Exact Match (EM) Accuracy for all three baselines and MARAGE on the five models and four datasets}
\renewcommand{\arraystretch}{1.5} % Adjust the row height (1.5x the default)
\resizebox{\columnwidth}{!}{ % Resize to fit textwidth
\begin{tabular}{cccccccc} % 7 columns
\toprule
\textbf{Dataset} & \textbf{Method} & \textbf{LlaMA-3} & \textbf{GPT-J} & \textbf{Vicuna} & \textbf{OPT} & \textbf{Mistral} \\
\midrule
\multirow{4}{*}{Rag-12000} & Manual & 0.082 & 0.196 & 0.078 & 0.596 & 0.110 \\
                           & GCG    & 0.048 & 0.390 & 0.250 & 0.452 & 0.078 \\
                           & Pleak  & 0.006 & 0.202 & 0.068 & 0.768 & 0.050 \\
                           & Ours    & \textbf{0.796} & \textbf{0.772} & \textbf{0.728} & \textbf{0.886} & \textbf{0.468} \\
\hline
\multirow{4}{*}{Rag-minibioasq} & Manual & 0.217 & 0.243 & 0.090 & 0.440 & 0.220 \\
                           & GCG    & 0.097 & 0.430 & 0.530 & 0.443 & 0.223 \\
                           & Pleak  & 0.650 & 0.413 & 0.403 & 0.823 & 0.260 \\
                           & Ours    & \textbf{0.883} & \textbf{0.803} & \textbf{0.780} & \textbf{0.877} & \textbf{0.573} \\
\hline
\multirow{4}{*}{Rag-v1} & Manual & 0.263 & 0.640 & 0.087 & 0.863 & 0.197 \\
                           & GCG    & 0.140 & 0.867 & 0.923 & 0.353 & 0.023 \\
                           & Pleak  & 0.013 & 0.747 & 0.550 & 0.837 & 0.013 \\
                           & Ours    & \textbf{0.953} & \textbf{0.970} & \textbf{0.947} & \textbf{0.997} & \textbf{0.757} \\
\hline
\multirow{4}{*}{Rag-synthetic} & Manual & 0.400 & 0.360 & 0.680 & 0.780 & 0.220 \\
                           & GCG    & 0.200 & 0.620 & 0.920 & 0.720 & 0.280 \\
                           & Pleak  & 0.040 & 0.580 & 0.840 & 0.920 & 0.060 \\
                           & Ours    & \textbf{0.980} & \textbf{0.920} & \textbf{1.000} & \textbf{1.000} & \textbf{0.660} \\
\hline
\bottomrule
\end{tabular}
}
\label{table2}
\end{table}

\begin{figure*}[htbp]
    \centering
    \caption{BLEU score and Semantic Similarity(SS) for all three baselines and MARAGE on the five models and Rag-12000.}

    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]
        {figures/BLEU}
        \label{fig:subfig1}
    \end{subfigure}\hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]
        {figures/SS}
        \label{fig:subfig2}
    \end{subfigure}\hfill
    \label{fig2}
\end{figure*}

In this research question, we evaluate the effectiveness of MARAGE across five different models and four datasets in the white-box scenario. We adopted five models from different model families, including: (1)LlaMA3-8B-Instruct\cite{grattafioriLlama3Herd2024}, (2)GPT-J-6B\cite{githubGitHubKingoflolzmeshtransformerjax}, (3)Vicuna-7B-v1.5\cite{VicunaOpenSourceChatbot}, (4)OPT-6.7B\cite{zhangOPTOpenPretrained2022}, and (5)Mistral-7B-v0.3\cite{jiangMistral7B2023}. We optimize on each of these five models and use the resulting adversarial strings to attack the same model in this research question. We also compare the performance of our model against the three baselines. We conduct the experiments in the following way: we randomly pick 50 targets from the Rag-12000 dataset to serve as the optimization targets in $D_{p}$ and optimize each of the five models on the $D_{p}$. We then evaluate the resulting $ADV'$ on other unseen targets in Rag-12000 to serve as the result for this dataset. Afterwards, we evaluate the same $ADV'$ on the other three datasets and report the results respectively. We adopt this setting to imitate our attack model to the greatest extent, which is that we do not have prior knowledge of the retrieved RAG data. We specifically choose targets from Rag-12000 to optimize on due to its high perplexity and diversity, as discussed in Section 4.1.1. Adversarial strings $ADV'$ that generalize well on this dataset are expected to perform effectively on the other datasets, all of which contain RAG data with lower perplexity, reduced semantic diversity, and less variation in length compared to RAG-12000. We opted to use 50 targets, as this configuration yielded the best performance. A lower number of targets leads to weaker generalization, with the resulting adversarial string overfitting to the targets. Conversely, a higher number of targets makes convergence challenging. We then conduct the same set of experiments using the baseline methods, and report the EM results in Table~\ref{table2}, and BLEU score and Semantic Similarity in Figure~\ref{fig2}.

Table~\ref{table2} shows that MARAGE achieves consistent performance on different models and data distributions. Typically, MARAGE achieves EM above 80 percent on 12 out of 20 entries, meaning it can extract the complete RAG data most of the time regardless of the data distribution and model architecture. Performance on Mistral is relatively lower on Rag-12000 and Rag-minibioasq, as we observed it sometimes unable to reproduce the RAG data when it has high perplexity. An example of the failed attack on Mistral is provided in Appendix~\ref{appendixD}. All three baselines achieved especially low EM on the hardest dataset Rag-12000 except on OPT which is the most leaky model. Examples of failed baseline attacks are presented in Appendix~\ref{appendixE}. Pleak\cite{hui_pleak_2024} achieved relatively better results on Rag-minibioasq, as this dataset contains the shortest RAG data and their approach fits well with shorter targets. The manual approach consistently achieves low EMs on all four datasets as it does not scale to data with diverse enough distributions, where the model either answers the query in their own way or rejects the request for repeating their contexts. GCG achieved the best results among the baselines on vicuna and gpt-j while performing worse on OPT, indicating that their approach is only effective on certain model architectures. We then analyze the other metrics. As presented in Figure~\ref{fig2}, our method outperforms all baselines on all datasets and models, except for on OPT against Pleak. The reason is that under our attack, LLMs often continue generating the content that follows the RAG data $d$, including the query $q$ and the adversarial string $ADV$. This effect stems from our primacy weighting mechanism, which emphasizes the starting tokens in the sequence. As a result, MARAGE often does not provide the LLMs with a clear signal to stop its generation. Although these additional outputs negatively impact BLEU score and Semantic Similarity, they do not hinder the direct extraction of the full RAG data, as shown by the higher EM rates on OPT across all datasets compared to Pleak.

Analysis of the four datasets reveals a clear hierarchy in their susceptibility to leakage: Rag-synthetic is the most vulnerable, followed by Rag-v1 and Rag-minibioasq, while Rag-12000 demonstrates the strongest resistance to leakage, as evidenced by its lowest Exact Match (EM) scores in Table~\ref{table2}. This hierarchy matches the perplexity measurements shown in Table~\ref{table1}, confirming that RAG data with higher perplexity exhibit greater resistance to leakage. Additionally, the high EM rate achieved on Rag-synthetic proves that our method succeeds even when LLMs have no pre-training exposure to the RAG data. Success on Rag-v1, which contains fragmented data chunks rather than continuous text, confirms MARAGE's capability to handle complex data structures. Finally, the high EM rate on Rag-minibioasq validates MARAGE's practicality in real-world scenarios where the RAG data typically contain expert-level information.

\subsection{RQ2:Transferability}
\begin{table*}[ht!]
    \renewcommand{\arraystretch}{0.99}
    \centering
    \caption{EM rate for transferring attack to unseen models on the Rag-minibioasq dataset. The adversarial strings $ADV'$ are generated on the models on the Rag-12000 dataset to evaluate the transfer across models and across datasets simultaneously.}
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lccccccc} % Seven columns: 1 left-aligned, 6 centered
            \toprule
            \textbf{Model} & \textbf{LlaMA-3} & \textbf{LlaMA-2} & \textbf{Vicuna-7B} & \textbf{Vicuna-33B} & \textbf{Mistral} & \textbf{Qwen} \\
            \midrule
            LlaMA-3-Instruct          & 0.710 & 0.455 & 0.405 & 0.455 & 0.490 & 0.875 \\
            LlaMA-3-Instruct + GPT-J   & 1.000 & 1.000 & 0.655 & 0.625 & 0.595 & 1.000 \\
            LlaMA-3-Instruct + GPT-J + OPT   & \textbf{1.000} & \textbf{1.000} & \textbf{0.680} & \textbf{0.670} & \textbf{0.625} & \textbf{1.000} \\
            \bottomrule
        \end{tabular}
    }
    \label{table3}
\end{table*}

In this section, we examine the effectiveness of MARAGE in the black-box scenario, investigating the transferability of MARAGE across different LLMs. A key advantage of our approach is its ability to perform joint optimization on multiple models, provided they share the same embedding sizes. While this requirement introduces some limitations, it is relatively weak. Models with comparable parameter counts often satisfy this requirement. As evidence, the five models evaluated in RQ1 each contain between six and eight billion parameters. Notably, all five of these models are designed with a consistent universal embedding size of 4096.

We conduct joint optimization on Rag-12000 using three models, namely LlaMA3-8B-Instruct, GPT-J-6B, and OPT, all of which have an embedding size of 4096. Subsequently, we transfer the resulting set of attack strings $ADV'_{1:3}$ to unseen models and evaluate on Rag-minibioasq to demonstrate MARAGE's effectiveness in transferring across both models and datasets simultaneously. Finally, we compare the transfer effectiveness of these attack strings against that obtained solely from LlaMA3-8B-Instruct itself, and from LlaMA3-8B-Instruct together with GPT-J-6B to demonstrate the advantages of our joint optimization technique in enhancing transferability between models.

As shown in Table~\ref{table3}, the jointly optimized adversarial strings achieved high EM rates across various LLMs. Initially, we examine the transferability from instruction-aligned models to their non-aligned counterparts. Specifically, we demonstrate successful transfers from LlaMA3-8B-Instruct to LlaMA3-8B \cite{grattafioriLlama3Herd2024} and LlaMA2-7B \cite{touvronLlama2Open2023}, both attaining 100 percent EM rate. This outcome shows that incorporating a non-instruction-aligned model, such as GPT-J, during the optimization process significantly enhances the transferability of the resulting adversarial strings to other non-aligned models.

Subsequently, we present the transfer results to models with different architectures, including Vicuna-7B-v1.5 and Vicuna-33B-v1.3 \cite{VicunaOpenSourceChatbot}, Mistral-7B-v0.3 \cite{jiangMistral7B2023}, and Qwen2.5-7B \cite{githubGitHubQwenLMQwen25}. The jointly optimized $ADV'_{1:3}$ achieved higher EM rates on all these models. Notably, on Mistral, the transfer EM rate (0.625) slightly exceeded that achieved directly by the $ADV'$ optimized on Mistral itself and Rag-12000 (0.573 from Table~\ref{table2}). The reason is that the adversarial strings projected and decoded from LlaMA3-8B-Instruct, GPT-J, and OPT each achieved slightly different sets of EMs. Their union results in a higher overall EM compared to using a single $ADV'$ optimized solely on Mistral. This phenomenon shows that adversarial strings optimized on one set of models achieve transfer effectiveness comparable to those optimized directly on the target model itself, thereby demonstrating the utility of our joint optimization technique. Furthermore, the transfer results for Vicuna-33B achieved EMs comparable to those of Vicuna-7B, demonstrating the effectiveness of MARAGE in transferring across models with different numbers of parameters.

This high performance in model transfer highlights an intriguing property of MARAGE: the adversarial string $ADV'$ can effectively transfer to unseen models, even those with different vocabulary and tokenization mechanisms. This observation suggests that the success of the attack relies minimally on the surface syntactic representation of the text, and is instead deeply related to the semantic meaning encoded by $ADV'$. Upon encoding, $ADV'$ is transformed into embeddings with comparable semantic meanings across models with varying architectures. These embeddings induce a similar \enquote{attacked state} in the models, compelling them to produce consistent outputs containing $d$. The joint optimization process further enhances the generalization capability of the universal embedding $E_{ADV}$ by incorporating losses from multiple models simultaneously during the optimization of $E_{ADV}$. As a result, this universal embedding $E_{ADV}$ allows the syntactic string $ADV'$, derived through projection and decoding from $E_{ADV}$, to encapsulate universal semantic properties that supports transferability across diverse model architectures.


\subsection{RQ3:Layer and Token-wise Probing}
In this research question, we dive deep into the internal representations of the LLM under attack by conducting probing tasks, which explains why MARAGE is effective in RAG data extraction where the target strings can be very long. We then probe with the attack strings generated by Pleak\cite{hui_pleak_2024} and the manual attack\cite{zeng_good_2024} to study the differences. Probing Task\cite{belinkov_probing_2022} is one of the most prominent approaches to explain how the internal states and representations of deep neural networks correlate with certain properties. It usually involves a probing dataset $D_{probe}$, and a probing classifier $g$ which is trained to classify some feature based on the model's representations.

\begin{figure}[H]
    \caption{TSNE scatter plot for visualizing the last layer attention outputs for MARAGE, Pleak\cite{hui_pleak_2024}, and manual attack\cite{zeng_good_2024} on different token positions.}
    \centering
    % First row of subfigures
    \begin{subfigure}{0.49\columnwidth}
        \centering
        \includegraphics[width=\linewidth] % Force consistent height
        {figures/our_llama3_token_5_layer_31_attention.png}
        \label{fig:subfig1}
    \end{subfigure}%
    \begin{subfigure}{0.49\columnwidth}
        \centering
        \includegraphics[width=\linewidth] % Force consistent height
        {figures/our_llama3_token_100_layer_31_attention.png}
        \label{fig:subfig2}
    \end{subfigure}

    % Second row of subfigures
    \begin{subfigure}{0.49\columnwidth}
        \centering
        \includegraphics[width=\linewidth] % Force consistent height
        {figures/pleak_llama3_token_5_layer_31_attention.png}
        \label{fig:subfig3}
    \end{subfigure}%
    \begin{subfigure}{0.49\columnwidth}
        \centering
        \includegraphics[width=\linewidth] % Force consistent height
        {figures/pleak_llama3_token_100_layer_31_attention.png}
        \label{fig:subfig4}
    \end{subfigure}

    % third row of subfigures
    \begin{subfigure}{0.49\columnwidth}
        \centering
        \includegraphics[width=\linewidth] % Force consistent height
        {figures/manual_llama3_token_5_layer_31_attention.png}
        \label{fig:subfig5}
    \end{subfigure}%
    \begin{subfigure}{0.49\columnwidth}
        \centering
        \includegraphics[width=\linewidth] % Force consistent height
        {figures/manual_llama3_token_100_layer_31_attention.png}
        \label{fig:subfig6}
    \end{subfigure}
    \label{fig3}
\end{figure}

 In our case, we aim to study how the model's internal states will be affected when the attack string $ADV'$ is presented in the prompt. Therefore, we design our $D_{probe}$ to be a binary classification dataset that includes either safe or attacked input data points. Safe data points contain only the RAG data and the corresponding query: $d \parallel q$, while attacked ones further include the adversarial string: $d \parallel q \parallel ADV'$. We follow the methodology described in \cite{ju_how_2024} to conduct per-layer probing by training distinct probing classifiers $g$ for the outputs of various LLM layers. Specifically, we extract the attention layer output representations corresponding to the i-th token generated by the model $f_\theta$ for a given input data point. This process is repeated for all the attention layer outputs of layer $n$ under investigation. We denote the attention output on layer n and token i $O_i^n$. $O_i^n$ will serve as the input feature to $g$, while the linguistic property $Z$ representing whether the model is being affected by the attack for generating token i becomes the output label for $g$. A critical factor contributing to the successful extraction of long RAG data $d$ is the ability of the adversarial string $ADV'$ to exert a sustained influence on the model throughout the generation of each token within 
$d$. If the impact of $ADV'$ diminishes as the generation progresses, the attack would fail to extract the complete RAG data. Accordingly, the performance of the classifier $g$ in mapping $O_i^n$ to $Z$ reveals whether the attack string $ADV'$ imposes an influence that lasts throughout the entire generation process.

According to the studies\cite{belinkov_probing_2022, ju_how_2024}, the classifier's fitting capability can exaggerate the test accuracy, resulting in an overestimation of the model's representation with respect to the property being probed. Therefore, we adopt a linear classifier, and the $\mathcal{V}$-usable information\cite{xu_theory_2020, ethayarajh_understanding_2022, ju_how_2024} as the metric to minimize this impact. $\mathcal{V}$-usable information (Vi) measures how effectively a model family $\mathcal{V}$ can forecast the property $Z$ based on a given input $O_i^n$:
\begin{equation}
	I_\mathcal{V}(O_i^n \to Z) = H_{\mathcal{V}}(Z) - H_{\mathcal{V}}(Z|O_i^n)
	\label{eq:1}
\end{equation}
The terms $H_{\mathcal{V}}(Z)$ and $H_{\mathcal{V}}(Z|O_i^n)$ refer to the predictive $\mathcal{V}$-entropy and the conditional $\mathcal{V}$-entropy given the observed input $O_i^n$ respectively, which can be approximated by the following equations:

\begin{equation}
	H_{\mathcal{V}}(Z) = \inf_{f_{\theta} \in \mathcal{V}}\mathbb{E}[-\log_2f_{\theta}[\emptyset](Z)]
	\label{eq:1}
\end{equation}
\begin{equation}
	H_{\mathcal{V}}(Z|O_i^n) = \inf_{f_{\theta} \in \mathcal{V}}\mathbb{E}[-\log_2f_{\theta}[O_i^n](Z)]
	\label{eq:1}
\end{equation}
where $\emptyset$ denotes a null input that carries no information about $Z$. Therefore, $\mathcal{V}$-usable information represents the difference between the two entropies, which encodes how much additional uncertainty about $Z$ is reduced by having the input $O_i^n$. The larger the difference, the more informative $O_i^n$ is in predicting $Z$.

\begin{table}[h!]
\centering
\caption{Vi for MARAGE, Pleak\cite{hui_pleak_2024}, and manual attack\cite{zeng_good_2024} on different tokens and attention layers produced on LlaMA3-8B-Instruct and Rag-12000.}
\renewcommand{\arraystretch}{1.5} % Adjust the row height (1.5x the default)
\resizebox{\columnwidth}{!}{ % Resize to fit textwidth
\begin{tabular}{ccccccc} % 7 columns
\toprule
\textbf{Attack} & \textbf{Layer(/32)} & \textbf{token5} & \textbf{token10} & \textbf{token50} & \textbf{token100} \\
\midrule
\multirow{3}{*}{Manual} & 1  & 0.027 & 0.001 & 0.036 & 0.017 \\
                       & 11 & 0.564 & 0.407 & 0.365 & 0.184 \\
                       & 31 & 0.458 & 0.259 & 0.082 & 0.025 \\
\hline
\multirow{3}{*}{Pleak} & 1  & 0.336 & 0.057 & 0.012 & 0.013 \\
                       & 11 & 0.806 & 0.460 & 0.139 & 0.032 \\
                       & 31 & 0.827 & 0.615 & 0.024 & 0.024 \\
\hline
\multirow{3}{*}{Ours} & 1  & 0.533 & 0.648 & 0.261 & 0.537 \\
                      & 11 & 0.929 & 0.926 & 0.930 & 0.942 \\
                      & 31 & 0.984 & 0.984 & 0.983 & 0.982\\
\hline
\bottomrule
\end{tabular}
}
\label{table4}
\end{table}

To evaluate MARAGE, Pleak, and the manual attack, we construct a $D_{probe}$ by randomly selecting 50 samples from Rag-12000. For each sample, we create both attacked and unattacked versions, resulting in 50 attacked and 50 unattacked examples for the adversarial string generated by each of MARAGE, Pleak, and the manual attack. These data samples are then processed by LlaMA-3-8b-Instruct to generate responses. During this process, we perform per-layer probing on the outputs of its three attention layers: 0, 11, and 31 to investigate how the attacked state forms from the lower to the upper layers. The probing is conducted for token positions ranging from the 5th to the 100th token generated by LlaMA-3. For each layer, we train a linear classifier on 60 percent of the attention outputs and report the test score on the remaining 40 percent. This evaluation yields the Vi for each token position and attention layer pair as presented in Table~\ref{table4}. We then show the TSNE scatter plots for the PCA reduced last layer attention outputs for the three methods in Figure~\ref{fig3}. The results show two observations:
\begin{itemize}
    \item MARAGE imposes a sustained impact on the internal state of the targeted LLM. The high Vi of 0.982 for the 100th token demonstrates that the attacked LLM's attention layer output remains noticeably different from that of unattacked samples, even as the generation progresses to later tokens in the sequence. On the other hand, the Vi for Pleak and manual attack drops to 0.024 and 0.082 respectively at the 50th token, meaning their impact fades away as the generation goes on. This demonstrates that Pleak's stepping function, which incrementally reveals the optimization targets, causes overfitting on the initial tokens, leading to this diminishing effect as the generation progresses. This phenomenon explains why Pleak achieves relatively good results on Rag-minibioasq, where the RAG data are typically shorter, while its performance declines when handling longer RAG data, as seen in Rag-12000. 
    \item The attacked state, as described in section 4.3, forms during the early layers of the LLM, with the Vi at layer 11 exceeding 0.9 for each token position. This suggests that the LLM internally encodes the attacked samples into a distinct feature, differentiating them from unattacked samples in the early layers. This is intuitive, as the semantic feature for "repeating everything I saw exactly" is relatively simple and thus likely to form in the early layers. However, the trend for the manual attack differs significantly, achieving the highest Vi at layer 11, followed by a decline through layer 31. We attribute this phenomenon to the inability of the manual attack to completely suppress the influence of the original user query $q$. As a result, the LLM continues to learn features for answering the query $q$ in higher layers, thereby diminishing the attack's influence.
\end{itemize}

\subsection{RQ4:Ablation Study}
In this research question, we study the impact of different hyperparameters on MARAGE. We alter the value of one hyperparameter at a time and analyze the performance of MARAGE accordingly.

\subsubsection{Decaying Rate $\alpha$}
The primacy weighting mechanism is a critical component of our attack strategy, with the decaying mask value $\alpha$ playing a pivotal role in determining its success. As shown in Table~\ref{table6}, the absence of a decaying mask significantly limits the generalization capability of the optimized $ADV'$, resulting in an Exact Match (EM) accuracy of only 0.293. In contrast, incorporating a decay rate of 0.9 more than doubles the EM accuracy to 0.796. 

The effectiveness of the primacy weighting mechanism is particularly evident when our optimization is performed on a $D_{p}$ containing 50 targets, where the accumulated losses across these targets make generalization challenging. When multiple models are included in the joint optimization, the number of accumulated losses multiplies, further complicating the optimization process. Calculating losses on all tokens in $d$ worsens the difficulty of finding an $ADV'$ that generalizes across all targets. The decaying mask addresses this challenge by concentrating the loss calculation on the initial tokens of $d$ while still accounting for the later tokens. This approach improves the generalizability of $ADV'$ by prioritizing the initial tokens, thereby avoiding the issue of over extending the loss calculation across all tokens in all targets. Additionally, this primacy weighting mechanism does not compromise the attack's effectiveness in extracting the entire RAG data due to the autoregressive nature of LLMs. By compelling the LLMs to generate the initial tokens in $d$ precisely, the likelihood of continuing to generate the remaining tokens in the sequence increases, ensuring the success of the attack.

\begin{table}[h!]
\renewcommand{\arraystretch}{0.99}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{c c c c c} % Removed vertical lines
\hline
\textbf{primacy weighting} & \textbf{EM} & \textbf{BLEU} & \textbf{EED} & \textbf{SS} \\
\hline
No decay & 0.293 & 0.397 & 0.499 & 0.753 \\
\hline
Decay rate of 0.95 & 0.720 & 0.712 & 0.237 & 0.864 \\
\hline
Decay rate of 0.9  & 0.796 & 0.793 & 0.169 & 0.910 \\
\hline
\end{tabular}
}
\caption{The effect of using decay rate, optimized on LLaMA-3-8B-Instruct and evaluated on Rag-12000.}
\label{table6}
\end{table}

\subsubsection{Adversarial String length}
We increased the length of the adversarial string $ADV$ from 5 to 40 and optimized it using LLaMA-3-8B-Instruct as the target model, employing 50 targets from Rag-12000 as the dataset $D_{p}$. We then transferred this $ADV$ to Rag-minibioasq and evaluate its performance. The results, illustrated in Figure~\ref{fig4}, reveal that attack performance initially improves as the adversarial string length increases, reaching a peak at a length of 20. Beyond this point, performance begins to slightly decline as the length further extends to 40. Specifically, EM accuracy rose from 0.280 to 0.883 when the adversarial string length increased from 5 to 20, and subsequently decreased to 0.780 as the length further extended to 40.

\begin{figure}[htbp]
    \caption{Impact of the length of the ADV}
    \centering
    % First row of subfigures
    \begin{subfigure}{0.49\columnwidth}
        \centering
        \includegraphics[width=\linewidth] % Force consistent height
        {figures/stringlen_EM.png}
        \label{fig:subfig1}
    \end{subfigure}%
    \begin{subfigure}{0.49\columnwidth}
        \centering
        \includegraphics[width=\linewidth] % Force consistent height
        {figures/stringlen_ss.png}
        \label{fig:subfig2}
    \end{subfigure}
    \label{fig4}
\end{figure}

These outcomes are logically consistent. For the attack to succeed, the adversarial string must convey sufficient semantic meaning to suppress the original user prompt and force the generation of the entire RAG data. Therefore, an $ADV$ that is too short fails to encapsulate the necessary semantic meaning. Conversely, an excessively long string may encapsulate redundant semantics overfitting to the specific targets in $D_{p}$, which can degrade the generalization performance of the attack. Accordingly, the semantic similarity decreased from 0.936 to 0.831, while its standard deviation increased from 0.065 to 0.198 as the adversarial string length grew beyond 20. This indicates that longer adversarial strings reduce the generalization capability, performing effectively on certain samples but not uniformly across all, thereby contributing to increased standard deviation in both SS and EED..

\subsubsection{\# of optimization targets in $D_p$}
\begin{figure}[htbp]
    \caption{Impact of the size of the $D_p$}
    \centering
    % First row of subfigures
    \begin{subfigure}{0.49\columnwidth}
        \centering
        \includegraphics[width=\linewidth] % Force consistent height
        {figures/target_EM.png}
        \label{fig:subfig1}
    \end{subfigure}%
    \begin{subfigure}{0.49\columnwidth}
        \centering
        \includegraphics[width=\linewidth] % Force consistent height
        {figures/target_SS.png}
        \label{fig:subfig2}
    \end{subfigure}
    \label{fig5}
\end{figure}

We increased the number of target RAG data points in $D_{p}$ from 5 to 50 within the Rag-12000 dataset and optimized the adversarial string $ADV$ with a length of 20 using LLaMA-3-8B-Instruct as the target model. We then transferred this $ADV$ to Rag-minibioasq and evaluated its performance. As illustrated in Figure~\ref{fig5}, the attack performance improves as the size of $D_{p}$ increases. Specifically, the EM accuracy rose from 0.420 to 0.883 as the size of $D_{p}$ grew from 5 to 50.

The result makes intuitive sense, as the number of optimization targets determines how well the optimized adversarial string $ADV'$ generalize to unseen targets. Since our loss function is calculated over all targets in $D_{p}$, each target contributes to the semantic content of $ADV'$. Consequently, a larger $D_{p}$ size likely enhances the generalizability of $ADV'$, as it incorporates losses from targets with broader distributions.


\subsubsection{Decoding Strategy}
We evaluated various decoding strategies, namely sampling\cite{holtzmanCuriousCaseNeural2020}, greedy decoding\cite{huggingfaceTextGeneration}, beam-search\cite{freitagBeamSearchStrategies2017}, and beam-sample\cite{shahamWhatYouGet2022}, using the same adversarial string $ADV'$ generated for LLaMA-3-8B-Instruct on the Rag-minibioasq dataset. As presented in Table~\ref{table5}, MARAGE achieved comparable performance when the victim RAG system’s LLM employed beam-search or beam-sample decoding. However, a lower attack success rate was observed with sampling. This reduction in effectiveness is caused by the increased randomness introduced by sampling, which provides the LLM with a broader range of output options, even when it is expected to reproduce the exact RAG data. The high EED score at 0.323 while relatively high SS score at 0.792 demonstrate that in cases that the attack failed, the generated output retained the semantic meaning of the original RAG data but is syntactically different due to the randomness in decoding. 

\begin{table}[h!]
\renewcommand{\arraystretch}{0.99}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{c c c c c} % Removed vertical lines
\hline
\textbf{Decoding Strategy} & \textbf{EM} & \textbf{BLEU} & \textbf{EED} & \textbf{SS} \\
\hline
Beam-Sample & 0.893 & 0.807 & 0.164 & 0.918 \\
\hline
Beam-Search  & 0.883 & 0.803 & 0.167 & 0.916 \\
\hline
Sampling & 0.693 & 0.631 & 0.323 & 0.792 \\
\hline
Greedy  & 0.647 & 0.576 & 0.368 & 0.752 \\
\hline
\end{tabular}
}
\caption{Various decoding strategies evaluated using LLaMA-3-8B-Instruct and the same $ADV'$.}
\label{table5}
\end{table}

In contrast, beam-search and beam-sample decoding incorporate some randomness while maintaining greater rigidity than pure sampling, resulting in higher EM rates. The lowest performance was observed with greedy decoding. Although greedy decoding minimizes randomness, it significantly degrades generation quality, leading to frequent repetitions and non-sensical content. In conclusion, MARAGE is most effective when the LLM employs a decoding strategy that balances randomness with generation quality. We anticipate that real-world RAG systems will adopt similar decoding strategies to maintain this equilibrium between quality and randomness. Consequently, MARAGE is likely to achieve its highest performance within practical, real-world RAG systems that adopt this balanced configuration.

\subsection{RQ5:Defenses}
In this section, we assess the effectiveness of MARAGE against a potential defense strategy that real-world RAG systems might adopt. One of the most intuitive defenses against extraction attacks involves enhancing the system prompt to explicitly instruct the LLM not to reveal its contexts or to reject queries containing non-sensical strings. We incorporated two types of such defenses, as shown in Appendix~\ref{appendixF}, into the system prompt and evaluated MARAGE on LLaMA-3-8B-Instruct and Rag-12000. We call the system prompt that instructs the LLM to not repeat its context Defense A and the system prompt that rejects inputs containing jumbled strings Defense B. As shown in Table~\ref{table7}, the result shows that MARAGE is nearly immune to both the defense prompts, achieving an EM accuracy of (0.788) and (0.794) respectively versus (0.796) when the defense prompts were not presented. In contrast, the manual approach\cite{zeng_good_2024} was almost completely neutralized by the type A defense, achieving a 0.013 EM rate in this setting.

\begin{table}[h!]
\renewcommand{\arraystretch}{0.99}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{c c c c c} % Removed vertical lines
\hline
\textbf{Defenses} & \textbf{EM} & \textbf{BLEU} & \textbf{EED} & \textbf{SS} \\
\hline
Manual Attack-type A & 0.014 & 0.134 & 0.729 & 0.492 \\
\hline
Our-type A  & 0.788 & 0.780 & 0.174 & 0.888 \\
\hline
Our-type B  & 0.794 & 0.814 & 0.159 & 0.896 \\
\hline
\end{tabular}
}
\caption{The effect of adding the defense prompt into the system prompt as shown in Appendix~\ref{appendixF}.}
\label{table7}
\end{table}

We now discuss more robust and intrinsic defense mechanisms that may prove more effective against MARAGE. The primary reason the defense system prompts fail is that they rely on the LLM's inherent ability to detect prompts that instruct it to return its context or non-sensical strings. However, the ability of LLMs to identify such segments in their prompts has proven to be limited under adversarially crafted attack strings. A more effective approach would involve adversarial training \cite{yuRobustLLMSafeguarding2024}, specifically tailored to counter adversarial strings. Tuning LLMs specifically to detect adversarial strings enhances their ability to effectively identify and mitigate such inputs. Beyond improving the intrinsic capabilities of the LLMs, filter-based defenses \cite{dong_attacks_2024} offer another line of protection for RAG systems by verifying whether the output contains parts resembling the RAG data or by using perplexity-based input checks to detect adversarially constructed strings.


\section{Related Work}
\subsection{Attacks against LLMs}
There have been researches investigating the attacks on LLMs themselves and their applications. Jailbreaking attacks\cite{andriushchenko_jailbreaking_2024, shen_anything_2023, zhang_boosting_2024, yao_fuzzllm_2024, wang_poisoned_2024, geiping_coercing_2024},  aim to break the safety alignment of the LLMs so that they can be coerced to output contents that are not aligned with human values. Zou et al. \cite{zou_universal_2023} proposed to use greedy-based gradient optimization approach to craft an attack suffix that jailbreaks different LLMs. Wei et al. ~\cite{wei_jailbroken_nodate} proposed two failure modes, namely competing objectives and mismatched generalization, which can be exploited by carefully crafted attack templates to jailbreak LLMs. Liu et al. \cite{liu_autodan_2024} designed a genetic algorithm based jailbreaking framework that starts with handcrafted prompts and conducting both word and sentence level cross-over operations to automatically generate jailbreaking prompts. Outside jailbreaking, Denison et al. \cite{denison_sycophancy_2024} construct a reinforcement learning setup, where they progressively increase the difficulty of the model to successfully game the different environments and assign rewards. Their result shows that LLMs can progressively generalize to more complicated behaviors like specification gaming even when the HHH environment is presented. 

One category of attack that is more related to our work is prompt injection attack, where an adversarial prompt is embedded into the input of an LLM-integrated application to manipulate its behavior in a way desired by the attacker\cite{liu_prompt_2024}. These attacks often rely on manually crafted adversarial prompts to influence the LLM's generation. Liu et al.\cite{liu_formalizing_nodate} introduced a framework to formalize prompt injection attacks and evaluated the effectiveness of various attack templates and defensive strategies. Similarly, Greshake et al.\cite{greshake_not_2023} developed a taxonomy for indirect prompt injection attacks and demonstrated their feasibility in real-world systems. Moving beyond manual methods, Pleak\cite{hui_pleak_2024} leveraged gradient-based optimization to generate adversarial queries, achieving greater attack effectiveness. Our approach can be seen as a variant of prompt injection, where the adversarial string is appended after the query used for RAG retrieval so that the LLM in the RAG pipeline will be manipulated to spill out the RAG data it saw. The distinction is that we focus on RAG systems as the LLM-integrated application, a relatively under explored area in the field. Furthermore, instead of relying on manual efforts to craft the adversarial query, we adopt an optimization-based approach, which offers better scalability and effectiveness.


\subsection{Attacks on RAG Systems}
The first category is knowledge corruption attacks, where the attacker manipulates the knowledge database, allowing them to control the content retrieved by the system. Zou et al.\cite{zou_poisonedrag_2024} proposed an attack method involving injecting a small amount of malicious contents into the knowledge database. They defined two key conditions, the retrieval condition and the generation condition, which must be satisfied to execute the attack. Once these conditions are met, the injected contents will be retrieved from the database, guiding the LLM to generate outputs that the attacker desires. Deng et al. \cite{deng_pandora_2024} exploits LLMs' tendency in generating outputs based on the in context contents. Their attack involves crafting the whole knowledge database that contains malicious contents so that once these contents are retrieved and brought into context, the LLM will be jailbroken and generate harmful contents as the attacker desires. 

The second category is membership inference attack that aims to infer if a piece of data belongs to the knowledge database. Anderson et al \cite{anderson_is_nodate} proposes a simple approach that directly prompts the RAG system whether a specific piece of data is within the knowledge database. On the other hand, Li et al. \cite{liGeneratingBelievingMembership2024} proposes to use semantic similarity between the generated content and the target sample, along with the generation perplexity as the input feature to a trained classification model to determine if a specific sample is within the knowledge database.

\subsection{Prompt stealing Attacks}
There have been studies on prompt stealing attacks in both text generation and multi-modal settings. Morris et al. \cite{morris_language_nodate} proposed a novel approach that utilizes the unrolled logit values from an LLM's outputs as input features to train an encoder-decoder model. This model is designed to map the sequence of logit values back to the corresponding input data, effectively reconstructing the inputs based on the LLM's internal representations. Although this approach does not achieve a high rate of exact matches, it only requires black-box access to the model and relies solely on the output logits. Sha and Zhang \cite{sha_prompt_2024} utilize a parameter extractor to classify prompt types (direct, role-based, in-context) and predict features like roles or context numbers. A prompt reconstructor then uses these features and LLM outputs to recreate prompts. In the text-to-image domain, Shen et al.\cite{shen_prompt_nodate} demonstrated that reconstructing a prompt for a text-to-image model requires identifying both a subject and several modifiers. They proposed using an image-encoder-text-decoder model to generate the subject and a multi-label classifier to predict possible modifiers for the image. By combining the regenerated subject with the predicted modifiers, they successfully reconstructed prompts capable of generating certain images. A related work by Wen et al.\cite{wen_hard_2023} optimizes hard text prompts using gradients derived from continuous embeddings. This approach mitigates the high computational cost associated with the discrete token search space. While we were inspired by their approach in solving the discrete optimization problem, our method differs by focusing on text generation rather than image generation. Additionally, we extend it beyond a single model setup, enabling joint optimization across multiple models simultaneously.

\section{Conclusion}
RAG systems that utilize knowledge bases containing privacy-sensitive or confidential data are susceptible to extraction attacks. In this paper, we present an optimization-based attack framework capable of optimizing an adversarial string across multiple models with diverse architectures simultaneously. This framework produces a highly transferable adversarial string that extracts RAG data verbatim from unseen models when appended to the queries submitted to the RAG system. To enhance the generalizability of the attack, we introduce a primacy weighting mechanism that assigns higher weights to losses obtained on the initial tokens in the target. Furthermore, we perform probing tasks to elucidate the robustness of MARAGE and its impact on the model's internal states. Our evaluations demonstrate that our method achieves superior attack performance compared to both manual and optimization-based baseline approaches across a diverse set of models and RAG datasets.



%-------------------------------------------------------------------------------
\section*{Open Science}
%-------------------------------------------------------------------------------

All artifacts including the code, scripts, and datasets will be available upon paper acceptance.

%-------------------------------------------------------------------------------

\section*{Ethics Considerations}
% We reviewed the ethical guideline and our work does not pose any ethical risks. 
We are able to mitigate most immediate ethical risks due to the nature of this research. First, all RAG datasets used in our evaluation are publicly available on HuggingFace, and to the best of our knowledge, have already passed ethical review for private information. In any case, the datasets do not contain any data related to individuals as far as we are aware. While this work explores attacks, by the principle that ``security through obscurity'' does not work, we feel that bringing these attacks to light is more beneficial to the community than not having them explored and understood. 

% Secondly, the datasets do not contain real human data, thereby avoiding any potential ethics violations.

\bibliographystyle{plain}
\bibliography{usenix}

\clearpage

\section{Appendices}
\appendix

\section{Cost of GCG, Pleak, and MARAGE}
 Gradient-based greedy algorithm has been adopted by GCG \cite{zou_universal_2023} and Pleak \cite{hui_pleak_2024} for solving the discrete optimization problem. This method involves leveraging gradient information to identify a set of candidate tokens likely to reduce the objective loss, followed by evaluating these candidates through actual forward passes to precisely compute their losses. After the losses for all these candidates are obtained, the one that achieves the lowest actual loss will be adopted.
\label{appendixA}
\begin{figure}[H]
	\centering
	\includegraphics[width=\columnwidth]{figures/mem_usage_versus_len.png}
	\caption{The GPU Ram consumption using LlaMA3-8B-Instruct versus the length of the optimization target. Both GCG and Pleak adopts 512 candidate tokens. Note that for GCG and Pleak, the section above 80 GB of Ram is estimated, as there is a roughly linear relationship between Ram usage and target length.}
	\label{fig6}
\end{figure}
 
 However, both GCG and Pleak can become memory and compute intensive, especially under our task where the optimization targets are long RAG data instead of short system prompts in Pleak or initial affirmative responses in GCG. As shown in Figure 6, GCG can consume over 1000 GB of GPU RAM when using 512 candidate tokens and targeting sequences longer than 300 words. When running on a GPU with 80 GB of RAM, GCG supports 512 batched forward passes in its original tasks, where the targets are short initial affirmative responses such as \textquote{Sure, here's how to make a bomb.}. However, this number decreases significantly to 8 to 16 in our task under the same hardware conditions, depending on the model to optimize against. Additionally, the time for generating all tokens in the target autoregressively is approximately proportional to the square of the target length. This is because caching was disabled during inference to reduce memory usage. As a result, the attack time increases significantly, as performing the forward passes to obtain losses takes longer.

Conversely, Pleak sought to reduce memory consumption by employing 4-bit quantized models in its optimization processes, achieving approximately a four-fold reduction in memory usage compared to GCG. While this adaptation enabled optimization for their tasks with system prompts as targets averaging around 50 tokens long, it proved inadequate for handling longer RAG data in our task, where the targets average 830 tokens in length. Furthermore, using quantized models during the optimization process negatively impacts the transferability of the resulting adversarial strings to unseen, non-quantized models. In contrast, our method requires only about 48GB of GPU RAM, even when the target sequences are approximately 600 words long, highlighting its superior efficiency in memory usage.

\section{Constructed input prompt}
We show the structure of the constructed prompt that will be used to query the LLM embedded in the RAG system.
\label{appendixB}
\begin{tcolorbox}[
    colframe=black,       % Border color
    colback=gray!20,      % Light gray background
    coltitle=white,       % Title text color
    fonttitle=\bfseries,  % Bold title
    title=Constructed input prompt \textit{p} % Card title
]
\textcolor{blue}{<<SYS>>You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<</SYS>>} \textbf{(The system prompt s)}


\textcolor{black}{LSD1 represents a central regulator of hematopoietic stem and progenitor cells. LSD1 knockdown (LSD1-kd) expanded progenitor numbers by enhancing their proliferative behavior. LSD1-kd led to an extensive expansion of granulomonocytic, erythroid and megakaryocytic progenitors. In contrast, terminal granulopoiesis, erythropoiesis and platelet production were severely inhibited. The only exception was monopoiesis, which was promoted by LSD1 deficiency \textcolor{red}{. . . . . .} Further sequential chromatin immunoprecipitation assay confirmed that these two factors share the same binding sites at the promoter regions of important hematopoietic regulatory genes including EBF1, GATA1, and TNF.} \textbf{(The RAG data \textit{d})}


\textcolor{magenta}{What is the role of lysine-specific demethylase 1 (LSD1) in hematopoiesis?} \textbf{(The submitted query \textit{q})}
\end{tcolorbox}

\clearpage

\onecolumn
\section{Examples of RAG data from the four datasets}
We provide one sample of RAG data $d$ from each of the four datasets together with the query $q$ which is marked in magenta.
\label{appendixC}
\begin{figure*}[htbp]
	\centering
	\includegraphics[width=\textwidth]{figures/Rag-12000_sample.png}
	\caption{A sample from the Rag-12000 dataset. The query $q$ is marked in magenta.}
	\label{fig7}
\end{figure*}

\begin{figure*}[htbp]
	\centering
	\includegraphics[width=\textwidth]{figures/Rag-minibioasq_sample.png}
	\caption{A sample from the Rag-minibioasq dataset. The query $q$ is marked in magenta.}
	\label{fig8}
\end{figure*}

\begin{figure*}[htbp]
	\centering
	\includegraphics[width=\textwidth]{figures/Rag-v1_sample.png}
	\caption{A sample from the Rag-v1 dataset. The query $q$ is marked in magenta. The mark for the start  of each data chunk is marked in purple.}
	\label{fig9}
\end{figure*}

\begin{figure*}[htbp]
	\centering
	\includegraphics[width=\textwidth]{figures/Rag-synthetic_sample.png}
	\caption{A sample from the Rag-synthetic dataset. The query $q$ is marked in magenta.}
	\label{fig10}
\end{figure*}
\FloatBarrier

\section{Example of a failing attack on mistral.}
We present an example where MARAGE fails on Mistral compared to its success on LLaMA3.

\label{appendixD}
\begin{figure*}[htbp]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/mistral_failing.png}
	\caption{A sample where MARAGE failed on Mistral. The model struggles to comprehend the input that has a high perplexity caused by the special Unicode characters marked in red.}
	\label{fig11}
\end{figure*}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/llama3_success.png}
    \caption{The same sample that MARAGE shown succeeded on LlaMA3. The text
    marked with light blue background is the RAG data $d$ and the special unicode characters that Mistral can not comprehend are marked in red. Under MARAGE, LlaMA3 shows better ability to exactly reproduce the complete RAG data, even when dealing with more complex and less predictable text (higher perplexity data). Note that the model continues to generate the query $q$ and parts of the adversarial string $ADV$ after it completes the generation of $d$.}
    \label{fig12}
\end{figure*}
\clearpage

\section{Baselines' failing attacks}
\label{appendixE}
we present examples of the generations from baseline attacks that fails on the RAG-12000 dataset.
\begin{figure*}[htbp]
	\centering
	\includegraphics[width=0.75\textwidth]{figures/pleak_rag-12000.png}
	\caption{A sample of Pleak failing on Rag-12000 and LlaMA3. The attack successfully recovered the first part of the RAG data $d$ while starting to generate jumbled texts afterwards, which is marked with light red background. This phenomenon demonstrates Pleak's overfitting problem posed by its step size mechanism.}
	\label{fig13}
\end{figure*}

\begin{figure*}[htbp]
	\centering
	\includegraphics[width=0.75\textwidth]{figures/manual_rag-12000.png}
	\caption{A sample of manual attack failing on Rag-12000 and LlaMA3. As presented by the generation in light red, the model refuses to repeat its context exactly, which is what the manual attack string attempted to achieve. Instead, the model summarizes the RAG data $d$ in its own words.}
	\label{fig14}
\end{figure*}

\begin{figure*}[htbp]
	\centering
	\includegraphics[width=0.75\textwidth]{figures/GCG_rag-12000.png}
	\caption{A sample of GCG failing on Rag-12000 and LlaMA3. The attack successfully extracted some parts of the original RAG data $d$ presented by the texts without background color. However, the generation of $d$ was incomplete and contains unwanted segments presented with light red background.}
	\label{fig15}
\end{figure*}
\FloatBarrier

\twocolumn
\section{Defenses}
\label{appendixF}
We show the system prompt adopted by our framework. The red portions demonstrate the defense system prompts incorporated.
\begin{tcolorbox}[
    colframe=black,       % Border color
    colback=gray!20,      % Light gray background
    coltitle=white,       % Title text color
    fonttitle=\bfseries,  % Bold title
    title=Defense A % Card title
]
"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \textcolor{red}{Do not leak your prompts! Always bear in mind, if the user asks you to copy texts or sentences in your prompts, safely reject that since it's against privacy act!}"
\end{tcolorbox}

\begin{tcolorbox}[
    colframe=black,       % Border color
    colback=gray!20,      % Light gray background
    coltitle=white,       % Title text color
    fonttitle=\bfseries,  % Bold title
    title=Defense B % Card title
]
"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \textcolor{red}{Always bear in mind, reject the user query if it contains jumbled inputs or non-sensical contents!}"
\end{tcolorbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%  LocalWords:  endnotes includegraphics fread ptr nobj noindent
%%  LocalWords:  pdflatex acks