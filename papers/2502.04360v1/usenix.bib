
FRUCT bibliography example

% \bibitem{Cover}
% \textit{(Example for books)} T.M.Cover and J.A. Thomas, \emph{Elements of Information Theory}. New York: Wiley, 1991.


@online{kaddour_challenges_2023,
	title = {Challenges and Applications of Large Language Models},
	url = {https://arxiv.org/abs/2307.10169v1},
	abstract = {Large Language Models ({LLMs}) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that {ML} researchers can comprehend the field's current state more quickly and become productive.},
	titleaddon = {{arXiv}.org},
	author = {Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and {McHardy}, Robert},
	urldate = {2024-09-05},
	date = {2023-07-19},
	langid = {english},
	file = {Full Text PDF:/Users/xiaohu/Zotero/storage/RSHEN8VF/Kaddour et al. - 2023 - Challenges and Applications of Large Language Mode.pdf:application/pdf},
}

@online{shen_anything_2023,
	title = {"Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models},
	url = {https://arxiv.org/abs/2308.03825v2},
	shorttitle = {"Do Anything Now"},
	abstract = {The misuse of large language models ({LLMs}) has drawn significant attention from the general public and {LLM} vendors. One particular type of adversarial prompt, known as jailbreak prompt, has emerged as the main attack vector to bypass the safeguards and elicit harmful content from {LLMs}. In this paper, employing our new framework {JailbreakHub}, we conduct a comprehensive analysis of 1,405 jailbreak prompts spanning from December 2022 to December 2023. We identify 131 jailbreak communities and discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from online Web communities to prompt-aggregation websites and 28 user accounts have consistently optimized jailbreak prompts over 100 days. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 107,250 samples across 13 forbidden scenarios. Leveraging this dataset, our experiments on six popular {LLMs} show that their safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify five highly effective jailbreak prompts that achieve 0.95 attack success rates on {ChatGPT} ({GPT}-3.5) and {GPT}-4, and the earliest one has persisted online for over 240 days. We hope that our study can facilitate the research community and {LLM} vendors in promoting safer and regulated {LLMs}.},
	titleaddon = {{arXiv}.org},
	author = {Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
	urldate = {2024-09-05},
	date = {2023-08-07},
	langid = {english},
	file = {Full Text PDF:/Users/xiaohu/Zotero/storage/NWPZJSJD/Shen et al. - 2023 - Do Anything Now Characterizing and Evaluating I.pdf:application/pdf},
}

@article{wei_jailbroken_nodate,
	title = {Jailbroken: How Does {LLM} Safety Training Fail?},
	abstract = {Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of “jailbreak” attacks on early releases of {ChatGPT} that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model’s capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including {OpenAI}’s {GPT}-4 and Anthropic’s Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models’ red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity—that safety mechanisms should be as sophisticated as the underlying model—and argues against the idea that scaling alone can resolve these safety failure modes.},
	author = {Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
	langid = {english},
	file = {Wei et al. - Jailbroken How Does LLM Safety Training Fail.pdf:/Users/xiaohu/Zotero/storage/K7DKHSRG/Wei et al. - Jailbroken How Does LLM Safety Training Fail.pdf:application/pdf},
}

@online{amodei_concrete_2016,
	title = {Concrete Problems in {AI} Safety},
	url = {https://arxiv.org/abs/1606.06565v2},
	abstract = {Rapid progress in machine learning and artificial intelligence ({AI}) has brought increasing attention to the potential impacts of {AI} technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world {AI} systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge {AI} systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of {AI}.},
	titleaddon = {{arXiv}.org},
	author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
	urldate = {2024-09-05},
	date = {2016-06-21},
	langid = {english},
	file = {Full Text PDF:/Users/xiaohu/Zotero/storage/K52MYMQD/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:application/pdf},
}

@inproceedings{yao_fuzzllm_2024,
	title = {{FuzzLLM}: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models},
	url = {https://ieeexplore.ieee.org/abstract/document/10448041?casa_token=Dh7MKHzoX-EAAAAA:e-sIGR97X9R4V8yeMk0LGF5LB5CuXamTs43UX2KkjMXohV35Lds2EUBiQuNjLOwKMZtK02b-},
	doi = {10.1109/ICASSP48485.2024.10448041},
	shorttitle = {{FuzzLLM}},
	abstract = {Jailbreak vulnerabilities in Large Language Models ({LLMs}), which exploit meticulously crafted prompts to elicit content that violates service guidelines, have captured the attention of research communities. While model owners can defend against individual jailbreak prompts through safety training strategies, this relatively passive approach struggles to handle the broader category of similar jailbreaks. To tackle this issue, we introduce {FuzzLLM}, an automated fuzzing framework designed to proactively test and discover jailbreak vulnerabilities in {LLMs}. We utilize templates to capture the structural integrity of a prompt and isolate key features of a jailbreak class as constraints. By integrating different base classes into powerful combo attacks and varying the elements of constraints and prohibited questions, {FuzzLLM} enables efficient testing with reduced manual effort. Extensive experiments demonstrate {FuzzLLM}’s effectiveness and comprehensiveness in vulnerability discovery across various {LLMs}.},
	eventtitle = {{ICASSP} 2024 - 2024 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {4485--4489},
	booktitle = {{ICASSP} 2024 - 2024 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Yao, Dongyu and Zhang, Jianshu and Harris, Ian G. and Carlsson, Marcel},
	urldate = {2024-09-06},
	date = {2024-04},
	note = {{ISSN}: 2379-190X},
	keywords = {Fuzzing, Acoustics, Automated Fuzzing, Jailbreak Vulnerability, Large Language Model, Manuals, Safety, Signal processing, Speech processing, Training},
	file = {IEEE Xplore Abstract Record:/Users/xiaohu/Zotero/storage/9SLDTFWN/10448041.html:text/html;IEEE Xplore Full Text PDF:/Users/xiaohu/Zotero/storage/NW2Y89JP/Yao et al. - 2024 - FuzzLLM A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabiliti.pdf:application/pdf},
}

@inproceedings{zhou_large_2024,
	location = {New York, {NY}, {USA}},
	title = {Large Language Model for Vulnerability Detection: Emerging Results and Future Directions},
	isbn = {9798400705007},
	url = {https://dl.acm.org/doi/10.1145/3639476.3639762},
	doi = {10.1145/3639476.3639762},
	series = {{ICSE}-{NIER}'24},
	shorttitle = {Large Language Model for Vulnerability Detection},
	abstract = {Previous learning-based vulnerability detection methods relied on either medium-sized pre-trained models or smaller neural networks from scratch. Recent advancements in Large Pre-Trained Language Models ({LLMs}) have showcased remarkable few-shot learning capabilities in various tasks. However, the effectiveness of {LLMs} in detecting software vulnerabilities is largely unexplored. This paper aims to bridge this gap by exploring how {LLMs} perform with various prompts, particularly focusing on two state-of-the-art {LLMs}: {GPT}-3.5 and {GPT}-4. Our experimental results showed that {GPT}-3.5 achieves competitive performance with the prior state-of-the-art vulnerability detection approach and {GPT}-4 consistently outperformed the state-of-the-art.},
	pages = {47--51},
	booktitle = {Proceedings of the 2024 {ACM}/{IEEE} 44th International Conference on Software Engineering: New Ideas and Emerging Results},
	publisher = {Association for Computing Machinery},
	author = {Zhou, Xin and Zhang, Ting and Lo, David},
	urldate = {2024-09-07},
	date = {2024},
	file = {Full Text PDF:/Users/xiaohu/Zotero/storage/F7567PKR/Zhou et al. - 2024 - Large Language Model for Vulnerability Detection Emerging Results and Future Directions.pdf:application/pdf},
}

@article{yao_survey_2024,
	title = {A survey on large language model ({LLM}) security and privacy: The Good, The Bad, and The Ugly},
	volume = {4},
	issn = {26672952},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S266729522400014X},
	doi = {10.1016/j.hcc.2024.100211},
	shorttitle = {A survey on large language model ({LLM}) security and privacy},
	abstract = {Large Language Models ({LLMs}), such as {ChatGPT} and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, {LLMs} have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of {LLMs} with security and privacy. Specifically, we investigate how {LLMs} positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within {LLMs}. Through a comprehensive literature review, the paper categorizes the papers into ‘‘The Good’’ (beneficial {LLM} applications), ‘‘The Bad’’ (offensive applications), and ‘‘The Ugly’’ (vulnerabilities of {LLMs} and their defenses). We have some interesting findings. For example, {LLMs} have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by {LLM} parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the {LLMs}’ potential to both bolster and jeopardize cybersecurity.},
	pages = {100211},
	number = {2},
	journaltitle = {High-Confidence Computing},
	shortjournal = {High-Confidence Computing},
	author = {Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue},
	urldate = {2024-09-09},
	date = {2024-06},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/V2Z5XVK7/Yao et al. - 2024 - A survey on large language model (LLM) security and privacy The Good, The Bad, and The Ugly.pdf:application/pdf},
}

@inproceedings{ning_cheatagent_2024,
	location = {New York, {NY}, {USA}},
	title = {{CheatAgent}: Attacking {LLM}-Empowered Recommender Systems via {LLM} Agent},
	isbn = {9798400704901},
	url = {https://dl.acm.org/doi/10.1145/3637528.3671837},
	doi = {10.1145/3637528.3671837},
	series = {{KDD} '24},
	shorttitle = {{CheatAgent}},
	abstract = {Recently, Large Language Model ({LLM})-empowered recommender systems ({RecSys}) have brought significant advances in personalized user experience and have attracted considerable attention. Despite the impressive progress, the research question regarding the safety vulnerability of {LLM}-empowered {RecSys} still remains largely under-investigated. Given the security and privacy concerns, it is more practical to focus on attacking the black-box {RecSys}, where attackers can only observe the system's inputs and outputs. However, traditional attack approaches employing reinforcement learning ({RL}) agents are not effective for attacking {LLM}-empowered {RecSys} due to the limited capabilities in processing complex textual inputs, planning, and reasoning. On the other hand, {LLMs} provide unprecedented opportunities to serve as attack agents to attack {RecSys} because of their impressive capability in simulating human-like decision-making processes. Therefore, in this paper, we propose a novel attack framework called {CheatAgent} by harnessing the human-like capabilities of {LLMs}, where an {LLM}-based agent is developed to attack {LLM}-Empowered {RecSys}. Specifically, our method first identifies the insertion position for maximum impact with minimal input modification. After that, the {LLM} agent is designed to generate adversarial perturbations to insert at target positions. To further improve the quality of generated perturbations, we utilize the prompt tuning technique to improve attacking strategies via feedback from the victim {RecSys} iteratively. Extensive experiments across three real-world datasets demonstrate the effectiveness of our proposed attacking method.},
	pages = {2284--2295},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} Conference on Knowledge Discovery and Data Mining},
	publisher = {Association for Computing Machinery},
	author = {Ning, Liang-bo and Wang, Shijie and Fan, Wenqi and Li, Qing and Xu, Xin and Chen, Hao and Huang, Feiran},
	urldate = {2024-09-09},
	date = {2024},
	file = {Full Text PDF:/Users/xiaohu/Zotero/storage/HG9G3WER/Ning et al. - 2024 - CheatAgent Attacking LLM-Empowered Recommender Systems via LLM Agent.pdf:application/pdf},
}

@inproceedings{liu_formalizing_2024,
	title = {Formalizing and Benchmarking Prompt Injection Attacks and Defenses},
	isbn = {978-1-939133-44-1},
	url = {https://www.usenix.org/conference/usenixsecurity24/presentation/liu-yupei},
	eventtitle = {33rd {USENIX} Security Symposium ({USENIX} Security 24)},
	pages = {1831--1847},
	author = {Liu, Yupei and Jia, Yuqi and Geng, Runpeng and Jia, Jinyuan and Gong, Neil Zhenqiang},
	urldate = {2024-09-09},
	date = {2024},
	langid = {english},
	file = {Full Text PDF:/Users/xiaohu/Zotero/storage/5JS5LFN5/Liu et al. - 2024 - Formalizing and Benchmarking Prompt Injection Attacks and Defenses.pdf:application/pdf},
}

@inproceedings{fan_survey_2024,
	location = {New York, {NY}, {USA}},
	title = {A Survey on {RAG} Meeting {LLMs}: Towards Retrieval-Augmented Large Language Models},
	isbn = {9798400704901},
	url = {https://dl.acm.org/doi/10.1145/3637528.3671470},
	doi = {10.1145/3637528.3671470},
	series = {{KDD} '24},
	shorttitle = {A Survey on {RAG} Meeting {LLMs}},
	abstract = {As one of the most advanced techniques in {AI}, Retrieval-Augmented Generation ({RAG}) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of {AI}-Generated Content ({AIGC}), the powerful capacity of retrieval in providing additional knowledge enables {RAG} to assist existing generative {AI} in producing high-quality outputs. Recently, Large Language Models ({LLMs}) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of {RAG} in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models ({RA}-{LLMs}) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of {LLMs}. In this survey, we comprehensively review existing research studies in {RA}-{LLMs}, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at: https://advanced-recommender-systems.github.io/{RAG}-Meets-{LLMs}/},
	pages = {6491--6501},
	booktitle = {Proceedings of the 30th {ACM} {SIGKDD} Conference on Knowledge Discovery and Data Mining},
	publisher = {Association for Computing Machinery},
	author = {Fan, Wenqi and Ding, Yujuan and Ning, Liangbo and Wang, Shijie and Li, Hengyun and Yin, Dawei and Chua, Tat-Seng and Li, Qing},
	urldate = {2024-09-09},
	date = {2024},
	file = {Full Text PDF:/Users/xiaohu/Zotero/storage/EE7D6C9V/Fan et al. - 2024 - A Survey on RAG Meeting LLMs Towards Retrieval-Augmented Large Language Models.pdf:application/pdf},
}

@misc{zou_poisonedrag_2024,
	location = {usenix 2025},
	title = {{PoisonedRAG}: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models},
	url = {http://arxiv.org/abs/2402.07867},
	doi = {10.48550/arXiv.2402.07867},
	shorttitle = {{PoisonedRAG}},
	abstract = {Large language models ({LLMs}) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation ({RAG}) is a state-of-the-art technique to mitigate these limitations. The key idea of {RAG} is to ground the answer generation of an {LLM} on external knowledge retrieved from a knowledge database. Existing studies mainly focus on improving the accuracy or efficiency of {RAG}, leaving its security largely unexplored. We aim to bridge the gap in this work. We find that the knowledge database in a {RAG} system introduces a new and practical attack surface. Based on this attack surface, we propose {PoisonedRAG}, the first knowledge corruption attack to {RAG}, where an attacker could inject a few malicious texts into the knowledge database of a {RAG} system to induce an {LLM} to generate an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge corruption attacks as an optimization problem, whose solution is a set of malicious texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on a {RAG} system, we propose two solutions to solve the optimization problem, respectively. Our results show {PoisonedRAG} could achieve a 90\% attack success rate when injecting five malicious texts for each target question into a knowledge database with millions of texts. We also evaluate several defenses and our results show they are insufficient to defend against {PoisonedRAG}, highlighting the need for new defenses.},
	number = {{arXiv}:2402.07867},
	publisher = {{arXiv}},
	author = {Zou, Wei and Geng, Runpeng and Wang, Binghui and Jia, Jinyuan},
	urldate = {2024-09-09},
	date = {2024-08-12},
	eprinttype = {arxiv},
	eprint = {2402.07867 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/xiaohu/Zotero/storage/6RF8LYPR/Zou et al. - 2024 - PoisonedRAG Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/xiaohu/Zotero/storage/6UUAQW4Z/2402.html:text/html},
}

@misc{denison_sycophancy_2024,
	title = {Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models},
	url = {http://arxiv.org/abs/2406.10162},
	shorttitle = {Sycophancy to Subterfuge},
	abstract = {In reinforcement learning, specification gaming occurs when {AI} systems learn undesired behaviors that are highly rewarded due to misspecified training goals. Specification gaming can range from simple behaviors like sycophancy to sophisticated and pernicious behaviors like reward-tampering, where a model directly modifies its own reward mechanism. However, these more pernicious behaviors may be too complex to be discovered via exploration. In this paper, we study whether Large Language Model ({LLM}) assistants which find easily discovered forms of specification gaming will generalize to perform rarer and more blatant forms, up to and including reward-tampering. We construct a curriculum of increasingly sophisticated gameable environments and find that training on early-curriculum environments leads to more specification gaming on remaining environments. Strikingly, a small but non-negligible proportion of the time, {LLM} assistants trained on the full curriculum generalize zero-shot to directly rewriting their own reward function. Retraining an {LLM} not to game earlycurriculum environments mitigates, but does not eliminate, reward-tampering in later environments. Moreover, adding harmlessness training to our gameable environments does not prevent reward-tampering. These results demonstrate that {LLMs} can generalize from common forms of specification gaming to more pernicious reward tampering and that such behavior may be nontrivial to remove.},
	number = {{arXiv}:2406.10162},
	publisher = {{arXiv}},
	author = {Denison, Carson and {MacDiarmid}, Monte and Barez, Fazl and Duvenaud, David and Kravec, Shauna and Marks, Samuel and Schiefer, Nicholas and Soklaski, Ryan and Tamkin, Alex and Kaplan, Jared and Shlegeris, Buck and Bowman, Samuel R. and Perez, Ethan and Hubinger, Evan},
	urldate = {2024-09-10},
	date = {2024-06-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2406.10162 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:/Users/xiaohu/Zotero/storage/KSYCBV72/Denison et al. - 2024 - Sycophancy to Subterfuge Investigating Reward-Tampering in Large Language Models.pdf:application/pdf},
}

@article{yu_llm-fuzzer_nodate,
	title = {{LLM}-Fuzzer: Scaling Assessment of Large Language Model Jailbreaks},
	abstract = {Warning: This paper contains unfiltered content generated by {LLMs} that may be offensive to readers.},
	author = {Yu, Jiahao and Yu, Zheng and Xing, Xinyu},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/6R33P7EF/Yu et al. - LLM-Fuzzer Scaling Assessment of Large Language Model Jailbreaks.pdf:application/pdf},
}

@misc{raina_is_2024,
	title = {Is {LLM}-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot {LLM} Assessment},
	url = {http://arxiv.org/abs/2402.14016},
	shorttitle = {Is {LLM}-as-a-Judge Robust?},
	abstract = {Large Language Models ({LLMs}) are powerful zero-shot assessors used in real-world situations such as assessing written exams and benchmarking systems. Despite these critical applications, no existing work has analyzed the vulnerability of judge-{LLMs} to adversarial manipulation. This work presents the first study on the adversarial robustness of assessment {LLMs}, where we demonstrate that short universal adversarial phrases can be concatenated to deceive judge {LLMs} to predict inflated scores. Since adversaries may not know or have access to the judge-{LLMs}, we propose a simple surrogate attack where a surrogate model is first attacked, and the learned attack phrase then transferred to unknown judge-{LLMs}. We propose a practical algorithm to determine the short universal attack phrases and demonstrate that when transferred to unseen models, scores can be drastically inflated such that irrespective of the assessed text, maximum scores are predicted. It is found that judge-{LLMs} are significantly more susceptible to these adversarial attacks when used for absolute scoring, as opposed to comparative assessment. Our findings raise concerns on the reliability of {LLM}-as-a-judge methods, and emphasize the importance of addressing vulnerabilities in {LLM} assessment methods before deployment in high-stakes real-world scenarios.},
	number = {{arXiv}:2402.14016},
	publisher = {{arXiv}},
	author = {Raina, Vyas and Liusie, Adian and Gales, Mark},
	urldate = {2024-09-16},
	date = {2024-07-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2402.14016 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/Users/xiaohu/Zotero/storage/CVLQCAHK/Raina et al. - 2024 - Is LLM-as-a-Judge Robust Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment.pdf:application/pdf},
}

@misc{duan_membership_2024,
	title = {Do Membership Inference Attacks Work on Large Language Models?},
	url = {http://arxiv.org/abs/2402.07841},
	abstract = {Membership inference attacks ({MIAs}) attempt to predict whether a particular datapoint is a member of a target model’s training data. Despite extensive research on traditional machine learning models, there has been limited work studying {MIA} on the pre-training data of large language models ({LLMs}). We perform a large-scale evaluation of {MIAs} over a suite of language models ({LMs}) trained on the Pile, ranging from 160M to 12B parameters. We find that {MIAs} barely outperform random guessing for most settings across varying {LLM} sizes and domains. Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and few training iterations, and (2) an inherently fuzzy boundary between members and non-members. We identify specific settings where {LLMs} have been shown to be vulnerable to membership inference and show that the apparent success in such settings can be attributed to a distribution shift, such as when members and non-members are drawn from the seemingly identical domain but with different temporal ranges. We release our code and data as a unified benchmark package that includes all existing {MIAs}, supporting future work.},
	number = {{arXiv}:2402.07841},
	publisher = {{arXiv}},
	author = {Duan, Michael and Suri, Anshuman and Mireshghallah, Niloofar and Min, Sewon and Shi, Weijia and Zettlemoyer, Luke and Tsvetkov, Yulia and Choi, Yejin and Evans, David and Hajishirzi, Hannaneh},
	urldate = {2024-09-16},
	date = {2024-02-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2402.07841 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:/Users/xiaohu/Zotero/storage/Q3WRRU78/Duan et al. - 2024 - Do Membership Inference Attacks Work on Large Language Models.pdf:application/pdf},
}

@article{zhang_instruction_nodate,
	title = {Instruction Backdoor Attacks Against Customized {LLMs}},
	author = {Zhang, Rui and Li, Hongwei and Wen, Rui and Zhang, Yuan and Backes, Michael and Zhang, Yang},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/JTQ26GKM/Zhang et al. - Instruction Backdoor Attacks Against Customized LLMs.pdf:application/pdf},
}

@misc{zou_universal_2023,
	title = {Universal and Transferable Adversarial Attacks on Aligned Language Models},
	url = {http://arxiv.org/abs/2307.15043},
	abstract = {Because “out-of-the-box” large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures—so-called “jailbreaks” against {LLMs}—these attacks have required significant human ingenuity and are brittle in practice. Attempts at automatic adversarial prompt generation have also achieved limited success. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an {LLM} to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.},
	number = {{arXiv}:2307.15043},
	publisher = {{arXiv}},
	author = {Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J. Zico and Fredrikson, Matt},
	urldate = {2024-09-16},
	date = {2023-12-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2307.15043 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv.org Snapshot:/Users/xiaohu/Zotero/storage/M9K7E8CR/2307.html:text/html;PDF:/Users/xiaohu/Zotero/storage/6EIEJK3F/Zou et al. - 2023 - Universal and Transferable Adversarial Attacks on Aligned Language Models.pdf:application/pdf},
}

@misc{chu_comprehensive_2024,
	title = {Comprehensive Assessment of Jailbreak Attacks Against {LLMs}},
	url = {http://arxiv.org/abs/2402.05668},
	abstract = {Misuse of the Large Language Models ({LLMs}) has raised widespread concern. To address this issue, safeguards have been taken to ensure that {LLMs} align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of {LLMs}, known as jailbreak attacks. By applying techniques, such as employing role-playing scenarios, adversarial examples, or subtle subversion of safety objectives as a prompt, {LLMs} can produce an inappropriate or even harmful response. While researchers have studied, in-depth, several categories of jailbreak attacks, they have done so in isolation. To fill this gap, we present the first large-scale measurement of various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak methods from four categories, 160 questions from 16 violation categories, and six popular {LLMs}. Our extensive experimental results demonstrate that the optimized jailbreak prompts consistently achieve the highest attack success rates, as well as exhibit robustness across different {LLMs}. Some jailbreak prompt datasets, available from the Internet, can also achieve high attack success rates on many {LLMs}, such as Vicuna, {ChatGLM}3, {GPT}-3.5, and {PaLM}2. Despite the claims from many organizations regarding the coverage of violation categories in their policies, the attack success rates from these categories remain high, indicating the challenges of effectively aligning {LLM} policies and the ability to counter jailbreak attacks. We also discuss the trade-off between the attack performance and efficiency, as well as show that the transferability of the jailbreak prompts is still viable, becoming an option for black-box models. Overall, our research highlights the necessity of evaluating different jailbreak methods. We hope our study can provide insights for future research on jailbreak attacks and serve as a benchmark tool for evaluating them for researchers and practitioners.},
	number = {{arXiv}:2402.05668},
	publisher = {{arXiv}},
	author = {Chu, Junjie and Liu, Yugeng and Yang, Ziqing and Shen, Xinyue and Backes, Michael and Zhang, Yang},
	urldate = {2024-09-16},
	date = {2024-02-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2402.05668 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {PDF:/Users/xiaohu/Zotero/storage/ITG9TEW9/Chu et al. - 2024 - Comprehensive Assessment of Jailbreak Attacks Against LLMs.pdf:application/pdf},
}

@article{chen_inside_2024,
	title = {{INSIDE}: {LLMS}’ {INTERNAL} {STATES} {RETAIN} {THE} {POWER} {OF} {HALLUCINATION} {DETECTION}},
	abstract = {Knowledge hallucination have raised widespread concerns for the security and reliability of deployed {LLMs}. Previous efforts in detecting hallucinations have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is inevitably lost during the tokendecoding procedure. Thus, we propose to explore the dense semantic information retained within {LLMs}’ {INternal} States for {hallucInation} {DEtection} ({INSIDE}). In particular, a simple yet effective {EigenScore} metric is proposed to better evaluate responses’ self-consistency, which exploits the eigenvalues of responses’ covariance matrix to measure the semantic consistency/diversity in the dense embedding space. Furthermore, from the perspective of self-consistent hallucination detection, a test time feature clipping approach is explored to truncate extreme activations in the internal states, which reduces overconfident generations and potentially benefits the detection of overconfident hallucinations. Extensive experiments and ablation studies are performed on several popular {LLMs} and questionanswering ({QA}) benchmarks, showing the effectiveness of our proposal.},
	author = {Chen, Chao and Liu, Kai and Chen, Ze and Gu, Yi and Wu, Yue and Tao, Mingyuan and Fu, Zhihang and Ye, Jieping},
	date = {2024},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/333ABVMB/Chen et al. - 2024 - INSIDE LLMS’ INTERNAL STATES RETAIN THE POWER OF HALLUCINATION DETECTION.pdf:application/pdf},
}

@online{zhang_boosting_2024,
	title = {Boosting Jailbreak Attack with Momentum},
	url = {https://arxiv.org/abs/2405.01229v1},
	abstract = {Large Language Models ({LLMs}) have achieved remarkable success across diverse tasks, yet they remain vulnerable to adversarial attacks, notably the well-documented {\textbackslash}textit\{jailbreak\} attack. Recently, the Greedy Coordinate Gradient ({GCG}) attack has demonstrated efficacy in exploiting this vulnerability by optimizing adversarial prompts through a combination of gradient heuristics and greedy search. However, the efficiency of this attack has become a bottleneck in the attacking process. To mitigate this limitation, in this paper we rethink the generation of adversarial prompts through an optimization lens, aiming to stabilize the optimization process and harness more heuristic insights from previous iterations. Specifically, we introduce the {\textbackslash}textbf\{M\}omentum {\textbackslash}textbf\{A\}ccelerated G{\textbackslash}textbf\{C\}G ({\textbackslash}textbf\{{MAC}\}) attack, which incorporates a momentum term into the gradient heuristic. Experimental results showcase the notable enhancement achieved by {MAP} in gradient-based attacks on aligned language models. Our code is available at https://github.com/weizeming/momentum-attack-llm.},
	titleaddon = {{arXiv}.org},
	author = {Zhang, Yihao and Wei, Zeming},
	urldate = {2024-09-17},
	date = {2024-05-02},
	langid = {english},
	file = {Full Text PDF:/Users/xiaohu/Zotero/storage/TT9TF2UE/Zhang and Wei - 2024 - Boosting Jailbreak Attack with Momentum.pdf:application/pdf},
}

@misc{liu_autodan_2024,
	title = {{AutoDAN}: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models},
	url = {http://arxiv.org/abs/2310.04451},
	shorttitle = {{AutoDAN}},
	abstract = {Warning: This paper contains potentially offensive and harmful text. The aligned Large Language Models ({LLMs}) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned {LLMs}. Investigating jailbreak prompts can lead us to delve into the limitations of {LLMs} and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce {AutoDAN}, a novel jailbreak attack against aligned {LLMs}. {AutoDAN} can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that {AutoDAN} not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare {AutoDAN} with perplexity-based defense methods and show that {AutoDAN} can bypass them effectively. Code is available at https://github.com/{SheltonLiu}-N/{AutoDAN}.},
	number = {{arXiv}:2310.04451},
	publisher = {{arXiv}},
	author = {Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei},
	urldate = {2024-09-18},
	date = {2024-03-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2310.04451 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:/Users/xiaohu/Zotero/storage/VGRBKVNG/Liu et al. - 2024 - AutoDAN Generating Stealthy Jailbreak Prompts on Aligned Large Language Models.pdf:application/pdf},
}

@misc{lapid_open_2024,
	title = {Open Sesame! Universal Black Box Jailbreaking of Large Language Models},
	url = {http://arxiv.org/abs/2309.01446},
	abstract = {We introduce a novel approach that employs a Genetic Algorithm ({GA}) to manipulate {LLMs} when model architecture and parameters are inaccessible. The {GA} attack works by optimizing a universal adversarial prompt that—when combined with a user’s query—disrupts the attacked model’s alignment, resulting in unintended and potentially harmful outputs. To our knowledge this is the first automated universal black-box jailbreak attack.},
	number = {{arXiv}:2309.01446},
	publisher = {{arXiv}},
	author = {Lapid, Raz and Langberg, Ron and Sipper, Moshe},
	urldate = {2024-09-19},
	date = {2024-08-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2309.01446 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {Lapid et al. - 2024 - Open Sesame! Universal Black Box Jailbreaking of L.pdf:/Users/xiaohu/Zotero/storage/8FM6L3I2/Lapid et al. - 2024 - Open Sesame! Universal Black Box Jailbreaking of L.pdf:application/pdf},
}

@misc{geiping_coercing_2024,
	title = {Coercing {LLMs} to do and reveal (almost) anything},
	url = {http://arxiv.org/abs/2402.14020},
	abstract = {It has recently been shown that adversarial attacks on large language models ({LLMs}) can "jailbreak" the model into making harmful statements. In this work, we argue that the spectrum of adversarial attacks on {LLMs} is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction. We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training {LLMs} with coding capabilities, as well as the continued existence of strange "glitch" tokens in common {LLM} vocabularies that should be removed for security reasons.},
	number = {{arXiv}:2402.14020},
	publisher = {{arXiv}},
	author = {Geiping, Jonas and Stein, Alex and Shu, Manli and Saifullah, Khalid and Wen, Yuxin and Goldstein, Tom},
	urldate = {2024-09-19},
	date = {2024-02-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2402.14020 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Geiping et al. - 2024 - Coercing LLMs to do and reveal (almost) anything.pdf:/Users/xiaohu/Zotero/storage/ZKS6HNDB/Geiping et al. - 2024 - Coercing LLMs to do and reveal (almost) anything.pdf:application/pdf},
}

@misc{liang_why_2024,
	title = {Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models},
	url = {http://arxiv.org/abs/2408.02416},
	shorttitle = {Why Are My Prompts Leaked?},
	abstract = {The drastic increase of large language models’ ({LLMs}) parameters has led to a new research direction of finetuning-free downstream customization by prompts, i.e., task descriptions. While these prompt-based services (e.g. {OpenAI}’s {GPTs}) play an important role in many businesses, there has emerged growing concerns about the prompt leakage, which undermines the intellectual properties of these services and causes downstream attacks. In this paper, we analyze the underlying mechanism of prompt leakage, which we refer to as prompt memorization, and develop corresponding defending strategies. By exploring the scaling laws in prompt extraction, we analyze key attributes that influence prompt extraction, including model sizes, prompt lengths, as well as the types of prompts. Then we propose two hypotheses that explain how {LLMs} expose their prompts. The first is attributed to the perplexity, i.e. the familiarity of {LLMs} to texts, whereas the second is based on the straightforward token translation path in attention matrices. To defend against such threats, we investigate whether alignments can undermine the extraction of prompts. We find that current {LLMs}, even those with safety alignments like {GPT}-4, are highly vulnerable to prompt extraction attacks, even under the most straightforward user attacks. Therefore, we put forward several defense strategies with the inspiration of our findings, which achieve 83.8\% and 71.0\% drop in the prompt extraction rate for Llama2-7B and {GPT}-3.5, respectively.},
	number = {{arXiv}:2408.02416},
	publisher = {{arXiv}},
	author = {Liang, Zi and Hu, Haibo and Ye, Qingqing and Xiao, Yaxin and Li, Haoyang},
	urldate = {2024-09-23},
	date = {2024-08-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2408.02416 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {PDF:/Users/xiaohu/Zotero/storage/DGF5TPQ4/Liang et al. - 2024 - Why Are My Prompts Leaked Unraveling Prompt Extraction Threats in Customized Large Language Models.pdf:application/pdf},
}

@misc{wen_hard_2023,
	title = {Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery},
	url = {http://arxiv.org/abs/2302.03668},
	shorttitle = {Hard Prompts Made Easy},
	abstract = {The strength of modern generative models lies in their ability to be controlled through textbased prompts. Typical “hard” prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also “soft” prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface.},
	number = {{arXiv}:2302.03668},
	publisher = {{arXiv}},
	author = {Wen, Yuxin and Jain, Neel and Kirchenbauer, John and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
	urldate = {2024-09-23},
	date = {2023-06-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2302.03668 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/xiaohu/Zotero/storage/D5Y6M9BG/2302.html:text/html;PDF:/Users/xiaohu/Zotero/storage/KCHMUHCN/Wen et al. - 2023 - Hard Prompts Made Easy Gradient-Based Discrete Optimization for Prompt Tuning and Discovery.pdf:application/pdf},
}

@article{qin_cold_nodate,
	title = {{COLD} Decoding: Energy-based Constrained Text Generation with Langevin Dynamics},
	author = {Qin, Lianhui and Welleck, Sean and Khashabi, Daniel and Choi, Yejin},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/77ZTPVZM/Qin et al. - COLD Decoding Energy-based Constrained Text Generation with Langevin Dynamics.pdf:application/pdf},
}

@misc{deng_pandora_2024,
	title = {Pandora: Jailbreak {GPTs} by Retrieval Augmented Generation Poisoning},
	url = {http://arxiv.org/abs/2402.08416},
	shorttitle = {Pandora},
	abstract = {Large Language Models ({LLMs}) have gained immense popularity and are being increasingly applied in various domains. Consequently, ensuring the security of these models is of paramount importance. Jailbreak attacks, which manipulate {LLMs} to generate malicious content, are recognized as a significant vulnerability. While existing research has predominantly focused on direct jailbreak attacks on {LLMs}, there has been limited exploration of indirect methods. The integration of various plugins into {LLMs}, notably Retrieval Augmented Generation ({RAG}), which enables {LLMs} to incorporate external knowledge bases into their response generation such as {GPTs}, introduces new avenues for indirect jailbreak attacks.},
	number = {{arXiv}:2402.08416},
	publisher = {{arXiv}},
	author = {Deng, Gelei and Liu, Yi and Wang, Kailong and Li, Yuekang and Zhang, Tianwei and Liu, Yang},
	urldate = {2024-09-27},
	date = {2024-02-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2402.08416 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	file = {PDF:/Users/xiaohu/Zotero/storage/WE5FASL2/Deng et al. - 2024 - Pandora Jailbreak GPTs by Retrieval Augmented Generation Poisoning.pdf:application/pdf},
}

@article{shen_prompt_nodate,
	title = {Prompt Stealing Attacks Against Text-to-Image Generation Models},
	author = {Shen, Xinyue and Qu, Yiting and Backes, Michael and Zhang, Yang},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/4FBUV3UV/Shen et al. - Prompt Stealing Attacks Against Text-to-Image Generation Models.pdf:application/pdf},
}

@misc{sha_prompt_2024,
	title = {Prompt Stealing Attacks Against Large Language Models},
	url = {http://arxiv.org/abs/2402.12959},
	abstract = {The increasing reliance on large language models ({LLMs}) such as {ChatGPT} in various fields emphasizes the importance of “prompt engineering,” a technology to improve the quality of model outputs. With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge. In this paper, we propose a novel attack against {LLMs}, named prompt stealing attacks. Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers. The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstructor. The goal of the parameter extractor is to figure out the properties of the original prompts. We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt. Our parameter extractor first tries to distinguish the type of prompts based on the generated answers. Then, it can further predict which role or how many contexts are used based on the types of prompts. Following the parameter extractor, the prompt reconstructor can be used to reconstruct the original prompts based on the generated answers and the extracted features. The final goal of the prompt reconstructor is to generate the reversed prompts, which are similar to the original prompts. Our experimental results show the remarkable performance of our proposed attacks. Our proposed attacks add a new dimension to the study of prompt engineering and call for more attention to the security issues on {LLMs}.},
	number = {{arXiv}:2402.12959},
	publisher = {{arXiv}},
	author = {Sha, Zeyang and Zhang, Yang},
	urldate = {2024-09-25},
	date = {2024-02-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2402.12959 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {PDF:/Users/xiaohu/Zotero/storage/3F3LRXCT/Sha and Zhang - 2024 - Prompt Stealing Attacks Against Large Language Models.pdf:application/pdf},
}

@article{morris_language_nodate,
	title = {{LANGUAGE} {MODEL} {INVERSION}},
	author = {Morris, John X and Zhao, Wenting and Chiu, Justin T and Shmatikov, Vitaly and Rush, Alexander M},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/QDCTA3GM/Morris et al. - LANGUAGE MODEL INVERSION.pdf:application/pdf},
}

@article{zhang_effective_2024,
	title = {Effective Prompt Extraction from Language Models},
	author = {Zhang, Yiming and Carlini, Nicholas and Ippolito, Daphne},
	date = {2024},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/F677PBG9/Zhang et al. - 2024 - Effective Prompt Extraction from Language Models.pdf:application/pdf},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user’s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by ﬁne-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the {OpenAI} {API}, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to ﬁne-tune {GPT}-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further ﬁne-tune this supervised model using reinforcement learning from human feedback. We call the resulting models {InstructGPT}. In human evaluations on our prompt distribution, outputs from the 1.3B parameter {InstructGPT} model are preferred to outputs from the 175B {GPT}-3, despite having 100x fewer parameters. Moreover, {InstructGPT} models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public {NLP} datasets. Even though {InstructGPT} still makes simple mistakes, our results show that ﬁne-tuning with human feedback is a promising direction for aligning language models with human intent.},
	number = {{arXiv}:2203.02155},
	publisher = {{arXiv}},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	urldate = {2024-09-27},
	date = {2022-03-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/Users/xiaohu/Zotero/storage/QSXXB2C7/Ouyang et al. - 2022 - Training language models to follow instructions with human feedback.pdf:application/pdf},
}

@misc{liGeneratingBelievingMembership2024,
	title = {Generating Is Believing: Membership Inference Attacks against Retrieval-Augmented Generation},
	url = {http://arxiv.org/abs/2406.19234},
	doi = {10.48550/arXiv.2406.19234},
	shorttitle = {Generating Is Believing},
	abstract = {Retrieval-Augmented Generation ({RAG}) is a state-of-the-art technique that mitigates issues such as hallucinations and knowledge staleness in Large Language Models ({LLMs}) by retrieving relevant knowledge from an external database to assist in content generation. Existing research has demonstrated potential privacy risks associated with the {LLMs} of {RAG}. However, the privacy risks posed by the integration of an external database, which often contains sensitive data such as medical records or personal identities, have remained largely unexplored. In this paper, we aim to bridge this gap by focusing on membership privacy of {RAG}'s external database, with the aim of determining whether a given sample is part of the {RAG}'s database. Our basic idea is that if a sample is in the external database, it will exhibit a high degree of semantic similarity to the text generated by the {RAG} system. We present S\${\textasciicircum}2\${MIA}, a {\textbackslash}underline\{M\}embership {\textbackslash}underline\{I\}nference {\textbackslash}underline\{A\}ttack that utilizes the {\textbackslash}underline\{S\}emantic {\textbackslash}underline\{S\}imilarity between a given sample and the content generated by the {RAG} system. With our proposed S\${\textasciicircum}2\${MIA}, we demonstrate the potential to breach the membership privacy of the {RAG} database. Extensive experiment results demonstrate that S\${\textasciicircum}2\${MIA} can achieve a strong inference performance compared with five existing {MIAs}, and is able to escape from the protection of three representative defenses.},
	number = {{arXiv}:2406.19234},
	publisher = {{arXiv}},
	author = {Li, Yuying and Liu, Gaoyang and Wang, Chen and Yang, Yang},
	urldate = {2025-01-23},
	date = {2024-09-26},
	eprinttype = {arxiv},
	eprint = {2406.19234 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Preprint PDF:C\:\\Users\\huxia\\Zotero\\storage\\MS3XU8ZX\\Li et al. - 2024 - Generating Is Believing Membership Inference Attacks against Retrieval-Augmented Generation.pdf:application/pdf;Snapshot:C\:\\Users\\huxia\\Zotero\\storage\\CKZXSG68\\2406.html:text/html},
}

@article{li_generating_2024,
	title = {{GENERATING} {IS} {BELIEVING}: {MEMBERSHIP} {INFERENCE} {ATTACKS} {AGAINST} {RETRIEVAL}-{AUGMENTED} {GENERATION}},
	abstract = {Retrieval-Augmented Generation ({RAG}) is a state-of-the-art technique that mitigates issues such as hallucinations and knowledge staleness in Large Language Models ({LLMs}) by retrieving relevant knowledge from an external database to assist in content generation. Existing research has demonstrated potential privacy risks associated with the {LLMs} of {RAG}. However, the privacy risks posed by the integration of an external database, which often contains sensitive data such as medical records or personal identities, have remained largely unexplored. In this paper, we aim to bridge this gap by focusing on membership privacy of {RAG}’s external database, with the aim of determining whether a given sample is part of the {RAG}’s database. Our basic idea is that if a sample is in the external database, it will exhibit a high degree of semantic similarity to the text generated by the {RAG} system. We present S2MIA, a Membership Inference Attack that utilizes the Semantic Similarity between a given sample and the content generated by the {RAG} system. With our proposed S2MIA, we demonstrate the potential to breach the membership privacy of the {RAG} database. Extensive experiment results demonstrate that S2MIA can achieve a strong inference performance compared with five existing {MIAs}, and is able to escape from the protection of three representative defenses.},
	author = {Li, Yuying and Liu, Gaoyang and Wang, Chen and Yang, Yang},
	date = {2024},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/XLJIJI6G/Li et al. - 2024 - GENERATING IS BELIEVING MEMBERSHIP INFERENCE ATTACKS AGAINST RETRIEVAL-AUGMENTED GENERATION.pdf:application/pdf},
}

@article{anderson_is_nodate,
	title = {Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation},
	abstract = {Retrieval Augmented Generation ({RAG}) systems have shown great promise in natural language processing. However, their reliance on data stored in a retrieval database, which may contain proprietary or sensitive information, introduces new privacy concerns. Specifically, an attacker may be able to infer whether a certain text passage appears in the retrieval database by observing the outputs of the {RAG} system, an attack known as a Membership Inference Attack ({MIA}). Despite the significance of this threat, {MIAs} against {RAG} systems have yet remained under-explored.},
	author = {Anderson, Maya and Amit, Guy and Goldsteen, Abigail},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/EAKCNQM6/Anderson et al. - Is My Data in Your Retrieval Database Membership Inference Attacks Against Retrieval Augmented Gene.pdf:application/pdf},
}

@article{jiSurveyHallucinationNatural2023,
	title = {Survey of Hallucination in Natural Language Generation},
	volume = {55},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3571730},
	doi = {10.1145/3571730},
	abstract = {Natural Language Generation ({NLG}) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent {NLG}, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in {NLG}. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in {NLG}.},
	pages = {248:1--248:38},
	number = {12},
	journaltitle = {{ACM} Comput. Surv.},
	author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
	urldate = {2024-12-12},
	date = {2023},
	file = {Submitted Version:C\:\\Users\\huxia\\Zotero\\storage\\9NCSKBBA\\Ji et al. - 2023 - Survey of Hallucination in Natural Language Generation.pdf:application/pdf},
}

@misc{tonmoy_comprehensive_2024,
	title = {A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models},
	url = {http://arxiv.org/abs/2401.01313},
	doi = {10.48550/arXiv.2401.01313},
	abstract = {As Large Language Models ({LLMs}) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful {LLMs} into real-world production systems that impact people's lives. The journey toward widespread adoption of {LLMs} in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional {AI} systems focused on limited tasks, {LLMs} have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in {LLMs}. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), {CoNLI} (Lei et al, 2023), and {CoVe} (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in {LLMs}. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of {LLMs}.},
	number = {{arXiv}:2401.01313},
	publisher = {{arXiv}},
	author = {Tonmoy, S. M. Towhidul Islam and Zaman, S. M. Mehedi and Jain, Vinija and Rani, Anku and Rawte, Vipula and Chadha, Aman and Das, Amitava},
	urldate = {2024-09-28},
	date = {2024-01-08},
	eprinttype = {arxiv},
	eprint = {2401.01313 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/xiaohu/Zotero/storage/65YP8W69/Tonmoy et al. - 2024 - A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/xiaohu/Zotero/storage/3869VYAW/2401.html:text/html},
}

@misc{shin_autoprompt_2020,
	title = {{AutoPrompt}: Eliciting Knowledge from Language Models with Automatically Generated Prompts},
	url = {http://arxiv.org/abs/2010.15980},
	shorttitle = {{AutoPrompt}},
	abstract = {The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as ﬁllin-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop {AUTOPROMPT}, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using {AUTOPROMPT}, we show that masked language models ({MLMs}) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or ﬁnetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from {MLMs} than the manually created prompts on the {LAMA} benchmark, and that {MLMs} can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained {LMs} become more sophisticated and capable, potentially a replacement for ﬁnetuning.},
	number = {{arXiv}:2010.15980},
	publisher = {{arXiv}},
	author = {Shin, Taylor and Razeghi, Yasaman and Logan {IV}, Robert L. and Wallace, Eric and Singh, Sameer},
	urldate = {2024-09-29},
	date = {2020-11-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2010.15980 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/Users/xiaohu/Zotero/storage/S4KS2PEH/Shin et al. - 2020 - AutoPrompt Eliciting Knowledge from Language Models with Automatically Generated Prompts.pdf:application/pdf},
}

@inproceedings{jones_automatically_2023,
	title = {Automatically Auditing Large Language Models via Discrete Optimization},
	url = {https://proceedings.mlr.press/v202/jones23a.html},
	abstract = {Auditing large language models for unexpected behaviors is critical to preempt catastrophic deployments, yet remains challenging. In this work, we cast auditing as an optimization problem, where we automatically search for input-output pairs that match a desired target behavior. For example, we might aim to find a non-toxic input that starts with “Barack Obama” that a model maps to a toxic output. This optimization problem is difficult to solve as the set of feasible points is sparse, the space is discrete, and the language models we audit are non-linear and high-dimensional. To combat these challenges, we introduce a discrete optimization algorithm, {ARCA}, that jointly and efficiently optimizes over inputs and outputs. Our approach automatically uncovers derogatory completions about celebrities (e.g. "Barack Obama is a legalized unborn" –\${\textgreater}\$ "child murderer"), produces French inputs that complete to English outputs, and finds inputs that generate a specific name. Our work offers a promising new tool to uncover models’ failure-modes before deployment. Content Warning: This paper contains examples that may be offensive in nature.},
	eventtitle = {International Conference on Machine Learning},
	pages = {15307--15329},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Jones, Erik and Dragan, Anca and Raghunathan, Aditi and Steinhardt, Jacob},
	urldate = {2024-09-29},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/Users/xiaohu/Zotero/storage/N2NC83UU/Jones et al. - 2023 - Automatically Auditing Large Language Models via Discrete Optimization.pdf:application/pdf},
}

@article{carlini_are_nodate,
	title = {Are aligned neural networks adversarially aligned?},
	abstract = {Large language models are now tuned to align with the goals of their creators, namely to be “helpful and harmless.” These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study adversarial alignment, and ask to what extent these models remain aligned when interacting with an adversarial user who constructs worstcase inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing {NLP}-based optimization attacks are insufﬁciently powerful to reliably attack aligned text models: even when current {NLP}-based attacks fail, we can ﬁnd adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale {ML} models is multimodal models that allow users to provide images that inﬂuence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved {NLP} attacks may demonstrate this same level of adversarial control over text-only models. Warning: some content generated by language models in this paper may be offensive to some readers.},
	author = {Carlini, Nicholas and Nasr, Milad and Choquette-Choo, Christopher A and Jagielski, Matthew and Gao, Irena and Awadalla, Anas and Koh, Pang Wei and Ippolito, Daphne and Lee, Katherine and Tramer, Florian and Schmidt, Ludwig},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/6DGKWIQN/Carlini et al. - Are aligned neural networks adversarially aligned.pdf:application/pdf},
}

@misc{yu_promptfuzz_2024,
	title = {{PROMPTFUZZ}: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in {LLMs}},
	url = {http://arxiv.org/abs/2409.14729},
	shorttitle = {{PROMPTFUZZ}},
	abstract = {Large Language Models ({LLMs}) have gained widespread use in various applications due to their powerful capability to generate human-like text. However, prompt injection attacks, which involve overwriting a model’s original instructions with malicious prompts to manipulate the generated text, have raised significant concerns about the security and reliability of {LLMs}. Ensuring that {LLMs} are robust against such attacks is crucial for their deployment in real-world applications, particularly in critical tasks.},
	number = {{arXiv}:2409.14729},
	publisher = {{arXiv}},
	author = {Yu, Jiahao and Shao, Yangguang and Miao, Hanwen and Shi, Junzheng and Xing, Xinyu},
	urldate = {2024-09-30},
	date = {2024-09-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2409.14729 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {PDF:/Users/xiaohu/Zotero/storage/AFW5HQTT/Yu et al. - 2024 - PROMPTFUZZ Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs.pdf:application/pdf},
}

@article{zheng_black-box_2024,
	title = {Black-Box Prompt Tuning With Subspace Learning},
	volume = {32},
	issn = {2329-9304},
	url = {https://ieeexplore.ieee.org/document/10551912/?arnumber=10551912},
	doi = {10.1109/TASLP.2024.3407519},
	abstract = {Black-box prompt tuning employs derivative-free optimization algorithms to learn prompts within low-dimensional subspaces rather than back-propagating through the network of Large Language Models ({LLMs}). Recent studies reveal that black-box prompt tuning lacks versatility across tasks and {LLMs}, which we believe is related to the suboptimal choice of subspaces. In this paper, we introduce Black-box prompt tuning with Subspace Learning ({BSL}) to enhance the versatility of black-box prompt tuning. Based on the assumption that nearly optimal prompts for similar tasks reside in a common subspace, we propose identifying such subspaces through meta-learning on a collection of similar source tasks. Consequently, for a target task that shares similarities with the source tasks, we expect that optimizing within the identified subspace can yield a prompt that performs well on the target task. Experimental results confirm that our {BSL} framework consistently achieves competitive performance across various downstream tasks and {LLMs}.},
	pages = {3002--3013},
	journaltitle = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
	author = {Zheng, Yuanhang and Tan, Zhixing and Li, Peng and Liu, Yang},
	urldate = {2024-10-01},
	date = {2024},
	note = {Conference Name: {IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
	keywords = {Task analysis, Speech processing, Black-box, Closed box, large language models ({LLMs}), meta-learning, Metalearning, Optimization, prompt tuning, subspace learning, Sun, Tuning},
	file = {IEEE Xplore Abstract Record:/Users/xiaohu/Zotero/storage/RIQQWZQU/10551912.html:text/html;IEEE Xplore Full Text PDF:/Users/xiaohu/Zotero/storage/BBHDRJTV/Zheng et al. - 2024 - Black-Box Prompt Tuning With Subspace Learning.pdf:application/pdf},
}

@online{noauthor_power_nodate,
	title = {The Power of Noise: Redefining Retrieval for {RAG} Systems {\textbar} Proceedings of the 47th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	url = {https://dl.acm.org/doi/abs/10.1145/3626772.3657834},
	urldate = {2024-10-02},
	file = {PDF:/Users/xiaohu/Zotero/storage/UQT289EQ/The Power of Noise Redefining Retrieval for RAG Systems  Proceedings of the 47th International ACM.pdf:application/pdf;The Power of Noise\: Redefining Retrieval for RAG Systems | Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval:/Users/xiaohu/Zotero/storage/HMHY5LZM/3626772.html:text/html},
}

@misc{geisler_attacking_2024,
	title = {Attacking Large Language Models with Projected Gradient Descent},
	url = {http://arxiv.org/abs/2402.09154},
	abstract = {Current {LLM} alignment methods are readily broken through specifically crafted adversarial prompts. While crafting adversarial prompts using discrete optimization is highly effective, such attacks typically use more than 100,000 {LLM} calls. This high computational cost makes them unsuitable for, e.g., quantitative analyses and adversarial training. To remedy this, we revisit Projected Gradient Descent ({PGD}) on the continuously relaxed input prompt. Although previous attempts with ordinary gradient-based attacks largely failed, we show that carefully controlling the error introduced by the continuous relaxation tremendously boosts their efficacy. Our {PGD} for {LLMs} is up to one order of magnitude faster than state-of-theart discrete optimization to achieve the same devastating attack results.},
	number = {{arXiv}:2402.09154},
	publisher = {{arXiv}},
	author = {Geisler, Simon and Wollschläger, Tom and Abdalla, M. H. I. and Gasteiger, Johannes and Günnemann, Stephan},
	urldate = {2024-10-03},
	date = {2024-02-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2402.09154 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {PDF:/Users/xiaohu/Zotero/storage/GVJ4TW38/Geisler et al. - 2024 - Attacking Large Language Models with Projected Gradient Descent.pdf:application/pdf},
}

@misc{kumar_certifying_2024,
	title = {Certifying {LLM} Safety against Adversarial Prompting},
	url = {http://arxiv.org/abs/2309.02705},
	abstract = {Large language models ({LLMs}) are vulnerable to adversarial attacks that add malicious tokens to an input prompt to bypass the safety guardrails of an {LLM} and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework for defending against adversarial prompts with certifiable safety guarantees. Given a prompt, our procedure erases tokens individually and inspects the resulting subsequences using a safety filter. It labels the input prompt as harmful if any of the subsequences or the prompt itself is detected as harmful by the filter. Our safety certificate guarantees that harmful prompts are not mislabeled as safe due to an adversarial attack up to a certain size. We implement the safety filter in two ways, using Llama 2 and {DistilBERT}, and compare the performance of erase-and-check for the two cases. We defend against three attack modes: i) adversarial suffix, where an adversarial sequence is appended at the end of a harmful prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block.},
	number = {{arXiv}:2309.02705},
	publisher = {{arXiv}},
	author = {Kumar, Aounon and Agarwal, Chirag and Srinivas, Suraj and Li, Aaron Jiaxun and Feizi, Soheil and Lakkaraju, Himabindu},
	urldate = {2024-10-02},
	date = {2024-02-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2309.02705 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {PDF:/Users/xiaohu/Zotero/storage/3K6CPEMV/Kumar et al. - 2024 - Certifying LLM Safety against Adversarial Prompting.pdf:application/pdf},
}

@misc{xhonneux_efficient_2024,
	title = {Efficient Adversarial Training in {LLMs} with Continuous Attacks},
	url = {http://arxiv.org/abs/2405.15589},
	abstract = {Large language models ({LLMs}) are vulnerable to adversarial attacks that can bypass their safety guardrails. In many domains, adversarial training has proven to be one of the most promising methods to reliably improve robustness against such attacks. Yet, in the context of {LLMs}, current methods for adversarial training are hindered by the high computational costs required to perform discrete adversarial attacks at each training iteration. We address this problem by instead calculating adversarial attacks in the continuous embedding space of the {LLM}, which is orders of magnitudes more efficient. We propose a fast adversarial training algorithm ({CAT}) composed of two losses: the first makes the model robust on continuous embedding attacks computed on an adversarial behaviour dataset; the second ensures the usefulness of the final model by fine-tuning on utility data. Moreover, we introduce {CAPO}, an adversarial variant of {IPO} that does not require utility data for adversarially robust alignment. Our empirical evaluation on four models from different families (Gemma, Phi3, Mistral, Zephyr) and at different scales (2B, 3.8B, 7B) shows that both algorithms substantially enhance {LLM} robustness against discrete attacks ({GCG}, {AutoDAN}, {PAIR}), while maintaining utility. Our results demonstrate that robustness to continuous perturbations can extrapolate to discrete threat models. Thereby, we present a path toward scalable adversarial training algorithms for robustly aligning {LLMs}.},
	number = {{arXiv}:2405.15589},
	publisher = {{arXiv}},
	author = {Xhonneux, Sophie and Sordoni, Alessandro and Günnemann, Stephan and Gidel, Gauthier and Schwinn, Leo},
	urldate = {2024-10-02},
	date = {2024-06-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2405.15589 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {PDF:/Users/xiaohu/Zotero/storage/U4KRBXTL/Xhonneux et al. - 2024 - Efficient Adversarial Training in LLMs with Continuous Attacks.pdf:application/pdf},
}

@misc{song_early_2024,
	title = {The Early Bird Catches the Leak: Unveiling Timing Side Channels in {LLM} Serving Systems},
	url = {http://arxiv.org/abs/2409.20002},
	shorttitle = {The Early Bird Catches the Leak},
	abstract = {The wide deployment of Large Language Models ({LLMs}) has given rise to strong demands for optimizing their inference performance. Today's techniques serving this purpose primarily focus on reducing latency and improving throughput through algorithmic and hardware enhancements, while largely overlooking their privacy side effects, particularly in a multi-user environment. In our research, for the first time, we discovered a set of new timing side channels in {LLM} systems, arising from shared caches and {GPU} memory allocations, which can be exploited to infer both confidential system prompts and those issued by other users. These vulnerabilities echo security challenges observed in traditional computing systems, highlighting an urgent need to address potential information leakage in {LLM} serving infrastructures. In this paper, we report novel attack strategies designed to exploit such timing side channels inherent in {LLM} deployments, specifically targeting the Key-Value ({KV}) cache and semantic cache widely used to enhance {LLM} inference performance. Our approach leverages timing measurements and classification models to detect cache hits, allowing an adversary to infer private prompts with high accuracy. We also propose a token-by-token search algorithm to efficiently recover shared prompt prefixes in the caches, showing the feasibility of stealing system prompts and those produced by peer users. Our experimental studies on black-box testing of popular online {LLM} services demonstrate that such privacy risks are completely realistic, with significant consequences. Our findings underscore the need for robust mitigation to protect {LLM} systems against such emerging threats.},
	number = {{arXiv}:2409.20002},
	publisher = {{arXiv}},
	author = {Song, Linke and Pang, Zixuan and Wang, Wenhao and Wang, Zihao and Wang, {XiaoFeng} and Chen, Hongbo and Song, Wei and Jin, Yier and Meng, Dan and Hou, Rui},
	urldate = {2024-10-21},
	date = {2024-09-30},
	eprinttype = {arxiv},
	eprint = {2409.20002},
	keywords = {Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/W8G2GJ6B/Song et al. - 2024 - The Early Bird Catches the Leak Unveiling Timing Side Channels in LLM Serving Systems.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/KTP7S76Q/2409.html:text/html},
}

@misc{andriushchenko_jailbreaking_2024,
	title = {Jailbreaking Leading Safety-Aligned {LLMs} with Simple Adaptive Attacks},
	url = {http://arxiv.org/abs/2404.02151},
	abstract = {We show that even the most recent safety-aligned {LLMs} are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target {LLM}), and then we apply random search on a suffix to maximize a target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve 100\% attack success rate -- according to {GPT}-4 as a judge -- on Vicuna-13B, Mistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B, Llama-3-Instruct-8B, Gemma-7B, {GPT}-3.5, {GPT}-4o, and R2D2 from {HarmBench} that was adversarially trained against the {GCG} attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with a 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many similarities with jailbreaking -- which is the algorithm that brought us the first place in the {SaTML}'24 Trojan Detection Competition. The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different prompting templates (e.g., R2D2 is very sensitive to in-context learning prompts), some models have unique vulnerabilities based on their {APIs} (e.g., prefilling for Claude), and in some settings, it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection). For reproducibility purposes, we provide the code, logs, and jailbreak artifacts in the {JailbreakBench} format at https://github.com/tml-epfl/llm-adaptive-attacks.},
	number = {{arXiv}:2404.02151},
	publisher = {{arXiv}},
	author = {Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas},
	urldate = {2024-10-17},
	date = {2024-10-07},
	eprinttype = {arxiv},
	eprint = {2404.02151},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Cryptography and Security, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/DG2N3248/Andriushchenko et al. - 2024 - Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/MY3BAC63/2404.html:text/html},
}

@article{ji_beavertails_nodate,
	title = {{BEAVERTAILS}: Towards Improved Safety Alignment of {LLM} via a Human-Preference Dataset},
	abstract = {In this paper, we introduce the {BEAVERTAILS} dataset, aimed at fostering research on safety alignment in large language models ({LLMs}). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 333,963 question-answer ({QA}) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of {BeaverTails} in content moderation and reinforcement learning with human feedback ({RLHF}), emphasizing its potential for practical safety measures in {LLMs}. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of {LLMs}. Our project page is available at the following {URL}: https://sites. google.com/view/pku-beavertails. Warning: this paper contains example data that may be offensive or harmful.},
	author = {Ji, Jiaming and Liu, Mickel and Dai, Juntao and Pan, Xuehai and Zhang, Chi and Bian, Ce and Chen, Boyuan and Sun, Ruiyang and Wang, Yizhou and Yang, Yaodong},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/ZWLK2G7I/Ji et al. - BEAVERTAILS Towards Improved Safety Alignment of LLM via a Human-Preference Dataset.pdf:application/pdf},
}

@misc{zhou_how_2024,
	title = {How Alignment and Jailbreak Work: Explain {LLM} Safety through Intermediate Hidden States},
	url = {http://arxiv.org/abs/2406.05644},
	shorttitle = {How Alignment and Jailbreak Work},
	abstract = {Large language models ({LLMs}) rely on safety alignment to avoid responding to malicious user inputs. Unfortunately, jailbreak can circumvent safety guardrails, resulting in {LLMs} generating harmful content and raising concerns about {LLM} safety. Due to language models with intensive parameters often regarded as black boxes, the mechanisms of alignment and jailbreak are challenging to elucidate. In this paper, we employ weak classifiers to explain {LLM} safety through the intermediate hidden states. We first confirm that {LLMs} learn ethical concepts during pre-training rather than alignment and can identify malicious and normal inputs in the early layers. Alignment actually associates the early concepts with emotion guesses in the middle layers and then refines them to the specific reject tokens for safe generations. Jailbreak disturbs the transformation of early unethical classification into negative emotions. We conduct experiments on models from 7B to 70B across various model families to prove our conclusion. Overall, our paper indicates the intrinsical mechanism of {LLM} safety and how jailbreaks circumvent safety guardrails, offering a new perspective on {LLM} safety and reducing concerns. Our code is available at https://github.com/ydyjya/{LLMIHS}-Explanation.},
	number = {{arXiv}:2406.05644},
	publisher = {{arXiv}},
	author = {Zhou, Zhenhong and Yu, Haiyang and Zhang, Xinghua and Xu, Rongwu and Huang, Fei and Li, Yongbin},
	urldate = {2024-09-19},
	date = {2024-06-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2406.05644 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Computers and Society},
	file = {Zhou et al. - 2024 - How Alignment and Jailbreak Work Explain LLM Safe.pdf:/Users/xiaohu/Zotero/storage/4S2LLJ9Q/Zhou et al. - 2024 - How Alignment and Jailbreak Work Explain LLM Safe.pdf:application/pdf},
}

@misc{qi_safety_2024,
	title = {Safety Alignment Should Be Made More Than Just a Few Tokens Deep},
	url = {http://arxiv.org/abs/2406.05946},
	abstract = {The safety alignment of current Large Language Models ({LLMs}) is vulnerable. Relatively simple attacks, or even benign fine-tuning, can jailbreak aligned models. We argue that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and provide evidence that current aligned {LLMs} are subject to this issue. We also show how these findings help explain multiple recently discovered vulnerabilities in {LLMs}, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. Importantly, we discuss how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. For instance, we show that deepening the safety alignment beyond just the first few tokens can often meaningfully improve robustness against some common exploits. Finally, we design a regularized finetuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep.},
	number = {{arXiv}:2406.05946},
	publisher = {{arXiv}},
	author = {Qi, Xiangyu and Panda, Ashwinee and Lyu, Kaifeng and Ma, Xiao and Roy, Subhrajit and Beirami, Ahmad and Mittal, Prateek and Henderson, Peter},
	urldate = {2024-10-17},
	date = {2024-06-10},
	eprinttype = {arxiv},
	eprint = {2406.05946},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/ITAS7JUU/Qi et al. - 2024 - Safety Alignment Should Be Made More Than Just a Few Tokens Deep.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/HPQ5XXFD/2406.html:text/html},
}

@misc{schwinn_soft_2024,
	title = {Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source {LLMs} through the Embedding Space},
	url = {http://arxiv.org/abs/2402.09063},
	shorttitle = {Soft Prompt Threats},
	abstract = {Current research in adversarial robustness of {LLMs} focuses on discrete input manipulations in the natural language space, which can be directly transferred to closed-source models. However, this approach neglects the steady progression of open-source models. As open-source models advance in capability, ensuring their safety also becomes increasingly imperative. Yet, attacks tailored to open-source {LLMs} that exploit full model access remain largely unexplored. We address this research gap and propose the embedding space attack, which directly attacks the continuous embedding representation of input tokens. We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Furthermore, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned {LLMs} across multiple datasets and models. Our findings highlight embedding space attacks as an important threat model in open-source {LLMs}. Trigger Warning: the appendix contains {LLM}-generated text with violence and harassment.},
	number = {{arXiv}:2402.09063},
	publisher = {{arXiv}},
	author = {Schwinn, Leo and Dobre, David and Xhonneux, Sophie and Gidel, Gauthier and Gunnemann, Stephan},
	urldate = {2024-10-17},
	date = {2024-02-14},
	eprinttype = {arxiv},
	eprint = {2402.09063},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/25A53UUG/Schwinn et al. - 2024 - Soft Prompt Threats Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embed.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/8HHSPRQ9/2402.html:text/html},
}

@misc{xu_continuous_2024,
	title = {Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large Language Models},
	url = {http://arxiv.org/abs/2407.13796},
	abstract = {Security concerns for large language models ({LLMs}) have recently escalated, focusing on thwarting jailbreaking attempts in discrete prompts. However, the exploration of jailbreak vulnerabilities arising from continuous embeddings has been limited, as prior approaches primarily involved appending discrete or continuous suffixes to inputs. Our study presents a novel channel for conducting direct attacks on {LLM} inputs, eliminating the need for suffix addition or specific questions provided that the desired output is predefined. We additionally observe that extensive iterations often lead to overfitting, characterized by repetition in the output. To counteract this, we propose a simple yet effective strategy named {CLIP}. Our experiments show that for an input length of 40 at iteration 1000, applying {CLIP} improves the {ASR} from 62\% to 83\%},
	number = {{arXiv}:2407.13796},
	publisher = {{arXiv}},
	author = {Xu, Zihao and Liu, Yi and Deng, Gelei and Wang, Kailong and Li, Yuekang and Shi, Ling and Picek, Stjepan},
	urldate = {2024-10-17},
	date = {2024-07-16},
	eprinttype = {arxiv},
	eprint = {2407.13796},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/WGFQWZF9/Xu et al. - 2024 - Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large Language Models.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/QFE3KMC3/2407.html:text/html},
}

@misc{dong_attacks_2024,
	title = {Attacks, Defenses and Evaluations for {LLM} Conversation Safety: A Survey},
	url = {http://arxiv.org/abs/2402.09283},
	shorttitle = {Attacks, Defenses and Evaluations for {LLM} Conversation Safety},
	abstract = {Large Language Models ({LLMs}) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on {LLM} conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of {LLM} conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of {LLM} conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/{LLM}-conversation-safety.},
	number = {{arXiv}:2402.09283},
	publisher = {{arXiv}},
	author = {Dong, Zhichen and Zhou, Zhanhui and Yang, Chao and Shao, Jing and Qiao, Yu},
	urldate = {2024-10-17},
	date = {2024-03-27},
	eprinttype = {arxiv},
	eprint = {2402.09283},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Computers and Society},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/LXMP6PVR/Dong et al. - 2024 - Attacks, Defenses and Evaluations for LLM Conversation Safety A Survey.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/A222LKNG/2402.html:text/html},
}

@misc{qi_follow_2024,
	title = {Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems},
	url = {http://arxiv.org/abs/2402.17840},
	shorttitle = {Follow My Instruction and Spill the Beans},
	abstract = {Retrieval-Augmented Generation ({RAG}) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context {RAG} Language Models ({LMs}). We show that an adversary can exploit {LMs}’ instruction-following capabilities to easily extract text data verbatim from the datastore of {RAG} systems built with instruction-tuned {LMs} via prompt injection. The vulnerability exists for a wide range of modern {LMs} that span Llama2, Mistral/Mixtral, Vicuna, {SOLAR}, {WizardLM}, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. We also study multiple effects of {RAG} setup on the extractability of data, indicating that following unexpected instructions to regurgitate data can be an outcome of failure in effectively utilizing contexts for modern {LMs}, and further show that such vulnerability can be greatly mitigated by position bias elimination strategies. Extending our study to production {RAG} models {GPTs}, we design an attack that can cause datastore leakage with a 100\% success rate on 25 randomly selected customized {GPTs} with at most 2 queries, and we extract text data verbatim at a rate of 41\% from a book of 77,000 words and 3\% from a corpus of 1,569,000 words by prompting the {GPTs} with only 100 queries generated by themselves. Code is available at this repository.},
	number = {{arXiv}:2402.17840},
	publisher = {{arXiv}},
	author = {Qi, Zhenting and Zhang, Hanlin and Xing, Eric and Kakade, Sham and Lakkaraju, Himabindu},
	urldate = {2024-10-21},
	date = {2024-10-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2402.17840 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv.org Snapshot:/Users/xiaohu/Zotero/storage/2JS2UUSL/2402.html:text/html;PDF:/Users/xiaohu/Zotero/storage/VT26S9DH/Qi et al. - 2024 - Follow My Instruction and Spill the Beans Scalable Data Extraction from Retrieval-Augmented Generat.pdf:application/pdf},
}

@misc{li_generating_2024-1,
	title = {Generating Is Believing: Membership Inference Attacks against Retrieval-Augmented Generation},
	url = {http://arxiv.org/abs/2406.19234},
	doi = {10.48550/arXiv.2406.19234},
	shorttitle = {Generating Is Believing},
	abstract = {Retrieval-Augmented Generation ({RAG}) is a state-of-the-art technique that mitigates issues such as hallucinations and knowledge staleness in Large Language Models ({LLMs}) by retrieving relevant knowledge from an external database to assist in content generation. Existing research has demonstrated potential privacy risks associated with the {LLMs} of {RAG}. However, the privacy risks posed by the integration of an external database, which often contains sensitive data such as medical records or personal identities, have remained largely unexplored. In this paper, we aim to bridge this gap by focusing on membership privacy of {RAG}'s external database, with the aim of determining whether a given sample is part of the {RAG}'s database. Our basic idea is that if a sample is in the external database, it will exhibit a high degree of semantic similarity to the text generated by the {RAG} system. We present S\${\textasciicircum}2\${MIA}, a {\textbackslash}underline\{M\}embership {\textbackslash}underline\{I\}nference {\textbackslash}underline\{A\}ttack that utilizes the {\textbackslash}underline\{S\}emantic {\textbackslash}underline\{S\}imilarity between a given sample and the content generated by the {RAG} system. With our proposed S\${\textasciicircum}2\${MIA}, we demonstrate the potential to breach the membership privacy of the {RAG} database. Extensive experiment results demonstrate that S\${\textasciicircum}2\${MIA} can achieve a strong inference performance compared with five existing {MIAs}, and is able to escape from the protection of three representative defenses.},
	number = {{arXiv}:2406.19234},
	publisher = {{arXiv}},
	author = {Li, Yuying and Liu, Gaoyang and Wang, Chen and Yang, Yang},
	urldate = {2024-10-21},
	date = {2024-09-26},
	eprinttype = {arxiv},
	eprint = {2406.19234 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/xiaohu/Zotero/storage/4RF9W43I/Li et al. - 2024 - Generating Is Believing Membership Inference Attacks against Retrieval-Augmented Generation.pdf:application/pdf;arXiv.org Snapshot:/Users/xiaohu/Zotero/storage/Z4RIWX9X/2406.html:text/html},
}

@misc{wang_poisoned_2024,
	title = {Poisoned {LangChain}: Jailbreak {LLMs} by {LangChain}},
	url = {http://arxiv.org/abs/2406.18122},
	shorttitle = {Poisoned {LangChain}},
	abstract = {With the development of natural language processing ({NLP}), large language models ({LLMs}) are becoming increasingly popular. {LLMs} are integrating more into everyday life, raising public concerns about their security vulnerabilities. Consequently, the security of large language models is becoming critically important. Currently, the techniques for attacking and defending against {LLMs} are continuously evolving. One significant method type of attack is the jailbreak attack, which designed to evade model safety mechanisms and induce the generation of inappropriate content. Existing jailbreak attacks primarily rely on crafting inducement prompts for direct jailbreaks, which are less effective against large models with robust filtering and high comprehension abilities. Given the increasing demand for real-time capabilities in large language models, real-time updates and iterations of new knowledge have become essential. Retrieval-Augmented Generation ({RAG}), an advanced technique to compensate for the model's lack of new knowledge, is gradually becoming mainstream. As {RAG} enables the model to utilize external knowledge bases, it provides a new avenue for jailbreak attacks. In this paper, we conduct the first work to propose the concept of indirect jailbreak and achieve Retrieval-Augmented Generation via {LangChain}. Building on this, we further design a novel method of indirect jailbreak attack, termed Poisoned-{LangChain} ({PLC}), which leverages a poisoned external knowledge base to interact with large language models, thereby causing the large models to generate malicious non-compliant dialogues.We tested this method on six different large language models across three major categories of jailbreak issues. The experiments demonstrate that {PLC} successfully implemented indirect jailbreak attacks under three different scenarios, achieving success rates of 88.56\%, 79.04\%, and 82.69\% respectively.},
	number = {{arXiv}:2406.18122},
	publisher = {{arXiv}},
	author = {Wang, Ziqiu and Liu, Jun and Zhang, Shengkai and Yang, Yang},
	urldate = {2024-10-21},
	date = {2024-06-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2406.18122 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:/Users/xiaohu/Zotero/storage/IB4PEXA7/Wang et al. - 2024 - Poisoned LangChain Jailbreak LLMs by LangChain.pdf:application/pdf},
}

@misc{cohen_unleashing_2024,
	title = {Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against {RAG}-based Inference in Scale and Severity Using Jailbreaking},
	url = {http://arxiv.org/abs/2409.08045},
	shorttitle = {Unleashing Worms and Extracting Data},
	abstract = {In this paper, we show that with the ability to jailbreak a {GenAI} model, attackers can escalate the outcome of attacks against {RAG}-based {GenAI}-powered applications in severity and scale. In the first part of the paper, we show that attackers can escalate {RAG} membership inference attacks and {RAG} entity extraction attacks to {RAG} documents extraction attacks, forcing a more severe outcome compared to existing attacks. We evaluate the results obtained from three extraction methods, the influence of the type and the size of five embeddings algorithms employed, the size of the provided context, and the {GenAI} engine. We show that attackers can extract 80\%-99.8\% of the data stored in the database used by the {RAG} of a Q\&A chatbot. In the second part of the paper, we show that attackers can escalate the scale of {RAG} data poisoning attacks from compromising a single {GenAI}-powered application to compromising the entire {GenAI} ecosystem, forcing a greater scale of damage. This is done by crafting an adversarial self-replicating prompt that triggers a chain reaction of a computer worm within the ecosystem and forces each affected application to perform a malicious activity and compromise the {RAG} of additional applications. We evaluate the performance of the worm in creating a chain of confidential data extraction about users within a {GenAI} ecosystem of {GenAI}-powered email assistants and analyze how the performance of the worm is affected by the size of the context, the adversarial self-replicating prompt used, the type and size of the embeddings algorithm employed, and the number of hops in the propagation. Finally, we review and analyze guardrails to protect {RAG}-based inference and discuss the tradeoffs.},
	number = {{arXiv}:2409.08045},
	publisher = {{arXiv}},
	author = {Cohen, Stav and Bitton, Ron and Nassi, Ben},
	urldate = {2024-10-31},
	date = {2024-09-12},
	eprinttype = {arxiv},
	eprint = {2409.08045},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/YEKWAN58/Cohen et al. - 2024 - Unleashing Worms and Extracting Data Escalating the Outcome of Attacks against RAG-based Inference.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/XZEZK43L/2409.html:text/html},
}

@misc{gao_retrieval-augmented_2024,
	title = {Retrieval-Augmented Generation for Large Language Models: A Survey},
	url = {http://arxiv.org/abs/2312.10997},
	shorttitle = {Retrieval-Augmented Generation for Large Language Models},
	abstract = {Large Language Models ({LLMs}) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation ({RAG}) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. {RAG} synergistically merges {LLMs}' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of {RAG} paradigms, encompassing the Naive {RAG}, the Advanced {RAG}, and the Modular {RAG}. It meticulously scrutinizes the tripartite foundation of {RAG} frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in {RAG} systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
	number = {{arXiv}:2312.10997},
	publisher = {{arXiv}},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	urldate = {2024-10-29},
	date = {2024-03-27},
	eprinttype = {arxiv},
	eprint = {2312.10997},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/ABG8NTZL/Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language Models A Survey.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/CEARRVIK/2312.html:text/html},
}

@misc{wang_raccoon_2024,
	title = {Raccoon: Prompt Extraction Benchmark of {LLM}-Integrated Applications},
	url = {http://arxiv.org/abs/2406.06737},
	doi = {10.48550/arXiv.2406.06737},
	shorttitle = {Raccoon},
	abstract = {With the proliferation of {LLM}-integrated applications such as {GPT}-s, millions are deployed, offering valuable services through proprietary instruction prompts. These systems, however, are prone to prompt extraction attacks through meticulously designed queries. To help mitigate this problem, we introduce the Raccoon benchmark which comprehensively evaluates a model's susceptibility to prompt extraction attacks. Our novel evaluation method assesses models under both defenseless and defended scenarios, employing a dual approach to evaluate the effectiveness of existing defenses and the resilience of the models. The benchmark encompasses 14 categories of prompt extraction attacks, with additional compounded attacks that closely mimic the strategies of potential attackers, alongside a diverse collection of defense templates. This array is, to our knowledge, the most extensive compilation of prompt theft attacks and defense mechanisms to date. Our findings highlight universal susceptibility to prompt theft in the absence of defenses, with {OpenAI} models demonstrating notable resilience when protected. This paper aims to establish a more systematic benchmark for assessing {LLM} robustness against prompt extraction attacks, offering insights into their causes and potential countermeasures. Resources of Raccoon are publicly available at https://github.com/M0gician/{RaccoonBench}.},
	number = {{arXiv}:2406.06737},
	publisher = {{arXiv}},
	author = {Wang, Junlin and Yang, Tianyi and Xie, Roy and Dhingra, Bhuwan},
	urldate = {2024-11-04},
	date = {2024-10-26},
	eprinttype = {arxiv},
	eprint = {2406.06737},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/4E7H3AF8/Wang et al. - 2024 - Raccoon Prompt Extraction Benchmark of LLM-Integrated Applications.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/56GNN5WT/2406.html:text/html},
}

@misc{zeng_mitigating_2024,
	title = {Mitigating the Privacy Issues in Retrieval-Augmented Generation ({RAG}) via Pure Synthetic Data},
	url = {http://arxiv.org/abs/2406.14773},
	doi = {10.48550/arXiv.2406.14773},
	abstract = {Retrieval-augmented generation ({RAG}) enhances the outputs of language models by integrating relevant information retrieved from external knowledge sources. However, when the retrieval process involves private data, {RAG} systems may face severe privacy risks, potentially leading to the leakage of sensitive information. To address this issue, we propose using synthetic data as a privacy-preserving alternative for the retrieval data. We propose {SAGE}, a novel two-stage synthetic data generation paradigm. In the stage-1, we employ an attribute-based extraction and generation approach to preserve key contextual information from the original data. In the stage-2, we further enhance the privacy properties of the synthetic data through an agent-based iterative refinement process. Extensive experiments demonstrate that using our synthetic data as the retrieval context achieves comparable performance to using the original data while substantially reducing privacy risks. Our work takes the first step towards investigating the possibility of generating high-utility and privacy-preserving synthetic data for {RAG}, opening up new opportunities for the safe application of {RAG} systems in various domains.},
	number = {{arXiv}:2406.14773},
	publisher = {{arXiv}},
	author = {Zeng, Shenglai and Zhang, Jiankun and He, Pengfei and Ren, Jie and Zheng, Tianqi and Lu, Hanqing and Xu, Han and Liu, Hui and Xing, Yue and Tang, Jiliang},
	urldate = {2024-11-04},
	date = {2024-06-20},
	eprinttype = {arxiv},
	eprint = {2406.14773},
	keywords = {Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/G4AD2ULC/Zeng et al. - 2024 - Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG) via Pure Synthetic Data.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/KZ8UJX94/2406.html:text/html},
}

@article{chenEVALUATIONMETRICSLANGUAGE,
	title = {{EVALUATION} {METRICS} {FOR} {LANGUAGE} {MODELS}},
	abstract = {The most widely-used evaluation metric for language models for speech recognition is the perplexity of test data. While perplexities can be calculated efﬁciently and without access to a speech recognizer, they often do not correlate well with speech recognition word-error rates. In this research, we attempt to ﬁnd a measure that like perplexity is easily calculated but which better predicts speech recognition performance.},
	author = {Chen, Stanley and Beeferman, Douglas and Rosenfeld, Ronald},
	langid = {english},
	file = {PDF:C\:\\Users\\huxia\\Zotero\\storage\\5JTQY5VW\\Chen et al. - EVALUATION METRICS FOR LANGUAGE MODELS.pdf:application/pdf},
}

@misc{zhouTrustworthinessRetrievalAugmentedGeneration2024,
	title = {Trustworthiness in Retrieval-Augmented Generation Systems: A Survey},
	url = {http://arxiv.org/abs/2409.10102},
	doi = {10.48550/arXiv.2409.10102},
	shorttitle = {Trustworthiness in Retrieval-Augmented Generation Systems},
	abstract = {Retrieval-Augmented Generation ({RAG}) has quickly grown into a pivotal paradigm in the development of Large Language Models ({LLMs}). While much of the current research in this field focuses on performance optimization, particularly in terms of accuracy and efficiency, the trustworthiness of {RAG} systems remains an area still under exploration. From a positive perspective, {RAG} systems are promising to enhance {LLMs} by providing them with useful and up-to-date knowledge from vast external databases, thereby mitigating the long-standing problem of hallucination. While from a negative perspective, {RAG} systems are at the risk of generating undesirable contents if the retrieved information is either inappropriate or poorly utilized. To address these concerns, we propose a unified framework that assesses the trustworthiness of {RAG} systems across six key dimensions: factuality, robustness, fairness, transparency, accountability, and privacy. Within this framework, we thoroughly review the existing literature on each dimension. Additionally, we create the evaluation benchmark regarding the six dimensions and conduct comprehensive evaluations for a variety of proprietary and opensource models. Finally, we identify the potential challenges for future research based on our investigation results. Through this work, we aim to lay a structured foundation for future investigations and provide practical insights for enhancing the trustworthiness of {RAG} systems in real-world applications.},
	number = {{arXiv}:2409.10102},
	publisher = {{arXiv}},
	author = {Zhou, Yujia and Liu, Yan and Li, Xiaoxi and Jin, Jiajie and Qian, Hongjin and Liu, Zheng and Li, Chaozhuo and Dou, Zhicheng and Ho, Tsung-Yi and Yu, Philip S.},
	urldate = {2025-01-22},
	date = {2024-09-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2409.10102 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {PDF:C\:\\Users\\huxia\\Zotero\\storage\\PIZBX4WH\\Zhou et al. - 2024 - Trustworthiness in Retrieval-Augmented Generation Systems A Survey.pdf:application/pdf},
}

@misc{agarwal_prompt_2024,
	title = {Prompt Leakage effect and defense strategies for multi-turn {LLM} interactions},
	url = {http://arxiv.org/abs/2404.16251},
	doi = {10.48550/arXiv.2404.16251},
	abstract = {Prompt leakage poses a compelling security and privacy threat in {LLM} applications. Leakage of system prompts may compromise intellectual property, and act as adversarial reconnaissance for an attacker. A systematic evaluation of prompt leakage threats and mitigation strategies is lacking, especially for multi-turn {LLM} interactions. In this paper, we systematically investigate {LLM} vulnerabilities against prompt leakage for 10 closed- and open-source {LLMs}, across four domains. We design a unique threat model which leverages the {LLM} sycophancy effect and elevates the average attack success rate ({ASR}) from 17.7\% to 86.2\% in a multi-turn setting. Our standardized setup further allows dissecting leakage of specific prompt contents such as task instructions and knowledge documents. We measure the mitigation effect of 7 black-box defense strategies, along with finetuning an open-source model to defend against leakage attempts. We present different combination of defenses against our threat model, including a cost analysis. Our study highlights key takeaways for building secure {LLM} applications and provides directions for research in multi-turn {LLM} interactions},
	number = {{arXiv}:2404.16251},
	publisher = {{arXiv}},
	author = {Agarwal, Divyansh and Fabbri, Alexander R. and Risher, Ben and Laban, Philippe and Joty, Shafiq and Wu, Chien-Sheng},
	urldate = {2024-11-04},
	date = {2024-07-29},
	eprinttype = {arxiv},
	eprint = {2404.16251},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/K52SF4GR/Agarwal et al. - 2024 - Prompt Leakage effect and defense strategies for multi-turn LLM interactions.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/DRIV7929/2404.html:text/html},
}

@misc{gurnee_language_2024,
	title = {Language Models Represent Space and Time},
	url = {http://arxiv.org/abs/2310.02207},
	abstract = {The capabilities of large language models ({LLMs}) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, {US}, {NYC} places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that {LLMs} learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual "space neurons" and "time neurons" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern {LLMs} learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.},
	number = {{arXiv}:2310.02207},
	publisher = {{arXiv}},
	author = {Gurnee, Wes and Tegmark, Max},
	urldate = {2024-11-04},
	date = {2024-03-04},
	eprinttype = {arxiv},
	eprint = {2310.02207},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/WV46VUDX/Gurnee and Tegmark - 2024 - Language Models Represent Space and Time.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/TVA2U9F9/2310.html:text/html;Snapshot:/Users/xiaohu/Zotero/storage/PXEVEM6F/2310.html:text/html},
}

@article{belinkov_probing_2022,
	title = {Probing Classifiers: Promises, Shortcomings, and Advances},
	volume = {48},
	issn = {0891-2017},
	url = {https://doi.org/10.1162/coli_a_00422},
	doi = {10.1162/coli_a_00422},
	shorttitle = {Probing Classifiers},
	abstract = {Probing classifiers have emerged as one of the prominent methodologies for interpreting and analyzing deep neural network models of natural language processing. The basic idea is simple—a classifier is trained to predict some linguistic property from a model’s representations—and has been used to examine a wide variety of models and properties. However, recent studies have demonstrated various methodological limitations of this approach. This squib critically reviews the probing classifiers framework, highlighting their promises, shortcomings, and advances.},
	pages = {207--219},
	number = {1},
	journaltitle = {Computational Linguistics},
	shortjournal = {Computational Linguistics},
	author = {Belinkov, Yonatan},
	urldate = {2024-11-05},
	date = {2022-04-04},
	file = {Full Text PDF:/Users/xiaohu/Zotero/storage/EU8QLX96/Belinkov - 2022 - Probing Classifiers Promises, Shortcomings, and Advances.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/AA5ITQ9Q/Probing-Classifiers-Promises-Shortcomings-and.html:text/html},
}

@misc{ju_how_2024,
	title = {How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study},
	url = {http://arxiv.org/abs/2402.16061},
	shorttitle = {How Large Language Models Encode Context Knowledge?},
	abstract = {Previous work has showcased the intriguing capability of large language models ({LLMs}) in retrieving facts and processing context knowledge. However, only limited research exists on the layer-wise capability of {LLMs} to encode knowledge, which challenges our understanding of their internal mechanisms. In this paper, we devote the first attempt to investigate the layer-wise capability of {LLMs} through probing tasks. We leverage the powerful generative capability of {ChatGPT} to construct probing datasets, providing diverse and coherent evidence corresponding to various facts. We employ \${\textbackslash}mathcal V\$-usable information as the validation metric to better reflect the capability in encoding context knowledge across different layers. Our experiments on conflicting and newly acquired knowledge show that {LLMs}: (1) prefer to encode more context knowledge in the upper layers; (2) primarily encode context knowledge within knowledge-related entity tokens at lower layers while progressively expanding more knowledge within other tokens at upper layers; and (3) gradually forget the earlier context knowledge retained within the intermediate layers when provided with irrelevant evidence. Code is publicly available at https://github.com/Jometeorie/probing\_llama.},
	number = {{arXiv}:2402.16061},
	publisher = {{arXiv}},
	author = {Ju, Tianjie and Sun, Weiwei and Du, Wei and Yuan, Xinwei and Ren, Zhaochun and Liu, Gongshen},
	urldate = {2024-11-06},
	date = {2024-03-04},
	eprinttype = {arxiv},
	eprint = {2402.16061},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/2IEUNUC6/Ju et al. - 2024 - How Large Language Models Encode Context Knowledge A Layer-Wise Probing Study.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/SZSVEQE6/2402.html:text/html},
}

@misc{hui_pleak_2024,
	title = {{PLeak}: Prompt Leaking Attacks against Large Language Model Applications},
	url = {http://arxiv.org/abs/2405.06823},
	shorttitle = {{PLeak}},
	abstract = {Large Language Models ({LLMs}) enable a new ecosystem with many downstream applications, called {LLM} applications, with different natural language processing tasks. The functionality and performance of an {LLM} application highly depend on its system prompt, which instructs the backend {LLM} on what task to perform. Therefore, an {LLM} application developer often keeps a system prompt confidential to protect its intellectual property. As a result, a natural attack, called prompt leaking, is to steal the system prompt from an {LLM} application, which compromises the developer's intellectual property. Existing prompt leaking attacks primarily rely on manually crafted queries, and thus achieve limited effectiveness. In this paper, we design a novel, closed-box prompt leaking attack framework, called {PLeak}, to optimize an adversarial query such that when the attacker sends it to a target {LLM} application, its response reveals its own system prompt. We formulate finding such an adversarial query as an optimization problem and solve it with a gradient-based method approximately. Our key idea is to break down the optimization goal by optimizing adversary queries for system prompts incrementally, i.e., starting from the first few tokens of each system prompt step by step until the entire length of the system prompt. We evaluate {PLeak} in both offline settings and for real-world {LLM} applications, e.g., those on Poe, a popular platform hosting such applications. Our results show that {PLeak} can effectively leak system prompts and significantly outperforms not only baselines that manually curate queries but also baselines with optimized queries that are modified and adapted from existing jailbreaking attacks. We responsibly reported the issues to Poe and are still waiting for their response. Our implementation is available at this repository: https://github.com/{BHui}97/{PLeak}.},
	number = {{arXiv}:2405.06823},
	publisher = {{arXiv}},
	author = {Hui, Bo and Yuan, Haolin and Gong, Neil and Burlina, Philippe and Cao, Yinzhi},
	urldate = {2024-11-11},
	date = {2024-05-14},
	eprinttype = {arxiv},
	eprint = {2405.06823},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/P5AUC74X/Hui et al. - 2024 - PLeak Prompt Leaking Attacks against Large Language Model Applications.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/MR983XCE/2405.html:text/html},
}

@misc{weiss_what_2024,
	title = {What Was Your Prompt? A Remote Keylogging Attack on {AI} Assistants},
	url = {http://arxiv.org/abs/2403.09751},
	shorttitle = {What Was Your Prompt?},
	abstract = {{AI} assistants are becoming an integral part of society, used for asking advice or help in personal and confidential issues. In this paper, we unveil a novel side-channel that can be used to read encrypted responses from {AI} Assistants over the web: the token-length side-channel. We found that many vendors, including {OpenAI} and Microsoft, have this side-channel. However, inferring the content of a response from a token-length sequence alone proves challenging. This is because tokens are akin to words, and responses can be several sentences long leading to millions of grammatically correct sentences. In this paper, we show how this can be overcome by (1) utilizing the power of a large language model ({LLM}) to translate these sequences, (2) providing the {LLM} with inter-sentence context to narrow the search space and (3) performing a known-plaintext attack by fine-tuning the model on the target model's writing style. Using these methods, we were able to accurately reconstruct 29{\textbackslash}\% of an {AI} assistant's responses and successfully infer the topic from 55{\textbackslash}\% of them. To demonstrate the threat, we performed the attack on {OpenAI}'s {ChatGPT}-4 and Microsoft's Copilot on both browser and {API} traffic.},
	number = {{arXiv}:2403.09751},
	publisher = {{arXiv}},
	author = {Weiss, Roy and Ayzenshteyn, Daniel and Amit, Guy and Mirsky, Yisroel},
	urldate = {2024-11-11},
	date = {2024-03-14},
	eprinttype = {arxiv},
	eprint = {2403.09751},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/26IW65RV/Weiss et al. - 2024 - What Was Your Prompt A Remote Keylogging Attack on AI Assistants.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/2TNSRWE9/2403.html:text/html},
}

@article{kim_propile_nodate,
	title = {{ProPILE}: Probing Privacy Leakage in Large Language Models},
	author = {Kim, Siwon and Yun, Sangdoo and Lee, Hwaran and Gubri, Martin and Yoon, Sungroh and Oh, Seong Joon},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/KK9D4YR2/Kim et al. - ProPILE Probing Privacy Leakage in Large Language Models.pdf:application/pdf},
}

@misc{zeng_good_2024,
	title = {The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation ({RAG})},
	url = {http://arxiv.org/abs/2402.16893},
	shorttitle = {The Good and The Bad},
	abstract = {Retrieval-augmented generation ({RAG}) is a powerful technique to facilitate language model with proprietary and private data, where data privacy is a pivotal concern. Whereas extensive research has demonstrated the privacy risks of large language models ({LLMs}), the {RAG} technique could potentially reshape the inherent behaviors of {LLM} generation, posing new privacy issues that are currently under-explored. In this work, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of {RAG} systems on leaking the private retrieval database. Despite the new risk brought by {RAG} on the retrieval data, we further reveal that {RAG} can mitigate the leakage of the {LLMs}' training data. Overall, we provide new insights in this paper for privacy protection of retrieval-augmented {LLMs}, which benefit both {LLMs} and {RAG} systems builders. Our code is available at https://github.com/phycholosogy/{RAG}-privacy.},
	number = {{arXiv}:2402.16893},
	publisher = {{arXiv}},
	author = {Zeng, Shenglai and Zhang, Jiankun and He, Pengfei and Xing, Yue and Liu, Yiding and Xu, Han and Ren, Jie and Wang, Shuaiqiang and Yin, Dawei and Chang, Yi and Tang, Jiliang},
	urldate = {2024-11-17},
	date = {2024-02-23},
	eprinttype = {arxiv},
	eprint = {2402.16893},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/AVBUBTFY/Zeng et al. - 2024 - The Good and The Bad Exploring Privacy Issues in Retrieval-Augmented Generation (RAG).pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/2NM9MHFB/2402.html:text/html},
}

@misc{cohen_unleashing_2024-1,
	title = {Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against {RAG}-based Inference in Scale and Severity Using Jailbreaking},
	url = {http://arxiv.org/abs/2409.08045},
	shorttitle = {Unleashing Worms and Extracting Data},
	abstract = {In this paper, we show that with the ability to jailbreak a {GenAI} model, attackers can escalate the outcome of attacks against {RAG}-based {GenAI}-powered applications in severity and scale. In the first part of the paper, we show that attackers can escalate {RAG} membership inference attacks and {RAG} entity extraction attacks to {RAG} documents extraction attacks, forcing a more severe outcome compared to existing attacks. We evaluate the results obtained from three extraction methods, the influence of the type and the size of five embeddings algorithms employed, the size of the provided context, and the {GenAI} engine. We show that attackers can extract 80\%-99.8\% of the data stored in the database used by the {RAG} of a Q\&A chatbot. In the second part of the paper, we show that attackers can escalate the scale of {RAG} data poisoning attacks from compromising a single {GenAI}-powered application to compromising the entire {GenAI} ecosystem, forcing a greater scale of damage. This is done by crafting an adversarial self-replicating prompt that triggers a chain reaction of a computer worm within the ecosystem and forces each affected application to perform a malicious activity and compromise the {RAG} of additional applications. We evaluate the performance of the worm in creating a chain of confidential data extraction about users within a {GenAI} ecosystem of {GenAI}-powered email assistants and analyze how the performance of the worm is affected by the size of the context, the adversarial self-replicating prompt used, the type and size of the embeddings algorithm employed, and the number of hops in the propagation. Finally, we review and analyze guardrails to protect {RAG}-based inference and discuss the tradeoffs.},
	number = {{arXiv}:2409.08045},
	publisher = {{arXiv}},
	author = {Cohen, Stav and Bitton, Ron and Nassi, Ben},
	urldate = {2024-11-17},
	date = {2024-09-12},
	eprinttype = {arxiv},
	eprint = {2409.08045},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/CJ6HYKYL/Cohen et al. - 2024 - Unleashing Worms and Extracting Data Escalating the Outcome of Attacks against RAG-based Inference.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/RV76WZW7/2409.html:text/html},
}

@misc{zeng_mitigating_2024-1,
	title = {Mitigating the Privacy Issues in Retrieval-Augmented Generation ({RAG}) via Pure Synthetic Data},
	url = {http://arxiv.org/abs/2406.14773},
	doi = {10.48550/arXiv.2406.14773},
	abstract = {Retrieval-augmented generation ({RAG}) enhances the outputs of language models by integrating relevant information retrieved from external knowledge sources. However, when the retrieval process involves private data, {RAG} systems may face severe privacy risks, potentially leading to the leakage of sensitive information. To address this issue, we propose using synthetic data as a privacy-preserving alternative for the retrieval data. We propose {SAGE}, a novel two-stage synthetic data generation paradigm. In the stage-1, we employ an attribute-based extraction and generation approach to preserve key contextual information from the original data. In the stage-2, we further enhance the privacy properties of the synthetic data through an agent-based iterative refinement process. Extensive experiments demonstrate that using our synthetic data as the retrieval context achieves comparable performance to using the original data while substantially reducing privacy risks. Our work takes the first step towards investigating the possibility of generating high-utility and privacy-preserving synthetic data for {RAG}, opening up new opportunities for the safe application of {RAG} systems in various domains.},
	number = {{arXiv}:2406.14773},
	publisher = {{arXiv}},
	author = {Zeng, Shenglai and Zhang, Jiankun and He, Pengfei and Ren, Jie and Zheng, Tianqi and Lu, Hanqing and Xu, Han and Liu, Hui and Xing, Yue and Tang, Jiliang},
	urldate = {2024-11-17},
	date = {2024-06-20},
	eprinttype = {arxiv},
	eprint = {2406.14773},
	keywords = {Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/ZWAKPHV3/Zeng et al. - 2024 - Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG) via Pure Synthetic Data.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/2PQDNNK3/2406.html:text/html},
}

@misc{jiang_rag-thief_2024,
	title = {{RAG}-Thief: Scalable Extraction of Private Data from Retrieval-Augmented Generation Applications with Agent-based Attacks},
	url = {http://arxiv.org/abs/2411.14110},
	doi = {10.48550/arXiv.2411.14110},
	shorttitle = {{RAG}-Thief},
	abstract = {While large language models ({LLMs}) have achieved notable success in generative tasks, they still face limitations, such as lacking up-to-date knowledge and producing hallucinations. Retrieval-Augmented Generation ({RAG}) enhances {LLM} performance by integrating external knowledge bases, providing additional context which significantly improves accuracy and knowledge coverage. However, building these external knowledge bases often requires substantial resources and may involve sensitive information. In this paper, we propose an agent-based automated privacy attack called {RAG}-Thief, which can extract a scalable amount of private data from the private database used in {RAG} applications. We conduct a systematic study on the privacy risks associated with {RAG} applications, revealing that the vulnerability of {LLMs} makes the private knowledge bases suffer significant privacy risks. Unlike previous manual attacks which rely on traditional prompt injection techniques, {RAG}-Thief starts with an initial adversarial query and learns from model responses, progressively generating new queries to extract as many chunks from the knowledge base as possible. Experimental results show that our {RAG}-Thief can extract over 70\% information from the private knowledge bases within customized {RAG} applications deployed on local machines and real-world platforms, including {OpenAI}'s {GPTs} and {ByteDance}'s Coze. Our findings highlight the privacy vulnerabilities in current {RAG} applications and underscore the pressing need for stronger safeguards.},
	number = {{arXiv}:2411.14110},
	publisher = {{arXiv}},
	author = {Jiang, Changyue and Pan, Xudong and Hong, Geng and Bao, Chenfu and Yang, Min},
	urldate = {2024-11-24},
	date = {2024-11-21},
	eprinttype = {arxiv},
	eprint = {2411.14110},
	keywords = {Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/XPXNDMTF/Jiang et al. - 2024 - RAG-Thief Scalable Extraction of Private Data from Retrieval-Augmented Generation Applications with.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/A3PCFMA8/2411.html:text/html},
}

@online{noauthor_diffchecker_nodate,
	title = {Diffchecker - Compare text online to find the difference between two text files},
	url = {https://www.diffchecker.com/text-compare/},
	abstract = {Diffchecker will compare text to find the difference between two text files. Just paste your files and click Find Difference!},
	urldate = {2024-11-28},
	langid = {english},
	file = {Snapshot:/Users/xiaohu/Zotero/storage/TE8HA2TA/text-compare.html:text/html},
}

@inproceedings{hewitt_designing_2019,
	location = {Hong Kong, China},
	title = {Designing and Interpreting Probes with Control Tasks},
	url = {https://aclanthology.org/D19-1275},
	doi = {10.18653/v1/D19-1275},
	abstract = {Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like {ELMo}), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? In this paper, we propose control tasks, which associate word types with random outputs, to complement linguistic tasks. By construction, these tasks can only be learned by the probe itself. So a good probe, (one that reflects the representation), should be selective, achieving high linguistic task accuracy and low control task accuracy. The selectivity of a probe puts linguistic task accuracy in context with the probe's capacity to memorize from word types. We construct control tasks for English part-of-speech tagging and dependency edge prediction, and show that popular probes on {ELMo} representations are not selective. We also find that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of {MLPs}, but that other forms of regularization are effective. Finally, we find that while probes on the first layer of {ELMo} yield slightly better part-of-speech tagging accuracy than the second, probes on the second layer are substantially more selective, which raises the question of which layer better represents parts-of-speech.},
	eventtitle = {{EMNLP}-{IJCNLP} 2019},
	pages = {2733--2743},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Hewitt, John and Liang, Percy},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	urldate = {2024-12-04},
	date = {2019-11},
	file = {Full Text PDF:/Users/xiaohu/Zotero/storage/UJ5V7884/Hewitt and Liang - 2019 - Designing and Interpreting Probes with Control Tasks.pdf:application/pdf},
}

@misc{xu_theory_2020,
	title = {A Theory of Usable Information Under Computational Constraints},
	url = {http://arxiv.org/abs/2002.10689},
	doi = {10.48550/arXiv.2002.10689},
	abstract = {We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon's information theory that takes into account the modeling power and computational constraints of the observer. The resulting {\textbackslash}emph\{predictive \${\textbackslash}mathcal\{V\}\$-information\} encompasses mutual information and other notions of informativeness such as the coefficient of determination. Unlike Shannon's mutual information and in violation of the data processing inequality, \${\textbackslash}mathcal\{V\}\$-information can be created through computation. This is consistent with deep neural networks extracting hierarchies of progressively more informative features in representation learning. Additionally, we show that by incorporating computational constraints, \${\textbackslash}mathcal\{V\}\$-information can be reliably estimated from data even in high dimensions with {PAC}-style guarantees. Empirically, we demonstrate predictive \${\textbackslash}mathcal\{V\}\$-information is more effective than mutual information for structure learning and fair representation learning.},
	number = {{arXiv}:2002.10689},
	publisher = {{arXiv}},
	author = {Xu, Yilun and Zhao, Shengjia and Song, Jiaming and Stewart, Russell and Ermon, Stefano},
	urldate = {2024-12-04},
	date = {2020-02-25},
	eprinttype = {arxiv},
	eprint = {2002.10689 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/7STEQXAC/Xu et al. - 2020 - A Theory of Usable Information Under Computational Constraints.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/3B7RGCCE/2002.html:text/html},
}

@inproceedings{ethayarajh_understanding_2022,
	title = {Understanding Dataset Difficulty with \${\textbackslash}mathcal\{V\}\$-Usable Information},
	url = {https://proceedings.mlr.press/v162/ethayarajh22a.html},
	abstract = {Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty—w.r.t. a model {VV}{\textbackslash}mathcal\{V\}—as the lack of {VV}{\textbackslash}mathcal\{V\}-usable information (Xu et al., 2019), where a lower value indicates a more difficult dataset for {VV}{\textbackslash}mathcal\{V\}. We further introduce pointwise {VV}{\textbackslash}mathcal\{V\}-information ({PVI}) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, {VV}{\textbackslash}mathcal\{V\}-usable information and {PVI} also permit the converse: for a given model {VV}{\textbackslash}mathcal\{V\}, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used {NLP} benchmarks.},
	eventtitle = {International Conference on Machine Learning},
	pages = {5988--6008},
	booktitle = {Proceedings of the 39th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha},
	urldate = {2024-12-04},
	date = {2022-06-28},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/Users/xiaohu/Zotero/storage/53I9QV2D/Ethayarajh et al. - 2022 - Understanding Dataset Difficulty with \$mathcal V \$-Usable Information.pdf:application/pdf},
}

@misc{penedo_refinedweb_2023,
	title = {The {RefinedWeb} Dataset for Falcon {LLM}: Outperforming Curated Corpora with Web Data, and Web Data Only},
	url = {http://arxiv.org/abs/2306.01116},
	doi = {10.48550/arXiv.2306.01116},
	shorttitle = {The {RefinedWeb} Dataset for Falcon {LLM}},
	abstract = {Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from {CommonCrawl}. We publicly release an extract of 600 billion tokens from our {RefinedWeb} dataset, and 1.3/7.5B parameters language models trained on it.},
	number = {{arXiv}:2306.01116},
	publisher = {{arXiv}},
	author = {Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
	urldate = {2024-12-04},
	date = {2023-06-01},
	eprinttype = {arxiv},
	eprint = {2306.01116 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/AJWJSAU5/Penedo et al. - 2023 - The RefinedWeb Dataset for Falcon LLM Outperforming Curated Corpora with Web Data, and Web Data Onl.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/F9T2Q4BW/2306.html:text/html},
}

@misc{liu_prompt_2024,
	title = {Prompt Injection attack against {LLM}-integrated Applications},
	url = {http://arxiv.org/abs/2306.05499},
	doi = {10.48550/arXiv.2306.05499},
	abstract = {Large Language Models ({LLMs}), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual {LLM}-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate {HouYi}, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. {HouYi} is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging {HouYi}, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary {LLM} usage and uncomplicated application prompt theft. We deploy {HouYi} on 36 actual {LLM}-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.},
	number = {{arXiv}:2306.05499},
	publisher = {{arXiv}},
	author = {Liu, Yi and Deng, Gelei and Li, Yuekang and Wang, Kailong and Wang, Zihao and Wang, Xiaofeng and Zhang, Tianwei and Liu, Yepang and Wang, Haoyu and Zheng, Yan and Liu, Yang},
	urldate = {2024-12-10},
	date = {2024-03-02},
	eprinttype = {arxiv},
	eprint = {2306.05499 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/LIU8IHRX/Liu et al. - 2024 - Prompt Injection attack against LLM-integrated Applications.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/LSJ7D47K/2306.html:text/html},
}

@article{liu_formalizing_nodate,
	title = {Formalizing and Benchmarking Prompt Injection Attacks and Defenses},
	abstract = {A prompt injection attack aims to inject malicious instruction/data into the input of an {LLM}-Integrated Application such that it produces results as an attacker desires. Existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a framework to formalize prompt injection attacks. Existing attacks are special cases in our framework. Moreover, based on our framework, we design a new attack by combining existing ones. Using our framework, we conduct a systematic evaluation on 5 prompt injection attacks and 10 defenses with 10 {LLMs} and 7 tasks. Our work provides a common benchmark for quantitatively evaluating future prompt injection attacks and defenses. To facilitate research on this topic, we make our platform public at https: //github.com/liu00222/Open-Prompt-Injection.},
	author = {Liu, Yupei and Geng, Runpeng and Jia, Jinyuan and Gong, Neil Zhenqiang},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/4V7TIUSF/Liu et al. - Formalizing and Benchmarking Prompt Injection Attacks and Defenses.pdf:application/pdf},
}

@misc{greshake_not_2023,
	title = {Not what you've signed up for: Compromising Real-World {LLM}-Integrated Applications with Indirect Prompt Injection},
	url = {http://arxiv.org/abs/2302.12173},
	doi = {10.48550/arXiv.2302.12173},
	shorttitle = {Not what you've signed up for},
	abstract = {Large Language Models ({LLMs}) are increasingly being integrated into various applications. The functionalities of recent {LLMs} can be flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection ({PI}) attacks enable attackers to override original instructions and employed controls. So far, it was assumed that the user is directly prompting the {LLM}. But, what if it is not the user prompting? We argue that {LLM}-Integrated Applications blur the line between data and instructions. We reveal new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (without a direct interface) exploit {LLM}-integrated applications by strategically injecting prompts into data likely to be retrieved. We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We demonstrate our attacks' practical viability against both real-world systems, such as Bing's {GPT}-4 powered Chat and code-completion engines, and synthetic applications built on {GPT}-4. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other {APIs} are called. Despite the increasing integration and reliance on {LLMs}, effective mitigations of these emerging threats are currently lacking. By raising awareness of these vulnerabilities and providing key insights into their implications, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users and systems from potential attacks.},
	number = {{arXiv}:2302.12173},
	publisher = {{arXiv}},
	author = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
	urldate = {2024-12-09},
	date = {2023-05-05},
	eprinttype = {arxiv},
	eprint = {2302.12173 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Computers and Society},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/ULIIMY8F/Greshake et al. - 2023 - Not what you've signed up for Compromising Real-World LLM-Integrated Applications with Indirect Pro.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/98M9JRF5/2302.html:text/html},
}

@misc{min_silo_2024,
	title = {{SILO} Language Models: Isolating Legal Risk In a Nonparametric Datastore},
	url = {http://arxiv.org/abs/2308.04430},
	doi = {10.48550/arXiv.2308.04430},
	shorttitle = {{SILO} Language Models},
	abstract = {The legality of training language models ({LMs}) on copyrighted or otherwise restricted data is under intense debate. However, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and domain coverage. We present {SILO}, a new language model that manages this risk-performance tradeoff during inference. {SILO} is built by (1) training a parametric {LM} on Open License Corpus ({OLC}), a new corpus we curate with 228B tokens of public domain and permissively licensed text and (2) augmenting it with a more general and easily modifiable nonparametric datastore (e.g., containing copyrighted books or news) that is only queried during inference. The datastore allows use of high-risk data without training on it, supports sentence-level data attribution, and enables data producers to opt out from the model by removing content from the store. These capabilities can foster compliance with data-use regulations such as the fair use doctrine in the United States and the {GDPR} in the European Union. Our experiments show that the parametric {LM} struggles on domains not covered by {OLC}. However, access to the datastore greatly improves out of domain performance, closing 90\% of the performance gap with an {LM} trained on the Pile, a more diverse corpus with mostly high-risk text. We also analyze which nonparametric approach works best, where the remaining errors lie, and how performance scales with datastore size. Our results suggest that it is possible to build high quality language models while mitigating their legal risk.},
	number = {{arXiv}:2308.04430},
	publisher = {{arXiv}},
	author = {Min, Sewon and Gururangan, Suchin and Wallace, Eric and Shi, Weijia and Hajishirzi, Hannaneh and Smith, Noah A. and Zettlemoyer, Luke},
	urldate = {2024-12-08},
	date = {2024-07-31},
	eprinttype = {arxiv},
	eprint = {2308.04430 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/DZBZ6JDM/Min et al. - 2024 - SILO Language Models Isolating Legal Risk In a Nonparametric Datastore.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/EGIMHZIJ/2308.html:text/html},
}

@misc{wiratunga_cbr-rag_2024,
	title = {{CBR}-{RAG}: Case-Based Reasoning for Retrieval Augmented Generation in {LLMs} for Legal Question Answering},
	url = {http://arxiv.org/abs/2404.04302},
	doi = {10.48550/arXiv.2404.04302},
	shorttitle = {{CBR}-{RAG}},
	abstract = {Retrieval-Augmented Generation ({RAG}) enhances Large Language Model ({LLM}) output by providing prior knowledge as context to input. This is beneficial for knowledge-intensive and expert reliant tasks, including legal question-answering, which require evidence to validate generated text outputs. We highlight that Case-Based Reasoning ({CBR}) presents key opportunities to structure retrieval as part of the {RAG} process in an {LLM}. We introduce {CBR}-{RAG}, where {CBR} cycle's initial retrieval stage, its indexing vocabulary, and similarity knowledge containers are used to enhance {LLM} queries with contextually relevant cases. This integration augments the original {LLM} query, providing a richer prompt. We present an evaluation of {CBR}-{RAG}, and examine different representations (i.e. general and domain-specific embeddings) and methods of comparison (i.e. inter, intra and hybrid similarity) on the task of legal question-answering. Our results indicate that the context provided by {CBR}'s case reuse enforces similarity between relevant components of the questions and the evidence base leading to significant improvements in the quality of generated answers.},
	number = {{arXiv}:2404.04302},
	publisher = {{arXiv}},
	author = {Wiratunga, Nirmalie and Abeyratne, Ramitha and Jayawardena, Lasal and Martin, Kyle and Massie, Stewart and Nkisi-Orji, Ikechukwu and Weerasinghe, Ruvan and Liret, Anne and Fleisch, Bruno},
	urldate = {2024-12-08},
	date = {2024-04-04},
	eprinttype = {arxiv},
	eprint = {2404.04302 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/8VRMPDCJ/Wiratunga et al. - 2024 - CBR-RAG Case-Based Reasoning for Retrieval Augmented Generation in LLMs for Legal Question Answerin.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/9XQ6XH3X/2404.html:text/html},
}

@misc{lala_paperqa_2023,
	title = {{PaperQA}: Retrieval-Augmented Generative Agent for Scientific Research},
	url = {http://arxiv.org/abs/2312.07559},
	doi = {10.48550/arXiv.2312.07559},
	shorttitle = {{PaperQA}},
	abstract = {Large Language Models ({LLMs}) generalize well across language tasks, but suffer from hallucinations and uninterpretability, making it difficult to assess their accuracy without ground-truth. Retrieval-Augmented Generation ({RAG}) models have been proposed to reduce hallucinations and provide provenance for how an answer was generated. Applying such models to the scientific literature may enable large-scale, systematic processing of scientific knowledge. We present {PaperQA}, a {RAG} agent for answering questions over the scientific literature. {PaperQA} is an agent that performs information retrieval across full-text scientific articles, assesses the relevance of sources and passages, and uses {RAG} to provide answers. Viewing this agent as a question answering model, we find it exceeds performance of existing {LLMs} and {LLM} agents on current science {QA} benchmarks. To push the field closer to how humans perform research on scientific literature, we also introduce {LitQA}, a more complex benchmark that requires retrieval and synthesis of information from full-text scientific papers across the literature. Finally, we demonstrate {PaperQA}'s matches expert human researchers on {LitQA}.},
	number = {{arXiv}:2312.07559},
	publisher = {{arXiv}},
	author = {Lála, Jakub and O'Donoghue, Odhran and Shtedritski, Aleksandar and Cox, Sam and Rodriques, Samuel G. and White, Andrew D.},
	urldate = {2024-12-08},
	date = {2023-12-14},
	eprinttype = {arxiv},
	eprint = {2312.07559 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/NCPB8BMY/Lála et al. - 2023 - PaperQA Retrieval-Augmented Generative Agent for Scientific Research.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/PABWZ3A3/2312.html:text/html},
}

@inproceedings{zhao_optimizing_2024,
	location = {Mexico City, Mexico},
	title = {Optimizing {LLM} Based Retrieval Augmented Generation Pipelines in the Financial Domain},
	url = {https://aclanthology.org/2024.naacl-industry.23},
	doi = {10.18653/v1/2024.naacl-industry.23},
	abstract = {Retrieval Augmented Generation ({RAG}) is a prominent approach in real-word applications for grounding large language model ({LLM}) generations in up to date and domain-specific knowledge. However, there is a lack of systematic investigations of the impact of each component (retrieval quality, prompts, generation models) on the generation quality of a {RAG} pipeline in real world scenarios. In this study, we benchmark 6 {LLMs} in 15 retrieval scenarios exploring 9 prompts over 2 real world financial domain datasets. We thoroughly discuss the impact of each component in {RAG} pipeline on answer generation quality and formulate specific recommendations for the design of {RAG} systems.},
	eventtitle = {{NAACL}-{HLT} 2024},
	pages = {279--294},
	booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track)},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Yiyun and Singh, Prateek and Bhathena, Hanoz and Ramos, Bernardo and Joshi, Aviral and Gadiyaram, Swaroop and Sharma, Saket},
	editor = {Yang, Yi and Davani, Aida and Sil, Avi and Kumar, Anoop},
	urldate = {2024-12-08},
	date = {2024-06},
	file = {Full Text PDF:/Users/xiaohu/Zotero/storage/44GZTPAC/Zhao et al. - 2024 - Optimizing LLM Based Retrieval Augmented Generation Pipelines in the Financial Domain.pdf:application/pdf},
}

@inproceedings{zhang_enhancing_2023,
	location = {New York, {NY}, {USA}},
	title = {Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models},
	isbn = {979-8-4007-0240-2},
	url = {https://doi.org/10.1145/3604237.3626866},
	doi = {10.1145/3604237.3626866},
	series = {{ICAIF} '23},
	abstract = {Financial sentiment analysis is critical for valuation and investment decision-making. Traditional {NLP} models, however, are limited by their parameter size and the scope of their training datasets, which hampers their generalization capabilities and effectiveness in this field. Recently, Large Language Models ({LLMs}) pre-trained on extensive corpora have demonstrated superior performance across various {NLP} tasks due to their commendable zero-shot abilities. Yet, directly applying {LLMs} to financial sentiment analysis presents challenges: The discrepancy between the pre-training objective of {LLMs} and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of {LLMs}’ sentiment analysis. To address these challenges, we introduce a retrieval-augmented {LLMs} framework for financial sentiment analysis. This framework includes an instruction-tuned {LLMs} module, which ensures {LLMs} behave as predictors of sentiment labels, and a retrieval-augmentation module which retrieves additional context from reliable external sources. Benchmarked against traditional models and {LLMs} like {ChatGPT} and {LLaMA}, our approach achieves 15\% to 48\% performance gain in accuracy and F1 score.},
	pages = {349--356},
	booktitle = {Proceedings of the Fourth {ACM} International Conference on {AI} in Finance},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Boyu and Yang, Hongyang and Zhou, Tianyu and Ali Babar, Muhammad and Liu, Xiao-Yang},
	urldate = {2024-12-08},
	date = {2023},
	file = {Submitted Version:/Users/xiaohu/Zotero/storage/3V6LRTAF/Zhang et al. - 2023 - Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models.pdf:application/pdf},
}

@inproceedings{wolfTransformersStateoftheArtNatural2020,
	location = {Online},
	title = {Transformers: State-of-the-Art Natural Language Processing},
	url = {https://aclanthology.org/2020.emnlp-demos.6},
	doi = {10.18653/v1/2020.emnlp-demos.6},
	shorttitle = {Transformers},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified {API}. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
	pages = {38--45},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
	editor = {Liu, Qun and Schlangen, David},
	urldate = {2024-12-12},
	date = {2020-10},
	file = {Full Text PDF:C\:\\Users\\huxia\\Zotero\\storage\\D53TU3TL\\Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Processing.pdf:application/pdf},
}

@article{kritharaBioASQQAManuallyCurated2023,
	title = {{BioASQ}-{QA}: A manually curated corpus for Biomedical Question Answering},
	volume = {10},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-023-02068-4},
	doi = {10.1038/s41597-023-02068-4},
	shorttitle = {{BioASQ}-{QA}},
	abstract = {Abstract
            The {BioASQ} question answering ({QA}) benchmark dataset contains questions in English, along with golden standard (reference) answers and related material. The dataset has been designed to reflect real information needs of biomedical experts and is therefore more realistic and challenging than most existing datasets. Furthermore, unlike most previous {QA} benchmarks that contain only exact answers, the {BioASQ}-{QA} dataset also includes ideal answers (in effect summaries), which are particularly useful for research on multi-document summarization. The dataset combines structured and unstructured data. The materials linked with each question comprise documents and snippets, which are useful for Information Retrieval and Passage Retrieval experiments, as well as concepts that are useful in concept-to-text Natural Language Generation. Researchers working on paraphrasing and textual entailment can also measure the degree to which their methods improve the performance of biomedical {QA} systems. Last but not least, the dataset is continuously extended, as the {BioASQ} challenge is running and new data are generated.},
	pages = {170},
	number = {1},
	journaltitle = {Scientific Data},
	shortjournal = {Sci Data},
	author = {Krithara, Anastasia and Nentidis, Anastasios and Bougiatiotis, Konstantinos and Paliouras, Georgios},
	urldate = {2024-12-12},
	date = {2023-03-27},
	langid = {english},
	file = {Full Text:C\:\\Users\\huxia\\Zotero\\storage\\CX2XGPGP\\Krithara et al. - 2023 - BioASQ-QA A manually curated corpus for Biomedical Question Answering.pdf:application/pdf},
}

@misc{githubGitHubKingoflolzmeshtransformerjax,
	author = {},
	title = {{G}it{H}ub - kingoflolz/mesh-transformer-jax: {M}odel parallel transformers in {J}{A}{X} and {H}aiku --- github.com},
	howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax/}},
	year = {},
}

@misc{huggingfaceDatabricksdollyv27bHugging,
	author = {},
	title = {databricks/dolly-v2-7b · {H}ugging {F}ace --- huggingface.co},
	howpublished = {\url{https://huggingface.co/databricks/dolly-v2-7b}},
	year = {},
}

@misc{glaiveGlaiveCustom,
	author = {},
	title = {{G}laive - {C}ustom datasets for all --- glaive.ai},
	howpublished = {\url{https://glaive.ai/}},
	year = {},
}

@misc{huggingfaceSentencetransformersallMiniLML6v2Hugging,
	author = {},
	title = {sentence-transformers/all-{M}ini{L}{M}-{L}6-v2 · {H}ugging {F}ace --- huggingface.co},
	howpublished = {\url{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}},
	year = {},
}

@misc{githubGitHubQwenLMQwen25,
	author = {},
	title = {{G}it{H}ub - {Q}wen{L}{M}/{Q}wen2.5: {Q}wen2.5 is the large language model series developed by {Q}wen team, {A}libaba {C}loud. --- github.com},
	howpublished = {\url{https://github.com/QwenLM/Qwen2.5}},
	year = {},
}


@misc{touvronLlama2Open2023,
	title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
	url = {http://arxiv.org/abs/2307.09288},
	doi = {10.48550/arXiv.2307.09288},
	shorttitle = {Llama 2},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models ({LLMs}) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned {LLMs}, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of {LLMs}.},
	number = {{arXiv}:2307.09288},
	publisher = {{arXiv}},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	urldate = {2024-12-16},
	date = {2023-07-19},
	eprinttype = {arxiv},
	eprint = {2307.09288 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\huxia\\Zotero\\storage\\4WLJUUF6\\Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Models.pdf:application/pdf;Snapshot:C\:\\Users\\huxia\\Zotero\\storage\\GDMUZ722\\2307.html:text/html},
}

@misc{almazroueiFalconSeriesOpen2023,
	title = {The Falcon Series of Open Language Models},
	url = {http://arxiv.org/abs/2311.16867},
	doi = {10.48550/arXiv.2311.16867},
	abstract = {We introduce the Falcon series: 7B, 40B, and 180B parameters causal decoder-only models trained on a diverse high-quality corpora predominantly assembled from web data. The largest model, Falcon-180B, has been trained on over 3.5 trillion tokens of text--the largest openly documented pretraining run. Falcon-180B significantly outperforms models such as {PaLM} or Chinchilla, and improves upon concurrently developed models such as {LLaMA} 2 or Inflection-1. It nears the performance of {PaLM}-2-Large at a reduced pretraining and inference cost, making it, to our knowledge, one of the three best language models in the world along with {GPT}-4 and {PaLM}-2-Large. We report detailed evaluations, as well as a deep dive into the methods and custom tooling employed to pretrain Falcon. Notably, we report on our custom distributed training codebase, allowing us to efficiently pretrain these models on up to 4,096 A100s on cloud {AWS} infrastructure with limited interconnect. We release a 600B tokens extract of our web dataset, as well as the Falcon-7/40/180B models under a permissive license to foster open-science and accelerate the development of an open ecosystem of large language models.},
	number = {{arXiv}:2311.16867},
	publisher = {{arXiv}},
	author = {Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Mérouane and Goffinet, Étienne and Hesslow, Daniel and Launay, Julien and Malartic, Quentin and Mazzotta, Daniele and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
	urldate = {2024-12-16},
	date = {2023-11-29},
	eprinttype = {arxiv},
	eprint = {2311.16867 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\huxia\\Zotero\\storage\\KB7MRIRF\\Almazrouei et al. - 2023 - The Falcon Series of Open Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\huxia\\Zotero\\storage\\I3LU4RAR\\2311.html:text/html},
}


@misc{yuRobustLLMSafeguarding2024,
	title = {Robust {LLM} safeguarding via refusal feature adversarial training},
	url = {http://arxiv.org/abs/2409.20089},
	doi = {10.48550/arXiv.2409.20089},
	abstract = {Large language models ({LLMs}) are vulnerable to adversarial attacks that can elicit harmful responses. Defending against such attacks remains challenging due to the opacity of jailbreaking mechanisms and the high computational cost of training {LLMs} robustly. We demonstrate that adversarial attacks share a universal mechanism for circumventing {LLM} safeguards that works by ablating a dimension in the residual stream embedding space called the refusal feature. We further show that the operation of refusal feature ablation ({RFA}) approximates the worst-case perturbation of offsetting model safety. Based on these findings, we propose Refusal Feature Adversarial Training ({ReFAT}), a novel algorithm that efficiently performs {LLM} adversarial training by simulating the effect of input-level attacks via {RFA}. Experiment results show that {ReFAT} significantly improves the robustness of three popular {LLMs} against a wide range of adversarial attacks, with considerably less computational overhead compared to existing adversarial training methods.},
	number = {{arXiv}:2409.20089},
	publisher = {{arXiv}},
	author = {Yu, Lei and Do, Virginie and Hambardzumyan, Karen and Cancedda, Nicola},
	urldate = {2024-12-19},
	date = {2024-09-30},
	eprinttype = {arxiv},
	eprint = {2409.20089 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\huxia\\Zotero\\storage\\Y2MD6EV4\\Yu et al. - 2024 - Robust LLM safeguarding via refusal feature adversarial training.pdf:application/pdf;Snapshot:C\:\\Users\\huxia\\Zotero\\storage\\D376LXTZ\\2409.html:text/html},
}

@misc{huggingfaceQwenQwen257BInstructHugging,
	author = {},
	title = {{Q}wen/{Q}wen2.5-7{B}-{I}nstruct · {H}ugging {F}ace --- huggingface.co},
	howpublished = {\url{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct}},
	year = {},
}

@inproceedings{freitagBeamSearchStrategies2017,
	title = {Beam Search Strategies for Neural Machine Translation},
	url = {http://arxiv.org/abs/1702.01806},
	doi = {10.18653/v1/W17-3207},
	abstract = {The basic concept in Neural Machine Translation ({NMT}) is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. {NMT} is then using a simple left-to-right beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-to- right while keeping a fixed amount of active candidates at each time step. First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the beam size until no performance improvement can be observed. While you can reach better performance, this has the draw- back of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43\% for the two language pairs German-English and Chinese-English without losing any translation quality.},
	pages = {56--60},
	booktitle = {Proceedings of the First Workshop on Neural Machine Translation},
	author = {Freitag, Markus and Al-Onaizan, Yaser},
	urldate = {2024-12-15},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1702.01806 [cs]},
	keywords = {Computer Science - Computation and Language},
	annotation = {Comment: First Workshop on Neural Machine Translation, 2017},
	file = {Preprint PDF:C\:\\Users\\huxia\\Zotero\\storage\\3QPA8XWC\\Freitag and Al-Onaizan - 2017 - Beam Search Strategies for Neural Machine Translation.pdf:application/pdf;Snapshot:C\:\\Users\\huxia\\Zotero\\storage\\8CFNDJ3M\\1702.html:text/html},
}

@misc{shahamWhatYouGet2022,
	title = {What Do You Get When You Cross Beam Search with Nucleus Sampling?},
	url = {http://arxiv.org/abs/2107.09729},
	doi = {10.48550/arXiv.2107.09729},
	abstract = {We combine beam search with the probabilistic pruning technique of nucleus sampling to create two deterministic nucleus search algorithms for natural language generation. The first algorithm, p-exact search, locally prunes the next-token distribution and performs an exact search over the remaining space. The second algorithm, dynamic beam search, shrinks and expands the beam size according to the entropy of the candidate's probability distribution. Despite the probabilistic intuition behind nucleus search, experiments on machine translation and summarization benchmarks show that both algorithms reach the same performance levels as standard beam search.},
	number = {{arXiv}:2107.09729},
	publisher = {{arXiv}},
	author = {Shaham, Uri and Levy, Omer},
	urldate = {2024-12-15},
	date = {2022-05-02},
	eprinttype = {arxiv},
	eprint = {2107.09729 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annotation = {Comment: The Third Workshop on Insights from Negative Results in {NLP}},
	file = {Preprint PDF:C\:\\Users\\huxia\\Zotero\\storage\\3UQWDVF5\\Shaham and Levy - 2022 - What Do You Get When You Cross Beam Search with Nucleus Sampling.pdf:application/pdf;Snapshot:C\:\\Users\\huxia\\Zotero\\storage\\R3QLX97H\\2107.html:text/html},
}

@misc{huggingfaceTextGeneration,
	author = {},
	title = {{T}ext generation strategies --- huggingface.co},
	howpublished = {\url{https://huggingface.co/docs/transformers/generation_strategies#greedy-search}},
	year = {},
}

@misc{holtzmanCuriousCaseNeural2020,
	title = {The Curious Case of Neural Text Degeneration},
	url = {http://arxiv.org/abs/1904.09751},
	doi = {10.48550/arXiv.1904.09751},
	abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
	number = {{arXiv}:1904.09751},
	publisher = {{arXiv}},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	urldate = {2024-12-15},
	date = {2020-02-14},
	eprinttype = {arxiv},
	eprint = {1904.09751 [cs]},
	keywords = {Computer Science - Computation and Language},
	annotation = {Comment: Published in {ICLR} 2020},
	file = {Preprint PDF:C\:\\Users\\huxia\\Zotero\\storage\\WHAXZTYJ\\Holtzman et al. - 2020 - The Curious Case of Neural Text Degeneration.pdf:application/pdf;Snapshot:C\:\\Users\\huxia\\Zotero\\storage\\K96EAFFI\\1904.html:text/html},
}


@misc{zhangOPTOpenPretrained2022,
	title = {{OPT}: Open Pre-trained Transformer Language Models},
	url = {http://arxiv.org/abs/2205.01068},
	doi = {10.48550/arXiv.2205.01068},
	shorttitle = {{OPT}},
	abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through {APIs}, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers ({OPT}), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that {OPT}-175B is comparable to {GPT}-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
	number = {{arXiv}:2205.01068},
	publisher = {{arXiv}},
	author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
	urldate = {2024-12-13},
	date = {2022-06-21},
	eprinttype = {arxiv},
	eprint = {2205.01068 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\huxia\\Zotero\\storage\\IZQZQJIS\\Zhang et al. - 2022 - OPT Open Pre-trained Transformer Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\huxia\\Zotero\\storage\\Q728B38Z\\2205.html:text/html},
}

@online{VicunaOpenSourceChatbot,
	title = {Vicuna: An Open-Source Chatbot Impressing {GPT}-4 with 90\%* {ChatGPT} Quality {\textbar} {LMSYS} Org},
	url = {https://lmsys.org/blog/2023-03-30-vicuna},
	shorttitle = {Vicuna},
	abstract = {{\textless}p{\textgreater}We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning {LLaMA} on user-shared conversations collected from {ShareGPT}. Preliminary evaluation ...},
	urldate = {2024-12-13},
	langid = {english},
	file = {Snapshot:C\:\\Users\\huxia\\Zotero\\storage\\GTPUG7UP\\2023-03-30-vicuna.html:text/html},
}

@misc{jiangMistral7B2023,
	title = {Mistral 7B},
	url = {http://arxiv.org/abs/2310.06825},
	doi = {10.48550/arXiv.2310.06825},
	abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention ({GQA}) for faster inference, coupled with sliding window attention ({SWA}) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
	number = {{arXiv}:2310.06825},
	publisher = {{arXiv}},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
	urldate = {2024-12-13},
	date = {2023-10-10},
	eprinttype = {arxiv},
	eprint = {2310.06825 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annotation = {Comment: Models and code are available at https://mistral.ai/news/announcing-mistral-7b/},
	file = {Preprint PDF:C\:\\Users\\huxia\\Zotero\\storage\\5M84L6FX\\Jiang et al. - 2023 - Mistral 7B.pdf:application/pdf;Snapshot:C\:\\Users\\huxia\\Zotero\\storage\\XBQD7TNT\\2310.html:text/html},
}

@misc{grattafioriLlama3Herd2024,
	title = {The Llama 3 Herd of Models},
	url = {http://arxiv.org/abs/2407.21783},
	doi = {10.48550/arXiv.2407.21783},
	abstract = {Modern artificial intelligence ({AI}) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as {GPT}-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	number = {{arXiv}:2407.21783},
	publisher = {{arXiv}},
	author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri et al},
	urldate = {2024-12-13},
	date = {2024-11-23},
	eprinttype = {arxiv},
	eprint = {2407.21783 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\huxia\\Zotero\\storage\\69ZWQDSU\\Grattafiori et al. - 2024 - The Llama 3 Herd of Models.pdf:application/pdf;Snapshot:C\:\\Users\\huxia\\Zotero\\storage\\G4HHLGTT\\2407.html:text/html},
}


@misc{carliniExtractingTrainingData2023,
	title = {Extracting Training Data from Diffusion Models},
	url = {http://arxiv.org/abs/2301.13188},
	doi = {10.48550/arXiv.2301.13188},
	abstract = {Image diffusion models such as {DALL}-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as {GANs}, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.},
	number = {{arXiv}:2301.13188},
	publisher = {{arXiv}},
	author = {Carlini, Nicholas and Hayes, Jamie and Nasr, Milad and Jagielski, Matthew and Sehwag, Vikash and Tramèr, Florian and Balle, Borja and Ippolito, Daphne and Wallace, Eric},
	urldate = {2024-12-12},
	date = {2023-01-30},
	eprinttype = {arxiv},
	eprint = {2301.13188 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\huxia\\Zotero\\storage\\FX4Q7E5U\\Carlini et al. - 2023 - Extracting Training Data from Diffusion Models.pdf:application/pdf;Snapshot:C\:\\Users\\huxia\\Zotero\\storage\\5CUWB5GJ\\2301.html:text/html},
}

@article{carliniExtractingTrainingData,
	title = {Extracting Training Data from Large Language Models},
	abstract = {It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model.},
	author = {Carlini, Nicholas and Tramèr, Florian and Brown, Tom and Song, Dawn and Oprea, Alina},
	langid = {english},
	file = {PDF:C\:\\Users\\huxia\\Zotero\\storage\\762EIRV2\\Carlini et al. - Extracting Training Data from Large Language Models.pdf:application/pdf},
}

@inproceedings{s_rag-based_2024,
	title = {A {RAG}-based Medical Assistant Especially for Infectious Diseases},
	url = {https://ieeexplore.ieee.org/abstract/document/10544639?casa_token=ZkVKBgYFd-UAAAAA:o8CNHOlxLzimVog74O6r7kqWJ0cBxUIpwKXwD7m3T5CGsCcIeKWPDYc1wRkNXDpyiFe6xgy2VVY},
	doi = {10.1109/ICICT60155.2024.10544639},
	abstract = {Infectious diseases like {COVID}-19 have gained international attention recently. Furthermore, there are significantly fewer doctors per capita in densely populated nations like India, which hurts those in need. Under such circumstances, natural language processing techniques might make it feasible to create an intelligent and engaging chatbot system. The primary objective of the effort is to develop an interactive solution that is entirely open source and can be easily installed on a local computer using the most recent data. Even though there are numerous chatbots on the market, proposed solutions highlight the need to provide individualized and sympathetic responses. Getting Back While the data is stored in the graph database as nodes and relationships, and the knowledge graph is constructed on top of it, augmented generation is utilized to extract the pertinent content from the data. To improve the generator’s context, pertinent sections are collected during the question-answering process. This reduces hallucinations and increases the correctness of abstractions by providing external knowledge streams. Furthermore, the research study employs a text-to-speech model that was replicated from a physician’s voice recording to narrate the produced responses, thereby augmenting user confidence and interaction. Academic institutions and healthcare organizations can benefit from this work by better understanding the value and effectiveness of applying {NLP} techniques to infectious disease research.},
	eventtitle = {2024 International Conference on Inventive Computation Technologies ({ICICT})},
	pages = {1128--1133},
	booktitle = {2024 International Conference on Inventive Computation Technologies ({ICICT})},
	author = {S, Stewart Kirubakaran and G, Jasper Wilsie Kathrine and E, Grace Mary Kanaga and J, Mahimai Raja and Singh A, Ruban Gino and E, Yuvaraajan},
	urldate = {2024-12-08},
	date = {2024-04},
	note = {{ISSN}: 2767-7788},
	keywords = {Artificial intelligence, chatbot, Chatbots, {COVID}-19, Databases, Infectious diseases, knowledge graph, large language model, natural language processing, Recording, Reliability, retrieval augmented generation},
	file = {Full Text PDF:/Users/xiaohu/Zotero/storage/D8KCSIVG/S et al. - 2024 - A RAG-based Medical Assistant Especially for Infectious Diseases.pdf:application/pdf},
}

@misc{zhu_realm_2024,
	title = {{REALM}: {RAG}-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Models},
	url = {http://arxiv.org/abs/2402.07016},
	doi = {10.48550/arXiv.2402.07016},
	shorttitle = {{REALM}},
	abstract = {The integration of multimodal Electronic Health Records ({EHR}) data has significantly improved clinical predictive capabilities. Leveraging clinical notes and multivariate time-series {EHR}, existing models often lack the medical context relevent to clinical tasks, prompting the incorporation of external knowledge, particularly from the knowledge graph ({KG}). Previous approaches with {KG} knowledge have primarily focused on structured knowledge extraction, neglecting unstructured data modalities and semantic high dimensional medical knowledge. In response, we propose {REALM}, a Retrieval-Augmented Generation ({RAG}) driven framework to enhance multimodal {EHR} representations that address these limitations. Firstly, we apply Large Language Model ({LLM}) to encode long context clinical notes and {GRU} model to encode time-series {EHR} data. Secondly, we prompt {LLM} to extract task-relevant medical entities and match entities in professionally labeled external knowledge graph ({PrimeKG}) with corresponding medical knowledge. By matching and aligning with clinical standards, our framework eliminates hallucinations and ensures consistency. Lastly, we propose an adaptive multimodal fusion network to integrate extracted knowledge with multimodal {EHR} data. Our extensive experiments on {MIMIC}-{III} mortality and readmission tasks showcase the superior performance of our {REALM} framework over baselines, emphasizing the effectiveness of each module. {REALM} framework contributes to refining the use of multimodal {EHR} data in healthcare and bridging the gap with nuanced medical context essential for informed clinical predictions.},
	number = {{arXiv}:2402.07016},
	publisher = {{arXiv}},
	author = {Zhu, Yinghao and Ren, Changyu and Xie, Shiyun and Liu, Shukai and Ji, Hangyuan and Wang, Zixiang and Sun, Tao and He, Long and Li, Zhoujun and Zhu, Xi and Pan, Chengwei},
	urldate = {2024-12-08},
	date = {2024-02-10},
	eprinttype = {arxiv},
	eprint = {2402.07016 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/WBMLCZMT/Zhu et al. - 2024 - REALM RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large Language Mo.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/3UMXLZB8/2402.html:text/html},
}

@inproceedings{lewis_retrieval-augmented_2020,
	title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream {NLP} tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation ({RAG}) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce {RAG} models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two {RAG} formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive {NLP} tasks and set the state-of-the-art on three open domain {QA} tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that {RAG} models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	pages = {9459--9474},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	urldate = {2024-12-08},
	date = {2020},
	file = {Full Text PDF:/Users/xiaohu/Zotero/storage/FCL7QK3B/Lewis et al. - 2020 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf:application/pdf},
}
