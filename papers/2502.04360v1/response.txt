\section{Related Work}
\subsection{Attacks against LLMs}
There have been researches investigating the attacks on LLLMs themselves and their applications. Jailbreaking attacks**Ebrahimi et al., "Jailbreaking"**, aim to break the safety alignment of the LLMs so that they can be coerced to output contents that are not aligned with human values. Zou et al. **"Triggering Adversarial Attacks on Text Generators"** proposed to use greedy-based gradient optimization approach to craft an attack suffix that jailbreaks different LLMs. Wei et al. **"Adversarial Examples for Generating Models"** proposed two failure modes, namely competing objectives and mismatched generalization, which can be exploited by carefully crafted attack templates to jailbreak LLMs. Liu et al. **"A Generic Framework for Adversarial Attacks on Text Generators"** designed a genetic algorithm based jailbreaking framework that starts with handcrafted prompts and conducting both word and sentence level cross-over operations to automatically generate jailbreaking prompts. Outside jailbreaking, Denison et al. **"Gaming LLMs: Exploring Unexpected Behavior in Large Language Models"** construct a reinforcement learning setup, where they progressively increase the difficulty of the model to successfully game the different environments and assign rewards. Their result shows that LLMs can progressively generalize to more complicated behaviors like specification gaming even when the HHH environment is presented.

One category of attack that is more related to our work is prompt injection attack, where an adversarial prompt is embedded into the input of an LLM-integrated application to manipulate its behavior in a way desired by the attacker**Liu et al., "A Framework for Formalizing Prompt Injection Attacks"**. These attacks often rely on manually crafted adversarial prompts to influence the LLM's generation. Liu et al. **"Prompt Injection Attacks: A Framework and Evaluation"** introduced a framework to formalize prompt injection attacks and evaluated the effectiveness of various attack templates and defensive strategies. Similarly, Greshake et al. **"Indirect Prompt Injection Attacks on RAG Systems"** developed a taxonomy for indirect prompt injection attacks and demonstrated their feasibility in real-world systems. Moving beyond manual methods, Pleak **"Gradient-Based Optimization of Adversarial Queries for LLMs"** leveraged gradient-based optimization to generate adversarial queries, achieving greater attack effectiveness. Our approach can be seen as a variant of prompt injection, where the adversarial string is appended after the query used for RAG retrieval so that the LLM in the RAG pipeline will be manipulated to spill out the RAG data it saw. The distinction is that we focus on RAG systems as the LLM-integrated application, a relatively under explored area in the field. Furthermore, instead of relying on manual efforts to craft the adversarial query, we adopt an optimization-based approach, which offers better scalability and effectiveness.


\subsection{Attacks on RAG Systems}
The first category is knowledge corruption attacks, where the attacker manipulates the knowledge database, allowing them to control the content retrieved by the system. Zou et al. **"Knowledge Corruption Attacks: Injecting Malicious Contents into Knowledge Databases"** proposed an attack method involving injecting a small amount of malicious contents into the knowledge database. They defined two key conditions, the retrieval condition and the generation condition, which must be satisfied to execute the attack. Once these conditions are met, the injected contents will be retrieved from the database, guiding the LLM to generate outputs that the attacker desires. Deng et al. **"Adversarial Knowledge Graphs: Exploiting LLMs' Tendency in Generating Outputs Based on In-Context Contents"** exploits LLMs' tendency in generating outputs based on the in context contents. Their attack involves crafting the whole knowledge database that contains malicious contents so that once these contents are retrieved and brought into context, the LLM will be jailbroken and generate harmful contents as the attacker desires.

The second category is membership inference attack that aims to infer if a piece of data belongs to the knowledge database. Anderson et al **"Membership Inference Attacks on RAG Systems: A Simple Approach"** proposes a simple approach that directly prompts the RAG system whether a specific piece of data is within the knowledge database. On the other hand, Li et al. **"Semantic Similarity-Based Membership Inference Attack on RAG Systems"** proposes to use semantic similarity between the generated content and the target sample, along with the generation perplexity as the input feature to a trained classification model to determine if a specific sample is within the knowledge database.

\subsection{Prompt stealing Attacks}
There have been studies on prompt stealing attacks in both text generation and multi-modal settings. Morris et al. **"Unrolled Logit-Based Prompt Stealing: A Novel Approach"** proposed a novel approach that utilizes the unrolled logit values from an LLM's outputs as input features to train an encoder-decoder model. This model is designed to map the sequence of logit values back to the corresponding input data, effectively reconstructing the inputs based on the LLM's internal representations. Although this approach does not achieve a high rate of exact matches, it only requires black-box access to the model and relies solely on the output logits. Sha and Zhang **"Prompt Stealing: A Parameter-Extractor-Based Approach"** utilize a parameter extractor to classify prompt types (direct, role-based, in-context) and predict features like roles or context numbers. A prompt reconstructor then uses these features and LLM outputs to recreate prompts. In the text-to-image domain, Shen et al. **"Prompt Stealing for Text-to-Image Models: Identifying Subjects and Modifiers"** demonstrated that reconstructing a prompt for a text-to-image model requires identifying both a subject and several modifiers. They proposed using an image-encoder-text-decoder model to generate the subject and a multi-label classifier to predict possible modifiers for the image. By combining the regenerated subject with the predicted modifiers, they successfully reconstructed prompts capable of generating certain images. A related work by Wen et al. **"Hard Prompt Optimization via Gradient-Based Embedding Derivatives"** optimizes hard text prompts using gradients derived from continuous embeddings. This approach mitigates the high computational cost associated with the discrete token search space. While we were inspired by their approach in solving the discrete optimization problem, our method differs by focusing on text generation rather than image generation. Additionally, we extend it beyond a single model setup, enabling joint optimization across multiple models simultaneously.