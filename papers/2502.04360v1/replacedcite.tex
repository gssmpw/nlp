\section{Related Work}
\subsection{Attacks against LLMs}
There have been researches investigating the attacks on LLMs themselves and their applications. Jailbreaking attacks____,  aim to break the safety alignment of the LLMs so that they can be coerced to output contents that are not aligned with human values. Zou et al. ____ proposed to use greedy-based gradient optimization approach to craft an attack suffix that jailbreaks different LLMs. Wei et al. ____ proposed two failure modes, namely competing objectives and mismatched generalization, which can be exploited by carefully crafted attack templates to jailbreak LLMs. Liu et al. ____ designed a genetic algorithm based jailbreaking framework that starts with handcrafted prompts and conducting both word and sentence level cross-over operations to automatically generate jailbreaking prompts. Outside jailbreaking, Denison et al. ____ construct a reinforcement learning setup, where they progressively increase the difficulty of the model to successfully game the different environments and assign rewards. Their result shows that LLMs can progressively generalize to more complicated behaviors like specification gaming even when the HHH environment is presented. 

One category of attack that is more related to our work is prompt injection attack, where an adversarial prompt is embedded into the input of an LLM-integrated application to manipulate its behavior in a way desired by the attacker____. These attacks often rely on manually crafted adversarial prompts to influence the LLM's generation. Liu et al.____ introduced a framework to formalize prompt injection attacks and evaluated the effectiveness of various attack templates and defensive strategies. Similarly, Greshake et al.____ developed a taxonomy for indirect prompt injection attacks and demonstrated their feasibility in real-world systems. Moving beyond manual methods, Pleak____ leveraged gradient-based optimization to generate adversarial queries, achieving greater attack effectiveness. Our approach can be seen as a variant of prompt injection, where the adversarial string is appended after the query used for RAG retrieval so that the LLM in the RAG pipeline will be manipulated to spill out the RAG data it saw. The distinction is that we focus on RAG systems as the LLM-integrated application, a relatively under explored area in the field. Furthermore, instead of relying on manual efforts to craft the adversarial query, we adopt an optimization-based approach, which offers better scalability and effectiveness.


\subsection{Attacks on RAG Systems}
The first category is knowledge corruption attacks, where the attacker manipulates the knowledge database, allowing them to control the content retrieved by the system. Zou et al.____ proposed an attack method involving injecting a small amount of malicious contents into the knowledge database. They defined two key conditions, the retrieval condition and the generation condition, which must be satisfied to execute the attack. Once these conditions are met, the injected contents will be retrieved from the database, guiding the LLM to generate outputs that the attacker desires. Deng et al. ____ exploits LLMs' tendency in generating outputs based on the in context contents. Their attack involves crafting the whole knowledge database that contains malicious contents so that once these contents are retrieved and brought into context, the LLM will be jailbroken and generate harmful contents as the attacker desires. 

The second category is membership inference attack that aims to infer if a piece of data belongs to the knowledge database. Anderson et al ____ proposes a simple approach that directly prompts the RAG system whether a specific piece of data is within the knowledge database. On the other hand, Li et al. ____ proposes to use semantic similarity between the generated content and the target sample, along with the generation perplexity as the input feature to a trained classification model to determine if a specific sample is within the knowledge database.

\subsection{Prompt stealing Attacks}
There have been studies on prompt stealing attacks in both text generation and multi-modal settings. Morris et al. ____ proposed a novel approach that utilizes the unrolled logit values from an LLM's outputs as input features to train an encoder-decoder model. This model is designed to map the sequence of logit values back to the corresponding input data, effectively reconstructing the inputs based on the LLM's internal representations. Although this approach does not achieve a high rate of exact matches, it only requires black-box access to the model and relies solely on the output logits. Sha and Zhang ____ utilize a parameter extractor to classify prompt types (direct, role-based, in-context) and predict features like roles or context numbers. A prompt reconstructor then uses these features and LLM outputs to recreate prompts. In the text-to-image domain, Shen et al.____ demonstrated that reconstructing a prompt for a text-to-image model requires identifying both a subject and several modifiers. They proposed using an image-encoder-text-decoder model to generate the subject and a multi-label classifier to predict possible modifiers for the image. By combining the regenerated subject with the predicted modifiers, they successfully reconstructed prompts capable of generating certain images. A related work by Wen et al.____ optimizes hard text prompts using gradients derived from continuous embeddings. This approach mitigates the high computational cost associated with the discrete token search space. While we were inspired by their approach in solving the discrete optimization problem, our method differs by focusing on text generation rather than image generation. Additionally, we extend it beyond a single model setup, enabling joint optimization across multiple models simultaneously.