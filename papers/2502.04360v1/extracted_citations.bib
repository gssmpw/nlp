@article{anderson_is_nodate,
	title = {Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation},
	abstract = {Retrieval Augmented Generation ({RAG}) systems have shown great promise in natural language processing. However, their reliance on data stored in a retrieval database, which may contain proprietary or sensitive information, introduces new privacy concerns. Specifically, an attacker may be able to infer whether a certain text passage appears in the retrieval database by observing the outputs of the {RAG} system, an attack known as a Membership Inference Attack ({MIA}). Despite the significance of this threat, {MIAs} against {RAG} systems have yet remained under-explored.},
	author = {Anderson, Maya and Amit, Guy and Goldsteen, Abigail},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/EAKCNQM6/Anderson et al. - Is My Data in Your Retrieval Database Membership Inference Attacks Against Retrieval Augmented Gene.pdf:application/pdf},
}

@misc{andriushchenko_jailbreaking_2024,
	title = {Jailbreaking Leading Safety-Aligned {LLMs} with Simple Adaptive Attacks},
	url = {http://arxiv.org/abs/2404.02151},
	abstract = {We show that even the most recent safety-aligned {LLMs} are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target {LLM}), and then we apply random search on a suffix to maximize a target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve 100\% attack success rate -- according to {GPT}-4 as a judge -- on Vicuna-13B, Mistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B, Llama-3-Instruct-8B, Gemma-7B, {GPT}-3.5, {GPT}-4o, and R2D2 from {HarmBench} that was adversarially trained against the {GCG} attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with a 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many similarities with jailbreaking -- which is the algorithm that brought us the first place in the {SaTML}'24 Trojan Detection Competition. The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different prompting templates (e.g., R2D2 is very sensitive to in-context learning prompts), some models have unique vulnerabilities based on their {APIs} (e.g., prefilling for Claude), and in some settings, it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection). For reproducibility purposes, we provide the code, logs, and jailbreak artifacts in the {JailbreakBench} format at https://github.com/tml-epfl/llm-adaptive-attacks.},
	number = {{arXiv}:2404.02151},
	publisher = {{arXiv}},
	author = {Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas},
	urldate = {2024-10-17},
	date = {2024-10-07},
	eprinttype = {arxiv},
	eprint = {2404.02151},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Cryptography and Security, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/DG2N3248/Andriushchenko et al. - 2024 - Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/MY3BAC63/2404.html:text/html},
}

@misc{deng_pandora_2024,
	title = {Pandora: Jailbreak {GPTs} by Retrieval Augmented Generation Poisoning},
	url = {http://arxiv.org/abs/2402.08416},
	shorttitle = {Pandora},
	abstract = {Large Language Models ({LLMs}) have gained immense popularity and are being increasingly applied in various domains. Consequently, ensuring the security of these models is of paramount importance. Jailbreak attacks, which manipulate {LLMs} to generate malicious content, are recognized as a significant vulnerability. While existing research has predominantly focused on direct jailbreak attacks on {LLMs}, there has been limited exploration of indirect methods. The integration of various plugins into {LLMs}, notably Retrieval Augmented Generation ({RAG}), which enables {LLMs} to incorporate external knowledge bases into their response generation such as {GPTs}, introduces new avenues for indirect jailbreak attacks.},
	number = {{arXiv}:2402.08416},
	publisher = {{arXiv}},
	author = {Deng, Gelei and Liu, Yi and Wang, Kailong and Li, Yuekang and Zhang, Tianwei and Liu, Yang},
	urldate = {2024-09-27},
	date = {2024-02-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2402.08416 [cs]},
	keywords = {Computer Science - Cryptography and Security},
	file = {PDF:/Users/xiaohu/Zotero/storage/WE5FASL2/Deng et al. - 2024 - Pandora Jailbreak GPTs by Retrieval Augmented Generation Poisoning.pdf:application/pdf},
}

@misc{denison_sycophancy_2024,
	title = {Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models},
	url = {http://arxiv.org/abs/2406.10162},
	shorttitle = {Sycophancy to Subterfuge},
	abstract = {In reinforcement learning, specification gaming occurs when {AI} systems learn undesired behaviors that are highly rewarded due to misspecified training goals. Specification gaming can range from simple behaviors like sycophancy to sophisticated and pernicious behaviors like reward-tampering, where a model directly modifies its own reward mechanism. However, these more pernicious behaviors may be too complex to be discovered via exploration. In this paper, we study whether Large Language Model ({LLM}) assistants which find easily discovered forms of specification gaming will generalize to perform rarer and more blatant forms, up to and including reward-tampering. We construct a curriculum of increasingly sophisticated gameable environments and find that training on early-curriculum environments leads to more specification gaming on remaining environments. Strikingly, a small but non-negligible proportion of the time, {LLM} assistants trained on the full curriculum generalize zero-shot to directly rewriting their own reward function. Retraining an {LLM} not to game earlycurriculum environments mitigates, but does not eliminate, reward-tampering in later environments. Moreover, adding harmlessness training to our gameable environments does not prevent reward-tampering. These results demonstrate that {LLMs} can generalize from common forms of specification gaming to more pernicious reward tampering and that such behavior may be nontrivial to remove.},
	number = {{arXiv}:2406.10162},
	publisher = {{arXiv}},
	author = {Denison, Carson and {MacDiarmid}, Monte and Barez, Fazl and Duvenaud, David and Kravec, Shauna and Marks, Samuel and Schiefer, Nicholas and Soklaski, Ryan and Tamkin, Alex and Kaplan, Jared and Shlegeris, Buck and Bowman, Samuel R. and Perez, Ethan and Hubinger, Evan},
	urldate = {2024-09-10},
	date = {2024-06-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2406.10162 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:/Users/xiaohu/Zotero/storage/KSYCBV72/Denison et al. - 2024 - Sycophancy to Subterfuge Investigating Reward-Tampering in Large Language Models.pdf:application/pdf},
}

@misc{geiping_coercing_2024,
	title = {Coercing {LLMs} to do and reveal (almost) anything},
	url = {http://arxiv.org/abs/2402.14020},
	abstract = {It has recently been shown that adversarial attacks on large language models ({LLMs}) can "jailbreak" the model into making harmful statements. In this work, we argue that the spectrum of adversarial attacks on {LLMs} is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction. We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training {LLMs} with coding capabilities, as well as the continued existence of strange "glitch" tokens in common {LLM} vocabularies that should be removed for security reasons.},
	number = {{arXiv}:2402.14020},
	publisher = {{arXiv}},
	author = {Geiping, Jonas and Stein, Alex and Shu, Manli and Saifullah, Khalid and Wen, Yuxin and Goldstein, Tom},
	urldate = {2024-09-19},
	date = {2024-02-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2402.14020 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Geiping et al. - 2024 - Coercing LLMs to do and reveal (almost) anything.pdf:/Users/xiaohu/Zotero/storage/ZKS6HNDB/Geiping et al. - 2024 - Coercing LLMs to do and reveal (almost) anything.pdf:application/pdf},
}

@misc{greshake_not_2023,
	title = {Not what you've signed up for: Compromising Real-World {LLM}-Integrated Applications with Indirect Prompt Injection},
	url = {http://arxiv.org/abs/2302.12173},
	doi = {10.48550/arXiv.2302.12173},
	shorttitle = {Not what you've signed up for},
	abstract = {Large Language Models ({LLMs}) are increasingly being integrated into various applications. The functionalities of recent {LLMs} can be flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection ({PI}) attacks enable attackers to override original instructions and employed controls. So far, it was assumed that the user is directly prompting the {LLM}. But, what if it is not the user prompting? We argue that {LLM}-Integrated Applications blur the line between data and instructions. We reveal new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (without a direct interface) exploit {LLM}-integrated applications by strategically injecting prompts into data likely to be retrieved. We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We demonstrate our attacks' practical viability against both real-world systems, such as Bing's {GPT}-4 powered Chat and code-completion engines, and synthetic applications built on {GPT}-4. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other {APIs} are called. Despite the increasing integration and reliance on {LLMs}, effective mitigations of these emerging threats are currently lacking. By raising awareness of these vulnerabilities and providing key insights into their implications, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users and systems from potential attacks.},
	number = {{arXiv}:2302.12173},
	publisher = {{arXiv}},
	author = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
	urldate = {2024-12-09},
	date = {2023-05-05},
	eprinttype = {arxiv},
	eprint = {2302.12173 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Computers and Society},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/ULIIMY8F/Greshake et al. - 2023 - Not what you've signed up for Compromising Real-World LLM-Integrated Applications with Indirect Pro.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/98M9JRF5/2302.html:text/html},
}

@misc{hui_pleak_2024,
	title = {{PLeak}: Prompt Leaking Attacks against Large Language Model Applications},
	url = {http://arxiv.org/abs/2405.06823},
	shorttitle = {{PLeak}},
	abstract = {Large Language Models ({LLMs}) enable a new ecosystem with many downstream applications, called {LLM} applications, with different natural language processing tasks. The functionality and performance of an {LLM} application highly depend on its system prompt, which instructs the backend {LLM} on what task to perform. Therefore, an {LLM} application developer often keeps a system prompt confidential to protect its intellectual property. As a result, a natural attack, called prompt leaking, is to steal the system prompt from an {LLM} application, which compromises the developer's intellectual property. Existing prompt leaking attacks primarily rely on manually crafted queries, and thus achieve limited effectiveness. In this paper, we design a novel, closed-box prompt leaking attack framework, called {PLeak}, to optimize an adversarial query such that when the attacker sends it to a target {LLM} application, its response reveals its own system prompt. We formulate finding such an adversarial query as an optimization problem and solve it with a gradient-based method approximately. Our key idea is to break down the optimization goal by optimizing adversary queries for system prompts incrementally, i.e., starting from the first few tokens of each system prompt step by step until the entire length of the system prompt. We evaluate {PLeak} in both offline settings and for real-world {LLM} applications, e.g., those on Poe, a popular platform hosting such applications. Our results show that {PLeak} can effectively leak system prompts and significantly outperforms not only baselines that manually curate queries but also baselines with optimized queries that are modified and adapted from existing jailbreaking attacks. We responsibly reported the issues to Poe and are still waiting for their response. Our implementation is available at this repository: https://github.com/{BHui}97/{PLeak}.},
	number = {{arXiv}:2405.06823},
	publisher = {{arXiv}},
	author = {Hui, Bo and Yuan, Haolin and Gong, Neil and Burlina, Philippe and Cao, Yinzhi},
	urldate = {2024-11-11},
	date = {2024-05-14},
	eprinttype = {arxiv},
	eprint = {2405.06823},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/P5AUC74X/Hui et al. - 2024 - PLeak Prompt Leaking Attacks against Large Language Model Applications.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/MR983XCE/2405.html:text/html},
}

@misc{liGeneratingBelievingMembership2024,
	title = {Generating Is Believing: Membership Inference Attacks against Retrieval-Augmented Generation},
	url = {http://arxiv.org/abs/2406.19234},
	doi = {10.48550/arXiv.2406.19234},
	shorttitle = {Generating Is Believing},
	abstract = {Retrieval-Augmented Generation ({RAG}) is a state-of-the-art technique that mitigates issues such as hallucinations and knowledge staleness in Large Language Models ({LLMs}) by retrieving relevant knowledge from an external database to assist in content generation. Existing research has demonstrated potential privacy risks associated with the {LLMs} of {RAG}. However, the privacy risks posed by the integration of an external database, which often contains sensitive data such as medical records or personal identities, have remained largely unexplored. In this paper, we aim to bridge this gap by focusing on membership privacy of {RAG}'s external database, with the aim of determining whether a given sample is part of the {RAG}'s database. Our basic idea is that if a sample is in the external database, it will exhibit a high degree of semantic similarity to the text generated by the {RAG} system. We present S\${\textasciicircum}2\${MIA}, a {\textbackslash}underline\{M\}embership {\textbackslash}underline\{I\}nference {\textbackslash}underline\{A\}ttack that utilizes the {\textbackslash}underline\{S\}emantic {\textbackslash}underline\{S\}imilarity between a given sample and the content generated by the {RAG} system. With our proposed S\${\textasciicircum}2\${MIA}, we demonstrate the potential to breach the membership privacy of the {RAG} database. Extensive experiment results demonstrate that S\${\textasciicircum}2\${MIA} can achieve a strong inference performance compared with five existing {MIAs}, and is able to escape from the protection of three representative defenses.},
	number = {{arXiv}:2406.19234},
	publisher = {{arXiv}},
	author = {Li, Yuying and Liu, Gaoyang and Wang, Chen and Yang, Yang},
	urldate = {2025-01-23},
	date = {2024-09-26},
	eprinttype = {arxiv},
	eprint = {2406.19234 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {Preprint PDF:C\:\\Users\\huxia\\Zotero\\storage\\MS3XU8ZX\\Li et al. - 2024 - Generating Is Believing Membership Inference Attacks against Retrieval-Augmented Generation.pdf:application/pdf;Snapshot:C\:\\Users\\huxia\\Zotero\\storage\\CKZXSG68\\2406.html:text/html},
}

@misc{liu_autodan_2024,
	title = {{AutoDAN}: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models},
	url = {http://arxiv.org/abs/2310.04451},
	shorttitle = {{AutoDAN}},
	abstract = {Warning: This paper contains potentially offensive and harmful text. The aligned Large Language Models ({LLMs}) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned {LLMs}. Investigating jailbreak prompts can lead us to delve into the limitations of {LLMs} and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce {AutoDAN}, a novel jailbreak attack against aligned {LLMs}. {AutoDAN} can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that {AutoDAN} not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare {AutoDAN} with perplexity-based defense methods and show that {AutoDAN} can bypass them effectively. Code is available at https://github.com/{SheltonLiu}-N/{AutoDAN}.},
	number = {{arXiv}:2310.04451},
	publisher = {{arXiv}},
	author = {Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei},
	urldate = {2024-09-18},
	date = {2024-03-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2310.04451 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:/Users/xiaohu/Zotero/storage/VGRBKVNG/Liu et al. - 2024 - AutoDAN Generating Stealthy Jailbreak Prompts on Aligned Large Language Models.pdf:application/pdf},
}

@article{liu_formalizing_nodate,
	title = {Formalizing and Benchmarking Prompt Injection Attacks and Defenses},
	abstract = {A prompt injection attack aims to inject malicious instruction/data into the input of an {LLM}-Integrated Application such that it produces results as an attacker desires. Existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a framework to formalize prompt injection attacks. Existing attacks are special cases in our framework. Moreover, based on our framework, we design a new attack by combining existing ones. Using our framework, we conduct a systematic evaluation on 5 prompt injection attacks and 10 defenses with 10 {LLMs} and 7 tasks. Our work provides a common benchmark for quantitatively evaluating future prompt injection attacks and defenses. To facilitate research on this topic, we make our platform public at https: //github.com/liu00222/Open-Prompt-Injection.},
	author = {Liu, Yupei and Geng, Runpeng and Jia, Jinyuan and Gong, Neil Zhenqiang},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/4V7TIUSF/Liu et al. - Formalizing and Benchmarking Prompt Injection Attacks and Defenses.pdf:application/pdf},
}

@misc{liu_prompt_2024,
	title = {Prompt Injection attack against {LLM}-integrated Applications},
	url = {http://arxiv.org/abs/2306.05499},
	doi = {10.48550/arXiv.2306.05499},
	abstract = {Large Language Models ({LLMs}), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual {LLM}-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate {HouYi}, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. {HouYi} is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging {HouYi}, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary {LLM} usage and uncomplicated application prompt theft. We deploy {HouYi} on 36 actual {LLM}-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.},
	number = {{arXiv}:2306.05499},
	publisher = {{arXiv}},
	author = {Liu, Yi and Deng, Gelei and Li, Yuekang and Wang, Kailong and Wang, Zihao and Wang, Xiaofeng and Zhang, Tianwei and Liu, Yepang and Wang, Haoyu and Zheng, Yan and Liu, Yang},
	urldate = {2024-12-10},
	date = {2024-03-02},
	eprinttype = {arxiv},
	eprint = {2306.05499 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/Users/xiaohu/Zotero/storage/LIU8IHRX/Liu et al. - 2024 - Prompt Injection attack against LLM-integrated Applications.pdf:application/pdf;Snapshot:/Users/xiaohu/Zotero/storage/LSJ7D47K/2306.html:text/html},
}

@article{morris_language_nodate,
	title = {{LANGUAGE} {MODEL} {INVERSION}},
	author = {Morris, John X and Zhao, Wenting and Chiu, Justin T and Shmatikov, Vitaly and Rush, Alexander M},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/QDCTA3GM/Morris et al. - LANGUAGE MODEL INVERSION.pdf:application/pdf},
}

@misc{sha_prompt_2024,
	title = {Prompt Stealing Attacks Against Large Language Models},
	url = {http://arxiv.org/abs/2402.12959},
	abstract = {The increasing reliance on large language models ({LLMs}) such as {ChatGPT} in various fields emphasizes the importance of “prompt engineering,” a technology to improve the quality of model outputs. With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge. In this paper, we propose a novel attack against {LLMs}, named prompt stealing attacks. Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers. The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstructor. The goal of the parameter extractor is to figure out the properties of the original prompts. We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt. Our parameter extractor first tries to distinguish the type of prompts based on the generated answers. Then, it can further predict which role or how many contexts are used based on the types of prompts. Following the parameter extractor, the prompt reconstructor can be used to reconstruct the original prompts based on the generated answers and the extracted features. The final goal of the prompt reconstructor is to generate the reversed prompts, which are similar to the original prompts. Our experimental results show the remarkable performance of our proposed attacks. Our proposed attacks add a new dimension to the study of prompt engineering and call for more attention to the security issues on {LLMs}.},
	number = {{arXiv}:2402.12959},
	publisher = {{arXiv}},
	author = {Sha, Zeyang and Zhang, Yang},
	urldate = {2024-09-25},
	date = {2024-02-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2402.12959 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	file = {PDF:/Users/xiaohu/Zotero/storage/3F3LRXCT/Sha and Zhang - 2024 - Prompt Stealing Attacks Against Large Language Models.pdf:application/pdf},
}

@online{shen_anything_2023,
	title = {"Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models},
	url = {https://arxiv.org/abs/2308.03825v2},
	shorttitle = {"Do Anything Now"},
	abstract = {The misuse of large language models ({LLMs}) has drawn significant attention from the general public and {LLM} vendors. One particular type of adversarial prompt, known as jailbreak prompt, has emerged as the main attack vector to bypass the safeguards and elicit harmful content from {LLMs}. In this paper, employing our new framework {JailbreakHub}, we conduct a comprehensive analysis of 1,405 jailbreak prompts spanning from December 2022 to December 2023. We identify 131 jailbreak communities and discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from online Web communities to prompt-aggregation websites and 28 user accounts have consistently optimized jailbreak prompts over 100 days. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 107,250 samples across 13 forbidden scenarios. Leveraging this dataset, our experiments on six popular {LLMs} show that their safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify five highly effective jailbreak prompts that achieve 0.95 attack success rates on {ChatGPT} ({GPT}-3.5) and {GPT}-4, and the earliest one has persisted online for over 240 days. We hope that our study can facilitate the research community and {LLM} vendors in promoting safer and regulated {LLMs}.},
	titleaddon = {{arXiv}.org},
	author = {Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
	urldate = {2024-09-05},
	date = {2023-08-07},
	langid = {english},
	file = {Full Text PDF:/Users/xiaohu/Zotero/storage/NWPZJSJD/Shen et al. - 2023 - Do Anything Now Characterizing and Evaluating I.pdf:application/pdf},
}

@article{shen_prompt_nodate,
	title = {Prompt Stealing Attacks Against Text-to-Image Generation Models},
	author = {Shen, Xinyue and Qu, Yiting and Backes, Michael and Zhang, Yang},
	langid = {english},
	file = {PDF:/Users/xiaohu/Zotero/storage/4FBUV3UV/Shen et al. - Prompt Stealing Attacks Against Text-to-Image Generation Models.pdf:application/pdf},
}

@misc{wang_poisoned_2024,
	title = {Poisoned {LangChain}: Jailbreak {LLMs} by {LangChain}},
	url = {http://arxiv.org/abs/2406.18122},
	shorttitle = {Poisoned {LangChain}},
	abstract = {With the development of natural language processing ({NLP}), large language models ({LLMs}) are becoming increasingly popular. {LLMs} are integrating more into everyday life, raising public concerns about their security vulnerabilities. Consequently, the security of large language models is becoming critically important. Currently, the techniques for attacking and defending against {LLMs} are continuously evolving. One significant method type of attack is the jailbreak attack, which designed to evade model safety mechanisms and induce the generation of inappropriate content. Existing jailbreak attacks primarily rely on crafting inducement prompts for direct jailbreaks, which are less effective against large models with robust filtering and high comprehension abilities. Given the increasing demand for real-time capabilities in large language models, real-time updates and iterations of new knowledge have become essential. Retrieval-Augmented Generation ({RAG}), an advanced technique to compensate for the model's lack of new knowledge, is gradually becoming mainstream. As {RAG} enables the model to utilize external knowledge bases, it provides a new avenue for jailbreak attacks. In this paper, we conduct the first work to propose the concept of indirect jailbreak and achieve Retrieval-Augmented Generation via {LangChain}. Building on this, we further design a novel method of indirect jailbreak attack, termed Poisoned-{LangChain} ({PLC}), which leverages a poisoned external knowledge base to interact with large language models, thereby causing the large models to generate malicious non-compliant dialogues.We tested this method on six different large language models across three major categories of jailbreak issues. The experiments demonstrate that {PLC} successfully implemented indirect jailbreak attacks under three different scenarios, achieving success rates of 88.56\%, 79.04\%, and 82.69\% respectively.},
	number = {{arXiv}:2406.18122},
	publisher = {{arXiv}},
	author = {Wang, Ziqiu and Liu, Jun and Zhang, Shengkai and Yang, Yang},
	urldate = {2024-10-21},
	date = {2024-06-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2406.18122 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:/Users/xiaohu/Zotero/storage/IB4PEXA7/Wang et al. - 2024 - Poisoned LangChain Jailbreak LLMs by LangChain.pdf:application/pdf},
}

@article{wei_jailbroken_nodate,
	title = {Jailbroken: How Does {LLM} Safety Training Fail?},
	abstract = {Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of “jailbreak” attacks on early releases of {ChatGPT} that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model’s capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including {OpenAI}’s {GPT}-4 and Anthropic’s Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models’ red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity—that safety mechanisms should be as sophisticated as the underlying model—and argues against the idea that scaling alone can resolve these safety failure modes.},
	author = {Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
	langid = {english},
	file = {Wei et al. - Jailbroken How Does LLM Safety Training Fail.pdf:/Users/xiaohu/Zotero/storage/K7DKHSRG/Wei et al. - Jailbroken How Does LLM Safety Training Fail.pdf:application/pdf},
}

@misc{wen_hard_2023,
	title = {Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery},
	url = {http://arxiv.org/abs/2302.03668},
	shorttitle = {Hard Prompts Made Easy},
	abstract = {The strength of modern generative models lies in their ability to be controlled through textbased prompts. Typical “hard” prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also “soft” prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface.},
	number = {{arXiv}:2302.03668},
	publisher = {{arXiv}},
	author = {Wen, Yuxin and Jain, Neel and Kirchenbauer, John and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
	urldate = {2024-09-23},
	date = {2023-06-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2302.03668 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/xiaohu/Zotero/storage/D5Y6M9BG/2302.html:text/html;PDF:/Users/xiaohu/Zotero/storage/KCHMUHCN/Wen et al. - 2023 - Hard Prompts Made Easy Gradient-Based Discrete Optimization for Prompt Tuning and Discovery.pdf:application/pdf},
}

@inproceedings{yao_fuzzllm_2024,
	title = {{FuzzLLM}: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models},
	url = {https://ieeexplore.ieee.org/abstract/document/10448041?casa_token=Dh7MKHzoX-EAAAAA:e-sIGR97X9R4V8yeMk0LGF5LB5CuXamTs43UX2KkjMXohV35Lds2EUBiQuNjLOwKMZtK02b-},
	doi = {10.1109/ICASSP48485.2024.10448041},
	shorttitle = {{FuzzLLM}},
	abstract = {Jailbreak vulnerabilities in Large Language Models ({LLMs}), which exploit meticulously crafted prompts to elicit content that violates service guidelines, have captured the attention of research communities. While model owners can defend against individual jailbreak prompts through safety training strategies, this relatively passive approach struggles to handle the broader category of similar jailbreaks. To tackle this issue, we introduce {FuzzLLM}, an automated fuzzing framework designed to proactively test and discover jailbreak vulnerabilities in {LLMs}. We utilize templates to capture the structural integrity of a prompt and isolate key features of a jailbreak class as constraints. By integrating different base classes into powerful combo attacks and varying the elements of constraints and prohibited questions, {FuzzLLM} enables efficient testing with reduced manual effort. Extensive experiments demonstrate {FuzzLLM}’s effectiveness and comprehensiveness in vulnerability discovery across various {LLMs}.},
	eventtitle = {{ICASSP} 2024 - 2024 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {4485--4489},
	booktitle = {{ICASSP} 2024 - 2024 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Yao, Dongyu and Zhang, Jianshu and Harris, Ian G. and Carlsson, Marcel},
	urldate = {2024-09-06},
	date = {2024-04},
	note = {{ISSN}: 2379-190X},
	keywords = {Fuzzing, Acoustics, Automated Fuzzing, Jailbreak Vulnerability, Large Language Model, Manuals, Safety, Signal processing, Speech processing, Training},
	file = {IEEE Xplore Abstract Record:/Users/xiaohu/Zotero/storage/9SLDTFWN/10448041.html:text/html;IEEE Xplore Full Text PDF:/Users/xiaohu/Zotero/storage/NW2Y89JP/Yao et al. - 2024 - FuzzLLM A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabiliti.pdf:application/pdf},
}

@online{zhang_boosting_2024,
	title = {Boosting Jailbreak Attack with Momentum},
	url = {https://arxiv.org/abs/2405.01229v1},
	abstract = {Large Language Models ({LLMs}) have achieved remarkable success across diverse tasks, yet they remain vulnerable to adversarial attacks, notably the well-documented {\textbackslash}textit\{jailbreak\} attack. Recently, the Greedy Coordinate Gradient ({GCG}) attack has demonstrated efficacy in exploiting this vulnerability by optimizing adversarial prompts through a combination of gradient heuristics and greedy search. However, the efficiency of this attack has become a bottleneck in the attacking process. To mitigate this limitation, in this paper we rethink the generation of adversarial prompts through an optimization lens, aiming to stabilize the optimization process and harness more heuristic insights from previous iterations. Specifically, we introduce the {\textbackslash}textbf\{M\}omentum {\textbackslash}textbf\{A\}ccelerated G{\textbackslash}textbf\{C\}G ({\textbackslash}textbf\{{MAC}\}) attack, which incorporates a momentum term into the gradient heuristic. Experimental results showcase the notable enhancement achieved by {MAP} in gradient-based attacks on aligned language models. Our code is available at https://github.com/weizeming/momentum-attack-llm.},
	titleaddon = {{arXiv}.org},
	author = {Zhang, Yihao and Wei, Zeming},
	urldate = {2024-09-17},
	date = {2024-05-02},
	langid = {english},
	file = {Full Text PDF:/Users/xiaohu/Zotero/storage/TT9TF2UE/Zhang and Wei - 2024 - Boosting Jailbreak Attack with Momentum.pdf:application/pdf},
}

@misc{zou_poisonedrag_2024,
	location = {usenix 2025},
	title = {{PoisonedRAG}: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models},
	url = {http://arxiv.org/abs/2402.07867},
	doi = {10.48550/arXiv.2402.07867},
	shorttitle = {{PoisonedRAG}},
	abstract = {Large language models ({LLMs}) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation ({RAG}) is a state-of-the-art technique to mitigate these limitations. The key idea of {RAG} is to ground the answer generation of an {LLM} on external knowledge retrieved from a knowledge database. Existing studies mainly focus on improving the accuracy or efficiency of {RAG}, leaving its security largely unexplored. We aim to bridge the gap in this work. We find that the knowledge database in a {RAG} system introduces a new and practical attack surface. Based on this attack surface, we propose {PoisonedRAG}, the first knowledge corruption attack to {RAG}, where an attacker could inject a few malicious texts into the knowledge database of a {RAG} system to induce an {LLM} to generate an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge corruption attacks as an optimization problem, whose solution is a set of malicious texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on a {RAG} system, we propose two solutions to solve the optimization problem, respectively. Our results show {PoisonedRAG} could achieve a 90\% attack success rate when injecting five malicious texts for each target question into a knowledge database with millions of texts. We also evaluate several defenses and our results show they are insufficient to defend against {PoisonedRAG}, highlighting the need for new defenses.},
	number = {{arXiv}:2402.07867},
	publisher = {{arXiv}},
	author = {Zou, Wei and Geng, Runpeng and Wang, Binghui and Jia, Jinyuan},
	urldate = {2024-09-09},
	date = {2024-08-12},
	eprinttype = {arxiv},
	eprint = {2402.07867 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/xiaohu/Zotero/storage/6RF8LYPR/Zou et al. - 2024 - PoisonedRAG Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/xiaohu/Zotero/storage/6UUAQW4Z/2402.html:text/html},
}

@misc{zou_universal_2023,
	title = {Universal and Transferable Adversarial Attacks on Aligned Language Models},
	url = {http://arxiv.org/abs/2307.15043},
	abstract = {Because “out-of-the-box” large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures—so-called “jailbreaks” against {LLMs}—these attacks have required significant human ingenuity and are brittle in practice. Attempts at automatic adversarial prompt generation have also achieved limited success. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an {LLM} to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.},
	number = {{arXiv}:2307.15043},
	publisher = {{arXiv}},
	author = {Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J. Zico and Fredrikson, Matt},
	urldate = {2024-09-16},
	date = {2023-12-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2307.15043 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv.org Snapshot:/Users/xiaohu/Zotero/storage/M9K7E8CR/2307.html:text/html;PDF:/Users/xiaohu/Zotero/storage/6EIEJK3F/Zou et al. - 2023 - Universal and Transferable Adversarial Attacks on Aligned Language Models.pdf:application/pdf},
}

