[
  {
    "index": 0,
    "papers": [
      {
        "key": "andriushchenko_jailbreaking_2024",
        "author": "Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas",
        "title": "Jailbreaking Leading Safety-Aligned {LLMs} with Simple Adaptive Attacks"
      },
      {
        "key": "shen_anything_2023",
        "author": "Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang",
        "title": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models"
      },
      {
        "key": "zhang_boosting_2024",
        "author": "Zhang, Yihao and Wei, Zeming",
        "title": "Boosting Jailbreak Attack with Momentum"
      },
      {
        "key": "yao_fuzzllm_2024",
        "author": "Yao, Dongyu and Zhang, Jianshu and Harris, Ian G. and Carlsson, Marcel",
        "title": "{FuzzLLM}: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models"
      },
      {
        "key": "wang_poisoned_2024",
        "author": "Wang, Ziqiu and Liu, Jun and Zhang, Shengkai and Yang, Yang",
        "title": "Poisoned {LangChain}: Jailbreak {LLMs} by {LangChain}"
      },
      {
        "key": "geiping_coercing_2024",
        "author": "Geiping, Jonas and Stein, Alex and Shu, Manli and Saifullah, Khalid and Wen, Yuxin and Goldstein, Tom",
        "title": "Coercing {LLMs} to do and reveal (almost) anything"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zou_universal_2023",
        "author": "Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J. Zico and Fredrikson, Matt",
        "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wei_jailbroken_nodate",
        "author": "Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob",
        "title": "Jailbroken: How Does {LLM} Safety Training Fail?"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "liu_autodan_2024",
        "author": "Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei",
        "title": "{AutoDAN}: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "denison_sycophancy_2024",
        "author": "Denison, Carson and {MacDiarmid}, Monte and Barez, Fazl and Duvenaud, David and Kravec, Shauna and Marks, Samuel and Schiefer, Nicholas and Soklaski, Ryan and Tamkin, Alex and Kaplan, Jared and Shlegeris, Buck and Bowman, Samuel R. and Perez, Ethan and Hubinger, Evan",
        "title": "Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "liu_prompt_2024",
        "author": "Liu, Yi and Deng, Gelei and Li, Yuekang and Wang, Kailong and Wang, Zihao and Wang, Xiaofeng and Zhang, Tianwei and Liu, Yepang and Wang, Haoyu and Zheng, Yan and Liu, Yang",
        "title": "Prompt Injection attack against {LLM}-integrated Applications"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "liu_formalizing_nodate",
        "author": "Liu, Yupei and Geng, Runpeng and Jia, Jinyuan and Gong, Neil Zhenqiang",
        "title": "Formalizing and Benchmarking Prompt Injection Attacks and Defenses"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "greshake_not_2023",
        "author": "Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario",
        "title": "Not what you've signed up for: Compromising Real-World {LLM}-Integrated Applications with Indirect Prompt Injection"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "hui_pleak_2024",
        "author": "Hui, Bo and Yuan, Haolin and Gong, Neil and Burlina, Philippe and Cao, Yinzhi",
        "title": "{PLeak}: Prompt Leaking Attacks against Large Language Model Applications"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zou_poisonedrag_2024",
        "author": "Zou, Wei and Geng, Runpeng and Wang, Binghui and Jia, Jinyuan",
        "title": "{PoisonedRAG}: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "deng_pandora_2024",
        "author": "Deng, Gelei and Liu, Yi and Wang, Kailong and Li, Yuekang and Zhang, Tianwei and Liu, Yang",
        "title": "Pandora: Jailbreak {GPTs} by Retrieval Augmented Generation Poisoning"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "anderson_is_nodate",
        "author": "Anderson, Maya and Amit, Guy and Goldsteen, Abigail",
        "title": "Is My Data in Your Retrieval Database? Membership Inference Attacks Against Retrieval Augmented Generation"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "liGeneratingBelievingMembership2024",
        "author": "Li, Yuying and Liu, Gaoyang and Wang, Chen and Yang, Yang",
        "title": "Generating Is Believing: Membership Inference Attacks against Retrieval-Augmented Generation"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "morris_language_nodate",
        "author": "Morris, John X and Zhao, Wenting and Chiu, Justin T and Shmatikov, Vitaly and Rush, Alexander M",
        "title": "{LANGUAGE} {MODEL} {INVERSION}"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "sha_prompt_2024",
        "author": "Sha, Zeyang and Zhang, Yang",
        "title": "Prompt Stealing Attacks Against Large Language Models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "shen_prompt_nodate",
        "author": "Shen, Xinyue and Qu, Yiting and Backes, Michael and Zhang, Yang",
        "title": "Prompt Stealing Attacks Against Text-to-Image Generation Models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "wen_hard_2023",
        "author": "Wen, Yuxin and Jain, Neel and Kirchenbauer, John and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom",
        "title": "Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery"
      }
    ]
  }
]