\section{Related Work}
There has been significant emphasis on using LLMs for mathematical and scientific reasoning. This has led to the popularity and proliferation of math- and STEM-based evaluations, such as \citep{hendrycks2020measuring, gsm8k}, and more recently, \citep{glazer2024frontiermathbenchmarkevaluatingadvanced, phan2025humanitysexam}. However, the generalizability of mathematical reasoning skills to broader domains remains unclear. Indeed, attempts to make existing benchmarks more robust—for example, \cite{mirzadeh2024gsm}—have highlighted an overall lack of robustness and logical reasoning capabilities. Several benchmarks have also been developed to address specific areas of reasoning, including temporal reasoning \citep{xiong2024large, beniwal2024remember, dhingra2022time}, spatial understanding \citep{bohnet2024exploringbenchmarkingplanningcapabilities, yamada2023evaluating, mirzaee2021spartqa, shi2022stepgame}, commonsense reasoning \citep{zellers2019hellaswag, talmor2018commonsenseqa, sakaguchi2021winogrande}, and logical reasoning \citep{saparov2022language, tafjord2020proofwriter, saparov2023testing,parmar2024logicbench}. However, these benchmarks tend to focus narrowly on specific domains, leading to potential evaluation biases if a more holistic view of model capabilities is not considered. To address this limitation, several benchmarks have been developed to integrate multiple tasks into a single evaluation framework, including \citep{wang2018glue, wang2019superglue, weston2015towards, lu2023mathvista, kazemi2024remi, hendrycks2020measuring,wang2024mmlu,parmar2024logicbench,srivastava2022beyond}. Our work builds on this line of research, introducing a new set of challenging tasks for future model evaluation and performance improvement. The multi-task nature of our benchmark with fine-grained tasks each focused on some reasoning skills enables model developers to discover and analyze failure modes in further depth.
Note that while private initiatives such as ChatBot Arena \citep{huggingfaceChatbotArena} and the SEAL leaderboard \citep{scaleSEALLeaderboards} conduct model evaluations across various aspects, they may suffer from several potential issues as pointed out in \cite{bansal2024peekingbehindclosed}. Our benchmark provides an open evaluation framework with an automatic and deterministic scoring mechanism, ensuring full transparency and reproducibility for the broader research community.