\section{Related Work}
There has been significant emphasis on using LLMs for mathematical and scientific reasoning. This has led to the popularity and proliferation of math- and STEM-based evaluations, such as ____, and more recently, ____. However, the generalizability of mathematical reasoning skills to broader domains remains unclear. Indeed, attempts to make existing benchmarks more robust—for example, ____—have highlighted an overall lack of robustness and logical reasoning capabilities. Several benchmarks have also been developed to address specific areas of reasoning, including temporal reasoning ____, spatial understanding ____, commonsense reasoning ____, and logical reasoning ____. However, these benchmarks tend to focus narrowly on specific domains, leading to potential evaluation biases if a more holistic view of model capabilities is not considered. To address this limitation, several benchmarks have been developed to integrate multiple tasks into a single evaluation framework, including ____. Our work builds on this line of research, introducing a new set of challenging tasks for future model evaluation and performance improvement. The multi-task nature of our benchmark with fine-grained tasks each focused on some reasoning skills enables model developers to discover and analyze failure modes in further depth.
Note that while private initiatives such as ChatBot Arena ____ and the SEAL leaderboard ____ conduct model evaluations across various aspects, they may suffer from several potential issues as pointed out in ____. Our benchmark provides an open evaluation framework with an automatic and deterministic scoring mechanism, ensuring full transparency and reproducibility for the broader research community.