\section{Method}

% 3.1 Preliminaries
% 3.2 Tree Structure Answer Distribution Estimator
    % divide conquer 思想
    % Training the QGen with Policy Gradient：两个reward实现目标
% 3.3 起个名
% 3.4 Cooperate with Multiple Baselines


% We first introduce background about this task and training framework. Then we introduce the proposed TSADE. Finally, we detail the appropriate rewards and the RL implementation. 


% Finally, we provide an explanation of how the Unified-Belief Mapper operates.


\subsection{Background}


% Similarly, in the third round, there are a total of 4 candidate objects, which are divided into two categories based on their color features: 2 green vases and 2 red vases. The questioner asked the question "is it partially in red?" and the oracle gave the answer "No," so the 2 red vases were eliminated from the candidate objects.
% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{images/algorithm.pdf}
%   \caption{binary search algorithm}
%   % \Description{A woman and a girl in white dresses sit in an open car.}
% \end{figure}

% \input{images/algorithm}


\textbf{Notations.} GuessWhat?! is a guessing game aiming to find the correct object from the image. Each instance of game is denoted as a tuple $\left( I, D, O, o^* \right) $, wherein $I$ represents the observed image, $D$ represents the dialogue consisting of $J$ rounds of Q\&A pairs $\left( q_j,a_j \right) _{j=1}^{J}$, and $O=\left( o_n \right) _{n=1}^{N}$ represents the list of $N$ objects in the image $I$, with $o^*$ referring to the target object. Each question $q_j=\left( w_{m}^{j} \right) _{m=1}^{M_j}$ is a sequence of $M_j$ tokens selected from a predetermined vocabulary $V$. The $V$ is comprised of word tokens, a question stop token $<\mathrm{?}>$, and a dialogue stop token $<\mathrm{End}>$. The answer $a_j\in \left\{ <\mathrm{Yes}>,<\mathrm{No}>,<\mathrm{NA}> \right\} $ can be categorized as either yes, no, or not applicable. 
% Regarding each object $o$, it possesses an object category $c_o\in \{1...C\}$ and a segmentation mask.

\textbf{QGen.} The QGen produces a new question \(q_{j+1}\), given an image \(I\) and a history of \(j\) questions and answers \((q,a)_{1:j}\). It consists of a question encoder, an image encoder, and a question decoder. 
% The question encoder and image encoder are usually recurrent neural network~(RNN)~\cite{schuster1997bidirectional}, convolutional neural network~(CNN)~\cite{gu2018recent}, or vision-language model~(VLM)~\cite{du2022survey}. The question decoder is usually LSTM or transformer~\cite{vaswani2017attention}.

\textbf{Oracle.} The oracle is required to produce a yes-no answer $a_j$ for a target object $o^*$ within an image $I$ given a natural language question. The question is usually represented as the hidden state of an encoder, which is either LSTM or vision-language model~(VLM)~\cite{du2022survey}. 
We then concatenate the question, the bounding box of the target object, and category of the target object into a single vector and feed it as input to a single hidden layer multilayer perceptron~(MLP).
It outputs the final answer distribution using a softmax layer.

\textbf{Guesser.} The guesser takes an image \(I\), a history of questions and answers \((q,a)_{1:J}\), and predicts the correct object \(o^*\) from the set of all objects. The dialogue history is usually represented as the last hidden state of an encoder, which is either LSTM or VLM. The object embeddings are obtained from the categorical and spatial features. 
They are subjected to a dot product, which is then passed through a softmax to obtain a prediction distribution over the objects.


\textbf{SL Training.} The ground truths for QGen, Oracle, and Guesser are respectively the question, answer, and target object label. They are all optimized using cross-entropy loss.

\textbf{RL Training.} The Question Generation process is regarded as a Markov Decision Process (MDP), where the Questioner acts as an agent. At each time step $t$, for the generated dialogue based on the image $I$, the agent's state is defined as the visual information of the image, along with the history of Q\&A and the tokens of the current question generated so far: $S_t=\left( I,(q,a)_{1:j-1},\left( w_{1}^{j},...,w_{m}^{j} \right) \right) $, where $t=\sum_{k=1}^{j-1}{M_k}+m$. The agent's action $A_t$ involves selecting the next output token $w_{m+1}^{j}$ from $V$.
Depending on the agent's actions, the transition between two states falls into the completion of the current question, the end of the dialogue, or a new token.
The dialogue is limited to a maximum number of rounds $J_{\max }$. 
We model the QGen using a stochastic policy $\pi _{\theta}(A\mid S)$, where $\theta $ represents the parameters of the deep neural network utilized in the QGen baseline. These parameters are responsible for generating probability distributions for each state. The objective of policy learning is to estimate the parameter $\theta $.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{images/fig4-CVPR.pdf}
  \caption{The framework of the Tree-structured Strategy with Answer Distribution Estimator (TSADE). The red box represents the target object.}
  \label{fig:2}
\end{figure*}

%shwang-checkpoint
\subsection{Tree-structured Strategy with Answer Distribution Estimator (TSADE)}


\textbf{Tree-structured Strategy.} When humans are faced with this guessing game, they hope that each question can provide maximum distinction among the candidate objects. We propose a Tree-Structured strategy to mimic human behavior. As shown in Figure~\ref{fig:1} (c), while ensuring that no misclassification occurs, the Q\&A generated in each round should divide the current candidate objects into two groups as clearer as possible. In other words, half of the answers here should be ``Yes'' and the other half ``No''. The type of this question can be of any aspect, such as category, color, shape, size, location, and so on, as long as it meets the above requirement. Obviously, we also know which group the target belongs to. Therefore, we select this group as the new candidate object for the next round. 
Finally, after multiple rounds of questioning, we will find a single target. For example, in the second round, there are a total of four candidate objects, which can be divided into two groups, two metal things and two coffees. The Questioner asks the question ``Is it a metal thing?'' and the Oracle gives the answer ``No'', so two coffees are eliminated from the candidate objects. 


\textbf{Answer Distribution Estimator (ADE).} As shown in Figure~\ref{fig:2}, ADE contains an Oracle to answer the question and obtains $a_{1:k}^{j}$ based on $C_{1:k}^{j}$ at each round. $C_{1:k}^{j}$ is a set of candidate objects maintained by ADE, which is updated in each round. $k$ refers to the total number of objects in the candidate objects. $a_{1:k}^{j}$ is an answer distribution aimed at $C_{1:k}^{j}$. $C_{1:k}^{j}$ is updated by selecting and only keeping the objects in $a_{1:k}^{j}$ that have the same answer as the target object. In addition, ADE outputs $r_b$ and $r_c$, which are \emph{binary} reward and \emph{candidate-minimization} reward based on the Tree-structured strategy. It measures $r_b$ and $r_c$ with $a_{1:k}^{j}$. It outputs $r_b$ in each round but outputs $r_c$ in the final round. When using the traditional RL paradigm~\cite{strub2017end}, a reward $r_s$ is given at the end. If the Guesser finds the target successfully, $r_s$ is 1, otherwise 0. $r_b$, $r_c$ and $r_s$ are added to the cumulative reward $r^{\left( j \right)}$ as the final reward.




% To realize the above motivation, we apply goal-oriented visual dialogue to an RL paradigm and propose \emph{binary} reward and \emph{candidate-minimization} reward to guide question generation.

\textbf{Binary Reward.} In a dialogue, to implement the Tree-structured strategy, we design the reward to measure whether we can eliminate half of the objects ($k/2$) to the maximum extent in each round of updating $C_{1:k}^{j}$. Given the state $S_t$, where the $<\mathrm{End}>$ token is sampled or the maximum round $J_{\max}$ is reached, the reward of the state-action pair is defined as follows:

\setlength{\abovedisplayskip}{3pt}
\begin{small}
\begin{equation}
r_b\left( S_t,A_t \right) =E\left[ \sum_{j=1}^{J_{end}}{\left( 1-\frac{|l_j-k_j/2|}{k_j/2} \right)} \right] 
\end{equation}
\end{small}
We score each round and calculate the overall expectation. $J_{end}$ refers to the round reaching $J_{\max}$ or the occurrence of $<\mathrm{End}>$ token. $k_j$ refers to the number of objects in $C_{1:k}^{j}$ in the $j$ round. $l_j$ has a value range of $\left[ 0,L_j \right] $, representing the maximum number of ``Yes'' or ``No'' in $a_{1:k}^{j}$. We hope that in each round of the dialogue, we can have $l_j=k_j/2$, where half of the answers are ``Yes'' and half are ``No''. In this case, the reward score for that round of dialogue is 1. When all answers are ``Yes'' or ``No'', the reward score is 0. 
% To ensure correctness, we will select the part of objects where the target is located as the candidate objects for the next round.

\textbf{Candidate-minimization Reward.} Based on the process of finding the target in a dialogue, it is essentially equivalent to continuously narrowing down the scope of candidate objects. When the candidate objects are reduced to only one target, the goal is achieved. In many cases (especially when the dialogue has to stop due to reaching the maximum round limit $J_{\max}$), even if the target is successfully found in a dialogue, the scope of candidate objects has not been narrowed down to only the target. In this case, the result is not of high quality. In order to encourage the generation of higher-quality successful dialogues, we design a reward to measure the quality of those successful dialogues (compared with failed dialogues), as follows:

\setlength{\abovedisplayskip}{3pt}
\begin{small}
\begin{equation}
\begin{aligned}
r_c\left( S_t,A_t \right) =\begin{cases}
	\alpha 
 +\beta  
 \left( 1-\frac{k_{j_{end}}-1}{N-1} \right), &\mathrm{If\ } S_t \in \Omega, \\
	0, &\mathrm{Otherwise}.
\end{cases}
\end{aligned}
\end{equation}
\end{small}
where $\Omega=\{S|\arg\max_o[\mathrm{Guesser}(S)]=o^*\}$, $\alpha $ and $\beta $ are weights used to balance the contribution of the 0-1 reward and the quality of successful dialogues. When $k_{j_{end}}=1$, the dialogue is considered successful and of the highest quality. When $k_{j_{end}}=N$, we consider that even if the Guesser successfully finds the target $o^*$, the quality is not high, and it is likely a lucky guess. As $\alpha $ increases, the contribution of successful dialogues relative to failed dialogues to the overall reward increases. When the target $o^*$ is not successfully found, no reward is given.
When there is no 0-1 reward in this scenario, $r_c =\beta \left( 1-\frac{k_{j_{end}}-1}{N-1} \right) $. 
We encourage the generation of higher-quality successful dialogues by providing higher reward scores to those that can minimize the scope of candidate objects. 

\textbf{Training the QGen with Policy Gradient.} Given state-action pair $\left( S_t,A_t \right) $, we combine two rewards to form the ultimate reward function:

\setlength{\abovedisplayskip}{3pt}
\begin{small}
\begin{equation}
r\left( S_t,A_t \right) =\gamma \cdot r_b\left( S_t,A_t \right) +r_c\left( S_t,A_t \right) 
\end{equation}
\end{small}
where $\gamma$ is a weight to balance $r_b$ and $r_c$. Given the extensive range of actions in the game setting, we employ the policy gradient method~\cite{1999Policy} to train the QGen using the suggested rewards. This training method is similar to the approach used in the FS~\cite{strub2017end}.
% For specific implementation details, please refer to the supplementary materials.
% The objective of the policy gradient is to update the policy parameters through gradient descent based on the anticipated return. As we are operating in an episodic environment, the policy $\pi _{\theta}$, which represents the generative network of the QGen, is considered. In this scenario, the objective function of the policy takes the following structure:
% {\fontsize{8}{12}
% $$
% J(\theta )=E_{\pi _{\theta}}\left[ \sum_{t=1}^T{r}\left( S_t,A_t \right) \right] 
% $$
% }

% The parameters $\theta $ can be optimized by applying the gradient update rule. In the REINFORCE algorithm\cite{1998Reinforcement}, the gradient of $J(\theta )$ can be estimated by sampling a batch of episodes $\tau $ from the policy $\pi _{\theta}$:
% {\fontsize{8}{12}
% $$
% \nabla J(\theta )\approx \left. \left< \sum_{t=1}^T{\sum_{A_t\in V}{\nabla _{\theta}}}\log \pi _{\theta}\left( S_t,A_t \right) \left( Q^{\pi _{\theta}}\left( S_t,A_t \right) -b_{\varphi} \right) \right> \right. _{\tau}
% $$
% }

% Here, $Q^{\pi _{\theta}}\left( S_t,A_t \right) $ represents the state-action value function, which provides the expected cumulative reward at $\left( S_t,A_t \right) $:
% {\fontsize{8}{12}
% $$
% Q^{\pi _{\theta}}\left( S_t,A_t \right) =E_{\pi _{\theta}}\left[ \sum_{t^{'}=t}^T{r}\left( S_{t^{'}},A_{t^{'}} \right) \right] 
% $$
% }

% By substituting the notations with QGen, we obtain the following policy gradient:
% {\fontsize{7}{12}
% $$
% \begin{array}{r}
% 	\nabla J(\theta )\approx \left. \langle \sum_{j=1}^J{\sum_{m=1}^{M_j}{\nabla _{\theta}}}\log \pi _{\theta}\left( w_{m}^{j}\mid I,(q,a)_{1:j-1},w_{1:m-1}^{j} \right) \right.\\
% 	\left. \left( Q^{\pi _{\theta}}\left( I,(q,a)_{1:j-1},w_{1:m-1}^{j},w_{m}^{j} \right) -b_{\varphi} \right) \right. \rangle _{\tau}\\
% \end{array}
% $$
% }

% $b_{\varphi}$ represents a baseline function utilized to mitigate gradient variance, and it can be selected arbitrarily. It is implemented as a one-layer MLP that takes the state $S_t$ as input in QGen and yields the anticipated reward. The baseline $b_{\varphi}$ is trained using mean squared error, given by:
% {\fontsize{8}{12}
% $$
% \min_{\varphi} L(\varphi )=\left< \left. \left[ b_{\varphi}\left( S_t \right) -\sum_{t^{'}=t}^T{r}\left( S_{t^{'}},A_{t^{'}} \right) \right] ^2 \right. \right> _{\tau}
% $$
% }

% It involves the Oracle, which is responsible for generating responses to various questions regarding objects present in an image scene. To represent the spatial feature of the object labeled as $o$, the bounding box information (obtained from the segment mask) is encoded. The box coordinates, width, and height are indicated by $o_{spa}=\left[ x_{\min},y_{\min},x_{\max},y_{\max},x_{\mathrm{center}},y_{\mathrm{center}},w,h \right] $. The category $c_o$ is incorporated using a learned look-up table, and the current question is encoded using an LSTM. These three features are combined into a unified vector, which is then inputted into a one hidden layer MLP. This MLP is followed by a softmax layer, which generates the probability ($p\left( a\mid o_{spa},c_o,q \right) $) for the answer.

% In the given context, the Guesser is tasked with predicting the correct object $o^*$ from a list of objects, given an image $I$ and a series of question-answer pairs. The dialogue, treated as a single sequence of tokens, is encoded using an LSTM, and the last hidden state is extracted as the representation of the dialogue. Additionally, the spatial features and categories of all objects are embedded using an MLP. To generate the final prediction, we compute the dot product between the dialogue features and the object features, followed by a softmax operation.



% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{images/fig3.pdf}
%   \caption{turn-by-turn introduce how binary search works}
%   % \Description{A woman and a girl in white dresses sit in an open car.}
% \end{figure}



% To simulate a complete GuessWhat?! game, we utilize our established models: Oracle, Guesser, and QGen baseline. The process begins with image $I$, from which an initial question $q_1$ is generated. We achieve this by sampling from the QGen baseline model until the stop question token is encountered. Subsequently, the Oracle receives question $q_1$, along with the assigned object category $o^*$ and its spatial information $o_{spa}^{*}$. The Oracle then generates the answer $a_1$, and the question-answer pair $\left( q_1,a_1 \right) $ is added to the dialogue history. This loop continues until either the end of the dialogue token is sampled or the maximum number of questions is reached. Finally, the Guesser employs the entire dialogue history $D$ and the object list $O$ as inputs to predict the object. The game is considered successful if $o^*$ is selected, otherwise it is deemed a failure.


% \subsection{Unified-Belief Mapper (UBM)}

% During the dialogue process, the continuously acquired information by QGen and Guesser greatly contributes to the final decision. However, QGen mainly influences the progression of the dialogue during the intermediate stages, while Guesser mainly influences the result of the task~\cite{le2022multimodal}. Compared with QGen, Guesser's outcome determines the success of the task. So Guesser is more sensitive to numerical changes and has lower fault tolerance. Therefore, as illustrated in Figure 2, our approach is to transfer the information from QGen to Guesser. QGen and Guesser have an Intersection over Union (IoU) overlap relationship with the corresponding bounding boxes of their visual features. Based on this, we construct the Unified-Belief Mapper, a function $f_{ubm}$ that integrates QGen and Guesser information, to get $p_{ubm}$. $p_{ubm}$ refers to the IoU between the bounding boxes of $p_{qgen}\left( O_{1:N} \right) $ and $p_{guesser}\left( O_{1:N} \right) $. By performing a Hadamard Product between $p_{ubm}$ and $p_{qgen}\left( O_{1:N} \right) $ (objects belief distribution in QGen) and then summing all objects, we adjust the dimension to update $p_{guesser}\left( O_{1:N} \right) $ (objects belief distribution in Guesser). Our goal is only to update the internal differences among different objects,thus we perform a normalization operation to ensure that the sum of $p_{guesser}\left( O_{1:N} \right) $ is equal to 1.

% \begin{small}
% \begin{gather}
% p_{ubm}=f_{ubm}\left( p_{qgen}\left( O_{1:N} \right) , p_{guesser}\left( O_{1:N} \right) \right) , \\
% p_{sum}=\sum\nolimits_{i=1}^N{\left( p_{ubm}\odot p_{qgen}\left( O_{1:N} \right) \right)} , \\
% p_{guesser}\left( O_{1:N} \right) =p_{guesser}\left( O_{1:N} \right) +p_{sum} ,
% \end{gather}
% \begin{equation}
%     \begin{split}
%         p_{guesser}\left( O_{1:N} \right) & =norm\left( p_{guesser}\left( O_{1:N} \right) \right) \\
%         & =\frac{p_{guesser}\left( O_{1:N} \right)}{\sum\nolimits_{i=1}^N{p_{guesser}\left( O_{1:N} \right)}}
%     \end{split}
% \end{equation}
% \end{small}
\subsection{Applicability in Two Settings}

Our method can be treated as a plugin that can be applied to different models.
We divide the results into two settings for appropriate comparison.

\textbf{Without $r_s$.} If the models do not cooperate with TSADE, we only train the Oracle, Guesser, and QGen models independently using cross-entropy loss.
% We first independently train the Oracle, Guesser, and QGen models using cross-entropy loss in SL. 
% Then, keeping the Oracle and Guesser models fixed, we train the QGen model in the described RL framework with the rewards TSADE proposed.
Then, if the models cooperate with TSADE, we keep the Oracle and Guesser models fixed and train the QGen model in the described RL framework with the rewards proposed by TSADE.
There is no 0-1 reward, which refers to $r_s$ from FS~\cite{strub2017end}.
FS can be considered as DV+$r_s$.
The purpose of the setting is to compare the model's results in SL with the model's results with TSADE in RL.
% The purpose of the setting is to prove that replacing the 0-1 reward with the reward proposed by TSADE can still improve model performance.
The training approach for QGen in SL follows the same details as DV~\cite{de2017guesswhat}.  %Readers can refer to the supplementary materials for more details. 

\textbf{With $r_s$.} We first independently train the Oracle, Guesser, and QGen models. Then keeping the Oracle and Guesser models fixed, we train the QGen model in the described RL framework using a comprehensive set of rewards. 
If the models follow the 0-1 reward proposed by FS, these rewards only include $r_s$.
On this basis, if the models cooperate with TSADE, these rewards consist of the $r_s$ and the rewards proposed by TSADE.
The purpose of the setting is to demonstrate the improvement results of TSADE on state-of-the-art models based on 0-1 reward in RL.

 % The previous model architecture ADVSE~\cite{2020Answer} can also be trained as baseline under both SL and RL. Therefore, we also apply our method to ADVSE to further demonstrate the effectiveness and generality of our method.
