\section{Related Work}
% Goal-oriented dialogue requires agents to complete a specific task through multi-round dialogue~\cite{bordes2016learning,rajendran2018learning,williams2007partially}. 

% Although goal-oriented spoken and text-based dialogues have been studied for many years in the field of Natural Language Processing\cite{bordes2016learning,rajendran2018learning,williams2007partially}, goal-oriented visual dialogue moves the scene into a more realistic visual environment, making it a relatively more practical and challenging field. 

% The goal of GuessWhat?!~\cite{de2017guesswhat} is to distinguish a defined object in an image through dialogue, while the goal of GuessWhich~\cite{das2017learning} is to identify the correct image from a series of images. 

% There are usually two dialogue agents, Questioner and Oracle. The Questioner keeps asking questions to find the defined but undisclosed target, and the Oracle defines the target object in advance and answers questions accordingly.
% In a dialogue, there are typically two agent types, {\it i.e.}, the Questioner and the Oracle. The Questioner consists of two sub-models, QGen and Guesser. 
% They all involve QGen, Guesser and Oracle. 
% Our main focus is on the QGen. Please refer to the supplementary materials for more details about Oracle and Guesser.



% \subsection{Oracle}

% In the initial work of GuessWhat?!, a baseline Oracle was proposed, which concatenates the question encoding and the spatial and category information of the target object together and inputs them into the MLP layer to predict the final answer. However, without the introduction of visual information, the baseline Oracle may have difficulty understanding questions that involve color, shape, and object relations. Tu et al.\cite{tu2021learning} introduced visual features predicted by object detection models such as Faster-RCNN\cite{ren2015faster} into Oracle's decision-making process, but the way did not effectively help Oracle understand questions that involve information such as object relations or color.

% \subsection{Guesser}

% Guesser not only needs to perform referring expression comprehension for dialogue describing visual objects but also needs to perform reasoning. The initial work proposed a model that combines the encoding of the entire dialogue history with each object category and spatial information to predict the target object\cite{de2017guesswhat,strub2017end}. Later work\cite{shukla2019should,lu202012,deng2018visual} treated the entire dialogue history as a whole. However, the Guesser model does not encode any visual information. Considering that the lack of turn-level visual grounding can cause the Guesser to confuse the object referred to in each question, some methods\cite{simonyan2014very,pang2020guessing} introduced features such as VGG and Faster-RCNN into the Guesser model. Considering the dynamic characteristic of multi-turn dialogue reasoning, Pang et al.\cite{pang2020guessing} proposed to decompose the dialogue into turn-level and use state tracking to dynamically update the guessing confidence, demonstrating a significant performance improvement. Recent work\cite{tu2021learning} introduced a Visual-Linguistic pre-trained model, giving the agent more visual language shared representations and prior knowledge, which has achieved good results.


% \subsection{Question Generator}

%CHANGED-0614
\subsection{Question Generator (QGen)}
% \textbf{QGen.} 
The QGen plays a core role in the goal-oriented visual dialogue, as it not only needs to ask questions that can acquire certain information gain but also guides the dialogue towards the direction of the target.  
De Vries et al.~\shortcite{de2017guesswhat} propose the first QGen model with an encoder-decoder structure, in which the dialogue history is encoded by a Hierarchical Recurrent Encoder-Decoder (HRED)~\cite{serban2015hierarchical}, and the image is conditionally encoded as VGG features~\cite{simonyan2014very}.
Strub et al.~\shortcite{strub2017end} introduce the approach of RL and provide a 0-1 reward, where 1 indicates successful finding of the target in the dialogue. Built upon this approach, Zhang et al.~\shortcite{zhang2018goal} propose intermediate rewards from three dimensions to improve the model performance. 
Shekhar et al.~\shortcite{shekhar2018beyond} introduce a shared dialogue state encoder for Guesser and QGen, in which the visual encoder is based on ResNet~\cite{he2016deep}, and the language encoder is based on LSTM~\cite{hochreiter1997long}. Pang et al.~\shortcite{pang2020visual} introduce a turn-level object state tracking mechanism to QGen. Tu et al.~\shortcite{tu2021learning} introduce a Visual-Linguistic pre-trained model to QGen, which makes the object's semantic coverage more comprehensive and better.
Our main focus is on how to train QGen. 
The fundamental difference between TSADE and prior work lies in its clever use of a non-goal-oriented questioning strategy~(NGOQS) to find target, whereas prior works~\cite{zhang2018goal,shukla2019should,testoni2021looking} utilize a goal-oriented questioning strategy~(GOQS). 
We experimentally prove that flexibly using NGOQS is more useful than simply using GOQS, and GOQS can benefit from NGOQS.



%Please refer to the supplementary materials for the difference between our method and prior work, as well as for more details about Oracle and Guesser.
% Please refer to the supplementary materials for more details about Oracle and Guesser.



\subsection{Answer Distribution Estimator (ADE)}
% \textbf{Answer Distribution Estimator (ADE).}
Given a question, ADE actually employs an internal Oracle to answer all objects in the image to obtain an answer distribution. Lee et al.~\shortcite{lee2018answerer} first introduce the ADE module to propose an Answerer in Questionerâ€™s Mind (AQM) algorithm to obtain question in each round.
In this work, ADE refers to an approximated model of the original Oracle explicitly trained by AQM's Questioner. 
It abandons the paradigm of deep learning, and uses mathematics and the approximated model to directly calculate information gain to select question from training data in each round. 
% However, this paradigm of selecting question from training data has great limitations. The fixed training data usually can't cover the huge actual scenes in life. 
% And the information gain of all training data must be calculated in each round, making the calculation cost very high.
% Different from AQM, TSADE is a paradigm based on question generation, which has stronger generalization and lower computational cost. TSADE employs the answer distribution to dynamically update the real-time candidate objects and calculate reward score for the quality of each question. 
% Then the reward score is put into RL to optimize question generation.
Zhang et al.~\shortcite{zhang2018goal} propose three intermediate rewards to optimize the model in RL. 
% It explicitly obtains higher rewards with fewer rounds. 
Based on the goal-oriented way, it hope that the probability of ground truth (target) will progressively increase during the whole process. It uses ADE to avoid useless questions based on answer distribution. However, it does not consider what kind of questions are most useful. The difference is that TSADE takes the issue into account and uses ADE to achieve the same final goal in a non-goal-oriented way, without paying attention to which target is during the whole process.
Testoni and Bernardi \shortcite{testoni2021looking} propose the ``confirm-it'' strategy to select question that can gradually increase the probability of the target from the candidate questions. It uses an internal Oracle to provide answers specific to the target for a set of candidate questions. These answers are then used by the Guesser to compute a probability distribution over candidate objects. 
% In contrast, TSADE uses the internal Oracle to obtain an answer distribution over the candidate objects. The former's internal Oracle responds to target based on a set of questions, while the latter's internal Oracle responds to candidate objects based on a single question.
%We can see that existing methods do not have an efficient and intuitive strategy to guide question generation. Previous research\cite{strub2017end,shukla2019should,zhang2018goal,zhao2018improving} has used Reinforcement Learning methods to learn the Questioner/Guesser model by designing different rewards, such as end-game success or information gain from question generation. However, the question-generation strategy under these methods is fuzzy, uninterpretable, and inefficient. This paper proposes an Answer Distribution Estimator (ADE) that explicitly uses a binary search strategy to guide question generation, further integrates the state distributions of different agents, and enhances the fusion of visual and textual information.

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.8\linewidth]{images/fig1_emnlp.pdf}
%   \caption{It shows an example of the GuessWhat?! game that describes the process of attention transfer in dialogue based on the Tree-structured strategy. The excluded objects are in the lower-right candidate box. The target object is highlighted in green box.}
%   \label{fig:example of strategy}
% \end{figure}