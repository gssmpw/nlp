

\section{Introduction}
Information-seeking through interaction is one of the most important abilities of artificial intelligence~\cite{matsumori2022study}. In recent years, goal-oriented visual dialogue has received increasing attention. This task is to generate questions by one agent and provide answer by users (or another agent) towards specific goal, as exemplified in many benchmarks, such as Guesswhat?!~\cite{de2017guesswhat}, GuessWhich~\cite{das2017learning} and VisDial~\cite{2017Visual}. %这个也得加引用
%ImageChat, VisualHow, and TikTalk, among others. 
The key of facilitating this task is how well the machine can mimic humans to raise questions about objects and seek answers about the visual scene. In fact, the efficiency and quality of the visual dialogue is measured by whether and how quickly the specific goal is achieved.


See an example from GuessWhat?!~\cite{de2017guesswhat} in Figures \ref{fig:1} (a), the specific goal is to find the correct object (green box) from all objects (detected by object detection) in the image at the end. 
Towards this goal, the Questioner, responsible for generating question by its Question Generator (QGen) only seeing the whole image and the goal description, generates several rounds of questions, and Answerer, either machine Oracle in the training stage or human in testing stage, gives answer prompt by knowing what kind of objects beforehand. After multiple rounds of Q\&A, the Guesser, as another submodule of the Questioner, guesses which object is correct.
% the Guesser agent in the Questioner guesses which object is correct.
Obviously, the bottleneck problem in this process is the dialogue strategy, {\it i.e.}, how the questions are generated by QGen.
% In real-world conversations, humans usually rely on both visual and language information to reason and make judgments. Therefore, AI agents are expected to combine Computer Vision (CV) and Natural Language Processing (NLP) to conduct meaningful reasoning. 
%The significance of goal-oriented visual dialogue research lies in the fact that, 
% Given complex real-world scene information that humans can hardly comprehend all the details, AI agents can have a deeper and more comprehensive understanding of scene information, and provide more efficient and accurate reasoning to assist humans in completing tasks. 
%Therefore, AI agents need to ask appropriate and targeted questions in each round of dialogue with existing image information and the understanding of images. 
%GuessWhat?!~\cite{de2017guesswhat} is 

%For example, an image-guessing task on  involves two agents: Questioner and Oracle. The Questioner has two sub-agents, Question Generator (QGen) and Guesser. QGen generates visual-grounded questions, while Guesser identifies the target after the dialogue ends. Questioner proposes a series of questions, each answered by Oracle with ``Yes/No/(N/A)''. The Guesser has to guess which object is correct after multiple rounds of Q\&A. At the beginning of the task, Questioner can only see an image, while Oracle can see the image and know which is the target object.


To enable agents to learn appropriate dialogue strategy and find the target object in a visual scene, mainstream solution is based on Reinforcement Learning (RL)~\cite{le2022multimodal} on benchmarks such as GuessWhat?!. 
These works~\cite{strub2017end,shukla2019should,zhang2018goal,abbasnejad2018active,abbasnejad2019s,zhao2018improving} usually design reward functions to achieve higher success rate or information gain based on the policy gradient~\cite{1999Policy}.
Other works improve the dialogue mechanism based on attention mechanism~\cite{anderson2018bottom} and the state of visual information in Supervised Learning (SL)~\cite{pang2020guessing, pang2020visual,tu2021learning}.
Despite the progress already made, existing mechanisms still need further investigation.
First, existing SL and RL methods only consider whether the target can be found in each round.
As shown in Figures \ref{fig:1} (b), their questioning strategies usually do not follow a clear path, thus questions are repeatedly and aimlessly asked to exclude a fraction of objects in each round. 
Second, even if the agent successfully identifies the target, the scope of the final candidate objects may not be reduced to a single object. 
The issue may be tolerable when there are only a few objects in the image. However, it would be difficult for the agent to find the target via aimless questions within limited rounds of Q\&A if there are dozens or even hundreds of objects, which is very common under real-world situations. In addition, the Questioner also encounters a high 
question repetition rate under these methods, lacking clear guidance for producing meaningful questions. 

%It means a fluke.
%shwang-checkpoint
% Second, for the current popular state tracking mechanism, there is no correlation between the state of the Questioner and the Guesser in multiple rounds of Q\&A, which is inconsistent with the logical reasoning process of humans~\cite{stenning2012human}. When humans are guessing, we will re-estimate the candidate objects after each round of Q\&A. Such a ``chain of thought'' is crucial for humans since it can help decide the question to ask in the next round and narrow down candidates step by step. 
%CHANGED-0614 
% Second, traditional Reinforcement Learning-based approaches provide a simple 0-1 reward only based on whether the target can be found. They fail to consider the impact of differences in quality between different successful dialogues and the differences between successful and failed dialogues. In the process of a dialogue interaction, each round of questioning narrows down the range of candidate objects. However, even if this set of dialogues successfully finds the target under the discourse constraints, the final range of candidate objects may not be narrowed down to a single target, but rather contains some degree of luck. Additionally, the significance and value of successful dialogues compared to failed dialogues have not been given sufficient attention.
%CHANGED-0614

%Such an overall evaluation result is not only used to help decide the question to ask in the next round but also to help judge which object is more likely to be the target. 
%Therefore, the state of the Questioner and the state of the Guesser should have a certain correlation or point to the same state.

%CHANGED-0614
%We argue that mimicking human behavior is a good way 


To address the above issues, in this paper, we propose a Tree-structured Strategy with Answer Distribution Estimator (TSADE) to guide Questioner to generate questions under RL paradigm.
In each round of Q\&A, Answer Distribution Estimator (ADE) employs a simulated Oracle that dynamically estimates the answer distribution of current candidate objects with the given question. Obviously, the objects that have the same answer as the ground truth (target object) have the potential to be selected as the target. With this answer distribution, we can constantly update the scope of candidate objects in the object pool.
We monitor candidate objects in order to calculate rewards under RL, which is independent of Guesser's final prediction. Even if the candidate objects are reduced to a a single object, the Guesser still predicts a target from all objects in the image.


We design two rewards to guide Questioner to learn TSADE for question generation, {\it i.e.}, \emph{binary} reward, and \emph{candidate-minimization} reward. The first reward is based on tree-structured strategy to perform ``divide and conquer''. Similar to the human decision-making process~\cite{stenning2012human}, in each round of Q\&A, we encourage the QGen to generate question that divides the current candidate objects into two groups with roughly the same number of objects. The grouping result should produce the greatest information gain compared to the previous round.  
Ideally, given $N$ candidate objects, this strategy can reduce the searching time complexity from $O(N)$ to $O(\log N)$.
We select the group where the ground truth (target) is located as the updated candidate objects in the next round. Although we use the ground truth mastered by Oracle, we do not break the information asymmetry between the Questioner and Oracle. We only use ground truth to calculate rewards when training QGen under the RL paradigm. In the inference stage, there is no need to get reward to calculate gradients to update model parameters, so the ground truth is not used. What needs to be emphasized is that when training QGen, we only use ground truth to maintain the correctness of candidate objects. The reward is calculated based on the changes in the overall distribution of answers. The ground truth information is not injected into the reward, and is not disclosed to the QGen that needs to be trained.
The second reward emphasizes the importance of dialogues that not only successfully finds the target but also narrows down the scope of candidate objects to only a single target at the end of the dialogue.
We encourage the models to find the target accurately, rather than finding it by a fluke.
%Furthermore, successful dialogues are assigned a higher value.
% Moreover, to enhance the correlation between the Questioner and the Guesser, we design a unified-belief mapper that aligns the state weight of the Questioner and the Guesser in each round. It is a complement to the popular state-tracking mechanism, making the final decision more convincing. 
%Moreover, similar to the popular state tracking mechanism, we align the state weight of the Questioner and the Guesser for each round, which enhances the correlation between the two agents, making the final decision more convincing. 
%Finally, we propose a new method for introducing visual information for the Oracle, in which visual information is added when putting the text information into the encoder layer, and the output hidden vector is used as the representation of the text information, replacing the traditional single text information vector and directly inputting it into the subsequent MLP layer. 

We apply our method on various baselines on the GuessWhat?! and VisDial dataset. Experimental results show that our method can effectively improve the performance and reduce question repetition rate compared to numerous competitors on the datasets.

%CHANGED-0614
Our main contributions are as follows:
\begin{itemize}
\item We propose the Tree-structured Strategy with Answer Distribution Estimator (TSADE) to perform ``divide and conquer'' for goal-oriented visual dialogues. 
It helps the agent to find the target in fewer rounds.
% It guides the Questioner to ask questions that can produce maximum information gains in each round of Q\&A. 

\item TSADE can cooperate with various baselines to generate meaningful questions by reinforcement learning.

\item Experiments show that our method can help the agent to generate more informative questions and achieve the specific goal more quickly in visual dialogues.
\end{itemize}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{images/fig1-CVPR3.pdf}
  \caption{(a) Illustration of goal-oriented visual dialogue. The target object is highlighted in green box. (b) Example of traditional dialogue strategy. (c) Example of Tree-structure dialogue strategy. The excluded objects are in the lower-right candidate box.}
  \label{fig:1}
\end{figure}
% \item We introduce a unified-belief mapper which aligns the state weight of the Questioner and the Guesser in each round. It mimics the human decision process and enhances the correlation between the Questioner and the Guesser.
%Our method generates more meaningful questions as well as enhances the correlation between the Questioner and the Guesser, which is important for goal-oriented question answering.
%can divide the current object candidates into two groups with maximum information gain.}
%\item {Our method can effectively improve performance and reduce question repetition rate, which is important for goal-oriented question answering.}

% as well as enhances the correlation between the Questioner and the Guesser. 

%\item The agent is able to find the target in fewer rounds when there are relatively more objects in the image.