\section{Related work}
\subsection{Interpretability of deep models}

Different from white-box models (e.g. decision tree-based models) that are intrinsically interpretable, deep models operate like black-box models where the internal working mechanism are not easily understood. Existing methods for interpreting deep models could be categorized into two types: feature attribution that focuses on understanding how a fixed black-box model leads to a particular prediction, and instance attribution is to trace back to training data and study how a training point influence the prediction of a given instance. For example, Integrated Gradients \cite{sundararajan2017axiomatic} measures feature importance by computing the path integral of the gradients respect to each dimension of input. LIME \cite{ribeiro2016should} generates explanation by learning an inherently interpretable model locally on the instance being explained. Shapley value \cite{shapley1953value} that is derived from cooperative game theory treats each feature as a player and computes the marginal contribution of each feature toward the model's output. Regarding instance attribution, typical methods include influence function \cite{koh2017understanding} which attends to the final iteration of the training and TracIn \cite{pruthi2020estimating} that keeps tracing the whole training process.

\subsection{Shapley value}

Lloyd Shapley’s idea \cite{shapley1953value} is that players in the game should receive payments or shares proportional to their marginal contributions. \citet{vstrumbelj2014explaining} and \citet{lundberg2017unified} generalize Shapley value to measure feature importance toward the model's output by averaging marginal contribution of the feature being explained over all possible permutations among features. There are many variants of Shapley value to address efficiency and faithfulness. For example, to improve efficiency of Shapley value, \citet{chen2018shapley} proposes L-Shapley to explore local permutations and C-Shapley to compute valid connected nodes for structured data. \citet{kwon2022weightedshap} proposes WeightedSHAP to optimize weight of each marginal contribution under the user-defined utility function to emphasize influential features. 

Computing Shapley value function requires choosing a baseline to represent feature’s missingness. Random baseline \cite{lundberg2017unified,chen2018shapley, kwon2022weightedshap} has been widely used due to its immediate availability. Since random baseline ignores inherent feature dependency and thus could result in inconsistent context, \citet{frye2020shapley} and \citet{hooker2021unrestricted} address the importance of conditional baseline where the missingness is replaced with the features generated by conditioning on the observed features. However, \citet{janzing2020feature} and \citet{kumar2020problems} use simple linear model to demonstrate that conditional baseline based Shapley value fails to distinguish correlated feature with different sensitivity toward the model’s output. \citet{watson2022rational} considers that the optimal baseline should follow the interventional distribution but it requires accessing to the underlying causal structure. Generally for the above baselines none is more broadly applicable than the others, which motivates us to analyze suboptimality of these baselines for Shapley value on interpreting black-box models and find out the optimal baseline.