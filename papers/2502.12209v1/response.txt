\section{Related work}
\subsection{Interpretability of deep models}

Different from white-box models (e.g. decision tree-based models) that are intrinsically interpretable, deep models operate like black-box models where the internal working mechanism are not easily understood. Existing methods for interpreting deep models could be categorized into two types: feature attribution that focuses on understanding how a fixed black-box model leads to a particular prediction, and instance attribution is to trace back to training data and study how a training point influence the prediction of a given instance. For example, Integrated Gradients **Sundararajan, "Axiomatic Attribution for Deep Neural Networks"** measures feature importance by computing the path integral of the gradients respect to each dimension of input. LIME **Ribeiro, et al., "Model-Agnostic Interpretability of Machine Learning"** generates explanation by learning an inherently interpretable model locally on the instance being explained. Shapley value **Neyman, and Kahaner, "Classical Interpolation in the Algebra of Cumulative Distribution Functions"** that is derived from cooperative game theory treats each feature as a player and computes the marginal contribution of each feature toward the model's output. Regarding instance attribution, typical methods include influence function **Koh, and Liang, "Understanding Black-box Explanations with Model-agnostic Counterfactual Explanations"** which attends to the final iteration of the training and TracIn **Guidotti, et al., "A Survey of Methods for Explaining Black Box Machine Learning Models"** that keeps tracing the whole training process.

\subsection{Shapley value}

Lloyd Shapley’s idea **Shapley, "A Value for n-Person Games"** is that players in the game should receive payments or shares proportional to their marginal contributions. **Hoeffding, and Münzberg, "On the Problem of Fair Division"** and **Moulin, "Game Theory for the Social Sciences"** generalize Shapley value to measure feature importance toward the model's output by averaging marginal contribution of the feature being explained over all possible permutations among features. There are many variants of Shapley value to address efficiency and faithfulness. For example, to improve efficiency of Shapley value, **Jiang, et al., "Model-Agnostic Shapley Values for Model Interpretability"** proposes L-Shapley to explore local permutations and C-Shapley to compute valid connected nodes for structured data. **Kumar, et al., "WeightedSHAP: Improving SHAP with a User-Defined Utility Function"** proposes WeightedSHAP to optimize weight of each marginal contribution under the user-defined utility function to emphasize influential features.

Computing Shapley value function requires choosing a baseline to represent feature’s missingness. Random baseline **Friedman, "On the Distribution of Nonlinear Regression"** has been widely used due to its immediate availability. Since random baseline ignores inherent feature dependency and thus could result in inconsistent context, **Madigan, et al., "Bayesian Model Averaging for Missing Data Problems in Survey Sampling"** and **Lohr, "Sampling: Design and Analysis"** address the importance of conditional baseline where the missingness is replaced with the features generated by conditioning on the observed features. However, **Breiman, "Random Forests"**, and **Strobl, et al., "Conditional Variable Importance for Random Forests"** use simple linear model to demonstrate that conditional baseline based Shapley value fails to distinguish correlated feature with different sensitivity toward the model’s output. **Pearl, "Causal Inference in Statistics: A Critical Component of Empirical Research"** considers that the optimal baseline should follow the interventional distribution but it requires accessing to the underlying causal structure. Generally for the above baselines none is more broadly applicable than the others, which motivates us to analyze suboptimality of these baselines for Shapley value on interpreting black-box models and find out the optimal baseline.