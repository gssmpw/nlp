
\documentclass{article} %
\usepackage[final]{neurips}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}


\usepackage{xcolor}         %
\usepackage{xspace}         %
\usepackage{colortbl}
\usepackage{color}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{tcolorbox}
\usepackage{booktabs}
\usepackage{times}
\usepackage{latexsym}
\usepackage{framed}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{fancyvrb}
\usepackage{wrapfig}
\usepackage{enumerate}
\usepackage{rotating}
\usepackage{cuted}

\usepackage{soul}
\usepackage{tipa}

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %

\usepackage{amsmath}
\usepackage{multirow}
\usepackage{array}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{comment}
\usepackage{tikz}
\usepackage{listings}
\lstset{breaklines=true} 

\definecolor{LightCyan}{rgb}{0.75,1,1}

\definecolor{pos}{RGB}{167, 199, 231}
\definecolor{neg}{RGB}{250, 160, 160}
\definecolor{amaranth}{rgb}{0.9, 0.17, 0.31}
\definecolor{kellygreen}{rgb}{0.3, 0.73, 0.09}
\definecolor{azure}{rgb}{0.0, 0.5, 1.0}

\usepackage{titlesec}

\titlespacing*{\paragraph}{0pt}{0.5ex plus 0.5ex minus 0.2ex}{1em}


\widowpenalty10000
\clubpenalty10000

\usepackage{inconsolata}

\newcommand{\modelname}{Rank1}
\newcommand{\modelnamepretty}{\textsc{Rank1}}


\newcommand{\todo}[1]{\textcolor{red}{#1}}
\newcommand{\orion}[1]{\textcolor{brown}{[OW: #1]}}

\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}






\title{\modelname: Test-Time Compute for \\ Reranking in Information Retrieval}


\author{
    \textbf{Orion Weller}
    \quad
    \textbf{Kathryn Ricci}
    \quad
    \textbf{Eugene Yang}
    \quad
    \textbf{Andrew Yates}  \\ \\
    \textbf{Dawn Lawrie}
    \quad
    \textbf{Benjamin Van Durme} \\ \\
    Johns Hopkins University \\ \\
    \texttt{oweller@cs.jhu.edu}
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}
We introduce \modelnamepretty, the first reranking model trained to take advantage of test-time compute.
\modelnamepretty\ demonstrates the applicability within retrieval of using a reasoning language model (i.e. OpenAI's o1, Deepseek's R1, etc.) for distillation in order to rapidly improve the performance of a smaller model.
We gather and open-source a dataset of more than 600,000 examples of R1 reasoning traces from queries and passages in MS MARCO.
Models trained on this dataset show: (1) state-of-the-art performance on advanced reasoning and instruction following datasets; (2) work remarkably well out of distribution due to the ability to respond to user-input prompts; and (3) have explainable reasoning chains that can be given to users or RAG-based systems.
Further, we demonstrate that quantized versions of these models retain strong performance while using less compute/memory.
Overall, \modelnamepretty\ shows that test-time compute allows for a fundamentally new type of explainable and performant reranker model for search.\footnote{Models, code, and data are available at \url{https://github.com/orionw/rank1}}
\end{abstract}



\section{Introduction}
Reasoning language models (LMs) like OpenAI's o1, Deepseek's R1, and Gemini's Flash-Thinking have shown improved reasoning abilities through the use of test-time compute, i.e. generating a \textit{reasoning chain} of tokens that allow the model to ``think" before giving the final answer. Another large benefit to these style of models is that the reasoning chain can easily be distilled into smaller models. As shown by Deepseek's R1 \citep{guo2025deepseek} smaller models learn incredibly well from simple supervised fine-tuning on the larger model's reasoning chains.%

The benefits that reasoning models bring to general text generation would also be valuable in an information retrieval (IR) context: allowing models additional time to reason why a passage could be relevant, while also allowing an auditable reasoning process to give to the user or RAG system. For this approach to be maximally effective, the model must be able to reason over both query and passage; if applied solely to the query, the reasoning model would not know the passage context and would have to try to infer it. This would be a form of query-expansion \citep{nogueira2019document} and limits the model's ability to be precise. Thus, our work focuses on bringing test-time compute to IR in a \emph{reranking} setting, where the model needs to compute the relevance of an initial top-k candidates.  

To accomplish this goal, we sample 635,000 examples of R1's thought process on the MS MARCO dataset \citep{msmarco}. We then fine-tune a suite of LMs on these reasoning chains and find that they show remarkable reasoning capabilities. Surprisingly, they also exhibit an ability to be prompted despite training from the base LMs only (without instruction fine-tuning) and while having no instruction-based IR training data (only MS MARCO). This includes state-of-the-art performance on the BRIGHT benchmark for reasoning \citep{su2024bright}, the NevIR benchmark 
 on complex negation understanding, and the mFollowIR dataset on multilingual instruction-following in IR despite having no non-English reasoning training data \citep{weller2025mfollowir}. 

 \begin{figure*}[t]
    \centering

    \includegraphics[width=0.99\linewidth,trim=0.75cm 0cm 0.75cm 1.5cm]{figures/Rank1_example.drawio.pdf}
    \caption{Example reasoning traces from R1, used to train \modelnamepretty. Note the self-inquisitory reasoning (\textcolor{darkblue}{in blue}) where the model questions is if it has the correct answer. 
    \vspace{-0.5em}
    }
    \label{fig:ablation_ours}
\end{figure*}

 We also conduct a detailed analysis of  performance on traditional IR benchmarks, such as TREC DL19 \citep{craswell2020overview} and BEIR \citep{thakur2021beir}. We find that these datasets are likely model-saturated, as \modelnamepretty\ surfaces an extremely large number of unjudged documents (364\% more than RankLLaMA-14B). We argue these benchmarks are no longer helpful for distinguishing between the best performing rerankers and that focus should be put on benchmarks that examine advanced reasoning, instruction-following, and have more modern (e.g. post-ChatGPT) annotations.

 Overall, \modelnamepretty\ shows the many benefits that test-time compute can bring to the field of IR: \textbf{explainable reasoning chains} that can be audited by users or used by agentic RAG systems, \textbf{significantly improved reasoning} performance, and \textbf{adaptability from user-given prompts}.

\section{Model Training}
\subsection{Data Preparation}
In order to distil from R1, we first need to gather data to use to prompt it. We use the MS MARCO collection due to its diversity in topics and common use in previous work. We use \href{https://kluster.ai}{kluster.ai} as the API service to access R1 using their batch mode. 

We generate data from an equal number (25\% of the data) from each of the (1) positive examples in MS MARCO, (2) sampled negatives from Tevatron\footnote{From \url{https://huggingface.co/datasets/Tevatron/msmarco-passage-aug}} (gathered from BM25 and CoCondenser), (3) rank 1-5 hard negatives from mT5-13B, and (4) rank 5-10 hard negatives from mT5-13B. However, we found that R1 classified roughly 80\% of the mT5 hard negatives as positives. Thus we did another round of generation using only hard negatives from rank 5-10 and easy negatives from Tevatron. As the mined hard negatives do not have an official label, it is likely that many of them were false negatives and that R1 classified them correctly.

After all generation, our dataset has 635,264 examples of R1 generations, where R1 labeled 62.9\% as relevant and 37.1\% as non-relevant. We show a plot of these generation lengths in Figure~\ref{fig:data} where we see a fairly normal distribution. Although we thought there may be length differences between these four subsets of data, we found that they all had the same rough distribution.

\subsection{Data Mix and Quality Filtering}
Since we had a surplus of documented judged relevant, we tried various methods to come to our final data mix. We initially tried using all the data, after balancing for the labels. We found that this performed significantly worse than filtering based on the labels we were most sure about (i.e. the positives from MS MARCO and the negatives from Tevatron). However, even on those subsets, 15\% of R1's final prediction disagreed with the implied labels -- thus we filtered out these instances. 

Beyond being labeled as positives, we found that a large number of the mT5 mined hard negative samples were noisy. To alleviate this, we used a model trained on the first mix to self-filter the data.\footnote{We used the Mistral 24B version as it was the largest that fit on 1 GPU, filtering all instances where the model's prediction didn't agree with R1's prediction.} This filtered approximately another 10\% of the data, mostly false negatives. Since we still had a surplus of "relevant" labelled instances, we took all positives from the official MS MARCO positives and all negatives from the self-filtered set. This left us with a training set of 386,336 high quality training samples: 136k from the original MS MARCO positives, 154k from the Tevatron negatives, and 96k from the mT5 negatives. We note that having more negatives is standard: RankLLaMA \citep{ma2024fine} trained on a 15:1 ratio of negatives to positives for 7 million examples.


 \begin{figure*}[t]
    \centering

    \includegraphics[width=0.99\linewidth,trim=0cm 0.5cm 0cm 0cm]{figures/word_distribution.pdf}
    \caption{Distribution of word lengths of the reasoning chains generated from R1. It has a slightly rightward skew but is generally normal shaped. Note that there is no noticeable difference in the distribution between passages that are predicted relevant vs non-relevant. 
    }
    \label{fig:data}
\end{figure*}


\subsection{Training}
We train three main models from the Qwen 2.5 family of models \citep{yang2024qwen2}. These models have shown improved performance on recent LM benchmarks and have a wide range of models, allowing us to show the effects of scale. We use the 7B, 14B, and 32B parameter models. We show that alternate base models are also effective in Section~\ref{sec:alternate_base}. During initial experiments we found that the base models outperformed their instruction-tuned variants, so we start from the base versions.\footnote{This could be because instruct-versions are optimized for chat and math data, whereas we have a large amount of reranking-specific data that doesn't benefit from chat/math-based instructions.}

We train the models with LoRA using LLaMA-Factory \citep{zheng2024llamafactory} for up to two epochs or for up to three days. We found that there was increased learning for roughly the 1.5 epochs but then performance saturated. For more details and hyperparameters configurations see Appendix~\ref{app:hyperparameters}.



\section{Experiments}
We show the capabilities of the \modelnamepretty\, through evaluation on advanced reasoning, instruction-following, and semantic-understanding datasets. We also demonstrate performance on traditional benchmarks. We use \texttt{mteb} \citep{muennighoff2022mteb,enevoldsen2025mmteb} to run all experiments except for DL19, which uses \texttt{rankllm} \citep{pradeep2023rankvicuna}. Inference is powered by \texttt{vllm} \citep{kwon2023efficient} which makes it significantly faster than vanilla \texttt{transformers} \citep{wolf-etal-2020-transformers}.

\subsection{Baselines}
We use BM25 \citep{Robertson1994OkapiAT,lu2024bm25s} and mE5-base \citep{wang2024multilingual} for our first stage models. For reranking models we show mainly other pointwise reranking models (e.g. models that output a score per document): MonoT5-3B \citep{nogueira2019document}, mT5-13B fine-tuned on MMARCO for multilingual tasks \citep{unicamp-at-neuclir}, and RankLLaMA 7 and 13B \citep{ma2024fine}. For instruction following tasks we also include FollowIR-7B \citep{weller2024followir} which was trained solely for instruction-following. 


\begin{table*}[t!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|rrrrrrr|rr|rrr|r}
\toprule
& \multicolumn{7}{c|}{StackExchange} & \multicolumn{2}{c|}{Coding} & \multicolumn{3}{c|}{Theorem-based} & \multirow{2}{*}{\centering Avg.}\\
\cmidrule(r){2-8} \cmidrule(r){9-10} \cmidrule(r){11-13}
& Bio. & Earth. & Econ. & Psy. & Rob. & Stack. & Sus. & Leet. & Pony & AoPS & TheoQ. & TheoT. \\
\midrule
BM25  & 19.2 & 27.1 & 14.9 & 12.5 & 13.5 & 16.5 & 15.2 & 24.4 & 7.9 & 6.0 & 13.0 & 6.9 & 14.8\\
BM25 on GPT-4o CoT & 53.6 & 53.6 & 24.3 & 38.6 & 18.8 & 22.7 & 25.9 & 19.3 & 17.7 & 3.9 & 18.9 & 20.2 & 26.5  \\
\midrule
MonoT5-3B & 16.0 & 24.0 & 17.7 & 19.5 & 8.0 & 10.5 & 19.5 & 17.2 & 29.2 & 7.1 & 20.3 & 12.0 & 16.8 \\
RankLLaMA-7B & 17.5 & 15.5 & 13.1 & 13.6 & 17.9 & 6.9 & 16.9 & 8.4 & \textbf{46.8} & 2.2 & 4.5 & 3.5 & 13.9 \\
RankLLaMA-13B  & 21.6 & 19.1 & 16.3 & 14.0 & 15.7 & 7.7 & 18.5 & 8.8 & 31.1 & 1.7 & 4.4 & 4.9 & 13.7 \\
 \modelname-7B & 48.8 & 36.7 & 20.8 & 35.0 & 22.0 & 18.7 & \textbf{36.2} & 12.7 & 31.2 & 6.3 & \textbf{23.7} & 37.8 & 27.5 \\

\modelname-14B & 49.3 & \textbf{37.7} & \textbf{22.6} & 35.2 & \textbf{22.5} & 20.8 & 33.6 & 17.7 & 33.2 & 8.4 & 22.5 & 41.4 & 28.7 \\

\modelname-32B & \textbf{49.7} & 35.8 & 22.0 & \textbf{37.5} & \textbf{22.5} & \textbf{21.7} & 35.0 & \textbf{18.8} & 32.5 & \textbf{10.8} & 22.9 & \textbf{43.7} & \textbf{29.4} \\

\bottomrule
\end{tabular}
}
\caption{The performance of retrieval models on BRIGHT. Numbers for ReasoningRank are taken from their paper \citep{ji2024reasoningrank} while BM25 scores are taken from the official BRIGHT paper. All models rerank from the BM25 on GPT-4o CoT top 100 documents, but are not given the GPT-4o CoT. \textbf{We find a large gap between similar sized rerankers and \modelnamepretty\ models (sometimes 2x)}. Bold indicates the best score for that subset in the reranker section.}
\label{tab:bright}
\end{table*}

We show results for listwise models when those scores are available \citep{pradeep2023rankzephyr,niu2024judgerank,sun2023chatgpt}, but we note that they are not comparable -- listwise models take an order of magnitude more time at inference due to their sequential dependencies and have the advantage of seeing all documents in their context when reranking. We show them generally as a strong upper bound, as when state-of-the-art (SOTA) LMs (i.e. GPT-4o) are used.

\subsection{Reasoning Capabilities}
We show results on the reasoning intensive BRIGHT benchmark \citep{su2024bright} in Table~\ref{tab:bright}. All reranker models judge the top 100 documents found using BM25 on the query plus GPT-4o's Chain of Thought (CoT) reasoning, which performed significantly better than BM25 without the query expansion (thus including more relevant documents in the top 100). However, at inference time, the rerankers are not given the CoT. We see a large gap between \modelnamepretty\ and other models: in many cases near double the nDCG@10 score (e.g. 48.8 vs 24.3 for \modelnamepretty-7B vs ReasoningRank GPT-4o on Biology, or 18.7 vs 7.7 for \modelnamepretty-7B vs RankLLaMA-13B on Stackoverflow).  

These results are especially notable when you consider that \modelnamepretty\ models were \textbf{trained on an order of magnitude less data} than models like RankLLaMA (7 million vs 600k) \textbf{while using the same training dataset} (MS MARCO). We also find that performance scales with model size, with the 32B model outperforming the smaller models. Thus, \modelnamepretty\ is SOTA when reasoning is needed.

\subsection{Semantic Understanding}
    \begin{wraptable}[11]{r}{0.4\textwidth}
    \vspace{-29pt}
        \centering
        \begin{tabular}{llr}
        \toprule
         & Model & Score (\%) \\
        \midrule
        \multirow{3}{*}[0em]{\rotatebox{90}{Listwise}} 
        & RankGPT 4o-mini & 64.1 \\
        & RankGPT 4o & 70.1 \\
        & RankGPT o3-mini & \textbf{77.3} \\
        \midrule
        \multirow{7}{*}[-0.5em]{\rotatebox{90}{Pointwise}} 
        & RankLlama 7B & 31.6 \\
        & RankLlama 13B & 43.2 \\
        & MonoT5 base & 34.9 \\
        & MonoT5 3B & 50.6 \\
        & \modelname-7B & 65.1 \\
        & \modelname-14B & 67.5 \\
        & \modelname-32B & \textbf{70.1} \\
        \bottomrule
        \end{tabular}
        \caption{Pairwise acc. on NevIR}
        \label{tab:nevir}
    \vspace{-1em}
\end{wraptable}



We also evaluate on the NevIR benchmark which requires reasoning over negation in Table~\ref{tab:nevir}. Models rerank all documents, and we report scores for listwise models from \citet{van2025reproducing}. 

We again find that \modelnamepretty\ performs extremely well, even matching GPT-4o and coming 15+ points higher than the next closest model. We see that even o3-mini in a listwise setup only performs 7 points higher.




\begin{table*}[t]
\centering
\label{tab:mfollowir_cross_lingual}
\vspace{0.5em}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|cc@{\hspace{0.2em}}r|cc@{\hspace{0.2em}}r|cc@{\hspace{0.2em}}r|c@{\hspace{0.2em}}r}
\toprule
 & \multicolumn{3}{c|}{Persian} & \multicolumn{3}{c|}{Chinese} & \multicolumn{3}{c|}{Russian} & \multicolumn{2}{c}{Average} \\
\cmidrule(l){2-4} \cmidrule(l){5-7} \cmidrule(l){8-10} \cmidrule(l){11-12}
 Model & nDCG@20 & & p-MRR & nDCG@20 & & p-MRR & nDCG@20 & & p-MRR & nDCG@20 & p-MRR \\
\midrule
 mE5-base & 0.289 & & -3.9 & 0.316 & & +3.4 & 0.307 & & -2.1 & 0.304 & -0.9 \\
\midrule
MonoT5-3B & 0.118 & & -2.4 & 0.231 & & +5.0 & 0.240 & & +6.8 & 0.196 & +3.1 \\ 
FollowIR-7B & 0.225 & & +1.8 & 0.375 & & +8.7 & 0.376 & & +0.4 & 0.325 & +3.7 \\
mT3-13B & 0.453 & & -0.7 & 0.474 & & +2.3 & 0.505 & & -0.6 & 0.477 & +0.4 \\
RankLLaMA 7B & 0.229 & & +0.8 & 0.272 & & +1.1 & 0.248 & & +0.1 & 0.250 & +0.7 \\
RankLLaMA 13B & 0.256 & & +0.8 & 0.287 & & +1.8 & 0.320 & & -0.6 & 0.288 & +0.6 \\
 \modelname-7B & 0.564 & & +7.0 & 0.582 & & +3.1 & 0.511 & & -0.0 & 0.552 & +3.4 \\
 \modelname-14B & \textbf{0.572} & & \textbf{+11.9} & \textbf{0.611} & & +4.6 & 0.516 & & +5.4 & \textbf{0.567} & \textbf{+7.3} \\
 \modelname-32B  & 0.555 & & +3.9 & 0.598 & & \textbf{+4.9} & \textbf{0.521} & & \textbf{+6.6} & 0.558 & +5.2 \\ 
\bottomrule
\end{tabular}
}
\caption{mFollowIR Cross-Lingual scores across three language subsets. Bold indicates best score.}
\vspace{-1em}
\end{table*}




\begin{table*}[t]
\centering
\label{tab:mfollowir_normal}
\vspace{0.5em}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|cc@{\hspace{0.1em}}r|cc@{\hspace{0.1em}}r|cc@{\hspace{0.1em}}r|cc@{\hspace{0.1em}}r}
\toprule
& \multicolumn{3}{c|}{Persian} & \multicolumn{3}{c|}{Chinese} & \multicolumn{3}{c|}{Russian} & \multicolumn{2}{c}{Average} \\
\cmidrule(l){2-4} \cmidrule(l){5-7} \cmidrule(l){8-10} \cmidrule(l){11-12}
 Model & nDCG@20 & & p-MRR & nDCG@20 & & p-MRR & nDCG@20 & & p-MRR & nDCG@20 & p-MRR \\
\midrule
mE5-base & 0.493 & & -4.2 & 0.441 & & +0.3 & 0.417 & & -3.5 & 0.450 & -2.5 \\
\midrule
 MonoT5-3B & 0.130 & & -3.8 & 0.233 & & +1.4 & 0.254 & & +2.2 & 0.206 & -0.1 \\
 FollowIR-7B & 0.163 & & -0.1 & 0.404 & & +6.6 & 0.379 & & +7.7 & 0.315 & +4.8 \\
 mT5-13B & 0.498 & & +0.1 & 0.548 & & +4.2 & 0.506 & & +1.9 & 0.517 & +2.0 \\
RankLLaMA-7B & 0.248 & & -0.9 & 0.397 & & +1.3 & 0.396 & & -0.1 & 0.347 & +0.1 \\
 RankLLaMA-13B & 0.333 & & -2.0 & 0.448 & & +1.0 & 0.484 & & +0.5 & 0.422 & -0.2 \\
 \modelname-7B & 0.564 & & +2.4 & 0.665 & & +5.9 & 0.528 & & +4.7 & 0.586 & +4.4 \\
  \modelname-14B & 0.605 & & +11.1 & 0.647 & & +0.8 & 0.530 & & -1.0 & 0.594 & +3.6 \\
 \modelname-32B  & \textbf{0.619} & & \textbf{+12.1} & \textbf{0.678} & & \textbf{+10.2} & \textbf{0.535} & & \textbf{+8.1} & \textbf{0.610} & \textbf{+10.1} \\

\bottomrule
\end{tabular}
}
\caption{Results for mFollowIR multilingual across three language subsets (Persian, Chinese, Russian). All models rerank the top 100 docs found from the mE5-base model. \textbf{We see a wide gap between \modelnamepretty\ and other models, despite it not having any multilingual reranking training data.}}
\end{table*}



\subsection{Instruction-Following}
We show results on the mFollowIR dataset \citep{weller2025mfollowir} as it illustrates both instruction-following and multilingual capabilities. Table~\ref{tab:mfollowir_normal} shows results on the cross-lingual setup (En-XX) and Table~\ref{tab:mfollowir_normal} on the XX-XX task. We have all models rerank the top 100 scores of a strong but small base model mE5-base. We find that \modelnamepretty\ has much higher nDCG@20 scores (from 0.586 to 0.611 on the multilingual average) compared to the next best model (mT5-13B trained on Multilingual MS MARCO) with 0.517. Furthermore, when considering just instruction following metrics there is a wide gap, especially in the multilingual setting (+10.1  vs +4.8 p-MRR on the custom instruction-trained FollowIR-7B). Other models are closer on the cross-lingual version, but there still remains a notable gap between the best \modelnamepretty\ and the closest other model (+7.3 vs +3.7 p-MRR).


We again find this especially notable considering that mT5-13B is a similar size and was trained on multilingual data. \modelnamepretty\ significantly outperforms it solely with English reasoning data.


\subsection{``Traditional" Benchmarks}

    \begin{wraptable}[11]{r}{0.7\textwidth}
    \vspace{-10pt}
        \centering
        \small
        \begin{tabular}{l|cc|c}
        \toprule
            & \multicolumn{2}{c}{Original} & \multicolumn{1}{c}{Fixed} \\
         Model & Judged@10 & nDCG@10 & nDCG@10 \\
         \midrule
        RankLlama 7B & 96.1 & 76.2 & 76.9 \\
        RankLlama 13B & 96.1 & 77.2 & 76.9 \\
        MonoT5 3B & 91.2 & 72.0 & 74.8 \\
        \modelname-7B & 83.5 &  66.1 & 78.6 \\
        \modelname-14B & 82.3 & 64.8 & 77.6  \\
        \modelname-32B & 81.9 & 66.0 & \textbf{80.1} \\
        \bottomrule
        \end{tabular}
        \caption{Results on TREC DL19}
        \label{tab:dl19}
    \vspace{-1em}
\end{wraptable}


\paragraph{DL19}
We also evaluate DL19 scores in Table~\ref{tab:dl19}. All models rerank the top 100 passages found using RepLLaMA. We found that our models performed significantly worse on the initial qrels and our analysis quickly showed this was due to the number of unjudged documents that our models ranks higher. As seen in Table~\ref{tab:dl19}, the top 10 lists of existing pointwise rerankers is almost entirely judged passages (i.e. 96.1\%), whereas \modelnamepretty\ finds 10-15\% less (down to 81.9\%). 

To remedy this, we annotated all top 10 documents that each model (baseline or \modelnamepretty) got wrong or were unjudged. We show examples of incorrectly labeled instances in Appendix~\ref{app:dl19}. We found that unjudged documents were mostly relevant, e.g. 70.6\% of the unjudged documents for RankLLaMA 14B were relevant and 86.8\% were relevant for \modelname-14B, etc.\footnote{We release our new judged qrels in the Github above to help facilitate future work.} After this annotation fix, we find that \modelnamepretty\ models are no longer penalized for finding new documents and that performance is generally better than all other models (e.g. 78.6 vs 76.8 nDCG for \modelnamepretty-7B vs RankLLaMA 7B).

Thus, it seems that the original DL19 benchmark is no longer suitable for discriminating between the top performing approaches. Although this was examined for some TREC collections in 2022 by \citet{voorhees2022can}, the largest model used in their experiments was BERT. Thus, motivated by our analysis, we would encourage the community to re-evaluate old TREC collections.



\paragraph{BEIR}
We show results on BEIR as a comparison in Table~\ref{tab:beir}, using the datasets with less than 2k queries. All models rerank the top 100 documents found using BM25s \citep{lu2024bm25s}. We find comparable but worse performance with \modelnamepretty. Although out of scope for this work, we found a large number of similar issues in BEIR datasets as we do in DL19. We discuss each dataset individually in Appendix~\ref{app:datasets}. It appears that traditional reranking benchmarks like DL19 and BEIR -- although extremely successful at driving the field forward and still useful for weaker models -- are no longer as useful for distinguishing between the best performing rerankers. 


\subsection{Test-Time Scaling}
Given the success of reasoning language models in \textit{scaling} test time compute (i.e. getting better results with more tokens used), we also attempted several ways of using extra test-time compute to improve performance. However, using the simple budget-forcing method from s1 \citep{muennighoff2025s1} did not improve performance and actually hurt performance on average. We hypothesize this may be due to the lack of difficulty in the reranking task -- after all, reasoning over BRIGHT requires significantly less reasoning than the typical AIME problems used for evaluation. Alternatively, perhaps new techniques are needed to carefully induce the desired results.



\begin{table*}[t]
\centering
\footnotesize
\setlength{\tabcolsep}{3pt}
\medskip
\begin{tabular}{l|rrrrrrrrr|r}
\toprule
Model & ArguA & ClimF & DBP & FiQA & NFCorp & SciDoc & SciFact & Touche & TrecC & Avg. \\
\midrule
BM25S & 47.2 & 18.6 & 32.0 & 25.4 & 34.3 & 16.5 & 69.1 & 34.7 & 68.8 & 38.5 \\
\midrule
MonoT5-3B & 42.5 & 25.4 & 44.5 & 46.5 & 37.8 & 19.3 & 76.1 & 30.7 & 79.6 & 44.7 \\
RankLLaMA-7B  & 54.4 & 23.2 & 43.7 & 42.1 & 27.0 & 16.6 & 71.1 & 41.4 & 80.2 & 44.4 \\
RankLLaMA-13B & 49.3 & 24.5 & 44.9 & 44.1 & 28.1 & 18.1 & 72.7 & 39.2 & 80.8 & 44.6 \\
\modelname-7B & 42.8 & 15.0 & 38.9 & 39.5 & 36.2 & 17.2 & 77.2 & 22.8 & 81.9 & 40.9 \\
\modelname-14B & 45.3 & 16.2 & 37.4 & 37.9 & 35.8 & 17.9 & 77.0 & 27.1 & 78.2 & 41.0 \\
\modelname-32B & 57.6 & 15.8 & 40.7 & 41.8 & 36.9 & 19.6 & 76.8 & 19.9 & 81.9 & 41.7 \\
\bottomrule
\end{tabular}
\caption{nDCG@10 results on the BEIR evaluation benchmark. Models rerank the top 100 documents from BM25S \citep{lu2024bm25s}. Best results on each dataset and the entire benchmark are boldfaced.}
\label{tab:beir}
\end{table*}

\section{Model Releases}
\paragraph{Alternate Base Models}
\label{sec:alternate_base}
Our main base models use the Qwen 2.5 series due to their strong performance and varying parameter sizes. However, to show that this approach also works on other base models, we train a version with Llama 3.1 8B \citep{llama3modelcard} and Mistral Small 2501 24B.\footnote{Mistral Small can be found at \url{https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501}} Similar to the Qwen models, we start from the base versions, not the instruct versions. We show the results on selected benchmarks in Table~\ref{tab:quantization} where we can see that our approach holds for other base models with performance generally increasing as they scale (i.e. 24B is better than 8B). We also see that Llama 3.1 8B slightly underperforms Qwen 7B. 

\paragraph{Quantization}
IR models typically require a heavier inference workload compared to many other LLM applications due to the number of passages to rerank. However, many common rerankers (MonoT5, RankLLaMA) do not take advantage of modern inference capabilities such as quantization. 

We quantize each of our models using AutoAWQ \citep{lin2023awq} and compare performance before and after. We see that although performance drops slightly, the model size is significantly smaller, enabling \textbf{all models} (including the 32B version) to be run on one 24GB GPU. Despite the performance loss, quantized \modelnamepretty\ models still significantly outperform the baselines on reasoning and instruction-following while being 1/3rd of the size.



\begin{table*}[t]
\centering
\setlength{\tabcolsep}{3pt}
\medskip
\begin{tabular}{l|r|rrrr}
\toprule
Model & Size (G) & SciFact & NevIR & Biology & Eng-Zho \\
\midrule
\modelname-7B &  15.2 &  77.2 & 65.1 & 48.8 & 58.2 \\
 \hspace{0.5em} quantized & 5.6 & 75.4 & 62.0 & 42.8 & 56.3 \\
 \midrule
\modelname-Llama3-8B & 16.1 & 73.2 & 61.9 & 45.8 & 58.1 \\
 \hspace{0.5em} quantized & 5.8 & 72.9 & 57.1 &  41.5 & 54.8 \\ 
  \midrule
\modelname-14B & 29.6 & 77.0 & 67.5 & 49.3 & 61.1  \\
 \hspace{0.5em} quantized & 10.0 & 75.6 & 66.4 & 44.6 & 58.8  \\
  \midrule
\modelname-Mistral-24B & 47.2 & 75.8 & 67.4 & 51.8 & 59.9 \\
 \hspace{0.5em} quantized & 14.3 & 72.7 & 64.6 & 47.1 & 59.4   \\ 
  \midrule
\modelname-32B & 65.5 & 76.8 & 70.1 & 49.7 & 59.8 \\
 \hspace{0.5em} quantized & 19.3 & 77.2 & 69.9 & 52.3 & 59.7   \\ 
\bottomrule
\end{tabular}
\caption{Quantization results on subsets of various tasks (SciFact from BEIR, Biology subset from BRIGHT, Eng-Zho on mFollowIR-CrossLingual). We use AutoAWQ for the quantization into int4. We see that models retain most of their performance while being significantly smaller.}
\label{tab:quantization}
\end{table*}


\section{Related Work}
\subsection{Advanced Reasoning and Instruction Following in IR}
In the last couple years, retrieval systems have started to move beyond simple phrase-based semantic matching, to more complex information retrieval tasks. This has included a focus on new benchmarks: such as reasoning in retrieval \citep{su2024bright}, instruction-following capabilities \citep{weller2024followir,oh2024instructir}, and retrieval for RAG systems \citep{lawrie2024overview,mayfield2024evaluation}.

On the modeling side, we have seen a surge of interest in models that can understand the meaning behind the user's query rather than doing phrase-based matching: this includes models like Instructor \citep{instructor_models}, TART \citep{asai2022tart}, GritLM \citep{muennighoff2024generative}, FollowIR \citep{weller2024followir}, Gecko \citep{lee2024gecko}, and Promptriever \citep{weller2024promptriever}. These models typically use instruction-based data in their training data, so that they learn to adapt to new user instructions. However, for \modelnamepretty\ we do not provide any instruction-based training data and fine-tune from the base (non-instruct) version of the LMs -- despite this our model shows SOTA ability in these tasks.

\subsection{Reasoning Language Models}
Reasoning language models were introduced by OpenAI with their o1 model \citep{jaech2024openai}. These models showed significantly improved performance on tasks that needed reasoning, such as math, logic, and programming. Since their release, many others have trained similar style models, including Google's Gemini Flash Thinking and Deepseek's R1. Notably, R1 is the only reasoning model that provides reasoning chains through APIs and is the only open-weights model.

Other than the impressive performance gains of these models, one additional feature is that models can quickly learn to emulate stronger models through basic supervised fine-tuning, rather than the more complex reinforcement learning pipelines that are typically used. We take advantage of this capability to train \modelnamepretty\ using a simple training process.

There have also been a flurry of works in the open-source space, both before and after o1 on reasoning language models and systems, focused on reproduction \citep{snell2024scaling,muennighoff2025s1}, calibration and confidence \citep{jurayj2025your}, agentic capabilities with explainable reasoning traces \citep{weir2022nellie,weir2024enhancing}, and much more. We expect this line of work to continue and continued collaboration will likely improve these models in retrieval as well.

\section{Limitations and Future Work}
\paragraph{Overthinking} Like other reasoning models, \modelnamepretty\ can make mistakes and it can be surprising to see the model's reasoning chain come close to a correct answer only to change it's mind. We also found in particular that \modelnamepretty\ can be particularly stringent in marking passages as true. For example, on a TREC COVID query about the origins of COVID-19 it marked every single passage as non-relevant, since none mentioned the specific wet market in Wuhan. However, when given a prompt to assume the user had no information about COVID-19, it was able to adapt better (although not perfectly). We observed this tendency to ``overthink" when using the model interactively as it would already know the answer and was looking for a very specific phrase. We expect that this could be lessened with data that specifically trains the model to calibrate this.

\paragraph{Inference Speed}
As a reasoning model using test-time compute \modelnamepretty\ is slower than a model with only a classification head (i.e. RankLLaMA). In practice this can be somewhat mitigated by the usage of modern paged attention libraries like vLLM \citep{kwon2023efficient} which we use, and also quantization techniques. Nonetheless, there is no getting around the fact that using test-time compute requires spending more compute than non-test-time compute models. Despite this additional compute usage, we see that users are willing to wait longer for quality search results, as illustrated by the popularity of the Deep Research products from Google and OpenAI.\footnote{\url{https://openai.com/index/introducing-deep-research/} and \url{https://blog.google/products/gemini/google-gemini-deep-research/}}

\paragraph{Future Work}
\modelnamepretty\ also brings many more exciting areas for future work. As highlighted in the experiments section, it does extremely well despite the relatively small and non-diverse training data. Some of the straightforward areas of future work include:
\begin{itemize}
    \item Fine-tuning with RL: although supervised fine-tuning works, it does not optimize for the final answer. It is likely that RL-based approaches would be able to add additional rewards/penalties that could better align the final prediction to the label.
    \item Listwise reasoning rerankers: although pointwise models are more efficient and more parallelizable, listwise rankers typically are more performant since they can see many documents at once. Simply gathering new data should enable this to be very successful.
    \item Multilingual and instruction-tuned versions: \modelnamepretty\ was solely trained on English and non-instruct data and still showed strong results in these settings. Training on a more curated set of datasets will likely significantly improve the performance on these tasks.
\end{itemize}



\section{Conclusion}
We build the first reasoning reranker model that uses test-time compute, \modelnamepretty. We do so by collecting 600k+ examples from the reasoning language model R1, fine-tuning on its reasoning traces. Despite only using English MS MARCO data and training from base (non-instruct-tuned) language models, \modelnamepretty\ shows state of the art reasoning and instruction following capabilities, even in multilingual settings. Overall, \modelnamepretty\ introduces a new category of reranking models that enable a wide variety of more complex information retrieval tasks.

\section*{Acknowledgments}
This work has been supported by both DARPA SciFy and the U.S. National Science Foundation under grant 2204926. Any opinions, findings, and conclusions or recommendations expressed in this article are those of the authors and do not necessarily reflect the views of the National Science Foundation or DARPA. OW is supported by an NSF GRFP fellowship. 



\bibliography{colm2025_conference}
\bibliographystyle{colm2025_conference}

\appendix

\section{Training and Hyperparameter Details}
\label{app:hyperparameters}
We use nodes of 4x80GB H100 machines for training. Models are trained for up to 2 epochs or until 3 days of training (for the 32B model).

We fine-tune with LLaMA-Factory, using LoRA on all parameters with rank 32 and alpha 64. We use a learning rate of 1e-4 and an effective batch size of 128. We use early stopping based on the Bright Biology and NevIR scores. Models use prompts for BEIR and non-stackexchange cases so that they can understand the task (Appendix~\ref{app:dataset_prompts}). 

Inference is done on 1 H100 80GB GPU. Baselines use default hyperparameters for max length and fp16 (and there are no other parameters). 

\section{Prompt for R1}
\label{app:prompt_generate}
We use the following prompt (Figure~\ref{fig:prompt}) to generate data for R1 (and use the same template for \modelnamepretty). 

\begin{figure*}
\begin{tcolorbox}[
    colback=white,
    colframe=black,
    title=Prompt for Inference
    ]
Determine if the following passage is relevant to the query. Answer only with 'true' or 'false'.

\medskip

Query: {{query}}

\medskip

Passage: {{document}}

\medskip

<think>
\end{tcolorbox}
\caption{Prompt used to generate data with R1 and also for inference with \modelnamepretty.}
\label{fig:prompt}
\end{figure*}

\section{Examples of Unjudged and Incorrect DL19 Labels}
\label{app:dl19}
We re-annotated the top 10 passages for each model that got an incorrect or unjudged label. This was 295 labels. We found that none of the labels changed from correct to incorrect, but some labels went from incorrect to correct (Table~\ref{tab:dl19_changes}).

We also show some examples of these cases in Table~\ref{tab:dl19_errors}. The incorrect labels affected all models roughly equally, as seen previous in Table~\ref{tab:dl19_changes} at around 6\% of the top 10 documents, while the unjudged documents were mostly found by \modelnamepretty.


\begin{table}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Unjudged$\rightarrow$Relevant} & \textbf{Changed$\rightarrow$Correct} \\
\midrule
MonoT5-3b & 50.00\% (19/38) & 6.74\% (29/430) \\
RankLLaMA-13B & 70.59\% (12/17) & 6.28\% (27/430) \\
RankLLAMA-7B & 70.59\% (12/17) & 6.74\% (29/430) \\
\modelname-7B & 85.92\% (61/71) & 5.35\% (23/430) \\
\modelname-14B & 86.84\% (66/76) & 6.05\% (26/430) \\
\modelname-32B & 88.46\% (69/78) & 6.51\% (28/430) \\
\bottomrule
\end{tabular}
\vspace{0.2em}
\caption{Qrel changes in the DL19 annotations}
\label{tab:dl19_changes}
\end{table}


\begin{table}
\centering
\begin{tabular}{p{0.95\textwidth}}
\toprule
\textbf{Example 1} \\
\midrule
\textbf{Query:} what is physical description of spruce \\
\textbf{Passage:} Spruces are large trees, from about 20--60 metres (about 60--200 feet) tall when mature, and can be distinguished by their whorled branches and conical form. The needles, or leaves, of spruce trees are attached singly to the branches in a spiral fashion, each needle on a small peg-like structure. \\
\textbf{Original label:} 0 (non-relevant) \\
\textbf{New label:} 3 (relevant) \\
\midrule
\textbf{Example 2} \\
\midrule
\textbf{Query:} causes of left ventricular hypertrophy \\
\textbf{Passage:} High blood pressure may also bring on heart failure by causing left ventricular hypertrophy, a thickening of the heart muscle that results in less effective muscle relaxation between heart beats. This makes it difficult for the heart to fill with enough blood to supply the body's organs, especially during exercise, leading your body to hold onto fluids and your heart rate to increase. \\
\textbf{Original label:} unjudged \\
\textbf{New label:} 3 (relevant) \\
\midrule
\textbf{Example 3} \\
\midrule
\textbf{Query:} what are the social determinants of health \\
\textbf{Passage:} © Zoltan Balogh. The social determinants of health (SDH) are the conditions in which people are born, grow, work, live, and age, and the wider set of forces and systems shaping the conditions of daily life. \\
\textbf{Original label:} 1 (on topic) \\
\textbf{New label:} 3 (relevant) \\
\midrule
\textbf{Example 4} \\
\midrule
\textbf{Query:} example of monotonic function \\
\textbf{Passage:} Overview of the exponential function. The exponential function is one of the most important functions in mathematics (though it would have to admit that the linear function ranks even higher in importance). To form an exponential function, we let the independent variable be the exponent. A simple example is the function $f(x)=2^x$. ... \\
\textbf{Original label:} unjudged \\
\textbf{New label:} 3 (relevant) \\
\bottomrule
\end{tabular}
\vspace{0.2em}
\caption{Examples of incorrect labels and unjudged documents in the DL19 annotations.}
\label{tab:dl19_errors}
\end{table}

\section{Prompts for specific datasets}
\label{app:dataset_prompts}
Table~\ref{tab:dataset_prompts} shows the data-specific prompts used for BEIR and the non-stackoverflow BRIGHT subsets.

\begin{table*}[htbp]
\small
\centering
\begin{tabular}{p{0.18\textwidth}|p{0.75\textwidth}}
\hline
\textbf{Dataset} & \textbf{Prompt} \\
\hline
\texttt{SciFact} & Claim: FILL\_QUERY\_HERE<newline><newline>A relevant passage would provide evidence that either **supports** or **refutes** this claim. A passage with any information on any related subpart should be relevant. \\
\hline
\texttt{ClimateFEVER} & Claim: FILL\_QUERY\_HERE<newline><newline>A relevant passage would provide evidence that either **supports** or **refutes** this claim. A passage with any information on any related subpart should be relevant.\\
\hline
\texttt{TRECCOVID} & FILL\_QUERY\_HERE If the article answers any part of the question it is relevant. \\
\hline
\texttt{ArguAna} & I am looking to write an essay and need to find counterarguments against this statement:<newline><newline>FILL\_QUERY\_HERE<newline><newline>Does this passage have any counterargument or evidence that could be used to help me? \\
\hline
\texttt{DBPedia} & I am looking to write an essay on this topic and need as much related background information to help me. The topic is:<newline><newline>FILL\_QUERY\_HERE<newline><newline>If the passage provides any background information that could be connected it is relevant. \\
\hline
\texttt{FiQA2018} & FILL\_QUERY\_HERE Find a passage that would be a good answer from StackExchange. \\
\hline
\texttt{NFCorpus} & Topic: FILL\_QUERY\_HERE<newline><newline>Given the above topic, I need to learn about all aspects of it. It does not need to be directly relevant, only tangentially informational. Please mark as relevant any passages with even weak connections. I need to learn fast for my job, which means I need to understand each part individually.<newline><newline>Again remember, any connection means relevant even if indirect. So if it is not addressed, that is okay -- it does not need to be explicitly.<newline><newline>Find me passages with any type of connection, including weak connections!!!! \\
\hline
\texttt{Touche2020} & FILL\_QUERY\_HERE **any** arguments for or against \\
\hline
\texttt{SCIDOCS} & papers that could be cited in FILL\_QUERY\_HERE. Anything with even indirect relevance should be relevant. This includes papers in the same broader field of science \\
\hline
\texttt{BrightRetrieval aops} & Find different but similar math problems to FILL\_QUERY\_HERE<newline><newline>A document is relevant if it uses the same class of functions and shares **any** overlapping techniques. \\
\hline
\texttt{BrightRetrieval theoremqa questions} & Find a passage which uses the same mathematical process as this one: FILL\_QUERY\_HERE \\
\hline
\texttt{BrightRetrieval leetcode} & I am looking to find different problems that share similar data structures (of any kind) or algorithms (e.g. DFS, DP, sorting, traversals, etc.). I am looking for problems that share one or both of these similarities to this:<newline><newline>FILL\_QUERY\_HERE<newline><newline>Does this passage share any similarities? e.g. if there was a textbook on leetcode problems, this would be in the same book even though it could be in a different chapter. \\
\hline
\texttt{BrightRetrieval pony} & I will use the programming language pony. Problem: FILL\_QUERY\_HERE<newline><newline>But to solve the problem above, I need to know things about pony. A passage is relevant if it contains docs that match \textbf{any} part (even basic parts) of the code I will have to write for the above program. \\
\hline
\texttt{BrightRetrieval} & Can you find background information about the concepts used to answer the question:<newline><newline>FILL\_QUERY\_HERE<newline><newline>A passage is relevant if it contains background information about a **sub-concept** that someone might cite/link to when answering the above question. \\
\hline
\texttt{BrightRetrieval theoremqa theorems} & Find a passage which uses the same mathematical process as this one: FILL\_QUERY\_HERE \\
\hline
\end{tabular}
\caption{Dataset-specific prompts used in the BEIR and non-Stackexchange subsets of BRIGHT.}
\label{tab:dataset_prompts}
\end{table*}

\section{Things we tried that didn't work}
We tried a few other things that didn't work:
\begin{itemize}
    \item We tried to calibrate the scores better by adding a ModernBERT model \citep{warner2024smarter} on top of the outputs of the reasoning chain. However, this ended up performing worse. It is possible that with better curation this would be an effective approach however.
    \item We tried adding extra loss to the last token of the next token prediction loss in LLaMA-Factory. However, this resulted in sub-par performance, likely because predicting the last true/false token at the end of reasoning chain is fairly easy (as the model already states beforehand ``the answer is X" leaving little room for doubt).
\end{itemize}

\section{Noise in BEIR benchmarks}
\label{app:datasets}
We discuss here a few of the issues with individual BEIR datasets which cause them to be noisy and less accurate at judging between highly effective systems. 

\paragraph{SciFact}
In the original SciFact work \citep{wadden2020fact} the authors have three labels: support, refute, or not enough information (NEI). However, when incorporated into BEIR, the NEI queries are still included. As there was not evidence to support these queries (as determined by the original authors) these queries are effectively noise. In practice, what it means is that tests model's ability to find the top ranked BM25 document, which is guaranteed to not have enough information to either refute or support the claim. The large number of these queries add a significant amount of noise.


\paragraph{FiQA2018}
FiQA is scraped from Financial Stackexchange \citep{maia201818}. However, during the original scrape the creators did not collect the post's details. Thus, the retrieval setup is to take the posts title and search for the top answer. Yet, in many cases the user clarified important details in the post that entirely changed the meaning of the query. Without this additional information for some queries it is impossible for someone to determine what the best answer is as the post details asked many other questions that were different from the title of the post.

\paragraph{DBPedia}
DBPedia has many partially relevant labels, that give credit for finding non-relevant information. For example, the entire passage with a relevance of ``1" for the query "Eiffel Tower" is ``The year 1989 in architecture involved some significant architectural events and new buildings." However, nothing about the Eiffel Tower is connected to 1989. After digging, the only connection is its the 100 year anniversary of it being built -- but the passage does not mention this. There are many such examples in the dataset (and more documents judged ``1" than ``2"), contributing to the noise.

\paragraph{Touche2020}
Touche2020 has been well examined by \citet{thakur2024systematic}. They created a much cleaner version of the data, however, it is not the ``standard" evaluation set in BEIR.

\paragraph{Other datasets with partial relevance}
Many of the other datasets give credit for partial relevance in a similar manner to DBPedia. This includes TREC COVID (which has so many real positives that it is a non-issue for the top-10), NFCorpus, and SciDocs. For many of these datasets, it is very difficult for even a human to match relevance: e.g. on NFCorpus you would have to guess any potential link that had been on that website and was even ancillarily related to the title. An example is the query ``How Fruits and Vegetables Can Treat Asthma" which matches a document with the title ``Effect of a single high-fat meal on endothelial function in healthy subjects." and does not discuss asthma at all. The reranker would have to assume that any passage discussing food or asthma separately in any context would be relevant. Although one intution is that these documents should rank higher than completely non-relevant documents, for \modelnamepretty\ it treats them as the same (as they are both equally non-relevant to the query). This may be suboptimal for some approaches, but for today's RAG use cases returning only actual relevant documents seems more useful.




\end{document}
