\section{} 
\label{reduction-P}

To prove our argument, we apply the splitting property of the Poisson process. Let \( N(t) \) be a Poisson process with rate parameter \( \lambda \). If events are split into two groups with probabilities \( p \) and \( 1-p \), then the resulting processes \( N_1(t) \) and \( N_2(t) \) are independent Poisson processes with rate parameters \( p\lambda \) and \( (1-p)\lambda \) respectively \cite{splitting_poisson}.

From process \( j \)'s perspective, we can split arrivals from sensor \( i \) into two groups: informative and uninformative arrivals with probabilities \( \nc_{ij} \) and \( 1-\nc_{ij} \), respectively. The rate of arrivals from sensor \( i \) is \( \lambda_i \), so the rate of informative arrivals for process \( j \) from sensor \( i \) is \( \nc_{ij}\lambda_i \). Additionally, we can further split the informative arrivals based on whether they can preempt ongoing service. The rate of informative arrivals that can preempt ongoing service for process \( j \) from sensor \( i \) is \( \np_{i}\nc_{ij}\lambda_i \) and the rate of informative arrivals that can not preempt ongoing service for process \( j \) from sensor \( i \) is \( (1-\np_{i})\nc_{ij}\lambda_i \). Since all these arrivals are Poisson, we can merge them into a single process. The total arrival rate of informative packets that can preempt ongoing service for process \( j \) is given by

\begin{equation}
\Tilde{\lambda}_j = \sum_{i=1}^{N} \np_{i}\nc_{ij}\lambda_i
\end{equation}

Similarly, the total arrival rate of informative packets that can not preempt ongoing service for process \( j \) is

\begin{equation}
\Tilde{\lambda}_j = \sum_{i=1}^{N} (1-\np_{i})\nc_{ij}\lambda_i
\end{equation}

We can express these rates in vector form as follows:

\begin{equation}
\boldsymbol{\Tilde{\lambda}}^T = \begin{bmatrix}
\Tilde{\lambda}_{1} & \Tilde{\lambda}_{2} & \dots & \Tilde{\lambda}_{M}
\end{bmatrix} = (\boldsymbol{\lambda}^T \odot \bfp^T) \bfc,
\end{equation}
\begin{equation}
\boldsymbol{\dot{\lambda}}^T = \begin{bmatrix}
\dot{\lambda}_{1} & \dot{\lambda}_{2} & \dots & \dot{\lambda}_{M}
\end{bmatrix} = (\boldsymbol{\lambda}^T \odot (1-\bfp^T)) \bfc,
\end{equation}


The importance of the packet is whether it has information of process $j$ so  we can say that The system with $N$ sensors and arrival rates $\boldsymbol{\lambda}$ shown in Figure \ref{fig:system_model} equivalents to the system with two sources as shown in Figure \ref{fig:equiv_model} from process $j$'s perspective.


\section{}\label{spv-appendix}


We adopt the stochastic hybrid system (SHS) model as defined in \cite{yates2019}, with a key distinction: our model incorporates probabilistic preemption. The system dynamics are depicted in Figure \ref{fig:equiv_model} so we can analyze the AoI for any process $i$ and generalize it. First, the discrete state is denoted as $q(t) = q \in Q = \{0, 1, 2\}$, where $q = 0$ represents an idle server, and $q \in \{1, 2\}$ signifies that an update packet is currently being serviced. The continuous state is described as $x(t) = [x_0(t), x_1(t)]$, where $x_0(t)$ represents the current age of the process, and $x_1(t)$ captures the potential age if the packet in service is successfully delivered. Notably, $x_1(t)$ is irrelevant in state $0$ since no packet is in service. In state $1$, $x_1(t)$ corresponds to the age of the informative update being serviced. Conversely, in state $2$, where an uninformative update is in service, the completion of this update does not affect the process age, rendering $x_1(t)$ irrelevant in this state as well.

\begin{table}[h]
\centering
\caption{Table of Transitions for the Markov Chain in Figure \ref{fig:shs}.}
\begin{tabular}{c c c c c c}
\toprule
$l$ & $q_l \rightarrow q'_l$ & $\lambda^{(l)}$ & $\mathbf{xA}_l$ & $\mathbf{A}_l$ & $\mathbf{v}_{q_l}\mathbf{A}_l$ \\
\midrule
1 & $0 \rightarrow 1$ & $\Tilde{\lambda}_{1}+\dot{\lambda}_{1}$ & $\begin{bmatrix} x_0 & 0 \end{bmatrix}$ & \small $\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ \normalsize & $\begin{bmatrix} v_{00} & 0 \end{bmatrix}$ \\
2 & $0 \rightarrow 2$ & $\lambda_{C}-\Tilde{\lambda}_{1}-\dot{\lambda}_{1}$ & $\begin{bmatrix} x_0 & 0 \end{bmatrix}$ & \small $\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ \normalsize & $\begin{bmatrix} v_{00} & 0 \end{bmatrix}$ \\
3 & $1 \rightarrow 0$ & $\mu$        & $\begin{bmatrix} x_1 & 0 \end{bmatrix}$ & \small$\begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}$ \normalsize & $\begin{bmatrix} v_{11} & 0 \end{bmatrix}$ \\
4 & $1 \rightarrow 1$ & $\Tilde{\lambda}_{1}
$  & $\begin{bmatrix} x_0 & 0 \end{bmatrix}$ & \small$\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$\normalsize & $\begin{bmatrix} v_{10} & 0 \end{bmatrix}$ \\
5 & $1 \rightarrow 2$ & $\Tilde{\lambda}_{C}-\Tilde{\lambda}_{1}$  & $\begin{bmatrix} x_0 & 0 \end{bmatrix}$ & \small$\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ \normalsize & $\begin{bmatrix} v_{10} & 0 \end{bmatrix}$ \\
6 & $2 \rightarrow 0$ & $\mu$        & $\begin{bmatrix} x_0 & 0 \end{bmatrix}$ & \small$\begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}$ \normalsize & $\begin{bmatrix} v_{20} & 0 \end{bmatrix}$ \\
7 & $2 \rightarrow 1$ & $\Tilde{\lambda}_{1}$  & $\begin{bmatrix} x_0 & 0 \end{bmatrix}$ & \small$\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ \normalsize & $\begin{bmatrix} v_{20} & 0 \end{bmatrix}$ \\
\bottomrule
\end{tabular}
\label{shs_table}
\end{table}


\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/shs.png}
    \caption{The Markov chain for updates.}
    \label{fig:shs}
\end{figure}

A Markov chain representing the discrete state $q(t)$ is depicted in Figure~\ref{fig:shs}. The corresponding transitions of the SHS at state $q_l$ are detailed in Table~\ref{shs_table}. In the figure, a directed edge $l$ from node $q$ to node $q'$ indicates that transitions from state $q$ to state $q'$ occur at an exponential rate $\lambda^{(l)}$, as specified in the table. 





%\suresh{Incomplete.}

We first show that the stationary probability vector $\pi$ satisfies $
\mathbf{\pi D} = \mathbf{\pi Q} \quad \text{with}$ 
\begin{align}
\quad
\mathbf{D} = \text{diag}[\lambda_{C}, \mu + \Tilde{\lambda}_{C}, \mu + \Tilde{\lambda}_{1}], \quad  \\ \mathbf{Q} = 
\begin{bmatrix}
0 & \Tilde{\lambda}_{1}+\dot{\lambda}_{1} & \lambda_{C}-\Tilde{\lambda}_{1}-\dot{\lambda}_{1} \\
\mu & \Tilde{\lambda}_{1} & \Tilde{\lambda}_{C}-\Tilde{\lambda}_{1} \\
\mu & \Tilde{\lambda}_{1} & 0
\end{bmatrix}.
\end{align}
Applying $\sum_{i=0}^{2} \pi_i = 1$, the stationary probabilities are 
\begin{equation}
\pi_0 = \frac{\mu}{(\lambda_C + \mu)}, \label{pi0}
\end{equation}
\begin{equation}
\pi_1 = \frac{\lambda_C\Tilde{\lambda}_{1} + \dot{\lambda}_{1}\mu + \Tilde{\lambda}_{1}\mu}{(\lambda_C + \mu)(\Tilde{\lambda}_{C} + \mu)}, \label{pi1}
\end{equation}
\begin{equation}
\pi_2 = \frac{\Tilde{\lambda}_{C}\lambda_C + \lambda_C\mu -\lambda_C\Tilde{\lambda}_{1}  - \dot{\lambda}_{1}\mu - \Tilde{\lambda}_{1}\mu}{(\lambda_C + \mu)(\Tilde{\lambda}_{C} + \mu)} . \label{pi2}
\end{equation}

\section{}\label{aoi-appendix}





Given the SHS model and $\pi$ in Appendix \ref{spv-appendix}, we can evaluate $\bar{v}$ to find the AoI. Let 
\begin{equation}
\mathbf{\bar{v}} = [\mathbf{\bar{v}_0} \ \mathbf{\bar{v}_1} \ \mathbf{\bar{v}_2}] = [\bar{v}_{00} \ \bar{v}_{01} \ \bar{v}_{10} \ \bar{v}_{11} \ \bar{v}_{20} \ \bar{v}_{21}].   
\end{equation}
It follows that
\begin{equation}
\mathbf{\bar{v}D} = \mathbf{\pi B} +  \mathbf{\bar{v}R},
\end{equation}
where 
\begin{equation}
\mathbf{D} = \text{diag}[\lambda_C, \lambda_C, \mu + \Tilde{\lambda}_{C}, \mu + \Tilde{\lambda}_{C}, \mu + \Tilde{\lambda}_{1}, \mu + \Tilde{\lambda}_{1}],
\end{equation}
\begin{equation}
\mathbf{B} =
\begin{bmatrix}
1 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 1 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0
\end{bmatrix},
\end{equation}
and
\begin{equation}
\mathbf{R} = 
\begin{bmatrix}
0 & 0 & \Tilde{\lambda}_{1}+\dot{\lambda}_{1}  & 0 & \lambda_{C}-\Tilde{\lambda}_{1}-\dot{\lambda}_{1} & 0 \\
0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & \Tilde{\lambda}_{1} & 0 & \Tilde{\lambda}_{C}-\Tilde{\lambda}_{1} & 0 \\
\mu & 0 & 0 & 0 & 0 & 0 \\
\mu & 0 & \Tilde{\lambda}_{1} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}.
\end{equation}

Then, we obtain $\bar{v}_{01}=\bar{v}_{21} = 0 $ and 
\begin{align}
&
\label{pi_v}
\begin{bmatrix}
\bar{\pi}_0 & \bar{\pi}_1 & \bar{\pi}_1 & \bar{\pi}_2
\end{bmatrix}
= \\ \nonumber \hat{\mathbf{v}}&
\begin{bmatrix}
\lambda_{C} & -\Tilde{\lambda}_{1}-\dot{\lambda}_{1} & 0 & \Tilde{\lambda}_{1}+\dot{\lambda}_{1}-\lambda_{C} \\
0 & \mu + \Tilde{\lambda}_{C}-\Tilde{\lambda}_{1} & 0 & \Tilde{\lambda}_{1} - \Tilde{\lambda}_{C} \\
-\mu & 0 & \mu + \Tilde{\lambda}_{C} & 0 \\
-\mu & -\Tilde{\lambda}_{1} & 0 & \mu + \Tilde{\lambda}_{1}
\end{bmatrix}, \\
\text{where } 
\hat{\mathbf{v}} &= 
\begin{bmatrix}
\bar{v}_{00} & \bar{v}_{10} & \bar{v}_{11} & \bar{v}_{20}
\end{bmatrix}. \nonumber
\end{align}

After solving eq. (\ref{pi_v}) using eqs. (\ref{pi0}), (\ref{pi1}), and (\ref{pi2}), we determine $\mathbf{\bar{v}}$. Later, we find the average age of information using the formula for a single process $j$ $\Delta_j = \sum_{q=0}^2 \bar{v}_{10}$ as follows:

%\suresh{Is this what you defined as $\Delta_{\rm sum}$ earlier?}

\footnotesize
\begin{align}
\Delta_j = \frac{\lambda_{C}^{2} \tilde{\lambda}_C + \lambda_{C}^{2} \mu + \lambda_{C} \dot{\lambda}_1 \mu + 2 \lambda_{C} \tilde{\lambda}_C \mu + 2 \lambda_{C} \mu^{2} + \tilde{\lambda}_C \mu^{2} + \mu^{3}}{\mu \left(\lambda_{C}^{2} \tilde{\lambda}_1 + \lambda_{C} \dot{\lambda}_1 \mu + 2 \lambda_{C} \tilde{\lambda}_1 \mu + \dot{\lambda}_1 \mu^{2} + \tilde{\lambda}_1 \mu^{2}\right)}
\end{align}
\normalsize

\section{}\label{iteration-appendix}


In this section, we discuss the upper bound on the number of iterations required by the outer space accelerating branch-and-bound algorithm to achieve a global $\epsilon_0$-optimal solution. According to Theorem 5 in \cite{JIAO2022112701}, for any given positive error $\epsilon_0 \in (0, 1)$, the algorithm converges to the desired solution in at most
\begin{equation}
p \cdot \left\lceil \log_2 \frac{p\tau \delta(\Omega)}{\epsilon_0} \right\rceil 
\end{equation}
iterations.

Here, the symbols used in the theorem are defined as follows:

\begin{itemize}
    \item $\Omega \subseteq \mathbf{R}^p$ is a compact hyper-subrectangle, and $\delta(\Omega)$ is defined as:
    \begin{equation}
    \delta(\Omega) = \max_{i=1,2,\dots,p} \{ \bar{U}_i - \bar{L}_i \},    
    \end{equation}
    where $\bar{U}_i$ and $\bar{L}_i$ represent the upper and lower bounds of the $i$-th dimension of the rectangle $\Omega$.

    \item $\tau$ is defined as:
    \begin{equation}\label{tau_eq}
    \tau = \max_{i=1,\dots,p} \frac{4 \max\{|\bar{l}_i|, |\bar{u}_i|\}}{\min\{\bar{L}_i, \bar{U}_i, \bar{L}_i^2, \bar{U}_i^2\}},
    \end{equation}
    where the terms are determined as follows:
    \begin{align}
        \bar{l}_i &= \min_{y \in \Theta} n_i(y), \quad \bar{u}_i = \max_{y \in \Theta} n_i(y), \nonumber \\
        \bar{L}_i &= \min_{y \in \Theta} d_i(y), \quad \bar{U}_i = \max_{y \in \Theta} d_i(y).
    \end{align}

    \item The terms $n_i(y)$ and $d_i(y)$ come from the problem defined as:
    \begin{align}
    \quad \min f(y) = \sum_{i=1}^p \frac{n_i(y)}{d_i(y)}, \quad \nonumber \\ \text{s.t.} \; y \in \Theta = \{y \in \mathbf{R}^n \mid Ay \leq b \}. 
    \end{align}
    \end{itemize}


We can reformulate our problem to determine the upper bound using these definitions. The variable in our problem is $\bfp$, and the objective is specified in (\ref{objective_func}). There are $M$ different linear fractions in the objective. The numerators of these fractions increase as any element of $\bfp$ increases. Consequently, we obtain $\bar{l}_i$ when $\bfp = 0$ and $\bar{u}_i$ when $\bfp = 1$ as follows:
\begin{align}
            \bar{l}_i &= \mu(\mu + \lambda_C)^2 + \sum_{i=1}^{N} \lambda_{i}\mu\lambda_C \nc_{ij},\quad \bar{u}_i = (\mu + \lambda_{C})^3
\end{align}

 Similarly, the denominators of these fractions decrease as any element of $\bfp$ increases, leading to $\bar{L}_i$ when $\bfp = 0$ and $\bar{U}_i$ when $\bfp = 1$.
 \begin{align}
            \bar{L}_i &=  (\mu + \lambda_C) \mu^2 \sum_{i=1}^{N} \nc_{ij} \lambda_{i}, \quad \bar{U}_i =  (\mu + \lambda_C)^2 \mu \sum_{i=1}^{N} \nc_{ij} \lambda_{i}.
\end{align}

After that, $\delta(\Omega)$ becomes: 

\begin{align}
        \delta(\Omega) = \max_{i=1,2,\dots,M} \{(\mu + \lambda_C)\lambda_C \mu \sum_{i=1}^{N} \nc_{ij} \lambda_{i}\} \leq (\mu + \lambda_C)\lambda_C^2 \mu , 
\end{align}

Last, we find $\tau$. In our problem, all parameters and variables are positive, so both the nominators and the denominators are positive, which can help us simplify eq. (\ref{tau_eq}) and obtain $\tau$ as follows:

    \begin{align}
    \tau = \max_{i=1,\dots,M} \frac{4 \bar{u}_i}{\bar{L}_i^2} = \frac{4 (\mu + \lambda_{C})}{\mu^4 \hat{\lambda}_{\min}^2},\\ \nonumber
    \text{where } \hat{\lambda}_{\min} = \min(\boldsymbol{\lambda}^T \bfc)
    \end{align}

Putting all together, for any given positive error $\epsilon_0 \in (0, 1)$, the outer space accelerating branch-and-bound algorithm can seek out a global $\epsilon_0$-optimum solution in at most 
\begin{equation}
M \cdot \left\lceil \log_2 \frac{4M (\mu+\lambda_C)^2\lambda_C^2}{\epsilon_0\mu^3\hat{\lambda}_{\min}^{2}} \right\rceil
\end{equation}
iterations as shown in Theorem \ref{Theo2}.