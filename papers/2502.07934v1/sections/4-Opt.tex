\section{Average Age Optimization}\label{aoi-opt}


Preemption is a possible strategy for minimizing AoI in networked systems, but its effectiveness depends on the specific context, such as sensor arrival rates and the correlation between updates. In multi-sensor systems with correlated processes, the goal is to identify when and how preemption can provide benefits. Thus, our objective is to find optimal preemption probabilities that minimize AoI for a given setup.


%\ali{Let us reduce this; we speak too much. Just say, the goal is to find out in what scenario is preemption going to help in this multi-sensors correlated systems, etc.}
%\suresh{I actually like this rationale but would suggest making it more concise. And it sounds like this is motivation and belongs in the Intro.}

To that end, the objective is to minimize the sum of AoI, $\Delta_{\text{sum}}$, as follows:
\begin{align}
\min_{\bfp \in [0,1]^{N}} \sum_{j=1}^{M}\Delta_j = \Delta_{\text{sum}}.
\end{align}

%\suresh{Pay attention to notation, you have an $NxM$ matrix for $R$ here. Also, did you define $\Delta_{\text{sum}}$}

%\ali{align it better}
The objective function can be reformulated as:
\begin{align}\label{objective_func}
    \min_{\bfp \in [0,1]^{N}}\Delta_{\text{sum}} =& 
    \sum_{j=1}^{M} \frac{\mathbf{g_j}^T \bfp + g_{j0}}{\mathbf{f_j}^T \bfp + f_{j0}} = \sum_{j=1}^{M} \frac{G_j(\bfp)}{F_j(\bfp)}
    \\ \nonumber
    &\text{subject to } 0 < \bfp < 1,
\end{align}
where
\vspace{-15pt}
\begin{align}
    g_{j0} &= \mu(\mu + \lambda_C)^2 + \sum_{i=1}^{N} \lambda_{i}\mu\lambda_C \nc_{ij}, \\
    g_{ji} &= ((\mu + \lambda_C)^2 - \mu\lambda_C \nc_{ij})\lambda_{i}, \\
    f_{j0} &= (\mu + \lambda_C) \mu^2 \sum_{i=1}^{N} \nc_{ij} \lambda_{i}, \\
    f_{ji} &= \lambda_C \mu (\mu + \lambda_C) \nc_{ij} \lambda_{i}.
\end{align} 

This is a classical sum of linear ratios problem studied in the literature. The problem is well known for its computational challenges \cite{schaible2003fractional}, and it is NP-hard, in general \cite{freund2001solving}. However, given the special case of our problem, as we will show afterward, we can achieve a global optimum efficiently using a branch-and-bound algorithm with a finite number of iterations \cite{JIAO2022112701}.

%\ali{say that it is NP-hard in general. also, don't say progress has been made, etc. Just say, however, given the special case of our problem, as we will show afterwards, we can achieve a global optimum in an efficient way.}

To address the global minimization of the objective function in (\ref{objective_func}), an essential step is the reformulation of the original problem into an equivalent problem (EP). This reformulation allows for more tractable computation while preserving the global properties of the original problem.

For clarity, consider the following notations: for each \( i = 1, 2, \dots, M \), define
\[
\begin{aligned}
    \bar{l}_i &= \min_{\bfp \in [0,1]^{N}} F_i(\bfp), \quad 
    \bar{u}_i = \max_{\bfp \in [0,1]^{N}} F_i(\bfp), \\
    \bar{L}_i &= \min_{\bfp \in [0,1]^{N}} G_i(\bfp), \quad 
    \bar{U}_i = \max_{\bfp \in [0,1]^{N}} G_i(\bfp).
\end{aligned}
\]

% \ali{can you explain what linear problems are there?} 
These bounds can be determined by solving \( 4 \times M \) linear programs, where each program involves optimizing \( F_i(\bfp) \) and \( G_i(\bfp) \) over the feasible region \( \bfp \in [0,1]^{N} \), separately minimizing and maximizing each function to yield the values of \( \bar{l}_i \), \( \bar{u}_i \), \( \bar{L}_i \), and \( \bar{U}_i \). Using these results, we define the feasible region $
\Omega = \{ (\beta, \alpha) \in \mathbf{R}^{2M} \mid \bar{l}_i \leq \beta_i \leq \bar{u}_i, \; \bar{L}_i \leq \alpha_i \leq \bar{U}_i, \; i = 1, 2, \dots, M \}.
$
%\suresh{Wouldn't it be more natural to call it $(\alpha, \beta)$?} \egemen{I followed the notation in the cited paper. Do you want me to change?}\suresh{No; didn't know that was how it was done in that paper.}
The problem can then be reformulated as the following EP:

\[
\text{EP}:
\begin{cases}
    \min \; h(\beta, \alpha) = \sum_{i=1}^M \frac{\beta_i}{\alpha_i}, \\
    \text{s.t. } \quad F_i(\bfp) - \beta_i = 0, \; i = 1, 2, \dots, M, \\
    \quad G_i(\bfp) - \alpha_i = 0, \; i = 1, 2, \dots, M, \\
    \quad \bfp \in [0,1]^N, \; (\beta, \alpha) \in \Omega.
\end{cases}
\]

The feasible region of the EP, denoted as $
Z = \{ F_i(\bfp) - \beta_i = 0, \; G_i(\bfp) - \alpha_i = 0, \; i = 1, 2, \dots, M, \; \bfp \in [0,1]^N, \; (\beta, \alpha) \in \Omega \},
$
is a bounded compact set. Importantly, \( Z \neq \emptyset \) holds if and only if \( [0,1]^N \neq \emptyset \).
%
If a solution is globally optimal for the EP, it can be converted into a globally optimal solution for the original problem, and vice versa, making the EP sufficient for addressing the original optimization problem \cite{JIAO2022112701}.

Moreover, reformulating the problem as the EP allows us to analyze its computational complexity. Specifically, leveraging the EP's structure, we derive an upper bound on the iterations required for a branch-and-bound algorithm to find a global solution. The upper bound on the number of iterations required for a branch-and-bound algorithm to solve the EP is established in Theorem~\ref{Theo2}, as follows.

%\ali{This paragraph and the one afterward, it feels we are speaking too much; let us make things more compact with the major stuff that makes us appear that we have done quite some stuff here.} 

\begin{Theorem}\label{Theo2}
For any given positive error \( \epsilon_0 \in (0, 1) \), the outer space accelerating branch-and-bound algorithm can find a global \( \epsilon_0 \)-optimal solution in at most 
\begin{equation}
M \cdot \left\lceil \log_2 \frac{4M (\mu+\lambda_C)^2\lambda_C^2}{\epsilon_0\mu^3\hat{\lambda}_{\min}^{2}} \right\rceil
\end{equation}
iterations, where \( \hat{\lambda}_{\min} = \min(\boldsymbol{\lambda}^T \bfc) \) denotes the minimum element of \( \boldsymbol{\lambda}^T \bfc \) over the feasible set.
\end{Theorem}

\begin{proof} 
The proof uses the compact and bounded feasible region properties of the reformulated optimization problem to derive the computational complexity. Full details are provided in \ifthenelse{\boolean{withappendix}}
{Appendix~\ref{iteration-appendix}}
{Appendix D in \cite{technicalNote}}.
\end{proof}

This upper bound highlights the effect of parameters on the iteration count. Increasing $\mu$ reduces the number of required iterations because it also decreases the AoI value. Therefore, there is a need for a reduction in $\epsilon_0$ to achieve a similar level of precision. Likewise, smaller $\hat{\lambda}_{\min}$ and larger $\lambda_C$ lead to higher AoI values, necessitating more iterations to maintain the same error tolerance $\epsilon_0$ as expected. For an example case with $M = 10$, $\mu = 5$, $\lambda_C = 15$, and $\epsilon_0 = 0.01$, the upper bound requires at most 82 iterations, which is not excessively high. %\ali{You did not give a small example to showcase how fast the algorithm converge for typical values. Let us make our case stronger here}


% This problem, known as the sum of linear ratios problem, is generally challenging to solve \cite{schaible2003fractional}. However, it has been demonstrated that the globally optimal solution can be achieved in a finite number of steps, and various algorithms have been proposed, some of them providing upper bounds on the required number of iterations \cite{falk1992optimizing,shen2006global,gao2013global,JIAO2022112701}. 


% To globally minimize the problem in (\ref{objective_func}), the major work is to globally minimize an equivalent problem EP.

% Without losing generality, for each $i = 1, 2, \dots, M$, we let
% \[
% \begin{aligned}
%     \bar{l}_i &= \min_{\bfp \in [0,1]^{N}} F_i(\bfp), \quad 
%     \bar{u}_i = \max_{\bfp \in [0,1]^{N}} F_i(\bfp), \\
%     \bar{L}_i &= \min_{\bfp \in [0,1]^{N}} G_i(\bfp), \quad 
%     \bar{U}_i = \max_{\bfp \in [0,1]^{N}} G_i(\bfp).
% \end{aligned}
% \]
% Obviously, by solving $4 \times M$ linear programs, we can compute the values of $\bar{l}_i$, $\bar{u}_i$, $\bar{L}_i$, and $\bar{U}_i$. Let $
% \Omega = \{ (\beta, \alpha) \in \mathbf{R}^{2M} \mid \bar{l}_i \leq \beta_i \leq \bar{u}_i, \; \bar{L}_i \leq \alpha_i \leq \bar{U}_i, \; i = 1, 2, \dots, M \}.$
% Then, we convert our problem into the following equivalence problem:
% \[
% \text{EP}:
% \begin{cases}
%     \min \; h(\beta, \alpha) = \sum_{i=1}^p \frac{\beta_i}{\alpha_i}, \\
%     \text{s.t. } \quad F_i(\bfp) - \beta_i = 0, \; i = 1, 2, \dots, M, \\
%     \quad G_i(\bfp) - \alpha_i = 0, \; i = 1, 2, \dots, M, \\
%     \quad \bfp \in [0,1]^N, \; (\beta, \alpha) \in \Omega.
% \end{cases}
% \]

% Obviously, the feasible region $Z = \{ F_i(\bfp) - \beta_i = 0, \; G_i(\bfp) - \alpha_i = 0, \; i = 1, 2, \dots, M, \; \bfp \in [0,1]^N, \; (\beta, \alpha) \in \Omega \}$ of the EP is a bounded compact set, and $Z \neq \emptyset$ holds if and only if $[0,1]^N \neq \emptyset$ holds.



% Theorem \ref{Theo2} below provides an upper bound on the number of iterations required for a branch-and-bound algorithm from the literature \cite{JIAO2022112701}.

% \begin{Theorem}\label{Theo2}
% For any given positive error $\epsilon_0 \in (0, 1)$, the outer space accelerating branch-and-bound algorithm can seek out a global $\epsilon_0$-optimum solution in at most 
% \begin{equation}
% M \cdot \left\lceil \log_2 \frac{4M (\mu+\lambda_C)^2\lambda_C^2}{\epsilon_0\mu^3\hat{\lambda}_{\min}^{2}} \right\rceil
% \end{equation}

% iterations, where $\hat{\lambda}_{\min} = \min(\boldsymbol{\lambda}^T \bfc)$ denotes the minimum element of $\boldsymbol{\lambda}^T \bfc$ over the feasible set.
% \end{Theorem}
% \begin{proof} The details can be found in Appendix \ref{iteration-appendix}.
% \end{proof}
