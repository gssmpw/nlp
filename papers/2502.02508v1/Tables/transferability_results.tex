\begin{table*}[!t]
\centering
\footnotesize
\captionsetup{font=small}
\caption{\textbf{Results on Out-of-domain Benchmarks.} Trained only on math datasets, Satori-Qwen-7B exhibits strong transferability across diverse out-of-domain benchmarks and outperforms Qwen-2.5-Math-7B-Instruct by a large margin. Moreover, despite not being trained in other domains, Satori-Qwen-7B achieves performance comparable to or exceeding other small-scale general instruct models.}
\vspace{-0.5em}
\begin{tabular}{llccccccc}
\toprule[1.5pt]
\textbf{Scale}  & \textbf{Model} & \textbf{FOLIO} & \textbf{BGQA} & \textbf{CRUXEval} & \textbf{StrategyQA} & \textbf{TableBench} & \textbf{STEM} & \textbf{Avg.} \\ \midrule
\multirow{3}{*}{Large} & Llama-3.1-70B-Instruct   & 65.0 & 58.3 & 59.6 & 88.8 & 34.2 & 61.7 & 61.3 \\
                       & OpenMath2-Llama3.1-70B   & 68.5 & 68.7 & 35.1 & 95.6 & 46.8 & 15.1 & 55.0 \\
                       & QwQ-32B-Preview          & 84.2 & 71.1 & 65.2 & 88.2 & 51.5 & 71.3 & 71.9 \\ \midrule
\multirow{7}{*}{Small} & Llama-3.1-8b-Instruct    & 63.5 & 50.3 & 38.5 & 92.2 & 32.4 & 43.4 & 53.4 \\
                       & OpenMath2-Llama3.1-8B    & 57.1 & 49.0 & 11.1 & 84.4 & 34.2 & 10.9 & 41.1 \\
                       & NuminaMath-7B-CoT        & 53.2 & 44.6 & 28.0 & 77.8 & 29.1 & 11.3 & 40.7 \\
                       & Qwen-2.5-7B-Instruct     & 72.4 & 53.0 & 58.1 & 91.3 & 43.2 & 57.1 & 62.5 \\
                       & Qwen-2.5-Math-7B-Instruct & 68.9 & 51.3 & 28.0 & 85.3 & 36.2 & 45.2 & 52.5 \\ \cmidrule{2-9}
                       & \textbf{Satori-Qwen-7B}           & 71.4 & 61.8 & 42.5 & 86.3 & 43.4 & 56.7 & 60.4 \\ 
                       & \textbf{Satori-Qwen-7B (Round 2)} & 72.9 & 58.5 & 41.1 & 90.4 & 44.6 & 57.4 & 60.8 \\ \bottomrule[1.5pt]
\end{tabular}
\label{table:transfer-results}
\end{table*}