% \begin{table*}[!t]
%   \begin{center}
%   \footnotesize
%   \captionsetup{font=small}
%   \caption{\textbf{Main Results on Math Domain Benchmark.} Satori-Qwen-7B achieves SOTA performance across benchmark datasets, and outperform Qwen-2.5-Math-7B-Instruct model, which uses the same base model as Satori-Qwen-7B.}
%   \begin{tabular}{cccccccccc}
%     \toprule
%      & \textbf{Model} & \textbf{GSM8K}  & \textbf{MATH500}  &  \textbf{Olympiad} & \textbf{AMC2023} & \textbf{AIME2024} \\
%      \midrule
%      \textbf{Large-sized} & Llama-3.1-70B-Instruct & 94.1 & 66.2 & 29.4 & 42.5 & 13.3 \\
%      & GPT-4o  & /  & 60.3 & 43.3 & / & 9.3  \\
%      & o1-preview  & / & /  & / & 82.5 & 44.6  \\
%      & QwQ-32B-Preview & 95.5 	&86.4	 &	48.8 &	77.5 & 36.7 \\
%     \midrule
%     \textbf{Small-sized} & Deepseek-Math-7B-RL & 88.3 & 50.6 &	18.5 & 2.0 & 0.0\\
%     & NuminaMath-Deepseek-7B & 78.8	&  54.6 &	15.9 & 20.0 & 10.0 \\
%     & LLaMA-3.1-8B-Instruct  & 84.4		& 45.0	&	15.1 & 22.5 & 3.3  \\
%     & OpenMath2-Llama3.1-8B & 90.4  &	67.8	& 28.9	& 37.5 & 6.7 \\
%     & Qwen-2.5-7B-Instruct & 92.5 & 75.6  &	35.5 &	52.5 & 6.7 \\
%     & Qwen-2.5-Math-7B-Instruct & 95.7	 &	83.2	 & 37.5 & 62.5 & 16.7 \\
%     \coloredmidrule
%     \rowcolor{lightblue}
%     \textbf{Ours} & \textbf{Satori-Qwen-7B} & 93.1 & 84.8 & 41.2 &	67.5 & 20.0 \\
%     \coloredbottomrule
%   \end{tabular}
%   \label{table:main-results}
%   \end{center}
% \end{table*}

\begin{table*}[!t]
\centering
\footnotesize
\captionsetup{font=small}
\caption{\textbf{Results on Mathematic Benchmarks.} Satori-Qwen-7B achieves SOTA performance across five benchmarks, and outperforms Qwen-2.5-Math-7B-Instruct which uses the same base model Qwen-2.5-Math-7B. After round-2 training, Satori-Qwen-7B (Round 2) demonstrates even stronger performance on hard tasks.}
\vspace{-0.5em}
\begin{tabular}{llcccccc}
\toprule[1.5pt]
\textbf{Scale}         & \textbf{Model}  & \textbf{GSM8K}   & \textbf{MATH500}  & \textbf{OlympiadBench} & \textbf{AMC2023} & \textbf{AIME2024} & \textbf{Avg.} \\ \midrule
\multirow{5}{*}{Large} & GPT-4o             & /       & 60.3                          & 43.3                   & /                & 9.3               & / \\
                       & o1-preview          & /      & 85.5                           & /                      & 82.5             & 44.6              & / \\
                       & Llama-3.1-70B-Instruct  & 94.1  & 68.0                        & 29.4                  & 42.5             & 13.3             & 49.5 \\
                       & OpenMath2-Llama3.1-70B   & 94.1 & 71.8                       & 30.1                  & 45.0               & 13.3             & 50.9 \\
                       & QwQ-32B-Preview       & 95.5    & 90.6                      & 61.2                   & 77.5             & 50.0               & 75.0 \\ \midrule
\multirow{7}{*}{Small} & Llama-3.1-8b-Instruct  & 84.4   & 51.9                      & 15.1                  & 22.5             & 3.3              & 35.4 \\
                      & OpenMath2-Llama3.1-8B   & 90.5  & 67.8                       & 28.9                  & 37.5             & 6.7              & 46.3 \\
                       & NuminaMath-7B-CoT      & 78.9   & 54.6                       & 15.9                  & 20.0               & 10.0                & 35.9 \\
                       & Qwen-2.5-7B-Instruct  & 91.6    & 75.5                      & 35.5                  & 52.5             & 6.7              & 52.4 \\
                       & Qwen-2.5-Math-7B-Instruct & 95.2 & 83.6                      & 41.6                  & 62.5             & 16.7             & 59.9 \\ \cmidrule{2-8} 
                       & \textbf{Satori-Qwen-7B}     & 93.2        & 85.6                 & 46.6                  & 67.5             & 20.0        & 62.6 \\
                       & \textbf{Satori-Qwen-7B (Round 2)}     & 93.9       & 83.6                   & 48.5                  & 72.5             & 23.3  & 64.4 \\ \bottomrule[1.5pt]
\end{tabular}

\label{table:main-results}
\end{table*}