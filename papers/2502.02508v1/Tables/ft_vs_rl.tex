\begin{table}[h]
  \begin{center}
  \scriptsize
  \captionsetup{font=small}
  \caption{\textbf{Large-scale FT V.S. Large-scale RL} Satori-Qwen (10K FT data + 300K RL data) outperforms same base model Qwen-2.5-Math-7B trained with 300K FT data (w/o RL) across all math and out-of-domain benchmarks.}
  \setlength{\tabcolsep}{1.15pt}
  \vspace{-0.5em}
\begin{tabular}{lccccc}
\toprule
\textbf{(In-domain)}   & \textbf{GSM8K}   & \textbf{MATH500} & \textbf{Olym.} & \textbf{AMC2023} & \textbf{AIME2024} \\ \midrule
Qwen-2.5-Math-7B-Instruct & 95.2 & 83.6                     & 41.6                  & 62.5             & 16.7                 \\
Satori-Qwen-7B-FT (300K)     & 92.3 & 78.2                       & 40.9           & 65.0               & 16.7              \\
\textbf{Satori-Qwen-7B}         & 93.2        & 85.6                     & 46.6           & 67.5             & 20.0                \\ \midrule
\textbf{(Out-of-domain)}  & \textbf{BGQA}    & \textbf{CRUX}  & \textbf{STGQA} & \textbf{TableBench}   & \textbf{STEM}     \\ \midrule
Qwen-2.5-Math-7B-Instruct & 51.3             & 28.0             & 85.3           & 36.3             & 45.2              \\
Satori-Qwen-7B-FT (300K)     & 50.5             & 29.5           & 74.0             & 35.0               & 47.8              \\
\textbf{Satori-Qwen-7B}               & 61.8             & 42.5           & 86.3           & 43.4             & 56.7              \\ \bottomrule
\end{tabular}
  \label{table:ablation-ft-rl}
  \end{center}
\end{table}