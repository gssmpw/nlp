\section{Method} \label{sec:method} 

We start this section by introducing the formulation of reasoning and how reasoning can be formulated as a sequential decision-making problem.
\textbf{Goal:}
We want to train LLMs to solve problems by reasoning through multiple steps rather than directly predicting the final answer. Given a problem statement \(\boldsymbol{x}\), the model generates a sequence of reasoning steps \(\{\boldsymbol{y}_1, \boldsymbol{y}_2, \dots, \boldsymbol{y}_L\}\), where \(\boldsymbol{y}_L\) provides the final answer. However, not all intermediate steps are helpful—repeating errors does not improve accuracy. Effective reasoning requires verifying correctness, identifying mistakes, and considering alternative solutions.
For instance, given \(\boldsymbol{x} = ``1+1=?"\), the model might initially output \(\boldsymbol{y}_1 = 3\), then recognize the mistake with \(\boldsymbol{y}_2\) (e.g., \textit{``Wait, let me verify..."}), before correcting it to \(\boldsymbol{y}_3 = 2\). 

\textbf{Chain-of-Action-Thought reasoning (COAT).} The key challenge is enabling the model to determine when to reflect, continue, or explore alternatives without external intervention. To enable this, we introduce special \textit{meta-action} tokens that guide the model’s reasoning process beyond standard text generation. These tokens serve as hint for the model to determine when to reassess its reasoning before proceeding.
\begin{itemize}[leftmargin=*]
    \item \textbf{Continue Reasoning} (\texttt{<|continue|>}): Encourages the model to build upon its current reasoning trajectory by generating the next intermediate step.
    \item \textbf{Reflect} (\texttt{<|reflect|>}): Prompts the model to pause and verify the correctness of prior reasoning steps.
    \item \textbf{Explore Alternative Solution} (\texttt{<|explore|>}): Signals the model to identify critical flaws in its reasoning and explore a new solution.
\end{itemize}
Each reasoning step \(\boldsymbol{y}_l\) is a sequence of tokens, with the starting token potentially being one of the designated meta-action tokens. We refer to this formulation as Chain-of-Action-Thought reasoning (COAT). In particular, typical Chain-of-Thought reasoning (CoT)~\cite{wei2022chain} can be viewed as a special case of COAT, where each reasoning step in CoT is restricted to continuation, without explicitly incorporating other types of meta-actions.


\textbf{Learning to Reason via RL.}  
We formulate reasoning as a sequential decision-making problem, where reasoning is a process of constructing and refining an answer step by step. Specifically, the model $\pi_\theta$ starts with an input context $\boldsymbol{x}$ (initial state $z_0$), generates a reasoning step $\boldsymbol{y}_l$ (action), updates the context by appending $\boldsymbol{y}_l$ (next state $z_{l+1} = z_{l} \oplus \boldsymbol{y}_l$, where $\oplus$ denotes string concatenation), and repeats this process until it produces a final answer $\boldsymbol{y}_L$. The reasoning terminates when the model signals completion (e.g., omitting EOS token). The simplest reward function can be $\mathbb{I}\{\boldsymbol{y}_L = \boldsymbol{y}^*\}$, evaluates whether the final answer $\boldsymbol{y}_L$ matches the ground truth $\boldsymbol{y}^*$. With this formulation, we could train the model to reason using RL, aiming to generate reasoning steps that maximize the expected reward. However, applying RL to reasoning presents two key challenges: 
\begin{enumerate}
\vspace{-0.5em}
    \item \textbf{Unawareness of meta-action tokens:} The model doesn't understand the purpose of special tokens and fails to recognize that encountering special meta-action tokens may require reflection or proposing alternatives.
    \item \textbf{Long horizon and sparse rewards:} Reasoning requires long-term decision-making with rewards only at the end, which hinders learning effectiveness \citep{bellemare2016unifying}. The model must take many correct reasoning steps before receiving rewards, and failures force it to restart from the initial state (i.e., the problem statement). This makes learning difficult because training data associated with rewards is scarce, yet rewards are essential for driving RL progress.
    \vspace{-0.5em}
\end{enumerate}


\textbf{Overview of Proposed Method.} 
To address the model’s initial unawareness of meta-action tokens, we introduce a warm-up “format-tuning” stage: we fine-tune a pre-trained LLM on a small dataset featuring a few demonstrated reasoning trajectories (Section~\ref{subsec:format}). This step familiarizes the model with using and reacting to meta-action tokens. Second, to tackle the challenges of long horizons and sparse rewards, we propose a ``restart and explore'' (RAE) strategy, inspired by Go-explore \citep{ecoffet2019go}. Here, the model restarts from intermediate steps, including those points where previous reasoning attempts failed, allowing it to focus on correcting errors rather than starting from scratch. We also add exploration bonuses to encourage deeper reflection, further increasing opportunities for the model to arrive at correct answers (Section~\ref{subsec:rl}).


\subsection{Format Tuning Through Imitation Learning} \label{subsec:format}
Training a base LLM $\pi_{\theta}$ to perform COAT reasoning presents a significant challenge: LLMs are typically not pre-trained on COAT reasoning data that incorporates trials and errors, necessitating a post-training stage to inject this capability. To address this, we introduce format tuning (FT), a method designed to train LLMs to emulate expert COAT trajectories through imitation learning. Imitation learning techniques~\cite{hussein2017imitation} are widely used in the robotics domain, where agents are trained using demonstration trajectories provided by human experts~\cite{ross2010efficient, ross2011reduction, ho2016generative}. However, generating high-quality demonstration trajectories for LLMs is prohibitively expensive for complex tasks. To efficiently construct a demonstration trajectory dataset $\Dc_{\text{syn}}=\{(\boldsymbol{x}^{(i)},\boldsymbol{\Tilde{y}}^{(i)})\}_{i=1}^N$, we propose a multi-agent data synthesis framework that leverages three LLMs:
\begin{itemize}[left=0.05cm]
    \item \textbf{Generator:} Given an input problem, a generator $\pi_g$ generates multiple reasoning paths for a given input problem using classical CoT techniques.
    \item \textbf{Critic:} A critic $\pi_c$ evaluates the correctness of the reasoning paths generated by the generator, providing feedback to refine the reasoning and address suboptimal steps.
    \item \textbf{Reward Model:} Additionally, a reward model $\pi_r$ assigns scores to the refined reasoning paths and selects the most effective path as the final demonstration trajectory. 
\end{itemize}
These three models collaborate to construct high-quality demonstration trajectories (details on the trajectory synthesis are provided in Appendix \ref{sec:synthesis}). For this work, we adopt the simplest imitation learning approach, behavior cloning, which utilizes supervised fine-tuning to train the LLM policy on the expert COAT demonstration trajectories $\Dc_{\text{syn}}$. Notably, we observe that even a small number (10K) of COAT demonstration trajectories is sufficient for $\pi_{\theta}$ to effectively follow the COAT reasoning format.



\subsection{Self-improvement via Reinforcement Learning} \label{subsec:rl}

After format tuning, the LLM policy \(\pi_\theta\) adopts the COAT reasoning style but struggles to generalize, particularly in using meta-actions for self-reflection. This limitation arises from the scarcity of demonstrations during format tuning. While collecting more demonstrations could help, it is costly and time-consuming. Instead, we explore whether the model can \textit{self-improve} its reasoning via RL.

We start with the format-tuned LLM and train it using PPO~\citep{schulman2017proximal} algorithm, a widely used RL method. In addition to training on problems \( \boldsymbol{x} \) from the dataset \( \mathcal{D} \), we also train the model \( \pi_\theta \) to begin reasoning from partial trajectories generated by the format-tuned LLM. Since reasoning errors typically arise from minor mistakes rather than fundamental flaws, re-exploring from the start is inefficient. Instead, we allow the model to restart from intermediate steps to correct errors and finally achieve correct answers. Inspired by Go-Explore \citep{ecoffet2019go}, we introduce the \textit{Restart and Explore (RAE)} strategy.

\scalebox{0.9}
{
\begin{minipage}{1.1\linewidth}
\begin{algorithm}[H]
\SetAlgoLined

    \textbf{input}{ Dataset $\Dc=\{(\boldsymbol{x}^{(i)},{\boldsymbol{y^*}}^{(i)})\}_{i=1}^n$; LLM policy $\pi_\theta$ after format tuning; maximum back-track steps $T$}
\\\\
$\triangleright$ Initialize $\Dc^+_{\text{restart}} \leftarrow \emptyset$; Initialize $\Dc^-_{\text{restart}} \leftarrow \emptyset$

\For{$i = 1,2, \ldots, n$}
{   
   $\triangleright$ Given input problem $\boldsymbol{x}^{(i)}$, sample $\pi_\theta$ and collect multiple initial trajectories. 

   $\triangleright$ Randomly select one correct trajectory $\boldsymbol{\Tilde{y}}^{+}$ and one incorrect trajectory $\boldsymbol{\Tilde{y}}^{-}$.

   $\triangleright$ Randomly backtrack last $t\leq T$ actions from $\boldsymbol{\Tilde{y}}^{+}$ and $\boldsymbol{\Tilde{y}}^{-}$.

   $\triangleright$ Obtain intermediate states at time-step $L-t$,  $z^{+}_{L-t}= [\boldsymbol{x}^{(i)}, \boldsymbol{\Tilde{y}}^{+}_{1}, \boldsymbol{\Tilde{y}}^{+}_{2}, \ldots, \boldsymbol{\Tilde{y}}^{+}_{L-t}]$; $z^{-}_{L-t}=[\boldsymbol{x}^{(i)}, \boldsymbol{\Tilde{y}}^{-}_{1}, \boldsymbol{\Tilde{y}}^{-}_{2}, \ldots, \boldsymbol{\Tilde{y}}^{-}_{L-t}]$.

    $\triangleright$ Add ``reflect'' special token to trigger self-reflection action, $z^{+}_{L-t}= [\boldsymbol{x}^{(i)}, \boldsymbol{\Tilde{y}}^{+}_{1}, \boldsymbol{\Tilde{y}}^{+}_{2}, \ldots, \boldsymbol{\Tilde{y}}^{+}_{L-t}, \texttt{<|reflect|>}]$; $z^{-}_{L-t}=[\boldsymbol{x}^{(i)}, \boldsymbol{\Tilde{y}}^{-}_{1}, \boldsymbol{\Tilde{y}}^{-}_{2}, \ldots, \boldsymbol{\Tilde{y}}^{-}_{L-t}, \texttt{<|reflect|>}]$.

    $\triangleright$ Update $\Dc^+_{\text{restart}} \leftarrow \Dc^+_{\text{restart}} \cup z^{+}_{L-t}$; $\Dc^-_{\text{restart}} \leftarrow \Dc^-_{\text{restart}} \cup z^{-}_{L-t}$.
}
$\Dc_{\text{restart}} = \{\boldsymbol{x}^{(i)}\}_{i=1}^n \cup \Dc^+_{\text{restart}} \cup \Dc^-_{\text{restart}}.$
\\\\
    \textbf{output}{ Augmented initial states dataset $\Dc_{\text{restart}}$.}

 \caption{Restart and Explore (RAE)}
 \label{alg:buffer}
\end{algorithm}
\end{minipage}
}

\paragraph{Initial States.}
RAE trains the model to reason not only from the problem statement but also from intermediate steps sampled from past trajectories, both correct and incorrect. This enables deeper exploration without redundant recomputation. 
As detailed in Algorithm~\ref{alg:buffer}, given an input problem \( x \in \mathcal{D} \), the format-tuned LLM first generates multiple reasoning trajectories. We then randomly backtrack \( T \geq 0 \) steps and append a reflect token \texttt{<|reflect|>} to prompt the model to refine its reasoning. To encourage diverse exploration, correct and incorrect trajectories are stored separately in restart buffers (\(\mathcal{D}^+_{\text{restart}}\) and \(\mathcal{D}^-_{\text{restart}}\)). RL training then optimizes reasoning across these buffers along with the original problem dataset, sampling initial states from the merged dataset $\mathcal{D}_{\text{restart}}$.


\paragraph{Reward Design.} RAE gives the model multiple opportunities to refine its reasoning, but effective reflection is key to making use of these chances. In addition to using correctness as rewards, we introduce the following bonuses rewards as hints to guide the model to reach correct answers:

\begin{itemize}[leftmargin=*]
 \item \textbf{Rule-based Reward:} Rule-based reward simply evaluates the correctness of the final answer.
    \[
     r_{\text{rule}}(\boldsymbol{\Tilde{y}}_L, \boldsymbol{y^*}) = \mathbf{1}_{\boldsymbol{\Tilde{y}}_L=\boldsymbol{y^*}}-1 \in \{-1, 0\}
    \]
    \item \textbf{Reflection Bonuses:} 
 To reinforce self-reflection, we introduce a reflection bonus \(r_{\text{bonus}}\).  
If the model starts from an incorrect reasoning trajectory stored in the \textit{negative restart buffer} (\(\mathcal{D}^-_{\text{restart}}\)) and successfully solves the problem, it obtains a reward bonus, encouraging it to correct past mistakes. Conversely, if it starts from a correct trajectory in the \textit{positive restart buffer} (\(\mathcal{D}^+_{\text{restart}}\)) but fails to solve the problem, it incurs a penalty, discouraging unnecessary revisions when it was already on the right track.  
Formally, the reflection bonus is defined as:  
\[
r_\text{bonus}(z, \boldsymbol{\Tilde{y}}) = 
\begin{cases}
\beta & \text{if } z \in \mathcal{D}^-_{\text{restart}} \text{ and } \boldsymbol{\Tilde{y}}_L = \boldsymbol{y^*}, \\
-\beta & \text{if } z \in \mathcal{D}^+_{\text{restart}} \text{ and } \boldsymbol{\Tilde{y}}_L \neq \boldsymbol{y^*}, \\
0 & \text{otherwise},
\end{cases}
\]
where \(\beta\) is a bonus scale hyperparameter.

\item \textbf{Preference Bonuses:} Since correct answers are rare at initial training stage, reward signals are often too sparse for effective RL training. Even with reflection, the model may fail to generate any correct reasoning trajectories, resulting in a sparse reward problem.  
To mitigate this, we train an Outcome Reward Model (ORM) using a Bradley-Terry (BT) preference framework. The ORM rates reasoning trajectories, assigning higher values to correct (preferred) ones.  
For each problem \(\boldsymbol{x} \in \Dc\), we generate multiple trajectories using \(\pi_\theta\) and construct a preference dataset by pairing correct and incorrect outputs. A BT model is trained to maximize the score gap between these pairs. The ORM's output, \(\sigma\bigl(r_\psi(z, \boldsymbol{\Tilde{y}})\bigr) \in [0,1]\), serves as a fine-grained reward signal, helping the model further refine its reasoning. See Appendix~\ref{subsec:rl_details} for details.
\end{itemize}
For an initial state \( z \in \Dc_{\text{restart}} \) and a sampled trajectory \( \boldsymbol{\Tilde{y}} \), the overall reward function \( r(z, \boldsymbol{\Tilde{y}}) \) is defined as:
\[
r(z, \boldsymbol{\Tilde{y}}) =  r_{\text{rule}}(\boldsymbol{\Tilde{y}}_L, \boldsymbol{y^*})+\sigma\big(r_\psi(z, \boldsymbol{\Tilde{y}})\big) + r_\text{bonus}(z, \boldsymbol{\Tilde{y}})
\]
 




\paragraph{Iterative Self-improvement.}
RL enables a policy to self-improve from self-generated trajectories, but it can also lead to a vicious cycle, where the policy converges to a local sub-optimum and cannot further improve. Inspired by~\cite{agarwal2022reincarnating,schmitt2018kickstarting}, we propose an iterative self-improvement strategy to mitigate this issue. Specifically, after each round of RL training, we distill the knowledge of the current well-optimized policy into the base model through supervised fine-tuning (SFT). Starting from the newly fine-tuned model, we then perform another round of RL training. Intuitively, from an optimization perspective, each round of distillation can be viewed as a parameter reset mechanism that helps the policy escape local optima in the loss landscape, allowing it to continue self-improving (more details are included in Section~\ref{subsec:rl_details}). In the next section, we provide empirical evidence to validate this approach.
