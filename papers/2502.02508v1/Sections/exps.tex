\section{Experiment}
\begin{figure}[!t]
    \centering
     \includegraphics[width=0.5\textwidth]
     {Figures/data_mixture.pdf}
     \vspace{-1.5em}
\caption{\textbf{Number of Training Samples of Satori-Qwen-7B and Qwen-2.5-Math-7B-Instruct.} Satori-Qwen-7B requires significantly less supervision (small-scale FT) and relies more on self-improvement (large-scale RL).}
\label{fig:mixture}
\vspace{-1em}
\end{figure}

\input{Tables/main_results}
\input{Tables/transferability_results}

\paragraph{Implementation Details.}
We employ Qwen-2.5-Math-7B as the base model due to its strong mathematical capabilities. Our training data is sourced from the publicly available math instruction datasets, OpenMathInstruct-2 and NuminaMath-CoT. For the multi-agent data synthesis framework, the generator is required to generate high-quality, step-by-step reasoning trajectories. Therefore, we use Qwen-2.5-Math-Instruct as the generator. Meanwhile, the critic must have robust instruction-following capabilities, so we choose Llama-3.1-70B-Instruct as the critic. To ensure data quality, we filter out problems with invalid questions or incorrect labels, resulting in approximately 550k samples. Additional implementation details can be found in Appendix ~\ref{app:exp-details}.

\paragraph{Benchmark and Evaluation.}
We conduct the main evaluation of the models using math benchmarks to assess their problem-solving abilities, including GSM8K, MATH500 (a subset of the MATH test set~\cite{prm800k}), AMC2023, AIME2024, and OlympiadBench. Except for GSM8K, all other datasets feature competition-level problems. The evaluation is performed using greedy decoding without tool integration. The main metric reported is the zero-shot pass@1 accuracy, which measures the percentage of problems correctly solved on the first attempt. We also conduct additional evaluations on a wide range of benchmarks beyond the math domain to evaluate general reasoning capabilities. This includes logical reasoning (FOLIO \cite{folio}, BoardgameQA (BGQA) \cite{bgqa}), code reasoning (CRUXEval \cite{cruxeval}), commonsense reasoning (StrategyQA (STGQA) \cite{strategyqa}), tabular reasoning (TableBench \cite{tablebench}), and domain-specific reasoning (STEM subsets of MMLU-Pro \cite{mmlupro}), including physics, chemistry, computer science, engineering, biology, and economics. For more evaluation details, please refer to Appendix \ref{app:eval}. 

\paragraph{Baseline Models.}
We compare our developed model, Satori-Qwen-7B, with several industry-developed LLMs. The main comparison is between our model and Qwen-2.5-Math-7B-Instruct~\cite{qwen2_5}, a math-specialized model built on the same base model (Qwen-2.5-Math-7B) as ours. Additionally, we report the performance of larger models, including o1-preview and QwQ-32B-Preview, which exhibit strong reasoning capabilities and serve as performance upper bounds.



\subsection{Main Results on Math Domain} \label{subsec:main_results}
We present math benchmark results in Table~\ref{table:main-results}, where Satori-Qwen-7B outperforms all small-scale baseline models. Notably, using Qwen-2.5-Math-7B as the base model, Satori-Qwen-7B achieves superior performance compared to Qwen-2.5-Math-7B-Instruct, despite requiring significantly less supervision (i.e., less SFT data) and relying more on self-improvement (i.e., more RL data) (see Figure~\ref{fig:mixture}). 

\subsection{Out-of-Domain Transferability} \label{subsec:transfer_results}
Although Satori-Qwen-7B is trained only on math domain datasets, we observe that it can extrapolate its reasoning capabilities to other domains. In Table~\ref{table:transfer-results}, we evaluate Satori-Qwen-7B on a diverse set of out-of-domain benchmarks that require reasoning capabilities but are not directly related to math. Similar to the observation on the math domain, Satori demonstrates superior performance on several benchmarks, outperforming Qwen-2.5-Math-7B-Instruct. In particular, on the challenging reasoning benchmark BoardgameQA, Satori-Qwen-7B surpasses all baseline models of the same scale. These results and demo examples in Appendix~\ref{sec:demo} suggest that Satori has acquired general reasoning capabilities rather than simply math problem solving skills. In Section \ref{sec:analysis}, we present further analysis to show that this transferability emerges as a result of large-scale reinforcement learning.

\subsection{Results on Iterative Self-improvement}
Finally, we present the results of the second-round training of Satori. As shown in Table~\ref{table:main-results} and Table~\ref{table:transfer-results}, compared to Satori-Qwen-7B, Satori-Qwen-7B (Round 2) demonstrates continuous performance gains across most in-domain and out-of-domain benchmarks. This suggests the significant potential of iterative self-improvement to push the limit of LLM's reasoning performance.
