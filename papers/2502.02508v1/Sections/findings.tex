\section{Analysis} \label{sec:analysis}
In this section, we provide a comprehensive analysis of Satori. First, we demonstrate that Satori effectively leverages self-reflection to seek better solutions and enhance its overall reasoning performance. Next, we observe that Satori exhibits test-scaling behavior through RL training, where it progressively acquires more tokens to improve its reasoning capabilities. Finally, we conduct ablation studies on various components of Satori's training framework. Additional results are provided in Appendix~\ref{app:results}.



\paragraph{COAT Reasoning v.s. CoT Reasoning.}
\input{Tables/coat_vs_cot}
We begin by conducting an ablation study to demonstrate the benefits of COAT reasoning compared to the classical CoT reasoning. Specifically, starting from the synthesis of demonstration trajectories in the format tuning stage, we ablate the ``reflect'' and  ``explore'' actions, retaining only the ``continue'' actions. Next, we maintain all other training settings, including the same amount of SFT and RL data and consistent hyper-parameters. This results in a typical CoT LLM (Qwen-7B-CoT) without self-reflection or self-exploration capabilities. As shown in Table~\ref{table:ablation-coat}, the performance of Qwen-7B-CoT is suboptimal compared to Satori-Qwen-7B and fails to surpass Qwen-2.5-Math-7B-Instruct, suggesting the advantages of COAT reasoning over CoT reasoning.



\paragraph{Satori Exhibits Self-correction Capability.}
\input{Tables/finegrained_reflect_evaluation}
We observe that Satori frequently engages in self-reflection during the reasoning process (see demos in Section~\ref{sec:demo}), which occurs in two scenarios: (1) it triggers self-reflection at intermediate reasoning steps, and (2) after completing a problem, it initiates a second attempt through self-reflection. We focus on quantitatively evaluating Satori's self-correction capability in the second scenario. Specifically, we extract responses where the final answer before self-reflection differs from the answer after self-reflection. We then quantify the percentage of responses in which Satori's self-correction is positive (i.e., the solution is corrected from incorrect to correct) or negative (i.e., the solution changes from correct to incorrect). The evaluation results on in-domain datasets (MATH500 and Olympiad) and out-of-domain datasets (MMLUPro) are presented in Table~\ref{table:finegrain-reflect}. First, compared to Satori-Qwen-FT which lacks the RL training stage, Satori-Qwen demonstrates a significantly stronger self-correction capability. Second, we observe that this self-correction capability extends to out-of-domain tasks (MMLUProSTEM). These results suggest that RL plays a crucial role in enhancing the model's true reasoning capabilities.


\paragraph{RL Enables Satori with Test-time Scaling Behavior.}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/rm_shaping_tot_len.pdf}
    \vspace{-2em}
\caption{\textbf{Policy Training Acc. \& Response length v.s. RL Train-time Compute.} Through RL training, Satori learns to improve its reasoning performance through longer thinking.}
\label{fig:test_time_scaling}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{Figures/length_across_levels.pdf}
    \vspace{-1.5em}
\caption{\textbf{Above: Test-time Response Length v.s. Problem Difficulty Level; Below: Test-time Accuracy v.s. Problem Difficulty Level.} Compared to FT model (Satori-Qwen-FT), Satori-Qwen uses more test-time compute to tackle more challenging problems.}
\label{fig:difficulty_level}
\vspace{-1em}
\end{figure}

Next, we aim to explain how reinforcement learning (RL) incentivizes Satori's autoregressive search capability. First, as shown in Figure~\ref{fig:test_time_scaling}, we observe that Satori consistently improves policy accuracy and increases the average length of generated tokens with more RL training-time compute. This suggests that Satori learns to allocate more time to reasoning, thereby solving problems more accurately. One interesting observation is that the response length first decreases from 0 to 200 steps and then increases. Upon a closer investigation of the model response, we observe that in the early stage, our model has not yet learned self-reflection capabilities. During this stage, RL optimization may prioritize the model to find a shot-cut solution without redundant reflection, leading to a temporary reduction in response length. However, in later stage, the model becomes increasingly good at using reflection to self-correct and find a better solution, leading to a longer response length.
 
Additionally, in Figure~\ref{fig:difficulty_level}, we evaluate Satori's test accuracy and response length on MATH datasets across different difficulty levels. Interestingly, through RL training, Satori naturally allocates more test-time compute to tackle more challenging problems, which leads to consistent performance improvements compared to the format-tuned (FT) model.



\paragraph{Large-scale FT v.s. Large-scale RL.}
\input{Tables/ft_vs_rl}
We investigate whether scaling up format tuning (FT) can achieve performance gains comparable to RL training. We conduct an ablation study using Qwen-2.5-Math-7B, trained with an equivalent amount of FT data (300K). As shown in Table~\ref{table:ablation-ft-rl}, on the math domain benchmarks, the model trained with large-scale FT (300K) fails to match the performance of the model trained with small-scale FT (10K) and large-scale RL (300K). Additionally, the large-scale FT model performs significantly worse on out-of-domain tasks, demonstrates RLâ€™s advantage in generalization.


\paragraph{Distillation Enables Weak-to-Strong Generalization.} 
\begin{figure}[!t]
    \centering
     \includegraphics[width=0.4\textwidth]
     {Figures/distillation.pdf}
     \vspace{-1.5em}
\caption{\textbf{Format Tuning v.s. Distillation.} Distilling from a Stronger model (Satori-Qwen-7B) to weaker base models (Llama-8B and Granite-8B) are more effective than directly applying format tuning on weaker base models.}
\label{fig:distill}
\vspace{-1em}
\end{figure}
Finally, we investigate whether distilling a stronger reasoning model can enhance the reasoning performance of weaker base models. Specifically, we use Satori-Qwen-7B to generate 240K synthetic data to train weaker base models, Llama-3.1-8B and Granite-3.1-8B. For comparison, we also synthesize 240K FT data (following Section~\ref{subsec:format}) to train the same models. We evaluate the average test accuracy of these models across all math benchmark datasets, with the results presented in Figure~\ref{fig:distill}. The results show that the distilled models outperform the format-tuned models. 

This suggests a new, efficient approach to improve the reasoning capabilities of weaker base models: (1) train a strong reasoning model through small-scale
FT and large-scale RL (our Satori-Qwen-7B) and (2) distill the strong reasoning capabilities of the model into weaker base models. Since RL only requires answer labels as supervision, this approach introduces minimal costs for data synthesis, i.e., the costs induced by a multi-agent data synthesis framework or even more expensive human annotation.


