\begingroup
\hypersetup{linkcolor=darkblue} % Set link color
\addtocontents{toc}{\protect\StartAppendixEntries}
\listofatoc
\endgroup


\section{Satori's Demo Examples}  \label{sec:demo}
\begin{figure*}[htbp]
    \centering
     \includegraphics[width=1.0\textwidth]
     {Figures/math-4.pdf}
     \caption{\textbf{Math Domain Example.} Satori verifies the correctness of the intermediate steps and proceeds to the next reasoning step.}
\end{figure*}
\vspace{-2em}
\begin{figure*}[htbp]
    \centering
     \includegraphics[width=1.0\textwidth]
     {Figures/math-1.pdf}
     \caption{\textbf{Math Domain Example.} Satori identifies the mistakes in the previous solution and proposes an alternative correct solution.}
\end{figure*}
\vspace{-2em}
\begin{figure*}[htbp]
    \centering
     \includegraphics[width=1.0\textwidth]
     {Figures/math-2.pdf}
     \caption{\textbf{Math Domain Example.} Satori verifies the correctness of previous solution and initiates a different solution.}
\end{figure*}
\vspace{-2em}
\begin{figure*}[htbp]
    \centering
     \includegraphics[width=1.0\textwidth]
     {Figures/math-5.pdf}
     \caption{\textbf{Math Domain Example.} Satori verifies the correctness of previous solution and further explores a simpler solution.}
\end{figure*}
\vspace{-2em}
\begin{figure*}[htbp]
    \centering
     \includegraphics[width=1.0\textwidth]
     {Figures/math-3.pdf}
     \caption{\textbf{Math Domain Example.} 1) Satori verifies the correctness of intermediate steps in early stage. 2) Satori realizes that the pervious solution is actually erroneous and then proposes an alternative correct solution.}
\end{figure*}
\vspace{-2em}
\begin{figure*}[htbp]
    \centering
     \includegraphics[width=1.0\textwidth]
     {Figures/logic.pdf}
     \caption{\textbf{Out-of-domain Example.} 1) Satori identifies the potential mistakes in intermediate steps and initiates another solution. 2) Satori realizes that the pervious solution is still erroneous and then proposes an alternative correct solution.}
\end{figure*}
\vspace{-2em}
\begin{figure*}[htbp]
    \centering
     \includegraphics[width=1.0\textwidth]
     {Figures/commonsense.pdf}
     \caption{\textbf{Out-of-domain Example.} Satori identifies the potential mistakes in intermediate steps and initiates another correct solution.}
\end{figure*}
\vspace{-2em}
\begin{figure*}[htbp]
    \centering
     \includegraphics[width=1.0\textwidth]
     {Figures/chemistry.pdf}
     \caption{\textbf{Out-of-domain Example.} 1) Satori verifies the correctness of intermediate steps in early stage. 2) Satori realizes that the pervious solution is actually erroneous and then proposes an alternative correct solution.}
\end{figure*}
\vspace{-2em}
\begin{figure*}[htbp]
    \centering
     \includegraphics[width=1.0\textwidth]
     {Figures/economics.pdf}
     \caption{\textbf{Out-of-domain Example.} Satori engages in multiple self-reflection processes during intermediate reasoning steps.}
\end{figure*}
\vspace{-2em}
\begin{figure*}[htbp]
    \centering
     \includegraphics[width=1.0\textwidth]
     {Figures/code.pdf}
     \caption{\textbf{Out-of-domain Example.} 1) Satori verifies the correctness of intermediate steps in early stage. 2) Satori realizes that the pervious solution is actually erroneous and then proposes an alternative correct solution.}
\end{figure*}
\vspace{-2em}
\begin{figure*}[htbp]
    \centering
     \includegraphics[width=1.0\textwidth]
     {Figures/table.pdf}
      \caption{\textbf{Out-of-domain Example.} Satori identifies the mistakes in previous solution and proposes an alternative correct solution.}
\end{figure*}


\clearpage
\input{Sections/related} \label{app:related}

\clearpage
\input{Sections/data_synthesis}


\clearpage
\section{Experimental Setup} 
\label{app:exp-details}

\subsection{Data Processing}
\paragraph{Data Source.}
We construct our training dataset by combining two open-source synthetic datasets: OpenMathInstruct2~\cite{openmathinstruct2} and NuminaMath-COT~\cite{numina}. After a careful review of the synthetic data, we identify and remove invalid questions to improve data reliability.
To further enhance the quality of the dataset, we adopt the mutual consistency filtering method inspired by rStar~\cite{qi2024mutual}, which removes examples with inconsistent answers provided by different models. Specifically, we utilize QwQ~\cite{qwq-32b-preview} to relabel the questions and compared the newly generated answers with the original answers from the source datasets. Only examples with consistent answers were retained. Additionally, we apply de-duplication tools from~\cite{Stack} to eliminate redundant examples. Through these filtering processes, we finalized a high-quality dataset with approximately 550K samples in total.

\paragraph{Multi-agent COAT Data Synthesis.}
For the multi-agent demonstration trajectory synthesis framework, we utilize three models: Qwen-2.5-Math-7B-Instruct as the generator, Llama-3.1-70B-Instruct as the critic, and Skywork-Reward-Llama-3.1-8B-v0.2 as the outcome reward model. For the generator, we set the temperature to 0.3 and the maximum generation token limit to 2048. First, the generator samples $K=100$ initial solutions for each problem, dividing the generated solutions into three subsets: correct, incorrect, and invalid (those that fail to produce a final answer). Invalid solutions are discarded, and we randomly select four correct solutions and four incorrect solutions. Next, we set sampling temperature of critic to 0.2 and a maximum token limit of 256, and let critic provides feedback on these selected solutions, allowing the generator to perform conditional generation. This iterative process is repeated for a maximum of $m=2$ iterations, potentially resulting in up to $8 \times 2 = 16$ demonstration trajectories.

During the generation process, various situations require different prompt templates:
\begin{enumerate} \item The generator produces an initial solution.
\item The initial solution is correct, and the critic verifies its correctness.
\item The initial solution is incorrect, and the critic identifies mistakes.
\item The generator generates continuations after the critic verifies the correctness of its correct initial solution.
\item The generator generates continuations after the critic identifies mistakes in its incorrect initial solution.
\item The generator fails to solve the problem after refinement, and the critic provides an additional feedback to identify errors in the generator's second attempt.
\end{enumerate}
The prompt templates for these situations are detailed in Appendix~\ref{subsec:template}.

Among the synthetic trajectories, we categorize them into the following types:
\begin{enumerate} \item \textbf{Type-I:} Synthetic trajectories without critic feedback, i.e., no reflection actions.
\item \textbf{Type-II-I:} Synthetic trajectories that include an intermediate reflection action to verify the correctness of previous reasoning steps.
\item \textbf{Type-II-II:} Synthetic trajectories that include 1) an intermediate reflection action to verify the correctness of previous reasoning steps, and 2) a second reflection action to correct mistakes in the previous solution, followed by an explore action to propose an alternative solution.
\item \textbf{Type-III-I:} Synthetic trajectories that include a reflection action to correct mistakes in the previous solution and an explore action to propose an alternative solution.
\item \textbf{Type-III-II:} Synthetic trajectories that include two rounds of self-reflection and self-explore.
\end{enumerate}
Examples of these five types of synthetic trajectories are provided in Appendix~\ref{subsec:example-synthetic}. Finally, the outcome reward model is applied to select the top-1 (k=1) sample of each type from the 16 demonstration trajectories based on the reward score, if such a type exists.

\newpage
\subsubsection{Prompt Templates} \label{subsec:template}
\input{Prompts/generator-generate_chains}
\vspace{-1.5em}
\input{Prompts/generator-generate_continuations_correct}
\vspace{-1.5em}
\input{Prompts/generator-generate_continuations_incorrect}
\vspace{-1.5em}
\input{Prompts/critic-verify_correctness}
\vspace{-1.5em}
\input{Prompts/critic-correct_mistakes}
\vspace{-1.5em}
\input{Prompts/critic-correct_mistakes_second_attempt}

\subsubsection{Synthetic Data Examples} \label{subsec:example-synthetic}
\input{SyntheticDataExamples/type-i}
\input{SyntheticDataExamples/type-ii-i}
\input{SyntheticDataExamples/type-ii-ii}
\input{SyntheticDataExamples/type-iii-i}
\input{SyntheticDataExamples/type-iii-ii}

\clearpage
\subsection{Format Tuning}

\paragraph{Training Details.}
We perform supervised fine-tuning (SFT) on the Qwen-2.5-Math-7B base model using our synthetic COAT dataset with 10K unique questions. The SFT prompt template is illustrated below. We utilize a cosine learning rate scheduler with an initial learning rate of 2e-5. The batch size is set to 128, the maximum sequence length is 4096, and the model is trained for a maximum of two epochs. We add the following special tokens \texttt{<|continue|>},\texttt{<|reflect|>},\texttt{<|explore|>} into the vocabulary. All experiments are implemented using the LLaMA-Factory framework~\cite{llamafactory}.


\begin{tcolorbox}[green_box, title = {{Prompt Template 3: SFT and RL}}]
\label{box:prompt_template_3}
\begin{verbatim}
<|im_start|>user
Solve the following math problem efficiently and clearly.
Please reason step by step, and put your final answerwithin \boxed{}.
Problem: <<<your instruction>>>
<|im_start|>assistant
\end{verbatim}
\label{fig:sft_template}
\end{tcolorbox}

\subsection{Reinforcement Learning}
\label{subsec:rl_details}

\paragraph{ORM Training.}
To construct the preference data for our ORM models, we utilize our format-tuned model, Satori-Qwen-7B-FT, to generate trajectories. Starting with our filtered training dataset of 550K unique questions, we follow these steps: (1) allow the FT model to sample eight solutions for each question; (2) evaluate the correctness of these solutions and label them accordingly; and (3) select only those questions that contain correct and incorrect solutions. For these selected questions, we construct preference data by pairing correct solutions with their corresponding incorrect ones, resulting in a preference dataset of approximately 300K unique questions.

For each problem $\boldsymbol{x}$, we allow $\pi_{\theta}$ to randomly generate multiple reasoning trajectories, constructing a dataset $\Dc_r$ with positive and negative pairs of trajectories. We select trajectories with the correct final answer as positive trajectories $\boldsymbol{\Tilde{y}}^{+}$ and trajectories with incorrect final answer as negative trajectories $\boldsymbol{\Tilde{y}}^{-}$. Assuming the Bradley-Terry (BT) preference model, we optimize the reward model $r_\psi(x, \Tilde{y})$ through negative log-likelihood,
    \begin{align}
    \mathcal{L}_{rm}(\psi) \defeq -\mathbb{E}_{(\boldsymbol{x}, \boldsymbol{\Tilde{y}}^{+}, \boldsymbol{\Tilde{y}}^{-}) \sim \Dc_r} \left[ \log \left( \sigma \left( r_\psi(\boldsymbol{x}, \boldsymbol{\Tilde{y}}^{+}) - r_\psi(x, \boldsymbol{\Tilde{y}}^{-}) -\tau \right) \right) \nonumber \right]
    \end{align}
where $\tau$ denotes a target reward margin. In practice, we observe that setting $\tau>0$ improves the performance of the reward model.

\paragraph{RL Data.}
Our RL training dataset consists of 300K unique questions from the preference dataset. This ensures that the questions are neither too easy (where the FT model always produces correct solutions) nor too difficult (where the FT model never succeeds). This encourages policy to learn through trial and error during RL training. To further guide the model to learn self-reflection capabilities, we apply the proposed RAE technique, augmenting input problems with restart buffers, i.e., intermediate reasoning steps collected from the FT model. These intermediate steps are extracted from the preference dataset, and for each question, we randomly select one correct and one incorrect trajectory, applying the back-track technique for up to $T=2$ steps.

\paragraph{Training Details.}
For both ORM and RL training, we implement our experiments using the OpenRLHF framework~\cite{hu2024openrlhf}. For ORM training, we employ a cosine learning rate scheduler with an initial learning rate of 2e-6. The batch size is set to 128, the maximum sequence length to 4096, and the model is trained for two epochs. As the objective function, we use PairWiseLoss~\cite{christiano2017deep} with a margin of $\tau=2$. For evaluation, we select the optimal ORM model checkpoint based on RM@8 performance, measured using the SFT model on a held-out validation dataset. Specifically, we allow the FT model to sample eight trajectories and let ORM select the best trajectory according to the highest reward score. The RM@8 accuracy is then computed based on the selected trajectories.

For RL training, we use the PPO algorithm~\cite{ppo}. The critic model is initialized from our ORM model, while the actor model is initialized from our FT model. We optimize the models using a cosine learning rate scheduler, setting the learning rate to 2e-7 for the actor model and 5e-6 for the critic model. During PPO training, we sample one trajectory per prompt. The training batch size is set to 128, while the rollout batch size is 1024. Both the number of epochs and episodes are set to 1. The maximum sequence length for prompts and generations is fixed at 2048. Additional parameter settings include a KL coefficient of 0.0, a sampling temperature of 0.6, and a bonus scale of $r_{\text{bonus}}=0.5$.

\paragraph{Second-round Self-improvement.}
We begin with a set of 240K unique questions, also used in the distillation experiments shown in Table~\ref{fig:distill}. The policy of the first round of RL training serves as a teacher model to generate synthetic reasoning trajectories. Among these 240K questions and corresponding trajectories, we filter the data based on question difficulty, selecting the most challenging 180K samples for distillation. This process results in a new fine-tuned (FT) model checkpoint, obtained from supervised fine-tuning (SFT) on these 180K trajectories. Since the new FT model has been trained on numerous high-quality trajectories, including reflection actions distilled from the teacher model, we do not apply restart and exploration (RAE) techniques in the second round of RL training to further encourage reflection. Additionally, we increase the sampling temperature from 0.6 to 1.2, generating eight samples per prompt to encourage more aggressive exploration to push the performance limit.

\subsection{Evaluation Details.}
\label{app:eval}
For each model, we use the same zero-shot CoT prompt template to obtain results on all test datasets. For Satori and all its variants, we use Prompt Template 3 (Appendix \ref{box:prompt_template_3}). We set the temperature to 0 (greedy decoding) for every model, and collect pass@1 accuracies. Details of each test dataset are as follows.

\textbf{MATH500} \cite{prm800k} is a subset of MATH \cite{MATH} of uniformly sampled 500 test problems. The distribution of difficulty levels and subjects in MATH500 was shown to be representative of the entire MATH test set.

\textbf{GSM8K} \cite{GSM8K} is a math dataset that consists of 8.5K high-quality, linguistically diverse grade-school math word problems designed for multi-step reasoning (2 to 8 steps). Solutions involve elementary arithmetic operations and require no concepts beyond early algebra. Its test set contains 1319 unique problems.

\textbf{OlympiadBench} \cite{olympiadbench} is a bilingual, multimodal scientific benchmark with 8,476 Olympiad-level math and physics problems, including those from the Chinese college entrance exam. We use the open-ended, text-only math competition subset, containing 674 problems in total. 

\textbf{AMC2023} and \textbf{AIME2024} contain 40 text-only problems from American Mathematics Competitions 2023 and 30 text-only problems from American Invitational Mathematics Examination 2024, respectively.

\textbf{FOLIO} \cite{folio} is a human-annotated dataset designed to evaluate complex logical reasoning in natural language, featuring 1,430 unique conclusions paired with 487 sets of premises, all validated with first-order logic (FOL) annotations. Its test set contains 203 unique problems.

\textbf{BoardgameQA (BGQA)} \cite{bgqa} is a logical reasoning dataset designed to evaluate language models' ability to reason with contradictory information using defeasible reasoning, where conflicts are resolved based on source preferences (e.g., credibility or recency). Its test set contains 15K unique problems.

\textbf{CRUXEval} \cite{cruxeval} is a benchmark for evaluating code reasoning, understanding, and execution, featuring 800 Python functions (3-13 lines) with input-output pairs for input and output prediction tasks. Given a function snippet and an input example, LLMs are tasked to generate the corresponding outputs. Its test set contains 800 unique problems.

\textbf{StrategyQA} \cite{strategyqa} is a question-answering benchmark designed for multi-hop reasoning where the necessary reasoning steps are implicit and must be inferred using a strategy. Each of the 2,780 examples includes a strategy question, its step-by-step decomposition, and supporting Wikipedia evidence.

\textbf{TableBench} \cite{tablebench} is a tabular reasoning benchmark designed to evaluate LLMs on real-world tabular data challenges, covering 18 fields across four major TableQA categories: Fact checking, numerical reasoning, data analysis, and code generation for visualization. We test all models on fact checking and numerical reasoning subsets for simplicity of answer validation, resulting in 491 unique problems.

\textbf{MMLUProSTEM} is a subset of MMLU-Pro \cite{mmlupro}. MMLU-Pro is an enhanced benchmark designed to extend MMLU \cite{mmlu} by incorporating more reasoning-focused questions, expanding answer choices from four to ten, and removing trivial or noisy items. We select six STEM subsets: physics, chemistry, computer science, engineering, biology, and economics (we remove the math subset as it belongs to in-domain tasks). Finally, we obtain 5371 unique problems in total.


\section{Additional Results} \label{app:results}

\subsection{Ablation on Reflection Bonus}
\input{Tables/reflect_bonus_ablation}
During RL training, we introduce a reflection bonus to facilitate the policy to learn self-reflection capabilities. The default value of the reflection bonus is set to $r_{\text{reflect}}=0.5$. To analyze its impact on performance, we also evaluate the model with the reflection bonus set to $r_{\text{reflect}}=0$. The results are presented in Table~\ref{table:reflect-bonus}. We observe that the performance slightly degrades on challenging benchmark AMC2023 and AIME2024 when set $r_{\text{reflect}}=0$ compared to $r_{\text{reflect}}=0.5$.


\subsection{Offline Restart Buffer v.s. Online Restart Buffer}
Complementary to the reflection bonus, the restart buffer is designed to enhance the policy's self-reflection capabilities by augmenting the initial states with a diverse set of intermediate states. This includes trajectories processed from both correct and incorrect reasoning paths, which are then categorized into positive ($\Dc^+_{\text{restart}}$) and negative ($\Dc^-_{\text{restart}}$) restart buffers, as described in Section~\ref{subsec:rl}.

In addition to constructing the restart buffer offline, we also explore an online restart buffer approach. Specifically, after each PPO episode, we use the updated policy to construct the restart buffer and collect rollouts from this buffer to optimize the policy, iteratively repeating this process. However, this approach is suboptimal. During PPO training, we observe that the majority of sampled trajectories are correct, leading to a significant imbalance between correct and incorrect intermediate states in the online restart buffer. As a result, the model fail to adequately learn from incorrect paths, which are essential for incentivize self-reflection actions.

To overcome this limitation, we opt for an offline restart buffer approach to mitigate the bias introduced by online collection. Offline sampling ensures a balanced inclusion of intermediate states from both correct and incorrect trajectories.