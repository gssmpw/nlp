\section{Introduction}
Large language models (LLMs) have demonstrated remarkable performance across a wide range of reasoning tasks, including mathematical problems~\cite{GSM8K, MATH}, programming~\cite{humaneval, bigcodebench} and logical reasoning~\cite{folio, logiqa}. One of the key techniques enabling these strong reasoning capabilities is Chain-of-Thought (CoT) prompting~\cite{wei2022chain}, which allows LLMs to address complex tasks by generating a series of intermediate reasoning steps. As a result, many early efforts focus on fine-tuning LLMs using large-scale, high-quality CoT reasoning chains, either through human annotation~\cite{MATH, mammoth} or by distilling synthetic data from more advanced models~\cite{metamath, openmathinstruct2, scalequest}. However, human annotation is extremely labor intensive, and distillation often limits the model's reasoning capabilities to certain level.

Apart from scaling up training resources, more recent work has focused on test-time scaling, i.e., allocating additional inference-time compute to search for more accurate solutions. This often involves extensive sampling, either by generating multiple complete solutions~\cite{wang2023selfconsistency} or by sampling multiple intermediate reasoning steps~\cite{yao2024tree, wan2024alphazero}. These methods typically require external feedback to guide the search process, usually through training an auxiliary reward model to rate final solutions or intermediate steps~\cite{easytohard, mathshepherd}. However, such two-player frameworks incur more model-deployment costs and do not internalize the search capabilities into a single LLM.

Orthogonal to the above work, our study investigates a new direction that enables LLMs with autoregressive search capabilities, i.e., an extended reasoning process with self-reflection and self-exploration of new strategies. Specifically, we introduce the Chain-of-Action-Thought (COAT) mechanism, which enables LLMs to take various meta-actions during problem solving. Unlike conventional post-training consisting of large-scale supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), we propose a novel two-stage training paradigm: (1) a small-scale format tuning (FT) stage to internalize the COAT reasoning format and (2) a large-scale self-improvement stage that utilizes reinforcement learning with ``Restart and Explore'' (RAE) techniques. Our approach leads to the development of Satori, a 7B LLM trained on open-source base models and mathematic data that achieve superior performance on both in-domain and out-of-domain tasks. To summarize, our contributions are threefold,
\begin{enumerate}
    \item \textbf{Efficiency:} Satori is a single LLM capable of autoregressive search without external guidance (Section~\ref{sec:analysis} and Section~\ref{sec:demo}). Moreover, this is achieved with minimal supervision and large-scale self-improvement.
    \item \textbf{Effectiveness:} Satori demonstrates superior performance on in-domain mathematical reasoning tasks and outperforms the instruct model built on the same base model (Section~\ref{subsec:main_results}).
    \item \textbf{Generalizability:} Unlike recent research on math reasoning, Satori exhibits strong transferability to out-of-domain tasks and demonstrates universal capabilities for self-reflection and self-exploration (Section~\ref{subsec:transfer_results}).
\end{enumerate}


\section{Related Work}
We summarize the literature that is closely aligned with the scope of this paper (refer to Section~\ref{sec:related} for more discussions).
\paragraph{Concurrent Work.}
Building on the impact of OpenAI's o1~\cite{o1}, significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. The most common approach relies on distilling knowledge from stronger teacher models~\cite{o1journey2, marcoo1, min2024imitate}. In contrast, Satori addresses this problem from a reinforcement learning (RL) perspective and requires minimal supervision (only 10K samples in the format tuning stage). The most related concurrent work is DeepSeekâ€™s recently released R1~\cite{guo2025deepseek}, which adopts a similar high-level strategy of small-scale cold-start SFT followed by large-scale RL training. Although both works coincide in this high-level idea, our work differs from R1 in key methodologies, including the data synthesis framework and RL algorithms. Additionally, DeepSeek-R1 focuses on training large-scale LLMs (671B), whereas our work provides insights into the development of smaller-scale LLMs (7B) for research purpose. Finally, as an industry-developed model, the technical details of DeepSeek-R1~\cite{guo2025deepseek} are not fully disclosed, making reproduction difficult, whereas our work is a fully transparent study that aims to open-source training data and training recipes. 

\paragraph{Post-training LLMs for Reasoning.}
Recent advancements have focused on extensive post-training to enhance reasoning. A line of work focus on constructing high-quality instruction-tuning datasets~\cite{MATH, mammoth, metamath, openmathinstruct2, scalequest}, but suffers from expensive annotatoin costs. More recent research has focused on self-improvement approaches, where models are trained on data generated by themselves \cite{star, quietstar, singh2023beyond, cpo}. Additionally, reinforcement learning methods, particularly those based on Proximal Policy Optimization (PPO)~\cite{ppo, rlhf}, have been demonstrated to be more effective, which typically leverage reward models to guide the learning process \cite{easytohard, mathshepherd, yuan2024implicitprm}.

\paragraph{Enabling LLMs with Searching Abilities.}
Prompting-based approaches~\cite{yao2024tree, shinn2024reflexion, hao2023reasoning, qi2024mutual} guide LLMs to search for solutions via error correction and exploring alternative paths. However, such approaches cannot fundamentally enhance the LLM's reasoning abilities. Moreover, recent work has pointed out the difficulties of LLMs in self-correction \cite{zhang2024small, kamoi2024can}. Recent research has pivoted toward training LLMs for self-exploration. Some focused on enabling \textit{trajectory-level search}---iteratively identify errors in previous complete responses and produce improved responses~\cite{saunders2022self, kumar2024training, qu2024recursive, havrilla2024glore}. Another line of research has explored \textit{step-level search}, which enables LLMs to identify and correct mistakes in a more fine-grained manner. Some achieve this using another model to provide step-level feedback \cite{xi2024enhancing, rewardingprogress, generativeverifiers, rstarmath, restmcts}, but such two-player frameworks suffer from high costs for model deployment. SoS \cite{gandhi2024stream} is another closely related work that attempts to train a single LLM to perform a tree search as a flattened string. However, the effectiveness of SoS has primarily been shown on simple symbolic tasks, and its ability to generalize to more complex problems remains to be explored. 