\section{Additional Related Work} \label{sec:related}
\subsection{Post-training LLMs for Reasoning}

State-of-the-art LLMs have achieved human-level and, in some cases, ``superhuman'' performance across diverse reasoning benchmarks. These benchmarks span various domains, including mathematics \cite{GSM8K, MATH, frontiermath, gsmsymbol}, programming \cite{humaneval, mbpp, bigcodebench, mhpp, swebench, cruxeval}, logical reasoning \cite{folio, logiqa, fimo, proofwriter}, commonsense reasoning \cite{strategyqa, commonsenseqa}, algorithmic reasoning \cite{clrs, clrstext, scylla, nphardeval}, semi-structured data \cite{tablebench, tabmwp}, scientific knowledge \cite{scibench, gpqa}, and world knowledge \cite{mmlu, mmlupro, bigbench, liang2022holistic, hle}. 

Recent advancements have concentrated on extensive post-training to enhance LLMs' reasoning abilities. One research direction in this area involves constructing instruction-tuning datasets annotated with high-quality CoT-like reasoning chains. These datasets are created either through extensive human annotation \cite{MATH, Selfcritiquing, mammoth} or by distilling data from more advanced models \cite{metamath, openmathinstruct, openmathinstruct2, scalequest, wizardmath, o1journey2, marcoo1, phi4, min2024imitate}. However, human annotation is resource-intensive, and model-generated data inherently caps the student model's potential at the level of the teacher model.

More recent research has focused on self-improvement approaches, where models are trained on data generated by themselves \cite{star, quietstar, singh2023beyond, cpo}. While self-training mitigates the reliance on external resources, it has raised concerns about potential ``model collapse'', a phenomenon where the iterative use of model-generated data degrades performance \cite{curseofrecursion, progressofregress}.
Additionally, reinforcement learning methods, particularly those based on Proximal Policy Optimization (PPO) \cite{ppo, rlhf}, have been explored to enhance reasoning capabilities. These approaches typically utilize reward models, such as Process-Reward Models (PRMs) or Outcome-Reward Models (ORMs), to guide the learning process \cite{easytohard, mathshepherd, yuan2024implicitprm}, resulting in significant performance improvements compared to supervised fine-tuning.

\subsection{Enabling LLMs with Searching Abilities}
Chain-of-Thought (CoT) prompting \cite{wei2022chain} demonstrated its potential to improve reasoning but lacked mechanisms to correct previous errors once committed. To address this, subsequent work proposed more sophisticated methods \cite{yao2024tree, shinn2024reflexion, besta2024graph, selfrefine, yang2024buffer} that prompt LLMs to search for solutions via forward exploration, backtracking from errors, and finding alternate paths. Heuristic search methods \cite{hao2023reasoning, qi2024mutual} have also been adopted to enable more effective exploration of high-quality solutions. However, prompting-based approaches improve task-specific performance without fundamentally enhancing the LLM's intrinsic reasoning capabilities. Moreover, recent work has pointed out the inherent difficulties of current LLMs in conducting effective self-correction \cite{huang2023large, zhang2024small, kamoi2024can}.

Recent research has pivoted toward training LLMs explicitly for exploration and backtracking. A large body of work has focused on enabling \textit{trajectory-level search} abilities, which train LLMs to iteratively identify errors in their previous complete responses and produce improved responses, relying on either human-annotated revisions \cite{saunders2022self} or model-generated data \cite{kumar2024training, qu2024recursive, havrilla2024glore} as training data. Another line of research has investigated \textit{step-level search} techniques, which induce more fine-grained and real-time correction that enables LLMs to identify and correct mistakes once they occur. Some achieve this by leveraging another model to provide step-level feedback to an actor model in the reasoning process \cite{xi2024enhancing, welleck2022generating, paul2023refiner, llamaberry, rewardingprogress, generativeverifiers, rstarmath, restmcts}, but such two-player frameworks suffer from high costs for model deployment. The most related to our work is SoS \cite{gandhi2024stream}, which attempted to train a single LLM to perform a tree search as a flattened string. However, the effectiveness of SoS has primarily been demonstrated on simple symbolic tasks, and the ability to generalize to more complex problems, such as math word problems, remains to be explored. 

