\section{Details about Data Synthesis Framework} \label{sec:synthesis}

\begin{figure*}[!t]
    \centering
     \includegraphics[width=0.8\textwidth]
     {Figures/synthesis.pdf}
\caption{\textbf{Demonstration Trajectories Synthesis.} First, multiple initial reasoning trajectories are sampled from the generator and sent to critic to ask for feedback. The critic model identifies the mistake for trajectories with incorrect final answers and proposes an alternative solution. For trajectories with correct final answers, the critic model provides verification of its correctness. Based on the feedback, the generator self-refines its previous trajectories, and the incorrect trajectories are sent to the critic again for additional feedback with maximum $m$ iterations. At each step, those trajectories with successful refinements are preserved and finally, a reward model rates and collects high-quality demonstration trajectories to form the synthetic dataset $\Dc_{\text{syn}}$.}
\label{fig:synthesis}
\end{figure*}

\paragraph{Sample Initial Trajectories.}
The details of data synthesis framework are illustrated in Figure~\ref{fig:synthesis}. Given an input problem $\boldsymbol{x}\in \Dc$, we begin by sampling the generator $\pi_g$ to generate $K$ initial reasoning trajectories. For each trajectory $\boldsymbol{\Tilde{y}} = [\boldsymbol{\Tilde{y}}_{1}, \boldsymbol{\Tilde{y}}_{2}, \ldots, \boldsymbol{\Tilde{y}}_{L}] \sim \pi_g(\cdot | \boldsymbol{x})$, we evaluate whether the final answer $\boldsymbol{\Tilde{y}}_{L}$ matches the ground-truth answer $\boldsymbol{y^*}$. Based on the evaluation, the generated trajectories are divided into two subsets according to their correctness, which are then processed differently in subsequent steps.

\paragraph{Critic and Refinement.}
For those incorrect trajectories, the critic $\pi_c$ provides feedback to help the generator address its flaws. Specifically, the critic, given the ground-truth solution, identifies the first erroneous step $\boldsymbol{\Tilde{y}}_{l}$ and generates a summary $\boldsymbol{\Tilde{y}}_{l+1}$ of the mistake as a reflection, along with a exploration direction (hint), $\boldsymbol{\Tilde{y}}_{l+2}$, i.e., $[\boldsymbol{\Tilde{y}}_{l+1}, \boldsymbol{\Tilde{y}}_{l+2}] \sim \pi_c(\cdot | \boldsymbol{x}, \boldsymbol{\Tilde{y}}_{1}, \ldots, \boldsymbol{\Tilde{y}}_{l}; \boldsymbol{y^*})$. Next, we ask the generator $\pi_g$ to self-refine its current trajectory based on the feedback provided by the critic, performing a conditional generation of the remaining reasoning steps, $[\boldsymbol{\Tilde{y}}_{l+3}, \ldots, \boldsymbol{\Tilde{y}}_{L}] \sim \pi_g(\cdot | \boldsymbol{x}, \boldsymbol{\Tilde{y}}_{1}, \ldots, \boldsymbol{\Tilde{y}}_{l}; \boldsymbol{\Tilde{y}}_{l+1}, \boldsymbol{\Tilde{y}}_{l+2})$. 

For correct trajectories, the critic $\pi_c$ focuses on verifying the correctness of the generatorâ€™s reasoning. A random intermediate reasoning step $\boldsymbol{\Tilde{y}}_{l}$ is selected, and the critic provides a summary explaining why the preceding steps are progressing toward the correct solution, i.e., $\boldsymbol{\Tilde{y}}_{l+1} \sim \pi_c(\cdot | \boldsymbol{x}, \boldsymbol{\Tilde{y}}_{1}, \ldots, \boldsymbol{\Tilde{y}}_{l}; \boldsymbol{y^*}), \text{where} \ \boldsymbol{\Tilde{y}}_{l+1}$. Similarly, the generator continues from the current solution, generating the subsequent steps as $[\boldsymbol{\Tilde{y}}_{l+2}, \ldots, \boldsymbol{\Tilde{y}}_{L}] \sim \pi_g(\cdot | \boldsymbol{x}, \boldsymbol{\Tilde{y}}_{1}, \ldots, \boldsymbol{\Tilde{y}}_{l}; \boldsymbol{\Tilde{y}}_{l+1})$.

Finally, we check whether the final answer $\boldsymbol{\Tilde{y}}_{L}$ aligns with $\boldsymbol{y^*}$. The above procedure is repeated iteratively, up to a maximum of $m$ iterations, until the generator produces the correct final answer. All feedback and refinements are then contaminated to synthesize the final demonstration trajectories. Additionally, the trajectories are post-processed by inserting meta-action tokens at the beginning of each reasoning step to indicate its meta-action type.

\paragraph{Trajectory Filtering.}
The above procedure may yield multiple demonstration trajectories for each problem $\boldsymbol{x}$. We then select the top-$k$ ($k<K$) trajectories based on the reward scores $r = \pi_r(\boldsymbol{\Tilde{y}}, \boldsymbol{x})$ assigned by the reward model $\pi_r$. This approach allows us to construct a diverse synthetic dataset $\Dc_{\text{syn}}$ containing high-quality demonstration trajectories, including (1) short-cut COAT paths that boil down CoT reasoning paths and (2) more complex COAT paths involving multiple rounds of self-reflection and exploration of alternative solutions.