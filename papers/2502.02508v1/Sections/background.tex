\section{Preliminaries}
\begin{figure*}[!t]
    \centering
     \includegraphics[width=1.0\textwidth]
     {Figures/schematic.pdf}
\caption{\textbf{A High-level Overview of Satori Training Framework: Format Tuning (FT) + Self-improvement.} First, Satori learns COAT reasoning format through imitation learning on \textbf{small-scale} demonstration trajectories. Next, Satori further leverages COAT reasoning format to self-improve via \textbf{large-scale} reinforcement learning. }
\label{fig:schematic}
\vspace{-1em}
\end{figure*}

\label{sec:pre}
We address mathematical problem-solving by training a language model $\pi_\theta$ to generate a solution $\boldsymbol{\Tilde{y}}$ that matches the ground truth $\boldsymbol{y}^*$, given a problem prompt $\boldsymbol{x}$. All sequences $\boldsymbol{x}$, $\boldsymbol{y}$, and $\boldsymbol{y}^*$ consist of tokens from a predefined dictionary. Since our approach uses reinforcement learning (RL) to train the model for solving math problems, we outline the key RL concepts below.

\paragraph{Reinforcement Learning (RL).} RL~\citep{kaelbling1996reinforcement} involves an agent making sequential decisions to maximize the expected cumulative rewards through interactions with an environment. Here, the language model $\pi_\theta$ acts as the agent's policy. Starting from an initial state $z_0$, at each step $l$, the agent observes the current state $z_l$, receives a reward $r_l$, selects an action based on $\pi_\theta$, transitions to the next state $z_{l+1}$, and continues until reaching a terminal state. A trajectory is the sequence of states and actions during this interaction. RL optimizes the policy to maximize the expected rewards $\sum_{l=1}^L r_l$, where $L$ is the trajectory length.