Our analysis thus far has shed light on how experts reason about originality when taking on the role of creativity evaluators and how this is affected by the inclusion of context. But do these same effects hold for LLMs when they are used to evaluate creativity? Conceptually, the examples given to humans serve a similar role as few-shot learning, a well-established method of improving the accuracy of LLMs on classification tasks \citep{brown2020language}. Yet exemplars may also bias models in other ways, especially in technically complex domains like science and engineering where the nature of truly creative ideas can be complex and difficult to discern a priori \citep{schmidt2011creativity,simonton2004creativity}. Scientific evaluations given by LLMs also tend to be markedly different from humans, with LLMs often overestimating the quality of scientific research \citep{schmidgall2025agent}. Given the increasing role generative AI is having both in expert-level science and in STEM education, it is crucial to perform a similar finegrained analysis of LLM creativity evaluations to enable a head-to-head comparison between human experts and LLMs as evaluators. Our second experiment sought to perform this comparison, using the same methods as in the first experiment but using ratings from LLMs.

% , and where maximizing performance on peer review benchmarks does not guarantee unbiased evaluations in the real world \TODO{cite}

\subsection{Methods}
We use \textsc{claude-3.5-haiku} and \textsc{gpt-4o-mini}, as these LLMs tend to achieve competitive performance on AI benchmarks while also being cost-effective \citep{chiang2024chatbot}. Further, because we use the larger variants of OpenAI and Anthropic models to rate explanations (\textsc{claude-3.5-sonnet} and \textsc{gpt-4o}), we chose to use these smaller variants for this study to avoid possible biases from LLMs recognizing their own output \citep{panickssery2024llm}. We set the temperature to $0$ and top P to $1$ for all trials while leaving other hyperparameters at their defaults. We structure our prompt similar to the instructions given to the human participants. We instruct the LLM to rate originality, cleverness, remoteness, and uncommonness and to explain its originality score. LLMs are given no exemplars in the no example condition and are given the same examples as humans in the example condition. We administer the same datasets for both conditions as we used in the first experiment, including duplicate archival responses, to make results from humans and LLMs as comparable as possible.\footnote{Note that, even with temperature set to zero, these LLMs may generate different ratings for the same response.} We include our prompts in the supplementary materials.

\subsection{Results}
LLM originality predictions correlated strongly with the true scores, and examples significantly boosted this correlation (no example: r = $0.6$, $0.67$; example: r = $0.74$, $0.76$; all correlations significant). \textsc{claude-3.5-haiku} and \textsc{gpt-4o-mini} exhibited strong agreement in their ratings, with correlations between their facet scores in the range $0.73$--$0.88$. Figure \ref{fig:experiment_2_gpt_correlations} summarizes facet correlations for \textsc{gpt-4o-mini}. Cleverness was the weakest predictor of originality scores in the no example condition, with remoteness and uncommonness being much more strongly correlated with originality. However, this effect dissipated in the example condition, with the strength in correlation between originality and each facet increasing significantly (all changes in correlations were found significant using Fisher's z test). To account for the possibility of these correlations being influenced by chance agreement among the facets, we also performed the same comparisons using Cohen's Kappa and obtained similar findings, detailed results are in the supplementary materials. We compare the distributions of human and \textsc{gpt-4o-mini} originality scores across both conditions in Figure \ref{fig:originality_distros}. We find that LLMs rarely use a five and rate a majority of responses as a two. 

\begin{figure}[htb]
    \centering
    \footnotesize
    \includegraphics[width=0.8\linewidth]{Figures/Correlations_GPT.eps}
    % \includesvg[width=0.85\linewidth]{Figures/Correlations_GPT.svg}
    \caption{Pearson correlations among pairwise Likert ratings for \textsc{gpt-4o-mini} in both conditions. o = originality, c = cleverness, u = uncommonness, r = remoteness.}
    \label{fig:experiment_2_gpt_correlations}
\end{figure}

\begin{figure}[htb]
    \centering
    \footnotesize
    \includegraphics[width=0.8\linewidth]{Figures/human_ai_originality_distros.eps}
    % \includesvg[width=0.7\linewidth]{Figures/human_ai_originality_distros.svg}
    \caption{Human and \textsc{gpt-4o-mini} originality scores.}
    \label{fig:originality_distros}
\end{figure}

Figure \ref{fig:liwc_analysis} compares human and \textsc{gpt-4o-mini} explanations. For \textsc{gpt-4o-mini}, both \textsc{gpt-4o} and \textsc{claude-3.5-sonnet} found no significant difference in perceptual details per condition, but did find significant differences in use of cleverness language (Claude U = 106495, p $<$ 0.001; GPT U = 1031445.5, p $<$ 0.001), with models producing conflicting ratings for the remaining markers (results were similar for \textsc{claude-3.5-haiku}). Comparing LLMs to humans, we find more variability in the presence or absence of all linguistic markers in humans as opposed to LLMs, with LLMs having much more heavily skewed rating distributions and sometimes having a marker completely absent across all explanations, which never occurred in humans. LLM explanations also tended to follow a more rigidly analytical structure, implying a more structured evaluation compared to humans, who had more instances of perceptual details that implied a more intuitive process drawing on memory. Results for \textsc{claude-3.5-haiku} are similar and are included in the supplementary materials.

% \begin{figure*}[t]
%     \centering
%     \small
%     % First row
%     \subfloat{
%         \includegraphics[width=0.35\textwidth]{Figures/human_cleverness_openai.png}
%         \label{fig:human_cleverness}
%     }
%     \hfill
%     \subfloat{
%         \includegraphics[width=0.35\textwidth]{Figures/gpt_cleverness_openai.png}
%         \label{fig:gpt_cleverenss}
%     }
%     \subfloat{
%         \includegraphics[width=0.35\textwidth]{Figures/human_comparative_language_openai.png}
%         \label{fig:human_comparative}
%     }
%     \hfill
%     \subfloat{
%         \includegraphics[width=0.35\textwidth]{Figures/gpt_comparative_language_openai.png}
%         \label{fig:gpt_comparative}
%     }
%     \caption{Comparison between linguistic marker use from humans (left) and LLMs (right) across several categories. For brevity, we have only included ratings from 
%     \textsc{gpt-4o} and examplanations from \textsc{gpt-4o-mini}, additional results are included in the supplementary materials.}
%     \label{fig:liwc_analysis}
% \end{figure*}

\begin{figure*}[t]
    \centering
    \footnotesize
    % First row
    \subfloat{
        \includegraphics[width=0.8\textwidth]{Figures/facet_counts_human.eps}
        % \includesvg[width=0.8\textwidth]{Figures/facet_counts_human.svg}
        \label{fig:human_cleverness}
    }
    % \hfill
    % \\[0.5ex]
    \hspace{-2cm}
    \subfloat{
        \includegraphics[width=0.8\textwidth]{Figures/facet_counts_ai.eps}
        % \includesvg[width=0.8\textwidth]{Figures/facet_counts_ai.svg}
        \label{fig:gpt_cleverenss}
    }
    \caption{Comparison between linguistic marker use from humans (top) and \textsc{gpt-4o-mini} (bottom), as assessed by \textsc{gpt-4o}. A rating of 1 indicates the feature is absent in the response, 2 indicates it is present.}
    %For brevity, we have only included ratings from 
    % \textsc{gpt-4o} and explanations from \textsc{gpt-4o-mini}, additional results are included in the supplementary materials
    \label{fig:liwc_analysis}
\end{figure*}

\subsection{Discussion}
Our findings highlight key differences in how LLMs reason about creativity as compared to humans. While human judges appeared to more strongly associate originality with cleverness, the opposite pattern emerged in LLMs, where originality was more strongly associated with remoteness and uncommonness. Though one might be inclined to trust the LLM originality evaluations more, given their stronger correlation with the true ratings, this should be balanced against the LLMs' apparent inability to distinguish among cleverness, remoteness, or uncommonness while rating. Human originality judgments of each facet never perfectly correlated with each other nor with originality regardless of condition, which reflects the conceptualization of these facets as impacting originality while not being the same construct. This lies in stark contrast to the much stronger facet correlations with originality that emerged in LLMs --- in some cases nearing perfect correlation with originality in the example condition. It is especially noteworthy that differences in facet correlations appeared washed out by the examples, even as model predictions became more accurate. Thus, while LLM originality scores had stronger predictive validity, they also had weaker construct validity due to a homogenization of cleverness, remoteness, and uncommonness, which unlike for humans was only strengthened by the presence of the examples. 

Our analysis of LLM explanations pointed to similar discrepancies between humans and AI. LLMs exhibited less diversity in explanation styles, with much more heavily skewed distributions of linguistic markers as compared to humans. This mirrors both the distributions of Likert originality scores, and similar findings in other areas of creativity and the social sciences, where generative AI has been found to suffer from less diversity in generations regardless of temperature or hyperparameter settings \citep{park2024diminished}. This trend may have partially been driven by our use of a low temperature value, though we note that exact duplicate explanations were uncommon even for the same problem (especially for \textsc{gpt-4o-mini}), making it unlikely this is the sole reason for redundancy. LLMs did align with humans along several linguistic markers, with both tending to not employ perceptual details and using less future-focused language in the no example condition as compared to the example condition, though not all such trends were significant, and LLMs did not closely align with humans for the majority of the linguistic markers explored. It appears that, like with humans, the examples qualitatively impacted LLM explanations, leading to shifts in the content of the explanations as opposed to the no example, which is an important consideration when LLMs are used to evaluate creativity in the real world.



