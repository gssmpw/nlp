Our first study sought to understand the key factors underlying human expert evaluation of the creativity of solutions to design problems (DPT) items. A participant in this task is given a scientific or engineering problem (e.g., increasing the use of renewable energy) and is instructed to come up with as many novel solutions to the problem as they can think of. Similar to expert-level science, the best solutions are both original and feasible, though unlike other STEM assessments the DPT benefits from but is not contingent on expertise to come up with creative ideas. The greater complexity of DPT responses compared to those from other creativity tests and its relationship to scientific creativity more broadly makes it a strong choice for our analysis. Unlike prior studies, which often have experts rate only the originality or quality of products, we instead ask our raters to provide fine-grained assessments of cleverness (whether the solution is insightful or witty), remoteness (whether the solution is ``far'' from everyday ideas), and uncommonness (whether the solution is rare, given by few people) in addition to originality, each of which is thought to influence ratings of creativity \citep{silvia2008assessing}. These assessments are performed both with and without the presence of example creativity ratings to DPT items, enabling us to examine how added context affects the evaluation process. Finally, we ask experts to briefly explain their originality scores, enabling us to employ methods from computational text analysis to probe the cognitive processes experts employ when rating and how such processes may be modulated by added context.

\subsection{Methods}
We use the data from \citet{Patterson2025}, who obtained more than 7000 responses to DPT items from undergraduate STEM majors. Each response was rated for originality using a five-point Likert scale by at least three expert raters with formal training in engineering. We drop items that did not obtain at least one rating from every point of the scale (certain items never had a response that received a five). We convert Likert scores into factor scores, as this has been shown to provide more accurate creativity ratings \citep{silvia2008another}, and we treat these factor scores as the true originality scores of each response.

We recruit 80 participants on Prolific to provide finegrained creativity ratings to DPT responses, requiring that they have a bachelor's degree or higher in a STEM field and are fluent in English. We split participants into two conditions: a \textit{no example} condition where participants are given responses to rate without any additional context, and an \textit{example} condition where participants are first shown example solutions with originality scores for responses to the same prompt being rated. We pull three example solutions from the same dataset while ensuring that participants never rate them. We include a solution with a score of one, one with a score of three, and one with a score of five, to avoid biasing participants towards either end of the scale. We first have each participant rate for originality following the same procedure, instructions, and facet definitions as \citet{Patterson2025}. After rating originality, participants in both groups then provide 1-2 sentences explaining their rating process \citep{orwig2024creative}, and they finish by rating the uncommonness, remoteness, and cleverness of the response using a five-point Likert scale for each. We instruct participants to be specific in their explanations, to draw on their domain expertise as holders of a STEM degree, and to avoid overly simplistic explanations (e.g., ``it's not original'' or ``it's an obvious answer''). We define a good explanation as being at least one sentence long and including specific details from the participant's prior experience, the response, or the examples (if applicable). We also provide definitions of uncommonness, remoteness, and cleverness for the final rating task, emphasizing that each facet is related while being distinct from originality. We include educational background and AI use checks at the end of the survey.

We administer each participant 15 DPT responses at random. To encourage high-quality explanations, we offer \$20 per hour to complete a 30-minute study. We exclude participants with an approval rating of less than 90\%, who report using AI to complete the task, or who report an education level lower than the minimum specified on Prolific. We also exclude participants who were exceptionally slow or fast (with a completion time further than three standard deviations from the mean), who gave the same rating for every response, or who did not follow our instructions for formatting explanations (as checked by a research assistant). This resulted in a final sample size of 37 participants and 481 ratings in the example condition and 35 participants and 455 responses for the no example.

% out of the full archival set

When examining the participants' explanations, we employ an analysis plan similar to \citet{orwig2024creative}, who used LIWC to analyze explanations of originality scores for AUTs. However, recent work has found that LLMs can predict psycholinguistic features of text more strongly than LIWC, even zero shot \citep{rathje2024gpt}. Therefore, we use LLMs to automatically rate linguistic markers in the explanations. We instruct LLMs to rate for the following variables: 

\begin{itemize}
    \item \textit{Past/future expressions}: Is the explanation past-focused or future-focused in its evaluation of the response?
    \item \textit{Perceptual details}: Does the explanation focus on the process of perceiving (``observe'', ``seen'', ``heard'', ``feel'', etc.)?
    \item \textit{Causal/analytical}: Does the explanation involve a structured evaluation of the response, evidencing an analytical process, or is the explanation more intuitive in its justifications?
    \item \textit{Comparative}: Does the explanation make explicit references to standards or examples or compare the response to other ideas?
    \item \textit{Cleverness}: Does the explanation refer to the cleverness, wittiness, shrewdness, or ingenuity (or lack thereof) of the response?
\end{itemize}

Both past/future language use and perceptual details have been explored to assess cognitive strategies employed on other creativity tests \citep{orwig2024creative}. We elect to use causal/analytical, comparative, and cleverness linguistic markers to aid in assessing whether participants employed a more structured process --- which might be evidenced by causal/analytical or comparative language use --- or a more intuitive process, as evidenced by language indicating sensory experiences or other ``gut reactions'' (e.g, ``it feels like a clever idea''). These linguistic markers also map onto the finegrained facets participants were asked to rate, with cleverness language mapping onto cleverness and comparative language mapping onto remoteness and uncommonness (as both remoteness and uncommonness often require making references to prior solutions). We use both \textsc{claude-3.5-sonnet}\footnote{https://www.anthropic.com/news/claude-3-5-sonnet} and \textsc{gpt-4o}\footnote{https://openai.com/index/hello-gpt-4o/} to check for reliability in ratings and avoid biases specific to a single LLM, though due to space constraints we mainly report results from \textsc{gpt-4o} as this is the model \citet{rathje2024gpt} validated. To encourage deterministic output, we set the temperature for both models to $0$ and top P to $1$. We instruct LLMs to rate each facet and provide a binary evaluation of whether the explanation does or does not contain the feature. Prompts are provided in the supplementary materials.

% We drop any explanations for which the model fails to follow this instruction.



% Specifically, we have each participant first rate for originality following the same procedure as \TODO{cite the paper that obtained the gold scores}, after which they are instructed to provide 1-2 sentences explaining their originality rating. We instruct participants to draw on their domain expertise while responding, asking them to consider if they have observed similar solutions to the problem in the past, to help ensure that the explanation is grounded in the originality of the solution and not merely its quality. Finally, participants rate the uncommonness, remoteness, and cleverness of the solution using separate five-point Likert scales, to disentangle how each facets contributes to the final creativity score. Figure \TODO{make it} shows the experimental interface, we collect data using Qualtrics and give 30 minutes to complete the task. Participants are split into two conditions: a \textit{no oracle} conditions where participants are shown DPT solutions without any additional context (equivalent to how creativity ratings are typically solicited), and an \textit{oracle} condition where participants are first shown example solutions with originality scores for responses to the same prompt being rated. We pull these solutions from the same dataset, while ensuring that participants never rate them. At the end, participants complete a demographic questionnaire and a check for use of generative AI during the study.

% The design problems task (DPT) is a test of domain-general creative problem solving and divergent thinking in science and engineering \TODO{cite}.   Further, creativity evaluation also hinges on weighing multiple competing factors: a highly uncommon solution may still receive a poor creativity score if it is not especially clever or could not be feasibly put into practice. This makes the DPT a strong testbed for our experiments: evaluation is more complex than purely theoretical creativity tests like the alternative uses task \TODO{cite}, yet it is not so challenging as to require domain experts to obtain meaningful creativity scores, enabling us to recruit a larger pool of participants.

% The task targets STEM undergraduate students; a general understanding of science and engineering is beneficial but not necessary for the task.

% Our goal is to obtain \textit{finegrained} creativity assessments for these responses, to better understand how each facet of creativity influences a raters final score, and to provide explanations for why responses are assigned a particular rating. 

% \TODO{define the research questions and null hypotheses}

\subsection{Results}
We begin by examining inter-correlations among all facets (cleverness, remoteness, uncommonness) and correlations between each facet and originality for both conditions. Results are in Figure \ref{fig:experiment_1_correlations}. As expected, each facet is moderately correlated with originality as well as each other, with Pearson r in the range 0.45--0.67 (all correlations are significant).\footnote{Results from all correlational analysis in both studies were similar using Spearman $\rho$.} Comparing the example to no example conditions, we see an increase in correlation between originality and cleverness and a decrease in correlation between originality and both remoteness and uncommonness. Changes in correlation across conditions were significant for cleverness-remoteness (Fisher's z = 2.83, p $<$ 0.01), remoteness-uncommonness (z = -4.61, p $<$ 0.001), and remoteness-originality (z = -2.96, p $<$ 0.01), but were insignificant for all other comparisons. Notably, the presence of the examples did not make experts significantly more accurate in their evaluations of originality, with correlations in the moderate range for both conditions (no example r = 0.44, example r = 0.47).

% We report descriptive statistics for all Likert evaluations in Table \TODO{make it}, broken down by condition.

% Examining the distribution of cleverness more closely (the only facet to become more strongly related to originality in the oracle condition), we plot the distribution of cleverness scores for both conditions in Figure \ref{fig:experiment_1_cleverness}. 

% Participants in the oracle condition appear to be stricter judges of cleverness, giving more 1 or 2 rating than their no oracle counterparts, though this difference was only marginally significant (Mann Whitney U test = 102948.5, p $<$ 0.1).

\begin{figure}[htb]
    \centering
    \footnotesize
    \includegraphics[width=0.8\linewidth]{Figures/Correlations_human.eps}
    % \includesvg[width=0.85\linewidth]{Figures/Correlations_human.svg}
    \caption{Pearson correlations among pairwise Likert ratings for both conditions. o = originality, c = cleverness, u = uncommonness, r = remoteness.}
    \label{fig:experiment_1_correlations}
\end{figure}

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=1\linewidth]{Figures/cleverness.png}
%     \caption{Distribution of cleverness ratings for both conditions.}
%     \label{fig:experiment_1_cleverness}
% \end{figure}

Turning to participant explanations, \textsc{gpt-4o}'s ratings did not reveal significant differences per condition for perceptual details, past/future language use, or cleverness, but differences are significant for both causal/analytical language (Mann-Whitney U = 78039.5, p $<$ 0.05) and comparative language (U = 75627.5, p $<$ 0.01) with the example condition using less comparative and causal/analytical language than the no examples. Distributions for linguistic markers are shown in Figure \ref{fig:liwc_analysis}. \textsc{claude-3.5-sonnet}'s ratings generally agreed with \textsc{gpt-4o} (Cramer's V in the range 0.549--0.798) with the only notable departure being that \textsc{claude-3.5-sonnet} found no significant difference in causal/analytical language between the conditions (U = 75076.5, p $<$ 0.5). We report additional linguistic marker analysis in the supplementary materials.

% We report model agreement statistics and linguistic marker analyses in the supplementary files.

\subsection{Discussion}
As expected, the facet ratings for cleverness, remoteness, and uncommonness did not perfectly correlate with each other nor with originality, implying that participants do not weigh each facet equally when assessing originality. Further, correlations changed by a significant degree when including example ratings, with both remoteness and uncommonness becoming weaker predictors of originality and cleverness becoming a stronger one. Given that participants in the no example condition needed to actively retrieve example solutions from memory when evaluating, a possible explanation is that this retrieval process biased them towards placing stronger emphasis on the remoteness and uncommonness of the response in relation to solutions they had seen in the past, while example participants would not need to focus as much effort on thinking of prior solutions and could instead focus on the cleverness of the idea. Notably, participants in both groups did not differ significantly in terms of education, making it unlikely this effect could be explained as a skill confound. The idea that participants in the example condition were biased toward cleverness rather than the other facets was also partially supported by their explanations, as no example participants used significantly more comparative language than example participants. Given that assessing remoteness or uncommonness often requires making direct comparisons to prior solutions, it makes sense that an evaluation rooted around these facets would contain more comparisons than an evaluation rooted around cleverness, which is more readily evaluated in isolation (e.g., whether the idea is resource efficient, not immediately obvious, etc.).