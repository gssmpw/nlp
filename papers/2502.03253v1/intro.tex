How do people evaluate and reason about creativity? In science and engineering, creativity assessment has traditionally relied on human experts to evaluate everything from grant proposals and scientific manuscripts to new technologies and engineering designs. Although experts routinely make these high-stakes decisions by weighing factors such as novelty and technical feasibility, the cognitive processes and biases that shape their evaluations are poorly understood. This challenge takes on new importance as artificial intelligence (AI) systems, particularly large language models (LLMs) \citep{brown2020language,ouyang2022training}, increasingly assume advanced roles in scientific research and innovation, from idea generation to peer review \citep{boiko2023emergent, d2024marg, lu2024ai,si2024can, wang-etal-2024-scimon}. LLMs can achieve impressive accuracy in predicting human creativity assessments, yet we know little about how they arrive at their judgments, what features they prioritize, or whether their evaluation strategies align with those of human experts. Understanding these cognitive and computational processes is crucial not only for advancing creativity research but also for developing AI systems that can better explain and steer creative evaluation in STEM.

Modern creativity assessment is based on the Consensual Assessment Technique (CAT) \citep{amabile1982social,silvia2008assessing}, which uses human judgments from experts to reach a consensus on the creativity of a product/idea, often by assessing its originality or quality. In the standard implementation of this method, expert raters independently evaluate creative products without specific scoring criteria, relying on their implicit domain understanding. While this approach has demonstrated remarkable reliability and predictive validity --- CAT ratings predict real-world creative achievements across multiple domains \citep{silvia2008assessing} --- it typically relies on global originality scores that may mask underlying evaluation processes \citep{Cseh2019}. Notably, originality itself can be understood as an aggregation of three distinct facets: uncommonness, remoteness, and cleverness \citep{silvia2008assessing}, though their relative contributions to expert judgments remain unclear. Understanding these evaluation patterns is especially crucial for STEM domains, where creative solutions must carefully balance novelty with real-world technical constraints. Even studies of simpler ideation tasks like the Alternate Uses Task (AUT) suggest this complexity, showing how different factors like novelty and appropriateness contribute distinctly to creativity judgments \citep{Diedrich2015}.

% The CAT has emerged as the gold standard for creativity assessment across domains, from artistic works to scientific innovation.

The cognitive processes underlying creativity evaluation are becoming clearer through recent empirical work. Early think-aloud studies like \citet{gilhooly2007divergent} focused on idea-generation processes in the AUT, showing how participants move from memory retrieval to more abstract strategies. More recently, \citet{orwig2024creative} used linguistic inquiry and word count (LIWC) analysis, a computerized text analysis method that quantifies psychological dimensions of language \citep{tausczik2010psychological}, to analyze how participants explain their originality ratings on the AUT. Results revealed that even when judges agree on creativity scores, they often employ different cognitive processes in their evaluations, as evidenced by variations in their use of memory-related terms, temporal focus (past vs. future orientation), and analytical language.

STEM creativity represents a crucial yet understudied domain for creativity assessment. Although extensive research has examined scientific hypothesis generation and engineering design thinking, we know surprisingly little about how experts evaluate creative merit in STEM contexts, where ideas must balance novelty with technical feasibility. To assess creative thinking in STEM, \citet{Patterson2025} developed a novel Design Problems Task (DPT) that measures the ability to generate solutions to real-world STEM challenges, capturing the dual constraints that characterize expert-level scientific and engineering creativity without the need for expert level knowledge to solve the items. The DPT spans three domains: ability difference and limitations (e.g., assisting people with learning impairments), transportation and mobility (e.g., reducing traffic congestion), and social environments and systems (e.g., improving access to clean water), with participants generating multiple solutions that are then rated for both originality and effectiveness.

The demanding nature of creativity evaluation, requiring expert raters to assess thousands of open-ended responses while maintaining consistent judgment standards, has sparked interest in automating assessment using AI. Recent work has demonstrated that LLMs trained on human creativity ratings can achieve impressive accuracy in evaluating novel responses \citep{organisciak2023beyond,Patterson2025}. Like human experts, these models may be influenced by contextual information and examples provided during evaluation. However, these AI evaluators present their own interpretability challenges. While AI may agree with human ratings, we know little about how they arrive at their judgments, what features of responses they attend to during evaluation, or whether those features are similar to those attended to by human experts. Understanding AI creativity evaluation mechanisms for STEM-related tasks has become increasingly urgent as LLMs take on expanded roles in scientific research, from peer review \citep{huang2025large,lin2024evaluating} to idea generation \citep{gu2024interesting,si2024can}. If these models are to serve as reliable creativity judges, we must better understand their evaluation processes and potential biases. This knowledge could inform both model interpretability efforts and attempts to align AI creativity assessment with human judgment. Moreover, improving evaluation capabilities may enhance idea generation abilities, consistent with cognitive models that link these processes \citep{smith1995creative}. 

% Some models even show strong correlation with human judges in zero-shot settings \TODO{cite}, suggesting they may capture key aspects of human creativity assessment.
% --- a parallel to few-shot learning in AI systems

The present research examines how human experts and LLMs evaluate STEM creativity through real-world design problems. In two studies, we conduct a fine-grained analysis of the factors influencing creativity assessment in STEM domains. First, we examine how human experts evaluate originality in STEM solutions, analyzing both their numerical ratings and their written explanations to understand the cognitive processes involved. We also investigate how providing examples impacts expert judgment, testing whether contextual information (example design solutions and originality ratings) changes how experts weigh different aspects of creativity. Second, we perform a parallel analysis of LLMs, examining whether these models show similar patterns in their creativity evaluations and exploring potential differences between human and AI assessment strategies.\footnote{Code and data to reproduce our results is available on OSF: https://tinyurl.com/cogsci2025}

% Understanding how humans reason about the creativity of ideas or products is a foundational question for understanding divergent thinking. Modern creativity assessment is based around the consensual assessment technique (CAT) \TODO{cite}, which uses human judgments to reach a consensus about the originality or quality of a creative product. Though the CAT is a well validated method --- having shown great success in both scoring creativity for a variety of cognitive tasks \TODO{cite} and in finetuning large language models (LLMs) to automatically score originality and quality \TODO{cite} --- it provides only limited information for understanding the factors underlying why judges perceive ideas as creative or not. Notably, creativity can be judged as an aggregation of three distinct facets: uncommonness, remoteness, and cleverness, each of which plays a distinct role in evaluation \TODO{cite}. Yet in typical CAT studies, judges are not asked to rate each of these facets separately, nor do they provide explanations for their creativity assessments. This becomes problematic when judges have equally plausible yet conflicting conclusions for an ideas creativity. For example, \TODO{cite Rogers work on AUTs with explanations} found that when participants were asked to explain the originality of ideas for an alternative uses task, their individual assessments of originality could be dramatically different from each other even if the scores obtained from the CAT had good rater agreement. \TODO{think there needs to be a bit more here}

% Such differences in rater perceptions of creativity are not easily resolved in high stakes contexts like the assessment of scientific or engineering ideas, where evaluation of novelty through peer review is crucial for ensuring that high quality research is published \TODO{cite}. Critically, though domain expertise is known to be beneficial for improving the validity of creativity assessments \TODO{cite}, expertise alone does not guarantee that scientific ideas will be assessed effectively \TODO{cite}... \TODO{continue outlining problems with modern peer review, as it relates to creativity assessment more broadly}.

% An emerging trend in creativity assessment is the use of AI, especially LLMs \TODO{cite}, to score originality in both novice \TODO{cite} and expert \TODO{cite} scientific contexts. LLM originality ratings are known to correlate well with humans on assessments of divergent thinking \TODO{cite} and in STEM assessments for scientific and engineering design \TODO{cite}. LLMs have also been thoroughly explored for their potential to both generate and evaluate novel scientific ideas...\TODO{finish} Yet whether and to what extent AI can serve as an expert evaluator of creativity remains unclear...\TODO{discuss current problems with AI creativity evaluation, framing the issues we aim to solve}.

% Improving creativity evaluation, especially areas requiring deep domain expertise like science and engineering, would benefit greatly 