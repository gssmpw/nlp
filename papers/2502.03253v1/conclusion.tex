Understanding how AI reasons about the creativity of products and whether that reasoning process aligns with human experts becomes ever more paramount as AI assumes the role of idea evaluators in complex domains like science and engineering. Although many works have studied LLM's ability to accurately rate originality across various tasks \citep{lin2024evaluating,schmidgall2025agent}, the coarse-grained nature of such evaluations makes drawing conclusions about their rating process difficult. We contribute to this literature by collecting finegrained originality evaluations of responses to science and engineering prompts from both human experts and LLMs. Our analysis reveals substantial differences in how these populations both rate for originality in relation to other facets and in how they structure explanations of their rating process. It appears that how LLMs are affected by context and how they rate individual facets of originality is markedly different from human judges, carrying important implications for deploying LLMs as evaluators in real-world applications. We also release our dataset of finegrained evaluations to facilitate future research on LLM alignment for this key area.

Future work can expand on our contributions in several ways. Due to budget constraints, we were unable to run multiple pairwise comparisons between humans and LLMs under multiple instruction sets and prompt variations, which is an important step for quantifying the sensitivity of LLM ratings to the input prompt. Though we mitigated this by using multiple LLM evaluators, it remains possible that our results may have been driven in part by the structure of the prompt. Similarly, due to time constraints in human studies, we selected only a small number of examples and were unable to run studies that varied either the examples themselves or their order of presentation. It is possible that this could have biased LLM judgments of cleverness, uncommonness, and remoteness, given that we were unable to include multiple examples at each originality rating. Though we do not believe this could fully explain the homogenization effect, since correlations among the facets were already stronger than in humans zero shot, it remains an important analysis to help us further understand this effect.

Creativity is often considered one of the most critical skills to master in modern economies \citep{illessy2020automation,tsegaye2019antecedent}, and AI must be developed to evaluate it both accurately and fairly. Achieving this goal requires understanding how AI arrives at creativity judgments and whether it rates individual facets of creativity in the same way as humans. We hope our work provides insights into how AI reasons about creativity and serves as a call to action to perform similar finegrained assessments of creativity in other domains.