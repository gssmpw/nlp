\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{caption}
\usepackage[justification=centering]{subcaption}
\usepackage{bbm}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{enumitem} %for no spaces between items 

\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[arxiv]{arxiv_style}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\arxivtitlerunning{Learning to Fuse Temporal Proximity Networks: A Case Study in Chimpanzee Social Interactions}

\begin{document}

\twocolumn[
\arxivtitle{Learning to Fuse Temporal Proximity Networks:\\A Case Study in Chimpanzee Social Interactions}
\arxivsetsymbol{equal}{*}

\begin{arxivauthorlist}
\arxivauthor{Yixuan He}{asu}
\arxivauthor{Aaron Sandel}{uta}
\arxivauthor{David Wipf}{Amazon}
\arxivauthor{Mihai Cucuringu}{UCLA,UnivOx}
\arxivauthor{John Mitani}{umich}
\arxivauthor{Gesine Reinert}{UnivOx,turing}
\end{arxivauthorlist}

\arxivaffiliation{asu}{School of Mathematical and Natural Sciences, Arizona State University, Phoenix, AZ, United States}
\arxivaffiliation{uta}{Department of Anthropology, University of Texas at Austin, Austin, TX, United States}
\arxivaffiliation{Amazon}{Amazon Web Services AI Shanghai Lablet, Shanghai, China}
\arxivaffiliation{UCLA}{Department of Mathematics, University of California Los Angeles, Los Angeles, CA, United States}
\arxivaffiliation{UnivOx}{Department of Statistics, University of Oxford, Oxford, United Kingdom}
\arxivaffiliation{umich}{Department of Anthropology, University of Michigan, Ann Arbor, MI, United States}
\arxivaffiliation{turing}{The Alan Turing Institute, London, United Kingdom}

\arxivcorrespondingauthor{Yixuan He}{Yixuan.He@asu.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\arxivkeywords{Machine Learning, Network Time Series, Time Series Modeling, Network Analysis, Animal Social Networks, Node Similarity, Clique Detection}

\vskip 0.3in
]


\printAffiliationsAndNotice{}%\arxivEqualContribution} % otherwise use the standard text.

\begin{abstract}
How can we identify groups of primate individuals which could be conjectured to drive social structure? To address this question, one of us has collected a time series of data for social interactions between chimpanzees. Here we use a network representation, leading to the task of combining these data into a time series of a single weighted network per time stamp, where different proximities should be given different weights reflecting their relative importance. We optimize these proximity-type weights in a principled way, using an innovative loss function which rewards structural consistency across time. The approach is empirically validated by carefully designed synthetic data. Using statistical tests, we provide a way of identifying groups of individuals that stay related for a significant length of time. Applying the approach to the chimpanzee data set, we detect cliques in the animal social network time series, which can be validated by real-world intuition from prior research and qualitative observations by chimpanzee experts.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
What drives social structure in primates? To address this, one of us has collected a rich data set on chimpanzees in Uganda. Starting in 1998, the data set records proximities between chimpanzees when observed. Due to visibility issues in the tropical forest and limits on the time that researchers can spend in the forest recording chimpanzees data, only a small number of these proximities have been observed. To analyze these social proximity data we choose a network representation; a key contribution of this paper is a principled way of obtaining such a representation. 

Social networks are an important tool in the social and biological sciences, where individuals are treated as nodes connected by some interactions which are treated as possibly weighted edges between nodes; see for example~\citet{wasserman1994social}. Creating networks based on various interactions is useful for modeling a range of dynamics, such as disease spread and information transfer. Networks are also valuable for determining social structures. Although social networks have featured prominently in sociology and, in the last two decades, animal behavior, see for example  \citet{pinter2014dynamics}, several  challenges persist: What behaviors should be used to construct networks? How are appropriate weights determined? 
These issues are compounded when data interactions of different types are available; here we are thinking of many proximity records with different proximity ranges. 


Multilayer networks are one possibility for representing such records, as in  \citet{kivela2014multilayer}, but such a representation may not be easy to interpret and may obfuscate that different proximities are related.
The situation becomes even more intricate when a time series of proximity data is available, as in the chimpanzee case study which motivates our work. As will be described in more detail in Section \ref{sec:chimp_data_description}, the animals form a variety of social groupings which change throughout the day. The same is true for other species with fission-fusion social dynamics, including humans and bonobos~\citep{silk2014importance, classen2016fission, ramos2018quantifying}. A biological research question is then to identify groups of individuals which are close to each other at multiple times; in a human data set one might interpret these as friendship groups. 


In order to address this question, we  combine the proximity data into a single network per time step,  addressing several methodological questions, namely: (1) How do we combine the available proximity data into one network per time step? 
(2) How do we identify groups of individuals which are close to one another more often than expected by chance?


\looseness=-1 Traditional methods for constructing animal social networks often rely on statistical models~\citep{farine2015constructing,brask2024introduction}, or on reference distributions obtained via randomizations \citep{hobson2021guide}. Instead, we take inspiration from network/graph optimization to model relationships between nodes/entities~\citep{Zhou, he2024robust}, by devising a novel loss function which we optimize. By leveraging different levels of proximity data collected over extended periods, our novel optimization approach is designed to capture the underlying consistency of social relationships. To our knowledge, there is no existing work on how to combine multiple networks representing nested levels of proximity into a single network. Here, we apply optimization concepts to create social networks of chimpanzees by combining networks with nested proximity levels and comparing them over time. Given the dynamic nature of chimpanzee groups, the resulting time series of networks provides a useful system to address these questions. 

Our key contributions are summarized as follows:
\vspace{-1mm}
\begin{itemize}[noitemsep, nolistsep, leftmargin=*]
    \item We propose a novel optimization pipeline to represent real-world multi-level proximity data using (multiplex) networks. The construction pipeline is empirically validated by a carefully designed set of synthetic data.
    \item We provide theoretical contributions on two notions of individual similarities across time based on sequences of Bernoulli trials with evolving success rates. The analysis can be applied to detect long-term close relationships.
    \item We apply the novel network combination/fusion pipeline to present an effective network time series representation of proximity data in a wild chimpanzee social group across time. Using additionally the similarity notions, we show that there is sufficient information in these data to identify groups of chimpanzees which stay in each other's wider community for a surprisingly large amount of time. 
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Literature Review}
Combining or fusing networks/graphs that represent different views into a unified structure is a well-studied problem relating to our proximity network fusion setting. \citet{kang2020multi} propose a multi-graph fusion model that simultaneously performs graph combination and spectral clustering. The idea that multi-view data admit the same clustering structure aligns with the structural consistency assumptions in our proposed method. However, this method, together with other multi-view fusion approaches such as \citet{yang2019adaptive} and \citet{yang2024bidirectional}, cannot naturally consider structural consistency over time, but rather focuses on fusing multiple views into a unified static graph. \citet{zhang2021adaptive} and \citet{hu2022spatio} leverage spatio-temporal fusion to address challenges in predicting remaining useful life and in trajectory data analytics, respectively. However, their fusion modules, being within a neural network architecture, cannot be readily applied to combine multiple levels of proximity, as in our case.

Node similarity in network time series is another key topic in our paper. \citet{gunecs2016link} consider neighborhood-based node similarity scores for link prediction. \citet{yang2019time} develop a diffusion model to drive the dynamic evolution of node states and proposes a novel notion of dissimilarity index. The approach in \citet{meng2018coupled} learns node similarities by incorporating both structural and attribute-based information. However, none of them considers node similarity based on long-term close relationships.

Detecting long-term relationships in dynamic systems has been explored from various perspectives. In long-term studies of primate relationships, scientists report how long certain pairs have a high frequency of interaction, but they do not provide a statistical approach to determine what constitutes a persisting relationship. For example,  \citet{mitani2009male} emphasizes the significance of long-term affiliative relationships in social mammals by considering pairwise affinity indexes between male dyads~\citep{pepper1999general} to quantify long-term relationships, but the indexes neglect consecutive proximities in the time series. \citet{derby2024female} present a Bayesian multimembership approach to test what factors predict the persistence of proximity relationships, but persistence is defined deterministically by being in proximity for more than a fixed length of time period without considering hypothesis-testing based on an expected duration. \citet{qin2019mining} investigate the interplay between temporal interactions and social structures, but their approach is constrained to analyzing periodic behaviors. \citet{escribano2023stability} study the stability of personal relationship networks in a longitudinal study of middle school students by exploring persistence of circle structures, which is different from our novel perspectives of node similarity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation: Chimpanzees in Uganga}
\label{sec:chimp_data_description}
\textbf{Problem Definition.} Denote a static undirected weighted network (graph, used interchangeably) as $\mathcal{G}=(\mathcal{V}, \mathcal{E}, w),$ with $\mathcal{V}$ the set of nodes, $\mathcal{E}$ the set of edges encoding node relationships, and $w \in [0, \infty)^{| \mathcal{E}|}$ the set of edge weights. Such a network can be represented by the adjacency matrix $\mathbf{A} = (A_{ij})_{i,j \in \mathcal{V}}$,  
with $\mathbf{A}_{ij}=0$ if no edge exists from $v_i$ to $v_j$; if there is an edge $e$ between $v_i$ and $v_j$, we set $A_{ij} = w_{e}$, the edge weight. A \emph{network time series} is a time series of networks,  $\{\mathcal{G}^{(t)}\},$ where for each time step $t,$ the network is static, and the set of nodes is the same for all time steps. At each time step, a \emph{multiplex network} is constructed with each type of interaction between the nodes being described by a single-layer network; the different layers of networks describe the different modes of interaction.
We denote the multiplex network time series with $H$ layers by $\{\mathcal{G}^{(h, t)}\},$ where $h\in\{1, \dots, H\}$ refer to the different types/layers.

\textbf{Data Description.} Our methods are motivated by a unique data set on chimpanzees one of us collected over nearly three decades. This data set concerns a community of wild chimpanzees in Ngogo, Kibale National Park, in Uganda. Long-term research began at Ngogo in 1995, and chimpanzees, especially adult males, were habituated to human observers starting in 1998. The social relationships of these chimpanzees have been studied for 30 years, see for example \citet{langergraber2017group}. The chimpanzees at Ngogo were part of one social group, but chimpanzees exhibit ``fission-fusion'' social dynamics, so they are rarely, if ever, in the same place at the same time. Instead, they form temporary associations that change throughout the day. Prior studies have identified variable structuring within chimpanzee groups~\citep{badihi2022flexibility, mitani2003social}. In our case, the total number of individuals varies from 150 to 200 per year. The data set we used in this paper relies on 24 years of data on 219 individuals (77 adult male chimpanzees who were the focus of behavioral observations, and 142 additional chimpanzees--20 males and 122 females--that were not the focus of direct observation, but interacted with the focal subject during observations). Data were collected every year except 2020, when COVID-19 restrictions prohibited this activity.
Field seasons typically occurred during two or three consecutive months per year. Observations targeted a subset of adult male chimpanzees. The behavioral observation procedure involved following one ``focal" male for an hour and recording three main social interactions: (1) ``party": all chimpanzees that were in social/spatial association (within ~100m of the focal subject during the hour session); (2) ``proximity": chimpanzees within physical proximity of the focal subject (within 0-2m or 2-5m) recorded at 10-minute intervals; and (3) ``grooming": chimpanzees involved in grooming with the focal subject during the hour sampling period (which should be within 2m of the focal subject). After the hour of observation, another focal subject was selected. Several other behaviors were also recorded (e.g., territorial patrols, hunting, aggression, self grooming). This is a standard sampling method in primate behavior, see \citet{altmann1974observational}. 


\begin{figure}[htb!]
    \centering
\includegraphics[width=0.93\linewidth, trim={0 0 0 0}, clip]{figures/chimps_proximity_types.pdf}
\vspace{-5mm}
    \caption{\looseness=-1 Proximity in chimpanzees. F is a focal male, A, B, and C are in ``prox2" to F, D, E, G, and H are in ``prox5" to F, while the rest are in ``party" to F. In particular, B is grooming with F.}
    \vspace{-5mm}
    \label{fig:proximity_types}
\end{figure}
\textbf{Proximity Types.} The data set contains information on the following 10 types of proximities. Let ``prox2" and ``prox5" denote that an individual is within 2m (two meters) or 2-5m of a certain focal chimpanzee, respectively. Let ``party" denote an individual in the social/spacial association (within roughly 100m) of a focal subject but not within 5m apart. In Fig.~\ref{fig:proximity_types} we illustrate different types of proximity levels. Here we use F to abbreviate a focal male, A, B, and C are in ``prox2" to F, D, E, G, and H are in ``prox5" to F, while the rest are in ``party" to F. In particular, B is grooming with F. We can derive ten proximity types as detailed in Appendix (App.)~\ref{app_sec:proximity_types}, where Type 1 indicates the relationship between two individuals in the ``party" of the focal subject, Types 2--4 represent other ``party"-related relationships, and Types 5--10 encode proximities within 5m to a focal subject. In particular, Types 8--10 are within 2m to a focal object, and Type 10 means two individuals are grooming each other.

A natural question then arises: \emph{How do we use these data to construct informative networks for the entire group}? 

As the recorded data is binary and there are various types of interactions (proximities), we  start with a multiplex network representation. Based on these various levels of proximity, we construct networks based on single relationships, where every edge is given a unit edge weight for a single day if on that date an occurrence of that type is observed. The yearly edge weight is computed as the sum of the edge weights (0 or 1) obtained from all dates involved. As a result, for each year, we obtain 10 weighted single-relationship networks, one for each type. An example is provided for August 2006 in Fig~\ref{fig:separate_graphs200608} in App.~\ref{app_sec:extended_plots}. In this example, isolated nodes are omitted in the plots for the 10 single-relationship networks. 


In addition, multiple consecutive occurrences of a proximity type on the same date might be treated differently from separate occurrences on different dates. Therefore, for each single-relationship network, we construct an ancillary network to record multiple occurrences on the same day. For each ancillary network, an edge is added for each day if multiple occurrences are observed, with the number of consecutive occurrences minus one as the daily weight, while the total edge weight is the sum of the daily weights.

Another question naturally follows: \emph{Can we combine these networks effectively to describe the time series of chimpanzee proximities?} 


Indeed, we observe inherent hierarchies in the proximity levels based on the upper bound of pairwise differences described in each type, indicating that not every type of interaction should be given the same weights. Hence, we propose an approach that adds weights to each type of interaction and thus obtains a weighted network as a representation. 

\section{A Parametric Network Model}
Motivated by Sec.~\ref{sec:chimp_data_description}, suppose the ground-truth network $\mathcal{G}^{(t)}$ for each time step $t$ can be expressed by its adjacency matrix $\mathbf{A}^{(t)}$, then with $H$ hierarchies of proximity levels ($H=10$ for Sec.~\ref{sec:chimp_data_description}), we have $\mathbf{A}^{(t)} = \sum_{h=1}^H W_h \mathbf{A}^{(h, t)}$ with positive nondecreasing weights $W_h$'s, i.e., $0 < W_{h_1} \leq W_{h_2}$ if $h_1\leq h_2.$ Given the considerations for consecutive occurrences, we further split each $\mathbf{A}^{(h, t)}$ into a weighted sum of the raw network, $\mathbf{A}^{(\text{raw}, h, t)},$ and an ancillary (``add'') network, $\mathbf{A}^{(\text{add}, h, t)}.$ In the chimpanzee example, the add networks represent consecutive occurrences.


For the purpose of modeling, we further express the nondecreasing weights as a sum of nonnegative increment weights, $W_h=\sum_{j=1}^h w_j,$ resulting in
\begin{equation}
    \begin{aligned}
    \label{eq:adj_t}
    \mathbf{A}^{(t)} &= \sum_{h=1}^H W_h \mathbf{A}^{(h, t)}\\
    &=\sum_{h=1}^H (\sum_{j=1}^h w_j) (\mathbf{A}^{(\text{raw}, h, t)} + w_\text{add}\mathbf{A}^{(\text{add}, h, t)}),
    \end{aligned}
\end{equation}
so that $W_h$ is the network combination weight for the $h$-th hierarchy, $w_j=W_j-W_{j-1}\geq 0$ is the nonnegative increment from $W_{j-1}$ to $W_{j},$ $w_\text{add}\in[0,1]$ is the increment weight for a single network type when increments are considered, e.g., when consecutive occurrences of a certain type of proximity are recorded, $\mathbf{A}^{(h, t)}$ is the adjacency matrix for a single network type $h$ at time step $t$, $\mathbf{A}^{(\text{raw}, h, t)}$ is the raw adjacency matrix without considering added increments, and $\mathbf{A}^{(\text{add}, h, t)}$ is for the increments for each type. Here we set $w_1=W_1=1$ fixed by default for normalization. Note that we assume that $\mathbf{A}^{(\text{raw}, h, t)}$ and $\mathbf{A}^{(\text{add}, h, t)}$ are known, while $w_j$ and $w_\text{add}$ are learnable parameters. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Method: ProxFuse}

\label{sec:proposed_method}
We propose a simple optimization model to fuse proximity networks (termed \emph{ProxFuse}) to learn the increment weights, based on the assumption that the underlying graph structure should stay roughly stable over time. The adjacency matrix may evolve, but we assume that the underlying similarity structure between nodes (based on which the adjacency matrix is generated, but with possible magnitude fluctuations)  and the relative magnitudes of weighted node degrees should stay consistent. 

Let $\mathcal{N}^{(t)}$ denote the set of nodes that exist at time step $t\in \{ 1, 2, \ldots, T\}$; we assume $T \geq 2$ so that we have at least two time steps.
For each time step $t$, we first extract the set of nodes $\mathcal{N}^{(t, t+1)}=\mathcal{N}^{(t)}\cap\mathcal{N}^{(t+1)}$ which co-exist (i.e., both with nonzero degrees) at time steps $t$ and $t+1$. We then construct subnetworks based on $\Tilde{\mathbf{A}}^{(t, t, t+1)}=\mathbf{A}^{(t)}_{\mathcal{N}^{(t, t+1)}, \mathcal{N}^{(t, t+1)}}$ and $\Tilde{\mathbf{A}}^{(t+1, t, t+1)}=\mathbf{A}^{(t+1)}_{\mathcal{N}^{(t, t+1)}, \mathcal{N}^{(t, t+1)}}$; both matrices take values in $\mathbbm{R}^{\left\lvert\mathcal{N}^{(t, t+1)}\right\rvert\times \left\lvert\mathcal{N}^{(t, t+1)}\right\rvert}$. For each network, we construct a similarity graph $\mathbf{S}^{(t, t, t+1)}$ based on the adjacency matrix $\Tilde{\mathbf{A}}^{(t, t, t+1)}$ by taking into account the node similarities. Indeed, we treat each row $\Tilde{\mathbf{A}}^{(t, t, t+1)}_i$ in $\Tilde{\mathbf{A}}^{(t, t, t+1)}$ as a feature vector for node $i$ at time $t$, and compute the cosine similarity values between individuals, i.e., $\mathbf{S}^{(t, t,t+1)}_{i,j}=\frac{\Tilde{\mathbf{A}}^{(t, t, t+1)}_i \cdot \Tilde{\mathbf{A}}^{(t,t, t+1)}_j}{\left\lVert \Tilde{\mathbf{A}}^{(t, t, t+1)}_i\right\rVert_2 \left\lVert \Tilde{\mathbf{A}}^{(t, t, t+1)}_j\right\rVert_2},$ where the numerator takes the vector dot-product and $\lVert \cdot\rVert_2$ denotes the vector 2-norm. We can similarly compute $\mathbf{S}^{(t+1, t,t+1)}_{i,j}=\frac{\Tilde{\mathbf{A}}^{(t+1, t, t+1)}_i \cdot \Tilde{\mathbf{A}}^{(t+1,t, t+1)}_j}{\left\lVert \Tilde{\mathbf{A}}^{(t+1, t, t+1)}_i\right\rVert_2 \left\lVert \Tilde{\mathbf{A}}^{(t+1, t, t+1)}_j\right\rVert_2}.$ Based on $\mathbf{S}^{(t, t, t+1)}$ and $\mathbf{S}^{(t+1, t, t+1)},$ one objective is to minimize 
\begin{align}
\label{eq:loss_similarity} \mathcal{L}_\text{sim}=\frac{1}{T-1}\sum_{t=1}^{T-1}\sum_{i,j\in\mathcal{N}^{(t,t+1)}}\Big(\mathbf{S}^{(t, t, t+1)}_{i,j}-\mathbf{S}^{(t+1, t, t+1)}_{i,j}\Big)^2.
\end{align}
In addition, we assume that weighted node degrees remain consistent over time. To this end, we compute normalized weighted node degrees for time step $t$ as $d_i^{(t,t,t+1)}=\frac{\sum_j\Tilde{\mathbf{A}}^{(t,t, t+1)}_{i,j}}{\sum_{j,k}\Tilde{\mathbf{A}}^{(t,t, t+1)}_{j,k}}.$ Likewise, we can compute $d_i^{(t+1,t,t+1)}.$ We then obtain another term of loss function as
\begin{align}
    \label{eq:loss_deg}
    \mathcal{L}_\text{deg}=\frac{1}{T-1}\sum_{t=1}^{T-1}\sum_{i\in\mathcal{N}^{(t,t+1)}}\left(d^{(t, t, t+1)}_{i}-d^{(t+1, t, t+1)}_{i}\right)^2.
\end{align}
To penalize extreme combination weights, we additionally add a regularization term as
\begin{equation}
    \label{eq:loss_reg}
    \mathcal{L}_\text{reg}=\frac{1}{H}\left(\lVert w_\text{add}\rVert_2^2 + \sum_{h=1}^{H}\lVert w_h\rVert_2^2\right).
\end{equation}

To summarize, our optimization loss function amounts to
\begin{equation}
    \label{eq:total_loss}
    \mathcal{L}=\alpha_1\mathcal{L}_\text{sim} + \alpha_2\mathcal{L}_\text{deg} + \alpha_3\mathcal{L}_\text{reg}.
\end{equation}
The values of $\alpha_1, \alpha_2,$ and $\alpha_3$ are considered to be hyperparameters, which we set to be $\alpha_1=1, \alpha_2=1,$ and $\alpha_3=0.001$ by default.

To cope with the nonnegativity requirement of the learnable network combination weights, $w_h$'s, we employ the inverse of the softplus function $\Tilde{w}_h=\log(\exp(w_h)-1)$ to the initial values of the $w_h$'s, replacing $w_h=0$ by $w_h=0.0001$ for numerical stability. We then employ the softplus function $w_h=\log(1+\exp(\Tilde{w}_h))$ to transform $\Tilde{w}_h\in\mathbbm{R}$ back to $w_h>0,$ and set $w_h=0$ for tiny $w_h$ to ensure $w_h\in[0, \infty)$. For $w_\text{add}\in(0, 1)$ (similarly treating $0$ as $0.0001$ and $1$ as $0.9999$), we apply a logit transformation with $\Tilde{w}_\text{add}=\log\left(\frac{w_\text{add}}{1-w_\text{add}}\right)\in\mathbbm{R}$, whose inverse function is the logistic function $w_\text{add}=\frac{1}{1+\exp(-\Tilde{w}_\text{add})}.$

We split the time series into training, validation, and test sets, where validation is used for early stopping, and the test set is used for learned model comparison and to select the ``best" optimized set of parameters based on the lowest test loss value (setting $\alpha_3=0$ in Eq.~\eqref{eq:total_loss} during selection).

To address the possible influence of parameter initialization, we run multiple initializations to obtain estimated optimal sets of the network combination weights. One heuristic of setting different initializations is: (1) uniform initial values from $[0.1, 0.2, 0.5, 1, 2, 5]$ (note that $w_\text{add}\leq1$ so we cap larger initial values to 1); (2) one parameter is initialized to be 1 and the rest initialized to be 0.1.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\label{sec:experiments}
To validate the efficacy of our novel network combination method, we test our proposed method on the chimpanzee data, and construct synthetic models with known combination weights for further empirical evidence. The experimental setup is provided in App.~\ref{app_sec:experiment_setup}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Synthetic Network Model}
We construct evolving individual networks, $\mathcal{G}^{(\text{raw}, h,t)}$ and $\mathcal{G}^{(\text{add}, h,t)}$, with stable cumulative network $\mathcal{G}^{(t)}=(\mathcal{N}^{(t)},\mathcal{E}^{(t)})$,  over time. We let $n$ be the number of nodes in total, $T$ be the number of time steps, $H$ be the number of hierarchies (e.g.,  different proximity levels), and $\{p_h\}_{h=1}^H$ edge probabilities for each layer of the multiplex network. Details of the construction are provided in App.~\ref{app_sec:synthetic_generation}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Synthetic Data Empirical Results}
\begin{table}[htb!]
\centering
\caption{Synthetic data results. ``GT" indicates ground-truth, $\alpha_3=0$ and $\alpha_3=0.001$ indicates optimized weights by our proposed method for $\alpha_3=0$ and $\alpha_3=0.001$, respectively.
} 
\vskip 0.1in
\resizebox{1\linewidth}{!}{
\begin{tabular}{lrrrrrrr}
\toprule
Type&$w_2$&$w_3$&$w_4$&$w_5$&$w_\text{add}$ \\ 
\midrule
GT&0.0&1.0&0.0&1.0&0.0\\
$\alpha_3=0.0$&$0.0\pm0.0$&$1.0\pm0.0$&$0.0\pm0.0$&$1.0\pm0.0$&$0.0\pm0.0$&\\
$\alpha_3=0.001$&$0.0\pm0.0$&$0.9\pm0.0$&$0.2\pm0.0$&$0.7\pm0.0$&$0.0\pm0.0$&\\
\midrule
GT&0.0&1.0&0.0&1.0&0.3\\
$\alpha_3=0.0$&$0.0\pm0.0$&$1.0\pm0.0$&$0.0\pm0.0$&$1.0\pm0.0$&$0.3\pm0.0$&\\
$\alpha_3=0.001$&$0.0\pm0.0$&$0.9\pm0.0$&$0.2\pm0.0$&$0.7\pm0.0$&$0.3\pm0.0$&\\
\midrule
GT&0.0&1.0&1.0&0.0&0.0\\
$\alpha_3=0.0$&$0.0\pm0.0$&$1.0\pm0.0$&$1.0\pm0.0$&$0.0\pm0.0$&$0.0\pm0.0$&\\
$\alpha_3=0.001$&$0.0\pm0.0$&$0.9\pm0.0$&$0.8\pm0.0$&$0.1\pm0.0$&$0.0\pm0.0$&\\
\midrule
GT&0.0&1.0&1.0&0.0&0.3\\
$\alpha_3=0.0$&$0.0\pm0.0$&$1.0\pm0.0$&$1.0\pm0.0$&$0.0\pm0.0$&$0.3\pm0.0$&\\
$\alpha_3=0.001$&$0.0\pm0.0$&$0.9\pm0.0$&$0.8\pm0.0$&$0.1\pm0.0$&$0.3\pm0.0$&\\
\midrule
GT&0.0&0.6&0.0&1.2&0.0\\
$\alpha_3=0.0$&$0.0\pm0.0$&$0.6\pm0.0$&$0.0\pm0.0$&$1.2\pm0.0$&$0.0\pm0.0$&\\
$\alpha_3=0.001$&$0.0\pm0.0$&$0.5\pm0.0$&$0.1\pm0.0$&$0.9\pm0.0$&$0.0\pm0.0$&\\
\midrule
GT&0.0&0.6&0.0&1.2&0.3\\
$\alpha_3=0.0$&$0.0\pm0.0$&$0.6\pm0.0$&$0.0\pm0.0$&$1.2\pm0.0$&$0.3\pm0.0$&\\
$\alpha_3=0.001$&$0.0\pm0.0$&$0.5\pm0.0$&$0.1\pm0.0$&$0.9\pm0.0$&$0.3\pm0.0$&\\
\midrule
GT&1.2&0.0&0.0&0.6&0.0\\
$\alpha_3=0.0$&$1.2\pm0.0$&$0.0\pm0.0$&$0.0\pm0.0$&$0.6\pm0.0$&$0.0\pm0.0$&\\
$\alpha_3=0.001$&$0.9\pm0.0$&$0.1\pm0.0$&$0.1\pm0.0$&$0.4\pm0.0$&$0.0\pm0.0$&\\
\midrule
GT&1.2&0.0&0.0&0.6&0.3\\
$\alpha_3=0.0$&$1.2\pm0.0$&$0.0\pm0.0$&$0.0\pm0.0$&$0.6\pm0.0$&$0.3\pm0.0$&\\
$\alpha_3=0.001$&$0.9\pm0.0$&$0.1\pm0.0$&$0.1\pm0.0$&$0.4\pm0.0$&$0.3\pm0.0$&\\
\midrule
GT&0.0&1.0&1.0&1.0&0.0\\
$\alpha_3=0.0$&$0.0\pm0.0$&$1.0\pm0.0$&$1.0\pm0.0$&$1.0\pm0.0$&$0.0\pm0.0$&\\
$\alpha_3=0.001$&$0.0\pm0.0$&$0.9\pm0.0$&$0.9\pm0.0$&$0.7\pm0.0$&$0.0\pm0.0$&\\
\midrule
GT&0.0&1.0&1.0&1.0&0.3\\
$\alpha_3=0.0$&$0.0\pm0.0$&$1.0\pm0.0$&$1.0\pm0.0$&$1.0\pm0.0$&$0.3\pm0.0$&\\
$\alpha_3=0.001$&$0.0\pm0.0$&$0.9\pm0.0$&$0.9\pm0.0$&$0.8\pm0.0$&$0.3\pm0.0$&\\
\midrule
GT&1.0&1.0&1.0&0.0&0.0\\
$\alpha_3=0.0$&$1.0\pm0.0$&$1.0\pm0.0$&$1.0\pm0.0$&$0.0\pm0.0$&$0.0\pm0.0$&\\
$\alpha_3=0.001$&$0.8\pm0.0$&$0.9\pm0.0$&$0.7\pm0.0$&$0.2\pm0.0$&$0.0\pm0.0$&\\
\midrule
GT&1.0&1.0&1.0&0.0&0.3\\
$\alpha_3=0.0$&$1.0\pm0.0$&$1.0\pm0.0$&$1.0\pm0.0$&$0.0\pm0.0$&$0.3\pm0.0$&\\
$\alpha_3=0.001$&$0.8\pm0.0$&$0.9\pm0.0$&$0.7\pm0.0$&$0.2\pm0.0$&$0.3\pm0.0$&\\
\midrule
GT&0.0&0.6&0.3&1.2&0.0\\
$\alpha_3=0.0$&$0.0\pm0.0$&$0.6\pm0.0$&$0.3\pm0.0$&$1.2\pm0.0$&$0.0\pm0.0$&\\
$\alpha_3=0.001$&$0.0\pm0.0$&$0.5\pm0.0$&$0.4\pm0.0$&$0.9\pm0.0$&$0.0\pm0.0$&\\
\midrule
GT&0.0&0.6&0.3&1.2&0.3\\
$\alpha_3=0.0$&$0.0\pm0.0$&$0.6\pm0.0$&$0.3\pm0.0$&$1.2\pm0.0$&$0.3\pm0.0$&\\
$\alpha_3=0.001$&$0.0\pm0.0$&$0.5\pm0.0$&$0.4\pm0.0$&$0.9\pm0.0$&$0.3\pm0.0$&\\
\midrule
GT&0.6&1.2&0.0&0.3&0.0\\
$\alpha_3=0.0$&$0.6\pm0.0$&$1.2\pm0.0$&$0.0\pm0.0$&$0.3\pm0.0$&$0.0\pm0.0$&\\
$\alpha_3=0.001$&$0.5\pm0.0$&$0.9\pm0.0$&$0.2\pm0.0$&$0.2\pm0.0$&$0.0\pm0.0$&\\
\midrule
GT&0.6&1.2&0.0&0.3&0.3\\
$\alpha_3=0.0$&$0.6\pm0.0$&$1.2\pm0.0$&$0.0\pm0.0$&$0.3\pm0.0$&$0.3\pm0.0$&\\
$\alpha_3=0.001$&$0.5\pm0.0$&$0.9\pm0.0$&$0.2\pm0.0$&$0.2\pm0.0$&$0.3\pm0.0$&\\
\bottomrule
\end{tabular}}
\vspace{-5mm}
\label{tab:synthetic_res}
\end{table}
We conduct experiments on multiple synthetic data sets. Here we set $p_h=0.1$ throughout, $n=100, p_\text{add}=0.1, T=14$ with training:validation:test=8:3:3 in the split. We take $H=5,$ vary $w_\text{add}\in[0, 0.3],$ and take different $w_h$'s. We compare the results for $\alpha_3=0$ and $\alpha_3=0.001.$ 

In our experiments, with a fixed $\alpha_3$ value, the final optimized weights are typically robust to initialization. In general, we conclude that for our synthetic data, the proposed method can perfectly recover the combination weights of interest up to one decimal point, while with $\alpha_3=0.001$ the final estimated values are typically closer but smaller than the actual values (due to regularization). Having a regularization term also urges more combination weights of interest to take nonzero values. The set of parameters and optimized values based on the lowest test loss is provided in Table~\ref{tab:synthetic_res}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Application: Chimpanzee Network Combination}
\label{sec:chimp_network_construction}
Applying the method in Sec.~\ref{sec:proposed_method} to the data described in Sec.~\ref{sec:chimp_data_description}, we optimize the network combination weights by assuming a stable normalized similarity matrix and a stable normalized weighted degree for consecutive years. As chimpanzee researchers observed major changes in the chimpanzee social interactions from 2014, we conduct experiments for the period 1998 to 2012, leaving a one-year gap for the change to happen. We use 1998-2005 as the starting year for the training set, 2006-2008 as the starting year of the validation set, and 2009-2011 as the starting year of the test set. 

From our experiments, no matter which initialization we use, the optimal set of parameters, with one decimal point, is mostly robustly optimized to: $w_1=1.0, w_2=w_3=w_4=0.0, w_5=4.7, w_6=1.3, w_7=1.6, w_8=2.0, w_9=0.0, w_{10}=0.1,$ and $w_\text{add}=0.0.$ Although one initialization produces $w_{10}=0.2,$ this produces a slightly larger test loss value (Eq.~\eqref{eq:total_loss} with $\alpha_3=0$) as $0.01063\pm 0.00000,$ compared to the others, which obtain $0.01061\pm 0.00000$. The robustness of the final set of learned parameters to initialization empirically validates the efficacy of our proposed method. This gives us network combination weights $W_1=W_2=W_3=W_4=1.0, W_5=5.7, W_6=7.0, W_7=8.6, W_8=W_9=10.6, W_{10}=10.7,$ and addition parameter $w_\text{add}=0.0.$


The combination of final learned weights implies that the first four types of proximities involving ``party" individuals should roughly be treated the same, possibly due to the high variance in possible actual distances between individuals (see J \& K in Type 1 and F \& I in Type 4 in Fig.~\ref{fig:proximity_types} for example). With the leading magnitude in $w_5,$ we observe the large proximity gap between ``party" individuals and those within 5m from the focal subject. We can also explore proximity differences from the nontrivial gaps between $W_5, W_6, W_7,$ and $W_8.$ With $w_9=0,$ we conclude that being within a circle with a radius of 2m is probably already a very close relationship. The small addition of $w_{10}$ to grooming indicates that individuals are actually more closely related if they groom each other. Finally, $w_\text{add}=0$ indicates that consecutive occurrences are normally recorded only because chimpanzees may stay in the same place for a while, instead of corresponding to another level of increased proximity.


For completeness, if we set $\alpha_3=0$ during training, we obtain an even smaller optimized average test loss of 0.007536 with a uniform initialization of all parameters to be 5.0 and $w_\text{add}$ initialized to 0.1. The final result is $w_1=1.0, w_2=w_3=w_4=0.0, w_5=39.7, w_6=w_7=0.0, w_8=40.0, w_9=w_{10}=0.0,$ and $w_\text{add}=0.0,$ resulting in $W_1=W_2=W_3=W_4=1.0, W_5=W_6=W_7=40.7, W_8=W_9=W_{10}=80.7,$ and addition parameter $w_\text{add}=0.0.$ This implies that the most notable proximity gap comes from being within 5m of the focal subject and from being within 2m of the focal subject. These two nontrivial gaps align with the two biggest learned increments when we set $\alpha_3=0.001$, i.e., $w_5$ and $w_8.$
\begin{figure}[htb]
\centering
    \begin{subfigure}[ht]{\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/average_weighted_degree.pdf}
      \subcaption{Average weighted degrees.}
    \end{subfigure}
    \begin{subfigure}[ht]{\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/average_clustering_coefficient.pdf}
      \subcaption{Average local clustering coefficients.}
    \end{subfigure}
    \begin{subfigure}[ht]{\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/average_close_centrality.pdf}
      \subcaption{Average closeness centrality values.}
    \end{subfigure}
    \vspace{-3mm}
    \caption{Key statistics of the learned networks.}
    \vspace{-6mm}
\label{fig:key_statistics}
\end{figure}

We compare individual learned networks over time and provide some key statistics in Fig.~\ref{fig:key_statistics}. 
The local clustering coefficient is a measure of the degree to which nodes tend to cluster together. Closeness centrality measures how close a node is to all other nodes, calculated by finding the average shortest distance between a node and all other nodes in the network. Here, the distance is computed by taking inverse degrees $d\rightarrow\frac{1}{d}$. More information about network summaries can be found for example in \citet{newman2018networks}. We observe evolution in average weighted degrees, average local clustering coefficient, and average closeness centrality values, although our optimization approach assumes a stable graph structure in terms of similarity and normalized weighted degrees. The summary statistics indicate the presence of network dynamics even under our structural consistency assumption over time. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Node Similarity in Network Time Series} 
\looseness=-1 In addition to network comparison over time, we are also interested in comparing individuals in network time series. Here, we propose two notions of similarity between individuals based on node-wise close relations over time. The notion of relatedness is in principle user-defined; here we base it on whether or not two individuals are in the same ``community'', as explained below. One notion of similarity is based on how many times they are in close relationship over the period when they \emph{co-exist} (i.e., both are non-isolated). We denote this similarity notion as \emph{count similarity}. Another notion is based on the longest time interval during which they keep the close relations; this is the longest stretch of time the two individuals stay related, excluding the time steps when either or both are isolated, unless they transition through the isolated status together to being related. The resulting similarity notion can be used to understand the longest duration of close relations for each pair of entities within the network. We denote this notion as \emph{duration similarity}. Here, we only consider time steps where both nodes co-exist, and hence the number of time steps considered for each pair of nodes may differ. Based on the analysis in this section regarding distributions, for each observed similarity value, we can compute its p-value for the null hypothesis that nodes stay related independently and randomly across time. We conduct Bonferroni correction~\citep{vanderweele2019some} to identify significantly similar nodes.

\subsection{Theoretical Analysis of Similarity Notions}
\label{subsec:similarity_theorems}
Suppose the event for two nodes being closely related is drawn randomly and independently over time. To quantify the two notions of similarity, we carry out a theoretical analysis on sequences of independent Bernoulli trials with different success probabilities over time. The distribution of the number of successes of independent but not necessarily identically distributed Bernoulli random variables is called the Poisson-Binomial distribution. In general, there is no closed-form available for its probability mass function, but Thm. \ref{thm:count_successes_distribution} in App.~\ref{app:proofs} provides a recursion formula for it. We use this recursion to assess the significance for the count similarity. Similarly, Thm. \ref{thm:longest_consecutive_successes_distribution} in App.~\ref{app:proofs} gives a recursion formula for the longest success run in such a sequence, which we use to assess significance for duration similarity. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Node Similarity via Community Identities}
A central concept here is the notion of \emph{close relations}. To yield independent samples, we propose to define two nodes to have a close relationship at a certain time step if and only if they share the same community identity, for a time step when they co-exist. For time steps when at least one node has no record, we disregard them in the computation.

Since we think of relatedness as being in the same community, we carry out community detection at every time step, yielding a sequence of partitions. For any two nodes $i$ and $j$, for each time step they co-exist, we record whether or not they are assigned to the same community, resulting in a sequence of entries taking value 0 (different communities) and 1 (same community), denoted as $\{B^t_{i,j}\}$. We then assess \emph{count similarity} via the number of times that $i$ and $j$ are in the same community, which is the number of 1's in this sequence; we also compute \emph{duration similarity} by the length of the longest shared path between them, which is the length of the longest run of consecutive 1's in this sequence. Applying Theorems~\ref{thm:count_successes_distribution} and \ref{thm:longest_consecutive_successes_distribution}, we use the random variable $C_{i,j}^T$ to count the number of times nodes $i$ and $j$ belong to the same community throughout time stamps they co-exist, and use $M_{i,j}^T$ to denote the longest shared path throughout times they co-exist. We are then left to compute the success probabilities (i.e., probabilities of two nodes staying in the same community) over time, which is denoted as $\{p_{i,j}^t\}.$ The following proposition (proved in App.~\ref{app:proofs}) computes these probabilities to fully apply Theorems~\ref{thm:count_successes_distribution} and \ref{thm:longest_consecutive_successes_distribution}.

\begin{proposition}\label{prop:prob_same_community_each_time}
Suppose for a time step $t,$ both nodes $i$ and $j$ exist in the network containing $n_t$ nodes and $K_t$ communities $\mathcal{C}_0^{t}, \mathcal{C}_1^{t}, \dots, \mathcal{C}_{K_t-1}^{t}.$ Suppose all nodes have the same i.i.d. community assignment distribution, then the distribution of $i$ and $j$ being in the same community at time step $t$ is a Bernoulli random variable $B_{i,j}^t$ with success probability 
\begin{equation*}
  p_{i,j}^t=\sum_{k=0}^{K_t-1}\frac{\left\lvert\mathcal{C}_k^{t}\right\rvert\left(\left\lvert\mathcal{C}_k^{t}\right\rvert-1\right)}{n_t(n_t-1)}.
\end{equation*}
\end{proposition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Application to Chimpanzee Networks}
\looseness=-1For each yearly graph, we employ the popular Leiden algorithm~\citep{traag2019louvain} to construct communities for each time step. In order to mitigate the effect of randomness inherent in the community detection algorithm, we run the Leiden algorithm 20 times for each network, and pick the partition with the largest value of {\em modularity}, a standard quality measure in community detection~\citep{newman2018networks}. 
With the above theorems, for each observed similarity value, we  compute its $p$-value for the null hypothesis that nodes are closely related (in our case, belonging to the same community) independently and randomly across time.
We conduct Bonferroni correction to select the most significantly similar nodes.  

\begin{figure}[htb]
\centering
    \begin{subfigure}[ht]{\linewidth}
      \centering
      \includegraphics[width=\linewidth, trim={0 0 0 1cm}, clip]{figures/combined_similarity_yearly_full.pdf}
    \subcaption{Similarity graphs.}
    \end{subfigure}
    \begin{subfigure}[ht]{\linewidth}
      \centering
      \includegraphics[width=\linewidth, trim={0 0 0 1cm}, clip]{figures/combined_p_value_similarity_yearly_full.pdf}
    \subcaption{$p$-values.}
    \end{subfigure}
    \vspace{-3mm}
    \caption{Similarity graphs and $p$-values for the full learned graphs. The left column corresponds to count similarity, while the right column corresponds to duration similarity.}
\label{fig:similarity_and_p_value_full}
\vspace{-2mm}
\end{figure}
Fig.~\ref{fig:similarity_and_p_value_full} visualizes the similarity graphs (with similarity values as edge weights) and $p$-values for the learned yearly networks in chimpanzees. In order to further prove the concept of our graph combination merits in Sec.~\ref{sec:chimp_network_construction}, we compare the learned networks with two baselines. The first baseline (``unlearned") simply uses hardcoded, unlearned yearly networks by setting $w_j=0.1$ for $j>1$, $w_\text{add}=0.1,$ and $w_1=1.0$. The second baseline (``binary") treats all yearly networks as binary by setting all edge weights to one.
\begin{figure}[htb]
\centering
    \begin{subfigure}[ht]{0.49\linewidth}
      \centering
      \includegraphics[width=\linewidth, trim={1cm 1cm 1cm 8cm}, clip]{figures/thresholded_graph_count_similarity_yearly_full.pdf}
      \subcaption{Learned: Count Similarity}
    \end{subfigure}
    \begin{subfigure}[ht]{0.49\linewidth}
      \centering
      \includegraphics[width=\linewidth, trim={1cm 1cm 1cm 8cm}, clip]{figures/thresholded_graph_longest_similarity_yearly_full.pdf}
      \subcaption{Learned: Duration Similarity}
    \end{subfigure}
    \centering
    \begin{subfigure}[ht]{0.49\linewidth}
      \centering
      \includegraphics[width=\linewidth, trim={1cm 1cm 1cm 8cm}, clip]{figures/thresholded_graph_count_similarity_yearly_full_unlearned.pdf}
      \subcaption{Unlearned: Count Similarity}
    \end{subfigure}
    \begin{subfigure}[ht]{0.49\linewidth}
      \centering
      \includegraphics[width=\linewidth, trim={1cm 1cm 1cm 8cm}, clip]{figures/thresholded_graph_longest_similarity_yearly_full_unlearned.pdf}
      \subcaption{Unlearned: Duration Similarity}
    \end{subfigure}
    \begin{subfigure}[ht]{0.49\linewidth}
      \centering
      \includegraphics[width=\linewidth, trim={1cm 1cm 1cm 8cm}, clip]{figures/thresholded_graph_count_similarity_yearly_full_binary.pdf}
      \subcaption{Binary: Count Similarity}
    \end{subfigure}
    \begin{subfigure}[ht]{0.49\linewidth}
      \centering
      \includegraphics[width=\linewidth, trim={1cm 1cm 1cm 8cm}, clip]{figures/thresholded_graph_longest_similarity_yearly_full_binary.pdf}
      \subcaption{Binary: Duration Similarity}
    \end{subfigure}
    \vspace{-2mm}
    \caption{Visualization of thresholded similarity graphs.}
    \vspace{-6mm}
\label{fig:thresholded_similarity}
\end{figure}

\begin{table}[htb]
\centering
\caption{Cliques detected by two notions of similarity on the chimpanzee networks for three network combination methods (learned, unlearned, and binary). Individuals are denoted by their codes.}
\vskip 0.1in
\resizebox{1\linewidth}{!}{%
\begin{tabular}{clll}
\toprule
Method & Count Similarity & Duration Similarity \\ 
\midrule
\multirow{3}{*}{learned} & 
[ri, hu, ro, wn, ga],& [ri, hu, ro, wn, ga, ws], \\ 
& [ri, ro, garbo, ga], [hi, mu, cs], & [hi, mu, cs],\\ 
& [pe, ct], [rh, pi], [bt, pp]
& [pe, ct], [rh, pi]
\\ 
\hline
\multirow{2}{*}{unlearned} & 
[ri, hu, ro, wn, ga], [pe, ct], 
 & [ri, hu, ro, wn, ga, ws], \\
&[ri, ro, garbo], [hi, mu, dx]
 & [hi, mu, dx], [pe, ct], [rh, ro]
\\
\hline
\multirow{2}{*}{binary} & 
[ri, ro], [ri, wn], [hi, mu], [mu, cs], &[ro, ws, bu], \\
&[ws, bu], [bu, ro], [mu, lo]&[mu, lo]\\
\bottomrule
\end{tabular}}
\label{tab:cliques}
\end{table}


For the full graphs as described by Eq.~\eqref{eq:adj_t}, Fig.~\ref{fig:thresholded_similarity} visualizes thresholded similarity graphs (keeping only significant entries) for learned, unlearned, and binary graphs, respectively, based on p-values and Bonferroni correction with a significance level of 0.05.

\looseness=-1 Based on the thresholded networks, we discover persistent cliques in Tab.~\ref{tab:cliques}. Cliques correspond to expectations based on qualitative observations over 10 years of observations on this population~\citep{mitani2009male}. Two maternal brothers appear in the same clique (ri and hu within a larger clique; pe and ct as a dyadic clique). Both the learned and the unlearned networks include those pairs in their significant cliques, but the binary networks fail to do so. These pairs are known to exhibit strong bonds (e.g., pe and ct were among the top grooming and proximity partners in a study with an independent data set collected during one year from 2014 to 2015, see \citet{sandel2020adolescent}). One mother-son pair appeared (ri and garbo) who are known to be close (although the same female did not appear in a persistent clique with her other son, hu, who she is also very close with from qualitative observations; male chimpanzees can maintain important relationships with their mothers into adulthood, including in this population~\citep{reddy2020social}.

Overall, the results from the learned and unlearned networks are similar. Learned networks include two additional dyads in the count similarity, which, from long-term qualitative observations, were known to have a persistent “mentor-mentee” relationship. One pair involved an adult male, bt, who “adopted” pp, a younger male as a juvenile, and the two remained close in adulthood. The other pair included an adult male (rh) and his biological father (pi); although chimpanzees do not appear to have kin recognition mechanisms for their biological fathers, there is evidence in this population that adolescent and young adult males preferentially groom and spend time in proximity to their biological fathers and other older “mentor” figures~\citep{sandel2020adolescent}. In line with \citet{SexualSegregationCliquesandSocialPowerinSquirrelMonkeySaimiriGroups}, we conjecture that cliques may have an advantage and perhaps individuals in these cliques may be key drivers of the social structure in the chimpanzee population.

%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Future Work}
This paper presents a novel optimization approach to combine proximity networks into a single network based on hierarchies of proximity levels. It also provides a novel perspective to identify long-term related nodes in network time series. Based on the learned networks, we plan to further investigate social interactions in the chimpanzee data and provide deeper insights to animal experts. The novel application-driven network combination approach, as a general method for constructing network time series based on focal observations, can be further applied to more animal social networks or even more general network construction procedures. For the similarity analysis, we plan to make the method more robust to randomness in the community assignments. We will also explore further ways of investigating individual relationships in network time series. 
%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgement}
The chimpanzee data set was collected with the approval of the Uganda Wildlife Authority, Uganda National Council for Science and Technology, and the Makerere University Biological Field Station. We thank members of the Ngogo Chimpanzee Project who provided support in the field, especially David Watts, Kevin Langergraber, Sam Angedakin and the late Jerry Lwanga. For support with data organization, we thank Veronika Städele. Research at Ngogo has been funded by: Arizona State University; Institute of Human Origins; Keo Films; L.S.B. Leakey Foundation; Max Planck Society; National Science Foundation (BCS-9253590, IOS-0516644, BSC-0850951, BCS-1613393, BSC-1850328, BCS- 1540259, BCS-0215622); National Institute on Aging (R01-AG049395); National Geographic Society; Wenner-Gren Foundation (including Gr. 9957); Silverback Films; Underdog Films; Wildstar Films; University of Michigan; and Yale University.

In addition, Gesine Reinert is funded in part by EPSRC grants EP/T018445/1, EP/V056883/1, EP/Y028872/1,  and EP/X002195/1.
\newpage
\bibliography{refs}
\bibliographystyle{arxiv_style}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Proximity Types}
\label{app_sec:proximity_types}
We can derive the following proximity types based on the level of proximity two individuals have at a certain recorded time (examples are from Fig.~\ref{fig:proximity_types}):
\begin{itemize}[noitemsep]
    \item Type 1 (within ~200m): Two individuals in the ``party" to a focal male (subject of observation), but not observed in ``prox2" or ``prox5". E.g., J \& K.
    \item Type 2 (within ~105m): One individual in ``prox5" to focal, and the other in ``party", but not observed in ``prox2" or ``prox5". E.g., H \& K.
    \item Type 3 (within ~102m): One individual in ``prox2" to focal, and the other individual in ``party". E.g., A \& I.
    \item Type 4 (within ~100m): Subject of observation + individual in ``party", but not observed in proximity or grooming. E.g., F \& K.
    \item Type 5 (within 10m): Two individuals, both in ``prox5" to focal. E.g., G \& H.
    \item Type 6 (within 7m): Two individuals, one in ``prox2" to focal and the other in ``prox5". E.g., A \& H.
    \item Type 7 (2-5m): Subject of observation + individual in ``prox5". E.g., F \& E.
    \item Type 8 (within 4m): Two individuals, both in ``prox2" to focal. E.g., A \& B.
    \item Type 9 (within 2m): Subject of observation + individual in ``prox2" but not grooming. E.g., F \& A.
    \item Type 10 (within 2m but semantically closer than type 9): Subject of observation + grooming individual in ``prox2". E.g., F \& B.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theorems and Proofs}\label{app:proofs}

\subsection{Theorem and Proof of Count Similarity}
Here we provide a theorem and the proof relating to count similarity. We note that the distribution of $C_t$ is also called a {\it Poisson-binomial distribution.} In general, there is no closed form available for its probability mass function.

\begin{theorem}
\label{thm:count_successes_distribution}
[Useful for Count Similarity] For a 
series of independent Bernoulli trials $\{B_t\},\;\; t\in [T-1]:=\{0, \dots, T-1\},$ with success rate $p_t$ for $B_t,$ denote $C_t \in \{ 0, \dots, t\}$ as the total number of successes in $[t]$. 
For $t\in\{0, \dots, T-1\}$, the probability distribution of $C_t$ satisfies
\begin{align}
\mathbbm{P}(C_t=0) &=\prod_{s=0}^t(1-p_s), \;\;
 \mathbbm{P}(C_t=t+1) =\prod_{s=0}^t p_s,
 \end{align}
 and for $L\in\{1, \dots, t\},$
 \begin{align}
 \mathbbm{P}(C_t=L) &=p_t\cdot\mathbbm{P}(C_{t-1}=L-1) + (1-p_t)\cdot \mathbbm{P}(C_{t-1}=L);
    \end{align}
    for $ L\in\{t+2, \dots, T\},$
    \begin{align}
    \mathbbm{P}(C_t=L)&=0.
\end{align}
\end{theorem}

\begin{proof}

We first prove the behavior at the boundaries.

Since the total number of successes is bounded by the number of trials, we always have $C_t\leq t+1.$ Therefore, $\mathbbm{P}(C_t=L)=0,\;\;t\in\{0, \dots, T-1\}, \; L\in\{t+2, \dots, T\}.$ The only chance of having pure successes is to never fail the trial, and hence $\mathbbm{P}(C_t=t+1) =\prod_{s=0}^t p_s, \;\; t\in\{0, \dots, T-1\}$ due to the independence between the Bernoulli random variables. Similarly, the only chance of having zero successes is to fail the trial every time, and hence $\mathbbm{P}(C_t=0) =\prod_{s=0}^t(1-p_s), \;\; t\in\{0, \dots, T-1\}$.

In order to compute $\mathbbm{P}(C_t=L)$ for $t\in\{1, \dots, T-1\},\;L\in\{1, \dots, t\},$ we first analyze the role of this time step $t$. There are two possibilities for time step $t$: either it is a success, or it is a ``failure" point. Note that we concentrate on the time series from the start (time step $0$) till time step $t,$ and we impose no constraints on future time steps. For the first situation, we require that we have $L-1$ successes before $t;$ while for the second situation, we require $L$ successes at time $t-1.$ Therefore, 
\begin{align*}
    \mathbbm{P}(C_t=L) &=p_t\cdot\mathbbm{P}(C_{t-1}=L-1)+ (1-p_t)\cdot \mathbbm{P}(C_{t-1}=L), \;\;t\in\{1, \dots, T-1\},\;L\in\{1, \dots, t\}.
\end{align*}
Combing the above, we have that 
\begin{align*}
\begin{split}
    \mathbbm{P}(C_t=L)&=0,\;\;t\in\{0, \dots, T-1\}, \; L\in\{t+2, \dots, T\},\\
    \mathbbm{P}(C_t=t+1) &=\prod_{s=0}^t p_s, \;\; t\in\{0, \dots, T-1\},\\
    \mathbbm{P}(C_t=0) &=\prod_{s=0}^t(1-p_s), \;\; t\in\{0, \dots, T-1\},\\
    \mathbbm{P}(C_t=L) &=p_t\cdot\mathbbm{P}(C_{t-1}=L-1)+ (1-p_t)\cdot \mathbbm{P}(C_{t-1}=L), \;\;t\in\{1, \dots, T-1\},\;L\in\{1, \dots, t\}.
\end{split}
\end{align*}
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Theorem and Proof for Duration Similarity}
The next theoretical result gives a means for assessing significant duration similarity.
\begin{theorem}
\label{thm:longest_consecutive_successes_distribution}
[Useful for Duration Similarity] For a time series of independent Bernoulli trials $\{B_t\},\;\; t\in\{0, \dots, T-1\},$ with success rate $p_t$ for $B_t,$ denote $M_t \in \{0, \dots, t\}$ as the longest consecutive successes from the start until time $t.$  The probability distribution satisfies for $t\in\{0, \dots, T-1\}$,
\begin{align}
    \mathbbm{P}(M_t=L)&=0,\;
    L\in\{t+2, \dots, T\},\\
    \mathbbm{P}(M_t=t+1) &=\prod_{s=0}^t p_s, \quad \quad 
    \mathbbm{P}(M_t=0) =\prod_{s=0}^t(1-p_s);
    \end{align}
    for $t\in\{2, \dots, T-1\}$,
    \begin{align}
    \mathbbm{P}(M_t=t)
    &=(1-p_t)\cdot\prod_{s=0}^{t-1} p_s +(1-p_0)\cdot \prod_{s=1}^{t} p_s ,\;\;\\
    \mathbbm{P}(M_t=1) &=\left[\sum_{l=0}^{1}\mathbbm{P}(M_{t-2}=l)\right]\cdot(1-p_{t-1})\cdot p_t \nonumber + (1-p_t)\cdot \mathbbm{P}(M_{t-1}=1);
    \end{align}
    and for $t\in\{3, \dots, T-1\}$ and $L\in\{2, \dots, t-1\}$,
    \begin{align}
    \mathbbm{P}(M_t=L) =& \left[\sum_{l=0}^{L}\mathbbm{P}(M_{t-L-1}=l)\right]\cdot
   (1-p_{t-L})\cdot\left(\prod_{s=t-L+1}^t p_s\right) \nonumber\\
    &+ \sum_{s=t-L+1}^{t-1}(1-p_s)
    \left(\prod_{k=s+1}^t p_k\right)\cdot
    \mathbbm{P}(M_{s-1}=L)+ (1-p_t)
    \mathbbm{P}(M_{t-1}=L).
\end{align}
\end{theorem}

To prove Thm.~\ref{thm:longest_consecutive_successes_distribution} we first show the following result. 

\begin{proposition}\label{prop:independence_M_B}
For a time series of independent Bernoulli trials $\{B_t\},\;\; t\in\{0, \dots, T-1\},$ with success rate $p_t$ for $B_t,$ denote $M_t$ as the longest consecutive successes from the start until time $t,$ then $M_t$ takes values from $\{0, \dots, t\}.$   Then, $M_t$ and $B_s$ are independent if $s>t.$
\end{proposition}
\begin{proof}
By definition, $M_t$ is only dependent on $B_0, \dots, B_t,$ and $B_s$ is independent of $B_k$ for any $k\neq s.$ Given $s>t,$ we have that $M_t$ and $B_s$ are independent.
\end{proof}

Now we present and prove Thm.~\ref{thm:longest_consecutive_successes_distribution}.

\begin{proof}
We first prove the behavior at the boundaries.

Since the total number of successes is bounded by the number of trials, we always have $M_t\leq t+1.$ Therefore, $\mathbbm{P}(M_t=L)=0,\;\;t\in\{0, \dots, T-1\}, \; L\in\{t+2, \dots, T\}.$ The only chance of having pure successes is to never fail the trial, and hence $\mathbbm{P}(M_t=t+1) =\prod_{s=0}^t p_s, \;\; t\in\{0, \dots, T-1\}$ due to the independence between the Bernoulli random variables. Similarly, the only chance of having zero successes is to fail the trial every time, and hence $\mathbbm{P}(M_t=0) =\prod_{s=0}^t(1-p_s), \;\; t\in\{0, \dots, T-1\}$.
For $\mathbbm{P}(M_t=t)$ with $t\in\{2, \dots, T-1\},$ we require exactly one failure at either the very end or the very beginning. Therefore, $\mathbbm{P}(M_t=t)=(1-p_t)\cdot\left(\prod_{s=0}^{t-1} p_s\right)+(1-p_0)\cdot\left(\prod_{s=1}^{t} p_s\right)$ for $t\in\{2, \dots, T-1\}.$

In order to compute $\mathbbm{P}(M_t=L)$ for $t\in\{2, \dots, T-1\},\;L\in\{1, \dots, t-1\},$ we first analyze the role of this time step $t$. There are two possibilities for time step $t$: either it is an endpoint for a chain with $L$ consecutive successes, or it is not such a point. Note that we concentrate on the time series from the start (time step $0$) till time step $t,$ and we impose no constraints on future time steps.


For the first situation, we require the point before this chain to be a ``failure" point, and that the longest consecutive success length before this ``failure" point is no more than $L$ (since $L$ is achieved by the chain already and we need to be consistent with the definition of the largest length). Mathematically, we require that 
\begin{itemize}[noitemsep]
    \item $\prod_{s=t-L+1}^tB_s=1$ for the definition of the chain containing $L$ consecutive successes until $t$; 
    \item $B_{t-L}=0$ for the definition of the ``failure" point;
    \item $M_{t-L-1}\leq L$ as $L$ is defined to be the largest length until$t$ and that this largest length could be achieved more than once (and hence we take $\leq L$ instead of $< L$). Note that the definition of $M_{t-L-1}$ is valid as we have $t-L-1\geq (t-1) - l \geq 0.$ 
\end{itemize}

For the second situation, since time step $t$ is not an endpoint of a chain with $L$ consecutive successes ending at $t$, and that $L\leq t-1,$ there must be at least one ``failure" point within the time steps $t-L+1, \dots, t.$ Denote the last ``failure" time step before or at time step $t$ as $s\in\{t-L+1, \dots, t\},$ then $B_s=0.$ If $s<t$, then by definition, all points after $s$ should be successes, i.e., $\prod_{k=s+1}^tB_k=1$. In addition, we require that the largest length is $L$ before this ``failure" point, i.e., $M_{s-1}=L$. Here, the definition of $M_{s-1}$ is valid as $s-1\geq (t-L+1)-1\geq t-L\geq 0.$ To summarize, for the second situation, the mathematical requirement is:
\begin{itemize}[noitemsep]
    \item $B_s=0$ for some $s\in\{t-L+1, \dots, t\};$
    \item $\prod_{k=s+1}^tB_k=1$ if $s<t;$ 
    \item $M_{s-1}=L$.
\end{itemize}
The second point implicitly requires $t-L+1\leq t,$ i.e., $L\geq 2.$

Therefore, we arrive at the following recurrence relation. For $t\in\{3, \dots, T-1\},\;L\in\{2, \dots, t-1\},$ with $\mathbbm{1}(\cdot)$ being an indicator function:
\begin{align}
\begin{split}
\label{eq:M_tL_recurrence}
    \mathbbm{P}(M_t=L) &= \mathbbm{P}\left[\mathbbm{1}(M_{t-L-1}\leq L) \bigcap \mathbbm{1}(B_{t-L}=0)\bigcap \mathbbm{1}(B_s=1\;\forall s=t-L+1, \dots, t)\right]\\
    &+ \sum_{s=t-L+1}^{t-1}\mathbbm{P}\left[\mathbbm{1}\left(B_s=0\right)\bigcap \mathbbm{1}\left(B_k=1\;\forall \;k=s+1, \dots, t\right)\bigcap \mathbbm{1}\left(M_{s-1}=L\right)\right]\\
    &+ \mathbbm{P}\left[\mathbbm{1}\left(B_t=0\right)\bigcap \mathbbm{1}\left(M_{t-1}=L\right)\right]\\
    &= \left[\sum_{l=0}^{L}\mathbbm{P}(M_{t-L-1}=l)\right]\cdot\mathbbm{P}(B_{t-L}=0)\cdot\left[\prod_{s=t-L+1}^t \mathbbm{P}(B_s=1)\right]\\
    &+ \sum_{s=t-L+1}^{t-1}\mathbbm{P}(B_s=0)\cdot\left[\prod_{k=s+1}^t \mathbbm{P}(B_k=1)\right]\cdot\mathbbm{P}(M_{s-1}=L) + \mathbbm{P}(B_t=0)\cdot \mathbbm{P}(M_{t-1}=L)\\
    &= \left[\sum_{l=0}^{L}\mathbbm{P}(M_{t-L-1}=l)\right]\cdot(1-p_{t-L})\cdot\left(\prod_{s=t-L+1}^t p_s\right)\\
    &+ \sum_{s=t-L+1}^{t-1}(1-p_s)\cdot\left(\prod_{k=s+1}^t p_k\right)\cdot\mathbbm{P}(M_{s-1}=L) + (1-p_t)\cdot \mathbbm{P}(M_{t-1}=L).
\end{split}
\end{align}
Note that the products could be taken as the $B_m$ terms have their indices $m$ greater than those from the $M_l$ terms, i.e., $m>l$, given Prop.~\ref{prop:independence_M_B}. 
Specifically, in the first term in the summation of \cref{eq:M_tL_recurrence}, $l=t-L-1<m$ for $m\in\{t-L, \dots, t\};$ in the second term of summation, $l=s-1<m=s$ and $l=s-1<m$ for $m\in\{s+1, \dots, t\};$ for the last term in the summation, $l=t-1<m=t.$

For $L=1$ and $t\in\{2, \dots, T-1\},$ we have, similarly,
\begin{align}
\begin{split}
    \mathbbm{P}(M_t=1) &= \mathbbm{P}\left[\mathbbm{1}(M_{t-2}\leq 1) \bigcap \mathbbm{1}(B_{t-1}=0)\bigcap \mathbbm{1}(B_t=1)\right] + \mathbbm{P}\left[\mathbbm{1}\left(B_t=0\right)\bigcap \mathbbm{1}\left(M_{t-1}=1\right)\right]\\
    &= \left[\sum_{l=0}^{1}\mathbbm{P}(M_{t-2}=l)\right]\cdot\mathbbm{P}(B_{t-1}=0)\cdot\mathbbm{P}(B_t=1) + \mathbbm{P}(B_t=0)\cdot \mathbbm{P}(M_{t-1}=1)\\
    &= \left[\sum_{l=0}^{L}\mathbbm{P}(M_{t-2}=l)\right]\cdot(1-p_{t-1})\cdot p_t + (1-p_t)\cdot \mathbbm{P}(M_{t-1}=1).
\end{split}
\end{align}

Combing the above, we have that 
\begin{align*}
\begin{split}
    \mathbbm{P}(M_t=L)&=0,\;\;t\in\{0, \dots, T-1\}, \; L\in\{t+2, \dots, T\},\\
    \mathbbm{P}(M_t=t+1) &=\prod_{s=0}^t p_s, \;\; t\in\{0, \dots, T-1\},\\
    \mathbbm{P}(M_t=0) &=\prod_{s=0}^t(1-p_s), \;\; t\in\{0, \dots, T-1\},\\
    \mathbbm{P}(M_t=t)
    &=(1-p_t)\cdot\left(\prod_{s=0}^{t-1} p_s\right)+(1-p_0)\cdot\left(\prod_{s=1}^{t} p_s\right),\;\;t\in\{2, \dots, T-1\},\\
    \mathbbm{P}(M_t=1) &=\left[\sum_{l=0}^{1}\mathbbm{P}(M_{t-2}=l)\right]\cdot(1-p_{t-1})\cdot p_t\\
    &+ (1-p_t)\cdot \mathbbm{P}(M_{t-1}=1), \;\;t\in\{2, \dots, T-1\},\\
    \mathbbm{P}(M_t=L) &=\left[\sum_{l=0}^{L}\mathbbm{P}(M_{t-L-1}=l)\right]\cdot(1-p_{t-L})\cdot\left(\prod_{s=t-L+1}^t p_s\right)\\
    &+ \sum_{s=t-L+1}^{t-1}(1-p_s)\cdot\left(\prod_{k=s+1}^t p_k\right)\cdot\mathbbm{P}(M_{s-1}=L)\\
    &+ (1-p_t)\cdot \mathbbm{P}(M_{t-1}=L), \;\;t\in\{3, \dots, T-1\},\;L\in\{2, \dots, t-1\}.
\end{split}
\end{align*}
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55
\subsection{Proof of Proposition~\ref{prop:prob_same_community_each_time}}
Here we provide the proof of Proposition~\ref{prop:prob_same_community_each_time}.
\begin{proof}
As all nodes have the same i.i.d. community assignment distribution, the probability for nodes $i$ and $j$ being both in community $\mathcal{C}_k^{t}$ for some $k\in\{0, \dots, K_t-1\}$ is
\begin{equation*}
\frac{\binom{\left\lvert\mathcal{C}_k^{t}\right\rvert}{2}}{\binom{n_t}{2}}=\frac{\left\lvert\mathcal{C}_k^{t}\right\rvert\left(\left\lvert\mathcal{C}_k^{t}\right\rvert-1\right)/2}{n_t(n_t-1)/2}=\frac{\left\lvert\mathcal{C}_k^{t}\right\rvert\left(\left\lvert\mathcal{C}_k^{t}\right\rvert-1\right)}{n_t(n_t-1)}.
\end{equation*}
Taking into account all communities at time step $t,$ we have 
\begin{equation}
    p_{i,j}^t=\sum_{k=0}^{K_t-1}\frac{\left\lvert\mathcal{C}_k^{t}\right\rvert\left(\left\lvert\mathcal{C}_k^{t}\right\rvert-1\right)}{n_t(n_t-1)}.
\end{equation}
This completes the proof.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation Details}
\subsection{Experimental Setup}
\label{app_sec:experiment_setup}
Experiments were conducted on two compute nodes, each with 8 Nvidia Tesla T4, 96 Intel Xeon Platinum 8259CL CPUs @ 2.50GHz and $378$GB RAM. We run at most 5000 epochs using the Adam optimizer~\citep{kingma2014adam} with a learning rate of $0.1$ and an early stopping parameter to be 3000 epochs. Anonymized codes are provided at \url{https://anonymous.4open.science/r/ProxFuse}. Experimental results are averaged over three runs on different random seeds.
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Synthetic Network Model}
\label{app_sec:synthetic_generation}
With the aim of constructing evolving individual networks, $\mathcal{G}^{(\text{raw}, h,t)}$ and $\mathcal{G}^{(\text{add}, h,t)}$, but stable cumulative network $\mathcal{G}^{(t)}=(\mathcal{N}^{(t)},\mathcal{E}^{(t)})$ over time, the synthetic models are constructed as follows. Let $n$ be the number of nodes in total, $T$ be the number of time steps, $H$ be the number of hierarchies (e.g., different proximity levels), and $\{p_h\}_{h=1}^H$ edge probabilities for each layer of the multiplex network. 

\textbf{Initialization.} For the first time step, $t=1,$  we assume that $\mathcal{G}^{(\text{raw}, h,t)}$, the raw single-type network for type $h$ at each time step $t,$ is generated independently as a Bernoulli random graph with $n$ nodes and edge probability $p_h$. The edge weights are sampled randomly from integers $[1, \dots, H]$. We use $\mathbf{A}^{(\text{raw}, h,t)}$ to denote its adjacency matrix. We then generate $\mathbf{A}^{(\text{add}, h,t)}$ by a Hadamard product of the binary version of $\mathbf{A}^{(\text{raw}, h,t)}$ with another Bernoulli random graph with $n$ nodes and edge probability $p_\text{add}$ (but the edge weights are again sampled randomly from integers $[1, \dots, H]$). Suppose networks are combined based on Eq.~\eqref{eq:adj_t}. We then obtain
$
\mathbf{A}^{(1)} = \sum_{h=1}^{H} \left( w_h \cdot \mathbf{A}^{\text{raw}, h, (1)} + w_{\text{add}} \cdot \mathbf{A}^{\text{add}, h, (1)} \right).$
This fixed network $\mathbf{A}^{(1)}$ is used to initialize the process. 

At each subsequent time step $t > 1$, $\mathbf{A}^{(t)}$ is kept fixed and equal to $\mathbf{A}^{(1)}$, and the raw and increment networks are constructed through a randomized decomposition as follows.

\textbf{Edge Assignment.} At time step $t > 1$, the edges of $\mathcal{G}^{(t)}$ are shuffled and redistributed among $H$ hierarchies based on normalized probabilities $\{p_h\}_{h=1}^H$. The number of edges assigned to each hierarchy $h$ is sampled from a multinomial distribution:
$
\left\lvert\mathcal{E}^{h,t}\right\rvert \sim \text{Multinomial}\left(\left\lvert\mathcal{E}^{(1)}\right\rvert, \frac{p_h}{\sum_{h=1}^{H} p_h}\right),$ where $\left\lvert\mathcal{E}^{(1)}\right\rvert$ denotes the total number of edges in the initial combined network $\mathcal{G}^{(1)}$ described in $\mathbf{A}^{(1)}.$ We then randomly assign the edges to each hierarchy based on $\left\lvert\mathcal{E}^{(h,t)}\right\rvert$, and construct subgraphs $\mathcal{G}^{(h,t)}=(\mathcal{N}^{(t)}, \mathcal{E}^{(h,t)})$, which share the same node set as $\mathcal{G}^{(t)}$, such that $\mathcal{E}^{(t)}=\cup_h\mathcal{E}^{(h,t)}$ and $\cap_h\mathcal{E}^{(h,t)}=\emptyset.$
The hierarchy-level combined adjacency matrix $\mathbf{A}^{(h,t)}$ is computed by normalizing the corresponding subgraph adjacency matrix:
$$
\mathbf{A}^{(h,t)}_{i,j} = \frac{\mathbf{A}^{(t)}_{i,j}}{W_h}= \frac{\mathbf{A}^{(t)}_{i,j}}{\sum_{k=1}^{h} w_k}
$$ for $(i,j)\in\mathcal{E}^{(h,t)}$ and $\mathbf{A}^{(h,t)}_{i,j} =0$ for $(i,j)\notin\mathcal{E}^{(h,t)}$ . 
This ensures that the weighted contributions of the network in hierarchy $h$ align with the structure of $\mathbf{A}^{(t)}$.

\textbf{Network Construction.}
Recall that $\mathbf{A}^{(h,t)}=\mathbf{A}^{(\text{raw}, h, t)} + w_\text{add}\mathbf{A}^{(\text{add}, h, t)},$ and that the increment networks $\mathcal{G}^{(\text{raw}, h, t)}$ are generated as subgraphs of the raw networks, $\mathcal{G}^{(\text{raw}, h, t)}$. First, we sample edges for $\mathcal{G}^{(\text{add}, h, t)}$ probabilistically based on $ p_{\text{add}}$, from $\mathcal{G}^{(h, t)}$. We then generate a temporary increment network with edge weights randomly sampled from $[1, \dots, H].$ We denote the adjacency matrix of this temporary increment network by $\mathbf{A}^{\text{temp-add}, h, t}$. If $w_\text{add}=0,$ then we set $\mathbf{A}^{\text{add}, h, t}=\mathbf{A}^{\text{temp-add}, h, t}$. Otherwise, the edge weights of $\mathcal{G}^{(\text{add}, h, t)}$  are constrained to ensure nonnegativity and consistency:
$$
\mathbf{A}^{\text{add}, h, t}=\max\left(0, \min\left(\mathbf{A}^{\text{temp-add}, h, t}, \frac{\mathbf{A}^{(h,t)}}{w_{\text{add}}} - \epsilon\right)\right),
$$ by taking elementwise minimum and maximum, where $\epsilon$ is a small constant to ensure that $\mathcal{G}^{(\text{add}, h, t)}$ is a subgraph of $\mathcal{G}^{(\text{raw}, h, t)}$. We then construct $\mathcal{G}^{(\text{raw}, h, t)}$ by
$\mathbf{A}^{(\text{raw}, h, t)}= \mathbf{A}^{(h,t)}- w_\text{add}\mathbf{A}^{(\text{add}, h, t)}.$

At each time step, the nodes with nonzero degrees in $\mathcal{G}^{(t)}$ are identified as participating nodes. This allows us to track changes in node participation over time.

This method ensures a controlled, randomized decomposition of $\mathbf{A}^{(t)}$ into hierarchical raw and increment networks, while preserving overall structural integrity and allowing for hierarchical variability.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extended Plots}
\label{app_sec:extended_plots}
\begin{figure}[htb!]
\centering
    \begin{subfigure}[ht]{0.19\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/separate_graphs/party_party_2006_8.pdf}
      \subcaption{Type 1.}
    \end{subfigure}
    \begin{subfigure}[ht]{0.19\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/separate_graphs/party_prox5_2006_8.pdf}
      \subcaption{Type 2.}
    \end{subfigure}
    \begin{subfigure}[ht]{0.19\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/separate_graphs/party_prox2_2006_8.pdf}
      \subcaption{Type 3.}
    \end{subfigure}
    \begin{subfigure}[ht]{0.19\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/separate_graphs/party_focal_2006_8.pdf}
      \subcaption{Type 4.}
    \end{subfigure}
    \begin{subfigure}[ht]{0.19\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/separate_graphs/prox5_prox5_2006_8.pdf}
      \subcaption{Type 5.}
    \end{subfigure}
    \begin{subfigure}[ht]{0.19\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/separate_graphs/prox2_prox5_2006_8.pdf}
      \subcaption{Type 6.}
    \end{subfigure}
    \begin{subfigure}[ht]{0.19\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/separate_graphs/prox5_focal_2006_8.pdf}
      \subcaption{Type 7.}
    \end{subfigure}
    \begin{subfigure}[ht]{0.19\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/separate_graphs/prox2_prox2_2006_8.pdf}
      \subcaption{Type 8.}
    \end{subfigure}
    \begin{subfigure}[ht]{0.19\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/separate_graphs/prox2_focal_2006_8.pdf}
      \subcaption{Type 9.}
    \end{subfigure}
    \begin{subfigure}[ht]{0.19\linewidth}
      \centering
      \includegraphics[width=\linewidth]{figures/separate_graphs/grooming_2006_8.pdf}
      \subcaption{Type 10.}
    \end{subfigure}
    \caption{Visualization of the single-relationship networks for August 2006 using the same layout, where only nodes with edges are visualized. Different indices correspond to different individuals in the chimpanzee population.}
    \label{fig:separate_graphs200608}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
