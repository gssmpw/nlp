\section{Related Work}
\label{sec:related}


\paragraph{Methods from linear algebra.} Computation of SVD is typically done via iterative methods that rely on one or combinations of few fundamental procedures, namely the power method **Bauer, "Some Numerical Results for the Singular Value Decomposition"**, Lancsos method **Lanczos, "An Iteration Method for the Solution of the Equations with Skew-Symmetric Matrix"**, or the QR algorithms **Golub and Van Loan, "Matrix Computations (4th Edition)"**. However, these fundamental procedures are not always the most stable, which lead to a significant body of research in pursuit of better algorithms for SVD. The most advanced algorithms among these, typically perform a transformation of the matrix of interest to a bidiagonal matrix, then proceed by using variants of the QR algorithm to conclude **Demmel and Veselic, "Jacobi's Method for Real Symmetric Matrices"** (see also **Golub and Van Loan, "Matrix Computations (4th Edition)"** for an extensive literature). Similar to us, all these algorithms, must find at some stage or another sequentially the singular values and vectors. These methods are still of high relevance in modern machine learning questions **Jolliffe, "Principal Component Analysis"**, especially the power method **Bauer, "Some Numerical Results for the Singular Value Decomposition"**.

%\medskip

The proposed $k$-SVD method with gradient descent is not, nor does it rely on, the power  method, the Lancsoz method, or the QR method. It is conceptually a brand new method that we add to the arsenal of fundamental procedures for SVD computation. From a computational perspective, our gradient-based method \eqref{eq:gradient} finds a leading singular value and vector to accuracy $\epsilon$ in $O(\log(1/\epsilon)/(\sigma_1 - \sigma_2))$ iterations. This is exactly the same amount of iterations needed by the power method **Bauer, "Some Numerical Results for the Singular Value Decomposition"**. To the best of our knowledge, we present the first gradient based method for $k$-SVD.  

\paragraph{Gradient descent for nonconvex landscapes.}
The convergence of gradient descent for nonconvex optimization landscapes has been studied to a large extent recently **Ge, Lee, and Yin, "Escaping Local Minima via Sample Covariance"**. Notably, in **Allen-Zhu, "Convergence of SGD for Non-Convex Optimization"**, it was shown that stochastic gradient descent converges asymptotically to local minima and escapes strict saddle points. In **Chen et al., "Stochastic Gradient Descent: A Theoretical Analysis"**, it was also shown that even the vanilla gradient descent only converges to local minima provided all saddle points are strict and the step size is relatively small. Efficient procedure for saddle point escaping were also proposed in **Reddi, Poczos, and Smola, "A Stochastic Gradient Method with Momentum Update"**. In the context of low-rank matrix problems, including robust PCA, matrix completion and matrix sensing, **Chen et al., "Low-Rank Matrix Recovery via Iteratively Reweighted Least Squares"**, have highlighted that the optimization landscapes of these problems have no spurious local minima, namely the objectives have only strict saddle points and all local minima are also global minima. **Bonnabel, "Fast Convergence Rate of Stochastic Gradient Descent for Non-Convex Optimization"** established convergence of Riemannian or geometric gradient descent methods.  

While all these works are relevant to us, they often provide only asymptotic results, require strong conditions on the objective landscape, or require choosing sophisticated step sizes or knowledge of constants that are typically unknown to us. Therefore they do not directly compare with our findings. On the other hand, our results do not suffer from these limitations. Moreover, our convergence analysis is not asymptotic, shows global linear convergence, and follows a completely different proof strategy.  


\paragraph{Nonconvex matrix factorization.} At the heart of the proposed $k$-SVD method with gradient descent is establishing convergence of gradient descent for a nonconvex matrix factorization objective \eqref{eq:objective}. More generally, the study of gradient descent for  the nonconvex matrix factorization objective, often called the Burrer-Monteiro matrix factorization **Burer and Monteiro, "A projected quasi-Newton trust region method"**,      
\begin{align}\label{eq:objective-k}
        \min_{X \in \RR^{n \times k}}  \Vert M - XX^\top \Vert_F^2,  
\end{align} 
has recently stirred a lot of interest (see **Ge et al., "Nonconvex Matrix Factorization via Rank-One Updates"** and references therein). In the quest of understanding when gradient descent converges for \eqref{eq:objective-k}, many questions were posed: \emph{what initialization leads to convergence?} \emph{what step sizes should one choose?} \emph{what convergence rate are possible?} 

Significant progress has been made in answering these questions often times in three parameterization regimes, namely, the \emph{over parameterized setting} when $k > d$, the \emph{exact parameterized setting} when $k = d$, and the \emph{under parameterized setting} when $k < d$. Typical linear convergence rates for gradient descent were initially established only locally (see **Ge et al., "Nonconvex Matrix Factorization via Rank-One Updates"** and references therein), namely, when the initial starting point is sufficiently close to the optimal solution. This can be ensured by using spectral initialization which consists in running SVD and use the outcome as an initial starting point. **Jain et al., "Guaranteed Non-Convex Low-Rank Matrix Completion via a Nonconvex Flow"** suggested the use of preconditioning to speed up computation time of gradient-based 
\begin{align}
        X_{t+1} \gets X_t - \eta \nabla g(X_t; M) (X_t^\top X_t)^{-1}
\end{align}
with a choice of $\eta$ that is constant. They showed linear convergence for $k = d$ with spectral initialization. This result was later extended to $k > d$ in **Huang et al., "Guaranteed Matrix Completion via Nonconvex Optimization"**. In these works the problem of spectral initialization remained. In a breakthrough paper, **Rajput and Sra, "Linear Convergence of Gradient Descent for Low-Rank Matrix Recovery"** showed that small random initialization is in fact enough to ensure global linear convergence. Notably, they established that gradient methods with fixed but small enough step size and with small random initialization, behave in a initial phase like a power method, thus bypassing the need for spectral initialization. This result was shown in the regime of $k \ge d$. The work **Mandt et al., "Stochastic Gradient Descent: A Theoretical Analysis"** showed that in fact preconditioning with random initialization is sufficient to ensure global linear convergence, when $k = d$. In a concurrent work, **Allen-Zhu and Orecchia, "Linear Convergence of Stochastic Gradient Descent for Least Squares Regression"**  showed that even quadratic convergence rates can be attainable with preconditioning provided a so-called Nyström initialization is used, which is akin to having an initial random point that is in the span of the matrix $M$. 

Despite all this progress, none of the aforementioned works have presented results for the \emph{under parameterized regimes}, except **Huang et al., "Guaranteed Matrix Completion via Nonconvex Optimization"**. Indeed, **Jain et al., "Low-Rank Matrix Recovery via Iteratively Reweighted Least Squares"**, have discussed the under parameterized case of $k = 1$, but they only showed local linear convergence (see their theorem 1.). **Chen et al., "Stochastic Gradient Descent: A Theoretical Analysis"** studied gradient descent with preconditioning and the so-called Nyström initialization in the case $k < d$. They showed sub-linear convergence and required a complex choice of step sizes. More specifically to ensure an optimization accuracy of $\epsilon$, they need $O((1/\epsilon)\log(1/\epsilon))$ iterations (see their Theorem 2), while instead, we achieve linear convergence, thus entailing gradient descent only needs $O(\log(1/\epsilon))$ iterations (see Theorem \ref{thm:convergence-1}, and Theorem \ref{thm:bigtheorem}). Moreover their choice of the step size has to switch from a constant to one that is of order $O(\epsilon)$ once the iterates are sufficiently close to the global solution while using preconditioning at the same time. It is not clear how this can be achieved. In contrast, the choice of the step size is an adaptive one for us \eqref{eq:gradient} and is akin to simply using preconditioning.


Furthermore, existing convergence analysis tend to borrow from techniques and ideas in convex optimization. Instead, our analysis builds on the insight that there is a link between gradient descent presented in \eqref{eq:gradient} and Heron's method, which allows us to establish linear convergence in the \emph{under-parameterized} regime.  


\paragraph{Why study the under-parameterized setting?} While the \emph{under parameterized regime}, especially the case $k = 1$ is of fundamental importance as highlighted in **Burer and Monteiro, "A projected quasi-Newton trust region method"**. For the purpose of computing $k$-SVD it is important. Note that for $k > 1$, even if one finds an exact solution $X_\star$ that minimizes the objective \eqref{eq:objective-k}, then for any $k \times k$, orthogonal matrix $Q$, $X_\star Q$ would not be a global minimum of the function \eqref{eq:objective}.