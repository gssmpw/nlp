\section{Introduction}

\noindent {\bf The Task.} Consider an $M \in \RR^{m \times n}$ a matrix of rank $d \le m \wedge n$. Let
SVD of $M$ be given as $M = U \Sigma V^\top$, where $\Sigma = \diag(\sigma_1, \dots, \sigma_d) \in \RR^{d \times d}$, 
with $ \sigma_1, \dots,\sigma_d$ being the singular values of $M$ in decreasing order,  
$U \in \RR^{n \times d}$ (resp. $V \in \RR^{d \times d}$) be semi-orthogonal matrix containing 
the left (resp. right) singular vectors.\footnote{We adopt the convention that the $\ell$-th element of 
the diagonal of $\Sigma$ is $\sigma_\ell$, and that $\ell$-th column of $U$ (resp. of $V$) denoted 
$u_\ell$ (resp. denoted $v_\ell$) are its corresponding $\ell$-th left (resp. left) singular vector. 
The condition number of $M$ is defined as $\kappa = \sigma_1 / \sigma_d$.} 
Our objective is to find the the $k$-SVD of $M$, i.e. the leading $k$ singular 
values, $\sigma_i, i\leq k$,  corresponding left (resp. right) singular vectors 
$u_i, i\leq k$ (resp. $v_i, i\leq k$). For ease and clarity of exposition, we will 
consider $M \in \RR^{n\times n}$ that is symmetric, positive semi-definite 
in which case $u_i = v_i$ for all $i \in [d]$. Indeed, we can always reduce the problem of 
$k$-SVD for any matrix $M$ to that of solving the $k$-SVD of $M M^\top$ and $M^\top M$, which are both
symmetric, positive semi-definite.


\medskip
\noindent {\bf A Bit of History.} Singular Value Decomposition is fundamental linear algebraic operation, cf. see \cite{stewart1993early} for early history. It has become an essential tool for in modern machine learning 
with widespread applications in numerous fields including biology, statistics, engineering, natural language processing, 
econometric and finance, etc (e.g., see \cite{chen2021spectral} for an exhaustive list and references therein).  

Traditional algorithms for performing SVD or related problems like principal component analysis, or eigenvalue and eigenvector computation, have relied on iterative methods such as the Power method \cite{mises1929praktische}, Lancsos method \cite{lanczos1950iteration}, the QR algorithm \cite{francis1961qr}, or variations thereof for efficient and better numerical stability \cite{cullum2002lanczos}. Notably, these methods have also been combined with stochastic approximation schemes to handle streaming and random access to data \cite{oja1985stochastic}. The rich history of these traditional algorithms still impacts the solutions of modern machine learning questions \cite{arora2012stochastic, hardt2014noisy, garber2015fast, shamir2015stochastic, garber2016faster}. 

\medskip
\noindent{\bf Why Study Gradient Based Method?} 
%
Given the rich history, it makes one wonder why look for another method? Especially, gradient based. 
To start with, the $k$-SVD is a non-convex optimization and ability to understand success (or failure) of gradient based methods for non-convex setting 
can expand our understanding for optimization in general. For example, the landscape of loss function associated with PCA has served as a 
natural nonconvex surrogate to understand training for neural networks \cite{baldi1989neural} \cite{chi2019nonconvex}\cite{ge2015escaping, ge2016matrix, ge2017no, stoger2021small}. Perhaps the closest to this work are \cite{de2015global, jain2017global, stoger2021small, zhang2023preconditioned, jia2024preconditioning, li2024crucial}. It is also worth noting the recent interest in understanding non-convex matrix factorization \cite{ge2015escaping, ge2016matrix, ge2017no, stoger2021small, de2015global, jain2017global, stoger2021small, zhang2023preconditioned, jia2024preconditioning, li2024crucial}.
%
The gradient based methods are known to be robust with respect to noise or missing values, see  \cite{hazan2016introduction}. 
%
Finally, the recent emergence of scalable compute infrastructure for iterative optimization methods like gradient based method can naturally 
enable computation of $k$-SVD for very large scale matrices in a seamless manner, and potentially overcome  existing challenges with scaling 
for SVD computation, see \cite{struski2024efficient}. 

\medskip
\noindent{\bf Main Result. Gradient descent for $k$-SVD.} Like the power method, the algorithm proceeds by sequentially finding one singular value and its corresponding  
singular vector at a time in a decreasing order until all the leading $k \geq 1$ vectors are found. For finding top singular value and vector of $M$, 
minimize objective 
\begin{align}\label{eq:objective}
    g(x; M) = \frac{1}{2} \Vert M - xx^\top \Vert_F^2.
\end{align}
Towards that, gradient descent starts by randomly sampling an initial iterate, $x_1 \in \RR^{n}$. Iteratively for $t \geq 1$, update 
\begin{align}\label{eq:gradient}
x_{t+1} & = x_{t} - \frac{1}{2 \Vert x_t \Vert^2} \nabla g (x_t; M). 
\end{align}

For the above algorithm, we establish the following:
\begin{theorem}\label{thm:convergence-1}
Given a symmetric, positive semi-definite $M \in \RR^{n\times n}$ with $\sigma_1 > \sigma_2 \geq 0$, the
iterate in \eqref{eq:gradient} satisfies for all $t \ge 1$,  
\begin{align}
\Vert x_{t} + \sqrt{\sigma_1} u_1 \Vert \wedge \Vert x_t - \sqrt{\sigma_1} u_1 \Vert &\le \epsilon, \\
\Vert x_{t}x_{t}^\top   - \sigma_1 u_1 u_1^\top \Vert & \le \epsilon,
\end{align}
for any $\epsilon > 0$ as long as 
\begin{align}
t & \ge \underbrace{c_1 \left(\frac{\sigma_1 + \sigma_2}{\sigma_1 - \sigma_2} \vee 1 \right) \log\left( \frac{c_2}{\sigma_1 \epsilon} \left(\frac{\sigma_1 + \sigma_2}{\sigma_1 - \sigma_2} \vee 1 \right)\right)}_{\textrm{time to converge within attracting region}}  + \underbrace{c_3\log\left(\frac{c_4}{\sigma_1} \vee 1 \right)}_{\textrm{time to reach attracting region}}
\end{align}

where $c_1, c_2, c_3, c_4$ are constant that only depend on the initial point $x_1$;  
with random initialization $c_1, c_2, c_3, c_4$ are almost surely strictly positive. 
\end{theorem}
As a consequence of Theorem \ref{thm:convergence-1}, it immediately follows that by sequential application of \eqref{eq:gradient}, we can recover
all $k$ singular values and vectors (to the desired precision). 

\begin{theorem}\label{thm:bigtheorem}
    Let $\epsilon > 0$.  Given a symmetric, positive definite matrix $M \in \RR^{n\times n}$ with $\sigma_i - \sigma_{i+1} \gtrsim \epsilon$, $ 1 \le i \le  k$, and $\sigma_i \gtrsim \epsilon$ for $i \ge 1$. Sequential application of \eqref{eq:gradient} with random initialization, recovers  $\hat{\sigma}_1, \dots, \hat{\sigma}_k$, and $\hat{u}_1, \dots, \hat{u}_k$ such that: 
    \begin{align}
        \vert \hat{\sigma}_i - \sigma_i \vert \le \epsilon \quad \text{and} \quad \Vert \hat{u}_i + u_i\Vert \wedge \Vert \hat{u}_i - u_i \Vert \le \epsilon,
    \end{align}
    as long as the number of iterations, denoted $t$, per every application of \eqref{eq:gradient} satisfies:  
    \begin{align}
        & t \ge C_1  \max_{i\in[k]}\left( \frac{\sigma_i }{\sigma_i - \sigma_{i+1}} \vee 1 \right) \left( k \log\left( C_2 \max_{i\in[k]}\left( \frac{\sigma_i}{\sigma_i - \sigma_{i+1}} \vee 1\right)\right)  
        + \log\left( \frac{k}{\epsilon'}\right) \right)
    \end{align}   
\end{theorem}



