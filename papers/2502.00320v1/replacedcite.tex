\section{Related Work}
\label{sec:related}


\paragraph{Methods from linear algebra.} Computation of SVD is typically done via iterative methods that rely on one or combinations of few fundamental procedures, namely the power method ____, Lancsos method ____, or the QR algorithms ____. However, these fundamental procedures are not always the most stable, which lead to a significant body of research in pursuit of better algorithms for SVD. The most advanced algorithms among these, typically perform a transformation of the matrix of interest to a bidiagonal matrix, then proceed by using variants of the QR algorithm to conclude ____ (see also ____ for an extensive literature). Similar to us, all these algorithms, must find at some stage or another sequentially the singular values and vectors. These methods are still of high relevance in modern machine learning questions ____, especially the power method for its simplicity ____.

%\medskip

The proposed $k$-SVD method with gradient descent is not, nor does it rely on, the power  method, the Lancsoz method, or the QR method. It is conceptually a brand new method that we add to the arsenal of fundamental procedures for SVD computation. From a computational perspective, our gradient-based method \eqref{eq:gradient} finds a leading singular value and vector to accuracy $\epsilon$ in $O(\log(1/\epsilon)/(\sigma_1 - \sigma_2))$ iterations. This is exactly the same amount of iterations needed by the power method ____. To the best of our knowledge, we present the first gradient based method for $k$-SVD.  

\paragraph{Gradient descent for nonconvex landscapes.}
The convergence of gradient descent for nonconvex optimization landscapes has been studied to a large extent recently ____. Notably, in ____, it was shown that stochastic gradient descent converges asymptotically to local minima and escapes strict saddle points. In ____, it was also shown that even the vanilla gradient descent only converges to local minima provided all saddle points are strict and the step size is relatively small. Efficient procedure for saddle point escaping were also proposed in ____. In the context of low-rank matrix problems, including robust PCA, matrix completion and matrix sensing, ____ have highlighted that the optimization landscapes of these problems have no spurious local minima, namely the objectives have only strict saddle points and all local minima are also global minima. ____ established convergence of Riemannian or geometric gradient descent methods.  

While all these works are relevant to us, they often provide only asymptotic results, require strong conditions on the objective landscape, or require choosing sophisticated step sizes or knowledge of constants that are typically unknown to us. Therefore they do not directly compare with our findings. On the other hand, our results do not suffer from these limitations. Moreover, our convergence analysis is not asymptotic, shows global linear convergence, and follows a completely different proof strategy.  


\paragraph{Nonconvex matrix factorization.} At the heart of the proposed $k$-SVD method with gradient descent is establishing convergence of gradient descent for a nonconvex matrix factorization objective \eqref{eq:objective}. More generally, the study of gradient descent for  the nonconvex matrix factorization objective, often called the Burrer-Monteiro matrix factorization ____,      
\begin{align}\label{eq:objective-k}
        \min_{X \in \RR^{n \times k}}  \Vert M - XX^\top \Vert_F^2,  
\end{align} 
has recently stirred a lot of interest (see ____ and references therein). In the quest of understanding when gradient descent converges for \eqref{eq:objective-k}, many questions were posed: \emph{what initialization leads to convergence?} \emph{what step sizes should one choose?} \emph{what convergence rate are possible?} 

Significant progress has been made in answering these questions often times in three parameterization regimes, namely, the \emph{over parameterized setting} when $k > d$, the \emph{exact parameterized setting} when $k = d$, and the \emph{under parameterized setting} when $k < d$. Typical linear convergence rates for gradient descent were initially established only locally (see ____ and references therein), namely, when the initial starting point is sufficiently close to the optimal solution. This can be ensured by using spectral initialization which consists in running SVD and use the outcome as an initial starting point. ____ suggested the use of preconditioning to speed up computation time of gradient-based 
\begin{align}
        X_{t+1} \gets X_t - \eta \nabla g(X_t; M) (X_t^\top X_t)^{-1}
\end{align}
with a choice of $\eta$ that is constant. They showed linear convergence for $k = d$ with spectral initialization. This result was later extended to $k > d$ in ____. In these works the problem of spectral initialization remained. In a breakthrough paper, ____ showed that small random initialization is in fact enough to ensure global linear convergence. Notably, they established that gradient methods with fixed but small enough step size and with small random initialization, behave in a initial phase like a power method, thus bypassing the need for spectral initialization. This result was shown in the regime of $k \ge d$. The work ____, showed that in fact preconditioning with random initialization is sufficient to ensure global linear convergence, when $k = d$. In a concurrent work, ____  showed that even quadratic convergence rates can be attainable with preconditioning provided a so-called Nyström initialization is used, which is akin to having an initial random point that is in the span of the matrix $M$. 

Despite all this progress, none of the aforementioned works have presented results for the \emph{under parameterized regimes}, except ____. Indeed, ____ have discussed the under parameterized case of $k = 1$, but they only showed local linear convergence (see their theorem 1.). ____ studied gradient descent with preconditioning and the so-called Nyström initialization in the case $k < d$. They showed sub-linear convergence and required a complex choice of step sizes. More specifically to ensure an optimization accuracy of $\epsilon$, they need $O((1/\epsilon)\log(1/\epsilon))$ iterations (see their Theorem 2), while instead, we achieve linear convergence, thus entailing gradient descent only needs $O(\log(1/\epsilon))$ iterations (see Theorem \ref{thm:convergence-1}, and Theorem \ref{thm:bigtheorem}). Moreover their choice of the step size has to switch from a constant to one that is of order $O(\epsilon)$ once the iterates are sufficiently close to the global solution while using preconditioning at the same time. It is not clear how this can be achieved. In contrast, the choice of the step size is an adaptive one for us \eqref{eq:gradient} and is akin to simply using preconditioning.


Furthermore, existing convergence analysis tend to borrow from techniques and ideas in convex optimization. Instead, our analysis builds on the insight that there is a link between gradient descent presented in \eqref{eq:gradient} and Heron's method, which allows us to establish linear convergence in the \emph{under-parameterized} regime.  


\paragraph{Why study the under-parameterized setting?} While the \emph{under parameterized regime}, especially the case $k = 1$ is of fundamental importance as highlighted in ____. For the purpose of computing $k$-SVD it is important. Note that for $k > 1$, even if one finds an exact solution $X_\star$ that minimizes the objective \eqref{eq:objective-k}, then for any $k \times k$, orthogonal matrix $Q$, $X_\star Q$ also minimizes \eqref{eq:objective-k}. This rotation problem poses a challenge in using the objective \eqref{eq:objective-k} for $k$-SVD while the objective \eqref{eq:objective} doesn't.