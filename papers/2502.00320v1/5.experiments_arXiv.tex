\section{Experiments}\label{sec:experiments}

We present here some of our experimental findings that consolidate our theoretical findings. We mainly compare with the power method to test the sensitivity of the gradient method with respect to $n$, $\sigma_1 - \sigma_2$, and runtime.   


\subsection{Experimental setup} 

Here we describe the generating mechanisms of the synthetic experiments we consider. We describe the generating mechanisms of the matrices we consider below:

\paragraph{The rank-$2$ setting.} For this setting, we consider matrices of rank $2$. We generate $U$ and $V$ uniformly at random from the space of semi-orthogonal matrices of size $n \times \log \lfloor \log (2) \rfloor$, and set $M = U\Sigma V^\top$, with $\Sigma = \diag(\sigma_1, \sigma_2)$ with $\sigma_1 = 1$, and $ \sigma_1 - \sigma_2$ varying in $\lbrace 10^{-k/4}: k \in [20] \rbrace$. These types of matrices, will be used to inspect the performance of the gradient descent method with respect to the gap $\sigma_1 - \sigma_2$.



\paragraph{The rank-$\log(n)$ settings.} Here, we focus on matrices of rank  $\lfloor \log(n) \rfloor $. First, we generate orthonormal matrices $U$ and $V$ uniformly at random from the space of semi-orthogonal matrices of size $n \times \lfloor \log (n) \rfloor$. Then we construct matrices of the form  $M = U \Sigma V^\top$ where $\Sigma = \diag(\sigma_1, \dots, \sigma_{\lfloor \log(n) \rfloor})$ and set the singular values $\sigma_1, \dots, \sigma_{\lfloor \log(n) \rfloor}$ to decay in three different ways: 
\begin{itemize}
    \item \emph{(i) Exponentially decaying singular values.} In this case, sample $a$ at random from $\lbrace 2, \dots, 10\rbrace$, then set $\sigma_i = a^{-i}$, for $i \in [\lfloor\log(n) \rfloor]$; 
    \item \emph{(ii) Polynomially decaying singular values.} Here we set $\sigma_i = i^{-1} + 1$, for $i \in [\lfloor\log(n) \rfloor]$.
    \item \emph{(iii) Linearly decaying singular values.} We sample $a $ uniformly at random from $\lbrace 1, \dots, 10\rbrace$ and $b$ uniformly at random from $[0,1]$, then we set $\sigma_i = a - b i$, for $i \in [\lfloor\log(n) \rfloor]$.
\end{itemize}
We use this setting to inspect the sensitivity of the gradient descent method with respect to scale of the matrix $n$ and compute time. 




\subsection{Implemented methods} 

When considering asymmetric matrices $M$ in the experiments, we apply $k$-SVD with the considered methods on $M M^\top$ to identify the leading $k$ singular values of $M$, $\hat{\sigma}_1, \dots, \hat{\sigma}_k$ and corresponding left singular vectors $\hat{u}_1, \dots, \hat{u}_k$, we then simply recover the right singular vectors by setting $\hat{v}_i = \hat{\sigma_i}^{-1} M^\top \hat{u}_i$, for $i \in [k]$. 


\paragraph{$k$-SVD with gradient descent.} The implementation $k$-SVD  with gradient is done by sequentially applying gradient descent to find one singular value and corresponding vectors at a time. At each time, we run gradient descent to minimize \eqref{eq:objective} with step sizes $\eta/\Vert x_t \Vert^2$ where $\eta \in \lbrace 0.3, 0.5, 0.7 \rbrace$. We use the initialization described in \eqref{eq:init-grad}. For every run of gradient descent, we stop as soon as $\Vert \Vert x_t \Vert^{-1} x_{t} - M M^\top x_t \Vert < \epsilon$ where $\epsilon$ is the tolerance threshold. We use $\epsilon = 10^{-14}$.  For a detailed pseudo-code refer to Appendix \ref{app:experiments}.


\paragraph{$k$-SVD with power method.} We implement a power method for finding the $k$-SVD of $M$. As with the gradient based method, we also proceed sequentially finding one singular value at a time and corresponding vector at a time. The method proceeds by iterating  the equations $x_{t+1} =  \Vert MM^\top x_t \Vert^{-1} MM^\top x_t $ until convergence. For the given tolerance $\epsilon = 10^{-14}$, we stop as soon as $\Vert x_{t+1} -   \Vert MM^\top x_t \Vert^{-1} MM^\top x_t \Vert\le \epsilon $. The method is initialized in the same fashion as the one with gradient descent. For a detailed pseudo-code refer to Appendix \ref{app:experiments}.




\subsection{Results}


In Fig.\ref{fig:gap-to-iterations} we report the growth of the number of iterations as the gap $\sigma_1 - \sigma_2$ changes running both the power method and gradient descent for finding the leading singular value and its corresponding vector. We generate matrices as described in the rank-$2$ setting for different values of $n \in \lbrace 50, 75, 100, 200, \dots, 1000\rbrace$. The curves are  obtained by averaging over multiple curves. Indeed, we see that the the dependence on the gap does not depend on the size of the matrices which is consistent with our theoretical findings in Theorem  \ref{thm:convergence-1}.
\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{.48\textwidth}
    \includegraphics[width=1\linewidth]{gap_to_iters_nohs.png}
    \caption{}
    \label{fig:gap-to-iterations}
    \end{subfigure}
   \begin{subfigure}[b]{.48\textwidth}
    \includegraphics[width=1\linewidth]{conv_gd_svd-0.5_1000.png}
    \caption{}
    \label{fig:convergence}
   \end{subfigure}
   \caption{}
   \caption{\emph{(a) Gap vs. Number of iterations.} We run the gradient methods and power method on matrices $M$ generated as per described in the rank-$2$ setting for values of $n \in \lbrace 50, 75, 100, \dots, 1000\rbrace$. We report the averaged curves ($\pm$ std. in the shaded area). All data points corresponding to runs below $10^3$ completed within the prescribed threshold; \emph{(b) Illustration of the convergence of $k$-SVD with gradient descent.} Here, the method was tested on three matrices $M$, each generated according the rank-$\log(n)$ description with $n = 1000$.}
\end{figure}

In Fig. \ref{fig:convergence}, we illustrate the convergence of the $k$-SVD method with gradient descent. The plots were obtained by running the method on matrices generated according to the rank-$\log(n)$ setting with $n=1000$. The results show that indeed the method converges as suggested by the theory (see Theorem \ref{thm:bigtheorem}).

\begin{figure}[ht!]
    \centering 
    \begin{subfigure}[b]{.32\textwidth}
        \includegraphics[width=1\textwidth]{runtime_ranklogn-lin_nohs.png} 
    \end{subfigure}
    \begin{subfigure}[b]{.32\textwidth}
        \includegraphics[width=1\textwidth]{runtime_ranklogn-poly_nohs.png} 
    \end{subfigure}
    \begin{subfigure}[b]{.32\textwidth}
        \includegraphics[width=1\textwidth]{runtime_ranklogn-exp_nohs.png}
    \end{subfigure}
    \caption{\emph{Runtime vs. Size.} We compare the runtime vs. size curves of the $k$-SVD method with gradient descent with  and  with power method. Here the matrices were generated according to the rank-$\log(n)$ setting, where (a) corresponds to \emph{the linearly decaying singular values setting}, (b) corresponds to \emph{the polynomially decaying singular values setting}, and (c) corresponds to \emph{the exponentially decaying singular values setting}.}
    \label{fig:runtime vs size 1}
\end{figure}

In Fig. \ref{fig:runtime vs size 1} and Fig. \ref{fig:runtime vs size 2} we report the evolution of the runtime of the various considered methods against the size of the matrices. Overall, we see that both, the $k$-SVD method with  gradient descent and with power method behave have similar performance. When the singular values are decaying linearly, we notice that the power method has some level of variance as the size increase suggesting some numerical instability of sorts but the methods do converge. In contrast, the $k$-SVD method with gradient descent performs consistently across all experiments, thus suggesting robustness of the method.    


\begin{figure}[ht!]
    \centering 
    \begin{subfigure}[b]{.32\textwidth}
        \includegraphics[width=1\textwidth]{runtime_rank2-10e-1.75_nohs.png}
        \caption{$\sigma_1 - \sigma_2 = 10^{-7/4}$}
    \end{subfigure} 
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=1\textwidth]{runtime_rank2-10e-2.0_nohs.png}
        \caption{$\sigma_1 - \sigma_2 = 10^{-2}$}
    \end{subfigure} 
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=1\textwidth] {runtime_rank2-10e-2.25_nohs.png}
        \caption{$\sigma_1 - \sigma_2 = 10^{-9/4}$}
    \end{subfigure}
    \caption{\emph{Runtime vs. Size.} We compare the runtime vs. size curves of the $k$-SVD method with gradient descent with  and  with power method. Here the matrices were generated according to the rank-$2$ setting, where (a) corresponds to $\sigma_1 - \sigma_2 = 10^{-7/4}$, (b) corresponds to $\sigma_1 - \sigma_2 = 10^{-7/4}$, and (c) $\sigma_1 - \sigma_2 = 10^{-9/4}$.}
        \label{fig:runtime vs size 2}
\end{figure}

