
\newpage
\section{Implementation and Experiments} \label{app:experiments}

In this section, we present and validate a GPU-friendly implementation of the $k$-SVD methods discussed in \textsection \ref{sec:experiments}. This implementation was developed on 6-core, 12-thread Intel chip, and tested on a 10-core, 20-thread Intel chip and NVIDIA CUDA 12.7.

\subsection{Implementation details}

\paragraph{Notation.} We use $U_\ell$ to denote the $\ell^{th}$ singular vector (column) of $U$,  $V^\top_\ell$ to denote the $\ell^{th}$ singular vector (row) of $V^\top$, and $S_\ell$ to denote the $\ell^{th}$ entry.

%\subsubsection{$k$-SVD with power method.}

\begin{algorithm}
\caption{Computes rank-SVD of an $m\times n$ matrix $M$ via power method}\label{alg:powermethod}
\begin{algorithmic}
    \State $U \gets$ a random $n \times k$ semi-orthogonal matrix
    \State $V \gets$ a random $n \times k$ semi-orthogonal matrix
    \State$S \gets \vec{1}_k$
    \State $M_1 \gets M$
    \For{$ \ell= 1, ..., k$}
    \While{$t < T$ and ($\Vert U_{\ell, t+1}-U_{\ell, t}\Vert_2 \geq \epsilon$ or $\Vert V^\top_{\ell, t+1} - V^\top_{\ell, t}\Vert_2 \geq \epsilon$)}
            \State $U_\ell \gets M_\ell V^\top_\ell$
            \State$V^\top_\ell \gets M_\ell^\top U_\ell$
            \State $S_\ell \gets \Vert U_\ell\Vert_2\Vert V^\top_\ell\Vert_2$
            \State $U_\ell \gets  U_\ell/\Vert U_\ell\Vert_2$
            \State $V^\top_\ell \gets V^\top_\ell/ \Vert V^\top_\ell\Vert _2$
            \State$t\gets t+1$
        \EndWhile
        \State$M_{\ell+1} \gets M_\ell - S_\ell U_\ell V^\top_\ell$
    \EndFor 
\State Return $U,S,V^\top$ 
\end{algorithmic}
\end{algorithm}



\begin{algorithm}
\caption{Computes $k$-SVD of an $m\times n$ matrix $M$ via gradient descent with step size $\eta / \Vert x\Vert ^2$}\label{alg:gdsvd}
\begin{algorithmic}
    \State $U \gets$ a random $n \times k$ semi-orthogonal matrix
    \State $V \gets$ a random $n \times k$ semi-orthogonal matrix
    \State$S \gets \vec{1}_k$
    \State $M_\ell \gets M$
    \For{$\ell= 1, ..., k$}
        \State $x_1\gets M_\ell V^\top_\ell$ 
        \While{$t < T$ and $\left\Vert \frac{x_{t+1}}{\Vert x_{t+1}\Vert _2} - \frac{M_\ell M_\ell^\top x_t}{\Vert x_t\Vert _2^2}\right\Vert_2 \geq \epsilon$}
            \State $S_\ell \gets \Vert x_t\Vert _2$
            \State $U_\ell \gets x_t/\Vert x_t\Vert _2$
            \State $V^\top_\ell \gets M_\ell^\top x_t/\Vert x_t\Vert$
            \State $x_{t+1} \gets (1-\eta)x_t + \frac{\eta}{\Vert x_t \Vert^2 } M_\ell M_\ell^\top x_t$
            \State $t\gets t+1$
        \EndWhile
        \State $M_\ell \gets M_\ell - S_\ell U_\ell V^\top_\ell$
    \EndFor
%\Return 
\State Return $U,S,V^\top$\;

\end{algorithmic}
\end{algorithm}

\noindent {\bf Notes on implementation. } A temporary file to store $M_\ell$ is memory-mapped during computation, rather than subtracting off previous rank-approximations each time $M_\ell$ is required. The latter takes only $O(1)$ memory but significantly slows computation. The centralized implementations were written in Python 3.x and these are the versions tested for correctness guarantees. All of the tested SVD methods spend the most time on matrix-vector multiplication, which is easily parallelizable by spawning threads each responsible for computing one index of the resulting vector, so that each thread only requires $O(\max\{n,m\})$ space and time. Several implementations of parallelization were tested:
\begin{itemize}
    \item Cython: native Python fed through Cython
    \item Numba: functions are jitted with Numba decorators
    \item CuPy: $U,S,V^\top$ and temporary variables are instantiated on the GPU device, rows/columns of $M$ are moved to the device when used for computation
    \item C/OpenCilk: a C implementation was attempted with parallel library OpenCilk but it suffered from numerical stability issues and required a great deal of step-size engineering
\end{itemize}
None were satisfactory in reducing the scaling of runtime with input matrix size, possibly due to unideal memory management, but CUDA specific programming may be able to improve performance.

\subsection{Correctness testing}
Synthetic datasets with various singular value profiles were generated to test correctness. All datasets consist of $M,U,S,V^\top$ stored in binary files, where $U,S,V^\top$ are generated according to a scheme detailed below and $M$ computed as the $n\times n$ square matrix result of $USV^\top$ ($n$ ranges from 50 to 1,000).

\paragraph{Synthetic $U,S,V^\top$ schemes.} 
\begin{enumerate}
    \item {\bf Rank $\log n$ dataset.} $U$ is a $n \times \lfloor\log n\rfloor$ orthonormal matrix generated using \texttt{scipy.stats.ortho\_group} and $V^\top$ a $\lfloor\log n\rfloor\times n$ matrix generated the same way and transposed. 
    \begin{itemize}
        \item Exponential decay: $\sigma_i = a^{-i}$ where $a$ is generated by \texttt{numpy.random.randint(1,10)}
        \item Polynomial decay: $\sigma_i = 1/({i+1})$
        \item Linear decay: $\sigma_i = a - bi$, where $a$ is generated by \texttt{numpy.random.randint(1,10)} and $b$ is generated by \texttt{numpy.random.rand()}, regenerating until $\sigma_i> 0 \quad\forall i$
    \end{itemize}
    \item {\bf Rank 2 dataset.} $U$ is a $n \times 2$ orthonormal matrix generated using \texttt{scipy.stats.ortho\_group} and $V^\top$ a $2\times n$ matrix generated the same way and transposed. $\sigma_1=1$ and $\sigma_2$ chosen as $\sigma_1 - 10^{-\delta}$ for $\delta\in[0.25,5]$ in $0.25$ increments.
\end{enumerate}

\paragraph{Test procedure.} Test files were memory-mapped using \texttt{np.memmap} and checked for contiguous formatting using \texttt{np.contiguousarray} before passing into any SVD method. We record several metrics:
\begin{itemize}
    \item Approximation error: $\Vert M-USV^\top\Vert_F$
    \item Subspace error: $\Vert U^\top U - \hat{U}^\top\hat{U}\Vert_F$ (analogous for $V^\top$)
    \item Orthogonal error: $\Vert U U^\top - I_n \Vert_F$ (analogous for $V^\top$)
\end{itemize}
including partial errors at the end of each iteration, as well as total runtime and final errors.

