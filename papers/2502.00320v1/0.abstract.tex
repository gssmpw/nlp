
\begin{abstract}
    We show that a gradient-descent with a simple, universal rule for step-size selection provably finds $k$-SVD, i.e., the $k\geq 1$ largest singular values and corresponding vectors, of any matrix, despite nonconvexity. There has been substantial progress towards this in the past few years where existing results are able to establish such guarantees for the \emph{exact-parameterized} and \emph{over-parameterized} settings, with choice of oracle-provided step size. 
    But guarantees for generic setting with a step size selection that does not require oracle-provided information has remained a challenge.
%        
    We overcome this challenge and establish that gradient descent with an appealingly simple adaptive step size (akin to preconditioning) and random initialization enjoys global linear convergence for generic setting. Our convergence analysis reveals that the gradient method has an attracting region, and within this attracting region, the method behaves like Heron's method (a.k.a. the Babylonian method). 
% 
    Empirically, we validate the theoretical results. The emergence of modern compute infrastructure for iterative optimization coupled with this work is likely 
    to provide means to solve $k$-SVD for very large matrices. 
    
%    From a practical standpoint, motivated by modern computing and storage infrastructures that allow efficient and distributed deployment of gradient methods at scale, we believe our proposed gradient method may open new opportunities for performing top-$k$ SVD for extremely large matrices. Finally, we empirically demonstrate the correctness of the gradient-based method and propose new adaptive step sizes suggesting further improvements. 

     

    
    

    %We consider the task of computing the Singular Value Decomposition of extremely large matrices. To that end, we propose a gradient descent method that provably solves this task. The method proceeds by finding  singular values with their corresponding singular vectors in decreasing order by solving at each time a non-convex objective. Interestingly, despite dealing with a non-convex objective we show convergence of the method. Moreover, our analysis reveals that gradient descent with a certain adaptive step-size acts as if running a Babylonian method (a.k.a. Heron's method). When the matrix of interest is of rank 1, we obtain a quadratic convergence rate in almost sure sense. In the case of any rank, we establish a linear convergence rate in almost sure sense.          
\end{abstract}


