\section{Proofs}\label{ref:appA}

\subsection{Proof of Proposition \ref{prop:grad convergence rank 1}}

\begin{proof}[Proof of Proposition \ref{prop:grad convergence rank 1}]
 First, we immediately see that the event $\lbrace \Vert x_1 \Vert \neq 0, \quad \text{and} \quad x_1 = \Vert x_1 \Vert u_1 \rbrace$ holds almost surely. 

We start with the observation that for all $t \ge 1$, $\vert x_t^\top u_1 \vert = \Vert x_t \Vert $. This is because by construction the initial point $x_1$ is already in the span of $M$. Next, we have that for all $t\ge1$, 
\begin{align*}
    x_{t+1}^\top u_1 = \left( (1- \eta)  + \eta \frac{\sigma_1}{\Vert x_t \Vert^2} \right) x_t^\top u_1 
\end{align*}
which leads to 
\begin{align*}
    \Vert x_{t+1} \Vert =   (1- \eta) \Vert x_t \Vert  + \eta \frac{\sigma_1}{\Vert x_t \Vert}. 
\end{align*}
In the above we recognize the iterations of a Babylonian method (a.k.a. as Heron's method) for finding the square root of a real number. For completeness, we provide the proof of convergence. Let us denote 
\begin{align*}
    \epsilon_t = \frac{\Vert x_t \Vert}{\sigma_1} - 1
\end{align*}
by replacing in the equation above we obtain that 
\begin{align*}
    \epsilon_{t+1} & = (1- \eta)(\epsilon_t + 1) + \frac{\eta}{(\epsilon_t + 1)} - 1 \\
    & = \frac{(1-\eta)\varepsilon_t^2 + (1- 2 \eta)\varepsilon_t}{\varepsilon_t + 1}.
\end{align*}
With the choice $\eta = 1/2$, we obtain that 
\begin{align*}
    \epsilon_{t+1} = \frac{\epsilon_t^2 }{2(\epsilon_t + 1)}
\end{align*}
from which we first conclude that for all $t\ge 1$, $\varepsilon_{t+1} > 0$ (note that we already have $\varepsilon_t > - 1 $). Thus we obtain 
\begin{align*}
    0 < \epsilon_{t+1} \le \frac{1}{2}\min\left( \epsilon_t^2, \epsilon_t  \right).
\end{align*}
This clearly implies that we have quadratic convergence. 
\end{proof}





\subsection{Proof of Theorem \ref{thm:convergence-1} and Theorem \ref{thm:convergence-2}}\label{sec:proof thm1}

Theorem \ref{thm:convergence-2} is an intermediate step in proving Theorem \ref{thm:convergence-1} and is therefore included in the proof below.

\begin{proof}[Proof of Theorem \ref{thm:convergence-1}]
    First, by applying Lemma \ref{lem:attracting region} (with $\eta = 1/2$), that after $\tau = \log(\frac{\Vert x_1 \Vert}{\beta \sqrt{\sigma_1}})$ that for all $t > \tau $, we have 
    $$
    \alpha \sqrt{\sigma_1} \le \Vert x_{t} \Vert \le \beta \sqrt{\sigma_1}.
    $$
    and we also have 
    \begin{align}
        \left\vert \frac{\Vert x_t \Vert^2}{\sigma_1} - 1 \right\vert \le   \left\vert \frac{\Vert x_t \Vert}{\sqrt{\sigma_1}} - 1 \right\vert \left\vert \frac{\Vert x_t \Vert}{\sqrt{\sigma_1}} + 1 \right\vert  \le ( \beta + 1)\left\vert \frac{\Vert x_t \Vert}{\sqrt{\sigma_1}} - 1 \right\vert 
    \end{align}
    Thus, using the decompositions \ref{lem:error-decomposition} and applying Lemma \ref{lem:conv singular vector} and \ref{lem:conv singular value},  we obtain that  
    \begin{align*}
         \Vert x_{t+\tau} \pm \sqrt{\sigma_1} u_1 \Vert & \le (\beta + 1)\sqrt{\sigma_1} t \left( \left(1 + \frac{\sigma_1 - \sigma_2 }{\beta^2 \sigma_1 + \sigma_2}\right) \vee \sqrt{2}  \right)^{-2t} \left(  \frac{\vert \tan(\theta_{1,1})\vert^2}{4\alpha \vert \cos(\theta_{1,1})\vert^2} \left(\beta + \frac{1}{\alpha}\right)^2 + \frac{\vert \tan(\theta_{1,1})\vert}{\vert \cos(\theta_{1,1})\vert}\left(\beta + \frac{1}{\alpha}\right) \right)  \\
         & + \sqrt{2\sigma_1}  \left( 1 + \frac{ (\sigma_1 - \sigma_2)}{ \beta^2 \sigma_1   +  \sigma_2} \right)^{-t} \vert \tan(\theta_{1, 1}) \vert^2
    \end{align*}
    We denote 
    \begin{align*}
           C_1 &= (\beta + 1)\left(  \frac{\vert \tan(\theta_{1,1})\vert^2}{4\alpha \vert \cos(\theta_{1,1})\vert^2} \left(\beta + \frac{1}{\alpha}\right)^2 + \frac{\vert \tan(\theta_{1,1})\vert}{\vert \cos(\theta_{1,1})\vert}\left(\beta + \frac{1}{\alpha}\right) \right) \\
           C_2 & = \sqrt{2}\vert \tan(\theta_{1, 1}) \vert^2
    \end{align*}
    and write 
    \begin{align*}
         \Vert x_{t+\tau} \pm \sqrt{\sigma_1} u_1 \Vert & \le C_1  \sqrt{\sigma_1} t  \left( \left(1 + \frac{\sigma_1 - \sigma_2 }{\beta^2 \sigma_1 + \sigma_2}\right) \vee \sqrt{2}  \right)^{-2t} + C_2\sqrt{\sigma_1}  \left( 1 + \frac{ (\sigma_1 - \sigma_2)}{ \beta^2 \sigma_1   +  \sigma_2} \right)^{-t}
    \end{align*}
    We can verify that 
    \begin{align}
        & t \ge 2 \left( \frac{\beta^2 \sigma_1 + \sigma_2}{\sigma_1 - \sigma_2} + 1 \right) \vee \left(\frac{\sqrt{2}}{\sqrt{2} - 1} \right)\log\left( 2 \left( \frac{\beta^2 \sigma_1 + \sigma_2}{\sigma_1 - \sigma_2} + 1 \right) \vee \left(\frac{\sqrt{2}}{\sqrt{2} - 1} \right) \right)  \label{eq:condition 1} \\
        & \qquad \qquad \qquad \qquad\qquad \qquad \qquad \qquad \qquad \implies t \ge \left( \frac{\beta^2 \sigma_1 + \sigma_2}{\sigma_1 - \sigma_2} + 1 \right) \vee \left(\frac{\sqrt{2}}{\sqrt{2} - 1} \right)\log(t)    \\
        & \qquad \qquad\qquad \qquad\qquad \qquad \qquad \qquad \qquad \implies t \left( \left(1 + \frac{\sigma_1 - \sigma_2 }{\beta^2 \sigma_1 + \sigma_2}\right) \vee \sqrt{2}  \right)^{-t} \le 1
    \end{align}
    where we use the elementary fact that if $t \ge 2a \log(2a)$ then $t \ge a \log(t)$ for any $a > 0$ and  $\log(1 + x) \ge \frac{x}{1 + x}$ for all $x > 0$.
    Thus, under condition \eqref{eq:condition 1}
    we obtain 
    \begin{align*}
         \Vert x_{t+\tau} \pm \sqrt{\sigma_1} u_1 \Vert & \le C_1  \sqrt{\sigma_1}   \left( \left(1 + \frac{\sigma_1 - \sigma_2 }{\beta^2 \sigma_1 + \sigma_2}\right) \vee \sqrt{2}  \right)^{-t} + C_2\sqrt{\sigma_1}  \left( 1 + \frac{ (\sigma_1 - \sigma_2)}{ \beta^2 \sigma_1   +  \sigma_2} \right)^{-t},
    \end{align*}
    In view of Lemma \ref{lem:error-decomposition}, the statement concerning $\Vert x_t x_t^\top - \sigma_1 u_1 u_1^\top \Vert$ follows similarly. At this stage we have shown the statement of Theorem \ref{thm:convergence-2}.
    
    We also see that 
    \begin{align}
        t \ge  \left(\frac{\beta^2 \sigma_1 + \sigma_2}{\sigma_1 - \sigma_2 } + 1\right)\vee \left( \frac{\sqrt{2}}{\sqrt{2}-1}\right) \log\left( \frac{2}{C_1 \sqrt{\sigma_1} \epsilon }  \right) & \implies C_1  \sqrt{\sigma_1}   \left( \left(1 + \frac{\sigma_1 - \sigma_2 }{\beta^2 \sigma_1 + \sigma_2}\right) \vee \sqrt{2}  \right)^{-t} \le \frac{\epsilon}{2} \label{eq:condition 2}  \\
        t \ge  \left(\frac{\beta^2 \sigma_1 + \sigma_2}{\sigma_1 - \sigma_2 } + 1\right) \log\left( \frac{2}{C_2 \sqrt{\sigma_1} \epsilon }  \right) & \implies C_1  \sqrt{\sigma_1}    \left(1 + \frac{\sigma_1 - \sigma_2 }{\beta^2 \sigma_1 + \sigma_2}  \right)^{-t} \le \frac{\epsilon}{2} \label{eq:condition 3} 
    \end{align}
    where we used again the elementary fact $\log(1 + x) \ge \frac{x}{1 + x}$. We conclude that if 
    \begin{align*}
        t \ge c_1 \left( \frac{\beta^2 \sigma_1 + \sigma_2}{\sigma_1 - \sigma_2} \vee 1\right) \log\left( \frac{c_2}{\sigma_1 \epsilon} \left( \frac{\beta^2 \sigma_1 + \sigma_2}{\sigma_1 - \sigma_2} \vee 1\right)\right) \qquad \text{and} \qquad \tau  =  \log\left( \frac{\Vert x_t \Vert}{\beta \sqrt{\sigma_1}}\right)
    \end{align*}
    then 
    \begin{align*}
        \Vert x_t \pm \sqrt{\sigma_1} u_1 \Vert \le \epsilon. 
    \end{align*}
\end{proof}




\subsection{Error decompositions}

\begin{lemma}\label{lem:spectral-perturbation}
    Let $M, \tilde{M} \in \RR^{n \times n}$ symmetric matrices. Let $\sigma_1, \dots, \sigma_n$ (resp. $\tilde{\sigma}_1, \dots, \tilde{\sigma}_n$) be the eigenvalues of $M$ (resp. $\tilde{M}$) in decreasing order, and $u_1, \dots, u_d$ (resp. $\tilde{u}_1, \dots, \tilde{u}_k$) be their corresponding eigenvectors. Then, for all $\ell \in [k]$, we have   
    \begin{align}
        \min_{W \in \mathcal{O}^{\ell \times \ell}} \Vert U_{1:\ell} - \tilde{U}_{1:\ell} W \Vert  & \le \sqrt{2} \frac{\Vert M - \tilde{M} \Vert }{\sigma_\ell - \sigma_{\ell+1}} \\
        \vert \sigma_i - \tilde{\sigma}_{i} \vert  & \le  \Vert M - \widetilde{M} \Vert  
    \end{align}
    where $\mathcal{O}^{\ell \times \ell}$ denotes the set of $\ell \times \ell$ orthogonal matrices, and using the convention that $\sigma_{d+1} = 0$.
\end{lemma}
\begin{proof}[Proof of \ref{lem:spectral-perturbation}]
    The proof of the result is an immediate consequence of Davis-Kahan and the inequality $\min_{W \in \mathcal{O}^{\ell \times \ell}} \Vert U_{1:\ell} - \tilde{U}_{1:\ell} W \Vert \le \Vert \sin( U_{1:\ell}, \tilde{U}_{1:\ell}) \Vert $ (e.g., see \cite{cape2019two}). The second inequality is simply Weyl's inequality
\end{proof}



\begin{lemma}[Error decompositions]\label{lem:error-decomposition}
    When $\Vert x_t \Vert \neq 0$, we have 
    \begin{align*}
        \left\Vert x_t x_t^\top - \sigma_1 u_1 u_1^\top \right\Vert  & \le \, \sigma_1 \left( 3 \left\vert \frac{\Vert x_t \Vert^2}{\sigma_1} - 1 \right\vert + \left\Vert \frac{1}{\Vert x_t \Vert} x_t - u_1 \right\Vert \wedge \left\Vert \frac{1}{\Vert x_t \Vert} x_t + u_1 \right\Vert   \right) \\ 
        \Vert x_t - \sqrt{\sigma_1} u_1 \Vert \wedge \Vert x_t + \sqrt{\sigma_1} u_1 \Vert  & \le \sqrt{\sigma_1}\left( \left\vert \frac{\Vert x_t \Vert^2}{\sigma_1} - 1 \right\vert +  \left\Vert \frac{1}{\Vert x_t \Vert} x_t - u_1 \right\Vert \wedge \left\Vert \frac{1}{\Vert x_t \Vert} x_t + u_1 \right\Vert  \right)
    \end{align*}
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lem:error-decomposition}]
     For ease of notation, we use the shorthand $\tilde{x}_t = \Vert x_t \Vert^{-1} x_t$. First, we have: 
\begin{align*}
        \left\Vert x_t x_t^\top - \sigma_1 u_1 u_1^\top \right\Vert & = \left\Vert (\Vert x_t \Vert^2 - \sigma_1) \left( \tilde{x}_t \tilde{x}_t^\top -  u_1 u_1^\top \right) + \sigma_1 \left( \tilde{x}_t \tilde{x}_t^\top -  u_1 u_1^\top \right) +  (\Vert x_t \Vert^2 - \sigma_1) u_1 u_1^\top  \right\Vert  \\
        & \le  \vert \Vert x_t \Vert^2 - \sigma_1\vert   \left\Vert  \tilde{x}_t \tilde{x}_t^\top -  u_1 u_1^\top \right\Vert + \sigma_1 \left\Vert  \tilde{x}_t \tilde{x}_t^\top -  u_1 u_1^\top \right\Vert  +  \vert \Vert x_t \Vert^2 - \sigma_1\vert  \\
        & \le \sigma_1 \left( \left\vert \frac{\Vert x_t \Vert^2}{\sigma_1} - 1 \right\vert \left\Vert  \tilde{x}_t \tilde{x}_t^\top -  u_1 u_1^\top \right\Vert + \left\Vert  \tilde{x}_t \tilde{x}_t^\top -  u_1 u_1^\top \right\Vert +  \left\vert \frac{\Vert x_t \Vert^2}{\sigma_1} - 1 \right\vert    \right) \\
        & \le \sigma_1 \left( 3 \left\vert \frac{\Vert x_t \Vert^2}{\sigma_1} - 1 \right\vert + \left\Vert  \tilde{x}_t \tilde{x}_t^\top -  u_1 u_1^\top \right\Vert    \right)
\end{align*}
Next, we also have 
\begin{align*}
    \left\Vert  \tilde{x}_t \tilde{x}_t^\top -  u_1 u_1^\top \right\Vert^2 & \le  \left\Vert  \tilde{x}_t \tilde{x}_t^\top -  u_1 u_1^\top \right\Vert_F^2 \\
    & \le (2 - 2 (u_1^\top x_t)^2) \\
    & \le \frac{1}{2}(2 -  2(u_1^\top x_t))(2 +  2(u_1^\top x_t)) \\
    & \le \frac{1}{2} \Vert \tilde{x}_t - u_1 \Vert \Vert \tilde{x}_t + u_1\Vert \\
    & \le \Vert \tilde{x}_t - u_1 \Vert \wedge \Vert \tilde{x}_t + u_1\Vert
\end{align*}
Thus we obtain 
\begin{align}
     \left\Vert x_t x_t^\top - \sigma_1 u_1 u_1^\top \right\Vert  \le \sigma_1 \left( 3 \left\vert \frac{\Vert x_t \Vert^2}{\sigma_1} - 1 \right\vert + \Vert \tilde{x}_t - u_1 \Vert \wedge \Vert \tilde{x}_t + u_1\Vert  \right)
\end{align}

\medskip     
Next, we have 
    \begin{align*}
        \Vert x_t \pm \sqrt{\sigma_1} u_1 \Vert & \le \left\Vert x_t - \frac{\sqrt{\sigma_1}}{\Vert x_t \Vert} x_t + \frac{\sqrt{\sigma_1}}{\Vert x_t \Vert} x_t   \pm  \sqrt{\sigma_1} u_1 \right \Vert   \\
        & \le \left\Vert \frac{  x_t}{\Vert x_t \Vert }\right\Vert \vert \Vert x_t \Vert - \sqrt{\sigma_1} \vert + \sqrt{\sigma_1} \left\Vert \frac{x_t }{\Vert x_t \Vert} \pm u_1\right\Vert \\
        & \le \sqrt{\sigma_1} \left\vert \frac{\Vert x_t \Vert}{\sqrt{\sigma_1}} - 1 \right\vert + \sqrt{\sigma_1} \left\Vert \frac{x_t }{\Vert x_t \Vert} \pm u_1\right\Vert  \\
        & \le \sqrt{\sigma_1} \left\vert \frac{\Vert x_t \Vert^2}{\sigma_1} - 1 \right\vert + \sqrt{\sigma_1} \left\Vert \frac{x_t }{\Vert x_t \Vert} \pm u_1\right\Vert 
    \end{align*}
\end{proof}






\subsection{Proof of Lemma \ref{lem:attracting region}}\label{sec:proof attracting region}

\begin{proof}[Proof of Lemma \ref{lem:attracting region}] Our proof supposes that $x_1^\top u_1 \neq 0$ which guaranteed almost surely by the random initialization \eqref{eq:init-grad}.
  

\medskip

\underline{\textbf{Claim 1.}} First, we establish the following useful inequalities: for all $t \ge 1$,
\begin{itemize}
    \item [$(i)$] $\Vert x_t \Vert > 0$.
    \item [$(ii)$] $\left((1-\eta) + \eta \frac{\sigma_d}{\Vert x_t \Vert^2}\right) \Vert x_t \Vert  \le \Vert x_{t+1} \Vert \le  \left( (1-\eta) + \eta \frac{\sigma_1}{\Vert x_t \Vert^2}\right) \Vert x_t \Vert$
\end{itemize}
We start by noting that we can project the iterations of the gradient descent updates \eqref{eq:gradient eta} onto $u_i$, for all $i \in [d]$, gives 
\begin{align}\label{eq:proj u}
         u_i^\top x_{t+1} = \left((1-\eta) + \eta \frac{\sigma_i}{\Vert x_t \Vert}\right) u_i^\top x_{t} 
\end{align}
which follows because of the simple fact that $u_i^\top M = \sigma_i u_i^\top$. Thus, squaring and summing the equations \eqref{eq:proj u} over $i \in [d]$, gives 
\begin{align}\label{eq:norm_iter_1}
    \Vert x_{t+1} \Vert^2 = \sum_{i=1}^d \left((1-\eta) + \eta \frac{\sigma_i}{\Vert x_t \Vert^2}\right)^2 \vert u_i^\top x_t \vert^2 
\end{align}
Recalling that $\sigma_1 \ge \cdots \ge \sigma_d > 0$, we immediately deduce from \eqref{eq:norm_iter_1} the inequality $(ii)$. Inequality $(i)$ because if $\Vert x_1 \Vert > 0$, than thanks to $(ii)$ it also follows $\Vert x_t \Vert > 0$.

\medskip 

\underline{\textbf{Claim 2.}} Let us denote for all $i \in [d]$, $t\ge 1$,  $\vert \cos(\theta_{i,t}) \vert  = \vert u_i^\top x_t \vert/ \Vert x_t \vert $. We establish that the following properties hold:
\begin{itemize}
    \item [$(i)$] $(\vert \cos(\theta_{1,t}) \vert)_{t \ge 1}$ is non-decreasing sequence
    \item [$(ii)$] $(\vert \cos(\theta_{d,t}) \vert)_{t \ge 1}$ is a non-increasing sequence
    \item [$(iii)$] for all $t\ge 1$, $\Vert x_{t+1}\Vert \ge \alpha \sqrt{\sigma_1}$.
\end{itemize}
To prove the aforementioned claim we start by noting from \eqref{eq:proj u} that for all $i \in [d]$, we have 
\begin{align}\label{eq:cos eq}
  \Vert x_{t+1} \Vert \vert \cos(\theta_{i, t+1}) \vert = \left((1- \eta) \Vert x_t \Vert + \eta \frac{\sigma_i}{\Vert x_t \Vert} \right)    \vert \cos(\theta_{i, t}) \vert
\end{align}
Thus, from the Claim 1 (Eq. (ii)), we immediately see that for all $t \ge 1$, we have  
\begin{align}
    1 \ge \vert \cos(\theta_{1, t+1}) \vert & \ge \vert \cos(\theta_{1, t}) \vert  \ge \vert \cos(\theta_{1, 1}) \vert  \label{eq:cos increase} \\
    \vert \cos(\theta_{d, t+1}) \vert & \le \vert \cos(\theta_{d, t}) \vert \le 1,
\end{align}
Now, we see that the l.h.s. inequality of Eq. (ii) in Claim 1, the combination of \eqref{eq:cos increase} and \eqref{eq:cos eq} leads to the following:
\begin{align}
    \Vert x_{t+1} \Vert & \ge \max\left\lbrace \left((1- \eta) \Vert x_t \Vert + \eta \frac{\sigma_1}{\Vert x_t \Vert} \right) \vert \cos(\theta_{1,1}) \vert, \left((1- \eta) \Vert x_t \Vert + \eta \frac{\sigma_d}{\Vert x_t \Vert} \right)   \right\rbrace  \\
    & \overset{(a)}{\ge} 2 \sqrt{\eta(1-\eta)}\max\left\lbrace      \vert \cos(\theta_{1,1}) \vert,  \sqrt{\frac{\sigma_d}{\sigma_1}}   \right\rbrace  \sqrt{\sigma_1} \\
    & = \alpha \sqrt{\sigma_1}
\end{align}
where we used in $(a)$ the elementary fact that $2\sqrt{\eta(1-\eta) \sigma } = \inf_{x > 0} (1-\eta) x + \eta(\sigma/x)$ for all $\sigma> 0$. This concludes Claim 2.

\medskip 

\underline{\textbf{Claim 3.}} We establish that if $
    \alpha \sqrt{\sigma_1} \le \Vert x_t \Vert \le \sqrt{\sigma_1}$
then, $ \Vert x_{t+1} \Vert \le  \beta \sqrt{\sigma_1}$. 

\medskip 

We can immediately see from Claim 1 that 
\begin{align}
    \Vert x_{t+1} \Vert & \le (1 - \eta) \Vert x_t \Vert + \eta \frac{\sigma_1}{\Vert x_t \Vert} \\
    & \le \left((1-\eta) + \frac{\eta}{2\sqrt{\eta(1-\eta)} \max(\vert \cos(\theta_{1,1} )\vert, \kappa^{-1/2} )}\right)  \sqrt{\sigma_1} \\
    & \le \left( (1-\eta) + \frac{1}{2}\sqrt{\frac{\eta}{1-\eta}} \min(\vert \cos(\theta_{1,1})\vert^{-1}, \kappa^{1/2} )\right) \sqrt{\sigma_1} \\
    & = \beta \sqrt{\sigma_1}
\end{align}
where we denote $\beta = (1-\eta) + \frac{1}{2}\sqrt{\frac{\eta}{1-\eta}} \min(\vert \cos(\theta_{1,1})\vert^{-1}, \sqrt{\kappa} ) $  and note that  $\beta >  1$. 

\medskip

\underline{\textbf{Claim 4:}} If $ \sqrt{\sigma_1} <  \Vert x_t \Vert  \le \beta \sqrt{\sigma_1}$, then $\Vert x_{t+1} \Vert \le \beta \sqrt{\sigma_1}$.

\medskip 

Indeed, start from the inequality in Claim 1, we can immediately verify that 
\begin{align}
    \Vert x_{t+1} \Vert \le \left( (1 - \eta) + \eta\frac{\sigma_1}{\Vert x_t \Vert^2 }\right) \Vert x_t \Vert \le \Vert x_t \Vert \le \beta \sqrt{\sigma_1}    
\end{align}
Indeed we observe that if $ \sqrt{\sigma_1} <  \Vert x_t \Vert  \le \beta \sqrt{\sigma_1}$, then  $\sqrt{\sigma_1} / \Vert x_t \Vert \le 1 $.



\medskip 


\underline{\textbf{Claim 5:}} If $\Vert x_1 \Vert > \beta \sqrt{\sigma_1}$, then there exists $k^\star \ge 1$, such that $\Vert x_{1+k^\star}\Vert \le \beta \sqrt{\sigma_1}$, satisfying 
\begin{align}
    k^\star \le \frac{\beta^2}{\eta (\beta^2 - 1)} \log\left( \frac{\Vert x_t \Vert }{\beta \sqrt{\sigma_1}}\right)
\end{align}
Assuming that $\Vert x_t \Vert > \beta \sqrt{\sigma_1}$, let us define 
$
k^\star = \min \lbrace k \ge 1: \Vert x_{t+k}\Vert \le \beta \sqrt{\sigma_1}   \rbrace
$, the number of iterations required for $\Vert x_{t+k} \Vert \le \beta \sqrt{\sigma_1}$. Using the r.h.s. of the inequality in Claim 1, and by definition of $k^\star$, we know that for $1 \le k < k^\star$, 
\begin{align}
    \Vert x_{t+k-1} \Vert \le \left((1-\eta) + \eta \frac{\sigma_1}{\Vert x_{t+k} \Vert^2}\right) \Vert z_{t+k}\Vert \le  \left((1-\eta) + \eta \beta^{-2}\right) \Vert z_{t+k}\Vert. 
\end{align}
Iterating the above inequality, we obtain that  
\begin{align*}
    \Vert x_{t+k^\star} \Vert \le  \left((1-\eta) + \eta \beta^{-2}\right)^{k^\star } \Vert x_t \Vert. 
\end{align*}
Now, let us note that $k^\star$ can not be too large. More specifically, we remark that: 
\begin{align*}
    k^\star \ge  \frac{\eta (\beta^2 - 1)}{(1- \eta) \beta^2  + \eta }  \log\left(\frac{\Vert x_t \Vert}{\beta \sqrt{\sigma_1}}\right) \ge \frac{\log\left(\frac{\Vert x_t \Vert}{\beta \sigma_1}\right)}{\log\left( \frac{1}{(1-\eta) + \eta \beta^{-2}}  \right)} \implies \left((1-\eta) + \eta \beta^{-2}\right)^{k^\star } \Vert x_t \Vert  \le \beta \sqrt{\sigma_1} 
\end{align*}
Therefore, it must hold that 
\begin{align*}
    k^\star \le  \frac{\eta (\beta^2 - 1)}{(1- \eta) \beta^2  + \eta }  \log\left(\frac{\Vert x_t \Vert}{\beta\sqrt{\sigma_1}}\right)
\end{align*}
Note that from Claim 3 and 4, as soon as $ \alpha \sqrt{\sigma_1} \le \Vert x_t \Vert \le \beta \sqrt{\sigma_1} $, then it will never escape this region. Therefore, we only need to use Claim 5 if the first iterate $\Vert x_1 \Vert$ is outside the region $[\alpha \sqrt{\sigma_1} , \beta \sqrt{\sigma_1} ]$.
\end{proof}

% For each $i \in [d]$,  let us denote the angle between $u_i$ and $x_t$ by $\theta_{i,t}$. More specifically, we have: 
% \begin{align}
%     \cos(\theta_{i,t}) = \frac{u_i^\top x_t }{\Vert x_t \Vert}
% \end{align}
% In the following Lemma we establish that $\cos(\theta_{i,t})$ converges almost surely to $1$ for $i = 1$ and to $0$ for $i \neq 1$.

\subsection{Proof of Lemma \ref{lem:saddle-avoidance}}\label{sec:proof saddle}


\begin{proof}[Proof of Lemma \ref{lem:saddle-avoidance}]
    We prove the saddle-avoidance property of gradient descent. The proof is an immediate application of \ref{lem:attracting region}. Indeed, we have 
    \begin{align*}
        \Vert \nabla g(x_t; M) \Vert & = \Vert \nabla Mx_t - \Vert x_t \Vert^2 x_t  \Vert \\
        & = \left\Vert   \sum_{i=1}^d (\sigma_i - \Vert x_t\Vert^2)  (u_i^\top x_t) u_i \right\Vert \\
        & = \left(\sum_{i=1}^d (\sigma_i - \Vert x_t\Vert^2)^2  (u_i^\top x_t)^2 \right)^{1/2} \\
        & \ge \vert \sigma_1 - \Vert x_t\Vert^2\vert   \vert u_1^\top x_t\vert  \\
        & \ge \vert \sqrt{\sigma_1} - \Vert x_t\Vert\vert      
         \vert \sqrt{\sigma_1} + \Vert x_t\Vert\vert  \Vert x_t  \Vert \vert \cos(\theta_{1,t})\vert \\
        & \ge   \vert \vert \sqrt{\sigma_1} - \sqrt{\sigma_i}\vert - \delta \vert    \sqrt{\sigma_1} \alpha \sqrt{\sigma_1} \vert \cos(\theta_{1,1})\vert  \\
        & \le \alpha \sigma_1 \vert \cos(\theta_{1,1})\vert \left\vert \vert \sqrt{\sigma_1} - \sqrt{\sigma_i}\vert - \delta \right\vert 
    \end{align*}
    where we used Lemma \ref{lem:attracting region} to bound $\Vert x_t \Vert \ge \alpha \sqrt{\sigma_1}$ and $\vert\cos(\theta_{i,t})\vert \ge \vert\cos(\theta_{i,1})\vert$, and reverse triangular inequality
\end{proof}
 




\subsection{Proof of Lemma \ref{lem:conv singular vector}}\label{sec:proof conv singular vector}


\begin{proof}[Proof of Lemma \ref{lem:conv singular vector}]
     We start from the observation that for all $i \in [d]$, we have 
    \begin{align}\label{eq: key equation}
        \Vert x_{t+1} \Vert  \vert \cos(\theta_{i, t+1}) \vert = \left( (1-\eta) \Vert x_t \Vert + \eta \frac{\sigma_i}{\Vert x_t \Vert }\right) \vert \cos(\theta_{i, t}) \vert.
    \end{align}
     Now, note dividing the equations corresponding to $1$ and $i$, we get 
    \begin{align}\label{eq: key equation}
     \frac{\vert \cos(\theta_{1, t+1}) \vert}{\vert \cos(\theta_{i, t+1}) \vert} & = \frac{\left( (1-\eta) \Vert x_t \Vert + \eta \frac{\sigma_1}{\Vert x_t \Vert }\right)}{\left( (1-\eta) \Vert x_t \Vert + \eta \frac{\sigma_i}{\Vert x_t \Vert }\right)} \frac{\vert \cos(\theta_{1, t}) \vert}{\vert \cos(\theta_{i, t}) \vert} \\ 
     & = \left(1+ \frac{\left( \eta \frac{\sigma_1 - \sigma_i}{\Vert x_t \Vert }\right)}{\left( (1-\eta) \Vert x_t \Vert + \eta \frac{\sigma_i}{\Vert x_t \Vert }\right)}  \right)\frac{\vert \cos(\theta_{1, t}) \vert}{\vert \cos(\theta_{i, t}) \vert} \\
     & = \underbrace{\left(1 + \frac{ \eta (\sigma_1 - \sigma_i) }{ (1-\eta) \Vert x_t \Vert^2 + \eta \sigma_i}  \right)}_{ > 1 \text{ because } \sigma_1 - \sigma_i > 0}\frac{\vert \cos(\theta_{1, t}) \vert}{\vert \cos(\theta_{i, t}) \vert}
    \end{align}
    Thus, we have for all $t\ge 1:$
    \begin{align*}
         \frac{\vert \cos(\theta_{i, t}) \vert}{\vert \cos(\theta_{i, t+1}) \vert} = \left(1 + \frac{ \eta (\sigma_1 - \sigma_i) }{ (1-\eta) \Vert x_t \Vert^2 + \eta \sigma_i}  \right)\frac{\vert \cos(\theta_{1, t}) \vert}{\vert \cos(\theta_{1, t+1}) \vert},
    \end{align*}
    which leads to 
     \begin{align*}
        \frac{\vert \cos(\theta_{i, 1}) \vert}{\vert \cos(\theta_{i, t+1}) \vert} = \left(\prod_{s = 1}^t \left(1 + \frac{ \eta (\sigma_1 - \sigma_i) }{ (1-\eta) \Vert x_t \Vert^2 + \eta \sigma_i}  \right) \right)\frac{\vert \cos(\theta_{1, 1}) \vert}{\vert \cos(\theta_{1, t+1}) \vert}.
    \end{align*}
     We recall that for all $t \ge t_0$,
    \begin{align*}
         \vert \cos(\theta_{1, t}) \vert \le \vert \cos(\theta_{1, t+1}) \vert \le 1  \quad \text{and} \qquad 
         \alpha \sqrt{\sigma_1} \le \Vert x_t \Vert \le  \beta  \sqrt{\sigma_1}
    \end{align*} 
    which implies that 
    \begin{align*}
        \frac{\vert \cos(\theta_{1, 1}) \vert}{\vert \cos(\theta_{1, t+1}) \vert} \ge \vert \cos(\theta_{1, 1}) \vert \\
        \left(1 + \frac{ \eta (\sigma_1 - \sigma_i) }{ (1-\eta) \Vert x_t \Vert^2 + \eta \sigma_i}  \right) & \ge 1 + \frac{\eta (\sigma_1 - \sigma_i)}{(1-\eta) \beta^2 \sigma_1   + \eta \sigma_i }
    \end{align*}
    Hence, 
    \begin{align*}
        \vert \cos(\theta_{i, t+1}) \vert \le \left( 1 + \frac{\eta (\sigma_1 - \sigma_i)}{(1-\eta) \beta^2 \sigma_1   + \eta \sigma_i} \right)^{-t} \frac{\vert \cos(\theta_{i, 1}) \vert}{\vert \cos(\theta_{1, 1}) \vert} \underset{t \to \infty}{\longrightarrow} 0. 
    \end{align*}
    Either $u_1^\top x_1 > 0$ or $u_1^\top x_1 <0$. Without loss of generality, we will present the case when $u_1^\top x_1 > 0$, in which case $\cos(\theta_{1,1}) > 0$ and consequently $\cos(\theta_{1, t}) >0$.  Let us further note that
    \begin{align*}
        \left\Vert \frac{1}{\Vert x_t \Vert } x_t  - u_1 \right\Vert^2 & = 2 (1 - \cos(\theta_{1,t})) \\
       & =  2 \frac{1-\cos^2(\theta_{1,t})}{1 + \cos(\theta_{1,t})} \\
       & \le 2 \sum_{i=2}^d \cos^2(\theta_{i,t}) \\
       & \le 2 \sum_{i=2}^d  \left( 1 + \frac{\eta (\sigma_1 - \sigma_i)}{(1-\eta) \beta^2 \sigma_1   + \eta \sigma_i} \right)^{-2t} \frac{\vert \cos(\theta_{i, 1}) \vert^2}{\vert \cos(\theta_{1, 1}) \vert^2} \\
       & \le 2 \left( 1 + \frac{\eta (\sigma_1 - \sigma_2)}{(1-\eta) \beta^2 \sigma_1   + \eta \sigma_2} \right)^{-2t} \frac{1 - \vert \cos(\theta_{1, 1}) \vert^2}{\vert \cos(\theta_{1, 1}) \vert^2} \\ 
       & \le 2 \left( 1 + \frac{\eta (\sigma_1 - \sigma_2)}{(1-\eta) \beta^2 \sigma_1   + \eta \sigma_2} \right)^{-2t} \vert \tan(\theta_{1, 1}) \vert^2
    \end{align*}
\end{proof}


\subsection{Proof of Lemma \ref{lem:conv singular value}}\label{sec:proof conv singular value}

\begin{proof}[Proof of Lemma \ref{lem:conv singular value}]
    Let us denote for all $t \ge 1$
    \begin{align}
        \rho_t & = 1 - \frac{\vert \cos(\theta_{1, t})\vert} {\vert \cos(\theta_{1, t+1})\vert} \\
        \epsilon_t & = \frac{\Vert x_t \Vert }{\sqrt{\sigma_1}} - 1 
    \end{align}
    First, we verify that $\rho_t \underset{t \to \infty}{\longrightarrow} 0$. Indeed, we have 
    \begin{align*}
        \rho_t & = 1 - \frac{\vert \cos(\theta_{1,t} ) \vert }{\vert \cos(\theta_{1,t+1} ) \vert} \\
        & \le \frac{ \left\vert \cos(\theta_{1,t+1} )  -  1   \right\vert +    \left\vert  1 - \cos(\theta_{1,t})
   \right\vert}{\vert \cos(\theta_{1,1})\vert} \\
        & \le  \frac{1}{\vert \cos(\theta_{1,1})\vert}   \max\left(  \left\Vert \frac{1}{\Vert x_{t+1} \Vert }x_{t+1} - u_1\right\Vert^2, \left\Vert \frac{1}{\Vert x_t \Vert }x_t - u_1\right\Vert^2\right)\\
        & \le \frac{2 \vert \tan(\theta_{1,1})\vert }{\vert \cos(\theta_{1,1})\vert } \left( 1 + \frac{\sigma_1 - \sigma_2 }{\beta^2 \sigma_1 + \sigma_2 }\right)^{-2t}
    \end{align*}
    where we used the fact that $(\cos(\theta_{1,t}))_{t\ge 1}$ is a non-decreasing sequence, and  used Lemma \ref{lem:conv singular vector} to obtain the final bound. Thus, we see that $\rho_t \underset{t \to \infty}{\longrightarrow} 0$.
    
    \medskip 
    Next, we also recall by assumption that $\alpha \sqrt{\sigma_1} \le \Vert x_1 \Vert \le \beta \sqrt{\sigma_1}$, and by Lemma \ref{lem:attracting region} that $ \alpha \sqrt{\sigma_1} \le \Vert x_t \Vert \le \beta \sqrt{\sigma_1}$ for all $t \ge 1$. Thus, we have: 
    \begin{align}
        -1 < \alpha - 1 \le \epsilon_t \le \beta - 1
    \end{align}


    \medskip 
    Now, let us show that $\epsilon_t \underset{t \to \infty}{\longrightarrow} 0$. We recall that  
    \begin{align}
        \Vert x_{t+1} \Vert \vert \cos(\theta_{1, t+1})\vert = \left((1-\eta)\Vert x_t \Vert + \eta \frac{\sigma_1}{\Vert x_t \Vert} \right) \vert \cos(\theta_{1, t})\vert.
    \end{align}
    Thus, we have 
    \begin{align}
        \epsilon_{t+1} & = \left( (1- \eta) (\epsilon_t + 1) + \frac{\eta}{\epsilon_t + 1}\right) (1 - \rho_t) - 1  \\
        & =  \left( (1- \eta) (\epsilon_t + 1) + \frac{\eta}{\epsilon_t + 1}\right) - 1 - \rho_t \left( (1- \eta) (\epsilon_t + 1) + \frac{\eta}{\epsilon_t + 1}\right) \\
        & = \frac{(1-\eta)\epsilon_t^2 + (1-2\eta)\epsilon_t }{(\epsilon_t + 1)} - \rho_t \left( (1- \eta) (\epsilon_t + 1) + \frac{\eta}{\epsilon_t + 1}\right) \\
        & = \frac{\epsilon_t^2}{2 (\epsilon_t + 1)} - \frac{\rho_t}{2} \left( (\epsilon_t + 1) + \frac{1}{\epsilon_t + 1}\right) 
    \end{align}
    where in the last equality we used  $\eta = 1/2$. We conclude from the above that for all $t \ge 1$
    \begin{align*}
        \varepsilon_{t+1} & \ge \max\left(- \frac{\rho_t}{2}\left(\beta + \frac{1}{\alpha}\right), \alpha - 1 \right) = - \min\left(\frac{\rho_t}{2}\left(\beta + \frac{1}{\alpha}\right), 1 - \alpha \right) \\
        \varepsilon_{t+1} & \le \frac{\epsilon_t^2}{2(\epsilon_t + 1)} - \rho_t 
    \end{align*}
    Thus, for all $t \ge 2$, we have 
    \begin{align*}
        \vert \varepsilon_{t+1} \vert  & \le \frac{\epsilon_t^2}{2(\epsilon_t + 1)} \indicator\lbrace \epsilon_t > 0\rbrace + \frac{\rho_t^2}{8\alpha} \left(\beta + \frac{1}{\alpha}\right)^2 + \frac{\rho_t}{2}\left(\beta + \frac{1}{\alpha}\right) \\
        & \le \frac{\min(\vert \epsilon_t \vert, \vert \epsilon_t \vert^2)}{2} + \frac{\rho_t^2}{8\alpha} \left(\beta + \frac{1}{\alpha}\right)^2 + \frac{\rho_t}{2}\left(\beta + \frac{1}{\alpha}\right)
    \end{align*}
    which further gives 
    \begin{align*}
        \vert \varepsilon_{t+1} \vert & \le \sum_{k = 0}^{t-2} \frac{1}{2^k} \left(  \frac{\rho_{t-k}^2}{8\alpha} \left(\beta + \frac{1}{\alpha}\right)^2 + \frac{\rho_{t-k}}{2}\left(\beta + \frac{1}{\alpha}\right)  \right) \\
        & \le \left(\sum_{k = 0}^{t-2} \frac{1}{2^k} \left( 1 + \frac{\sigma_1 - \sigma_2 }{\beta^2 \sigma_1 + \sigma_2 }\right)^{-2t + 2k} \right) \left(  \frac{\vert \tan(\theta_{1,1})\vert^2}{4\alpha \vert \cos(\theta_{1,1})\vert^2} \left(\beta + \frac{1}{\alpha}\right)^2 + \frac{\vert \tan(\theta_{1,1})\vert}{\vert \cos(\theta_{1,1})\vert}\left(\beta + \frac{1}{\alpha}\right) \right)  
    \end{align*}
    To conclude, we will use the following elementary fact that for $\gamma \in (0,1)$, we have 
    \begin{align*}
        \sum_{k=0}^{t-2}\frac{\gamma^{t-k}}{2^k} & = \indicator\lbrace \gamma = 1/2\rbrace (t-1)\gamma^t + \indicator\lbrace \gamma \neq 1/2\rbrace  \gamma^t \frac{1 - \frac{1}{(2\gamma)^{t-1}}}{1- \frac{1}{2\gamma}} \\
        & \le \indicator\lbrace \gamma = 1/2\rbrace (t-1)\gamma^t + \indicator\lbrace \gamma \neq 1/2\rbrace  \frac{2 \gamma^2}{\vert 2\gamma - 1\vert} \left\vert \gamma^{t-1}  - \frac{1}{2^{t-1}}\right\vert \\
        & \le \indicator\lbrace \gamma = 1/2\rbrace \frac{(t-1)}{2^t} + \indicator\lbrace \gamma \neq 1/2\rbrace \gamma^2 (t-1) \left( \frac{1}{2} \vee \gamma \right)^{t-2} \\
        & \le (t-1)\left( \gamma \vee \frac{1}{2}\right)^{t}
    \end{align*}
  
    where in our case we have 
    \begin{align*}
        \gamma = \left( 1 + \frac{\sigma_1 - \sigma_2 }{\beta^2 \sigma_1 + \sigma_2 }\right)^{-2} = \left(\frac{\beta^2\sigma_1 + \sigma_2}{(\beta^2 + 1) \sigma_1} \right)^2
    \end{align*}
    Indeed, we obtain that 
    \begin{align*}
         \vert \epsilon_{t+1} \vert \le  \left(t-1 \right) \left( \left(1 + \frac{\sigma_1 - \sigma_2}{\beta^2 \sigma_1 + \sigma_2}\right) \vee \sqrt{2}\right)^{-2t}   \left(  \frac{\vert \tan(\theta_{1,1})\vert^2}{4\alpha \vert \cos(\theta_{1,1})\vert^2} \left(\beta + \frac{1}{\alpha}\right)^2 + \frac{\vert \tan(\theta_{1,1})\vert}{\vert \cos(\theta_{1,1})\vert}\left(\beta + \frac{1}{\alpha}\right) \right) 
    \end{align*}
\end{proof}



\subsection{Proof of Theorem \ref{thm:bigtheorem}}

\begin{proof}
    We recall that $\tilde{M}_{\ell+1} = M - \sum_{i=1}^\ell \hat{\sigma}_{i} \hat{u}_i \hat{u}_i^\top$ and denote $M_{\ell+1} = M - \sum_{i=1}^\ell \sigma_{i} u_i u_i^\top$ with $M_1 = \tilde{M}_1 = M$. We will also denote the leading eigenvalue of $\tilde{M}_\ell$ by $\tilde{\sigma}_\ell$ and its corresponding eigenvector by $\tilde{u}_\ell$. Note that the leading singular value of $M_\ell$ is $\sigma_\ell$ and its corresponding vector is $u_\ell$. We also note that  
    \begin{align}
        \left\Vert \tilde{M}_{\ell+1} - \left(M - \sum_{i=1}^\ell \sigma_i u_i u_i^\top\right) \right\Vert & \le  \sum_{i=1}^\ell 
        \left\Vert \hat{\sigma}_i \hat{u}_i \hat{u}_i^\top - \sigma_i u_i u_i^\top  \right\Vert   \\
        &  \le \sum_{i=1}^\ell 
        \left\Vert \hat{\sigma}_i \hat{u}_i \hat{u}_i^\top - \tilde{\sigma}_i \tilde{u}_i \tilde{u}_i^\top  \right\Vert  + \left\Vert \tilde{\sigma}_i \tilde{u}_i \tilde{u}_i^\top - \sigma_i u_i u_i^\top  \right\Vert   \label{eq:perturbed M}\\
    \end{align}
    By application of gradient descent \eqref{eq:gradient}, we know that if method is run for $t$ large enough then for all for each $i \in [k]$, we have the guarantee:
    \begin{align}
        \vert \tilde{\sigma}_i - \hat{\sigma}_i \vert & \le \epsilon  \\
        \Vert \hat{u}_i + \tilde{u}_1 \Vert \wedge \Vert \hat{u}_i - \tilde{u}_i \Vert & \le \epsilon \\
        \Vert \tilde{\sigma}_i \tilde{u}_i \tilde{u}_1^\top -  \hat{\sigma}_i  \hat{u}_i \hat{u}_i^\top \Vert & \le \epsilon
    \end{align}    
    
    Now, we show by induction that things remain well behaved. (for $\ell = 1$), we have  
    \begin{align*}
        \vert \tilde{\sigma}_1 - \sigma_1 \vert & = 0  \\
        \vert u_1  - \tilde{u}_1 \vert & = 0 \\
        \vert \sigma_1  u_1 u_1^\top - \sigma_1 \tilde{u}_1 \tilde{u}_1 & = 0
    \end{align*}
    (for $\ell = 2$) We have 
    \begin{align}
        \vert \tilde{\sigma}_2 - \sigma_2 \vert & \le  \epsilon   \\
        \Vert \hat{u}_2 + \tilde{u}_2 \Vert \wedge \Vert \hat{u}_2 - \tilde{u}_2 \Vert & \le \frac{\sqrt{2} \epsilon}{\sigma_2 - \sigma_3} & (\text{Lemma \ref{lem:spectral-perturbation}})  \\
        \Vert \tilde{\sigma}_2 \tilde{u}_2 \tilde{u}_1^\top -  \hat{\sigma}_2  \hat{u}_2 \hat{u}_2^\top \Vert  & \le  3\epsilon + \frac{\sigma_2\sqrt{2} \epsilon }{\sigma_2 - \sigma_3} & (\text{Lemma \ref{lem:error-decomposition}})
    \end{align}
    (for $\ell = 3$) we have 
    \begin{align}
        \vert \tilde{\sigma}_3 - \sigma_3 \vert & \le  \epsilon + \left(\epsilon + 3\epsilon + \frac{\sigma_2\sqrt{2}\epsilon}{\sigma_2 - \sigma_3}\right) = \left( 5  + \frac{\sigma_2\sqrt{2}}{\sigma_2 - \sigma_3}\right) \epsilon \\
        \Vert \hat{u}_3 + \tilde{u}_3 \Vert \wedge \Vert \hat{u}_3 - \tilde{u}_3 \Vert & \le \frac{\sqrt{2} }{\sigma_3 - \sigma_4}  \left(5  + \frac{\sigma_2\sqrt{2}}{\sigma_2 - \sigma_3}\right) \epsilon  & (\text{Lemma \ref{lem:spectral-perturbation}})  \\
        \Vert \tilde{\sigma}_3 \tilde{u}_3 \tilde{u}_3^\top -  \hat{\sigma}_3  \hat{u}_3 \hat{u}_3^\top \Vert  & \le  \left(3 \left( 5  + \frac{\sigma_2\sqrt{2}}{\sigma_2 - \sigma_3}\right) + \frac{\sqrt{2} \sigma_3 }{\sigma_3 - \sigma_4}  \left(5  + \frac{\sigma_2\sqrt{2}}{\sigma_2 - \sigma_3}\right) \right) \epsilon & (\text{Lemma \ref{lem:error-decomposition}})
    \end{align}
    and so forth, we see that at the end the error is at most 
    \begin{align}
    \vert \tilde{\sigma}_\ell - \sigma_\ell \vert & \le  k C_3^k  \left(\max_{i \in [k]}\left(\frac{\sigma_i}{\sigma_i - \sigma_{i+1}}\right)\vee 1 \right)^{k-1}   \epsilon 
          \\ 
          \Vert \hat{u}_\ell + \tilde{u}_\ell \Vert \wedge \Vert \hat{u}_\ell - \tilde{u}_\ell \Vert & \le k C^k \frac{1}{\sigma_\ell} \left(\max_{i \in [k]}\left(\frac{\sigma_i}{\sigma_i - \sigma_{i+1}}\right)\vee 1 \right)^{k-1}   \epsilon  \\ 
          \Vert \tilde{\sigma}_\ell \tilde{u}_\ell \tilde{u}_\ell^\top -  \hat{\sigma}_\ell \hat{u}_\ell \hat{u}_\ell^\top \Vert & \le k C^k  \left(\max_{i \in [k]}\left(\frac{\sigma_i}{\sigma_i - \sigma_{i+1}}\right)\vee 1 \right)^{k-1}   \epsilon 
    \end{align}
    Let us denote 
    \begin{align} \label{eq:epsilon condition}
         k C^k  \left(\max_{i \in [k]}\left(\frac{\sigma_i}{\sigma_i - \sigma_{i+1}}\right)\vee 1 \right)^{k-1}   \epsilon  = \frac{\epsilon'}{2}. 
    \end{align}
    If  $\epsilon'$ is such that $\sigma_i - \sigma_{i+1} >2k \epsilon'$, for $i \in[d]$, and $\sigma_n \ge 2k\epsilon'$, then it will be ensured that the singular values of $\tilde{M}_\ell$ remain close to those of $M_\ell$ and $\tilde{M}_\ell$  stays positive definite.  We may then apply gradient descent and use Theorem \ref{thm:convergence-1}. Now, for a given $\epsilon'$, and a precision $\epsilon$ satisfying \eqref{eq:epsilon condition}, by Theorem \ref{thm:convergence-1}, running gradient descent long enough, namely $t_\circ$, ensures that:
    \begin{align*}
        \vert \tilde{\sigma}_i - \sigma_i \vert & \le \epsilon'  \\
        \sigma_i (\Vert u_i  + \hat{u}_i  \Vert \wedge  \Vert u_i  - \hat{u}_i \Vert) & \le  \epsilon' \\
        \Vert \sigma_i  u_i u_1^\top - \hat{\sigma}_i \hat{u}_i \hat{u}_i \Vert  & \le \epsilon' \\
        \Vert \sqrt{\sigma_i}  u_i  - \sqrt{\hat{\sigma}_i} \hat{u}_i \hat{u}_i \Vert  \wedge  \Vert \sqrt{\sigma_i}  u_i  + \sqrt{\hat{\sigma}_i} \hat{u}_i \hat{u}_i \Vert  & \le \epsilon'
    \end{align*}
    More precisely, $t_\circ$ is given by 
    \begin{align*}
        t_\circ \ge C_1  \max_{i\in[k]}\left( \frac{\sigma_i }{\sigma_i - \sigma_{i+1}} \vee 1 \right)   \left( k \log\left( C_2 \max_{i\in[k]}\left( \frac{\sigma_i}{\sigma_i - \sigma_i} \vee 1\right)\right) + \log\left( \frac{k}{\epsilon'}\right) \right)
    \end{align*}
    In total, we need the total number of iterations to be 
    \begin{align*}
        t \ge C_1  k \max_{i\in[k]}\left( \frac{\sigma_i }{\sigma_i - \sigma_{i+1}} \vee 1 \right)   \left( k \log\left( C_2 \max_{i\in[k]}\left( \frac{\sigma_i}{\sigma_i - \sigma_i} \vee 1\right)\right) + \log\left( \frac{k}{\epsilon'}\right) \right)
    \end{align*} 
\end{proof}


\subsection{Properties of the nonconvex objective $g$}

In the following lemma, we present a characterization of the properties of critical points of $g$:   

\begin{lemma}[1$^{\textrm{st}}$ and 2$^{\textrm{nd}}$ order optimality conditions]\label{lem:opt-conditions}
     Let $x \in \RR^{n}$, the following properties hold:
\begin{itemize}
    \item [(i)] $x$ is a critical point, i.e. $\nabla g(x;M) = 0$, if and only if $x = 0$ or $x = \pm \sqrt{\sigma_i} u_i$, $i\in [k]$.
    \item [(ii)] if for all $i \neq 1$, $\sigma_1 > \sigma_i$, then all local minima of the loss function $g$ are also global minima\footnote{We recall that $x$ is a local minima of $g$ iff $\nabla g(x) = 0$ and $\lambda_{\min}(\nabla^2 g(x)) \ge 0$.}, and all other critical points are strict saddle\footnote{We recall that $x$ is a strict saddle point iff $\nabla g(x) = 0$, and  $\lambda_{\min}(\nabla^2 g(x)) < 0$.}. More specifically, the only local and global minima of $g$ are the points $ - \sqrt{\sigma_1} u_1$ and $+\sqrt{\sigma_1} u_1$.
\end{itemize}
\end{lemma}
A proof of Lemma \ref{lem:opt-conditions} can be found for instance in \cite{chi2019nonconvex}. 



