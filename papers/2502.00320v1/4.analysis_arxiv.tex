\section{Convergence analysis: gradient descent as Heron's method in disguise}\label{sec:proof}

In this section, we present the key ingredients for proving Theorem \ref{thm:convergence-1}. The proof is simple and can be broken into three intermediate steps: \emph{(i)}  establishing that gradient descent has a natural region of attraction; \emph{(ii)} showing that once within this region of attraction $(x_t)_{t \ge 1}$ aligns with $u_1$ at a linear rate; \emph{(iii)}  showing that the sequence $(\Vert x_t \Vert)_{t \ge 1}$ is evolving according to an approximate Heron's method, and is convergent to $\sqrt{\sigma_1}$ at a linear rate.


Most of our analysis holds for gradient descent with an adaptive step sizes $\eta / \Vert x_t \Vert^2$ and $\eta \in (0,1)$:  
\begin{align}\label{eq:gradient eta}
    x_{t+1} = x_t -  \frac{\eta}{\Vert x_t \Vert^2} \nabla g(x_t; M),
\end{align}
Without loss of generality, we consider that $x_1$ is randomly initialized as in \eqref{eq:init-grad}. This choice of initialization allows us to simplify the analysis as it ensures that $x_1$ is in the image of $M$. Most importantly, our analysis only requires that $\PP( x_1^\top u_1 \neq 0) = 1$. It will be also convenient to define: for all $t \ge 1$, $i \in [d]$, the angle $\theta_{i,t}$ between $u_i$ and $x_t$, which also satisfies:  
\begin{align}
    \cos(\theta_{i,t}) = \frac{x_t ^\top u_i}{\Vert x_t \Vert}
\end{align}
whenever $\Vert x_t \Vert > 0$. 

\subsection{Region of attraction}

It is crucial for gradient descent to stabilize in order to have to achieve. One hopes that the iterates $x_t$ do not escape to infinity or vanish at $0$. It turns out that this is indeed the case and this is what we present in Lemma \ref{lem:attracting region}. We define: 
\begin{align}
    \alpha & =   2\sqrt{\eta(1 - \eta)} \left(\vert \cos(\theta_{1,1}) \vert \vee  \frac{1}{\sqrt{\kappa}}\right),  \\
    \beta  & =  (1-\eta) + \frac{1}{2}\sqrt{\frac{\eta}{1-\eta}} \left(\frac{1}{\vert \cos(\theta_{1,1}) \vert } \wedge  \sqrt{\kappa} \right),
\end{align}
where it is not difficult to verify that $\alpha < 1$ and $\beta > 1$. 
\begin{lemma}[Attracting region of gradient descent]\label{lem:attracting region}
Gradient descent as described in \eqref{eq:gradient eta} with the random initialization \eqref{eq:init-grad} ensures that:
\begin{align}
    \forall t > 1, \qquad   \alpha \sqrt{\sigma_1}  \le \, &    \Vert x_{t} \Vert  \phantom{\le \beta \, \sqrt{\sigma_1}} \\
   \forall t > \tau, \, \qquad   \phantom{\alpha  \sqrt{\sigma_1} \le}  & \Vert x_{t} \Vert \le \beta \, \sqrt{\sigma_1}    
\end{align}
where $\tau$ is given by:
\begin{align}
    \tau & =   \frac{\eta (\beta^2 - 1)}{(1- \eta) \beta^2  + \eta }   \log\left(\frac{\Vert x_1 \Vert}{\beta \sqrt{\sigma_1}}\right). 
\end{align}
Moreover, the sequence $(\vert \cos(\theta_{1,t})\vert )_{t\ge 1}$ is non-decreasing.
\end{lemma}
The proof of Lemma \ref{lem:attracting region} is differed to Appendix \ref{sec:proof attracting region}. Remarkably, $\alpha$ and $\beta$ do not arbitrarily degrade with the quality of the initial random point. Thus, the number of iterations required to enter the region $[\alpha \sqrt{\sigma_1}$, $\beta \sqrt{\sigma_1}]$ can be made constant if for instance one further constrains $\Vert x_1 \Vert = 1$. 

Furthermore, the fact that $\vert \cos(\theta_{1,t})\vert$ is non-decreasing is interesting because it means that $\Vert x_t \Vert^{-1} x_t$ can only get closer to $\lbrace -u_1 , u_1 \rbrace$. To see that, note that we have $\Vert \Vert x_t \Vert^{-1} x_t \pm u_1 \Vert^2 = 2 \pm 2 \cos(\theta_{1, t}) \ge 2 (1 - \vert \cos(\theta_{1, t})\vert )$.  Indeed, a consequence \ref{lem:attracting region}, is the following saddle-point avoidance result. 

\begin{lemma}[Saddle-point avoidance]\label{lem:saddle-avoidance}
    Let $\delta > 0$. Assume that $\sigma_1 > \sigma_2 \ge  0$ and that $\vert \Vert x_t \Vert - \sqrt{\sigma_i} \vert <  \delta $ for $i \neq 1$ and some $t >  1$. Then, gradient descent as described in \eqref{eq:gradient eta} with the random initialization \eqref{eq:init-grad} ensures that
    \begin{align}
        \Vert \nabla g(x_t; M)\Vert \ge   \alpha  \sigma_1 \vert \cos(\theta_{1,1})\vert \left\vert  \vert \sqrt{\sigma_1} - \sqrt{\sigma_i}\vert - \delta \right\vert 
    \end{align}
\end{lemma}

We provide a proof of Lemma \ref{lem:saddle-avoidance} in Appendix \ref{sec:proof saddle}. We remark from the result that if $\delta \ll \sigma_1 - \sigma_i$, then the gradient cannot vanish. The lower bound depends on the initial point $x_1$ through $\vert \cos(\theta_{1,1})\vert$ which can be very small. In general with the random initialization \eqref{eq:init-grad}, small values of  $\vert \cos(\theta_{1,1})\vert$ are highly improbable.  



  
\subsection{Alignment at linear rate}

Another key ingredient in the analysis is Lemma \ref{lem:conv singular vector} which we present below:

\begin{lemma}\label{lem:conv singular vector}
    Assume that $\sigma_1 > \sigma_2 \ge 0$ and let $\tau$ be defined as in Lemma \ref{lem:attracting region}. Gradient descent as described in \eqref{eq:gradient eta} with random initialization \eqref{eq:init-grad} ensures that: $t > \tau$, $i \neq 1$, 
    \begin{align*}
        \left\Vert \frac{x_t}{\Vert x_t \Vert}  -   u_1  \right\Vert \wedge \left\Vert \frac{x_t }{\Vert x_t \Vert} + u_1  \right\Vert & \le 
        \frac{\sqrt{2}}{(1 + \Delta_2)^{t-\tau}}  \left\vert \tan(\theta_{1, 1})\right\vert, \\
         \vert \cos(\theta_{i, t+1}) \vert  & \le \frac{1}{(1 + \Delta_i)^{t-\tau}}  \frac{\vert \cos(\theta_{i, 1}) \vert}{\vert \cos(\theta_{1, 1}) \vert}, 
    \end{align*}
    with 
    $$
    \Delta_i = \frac{\eta (\sigma_1 - \sigma_i)}{(1-\eta) \beta^2 \sigma_1   + \eta \sigma_i}. 
    $$ 
\end{lemma}
We defer the proof of Lemma \ref{lem:conv singular vector} to Appendix \ref{sec:proof conv singular vector}. The result shows that gradient descent is implicitly biased towards aligning its iterates with $u_1$. This alignment happens at a linear rate with a factor that depends on $\sigma_1 - \sigma_2$, and this is why we require the condition $\sigma_1 - \sigma_2 > 0$. Actually, if $\sigma_1 = \sigma_2 > \sigma_3$, then our proof can be easily adjusted to show that $x_t$ will align with the singular subspace spanned by $u_1, u_2$. Thus, our assumptions are without loss of generality.   


\subsection{Convergence with approximate Heron's iterations}

The final step of our analysis is to show that $\left\vert \Vert x_{t}\Vert / \sqrt{\sigma_1} - 1\right\vert$ vanishes at a linear rate. We will only show this when $\eta=1/2$ (running gradient descent \cite{eq:gradient}) because it has the cleanest proof. This is presented in the following lemma:
\begin{lemma}\label{lem:conv singular value} 
 Assume that $\sigma_1 > \sigma_2 \ge 0$ and let $\tau$ be defined as in Lemma \ref{lem:attracting region}. Gradient descent as described in \eqref{eq:gradient} with random initialization \eqref{eq:init-grad} ensure that: for all $t > \tau$, 
\begin{align}
    \left\vert \frac{\Vert x_{t+1} \Vert}{\sqrt{\sigma_1}} - 1 \right\vert \le \frac{  (t - \tau) C}{\left((1 + \Delta_2)\vee \sqrt{2}\right)^{2(t - \tau)}} \left(  \frac{\vert \tan(\theta_{1,1})\vert^2}{4\alpha \vert \cos(\theta_{1,1})\vert^2} \left(\beta + \frac{1}{\alpha}\right)^2 + \frac{\vert \tan(\theta_{1,1})\vert}{\vert \cos(\theta_{1,1})\vert}\left(\beta + \frac{1}{\alpha}\right) \right)
\end{align}
with 
% $
%     C = \left(  \frac{\vert \tan(\theta_{1,1})\vert^2}{4\alpha \vert \cos(\theta_{1,1})\vert^2} \left(\beta + \frac{1}{\alpha}\right)^2 + \frac{\vert \tan(\theta_{1,1})\vert}{\vert \cos(\theta_{1,1})\vert}\left(\beta + \frac{1}{\alpha}\right) \right), 
% $ and 
$$
\Delta_2 = \frac{\sigma_1 - \sigma_2}{\beta^2 \sigma_1 + \sigma_2}.
$$  



\end{lemma}
 We provide simply a proof sketch to highlight how the the behavior of the iterates $\Vert x_t \Vert$ resemble those of an approximate Heron's method. We refer the reader to  Appendix \ref{sec:proof conv singular value} for the complete proof.

\begin{proof}[Proof sketch of Lemma \ref{lem:conv singular value}]
     By taking the scalar product of the the two sides of the gradient update equation \eqref{eq:gradient} with $u_1$, we deduces that   
\begin{align}\label{eq:1}
    \Vert x_{t+1} \Vert = \frac{1}{2}\left(\Vert x_t \Vert + \frac{\sigma_1}{\Vert x_t \Vert} \right) \frac{\vert \cos(\theta_{1, t})\vert}{ \vert \cos(\theta_{1, t+1})\vert }.
\end{align}
Next, we can leverage Lemma \ref{lem:conv singular vector} to show that the ratio $ \vert \cos(\theta_{1, t})\vert /  \vert \cos(\theta_{1, t+1})\vert  $ converges to $1$ at a linear rate. We then recognize the familiar form of Heron's iterations \ref{eq:heron's iterates}. Starting with this form, we can show converge of $\Vert x_t \Vert$ to $\sqrt{\sigma_1}$. We spare the reader the tedious details of this part and refer them to the proof.
\end{proof}

Interestingly, the proof analysis thus far did not rely on any of the classical derivations for showing convergence of gradient methods which typically require special conditions on the objective.  






\subsection{Putting everything together}

Below, we present a result which is an immediate consequence of of Lemma \ref{lem:attracting region}, Lemma \ref{lem:conv singular vector}, and \ref{lem:conv singular value}. 
\begin{theorem}\label{thm:convergence-2}
Given a symmetric, positive semi-definite $M \in \RR^{n\times n}$ with $\sigma_1 > \sigma_2 \geq 0$, the
iterate in \eqref{eq:gradient} satisfies for all $t \ge 1$,  
\begin{align}
\Vert x_{t+\tau} + \sqrt{\sigma_1} u_1 \Vert \wedge \Vert x_{t+\tau} - \sqrt{\sigma_1} u_1 \Vert & \le c_1 \sqrt{\sigma_1}\left(\left(1 + \frac{ (\sigma_1 - \sigma_2)}{ \beta^2 \sigma_1   +  \sigma_2} \right) \vee \sqrt{2} \right)^{-t}, \\ 
\Vert x_{t+\tau}x_{t+\tau}^\top   - \sigma_1 u_1 u_1^\top \Vert & \le  c_1 \sigma_1\left(\left(1 + \frac{ (\sigma_1 - \sigma_2)}{ \beta^2 \sigma_1   +  \sigma_2} \right) \vee \sqrt{2} \right)^{-t},
\end{align}
as long as long 
\begin{align}
\tau  & \ge c_2 \left(\frac{\sigma_1 + \sigma_2}{\sigma_1 - \sigma_2} \vee 1\right) + \log\left(\frac{c_3}{(\sigma_1- \sigma_2)}\right) 
\end{align}
with  positive constants $c_1, c_2, c_3$ that depend only on $x_1$; with random initialization $c_1, c_2, c_3$ are strictly positive almost surely.
\end{theorem}
For the proof, we refer the reader to Appendix \ref{sec:proof thm1}.  Theorem \ref{thm:convergence-1} is a consequence of Theorem \ref{thm:convergence-2}.








