\section{Discussion \& Conclusion}\label{sec:discussion}

In this work, we presented a method for $k$-SVD with gradient descent. The proposed  gradient descent method uses an appealingly  simple adaptive step-size towards optimizing a non-convex matrix factorization objective. We showed that the gradient method enjoys global linear convergence guarantee with random initialization in spite of the convexity of the optimization landscape. The method is shown to converge in the \emph{under parameterized} setting, a result which to the best of our knowledge has eluded prior work. Critical to our convergence analysis is the observation that the gradient method behaves like Heron's method. We believe this observation offers an interesting insight that might be of independent interest in the study of nonconvex optimization landscapes. Finally, we carried experimental results to corroborate our findings. 

Because of modern computing infrastructure that allow for efficient and optimized deployment of gradient methods at scale, our method is likely to find use for computing $k$-SVD for massively large matrices. Therefore a natural extension of our work is to develop a parallelized version of the gradient method and test it on massively large matrices.     

Furthermore, gradient methods are known to possess robustness properties, thus we foresee that our proposed method will find widespread applications and further development, for settings where data is incomplete or noisy.  In addition to this, a natural question is to investigate whether acceleration schemes may enhance the performance of the method, notably by reducing number of iterations required to achieve $\epsilon$ accuracy from $\log(1/\epsilon)/ (\sigma_1 - \sigma_2)$ to $\log(1/\epsilon)/ \sqrt{(\sigma_1 - \sigma_2)}$. Finally, another exciting direction is extending our method to solve tensor decompositions which are  known to be notoriously expansive.
      


% 1. Summarize our findings 


% 2. Open questions

% 1.1. Acceleration schemes for gradient methods for nonconvex matrix factorization. Lancsos methods and variants depend on $1/\sqrt{\sigma_1 - \sigma_2}$ while have a dependence that is of order $1/(\sigma_1 - \sigma_2)$ which is comparable to Power method. Can gradient based methods also attain similar performances to Lancsos methos? 

% 1.2. Extensions to tensor settings, seems natural. 

% 1.3. Settings for matrix sensing, matrix completion for exact recovery, phase retrieval, where only incomplete knowledge is available. Towards, fully gradient based with spectral initialization methods?

% Nuclear norm penalization achieves exact recovery but it is computationally more demanding than SVD for instance. 

% 1.4. Does our analysis extend to other nonconvex optimization problems beyond nonconvex matrix factorization?

% 3. Discuss some limitations and future work

% 3.1 Parallelized implementation at scale. Testing on large scale problems 