
@inproceedings{tu2016low,
  title={Low-rank solutions of linear matrix equations via procrustes flow},
  author={Tu, Stephen and Boczar, Ross and Simchowitz, Max and Soltanolkotabi, Mahdi and Recht, Ben},
  booktitle={International Conference on Machine Learning},
  pages={964--973},
  year={2016},
  organization={PMLR}
}

@article{burer2003nonlinear,
  title={A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization},
  author={Burer, Samuel and Monteiro, Renato DC},
  journal={Mathematical programming},
  volume={95},
  number={2},
  pages={329--357},
  year={2003},
  publisher={Springer}
}

@article{chi2019nonconvex,
  title={Nonconvex optimization meets low-rank matrix factorization: An overview},
  author={Chi, Yuejie and Lu, Yue M and Chen, Yuxin},
  journal={IEEE Transactions on Signal Processing},
  volume={67},
  number={20},
  pages={5239--5269},
  year={2019},
  publisher={IEEE}
}

@article{zhang2016riemannian,
  title={Riemannian SVRG: Fast stochastic optimization on Riemannian manifolds},
  author={Zhang, Hongyi and J Reddi, Sashank and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@article{chatterjee2015matrix,
  title={Matrix estimation by universal singular value thresholding},
  author={Chatterjee, Sourav},
  year={2015}
}


@article{struski2024efficient,
  title={Efficient GPU implementation of randomized SVD and its applications},
  author={Struski, {\L}ukasz and Morkisz, Pawe{\l} and Spurek, Przemys{\l}aw and Bernabeu, Samuel Rodriguez and Trzci{\'n}ski, Tomasz},
  journal={Expert Systems with Applications},
  volume={248},
  pages={123462},
  year={2024},
  publisher={Elsevier}
}

@article{hardt2014noisy,
  title={The noisy power method: A meta algorithm with applications},
  author={Hardt, Moritz and Price, Eric},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{du2018algorithmic,
  title={Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced},
  author={Du, Simon S and Hu, Wei and Lee, Jason D},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{jin2017escape,
  title={How to escape saddle points efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  booktitle={International conference on machine learning},
  pages={1724--1732},
  year={2017},
  organization={PMLR}
}

@article{demmel1990accurate,
  title={Accurate singular values of bidiagonal matrices},
  author={Demmel, James and Kahan, William},
  journal={SIAM Journal on Scientific and Statistical Computing},
  volume={11},
  number={5},
  pages={873--912},
  year={1990},
  publisher={SIAM}
}

@article{cape2019two,
  title={The two-to-infinity norm and singular subspace geometry with applications to high-dimensional statistics},
  author={Cape, Joshua and Tang, Minh and Priebe, Carey E},
  year={2019}
}


@article{tong2021accelerating,
  title={Accelerating ill-conditioned low-rank matrix estimation via scaled gradient descent},
  author={Tong, Tian and Ma, Cong and Chi, Yuejie},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={150},
  pages={1--63},
  year={2021}
}


@article{hazan2016introduction,
  title={Introduction to online convex optimization},
  author={Hazan, Elad and others},
  journal={Foundations and Trends{\textregistered} in Optimization},
  volume={2},
  number={3-4},
  pages={157--325},
  year={2016},
  publisher={Now Publishers, Inc.}
}

@article{baldi1989neural,
  title={Neural networks and principal component analysis: Learning from examples without local minima},
  author={Baldi, Pierre and Hornik, Kurt},
  journal={Neural networks},
  volume={2},
  number={1},
  pages={53--58},
  year={1989},
  publisher={Elsevier}
}

@article{hardt2018gradient,
  title={Gradient descent learns linear dynamical systems},
  author={Hardt, Moritz and Ma, Tengyu and Recht, Benjamin},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={29},
  pages={1--44},
  year={2018}
}


@inproceedings{zinkevich2003online,
  title={Online convex programming and generalized infinitesimal gradient ascent},
  author={Zinkevich, Martin},
  booktitle={Proceedings of the 20th international conference on machine learning (icml-03)},
  pages={928--936},
  year={2003}
}

@article{oja1985stochastic,
  title={On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix},
  author={Oja, Erkki and Karhunen, Juha},
  journal={Journal of mathematical analysis and applications},
  volume={106},
  number={1},
  pages={69--84},
  year={1985},
  publisher={Elsevier}
}


@article{mises1929praktische,
  title={Praktische Verfahren der Gleichungsaufl{\"o}sung.},
  author={Mises, RV and Pollaczek-Geiringer, Hilda},
  journal={ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift f{\"u}r Angewandte Mathematik und Mechanik},
  volume={9},
  number={1},
  pages={58--77},
  year={1929},
  publisher={Wiley Online Library}
}

@book{cullum2002lanczos,
  title={Lanczos algorithms for large symmetric eigenvalue computations: Vol. I: Theory},
  author={Cullum, Jane K and Willoughby, Ralph A},
  year={2002},
  publisher={SIAM}
}


@article{lanczos1950iteration,
  title={An iteration method for the solution of the eigenvalue problem of linear differential and integral operators},
  author={Lanczos, Cornelius},
  year={1950},
  publisher={United States Governm. Press Office Los Angeles, CA}
}

@article{francis1961qr,
  title={The QR transformation a unitary analogue to the LR transformationâ€”Part 1},
  author={Francis, John GF},
  journal={The Computer Journal},
  volume={4},
  number={3},
  pages={265--271},
  year={1961},
  publisher={Oxford University Press}
}

@article{chen2021spectral,
  title={Spectral methods for data science: A statistical perspective},
  author={Chen, Yuxin and Chi, Yuejie and Fan, Jianqing and Ma, Cong and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={14},
  number={5},
  pages={566--806},
  year={2021},
  publisher={Now Publishers, Inc.}
}



@article{chen2019gradient,
  title={Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval},
  author={Chen, Yuxin and Chi, Yuejie and Fan, Jianqing and Ma, Cong},
  journal={Mathematical Programming},
  volume={176},
  pages={5--37},
  year={2019},
  publisher={Springer}
}

@inproceedings{jain2017global,
  title={Global convergence of non-convex gradient descent for computing matrix squareroot},
  author={Jain, Prateek and Jin, Chi and Kakade, Sham and Netrapalli, Praneeth},
  booktitle={Artificial Intelligence and Statistics},
  pages={479--488},
  year={2017},
  organization={PMLR}
}

@article{stewart1993early,
  title={On the early history of the singular value decomposition},
  author={Stewart, Gilbert W},
  journal={SIAM review},
  volume={35},
  number={4},
  pages={551--566},
  year={1993},
  publisher={SIAM}
}


@article{zhang2023preconditioned,
  title={Preconditioned Gradient Descent for Overparameterized Nonconvex Burer--Monteiro Factorization with Global Optimality Certification},
  author={Zhang, Gavin and Fattahi, Salar and Zhang, Richard Y},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={163},
  pages={1--55},
  year={2023}
}

@inproceedings{xu2023power,
  title={The power of preconditioning in overparameterized low-rank matrix sensing},
  author={Xu, Xingyu and Shen, Yandi and Chi, Yuejie and Ma, Cong},
  booktitle={International Conference on Machine Learning},
  pages={38611--38654},
  year={2023},
  organization={PMLR}
}

@article{stoger2021small,
  title={Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction},
  author={St{\"o}ger, Dominik and Soltanolkotabi, Mahdi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23831--23843},
  year={2021}
}

@article{hou2020fast,
  title={Fast global convergence for low-rank matrix recovery via Riemannian gradient descent with random initialization},
  author={Hou, Thomas Y and Li, Zhenzhen and Zhang, Ziyun},
  journal={arXiv preprint arXiv:2012.15467},
  year={2020}
}

@article{jia2024preconditioning,
  title={Preconditioning matters: Fast global convergence of non-convex matrix factorization via scaled gradient descent},
  author={Jia, Xixi and Wang, Hailin and Peng, Jiangjun and Feng, Xiangchu and Meng, Deyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{li2024crucial,
  title={On the Crucial Role of Initialization for Matrix Factorization},
  author={Li, Bingcong and Zhang, Liang and Mokhtari, Aryan and He, Niao},
  journal={arXiv preprint arXiv:2410.18965},
  year={2024}
}


@article{candes2015phase,
  title={Phase retrieval via Wirtinger flow: Theory and algorithms},
  author={Candes, Emmanuel J and Li, Xiaodong and Soltanolkotabi, Mahdi},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={4},
  pages={1985--2007},
  year={2015},
  publisher={IEEE}
}


@inproceedings{balzano2010online,
  title={Online identification and tracking of subspaces from highly incomplete information},
  author={Balzano, Laura and Nowak, Robert and Recht, Benjamin},
  booktitle={2010 48th Annual allerton conference on communication, control, and computing (Allerton)},
  pages={704--711},
  year={2010},
  organization={IEEE}
}


@article{garber2015fast,
  title={Fast and simple PCA via convex optimization},
  author={Garber, Dan and Hazan, Elad},
  journal={arXiv preprint arXiv:1509.05647},
  year={2015}
}

@inproceedings{shamir2015stochastic,
  title={A stochastic PCA and SVD algorithm with an exponential convergence rate},
  author={Shamir, Ohad},
  booktitle={International conference on machine learning},
  pages={144--152},
  year={2015},
  organization={PMLR}
}

@inproceedings{arora2012stochastic,
  title={Stochastic optimization for PCA and PLS},
  author={Arora, Raman and Cotter, Andrew and Livescu, Karen and Srebro, Nathan},
  booktitle={2012 50th annual allerton conference on communication, control, and computing (allerton)},
  pages={861--868},
  year={2012},
  organization={IEEE}
}

@inproceedings{garber2016faster,
  title={Faster eigenvector computation via shift-and-invert preconditioning},
  author={Garber, Dan and Hazan, Elad and Jin, Chi and Musco, Cameron and Netrapalli, Praneeth and Sidford, Aaron and others},
  booktitle={International Conference on Machine Learning},
  pages={2626--2634},
  year={2016},
  organization={PMLR}
}


@article{chen2019model,
  title={Model-free nonconvex matrix completion: Local minima analysis and applications in memory-efficient kernel PCA},
  author={Chen, Ji and Li, Xiaodong},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={142},
  pages={1--39},
  year={2019}
}


@inproceedings{de2015global,
  title={Global convergence of stochastic gradient descent for some non-convex matrix problems},
  author={De Sa, Christopher and Re, Christopher and Olukotun, Kunle},
  booktitle={International conference on machine learning},
  pages={2332--2341},
  year={2015},
  organization={PMLR}
}

@article{lee2016gradient,
  title={Gradient descent converges to minimizers},
  author={Lee, Jason D and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
  journal={arXiv preprint arXiv:1602.04915},
  year={2016}
}

@article{chen2020noisy,
  title={Noisy matrix completion: Understanding statistical guarantees for convex relaxation via nonconvex optimization},
  author={Chen, Yuxin and Chi, Yuejie and Fan, Jianqing and Ma, Cong and Yan, Yuling},
  journal={SIAM journal on optimization},
  volume={30},
  number={4},
  pages={3098--3121},
  year={2020},
  publisher={SIAM}
}

@inproceedings{ge2015escaping,
  title={Escaping from saddle pointsâ€”online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on learning theory},
  pages={797--842},
  year={2015},
  organization={PMLR}
}


@inproceedings{li2021communication,
  title={Communication-efficient distributed SVD via local power iterations},
  author={Li, Xiang and Wang, Shusen and Chen, Kun and Zhang, Zhihua},
  booktitle={International Conference on Machine Learning},
  pages={6504--6514},
  year={2021},
  organization={PMLR}
}

@article{ge2016matrix,
  title={Matrix completion has no spurious local minimum},
  author={Ge, Rong and Lee, Jason D and Ma, Tengyu},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{morwani2024new,
  title={A New Perspective on Shampoo's Preconditioner},
  author={Morwani, Depen and Shapira, Itai and Vyas, Nikhil and Malach, Eran and Kakade, Sham and Janson, Lucas},
  journal={arXiv preprint arXiv:2406.17748},
  year={2024}
}

@inproceedings{ge2017no,
  title={No spurious local minima in nonconvex low rank problems: A unified geometric analysis},
  author={Ge, Rong and Jin, Chi and Zheng, Yi},
  booktitle={International Conference on Machine Learning},
  pages={1233--1242},
  year={2017},
  organization={PMLR}
}


@article{bandeira2016sharp,
  title={Sharp nonasymptotic bounds on the norm of random matrices with independent entries},
  author={Bandeira, Afonso S and Van Handel, Ramon},
  year={2016}
}



@article{stojanovic2024spectral,
  title={Spectral entry-wise matrix estimation for low-rank reinforcement learning},
  author={Stojanovic, Stefan and Jedra, Yassir and Proutiere, Alexandre},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}{2024}


@article{mann2023exploiting,
  title={Exploiting Observation Bias to Improve Matrix Completion},
  author={Mann, Sean and Park, Charlotte and Shah, Devavrat},
  journal={arXiv preprint arXiv:2306.04775},
  year={2023}
}


