\section{Warm-up analysis; gradient descent as Heron's method in disguise}

A key insight from our analysis is the observation that gradient descent with adaptive step-size \eqref{eq:gradient} behaves like Heron's method. This is in stark contrast, with the observation of \cite{stoger2021small} suggesting that gradient descent for low-rank matrix factorization with a fixed step-size and small random initialization behaves like a power-method at an initial phase. To clarify our observation, we present here a convergence analysis in the simple yet instructive case of $M$ being exactly of rank $1$. 

First, let us note that gradient of the function $g$ at any given point $x \in \RR^{n}$ is given by: 
\begin{align}
    \nabla g (x;M) = M x - \Vert x \Vert^2 x 
\end{align}
When $M$ is exactly of rank $1$, i.e., $M = \sigma_1 u_1 u_1^\top$, the gradient updates \eqref{eq:gradient} become: for all $t \ge 1$,
\begin{align}\label{eq:grad-iterations-ada-rank1}
    x_{t+1} = \frac{1}{2} \left(x_t +  \frac{ \sigma_1  u_1^\top x_t }{\Vert x_t \Vert^2} u_1\right)
\end{align}
To further simplify things, let us consider that the initial point $x_1$ is randomly selected as follows: 
\begin{align}\label{eq:init-grad}
    x_1 = M x \qquad \text{and} \qquad x \sim \mathcal{N}(0, I_n). 
\end{align}
Thus, we see that $x_1 = \sigma_1 (u_1^\top x) u_1$. This, together with the iterations \eqref{eq:grad-iterations-ada-rank1}, clearly shows that for all $t \ge 1$,  $x_t = \Vert x_t \Vert u_1$. Hence, for all $t \ge 1$, we have:  
\begin{align}\label{eq:heron's iterates}
    \Vert x_{t+1} \Vert u_1 = \frac{1}{2}\left( \Vert x_t \Vert +  \frac{\sigma_1}{\Vert x_t \Vert} \right) u_1.
\end{align}
We see then that $\Vert x_t \Vert$ is evolving precisely according to Heron's method (a.k.a. the Babylonian method) for finding the square root of $\sigma_1$, which is a special case of Newton's method. Below, we present the convergence rate of the method in this case: 
\begin{proposition}\label{prop:grad convergence rank 1}
    When $M = \sigma_1 u_1 u_1^\top$. Gradient descent as described in \eqref{eq:gradient} with an initial random point as in \eqref{eq:init-grad} is guaranteed to converge almost surely:
    \begin{align}
        \Vert x_t \pm \sqrt{\sigma_1} u_1 \Vert \underset{t \to \infty}{\longrightarrow} 0 \qquad {a.s.}
    \end{align}
    More precisely, denoting $\epsilon_t = \frac{\Vert x_t \Vert}{\sqrt{\sigma_1}} - 1$, we have for all $t\ge 2$, $
         0 < \epsilon_{t+1} \le \frac{1}{2}\min( \epsilon_t^2, \epsilon_t).$
\end{proposition}

We defer the proof of Proposition of \ref{prop:grad convergence rank 1} is immediate and corresponds exactly to the convergence analysis of Heron's method. We provide a proof in  Appendix \ref{ref:appA} for completeness. The established convergence guarantee indicates that gradient descent converges at a quadratic rate, meaning, that in order to attain an error accuracy of order $\varepsilon$, the method only requires $O(\log\log(1/\varepsilon))$ iterations.

It is worth mentioning that when the rank $M$ is exactly 1 then the objective $g$ corresponds to that of an \emph{exact-parameterization} regime. The random initialization scheme \eqref{eq:init-grad} we consider is only meant for ease of exposition and has been studied in the concurrent work of \cite{li2024crucial}. Like us, they also obtain quadratic convergence in this \emph{exact-parameterization} regime. Indeed, if one uses an alternative random initialization, say  $x_1 \sim \mathcal{N}(0, I_n)$, then one only obtains a linear convergence rate. 

In general, we do not expect the matrix $M$ to be exactly of rank $1$. Extending the analysis to the generic setting is more challenging and is exactly the subject of \textsection\ref{sec:proof}.


