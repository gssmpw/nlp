\section{Related Work}
\subsection{Early CNN Architectures}
LeNet-5, proposed by LeCun et al., was one of the first successful CNNs, designed specifically for handwritten digit recognition. It introduced key components like convolutional layers, pooling layers, and fully connected layers, which form the foundation of modern CNNs.
\subsection{Deeper Networks}
With advancements in computational resources, deeper architectures like AlexNet, VGG, and ResNet have emerged. AlexNet showcased the importance of using ReLU activations and GPU acceleration, while VGG demonstrated the utility of stacking small convolutional kernels. ResNet introduced residual connections, which alleviate the vanishing gradient problem in deep networks.

As images can be regarded as special grid graphs, Graph Neural Networks (GNNs), a robust framework for learning from graph structures, have gained numerous attention. The Graph Attention Network (GAT) \cite{petar17gat}, a foundational model in GNNs, utilizes attention mechanisms for weighted information aggregation, whereas GraphSAGE \cite{ham17graphsage} offers a generalized framework for graph convolution. Concurrently, Fuzzy Rough Sets (FRS) are recognized for their ability to model imprecise and vague information \cite{xing2022weighted, gao2022parameterized}. The integration of FRS with GNNs to enable robust graph convolution has sparked considerable research interest. In \cite{xing24enhancing}, the Fuzzy Graph Attention Network (FGAT) was introduced, combining FRS with GAT for the first time to capture fuzzy relationships between neighbors. This development has inspired numerous follow-up studies. To address spatio-temporal information, FGATT \cite{xing24fgatt}, which integrates FGAT with Transformers, was designed. Additionally, in \cite{xing24mfgat}, the Multi-view Fuzzy Graph Attention Network (MFGAT) was proposed to capture fuzzy dependencies from multiple perspectives.
\subsection{CIFAR-10 Specific Research}
Many works on CIFAR-10 focus on lightweight models (e.g., MobileNet) or efficient training strategies. However, the potential of optimized deeper CNN architectures, tailored for small-scale datasets, remains underexplored. Our work bridges this gap by leveraging architectural enhancements to achieve improved performance on CIFAR-10.