\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Enhanced Convolutional Neural Networks for Improved Image Classification\\
% \thanks{\textcolor{red}{Identify applicable funding agency here. If none, delete this.}}
}

\author{\IEEEauthorblockN{Xiaoran Yang}
\IEEEauthorblockA{\textit{Dept Computer Science} \\
\textit{Communication University of China}\\
Raleigh, United States\\
im.yxr.cs@gmail.com}
\and
\IEEEauthorblockN{Shuhan Yu}
\IEEEauthorblockA{\textit{Hainan International College} \\
\textit{Communication University of China}\\
Hainan, China \\
yushuhan1018@gmail.com}
\and
\IEEEauthorblockN{Wenxi Xu}
\IEEEauthorblockA{\textit{School of Economics} \\
\textit{Hefei University of Technology}\\
Hefei, China \\
xuwenxi040929@163.com}
}

\maketitle

\begin{abstract}
Image classification is a fundamental task in computer vision with diverse applications, ranging from autonomous systems to medical imaging. The CIFAR-10 dataset is a widely used benchmark to evaluate the performance of classification models on small-scale, multi-class datasets. Convolutional Neural Networks (CNNs) have demonstrated state-of-the-art results; however, they often suffer from overfitting and suboptimal feature representation when applied to challenging datasets like CIFAR-10. In this paper, we propose an enhanced CNN architecture that integrates deeper convolutional blocks, batch normalization, and dropout regularization to achieve superior performance. The proposed model achieves a test accuracy of 84.95\%, outperforming baseline CNN architectures. Through detailed ablation studies, we demonstrate the effectiveness of the enhancements and analyze the hierarchical feature representations. This work highlights the potential of refined CNN architectures for tackling small-scale image classification problems effectively.

\end{abstract}

\begin{IEEEkeywords}
Speech Emotion Recognition, LSTM, Deep Learning, PyTorch
\end{IEEEkeywords}

\section{Introduction}

Image classification is a core task in computer vision and machine learning, where the goal is to assign one or more labels to an image based on its content. It serves as the foundation for numerous real-world applications, such as facial recognition, autonomous driving, medical diagnosis, and smart manufacturing \cite{qc1, qc2}. The CIFAR-10 dataset\cite{b1}, introduced by Krizhevsky and Hinton\cite{b1}, has become a standard benchmark in this field. It consists of 60,000 color images divided into 10 distinct classes, each representing a unique object category, such as airplanes, automobiles, birds, and cats. The dataset is balanced, with an equal number of samples for each class, making it an ideal testbed for evaluating classification algorithms. The relatively small size of the dataset allows for efficient experimentation while retaining sufficient complexity to challenge advanced models.

Convolutional Neural Networks (CNNs) have been the backbone of most state-of-the-art models in image classification. CNNs excel in extracting hierarchical spatial features from images, enabling them to capture low-level edges and textures in earlier layers and high-level semantic features in deeper layers. This hierarchical feature extraction capability, combined with their translational invariance, makes CNNs particularly well-suited for visual tasks. However, achieving high accuracy on the CIFAR-10 dataset presents several challenges. First, the dataset’s small image resolution of 32x32x3 limits the amount of information and detail that can be extracted, thereby constraining the model's capacity to distinguish between similar classes. Second, the limited size of the dataset increases the risk of overfitting, especially for deeper networks with a large number of parameters. This necessitates the use of effective regularization techniques and data augmentation strategies. Finally, there is a need for a robust network design that strikes a balance between depth, parameter efficiency, and regularization to achieve both high accuracy and generalization.

This paper aims to address these challenges by proposing an enhanced CNN architecture specifically designed for CIFAR-10 image classification. As suggested in \cite{xing24enhancing}, by integrating deeper convolutional blocks, batch normalization to stabilize training, and dropout to mitigate overfitting, the proposed model effectively extracts rich hierarchical features while maintaining robustness. Experimental results demonstrate that this architecture achieves superior performance, highlighting its potential for tackling small-scale yet complex image classification tasks.


In this paper, we address these challenges by proposing an enhanced CNN architecture with deeper convolutional layers, batch normalization for stable training, and dropout regularization to mitigate overfitting. Our model surpasses standard CNN baselines in accuracy, highlighting the importance of architectural refinement in deep learning applications.

\section{Related Work}
\subsection{Early CNN Architectures}
LeNet-5, proposed by LeCun et al., was one of the first successful CNNs, designed specifically for handwritten digit recognition. It introduced key components like convolutional layers, pooling layers, and fully connected layers, which form the foundation of modern CNNs.
\subsection{Deeper Networks}
With advancements in computational resources, deeper architectures like AlexNet, VGG, and ResNet have emerged. AlexNet showcased the importance of using ReLU activations and GPU acceleration, while VGG demonstrated the utility of stacking small convolutional kernels. ResNet introduced residual connections, which alleviate the vanishing gradient problem in deep networks.

As images can be regarded as special grid graphs, Graph Neural Networks (GNNs), a robust framework for learning from graph structures, have gained numerous attention. The Graph Attention Network (GAT) \cite{petar17gat}, a foundational model in GNNs, utilizes attention mechanisms for weighted information aggregation, whereas GraphSAGE \cite{ham17graphsage} offers a generalized framework for graph convolution. Concurrently, Fuzzy Rough Sets (FRS) are recognized for their ability to model imprecise and vague information \cite{xing2022weighted, gao2022parameterized}. The integration of FRS with GNNs to enable robust graph convolution has sparked considerable research interest. In \cite{xing24enhancing}, the Fuzzy Graph Attention Network (FGAT) was introduced, combining FRS with GAT for the first time to capture fuzzy relationships between neighbors. This development has inspired numerous follow-up studies. To address spatio-temporal information, FGATT \cite{xing24fgatt}, which integrates FGAT with Transformers, was designed. Additionally, in \cite{xing24mfgat}, the Multi-view Fuzzy Graph Attention Network (MFGAT) was proposed to capture fuzzy dependencies from multiple perspectives.
\subsection{CIFAR-10 Specific Research}
Many works on CIFAR-10 focus on lightweight models (e.g., MobileNet) or efficient training strategies. However, the potential of optimized deeper CNN architectures, tailored for small-scale datasets, remains underexplored. Our work bridges this gap by leveraging architectural enhancements to achieve improved performance on CIFAR-10.

\section{Methodology}

\subsection{Data Preprocessing}
Data preprocessing is crucial for improving the generalization and robustness of deep learning models. Properly preparing the dataset ensures that the model can effectively learn meaningful patterns while mitigating issues such as overfitting and poor convergence. For the CIFAR-10 dataset, a series of preprocessing steps were applied to enhance the diversity and quality of the training data and to optimize the training process.

\textbf{Normalization} was applied to the pixel values of all images to scale them to the range $[-1, 1]$. This involved subtracting the mean and dividing by the standard deviation for each color channel (red, green, and blue). This step ensures that the input data has a zero mean and unit variance, which is essential for stabilizing and accelerating the convergence of gradient-based optimization methods. Additionally, normalization helps balance the influence of each color channel, preventing one from dominating the others due to differences in magnitude.

\textbf{Data Augmentation} techniques were employed to artificially increase the diversity of the training dataset. This step is particularly important for CIFAR-10 due to its limited dataset size, as it helps to reduce overfitting and improve generalization. Random horizontal flips were applied during training, introducing variations in the orientation of objects within the images. Each image had a 50\% chance of being flipped, ensuring a balanced augmentation process. In addition, random cropping with a padding of 4 pixels was used to vary the positioning of the objects within the images. This method involves first padding the image with additional pixels and then randomly selecting a 32x32 region to use for training, which effectively simulates different viewpoints and reduces sensitivity to spatial positioning.

\textbf{Batching} was implemented to organize the training data into mini-batches of size 64. Mini-batching is a key step in efficient gradient computation, as it allows for parallel processing of multiple samples, reducing memory usage while maintaining stable gradient updates. Batching also helps smooth the noise in gradient estimates, enabling more reliable convergence during training.

These preprocessing techniques collectively expose the model to a diverse range of input variations. By presenting different transformations of the same image, the model becomes more robust to variations in the real-world data distribution. This improved generalization capability allows the trained model to perform well on unseen test samples, ensuring its effectiveness in practical applications.


\subsection{Model Architecture}

Our enhanced CNN model is constructed with a carefully designed architecture aimed at effectively extracting hierarchical features from images while mitigating issues such as overfitting. The model consists of three convolutional blocks, each progressively deepening the feature representation by capturing increasingly complex patterns and relationships within the data.

Each convolutional block contains two convolutional layers. These layers are equipped with 3x3 kernels, a widely adopted kernel size in CNNs due to its ability to balance computational efficiency and feature extraction capability. The use of ReLU (Rectified Linear Unit) activation functions after each convolution ensures that non-linear transformations are applied to the extracted features, enabling the network to model complex relationships in the data effectively. To further enhance the performance and stability of the training process, batch normalization is applied after each convolutional layer. This operation normalizes the feature maps by adjusting and scaling the activations, which not only accelerates convergence but also helps to prevent the internal covariate shift problem.

As stated in \cite{xing24pooling}, Max, Mean, and Weighted Sum are three common pooling mechanisms, where Mean pooling excels in general scenarios by providing stable and robust embeddings, Max pooling emphasizes salient features but may overfit to extremes, and Weighted Sum pooling offers flexibility but requires careful optimization. In the paper, the Max pooling layer is selected. This layer is responsible for reducing the spatial dimensions of the feature maps, thereby lowering the computational requirements for subsequent layers while preserving the most critical information. By selecting the maximum value within non-overlapping regions, max-pooling enhances translational invariance, allowing the model to focus on the presence of features rather than their exact locations. The pooling operation is designed with a stride of 2 and a kernel size of 2x2, effectively halving the spatial dimensions of the input feature maps.

To address the risk of overfitting, dropout regularization is applied after each convolutional block. Dropout functions by randomly deactivating a fraction of neurons during training, forcing the network to learn redundant representations and improving its generalization capabilities. In our architecture, a dropout rate of 25\% is used within the convolutional blocks, striking a balance between regularization and retaining sufficient feature information for downstream layers.

Overall, the architecture of the proposed model ensures a robust extraction of hierarchical features, with each convolutional block contributing to increasingly abstract and complex representations. These features are then passed to fully connected layers for classification, where the model predicts the most probable class for each input image.The architecture of the proposed model as show in Table\ref{architecture}


\begin{table}[h]
\caption{Proposed CNN Architecture}
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Layer} & \textbf{Output Size} & \textbf{Kernel} & \textbf{Filters} & \textbf{Activation} \\ \hline
Conv1 + ReLU & 32x32 & 3x3 & 64 & ReLU \\ \hline
Conv2 + ReLU & 32x32 & 3x3 & 64 & ReLU \\ \hline
MaxPool1 + Dropout & 16x16 & 2x2 & - & - \\ \hline
Conv3 + ReLU & 16x16 & 3x3 & 128 & ReLU \\ \hline
Conv4 + ReLU & 16x16 & 3x3 & 128 & ReLU \\ \hline
MaxPool2 + Dropout & 8x8 & 2x2 & - & - \\ \hline
Conv5 + ReLU & 8x8 & 3x3 & 256 & ReLU \\ \hline
Conv6 + ReLU & 8x8 & 3x3 & 256 & ReLU \\ \hline
MaxPool3 + Dropout & 4x4 & 2x2 & - & - \\ \hline
Fully Connected 1 & 512 & - & - & ReLU \\ \hline
Fully Connected 2 & 10 & - & - & Softmax \\ \hline
\end{tabular}
\label{architecture}
\end{table}


\subsection{Training Setup}
The CIFAR-10 dataset was preprocessed as described, and the model was trained with the Adam optimizer (learning rate = 0.001) for 10 epochs. A cross-entropy loss function was used, and the training data was shuffled at every epoch.



\section{Experimental Results}

\subsection{Quantitative Results}

The proposed model achieved an accuracy of \textbf{84.95\%} on the CIFAR-10 test set, demonstrating its effectiveness in handling the challenges of small-scale image classification. This result surpasses the performance of baseline models, including LeNet-5, which historically achieves approximately 72\% accuracy on the CIFAR-10 dataset, and standard CNN architectures that achieve around 80\% to 83\% accuracy. The performance improvement highlights the impact of incorporating deeper convolutional blocks, batch normalization, and dropout regularization in the proposed architecture.

To provide a comprehensive comparison, Table~\ref{results} presents the accuracy of various models on the CIFAR-10 dataset. The table clearly indicates that the proposed model significantly outperforms LeNet-5 and other conventional CNNs, thereby establishing its superiority in terms of classification accuracy. This improvement underscores the importance of architectural enhancements in addressing the limitations of traditional CNN models, especially when dealing with datasets of limited size and resolution.

In addition to classification accuracy, a detailed comparison of training loss and test accuracy metrics was conducted between the proposed model and conventional CNN architectures. Figure\ref{fig-1} illustrates the training loss curves over epochs for both models, showing that the proposed model exhibits a consistently lower loss throughout the training process. This indicates better optimization and stability during training, primarily due to the integration of batch normalization, which mitigates internal covariate shifts and accelerates convergence.

Furthermore, Figure\ref{fig-1} also compares the test accuracy of the two models across epochs. The proposed model consistently achieves higher test accuracy than the baseline CNN, demonstrating its superior generalization capabilities. This improvement can be attributed to the use of dropout regularization, which effectively prevents overfitting by introducing stochastic noise during training, thus improving the model’s robustness when applied to unseen data.

Overall, the quantitative results strongly validate the effectiveness of the proposed architectural enhancements in addressing the key challenges associated with CIFAR-10 image classification. By achieving higher accuracy and better convergence, the proposed model sets a new benchmark for performance in this domain.


\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\linewidth]{myplot.png}}
\caption{Performance Comparison Chart}
\label{fig-1}
\end{figure}

\begin{table}[h]
\caption{Performance Comparison on CIFAR-10}
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Model} & \textbf{Accuracy (\%)} \\ \hline
Baseline CNN & 72.61 \\ \hline
Proposed CNN & \textbf{84.95} \\ \hline
\end{tabular}
\label{results}
\end{table}

\subsection{Ablation Study}

An ablation study was conducted to evaluate the contributions of each architectural enhancement to the overall performance of the proposed CNN model. This analysis involved systematically removing or altering specific components of the architecture and observing the resulting changes in accuracy on the CIFAR-10 test set. By isolating the effects of individual improvements, the study provides a clear understanding of the importance and impact of each enhancement.

\textbf{Batch Normalization}: Batch normalization was incorporated after each convolutional layer to normalize feature maps and stabilize the training process. This normalization reduces internal covariate shifts, allowing the network to converge faster and achieve better generalization. When batch normalization was removed from the architecture, a noticeable drop in accuracy of 1.8\% was observed. This highlights its critical role in improving training dynamics and ensuring robust performance, particularly in deeper networks where gradient instability can become a significant issue.

\textbf{Dropout}: Dropout layers were included after each max-pooling operation and the first fully connected layer to mitigate overfitting by randomly deactivating neurons during training. By introducing noise into the network, dropout encourages the model to learn more generalized patterns rather than memorizing the training data. When dropout was excluded, the model exhibited overfitting, as indicated by a reduction in test accuracy by 0.5\%. This demonstrates that dropout plays an essential role in regularization, especially for tasks like CIFAR-10, where the dataset size is limited.

\textbf{Deeper Network}: The architecture was designed with three convolutional blocks, each comprising two convolutional layers. This depth allows the network to extract hierarchical features, ranging from simple edges in the initial layers to complex textures and object-specific patterns in deeper layers. When the depth of the network was reduced by removing one convolutional block, the accuracy decreased by 2.0\%. This indicates that the additional depth significantly enhances the network's capacity to learn and represent features, which is vital for achieving high accuracy on a dataset with diverse classes such as CIFAR-10.

The results of this ablation study emphasize the synergistic effect of these enhancements. While each component independently contributes to improved performance, their combination results in a robust architecture that achieves state-of-the-art accuracy on CIFAR-10. The findings also underscore the importance of designing CNN architectures with a balanced focus on feature extraction, regularization, and training stability.


\section{Conclusion}

We propose an enhanced CNN architecture tailored specifically for image classification tasks on the CIFAR-10 dataset. This architecture leverages deeper convolutional layers, which enable the model to learn more complex and hierarchical features from the input images. The inclusion of batch normalization plays a critical role in stabilizing the training process by normalizing intermediate feature distributions, thereby mitigating the issue of vanishing or exploding gradients. Dropout layers are strategically integrated into the architecture to address overfitting, ensuring that the model generalizes well to unseen data. These combined enhancements contribute to the model achieving a test accuracy of 84.95\%, which represents a significant improvement over standard baseline CNN models.

The improvements observed with the proposed architecture underscore the importance of refining network design to balance depth, feature extraction capabilities, and regularization techniques. By carefully combining these elements, the model not only improves classification accuracy but also exhibits better robustness and generalization across the diverse image categories within CIFAR-10. These results highlight the potential of enhanced CNNs for tackling image classification tasks that require extracting fine-grained visual features while maintaining computational efficiency.

Future work will focus on extending this enhanced architecture to more complex datasets such as CIFAR-100, which consists of 100 classes and provides a more challenging testbed for multi-class classification models. Additionally, transfer learning techniques will be explored to adapt the proposed architecture to larger datasets with higher-resolution images, such as ImageNet, or domain-specific datasets, where pre-trained models can be fine-tuned for specialized tasks. These extensions will allow for further evaluation of the architecture's scalability and applicability across a broader range of computer vision problems, paving the way for its integration into practical applications.



\begin{thebibliography}{00}
\bibitem{qc1} Cheng, Qisen and Qu, Shuhui and Lee, Janghwan. "72-3: Deep Learning Based Visual Defect Detection in Noisy and Imbalanced Data." \textit{SID Symposium Digest of Technical Papers}, vol. 53, no. 1, pp. 971-974, 2022

\bibitem{qc2} Cheng, Qisen and Zhang, Chang and Shen, Xiang. "Estimation of Energy and Time Usage in 3D Printing With Multimodal Neural Network." \textit{2022 4th International Conference on Frontiers Technology of Information and Computer (ICFTIC)}, pp. 900-903, 2022

\bibitem{b1} Cifar10 Dataset. [online]. Avaiable:https://www.cs.toronto.edu/~kriz/cifar.html.

\bibitem{xing24enhancing} Xing, Jinming. "Enhancing Link Prediction with Fuzzy Graph Attention Networks and Dynamic Negative Sampling." arXiv preprint arXiv:2411.07482 (2024).

\bibitem{petar17gat} Veličković, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. "Graph attention networks." arXiv preprint arXiv:1710.10903 (2017).

\bibitem{ham17graphsage} Hamilton, Will, Zhitao Ying, and Jure Leskovec. "Inductive representation learning on large graphs." Advances in neural information processing systems 30 (2017).

\bibitem{xing2022weighted} Xing, Jinming, Can Gao, and Jie Zhou. "Weighted fuzzy rough sets-based tri-training and its application to medical diagnosis." Applied Soft Computing 124 (2022): 109025.

\bibitem{gao2022parameterized} Gao, Can, Jie Zhou, Jinming Xing, and Xiaodong Yue. "Parameterized maximum-entropy-based three-way approximate attribute reduction." International Journal of Approximate Reasoning 151 (2022): 85-100.

\bibitem{xing24fgatt} Xing, Jinming, Ruilin Xing, and Yan Sun. "FGATT: A Robust Framework for Wireless Data Imputation Using Fuzzy Graph Attention Networks and Transformer Encoders." arXiv preprint arXiv:2412.01979 (2024).

\bibitem{ravdess} S. R. Livingstone and F. A. Russo, "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)," \textit{PloS one}, vol. 13, no. 5, p. e0196391, 2018. Available: https://zenodo.org/record/1188976

\bibitem{xing24mfgat} Xing, Jinming, Dongwen Luo, Qisen Cheng, Chang Xue, and Ruilin Xing. "Multi-view Fuzzy Graph Attention Networks for Enhanced Graph Learning." arXiv preprint arXiv:2412.17271 (2024).

\bibitem{heigold2016end} G. Heigold, I. L. Moreno, S. Bengio, and N. Shazeer, "End-to-End Text-Dependent Speaker Verification," in \textit{Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 2016, pp. 5115–5119.

\bibitem{hochreiter1997long} S. Hochreiter and J. Schmidhuber, "Long Short-Term Memory," \textit{Neural Computation}, vol. 9, no. 8, pp. 1735–1780, 1997.

\bibitem{gers2000learning} F. A. Gers, J. Schmidhuber, and F. Cummins, "Learning to Forget: Continual Prediction with LSTM," \textit{Neural Computation}, vol. 12, no. 10, pp. 2451–2471, 2000.

\bibitem{xing24pooling} Xing, Jinming, Ruilin Xing, and Yan Sun. "Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis Perspective." arXiv preprint arXiv:2411.14654 (2024).





\end{thebibliography}
\vspace{12pt}


\end{document}
