\section{Related Work}

\subsection{Vision-based Navigation}

% Early approaches to robotics navigation primarily relied on manually crafted features such as optical flow and traditional algorithms like Markov localization, incremental localization, or landmark tracking. 
% %
% These methods required extensive tuning of hyperparameters and struggled to generalize effectively in unfamiliar environments. 
% %
% However, recent advancements in deep learning have demonstrated its capacity to learn robust models from large-scale datasets. 
% %
% Vision-based robots trained using end-to-end deep learning methods exhibit greater robustness, reduced reliance on hyperparameters, and improved generalization in unseen environments.

Classical SLAM-based methods~\cite{chaplot2020neural} and learning-based approaches~\cite{maksymets2021thda,mezghan2022memory,wang2024toward,qin2023supfusion} have both been applied to embodied visual navigation. While end-to-end learning techniques tend to rely less on manually designed components, they have demonstrated greater potential. For instance, Memory-Augmented RL~\cite{mezghan2022memory} employs an attention mechanism that uses episodic memory to facilitate navigation, achieving state-of-the-art results in ImageNav~\cite{zhu2017target} with four RGB cameras. In contrast, our model adopts a more straightforward architecture and still achieves superior performance. For single-camera setups, \cite{al2022zero} enhances performance by incorporating both goal-view rewards and goal-view sampling. We observe that applying this reward system and view sampling leads to further gains for OVRL models.

Similarly, end-to-end reinforcement learning methods have been applied to ObjectNav, utilizing data augmentation~\cite{maksymets2021thda} and auxiliary rewards~\cite{ye2021auxiliary} to enhance generalization. On the other hand, modular methods~\cite{chaplot2020object,ramakrishnan2022poni} separate the tasks of navigation and semantic mapping. A recent competitive imitation learning approach~\cite{ramrakhya2022habitat}, built on a large-scale dataset, offers another direction, which our work builds upon. In contrast to~\cite{mousavian2019visual}, which focuses on improving visual representations by incorporating semantic segmentation, we prioritize RGB-based representations in our approach.

\subsection{Diffusion Models for Image Generation}

% Diffusion models have recently emerged as a powerful class of generative models, showing remarkable success in the field of image generation. 
% %
% These models, inspired by non-equilibrium thermodynamics, iteratively add noise to data and then learn to reverse this process to generate new, high-quality samples from random noise. 
% %
% Compared to traditional generative models like GANs and VAEs, diffusion models offer advantages in stability during training and the ability to generate more diverse and realistic images. 
% %
% Recent works have demonstrated their effectiveness across various tasks, including unconditional image generation, super-resolution, and even text-to-image synthesis, highlighting their versatility and potential as a cornerstone in generative modeling research.

Recent advances in text-to-image diffusion models~\cite{dhariwal2021diffusion, ho2022classifier,nichol2021glide} have greatly improved instruction-driven image-to-image methods~\cite{brooks2023instructpix2pix,fu2023guiding,geng2024instructdiffusion,zhou2024minedreamer} like InstructPix2Pix~\cite{brooks2023instructpix2pix}, primarily used for image editing that aims to alter content while keeping the background constant. 
%
The method struggles with egocentric images in dynamic, navigation tasks, where the backgrounds change significantly and the images should conform more to physical rules.
% 
In this work, we train the MLLM-enhanced diffusion model to generate continuous future images for guiding real-time, low-level control for image navigation tasks.

\subsection{Pretrained Foundation Models for Embodied Tasks}

% Pretrained vision-language models have become increasingly influential in enabling robots to perform embodied tasks, where they interact with and understand their environment through both visual and linguistic inputs. 
% %
% These models leverage large-scale datasets to learn rich, multi-modal representations, which can be fine-tuned for specific tasks such as navigation, object manipulation, and human-robot interaction. 
% %
% By integrating vision and language, these models allow robots to understand complex instructions and reason about their surroundings in a more human-like manner. 
% %
% Recent advancements have demonstrated that pretrained models can significantly improve performance in embodied tasks, offering more generalizable and adaptable solutions compared to traditional, task-specific approaches.

Foundation models like Large Language Models (LLMs) and Diffusion Models have become a powerful tool for prior-knowledge reasoning in navigation, due to their information processing and generative abilities. For example, LLMs are used to predict correlations with the target object at both the object and room levels, aiding in locating the target~\cite{zhou2023esc, gadre2023cows}. They also help cluster unexplored areas and infer relationships between the target and surrounding objects to guide navigation~\cite{yu2023l3mvn, cai2024bridging}. Chain-of-thought (CoT) reasoning is integrated into LLMs to encourage exploration in areas likely to contain the target while avoiding irrelevant regions~\cite{shah2023navigation}. In multi-robot collaborative navigation, LLMs centrally plan mid-term goals by analyzing data such as obstacles, frontiers, and object coordinates from online maps~\cite{yu2023co}. Furthermore, path and farsight descriptions are combined to enable LLMs to apply commonsense reasoning for waypoint identification~\cite{wu2024voronav}.
%
Additionally, Diffusion Models have made significant advances in embodied scenarios. For instance, video diffusion is combined with inverse dynamics to generate robot control signals for specific tasks~\cite{du2024learning, ajay2024compositional}. In another approach, diffusion models are used for interpretable hierarchical planning through skill abstractions during task execution~\cite{liang2024skilldiffuser}. Furthermore, these models are employed to guide agents through open-ended tasks~\cite{qin2023mp5, zhou2024minedreamer}.