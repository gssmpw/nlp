\begin{table}[t]
\centering
\caption{Quantitative comparison (FID$\downarrow$, PSNR$\uparrow$, LPIPS$\downarrow$). IP2P has also been fine-tuned using the navigation training data.}
\vspace{-3mm}
\begin{tabular}{@{}lccc@{}}
\toprule
Setting            & FID$\downarrow$ & PSNR$\uparrow$    & LPIPS$\downarrow$   \\ 
\midrule
IP2P\cite{brooks2023instructpix2pix}        & 26.59          & 14.59            & 42.25   \\
\textbf{Predictor(Ours)}    & \textbf{25.93}          & \textbf{14.73}            & \textbf{40.82}   \\
\bottomrule
\end{tabular}
\label{tab:Quantitative_result}
\vspace{-3mm}
\end{table}

\begin{figure}[t]
  \centering 
  \includegraphics[width=0.45\textwidth]{fig/visualization.pdf}
  \vspace{-3mm}
  \caption{Example rollouts in Gibson dataset.}
  \label{fig:visualization}
\vspace{-7mm}
\end{figure}

\section{Experiments}
\label{sec_experiments}

\subsection{Predictor}
\label{exp_ffp}
\textbf{Dataset.}We used the dataset format constructed in Sec.~\ref{sec_data} and collected video sequences from all image navigation tasks in the GIBSON dataset's training set for training. The interval hyperparameter was set to $k=5$ during the training process.


\textbf{Training Process.} The training process of the Predictor consists of two key stages. First,  InstructPix2Pix~\cite{brooks2023instructpix2pix} is used to pre-train the diffusion model weights in the navigation environment. Next, the Predictor is optimized in an end-to-end fashion. The diffusion model's initial weights in the Predictor are taken directly from those pre-trained in the first stage.


\begin{table}[t]
\centering
\footnotesize
\caption{Comparison with state-of-the-art methods on Gibson.}
\vspace{-3mm}
\begin{tabular}{@{}lccccccc@{}}
\toprule
Method                  & Train Dataset        & Sensor(s)  & SPL           & SR                        \\ 
\midrule
NTS~\cite{chaplot2020neural}          & Full         & RGBD+Pose  & 43.0\%        & 63.0\%                    \\
Act-Neur-SLAM~\cite{chaplot2020neural}   & Full    & RGB+Pose      & 23.0\%        & 35.0\%    \\
SPTM~\cite{savinov2018semi}        & Full         & RGB+Pose   & 27.0\%        & 51.0\%                    \\
\midrule
ZER~\cite{al2022zero}          & Full         & RGB        & 21.6\%        & 29.2\%                    \\
ZSON~\cite{majumdar2022zson}        & Full        & RGB        & 28.0\%        & 36.9\%                    \\
OVRL~\cite{yadav2023offline}        & Full        & RGB        & 27.0\%        & 54.2\%                    \\
OVRL-V2~\cite{yadav2023ovrl}  & Full        & RGB        & 58.7\%        & 82.0\%                    \\
FGPrompt                & Full         & RGB        & 66.5\%        & 90.4\%                    \\
\textbf{NavigateDiff(Ours)} & Full     & RGB        & 64.8\%           & \textbf{91.0\%}                       \\
\midrule
FGPrompt~\cite{sun2024fgprompt}                & 1/4          & RGB        & 48.5\%        & 77.9\%                   \\
\textbf{NavigateDiff(Ours)} & 1/4     & RGB        & \textbf{52.1\%}           & \textbf{81.2\%}     \\
\midrule
FGPrompt~\cite{sun2024fgprompt}                & 1/8          & RGB        & 43.4\%        & 68.1\%                   \\
\textbf{NavigateDiff(Ours)} & 1/8     & RGB        & \textbf{46.4\%}           & \textbf{71.1\%}     \\

\bottomrule
\end{tabular}
\vspace{-5mm}
\label{tab:sota-gibson}
\end{table}



\textbf{Evaluation.} We implement three image-level metrics to evaluate the Predictor's generation ability. (1) Fr\'echet Inception Distance (FID)~\cite{heusel2017gans}, (2) Peak Signal-to-Noise Ratio (PSNR), (3)Learned Perceptual Image Patch Similarity (LPIPS)~\cite{zhang2018unreasonable}. We measure the similarity between the generated future frame and ground truth. In terms of image-level metrics in Tab.~\ref{tab:Quantitative_result}, our Predictor outperforms IP2P by a large margin (0.66, 0.14, and 1.43 respectively) in all three metrics on the Gibson dataset.
%
In Fig.~\ref{fig:visualization}, we also visualize predicted future frame sequences and trajectory rollouts in the Gibson dataset. We observe that generating future frames one by one could efficiently guide the PolicyNet in action generation.





\subsection{Simulation Experiments}
\textbf{Dataset}
For image-goal navigation, we employ the Habitat simulator and train our agent on the Gibson dataset, using 72 training scenes and 14 testing scenes under the standard configuration. We train the agent for 500M steps, following the rules as outlined in FGPrompt~\cite{sun2024fgprompt}. Results are reported across multiple datasets to enable direct comparison with previous works. For the Gibson dataset, we validate our agent on split A generated by~\cite{mezghan2022memory}. For the MP3D datasets, we use test episodes collected by~\cite{al2022zero}  and the instance image navigation dataset released by~\cite{krantz2022instance}. 

\textbf{Setting}
We report the Success Rate (SR) and Success weighted by Path Length (SPL)~\cite{anderson2018evaluation}, which accounts for the efficiency of the navigation path. An episode is deemed successful if the agent stops within a 1.0m Euclidean distance from the goal. The maximum number of steps per episode is set to 500 by default.
In agent configuration, We follow the setup outlined in prior works~\cite{al2022zero}~\cite{majumdar2022zson}~\cite{yadav2023offline}~\cite{sun2024fgprompt} to initialize an agent equipped with RGB cameras, featuring a 128 × 128 resolution and a 90° field of view (FOV). The agent's action space includes four discrete actions—MOVE\_FORWARD, TURN\_LEFT, TURN\_RIGHT, and STOP—with minimum rotation and forward movement units set to 30° and 0.25m, respectively.



\textbf{Result}
In Tab.~\ref{tab:sota-gibson}, we present a detailed comparison between our model and several state-of-the-art approaches across various metrics.The results highlight the superior performance of our model, particularly in challenging navigation scenarios. To further evaluate the generalization capability of our approach, we conducted additional experiments using a smaller dataset that poses a greater challenge in terms of data availability. Despite the reduced dataset size, our model not only maintained its performance but also outperformed the baseline models. This demonstrates the robustness and adaptability of our model, suggesting it can effectively generalize to new environments even with limited training data. 

As illustrated in Tab.~\ref{tab:cross-domain}, we test the model on the MP3D dataset as part of a cross-task evaluation.  Our \mname  ~achieves 68.0\% Success Rate (SR) and 41.1\% Success weighted by Path Length (SPL) by using a smaller training dataset, surpassing both existing methods on the full dataset and the baseline.

\begin{table}[t]
\centering
\footnotesize
\caption{Cross-domain evaluation on MP3D. The agent is trained in Gibson environments and directly transferred to new environments for evaluation.}
\vspace{-3mm}
\begin{tabular}{@{}lccc@{}}
\toprule
Methods        &               Train Dataset                  & SPL         & SR                  \\ 
\midrule
Mem-Aug~\cite{mezghan2022memory}   & Gibson                       & 3.9\%         & 6.9\%               \\
NRNS~\cite{hahn2021no}         & Gibson                        & 5.2\%         & 9.3\%               \\
ZER~\cite{al2022zero}           & Gibson                         & 10.8\%        & 14.6\%            \\
\midrule
FGPrompt~\cite{sun2024fgprompt}      & 1/4 Gibson                         &37.1\%             & 65.7\%\\ 
\textbf{NavigateDiff (Ours)}            & 1/4 Gibson    & \textbf{41.1\%}   & \textbf{68.0\%}\\ 
\midrule
FGPrompt~\cite{sun2024fgprompt}             & 1/8 Gibson                         &31.8\%             & 55.0\%\\ 
\textbf{NavigateDiff (Ours)}            & 1/8 Gibson    &\textbf{34.9\%}    & \textbf{57.7\%}\\ 
\bottomrule
\end{tabular}
\label{tab:cross-domain}
\end{table}


\begin{table}[t]
\centering
\scriptsize
\caption{Comparing in three diverse real-world environments scenes. (Office, Parking Lot and Corridor).}
\vspace{-3mm}
\begin{tabular}{@{}lccccccc@{}}
\toprule
\multirow{2}{*}{Methods}  & \multicolumn{2}{c}{Office} & \multicolumn{2}{c}{Parking Lot} & \multicolumn{2}{c}{Corridor} \\ 
\cmidrule(l){2-7} 
                     & SPL         & SR         & SPL         & SR       & SPL         & SR   \\ 
\midrule

FGPrompt~\cite{sun2024fgprompt}            &41.0\%& 50.0\%& 52.3\%& 64.3\% & 54.8\%& 71.4\%\\ 
\textbf{NavigateDiff (Ours)}        & \textbf{41.2\%}& \textbf{57.1\%}& 51.1\%& \textbf{71.4\%} & \textbf{55.6\%}& \textbf{78.6\%}\\ 
\bottomrule
\end{tabular}
\vspace{-2mm}
\label{tab:real-world}
\end{table}


\begin{figure}[t]
  \centering 
  \includegraphics[width=0.45\textwidth]{fig/concat4.png}
  \vspace{-2mm}
  \caption{Scenes in Real-world Environment, from the left, in the order of Corridor, Office and Parking Lot}
  \label{fig:real-world scenes}
\end{figure}

\begin{table}[t]
\centering
\small
\caption{Different fusion strategies on Gibson ImageNav task.}
\vspace{-3mm}
\begin{tabular}{@{}lcc@{}}
\toprule
Setting                       & SPL             & SR     \\ 
\midrule
Late Fusion                  & 11.7\%          & 13.7\% \\
Early Fusion                  & 20.5\%          & 40.1\% \\
\textbf{Hybrid Fusion} & \textbf{64.8\%} & \textbf{91.0\%} \\
\bottomrule
\end{tabular}
\label{tab:fusion}
\vspace{-3mm}
\end{table}



\subsection{Real-world Experiments}
\textbf{Setting}
 In our real-world experiments, we focused on indoor environments to evaluate the zero-shot navigation capabilities of our model, \mname. As illustrated in  Fig.~\ref{fig:real-world scenes}, we conducted tests in three types of indoor environments: an office, a parking lot, and a corridor. Each environment represents a unique set of challenges in terms of layout, lighting, and obstacles. 
%
The office setting is characterized by cluttered spaces, including desks, chairs, and other furniture. 
%
The indoor parking lot represents a semi-structured environment with clearly defined paths and open spaces but is filled with parked vehicles that act as static obstacles. 
%
The corridor is a long, narrow space with fewer obstacles but presents challenges in terms of navigation through tight spaces and sharp turns.

\textbf{Result}
As detailed in Tab.~\ref{tab:real-world}, We evaluate the performance of \mname ~in terms of success rate and SPL. The metric across the three real-world scenarios demonstrates that our model consistently surpasses the baseline.

Overall, \mname ~demonstrates strong zero-shot navigation capabilities across all environments. The model's ability to adapt to different layouts and lighting conditions without prior training in those specific environments highlights its robustness in real-world applications.


\subsection{Fusion Strategy Design}
As shown in Tab.~\ref{tab:fusion}, we evaluate different fusion strategies on the Gibson ImageNav task. Our proposed Hybrid Fusion achieves 91.0\% SR and 64.8\% SPL, significantly outperforming both Early Fusion and Late Fusion. These results demonstrate the effectiveness of Hybrid Fusion in integrating future frames into the navigation policy.


% The Gibson dataset offers a diverse range of photorealistic indoor environments, making it an ideal benchmark for testing various navigation policy designs. We experimented with three different fusion strategies: Early Fusion, Late Fusion, and our proposed Hybrid Fusion.

% Early Fusion: In this approach, the visual input from the target image and the current observation are fused at the initial stage before passing through any layers of the network. This method allows the model to process both inputs simultaneously but lacks a refined strategy to distinguish between the contributions of the target and the current scene. In our tests, Early Fusion showed moderate performance, struggling with complex scenes where the target and observation contained visually similar but contextually different elements.

% Late Fusion: Here, the fusion occurs at a much later stage, after the model has processed the current observation and target image separately. The two streams of information are only merged at the decision-making phase. This approach allowed the model to extract more complex features from both inputs independently before combining them. However, this method suffered from slower decision-making and slightly reduced accuracy, as the separation of information streams hindered real-time adjustments.

% Hybrid Fusion (Ours): Our proposed Hybrid Fusion strategy balances the strengths of both Early and Late Fusion. It fuses low-level features early in the network, allowing for joint learning of spatial correlations, while still maintaining separate pathways for high-level feature extraction. This hybrid approach enables the model to dynamically adjust to changes in the environment while efficiently processing both inputs. In the Gibson ImageNav task, Hybrid Fusion outperformed both Early and Late Fusion in terms of success rate, efficiency, and robustness, especially in complex, cluttered environments.

% We observed that Hybrid Fusion achieved the highest success rate, showing improved navigational accuracy and robustness across diverse environments. Early Fusion performed adequately but struggled in visually complex scenarios. Late Fusion, while extracting more detailed features, showed slower response times and was less adaptable in dynamic situations. Hybrid Fusion’s ability to integrate both low- and high-level features made it the most effective method for zero-shot navigation.



