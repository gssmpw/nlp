\section{Introduction}

A useful generalist navigation robot must be able to, much like a human, recognize and reason about novel environments and pathways it has never encountered before. 
%
For example, if a user instructs the robot to ``find and navigate to that red building," the robot should be able to accomplish this task even if it has never navigated in that environment or even seen a red building before. 
%
In other words, the robot needs not only the physical capability to move through complex environments but also the ability to understand the underlying world rules and perform logical reasoning in environments beyond its training distribution.
%
Although the size of datasets used for robotic navigation has increased in recent years, these datasets cannot possibly cover every environment and scenario the robot might encounter—just as a person's life experiences cannot include physical interactions with every type of environment. 
%
While these datasets provide numerous examples of navigation scenarios, they lack the broad logical knowledge required for robots to navigate the specific environments they may encounter in everyday tasks.

\begin{figure}[thpb]
  \centering 
  \includegraphics[width=0.45\textwidth]{fig/motivation_v2.pdf}
  \caption{\mname ~leverages logical reasoning and generalization capabilities of pre-trained foundation models to enhance zero-shot navigation by predicting future observations to assist robot generate robust actions}
  \label{fig:motivation}
\end{figure}

Given this, one question naturally arises: how can we integrate this logical knowledge into navigation control? 
%
Recently, machine learning methods have achieved broad success in natural language processing~\cite{brown2020language}, visual perception~\cite{podell2023sdxl,yu2025gamefactory,qin2024worldsimbench,li2025t2isafety,an2024agfsync}, and other domains~\cite{brohan2022rt,brohan2023rt,huang2024story3d,zhang2024ad} by leveraging Internet-scale data to train general-purpose “foundation” models. 
%
These models are equipped with extensive logical knowledge and can adapt to new tasks through zero-shot transfer, prompt tuning, or fine-tuning on target data~\cite{qin2023mp5,li2024manipllm,zhou2024code}. 
%
Building on these advancements, researchers in the navigation field have begun to utilize pre-trained foundation models in vision and language to build navigation systems~\cite{shah2023vint} or use pre-trained vision-language encoders to initialize navigation policies~\cite{ lin2024navcot, NavGPT}.
%
Although these methods aim to incorporate the logical knowledge in Internet-scale data into navigation model training, they (1) often fail to fully exploit the diversity of Internet data, instead being confined to navigation-specific data; (2) merely improve high-level text instruction generalization, which is difficult to apply to specific navigation tasks as the policy network cannot grasp abstract textual concepts.


%While these methods aim to incorporate the logical knowledge in Internet-scale data into navigation model training, they either fail to fully leverage the diversity of Internet data (being limited to navigation-specific data) or are restricted to text-based modalities, improving policy performance only in terms of high-level generalization of text instructions. 
%
%Generalization at the textual level is difficult to transfer to specific navigation tasks since the vision-based downstream policy network cannot comprehend abstract textual concepts.



To address this, we propose \mname, a novel navigation framework to realize generalizable navigation, which introduces the visual predictors to bridge the gap between the high-level trajectory planning in the scene and low-level policy generation for the robot control.
%
As illustrated in Fig.~\ref{fig:motivation}, unlike existing approaches that directly generate robotic policies using current observations and goal images, the proposed method leverages the logical knowledge of foundation models to infer the robot's next state and generate the corresponding visual content. The results of this visual prediction, along with the original goal image, are then used to assist the policy network in generating specific actions for the robot.
%
In practice, We first construct a \textbf{Predictor} by leveraging a large vision-language model followed by a diffusion model. This predictor reasons about the next predicted frame, which is helpful for navigation task execution, by leveraging the goal image, current observation, and optional additional information (such as text instructions or historical observation data). 
%
Next, to achieve robot control, we also train a \textbf{Hybrid Fusion Policy Network}, which outputs the final control signal by integrating multiple visual information sources, \textit{i.e.} the goal image, current observation, and generated future frame.



%To address this, we propose \mname ~for achieving generalizable navigation by constructing a pre-trained image generation model. As shown in Fig.~\ref{fig:motivation}, with the help of the foundation model's logical knowledge and generation ability, future state plannings within the vision modality are generated as the goal of the agent to assist the low-level control of navigation.
%
%By integrating the rich logical knowledge of large language models (LLMs) with the powerful image generation capabilities of diffusion models, alongside rich historical frame information from the navigation process, we have developed a Predictor with scene understanding and predictive capabilities. 
%
%This model can generate a corresponding future frame based on the goal image, current observation, and optional additional information (such as text instructions or historical observation data).


Our proposed method has achieved impressive navigation results in both simulation and real-world scenarios. These results are attributed to the fact that our framework decouples the complex high-level reasoning and efficient low-level control required for navigation in open environments.
%
Specifically, when the model needs to analyze and reason about navigation tasks, it can directly leverage the logical knowledge of the vision-language model for effective reasoning, bypassing the need to understand the low-level dynamics of the robot.
%
Conversely, when the model is required to control the robot, the policy network can focus solely on the relationship between the current state and the immediate next state (\textit{i.e.} generated future frame), without the burden of considering long-term task logic or intricate environmental rules.
%
%
%This approach does not require the model to precisely understand the low-level dynamics of the robot during navigation, making it easier to learn physical laws (such as perspective relationships) and logical information (such as motion trajectories) from other data sources (e.g., human videos), even if the low-level physical interactions and specific characteristics of the navigation platform in these data sources do not fully match. 
%
%Although the Predictor provides one-step future state planning within the vision modality, we also need to train a low-level controller to select appropriate navigation actions. 
%
%this policy only needs to utilize the generated future frames to infer the relationship between vision and motion to determine the correct navigation actions, without needing to understand high-level world rules and logic.
%
%We designed a Fusion policy based on Hybrid Fusion to effectively incorporate future frames into the low-level policy. This approach achieved excellent results in image navigation tasks and was compared with various fusion strategies to validate the effectiveness of Hybrid Fusion.
%
In summary, the main contributions are as follows:
%Our contributions can be summarized as follows:
\begin{itemize}
%\item We propose a novel navigation framework termed \mname ~that leverages logical information from pre-trained foundation models to improve robot navigation control and enhance generalization.
%
\item We propose \mname, a novel navigation framework designed to separate high-level task reasoning from low-level robot control. This decoupling enhances the effectiveness and generalizability of visual navigation.

\item We introduce a Predictor that harnesses the advanced reasoning capabilities of large vision-language models to produce a plausible intermediate frame, facilitating downstream robotic control. Additionally, the Hybrid Fusion Policy Network ensures that the generated actions are more stable in practice.
%
%the generative ability of diffusion models, and the situation-aware embodied environmental information, enabling the generation of logically coherent and physically accurate future frames.
%
\item We conduct extensive experiments in both simulated and real-world settings, yielding numerous experimental results that validate the effectiveness and robustness of our proposed framework.
\end{itemize}