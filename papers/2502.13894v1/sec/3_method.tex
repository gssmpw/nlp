\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.99\textwidth]{fig/generator.pdf}
    \caption{\textbf{The Overall Architecture of Predictor.} Instruction, current observation, and goal image are encoded separately and sent into LLaVA~\cite{liu2024visual}. Then LLaVA~\cite{liu2024visual} generates hidden states for the Special Image Tokens $<$image$>$ tokens, we transform $<$image$>$ into semantically relevant representations $f^N$ using a Q-Former. The feature $f^H$, extracted from the 2D encoder, is fused with the feature $f^N$. The resulting fused feature $f^*$ is then used as a condition in the Edit-based diffusion model to generate future images.}
    \label{fig:generator}
\end{figure*}

\section{Method}





\mname ~utilizes the logical knowledge and the generalization ability of pre-trained foundation models to improve zero-shot navigation in the presence of novel environments, scenes, and objects. 
%
How can we achieve this when foundation models trained on general-purpose Internet data do not provide direct guidance for selecting low-level navigation actions? 
%
Our key insight is to decouple the navigation problem into two stages: (I) generating intermediate future goals that need to be reached to successfully navigate, and (II) learning low-level control policies for reaching these future goals. 
%
In Stage (I), we build a \textbf{Predictor} by incorporating a Multimodal Large Language Model (MLLM) with a diffusion model, fine-tuned with parameter efficiency, specifically designed for Image Navigation~\cite{zhu2017target}. 
%
Stage (II) involves training a \textbf{Fusion Navigation Policy} on image navigation data and testing in new environments. We describe data collection and each of these stages in detail below and summarize the resulting navigation algorithm.

\subsection{Data Formulation}
\label{sec_data}
For the generation of future frame training data, we utilized the simulator's built-in ``shortest path follower" algorithm in the simulation environment to obtain the standard route for each task and generate corresponding videos. 
%
In the real world, we recorded ego-view perspective videos of a human remotely controlling a navigation robot to complete the image navigation tasks.
%
Based on the collected videos, we randomly selected a starting frame from each video and chose the corresponding future frame according to a predefined prediction interval $k$. 
%
We also recorded the relevant navigation task information, ultimately forming the training tuples $(x_t, x_{t+k}, x_h, y, x_g)$.
%
where $x_t$ represents the current observation image, $x_{t+k}$ is the future frame image that needs to be predicted, $x_h$ is the history frame of $x_t$, $y$ indicates the task's corresponding textual instruction and $x_g$ is the final goal image for the navigation task.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{fig/pipeline.pdf}
    \caption{\textbf{\mname} ~leverages a Future Frame pretrained Predictor to generate future frames based on current observation and navigation task information. A fusion navigation policy then executes the actions needed to reach the future frames. Alternating this loop enables us to accomplish the navigation task.}
    \label{fig:pipeline}
\end{figure*}


\subsection{Predictor}

As illustrated in Fig.~\ref{fig:generator}, our Predictor integrates a \textbf{Multimodal Large Language Model~(MLLM)} with a \textbf{Future Frame Prediction Model}, which could process current observations, target images, and instructions and generate a predicted future image based on the provided input.

 \textbf{Multimodal Large Language Model.} 
Given a current observation $x_t$, a goal image $x_g$, and a textual instruction $y$, the Predictor generates a future frame. 
%
As shown in Fig.~\ref{fig:generator}, the current observation and goal image are processed through a frozen image encoder, while the textual instruction is tokenized and passed to the LLM. 
%
The Predictor can now generate Special Image Tokens $<$image$>$ based on the instruction's intent, though initially limited to the language modality. 
%
These tokens are then sent to the Future Frame Prediction Model to produce the final future frame prediction.
%
In practice, we utilized LLaVA as our base model, which consists of a pre-trained CLIP visual encoder (ViT-L/14) and Vicuna-7B as the LLM backbone. 
%
To fine-tune the LLM, we applied LoRA, initializing a trainable matrix to adapt the model for the task.

 \textbf{Future Frame Prediction Model.} For future frame generation, we bridge the gap between the LLM's hidden states and the LLaVA text encoder's spaces, we transform the Special Image Tokens $<$image$>$ into semantically relevant representations $f^N$ using a Q-Former. 
%
The feature $f^H$, extracted from the 2D encoder, is fused with the feature $f^N$. 
%
The resulting fused feature $f^*$ is then used as a condition in the Edits-based diffusion model~\cite{koh2024generating} to generate future images:
{
\vspace{-2mm}
% \small
\begin{equation}
f^*=\mathbf{H}(\mathbf{Q}\left(h_{<image>}\right), \mathbf{E}_{v}(x_{h}))
\end{equation}
}
%
where $\mathbf{Q}$ denotes the Q-Former, $\mathbf{E}_{v}$ is a 2D encoder, $\mathbf{E}_{v}(x_{h})$ indicates history observations, and $\mathbf{H}$ is the fusion block that includes two self-attention blocks, a cross-attention block, and an MLP layer.

We employ a latent diffusion model with a VAE to perform denoising diffusion in the latent space, conditioned on fused feature $f^*$. Formally, the training objective is given by:
{
\begin{equation}
\begin{split}
\mathcal{L}_{\mathrm{predictor}}=\mathbb{E}_{\mathcal{E}(x_{t+k}), \mathcal{E}(x_{t}), \epsilon \sim \mathcal{N}(0,1), s}[\| \epsilon \nonumber -\\\epsilon_\delta(s, [z_s, \mathcal{E}(x_{t})]+f^*) \|_2^2]
\label{eq:diffusion}
\end{split}
\tag{2}
\end{equation}
}
where $\epsilon$ represents unscaled noise, $s$ denotes the sampling step, and $z_{s}$ is the latent noise at step $s$. The term $\mathcal{E}(x_t)$ corresponds to the current observation condition. 

By incorporating temporal information, this hybrid approach captures object trajectories and richer scene semantics, allowing for more accurate predictions of object movements and environmental changes. This makes our method particularly effective for zero-shot navigation in dynamic scenes. Dataset construction and implementation details can be found in \ref{exp_ffp}.




\subsection{Fusion Navigation Policy}

Although the Predictor provides one-step future state planning within the vision modality, we also need to train a low-level controller to select appropriate navigation actions. 
%
As the Predictor generates future frame images infused with prior knowledge from the pre-trained foundation model, our approach requires only the design of an image fusion navigation policy to effectively leverage the foundational model's logical reasoning and generalization capabilities for robot navigation tasks.

 \textbf{Image Fusion Policy.}
During the training phase, the current observation $x_t$ is concatenated with the future frame $x_{t+k}$ along the RGB channel dimension and processed through a trainable 2D Encoder to obtain $f_p$. The current observation $x_t$ is also concatenated with the final goal image $x_g$ along the RGB channel dimension and passed through a pre-trained 2D Encoder to obtain $f_o$. Based on the fused representations of the future and goal images, we train a navigation policy $\pi$ using reinforcement learning (RL):
\begin{equation}
s_t = \pi(~[~f_p, f_o, a_{t-1}~]~|~h_{t-1}~)
\end{equation}
where $s_t$ represents the embedding of the agent's current state, and $h_{t-1}$ denotes the hidden state of the recurrent layers in the policy $\pi$ from the previous step. We employ an actor-critic network~\cite{al2022zero} to predict both the state value and the action $a_t$ using $s_t$, training the model end-to-end with PPO \cite{schulman2017proximal}.

During the testing phase, we utilize trained Predictor and Fusion Navigation Policy to navigate in new environments based on corresponding goal images. For the image navigation task with a goal image $x_g$, given a current observation $x_t$, we generate the next future frame $x_t^* \leftarrow \mathcal{G}(x_t,x_g,y)$, once the future frame is sampled, we then execute the Fusion Navigation Policy $\pi$ conditioned on $x_t^*,x_t,x_g$ for $k$ timesteps, where $k$ is a testing hyperparameter. After $k$ timesteps, the future frame is refreshed by sampling from the Predictor again, and the process is repeated. Conventional wisdom suggests that regenerating future frames more frequently could result in more robust navigation control, assuming an unlimited computational budget. However, in practice, to maintain computational efficiency, we set $k$ to closely match the corresponding timesteps used during training, which has consistently delivered satisfactory performance. The pseudocode for navigation testing is detailed in Algorithm.~\ref{alg:NavigateDiff}.

\begin{algorithm}[ht]
\small
\caption{NavigateDiff: Zero-Shot Navigation Testing}
\label{alg:NavigateDiff}
\begin{algorithmic}[1]
\REQUIRE Predictor $\mathcal{G}$, Fusion Policy $\mathcal{\pi}$, time limit $T$ , future frame generation interval $k$, goal image $x_g$, instruction $y$

\STATE $t \leftarrow 0$

\WHILE{$t < T$}
    \STATE $x_t^* \leftarrow \mathcal{G}(x_t,x_g,y)$
    \FOR{$i = 1$ to $k$}
        \STATE $a_t \leftarrow \mathcal{\pi}(x_t^*,x_t,x_g)$
        \STATE Execute $a_t$
        \STATE $x_{t+1} \leftarrow $ Environment
        \STATE $t \leftarrow t+1$
    \ENDFOR
\ENDWHILE

\end{algorithmic}
\end{algorithm}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/fusion.png}
    \caption{Comparison of different image fusion strategies (a)~Early Fusion, (b)~Late Fusion, and (c)~\textbf{Hybrid Fusion(Ours)}.}
    \label{fig:fusion}
\end{figure*}

\noindent \textbf{Fusion Strategy Design.}
%
As shown in Fig.~\ref{fig:fusion}, we designed a Hybrid Fusion approach to fuse image features and compared its performance with Early Fusion and Late Fusion. 
%
In Early Fusion, the current observation, future frame prediction, and goal image are concatenated along the RGB channels and then passed through a visual encoder for feature extraction. 
%
While this method can capture pixel-level semantic relationships among the three images, it struggles to effectively associate the logical relationships among them. 
%
In contrast, Late Fusion processes the three images separately through the visual encoder and then fuses them at the feature level, but this approach fails to capture pixel-level semantic correlations, leading to suboptimal performance.

Our proposed Hybrid Fusion takes a different approach: one branch concatenates the current observation and future frame prediction along the RGB channels, while another branch concatenates the current observation and goal image. 
%
This not only establishes semantic associations at the pixel level but also separates local information (from the current observation to the future frame prediction) and global information (from the current observation to the goal image) in the temporal dimension, resulting in superior performance. 
