\section{Related Work}
\subsection{Few-sample Model Compression}
Few-sample model compression aims to derive compact models from pre-trained overparameterized networks using few samples. The main challenge is that the student model tends to overfit due to the scarcity of training data, leading to high inference errors. These errors progressively escalate through layer-wise propagation and accumulation~\citep{dong2017learning}, severely compromising output reliability.


To mitigate this problem, existing approaches employ layer-wise optimization to enforce intermediate-layer consistency between the compressed and original models. For instance, \citet{bai2020few} developed cross distillation (CD), which interleaves the teacher and student hidden layers to suppress inter-layer error propagation. Similarly, FSKD~\citep{li2020few} introduces learnable 
$1 \times 1$ convolutions at student network blocks, optimizing auxiliary parameters to bridge block-level representation gaps with the teacher mode. Such layer-wise optimization methods are computationally inefficient and not immune to the risk of error accumulation inherent in their design. 

Therefore, new approaches have gradually abolished the layer-wise reconstruction framework. For instance, MiR~\citep{wang2022mir} aligned outputs at the penultimate layer of teacher and pruned student models, then substituted all layers except the head before the penultimate layer in the teacher model with the trained student model. Another work~\citep{wang2023practical} replaced traditional filter pruning with block dropping and introduced a latency-accuracy evaluation metric, emphasizing the recovery efficiency of the compressed model and advocating for compression strategies with a high acceleration ratio. 

However, these methods overlook sample distribution imbalances, which can result in the loss of important information from minority classes during compression. Moreover, fine-tuning may exacerbate this bias towards majority classes, reducing the overall accuracy and weakening the generalization ability of the compressed model.


\subsection{Class Imbalance Problem}
In classification tasks, class imbalance occurs when the sample sizes of different classes vary significantly~\citep{ochal2023few}, causing the model to focus on majority classes and neglect critical minority-class information. Current solutions for addressing class imbalance problems can be classified into three main categories: data-level methods, algorithm-level methods, and hybrid approaches that combine both.

Data-level methods achieve class balance by either removing majority-class samples~\citep{liu2008exploratory,lin2017clustering,mohammed2020machine} or generating additional minority-class samples~\citep{chawla2002smote,sharma2022review,abdi2015combat}. These methods are simple and effective, making them the most commonly employed strategies. However, they also have limitations. For instance, removing majority-class samples may result in the loss of valuable information, leading to underfitting, while generating additional minority-class samples may cause overfitting~\citep{zhou2020bbn}, thereby impacting the model's generalization performance. 

Algorithm-level methods~\citep{zhou2005training,fernandez2018cost,he2024multi} attempt to mitigate the preference for majority classes by modifying existing machine learning algorithms. However, these methods often require extensive domain knowledge and experimentation, which may not be ideal for few-shot scenarios. Hybrid methods typically combine data-level or algorithm-level strategies with ensemble learning~\citep{galar2011review,chawla2003smoteboost}, taking advantage of the strengths of both strategies. Nevertheless, such hybrid methods inherit the limitations of the data-level or algorithm-level approaches and usually involve substantial computational overhead, making them difficult to implement on lightweight devices.

Although the class imbalance problem has been extensively studied across various tasks, research on the generalization performance of these methods in the context of few-sample model compression remains limited.